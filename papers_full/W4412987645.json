{
  "title": "Reviewing clinical knowledge in medical large language models: Training and beyond",
  "url": "https://openalex.org/W4412987645",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2143171724",
      "name": "Qiyuan Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2131874151",
      "name": "Haijiang Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2668067220",
      "name": "Caicai Guo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2020753383",
      "name": "Chao Gao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096410264",
      "name": "Deyu Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102035757",
      "name": "Meng Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1990346629",
      "name": "Feng Gao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A333553561",
      "name": "Frank van Harmelen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2421283178",
      "name": "Jinguang Gu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6873032070",
    "https://openalex.org/W6947997694",
    "https://openalex.org/W4396856309",
    "https://openalex.org/W6872110251",
    "https://openalex.org/W6877881676",
    "https://openalex.org/W6876447090",
    "https://openalex.org/W6879678122",
    "https://openalex.org/W6876496981",
    "https://openalex.org/W6870111381",
    "https://openalex.org/W6966881265",
    "https://openalex.org/W4399737726",
    "https://openalex.org/W4409695426",
    "https://openalex.org/W6874165135",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4385381606",
    "https://openalex.org/W4385266429",
    "https://openalex.org/W4406602253",
    "https://openalex.org/W4321015976",
    "https://openalex.org/W6856051742",
    "https://openalex.org/W6850625674",
    "https://openalex.org/W6847118041",
    "https://openalex.org/W6781031682",
    "https://openalex.org/W6785964655",
    "https://openalex.org/W4403625924",
    "https://openalex.org/W4312220150",
    "https://openalex.org/W6857112741",
    "https://openalex.org/W6859002407",
    "https://openalex.org/W6788175385",
    "https://openalex.org/W6863458788",
    "https://openalex.org/W6854691855",
    "https://openalex.org/W6851195955",
    "https://openalex.org/W6758679787",
    "https://openalex.org/W6846498073",
    "https://openalex.org/W6712503630",
    "https://openalex.org/W6761260114",
    "https://openalex.org/W4366280062",
    "https://openalex.org/W6884907361",
    "https://openalex.org/W6853244322",
    "https://openalex.org/W3162922479",
    "https://openalex.org/W6767102903",
    "https://openalex.org/W2903314293",
    "https://openalex.org/W6758691756",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W6861291033",
    "https://openalex.org/W6775041699",
    "https://openalex.org/W6756644427",
    "https://openalex.org/W6753792695",
    "https://openalex.org/W6766819354",
    "https://openalex.org/W6785642512",
    "https://openalex.org/W6791613716",
    "https://openalex.org/W6849617328",
    "https://openalex.org/W6858555813",
    "https://openalex.org/W4392044798",
    "https://openalex.org/W6847260572",
    "https://openalex.org/W6850758233",
    "https://openalex.org/W6853920016",
    "https://openalex.org/W6852178287",
    "https://openalex.org/W6875417482",
    "https://openalex.org/W6852126529",
    "https://openalex.org/W6858608540",
    "https://openalex.org/W6868089270",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6846002521",
    "https://openalex.org/W6633714834",
    "https://openalex.org/W6810738896",
    "https://openalex.org/W4388725043",
    "https://openalex.org/W6797324770",
    "https://openalex.org/W4390884707",
    "https://openalex.org/W6809683662",
    "https://openalex.org/W4406152279",
    "https://openalex.org/W6857797255",
    "https://openalex.org/W6851565042",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W6852800892",
    "https://openalex.org/W6853688356",
    "https://openalex.org/W6858556105",
    "https://openalex.org/W4402671943",
    "https://openalex.org/W6858028522",
    "https://openalex.org/W4404708476",
    "https://openalex.org/W6775750170",
    "https://openalex.org/W4401397240",
    "https://openalex.org/W6839015040",
    "https://openalex.org/W6850503672",
    "https://openalex.org/W6853469104",
    "https://openalex.org/W4408079830",
    "https://openalex.org/W6855873836",
    "https://openalex.org/W6856694485",
    "https://openalex.org/W6847873901",
    "https://openalex.org/W6857871135",
    "https://openalex.org/W6785768455",
    "https://openalex.org/W6857711689",
    "https://openalex.org/W6948117142",
    "https://openalex.org/W6873881427",
    "https://openalex.org/W4366563389",
    "https://openalex.org/W6859385493",
    "https://openalex.org/W4402670791",
    "https://openalex.org/W6875765046",
    "https://openalex.org/W6870684966",
    "https://openalex.org/W6948194969",
    "https://openalex.org/W6876507115",
    "https://openalex.org/W4408705495",
    "https://openalex.org/W4390692489",
    "https://openalex.org/W4409825362",
    "https://openalex.org/W6864885487",
    "https://openalex.org/W6810368914",
    "https://openalex.org/W6852382510",
    "https://openalex.org/W6856973282",
    "https://openalex.org/W6849644506",
    "https://openalex.org/W6875897013",
    "https://openalex.org/W6877810231",
    "https://openalex.org/W6871827379",
    "https://openalex.org/W6873237160",
    "https://openalex.org/W6852466558",
    "https://openalex.org/W6856309975",
    "https://openalex.org/W4404138521",
    "https://openalex.org/W6872941299",
    "https://openalex.org/W6891767026",
    "https://openalex.org/W4402567773",
    "https://openalex.org/W6877303497",
    "https://openalex.org/W6858158177",
    "https://openalex.org/W6879416724",
    "https://openalex.org/W6854078521",
    "https://openalex.org/W4311821345",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W6678262379",
    "https://openalex.org/W6761205521",
    "https://openalex.org/W4386492483",
    "https://openalex.org/W6869677574",
    "https://openalex.org/W6855498594",
    "https://openalex.org/W6856947538",
    "https://openalex.org/W6860320582",
    "https://openalex.org/W6903512047",
    "https://openalex.org/W4390146402",
    "https://openalex.org/W6810657915",
    "https://openalex.org/W6873168771",
    "https://openalex.org/W4382930233",
    "https://openalex.org/W6868742288",
    "https://openalex.org/W4391170193",
    "https://openalex.org/W4386120650",
    "https://openalex.org/W6875775607",
    "https://openalex.org/W6873782514",
    "https://openalex.org/W4379769651",
    "https://openalex.org/W4393021028",
    "https://openalex.org/W6861845406",
    "https://openalex.org/W4407308237",
    "https://openalex.org/W2600463316",
    "https://openalex.org/W3172625415",
    "https://openalex.org/W4387389711",
    "https://openalex.org/W4411147644",
    "https://openalex.org/W2610930722",
    "https://openalex.org/W4225401196",
    "https://openalex.org/W4404510240",
    "https://openalex.org/W4412888943",
    "https://openalex.org/W2913279579",
    "https://openalex.org/W4406800520",
    "https://openalex.org/W4393300262",
    "https://openalex.org/W4406658975",
    "https://openalex.org/W4388482211",
    "https://openalex.org/W4389912020",
    "https://openalex.org/W4409657429",
    "https://openalex.org/W4247924304",
    "https://openalex.org/W4399400859",
    "https://openalex.org/W2990138404",
    "https://openalex.org/W2606555609",
    "https://openalex.org/W1593271688",
    "https://openalex.org/W4389519153",
    "https://openalex.org/W4406841370",
    "https://openalex.org/W4407759974",
    "https://openalex.org/W4389116614",
    "https://openalex.org/W4403322754",
    "https://openalex.org/W4406549032",
    "https://openalex.org/W4407632471",
    "https://openalex.org/W4388092223",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4382618657",
    "https://openalex.org/W4205807230",
    "https://openalex.org/W4366999191",
    "https://openalex.org/W1583837637",
    "https://openalex.org/W4400324908",
    "https://openalex.org/W4402692877",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4387947536",
    "https://openalex.org/W4402671009",
    "https://openalex.org/W4399807068",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4403346066",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W4389780630",
    "https://openalex.org/W4366198844",
    "https://openalex.org/W4402923149",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4403443396",
    "https://openalex.org/W4323066559",
    "https://openalex.org/W4381253519",
    "https://openalex.org/W4386794440",
    "https://openalex.org/W4405766628",
    "https://openalex.org/W4406870160",
    "https://openalex.org/W4388788058",
    "https://openalex.org/W128638292",
    "https://openalex.org/W4408165783",
    "https://openalex.org/W4221149706",
    "https://openalex.org/W2750779823",
    "https://openalex.org/W4391028459",
    "https://openalex.org/W2599674900",
    "https://openalex.org/W3012608737",
    "https://openalex.org/W4386554558",
    "https://openalex.org/W4361193485",
    "https://openalex.org/W2620949368",
    "https://openalex.org/W4410232646",
    "https://openalex.org/W4362598952",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2915027006",
    "https://openalex.org/W4403851311",
    "https://openalex.org/W4405424255",
    "https://openalex.org/W4386081793",
    "https://openalex.org/W2901466771",
    "https://openalex.org/W2913352150",
    "https://openalex.org/W4381930847",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4386273007",
    "https://openalex.org/W4387687904"
  ],
  "abstract": null,
  "full_text": "Reviewing Clinical Knowledge in Medical Large Language\nModels: Training and Beyond\nQiyuan Lia,b, Haijiang Liua,b, Caicai Guoa,b, Chao Gaoa, Deyu Chenc, Meng Wangd, Feng\nGaoa,b, Frank van Harmelene, Jinguang Gua,b,∗\naSchool of Computer Science and Technology, Wuhan University of Science and\nTechnology, Wuhan, 430065, Hubei, China\nbHubei Province Key Laboratory of Intelligent Information Processing and Real-time Industrial\nSystem, Wuhan, 430065, Hubei, China\ncSchool of Computer Science and Technology, Huazhong University of Science and\nTechnology, Wuhan, 430074, Hubei, China\ndSchool of Cyber Science and Engineering, Wuhan University, Wuhan, 430072, Hubei, China\neDepartment of Computer Science, Vrije Universiteit Amsterdam, Amsterdam, The Netherlands\nAbstract\nThe large-scale development of large language models (LLMs) in medical contexts, such as\ndiagnostic assistance and treatment recommendations, necessitates that these models possess ac-\ncurate medical knowledge and deliver traceable decision-making processes. Clinical knowledge,\nencompassing the insights gained from research on the causes, prognosis, diagnosis, and treat-\nment of diseases, has been extensively examined within real-world medical practices. Recently,\nthere has been a notable increase in research e fforts aimed at integrating this type of knowl-\nedge into LLMs, encompassing not only traditional text and multimodal data integration but\nalso technologies such as knowledge graphs (KGs) and retrieval-augmented generation (RAG).\nIn this paper, we review the various initiatives to embed clinical knowledge into training-based,\nKG-supported, and RAG-assisted LLMs. We begin by gathering reliable knowledge sources\nfrom the medical domain, including databases and datasets. Next, we evaluate implementations\nfor integrating clinical knowledge through specialized datasets and collaborations with exter-\nnal knowledge sources such as KGs and relevant documentation. Furthermore, we discuss the\napplications of the developed medical LLMs in the industrial sector to assess the disparity be-\ntween models developed in academic settings and those in industry. We conclude the survey by\npresenting evaluation systems applicable to relevant tasks and identifying potential challenges\nfacing this field. In this review, we do not aim for completeness, since any ostensibly ”complete”\nreview would soon be outdated. Our goal is to illustrate diversity by selecting representative and\naccessible items from current research and industry practices, reflecting real-world situations\nrather than claiming completeness. Thus, we emphasize showcasing diverse approaches.\nKeywords: Large Language Models; Clinical Knowledge; Medical Academic; Medical Practice\n∗Corresponding author at: School of Computer Science and Technology, Wuhan University of Science and Technol-\nogy.\nEmail addresses:vickyuan@wust.edu.cn (Qiyuan Li), alecliu@ontoweb.wust.edu.cn (Haijiang Liu),\nguocaicai@ontoweb.wust.edu.cn (Caicai Guo), gchao@ieee.org (Chao Gao), deyuchen@hust.edu.cn (Deyu\nPreprint submitted to Elsevier August 12, 2025\narXiv:2502.20988v2  [cs.AI]  11 Aug 2025\n1. Introduction\nClinical knowledge, derived from a large amount of medical literature, expert experience,\nand cases, is the principles doctors follow in diagnosis and treatment. Various studies have\nintegrated clinical knowledge in accurate medical artificial intelligence (AI) systems for real-life\nassistance. Traditional medical AI research [1, 2] tends to discover patterns from large amounts\nof medical data to learn these principles. With the development of deep machine learning, two\nlines of studies have formed to build a more accurate model: direct learning [3] and knowledge\nextraction [4, 5].\n(1) Researchers on direct learning [3] use deep neural networks to learn knowledge directly\nfrom massive medical data and build medical models based on clinical knowledge. Integrating\nand modeling this knowledge can provide a deeper understanding of the disease mechanism and\nsupport clinical decision-making. (2) Researchers on knowledge extractions [5] have developed\nnatural language processing (NLP) frameworks to extract knowledge from vast medical corpora\nand construct clinical medical knowledge graphs (KGs), which are used as external knowledge to\nassist prediction models. Moreover, these clinical knowledge bases (KBs) are continuously im-\nproved and updated to help medical models better adapt to clinical needs and improve diagnosis\nand treatment.\nLarge language models (LLMs) and their powerful learning capabilities have recently at-\ntracted attention in medical AI [6, 7, 8, 9, 10, 11, 12]. By mining massive medical data, such\nas electronic medical records (EMRs), medical images, and genetic information, these models\naim to learn more accurate and fine-grained knowledge in medical data and provide more e ffi-\ncient diagnostics for doctors in real-life medical scenarios. Incorporating clinical knowledge into\nmedical AI can play crucial roles in real-world medical scenarios:\n• Improve models’ development and capabilities in the medical field. By analyzing a large\namount of clinical data, medical LLMs can continuously adjust and optimize model pa-\nrameters to improve the accuracy of prediction and diagnosis. At the same time, clinical\nknowledge can also guide researchers to pay attention to the differences and specificities of\nthe patient population during model training and improve the model’s generalization abil-\nity. This helps medical AI systems play a greater role in clinical applications and improve\nthe quality of healthcare services.\n• Improve the diagnostic e fficiency of real-life clinical practice. Medical LLMs can use\nclinical knowledge to quickly and accurately diagnose the patient’s condition and provide\ndoctors with treatment plans. In clinical practice, medical AI systems can recommend\npersonalized treatment plans based on the patient’s condition, present history, family his-\ntory, and other information to improve the treatment. In addition, medical LLMs can also\ndiscover potential disease risks and complications by mining and analyzing clinical data,\nproviding the basis for clinical prevention and management.\nSo far, there are numerous comprehensive reviews of medical AI. For instance, Thirunavukarasu\net al. [6] focuses on the e fficiency and effectiveness of LLM applications, such as the power of\nChatGPT [13] in clinical, educational, and research. Wornow et al. [7] investigates 84 base\nmodels in non-imaging EMR data and proposes an enhanced evaluation framework for these\nChen), wang_meng@whu.edu.cn (Meng Wang), feng.gao86@wust.edu.cn (Feng Gao),\nfrank.van.harmelen@vu.nl (Frank van Harmelen), simon@wust.edu.cn (Jinguang Gu)\n2\nmodels to align with essential metrics in healthcare closely. Safranek et al. [8] highlights the\ncritical limitations of LLM in medical education. Zhou et al. [9] summarises the approaches to\nconstructing medical LLMs, compares the performance of existing models, and provides relevant\ninsights into the development opportunities. He et al. [10] outline the development process of the\nLLM created by healthcare and provide an overview of the development roadmap from the tradi-\ntional pre-trained language model (PLM) to the LLM. Ali et al. [11] reported recent advances in\ndeep learning-based drug recommendation methods. Based on the above-detailed research, we\npresent a comprehensive review of data, methodology, evaluation, and application in the clinical\nliterature, as well as explore the differences between academic research and industry practices of\nbuilding medical AI systems.\nTherefore, this paper begins by discussing existing clinical databases and datasets. Based on\nthis, we elaborate on the construction mechanisms of medical academic LLMs and analyze the\nchallenges of current medical model research. In addition to analyzing medical LLMs trained on\nclinical databases and datasets, we have also presented the integration of medical LLMs and KGs,\nwhich contain a rich and strong semantic connectivity. Then, we present an overview of practical\napplications in real-world medical circumstances and identify the gap between academic model\nresearch and industry practice. Additionally, we summarize the principles and methods for eval-\nuating current medical LLMs and perform a systematic analysis. Finally, we propose potential\nfuture development directions for medical AI research, aiming to promote the deep integration\nof academic research and clinical practice and to provide scientific basis and practical guidance\nfor the further development of medical AI.\nThe overall structure of this survey is shown in Figure 1. In the following chapters, we\nsummarise existing clinical databases and datasets in Chapter 2. Subsequently, we analyze the\nconstruction mechanisms and performance of numerous academic medical LLMs in Chapter 3.\nThen, we list some of the medical LLMs that have attracted wide attention in practice and analyze\nthe possible limitations of the current academic medical LLMs in Chapter 4. Furthermore, we\nsystematically review the principles and methods for evaluating current medical LLMs and per-\nform a systematic analysis in Chapter 5. Finally, we recommend how future scientific research\non medical AI can better play a role in healthcare scenarios in Chapter 6.\n2. Clinical Databases and Datasets\nIn this chapter, we will first introduce open-source clinical databases. Then, we will intro-\nduce the clinical datasets by categorizing them into pre-training and fine-tuning stages from the\nperspective of constructing medical LLMs. This will establish a solid data foundation for the\nnext chapter, which focuses on building academic medical LLMs.\n2.1. Clinical Databases\nClinical databases are comprehensive and systematic summaries of clinical knowledge, which\ncontain a quantity of medical expertise and real-life clinical experience. Obtaining this knowl-\nedge can help LLMs identify deceases based on symptoms and assist in decision-making of com-\nplex conditions. LLMs utilizing clinical databases provide personalized treatment plans based on\npatient circumstances and make accurate treatment recommendations. More importantly, up-to-\ndate clinical databases allow LLMs to update medical results and improve prediction qualities.\nThe clinical databases serve as a bridge integrating clinical domain knowledge and lay a solid\nfoundation for LLMs to learn domain knowledge. Table 1 lists some commonly used clinical\ndatabases.\n3\nClinical Datasets\n◼ Pre-training Datasets\n◼ Fine-tuning Datasets\nClinical Databases and Datasets\nAcademic Medical LLMs\nAssessment Principles\nAssessment Methodology\nApplications in Medicine\nEvaluation System\nMechanisms for Building \nMedical LLMs\n◼ Building Medical LLMs from \nText Corpora\n◼ Building Medical LLMs from \nMultimodal Data\n◼ Biomedical Agent\nCollaborative Development \nof Medical LLMs and \nClinical KGs\n◼ LLMs for Medical KGs\n◼ KGs for Medical LLMs\n◼ Medical LLMs and KGs\nChapter 2\nChapter 3\nChapter 4\nChapter 5\nSection 5.1\nSection 5.2\nSection 3.1\nSection 3.2\nSection 2.2\nIndustrial Text Medical \nLLMs\nIndustrial Multimodal \nMedical LLMs\nAcademic vs Industrial\nSection 4.1\nSection 4.2\nSection 4.3\nClinical Databases\nSection 2.1\nChallenges Challenges and Future Work\nChapter 6Section 6.1\nFuture Work\nSection 6.2\nFigure 1: The overall structure of the paper. Chapter 2: clinical databases and datasets. Chapter 3: construction mecha-\nnisms of academic medical LLMs. Chapter 4: applications of medical LLMs. Chapter 5: evaluation of medical LLMs.\nChapter 6: challenges and future works.\nStructured Drug Information. Among them, drug information is one of the subjects that re-\nsearchers focus on building KGs, as it is usually represented by structured descriptions, which\nmake it easier to construct ontology and extract knowledge. This type of knowledge is often sig-\nnificant for clinical practice because of its informativeness. Common KBs for drug information\nare:\n1. Drugs1 provides a global online database of drug information, including descriptions,\nuses, dosages, side effects, and more. Users can access drug information for free, includ-\ning details on drug names, specifications, manufacturers, and more. Additionally, it offers\nexpert advice on drug interactions, side effects, and the use of drugs during pregnancy and\nbreastfeeding.\n2. DrugBank2 is an online database containing a vast amount of drug-related information. It\nis unique in that it provides information on the transformation process of drugs in the body,\nthe mechanism of drug-drug interaction, and more. This makes it an invaluable resource\nfor researchers and pharmaceutical professionals.\n3. National Health Service (NHS) Medicine3 provides an online drug information platform\nthat offers detailed information on prescription and over-the-counter drugs, drug-drug in-\nteraction, side e ffects, and more. The platform aims to help doctors, pharmacists, and\npatients understand and use medicines better and provide practical treatment advice.\n1https://www.drugs.com/\n2https://go.drugbank.com/\n3https://www.nhs.uk/medicines/\n4\nTable 1: The collection of common clinical databases. ”Text Corpora” indicates that the knowledge is stored as text, and\n”Multimodal” includes not only text but also other modalities such as images and videos.\nName Content Modality\nDrugs Drugs.com provides accurate and independent information on over\n24,000 prescription drugs, over-the-counter medicines, and prod-\nucts.\nText Corpora\nDrugBank The latest release of DrugBank Online contains 16,568 drug entries,\nincluding 2,761 approved small molecule drugs, 1,611 approved bi-\nologics, 135 nutraceuticals, and over 6,723 experimental drugs. Ad-\nditionally, 5,303 non-redundant protein sequences are linked to these\ndrug entries.\nText Corpora\nNHS Medicine NHS Medicine provides detailed information on 288 prescription\ndrugs, over-the-counter medicines, drug-drug interaction, and side\neffects.\nText Corpora\nMedline The flagship bibliographic resource of the National Library of\nMedicine cites over 31 million journal articles across the life sci-\nences, particularly on biomedical topics.\nText Corpora\nEmbase Embase has over 32 million entries and over 2,900 proprietary jour-\nnal indexes. Its unique Emtree thesaurus includes a MeSH thesaurus,\n56,000 specialized search terms, and 230,000 synonyms.\nText Corpora\nUMLS UMLS includes about 2 million medical concepts and a medical vo-\ncabulary of more than 5 million words.\nText Corpora\nHPO HPO currently contains over 13,000 terms and over 156,000 annota-\ntions to hereditary diseases.\nText Corpora\nUpToDate Clinical Advisor UpToDate Clinical Advisor includes 12,400 + clinical topics cover-\ning 25 specialties, 9,800+ graded recommendations, 37,000+ image\nprofiles, 220 + medical calculators, 7,600 + English-language drug\nmonographs, and 544,000+ Medline references.\nMultimodal\nClinicalKey Clinicalkey comprises over 676 literature resources, 1,005 classic\nbooks, 63,699 medical videos, 4.64 million images, 5,000 clinical\nguidelines, 210,000 diagnostic trials, 2,562 drug monographs, 99.01\nmillion patient educations, 50 North American clinics, 339 operating\nvideos, 555 clinical spotlights, and 22 million Medline abstracts.\nMultimodal\nNHS Health NHS Health provides an overview of 1,216 diseases, treatments, and\ninformation on healthy lifestyles.\nText Corpora\nThe combination of medical LLMs and databases mentioned above can obtain information\non drugs, dosages, contraindications, and side effects for treating corresponding diseases, which\ncan be implemented in medical encyclopedias, assisted diagnosis, and other related tasks.\nMedical Literature. As a high-quality medical data source, medical literature has become one\nof the most essential sources of knowledge extraction for researchers. It contains the latest find-\nings and protocols that can supplement or update diagnostic and treatment principles in medical\nmodels. At the same time, the results of some medical experiments can also provide evidence to\nsupport the model reasoning in diagnosis and treatment recommendations.\n1. Medline4 serves as the flagship bibliographic resource of the National Library of Medicine\n(NLM), featuring over 31 million citations for journal articles across the life sciences, with\na particular emphasis on biomedical topics.\n4https://www.nlm.nih.gov/medline/index.html\n5\n2. Embase5 is a scholarly repository created by Elsevier covering over 8,500 journals pub-\nlished in 95 countries and regions worldwide since 1974. The database contains over 32\nmillion entries and over 2,900 proprietary journal indexes. The database includes 56,000\nspecialized search terms and 230,000 synonyms, greatly enhancing the accuracy and depth\nof searches.\nBiomedical Information. KBs with knowledge of large quantities, small granularity, and high\nprecision can help the medical model improve its mastery of medical knowledge.\nFor example, the National Institutes of Health (NIH) has developed the Unified Medical\nLanguage System6 (UMLS) to unify various medical terminologies and provide standardized\nand precise information for medical research and applications. Its subsystems, includingMeSH7\nand RxNorm8, are widely used in medical literature retrieval, bioinformatics research, and EMR\nsystems.\nThe Human Phenotype Ontology9 (HPO) was developed to aid researchers and clinicians\nin studying genetic diseases. It represents the complex changes in the human body during dis-\nease. HPO organizes phenotypic features into hierarchical structures, including morphological,\nphysiological, and behavioral features. Using an object-oriented approach, researchers can com-\nmunicate and share phenotypic information about genetic diseases more e ffectively, supporting\ndisease diagnosis, treatment, and genetic counseling.\nTo assist models with evidence-based reasoning of medicine, researchers developed UpTo-\nDate Clinical Advisor 10. This is the Chinese version of UpToDate’s product. It continuously\ncombines existing medical evidence with the clinical experience of world experts. They present\nhigh-level, practical medical information that has undergone multiple rounds of filtering, digest-\ning, and assimilating before being presented to users. In particular, the medical model can give\ngraded diagnosis and treatment recommendations based on the GRADE principle of evidence-\nbased medicine based on the comprehensive integration of research evidence based on the prod-\nuct.\nCollective Database. Based on the above KBs, some comprehensive large-scale KBs were con-\nstructed to provide a complete view of medical knowledge and facilitate the model to learn the\ninteractions and correlations between various types of knowledge, better serving real medical\nscenarios.\nFor instance, ClinicalKey11, an online service platform developed by Elsevier, consolidates\nthe most current medical information resources worldwide. Similarly, NHS Health, an online\nhealth information platform created by the NHS, offers information on diseases, treatments, and\nhealthy lifestyles to assist the public in comprehending and maintaining their health. The in-\nformation provided by NHS Health is based on clinical evidence. It is regularly reviewed and\nupdated by a team of experts accredited by the UK Department of Health and Social Security.\nThe clinical KB is the cornerstone of medical LLMs, providing deep and reliable expertise\nand enabling continuous absorption and updating of medical knowledge. This dynamic learning\n5https://www.embase.com/\n6https://www.nlm.nih.gov/research/umls/index.html\n7https://www.nlm.nih.gov/mesh/\n8http://www.nlm.nih.gov/research/umls/rxnorm/\n9https://hpo.jax.org\n10https://www.uptodate.cn/home\n11https://www.clinicalkey.com/#!/\n6\nprocess dramatically facilitates the application of medical LLMs in clinically assisted decision-\nmaking and personalized treatment recommendations, which is pivotal in providing high-quality\nhealthcare services.\n2.2. Clinical Datasets\nBuilding on the limited data available in each clinical database, medical LLMs often require\na more comprehensive range of knowledge and information from rich and real medical data. Fol-\nlowing the classification scheme for general LLM datasets, the datasets employed in constructing\nmedical LLM are categorized into pre-training and fine-tuning datasets.\n2.2.1. Pre-training Datasets\nPre-training of general LLMs refers to the extensive training of deep learning models on tongs\nof textual corpora to enable them to understand the generalized natural language expressions.\nSuch pre-training gives the model a deep linguistic knowledge base, allowing it to perform better\nin subsequent tasks.\nSimilarly, medical LLMs require large amounts of corpora to learn vast medical knowledge\nand improve the domain’s specific understanding of the model. The corpora can be obtained from\nmedical-related literature, books, journals, websites, and other related resources. Since textual\ncontent often does not fully cover fine-grained medical knowledge such as lesion features and\norgan images. Some pre-training datasets also combine multimodal information, such as aligned\ntext-image data. Table 2 shows some commonly used pre-training datasets.\nTable 2: Common pre-training datasets for medical LLMs. These datasets often contain vast textual information (”Text\nCorpora”) or multimodal data, which is designed to provide a wide range of knowledge for medical LLMs during pre-\ntraining.\nModality Dataset Data Scale Typical Models\nText Corpora\nPubMed >36M literatures PubMedBERT [14]\nMedDialog [15] 704.73M tokens OphGLM [16]\nEHRs [17] >82B tokens GatorTron [17]\nChiMed-CPT [18] 3GB Qilin-Med [18]\nGAP-REPLAY [19] 48.1B tokens Meditron [19]\nThe Pile [20] >109.5 GiB BioMedLM [21]\nMultimodal\nMTB [22] 584M tokens Med-Flamingo [22]\nPMC-OA [23] 1.6M medical image-text PMC-CLIP [23]\nMIMIC-CXR [24] >377k medical image-text MedCLIP [25]\nMIMIC-III [26] >53.4k EHRs ClinicalBERT [27]\nMIMIC-IV [28]\nMIMIC-CXR-JPG [29]\nMIMIC-IV-Note [30]\n>40k EHRs\n>377k medical images\n>2.65M medical texts\nLLaMA-Care [31]\nText Corpora. These corpora are the primary source of information for LLMs, including medical\nand linguistic knowledge. Most medical pre-training datasets contain text from medical litera-\nture, authentic consultation dialogues, and clinical practice records. Techniques like anomaly\ncleaning, representation normalization, and privacy encryption often pre-process these data. We\nlist some typical text pre-training datasets as follows:\n• PubMed12: PubMed collects over 36 million citations and abstracts of biomedical litera-\nture from Medline. Despite medicine, it also encompasses data such as nursing and other\n12https://pubmed.ncbi.nlm.nih.gov/download/\n7\nhealth disciplines. While PubMed contains journal article citations, it does not provide\nfull-text access but includes a link to the full text.\n• MedDialog: MedDialog comprises two extensive medical conversation datasets, MedDialog-\nEN and MedDialog-CN. MedDialog-EN is an English dataset with 260k dialogues, 510k\ncorpus, and 44.53M tokens, covering approximately 96 specialty categories. MedDialog-\nCN is a Chinese dataset comprising 3.4 million dialogues, 11.3 million corpus, and 660.2M\ntokens, covering 29 major and 172 sub-specialty categories. Both datasets include real-\nlife communication scenarios between patients and healthcare professionals with different\ncharacteristics, providing the model with knowledge for generating more human-like re-\nsponses.\n• EHRs: The EHRs were retrieved from the UF Health Integrated Data Repository (IDR),\nwhich comprises 200 million clinical notes from 2011 to 2021 for over 2 million patients.\nThese records come from over 126 clinical departments and approximately 50 million vis-\nits, covering healthcare settings such as inpatient, outpatient, and emergency room visits.\nIn total, there are over 82 billion medical words. This dataset can assist models in un-\nderstanding the correlation between data gathered in actual healthcare situations and the\nultimate medical diagnosis.\n• ChiMed-CPT: ChiMed-CPT is a unique dataset comprising multiple sub-datasets of four\ntypes: QA, Text, KG, and Dialogue. These sub-datasets can help medical models en-\nhance multiple downstream tasks. The QA category includes Huatuo-26M-encyclopedias,\nHuatuo-26M-medical knowledge, and CMExam. The text category includes MedQA text-\nbooks. The KG category includes CPubMed-KG, Xywy-KG, and 39Health-KG. The di-\nalogue category includes Chinese-medical-dialogue-data, Medical-Dialogue-System, and\nCMD.\n• GAP-REPLAY: The GAP-REPLAY dataset incorporates 48.1B tokens from four datasets:\nClinical Guidelines, which includes 46K clinical practice guidelines from various healthcare-\nrelated sources, Paper Abstracts, comprising openly accessible abstracts from 16.1M closed-\naccess PubMed and PubMed Central papers, Medical Papers, consisting of full-text articles\nfrom 5M publicly available PubMed and PubMed Central papers, and a Replay dataset in-\ncluding general domain data distilled to compose 1% of the entire corpus.\nIn addition to the above datasets constructed specifically for medical scenarios, several rel-\nevant medical-themed data exist in general domain pre-trained datasets that can also be used to\nbuild medical LLMs.\nOne of the commonly used corpora for building general domain LLMs isThe Pile. It is a pre-\ntraining dataset for general domain LLMs. It contains 825.18 GiB of English text, divided into 22\nsub-datasets covering academic (such as PubMed Central, and ArXiv), internet (such as Pile-CC\nand OpenWebText2), prose (such as Books3 and BookCorpus2), dialogue (such as OpenSubtitles\nand YouTube Subtitles), and other types (such as GitHub, DeepMind Mathematics, and Enron\nEmails).\nNotably, the PubMed Central and PubMed Abstracts datasets contain medical-related\nknowledge, which accounts for approximately 17.47% or about 109.53 GiB. Models such as\nBioMedLM-2.7B are constructed using purely medical-themed datasets and achieve state-of-\nthe-art (SOTA) results on the medical question-answering task.\n8\nMultimodal Data. Multimodal data can assist models in acquiring detailed knowledge of med-\nical images and modeling the text-image relationships through pre-training. The data is often\nobtained from o fficial sources such as textbooks and literature and real data sources such as\nradiology departments. To create these multimodal medical pre-training datasets, we must pre-\nprocess the images by denoising, resampling, enhancing, and normalizing them. Additionally,\nwe should follow the textual data pre-processing steps mentioned earlier. Below are some brief\ndescriptions of a few multimodal pre-training datasets.\n• MTB: A multimodal dataset Medical TextBooks (MTB) constructed from 4,721 textbooks\nfrom different medical specialties. During pre-processing, each book with cleaned text and\nimages is segmented into several segments for pre-training so that each segment contains\nat least one image, thus matching the images with the associated text and unifying the\nsemantic vectors of the text and images. The final MTB contains about 800,000 images\nand 584M tokens.\n• PMC-OA: The PubMed Central Open Access (PMC-OA) dataset is derived from medi-\ncal literature and contains 1.6M image-text pairs. This dataset is eight times larger than\nsimilar datasets and covers various diseases, organs, and imaging modalities. PMC-OA\noffers advantages in sample richness, number of modalities, disease types, and patient bal-\nance, regardless of diagnostic means, diseases, or patients’ age and gender. The dataset\nprovides detailed organ imaging features and reduces the problem of out-of-distribution\ngeneralisability for the model due to excessive data for some diseases.\n• MIMIC-CXR: Medical Information Mart for Intensive Care Chest X-ray (MIMIC-CXR)\nis the largest chest X-ray dataset for modeling, providing comprehensive and accurate\nmedical descriptions of chest X-ray features and corresponding symptoms, including chest\nX-rays in dicom format and free text radiological reports. The dataset comprises 377,110\nimage-text pairs, with each image being accompanied by a clinical report that describes\nthe physician’s findings.\n• MIMIC-III: Medical Information Mart for Intensive Care (MIMIC-III) is an extensive,\nsingle-center database comprising information about patients admitted to critical care units\nat a large tertiary care hospital. Data includes vital signs, medications, laboratory mea-\nsurements, observations and notes charted by care providers, fluid balance, procedure\ncodes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more.\nMIMIC-III includes data from more than 50,000 hospital admissions for adult patients (16\nyears old or above) admitted to critical care units between 2001 and 2012. Additionally, it\nincludes data for over 7,870 neonates admitted between 2001 and 2008. The dataset can\nsupport broad research efforts, including epidemiology, clinical decision rule optimization,\nand medical e-tool development.\n• MIMIC-IV: MIMIC-IV is an updated version of MIMIC-III, featuring current data and\nenhancements in various aspects. It comprises de-identified health data from more than\n40,000 patients who are admitted to intensive care units at Beth Israel Deaconess Medical\nCenter. The data includes detailed patient demographics, hospitalizations, lab measure-\nments, and medication prescriptions.\n• MIMIC-CXR-JPG: MIMIC-CXR-JPG consists of 377,110 chest radiographs in JPG for-\nmat derived from the MIMIC-CXR dataset. This dataset aims to facilitate medical image\n9\nunderstanding and analysis research by providing a more accessible JPG format of the\noriginal DICOM images. This dataset contains metadata and structured labels, all fully\nde-identified.\n• MIMIC-IV-Note: MIMIC-IV-Note is a collection of de-identified free-text clinical notes\nfor patients in the MIMIC-IV clinical database. It contains 331,794 de-identified discharge\nsummaries from 145,915 patients admitted to the hospital and emergency department at the\nBeth Israel Deaconess Medical Center in Boston, MA, USA. Additionally, it also contains\n2,321,355 de-identified radiology reports for 237,427 patients.\n2.2.2. Fine-tuning Datasets\nUnlike the pre-training phase, which enables the model to learn a generic feature representa-\ntion, the fine-tuning phase adapts the model with a suitable fine-tuned dataset to enhance its per-\nformance on a specific task. The fine-tuning datasets usually consist of well-labeled task-specific\ndata, which are then adapted to the particular task by updating some of the parameters of the pre-\ntrained model to improve the prediction accuracy of the model in the particular task. Table 3\ndemonstrates the fine-tuning datasets used in the medical for nine tasks: medical examination\n(ME), medical conversation (MC), medical question answering (MQA), medical visual ques-\ntion answering (MVQA), medical visual question generation (MVQG), image-to-text retrieval\n& text-to-image retrieval (I2T & T2I), report summarization (RS), report generation (RG), and\nmedical image classification (MIC).\nTable 3: Common fine-tuning datasets for medical LLMs. These datasets are often used to improve or evaluate the model\nperformance on specific tasks and they often support multi-task analysis.\nModality Dataset Task\nME MC MQA MVQA MVQG I2T & T2I RS RG MIC\nText Corpora\nCMExam [32] !\nMedQA [33] !\nPubMedQA [34] !\ncMedQA2 [35] !\nMedQuAD [36] !\nCMD. ! !\nMedDialog-CN [15] !\nMultiMedQA [37] ! !\nMultimodal\nPathVQA [38] !\nVQA-RAD [39] !\nVQA-med-2018 [40] !\nVQA-med-2019 [41] !\nVQA-med-2020 [42] ! !\nVQA-med-2021 [43] ! !\nSLAKE [44] !\nPMC-15M [45] !\nChiMed-VL [46] !\nMultiMedBench [47] ! ! ! ! !\nWe categorize these fine-tuned datasets into five groups for various medical tasks: medical\nexams, medical question answering, medical conversations, medical visual question answering,\nand image-text retrieval.\nMedical Exams. Medical exams are crucial for verifying the professional competence of medi-\ncal students. They rely on real-world data, cover a broad range of knowledge, use standardized\n10\nassessments (including standard answers and scoring criteria), provide constantly updated infor-\nmation resources, and involve multidisciplinary cross-fertilization, which has proven e ffective\nfor evaluating medical LLMs. For example:\n• CMExam: The Chinese Medical Exam (CMExam) is a Chinese National Medical Licens-\ning Examination dataset. The dataset contains over 60,000 multiple-choice questions and\nfive additional annotations, including disease groups, clinical departments, medical disci-\nplines, competency areas, and question difficulty levels. It is also the first Chinese medical\nexam dataset to provide comprehensive annotation by medical personnel, primarily for\nstandardized and objective assessment and answer explanations for open model reasoning\nassessment.\n• MedQA: The MedQA dataset was collected from professional medical board exams, in-\ncluding the United States Medical Licensing Examination (USMLE), the Mainland China\nMedical Licensing Examination (MCMLE), and the Taiwan Medical Licensing Examina-\ntion (TWMLE). It consists of 61,097 multiple-choice questions covering three languages,\nwith 12,723 in English, 34,251 in traditional Chinese, and 14,123 in simplified Chinese.\nMedical Question Answering.Medical question answering, as one of the most common health-\ncare practices, often involves patients obtaining medical encyclopedia knowledge, disease differ-\nentiation, and medication recommendations. Researchers have designed the following fine-tuned\ndatasets to enhance the model’s performance in these scenarios.\n• PubMedQA: PubMedQA collects abstracts from PubMed literature. Models are required\nto answer questions based on the article abstracts. The dataset is divided into three subsets:\n1k expert labeled, 61.2k unlabeled, and 211.3k artificially generated QA instances. This\ndataset aims to enhance the model’s understanding of medical literature and improve the\nidentification of key scientific issues through simple classification problems.\n• cMedQA2: The cMedQA2 dataset is designed for routine medical scenarios and contains\nreal drug-related questions from a Chinese medical search website13. It comprises 108,000\nquestions and 203,569 answers, with qualified doctors providing the real answers to rel-\nevant questions after privacy treatment. Researchers can use this dataset to evaluate their\nmodels’ medication knowledge and QA ability.\n• MedQuAD: For more complex and specialized medical scenarios, the Medical Question\nAnswering Dataset (MedQuAD) collates more than 40,000 medical QA pairs from the\nNIH’s 12 websites (such as National Cancer Institute (NCI), Genetics Home Reference\n(GHR), Genetic and Rare Diseases Information Center (GARD), and MedlinePlus Health\nTopics). These data cover 37 question types (such as treatment, diagnosis, and side effects)\nrelated to diseases, medications, and other medical entities (such as examination).\nMedical Conversations. Medical conversations are the primary means of daily treatment and\ntypically involve multiple QA pairs with larger contexts. Doctors use them to gather informa-\ntion about patients’ conditions and provide medical advice. Researchers have constructed the\nfollowing medical dialogue dataset to enhance the ability of intelligent treatment and assisted\ndecision-making.\n13http://www.xywy.com/\n11\n• CMD.14: The Chinese medical dialogue dataset comprises 792,099 data pieces that cover\nsix types of medical dialogues: Andriatria (90,000 QA pairs), Internal Medicine (220,000\nQA pairs), Obstetrics and Gynecology Department (180,000 QA pairs), Oncology (70,000\nQA pairs), Pediatric (100,000 QA pairs), and Surgical (110,000 QA pairs). This dataset\ncan be used to study the model’s expertise and KB of the questioning process across a wide\nrange of departments.\n• MedDialog-CN: To address the issue of varying linguistic expressions and causes of mor-\nbidity among individuals, MedDialog-CN is a database that collects patient data from di-\nverse demographics, including age, gender, occupation, education, and income, across 31\nprovincial-level administrative regions in China. This database is used to construct Chi-\nnese medical dialogue datasets containing 1.1 million dialogues and 4 million utterances,\ncovering 29 major specialties and 172 sub-specialties.\nMedical Visual Question Answering.Medical images often contain numerous patient situations\nand fine-grained disease features. They are a crucial component of medical diagnosis. Therefore,\nresearchers have developed multiple multimodal medical QA datasets to enhance the extraction\nand alignment of multimodal information for the model.\n• PathVQA: PathVQA is a pathology VQA dataset that generates QA pairs from captions\nextracted from pathology textbooks and online digital libraries using NLP. The dataset con-\ntains 4,998 pathology images and 32,799 QA pairs, with each question manually checked\nfor correctness. This dataset provides the model with rich disease image-text pairing data,\nimproving the model’s modal alignment.\n• VQA-RAD: VQA-RAD is a unique dataset where clinicians ask natural questions about\nradiological images and provide reference answers. The dataset includes a human trans-\nverse image set containing samples of the head, chest, and abdomen from MedPix 15. The\nfinal VQA-RAD dataset comprises 315 images and 3,515 question pairs.\n• VQA-med series : ImageCLEF released datasets for real medical scenarios during the\n2018, 2019, 2020, and 2021 Challenges: VQA-med-2018, VQA-med-2019, VQAmed-\n2020, and VQA-med-2021. The datasets have been updated over time, with increasing\nvolumes of data and more refined medical theories. For instance, VQA-med-2019 is based\non VQA-RAD and focuses on four prevalent problem categories: modality, plane, organ\nsystem, and abnormality. The first three categories can be addressed as a classification\ntask, while the fourth category requires generating answers. Moreover, the dataset’s task\ncomplexity has increased with the inclusion of the VQG task in VQA-med-2020. The\nVQA-Med-2021 dataset has been enhanced with a validation set and a test set, following\nthe principles of VQA-Med-2020. A medical professional manually reviewed these sets,\nresulting in improved accuracy and task complexity.\n• SLAKE: SLKAE is a bilingual dataset with semantic labels annotated by experienced\nphysicians and new structured medical knowledge. It covers more body parts and richer\nmodalities than other datasets. The dataset consists of 642 images and 14,028 questions\nrelated to 12 diseases and 39 organs and contains 2,603 English triples and 2,629 Chinese\ntriples.\n14https://github.com/Toyhom/Chinese-medical-dialogue-data\n15https://medpix.nlm.nih.gov/\n12\nImage-text Retrieval. Image-text retrieval in traditional multimodal medical modeling involves\ndetecting a model’s multimodal alignment. This requires the model to retrieve the most matching\ntext or image data from a large dataset based on a particular image or text. To achieve this, the\nresearchers developed the PMC-15M dataset to fine-tune the model.\nThe PMC-15M dataset was created using PubMed Central. This involved downloading\nand extracting a compressed catalog containing full article packages. Each package contains\nXML, PDF, media, and supplementary material. The dataset also includes image files with cor-\nresponding captions and the PMIDs and PMCIDs of the original articles. The PMC-15M dataset\ncomprises 15 million image-caption pairs for over 3 million articles.\nFurthermore, various extensive datasets are frequently utilized in developing medical models.\nThese datasets are crucial for enhancing the model’s generalizability and performance across\nmultiple tasks.\n• MultiMedQA: MultiMedQA is a textual dataset comprising seven medical QA datasets,\nincluding six existing datasets and one newly introduced dataset: MedQA, MedMCQA\n[48], PubMedQA, LiveQA [49], MedicationQA [50], MMLU [51], and HealthSearchQA.\nHealthSearchQA contains 3,375 commonly searched medical questions by users. This\ndataset was used to evaluate the diversity of LLM’s clinical knowledge and QA abilities.\nThe emphasis was on question complexity and response length during data construction,\nresulting in approximately 213k data points.\n• ChiMed-VL: ChiMed-VL was created by translating several open-source English med-\nical multimodal datasets into Chinese using GPT-3.5 with expert quality control. The\ndataset comprises two parts: ChiMed-VL-Alignment and ChiMed-VL-Instruction. The\ndata for ChiMed-VL-Alignment is derived from PMC-OA and PMC-CaseReport and con-\ntains 580,000 image-text pairs. Each pair includes contextual information or a descrip-\ntion of the image, providing detailed alignment data for the model. The ChiMed-VL-\nInstruction dataset is sourced from PMC-Report and PMC-VQA and comprises 460,000\nQA pairs. These datasets encompass various diagnostic modalities, including X-rays,\nMRIs, CTs, radioisotopes, mitosis, and other biomedical knowledge.\n• MultiMedBench: MultiMedBench is a benchmark created to develop and evaluate general\nbiomedical AI. The benchmark is multimodal and multi-task, comprising 12 de-identified\nopen-source datasets for 5 task types, including QA, RS, VQA, RG, and MIC, with 14\nindividual tasks. It assesses the capacity of medical LLMs to carry out a range of clin-\nically relevant tasks. The benchmark includes over 1 million samples covering various\ntopics, including medical issues, radiology reports, pathology, dermatology, chest X-rays,\nmammography, and genomics.\n2.2.3. Findings\nWe have summarized and analyzed the above clinical datasets, including both the pre-training\nand fine-tuning datasets, in terms of data type, authenticity, diversity, coverage, update frequency,\nusability, reliability, and language, as shown in Table 4. Additionally, we have presented exam-\nples of these datasets in github16 to provide further insight into their format and applications.\nAs can be seen in Table 4, the available datasets exhibit the following characteristics:\n16https://github.com/vicky-yuan/survey-datasets\n13\n• Data Sources and Types. Most datasets are based on real data and primarily contain\ntextual information, while image data is mainly concentrated in radiology.\n• Coverage. The coverage of the datasets can be divided into two categories: one encom-\npasses a broad range of medical foundational knowledge, while the other focuses on spe-\ncific specialty knowledge.\n• Time Span and Update Frequency. The datasets vary widely in their timeframe and are\ngenerally updated less frequently, except for PubMed, which is kept steadily updated, and\nother datasets with irregular update cycles.\n• Accessibility. Most datasets are open-source and available, while a few require permission\nto access.\n• Data Reliability. Most datasets are considered reliable, as they are based on real data.\nHowever, the reliability of a few datasets is partially reliable due to some data being gen-\nerated by models like GPT-3.5/GPT-4.\n• Language Distribution. The datasets are predominantly in English, followed by bilingual\n(English and Chinese), while datasets in other languages are relatively scarce.\n2.3. Summary\nBesides utilizing the above training data, the instruction fine-tuning phase also involves uti-\nlizing instructional datasets to enhance the model’s generalization among tasks. The general\napproach is to automatically generate instruction data using pre-trained models such as Self-\nInstruct framework [52], BELLE [53], or GPT [54]. These models are used to model the re-\nsponses of real users for various tasks, resulting in instruction datasets that can understand and\nexecute human instructions. For instance, ChatDoctor [55] has created prompt templates us-\ning the fine-tuned LLaMA-7B [56], which incorporates the alpaca instruction dataset and the\nHealthCareMagic100k doctor-patient dialogue dataset. These templates are designed to retrieve\nexternal knowledge databases during doctor-patient dialogues, resulting in more accurate model\noutputs. Huatuo [57] utilized the LLaMA-7B to generate a dataset for Chinese medical in-\nstructions. The model was fine-tuned with the Medical KG and GPT3.5 API and subsequently\nenhanced to improve the QA effectiveness of LLaMA in the medical field.\nHowever, current medical KBs and training datasets do not yet have su fficient multilingual\nsupport. For example, most existing resources are only available in English or Chinese, with few\naligned datasets. This limitation can hinder the applications of medical AI systems for non-native\ndoctors and patients. Furthermore, the integrity of disease knowledge in these KBs and datasets\nmay not be sufficient enough. For instance, information on certain rare or endemic diseases, such\nas Gaucher disease, Pompe disease, and Fabry disease, is insufficient for model learning, and up-\ndating the latest research results and treatments is also costly. In real healthcare scenarios, these\nshortcomings may impact diagnostic accuracy, treatment e ffectiveness, and healthcare quality.\nWe will discuss these limitations in Chapter 6.\n3. Academic Medical LLMs\nLLM refers to deep neural networks based on the Transformer architecture [58], designed\nto process and generate natural language text. Unlike traditional language models, LLMs are\n14\nTable 4: A summary of the above clinical datasets, which include both pre-training datasets (text pre-training datasets and multimodal pre-training datasets) and fine-tuning datasets (text fine-tuning datasets and\nmultimodal fine-tuning datasets), in terms of data type, authenticity, diversity, coverage, update frequency, usability, reliability, and language. “Y” means “yes”, “N” means “no”, “M” means “mixed datasets (i.e., both\nreal and generated data)”, “CA” means “conditional access”, and “P” means “partially reliable”.\nDataType Authenticity Diversity Medical Coverage Update Frequency Usability Reliability LanguageText Figure Radiology Pathology Time series Numerical Value Fundamentals Specialty Knowledge Time Span\nPubMed Medical Literature Y Y - - - - Y Y - - Once a year,\naround December Y Y English\nMedDialog Doctor-Patient Dialogue Y Y - - - - - - 96 /172 diseases 2008-2020/\n2010-2020 - Y Y English /Chinese\nEHRs Patient Clinical Records Y Y - - - - - - 126 clinical departments 2011-2021 - N Y English\nChiMed-CPT Combined Dataset Y Y - - - - - Y - - - Y Y English /Chinese\nGAP-REPLAY Combined Dataset Y Y - - - - Y Y - - - Y Y English\nThe Pile Combined Dataset Y Y - - - - Y Y - - - Y Y English\nMTB Medical Textbooks Y Y Y - - - - Y - - - N Y English\nPMC-OA Literature Image-Text Y Y Y - - - - Y - - - Y Y English\nMIMIC-CXR Chest X-ray Image-Text Y Y - Y - - - - 14 types of chest findings 2011-2016 - CA Y English\nMIMIC-III Patient Health Data Y Y - Y - Y Y Y - 2001-2012 - CA Y English\nMIMIC-IV Patient Health Data Y Y - Y - Y Y Y - 2008-2019 - CA Y English\nMIMIC-CXR-JPG Chest X-ray Images Y - - Y - - - - 14 types of chest findings 2011-2016 - CA Y English\nMIMIC-IV-Note Patient Clinical Records Y Y - Y - Y Y Y - Within one year\nafter the visit - CA Y English\nCMExam Medical Licensing Exam QA Y Y - - - - - - 26 diseases,\n35 clinical departments - - Y Y Chinese\nMedQA Medical Board Exam QA Y Y - - - - - Y - - - Y Y\nSimplified Chinese/\nTraditional Chinese/\nEnglish\nPubMedQA Medical Literature QA M Y - - - - Y Y - - - Y P English\ncMedQA2 Chinese Community Medicine QA Y Y - - - - - Y - - - Y Y Chinese\nMedQuAD Medical QA Pairs Y Y - - - - - Y - - - Y Y English\nCMD. Medical Dialogue Y Y - - - - - - 6 types of clinical departments - - Y Y Chinese\nMedDialog-CN Doctor-Patient Dialogue Y Y - - - - - - 29 broad categories of specialties\n& 172 fine-grained specialties 2010-2020 - Y Y Chinese\nMultiMedQA Combined Dataset Y Y - - - - - Y - - - Y Y English\nPathVQA Pathology Image QA Y Y - - Y - - - Pathology - - Y Y English\nVQA-RAD Radiology QA Y Y - Y - - - - Radiology - - Y Y English\nVQA-med-2018 Medical Image QA Y Y - Y - - - - Radiology - - Y Y English\nVQA-med-2019 Medical Image QA Y Y - Y - - - - Radiology - - Y Y English\nVQA-med-2020 Medical Image QA Y Y - Y - - - - Radiology - - Y Y English\nVQA-med-2021 Medical Image QA Y Y - Y - - - - Radiology - - Y Y English\nSLAKE Medical Image QA Y Y - Y - - - - 12 diseases, 39 organs - - Y Y English /Chinese\nPMC-15M Biomedical Image-Text Pairs Y Y Y - - - - Y - - - N Y English\nChiMed-VL Combined Dataset Y Y Y Y - - - Y - - - Y P English /Chinese\nMultiMedBench Combined Dataset Y Y - Y Y - - Y Pathology, radiology,\ndermatology, genomics - - CA Y English\n15\ntrained using large-scale textual data with parameter sizes up to billions. For instance, models\nlike GPT-3 [59], GLM [60], LLaMA, and BLOOM [61] are trained on large corpora such as\nWikipedia17 and BookCorpus [62]. Moreover, recent studies [13] have demonstrated that these\nmodels can exhibit exceptional generalizability across multiple domains through pre-training and\nfine-tuning.\nSimilarly, LLMs also have attracted extensive research [10] [17] in the healthcare field, and\nthese models can process rich medical literature, clinical records, and other health-related data,\nproviding powerful support for medical research, diagnosis, and treatment. Medical LLMs can\nhelp medical professionals understand patient conditions and make decisions after integrating\nclinical knowledge. Below, we will introduce and analyze the mechanisms of building medical\nLLM from two perspectives: text corpora and multimodal data.\n3.1. Mechanisms for Building Medical LLMs\nIn the previous section, we examined open-source medical pre-training datasets and fine-\ntuning datasets to enhance the clinical knowledge of LLM. Based on our analysis of existing\nstudies, to achieve optimal results, it is necessary to select a suitable LLM backbone in the\ngeneral domain or a model with excellent performance in a specific domain at the beginning.\nThen, the backbone model should be trained for specific medical goals. For example, enhancing\nthe clinical knowledge of the model requires mixing clinical data corpora with the pre-training\ndataset that trains the model backbone for the supplement, which can significantly improve the\nmodel’s understanding of clinical applications with minimal computation cost.\nTo better illustrate the building mechanisms for LLM from clinical knowledge, we classify\nthis process into two categories based on the representation of clinical knowledge: text corpora\nand multimodal. These two building mechanisms also require customized training objectives for\ndifferent types of clinical data and application scenarios, enabling the model to perform excel-\nlently in specific clinical tasks.\n3.1.1. Building Medical LLMs from Text Corpora\nText corpora play a crucial role in medical research and building domain LLMs. This data\nprovides an in-depth analysis and summary of multiple findings and knowledge in medical re-\nsearch. In the following section, we first present the training objectives during pre-training with\nstandard practices. We then present the fine-tuning strategies for various downstream medical\ntasks. Finally, we introduce several comprehensive works that combine pre-training and fine-\ntuning into a multi-stage training pipeline to obtain more capable medical LLMs.\nPre-training. BioMedLM [21] and GatorTronGPT [63] are the main studies using these pre-\ntraining methods to build medical LLMs with strong multi-task generalization capabilities.\nBioMedLM, previously named PubMedGPT, is a biomedical language model developed\njointly by the Stanford Center for Basic Modeling Research and MosaicML. It is based on\nGPT-2 [64] and pre-trained using the PubMed Abstracts and PubMed Central portions of the\nPile dataset, which contains approximately 50 billion tokens covering 16 million abstracts and 5\nmillion full-text articles from the biomedical literature. The final model achieves an accuracy of\n50.3% on the MedQA-USMLE, 74.4% on the PubMedQA, and 95.7% on the BioASQ. These re-\nsults demonstrate the capability of LLM, especially in the biomedical domain, and their language\ngeneration capabilities in real-world applications.\n17https://huggingface.co/datasets/wikipedia\n16\nGatorTronGPT was trained as a generative clinical LLM on 277 billion words of textual data.\nThis included 82 billion words of clinical texts from 126 clinical departments and about 2 mil-\nlion patients at the medical center and 195 billion words of various general English texts. Their\nevaluation of the training results based on the GPT-3 in biomedical NLP and medical text gen-\neration showed that GatorTronGPT was comparable to humans regarding linguistic readability\nand clinical relevance. Physicians were unable to distinguish between them.\nFine-tuning. In addition to pre-training, fine-tuning methods are commonly used to construct\nmedical LLMs for specific downstream tasks. Table 5 below, these tasks include information\nextraction (IE), MQA, multi-turn dialogue (MD), ME, and text-to-text (T2T).\nTable 5: Training tasks for various medical tasks. Each of the tasks uses di fferent training objectives and serves various\npurposes in real-life applications.\nTask Training Objective Medical Application Scenarios\nIE Sequence Labelling\nGenerative Extraction Case Structurization\nMQA Text Generation Knowledge QA, Assisted Diagnostic Suggestions,\nDrug Indication Evaluation, Disease Evaluation, Report Interpretation\nMD Long Context Text Generation Simulated Diagnosis and Treatment, Guidance\nME Text Classification Learning Assistance\nT2T Text Generation Synthesizing Clinical Text Generation\nInformation Extraction (IE) automatically extracts structured information from unstruc-\ntured or semi-structured documents and other electronically represented sources. Medical IE\ntasks can enhance the quality of models for applications such as case description structurization\nand medical KB construction.\nThe conventional approach to IE involves training the model using sequential labeling. The\nresearcher uses BILOU to tag important information in a given text and then models the tag-\nging patterns to extract the tagged text as the final result. BiLSTM +CRF [65] techniques are\nfrequently employed for this type of IE.\nFollowing the emergence of generative modeling, researchers began using generative ex-\ntraction methods to gain more direct and e fficient access to extracted information. Generative\nextraction takes the input text and serializes the structured result of the extraction as the final\noutput. SOTA generative extraction approaches [66] typically use a method similar to the UIE\n[67] architecture.\nMedical Question & Answering (MQA) involves generating a response to a question based\non the model’s given context or knowledge stock. This task can enhance the performance of\nmodels in scenarios such as medical encyclopedias, assisted diagnosis, drug indications, disease\nassessment, and report interpretation.\nQA tasks with context can also be used to train models using the sequence annotation method\ndescribed above. However, researchers often prefer to use generative methods (such as Sun-\nsimiao18, and QiZhenGPT19) in conjunction with context-free QA tasks to improve training effi-\nciency and quality.\nMulti-turn Dialogue (MD) is an extension of the medical QA task, where multi-turn di-\nalogues consist of multiple coherent QA pairs. The historical information of the dialogues is\n18https://github.com/X-D-Lab/Sunsimiao\n19https://github.com/CMKRG/QiZhenGPT\n17\naggregated into contextual information and sent to the model as a new input for processing. This\ntask requires a longer context length limit for the model, making it more challenging than the\nmedical QA task. However, this task can greatly enhance the model’s performance in complex\nmedical scenarios, such as simulated consultation sessions, which is of greater practical value.\nMedical Examination (ME) requires providing medical exam content as multiple choice\nquestions to a model for analysis and selecting the best answer from the options. Traditional\nmodels typically split the question stem and option content into separate inputs for a sequential\nmulti-classification task to determine the most suitable answer. The task involves testing the\nmodel’s understanding of specific medical knowledge.\nText-to-Text (T2T) is the combination of text generation tasks other than tasks in the QA\nform. It requires the model to process text based on the given instruction. For example, for\na given medical report and summarization instruction, the model should generate an abstract\nof the report. The instructions for textual tasks are usually expressed in declarative sentences.\nMany textual tasks do not require specifying a task instruction. Instead, the model can learn the\nrequirements of a single instruction through training. These tasks are similar to the instruction\nfine-tuning tasks of the LLMs. Therefore, they are less computationally expensive to learn.\nMost medical LLMs obtained through fine-tuning are designed for MQA and MD tasks,\nwhich are closely linked to real medical scenarios and, therefore, receive more attention. Table\n6 summarizes the construction of medical LLMs for specific downstream tasks using fine-tuning\nmethods, which will be briefly presented as follows:\nTable 6: Medical LLMs constructed using fine-tuning methods. These models often use an LLM backbone from the\ngeneral domain as the foundation and are fine-tuned for different downstream medical tasks.\nModel Anchor Model Data Source Task\nSunsimiao\nBaichuan-7B\nChatGLM-6B\nInternLM-Chat-7B\n- QA\nQiZhenGPT\nChinese-LLaMA-7B\nCaMA-13B\nChatGLM-6B\nQizhen Medical KB QA\nPMC-LLaMA [68] LLaMA-7B Books+Literature\nMedC-I [68] QA\nChatMed-Consult LLaMA-7B ChatMed ConsultDataset QA\nBenTsao LLaMA-7B CMeKG-8K QA\nMed-PaLM PaLM [69] MultiMedQA QA\nMed-PaLM 2 [70] PaLM2 [71] MultiMedQA QA\nClinicalGPT [72] Bloom-7B\ncMedQA2\ncMedQA-KG\nMD-HER\nMEDQA-MCMLE\nMedDialog\nMD, QA\nMedicalGPT\nBloom\nLLaMA\nChatGLM2-6B\nBaichuan-7B/13B\nChinese medical datasets\nHuatuoGPT-Hybrid SFTMD\nHuatuoGPT [73] Baichuan-7B\nLLaMA-13B Hybrid SFT [73] MD\nBianQue [74] ChatGLM-6B BianQueCorpus [74] MD\nDoctorGLM [75] ChatGLM-6B CMD. MD\nChatDoctor [55] LLaMA HealthCareMagic\niCliniq MD\nPULSE OpenChina LLaMA 13B - QA, MT, T2T\n18\nSunsimiao: Yan et al. developed the Sunsimiao Chinese Medical LLM to synthesize folk\nmedical experiences and accumulate Chinese medical data. The model aims to provide a safe, re-\nliable, inclusive Chinese medical LLM. It was mainly fine-tuned using Baichuan-7B20, ChatGLM-\n6B21, and InternLM-Chat-7B22 on 100,000 high-quality Chinese medical data. However, the data\nis not open source.\nQiZhenGPT: QiZhenGPT is an LLM constructed by Zhejiang University based on Chinese-\nLLaMA-Plus-7B23, CaMA-13B24, and ChatGLM-6B models. It was fine-tuned using instruc-\ntions from the Chinese medical instruction dataset constructed by the QiZhen Medical KB 25.\nUnlike many open-source ChatLLM projects that use instruction data generated by other mod-\nels, such as ChatGPT, this model utilizes real doctor-patient knowledge QA datasets from the\nQizhen Medical KB to enhance its accuracy in Chinese medical scenarios.\nPMC-LLaMA: PMC-LLaMA is a medical LLM released by Shanghai Jiao Tong University.\nIt is based on the LLaMA-7B model and has been fine-tuned using 4.8 million biomedical aca-\ndemic papers. The model has been evaluated on three bioQA datasets: PubMedQA, MedMCQA,\nand USMLE. The results indicate a greater comprehension of biomedical domain-specific con-\ncepts, leading to a high level of performance in the QA benchmark. However, due to equipment\nperformance, the limited training data and epochs for PMC-LLaMA also highlight the challenge\nof high costs and hardware requirements for fine-tuning LLMs.\nChatMed: East China Normal University has launched the ChatMed series of LLMs for\nChinese medical to promote the development and landing of LLMs in the Chinese medical\nfield. The ChatMed-Consult 26 is based on the ChatMed Consult dataset27, which contains over\n500,000 online consultations and ChatGPT responses, to improve LLM’s medical knowledge\nand ability to answer medical consultations. The model backbone used is LLaMA-7b, which\ncombines the LoRA weights of Chinese-LLaMA-Alpaca with the Chinese extended word list.\nEfficient fine-tuning of parameters based on LoRA is then performed. Finally, after comparing\nwith the Chinese-LLaMA-7B model on the e ffect of the questioning dialogue, it is evident that\nthe ChatMed-Consult model is more e ffective in comprehending the user’s query despite inter-\nference. Additionally, the responses are more humane and cautious, providing more practical\nsuggestions.\nBenTsao: BenTsao is a medical LLM developed by the SCIR Lab of Harbin Institute of\nTechnology, based on the LLaMA-7B model. The model has been fine-tuned with Chinese\nmedical instructions. They constructed a Chinese medical instruction dataset through medical\nKG and GPT3.5 API and further fine-tuned the model on this basis to improve the QA e ffect of\nLLaMA in the medical field.\nMed-PaLM series: Google created Med-PaLM, a version of PaLM for fine-tuning in the\nmedical field. It was the first model to achieve a passing score (> 60%) on US medical licensing-\nstyle questions. This study presented an evaluation dataset MultiMedQA, demonstrating the\nimportance of a comprehensive benchmark for MQA. The dataset covers diverse MQA bench-\nmarks for medical examination, consumer health, and medical research. The study also empha-\n20https://github.com/baichuan-inc/baichuan-7B\n21https://github.com/THUDM/ChatGLM-6B\n22https://huggingface.co/internlm/internlm-chat-7b\n23https://github.com/ymcui/Chinese-LLaMA-Alpaca\n24https://github.com/zjunlp/CaMA\n25http://www.mk-base.com/#/official/home\n26https://github.com/michael-wzhu/ChatMed\n27https://huggingface.co/datasets/michaelwzhu/ChatMed Consult Dataset\n19\nsized the significance of manual evaluation of model answers and alignment strategies in the\nmedical domain. Med-PaLM accurately answered both multiple-choice and open-ended ques-\ntions and explained its responses. However, the quality of the modeled answers still has some\nshortcomings when compared to those of physicians.\nTo address these gaps, Google has released PaLM2, a next-generation AI language model.\nAdditionally, they have developed Med-PaLM 2, a medical domain variant based on PaLM2. To\nimprove the reasoning ability of LLM, a new cueing strategy called ”Ensemble refinement” has\nbeen proposed. Med-PaLM 2 consistently performed at the ”expert” physician level on medical\nexam questions, scoring 85%. This represents an 18% improvement over Med-PaLM’s previous\nperformance and significantly outperforms similar AI models. The final results demonstrate that\nMed-PaLM 2 performs considerably better than Med-PaLM on both the multiple-choice and\nlong-form assessments.\nClinicalGPT: ClinicalGPT, proposed by Wang et al., is a language model specifically de-\nsigned and optimized for clinical scenarios. The model can e ffectively handle a wide range of\nclinical tasks by incorporating a diverse range of real-world data, including medical records,\ndomain-specific knowledge, and multi-turn dialogue consultations during the training process.\nThe model is based on BLOOM-7B and incorporates LoRA fine-tuning for training. Experimen-\ntal results demonstrate its suitability for healthcare encyclopedic tasks.\nMedicalGPT28: Xv’s MedicalGPT implements the training of language models for the med-\nical industry using the ChatGPT Training Pipeline. The model is based on Bloom, LLaMA,\nChatGLM29, Baichuan 7B /13B30, and other general-purpose LLMs. These models were fine-\ntuned using the ChatGPT training pipeline, which includes secondary pre-training, supervised\nfine-tuning, reward modeling, and reinforcement learning training. The training dataset com-\nprises both medical and general-purpose datasets. The medical dataset consists of 2.4 million\nChinese medical datasets31, including pre-training, instruction fine-tuning, and reward datasets,\nas well as 220,000 Chinese medical dialogue datasets from the HuaTuo project. The results of\nthe experiments indicate that MedicalGPT is significantly more e ffective in both daily QA and\nMQA. However, there are still issues with factual errors and unclear identification of hazardous\ncommands, and the performance in scenarios involving reasoning and multi-turn dialogues needs\nimprovement.\nHuatuoGPT: Zhang et al. developed HuatuoGPT to provide the language model with the\nability to diagnose and provide medical consultation advice in Chinese, similar to that of a physi-\ncian. The model is constructed using Baichuan-7B and LLaMA-13B and fine-tuned with four\ntypes of corpora. These corpora consist of distilled data generated by ChatGPT, instruction, and\ndialogue data from real-world physicians responding to patients’ questions. The aim is to main-\ntain knowledge-rich communication with the user. Finally, following mutual validation of the\nautomatic and manual evaluations, the models demonstrate strong performance in both single-\nturn QA and multi-turn interactive diagnostic scenarios.\nBianQue: Bianque is a large-scale healthcare dialogue model initialized with ChatGLM-\n6B and fine-tuned with instructions from the BianQue corpus. BianQue-2.0 has expanded the\ninstruction data with drug, medical encyclopedic knowledge, and ChatGPT distillation instruc-\ntions, which improves the model’s ability to make suggestions and answer knowledge queries.\n28https://github.com/shibing624/MedicalGPT\n29https://github.com/thudm/chatglm2-6b\n30https://github.com/baichuan-inc/Baichuan-13B\n31https://huggingface.co/datasets/shibing624/medical\n20\nUnlike most language models, this model can be more closely related to everyday life and im-\nprove questioning skills using question chains.\nDoctorGLM: DoctorGLM is the first Chinese diagnostic language model developed by the\nShanghai University of Science and Technology. It is based on ChatGLM-6B and uses real-\nworld online diagnostic dialogue data, including CMD, MedDialog, ChatDoctor, and Health-\ncareMagic. The model uses two di fferent parameter fine-tuning methods, p-tuning and LoRA,\nto enable medical communication in natural dialogue. However, the diagnostic capability may\nneed improvement.\nChatDoctor: ChatDoctor is an advanced language model designed specifically for medical\napplications, aiming to provide patients with an intelligent and reliable medical companion that\noffers personalized medical advice while answering medical queries. Based on the LLaMA\nmodel, it is trained and fine-tuned using over 110,000 real doctor-patient dialogue datasets\n(including the HealthCareMagic-100k dataset from the medical consultation website Health-\nCareMagic, the iCLINIQ-10K dataset from the medical consultation website iCliniq, and the\nGenMedGPT-5k dataset generated using ChatGPT from a disease database). It is equipped with\nan external knowledge base, which includes a knowledge library containing over 700 diseases, to\naccurately retrieve the corresponding knowledge and reliable sources to answer patient inquiries.\nPULSE32: The PULSE model is developed on the OpenMEDLab platform and is based on\nthe OpenChina LLaMA 13B. The model is further fine-tuned using approximately 4 million SFT\ndata from medical and general fields to support various NLP tasks in the medical field, including\nhealth education, doctor examination questions, report interpretation, medical record structure,\nand simulated diagnosis and treatment. However, due to the relatively small model size, although\nthe model provides reasoning results about disease diagnosis and treatment, these results cannot\nreplace offline professional doctors’ advice and treatment plans. All responses are for reference\nonly and should not be used as the basis for diagnosis or treatment.\nMulti-stage Training. In addition to the methods mentioned above, some researchers construct\nmore comprehensive knowledge-gaining and widely applicable medical models across tasks by\nusing a combination of pre-training and fine-tuning in multi-stage training, as shown in Table 7.\nWe will describe them briefly:\nTable 7: Pre-training and fine-tuning integrated for Medical LLMs. These models often integrate clinical data in multiple\ntraining stages to ensure the accuracy of the knowledge representations and multi-task utilization.\nModel Anchor Model Data Source Task\nChiMed-GPT [76] Ziya-13B-v2 [77] CMD. IE, QA, MD\nQilin-Med Baichuan-7B ChiMed MT, QA\nChiMed-GPT:Tian et al. proposed a new benchmark LLM designed for the Chinese medical\nfield, based on Ziya-13B-v2, inheriting its ability to handle a wide range of context lengths and\nextending the context length to 4096 tokens. ChiMed-GPT integrates pre-training, supervised\nfine-tuning (SFT), and reinforcement learning from human feedback (RLHF) stages, ensuring\nthat it not only captures domain-specific knowledge but also adapts to various situations, surpass-\ning existing models that typically rely solely on SFT. Evaluations on practical tasks (including IE,\nQA, and dialogue generation) show that ChiMed-GPT performs superiorly on general-domain\n32https://github.com/openmedlab/PULSE\n21\nLLMs. However, researchers have found that the model exhibits potential biases that urgently\nneed to be addressed.\nQilin-Med: Ye et al. have constructed a subsample dataset ChiMed, which includes medical\nquestion answering, plain text, KGs, and dialogue, among others. Based on the characteristics\nof the data, they proposed a multi-stage training method that combines domain-specific contin-\nued pre-training (DCPT), SFT, and direct preference optimization (DPO). The final experiments\ndemonstrate that, during the CPT and SFT stages, the Qilin-Med trained using the multi-stage\ntraining method achieved accuracy rates of 38.4% and 40.0% on the CMExam, respectively,\nsurpassing the baseline model Baichuan-7B’s 33.5%. In the DPO stage, on the Huatuo-26M\ntest set, Qilin-Med scored 16.66 and 27.44 on the BLEU-1 and ROUGE-1 metrics, respectively,\nbetter than SFT’s 12.69 and 24.21. Although the training method of Qilin-Med shows clear\nadvantages in improving the performance of LLMs for medical applications, it still has certain\nlimitations in terms of datasets, multi-stage training preferences, and evaluation metrics. For\ninstance, although the dataset covers comprehensive medical knowledge, it mainly focuses on\nChinese medical knowledge, limiting the model’s global applicability. The preference risk in the\nmulti-stage training process may introduce the preferences of human evaluators, and metrics like\nBLEU and ROUGE may not be able to capture the entire performance of the model in medical\nscenarios.\n3.1.2. Building Medical LLMs from Multimodal Data\nCompared to aforementioned monomodal medical LLMs, multimodal medical LLMs con-\ntain more comprehensive knowledge that contributes to more accurate predictions. Moreover,\nmultimodal data contains knowledge with smaller granularity, which can compensate for the de-\nficiencies of a single modality. Therefore, multimodal LLMs can help extract common features\nacross modalities.\nIn the following section, we will conduct an in-depth analysis of the current e fforts in the\nfield, presenting the current research status and development trends using the construction meth-\nods of strategies such as pre-training and fine-tuning. Table 8 summarises the construction meth-\nods of multimodal medical LLMs.\nPre-training. Building multimodal LLMs often requires creating uniform multimodal represen-\ntations of the same object across di fferent modalities through pre-training methods. Traditional\nmultimodal pre-training methods usually employ contrastive learning (CL) to help models distin-\nguish between different modal representations. As shown in Figure 2. The core idea is to teach a\nmodel’s representation by comparing the similarity between positive and negative samples. This\nis done even if the distance between positive examples becomes closer and the distance between\nnegative examples becomes farther. Models [25] created with CL generally perform better for\nmultimodal image-text retrieval.\nOnce the LLM is proposed, pre-training the model using full parametric CL methods be-\ncomes challenging. This is because pre-training typically necessitates a substantial amount of\nhigh-quality data, and full parametric training is hardware-intensive and costly. Therefore, some\nresearchers have proposed new training approaches, as shown in Figure 3.\nFor example, Luo et al. [78] proposed a novel end-to-end multimodal semantic understand-\ning framework called BioMedGPT. This framework uses the pre-trained large language model\nBioMedGPT-LM, specifically designed for the biomedical domain, to connect natural language,\nbiological encoding language, and chemical molecular language. The text, molecules, and pro-\nteins are encoded separately using encoders in the model. The feature vectors of the pairs are\n22\nFocal consolidation \nat the left lung base...\nt1 𝑡2 … 𝑡𝑛\n𝑙1\n𝑙2\n𝑙𝑛\n… ……\n…\n…\n…\n𝑙1t𝑛\n𝑙2t𝑛𝑙2t1\n𝑙𝑏t1\n…\n𝑙𝑛t𝑛\n𝑙1t1 𝑙1t2\n𝑙𝑏t2\n…\n𝑙2t2\nText\nEncoder\nImage\nEncoder\nFigure 2: Contrastive learning multimodal pre-training technique. This type of pre-training has strong transferability and\nzero-shot capabilities.\nTransformer Encoder\nAutoregressive Decoder\n3 ResNet\nBlocks\nBPE & Token\nEmbedding\n(Image Infilling) What is the \nimage in the middle part?\n(VQA) What does the CT \nscan of thorax show?\nAutoregressive Decoder\nBilateral\nmultiple nodulespulmonary\nmultiple pulmonary\nor\nFigure 3: An end-to-end multimodal pre-training technique. This type of pre-training views the model as a pipeline\nsolution for specific tasks. Therefore, their abilities can be transferred among similar tasks easily.\n23\nTable 8: Summary of methods for constructing multimodal medical LLMs. We present these methods according to the\nmodality of their training strategies.\nStrategy Model Anchor Model Data Source Task\nPre-training BioMedGPT [78] LLaMA2-Chat-7B S2ORC [79]\nBioMedical QA\nMolecule QA\nProtein QA\nFine-tuning\nMed-PaLM M PaLM-E [80] MultiMedBench\nQA\nRS\nVQA\nRG\nMIC\nQilin-Med-VL Clip-ViT-large-patch14-336\nChineseLLaMA2-13B-ChatChiMed-VL VQA\nLLaV A-Med [81] GPT-4 [82] PMC-15M VQA\nXrayPULSE PULSE MIMIC-CXR\nOpenI Multimodal MD\nXrayGLM [83] VisualGLM-6B MIMIC-CXR\nOpenI\nMedical Imaging diagnosis\nMD\nVisual Med-Alpaca [84] LLaMA-7B VariousMedQA [84]\nQA\nVQA\nMD\nthen concatenated into the input for the model’s encoder. Finally, a self-regressive decoder is\nemployed to generate results for each modality. Consequently, the model can support interac-\ntive QA across natural language and molecular language modalities. It has potential applications\nin drug target exploration and mining, lead compound design and optimization, and protein de-\nsign. Additionally, it has achieved SOTA performance on multiple biomedical QA benchmark\ndatasets, including USMLE, MedMCQA, and PubMedQA, matching the level of human medical\nexperts. It has also successfully passed the United States Medical Licensing Examination.\nFine-tuning. Similar to constructing large medical text models, researchers prefer to fine-tune\nmodel parameters to improve the model’s performance on specific tasks, which incurs a smaller\ntraining cost. This approach to fine-tuning can be divided into three main categories: end-to-end\nmethods, BLIP-derived methods, and prompt-combination methods.\nEnd-to-end Methods. The end-to-end approach is comparable to the pre-training approach\nof BioMedGPT mentioned earlier. This involves encoding data from di fferent modalities sepa-\nrately and then aligning the multimodal vector representation using a unified model. Med-PaLM\nM is a typical example of this approach.\nMed-PaLM M is a multimodal generative model developed by Google Research and Deep-\nMind for encoding and interpreting biomedical data, including clinical language, medical imag-\ning, and genomics, among others. The model is fine-tuned to align the PaLM-E to the biomed-\nical domain through the MultiMedBench benchmark. Med-PaLM M achieves competitive or\nsuperior performance to the SOTA in all tasks of MultiMedBench. Additionally, Med-PaLM M\nsignificantly outperforms PaLM-E, demonstrating the importance of biomedical fine-tuning and\nalignment. The experiments on clinical adaptability also show the potential clinical utility of\nthe model. Additionally, it demonstrates zero-shot generalization to medical concepts and tasks,\nindicating the model’s ability to reason and make decisions for medical situations for which it\nhas not been explicitly trained.\nBLIP-derived Methods. However, the end-to-end approach lacks the flexibility to solely\nutilize pre-trained models that support multimodality or combinations of already trained multi-\n24\nmodal encoders. To address this issue, the researchers have utilized the BLIP training framework,\nwhich offers greater flexibility in combining LLMs that support either image processing or text\nanalysis. As shown in Figure 4, the approach integrates a Q-Former neural network between the\nimage encoder and text model. The multimodal models can be linked by training the Q-Former\nalone while keeping the image and text models frozen.\nImage\nEncoder LLM\n Please describe the diagnostic\nfindings on this chest x-ray.\nThe heart is normal in size and the\nlungs are clear. There is no\nevidence of acute cardiopulmonary\ndisease.\nQ-Former\nQuerying Transformer\n… Text\nQueries\nBootstrapping Pre-trained \nImage Models\nVision-and-Language \nRepresentation Learning\nVision-to-Language \nGenerative Learning\nBootstrapping Pre-trained LLMs\nFigure 4: BILP fine-tuning method. This type of method treats di fferent parts of the model separately so that we can\nimprove the model performance without impacting the model generalization.\nThe framework introduces three main learning objectives for training tasks: image-text match-\ning, image-based text generation, and image-text CL. In the medical field, the models constructed\nusing this method are mainly:\n• Qilin-Med-VL: Liu et al. proposed the Qilin-Med-VL, the first Chinese large-scale visual\nlanguage model that integrates textual and visual data analysis. The model combines a pre-\ntrained visual transformer (Clip-ViT-large-patch14-336) with a base LLM (ChineseLLaMA2-\n13B-Chat) to improve the ability to generate medical text and answer complex medical\nqueries. Additionally, Liu et al. released the ChiMed-VL dataset, which consists of over\n1 million image-text pairs. This dataset has been meticulously crafted to o ffer a thorough\nand complete analysis of medical information through diverse image types.\n• LLaV A-Med: To address the lack of complexity in the understanding and dialoguing of\nbiomedical images by general-domain vision-language models, Li et al. have proposed\na cost-e ffective method for training a vision-language dialogue assistant, LLaV A-Med,\ncapable of answering open research questions in biomedical images. This is the first at-\ntempt to extend multimodal instruction tuning to the biomedical field, enabling end-to-end\ntraining for developing a biomedical multimodal dialogue assistant. The model is based\non LLaMA, fine-tuned on image-text pairs to align with biomedical vocabulary, and then\nfurther trained with instruction data automatically generated by GPT-4 based on sampled\nbiomedical image-text pairs from PMC-15M, to learn open-ended dialogue semantics. Ex-\nperimental studies finally validate the effectiveness of domain-specific instruction tuning.\nOn established biomedical VQA datasets, the performance of fine-tuned LLaV A-Med gen-\nerally surpasses that of supervised SOTA methods.\n25\n• XrayPULSE33: XrayPULSE is an extension of PULSE, a model that uses MedCLIP as\na visual encoder and a simple linearly transformed Q-former (BLIP2) as an adapter to\nincorporate the image into PULSE. To align frozen visual encoders and LLMs via adapters,\nOpenMEDLab generates the Chinese version of Xray-Report paired data from radiological\nreports (MIMIC-CXR and OpenI) to create the Chinese version of Xray-Report paired\ndata. Finally, XrayPULSE has been fine-tuned on this dataset to serve as a biomedical\nmultimodal dialogue assistant, extending PULSE.\n• XrayGLM: The XrayGLM proposed by Wang et al. is the first Chinese multimodal med-\nical LLM for chest X-rays, demonstrating extraordinary potential in medical image diag-\nnosis and multi-turn interactive dialogue. The base model, VisualGLM-6B, is trained via\nthe BILP method based on ViT and ChatGLM2. In the fine-tuning stage, the model was\ntrained on the publicly available chest X-ray dataset MIMIC-CXR and a medical multi-\nmodal dataset constructed with X-ray images and diagnostic reports, aided by ChatGPT\nand publicly available datasets.\nPrompt-combination Methods. In addition to the previously mentioned multimodal con-\nstruction mechanism, another way to construct multimodal models is to link already trained mul-\ntimodal encoders through prompts. Figure 5 below. This method is cheaper and more convenient\nfor training than the abovementioned methods.\nType \nClassifier\nMed-GIT\nDePlot\n…\nPrompt \nManager Med-Alpaca\nResponse\nImage\nText\nData Preprocessing and Representation Prompt Output\nFigure 5: Prompt-combination fine-tuning method. This type of method combines the excellent performance of models\nover multiple modalities to create a stronger model through prompt design.\nVisual Med-Alpaca is a prime example of a multimodal healthcare LLM constructed with\nprompts. The model is based on LLaMa-7B and is trained using an instruction set developed\ncollaboratively by GPT-3.5-Turbo and human experts. The instruction set extracts medical ques-\ntions from medical datasets in the BigBIO repository. These questions are then used to direct\nGPT-3.5-Turbo to generate answers. The resulting QA pairs undergo multiple human filtering\nand editing rounds to optimize their quality. The final instruction set contains 54k high-quality\ninstructions. Finally, with hours of fine-tuning and a plug-and-play vision module, Visual Med-\nAlpaca can perform various medical tasks.\n33https://github.com/openmedlab/XrayPULSE\n26\nIn summary, these two building mechanisms of LLMs above have shown excellent perfor-\nmance in one or more medical tasks. However, they still face several challenges, including\nquality issues in data annotation, uneven data distribution, insufficient model interpretability and\ntransparency, and limitations in multimodal information fusion. During the evaluation of model\nperformance, most methods rely primarily on automated medical task evaluation metrics, while\nrelatively few incorporate manual evaluation. Furthermore, the practical application value of\nthese models still requires further validation and evaluation.\nAlthough these LLMs are not yet fully ready for practical application, their great potential\ncannot be ignored. They o ffer useful references and insights for practical applications in the\nmedical field and indicate directions for future research and development.\n3.1.3. Biomedical Agent\nIn addition to the traditional approach of pre-training and fine-tuning to develop medical\nLLMs, the ability of complex reasoning and tool invocation possessed by LLMs has led to an\nincreasing interest in using agents, which are LLM derivatives. Agents aim to design and build\ncomputer-based agents that exhibit intelligent behavior. There is currently no widely accepted\ndefinition of an agent. However, it is generally understood [85] that artificial agents are artificial\nentities that can perceive their surroundings using sensors, make decisions, and take responsive\nactions using actuators, as shown in Figure 6.\nAgent\nEnvironment\nBased on the x-rays you have \nprovided and the symptoms you \nhave described, pneumonia is \nmore likely. You are advised to \ncome to the hospital for further \ntests such as CT scan and blood \ntests to confirm the diagnosis. \nBelow is your appointment \ninformation and checklist.\nText\n Tools\nCalling API…\nEmbodiment\nInputs\n…\n…\nPerception\nAction\nStorage\nMemory Knowledge\nDecision Making\nPlanning\n/Reasoning\nSummary Recall Learn Retrieve\nGeneralize/Transfer\nBrain\nI've been coughing and \nfeeling a little short of \nbreath lately. Here is my \nx-ray. What is wrong \nwith me?\nFigure 6: LLM-based agent framework. It contains three components: brain, perception, and action.\nIntegrating LLMs through agents enables the automation of complex tasks using LLMs.\nThese agents can interact with the environment, humans, or other agents through perception,\nreasoning, and planning [85] [86] [87]. Recent work [88] has shown the potential of combining\nagents with LLMs, advancing healthcare automation.\nTo facilitate online communication with adolescents with autism spectrum disorders, Ali et\nal. [89] proposed an online virtual dialogue agent. Users engage in casual conversations with the\nvirtual agent, receiving real-time feedback and summaries. The agent generates feedback and\ndialogue automatically using hidden Markov models and a pattern-driven dialogue manager ca-\n27\npable of handling multi-topic conversations. This system is expected to help adolescents alleviate\nautistic symptoms and positively impact behavior change.\nAbbasian presented a conversational health agent (CHAs) framework [90] based on an LLM\nthat enables CHAs to generate personalized responses to users’ health queries. This framework\nenhances personalized medical healthcare services by participating in emotionally charged di-\nalogues and processing multimodal data in an interactive system. Integrating healthcare data\nsources, supporting multilingual and multimodal dialogue, and interacting with various user data\nanalysis tools provide critical thinking, knowledge acquisition, and problem-solving capabilities.\nThe framework aims to improve the ability of CHAs to respond effectively to healthcare queries\nby analyzing input questions, collecting data, performing operations, and providing personalized\nresponses.\nTo address LLMs’ challenges in handling domain-specific terminology and reasoning with\nexpert knowledge, Tang et al. [91] proposed a multidisciplinary collaboration framework. In the\nmedical domain, this framework uses role-playing agents based on LLMs to engage in iterative\nmulti-turn discussions, thereby enhancing LLMs’ knowledge and reasoning capabilities. The\nframework comprises five key steps: gathering domain expertise, proposing individual analyses,\nsummarising these analyses in reports, iterating in discussions until consensus is reached, and\nfinally, making decisions. Experimental results on nine datasets (including MedQA, MedMCQA,\nPubMedQA, and six subtasks from MMLU) demonstrate the exceptional performance of the\nframework in mining and exploiting medical expertise within large models and extending their\nreasoning capabilities.\nUsing agents allows LLMs to more accurately use external tools for medical and biological\ndomain expertise, thereby mitigating the problem of hallucination that LLMs face in certain do-\nmains. Jin et al. [92] presented GeneGPT, a method that guides LLMs to use the National Center\nfor Biotechnology Information (NCBI) Web API to answer genomics questions. Specifically,\nthrough context learning and an improved decoding algorithm, this method encourages large\nmodels to use external tools to solve related judgments by detecting and executing API calls.\nExperimental results show that GeneGPT achieves SOTA performance compared to large mod-\nels such as Bing, BioMedLM, BioGPT, GPT-3, and ChatGPT on eight tasks of the GeneTuring\nbenchmark.\nTo improve the effectiveness of LLMs in clinical tasks, Alex et al. [93] investigated the abil-\nity of ChatGPT to perform medical calculations and evaluated its performance on 48 di fferent\nclinical calculation tasks. The study results showed that ChatGPT performed inconsistently in\nclinical calculations, providing inaccurate answers in a third of these trials. To address this is-\nsue, the study developed an open-source clinical computation API. It integrated it into ChatGPT,\nallowing the LLM to interact with external programs and tools the same way as an agent. The\nresults show that the augmented model significantly improves accuracy by evaluating perfor-\nmance on three common clinical computing tasks using 75 clinical cases and comparing it to\nother models.\nThese models showcase the potential of applying AI to agents in various domains, from aid-\ning social interactions for individuals with autism spectrum disorders to o ffering personalized\nhealthcare guidance. However, some unresolved issues remain, such as improving the handling\nof complex contexts and enhancing the naturalness of dialogues in virtual conversational agents.\nAdditionally, there is a need to more e fficiently integrate multimodal data and improve the ac-\ncuracy of personalized responses in the framework of health agents. These areas require further\ninvestigation.\n28\n3.2. Collaborative Development of Medical LLMs and Clinical KGs\nIn recent years, with the rapid development of KGs and LLMs, combining KGs and LLMs\nhas become a popular research direction [94].\nLLMs can adapt to di fferent contexts and generate natural language for various NLP tasks,\nincluding text summarisation, QA systems, and machine translation. However, it su ffers from\nproblems such as factual fabrication and lack of interpretability due to being parameterized im-\nplicit knowledge. In contrast, KG is structured explicit knowledge that is naturally interpretable.\nDomain-specific knowledge is of high quality but has high construction overheads, is often in-\ncomplete, and lacks NLP capabilities. Therefore, combining the two can effectively compensate\nfor their deficiencies, take advantage of their complementary strengths, and jointly improve the\nNLP capability and application scope.\nIn medical AI research, we categorize the integration of LLM and KG into three types: (1)\nLLMs for Medical KGs: LLM empowers medical KG construction, which means using LLM’s\nadvantage to enrich and optimize medical KGs. (2) KGs for Medical LLMs: Medical KG em-\npowers LLM, integrating structured knowledge from KGs into LLM and enhancing its knowl-\nedge understanding and application capabilities in specific fields. (3) Medical LLMs and KGs:\nThe mutual synergy between medical KG and LLM is to achieve more e fficient knowledge re-\ntrieval and language generation through their interaction. Next, we will introduce the relevant\nresearch progress in detail.\n3.2.1. LLMs for Medical KGs\nConstructing and maintaining traditional KGs is time-consuming and requires much manual\ninput. Each step, from data collection and cleaning to knowledge collation and annotation, as\nwell as regular updating and iteration, requires many annotators and professional knowledge\nsupport. This process is costly and inefficient. LLM provides an effective solution for automating\nknowledge acquisition on KG platforms. It has excellent generalization capabilities and can\nprocess textual information. LLMs are used for constructing medical KGs in three main types:\nmethod paradigm extension, processing object extension, and task type extension.\nMethod Paradigm Extension. In healthcare, method paradigm extension refers to combining\nLLM with KGs to extract, represent, and reason about medical data and knowledge using tech-\nniques such as graph neural networks (GNNs) and NLP. This approach can help medical LLMs\nbetter understand and apply medical knowledge. Research in this direction has:\n• ERNIE-Health34: ERNIE-Health, leveraging Baidu Wenxin’s knowledge-enhanced pre-\ntrained language model ERNIE, acquires a vast amount of medical knowledge through\ntechnologies that enhance medical knowledge. ERNIE-Health has learned over 600,000\nmedical terms and over 40 million medical QA data, significantly outperforming other\ncompared models on the CBLUE multi-tasking list (which covers five major tasks: medical\ninformation extraction, medical term normalization, medical text classification, medical\nsentence relation judgment, and QA). This greatly enhances the model’s understanding\nand modeling capabilities of medical professional knowledge. As a result, the model can\naccurately identify medical terms within a vast amount of medical text, thus automating\nthe extraction of medical knowledge.\n34https://huggingface.co/nghuyong/ernie-health-zh\n29\n• GatorTron [17] [95]: GatorTron is an early LLM developed for healthcare specifically to\nexplore how clinical LLMs with tens of billions of parameters can derive knowledge from\nunstructured EHRs. The model was trained on over 9 billion data tokens, including more\nthan 8.2 billion words of non-identifying clinical text. It was systematically evaluated on\nfive clinical NLP tasks: clinical concept extraction, medical relationship extraction, seman-\ntic text similarity, medical natural language inference, and MQA. The final experimental\nresults demonstrate that GatorTron significantly improves sentence-level and document-\nlevel NLP tasks. Thus, the model accurately represents medical knowledge, making it an\nimportant tool for medical KG vectorization.\n• Graph-ToolFormer [96]: Zhang et al. proposed Graph-ToolFormer, a framework fo-\ncused on graph inference that teaches LLMs to use external graph inference tools through\nChatGPT-enhanced prompts. During training, the model instructs Graph-ToolFormer to\nhandle various graph data reasoning tasks, including basic graph data loading and graph\nattribute reasoning in the field of biochemical molecules. When compared to other LLMs\non reasoning tasks, it is evident that Graph-ToolFormer improves LLM’s capacity to apply\nknowledge and reasoning-related aspects.\n• KSL [97]: To use the reasoning capabilities of LLMs in assisting with retrieving knowl-\nedge from external KBs, researchers have also explored the use of additional modules, such\nas GNNs. Feng et al. proposed the Knowledge Solver (KSL), which teaches additional\nnetworks to search for basic knowledge from external KBs by exploiting the strong gen-\nerality of LLMs themselves. Specifically, the model transforms retrieval into a multi-hop\ndecision sequence, enabling LLMs to search for knowledge in a zero-shot task. Addition-\nally, the KSL provides complete search paths, increasing the interpretability of the LLM\nreasoning process. Experiments on three datasets, including MedQA-USMLE in health-\ncare, demonstrate that the approach significantly improves the benchmark performance of\nthe LLMs.\nProcessing Object Extension.In healthcare, processing object extension refers to expanding the\napplication of LLM to additional types of medical data and knowledge, such as medical records,\nmedical images, and drug information. This approach can enhance the e ffectiveness of medical\nLLM applications.\nFor instance, BioMedGPT is a fundamental model in the multimodal biomedical field that\naims to improve the capability of various downstream tasks by unifying the representation learn-\ning of molecules, texts, and knowledge. BioMedGPT integrates diverse and heterogeneous data\nat the data level, including genes, molecules, cells, proteins, literature, patents, and KBs. For the\nfirst time, it introduces knowledge into the model construction, enabling unified representation\nlearning of biological text and knowledge, which enhances the model’s generalization ability\nand interpretability. Regarding application tasks, BioMedGPT can handle multiple tasks such as\nnatural language, drug property prediction, and cross-modal generation, allowing for a compre-\nhensive exploration of all tasks in the field of life sciences. It has achieved the best results in\nseveral key downstream tasks.\nTask Type Extension.In healthcare, task type extension refers to expanding the application sce-\nnarios of LLM to include more medical tasks, such as disease diagnosis, treatment recommenda-\ntions, and medical research. This approach can enhance the flexibility and applicability of med-\nical LLM. For instance, ChatDoctor is built on the LLaMA model, which has been trained and\n30\nfine-tuned using over 11 million real doctor-patient dialogues. It is also equipped with an external\nKB containing information on more than 700 diseases. By leveraging the excellent generative\nand multi-tasking generalization capabilities of the LLM, ChatDoctor can provide patients with\naccurate responses to their queries, along with personalized treatment recommendations from\nreliable sources.\n3.2.2. KGs for Medical LLMs\nWhile ChatGPT, GPT-4, and other LLMs have shown impressive language comprehension\nand generation abilities, they are still limited by their pre-trained corpus and model capabili-\nties. They also face challenges such as limited knowledge memory, weak knowledge reasoning,\nand difficulty in updating knowledge. To address these issues, KGs can be a viable solution\ndue to their structured and explicit representation of knowledge, natural interpretability, and\nhigh-quality information. Additionally, KGs can also represent and generate Chain-of-Thought\n(CoT), which can further enhance the reasoning ability of LLMs by structuring better CoT. The\napproaches with KGs for medical LLMs can be broadly classified into three categories: knowl-\nedge learning during the training, knowledge integration during the reasoning, and continuous\nlearning from historical experience.\nKnowledge Learning During the Training.In healthcare, knowledge learning during the training\nmainly involves training the LLM with a large amount of data from medical texts and KGs to\nenable it to understand professional knowledge and concepts. The key to this phase is to ensure\nthe training data’s quality and diversity, the training task’s rational design, and the loss function\nto improve the LLM’s ability to understand and express medical knowledge.\n• BenTsao used the LLaMA-7B model along with Chinese medical instruction-tuning tech-\nnology to create an efficient medical LLM. Using the medical KG and GPT3.5 API, they\ncreated a dataset of Chinese medical instructions. They then fine-tuned LLaMA’s instruc-\ntions based on this dataset, significantly improving the model’s QA performance. The\nmodel is trained using CMeKG and other relevant Chinese medical literature resources.\nAfter a series of designs and optimization, a large-scale medical LLM named ”BenTsao”\nhas been created. The model outperforms comparative models like LLaMA and Alpaca in\nthe MQA task.\n• ShenNong-TCM-LLM35 generates data using the open-source TCM KG and is trained\nusing the entity-centric self-instruct method. It outperforms other instruction-tuning mod-\nels on the cMedKnowQA dataset.\n• DISC-MedLLM [98] is a medical LLM designed specifically for healthcare conversa-\ntional scenarios. The model combines medical KGs, real-world dialogues, and human\nfeedback methods to create a high-quality supervised fine-tuning dataset. This dataset is\nthen used to train the model on the Baichuan-13B-Base. Finally, experiments on a large\nmedical dataset validate the significant impact of the model in improving the medical con-\nsultation capabilities of LLM.\n35https://github.com/michael-wzhu/ShenNong-TCM-LLM\n31\nKnowledge Integration During the Reasoning.In healthcare, knowledge integration during rea-\nsoning focuses on combining LLMs with medical KGs to answer medical questions or provide\nmedical advice through reasoning and querying. The key to this phase is to design e ffective\nreasoning algorithms and query strategies to fully utilize the information in the medical KG and\nimprove the LLM’s reasoning and answering capabilities.\nGao et al. [99] proposed an innovative approach to improve LLMs for automatic diagno-\nsis generation by introducing medical KGs and graph-based models. This approach eliminates\nthe need for pre-training and instead uses KGs to aid in explaining and summarising complex\nmedical concepts during inference. Experimental results on a real hospital dataset show that the\nmethod effectively improves the accuracy of automatic diagnosis generation.\nThere has been less research in this direction in healthcare, but Sun et al. [100] have pro-\nposed an LLM-KG integration paradigm that could potentially be extended to medical LLMs.\nThis paradigm considers the LLM as an agent that interactively explores relevant entities and re-\nlationships on the KG and performs reasoning based on the retrieved knowledge. The paradigm\nis achieved by introducing a new approach called Think-on-Graph (ToG). The LLM agent per-\nforms an iterative bundle search on the KG, discovers the most promising inference paths, and\nreturns the most probable inference results. This approach achieves SOTA performance on sev-\neral KBQA datasets.\nContinuous Learning from Historical Experience.In healthcare, continuous learning from his-\ntorical experience focuses on continuously optimizing and improving the performance of LLM\nby recording and analyzing historical data and cases. The key to this phase is to design effective\nmethods for collecting and analyzing experiences and cases to help LLMs continuously learn\nand improve. Among the above models, models such as DISC-MedLLM and HuaTuoGPT per-\nform well in multi-turn dialogue scenarios, partly due to the continuous learning and analysis of\nhistorical dialogue data.\n3.2.3. Medical LLMs and KGs\nIn the field of AI applications within healthcare, the demand for interpretability, credibility,\nand traceability of outcomes exceeds that of other domains. This necessitates a combination of\nmedical KGs with various LLM technologies to play a more significant role. LLMs, often due to\ntheir black-box nature, may produce biased results. However, KGs contribute to interpretability,\ncredibility, and traceability, aiding in understanding the workings of LLMs. Therefore, integrat-\ning KG throughout the entire lifecycle of LLMs, from pre-training to various application stages,\nis an efficient strategy. This integration not only enhances the training efficacy of LLMs but also\nimproves the practicality and reliability of their inferential results. Nevertheless, development in\nthis area is still somewhat lacking.\nCurrently, medical KGs and LLMs are constructed collaboratively using various methods\nsuch as information filtering, retrieval augmented generation by KG, generation enhanced ex-\ntraction, and dynamic collaborative enhancement.\n• Information Filtering. Information filtering refers to extracting relevant knowledge from\nKBs and then sending it to the LLM for answering. Zhang et al. [101] proposed using\nLangChain to create a new model that combines KGs and LLMs deeply. Firstly, the input\ntext of questions related to TCM formulas undergoes information filtering, specifically\ntext classification, to determine its relevance. Secondly, LangChain retrieves knowledge\nrelated to the text from the KB and inputs it into LLMs such as ChatGPT and ChatGLM,\n32\nalong with the question in the form of prompts. The LLM then generates a professional\nanswer through reasoning. Finally, the answer undergoes knowledge extraction to extract\nthe triples. The extracted triple is then matched with the existing prescription KG to verify\nits expertise. Furthermore, the nodes in the KG are used as input for the LLM to obtain\nnatural language explanations, achieving bidirectional conversion between the LLM and\nthe KG.\n• Retrieval Augmented Generation by KG. To generate more knowledgeable responses\nbased on LLM and a self-built KB, collaboration between LLM and KG can also be\nachieved. Soman et al. [102] introduced a task-agnostic KG-based retrieval-augmented\ngeneration (KG-RAG) framework, which leverages large biomedical KGs like SPOKE\nin conjunction with LLMs (e.g., Llama2, GPT-3.5, and GPT-4) to generate meaningful\nbiomedical texts based on established knowledge. This framework combines explicit and\nimplicit knowledge from KG and LLM, respectively, thereby enhancing the adaptability\nof general LLMs to address specific domain problems within a unified framework. KG-\nRAG continuously improves the performance of LLMs on di fferent task types, including\ndrug-use queries, biomedical questions, and multiple-choice questions.\n• Generation Enhanced Extraction. By reinforcing the information extraction capability\nof LLM, the generated natural language responses are extracted to structured knowledge\nand matched with the professional KG for verification, realizing a deep integration method\nbetween LLMs and KGs. Xu et al. [103] conducted in-depth research on using LLMs for\nclinical text generation tasks and proposed an innovative and resource-e fficient method\ncalled ClinGen, which integrates knowledge into the entire clinical process. This model\nutilizes a medical domain-specific KG and an LLM to guide data generation. Research on\nseven clinical NLP tasks and 16 datasets shows that ClinGen can continuously improve\nperformance and significantly enrich the diversity of generated instances in various tasks.\n• Dynamic Collaborative Enhancement. Unlike previous approaches that synergize the\nLLM with static KGs, Li et al. [104] propose a DALK framework that combines the LLM\nwith dynamic KGs to improve the model’s ability to answer Alzheimer’s disease (AD)\nquestions. The approach first utilizes the LLM to construct an evolving AD-specific KG\nfrom AD-related scientific literature, then enhances the inference of the LLM by select-\ning appropriate knowledge from the KG using a coarse-to-fine sampling method with a\nnovel self-aware knowledge retrieval approach, and finally achieves mutual enhancement\nbetween the LLM and the KG. DALK also performs well in a specially constructed ADQA\nbenchmark, demonstrating its effectiveness within its domain of expertise.\n4. Applications in Medicine\nIn addition to the scientific research progress summarized above, medical LLMs that integrate\nclinical knowledge have been applied in various scenarios, including medical research, drug\nresearch and development (R&D), intelligent diagnosis and treatment (D&T), medical equipment\noperation and maintenance, and hospital management.\nThese applications can assist medical development in terms of diagnosis, medical imaging,\ndrug development and innovation, medical dialogue services, and personalized treatment plans,\n33\nwhich require knowledge of Chinese medicine, pharmaceutical molecules, biomedicine, and ge-\nnomics in multilingual contexts. To better understand medical LLM applications, we classify\nthem into two types based on different modalities: plain-text and multimodal.\n4.1. Industrial Text Medical LLMs\nTable A.10 presents successful cases of transitioning text-based medical LLMs from aca-\ndemic to industry:\nUniGPT-Med: UniGPT-Med, based on UniGPT, is specifically designed to understand med-\nical data and knowledge. It is trained on up to two trillion tokens, including a knowledge graph\ncontaining 1.6 million concepts, 3.7 million terms, and 8.4 million relationships. Unisound has\nsuccessfully developed a medical record generation and quality control system using this tech-\nnology. These related products have been implemented in hospitals to help medical professionals\ndiagnose diseases using AI, improve the e fficiency of outpatient medical record entry, and save\ntime while enhancing the accuracy of diagnosis and treatment.\nEyeGPT: EyeGPT, developed by Wenzhou International Optometry Innovation Centre for\nophthalmology, is pre-trained based on many natural and medical professional corpus and fine-\ntuned for medical scenarios using professional data such as ophthalmology EHR information.\nIts application is mainly used in scientific research and clinical medical assistance by integrating\nNLP, computer vision (CV), and other technologies to provide intelligent assistance tools for\nmedical professionals and patients with interactive questions and answers.\nDaJing TCM: DaJing TCM has released the QiHuangWenDao Model, which focuses on\nTCM and aims to achieve intelligent clinical diagnosis, treatment, and health regulation. Based\non the KG, diagnosis, and treatment knowledge, the model is trained using a four-layer progres-\nsive training approach of pre-training, supervised fine-tuning, rewarding model, and reinforce-\nment learning. Currently, 1,704,605 TCM experts have participated in project evaluation. Ad-\nditionally, DaJing TCM has released two sub-models: the Medical Model for clinical diagnosis\nand treatment and the Health Model for recommending TCM health and conditioning programs.\nDingDangKuaiYao: Dingdang Health has launched applied pharmaceutical AI products -\nDingdang Pharmacist and Dietitian AI Assistant - based on its self-developed HealthGPT. These\nAI products are integrated into the DingdangKuaiYao app. The company’s offline physical chain\nsmart pharmacies operate in 19 cities across the country, utilizing an integrated online and offline\noperations model. Through its three core businesses of Fast Medicine service, online health con-\nsultation, and chronic disease and health management, DingdangKuaiYao provides users with\nconvenient and beneficial health services such as 7×24 instant health support and online consul-\ntations with professional doctors and pharmacists.\nGushengtang: Gushengtang, in collaboration with Baidu Lingyi, has jointly developed the\nGushengtang TCM model to provide intelligent TCM services like diagnosis and treatment, and\nformula recommendations. This model integrates a wealth of clinical EHRs, case information,\nand KGs from renowned Chinese medical masters. The products derived from this model are\ndivided into the patient and doctor ends. The patient end provides various services such as disease\nconsultation, quick finding of doctors, intelligent department guidance, medication guidance,\nand intelligent customer service through a dialogue mode, while the doctor end mainly provides\ndigital empowerment for clinical diagnosis and academic research for doctors.\nXunfei Xiaoyi: The Xinghuo Medical Model is based on the Xinghuo Cognitive Model V3.0,\naiming to create an AI health assistant for everyone. It has billions of high-quality medical data,\nallowing the model to master professional medical knowledge further. Based on this, the Xunfei\n34\nXiaoyi APP and mini program have been released, providing services such as medical health\nknowledge inquiry and Q&A, medical strategy, medication inquiry, report interpretation, TCM\nsyndrome differentiation, and health record management. These services aim to assist users in\nimproving medical e fficiency and experience, allowing the inclusive light of AI healthcare to\nshine into every household and become everyone’s AI health assistant.\nZhiyun Health: Zhiyun Health, based on ClouD GPT, is mainly used in hospitals, Internet\nhospitals, and other scenarios. It has been implemented in the medical application scenarios of\nintelligent cloud AI-assisted diagnosis and AI drug and device research and development. In\nhospital product applications, ClouD GPT provides AI-assisted clinical diagnosis and treatment\nmodules. In pharmacy product applications, ClouD GPT supports doctors and pharmacists in\nonline diagnosis and treatment service management. Currently, Zhiyun Health has made rapid\nprogress in the research and development of AI-assisted medical devices. Product development\nfor diabetes, coronary heart disease, hyperlipidemia, and other areas will gradually complete\nclinical validation, providing core technical solutions for digital chronic disease management.\nWiNEX: WiNGPT is a medical vertical-oriented LLM developed by Wining Health. Based\non the general LLM, WiNGPT strengthens the model capability by obtaining high-quality train-\ning data through data engineering based on public and medical domain data. Based on WiNGPT,\nWiNEX Copilot Healthcare Intelligent Assistant has been developed to provide intelligent ser-\nvices for healthcare professionals in the areas of medical record document generation, medical\nquality supervision, risk monitoring and early warning, medical knowledge service, as well as\noptimization of diagnosis and treatment process and improvement of management e fficiency by\ncombining various kinds of patient’s diagnosis and treatment service data.\nTongyi Renxin: Tongyi Renxin is an AI product launched by Alibaba Cloud, which focuses\non the medical field. It is based on the Tongyi generative LLM and integrates massive medi-\ncal knowledge literature and medical data for training based on the total training data of Tongyi\nQwen of more than 3 trillion tokens. Targeting the knowledge-intensive and serious character-\nistics of the pharmaceutical industry, the model has stronger industry knowledge reasoning and\ncognitive abilities in the medical field, including the accuracy of knowledge question answer-\ning, the authority of single round consultation, scenario-based multi-turn consultation, domain\ntext generation, and domain literature understanding. At present, Tongyi Renxin is still in the\ninvitation testing stage.\n4.2. Industrial Multimodal Medical LLMs\nIn addition to text-based medical industrial LLM, the development of multimodal data has\nled to the emergence of multimodal industrial LLMs. Table B.11 presents successful cases of\nmultimodal medical LLM transitioning from academia to industry:\nStoneNeedle: The Stone Needle LLM is the first multimodal LLM released by AthenaEyes\nthat supports text, image, video, and audio inputs in the medical field and can provide an in-\nteractive experience that combines Chinese and Western medicine and integrates multimodality.\nAdopting the technical route of combining KG and LLM overcomes the pain point of low infor-\nmation accuracy of LLM technology in the medical field. At the same time, it can realize the\nmultimodal processing of the text data of consultation, medical image data, video data of user’s\nfacial signs, and audio data of user’s sleep to realize the diversified tasks such as medical auxil-\niary diagnosis, intelligent cognition, and health management. This revolutionizes the traditional\nsingle-task assisted diagnosis model in the healthcare industry.\nTencent MedLLM: Tencent MedLLM takes Tencent’s newly released all-link self-research\nHunyuan model as a base and continues to add medical KGs and medical literature covering 2.85\n35\nmillion medical entities, 12.5 million medical relationships, and 98% of medical knowledge, to\nmake the LLM further master professional medical knowledge. After 30 million Q&A conver-\nsations covering patients, doctors, pharmaceutical companies, and other scenarios and medical\nprocesses for multi-task fine-tuning, as well as 360,000 sets of expert doctor-labeled data for\nreinforcement learning, it makes Tencent MedLLM more professional and precise in handling\nmedical needs, while also taking into account patient care and getting closer to human doctors.\nIt includes scenarios such as text generation, intelligent Q&A, medical record structuring and\nretrieval, image reporting, and assisted diagnosis, and these all can be embedded in the whole\nprocess of medical sessions to achieve a comprehensive improvement in the level and quality of\nmedical services.\nPanGu: Based on the Pangu-Drug and relying on the Huawei Cloud Medical Intelligence\nBody EIHealth platform, the PanGu model has been trained with a super-large-scale model of\ncompound characterization, and the chemical structures of 1.7 billion drug molecules have been\npre-learned. It is mainly oriented to drug discovery and development, providing binding predic-\ntion, property prediction, molecular optimization, and generation capabilities. Pangu-Drug has a\n20% higher accuracy of drug-ability prediction than the traditional way, which helps researchers\nsave a lot of the cost of drug design. In addition, the model has a built-in e fficient molecular\ngenerator to generate a screening library of 100 million innovative drug-like small molecules\nwith 99.68% structural novelty, creating more possibilities for discovering new drugs.\nMedical Sense: Medical Sense, based on the large-scale language model ”SenseChat” with\n100 billion parameters developed by Shangtang, is trained with high-quality medical knowledge\ndata of more than 20 billion tokens, which covers medical textbooks, medical guides, clinical\npaths, as well as 40 million real medical records, Q&A, and dialogues between doctors and pa-\ntients. Medical Sense focuses on four major application areas: intelligent health, intelligent pa-\ntient service, intelligent clinic, and digital intelligence construction. It has covered 13 segmented\nmedical and healthcare scenarios such as intelligent self-diagnosis, medical checkup consulta-\ntion, and health Q&A. Its goal is to accurately match model functions with specific scenarios and\npromote the digital transformation of the entire healthcare industry chain.\nMedLinker: The MedLinker app is focused on real medical scenarios, using the MedGPT\nmodel with a 100 billion parameter scale. It is based on over 2 billion pieces of medical text\ndata in the pre-training phase and 8 million high-quality structured clinical diagnosis and treat-\nment data in the fine-tuning training phase. Over 100 doctors are involved in providing manual\nfeedback to supervise the fine-tuning training. Currently, MedLinker can cover 60% of ICD10\ndisease categories. Additionally, it overcomes the challenge of AI doctors not being able to\nengage in continuous free conversations with real patients for the first time. It supports multi-\nmodal input and output in medical consultation scenarios, fully realizing intelligence in disease\nprevention, diagnosis, treatment, and rehabilitation.\nWeiMai: WeiMai and WeiMai-Doctor, based on Weimai’s self-developed health manage-\nment LLM model CareGPT, are mainly dedicated to giving full play to the value of health\nmanagement in real healthcare service scenarios and realizing the full-cycle intelligent health\nmanagement capabilities of prevention, consultation, appointment, and recovery. CareGPT com-\nbines a series of engineering tuning and full-course management technologies with a current\nparameter scale of 7 billion and can support medical health scenarios with multimodal inputs\nand outputs. Currently, WeiMai Total Disease Management covers 32 departments and more\nthan 1,000 disease types, serving nearly 1 billion people.\n01Bot: 01Bot, based on Baidu Wenxin, uses training corpus data of hundreds of billions\nof tokens, including massive clinical desensitization data, massive medical KGs, multimodal\n36\nimage data, health science contents, and clinical trial research information. It integrates the\nintelligent medical service experience of more than 800 hospitals and 4,000 grassroots clinics\nnationwide. It focuses on three major directions: intelligent health manager, intelligent physi-\ncian assistant, and intelligent enterprise service, to satisfy the specific needs of ”doctors, patients,\nand medicines.” Intelligent Health Manager provides intelligent guidance, pre-questioning, and\nhealth consultation to provide patients with medical advice and guidance. Intelligent physician\nassistant provides services for physicians in terms of assisted diagnosis, medical record genera-\ntion, and quick literature review. Intelligent enterprise services provide capabilities for enterprise\ncustomers, from operation assistance, vocational training, and knowledge services, helping en-\nterprises quickly promote new drugs after market launch.\n4.3. Academic vs Industrial\nThe anchor models used in practical applications involve analyzing large datasets during\nthe training process, which cover both public and non-public medical data. Moreover, many\nmedical experts must participate in manual feedback supervision and fine-tuning training during\nthe training process. Finally, to ensure the practicality and effectiveness of the model, a series of\nstrict evaluation and validation measures need to be carried out before the model is applied.\nObviously, this is significantly di fferent from academic medical LLMs in terms of training\ndata, training process, and model evaluation. Subsequently, in the following section, we will\ncompare industrial and academic medical LLM in more detail regarding research objectives,\ndata resources, training methods, and application scenarios, as shown in Table 9.\nBased on the differences between academic medical LLMs and industrial medical LLMs, we\nsummarize the potential issues of implementing academic models into real-life applications in\nthe following aspects:\n• Limitations of Data Resources:Academic research often utilizes publicly available datasets,\nwhile industry research relies more on private data resources that o ffer a wider range of\nreal-world application scenarios and diverse cases. However, industry data may contain\nsensitive information that needs to be desensitized, potentially impacting the effectiveness\nof model training. Therefore, academic research may need to adjust to data quality and\nscale changes when transitioning to industry.\n• Balance of Model Performance and Practicality: In academia, research models may\nprioritize theoretical innovation and performance improvement, whereas in industry, mod-\nels must be stable and highly interpretable. As a result, industry application scenarios\noften require models with stronger generalization ability and robustness to handle various\nuncertainties in practical operations.\n• Clinical and Regulatory Compliance: The healthcare industry requires models to meet\nstrict clinical and ethical standards. Therefore, academic research results must undergo a\nrigorous clinical trial and regulatory approval when translated into industrial products.\n• Difficulty of Interdisciplinary Cooperation: Differences in culture and working prac-\ntices between academia and industry may impact interdisciplinary collaboration. There-\nfore, academic researchers may require a deeper understanding of the industry’s needs and\ntechnical constraints, while industrial personnel may need to adapt to academic research\nmethodologies.\n37\nTable 9: Academic Medical LLMs versus Industrial Medical LLMs. We compare and analyze them with research\nobjectives, data resources, training & optimization, performance metrics, application scenarios, data privacy & security,\ncollaboration & open source, and updates & iterations.\nAcademic Medical LLMs Industrial Medical LLMs\nResearch Objectives Research and innovation are often\npursued to achieve theoretical break-\nthroughs and technological advance-\nments.\nWhile commercial interests may not be\nthe primary driver, the ultimate goal is\nto solve practical problems and create\ncommercial value in response to mar-\nket demand.\nData Resources Publicly available datasets are fre-\nquently used, or data is obtained\nthrough research collaborations with\nlimited data volumes.\nThe organization utilizes a signifi-\ncant amount of data, including patient\nrecords and medical images, which are\nboth diverse and voluminous.\nTraining & Optimization Focuses on innovations in model struc-\nture and algorithms, as well as theoret-\nical performance improvements.\nFocuses more on the practical applica-\ntion performance of the model, such\nas deployment efficiency, stability, and\nsecurity.\nPerformance Metrics Accuracy, Recall, F1 score, etc. More diversified, including actual ap-\nplication e ffects, user feedback, busi-\nness metrics, etc.\nApplication Scenarios Demonstrate research results through\nacademic research, published papers,\nand algorithmic competitions.\nFocuses on the effectiveness of models\nin real clinical applications as well as\ncommercial solutions.\nData Privacy & Security Often adhere to data privacy and secu-\nrity regulations, but may not be as strict\ncompared to the industry.\nMust adhere to strict healthcare data\nprivacy and security regulations.\nCollaboration &Open Source Prefers academic collaboration, paper\npublication, and open source code.\nPrefers commercial collaboration and\nproductization, technology licensing,\nmay not be fully open source.\nUpdates & Iterations Relatively slow update frequency,\nmainly paper publication.\nIterate quickly to adapt to market needs\nand business changes.\n• Technology Transfer and Landing Challenges: When applying academic research to\npractical industrial applications, it is often necessary to address specific issues such as\nhardware compatibility and computational resource limitations. Additionally, the indus-\ntry may need to modify and engineer academic research findings before they can operate\nreliably in real clinical environments.\n5. Evaluation System\nIn the fast-paced world of technology, evaluating models has become crucial for verifying\ntheir validity and reliability. A robust evaluation framework not only measures a model’s perfor-\nmance but also guides its ongoing improvement and progression. This subsection will thoroughly\nexamine the construction of the evaluation framework, covering its guiding principles, evaluation\ntechniques, and real-world application scenarios to provide researchers with a comprehensive and\nup-to-date perspective.\n5.1. Assessment Principles\nThe cornerstone of the evaluation system is a series of core principles that together ensure\nthe assessment process’s fairness, accuracy, and effectiveness. Let us take a closer look at these\nkey principles:\n38\n• Accuracy: The overarching principle of model assessment is accuracy, which refers to\nhow closely the model predicts or generates results compared to the true or expected val-\nues. When evaluating mathematical questions, accuracy is determined by whether the\nmodel can correctly answer the questions. For language models, accuracy is reflected in\nthe correctness and reasonableness of the content generated by the model.\n• Robustness: The model can maintain efficient performance when faced with different data\ninputs, ensuring accuracy even in the presence of noise, missing data, or anomalies.\n• Generalization: The model can maintain good performance when faced with unseen data,\nindicating a true understanding of the problem rather than just memorizing the training\ndata.\n• Interpretability: The workings and predictions of the model can be explained and under-\nstood, which is especially critical in scenarios involving important decisions.\n• Efficiency: The time and resources consumed by the model in processing the task. In\npractice, efficient models can respond to requests faster and save computational resources.\n• Security: Guaranteeing that the model is not maliciously exploited, ensuring data confi-\ndentiality during processing, and preventing the model’s predictions from leading to unde-\nsirable consequences.\nIn addition to the above, other aspects, such as simplicity and novelty, can also be evaluated\nfor the preferences of different models.\n5.2. Assessment Methodology\nSelecting appropriate assessment methods is essential to ensuring the quality and reliability\nof model evaluation results. When evaluating model performance, comprehensive considerations\nshould be made to select assessment methods and metrics accurately reflecting the model’s actual\nperformance. This section will examine two primary assessment methods: automatic evaluation\nand human evaluation.\n5.2.1. Automatic Evaluation\nAutomated quantitative metrics are used to assess model performance and applicability. This\napproach relies on algorithms and computational tools to quickly and objectively evaluate model\nperformance. Its advantages are processing large amounts of data, reducing human bias, and\nallowing real-time updating of assessment results. Commonly automatic evaluation metrics such\nas accuracy, recall, and F1 score. They can reflect the model’s predictive ability from di fferent\nperspectives and provide strong support for quantitative analysis of the model. Here are some\ncommonly automatic evaluation metrics:\n• Accuracy refers to the proportion of samples the model correctly predicts out of the total\nnumber of samples. The equation is as follows:\nAccuracy = T P+ T N\nT P+ T N+ FP + FN (1)\nwhere TP stands for true positive, which means predicted positive and actual positive. FP\nstands for false positive, which means predicted positive but actual negative. FN stands for\nfalse negative, which means predicted negative but actual positive.\n39\n• Precision refers to the proportion of truly positive samples among those the model predicts\nas positive. The equation is as follows:\nPrecision = T P\nT P+ FP (2)\nwhere TP and FP have the same meaning as described in Equation 1.\n• Recall refers to the proportion of samples correctly predicted as positive by the model out\nof all the actual positive samples. The equation is as follows:\nRecall = T P\nT P+ FN (3)\nwhere TP and FN have the same meaning as described in Equation 1.\n• F1 score is the harmonic mean of precision and recall, providing a comprehensive reflec-\ntion of the model’s accuracy and robustness. The equation is as follows:\nF1score = 2 ×Precision ×Recall\nPrecision + Recall (4)\nwhere Precision and Recall are described above.\n• Micro-F1&Macro-F1 are F1 scores computed in macro-averaging and micro-averaging\napproaches, respectively, and are suitable for classification tasks. Micro-f1 aggregates\nthe prediction results of all classes to compute global precision and recall. Macro-F1\ncalculates the F1 score for each class separately and then takes the average of all classes.\nIn contrast, the latter is more suitable for multi-classification tasks.\n• BLEU [105] usually evaluates the quality of machine translation. It scores by comparing\nthe overlap between the machine-generated translations and human translations.\n• ROUGE-L [106] assesses the performance of automatic summarization and machine trans-\nlation. It calculates the longest common subsequence between the predicted and reference\ntext.\n• METEOR [107] also evaluates the quality of machine translation. It combines precision\nand recall, considering word matches and word order between the candidate and reference\ntranslation.\n• BERTScore [108], based on the pre-trained language model BERT, evaluates the quality\nof text generation tasks such as machine translation and text summarization.\nThe above evaluation metrics, while commonly applied to a wide range of fields, their appli-\ncation in medicine needs to be appropriately adjusted to adapt to the uniqueness and complexity\nof medical tasks. For example, the medical concepts-based assessment metric, MEDCON [109],\nis used to measure the accuracy and consistency of clinical concepts. This metric calculates the\nF1 score to determine the similarity between the UMLS concept set in the candidate and refer-\nence clinical notes. There is also the TCMScore metric [110] for assessing TCM semantic and\nknowledge coherence, which combines the matching of TCM terms and semantic consistency\nbetween the generated and standard analysis. In this case, term matching calculates by adding\nterm diversity to the original F1 score calculated from precision and recall, thus evolving into a\nTerm F1 Score.\n40\n5.2.2. Human Evaluation\nModel’s qualitative analysis and evaluation outputs by professionals. Compared with auto-\nmatic evaluation, human evaluation focuses on subjectively evaluating the model outputs using\nthe knowledge and experience of the experts, especially where the model is in areas that are dif-\nficult to capture in automatic evaluation, such as the interpretability and robustness of the model.\nIn addition, this approach also focuses on the utility and user acceptance of the model. Human\nevaluation typically includes user testing, expert reviews, and case studies, which can provide\ninsights into the qualitative analysis of the model.\nCurrently, medical academic LLMs emphasize the model’s performance in response to spe-\ncific medical scenarios, such as medical exams and QA. These tasks are often derived from\nreal medical data and evaluated for accuracy using an automatic matching method. Common\nevaluation datasets include MultiMedQA, MultiMedBench, CMB [111], PromptCBLUE [112],\nRJUA-QA Datasets [113], EHRNoteQA [114], Aci-bench [109] and TCM-ED [110]. While\nthese evaluation datasets provide diverse medical information, including images, text, and clini-\ncal Q&A, and some also introduce human evaluation frameworks to improve data accuracy, they\nhave some limitations. For example, MultiMedQA has limited labeled data based on diagnostic\nreports and can only accurately assess a limited number of tasks. MultiMedBench has a limited\namount of data in the transcriptomics and proteomics domains. CMB, PromptCBLUE, RJUA-\nQA Datasets, and TCM-ED still have limitations in terms of data diversity, and the EHRNoteQA\nand Aci-bench are insu fficient in task diversity. They all have room for improvement in data\nscale or disease coverage.\n5.3. Summary and Discussion\nWe systematically summarize the data distribution of academic LLMs across different med-\nical capabilities (see Figures 7 and 8 for details) and conduct a comparative analysis of these\nmodels’ performance in medical capabilities (see Appendix C and Appendix D for details).\nOur research finds that there are significant differences between models when assessing the same\nmedical capabilities, mainly in the following aspects:\n• Dataset Inconsistency. Some models are evaluated using different datasets, making direct\ncomparisons difficult.\n• Variability in Evaluation Methods and Metrics. Most models employ automated eval-\nuation methods. However, the evaluation metrics were not entirely uniform, even within\nthe same dataset.\n• Limitations of Human Evaluation. A few models incorporate human evaluations, but\ntheir evaluation systems vary, hindering unified assessments across models.\nThese issues limit fair and systematic comparisons between models with the same capabilities\nand affect the generalizability and reproducibility of the research findings.\nAlthough automatic evaluation methods can provide an initial performance quantification,\ntheir limitation lies in the difficulty in capturing the complexities and nuances of medical clinical\npractice. In contrast, human evaluation enables medical experts to analyze model outputs in\ndepth, especially in key areas such as diagnostic recommendations, treatment recommendations,\nand patient management strategies. Its importance is not only for its ability to identify details\nmissed by automatic evaluation but also for validating the clinical adaptability of the model’s\ndecision-making process. Furthermore, human evaluation helps to identify and correct model\n41\n0 20 40 60 80 100Percentage\nMedical Professional \nKnowledge Mastery\nMedical Content \nGeneration Ability\nMedical Q&A Ability\nMedical Information \nExtraction Ability\nMedical Dialogue Ability\nCMExam\nCMB-Exam\nMedQA\nMedMCQA\nC-Eval\nDialogSumm\nPubMedQA\nChiMed\nCMMLU\nCCKS-2019\nChiMST\nBC5CDR\nMedDialog\nHuatuo-26M\nFigure 7: Data distribution of different capabilities of text medical LLMs.\n0 20 40 80 100Percentage 60\nMedical Q&A Ability\nMedical Report \nSummarization Ability\nMedical Visual Question \nAnswering Ability\nMedical Report Generation \nAbility\nMedical Image Classification \nAbility\nMedQA \nMedMCQA \nPubMedQA \nChEBI-20 \nMIMIC-III \nVQA-RAD \nSlake-VQA \nPath-VQA \nMIMIC-CXR  \nPAD-UFES-20 \nCBIS-DDSM\nFigure 8: Data distribution of different capabilities of multimodal medical LLMs.\nbiases, enhancing the fairness and ethicality of the model, while feedback from medical experts\nis crucial for the continuous training and optimization of the model.\nGiven the specificities of the medical field, we recommend a combined approach of both\nautomatic and human evaluations when assessing models. Integrating the e fficiency of auto-\nmatic evaluation with the depth of human evaluation, we can create a comprehensive assessment\nframework. This framework enhances the thoroughness and depth of model evaluations while\nensuring the reliability and effectiveness of models in practical applications. Specifically, future\nresearch should focus on the following points:\n1. Establish Unified Evaluation Benchmarks and Datasets. This provides a foundation\nfor horizontal comparisons between different models.\n2. Clarify Evaluation Metrics and Methods. This will improve the comparability and con-\nsistency of evaluation results.\n3. Develop a Standardized Human Evaluation System.This aims to reduce subjective bias\nin the evaluation process.\n42\nThrough comprehensive evaluations, we can gain a more holistic understanding of model\nperformance, identify their strengths and limitations, and ultimately provide better services to\npatients while guiding further development and application of the models. This approach pro-\nmotes the application of LLMs in the medical field and enhances the transparency and credibility\nof research, thereby adding value to medical research and practice.\n6. Challenges and Future Work\n6.1. Challenges\nMedical LLMs are gaining academic researcher’s attention due to their advanced language\nunderstanding and generation capabilities. The development of academic medical AI systems\nhas enabled models to process and generate complex medical texts, making them useful for tasks\nsuch as medical QA, medical dialogue, medical text generation, and medical knowledge graph\nconstruction.\nHowever, despite the exciting research results of medical LLMs, there are still challenges in\ntheir academic research and practical implementation. As far as academic research is concerned,\nthere are many challenges in the whole process, from data to models and evaluation.\n• Clinical Databases and Datasets: The quantity, quality, and accuracy of existing medical\nknowledge bases and training datasets need to be expanded and improved due to medical\ndata’s high complexity and diversity. Furthermore, multi-language support is still lacking.\nAdditionally, the disease knowledge in these knowledge bases and datasets may not be\ncomprehensive enough. For instance, the information on some rare or endemic diseases is\nnot detailed enough, and the latest research results and treatments are not updated in time.\n• Model Construction: Although LLM model construction based on medical text corpora\nor multimodal data shows excellent performance in some or more medical tasks, it still\nfaces several problems. These include data annotation quality, uneven data distribution,\nlack of model interpretability and transparency, and limitations in multimodal information\nfusion.\n• Assessment Systems: During the assessment of model performance, most methods rely\non automated medical task evaluation metrics. They generally focus on specific medical\ntasks, such as medical examinations and QA. However, relatively few methods incorporate\nmanual assessment. In particular, many evaluation datasets are deficient in data size or\ndisease coverage. This can compromise the overall assessment of model performance.\n• Collaborative knowledge graphs: Combining KG with LLM can enhance the model’s\nNLP abilities while ensuring the medical field’s interpretability, trustworthiness, and trace-\nability. However, challenges such as catastrophic forgetting and erroneous knowledge\nediting when updating LLM knowledge with knowledge graphs or LLM’s illusions still\nneed to be addressed. Furthermore, additional research is required to investigate e ffective\ntechniques for integrating knowledge into LLM using KGs.\nDespite these technical concerns for developing academic medical LLMs, we still need to\ntackle the following endeavors while putting these models into real-life applications:\n43\n• Evaluations: As a common evaluation strategy for evaluating model performances, accu-\nracy usually refers to the rate of correct predictions by a model. In clinical applications,\ndoctors often focus on the number of specific information that a model can correctly iden-\ntify or provide. In this case, even if a model has a high accuracy rate, it may still fail\nto satisfy the clinical decision requirements such as drug dosage and treatment duration.\nThese high-quality models only have a higher probability of producing correct answers,\nwhich will increase the skepticism. Therefore, more practical evaluation metrics with bet-\nter reflections of model capacities in real-life practice are needed.\n• Diversity: Academic research findings may only be applicable under specific conditions,\nwhile practical applications must consider a wider patient population, different etiologies,\nand comorbidities.\n• Interpretability: Additionally, models must be interpretable, traceable, and evidence-\nbased to gain the trust of both doctors and patients and to be e ffectively applied in real\nclinical environments for the convenience of doctors and patients.\nIn addition to the challenges mentioned above, data privacy, data security, and the corre-\nsponding regulatory and ethical issues are equally important for medical LLM practices [115].\nThese concerns must be strictly adhered to, whether in academic research or practical applica-\ntion.\n6.2. Future Work\nTo address these challenges, further investigations can consider the following paths:\n1. In academic research, improving the medical knowledge base [116, 117] can enhance the\naccuracy of the models. Similarly, larger and more diverse evaluation datasets [118] can\nhelp in the multi-dimensional assessment of models and validate their clinical applicability.\n2. Increasing the transparency [119] of the models can improve their interpretability [120].\nAdditionally, closely integrating the models with clinical practice can ensure their trace-\nability and evidence-based transferability [91, 121].\nWith the accumulation of medical data and optimization of algorithms, medical LLMs are\nexpected to perform even better in real-world applications. This will provide doctors with more\naccurate decision-making assistance and patients with higher-quality medical services.\nOther than technical issues, mitigating the doctor-patient relationship and providing human-\nistic care can also be integrated into academic medical LLMs research:\n1. Optimising Patients’ Medical Experience: By analyzing patients’ behavioral patterns\nand combining them with the construction of a doctor-patient trust model, we can predict\nand enhance trust between doctors and patients through LLMs. This will optimize the\nmedical service process and reduce patients’ waiting time. Furthermore, we can o ffer pa-\ntients customized and accurate medical services, along with personalized health education\nand programs for managing their conditions. These improvements will enhance patient\nsatisfaction and treatment compliance.\n2. Medical Resources and Service Improvement: We can utilize LLMs for the optimal\nallocation of healthcare resources to ensure that resources are fairly and reasonably dis-\ntributed across various regions and groups. Meanwhile, we can use the LLMs to analyze\n44\nthe causes and patterns of medical disputes, propose preventive and handling strategies, re-\nduce doctor-patient conflicts, ensure medical safety, and enhance the quality of healthcare\nservices.\n3. Patient Psychological Support and Cross-Cultural Communication: We can employ\nLLMs to provide patients with mental health assessments and support, identify psycho-\nlogical issues, and o ffer psychological therapies and interventions [122]. Additionally,\nwe can use the LLMs to achieve cross-cultural doctor-patient communication, providing\nmore considerate and understanding medical services for patients from diverse cultural\nbackgrounds [123]. This will reduce misunderstandings and conflicts caused by cultural\ndifferences, thereby enhancing patients’ mental health and overall healthcare experience.\n7. Conculsion\nClinical knowledge about the causes, prognosis, diagnosis, and treatment of diseases can\nimprove curing performances, and promote physical health. The recent development of LLMs\nopens new possibilities in the field of medical AI. In this survey, we gather the building paradigms\nof medical AI systems including the use of clinical databases, datasets, training pipelines, evalu-\nation systems, as well as methods integrating medical KGs.\nFinally, we present the differences between academic medical LLMs and industrial ones and\nsummarize the challenges to implementing academic LLMs in real-life medical situations. Based\non this analysis, we present some of the future directions for academic research and applications.\nWe hope that our survey presents an overview of the field of medical AI and its applications as\nwell as challenges and future works for these technologies.\n45\nReferences\n[1] J. Egger, C. Gsaxner, A. Pepe, K. L. Pomykala, F. Jonske, M. Kurz, J. Li, J. Kleesiek, Medical deep learning -\nA systematic meta-review, Computer Methods and Programs in Biomedicine 221 (2022) 106874. URL: https:\n//doi.org/10.1016/j.cmpb.2022.106874.\n[2] A. Anaya-Isaza, L. Mera-Jim ´enez, M. Zequera-Diaz, An overview of deep learning in medical imaging, Infor-\nmatics in Medicine Unlocked 26 (2021) 100723. URL: https://doi.org/10.1016/j.imu.2021.100723.\n[3] F. Shenavarmasouleh, F. G. Mohammadi, K. M. Rasheed, H. R. Arabnia, Deep learning in healthcare: An in-depth\nanalysis, CoRR abs/2302.10904 (2023). URL: https://doi.org/10.48550/arXiv.2302.10904.\n[4] E. Rajabi, S. Kafaie, Knowledge graphs and explainable AI in healthcare, Inf. 13 (2022) 459. URL: https:\n//doi.org/10.3390/info13100459.\n[5] L. Li, P. Wang, J. Yan, Y . Wang, S. Li, J. Jiang, Z. Sun, B. Tang, T. Chang, S. Wang, Y . Liu, Real-world\ndata medical knowledge graph: construction and applications, Artif. Intell. Medicine 103 (2020) 101817. URL:\nhttps://doi.org/10.1016/j.artmed.2020.101817.\n[6] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez, T. F. Tan, D. S. W. Ting, Large language models\nin medicine, Nature Medicine 29 (2023) 1930–40. URL:https://doi.org/10.1038/s41591-023-02448-8 .\n[7] M. Wornow, Y . Xu, R. Thapa, B. S. Patel, E. Steinberg, S. L. Fleming, et al., The shaky foundations of large\nlanguage models and foundation models for electronic health records, NPJ Digital Medicine 6 (2023) 135. URL:\nhttps://doi.org/10.1038/s41746-023-00879-8 .\n[8] C. W. Safranek, A. E. Sidamon-Eristo ff, A. Gilson, D. Chartash, The role of large language models in medical\neducation: applications and implications, JMIR Medical Education 9 (2023) e50945. URL: https://doi.org/\n10.1038/s41746-023-00879-8 .\n[9] H. Zhou, B. Gu, X. Zou, Y . Li, S. S. Chen, P. Zhou, et al., A survey of large language models in medicine: Progress,\napplication, and challenge, CoRR abs/2311.05112 (2023). URL: https://doi.org/10.48550/arXiv.2311.\n05112.\n[10] K. He, R. Mao, Q. Lin, Y . Ruan, X. Lan, M. Feng, et al., A survey of large language models for healthcare: from\ndata, technology, and applications to accountability and ethics, CoRR abs /2310.05694 (2023). URL: https:\n//doi.org/10.48550/arXiv.2310.05694.\n[11] Z. Ali, Y . Huang, I. Ullah, J. Feng, C. Deng, N. Thierry, et al., Deep learning for medication recommendation: A\nsystematic survey, Data Intelligence 5 (2023) 303–54. URL: https://doi.org/10.1162/dint_a_00197.\n[12] Y . Gao, R. Li, J. R. Caskey, D. Dligach, T. A. Miller, M. M. Churpek, M. Afshar, Leveraging A medical knowledge\ngraph into large language models for diagnosis prediction, CoRR abs /2308.14321 (2023). URL: https://doi.\norg/10.48550/arXiv.2308.14321.\n[13] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, et al., Training language models to follow\ninstructions with human feedback, Advances in Neural Information Processing Systems 35 (2022) 27730–44.\nURL: https://doi.org/10.48550/arXiv.2203.02155.\n[14] Y . Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu, et al., Domain-specific language model pretraining for\nbiomedical natural language processing, ACM Transactions on Computing for Healthcare (HEALTH) 3 (2022)\n1–23. URL: https://doi.org/10.1145/3458754.\n[15] G. Zeng, W. Yang, Z. Ju, Y . Yang, S. Wang, R. Zhang, et al., Meddialog: Large-scale medical dialogue datasets,\nin: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, Association for\nComputational Linguistics, Online, 2020 Nov 16-20, pp. 9241–50. URL: https://doi.org/10.18653/v1/\n2020.emnlp-main.743.\n[16] W. Gao, Z. Deng, Z. Niu, F. Rong, C. Chen, Z. Gong, et al., Ophglm: Training an ophthalmology large language-\nand-vision assistant based on instructions and dialogue, CoRR abs /2306.12174 (2023). URL: https://doi.\norg/10.48550/arXiv.2306.12174.\n[17] X. Yang, A. Chen, N. M. Pournejatian, H. C. Shin, K. E. Smith, C. Parisien, et al., A large language\nmodel for electronic health records, NPJ Digital Medicine 5 (2022) 194. URL: https://doi.org/10.1038/\ns41746-022-00742-2 .\n[18] Q. Ye, J. Liu, D. Chong, P. Zhou, Y . Hua, A. Liu, Qilin-med: Multi-stage knowledge injection advanced medi-\ncal large language model, CoRR abs /2310.09089 (2023). URL: https://doi.org/10.48550/arXiv.2310.\n09089.\n[19] Z. Chen, A. Hern ´andez-Cano, A. Romanou, A. Bonnet, K. Matoba, F. Salvi, M. Pagliardini, S. Fan, A. K ¨opf,\nA. Mohtashami, A. Sallinen, A. Sakhaeirad, V . Swamy, I. Krawczuk, D. Bayazit, A. Marmet, S. Montariol, M.-A.\nHartley, M. Jaggi, A. Bosselut, Meditron-70b: Scaling medical pretraining for large language models, 2023. URL:\nhttps://github.com/epfLLM/meditron.\n[20] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, et al., The pile: An 800gb dataset of diverse text\nfor language modeling, CoRR abs/2101.00027 (2021). URL: https://arxiv.org/abs/2101.00027.\n[21] E. Bolton, A. Venigalla, M. Yasunaga, D. Hall, B. Xiong, T. Lee, et al., Biomedlm: A 2.7b parameter language\n46\nmodel trained on biomedical text, CoRR abs/2403.18421 (2022). URL: https://doi.org/10.48550/arXiv.\n2403.18421.\n[22] M. Moor, Q. Huang, S. Wu, M. Yasunaga, C. Zakka, Y . Dalmia, et al., Med-flamingo: A multimodal medical\nfew-shot learner, in: S. Hegselmann, A. Parziale, D. Shanmugam, S. Tang, M. N. Asiedu, S. Chang, et al. (Eds.),\nProceedings of the Machine Learning for Health (ML4H), PMLR, New Orleans, Louisiana, USA, 2023 Dec 10,\npp. 353–67. URL: https://doi.org/10.48550/arXiv.2307.15189.\n[23] W. Lin, Z. Zhao, X. Zhang, C. Wu, Y . Zhang, Y . Wang, et al., Pmc-clip: Contrastive language-image pre-training\nusing biomedical documents, in: H. Greenspan, A. Madabhushi, P. Mousavi, S. Salcudean, J. Duncan, T. F.\nSyeda-Mahmood, et al. (Eds.), Proceedings of the International Conference on Medical Image Computing and\nComputer-Assisted Intervention, Springer, Vancouver, BC, Canada, 2023 Oct 8-12, pp. 525–36. URL: https:\n//doi.org/10.1007/978-3-031-43993-3_51 .\n[24] A. E. W. Johnson, T. J. Pollard, S. J. Berkowitz, N. R. Greenbaum, M. P. Lungren, C. ying Deng, et al., Mimic-\ncxr: A large publicly available database of labeled chest radiographs, CoRR abs /1901.07042 (2019). URL:\nhttp://arxiv.org/abs/1901.07042.\n[25] Z. Wang, Z. Wu, D. Agarwal, J. Sun, Medclip: Contrastive learning from unpaired medical images and text, in:\nY . Goldberg, Z. Kozareva, Y . Zhang (Eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), Association for Computational Linguistics, Abu Dhabi, United Arab Emirates,\n2022 Dec 7-11, pp. 3876–87. URL: https://doi.org/10.18653/v1/2022.emnlp-main.256.\n[26] A. E. W. Johnson, T. J. Pollard, L. Shen, L.-W. H. Lehman, M. Feng, M. Ghassemi, et al., Mimic-iii, a freely\naccessible critical care database, Scientific Data 3 (2016) 1–9. URL: https://doi.org/10.1038/sdata.\n2016.35.\n[27] E. Alsentzer, J. R. Murphy, W. Boag, W.-H. Weng, D. Jin, T. Naumann, et al., Publicly available clinical bert\nembeddings, CoRR abs/1904.03323 (2019). URL: http://arxiv.org/abs/1904.03323.\n[28] A. E. W. Johnson, L. Bulgarelli, L. Shen, A. Gayles, A. Shammout, S. Horng, T. J. Pollard, S. Hao, B. Moody,\nB. Gow, L. wei H. Lehman, L. A. Celi, R. G. Mark, Mimic-iv, a freely accessible electronic health record dataset,\nScientific data 10 (2023) 1. URL: https://doi.org/10.1038/s41597-022-01899-x .\n[29] A. E. W. Johnson, T. J. Pollard, N. R. Greenbaum, M. P. Lungren, C. ying Deng, Y . Peng, Z. Lu, R. G. Mark,\nS. J. Berkowitz, S. Horng, Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs (2019).\nURL: https://arxiv.org/abs/1901.07042.\n[30] A. Johnson, T. Pollard, S. Horng, L. A. Celi, R. Mark, Mimic-iv-note: Deidentified free-text clinical notes (2023).\nURL: https://doi.org/10.13026/1n74-ne17.\n[31] J. Fehr, Llama-care, a multimodal medical large language model for hospital discharge instruction generation\n(????). URL: https://jf-11.github.io/pages/llama_care.html.\n[32] J. Liu, P. Zhou, Y . Hua, D. Chong, Z. Tian, A. Liu, et al., Benchmarking large language models on cmexam -\na comprehensive chinese medical exam dataset, Advances in Neural Information Processing Systems 36 (2023)\n52430–52. URL: https://doi.org/10.48550/arXiv.2306.03030.\n[33] D. Jin, E. Pan, N. Oufattole, W.-H. Weng, H. Fang, P. Szolovits, What disease does this patient have? a large-\nscale open domain question answering dataset from medical exams, Applied Sciences 11 (2021) 6421. URL:\nhttps://doi.org/10.3390/app11146421.\n[34] Q. Jin, B. Dhingra, Z. Liu, W. W. Cohen, X. Lu, Pubmedqa: A dataset for biomedical research question an-\nswering, in: K. Inui, J. Jiang, V . Ng, X. Wan (Eds.), Proceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), Association for Computational Linguistics, Hong Kong, China, 2019 Nov 3-7, pp. 2567–77.\nURL: https://doi.org/10.18653/v1/D19-1259.\n[35] S. Zhang, X. Zhang, H. Wang, L. Guo, S. Liu, Multi-scale attentive interaction networks for chinese medical\nquestion answer selection, IEEE Access 6 (2018) 74061–71. URL: https://doi.org/10.1109/ACCESS.\n2018.2883637.\n[36] A. B. Abacha, D. Demner-Fushman, A question-entailment approach to question answering, BMC Bioinformatics\n20 (2019) 1–23. URL: https://doi.org/10.1186/s12859-019-3119-4 .\n[37] K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, et al., Large language models encode clinical\nknowledge, Nature 620 (2023) 172–80. URL: https://doi.org/10.1038/s41586-023-06291-2 .\n[38] X. He, Y . Zhang, L. Mou, E. P. Xing, P. Xie, Pathvqa: 30000 + questions for medical visual question answering,\nCoRR abs/2003.10286 (2020). URL: https://arxiv.org/abs/2003.10286.\n[39] J. J. Lau, S. Gayen, A. B. Abacha, D. Demner-Fushman, A dataset of clinically generated visual questions and\nanswers about radiology images, Scientific Data 5 (2018) 1–10. URL: https://doi.org/10.17605/OSF.IO/\n89KPS.\n[40] S. A. Hasan, Y . Ling, O. Farri, J. Liu, H. M ¨uller, M. P. Lungren, Overview of imageclef 2018 medical domain\nvisual question answering task, in: L. Cappellato, N. Ferro, J. Nie, L. Soulier (Eds.), Proceedings of the Confer-\nence and Labs of the Evaluation Forum (Working Notes), CEUR-WS.org, Avignon, France, 2018 Sept 10-14, pp.\n47\n1–8. URL: https://ceur-ws.org/Vol-2125/paper_212.pdf.\n[41] A. B. Abacha, S. A. Hasan, V . V . Datla, J. Liu, D. Demner-Fushman, H. M¨uller, Vqa-med: Overview of the medi-\ncal visual question answering task at imageclef 2019, in: L. Cappellato, N. Ferro, D. E. Losada, H. M¨uller (Eds.),\nProceedings of the Conference and Labs of the Evaluation Forum (Working Notes), CEUR-WS.org, Lugano,\nSwitzerland, 2019 Sept 9-12, pp. 1–11. URL: https://ceur-ws.org/Vol-2380/paper_272.pdf.\n[42] A. B. Abacha, V . V . Datla, S. A. Hasan, D. Demner-Fushman, H. M ¨uller, Overview of the vqa-med task at\nimageclef 2020: Visual question answering and generation in the medical domain, in: L. Cappellato, C. Eickhoff,\nN. Ferro, A. N ´ev´eol (Eds.), Proceedings of the Conference and Labs of the Evaluation Forum (Working Notes),\nCEUR-WS.org, Thessaloniki, Greece, 2020 Sept 22-25, pp. 1–10. URL: https://ceur-ws.org/Vol-2696/\npaper_106.pdf.\n[43] A. B. Abacha, M. Sarrouti, D. Demner-Fushman, S. A. Hasan, H. M ¨uller, Overview of the vqa-med task at\nimageclef 2021: Visual question answering and generation in the medical domain, in: G. Faggioli, N. Ferro,\nA. Joly, M. Maistro, F. Piroi (Eds.), Proceedings of the Conference and Labs of the Evaluation Forum (Work-\ning Notes), CEUR-WS.org, Bucharest Romania, 2021 Sept 21st, pp. 1–9. URL: https://ceur-ws.org/\nVol-2936/paper-87.pdf.\n[44] B. Liu, L. Zhan, L. Xu, L. Ma, Y . Yang, X. Wu, Slake: A semantically-labeled knowledge-enhanced dataset\nfor medical visual question answering, in: Proceedings of the 18th IEEE International Symposium on Biomed-\nical Imaging (ISBI), IEEE, Nice, France, 2021 Apr 13-16, pp. 1650–54. URL: https://doi.org/10.1109/\nISBI48211.2021.9434010.\n[45] S. Zhang, Y . Xu, N. Usuyama, J. Bagga, R. Tinn, S. Preston, et al., Large-scale domain-specific pretraining for\nbiomedical vision-language processing, CoRR abs /2303.00915 (2023). URL: https://doi.org/10.48550/\narXiv.2303.00915.\n[46] J. Liu, Z. Wang, Q. Ye, D. Chong, P. Zhou, Y . Hua, Qilin-med-vl: Towards chinese large vision-language\nmodel for general healthcare, CoRR abs /2310.17956 (2023). URL: https://doi.org/10.48550/arXiv.\n2310.17956.\n[47] T. Tu, S. Azizi, D. Driess, M. Schaekermann, M. Amin, P. Chang, et al., Towards generalist biomedical ai, NEJM\nAI 1 (2024) AIoa2300138. URL: https://ai.nejm.org/doi/abs/10.1056/AIoa2300138.\n[48] A. Pal, L. K. Umapathi, M. Sankarasubbu, Medmcqa: A large-scale multi-subject multi-choice dataset for medical\ndomain question answering, in: G. Flores, G. H. Chen, T. J. Pollard, J. C. Ho, T. Naumann (Eds.), Proceedings\nof the Conference on Health, Inference, and Learning (CHIL), PMLR, Virtual Event, 2022 Apr 7-8, pp. 248–60.\nURL: https://proceedings.mlr.press/v174/pal22a.html.\n[49] A. B. Abacha, E. Agichtein, Y . Pinter, D. Demner-Fushman, Overview of the medical question answering task\nat trec 2017 liveqa, in: E. M. V oorhees, A. Ellis (Eds.), Proceedings of The 26th Text REtrieval Conference\n(TREC), National Institute of Standards and Technology (NIST), Gaithersburg, Maryland, 2017 Nov 15-17, pp.\n1–12. URL: https://trec.nist.gov/pubs/trec26/papers/Overview-QA.pdf.\n[50] A. B. Abacha, Y . Mrabet, M. Sharp, T. R. Goodwin, S. E. Shooshan, D. Demner-Fushman, Bridging the gap\nbetween consumers’ medication questions and trusted answers, in: L. Ohno-Machado, B. S ´eroussi (Eds.), Pro-\nceedings of the 17th World Congress on Medical and Health Informatics (MedIinfo), IOS Press, Lyon, France,\n2019 Aug 25-30, pp. 25–29. URL: https://doi.org/10.3233/SHTI190176.\n[51] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep bidirectional transformers for language\nunderstanding, in: J. Burstein, C. Doran, T. Solorio (Eds.), Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies(NAACL-\nHLT), Association for Computational Linguistics, Minneapolis, MN, USA, 2019 Jun 2-7, pp. 4171–86. URL:\nhttps://doi.org/10.18653/v1/n19-1423.\n[52] Y . Wang, Y . Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, et al., Self-instruct: Aligning language mod-\nels with self-generated instructions, in: A. Rogers, J. L. Boyd-Graber, N. Okazaki (Eds.), Proceedings of the\n61st Annual Meeting of the Association for Computational Linguistics (ACL), Association for Computational\nLinguistics, Toronto, Canada, 2023 Jul 9-14, pp. 13484–508. URL: https://doi.org/10.18653/v1/2023.\nacl-long.754.\n[53] Y . Ji, Y . Deng, Y . Gong, Y . Peng, Q. Niu, L. Zhang, et al., Exploring the impact of instruction data scaling\non large language models: An empirical study on real-world use cases, CoRR abs /2303.14742 (2023). URL:\nhttps://doi.org/10.48550/arXiv.2303.14742.\n[54] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, Improving language understanding by generative pre-\ntraining, OpenAI Blog (2018).\n[55] Y . Li, Z. Li, K. Zhang, R. Dan, Y . Zhang, Chatdoctor: A medical chat model fine-tuned on a large language\nmodel meta-ai (llama) using medical domain knowledge, Cureus 15 (2023). URL: https://doi.org/10.\n7759/cureus.40895.\n[56] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y . Babaei, et al., Llama 2: Open foundation and fine-\ntuned chat models, CoRR abs/2307.09288 (2023). URL: https://doi.org/10.48550/arXiv.2307.09288.\n48\n[57] H. Wang, C. Liu, N. Xi, Z. Qiang, S. Zhao, B. Qin, et al., Huatuo: Tuning llama model with chinese medical\nknowledge, CoRR abs/2304.06975 (2023). URL: https://doi.org/10.48550/arXiv.2304.06975.\n[58] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, et al., Attention is all you need, in:\nI. Guyon, U. von Luxburg, S. Bengio, H. M. Wallach, R. Fergus, S. V . N. Vishwanathan, et al. (Eds.), Advances\nin Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems,\nMIT Press, Long Beach, CA, USA, 2017 Dec 4-9, pp. 5998–6008. URL: https://proceedings.neurips.\ncc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.\n[59] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, et al., Language models are few-shot\nlearners, in: H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, H. Lin (Eds.), Advances in Neural Information\nProcessing Systems 33: Annual Conference on Neural Information Processing Systems; NeurIPS 2020 Dec 6-12;\n:, MIT Press, Virtual Event, 2020, pp. 1877–901. URL: https://proceedings.neurips.cc/paper/2020/\nhash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\n[60] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, et al., Glm-130b: An open bilingual pre-trained model, in:\nProceedings of the 11th International Conference on Learning Representations (ICLR), OpenReview.net, Kigali,\nRwanda, 2023 May 1-5, pp. 1–56. URL: https://openreview.net/pdf?id=-Aw0rrrPUF.\n[61] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, et al., Bloom: A 176b-parameter open-access mul-\ntilingual language model, CoRR abs /2211.05100 (2022). URL: https://doi.org/10.48550/arXiv.2211.\n05100.\n[62] Y . Zhu, R. Kiros, R. S. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, et al., Aligning books and movies:\nTowards story-like visual explanations by watching movies and reading books, in: Proceedings of the 2015 IEEE\nInternational Conference on Computer Vision (ICCV), IEEE Computer Society, Santiago, Chile, 2015 Dec 7-13,\npp. 19–27. URL: https://doi.org/10.1109/ICCV.2015.11.\n[63] C. Peng, X. Yang, A. Chen, K. E. Smith, N. M. Pournejatian, A. B. Costa, et al., A study of generative large\nlanguage model for medical research and healthcare, NPJ Digital Medicine 6 (2023) 1–26. URL:https://doi.\norg/10.1038/s41746-023-00958-w .\n[64] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, Language models are unsupervised multitask\nlearners, OpenAI blog 1 (2019) 9.\n[65] N. Deng, H. Fu, X. Chen, Named entity recognition of traditional chinese medicine patents based on bilstm-\ncrf, Wireless Communications and Mobile Computing 2021 (2021) 1–12. URL: https://doi.org/10.1155/\n2021/6696205.\n[66] Z. Hu, Z. Ni, J. Shi, S. Xu, B. Xu, A knowledge-enhanced two-stage generative framework for medical dialogue\ninformation extraction, Machine Intelligence Research 21 (2024) 153–68. URL: https://doi.org/10.1007/\ns11633-023-1461-5 .\n[67] Y . Lu, Q. Liu, D. Dai, X. Xiao, H. Lin, X. Han, et al., Unified structure generation for universal information\nextraction, in: S. Muresan, P. Nakov, A. Villavicencio (Eds.), Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (ACL), Association for Computational Linguistics, Dublin, Ireland,\n2022 May 22-27, pp. 5755–72. URL: https://doi.org/10.18653/v1/2022.acl-long.395.\n[68] C. Wu, X. Zhang, Y . Zhang, Y . Wang, W. Xie, Pmc-llama: Further finetuning llama on medical papers, CoRR\nabs/2304.14454 (2023). URL: https://doi.org/10.48550/arXiv.2304.14454.\n[69] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, et al., Palm: Scaling language modeling\nwith pathways, Journal of Machine Learning Research 24 (2023) 1–113. URL: http://jmlr.org/papers/\nv24/22-1144.html.\n[70] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, et al., Towards expert-level medical question\nanswering with large language models, CoRR abs /2305.09617 (2023). URL: https://doi.org/10.48550/\narXiv.2305.09617.\n[71] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, et al., Palm 2 technical report, CoRR\nabs/2305.10403 (2023). URL: https://doi.org/10.48550/arXiv.2305.10403.\n[72] G. Wang, G. Yang, Z. Du, L. Fan, X. Li, Clinicalgpt: Large language models finetuned with diverse medical data\nand comprehensive evaluation, CoRR abs /2306.09968 (2023). URL: https://doi.org/10.48550/arXiv.\n2306.09968.\n[73] H. Zhang, J. Chen, F. Jiang, F. Yu, Z. Chen, G. Chen, et al., Huatuogpt, towards taming language model to be\na doctor, in: H. Bouamor, J. Pino, K. Bali (Eds.), Findings of the Association for Computational Linguistics\n(EMNLP), Association for Computational Linguistics, Singapore, 2023 Dec 6-10, pp. 10859–85. URL: https:\n//aclanthology.org/2023.findings-emnlp.725.\n[74] Y . Chen, Z. Wang, X. Xing, H. Zheng, Z. Xu, K. Fang, et al., Bianque: Balancing the questioning and suggestion\nability of health llms with multi-turn health conversations polished by chatgpt, CoRR abs /2310.15896 (2023).\nURL: https://doi.org/10.48550/arXiv.2310.15896.\n[75] H. Xiong, S. Wang, Y . Zhu, Z. Zhao, Y . Liu, L. Huang, et al., Doctorglm: Fine-tuning your chinese doctor is not\na herculean task, CoRR abs/2304.01097 (2023). URL: https://doi.org/10.48550/arXiv.2304.01097.\n49\n[76] Y . Tian, R. Gan, Y . Song, J. Zhang, Y . Zhang, Chimed-gpt: A chinese medical large language model with\nfull training regime and better alignment to human preferences, CoRR abs /2311.06025 (2023). URL: https:\n//doi.org/10.48550/arXiv.2311.06025.\n[77] R. Gan, Z. Wu, R. Sun, J. Lu, X. Wu, D. Zhang, et al., Ziya2: Data-centric learning is all llms need, CoRR\nabs/2311.03301 (2023). URL: https://doi.org/10.48550/arXiv.2311.03301.\n[78] Y . Luo, J. Zhang, S. Fan, K. Yang, Y . Wu, M. Qiao, et al., Biomedgpt: Open multimodal generative pre-trained\ntransformer for biomedicine, CoRR abs /2308.09442 (2023). URL: https://doi.org/10.48550/arXiv.\n2308.09442.\n[79] K. Lo, L. L. Wang, M. Neumann, R. Kinney, D. S. Weld, S2orc: The semantic scholar open research corpus,\nin: D. Jurafsky, J. Chai, N. Schluter, J. R. Tetreault (Eds.), Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics (ACL), Association for Computational Linguistics, Online, 2020 Jul\n5-10, pp. 4969–83. URL: https://doi.org/10.18653/v1/2020.acl-main.447.\n[80] D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, et al., Palm-e: An embodied multimodal\nlanguage model, in: A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, J. Scarlett (Eds.), Proceedings of\nthe International Conference on Machine Learning (ICML), PMLR, Honolulu, Hawaii, USA, 2023 Jul 23-29, pp.\n8469–88. URL: https://proceedings.mlr.press/v202/driess23a.html.\n[81] C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, et al., Llava-med: Training a large language-and-vision\nassistant for biomedicine in one day, Advances in Neural Information Processing Systems 36 (2024) 1–17.\n[82] A. Josh, A. Steven, A. Sandhini, A. Lama, A. Ilge, A. F. Leoni, et al., Gpt-4 technical report, CoRR\nabs/2303.08774 (2023). URL: https://doi.org/10.48550/arXiv.2303.08774.\n[83] R. Wang, Y . Duan, J. Li, P. Pang, T. Tan, Xrayglm: The first chinese medical multimodal model that chest\nradiographs summarization, https://github.com/WangRongsheng/XrayGLM, 2023.\n[84] C. Shu, B. Chen, F. Liu, Z. Fu, E. Shareghi, N. Collier, Visual med-alpaca: A parameter-e fficient biomedical llm\nwith visual capabilities, https://github.com/cambridgeltl/visual-med-alpaca, 2023.\n[85] Z. Xi, W. Chen, X. Guo, W. He, Y . Ding, B. Hong, et al., The rise and potential of large language model based\nagents: A survey, CoRR abs/2309.07864 (2023). URL: https://doi.org/10.48550/arXiv.2309.07864.\n[86] W. Zhou, Y . E. Jiang, L. Li, J. Wu, T. Wang, S. Qiu, et al., Agents: An open-source framework for autonomous\nlanguage agents, CoRR abs/2309.07870 (2023). URL: https://doi.org/10.48550/arXiv.2309.07870.\n[87] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, Y . Su, Llm-planner: Few-shot grounded planning\nfor embodied agents with large language models, in: Proceedings of the IEEE /CVF International Conference\non Computer Vision (ICCV), IEEE, Paris, France, 2023 Oct 1-6, pp. 2998–3009. URL: https://doi.org/10.\n48550/arXiv.2212.04088.\n[88] Y . M. Cho, S. Rai, L. H. Ungar, J. Sedoc, S. C. Guntuku, An ”integrative survey on mental health conversational\nagents to bridge computer science and medical perspectives”, in: H. Bouamor, J. Pino, K. Bali (Eds.), Proceedings\nof the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Com-\nputational Linguistics, Singapore, 2023 Dec 6-10, pp. 11346–69. URL: https://aclanthology.org/2023.\nemnlp-main.698.\n[89] M. R. Ali, S. Z. Razavi, R. Langevin, A. A. Mamun, B. Kane, R. Rawassizadeh, et al., A virtual con-\nversational agent for teens with autism spectrum disorder: Experimental results and design lessons, in:\nS. Marsella, R. Jack, H. H. Vilhj ´almsson, P. Sequeira, E. S. Cross (Eds.), Proceedings of the ACM Interna-\ntional Conference on Intelligent Virtual Agents (IV A), ACM, Scotland, UK, 2020 Oct 20-22, pp. 1–8. URL:\nhttps://doi.org/10.1145/3383652.3423900.\n[90] M. Abbasian, I. Azimi, A. M. Rahmani, R. C. Jain, Conversational health agents: A personalized llm-powered\nagent framework, CoRR abs/2310.02374 (2023). URL: https://doi.org/10.48550/arXiv.2310.02374.\n[91] X. Tang, A. Zou, Z. Zhang, Y . Zhao, X. Zhang, A. Cohan, et al., Medagents: Large language models as collab-\norators for zero-shot medical reasoning, CoRR abs /2311.10537 (2023). URL: https://doi.org/10.48550/\narXiv.2311.10537.\n[92] Q. Jin, Y . Yang, Q. Chen, Z. Lu, Genegpt: Augmenting large language models with domain tools for improved\naccess to biomedical information, Bioinformatics 40 (2024) btae075. URL: https://doi.org/10.48550/\narXiv.2304.09667.\n[93] A. J. Goodell, S. N. Chu, D. Rouholiman, L. F. Chu, Augmentation of chatgpt with clinician-informed tools\nimproves performance on medical calculation tasks, medRxiv 12 (2023) 1–11. URL: https://doi.org/10.\n1101/2023.12.13.23299881.\n[94] S. Pan, L. Luo, Y . Wang, C. Chen, J. Wang, X. Wu, Unifying large language models and knowledge graphs: A\nroadmap, IEEE Transactions on Knowledge and Data Engineering (2024) 1–20.\n[95] X. Yang, N. M. Pournejatian, H. C. Shin, K. E. Smith, C. Parisien, C. Compas, et al., Gatortron: A large clinical\nlanguage model to unlock patient information from unstructured electronic health records, CoRR abs/2203.03540\n(2022). URL: https://doi.org/10.48550/arXiv.2203.03540.\n[96] J. Zhang, Graph-toolformer: To empower llms with graph reasoning ability via prompt augmented by chatgpt,\n50\nCoRR abs/2304.11116 (2023). URL: https://doi.org/10.48550/arXiv.2304.11116.\n[97] C. Feng, X. Zhang, Z. Fei, Knowledge solver: Teaching llms to search for domain knowledge from knowledge\ngraphs, CoRR abs/2309.03118 (2023). URL: https://doi.org/10.48550/arXiv.2309.03118.\n[98] R. Xu, H. Cui, Y . Yu, X. Kan, W. Shi, Y . Zhuang, et al., Knowledge-infused prompting: Assessing and advancing\nclinical text data generation with large language models, CoRR abs /2311.00287 (2023). URL: https://doi.\norg/10.48550/arXiv.2311.00287.\n[99] Y . Gao, R. Li, J. R. Caskey, D. Dligach, T. A. Miller, M. M. Churpek, et al., Leveraging a medical knowledge\ngraph into large language models for diagnosis prediction, CoRR abs /2308.14321 (2023). URL: https://doi.\norg/10.48550/arXiv.2308.14321.\n[100] J. Sun, C. Xu, L. Tang, S. Wang, C. Lin, Y . Gong, et al., Think-on-graph: Deep and responsible reasoning of large\nlanguage model with knowledge graph, CoRR abs /2307.07697 (2023). URL: https://doi.org/10.48550/\narXiv.2307.07697.\n[101] Z. Heyi, W. Xin, H. Lifan, L. Zhao, C. Zirui, C. Zhe, Research on question answering system on joint of\nknowledge graph and large language models, Journal of Frontiers of Computer Science and Technology 17\n(2023). URL: http://fcst.ceaj.org/EN/10.3778/j.issn.1673-9418.2308070.\n[102] K. Soman, P. W. Rose, J. H. Morris, R. E. Akbas, B. Smith, B. Peetoom, et al., Biomedical knowledge graph-\nenhanced prompt generation for large language models, CoRR abs /2311.17330 (2023). URL: https://doi.\norg/10.48550/arXiv.2311.17330.\n[103] R. Xu, H. Cui, Y . Yu, X. Kan, W. Shi, Y . Zhuang, et al., Knowledge-infused prompting: Assessing and advancing\nclinical text data generation with large language models, CoRR abs /2311.00287 (2023). URL: https://doi.\norg/10.48550/arXiv.2311.00287.\n[104] D. Li, S. Yang, Z. Tan, J. Y . Baik, S. Yun, J. Lee, A. Chacko, B. Hou, D. Duong-Tran, Y . Ding, H. Liu, L. Shen,\nT. Chen, DALK: dynamic co-augmentation of llms and KG to answer alzheimer’s disease questions with scientific\nliterature, CoRR abs/2405.04819 (2024). URL: https://doi.org/10.48550/arXiv.2405.04819.\n[105] K. Papineni, S. Roukos, T. Ward, W. Zhu, Bleu: a method for automatic evaluation of machine translation,\nin: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002,\nPhiladelphia, PA, USA, ACL, 2002, pp. 311–318. URL:https://aclanthology.org/P02-1040/.\n[106] C.-Y . Lin, ROUGE: A package for automatic evaluation of summaries, in: Text Summarization Branches Out,\nAssociation for Computational Linguistics, 2004, pp. 74–81. URL: https://aclanthology.org/W04-1013.\n[107] S. Banerjee, A. Lavie, METEOR: an automatic metric for MT evaluation with improved correlation with hu-\nman judgments, in: Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine\nTranslation and/or Summarization@ACL 2005, Ann Arbor, Michigan, USA, June 29, 2005, Association for Com-\nputational Linguistics, 2005, pp. 65–72. URL: https://aclanthology.org/W05-0909/.\n[108] T. Zhang, V . Kishore, F. Wu, K. Q. Weinberger, Y . Artzi, Bertscore: Evaluating text generation with BERT, in: 8th\nInternational Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020,\nOpenReview.net, 2020. URL: https://openreview.net/forum?id=SkeHuCVFDr.\n[109] W. wai Yim, Y . Fu, A. B. Abacha, N. Snider, T. Lin, M. Yetisgen, Aci-bench: a novel ambient clinical intelligence\ndataset for benchmarking automatic visit note generation, Scientific Data 10 (2023) 586. URL: https://doi.\norg/10.1038/s41597-023-02487-3 .\n[110] W. Yue, X. Wang, W. Zhu, M. Guan, H. Zheng, P. Wang, C. Sun, X. Ma, Tcmbench: A comprehensive bench-\nmark for evaluating large language models in traditional chinese medicine, CoRR abs /2406.01126 (2024). URL:\nhttps://doi.org/10.48550/arXiv.2406.01126.\n[111] X. Wang, G. H. Chen, D. Song, Z. Zhang, Z. Chen, Q. Xiao, et al., Cmb: A comprehensive medical benchmark\nin chinese, CoRR abs/2308.08833 (2023). URL: https://doi.org/10.48550/arXiv.2308.08833.\n[112] W. Zhu, X. Wang, H. Zheng, M. Chen, B. Tang, Promptcblue: A chinese prompt tuning benchmark for the\nmedical domain, CoRR abs/2310.14151 (2023). URL: https://doi.org/10.48550/arXiv.2310.14151.\n[113] S. Lyu, C. Chi, H. Cai, L. Shi, X. Yang, L. Liu, et al., Rjua-qa: A comprehensive qa dataset for urology, CoRR\nabs/2312.09785 (2023). URL: https://doi.org/10.48550/arXiv.2312.09785.\n[114] S. Kweon, J. Kim, H. Kwak, D. Cha, H. Yoon, K. Kim, J. Yang, S. Won, E. Choi, Ehrnoteqa: An llm benchmark\nfor real-world clinical practice using discharge summaries (2024). URL: https://arxiv.org/abs/2402.\n16040.\n[115] J. C. L. Ong, S. Y .-H. Chang, W. William, A. J. Butte, N. H. Shah, L. S. T. Chew, N. Liu, F. Doshi-Velez,\nW. Lu, J. Savulescu, D. S. W. Ting, Medical ethics of large language models in medicine, NEJM AI 1 (2024)\nAIra2400038. URL: https://ai.nejm.org/doi/abs/10.1056/AIra2400038.\n[116] P. Yang, H. Wang, Y . Huang, S. Yang, Y . Zhang, L. Huang, Y . Zhang, G. Wang, S. Yang, L. He, Y . Huang, LMKG:\nA large-scale and multi-source medical knowledge graph for intelligent medicine applications, Knowledge-Based\nSystems 284 (2024) 111323. URL: https://doi.org/10.1016/j.knosys.2023.111323.\n[117] P. Chandak, K. Huang, M. Zitnik, Building a knowledge graph to enable precision medicine, Scientific Data 10\n(2023) 67. URL: https://doi.org/10.1038/s41597-023-01960-3 .\n51\n[118] S. Reddy, Evaluating large language models for use in healthcare: A framework for translational value assess-\nment, Informatics in Medicine Unlocked 41 (2023) 101304. URL:https://doi.org/10.1016/j.imu.2023.\n101304.\n[119] J. Maharjan, A. Garikipati, N. P. Singh, L. Cyrus, M. Sharma, M. Ciobanu, G. Barnes, R. Thapa, Q. Mao,\nR. Das, Openmedlm: prompt engineering can out-perform fine-tuning in medical question-answering with\nopen-source large language models, Scientific Reports 14 (2024) 14156. URL: https://doi.org/10.1038/\ns41598-024-64827-6 .\n[120] T. Savage, A. Nayak, R. Gallo, E. Rangan, J. H. Chen, Diagnostic reasoning prompts reveal the potential for\nlarge language model interpretability in medicine, npj Digital Medicine 7 (2024). URL:https://doi.org/10.\n1038/s41746-024-01010-1 .\n[121] L. Tang, Z. Sun, B. R. S. Idnay, J. G. Nestor, A. Soroush, P. A. Elias, Z. Xu, Y . Ding, G. Durrett, J. F. Rousseau,\nC. Weng, Y . Peng, Evaluating large language models on medical evidence summarization, npj Digital Medicine\n6 (2023). URL: https://doi.org/10.1038/s41746-023-00896-7 .\n[122] X. Wang, N. Chen, J. Chen, Y . Hu, Y . Wang, X. Wu, A. Gao, X. Wan, H. Li, B. Wang, Apollo: An lightweight\nmultilingual medical LLM towards democratizing medical AI to 6b people, CoRR abs/2403.03640 (2024). URL:\nhttps://doi.org/10.48550/arXiv.2403.03640.\n[123] S. Pieri, S. S. Mullappilly, F. S. Khan, R. M. Anwer, S. H. Khan, T. Baldwin, H. Cholakkal, Bimedix: Bilingual\nmedical mixture of experts LLM, CoRR abs/2402.13253 (2024). URL: https://doi.org/10.48550/arXiv.\n2402.13253.\n[124] W. Liu, Y . Zuo, Stone needle: A general multimodal large-scale model framework towards healthcare, CoRR\nabs/2306.16034 (2023). URL: https://doi.org/10.48550/arXiv.2306.16034.\n[125] X. Lin, C. Xu, Z. Xiong, X. Zhang, N. Ni, B. Ni, et al., Pangu drug model: Learn a molecule like a human,\nSCIENCE CHINA Life Sciences 66 (2023). URL: https://doi.org/10.1007/s11427-022-2239-y .\nAppendix A. Industrial Text Medical LLMs\nHere we present the summarization of industrial medical LLMs building from text corpora.\nAppendix B. Industrial Multimodal Medical LLMs\nHere we present the summarization of industrial medical LLMs building from multimodal\ndata.\nAppendix C. Academic Medical Texts LLMs Assessment\nHere, we systematically summarize the performance of academic medical texts LLMs across\nfive key medical capabilities: medical professional knowledge mastery, medical Q&A ability,\nmedical information extraction ability, medical dialogue ability, and medical content generation\nability. Moreover, we provide a detailed overview of their evaluation methods.\nAppendix D. Academic Medical Multimodal LLMs Assessment\nHere, we systematically summarize the performance of academic medical multimodal LLMs\nacross five key medical capabilities: medical Q&A ability, medical report summarization ability,\nmedical visual question answering ability, medical report generation ability, and medical image\nclassification ability. Additionally, we provide a detailed overview of their evaluation methods.\n52\nTable A.10: Application for plain-text industrial medical LLMs. These models often were built on vast real-life clinical data collected by cooperation’s, which reflects the\npatient condition more comprehensively.\nApplication Name Anchor Model Data Scale Enterprise Name Function\nUniGPT-Med UniGPT 1 2,000B tokens Unisound AI Technology Co., Ltd. AI-assisted D&T\nMedical Record Generation\n- EyeGPT 2 - Wenzhou International Optometry Innovation Centre Clinical Medical Assistance\nDaJing TCM QiHuangWenDao 3\n11M TCM-KG\n1,500 books&literatures\n100,000 real medical cases\n100,000 P&T&M&A4\n2M diagnosis\nNanjing Dajing TCM IT Co., Ltd. Intelligent Assisted D&T\nIntelligent Health Conditioning\nDingDangKuaiYao5 HealthGPT - DingDangKuaiYao Technology Group Co., Ltd.\nQuick Medicine Service\nOnline Consultation\nChronic and Health Management\nGushengtang TCM GushengtangTCM - Gushengtang TCM Chain Management Group AI-assisted D&T\nRecommended TCM FormulasGushengtang-Doctor\nXunfei Xiaoyi Xinghuo 6 - iFLYTEK Co., Ltd. Report Interpretation\nTCM Syndromes Identification\nZhiyun Health7 ClouD GPT - Hangzhou Comms IT Co., Ltd.\nAI-assisted Diagnosis\nAI Drug\nDevice Development\nWiNEX8 WiNGPT9 30B/65B tokens Winning Health Technology Group Co.,Ltd Health Care Assistant\nTongyi Renxin10 Tongyi Qwen11 >3,000B tokens Alibaba Group\nIntelligent Inquiry\nReport Interpretation\nSummary Abstract\n1 http://shanhai.unisound.com/\n2 http://eyegpt.com.cn/#/\n3 http://www.dajingtcm.com/dajinggpt\n4 Pulse&Tongue&Meridian&Acupuncture\n5 https://www.ddky.com/\n6 https://xinghuo.xfyun.cn/\n7 https://www.zyhealth.com\n8 https://www.winning.com.cn/WiNEX/\n9 https://github.com/winninghealth/WiNGPT2\n10 https://tongyi.aliyun.com/renxin\n11 https://tongyi.aliyun.com/qianwen\n53\nTable B.11: Application for multimodal industrial medical LLMs. The integration of numerous clinical data gives the model a better understanding over the fine-grained medical knowledge.\nApplication Name Anchor Model Data Scale Enterprise Name Function\nStoneNeedle[124] - - AthenaEyesCo., LTD.\nAI-assisted Diagnosis\nAI-assisted Reading\nPhysiological Prediction\nSleep Monitoring\nTencent MedLLM Hunyuan 1\n2.85M medical entities\n12.5M medical relations\nMedical KG&literatures covering 98%\n30M dialogues\n360,000 expert’s annotation\nTencent Cloud Computing (Beijing) Co., Ltd.\nContent Generation\nMedical Record Structuralization\nAI-assisted Diagnosis\nAI Rational Drug Use\nAI-assisted Reading\nPanGu Pangu-Drug[125] >3,000B tokens Huawei Technologies Co., Ltd. AI-assisted drug R&D\nMedical Sense SenseChat 2 >20B tokens Shanghai SenseTime IT Co., Ltd.\nAI-assisted diagnosis\nClinic Interpreter Robot\nMedical Record Structuralization\nMedLinker3 MedGPT\n>2B medical texts\nSFT 8M diagnosis\n>100 doctors RLHF\nChengdu MedCloud Technology Co., Ltd. Intelligent Health Inquiry\nWeiMai CareGPT\n>1B medical texts\nMillions of medical and health KB Weima Technology Co., Ltd. Personalized Matching and Recommendation\nAiding Decision-makingWeiMai-Doctor SFT >100 doctors RLHF\n01Bot4 Baidu Wenxin5 Hundred billion tokens Baidu Online Network Technology (Beijing) Co., Ltd. AI-assisted Diagnosis\nMedical Record Generation\n1 https://hunyuan.tencent.com/\n2 https://chat.sensetime.com/\n3 https://www.medlinker.com/\n4 https://01.baidu.com/bot.html\n5 https://wenxin.baidu.com/\n54\nTable C.12: Summary of academic medical texts LLMs in medical professional knowledge mastery.\nAssessment\nMedical Professional Knowledge Mastery\nCMExam CMB-Exam MedQA MedMCQA C-Eval\nCMExam Prediction |CMExam Reasoning Physician |Nurse |Pharmacist |Technician |Disciplines |Graduate Entrance USMLE |MCMLE |TWMLE Clinical Medicine |Physician |Basic Medicine\nAcc |BLEU-1/4\u000eRouge-1/2/L Acc Avg. Acc |Elo Rating Acc Acc\nBioMedLM-2.7B Auto 50.3// | 57.3\nGatorTronGPT-5B Auto +Human 40.2// | 35.8\nGatorTronGPT-20B Auto +Human 45.1// | 42.9\nPMC-LLaMA-13B Auto 56.36// | 56.04\nMed-PaLM Auto +Human 67.2// |\nMed-PaLM 2(Ensemble Refinement) Auto +Human 85.4// | 72.3\nMed-PaLM 2 Auto +Human 86.5// | 72.3\nChatDoctor-7B Auto 33.93// | 31.1\nSunsimiao(Baichuan) Auto 38.75/44.37/38.81/38.33/37.5/33.31\nSunsimiao-7B(Qwen) Auto 66.75/72/71.53/64.83/63.06/75.19\nQiZhenGPT-QiZhen-CaMA-13B-Checkpoint-12400 CS(Drug Indication Review /Disease Review) |955/959/\nChatMed-Consult CS(Patients’ Daily Medical Problems) 21.41 /23.48/21.58/23.55/21.36/18.08\nBenTsao Human 12.9 |0.21/0.12/25.11/11.56/9.73 21.67 /19.99/21.07/22.85/19.83/16.93 |961/921/\nClinicalGPT Auto +GPT4 /37.62/ |\nMedicalGPT(Baichuan) CS(Medical Q&A and Daily Q&A) /18.5/ | 39.023\nMedicalGPT(Ziya) CS(Medical Q&A and Daily Q&A) 26.56 /30.94/24.72/27.17/25.44/21.5 /25.99/ | 48.783\nHuatuoGPT Auto +GPT4+Human 31.07 | 31.85/35/30.56/31.38/35/28.25 25.77 // | 31.2 36.53 3\nHuatuoGPT21 - |955/993/\nHuatuoGPTII-7B Auto +GPT4+Human 65.81 | 64.55/63.75/61.06/56.25/56.63/51.81 41.13 // | 41.87 62.4 3\nHuatuoGPTII-13B - 68.98 | 62.75/66.13/64.91/62/61.94/53.69 45.68 // | 47.41 64 3\nHuatuoGPTII-34B - 75.65/82.31/76.81/76.17/74.38/75.56\nDISC-MedLLM Auto +GPT3.5/GPT4 36.62 | 42.25/46.88/38.44/38.83/40.75/31.44 28.67 // | - -\nBianQue Auto\nBianQue 2 Auto 6.95/7.31/7.25/9.75/6.94/6.06 |913/928/\nDoctorGLM Auto +Semi-human - |9.43/2.65/21.11/6.86/9.99 6.95 /7.31/8.28/9.75/7.5/6.06 |906/896/\nPULSE-20b(InternLM) Auto |1042/1024/\nChiMed-GPT Auto +Human /44.5/ | 68.293\nQilin-Med-7B-CPT Auto 38.4 |13.98/4.43/23.51/8.68/7.41 41/44.9/34.3\nQilin-Med-7B-SFT Auto 40 |40.31/25.05/53.56/36.39/34.17 48.5/55.5/43.4\nQilin-Med-7B-DPO Auto\n1 https://www.huatuogpt.cn/#/\n2 Respiratory-Urinary-Digestive-Rheumatic immune\n3 (Clinical Medicine+ Basic Medicine)/2\n55\nTable C.13: Summary of academic medical texts LLMs in medical Q&A and information extraction ability.\nMedical Q&A Ability Medical Information Extraction Ability\nPubMedQA ChiMed CMMLU iCliniq CCKS-2019 ChiMST BC5CDR\nAnatomy |Clinical Knowledge |College Medicine |Genetics |Nutrition |Traditional Chinese Medicine |Virology\nAcc BLEU-1 /2\u000eRouge-1/2/L Acc Precision \u000eRecall\u000eF1 F1 F1 Precision \u000eRecall\u000eF1\nBioMedLM-2.7B 74.4\nGatorTronGPT-5B 75.8 58.7/43.4/47.2\nGatorTronGPT-20B 77.6 54.3/49.9/49.4\nPMC-LLaMA-13B 77.9\nMed-PaLM\nMed-PaLM 2(Ensemble Refinement) 75\nMed-PaLM 2 81.8\nChatDoctor-7B 54.3 84.44±1.85/84.51±1.57/84.46±1.38\nSunsimiao(Baichuan)\nSunsimiao-7B(Qwen)\nQiZhenGPT-QiZhen-CaMA-13B-Checkpoint-12400\nChatMed-Consult\nBenTsao\nClinicalGPT\nMedicalGPT(Baichuan) 5.82 /5.26/16.61/2.94/11.11 43.82 23.8 26.16\nMedicalGPT(Ziya) 39.02 /32.35/26.76/8.10/18.16 34.56 29.59 28.12\nHuatuoGPT 33.23\nHuatuoGPT21\nHuatuoGPTII-7B 59.08\nHuatuoGPTII-13B 61.45\nHuatuoGPTII-34B\nDISC-MedLLM -\nBianQue\nBianQue 2\nDoctorGLM\nPULSE-20b(InternLM)\nChiMed-GPT 44.58 /37.22/27.11/8.89/19.86 52.92 40.82 41.04\nQilin-Med-7B-CPT\nQilin-Med-7B-SFT\nQilin-Med-7B-DPO\n1 https://www.huatuogpt.cn/#/\n56\nTable C.14: Summary of academic medical texts LLMs in medical dialogue and content generation ability.\nMedical Dialogue Ability Medical Content Generation Ability\nMedDialog-CN Huatuo-26M DialogSumm\nBLEU-1/2/3/4\u000eRouge-1/2/L BLEU-1 /4\u000eRouge-1/2/L Elo Rating\nBioMedLM-2.7B\nGatorTronGPT-5B\nGatorTronGPT-20B\nPMC-LLaMA-13B\nMed-PaLM\nMed-PaLM 2(Ensemble Refinement)\nMed-PaLM 2\nChatDoctor-7B\nSunsimiao(Baichuan)\nSunsimiao-7B(Qwen)\nQiZhenGPT-QiZhen-CaMA-13B-Checkpoint-12400 921\nChatMed-Consult\nBenTsao 920\nClinicalGPT 13.9 /3.7/2.0/1.2/27.9/6.5/21.3\nMedicalGPT(Baichuan)\nMedicalGPT(Ziya)\nHuatuoGPT 25.16 /4.4/27.76/7.45/17.99\nHuatuoGPT21 980\nHuatuoGPTII-7B\nHuatuoGPTII-13B\nHuatuoGPTII-34B\nDISC-MedLLM\nBianQue 11.12 /6.5/4.42/3.1/15.55/2.15/12.96\nBianQue 2 908\nDoctorGLM 10.39 /5.06/2.94/1.8/13.27/1.04/11.17 905\nPULSE-20b(InternLM) 1076\nChiMed-GPT\nQilin-Med-7B-CPT 10.63 /0.98/19.97/3.33/4.94\nQilin-Med-7B-SFT 12.69 /2.07/24.21/6.34/11.56\nQilin-Med-7B-DPO 16.66 /2.64/27.44/6.88/9.36\n1 https://www.huatuogpt.cn/#/\n57\nTable D.15: Summary of academic medical multimodal LLMs in medical Q&A ability.\nAssessment\nMedical Q&A Ability\nMedQA MedMCQA PubMedQA ChEBI-20 UniProtQA\nUSMLE |MCMLE |TWMLE\nAcc Acc Acc BLEU-2 /4\u000eRouge-1/2/L\u000eMETEOR BLEU-2 /4\u000eRouge-1/2/L\u000eMETEOR\nBioMedGPT-10B Auto 50.4 // 51.4 76.1 23.4 /14.1/38.6/20.6/33.2/30.8 57.1 /53.5/74.3/75.9/62.2/75.4\nMed-PaLM M-12B Auto +Human 29.22 // 32.2 48.6\nMed-PaLM M-84B Auto +Human 46.11 // 47.6 71.4\nMed-PaLM M-562B Auto +Human 69.68 // 62.59 80\nVisual Med-Alpaca CS(ROCO)\nBiomedGPT2-S-33M Auto +Human\nBiomedGPT-M-93M Auto +Human\nBiomedGPT-B-182M Auto +Human\nQilin-Med-VL CS(PMC-VQA)\nLLaV A-Med(BioMed CLIP) Auto\nXrayPULSE CS(MIMIC-CXR /OpenI)\nXrayGLM CS(MIMIC-CXR /OpenI)\n1 Where “Auto” means “Automatic Evaluation”. “Human” means “Human Evaluation”. “CS” means “Case Study”.\n2 https://github.com/taokz/BiomedGPT\n58\nTable D.16: Summary of academic medical multimodal LLMs in medical report summarization and visual question answering ability.\nMedical Report Summarization Ability Medical Visual Question Answering Ability\nMIMIC-III VQA-RAD Slake-VQA Path-VQA\nClosed-ended |Open-ended | Closed-ended |Open-ended | Closed-ended |Open-ended |\nRouge-L\u000eBLEU\u000eF1-RadGraph Acc |Acc\u000eRecall |Acc\u000eBLEU-1\u000eF1 Acc |Acc\u000eRecall |Acc\u000eBLEU-1\u000eF1 Acc |Acc\u000eRecall |Acc\u000eBLEU-1\u000eF1\nBioMedGPT-10B\nMed-PaLM M-12B 29.45 /12.14/31.43 || /64.02/50.66 || /90.77/86.22 || /68.97/57.24\nMed-PaLM M-84B 31.47 /15.36/33.96 || /69.38/59.9 || /92.7/89.28 || /70.16/59.51\nMed-PaLM M-562B 32.03 /15.21/34.71 || /71.27/62.06 || /91.64/87.5 || /72.27/62.69\nVisual Med-Alpaca\nBiomedGPT-S-33M 57.8 |13.4/ |40.1// 73.3 |66.5/ |69.2// 84.2 |10.7/ |47.6//\nBiomedGPT-M-93M 79.8 |53.6/ |69.4// 86.8 |78.3/ |81.6// 85.7 |12.5/ |49.2//\nBiomedGPT-B-182M 30.7 //31.2 81.3 |60.9/ |73.2// 89.9 |84.3/ |86.1// 88 |28/ |58.1//\nQilin-Med-VL\nLLaV A-Med(BioMed/CLIP) 83.09 |/64.75 | 86.78 |/87.11 | 91.09 |/39.6 |\nXrayPULSE\nXrayGLM\n59\nTable D.17: Summary of academic medical multimodal LLMs in medical report generation and image classification ability.\nMedical Report Generation Ability Medical Image Classification Ability\nMIMIC-CXR MIMIC-CXR PAD-UFES-20 CBIS-DDSM\nMass |Classification\nMicro/Macro-F1-14\u000eMicro/Macro-F1-5\u000eF1-RadGraph\u000eBLEU-1/4\u000eRouge-L\u000eMETEOR\u000eCIDEr-D Macro-AUC /F1 Macro-AUC /F1 Macro-AUC /F1 |Macro-AUC/F1\nBioMedGPT-10B\nMed-PaLM M-12B 51.41 /37.31/56.54/50.57/25.2/30.9/10.43/26.16//23.43 76.67 /38.33 95.57 /78.42 70.11 /47.23 |81.4/67.86\nMed-PaLM M-84B 53.56 /39.83/57.88/51.6/26.71/32.31/11.31/27.29//26.17 78.35 /36.83 97.27 /84.32 73.09 /49.98 |82.22/63.81\nMed-PaLM M-562B 51.6 /37.81/56.28/49.86/26.06/31.73/11.5/27.49//25.27 79.09 /41.57 96.08 /77.03 73.31 /51.12 |80.9/63.03\nVisual Med-Alpaca\nBiomedGPT-S-33M ///////23/13/12.8 - |-\nBiomedGPT-M-93M ///////23.2/13/12.9 /18.7 |/18.9\nBiomedGPT-B-182M ///////28.7/15.9/23.4 /57.2 |/72.8\nQilin-Med-VL\nLLaV A-Med(BioMed/CLIP)\nXrayPULSE\nXrayGLM\n60",
  "topic": "Training (meteorology)",
  "concepts": [
    {
      "name": "Training (meteorology)",
      "score": 0.6273431777954102
    },
    {
      "name": "Medical knowledge",
      "score": 0.4952909052371979
    },
    {
      "name": "Computer science",
      "score": 0.47529137134552
    },
    {
      "name": "Medical education",
      "score": 0.40643927454948425
    },
    {
      "name": "Knowledge management",
      "score": 0.34592393040657043
    },
    {
      "name": "Medicine",
      "score": 0.23357847332954407
    },
    {
      "name": "Geography",
      "score": 0.10988789796829224
    },
    {
      "name": "Meteorology",
      "score": 0.0
    }
  ],
  "institutions": []
}