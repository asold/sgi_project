{
  "title": "Proof Artifact Co-training for Theorem Proving with Language Models",
  "url": "https://openalex.org/W3128644037",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4226591322",
      "name": "Han, Jesse Michael",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287536308",
      "name": "Rute, Jason",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3091385409",
      "name": "Wu, Yuhuai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287536310",
      "name": "Ayers, Edward W.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226591321",
      "name": "Polu, Stanislas",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2997319416",
    "https://openalex.org/W3098600782",
    "https://openalex.org/W2963117013",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2295529234",
    "https://openalex.org/W2932237430",
    "https://openalex.org/W2963420272",
    "https://openalex.org/W343636949",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W3035758697",
    "https://openalex.org/W2989929945",
    "https://openalex.org/W3128367099",
    "https://openalex.org/W2963644595",
    "https://openalex.org/W1464569014",
    "https://openalex.org/W3118411276",
    "https://openalex.org/W2025768430",
    "https://openalex.org/W3095645723",
    "https://openalex.org/W2321533354",
    "https://openalex.org/W3042720530",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2127812195",
    "https://openalex.org/W2509751917",
    "https://openalex.org/W3103412473",
    "https://openalex.org/W2962742544",
    "https://openalex.org/W3125166688",
    "https://openalex.org/W2945576559",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3173777491",
    "https://openalex.org/W2970680991",
    "https://openalex.org/W2981815722",
    "https://openalex.org/W2753707546",
    "https://openalex.org/W2963147113",
    "https://openalex.org/W3118095107",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3133180075",
    "https://openalex.org/W3013302896",
    "https://openalex.org/W2995359496",
    "https://openalex.org/W3127973970",
    "https://openalex.org/W2262606152",
    "https://openalex.org/W2795795481",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W2962765587",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2899508538",
    "https://openalex.org/W3115293622",
    "https://openalex.org/W1982239968",
    "https://openalex.org/W2946011656",
    "https://openalex.org/W2064800569",
    "https://openalex.org/W2413604966",
    "https://openalex.org/W2428112462",
    "https://openalex.org/W2951122437",
    "https://openalex.org/W1573726182",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2593925740",
    "https://openalex.org/W3083835029"
  ],
  "abstract": "Labeled data for imitation learning of theorem proving in large libraries of formalized mathematics is scarce as such libraries require years of concentrated effort by human specialists to be built. This is particularly challenging when applying large Transformer language models to tactic prediction, because the scaling of performance with respect to model size is quickly disrupted in the data-scarce, easily-overfitted regime. We propose PACT ({\\bf P}roof {\\bf A}rtifact {\\bf C}o-{\\bf T}raining), a general methodology for extracting abundant self-supervised data from kernel-level proof terms for co-training alongside the usual tactic prediction objective. We apply this methodology to Lean, an interactive proof assistant which hosts some of the most sophisticated formalized mathematics to date. We instrument Lean with a neural theorem prover driven by a Transformer language model and show that PACT improves theorem proving success rate on a held-out suite of test theorems from 32\\% to 48\\%.",
  "full_text": "Published as a conference paper at ICLR 2022\nPROOF ARTIFACT CO-TRAINING FOR THEOREM PROV-\nING WITH LANGUAGE MODELS\nJesse Michael Han\nUniversity of Pittsburgh\nOpenAI\nJason Rute\nIBM Research∗\nYuhuai Wu\nGoogle Research\nStanford University†\nEdward W. Ayers\nCarnegie Mellon University‡\nStanislas Polu\nOpenAI\nABSTRACT\nLabeled data for imitation learning of theorem proving in large libraries of formal-\nized mathematics is scarce, as such libraries require years of concentrated effort\nby human specialists to be built. This is particularly challenging when applying\nlarge Transformer language models to tactic prediction, because the scaling of\nperformance with respect to model size is quickly disrupted in the data-scarce,\neasily-overﬁtted regime. We propose PACT (Proof Artifact Co-Training), a general\nmethodology for extracting abundant self-supervised data from kernel-level proof\nterms for joint training alongside the usual tactic prediction objective. We apply\nthis methodology to Lean, a proof assistant host to some of the most sophisticated\nformalized mathematics to date. We instrument Lean with a neural theorem prover\ndriven by a Transformer language model and show that PACT improves theorem\nproving success rate on a held-out suite of test theorems from 32% to 48%.\n1 I NTRODUCTION\nDeep learning-driven automated theorem proving in large libraries of formalized mathematics (hence-\nforth “neural theorem proving”) has been the focus of increased attention in recent years. Labeled\ndata for imitation learning of theorem proving is scarce—formalization is notoriously labor-intensive,\nwith an estimated cost of 2.5 man-years per megabyte of formalized mathematics (Wiedijk, 2000),\nand complex projects require years of labor from human specialists. Within a ﬁxed corpus of (possibly\nunproven) theorem statements, it is possible to augment a seed dataset of human proofs with new\nsuccessful trajectories using reinforcement learning or expert iteration. However, for some large\nmodels this can be quite computationally intensive, and without a way to expand the curriculum of\ntheorems, the agent will inevitably saturate and suffer from data starvation.\nData scarcity is a particularly thorny obstruction for applying large language models (LLMs) to\nneural theorem proving. LLMs have achieved spectacular success in data-rich regimes such as plain\ntext (Brown et al., 2020), images (Dosovitskiy et al., 2021), and joint text-image modeling (Radford\net al., 2021), and the performance of decoder-only Transformers has been empirically shown to\nobey scaling power laws in model and data size (Henighan et al., 2020). However, existing datasets\nof human proof steps for neural theorem proving are extremely small and exist at scales at which\noverﬁtting occurs extremely rapidly, disrupting the scaling of performance with respect to model\nsize (Kaplan et al., 2020).\nWe make two contributions towards addressing the problem of data scarcity in the context of formal\nmathematics. First, we introduce PACT (Proof Artifact Co-Training), a general methodology for\nextracting self-supervised auxiliary tasks for jointly training a language model alongside a tactic\nprediction objective for interactive theorem proving. Second, we present LEAN STEP , a collection of\n∗Work performed while Jason Rute was at CIBO Technologies.\n†Work performed while Yuhuai Wu was at University of Toronto.\n‡Work performed while Edward W. Ayers was at University of Cambridge.\n1\narXiv:2102.06203v2  [cs.AI]  16 Mar 2022\nPublished as a conference paper at ICLR 2022\ndatasets and a machine learning environment for the Lean 3 theorem prover with support for PACT,\nsupervised learning of tactic prediction, theorem proving evaluation, and reinforcement learning.\nWe train large language models on these data and demonstrate that PACT signiﬁcantly improves\ntheorem proving success rate on a held-out suite of test theorems, from 32% to 48%. We then embark\non a careful study of the effects of pre-training vs. co-training and show that PACT combined with\nWebMath pre-training (Polu & Sutskever, 2020) achieves the best validation loss and theorem proving\nsuccess rate. Finally, on an out-of-distribution collection of thousands of theorems (some involving\nnovel deﬁnitions) added to Lean’s mathematical library after we extracted our train/test data, we\nachieve a theorem proving success rate of 37%, suggesting strong generalization and usefulness at\nthe frontier of formalized mathematics.\n2 B ACKGROUND AND RELATED WORK\nLEAN Lean is an interactive theorem prover and functional programming language (de Moura et al.,\n2015). It has an extremely active community and is host to some of the most sophisticated formalized\nmathematics in the world, including scheme theory (Buzzard et al., 2021), forcing (Han & van Doorn,\n2020), perfectoid spaces (Buzzard et al., 2020), and condensed mathematics (Scholze, 2020). Lean’s\nfoundational logic is a dependent type theory called the calculus of inductive constructions (Pfenning\n& Paulin-Mohring, 1989). This design means that terms, types and proofs are all represented with a\nsingle datatype called an expression. A proof term is a Lean expression whose type is a proposition,\ni.e. a theorem. This proof term serves as a checkable artifact for verifying the proposition. Lean uses a\nsmall, trusted kernel to verify proof terms. The primary repository of formalized mathematics in Lean\nis mathlib (mathlib, 2020). At the time of writing, 140 contributors have added almost 500,000\nlines of code; mathlib contains over 46,000 formalized lemmas backed by over 21,000 deﬁnitions,\ncovering topics such as algebraic geometry, computability, measure theory, and category theory. The\nrange of topics and the monolithic, uniﬁed organization of mathlib make it an excellent foundation\nfor a neural theorem proving dataset.\nMACHINE LEARNING IN INTERACTIVE THEOREM PROVING In a tactic-based interactive theorem\nprover (ITP) such as Lean, a proof is a list of tactics, i.e. small proof-term-generating programs.\nTactics can be simple one-word commands, e.g. refl, or be composed of many nested parts, e.g.\nsimpa [le_antisymm_iff, norm_nonneg] using @norm_eq_zero α _ g\nHere the brackets enclose a list of simpliﬁer rules (which often are just lemmas from the library), and\n@norm_eq_zero α _ g is a proof term applying the lemma norm_eq_zero to the local variables\nαand g.\nOther ML and neural theorem provers for tactic-based ITPs take one of two approaches to tactic\ngeneration. TacticToe (Gauthier et al., 2021) for HOL4 and Tactician (Blaauwbroek et al., 2020)\nfor Coq use k-NN to select similar tactics in the training set and apply modiﬁcations to the result,\ne.g. swapping the tactic variables with those found in the local context. HOList/DeepHOL (Bansal\net al., 2019b;a; Paliwal et al., 2020) for HOL Light; TacticZero (Wu et al., 2021a) for HOL4; and\nCoqGym/ASTactic (Yang & Deng, 2019) and ProverBot9001 (Sanchez-Stern et al., 2020) for Coq\nhard-code the DSL for every tactic command. The model chooses a tactic command, and then ﬁlls in\nthe tactic arguments using specialized argument selectors (such as a lemma selector, a local hypothesis\nselector, and/or a variable selector). None of these selectors currently synthesize arbitrary terms. This\nprevents the tactic synthesis from constructing tactics with proof terms, such as @norm_eq_zero α\n_ g, or directly proving an existential, e.g. ∃ (x : R), x + 3 = 0, by supplying the witnessing\nterm -3.\nDirectly applying generative language modeling to tactic generation allows this setup to be consid-\nerably simpliﬁed. Our tactic generator is able to synthesize tactics of any form found in mathlib\nincluding, for example, the simpa example above as a one line proof to a test theorem, even though\nthe string @norm_eq_zero does not occur in our dataset. (See more examples in Appendix D.) We\nleave as future work the possibility of re-integrating specialized components, e.g. lemma selection,\nfound in other works (possibly as, say, a source of additional prompts for the language model).\nLanguage models have also been explored in the ﬁrst-order ITP Mizar for conjecturing and proof\nsynthesis (Urban & Jakubuv, 2020). While their work shows the promise of such approaches,\n2\nPublished as a conference paper at ICLR 2022\nis not intended as a complete end-to-end theorem prover. For Metamath, which does not use\ntactics, language modeling approaches have been quite successful. Holophrasm (Whalen, 2016),\nMetaGen (Wang & Deng, 2020), and GPT-f (Polu & Sutskever, 2020) all use RNNs or Transformers\nto generate proof steps. Indeed, our paper builds on the work of Metamath GPT-f (Polu & Sutskever,\n2020) (MM GPT-f). Whereas MM GPT-f trained primarily on the Metamath proof step objective\n(i.e. guessing the next lemma to be applied to a goal, which is similar to our NEXTLEMMA task\nin Section 3.2), we co-train on a diverse suite of self-supervised tasks extracted from Lean proof\nterms and demonstrate signiﬁcant improvements in theorem proving performance when doing so.\nThis is our main result.\nREASONING WITH TRANSFORMERS Besides theorem proving, a number of recent papers have\nshown that language models, especially Transformers, are capable of something like mathematical\nand logical reasoning in integration (Lample & Charton, 2020), differential equations (Charton et al.,\n2021), Boolean satisﬁability (Hahn et al., 2021), and inferring missing proof steps (Li et al., 2021).\nA closely-related vein of work has shown that pre-training Transformers on data engineered to reﬂect\ninductive biases conducive to mathematical reasoning is beneﬁcial for downstream mathematical\nreasoning tasks (Rabe et al., 2021; Wu et al., 2021b). Our work both builds on and departs from\nthese ideas in several ways. Unlike skip-tree training (Rabe et al., 2021), which focuses solely on\npredicting masked subterms of theorem statements, PACT derives its self-supervised training data\nfrom far more complex proofs. Unlike LIME (Wu et al., 2021b), which uses purely synthetic data and\nis presented as a pre-training methodology, our self-supervised tasks are extracted from non-synthetic\nhuman proofs. Moreover, we show that not only are Transformers capable of performing well on\nauxiliary tasks gathered from low-level proof artifact data, but that we can directly leverage this\nlow-level data by jointly training a language model to greatly improve its performance at high-level\ntheorem proving.\nMACHINE LEARNING WITH PROOF ARTIFACTS The idea of mining low-level proof artifacts was\npreviously explored by Kaliszyk and Urban in the context of automated lemma extraction (Kaliszyk\n& Urban, 2015b; Kaliszyk et al., 2015). It has also been previously observed that training on fully\nelaborated Coq terms (Nie et al., 2020) helps with a downstream theorem naming task. However,\nsimilar to previous work on skip-tree training, their dataset focuses solely on theorem statements,\ni.e. types, does not cover the far more complex proof terms, and does not evaluate the effect of such\ntraining on theorem proving evaluations.\nWhile there exist environments and datasets for other formal mathematics libraries (Kaliszyk et al.,\n2017; Li et al., 2021; Huang et al., 2019; Kaliszyk & Urban, 2015a), LEAN STEP is the ﬁrst and only\ntactic proof dataset for the Lean theorem prover. This makes available a large set of formal mathe-\nmatical data to researchers covering a diverse and deep spectrum of pure mathematics. Moreover,\nLEAN STEP is unique in that it contains both high-level human-written tactics as well as kernel-level\nproof terms, which enables the extraction of self-supervised tasks for PACT (Section 3.2).\n3 T HE LEAN STEP DATASETS AND MACHINE LEARNING ENVIRONMENT\n3.1 H UMAN TACTIC PROOF STEPS\nTactics in Lean are metaprograms (Ebner et al., 2017), which can construct Lean expressions, such as\nproof terms. A tactic state which tracks the list of open goals and other metadata (like the partial\nproof term constructed so far) is threaded through each tactic invocation. Lean has special support for\ntreating tactics as an extensible domain-speciﬁc language (DSL); this DSL is how Lean is typically\nused as an interactive theorem prover. The DSL amounts to a linear chain of comma-separated\ninvocations. The Lean proof step task is to predict the next tactic given this goal state. We refer the\nreader to Appendix A for examples and further explanation.\nOur human tactic proof step dataset consists of source-target pairs of strings, one for each tactic\ninvocation in the Lean core library and in mathlib. The source string is the pretty-printed tactic\nstate. The target string is the tactic invocation as entered by a human author of the source code. This\ndata is gathered by hooking into the Lean parser and Lean’s compilation process. We refer to the task\nof predicting the next human tactic proof step given a tactic state as the proofstep objective.\n3\nPublished as a conference paper at ICLR 2022\n3.2 P ROOF ARTIFACT CO -TRAINING\nIn this section, we describe the PACT task suite and how data for these tasks are extracted.\nFor every proof term τ, we record the type Γ of τ, its name nm, and a list ps of all premises (i.e.\nnamed references to other lemmas in the library) which are used in τ. We then recurse through\nτ, tracking a list bs of bound variables which we update whenever navigating into the body of a\nλ-expression. At every sub-term τ′⊆τ we record τ′, its type Γ′, the current state of bs, and the\nfollowing data:\n1. A tactic state, where the goal is set to be Γ′and the list of hypotheses in the local context is\nset to be the list bs, i.e. those bound variables in scope at τ′.\n2. A partial proof term, i.e. τ with τ′masked out.\n3. A premise selection bitmask, i.e. Boolean labels for every p in ps indicating whether p is\nused in τ′.\n4. A local context bitmask, i.e. similar Boolean labels for every b in bs indicating whether b\nis used in τ′.\n5. An optional next lemma: if the ﬁrst step of τ′is to apply a premise p in ps, we record p.\nWhenever we record a term, we record both pretty-printed and far more explicit fully elaborated\nversions of it. The fully elaborated terms explicitly display enormous amounts of type information\nwhich are usually silently inferred by Lean. From these data, we assemble the following language\nmodeling tasks:\n1. Next lemma prediction. Given the tactic state, predict the next lemma to be applied.\n2. Proof term prediction. Given the tactic state, predict the entire proof term τ′.\n3. Skip-proof. Given the partial proof term, predict the masked subterm τ′.\n4. Type prediction. Given the partial proof term, predict the type Γ′of the masked subterm\nτ′.\n5. Tactic state elaboration. Given the tactic state, predict the fully elaborated tactic state.\n6. Proof term elaboration. Given τ, predict the fully elaborated version of τ.\n7. Premise classiﬁcation. Given the tactic state and a premise p ∈ps, predict either <TRUE>\nor <FALSE> according to the premise selection bitmask.\n8. Local context classiﬁcation. Given the tactic state (which consists of a list of local as-\nsumptions bs and the goal Γ′), predict the sublist of bs which is true on the local context\nbitmask.\n9. Theorem naming. Given the type Γ of the top-level proof term τ, predict the name nm.\nWe remark that our next lemma prediction task is precisely the low-level PROOFSTEP objective\nstudied in (Polu & Sutskever, 2020), and our skip-proof task superﬁcially resembles, but is much\nmore difﬁcult than the skip-tree task studied in (Rabe et al., 2021), as proof terms tend to be far more\ncomplex than the syntax trees of theorem statements.\n3.3 T HE LEAN STEP MACHINE LEARNING ENVIRONMENT\nWe instrument Lean for automatic theorem proving with a language model, including utilities for\n(1) setting the runtime environment at a particular theorem (ensuring proofs are never circular), (2)\nserializing the tactic state as environment observations for a theorem-proving agent, (3) exposing\nLean’s parser to re-parse strings emitted by a language model into tactic invocations, and (4) executing\nand capturing the results of the re-parsed tactics, enabling the recording of trajectories for expert\niteration and reinforcement learning.\nIn addition to this general instrumentation, we implement a generic best-ﬁrst search algorithm for\ntheorem proving; it forms the basis for our evaluations and is written entirely in Lean itself. The\nalgorithm is parametrized by an oracle (Ω : tactic_state → list (string × float))\nthat accepts a tactic state and returns a list of strings and heuristic scores. The search is controlled\n4\nPublished as a conference paper at ICLR 2022\nby a priority queue of search nodes, which consist of a tactic state (i.e. a partial proof) and search\nmetadata. In the outer loop of the algorithm—which continues until either the theorem is completely\nproved (i.e. no goals are remaining on the current node), the priority queue is empty (i.e. the search\nhas failed), or a pre-set timeout or budget of iterations is exceeded—we pop a node off the queue,\nserialize the associated tactic state and use it to query the oracle, producing a list of candidates\ncs : list (string × float). We then loop over the candidates cs to produce a list of new\nsearch nodes, by re-parsing each string into a tactic and adding a new node if the parsed tactic\nadvances the proof without raising errors. These new search nodes are then re-inserted into the\nqueue in order of decreasing priority and the search continues. We optionally constrain the search\nby enforcing maximum width and depth limits wmax and dmax that guard insertion into the queue.\nWhen considering nodes for insertion, any node whose depth exceeds dmax is ignored, and all\nnodes are ignored if the queue size is strictly larger than wmax. Due to the ﬂexibility in assigning\nheuristic scores and in choosing the maximum width and depth hyperparameters, our algorithm is\nquite general—for example, it reduces to (1) a greedy depth-ﬁrst search when wmax = 0, and (2) a\nnaïve breadth-ﬁrst search when heuristic scores are identical and wmax = dmax = ∞.\n4 E XPERIMENTS\nTRAINING In all of our experiments, we use decoder-only Transformers similar to GPT-3 (Brown\net al., 2020). Unless mentioned otherwise, all of our models have 24 layers with dmodel = 1536 and\n24 heads, accruing to 837M trainable parameters. They are also pre-trained on WebMath (Polu &\nSutskever, 2020) for 72B tokens. We use the standard BPE encoding (Brown et al., 2020), a batch\nsize of 512 and a learning rate of 0.00025 with a cosine schedule and a 100-step ramp-up.\nWe use an 80-5-15 train-validation-test split. We split all datapoints deterministically by theorem\nname, by hashing each name to a ﬂoat in (0,1). This ensures, for example, that proof steps used to\nprove a test theorem never appear in the training data and vice-versa.\nWhen ﬁne-tuning a model we load its saved parameters but re-initialize the optimizer. We start each\ntraining for a ﬁxed number of tokens (deﬁning the cosine schedule) and record the number of tokens\nconsumed as we reach a minimal validation loss. We use the minimum validation loss snapshot to\nevaluate each model on our held-out test set.\nWe partition our datasets into three groups:\n1. tactic: the dataset described in Section 3.1.\n2. mix1: the union of the PACT tasks next lemma prediction and proof term predic-\ntion (Section 3.2), selected because of their close relation to tactic.\n3. mix2: all other datasets described in Section 3.2.\nThis grouping is motivated by the impossibility to ablate each dataset separately given our compute\nbudget. They nonetheless enable us to study the effect of tasks that are very close to the tactic\nobjective in comparison to others. Our choice of next lemma prediction and proof term prediction\nfor mix1 is motivated by the observation that these tasks are closely related to the theorem proving\nobjective: a proof can be given entirely in terms of a sequence of lemmas to apply (as in Metamath),\nor the proof can be ﬁnished in one step by supplying the entire proof term. Despite their logical\nsimilarity to the PROOFSTEP objective, we nevertheless use different keywords in the prompt to the\nmodel to disambiguate (NEXTLEMMA and PROOFTERM) from (PROOFSTEP) because the data is\nnoisy and represents a signiﬁcant distribution shift: during pretty-printing, subtrees of proof terms\nbeyond a certain depth are dropped entirely, there is generally no guarantee that they can be re-parsed,\nand the data is much more verbose than what humans typically supply in source code.\nTHEOREM PROVING EVALUATION We run theorem-proving evaluations on our held-out test\nset, comprising 3071 theorems. Since the split was conducted by theorem name, the proofs of these\ntheorems never appear in the training data. For each theorem in the test set, we set the runtime\nenvironment to the location where the theorem is proved in the source code, preventing the use of\ntheorems deﬁned later in mathlib and ensuring that we never derive circular proofs. We compare\nagainst existing proof automation In Lean by also evaluating the tactics refl, which attempts to\nprove statements via deﬁnitional equality, and tidy, which conducts a greedy depth-ﬁrst search\n5\nPublished as a conference paper at ICLR 2022\ntactic\ntactic proof steps GOAL <TacticState> PROOFSTEP <Tactic>\nmix1\nnext lemma prediction GOAL <TacticState> NEXTLEMMA apply (<NextLemma>)\nproof term prediction GOAL <TacticState> PROOFTERM exact (<ProofTerm>)\nmix2\nskip proof RESULT <MaskedProofTerm> SKIPPROOF <ProofTerm>\ntype prediction RESULT <MaskedProofTerm> PREDICTTYPE <Type>\ntactic state elaboration GOAL <TacticState> ELABGOAL <ElaboratedTacticState>\nproof term elaboration PROOFTERM <ProofTerm> ELABPROOFTERM <ElaboratedProofTerm>\npremise classiﬁcation GOAL <TacticState> CLASSIFYPREMISE <Premise> <True|False>\nlocal context classiﬁcation GOAL <TacticState> CLASSIFYLOCALS <LocalsList>\ntheorem naming TYPE <Type> NAME <Name>\nFigure 1: Auto-regressive objectives used for each task described in Section 3. Placeholders repre-\nsented with brackets (such as <TacticState>) are substituted by the context-completion pairs\nfrom each datasets in the prompts above. Each task is presented to the model with its respective key-\nword (PROOFSTEP, NEXTLEMMA,...). We wrap the completions ofmix1 tasks (with apply(...)\nand exact(...) respectively) as a hint that they are related to the respective Lean tactics; this is\nnot directly possible for the other tasks.\nusing a ﬁxed list of tactics at each step. We re-implement tidy as a special case of our best-ﬁrst\nsearch algorithm using an oracle which always emits the same list of tactics, and so henceforth\nrefer to it as tidy-bfs. In all of our experiments, we use a maximum width of wmax = 16, a\nmaximum depth of dmax = 128, a maximum budget of 512 iterations of the outer loop, a timeout of\n5 seconds per tactic execution, and a global timeout of 600 seconds per theorem. Because sampling\ncompletions from our models is much slower (≈1 second) than querying the constant tidy-bfs\noracle (instantaneous), the tidy-bfs search runs many more iterations than gptf before timeout.\nWe report the pass-rate (i.e. percentage of theorems proved) from the randomly-chosen held-out test\nset, following (Whalen, 2016), (Bansal et al., 2019c), and others. We provide an alternative pass-rate\nat the end of this section, using theorems added to mathlib after our dataset was collected. We\naverage over three evaluation runs when reporting the pass rate.\nEFFECT OF CO -TRAINING VS PRE -TRAINING We ﬁrst study the effects of pre-training versus\nco-training with the mix1 and mix2 datasets. We pre-train using the methodology described above\n(potentially pre-training ﬁrst on WebMath, and then on a PACT dataset in sequence). For co-training,\nwe simply concatenate and shufﬂe the datasets together without applying any particular weight to a\ngiven dataset.\nThe main results are presented in Figure 2. Pre-training exhibits an effective transfer from mix-1\nand/or mix-2 but the best result is achieved by co-training with both these datasets. With this\nsetup, we are able to train for much longer (71B tokens vs 22B+18B for the best pre-training setup)\nbefore overﬁtting on the PROOFSTEP task. We hypothesize that PACT regularizes overﬁtting to\nthe PROOFSTEP task while still imparting useful knowledge to the model due to large amounts of\nmutual information, and that this is the main driver of increased performance.\nABLATING WE BMA T HPRE -TRAINING Next, we ablate the effect ofWebMath pre-training (instead\nstarting with a model pre-trained on the same English language mix as GPT-3). As expected, co-\ntrained models suffer from a performance drop without Webmath pretraining. but we were more\ninterested in measuring the effect on pre-trained models on mix-1 and mix-2, as they may not\nbeneﬁt from WebMath as much due to the two successive pre-training steps.\nWe report the optimal validation losses in Figure 3. WebMath appears as substantially beneﬁcial\neven in the sequential pre-training setup. This indicates that PACT is not a replacement forWebMath\npre-training, but rather a complementary method for enhancing the performance of language models\nfor theorem proving.\n6\nPublished as a conference paper at ICLR 2022\nTokens\nModel elapsed mix1 mix2 tactic Pass-rate\nBaselines\nrefl 1.1%\ntidy-bfs 9.9%\nWebMath > tactic 1B 1.02 32.2%\nPre-training\nWebMath > mix1 11B 0.08\nWebMath > mix2 16B 0.08\nWebMath > mix1 + mix2 22B 0.11 0.08\nWebMath > mix1 > tactic 1B 1.00 39.8%\nWebMath > mix1 + mix2 > tactic 1B 0.97 44.0%\nCo-training (PACT)\nWebMath > mix1 + tactic 18B 0.08 0.94 40.0%\nWebMath > mix2 + tactic 75B 0.09 0.93 46.0%\nWebMath > mix1 + mix2 + tactic 71B 0.09 0.09 0.91 48.4 %\nPre-training and co-training\nWebMath > mix2 > mix1 + tactic 18B 0.08 0.93 46.9%\nFigure 2: Comparison of pre-training and co-training onmix-1 and mix-2. > denotes a pre-training\nstep and + denotes a co-training. As an example, WebMath > mix2 > mix1 + tactic\nsigniﬁes a model successively pre-trained on WebMath then mix2 and ﬁnally co-trained as a ﬁne-\ntuning step on mix1 and tactic. Columns mix1, mix2, tactic report the optimal validation\nloss achieved on these respective datasets. We provide a detailed description of experiment runtime\nand computing infrastructure in Appendix B.\nTokens Tokens\nModel budget elapsed mix1 mix2 tactic Pass-rate†\nBaselines\ntactic 32B 1B 1.59 —\nPre-training\nmix1 32B 20B 0.12\nmix2 32B 25B 0.10\nmix1 + mix2 32B 27B 0.13 0.10\nmix1 > tactic 32B 1B 1.26 —\nmix1 + mix2 > tactic 32B 1B 1.16 —\nCo-training\nmix1 + tactic 32B 27B 0.11 1.12 —\nmix2 + tactic 96B 75B 0.10 1.02 40.4%\nmix1 + mix2 + tactic 96B 71B 0.10 0.11 1.07 —\nPre-training and co-training\nmix2 > mix1 + tactic 32B 26B 0.11 1.09 —\nFigure 3: Validation losses achieved in the pre-training and co-training setups without WebMath\npre-training. See Figure 2 for a description of the columns and the models nomenclature used. †Due\nto technical constraints, we are unable to provide pass-rates for some of the models.\nABLATING REGULARIZATION We rule out the possibility that the beneﬁts from PACT come from\nsimply regularizing our models on the scarce tactic data alone. We checked that a WebMath >\ntactic model trained with 15% residual dropout achieved a minimum validation loss of 1.01 and\n33.6% pass rate, far below the 48.4% PACT pass rate.\n7\nPublished as a conference paper at ICLR 2022\nTokens Tokens\nModel budget elapsed mix1 mix2 tactic Pass-rate\n121m 96B 82B 0.13 0.10 1.23 35.1%\n163m 96B 80B 0.12 0.09 1.11 39.8%\n837m 96B 71B 0.09 0.09 0.91 48.4 %\nFigure 4: Validation losses and pass-rates achieved for various model sizes using PACT. See Figure 2\nfor a description of the columns. The setup used is WebMath > mix1 + mix2 + tactic.\nEFFECT OF MODEL SIZE Finally, we study how performance scales with respect to model size. We\nuse the best training setup reported in Figure 2, WebMath > mix1 + mix2 + tactic. The\n837m model is our main model. The 163m and 121m models respectively have 12 and 6 layers,\nwith dmodel = 768. The learning rates are respectively adjusted to 0.0014 and 0.0016.\nAs demonstrated by Figure 4, performance is highly correlated with model size, with larger models\ngenerally achieving better generalization even in the overﬁtted regime. We leave as future work a\ncareful study of how evaluation performance is affected when scaling to multi-billion parameter\nmodels, as well as the feasibility of deploying them for interactive use by Lean users.\nTIME -STRATIFIED EVALUATION In the 5 week period that separated our last dataset extraction\nand the writing of this paper, mathlib grew by 30K lines of code, adding 2807 new theorems.\nEvaluating our models on these new theorem statements gives a unique way to assess their capability\nto assist humans in formalizing proofs and to test their generalization to completely unseen theorems\nand deﬁnitions. This evaluation set also addresses one of the weaknesses of using a random split\nof theorems from a formal mathematics library, namely that the split is non-chronological; e.g. test\ntheorems can appear as lemmas in proofs of train theorems.\nWe call this temporally held-out test set future-mathlib and evaluate our best model as well\nas the refl and tidy-bfs baselines on it. In contrast to evaluation on our test split, the refl\nbaseline (simply attempting a proof by the refl tactic) closes 328 proofs (11.6%), demonstrating\nan important skew towards trivial boilerplate lemmas generally deﬁned to provide alternate inter-\nfaces to new deﬁnitions. The tidy-bfs baseline closed 611 proofs (21.8%), and our best model\nwm-tt-m1-m2 closed 1043 proofs (37.1%), proving 94% of the refl lemmas. We attribute the\nweaker performance to heavy distribution shift: by the nature of the dataset, the future-mathlib\ntheorems frequently involve new deﬁnitions and concepts which the model was never exposed to\nduring training. Nevertheless, the success rate remains high enough to suggest strong generalization\nand usefulness at the frontier of formalized mathematics.\n5 D ISCUSSION\nCHAINED TACTIC PREDICTIONS In Lean, multiple tactic commands can be chained together using\nsemicolons. Our data pipeline treats these tactic chains as a single sequence in our training data, and\nthey are occasionally predicted by the model. Such chained tactic applications are difﬁcult for human\nformalizers to synthesize on their own, as they require reasoning about the semantics of multiple\ntactics in sequence and their effects on the tactic state, and the examples present in the training data\nare usually optimized by hand from longer, less succinct proofs. We observed that PACT signiﬁcantly\nboosts the capability of our models to successfully predict longer chained tactic applications. This\noccurs despite the fact that the tactic chaining idiom is speciﬁc to the tactic proofstep dataset and\ndoes not appear in the PACT data whatsoever. We supply more detail in Appendix C.1.\nTHEOREM NAMING We also evaluate our best PACT model (wm-to-tt-m1-m2) on the theorem\nnaming task, using the theorem statements and human-supplied names from the future-mathlib\nevaluation set. It achieved 20% acc@1, 27% acc@10, and 30% acc@16. An inspection of its outputs\nreveals that even when its predictions diverge from the ground truth, they are often idiomatic and\nsemantically correct alternatives. We supply more detail in Appendix C.2.\n8\nPublished as a conference paper at ICLR 2022\nIMPACT ON LEAN COMMUNITY Lean’smathlib (mathlib, 2020) is a rapidly growing open\nsource library of formal mathematics which has grown considerably in size each year for the past\nfour years.1 Our work has been welcomed by members of this community, with Lean power users\ndescribing some of the new proofs found by GPT-f as “nontrivial” and “clever”. More than one-third\nof the proofs found by our models are shorter and produce smaller proof terms (sometimes by several\norders of magnitude) than the ground truth. Manually inspecting a small, non-cherry picked sample\nof these shorter proofs has led to 19 GPT-f co-authored commits tomathlib, some of which reduce\nproof term sizes and theorem compilation times by an order of magnitude (see Appendix D).\nPOTENTIAL SOCIETAL IMPACT Strong automated reasoning systems have enormous potential\nimpact for mathematical research and scientiﬁc progress in other disciplines. The methods that we\ndiscuss in this paper could accelerate the development of strong automated reasoning systems. We\nhave also observed that our language models absorb stylistic biases from their training data which\ncould be ampliﬁed via reinforcement learning. However, since we focus on mathematics codiﬁed in\nproof assistants, we believe that there is little immediate negative societal impact from our work.\nFUTURE DIRECTIONS There are many elaborations on the training data, training methodology, and\ntree search wrapping lean-gptf which can be reasonably expected to improve its performance\nat theorem proving. Our dataset can be synthetically augmented using similar methods as (Polu &\nSutskever, 2020). Our dataset could be cleaned further, and proofs minimized. Merely making the\ndecoded rewrites robust by only using the largest preﬁx of successful rewrites signiﬁcantly boosts the\nsuccess rate of suggested rewrites. In a similar vein, predicted lemmas generated as arguments to\nunsuccessful tactic applications could be cached and re-used as hints for an intermittently-queried\nhammer. The increased success rate of chained tactic predictions mentioned above shows the\nfeasibility of having language models perform multiple reasoning steps in a single query, potentially\nimproving the efﬁciency of the proof search. From the experiments described in Section 4, it is clear\nthat the composition of the dataset used for co-training signiﬁcantly affects performance on theorem\nproving. Although we uniformly sampled across all co-training tasks, it would be interesting to\noptimize a dynamic mixture schedule, perhaps annealing towards a desired task.\nCONCLUSION There is a sense in which PACT is merely an application of the well known principle\nthat compute in the form of search should be exchanged for training signal whenever possible. In\nLean, typeclass inference relies on a backtracking Prolog-style search; the elaborator performs search\nto disambiguate overloaded notation and infer types; Lean tactics have complex semantics precisely\nbecause they can perform search to ﬁnd subproofs automatically. The work done by these subroutines\nis preserved in the proof artifacts, and PACT can be viewed as a way of extracting this information\nofﬂine for more training signal.\nWe have presented PACT as a way of addressing the data scarcity issue for learning theorem proving\nfrom human tactic scripts in proof assistant libraries. Another well-studied solution for this is expert\niteration and reinforcement learning. In the setting of HOL Light, and under the assumption of a\nhardcoded ﬁnite action space of tactics, Bansal et al. (2019a) in conjunction with supervised seed\ndata was able to achieve up to 70% proof success rate on the HOList theorem proving task. Similarly,\nin a set-up much closer to ours, MM GPT-f demonstrated the feasibility of expert iteration when\nusing generative language models for theorem proving.\nWithin a ﬁxed corpus of theorems (and hence proof terms), however, both PACT and RL are\nfundamentally constrained by a lack of exploration—as the performance of the theorem proving\nagent improves, it will eventually saturate and become starved for data, and its curriculum will need\nto be expanded. Although self-supervised methods such as PACT represent a way to signiﬁcantly\nimprove the data-efﬁciency of reinforcement learning loops over existing theorem prover libraries, the\ndevelopment of continuously self-improving and inﬁnitely scalable neural theorem provers remains\ncontingent on sufﬁciently powerful exploration and automated curriculum generation; we consider\nthese challenges to be of paramount importance.\n1See https://leanprover-community.github.io/mathlib_stats.html for up-to-date\nstatistics on mathlib’s size and growth over time.\n9\nPublished as a conference paper at ICLR 2022\n6 A CKNOWLEDGMENTS\nWe thank the members of the Lean community, in particular Kevin Buzzard, Simon Hudon, Johan\nCommelin, Mario Carneiro, Bhavik Mehta, and Gabriel Ebner for their valuable feedback on our\nwork. We are indebted to Markus Rabe and Christian Szegedy for many hours of helpful discussion.\nWe also thank Daniel Selsam, Tom Hales, and Josef Urban for feedback on earlier drafts of this\npaper.\n7 R EPRODUCIBILITY STATEMENT\nThe source code used to generate the Lean datasets and run the evaluation is open source and made\navailable in the following repositories:\nLean theorem proving environment :\nhttps://github.com/jesse-michael-han/lean-tpe-public\nTactic step data pipeline :\nhttps://github.com/jasonrute/lean_proof_recording\nPACT data pipeline :\nhttps://github.com/jesse-michael-han/lean-step-public\nOur Transformer model was pre-trained on two proprietary datasets. The ﬁrst is the same mix used\nby GPT-3 (Brown et al., 2020) and the second is WebMath (Polu & Sutskever, 2020). More details\ncan be found in Appendix B.\nWhile our weights and the API through which we query our models are not currently public, tech-\nniques for training decoder-only transformers and efﬁciently performing inference with them are\nwell-known. Our released theorem proving code is agnostic to these implementation details and will\nwork with any language model exposed via an HTTP server. The provided code also supports query-\ning a locally hosted Transformer from the open-source library fairseq via the Fairseq CLI (Ott\net al., 2019).\nWe have released a simpliﬁed version of the proof search described in Section 3.3 as a tactic to\nthe Lean community in a public beta, opening the way for our models to directly accelerate the\ndevelopment of formalized mathematics and for human experts to provide feedback and additional\ntraining signal in a virtuous cycle. The tactic and code are available at https://github.com/\njesse-michael-han/lean-gptf , and users who sign up for the beta are granted access to\nour Transformer model through an API.\nREFERENCES\nKshitij Bansal, Sarah M. Loos, Markus N. Rabe, and Christian Szegedy. Learning to reason in large\ntheories without imitation. CoRR, abs/1905.10501, 2019a. URL http://arxiv.org/abs/\n1905.10501.\nKshitij Bansal, Sarah M. Loos, Markus N. Rabe, Christian Szegedy, and Stewart Wilcox. Holist: An\nenvironment for machine learning of higher order logic theorem proving. In Kamalika Chaudhuri\nand Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine\nLearning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings\nof Machine Learning Research, pp. 454–463. PMLR, 2019b. URL http://proceedings.\nmlr.press/v97/bansal19a.html.\nKshitij Bansal, Sarah M. Loos, Markus N. Rabe, Christian Szegedy, and Stewart Wilcox. Holist: An\nenvironment for machine learning of higher-order theorem proving (extended version). CoRR,\nabs/1904.03241, 2019c. URL http://arxiv.org/abs/1904.03241.\nLasse Blaauwbroek, Josef Urban, and Herman Geuvers. The tactician - A seamless, interactive\ntactic learner and prover for coq. In Christoph Benzmüller and Bruce R. Miller (eds.), Intelligent\nComputer Mathematics - 13th International Conference, CICM 2020, Bertinoro, Italy, July 26-\n31, 2020, Proceedings , volume 12236 of Lecture Notes in Computer Science , pp. 271–277.\n10\nPublished as a conference paper at ICLR 2022\nSpringer, 2020. doi: 10.1007/978-3-030-53518-6\\_17. URL https://doi.org/10.1007/\n978-3-030-53518-6_17 .\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-\nwal, Ariel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCan-\ndlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot\nlearners. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,\nand Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual\nConference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\n1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.\nKevin Buzzard, Johan Commelin, and Patrick Massot. Formalising perfectoid spaces. In Jas-\nmin Blanchette and Catalin Hritcu (eds.), Proceedings of the 9th ACM SIGPLAN Interna-\ntional Conference on Certiﬁed Programs and Proofs, CPP 2020, New Orleans, LA, USA,\nJanuary 20-21, 2020 , pp. 299–312. ACM, 2020. doi: 10.1145/3372885.3373830. URL\nhttps://doi.org/10.1145/3372885.3373830.\nKevin Buzzard, Chris Hughes, Kenny Lau, Amelia Livingston, Ramon Fernández Mir, and Scott\nMorrison. Schemes in lean. Experimental Mathematics, 0(0):1–9, 2021. doi: 10.1080/10586458.\n2021.1983489. URL https://doi.org/10.1080/10586458.2021.1983489.\nFrançois Charton, Amaury Hayat, and Guillaume Lample. Learning advanced mathematical\ncomputations from examples. In 9th International Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021. URL https:\n//openreview.net/forum?id=-gfhS00XfKj.\nLeonardo Mendonça de Moura, Soonho Kong, Jeremy Avigad, Floris van Doorn, and Jakob von\nRaumer. The lean theorem prover (system description). In Amy P. Felty and Aart Middeldorp\n(eds.), Automated Deduction - CADE-25 - 25th International Conference on Automated Deduction,\nBerlin, Germany, August 1-7, 2015, Proceedings , volume 9195 of Lecture Notes in Computer\nScience, pp. 378–388. Springer, 2015. doi: 10.1007/978-3-319-21401-6\\_26. URL https:\n//doi.org/10.1007/978-3-319-21401-6_26 .\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\nand Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale.\nIn 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,\nMay 3-7, 2021 . OpenReview.net, 2021. URL https://openreview.net/forum?id=\nYicbFdNTTy.\nGabriel Ebner, Sebastian Ullrich, Jared Roesch, Jeremy Avigad, and Leonardo de Moura. A metapro-\ngramming framework for formal veriﬁcation. Proc. ACM Program. Lang., 1(ICFP):34:1–34:29,\n2017. doi: 10.1145/3110278. URL https://doi.org/10.1145/3110278.\nM. Ganesalingam and W. T. Gowers. A fully automatic theorem prover with human-style output.\nJ. Autom. Reason. , 58(2):253–291, 2017. doi: 10.1007/s10817-016-9377-1. URL https:\n//doi.org/10.1007/s10817-016-9377-1 .\nThibault Gauthier and Cezary Kaliszyk. Sharing HOL4 and HOL light proof knowledge. In Martin\nDavis, Ansgar Fehnker, Annabelle McIver, and Andrei V oronkov (eds.),Logic for Programming,\nArtiﬁcial Intelligence, and Reasoning - 20th International Conference, LPAR-20 2015, Suva, Fiji,\nNovember 24-28, 2015, Proceedings, volume 9450 of Lecture Notes in Computer Science , pp.\n372–386. Springer, 2015. doi: 10.1007/978-3-662-48899-7\\_26. URL https://doi.org/\n10.1007/978-3-662-48899-7_26 .\nThibault Gauthier, Cezary Kaliszyk, Josef Urban, Ramana Kumar, and Michael Norrish. Tactic-\ntoe: Learning to prove with tactics. J. Autom. Reason., 65(2):257–286, 2021. doi: 10.1007/\ns10817-020-09580-x. URL https://doi.org/10.1007/s10817-020-09580-x .\n11\nPublished as a conference paper at ICLR 2022\nChristopher Hahn, Frederik Schmitt, Jens U. Kreber, Markus Norman Rabe, and Bernd Finkbeiner.\nTeaching temporal logics to neural networks. In 9th International Conference on Learning\nRepresentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL\nhttps://openreview.net/forum?id=dOcQK-f4byz.\nJesse Michael Han and Floris van Doorn. A formal proof of the independence of the continuum\nhypothesis. In Jasmin Blanchette and Catalin Hritcu (eds.), Proceedings of the 9th ACM SIGPLAN\nInternational Conference on Certiﬁed Programs and Proofs, CPP 2020, New Orleans, LA, USA,\nJanuary 20-21, 2020, pp. 353–366. ACM, 2020. doi: 10.1145/3372885.3373826. URL https:\n//doi.org/10.1145/3372885.3373826.\nTom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo\nJun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford,\nAditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam McCan-\ndlish. Scaling laws for autoregressive generative modeling. CoRR, abs/2010.14701, 2020. URL\nhttps://arxiv.org/abs/2010.14701.\nDaniel Huang, Prafulla Dhariwal, Dawn Song, and Ilya Sutskever. Gamepad: A learning environment\nfor theorem proving. In 7th International Conference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.\nnet/forum?id=r1xwKoR9Y7.\nCezary Kaliszyk and Josef Urban. Mizar 40 for mizar 40. J. Autom. Reason. , 55(3):245–\n256, 2015a. doi: 10.1007/s10817-015-9330-8. URL https://doi.org/10.1007/\ns10817-015-9330-8 .\nCezary Kaliszyk and Josef Urban. Learning-assisted theorem proving with millions of lemmas. J.\nSymb. Comput., 69:109–128, 2015b. doi: 10.1016/j.jsc.2014.09.032. URL https://doi.org/\n10.1016/j.jsc.2014.09.032.\nCezary Kaliszyk, Josef Urban, and Jirí Vyskocil. Lemmatization for stronger reasoning in large\ntheories. In Carsten Lutz and Silvio Ranise (eds.), Frontiers of Combining Systems - 10th In-\nternational Symposium, FroCoS 2015, Wroclaw, Poland, September 21-24, 2015. Proceedings,\nvolume 9322 of Lecture Notes in Computer Science, pp. 341–356. Springer, 2015. doi: 10.1007/\n978-3-319-24246-0\\_21. URL https://doi.org/10.1007/978-3-319-24246-0_\n21.\nCezary Kaliszyk, François Chollet, and Christian Szegedy. Holstep: A machine learning dataset for\nhigher-order logic theorem proving. In 5th International Conference on Learning Representations,\nICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net,\n2017. URL https://openreview.net/forum?id=ryuxYmvel.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.\nCoRR, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361.\nGuillaume Lample and François Charton. Deep learning for symbolic mathematics. In 8th\nInternational Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=\nS1eZYeHFDS.\nWenda Li, Lei Yu, Yuhuai Wu, and Lawrence C. Paulson. Isarstep: a benchmark for high-level\nmathematical reasoning. In International Conference on Learning Representations, 2021. URL\nhttps://openreview.net/forum?id=Pzj6fzU6wkj.\nmathlib. The lean mathematical library. In Jasmin Blanchette and Catalin Hritcu (eds.), Proceedings\nof the 9th ACM SIGPLAN International Conference on Certiﬁed Programs and Proofs, CPP 2020,\nNew Orleans, LA, USA, January 20-21, 2020, pp. 367–381. ACM, 2020. doi: 10.1145/3372885.\n3373824. URL https://doi.org/10.1145/3372885.3373824.\nPengyu Nie, Karl Palmskog, Junyi Jessy Li, and Milos Gligoric. Deep generation of coq lemma\nnames using elaborated terms. In Nicolas Peltier and Viorica Sofronie-Stokkermans (eds.), Au-\ntomated Reasoning - 10th International Joint Conference, IJCAR 2020, Paris, France, July 1-4,\n12\nPublished as a conference paper at ICLR 2022\n2020, Proceedings, Part II, volume 12167 of Lecture Notes in Computer Science , pp. 97–118.\nSpringer, 2020. doi: 10.1007/978-3-030-51054-1\\_6. URL https://doi.org/10.1007/\n978-3-030-51054-1_6 .\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Waleed Ammar, Annie\nLouis, and Nasrin Mostafazadeh (eds.), Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-\nHLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Demonstrations, pp. 48–53. Association for\nComputational Linguistics, 2019. doi: 10.18653/v1/n19-4009. URL https://doi.org/10.\n18653/v1/n19-4009.\nAditya Paliwal, Sarah M. Loos, Markus N. Rabe, Kshitij Bansal, and Christian Szegedy. Graph\nrepresentations for higher-order logic and theorem proving. In The Thirty-Fourth AAAI Conference\non Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artiﬁcial Intel-\nligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artiﬁcial\nIntelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 2967–2974. AAAI Press,\n2020. URL https://aaai.org/ojs/index.php/AAAI/article/view/5689.\nFrank Pfenning and Christine Paulin-Mohring. Inductively deﬁned types in the calculus of construc-\ntions. In Michael G. Main, Austin Melton, Michael W. Mislove, and David A. Schmidt (eds.),\nMathematical Foundations of Programming Semantics, 5th International Conference, Tulane\nUniversity, New Orleans, Louisiana, USA, March 29 - April 1, 1989, Proceedings, volume 442\nof Lecture Notes in Computer Science, pp. 209–228. Springer, 1989. doi: 10.1007/BFb0040259.\nURL https://doi.org/10.1007/BFb0040259.\nStanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving.\nCoRR, abs/2009.03393, 2020. URL https://arxiv.org/abs/2009.03393.\nMarkus Norman Rabe, Dennis Lee, Kshitij Bansal, and Christian Szegedy. Mathematical reasoning\nvia self-supervised skip-tree training. In 9th International Conference on Learning Representations,\nICLR 2021, Virtual Event, Austria, May 3-7, 2021 . OpenReview.net, 2021. URL https://\nopenreview.net/forum?id=YmqAnY0CMEy.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\nLearning transferable visual models from natural language supervision. In Marina Meila and\nTong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning,\nICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning\nResearch, pp. 8748–8763. PMLR, 2021. URL http://proceedings.mlr.press/v139/\nradford21a.html.\nAlex Sanchez-Stern, Yousef Alhessi, Lawrence K. Saul, and Sorin Lerner. Generating correctness\nproofs with neural networks. In Koushik Sen and Mayur Naik (eds.), Proceedings of the 4th\nACM SIGPLAN International Workshop on Machine Learning and Programming Languages,\nMAPL@PLDI 2020, London, UK, June 15, 2020, pp. 1–10. ACM, 2020. doi: 10.1145/3394450.\n3397466. URL https://doi.org/10.1145/3394450.3397466.\nPeter Scholze. Liquid tensor experiment. https://xenaproject.wordpress.com/\n2020/12/05/liquid-tensor-experiment/, 2020. Formalization available at https:\n//github.com/leanprover-community/lean-liquid.\nJosef Urban and Jan Jakubuv. First neural conjecturing datasets and experiments. In Christoph\nBenzmüller and Bruce R. Miller (eds.), Intelligent Computer Mathematics - 13th International\nConference, CICM 2020, Bertinoro, Italy, July 26-31, 2020, Proceedings, volume 12236 of Lecture\nNotes in Computer Science, pp. 315–323. Springer, 2020. doi: 10.1007/978-3-030-53518-6\\_24.\nURL https://doi.org/10.1007/978-3-030-53518-6_24 .\nMingzhe Wang and Jia Deng. Learning to prove theorems by learning to generate theo-\nrems. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and\nHsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,\n13\nPublished as a conference paper at ICLR 2022\n2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/\nd2a27e83d429f0dcae6b937cf440aeb1-Abstract.html.\nQingxiang Wang, Cezary Kaliszyk, and Josef Urban. First experiments with neural translation of\ninformal to formal mathematics. In Florian Rabe, William M. Farmer, Grant O. Passmore, and\nAbdou Youssef (eds.), Intelligent Computer Mathematics - 11th International Conference, CICM\n2018, Hagenberg, Austria, August 13-17, 2018, Proceedings, volume 11006 of Lecture Notes in\nComputer Science, pp. 255–270. Springer, 2018. doi: 10.1007/978-3-319-96812-4\\_22. URL\nhttps://doi.org/10.1007/978-3-319-96812-4_22 .\nQingxiang Wang, Chad E. Brown, Cezary Kaliszyk, and Josef Urban. Exploration of neural machine\ntranslation in autoformalization of mathematics in mizar. In Jasmin Blanchette and Catalin Hritcu\n(eds.), Proceedings of the 9th ACM SIGPLAN International Conference on Certiﬁed Programs\nand Proofs, CPP 2020, New Orleans, LA, USA, January 20-21, 2020, pp. 85–98. ACM, 2020. doi:\n10.1145/3372885.3373827. URL https://doi.org/10.1145/3372885.3373827.\nDaniel Whalen. Holophrasm: a neural automated theorem prover for higher-order logic. CoRR,\nabs/1608.02644, 2016. URL http://arxiv.org/abs/1608.02644.\nFreek Wiedijk. The De Bruijn factor, 2000. URL http://www.cs.ru.nl/F.Wiedijk/\nfactor/factor.pdf.\nMinchao Wu, Michael Norrish, Christian Walder, and Amir Dezfouli. Tacticzero: Learning to prove\ntheorems from scratch with deep reinforcement learning. In A. Beygelzimer, Y . Dauphin, P. Liang,\nand J. Wortman Vaughan (eds.),Advances in Neural Information Processing Systems, 2021a. URL\nhttps://openreview.net/forum?id=edmYVRkYZv.\nYuhuai Wu, Markus N. Rabe, Wenda Li, Jimmy Ba, Roger B. Grosse, and Christian Szegedy. LIME:\nlearning inductive bias for primitives of mathematical reasoning. In Marina Meila and Tong Zhang\n(eds.), Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24\nJuly 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pp. 11251–\n11262. PMLR, 2021b. URL http://proceedings.mlr.press/v139/wu21c.html.\nKaiyu Yang and Jia Deng. Learning to prove theorems via interacting with proof assistants. In\nKamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International\nConference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA ,\nvolume 97 of Proceedings of Machine Learning Research, pp. 6984–6994. PMLR, 2019. URL\nhttp://proceedings.mlr.press/v97/yang19a.html.\n14\nPublished as a conference paper at ICLR 2022\nA A DDITIONAL BACKGROUND\nPROOF TERMS Lean’s fundamental logic is a dependent type theory called the calculus of inductive\nconstructions Pfenning & Paulin-Mohring (1989). This design means that terms (4, x+ y, f), types\n(N, list Z, α→β) and proofs are all represented with a single datatype called an expression. Given\nan environment of available constants and deﬁnitions and a context Γ of variables, Lean can infer\na type α for each well-formed expression t. A proof term is a Lean expression whose type is a\nproposition. This proof term serves as a checkable artifact for verifying the proposition. Lean uses a\nsmall, trusted kernel to verify proof terms.\nTACTICS Tactics in Lean are metaprograms Ebner et al. (2017), which can construct Lean expres-\nsions, such as terms. A tactic state which tracks the list of open goals and other metadata is threaded\nthrough each tactic invocation. Lean has special support for treating tactics as an extensible domain-\nspeciﬁc language (DSL); this DSL is how Lean is typically used as an interactive theorem prover. The\nDSL amounts to a linear chain of comma-separated invocations. The process of interactive proving is\nmediated through Lean’s language server, which will present the context and type for the current goal\nin the proof to the user, dependent on where their cursor is in the source text. The tactic prediction\ntask is to predict the next tactic given this goal state. We extract supervised training data for this task\nby extracting all human-supplied proof steps from Lean’smathlib.\nAn object called the tactic state is threaded through each invocation of a tactic. Among other things,\nthe tactic state maintains a context of metavariables: placeholders in to which expressions will be\nsubstituted later. At each point in the proof, one or more of these metavariables are selected as the\ngoal of the tactic state which is present As the proof progresses, there are multiple values to be found\nEXAMPLE Consider this (modiﬁed) example of a tactic proof from the library.\ntheorem int.sub_ne_zero_of_ne : ∀ (a b : Z), a ̸= b -> a - b ̸= 0 :=\nbegin\nintros a b h hab,\napply h,\napply int.eq_of_sub_eq_zero hab,\nend\nEach tactic line modiﬁes the proof state, which we explicitly annotate below with comments between\neach tactic.\ntheorem int.sub_ne_zero_of_ne : ∀ (a b : Z), a ̸= b -> a - b ̸= 0 :=\nbegin\n-- ⊢ ∀(a b : Z), a ̸= b → a - b ̸= 0\nintros a b h hab,\n-- a b : Z,\n-- h : a ̸= b,\n-- hab : a - b = 0\n-- ⊢ false\napply h,\n-- a b : Z,\n-- h : a ̸= b,\n-- hab : a - b = 0\n-- ⊢ a = b\napply int.eq_of_sub_eq_zero hab,\n-- no goals\nend\nOur proofstep objective is to predict the tactic applied to a given tactic state.\nLean stores this proof internally as a proof term:\ntheorem int.sub_ne_zero_of_ne : ∀ (a b : Z), a ̸= b → a - b ̸= 0 :=\nλ (a b : Z) (h : a ̸= b), id (λ (hab : a - b = 0), h\n(int.eq_of_sub_eq_zero hab))\nSince this proof term is just stored internally as a tree, any branch of this term tree can be removed,\nto create a hole _, for example:\n15\nPublished as a conference paper at ICLR 2022\nλ (a b : Z) (h : a ̸= b), id (λ (hab : a - b = 0), h _)\nLean will automatically provide a list of both the local context and the type of a term needed to ﬁll\nthat hole as shown below. Notice this is the same as a tactic state we saw from the term proof above.\na b : Z,\nh : a ̸= b,\nhab : a - b = 0\n⊢ a = b\nUsing this methodology of following proof term trees, we can mine low level proof data for every\nnode of a term proof to produce the PACT dataset described in Section 3.2.\nB D ATASETS\nB.1 P RE-TRAINING DATASETS\nWe pre-train on WebMath as described in (Polu & Sutskever, 2020). All models, including the\nWebMath pre-trained models, and the non- WebMath models used in ablations, were ﬁrst pre-\ntrained on the mix used by GPT-3 (Brown et al., 2020) which includes a ﬁltered CommonCrawl,\nWebText2, Book1, Book2 and Wikipedia. WebMath includes Python-only GitHub data, as\nwell as arXiv and Math StackExchange.\nFrom these datasets, a potential risk for test-set contamination (presence of mathlib) exists for the\ncrawled datasets, namely CommonCrawl, WebText2, and (in case of a ﬁltering bug) Python-only\nGitHub. The other datasets (in particulararXiv and Math StackExchange) may contain short\nreferences of mathlib code but in shape and forms that would not lead to effective contamination.\nTo assess the contamination risk related with the crawled datasets, we searched CommonCrawl,\nWebText2, arXiv, Python-only GitHub, and Math StackExchange for test theorems. For\nexample, given the test theorem nat.div_eq_sub_div we searched for any occurrences of the\nstring div_eq_sub_div. Of over 3000 test theorem names, we found 595 which occurred in the\ndatasets. Many instances were innocuous, but some were in Lean ﬁles, and in some cases there was a\nproof of a test theorem. There were also 160 additional test theorems with no underscore in their\nname, which we did not check, but whose name is likely to be found in the datasets. (There is no need\nto check for training theorems since they are already in the training data and it would not constitute\ncontamination.) We re-calculated the pass-rates of the results in Figure 2 omitting these 755 test\ntheorems. This decreases the reported pass-rates slightly, ranging from 0.6 to 1.1 percentage points.\nThe adjusted pass-rate of our best model WebMath > mix1 + mix2 + tactic is 47.4%,a\ndecrease of 1 percentage point. Our main results still hold even with the adjusted pass-rates.\nAdditionally we also look at the results for the 1,350 test theorems in our dataset that were added\nto Lean and mathlib after April 18, 2020, which is after CommonCrawl and WebText2 were\ngathered, and the 544 test theorems added after September 11, 2020, which is after WebMath was\ngathered. Unlike future-mathlib, these theorems were part of the originally extracted data. The\npass-rates for the WebMath > mix1 + mix2 + tactic model on these restricted sets of test\ntheorems are 45.6% and 43.3%, respectively.\nWe also looked for the following Metamath speciﬁc and HOL speciﬁc strings in CommonCrawl,\nWebText2, and Python-only GitHub:\nMetamath:\n\"( ph -> A = C )\"\n\"( ph -> A R C )\"\n\"( sqrt ‵ 2 ) e/ QQ\"\nHOL:\n\"apply (rule \"\n\"apply (drule \"\nWe found 0 occurrence of the Metamath-related strings but interestingly found a non-negligible\namount of HOL-related documents, which does not constitute a test-set contamination but potentially\nbeneﬁts the downstream tasks studied in this paper.\n16\nPublished as a conference paper at ICLR 2022\nWhile our results show a signiﬁcant beneﬁt to pre-training on WebMath, it is unclear exactly\nhow pre-training helps. Since Lean’s theorem names are made of coded mathematical phases,\ne.g. affine.simplex.dist_circumcenter_eq_circumradius, it is not unreasonable\nto suspect that important statistical connections are extracted from math sources. It is even possible\nthat simple instances of auto-formalization or ITP translation are happening. There is prior work\n(Gauthier & Kaliszyk, 2015; Wang et al., 2018; 2020) suggesting that both of these are possible.\nFrom the point of view of a lean-gptf end-user, any such extraction of prior, publicly available\ndata is useful and helpful. Nonetheless, our results are of a different nature than other AI for theorem\nproving research which do not use data outside of a given theorem proving library. This should be\ntaken into account in any future comparisons and benchmarks.\nB.2 D ATASET SIZES\n• tactic: ≈128K examples.\n• mix1\n– Next lemma prediction : ≈2.5M examples\n– Proof term prediction : ≈2.9M examples\n• mix2\n– Skip-proof : ≈1.7M examples\n– Type-prediction : ≈1.7M examples\n– Tactic state elaboration : ≈346K examples\n– Proof term elaboration : ≈1.0M examples\n– Premise classiﬁcation : ≈9.3M examples\n– Local context classiﬁcation : ≈2.0M examples\n– Theorem naming : ≈32Kexamples.\nB.3 E XAMPLE DATAPOINTS\nWe present datapoints extracted from a toy example, namely the proof of the Peirce identity, viz.\nlemma peirce_identity {P Q :Prop} : ((P → Q) → P) → P :=\nbegin\napply or.elim (em P),\nintros h _,\nexact h,\ntauto!\nend\nFrom this, we can extract four tactic datapoints (i.e. human-generated tactic proof steps):\n-- GOAL P Q : Prop ⊢ ((P → Q) → P) → P PROOFSTEP apply or.elim (em P)\n-- GOAL P Q : Prop ⊢ P → ((P → Q) → P) → P P Q : Prop ⊢ ¬P → ((P →\nQ) → P) → P PROOFSTEP intros h _\n-- GOAL P Q : Prop, h : P, ˇα : (P → Q) → P ⊢ P P Q : Prop ⊢ ¬P → ((P\n→ Q) → P) → P PROOFSTEP exact h\n-- GOAL P Q : Prop ⊢ ¬P → ((P → Q) → P) → P PROOFSTEP tauto!\nIn contrast, we can extract dozens of raw PACT datapoints. Due to space constraints, we list a\nrepresentative sample of four such datapoints, from each of which we can derive the nine self-\nsupervised auxiliary PACT tasks studied in our present work. For example, proof term prediction is\nprecisely predicting the \"proof_term\" given the concatenation of \"hyps\", \"⊢\", and the \"goal\",\nskip-proof is predicting the \"proof_term\" given \"result\", etc.\nDATAPOINT:\n---\n{ \"decl_nm\":\"peirce_identity\",\n\"decl_tp\":\"∀ {P Q : Prop}, ((P → Q) → P) → P\",\n\"hyps\":[[\"P\", \"Prop\"], [\"Q\", \"Prop\"], [\"ˇα\", \"¬P\"], [\"ˇα_1\", \"(P → Q) →\nP\"], [\"ˇα_1\", \"¬(P → Q)\"]],\n17\nPublished as a conference paper at ICLR 2022\n\"hyps_mask\":[true, false, false, false, false],\n\"decl_premises\":[[\"absurd\", \"∀ {a b : Prop}, a → ¬a → b\"],\n[\"absurd\", \"∀ {a b : Prop}, a → ¬a → b\"],\n[\"decidable.not_imp\", \"∀ {a b : Prop} [_inst_1 : decidable a], ¬(a →\nb) ↔ a ∧ ¬b\"],\n[\"iff.mp\", \"∀ {a b : Prop}, (a ↔ b) → a → b\"],\n[\"and.dcases_on\",\n\"∀ {a b : Prop} {C : a ∧ b → Prop} (n : a ∧ b), (∀ (left : a)\n(right : b), C _) → C n\"],\n[\"decidable.not_or_of_imp\", \"∀ {a b : Prop} [_inst_1 : decidable a],\n(a → b) → ¬a ∨ b\"],\n[\"or.dcases_on\",\n\"∀ {a b : Prop} {C : a ∨ b → Prop} (n : a ∨ b), (∀ (h : a), C _) →\n(∀ (h : b), C _) → C n\"],\n[\"em\", \"∀ (p : Prop), p ∨ ¬p\"],\n[\"or.elim\", \"∀ {a b c : Prop}, a ∨ b → (a → c) → (b → c) → c\"]],\n\"decl_premises_mask\":[false, false, true, false, false, false, false,\nfalse, false],\n\"goal\":\"∀ {b : Prop} [_inst_1 : decidable P], ¬(P → b) ↔ P ∧ ¬b\",\n\"proof_term\":\"decidable.not_imp\",\n\"result\":\"λ {P Q : Prop}, (em P).elim (λ (h : P) (ˇα : (P → Q) → P),\nh) (λ (ˇα : ¬P) (ˇα_1 : (P → Q) → P), (decidable.not_or_of_imp ˇα\n_1).dcases_on (λ (ˇα_1 : ¬(P → Q)), ((PREDICT Q\n(classical.prop_decidable P)).mp ˇα_1).dcases_on (λ (ˇα_1_left : P)\n(ˇα_1_right : ¬Q), absurd ˇα_1_left ˇα)) (λ (ˇα_1 : P), absurd ˇα_1 ˇα))\",\n\"next_lemma\":[\"decidable.not_imp\", \"∀ {a b : Prop} [_inst_1 :\ndecidable a], ¬(a → b) ↔ a ∧ ¬b\"],\n\"goal_is_prop\":true,\n\"verbose_proof_term\":\"@decidable.not_imp P\",\n\"verbose_goal\":\"∀ {b : Prop} [_inst_1 : decidable P], ¬(P → b) ↔ P ∧\n¬b\",\n\"verbose_result\":\"λ {P Q : Prop}, (em P).elim (λ (h : P) (ˇα : (P → Q)\n→ P), h) (λ (ˇα : ¬P) (ˇα_1 : (P → Q) → P),\n(@decidable.not_or_of_imp (P → Q) P (classical.prop_decidable (P →\nQ)) ˇα_1).dcases_on (λ (ˇα_1 : ¬(P → Q)), (@iff.mp (¬(P → Q)) (P ∧ ¬\nQ) (PREDICT Q (classical.prop_decidable P)) ˇα_1).dcases_on (λ\n(ˇα_1_left : P) (ˇα_1_right : ¬Q), @absurd P P ˇα_1_left ˇα)) (λ (ˇα_1 :\nP), @absurd P P ˇα_1 ˇα))\"}\n---\nDATAPOINT:\n---\n{ \"decl_nm\":\"peirce_identity\",\n\"decl_tp\":\"∀ {P Q : Prop}, ((P → Q) → P) → P\",\n\"hyps\":[[\"P\", \"Prop\"], [\"Q\", \"Prop\"], [\"ˇα\", \"¬P\"], [\"ˇα_1\", \"(P → Q) →\nP\"], [\"ˇα_1\", \"¬(P → Q)\"]],\n\"hyps_mask\":[false, true, false, false, false],\n\"decl_premises\":[[\"absurd\", \"∀ {a b : Prop}, a → ¬a → b\"],\n[\"absurd\", \"∀ {a b : Prop}, a → ¬a → b\"],\n[\"decidable.not_imp\", \"∀ {a b : Prop} [_inst_1 : decidable a], ¬(a →\nb) ↔ a ∧ ¬b\"],\n[\"iff.mp\", \"∀ {a b : Prop}, (a ↔ b) → a → b\"],\n[\"and.dcases_on\",\n\"∀ {a b : Prop} {C : a ∧ b → Prop} (n : a ∧ b), (∀ (left : a)\n(right : b), C _) → C n\"],\n[\"decidable.not_or_of_imp\", \"∀ {a b : Prop} [_inst_1 : decidable a],\n(a → b) → ¬a ∨ b\"],\n[\"or.dcases_on\",\n\"∀ {a b : Prop} {C : a ∨ b → Prop} (n : a ∨ b), (∀ (h : a), C _) →\n(∀ (h : b), C _) → C n\"],\n[\"em\", \"∀ (p : Prop), p ∨ ¬p\"],\n[\"or.elim\", \"∀ {a b c : Prop}, a ∨ b → (a → c) → (b → c) → c\"]],\n\"decl_premises_mask\":[false, false, false, false, false, false, false,\nfalse, false],\n\"goal\":\"Prop\",\n18\nPublished as a conference paper at ICLR 2022\n\"proof_term\":\"Q\",\n\"result\":\"λ {P Q : Prop}, (em P).elim (λ (h : P) (ˇα : (P → Q) → P),\nh) (λ (ˇα : ¬P) (ˇα_1 : (P → Q) → P), (decidable.not_or_of_imp ˇα\n_1).dcases_on (λ (ˇα_1 : ¬(P → Q)), (decidable.not_imp.mp ˇα\n_1).dcases_on (λ (ˇα_1_left : P) (ˇα_1_right : ¬Q), absurd ˇα_1_left ˇα\n)) (λ (ˇα_1 : P), absurd ˇα_1 ˇα))\",\n\"next_lemma\":[\"Q\", \"Prop\"],\n\"goal_is_prop\":false,\n\"verbose_proof_term\":\"Q\",\n\"verbose_goal\":\"Prop\",\n\"verbose_result\":\"λ {P Q : Prop}, (em P).elim (λ (h : P) (ˇα : (P → Q)\n→ P), h) (λ (ˇα : ¬P) (ˇα_1 : (P → Q) → P),\n(@decidable.not_or_of_imp (P → Q) P (classical.prop_decidable (P →\nQ)) ˇα_1).dcases_on (λ (ˇα_1 : ¬(P → Q)), ((@decidable.not_imp P\nPREDICT (classical.prop_decidable P)).mp ˇα_1).dcases_on (λ (ˇα_1_left\n: P) (ˇα_1_right : ¬Q), @absurd P P ˇα_1_left ˇα)) (λ (ˇα_1 : P),\n@absurd P P ˇα_1 ˇα))\"}\n---\nDATAPOINT:\n---\n{ \"decl_nm\":\"peirce_identity\",\n\"decl_tp\":\"∀ {P Q : Prop}, ((P → Q) → P) → P\",\n\"hyps\":[[\"P\", \"Prop\"], [\"Q\", \"Prop\"], [\"ˇα\", \"¬P\"], [\"ˇα_1\", \"(P → Q) →\nP\"], [\"ˇα_1\", \"¬(P → Q)\"]],\n\"hyps_mask\":[true, true, false, false, false],\n\"decl_premises\":[[\"absurd\", \"∀ {a b : Prop}, a → ¬a → b\"],\n[\"absurd\", \"∀ {a b : Prop}, a → ¬a → b\"],\n[\"decidable.not_imp\", \"∀ {a b : Prop} [_inst_1 : decidable a], ¬(a →\nb) ↔ a ∧ ¬b\"],\n[\"iff.mp\", \"∀ {a b : Prop}, (a ↔ b) → a → b\"],\n[\"and.dcases_on\",\n\"∀ {a b : Prop} {C : a ∧ b → Prop} (n : a ∧ b), (∀ (left : a)\n(right : b), C _) → C n\"],\n[\"decidable.not_or_of_imp\", \"∀ {a b : Prop} [_inst_1 : decidable a],\n(a → b) → ¬a ∨ b\"],\n[\"or.dcases_on\",\n\"∀ {a b : Prop} {C : a ∨ b → Prop} (n : a ∨ b), (∀ (h : a), C _) →\n(∀ (h : b), C _) → C n\"],\n[\"em\", \"∀ (p : Prop), p ∨ ¬p\"],\n[\"or.elim\", \"∀ {a b c : Prop}, a ∨ b → (a → c) → (b → c) → c\"]],\n\"decl_premises_mask\":[false, false, true, false, false, false, false,\nfalse, false],\n\"goal\":\"∀ [_inst_1 : decidable P], ¬(P → Q) ↔ P ∧ ¬Q\",\n\"proof_term\":\"decidable.not_imp\",\n\"result\":\"λ {P Q : Prop}, (em P).elim (λ (h : P) (ˇα : (P → Q) → P),\nh) (λ (ˇα : ¬P) (ˇα_1 : (P → Q) → P), (decidable.not_or_of_imp ˇα\n_1).dcases_on (λ (ˇα_1 : ¬(P → Q)), ((PREDICT\n(classical.prop_decidable P)).mp ˇα_1).dcases_on (λ (ˇα_1_left : P)\n(ˇα_1_right : ¬Q), absurd ˇα_1_left ˇα)) (λ (ˇα_1 : P), absurd ˇα_1 ˇα))\",\n\"next_lemma\":[\"decidable.not_imp\", \"∀ {a b : Prop} [_inst_1 :\ndecidable a], ¬(a → b) ↔ a ∧ ¬b\"],\n\"goal_is_prop\":true,\n\"verbose_proof_term\":\"@decidable.not_imp P Q\",\n\"verbose_goal\":\"∀ [_inst_1 : decidable P], ¬(P → Q) ↔ P ∧ ¬Q\",\n\"verbose_result\":\"λ {P Q : Prop}, (em P).elim (λ (h : P) (ˇα : (P → Q)\n→ P), h) (λ (ˇα : ¬P) (ˇα_1 : (P → Q) → P),\n(@decidable.not_or_of_imp (P → Q) P (classical.prop_decidable (P →\nQ)) ˇα_1).dcases_on (λ (ˇα_1 : ¬(P → Q)), (@iff.mp (¬(P → Q)) (P ∧ ¬\nQ) (PREDICT (classical.prop_decidable P)) ˇα_1).dcases_on (λ\n(ˇα_1_left : P) (ˇα_1_right : ¬Q), @absurd P P ˇα_1_left ˇα)) (λ (ˇα_1 :\nP), @absurd P P ˇα_1 ˇα))\"}\n---\nDATAPOINT:\n19\nPublished as a conference paper at ICLR 2022\n---\n{ \"decl_nm\":\"peirce_identity\",\n\"decl_tp\":\"∀ {P Q : Prop}, ((P → Q) → P) → P\",\n\"hyps\":[[\"P\", \"Prop\"], [\"Q\", \"Prop\"], [\"ˇα\", \"¬P\"], [\"ˇα_1\", \"(P → Q) →\nP\"], [\"ˇα_1\", \"¬(P → Q)\"]],\n\"hyps_mask\":[false, false, false, false, false],\n\"decl_premises\":[[\"absurd\", \"∀ {a b : Prop}, a → ¬a → b\"],\n[\"absurd\", \"∀ {a b : Prop}, a → ¬a → b\"],\n[\"decidable.not_imp\", \"∀ {a b : Prop} [_inst_1 : decidable a], ¬(a →\nb) ↔ a ∧ ¬b\"],\n[\"iff.mp\", \"∀ {a b : Prop}, (a ↔ b) → a → b\"],\n[\"and.dcases_on\",\n\"∀ {a b : Prop} {C : a ∧ b → Prop} (n : a ∧ b), (∀ (left : a)\n(right : b), C _) → C n\"],\n[\"decidable.not_or_of_imp\", \"∀ {a b : Prop} [_inst_1 : decidable a],\n(a → b) → ¬a ∨ b\"],\n[\"or.dcases_on\",\n\"∀ {a b : Prop} {C : a ∨ b → Prop} (n : a ∨ b), (∀ (h : a), C _) →\n(∀ (h : b), C _) → C n\"],\n[\"em\", \"∀ (p : Prop), p ∨ ¬p\"],\n[\"or.elim\", \"∀ {a b c : Prop}, a ∨ b → (a → c) → (b → c) → c\"]],\n\"decl_premises_mask\":[false, false, false, false, false, false, false,\nfalse, false],\n\"goal\":\"Π (a : Prop), decidable a\",\n\"proof_term\":\"classical.prop_decidable\",\n\"result\":\"λ {P Q : Prop}, (em P).elim (λ (h : P) (ˇα : (P → Q) → P),\nh) (λ (ˇα : ¬P) (ˇα_1 : (P → Q) → P), (decidable.not_or_of_imp ˇα\n_1).dcases_on (λ (ˇα_1 : ¬(P → Q)), (decidable.not_imp.mp ˇα\n_1).dcases_on (λ (ˇα_1_left : P) (ˇα_1_right : ¬Q), absurd ˇα_1_left ˇα\n)) (λ (ˇα_1 : P), absurd ˇα_1 ˇα))\",\n\"next_lemma\":[\"classical.prop_decidable\", \"Π (a : Prop), decidable a\"],\n\"goal_is_prop\":false,\n\"verbose_proof_term\":\"classical.prop_decidable\",\n\"verbose_goal\":\"Π (a : Prop), decidable a\",\n\"verbose_result\":\"λ {P Q : Prop}, (em P).elim (λ (h : P) (ˇα : (P → Q)\n→ P), h) (λ (ˇα : ¬P) (ˇα_1 : (P → Q) → P),\n(@decidable.not_or_of_imp (P → Q) P (PREDICT (P → Q)) ˇα\n_1).dcases_on (λ (ˇα_1 : ¬(P → Q)), ((@decidable.not_imp P Q\n(PREDICT P)).mp ˇα_1).dcases_on (λ (ˇα_1_left : P) (ˇα_1_right : ¬Q),\n@absurd P P ˇα_1_left ˇα)) (λ (ˇα_1 : P), @absurd P P ˇα_1 ˇα))\"}\n---\nC E XPERIMENTS\nC.1 C HAINED TACTIC PREDICTION\nIndividual Lean tactics are chained together with commas. However, the Lean interactive tactic DSL\nalso includes a number of other tactic combinators for creating composite tactics. A frequently used\ncombinator is the inﬁx semicolon t; s which will perform the tactic t and then apply the tactic\ns to each of the resulting subgoals produced by t. Our data pipeline for human tactic proof steps\ntreats these semicolon-chained tactics as a single string for the language modeling objective. Thus,\nour models learn to occasionally emit multiple-step tactic predictions using semicolons. For example,\nwm-to-tt-m1-m2 solved the following lemma in category theory with a single prediction chaining\nfour tactics in a row:\ntheorem category_theory.grothendieck.congr\n{X Y : grothendieck F} {f g : X −→Y} (h : f = g) :\nf.fiber = eq_to_hom (by subst h) ≫ g.fiber :=\nbegin\nrcases X; rcases Y; subst h; simp\nend\n20\nPublished as a conference paper at ICLR 2022\nOne way of measuring the sophistication of predicted tactics is to consider the number of successful\nproofs on the evaluation set which have this composite form using semicolon-chaining. We display\nthis analysis in Table 1, which shows that training with PACT in addition to the human-made tactics\ncauses longer semicolon-chained tactics to be successfully predicted during theorem proving. This is\nremarkable because the semicolon idiom is speciﬁc to the tactic DSL which does not occur in the\nPACT data whatsoever, and yet the co-training causes longer and more frequent successful composite\ntactic predictions.\nC.2 T HEOREM NAMING CASE STUDY\nWe included theorem naming as part of the PACT task suite. By mathlib convention, theorem\nnames are essentially snake-cased, natural language summaries of the type signature of a theorem,\nand so the theorem naming task is analogous to a formal-to-informal translation task. We evaluate\nthe ability of our best model (in terms of theorem proving success rate) wm-to-tt-m1-m2 on\nits ability to guess theorem names on the completely unseen future-mathlib set of theorems.\nThe distribution shift inherent in the future-mathlib dataset particularly impacts the theorem\nnaming task, because many of the ground-truth names will involve names for concepts that were only\ndeﬁned in mathlib after we extracted our training data.\nOn the ≈2.8K future-mathlib theorems, we queried wm-to-tt-m1-m2 for up to N = 16\ncandidates. We order these candidates into a list xs by decreasing cumulative log-probability and\ncalculate the top-K accuracy by checking if any of the ﬁrst K candidates of xs match the ground\ntruth exactly. The model wm-to-tt-m1-m2 was able to achieve 20.1% top-1 accuracy, 21.1%\ntop-3 accuracy, 26.7% top-10 accuracy, and 30.0% top-16 accuracy. We display a sample of correct\ntop-1 guesses (Figure 5) and a sample of failed guesses in (Figure 6). We note that the failed guesses,\nwhile containing no syntactic matches, are both semantically reasonable and syntactically very similar\nto the ground truth.\nC.3 T EST SET EVALUATION BREAKDOWN BY MODULE\nLean’smathlib is organized into top-level modules, which roughly organize theorems into mathe-\nmatical subject area. In Figure 7, we break down the evaluation results on our test set between\nour PACT-trained modelswm-to-tt-m1-m2 and wm-to-tt-m1 and our baselines wm-to-tt\nand tidy. We see that full PACT mostly dominates over co-training on just themix1 tasks over all\nsubject areas, and that wm-to-tt-m1 dominates the model wm-to-tt trained on human tactic\nproof steps only.\nC.4 B ASELINE DESCRIPTION\nThe tidy backend is determined by a constant oracle\nΩ : tactic_state → list (string × float)\nwhich always returns the same list of tactics, namely:\nmeta def tidy_default_tactics : list (string × float) :=\nlist.map (flip prod.mk 0.0) [\n\"refl\"\n, \"exact dec_trivial\"\n, \"assumption\"\nTable 1: Counting the number of semicolon-chained tactics predicted by our models that appear\nin successful proofs. Each column headed by a number n; indicates the number of times that a\nsuggestion appeared with noccurrences of ‘;’.\nMODEL 1; 2; 3; 4; MEAN\nwm-to-tt 215 49 2 0 1.199\nwm-to-tt-m1 186 39 5 1 1.225\nwm-to-tt-m1-m2 328 82 12 3 1.271\n21\nPublished as a conference paper at ICLR 2022\nCorrect top-1 guesses\nTheorem statement\n∀ {α : Type u_1} {β : Type u_2} [_inst_1 : decidable_eq α]\n[_inst_2 : decidable_eq β] (s : finset α) (t : finset β),\ns.product t = s.bUnion\n(λ (a : α), finset.image (λ (b : β), (a, b)) t)\nGround truth finset.product_eq_bUnion\nTheorem statement\n∀ {α : Type u_1} {β : Type u_2} [_inst_1 : topological_space α]\n[_inst_2 : topological_space β] {f : α → β},\nquotient_map f → function.surjective f\nGround truth quotient_map.surjective\nTheorem statement\n∀ {α : Type u_1} {β : Type u_2} (f : α → option β)\n(x : option α), x.pbind (λ (a : α) (_x : a ∈ x), f a) = x.bind f\nGround truth option.pbind_eq_bind\nTheorem statement\n∀ {C : Type u1} [_inst_1 : category_theory.category C]\n{G : C ⇒ C} [_inst_2 : category_theory.comonad G]\n{A B : category_theory.comonad.coalgebra G} (h : A.A ∼= B.A)\n(w : A.a ≫ G.map h.hom = h.hom ≫ B.a),\n(category_theory.comonad.coalgebra.iso_mk h w).hom.f = h.hom\nGround truth category_theory.comonad.coalgebra.iso_mk_hom_f\nTheorem statement\n∀ {k : Type u_1} {E : Type u_2} [_inst_1 : is_R_or_C ,k]\n[_inst_2 : inner_product_space k E]\n[_inst_4 : normed_space R E] [_inst_5 : is_scalar_tower R k E]\n(p x : E × E),\n⇑(fderiv_inner_clm p) x =\nhas_inner.inner p.fst x.snd + has_inner.inner x.fst p.snd\nGround truth fderiv_inner_clm_apply\nFigure 5: A sample of correct top-1 guesses by our best model wm-to-tt-m1-m2 on the theorem\nnaming task. We performed this experiment on the future-mathlib evaluation set, which\ncomprises entirely unseen theorems added to mathlib only after we last extracted training data.\n22\nPublished as a conference paper at ICLR 2022\nIncorrect guesses\nTheorem statement\n∀ {α : Type u_1} (t : ordnode α) (x : α),\nt.dual.find_min′ x = ordnode.find_max′ x t\nGuesses (top 8)\nordinal.find_min′_eq, ordinal.find_min′_eq_max′, ordinal.find_min′_def,\nordinal.find_min′_eq_max, ordinal.find_min′, ordinal.dual_find_min′,\nordinal.find_min′_gt, ordinal.find_min′_q\nGround truth ordnode.find_min′_dual\nTheorem statement\n∀ {α : Type u_1} {β : Type u_3} {γ : Type u_5} [_inst_1 :\nmeasurable_space α] [_inst_3 : measurable_space β]\n[_inst_5 : measurable_space γ] {µ : measure_theory.measure α}\n{ν : measure_theory.measure β}\n[_inst_8 : measure_theory.sigma_finite ν]\n{f : α × β → γ},\nae_measurable f (µ.prod ν) → (∀m(x : α) ∂µ,\nae_measurable (λ (y : β), f (x, y)) ν)\nGuesses (top 8)\nmeasure_theory.ae_prod, measure_theory.ae_of_ae_prod,\nmeasure_theory.ae_eq_prod_of_ae, measure_theory.ae_ae_of_ae_prod,\nmeasure_theory.ae_measure_prod_mk_left,\nmeasure_theory.ae_prod_of_ae_prod,\nmeasure_theory.ae_measure_prod, measure_theory.ae_eq_refl\nGround truth ae_measurable.prod_mk_left\nTheorem statement\n∀ {α : Type u_1} {β : Type u_2} {γ : Type u_3}\n{f : filter α} {h : set α → set β} {m : γ → β}\n{l : filter γ}, filter.tendsto m l (f.lift′ h) ↔\n∀ (s : set α), s ∈ f → (∀f (a : γ) in l, m a ∈ h s)\nGuesses (top 8) filter.tendsto_lift′_iff, filter.tendsto_lift′_def\nGround truth filter.tendsto_lift′\nTheorem statement\n∀ {R : Type} [_inst_1 : comm_ring R]\n{d : Z} (f : Z√d →+∗ R),\n↑(⇑(zsqrtd.lift.symm) f) = ⇑f zsqrtd.sqrtd\nGuesses (top 8)\nzsqrtd.coe_lift_symm, zsqrtd.coe_lift.symm, zsqrtd.lift.coe_symm_apply,\nzsqrtd.lift_symm_apply, zsqrtd.lift.coe_coe_symm,\nzsqrtd.lift.coe_symm_coe,\nzsqrtd.lift.symm_coe_zsqrtd, zsqrtd.lift_symm_to_zsqrtd\nGround truth zsqrtd.lift_symm_apply_coe\nFigure 6: A sample of incorrect guesses by our best model wm-to-tt-m1-m2 on the theorem\nnaming task. We performed this experiment on the future-mathlib evaluation set, which\ncomprises entirely unseen theorems added to mathlib only after we last extracted training data.\nMost of the top-8 guesses displayed in the above table are very similar to the ground truth, in some\ncases being equivalent up to permutation of underscore-separated tokens. Note that for the ﬁrst\nexample, the concept of ordnode was not in the training data whatsoever and all predictions are in\nthe syntactically similar ordinal namespace.\n23\nPublished as a conference paper at ICLR 2022\nlogic\nalgebra\norder\ndata\ncategory_theory\ncontrol\ngroup_theory\ncombinatorics\ntopology\nlinear_algebra\nset_theory\nanalysis\ngeometry\ndynamics\ncomputability\nnumber_theory\nmeasure_theory\nfield_theory\nring_theory\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7Success Rate\nTest Set Evaluation Breakdown By Modules\nwm-to-tt-m1-m2 (PACT full)\nwm-to-tt-m1 (PACT mix1 only)\nwm-to-tt (tactic step only)\ntidy (baseline)\nFigure 7: A breakdown of theorem proving success rate on the test set for wm-to-tt-m1-m2,\nwm-to-tt-m1, wm-to-tt, and the tidy baseline across top-level modules in Lean’smathlib.\nWe see that wm-to-tt-m1-m2 mostly dominates wm-to-tt-m1 and the models trained using\nPACT dominate the modelwm-to-tt trained on human tactic proof steps.\n, \"tactic.intros1\"\n, \"tactic.auto_cases\"\n, \"apply_auto_param\"\n, \"dsimp at ∗\"\n, \"simp at ∗\"\n, \"ext1\"\n, \"fsplit\"\n, \"injections_and_clear\"\n, \"solve_by_elim\"\n, \"norm_cast\"\n]\nUnlike the gptf backend, which generates a list of candidates in parallel independently,tidy enjoys\nthe advantage that the list of tactics it emits is carefully chosen and ordered in order to optimize\nthe proof search—this is based on the “waterfall” technique of the human-style automated theorem\nprover described in (Ganesalingam & Gowers, 2017).\nC.5 C OMPUTATIONAL RESOURCE ESTIMATES\nFor each evaluation loop over the test set, we distributed the theorems over a pool of 32 CPU\nworkers whose inference requests were load-balanced over 4 V100 GPUs. Each evaluation required\n≈10 hours with ≈30% GPU utilization. We observed that our evaluation was bottlenecked by\ninference and in practice, we hosted up to three evaluation loops at once on a VM with 80 logical\ncores without achieving full CPU utilization. In addition to the wall-clock timeout of 600s, we also\nlimited the proof search to a logical timeout of 512 iterations, where one iteration corresponds to a\nsingle expansion of a node of the BFS search tree. In practice, so much time was spent either blocked\non inference or performing the tactic executions in the inner loop of each iteration that we rarely\nexceeded the logical timeout, usually exceeding the wall-clock timeout instead.\nFine-tuning on our largest dataset mix1 + mix2 + tactic required 26 hours using 64 A100\nGPUs exhibiting high FP16 usage, totalling an estimated ≈1.5K A100(FP16)-hours. This gives\nan estimated cost of 17.33 A100(FP16)-hours per billion elapsed tokens during training. We note\n24\nPublished as a conference paper at ICLR 2022\nthat when calculating the number of elapsed tokens for training, we overestimate the actual number\nof tokens effectively trained on by summing full context windows (in this case, 2048 tokens).\nD E XAMPLE PROOFS\nLean’smathlib is one of the most active open-source software projects in the world. More than\none-third of the proofs found by our models are shorter and produce smaller proof terms than the\nground truth, leading to dozens of GPT-f co-authored commits to mathlib. We examine some of\nthe proofs found by our models in more detail.\nD.1 L I E_A L G E B R A.M O R P H I S M.M A P_B O T_I F F\nThis proof produces a proof term which is 4X smaller than the original:\nlemma map_bot_iff : I.map f = ⊥ ↔I ≤ f.ker :=\nby { rw ← le_bot_iff, apply lie_ideal.map_le_iff_le_comap }\nThe original, human-written proof is much longer, viz.\nlemma map_bot_iff : I.map f = ⊥ ↔I ≤ f.ker :=\nbegin\nrw le_ker_iff, unfold lie_ideal.map, split; intros h,\n{ rwa [eq_bot_iff, lie_submodule.lie_span_le, set.image_subset_iff,\nlie_submodule.bot_coe] at h,},\n{ suffices : f ′′ I = ↑(⊥ : lie_ideal R L′), { rw [this,\nlie_submodule.lie_span_eq], },\next x, rw [lie_submodule.bot_coe, set.mem_singleton_iff,\nset.mem_image],\nsplit,\n{ rintros ⟨y, hy, hx⟩, rw ← hx, exact h y hy, },\n{ intros hx, use 0, simp [hx], }, },\nend\nD.2 P R I M R E C.O F_E Q U I V\nThis proof produces a proof term which is 12X smaller than the original:\ntheorem of_equiv {β} {e : β ≃ α} :\nby haveI := primcodable.of_equiv α e; exact\nprimrec e :=\nby letI : primcodable β := primcodable.of_equiv α e; exact encode_iff.1\nprimrec.encode\nThe author of the original proof and maintainer of that package commented:\nencode_iff.1 primrec.encode is clever, it’s a way to translateprimrec\nacross an equivalence when the encode function is deﬁned as encode x =\nencode (e x) where e is the isomorphism.\nAs far as they knew, this trick was never used before in thecomputability package.\nD.3 R E A L.T A N_E Q_S I N_D I V_C O S\nThis proof demonstrates our model’s library knowledge and ability at premise selection.\nlemma real.tan_eq_sin_div_cos (x : R) : tan x = sin x / cos x :=\nbegin\nrw ← of_real_inj,\nsimp only [complex.tan_eq_sin_div_cos, of_real_sin, of_real_cos,\nof_real_div, of_real_tan]\nend\n25\nPublished as a conference paper at ICLR 2022\nOur model was able to predict this entire list of simp lemmas in one shot. Note that the lemma\ncomplex.tan_eq_sin_div_cos in this list is the complex number version of the result, i.e. ∀\n(x : C), tan x = sin x / cos x. The previous human-written version of the proof did\nnot use the more general version of the lemma on complex numbers, demonstrating our model’s\nability to ﬁnd more general cases of lemmas. We contrast this with the human-written ground truth,\nwhich is more complex and performs a case analysis using the complex cosine:\nlemma tan_eq_sin_div_cos : tan x = sin x / cos x :=\nif h : complex.cos x = 0 then by simp [sin, cos, tan, ∗, complex.tan,\ndiv_eq_mul_inv] at ∗\nelse\nby rw [sin, cos, tan, complex.tan, ← of_real_inj, div_eq_mul_inv,\nmul_re];\nsimp [norm_sq, (div_div_eq_div_mul _ _ _).symm, div_self h]; refl\nD.4 S Y M2.I S_D I A G_I F F_P R O J_E Q\nThe proof of this lemma is longer than the ground truth and was not contributed to mathlib, but we\ndescribe it here because the proof is original and includes a nontrivial instantiation of an existential\nquantiﬁer.\ntheorem sym2.is_diag_iff_proj_eq (z : α × α) :\nis_diag JzK ↔ z.1 = z.2 :=\nbegin\nintros,\nsimp only [is_diag, prod.ext_iff, quot.exists_rep, iff_true,\nnot_true, eq_self_iff_true],\nsimp [diag], split,\n{ rintros ⟨y, hy⟩, cases hy; refl },\nintro h, cases z, existsi z_snd,\ncases h, refl,\nend\nBefore existsi z_snd, the goal state is\nz_fst z_snd: α\nh: (z_fst, z_snd).fst = (z_fst, z_snd).snd\n⊢ ∃(y : α), (y, y) ≈ (z_fst, z_snd)\nThis goal state never appeared in mathlib.\nD.5 N O R M_L E_Z E R O_I F F\nThe following proof is remarkable because it uses fewer tactic steps and takes a different route to the\nproof than the ground truth, uses a complex idiom simpa [...] using @..., and was predicted\nin one shot.\nlemma norm_le_zero_iff {α : Type u_1} [_inst_1 : normed_group α]\n{g : α} : ||g|| ≤0 ↔ g = 0 :=\nby { simpa [le_antisymm_iff, norm_nonneg] using @norm_eq_zero α _ g }\n-- ground truth:\n-- by { rw[←dist_zero_right],\n-- exact dist_le_zero }\nThe lemmas supplied between the square brackets are used to simplify the main goal. The lemma\nsupplied after the keyword using can further simplify the lemmas supplied between the square\nbrackets. The @ modiﬁer makes all arguments explicit. The string @norm_eq_zero never appeared\nin our training data but the prediction includes the correct number of correctly typed arguments, and\neven replaces the second argument with a placeholder _, correctly guessing that it can be inferred\nby the elaborator. Finally, this again showcases the strength of our models as premise selectors:\nall three lemmas le_antisymm_iff, norm_nonneg, and norm_eq_zero were not used in the\nhuman-supplied proof but are necessary for this proof.\n26\nPublished as a conference paper at ICLR 2022\nMoving forward, we hope that our neural theorem provers will continue to ﬁnd ways to improve\nmathlib and assist in creating new proofs. More generally, we hope neural theorem proving will\none day be become a routine part of the formalization workﬂow.\n27",
  "topic": "Automated theorem proving",
  "concepts": [
    {
      "name": "Automated theorem proving",
      "score": 0.7418313026428223
    },
    {
      "name": "Proof assistant",
      "score": 0.6672865748405457
    },
    {
      "name": "Computer science",
      "score": 0.5878019332885742
    },
    {
      "name": "Benchmarking",
      "score": 0.5187942385673523
    },
    {
      "name": "Suite",
      "score": 0.514710009098053
    },
    {
      "name": "Transformer",
      "score": 0.5090846419334412
    },
    {
      "name": "Proof of concept",
      "score": 0.442449688911438
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4358232021331787
    },
    {
      "name": "Training set",
      "score": 0.4201413691043854
    },
    {
      "name": "Machine learning",
      "score": 0.3828631639480591
    },
    {
      "name": "Theoretical computer science",
      "score": 0.31904101371765137
    },
    {
      "name": "Mathematics",
      "score": 0.2503831386566162
    },
    {
      "name": "Engineering",
      "score": 0.09194445610046387
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Mathematical proof",
      "score": 0.0
    }
  ]
}