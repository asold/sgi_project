{
  "title": "Finetuned Language Models Are Zero-Shot Learners",
  "url": "https://openalex.org/W3198002980",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3174929027",
      "name": "Wei, Jason",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202120588",
      "name": "Bosma, Maarten",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226407210",
      "name": "Zhao, Vincent Y.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281172484",
      "name": "Guu, Kelvin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3115150678",
      "name": "Yu, Adams Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224559922",
      "name": "Lester, Brian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2487280405",
      "name": "Du, Nan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224379245",
      "name": "Dai, Andrew M.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202120575",
      "name": "Le, Quoc V.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2396767181",
    "https://openalex.org/W3214109107",
    "https://openalex.org/W3100198908",
    "https://openalex.org/W2953209111",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2898662126",
    "https://openalex.org/W3154200459",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2963721761",
    "https://openalex.org/W2963204221",
    "https://openalex.org/W131533222",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W1752492850",
    "https://openalex.org/W2963453233",
    "https://openalex.org/W3194309076",
    "https://openalex.org/W3093713068",
    "https://openalex.org/W2958953787",
    "https://openalex.org/W3035625205",
    "https://openalex.org/W3212382565",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2962849707",
    "https://openalex.org/W1970409510",
    "https://openalex.org/W3094474409",
    "https://openalex.org/W2932893307",
    "https://openalex.org/W2316776689",
    "https://openalex.org/W2067624665",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W3170806096",
    "https://openalex.org/W2886829407",
    "https://openalex.org/W3122317902",
    "https://openalex.org/W3106445907",
    "https://openalex.org/W2525127255",
    "https://openalex.org/W2070246124",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W3200980294",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W3175060421",
    "https://openalex.org/W2119409989",
    "https://openalex.org/W3195140297",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W3186655327",
    "https://openalex.org/W2971152344",
    "https://openalex.org/W2134270519",
    "https://openalex.org/W2888302696",
    "https://openalex.org/W2919420119",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W3026404337",
    "https://openalex.org/W2970780738",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W2962881743",
    "https://openalex.org/W2251199578",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W2023286866",
    "https://openalex.org/W3197754201",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W2963005248",
    "https://openalex.org/W2804897457",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W2895531857",
    "https://openalex.org/W3210277894",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W3102187933",
    "https://openalex.org/W2996176596",
    "https://openalex.org/W1905522558",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2946659172",
    "https://openalex.org/W3212487317",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W3154856305",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2604763608",
    "https://openalex.org/W2962801832",
    "https://openalex.org/W3124687886",
    "https://openalex.org/W3096032230",
    "https://openalex.org/W2886049129",
    "https://openalex.org/W2889326796",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W2624871570",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2786660442",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2970200208",
    "https://openalex.org/W3201090304",
    "https://openalex.org/W2962911098",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W3119866685",
    "https://openalex.org/W3035310376",
    "https://openalex.org/W2963842982",
    "https://openalex.org/W3034797437",
    "https://openalex.org/W3039127676",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W2964223283",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W2970254524",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2550821151",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W2964091467",
    "https://openalex.org/W3212149957",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W3028836324",
    "https://openalex.org/W652269744"
  ],
  "abstract": "This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",
  "full_text": "Published as a conference paper at ICLR 2022\nFINETUNED LANGUAGE MODELS ARE ZERO -SHOT\nLEARNERS\nJason Wei∗, Maarten Bosma∗, Vincent Y. Zhao∗, Kelvin Guu∗, Adams Wei Yu,\nBrian Lester, Nan Du, Andrew M. Dai, and Quoc V . Le\nGoogle Research\nABSTRACT\nThis paper explores a simple method for improving the zero-shot learning abilities\nof language models. We show that instruction tuning—ﬁnetuning language models\non a collection of datasets described via instructions—substantially improves zero-\nshot performance on unseen tasks.\nWe take a 137B parameter pretrained language model and instruction tune it on\nover 60 NLP datasets verbalized via natural language instruction templates. We\nevaluate this instruction-tuned model, which we call FLAN, on unseen task types.\nFLAN substantially improves the performance of its unmodiﬁed counterpart and\nsurpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even\noutperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC,\nOpenbookQA, and StoryCloze. Ablation studies reveal that number of ﬁnetuning\ndatasets, model scale, and natural language instructions are key to the success of\ninstruction tuning.\nTarget\nInput (Commonsense Reasoning)\nkeep stack of pillow cases in fridge\nInference on unseen task type\nFinetune on many tasks (“instruction-tuning”)\n…\nTranslate this sentence to \nSpanish: \nThe new office building \nwas built in less than three \nmonths.\nInput (Translation)\nEl nuevo edificio de oficinas \nse construyó en tres meses.\nTarget\nInput (Natural Language Inference)\nIt is not possible to tell\nFLAN ResponseCoreference resolution tasks\nSentiment analysis tasks\nGPT-3 175B zero shot GPT-3 175B few-shot FLAN 137B zero-shot\nPerformance \non unseen \ntask types\nNatural language inference\n42.9\n53.2 56.2\nReading Comprehension\n63.7\n72.6\n77.4\nClosed-Book QA\n49.8\n55.7 56.6\nHere is a goal: Get a cool sleep on \nsummer days. \nHow would you accomplish this goal? \nOPTIONS: \n-Keep stack of pillow cases in fridge. \n-Keep stack of pillow cases in oven.\nPremise: At my age you will probably \nhave learnt one lesson. \nHypothesis: It's not certain how many \nlessons you'll learn by your thirties. \nDoes the premise entail the hypothesis? \nOPTIONS: \n-yes     -it is not possible to tell      -no\nFigure 1: Top: overview of instruction tuning and FLAN. Instruction tuning ﬁnetunes a pretrained\nlanguage model on a mixture of tasks phrased as instructions. At inference time, we evaluate on\nan unseen task type; for instance, we could evaluate the model on natural language inference (NLI)\nwhen no NLI tasks were seen during instruction tuning. Bottom: performance of zero-shot FLAN,\ncompared with zero-shot and few-shot GPT-3, on three unseen task types where instruction tuning\nimproved performance substantially out of ten we evaluate. NLI datasets: ANLI R1–R3, CB, RTE.\nReading comprehension datasets: BoolQ, MultiRC, OBQA. Closed-book QA datasets: ARC-easy,\nARC-challenge, NQ, TriviaQA.\n∗Lead contributors. Author contributions listed at end of paper.\n1\narXiv:2109.01652v5  [cs.CL]  8 Feb 2022\nPublished as a conference paper at ICLR 2022\n1 I NTRODUCTION\nLanguage models (LMs) at scale, such as GPT-3 (Brown et al., 2020), have been shown to perform\nfew-shot learning remarkably well. They are less successful at zero-shot learning, however. For\nexample, GPT-3’s zero-shot performance is much worse than few-shot performance on tasks such as\nreading comprehension, question answering, and natural language inference. One potential reason\nis that, without few-shot exemplars, it is harder for models to perform well on prompts that are not\nsimilar to the format of the pretraining data.\nIn this paper, we explore a simple method to improve the zero-shot performance of large language\nmodels, which would expand their reach to a broader audience. We leverage the intuition that NLP\ntasks can be described via natural language instructions, such as “Is the sentiment of this movie review\npositive or negative?” or “Translate ‘how are you’ into Chinese.” We take a pretrained language\nmodel of 137B parameters and perform instruction tuning—ﬁnetuning the model on a mixture of\nmore than 60 NLP datasets expressed via natural language instructions. We refer to this resulting\nmodel as FLAN, for Finetuned Language Net.\nTo evaluate the zero-shot performance of FLAN on unseen tasks, we group NLP datasets into clusters\nbased on their task types and hold out each cluster for evaluation while instruction tuning FLAN\non all other clusters. For example, as shown in Figure 1, to evaluate FLAN’s ability to perform\nnatural language inference, we instruction tune the model on a range of other NLP tasks such as\ncommonsense reasoning, translation, and sentiment analysis. As this setup ensures that FLAN has\nnot seen any natural language inference tasks in instruction tuning, we then evaluate its ability to\nperform zero-shot natural language inference.\nOur evaluations show that FLAN substantially improves the zero-shot performance of the base\n137B-parameter model. FLAN’s zero-shot also outperforms 175B-parameter GPT-3’s zero-shot on 20\nof 25 datasets that we evaluate, and even outperforms GPT-3’s few-shot by a large margin on ANLI,\nRTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. In ablation studies, we ﬁnd that increasing\nthe number of task clusters in instruction tuning improves performance on unseen tasks and that the\nbeneﬁts of instruction tuning emerge only with sufﬁcient model scale.\nInstruction tuning is a simple method that, as depicted in Figure 2, combines appealing aspects\nof both the pretrain–ﬁnetune and prompting paradigms by using supervision via ﬁnetuning to\nimprove language model’s responses to inference-time text interactions. Our empirical results\ndemonstrate promising abilities of language models to perform tasks described purely via instructions.\nSource code for loading the instruction tuning dataset used for FLAN is publicly available at\nhttps://github.com/google-research/flan.\n(A) Pretrain–finetune (BERT, T5)\nFinetune on \ntask A\nInference \non task A\nPretrained \nLM\n• Typically requires many \ntask-specific examples \n• One specialized model \nfor each task\n(B) Prompting (GPT-3)\nInference \non task A\nPretrained \nLM\nImprove performance \nvia few-shot prompting \nor prompt engineering\nPretrained  \nLM\n(C) Instruction tuning (FLAN)\nInstruction-tune on \nmany tasks:  \nB, C, D, …\nInference \non task A\nInference on \nunseen task\nModel learns to perform \nmany tasks via natural \nlanguage instructions\nFigure 2: Comparing instruction tuning with pretrain–ﬁnetune and prompting.\n2 FLAN: I NSTRUCTION TUNING IMPROVES ZERO -SHOT LEARNING\nThe motivation of instruction tuning is to improve the ability of language models to respond to NLP\ninstructions. The idea is that by using supervision to teach an LM to perform tasks described via\ninstructions, the LM will learn to follow instructions and do so even for unseen tasks. To evaluate\nperformance on unseen tasks, we group datasets into clusters by task type and hold out each task\ncluster for evaluation while instruction tuning on all remaining clusters.\n2\nPublished as a conference paper at ICLR 2022\n2.1 T ASKS & TEMPLATES\nAs creating an instruction tuning dataset with many tasks from scratch would be resource-intensive,\nwe transform existing datasets from the research community into an instructional format. We\naggregate 62 text datasets that are publicly available on Tensorﬂow Datasets, including both language\nunderstanding and language generation tasks, into a single mixture. Figure 3 shows these datasets—\neach dataset is categorized into one of twelve task clusters, for which datasets in a given cluster are\nof the same task type. Descriptions, sizes, and examples of each dataset are shown in Appendix G.\nNatural language inference \n(7 datasets)\nCB\nANLI (R1-R3)\nMNLI\nQNLI\nRTE\nSNLI\nWNLI\nCommonsense \n(4 datasets)\nHellaSwag\nCoPA\nPiQA\nStoryCloze\nSentiment \n(4 datasets)\nSent140\nIMDB\nSST-2\nYelp\nStruct to text \n(4 datasets)\nDART\nCommonGen\nE2ENLG\nWEBNLG\nClosed-book QA \n(3 datasets)\nNQ\nARC (easy/chal.)\nTQA\nCoreference \n(3 datasets)\nWinogrande\nDPR\nWSC273\nTranslation \n(8 datasets)\nParaCrawl EN/ES\nParaCrawl EN/DE\nParaCrawl EN/FR\nWMT-16 EN/CS\nWMT-16 EN/DE\nWMT-16 EN/FI    \nWMT-16 EN/RO\nWMT-16 EN/RU\nWMT-16 EN/TR\nSummarization \n(11 datasets)\nAG News\nAESLC\nCNN-DM\nGigaword\nMulti-News\nNewsroom\nOpin-Abs: iDebate\nOpin-Abs: Movie\nSamSum\nWiki Lingua EN\nXSum\nReading comp. \n(5 datasets)\nDROP\nBoolQ\nMultiRC\nOBQA\nSQuAD\nParaphrase \n(4 datasets)\nQQP\nMRPC\nPAWS\nSTS-B\nRead. comp. w/ \ncommonsense \n(2 datasets)\nCosmosQA\nReCoRD\nMisc. \n(7 datasets)\nQuAC\nCoQA\nWIC\nTREC\nCoLA\nMath\nFix Punctuation (NLG)\nFigure 3: Datasets and task clusters used in this paper (NLU tasks in blue; NLG tasks in teal).\nFor each dataset, we manually compose ten unique templates that use natural language instructions\nto describe the task for that dataset. While most of the ten templates describe the original task, to\nincrease diversity, for each dataset we also include up to three templates that “turned the task around,”\n(e.g., for sentiment classiﬁcation we include templates asking to generate a movie review). We\nthen instruction tune a pretrained language model on the mixture of all datasets, with examples in\neach dataset formatted via a randomly selected instruction template for that dataset. Figure 4 shows\nmultiple instruction templates for a natural language inference dataset.\nEntailment \nNot entailment\nRussian cosmonaut Valery Polyakov \nset the record for the longest \ncontinuous amount of time spent in \nspace, a staggering 438 days, \nbetween 1994 and 1995.\nPremise\nRussians hold the record for the \nlongest stay in space.\nHypothesis\nTarget Options:  \n- yes \n- no\n<premise> \nCan we infer the following? \n<hypothesis> \n<options>\nTemplate 2\nTemplate 1\n<premise> \nBased on the paragraph \nabove, can we conclude that \n<hypothesis>? \n<options>\nRead the following and \ndetermine if the hypothesis can \nbe inferred from the premise:  \nPremise: <premise> \nHypothesis: <hypothesis> \n<options>\nTemplate 3\nTemplate 4, …\nFigure 4: Multiple instruction templates describing a natural language inference task.\n2.2 E VALUATION SPLITS\nWe are interested in how FLAN performs on tasks not seen in instruction tuning, and so it is crucial to\ndeﬁne what counts as an unseen task. Whereas some prior work deﬁnes unseen tasks by disallowing\nthe same dataset to appear in training, we use a more conservative deﬁnition that leverages the\ntask clusters from Figure 3. In this work, we only consider dataset Dunseen at evaluation time\nif no datasets from any task clusters that Dbelongs to were seen during instruction tuning. For\ninstance, if Dis an entailment task, then no entailment datasets appeared in instruction tuning, and\nwe instruction-tuned on all other clusters.1 Hence, to evaluate zero-shot FLAN on c task clusters, we\ninstruction tune c models, where each model holds out a different task cluster for evaluation.\n1When evaluating on the read. comp. with commonsense cluster, both read. comp. and commonsense\nreasoning were dropped from instruction tuning. Conversely, the read. comp. with commonsense cluster was\nnot used for instruction tuning when evaluating on read. comp. or commonsense reasoning. We also drop the\nparaphrase cluster from instruction tuning when evaluating on NLI tasks and vice-versa.\n3\nPublished as a conference paper at ICLR 2022\n2.3 C LASSIFICATION WITH OPTIONS\nThe output space for a given task is either one of several classes (classiﬁcation) or free text (generation).\nAs FLAN is an instruction-tuned version of a decoder-only language model, it naturally responds in\nfree text, and so no further modiﬁcations are needed for generation tasks.\nFor classiﬁcation tasks, prior work (Brown et al., 2020) used a rank classiﬁcation approach where,\nfor example, only two outputs (“ yes” and “no”) are considered and the higher probability one is\ntaken as the model’s prediction. Though this procedure is logically sound, it is imperfect in that the\nprobability mass for answers may have an undesired distribution among ways of saying each answer\n(e.g., a large number of alternative ways of saying “yes” may lower the probability mass assigned\nto “yes”). Therefore, we include an options sufﬁx, in which we append the token OPTIONS to the\nend of a classiﬁcation task along with a list of the output classes for that task. This makes the model\naware of which choices are desired when responding to classiﬁcation tasks. Example use of options\nis shown in the NLI and commonsense examples in Figure 1.\n2.4 T RAINING DETAILS\nModel architecture and pretraining.In our experiments, we use LaMDA-PT, a dense left-to-right,\ndecoder-only transformer language model of 137B parameters (Thoppilan et al., 2022). This model\nis pretrained on a collection of web documents (including those with computer code), dialog data,\nand Wikipedia, tokenized into 2.49T BPE tokens with a 32k vocabulary using the SentencePiece\nlibrary (Kudo & Richardson, 2018). Around 10% of the pretraining data was non-English. Note that\nLaMDA-PT only has language model pretraining (c.f. LaMDA, which was ﬁnetuned for dialog).\nInstruction tuning procedure.FLAN is the instruction-tuned version of LaMDA-PT. Our instruc-\ntion tuning pipeline mixes all datasets and randomly samples from each dataset. To balance the\ndifferent sizes of datasets, we limit the number of training examples per dataset to 30k and follow\nthe examples-proportional mixing scheme (Raffel et al., 2020) with a mixing rate maximum of 3k.2\nWe ﬁnetune all models for 30k gradient steps with a batch size of 8,192 tokens using the Adafactor\nOptimizer (Shazeer & Stern, 2018) with a learning rate of 3e-5. The input and target sequence lengths\nused in ﬁnetuning are 1024 and 256, respectively. We use packing (Raffel et al., 2020) to combine\nmultiple training examples into a single sequence, separating inputs from targets using a special EOS\ntoken. This instruction tuning takes around 60 hours on a TPUv3 with 128 cores. For all evaluations,\nwe report results on the ﬁnal checkpoint trained for 30k steps.\n3 R ESULTS\nWe evaluate FLAN on natural language inference, reading comprehension, closed-book QA, transla-\ntion, commonsense reasoning, coreference resolution, and struct-to-text. As described in §2.2, we\nevaluate on unseen tasks by grouping datasets into task clusters and holding out each cluster for\nevaluation while instruction tuning on all remaining clusters (i.e., each evaluation task cluster uses\na different checkpoint). For each dataset, we evaluate the mean of performance on all templates,\nwhich proxies the expected performance given a typical natural language instruction. As a dev set is\nsometimes available for manual prompt engineering (Brown et al., 2020), for each dataset we also\nobtain the test set performance using the template with the best dev set performance.\nFor comparison, we report zero and few-shot results for LaMDA-PT using the same prompts as\nGPT-3 (as LaMDA-PT is not suitable for natural instructions without instruction tuning). This\nbaseline provides the most direct ablation of how much instruction tuning helps. Instruction tuning\nsigniﬁcantly improves LaMDA-PT on most datasets.\nWe also show the zero-shot performances of GPT-3 175B (Brown et al., 2020) and GLaM 64B/64E\n(Du et al., 2021), as reported in their respective papers. With the best dev template, zero-shot FLAN\noutperforms zero-shot GPT-3 on 20 of 25 datasets and even surpasses GPT-3’s few-shot performance\non 10 datasets. With the best dev-template, zero-shot FLAN outperforms zero-shot GLaM on 13 of\n19 available datasets and one-shot GLaM on 11 of 19 datasets.\n2In this mixing scheme, a mixing rate maximum of 3,000 means that a dataset does not receive additional\nsampling weight for examples in excess of 3,000.\n4\nPublished as a conference paper at ICLR 2022\nOverall, we observe that instruction tuning is very effective on tasks naturally verbalized as instruc-\ntions (e.g., NLI, QA, translation, struct-to-text) and is less effective on tasks directly formulated as\nlanguage modeling, where instructions would be largely redundant (e.g., commonsense reasoning\nand coreference resolution tasks that are formatted as ﬁnishing an incomplete sentence or paragraph).\nResults on natural language inference, reading comprehension, closed-book QA, and translation are\nsummarized in Figure 5 and described below.\nNatural language inference\nReading comprehension\nClosed-book QA\n0 10080604020\nZero-shot performance\nTQA\nNQ\nARC-c\nARC-e\nOBQA\nRTE\nCB\nANLI R3\nANLI R2\nANLI R1\nMultiRC\nBoolQ\nTranslation\nEN to FR\nEN to DE\nFR to EN\nDE to EN\nEN to RO\nRO to EN\nLaMDA-PT137B\nFLAN 137B\nGPT-3 175B\nGLaM 64B/64E\nSupervised model\nFigure 5: Zero-shot performance of FLAN compared to LaMDA-PT 137B, GPT-3 175B, and GLaM\n64B/64E on natural language inference, reading comprehension, closed-book QA, and translation.\nPerformance of FLAN is the mean of up to 10 instructional templates per task. Supervised models\nwere either T5, BERT, or translation models (speciﬁed in Table 2 and Table 1 in the Appendix).\nNatural language inference (NLI).On ﬁve NLI datasets, where a model must determine whether a\nhypothesis is true given some premise, FLAN outperforms all baselines by a large margin. As noted\nby Brown et al. (2020), perhaps one reason why GPT-3 struggles with NLI is that NLI examples are\nunlikely to have appeared naturally in an unsupervised training set and are thus awkwardly phrased\nas a continuation of a sentence. For FLAN, we phrase NLI as the more natural question “ Does\n<premise> mean that <hypothesis>?”, achieving much higher performance.\nReading comprehension.On reading comprehension, where models are asked to answer a question\nabout a provided passage, FLAN outperforms baselines for MultiRC (Khashabi et al., 2018) and\nOBQA (Mihaylov et al., 2018). On BoolQ (Clark et al., 2019a), FLAN outperforms GPT-3 by a large\nmargin, though LaMDA-PT already achieves high performance on BoolQ.\nClosed-book QA.For closed-book QA, which asks models to answer questions about the world\nwithout access to speciﬁc information containing the answer, FLAN outperforms GPT-3 on all four\ndatasets. Compared to GLaM, FLAN has better performance on ARC-e and ARC-c (Clark et al.,\n2018), and slightly lower performance on NQ (Lee et al., 2019; Kwiatkowski et al., 2019) and TQA\n(Joshi et al., 2017).\nTranslation. Similar to GPT-3, the training data for LaMDA-PT is around 90% English and includes\nsome text in other languages that was not speciﬁcally used to train the model to perform machine\ntranslation. We also evaluate FLAN’s performance on machine translation for the three datasets\nevaluated in the GPT-3 paper: French–English from WMT’14 (Bojar et al., 2014), and German–\n5\nPublished as a conference paper at ICLR 2022\nEnglish and Romanian–English from WMT’16 (Bojar et al., 2016). Compared with GPT-3, FLAN\noutperforms zero-shot GPT-3 for all six evaluations, though it underperforms few-shot GPT-3 in\nmost cases. Similar to GPT-3, FLAN shows strong results for translating into English and compares\nfavorably against supervised translation baselines. Translating from English into other languages,\nhowever, was relatively weaker, as might be expected given that FLAN uses an English sentencepiece\ntokenizer and that the majority of pretraining data is English.\nAdditional tasks. Although we see strong results for the above task clusters, one limitation with\ninstruction tuning is that it does not improve performance for many language modeling tasks (e.g.,\ncommonsense reasoning or coreference resolution tasks formulated as sentence completions). For\nseven commonsense reasoning and coreference resolution tasks (see Table 2 in the Appendix), FLAN\nonly outperforms LaMDA-PT on three of the seven tasks. This negative result indicates that when the\ndownstream task is the same as the original language modeling pre-training objective (i.e., in cases\nwhere instructions are largely redundant), instruction tuning is not useful. Finally, we report results for\nsentiment analysis, paraphrase detection, and struct-to-text, as well as additional datasets for which\nGPT-3 results are not available, in Table 2 and Table 1 in the Appendix. Generally, zero-shot FLAN\noutperforms zero-shot LaMDA-PT and is comparable with or better than few-shot LaMDA-PT.\n4 A BLATION STUDIES & FURTHER ANALYSIS\n4.1 N UMBER OF INSTRUCTION TUNING CLUSTERS\nAs the core question of our paper asks how instruction tuning improves a model’s zero-shot perfor-\nmance on unseen tasks, in this ﬁrst ablation we examine how performance is affected by the number\nof clusters and tasks used in instruction tuning. For this setup, we hold out NLI, closed-book QA, and\ncommonsense reasoning as evaluation clusters, and use the seven remaining clusters for instruction\ntuning.3 We show results for one to seven instruction tuning clusters, where clusters are added in\ndecreasing order of number of tasks per cluster.\nFigure 6 shows these results. As expected, we observe that average performance across the three\nheld-out clusters improves as we add additional clusters and tasks to instruction tuning (with the\nexception of the sentiment analysis cluster), conﬁrming the beneﬁts of our proposed instruction\ntuning approach on zero-shot performance on novel tasks. It is further interesting to see that, for\nthe seven clusters we test, the performance does not appear to saturate, implying that performance\nmay further improve with even more clusters added to instruction tuning. Of note, this ablation does\nnot allow us to draw conclusions about which instruction tuning cluster contributes the most to each\nevaluation cluster, although we see minimal added value from the sentiment analysis cluster.\nPerformance (%)  \non held-out cluster\nClusters used for instruction tuning\n50\n70\n90\n– Average\nHeld-out clusters\n30\n# clusters: 1 2 3 4 5 6 7\n+ summarization\n+ translation+ read. comp.\n(# datasets): (11) (20) (26) (30) (34) (37) (39)\n+ sentiment+ data to text+ coreference+ conv. QA\n– Commonsense\n NLI\n  Closed-book QA\n49.9\n55.0 59.3 59.2 60.8 61.9 63.5\nBase LM: Commonsense\nBase LM: NLI\nBase LM: Closed-book QA\nFigure 6: Adding additional task clusters to instruction tuning improves zero-shot performance on\nheld-out task clusters. The evaluation tasks are the following. Commonsense: CoPA, HellaSwag,\nPiQA, and StoryCloze. NLI: ANLI R1–R3, QNLI, RTE, SNLI, and WNLI. Closed-book QA: ARC\neasy, ARC challenge, Natural Questions, and TriviaQA.\n3We do not use the paraphrase or reading comprehension with commonsense clusters for instruction tuning\nin this ablation because they are too similar to NLI and commmonsense reasoning, respectively.\n6\nPublished as a conference paper at ICLR 2022\n4.2 S CALING LAWS\n0.4B 2B 8B 68B 137B\nModel Size (# parameters)\n30\n40\n50\n60\n70\nInstruction tuning\nUntuned model\nAverage zero-shot accuracy  \non 13 held-out tasks (%) \nPerformance on held-out tasks\nFigure 7: Whereas instruction tuning helps large\nmodels generalize to new tasks, for small models it\nactually hurts generalization to unseen tasks, poten-\ntially because all model capacity is used to learn the\nmixture of instruction tuning tasks.\nAs Brown et al. (2020) shows that zero and\nfew-shot capabilities of language models sub-\nstantially improve for larger models, we next\nexplore how the beneﬁts of instruction tuning\nare affected by model scale. Using the same\ncluster split as in the previous ablation study,\nwe evaluate the effect of instruction tuning\non models of size 422M, 2B, 8B, 68B, and\n137B parameters.\nFigure 7 shows these results. We see that\nfor the two models on the order of 100B pa-\nrameters, instruction tuning substantially im-\nproves performance on held-out tasks, as is\nexpected given the prior results in our pa-\nper. The behavior on held-out tasks for the\n8B and smaller models, however, is thought-\nprovoking—instruction tuning actually hurts performance on held-out tasks. One potential explana-\ntion for this result could be that for small-scale models, learning the∼40 tasks used during instruction\ntuning ﬁlls the entire model capacity, causing these models to perform worse on new tasks. Under\nthis potential explanation, for the larger scale models, instruction tuning ﬁlls up some model capacity\nbut also teaches these models how to follow instructions, allowing them to generalize to new tasks\nwith the remaining capacity.\n4.3 R OLE OF INSTRUCTIONS\n20 30 40 50 60\nFT: no instruction\nEval: instruction\nFT: dataset name\nEval: instruction\nFT: dataset name\nEval: dataset name\nFT: instruction\nEval: instruction\n(FLAN)\n37.3\n46.6\n47.0\n55.2\nZero-shot performance \n(4 task cluster avg.) \nFigure 8: Ablation study result using mod-\nels with instructions removed from ﬁnetun-\ning (FT).\nIn a ﬁnal ablation study, we explore the role of in-\nstructions during ﬁnetuning, as one possibility is that\nperformance gains come entirely from multi-task ﬁne-\ntuning and the model could perform just as well without\ninstructions. We hence consider two ﬁnetuning setups\nwithout instructions. In a no template setup, only inputs\nand outputs were given to the model (e.g., for transla-\ntion the input would be “The dog runs.” and the output\nwould be “Le chien court.”). In a dataset name setup,\neach input is prepended with the name of the task and\ndataset (e.g., for translation to French, the input would\nbe “[Translation: WMT’14 to French] The dog runs.”).\nWe compare these two ablations to FLAN’s ﬁnetun-\ning procedure, which used natural instructions (e.g.,\n“Please translate this sentence to French: ‘The dog\nruns. ’”). We perform evaluations for four held-out clus-\nters from Figure 5. For the no template setup, we used the FLAN instructions during zero-shot\ninference (because if we used no template, the model would not know what task to perform). For\nmodels ﬁnetuned on dataset name only, we report zero-shot performance for FLAN instructions as\nwell as using the dataset name. Figure 8 shows the results—both ablation conﬁgurations performed\nsubstantially worse than FLAN, indicating that training with instructions is crucial for zero-shot\nperformance on unseen tasks.\n4.4 I NSTRUCTIONS WITH FEW-SHOT EXEMPLARS\nSo far, we have focused on instruction tuning in the zero-shot setting. Here, we study how instruction\ntuning can be used when few-shot exemplars are available at inference time. The format for the\nfew-shot setting builds on the zero-shot format. For some input x and output y, let instruct(x)\ndenote the zero-shot instructions. Then, given k few-shot exemplars (xi, yi)k\ni=1 and a new input\nx, the instruction format for the few-shot setting is “instruct(x1) ⊕y1 ⊕instruct(x2) ⊕y2 ⊕. . .⊕\n7\nPublished as a conference paper at ICLR 2022\ninstruct(xk)⊕yk ⊕instruct(x)”, where ⊕denotes string concatenation with a delimiter token inserted\nin between. At both training and inference time, exemplars are randomly drawn from the training set,\nand the number of exemplars is capped at 16 and such that the total sequence length is less than 960\ntokens. Our experiment uses the same task splits and evaluation procedure as §3, such that few-shot\nexemplars for an unseen task are only used at inference time.\nAs shown in Figure 9, few-shot exemplars improve the performance on all task clusters, compared\nwith zero-shot FLAN. Exemplars are especially effective for tasks with large/complex output spaces,\nsuch as struct to text, translation, and closed-book QA, potentially because exemplars help the model\nbetter understand the output format. In addition, for all task clusters, standard deviation among\ntemplates is lower for few-shot FLAN, indicating reduced sensitivity to prompt engineering.\nNLI Read. Comp. Closed-Book QA Commonsense Coreference Translation\nZero-shot FLAN\nFew-shot FLAN\nPerformance\n20\n40\n60\n80\n54.7 59.3 59.6 60.0\n53.7\nStruct to text\n57.2\n31.0 33.0\n80.0 80.8\n63.8 67.4\n39.2\n49.4\nTask Cluster:\n# datasets: 7 5 3 4 2 3 4\nFigure 9: Adding few-shot exemplars to FLAN is a complementary method for improving the\nperformance of instruction-tuned models. The orange bars indicate standard deviation among\ntemplates, averaged at the dataset level for each task cluster.\n4.5 I NSTRUCTION TUNING FACILITATES PROMPT TUNING\n32 training \nexamples\nFull training \nset\n100\n0\n50\n75\n25\nPerformance after \nprompt tuning\nInstruction-tuned model\nUntuned model\n63.8\n78.179.1\n87.4\nFigure 10: Instruction-tuned\nmodels respond better to contin-\nuous inputs from prompt tuning.\nWhen prompt tuning on a given\ndataset, no tasks from the same\ncluster as that dataset were seen\nduring instruction tuning. Perfor-\nmance shown is the average on\nthe SuperGLUE dev set.\nAs we’ve seen that instruction tuning improves the ability of\na model to respond to instructions, it follows that, if FLAN is\nindeed more amenable to performing NLP tasks, then it should\nalso achieve better performance when performing inference using\nsoft prompts, represented by prepended continuous variables\noptimized via prompt tuning (Li & Liang, 2021; Lester et al.,\n2021). As further analysis, we train continuous prompts for each\nof the SuperGLUE (Wang et al., 2019a) tasks in accordance with\nthe cluster splits from §2.2 such that when prompt-tuning on task\nT, no tasks in the same cluster as T were seen during instruction\ntuning. Our prompt tuning setup follows the procedure of Lester\net al. (2021) except that we use a prompt length of 10, weight\ndecay of 1e-4, and did not use dropout on the attention scores; we\nfound in preliminary experiments that these changes improved\nthe performance of LaMDA-PT.\nFigure 10 shows the results of these prompt tuning experiments\nfor both using a fully-supervised training set and in a low-resource\nsetting with only 32 training examples. We see that in all sce-\nnarios, prompt tuning works better with FLAN than LaMDA-PT. In many cases, especially for the\nlow-resource setting, prompt tuning on FLAN even achieves more than 10% improvement over\nprompt tuning on the LaMDA-PT. This result exempliﬁes in another way how instruction tuning can\nresult in a checkpoint that is more desirable for performing NLP tasks.\n5 R ELATED WORK\nOur work relates to several broad research areas including zero-shot learning, prompting, multi-task\nlearning, and language models for NLP applications (Radford et al., 2019; Raffel et al., 2020; Brown\net al., 2020; Efrat & Levy, 2020; Aghajanyan et al., 2021; Li & Liang, 2021, inter alia). We describe\nprior work for these broad areas in an extended related work section (Appendix D), and here we\ndescribe two subareas narrower in scope that perhaps relate most closely to our work.\n8\nPublished as a conference paper at ICLR 2022\nThe way we ask a model to respond to instructions is similar to QA-based task formulation (Kumar\net al., 2016; McCann et al., 2018), which aims to unify NLP tasks by casting them as QA over a\ncontext. Though these methods are very similar to ours, they mostly focus on multi-task learning\ninstead of zero-shot learning, and—as noted by Liu et al. (2021)—they are generally not motivated\nby using existing knowledge in pretrained LMs. Moreover, our work supercedes recent work such as\nChai et al. (2020) and Zhong et al. (2021) in terms of both model scale and scope of tasks.\nThe success of language models has led to nascent research on the ability of models to follow\ninstructions. Most recently, Mishra et al. (2021) ﬁnetune 140M parameter BART on instructions\nwith few-shot exemplars, and evaluate its few-shot abilities on unseen tasks—this is similar to our\nfew-shot instruction tuning result from §4.4. This promising result (as well as one from Ye et al.\n(2021), which does not emphasize instructions as much) suggests that ﬁnetuning on a collection of\ntasks improves few-shot performance on unseen tasks, even at a smaller model scale. Sanh et al.\n(2021) ﬁnetune T5 in a setup similar to ours, ﬁnding that zero-shot learning can be improved in a\nmodel of 11B parameters. At a model scale similar to ours, OpenAI’s InstructGPT models are trained\nvia both ﬁnetuning and reinforcement learning to produce outputs that are more preferred by human\nraters (Ouyang et al., 2022).\n6 D ISCUSSION\nOur paper has explored a simple question in zero-shot prompting: does ﬁnetuning a model on a\ncollection of tasks phrased as instructions improve its performance on unseen tasks? We operationalize\nthis question via instruction tuning, a simple method that combines appealing aspects of both\nthe pretrain–ﬁnetune and prompting paradigms. Our instruction-tuned model, FLAN, improves\nperformance against an untuned model and surpasses zero-shot GPT-3 on the majority of tasks that\nwe evaluate on. Ablation studies reveal that performance on unseen tasks improves with the number\nof instruction tuning task clusters, and, interestingly, that performance improvements from instruction\ntuning emerge only with sufﬁcient model scale. Moreover, instruction tuning can be combined with\nother prompting methods such as few-shot prompting and prompt tuning.\nThe diverse capabilities of language models at scale have drawn attention to the tradeoffs between\nspecialist models (one model per task) and generalist models (one model for many tasks; Arivazhagan\net al., 2019; Pratap et al., 2020), for which our study has potential implications. Although one might\nexpect labeled data to have the most natural role in improving specialist models, instruction tuning\ndemonstrates how labeled data can be used to help large language models perform many, unseen\ntasks. In other words, the positive effect of instruction tuning on cross-task generalization shows that\ntask-speciﬁc training is complementary to general language modeling and motivates further research\non generalist models.\nAs for limitations of our study, there is a degree of subjectivity in assigning tasks to clusters (though\nwe try to use accepted categorizations in the literature), and we only explore the use of relatively\nshort instructions of typically a single sentence (c.f. detailed instructions given to crowd-workers).\nA limitation for our evaluation is that individual examples might have appeared in the models’\npretraining data, which includes web documents, though in post-hoc analysis (Appendix C) we do\nnot ﬁnd any evidence that data overlap substantially impacted the results. Finally, the scale of FLAN\n137B makes it costly to serve. Future work on instruction tuning could include gathering/generating\neven more task clusters for ﬁnetuning, cross-lingual experiments, using FLAN to generate data for\ntraining downstream classiﬁers, and using ﬁnetuning to improve model behavior with respect to bias\nand fairness (Solaiman & Dennison, 2021).\n7 C ONCLUSIONS\nThis paper has explored a simple method for improving the ability of language models at scale to\nperform zero-shot tasks based purely on instructions. Our instruction-tuned model, FLAN, compares\nfavorably against GPT-3 and signals the potential ability for language models at scale to follow\ninstructions. We hope that our paper will spur further research on instructions-based NLP, zero-shot\nlearning, and using labeled data to improve large language models.\n9\nPublished as a conference paper at ICLR 2022\nETHICAL CONSIDERATIONS\nThis work uses language models, for which the risks and potential harms are discussed in Bender &\nKoller (2020), Brown et al. (2020), Bender et al. (2021), Patterson et al., (2021), and others. As our\ncontribution in this paper is not a pretrained language model itself but rather an empirical study of\nhow instruction tuning affects the zero-shot performance of a language model on unseen tasks, we\nadditionally highlight two relevant ethical considerations. First, labeled datasets such as those we\nuse for ﬁnetuning can contain undesirable biases, and these biases can be propagated into zero-shot\napplications of the model on downstream tasks. And second, instruction-tuned models can potentially\nrequire less data and expertise to use; such lower barriers to access could increase both the beneﬁts\nand associated risks of such models.\nENVIRONMENTAL CONSIDERATIONS\nWe use the same pretrained language models as Austin et al. (2021). The energy cost and carbon\nfootprint for the pretrained models were 451 MWh and 26 tCO2e, respectively. The additional\ninstruction tuning gradient-steps for ﬁnetuning FLAN is less than 2% of the number of pretraining\nsteps, and so the estimated additional energy cost is comparatively smaller.\nAUTHOR CONTRIBUTIONS\nMaarten Bosma conceived the original idea and implemented the ﬁrst version of FLAN. Vincent Zhao\nprototyped the training and evaluation pipelines, as well as rank classiﬁcation. Kelvin Guu proposed\nand implemented the idea of task clusters and evaluation using inter-cluster splits. Jason Wei, Maarten\nBosma, Vincent Zhao, and Adams Wei Yu implemented the NLP tasks. Jason Wei, Vincent Zhao,\nand Adams Wei Yu conducted and managed most of the experiments. Jason Wei designed and ran the\nablation studies. Jason Wei, Maarten Bosma, and Quoc V . Le wrote most of the paper. Jason Wei,\nMaarten Bosma, and Nan Du obtained the zero and few-shot baselines. Vincent Zhao and Kelvin Guu\ndesigned, implemented, and conducted the few-shot FLAN experiments. Maarten Bosma and Jason\nWei ran the data contamination analysis. Brian Lester ran the prompt tuning experiments. Quoc V . Le\nand Andrew M. Dai advised, provided high-level guidance, and helped edit the paper.\nACKNOWLEDGEMENTS\nWe thank Ed Chi, Slav Petrov, Dan Garrette, Ruibo Liu, and Clara Meister for providing feedback\non our manuscript. We thank Adam Roberts, Liam Fedus, Hyung Won Chung, and Noam Shazeer\nfor helping debug some of our models. We thank Ellie Pavlick for feedback on the study design\nduring the middle stages of the project. We thank Daniel De Freitas Adiwardana for helping initiate\nthe project, large language model advising, and giving us access to some computational resources.\nFinally, we thank the team involved in pretraining LaMDA-PT: Daniel De Freitas Adiwardana, Noam\nShazeer, Yanping Huang, Dmitry Lepikhin, Dehao Chen, Yuanzhong Xu and Zhifeng Chen.\n10\nPublished as a conference paper at ICLR 2022\nREFERENCES\nArmen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and\nSonal Gupta. Muppet: Massive multi-task representations with pre-ﬁnetuning. arXiv preprint\narXiv:2101.11038, 2021. URL https://arxiv.org/abs/2101.11038.\nNaveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun,\nMia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al. Massively multilingual neural machine\ntranslation in the wild: Findings and challenges. arXiv preprint arXiv:1907.05019, 2019. URL\nhttps://arxiv.org/abs/1907.05019.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large\nlanguage models. arXiv preprint arXiv:2108.07732, 2021. URL https://arxiv.org/abs/\n2108.07732.\nAmittai Axelrod, Xiaodong He, and Jianfeng Gao. Domain adaptation via pseudo in-domain data\nselection. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language\nProcessing, pp. 355–362, 2011. URL https://aclanthology.org/D11-1033.\nMarta Bañón, Pinzhen Chen, Barry Haddow, Kenneth Heaﬁeld, Hieu Hoang, Miquel Esplà-Gomis,\nMikel L. Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio Ortiz Rojas, Leopoldo\nPla Sempere, Gema Ramírez-Sánchez, Elsa Sarrías, Marek Strelec, Brian Thompson, William\nWaites, Dion Wiggins, and Jaume Zaragoza. ParaCrawl: Web-scale acquisition of parallel corpora.\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp.\n4555–4567, 2020. URL https://aclanthology.org/2020.acl-main.417.\nEmily M. Bender and Alexander Koller. Climbing towards NLU: On meaning, form, and under-\nstanding in the age of data. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pp. 5185–5198, 2020. URL https://aclanthology.org/\n2020.acl-main.463.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On\nthe dangers of stochastic parrots: Can language models be too big? In Proceedings of the\n2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT ’21, pp. 610–623.\nAssociation for Computing Machinery, 2021. URLhttps://doi.org/10.1145/3442188.\n3445922.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The Fifth PASCAL Recognizing\nTextual Entailment Challenge. In TAC, 2009. URL https://citeseerx.ist.psu.edu/\nviewdoc/download?doi=10.1.1.232.1231&rep=rep1&type=pdf.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning\nabout physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artiﬁcial\nIntelligence, 2020. URL https://arxiv.org/abs/1911.11641.\nOndˇrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz,\nMatt Post, and Lucia Specia (eds.). Proceedings of the Ninth Workshop on Statistical Machine\nTranslation, 2014. URL https://aclanthology.org/W14-3300.\nOndˇrej Bojar, Christian Buck, Rajen Chatterjee, Christian Federmann, Liane Guillou, Barry Haddow,\nMatthias Huck, Antonio Jimeno Yepes, Aurélie Névéol, Mariana Neves, Pavel Pecina, Martin\nPopel, Philipp Koehn, Christof Monz, Matteo Negri, Matt Post, Lucia Specia, Karin Verspoor, Jörg\nTiedemann, and Marco Turchi (eds.). Proceedings of the First Conference on Machine Translation:\nVolume 1, Research Papers, 2016. URL https://aclanthology.org/W16-2200.\nRishi Bommasani, Drew A. Hudson, E. Adeli, R. Altman, Simran Arora, Sydney von Arx, Michael S.\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, E. Brynjolfsson, S. Buch, D. Card,\nRodrigo Castellon, Niladri S. Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora\nDemszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, S. Ermon, J. Etchemendy, Kawin\nEthayarajh, L. Fei-Fei, Chelsea Finn, Trevor Gale, Lauren E. Gillespie, Karan Goel, Noah D. Good-\nman, S. Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho,\n11\nPublished as a conference paper at ICLR 2022\nJenny Hong, Kyle Hsu, Jing Huang, Thomas F. Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri,\nSiddharth Karamcheti, G. Keeling, Fereshte Khani, O. Khattab, Pang Wei Koh, M. Krass, Ranjay\nKrishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, J. Leskovec, Is-\nabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir P.\nMirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, D. Narayanan, Ben\nNewman, Allen Nie, J. C. Niebles, H. Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel\nPapadimitriou, Joon Sung Park, C. Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan,\nRobert Reich, Hongyu Ren, Frieda Rong, Yusuf H. Roohani, Camilo Ruiz, Jack Ryan, Christopher\nR’e, D. Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, K. Srinivasan, Alex Tamkin, Rohan\nTaori, Armin W. Thomas, Florian Tramèr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu,\nYuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, M. Zaharia, Michael Zhang,\nTianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On\nthe opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. URL\nhttps://arxiv.org/abs/2108.07258.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large\nannotated corpus for learning natural language inference. In Proceedings of the 2015 Conference\non Empirical Methods in Natural Language Processing , pp. 632–642, 2015. URL https:\n//aclanthology.org/D15-1075.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. Language models are few-shot learners. InAdvances in Neural Information Pro-\ncessing Systems, volume 33, pp. 1877–1901, 2020. URL https://proceedings.neurips.\ncc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nDuo Chai, Wei Wu, Qinghong Han, Fei Wu, and Jiwei Li. Description based text classiﬁcation with\nreinforcement learning. In Proceedings of the International Conference on Machine Learning, pp.\n1371–1382. PMLR, 2020. URL http://proceedings.mlr.press/v119/chai20a/\nchai20a.pdf.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Edwards,\nYura Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained\non code. arXiv preprint arXiv:2107.03374, 2021. URL https://arxiv.org/abs/2107.\n03374.\nEunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke\nZettlemoyer. QuAC: Question answering in context. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing , pp. 2174–2184, 2018. URL https:\n//aclanthology.org/D18-1241.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. BoolQ: Exploring the surprising difﬁculty of natural yes/no questions. In Proceedings\nof the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2924–2936,\n2019a. URL https://aclanthology.org/N19-1300.\nKevin Clark, Minh-Thang Luong, Urvashi Khandelwal, Christopher D. Manning, and Quoc V . Le.\nBAM! born-again multi-task networks for natural language understanding. In Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics , pp. 5931–5937, 2019b.\nURL https://aclanthology.org/P19-1595.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? Try ARC, the AI2 reasoning challenge.\narXiv preprint arXiv:1803.05457, 2018. URL https://arxiv.org/abs/1803.05457.\nRonan Collobert, Jason Weston, Léon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel\nKuksa. Natural language processing (almost) from scratch. Journal of Machine Learning\n12\nPublished as a conference paper at ICLR 2022\nResearch, 12:2493–2537, 2011. URL https://www.jmlr.org/papers/volume12/\ncollobert11a/collobert11a.pdf.\nMichele Corazza, Stefano Menini, Elena Cabrio, Sara Tonelli, and Serena Villata. Hybrid emoji-\nbased masked language models for zero-shot abusive language detection. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2020 , pp. 943–949, 2020. URL https:\n//aclanthology.org/2020.findings-emnlp.84.\nIdo Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL Recognising Textual Entailment\nchallenge. In Proceedings of the First International Conference on Machine Learning Challenges:\nEvaluating Predictive Uncertainty Visual Object Classiﬁcation, and Recognizing Textual Entail-\nment, MLCW’05, pp. 177–190, 2005. URL https://doi.org/10.1007/11736790_9.\nAndrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Proceedings of the Confer-\nence on Neural Information Processing Systems, 2015. URL https://papers.nips.cc/\npaper/2015/file/7137debd45ae4d0ab9aa953017286b20-Paper.pdf.\nMarie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The CommitmentBank:\nInvestigating projection in naturally occurring discourse. InProceedings of Sinn und Bedeutung, pp.\n107–124, 2019. URL https://ojs.ub.uni-konstanz.de/sub/index.php/sub/\narticle/view/601.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of\ndeep bidirectional transformers for language understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019. URL\nhttps://aclanthology.org/N19-1423.\nWilliam B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005. URL\nhttps://aclanthology.org/I05-5002.\nNan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. GLaM: Efﬁcient scaling of language\nmodels with mixture-of-experts. arXiv preprint arXiv:2112.06905 , 2021. URL https://\narxiv.org/pdf/2112.06905.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.\nDROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In\nProceedings of the 2019 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.\n2368–2378, 2019. URL https://aclanthology.org/N19-1246.\nNadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heaﬁeld. Edinburgh’s phrase-based\nmachine translation systems for WMT-14. In Proceedings of the Ninth Workshop on Statistical\nMachine Translation, pp. 97–104, 2014. URL https://aclanthology.org/W14-3309.\nOndˇrej Dušek, David M. Howcroft, and Verena Rieser. Semantic noise matters for neural natural\nlanguage generation. In Proceedings of the 12th International Conference on Natural Language\nGeneration, pp. 421–426, 2019. URL https://aclanthology.org/W19-8652.\nSergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation at scale.\nIn Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,\npp. 489–500, 2018. URL https://aclanthology.org/D18-1045.\nAvia Efrat and Omer Levy. The Turking Test: Can language models understand instructions? arXiv\npreprint arXiv:2010.11982, 2020. URL https://arxiv.org/abs/2010.11982.\nAlexander Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. Multi-news: A large-scale\nmulti-document summarization dataset and abstractive hierarchical model. In Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics, pp. 1074–1084, 2019. URL\nhttps://aclanthology.org/P19-1102.\n13\nPublished as a conference paper at ICLR 2022\nFast.AI. Yelp Sentiment Classiﬁcation Dataset. https://course.fast.ai/datasets.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\nmodels with simple and efﬁcient sparsity. arXiv preprint arXiv:2101.03961, 2021. URL https:\n//arxiv.org/abs/2101.03961.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\ndeep networks. In Proceedings of the International Conference on Machine Learning (ICML), pp.\n1126–1135, 2017. URL https://arxiv.org/abs/1703.03400.\nTianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguis-\ntics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long\nPapers), pp. 3816–3830, 2021. URL https://aclanthology.org/2021.acl-long.\n295.\nClaire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. The WebNLG\nchallenge: Generating text from RDF data. In Proceedings of the 10th International Conference\non Natural Language Generation, pp. 124–133, 2017. URL https://aclanthology.org/\nW17-3518.\nSebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, An-\nuoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan\nDas, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ondˇrej Dušek, Chris Chinenye Emezue, Varun\nGangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani,\nYangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica\nMaddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique\nMartins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi\nNarayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura\nPerez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank San-\nthanam, João Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio\nSobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola,\nand Jiawei Zhou. The GEM benchmark: Natural language generation, its evaluation and metrics.\nIn Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics\n(GEM 2021), pp. 96–120, 2021. URL https://aclanthology.org/2021.gem-1.10.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing\ntextual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment\nand Paraphrasing, pp. 1–9, 2007. URL https://aclanthology.org/W07-1401.\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. SAMSum corpus: A human-\nannotated dialogue dataset for abstractive summarization. In Proceedings of the 2nd Workshop\non New Frontiers in Summarization, pp. 70–79, 2019. URL https://aclanthology.org/\nD19-5409.\nAlec Go, Richa Bhayani, and Lei Huang. Twitter sentiment classiﬁcation using distant supervision.\nCS224N project report, Stanford , 1(12):2009, 2009. URL https://www-cs.stanford.\nedu/people/alecmgo/papers/TwitterDistantSupervision09.pdf.\nDan Goldwasser and Dan Roth. Learning from natural instructions. Machine learn-\ning, 94(2):205–232, 2014. URL https://link.springer.com/article/10.1007/\ns10994-013-5407-y .\nMax Grusky, Mor Naaman, and Yoav Artzi. Newsroom: A dataset of 1.3 million summaries with\ndiverse extractive strategies. InProceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long\nPapers), pp. 708–719, 2018. URL https://aclanthology.org/N18-1065.\nR Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan\nSzpektor. The Second PASCAL Recognising Textual Entailment Challenge. In Proceedings\nof the Second PASCAL Challenges Workshop on Recognising Textual Entailment, 2006. URL\nhttp://www.cs.biu.ac.il/~szpekti/papers/RTE2-organizers.pdf.\n14\nPublished as a conference paper at ICLR 2022\nLuheng He, Mike Lewis, and Luke Zettlemoyer. Question-answer driven semantic role labeling:\nUsing natural language to annotate natural language. In Proceedings of the 2015 Conference\non Empirical Methods in Natural Language Processing , pp. 643–653, 2015. URL https:\n//aclanthology.org/D15-1076.\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi, and Luke Zettlemoyer. Surface form\ncompetition: Why the highest probability answer isn’t always right. In Proceedings of the\n2021 Conference on Empirical Methods in Natural Language Processing, 2021. URL https:\n//aclanthology.org/2021.emnlp-main.564.\nEduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran. To-\nward semantics-based answer pinpointing. In Proceedings of the First International Confer-\nence on Human Language Technology Research, 2001. URL https://www.aclweb.org/\nanthology/H01-1069.\nJeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pp. 328–339, 2018. URL https://aclanthology.org/P18-1031.\nLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Cosmos QA: Machine reading\ncomprehension with contextual commonsense reasoning. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP), pp. 2391–2401, 2019. URL https://\naclanthology.org/D19-1243.\nMelvin Johnson, Mike Schuster, Quoc V . Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil\nThorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey\nDean. Google’s multilingual neural machine translation system: Enabling zero-shot transla-\ntion. Transactions of the Association for Computational Linguistics , 5:339–351, 2017. URL\nhttps://aclanthology.org/Q17-1024.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pp. 1601–\n1611, 2017. URL https://aclanthology.org/P17-1147.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. Looking\nbeyond the surface: A challenge set for reading comprehension over multiple sentences. In\nProceedings of the 2018 Conference of the North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 252–262, 2018.\nURL https://aclanthology.org/N18-1023.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and\nHannaneh Hajishirzi. UNIFIEDQA: Crossing format boundaries with a single QA system. In\nFindings of the Association for Computational Linguistics: EMNLP 2020, pp. 1896–1907, 2020.\nURL https://aclanthology.org/2020.findings-emnlp.171.\nDimitrios Kotzias, Misha Denil, Nando de Freitas, and Padhraic Smyth. From group to individual\nlabels using deep features. Proceedings of the 21th ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining , 2015. URL https://dl.acm.org/doi/10.\n1145/2783258.2783380.\nTaku Kudo and John Richardson. Sentencepiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing. In Eduardo Blanco and Wei Lu (eds.),\nProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,\nEMNLP 2018: System Demonstrations, Brussels, Belgium, October 31 - November 4, 2018, pp.\n66–71. Association for Computational Linguistics, 2018. doi: 10.18653/v1/d18-2012. URL\nhttps://doi.org/10.18653/v1/d18-2012.\nAnkit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor\nZhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for\nnatural language processing. In Proceedings of the International Conference on Machine Learning,\npp. 1378–1387. PMLR, 2016. URL https://arxiv.org/abs/1506.07285.\n15\nPublished as a conference paper at ICLR 2022\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion\nJones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav\nPetrov. Natural Questions: A benchmark for question answering research. Transactions of the\nAssociation for Computational Linguistics, 7:452–466, 2019. URL https://aclanthology.\norg/Q19-1026.\nFaisal Ladhak, Esin Durmus, Claire Cardie, and Kathleen McKeown. WikiLingua: A new\nbenchmark dataset for cross-lingual abstractive summarization. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020 , pp. 4034–4048, 2020. URL https:\n//aclanthology.org/2020.findings-emnlp.360.\nChristoph H Lampert, Hannes Nickisch, and Stefan Harmeling. Learning to detect unseen object\nclasses by between-class attribute transfer. In 2009 IEEE Conference on Computer Vision and\nPattern Recognition, pp. 951–958. IEEE, 2009. URL https://ieeexplore.ieee.org/\ndocument/5206594.\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli ´c, and Goran Glavaš. From zero to hero: On the\nlimitations of zero-shot language transfer with multilingual Transformers. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 4483–4499,\n2020. URL https://aclanthology.org/2020.emnlp-main.363.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised\nopen domain question answering. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pp. 6086–6096, 2019. URL https://aclanthology.org/\nP19-1612.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\ncomputation and automatic sharding. In International Conference on Learning Representations,\n2020. URL https://openreview.net/forum?id=qrwe7XHTmYb.\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt\ntuning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing,\n2021. URL https://arxiv.org/abs/2104.08691.\nHector Levesque, Ernest Davis, and Leora Morgenstern. The Winograd Schema Challenge. In\nThirteenth International Conference on the Principles of Knowledge Representation and Reasoning,\n2012. URL https://dl.acm.org/doi/10.5555/3031843.3031909.\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction\nvia reading comprehension. In Proceedings of the 21st Conference on Computational Natural\nLanguage Learning (CoNLL 2017) , pp. 333–342, 2017. URL https://aclanthology.\norg/K17-1034.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and comprehension. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics , pp. 7871–7880, 2020. URL\nhttps://aclanthology.org/2020.acl-main.703.\nXiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the\n11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),\npp. 4582–4597, 2021. URL https://aclanthology.org/2021.acl-long.353.\nXiaoya Li, Jingrong Feng, Yuxian Meng, Qinghong Han, Fei Wu, and Jiwei Li. A uniﬁed\nMRC framework for named entity recognition. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics , pp. 5849–5859, 2020. URL https:\n//aclanthology.org/2020.acl-main.519.\n16\nPublished as a conference paper at ICLR 2022\nXin Li and Dan Roth. Learning question classiﬁers. In COLING 2002: The 19th International Confer-\nence on Computational Linguistics, 2002. URL https://www.aclweb.org/anthology/\nC02-1150.\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei Zhou, Chandra Bhagavatula, Yejin Choi, and\nXiang Ren. CommonGen: A constrained text generation challenge for generative commonsense\nreasoning. In Findings of the Association for Computational Linguistics: EMNLP 2020 , pp.\n1823–1840, 2020. URL https://aclanthology.org/2020.findings-emnlp.165.\nHan Liu, Xiaotong Zhang, Lu Fan, Xuandi Fu, Qimai Li, Xiao-Ming Wu, and Albert Y .S. Lam.\nReconstructing capsule networks for zero-shot intent classiﬁcation. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-IJCNLP), pp. 4799–4809, 2019a. URL\nhttps://aclanthology.org/D19-1486.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhenbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train,\nprompt, and predict: A systematic survey of prompting methods in natural language processing.\narXiv preprint arXiv:2107.13586, 2021. URL https://arxiv.org/abs/2107.13586.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for\nnatural language understanding. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pp. 4487–4496, 2019b. URL https://aclanthology.org/\nP19-1441.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike\nLewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation.\nTransactions of the Association for Computational Linguistics, 8:726–742, 2020. URL https:\n//aclanthology.org/2020.tacl-1.47.\nMinh-Thang Luong, Quoc V Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. Proceedings of ICLR, 2016. URL https://arxiv.org/\nabs/1511.06114.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y . Ng, and Christopher\nPotts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting\nof the Association for Computational Linguistics: Human Language Technologies, pp. 142–150,\nPortland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http:\n//www.aclweb.org/anthology/P11-1015.\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language\ndecathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018.\nURL https://arxiv.org/abs/1806.08730.\nJohn McCarthy. Programs with common sense. RLE and MIT computation center, 1960. URL\nhttp://jmc.stanford.edu/articles/mcc59/mcc59.pdf.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? A new dataset for open book question answering. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing, pp. 2381–2391, 2018. URL\nhttps://aclanthology.org/D18-1260.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn\nin context. arXiv preprint arXiv:2110.15943, 2021. URL https://arxiv.org/abs/2110.\n15943.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Natural Instructions:\nBenchmarking generalization to new tasks from natural language instructions. arXiv preprint\narXiv:2104.08773, 2021. URL https://arxiv.org/abs/2104.08773.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vander-\nwende, Pushmeet Kohli, and James Allen. A corpus and cloze evaluation for deeper understanding\nof commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, pp. 839–849,\n2016. URL https://aclanthology.org/N16-1098.\n17\nPublished as a conference paper at ICLR 2022\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh,\nXiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, Yangxiaokang Liu, Nadia Irwanto,\nJessica Pan, Faiaz Rahman, Ahmad Zaidi, Mutethia Mutuma, Yasin Tarabar, Ankit Gupta, Tao\nYu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani.\nDART: Open-domain structured data record to text generation. In Proceedings of the 2021\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pp. 432–447, 2021. URL https://aclanthology.org/\n2021.naacl-main.37.\nCourtney Napoles, Matthew Gormley, and Benjamin Van Durme. Annotated Gigaword. In Pro-\nceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale\nKnowledge Extraction (AKBC-WEKEX), pp. 95–100, 2012. URL https://aclanthology.\norg/W12-3018.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Language Processing, pp. 1797–1807, 2018. URL\nhttps://aclanthology.org/D18-1206.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial\nNLI: A new benchmark for natural language understanding. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, pp. 4885–4901, 2020. URL https:\n//aclanthology.org/2020.acl-main.441.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and\nRyan Lowe. Training language models to follow instructions with human feedback. Preprint,\n2022. URL https://cdn.openai.com/papers/Training_language_models_\nto_follow_instructions_with_human_feedback.pdf.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,\nand Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Papers), pp. 2227–2237, 2018. URL https:\n//aclanthology.org/N18-1202.\nNgoc-Quan Pham, Jan Niehues, Thanh-Le Ha, and Alexander Waibel. Improving zero-shot translation\nwith language-independent constraints. In Proceedings of the Fourth Conference on Machine\nTranslation (Volume 1: Research Papers), pp. 13–23, 2019. URL https://aclanthology.\norg/W19-5202.\nMohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for\nevaluating context-sensitive meaning representations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and Short Papers), pp. 1267–1273, 2019. URL\nhttps://aclanthology.org/N19-1128.\nVineel Pratap, Anuroop Sriram, Paden Tomasello, Awni Hannun, Vitaliy Liptchinsky, Gabriel\nSynnaeve, and Ronan Collobert. Massively multilingual ASR: 50 languages, 1 model, 1 billion\nparameters. arXiv preprint arXiv:2007.03001, 2020. URL https://arxiv.org/abs/2007.\n03001.\nGuanghui Qin and Jason Eisner. Learning how to ask: Querying LMs with mixtures of soft prompts.\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies (NAACL-HLT), pp. 5203–5212, 2021.\nURL http://cs.jhu.edu/~jason/papers/#qin-eisner-2021.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving\nlanguage understanding by generative pre-training. https://blog.openai.com/\nlanguage-unsupervised, 2018.\n18\nPublished as a conference paper at ICLR 2022\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage models are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019. URL\nhttps://d4mucfpksywv.cloudfront.net/better-language-models/\nlanguage_models_are_unsupervised_multitask_learners.pdf.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer. Journal of Machine Learning Research , 21(140):1–67, 2020. URL\nhttp://jmlr.org/papers/v21/20-074.html.\nAltaf Rahman and Vincent Ng. Resolving complex cases of deﬁnite pronouns: The Winograd\nschema challenge. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural\nLanguage Processing and Computational Natural Language Learning, pp. 777–789, 2012. URL\nhttps://aclanthology.org/D12-1071.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in\nNatural Language Processing, pp. 2383–2392, 2016. URL https://aclanthology.org/\nD16-1264.\nPranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions\nfor SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pp. 784–789, 2018. URL https://aclanthology.\norg/P18-2124.\nSiva Reddy, Danqi Chen, and Christopher D. Manning. CoQA: A conversational question answering\nchallenge. Transactions of the Association for Computational Linguistics, 7:249–266, 2019. URL\nhttps://aclanthology.org/Q19-1016.\nEmily Reif, Daphne Ippolito, Ann Yuan, Andy Coenen, Chris Callison-Burch, and Jason Wei. A\nrecipe for arbitrary text style transfer with large language models.arXiv preprint arXiv:2109.03910,\n2021. URL https://arxiv.org/abs/2109.03910.\nMelissa Roemmele, Cosmin Bejan, and Andrew Gordon. Choice of plausible alternatives: An\nevaluation of commonsense causal reasoning. In AAAI Spring Symposium Series, 2011. URL\nhttps://www.aaai.org/ocs/index.php/SSS/SSS11/paper/view/2418.\nBernardino Romera-Paredes and Philip Torr. An embarrassingly simple approach to zero-shot\nlearning. In Proceedings of the International Conference on Machine Learning, pp. 2152–2161,\n2015. URL https://proceedings.mlr.press/v37/romera-paredes15.pdf.\nSebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint\narXiv:1706.05098, 2017. URL https://arxiv.org/abs/1706.05098.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An adver-\nsarial winograd schema challenge at scale. In Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, pp. 8732–8740, 2020. URL https://arxiv.org/abs/1907.10641.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai,\nAntoine Chafﬁn, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training\nenables zero-shot task generalization. Proceedings of the International Conference on Learning\nRepresentations, 2021. URL https://arxiv.org/abs/2110.08207.\nDavid Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical\nreasoning abilities of neural models. Proceedings of the International Conference on Learning\nRepresentations, 2019. URL https://arxiv.org/pdf/1904.01557.\nTimo Schick and Hinrich Schütze. Exploiting cloze-questions for few-shot text classiﬁcation and\nnatural language inference. In Proceedings of the 16th Conference of the European Chapter\nof the Association for Computational Linguistics: Main Volume , pp. 255–269, 2021. URL\nhttps://aclanthology.org/2021.eacl-main.20.\n19\nPublished as a conference paper at ICLR 2022\nAbigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with\npointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) , pp. 1073–1083, 2017. URL https://\naclanthology.org/P17-1099.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Edinburgh neural machine translation systems\nfor WMT 16. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared\nTask Papers, pp. 371–376, 2016. URL https://aclanthology.org/W16-2323.\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory\ncost. In International Conference on Machine Learning , pp. 4596–4604. PMLR, 2018. URL\nhttps://arxiv.org/abs/1804.04235.\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,\npp. 1631–1642, 2013. URL https://aclanthology.org/D13-1170.\nIrene Solaiman and Christy Dennison. Process for adapting language models to society (palms) with\nvalues-targeted datasets. arXiv preprint arXiv:2106.10328, 2021. URL https://arxiv.org/\nabs/2106.10328.\nShashank Srivastava, Igor Labutov, and Tom Mitchell. Zero-shot learning of classiﬁers from natural\nlanguage quantiﬁcation. In Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) , pp. 306–316, 2018. URL https://\naclanthology.org/P18-1029.\nDerek Tam, Menton Rakesh R., Mohit Bansal, Shashank Srivastava, and Colin Raffel. Improving\nand simplifying pattern exploiting training. In Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), 2021. URL https://arxiv.org/pdf/\n2103.11955.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog\napplications. arXiv preprint arXiv:2201.08239 , 2022. URL https://arxiv.org/pdf/\n2201.08239.\nJoaquin Vanschoren. Meta-learning: A survey. arXiv preprint arXiv:1810.03548 , 2018. URL\nhttps://arxiv.org/abs/1810.03548.\nMarc Velay and Fabrice Daniel. Seq2seq and multi-task learning for joint intent and content\nextraction for domain speciﬁc interpreters. arXiv preprint arXiv:1808.00423, 2018. URL https:\n//arxiv.org/abs/1808.00423.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE:\nA multi-task benchmark and analysis platform for natural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for\nNLP, pp. 353–355, 2018. URL https://aclanthology.org/W18-5446.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R Bowman. Superglue: A stickier benchmark for general-purpose language\nunderstanding systems. Conference on Neural Information Processing Systems (NeurIPS), 2019a.\nURL https://arxiv.org/abs/1905.00537.\nLu Wang and Wang Ling. Neural network-based abstract generation for opinions and arguments.\nIn Proceedings of the 2016 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies , pp. 47–57, 2016. URL https:\n//www.aclweb.org/anthology/N16-1007.\nYiren Wang, Yingce Xia, Tianyu He, Fei Tian, Tao Qin, ChengXiang Zhai, and Tie-Yan Liu. Multi-\nagent dual learning. In Proceedings of the International Conference on Learning Representations\n(ICLR) 2019, 2019b. URL https://openreview.net/forum?id=HyGhN2A5tm.\n20\nPublished as a conference paper at ICLR 2022\nZirui Wang, Adams Wei Yu, Orhan Firat, and Yuan Cao. Towards zero-label language learning.arXiv\npreprint arXiv:2109.09193, 2021. URL https://arxiv.org/abs/2109.09193.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bowman. Neural network acceptability judgments.\nTransactions of the Association for Computational Linguistics, 7:625–641, 2019. doi: 10.1162/\ntacl_a_00290. URL https://aclanthology.org/Q19-1040.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\narXiv:2201.11903, 2022. URL https://arxiv.org/pdf/2201.11903.\nAdina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceedings of the 2018 Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pp. 1112–1122, 2018. URL http://aclweb.org/anthology/\nN18-1101.\nJoseph Worsham and J. Kalita. Multi-task learning for natural language processing in the 2020s:\nwhere are we going? arXiv preprint arXiv:2007.16008, 2020. URL https://arxiv.org/\nabs/2007.16008.\nJeff Wu, Long Ouyang, Daniel M Ziegler, Nissan Stiennon, Ryan Lowe, Jan Leike, and Paul Chris-\ntiano. Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862,\n2021. URL https://arxiv.org/abs/2109.10862.\nWei Wu, Fei Wang, Arianna Yuan, Fei Wu, and Jiwei Li. CorefQA: Coreference resolution as\nquery-based span prediction. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pp. 6953–6963, 2020. URL https://aclanthology.org/\n2020.acl-main.622.\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren. Crossﬁt: A few-shot learning challenge for cross-task\ngeneralization in NLP. In Proceedings of the 2021 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), 2021. URL https://arxiv.org/abs/2104.08835.\nWenpeng Yin, Jamaal Hay, and Dan Roth. Benchmarking zero-shot text classiﬁcation: Datasets,\nevaluation and entailment approach. In Proceedings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pp. 3914–3923, 2019. URL https://aclanthology.org/\nD19-1404.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a\nmachine really ﬁnish your sentence? In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pp. 4791–4800, 2019. URL https://aclanthology.org/\nP19-1472.\nRui Zhang and Joel Tetreault. This email could save your life: Introducing the task of email subject\nline generation. In Proceedings of the 57th Annual Meeting of the Association for Computational\nLinguistics, 2019. URL https://aclanthology.org/P19-1043.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.\nRecord: Bridging the gap between human and machine commonsense reading comprehension.\nCoRR, abs/1810.12885, 2018. URL http://arxiv.org/abs/1810.12885.\nXiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text clas-\nsiﬁcation. In NIPS, 2015. URL https://proceedings.neurips.cc/paper/2015/\nfile/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf.\nYuan Zhang, Jason Baldridge, and Luheng He. PAWS: Paraphrase adversaries from word scrambling.\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),\npp. 1298–1308, 2019. URL https://aclanthology.org/N19-1131.\nRuiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. Meta-tuning language models to answer\nprompts better. arXiv preprint arXiv:2104.04670, 2021. URL https://arxiv.org/abs/\n2104.04670.\n21\nPublished as a conference paper at ICLR 2022\nA A DDITIONAL RESULTS\nThis section shows the full results for all datasets we evaluate. Results for translation and struct to\ntext are shown in Table 1, and the results for eight NLU task clusters are shown in Table 2.\nWe show FLAN’s performance using the best of up to ten instruction templates as well as the template\nwith the best performance on the dev set. For LaMDA-PT, we use the templates from Brown et al.\n(2020), which were optimized for GPT-3, without performing any prompt engineering to optimize\nthem on our model. For simplicity, we use greedy search for all generative tasks (compared with\nbeam search used in Brown et al. (2020)). Unlike GPT-3, which chooses the number of few-shot\nexemplars k via best dev set performance, for few-shot LaMDA-PT we choose the highest k that ﬁts\nin the context length of 1024 tokens, from k ∈{1, 3, 5, 10}.\nFor DROP (Dua et al., 2019) and SQuADv2 (Rajpurkar et al., 2018), based on email correspondence\nwith Brown et al. (2020), their deﬁnition of zero-shot differs from ours in that they actually use\nexemplars, but only from the same passage as the inference question (each passage has more than\none question). Hence, GPT-3 zero-shot results are not directly comparable with ours for DROP and\nSQuADv2. We mark these results using the †symbol. Moreover, it is unclear how to parse the end of\nan answer for these two datasets, and so we use curly bracket delimiters { and }, where we expect }\nto indicate the end of the answer.\nFor struct to text, reported T5/mT5 results are from the GEM benchmark paper (Gehrmann et al.,\n2021), though we do not report their results for DART (through correspondence with authors, we\nconﬁrmed that their results for DART were incorrect). Though we use a summarization task cluster\nduring instruction tuning, we leave evaluation of summarization for future work, as the mean input of\nmost summarization datasets exceeds FLAN’s input length of 1024 tokens.\nFLAN 137B\nLaMDA-PT GPT-3 175B zero-shot few-shot\nMetric Supervised\nModel\nzero-\nshot\nfew-\nshot [k]\nzero-\nshot\nfew-\nshot [k]\naverage\ntemplate\nbest dev\ntemplate\naverage\ntemplate\nbest dev\ntemplate [k] #t\nTRANSLATION\nWMT ’14 En→Fr BLEU 35.0 d 11.2 31.5 [5] 25.2 32.6 [64] 32.9±1.1 33.9 33.9 ±0.2 33.8 [9] 5\nWMT ’14 Fr→En BLEU 45.6 c 7.2 34.7 [5] 21.2 39.2 [64] 35.5±1.3 35.9 38.0 ±0.1 37.9 [9] 3\nWMT ’16 En→De BLEU 38.6 f 7.7 26.7 [5] 24.6 29.7 [64] 25.4±1.8 27.0 26.8 ±0.4 26.1 [11] 5\nWMT ’16 De→En BLEU 41.2 e 20.8 36.8 [5] 27.2 40.6 [64] 38.9±0.3 38.9 40.6 ±0.1 40.7 [11] 3\nWMT ’16 En→Ro BLEU 39.9 g 3.5 22.9 [5] 14.1 21.0 [64] 16.7±1.6 18.9 20.5 ±0.1 20.5 [9] 5\nWMT ’16 Ro→En BLEU 38.5 g 9.7 37.5 [5] 19.9 39.5 [64] 36.8±0.5 37.3 38.2 ±0.1 38.1 [9] 3\nSTRUCT TO TEXT\nCommonGen Rouge-1 64.0 a 3.9 56.7 [3] – – 54.6±2.3 56.3 56.6 ±0.3 56.4 [16] 6\nRouge-2 29.4 a 1.5 29.6 [3] – – 28.8±2.4 27.6 30.9 ±0.7 29.9 [16] 6\nRouge-L 54.5 a 3.2 48.5 [3] – – 48.4±1.9 48.7 50.7 ±0.2 51.0 [16] 6\nDART Rouge-1 – 11.3 56.0 [3] – – 45.5±4.2 48.9 57.9 ±1.6 59.2 [11] 7\nRouge-2 – 1.5 29.6 [3] – – 25.0±3.7 30.0 35.8 ±1.0 36.2 [11] 7\nRouge-L – 3.2 48.5 [3] – – 38.4±3.8 43.4 48.5 ±0.9 48.2 [11] 7\nE2ENLG Rouge-1 72.6 a 6.2 56.7 [3] – – 44.8±3.9 51.4 59.1 ±1.3 59.7 [12] 9\nRouge-2 47.5 a 2.5 31.4 [3] – – 24.2±3.6 30.1 33.2 ±1.1 33.6 [12] 9\nRouge-L 56.4 a 4.9 41.1 [3] – – 37.0±3.5 42.4 44.9 ±0.8 45.1 [12] 9\nWebNLG Rouge-1 83.5 a 13.9 68.3 [3] – – 50.6±4.7 57.7 68.5 ±2.2 71.2 [10] 8\nRouge-2 63.6 a 6.9 46.0 [3] – – 29.8±4.2 35.4 48.0 ±1.5 49.8 [10] 8\nRouge-L 71.0 a 11.8 56.5 [3] – – 43.4±4.5 49.7 58.8 ±1.1 60.2 [10] 8\nTable 1: Results for translation and struct-to-text tasks. [k] indicates the number of few-shot\nexemplars. #t indicates the number of templates that FLAN is evaluated on.aT5-11B, cEdunov et al.\n(2018), dDurrani et al. (2014), eWang et al. (2019b), f Sennrich et al. (2016), gLiu et al. (2020).\n22\nPublished as a conference paper at ICLR 2022\nFLAN 137B\nGLaM LaMDA-PT GPT-3 175B zero-shot few-shot\nRandom\nGuess\nSupervised\nModel\nzero-\nshot\none-\nshot\nzero-\nshot\nfew-\nshot [k]\nzero-\nshot\nfew-\nshot [k]\naverage\ntemplate\nbest dev\ntemplate\naverage\ntemplate\nbest dev\ntemplate [k] #t\nNLI\nANLI R1 33.3 57.4 b 40.9 42.4 39.6 39.0 [5] 34.6 36.8 [50] 47.7±1.4 46.4 44.2 ±2.3 47.9 [6] 8\nANLI R2 33.3 48.3 b 38.2 40.0 39.9 37.5 [5] 35.4 34.0 [50] 43.9±1.3 44.0 41.6 ±1.4 41.1 [6] 8\nANLI R3 33.3 43.5 b 40.9 40.8 39.3 40.7 [5] 34.5 40.2 [50] 47.0±1.3 48.5 42.8 ±2.2 46.8 [6] 8\nCB 33.3 93.6 a 33.9 73.2 42.9 34.4 [5] 46.4 82.1 [32] 64.1±14.7 83.9 82.6 ±4.4 82.1 [7] 10\nMNLI-m 33.3 92.2 a – – 35.7 43.7 [5] – – 51.1±6.2 61.2 60.8 ±3.7 63.5 [10] 10\nMNLI-mm 33.3 91.9 a – – 37.0 43.8 [5] – – 51.0±6.5 62.4 61.0 ±3.5 63.5 [10] 10\nQNLI 50.0 96.9 a – – 50.6 55.7 [5] – – 59.6±4.9 66.4 62.0 ±1.7 63.3 [12] 9\nRTE 50.0 92.5 a 68.8 71.5 73.3 70.8 [5] 63.5 72.9 [32] 78.3±7.9 84.1 79.9 ±6.9 84.5 [8] 10\nSNLI 33.3 91.3 b – – 33.3 54.7 [5] – – 43.0±7.4 53.4 62.3 ±2.4 65.6 [15] 9\nWNLI 50.0 94.5 a – – 56.3 64.8 [5] – – 61.0±10.6 74.6 55.4 ±11.0 70.4 [14] 10\nREADING COMP.\nBoolQ 50.0 91.2 a 83.0 82.8 81.0 80.0 [1] 60.5 77.5 [32] 80.2±3.1 82.9 83.6 ±0.8 84.6 [4] 9\nDROP – 80.5b 54.9 55.2 3.8 10.3 [1] 23.6†36.5 [20] 21.9±0.9 22.7 22.3 ±1.1 23.9 [2] 7\nMultiRC – 88.1a 45.1 62.0 60.0 59.6 [5] 72.9 74.8 [32] 74.5±3.7 77.5 69.2 ±3.2 72.1 [1] 8\nOBQA 25.0 85.4 a 53.0 55.2 41.8 50.6 [10] 57.6 65.4 [100] 77.4±1.3 78.4 77.2 ±1.3 78.2 [16] 7\nSQuADv1 – 96.2a – – 22.7 50.2 [3] – – 79.5±1.6 80.1 82.1 ±0.5 82.7 [4] 8\nSQuADv2 – 83.4b 68.3 70.0 11.1 34.9 [3] 59.5†69.8 [16] 40.9±1.8 44.2 40.8 ±0.9 43.1 [3] 10\nCLOSED -BOOK QA\nARC-c 25.0 81.1 a 48.2 50.3 42.0 49.4 [10] 51.4 51.5 [50] 61.7±1.4 63.1 63.7 ±0.6 63.8 [13] 7\nARC-e 25.0 92.6 a 71.9 76.6 76.4 80.9 [10] 68.8 70.1 [50] 79.5±0.8 79.6 80.5 ±0.5 80.7 [14] 7\nNQ – 36.6a 21.5 23.9 3.2 22.1 [5] 14.6 29.9 [64] 18.6±2.7 20.7 27.2 ±0.5 27.6 [16] 10\nTQA (wiki) – 60.5a 68.8 71.5 21.9 63.3 [10] 64.3 71.2 [64] 66.5±2.6 68.1 66.5 ±1.0 67.3 [16] 10\nTQA (tfds-dev) – 51.0a – – 18.4 55.1 [10] – – – 55.0±2.3 56.7 57.2 ±0.6 57.8 [16] 10\nCOMMONSENSE\nCOPA 50.0 94.8 a 90.0 92.0 90.0 89.0 [10] 91.0 92.0 [32] 90.6±2.0 91.0 88.5 ±3.8 87.0 [16] 8\nHellaSwag 25.0 47.3 b 77.1 76.8 57.0 58.8 [10] 78.9 79.3 [20] 56.4±0.5 56.7 59.4 ±0.2 59.2 [3] 8\nPIQA 50.0 66.8 b 80.4 81.4 80.3 ∗80.2∗[10] 81.0 82.3 [50] 80.9∗±0.8 80.5∗ 82.1∗±0.3 81.7∗ [10] 8\nStoryCloze 50.0 89.2 b 82.5 84.0 79.5 83.7 [10] 83.2 87.7 [70] 92.2±1.3 93.4 93.3 ±0.9 94.7 [10] 8\nSENTIMENT\nIMDB 50.0 95.5 b – – 76.9 83.3 [1] – – 94.1±0.4 94.3 94.8 ±0.3 95.0 [2] 7\nSent140 50.0 87.0 b – – 41.4 63.3 [5] – – 69.9±2.5 73.5 68.7 ±1.2 69.3 [16] 6\nSST-2 50.0 97.5 a – – 51.0 92.3 [5] 71.6 95.6 [8] 92.6±1.7 94.6 94.4 ±0.8 94.6 [16] 8\nYelp 50.0 98.1 b – – 84.7 89.6 [3] – – 97.8±0.2 98.1 97.9 ±0.1 98.0 [4] 7\nPARAPHRASE\nMRPC 50.0 90.4 a – – 53.7 64.0 [5] – – 69.1±1.3 69.1 67.5 ±1.7 67.2 [10] 10\nQQP 50.0 90.6 a – – 34.9 58.9 [3] – – 72.1±6.8 75.9 73.5 ±2.9 75.9 [16] 7\nPAWS Wiki 50.0 91.9 a – – 45.5 53.5 [5] – – 61.5±6.5 69.4 66.2 ±0.9 70.2 [10] 10\nCOREFERENCE\nDPR 50.0 84.8 b – – 54.6 57.3 [5] – – 60.3±3.5 66.8 62.4 ±1.6 63.3 [16] 10\nWinogrande 50.0 65.8 b 73.4 73.0 68.3 68.4 [10] 70.2 77.7 [50] 67.3±2.5 71.2 72.3 ±0.9 72.8 [16] 10\nWSC273 50.0 70.0 b 86.8 83.9 81.0 61.5 [5] 88.3 88.5 [32] 80.8±3.7 – – ± – – [ – ] 10\nREAD . COMP. W/ COMMONSENSE\nCosmosQA 25.0 67.1 b – – 34.1 33.8 [5] – – 58.4±1.3 60.6 56.7 ±1.3 56.0 [5] 8\nReCoRD – 93.4a 90.3 90.3 87.8 ∗87.6∗[1] 90.2 89.0 [32] 67.8∗±3.0 72.5∗ 77.0∗±2.0 79.0∗ [1] 10\nTable 2: Results for eight NLU task clusters. All values shown are for accuracy (or exact match)\nexcept DROP, MultiRC, and SQuAD v1 and v2, which are F1.[k] indicates the number of few-shot\nexemplars. #t indicates the number of templates that FLAN is evaluated on.aT5-11B, bBERT-large.\n∗see data contamination (Appendix C). WSC273 (Levesque et al., 2012) does not have training or\nvalidation sets, and so we do not compute few-shot results for FLAN. For Trivia QA (TQA), we\nreport exact match (EM) on both the wikipedia subset of the dev set to compare with GPT-3, as well\nas the full TFDS dev set.\n23\nPublished as a conference paper at ICLR 2022\nB F URTHER ABLATION STUDIES AND ANALYSIS\nB.1 D ATASETS PER TASK CLUSTER & TEMPLATES PER DATASET\nOur primary hypothesis is that instruction tuning on a diverse set of tasks improves performance on\nunseen tasks. §4.1 showed that adding more task clusters improves performance; here, we further\nexplore whether adding additional datasets improves performance when the number of task clusters\nis held constant. We use the same split as in §4.1, where the NLI, commonsense reasoning, and\nclosed-book QA clusters are held-out, and seven other task clusters remain for instruction tuning. For\nthese seven task clusters, we instruction tune models using just one dataset per task cluster and using\nfour datasets per task cluster (for task clusters that did not have four tasks, we just used all available\ntasks). In addition, we simultaneously explore the role of the number of instruction templates per\ndataset; as mentioned in §2.1, for each dataset we manually composed ten instructional templates for\ninstruction tuning. Here, we instruction tune models using 1, 4, and 10 templates per dataset.\nFigure 11 shows these results. Using more datasets per cluster improved performance by almost\n10% on average across the three held-out clusters. Using more templates per dataset, however,\nhad a comparatively negligible effect on performance when there was one task per cluster, which\ndisappeared when there were four tasks per cluster. The small effect of templates is striking given our\noriginal motivation that composing ten templates per task would mitigate overﬁtting to any particular\ntemplate. This results serves to underscore, however, the unpredictability of ﬁnetuning large language\nmodels, as one hypothesis is that models at such scale do not easily overﬁt to a ﬁnetuning single task.\nFigure 11: Effect of datasets per task cluster and templates per dataset on performance on three\nheld-out clusters: NLI, commonsense reasoning, and closed-book QA. Adding more datasets per task\ncluster substantially improves performance. Using more templates per dataset, however, only had\na very small effect on performance, which disappeared when there were sufﬁcient dataset per task\ncluster.\nB.2 R OLE OF INSTRUCTIONS DURING FINETUNING\nThe per-cluster results for the ablation study from §4.3 are shown in Table 3.\nB.3 F URTHER ANALYSIS : I NSTRUCTION TUNING FACILITATES PROMPT TUNING\nThe per-dataset results for the analysis in §4.5 are given in Table 4. As the above tasks are all\nclassiﬁcation, further work in this direction might include tasks such as summarization or question\nanswering, or try to ﬁnetune the model using the supervised datasets.\nC D ATA CONTAMINATION ANALYSIS\nOne reasonable concern is that since the pretraining corpus of FLAN has more than 2 trillion tokens,\nit is possible that examples from a given evaluation dataset may have already been seen verbatim\nby the model during pre-training, hence inﬂating the performance of our purported zero-shot model.\nTo this end, like GPT-3 (Brown et al., 2020), we perform post-hoc data contamination analysis to\n24\nPublished as a conference paper at ICLR 2022\nZero-shot performance on unseen task cluster\nFinetuning prompt Inference prompt NLI Read.\nComp.\nClosed-\nBook QA Translation Four-Task\nAverage\nNatural instructions\n(= FLAN) Natural instructions 56.2 77.4 56.6 30.7 55.2\nNo template Natural instructions 50.5 58.2 25.5 15.0 37.3\nTask/dataset name Natural instructions 52.8 63.0 44.8 25.9 46.6\nTask/dataset name Task/dataset name 60.2 64.9 40.8 21.9 47.0\nTable 3: Ablation study result using models where instructions are removed from the ﬁnetuning\nprocess. In “no template,” only inputs and outputs are given, which does not distinguish among\ntasks during multi-task ﬁnetuning. In “task/dataset name”, inputs during multi-task ﬁnetuning are\nprepended with the name of the task and dataset (e.g., “[Translation: WMT’14 to French] The dog\nruns”) NLI datasets: ANLI R1–R3, CB, and RTE; reading comprehension datasets: BoolQ, MultiRC,\nand OpenbookQA; closed-book QA datasets: ARC-c, ARC-e, NQ, and TQA; translation datasets:\nWMT’14 Fr↔En, WMT’16 De↔En, and WMT’16 Ro↔En. Notably, training with task/dataset\nname achieved a high NLI score largely because it achieved a score of 83.9 on the CB dataset, for\nwhich the validation set only has 56 examples (FLAN also gets 83.9 with the best dev template, but\nthe average template was only 64.1).\nPROMPT TUNING ANALYSIS\nPrompt tuning\ntrain. examples\nBoolQ\nacc.\nCB\nacc.\nCoPA\nacc.\nMultiRC\nF1\nReCoRD\nacc.\nRTE\nacc.\nWiC\nacc.\nWSC\nacc.\nLaMDA-PT 32 55.5 55.4 87.0 65.4 78.0 52.4 51.6 65.4\nFLAN 77.5 87.5 91.0 76.8 80.8 83.0 57.8 70.2\nLaMDA-PT full\ndataset\n82.8 87.5 90.0 78.6 84.8 82.0 54.9 72.7\nFLAN 86.3 98.2 94.0 83.4 85.1 91.7 74.0 86.5\nTable 4: FLAN (instruction tuning) responds better to continuous inputs attained via prompt tuning\nthan LaMDA-PT (no instruction tuning). When prompt tuning on a given dataset, no tasks from the\nsame cluster as that dataset were seen during instruction tuning.\ninvestigate whether the performance of the model is in fact inﬂated by evaluating on examples that\noccurred in the pretraining dataset.\nOur data contamination procedure follows the setup of Brown et al. (2020), which, for each bench-\nmark, produces a “clean” version that removes all potentially leaked examples, deﬁned as examples\nfor which any n-gram (n varies per dataset but is roughly 13) overlapped with anything in the\npretraining corpus. We use the same n per dataset as Brown et al. (2020) and also split on spaces. We\nthen evaluate our model on this clean subset, comparing against model performance on the original\ndataset (clean + dirty). Lower performance on the clean subset would suggest that data contamination\nleads to inﬂated results.\nFigure 12 summarizes these results, with the exact numbers given in Table 5. We see several trends\nvery similar to those in the GPT-3 paper: (1) many datasets had a substantial number of examples\nthat overlapped with the pretraining data, (2) across all datasets, we do not see a correlation that\nevaluating on clean data does worse than evaluating on the total dataset, and (3) as datasets had fewer\nclean examples, there was higher variance in the percent change in performance (likely due to a\nsmaller number of clean examples).\nLike GPT-3, we also found that DROP and SQuADv2 had almost total overlap with the pretraining\ndata. We follow their procedure of manually inspecting the data, and ﬁnd that most overlapping n-\ngrams were only in the contexts of examples (99.6% for DROP and 97.2% for SQuADv2). Overlaps\nnever occurred in both the question and answer for DROP, and only occurred for both the question\nand answer for SQuADv2 in 5 of the 11,153 evaluation examples. Hence, for these two datasets, the\n25\nPublished as a conference paper at ICLR 2022\n0\n-20\n20\n40\n60\nPercent of Data Clean in Dataset \nPercent change \nin performance \nof FLAN \n(accuracy, F1, or \nBLEU)\neval on only \nclean data \ndid better  \neval on all data \n(including dirty) \ndid betterANLI R2\nSQuAD v2\nANLI R1\nDROP\nReCoRD PIQA\n25 500 75 100\nFigure 12: Like GPT-3, we also measured performance on cleaned versions of our datasets, which\nhad high conﬁdence to be unseen in the pretraining data of FLAN. We do not see a correlation that\nFLAN performed better on evaluation sets for which examples occurred more often in the pretraining\ndata. When the percent of clean data is very small, there are fewer examples for computing the clean\nperformance, which leads to high variance.\nmodel gains only background information and cannot memorize the answer to any speciﬁc questions\n(aside from the ﬁve examples in SQuADv2).\nANLI R1 and R2 (Nie et al., 2020) also had almost complete data contamination, to a much higher\ndegree than GPT-3. Upon further inspection, we see that most overlaps occur in example contexts\nand not hypotheses (97.3% for ANLI R1 and 98.2% for ANLI R2). As ANLI R1 and R2 are based\nentirely from Wikipedia examples (R3 is not), we posit that this higher degree of contamination in\nour pretraining dataset compared with GPT-3’s is potentially due to using a more-recent version\nof Wikipedia that includes the contexts used in ANLI R1 and R2 (which were collected in 2019).\nBecause seeing a particular context in pretraining does not help with the NLI task given a new, unseen\nsentence, we think it is unlikely that these overlaps affected performance on the two datasets.\nOf the remaining datasets, only ReCoRD and PIQA had a clean subset performance that was lower\nthan the overall evaluation set performance by more than 1%. These two datasets are language\nmodeling (i.e., “what’s the best continuation of this sentence?”), and so it is more likely compared\nwith previous tasks that seeing a complete sentence in the pretraining data could help the model\npredict the right answer in downstream evaluations. For PIQA, both the goal and solution had\noverlaps in 93 of the 1,838 evaluation examples, and for ReCoRD, the query had overlaps in 2,320 of\nthe 10,000 training examples. We hence mark these results with an asterisk ∗in Table 2. Brown et al.\n(2020) also reported substantial contamination rates for these two datasets (61% dirty for ReCoRD\nand 29% for PIQA), and also mark PIQA with an asterisk.\nAs this overlap analysis follows that performed in Brown et al. (2020), we reiterate the same caveats:\nthe conservative nature of our n-gram matching procedure likely introduces additional false positives;\nthere are no guarantees that the clean subset is drawn from the same distribution as the overall subset;\nand, accurately detecting test contamination is a relatively new research area without established best\npractices. Moreover, as our pretraining corpus is almost ﬁve times larger than that used for GPT-3\n(which was 500B tokens), it is possible that there are more false positives in detecting dirty data.\n26\nPublished as a conference paper at ICLR 2022\nDataset Metric Total\ncount\nTotal\nacc/F1/BLEU\nClean\ncount\nClean\nacc/F1/BLEU % clean\n% Diff\n(clean −\noverall)\nDROP F1 9,536 22.4 61 33.0 0.6 47.4\nSQuADv2 F1 11,873 41.3 106 38.7 0.9 -6.2\nANLI R1 acc 1,000 48.1 14 57.1 1.4 18.8\nANLI R2 acc 1,000 42.9 21 38.1 2.1 -11.2\nReCoRD acc 10,000 4.6 3,203 4.5 32.0 -2.7\nMultiRC acc 4,848 75.4 1,972 75.7 40.7 0.5\nPIQA acc 1,838 23.7 896 23.3 48.7 -1.7\nANLI R3 acc 1,200 44.2 718 45.3 59.8 2.5\nHellaSwag acc 10,042 28.5 6,578 28.7 65.5 0.7\nRTE acc 2,77 84.1 183 84.2 66.1 0.0\nWMT’14 En→Fr BLEU 3,003 31.3 2,243 31.5 74.7 0.9\nWMT’14 Fr→En BLEU 3,003 34.0 2,243 34.1 74.7 0.2\nBoolQ acc 3,270 76.5 2,515 76.3 76.9 -0.4\nTQA (tfds-dev) F1 11,313 62.2 8,731 62.0 77.2 -0.2\nARC Easy acc 2,365 79.5 1,888 79.0 79.8 -0.6\nARC Challenge acc 1,165 63.1 983 64.2 84.4 1.7\nOpenbookQA acc 500 74.6 425 74.8 85.0 0.3\nWMT’16 En→De BLEU 2,999 22.7 2,569 23.0 85.7 1.4\nWMT’16 De→En BLEU 2,999 38.6 2,569 38.7 85.7 0.2\nWMT’16 En→Ro BLEU 1,999 15.5 1,752 15.4 87.6 -0.7\nWMT’16 Ro→En BLEU 1,999 36.7 1,752 36.8 87.6 0.1\nCOPA acc 100 88.0 91 87.9 91.0 -0.1\nCB acc 56 41.1 53 41.5 94.6 1.1\nNQ F1 3,610 24.5 3,495 24.3 96.8 -0.5\nStoryCloze acc 1,871 92.1 1,864 92.1 99.6 0.0\nWinogrande acc 1,267 39.4 1,265 39.4 99.8 0.2\nTable 5: Overlap statistics for the subset of datasets that are also used in GPT-3, sorted from dirtiest\nto cleanest. An evaluation example was dirty if it had any n-gram collision with the pretraining\ncorpus. We computed these results for FLAN’s performance using only a single template for each\ndataset, so they differ slightly compared with the results for average performance over all templates.\n27\nPublished as a conference paper at ICLR 2022\nD E XTENDED RELATED WORK\nD.1 L ANGUAGE MODELS AND MULTI -TASK LEARNING\nOur work is broadly inspired by a long line of prior work on language models for NLP applications\n(Dai & Le, 2015; Peters et al., 2018; Howard & Ruder, 2018; Radford et al., 2018; 2019, inter\nalia). Instruction tuning can be seen as a formulation of multitask learning (MTL), which is an\nestablished area within deep learning (Collobert et al., 2011; Luong et al., 2016; Ruder, 2017; Velay\n& Daniel, 2018; Clark et al., 2019b; Liu et al., 2019b, inter alia)—see Worsham & Kalita (2020) for\na recent survey on MTL for NLP. Differing from prior MTL work which focuses on performance\nimprovements across training tasks (Raffel et al., 2020; Aghajanyan et al., 2021) or to new domains\n(Axelrod et al., 2011), our work is motivated by improving zero-shot generalization to tasks that were\nnot seen in training.\nD.2 Z ERO -SHOT LEARNING AND META-LEARNING\nOur work also falls in the well-established category of zero-shot learning, which has historically\nbeen used to refer to classifying instances among a set of unseen categories (Lampert et al., 2009;\nRomera-Paredes & Torr, 2015; Srivastava et al., 2018; Yin et al., 2019,inter alia). In NLP, zero-shot\nlearning work also includes translating between unseen language pairs (Johnson et al., 2017; Pham\net al., 2019), language modeling on unseen languages (Lauscher et al., 2020), as well as various NLP\napplications (Liu et al., 2019a; Corazza et al., 2020; Wang et al., 2021). Most recently, the emergent\nability of language models (Brown et al., 2020) has led to increased interest in how models generalize\nto unseen tasks, the deﬁnition of zero-shot learning used in our paper. In addition, meta-learning\n(Finn et al., 2017; Vanschoren, 2018, inter alia) also broadly tries to train models that adapt quickly\nto unseen tasks, typically based on a few examples.\nD.3 P ROMPTING\nInstruction tuning leverages the intuition that language models at scale contain substantial world\nknowledge and can perform a range of NLP tasks (Brown et al., 2020, see also Bommasani et al.\n(2021)). Another line of work that shares this goal prompts models with continuous inputs optimized\nvia backpropagation to substantially improve performance (Li & Liang, 2021; Lester et al., 2021;\nQin & Eisner, 2021), as well as work that prompts models to produce specialized outputs (Wei et al.,\n2022). Although the success of these approaches depends heavily on model scale (Lester et al., 2021),\nfor which large models can be costly to serve, the ability of a single large model to perform many\ntasks slightly eases this burden. As shown by our experiments in §4.5, prompt tuning is an orthogonal\nmethod for which instruction tuning can additionally improve performance. Reif et al. (2021) is\nsimilar to our work in that they also use related tasks to improve zero-shot learning, though they\ndiffer by only using related tasks in the context (and not ﬁnetuning), and focus on the application of\ntext style transfer.\nOur work shares similar motivations with prompting in that we use inference-time text interactions\nto prompt a single model, without creating separate checkpoints for each task. Whereas prompting\nwork such as GPT-3 uses prompt engineering to write prompts that intentionally mimic text that is\nlikely to be seen during pretraining (e.g., for MultiRC GPT-3 tries a prompt that mimics a test with\nan answer key), we hope that ﬁnetuning models to respond to natural language instructions instead of\ncompleting a sentence will make such large models more accessible to non-technical users.\nD.4 F INETUNING LARGE LANGUAGE MODELS\nFinetuning pretrained language models is a well-established method in NLP, with much of the work\nso far occurring on models in the range of 100M to 10B parameters (Dai & Le, 2015; Devlin et al.,\n2019; Raffel et al., 2020; Lewis et al., 2020, inter alia). For models of O(100B) parameters, recent\nwork has ﬁnetuned task-speciﬁc models for program synthesis (Austin et al., 2021; Chen et al.,\n2021), summarization (Wu et al., 2021), as well as improved bias and fairness behavior (Solaiman\n& Dennison, 2021). In addition to the traditional “dense” models, sparse mixture of experts (MoE)\nmodels of up to more than 1T parameters have been trained and ﬁnetuned (Lepikhin et al., 2020; Fedus\n28\nPublished as a conference paper at ICLR 2022\net al., 2021). Compared with this prior work that ﬁnetunes and evaluates on the same downstream\ntask, our setup studies the effect of instruction tuning on ability to perform unseen tasks.\nD.5 M ULTI -TASK QUESTION ANSWERING\nThe instructions we use for instruction tuning are similar to QA-based task formulation research,\nwhich aims to unify NLP tasks by casting them as question-answering over a context. For instance,\nMcCann et al. (2018) cast ten NLP tasks as QA and train a model on a collection of tasks formulated\nwith natural language prompts; they report transfer learning gains on ﬁnetuning tasks as well as\nzero-shot domain adaptation results on SNLI (Bowman et al., 2015) and Amazon/Yelp Reviews\n(Kotzias et al., 2015). While McCann et al. (2018) does not leverage unsupervised pre-training and\nonly reports zero-shot transfer to unseen domains, our work uses a pretrained LM and focuses on\nzero-shot performance on unseen task clusters. UniﬁedQA (Khashabi et al., 2020) shows similar\ntransfer learning gains as McCann et al. (2018) across 20 datasets and reports good generalization\nto unseen tasks across four types of QA. Focusing on binary text classiﬁcation, Zhong et al. (2021)\nﬁnetune T5-770M on 43 tasks phrased as yes/no questions and study the zero-shot performance on\nunseen tasks. In comparison, our paper is much larger in scope, empirically demonstrating the idea\non a wide range of tasks with a much larger model. Other work has used QA-based task formulation\nfor more-targeted applications including semantic role labeling (He et al., 2015), relation extraction\n(Levy et al., 2017), coreference resolution (Wu et al., 2020) and named entity recognition (Li et al.,\n2020) as question answering.\nD.6 I NSTRUCTIONS -BASED NLP\nRecent improvements in the capabilities of language models have led to increased interest in a nascent\narea of instructions-based NLP (Goldwasser & Roth, 2014, and see McCarthy (1960)). Schick &\nSchütze (2021) (also see Gao et al., 2021; Tam et al., 2021) use task descriptions in cloze-style\nphrases to help language models assign soft labels for few-shot and semi-supervised learning, though\nthis line of work ﬁnetunes new checkpoints for each downstream task. Efrat & Levy (2020) evaluated\nGPT-2 (Radford et al., 2019) on simple tasks ranging from retrieving thenth word of a sentence to\ngenerating examples for SQuAD, concluding that GPT-2 performs poorly across all tasks.\nIn terms of the setup of ﬁnetuning on a large number of tasks and evaluating on unseen tasks, two\nrecent papers are similar to ours. Mishra et al. (2021) ﬁnetune BART (Lewis et al., 2020) using\ninstructions and few-shot examples for tasks such as question answering, text classiﬁcation, and text\nmodiﬁcation, and ﬁnd that this few-shot ﬁnetuning with instructions improves performance on unseen\ntasks. Ye et al. (2021) introduce a setup for cross-task few-shot learning, ﬁnding that multi-task\nmeta-learning using MAML (Finn et al., 2017) improves the few-shot capabilities of BART on unseen\ndownstream tasks. Our work differs from these two papers in that we focus on zero-shot learning, for\nwhich we observe the crucial importance of model scale (FLAN is 1,000x larger than BART-base).\nPerhaps the papers most related to ours are the recent Sanh et al. (2021) and Min et al. (2021), which\nwere released after our initial preprint. Min et al. (2021) ﬁnetunes GPT-2 Large (770M parameters)\nto be a few-shot learner, which is the same approach as our experiment in Section 4.3. Similar\nto our conclusions, they also observe that including few-shot exemplars and instruction tuning are\ncomplementary ways to improve performance. Sanh et al. (2021) propose to ﬁnetune T5-11B to\nrespond to prompts, and they also report performance improvements on zero-shot learning. These\ntwo papers and our work all study ﬁnetuning with instructions, but, as noted by Min et al. (2021),\nit is hard to directly compare results, due to differing model sizes, model types (decoder-only vs\nencoder-decoder), pretraining data, task mixtures, and type of instructions (Sanh et al. (2021) say that\ntheir instructions are more diverse).\nFinally, OpenAI has a model called InstructGPT (Ouyang et al., 2022). InstructGPT uses human\nanntations to guide desired model behavior, both via ﬁnetuning and reinforcement learning, ﬁnding\nthat InstructGPT is preferred by human rathers compared with unmodiﬁed GPT-3.\n29\nPublished as a conference paper at ICLR 2022\nE F REQUENTLY ASKED QUESTIONS\nHow do the FLAN instructions differ from GPT-3 or T5 prompts?\nGPT-3 prompting is done in a way such that the prompt looks like data that the model has been\npretrained on, and the model ﬁnishes the continuation. T5 prompts are mostly just a tag for the\ndataset, which would not work in the zero-shot setting. In contrast, the prompts that we use for FLAN\nare similar to what would be used to ask a human to perform the task.\nFor instance, given an input for an NLI task, these would be the prompts.\nT5 prompt:\ncb hypothesis: At my age you will probably have learnt one lesson.\npremise: It’s not certain how many lessons you’ll learn by your\nthirties.\nGPT-3 prompt:\nAt my age you will probably have learnt one lesson.\nquestion: It’s not certain how many lessons you’ll learn by your\nthirties. true, false, or neither? answer:\nFLAN prompt:\nPremise: At my age you will probably have learnt one lesson.\nHypothesis: It’s not certain how many lessons you’ll learn by your\nthirties.\nDoes the premise entail the hypothesis?\nSo because FLAN prompts are formulated as responding to an instruction, they do not work well\nfor pretrained language models without ﬁnetuning. Performance was near zero for most generation\ntasks. For instance, given the input “‘The dog runs. ’ Translate this sentence to French. ”, LaMDA-PT\ncontinues with ”The dog runs after the cat” instead of actually translating the sentence. Hence, we\nused the established GPT-3 prompts for our LaMDA-PT baselines.\nWhat are some limitations/failure cases of FLAN?\nWhile we qualitatively ﬁnd that FLAN responds well to most tasks, it does fail on some simple tasks.\nFor instance, as shown in Figure 22, FLAN fails at the very simple task of returning the second word\nin a sentence, and also incorrectly translates a question to Danish when asked to answer the question\nin Danish. Additional limitations include a context length of only 1024 tokens (which is not enough\nfor most summarization tasks), and that the model was mostly trained on English data.\nCan FLAN be used when large amounts of training data are available?\nIn this work, we focus on cross-task generalization to zero-shot tasks, but we also believe that\ninstruction tuning could result in positive task transfer among seen tasks, depending on the mixture\nof tasks (though we leave this for future work). In §4.5, where we apply prompt tuning to the FLAN\ncheckpoint, we see promising results that indicate positive task transfer in a supervised setting.\nAre the ten unique templates per dataset or per task cluster?\nThe ten unique templates are for each dataset and not for a task cluster. This is because datasets in the\nsame task cluster often differed slightly (e.g., “is this movie review positive” vs “is this yelp review\npositive”).\nIn Figure 7A, why does the untuned LaMDA-PT model see worse performance with more\nparameters for reading comprehension and sentiment analysis?\nFor context, Figure 7A is a check of correctness for Figure 7B. Figure 7A conﬁrms that scale improves\nperformance for tasks that were seen during instruction tuning, as expected. The untuned LaMDA-PT\nmodel performance in Figure 7A is shown just for completeness.\n30\nPublished as a conference paper at ICLR 2022\nNonetheless, the fact that scale does not always improve zero-shot performance of untuned LaMDA-\nPT is an interesting artifact. Initially, we were surprised, because Brown et al. (2020) shows that scale\nimproves performance across a large number of tasks in aggregate.\nIt turns out that scale does not improve performance for certain tasks. This is especially true for\nzero-shot learning, and we think that this happens to be the case for the reading comprehension\nand sentiment analysis tasks we evaluate. The GPT-3 paper itself similarly reports that zero-shot\nperformance on BoolQ and DROP decreases from 13B to 175B parameters. The GPT-3 paper does\nnot show results on sentiment analysis, but Holtzman et al. (2021) ﬁnd that zero-shot performance on\nSST-2 also gets worse from 13B to 175B parameters. Hence, this artifact is consistent across both\nGPT-3 and the models we use.\nThis artifact is certainly worth further study, but is outside the scope of instruction tuning. Ideally,\nwe would have performed the Figure 7 ablation with cross-validation instead of a single split, which\nlikely would have smoothed out that artifact.\nF Q UALITATIVE EXAMPLES\nThis section shows qualitative examples of FLAN responding to various prompts.\nMovie review: This movie is the \nbest RomCom since Pretty Woman.\nDid this critic dislike the movie?\nOPTIONS\n-yes\n-no\nno\nInput:\nFLAN output:\nMovie review: This movie is the \nbest RomCom since Pretty Woman.\nDid this critic like the movie?\nOPTIONS\n-yes\n-no\nyes\nInput:\nFLAN output:\nFigure 13: For sentiment analysis, FLAN changes the answer appropriately when the question is\nﬂipped.\nWhat is Barack Obama’s \nprofession?\nAnswer in Chinese\n● 总统\n● 美国总统\nInput:\nFLAN output:\nWhat is Barack Obama’s \nprofession?\nInput:\n● politician\n● lawyer\n● senator\n● us president\nFLAN output:\nFigure 14: For question answering, FLAN can answer a question in another language when instructed\nto do so. Multiple FLAN outputs are generated via random sampling with a temperature of 0.9 and\ntop k of 40.\n31\nPublished as a conference paper at ICLR 2022\nJohn is reading the paper \n“Finetuned Language Models are \nZero-Shot Learners.”\nChange to passive voice.\nThe paper “Finetuned Language \nModels are Zero-Shot Learners” is \nbeing read by John.\nInput:\nFLAN output:\nJohn is reading the paper \n“Finetuned Language Models are \nZero-Shot Learners.”\nChange to past tense.\nInput:\nJohn was reading the paper \n“Finetuned Language Models are \nZero-Shot Learners.”\nFLAN output:\nJohn is reading the paper \n“Finetuned Language Models are \nZero-Shot Learners.”\nChange the verb to eat and the \nsubject to Noam Chomsky.\nNoam Chomsky is eating the paper \n“Finetuned Language Models are \nZero-Shot Learners.”\nInput:\nFLAN output:\nJohn is reading the paper \n“Finetuned Language Models are \nZero-Shot Learners.”\nChange to the verb to eat.\nInput:\nJohn is eating the paper \n“Finetuned Language Models are \nZero-Shot Learners.”\nFLAN output:\nFigure 15: FLAN can perform zero-shot rule-based manipulations.\nRecommend activities to do on a \nsunny weekend in Copenhagen.\n● Visit Christiania\n● Copenhagen Zoo\n● Visit the Tivoli Gardens\n● Take a canal tour\n● Museums\n● Visit The Little Mermaid Statue\n● Visit Copenhagen Zoo\n● Visit the Danish Maritime Museum\nInput:\nFLAN output:\nRecommend fun activities to do on \na rainy night in Seattle.\nInput:\n● Go to the paciﬁc science center\n● Go out to eat\n● Go to a comedy club\n● Build a blanket fort\n● Go to the Space Needle\n● Go to underground tours\n● Find a karaoke bar\nFLAN output:\nFigure 16: FLAN can make zero-shot recommendations. Multiple FLAN outputs are generated via\nrandom sampling with a temperature of 0.9 and top k of 40.\n32\nPublished as a conference paper at ICLR 2022\nGenerate utterances with the intent \n“where to vote in the state Senate \nelection\"\n● Tell me where to vote in the state \nsenate election\n● Find out where to vote in the state \nsenate election\n● Where am i supposed to vote in \nthe state senate election\n● Where can i get information on \nwhere to vote in the state Senate \nelection?\nInput:\nFLAN output:\nGenerate utterances with the intent \n“get COVID vaccine\"\nInput:\n● I would like to get the covid \nvaccine.\n● I would like a COVID vaccine as \nsoon as possible\n● Can I get a COVID vaccine as a \npreventative measure?\n● Get COVID vaccine for a ﬁve year \nold\n● Where is a center for COVID \nvaccine testing?\nFLAN output:\nFigure 17: FLAN can be used in a zero-shot fashion to generate data, such as utterances that are\nconsistent with a given intent. Multiple FLAN outputs are generated via random sampling with a\ntemperature of 0.9 and top k of 40.\nSuggest a related search term to \n“Novak Djokovic\"\n● novak djokovic girlfriend\n● serena williams\n● rafael nadal\n● novak djokovic wife\n● roger federer\n● andy murray\n● novak djokovic news\n● novak djokovic serbia\n● novak djokovic wimbledon\nInput:\nFLAN output:\nSuggest a related search term to \n“watermelon”\nInput:\n● Cantaloupe\n● Cucumber\n● Melon\n● Fruit\n● Watermelon juice\n● Watermelons\n● Lemon\n● Apple\nFLAN output:\nFigure 18: FLAN can be used for zero-shot query expansion. Multiple FLAN outputs are generated\nvia random sampling with a temperature of 0.9 and top k of 40.\n\"I guess I need to do laundry.”\nRewrite the above sentence in a\nShakespearean style.\n● To my mind, verily, I suppose 'tis \ntime, I wash my clothes.\n● Mine eyes and ears and tongue \ndeclare me idle; and yet it is true I \nguess, By my garments, that I need \nto do laundry\n● Lo my duty now calls To thee, to \nwash this body's clothes.\nInput:\nFLAN output:\nOn the way to Yosemite, our front \ntire blew.\nFinish writing the story, and include \na happy ending.\nInput:\n● We got our tire replaced. The \nmechanic was honest, kind and \nhelpful. Then we had a smooth ride \nfor the rest of the trip. Our trip was \nﬁlled with fun and excitement.\n● We waited three hours for the \ntow truck, but had a great picnic. \nThe tow truck driver helped us put \nthe spare on, and we arrived\nin Yosemite for our reservations \ndinner, with time to spare.\nFLAN output (cherrypicked):\nFigure 19: FLAN can perform zero-shot tasks relevant to assisted-writing applications. Multiple\nFLAN outputs are generated via random sampling with a temperature of 0.9 and top k of 40.\n33\nPublished as a conference paper at ICLR 2022\nMake up a word that means \"like a\npipedream\"\n● Pipedreamic\n● Pipedreamish\n● Pipedreamlike\n● Phantasmagoric\nInput:\nFLAN output (cherrypicked):\nMake up a new word that means \n“when bananas shower from the \nsky\"\nInput:\n● Bananation\n● Banana shower\n● Banania\n● Bananafall\n● Bananada\nFLAN output:\nFigure 20: FLAN can be used for zero-shot word formation. Multiple FLAN outputs are generated\nvia random sampling with a temperature of 0.9 and top k of 40.\nWrite a sad story about carrots.\nThe sun had barely risen when the carrots were taken \nfrom their beds and brought to market. They'd had no \ntime to gather their strength. They smelled of earth, \ntheir greens still wet from the dew. At ﬁrst no one was \ninterested in buying them. But one man ﬁnally reached \nout and picked up one of the carrots. \"I'll take this \none!” he cried out. \"It's for my mother! I'll pay you well \nfor it.” The man took the carrot to the cashier, eager to \nget it home to his mother. He was careful to wrap it \ncarefully so that it wouldn't get bruised or dirty. He \nhurried home, and when he saw his mother, he gave \nher the carrot. “Is this one of the ones I grew?\" she \nasked, her voice wavering with emotion. \"Yes, mother,\" \nhe replied. \"It is, dear.\" The man left the carrot with his \nmother and hurried back to market. His mother was \nvery sick, and he was afraid she wouldn't last much \nlonger. He needed to buy all the carrots in market so \nthat none would be left when she was gone.\nInput:\nFLAN output:\nJohn moved to mountain view. He \nspent friday night in San Francisco. \nHe got a COVID vaccine. He voted \nin the state senate election.\nWrite a story that follows the above \nplot.\nInput:\nJohn arrived at the city with his vaccine, \nfeeling prepared enough for the state \nsenate election of San Francisco. He had \nvoted for the ﬁrst time last night and was \nfeeling good about it. He felt more \nprepared than ever to win, and win big, \nbecause he was getting a COVID vaccine. \nHe knew that his opponent had not \ngotten a COVID vaccine, and he knew he \nhad already defeated him before the \nelection began.\nFLAN output (cherrypicked):\nFigure 21: Open-ended generation tasks by FLAN. The carrot story was from sampling sixteen\noutputs with a minimum length of 150 and choosing the highest probability output.\nWhat is Barack Obama’s \nprofession?\nAnswer in Danish\n● Hvad er Barack Obamas \nbeskæftigelse?\n● Hvad er Barack Obamas \nerhverv?\nInput:\nFLAN output:\nConan went to the store to buy \nsome pencils.\nWhat is the second word in this \nsentence?\nInput:\n● pencils\n● Conan\n● store\n● buy\nFLAN output:\nFigure 22: Example failure cases for FLAN. Left: FLAN fails to perform a simple task of returning\nthe nth word. Right: FLAN translates a question instead of answering it. Multiple FLAN outputs are\ngenerated via random sampling with a temperature of 0.9 and top k of 40.\n34\nPublished as a conference paper at ICLR 2022\nCHANGES FROM V4 TO V5\n• Replaced the tables in the main ﬁgure with a ﬁgure, which takes up less space and focuses\non zero-shot performance.\n• Added GLaM 64B/64E as a baseline.\n• Moved the ablation about the role of instructions, as well as prompt tuning, into the main\npaper (and condensed the ﬁgures).\nCHANGES TO V4 FROM V3\n• We added a Frequently Asked Questions section (Appendix E).\n• We added a section with qualitative examples (Appendix F).\n• We added an additional ablation study on the role of instructions during ﬁnetuning (Ap-\npendix B.2).\n• We updated the related work (Appendix D) with manuscripts posted on arxiv since our\ninitial preprint.\nCHANGES TO V3 FROM V2\n• The number of tokens used in pretraining was corrected from 2.81T to 2.49T tokens.\nCHANGES TO V2 FROM V1\n• We updated the terminology to “datasets” and “task clusters.”\n• We renamed the previous “open-domain QA” task cluster to “closed-book QA.”\n• We extended the related work section and moved it to the Appendix D, using a shorter\nversion in the main body.\n• We added FLAN and LaMDA-PT results for additional datasets for which GPT-3 results\nwere not reported.\n• For TriviaQA, v1 reported results on the tfds dev set of 11,313 examples. GPT-3 actually\nevaluates on the wikipedia dev set of 7,993 examples, so we ran an additional evaluation\non that dev set in order to compare with GPT-3’s performance. Zero-shot FLAN now beats\nzero-shot GPT-3 on that task (and therefore on 20 of 25 tasks). We still show the original\nresult in Table 2, though there is no GPT-3 result to compare with.\n• We moved commonsense reasoning and coreference resolution from the main body to the\nAppendix.\n• We moved prompt tuning from the main body to §4.5.\n• We added data contamination analysis (Appendix C).\n• We added few-shot instruction tuning (§4.4).\n• We cited additional datasets in Appendix G.\n• The number of tokens used in pretraining was corrected from 2.81T to 2.49T tokens.\n35\nPublished as a conference paper at ICLR 2022\nG T ASKS AND DATASETS\nThis appendix further details the datasets that we use in this paper. We group datasets into one of the\nfollowing task clusters:\n• Natural language inferenceconcerns how two sentences relate, typically asking, given a ﬁrst\nsentence, whether a second sentence is true, false, or possibly true. We use the following datasets:\n1. ANLI (Nie et al., 2020)\n2. CB (De Marneffe et al., 2019)\n3. MNLI (Williams et al., 2018)\n4. QNLI (Rajpurkar et al., 2018)\n5. SNLI (Bowman et al., 2015)\n6. WNLI (Levesque et al., 2012)\n7. RTE (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009)\n• Reading comprehensiontests the ability to answer a question when given a passage that contains\nthe answer. We use the following datasets:\n1. BoolQ Clark et al. (2019a)\n2. DROP (Dua et al., 2019)\n3. MultiRC (Khashabi et al., 2018)\n4. OBQA (Mihaylov et al., 2018)\n5. SQuADv1 (Rajpurkar et al., 2016)\n6. SQuADv2 (Rajpurkar et al., 2018)\n• Commonsense reasoningevaluates the ability to perform physical or scientiﬁc reasoning with an\nelement of common sense. We use the following datasets:\n1. COPA (Roemmele et al., 2011)\n2. HellaSwag (Zellers et al., 2019)\n3. PiQA (Bisk et al., 2020)\n4. StoryCloze (Mostafazadeh et al., 2016)\n• Sentiment analysisis a classic NLP task aims to understand whether a piece of text is positive or\nnegative. We use the following datasets:\n1. IMDB (Maas et al., 2011)\n2. Sentiment140 (Go et al., 2009)\n3. SST-2 (Socher et al., 2013)\n4. Yelp (Fast.AI)\n• Closed-book QAasks models to answer questions about the world without speciﬁc access to\ninformation that contains the answer. We use the following datasets:\n1. ARC (Clark et al., 2018)\n2. NQ (Lee et al., 2019; Kwiatkowski et al., 2019)\n3. TriviaQA Joshi et al. (2017)\n• Paraphrase detectionasks a model to determine whether two sentences are semantically equiva-\nlent.4 We use the following datasets:\n1. MRPC (Dolan & Brockett, 2005)\n2. QQP (Wang et al., 2018, see)\n3. Paws Wiki (Zhang et al., 2019)\n• Coreference resolutiontests the ability to identify expressions of the same entity in some given\ntext. We use the following datasets:\n1. DPR (Rahman & Ng, 2012)\n2. Winogrande (Sakaguchi et al., 2020)\n4Although paraphrasing can be seen as positive entailment in both directions, it has been distinct from NLI\nin the academic literature.\n36\nPublished as a conference paper at ICLR 2022\n3. WSC273 (Levesque et al., 2012)\n• Reading comprehension with commonsensecombines elements of both reading comprehension\nwith commonsense. We use the following datasets:\n1. CosmosQA (Huang et al., 2019)\n2. ReCoRD (Zhang et al., 2018)\n• Struct to texttests the ability to describe some structured data using natural language. We use the\nfollowing datasets:\n1. CommonGen (Lin et al., 2020)\n2. DART (Nan et al., 2021)\n3. E2ENLG (Dušek et al., 2019)\n4. WebNLG (Gardent et al., 2017)\n• Translation is the task of translating text from one language into a different language. We use the\nfollowing datasets:\n1. En–Fr from WMT’14 (Bojar et al., 2014)\n2. En–De, En–Tr, En–Cs, En–Fi, En–Ro, and En–Ru from WMT’16 (Bojar et al., 2016)\n3. En–Es from Paracrawl (Bañón et al., 2020)\n• Summarization asks models to read a piece of text and generate an abbreviated summary of it.\nWe use the following datasets:\n1. AESLC (Zhang & Tetreault, 2019)\n2. CNN-DM (See et al., 2017)\n3. Gigaword (Napoles et al., 2012)\n4. MultiNews (Fabbri et al., 2019)\n5. Newsroom (Grusky et al., 2018)\n6. Samsum (Gliwa et al., 2019)\n7. XSum (Narayan et al., 2018)\n8. AG News (Zhang et al., 2015)\n9. Opinion Abstracts - Rotten Tomatoes (Wang & Ling, 2016)\n10. Opinion Abstracts - iDebate (Wang & Ling, 2016)\n11. Wiki Lingua English (Ladhak et al., 2020)\n• Additional datasets that we assign to a miscellaneous task cluster include:\n1. Conversational question-answering: QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2019)\n2. Evaluating context-sentence word meanings: WiC (Pilehvar & Camacho-Collados, 2019)\n3. Question classiﬁcation: TREC (Li & Roth, 2002; Hovy et al., 2001)\n4. Linguistic acceptability: CoLA (Warstadt et al., 2019)\n5. Math questions (Saxton et al., 2019)\nFor all tasks, our ﬁnetuning and evaluation code uses tensorﬂow datasets (TFDS) to load and process\ndatasets. Regarding the number of training examples per dataset, we limited the training set size\nper dataset to 30,000 so that no dataset dominated the ﬁnetuning distribution. When a test set with\nlabels was available in TFDS, we used it; otherwise, we used the TFDS validation set as our test set,\nsplitting the training set into a train and dev set.\nOn the following pages, we show inputs and outputs for evaluation tasks where we compared with\nGPT-3. See the attached supplementary material for the templates for all other datasets.\n37\nPublished as a conference paper at ICLR 2022\nG.1 N ATURAL LANGUAGE INFERENCE\nINPUT\nJoey Heindle (born 14 May 1993 in Munich) is a German singer. He is best known for winning\nthe seventh season of the game show Ich bin ein Star – Holt mich hier raus! and ﬁnishing in 5th\nplace in season 9 of Deutschland sucht den Superstar, despite universally negative reviews from\nthe jury each week.\nBased on the paragraph above can we conclude that \"Joey Heindle was highly disliked by people\non television.\"?\nOPTIONS:\n- Yes\n- It’s impossible to say\n- No\nTARGET\nYes\nTable 6: Example input and target for Adversarial NLI (ANLI). ANLI (Nie et al., 2020) is a\nlarge-scale NLI benchmark with adversarial examples collected iteratively with a human and model\nin the loop. The task is to determine whether a hypothesis is entailed by a premise (entailment, not\nentailment, or impossible to say). There are three rounds, R1–R3. Of the three training sets with\n16,946, 45,460, and 100,459 examples, we use 16,946, 30,000, and 30,000 for train and 200 from\neach of the three TFDS validation sets for dev. We use the TFDS “test” sets of 1,000, 1,000, and\n1,200 examples as our test set for reporting numbers.\nINPUT\nA: so I watch the ﬁsh, you know. Whatever I can do to keep myself occupied. I like to have the\nTV on, because that usually keeps me, um, more occupied. It kind of takes the time away and\nI don’t realize, that’s really the only time I ever watch TV , is when I’m on the bike. and then\nusually after I’m done riding the bike, just to cool myself down, I usually take a walk, you know,\nand that just kind of uh, gets me, you know, to where I’m not quite as tired I guess. But it’s\ndeﬁnitely a task. B: You think so? A: I can’t say that I really enjoy it.\nBased on the paragraph above can we conclude that \"she really enjoys it\"?\nOPTIONS:\n- Yes\n- No\n- It’s impossible to say\nTARGET\nNo\nTable 7: Example input and target for Commitment Bank (CB). CB (De Marneffe et al., 2019) is a\ncorpus of texts in which a hypothesis is extracted from a premise, and the task is to determine whether\nthe hypothesis is entailed by the premise (entailment, not entailment, or impossible to say). Of the\ntraining set with 250 examples, we use 200 for train and 50 for dev. We use the TFDS validation set\nof 56 examples as our test set for reporting numbers.\n38\nPublished as a conference paper at ICLR 2022\nINPUT\nAfter years of study, the Vatican’s doctrinal congregation has sent church leaders a conﬁdential\ndocument concluding that \"sex-change\" procedures do not change a person’s gender in the eyes\nof the church.\nBased on the paragraph above can we conclude that \"Sex-change operations become more\ncommon.\"?\nOPTIONS:\n- yes\n- no\nTARGET\nno\nTable 8: Example input and target for Recognizing Textual Entailment (RTE). RTE (Dagan et al.,\n2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) asks whether a second\nsentence is entailed by a ﬁrst (binary, either entailed or not entailed). Of the training set with 2490\nexamples, we use 2,290 for train and 200 for dev. We use the TFDS validation set of 277 examples as\nour test set for reporting numbers.\n39\nPublished as a conference paper at ICLR 2022\nG.2 R EADING COMPREHENSION\nINPUT\nThere are four ways an individual can acquire Canadian citizenship: by birth on Canadian\nsoil; by descent (being born to a Canadian parent); by grant (naturalization); and by adoption.\nAmong them, only citizenship by birth is granted automatically with limited exceptions, while\ncitizenship by descent or adoption is acquired automatically if the speciﬁed conditions have been\nmet. Citizenship by grant, on the other hand, must be approved by the Minister of Immigration,\nRefugees and Citizenship.\nCan we conclude that can i get canadian citizenship if my grandfather was canadian?\nOPTIONS:\n- no\n- yes\nTARGET\nno\nTable 9: Example input and target for Boolean Questions (BoolQ). BoolQ Clark et al. (2019a) asks a\nyes/no question based on a passage and a question. Of the training set with 9,427 examples, we use\n9,227 for train and 200 for dev. We use the TFDS validation set of 3,270 examples as our test set for\nreporting numbers.\nINPUT\nImagine you are standing in a farm ﬁeld in central Illinois. The land is so ﬂat you can see for\nmiles and miles. On a clear day, you might see a grain silo 20 miles away. You might think to\nyourself, it sure is ﬂat around here. If you drive one hundred miles to the south, the landscape\nchanges. In southern Illinois, there are rolling hills. Why do you think this is? What could have\ncaused these features? There are no big rivers that may have eroded and deposited this material.\nThe ground is capable of supporting grass and trees, so wind erosion would not explain it. To\nanswer the question, you need to go back 12,000 years. Around 12,000 years ago, a giant ice\nsheet covered much of the Midwest United States. Springﬁeld, Illinois, was covered by over a\nmile of ice. Its hard to imagine a mile thick sheet of ice. The massive ice sheet, called a glacier,\ncaused the features on the land you see today. Where did glaciers go? Where can you see them\ntoday? Glaciers are masses of ﬂowing ice.\nQuestion: \"How big were the glaciers?\"\nResponse: \"One mile\"\nDoes the response correctly answer the question?\nOPTIONS:\n- no\n- yes\nTARGET\nyes\nTable 10: Example input and target for Multi-Sentence Reading Comprehension (MultiRC). MultiRC\nKhashabi et al. (2018) asks an open-ended question given a paragraph that contains the answer. Of\nthe training set with 27,243 examples, we use 27,043 for train and 200 for dev. We use the TFDS\nvalidation set of 4,848 examples as our test set for reporting numbers.\n40\nPublished as a conference paper at ICLR 2022\nINPUT\nsoil is a renewable resource for growing plants\nA plant that needs to expand will be able to have an endless resource in\nOPTIONS:\n- dirt\n- pesticides\n- pay\n- beans\nTARGET\ndirt\nTable 11: Example input and target for Openbook Question Answering (OBQA). OBQA (Mihaylov\net al., 2018) asks 4-way multiple choice questions based facts. Of the training set with 4,957 examples,\nwe use all for train and 200 in the TFDS validation set of 500 examples for dev. We use the TFDS\ntest set of 500 examples as our test set for reporting numbers.\n41\nPublished as a conference paper at ICLR 2022\nG.3 C OMMONSENSE REASONING\nINPUT\nI packed up my belongings. What is the cause?\nOPTIONS:\n- I was hunting for a new apartment.\n- I was moving out of my apartment.\nTARGET\nI was moving out of my apartment.\nTable 12: Example input and target for Choice of Plausible Alternatives (COPA). COPA (Roemmele\net al., 2011) is a causal reasoning task that asks to infer either a cause of effect of a premise from\ntwo choices. Of the training set with 400 examples, we use 350 for train and 50 for dev. We use the\nTFDS validation set of 100 examples as our test set for reporting numbers.\nINPUT\nWhat happens next in this paragraph?\nOnce the rope is inside the hook, he begins moving up the wall but shortly after he stops and\nbegins talking. The male then begins talking about the clip again and goes back up the wall. as\nhe\nOPTIONS:\n- progresses, there are hooks everywhere on the wall and when he gets near them, he puts his\nrope inside of it for support and safety.\n- changes time, an instant replay of his initial move is shown a second time.\n- continues to talk, another male speaks about the move and shows another closeup of the plex\nby the male.\n- continues, other people start to arrive and begin to hang out with him as he makes a few parts\nof the rope.\nTARGET\nprogresses, there are hooks everywhere on the wall and when he gets near them, he puts his rope\ninside of it for support and safety.\nTable 13: Example input and target for Commonsense Sentence Completion (HellaSwag). HellaSwag\n(Zellers et al., 2019) tests for sentence completion that requires common sense, asking for the most\nprobable ending given four contexts. Of the training set with 39,905 examples, we use 30,000 for\ntrain and 200 for dev. We use the TFDS validation set of 10,042 examples as our test set for reporting\nnumbers.\n42\nPublished as a conference paper at ICLR 2022\nINPUT\nHere is a goal: Remove smell from garbage disposal.\nHow would you accomplish this goal?\nOPTIONS:\n- Create soda ice cubes and grind through disposal.\n- Create vinegar ice cubes and grind through disposal.\nTARGET\nCreate vinegar ice cubes and grind through disposal.\nTable 14: Example input and target for Physical Question Answering (PiQA). PiQA (Bisk et al.,\n2020) is a commonsense QA benchmark for naive physics reasoning, where a solution to a goal must\nbe selected from two choices. Of the training set with 16,113 examples, we use 16,013 for train and\n100 for dev. We use the TFDS validation set of 1,838 examples as our test set for reporting numbers.\nINPUT\nCaroline never drinks carbonated beverages. Her friends pick on her because of it. One day they\nchallenged her to drink a soda. Caroline wanted to win the challenge.\nPredict the next sentence.\nOPTIONS:\n- Caroline refused to open the soda.\n- Caroline opened the soda and drank it all in one gulp!\nTARGET\nCaroline opened the soda and drank it all in one gulp!\nTable 15: Example input and target for The Story Cloze Test (StoryCloze). StoryCloze (Mostafazadeh\net al., 2016) is a commonsense reasoning framework for story generation, where a system chooses\nthe correct ending to a four-sentence story. We use the 2016 version on TFDS. Of the validation set\nwith 1,871 examples (no training set is available), we use 1,671 for train and 200 for dev. We use the\nTFDS test set of 1,871 examples as our test set for reporting numbers.\n43\nPublished as a conference paper at ICLR 2022\nG.4 C LOSED -BOOK QA\nINPUT\nWhat season is the Northern Hemisphere experiencing when it is tilted directly toward the Sun?\nOPTIONS:\n- fall\n- winter\n- spring\n- summer\nTARGET\nsummer\nTable 16: Example input and target for The AI2 Reasoning Challenge (ARC). ARC (Clark et al.,\n2018) asks grade-school level 4-way multiple choice science questions. There is a challenge set and\nan easy set, where the challenge set questions were answered incorrectly by both a retrieval-based\nalgorithm and a co-occurrence algorithm. Of the training sets with 1,119 examples (challenge) and\n2,251 (easy), we use we use 919 and 2,051 respectively for train and 200 each for dev. We use the\nTFDS test sets of 1,172 and 2,376 examples respectively as our test set for reporting numbers.\nINPUT\nQuestion: who is the girl in more than you know??\nAnswer:\nTARGET\nRomi Van Renterghem.\nTable 17: Example input and target for Natural Questions (Open) (NQ). NQ (Lee et al., 2019;\nKwiatkowski et al., 2019) asks for an open-ended answer given a question, where all questions can be\nanswered using the contents of Wikipedia. Of the training set of 87,925 examples, we use 30,000 for\ntrain and 200 for dev. We use the TFDS validation set of 3,610 examples as our test set for reporting\nnumbers.\nINPUT\nPlease answer this question: Henry Croft, an orphan street sweeper who collected money for\ncharity, is associated with what organised charitable tradition of working class culture in London,\nEngland?\nTARGET\npearly kings and queens\nTable 18: Example input and target for Trivia Question Answering (TriviaQA). TriviaQA Joshi et al.\n(2017) includes question-answer pairs authored by trivia enthusiasts. Of the training set of 87,622\nexamples, we use 30,000 for train and 200 for dev. We use 7,993 examples from Wikipedia of the\n11,313 examples in the TFDS validation set, which is the same validation set used in (Brown et al.,\n2020). as our test set for reporting numbers.\n44\nPublished as a conference paper at ICLR 2022\nG.5 C OREFERENCE RESOLUTION\nINPUT\nHow does the sentence end?\nElena wanted to move out of her parents fast but Victoria wanted to stay for a while,\nOPTIONS:\n- Elena went to school.\n- Victoria went to school.\nTARGET\nVictoria went to school.\nTable 19: Example input and target for Adversarial Winograd Schema Challenge (Winogrande).\nWinogrande (Sakaguchi et al., 2020) tests for coreference resolution by asking a model to ﬁll in a\nmasked token in a sentence by choosing an entity from two options. Of the 40.4k examples in the XL\ntraining set, we use 30,000 for train and 200 for dev. We use the TFDS validation set of 1,267 as our\ntest set for reporting numbers.\nINPUT\nJane knocked on Susan’s door, but there was no answer.\nOPTIONS:\n- Jane was out.\n- Susan was out.\nTARGET\nSusan was out.\nTable 20: Example input and target for Winograd Schema Challenge (WSC273). WSC273 (Levesque\net al., 2012) tests for coreference resolution by asking a model to complete the sentence in a fashion\nthat requires understanding the entities in the sentence. Of the 0 examples in the training set (WSC273\nis test-set only), we use none for train and none for dev. We use the TFDS test set as our test set for\nreporting numbers.\n45\nPublished as a conference paper at ICLR 2022\nG.6 R EADING COMPREHENSION WITH COMMONSENSE\nINPUT\nComplete the passage.\n(CNN) – At ﬁrst glance, \"The Flat\" might seem like an episode of \"Hoarders,\" Israeli-style. The\ndocumentary ﬁlm opens after an elderly woman dies in Tel Aviv. Her grandchildren assemble to\nclean out her apartment, packed with dusty books, vintage clothing (dozens of pairs of fancy\ngloves, for instance), enough purses to stock a department store, jewelry, mementoes and closets\nfull of knickknacks. But buried among the detritus they chance upon something remarkable –\nmysterious papers linking the grandparents to an important Nazi ﬁgure. How could such ardent\nZionists, who left their native Germany in the early 1930s, have been involved with an SS ofﬁcial\nlike Leopold von Mildenstein?\nWhat I found out was this journey, the Nazi (\nOPTIONS:\n- Arnon Goldﬁnger) and his wife were accompanied by my grandparents,\" Goldﬁnger told CNN.\n- CNN) and his wife were accompanied by my grandparents,\" Goldﬁnger told CNN.\n- Germany) and his wife were accompanied by my grandparents,\" Goldﬁnger told CNN.\n- Israeli) and his wife were accompanied by my grandparents,\" Goldﬁnger told CNN.\n- Leopold von Mildenstein) and his wife were accompanied by my grandparents,\" Goldﬁnger\ntold CNN.\n- Nazi) and his wife were accompanied by my grandparents,\" Goldﬁnger told CNN.\n- SS) and his wife were accompanied by my grandparents,\" Goldﬁnger told CNN.\n- Tel Aviv) and his wife were accompanied by my grandparents,\" Goldﬁnger told CNN.\n- The Flat) and his wife were accompanied by my grandparents,\" Goldﬁnger told CNN.\n- Zionists) and his wife were accompanied by my grandparents,\" Goldﬁnger told CNN.\nTARGET\nLeopold von Mildenstein) and his wife were accompanied by my grandparents,\" Goldﬁnger told\nCNN.\nTable 21: Example input and target for Reading Comprehension with Commonsense Reasoning\n(ReCoRD). ReCoRD (Zhang et al., 2018) asks for the answer to a cloze-style question where an\nentity is masked out. Of the the training set of 100,730 examples, we use 30,000 for train and 200 for\ndev. We use the TFDS validation set of 10,000 examples as our test set for reporting numbers.\nG.7 T RANSLATION (7 LANGUAGES )\nINPUT\nHere the largest town of the district is located: Nordenham , lying opposite to Bremerhaven at\nthe Weser mouth.\nTranslate to German\nTARGET\nAn der B 211 beﬁndet sich in Loyermoor der so genannte “Geest-Abbruch”, der eine Höhendif-\nferenz von gut 30 Meter überbrückt.\nTable 22: Example input and output for translation. This example is from WMT’16 English–German;\nall languages use the same translation templates.\n46",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8774510622024536
    },
    {
      "name": "Task (project management)",
      "score": 0.7282065153121948
    },
    {
      "name": "Shot (pellet)",
      "score": 0.6093952059745789
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.5800075531005859
    },
    {
      "name": "Key (lock)",
      "score": 0.5410926938056946
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.528717577457428
    },
    {
      "name": "Template",
      "score": 0.4813750088214874
    },
    {
      "name": "Language model",
      "score": 0.47852954268455505
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.46430638432502747
    },
    {
      "name": "Natural language",
      "score": 0.4332785904407501
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4330458343029022
    },
    {
      "name": "Natural language processing",
      "score": 0.37398433685302734
    },
    {
      "name": "Programming language",
      "score": 0.24852651357650757
    },
    {
      "name": "Machine learning",
      "score": 0.15975716710090637
    },
    {
      "name": "Linguistics",
      "score": 0.06310418248176575
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": []
}