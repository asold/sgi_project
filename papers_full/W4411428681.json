{
  "title": "Generalized biological foundation model with unified nucleic acid and protein language",
  "url": "https://openalex.org/W4411428681",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A1982899729",
      "name": "Yong He",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2010770749",
      "name": "Pan Fang",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A3194159946",
      "name": "Yongtao Shan",
      "affiliations": [
        "Institute of Infection and Immunity"
      ]
    },
    {
      "id": "https://openalex.org/A3189241738",
      "name": "Yuanfei Pan",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2117545886",
      "name": "Yanhong Wei",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2100750335",
      "name": "Yi-Chang Chen",
      "affiliations": [
        "Zhejiang University",
        "Chinese Academy of Medical Sciences & Peking Union Medical College"
      ]
    },
    {
      "id": "https://openalex.org/A2125251258",
      "name": "Yihao Chen",
      "affiliations": [
        "Chinese Academy of Medical Sciences & Peking Union Medical College",
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2024876568",
      "name": "Yi Liu",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2636662135",
      "name": "Zhenyu Zeng",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2104082825",
      "name": "Zhan Zhou",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A1977056848",
      "name": "Feng Zhu",
      "affiliations": [
        "Second Affiliated Hospital of Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2169214606",
      "name": "Edward C. Holmes",
      "affiliations": [
        "University of Sydney",
        "Taronga Conservation Society Australia"
      ]
    },
    {
      "id": "https://openalex.org/A2628937019",
      "name": "Jieping Ye",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2012836890",
      "name": "Jun Li",
      "affiliations": [
        "City University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2173412116",
      "name": "Yuelong Shu",
      "affiliations": [
        "Sun Yat-sen University",
        "Chinese Academy of Medical Sciences & Peking Union Medical College"
      ]
    },
    {
      "id": "https://openalex.org/A2121444646",
      "name": "Mang Shi",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A2115699384",
      "name": "Zhaorong Li",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1982899729",
      "name": "Yong He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2010770749",
      "name": "Pan Fang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3194159946",
      "name": "Yongtao Shan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3189241738",
      "name": "Yuanfei Pan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117545886",
      "name": "Yanhong Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100750335",
      "name": "Yi-Chang Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2125251258",
      "name": "Yihao Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2024876568",
      "name": "Yi Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2636662135",
      "name": "Zhenyu Zeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104082825",
      "name": "Zhan Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1977056848",
      "name": "Feng Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2169214606",
      "name": "Edward C. Holmes",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2628937019",
      "name": "Jieping Ye",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2012836890",
      "name": "Jun Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2173412116",
      "name": "Yuelong Shu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2121444646",
      "name": "Mang Shi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115699384",
      "name": "Zhaorong Li",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2008365039",
    "https://openalex.org/W2093205346",
    "https://openalex.org/W2475627770",
    "https://openalex.org/W1972177221",
    "https://openalex.org/W87274109",
    "https://openalex.org/W2896342372",
    "https://openalex.org/W4401443086",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W4205773061",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W4317374308",
    "https://openalex.org/W3186179742",
    "https://openalex.org/W4403245162",
    "https://openalex.org/W4362471278",
    "https://openalex.org/W4382490702",
    "https://openalex.org/W4382603228",
    "https://openalex.org/W4297243391",
    "https://openalex.org/W4404349982",
    "https://openalex.org/W4403789768",
    "https://openalex.org/W2055214702",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W1901263135",
    "https://openalex.org/W1995538509",
    "https://openalex.org/W4304700957",
    "https://openalex.org/W2009371644",
    "https://openalex.org/W4404377414",
    "https://openalex.org/W4327718177",
    "https://openalex.org/W2102709979",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4390783938",
    "https://openalex.org/W4220685469",
    "https://openalex.org/W3037888463",
    "https://openalex.org/W3203588026",
    "https://openalex.org/W2977069059",
    "https://openalex.org/W4287447409",
    "https://openalex.org/W6772383348",
    "https://openalex.org/W3217314753",
    "https://openalex.org/W4388134851",
    "https://openalex.org/W4210592951",
    "https://openalex.org/W2541638185",
    "https://openalex.org/W4221098111",
    "https://openalex.org/W4205631037",
    "https://openalex.org/W2927351257",
    "https://openalex.org/W6772381481",
    "https://openalex.org/W4388979610",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W4225258284",
    "https://openalex.org/W1980759842",
    "https://openalex.org/W6763868836",
    "https://openalex.org/W2740619578",
    "https://openalex.org/W2106159290",
    "https://openalex.org/W2618265628",
    "https://openalex.org/W4407269347",
    "https://openalex.org/W4396904350",
    "https://openalex.org/W4293150093"
  ],
  "abstract": "Abstract The language of biology, encoded in DNA, RNA and proteins, forms the foundation of life but remains challenging to decode owing to its complexity. Traditional computational methods often struggle to integrate information across these molecules, limiting a comprehensive understanding of biological systems. Advances in natural language processing with pre-trained models offer possibilities for interpreting biological language. Here we introduce LucaOne, a pre-trained foundation model trained on nucleic acid and protein sequences from 169,861 species. Through large-scale data integration and semi-supervised learning, LucaOne shows an understanding of key biological principles, such as DNA–protein translation. Using few-shot learning, it effectively comprehends the central dogma of molecular biology and performs competitively on tasks involving DNA, RNA or protein inputs. Our results highlight the potential of unified foundation models to address complex biological questions, providing an adaptable framework for bioinformatics research and enhancing the interpretation of life’s complexity.",
  "full_text": "Nature Machine Intelligence | Volume 7 | June 2025 | 942–953 942\nnature machine intelligence\nArticle\nhttps://doi.org/10.1038/s42256-025-01044-4\nGeneralized biological foundation model \nwith unified nucleic acid and protein \nlanguage\n \nYong He    1 , Pan Fang    1, Yongtao Shan    2, Yuanfei Pan3, Yanhong Wei4, \nYichang Chen    5, Yihao Chen    6, Yi Liu1, Zhenyu Zeng    1, Zhan Zhou    5, \nFeng Zhu    7, Edward C. Holmes8, Jieping Ye1, Jun Li9, Yuelong Shu6,10,11, \nMang Shi    12  & Zhaorong Li    1 \nThe language of biology, encoded in DNA, RNA and proteins, forms \nthe foundation of life but remains challenging to decode owing to its \ncomplexity. Traditional computational methods often struggle to \nintegrate information across these molecules, limiting a comprehensive \nunderstanding of biological systems. Advances in natural language \nprocessing with pre-trained models offer possibilities for interpreting \nbiological language. Here we introduce LucaOne, a pre-trained foundation \nmodel trained on nucleic acid and protein sequences from 169,861 species. \nThrough large-scale data integration and semi-supervised learning, \nLucaOne shows an understanding of key biological principles, such as DNA–\nprotein translation. Using few-shot learning, it effectively comprehends the \ncentral dogma of molecular biology and performs competitively on tasks \ninvolving DNA, RNA or protein inputs. Our results highlight the potential \nof unified foundation models to address complex biological questions, \nproviding an adaptable framework for bioinformatics research and \nenhancing the interpretation of life’s complexity.\nFrom the discovery of DNA to the sequencing of every living form, the \nfaithful rule-based flow of biological sequence information from DNA to \nRNA and protein has been the central tenet of life science. These three \nmajor information-bearing biopolymers carry out most of the work in \nthe cell and then determine the structure, function and regulation of \ndiverse living organisms1,2.\nThe basic information in these three biopolymers is presented in \na linear order of letters: 4 nucleotides for DNA or RNA and 20 standard \nand several non-standard amino acids for proteins. Their secondary \nor higher structure also contains information attributed to biological \nfunctions and phenotypes. This genetic principle resembles the human \nlinguistic system. Darwin wrote in his The Descent of Man: “The forma-\ntion of different languages and distinct species, and the proofs that \nboth have been developed through a gradual process, are curiously \nthe same3. ” Various studies have testified to these parallels ever since, \npromoting the understanding and decoding of biological language4–6.\nGiven the rapid advancements in machine learning technolo -\ngies for human language processing, our efforts to decode biologi -\ncal language are bound to accelerate by leveraging insights from the \nformer. The recent development of transformer architecture showed \nthe superior capability of generalizing massive sequence-based knowl-\nedge from large-scale labelled and unlabelled data, which empowered \nlanguage models and achieved unprecedented success in natural lan-\nguage processing tasks. By pre-training on large datasets, foundational \nmodels learn the general characteristics of biological sequences. These \nmodels compute the input sequence into an embedding, a numerical \nrepresentation that succinctly captures its semantic or functional \nproperties. On this basis, various biological computation problems \nReceived: 9 September 2024\nAccepted: 29 April 2025\nPublished online: 18 June 2025\n Check for updates\nA full list of affiliations appears at the end of the paper.  e-mail: sanyuan.hy@alibaba-inc.com; shim23@mail.sysu.edu.cn; lzr098@gmail.com\nNature Machine Intelligence | Volume 7 | June 2025 | 942–953\n 943\nArticle https://doi.org/10.1038/s42256-025-01044-4\nFig. 1). A semi-supervised learning 19 approach was employed to \nenhance its applicability in biological language modelling. Therefore, \nour pre-training tasks have been augmented with eight foundational \nsequence-based annotation categories. These annotations comple -\nment the fundamental self-supervised masking tasks, facilitating more \neffective learning for improved performance in downstream applica-\ntions (Fig. 2b and Supplementary Fig. 3). Overall, LucaOne comprised \n20 transformer-encoder blocks with an embedding dimension of 2,560 \nand a total of 1.8 billion parameters. The downstream task utilized a \nmodel checkpoint at 5.6 million (‘Pre-training information’ in Meth -\nods). T o illustrate the benefits of mixed training for nucleic acids and \nproteins, we trained the two additional models (LucaOne-Gene and \nLucaOne-Prot) separately using nucleic acids and proteins individu -\nally, and made a comparison using the same checkpoint in the central \ndogma of molecular biology task. Details of the pre-training data, \npre-training tasks and pre-training details refer to Pre-training data \ndetails’ , ‘Pre-training tasks details’ and ‘Pre-training information’ in \nMethods, respectively.\nWe utilized t-distributed stochastic neighbour embedding (t-SNE) \nto visualize the embeddings from three distinct datasets: a nucleic acid \ndataset (S1), comprising sequences from 12 marine species, a protein \ndataset (S2), consisting of sequences from 12 clans (Pfam clans are \ngroups of protein families that are evolutionarily related and share simi-\nlar structures and functions), and another protein dataset (S3), organ-\nizing recently updated sequences from the top 12 most prevalent Gene \nOntology (GO) terms, biological processes subset. This visualization \nwas compared with the results obtained using the MultiHot, DNABert215 \nand ESM2-3B10 embedding approaches. The outcomes, as illustrated \nin Fig. 2c–j, revealed that the embeddings produced by LucaOne were \nmore densely clustered, indicating that this method may encapsulate \nadditional contextual information beyond the primary sequence data \n(dataset S1, S2 and S3 details are in ‘LucaOne embeddings level analysis’ \nin Methods, and the embedding clustering metrics are in Extended Data \nTable 1). In addition, we examined the correlation between nucleic acid \nsequences and protein sequences of the same genes based on embed-\ndings. The results demonstrated that, despite the absence of paired \ndata and explicit correspondence relationships during training, the \nsequences (nucleic acids and proteins) of the same gene exhibited \nconvergence within the LucaOne embedding space. Moreover, this con-\nvergence was more pronounced compared with other independently \ntrained pre-trained models and sequence alignment methods (details \nin ‘LucaOne embeddings level analysis’ in Methods).\nLearning the central dogma of molecular biology\nOur additional objective was to account for known gene and protein \nsequences occupying a minuscule yet biologically active niche within \ntheir respective design spaces, with a subset of these sequences exhib-\niting correspondence based on the central dogma. Consequently, \nthroughout the training phase of the LucaOne model, we refrained \nfrom incorporating any explicit representations of the relationships \nbetween DNA, RNA and protein sequence, seeking to test whether the \nmodel inherently grasped the correlation between the genetic and \nprotein data22,23.\nWe designed an experimental task to assess the ability of LucaOne \nto recognize the inherent link between DNA sequences and their corre-\nsponding proteins. We have constructed a dataset comprising DNA and \nprotein matching pairs derived from the National Center for Biotech-\nnology Information (NCBI) RefSeq database, with a proportion of 1:2 \nbetween positive and negative samples (Fig. 3a,b and ‘Details of central \ndogma tasks’ in Methods). T o better test whether the LucaOne model \nhas already learned the correspondence between nucleic acid and pro-\ntein sequences in the central dogma, few-shot learning was employed \nfor validation. The samples were then randomly allocated across the \ntraining, validation and testing sets in a ratio of 4:3:25, respectively \n(refer to ‘original dataset’ in the following sections).\ncan be addressed through direct prediction, embedding analysis or \ntransfer learning7. In life science, substantial efforts have been put into \nadopting such language models, especially in protein tasks (ProTrans8, \nProteinBERT9, ESM210, Ankh11), such as structure prediction 10,12 and \nfunction annotation 13,14. In the realm of nucleic acid-focused tasks, \nseveral models have been introduced within niche areas (DNABert215, \nHyenaDNA16, ScBert17). However, a broadly applicable, foundational \nmodel for nucleic acids remains elusive in widespread adoption across \nvarious disciplines.\nTherefore, we have opted for a more fundamental and uni -\nversal approach and developed a pre-trained, biological language \nsemi-supervised foundation model, designated as ‘LucaOne’ , which \nintegrates nucleic acid (DNA and RNA) and protein sequences for \nconcurrent training. This methodology allows the model to process \nand analyse data from nucleic acids and proteins simultaneously, \nfacilitating the extraction of complex patterns and relationships inher-\nent in the processes of gene transcription and protein translation18,19.\nWe further examine that LucaOne shows an emergent understand-\ning of the central dogma in molecular biology: the correlation between \nDNA sequences and their corresponding amino acid sequences, sup-\nporting the notion that the concurrent training of nucleic acid and \nprotein sequences together yields valuable insights 20. T o illustrate \nLucaOne’s practical effectiveness, we present seven distinct bioinfor-\nmatics computational scenarios. These examples highlight LucaOne’s \nease of use in real-world applications and demonstrate its superior \nperformance compared with state-of-the-art models and other exist-\ning pre-trained models.\nResults\nLucaOne as a unified nucleic acid and protein foundation \nmodel\nLucaOne was designed as a biological language foundation model \nthrough extensive pre-training on massive datasets, enabling the \nextraction of generalizable features for effective adaptation to vari-\nous downstream tasks, therefore allowing researchers to efficiently \nemploy pre-trained embeddings from LucaOne for a diverse range \nof bioinformatics analysis, even when there is limited training data, \nthereby substantially enhancing their performance. This model lever-\nages a multifaceted computational training strategy that simultane-\nously processes nucleic acids (DNA and RNA) and protein data from \n169,861 species (only those with a minimum of 10 sequences within \nthe training dataset are counted). Consequently, LucaOne has the \ncapability to interpret biological signals and, as a foundation model, \ncan be guided through input data prompts to perform a wide array of \nspecialized tasks in biological computation.\nFigure 1 depicts the LucaOne framework, which adopts and \nenhances the transformer encoder 21 (‘Model architecture’ in Meth -\nods). LucaOne’s vocabulary comprises 39 unique tokens represent -\ning nucleotides and amino acids (‘Vocabulary’ in Methods). We used \npre-layer normalization to supersede post-layer normalization to make \ndeep networks easier to train. Rotary position embedding replaces tra-\nditional absolute positional encoding for inferring longer sequences. \nIn addition, the mixed-training model distinguishes nucleotides and \namino acids by utilizing token-type encoding, assigning 0 to nucleo-\ntides and 1 to amino acids.\nT o comprehensively assimilate the patterns and structures per-\nvasive in universal biological language and the inherent knowledge \nthese patterns convey, we have compiled an extensive collection of \nnucleic acid and protein datasets as the foundational pre-training \nmaterial. RefSeq provided nucleic acid sequences, including DNA \nand RNA, and annotations for eight selected genome region types \nand their order-level taxonomy. Protein data included sequences \n(from UniProt and ColabFoldDB), annotations (from InterPro, Uni -\nProt and ColabFoldDB) and tertiary structures (from RCSB-PDB and \nAlphaFold2; Fig. 2a, Extended Data Figs. 1 and 2, and Supplementary \nNature Machine Intelligence | Volume 7 | June 2025 | 942–953 944\nArticle https://doi.org/10.1038/s42256-025-01044-4\nThe study employed a simple downstream network to evaluate \nLucaOne’s predictive capacity (Fig. 3c). LucaOne encoded nucleic acid \nand protein sequences into two distinct fixed embedding matrices \n(Frozen LucaOne). Then, each matrix was processed through pooling \nlayers (either max pooling or value-level attention pooling24) to pro-\nduce two separate vectors. The vectors were concatenated and passed \nthrough a dense layer for classification.\nWe compared the performance of different modelling approaches, \nincluding one-hot with a transformer, a transformer model with the ran-\ndom initialization, nucleic acid embeddings from DNABert2, protein \nembeddings from ESM2-3B, and two separate versions of the LucaOne \nfoundation model trained independently on nucleic acid and protein \nsequences (LucaOne-Gene and LucaOne-Prot), and the unified training \nfoundational version of LucaOne (Fig. 3d and Extended Data Table 2). \nThe findings indicated that modelling methods lacking pre-trained \nelements (one-hot and random initialization; Extended Data Table 2) \nwere unable to acquire the capacity for DNA–protein translation in \nthis dataset. In contrast, LucaOne’s embeddings were able to learn this \ncapacity with limited training examples effectively and substantially \nsurpassed both the amalgamation of the other two pre-trained models \n(DNABert2 + ESM2-3B) and the combined independent nucleic acid \nand protein LucaOne models using the same dataset, architecture \nand checkpoint. This suggests that pre-trained foundational models \ncan provide additional information beyond the specific task samples \nfor such biological computation tasks. Moreover, LucaOne’s unified \ntraining approach for nucleic acids and proteins enabled it to learn \nwithin a single framework, thereby capturing the fundamental intrinsic \nrelationships between these two categories of biological macromol-\necules to some extent.\nA CDS–protein dataset using data from the original task was \nprepared to further evaluate the model’s capabilities. Figure 3d  \nshows that models trained exclusively on the CDS–protein dataset \ndemonstrated improvements across multiple performance metrics, \nincluding accuracy, F1 score and AUC. When comparing the LucaOne \nmodel with the LucaOne-Gene and LucaOne-Prot models and the \nDNABert2 + ESM2-3B model, the enhancements were more substan -\ntial in the latter two model groups compared with LucaOne alone. \nThis suggests that the LucaOne model has marginally enhanced \nData process\nUnified LLM\nDownstream\ntasks\nDNA\nDNA RNA Protein\nProtein\nInfluenza A virusNon-coding RNA Protein BProtein A\nRefSeq UniProt UniRef50 ColabFoldDB InterPro RCSB-PDB AlphaFoldDB\nDNA and RNA\nSequence Sequence StructureAnnotationAnnotation\nUnified dataset\nUnif orm nuc leot ide and amino acid charact erizat ion\n[CLS] A G H H\nG H H\nD G A\nD G A\nE [SEP]\n[SEP][CLS]\nToken-level pretraining task\nT ransc ript ion T ranslat ion\nFimbrium\nInner membrane\nNucleoid\nOuter membrane\nCytoplasm\nFlagellum\nPeriplasm\nV a r i d n a v i r i a G r een n on-sul fur bac t e ria\nMeth a n o micr obiale s\nC ilia t e s G r e e n\nPla ntsAnima l s\nF ungi\nFlagellat es\nH a l oar cha e a\nMeth a n o bac t erium\nMeth a n o c o c c u s\nT h erm o c o c cus\nT h erm opr o t e u s\nPyr odic tium\nG r am-p osi tiv e  bact e r i a\nPurp l e bact eri a\nC y a nobac t e r i a\nF l a v o bac t eri a  and all ies\nT h erm o t oga l e s\nR i b ozy v i r i a\nR i b o v i r i a\nM o n o dna viria\nDupl odna vir i a\nA d n a v iria\nViru s Bact e r i a Archaea\nLuca\nE xtracel l u lar\nrRNAs tRNAs\ns n RNAs\nIncRNAs\ns noRNAsmiRNAs and siRNAs\nFIT C  flu o r escence  intensity\nA f t e r  p r o t e a s e B e fo r e p r o t e ase\nFraction of  cell s\nE u k a r y a\nReplic a t ion\nReverse\nt ransc ription\nCent ral dogma G enusTa x P rot Loc P rotSta b ncRNAFa m I n f A ncRP I\nSpan-level pretraining task Structure-level pretraining task Sequence-level pretraining task\nE1\nP1 P2 P3 P4 P5\nO2 O3 O4 O5\nE3 E4 E5\nT2 T2 T2 T2 T2 T2 T2 T2 T2 T2\nP10 P11 P12 P13 P14\nE10 E 11 E 12 E 14\nO1 O11 O12 O13 O14O10\n[Mask]\nMask Mask\n[Mask]\nProteinData 1 Data 2 Data 1 Data 2 Data 3\nRNA\nProtein\nTokenization\nDynamicRandomMask\nInput\nEncoder\nOutput\nToken-level classification Span-level classification Sequence-level classificationStructure-level regression\na    LucaData\nb    LucaOne\nc    LucaTasks\n1 0\n1\n1 0\n2\n1 0\n3\n1 0\n4\n1 0\n5\nPPI\n+\n+\n+\nOptimized transformer encoder\nEmbedding\nNucleic acid dataset Protein dataset\nMicrosporidia\nFig. 1 | The workflow of LucaOne. a, Data source and processing for pre-\ntraining. The nucleic acid data are from RefSeq and included sequences and \nannotations, which consisted of order-level taxonomy and eight selected \ngenome region types. Protein encompasses sequences (from UniRef50, UniProt \nand the ColabFoldDB metagenomic protein collection (that is, ColabFoldDB), \nwhere UniRef50 is clustered set of sequences from the UniProt with at least 50% \nsequence identity to enhance the learning of these representative sequences), \nannotations (order-level taxonomy from UniProt and ColabFoldDB, keywords \nfrom UniProt, and features such as sites, homologous superfamilies and domains \nfrom InterPro) and tertiary structures (experimentally determined structure \nfrom RCSB-PDB and predicted structure from AlphaFold2-Swiss-Prot). b, Pre-\ntraining model architecture and pre-training tasks. The encoder is an improved \ntransformer encoder. Based on two self-supervised mask tasks, an additional \neight semi-supervised pre-training tasks were introduced to enhance the model’s \nunderstanding of the data through annotations in the sequences. c, Downstream \ntasks for validation based on LucaOne embedding. The representational \ncapabilities of LucaOne were verified using eight downstream tasks, whose \ninputs include DNA, RNA, proteins and their interrelated pairs. [CLS], a special \ntoken added at the start of the input sequence to indicate its beginning. [SEP], \na special token added at the end of the input sequence to indicate its ending. X \n(A, G, H, D, E, etc.) represents the input sequence tokens (nucleotides or amino \nacids). E, embeddings of amino acids or nucleotides; P, positional embeddings; \nT, the molecular type embedding of the input sequence, where T1 denotes the \nnucleic acid and T2 denotes the protein. O, the output representation vectors \nof each token in the input sequence via transformer-encoder. FITC, fluorescein \nisothiocyanate.\nNature Machine Intelligence | Volume 7 | June 2025 | 942–953\n 945\nArticle https://doi.org/10.1038/s42256-025-01044-4\ndiscriminative capabilities between coding and non-coding regions. \nHowever, our experimental results (Supplementary Fig. 9) demon-\nstrate a decline in LucaOne’s prediction accuracy as the number of \nexons within the target sequence region increases. This observed \nlimitation represents a critical area for future model optimization. \nFurthermore, when evaluating performance across datasets from \ndifferent species, both models show consistent results, except for a \nnotable decrease in performance with Ciona intestinalis. This devia -\ntion can largely be attributed to its unique codon usage patterns, \nwhich differ significantly from other species in the study (Fig. 3e,f ). \nGiven the minimal sample size for this species in the dataset and \nwith only 16% designated for training, it is likely that the models \nwere unable to adequately learn the specific rules of the central \ndogma under these codon preferences, even though the analysis \nwas conducted under the rule of the standard code. The observed \ndivergence in codon preference suggests that C. intestinalis may have \nmore distinctive translation mechanisms from genetic material to \nproteins, which could be attributed to its unique evolutionary tra -\njectory and selective pressures 25. Furthermore, a dataset expanded \nwith two urochordate species was utilized for model training and \ntesting. The F1 score of the new model improved significantly for \nC. intestinalis, while the performance for other species remained \ncomparable to that of the original model (‘Details of central dogma \ntasks’ in Methods and Extended Data Table 3). Based on this, it is \nc    MultiHot (S1)a\nb\nA\nM K D V VE\nE E\nE\nV V\nD S P F L\nL L\nF\nF\nF\nFFFP\nP\nS\nS\nD\nK\nK\nK\nM\nxyz 0\nxyz 0\nRodentia\nCDS Intron Regulatory\nCDS\nS1 D13713 H3440\nIntron Regulatory\ntRNA ncRNA rRNA miscRNA tmRNA\nEight selected genome region types \nNucleic acid sequence\nEight types regions\nHomologous\nPosition\nPosition\nAnnotaion\nAnnotaion\nSpan:type:CDS\nSpan:site:S8 Span:domain:D100 Span:homo:H10\nSeq:kewords:\nTaxonomy\ntree\nSpan:type:intron Span:type:regulator\nGene Segment\nToken:mask:NT\nToken:mask:AA Token:mask:AA Token:mask:AA\nToken:mask:NT\nOrder\nSeq:order:rodentia\nSeq:order:vitales\nAmino acid sequence\nSequence\nSequence\nSelected regions\nDomain Site\nRodentia\nO0\nH0 D0 S0 S945D13716H3442\n3,443 types\n10 pre-training tasks:\nToken level\nGene mask\nNucleic acid\nProtein\nProt mask Genome type Prot homo Prot site Prot domain Gene taxonomy Prot taxonomy Prot keyword Prot AA position\nSpan level Seq level Structure level\n13,717 types 946 types\nO734 K0 ... K1178...\n... ... ...\nVitales\nVitales\nAmyloid\nAmyloid\nAmyloid\nCytoplasm\nCytoplasm\nCytoplasm\nTaxonomy order Keywords\nn–1xyz 1 xyz 2 xyz 3 xyz 4 xyz 5 xyz 6 xyz 7 xyz 8 xyz 9 xyz 10 xyz 11 xyz 12 xyz 13 xyz 14 xyz 15 xyz 16 xyz 17\nn–1xyz 1 xyz 2 xyz 3 xyz 4 xyz 5 xyz 6 xyz 7 xyz 8 xyz 9 xyz 1 0 xyz 1 1 xyz 1 2 xyz 1 3 xyz 1 4 xyz 1 5 xyz 1 6 xyz 1 7 xyz\nxyz\nA T G C\nC\nC\nC\nO\nQ\nQ\nG G G\nG G\nG\nG G G\nT T\nT T\nA A A A A A A\nAAA\nA A A A A A A A A A A\nAA\nf    MultiHot (S2)\ng     ESM2-3B (S2)d     DNABert2 (S1)\ne    LucaOne (S1)\ni    ESM2-3B (S3) j    LucaOne (S3)\nh     LucaOne (S2)\nsp.01 CL0023\nGO:0000105\nGO:0006094\nGO:0009423\nGO:0009245\nGO:0009432\nGO:0044205\nGO:0002181\nGO:0006099\nGO:0006189 GO:0006526\nGO:0006207\nGO:0042274\nCL0036 CL0010\nCL0159 CL0063 CL0123\nCL0186 CL0167 CL0021\nCL0020 CL0193 CL0236\nsp.02 sp.03\nsp.04 sp.05 sp.06\nsp.07 sp.08 sp.09\nsp.10 sp.11 sp.12\nFig. 2 | The data and tasks for pre-training LucaOne, and t-SNE on four \nembedding models. a, Details of pre-training data. Nucleic acids included \nsequence and two kinds of annotation. The protein consisted of sequence, five \ntypes of annotation and tertiary structure coordinates. NT, nucleotides; AA, \namino acids. b, Details of pre-training tasks. The pre-training tasks included two \nself-supervised mask tasks and eight semi-supervised tasks. c–j, t-SNEs of the \nfour embedding methods on the S1 nucleic acid contigs with 12 species from the \nCAMI2 database (c,d,e by MultiHot, DNABert2 and LucaOne, respectively), S2 \nprotein sequences across 12 clan categories from the Pfam database (f,g,h by \nMultiHot, ESM2-3B and LucaOne, respectively), and S3 protein sequences across \nthe top 12 most prevalent GO terms from the UniProt database (i,j by ESM2-3B \nand LucaOne, respectively). The results show that LucaOne’s representation has \nbetter clustering on these three datasets (nucleic acid sequences of the same \nspecies should be clustered because of high sequence similarity, and protein \nsequences of the same Pfam clan or GO term should be clustered of similar \nstructures and functions). sp.01, unclassified Pseudomonas species; sp.02, \nAeromonas salmonicida; sp.03, unclassified Vibrio species; sp.04, Streptomyces \nalbus; sp.05, Aliivibrio salmonicida; sp.06, unclassified Brevundimonas \nspecies; sp.07, Vibrio anguillarum; sp.08, Aliivibrio wodanis; sp.09, Moritella \nviscosa; sp.10, unclassified Enterobacterales species; sp.11, unclassified \nTenacibaculum species; sp.12, unclassified Aliivibrio species; GO:0000105, \nl-histidine biosynthetic process; GO:0009245, lipid A biosynthetic process; \nGO:0002181, cytoplasmic translation; GO:0006207, ‘de novo’ pyrimidine \nnucleobase biosynthetic process; GO:0006094, gluconeogenesis; GO:0009432, \nSOS response; GO:0006099, tricarboxylic acid cycle; GO:0042274, ribosomal \nsmall subunit biogenesis; GO:0009423, chorismate biosynthetic process; \nGO:0044205, 'de novo' uridine monophosphate (UMP) biosynthetic process; \nGO:0006189: 'de novo' inosine monophosphate (IMP) biosynthetic process; \nGO:0006526, l-arginine biosynthetic process).\nNature Machine Intelligence | Volume 7 | June 2025 | 942–953 946\nArticle https://doi.org/10.1038/s42256-025-01044-4\ninferred that with an expanded training data size encompassing a \nwider array of central dogma rules, LucaOne has the potential to \nmore thoroughly assimilate the syntactical rules associated with \ngenetic information processing, enabling its application to a more \ndiverse set of scenarios.\nLucaOne provides embeddings for diverse biological \ncomputational tasks\nT o ascertain the capacity of the LucaOne model to provide effective \nembeddings for a variety of downstream tasks, we conducted valida-\ntion studies across seven distinct downstream tasks, which include \nsingle-sequence tasks such as prediction of genus taxon (GenusTax), \nclassification of non-coding RNA (ncRNA) families (ncRNAFam), and \nthe prediction of protein subcellular localization (ProtLoc) as well as \nthe assessment of protein thermostability (ProtStab). For homogene-\nous paired-sequence tasks, we predicted influenza haemagglutination \nassays based on a pair of nucleic acid sequences (InfA) and assessed \nprotein–protein interactions (PPI) utilizing pairs of protein sequences. \nIn addition, we forecasted the interactions between ncRNA and proteins \n(ncRPI) for the heterogeneous sequence task (full task descriptions in \n‘Downstream tasks details’ in Methods and Extended Data Table 4).\nFor each task, we performed two types of comparative analysis: \none against the state-of-the-art results and another using the same \ndownstream network to assess LucaOne embeddings against the widely \na c\nRefSeq\nData source\nTotal 25,600 samples\nDataset statistics Pair sequence\nCodon usage bias\nPositive\nNegative 1\nInsert\nReplace\nGene\nSame\ngene\nOriginal\ndataset\nValues\nAccuracy\nLucaOne (CDS–protein) LucaOne (Gene/Prot)(CDS–protein) DNABert2 + ESM2-3B (CDS–protein)\nDNABert2 + ESM2-3B (original)LucaOne (Gene/Prot)(original)LucaOne (original)\nF1 score AUC\nCDS–protein\ndataset\nCDS\nProtein\nReplace\nDelete\nInsert\nDelete\nTranslation\nCysAla\nCiona intestinalis (738) Homo sapiens (1,806) Danio rerio (1,263) Drosophila melanogaster (3,004) Gallus gallus (1,177)\nAsp Glu Phe Gly His Lle Lys Leu Asn Pro Gln Arg Ser Stop Thr Val Tyr Met Trp\nDNA\n–protein\n2\n(Binary class)\n3,200 309-11\n2,455-617\n1,340-244\n2,400\n20,000\nNegative 2\n13 species\n10,741 genes\nNetwork\nExon Exon ExonIntron Intron\nb\nd\nf\nDNA\nA\nProt\nA\nM\nT\nG\nC\n…\nG\nR\n…\ne\nMatrix 1\nYes?\nOutput\nSpecies\nTask Input\ntype\nTraining,\nvalidation\nand test size\nLabel\nsize\nMinimum, maximum\nand average\nsequence length\n(DNA-Prot)\nGallus gallus (1,177)\nManis pentadactyla (1,430)\nHomo sapiens (1,806)\n0.7235\n0.7449\n0.6691\n0.7103\n0.7097\n0.8170\n0.7512\n0.7572\n0.7298\n0.7304\n0.5174\n0.7585\n0.6967\n0.8318\n0.8587\n0.8192\n0.8283\n0.8331\n0.8761\n0.8491\n0.8540\n0.8412\n0.8467\n0.7927\n0.8540\n0.8345\n0.5401\n0.5590\n0.4644\n0.5510\n0.6040\n0.6635\n0.6044\n0.5803\n0.5896\n0.5274\n0.3043\n0.5956\n0.4808\n0.7077\n0.7385\n0.6967\n0.7248\n0.7656\n0.7543\n0.7462\n0.7358\n0.7441\n0.7270\n0.6965\n0.7421\n0.7110\nDrosophila melanogaster (3,004)\nPteropus vampyrus (593)\nMus musculus (4,375)\nDasypus novemcinctus (1,769)\nSus scrofa (1,336)\nMacaca mulatta (2,192)\nSarcophilus harrisii (1,011)\nCiona intestinalis (738)\nErinaceus europaeus (1,706)\nDanio rerio (1,263)\nF1 score\nLucaOne DNABert2\n+ ESM2-3B LucaOne DNABert2\n+ ESM2-3B\nAccuracy\nConcat\nMatrix 2\nPooling\nFC\nPooling\nLucaOne\nExon\nCDS\n100 bp 100 bp\n0.8\n1.0\n0.8451 0.8453 0.8163 0.8048\n0.7400 0.7309\n0.8089\n0.7392 0.7806\n0.6804 0.6661\n0.5689\n0.9298 0.9101 0.9055\n0.8616\n0.7959\n0.7491\n0.6\n0.4\n0.2\n0\nIntron Exon ExonIntron\n100.00%\nProportion of diﬀerent\ncodon usage\n80.00%\n60.00%\n40.00%\n20.00%\n0%\nFig. 3 | The workflow of the central dogma of molecular biology task.  \na, Dataset from 13 species with 10,471 genes in RefSeq. b, Prepared 8,533 positive \nsamples and 17,067 negative samples and took a specific sample dividing \nstrategy to test the model performance in this task (training, validation and \ntesting sets in a ratio of 4:3:25). c, Based on different embedding methods \nof DNA–protein pair sequences, a simple downstream network was used for \nmodelling and illustrating their representational ability. d, Models performance \ncomparison (validation + testing dataset) on original and CDS–protein datasets. \ne, Comparative performance analysis (validation + testing dataset) of the models \nacross diverse species datasets (sample counts in brackets). FC, fully connected \nlayer. f, One species for each class was selected to undergo a codon usage bias \nanalysis, which adheres to the conventions of the standard genetic code; this \nentails comparing the relative usage frequencies of different codons for each \namino acid, ensuring that the total adds up to 100%. The species C. intestinalis \nexhibits a codon usage bias that is markedly distinct from that of other species—\noverall lower GC content. Details in ‘Details of central dogma tasks’ in Methods.\nNature Machine Intelligence | Volume 7 | June 2025 | 942–953\n 947\nArticle https://doi.org/10.1038/s42256-025-01044-4\nused nucleic acid and protein pre-trained language models, DNABert2 \nand ESM2-3B, respectively. These comparative analyses are instru -\nmental in elucidating the incremental contributions of foundation \nmodels when addressing related analytical tasks and in evaluating the \nspecific effectiveness of the embeddings generated by LucaOne with \nDNABert2 and ESM2-3B.\nSimilarly, we used a simple downstream network to facilitate pro-\ncessing these tasks. We illustrated the capacity of trained and frozen \nLucaOne to analyse nucleic acid (DNA and RNA) and protein sequences. \nFigure 4a–c shows the network architectures for three distinct input \ntypes. For tasks requiring paired inputs, a concatenation step is neces-\nsary to merge the output vectors of the pairs into a single extended vec-\ntor. Finally, a fully connected layer was utilized for the ultimate output, \nwhich could be for classification or regression purposes.\nFigure 4d–k shows a comparative analysis of performance on \nseven distinct biomedical tasks, revealing that LucaOne demonstrates \nsuperior representational capabilities over competing models in the \nGenusTax, ProtStab, ncRNAFam, influenza A antigenic relationship pre-\ndiction (InfA) and PPI evaluations, and comparable performance on the \nother two, ProtLoc and ncRPI. Notably, within the nucleic acid-centric \nGenusTax and ncRNAFam, LucaOne’s accuracy has increased by 0.05 \nand 0.026, respectively, indicating a marked improvement over DNA-\nBert2. In the InfA task, LucaOne excelled with an exceptional accuracy \nof 1.0, reflecting its outstanding ability to represent these task data. \nFor the ProtStab task, it surpassed ESM2-3B with a 0.015 increase in \nSpearman’s rank correlation coefficient and similarly showed a slight \nimprovement in the evaluation of PPI. Compared with DeepLocPro26 \nin the task of ProtLoc, LucaOne was competitive with ESM2-3B and \nshowed a 0.025 accuracy improvement. Although LucaOne did not \noutperform the elaborate network model ncRPI-LGAT27 in evaluating \nncRPI, it still exceeded the combined abilities of DNABert2 and ESM2-\n3B. LucaOne’s effectiveness was particularly notable in processing \ntasks involving heterogeneous sequences of nucleic acids and proteins; \nemploying a unified representation model is advantageous compared \nwith using separate models. The outcomes of these tasks underscored \nthe robust representational capabilities of LucaOne for both nucleic \nacid and protein sequences. LucaOne could improve performance \nacross a spectrum of downstream tasks, streamline networks for down-\nstream tasks and reduce computational resource demands (more \nresults of hyperparameter comparison experiments and detailed met-\nrics in ‘Comparison result details’ in Methods, Extended Data Table 5 \nand Supplementary Fig. 4).\na    Single\nMatrix\nMatrix 1\nDNA–protein\nRNA RNA–RNA Protein–protein RNA–protein\nProtein AInfluenza A virusNon-coding RNA\nrRNAs tRNAs\nsnRNAs\nsnoRNAs\nlncRNAs\nmiRNAs and siRNAs\nProtein B\nDNA Protein Protein\n0.7309\n0.8048\nDNABert2/ESM2-3B\n0.124\nBERTax\n0.92\nDeepLocPro\n0.9452\nLucaOne\n0.767\nDNABert2\n0.9610\nDNABert2\nLucaOne (Gene/Prot)\nMatrix 1\nEmbedding\nDNA\nTranscription\nReplication\nReverse\ntranscription\nTranslation\nRNA Protein\n0.8453\nLucaOne\n0.9864\nLucaOne\n0.817\nLucaOne\n0.7718\nLucaOne\n0.9496\nESM2-3B\n*\nEncoder or not Pooling/FC Direction Concat Output\nMatrix 2\nMatrix 2\nOutput\nOutput\nOutput\nEncoder\nPooling\nFC\nEncoder\nPooling\nEncoder\nPooling\nEncoder\nPooling\nFC\nc    Heterogeneous pair\nb     Homogeneous pair\nd     Central dogma\nh    ncRNAFam i    InfA j    PPI\nf    ProtLoc\nk    ncRPI\ne    GenusTax g     ProtStab\nFC\n0.90\nRNAGCN\n0.9966\nDNABert2\n1.0\nLucaOne\n0.9010\nPREDAC\n0.9764\nESM2-3B\n0.9460\nDNABert2/ESM2-3B\n0.9479\nLucaOne\n0.9774\nLucaOne\n0.966\nncRPI-LGAT\n0.9719\nDeepPPI\n0.73\nTAPE\n0.7556\nESM2-3B\nFIT C  flu o r escence  intensity\n1 0\n1\n1 0\n2\n1 0\n3\n1 0\n4\n1 0\n5\nProtein\nA f t e r  p r o t e a s e B e fo r e p r o t e ase\nFraction of  cell s\nFimbrium\nInner membrane\nNucleoid\nOuter membrane\nCytoplasm\nFlagellum\nPeriplasm\nE xtracel l u lar\nV a r i d n a v i r i a G r een n on-sul fur bac t e ria\nMeth a n o micr obiale s\nC ilia t e s G r e e n\nPla ntsAnima l s\nF ungi\nFlagellat es\nMicrosporidia\nLuca\nH a l oar cha e a\nMeth a n o bac t erium\nMeth a n o c o c c u s\nT h erm o c o c cus\nT h erm opr o t e u s\nPyr odic tium\nG r am-p osi tiv e  bact e r i a\nPurp l e bact eri a\nC y a nobac t e r i a\nF l a v o bac t eri a  and\nall ies\nT h erm o t oga l e s\nR i b ozy v i r i a\nR i b o v i r i a\nM o n o dna viria\nDupl odna vir i a\nA d n a v iria\nViru s Bact e r i a Archaea E u k a r y a\nFig. 4 | Downstream task networks with three input types and results \ncomparison of eight verification tasks. Based on the embedding matrix, three \ntypes of input in the downstream task are corresponding networks. a, A single \nsequence, including GenusTax, ncRNAFam, ProtLoc and ProtStab. b, Two same-\ntype sequences, including InfA and PPI. c, Two heterogeneous sequences: central \ndogma and ncRPI. d–k, Comparison results of eight downstream tasks (Central \ndogma (d), Genus taxonomy (e), ProtLoc (f), ProtStab (g), ncRANFam (h), InfA (i), \nPPI (j), ncRPI (k)). The Spearman correlation coefficient was used for the ProtStab \nregression task and accuracy was used for other tasks. Comparative methods \ninclude the state of the art, DNABert2-based (for nucleic acids), ESM2-3B-based \n(for proteins) and LucaOne-based. The top right asterisk indicates inference \nusing the trained method, the top right triangle indicates direct use of the results \nin its paper, and the top right circle indicates repetition using its method and is \nbetter than the results in the paper.\nNature Machine Intelligence | Volume 7 | June 2025 | 942–953 948\nArticle https://doi.org/10.1038/s42256-025-01044-4\nDiscussion\nThe attempt to build a universal biological language model is to develop \na sophisticated cataloguing and retrieval system for ‘The Library of \nMendel’—the genetic version of ‘The Library of Babel’28,29. The diversity \nof genetic variations presents a vast ‘design space’ that is arguably as \nrich as the entirety of human literature, if not more so, given the far \nlonger history of life on Earth compared with our record of literature. \nHowever, in stark contrast, the proportion of genetic sequences we \nhave successfully identified and catalogued remains considerably \nsmaller than the volume of documented human languages. Moreover, \nthe growth of our understanding and documentation of this ‘biological \nlanguage’ is unlikely to occur suddenly or rapidly30,31. Our endeavour \nherein offers a computational model that posits the potential to rep-\nresent the paradigm of biological language. However, we must temper \nour expectations regarding this model’s rapid and seamless refinement \ntowards an idealized state of perfection.\nIn developing the LucaOne model, we used deep learning frame-\nworks and techniques from natural language processing. However, we \nobserved systemic discrepancies when applying these models, which \nwere highly successful in natural language contexts, to genomic lan-\nguage32. The architecture of BERT-based pre-trained language mod-\nels focuses on understanding context but may not efficiently capture \nbiological sequences’ unique attributes and characteristics33,34. Fur-\nthermore, the functions and expressions of biological sequences are \ndetermined not solely by their genetic sequences but also by the environ-\nment in which they are expressed—a factor for which there is at present \nno practical modelling approach. Standardized methods for processing \nannotated or phenotypic data are lacking, which can lead to inaccura-\ncies and omissions35,36. Moreover, the continual learning and scalabil-\nity aspects have yet to be fully explored in this study, primarily owing \nto resource constraints. As a result, the complexities of the model’s \nlearning capabilities have not been thoroughly examined at this point, \nhighlighting the primary area of research for the subsequent phase37. In \nterms of application, owing to the diversity of contexts, a robust evalua-\ntion system is absent for generalizability and domain adaptability, with \nsmall, specialized models occasionally outperforming large pre-trained \nmodels in conjunction with downstream tasks in certain areas32,38.\nIn light of these considerations, researchers may need to develop \nspecialized pre-trained models tailored to genomic language to \nimprove encoding and comprehension of biological data, ensuring \nadaptability across a broader spectrum of computational biology tasks. \nPromising directions include architectural innovations in pre-training \nmodels, such as incorporating genetic programming concepts into \nlarge language models39,40. Another avenue is to harmonize multimodal \ndata, encompassing sequences, feature annotations, experimental \nresults, images and phenotypical information to better understand \nbiological systems beyond unsupervised sequence data learning41,42. \nIn addition, employing more transparent algorithms may enhance the \ninterpretability of the model, facilitating better integration with exist-\ning biological research frameworks and model development43,44. Lastly, \ngiven the necessity for pre-trained models to efficiently fine-tune or \napply to downstream tasks, paradigms need to expedite model adapta-\ntion to new tasks and broader application contexts32.\nT o conclude, this paper documents our effort to build a compre-\nhensive large model to represent the intricacies of the biological world. \nThe capabilities demonstrated by LucaOne showed considerable prom-\nise and highlighted several areas that necessitate substantial advance-\nments. Such multimodal pre-trained foundational models, grounded \nin bioinformatics, will prove immensely valuable in accelerating and \nenhancing our comprehension of biological phenomena.\nMethods\nModel architecture\nFigure 1b illustrates the design of LucaOne, which utilizes the \ntransformer-encoder21 architecture with the following enhancements:\n(1) The vocabulary of LucaOne comprises 39 tokens, includ-\ning both nucleotide and amino acid symbols (refer to \n‘Vocabulary’).\n(2) The model employs pre-layer normalization over post-layer \nnormalization, facilitating the training of deeper networks45.\n(3) Rotary position embedding46 is implemented instead of \nabsolute positional encoding, enabling the model to handle \nsequences longer than those seen during training.\n(4) It incorporates mixed training of nucleic acid and protein \nsequences by introducing token-type embeddings, assigning 0 \nfor nucleotides and 1 for amino acids.\n(5) Besides the pre-training masking tasks for nucleic acid and \nprotein sequences, eight semi-supervised pre-training tasks \nhave been implemented based on selected annotation infor-\nmation (refer to ‘Pre-training tasks details’).\nVocabulary\nThe vocabulary of LucaOne consists of 39 tokens. Owing to the unified \ntraining of nucleic acid and protein sequences, the vocabulary includes \n4 nucleotides (‘ A’ , ‘T’ , ‘C’ and ‘G’) of nucleic acid (‘U’ compiled ‘T’ in RNA), \n‘N’ for unknown nucleotides, 20 amino acids of protein (20 uppercase \nletters excluding ‘B’ , ‘J’ , ‘O’ , ‘U’ , ‘X’ and ‘Z’), ‘X’ for unknown amino acids, \n‘O’ for pyrrolysine, ‘U’ for selenocysteine, other 3 letters (‘B’ , ‘J’ and ‘Z’) \nnot used by amino acids, 5 special tokens (‘[PAD]’ , ‘[UNK]’ , ‘[CLS]’ , ‘[SEP]’ \nand ‘[MASK]’), and 3 other ‘ . ’ , ‘-’ and ‘*’). Owing to the amino acid letters \noverlapping with the nucleotide letters, the use of ‘1’ , ‘2’ , ‘3’ , ‘4’ and ‘5’ \ninstead of ‘ A’ , ‘T’ , ‘C’ , ‘G’ and ‘N’ , respectively.\nPre-training data details\nNucleic acid. The nucleic acid was collected from the NCBI RefSeq \ngenome database, involving 297,780 assembly accessions. The molecu-\nlar types included DNA and RNA (Fig. 2a). The DNA sequence, DNA \nselected annotation, RNA sequence and RNA selected annotation were \nobtained from the format files ’genomic.fna’ , ’genomic.gbff’ , ’rna.gbff’ \nand ’rna.fna’ , respectively. Among all pre-training sequences, 70% of \nDNA sequences and 100% of RNA sequences were derived from anno-\ntated genomes, while the remaining unannotated sequences were \nretained to ensure diversity.\nDNA reverse strand: the DNA dataset expanded reverse strand \nsequences with their annotation. A total of 23,095,687 reverse-strand \nDNA sequences were included.\nGenome region types: eight important genome region types in \nnucleic acids were selected, including ‘CDS’ , ‘intron’ , ‘tRNA’ , ‘ncRNA’ , \n‘rRNA’ , ‘miscRNA’ , ‘tmRNA’ and ‘regulatory’ . Each nucleotide in the \nsequence had a label index of 8 categories (0–7) or −100 when it did \nnot belong to these 8 categories.\nOrder-level taxonomy: the order-level label of the taxonomy tree \nwas selected as the classification label of the nucleic acid sequence. \nEach sequence had a label index of 735 categories (0–734) or −100 \nwithout the order-level taxonomy.\nSegmentation: owing to the limited computing resources, each \nnucleic acid sequence was segmented according to a given maximum \nlength. The fragmentation strategy was presented in Supplementary \nFig. 2.\nProtein. Protein sequence data were obtained from UniRef50, UniProt \nand ColabFoldDB. UniRef50 was added to the UniProt database to \nsample high-quality representative sequences, while ColabFoldDB was \nincorporated to enhance the diversity of protein sequences. For Colab-\nFoldDB, redundancy within each cluster was minimized by retaining \nonly the ten most diverse sequences. Duplicated sequences between \nUniProt and ColabFoldDB were excluded. Sequence, taxonomy and \nkeywords were collected from UniProt and ColabFoldDB. The sites, \ndomains and homology regions were extracted from Interpro. The ter-\ntiary structure was derived from RCSB-PDB and AlphaFold2-Swiss-Prot.\nNature Machine Intelligence | Volume 7 | June 2025 | 942–953\n 949\nArticle https://doi.org/10.1038/s42256-025-01044-4\nSequence: the right truncation strategy was applied when the \nsequence exceeded the maximum length.\nOrder-level taxonomy: order-level classification information is \nused as the protein sequence taxonomy. There were 2,196 categories; \neach sequence had a label index (0–2,195) or −100 if its order-level \ninformation was missing.\nSite: four types of site regions (‘active site’ , ‘binding site’ , ‘con-\nserved site’ and ‘PTM’) with 946 categories were included. For each \namino acid in a sequence, if it was a site location, there was a label index \n(0–945); otherwise, it was marked with −100.\nHomology: a homologous superfamily is a group of proteins \nthat share a common evolutionary origin with a sequence region, \nreflected by similarity in their structure. There were 3,442 homolo -\ngous region types; each amino acid in these regions had a label index \n(0–3,441) corresponding to its type, and the other amino acids were \nlabelled −100.\nDomain: domain regions are distinct functional, structural or \nsequence units that may exist in various biological contexts. A total \nof 13,717 domain categories were included; each amino acid in these \nregions had a label index (0–13,716) corresponding to its category, and \nthe other amino acids were marked with −100.\nKeyword: keywords are generated based on functional, structural \nor other protein categories. Each sequence was labelled as a set of label \nindices with 1,179 keywords or −100 without keywords.\nStructure: the spatial coordinates of the Cα atom were used here \nas the amino acid coordinates. Each amino acid was labelled with a \nthree-dimensional coordinate normalized within the protein chain \n(preferentially selected the structure from RCSB-PDB). The amino \nacids at the missing locations were labelled (−100, −100, −100). In \ntotal, only about half a million protein sequences had structural \ninformation.\nPre-training tasks details\nLucaOne has employed a semi-supervised learning approach to \nenhance its applicability in biological language modelling. Bioin -\nformatics analysis often involves different modalities for input and \noutput data, and most downstream tasks extend from understand-\ning nucleic acid or protein sequences, so our pre-training tasks have \nbeen augmented with eight foundational sequence-based annotation \ncategories. These annotations complement the self-supervised mask-\ning task, facilitating more effective learning for improved perfor -\nmance in downstream applications. The selection criteria for these \nannotations focused on universality, lightweight design and high \nconfidence level; consequently, only a subset of the data has such \nannotations. As listed in Supplementary Fig. 3, there are ten specific \npre-training tasks at four levels. All loss functions are presented in \nSupplementary Note 1.\nT oken-level tasks: Gene-Mask and Prot-Mask tasks randomly mask \nnucleotides or amino acids in the sequence following the BERT masking \nscheme47 and predict these masked nucleotides or amino acids based \non the sequence context in training.\nSpan-level tasks: the model is trained to recognize some essential \nregions based on the sequence context. For nucleic acid sequences, \neight essential genome region types are learned. For protein sequences, \nthree types of region are labelled: site, homology and domain regions.\nSequence-level tasks: Gene-Taxonomy, Prot-Taxonomy and \nProt-Keyword are the order-level taxonomies of nucleic acid, protein \nand protein-tagged keywords, respectively. They are all sequence-level \nlearning tasks.\nStructure-level tasks: as the structure of a protein determines its \nfunction, we use a small amount of protein data with a tertiary struc-\nture for simple learning in the pre-training phase. Instead of learning \nthe spatial position at the atomic level, the spatial position of amino \nacids is trained (using the position of the C α atom as the position of \nthe amino acid).\nPre-training information\nOn the dimensions of the embedding, the research conducted by \nElnaggar et al.11 demonstrates that the ESM2-3B (embedding dimen -\nsion 2,560) model outperforms the 650 million (embedding dimen -\nsion 1,280) version, while the 15 billion (embedding dimension:  \n5,120) version does not consistently improve performance and sub -\nstantially increases the computational burden. For the relationship \nbetween model size and training data size, Hoffmann et al. suggest \nthat a minimum of 20.2 billion tokens is essential to adequately train \na 1 B-sized model48.\nThe critical hyperparameters we adopted are as follows: the archi-\ntecture of LucaOne consists of 20 transformer-encoder blocks with 40 \nattention heads each, supports a maximal sequence length of 1,280 and \nfeatures an embedding dimension of 2,560. The model is composed of \na total of 1.8 billion parameters. We employed 10 different pretraining \ntasks, assigning an equal weight of 1.0 to the Gene-Mask, Prot-Mask, \nProt-Keyword and Prot-Structure tasks, while assigning a reduced \nweight of 0.2 to the remaining tasks to equilibrate task complexity (Sup-\nplementary Note 1, equation (11)). We used the AdamW optimizer with \nbetas (0.9, 0.98) and a maximum learning rate of 2 × 10−4, incorporating \na linear warm-up schedule throughout the learning-rate updates. For \nthe model training regimen, we utilized a batch size of 8 coupled with \na gradient accumulation step of 32. The model underwent training on \n8 Nvidia A100 graphics processing units spanning 120 days. A model \ncheckpoint of 5.6 million (5.6 million, trained with 36.95 billion tokens) \nwas selected for the subsequent validation tasks, aligned with ESM2-3B \nin terms of the volume of data trained for comparison.\nT o elucidate the advantages of mixed training involving both \nnucleic acids and proteins, we further conducted experiments with \ntwo supplementary models, LucaOne-Gene and LucaOne-Prot, trained \nexclusively with nucleic acids and proteins, respectively. Their perfor-\nmance in the central dogma of the biology task was evaluated with the \nsame checkpoint (5.6 million) of the two models.\nCheckpoint selection criteria: we have released the 5.6 million \ncheckpoint aligned with the ESM2-3B model for a comparable volume \nof data trained, which was trained with 36.95 billion tokens over 20 \ntimes the model’s parameters. We also released the 17.6 million check-\npoint (trained with 116.62 billion tokens) based on three criteria: (1) the \nloss curve slowly descended after 17.6 million steps during training \n(Extended Data Fig. 3a); (2) the losses are relatively stable on the vali-\ndation and testing set between 15 million and 20 million steps, making \n17.6 million optimal (Extended Data Fig. 3b,c); (3) the improvement in \nthe performance of representative downstream tasks is very limited. \nFor example, in the ncRPI task, the accuracy is 94.93% at checkpoint \n17.6 million, which is only a marginal improvement compared to an \naccuracy of 94.78% at checkpoint 5.6 million (Extended Data Fig. 3d).\nLucaOne embeddings-level analysis\nDetails of t-SNE datasets: the S1 dataset was curated from marine data \navailable in CAMI249, selecting contigs with lengths ranging from 300 \nto 1,500 nucleotides. The contigs of each species were redundant by \nMMSeqs, employing a coverage threshold of 80% and sequence iden-\ntity of 95%, culminating in a collection of 37,895 nucleic acid contigs \nfrom 12 species. We randomly selected 100 samples from each species, \ntotalling 1,200 items for visualization.\nThe S2 dataset originated from clan data within Pfam, maintain-\ning clan categories with a minimum of 100 Pfam entries, resulting in \n189,881 protein sequences across 12 clan categories. For visualization, \nwe randomly selected one sample for each Pfam entry in every clan, \namounting to 2,738 samples.\nThe S3 dataset was selected from the UniProt database from \n1 May 2023 to 16 December 2024, which does not overlap with the \npre-training data of LucaOne (before 29 May 2022). This dataset \nfocused on the lowest-tier GO annotations within the hierarchical \nannotation framework of the biological-processes subset, identifying \nNature Machine Intelligence | Volume 7 | June 2025 | 942–953 950\nArticle https://doi.org/10.1038/s42256-025-01044-4\nthe 12 most prevalent terms at this foundational level. Each GO term \nrandomly samples 100 sequences between 100 and 2,500 amino acids \nin length, resulting in 1,200 protein sequences across the 12 GO terms \n(Supplementary Note 2).\nConvergence of nucleic acid and protein sequences for the same \ngene: we prepared an additional dataset comprising nucleic acid and \nprotein sequences for the same genes and examined their correlations \nsolely on embeddings. The results indicated that, despite nucleic acid \nand protein sequences not being paired during model training, those \ncorresponding to the same gene demonstrated convergence within \nthe LucaOne Embedding Space. More details in Supplementary Note \n6 and Supplementary Fig. 12.\nTask on pseudogene correction: we conducted a mask task predic-\ntion analysis (zero shot) on the data of the true gene (protein coding) \nand pseudogene pairs. The higher pseudogene correction rate and the \ntrue gene recovery rate demonstrated the model’s ability to identify \nthe differences between pseudogenes and functional genes. More \ndetails in Supplementary Note 7, and Supplementary Figs. 13 and 14.\nTask on codon degeneracy: we designed an additional task based \non influenza virus haemagglutinin sequence data to verify whether \nLucaOne can distinguish between synonymous and non-synonymous \nmutations in a zero-shot manner (more details in Supplementary \nFig. 16).\nDetails of central dogma tasks\nDataset construction, original dataset: we devised an experimental task \nto determine whether LucaOne has established the intrinsic associa-\ntion between DNA sequences and their corresponding proteins. A total \nof 8,533 accurate DNA–protein pairs from 13 species were selected in \nthe NCBI RefSeq database, each DNA sequence extending to include \nan additional 100 nucleotides in the 5′ and 3′ contexts, preserving \nintron sequences within the data. In contrast, we generated double the \nnumber of negative samples by implementing substitutions, insertions \nand deletions within the DNA sequences or altering amino acids in the \nprotein sequences to ensure the resultant DNA sequences could not be \naccurately translated into their respective proteins, resulting in a total \nof 25,600 samples—DNA–protein pairs. Then the positive and negative \nsamples were each subjected to random shuffles and subsequently \ndivided into 32 equally sized subsets. Then these subsets were assigned \nto the training, validation and testing sets in a 4:3:25 ratio. For more \ndetails, see Extended Data Table 4 and ‘Data availability’ .\nAnalysis of misclassified samples: we analyse the misidentified \nsamples from two perspectives—sequence and embedding. The rela-\ntionship between sequence identity metrics and the prediction accu-\nracy of the LucaOne embedding is presented in Extended Data Fig. 4a,b. \nData details are presented in Supplementary Note 3. Extended Data \nFig. 4a,b shows that the prediction accuracy of LucaOne for mutated \nsample pairs improved as sequence similarity decreased. We also \nevaluated the embedding distance alterations corresponding to modi-\nfications in nucleic acid and protein sequences by employing mean \npooling to calculate these distances. As illustrated in Extended Data \nFig. 4c,d, greater changes in embedding distances were correlated \nwith improved predictive precision.\nDataset construction, two more species of urochordates: we \nincorporated two species with annotated reference genome urochor-\ndates (referred to as tunicate in the NCBI taxonomy) into our dataset: \nStyela clava (ASM1312258v2, GCF_013122585.1) and Oikopleura dioica \n(OKI2018_ I68_ 1.0, GCA_ 907165135.1). For each of these urochordate \nspecies, 480 genes were randomly selected, and positive gene sam-\nples, nucleic acid negative samples and protein-negative samples \nwere constructed using the same approach as in the original dataset. \nThe same data shuffling and partitioning principles were applied and \nintegrated with the original dataset to retrain the central dogma model. \nData details and model performance are presented in Extended Data \nTable 3, Extended Data Fig. 5 and ‘Data availability’ .\nComparative performance analysis: upon integrating two addi -\ntional urochordate species data, dataset version 2 as the model showed \nperformance comparable to the original dataset models across all \nspecies except C. intestinalis. In particular, the F1 score for C. intesti -\nnalis improved significantly, despite the nearly unchanged accuracy. \nThese findings suggest that supplementing the dataset with species \nthat utilize a codon code similar to C. intestinalis enhances the model’s \nsensitivity to DNA–protein correlations in these organisms while pre-\nserving its sensitivity to DNA–protein correlations in species adhering \nto the standard codon code. For more details, see Extended Data Table 3 \nand ‘Data availability’ .\nCDS–protein task: in the current NCBI RefSeq database, genomes \nwith complete intron annotations are limited, and the accuracy of \nintron predictions from alternative tools may directly impact model \nperformance. T o mitigate these challenges, coding sequence (CDS) \nregions corresponding to genes in the original dataset were extracted \nas intron-free nucleic acid sequences to perform the same task. See \nSupplementary Notes 4 for data details and Fig. 3d for analysis.\nTask for cross-species homologous gene pairs: we designed an \nadditional task related to the central dogma by modifying the nega -\ntive samples in the original study. Instead of manually altering the \nsequences, the negative samples were replaced with homologous \ngenes from closely related species. Please refer to Supplementary \nNotes 5 for details.\nDownstream tasks details\nGenus taxonomy annotation (GenusTax): this task is to predict which \ngenus (taxonomy) the nucleic acid fragment may come from, which is \nvery important in metagenomic analysis. A comparative dataset was \nconstructed utilizing NCBI RefSeq, comprising 10,000 nucleic acid \nsequences, each extending 1,500 nucleotides and annotated with labels \ncorresponding to 157 distinct genera (distributed as 33, 50, 29 and 45 \nacross the four kingdoms: Archaea, Bacteria, Eukaryota and Viruses, \nrespectively). The dataset was randomly segregated into training, \nvalidation and testing sets, adhering to an 8:1:1 partitioning ratio. It \nis important to note that while the LucaOne pre-training task utilized \ntaxonomy annotations at the order level, the current task employs more \ngranular genus-level annotations, thereby preventing label informa-\ntion contamination. This dataset was also employed for two additional \nanalyses: predicting the taxonomy of sequences at the superkingdom \nand species levels. The details are presented in Extended Data Table 5.\nProkaryotic protein subcellular location (ProtLoc): this task is \nto predict the subcellular localization of proteins within prokary -\notic cells, which has garnered substantial attention in proteomics \ndue to its critical role50. It involves classifying proteins into one of six \nsubcellular compartments: the cytoplasm, cytoplasmic membrane, \nperiplasm, outer membrane, cell wall and surface, and extracellular \nspace. Our approach adopted the same dataset partitioning strategy \nas DeepLocPro26, a model based on experimentally verified data from \nthe UniProt and PSORT db databases. For this dataset, we additionally \ndesigned a task based on the corresponding nucleic acid embeddings of \nthe proteins. The result showed that embeddings derived from nucleic \nacid sequences are applicable to the task related to their corresponding \nprotein sequences. The dataset and analytical results are provided in \nSupplementary Notes 8.\nProtein stability (ProtStab): the evaluation of protein stability is \nparamount for elucidating the structural and functional characteristics \nof proteins, which aids in revealing the mechanisms through which \nproteins maintain their functionality in vivo and the circumstances \nthat predispose them to denaturation or deleterious aggregation. \nWe utilized the same dataset from TAPE51, which includes a range of \nde novo-designed proteins, natural proteins, mutants and their respec-\ntive stability measurements. It is a regression task; each protein input \n(x) correlates with a numerical label (y ∈ ℜ), quantifying the protein’s \nintrinsic stability.\nNature Machine Intelligence | Volume 7 | June 2025 | 942–953\n 951\nArticle https://doi.org/10.1038/s42256-025-01044-4\nNon-coding RNA family (ncRNAFam): ncRNA represents gene \nsequences that do not code for proteins but have essential functional \nand biological roles. The objective is to assign ncRNA sequences to their \nrespective families based on their characteristics. For this purpose, \nwe utilize the dataset from the nRC52, which is consistent with the data \nemployed in the RNAGCN53 study. Our methodology adheres to the \nsame data partitioning into training, validation and testing sets as done \nin these previous studies, enabling direct comparison of results. This \nproject involves a multi-class classification challenge that encompasses \n88 distinct categories.\nInfluenza A antigenic relationship prediction (InfA): one of the \nforemost tasks in influenza vaccine strain selection is monitoring \nhaemagglutinin variant emergence, which induces changes in the \nvirus’s antigenicity. Precisely predicting antigenic responses to novel \ninfluenza strains is crucial for developing effective vaccines and pre-\nventing outbreaks. The study utilizes data from the PREDAC54 project \nto inform vaccine strain recommendations. Each data pair in this study \ncomprises two RNA sequences of the haemagglutinin fragment from \ndistinct influenza strains, accompanied by corresponding antigenic \nrelationship data. The objective is framed as a binary classification task \nidentifying the antigenic similarity or difference between virus pairs.\nProtein–protein interaction (PPI): the forecasting of PPI networks \nrepresents a significant area of research interest. Our study utilized \nthe DeepPPI55 database, whose positive dataset samples were sourced \nfrom the Human Protein Reference Database after excluding redundant \ninteractions, leaving 36,630 unique pairs. This dataset was randomly \npartitioned into three subsets: training (80%), validation (10%) and \ntesting (10%). The primary objective of this research is to perform \nbinary classification of PPI sequences.\nncRNA–protein interactions (ncRPIs): an increasing number of \nfunctional ncRNAs, such as snRNAs, snoRNAs, miRNAs and lncRNAs, \nhave been discovered. ncRNAs have a crucial role in many biological \nprocesses. Experimentally identifying ncRPIs is typically expensive and \ntime-consuming. Consequently, numerous computational methods \nhave been developed as alternative approaches. For comparison, we \nhave utilized the same dataset as the currently best-performing study, \nncRPI-LGAT27. It is a binary classification task involving pairs of sequences.\nComparison result details\nWe conducted a series of comparative experiments. According to Fig. 4, \nfor all embedding methods, we compare whether the transformer \nencoder and two pooling strategies (max pooling and value-level atten-\ntion pooling) were used on the model. At the hyperparameter level, we \ncompared the number of encoder layers with the number of heads (4 \nlayers with 8 heads and 2 layers with 4 heads), the peak learning rate \nof the Warmup strategy (1 × 10−4 and 2 × 10−4), and the batch size (8 and \n16). Extended Data Table 5 shows the result of comparing whether the \nencoder was used and which pooling method was used accordingly, and \nSupplementary Fig. 4 shows more metrics on comparison experiments.\nIn the ProtLoc task, LucaOne’s accuracy is very close to that of the \nESM2-3B. In the ncRPI task, the accuracy of the simple network with \nLucaOne’s embedding matrix is less than that of ncRPI-LGAT 27 but \nhigher than that of DNABert2 + ESM2-3B. In the other five tasks, we \nachieved the best results. It is better not to use an encoder for ProtLoc, \nInfA, PPI and ncRPI tasks. Using the max pooling strategy straightfor-\nwardly for the ncRNAFam and GenusTax tasks can obtain better results. \nWe extended 2 tasks, 4 superkingdoms and 180 species prediction \ntasks for the genus classification task with the same sequence data. \nLucaOne’s accuracy improved by 0.1 and 0.054, respectively. In particu-\nlar, LucaOne is more effective than other large models in embedding \nsequences without an encoder.\nComputational resources\nThe data processing and training operations for LucaOne were carried \nout on Alibaba Cloud Computing. In addition, several tasks related to \nfurther processing or downstream computing were performed on alter-\nnative computing platforms, including Yunqi Academy of Engineering \n(Hangzhou, China) and Zhejiang Lab (Hangzhou, China).\nReporting summary\nFurther information on research design is available in the Nature \nPortfolio Reporting Summary linked to this article.\nData availability\nThe pre-training dataset of LucaOne has been deposited into CNGB \nSequence Archive (CNSA)56 with accession number CNP0007266. The \ndatasets of all downstream tasks and other supplementary materials \nare available at https://doi.org/10.5281/zenodo.15171943 (ref. 57).\nCode availability\nThe model code of LucaOne is available at https://github.com/LucaOne/\nLucaOne. The embedding inference code is available at https://github.\ncom/LucaOne/LucaOneApp and the downstream tasks are available at \nhttps://github.com/LucaOne/LucaOneTasks. The trained checkpoint \nfiles and an archived version of the above mentioned code repositories \nare available at https://doi.org/10.5281/zenodo.15171943 (ref. 57).\nReferences\n1. Crick, F. et al. General nature of the genetic code for proteins. \nNature 192, 1227–1232 (1961).\n2. Searls, D. B. The language of genes. Nature 420, 211–217 (2002).\n3. Darwin, C. The Descent of Man, and Selection in Relation to Sex \nVol. 1. (John Murray, 1871).\n4. Gimona, M. Protein linguistics—a grammar for modular protein \nassembly? Nat. Rev. Mol. Cell Biol. 7, 68–73 (2006).\n5. Barbieri, M. The Organic Codes An Introduction to Semantic \nBiology (Cambridge Univ. Press, 2002).\n6. Pinker, S. The Language Instinct (William Morrow, 1994).\n7. Simon, E., Swanson, K. & Zou, J. Language models for biological \nresearch: a primer. Nat. Methods 21, 1422–1429 (2024).\n8. Elnaggar, A. et al. ProtTrans: toward understanding the language \nof life through self-supervised learning. IEEE Trans. Pattern Anal. \nMach. Intell. 44, 7112–7127 (2021).\n9. Brandes, N., Ofer, D., Peleg, Y., Rappoport, N. & Linial, M. \nProteinBERT: a universal deep-learning model of protein \nsequence and function. Bioinformatics 38, 2102–2110 (2022).\n10. Lin, Z. et al. Evolutionary-scale prediction of atomic-level  \nprotein structure with a language model. Science 379, 1123–1130 \n(2023).\n11. Elnaggar, A. et al. Ankh: optimized protein language model \nunlocks general-purpose modelling. Preprint at https://arxiv.org/\nabs/2301.06568 (2023).\n12. Baek, M. et al. Accurate prediction of protein structures and \ninteractions using a three-track neural network. Science 373, \n871–876 (2021).\n13. Hou, X. et al. Using artificial intelligence to document the hidden \nRNA virosphere. Cell 187, 6929–6942 (2024).\n14. Yu, T. et al. Enzyme function prediction using contrastive learning. \nScience 379, 1358–1363 (2023).\n15. Zhou, Z. et al. DNABert-2: efficient foundation model and \nbenchmark for multi-species genome. In 12th International \nConference on Learning Representations (ICLR, 2024).\n16. Nguyen, E. et al. HyenaDNA: long-range genomic sequence \nmodeling at single nucleotide resolution. Adv. Neural. Inf. Process. \nSyst. 36, 43177–43201 (2023).\n17. Yang, F. et al. ScBERT as a large-scale pretrained deep language \nmodel for cell type annotation of single-cell RNA-seq data.  \nNat. Mach. Intell. 4, 852–866 (2022).\n18. Nguyen, E. et al. Sequence modeling and design from molecular \nto genome scale with evo. Science 386, eado9336 (2024).\nNature Machine Intelligence | Volume 7 | June 2025 | 942–953 952\nArticle https://doi.org/10.1038/s42256-025-01044-4\n19. Li, Q. et al. Progress and opportunities of foundation models in \nbioinformatics. Brief. Bioinform. 25, bbae548 (2024).\n20. Crick, F. Central dogma of molecular biology. Nature 227, \n561–563 (1970).\n21. Vaswani, A. et al. Attention is all you need. Adv. Neural. Inf. \nProcess. Syst. 30, 6000–6010 (2017).\n22. Koonin, E. V. Why the central dogma: on the nature of the great \nbiological exclusion principle. Biol. Direct 10, 1–5 (2015).\n23. Yockey, H. P. Information theory, evolution and the origin of life. \nInf. Sci. 141, 219–225 (2002).\n24. He, Y. et al. KG-MTT-BERT: knowledge graph enhanced bert for \nmulti-type medical text classification. Preprint at https://arxiv.org/\nabs/2210.03970 (2022).\n25. Delsuc, F., Brinkmann, H., Chourrout, D. & Philippe, H. Tunicates \nand not cephalochordates are the closest living relatives of \nvertebrates. Nature 439, 965–968 (2006).\n26. Moreno, J., Nielsen, H., Winther, O. & Teufel, F. Predicting the \nsubcellular location of prokaryotic proteins with DeepLocPro. \nBioinformatics 40, btae677 (2024).\n27. Han, Y. & Zhang, S.-W. ncRPI-LGAT: prediction of ncRNA–protein \ninteractions with line graph attention network framework. \nComput. Struct. Biotechnol. J. 21, 2286–2295 (2023).\n28. Robbins, J. W. Darwin’s Dangerous Idea: Evolution and the \nMeanings of Life (JSTOR, 1996).\n29. Chomsky, N. Three factors in language design. Linguist. Inq. 36, \n1–22 (2005).\n30. Touvron, H. et al. Llama: open and efficient foundation language \nmodels. Preprint at https://arxiv.org/abs/2302.13971 (2023).\n31. Liu, J. et al. Large language models in bioinformatics: applications \nand perspectives. Preprint at https://arxiv.org/abs/2401.04155v1 \n(2024).\n32. Sapoval, N. et al. Current progress and open challenges for \napplying deep learning across the biosciences. Nat. Commun. 13, \n1728 (2022).\n33. Vig, J. et al. BERTology meets biology: interpreting attention \nin protein language models. Preprint at https://arxiv.org/\nabs/2006.15222 (2020)\n34. Avsec, Ž. et al. Effective gene expression prediction from \nsequence by integrating long-range interactions. Nat. Methods \n18, 1196–1203 (2021).\n35. Nakano, F. K., Lietaert, M. & Vens, C. Machine learning for \ndiscovering missing or wrong protein function annotations: a \ncomparison using updated benchmark datasets. BMC Bioinform. \n20, 485 (2019).\n36. Alharbi, W. S. & Rashid, M. A review of deep learning applications \nin human genomics using next-generation sequencing data. \nHum. Genomics 16, 26 (2022).\n37. Kaplan, J. et al. Scaling laws for neural language models. Preprint \nat https://arxiv.org/abs/2001.08361 (2020).\n38. Whalen, S., Schreiber, J., Noble, W. S. & Pollard, K. S. Navigating \nthe pitfalls of applying machine learning in genomics. Nat. Rev. \nGenet. 23, 169–181 (2022).\n39. Banzhaf, W., Machado, P. & Zhang, M. (eds) Handbook of \nEvolutionary Machine Learning Genetic and Evolutionary \nComputation (Springer, 2024).\n40. Blanchard, A. E. et al. Automating genetic algorithm mutations \nfor molecules using a masked language model. IEEE Trans. Evol. \nComput. 26, 793–799 (2022).\n41. Ebrahim, A. et al. Multi-omic data integration enables discovery of \nhidden biological regularities. Nat. Commun. 7, 13091 (2016).\n42. Vahabi, N. & Michailidis, G. Unsupervised multi-omics data \nintegration methods: a comprehensive review. Front. Genet. 13, \n854752 (2022).\n43. Han, H. & Liu, X. The challenges of explainable AI in biomedical \ndata science. BMC Bioinform. 22, 443 (2022).\n44. Holzinger, A., Langs, G., Denk, H., Zatloukal, K. & Müller, H. \nCausability and explainability of artificial intelligence in medicine. \nWiley Interdiscip. Rev. Data Min. Knowl. Discov. 9, 1312 (2019).\n45. Xiong, R. et al. On layer normalization in the transformer \narchitecture. In International Conference on Machine Learning \n10524–10533 (PMLR, 2020).\n46. Su, J. et al. RoFormer: enhanced transformer with rotary position \nembedding. Neurocomputing 568, 127063 (2024).\n47. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, L. K. BERT: \npre-training of deep bidirectional transformers for language \nunderstanding. In Advances in North American Chapter of the \nAssociation for Computational Linguistics: Human Language \nTechnologies, 4171–4186 (Association for Computational \nLinguistics, 2019).\n48. Hoffmann, J. et al. Training compute-optimal large language \nmodels. In Proc. 36th International Conference on Neural \nInformation Processing Systems, 30016–30030 (NeurIPS, 2022).\n49. Meyer, F. et al. Critical assessment of metagenome interpretation: \nthe second round of challenges. Nat. Methods 19, 429–440 \n(2022).\n50. Xu, Q., Hu, D. H., Xue, H., Yu, W. & Yang, Q. Semi-supervised \nprotein subcellular localization. BMC Bioinform. 10, S47 (2009).\n51. Rao, R. et al. Evaluating protein transfer learning with tape.  \nAdv. Neural. Inf. Process. Syst. 32, 9689–9701 (2019).\n52. Fiannaca, A., La Rosa, M., La Paglia, L., Rizzo, R. & Urso, A. nRC: \nnon-coding RNA classifier based on structural features.  \nBiodata Min. 10, 27 (2017).\n53. Rossi, E., Monti, F., Bronstein, M. & Liò, P. ncRNA classification with \ngraph convolutional networks. In Proc. 1st International Workshop \non Deep Learning on Graphs: Methods and Applications  \n(DLG, 2019).\n54. Du, X. et al. Mapping of H3N2 influenza antigenic evolution in \nChina reveals a strategy for vaccine strain recommendation.  \nNat. Commun. 3, 709 (2012).\n55. Sun, T., Zhou, B., Lai, L. & Pei, J. Sequence-based prediction of \nprotein-protein interaction using a deep-learning algorithm.  \nBMC Bioinform. 18, 277 (2017).\n56. Wang, W. et al. The China National Genebank Sequence Archive \n(CNSA) 2024 update. Hortic. Res. 12, 036 (2025).\n57. He, Y. Generalized biological foundation model with \nunified nucleic acid and protein language. Zenodo 10.5281/\nzenodo.15171943 (2025).\n58. Mock, F., Kretschmer, F., Kriese, A., Böcker, S. & Marz, M. \nTaxonomic classification of DNA sequences beyond sequence \nsimilarity using deep neural networks. Proc. Natl Acad. Sci. USA \n119, e2122636119 (2022).\nAcknowledgements\nThis work was supported by the National Natural Science Foundation \nof China (82341118). M.S. is funded by the Shenzhen Science and \nTechnology Program (KQTD20200820145822023), the Guangdong \nProvince ‘Pearl River Talent Plan’ Innovation and Entrepreneurship \nTeam project (2019ZT08Y464), and the Guangzhou National \nLaboratory Major Project (GZNL2023A01001). Y.P. is funded by the \nNational Natural Science Foundation of China (NSFC) Basic Research \nProject for Doctoral Students (grant number 323B2018). We thank J. \nWang, D.-C. Ma and D.-Z. Shi for computational resource coordination. \nWe thank H.-W. Zhang for maintaining computational resources and \noptimizing specific computing tasks at Yunqi Academy of Engineering \n(Hangzhou). We thank Y.-Q. Liu and M. Zhou for their participation in \nthe subsequent technical validation in conjunction with this research. \nWe thank X.-J. Du, W.-C. Wu, J.-Y. Yang and S.-Q. Mei from Sun Yat-sen \nUniversity (Shenzhen) for a valuable conversation on the development \nof the method, especially on understanding the downstream tasks. \nWe thank C. Darwin, R. Dawkins, S. Pinker and D. Dennett for their \nNature Machine Intelligence | Volume 7 | June 2025 | 942–953\n 953\nArticle https://doi.org/10.1038/s42256-025-01044-4\nprofound insights that led to the early conceptual foundations of this \nstudy and guided its development pathway.\nAuthor contributions\nConceptualization: Y.H., Z.L. and M. S. Model development and \ndata preparation for LucaOne: Y.H., Y.W., Y.P. and Yichang Chen. \nDownstream tasks understanding and models training: Y.H., P.F.,  \nY. Shan and Yihao Chen. Original draft: Y.H., Z.L. and P.F. Writing—\nreview and editing: all authors. Graphic presentation design: Y. L. and \nY.H. Engineering leadership and resource acquisition: Z. Zeng and J.Y. \nScience leadership and resource acquisition: J.L., E.C.H., Z. Zhou, F.Z. \nand Y. Shu. Supervision: Y.H., M.S. and Z.-L.\nCompeting interests\nY.H., Z.L., P. F. and J.Y. have filed an application for a patent covering \nthe work presented. The other authors declare no competing interests.\nAdditional information\nExtended data is available for this paper at  \nhttps://doi.org/10.1038/s42256-025-01044-4.\nSupplementary information The online version contains supplementary \nmaterial available at https://doi.org/10.1038/s42256-025-01044-4.\nCorrespondence and requests for materials should be addressed to \nYong He, Mang Shi or Zhaorong Li.\nPeer review information Nature Machine Intelligence thanks \nanonymous reviewer(s) for their contribution to the peer review of  \nthis work.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard  \nto jurisdictional claims in published maps and institutional  \naffiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\n© The Author(s) 2025\n1Apsara Lab, Alibaba Cloud Intelligence, Alibaba Group, Hangzhou, China. 2Centre for Infection and Immunity Study (CIIS), School of Medicine \n(Shenzhen),  Sun Yat-sen University, Shenzhen, China. 3Ministry of Education Key Laboratory of Biodiversity Science and Ecological Engineering, School \nof Life Sciences, Fudan University, Shanghai, China. 4Alibaba Health Information Technology, Alibaba Group, Hangzhou, China. 5State Key Laboratory \nof Advanced Drug Delivery and Release Systems and Innovation Institute for AI in Medicine, College of Pharmaceutical Sciences, Zhejiang University, \nHangzhou, China. 6Institute of Pathogen Biology, Chinese Academy of Medical Sciences and Peking Union Medical College, Beijing, China. 7College \nof Pharmaceutical Sciences, The Second Affiliated Hospital, Zhejiang University School of Medicine, Zhejiang University, Hangzhou, China. 8Sydney \nInstitute for Infectious Diseases, School of Medical Sciences, The University of Sydney, Sydney, New South Wales, Australia. 9Department of Infectious \nDiseases and Public Health, Jockey Club College of Veterinary 33 Medicine and Life Sciences, City University of Hong Kong, Hong Kong, China. 10Key \nLaboratory of Pathogen Infection Prevention and Control, Peking Union Medical College, Ministry of Education, Beijing, China. 11School of Public Health \n(Shenzhen), Sun Yat-sen University, Shenzhen, China. 12State Key Laboratory for Biocontrol, Centre for Infection and Immunity Studies, School of \nMedicine, Sun Yat-sen University, Shenzhen, China.  e-mail: sanyuan.hy@alibaba-inc.com; shim23@mail.sysu.edu.cn; lzr098@gmail.com\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-025-01044-4\nExtended Data Fig. 1 | Overall statistics on pre-training data of LucaOne. \na. Sequences (DNA, RNA, and proteins) were derived from RefSeq, UniProt, \nColabFoldDB, and UniRef50. b. The data (nucleic acids and proteins) involved \nfour superkingdom types: Viruses, Archaea, Eukarya, and Bacteria, of which \nBacteria accounted for the most. c. The sequence length distribution of nucleic \nacids, with the most being more than 1,000. d. The sequence length distribution \nof proteins, with the maximum length ratio between 100 and 1,000. e. The \nproportion of five nucleotides (’ A’ , ’T’ , ’C’ , ’G’ , and ’Unknown’) in nucleic acid \nsequences (’U’ compiled with ’T’ in RNA) and the four identified nucleotides were \nclose in proportion. f. The proportion of the 20 standard amino acid letters and \nfive other letters (including four non-standard amino acids and ’X’ for unknown \namino acid) in the protein sequence, and Leucine has the highest proportion.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-025-01044-4\nExtended Data Fig. 2 | Annotation statistics on pre-training data of LucaOne.  \na. The proportion of genome region types and order-level taxonomy in nucleic \nacid. Most sequences have both types of annotation information. b. The \nproportion of the count of sequences with each of the selected six annotations, \nincluding order-level taxonomy, keyword, site, domain, homology, and tertiary \nstructure, of which the proportion of sequence count with tertiary structure \nis tiny. c. and d. The proportion of sequence counts in the top 10 phylum-level \ntaxonomy of nucleic acids and proteins, respectively. e. The distribution of \neight selected genome region types in nucleic acids, of which the CDS region is \nthe most. f. and g. The proportion of sequence counts in the top 10 order-level \ntaxonomy (total 2,196 categories) of nucleic acids and proteins, respectively. \nh–k. The proportion of protein sequence counts in the top 10 keywords (total \n1,179 categories), the top 10 site types (total 946 categories), the top 10 domain \ntypes (total 13,717 categories), and the top 10 homology types (total 3,442 \ncategories), respectively. l. The coord-(x, y, z) distribution of Cα-atom position \n(local normalization within a protein chain). It is very similar to the normal \ndistribution. The distribution has a long tail in c–f. The distribution is ladder \ndecreasing in g–k.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-025-01044-4\nExtended Data Fig. 3 | LucaOne 17.6M checkpoint selection criteria. a. The loss trend during training. b. and c. The trend of loss on the validation and testing sets.  \nd. Performance evaluation between 5.6M and 17.6M checkpoints on downstream tasks - ncRPI.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-025-01044-4\nExtended Data Fig. 4 | Identity between positive and negative samples and \nprediction accuracy on the Central Dogma task. a. and b. The relationship \nbetween sequence identity metrics and LucaOne model prediction accuracy: \nNCBI blastn sequence identity for nucleic acid and protein sequences before and \nafter mutation. c. and d. Embedding Euclidean distances based on mean pooling \nand their prediction accuracy in LucaOne for nucleic acid and protein sequences \nbefore and after mutation. Upper panels: Sample distributions across sequence \nsimilarity, change ratio, or embedding Euclidean distance ranges. Lower \npanels: Prediction counts and accuracy of the LucaOne embedding within each \nrespective range. Note: Data for a. and b. includes all nucleic acid and protein-\nnegative samples from the validation and testing sets. Data for c. and d. includes \nonly positive-negative sample pairs that are both present in the combined \nvalidation and testing datasets. Divide the statistical intervals of the metrics into \nquartiles according to the data distribution.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-025-01044-4\nExtended Data Fig. 5 | Codon usage heatmap. Based on the dataset for 15 species - Original Dataset plus two urochordate species. The distribution of different  \ncodons for a single amino acid totals 100%. Coloured representations indicate the relative proportions, where red signifies higher proportions and green signifies \nlower proportions.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-025-01044-4\nExtended Data Table 1 | Embedding-based clustering metrics on three datasets\nThe clustering scores (using K-Means++) of the four embedding methods on the S1, S2, and S3 datasets.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-025-01044-4\nExtended Data Table 2 | Performance comparison for LucaOne and other embedding tools on the Central Dogma task\nLucaOne was not only compared with several existing embedding methods but also with itself, which was trained using nucleic acids and proteins separately (LucaOne-Gene/LucaOne-Prot). \nLucaOne, with mixing training, obtained the best performance for both the original dataset and the CDS-Protein dataset.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-025-01044-4\nExtended Data Table 3 | Comparative performance analysis on the Central Dogma task\nComparative performance analysis (validation and testing set) of the models across diverse species datasets (Sample counts in brackets). The original dataset includes 13 species, and the \nnew dataset adds two more urochordate species data. F1-score and accuracy are calculated and presented. The top right * indicates the predictive performances of the model trained by the \noriginal version of the Central Dogma dataset (w/o Oikopleura Dioica and Styela Clava data). More details in Data Availability.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-025-01044-4\nExtended Data Table 4 | Details on downstream validation tasks\nDetails of 10 downstream tasks, including task name, task type, input type of task, sample number of training set, validation set, test set, and sequence length statistics of each task.\nNature Machine Intelligence\nArticle https://doi.org/10.1038/s42256-025-01044-4\nExtended Data Table 5 | Detailed results of the testing set on downstream validation tasks (results of the better pooling \nmethod for each task with or without encoder)\nThe top right ⋆ indicates inference using the trained method, the top right ▴ indicates direct use of the results in its paper, and the top right • indicates repetition using its method and higher \nthan the results in the paper. BERTax from ref. 58.\n\n\n\n\n\n\n",
  "topic": "Limiting",
  "concepts": [
    {
      "name": "Limiting",
      "score": 0.6237524747848511
    },
    {
      "name": "Computer science",
      "score": 0.5943726301193237
    },
    {
      "name": "Foundation (evidence)",
      "score": 0.5496722459793091
    },
    {
      "name": "Nucleic acid",
      "score": 0.528735876083374
    },
    {
      "name": "Computational biology",
      "score": 0.476727157831192
    },
    {
      "name": "Biological data",
      "score": 0.4606151580810547
    },
    {
      "name": "RNA",
      "score": 0.43508023023605347
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4223966598510742
    },
    {
      "name": "Biology",
      "score": 0.31085431575775146
    },
    {
      "name": "Bioinformatics",
      "score": 0.23129495978355408
    },
    {
      "name": "Engineering",
      "score": 0.11729344725608826
    },
    {
      "name": "Genetics",
      "score": 0.11584880948066711
    },
    {
      "name": "Gene",
      "score": 0.10653510689735413
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": []
}