{
    "title": "Pre-trained Language Model for Biomedical Question Answering",
    "url": "https://openalex.org/W2974058876",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2744018581",
            "name": "Yoon, Wonjin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2229345596",
            "name": "Lee, Jinhyuk",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2485547936",
            "name": "Kim Dong-Hyeon",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221644252",
            "name": "Jeong, Minbyul",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2258364691",
            "name": "Kang, Jaewoo",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2510959134",
        "https://openalex.org/W2922551710",
        "https://openalex.org/W2963323070",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2527896214",
        "https://openalex.org/W2158139315",
        "https://openalex.org/W2948909602",
        "https://openalex.org/W2809349863",
        "https://openalex.org/W2731106650",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2913962323",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W1981208470",
        "https://openalex.org/W2626667877",
        "https://openalex.org/W2951873305",
        "https://openalex.org/W2740815822",
        "https://openalex.org/W3105491236",
        "https://openalex.org/W2963716420",
        "https://openalex.org/W2898355739",
        "https://openalex.org/W2403393286",
        "https://openalex.org/W3080121084",
        "https://openalex.org/W2956155414",
        "https://openalex.org/W2953868496",
        "https://openalex.org/W2510358553",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2973154071",
        "https://openalex.org/W2427527485",
        "https://openalex.org/W2962739339"
    ],
    "abstract": "The recent success of question answering systems is largely attributed to pre-trained language models. However, as language models are mostly pre-trained on general domain corpora such as Wikipedia, they often have difficulty in understanding biomedical questions. In this paper, we investigate the performance of BioBERT, a pre-trained biomedical language model, in answering biomedical questions including factoid, list, and yes/no type questions. BioBERT uses almost the same structure across various question types and achieved the best performance in the 7th BioASQ Challenge (Task 7b, Phase B). BioBERT pre-trained on SQuAD or SQuAD 2.0 easily outperformed previous state-of-the-art models. BioBERT obtains the best performance when it uses the appropriate pre-/post-processing strategies for questions, passages, and answers.",
    "full_text": "arXiv:1909.08229v1  [cs.CL]  18 Sep 2019\nPre-trained Language Model for\nBiomedical Question Answering\nWonjin Yoon, Jinhyuk Lee, Donghyeon Kim,\nMinbyul Jeong, and Jaewoo Kang ⋆\nKorea University, Seoul, Korea\n{wjyoon, jinhyuk lee, donghyeon, minbyuljeong, kangj}@korea.ac.kr\nAbstract. The recent success of question answering systems is largely\nattributed to pre-trained language models. However, as language models\nare mostly pre-trained on general domain corpora such as Wik ipedia,\nthey often have diﬃculty in understanding biomedical quest ions. In this\npaper, we investigate the performance of BioBERT, a pre-trained biomed-\nical language model, in answering biomedical questions including factoid,\nlist, and yes/no type questions. BioBERT uses almost the sam e struc-\nture across various question types and achieved the best per formance in\nthe 7th BioASQ Challenge (Task 7b, Phase B). BioBERT pre-tra ined\non SQuAD or SQuAD 2.0 easily outperformed previous state-of -the-\nart models. BioBERT obtains the best performance when it use s the\nappropriate pre-/post-processing strategies for questio ns, passages, and\nanswers.\nKeywords: Biomedical Question Answering·Pre-trained Language Model\n· Transfer Learning\n1 Introduction\nLanguage models pre-trained on large-scale text corpora achieve state-of-the-art\nperformance in various natural language processing (NLP) tasks when ﬁne-tuned\non a given task [\n4,13,15]. Language models have been shown to be highly eﬀective\nin question answering (QA), and many current state-of-the-art QA models often\nrely on pre-trained language models [ 20]. However, as language models are mostly\npre-trained on general domain corpora, they cannot be generaliz ed to biomedical\ncorpora [ 1,2,8,29]. Hence, similar to using Word2Vec for the biomedical domain\n[14], a language model pre-trained on biomedical corpora is needed for building\neﬀective biomedical QA models.\nRecently, Lee et al. [ 8] have proposed BioBERT which is a pre-trained lan-\nguage model trained on PubMed articles. In three representative biomedical\nNLP (bioNLP) tasks including biomedical named entity recognition, re lation\nextraction, and question answering, BioBERT outperforms most o f the previ-\nous state-of-the-art models. In previous works, models were us ed for a speciﬁc\n⋆ To whom correspondence should be addressed.\n2 W. Yoon et al.\nbioNLP task [\n9, 18, 24, 28]. However, the structure of BioBERT allows a single\nmodel to be trained on diﬀerent datasets and used for various tas ks with slight\nmodiﬁcations in the last layer.\nIn this paper, we investigate the eﬀectiveness of BioBERT in biomedic al\nquestion answering and report our results from the 7th BioASQ Cha llenge [ 7,10,\n11, 21]. Biomedical question answering has its own unique challenges. First, the\nsize of datasets is often very small (e.g., few thousands of samples in BioASQ) as\nthe creation of biomedical question answering datasets is very exp ensive. Second,\nthere are various types of questions including factoid, list, and yes /no questions,\nwhich increase the complexity of the problem.\nWe leverage BioBERT to address these issues. To mitigate the small s ize\nof datasets, we ﬁrst ﬁne-tune BioBERT on other large-scale extr active question\nanswering datasets, and then ﬁne-tune it on BioASQ datasets. Mo re speciﬁcally,\nwe train BioBERT on SQuAD [ 17] and SQuAD 2.0 [ 16] for transfer learning.\nAlso, we modify the last layer of BioBERT so that it can be trained/tes ted on\nthree diﬀerent types of BioASQ questions. This signiﬁcantly reduce s the cost of\nusing biomedical question answering systems as the structure of B ioBERT does\nnot need to be modiﬁed based on the type of question.\nThe contributions of our paper are three fold: 1) We show that BioB ERT pre-\ntrained on general domain question answering corpora such as SQu AD largely\nimproves the performance of biomedical question answering models . Wiese et\nal. [ 25] showed that pre-training on SQuAD helps improve performance. W e test\nthe performance of BioBERT pre-trained on both SQuAD and SQuAD 2.0. 2)\nWith only simple modiﬁcations, BioBERT can be used for various biomedic al\nquestion types including factoid, list, and yes/no questions. BioBER T achieves\nthe overall best performance on all ﬁve test batches of BioASQ 7b Phase B 1,\nand achieves state-of-the-art performance in BioASQ 6b Phase B . 3) We further\nanalyze the role of pre- and post-processing in our system and sho w that diﬀerent\nstrategies often lead to diﬀerent results.\nThe rest of our paper is organized as follows. First, we introduce ou r system\nbased on BioBERT. We describe task-speciﬁc layers of our system a nd vari-\nous pre- and post-processing strategies. We present the result s of BioBERT on\nBioASQ 7b (Phase B), which were obtained using two diﬀerent transf er learning\nstrategies, and we further test BioBERT on BioASQ 6b on which our s ystem\nwas trained.\n2 Methods\nIn this section, we will brieﬂy discuss BioBERT 2 [\n8] and our modiﬁcations 3 for\nthe BioASQ Challenge (Figure 1).\n1 http://participants-area.bioasq.org/results/7b/phaseB/\n2 The source code for BioBERT is available at https://github.com/dmis-lab/\nbiobert.\n3 The source code and pre-processed datasets are available at https://github.com/\ndmis-lab/bioasq-biobert.\nPre-trained Language Model for Biomedical Question Answer ing 3\nPre-processing BioBERT & Task-specific Layer \nYes/No \nSnippet as-is, Full Abstract \nFactoid \nSnippet as-is, Full Abstract, Appended Snippet \nList \nSnippet as-is, Full Abstract, Appended Snippet \nYes or No answers \nfor questions \nExact answer \nList of exact answers \nFine-tuning for each task \nPost-processing \nTraining Data Undersampling \nAnswer Filtering \nProbability Threshold \nAnswer Filter \nAnswer Number Extraction \nAccuracy \nMacro F1 \nPrecision \nRecall \nF1 \nSAcc \nLAcc \nMRR \nFig. 1: Overview of our system.\n2.1 BioBERT\nWord embeddings are crucial for various text mining systems since t hey repre-\nsent semantic and syntactic features of words [\n14, 22]. While traditional models\nuse context-independent word embeddings, recently proposed m odels use contex-\ntualized word representations [ 4, 13, 15]. Among them, BERT [ 4], which is built\nupon multi-layer bidirectional Transformers [ 23], achieved new state-of-the-art\nresults on various NLP tasks including question answering. BioBERT [ 8] is the\nﬁrst domain-speciﬁc BERT based model pre-trained on PubMed abs tracts and\nfull texts. BioBERT outperforms BERT and other state-of-the- art models in\nbioNLP tasks such as biomedical named entity recognition, relation e xtraction,\nand question answering [ 6, 19].\nAn input representation of BioBERT for a given token is composed of the cor-\nresponding token, segment, and position embeddings. BioBERT utiliz es Word-\nPiece embeddings [ 26] which use sub-word units to address the out-of-vocabulary\n(OOV) problem. Broken sub-word units are denoted by ## (e.g. org anoid =\norgan + ##iod). Positional embeddings are learned during training an d seg-\nment embeddings are used to mark the location of question and pass age tokens\nin the input sequence. Following the design of BERT, a special token e mbedding\nfor [CLS] was added to the beginning of every sequence to process yes/no type\nquestions.\n2.2 Task-speciﬁc layer\nThe BioBERT model for QA is illustrated in Figure 2. Following the approa ch\nof BioBERT [\n8], a question and its corresponding passage are concatenated to\nform a single sequence which is marked by diﬀerent segment embeddin gs. The\ntask-speciﬁc layer for factoid type questions and the layer for list type questions\nboth utilize the output of the passage whereas the layer for yes/n o type questions\nuses the output of the ﬁrst [CLS] token.\nFactoid and List Questions In (Bio)BERT, the only additional trainable\nparameters needed for factoid and list type questions are the sof tmax layer for a\n4 W. Yoon et al.\n$KQ$'46 \n(>&/6@ (:KDW (LV (DQ (RUJDQ (\u0006\u0006RLG (\" (>6(3@ (7KH (VLQJOH (FHOO (\u0006\u0006\u0010 (\u0006\u0006EDVHG (\u0016' (RUJDQ đ\n($ ($ ($ ($ ($ ($ ($ ($ (% (% (% (% (% (% (% đ\n(\u0013 (\u0014 (\u0015 (\u0016 (\u0017 (\u0018 (\u0019 (\u001a (\u001b (\u001c (\u0014\u0013 (\u0014\u0014 (\u0014\u0015 (\u0014\u0016 (\u0014\u0017 đ\n>&/6@ \n :KDW \n LV \n DQ \n RUJDQ \n \u0006\u0006RLG \n \"\n >6(3@ \n 7KH \n VLQJOH \n FHOO \n \u0006\u0006\u0010\n \u0006\u0006EDVHG \n \u0016' \n RUJDQ \n đ\n4XHVWLRQ 3DVVDJH \n,QSXW\u0003WRNHQ \n3RVLWLRQ\u0003 \n(PEHGGLQJV \n6HJPHQW\u0003 \n(PEHGGLQJV \n7RNHQ\u0003 \n(PEHGGLQJV \n%\n 6\u0013\n 6\u0014\n 6\u0015\n 6\u0016\n 6\u0017\n 6\u0018\n 6=5'2? \n 6\u0013ğ\n 6\u0014ğ\n 6\u0015ğ\n 6\u0016ğ\n 6\u0017ğ\n 6\u0018ğ\n 6\u0019ğ\n Ĕ\n;GU\u00110Q \n% 8GEVQT \n(CEVQKF\u0003\u0011\u0003.KUV \n5VCTV\u0003 5 'PF\u0003 ' 8GEVQT\u0003 \nFig. 2: Example of a single sequence (Question-Passage pair) processed b y the\nBioBERT.\nlinear transformation of hidden vectors from BioBERT. Following the notation\nused in the BERT study, we denote the trainable start vector as S ∈ RH and the\ntrainable end vector as E ∈ RH where H denotes the hidden size of BioBERT.\nThe probabilities of the i-th token being the start of the answer token and the\nj-th token being the end of the answer token can be calculated by th e following\nequations:\nP start\ni = eS·Ti\n∑\nk eS·Tk\n, Pend\nj = eE·Tj\n∑\nk eE·Tk\nwhere Tl ∈ RH denotes l-th token representation from BioBERT and · denotes\nthe dot product between two vectors.\nYes/no Questions We use the ﬁrst [CLS] for the classiﬁcation of yes/no ques-\ntions. Here, we denote the representation of the [CLS] token fro m BioBERT as\nC ∈ RH . The parameter learned during training is a sigmoid layer consisting of\nW ∈ RH which is used for binary classiﬁcation. The probability for the sequen ce\nto be “yes” is calculated using the following equation.\nPyes = 1\n1 + e− CW\nLoss For the factoid/list question layer, we minimize Loss during training,\nwhich is deﬁned below. Loss is the arithmetic mean of the Lossstart and Lossend,\nwhich correspond to the negative log-likelihood for the correct sta rt and end\npositions, respectively. The ground truth start/end positions ar e denoted as ys\nPre-trained Language Model for Biomedical Question Answer ing 5\nfor the start token, and ye for the end token. The losses are deﬁned as follows:\nLossstart = − 1\nN\nN∑\nk=1\nlog P start,k\nys , Lossend = − 1\nN\nN∑\nk=1\nlog P end,k\nye\nLoss = ( LossStart + LossEnd )/2\nwhere k iterates for a mini-batch of size N.\nFor yes/no questions, the binary cross entropy between probab ility Pyes and the\ncorresponding ground truth was used as the training loss.\nLoss = − (yyes log Pyes + (1 − yyes) log (1 − Pyes))\n2.3 Pre-processing\nTo solve the BioASQ 7b Phase B dataset as extractive question answ ering, the\nchallenge datasets containing factoid and list type questions were c onverted into\nthe format of the SQuAD datasets [\n16, 17]. For yes/no type questions, we used\n0/1 labels for each question-passage pair.\nThe dataset in the SQuAD format consists of passages and their respective\nquestion-answer sets. A passage is an article which contains answe rs or clues for\nanswers and is denoted as the context in the dataset. The length of a passage\nvaries from a sentence to a paragraph. An exact answer may or ma y not exist\nin the passage, depending on the task. According to the rules of th e BioASQ\nChallenge, all the factoid and list type questions should be answerab le with the\ngiven passages [ 21]. An exact answer and its starting position are provided in the\nanswers ﬁeld. We used various sources including snippets and PubMed abstra cts,\nas passages. Multiple passages attached to a single question were d ivided to\nform question-passage pairs, which increased the number of ques tion-passage\npairs. The predicted answers of the question-passage pairs which share the same\nquestion are later combined in the post-processing layer.\nYes/no type questions are in the same format as the questions in th e SQuAD\ndataset. However, binary answers are given to yes/no type ques tions, rather than\nanswers selected based on their location in passages. Instead of p roviding an\nexact answer and its starting position in the answers ﬁeld, we marked yes/no\ntype questions using the strings “yes” or “no” and the Boolean valu es “false”\nand “true” in the is impossible ﬁeld. Since the distribution of yes/no answers in\nthe training set is usually skewed, we undersampled the training data to balance\nthe number of “yes” and “no” answers.\nWe used the following strategies for developing the datasets: Snippet as-is\nStrategy, Full Abstract Strategy, and Appended Snippet Strategy.\n• Snippet as-is Strategy Using snippets in their original form is a basic\nmethod for ﬁlling passages. The starting positions of exact answer s indicate\nthe positional oﬀsets of exact matching words. If a single snippet h as more\nthan one exact matching answer word, we form multiple question-pa ssage\n6 W. Yoon et al.\npairs for the snippet.\n• Full Abstract Strategy In the Full Abstract Strategy, we use an entire\nabstract, including the title of an article, as a passage. Full abstra cts are\nretrieved from PubMed using their provided PMIDs. The snippets ﬁeld of\nthe original dataset is used to ﬁnd the location of the correct answ er. First,\nwe look for the given snippet (e.g., a sentence in a typical case) from the\nretrieved abstract. Then, we search for the oﬀset of the ﬁrst e xact matching\nwords in the snippet, and add it to the oﬀset of the snippet in the par a-\ngraph. In this way, we can ﬁnd a plausible location of the answer within the\nparagraph.\n• Appended Snippet StrategyThe Appended Snippet Strategy is a compro-\nmise between using snippets as-is and full abstracts. We ﬁrst sear ch a given\nsnippet from an abstract and concatenate N ∈ N sentences before and after\nthe given snippet, forming 2 N + k sentences into a passage ( k denotes the\nnumber of sentences in a snippet, which is usually 1).\n2.4 Post-processing\nSince our pre-processing step involves dividing multiple passages with a same\nsingle question into multiple question-passage pairs, a single question can have\nmultiple predicted answers. The probabilities of predicted answers f or question-\npassage pairs sharing the same question, were merged to form a sin gle list of\npredicted answers and their probabilities for a question. The answe r candidate\nwith the highest probability is considered as the ﬁnal answer for a giv en factoid\ntype question. For list type questions, probability thresholding was the default\nmethod for providing answers. Answer candidates with a probability higher than\nthe threshold were included in the answer list. However, a considera ble number\n(28.6% of BioASQ 6b list type questions) of list type questions contain the\nnumber of required answers. From the training example “Please list 6 symptoms\nof Scarlet fever,” we can extract the number 6 from the given ques tion. We\nextracted the number provided in the question and used it to limit the length\nof the answer list for the question. For questions that contain the number of\nanswers, the extracted number of answers were yielded.\nFor factoid and list type questions, we also ﬁltered incomplete answe rs. An-\nswers with non-paired parenthesis were removed from the list of po ssible answers.\nPairs of round brackets and commas at the beginning and end of ans wers were\nremoved.\n3 Experimental Setup\n3.1 Dataset\nFor factoid and list type questions, exact answers are included in th e given\nsnippets, which is consistent with the extractive QA setting of the S QuAD [\n17]\nPre-trained Language Model for Biomedical Question Answer ing 7\ndataset. Only binary answers are provided for yes/no questions. For each ques-\ntion, regardless of the question type, multiple snippets or documen ts are provided\nas corresponding passages.\nThe statistics of the BioASQ datasets are listed in Table 1. A list type q ues-\ntion can have one or more than one answer; question-context pair s are made for\nevery answer of a list type question. In our pre-processing step, 3,722 question-\ncontext pairs were made from 779 factoid questions in the BioASQ 7b training\nset. For yes/no questions, we undersampled the training data to b alance the\nnumber of “yes” and “no” answers.\nAbout 28.2% of factoid type questions and 5.6% of list type questions in the\nBioASQ 7b training set do not have an answer in their corresponding s nippets.\nWe excluded unanswerable questions, following the approach of Wies e et al. [ 24].\nTable 1: Statistics of the BioASQ training set.\nQuestion Type BioASQ # of Questions in\noriginal datasets\n# of Pre-processed\nquestion-passage pairsVersion\nFactoid 6b 618 3,121\n7b 779 3,722\nList 6b 485 6,896\n7b 556 7,716\nYes/No 6b 612 5,921\n7b 745 6,676\n3.2 Training\nOur system is composed of BioBERT, task-speciﬁc layers, and a pos t-processing\nlayer. The parameters of BioBERT and a task-speciﬁc layer are tra inable. Our\ntraining procedure starts with pre-training the system on the SQu AD dataset.\nThe trainable parameters for factoid and list type questions were p re-trained on\nthe SQuAD 1.1 dataset, and the parameters for yes/no type ques tions were pre-\ntrained on the SQuAD 2.0 dataset. The pre-trained system is then ﬁ ne-tuned on\neach task.\nWe tuned the hyperparameters on the BioASQ 4/5/6b training and t est\nsets. We used a probability threshold of 0.42 as one of the hyperpar ameters\nfor list type questions. The probability threshold was decided using t he tuning\nprocedure.\n4 Results & Discussion\nIn this section, we ﬁrst report our results for the BioASQ 7b (Phas e B) Challenge,\nwhich are shown in Table 2. Please note that the results and ranks we re obtained\n8 W. Yoon et al.\nfrom the leaderboard of BioASQ 7b [\n3]. Then we evaluate our system and other\ncompeting systems on the validation set (BioASQ 6b). The results ar e presented\nin Table 3. Finally, we investigate the performance gain due to the sub -structures\nof the system (Table 5 and Table 6). Mean reciprocal rank (MRR) an d mean\naverage F-measure ( F1) were used as oﬃcial evaluation metrics to measure the\nperformance on factoid and list type questions from BioASQ, respe ctively. We\nreported strict accuracy (SAcc), lenient accuracy (LAcc) and M RR for factoid\nquestions and mean average precision, mean average recall, and me an average F1\nscore for list questions 4. Since the label distribution was skewed, macro average\nF1 score was used as an evaluation metric for yes/no questions.\n4.1 Results on BioASQ 7b\nOur results on Task 7b (Phase B) of the BioASQ Challenge are report ed in Ta-\nble 2. Each participant can submit up to 5 systems per batch. We sub mitted 1\nto 5 systems which use diﬀerent combinations of pre- and post-pro cessing strate-\ngies. We report the rankings and scores of our best performing sy stem and those\nof other competing systems for each task in Table 2. Competing sys tems are the\nbest and second best systems, other than our system, from dist inct participants.\nManually corrected gold-standard answers are not yet available at the time of\nwriting; therefore, we report the scores based on the online leade rboard 5.\nTable 2: Batch results of the BioASQ 7b Challenge. We report the rank of the\nsystems in parentheses.\nBatch Yes/no Factoid List # of\nSystemsParticipating system Mac F1 Participating system MRR Participating system F1\n1\n(1) Ours 67.12 (1) Ours 46.37 (3) Ours 30.51\n17(2) auth-qa-1 53.97 (2) BJUTNLPGroup 34.83 (1) Lab Zhu,Fudan Univer 32.76\n(3) BioASQ Baseline 47.27 (3) auth-qa-1 27.78 (4) auth-qa-1 25.94\n2\n(1) Ours 83.31 (1) Ours 56.67 (1) Ours 47.32\n21(2) auth-qa-1 62.96 (3) QA1 40.33 (3) LabZhu,FDU 25.79\n(4) BioASQ Baseline 42.58 (4) transfer-learning 32.67 (5) auth-qa-1 23.21\n3\n(5) Ours 46.23 (6) Ours 47.24 (1) Ours 32.98\n24(1) unipi-quokka-QA-2 74.73 (1) QA1/UNCC QA 1 51.15 (2) auth-qa-1 25.13\n(3) auth-qa-2 51.65 (3) google-gold-input 50.23 (4) BioASQ Baseline 22.75\n4\n(2) Ours 79.28 (1) Ours 69.12 (1) Ours 46.04\n36(1) unipi-quokka-QA-1 82.08 (4) FACTOIDS/UNCC... 61.03 (2) google-gold-input-nq 43.64\n(8) bioasq experiments 58.01 (9) google-gold-input 54.95 (9) LabZhu,FDU 32.14\n5\n(1) Ours 82.50 (1) Ours 36.38 (1) Ours 46.19\n40(2) unipi-quokka-QA-5 79.39 (3) BJUTNLPGroup 33.81 (6) google-gold-input-nq 28.89\n(6) google-gold-input-ab 69.41 (4) UNCC QA 1 33.05 (7) UNCC * 28.62\n4 For more details, please visit http://participants-area.bioasq.org/Tasks/b/\neval_meas_2018/.\n5 The oﬃcial results of the competition will be provided at http://bioasq.org.\nPre-trained Language Model for Biomedical Question Answer ing 9\n4.2 Validating on the BioASQ 6b dataset\nWe compared the performance of existing systems and our system on the BioASQ\n6b dataset from the last year (2018), which is shown in Table 3. We mic ro\naveraged the scores from ﬁve experiments and reported the sco res in Table 3.\nSimilarly, the leaderboard scores of the best performing system fo r each batch\nwere micro averaged and reported as the Best System scores [\n5, 12, 27]. Our\nsystem obtained much higher scores on the BioASQ 6b dataset than the top\nsystems from leaderboard of BioASQ 6b Challenge.\nTable 3: Performance comparison between existing systems and our syste m on\nthe BioASQ 6b dataset (from last year). Note that our system obt ained a 20%\nto 60% performance improvement over the best systems.\nSystem Factoid (MRR) List (F1) Yes/no (Macro F1)\nBest System 27.84 % 27.21 % 62.05 %\nOurs 48.41 % 43.16 % 75.87 %\nPre-training In Table 4, we compare the performance of the pre-trained mod-\nels. BioBERT ﬁne-tuned on the BioASQ 6b dataset outperformed BE RTBASE\nﬁne-tuned on BioASQ in both factoid and list type questions. BioBERT ﬁrst\npre-trained on SQuAD and then ﬁne-tuned on BioASQ 6b obtained th e best\nperformance over other two experiments, demonstrating the eﬀ ectiveness of pre-\ntraining BioBERT on SQuAD, a comprehensive and large-scale questio n answer-\ning corpus.\nTable 4: Performance comparison between pre-trained models.\nPre-trained models Factoid List\nSAcc LAcc MRR Prec Recall F1\nBERTBASE+BioASQ Finetune 24.84% 36.03% 28.76% 42.41% 35.88% 35.37%\nBioBERT+BioASQ Finetune 34.16% 47.83% 39.64% 44.62% 39.49% 38.45%\nBioBERT+SQuAD+BioASQ Finetune 42.86% 57.14% 48.41% 51.58% 43.24% 43.16%\nPre-/Post-processing The performance of our system is largely aﬀected by\nhow the data is pre-processed (Table 5). However, the eﬀectiven ess of the pre-\nprocessing strategy varies depending on the type of question. Fo r example, the\n10 W. Yoon et al.\nAppended Snippet strategy and Full Abstract strategy obtained good perfor-\nmance on factoid questions, while the Snippet As-is strategy achiev ed the high-\nest performance on list and yes/no type questions. Table 6 shows t he eﬀect of\npost-processing on the performance of a system evaluated on list type questions.\nIn our study, both extracting the number of answers from quest ions and ﬁltering\npredicted answers were eﬀective.\nTable 5: Performance comparison between pre-processing methods. Sco res on\nthe BioASQ 6b dataset.\nStrategy Factoid List Yes/no\nSAcc LAcc MRR Prec Recall F1 MacroF1\nSnippet 40.99 55.90 47.38 51.58 43.24 43.16 75.10\nFull Abstract 42.86 57.14 48.41 42.66 32.58 33.52 66.76\nAppended Snippet 39.75 58.39 48.00 44.04 41.26 39.36 -\nTable 6: Ablation study on the post-processing methods. Scores for list ty pe\nquestions in the BioASQ 6b dataset.\nStrategy Precision Recall F1\nBaseline (Snippet) 51.58 43.24 43.16\nBaseline without ﬁlter 50.79 43.24 42.64\nBaseline without answer # extraction 50.01 44.32 42.58\nEnsemble Starting from test batch 4 of BioASQ 7b, we submitted model ensem -\nble results as one of our systems. The performance gain of the mod el ensemble\non our evaluation set was relatively small; the performance ranged f rom 0.2% to\n2% depending on the task. The model ensemble improved the perfor mance on\nfactoid questions the most (2% gain), but applying the model ensem ble to list\nquestions did not obtain higher performance than the single model. A lthough\nthe model ensemble obtained high scores in the BioASQ 7b Challenge, it could\nonly obtain the highest score on factoid type questions in batch 5.\nQualitative Analysis In Table 7, we show three predictions generated by\nour system on the BioASQ 6b factoid dataset. Due to the space limita tion,\nwe show only small parts of a passage, which contain the answers (p redicted\nPre-trained Language Model for Biomedical Question Answer ing 11\nanswers might be contained in other parts of the passage). We sho w the top\nﬁve predictions generated by our system which can also be used for list type\nquestions. In the ﬁrst example, our system successfully ﬁnds the answer and\nother plausible answers. The second example shows that most of th e predicted\nanswers are correct and have only minor diﬀerences. In the last ex ample, we\nobserve that the ground truth answer does not exist in the passa ge. Also, the\npredicted answers are indeed correct despite the incorrect anno tation.\nTable 7: Predictions by our BioBERT based QA system on the BioASQ 6b\nfactoid dataset\nNo. Type Description\n1 Question What causes “puﬀy hand syndrome?”\nPassage Puﬀy hand syndrome is a complication of intravenous drug abuse,\nwhich has no current available treatment.\nGround Truth “intravenous drug abuse”\nPredicted Answer “intravenous drug abuse”,\n“drug addiction”,\n“Intravenous drug addiction”,\n“staphylococcal skin infection”,\n“major depression”\n2 Question In which syndrome is the RPS19 gene most frequentl y mutated?\nPassage A transgenic mouse model demonstrates a dominant ne gative eﬀect\nof a point mutation in the RPS19 gene associated\nwith Diamond-Blackfan anemia.\nGround Truth “Diamond-Blackfan Anemia”,\n“DBA”\nPredicted Answer “Diamond-Blackfan anemia”,\n“Diamond-Blackfan anemia (DBA)”,\n“DBA”,\n“Diamond Blackfan anemia”,\n“Diamond-Blackfan anemia. Diamond-Blackfan anemia”\n3 Question What protein is the most common cause of hereditar y renal amyloidosis?\nPassage We suspected amyloidosis with ﬁbrinogen A alpha cha in deposits,\nwhich is the most frequent cause of hereditary amyloidosis i n Europe,\nwith a glomerular preferential aﬀectation.\nGround Truth “Fibrinogen A Alpha protein”\nPredicted Answer “ﬁbrinogen”,\n“ﬁbrinogen alpha-chain. Variants of circulating ﬁbrinoge n”,\n“ﬁbrinogen A alpha chain (FGA)”,\n“Fibrinogen A Alpha Chain Protein. Introduction: Fibrinog en”,\n“apolipoprotein AI”\nThe prediction result of list question from the BioASQ 6b is presented in\nTable 8. We found that our system is more likely to produce incorrect predic-\n12 W. Yoon et al.\ntions on list questions than on factoid questions. Our system intern ally outputs a\nlist of predictions and the list is likely to include prediction with erroneou s span.\nEven though incorrect prediction (“JBP”) with erroneous span ha s a lower prob-\nability than the true prediction (“JBP1” and “JBP2”), it can have co nsiderable\nabsolute probabilities. On factoid questions, selecting a top one ans wer is re-\nquired. Hence we can ignore incorrect prediction on factoid questio ns. On the\ncontrary, on list questions, prediction with erroneous span gets h igher probabil-\nity through merging predictions in post-processing step. Since our model utilizes\nﬁxed threshold value, prediction with erroneous span is imperfect b ut achieved\na higher possibility than the threshold.\nTable 8: Prediction by our BioBERT based QA system on the BioASQ 6b list\ndataset\nNo. Type Description\n1 Question Which enzymes are responsible for base J creation in Trypanosoma brucei?\nPassage JBP1 and JBP2 are two distinct thymidine hydroxylas es involved in\nJ biosynthesis in genomic DNA of African trypanosomes.\nHere we discuss the regulation of hmU and base J formation in t he\ntrypanosome genome by JGT and base J-binding protein.\nGround Truth “JBP1”,\n“JBP2”,\n“JGT”\nPredicted Answer “JBP1”,\n“JBP”,\n“thymidine hydroxylase”,\n“JGT”,\n“hmU”,\n“JBP2”\n5 Conclusion\nIn this paper, we proposed BioBERT based QA system for the BioASQ biomed-\nical question answering challenge. As the size of the biomedical ques tion an-\nswering dataset is very small, we leveraged pre-trained language mo dels for\nbiomedical domain which eﬀectively exploit the knowledge from large bio medical\ncorpora. Also, while existing systems for the BioASQ challenge requir e diﬀer-\nent structures for diﬀerent question types, our system uses alm ost the same\nstructure for various question types. By exploring various pre-/ post-processing\nstrategies, our BioBERT based system obtained the best perform ance in the 7th\nBioASQ Challenge, achieving state-of-the-art results on factoid, list, and yes/no\ntype questions. In future work, we plan to further systematically analyze the\nincorrect predictions of our systems, and develop biomedical QA sy stems that\ncan eventually outperform humans.\nPre-trained Language Model for Biomedical Question Answer ing 13\nAcknowledgements\nWe appreciate Susan Kim for editing the manuscript.\nFunding\nThis work was funded by the National Research Foundation of Kore a (NRF-\n2017R1A2A1A17069645, NRF-2016M3A9A7916996) and the Natio nal IT Indus-\ntry Promotion Agency grant funded by the Ministry of Science and I CT and\nMinistry of Health and Welfare (NO. C1202-18-1001, Development P roject of\nThe Precision Medicine Hospital Information System (P-HIS)).\nReferences\n1. Alsentzer, E., Murphy, J.R., Boag, W., Weng, W.H., Jin, D. , Naumann, T.,\nMcDermott, M.: Publicly available clinical bert embedding s. arXiv preprint\narXiv:1904.03323 (2019)\n2. Beltagy, I., Cohan, A., Lo, K.: Scibert: Pretrained conte xtualized embeddings for\nscientiﬁc text. arXiv preprint arXiv:1903.10676 (2019)\n3. BioASQ Participants Area BioASQ (May, 2019),\nhttp://participants-area.\nbioasq.org/results/7b/phaseB/\n4. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv pre print arXiv:1810.04805\n(2018)\n5. Dimitriadis, D., Tsoumakas, G.: Word embeddings and exte rnal resources for an-\nswer processing in biomedical factoid question answering. Journal of biomedical\ninformatics 92, 103118 (2019)\n6. Kim, D., Lee, J., So, C.H., Jeon, H., Jeong, M., Choi, Y., Yo on, W., Sung, M.,\nKang, J.: A neural named entity recognition and multi-type normalization tool for\nbiomedical text mining. IEEE Access 7, 73729–73740 (2019)\n7. Krithara, A., Nentidis, A., Paliouras, G., Kakadiaris, I .: Results of the 4th\nedition of BioASQ challenge. In: Proceedings of the Fourth B ioASQ work-\nshop. pp. 1–7. Association for Computational Linguistics, Berlin, Germany\n(Aug 2016). https://doi.org/10.18653/v1/W16-3101, https://www.aclweb.org/\nanthology/W16-3101\n8. Lee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C.H., Kang, J.: BioBERT: a\npre-trained biomedical language representation model for biomedical text mining.\nBioinformatics (09 2019). https://doi.org/10.1093/bioinformatics/btz682\n9. Lim, S., Kang, J.: Chemical–gene relation extraction usi ng recursive neural net-\nwork. Database 2018 (2018)\n10. Nentidis, A., Bougiatiotis, K., Krithara, A., Palioura s, G., Kakadiaris, I.: Results\nof the ﬁfth edition of the bioasq challenge. In: BioNLP 2017. pp. 48–57 (2017)\n11. Nentidis, A., Krithara, A., Bougiatiotis, K., Palioura s, G., Kakadiaris, I.: Results\nof the sixth edition of the BioASQ challenge. In: Proceeding s of the 6th BioASQ\nWorkshop A challenge on large-scale biomedical semantic in dexing and question\nanswering. pp. 1–10. Association for Computational Linguistics, Brussels, Belgium\n(Nov 2018), https://www.aclweb.org/anthology/W18-5301\n14 W. Yoon et al.\n12. Peng, S., Zhang, Y., You, R., Xie, Z., Wang, B., Zhu, S.: Th e fudan participa-\ntion in the 2015 bioasq challenge: Large-scale biomedical s emantic indexing and\nquestion answering. In: CEUR Workshop Proceedings. vol. 1391. CEUR Workshop\nProceedings (2015)\n13. Peters, M., Neumann, M., Iyyer, M., Gardner, M., Clark, C ., Lee, K., Zettlemoyer,\nL.: Deep contextualized word representations. In: Proceed ings of the 2018 Confer-\nence of the North American Chapter of the Association for Com putational Lin-\nguistics: Human Language Technologies, Volume 1 (Long Pape rs). pp. 2227–2237\n(2018)\n14. Pyysalo, S., Ginter, F., Moen, H., Salakoski, T., Anania dou, S.: Distributional\nsemantics resources for biomedical text processing. Proce edings of LBM pp. 39–44\n(2013)\n15. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I .: Improving language\nunderstanding with unsupervised learning. Tech. rep., Tec hnical report, OpenAI\n(2018)\n16. Rajpurkar, P., Jia, R., Liang, P.: Know what you don’t kno w: Unanswerable ques-\ntions for squad. arXiv preprint arXiv:1806.03822 (2018)\n17. Rajpurkar, P., Zhang, J., Lopyrev, K., Liang, P.: Squad: 100,000+ questions for\nmachine comprehension of text. arXiv preprint arXiv:1606. 05250 (2016)\n18. Rosso-Mateus, A., Gonz´ alez, F.A., Montes-y G´ omez, M.: Mindlab neural network\napproach at bioasq 6b. In: Proceedings of the 6th BioASQ Work shop A challenge\non large-scale biomedical semantic indexing and question a nswering. pp. 40–46\n(2018)\n19. Sousa, D., Lamurias, A., Couto, F.M.: Using neural networks for relation extraction\nfrom biomedical literature. arXiv preprint arXiv:1905.11 391 (2019)\n20. Talmor, A., Berant, J.: Multiqa: An empirical investiga tion of generalization and\ntransfer in reading comprehension. arXiv preprint arXiv:1 905.13453 (2019)\n21. Tsatsaronis, G., Balikas, G., Malakasiotis, P., Partal as, I., Zschunke, M., Alvers,\nM.R., Weissenborn, D., Krithara, A., Petridis, S., Polychr onopoulos, D., et al.:\nAn overview of the bioasq large-scale biomedical semantic i ndexing and question\nanswering competition. BMC bioinformatics 16(1), 138 (2015)\n22. Turian, J., Ratinov, L., Bengio, Y.: Word representatio ns: a simple and general\nmethod for semi-supervised learning. In: Proceedings of th e 48th annual meet-\ning of the association for computational linguistics. pp. 3 84–394. Association for\nComputational Linguistics (2010)\n23. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n/suppress L., Polosukhin, I.: Attention is all you need. In: Advancesin neural information\nprocessing systems. pp. 5998–6008 (2017)\n24. Wiese, G., Weissenborn, D., Neves, M.: Neural domain ada ptation for biomedical\nquestion answering. arXiv preprint arXiv:1706.03610 (201 7)\n25. Wiese, G., Weissenborn, D., Neves, M.: Neural question a nswering at bioasq 5b.\narXiv preprint arXiv:1706.08568 (2017)\n26. Wu, Y., Schuster, M., Chen, Z., Le, Q.V., Norouzi, M., Mac herey, W., Krikun,\nM., Cao, Y., Gao, Q., Macherey, K., et al.: Google’s neural ma chine translation\nsystem: Bridging the gap between human and machine translat ion. arXiv preprint\narXiv:1609.08144 (2016)\n27. Yang, Z., Zhou, Y., Nyberg, E.: Learning to answer biomed ical questions: Oaqa at\nbioasq 4b. In: Proceedings of the Fourth BioASQ workshop. pp . 23–37 (2016)\n28. Yoon, W., So, C.H., Lee, J., Kang, J.: Collabonet: collab oration of deep neural\nnetworks for biomedical named entity recognition. BMC bioi nformatics 20(10),\n249 (2019)\nPre-trained Language Model for Biomedical Question Answer ing 15\n29. Zhu, H., Paschalidis, I.C., Tahmasebi, A.: Clinical con cept extraction with contex-\ntual word embedding. arXiv preprint arXiv:1810.10566 (201 8)"
}