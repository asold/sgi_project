{
  "title": "A publicly available benchmark for assessing large language models’ ability to predict how humans balance self-interest and the interest of others",
  "url": "https://openalex.org/W4411868579",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5068651250",
      "name": "Valerio Capraro",
      "affiliations": [
        "University of Milano-Bicocca"
      ]
    },
    {
      "id": "https://openalex.org/A5047709816",
      "name": "Roberto Di Paolo",
      "affiliations": [
        "University of Parma"
      ]
    },
    {
      "id": "https://openalex.org/A5012785291",
      "name": "Veronica Pizziol",
      "affiliations": [
        "University of Bologna"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4380769213",
    "https://openalex.org/W2766718178",
    "https://openalex.org/W4361289889",
    "https://openalex.org/W4384154918",
    "https://openalex.org/W4366817968",
    "https://openalex.org/W4389611357",
    "https://openalex.org/W2551317447",
    "https://openalex.org/W4206216660",
    "https://openalex.org/W4399518864",
    "https://openalex.org/W3125314754",
    "https://openalex.org/W4406443114",
    "https://openalex.org/W4399671556",
    "https://openalex.org/W1972084227",
    "https://openalex.org/W3009803536",
    "https://openalex.org/W4402902180",
    "https://openalex.org/W2117401803",
    "https://openalex.org/W2119432837",
    "https://openalex.org/W4385897789",
    "https://openalex.org/W1193784276",
    "https://openalex.org/W2795191834",
    "https://openalex.org/W1983084915",
    "https://openalex.org/W2081124617",
    "https://openalex.org/W4383684495",
    "https://openalex.org/W4376117416",
    "https://openalex.org/W4352977353",
    "https://openalex.org/W4391673229",
    "https://openalex.org/W4386843972",
    "https://openalex.org/W2465714213",
    "https://openalex.org/W1988840588",
    "https://openalex.org/W2969294681",
    "https://openalex.org/W4392550719",
    "https://openalex.org/W2082064502",
    "https://openalex.org/W2169335427",
    "https://openalex.org/W3124210697",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W4205508242",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W3159959439",
    "https://openalex.org/W4392971563",
    "https://openalex.org/W4411121004",
    "https://openalex.org/W4384155043",
    "https://openalex.org/W4377967281",
    "https://openalex.org/W2800825839",
    "https://openalex.org/W4404633323",
    "https://openalex.org/W4385852384",
    "https://openalex.org/W4407095609",
    "https://openalex.org/W4401170130",
    "https://openalex.org/W2145825528",
    "https://openalex.org/W4253706277",
    "https://openalex.org/W2321977339",
    "https://openalex.org/W4363624465",
    "https://openalex.org/W4220685807",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4378942694",
    "https://openalex.org/W1531590217"
  ],
  "abstract": null,
  "full_text": "A publicly available benchmark for \nassessing large language models’ \nability to predict how humans \nbalance self-interest and the \ninterest of others\nValerio Capraro1, Roberto Di Paolo2 & Veronica Pizziol3\nLarge language models (LLMs) hold enormous potential to assist humans in decision-making \nprocesses, from everyday to high-stake scenarios. However, as many human decisions carry social \nimplications, for LLMs to be reliable assistants a necessary prerequisite is that they are able to capture \nhow humans balance self-interest and the interest of others. Here we introduce a novel, publicly \navailable, benchmark to test LLM’s ability to predict how humans balance monetary self-interest \nand the interest of others. This benchmark consists of 106 textual instructions from dictator games \nexperiments conducted with human participants from 12 countries, alongside with a compendium \nof actual human behavior in each experiment. We investigate the ability of four advanced chatbots \nagainst this benchmark. We find that none of these chatbots meet the benchmark. In particular, only \nGPT-4 and GPT-4o (not Bard nor Bing) correctly capture qualitative behavioral patterns, identifying \nthree major classes of behavior: self-interested, inequity-averse, and fully altruistic. Nonetheless, \nGPT-4 and GPT-4o consistently underestimate self-interest, while overestimating altruistic behavior. \nIn sum, this article introduces a publicly available resource for testing the capacity of LLMs to estimate \nhuman other-regarding preferences in economic decisions and reveals an “optimistic bias” in current \nversions of GPT.\nKeywords Generative artificial intelligence, Human behavior, Economic games, Dictator game, Altruism\nLarge language models (LLMs) represent a transformative development in computational technology, fostering \na range of applications across diverse domains. Recent work has demonstrated their effectiveness in crafting \nrealistic narratives in creative writing1, predicting market trends in finance2, providing medical consultations3 as \nwell as offering suggestions to increase productivity, especially for less-skilled workers4,5.\nAs LLMs continue to advance, it is increasingly apparent that they have the potential to assist in human \ndecision-making6. The promise of LLMs lies not only in their capacity to undertake a diverse set of tasks but \nalso in its potential to aid us in making decisions in a range of contexts, from everyday low-stake situations \nto high-stake individual or policy decisions 7–10. However, given that many human decisions inherently bear \nsocial implications—that is, they impact others besides the decision-maker—it is critical to ensure that LLMs \ncan precisely grasp the delicate balance between self-interest and the interest of others. Accurate predictions \nregarding actual human behavior are essential for an LLM to be considered a valuable assistant to human \ndecision-making because these predictions require the LLM forming a model of people’s behavior, which is a \nprecondition for an LLM to formulate behavioral guidance or recommendations to users seeking advice in their \ndecision-making processes. Inaccurate predictions can provide misguided guidance with potential downstream \nnegative effects on individuals and society. For example, if an LLM were to overestimate people’s self-interest, \nthis may result in reduced engagement in prosocial behaviors overall, because individuals are strongly motivated \nby conditional cooperation and positive reciprocity11. This may have negative consequences on timely prosocial \nbehaviors, like sustainability12,13. On the other hand, if an LLM were to overestimate people’s altruistic behavior, \nthis may result in unrealistic expectations about others’ behavior which in turn may lead to disappointment 14. \n1Department of Psychology, University of Milan Bicocca, 20126 Milan, Italy. 2Department of Economics and \nManagement, University of Parma, 43121 Parma, Italy. 3Department of Economics, University of Bologna, 40126 \nBologna, Italy. email: valerio.capraro@unimib.it\nOPEN\nScientific Reports |        (2025) 15:21428 1| https://doi.org/10.1038/s41598-025-01715-7\nwww.nature.com/scientificreports\n\nIn the case of charity donations or crowdfunding campaigns, for instance, higher expectations not aligned with \nreality may result in failing to meet the anticipated outcomes15.\nWe acknowledge that LLMs could, in principle, serve as “normative guides” rather than merely descriptive \nmirrors of human behavior. This role is especially promising when LLMs are used to offer advice16, as they could \nhelp counteract certain human biases. However, in this work we are not concerned with scenarios in which \nLLMs are directly solicited for behavioral guidance. Instead, we focus on a preliminary step that is likely to \nunderpin any subsequent advice-giving function: what LLMs expect humans to do. Inaccurate predictions at this \nfoundational level could lead to erroneous or even harmful advice. Furthermore, employing LLMs as normative \nguides raises significant ethical challenges. Chief among these is the question of who should determine the \nappropriate standards for normative behavior, given the wide variability in moral judgments among individuals \nand across cultures17–21. In this work, we deliberately choose not to engage with this ethical complexity. Instead, \nwe aim to introduce a benchmark to assess whether LLMs provide accurate predictions of how humans \nbalance self-interest with the interests of others. We view this as a crucial preliminary step that can inform \nfuture developments, potentially paving the way for more effective advisory functions for LLMs. As a secondary \ncontribution, we use four advanced chatbots as case studies to evaluate their ability to estimate this trade-off.\nIn sum, here we ask two, albeit related, questions: (i) How can we test LLMs’ capability to capture how \nhumans balance self- vs other-interest?; (ii) Can current LLMs accurately estimate this trade-off?\nTo investigate these questions, we have adopted methodologies from behavioral economics 22 and created a \nnovel, publicly available, benchmark for testing LLMs’ ability to capture how humans balance monetary self-\ninterest and the interest of others. Although self- and other- interest can reflect various forms of utility (e.g., \npsychological benefit), here we exclusively refer to the monetary benefit. We have collected and made publicly \navailable, in a simple, ready-to-use format, over one-hundred instructions from dictator game experiments \nconducted with human participants. In this game, one participant, the “dictator” , is endowed with a sum of \nmoney and must decide how much, if any, to share with another participant. The second participant has no \ninput in this decision and can only accept what the dictator offers 23,24. Consequently, the dictator game serves \nas a measure of the trade-off between monetary self-interest (keeping all the money for the self) and other-\ninterest (giving away some or all of the money to the other). For each experiment, we have collected average \ngiving rates and, where feasible, the precise distribution of choices, thereby creating a public resource useful \nfor evaluating whether an LLM generates predictions consistent with actual human behavior. Subsequently, \nwe assessed whether four sophisticated generative LLM-based chatbots currently available—namely OpenAI’s \nGenerative Pre-trained Transformer 4 (GPT-4) and 4o (GPT-4o), Google’s Bard and Microsoft’s Bing Chat—\nmeet this benchmark. It is important to note that, since LLMs can learn from user interactions and undergo \nmajor updates, it is possible (and indeed likely) that future versions will perform better on this benchmark. We \nacknowledge this. Our point is not to claim that these or future models will never meet the benchmark. Rather, \nour goal is to introduce a benchmark that can be used to test current and future models, and to show that the \ncurrent versions do not yet meet it.\nIn the recent months, there has been a proliferation of studies exploring LLMs’ decision-making processes \nacross diverse contexts, including cooperative, altruistic, moral, risk, time, and food decisions 6,25–32. It is \nimportant to note, however, that these studies typically compare human decisions and chatbot decisions in \na single experimental setting designed by the researchers. While this research is undoubtedly significant, to \neffectively evaluate the capacity of chatbots to estimate human decision-making, it is crucial to compare not solely \nchatbot decisions and human decisions in a single experimental setting, but in a variety of settings conducted \nto measure a specific human behavior. In this article we take a step towards this direction. Our analysis spans \nmultiple experimental contexts, drawing on all available dictator game studies for which we could obtain the \noriginal instructions, thereby offering a public resource that allows behavioral scientists and AI developers to \nassess LLMs’ ability to predict how humans balance monetary self-interest and the interest of others in a variety \nof contexts.\nTo assemble as many experimental instructions as possible, we combined two search methods: (i) manual \nsearches of the relevant literature and (ii) public calls on the Economic Science Association (ESA) and Society \nfor Judgment and Decision Making (SJDM) forums, as well as at conferences and research meetings, asking \nbehavioral scientists to provide instructions for dictator game experiments with human participants that they \nhave conducted. To reduce potential publication bias, we included working papers and unpublished studies. The \nsearches were open to all dictator game experiments, provided that experimental instructions in English were \navailable. In doing so, we collected 106 unique experimental instructions, drawn from a total of 38 different \nresearch articles (32 from manual searches, 6 from public calls), which reported on experiments conducted \nacross 12 distinct countries. See Table S1 in the Supplementary Information for the full list of experimental \ninstructions, organized by research article.\nThis collection includes a diverse range of dictator game types, specifically:\n 1. The standard dictator game (93 instructions): In this game, participants assigned the role of dictators can \ntransfer any amount of their endowment to the other participant, ranging from zero to their entire allotment.\n 2. Extreme dictator games (6 instructions): This variant of the standard dictator game allows for only two op -\ntions, namely, transferring nothing or transferring the entire endowment.\n 3. Dictator games with a “take” option (7 instructions): Here, the dictators begin with a larger endowment than \nthe recipients. Beyond transferring some of their endowment to the recipient, they can also take a portion of \nthe recipients’ endowment for themselves.\nScientific Reports |        (2025) 15:21428 2| https://doi.org/10.1038/s41598-025-01715-7\nwww.nature.com/scientificreports/\nIn the OSF link associated with this article (https://osf.io/4gkb3/), we include the full list of instructions (name \nof file: dg_instructions). For each instruction, we collected mean giving and, when feasible, exact distributions of \nchoices. These data are reported in the dg_data file.\nEach instruction was then inputted into the chatbot with the prompt, “Now imagine that there is a population \nof 1000 people living in [country] facing this decision problem. How would you estimate the distribution of their \nchoices? Please, for each choice, report a result in the form X ± Y , where X is the estimated number of people \nmaking their choice, and Y is the error. ” We refer to Table 1 for an example of prompt, along with an example of \na response from GPT-4.\nWe recorded responses in the form of frequencies of choices at each 10% step. In cases in which GPT-4’s \npredictions encompassed more than one 10% step (as in the example in Table 1), we evenly distributed this \nprediction across each affected 10% step. For instance, in the example in Table 1, the recorded frequencies were: \n0.2, 0.075, 0.075, 0.075, 0.075, 0.35, 0.025, 0.025, 0.025, 0.025, 0.05. The output of this methodology provided \na series of LLM-generated estimations on the distribution of decisions. See Materials and Methods for further \ndetails.\nWe propose a benchmark consisting of two tests:\nWeak test. Compare the grand mean giving predicted by an LLM with the actual grand mean giving \nin human experiments. As a measure for evaluating LLM’s performance, we consider the signed error: \nEW = µLLM − µHuman, where µLLM is the estimated grand mean and µHuman is the actual grand \nmean. The signed error is preferred to the absolute error in this task, as it is important to distinguish between \noverestimations and underestimations. By normalizing the total endowment in the dictator game to be 1, the \nerror in the standard dictator game and in the extreme dictator game ranges from -1 to 1, while in the dictator \ngame with a take option ranges from -2 to 2.\nStrong test. Compare the frequency of each potential giving decision predicted by an LLM with the actual \nfrequency observed. As a measure for evaluating LLM’s performance, we consider the vector of signed errors, where \nthe i-th component is defined as ES(i)= ϕLLM(10i) − ϕHuman(10i), where ϕLLM(10i) is the estimated \nproportion of people giving 10i% of the total endowment and ϕHuman(10i) is the actual proportion. In the \nstandard dictator game, i = 0,1, ...,10; in the dictator game with a take option, i = −10, −9, ...,−1, 0,1, ...,10. \nIn the extreme dictator game, the error vector can still be defined but it is redundant, because there are only two \npossible choices for the dictators; in other words, the strong test and the weak test are equivalent in the extreme \ndictator game. The vector of signed errors is preferred to scalar measures in this case because it allows for a more \ngranular understanding of potential systematic biases in the LLM’s estimations. Each error component ranges \nfrom -1 to 1, being a difference between proportions.\nResults\nGPT-4’s predictions in the standard dictator game\nWe begin our analysis by examining the standard dictator game. The behavioral experiments across all 93 \nconditions revealed an average giving of 30.7%. This average is aligned with Engel’s meta-analysis 34, which \nreported an average of 28.3% over 616 dictator game experiments, thus suggesting that our collection of studies \nis representative of the largest dictator games meta-analysis to date. However, GPT-4’s average estimate was \nconsiderably higher at 42.6%, exceeding the observed levels of giving by roughly 12 percentage points. This \ncorresponds to a signed error EW = 0.119. We can reject the null hypothesis that the signed error is equal to zero \n(Wilcoxon rank-sum test: z = 6.807, p < 0.001). We use the Wilcoxon test because the distributions of predicted \nand actual giving deviate from the normal distribution (see Fig.  2; this is also confirmed by a test of normality: \np < 0.001 and p < 0.01).\nOur prompt:\nPlease read the following decision problem:\nY ou will now participate in an economic task. For this task you will be “the giver” and another participant will be the “receiver. ” As the giver, your role is to take and keep as many of \nthe $1 bills in the envelope in front of you as you would like (it contains ten $1 bills in total). Please leave in the envelope any of the $1 bills you do not claim for yourself, if any, as \nthey will be given to another participant (“the receiver”) to keep. Remember, your identity will remain anonymous from the receiver\nAfter you have completed this task, move on to the rest of the tasks; you may not return to this decision later. Please make your decision now and put the envelope into the box\nNow imagine that there is a population of 1,000 people living in the USA facing this decision problem. How would you estimate the distribution of their choices? Please, for each \nchoice, report a result in the form X ± Y , where X is the estimated number of people making their choice, and Y is the error\nGPT-4’s reply:\nPredicting the exact distribution of this decision problem is challenging due to the variability in human behavior. However, I can provide an estimation based on the tendencies \nobserved in related research areas such as behavioral economics, game theory, and altruism\n1. Keep all 10 dollars: People with a strictly individualistic or profit-maximizing mindset may decide to keep all the money. However, social norms and a sense of fairness might deter \nothers from taking all the money. Let’s say about 200 ± 50 people fall into this category\n2. Leave some, but not half: Some people might take more than half but still leave a small amount, trying to balance self-interest and a sense of fairness. Let’s estimate this as 300 ± 50\n3. Split equally, 5 dollars each: People who strictly follow norms of fairness and equality might decide to split the money equally. This could be around 350 ± 50 people\n4. Leave more than half: Some people might want to appear (to themselves or a hypothetical observer) more generous or altruistic and may, therefore, leave more than half of the \nmoney. This could account for about 100 ± 50 people\n5. Leave all 10 dollars: The least likely scenario might be individuals who leave all the money, either out of extreme generosity or misunderstanding of the task. This might be around \n50 ± 30 people\nThese are rough estimates and the actual outcomes could be different. It’s also important to consider that many factors such as the participants’ current financial situation, \nunderstanding of the task, perception of fairness, cultural background, and mood can influence their decisions\nTable 1. Example of a prompt input to GPT-4, along with GPT-4’s reply. Instructions from33.\n \nScientific Reports |        (2025) 15:21428 3| https://doi.org/10.1038/s41598-025-01715-7\nwww.nature.com/scientificreports/\nThe disparity between predictions and actual behavior is evident in Fig.  1, which shows that GPT-4 \noverestimates the mean level of giving in the majority of studies (73 out of 93). See Table S1 in SI Appendix 1 for \ndescriptive statistics.\nTo further investigate GPT-4’s overestimation of human giving, we compared its predicted giving \ndistribution with the actual giving distribution reported in the corresponding studies, available for 41 studies. \nThe actual behavior (Fig. 2, red diamonds) displayed a tri-peak distribution of donations: a primary peak at zero \ndonations (34.7% of observations), a secondary peak at half the endowment (25.9%), and a tertiary peak at full \nendowment (8.5%). In contrast, while GPT-4 qualitatively predicted the same peaks, the quantitative predictions \nwere significantly different (Fig. 2, blue bars). Specifically, the frequency of full givers predicted by GPT-4 was \nsignificantly higher than the actual frequency reported (21.6% vs 8.5%, two-sided t-test: t = 3.831, p < 0.001), \nwhile the frequency of zero givers and half givers were significantly lower (22.3% vs 34.7%t = − 4.229, p < 0 0.001; \n15.7% vs 25.9%, t-test: t = − 4.239, p < 0.001). More in detail, we can estimate the vector of signed errors to be: \nES = (− 0.124, − 0.007, − 0.023, 0.001, − 0.009, − 0.102, 0.030, 0.038, 0.029, 0.034, 0.131). We can reject the null \nFig. 2. Predicted vs actual distribution of giving in the standard dictator game. Red diamonds represent the \ndistribution of giving in the standard dictator games. Blue bars represent the distribution of giving predicted by \nGPT-4 in the same games. Error bars represent standard errors of the mean.\n \nFig. 1. Predicted vs actual average giving in the standard dictator game. Each dot represents an experiment \nwhere human participants played the standard dictator game. On the horizontal axis, we report the actual \naverage giving, on the vertical axis we report the average giving predicted by GPT-4. The red line corresponds \nto the 45° line.\n \nScientific Reports |        (2025) 15:21428 4| https://doi.org/10.1038/s41598-025-01715-7\nwww.nature.com/scientificreports/\nhypothesis that the error component ES(i) is equal to zero (p < 0.001) for all components except ES(1), ES(3), \nand ES(4)( p = 0.417, p = 0.822, and p = 0.334, respectively).\nIn summary, while GPT-4 was able to qualitatively predict that the distribution of choices would exhibit \nthree peaks, its quantitative predictions were imprecise. Specifically, GPT-4 underestimated the proportion \nof individuals who distribute the amount equally and the proportion of individuals who keep all the money \nfor themselves, while it overestimated the percentage of people who give the entire sum to the recipient. A \nmore granular comparison of predicted frequencies vs actual frequencies showed that GPT-4 overestimated the \nfrequency of each giving level greater than half of the endowment.\nGPT-4’s predictions in the extreme dictator games\nWe then proceed to analyze the “extreme dictator games” introduced in 35. These games were specifically \ndesigned to investigate the influence of language on altruistic behavior 36. Six variations of the dictator game \nwere conducted, each differing only by the verb used to describe the available actions (i.e., “boost, ” “steal, ” “give, ” \n“donate, ” “take, ” “demand”). Although these games were economically identical, the level of altruistic behavior \nvaried significantly depending on the verb choice, ranging from 5% in the “boost” condition to 29% in the “steal” \ncondition (refer to Fig.  3, red diamonds). In particular, the “boost” condition led to a significantly lower level \nof altruism compared to the “donate” , “demand” , “take” , and “steal” conditions, whereas the “steal” condition \nproduced a higher level of altruism compared to all other conditions.\nIn comparison, GPT-4 also predicted that the linguistic frame would impact altruistic behavior to \nsome extent (see Fig.  3, blue bars). To enhance precision, we prompted GPT-4 to predict behavior for each \ncondition eight times, from which we calculated a mean prediction. The error bars in the figure indicate \nthe standard error of the mean. Notably, GPT-4 accurately predicted that the “steal” frame would result \nin higher levels of giving. However, it failed to predict other patterns, such as the “boost” frame leading to \nlower levels of altruism compared to the “donate” , “demand” , “take” , and “steal” conditions. Additionally, once \nagain the model’s estimates were significantly higher than the actual observed behavior (t = 17.793, p < 0.001). \nFor instance, in the “boost” condition, GPT-4 predicted that 36.8% of dictators would choose to boost the \nrecipient. In the “steal” condition, it predicted that 55% of dictators would opt not to steal. The signed errors \nwere EW,boost =0 .318, EW,give =0 .230, EW,donate =0 .222, EW,demand =0 .240, EW,take =0 .255, \nEW,steal =0 .245. We can reject the null hypothesis that these errors are zero in all frames (all p’s < 0.001).\nGPT-4’s predictions in the dictator game with a “take” option\nLastly, we examine the dictator game with a “take” option. Prior experimental findings on this game format \nhave established two regularities: (i) the frequency of giving more than zero decreases compared to the standard \ndictator game, and (ii) the peak at half-endowment tends to diminish37–39.\nGPT-4 correctly predicted both of these regularities. Specifically, regarding the first regularity, in the standard \ndictator games, the proportion of people who gave a positive amount was 77.6%. This proportion decreased to \n55.1% in the dictator game with a “take” option (t = − 4.491, p < 0.001). As for the second regularity, the peak \nat half-donors, which had a frequency of 23.7% in the standard dictator games, dropped to a frequency of only \n9.1% in dictator games with a “take” option (t = − 2.076, p = 0.040).\nHowever, despite GPT-4’s accurate qualitative predictions, its quantitative estimates were again higher than \nactual giving. The large language model predicted a mean giving of 0.177, in contrast to the actual average giving \nof − 0.166. This corresponds to a signed error EW= 0.343. We can reject the null hypothesis that this error is \nequal to zero (z = 3.130, p < 0.001). For comparative purposes, in Fig.  4 we report the frequencies obtained in 33 \nalongside those predicted by GPT-4.\nRobustness checks for GPT-4\nWe conducted several robustness checks. The GPT-4 estimates reported in the previous sections were made in \nJuly 2023. This raises two potential concerns: (i) since GPT-4 may improve over time, it is important to check \nFig. 3. Predicted vs actual frequency of monetary altruism in the extreme dictator games. Red diamonds \nrepresent the frequency of monetary altruistic choices in each of the six conditions of the extreme dictator \ngame reported in30. Blue bars represent the frequency of altruism in the six conditions predicted by GPT-4. \nError bars represent standard errors of the mean.\n \nScientific Reports |        (2025) 15:21428 5| https://doi.org/10.1038/s41598-025-01715-7\nwww.nature.com/scientificreports/\nwhether the results can be replicated with later versions of GPT-4, including GPT-4o; (ii) since the training data \nfor the July 2023 version of GPT-4 was truncated in April 2021, it is key to test whether the results also hold for \npapers made available online before 2021, because GPT-4 may potentially be more accurate in these instances. \nSimilarly, (iii) considering most of the training data are in English, it is crucial to check whether the results are \nconsistent specifically in English-speaking countries. As reported in SI Appendix 2, all the previous results are \nrobust to these specifications.\nFurthermore, we checked whether GPT’s predictions may be influenced by the length of the instructions \n(defined as the number of words in the text), the impact of the article (measured by the number of citations \naccording to Google Scholar), the type of participants (students or not), the percentage of the total stake \ninitially assigned to the dictator, the publication year of the articles (before or after 2021), and the location of \nthe experiment (English-speaking countries). To this end, we conducted a regression analysis with dependent \nvariable the accuracy of GPT predictions, defined as the difference between the predicted average by GPT-4 and \nthe actual average reported in each study. The results show that none of the determinants described above have \na statistically significant effect on prediction accuracy. See Table S2 in SI Appendix 2 for the regression table.\nOne possible explanation is that GPT-4 generated its estimates by retrieving data from academic papers \navailable online. If this were the case, we would expect two outcomes: first, that its performance would be \nbetter for studies published before April 2021—when GPT-4’s training data cutoff occurred; second, that its \npredictions would be correlated with actual human responses. As shown above, the first expectation is not met. \nRegarding the second, we found no significant correlation between GPT-4’s predictions and actual outcomes in \nthe standard dictator game (p = 0.860), the dictator game with a take option (p = 0.100), or the extreme dictator \ngame (p = 0.078). Taken together, these findings suggest that GPT-4 is not simply retrieving and synthesizing \nexisting empirical results from the web.\nBard and bing’s predictions\nWe also attempted to utilize Bard and Bing as alternative chatbots. However, it appears that they both perform \nnotably worse than GPT-4 in this particular task. They often encounter difficulties in comprehending the \nprompt, display instances of “hallucination” , or become trapped in repetitive errors without any clear way out. \nFurther details can be found in SI Appendix 3 and 4.\nDiscussion\nThis article makes two contributions: (i) it introduces a publicly available benchmark designed to evaluate LLMs’ \ncapabilities in estimating how humans balance monetary self-interest with the interests of others; (ii) it applies \nthis benchmark to study the performance of four advanced chatbots.\nPrevious research has introduced benchmarks for evaluating LLMs’ abilities in various domains, including \nreading comprehension40,41, mathematical abilities 42, reasoning about physical common sense 43, numerical \nabilities over financial data 44, and common-sense reasoning 45,46. While these benchmarks assess important \nabilities, they overlook a class of decisions of fundamental importance, given the increasing potential for using \nLLMs as decision-making assistants: social decisions.\nThis work presents a publicly available benchmark aimed specifically at evaluating how well LLMs can \npredict the balance between monetary self-interest and the interests of others in decision-making processes. The \nclosest existing research we are aware of is ref. 47, which also focuses on decision-making. However, their work \nprimarily uses game theory to benchmark decision-making, comparing LLMs’ decisions to Nash equilibria, \nrather than actual human behavior. While insightful, this approach does not allow to measure LLMs’ ability to \nsimulate real-world human decision-making, which this new benchmark aims to address.\nFig. 4. Predicted vs actual distribution of giving in the dictator game with a “take” option. Red diamonds \nrepresent the distribution of giving in the dictator games with a “take” option reported in37. Blue bars represent \nthe distribution of giving in the dictator games with a “take” option predicted by GPT-4. Error bars represent \nstandard errors of the mean.\n \nScientific Reports |        (2025) 15:21428 6| https://doi.org/10.1038/s41598-025-01715-7\nwww.nature.com/scientificreports/\nWe studied the capability of four advanced generative AI-based chatbots (GPT-4, GPT-4o, Bard, Bing) to \nestimate how humans trade off monetary self-interest and the interest of others. We found that only GPT-4 and \nGPT-4o were capable of qualitatively predicting human behavioral patterns in various dictator game formats \nand linguistic frames. However, they consistently overestimated the average level of giving. Specifically, this \n“optimistic bias” descends from an overestimation of the frequency of monetary altruistic behavior, while \nunderestimating the frequency of monetary self-interest.\nOne might hypothesize that GPT-4 has solely relied on retrieving information from papers available on the \nweb to formulate its estimates. However, if this were true, not only its estimates should have been quantitatively \naccurate (or at least correlated with actual human behavior), but also its accuracy should have improved for \nthose studies that were accessible online by GPT-4 at the time of our analysis, i.e., those published before April \n2021. However, our robustness checks indicate that this is not the case, as the accuracy did not improve for these \nstudies and GPT-4 predictions were not significantly correlated with human behavior. Therefore, there is no \nevidence that GPT-4 has retrieved information from papers available on the web.\nOverall, these findings align with previous research comparing human decisions to those generated by \nGPT-3.5 in similar economic decision-making contexts6,26,48. For example, one study involving a series of mini-\ndictator games found that the estimated altruism parameter for GPT-3.5 was significantly higher than that \nobserved for human participants6.\nThis “optimistic bias” may be attributed to GPT-4’s training datasets, as is commonly explained when GPTs \ndisplay biases, such as in the case of gender or racial biases49,50. However, if this were the case, we would see the \nsame “optimistic bias” also among humans. Instead, this “optimistic bias” does not appear to be shared by humans. \nFor example, ref.51 even find a “pessimistic bias” , whereby individuals tend to underestimate others’ prosociality \nand overestimate selfishness. In contrast, Ref. 52 found that human predictions generally align with observed \nbehavior when the dictator is male, but tend to overestimate the frequency of inequity-averse behavior when the \ndictator is female. These findings suggest that GPT-4’s tendency to overpredict monetary altruistic behavior may \nreflect specific features of LLM training or inference rather than mirroring typical human decisions.\nIn fact, as an alternative explanation, it is possible that the reinforcement learning with human feedback  \n(RLHF) stage of GPT-4’s training, intended to enhance its ability to generate human-like language and respond \nappropriately to different inputs, may have contributed indirectly to its adoption of an over-optimistic view of \nhuman altruism16. Recent research indicates that several LLMs, including GPT-4, display liberal biases across \nmultiple domains—ranging from political preferences to decision-making in moral dilemmas and the gender \nassignment of protagonists in stereotypical contexts53–55. Furthermore, models that have not undergone RLHF \ndo not exhibit these biases 56, suggesting that RLHF may be contributing to outputs that are more “politically \ncorrect” . Consequently, the optimistic bias observed in our study might partly stem from these influences. \nNevertheless, the precise mechanisms behind it are difficult to ascertain due to the “black-box” nature of current \nchatbots. Future work could disentangle these possibilities by comparing the performance of LLMs that have \ngone through RLHF with base models56 and by comparing LLM predictions with those of both domain experts \nand non-experts to more comprehensively assess their relative accuracy and underlying biases.\nOur findings have implications for the use of LLMs as assistants in decision-making in social domains, as \nestimating people’s behavior is a precondition for LLMs to formulate behavioral guidance or recommendations. \nThis is true especially for users who expect LLMs to be correct and heavily rely on LLM-generated insights to form \ntheir beliefs. Overly optimistic expectations about human altruistic behavior may lead to disappointment13,57,58 \nand, in turn, potentially to frustration59 and even social conflict60 or ineffective and harmful decisions based on \ninaccurate predictions in public policy or business contexts, e.g., in the case of healthcare and environmental \npolicies. Related to this, future research could study users’ expectations regarding LLM-generated advice in \ndomains related to prosocial behavior and assess their reactions when confronted with biased outputs, to test \nthe extent to which users experience disappointment or frustration when receiving LLM-generated advice \nthat conflicts with their own beliefs or expectations. Furthermore, comparing the emotional impact of biased \nLLM-generated advice with that of human-generated advice could provide insights into whether human-AI \ninteractions elicit distinct reactions compared to traditional human–human exchanges.\nIt is important to note that we obtained our results using the default LLMs available online, without changing \nany model parameter. We made this methodological choice because this is how most users engage with LLMs. \nHowever, aware of this limitation, we encourage future work investigating how changes in temperature and other \nmodel parameters affect predictions. Furthermore, it is important to highlight that we used one-shot, simple \nprompts asking the LLMs to estimate giving behavior. Future work could investigate the effects of more complex \nprompting strategies, including iterative feedback and exposure to external summaries such as meta-analyses. \nThis would shed light on whether and how LLMs can self-correct and potentially improve their predictive \naccuracy, contributing to broader conversations around human-AI collaboration.\nOur work also contributes to the growing literature on the research-based use of generative AI. Prior studies \nsuggest that GPT-3, GPT-3.5, and GPT-4 responses are often aligned with those of human participants 6,24–30, \nraising the possibility that synthetic AI participants could substitute for humans in certain experimental settings61. \nIn contrast, we find that while GPT-4 and GPT-4o can qualitatively capture key behavioral patterns, they fail to \nproduce quantitatively accurate predictions, at least in the context of the dictator game. These findings suggest \nimportant limitations in relying on synthetic data in behavioral research, and offer a more cautious perspective \nwithin the broader discussion about the potential for LLMs to replace human participants.\nMethods\nWe gathered experimental instructions for dictator games from diverse sources. First, we issued a public call \non the forums of the Economic Science Association (ESA) and the Society for Judgment and Decision Making \n(SJDM), inviting behavioral scientists to contribute instructions from dictator game experiments they had \nScientific Reports |        (2025) 15:21428 7| https://doi.org/10.1038/s41598-025-01715-7\nwww.nature.com/scientificreports/\nconducted. Additionally, we conducted manual searches in the relevant literature to supplement the instructions \ncollected through the forums. This data collection process resulted in 106 unique experimental instructions for \ndictator games. These instructions were drawn from 38 different research articles and represented experiments \nconducted in 12 distinct countries.\nWe obtained the mean donation values and choice distributions from the corresponding papers or directly \nfrom the authors for each of the games included in the analysis. In cases where the distributions of choices \nwere not explicitly provided but were represented in histograms in the papers, we employed a graphic design \nsoftware (GIMP) to estimate the frequency of each choice. We utilized the “measure” tool within the software \nto evaluate the height of each column in the histograms. By vertically traversing from the top to the bottom of \neach column, we determined the measurement indicating the vertical distance between the column’s top and its \nbase. After measuring all the columns, we used the reference frequency reported on the y-axis to determine the \nproportional relationships between the columns and estimate the frequency of each choice.\nNext, we utilized GPT-4 (or GPT-4o, Bard, or Bing) to estimate the distribution of choices in each dictator \ngame. The prompt structure remained consistent across all games. We started by asking GPT-4 to “Please read \nthe following decision problem” , followed by copying the specific instructions of the game. Then, we asked GPT-\n4 to imagine a population of 1,000 people living in the country where the experiment was conducted, and \nestimate the distribution of their choices. The prompt requested GPT-4 to report the estimated number of people \nmaking each choice, along with the corresponding error, in the format X ± Y . An example of the prompt provided \nto GPT-4, along with its response, is presented in Table 1 in the main text.\nTo prevent any learning within the chat and maintain the integrity of the analysis, we deleted the chat with \nGPT-4 after collecting the estimations for each experiment, ensuring that subsequent estimations were not \ninfluenced by prior conversations.\nAs some experimental instructions were in languages other than English, we relied on translations provided \nby the authors of the corresponding papers. While using translated instructions may introduce some degree \nof error, we addressed this concern by conducting a robustness check. Specifically, we repeated the analysis \nrestricting the sample to studies conducted in English-speaking countries, namely the USA, UK, Australia, and \nIndia. Importantly, all results were replicated in these restricted samples (see SI Appendix 2).\nIn a handful of cases (11 out of 106), and particularly when the endowment is not a multiple of 10, GPT-4 does \nnot provide estimations in a manner that can be easily translated into 10% increments. Consider, for instance, \nthe following instructions from62:\nOur prompt:\nPlease read the following decision problem:\nY ou have been assigned into the role of a decision-maker. Y ou have 16 euros. It is your job to decide if you want to give some of the 16 euros \nto the recipient who has been randomly assigned to you. Should you choose to give them something, it is for you to decide how much you \nwant to give\nNow imagine that there is a population of 1,000 people living in Finland facing this decision problem. How would you estimate the \ndistribution of their choices? Please, for each choice, report a result in the form X ± Y , where X is the estimated number of people making \ntheir choice, and Y is the error\n \nGPT-4 provides the following answer:\nGPT-4’s reply:\nThe distribution of choices can be estimated based on various factors such as social norms, personal values, and economic circumstances, \nbut it’s challenging to assign precise numbers without access to specific data about the population in question\nAssuming that the population in Finland follows some general trends of generosity and sharing seen in many societies, an estimation might \nbe as follows:\n1. Giving nothing (0 euro): 200 ± 50 people. Some people may choose not to give anything due to their personal beliefs, financial situation, \nor lack of trust\n2. Giving a small amount (1–4 euro): 400 ± 50 people. Many people might prefer to share a small portion of their money, which is seen as a \ntoken of goodwill without significantly affecting their own wealth\n3. Giving a moderate amount (5–9 euro): 250 ± 50 people. Some individuals might give about half of their money, balancing their personal \nfinancial needs with a sense of fairness and empathy\n4. Giving a generous amount (10–15 euro): 100 ± 30 people. A smaller group might be inclined to give the majority of their money, perhaps \ndriven by altruistic values or a desire to make a significant impact on the recipient\n5. Giving everything (16 euro): 50 ± 20 people. The least common decision might be to give all of their money. This could reflect particularly \nstrong altruistic values or a minimal concern for personal financial loss in this context\nThis distribution is just an estimation and can vary significantly based on numerous factors such as cultural, socioeconomic, or \npsychological differences within the population. To obtain a more accurate distribution, a survey or experiment within the specific \npopulation would be necessary\n \nTo transform these estimations into 10% increment steps, we proceeded in two ways. First, we asked GPT-4 \nto recast the estimations in 10% increment steps. On its first attempt, GPT-4 produced a new set of estimations \nthat did not even add up to 1,000. Consequently, we asked GPT-4 to maintain the same underlying distribution \nas in its initial response, and to ensure that the estimated number of people in the various categories summed up \nto 1,000. However, despite these explicit instructions, GPT-4 was unable to provide estimations that summed up \nto 1,000. After several trials, GPT-4 consistently failed to provide a set of estimations that added up to 1,000. This \nfailure likely stems from GPT-4’s lack of planning ahead that was observed also in other studies63,64. Therefore, \nwe resorted to mathematically transforming the estimations provided by GPT-4 into 10% increment steps.\nTo achieve this, we divided each category proposed by GPT-4 into 10% increment steps; e.g., the “giving a small \namount” category was sectioned into two and a half blocks, corresponding to the intervals (0,10%], (10%, 20%], \nand (20%,25%]. Then, the estimated 400 people in this category were uniformly spread across these categories, \nScientific Reports |        (2025) 15:21428 8| https://doi.org/10.1038/s41598-025-01715-7\nwww.nature.com/scientificreports/\nyielding 160 subjects for the full blocks and 80 subjects for the half block. We followed this approach for all the \ncategories. For simplicity, if a category spanned more than half of a 10% interval, we placed the whole interval \nwithin that category. The outcome of this procedure is reported in the table below. We acknowledge that this \nmethod may introduce a potential error in the estimations. Therefore, as an additional robustness check, we \nreplicated all analyses excluding instructions for which GPT-4 does not provide results in 10% increment steps. \nThe outcomes were qualitatively similar and are reported in SI Appendix 2.\nPercentage of giving Estimated proportion\n0% 0.2\n10% 0.16\n20% 0.16\n30% 0.1425\n40% 0.0625\n50% 0.0625\n60% 0.0625\n70% 0.0333\n80% 0.0333\n90% 0.0333\n100% 0.05\n \nData availability\nAll data and code used in the analysis are available on https://osf.io/4gkb3/.\nReceived: 17 July 2024; Accepted: 6 May 2025\nReferences\n 1. Epstein, Z. et al. Art and the science of generative AI. Science 380, 1110–1111 (2023).\n 2. Xing, F . Z., Cambria, E. & Welsch, R. E. Natural language based financial forecasting: A survey. Artif. Intell. Rev. 50, 49–73 (2018).\n 3. Lee, P ., Bubeck, S. & Petro, J. Benefits, limits, and risks of GPT-4 as an AI chatbot for medicine. N. Engl. J. Med. 388, 1233–1239 \n(2023).\n 4. Noy, S. & Zhang, W . Experimental evidence on the productivity effects of generative artificial intelligence. Science 381, 187–192 \n(2023).\n 5. Brynjolfsson, E., Li, D. & Raymond, L. R. Generative AI at work. Natl. Bur. Econ. Res. (2023).\n 6. Chen, Y ., Liu, T. X., Shan, Y . & Zhong, S. The emergence of economic rationality of GPT. Proc. Natl. Acad. Sci. U.S.A.  120, \ne2316205120 (2023).\n 7. Kleinberg, J., Lakkaraju, H., Leskovec, J., Ludwig, J. & Mullainathan, S. Human decisions and machine predictions. Q. J. Econ. 133, \n237–293 (2018).\n 8. Mullainathan, S. & Obermeyer, Z. Diagnosing physician error: A machine learning approach to low-value health care. Q. J. Econ. \n137, 679–727 (2022).\n 9. Sunstein, C. R. The use of algorithms in society. Rev. Austrian Econ. 1–22 (2023).\n 10. Capraro, V ., Lentsch, A., Acemoglu, D., Akgun, S., Akhmedova, A., Bilancini, E., & Viale, R. The impact of generative artificial \nintelligence on socioeconomic inequalities and policy making. PNAS Nexus 3(6), 191 (2024).\n 11. Fischbacher, U., Gächter, S. & Fehr, E. Are people conditionally cooperative? Evidence from a public goods experiment. Econ. Lett. \n71, 397–404 (2001).\n 12. Vlasceanu, M. et al. Addressing climate change with behavioral science: A global intervention tournament in 63 countries. Sci. Adv. \n10, eadj5778 (2024).\n 13. Pizziol, V . & Tavoni, A. From niches to norms: the promise of social tipping interventions to scale climate action. NPJ Clim. Action \n3, 46 (2024).\n 14. Shepperd, J. A., Klein, W . M., Waters, E. A. & Weinstein, N. D. Taking stock of unrealistic optimism. Perspect. Psychol. Sci. 8, \n395–411 (2013).\n 15. Kim, C., Kannan, P . K., Trusov, M. & Ordanini, A. Modeling dynamics in crowdfunding. Mark. Sci. 39, 339–365 (2020).\n 16. Schmidt, E. M., Bonati, S., Köbis, N., & Soraperra, I. GPT-3.5 altruistic advice is sensitive to reciprocal concerns but not to strategic \nrisk. Sci. Rep. 14(1), 22274 (2024).\n 17. Haidt, J. & Joseph, C. Intuitive ethics: How innately prepared intuitions generate culturally variable virtues. Daedalus 133(4), \n55–66 (2004).\n 18. Graham, J., Haidt, J. & Nosek, B. A. Liberals and conservatives rely on different sets of moral foundations. J. Pers. Soc. Psychol. \n96(5), 1029 (2009).\n 19. Atari, M. et al. Morality beyond the WEIRD: How the nomological network of morality varies across cultures. J. Pers. Soc. Psychol. \n125(5), 1157 (2023).\n 20. Curry, O. S. Morality as cooperation: A problem-centred approach. Evol. Moral. 27–51 (2016).\n 21. Curry, O. S., Mullins, D. A. & Whitehouse, H. Is it good to cooperate? Testing the theory of morality-as-cooperation in 60 societies. \nCurr. Anthropol. 60(1), 47–69 (2019).\n 22. Camerer, C. Behavioral Game Theory: Experiments in Strategic Interaction (Russell Sage Foundation; Princeton Univ Press, 2003).\n 23. Kahneman, D., Knetsch, J. L. & Thaler, R. H. Fairness and the assumptions of economics. J. Bus. 59, S285–S300 (1986).\n 24. Forsythe, R., Horowitz, J. L., Savin, N. E. & Sefton, M. Fairness in simple bargaining experiments. Games Econ. Behav. 6, 347–369 \n(1994).\n 25. Aher, G. V ., Arriaga, R. I. & Kalai, A. T. Using large language models to simulate multiple humans and replicate human subject \nstudies. In Int. Conf. Mach. Learn., 337–371 (PMLR, 2023, July).\n 26. Akata, E., Schulz, L., Coda-Forno, J., Oh, S. J., Bethge, M. & Schulz, E. Playing repeated games with Large Language Models. \nPreprint at https://arxiv.org/abs/2305.16867 (2023).\nScientific Reports |        (2025) 15:21428 9| https://doi.org/10.1038/s41598-025-01715-7\nwww.nature.com/scientificreports/\n 27. Brookins, P . & DeBacker, J. M. Playing games with GPT: What can we learn about a large language model from canonical strategic \ngames?. Preprint at https://ssrn.com/abstract=4493398 (2023).\n 28. Dillion, D., Tandon, N., Gu, Y . & Gray, K. Can AI language models replace human participants?. Trends Cogn. Sci.(2023).\n 29. Kasberger, B., Martin, S., Normann, H. T. & Werner, T. Algorithmic Cooperation. Preprint at https://ssrn.com/abstract=4389647 \n(2023).\n 30. Guo, F . GPT Agents in Game Theory Experiments. Preprint at https://arxiv.org/abs/2305.05516 (2023).\n 31. Xie, C. et al. Can Large Language Model Agents Simulate Human Trust Behaviors?. Preprint at https://arxiv.org/abs/2402.04559 \n(2024).\n 32. Bauer, K., Liebich, L., Hinz, O. & Kosfeld, M. Decoding GPT’s Hidden ‘Rationality’ of Cooperation. Preprint at  h t t p s : / / s s r n . c o m / a \nb s t r a c t = 4 5 7 6 0 3 6     (2023).\n 33. Gomes, C. M. & McCullough, M. E. The effects of implicit religious primes on dictator game allocations: A preregistered replication \nexperiment. J. Exp. Psychol. Gen. 144, e94–e104 (2015).\n 34. Engel, C. Dictator games: A meta study. Exp. Econ. 14, 583–610 (2011).\n 35. Capraro, V . & Vanzo, A. The power of moral words: Loaded language generates framing effects in the extreme dictator game. \nJudgm. Decis. Mak. 14, 309–317 (2019).\n 36. Capraro, V ., Halpern, J. Y . & Perc, M. From outcome-based to language-based preferences. J. Econ. Lit. (2024).\n 37. List, J. A. On the interpretation of giving in dictator games. J. Polit. Econ. 115, 482–493 (2007).\n 38. Bardsley, N. Dictator game giving: altruism or artefact?. Exp. Econ. 11, 122–133 (2008).\n 39. Cappelen, A. W ., Nielsen, U. H., Sørensen, E. Ø., Tungodden, B. & Tyran, J. R. Give and take in dictator games. Econ. Lett. 118, \n280–283 (2013).\n 40. Lai, G., Xie, Q., Liu, H., Y ang, Y . & Hovy, E. Race: Large-scale reading comprehension dataset from examinations. Preprint at \nhttps://arxiv.org/abs/1704.04683 (2017).\n 41. Rajpurkar, P ., Zhang, J., Lopyrev, K. & Liang, P . Squad: 100,000+ questions for machine comprehension of text. Preprint at  h t t p s : / / \na r x i v . o r g / a b s / 1 6 0 6 . 0 5 2 5 0     (2016).\n 42. Saxton, D., Grefenstette, E., Hill, F . & Kohli, P . Analysing mathematical reasoning abilities of neural models. Preprint at  h t t p s : / / a r \nx i v . o r g / a b s / 1 9 0 4 . 0 1 5 5 7     (2019).\n 43. Bisk, Y ., Zellers, R., Gao, J. & Choi, Y . Piqa: Reasoning about physical commonsense in natural language. In Proc. AAAI Conf. Artif. \nIntell. 34, 7432–7439 (2020).\n 44. Chen, Z. et al. Finqa: A dataset of numerical reasoning over financial data. Preprint at https://arxiv.org/abs/2109.00122 (2021).\n 45. Zellers, R., Bisk, Y ., Schwartz, R. & Choi, Y . Swag: A large-scale adversarial dataset for grounded commonsense inference. Preprint \nat https://arxiv.org/abs/1808.05326 (2018).\n 46. Geva, M. et al. Did aristotle use a laptop? A question answering benchmark with implicit reasoning strategies. Trans. Assoc. \nComput. Linguist. 9, 346–361 (2021).\n 47. Huang, J. T. et al. How far are we on the decision-making of LLMs? Evaluating LLMs’ gaming ability in multi-agent environments. \nPreprint at https://arxiv.org/abs/2403.11807 (2024).\n 48. Fontana, N., Pierri, F ., & Aiello, L. M. Nicer Than Humans: How do Large Language Models Behave in the Prisoner’s Dilemma?. \narXiv preprint arXiv:2406.13605 (2024).\n 49. DeCamp, M. & Lindvall, C. Mitigating bias in AI at the point of care. Science 381, 150–152 (2023).\n 50. Ornes, S. Peering inside the black box of AI. Proc. Natl. Acad. Sci. U.S.A. 120, e2307432120 (2023).\n 51. Molnár, A., & Heintz, C. Beliefs About People’s Prosociality: Eliciting predictions in dictator games. CEU: Department of Economics-\nWorking Paper, 19 (2016).\n 52. Brañas-Garza, P ., Capraro, V . & Rascon-Ramirez, E. Gender differences in altruism on mechanical turk: Expectations and actual \nbehaviour. Econ. Lett. 170, 19–23 (2018).\n 53. Fulgu, R. A. & Capraro, V . Surprising gender biases in GPT. Comput. Hum. Behav. Rep. 16, 100533 (2024).\n 54. Motoki, F ., Pinho Neto, V . & Rodrigues, V . More human than human: measuring ChatGPT political bias. Public Choice 198(1), \n3–23 (2024).\n 55. Motoki, F . Y ., Neto, V . P ., & Rangel, V . Assessing political bias and value misalignment in generative artificial intelligence. J.Econ. \nBehav. Org. 106904 (2025).\n 56. Rozado, D. The political preferences of LLMs. PloS One, 19(7), e0306621.\n 57. Bell, D. E. Disappointment in decision making under uncertainty. Oper. Res. 33, 1–27 (1985).\n 58. Carroll, P ., Sweeny, K. & Shepperd, J. A. Forsaking optimism. Rev. Gen. Psychol. 10, 56–73 (2006).\n 59. Clore, G. L., Schwarz, N. & Conway, M. Affective causes and consequences of social information processing. Handb. Soc. Cogn. 1, \n323–417 (1994).\n 60. Coser, L. A. Social conflict and the theory of social change. Br. J. Sociol. 8, 197–207 (1957).\n 61. Horton, J. J. Large language models as simulated economic agents: What can we learn from homo silicus? Preprint at  h t t p s : / / a r x i v \n. o r g / a b s / 2 3 0 1 . 0 7 5 4 3     (2023).\n 62. Herne, K., Hietanen, J. K., Lappalainen, O. & Palosaari, E. The influence of role awareness, empathy induction and trait empathy \non dictator game giving. PLoS ONE 17(3), e0262196 (2022).\n 63. Bubeck, S. et al. Sparks of artificial general intelligence: Early experiments with GPT-4. Preprint at https://arxiv.org/abs/2303.12712 \n(2023).\n 64. Dziri, N. et al. Faith and fate: Limits of transformers on compositionality. Adv. Neural. Inf. Process. Syst. 36, 70293–70332 (2023).\nAcknowledgements\nWe thank Redi Elmazi for assistance during materials collection. We thank Mikalai Korbit, the participants of \nthe 19th Annual Conference of the Italian Association for Cognitive Sciences at the University of Genova and \nthose of the BEE meeting at the IMT School for Advanced Studies Lucca for their comments. We are grateful \nto the behavioral scientists who responded to our call on the ESA and SJDM forums and provided their exper -\nimental instructions.\nAuthor contributions\nVC, RDP , VP contributed equally to this work.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nScientific Reports |        (2025) 15:21428 10| https://doi.org/10.1038/s41598-025-01715-7\nwww.nature.com/scientificreports/\nAdditional information\nSupplementary Information The online version contains supplementary material available at  h t t p s : / / d o i . o r g / 1 \n0 . 1 0 3 8 / s 4 1 5 9 8 - 0 2 5 - 0 1 7 1 5 - 7     .  \nCorrespondence and requests for materials should be addressed to V .C.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2025 \nScientific Reports |        (2025) 15:21428 11| https://doi.org/10.1038/s41598-025-01715-7\nwww.nature.com/scientificreports/",
  "topic": "Benchmark (surveying)",
  "concepts": [
    {
      "name": "Benchmark (surveying)",
      "score": 0.7087689638137817
    },
    {
      "name": "Computer science",
      "score": 0.6550591588020325
    },
    {
      "name": "Balance (ability)",
      "score": 0.6021895408630371
    },
    {
      "name": "Data science",
      "score": 0.33902084827423096
    },
    {
      "name": "Psychology",
      "score": 0.12418416142463684
    },
    {
      "name": "Neuroscience",
      "score": 0.060648441314697266
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I66752286",
      "name": "University of Milano-Bicocca",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I124601658",
      "name": "University of Parma",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I9360294",
      "name": "University of Bologna",
      "country": "IT"
    }
  ]
}