{
  "title": "Parameter-efficient fine-tuning on large protein language models improves signal peptide prediction",
  "url": "https://openalex.org/W4401021891",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2105522723",
      "name": "Shuai Zeng",
      "affiliations": [
        "University of Missouri"
      ]
    },
    {
      "id": "https://openalex.org/A2788354788",
      "name": "Duolin Wang",
      "affiliations": [
        "University of Missouri"
      ]
    },
    {
      "id": "https://openalex.org/A2096185210",
      "name": "Lei Jiang",
      "affiliations": [
        "University of Missouri"
      ]
    },
    {
      "id": "https://openalex.org/A2094446072",
      "name": "Dong Xu",
      "affiliations": [
        "University of Missouri"
      ]
    },
    {
      "id": "https://openalex.org/A2105522723",
      "name": "Shuai Zeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2788354788",
      "name": "Duolin Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096185210",
      "name": "Lei Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2094446072",
      "name": "Dong Xu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2792643794",
    "https://openalex.org/W2949676527",
    "https://openalex.org/W2912990441",
    "https://openalex.org/W2006201402",
    "https://openalex.org/W2123186393",
    "https://openalex.org/W2164751980",
    "https://openalex.org/W2161746138",
    "https://openalex.org/W1935158889",
    "https://openalex.org/W4386978016",
    "https://openalex.org/W2020969907",
    "https://openalex.org/W2004406780",
    "https://openalex.org/W2171709077",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W2164186284",
    "https://openalex.org/W2802730096",
    "https://openalex.org/W2047298552",
    "https://openalex.org/W3176693010",
    "https://openalex.org/W2126975650",
    "https://openalex.org/W3129049721",
    "https://openalex.org/W1971147414",
    "https://openalex.org/W2124088653",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W4285247752",
    "https://openalex.org/W3179485843",
    "https://openalex.org/W2171091522",
    "https://openalex.org/W2809247618",
    "https://openalex.org/W2061385538",
    "https://openalex.org/W3001338162",
    "https://openalex.org/W2094201044",
    "https://openalex.org/W1965154000",
    "https://openalex.org/W2164025376",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W2776525063",
    "https://openalex.org/W2076048958",
    "https://openalex.org/W2102461176",
    "https://openalex.org/W4205192056",
    "https://openalex.org/W2060300932",
    "https://openalex.org/W2737196618",
    "https://openalex.org/W2600277188",
    "https://openalex.org/W4361229539",
    "https://openalex.org/W3211728297",
    "https://openalex.org/W4206950245"
  ],
  "abstract": "Signal peptides (SPs) play a crucial role in protein translocation in cells. The development of large protein language models (PLMs) and prompt-based learning provide a new opportunity for SP prediction, especially for the categories with limited annotated data. We present a parameter-efficient fine-tuning (PEFT) framework for SP prediction, PEFT-SP, to effectively utilize pretrained PLMs. We integrated low-rank adaptation (LoRA) into ESM-2 models to better leverage the protein sequence evolutionary knowledge of PLMs. Experiments show that PEFT-SP using LoRA enhances state-of-the-art results, leading to a maximum Matthews correlation coefficient (MCC) gain of 87.3% for SPs with small training samples and an overall MCC gain of 6.1%. Furthermore, we also employed two other PEFT methods, prompt tuning and adapter tuning, in ESM-2 for SP prediction. More elaborate experiments show that PEFT-SP using adapter tuning can also improve the state-of-the-art results by up to 28.1% MCC gain for SPs with small training samples and an overall MCC gain of 3.8%. LoRA requires fewer computing resources and less memory than the adapter tuning during the training stage, making it possible to adapt larger and more powerful protein models for SP prediction.",
  "full_text": null,
  "topic": "Adapter (computing)",
  "concepts": [
    {
      "name": "Adapter (computing)",
      "score": 0.858977198600769
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.7831236124038696
    },
    {
      "name": "Biology",
      "score": 0.5719160437583923
    },
    {
      "name": "Computer science",
      "score": 0.5082197189331055
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4015713930130005
    },
    {
      "name": "Machine learning",
      "score": 0.3762222230434418
    },
    {
      "name": "Computational biology",
      "score": 0.3358677923679352
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I76835614",
      "name": "University of Missouri",
      "country": "US"
    }
  ],
  "cited_by": 21
}