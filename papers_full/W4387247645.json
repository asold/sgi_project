{
    "title": "Mel-MViTv2: Enhanced Speech Emotion Recognition With Mel Spectrogram and Improved Multiscale Vision Transformers",
    "url": "https://openalex.org/W4387247645",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4364765659",
            "name": "Kah Liang Ong",
            "affiliations": [
                "Multimedia University"
            ]
        },
        {
            "id": "https://openalex.org/A2097415797",
            "name": "Chin-Poo Lee",
            "affiliations": [
                "Multimedia University"
            ]
        },
        {
            "id": "https://openalex.org/A2133491723",
            "name": "Heng Siong Lim",
            "affiliations": [
                "Multimedia University"
            ]
        },
        {
            "id": "https://openalex.org/A2053525470",
            "name": "Kian Ming Lim",
            "affiliations": [
                "Multimedia University"
            ]
        },
        {
            "id": "https://openalex.org/A2163176472",
            "name": "Ali Alqahtani",
            "affiliations": [
                "King Khalid University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4214614183",
        "https://openalex.org/W4312769131",
        "https://openalex.org/W2803193013",
        "https://openalex.org/W175750906",
        "https://openalex.org/W4362579608",
        "https://openalex.org/W4224293311",
        "https://openalex.org/W2795986449",
        "https://openalex.org/W2777468850",
        "https://openalex.org/W6755977018",
        "https://openalex.org/W2146334809",
        "https://openalex.org/W6767164110",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W4284899427",
        "https://openalex.org/W6760599669",
        "https://openalex.org/W3195259391",
        "https://openalex.org/W3170774791",
        "https://openalex.org/W4205567678",
        "https://openalex.org/W3201437689",
        "https://openalex.org/W2950518992",
        "https://openalex.org/W2968917279",
        "https://openalex.org/W2896101215",
        "https://openalex.org/W2923871787",
        "https://openalex.org/W1522301498"
    ],
    "abstract": "Speech emotion recognition aims to automatically identify and classify emotions from speech signals. It plays a crucial role in various applications such as human-computer interaction, affective computing, and social robotics. Over the years, researchers have proposed different approaches for speech emotion recognition, leveraging various classifiers and features. However, despite the advancements, existing methods in speech emotion recognition still have certain limitations. Some approaches rely on handcrafted features that may not capture the full complexity of emotional information present in speech signals, while others may suffer from a lack of robustness and generalization when applied to different datasets. To address these challenges, this paper proposes a speech emotion recognition method that combines Mel spectrogram with Short-Term Fourier Transform (Mel-STFT) and the Improved Multiscale Vision Transformers (MViTv2). The Mel-STFT spectrograms capture both the frequency and temporal information of speech signals, providing a more comprehensive representation of the emotional content. The MViTv2 classifier introduces multi-scale visual modeling with different stages and pooling attention mechanisms. MViTv2 incorporates relative positional embeddings and a residual pooling connection to effectively model the interactions between tokens in the space-time structure, preserve essential information, and improve the efficiency of the model. Experimental results demonstrate that the proposed method generalizes well on different datasets, achieving an accuracy of 91.51% on the Emo-DB dataset, 81.75% on the RAVDESS dataset, and 64.03% on the IEMOCAP dataset.",
    "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1109/ACCESS.2017.DOI\nMel-MViTv2: Enhanced Speech Emotion\nRecognition with Mel Spectrogram and\nImproved Multiscale Vision Transformers\nKAH LIANG ONG1, CHIN POO LEE1, HENG SIONG LIM2, KIAN MING LIM1, and ALI\nALQAHTANI3\n1Faculty of Information Science and Technology, Multimedia University, Jalan Ayer Keroh Lama, 75450, Melaka, Malaysia\n2Faculty of Engineering and Technology, Multimedia University, Melaka, 75450 Malaysia\n3Department of Computer Science, King Khalid University, Abha 61421, Saudi Arabia\n4Center for Artificial Intelligence (CAI), King Khalid University, Abha 61421, Saudi Arabia\nCorresponding author: Chin Poo Lee (e-mail: cplee@mmu.edu.my).\nResearch reported in this paper is funded by the TM R&D grant (RDTC/231075) and Deanship of Scientific Research, King Khalid\nUniversity, Saudi Arabia, under Grant number RGP2/332/44.\nABSTRACT Speech emotion recognition aims to automatically identify and classify emotions from\nspeech signals. It plays a crucial role in various applications such as human-computer interaction, affective\ncomputing, and social robotics. Over the years, researchers have proposed different approaches for speech\nemotion recognition, leveraging various classifiers and features. However, despite the advancements,\nexisting methods in speech emotion recognition still have certain limitations. Some approaches rely on\nhandcrafted features that may not capture the full complexity of emotional information present in speech\nsignals, while others may suffer from a lack of robustness and generalization when applied to different\ndatasets. To address these challenges, this paper proposes a speech emotion recognition method that\ncombines Mel spectrogram with Short-Term Fourier Transform (Mel-STFT) and the Improved Multiscale\nVision Transformers (MViTv2). The Mel-STFT spectrograms capture both the frequency and temporal\ninformation of speech signals, providing a more comprehensive representation of the emotional content.\nThe MViTv2 classifier introduces multi-scale visual modeling with different stages and pooling attention\nmechanisms. MViTv2 incorporates relative positional embeddings and a residual pooling connection to\neffectively model the interactions between tokens in the space-time structure, preserve essential information,\nand improve the efficiency of the model. Experimental results demonstrate that the proposed method\ngeneralizes well on different datasets, achieving an accuracy of 91.51% on the Emo-DB dataset, 81.75% on\nthe RA VDESS dataset, and 64.03% on the IEMOCAP dataset.\nINDEX TERMS Speech, Speech emotion, Speech emotion recognition, Spectrogram, Mel spectrogram,\nMel spectrogram with short-time Fourier transform, Vision transformer, Improved Multiscale Vision\nTransformers, Emo-DB, RA VDESS, IEMOCAP\nI. INTRODUCTION\nS\nPEECH emotion recognition is a significant task within\nthe field of signal processing and machine learning,\nfocused on detecting and analyzing emotional information\nconveyed through speech signals. Previous research has em-\nployed various techniques to extract emotional cues and\nclassify them into discrete emotional states. However, most\nexisting approaches rely on handcrafted time and frequency\ndomain features, which possess potential limitations. These\nlimitations include limited resolution in the time and fre-\nquency domains, impacting classification accuracy and dis-\ncriminative power.\nTo overcome these limitations, some studies have explored\nthe use of deep learning models such as Convolutional\nNeural Networks (CNNs) and Long Short-Term Memory\n(LSTM) networks for speech emotion recognition. CNNs\nexcel at capturing local patterns and spectral information,\nbut struggle with modeling long-term dependencies and se-\nquential dynamics present in speech signals. In contrast,\nLSTM networks effectively model temporal dependencies\nVOLUME 4, 2016 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321122\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nbut face challenges in capturing intricate spectral character-\nistics. These potential limitations emphasize the necessity\nfor novel approaches that overcome the shortcomings of\nhandcrafted features and explore sophisticated architectures\ncapable of capturing both spectral and temporal dynamics in\nspeech emotion recognition.\nTo address this, this paper proposes the “Mel-MViTv2”\nmethod, which combines the strengths of Mel spectro-\ngram with short-time Fourier transform (Mel-STFT) and\nImproved Multiscale Vision Transformers (MViTv2). Mel-\nSTFT merges the concepts of short-time Fourier transform\n(STFT) and Mel-frequency spectrogram to provide a de-\nscriptive representation of speech signals. STFT captures\nthe sinusoidal frequency and phase content of local signal\nsections over time, while the Mel-frequency spectrogram\napplies a non-linear transform to the frequency axis using\nthe Mel-scale, emphasizing perceptually important frequency\nranges.\nAdditionally, the paper employs Improved Multiscale Vi-\nsion Transformers (MViTv2) to learn and classify the Mel-\nSTFT representation. MViTv2 incorporates multi-scale vi-\nsual modeling by utilizing different stages instead of single-\nscale blocks in the Vision Transformer architecture. The\nnetwork gradually expands the channel width while reduc-\ning the resolution from input to output stages. MViTv2\nalso incorporates relative positional embeddings to capture\nrelative location distance between input tokens, enhancing\nshift-invariance properties. Furthermore, it employs resid-\nual pooling connections to enhance information flow and\naid in training pooling attention blocks while maintaining\nlow-complexity attention computation. This approach allows\nMViTv2 to extract features from multiple scales and reso-\nlutions, capturing fine details and larger context simultane-\nously, thereby enhancing the accuracy of speech emotion\nrecognition.\nThe combination of Mel-STFT and MViTv2 demonstrates\nremarkable performance in speech emotion recognition. Mel\nspectrograms provide visual representations of speech sig-\nnals, highlighting frequency content relevant for emotion\nanalysis. MViTv2, with its multiscale feature hierarchies\nand transformer-based architecture, classifies and recognizes\nemotional patterns within the Mel spectrograms. The hi-\nerarchical nature of the MViTv2 model captures complex\nrelationships between different scales of features, facilitating\nthe identification and understanding of emotional cues in the\nvisual representations of speech. Subsequently, the general-\nization capabilities of the Mel-MViTv2 method are evaluated\non three speech emotion datasets that encompass diverse\ncharacteristics such as gender, language, and recording envi-\nronments. This evaluation assesses how effectively the Mel-\nMViTv2 method can adapt to variations in these important\nfactors, providing insights into its robustness and applicabil-\nity across different contexts. The main contributions of this\npaper are:\n• The utilization of Mel-STFT in speech representation\nprovides a descriptive and informative representation\nthat effectively captures the intricate temporal and spec-\ntral characteristics of speech signals, thereby enhancing\nthe discriminative power and interpretability of speech\nanalysis in the context of speech emotion recognition.\nMel-STFT comprehensively captures both the nuanced\ntemporal dynamics and the perceptually significant fre-\nquency components, facilitating precise characterization\nof emotional content within speech signals.\n• The adoption of Improved Multiscale Vision Transform-\ners (MViTv2) for representation learning and classifi-\ncation of Mel-STFT further enhances the capability of\nthe model. MViTv2 excels in extracting hierarchical\nand multi-scale features, enabling the model to capture\nfine-grained variations and contextual information in\nthe representation. This multi-scale feature extraction\nallows the model to capture both fine details and larger\ncontext simultaneously, resulting in higher accuracy and\nimproved generalization ability in speech emotion clas-\nsification.\n• The performance evaluation on three diversified speech\nemotion datasets, namely the Berlin Database of Emo-\ntional Speech (Emo-DB), the Ryerson Audio-Visual\nDatabase of Emotional Speech and Song (RA VDESS),\nand the Interactive Emotional Dyadic Motion Capture\n(IEMOCAP) to assess the generalization capabilities of\nthe model.\nII. RELATED WORKS\nRecognizing emotions from speech signals is a complex task\ndue to the inherent variability in speech patterns and the\nsubjective nature of emotional expression. Researchers have\nmade significant progress in this field by employing various\napproaches ranging from traditional machine learning tech-\nniques to deep learning models. However, there are still chal-\nlenges that need to be addressed, such as the impact of cross-\ncultural differences in emotional expression and the need for\nlarger annotated datasets. This section provides an overview\nof the existing research in speech emotion recognition.\nZeng et al. (2017) [1] introduced a deep neural network\narchitecture called Gated Residual Neural Networks (GRes-\nNets) for recognizing emotions in speech. This architecture\ncombines the power of Deep Residual Networks with a\ngate mechanism that helps minimize the gradient exploding\nproblem and identify more informative features from the\nspectrogram. The output of this feature representation was\nevaluated on two tasks: emotion recognition from speech\nand speaker accent recognition. On the RA VDESS dataset,\nGResNets recorded an accuracy of 64.48% for multi-task\nrecognition, demonstrating its potential in speech emotion\nrecognition tasks.\nA deep belief networks (DBN) for speech emotion recog-\nnition was proposed by Latif et al. (2018) [2]. The proposed\nmethod utilized a cross-language and cross-corpus transfer\nlearning technique to improve the performance of speech\nemotion recognition. The speech signals were represented\nusing eGeMAPS feature set, which includes 88 features such\n2 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321122\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nas frequency, energy, spectral, cepstral and dynamic informa-\ntion. The proposed DBN obtained a recognition accuracy of\n54.77% on the IEMOCAP dataset and 72.38% on the Emo-\nDB dataset.\nSingh et al. (2021) [3] performed speech emotion recog-\nnition using a kernel-based support vector machine (SVM)\nwith radial basis function (RBF). The researchers utilized the\nscattering transform to extract both frequency domain and\ntime domain feature representations from the audio signals.\nThree feature representations were extracted using the scat-\ntering transform, namely frequency scattering (F-ScatNet),\ntime-domain scattering (ScatNet), and mel-frequency cep-\nstral coefficients (MFCC). The performance of the proposed\nRBF with SVM model was evaluated using these features.\nThe F-ScatNet feature representation achieved the highest\naccuracy among all three features, with 74.59% accuracy\non the Emo-DB dataset, 51.81% accuracy on the RA VDESS\ndataset, and 61.55% accuracy on the IEMOCAP dataset.\nHan et al. (2021) [4] suggested a parallel method\nfor speech emotion recognition, called ResNet CNN-\nTransformer which combines the Residual Neural Network\n(ResNet) and Convolutional Neural Network (CNN). A\ntransformer encoder was implemented to classify the fre-\nquency distribution of each emotion. The proposed method\nutilized mel spectrograms and MFCC feature representations\nfor training. The performance of the proposed ResNet-CNN-\nTransformer was evaluated on the RA VDESS dataset and\nyielded an accuracy of 80.89%.\nSimilarly, Jahangir et al. (2021) [5] proposed a com-\nbination speech emotion recognition model by combining\nthe CNN and Bi-directional Long Short-Term Memory (Bi-\nLSTM) classifier. The proposed model was trained on three\ndifferent feature representations, namely spectrogram, mel\nspectrogram, and MFCC. The results showed that the best\nperformance was achieved with MFCC features, with an\naccuracy of 82.35% on the Emo-DB dataset.\nMoreover, Swain et al. (2021) [6] devised a Concate-\nnated Convolution Neural Network model that applied Gated\nRecurrent Unit (CGRU). The GRU was utilized to learn\nthe small sequences of information from the prosodic and\nspectral features. The proposed method was evaluated on the\nRA VDESS dataset and recorded an accuracy of 73.26%.\nKerkeni et al. (2019) [7] utilized multiple classifiers and\nmajority voting approach for speech emotion recognition.\nThe approach used two classifiers, K-Nearest Neighbors\n(KNN) and SVM in combination with multiple feature ex-\ntraction techniques. To select the most relevant features, an it-\nerative neighborhood component analysis (INCA) technique\nwas applied. The INCA worked by selecting a subset of\nfeatures that capture the most relevant information in the\ninput. Then, the highest correlation target input was passed\ninto the two classifiers for voting. The proposed INCA with\nmajority voting method achieved 80.76% accuracy on the\nRA VDESS dataset.\nLikewise, Jha et al. (2022) [8] leveraged various machine\nlearning models for speech emotion recognition, such as\nGaussian Naive Bayes (GNB), Random Forest (RF), KNN,\nSVM, and Multilayer Perceptron (MLP). The proposed ap-\nproach combined prosodic and spectral features, including\nMFCC, linear frequency cepstral coefficients (LFCC), spec-\ntral centroids, formants, pitch, and intensity. The RA VDESS\ndataset was used to evaluate the performance of the ma-\nchine learning models. The best performance was achieved\nby MLP classifier using combined features, achieving an\naccuracy of 79.62%.\nZhang et al. (2021) [9] proposed a parallel model named\nHeterogeneous Parallel Convolution Bi-LSTM (HPCB) that\ncombined CNN and Bi-LSTM classifiers for speech emotion\nrecognition. The model used low-level descriptors (LLD)\nand high-level statistical functions (HSF), including Chroma,\nMFCC, mean of Chroma, and variance of MFCC. A total of\n70 acoustic features were used to train the HPCB model. The\nHPCB model obtained an accuracy of 84.65% on the Emo-\nDB dataset.\nAndayani et al. (2022) [10] combined the Long-Short\nTerm Memory with Transformer (LSTM-Transformer) for\nspeech emotion recognition. The audio signals were first\nprocessed using MFCC to extract the relevant features and\nthen fed into the proposed LSTM-Transformer model for\nclassification. The proposed model shown an accuracy of\n75.33% on the RA VDESS dataset.\nIn a recent study, Ong et al. (2023) [11] proposed a speech\nemotion recognition method with Light Gradient Boost-\ning Machine (LightGBM) referred to as the Emo-LGBM\nmethod. The data augmentation techniques, time stretching\nand pitch shifting, were applied to expand the dataset for\nmodel training. Following that, seven frequency domain and\ntime domain features were extracted from the augmented\naudio samples. Subsequently, the extracted features were\nutilized as input for the LightGBM method to classify the\nemotional state conveyed in speech. The proposed method\nachieved 84.91% accuracy on the Emo-DB dataset, 67.72%\non the RA VDESS dataset, and 62.94% on the IEMOCAP\ndataset.\nIII. SPEECH EMOTION RECOGNITION WITH MEL-STFT\nAND IMPROVED MULTISCALE VISION TRANSFORMERS\nThe research paper presents a methodology for speech emo-\ntion recognition that utilizes the Mel Spectrogram with Short-\nTime Fourier Transform (Mel-STFT) as a feature represen-\ntation technique. The Mel-STFT captures relevant acoustic\nfeatures by applying the STFT equation to short-time frames\nof the audio signal and converting the magnitude spectrum\ninto the Mel scale using triangular filterbanks.\nTo improve classification performance, the methodology\nincorporates the Improved Multiscale Vision Transformers\n(MViTv2) as the classifier. MViTv2 introduces multiple\nstages for capturing information at varying granularity levels,\nresulting in a comprehensive representation of the input data.\nIt addresses limitations of its predecessor by incorporating\nrelative positional embeddings to enhance space-time inter-\naction modeling and utilizing a residual pooling connection\nVOLUME 4, 2016 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321122\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\ntechnique to minimize information loss during pooling at-\ntention operations. Figure 1 shows the system flow of the\nproposed Mel-MViTv2.\nFIGURE 1. The system flow of Mel-MViTv2.\nA. MEL SPECTROGRAM WITH SHORT TIME FOURIER\nTRANSFORM\nThe Mel Spectrogram with Short-Time Fourier Transform\n(Mel-STFT) is a powerful audio visualization technique that\nconverts audio signals into visual representations. The Mel-\nSTFT combines the Short-Time Fourier Transform (STFT)\nwith frequency-to-Mel scale conversion to create a more\nperceptually relevant representation of audio. To generate the\nMel-STFT, the audio signal is first divided into short-time\nframes using the Hann window function,w(n). This window\nfunction is applied to a small segment of the audio signal at\na time and then shifted to cover the entire signal. The Hann\nwindow function helps in reducing the spectral leakage and\nimproving frequency resolution. The STFT equation is used\nto calculate the frequency content of the audio signal at each\npoint in time. The equation is given as follows:\nX(m, k) =\nN−1X\nn=0\nx(n + mH) × w(n) × e−j2πn k\nN (1)\nwhere m is the frame index, k is the frequency bin index,\nand j is the imaginary unit. The N refers to the length of\nthe window function used to segment the audio signal into\nshorter frames. The Hann window functionw(n) is applied to\neach frame to reduce spectral leakage and improve frequency\nresolution. The hop size H determines the amount of overlap\nbetween adjacent frames, affecting the time resolution of the\nSTFT.\nTo convert the frequency bin index k into Mel-scale, the\nfollowing equation is applied:\nMel(k) = 2595× log10(1 +f(k)/700) (2)\nwhere f(k) is the frequency corresponding to the frequency\nbin in Hz. The resulting Mel-scale is used to apply a filter-\nbank of triangular filters to the magnitude spectrum obtained\nfrom the STFT. The filterbank computes the energy in each\nfilter, which represents the distribution of energy in different\nfrequency bands over time. The resulting energy distribution\nis known as the Mel-spectrogram with Short-Time Fourier\nTransform.\nThe parameters of the Mel-STFT, such as the frame length\nand hop size, affect the time and frequency resolution of the\nspectrogram. In this experiment, a frame length of 4096 sam-\nples and a hop size of 256 samples are used. An example of a\nMel-STFT is shown in Figure 2, illustrating the distribution\nof energy in different frequency bands over time.\nFIGURE 2. An example of a Mel-STFT showing the distribution of energy in\ndifferent frequency bands over time.\nB. IMPROVED MULTISCALE VISION TRANSFORMERS\nThe Improved Multiscale Vision Transformers (MViTv2)\n[12] are an upgrade to MViTv1 [13] that introduces different\nstages for multi-scale visual modeling, instead of single-scale\nblocks in Vision Transformer. The channel width is slowly\nexpanded, while the resolution is reduced from input to out-\nput stages of the network. To perform downsampling within\na transformer block, MViTv1 introduces Pooling Attention.\nThis mechanism applies linear projections followed by pool-\ning operators to query, key, and value tensors. Pooling atten-\ntion enables resolution and computation reduction between\ndifferent stages by pooling the query, key and value tensors.\nGiven the input sequence X ∈ RL×D with sequence length\nL and channel width D, the pooling attention operations are\ndefined as:\nQ = PQ (XWQ) , K= PK (XWK) , V= PV (XWV )\n(3)\nwhere WQ, WK, WV ∈ RD×D denote the linear projections\nfor query tensor Q, key tensor K and value tensor V , while\nPQ, PK, PV denote the pooling operators for Q, K, and V .\nHowever, MViTv1 has some potential improvements,\nwhich are addressed in the MViTv2. The first issue addressed\nis the modeling of the interactions between tokens in the\nspace-time structure, which relies solely on the \"absolute\"\npositional embedding to offer location information. This ig-\nnores the fundamental principle of shift-invariance in vision,\nwhere the way MViT models the interaction between two\npatches will change depending on their absolute position in\nimages even if their relative positions stay unchanged. To\naddress this issue, MViTv2 incorporates relative positional\nembeddings, which only depend on the relative location\n4 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321122\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\ndistance between input tokens into the pooled self-attention\ncomputation. The relative distance between two input tokens\ni and j with spatial position p(i) and p(j) is encoded into a\npositional embedding Rp(i),p(j). More specifically, the com-\nputation of Rp(i),p(j) is decomposed as:\nRp(i),p(j) = Rh\nh(i),h(j) + Rw\nw(i),w(j) (4)\nwhere Rh and Rw are the positional embeddings along the\nheight and width axes. The symbols h(i) and h(j) represent\nthe vertical position of the token i and j, while w(i) and\nw(j) denote the horizontal position of the token i and j. The\npositional embedding is then integrated into the self-attention\nmodule as:\nAttn(Q, K, V) = Softmax\n\u0010\u0010\nQK⊤ + E(rel)\n\u0011\n/\n√\nd\n\u0011\nV\nwhere E(rel)\nij = Qi · Rp(i),p(j)\n(5)\nThe second issue addressed is the possible information\nloss in the pooling attention while reducing computation\ncomplexity and memory requirements in attention blocks. To\nthis end, the MViTv2 model employs a pooling technique\nknown as residual pooling connection with pooled Q tensor\nwhich significantly enhances the information flow and assists\nin training pooling attention blocks. This technique helps to\nmaintain low-complexity attention computation with large\nstrides in the key,K and value V pooling, thereby improving\nthe overall efficiency of the model. By applying the pooling\nconnection in the query Q tensor, the input sequence size re-\nmains unchanged and there is no additional learning compu-\ntation cost. The equation for the residual pooling connection\nis defined as:\nZ := Attn(Q, K, V) +Q (6)\nThe MViTv2 model integrates features from multiple\nscales, which enables the model to capture information at\ndifferent granularity levels, leading to improved accuracy and\nperformance. Additionally, MViTv2 utilizes the decomposed\nrelative position embedding and residual pooling connection\nto preserve essential information at a lower computation cost.\nFigure 3 illustrates the architecture of MViTv2.\nC. DATASETS\nIn order to ensure an objective evaluation of the proposed\nMel-MViTv2 method, it has been tested on three speech\nemotion datasets: the Berlin Database of Emotional Speech\n(Emo-DB), the Ryerson Audio-Visual Database of Emo-\ntional Speech and Song (RA VDESS), and the Interactive\nEmotional Dyadic Motion Capture (IEMOCAP).\nThe Emo-DB [14] is a well-established speech emotion\nrecognition dataset consisting of 535 audio samples from\nten professional German speakers, including an equal rep-\nresentation of 5 male and 5 female actors. This dataset\ncovers 7 distinct emotions, namely anger, boredom, neutral,\nhappiness, anxiety, sadness, and disgust.\nAnother widely recognized dataset employed in this evalu-\nation is the RA VDESS [15]. It comprises 1440 audio samples\nFIGURE 3. The architecture of Improved Multiscale Vision Transformers.\nin the English language, recorded by 24 professional actors,\nwith an equal distribution of 12 male and 12 female speakers.\nRA VDESS covers a wide spectrum of emotions, encompass-\ning neutral, calm, happy, sad, angry, fearful, disgust, and\nsurprised.\nThe IEMOCAP [16] dataset, chosen for its extensive\ncontent and prior utilization in related research, consists of\n5507 English-language audio samples recorded by five male\nand five female actors. In line with previous studies, our\nresearch focuses on analyzing four specific emotions within\nIEMOCAP: neutral, happiness, anger, and sadness.\nTABLE 1. Speech Emotion Recognition Datasets\nDataset Speakers Classes Samples\nEmo-DB [14] 10 (5M, 5F) 7 535\nRA VDESS [15] 24 (12M, 12F) 8 1440\nIEMOCAP [16] 10 (5M, 5F) 4 5507\nIV. EXPERIMENTS AND ANALYSIS\nTo ensure consistency and compatibility, the data samples\nwere resampled at a frequency of 44.1kHz. Subsequently,\nthese samples were transformed into Mel-STFT spectro-\ngrams and resized to a resolution of 244 × 244, meeting\nthe input size requirements of the MViTv2 classifier. The\nobtained spectrograms were then utilized as input for the\nMViTv2 model to perform speech emotion recognition. To\nensure a fair comparison with existing works, the three\ndatasets were divided into training and testing sets using an\n80:20 ratio.\nVOLUME 4, 2016 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321122\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nA. EXPERIMENTAL RESULTS WITH DIFFERENT\nSPECTROGRAMS\nThe effectiveness of speech emotion recognition using the\nMViTv2 model was evaluated by comparing four different\nspectrograms on three datasets: Emo-DB, RA VDESS, and\nIEMOCAP. Table 2 presents the results for the spectrograms,\nwhich were Linear-STFT, MFCC, CQT, and Mel-STFT with\nsame optimizer and learning rate. The results indicate that\namong the four different spectrograms, the Linear-STFT\nspectrogram exhibited the lowest accuracy. In contrast, the\nMel-STFT spectrogram displayed exceptional accuracy and\nefficiency across all three datasets. While the MFCC spec-\ntrogram provides reasonable accuracy, it exhibited compara-\ntively lower performance than the other methods. Although\nthe CQT spectrogram yielded satisfactory results, its overall\nperformance across the three datasets fell short of the Mel-\nSTFT spectrogram.\nTABLE 2. Experimental results of different spectrograms with MViTv2 as the\nclassifier. [Optimizer = Adam, Learning rate = 0.02]\nMethods Accuracy (%)\nEmo-DB RA VDESS IEMOCAP\nLinear-STFT with\nMViTv2\n79.25 63.16 57.95\nMFCC with MViTv2 76.42 76.14 61.67\nCQT with MViTv2 86.79 73.33 61.76\nMel-STFT with\nMViTv2\n90.57 81.75 63.49\nB. HYPERPARAMETER TUNING\nHyperparameter tuning is a critical process aimed at de-\ntermining the optimal hyperparameter settings for the Mel-\nMViTv2 method. By utilizing grid search, the hyperparam-\neter tuning phase focuses on two key hyperparameters: the\noptimizer and the learning rate. The optimizer plays a central\nrole in the model training procedure, as it aims to minimize\nthe loss function and guide the model towards attaining\nan optimal configuration of parameters that yield superior\nperformance. Simultaneously, the learning rate, a scalar hy-\nperparameter, governs the step size by which the optimizer\nadjusts the model’s parameters during each training iteration.\nThe optimizer and learning rate collectively determine the\nmagnitude of parameter updates and exert influence over the\nspeed and quality of the model’s convergence. In the hy-\nperparameter tuning phase, several popular optimizers were\nevaluated, including Adaptive Moment Estimation, Rectified\nAdam, and Quasi-Hyperbolic Adam. Additionally, three dis-\ntinct learning rates, specifically 0.01, 0.02, and 0.03, were\ntested to explore their impact on model performance.\nTable 3 shows the hyperparameter tuning results of the\nEmo-DB dataset, the highest accuracy of 91.51% was\nachieved using the Quasi-Hyperbolic Adam (QHAdam) [17]\noptimizer with a learning rate of 0.03. QHAdam is an opti-\nmizer that combines the advantages of the quasi-hyperbolic\nmomentum (QHM) algorithm with Adam. While Adam is\nwidely recognized for its effectiveness in large-scale training,\nQHAdam proves to be particularly valuable when working\nwith small datasets. QHAdam introduces a new weight up-\ndate rule by controlling the influence of the current and\npast gradient. By appropriately adjusting the unmodified and\nprevious gradients, QHAdam strikes a balance between mo-\nmentum and adaptive gradient scaling, leading to improved\nconvergence behavior. Hence, the QHAdam optimizer and\nlearning rate of 0.03 are chosen as the optimal optimizer for\nthe proposed Mel-MViTv2 method on the Emo-DB dataset.\nThe hyperparameter tuning results of the RA VDESS\ndataset are presented in Table 4, the highest accuracy of\n81.75% was achieved using the Adaptive Moment Estimation\n(Adam) [18] optimizer with a learning rate of 0.02. Adam\nis a first-order gradient-based optimization algorithm that\nutilizes adaptive estimates of lower-order moments. While\nAdam performs well in various scenarios, it offers particular\nadvantages for medium-sized datasets. One of its key features\nis adaptive learning rate adjustment, where the learning rate\nfor each parameter is adapted based on the magnitude of the\ngradients and historical gradient information. This adaptiv-\nity enables Adam to automatically adjust the learning rate\nduring training, making it well-suited for handling medium-\nsized datasets effectively. Therefore, the Adam optimizer and\nlearning rate of 0.02 are the optimal selection for the pro-\nposed Mel-MViTv2 method when applied to the RA VDESS\ndataset.\nTable 5 outlines the hyperparameter tuning results of the\nIEMOCAP dataset, the highest accuracy of 64.03% was\nachieved using the Rectified Adam (RAdam) [19] optimizer\nwith a learning rate of 0.02. RAdam is a variant of the Adam\noptimizer that aims to address the limitations of the original\nAdam optimizer, specifically the issue of unstable adaptive\nlearning rates during the early stages of training when dealing\nwith large datasets. RAdam incorporates a rectified behavior\nbeyond a specific threshold, which enhances the stability\nof the learning rate and improves the overall training pro-\ncess. By adjusting its behavior, RAdam provides improved\nperformance during the early training iterations, resulting\nin enhanced optimization outcomes. The RAdam optimizer\nand learning rate of 0.02 are chosen for the proposed Mel-\nMViTv2 method when working with the IEMOCAP dataset.\nTABLE 3. Experimental results in accuracy (%) on Emo-DB dataset with\ndifferent optimizers and learning rates.\nOptimizer Learning Rate\n0.01 0.02 0.03\nAdam 83.02 90.57 85.85\nRAdam 80.19 88.68 84.91\nQHAdam 87.74 84.91 91.51\n6 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321122\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 4. Experimental results in accuracy (%) on RAVDESS dataset with\ndifferent optimizers and learning rates.\nOptimizer Learning Rate\n0.01 0.02 0.03\nAdam 76.49 81.75 80.70\nRAdam 80.35 77.89 81.40\nQHAdam 79.30 79.30 80.70\nTABLE 5. Experimental results in accuracy (%) on IEMOCAP dataset with\ndifferent optimizers and learning rates.\nOptimizer Learning Rate\n0.01 0.02 0.03\nAdam 59.58 63.49 63.40\nRAdam 60.13 64.03 62.31\nQHAdam 62.31 62.31 61.94\nC. COMPARATIVE RESULTS WITH EXISTING WORKS\nBased on the results presented in Table 6, the proposed Mel-\nMViTv2 method demonstrated superior performance com-\npared to existing methods in the field of emotion recognition\nin speech. On the Emo-DB dataset, the proposed method\nachieved an impressive accuracy of 91.51%. This accuracy\noutperformed all existing methods, whose accuracies ranged\nfrom 58.39% to 84.91%. The diverse nature of the emotional\ndata in Emo-DB requires robust and adaptive methods for\naccurate recognition. The high accuracy achieved by the\nproposed Mel-STFT with MViTv2 approach, suggests that\nthe method can effectively capture and analyze the discrimi-\nnative emotional features present in the database.\nFor the RA VDESS dataset, the proposed Mel-MViTv2\nmethod yielded an accuracy of 81.75%. This method demon-\nstrated an improvement of 0.86% compared to the best-\nperforming method, ResNet-CNN-Transformer [4]. The\nRA VDESS dataset is known for its relatively challenging\nnature, primarily due to the larger number of speakers it\nencompasses. This larger speaker count leads to inherently\nhigher inter-subject variations, making emotion recognition\nmore complex and demanding.\nAll existing methods tend to exhibit relatively lower per-\nformance on the IEMOCAP dataset, primarily due to its\nunique characteristics. The dataset captures emotional ex-\npressions in dyadic sessions, where interactions between\nactors occur, potentially resulting in mixtures of emotions\nwithin the samples. However, despite this challenge, the pro-\nposed Mel-STFT with MViTv2 method managed to record\nan accuracy of 64.03%. This accuracy surpassed the range of\n55.54% to 62.94% achieved by the methods in comparison.\nThe results indicate that the proposed Mel-MViTv2\nmethod outperforms existing methods on all three datasets,\nnamely Emo-DB, RA VDESS, and IEMOCAP. The mel-\nSTFT features are effective in capturing relevant acoustic\ninformation related to speech, such as spectral characteristics\nand energy distribution. The mel-frequency scale represents\nTABLE 6. Comparative results on Emo-DB, RAVDESS, IEMOCAP dataset.\nMethods Accuracy (%)\nEmo-DB RA VDESS IEMOCAP\nGResNets [1] - 64.48 -\nDBN [2] 72.38 54.77 -\nF-ScatNet features\nwith SVM [3]\n74.59 51.81 61.55\nScatNet features with\nSVM [3]\n74.40 50.00 60.41\nMFCC features with\nSVM [3]\n58.39 36.74 55.54\nResNet-CNN-\nTransformer [4]\n- 80.89 -\nCNN-BLSTM [5] 82.35 - -\nCGRU [6] - 73.26 -\nINCA with Majority\nV oting [7]\n- 80.76 -\nMLP [8] - 79.62 -\nSVM [8] - 77.86 -\nHPCB [9] 84.65 - -\nLSTM-\nTransformer [10]\n- 75.33 -\nEmo-LGBM [11] 84.91 67.72 62.94\nMel-MViTv2\n(Proposed)\n91.51 81.75 64.03\nthe perception of pitch and frequency in a manner more\naligned with human hearing, allowing the model to focus\non important aspects of the audio signal. The MViTv2 ar-\nchitecture leverages the power of the transformer model to\ncapture complex patterns and long-range dependencies in\nsequential data, making them well-suited for analyzing mel-\nSTFT spectrograms. Not only that, multiscale feature hier-\narchies, a key aspect of the MViTv2 architecture, facilitate\nthe modeling of mel-STFT at multiple levels of abstraction.\nMel-STFT spectrograms contain both local and global acous-\ntic information, and the hierarchical nature of the MViTv2\narchitecture enables the extraction of meaningful features\nacross different scales. The stages in the MViTv2 architecture\nhierarchically expand the channel capacity while reducing\nthe spatial resolution. This allows the model to capture fine-\ngrained details as well as higher-level semantic information\nfrom the spectrograms, enhancing the discriminative power\nof the features.\nFigure 4 and Figure 5 present the confusion matrices\nobtained from evaluating the proposed Mel-MViTv2 method\non the Emo-DB and RA VDESS datasets. These matrices\noffer a comprehensive view of the classification performance\nby comparing the predicted classes against the ground truth\nlabels. It is noteworthy that the misclassification rate tends to\nbe higher for classes with limited sample sizes, primarily due\nto the scarcity of training data available for these classes. This\nphenomenon can be attributed to the similarities in acoustic\nfeatures, such as pitch variations, intensity fluctuations, and\nspeech rate alterations, which pose challenges in distinguish-\ning between certain emotions. Moreover, accurately classi-\nfying speech emotion is further complicated by the inter-\nindividual variability in the expression of emotions.\nIn the case of the IEMOCAP dataset, the confusion matrix\nVOLUME 4, 2016 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321122\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 4. Confusion matrix of the Emo-DB.\nFIGURE 5. Confusion matrix of the RAVDESS dataset.\nin Figure 6 provides insights into the performance of the\nproposed Mel-MViTv2 method. This dataset poses addi-\ntional challenges as it comprises dyadic speech with multi-\nple sources simultaneously. Analyzing the confusion matrix\nreveals that happiness and sadness are particularly prone to\nmisclassification. This can be attributed to the intricate nature\nof emotional expression, where inter-individual differences,\nthe presence of mixed emotions within samples, and the\nsubjectivity involved in emotion labeling contribute to the\nclassification difficulties encountered.\nFIGURE 6. Confusion matrix of the IEMOCAP dataset.\nV. CONCLUSION\nThis paper presents a speech emotion recognition method,\nknown as “Mel-MViTv2” using Mel-STFT spectrograms and\nMViTv2 as a classifier. The Mel-STFT leverages the Short-\nTime Fourier Transform technique along with frequency-\nto-Mel scale conversion to generate perceptually relevant\nvisual representations of audio signals. By dividing the audio\nsignal into short-time frames using a window function, such\nas the Hann window, and applying the STFT equation, the\nfrequency content of the audio signal at each time point is\nobtained. The resulting magnitude spectrum is then trans-\nformed into the Mel scale through triangular filterbanks,\nyielding the Mel-STFT representation. This technique ef-\nfectively captures acoustic features such as pitch changes,\nintensity variations, and speech rate, which are relevant for\nspeech emotion classification.\nThe MViTv2 introduces different stages for multi-scale\nvisual modeling, improving the capture of information at dif-\nferent granularity levels. MViTv2 incorporates relative posi-\ntional embeddings that consider the relative distance between\ninput tokens, enhancing the modeling of token interactions.\nFurthermore, MViTv2 employs a pooling technique called\nresidual pooling connection to mitigate potential information\nloss during pooling attention while maintaining computa-\ntional efficiency. This technique enhances information flow\nand facilitates training of pooling attention blocks. By inte-\ngrating features from multiple scales and incorporating rela-\ntive positional embeddings and residual pooling connections,\nMViTv2 improves the accuracy and efficiency of speech\nemotion recognition models. The proposed Mel-STFT with\nMViTv2 model achieved promising results, recording the\nhighest accuracy of 91.51%, 81.75%, and 64.03% on the\nEmo-DB, RA VDESS, and IEMOCAP datasets, respectively.\nREFERENCES\n[1] Yuni Zeng, Hua Mao, Dezhong Peng, and Zhang Yi. Spectrogram\nbased multi-task audio classification. Multimedia Tools and Applications,\n78:3705–3722, 2019.\n[2] Siddique Latif, Rajib Rana, Shahzad Younis, Junaid Qadir, and Julien\nEpps. Transfer learning for improving speech emotion classification\naccuracy. arXiv preprint arXiv:1801.06353, 2018.\n[3] Premjeet Singh, Goutam Saha, and Md Sahidullah. Deep scattering\nnetwork for speech emotion recognition. In 2021 29th European Signal\nProcessing Conference (EUSIPCO), pages 131–135. IEEE, 2021.\n[4] Siqi Han, Feng Leng, and Zitong Jin. Speech emotion recognition with\na resnet-cnn-transformer parallel neural network. In 2021 International\nConference on Communications, Information System and Computer En-\ngineering (CISCE), pages 803–807. IEEE, 2021.\n[5] Sandeep Kumar Pandey, H. S. Shekhawat, and S. R. M. Prasanna. Deep\nlearning techniques for speech emotion recognition: A review. In 2019\n29th International Conference Radioelektronika (RADIOELEKTRON-\nIKA), pages 1–6, 2019.\n[6] Monorama Swain, Bubai Maji, and Umasankar Das. Convolutional gated\nrecurrent units (cgru) for emotion recognition in odia language. In IEEE\nEUROCON 2021 - 19th International Conference on Smart Technologies,\npages 269–273, 2021.\n[7] Leila Kerkeni, Youssef Serrestou, Mohamed Mbarki, Kosai Raoof, Mo-\nhamed Ali Mahjoub, and Catherine Cleder. Automatic speech emotion\nrecognition using machine learning, 2019.\n[8] Tulika Jha, Ramisetty Kavya, Jabez Christopher, and Vasan Arunachalam.\nMachine learning techniques for speech emotion recognition using par-\nalinguistic acoustic features. International Journal of Speech Technology,\n25(3):707–725, 2022.\n8 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321122\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n[9] Huiyun Zhang, Heming Huang, and Henry Han. A novel heterogeneous\nparallel convolution bi-lstm for speech emotion recognition. Applied\nSciences, 11(21):9897, 2021.\n[10] Felicia Andayani, Lau Bee Theng, Mark TeeKit Tsun, and Caslon\nChua. Recognition of emotion in speech-related audio files with lstm-\ntransformer. In 2022 5th International Conference on Computing and\nInformatics (ICCI), pages 087–091. IEEE, 2022.\n[11] Kah Liang Ong, Chin Poo Lee, Heng Siong Lim, and Kian Ming Lim.\nSpeech emotion recognition with light gradient boosting decision trees\nmachine. International Journal of Electrical and Computer Engineering\n(IJECE), 13(4):4020, 2023.\n[12] Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo Xiong,\nJitendra Malik, and Christoph Feichtenhofer. Mvitv2: Improved multiscale\nvision transformers for classification and detection. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 4804–4814, 2022.\n[13] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan,\nJitendra Malik, and Christoph Feichtenhofer. Multiscale vision trans-\nformers. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 6824–6835, 2021.\n[14] Felix Burkhardt, Astrid Paeschke, Miriam Rolfes, Walter F Sendlmeier,\nBenjamin Weiss, et al. A database of German emotional speech. In\nInterspeech, volume 5, pages 1517–1520, 2005. doi:10.21437/interspeech.\n2005-446.\n[15] Steven R Livingstone and Frank A Russo. The Ryerson Audio-Visual\nDatabase of Emotional Speech and Song (RA VDESS): A dynamic, multi-\nmodal set of facial and vocal expressions in North American English. PloS\none, 13(5):e0196391, 2018. doi:10.1371/journal.pone.0196391.\n[16] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily\nMower, Samuel Kim, Jeannette N Chang, Sungbok Lee, and Shrikanth S\nNarayanan. IEMOCAP: Interactive emotional dyadic motion capture\ndatabase. Language Resources and Evaluation, 42(4):335–359, 2008.\ndoi:10.1007/s10579-008-9076-6.\n[17] Jerry Ma and Denis Yarats. Quasi-hyperbolic momentum and adam for\ndeep learning. arXiv preprint arXiv:1810.06801, 2018.\n[18] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\n[19] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu,\nJianfeng Gao, and Jiawei Han. On the variance of the adaptive learning\nrate and beyond. arXiv preprint arXiv:1908.03265, 2019.\nKAH LIANG ONG received his Bachelor’s De-\ngree in Information Technology (Hons.) Arti-\nficial Intelligence from Multimedia University,\nMalaysia, in 2021. Currently, he is a full-time\nMaster’s student and his current research inter-\nest is speech emotion recognition which mainly\ninvolves audio pre-processing, feature extraction,\nand emotion classification.\nCHIN POO LEE is a Senior Lecturer in the\nFaculty of Information Science and Technology at\nMultimedia University, Malaysia. She completed\nher Masters of Science and Ph.D. in Information\nTechnology in the area of abnormal behaviour de-\ntection and gait recognition. She is a certified Pro-\nfessional Technologist since 2018, a member of\nInternational Association of Engineers since 2020\nas well as Outcome-Based Education Consultant\nand Trainer. Her research interests include action\nrecognition, computer vision, gait recognition, natural language processing\nand deep learning.\nHENG SIONG LIM received his BEng (Hons)\nDegree in Electrical Engineering from Univer-\nsiti Teknologi Malaysia in 1999. He obtained his\nMEngSc and PhD in Engineering focusing on sig-\nnal processing for wireless communications from\nMultimedia University in 2002 and 2008 respec-\ntively. He is currently a Professor of Faculty of\nEngineering and Technology, Multimedia Univer-\nsity. His current research interests are in the areas\nof signal processing for advanced communication\nsystems, with emphasis on detection and estimation theory as well as their\napplications.\nKIAN MING LIM received B.IT (Hons) in Infor-\nmation Systems Engineering, Master of Engineer-\ning Science (MEngSc) and Ph.D. (I.T.) degrees\nfrom Multimedia University. He is currently a Lec-\nturer with the Faculty of Information Science and\nTechnology, Multimedia University. His research\nand teaching interests includes machine learning,\ndeep learning, and computer vision and pattern\nrecognition.\nALI ALQAHTANI received the Ph.D. degree\nin computer science from Swansea University,\nSwansea, U.K., in 2021. He is currently an Assis-\ntant Professor with the Department of Computer\nScience, King Khalid University, Abha, Saudi\nArabia. He has published several refereed confer-\nence and journal publications. His research inter-\nests include various aspects of pattern recognition,\ndeep learning, and machine intelligence and their\napplications to real-world problems.\nVOLUME 4, 2016 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3321122\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}