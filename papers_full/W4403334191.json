{
  "title": "Patchview: LLM-powered Worldbuilding with Generative Dust and Magnet Visualization",
  "url": "https://openalex.org/W4403334191",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5081068980",
      "name": "John Joon Young Chung",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5034568168",
      "name": "Max Kreminski",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4391556516",
    "https://openalex.org/W4385714563",
    "https://openalex.org/W4396832834",
    "https://openalex.org/W4281741206",
    "https://openalex.org/W4396833426",
    "https://openalex.org/W2789415069",
    "https://openalex.org/W2087641817",
    "https://openalex.org/W4386555418",
    "https://openalex.org/W4385774603",
    "https://openalex.org/W4282561746",
    "https://openalex.org/W4225012671",
    "https://openalex.org/W3159979163",
    "https://openalex.org/W2790165607",
    "https://openalex.org/W4387835509",
    "https://openalex.org/W1724908488",
    "https://openalex.org/W4366594764",
    "https://openalex.org/W4389574988",
    "https://openalex.org/W4383679902",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4385682544",
    "https://openalex.org/W4392427378",
    "https://openalex.org/W4387801187",
    "https://openalex.org/W4306759153",
    "https://openalex.org/W4285220829",
    "https://openalex.org/W4393118307",
    "https://openalex.org/W4220747294",
    "https://openalex.org/W3122241445",
    "https://openalex.org/W3031000691",
    "https://openalex.org/W4366547384",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W4287891464",
    "https://openalex.org/W2264205808",
    "https://openalex.org/W2537060787",
    "https://openalex.org/W4396832739",
    "https://openalex.org/W4377371585",
    "https://openalex.org/W3203321135",
    "https://openalex.org/W1969045498",
    "https://openalex.org/W4221055872",
    "https://openalex.org/W2767546953",
    "https://openalex.org/W4387394580",
    "https://openalex.org/W2606555609",
    "https://openalex.org/W4399917667",
    "https://openalex.org/W884650706",
    "https://openalex.org/W3006277708",
    "https://openalex.org/W4250337847"
  ],
  "abstract": "Large language models (LLMs) can help writers build story worlds by\\ngenerating world elements, such as factions, characters, and locations.\\nHowever, making sense of many generated elements can be overwhelming. Moreover,\\nif the user wants to precisely control aspects of generated elements that are\\ndifficult to specify verbally, prompting alone may be insufficient. We\\nintroduce Patchview, a customizable LLM-powered system that visually aids\\nworldbuilding by allowing users to interact with story concepts and elements\\nthrough the physical metaphor of magnets and dust. Elements in Patchview are\\nvisually dragged closer to concepts with high relevance, facilitating\\nsensemaking. The user can also steer the generation with verbally elusive\\nconcepts by indicating the desired position of the element between concepts.\\nWhen the user disagrees with the LLM's visualization and generation, they can\\ncorrect those by repositioning the element. These corrections can be used to\\nalign the LLM's future behaviors to the user's perception. With a user study,\\nwe show that Patchview supports the sensemaking of world elements and steering\\nof element generation, facilitating exploration during the worldbuilding\\nprocess. Patchview provides insights on how customizable visual representation\\ncan help sensemake, steer, and align generative AI model behaviors with the\\nuser's intentions.\\n",
  "full_text": "Patchview: LLM-Powered Worldbuilding with Generative Dust\nand Magnet Visualization\nJohn Joon Young Chung\njchung@midjourney.com\nMidjourney\nSan Francisco, CA, USA\nMax Kreminski\nmkreminski@midjourney.com\nMidjourney\nSan Francisco, CA, USA\nFigure 1: For generating story world elements with LLMs, Patchview leverages a dust and magnet visual representation to help\nusers a) sensemake, b) steer, and c) correct LLM generation. a) In dust and magnet visual representation, user-defined concepts\nserve as ‚Äúmagnets‚Äù (larger dark circles), attracting ‚Äúdust particle‚Äù elements (smaller light circles) more strongly if an element is\nmore relevant to the concept. Note that we only show partial excerpts of magnets due to the limited space. b) By placing a red\nmarker between magnets, the user can steer the generation with a mix of different concepts. c) When LLM behaviors (steering,\nrecognition) do not align with the user‚Äôs perception, the user can correct them simply by moving the element, either 1) revising\nthe element position or 2) re-steering generation to rewrite the element. The corrected placement will be fed into the LLM\npipeline as an example to improve future steering and recognition.\nABSTRACT\nLarge language models (LLMs) can help writers build story worlds\nby generating world elements, such as factions, characters, and\nlocations. However, making sense of many generated elements can\nbe overwhelming. Moreover, if the user wants to precisely control\naspects of generated elements that are difficult to specify verbally,\nprompting alone may be insufficient. We introduce Patchview, a cus-\ntomizable LLM-powered system that visually aids worldbuilding by\nallowing users to interact with story concepts and elements through\nthe physical metaphor of magnets and dust. Elements in Patchview\nare visually dragged closer to concepts with high relevance, facil-\nitating sensemaking. The user can also steer the generation with\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nUIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0628-8/24/10\nhttps://doi.org/10.1145/3654777.3676352\nverbally elusive concepts by indicating the desired position of the\nelement between concepts. When the user disagrees with the LLM‚Äôs\nvisualization and generation, they can correct those by reposition-\ning the element. These corrections can be used to align the LLM‚Äôs\nfuture behaviors to the user‚Äôs perception. With a user study, we\nshow that Patchview supports the sensemaking of world elements\nand steering of element generation, facilitating exploration during\nthe worldbuilding process. Patchview provides insights on how\ncustomizable visual representation can help sensemake, steer, and\nalign generative AI model behaviors with the user‚Äôs intentions.\nCCS CONCEPTS\n‚Ä¢ Human-centered computing ‚ÜíInteractive systems and\ntools; Visualization systems and tools ; ‚Ä¢ Computing methodolo-\ngies ‚ÜíNatural language generation .\nKEYWORDS\nworldbuilding, large language models, dust and magnet visualiza-\ntion\narXiv:2408.04112v1  [cs.HC]  7 Aug 2024\nUIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA Chung and Kreminski.\nACM Reference Format:\nJohn Joon Young Chung and Max Kreminski. 2024. Patchview: LLM-Powered\nWorldbuilding with Generative Dust and Magnet Visualization. In The 37th\nAnnual ACM Symposium on User Interface Software and Technology (UIST\n‚Äô24), October 13‚Äì16, 2024, Pittsburgh, PA, USA. ACM, New York, NY, USA,\n19 pages. https://doi.org/10.1145/3654777.3676352\n1 INTRODUCTION\nRapid progress in the development of generative large language\nmodels (LLMs) [8, 14] has recently led to the introduction of numer-\nous LLM-based tools for storywriting [1, 17, 34, 61]. While many\nof these tools aim to generate text for direct inclusion in a finished\nstory, opportunities also lie in using LLMs to support other aspects\nof the writing process, such as worldbuilding. Worldbuilding‚Äîthe\nact of constructing a coherent fictional world [24]‚Äîestablishes a\nsetting from which a variety of stories could arise. It requires writ-\ners to envision myriad aspects of a world, from abstract values\n(e.g., religion, ideology) to more specific elements, such as factions,\ncharacters, places, or props. As worldbuilding involves creating\nmany different world elements, writers often put a lot of time and\neffort into it. Generative LLMs could be used to support this pro-\ncess, for instance by producing additional world elements that fit\ninto the established setting or even inspire writers to expand their\nconception of the world they are creating.\nHowever, when generating many world elements with LLMs,\nunderstanding their overall landscape can be challenging. That\nis, to unfold a story where different elements interact with each\nother, the writers would need to have a holistic view regarding\nwhat kind of attributes and values those elements hold. As LLMs\ncan quickly add many elements to the world, it can be challenging\nfor a writer to understand the rapidly growing world. Moreover,\nonce the writer has understood existing world elements, they might\nwant to generate a specific type of world elements. One way to\nguide LLMs for such a purpose would be to write natural language\nprompts. However, if the writer wants to express verbally elusive or\nambiguous concepts, writing natural language prompts can either\nbe cumbersome [49] or have limited expressivity [15].\nTo support sensemaking and steering of world element genera-\ntion, we propose generative dust and magnet (GD&M) visualization,\nwhich adapts dust and magnet visual representation [11, 60] to the\nuse of generative models. GD&M visualizes elements as ‚Äúparticles\nof iron dust‚Äù which are attracted to different concepts, or ‚Äúmagnets‚Äù,\nbased on their relevance to each concept (i.e., placed more closely\nif more relevant) (Figure 1a). This approach supports flexible visu-\nalization of semantic association between concepts and elements\nwith an arbitrary number of concepts, by leveraging intermediate\nspaces between extreme ‚Äúanchoring‚Äù concepts. Moreover, spaces\nbetween concepts can be used for guiding generation, even allow-\ning expression of ambiguity between concepts (Figure 1b). When\nthe user disagrees with steered generation and recognition results,\nthe user can straightforwardly correct them by simply moving dust\nparticles to other positions (Figure 1c). With repositioning, the user\ncan indicate the generated element‚Äôs correct placement (Figure 1c1)\nor command AI to revise the element to fit in the new position (Fig-\nure 1c2). These corrections can feed back into the AI as examples\nof the user‚Äôs perspective for future steering and recognition.\nWe instantiated these interactions in Patchview, an LLM-powered\nstory worldbuilding tool. Via a user study with eight hobbyists and\none professional in worldbuilding, we show that Patchview allows\nusers to quickly understand the landscape of elements within the\nstory world. Moreover, we find that visual steering of LLM genera-\ntion could function as an intuitive alternative to natural language\nprompting, allowing users to express nuances that are difficult to\narticulate with natural language. While participants found the inter-\naction of correcting AI results on the visual space straightforward,\nuser-provided corrections did not have a significant impact on align-\ning AI behaviors to user intent. However, participants found the\ntool overall helpful for worldbuilding, flexibly creating worlds that\nthey found to be of interest. We conclude with discussions on visual\nrepresentations for interacting with generative AI; using worlds\nfrom Patchview for story writing; technical alternatives for prompt\nengineering and closed models; and limitations.\nIn summary, this work has three main contributions:\n(1) Generative Dust and Magnet (GD&M) visualization, a novel\nvisual interaction approach to make sense of, guide, and\nintervene in the use of generative AI models.\n(2) Patchview, an LLM-powered tool that supports story world\nelement creation with GD&M.\n(3) An evaluation that shows how Patchview supports sense-\nmaking and steering of LLM outputs while revealing limita-\ntions in aligning LLM behaviors to the user‚Äôs perspective.\n2 RELATED WORK\n2.1 Worldbuilding\nWorldbuilding is a process of architecting fictional worlds that\ncan be cornerstones of narrative fiction [ 24]. It considers vari-\nous aspects, such as places, characters, or even cultures, and well-\nconstructed worlds add believability to the stemming narrative\nstories. A well-built story world also entertains readers, as read-\ners build out the conception of a coherent world out of various\nstemming stories [ 21, 39]. With a story world, readers can also\nparticipate in active consumer experiences, such as creating fan\nfiction and even transforming the canon world into alternative\nworlds [21, 46]. While worldbuilding can be a complex process\nwith many aspects to consider, there have been practical frame-\nworks and structures that practitioners use. Practitioners would\nlikely first focus on the frameworks of the world, which can include\nscope (geography of the world), sequence (temporal history of the\nworld), and perspective (from whom the world is explained) [24].\nUnder such frameworks, practitioners would create structures of\nthe world. Governance (e.g., government presence, rule of law),\neconomics (e.g., economic strength, wealth distribution), social rela-\ntions (e.g., class, race, and ethnic relations), cultural influences (e.g.,\nreligious influences, cultural influences), and character alignments\n(e.g., good-evil, lawful-chaotic) [24] are some examples of world\nstructures. Then, around frameworks and structures, practitioners\nwould create catalogs of fictional worlds, or elements of the world,\nsuch as characters, places, props, and events [24]. Worldbuilding\ncan be done either solely (e.g., Tolkien‚Äôs world of Lord of Rings) or\ncollectively (e.g., Marvel Universe), and commercial projects often\ntend to be collaborative as doing creative work can be overloading\nto an individual. WorldSmith is one of the few AI-powered tools\nPatchview: LLM-Powered Worldbuilding with Generative Dust and Magnet Visualization UIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA\nto support worldbuilding but focuses on creating visual aspects of\nthe world [20]. In this work, which focuses on supporting world\nelement creation, we introduce an AI-powered worldbuilding tool\nthat co-constructs the story world with users by generating new\nworld elements based on what the user has. Specifically, we facili-\ntate the use of AI models by incorporating visual means for users\nto sensemake and control world element generation.\n2.2 AI-Powered Story Writing\nWith advancing AI technologies, researchers and practitioners\nhave developed many tools to support story writing. For example,\nTaleStream supports story ideation by showing potentially inspir-\ning story tropes [13]. Loose Ends is a rule-based mixed-initiative\nAI system that allows users to explore plot threads with some con-\nstraints [31]. Portrayal leveraged NLP and visualizations to help\nwriters analyze characters in their stories [25]. LLM‚Äôs advanced gen-\nerative capabilities introduced tools that suggest texts that users can\nincorporate into their writing [9, 19, 35]. Researchers investigated\ndiverse interactions for such tools, from allowing distinct sugges-\ntion operations [61] to incorporating multimodality [23], hierarchi-\ncal generation [40], and sketching inputs [17]. With these rapidly\nadvancing capabilities, researchers also studied story writer‚Äôs ex-\npectations for these technologies, such as what they would take as\na benefit and what they want [7, 22, 29]. Lee et al. [34] reflected on\nthe design space of writing tools through a literature survey. LLMs\nalso enabled story applications where the story is generated with\nminimal writer interventions, directly facing the audience [44, 55].\nWhile many LLM-powered story writing tools focused on support-\ning prose text writing, some focused on other types of support.\nFor example, CALYPSO leverages LLMs to provide support to dun-\ngeon masters when playing Dungeons & Dragons [63]. In a similar\nvein, we design Patchview to provide LLM-powered support in\nworldbuilding, which is other than writing story texts themselves.\n2.3 Visually Interacting with Generative AI\nWhile natural language-based interfaces (e.g., prompts, chat) have\nbeen widely used for generative AI models, many previous systems\nused visual interactions to complement natural language interac-\ntions. Some tools leverage node-based input interactions to control\ngeneration, such as chaining subtasks [4, 6, 30, 58]. Among them,\nChainForge [6] and Cells-Generators-Lenses framework [30] also\nallowed evaluation of generated results with visualization nodes.\nWhile these tools allowed flexible control, steering and evaluation\nhappened in separated interfaces, leading to visual complexity. As\nanother type, Scenescape [ 51] and Graphologue [ 28] leveraged\ngraph and tree visualization to help understand complex informa-\ntion. While the user can steer further generations by clicking on\nthe node which the user is willing to learn more details about, these\nfocus more on presenting information than allowing flexible steer-\ning. Some tools allow steering or evaluation of multiple generation\nresults on dimensional spaces of attributes, represented in either\nsliders [38], mixed color spaces [15], temporal line drawing [17], or\nscatter plots organized in grids [50]. Among them, TaleBrush [17]\nand Luminate [50] tied steering and evaluation interactions on a\nsingle visual representation, minimizing clutters. TaleBrush con-\nsiders a continuous dimensional scale but on a fixed attribute. On\nthe other hand, Luminate allows arbitrary dimensional attributes\nbut only with categorical/ordinal attributes. Moreover, all afore-\nmentioned tools do not allow users to correct AI behaviors when\nAI‚Äôs steered generation and recognition results do not align with\nthe user‚Äôs thoughts. Patchview extends previous work by allowing\ngeneration steering, evaluation, and user corrections on an inte-\ngrated single visual representation with the flexibility of allowing\ncontinuous scales of any arbitrary concepts of interest.\n3 GENERATIVE DUST AND MAGNET\nPatchview‚Äôs central design metaphor‚Äîgenerative dust and magnet\n(GD&M)‚Äîleverages a dust and magnet (D&M) visual representa-\ntion [60] to facilitate interaction with generative AI models. In this\nsection, we first describe settings where GD&M can be helpful (Sec-\ntion 3.1). Then, we describe the original D&M visualization and how\nwe translate its components for use with generative models (Sec-\ntion 3.2). Finally, we describe specific GD&M interactions that close\ngaps in the interactive alignment of AI models: evaluation support,\nspecification alignment, and process alignment [52] (Section 3.3).\n3.1 Need for Generative Dust and Magnet\nInteraction with generative AI might benefit from a wide range of\ndifferent interaction approaches in different settings. In general,\nwe expect GD&M interaction to be most effective when the user\nmust generate many distinct units of output (e.g., storyworld\nelements) that vary along diverse and expressive conceptual\ndimensions. Breaking this ideal setting down further, we arrive at\na set of three conditions that typify good application domains for\nGD&M interaction.\nFirst, the user must make use of generative models to gather\na collection of many generated outputs. This imposes a need for\nsensemaking (N1), as understanding how outputs distribute along\nthe user‚Äôs conceptual dimensions of interest is difficult due to the\nlarge scale of generation.\nSecond, the user must have desires to create artifacts within\ntheir unique characteristics and values, which often occurs in artis-\ntic creation [16]. This imposes a need for configurability (N2),\nwhere behaviors of AI functions (e.g., generation and evaluation of\ngenerated results) consider the user‚Äôs unique styles and interests.\nThird, the user must need to express nuanced specifications that\nalign generation with the user‚Äôs specific intentions and facilitate\nexploration of subtly different options. This imposes a need for\nexpressivity (N3) where the user can guide generation even with\nsubtle intentions.\nGD&M interaction would be ideal for user tasks with the above\ncharacteristics. Worldbuilding meets all of these conditions: the\nwriter must create many world elements to fill out a unique and\nidiosyncratic world, and created elements can have nuanced dif-\nferences between each other [ 24]. In the following sections, we\ndescribe how GD&M can fulfill the aforementioned needs.\n3.2 From D&M Visualization to GD&M\nYi et al.‚Äôs original dust and magnet visualization represents indi-\nvidual data elements as ‚Äúdust particles‚Äù while representing each\nvariable for which data elements can possess different values as a\n‚Äúmagnet‚Äù. Both dust particles and magnets are rendered as glyphs\nUIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA Chung and Kreminski.\nFigure 2: Compared to dust and magnet visualization, gener-\native dust and magnet replaces data elements (dust particles)\nand variables (magnets) with generated data elements and\nconcepts, respectively. In generative dust and magnet, the\ndistance between a magnet and a dust particle indicates the\nintensity of relevance between them.\non a 2D plane; a data element with a particularly high value for a\ncertain variable will be placed closer to the magnet representing\nthat variable. This approach can facilitate the accessibility of un-\nderstanding many multivariate data instances [60] while allowing\nusers to identify notable patterns within a dataset [11].\nWe extend D&M visualization to an interface for generative mod-\nels (Figure 2). Generative D&M replaces multivariate data elements\nwith generated data elements in the output modality of a generative\nmodel (e.g., passages of text for LLMs, and images for text-to-image\nmodels). Accordingly, variables in the original D&M visualization\ntranslate to concepts that characterize the generated outputs (e.g.,\n‚Äúpositive sentiment‚Äù for texts, ‚Äúpastel colors‚Äù for images). Under\nthis translation, a generated element that is more strongly rele-\nvant to a specific concept is drawn closer to the magnet for the\ncorresponding concept.\n3.3 Specific GD&M Interactions\nSeveral specific GD&M interactions are designed to meet user needs\ndiscussed in Section 3.1 (Figure 3). We organize these interactions in\nterms of how they support interactive alignment of AI models [52].\nExtending challenges of the gulf of evaluation and execution [42],\nTerry et al. [52] emphasized three facets of interactive alignment\nof AI models: 1) evaluation support ( I1), or users making sense\nof AI outputs; 2) specification alignment (I2), or users efficiently\nand reliably communicating their objectives to AI; and 3) process\nalignment (I3), or users verifying or controlling AI‚Äôs execution\nprocess.\n3.3.1 I1: Evaluation Support - User Configurable Dust and Magnet\nVisualization (N1, N2). To support users‚Äô sensemaking of many\ngenerated elements according to their concepts of interest, the user\ncan add generated elements to the magnet space configured with\nconcepts of the user‚Äôs interest (Figure 3-I1). Then, an AI model\nmeasures the relevance of each element to different concepts and\nFigure 3: Input-Output schemes for GD&M Interactions\nFigure 4: Configuration interactions for evaluation support\nin generative dust and magnet. As the user adds, removes,\nedits, and moves concepts according to how they want to or-\nganize elements and concepts, the positions of data elements\nget updated.\nvisualizes this information as the relative distance to those concepts\n(e.g., good characters being closer to the concept of ‚Äúgood‚Äù than\nto ‚Äúevil‚Äù). With this support, the user can quickly get an overview\nof how generated elements are different from each other. GD&M\nfurther provides users with flexible configurability, as the user\ncan add, remove, or even edit concepts. With this configurability\n(Figure 4a, b, and c), users can easily reassess the set of generated\noutputs in terms of the concepts that are most relevant to their\ncurrent focus. Moreover, the user can adjust the layout of concepts\n(Figure 4d), aligning their visual presentation with how they want\nto think about these concepts and elements.\n3.3.2 I2: Specification Alignment - Generation Specifications within\nMagnet Space (N2, N3).GD&M interaction also allows users to guide\nthe generation of new elements by indicating the ideal placement\nof these elements within the visual-semantic magnet space defined\nby a set of user-configured concepts. That is, the user can place a\nmarker on the magnet space to request the generation of elements\nPatchview: LLM-Powered Worldbuilding with Generative Dust and Magnet Visualization UIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA\nthat would be placed near the specified marker (as in Figure 1b and 3-\nI2). For instance, if the user wants to generate a good character,\nin the magnet space between ‚Äúgood‚Äù and ‚Äúevil, ‚Äù they can place the\nmarker closer to ‚Äúgood. ‚Äù One benefit of this visual magnet space is\nthat the user can express vague or ambiguous specifications in this\ncontinuous space between concepts (e.g., generating an array of\ncharacters that vary subtly along the ‚Äúgood‚Äù-‚Äúevil‚Äù spectrum).\n3.3.3 I3: Process Alignment - Correcting AI on Magnet Space (N2).\nAI behaviors may not always align with user intent: for instance, the\nuser might not agree with how the AI interprets concepts during\ngeneration and placement of generated elements. In such cases,\nthe user can freely re-specify concepts to more accurately convey\nhow they think about each concept (e.g., adding more details about\nwhat ‚Äúgood‚Äù means in a specific story world, Figure 4c). They can\nalso leverage the magnet space itself to correct AI behavior, by\nsimply moving a misplaced generated element to wherever the\nuser thinks it should be in the magnet space (Figure 1c and 3-\nI3). Repositioning an element can convey two intentions: either 1)\nthat the element‚Äôs ‚Äúcorrect‚Äù placement is in a new position (e.g.,\nindicating that the character should sit in the middle of ‚Äúgood‚Äù and\n‚Äúevil‚Äù as in Figure 1c1) or 2) that the element should be revised\nto better fit the indicated position (e.g., request AI to rewrite the\ncharacter description to sit in the middle of ‚Äúgood‚Äù and ‚Äúevil‚Äù, as\nin Figure 1c2). These corrections can then be used as examples to\nbetter align future generation and placement with user perception\nof concepts.\n4 PATCHVIEW: INTERFACE AND TECHNICAL\nDETAILS\nWith GD&M, we built Patchview, an LLM-powered tool for world\nelement creation (Figure 5). Specifically, Patchview supports sense-\nmaking and steering of world element generation. To demonstrate\nthe effectiveness of GD&M for sensemaking and steering, Patchview\nfocuses on creating initial ‚Äúseeds‚Äù of story world elements in two\nto three sentences. Afterward, users can develop details of these\nseed elements either by themselves or with the help of AI; the final\nrendering of seed elements into a more complete form is left to\nfuture work.\nPatchview‚Äôs user interface consists of the list module, which\nshows existing world elements as a list of notes (Figure 5b), and\nthe view module, which organizes world elements via GD&M (Fig-\nure 5a). Note that Patchview leverages AI to generate specific world\nelements (e.g., characters, places) rather than generating frame-\nworks or structures of the world (e.g., ideology, values). The user\ncan manually specify frameworks and structures as open-ended\ntext in notes.\nWe explain the envisioned usage pattern with a hypothetical\nuser, Alex. Alex is a game scenario writer who is trying to design a\nstory world for the new game her team is developing. To get help\nwith the process, Alex decides to use Patchview.\n4.1 List Module\nAs Alex loads Patchview, she first sees the list module on the right.\nWith this module, Alex can generate and create an initial set of\nworld elements as textual notes. To set an initial high-level concept\nfor the world, Alex decides to manually create a note by clicking\nthe note button (Figure 5b-3) and modifying the text to ‚Äútower\nof eyeballs. ‚Äù Next, extrapolating from this high-level idea, Alex\ndecides to generate factions in the story world. Generating world\nelements with AI is straightforward, as Alex can simply click on\nthe button that corresponds to the type of the element that Alex\nwants to introduce (Figure 5b-1, thin solid line in Figure 6). When\ngenerating the new element, by default, Patchview will take all\nexisting elements into context to ensure that the generated element\nis relevant to the current story world. If Alex wants the generation\nto consider only a subset of existing elements, she can select only\nthose notes as context for generation. In case Alex wants to generate\na more specific world element, Alex can also directly prompt the\nAI with natural language (Figure 5b-5, thick solid line in Figure 6).\nWith this generation function, Alex first generates a few factions\nand then a handful of characters. As the number of world elements\nincreases, Alex can organize them in the list by reordering them\nwith dragging. However, at a certain point, she feels that the list is\ngetting longer and becoming hard to understand.\n4.2 View Module\n4.2.1 Creating and Configuring View (I1).To make sense of this pro-\nliferation of world elements, Alex decides to use the view module\nto organize them. In Patchview, a view is a single GD&M visual-\nsemantic space that organizes world elements in relation to a spe-\ncific set of user-defined concepts. Alex can create a new view by\nfirst clicking the View button in the bottom left corner and then\nclicking the + button. The user can set the concept associated with\neach magnet in the view either by dropping existing notes into\nplaceholder magnets (i.e., using elements as concepts, Figure 7a) or\nclicking the ‚ÄúType in a new magnet‚Äù button that shows up when\nthe user hovers their mouse close to the placeholder or existing\nconcepts (Figure 7b). Once Alex configures the view with a set of\nconcepts, she adds relevant elements as dust particles in the view by\ndragging and dropping elements from the list module to the target\nview. As Alex adds an element to the view, Patchview calculates\nits position within the view visualization space. Alex can also add\nmultiple elements by first checking multiple of those in the list\nmodule. Note that Alex can add elements of different types in a\nsingle view, if they are relevant (e.g., putting a good character and a\ngood faction under ‚Äúgood‚Äù-‚Äúevil‚Äù view). For concepts and elements\nin the view, Alex can read their full descriptions by hovering the\ncursor over them (Figure 8). When Alex selects added elements\nfrom the list module, to let her know where they are in the view,\nthe tool highlights them on the visualization.\n4.2.2 Correcting View Visualization (I3). For some elements added\nto the view, Alex does not agree with how Patchview positioned\nthem. If Alex thinks the description of a particular concept is not\ndetailed enough for the tool to grasp, she can modify it via the list\nmodule. Alternatively, she can edit the concept‚Äôs definition text\ndirectly within the view module by hitting enter while hovering the\ncursor over the concept‚Äôs magnet (similar to Figure 8, but with con-\ncepts). As Alex updates the concept, Patchview tries to reposition\nelements in relation to the concept. For elements still misplaced\nfrom Alex‚Äôs perspective, Alex can manually adjust their positions\nby dragging them in the view (Figure 1c1). When positioning future\nUIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA Chung and Kreminski.\nFigure 5: Patchview interface. a) View module visualizes world elements in relation to concepts of the user‚Äôs interest. Specific\ninteractions are shown in Figure 1. b) List module lists world elements as notes (b-4). This module allows users to generate ele-\nments by clicking buttons for different element types (b-1) or by prompting an LLM with specific natural-language instructions\n(b-5). The user can steer generation with a view interface (as in Figure 1b) by entering the steering mode with a toggle switch\n(b-2). They can also manually create notes (b-3). c) The user can see a list of existing views by clicking the Views button and\ncreate a new view with the + button.\nFigure 6: Possible inputs to generate elements.\nFigure 7: Interactions to add a new concept to the view.\nFigure 8: The user can read each concept and element by\nhovering their mouse over them. While hovering the mouse,\nthey can 1) edit them by hitting the enter key, 2) exclude\nthem from the view by hitting the - key, and 3) delete them\nby hitting backspace. For elements, the user can re-steer the\ngeneration by dragging the element to a new position while\nholding the shift key; the LLM will then attempt to rewrite\nthe element‚Äôs text to better match the target position.\nPatchview: LLM-Powered Worldbuilding with Generative Dust and Magnet Visualization UIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA\nFigure 9: By dragging and dropping the anchor, the user can\nconnect multiple views to have a better understanding of how\nelements distribute along those views. The same element is\nconnected by the thin line, and for the highlighted element,\nthe connecting line is also highlighted.\nelements, Patchview leverages user-adjusted elements as examples\nto better follow the user‚Äôs perspective.\n4.2.3 Sensemaking Multiple Views (I1). Alex continues organizing\nworld elements by creating multiple views. Alex organizes these\nviews by dragging view names and concepts. At a certain point,\nAlex realizes that it is difficult to understand how characters are\ndistributed along the conceptual dimensions of two views, good-evil\nand lawful-chaotic alignments. To have a better understanding, Alex\nanchors them together and Patchview connects the same elements\nin both views with a thin line (Figure 9). Note that only elements that\nexist in both views get connected. As Alex hovers her cursor over an\nelement in one of the views, the identical element in another view\nand the thin connecting line between these elements are highlighted\n(Figure 9). After connecting these views, as each view is defined\nby only two concepts, Alex thinks that it would be easiest to make\nsense of these elements via a 2-dimensional visualization with two\naxes. For that, Alex can cross two views, and Patchview renders the\nview in 2D plane visualization instead of connecting elements with\nlines (Figure 10). Note that Patchview only visualizes elements that\nexist in both crossed views. As Alex adds more views, she continues\nto experiment with other visual arrangements, such as radar charts\nand parallel coordinate charts [18, 41] (Figure 11).\nFigure 10: When two separate views are each defined by ex-\nactly two concepts, the user can cross these views into a 2D\nplane visualization. Analogically, this would correspond to\nputting dust particle elements under the simultaneous influ-\nence of two uniform magnet fields of concepts.\nFigure 11: Multiple views can be flexibly organized to form\na) a radar chart or b) a parallel coordinate chart.\n4.2.4 Steering Generation in the View (I2).As Alex organizes world\nelements in the view, she finds herself wanting to add more char-\nacters to populate empty spaces within view visualizations. To\nsteer the generation with this nuanced intention, Alex leverages a\ngenerative steering function on each view. Alex first clicks on the\n‚Äúwith Steering‚Äù toggle switch at the top right to enter the genera-\ntion mode. Then, Alex places the generation control right on the\nview space itself, indicating that Alex wants the newly generated\nelement to be placed near the specified position (Figure 1b). To\nsteer generation along multiple aspects, Alex can also place multi-\nple generation controls on multiple views. After placing controls,\nAlex clicks on one of the type buttons or prompts the LLM (thin\nand thick dashed lines in Figure 6, respectively) to generate an\nelement with the steering constraints applied. After generating the\nelement, Patchview calculate its position in the view to place it on\nthe view. Similar to how Patchview visualizes elements in the view,\nUIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA Chung and Kreminski.\nPatchview leverages elements added and edited by Alex to adjust\nits steering behavior to the user‚Äôs perception of concepts.\n4.2.5 Correcting Generation (I3). Sometimes, generated items do\nnot perfectly align with Alex‚Äôs specifications. To iterate on those,\nshe can directly modify the text of the element or ask Patchview\nto rewrite it by dragging the element while holding the shift key\n(Figure 1c2). If Alex disagrees with how Patchview places a gen-\nerated element, she can reposition it by dragging (Figure 1c1). For\nelements that Alex does not want to keep, Alex can either remove\nthem from the view by hitting the minus key while hovering the\ncursor over the element or delete them from the view and the list by\nhitting the backspace key while hovering the cursor. Alex continues\ngenerating, editing, and organizing elements until she is done.\n4.3 Technical Details and Implementation\nWe built Patchview as a web application with a React-based fron-\ntend and a Node-based backend server. We provide technical details\non 1) mapping between the position of the element and the its\nrelevance to the concepts and 2) LLM prompting.\n4.3.1 Mapping Between Position and Weight.To enable visualiza-\ntion and visual steering on the view, Patchview needs to map the\nvisualized position of an element to its relevance to considered con-\ncepts and vice versa. Here, we quantified the relevance of an element\nto the concept as weight values between 0 and 1. We first forced the\nplacement of concepts to be convex, as the non-convex arrangement\nof concepts can bring in more complexities with those mappings.\nWith the convex arrangement of concepts, we can compute the\nelement position easily by weight-summing the concept positions\nwith weights on those concepts. However, deciding weights from\nthe element position is not trivial if there are more than three con-\ncepts, as a single position does not fall into one weight combination.\nThat is, with more than three concepts, there can be more than\nthree weights that need to be decided, but there would be only\nthree equations with a 2D arrangement of concepts:\nŒ£ùëõ\nùëñ=1ùë§ùëñùë•ùëñ = ùë•ùëí\nŒ£ùëõ\nùëñ=1ùë§ùëñùë¶ùëñ = ùë¶ùëí\nŒ£ùëõ\nùëñ=1ùë§ùëñ = 1\n(1)\nùë•ùëñ and ùë¶ùëñ stand for the position of each concept, while ùë•ùëí and ùë¶ùëí\nindicate the position of the element. ùë§ùëñ stands for the weight that\nneeds to be inferred and ùëõ is the number of concepts.\nDue to the above reason, with more than three concepts, we\nused the following heuristic to compute one weight combination:\nWith all combinations of three concepts from all concepts, we first\ncalculated weights for each combination. Then, we filtered out\ncombinations with negative weights. After that, we calculated a\nweight for each concept by summing all weights from all the left\ncombinations. Then, we finalized the weights by dividing each\nweight by the sum of all weights for all concepts. Note that with\nthis approach, steering element generation with more than three\nconcepts does have a limitation as not all possible weights are\nexpressible with one geometric positioning of concepts. When two\naxes are crossed to form a 2D plane visualization (as in Figure 10),\nfor each view, we first calculated the crossing point of the following\ntwo lines: 1) the line that passes through two concepts of the view\nand 2) the line that passes through the position of the element and\nis parallel to the line of two concepts from the other view. Then, as\nthis calculated point is on the line that passes through two concepts\nof the view, we can calculate the weight from the equation 1.\n4.3.2 LLM Prompting. To generate elements with steering inputs\nand recognize the relevance of elements to concepts, we prompted\nclaude-2.0 and claude-instant-1.2 from Anthropic [5], respec-\ntively. We chose these models because they have shown better per-\nformance in creative writing contexts than leading alternatives [10].\nPrompts for both generation and recognition began by introduc-\ning a set of existing world elements for context, as in Figure 12a.\nBy default, all existing world elements were supplied as part of this\ncontext; the user could also select a subset of existing elements to\nensure that only those elements would be provided as context.\nWhen generating new world elements without visual steering\ninput, introductory context was followed directly by an instruction\ndescribing what kind of element to generate. When generating with\nvisual steering input, we first appended concepts of all views (Fig-\nure 12b-1-1) and examples of how existing elements have relevance\nto those concepts (Figure 12b-1-2). These examples came from ele-\nments that the user has already placed in the view, including those\nrepositioned by the user. Note that in the prompt, all views and\nconcepts are phrased as ‚Äúdimensions‚Äù and ‚Äúcharacteristics‚Äù, respec-\ntively. A chain-of-thought [57] style generation instruction prompt\nfollowed after (Figure 12b-2), which asked the LLM to first reason\nabout how the element description should be written considering\nsteering inputs and then to write the element description.\nRecognition of concept relevance values takes place on a per-\nview basis, so a prompt for the recognition task included concept\ndescriptions for only a particular considered view, as in Figure 12c.\nIn the recognition prompts, introductory context (Figure 12a) and\nconcept descriptions (Figure 12c-1) were followed by instructions\nabout how to interpret numbers (Figure 12c-2), and then by exam-\nples of the correct performance of a recognition task for this set of\nconcepts (Figure 12c-2-1). Because these examples were taken from\npast placements of elements into this specific view, information\nabout how the user repositioned elements in this view were taken\ninto account at this step. Finally, the world element to be analyzed\nwas attached (Figure 12c-2-2), with the chain-of-thought [57] in-\nstruction that the LLM should provide reasoning before the result.\nThe LLM was asked to provide recognition results in a JSON format\nwith concept identifiers as keys and concept relevance weights as\nvalues.\n5 USER STUDY\nWe conducted a user study on Patchview to learn if it supports\nsensemaking and steering of world element generation under the\nuser‚Äôs unique story world context. Specifically, we tried to answer\nthe following research questions to determine if Patchview effec-\ntively supports the interactions described in Section 3.3.\n‚Ä¢RQ1: Does Patchview help the user with sensemaking world\nelements? (I1)\n‚Ä¢RQ2: Does Patchview help users express nuanced intentions\nwith visual steering? (I2)\n‚Ä¢RQ3: Does Patchview help users correct AI results and be-\nhaviors? (I3)\nPatchview: LLM-Powered Worldbuilding with Generative Dust and Magnet Visualization UIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA\nFigure 12: Prompts used for Patchview.\nTable 1: Participant backgrounds. AI Exp* stands for expe-\nrience with generative AI technologies (e.g., LLM, text-to-\nimage models), the former denoting any experience and the\nlatter indicating the use in their writing practice.\nExpertise Year Domain AI Exp*\nP1 Hobbyist 10 novel Y/Y\nP2 Hobbyist 19 novel, TRPG N/N\nP3 Hobbyist 8 novel Y/N\nP4 Hobbyist 6 novel Y/Y\nP5 Hobbyist 7 novel N/N\nP6 Hobbyist 5 novel Y/Y\nP7 Expert 8 screenwriting, game, TRPG Y/Y\nP8 Hobbyist 25 novel, fan fiction N/N\nP9 Hobbyist 5 novel Y/Y\nAdditionally, we aimed to discover how Patchview might be used\nin the worldbuilding process.\n‚Ä¢RQ4: How do users leverage features of Patchview for world-\nbuilding?\nTo answer these questions, we conducted a study that mixes a\nwithin-subject comparative task and an observational task, along\nwith both quantitative and qualitative analyses of collected data.\n5.1 Participants\nWe recruited nine participants (four women, three men, one non-\nbinary, and one who did not disclose gender, ages 24-51, M=33.4,\nSD=8.7) through Upwork1, a gigwork platform. We focused on re-\ncruiting hobbyists with extensive years of experience (at least five)\n1https://www.upwork.com/\nor professionals who make a living out of story writing and world-\nbuilding. Participants were proficient in English. Six participants\nhad experience using AI for story writing, and among them, five\nactively used AI for their practice. We detail participants in Table 1.\n5.2 Procedure\nThe study was conducted remotely via Google Meet 2. After wel-\ncoming the participants, we asked if they were okay with recording\nthe session. Then, we asked participants to watch two instruction\nvideos, each on 1) the overview of Patchview and ways to generate\nor create notes on the list module and 2) reading view visualiza-\ntions. After each video, participants were given an opportunity to\nexperiment with the functions that had just been introduced.\nAfter two instruction videos, we asked participants to conduct\nthe first task, answering sensemaking questions (RQ1). Specifically,\nwe provided two types of questions: 1) landscape questions , charac-\nterizing the distribution of world elements in relation to specific\nconcepts (e.g., To which faction most characters are associated\nwith?), and 2) comparison questions , comparing different charac-\nters according to their relevance to concepts (e.g., Which character\nis most associated with faction A?). These were multiple choice\nquestions with one correct option. We measured whether the par-\nticipants were correct and the time taken to answer. We expected\nthat if the visualization could help users with sensemaking, they\nwould answer more accurately in less time.\nParticipants conducted the task in a within-subject fashion, in\ntwo conditions: only with the list interface of Figure 5b (baseline)\nand together with the view visualization (treatment). We prepared\ntwo collections of elements, both focusing on character descriptions.\nOne collection considered three different factions to characterize\n2https://meet.google.com/\nUIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA Chung and Kreminski.\nelements. Another considered two axes of good-evil and law-chaos\nas concepts, which are often used as character alignment structures\nfor role-playing games such as Dungeons & Dragons [37]. We popu-\nlated each collection with 10 characters generated with Patchview.\nTo visualize characters, we leveraged Patchview‚Äôs recognition re-\nsults. The authors crafted questions after carefully reading through\nall generated elements (Appendix A). For each collection, we asked\nparticipants to answer both types of questions, with one collection\ngiven the baseline condition and the other with the treatment con-\ndition. We randomized the order of conditions to minimize ordering\neffects. For each question, we asked participants first to open the\nlink to the question. After they understood the question, we asked\nthem to open the link to the tool and answer the question with\nthe story world provided in the tool. We timed the time taken to\nanswer questions.\nAfter the first task, participants watched three more videos on\nPatchview‚Äôs functions: 1) creating and configuring views, 2) gen-\nerating and editing world elements with views, and 3) rewriting\nelements and connecting multiple views. As before, participants\nwere allowed to experiment with the just-introduced functions after\nfinishing each video.\nOnce all functions were introduced, we asked participants to\nperform a second task: building a story world with Patchview while\nthinking aloud. We asked them to create at least one view and put\nfive elements in the view. Moreover, we asked participants to place\nelements in the view where they think should be when finishing the\ntask. Through this task, we wanted to understand if participants\ncould use Patchview with concepts of their interest, visualizing\nelements (RQ1), steering generation with their nuanced intentions\n(RQ2), and correcting AI results and behaviors during the usage\n(RQ3). Moreover, we wanted to learn how Patchview supports the\nworldbuilding process (RQ4).\nTo understand participant behavior during this task, we col-\nlected logs of Patchview usage, including concepts that participants\nconsidered, steering inputs they made, outputs they received from\nPatchview, and corrections that they made to outputs. We also col-\nlected screen and think-aloud recordings. Participants could spend\nat most 40 minutes on this task. After the task, we asked participants\nto complete a survey and an interview. The survey asked about\nthe helpfulness of each Patchview feature and included Creativity\nSupport Index [12] questions on enjoyment, exploration, expres-\nsiveness, immersion, and the results of tool usage being worth the\neffort. Note that we did not use Creativity Support Index questions\nto compare the tool to others, only to gather participants‚Äô overall\nimpressions of the tool. The interview aimed to elicit detailed per-\nceptions about functions of Patchview and how Patchview could\nbe used in participants‚Äô actual practices. The whole study took at\nmost 120 minutes. Each participant received $60 for participation.\n5.3 Results\nWe analyzed survey responses (Figure 13), recognition and steering\nerrors from log data (Figure 14 and 16), answer time and correctness\nof the sensemaking questions (Figure 15), video recordings, and\ninterview data. We measured recognition errors by the difference\nbetween Patchview‚Äôs automatic placements of elements and the\nuser‚Äôs final placements of the same elements in views. This error\nwill be zero if the user does not reposition the placement, and\none if the user repositions an extreme value to another extreme\none (e.g., fully good to fully evil). We calculated steering errors\nby the difference between where the user placed steering inputs\nand the user‚Äôs final placement of world elements generated with\nthese inputs. This error will be zero if the content of the generated\nelement perfectly aligns with the user input, and one when the AI\ngenerates a totally misaligned element with the user input (e.g., a\nfully evil character generated with fully good input). This approach\nmeasures errors from the natural usage of the tool. However, note\nthat this approach also has a limitation, as correcting the error\ndoes have the cost of moving the element. Moreover, it cannot\nconsider errors of elements deleted during usage, as it requires the\nfinal placement of the element in the view by the user. Note that\nparticipants did not delete a high number of elements‚Äîparticipants\ndeleted 10 elements out of 181 for recognition and two elements\nout of 33 for steering. Moreover, we could not collect errors for\nrewriting interactions due to a technical issue. We analyzed video\nrecordings and interview data by iterative coding with inductive\nanalysis.\n5.3.1 RQ1: Visualization helped users with sensemaking world ele-\nments. The participants seemed to largely agree with how Patchview\nplaced world elements in the view. Figure 14 shows that the mean\nrecognition error was measured to be 0.04 on a 0-to-1 scale for\nthe user‚Äôs arbitrary concepts. This result resonates with partici-\npants‚Äô interview responses ( ùëÅ = 6). For instance, P9 mentioned\nthat Patchview accurately recognized concept relevance even in\nchallenging cases: ‚ÄúIt actually grasped my intention even though I\ngave two words, basically. ‚Äù\nWith largely accurate automatic visualization, in the first sur-\nvey question (Figure 13), participants responded that Patchview\nhelped them understand the landscape of elements in the story\nworld. The helpfulness of visualization also manifests in the sense-\nmaking question results (Figure 15), specifically for landscape ques-\ntions. When answering landscape questions, participants were sig-\nnificantly faster with visualization than without (Mann-Whitney\nùëà = 79, ùëõ1 = ùëõ2 = 9, ùëù < 0.0013) and more correctly answered ques-\ntions. However, for comparison questions, there was no significant\ndifference in time taken to answer questions between conditions\n(Mann-Whitney ùëà = 60, ùëõ1 = ùëõ2 = 9, ùëù > 0.05). Moreover, partici-\npants were similarly accurate in answering comparison questions.\nInterview results resonated with these findings: participants\nmentioned that they could easily understand the landscape of world\nelements with the help of visualization (ùëÅ = 9), allowing them to\ntrack generated elements while keeping the world under the rule\nand the structure. P1 mentioned: ‚Äú The different views and stuff,\nactually seeing that on there and keeping track of it, I think, would be\nhelpful. ... Because I end up building up too many and then I forget\nwhat the differences in each one‚Äôs personality are sometimes. ‚Äù P9 also\nappreciated the customizability of the visualization.\nPatchview‚Äôs visualization also influenced how participants thought\nabout each concept. That is, when participants do not agree with\nPatchview‚Äôs placement of elements, some participants reflected on\ntheir own perception of concepts (ùëÅ = 5), often concretizing how\nthey think about concepts. For example, P4 mentioned: ‚Äú When I\n3We used non-parametric test due to small sample size and non-equal variances.\nPatchview: LLM-Powered Worldbuilding with Generative Dust and Magnet Visualization UIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA\nThe visualization of the tool helped me understand the overall landscape of world elements\nThe visual steering helped me steer the generation of world elements with my nuanced intentions\nRepositioning of world element helped me convey my interpretation of the concept to the tool\nThe tool helped me to configure and create the story world with the topics and concepts of my interest\nExploration\n(It was easy for me to explore many different options, ideas, designs, or outcomes without a lot of tedious, repetitive intercations)\nEnjoyment\n(I was very absorbed/engaged in this activity - I enjoyed it and would do it again)\nResult worth effort\n(What I was able to produce was worth the effort required to produce it)\nImmersion\n(While I was doing the activity, the tool/interface/system \"disappeared,\" and I was able to concentrate on the activity)\nExpressiveness\n(I was able to be very expressive and creative while doing the activity)\n1 2 3 4 5 6 7\nFigure 13: Survey results.\n0.0 0.2 0.4 0.6 0.8 1.0\nError\nSteering (N=31)\nRecogition (N=171)\nFigure 14: Errors in 1) recognizing the concept weights for\nelements placed in the visualization and 2) steering the gen-\neration of elements according to concept weights specified\nby the user, measured on a 0-to-1 scale. The error bars in this\npaper indicate the 95% confidence intervals.\n0 50 100 150\na) Time taken (seconds)\nLandscape-Baseline\nLandscape-Visualization\nComparison-Baseline\nComparison-Visualization\n*\n0.0 0.5 1.0\nb) Correct Ratio\nFigure 15: Sensemaking question results. a) Time taken for\nanswering questions and b) correct ratio. * indicates ùëù < 0.001.\nput this ‚Äòstrength‚Äô, I was, kind of, just thinking about overall strength.\nAnd then, it interpreted it as physical strength. So I was like, ‚Äòthis\nis now all across these (elements), and now that really gives a more\nnuanced idea of. . . power‚Äô ‚Äù (Figure 17). However, there were also\ncases where the participant had a strong idea of how they think\nabout concepts, and for those cases, they realized that they would\nneed to sharpen their verbiage about the concept ( ùëÅ = 3). Some\nmentioned that future versions of the tool can explicitly support\nit. For example, P3 mentioned: ‚ÄúI think if there‚Äôs a pop-up that says,\n‚ÄòCan you give more information‚Äô or something like that, I think that\nwould help me to force some clarity before it visualizes. ‚Äù\n5.3.2 RQ2: Visual steering of Patchview allowed users to steer the\ngeneration with nuanced intentions.The results indicate that the\nsteering function was fairly accurate when used for arbitrary con-\ncepts of the participants‚Äô interests. The mean steering error was\nmeasured to be 0.18 on a 0-to-1 scale (Figure 14). As the ordinal\nscale of five on a bi-directional dimension is often considered to be\neasily discernible by people [48], if we assume uniform intervals\nbetween levels (which is often used in ML [ 48, 62]), the error of\n0.18 would be smaller than a single gap in a five-level ordinal scale\n(0.25). Hence, we conclude that Patchview allows users to steer the\ngeneration accurately in a granularity finer than easily discernible\nfive-level scale on dimensions with two concepts. While this stan-\ndard would need to be different for cases when a view has more\nthan two concepts, in our study, only four steered generation re-\nsults considered more than two concepts. As in the second question\nin Figure 13, participants also perceived that the tool helped them\nsteer the world element generation with their nuanced intentions.\nParticipants mentioned that visual steering for element genera-\ntion and rewriting was intuitive (ùëÅ = 7). For example, for visual\nrewriting interaction, P6 mentioned: ‚ÄúAll I had to do is to move where\nI wanted the story element to be reconnected to and that‚Äôs like a no-\nbrainer that just takes a couple of mouse clicks and you‚Äôre good to go. ‚Äù\nParticipants also noted that visual steering helped them express nu-\nanced intentions, even allowing them to realize the semantic space\nthat they could not think about (ùëÅ = 6). For example, P2 mentioned\nthat they could use visual steering to create a set of characters that\nwould make more conflicts than randomly generating them. On\nthe other hand, participants thought that natural language prompts\noften require more cognitive effort as they need to bring up specific\ninstructions (ùëÅ = 2). However, participants thought that natural\nlanguage prompts are beneficial as the user can be more specific in\nthe instruction (ùëÅ = 4). With different strengths, some participants\n(ùëÅ = 2) thought that visual steering and natural language prompt-\ning complement each other, as P7 mentioned, by ‚Äúchoosing the point\nvia steering and then giving it a little bit of (natural language) input. ‚Äù\nFor example, one limitation of the current visual rewriting interac-\ntion is that it often changes aspects the user likes. Adding a natural\nUIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA Chung and Kreminski.\n0 3 6 9 12 15\nNumber of Examples\n0.00\n0.25\n0.50\n0.75\n1.00\ncoeff = 0.014, p>0.05, R2 = 0.088\nb) Steering\n0 3 6 9 12 15\nNumber of Examples\n0.00\n0.25\n0.50\n0.75\n1.00Error\ncoeff = 0.006, p < 0.05, R2 = 0.036\na) Recognition\nFigure 16: a) Recognition and b) steering errors according to\nthe number of examples added by the user. Dot sizes indicate\nthe number of data points with the same errors and example\nnumbers. Each plot contains the linear regression result.\nlanguage prompt, such as specifying which aspects not to change,\ncould have alleviated the issue.\n5.3.3 RQ3: With more user examples, Patchview could only im-\nprove recognition, not the steering, but to a small extent.In the third\nquestion of Figure 13, most participants answered that they could\nconvey their interpretations of the concepts through repositioning\nelements. Similarly, during the interview, participants mentioned\nthat it was easy to revise AI results by simply moving elements\non the view or by rewriting interactions (ùëÅ = 6). For example, P7\nmentioned: ‚ÄúI really like that you have the ability to say, kind of like,\n‚ÄòNo I‚Äôm telling you where this should go‚Äô versus ‚ÄòI want you to actually\nadjust it to fit there. ‚Äô ‚Äù\nHowever, the user‚Äôs correction of concepts through the addition\nof more examples did not turn into dramatic changes in AI behav-\niors. As in Figure 16a, when we conducted a linear regression on\nthe relation between the number of examples and the recognition\nerror, the addition of more examples significantly decreased errors\n(ùëù < 0.05), but with a small magnitude (ùëêùëúùëí ùëì ùëì= ‚àí0.006) and a small\nability to explain variations (ùëÖ2 = 0.036). The analysis on steering\nerrors (Figure 16b) revealed no significance in the correlation be-\ntween the number of added examples and errors (ùëù > 0.05). These\nresonated with the interview responses: participants felt that the\nstudy session was not long enough to sense that the tool is learning\nfrom what they are doing in the tool (ùëÅ = 3). P7 mentioned that\nrather than having such tool behavior changes implicit, making\nthem more explicit to the user would be helpful: ‚ÄúI would have had\nto play with it a lot more to know if it actually was learning ... It‚Äôd\nbe interesting if I could have a feature to refresh ... So a refresh thing\nwould help me see what it was learning from me. ‚Äù\n5.3.4 RQ4: Participants could flexibly create their own story world\nand suggested ways to improve the tool for more comprehensive story\nwriting. With Patchview, participants could structure the story\nworld according to concepts of their interest. Table 2 shows the\nsummary of views participants created. Many participants created\nviews for alignments [24], either good-evil (ùëÅ = 5) or law-chaos\n(ùëÅ = 4) dimensions. It might be because these alignments are widely\nused to organize characters or because we used these dimensions\nas examples in the tutorial. Participants also created views with\ncustom concepts, such as factions ( ùëÅ = 3), locations (ùëÅ = 1), or\nother concepts of the participant‚Äôs interest (e.g., magical aptitude\nTable 2: Views created by participants. √óindicates that mul-\ntiple views are either tied or crossed with each other. For\ncases with more than two concepts in view, we noted com-\nmonalities between concepts instead of directly showing the\nconcepts themselves. El and char stand for element and char-\nacter, respectively.\nView concepts El # El Type\nP1 good - evil √ólaw - chaos 9 char\nP2 four factions 9 char\nP3 three life focuses 6 char\nP4\nstory timeline (beginning - end) 4 event\nmagical aptitude (high - low)\n√óphysical strength (strong - weak) 10 char, faction\ngood - evil 8 char, faction\nP5 good - evil √ólaw - chaos 3 faction, place\nthree locations 1 character\nP6 cats - pugs √ólibrary - tombs 13 char, faction,\nprop, event\nP7\ngood - evil √ólaw - chaos 5 char\nstory timeline (beginning - end) 3 event\ntwo factions 5 char\nP8\ngood - evil 8 char\nlaw - chaos 8 char\nhonest - deceitful 5 char\nP9\nlogical - emotional\n√óscience-oriented - belief-oriented 7 char, event\npositive event - negative event 3 event\nthree factions 5 char\nfrom Figure 17, ùëÅ = 9). One interesting view type was story time-\nline, where participants tried to align events between the story‚Äôs\nbeginning and end (Figure 18, ùëÅ = 2). It shows that participants\nwould eventually want to create a coherent storyline with the world\nelements they created. Participants added various element types to\nthe views, but the character was most frequently added.\nAs shown in the results of the fifth to ninth questions of Figure 13,\nparticipants felt that Patchview helped them expressively explore\nvarious ideas while enjoying and being immersed in the process,\nending up with a result that was worth their efforts. Participants\nthought that the tool could help with ideation or filling in details for\nthe part they are not good at (ùëÅ = 8). As generative features could\nadd new things to the user‚Äôs world, some participants mentioned\nthat AI generation would be most useful for the ideation stage\nor the settings where the user needs to constantly come up with\nnew elements (e.g., D&D), rather than for the cases where the user\nalready has a highly-structured and consistent world (ùëÅ = 2). P9\nalso mentioned that the visualization could facilitate collaboration\nwhen multiple people are working on the worldbuilding: ‚ÄúIf you‚Äôre\nwriting, let‚Äôs say with a group of people, ... it‚Äôs helpful to be able to\nquickly see ‚ÄòOkay this character is in this faction‚Äò, instead of having\nto go through it because you might not be the one who wrote it. ‚Äù\nWhile participants appreciated Patchview, they also made sug-\ngestions to improve the tool for worldbuilding and story writ-\ning practices. First, as story elements can have relationships with\nPatchview: LLM-Powered Worldbuilding with Generative Dust and Magnet Visualization UIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA\nFigure 17: Views created by P4, organized by magical aptitude, physical strength (which is noted as Weak-Strong), and good-evil\nalignment. Views include characters and factions as elements. Example elements are presented on the right and where they are\npositioned in the view is marked with the circles of the same border color.\nFigure 18: A timeline view created by P7.\neach other (e.g., relationships between characters), participants\nsuggested features to visualize such relationships, such as arrows\nbetween elements (ùëÅ = 4). Specifically with the temporal relation-\nships, as shown in the participants‚Äô usage patterns, participants\nmentioned that a view more specialized for timeline would help\n(ùëÅ = 2). As some elements could change with the story‚Äôs progress,\nP9 mentioned that it would be helpful to have the feature to ‚Äúdraw\nout‚Äú the trajectory of changes so that it can direct the tool to gen-\nerate those changes. Second, as world elements could constantly\nchange with the use of the tool, participants wanted the tool to\nhandle the consistency between elements (ùëÅ = 2). For example, if\nthe user edits one character‚Äôs name in one note, participants wanted\nthe tool to automatically update the character‚Äôs name appearing in\nother notes. Third, some participants wanted to flesh out world ele-\nments by adding relevant images with AI image generation models\n(ùëÅ = 2). They mentioned that additional visuals could help them\nnot only concretize the world element but also quickly grasp it.\nLastly, some wanted a feature to import their own world to the tool\nif they already have the world that they are working on (ùëÅ = 2).\n6 DISCUSSION\nWe discuss Patchview‚Äôs interaction design, utility in worldbuilding\nand story writing practices, technical alternatives, and limitations.\n6.1 Visually Bridging User and Generative AI\nWe introduce GD&M as a visual interaction to bridge the user and\ngenerative AIs. As mentioned in Section 3, GD&M is most applica-\nble when the user generates a lot of things within the conceptual\ndimensions of their interests. The interaction helps with the user‚Äôs\nevaluation, specification, and alignment of AI behaviors [ 42, 52].\nWe believe this interaction can be adopted to other use cases. In\nthe text domain, for instance, GD&M could be used for organizing\nand steering idea generation [47] with LLMs. Beyond text, it could\nalso be extended to the curation of image or video generation [2],\nshowing thumbnails instead of plain glyphs in the visualization.\nAs mentioned in Section 2.3, GD&M extends previous work [50]\nby allowing flexible configuration of arbitrary concepts on contin-\nuous dimensions. It has a benefit over the previous approach for\ncases when a single element has a mix of multiple attributes (e.g.,\na single character is associated with three factions). One finding\nregarding this continuous dimension was that people occasionally\ndisagree with where the tool places elements in the view. This\nmight be because finer granularity in continuous spaces allows\nusers to easily see such disagreements. Our findings indicate that\nthese disagreements facilitate reflection [33] and critical thinking\nabout the user‚Äôs concepts.\nGD&M also extends previous work by allowing users to correct\nAI behaviors directly in the visual spaces. While the interaction\nitself holds promise, aligning AI behaviors to user-corrected ex-\namples was challenging in our version of the tool. This might be\nbecause there was little room for improvement as the error was\nalready low without any user-added examples. Future work can\nUIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA Chung and Kreminski.\nexplore technical improvements, such as selecting examples that\ncan maximize the performance of recognition and steering [36, 45].\n6.2 AI-Supported Worldbuilding and\nStorytelling\nWe found that LLMs could support worldbuilding by providing\nideas and filling in parts of the world on behalf of the users. How-\never, AI may have both positive and negative effects on creative\ntasks like worldbuilding. For example, previous work showed that\nLLM usage could drive users to produce more homogenous re-\nsponses to a divergent ideation task [3]. This could be a problem in\na worldbuilding context if users hope to create truly unique worlds.\nAs future work, it would be worthwhile to investigate what other\nproblems LLMs might introduce in the context of story writing and\nwhat measures might be taken to tackle those problems.\nStudy participants expressed a clear desire to use worlds created\nwith Patchview as the basis for longer-form stories. In particular,\nparticipants repeatedly expressed a desire for a specialized timeline\nview that would enable them to organize world events into a coher-\nent chronology, and in two cases even improvised a timeline view\nusing the existing Patchview feature set (Figure 18). AI-supported\nstorytelling might involve many different levels of user control,\nranging from humans writing every aspect of the story to the full\nsimulation of the story world by AI [44]; previous work hinted at\nuser specification of a high-level story arc [17] and participation as\na character in the story [44], but many novel interaction paradigms\nremain for future work to explore. Technically, LLM-based story\nworld simulation would likely face consistency issues [32] due to\nthe tendency of LLMs to ‚Äúhallucinate‚Äù [26], which would need to\nbe addressed to support coherent storytelling.\n6.3 Technical Alternatives to Prompt\nEngineering and Closed Models\nThe prompt engineering techniques we used in Patchview (includ-\ning chain-of-thought) yielded good performance on both recogni-\ntion and steering tasks without any additional training or control\ntechniques. This suggests that current general-purpose LLMs are\ncapable of reasoning effectively about concept relatedness, even\nalong continuous dimensions defined between arbitrary concepts.\nHowever, the current Patchview prototype is limited in its interac-\ntivity by relatively high latency. For instance, with two concepts in\na view, both steering the generation of a new world element and\nrecognizing its position took longer than 15 seconds. This latency\nis due partly to the large size of the underlying LLM and partly to\nour prompting techniques: increasing numbers of world elements,\nconcepts, and user examples cause our prompts to become pro-\ngressively longer, and chain-of-thought prompting increases the\nnumber of tokens generated in response to each prompt, further\ndriving up latency. Additionally, current general-purpose LLMs are\nnot specifically tuned for creative applications, resulting in clear\nweaknesses for creative work [10]. These options are often trained\non loosely defined preferences [43], rather than using rewards more\ntargeted for creative applications. As they are often closed models,\nimproving on those models would be challenging.\nWe suggest that future work can explore other options than\nprompt engineering and closed models. To control generation with\nthe concepts on the continuous dimensions, we can consider the\nmanipulation of task representations in the hidden layers of the\nLLM [54, 64]. One benefit of this approach is that, once we have\nthe vector about the concept, it does not require tokens for con-\nveying concepts and examples in the prompts. Moreover, if this\napproach is robust enough, chain-of-thought also might not be\nnecessary. However, future work would need to validate if this ap-\nproach can effectively steer generation with higher efficiency than\nprompt engineering. Moreover, how to consider the user-corrected\nexamples with this approach is also moot. Alternative to closed\nmodels would be smaller-sized open models that are fine-tuned\nto creative use cases, which is becoming more feasible with many\nopen-source LLM options [27, 53]. Building upon these open-source\nLLMs, researchers recently introduced models specialized for cre-\native writing [ 56]. Building upon these efforts, avenues we can\nexplore include having a higher quality creative writing dataset\nwith annotations on various aspects of the text quality (e.g., engag-\ningness, novelty, diversity) and tuning models while considering\nthose aspects as losses or rewards [59].\n6.4 Limitations\nOur tool‚Äôs usability and functionalities could be improved in the\nfuture. For instance, adding filtering functions to the list mod-\nule would likely help the user find relevant elements when there\nare many elements to sort through. Future versions could also\nbetter handle under-specified or irrelevant concepts. The current\nPatchview would try to get how the user interprets those concepts\nfrom examples provided during the usage. However, such an ap-\nproach would still have limitations as Patchview relies on prompt\nengineering, and alleviating those issues can be future work.\nOne limitation of our study is that we could not collect steering\nresults data for rewriting interactions due to technical issues. More-\nover, our technical analysis is not the most rigorous but prioritized\nanalyzing data naturally collected during the study. For instance,\nthe user study data might have only covered a subset of genres,\nsettings, and concepts that would frequently used for worldbuilding.\nThe error rates could also have been confounded by the fact that\nmarking errors could incur a small additional cost to users as they\nneed to move elements manually. Due to these reasons, future work\nmight involve a more rigorous technical evaluation. This future\nwork could also evaluate other technical implementation options\nmentioned in Section 6.3.\nWhile Patchview might be most effective in long-term projects\n(as it is designed to help users create and organize an expansive\nfictional world consisting of many distinct elements), our study\ninvolved only a single session. Future work might investigate the\nuse of Patchview for long-term projects, including the extension\nof already existing story worlds. Furthermore, we did not compare\nthe design of Patchview to other alternatives when the user creates\ntheir own story world with LLMs; we instead focused more on\nidentifying usage patterns and whether the tool is technically able\nto achieve its design goals. Future work may investigate comparison\nto other tools.\nPatchview: LLM-Powered Worldbuilding with Generative Dust and Magnet Visualization UIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA\n7 CONCLUSION\nWe introduce Patchview, an LLM-powered worldbuilding tool that\nadopts generative dust and magnet (GD&M) interactions to support\ninteraction with generative AI. With GD&M, Patchview facilitates\nsensemaking of generated story elements by placing elements close\nto concepts of high relevance, similar to how magnets attract iron\ndust particles. It also supports generation steering and AI behavior\ncorrection by leveraging the visual space configured by concepts.\nA user study showed that Patchview could facilitate understanding\nthe landscape of story world elements and steering of element\ngeneration with nuanced intentions that are difficult to express in\nnatural language alone. The interaction of correcting misaligned AI\nresults was intuitive, but those corrections minimally improved the\nalignment of AI behaviors to the user‚Äôs perception, indicating one\npossible direction for future work. We hope Patchview and GD&M\nprovide insights on visual interactions for evaluation, specification,\nand alignment of generative AI behaviors to the user‚Äôs intention.\nACKNOWLEDGMENTS\nWe thank the many people at Midjourney who provided infrastruc-\ntural and logistical support for this work. We also thank Yoonjoo\nLee, Jordan Huffaker, and Kihoon Son for giving feedback to the\nearly prototype of Patchview, and user study participants for their\nvaluable insights on the tool.\nREFERENCES\n[1] [n. d.]. Sudowrite. https://www.sudowrite.com/\n[2] Shm Garanganao Almeda, J. D. Zamfirescu-Pereira, Kyu Won Kim, Pradeep Mani\nRathnam, and Bjoern Hartmann. 2024. Prompting for Discovery: Flexible Sense-\nMaking for AI Art-Making with Dreamsheets. arXiv:2310.09985 [cs.HC]\n[3] Barrett R. Anderson, Jash Hemant Shah, and Max Kreminski. 2024. Homog-\nenization Effects of Large Language Models on Human Creative Ideation.\narXiv:2402.01536 [cs.HC]\n[4] Tyler Angert, Miroslav Suzara, Jenny Han, Christopher Pondoc, and Hariharan\nSubramonyam. 2023. Spellburst: A Node-Based Interface for Exploratory Creative\nCoding with Natural Language Prompts. In Proceedings of the 36th Annual ACM\nSymposium on User Interface Software and Technology (San Francisco, CA, USA)\n(UIST ‚Äô23) . Association for Computing Machinery, New York, NY, USA, Article\n100, 22 pages. https://doi.org/10.1145/3586183.3606719\n[5] Anthropic. 2023. Model Card and Evaluations for Claude Models. https://www-\nfiles.anthropic.com/production/images/Model-Card-Claude-2.pdf\n[6] Ian Arawjo, Chelse Swoopes, Priyan Vaithilingam, Martin Wattenberg, and Elena\nGlassman. 2023. ChainForge: A Visual Toolkit for Prompt Engineering and LLM\nHypothesis Testing. arXiv:2309.09128 [cs.HC]\n[7] Oloff C. Biermann, Ning F. Ma, and Dongwook Yoon. 2022. From Tool to Compan-\nion: Storywriters Want AI Writers to Respect Their Personal Values and Writing\nStrategies. In Proceedings of the 2022 ACM Designing Interactive Systems Confer-\nence (Virtual Event, Australia) (DIS ‚Äô22) . Association for Computing Machinery,\nNew York, NY, USA, 1209‚Äì1227. https://doi.org/10.1145/3532106.3533506\n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Pra-\nfulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\nSandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse,\nMark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language Models are Few-Shot Learners. InAdvances in Neural In-\nformation Processing Systems , H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan,\nand H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877‚Äì1901. https://proceedings.\nneurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\n[9] Alex Calderwood, Vivian Qiu, K. Gero, and Lydia B. Chilton. 2020. How Nov-\nelists Use Generative Language Models: An Exploratory User Study. In HAI-\nGEN+user2agent@IUI. https://api.semanticscholar.org/CorpusID:233479959\n[10] Tuhin Chakrabarty, Philippe Laban, Divyansh Agarwal, Smaranda Muresan, and\nChien-Sheng Wu. 2024. Art or Artifice? Large Language Models and the False\nPromise of Creativity. arXiv:2309.14556 [cs.CL]\n[11] Nan-Chen Chen, Jina Suh, Johan Verwey, Gonzalo Ramos, Steven Drucker, and\nPatrice Simard. 2018. AnchorViz: Facilitating Classifier Error Discovery through\nInteractive Semantic Data Exploration. In 23rd International Conference on Intelli-\ngent User Interfaces (Tokyo, Japan) (IUI ‚Äô18) . Association for Computing Machin-\nery, New York, NY, USA, 269‚Äì280. https://doi.org/10.1145/3172944.3172950\n[12] Erin Cherry and Celine Latulipe. 2014. Quantifying the Creativity Support of\nDigital Tools through the Creativity Support Index. ACM Trans. Comput.-Hum.\nInteract. 21, 4, Article 21 (jun 2014), 25 pages. https://doi.org/10.1145/2617588\n[13] Jean-Pe√Øc Chou, Alexa Fay Siu, Nedim Lipka, Ryan Rossi, Franck Dernoncourt,\nand Maneesh Agrawala. 2023. TaleStream: Supporting Story Ideation with Trope\nKnowledge. In Proceedings of the 36th Annual ACM Symposium on User Interface\nSoftware and Technology (San Francisco, CA, USA) (UIST ‚Äô23) . Association for\nComputing Machinery, New York, NY, USA, Article 52, 12 pages. https://doi.\norg/10.1145/3586183.3606807\n[14] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez,\nAbhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran,\nEmily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin,\nMichael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay\nGhemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin\nRobinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek\nLim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\nAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr\nPolozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz,\nOrhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling\nwith Pathways. arXiv:2204.02311 [cs.CL]\n[15] John Joon Young Chung and Eytan Adar. 2023. PromptPaint: Steering Text-to-\nImage Generation Through Paint Medium-like Interactions. In Proceedings of\nthe 36th Annual ACM Symposium on User Interface Software and Technology (San\nFrancisco, CA, USA) (UIST ‚Äô23). Association for Computing Machinery, New York,\nNY, USA, Article 6, 17 pages. https://doi.org/10.1145/3586183.3606777\n[16] John Joon Young Chung, Shiqing He, and Eytan Adar. 2022. Artist Support\nNetworks: Implications for Future Creativity Support Tools. In Proceedings of\nthe 2022 ACM Designing Interactive Systems Conference (Virtual Event, Australia)\n(DIS ‚Äô22). Association for Computing Machinery, New York, NY, USA, 232‚Äì246.\nhttps://doi.org/10.1145/3532106.3533505\n[17] John Joon Young Chung, Wooseok Kim, Kang Min Yoo, Hwaran Lee, Eytan\nAdar, and Minsuk Chang. 2022. TaleBrush: Sketching Stories with Generative\nPretrained Language Models. In Proceedings of the 2022 CHI Conference on Human\nFactors in Computing Systems (New Orleans, LA, USA) (CHI ‚Äô22) . Association\nfor Computing Machinery, New York, NY, USA, Article 209, 19 pages. https:\n//doi.org/10.1145/3491102.3501819\n[18] John Joon Young Chung, Hijung Valentina Shin, Haijun Xia, Li-yi Wei, and Ruba-\niat Habib Kazi. 2021. Beyond Show of Hands: Engaging Viewers via Expressive\nand Scalable Visual Communication in Live Streaming. In Proceedings of the 2021\nCHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI\n‚Äô21). Association for Computing Machinery, New York, NY, USA, Article 109,\n14 pages. https://doi.org/10.1145/3411764.3445419\n[19] Elizabeth Clark, Anne Spencer Ross, Chenhao Tan, Yangfeng Ji, and Noah A.\nSmith. 2018. Creative Writing with a Machine in the Loop: Case Studies on\nSlogans and Stories. In 23rd International Conference on Intelligent User Interfaces\n(Tokyo, Japan) (IUI ‚Äô18) . Association for Computing Machinery, New York, NY,\nUSA, 329‚Äì340. https://doi.org/10.1145/3172944.3172983\n[20] Hai Dang, Frederik Brudy, George Fitzmaurice, and Fraser Anderson. 2023.\nWorldSmith: Iterative and Expressive Prompting for World Building with a\nGenerative AI. In Proceedings of the 36th Annual ACM Symposium on User\nInterface Software and Technology (San Francisco, CA, USA) (UIST ‚Äô23) . Asso-\nciation for Computing Machinery, New York, NY, USA, Article 63, 17 pages.\nhttps://doi.org/10.1145/3586183.3606772\n[21] Karin Fast and Henrik √ñrnebring. 2017. Transmedia world-building: The Shadow\n(1931‚Äìpresent) and Transformers (1984‚Äìpresent). International Journal of Cul-\ntural Studies 20, 6 (2017), 636‚Äì652. https://doi.org/10.1177/1367877915605887\narXiv:https://doi.org/10.1177/1367877915605887\n[22] Katy Ilonka Gero, Tao Long, and Lydia B Chilton. 2023. Social Dynamics of\nAI Support in Creative Writing. In Proceedings of the 2023 CHI Conference on\nHuman Factors in Computing Systems (Hamburg, Germany) (CHI ‚Äô23). Association\nfor Computing Machinery, New York, NY, USA, Article 245, 15 pages. https:\n//doi.org/10.1145/3544548.3580782\n[23] Yuan Gong, Youxin Pang, Xiaodong Cun, Menghan Xia, Yingqing He, Haoxin\nChen, Longyue Wang, Yong Zhang, Xintao Wang, Ying Shan, and Yujiu Yang.\n2023. Interactive Story Visualization with Multiple Characters. In SIGGRAPH\nAsia 2023 Conference Papers (Sydney, NSW, Australia)(SA ‚Äô23). Association for\nComputing Machinery, New York, NY, USA, Article 101, 10 pages. https://doi.\norg/10.1145/3610548.3618184\n[24] T. Hergenrader. 2018.Collaborative Worldbuilding for Writers and Gamers . Blooms-\nbury Academic. https://books.google.co.kr/books?id=z-_7swEACAAJ\nUIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA Chung and Kreminski.\n[25] Md Naimul Hoque, Bhavya Ghai, Kari Kraus, and Niklas Elmqvist. 2023. Por-\ntrayal: Leveraging NLP and Visualization for Analyzing Fictional Characters. In\nProceedings of the 2023 ACM Designing Interactive Systems Conference (Pittsburgh,\nPA, USA) (DIS ‚Äô23) . Association for Computing Machinery, New York, NY, USA,\n74‚Äì94. https://doi.org/10.1145/3563657.3596000\n[26] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,\nYe Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of Hallucination in\nNatural Language Generation. ACM Comput. Surv. 55, 12, Article 248 (mar 2023),\n38 pages. https://doi.org/10.1145/3571730\n[27] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche\nSavary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou\nHanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample,\nL√©lio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep\nSubramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Th√©ophile Gervet,\nThibaut Lavril, Thomas Wang, Timoth√©e Lacroix, and William El Sayed. 2024.\nMixtral of Experts. arXiv:2401.04088 [cs.LG]\n[28] Peiling Jiang, Jude Rayan, Steven P. Dow, and Haijun Xia. 2023. Graphologue: Ex-\nploring Large Language Model Responses with Interactive Diagrams. In Proceed-\nings of the 36th Annual ACM Symposium on User Interface Software and Technology\n(San Francisco, CA, USA) (UIST ‚Äô23) . Association for Computing Machinery, New\nYork, NY, USA, Article 3, 20 pages. https://doi.org/10.1145/3586183.3606737\n[29] Taewook Kim, Hyomin Han, Eytan Adar, Matthew Kay, and John Joon Young\nChung. 2024. Authors‚Äô Values and Attitudes Towards AI-bridged Scalable Person-\nalization of Creative Language Arts. In Proceedings of the 2024 CHI Conference on\nHuman Factors in Computing Systems (Honolulu, HI, USA) (CHI ‚Äô24) . Association\nfor Computing Machinery, New York, NY, USA.\n[30] Tae Soo Kim, Yoonjoo Lee, Minsuk Chang, and Juho Kim. 2023. Cells, Gen-\nerators, and Lenses: Design Framework for Object-Oriented Interaction with\nLarge Language Models. In Proceedings of the 36th Annual ACM Symposium on\nUser Interface Software and Technology (San Francisco, CA, USA) (UIST ‚Äô23) . As-\nsociation for Computing Machinery, New York, NY, USA, Article 4, 18 pages.\nhttps://doi.org/10.1145/3586183.3606833\n[31] Max Kreminski, Melanie Dickinson, Noah Wardrip-Fruin, and Michael Mateas.\n2022. Loose Ends: A Mixed-Initiative Creative Interface for Playful Storytelling.\nProceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital\nEntertainment 18, 1 (Oct. 2022), 120‚Äì128. https://doi.org/10.1609/aiide.v18i1.21955\n[32] Max Kreminski and Chris Martens. 2022. Unmet creativity support needs in\ncomputationally supported creative writing. In Proceedings of the First Workshop\non Intelligent and Interactive Writing Assistants (In2Writing 2022) . 74‚Äì82.\n[33] Max Kreminski and Michael Mateas. 2021. Reflective Creators. In International\nConference on Computational Creativity . 309‚Äì318.\n[34] Mina Lee, Katy Ilonka Gero, John Joon Young Chung, Simon Buckingham Shum,\nVipul Raheja, Hua Shen, Subhashini Venugopalan, Thiemo Wambsganss, David\nZhou, Emad A. Alghamdi, Tal August, Avinash Bhat, Madiha Zahrah Choksi,\nSenjuti Dutta, Jin L.C. Guo, Md Naimul Hoque, Yewon Kim, Simon Knight,\nSeyed Parsa Neshaei, Antonette Shibani, Disha Shrivastava, Lila Shroff, Agnia\nSergeyuk, Jessi Stark, Sarah Sterman, Sitong Wang, Antoine Bosselut, Daniel\nBuschek, Joseph Chee Chang, Sherol Chen, Max Kreminski, Joonsuk Park, Roy\nPea, Eugenia Ha Rim Rho, Zejiang Shen, and Pao Siangliulue. 2024. A Design\nSpace for Intelligent and Interactive Writing Assistants. InProceedings of the 2024\nCHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA)\n(CHI ‚Äô24) . Association for Computing Machinery, New York, NY, USA.\n[35] Mina Lee, Percy Liang, and Qian Yang. 2022. CoAuthor: Designing a Human-AI\nCollaborative Writing Dataset for Exploring Language Model Capabilities. In\nProceedings of the 2022 CHI Conference on Human Factors in Computing Systems\n(New Orleans, LA, USA) (CHI ‚Äô22) . Association for Computing Machinery, New\nYork, NY, USA, Article 388, 19 pages. https://doi.org/10.1145/3491102.3502030\n[36] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and\nWeizhu Chen. 2022. What Makes Good In-Context Examples for GPT-3?. In Pro-\nceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowl-\nedge Extraction and Integration for Deep Learning Architectures , Eneko Agirre, Mar-\nianna Apidianaki, and Ivan Vuliƒá (Eds.). Association for Computational Linguis-\ntics, Dublin, Ireland and Online, 100‚Äì114. https://doi.org/10.18653/v1/2022.deelio-\n1.10\n[37] I. Livingstone. 1986. Dicing with Dragons: An Introduction to Role-Playing\nGames. Penguin Group USA, Incorporated. https://books.google.co.kr/books?\nid=uBqXPwAACAAJ\n[38] Ryan Louie, Andy Coenen, Cheng Zhi Huang, Michael Terry, and Carrie J. Cai.\n2020. Novice-AI Music Co-Creation via AI-Steering Tools for Deep Generative\nModels. In Proceedings of the 2020 CHI Conference on Human Factors in Computing\nSystems (Honolulu, HI, USA) (CHI ‚Äô20). Association for Computing Machinery,\nNew York, NY, USA, 1‚Äì13. https://doi.org/10.1145/3313831.3376739\n[39] Krzysztof M Maj. 2015. Transmedial world-building in fictional narratives. IM-\nAGE. Zeitschrift f√ºr interdisziplin√§re Bildwissenschaft 11, 2 (2015), 83‚Äì96.\n[40] Piotr Mirowski, Kory W. Mathewson, Jaylen Pittman, and Richard Evans. 2023.\nCo-Writing Screenplays and Theatre Scripts with Language Models: Evaluation\nby Industry Professionals. In Proceedings of the 2023 CHI Conference on Human\nFactors in Computing Systems (Hamburg, Germany) (CHI ‚Äô23) . Association for\nComputing Machinery, New York, NY, USA, Article 355, 34 pages. https://doi.\norg/10.1145/3544548.3581225\n[41] Tamara Munzner. 2014. Visualization analysis and design . CRC press.\n[42] Donald A. Norman. 2002. The design of everyday things . Basic Books,\n[New York]. http://www.amazon.de/The-Design-Everyday-Things-\nNorman/dp/0465067107/ref=wl_it_dp_o_pC_S_nC?ie=UTF8&colid=\n151193SNGKJT9&coliid=I262V9ZRW8HR2C\n[43] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022.\nTraining language models to follow instructions with human feedback. Advances\nin neural information processing systems 35 (2022), 27730‚Äì27744.\n[44] Joon Sung Park, Joseph O‚ÄôBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy\nLiang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simulacra\nof Human Behavior. In Proceedings of the 36th Annual ACM Symposium on User\nInterface Software and Technology (San Francisco, CA, USA)(UIST ‚Äô23). Association\nfor Computing Machinery, New York, NY, USA, Article 2, 22 pages. https:\n//doi.org/10.1145/3586183.3606763\n[45] Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022. Learning To Retrieve\nPrompts for In-Context Learning. In Proceedings of the 2022 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Hu-\nman Language Technologies , Marine Carpuat, Marie-Catherine de Marneffe, and\nIvan Vladimir Meza Ruiz (Eds.). Association for Computational Linguistics, Seat-\ntle, United States, 2655‚Äì2671. https://doi.org/10.18653/v1/2022.naacl-main.191\n[46] Natalia Samutina. 2016. Fan fiction as world-building: Transformative reception\nin crossover writing. Continuum 30, 4 (2016), 433‚Äì450.\n[47] Pao Siangliulue, Joel Chan, Steven P. Dow, and Krzysztof Z. Gajos. 2016. Idea-\nHound: Improving Large-scale Collaborative Ideation with Crowd-Powered Real-\ntime Semantic Modeling. In Proceedings of the 29th Annual Symposium on User\nInterface Software and Technology (Tokyo, Japan) (UIST ‚Äô16). Association for Com-\nputing Machinery, New York, NY, USA, 609‚Äì624. https://doi.org/10.1145/2984511.\n2984578\n[48] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning,\nAndrew Ng, and Christopher Potts. 2013. Recursive Deep Models for Semantic\nCompositionality Over a Sentiment Treebank. In Proceedings of the 2013 Con-\nference on Empirical Methods in Natural Language Processing , David Yarowsky,\nTimothy Baldwin, Anna Korhonen, Karen Livescu, and Steven Bethard (Eds.).\nAssociation for Computational Linguistics, Seattle, Washington, USA, 1631‚Äì1642.\nhttps://aclanthology.org/D13-1170\n[49] Hari Subramonyam, Roy Pea, Christopher Lawrence Pondoc, Maneesh Agrawala,\nand Colleen Seifert. 2024. Bridging the Gulf of Envisioning: Cognitive Challenges\nin Prompt Based Interactions with LLMs. (2024).\n[50] Sangho Suh, Meng Chen, Bryan Min, Toby Jia-Jun Li, and Haijun Xia. 2023.\nStructured Generation and Exploration of Design Space with Large Language\nModels for Human-AI Co-Creation. arXiv:2310.12953 [cs.HC]\n[51] Sangho Suh, Bryan Min, Srishti Palani, and Haijun Xia. 2023. Sensecape: Enabling\nMultilevel Exploration and Sensemaking with Large Language Models. InProceed-\nings of the 36th Annual ACM Symposium on User Interface Software and Technology\n(San Francisco, CA, USA) (UIST ‚Äô23) . Association for Computing Machinery, New\nYork, NY, USA, Article 1, 18 pages. https://doi.org/10.1145/3586183.3606756\n[52] Michael Terry, Chinmay Kulkarni, Martin Wattenberg, Lucas Dixon, and Mered-\nith Ringel Morris. 2023. AI Alignment in the Design of Interactive AI: Specification\nAlignment, Process Alignment, and Evaluation Support. arXiv:2311.00710 [cs.HC]\n[53] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucu-\nrull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia\nGao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini,\nRui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut\nLavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet,\nTodor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton,\nJeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,\nEric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross\nTaylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov,\nYuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2:\nOpen Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL]\n[54] Theia Vogel. 2024. repeng. https://github.com/vgel/repeng/\n[55] Nick Walton. 2019. AI Dungeon 2. https://aidungeon.cc/\n[56] Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin Wang,\nZhaowei Gao, Chunzhao Xie, Chuou Xu, Jihong Dai, Yibin Liu, Jialong Wu,\nShengwei Ding, Long Li, Zhiwei Huang, Xinle Deng, Teng Yu, Gangan Ma, Han\nXiao, Zixin Chen, Danjun Xiang, Yunxia Wang, Yuanyuan Zhu, Yi Xiao, Jing\nWang, Yiru Wang, Siran Ding, Jiayang Huang, Jiayi Xu, Yilihamu Tayier, Zhenyu\nHu, Yuan Gao, Chengfeng Zheng, Yueshu Ye, Yihang Li, Lei Wan, Xinyue Jiang,\nYujie Wang, Siyu Cheng, Zhule Song, Xiangru Tang, Xiaohua Xu, Ningyu Zhang,\nHuajun Chen, Yuchen Eleanor Jiang, and Wangchunshu Zhou. 2024. Weaver:\nFoundation Models for Creative Writing. arXiv:2401.17268 [cs.CL]\nPatchview: LLM-Powered Worldbuilding with Generative Dust and Magnet Visualization UIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA\n0.0 0.2 0.4 0.6 0.8 1.0\nError\nRecog for steered (N=31)\nRecog for non-steered (N=140)\nFigure 19: Errors in recognizing concept weights for elements\nplaced in the visualization, when elements are generated (1)\nwithout and (2) with steering inputs.\n[57] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter,\nFei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022. Chain-of-Thought\nPrompting Elicits Reasoning in Large Language Models. In Advances in\nNeural Information Processing Systems , S. Koyejo, S. Mohamed, A. Agar-\nwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates,\nInc., 24824‚Äì24837. https://proceedings.neurips.cc/paper_files/paper/2022/file/\n9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf\n[58] Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022. AI Chains: Transparent\nand Controllable Human-AI Interaction by Chaining Large Language Model\nPrompts. InProceedings of the 2022 CHI Conference on Human Factors in Computing\nSystems (New Orleans, LA, USA)(CHI ‚Äô22). Association for Computing Machinery,\nNew York, NY, USA, Article 385, 22 pages. https://doi.org/10.1145/3491102.\n3517582\n[59] Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Am-\nmanabrolu, Noah A Smith, Mari Ostendorf, and Hannaneh Hajishirzi. 2023. Fine-\nGrained Human Feedback Gives Better Rewards for Language Model Training.\narXiv preprint arXiv:2306.01693 (2023).\n[60] Ji Soo Yi, Rachel Melton, John Stasko, and Julie A. Jacko. 2005. Dust & Magnet:\nMultivariate Information Visualization Using a Magnet Metaphor.Information Vi-\nsualization 4, 4 (oct 2005), 239‚Äì256. https://doi.org/10.1057/palgrave.ivs.9500099\n[61] Ann Yuan, Andy Coenen, Emily Reif, and Daphne Ippolito. 2022. Wordcraft: Story\nWriting With Large Language Models. In 27th International Conference on Intelli-\ngent User Interfaces (Helsinki, Finland) (IUI ‚Äô22) . Association for Computing Ma-\nchinery, New York, NY, USA, 841‚Äì852. https://doi.org/10.1145/3490099.3511105\n[62] Biqiao Zhang, Georg Essl, and Emily Mower Provost. 2017. Predicting the distri-\nbution of emotion perception: capturing inter-rater variability. In Proceedings of\nthe 19th ACM International Conference on Multimodal Interaction (Glasgow, UK)\n(ICMI ‚Äô17) . Association for Computing Machinery, New York, NY, USA, 51‚Äì59.\nhttps://doi.org/10.1145/3136755.3136792\n[63] Andrew Zhu, Lara Martin, Andrew Head, and Chris Callison-Burch. 2023. CA-\nLYPSO: LLMs as Dungeon Masters‚Äô Assistants. In Proceedings of the Nine-\nteenth AAAI Conference on Artificial Intelligence and Interactive Digital En-\ntertainment (Salt Lake City) (AIIDE ‚Äô23) . AAAI Press, Article 39, 11 pages.\nhttps://doi.org/10.1609/aiide.v19i1.27534\n[64] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren,\nAlexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shash-\nwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart,\nSanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, and Dan Hendrycks.\n2023. Representation Engineering: A Top-Down Approach to AI Transparency.\narXiv:2310.01405 [cs.LG]\nA SENSEMAKING QUESTIONS\nWe list sensemaking questions used in the user study in Table 3.\nB COMPARISON BETWEEN ELEMENTS\nGENERATED WITH OR WITHOUT VISUAL\nSTEERING\nWe compare how Patchview generates elements differently with\nand without visual steering inputs. In Figure 19, we show how the\nrecognition errors vary, and observe that they are similarly low\nregardless of whether visual steering input was used. With Welch‚Äôs\nt-test, we found no significant difference between these two groups\n(ùë° (51.86)= 0.058, ùëù> 0.5).\nFigure 20 shows world elements generated by P1 without any\nsteering (i.e., using only the element generation button); with a\nnatural language prompt; and with visual steering. As shown in the\ncase of ‚ÄúTristan Blackmoore, ‚Äù not using any steering could result in\nan under-specified element description. Natural language prompts\ncould help with steering generation, but as shown in Figure 20b,\nexpressing nuanced intentions could be tough and not all prompts\nresulted in detailed and expressive descriptions of elements. Visual\nsteering (Figure 20c) could be a complement to this, allowing users\nto express nuanced intentions with simple placement of a visual\nmarker.\nC EXAMPLE WORLD\nWe share an additional partial example of a user-created world from\nthe user study in Figure 21.\nUIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA Chung and Kreminski.\nTable 3: Questions used in the sensemaking tasks of the user study. In the type, L stands for landscape understanding questions\nand C stands for the comparison questions.\nWorld Type Question Options Answer\n1 L\nWhich faction is\nlinked to the\nsmallest number of\ncharacters in this\nworld?\nThe Faerie Fleet (a mysterious group of tiny winged humanoids that pilot\ndelicate yet powerful ships grown from seeds)\nThe Iron Brigade (a regiment of steampunk automatons that pilot bulky\nironclad warships)\nO\nThe Skysharks (a clan of winged reptilian mercenaries that fly agile bioships\ngrown from eggs)\n1 C\nChoose the\ncharacter that is\nleast associated with\nSkysharks.\nCogwhistle is a 112-year old brass automaton who serves as an elite com-\nmander in the Iron Brigade. With a clockwork mind and pneumatic limbs,\nCogwhistle is utterly devoted to his steam-driven brethren yet feels a flicker-\ning fascination with the graceful faeries that contrasts his mechanical nature.\nFrostwind is a 31-year old winged velociraptor mercenary who serves as\nRazortooth‚Äôs trusted lieutenant in the Skysharks. Hatched from a faerie-\nspliced egg, he has some fae ancestry that gives him an icy demeanor and\ntalent for aerial combat. Frostwind is coldly loyal to Razortooth yet feels a\nfaint kinship with Silverblossom.\nRazortooth is a 37-year old winged velociraptor mercenary who leads the\nSkysharks clan. He is larger and more cunning than the rest of his kind, and\nis utterly ruthless in battle. His personal bioship Razors Edge is the fastest\nand most maneuverable ship in the clan.\nSilvercog is a 17-year-old faerie automaton who escaped the Iron Brigade\nto join the Skysharks. Forged from faerie dust and brass, she has a precise\nclockwork mind yet yearns for the grace and freedom of her fae ancestors.\nThough mistrusted by Razortooth, Silvercog bonds with Silverslice over their\nshared outcast status and conflicted origins.\nO\n2 L\nWhich dimension is\nassociated with the\ngreatest number of\ncharacters?\nGood-Law O\nGood-Chaotic\nEvil-Law\nEvil-Chaotic\n2 C Which character is\nmost chaotic?\nSir Galahad Pureheart, age 45, is a devoted paladin who lives by a strict code\nof honor, righteousness and duty. Unwavering in his beliefs, he shows no\nmercy to those he views as evil or chaotic, though his actions are driven by a\ndesire to protect the innocent and punish wrongdoers. His rigid worldview\noften puts him at odds with more free-spirited allies.\nCaptain Jade Stormcloud, age 32, is a brash but big-hearted pirate who lives\nlife to the fullest. Though she chafes at rules and restrictions, her strong moral\ncompass keeps her from taking her freedom too far. She would find common\nground with Sir Galahad in fighting evil, but her flexible worldview would\nhelp temper his rigidity.\nO\nLord Vladimir Skullreaper, age 67, is a cruel tyrant who rules his lands with\nan iron fist. Public executions are commonplace under his absolute authority,\nas he shows no mercy to those who dare question his laws.\nBrother Lucian Greymane, age 37, is a battle-hardened templar who tirelessly\nwages war against the forces of darkness. Though devoted to his holy crusade,\nhints of disillusionment sometimes pierce his staunch faith and code of honor.\nHis zeal for righteousness is tempered with shades of world-weariness and\nmoral ambiguity. While righteous at heart, he is no stranger to employing\nharsh methods when he deems the ends justify them. He would find kinship\nwith Galahad but also empathize with Stormcloud‚Äôs flexibility in fighting evil.\nPatchview: LLM-Powered Worldbuilding with Generative Dust and Magnet Visualization UIST ‚Äô24, October 13‚Äì16, 2024, Pittsburgh, PA, USA\nFigure 20: Elements generated by P1, (a) without any steering, (b) with natural language prompting, and (c) with visual steering.\nFigure 21: A view created by P2.",
  "topic": "Visualization",
  "concepts": [
    {
      "name": "Visualization",
      "score": 0.6342973709106445
    },
    {
      "name": "Magnet",
      "score": 0.621739387512207
    },
    {
      "name": "Computer science",
      "score": 0.4918101131916046
    },
    {
      "name": "Generative grammar",
      "score": 0.43638160824775696
    },
    {
      "name": "Mechanical engineering",
      "score": 0.22462880611419678
    },
    {
      "name": "Engineering",
      "score": 0.2045154571533203
    },
    {
      "name": "Artificial intelligence",
      "score": 0.18069988489151
    }
  ]
}