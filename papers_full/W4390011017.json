{
  "title": "MaScQA: investigating materials science knowledge of large language models",
  "url": "https://openalex.org/W4390011017",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2096813752",
      "name": "Mohd Zaki",
      "affiliations": [
        "Indian Institute of Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A2202485175",
      "name": "Jayadeva",
      "affiliations": [
        "Indian Institute of Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A4222705522",
      "name": "Mausam",
      "affiliations": [
        "Indian Institute of Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A2583359429",
      "name": "N. M. Anoop Krishnan",
      "affiliations": [
        "Indian Institute of Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A2096813752",
      "name": "Mohd Zaki",
      "affiliations": [
        "Indian Institute of Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A2202485175",
      "name": "Jayadeva",
      "affiliations": [
        "Indian Institute of Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A4222705522",
      "name": "Mausam",
      "affiliations": [
        "Indian Institute of Technology Delhi"
      ]
    },
    {
      "id": "https://openalex.org/A2583359429",
      "name": "N. M. Anoop Krishnan",
      "affiliations": [
        "Indian Institute of Technology Delhi"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3213394935",
    "https://openalex.org/W4221153884",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4362707064",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W2964864162",
    "https://openalex.org/W4281559792",
    "https://openalex.org/W3175823016",
    "https://openalex.org/W3201869313",
    "https://openalex.org/W4229443452",
    "https://openalex.org/W2966049804",
    "https://openalex.org/W4224442790",
    "https://openalex.org/W4362640271",
    "https://openalex.org/W4327913228",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W4385671288",
    "https://openalex.org/W4386269388",
    "https://openalex.org/W4387635881",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W4300113532",
    "https://openalex.org/W4319996831",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4308760226",
    "https://openalex.org/W4379468930",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4362508231",
    "https://openalex.org/W4353112996",
    "https://openalex.org/W4381586770",
    "https://openalex.org/W4386609350",
    "https://openalex.org/W4212774754",
    "https://openalex.org/W4389519108",
    "https://openalex.org/W2781528640",
    "https://openalex.org/W2620949368",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W4311642023"
  ],
  "abstract": "Different materials science domains from which questions are present in Materials Science Question Answering (MaScQA) database.",
  "full_text": "MaScQA: investigating materials science\nknowledge of large language models†\nMohd Zaki, a Jayadeva, bc Mausam cd and N. M. Anoop Krishnan *ac\nInformation extraction and textual comprehension from materials literature are vital for developing an\nexhaustive knowledge base that enables accelerated materials discovery. Language models have\ndemonstrated their capability to answer domain-speci ﬁc questions and retrieve information from\nknowledge bases. However, there are no benchmark datasets in the materials science domain that can\nbe used to evaluate the understanding of the key concepts by these language models. In this work, we\ncurate a dataset of 650 challenging questions from the materials domain that require the knowledge and\nskills of a materials science student who has cleared their undergraduate degree. We classify these\nquestions based on their structure and the materials science domain-based subcategories. Further, we\nevaluate the performance of LLaMA-2-70B, GPT-3.5, and GPT-4 models on solving these questionsvia\nzero-shot and chain of thought prompting. It is observed that GPT-4 gives the best performance (∼62%\naccuracy) as compared to other models. Interestingly, in contrast to the general observation, no\nsigniﬁcant improvement in accuracy is observed with the chain of thought prompting. To evaluate the\nlimitations, we performed an error analysis, which revealed conceptual errors (∼72%) as the major\ncontributor compared to computational errors (∼28%) towards the reduced performance of the LLMs.\nWe also compared GPT-4 with human performance and observed that GPT-4 is better than an average\nstudent and comes close to passing the exam. We also show applications of the best performing model\n(GPT-4) on composition–extraction from tables of materials science research papers and code writing\ntasks. While GPT-4 performs poorly on composition extraction, it outperforms all other models on the\ncode writing task. We hope that the dataset, analysis, and applications discussed in this work will\npromote further research in developing better materials science domain-speciﬁc LLMs and strategies for\ninformation extraction.\nIntroduction\nLarge language models (LLMs) are machine learning (ML)\nmodels based on transformer neural network architecture. 1\nThese models are calledlarge due to their billions of inherent\nparameters. The increase in the number of model parameters\nand diﬀerent training strategies have improved the perfor-\nmance of these models on natural language tasks such as\nquestion answering,\n2,3 text summarization,4,5 sentiment anal-\nysis,1,3 machine translation,6 conversational abilities,7–9 and\ncode generation.10 In the materials science domain, existing\ndatasets are mainly related to tasks like named entity recogni-\ntion (NER),11,12 text classication,13–15 synthesis process and\nrelation classi cation,16 and composition extraction from\ntables,17 which are used by researchers to benchmark the\nperformance of materials domain language models like MatS-\nciBERT14 (the rst materials-domain language model), Mat-\nBERT,18 MaterialsBERT,19 OpticalBERT,20 and BatteryBERT.15\nRecently, Song et al. (2023) reported better performance of\nmaterials science domain specic language models compared\nto BERT and SciBERT on seven materials domain datasets\nrelated to named entity recognition, relation classication, and\ntext classication.\n21\nThere exist several large-sized datasets like MMLU,22,23 Hel-\nlaSwag,24 WinoGrande,25 HumanEval,10 and DROP26 to evaluate\nthe capabilities of LLMs. However, there are limited datasets in\nthe materials science domain for assessing their question-\nanswering abilities. Table 1 lists datasets related to mathe-\nmatics, chemistry, and materials science suitable for evaluating\nLLMs. In addition to these datasets, Jablonka et al. (2023)\ndemonstrated the application of LLMs on 14 chemistry and\nmaterials science specic datasets.\n27 Further, based on datasets\naDepartment of Civil Engineering, Indian Institute of Technology Delhi, Hauz Khas,\nNew Delhi 110016, India. E-mail: krishnan@iitd.ac.in; cez198233@iitd.ac.in\nbDepartment of Electrical Engineering, Indian Institute of Technology Delhi, Hauz\nKhas, New Delhi 110016, India\ncYardi School of Articial Intelligence, Indian Institute of Technology Delhi, Hauz\nKhas, New Delhi 110016, India\ndDepartment of Computer Science & Engineering, Indian Institute of Technology Delhi,\nHauz Khas, New Delhi 110016, India\n† Electronic supplementary information (ESI) available. See DOI:\nhttps://doi.org/10.1039/d3dd00188a\nCite this:Digital Discovery,2 0 2 4 ,3,3 1 3\nReceived 25th September 2023\nAccepted 19th December 2023\nDOI: 10.1039/d3dd00188a\nrsc.li/digitaldiscovery\n© 2024 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 4 ,3,3 1 3–327 | 313\nDigital\nDiscovery\nPAPER\nOpen Access Article. Published on 20 December 2023. Downloaded on 11/5/2025 1:58:56 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nView Journal\n | View Issue\nlisted in Table 1, researchers have attempted tonetune mate-\nrials domain LLMs and proposed DARWIN28 and HoneyBee29\nand compared them with the performance of LLaMA, GPT3.5\nand GPT-4 on diﬀerent tasks.\nAlthough these datasets are diverse, the complexity of\nquestions asked in examinations for testing students who have\ncompleted their undergraduate-level education, is quite\ndiﬀerent from the existing ones. Therefore, developing such\na dataset is crucial to investigate the materials science domain\nknowledge of these LLMs so that they can be further used for\naddressing challenging problems related to materials discovery\nfor areas such as manufacturing, energy, environment, and\nsustainability. This information is further essential to under-\nstand the lacunae of the understanding of such LLMs, which are\nbeing proposed to be used for several domains such as\nmanufacturing, planning, material synthesis, and materials\ndiscovery.\n14,19\nTo this end, we collected questions that require students to\nhave a undergraduate-level understanding of materials science\ntopics to solve them. These questions and answers are carefully\ncurated from the original questions in the graduate aptitude\ntest in engineering (GATE) exam— a national-level examination\nfor graduate admission in India. More than 800 000 students\ntake this exam annually, with an average of 100 000 students in\nmajor disciplines, such as mechanical or civil engineering, to\nenroll in master's/doctoral courses in the premier institutes in\nIndia. We classify these questions based on their (a) structure,\nwhich leads to 4 types of questions, and (b) domain knowledge\nrequired to solve them, which divides the database into 14\ncategories. The questions in MaScQA also have diversity in\nlength, ranging from 9 words per question to 145 words ques-\ntion, with an average of 50 words (see Fig. S1†). We then eval-\nuate the performance of state-of-the-art proprietary models,\nGPT-3.5 and GPT4, and an open-source model, LLaMA-2-70B,\nin solving these questions. The availability of MaScQA will\nallow the researchers to benchmark existing models and\nprompting strategies. Specically, the analysis from a domain-\nspecic perspective will enable the researchers to train better\ndomain-specic LLMs and help them decide where these\nmodels can be used in the materials discovery pipeline. Note\nthat, MaScQA is an open database where other researchers can\nalso contribute questions for increasing the diversity of topics\non which LLMs can be evaluated.\nFinally, we evaluate LLMs on domain-speci c tasks and\ncompare their performance with the existing models suitable\nfor such tasks. Therst task, introduced by Guptaet al.(2023),\n17\nis composition extraction from tables in materials-related\nresearch papers. The second task, related to code writing,\nemployed the dataset released by Whiteet al.(2022),\n36 which is\na compilation of ∼100 Python functions comprising of the\ndocstring, return statement, and the [insert] token, which has to\nbe replaced upon prompting the LLM. The performance of GPT-\n4 on these tasks further allows researchers to devise strategies\nfor task-oriented netuning of the LLMs. Overall, we try to\nanswer the following questions in this paper:\n1. How well do general-purpose LLMs perform in answering\ncomplex questions from the materials science domain?\n2. Can we improve the performance of the LLMs by using the\nchain of thought prompting methods?\n3. What are the factors limiting the performance of these\nLLMs?\n4. Can LLMs be used for accelerated materials modelling and\ndesign through information extraction and code writing?\nMethodology\nDataset preparation\nWe are motivated to investigate how LLMs will perform on\nquestions that require an undergraduate-level understanding of\nmaterials science topics for their solution. To compile a dataset\nof such questions, we take question papers related to materials\nscience and metallurgical engineering asked in the GATE\nexamination conducted in India for admission to masters and\ndoctorate courses. To this end, we compiled 650 questions and\nclassied them into four types based on their structure:\nmultiple choice questions (MCQs), match the following type\nquestions (MATCH), numerical questions where options are\ngiven (MCQN), and numerical questions (NUM). MCQs are\ngenerally conceptual, given four options, out of which mostly\none is correct and sometimes more than one option is also\ncorrect (Fig. 1(a)). In MATCH, two lists of entities are given,\nwhich are to be matched with each other. These questions are\nalso provided with four options, out of which one has the\ncorrect set of matched entities (Fig. 1(b)). In MCQN, the ques-\ntion has four choices, out of which the correct one is identied\naer solving the numerical stated in the question (Fig. 1(c)). The\nNUM type questions have numerical answers, rounded to the\nnearest integer or oating-point number as specied in the\nquestions (Fig. 1(d)).\nTable 1 List of question answering datasets in mathematics, basic sciences, and materials science domain\nDataset Description\nGSM8K30 A set of 8.5 K linguistically diverse grade school mathematics word problems\nAI2RC (ARC)31 A set of∼7.7 K school-level science questions\nChemistryQA32 A dataset of 4.5 K chemistry question answers\nScienceQA33 ∼21 K multimodal multiple choice questions from natural, language, and social sciences\nSciQ34 Crowdsourced dataset of∼13.7 K science questions\nMatSci-Instruct29 52 K synthetically generated instructions dataset tonetune LLMs for materials science information extraction\nJEEBench35 450 questions on physics, chemistry, and mathematics from JEE advanced examination of India for admission to IITs\n314 | Digital Discovery,2 0 2 4 ,3,3 1 3–327 © 2024 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 20 December 2023. Downloaded on 11/5/2025 1:58:56 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nTo understand the performance of LLMs from the materials\nscience domain perspective, we classied the questions into 14\ncategories. The list of categories was prepared in consultation\nwith domain experts who teach materials science subjects at the\ninstitute where this research is conducted. Then, two experts\nassign all the questions to one of the categories. The conict in\nthe category assignments was resolved through discussion and\nmutual agreement. Fig. 2 shows the number of questions in\neach category. The color of the bars represents the broad\ncategory of materials science topics under which each subtopic\nis shown in the graphical abstract. The database can be\naccessed athttps://github.com/M3RG-IITD/MaScQA.\nSolutions using LLMs\nIn this work, we benchmark the question-answering ability of\nLLaMA 2-70B\n37 chat model (will be referred to as LLaMA-70B\nfrom now onwards in the paper), GPT-3.5, and GPT-4 models\nFig. 1 Sample questions from each category: (a) multiple choice question (MCQ), (b) matching type question (MATCH), (c) numerical question\nwith multiple choices (MCQN), and (d) numerical question (NUM). The correct answers are in bold and underlined.\nFig. 2 The number of questions in each materials science sub-domain. The bar chart shows the number of questions in diﬀerent sub-domains.\nThe pie chart shows the number of questions classiﬁed according to question structure.\n© 2024 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 4 ,3,3 1 3–327 | 315\nPaper Digital Discovery\nOpen Access Article. Published on 20 December 2023. Downloaded on 11/5/2025 1:58:56 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\non the MaScQA dataset. Note that there exist other open-source\nLLMs like BLOOM 38 and Falcon,39 but we considered only\nLLaMA-2 because of its higher number of weights and avail-\nability of hardware requirements with us. Further, LLaMA-2\ncomes in three open-source variants based on the number of\nparameters, i.e. 2B, 13B, and 70B models having 2 billion, 13\nbillion, and 70 billion parameters respectively. We use only 70B\nvariant due to its better performance on tasks demonstrated in\nthe paper introducing these models. We used the API of the\nproprietary models to obtain answers to the questions in two\nways: rst, by directly prompting the models to answer the\nquestions (zero-shot prompting), and second, by asking the\nmodels to solve the questions step by step, also known as the\nChain of Thoughtprompting (CoT).\n40 The questions are provided\nto the GPT models using the OpenAI API and selecting the\nappropriate model type. The prompt used in therst approach\nis “Solve the following question. Write the correct answer inside\na list at the end.”, and for the second approach, the prompt is\n“Solve the following question with highly detailed step-by-step\nexplanation. Write the correct answer inside a list at the end.”\nThe last sentence in the prompt was used to automatically\nretrieve the correct option/answer from the model output and\nmatch it with the answer key. However, the model did not\nalways give output in the desired format. Hence, the entire\nmodel output is saved as a textle, which was then used for\nmanually extracting the answers for comparison with the actual\nanswers provided in the oﬃcial answer keys of the respective\npapers. Note that evaluation using LLaMA-70B requires 8 A100\n80 GB GPUs.\n37 Since CoT prompting is known to obtain the best\nresults, we only evaluate LLaMA-70B-CoT to use the computa-\ntional resources eﬃciently. Also, a temperature of 1.0 was used\nwhile prompting the LLMs in this work. Researchers have\ndeveloped di ﬀerent prompting strategies to improve the\nperformance of LLMs on QA tasks, like the chain of thought\nFig. 3 Word-cloud for diﬀerent materials science subdomains in MaScQA (a) thermodynamics, (b) atomic structure, (c) mechanical behavior, (d)\nmaterial manufacturing, (e) material applications, (f) phase transition, (g) electrical properties, (h) material processing, (i) transport phenomenon,\n(j) magnetic properties, (k) material characterization, (l)ﬂuid mechanics, (m) material testing, and (n) miscellaneous.\n316 | Digital Discovery,2 0 2 4 ,3,3 1 3–327 © 2024 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 20 December 2023. Downloaded on 11/5/2025 1:58:56 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nprompting,40 self-consistency,41 and self-critique.42,43 However,\nin this work, we choose the basic prompting methodologies like\nzero-shot and CoT to investigate existing knowledge in LLMs\nand present baselines on the newly introduced dataset of\nMaScQA.\nThe solutions to all the questions obtained using two\napproaches for both models can be accessed at https://\ngithub.com/M3RG-IITD/MaScQA. The oﬃcial answer keys are\nobtained from the oﬃcial website of IIT Kharagpur, which is\none of the organizing institutes of the GATE exam. https://\ngate.iitkgp.ac.in/old_question_papers.html. The LLMs'\nperformance on two prompting methods is discussed in detail\nin the following sections.\nResults\nDataset visualization\nFig. 2 shows the details of the dataset comprising a total of 650\nquestions in di ﬀerent categories. First, we categorize the\nquestions based on their structure. We observe that the largest\ncategory of questions (284) are MCQs, while 70 are MATCH-type\nquestions. Further, 68 questions are MCQN, while the remain-\ning 228 questions are NUM that do not provide any choices. We\nthen analyze these questions from materials science domain\nperspective. To this extent, the questions are categorized into 14\ndomains: thermodynamics, atomic structure, mechanical\nbehaviour, materials manufacturing, material applications,\nphase transition, electrical properties, material processing,\ntransport phenomenon, magnetic properties, material charac-\nterization, uid mechanics, material testing, and\nmiscellaneous.\nFig. 2 shows the number of questions in diﬀerent domain-\nspecic categories. To visualize the frequently used words\nrelated to each domain-specic category of questions, word\nclouds are shown in Fig. 3 and top 10 most occurring words are\nshown in Fig. 4. The maximum number of questions (114) are in\nthe thermodynamics category, which deals with questions\nrelated to enthalpy of formation, energy balance during\nFig. 4 Frequency of top– 10 words in each materials science subdomain present in MaScQA (a) thermodynamics, (b) atomic structure, (c)\nmechanical behavior, (d) material manufacturing, (e) material applications, (f) phase transition, (g) electrical properties, (h) material processing, (i)\ntransport phenomenon, (j) magnetic properties, (k) material characterization, (l)ﬂuid mechanics, (m) material testing, and (n) miscellaneous.\n© 2024 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 4 ,3,3 1 3–327 | 317\nPaper Digital Discovery\nOpen Access Article. Published on 20 December 2023. Downloaded on 11/5/2025 1:58:56 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nchemical reactions, transition temperatures, activation energy,\nand heat transfer (Fig. 3(a)) which is also reected by most\noccurring words like energy,k (Kelvin), j (Joule), g (indicating\ngaseous state of chemicals in reactions and units of material\nproperties), as shown in Fig. 4(a). The category ofatomic struc-\nture comprises 100 questions, which are based on concepts\nsuch as dislocations, diﬀraction planes, and crystal structures\n(Fig. 3(b) and 4(b)). Themechanical behaviorcategory is based on\nthe concepts of stress –strain behavior of materials, creep,\nfatigue, and fracture mechanics (Fig. 3(c)). Further, the pres-\nence of words like“mpa and gpa”(Fig. 4(c)), which are units of\nstress and strength (MPa and GPa), indicate the correct classi-\ncation of questions. Inmaterials manufacturing (Fig. 3(d) and\n4(d)) andmaterial applications(Fig. 3(e) and 4(e)), the questions\ntest the knowledge of extraction processes of materials from\ntheir respective ores and why a particular material,e.g. oxides,\nalloys are used for a specic application. Thus, these questions\nrequire logical understanding connecting multiple concepts:\nrst, “recall”or “deduce”the properties of a material based on\nits composition, label, or processing conditions, and second,\n“identify” the properties required for a particular application\nand then connect these two concepts to “derive” a logical\nexplanation to arrive at the correct answer. The questions on\nphase transitiontest the knowledge of how phase transition can\nbe induced in materials, how to calculate the percentage of\ndiﬀerent phases in the materials, and the characteristics of\ndiﬀerent phases. This is also indicated by the high frequency of\nwords related to diﬀerent phases of materials (Fig. 3(f) and 4(f)).\nThe questions on electrical properties include fuel cells, char-\nacteristics of materials used in batteries, and semiconductor\ndevices (Fig. 3(g)). This is also seen in the frequency of top-10\nwords in this domain (Fig. 4(g)), which comprises of electron,\nv (Volt), and electrode. Then, questions are based onmaterial\nprocessing such as welding, annealing, tempering, recrystalli-\nzation, welding, etc. (Fig. 3(h) and 4(h)). The questions on\ntransport phenomena test concepts related to the diﬀusion or\ntransport of ions, corrosion, and duration of the phenomena\n(Fig. 3(i) and 4(i)). The question related tomagnetic properties\ntests the knowledge about magnetization and the characteris-\ntics of diﬀerent magnetic materials (Fig. 3(j) and 4(j)). The\nmaterial characterization topic has questions related to\nmethods like scanning electron microscopy, diﬀraction studies,\nand back-scattered electron microscopy (Fig. 3(k) and 4(k)). The\nuid mechanics topic comprises questions on the viscosity of\nthe\nuid and the movement of particles in a viscous medium\n(Fig. 3(l) and 4(l)). In thematerial testingtopic, the questions are\nbased primarily on non-destructive material testing methods\n(Fig. 3(m) and 4(m)). The miscellaneous category deals with\nquestions requiring a simultaneous understanding of multiple\nmaterials science domains like optical properties, piezoelec-\ntricity, and microscopy for their solution (Fig. 3(n) and 4(n)).\nPerformance evaluation\nNow, we evaluate the performance of LLMs on MaScQA and the\neﬀect of prompting methods on the performance, correspond-\ning to therst two questions posed in this work. Table 2 reports\nthe accuracy of the LLMs on the MaScQA corpus. The scores\ncorresponding to model names GPT-3.5 and GPT-4 represent\nthe accuracy of the models when questions are asked directly to\nthe models representing zero-shot answering. The model\nnames with the suﬃx “CoT”imply we have asked the models to\nprovide detailed“stepwise”solutions to the given questions. In\nMCQs, we observe that GPT-4 signicantly outperforms GPT-3.5\nand LLaMA. We observed that LLaMA yields very low perfor-\nmance, which might be due to limited training corpora and\nfewer parameters than GPT models. Further, we also observe\nthat the CoT provides only marginal improvement in the result\nfor GPT-3.5 and GPT-4.\nHere, GPT-4-CoT gives an accuracy of 77.11% on MCQ, which\nis a high score considering the diﬃculty levels of this exam.\nAlso, the performance of GPT-4-CoT is∼20% higher than GPT-\n3.5-CoT for MCQ type of questions. For MATCH questions, GPT-\n4-CoT exhibits the maximum performance with a score of\n92.86%, a very high score considering the amount of knowledge\nrequired to connect the entities. In contrast, the variants of\nGPT-3.5 performed poorly on MATCH questions, with a score of\n40% and 38.57% for the variants without and with CoT,\nrespectively. In this case, the GPT-4-CoT provides ∼4%\nimprovement over direct prompting. For MCQN, GPT-4 gives\nthe best performance with a score of 58.82%, while CoT reduces\nthe model's performance to 50.0%. The same trend of reduced\nperformance on these questions is observed with the GPT-3.5\nmodel. This implies that CoT prompting may not always lead\nto better performance. Now, we focus on the numerical ques-\ntions. Among all the categories, models exhibit the worst\nperformance in the NUM category. Here, GPT-4 and GPT-4-CoT\nobtain the maximum score of 37.28% and 39.04%. Interest-\ningly, we observe that CoT yields poorer results in the case of\nGPT-3.5, while it yields better accuracy in the case of GPT-4.\nTable 2 Performance (% accuracy) of diﬀerent evaluation styles using LLaMA and GPT models on various question types. The number in\nparenthesis represents the total number of questions under respective categories\nEvaluation method MCQ (284)\nMatching (MATCH)\n(70)\nNumerical with\nMCQ (MCQN) (68)\nNumerical (NUM)\n(228) Overall accuracy\nBaseline scores 25 25 25 0\nLLaMA-70B-CoT 41.20 22.86 20.59 3.95 24.0\nGPT-3.5 56.69 40.00 35.29 15.79 38.31\nGPT-3.5-CoT 57.04 38.57 33.82 14.91 37.85\nGPT-4 74.65 88.57 58.82 37.28 61.38\nGPT-4-CoT 77.11 92.86 50.00 39.04 62.62\n318\n| Digital Discovery,2 0 2 4 ,3,3 1 3–327 © 2024 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 20 December 2023. Downloaded on 11/5/2025 1:58:56 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nFinally, regarding overall performance, GPT-4-COT gives the\nbest score of 62.62%, with GPT-4 following closely at 62%. It\nshould be noted that in MCQ, there are 13 questions where\nmore than one option is correct, of which GPT-4 and GPT-4-CoT\nanswered six and seven questions correctly, respectively. Inter-\nestingly, we observe that CoT does not always give improved\nresults. In fact, for GPT-3.5, CoT gives poorer results in all the\ncases except MCQs and marginally better results for GPT-4 in all\nthe cases except MCQN. Note that this observation contrasts\nwith the general observation that the CoT prompting results in\nimproved performance of LLMs on QA tasks. To identify\nwhether the overall performance of LLMs-based evaluation\nstrategies on MaScQA is statistically signicant, we perform\npaired t-test by taking the performance of two evaluation\nstrategies at a time and report the resultingp-values in Table 3.\nThe null hypothesis tested is“there is no signicant diﬀerence\nbetween the performance of two LLMs-based evaluation strat-\negies in solving the questions of MaScQA”. Since thep-values\nwhen comparing the performance of GPT-3.5 with GPT-3.5-CoT\nand GPT-4 with GPT-4-CoT are quite higher than 0.05, it is\naccepted that in these two cases, there is no signicant diﬀer-\nence between the performance of the two evaluation strategies.\nHowever, in all other cases, thep-values are lower than 0.05,\nimplying signicant diﬀerence in the performance of the LLMs-\nbased evaluation strategies.\nIn addition to evaluating the performance of LLMs in\nanswering diﬀerent types of questions like MCQ, MATCH,\nMCQN, and NUM, which test diﬀerent abilities of the students,\nit is also essential to analyze the performance of the models\nfrom a domain perspective. To this end, we classify all the\nquestions of our dataset into 14 broad categories. Fig. 5 shows\nthe accuracy of the GPT-4-CoT prompting method while\nanswering the questions. Since the number of questions diﬀers\nunder each category, we report the percentage of questions\nanswered correctly and incorrectly to show proper comparison.\nThe number of question for each case are written with white\ncolor inside the respective bars. It is observed that questions\nrelated to materials' mechanical behavior and electrical prop-\nerties have the most percentage of incorrectly answered ques-\ntions ( ∼60%). The questions on thermodynamics, atomic\nTable 3 p-Values obtained from statistical testing of LLMs performance using pairedt-test\nLLMs GPT-3.5 GPT-3.5-CoT GPT-4 GPT-4-CoT\nGPT-3.5-CoT 0.864\nGPT-4 3.56 × 10−17 7.96 × 10−18\nGPT-4-CoT 6.12 × 10−19 1.26 × 10−19 0.648\nLLAMA-70B-CoT 2.17 × 10−8 5.75 × 10−8 1.76 × 10−42 2.17 × 10−48\nFig. 5 Performance of GPT-4-CoT on questions classiﬁed from materials science domain perspective.\n© 2024 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 4 ,3,3 1 3–327 | 319\nPaper Digital Discovery\nOpen Access Article. Published on 20 December 2023. Downloaded on 11/5/2025 1:58:56 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nstructure, magnetic properties, transport phenomena, and\nphase transition have∼40% of incorrectly answered questions\nin the respective categories. Further, ∼30% of materials\nmanufacturing and characterization questions are incorrectly\nanswered. In the categories ofuid mechanics and materials\napplications, ∼15% of questions are incorrectly answered with\nthe lowest error rates for material processing and no mistakes\nmade on material testing questions. To further gain insights\ninto the factors limiting LLMs' performance, we will discuss\nthem by classifying the errors into two categories, as explained\nin the Discussion section.\nDiscussion\nError analysis\nTo use LLMs eﬀectively for materials discovery and identify\nareas that require further research, it is important to under-\nstand the mistakes made by the LLMs in the materials domain.\nAnswering a question requires retrieval of correct concepts/\nfacts, applying them to the scenarios posed in the question by\nappropriate substitution in the relevant formulae, and then\nsolving it correctly by applying relevant computational steps. To\nunderstand further, we can divide these errors into three cate-\ngories: namely, (i) conceptual error, where the correct concept,\nequation, or facts related to the problem are not retrieved, or the\nLLM hallucinates some facts; (ii) grounding error: where the\nrelevant concepts are not correctly applied to the scenario or\nincorrect values are substituted in the equations (for example, °\nC to K conversion not applied) and (iii) computational error:\nwhere the numerical computation is performed incorrectly.\n35\nNote that CoT prompting enables the model to reect upon the\nknowledge it already has, connect it with multiple choices, and\nthen arrive at the answer. Thus, in general, it has been observed\nthat CoT helps in reducing grounding errors (in our case, it\nvirtually eliminates them).\nTo analyze diﬀerent errors, we perform error analysis on\nGPT-4-CoT response because this strategy performed best on\nMaScQA. We take all the incorrectly answered questions by GPT-\n4-CoT, in which 139 are NUM, 65 are MCQs, 34 are MCQN, and 5\nare matching-type questions (MATCH) (Fig. 6). The number of\nincorrectly answered questions across materials science sub-\nFig. 6 Types of errors made by GPT-4-CoT on the questions classiﬁed\nbased on the structure.\nFig. 7 Types of the error made by GPT-4-CoT on questions classiﬁed according to materials science perspective.\n320 | Digital Discovery,2 0 2 4 ,3,3 1 3–327 © 2024 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 20 December 2023. Downloaded on 11/5/2025 1:58:56 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\ndomains are shown in Fig. 7. Note that there may be questions\nwith conceptual and numerical errors, but we have considered\nonly the conceptual error in these questions since it is therst\nto be found. If the retrieved concept is incorrect, we deem the\ncomputational error secondary.\nFig. 6 shows the distribution of errors made by GPT-4-CoT in\ndiﬀerent categories of question based on their structure. The\ntext inside the bars representing conceptual and computational\nerror shows the number of questions in respective category. The\nanalysis of all the incorrectly answered questions reveals that\nmajority of errors are conceptual. Further, in MCQs and\nMATCH type questions, the error is always conceptual because\nanswering such questions requires retrieval of appropriate\nconcepts and facts and then connecting them with relevant\noptions. For MCQN and NUM, majority of the questions were\nanswered incorrectly (∼65% and ∼59%) due to conceptual\nerrors implying the need for domain-specic models or better\nprompting and problem-solving approaches.\nAs mentioned earlier, we observed that GPT-4-CoT made no\ngrounding errors. To evaluate whether this is due to the eﬀec-\ntiveness of CoT, we investigate questions that are incorrectly\nanswered by GPT-4 and correctly by GPT-4-CoT. Out of 66 such\nquestions from the entire dataset, GPT-4's solutions had∼70%\nconceptual errors, ∼30% computational errors, and no\ngrounding errors. Further, we also analyzed the erroneously\nanswerd questions by GPT-4-CoT and are correctly answered by\nGPT-4. There were 58 such questions in the complete dataset.\nOut of these questions, solutions of 45 questions (∼78%) had\nconceptual errors; for one question, there was a grounding\nerror, and the remaining 12 questions had computational\nerrors when solved using GPT-4-CoT. Since there are little to no\ngrounding errors in either GPT-4 or GPT4-CoT, both models are\nadept in this regard. The CoT prompting is helping reduce some\nnumerical errors.\nFig. 7 shows the domain-wise distribution of conceptual and\ncomputational errors on the all the incorrectly answered\nFig. 8 Visualizing some of the questions where GPT-4-CoT made conceptual errors in the solution. The correct answers to each question are\nmarked in bold and underlined.\n© 2024 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 4 ,3,3 1 3–327 | 321\nPaper Digital Discovery\nOpen Access Article. Published on 20 December 2023. Downloaded on 11/5/2025 1:58:56 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nquestions by GPT-4-CoT. The number written in white color\nover colored bars respresent the number of question in each\ncase. All categories have conceptual errors in more than 50% of\nthe respective questions except for transport phenomena\n(∼45%) and uid mechanics. Now, we will discuss some\nconceptual errors in diﬀerent domains. The list of all questions\nsubjected to analysis is provided in the GitHub repository of this\nwork.\nFig. 8(a) shows an example of the conceptual error made on\na question related to thermodynamics. In this question, instead\nof considering the coeﬃcient of thermal expansion same in the\nplanar dimension, it considered the coe ﬃcient of thermal\nexpansion in the perpendicular direction as the same in one of\nthe planar directions. Mathematically, instead of obtaining the\nnal coeﬃcient using 2× parallel + perpendicular coeﬃcients,\nGPT-4-CoT used parallel + 2 × perpendicular, leading to an\nincorrect answer. While solving a question on atomic structure,\nas shown in Fig. 8(b), GPT-4-CoT mistook the relation between\nlattice parameter (a) and atomic diameter (D)a s a ¼\nO3\n2 D\ninstead ofa ¼ 2\nO3 D. In a question on the electrical properties\nof materials (Fig. 8(c)), the GPT-4-CoT answered that all the\ngiven statements were correct. Hence, it could not choose from\nthe four options shown as answers. According to the materials\nscience domain and the Wikipedia entry of Pourbaix diagrams,\none of their major limitations is that these diagrams do not\nestimate actual corrosion rates; also, these diagrams cannot be\nused while studying corrosion due to chloride ions. Hence, the\nstatement R is incorrect, making (C) the correct choice. While\nsolving the question shown in Fig. 8(d), GPT-4-CoT did not\nconvert the lattice parameter into the atomic diameter and\nconsidered them the same while using it in the formula\nrequired for solving the problem. For a question on materials\nmanufacturing (Fig. 8(e)), GPT-4-CoT retrieved the functions of\n(P) blast furnace slag and (R) Torpedo car as opposite, thus\nleading to a wrong answer,C, when the correct option wasA.\nThe complete solution of GPT-4-CoT can be found in the\nGitHub repository of this work. Some examples of correct\nanswers given by GPT-4-CoT on four types of question according\nto structure (MCQ, MATCH, MCQN, and NUM) are shown in ESI\n(Fig. S2–S5).†\nTo summarise, the CoT prompting cannot signi cantly\nimprove the LLM performance as the mistakes are mainly\nconceptual. This makes a strong case for a domain-specic LLM\nfor materials and potentially domain-speci c alternate\nprompting strategies. Further, for questions where the LLMs\ngive the incorrect response due to computational error, the\nsolution involved unit conversions, logarithms, and exponen-\ntials and had numbers with multiplying factors (e.g.,1 0\n10).\nThere have been recent works in the literature that suggest\nmethods for improving calculations and for improving on\nconcept-based mistakes.\n44 Introducing such heuristics while\nprompting can help researchers in two ways: (1) probe the\nexisting LLMs more deeply and (2) generate datasets to train\nLLMs with lesser parameters, thus making the use of these\nmodels economical. Hence, this answers the third research\nquestion (limiting factors for LLMs) raised in this work.\nComparison with human performance\nBased on the reports published by organising institutes of\nGATE, marking criteria is as follows: for NUM questions, there\nis no negative marking. For all other types of questions, there is\na negative marking of 1/3 times the marks of the question. The\nquestions can carry 1 or 2 marks. Further, GATE for the mate-\nrials science domain has only 25 questions, which is too few to\ncompare with human performance. Therefore, we consider\nquestions asked in the years 2020, 2021, and 2022 in the\nmetallurgical engineering exam, where 65 questions are asked\nyearly. Out of 65 questions, 10 are of general aptitude and,\nhence, ignored in this work. By considering the remaining\nquestions (119) and associated marks (185), GPT-4-CoT ob-\ntained 79 marks, translating to an average of 42.7% marks over\nthree years. Table 4 shows the maximum marks obtained by\nhumans in the GATE metallurgical engineering exam, the cut-\noﬀ marks required to qualify, and the average of the marks\nobtained by students who appeared. It can be concluded that\nGPT-4 is better than an average student appearing in the exam\nand comes quite close to the cut-oﬀ required for qualication.\nAdditional tasks based on question-\nanswering\nIn this section, we evaluate the performance of LLMs on two\nadditional tasks that enable accelerated materials modelling\nand discovery, namely, composition extraction from tables in\nmaterials science articles and code-writing for materials\nmodelling. Note that both the problems are formulated as\nquestion-answering tasks and hence evaluate the ability of\nLLMs to answer materials science domain questions consistent\nwith the previous section.\nCompositions extraction from tables in materials science\nresearch papers\nThe understanding of materials compositions, their processing,\nand testing conditions, structure, and properties form the basis\nof automated material discovery pipelines.\n45 According to Gupta\net al. (2023),17 ∼85% of materials composition in existing\ndatabases are extracted from tables. Guptaet al. (2023) devel-\noped a graph neural network based pipeline, DiSCoMaT, which\ncan extract materials compositions from the tables published in\nmaterials science research papers.\n17 In this work, we sample 100\ncompositions from the dataset, which is used to evaluate the\nTable 4 Comparing the performance of GPT-4-CoT with human\nperformance\nYear Maximum marks Cut o ﬀ marks Average marks GPT-4-CoT\n2020 83 49.2 N.A. 46.46\n2021 87.67 48.5 28.7 42.86\n2022 77.67 46.2 27.6 38.62\n322\n| Digital Discovery,2 0 2 4 ,3,3 1 3–327 © 2024 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 20 December 2023. Downloaded on 11/5/2025 1:58:56 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nperformance of DiSCoMaT and compare it with the composi-\ntions extracted by GPT-4 from the same tables. The table types\nand the prompts are given in the GitHub repository. Diﬀerent\nprompts are used for diﬀerent tables (see ref. 17 for diﬀerent\ntable types) to improve the results of the LLM. For example,\nFig. 9 shows the system message provided in the API for\nextracting compositions and the table, and its caption con-\nverted into the prompt.\nIn the composition extraction task, the extracted composi-\ntions must be expressed as a set of tuples containing material ID\n(as dened in the paper), constituent elements or compounds,\ncorresponding percentage contributions, and corresponding\nunits. To evaluate the performance on this task, two categories\nof metrics are used:tuple level precision, recall and F1-scores,\nand material level precision, recall, and F1-scores. Tuple level\nmetrics imply that the individual components of a material\nare extracted along with its value and unit. In contrast, the\nmaterial level metrics also consider extracting material id in the\nextracted tuple. For more details about the extraction task and\nmetrics, the readers are requested to refer to the paper intro-\nducing this dataset and models.\n17 Table 5 shows the perfor-\nmance of GPT-4 along with the DiSCoMaT's performance on the\nsame dataset. Since GPT-4 is not particularly trained for this\ntask, it produces extra text like “The extracted compositions\nare.”which is incompatible with the evaluation pipeline used\nin DiSCoMaT. Therefore, we analyse only the relevant part of the\nextractions from the GPT-4 output. The lower performance of\nGPT-4 compared to DiSCoMaT can be attributed to the fact that\nGPT-4 was not pre-trained/ netuned for such tasks. The\nmistakes made by GPT-4 include non-extraction of material ids,\nnot being able to normalise the component values if the sum of\nall components is not 100, and not being able to extract\nnominal compositions when both nominal and experimental\ncompositions are reported in the table. These mistakes, thus,\nTable 5 Comparing the performance of GPT-4 and DiSCoMaT on the\ncomposition extraction task\nModel Tuple level metrics Material level metrics\nGPT-4 76.39 76.0 76.2 57.45 51.92 54.55\nDiSCoMaT 83.24 66.33 73.80 88.18 62.50 73.11\nFig. 9 (a) System message (b) table as prompt to extract materials compositions from tables using GPT-4 API.\n© 2024 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 4 ,3,3 1 3–327 | 323\nPaper Digital Discovery\nOpen Access Article. Published on 20 December 2023. Downloaded on 11/5/2025 1:58:56 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nconstitute both computational and factual errors, as investi-\ngated in the Discussion section of this paper. The dataset of 100\ncompositions, prompts used for this study and response of GPT-\n4 are provided in the GitHub repository of this work.\nCode writing\nOne of the important usages of LLMs for materials discovery\ncould be in developing codes for materials simulation.\n45 To\nevaluate this capability, we obtain the performance of GPT-4 on\nthe code completion dataset provided by Whiteet al. (2023).\n36\nAlthough this dataset was prepared to evaluate code-generating\nLLM's understanding of chemistry, the questions belonging to\ncategories like thermodynamics, molecular simulations, spec-\ntroscopy, and atomic structure are common with that of mate-\nrials science. In this work, the questions are provided to GPT-4\nwith the system message “Complete the following code by\nfollowing the docstring and replacing [insert].” followed by the\nprompt which contains the skeleton of the python function with\nthe docstring, [insert] marker and return statement. An example\nof the prompt is shown in Fig. 10(a), along with the solution\nprovided by GPT-4 (Fig. 10(b)).\nThe performance of GPT-4, compared to the output of other\nmodels, is reported in Table 6. It was observed that most of the\nmistakes made by GPT-4 are in the codes related to molecular\ndynamics, spectroscopy, chemical informatics, and quantum\nmechanics, which is consistent with the performance of GPT-4-\nCoT on MaScQA. Another interesting observation is that GPT-4\nanswered all code-related thermodynamics questions (a total of\nten questions), which is consistent with the observation that\nGPT-4 has a reasonable understanding of thermodynamics\nconcepts, and the poor performance on MaScQA was mainly\ndue to computational error. We have provided the output of\nGPT-4 on all the questions in the GitHub repository of this work.\nAltogether, we observe that GPT-4 achieves state-of-the-art\nperformance for the code writing task.\nConclusion\nThis work evaluated how well LLMs understand the materials\nscience domain to determine their applications in materials\ndiscovery, synthesis, and planning pipelines. To this end, our\nnew dataset, MaScQA, annotated questions and answers on the\nmaterials science domain, will provide a means to gain deeper\ninsights. Evaluation of LLMs on MaScQA revealed that the LLMs\nmake both numerical and conceptual mistakes. There are\nseveral core materials science domains where LLMs show poor\nperformance, such as the atomic and crystal structure of\nmaterials and their electrical, magnetic, and thermodynamic\nbehavior. Further, we evaluated the ability of LLMs for advanced\ntasks such as composition extraction from tables and code\nwriting. These tasks require LLMs to have domain insights and\nthe ability to produce output in the desired format, thus testing\ntheir conceptual, grounding, and computational capabilities.\nWhile GPT-4 performs poorly on the composition extraction\ntask, it outperformed all other models on the code writing tasks.\nInterestingly, the results suggest that domain-adaption and\ntask-specic prompting strategies are necessary to elicit the\ndesired output from the LLMs. Therefore, the language models\nmust be netuned on a domain-speci c and task-speci c\ndatasets to enable the use of LLMs in the materials discovery\npipeline. Moreover, the performance of the LLMs on MaScQA\ncan enable a deeper understanding of the lacunae of materials\nscience knowledge in the LLMs, thereby providing new research\navenues. For instance, LLMs' poor performance in NUM ques-\ntions suggests that a pipeline connecting the LLM to a math\ncalculator can potentially yield improved results. Further, the\nconceptual mistakes made by the LLMs indicate that the\ndevelopment of an LLM trained on materials literature could\nprovide improved results. The materials science domain is\na eld that derives concepts from physics, chemistry, and\nmechanics. Therefore, a benchmark like MaScQA will allow the\nresearchers to benchmark their domain specic models and\nprompting strategies against a standard dataset. Further, the\ncorrect solutions can help researchers create a new dataset for\ntraining lightweight or small language models, which are\neconomical and, hence, can be easily deployed on low-memory\nindustrial devices for materials discovery and their usage for\neducational purposes.\nFig. 10 (a) An example prompt provided to the GPT-4 model for generating the complete output (b) response of GPT-4.\nTable 6 Comparing the performance of diﬀerent LLMs on code\ngeneration task\nModel Performance\nOpenai/text-davinci-003 (ref. 36) 60.49%\nOpenai/code-davinci-002 (ref. 36) 53.09%\nOpenai/code-cushman-001 (ref.\n36)\n56.79%\nOpenai/gpt-4-0613 (GPT-4) 71.60%\n324 | Digital Discovery,2 0 2 4 ,3,3 1 3–327 © 2024 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 20 December 2023. Downloaded on 11/5/2025 1:58:56 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nData availability\nThe code, data, and links used in our work titled as MaScQA:\na question answering dataset for investigating materials science\nknowledge of large language models authored by Mohd Zaki,\nJayadeva, Mausam, and N. M. Anoop Krishnan can be found at\nhttps://github.com/M3RG-IITD/MaScQA.\nConﬂicts of interest\nThe authors declare no competingnancial interest.\nAcknowledgements\nN. M. A. K. acknowledges the funding support received from\nBRNS YSRA (53/20/01/2021-BRNS), ISRO RESPOND as part of\nthe STC at IIT Delhi, and the Google Research Scholar Award. M.\nZ. acknowledges the funding received from the PMRF award by\nthe Ministry of Education, Government of India. M. acknowl-\nedges grants by Google, IBM, Microso, Wipro, and a Jai Gupta\nChair Fellowship. The authors acknowledge the assistance of\nMr Aditya Pratap Singh (B. Tech. student in the Department of\nMaterials Science and Engineering, IIT Delhi) in compiling\nGATE questions from previous year papers, and Ms Devanshi\nKhatsuriya (B. Tech. student in the Department of Computer\nScience and Engineering, IIT Delhi) in evaluating the perfor-\nmance of GPT-4 on composition extraction task. The authors\nthank Microso  Accelerate Foundation Models Research\n(AFMR) for access to OpenAI models. The authors thank the\nHigh Performance Computing (HPC) facility at IIT Delhi for\ncomputational and storage resources.\nReferences\n1 J. Devlin, M. W. Chang, K. Lee and K. Toutanova, BERT: Pre-\ntraining of deep bidirectional transformers for language\nunderstanding, in Proceedings of NAACL , Association for\nComputational Linguistics, Minneapolis, Minnesota, 2019,\np. 4171 –4186, available from: https://www.aclweb.org/\nanthology/N19-1423.\n2 A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,\nA. Roberts, et al., PaLM: Scaling Language Modeling with\nPathways, arXiv, 2022, preprint, arXiv:2204.02311 [cs.CL],\nDOI: 10.48550/arXiv.2204.02311.\n3C . R aﬀel, N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, et al. , Exploring the Limits of Transfer\nLearning with a Uni ed Text-to-Text Transformer, arXiv,\n2020, preprint, arXiv:1910.10683v4 [cs.LG], DOI: 10.48550/\narXiv.1910.10683.\n4 A. Kedia, S. C. Chinthakindi and W. Ryu, Beyond reptile:\nmeta-learned dot-product maximization between gradients\nfor improved single-task regularization, in Findings of the\nassociation for computational linguistics: EMNLP 2021 ,\nAssociation for Computational Linguistics, Punta Cana,\nDominican Republic, 2021, p. 407 –420, available from:\nhttps://aclanthology.org/2021.ndings-emnlp.37.\n5 B. Pang, E. Nijkamp, W. Kry´sci´nski, S. Savarese, Y. Zhou and\nC. Xiong, Long Document Summarization with Top-down\nand Bottom-up Inference, arXiv, 2022, preprint,\narXiv:2203.07586v1 [cs.CL], DOI:10.48550/arXiv.2203.07586.\n6 A. Fan, S. Bhosale, H. Schwenk, Z. Ma, A. El-Kishky, S. Goyal,\net al. , Beyond english-centric multilingual machine\ntranslation, Journal of Machine Learning Research, 2021, 22,\n107.\n7 OpenAI R. Gpt-4 technical report, arXiv, 2023, preprint,\narXiv:2303.08774v4, DOI:10.48550/arXiv.2303.08774.\n8 H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. A. Lachaux,\nT. Lacroix, et al., LLaMA: Open and Eﬃcient Foundation\nLanguage Models,arXiv, 2023, preprint, arXiv:2304.03277v1\n[cs.CL], DOI:10.48550/arXiv.2302.13971.\n9 B. Peng, C. Li, P. He, M. Galley and J. Gao, Instruction Tuning\nwith GPT-4, arXiv, 2023, preprint, arXiv:2304.03277v1\n[cs.CL], DOI:10.48550/arXiv.2304.03277.\n10 M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. de Oliveira Pinto,\nJ. Kaplan,et al., Evaluating large language models trained on\ncode, arXiv\n, 2021, preprint, arXiv:2107.03374v2 [cs.LG], DOI:\n10.48550/arXiv.2107.03374.\n11 L. Weston, V. Tshitoyan, J. Dagdelen, O. Kononova,\nA. Trewartha, K. A. Persson, et al. , Named Entity\nRecognition and Normalization Applied to Large-Scale\nInformation Extraction from the Materials Science\nLiterature, J. Chem. Inf. Model., 2019,59(9), 3692–3702.\n12 K. Cruse, A. Trewartha, S. Lee, Z. Wang, H. Huo, T. He,et al.,\nText-mined dataset of gold nanoparticle synthesis\nprocedures, morphologies, and size entities, Sci. Data ,\n2022, 9(1), 234.\n13 V. Venugopal, S. Sahoo, M. Zaki, M. Agarwal, N. N. Gosvami\nand N. M. A. Krishnan, Looking through glass: Knowledge\ndiscovery from materials science literature using natural\nlanguage processing,Patterns, 2021,2(7), 100290.\n14 T. Gupta, M. Zaki, N. M. A. Krishnan and M. Mausam,\nMatSciBERT: a materials domain language model for text\nmining and information extraction, npj Comput. Mater. ,\n2022, 8(1), 102.\n15 S. Huang and J. M. Cole, BatteryBERT: A Pretrained\nLanguage Model for Battery Database Enhancement, J.\nChem. Inf. Model., 2022,62(24), 6365–6377.\n16 S. Mysore, Z. Jensen, E. Kim, K. Huang, H. S. Chang,\nE. Strubell, et al., The materials science procedural text\ncorpus: annotating materials synthesis procedures with\nshallow semantic structures, in Proceedings of the 13th\nlinguistic annotation workshop , Association for\nComputational Linguistics, Florence, Italy, 2019, p. 56–64,\navailable from:https://aclanthology.org/W19-4007.\n17 T. Gupta, M. Zaki, D. Khatsuriya, K. Hira, N. M. A. Krishnan\nand M. Mausam, DiSCoMaT: distantly supervised\ncomposition extraction from tables in materials science\narticles, in Proceedings of the 61st annual meeting of the\nassociation for computational linguistics , Association for\nComputational Linguistics, Toronto, Canada, 2023, vol. 1,\np. 13465–13483, available from: https://aclanthology.org/\n2023.acl-long.753.\n© 2024 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 4 ,3,3 1 3–327 | 325\nPaper Digital Discovery\nOpen Access Article. Published on 20 December 2023. Downloaded on 11/5/2025 1:58:56 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\n18 A. Trewartha, N. Walker, H. Huo, S. Lee, K. Cruse,\nJ. Dagdelen, et al., Quantifying the advantage of domain-\nspecic pre-training on named entity recognition tasks in\nmaterials science,Patterns, 2022,3(4), 100488.\n19 P. Shetty, A. C. Rajan, C. Kuenneth, S. Gupta,\nL. P. Panchumarti, L. Holm, et al. , A general-purpose\nmaterial property data extraction pipeline from large\npolymer corpora using natural language processing, npj\nComput. Mater., 2023,9(1), 1–12.\n20 J. Zhao, S. Huang and J. M. Cole, OpticalBERT and\nOpticalTable-SQA: Text- and Table-Based Language Models\nfor the Optical-Materials Domain, J. Chem. Inf. Model. ,\n2023, 63(7), 1961–1981.\n21 Y. Song, S. Miret and B. Liu, MatSci-NLP: evaluating\nscientic language models on materials science language\ntasks using text-to-schema modeling, inProceedings of the\n61st annual meeting of the association for computational\nlinguistics, Association for Computational Linguistics,\nToronto, Canada, 2023, vol. 1, p. 3621 –3639, available\nfrom: https://aclanthology.org/2023.acl-long.201.\n22 D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika,\nD. Song, et al., Proceedings of the international conference\nlearning, Measuring massive multitask language\nunderstanding, ICLR, 2021, p. 2049.\n23 D. Hendrycks, C. Burns, S. Basart, A. Critch, J. Li, D. Song,\net al., Aligning AI with shared human values, Proc. Int.\nConf. Learn Represent ICLR, 2021.\n24 R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi and Y. Choi,\nHellaSwag: can a machine reallynish your sentence?, in\nProceedings of the 57th annual meeting of the association for\ncomputational linguistics , Association for Computational\nLinguistics, Florence, Italy, 2019, p. 4791–4800, available\nfrom: https://aclanthology.org/P19-1472.\n25 K. Sakaguchi, R. Le Bras, C. Bhagavatula and Y. Choi,\nWinoGrande: An Adversarial Winograd Schema Challenge\nat Scale, Proc. AAAI Conf. Artif. Intell., 2020, vol. 34(5), pp.\n8732–8740.\n26 D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh and\nM. Gardner, DROP: a reading comprehension benchmark\nrequiring discrete reasoning over paragraphs, in\nProceedings of the 2019 conference of the north American\nchapter of the association for computational linguistics:\nHuman language technologies , Association for\nComputational Linguistics, Minneapolis, Minnesota, 2019,\nvol. 1, p. 2368 –2378, available from: https://\naclanthology.org/N19-1246.\n27 K. M. Jablonka, Q. Ai, A. Al-Feghali, S. Badhwar,\nJ. D. Bocarsly, A. M. Bran,et al., 14 examples of how LLMs\ncan transform materials science and chemistry:\nar eection on a large language model hackathon,Digital\nDiscovery, 2023,2(5), 1233–1250.\n28 T. Xie, Y. Wan, W. Huang, Y. Zhou, Y. Liu, S. Wang,et al.,\nDARWIN series: Domain specic large language models for\nnatural science, arXiv, 2023, preprint, arXiv:2308.13565v1\n[cs.CL], DOI:10.48550/arXiv.2308.13565.\n29 Y. Song, S. Miret, H. Zhang and B. Liu, HoneyBee:\nProgressive instruction netuning of large language\nmodels for materials science, arXiv, 2023, preprint,\narXiv:2310.08511v1 [cs.CL], DOI:10.48550/arXiv.2310.08511.\n30 K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun,\nL. Kaiser, et al., Training veri ers to solve math word\nproblems, arXiv, 2021, preprint, arXiv:2110.14168v2\n[cs.LG], DOI:10.48550/arXiv.2110.14168.\n31 P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal,\nC. Schoenick, et al. , Think you have solved question\nanswering? try arc, the ai2 reasoning challenge, arXiv,\n2018, preprint, arXiv:1803.05457v1 [cs.AI], DOI: 10.48550/\narXiv.1803.05457.\n32 ChemistryQA Data, Microso, 2023, available from: https://\ngithub.com/microso/chemistry-qa.\n33 P. Lu, S. Mishra, T. Xia, L. Qiu, K. W. Chang, S. C. Zhu,et al.,\nLearn to explain: Multimodal reasoning via thought chains\nfor science question answering, inThe 36th conference on\nneural information processing systems (NeurIPS), 2022.\n34 J. Welbl, N. F. Liu and M. Gardner, Crowdsourcing multiple\nchoice science questions, arXiv, 2017, preprint,\narXiv:1707.06209v1 [cs.HC], DOI: 10.48550/\narXiv.1707.06209.\n35 D. Arora, H. Singh, M. Mausam, Proceedings of the 2023\nconference on empirical methods in natural language\nprocessing, in Have LLMs advanced enough? A challenging\nproblem solving benchmark for large language models , ed.\nBouamor H., Pino J., Bali K., Association for\nComputational Linguistics, Singapore, 2023, pp. 7527 –\n7543, Available from: https://aclanthology.org/2023.emnlp-\nmain.468.\n36 A. D. White, G. M. Hocky, H. A. Gandhi, M. Ansari, S. Cox,\nG. P. Wellawatte, et al. , Assessment of chemistry\nknowledge in large language models that generate code,\nDigital Discovery, 2023,2(2), 368–376.\n37 H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi,\nY. Babaei, et al., Llama 2: Open Foundation and Fine-\nTuned Chat Models, arXiv, 2023, preprint,\narXiv:2307.09288v2 [cs.CL], DOI:10.48550/arXiv.2307.09288.\n38 B. Workshop, T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili´c,\net al., BLOOM: A 176B-Parameter Open-Access Multilingual\nLanguage Model, arXiv, 2023, preprint, arXiv:2211.05100v4\n[cs.CL], DOI:10.48550/arXiv.2211.05100.\n39 G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli,\nH. Alobeidli,et al., The RenedWeb Dataset for Falcon LLM:\nOutperforming Curated Corpora with Web Data, and Web\nData Only, arXiv, 2023, preprint, arXiv:2211.05100v4\n[cs.CL], DOI:10.48550/arXiv.2306.01116.\n40 J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi,\net al. , Chain-of-thought prompting elicits reasoning in\nlarge language models, Advances in Neural Information\nProcessing Systems, 2022, vol. 35, pp. 24824–24837.\n41 X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang,\net al. , Self-Consistency Improves Chain of Thought\nReasoning in Language Models, arXiv, 2023, preprint,\narXiv:2203.11171v4 [cs.CL], DOI:10.48550/arXiv.2203.11171.\n42 A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao,\nS. Wiegreﬀe, et al., Self-Rene: Iterative Renement with\n326 | Digital Discovery,2 0 2 4 ,3,3 1 3–327 © 2024 The Author(s). Published by the Royal Society of Chemistry\nDigital Discovery Paper\nOpen Access Article. Published on 20 December 2023. Downloaded on 11/5/2025 1:58:56 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online\nSelf-Feedback, arXiv, 2023, preprint, arXiv:2303.17651v2\n[cs.CL], DOI:10.48550/arXiv.2303.17651.\n43 N. Shinn, F. Cassano, E. Berman, A. Gopinath,\nK. Narasimhan and S. Yao, Re exion: Language Agents\nwith Verbal Reinforcement Learning,arXiv, 2023, preprint,\narXiv:2303.11366v4 [cs.AI], DOI:10.48550/arXiv.2303.11366.\n44 S. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. Del\nGiorno, S. Gopi, et al., Textbooks Are All You Need,arXiv,\n2023, arXiv:2306.11644v2 [cs.CL], DOI: 10.48550/\narXiv.2306.11644.\n45 M. Zaki, A. Jan, N. M. A. Krishnan and J. C. Mauro,\nGlassomics: An omics approach toward understanding\nglasses through modeling, simulations, and arti cial\nintelligence, MRS Bull., 2023,48(10), 1026–1039.\n© 2024 The Author(s). Published by the Royal Society of Chemistry Digital Discovery,2 0 2 4 ,3,3 1 3–327 | 327\nPaper Digital Discovery\nOpen Access Article. Published on 20 December 2023. Downloaded on 11/5/2025 1:58:56 PM. \n This article is licensed under a \nCreative Commons Attribution 3.0 Unported Licence.\nView Article Online",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.523322343826294
    },
    {
      "name": "Question answering",
      "score": 0.4493808448314667
    },
    {
      "name": "Data science",
      "score": 0.3314334750175476
    },
    {
      "name": "Natural language processing",
      "score": 0.2723888158798218
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I68891433",
      "name": "Indian Institute of Technology Delhi",
      "country": "IN"
    }
  ],
  "cited_by": 34
}