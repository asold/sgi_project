{
  "title": "LFOSum: Summarizing Long-form Opinions with Large Language Models",
  "url": "https://openalex.org/W4403813734",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2147508193",
      "name": "Mir Tafseer Nayeem",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A272765103",
      "name": "Davood Rafiei",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A2147508193",
      "name": "Mir Tafseer Nayeem",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A272765103",
      "name": "Davood Rafiei",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2006723435",
    "https://openalex.org/W1788852239",
    "https://openalex.org/W2286787313",
    "https://openalex.org/W2160660844",
    "https://openalex.org/W4205184193",
    "https://openalex.org/W3034352114",
    "https://openalex.org/W1939882552",
    "https://openalex.org/W4385570501",
    "https://openalex.org/W6632455782",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W3035576805",
    "https://openalex.org/W3020873268",
    "https://openalex.org/W2888507208",
    "https://openalex.org/W2913407944",
    "https://openalex.org/W3098648976",
    "https://openalex.org/W3153621364",
    "https://openalex.org/W3198269165",
    "https://openalex.org/W4284707793",
    "https://openalex.org/W4205742210",
    "https://openalex.org/W4385571766",
    "https://openalex.org/W3034383590",
    "https://openalex.org/W4385572316",
    "https://openalex.org/W4399912398",
    "https://openalex.org/W4402672021",
    "https://openalex.org/W4387156782",
    "https://openalex.org/W3035043191",
    "https://openalex.org/W4387323904",
    "https://openalex.org/W4389519872",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2251777082",
    "https://openalex.org/W2890989031",
    "https://openalex.org/W2251294039",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4385572201",
    "https://openalex.org/W4385734302",
    "https://openalex.org/W4401042146",
    "https://openalex.org/W6761205521",
    "https://openalex.org/W4401042342",
    "https://openalex.org/W4401043234",
    "https://openalex.org/W4402667081",
    "https://openalex.org/W3117109765",
    "https://openalex.org/W4400342148",
    "https://openalex.org/W4402667059",
    "https://openalex.org/W4386576789",
    "https://openalex.org/W4404782547",
    "https://openalex.org/W4285118796",
    "https://openalex.org/W4390962650",
    "https://openalex.org/W4401043828",
    "https://openalex.org/W2696068122",
    "https://openalex.org/W4392427635",
    "https://openalex.org/W3153949951"
  ],
  "abstract": "Online reviews play a pivotal role in influencing consumer decisions across various domains, from purchasing products to selecting hotels or restaurants. However, the sheer volume of reviews—often containing repetitive or irrelevant content—leads to information overload, making it challenging for users to extract meaningful insights. Traditional opinion summarization models face challenges in handling long inputs and large volumes of reviews, while newer Large Language Model (LLM) approaches often fail to generate accurate and faithful summaries. To address those challenges, this paper introduces (1) a new dataset of long-form user reviews, each entity comprising over a thousand reviews, (2) two training-free LLM-based summarization approaches that scale to long inputs, and (3) automatic evaluation metrics. Our dataset of user reviews is paired with in-depth and unbiased critical summaries by domain experts, serving as a reference for evaluation. Additionally, our novel reference-free evaluation metrics provide a more granular, context-sensitive assessment of summary faithfulness. We benchmark several open-source and closed-source LLMs using our methods. Our evaluation reveals that LLMs still face challenges in balancing sentiment and format adherence in long-form summaries, though open-source models can narrow the gap when relevant information is retrieved in a focused manner1",
  "full_text": null,
  "topic": "Linguistics",
  "concepts": [
    {
      "name": "Linguistics",
      "score": 0.555225133895874
    },
    {
      "name": "Computer science",
      "score": 0.4908369183540344
    },
    {
      "name": "Natural language processing",
      "score": 0.40178340673446655
    },
    {
      "name": "Philosophy",
      "score": 0.13537967205047607
    }
  ]
}