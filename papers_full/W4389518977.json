{
  "title": "HoneyBee: Progressive Instruction Finetuning of Large Language Models for Materials Science",
  "url": "https://openalex.org/W4389518977",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2096641451",
      "name": "Yu Song",
      "affiliations": [
        "Université de Montréal"
      ]
    },
    {
      "id": "https://openalex.org/A2155457381",
      "name": "Santiago Hector Raúl Miret",
      "affiliations": [
        "Intel (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1522334356",
      "name": "Huan Zhang",
      "affiliations": [
        "University of Waterloo"
      ]
    },
    {
      "id": "https://openalex.org/A2130952716",
      "name": "Bang Liu",
      "affiliations": [
        "Université de Montréal"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385571746",
    "https://openalex.org/W3127365350",
    "https://openalex.org/W3201869313",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3115677442",
    "https://openalex.org/W4365597205",
    "https://openalex.org/W4248414713",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4303443398",
    "https://openalex.org/W4311991106",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4385570852",
    "https://openalex.org/W4365211632",
    "https://openalex.org/W4229443452",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4312091890",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4366198844",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W3014297634"
  ],
  "abstract": "We propose an instruction-based process for trustworthy data curation in materials science (MatSci-Instruct), which we then apply to finetune a LLaMa-based language model targeted for materials science (HoneyBee). MatSci-Instruct helps alleviate the scarcity of relevant, high-quality materials science textual data available in the open literature, and HoneyBee is the first billion-parameter language model specialized to materials science. In MatSci-Instruct we improve the trustworthiness of generated data by prompting multiple commercially available large language models for generation with an Instructor module (e.g. Chat-GPT) and verification from an independent Verifier module (e.g. Claude). Using MatSci-Instruct, we construct a dataset of multiple tasks and measure the quality of our dataset along multiple dimensions, including accuracy against known facts, relevance to materials science, as well as completeness and reasonableness of the data. Moreover, we iteratively generate more targeted instructions and instruction-data in a finetuning-evaluation-feedback loop leading to progressively better performance for our finetuned HoneyBee models. Our evaluation on the MatSci-NLP benchmark shows HoneyBee's outperformance of existing language models on materials science tasks and iterative improvement in successive stages of instruction-data refinement. We study the quality of HoneyBee's language modeling through automatic evaluation and analyze case studies to further understand the model's capabilities and limitations. Our code and relevant datasets are publicly available at https://github.com/BangLab-UdeM-Mila/NLP4MatSci-HoneyBee.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5724–5739\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nHoneyBee: Progressive Instruction Finetuning of Large Language\nModels for Materials Science\nYu Song1, Santiago Miret2,∗, Huan Zhang3, Bang Liu1,∗†\n1University of Montreal / Mila - Quebec AI, 2Intel Labs 3University of Waterloo\n{yu.song, bang.liu}@umontreal.ca\nsantiago.miret@intel.com\nh648zhan@uwaterloo.ca\nAbstract\nWe propose an instruction-based process for\ntrustworthy data curation in materials science\n(MatSci-Instruct), which we then apply to fine-\ntune a LLaMa-based language model targeted\nfor materials science (HoneyBee). MatSci-\nInstruct helps alleviate the scarcity of rele-\nvant, high-quality materials science textual data\navailable in the open literature, and HoneyBee\nis the first billion-parameter language model\nspecialized to materials science. In MatSci-\nInstruct we improve the trustworthiness of gen-\nerated data by prompting multiple commer-\ncially available large language models for gen-\neration with an Instructor module (e.g. Chat-\nGPT) and verification from an independent Ver-\nifier module (e.g. Claude). Using MatSci-\nInstruct, we construct a dataset of multiple\ntasks and measure the quality of our dataset\nalong multiple dimensions, including accuracy\nagainst known facts, relevance to materials sci-\nence, as well as completeness and reasonable-\nness of the data. Moreover, we iteratively gen-\nerate more targeted instructions and instruction-\ndata in a finetuning-evaluation-feedback loop\nleading to progressively better performance for\nour finetuned HoneyBee models. Our evalu-\nation on the MatSci-NLP benchmark shows\nHoneyBee’s outperformance of existing lan-\nguage models on materials science tasks and\niterative improvement in successive stages of\ninstruction-data refinement. We study the qual-\nity of HoneyBee’s language modeling through\nautomatic evaluation and analyze case studies\nto further understand the model’s capabilities\nand limitations. Our code and relevant datasets\nare publicly available.1\n1 Introduction\nNatural language processing (NLP) holds consid-\nerable promise in expediting the discovery and un-\n∗Equal advising.\n† Corresponding author. Canada CIFAR AI Chair.\n1https://github.com/BangLab-UdeM-Mila/NLP4MatSci-\nHoneyBee\nderstanding of novel material systems, which will\nbe crucial for addressing contemporary societal\nchallenges like climate change and drug discovery.\nThe potential impact of NLP in materials science\nis chiefly underpinned by the vast reservoir of ma-\nterials science knowledge contained in text-based\nresources, such as textbooks, scientific journals,\nand assorted reports. In spite of the prospective\nrichness of materials science textual data available\nfrom diverse sources, a number of challenges con-\ntinue to significantly hinder the effective digestion\nand comprehension of relevant materials science\ntextual knowledge (Song et al., 2023; Kononova\net al., 2021). Some of the challenges relate to the\ngeneral availability of data, while other relate to\nthe ability to effectively process domain-specific\ninformation, such as chemical notation and data\ncontained in figures and tables (Gupta et al., 2023).\nThis scarcity of readily accessible, high-quality\ntext corpora suitable for efficient language model\ntraining has in turn slowed the development of com-\nprehensive language models capable of spanning\nthe extensive conceptual range within the highly\ninterdisciplinary materials science field.\nWhile data availability remains an ongoing chal-\nlenge in applying modern NLP tools for materials\nscience, recent advancements have led to the emer-\ngence of large language models (LLMs) proficient\nin handling general language tasks that concur-\nrently demonstrate substantial aptitude in areas like\nchemistry and materials science (Bran et al., 2023;\nBoiko et al., 2023). Such advancements provide the\npotential to harness the implicit knowledge encap-\nsulated in these models, which have been trained on\nvast text corpora spanning a broad range of subjects,\nto generate accessible, instruction-based datasets\nfor specialized domains like materials science.\nYet, while we can generate targeted instruction-\nbased data to make applying NLP for materials\nscience more accessible, the quality of these in-\nstructions requires rigorous evaluation before be-\n5724\nFigure 1: Example instruction generated by the MatSci-\nInstruct process used to train the HoneyBee language\nmodel that contains general language knowledge and\nis specialized in materials science. The relevant text to\ncorrectly answer the instruction is highlighted. MatSci-\nInstruct follows a structured instruction generation tem-\nplate and ensures instruction quality through an iterative\nverification loop described in Section 3.1.\ning utilized for language model training. This is\nparticularly salient in the context of complex sci-\nentific applications like materials science, which\nencompasses a wide range of subfields that together\ndescribe the properties and behavior of matter that\nmake up materials systems. This need for trust-\nworthy and pertinent instructions necessitates the\ncreation of a robust process to validate the quality\nof instructions for downstream applications.\nAside from data scarcity in scientific domains,\nanother significant impediment to the application\nof NLP in materials science is the limited presence\nof specialized language models that incorporate\nboth in-depth materials science knowledge and a\nrobust understanding of general language. The bulk\nof today’s available language models for materials\nscience are built on the BERT architecture (Gupta\net al., 2022; Walker et al., 2021; Huang and Cole,\n2022), whose performance in general NLP tasks\nhas been superseded by several more advanced lan-\nguage model architectures in recent years (Touvron\net al., 2023; Scao et al., 2022; Brown et al., 2020;\nChung et al., 2022). This highlights the need for\nthe development of more capable language mod-\nels in materials science that can accommodate a\nbroader knowledge base while effectively perform-\ning pertinent materials science language tasks.\nThis paper seeks to concurrently address the pre-\nviously outlined challenges of trustworthy instruc-\ntion generation and capable, open-source language\nmodels for materials science. We propose MatSci-\nInstruct to generate reliable, instruction-based data\nfrom large language models. This data is then\nused to train HoneyBee, a billion-parameter spe-\ncialized materials science language model based on\nthe LLaMa architecture (Touvron et al., 2023). The\nkey contributions of our research are as follows:\n• MatSci-Instruct – A Two-Step Framework\nfor Trustworthy Instruction-Data Gener-\nation: We propose a universally applicable\nmethodology suited for instruction-data gen-\neration in scientific domains. MatSci-Instruct\ngenerates specialized instruction-data using a\ntwo-step framework – Generation and Verifi-\ncation. In the Generation step, an instructor\nmodel (Chat-GPT 2) creates domain-specific\ninstruction-data focused on materials science.\nDuring the Verification step, the instruction-\ndata are cross-verified by a separate verifier\nmodel (Claude 3) for accuracy and relevance\nas shown by the example in Figure 1. More-\nover, in Section 4.1 we conduct human evalu-\nations that suggest good alignment of our gen-\nerated MatSci-Instruct Dataset with human\nexperts across several dimensions: accuracy\nagainst known facts, relevance to materials\nscience, and the completeness and reasonable-\nness of the language model output.\n• HoneyBee – A High-Performance LLaMa-\nBased Model Progressively Trained via\nMatSci-Instruct: Utilizing the MatSci-\nInstruct two-step framework, we apply a\nProgressive Refinement-Feedback strategy to\nfinetune a LLaMa model, culminating in the\nHoneyBee model. In our progressive finetun-\ning strategy, the HoneyBee model’s perfor-\nmance on MatSci-Instruct data guides subse-\nquent instruction-data generation. This itera-\ntive process results in further refined instruc-\ntions to generate higher-quality instruction-\ndata for finetuning, ensuring the progressive\nacquisition of specialized knowledge by the\n2https://platform.openai.com/docs/\napi-reference/chat\n3https://docs.anthropic.com/claude/docs\n5725\nmodel. We evaluate the performance of Hon-\neyBee using a materials science language\nbenchmark (Song et al., 2023), and thoroughly\nanalyze its strengths and limitations.\n2 Related Work\nLarge Language Models Large Language Mod-\nels (LLMs) have gained substantial attention from\nthe NLP research and wider technology communi-\nties due to their remarkable proficiency in language\nunderstanding and generative tasks. Pioneers like\nGPT-3 (Brown et al., 2020), with its 175 billion\nparameters, demonstrated the capacity to capture\ncomplex linguistic patterns, and subsequent mod-\nels like Gopher (Rae et al., 2022), GLM (Zeng\net al., 2022), PaLM (Chowdhery et al., 2022),\nBloomZ (Scao et al., 2022), Chincilla (Hoffmann\net al., 2022), and OPT (Zhang et al., 2022) continue\nto drive progress. Commercial models like Chat-\nGPT (OpenAI, 2022) and Claude (Bai et al., 2022)\nfurther expand the landscape of performant LLMs.\nCompared to commercial LLMs, LLaMa (Touvron\net al., 2023) stands out for its greater accessibility\nand good performance, offering an efficient and\naccessible platform for domain-specific finetuning\nin various domains, including materials science.\nNLP for Materials Science NLP applications\nwithin materials science are constrained by the\ndual shortage of openly accessible, high-quality\ndata and high-performing language models. While\nstrides have been made towards enhancing data\navailability (Song et al., 2023; Olivetti et al.,\n2020; Kononova et al., 2021; Gao et al., 2020),\nthe primary focus has been on generating expert-\nannotated data for finetuning BERT-based models,\nwhich lack the advanced capabilities of contem-\nporary LLMs. For a detailed review of the per-\nformance of various BERT models on materials\nscience language tasks, we refer the reader to Song\net al. (2023). The prevailing scarcity of data and\nspecialized LLMs in materials science motivates\nus to propose MatSci-Instruct, an instruction-based\nmethod for data creation, and HoneyBee, a special-\nized LLM tailored for materials science.\nInstruction Finetuning LLMs LLMs consis-\ntently demonstrate substantial improvements when\nfinetuned for specialized tasks, as seen with\nbiomedical models like ChatDoctor (Li et al., 2023)\nand HuaTuo (Wang et al., 2023). While the large\nmodel size of LLMs poses a challenge for effec-\ntive finetuning, several efficient methods have been\nproposed (Mangrulkar et al., 2022), such as P-\nTuning (Liu et al., 2021), Prefix Tuning (Li and\nLiang, 2021), Prompt Tuning (Lester et al., 2021),\nand LoRA (Hu et al., 2021). Among these, LoRA\nutilizes low-rank matrix decomposition to limit\nthe additional parameters required for fine-tuning.\nFor data curation in specialized fields, instructions-\nbased fine-tuning extracts detailed data directly\nfrom LLMs (Ouyang et al., 2022), reducing human\nannotation effort and providing scalable solutions.\nFor example, Alpaca (Taori et al., 2023; Wang\net al., 2022) exploits LLMs to generate synthetic\ninstructions for model finetuning. However, LLM-\nsynthesized data still suffer from data quality issues,\nwhich is especially critical for scientific domains.\nTo address these concerns, we design a generation-\nverification strategy for trustworthy data generation\nand a progressive refinement-feedback strategy for\nfinetuning LLMs on specialized instructions.\n3 Method\nOur work consists of two interacting components:\n1) MatSci-Instruct: a trustworthy instruction gen-\neration framework for obtaining scientific textual\ndata from LLMs; 2) HoneyBee: a materials science\nLLM progressively finetuned from LLaMA (Tou-\nvron et al., 2023) using MatSci-Instruct generated\ndata. We connect HoneyBee to MatSci-Instruct\nwith a refinement-feedback loop to progressively\ngenerate new data and finetune HoneyBee based\non its training status as shown in Figure 2.\n3.1 MatSci-Instruct\nThe challenges of cost-effectively generating high-\nquality instruction data are not unique to materials\nscience, but rather, pervasive across various sci-\nentific domains. Our proposed solution, MatSci-\nInstruct, is an innovative, domain-agnostic method-\nology that leverages the power of large language\nmodels (LLMs) to generate specialized instruction\nsets for subsequent model finetuning.\nDepicted in Figure 2, MatSci-Instruct employs\na trifecta of distinct LLMs. The Instructor model\ncrafts instructions using structured prompts encap-\nsulating topic and task details. The Verifier then\nevaluates these instructions against accuracy, rele-\nvance, completeness, and reasonableness criteria,\nensuring only dependable instructions advance to\nfinetuning. Finally, the Evaluator assesses the out-\nput of the finetuned model along similar dimen-\nsions as the Verifier. Poorly executed instructions\n5726\nH/o_neyBee\nInstructor Verifier Evaluator\nTask/topic\n<Instruction> <Input> <Output>\nSample arXiv\n“Evaluate this example by …”\nAccuracy\nCompleteness Relevance\nReasonableness\n“Evaluate the model by …”\nAccuracy\nCompleteness\nReasonableness\nModel capability evaluation:\n- Accuracy of output …\n- Reasonableness of output …\n- Completeness of output …\n- …\nMatSci-Instruct\nProgressive Instruction Fine-tuning\nFigure 2: MatSci-Instruct and HoneyBee training workflow. We start with a series of predetermined structured\ninstruction generation prompts that contain both topic and task descriptions. The Instructor (Chat-GPT) then\ngenerates a series of instructions that are then passed through the Verfifier (Claude). The instructions that receive\nhigh scores with the Verifier are used for progressive finetuning in HoneyBee. The Evaluator (GPT-4) then evaluates\nHoneyBee’s outputs and poor instructions that lead to bad performance are subsequently regenerated from the\nbeginning creating an instruction generation feedback loop for greater instruction quality.\nare flagged for further refinement, verification, and\nevaluation. Ultimately, we generate 52k instruc-\ntions spanning content-based and open-ended tasks,\nsome of which include empty inputs. Table 1 shows\nthat the number of instructions gets reduced in later\nstages of the progressive-refinement-feedback loop\nmainly due to greater emphasis on quality.\nMatSci-Instruct Statistics\n# instructions for first stage 52,658\n# open-ended instructions 9,931\n# content-based instructions 39,170\n# instructions with empty input 3,557\n# instructions for subsequent stages 3,020\navg. input length (in words) 920.8\navg. instruction length (in words) 76.5\navg. output length (in words) 211.2\nTable 1: Statistics of instruction data generated by\nMatSci-Instruct spanning diverse instruction types.\n3.1.1 MatSci-Instruct Example\nNext, we show a full example of the progressive\nMatSci-Instruct instruction-data refinement proce-\ndure in the loop with HoneyBee model finetuning:\n1. Instruction-Data Generation and Finetuning\n• Data Generation: Instructor generates\ntraining data (data_train0[1 − 10]).\n• Data Verification: Verifier removes low-\nscoring data (data_train0[1,2])\n• Finetuning: LLaMa-7b becomes\nHoneyBee-7b-Stage-1 after finetuning\nwith data from (data_train0[3 − 10]).\n2. Evaluation\n• HoneyBee-7b-Stage-1 performs infer-\nence on new test data ( data_test0),\ncrafted by the Instructor, with outputs\nevaluated by the Evaluator.\n3. Feedback Response\n• Response Generation: HoneyBee-7b-\nStage-1 generates responses for the test\ndata (data_test0).\n• Scoring Responses: The Evaluator spots\nweak responses (data_test0[7,8]).\n4. Instruction-Data Adaptation and Refinement\n• Focusing on Weaknesses: The Instruc-\ntor crafts more training ( data_train1)\nand test data (data_test1), focusing on\nissues identified by the Evaluator.\n• Finetuning Stage 2: HoneyBee-7b-Stage-\n1 refines to HoneyBee-7b-Stage-2.\n• Re-Evaluation: HoneyBee-7b-Stage-2 is\ntested with new test data (data_test1).\nThis process is repeated in an iterative feedback\nloop for continued refinement as shown in Figure 2.\n3.1.2 Instructor Module\nThe Instructor module of our framework, embod-\nied by ChatGPT, performs the generation of mate-\nrial science instruction-data. This module employs\na concise instruction prompt schema composed\nof three elements: <instruction>, <input>, and\n<output>. The <instruction> outlines the task\nusing a standardized NLP task set, the <input>\n5727\nFigure 3: Wordcloud of diverse materials science topics\ncontained in the MatSci-Instruct dataset.\ncontains the relevant data, and the <output> gen-\nerates a pertinent response to the task.\nWe query ChatGPT with this schema, populat-\ning the <instruction> and <input> fields with a\nselection of 20 NLP tasks and 20 materials science\nsubtopics shown in Figure 3, to ensure task and con-\ntent diversity. These selections are manually veri-\nfied before they are utilized in structured prompts\nfor generating detailed finetuning instruction-data.\nDetailed lists of prompts and materials science top-\nics are available in Appendix B and Appendix E.\nFollowing the schema, we engage in a random\nsampling process, selecting five candidate topics\nand five tasks, then applying them to the instruction\nprompts for data generation. For robustness, we\ndirect ChatGPT to flag in the <output> field any\ninstruction that cannot be processed based solely\non the <input> and <instruction>. To control\ntask difficulty and boost diversity, we occasionally\nlimit the length of <instruction> or <output>.\nTo enhance the diversity and robustness of the\ninstruction-data generation process, our design in-\ncorporates several additional strategies. One such\nstrategy employs an open-ended task where the\n<input> field remains intentionally blank, allow-\ning the model to generate responses without pre-\ndefined constraints. This approach tests the gener-\native abilities of the model under uncertainty and\npromotes more varied outcomes. Another key strat-\negy is content-based instruction-data generation.\nInstead of relying on predefined topics and tasks,\nthis approach utilizes real-world materials science\nliterature. We select a random open-access paper\nfrom the materials science category on arXiv and\nextract a specific fragment to fill the <input> field.\nThis method not only diversifies the task prompts\nbut also aligns the generated instruction-data more\nclosely with practical, domain-specific contexts.\nTo conclude the instruction-data generation pro-\ncess, ChatGPT compiles ten representative instruc-\ntion prompt samples from all possible options.\nThese samples are formatted in a standardized\nJSON format, readily available for use in the sub-\nsequent steps of the MatSci-Instruct process. This\napproach ensures a comprehensive and diverse set\nof instructions, which in turn contributes to a robust\nand adaptable language model during finetuning.\n3.1.3 Verifier Module\nGenerating high-quality instruction-data can be\nchallenging, and the presence of low-quality data\nin finetuning a model can lead to misleading re-\nsults. To address this issue, MatSci-Instruct em-\nploys a two-step framework by incorporating a Ver-\nfier model to improve the trustworthiness of gen-\nerated data. Specifically, we use Claude as the\nVerifier to ensure the quality of the instructions\ngenerated by the Instructor (Chat-GPT).\nOur evaluation is based on four dimensions: ac-\ncuracy, relevance, completeness, and reasonable-\nness. Similar to the instruction-data generation,\ninstruction verification is based on a standard set\nof prompts, shown in Appendix E, which include\nprecise definitions of the evaluation criteria along\nwith the complete instructions generated by the\nInstructor. Concretely, the evaluation criteria are:\n• Accuracy: The accuracy of the instruction-\ndata is evaluated by comparing it with known\nfacts or credible sources. This involves check-\ning the accuracy of any claims or statements\nmade in the text and verifying that they are\nsupported by evidence.\n• Relevance: The relevance of the instruction-\ndata is assessed by determining how directly\nit relates to materials science. This is achieved\nby analyzing the text’s content and ascertain-\ning its applicability to the field.\n• Completeness: Completeness is an essential\ndimension to ensure the instruction-data com-\nprehensively address the given task, inclusive\nof all sub-questions. This involves consider-\ning both depth and conciseness to ensure that\nthe output is complete and comprehensive.\n• Reasonableness: The reasonableness of the\ninstruction-data is about logical consistency.\n5728\nThis dimension ensures no evident contradic-\ntions exist within the generated data.\nThe verifier module (i.e., Claude) evaluates the\ninstruction-data based on the four dimensions men-\ntioned above and identifies any low-quality data\nthat falls below a predetermined threshold. This\nrigorous verification ensures the use of high-quality\ndata in model fine-tuning, thereby improving the\noverall efficacy and accuracy of the system. Our\nverification protocol is designed for modularity and\nextensibility. This modular design facilitates the in-\ncorporation of additional agents into a multi-agent\nsystem, each assessing instruction-data based on\npre-defined criteria. The final decision on data qual-\nity is then reached through a consensus mechanism,\naugmenting the robustness and comprehensiveness\nof the verification process, ensuring high-quality\ndata for model finetuning.\n3.1.4 Evaluator Module\nThe Evaluator model assesses the output of the\nHoneyBee language model along similar evalua-\ntion dimensions as the Verifier, namely: accuracy,\ncompleteness, and reasonableness. We no longer\nconsider relevance at this stage since the verifi-\ncation step filtered out all instructions with little\nrelevance to materials science. In this paper, we use\nGPT-4 4 (OpenAI, 2023) as the Evaluator model,\nwhich provides an additional independent LLM\nthat is different, and potentially more advanced,\nthan the Instructor and Verifier LLMs. The Evalu-\nator also helps with the identification of poorly for-\nmulated instructions according to the performance\nof the HoneyBee model. These instructions are\nthen passed back to the Instructor for additional\niterative refinement.\n3.2 HoneyBee\nUpon obtaining a set of trustworthy instruction data\nfrom the Verifier, we can use the generated instruc-\ntion dataset to finetune a LLaMa-based model for a\nspecific domain. In this work, we finetune a model\nfor materials science using a progressive finetuning\ntechnique to convert a standard LLaMa model to a\nspecialized model in material science: HoneyBee.\n3.2.1 Progressive Instruction Finetuning\nIn our approach, as depicted in Figure 2, we harness\na progressive instruction finetuning methodology\nthat relies on a feedback loop. This loop enables\n4https://openai.com/research/gpt-4\nthe progressive generation of new instruction data\nthat takes into account the evaluated model’s per-\nformance on different criteria, tasks, and topics.\nInstructions leading to suboptimal performance\nby HoneyBee are returned to the Instructor, trig-\ngering the creation of more detailed and targeted\ninstructions for future iterations. This iterative pro-\ncess also includes instruction evaluation by the In-\nstructor, enabling the generation of more precise in-\nstruction data for subsequent rounds. For instance,\nshould HoneyBee score low on ’Completeness’ for\na particular instruction, we inform the Instructor\nof this deficiency, providing the criteria for ’Com-\npleteness’. Consequently, the Instructor generates\nenhanced instruction-data to improve HoneyBee’s\ncompleteness in responding to similar tasks.\nOur progressive finetuning process for the lan-\nguage model is based on LoRA (Hu et al., 2021),\nwhere we create and train a separate set of low-\nrank matrices ψ that bypass the need for chang-\ning the actual parameters of the language model ϕ.\nSince ψconsists of low rank-matrices, it is signif-\nicantly more parameter and compute efficient for\nmodel finetuning. In our finetuning process, we\nassume that the Instructor + Verifier models act as\nthe teacher model and the HoneyBee model acts as\nthe student model. In this setting, the student model\nwill continually learn from the instruction-data and\nundergo testing during the learning process, allow-\ning us to monitor its performance in real-time. The\nfinetuning process continues for a set number of\nepochs with early stopping if the student model\nconverges to a given loss value. Next, we evaluate\nthe response quality of the student model for any\ngiven instruction with the Evaluator. In our pro-\ngressive finetuning strategy, we monitor the eval-\nuation scores after each stage, denoted as Svalbest ,\nand terminate the process when the Svalbest stops\nyielding significant improvements. In our exper-\niments in Section 4, we perform three stages of\nprogressively finetuning both the instruction-data\nand the HoneyBee model parameters.\n4 Experiments\nOur experiments focus on assessing the ability of\nMatSci-Instruct to create high-quality, trustworthy\ninstruction-data relevant to materials science, as\ndescribed in Section 3.1, along with understanding\nthe capabilities and limitations of HoneyBee.\n5729\n4.1 MatSci-Instruct Evaluation\nA critical piece of the MatSci-Instruct pipeline is\nthe independent verification and evaluation of the\ninstruction-data generated by the Instructor model.\nGiven the importance of the Verifier and Evalua-\ntor in ensuring the quality of the instruction-data,\nand the fact that understanding materials science\ntextual data requires deep domain understanding,\nwe conducted an evaluation with human experts\non the trustworthiness of the instructions generated\nby MatSci-Instruct. In our human expert evalu-\nation, we asked two graduate students majoring\nin material science to evaluate 50 randomly se-\nlected instruction data along the same evaluation\ndimensions as the Verifier module (accuracy, rel-\nevance, completeness, reasonableness). Next, we\nconducted a verification and evaluation of the same\n50 instructions using Claude and GPT-4 respec-\ntively. We measure agreement between the human\nexperts and the LLMs by calculating Spearman and\nPearson correlation coefficients between the scores\nalong each of the dimensions.\nFigure 4: Correlation between human evaluation and\nLLM evaluation (Claude, GPT-4). Both Spearman and\nPearson correlation coefficients consistently exceed 0.6\nbetween both methods indicating good agreement.\nAs shown in Figure 4, both Claude and GPT-4\nhad correlation coefficients higher than 0.6 for each\ndimension and an overall coefficient as high as 0.8\nwhen compared to manual evaluation. This indi-\ncates a decent level of consistency between manual\nand automatic evaluations for a random sample\nof instructions, which gives us confidence in the\nability of MatSci-Instruct to generate trustworthy,\nhigh-quality instructions for HoneyBee finetuning.\nModel AccuracyCompletenessReasonableness\nZero-Shot LLMs\nChat-GPT 92.55 98.74 99.84\nLlama-7b 78.81 90.36 97.64\nLlama-13b 84.22 91.22 98.33\nAlpaca-7b 81.35 92.01 98.49\nAlpaca-13b 86.24 92.17 98.80\nHoneyBee without Verfication\nHoneyBee-7b 85.42 93.24 98.49\nHoneyBee-13b88.76 93.99 98.93\nHoneyBee with MatSci-Instruct\nHB-7b-Stage1 88.81 93.42 99.07\nHB-7b-Stage2 89.99 94.84 99.64\nHB-7b-Stage3 91.95 95.78 99.90\nHB-13b-Stage194.17 94.42 99.40\nHB-13b-Stage296.42 95.42 99.78\nHB-13b-Stage398.11 97.00 99.89\nTable 2: Evaluation results for various LLMs based on\nperformance on MatSci-Instruct data along with accu-\nracy, completeness, and reasonableness performed by\nGPT-4. HoneyBee performs better with verification and\ngets progressively better with each iterative stage of\nMatSci-Instruct approaching and exceeding the perfor-\nmance of Chat-GPT in the case of HoneyBee-13b. We\nhighlight scores that outperform Chat-GPT.\n4.2 HoneyBee Task Evaluation\nWe generated a test set of instruction-data with the\nsame generation process as the train set that is only\nintroduced it during model evaluation. The results\nin Table 2 show that HoneyBee gets progressively\nbetter with each iteration of MatSci-Instruct for\nboth HoneyBee-7b and HoneyBee-13b. HoneyBee\nwithout verification also outperforms LLaMA and\nAlpaca LLMs of equal size indicating the value of\nthe progressive finetuning approach on specialized\nmaterials science instruction-data. HoneyBee-13b\nclosely matches, and in some exceeds, the evalua-\ntion performance of Chat-GPT which served as the\nInstructor. Notably, HoneyBee-13b is ∼ 10xmore\nparameter efficient than GPT-3.\n4.3 HoneyBee Performance on MatSci-NLP\nIn addition to evaluating the performance of Hon-\neyBee based on LLM assessment in Section 4.2,\nwe investigate the performance of HoneyBee on\nMatSci-NLP, a broad benchmark of materials sci-\nence NLP tasks (Song et al., 2023). We study Hon-\neyBee’s performance under two settings: 1. Low-\ndata training setting as applied in the original paper\nby Song et al. (2023); 2. Zero-shot performance\n5730\nTable 3: Low-resource finetuning and zero-shot evaluation results for various HoneyBee on MatSci-NLP tasks.\nFor low-resource finetuning, we follow the method described in Song et al. (2023). HoneyBee outperforms all\nmodels across the vast majority of tasks for both low-resource finetuning and zero-shot settings. MatSci-Instruct’s\nProgressive-Refinement-Feedback method improves HoneyBee’s performance for each consecutive stage. We report\nmacro-F1 (top) and micro-F1 (bottom) scores highlighting the best, second-best and third-best performing LLM.\nHoney-7b and HoneyBee-13b outperform both ChatGPT and Claude and are generally competitive with GPT-4.\nModel Named Entity\nRecognition\nRelation\nExtraction\nEvent Argument\nExtraction\nParagraph\nClassification\nSynthesis Action\nRetrieval\nSentence\nClassification\nSlot\nFilling\nOverall\n(All Tasks)\nLow-Resource Finetuning on MatSci-NLP\nMatSciBERT\n(Gupta et al., 2022)\n0.707\n0.470\n0.791\n0.507\n0.436\n0.251\n0.719\n0.623\n0.692\n0.484\n0.914\n0.660\n0.436\n0.194\n0.671\n0.456\nMatBERT\n(Walker et al., 2021)\n0.875\n0.630\n0.804\n0.513\n0.451\n0.288\n0.756\n0.691\n0.717\n0.594\n0.909\n0.614\n0.548\n0.273\n0.722\n0.517\nHoneyBee-7b 0.787\n0.644\n0.852\n0.518\n0.551\n0.389\n0.741\n0.641\n0.792\n0.617\n0.991\n0.711\n0.529\n0.391\n0.749\n0.559\nHoneyBee-13b 0.860\n0.748\n0.921\n0.578\n0.653\n0.486\n0.761\n0.658\n0.853\n0.662\n0.998\n0.743\n0.554\n0.401\n0.80\n0.611\nZero-Shot LLM Performance\nLLaMA-7b\n(Touvron et al., 2023)\n0.042\n0.064\n0.094\n0.013\n0.160\n0.042\n0.279\n0.218\n0.052\n0.013\n0.096\n0.087\n0.142\n0.010\n0.208\n0.064\nLLaMA-13b\n(Touvron et al., 2023)\n0.057\n0.066\n0.109\n0.016\n0.042\n0.054\n0.233\n0.189\n0.039\n0.009\n0.079\n0.074\n0.138\n0.008\n0.1\n0.059\nAlpaca-7b\n(Taori et al., 2023)\n0.031\n0.018\n0.053\n0.037\n0.029\n0.009\n0.375\n0.294\n0.179\n0.129\n0.180\n0.180\n0.139\n0.039\n0.141\n0.101\nAlpaca-13b\n(Taori et al., 2023)\n0.053\n0.046\n0.016\n0.035\n0.111\n0.072\n0.310\n0.237\n0.442\n0.278\n0.375\n0.334\n0.110\n0.015\n0.202\n0.145\nChat-GPT\n(OpenAI, 2022)\n0.063\n0.052\n0.232\n0.145\n0.204\n0.203\n0.433\n0.450\n0.300\n0.183\n0.320\n0.318\n0.368\n0.280\n0.274\n0.233\nClaude\n(Bai et al., 2022)\n0.063\n0.048\n0.232\n0.143\n0.195\n0.169\n0.442\n0.467\n0.280\n0.177\n0.329\n0.326\n0.393\n0.305\n0.276\n0.234\nGPT-4\n(OpenAI, 2023)\n0.189\n0.121\n0.445\n0.432\n0.453\n0.353\n0.679\n0.522\n0.743\n0.677\n0.788\n0.689\n0.502\n0.483\n0.543\n0.468\nZero-Shot HoneyBee with MatSci-Instruct\nHoneyBee-7b-Stage10.173\n0.148\n0.138\n0.120\n0.196\n0.096\n0.380\n0.207\n0.592\n0.208\n0.416\n0.334\n0.292\n0.105\n0.301\n0.174\nHoneyBee-7b-Stage20.243\n0.166\n0.199\n0.145\n0.237\n0.123\n0.440\n0.301\n0.612\n0.289\n0.467\n0.345\n0.344\n0.176\n0.363\n0.221\nHoneyBee-7b-Stage30.267\n0.190\n0.245\n0.178\n0.290\n0.189\n0.490\n0.343\n0.688\n0.342\n0.490\n0.365\n0.393\n0.289\n0.409\n0.271\nHoneyBee-13b-Stage10.369\n0.256\n0.301\n0.224\n0.389\n0.265\n0.500\n0.379\n0.701\n0.378\n0.512\n0.402\n0.467\n0.334\n0.463\n0.320\nHoneyBee-13b-Stage20.391\n0.299\n0.367\n0.290\n0.437\n0.303\n0.576\n0.411\n0.765\n0.401\n0.557\n0.461\n0.508\n0.379\n0.514\n0.363\nHoneyBee-13b-Stage30.429\n0.372\n0.412\n0.346\n0.481\n0.378\n0.611\n0.467\n0.801\n0.429\n0.589\n0.503\n0.578\n0.423\n0.557\n0.417\non MatSci-NLP tasks shown in Table 3. MatSci-\nNLP contains a wide range of text data related to\nmaterial science that spans a wide range of NLP\ntasks and types of materials, including but not lim-\nited to fuel cells, inorganic materials, glasses, and\nsuperconductors. For evaluation on MatSci-NLP,\nwe follow the same convention as in Song et al.\n(2023) where we report both macro-F1 and micro-\nF1 scores in Table 3.\nLow-Resource Finetuning: In our experiments,\nwe follow the same low-resource procedure de-\nscribed in (Song et al., 2023) by splitting the data\nin MatSci-NLP into 1% training subset and a 99%\ntest subset for evaluation. Subsequently, all models,\nincluding HoneyBee, are finetuned on the training\nsubset and evaluated on the test subset of MatSci-\nNLP data. The results on low-resource finetun-\ning in Table 3 show that both HoneyBee-7b and\nHoneyBee-13b perform best overall, outperform-\n5731\ning MatBERT (Walker et al., 2021) and MatSciB-\nERT (Gupta et al., 2022) among all tasks in MatSci-\nNLP with the exception of named entity recogni-\ntion. MatBERT and MatSci-BERT are both BERT\nmodels pretrained on different corpora of materials\nscience textual data. While the domain-specific\npretraining significantly boosts the score of both\nmodels for MatSci-NLP tasks, HoneyBee shows\nbetter performance without requiring pretraining\non materials science textual data. This is a signifi-\ncant advantage of HoneyBee and MatSci-Instruct\ngiven that large, high-quality corpora of materi-\nals science text are generally difficult to obtain as\ndescribed in Section 2.\nZero-Shot Performance: The zero-shot perfor-\nmance results, where we assess performance di-\nrectly on the test subset, in the lower part of Table 3\nshow that HoneyBee outperforms both LLaMa and\nAlpaca models. Notably, HoneyBee-7b-Stage1,\nwhich corresponds to only one round of MatSci-\nInstruct, outperforms both LLaMa and Alpaca\nmodels for equal (7b) and larger (13b) parame-\nter sizes. The data in Table 3 further confirms the\nresults from Table 2 that show progressive improve-\nment with each stage of MatSci-Instruct where\nboth HoneyBee-7 and Honey13b exhibit clear im-\nprovement in iterative stages. We also observe\nthat model parameter size matters for zero-shot\nperformance with 13b parameter models outper-\nforming 7b for HoneyBee and Alpaca, both of\nwhich are instruction finetuned models. Interest-\ningly, LLaMA-7b generally outperforms LLaMa-\n13b across most MatSci-NLP tasks and in the over-\nall score on MatSci-NLP.\n4.4 HoneyBee — Case Study\nWe perform a case study to further understand the\ncapabilities and limitations of the various LLMs\nwe studied, including HoneyBee, Alpaca, and Chat-\nGPT. Our case study results, with full data and text\nincluded in Appendix D, show that HoneyBee-13b\ngenerally produces outputs of the same quality as\nChat-GPT while other models generally produce\nlower quality outputs. This provides additional\nweight to the results in Section 4.1 indicating that\nHoneyBee-13b can match the quality of Chat-GPT\nafter multiple rounds of progressive refinement-\nfeedback finetuning using MatSci-Instruct.\n5 Conclusion\nIn this work, we introduce MatSci-Instruct, an it-\nerative instruction generation method for materials\nscience, and HoneyBee, a state-of-the-art large lan-\nguage model for materials science. To the best\nof our knowledge, HoneyBee is the first billion-\nparameter scale language model that is special-\nized in materials science. HoneyBee outperforms\ncurrent state-of-the-art general language models\n(LLaMa, Alpaca) and materials science BERT-\nbased language models (MatBERT, MatSciBERT)\nin various materials science NLP tasks, and note\nHoneyBee’s performance improvement with each\nsuccessive MatSci-Instruct stage. MatSci-Instruct\nprovides a valuable framework for generating\ninstruction-data to progressively finetune LLMs\nwhere instruction-data from an Instructor are veri-\nfied by a Verifier before being used for finetuning.\nAdditionally, poor instruction-data is refined based\non feedback from an Evaluator leading to higher\nquality instruction-data and model performance for\nthe desired specialization as shown by the results\nin Section 4. Future work remains in augmenting\nmaterials science LLMs with external knowledge,\nsuch as known scientific facts, which can further\nimprove an LLM’s reliability and interpretability.\nLimitations\nWhile HoneyBee outperforms current state-of-the-\nart methods in various materials science NLP tasks,\nit remains unclear how well HoneyBee would gen-\neralize the tasks outside of the MatSci-NLP bench-\nmark and MatSci-Instruct instruction-data to solve\ncomplex materials science challenges. Such chal-\nlenges may include creating a synthesis recipe for\na new materials or explaining the behavior of a ma-\nterials system based on fundamental scientific con-\ncepts. Materials science remains a wide-ranging\nand complex field with many open questions re-\nmaining on the true capability of HoneyBee and\nother LLMs to understand important materials sci-\nence concepts. MatSci-Instruct also relies on the\navailability of highly performant LLMs to serve as\nthe Instructor, Verifier and Evaluator which can be\nlimited in their own capabilities. Furthermore, our\nwork focuses primarily on the materials science do-\nmain and further studies are needed to understand\nhow applicable our methods would be to additional\nscientific domains.\n5732\nBroader Impact\nBoth HoneyBee and MatSci-Instruct can help pro-\nmote research on NLP for material science in apply-\ning, training and evaluating LLMs for practical ap-\nplications in the field. The general frameworks de-\nscribed in this paper can also be transferred to other\nscientific domain, such biology, physics and chem-\nistry, where trustworthy textual data is required.\nOur research does not raise major ethical con-\ncerns.\nAcknowlegments\nThis work is supported by the Mila internal funding\n- Program P2-V1: Industry Sponsored Academic\nLabs (project number: 10379), the Canada CIFAR\nAI Chair Program, and the Canada NSERC Discov-\nery Grant (RGPIN-2021-03115).\nReferences\nYuntao Bai, Saurav Kadavath, Sandipan Kundu,\nAmanda Askell, Jackson Kernion, Andy Jones, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Cameron\nMcKinnon, Carol Chen, Catherine Olsson, Christo-\npher Olah, Danny Hernandez, Dawn Drain, Deep\nGanguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,\nJamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua\nLandau, Kamal Ndousse, Kamile Lukosuite, Liane\nLovitt, Michael Sellitto, Nelson Elhage, Nicholas\nSchiefer, Noemi Mercado, Nova DasSarma, Robert\nLasenby, Robin Larson, Sam Ringer, Scott John-\nston, Shauna Kravec, Sheer El Showk, Stanislav Fort,\nTamera Lanham, Timothy Telleen-Lawton, Tom Con-\nerly, Tom Henighan, Tristan Hume, Samuel R. Bow-\nman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,\nNicholas Joseph, Sam McCandlish, Tom Brown, and\nJared Kaplan. 2022. Constitutional ai: Harmlessness\nfrom ai feedback.\nDaniil A Boiko, Robert MacKnight, and Gabe Gomes.\n2023. Emergent autonomous scientific research ca-\npabilities of large language models. arXiv preprint\narXiv:2304.05332.\nAndres M Bran, Sam Cox, Andrew D White, and\nPhilippe Schwaller. 2023. Chemcrow: Augmenting\nlarge-language models with chemistry tools. arXiv\npreprint arXiv:2304.05376.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nXiang Gao, Rong Tan, and Guanghui Li. 2020. Re-\nsearch on text mining of material science based on\nnatural language processing. In IOP conference se-\nries: materials science and engineering, volume 768,\npage 072094. IOP Publishing.\nTanishq Gupta, Mohd Zaki, Devanshi Khatsuriya,\nKausik Hira, N M Anoop Krishnan, and Mausam .\n2023. DiSCoMaT: Distantly supervised composition\nextraction from tables in materials science articles.\nIn Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), pages 13465–13483, Toronto, Canada.\nAssociation for Computational Linguistics.\nTanishq Gupta, Mohd Zaki, NM Krishnan, et al. 2022.\nMatscibert: A materials domain language model for\ntext mining and information extraction. npj Compu-\ntational Materials, 8(1):1–11.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\nlarge language models.\nShu Huang and Jacqueline M Cole. 2022. Batterybert:\nA pretrained language model for battery database\nenhancement. Journal of Chemical Information and\nModeling.\nOlga Kononova, Tanjin He, Haoyan Huo, Amalie Tre-\nwartha, Elsa A Olivetti, and Gerbrand Ceder. 2021.\n5733\nOpportunities and challenges of text mining in mate-\nrials research. Iscience, 24(3):102155.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582–\n4597, Online. Association for Computational Lin-\nguistics.\nYunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, and\nYou Zhang. 2023. Chatdoctor: A medical chat model\nfine-tuned on llama model using medical domain\nknowledge.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt\nunderstands, too.\nSourab Mangrulkar, Sylvain Gugger, Lysandre Debut,\nYounes Belkada, and Sayak Paul. 2022. Peft: State-\nof-the-art parameter-efficient fine-tuning methods.\nhttps://github.com/huggingface/peft.\nElsa A Olivetti, Jacqueline M Cole, Edward Kim, Olga\nKononova, Gerbrand Ceder, Thomas Yong-Jin Han,\nand Anna M Hiszpanski. 2020. Data-driven materials\nresearch enabled by natural language processing and\ninformation extraction. Applied Physics Reviews ,\n7(4):041317.\nOpenAI. 2022. openaiintroducingchatgpt. https:\n//openai.com/blog/chatgpt. [Accessed 22-Jun-\n2023].\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, Anto-\nnia Creswell, Nat McAleese, Amy Wu, Erich Elsen,\nSiddhant Jayakumar, Elena Buchatskaya, David Bud-\nden, Esme Sutherland, Karen Simonyan, Michela Pa-\nganini, Laurent Sifre, Lena Martens, Xiang Lorraine\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou,\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\npoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-\ntiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,\nDaniel Toyama, Cyprien de Masson d’Autume, Yujia\nLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\nAidan Clark, Diego de Las Casas, Aurelia Guy,\nChris Jones, James Bradbury, Matthew Johnson,\nBlake Hechtman, Laura Weidinger, Iason Gabriel,\nWilliam Isaac, Ed Lockhart, Simon Osindero, Laura\nRimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,\nJeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko-\nray Kavukcuoglu, and Geoffrey Irving. 2022. Scaling\nlanguage models: Methods, analysis & insights from\ntraining gopher.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nYu Song, Santiago Miret, and Bang Liu. 2023. Matsci-\nnlp: Evaluating scientific language models on ma-\nterials science language tasks using text-to-schema\nmodeling. arXiv preprint arXiv:2305.08264.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nNicholas Walker, Amalie Trewartha, Haoyan Huo,\nSanghoon Lee, Kevin Cruse, John Dagdelen, Alexan-\nder Dunn, Kristin Persson, Gerbrand Ceder, and\nAnubhav Jain. 2021. The impact of domain-specific\npre-training on named entity recognition tasks in ma-\nterials science. Available at SSRN 3950755.\nHaochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang,\nSendong Zhao, Bing Qin, and Ting Liu. 2023. Hu-\natuo: Tuning llama model with chinese medical\nknowledge.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\nguage model with self generated instructions. arXiv\npreprint arXiv:2212.10560.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan\nMa, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng\nZhang, Yuxiao Dong, and Jie Tang. 2022. Glm-130b:\nAn open bilingual pre-trained model.\n5734\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\n5735\nAppendix\nA Experimental Details\nWe finetune LLaMA models with 7B and 13B pa-\nrameters using instructions from MatSci-Instruct.\nAs shown in Table 2, we analyze the effect of var-\nious rounds of iterative instruction feedback. For\nfinetuning, we use the AdamW optimizer with an\ninitial learning rate of 1e-4 on 2 A100 GPUs for\nLLaMa-7b and 4 A100 GPUs for LLaMa-13b. We\nassign a batch size of 4 to each GPU with a gradient\naccumulation step of 32 and a maximum sequence\nlength of 2048.\nB Instruction Generation Details\nTable 4: MatSci-Instruct samples a diverse set of mate-\nrials science topic areas.\nMatSci-Instruct Topics\nBio-inspired Materials 221\nSelf-Healing Materials 209\nMagnetic Materials 195\nSmart Materials 190\nMetals 189\nSemiconductors 188\nCarbon Nanotubes 184\nPolymers 182\nThermoelectric Materials 180\nOptical Materials 180\nSuperconductors 179\nGraphene 177\nGlass 174\nEnergy Materials 166\nComposites 165\nElectronic Materials 163\nConstruction Materials 158\nCeramics 155\nNanoaterials 153\nbiomaterials 149\nThe full list of materials science topics and NLP\ntasks sampled for MatSci-Instruct instruction are\nincluded in Table 4 and Table 5. We sample a broad\nrange of materials science topics and NLP that are\ngenerally balanced yielding a set of instructions\nthat includes specialized materials science text as\nwell as general language capabilities.\nTable 5: MatSci-Instruct samples a diverse set of NLP\ntasks to generate instructions including general NLP\ntasks to main general language capabilities.\nMatSci-Instruct NLP Tasks\nMachine Reading Comprehension 224\nQuestion Answering 224\nOpen-Ended Generation 214\nClassification 203\nInformation Extraction 201\nRelation Extraction 193\nAnalysis 189\nTopic Modeling 188\nWriting 180\nCommonsense Reasoning 172\nCode Interpretation 172\nEvent Extraction 167\nGrammar Correction 165\nClustertin 162\nNamed Entity Recognition 160\nText Simplification 153\nSummarization 149\nSentiment Analysis 149\nPart-of-Speech Tagging 146\nEditing 145\nC Quality of Instructions for each\nMatSci-Instruct Stage\nThe preset threshold for filtering low-quality data\nis an average score of 95 or above for all four di-\nmensions and a score of no less than 90 for each\nindividual dimension. As shown in Table 6, after\nfiltering, the remaining instruction data exhibits sig-\nnificantly improved quality, approaching full marks\nmaking the data significantly more trustworthy for\nfinetuning specialized LLMs.\nD LLM Case Study\nThis section provides the details for the case study\nin Section 4.4.\nD.1 Open-ended Instruction\nWe analyze the response of different LLMs to the\nfollowing open-ended query: Analyze the proper-\nties and applications of electronic materials.\n• Chat-GPT: Electronic materials are a class of\nmaterials that possess unique properties that\nmake them suitable for use in electronic de-\nvices and technologies. These materials are\ncharacterized by their electrical conductivity,\n5736\nTable 6: Claude evaluation scores of MatSci-Instruct\nbefore and after removing low-quality instruction data\nScore/Stage 1 2 3\navg. Accuracy before 92.45 92.20 92.56\nafter 100.0 100.0 100.0\navg. Relevance before 86.01 87.18 86.96\nafter 100.0 100.0 100.0\navg. Completeness before 88.80 86.13 89.33\nafter 99.98 99.99 99.99\navg. Reasonableness before 97.75 98.21 98.16\nafter 100.0 100.0 100.0\noptical properties, magnetic properties, and\nthermal properties. The electrical conductiv-\nity of electronic materials is a critical prop-\nerty for materials used in electronic devices\nsuch as transistors, diodes, and integrated cir-\ncuits. Semiconductors such as silicon and\ngermanium are widely used in microproces-\nsors and solar cells because of their unique\nelectrical properties. Optical properties are\nalso important for electronic materials used\nin displays, touch screens, and flat panel dis-\nplays. Optoelectronic materials like indium\ntin oxide (ITO) and gallium nitride (GaN) pos-\nsess unique optical properties that make them\nideal for use in these devices. Magnetic prop-\nerties are important for electronic materials\nused in magnetic sensors, hard drives, and\nmagnetic memory. Magnetic materials like\niron, cobalt, and nickel possess unique mag-\nnetic properties that make them ideal for use\nin these devices. Lastly, thermal properties\nare important for electronic materials used in\nhigh-temperature electronic devices such as\nmicroprocessors and power electronics. Ma-\nterials like diamond and silicon carbide pos-\nsess unique thermal properties that make them\nideal for use in these devices.\n• HoneyBee-13b: Electronic materials have\nunique properties such as electrical conduc-\ntivity, optical properties, magnetic properties,\nand thermal properties, making them suitable\nfor use in electronic devices. Semiconductors\nsuch as silicon and germanium are used in\nmicroprocessors and solar cells due to their\nunique electrical properties. Optoelectronic\nmaterials like indium tin oxide (ITO) and gal-\nlium nitride (GaN) possess unique optical\nproperties, ideal for use in displays, touch\nscreens, and flat panel displays. Magnetic\nmaterials like iron, cobalt, and nickel are used\nin magnetic sensors, hard drives, and mag-\nnetic memory due to their unique magnetic\nproperties. Materials like diamond and sili-\ncon carbide possess unique thermal properties\nideal for use in high-temperature electronic de-\nvices. The properties of electronic materials\nare critical for modern electronic devices, and\nresearch is ongoing to discover new materials\nwith even more unique properties.\n• HoneyBee-7B: Electronic materials possess\nunique properties that make them suitable for\nvarious applications. They exhibit high elec-\ntrical conductivity, low resistivity, and excel-\nlent thermal conductivity. These materials\nare commonly used in electronic devices such\nas transistors, integrated circuits, and sensors.\nThey also find applications in optoelectronics,\nenergy storage devices, and flexible electron-\nics. The properties of electronic materials\nenable efficient charge transport and facilitate\nthe flow of electrons, making them essential\ncomponents in modern technology.\n• Alpaca-13b: Electronic materials are mate-\nrials that have electrical properties and can\nbe used in a variety of applications. These\nmaterials can be classified into two categories:\nconductors and insulators. Conductors are ma-\nterials that allow the flow of electricity, while\ninsulators are materials that do not allow the\nflow of electricity. Electronic materials can\nbe used in a variety of applications, such as in\nsemiconductors, transistors, and diodes. Semi-\nconductors are materials that have properties\nthat are intermediate between those of conduc-\ntors and insulators and are used in the manu-\nfacture of integrated circuits. Electronic mate-\nrials are also used in the manufacture of solar\ncells, which convert light into electricity.\n• Alpaca-7b: Electronic materials are materi-\nals that have the ability to control the flow of\nelectrons. These materials are used in a vari-\nety of applications, such as electronics, optics,\nand sensors. Examples of electronic materials\ninclude semiconductors, which are materials\nthat can be doped to control the flow of elec-\ntrons, and metallic materials, which are con-\n5737\nductors of electricity. Other examples include\nsuperconductors, which are materials that can\nconduct electricity with zero resistance, and\ndielectrics, which are materials that can store\nelectricity. Electronic materials are used in\na variety of applications, such as electronics,\noptics, and sensors.\nChatGPT is the most comprehensive and rele-\nvant answer as it provides a detailed analysis of\nthe properties and applications of electronic mate-\nrials, covering electrical conductivity, optical prop-\nerties, magnetic properties, and thermal properties.\nChat-GPT also gives specific examples of mate-\nrials and their applications in various electronic\ndevices. HoneyBee-13b is the second-best answer\nas it also covers the key properties of electronic ma-\nterials and their applications in various electronic\ndevices. HoneyBee-13b provides specific exam-\nples of materials and their unique properties that\nmake them ideal for use in different applications.\nHowever, HoneyBee-13b’s output lacks specificity\nand does not provide enough detail to fully ad-\ndress the question. HoneyBee-7b gives the third-\nbest answer as it gives a general overview of the\nunique properties of electronic materials and their\napplications in various electronic devices. How-\never, HoneyBee-7b’s output lacks specificity and\ndoes not provide enough detail. Alpaca-13b is the\nfourth-best answer as it provides a basic under-\nstanding of electronic materials and their use in\nvarious applications but lacks detail and specificity.\nAlpaca-7b is the least comprehensive and relevant\nanswer as it provides a limited understanding of\nelectronic materials and their use in various applica-\ntions. Alpaca-7b’s output does not provide enough\ndetail or specific examples to support its claims.\nD.2 Content-based Instruction\nNext, we analyze the response of different LLMs to\nthe following query: What are the exciton interac-\ntions responsible for? with the following <input>:\n“ Inspired by the recent experimental observation\nof strongly coupled polaritons in a Moire heter-\nobilayer, we study a model of dipole-interacting\nexcitons localized on sites of a lattice and coupled\nto planar cavity photons. We calculate the phase\ndiagram of this system by computing fluctuations\naround the mean field and determining the stability\nof the excitation spectrum. We find that the tran-\nsition from the normal state to a polariton conden-\nsate is intermediated by a series of ordered states at\npartial fillings of the exciton lattice, stabilized by\nthe exciton interactions. In particular we predict a\nsupersolid phase in which a polariton condensate\ncoexists with spatial order. ” The replies from the\ndifferent LLMs are below:\n• Chat-GPT: The exciton interactions are re-\nsponsible for stabilizing the ordered states at\npartial fillings of the exciton lattice, and in\nparticular for predicting a supersolid phase in\nwhich a polariton condensate coexists with\nspatial order.\n• HoneyBee-13b: The exciton interactions are\nresponsible for stabilizing the ordered states at\npartial fillings of the exciton lattice, which are\nintermediates between the normal state and a\npolariton condensate. In particular, they are\nresponsible for the predicted supersolid phase,\nwhere a polariton condensate coexists with\nspatial order.\n• HoneyBee-7b: The exciton interactions are\nresponsible for stabilizing the ordered phases\nthat mediate the transition to the polariton con-\ndensate, including the supersolid phase.\n• Alpaca-13b: The exciton interactions studied\nin the given text are responsible for the phase\ntransition from the normal state to a polariton\ncondensate, as well as the supersolid phase\n• Alpaca-7b: The exciton interactions are re-\nsponsible for the transition from the normal\nstate to a polariton condensate, as well as the\nstabilization of the supersolid phase\nThe answers from HoneyBee-13b and Chat-GPT\nare better compared to the outputs of the other mod-\nels. HoneyBee-13b and Chat-GPT directly address\nthe question by mentioning the ordered states at\npartial fillings of the exciton lattice, which are in-\ntermediates between the normal state and a polari-\nton condensate, and the predicted supersolid phase.\nThe answers also use language that closely matches\nthe language used in the original text, indicating a\ngood understanding of the material.\nE LLM Prompts\nIn this section we provide some of the prompts\nused for the different modules in MatSci-Instruct.\nWe plan to make the full list of prompts, data and\ncode available upon publication.\n5738\n• “Evaluate accuracy of the given text by com-\nparing with known facts or credible sources.\nThis involves checking the accuracy of any\nclaims or statements made in the text, and\nverifying that they are supported by evidence.\nThe next line directly provide the text. {out-\nput_text} Please return a score ranging from\n0 to 100, with 0 being the worst and 100 be-\ning the best. Please use the strictest grading\nstandard. The score should be in JSON format\nwith a field name of ’score’. You should not\noutput any other information or text.”\n• “Evaluate relevance of the given text by con-\nsidering how directly the text is related to ma-\nterials science. The next line directly pro-\nvide the text. {output_text} Please return a\nscore ranging from 0 to 100, with 0 being the\nworst and 100 being the best. Please use the\nstrictest grading standard. The score should\nbe in JSON format with a field name of ’score’.\nYou should not output any other information\nor text.”\n• “Evaluate completeness of the given text (in-\ncluding input, instruction and output) by as-\nsessing how fully the output addresses the\ninstruction, including all sub-questions. Con-\nsider both depth and conciseness. The next\n3 lines directly provide the input, instruction\nand output respectively. {input_text} {instruc-\ntion} {output_text} Please return a score rang-\ning from 0 to 100, with 0 being the worst and\n100 being the best. Please use the strictest\ngrading standard. The score should be in\nJSON format with a field name of ’score’. You\nshould not output any other information or\ntext.”\n• “Evaluate reasonableness of the given text by\nconsidering how logically consistent the con-\ntent is, with no obvious contradictions. The\nnext line directly provide text. {output_text}\nThe score should range from 0 to 100, with 0\nbeing the worst and 100 being the best. Please\nuse the strictest grading standard. The score\nshould be in JSON format with a field name\nof ’score’. You should not output any other\ninformation or text.”\n5739",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8834891319274902
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6793122887611389
    },
    {
      "name": "Construct (python library)",
      "score": 0.6008362770080566
    },
    {
      "name": "Trustworthiness",
      "score": 0.600644588470459
    },
    {
      "name": "Language model",
      "score": 0.5191894769668579
    },
    {
      "name": "Process (computing)",
      "score": 0.5127431154251099
    },
    {
      "name": "Soundness",
      "score": 0.47836583852767944
    },
    {
      "name": "Code (set theory)",
      "score": 0.4616737365722656
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.45666980743408203
    },
    {
      "name": "Artificial intelligence",
      "score": 0.433646023273468
    },
    {
      "name": "Machine learning",
      "score": 0.34972506761550903
    },
    {
      "name": "Data science",
      "score": 0.3287234604358673
    },
    {
      "name": "Programming language",
      "score": 0.2927331328392029
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I70931966",
      "name": "Université de Montréal",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I1343180700",
      "name": "Intel (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I151746483",
      "name": "University of Waterloo",
      "country": "CA"
    }
  ]
}