{
  "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
  "url": "https://openalex.org/W4385572634",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2106880482",
      "name": "Yizhong Wang",
      "affiliations": [
        "Johns Hopkins University Applied Physics Laboratory",
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A5037232402",
      "name": "Yeganeh Kordi",
      "affiliations": [
        "Johns Hopkins University",
        "Johns Hopkins University Applied Physics Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2170936942",
      "name": "Swaroop Mishra",
      "affiliations": [
        "Johns Hopkins University",
        "Johns Hopkins University Applied Physics Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A3179416558",
      "name": "Alisa Liu",
      "affiliations": [
        "Johns Hopkins University",
        "Johns Hopkins University Applied Physics Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2183947846",
      "name": "Noah A. Smith",
      "affiliations": [
        "Johns Hopkins University",
        "Johns Hopkins University Applied Physics Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2039793395",
      "name": "Daniel Khashabi",
      "affiliations": [
        "Johns Hopkins University",
        "Johns Hopkins University Applied Physics Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A91410043",
      "name": "Hannaneh Hajishirzi",
      "affiliations": [
        "Johns Hopkins University Applied Physics Laboratory",
        "Johns Hopkins University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4389524372",
    "https://openalex.org/W2805984364",
    "https://openalex.org/W4385574293",
    "https://openalex.org/W3207166518",
    "https://openalex.org/W4221151371",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4308613148",
    "https://openalex.org/W2976223659",
    "https://openalex.org/W4389523706",
    "https://openalex.org/W4385571260",
    "https://openalex.org/W3035160371",
    "https://openalex.org/W2963084773",
    "https://openalex.org/W4303649108",
    "https://openalex.org/W4385574313",
    "https://openalex.org/W4226103723",
    "https://openalex.org/W3103291112",
    "https://openalex.org/W3201090304",
    "https://openalex.org/W3168921237",
    "https://openalex.org/W4308244910",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W3100452485",
    "https://openalex.org/W3162734203",
    "https://openalex.org/W4385571789",
    "https://openalex.org/W4285178342",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2963643701",
    "https://openalex.org/W4302366816",
    "https://openalex.org/W4385571291",
    "https://openalex.org/W4385574298",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4308014717",
    "https://openalex.org/W4385570984",
    "https://openalex.org/W4309217888",
    "https://openalex.org/W4286905705",
    "https://openalex.org/W1821462560"
  ],
  "abstract": "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 13484‚Äì13508\nJuly 9-14, 2023 ¬©2023 Association for Computational Linguistics\nSELF -INSTRUCT : Aligning Language Models\nwith Self-Generated Instructions\nYizhong Wang‚ô£ Yeganeh Kordi‚ô¢ Swaroop Mishra‚ô° Alisa Liu‚ô£\nNoah A. Smith‚ô£+ Daniel Khashabi‚ô† Hannaneh Hajishirzi‚ô£+\n‚ô£University of Washington ‚ô¢Tehran Polytechnic ‚ô°Arizona State University\n‚ô†Johns Hopkins University +Allen Institute for AI\nyizhongw@cs.washington.edu\nAbstract\nLarge ‚Äúinstruction-tuned‚Äù language models\n(i.e., finetuned to respond to instructions) have\ndemonstrated a remarkable ability to general-\nize zero-shot to new tasks. Nevertheless, they\ndepend heavily on human-written instruction\ndata that is often limited in quantity, diversity,\nand creativity, therefore hindering the general-\nity of the tuned model. We introduce SELF -\nINSTRUCT , a framework for improving the\ninstruction-following capabilities of pretrained\nlanguage models by bootstrapping off their\nown generations. Our pipeline generates in-\nstructions, input, and output samples from a\nlanguage model, then filters invalid or similar\nones before using them to finetune the original\nmodel. Applying our method to the vanilla\nGPT3 , we demonstrate a 33% absolute im-\nprovement over the original model on SUPER -\nNATURAL INSTRUCTIONS , on par with the\nperformance of InstructGPT001,1 which was\ntrained with private user data and human anno-\ntations. For further evaluation, we curate a set\nof expert-written instructions for novel tasks,\nand show through human evaluation that tun-\ning GPT3 with SELF -INSTRUCT outperforms\nusing existing public instruction datasets by\na large margin, leaving only a 5% absolute\ngap behind InstructGPT001. SELF -INSTRUCT\nprovides an almost annotation-free method for\naligning pretrained language models with in-\nstructions, and we release our large synthetic\ndataset to facilitate future studies on instruction\ntuning.2\n1 Introduction\nThe recent NLP literature has witnessed a tremen-\ndous amount of activity in building models that can\n1Unless otherwise specified, our comparisons are with\ntext-davinci-001 engine. We focus on this engine since it\nis the closest to our experimental setup: supervised finetuning\nwith human demonstrations. The newer engines are more pow-\nerful, though use more data (e.g., code or latest user queries)\nor algorithms (e.g., PPO) that are difficult to compare with.\n2Code and data are available at https://github.com/\nyizhongw/self-instruct\nInstruction:Write a letter from the perspective of a cat.Input:NullOutput:Dear [Owner], I am writing to you today because I have aproblem. You see, I‚Äôve been living with you for the past[Number of Years] and I love it here. The food is great,the bed is comfy, and there are plenty of toys to playwith. But lately, I‚Äôve noticed that you‚Äôre not asaffectionate towards me as you used to be. In fact,sometimes when I try to cuddle up next to you on thecouch, you push me away...\nInstruction:I am looking for a job and Ineed to fill out an application form. Canyou please help me complete it?Input:Application Form:Name:_________Age:__________Sex:________Phone Number:______Email Address:________Education:___________________...Output:Name: John DoeAge: 25Sex: MalePhone Number: ...\nInstruction:How to write a code for converting degrees fahrenheitto celsius.Input:NullOutput:def convert_fahrenheit_to_celsius(fahr):celsius= (fahr-32) * 5 / 9return celsius\nInstruction:Given an address and city, come up withthe zip code.Input:Address: 123 Main Street, City: San FranciscoOutput:94105\nFigure 1: Selected tasks from the generated instruction\ndata using vanilla GPT3 . Some texts are reformatted\nfor presentation. See Table 10 for more examples.\nfollow natural language instructions (Mishra et al.,\n2022; Wei et al., 2022; Sanh et al., 2022; Wang\net al., 2022; Ouyang et al., 2022; Chung et al., 2022,\ni.a.). These developments are powered by two\nkey components: large pretrained language mod-\nels (LM) and human-written instruction data (e.g.,\nPROMPT SOURCE (Bach et al., 2022) and SUPER -\nNATURAL INSTRUCTIONS (Wang et al., 2022, SU-\nPER NI for short)). However, collecting such in-\nstruction data is costly and often suffers limited\ndiversity given that most human generations tend\nto be popular NLP tasks, falling short of cover-\ning a true variety of tasks and different ways to\n13484\n175seed tasks with1instructionand1instancepertaskTask PoolStep1:InstructionGeneration\nNo\nStep4:Filtering Output-first\nInput-first\nStep2:ClassificationTaskIdentification\nStep3:InstanceGeneration\nInstruction :Give me a quote from a famous person on this topic.Task\nYes\nTaskInstruction :Give me a quote from a famous person on this topic.Input:Topic: The importance of being honest.Output:\"Honesty is the first chapter in the book of wisdom.\" -Thomas Jefferson\nTaskTaskInstruction :Find out if the given text is in favor of or against abortion.Class Label: Pro-abortionInput:Text: I believe that women should have the right to choose whether or notthey want to have an abortion.\nTask\nLM LM\nLM\nü§ñ\n ü§ñ\nü§ñ\nFigure 2: A high-level overview of SELF -INSTRUCT . The process starts with a small seed set of tasks as the task\npool. Random tasks are sampled from the task pool, and used to prompt an off-the-shelf LM to generate both new\ninstructions and corresponding instances, followed by filtering low-quality or similar generations, and then added\nback to the initial repository of tasks. The resulting data can be used for the instruction tuning of the language model\nitself later to follow instructions better. Tasks shown in the figure are generated by GPT3.\ndescribe them. Continuing to improve the quality\nand coverage of instruction-tuned models necessi-\ntates the development of alternative approaches for\nsupervising the instruction tuning process.\nIn this work, we introduce SELF -INSTRUCT ,\na semi-automated process for instruction-tuning\na pretrained LM using instructional signals from\nthe model itself. The overall process is an itera-\ntive bootstrapping algorithm (see Figure 2), which\nstarts off with a limited (e.g., 175 in our study)\nseed set of manually-written tasks that are used to\nguide the overall generation. In the first phase, the\nmodel is prompted to generate instructions for new\ntasks. This step leverages the existing collection\nof instructions to create more broad-coverage in-\nstructions that define (often new) tasks. Given the\nnewly-generated set of instructions, the framework\nalso creates input-output instances for them, which\ncan be later used for supervising the instruction\ntuning. Finally, various heuristics are used to auto-\nmatically filter low-quality or repeated instructions,\nbefore adding the remaining valid tasks to the task\npool. This process can be repeated for many itera-\ntions until reaching a large number of tasks.\nTo evaluate SELF -INSTRUCT empirically, we\nrun this framework on GPT3 (Brown et al., 2020),\nwhich is a vanilla LM (¬ß3). The iterative SELF -\nINSTRUCT process on this model leads to about 52k\ninstructions, paired with about 82K instance inputs\nand target outputs. We observe that the resulting\ndata provides a diverse range of creative tasks, as\nis demonstrated by examples in Figure 1. These\ngenerated tasks deviate from the distribution of typ-\nical NLP tasks, and also have fairly small overlap\nwith the seed tasks (¬ß3.2). On this resulting data,\nwe build GPT3 SELF -INST by finetuning GPT3 (i.e.,\nthe same model used for generating the instruction\ndata). We evaluateGPT3 SELF -INST in comparison to\nvarious other models on both typical NLP tasks in-\ncluded in SUPER NI (Wang et al., 2022), and a set of\nnew instructions that are created for novel usage of\ninstruction-following models (¬ß4). The results in-\ndicate that GPT3 SELF -INST outperforms GPT3 (the\noriginal model) by a large margin (+33.1%) and\nnearly matches the performance of InstructGPT001.\nMoreover, our human evaluation on the newly-\ncreated instruction set shows that GPT3 SELF -INST\ndemonstrates a broad range of instruction follow-\ning ability, outperforming models trained on other\npublicly available instruction datasets and leaving\nonly a 5% gap behind InstructGPT001.\nIn summary, our contributions are: (1) we in-\ntroduce SELF -INSTRUCT , a method for induc-\ning instruction following capabilities with mini-\nmal human-labeled data; (2) we demonstrate its\neffectiveness via extensive instruction-tuning ex-\nperiments; and (3) we release a large synthetic\ndataset of 52K instructions and a set of manually-\nwritten novel tasks for building and evaluating fu-\nture instruction-following models.\n13485\n2 Method\nAnnotating large-scale instruction data can be chal-\nlenging for humans because it requires 1) creativity\nto come up with novel tasks and 2) expertise for\nwriting the solutions to each task. Here, we de-\ntail our process for SELF -INSTRUCT , which refers\nto the pipeline of generating tasks with a vanilla\npretrained language model itself, filtering the gen-\nerated data, and then conducting instruction tuning\nwith this generated data in order to align the LM to\nfollow instructions better. This pipeline is depicted\nin Figure 2.\n2.1 Defining Instruction Data\nThe instruction data we want to generate con-\ntains a set of instructions {It}, each of which\ndefines a task t in natural language. Task t has\nnt ‚â•1 input-output instances {(Xt,i,Yt,i)}nt\ni=1. A\nmodel M is expected to produce the output, given\nthe task instruction and the corresponding input:\nM(It,Xt,i) = Yt,i, for i‚àà{1,...,n t}. Note that\nthe instruction and instance input does not have a\nstrict boundary in many cases. For example, ‚Äúwrite\nan essay about school safety‚Äù can be a valid instruc-\ntion that we expect models to respond to directly,\nwhile it can also be formulated as ‚Äúwrite an essay\nabout the following topic‚Äù as the instruction, and\n‚Äúschool safety‚Äù as an instance input. To encourage\nthe diversity of the data format, we allow such in-\nstructions that do not require additional input (i.e.,\nX is empty).\n2.2 Automatic Instruction Data Generation\nOur pipeline for data generation consists of four\nsteps: 1) generating task instructions, 2) determin-\ning if the instruction represents a classification task,\n3) instance generation with either an input-first or\noutput-first approach, and 4) filtering low-quality\ndata.\nInstruction Generation. At the first step, SELF -\nINSTRUCT generates new instructions from a small\nset of seed human-written instructions in a boot-\nstrapping fashion. We initiate the task pool with\n175 tasks (1 instruction and 1 instance for each\ntask).3 For every step, we sample 8 task instruc-\ntions from this pool as in-context examples. Of\n3These tasks were newly written by the authors and their\nlabmates at UW, without reference to existing datasets or the\ntest set used in this work. We provide more details about these\ntasks and analyze their similarity to the test tasks in Appendix\n¬ßA.1.\nthe 8 instructions, 6 are from the human-written\ntasks, and 2 are from the model-generated tasks in\nprevious steps to promote diversity. The prompting\ntemplate is shown in Table 5.\nClassification Task Identification. Because we\nneed two different approaches for classification and\nnon-classification tasks, we next identify whether\nthe generated instruction represents a classification\ntask or not.4 We prompt the LM in a few-shot way\nto determine this, using 12 classification instruc-\ntions and 19 non-classification instructions from\nthe seed tasks. The prompting template is shown\nin Table 6.\nInstance Generation. Given the instructions and\ntheir task type, we generate instances for each in-\nstruction independently. This is challenging be-\ncause it requires the model to understand what the\ntarget task is, based on the instruction, figure out\nwhat additional input fields are needed and gener-\nate them, and finally complete the task by produc-\ning the output. We found that pretrained LMs can\nachieve this to a large extent when prompted with\ninstruction-input-output in-context examples from\nother tasks. A natural way to do this is the Input-\nfirst Approach, where we can ask an LM to come\nup with the input fields first based on the instruc-\ntion, and then produce the corresponding output.\nThis generation order is similar to how models are\nused to respond to instruction and input, but here\nwith in-context examples from other tasks. The\nprompting template is shown in Table 7.\nHowever, we found that this approach can gen-\nerate inputs biased toward one label, especially\nfor classification tasks (e.g., for grammar error de-\ntection, it usually generates grammatical input).\nTherefore, we additionally propose anOutput-first\nApproach for classification tasks, where we first\ngenerate the possible class labels, and then condi-\ntion the input generation on each class label. The\nprompting template is shown in Table 8.5 We ap-\nply the output-first approach to the classification\ntasks identified in the former step, and the input-\nfirst approach to the remaining non-classification\ntasks.\n4More concretely, we regard tasks that have a small limited\noutput label space as classification tasks.\n5In this work, we use a fixed set of seed tasks for prompt-\ning the instance generation, and thus only generate a small\nnumber of instances per task in one round. Future work can\nuse randomly sampled tasks to prompt the model to generate\na larger number of instances in multiple rounds.\n13486\nFiltering and Postprocessing. To encourage di-\nversity, a new instruction is added to the task pool\nonly when its ROUGE-L similarity with any exist-\ning instruction is less than 0.7. We also exclude\ninstructions that contain some specific keywords\n(e.g., image, picture, graph) that usually can not be\nprocessed by LMs. When generating new instances\nfor each instruction, we filter out instances that are\nexactly the same or those with the same input but\ndifferent outputs. Invalid generations are identified\nand filtered out based on heuristics (e.g., instruc-\ntion is too long or too short, instance output is a\nrepetition of the input).\n2.3 Finetuning the LM to Follow Instructions\nAfter creating large-scale instruction data, we use it\nto finetune the original LM (i.e., SELF -INSTRUCT ).\nTo do this, we concatenate the instruction and in-\nstance input as a prompt and train the model to\ngenerate the instance output in a standard super-\nvised way. To make the model robust to different\nformats, we use multiple templates to encode the\ninstruction and instance input together. For exam-\nple, the instruction can be prefixed with ‚ÄúTask:‚Äù or\nnot, the input can be prefixed with ‚ÄúInput:‚Äù or not,\n‚ÄúOutput:‚Äù can be appended at the end of the prompt\nor not, and different numbers of break lines can be\nput in the middle, etc.\n3 S ELF -INSTRUCT Data from GPT3\nIn this section, we apply our method for inducing\ninstruction data to GPT3 as a case study. We use\nthe largest GPT3 LM (‚Äúdavinci‚Äù engine) accessed\nthrough the OpenAI API.6 The parameters for mak-\ning queries are described in Appendix A.2. Here\nwe present an overview of the generated data.\n3.1 Statistics\nTable 1 describes the basic statistics of the gener-\nated data. We generate a total of over 52K instruc-\ntions and more than 82K instances corresponding\nto these instructions after filtering.\n3.2 Diversity\nTo study what types of instructions are generated\nand how diverse they are, we identify the verb-noun\nstructure in the generated instructions. We use the\nBerkeley Neural Parser7 (Kitaev and Klein, 2018;\nKitaev et al., 2019) to parse the instructions and\n6https://openai.com/api/\n7https://parser.kitaev.io/\nstatistic\n# of instructions 52,445\n- # of classification instructions 11,584\n- # of non-classification instructions 40,861\n# of instances 82,439\n- # of instances with empty input 35,878\nave. instruction length (in words) 15.9\nave. non-empty input length (in words) 12.7\nave. output length (in words) 18.9\nTable 1: Statistics of the generated data by applying\nSELF -INSTRUCT to GPT3.\nthen extract the verb that is closest to the root as\nwell as its first direct noun object. 26,559 out of the\n52,445 instructions contain such structure; other\ninstructions usually contain more complex clauses\n(e.g., ‚ÄúClassify whether this tweet contains political\ncontent or not.‚Äù) or are framed as questions (e.g.,\n‚ÄúWhich of these statements are true?‚Äù). We plot\nthe top 20 most common root verbs and their top 4\ndirect noun objects in Figure 3, which account for\n14% of the entire set. Overall, we see quite diverse\nintents and textual formats in these instructions.\nWe further study how the generated instructions\ndiffer from the seed instructions used to prompt\nthe generation. For each generated instruction, we\ncompute its highest ROUGE-L overlap with the\n175 seed instructions. We plot the distribution of\nthese ROUGE-L scores in Figure 4. The results\nindicate a decent number of new instructions were\ngenerated, which do not have much overlap with\nthe seeds. We also demonstrate diversity in the\nlength of the instructions, instance inputs, and in-\nstance outputs in Figure 5.\n3.3 Quality\nSo far, we have shown the quantity and diversity\nof the generated data, but its quality remains un-\ncertain. To investigate this, we randomly sample\n200 instructions and randomly select 1 instance\nper instruction. We asked an expert annotator (au-\nthor of this work) to label whether each instance\nis correct or not, in terms of the instruction, the\ninstance input, and the instance output. Evaluation\nresults in Table 2 show that most of the generated\ninstructions are meaningful, while the generated\ninstances may contain more noise (to a reasonable\nextent). However, we found that even though the\ngenerations may contain errors, most of them are\nstill in the correct format or partially correct, which\ncan provide useful guidance for training models to\nfollow instructions. We listed a number of good\n13487\nwrite give \nfind \ncreate make \nd e s c r i b e \nd e s i g n \ng e n e r a t e \nc l a s s i f y \nh a v e \nexplain t e l l \ni d e n t i f y \noutput \np r e d i c t \nd e t e c t \nfunction \nessay \nletter \nparagraph \nexample \nlist \nset \na d v i c e \nword \nnumber \ns e n t e n c e \nway \nprogram \nlist \na l g o r i t h m \nf u n c t i o n \nlist \n s t o r y \ns e n t e n c e \np r o g r a m \nsituation \np e r s o n \np r o c e s s \nt i m e \ns y s t e m \ng a m e \na l g o r i t h m \ns t r u c t u r e \nl i s t \nn u m b e r \ns e n t e n c e \ns e r i e s \ns e n t e n c e \ns e n t i m e n t \na r t i c l e \nt e x t \nl i s t \narray \nc o i n \ns e t \nd i f f e r e n c e \nc o n c e p t \ns t o r y \nj o k e \ns e n t i m e n t \nt o p i c \nn u m b e r \nw o r d \ns e n t i m e n t \ns a r c a s m \nFigure 3: The top 20 most common root verbs (inner circle) and\ntheir top 4 direct noun objects (outer circle) in the generated\ninstructions. Despite their diversity, the instructions shown here\nonly account for 14% of all the generated instructions because\nmany instructions (e.g., ‚ÄúClassify whether the user is satisfied\nwith the service.‚Äù) do not contain such a verb-noun structure.\n0 0.2 0.4 0.6 0.8 1 0 \n1000 \n2000 \n3000 \nROUGE-L Overlap with the Most Similar Seed Instruction\n# Instructions\nFigure 4: Distribution of the ROUGE-L scores\nbetween generated instructions and their most\nsimilar seed instructions.\n10 20 30 40 50 60 0 \n2000 \n4000 \n6000 \nInstruction Length\n# Instructions\n10 20 30 40 50 60 0 \n1000 \n2000 \n3000 \nInput Length\n# Inputs\n10 20 30 40 50 600\n10k\n20k\n30k\nOnput Length\n# Onputs\nFigure 5: Length distribution of the generated\ninstructions, non-empty inputs, and outputs.\nexamples and bad examples in Table 10 and 11,\nrespectively.\nQuality Review Question Yes %\nDoes the instruction\ndescribe a valid task? 92%\nIs the input appropriate\nfor the instruction? 79%\nIs the output a correct and acceptable\nresponse to the instruction and input? 58%\nAll fields are valid 54%\nTable 2: Data quality review for the instruction, input,\nand output of the generated data. See Table 10 and\nTable 11 for representative valid and invalid examples.\n4 Experimental Results\nWe conduct experiments to measure and compare\nthe performance of models under various instruc-\ntion tuning setups. We first describe our models\nand other baselines, followed by our experiments.\n4.1 GPT3 SELF -INST : finetuning GPT3 on its\nown instruction data\nGiven the instruction-generated instruction data,\nwe conduct instruction tuning with the GPT3\nmodel itself (‚Äúdavinci‚Äù engine). As described in\n¬ß2.3, we use various templates to concatenate the\ninstruction and input, and train the model to gen-\nerate the output. This finetuning is done through\nthe OpenAI finetuning API. 8 We use the default\nhyper-parameters, except that we set the prompt\nloss weight to 0, and we train the model for 2\nepochs. We refer the reader to Appendix A.3 for\nadditional finetuning details. The resulting model\nis denoted by GPT3 SELF -INST .\n4.2 Baselines\nOff-the-shelf LMs. We evaluate T5-LM (Lester\net al., 2021; Raffel et al., 2020) and GPT3 (Brown\net al., 2020) as the vanilla LM baselines (only pre-\ntraining, no additional finetuning). These baselines\nwill indicate the extent to which off-the-shelf LMs\nare capable of following instructions naturally im-\nmediately after pretraining.\nPublicly available instruction-tuned models.\nT0 and Tk-INSTRUCT are two instruction-tuned\nmodels proposed in Sanh et al. (2022) and Wang\net al. (2022), respectively, and are demonstrated\nto be able to follow instructions for many NLP\ntasks. Both of these models are finetuned from the\n8See OpenAI‚Äôs documentation on finetuning.\n13488\nT5 (Raffel et al., 2020) checkpoints and are pub-\nlicly available.9 For both of these models, we use\ntheir largest version with 11B parameters.\nInstruction-tuned GPT3 models. We evaluate\nInstructGPT (Ouyang et al., 2022), which is de-\nveloped by OpenAI based on GPT3 to follow hu-\nman instructions better and has been found by\nthe community to have impressive zero-shot abil-\nities. There are various generations of these mod-\nels, where newer ones use more expansive data\nor algorithmic novelties.10 For our SUPER NI ex-\nperiments in ¬ß4.3, we only compare with their\ntext-davinci-001 engine, because their newer\nengines are trained with the latest user data and are\nlikely to have already seen the SUPER NI test set.\nFor our human evaluation on newly written instruc-\ntions, we include their 001, 002 and 003 engines\nfor completeness.\nAdditionally, to compare SELF -INSTRUCT train-\ning with other publicly available instruction tuning\ndata, we further finetune GPT3 model with data\nfrom PROMPT SOURCE and SUPER NI, which are\nused to train the T0 and Tk-INSTRUCT models.\nWe call them T0 training and SUPER NI training\nfor short, respectively. To save the training bud-\nget, we sampled 50K instances (but covering all\ntheir instructions) for each dataset, which has a\ncomparable size to the instruction data we gener-\nated. Based on the findings from Wang et al. (2022)\nand our early experiments, reducing the number of\ninstances per task does not degrade the model‚Äôs\ngeneralization performance to unseen tasks.\n4.3 Experiment 1: Zero-Shot Generalization\non SUPER NI benchmark\nWe first evaluate the models‚Äô ability to follow in-\nstructions on typical NLP tasks in a zero-shot fash-\nion. We use the evaluation set of SUPER NI (Wang\net al., 2022), which consists of 119 tasks with 100\ninstances in each task. In this work, we mainly\nfocus on the zero-shot setup, i.e., the model is\nprompted with the definition of the tasks only, with-\nout in-context demonstration examples. For all our\nrequests to the GPT3 variants, we use the deter-\nministic generation mode (temperature as 0 and no\nnucleus sampling) without specific stop sequences.\nResults. We make the following observations\nfrom the results in Table 3. SELF -INSTRUCT\n9T0 is available at here and Tk-INSTRUCT is here.\n10See OpenAI‚Äôs documentation on their models.\nModel # Params ROUGE-L\nVanilla LMs\nT5-LM 11B 25.7\nGPT3 175B 6.8\nInstruction-tuned w/o SUPERNI\nT0 11B 33.1\nGPT3 + T0 Training 175B 37.9\nGPT3SELF-INST (Ours) 175B 39.9\nInstructGPT001 175B 40.8\nInstruction-tuned w/ SUPERNI\nTk-INSTRUCT 11B 46.0\nGPT3 + SUPERNI Training 175B 49.5\nGPT3SELF-INST + SUPERNI Training (Ours) 175B 51.6\n1‚Éù\n2‚Éù\n3‚Éù\nTable 3: Evaluation results on unseen tasks from SU-\nPER NI (¬ß4.3). From the results, we see that 1‚ÉùSELF -\nINSTRUCT can boost GPT3 performance by a large mar-\ngin (+33.1%) and 2‚Éùnearly matches the performance of\nInstructGPT001. Additionally, 3‚Éùit can further improve\nthe performance even when a large amount of labeled\ninstruction data is present.\nboosts the instruction-following ability ofGPT3 by\na large margin. The vanilla GPT3 model basically\ncannot follow human instructions at all. Upon man-\nual analysis, we find that it usually generates irrele-\nvant and repetitive text, and does not know when\nto stop generation. Compared with other mod-\nels that are not specifically trained for SUPER NI,\nGPT3 SELF -INST achieves better performance than\nT0 or the GPT3 finetuned on the T0 training set,\nwhich takes tremendous human labeling efforts.\nNotably, GPT3 SELF -INST also nearly matches the\nperformance of InstructGPT001, which is trained\nwith private user data and human-annotated labels.\nModels trained on the SUPER NI training set still\nachieve better performance on its evaluation set,\nwhich we attribute to the similar instruction style\nand formatting. However, we show that SELF -\nINSTRUCT still brings in additional gains when\ncombined with the SUPER NI training set, proving\nits value as complementary data.\n4.4 Experiment 2: Generalization to\nUser-oriented Instructions on Novel Tasks\nDespite the comprehensiveness of SUPER NI in col-\nlecting existing NLP tasks, most of these NLP tasks\nwere proposed for research purposes and skewed\ntoward classification. To better access the practi-\ncal value of instruction-following models, a sub-\nset of the authors curate a new set of instructions\nmotivated by user-oriented applications. We first\nbrainstorm various domains where large LMs may\nbe useful (e.g., email writing, social media, pro-\nductivity tools, entertainment, programming), then\n13489\nVanillaGPT3GPT3+T0TrainingGPT3+SuperNITrainingGPT3Self-Inst+SuperNIGPT3Self-InstInstructGPT001InstructGPT002InstructGPT003\n187\n118\n68\n31 25 18 10 2\n64\n59\n80\n84\n66 61\n34 28\n1\n31\n30 54\n49\n45\n40\n30\n0\n44 74 83 112 128 168 192\n0%\n25%\n50%\n75%\n100%\nGPT3\nT0 Training\nSuperNI Training\nSelf-Instruct + SuperNI\nSelf-InstructInstructGPT-001InstructGPT-002InstructGPT-003\ncorrect and satisfying responseacceptable response with minor imperfections\nresponds to the instruction but has significant errors  irrelevant or invalid response\n186\n117\n68\n31 25 18 10 2\n64\n59\n80\n84\n66 61\n34 28\n1\n31\n30 54\n48\n44\n39\n30\n0\n44 74 83 113 129 169 192\n0%\n25%\n50%\n75%\n100%\nGPT3\nT0 Training\nSuperNI Training\nSelf-Instruct + SuperNI\nSelf-InstructInstructGPT-001InstructGPT-002InstructGPT-003\ncorrect and satisfying responseacceptable response with minor imperfections\nresponds to the instruction but has significant errors  irrelevant or invalid responseC:respondstotheinstructionbuthassignificanterrorsA:correctandsatisfyingresponse\n186\n117\n68\n31 25 18 10 2\n64\n59\n80\n84\n66 61\n34 28\n1\n31\n30 54\n48\n44\n39\n30\n0\n44 74 83 113 129 169 192\n0%\n25%\n50%\n75%\n100%\nGPT3\nT0 Training\nSuperNI Training\nSelf-Instruct + SuperNI\nSelf-InstructInstructGPT-001InstructGPT-002InstructGPT-003\ncorrect and satisfying responseacceptable response with minor imperfections\nresponds to the instruction but has significant errors  irrelevant or invalid response\nB:acceptableresponsewithminorimperfections\n186\n117\n68\n31 25 18 10 2\n64\n59\n80\n84\n66 61\n34 28\n1\n31\n30 54\n48\n44\n39\n30\n0\n44 74 83 113 129 169 192\n0%\n25%\n50%\n75%\n100%\nGPT3\nT0 Training\nSuperNI Training\nSelf-Instruct + SuperNI\nSelf-InstructInstructGPT-001InstructGPT-002InstructGPT-003\ncorrect and satisfying responseacceptable response with minor imperfections\nresponds to the instruction but has significant errors  irrelevant or invalid responseD:irrelevantorinvalidresponse\n186\n117\n68\n31 25 18 10 2\n64\n59\n80\n84\n66 61\n34 28\n1\n31\n30 54\n48\n44\n39\n30\n0\n44 74 83 113 129 169 192\n0%\n25%\n50%\n75%\n100%\nGPT3\nT0 Training\nSuperNI Training\nSelf-Instruct + SuperNI\nSelf-InstructInstructGPT-001InstructGPT-002InstructGPT-003\ncorrect and satisfying responseacceptable response with minor imperfections\nresponds to the instruction but has significant errors  irrelevant or invalid response\n186\n117\n68\n31 25 18 10 2\n64\n59\n80\n84\n66 61\n34 28\n1\n31\n30 54\n49\n45\n40\n30\n0\n44 74 83 112 128 168 192\n0%\n25%\n50%\n75%\n100%\nGPT3\nT0 Training\nSuperNI Training\nSelf-Instruct + SuperNI\nSelf-InstructInstructGPT-001InstructGPT-002InstructGPT-003\ncorrect and satisfying responseacceptable response with minor imperfections\nresponds to the instruction but has significant errors  irrelevant or invalid response\nFigure 6: Performance of GPT3 model and its instruction-tuned variants, evaluated by human experts on our 252\nuser-oriented instructions (¬ß4.4). Human evaluators are instructed to rate the models‚Äô responses into four levels. The\nresults indicate that GPT3 SELF -INST outperforms all the other GPT3 variants trained on publicly available instruction\ndatasets. Additionally, GPT3 SELF -INST scores nearly as good as InstructGPT001 (cf. footnote 1).\ncraft instructions related to each domain along with\nan input-output instance (again, input is optional).\nWe aim to diversify the styles and formats of these\ntasks (e.g., instructions may be long or short; in-\nput/output may take the form of bullet points, ta-\nbles, codes, equations, etc.). In total, we create 252\ninstructions with 1 instance per instruction. We\nbelieve it can serve as a testbed for evaluating how\ninstruction-based models handle diverse and unfa-\nmiliar instructions. Table 9 presents a small portion\nof them. The entire set is available in our GitHub\nrepository. We analyze the overlap between this set\nset and the seed instructions in ¬ßA.1.\nHuman evaluation setup. Evaluating models‚Äô\nperformance on this evaluation set of diverse tasks\nis extremely challenging because different tasks\nrequire different expertise. Indeed, many of these\ntasks cannot be measured by automatic metrics or\neven be judged by normal crowdworkers (e.g., writ-\ning a program or converting first-order logic into\nnatural language). To get a more faithful evalua-\ntion, we asked the authors of the instructions to\njudge model predictions. Details on how we set up\nthis human evaluation are described in Appendix B.\nThe evaluators were asked to rate the output based\non whether it accurately and effectively completes\nthe task. We implemented a four-level rating sys-\ntem for categorizing the quality of models‚Äô outputs:\n‚Ä¢ RATING -A: The response is valid and satisfying.\n‚Ä¢ RATING -B: The response is acceptable but has\nminor errors or imperfections.\n‚Ä¢ RATING -C: The response is relevant and re-\nsponds to the instruction, but it has significant\nerrors in the content. For example, GPT3 might\ngenerate a valid output first, but continue to gen-\nerate other irrelevant things.\n‚Ä¢ RATING -D: The response is irrelevant or com-\npletely invalid.\nResults. Figure 6 shows the performance of\nGPT3 model and its instruction-tuned counterparts\non this newly written instruction set (w. inter-\nrater agreement Œ∫ = 0 .57 on the 4-class cate-\ngorical scale, see Appendix B for details). As\nanticipated, the vanilla GPT3 LM is largely un-\nable to respond to instructions, and all instruction-\ntuned models demonstrate comparatively higher\nperformance. Nonetheless, GPT3 SELF -INST (i.e.,\nGPT3 model finetuned with SELF -INSTRUCT )\noutperforms those counterparts trained on T0 or\nSUPER NI data by a large margin, demonstrating\nthe value of the generated data despite the noise.\nCompared with InstructGPT001, GPT3 SELF -INST is\nquite close in performance‚Äîif we count accept-\nable response with minor imperfections (RATING -\nB) as valid, GPT3 SELF -INST is only 5% behind\nInstructGPT001. Lastly, our evaluation confirms\nthe impressive instruction-following ability of\nInstructGPT002 and InstructGPT003. Although\nthere are many factors behind this success, we con-\njecture that future work can largely benefit from\nimproving the quality of our generated data by us-\ning human annotators or training a reward model to\nselect better generations, similar to the algorithm\nused by Ouyang et al. (2022).\n13490\n4.5 Effect of Data Size and Quality\nData size. SELF -INSTRUCT provides a way to\ngrow instruction data at a low cost with almost no\nhuman labeling; could more of this generated data\nlead to better instruction-following ability? We an-\nalyze the size of generated data by subsampling dif-\nferent numbers of instructions from the generated\ndataset, finetuning GPT3 on the sampled subsets,\nand evaluating how the resulting models perform\non the 252 user-oriented instruction set. We con-\nduct the same human evaluation as in ¬ß4.4. Figure 7\npresents the performance of GPT3 SELF -INST mod-\nels finetuned with different sizes of generated data.\nOverall, we see consistent improvement as we grow\nthe data size. However, this improvement almost\nplateaus after 16K. This is in-line with the data\nscaling experiments in Wang et al. (2022, Fig. 5).\nInterestingly, when evaluating on SUPER NI we\nfound the model‚Äôs performance gain plateaus ear-\nlier at around hundreds of instructions. This may\nbe due to the fact that the new generated data is\ndistinct from typical NLP tasks in SUPER NI, indi-\ncating that future research may benefit from using a\ncombination of different instruction data for better\nperformance on various types of tasks.\nData quality. Another direction to improve the\nmodel‚Äôs performance is to take our generated data\nand get better supervision (with less noise). We\nexplore this idea by using InstructGPT003 (the best\navailable general-purpose model) to regenerate the\noutput field of all our instances given the instruc-\ntion and input. We then use this improved ver-\nsion of our data to finetune GPT3 . As is shown\nin Figure 7, the resulting model outperforms the\ncounterpart trained with the original data by 10%,\nwhich suggests big room for future work on using\nour generation pipeline to get initial data and then\nimproving the data quality with human experts or\ndistillation from better models.\n5 Related Work\nInstruction-following LMs. A series of works\nhave found evidence that vanilla LMs can be effec-\ntive at following general language instructions if\ntuned with annotated ‚Äúinstructional‚Äù data‚Äîdatasets\ncontaining language instructional commands and\ntheir desired outcomes based on human annota-\ntion (Weller et al., 2020; Mishra et al., 2022; Wei\net al., 2022; Sanh et al., 2022, i.a.). Additionally,\nthey show a direct correlation between the size and\n31.0%36.9%43.7%44.4%54.4%\n20%\n30%\n40%\n50%\n60%\n100800640051200\nPercentageofResponsesRatedasA\nNumberofInstructions\n  w. GPT3 Self-Instruct data  w. improved output from InstructGPT\nFigure 7: Human evaluation performance of\nGPT3 SELF -INST models tuned with different sizes of\ninstructions. x-axis is in log scale. The smallest size is\n175, where only the seed tasks are used for instruction\ntuning. We also evaluate whether improving the data\nquality further improves the performance by distilling\nthe outputs from InstructGPT003. We see consistent\nimprovement from using larger data with better quality.\ndiversity of the ‚Äúinstructional‚Äù data and the general-\nizability of resulting models to unseen tasks (Wang\net al., 2022; Chung et al., 2022). However, since\nthese developments largely focus on existing NLP\ntasks and depend on human-annotated instructions,\nthis poses a bottleneck for progress toward more\ngeneralizable models (e.g., see Fig. 5a in Wang\net al., 2022). Our work aims to move beyond classi-\ncal NLP tasks and tackle the challenges of creating\ndiverse instruction data by employing pretrained\nLMs. InstructGPT (Ouyang et al., 2022) shares\na similar goal as ours in building more general-\npurpose LMs, and has demonstrated remarkable\nperformance in following diverse user instructions.\nHowever, as a commercial system, their construc-\ntion process still remains quite opaque. In partic-\nular, the role of data has remained understudied\ndue to limited transparency and the private user\ndata they used in their study. Addressing such chal-\nlenges necessitates the creation of a large-scale,\npublic dataset covering a broad range of tasks.\nLanguage models for data generation and aug-\nmentation. A variety of works have proposed us-\ning LMs for data generation (Schick and Sch√ºtze,\n2021; Wang et al., 2021; Liu et al., 2022; Meng\net al., 2023) or augmentation (Feng et al., 2021;\nYang et al., 2020; Mekala et al., 2022). Our work\ndiffers from this line in that it isnot specific to a par-\nticular task (say, QA or NLI). In contrast, a distinct\n13491\nmotivation for SELF -INSTRUCT is to bootstrap new\ntask definitions that may not have been defined be-\nfore by NLP practitioners (though potentially still\nimportant for real users). In parallel with our work,\nHonovich et al. (2022a) also propose to generate\nlarge-scale instruction data (so-called Unnatural\nInstructions) with GPT3 models. The major differ-\nences are that 1) they use tasks in SUPER NI (Wang\net al., 2022) as their seed tasks, resulting in a differ-\nent distribution of generated tasks; 2) they employ\nInstructGPT002 for generating the data, in which\nsense they are distilling knowledge from an already\ninstruction-tuned model, while we solely rely on\nthe vanilla LM; 3) the detailed generation pipeline\nand templates are different. Nevertheless, we be-\nlieve that both efforts in expanding instruction data\nare complementary, and the community will benefit\nfrom these diverse datasets.\nInstruction generation. A series of recent\nworks (Zhou et al., 2022b; Ye et al., 2022; Singh\net al., 2022; Honovich et al., 2022b) generate in-\nstructions of a task given a few examples. While\nSELF -INSTRUCT also involves instruction gener-\nation, a major difference in our case is it is task-\nagnostic; we generate new tasks (instructions along\nwith instances) from scratch.\nModel self-training. A typical self-training\nframework (He et al., 2019; Xie et al., 2020; Du\net al., 2021; Amini et al., 2022; Huang et al., 2022)\nuses trained models to assign labels to unlabeled\ndata and then leverages the pseudo-labeled data to\nimprove the model. Zhou et al. (2022a) use mul-\ntiple prompts to specify a single task and propose\nto regularize via prompt consistency, encouraging\nconsistent predictions over the prompts. This al-\nlows either finetuning the model with extra unla-\nbeled training data, or direct application at infer-\nence time. While SELF -INSTRUCT has similarities\nwith the self-training literature, most self-training\nmethods assume a specific target task as well as\nunlabeled examples under it; in contrast, SELF -\nINSTRUCT produces a variety of tasks from scratch.\nKnowledge distillation. Knowledge distilla-\ntion (Hinton et al., 2015; Sanh et al., 2019; West\net al., 2021; Magister et al., 2022) often involves\nthe transfer of knowledge from larger models to\nsmaller ones. SELF -INSTRUCT can also be viewed\nas a form of ‚Äúknowledge distillation\", however, it\ndiffers from this line in the following ways: (1) the\nsource and target of distillation are the same, i.e., a\nmodel‚Äôs knowledge is distilled to itself; (2) the con-\ntent of distillation is in the form of an instruction\ntask (i.e., instructions that define a task, and a set\nof examples that instantiate it).\nBootstrapping with limited resources. A se-\nries of recent works use language models to boot-\nstrap some inferences using specialized methods.\nNPPrompt (Zhao et al., 2022) provides a method\nto generate predictions for semantic labels without\nany finetuning. It uses a model‚Äôs own embeddings\nto automatically find words relevant to the label of\nthe data sample and hence reduces the dependency\non manual mapping from model prediction to la-\nbel (verbalizers). STAR (Zelikman et al., 2022)\niteratively leverages a small number of rationale\nexamples and a large dataset without rationales,\nto bootstrap a model‚Äôs ability to perform reason-\ning. Self-Correction (Welleck et al., 2023) decou-\nples an imperfect base generator (model) from a\nseparate corrector that learns to iteratively correct\nimperfect generations and demonstrates improve-\nment over the base generator. Our work instead\nfocuses on bootstrapping new tasks in the instruc-\ntion paradigm.\nMulti-modal instruction-following. Instruction-\nfollowing models have also been of interest in the\nmulti-modal learning literature (Fried et al., 2018;\nShridhar et al., 2020; Min et al., 2022; Weir et al.,\n2022). SELF -INSTRUCT , as a general approach to\nexpanding data, can potentially also be helpful in\nthose settings, which we leave to future work.\n6 Conclusion\nWe introduce SELF -INSTRUCT , a method to im-\nprove the instruction-following ability of LMs via\ntheir own generation of instruction data. On ex-\nperimenting with vanilla GPT3 , we automatically\nconstruct a large-scale dataset of 52K instructions\nfor diverse tasks, and finetuning GPT3 on this data\nleads to a 33% absolute improvement on SUPER NI\nover the original GPT3 . Furthermore, we curate\na set of expert-written instructions for novel tasks.\nHuman evaluation on this set shows that tuning\nGPT3 with SELF -INSTRUCT outperforms using ex-\nisting public instruction datasets by a large margin\nand performs closely to InstructGPT001. We hope\nSELF -INSTRUCT can serve as the first step to align\npretrained LMs to follow human instructions, and\nfuture work can build on top of this data to improve\ninstruction-following models.\n13492\n7 Broader Impact\nBeyond the immediate focus of this paper, we\nbelieve that SELF -INSTRUCT may help bring\nmore transparency to what happens ‚Äúbehind the\nscenes‚Äù of widely-used instruction-tuned models\nlike InstructGPT or ChatGPT. Unfortunately, such\nindustrial models remain behind API walls as their\ndatasets are not released, and hence there is lit-\ntle understanding of their construction and why\nthey demonstrate impressive capabilities. The bur-\nden now falls on academia to better understand the\nsource of success in these models and strive for\nbetter‚Äîand more open‚Äîmodels. We believe our\nfindings in this paper demonstrate the importance\nof diverse instruction data, and our large synthetic\ndataset can be the first step toward higher-quality\ndata for building better instruction-following mod-\nels. At this writing, the central idea of this paper\nhas been adopted in several follow-up works for\nsuch endeavors (Taori et al., 2023; Xu et al., 2023;\nSun et al., 2023, i.a.).\n8 Limitations\nHere, we discuss some limitations of this work to\ninspire future research in this direction.\nTail phenomena. SELF -INSTRUCT depends on\nLMs, and it will inherit all the limitations that\ncarry over with LMs. As recent studies have\nshown (Razeghi et al., 2022; Kandpal et al., 2022),\ntail phenomena pose a serious challenge to the suc-\ncess of LMs. In other words, LMs‚Äô largest gains\ncorrespond to the frequent uses of languages (head\nof the language use distribution), and there might\nbe minimal gains in the low-frequency contexts.\nSimilarly, in the context of this work, it would not\nbe surprising if the majority of the gains by SELF -\nINSTRUCT are skewed toward tasks or instructions\nthat present more frequently in the pretraining cor-\npus. As a consequence, the approach might show\nbrittleness with respect to uncommon and creative\ninstructions.\nDependence on large models. Because of SELF -\nINSTRUCT ‚Äôs dependence on the inductive biases\nextracted from LMs, it might work best for larger\nmodels. If true, this may create barriers to ac-\ncess for those who may not have large comput-\ning resources. We hope future studies will care-\nfully study the gains as a function of model size\nor various other parameters. It is worthwhile to\nnote that instruction-tuning with human annota-\ntion also suffers from a similar limitation: gains of\ninstruction-tuning are higher for larger models (Wei\net al., 2022).\nReinforcing LM biases. A point of concern for\nthe authors is the unintended consequences of\nthis iterative algorithm, such as the amplification\nof problematic social biases (stereotypes or slurs\nabout gender, race, etc.). Relatedly, one observed\nchallenge in this process is the algorithm‚Äôs diffi-\nculty in producing balanced labels, which reflected\nmodels‚Äô prior biases. We hope future work will\nlead to better understanding of the pros and cons\nof the approach.\nAcknowledgements\nThe authors would like to thank the anonymous\nreviewers for their constructive feedback. We espe-\ncially thank Sewon Min, Eric Wallace, Ofir Press,\nand other members of UWNLP and AllenNLP for\ntheir encouraging feedback and intellectual sup-\nport. This work was supported in part by DARPA\nMCS program through NIWC Pacific (N66001-19-\n2-4031), ONR N00014-18-1-2826, ONR MURI\nN00014-18-1-2670, and gifts from AI2 and an\nAllen Investigator award.\nReferences\nMassih-Reza Amini, Vasilii Feofanov, Loic\nPauletto, Emilie Devijver, and Yury Maximov.\n2022. Self-training: A survey. arXiv preprint\narXiv:2202.12040.\nStephen H Bach, Victor Sanh, Zheng-Xin Yong, Al-\nbert Webson, Colin Raffel, Nihal V Nayak, Abheesht\nSharma, Taewoon Kim, M Saiful Bari, Thibault\nFevry, et al. 2022. PromptSource: An Integrated\nDevelopment Environment and Repository for Nat-\nural Language Prompts. In Annual Meeting of the\nAssociation for Computational Linguistics (ACL) -\nSystem Demonstrations.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, and et al. 2020. Language\nmodels are few-shot learners. In Advances in Neural\nInformation Processing Systems (NeurIPS).\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\n13493\nJingfei Du, √âdouard Grave, Beliz Gunel, Vishrav Chaud-\nhary, Onur Celebi, Michael Auli, Veselin Stoyanov,\nand Alexis Conneau. 2021. Self-training improves\npre-training for natural language understanding. In\nConference of the North American Chapter of the As-\nsociation for Computational Linguistics (NAACL):\nHuman Language Technologies, pages 5408‚Äì5418.\nSteven Y Feng, Varun Gangal, Jason Wei, Sarath Chan-\ndar, Soroush V osoughi, Teruko Mitamura, and Ed-\nuard Hovy. 2021. A survey of data augmentation\napproaches for nlp. In Annual Meeting of the Asso-\nciation for Computational Linguistics (ACL) ACL-\nIJCNLP - Findings, pages 968‚Äì988.\nDaniel Fried, Ronghang Hu, V olkan Cirik, Anna\nRohrbach, Jacob Andreas, Louis-Philippe Morency,\nTaylor Berg-Kirkpatrick, Kate Saenko, Dan Klein,\nand Trevor Darrell. 2018. Speaker-follower models\nfor vision-and-language navigation. In Advances in\nNeural Information Processing Systems (NeurIPS).\nJunxian He, Jiatao Gu, Jiajun Shen, and Marc‚ÄôAurelio\nRanzato. 2019. Revisiting self-training for neural\nsequence generation. In International Conference on\nLearning Representations (ICLR).\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, et al. 2015.\nDistilling the knowledge in a neural network. In\nAdvances in Neural Information Processing Systems\n(NeurIPS) Workshop on Deep Learning.\nOr Honovich, Thomas Scialom, Omer Levy, and Timo\nSchick. 2022a. Unnatural instructions: Tuning lan-\nguage models with (almost) no human labor. arXiv\npreprint arXiv:2212.09689.\nOr Honovich, Uri Shaham, Samuel R Bowman, and\nOmer Levy. 2022b. Instruction induction: From few\nexamples to natural language task descriptions. arXiv\npreprint arXiv:2205.10782.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,\nXuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.\nLarge language models can self-improve. arXiv\npreprint arXiv:2210.11610.\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\nWallace, and Colin Raffel. 2022. Large language\nmodels struggle to learn long-tail knowledge. arXiv\npreprint arXiv:2211.08411.\nNikita Kitaev, Steven Cao, and Dan Klein. 2019. Multi-\nlingual constituency parsing with self-attention and\npre-training. In Annual Meeting of the Association\nfor Computational Linguistics (ACL), pages 3499‚Äì\n3505.\nNikita Kitaev and Dan Klein. 2018. Constituency pars-\ning with a self-attentive encoder. In Annual Meet-\ning of the Association for Computational Linguistics\n(ACL), pages 2676‚Äì2686.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP).\nAlisa Liu, Swabha Swayamdipta, Noah A. Smith, and\nYejin Choi. 2022. W ANLI: Worker and ai collabora-\ntion for natural language inference dataset creation.\nIn Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) - Findings.\nLucie Charlotte Magister, Jonathan Mallinson, Jakub\nAdamek, Eric Malmi, and Aliaksei Severyn. 2022.\nTeaching small language models to reason. arXiv\npreprint arXiv:2212.08410.\nDheeraj Mekala, Tu Vu, Timo Schick, and Jingbo\nShang. 2022. Leveraging qa datasets to improve\ngenerative data augmentation. arXiv preprint\narXiv:2205.12604.\nYu Meng, Martin Michalski, Jiaxin Huang, Yu Zhang,\nTarek Abdelzaher, and Jiawei Han. 2023. Tun-\ning language models as training data generators for\naugmentation-enhanced few-shot learning. In Inter-\nnational Conference on Machine Learning (ICML).\nSo Yeon Min, Devendra Singh Chaplot, Pradeep Raviku-\nmar, Yonatan Bisk, and Ruslan Salakhutdinov. 2022.\nFILM: Following Instructions in Language with Mod-\nular Methods. In International Conference on Learn-\ning Representations (ICLR).\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2022. Cross-Task Generaliza-\ntion via Natural Language Crowdsourcing Instruc-\ntions. In Annual Meeting of the Association for Com-\nputational Linguistics (ACL).\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training Language Models to Follow Instruc-\ntions with Human Feedback. In Advances in Neural\nInformation Processing Systems (NeurIPS).\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research\n(JMLR).\nYasaman Razeghi, Robert L Logan IV , Matt Gardner,\nand Sameer Singh. 2022. Impact of pretraining term\nfrequencies on few-shot reasoning. arXiv preprint\narXiv:2202.07206.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter. In Advances\nin Neural Information Processing Systems (NeurIPS)\nWorkshop on Energy Efficient Machine Learning and\nCognitive Computing.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal Nayak, Debajyoti\n13494\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M Rush. 2022. Multi-\ntask Prompted Training Enables Zero-Shot Task Gen-\neralization. In International Conference on Learning\nRepresentations (ICLR).\nTimo Schick and Hinrich Sch√ºtze. 2021. Generating\ndatasets with pretrained language models. In Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP).\nMohit Shridhar, Jesse Thomason, Daniel Gordon,\nYonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke\nZettlemoyer, and Dieter Fox. 2020. ALFRED: A\nBenchmark for Interpreting Grounded Instructions\nfor Everyday Tasks. In IEEE Conference on Com-\nputer Vision and Pattern Recognition(CVPR).\nChandan Singh, John X Morris, Jyoti Aneja, Alexan-\nder M Rush, and Jianfeng Gao. 2022. Explaining pat-\nterns in data with language models via interpretable\nautoprompting. arXiv preprint arXiv:2210.01848.\nZhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin\nZhang, Zhenfang Chen, David Cox, Yiming Yang,\nand Chuang Gan. 2023. Principle-driven self-\nalignment of language models from scratch with\nminimal human supervision. arXiv preprint\narXiv:2305.03047.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nYizhong Wang, Swaroop Mishra, Pegah Alipoor-\nmolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan\nDhanasekaran, Atharva Naik, David Stap, Eshaan\nPathak, Giannis Karamanolakis, Haizhi Gary Lai, Is-\nhan Purohit, Ishani Mondal, Jacob Anderson, Kirby\nKuznia, Krima Doshi, Maitreya Patel, Kuntal Ku-\nmar Pal, Mehrad Moradshahi, Mihir Parmar, Mi-\nrali Purohit, Neeraj Varshney, Phani Rohitha Kaza,\nPulkit Verma, Ravsehaj Singh Puri, Rushang Karia,\nShailaja Keyur Sampat, Savan Doshi, Siddhartha\nMishra, Sujan Reddy, Sumanta Patro, Tanay Dixit,\nXudong Shen, Chitta Baral, Yejin Choi, Noah A.\nSmith, Hannaneh Hajishirzi, and Daniel Khashabi.\n2022. Super-naturalinstructions: Generalization via\ndeclarative instructions on 1600+ tasks. In Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP).\nZirui Wang, Adams Wei Yu, Orhan Firat, and Yuan Cao.\n2021. Towards zero-label language learning. arXiv\npreprint arXiv:2109.09193.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V Le. 2022. Finetuned Language\nModels are Zero-Shot Learners. In International\nConference on Learning Representations (ICLR).\nNathaniel Weir, Xingdi Yuan, Marc-Alexandre C√¥t√©,\nMatthew Hausknecht, Romain Laroche, Ida Momen-\nnejad, Harm Van Seijen, and Benjamin Van Durme.\n2022. One-Shot Learning from a Demonstration\nwith Hierarchical Latent Language. arXiv preprint\narXiv:2203.04806.\nSean Welleck, Ximing Lu, Peter West, Faeze Brahman,\nTianxiao Shen, Daniel Khashabi, and Yejin Choi.\n2023. Generating sequences by learning to self-\ncorrect. In International Conference on Learning\nRepresentations (ICLR).\nOrion Weller, Nicholas Lourie, Matt Gardner, and\nMatthew Peters. 2020. Learning from Task Descrip-\ntions. In Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP).\nPeter West, Chandra Bhagavatula, Jack Hessel, Jena D\nHwang, Liwei Jiang, Ronan Le Bras, Ximing Lu,\nSean Welleck, and Yejin Choi. 2021. Symbolic\nknowledge distillation: from general language mod-\nels to commonsense models. In Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics (NAACL).\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and\nQuoc V Le. 2020. Self-training with noisy student\nimproves imagenet classification. In IEEE Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR), pages 10687‚Äì10698.\nCanwen Xu, Daya Guo, Nan Duan, and Julian McAuley.\n2023. Baize: An open-source chat model with\nparameter-efficient tuning on self-chat data. arXiv\npreprint arXiv:2304.01196.\nYiben Yang, Chaitanya Malaviya, Jared Fernandez,\nSwabha Swayamdipta, Ronan Le Bras, Ji-Ping Wang,\nChandra Bhagavatula, Yejin Choi, and Doug Downey.\n2020. Generative data augmentation for common-\nsense reasoning. In Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP) -\nFindings.\nSeonghyeon Ye, Doyoung Kim, Joel Jang, Joongbo\nShin, and Minjoon Seo. 2022. Guess the instruction!\nmaking language models stronger zero-shot learners.\narXiv preprint arXiv:2210.02969.\nEric Zelikman, Jesse Mu, Noah D Goodman, and\nYuhuai Tony Wu. 2022. STar: Self-taught reasoner\nbootstrapping reasoning with reasoning. In Advances\nin Neural Information Processing Systems(NeurIPS).\nXuandong Zhao, Siqi Ouyang, Zhiguo Yu, Ming Wu,\nand Lei Li. 2022. Pre-trained language models\ncan be fully zero-shot learners. arXiv preprint\narXiv:2212.06950.\n13495\nChunting Zhou, Junxian He, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2022a. Prompt\nConsistency for Zero-Shot Task Generalization. In\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP) - Findings.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiran Paster, Silviu Pitis, Harris Chan, and Jimmy\nBa. 2022b. Large language models are human-level\nprompt engineers. arXiv preprint arXiv:2211.01910.\n13496\nSupplemental Material\nA Implementation Details\nA.1 Writing the Seed Tasks\nOur method relies on a set of seed tasks to bootstrap the generation. The seed tasks are important for both\nencouraging the task diversity and demonstrating correct ways for solving the diverse tasks. For example,\nwith coding tasks to prompt the model, it has a larger chance to generate coding-related tasks; it‚Äôs also\nbetter to have coding output to guide the model in writing code for new tasks. So, the more diverse the\nseed tasks are, the more diverse and better quality the generated tasks will be.\nOur seed tasks were written when we initiated this project, and targeted for the diverse and interesting\nusages of LLMs. The tasks were written by the authors and our labmates at UWNLP, without explicit\nreference to existing datasets or specific testing tasks. We further categorized the tasks into classification\nand non-classification tasks, based on whether the task has a limited output label space. In total, there are\n25 classification tasks and 150 non-classification tasks. We release this data in our GitHub repository.11\nTo provide a sense of how much the model is generalizing beyond these seed tasks, we further quantify\nthe overlap between the instructions of these seed tasks and the instructions of our test sets, including both\nSUPER NI task instructions (¬ß4.3) and the user-oriented instructions in our human evaluation(¬ß4.4). We\ncompute ROUGE-L similarities between each seed instruction and its most similar instruction in the test\nset. The distribution of the ROUGE-L scores are plotted in Figure 8, with the average ROUGE-L similarity\nbetween the seed instructions and SUPER NI as 0.21, and the average ROUGE-L similarity between the\nseed instructions and user-oriented instructions as 0.34. We see a decent difference between the seed tasks\nand both test sets. There is exactly one identical seed instruction occurring in the user-oriented instruction\ntest set, which is ‚Äúanswer the following question‚Äù and the following questions are actually very different.\n0 0.2 0.4 0.6 0.8 1 0 \n10 \n20 \n30 \n40 \n50 \nROUGE-L Similarity\nNumber of Instructions\n0 0.2 0.4 0.6 0.8 1 0 \n5 \n10 \n15 \n20 \n25 \n30 \n35 \nROUGE-L Similarity\nNumber of Instructions\nFigure 8: Distribution of the ROUGE-L scores between seed instructions and their most similar instructions in\nSUPER NI (left) and the 252 user-oriented instructions (right).\nA.2 Querying the GPT3 API\nWe use different sets of hyperparameters when querying GPT3 API for different purposes. These\nhyperparameters are found to work well with the GPT3 model (‚Äúdavinci‚Äù engine) and the other instruction-\ntuned GPT3 variants. We listed them in Table 4. OpenAI charges $0.02 per 1000 tokens for making\ncompletion request to the ‚Äúdavinci‚Äù engine as of December, 2022. The generation of our entire dataset\ncost around $600.\nA.3 Finetuning GPT3\nGPT3 SELF -INST and some of our baselines are finetuned from GPT3 model (‚Äúdavinci‚Äù engine with 175B\nparameters). We conduct this finetuning via OpenAI‚Äôs finetuning API. 12 While the details of how the\nmodel is finetuned with this API are not currently available (e.g., which parameters are updated, or what\n11https://github.com/yizhongw/self-instruct/blob/main/human_eval/user_oriented_instructions.\njsonl\n12See the the details on OpenAI‚Äôs API.\n13497\nExperiments ‚Üì Temp. Top_P Freq. Penalty Presence Penalty Beam Size Max Length Stop Sequences\nGenerating instructions 0.7 0.5 0 2 1 1024 \"\\n\\n\", \"\\n16\", \"16.\", \"16 .\"\nIdentifying clf. tasks 0 0 0 0 1 3 \"\\n\", \"Task:\"\nGenerating instances 0 0 0 1.5 1 300 \"Task:\"\nEvaluating models 0 0 0 0 0 1024 None (default)\nTable 4: Hyper-parameters for querying OpenAI API in different experiments.\nthe optimizer is), we tune all our models with the default hyperparameters of this API so that the results are\ncomparable. We only set the ‚Äúprompt_loss_weight‚Äù to 0 since we find this works better in our case, and\nevery finetuning experiment is trained for two epochs to avoid overfitting the training tasks. Finetuning is\ncharged based on the number of tokens in the training file. In our case, finetuning GPT3 SELF -INST from\nthe GPT3 model on the entire generated data cost $338.\nA.4 Prompting Templates for Data Generation\nSELF -INSTRUCT relies on a number of prompting templates in order to elicit the generation from language\nmodels. Here we provide our four templates for generating the instruction (Table 5), classifying whether\nan instruction represents a classification task or not (Table 6), generating non-classification instances with\nthe input-first approach (Table 7), and generating classification instances with the output-first approach\n(Table 8).\nCome up with a series of tasks:\nTask 1: {instruction for existing task 1}\nTask 2: {instruction for existing task 2}\nTask 3: {instruction for existing task 3}\nTask 4: {instruction for existing task 4}\nTask 5: {instruction for existing task 5}\nTask 6: {instruction for existing task 6}\nTask 7: {instruction for existing task 7}\nTask 8: {instruction for existing task 8}\nTask 9:\nTable 5: Prompt used for generating new instructions. 8 existing instructions are randomly sampled from the task\npool for in-context demonstration. The model is allowed to generate instructions for new tasks, until it stops its\ngeneration, reaches its length limit or generates ‚ÄúTask 16‚Äù tokens.\n13498\nCan the following task be regarded as a classification task with finite output labels?\nTask: Given my personality and the job, tell me if I would be suitable.\nIs it classification? Yes\nTask: Give me an example of a time when you had to use your sense of humor.\nIs it classification? No\nTask: Replace the placeholders in the given text with appropriate named entities.\nIs it classification? No\nTask: Fact checking - tell me if the statement is true, false, or unknown, based on your\nknowledge and common sense.\nIs it classification? Yes\nTask: Return the SSN number for the person.\nIs it classification? No\nTask: Detect if the Reddit thread contains hate speech.\nIs it classification? Yes\nTask: Analyze the sentences below to identify biases.\nIs it classification? No\nTask: Select the longest sentence in terms of the number of words in the paragraph, output\nthe sentence index.\nIs it classification? Yes\nTask: Find out the toxic word or phrase in the sentence.\nIs it classification? No\nTask: Rank these countries by their population.\nIs it classification? No\nTask: You are provided with a news article, and you need to identify all the categories that\nthis article belongs to. Possible categories include: Music, Sports, Politics, Tech, Finance,\nBasketball, Soccer, Tennis, Entertainment, Digital Game, World News. Output its categories one\nby one, seperated by comma.\nIs it classification? Yes\nTask: Given the name of an exercise, explain how to do it.\nIs it classification? No\nTask: Select the oldest person from the list.\nIs it classification? Yes\nTask: Find the four smallest perfect numbers.\nIs it classification? No\nTask: Does the information in the document supports the claim? You can answer \"Support\" or\n\"Unsupport\".\nIs it classification? Yes\nTask: Create a detailed budget for the given hypothetical trip.\nIs it classification? No\nTask: Given a sentence, detect if there is any potential stereotype in it. If so, you should\nexplain the stereotype. Else, output no.\nIs it classification? No\n¬∑¬∑¬∑\nTask: To make the pairs have the same analogy, write the fourth word.\nIs it classification? No\nTask: Given a set of numbers, find all possible subsets that sum to a given number.\nIs it classification? No\nTask: {instruction for the target task}\nTable 6: Prompt used for classifying whether a task instruction is a classification task or not.\n13499\nCome up with examples for the following tasks. Try to generate multiple examples when possible.\nIf the task doesn‚Äôt require additional input, you can generate the output directly.\nTask: Which exercises are best for reducing belly fat at home?\nOutput:\n- Lying Leg Raises\n- Leg In And Out\n- Plank\n- Side Plank\n- Sit-ups\nTask: Extract all the country names in the paragraph, list them separated by commas.\nExample 1\nParagraph: Dr. No is the sixth novel by the English author Ian Fleming to feature his British\nSecret Service agent James Bond. Written at Fleming‚Äôs Goldeneye estate in Jamaica, it was\nfirst published in the United Kingdom by Jonathan Cape in 1958. In the novel Bond looks into\nthe disappearance in Jamaica of two fellow MI6 operatives who had been investigating Doctor\nNo. Bond travels to No‚Äôs Caribbean island and meets Honeychile Rider, who is there to collect\nshells. They are captured and taken to a luxurious facility carved into a mountain. The\ncharacter of Doctor No, the son of a German missionary and a Chinese woman, was influenced by\nSax Rohmer‚Äôs Fu Manchu stories. Dr. No was the first of Fleming‚Äôs novels to face widespread\nnegative reviews in Britain, but it was received more favourably in the United States.\nOutput: English, British, Jamaica, the United Kingdom, German, Chinese, Britain, the United\nStates.\nTask: Converting 85 F to Celsius.\nOutput: 85 ¬∞F = 29.44¬∞C\nTask: Sort the given list ascendingly.\nExample 1\nList: [10, 92, 2, 5, -4, 92, 5, 101]\nOutput: [-4, 2, 5, 5, 10, 92, 92, 101]\nExample 2\nInput 2 - List: [9.99, 10, -5, -1000, 5e6, 999]\nOutput: [-1000, -5, 9.99, 10, 999, 5e6]\nTask: Suggest a better and more professional rephrasing of the following sentence.\nExample 1\nSentence: This house is surprisingly not constructed very well, and you probably need more\nmoney to fix it after you buy it. If you ask me, I would suggest you to consider other\ncandidates.\nOutput: This house does not seem to be constructed well, so you may need to spend more money\nto fix it after you purchase it. I would suggest that you look at other properties.\nExample 2\nSentence: Just so you know, we did an experiment last week and found really surprising results\n- language model can improve itself!\nOutput: Our experiments last week demonstrated surprising results, proving that the language\nmodel can improve itself.\n¬∑¬∑¬∑\nTask: Turn down a job offer by sending an email to a recruiter explaining the reason.\nOutput: Hi [Recruiter],\nThank you so much for the generous offer to join your team. As we discussed, I‚Äôve admired the\ncompany for a number of years, and am a proud endorser of its products. However, after further\nconsideration of where I currently am in my career, I‚Äôve decided to accept an offer at another\ncompany.\nI would love to stay in touch with you and have already started following you on [Social Media\nPlatform]. Again, thank you so much for your time and consideration.\nThanks again,\n[Your Name]\nTask: {Instruction for the target task}\nTable 7: Prompt used for the input-first approach of instance generation. The model is prompted to generate the\ninstance first, and then generate the corresponding output. For instructions that don‚Äôt require additional input, the\noutput is allowed to be generated directly.\n13500\nGiven the classification task definition and the class labels, generate an input that\ncorresponds to each of the class labels. If the task doesn‚Äôt require input, just generate the\ncorrect class label.\nTask: Classify the sentiment of the sentence into positive, negative, or mixed.\nClass label: mixed\nSentence: I enjoy the flavor of the restaurant but their service is too slow.\nClass label: Positive\nSentence: I had a great day today. The weather was beautiful and I spent time with friends.\nClass label: Negative\nSentence: I was really disappointed by the latest superhero movie. I would not recommend it.\nTask: Given a dialogue, classify whether the user is satisfied with the service. You should\nrespond with \"Satisfied\" or \"Unsatisfied\".\nClass label: Satisfied\nDialogue:\n- Agent: Thank you for your feedback. We will work to improve our service in the future.\n- Customer: I am happy with the service you provided. Thank you for your help.\nClass label: Unsatisfied\nDialogue:\n- Agent: Sorry that we will cancel your order. You will get a refund within 7 business days.\n- Customer: oh that takes too long. I want you to take quicker action on this.\nTask: Given a political opinion, classify whether the speaker is a Democrat or Republican.\nClass label: Democrats\nOpinion: I believe, all should have access to quality healthcare regardless of their income.\nClass label: Republicans\nOpinion: I believe that people should be able to keep more of their hard-earned money and\nshould not be taxed at high rates.\nTask: Tell me if the following email is a promotion email or not.\nClass label: Promotion\nEmail: Check out our amazing new sale! We‚Äôve got discounts on all of your favorite products.\nClass label: Not Promotion\nEmail: We hope you are doing well. Let us know if you need any help.\nTask: Detect if the Reddit thread contains hate speech.\nClass label: Hate Speech\nThread: All people of color are stupid and should not be allowed to vote.\nClass label: Not Hate Speech\nThread: The best way to cook a steak on the grill.\nTask: Does the document supports the claim? Answer with \"Support\" or \"Unsupport\".\nClass label: Unsupport\nDocument: After a record-breaking run that saw mortgage rates plunge to all-time lows and\nhome prices soar to new highs, the U.S. housing market finally is slowing. While demand and\nprice gains are cooling, any correction is likely to be a modest one, housing economists and\nanalysts say. No one expects price drops on the scale of the declines experienced during the\nGreat Recession.\nClaim: The US housing market is going to crash soon.\nClass label: Support\nDocument: The U.S. housing market is showing signs of strain, with home sales and prices\nslowing in many areas. Mortgage rates have risen sharply in recent months, and the number\nof homes for sale is increasing. This could be the beginning of a larger downturn, with some\neconomists predicting a potential housing crash in the near future.\nClaim: The US housing market is going to crash soon.\n¬∑¬∑¬∑\nTask: Which of the following is not an input type? (a) number (b) date (c) phone number (d)\nemail address (e) all of these are valid inputs.\nClass label: (e)\nTask: {instruction for the target task}\nTable 8: Prompt used for the output-first approach of instance generation. The model is prompted to generate the\nclass label first, and then generate the corresponding input. This prompt is used for generating the instances for\nclassification tasks.\n13501\nB Human Evaluation Details for Following the User-oriented Instructions\nB.1 Human Evaluation Setup\nHere we provide more details for the human evaluation described in ¬ß4.4 for rating the models‚Äô responses\nto the 252 user-oriented instructions. To ensure faithful and reliable evaluation, we asked two authors\nof these instructions (and of this paper) to judge model predictions. These two evaluators coordinated\nthe standards for the 4-level rating system before starting annotation and then each of them rated all\nthe instances independently. They were presented with the instruction, instance input, target output\n(as a reference), and model responses. Model responses are listed in random order, with all the model\ninformation anonymized. Figure 9 provides a screenshot of the annotation interface. The reported\nperformance in this paper is based on the results from one of the evaluators, and the trends from the other\nevaluator‚Äôs results are the same.\nFigure 9: Human evaluation in done using a Google sheet, with predictions from different models present in random\norder and the model information being anonymized. Our expert evaluators are required to read the instruction and\ninput, refer to the target, and then select the rating for the model‚Äôs response from A/B/C/D, corresponding to the 4\nlevels described in ¬ß4.4.\nB.2 Human Evaluation Agreement\nTo measure how reliable our human evaluation is, we calculate the inner-rater agreement between our two\nevaluators.\nWe first report Cohen‚ÄôsŒ∫, which is commonly used to measure inter-rater agreement for categorical\nitems. When calculating this, we treat the 4-level rating (A-D) as a categorical variable, leading to a Œ∫of\n0.58, which is a moderate agreement according to common practice.13 Furthermore, we also calculate the\nagreement of our evaluators on classifying acceptable responses ((A or B) vs. (C or D)), with a final Œ∫of\n0.75, indicating substantial agreement.\n13https://en.wikipedia.org/wiki/Cohen%27s_kappa\n13502\nWe also compute the Spearman correlation coefficient œÅbetween the ratings of our two evaluators by\ntreating the rating as an ordinal variable (A>B>C>D). The final coefficient is œÅ= 0.81, indicating a high\ncorrelation between the two evaluators.\nB.3 Example Predictions from GPT3 SELF -INST\nWe present a selection of user-oriented tasks, the corresponding GPT3 SELF -INST -produced responses and\nannotator ratings in Table 9. We see that even for responses rated as level C, the model demonstrates\nextensive steps in solving the task, even though its final output is incorrect.\n13503\nInstruction + Instance Input Target Response GPT3 SELF-INST Response Rating\nInstruction: Give students tips\non how to keep their nerves under\ncontrol during class presentations.\nEveryone gets a little nervous when speaking\nin front of a group. That‚Äôs normal. But the\nmore preparation you‚Äôve done‚Äîmeaning plenty of\nresearching, organizing, and rehearsing‚Äîthe more\nconfident you‚Äôll be. Preparation is the key.\nAnd if you make a mistake or stumble over your\nwords, just regroup and keep going. Nobody‚Äôs\nperfect, and nobody expects you to be.\n‚Äì Practice your presentation in front\nof friends and family.\n‚Äì Don‚Äôt drink too much caffeine before\nthe presentation.\n‚Äì Take deep breaths before you start\nspeaking.\n‚Äì If you feel nervous, smile.\n‚Äì Remember that everyone in the\naudience is rooting for you.\nA\nInstruction: Make a list of the pros\nand cons of the given decision.\nInput: Implementing a Remote Working\nPolicy\nPros:\n‚Äì Allowing employees to work from home can\nreduce their stressful commutes and increase\ntheir productivity.\n‚Äì Remote working provides greater flexibility,\nparticularly for working parents and\ncaregivers who have other responsibilities.\n‚Äì ...(more points)\nCons:\n‚Äì It may be more difficult for team members\nto build meaningful, productive, creative\nrelationships with one another.\n‚Äì ...(more points)\nPros:\n‚Äì Employees can work from home and\nsave on commuting costs.\n‚Äì Employees can spend more time with\ntheir families.\nCons:\n‚Äì Employees may not be as productive\nas they would be in the office.\n‚Äì Employees may not be as motivated to\nwork.\nA\nInstruction: A user story is an\ninformal, general explanation of\na software feature written from\nthe perspective of the end-user or\ncustomer. Write a user story for a\ngiven software.\nInput: Gmail\nAs a user, I want to be able to recover the\npassword to my account, so that I will be\nable to access my account in case I forgot\nthe password.\nAs a user, I want to be able to send\nemails to my friends and family. I\nwant to be able to attach files to my\nemails. I want to be able to search\nfor emails in my inbox.\nA\nInstruction: A color description has\nbeen provided. Find the CSS code\nassociated with that color.\nInput: A light red color with a\nmedium light shade of pink\n#DB7093 () #ff66cc () B\nInstruction: Someone gives you some\ninformation about where they want\nto go and what they enjoy seeing and\ndoing. Describe a proper road trip\nfor them and find the best stops and\nactivities on the way.\nInput: I‚Äôm looking for a west coast\nroad trip. My goal is to start near\nSan Diego and travel to Los Angeles.\nBegin near the city of San Diego and make the\nfirst stop in coastal La Jolla. This upscale\nlittle village is a great place for an overnight,\na day of sunbathing at La Jolla Shores Beach,\nor a cocktail with a view. Just 20 minutes\nnorth, stop into beachy Encinitas to window\nshop the boutiques or sample fish tacos. Walk\nthrough the coastal meditation garden at the\nSelf-Realization Fellowship or join surfers\nin the line-up at one of the fantastic local\nbeaches here ...(more ideas)\nYou should start your trip in San\nDiego, California. You can visit the\nSan Diego Zoo, which is one of the\nbest zoos in the world. Then you can\ndrive to Los Angeles, where you can see\nthe Hollywood signs and walk along the\nbeach.\nB\nInstruction: Give some examples of\nwhat people usually say in the given\nsocial situation.\nInput: when someone arrives safely\n‚Äì ‚ÄúGlad you made it safe and sound.‚Äù\n‚Äì ‚ÄúGood to see you got here unscathed.‚Äù\n‚Äì ‚ÄúSo, bulletproof vests actually work!‚Äù\nThank God, I made it. B\nInstruction: You are given a sentence\nin first-order logic. Translate it\ninto natural language.\nInput:‚àÉx‚àÄy(soldier(x) ‚Üí\ngeneral(x, y))\nThere is someone (x) such that if he is a\nsoldier, then he is general of everyone.\nThere exists a soldier such that for\nevery general, he is a general.\nC\nInstruction: Make a questionnaire\nto help hotel guests write hotel\nreviews.\n‚Äì Your overall rating of this hotel:\n‚Äì Summarize your visit or highlight an\ninteresting detail:\n‚Äì ...(more questions)\nQuestion: How would you rate the\nservice at the front desk?\nAnswer: Excellent, Good, Average, Poor\nC\nInstruction: Design an outline\nfor a blog post based on the given\ninformation and list the sections\naccordingly.\nInput: This blog post is going to\nabout making a list of last-minute\ngift ideas. Also, the tone of the\npost is going to be relaxed and\ncasual.\n‚Äì Section: Movie tickets\n‚Äì Section: A fun game console\n‚Äì Section: Personalized items with\nphotos/artwork\n‚Äì ...(more sections)\n‚Äì Takeaway: Don‚Äôt stress about out running out\nof time to buy, make a gift.\n‚Äì Introduction\n‚Äì List of Gift Ideas\n‚Äì Conclusion\nC\nInstruction: Write a Jira ticket for\nthe given task.\nInput: New Employee onboarding\nTitle: Please help our new employee get settled.\nPrepare a desk, chair, and computer for them to\nuse. Create their company account, email, and\ntemporary password. Grant access to company\ndocuments.\nHi [Manager], I‚Äôm writing to you\nbecause I have a question about the new\nemployee onboarding process. I‚Äôve been\nworking here for a few months now, and\nI‚Äôm still not sure how it works. Could\nyou please explain it to me?\nD\nInstruction: Create alliterations\nby finding synonyms for words in the\ngiven sentence.\nInput: David wears a hat everyday.\nDavid dons a derby daily. David wears a hat every day. D\nTable 9: Examples in the user-oriented instructions dataset (¬ß4.4) and predictions from GPT3 SELF -INST . The right\ncolumn indicates one of the four quality ratings assigned to the model‚Äôs response, with ‚ÄúA‚Äù indicating ‚Äúvalid and\nsatisfying‚Äù responses (highest) and ‚ÄúD‚Äù indicating ‚Äúirrelevant or invalid response‚Äù (lowest).\n13504\nC Task and Instance Examples from the Generated Instruction Data\nInstruction + Instance Input Target Output Validity\nInstruction: Generate a random password\nwith at least 6 characters.\ndef generateRandomPassword():\npassword = \"\"\nwhile len(password) < 6:\npassword += chr(random.randint(65, 90))\nreturn password\nInstruction:‚úì\nInput: ‚úì\nOutput:‚úì\nInstruction: Write a paragraph describing\nhow to tie shoelaces for people who have\nnever tied their shoes before. explain\neach step briefly.\nParagraph: To tie shoelaces, first you need to make a loop with one\nend of the lace. Then pass the other end through the loop and pull\nit tight. Next, you need to wrap the lace around the shoe several\ntimes. Finally, tuck in the loose end into the wrapped part of the\nlace.\nInstruction:‚úì\nInput: ‚úì\nOutput:‚úì\nInstruction: Given a word, find out its\nlength and its number of vowels.\nInput: Word = \"hello\"\nLength = 5, Number of vowels = 2 Instruction:‚úì\nInput: ‚úì\nOutput:‚úì\nInstruction: Write a story with three\ncharacters: a person, an animal and an\nobject.\nInput: Person: John, a programmer.\nAnimal: A dog.\nObject: A laptop.\nJohn was sitting on the floor in front of his laptop when he heard a\nknock at the door. He opened it to find a dog standing there. The\ndog walked into the room and jumped onto the couch next to John‚Äôs\nlaptop.\nInstruction:‚úì\nInput: ‚úì\nOutput:‚úì\nInstruction: Compose an email and send it\nto your friend, asking for advice on what\nto do in this situation.\nInput: Situation: You are a programmer\nand you have been working on a project\nfor the past few months. The deadline is\napproaching, but you haven‚Äôt finished yet.\nYour manager has asked you to stay late\nevery day this week to finish it.\nHi [Friend],\nI‚Äôm having a bit of trouble with my current job. I‚Äôve been\nworking on a project for the past few months, and the deadline\nis approaching. My manager has asked me to stay late every day this\nweek to finish it. I don‚Äôt think I can do that because I have other\ncommitments. What should I do?\nInstruction:‚úì\nInput: ‚úì\nOutput:‚úì\nInstruction: What are some ways we can\nmake our school more eco-friendly? ‚Äì Use reusable water bottles instead of disposable plastic ones.\n‚Äì Turn off lights when you leave a room.\n‚Äì Recycle paper, cans and bottles.\n‚Äì Plant trees to absorb carbon dioxide.\nInstruction:‚úì\nInput: ‚úì\nOutput:‚úì\nInstruction: I am looking for a new\napartment. there are many options\navailable but they are all quite expensive.\ni want to move as soon as possible. give\nme advice on what to do.\nYou should look for a cheaper apartment. You can try to negotiate\nwith the landlord and offer him/her a lower price. If you are lucky,\nthey will accept your offer. Otherwise, you can wait until there is\nan apartment available at a more reasonable price.\nInstruction:‚úì\nInput: ‚úì\nOutput:‚úì\nInstruction: Write a dialogue between\ntwo people who have been friends for a\nlong time, and one of them is trying to\nconvince the other to do something that he\nor she doesn‚Äôt want to do.\nA: Hey, I have a great idea! Let‚Äôs go to the beach today.\nB: No thanks, I‚Äôm not really in the mood for that.\nA: Come on, it‚Äôll be fun! We haven‚Äôt been there together in ages.\nB: I know, but I just don‚Äôt feel like going.\nA: But you love the beach! You always say how much you enjoy\nswimming and lying in the sun.\nB: Yeah, I do. It‚Äôs just that I‚Äôve been feeling kind of tired\nlately.\nA: Oh, come on. You‚Äôre never tired. You‚Äôre always so energetic.\nB: Well, I am now. I think I need some rest.\nA: Okay, fine. If you change your mind, let me know.\nInstruction:‚úì\nInput: ‚úì\nOutput:‚úì\nInstruction: How can i create an app that\nwill be able to identify different kinds\nof plants by taking photos of them?\nYou can use the following steps to create an app that will be able\nto identify different kinds of plants by taking photos of them.\nStep 1: Create a database with all the information about the plants\nyou want to identify.\nStep 2: Create a user interface for your application.\nStep 3: Use image recognition techniques to identify the plant in\nthe photo.\nStep 4: Display the name of the plant on the screen.\nInstruction:‚úì\nInput: ‚úì\nOutput:‚úì\nInstruction: Find the closest matching\nemoji to a given one.\nInput: Emoji:\nInstruction:‚úì\nInput: ‚úì\nOutput:‚úì\nInstruction: Describe your favorite video\ngame using only verbs.\nInput: Game: The Legend of Zelda\nExplore, fight, solve, upgrade, repeat Instruction:‚úì\nInput: ‚úì\nOutput:‚úì\nTable 10: Representative valid tasks generated by GPT3. As is discussed in ¬ß3, these generated tasks cover a broad\nrange of formats, text types, and underlying expertise, while being correct on more than half of all the generated\ntasks.\n13505\nInstruction + Instance Input Target Output Validity\nInstruction: Given a set of words\nw = {w1, w2, ..., wn}where each word\nwi has n letters, write a function that\ntakes a target lengthl and returns a\nnew set of wordss such that |s| = l and\n|{i : wi ‚ààs}|‚â§l/|w|.\ndef wordSubsetSum(w, l):\ns = []\nfor i in range(len(w)):\nif len(s) + 1 < l:\ns.append(w[i])\nreturn s\nInstruction:‚úó\nInput: ‚úì\nOutput: ‚úó\nInstruction: The house on fire! go! ‚Äì Go to the kitchen and take a pan.\n‚Äì Take water from the tap.\n‚Äì Pour water into the pan.\n‚Äì Put out the fire with the pan.\nInstruction:‚úó\nInput: ‚úì\nOutput: ‚úó\nInstruction: Find out if you have any\nfriends who like to play football or\nchess.\ndef hasFriendsWhoLikeToPlayFootballOrChess(self):\nfriends = []\nfor friend in self.friends:\nif friend.likes == ‚Äôfootball‚Äô or friend.likes == ‚Äôchess‚Äô:\nfriends.append(friend)\nreturn len(friends) > 0\nInstruction:‚úó\nInput: ‚úì\nOutput: ‚úì\nInstruction: A number of people are lined\nup for an interview. they are asked\nseveral questions and their answers are\nrecorded as numbers. you need to identify\nwhich person has the highest salary among\nall the candidates.\nInput: Person 1: 2, 3, 4, 5, 6\nPerson 2: 7, 8, 9, 10, 11\nPerson 3: 12, 13, 14, 15, 16\nPerson 4: 17, 18, 19, 20, 21\nPerson 5: 22, 23, 24, 25, 26\nPerson 6: 27, 28, 29, 30, 31\nHighest salary Instruction:‚úì\nInput: ‚úó\nOutput: ‚úó\nInstruction: Given a list of words,\ngenerate a story about a group of people\nwho live on a planet. the story should\ncontain at least 10 sentences. each\nsentence should be generated by randomly\npicking one word from the list of words.\nThe story starts with a group of people who live on a planet. They\nare very intelligent and have created many things that help them in\ntheir daily lives. One day, they decided to create a machine that\ncan make food for them. The machine was able to produce food but it\nalso produced something else. It produced a liquid that could be\nused as fuel for the machines. The people were happy because they\ndidn‚Äôt need to go out and look for food or fuel anymore. They had\neverything they needed right there in their homes. However, one\nday, the machine stopped working. The people tried to fix it but\ncouldn‚Äôt figure out what went wrong. They tried to find someone\nwho could help them but no one knew how to fix the machine. After\na few days, the people realized that they would die if they didn‚Äôt\ndo anything about it. So, they decided to send some of their best\nscientists to space to find a new planet where they could live. The\nscientists traveled for a long time before they found a planet that\nlooked like a good place to live. They sent a message back to the\npeople telling them about the planet. The people were excited and\nstarted preparing for the journey. They built spaceships and flew\nto the new planet. When they arrived, they saw that the planet was\nbeautiful. There were trees and rivers and mountains. The people\nwere so happy that they finally found a new home. They started\nbuilding houses and planting crops. They lived happily ever after.\nInstruction:‚úì\nInput: ‚úó\nOutput: ‚úì\nInstruction: Find out what the author‚Äôs\nattitude towards working part-time is.\nParagraph: Working part-time is a great way to earn some extra\nmoney while still being able to enjoy your free time. It‚Äôs also\na good way to get experience in a new field, or just try something\ndifferent for a while. However, it can be difficult to find the\nright balance between work and play.\nThe author seems to be positive about working part-time.\nInstruction:‚úì\nInput: ‚úó\nOutput: ‚úì\nInstruction: Is it ethical to use a new\ntechnology that could potentially cause\nharm to others?\nNew Technology: Self-driving cars\nPotential Harm: Accidents caused by self-driving cars\nEthical? Yes, because the number of lives saved by self-driving\ncars will be greater than the number of lives lost due to accidents\ncaused by them.\nInstruction:‚úì\nInput: ‚úó\nOutput: ‚úì\nInstruction: What is your salary? class Solution(object):\ndef getSalary(self):\nreturn self.salary\nInstruction:‚úì\nInput: ‚úì\nOutput: ‚úó\nInstruction: A person wants to buy a car\nfrom another person. he has some bitcoins,\nbut he doesn‚Äôt want to reveal his identity.\nhow can they make the transaction happen?\nThe buyer and seller should agree on a price for the car. The buyer\nthen creates a bitcoin transaction that sends the agreed upon amount\nof bitcoins to the seller‚Äôs bitcoin address, but the buyer does not\nsign this transaction with his private key. Instead, he gives the\nunsigned transaction to the seller. The seller can now sign the\ntransaction using his private key, and broadcast it to the network.\nOnce the transaction is confirmed by the network, the buyer has no\nway to get his money back, so he will be forced to go through with\nthe purchase.\nInstruction:‚úì\nInput: ‚úì\nOutput: ‚úó\nTable 11: Representative invalid tasks generated by GPT3 . The problematic fields are indicated in the validity\ncolumn. As discussed in ¬ß3.3, although these tasks contain errors, they still provide many useful signals in\nsupervising models to follow instructions.\n13506\nACL 2023 Responsible NLP Checklist\nA For every submission:\n‚ñ°\u0013 A1. Did you describe the limitations of your work?\n7\n‚ñ°\u0013 A2. Did you discuss any potential risks of your work?\n7\n‚ñ°\u0013 A3. Do the abstract and introduction summarize the paper‚Äôs main claims?\n1\n‚ñ°\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB ‚ñ°\u0013 Did you use or create scientiÔ¨Åc artifacts?\n3, 4\n‚ñ°\u0013 B1. Did you cite the creators of artifacts you used?\n4\n‚ñ°\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\n1, 4\n‚ñ°\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciÔ¨Åed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\n3, 4\n‚ñ°\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiÔ¨Åes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nThe data released in this paper is synthetic data generated by models and cannot be easily veriÔ¨Åed.\n‚ñ°\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\n3\n‚ñ°\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiÔ¨Åcant, while on small test sets they may not be.\n3\nC ‚ñ°\u0013 Did you run computational experiments?\n3,4\n‚ñ°\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\n4, Appendix A\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n13507\n‚ñ°\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nAppendix A\n‚ñ°\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\n4\n‚ñ°\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\n3\nD ‚ñ°\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\n4\n‚ñ°\u0013 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nAppendix B\n‚ñ°\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants‚Äô demographic\n(e.g., country of residence)?\nAppendix B\n‚ñ° D3. Did you discuss whether and how consent was obtained from people whose data you‚Äôre\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. The data collection was done by the authors themselves.\n‚ñ° D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. The data collection was done by the authors themselves.\n‚ñ° D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. The data collection was done by the authors themselves.\n13508",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6063989996910095
    },
    {
      "name": "Computational linguistics",
      "score": 0.5636082887649536
    },
    {
      "name": "Programming language",
      "score": 0.4852883517742157
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.44406747817993164
    },
    {
      "name": "Natural language processing",
      "score": 0.425519198179245
    },
    {
      "name": "Linguistics",
      "score": 0.38498011231422424
    },
    {
      "name": "Artificial intelligence",
      "score": 0.329270601272583
    },
    {
      "name": "Philosophy",
      "score": 0.17684537172317505
    },
    {
      "name": "Physics",
      "score": 0.0784686803817749
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210140341",
      "name": "Allen Institute",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I55732556",
      "name": "Arizona State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    }
  ]
}