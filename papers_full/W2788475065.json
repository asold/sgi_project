{
  "title": "Retrieval-Augmented Convolutional Neural Networks for Improved Robustness against Adversarial Examples",
  "url": "https://openalex.org/W2788475065",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4224482625",
      "name": "Zhao, Jake",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753220617",
      "name": "Cho, Kyunghyun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2948066818",
    "https://openalex.org/W2335728318",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2756540778",
    "https://openalex.org/W2162006472",
    "https://openalex.org/W2503523779",
    "https://openalex.org/W2949117887",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W2786118190",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2753484907",
    "https://openalex.org/W2612866063",
    "https://openalex.org/W2951004968",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W1883420340",
    "https://openalex.org/W2173520492",
    "https://openalex.org/W2964153729",
    "https://openalex.org/W1999360130",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2963207607",
    "https://openalex.org/W2522770641",
    "https://openalex.org/W2735607295",
    "https://openalex.org/W2005126631",
    "https://openalex.org/W2964197269",
    "https://openalex.org/W2460937040",
    "https://openalex.org/W2963070423",
    "https://openalex.org/W2765233338",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W2610190180",
    "https://openalex.org/W2618463334",
    "https://openalex.org/W2765384636",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W2089497633",
    "https://openalex.org/W2243397390",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2423557781",
    "https://openalex.org/W2963003451",
    "https://openalex.org/W2103559027",
    "https://openalex.org/W2734506812",
    "https://openalex.org/W2145094598",
    "https://openalex.org/W2593864460"
  ],
  "abstract": "We propose a retrieval-augmented convolutional network and propose to train it with local mixup, a novel variant of the recently proposed mixup algorithm. The proposed hybrid architecture combining a convolutional network and an off-the-shelf retrieval engine was designed to mitigate the adverse effect of off-manifold adversarial examples, while the proposed local mixup addresses on-manifold ones by explicitly encouraging the classifier to locally behave linearly on the data manifold. Our evaluation of the proposed approach against five readily-available adversarial attacks on three datasets--CIFAR-10, SVHN and ImageNet--demonstrate the improved robustness compared to the vanilla convolutional network.",
  "full_text": "Retrieval-Augmented Convolutional Neural Networks\nfor Improved Robustness against Adversarial Examples\nJake (Junbo) Zhao 1 2 Kyunghyun Cho1 2 3\nAbstract\nWe propose a retrieval-augmented convolutional\nnetwork and propose to train it with local mixup,\na novel variant of the recently proposed mixup\nalgorithm. The proposed hybrid architecture com-\nbining a convolutional network and an off-the-\nshelf retrieval engine was designed to mitigate the\nadverse effect of off-manifold adversarial exam-\nples, while the proposed local mixup addresses\non-manifold ones by explicitly encouraging the\nclassiﬁer to locally behave linearly on the data\nmanifold. Our evaluation of the proposed ap-\nproach against ﬁve readily-available adversarial\nattacks on three datasets–CIFAR-10, SVHN and\nImageNet– demonstrate the improved robustness\ncompared to the vanilla convolutional network.\n1. Introduction\nSince the initial investigation by Szegedy et al. (2013), ad-\nversarial examples have drawn a large interest. Various\nmethods for both generating adversarial examples as well as\nprotecting a classiﬁer from them have been proposed (see\nSec. 3–4 for more details.) Adversarial examples exist due\nto misbehaviors of a classiﬁer in some regions of the input\nspace and are generated often by ﬁnding a point in such a\nregion using optimization.\nAccording to (Gilmer et al., 2018), adversarial examples\ncan be categorized into those off the data manifold, which\nis deﬁned as a manifold on which training examples lie,\nand those on the data manifold. Off-manifold adversarial\nexamples occur as the classiﬁer does not have a chance to\nobserve any off-manifold examples during training, which\nis a natural consequence from the very deﬁnition of the data\nmanifold. On-manifold adversarial examples however exist\nbetween training examples on the data manifold. There\nare two causes behind this phenomenon; (1) the sparsity of\n1New York University 2Facebook AI Research 3CIFAR\nAzrieli Global Scholar. Correspondence to: Jake (Junbo) Zhao\n<j.zhao@nyu.edu>.\ntraining examples and (2) the non-smooth behavior of the\nclassiﬁer on the data manifold.\nIn this paper, we propose to tackle both off- and on-manifold\nadversarial examples by incorporating an off-the-shelf re-\ntrieval mechanism which indexes a large set of examples and\ntraining this combination of a deep neural network classiﬁer\nand the retrieval engine to behave linearly on the data mani-\nfold using a novel variant of the recently proposed mixup\nalgorithm (Zhang et al., 2017), to which we refer as “local\nmixup.”\nThe retrieval mechanism efﬁciently selects a subset of neigh-\nboring examples from a candidate set near the input. These\nneighboring examples are used as a local approximation to\nthe data manifold in the form of a feature-space convex hull\nonto which the input is projected. The classiﬁer then makes\na decision based on this projected input. This addresses off-\nmanifold adversarial examples. Within this feature-space\nconvex hull, we encourage the classiﬁer to behave linearly\nby using local mixup to further address on-manifold adver-\nsarial examples.\nWe evaluate the proposed approach, called a retrieval-\naugmented classiﬁer, with a deep convolutional network (Le-\nCun et al., 1998) on object recognition. We extensively test\nthe retrieval-augmented convolutional network (RaCNN)\non datasets with varying scales; CIFAR-10 (Krizhevsky &\nHinton, 2009), SVHN (Netzer et al., 2011) as well as Ima-\ngeNet (Deng et al., 2009), against ﬁve readily-available ad-\nversarial attacks including both white-box (FGSM, iFGSM,\nDeepFool and L-BFGS) and black-box attacks (Boundary).\nOur experiments reveal that the RaCNN is more robust to\nthese ﬁve attacks than the vanilla convolutional network.\n2. Retrieval-Augmented CNN\nGilmer et al. (2018) have recently demonstrated that ad-\nversarial examples exist both on and off the data manifold\nin a carefully controlled setting in which examples from\ntwo classes are placed on two disjoint spheres. This result\nsuggests that it is necessary to tackle both types of adver-\nsarial examples to improve the robustness of a deep neural\nnetwork based classiﬁer to adversarial examples. In this\nsection, we describe our approach toward building a more\narXiv:1802.09502v1  [cs.LG]  26 Feb 2018\nRetrieval-augmented Convolutional Neural Networks against adversarial examples\nrobust classiﬁer by combining an off-the-shelf retrieval en-\ngine and a variant of the recently proposed mix-up learning\nstrategy.\n2.1. Setup\nLet D′ = {(x′\n1,y′\n1),..., (x′\nM,y′\nM)}be a candidate set of\nexamples. This set may be created as a subset from a train-\ning set D= {(x1,y1),..., (xN,yN)}or may be an entire\nseparate set. We use D′as a proxy to the underlying data\nmanifold.\nk(x,x′) is a distance function that measures the dissimilarity\nbetween two inputs xand x′. In order to facilitate the use\nof an off-the-shelf retrieval engine, we use\nk(x,x′) =∥φ′(x) −φ′(x′)∥2, (1)\nwhere φ′is a predeﬁned, or pretrained, feature extractor. We\nassume the existence of a readily-available retrieval engine\nF that takes xas input and returns the Knearest neighbors\nin D′according to k(x,x′).\nWe then have a deep neural network classiﬁer composed of\na feature extraction φand a classiﬁer g. This classiﬁer is\ntrained on a training set D, taking into account the extra set\nD′and the retrieval engine.\n2.2. Inference\nIn this setup, we ﬁrst describe the forward evaluation of the\nproposed network. This forward pass is designed to handle\nadversarial examples “off” the data manifold by projecting\nthem onto the data manifold.\nLocal Characterization of Data Manifold Given a new\ninput x, we use the retrieval engine F to retrieve the\nexamples x′\nk’s from D′ that are closest to x: F(x) =\n{x′\n1,...,x ′\nK}. We then build a feature-space convex hull\nby\nC(F(x)) =\n{K∑\nk=1\nαkφ(x′\nk)\n⏐⏐⏐⏐⏐\nK∑\nk=1\nαk = 1∧∀k: αk ≥0\n}\n.\nAs observed earlier, linear interpolation of two input vec-\ntors in the feature space of a deep neural network often\ncorresponds to a plausible input vector, unlike when inter-\npolation was done in the raw input space (see, e.g., Bengio\net al., 2013; Kingma & Welling, 2013; Radford et al., 2015).\nBased on this observation, we consider the feature-space\nconvex hull C(F(x)) as a reasonable local approximation to\nthe underlying data manifold.\nTrainable Projection Exact projection of the inputxonto\nthis convex hull C(F(x)) requires expensive optimization,\nespecially in the high-dimensional space. As we consider a\ndeep neural network classiﬁer, the dimension of the feature\nspace φ′ could be hundreds or more, making this exact\nprojection computationally infeasible. Instead, we propose\nto learn a goal-driven projection procedure based on the\nattention mechanism (Bahdanau et al., 2014).\nWe compare each input x′\nk ∈F(x) against xand compute\na score:\nβk = φ(x′\nk)⊤Uφ(x),\nwhere U is a trainable weight matrix (Luong et al., 2015).\nThese scores are then normalized to form a set of coef-\nﬁcients: αk = exp(βk)∑K\nk′=1 exp(βk′ ) .These coefﬁcients αk’s are\nthen used to form a projection point ofxin the feature-space\nconvex hull C(F(x)):\nP(x) =PC(F(x))(x) =\nK∑\nk=1\nαkφ(x′\nk).\nThis trainable projection could be thought of as learn-\ning to project an off-manifold example on the locally-\napproximated manifold to maximize the classiﬁcation accu-\nracy.\nClassiﬁcation The projected feature PC(F(x))(x) now\nrepresents the original input x and is fed to a ﬁnal clas-\nsiﬁer g. In other words, we constrain the ﬁnal classiﬁer to\nwork only with a point inside a feature-space convex hull\nof neighboring training examples. This constraint alleviates\nthe issue of the classiﬁer’s misbehavior in the region outside\nthe data manifold up to a certain degree.1\n2.3. Training\nThe output of the classiﬁer g(P(x)) is almost fully differ-\nentiable with respect to the classiﬁer g, both of the features\nextractors (φ′ and φ) and the attention weight matrix U,\nexcept for the retrieval engine F.2 This allows us to train\nthe entire pipeline in the previous section using backpropa-\ngation (Rumelhart et al., 1986) and gradient-based optimiza-\ntion.\nLocal Mixup This is however not enough to ensure the\nrobustness of the proposed approach to on-manifold ad-\nversarial examples. During training, the classiﬁer g only\nobserves a very small subset of any feature-space convex\nhull. Especially in a high-dimensional space, this greatly\nincrease the chance of the classiﬁer’s misbehavior within\nthese feature-space convex hulls, as also noted by Gilmer\n1 The quality of the local approximation may not be uniformly\nhigh across the input space, and we do not claim that it solves the\nproblem of off-manifold adversarial examples.\n2 We believe the introduction of this non-differentiable, black-\nbox retrieval engine further contributes to the increased robustness\nagainst white-box attacks.\nRetrieval-augmented Convolutional Neural Networks against adversarial examples\net al. (2018). In order to address this issue, we propose\nto augment learning with a local variant of the recently\nproposed mix-up algorithm (Zhang et al., 2017).\nThe goal of original mixup is to encourage a classiﬁer to\nact linearly between any pair of training examples. This is\ndone by linearly mixing in two randomly-drawn training\nexamples and creating a new linearly-interpolated exam-\nple pair during training. Let two randomly-drawn pairs\nbe (xi,yi) and (xj,yj), where yi and yj are one-hot vec-\ntors in the case of classiﬁcation. Mixup creates a new pair\n(λxi+ (1−λ)xj,λyi+ (1−λ)yj) and uses it as a training\nexample, where λ∈[0,1] is a random sample from a beta\ndistribution. We call this original version global mixup, as\nit increases the linearity of the classiﬁer between any pair\nof training examples.\nIt is however unnecessary for our purpose to use global\nmixup, as our goal is to make the classiﬁer better be-\nhave (i.e., linearly behave) within a feature-space con-\nvex hull C(F(x)). Thus, we use a local mixup in\nwhich we uniformly sample the convex coefﬁcients\nαk’s at random to create a new mixed example pair\n(∑K\nk=1 αkφ(x′\nk),∑K\nk=1 αky′\nk). We use the Kraemer Al-\ngorithm (see Sec. 4.2 in Smith & Tromble, 2004).\nOverall We use stochastic gradient descent (SGD) to train\nthe proposed network. At each update, we perform NCE\ndescent steps for the usual classiﬁcation loss, and NMU\ndescent steps for the proposed local mixup.\n2.4. Retrieval Engine F\nThe proposed approach does not depend on the speciﬁcs\nof a retrieval engine F. Any off-the-shelf retrieval engine\nthat supports dense vector lookup could be used, enabling\nthe use of a very large-scale D′with latest fast dense vector\nlookup algorithms, such as FAISS (Johnson et al., 2017).\nIn this work, we used a more rudimentary retrieval engine\nbased on locality-sensitive hashing (LSH; see, e.g., Datar\net al., 2004) with a reduced feature dimension using ran-\ndom projection (see, e.g., Bingham & Mannila, 2001, and\nreferences therein), as the sizes of candidate sets D′in the\nexperiments contain approximately 1M or less examples.\nThe key φ′(x) from Eq. (1) was chosen to be a pretrained\ndeep neural network without the ﬁnal fully-connected clas-\nsiﬁer layers (Krizhevsky et al., 2012; He et al., 2016).\n3. Adversarial Attack\n3.1. Attack Scenarios\nScenario 1 (Direct Attack) In this work, we consider the\ncandidate set D′and the retrieval engine which indexes it to\nbe “hidden” from the outside world. This property makes a\nusual white-box attack more of a gray-box attack in which\nthe attacker has access to the entire system except for the\nretrieval part. This is our ﬁrst attack scenario.\nScenario 2 (Retrieval Attack) Despite the hidden nature\nof the retrieval engine and the candidate set, it is possible for\nthe attacker to confuse the retrieval engine, if she/he could\naccess the feature extractor φ′. We furthermore give the\nattacker the access not only toφ′but the original classiﬁer g′\nwhich was tuned together with φ′. This allows the attacker\nto create an adversarial example on g′(φ′(x)) that could\npotentially disrupt the retrieval process, thereby fooling the\nproposed network. Although this is unlikely in practice,\nwe test this second scenario to investigate the possibility of\ncompromising the retrieval engine.\n3.2. Attack Methods\nUnder each of these scenarios, we evaluate the robustness of\nthe proposed approach on the ﬁve widely used/tested adver-\nsarial attack algorithms including both white-box and black-\nbox attacks. They are fast gradient sign method (FGSM,\nGoodfellow et al., 2014b), its iterative variant (iFGSM,\nKurakin et al., 2016), DeepFool (Moosavi-Dezfooli et al.,\n2016), L-BFGS (Tabacof & Valle, 2016) and Bound-\nary (Brendel et al., 2017). We acknowledge that this is\nnot an exhaustive list of attacks, however ﬁnd it to be ex-\ntensive enough to empirically evaluate the robustness of the\nproposed approach.\nFast Gradient Sign Method (FGSM) FGSM creates an\nadversarial example by adding the scaled sign of the gradient\nof the loss function Lcomputed using a target class ˆyto the\ninput:\nx′= x+ ϵ·sign(∇xL(x,ˆy)),\nwhere the scaleϵcontrols the difference between the original\ninput xand its adversarial version x′. This is a white-box\nattack, requiring the availability of the gradient of the loss\nfunction with respect to the input.\nIterative FGSM (iFGSM) iFGSM improves upon the\nFGSM by iteratively modifying the original input x for\na ﬁxed number Sof steps. At each step,\nx(s) = x(s−1) + ϵ\nSsign(∇xL(x(s−1),y)),\nwhere s = 1,...,S and x0 = x. Similarly to the FGSM,\nthe iFGSM is a white-box attack.\nDeepFool Moosavi-Dezfooli et al. (2016) proposed to\ncreate an adversarial example by ﬁnding a residual vector\nr∈Rdim(x) with the minimum Lp-norm with the constraint\nthat the output of a classiﬁer must ﬂip. They presented an\nefﬁcient iterative procedure to ﬁnd such a residual vector.\nSimilarly to the FGSM and iFGSM, this approach relies\nRetrieval-augmented Convolutional Neural Networks against adversarial examples\non the gradient of the classiﬁer’s output with respect to the\ninput, and is hence a white-box attack.\nL-BFGS Tabacof & Valle (2016) proposed an\noptimization-based approach, similar to DeepFool\nabove, however, more explicitly constraining the input to lie\ninside a tight box deﬁned by training examples. They use\nL-BFGS-B (Zhu et al., 1997) to solve this box-constrained\noptimization problem. This is also a white-box attack.\nBoundary Brendel et al. (2017) proposed a powerful\nblack-box attack, or more speciﬁcally decision-based at-\ntack, that requires neither the gradient of a classiﬁer nor\nthe predictive distribution. It only requires the ﬁnal deci-\nsion of the classiﬁer. Starting from an adversarial example,\npotentially far away from the original input, it iteratively\nsearches for a next adversarial example that has a smaller\ndifference to the original input. This procedure guarantees\nthe reduction in the difference by rejecting any step that\nneither decreases the difference nor makes the example not\nadversarial.\nImplementation We use Foolbox 3 released by Rauber\net al. (2017). Whenever necessary for further analysis, such\nas the accuracy per the amount of adversarial perturbation,\nwe implement some of these attacks ourselves.\n4. Related Work\nSince the phenomenon of adversarial examples was noticed\nby Szegedy et al. (2013), there have been a stream of at-\ntempts at making a deep neural network more robust. Most\nof the existing work are orthogonal to the proposed approach\nhere and could be used together. We however detail them\nhere to demonstrate similarities and contrasts against our\napproach.\n4.1. Input Transformation\nAn off-manifold adversarial example can be avoided, if it\ncould be projected onto the data manifold, characterized by\ntraining examples. This could be thought of as transforming\nan input. There have been two families of algorithms in this\ndirection.\nData-Independent Transformation The ﬁrst family of\ndefense mechanisms aims at reducing the input space so as\nto minimize regions that are off the data manifold. Dziugaite\net al. (2016) demonstrated that JPEG-compressed images\nsuffer less from adversarial attacks. Lu et al. (2017) suggest\nthat trying various scaling of an image size could overcome\nadversarial attacks, as they seem to be sensitive to the scal-\n3 Available at http://foolbox.readthedocs.io/\nen/latest/. Revision 2d468cb6.\ning of objects. Guo et al. (2017) uses an idea of compressed\nsensing to transform an input image by reconstructing it\nfrom its lower-resolution version while minimizing the total\nvariation (Rudin et al., 1992). More recently, Jacob Buck-\nman (2018) proposed to discretize each input dimension\nusing thermometer coding. These approaches are attractive\ndue to their simplicity, but there have some work showing\nthat it is often not enough to defend against sophisticated\nadversarial examples (see, e.g., Shin & Song, 2017).\nData-Dependent Transformation On the other hand,\nvarious groups have tried using a data-dependent transfor-\nmation mostly relying on density estimation. Gu & Rigazio\n(2014) used a denoising autoencoder (Vincent et al., 2010) to\npush an input back toward the data manifold. Pouya Saman-\ngouei (2018) and Song et al. (2017) respectively use a pixel-\nCNN (van den Oord et al., 2016) and generative adversarial\nnetwork (Goodfellow et al., 2014a) to replace an input image\nwith a nearby, likely image. Instead of using a separately\ntrained generative model, Guo et al. (2017) uses a tech-\nnique of image quilting (Efros & Freeman, 2001). These\napproaches are similar to our use of a retrieval engine over\nthe candidate set. They however do not attempt at address-\ning the issue of misbehaviors of a classiﬁer on the data\nmanifold.\n4.2. Attack-Aware Learning\nAnother direction has been on modifying a learning algo-\nrithm to make a classiﬁer more robust to adversarial exam-\nples. As our approach relies on usual backpropagation with\nstochastic gradient descent, most of the approaches below,\nas well as above, are readily used together.\nAdversarial Training Already early on, Goodfellow et al.\n(2014b) proposed a procedure of adversarial training, where\na classiﬁer is trained on both training examples and ad-\nversarial examples generated on-the-ﬂy. Lee et al. (2017)\nextended this procedure by introducing a generative adver-\nsarial network (GAN, Goodfellow et al., 2014a) that learns\nto generate adversarial examples while simultaneously train-\ning a classiﬁer. These approaches are generally applicable\nto any system that could be tuned frequently, and could be\nused to train the proposed model.\nRobust Optimization Instead of explicitly including ad-\nversarial examples during training, there have been attempts\nto modify a learning algorithm to induce robustness. Cisse\net al. (2017) proposed parseval training that encourages the\nLipschitz constant of each layer of a deep neural network\nclassiﬁer to be less than one. More recently, Aman Sinha\n(2018) proposed a tractable robust optimization algorithm\nfor training a deep neural net classiﬁer to be more robust to\nadversarial examples. This robust optimization algorithm\nRetrieval-augmented Convolutional Neural Networks against adversarial examples\nTable 1.The CIFAR-10 classiﬁers’ robustness to the adversarial attacks in the Scenario 2 (Retrieval Attack)\nClean FGSM iFGSM DeepFool\nL2 0 1e-04 2e-04 4e-04 1e-05 2e-05 8e-05 1e-05 2e-05 8e-05\nBaseline 85.15 14.05 7.5 4.22 55.2 26.17 2.59 26.04 11.72 0.34\nRaCNN-K5 72.57 42.97 34.29 24.55 72.57 72.48 45.46 64.34 61.34 60.96\nRaCNN-K5-mixup 75.6 46.37 37.9 28.11 74.89 74.89 48.12 66.96 63.84 63.55\nRaCNN-K10 79.52 52.95 43.9 33.77 79.12 79 55.27 72.89 71.81 71.14\nRaCNN-K10-mixup 80.80 53 44.01 33.47 79.87 79.72 54.36 73.63 72.35 71.26\nensures that the classiﬁer well-behaves in the neighborhood\nof each training point. It is highly relevant to the proposed\nlocal mixup which also aims at making a classiﬁer well-\nbehave between any pair of neighboring training examples.\n4.3. Retrieval-Augmented Neural Networks\nThe proposed approach tightly integrates an off-the-shelf\nretrieval engine into a deep neural network. This approach\nof retrieval-augmented deep learning has recently been pro-\nposed in various tasks. Gu et al. (2017) use a text-based\nretrieval engine to efﬁciently retrieve relevant training trans-\nlation pairs and let their non-parametric neural machine\ntranslation system seamlessly fuse an input sentence and\nthe retrieved pairs for better translation. Wang et al. (2017)\nproposed a similar approach to text classiﬁcation, and Guu\net al. (2017) to language modeling. More recently, Sprech-\nmann et al. (2018) applied this retrieval-based mechanism\nfor online learning, similarly to the earlier work by Li et al.\n(2016) in the context of machine translation.\n5. Experiments\n5.1. Settings\nDatasets We test the proposed approach (RaCNN) on\nthree datasets of different scales. CIFAR-10 has 50k train-\ning and 10k test examples, with 10 classes. SVHN has 73k\ntraining and 26k test examples, with 10 classes. ImageNet\nhas 1.3M training and 50k validation examples with 1,000\nclasses. For CIFAR-10 and ImageNet, we use the original\ntraining set as a candidate set, i.e., D′= D, while we use\nthe extra set of 531k examples as a candidate set in the\ncase of SVHN. The overall training process involves data\naugmentation on Dbut not D′.\nPretrained Feature Extractor φ′ We train a deep con-\nvolutional network for each dataset, remove the ﬁnal fully-\nconnected layers and use the remaining stack as a feature\nextractor φfor retrieval. This feature extractor is ﬁxed when\nused in the proposed RaCNN.\nRaCNN: Feature Extractor φand Classiﬁer g We use\nthe same convolutional network from above for the RaCNN\nFGSM\n iFGSM\nDeepFool\n L-BFGS\nBoundary\nFigure 1.The CIFAR-10 classi-\nﬁers’ robustness to the adversar-\nial attacks in the Scenario 1 (Di-\nrect Attack). The x-axis indi-\ncates the strength of attack in\nterms of the normalized L2 dis-\ntance. The y-axis corresponds\nto the accuracy.\nas well (separated into φand gby the ﬁnal average pooling)\nfor each dataset. For CIFAR-10 and SVHN, we train φand\ngfrom scratch. For ImageNet, on the other hand, we ﬁxφ=\nφ′and train g from the pretrained ResNet-18 above. The\nlatter was done, as we observed it greatly reduced training\ntime in the preliminary experiments.\nTraining We use Adam (Kingma & Ba, 2014) as an op-\ntimizer. We investigate the inﬂuence of the newly intro-\nduced components–retrieval and local mixup– by varying\nK ∈{5,10}and NMU ∈{0 (no mixup),5}.\nEvaluation In addition to the accuracy on the clean test\nset, we look at the accuracy per the amount of perturbation\nused to create adversarial examples. We use the default\nMeanSquaredDistance from the Foolbox library; this\namount is computed as a normalized L2 distance between\nRetrieval-augmented Convolutional Neural Networks against adversarial examples\nTable 2.The SVHN classiﬁers’ robustness to the adversarial attacks in the Scenario 2 (Retrieval Attack)\nClean FGSM iFGSM DeepFool\nL2 0 2e-04 4e-04 8e-04 2e-05 8e-05 2e-04 2e-05 8e-05 2e-04\nBaseline 95.48 42.09 30.95 21.61 70.41 35.53 11.17 51.10 16.00 4.28\nRaCNN-K5 90.78 64.87 53.31 39.44 90.73 75.80 63.41 84.62 81.30 80.55\nRaCNN-K5-mixup 91.64 68.31 57.20 43.73 91.55 77.74 65.75 86.18 83.20 82.43\nRaCNN-K10 92.19 64.94 52.24 37.73 92.10 76.41 62.70 86.18 84.25 82.21\nRaCNN-K10-mixup 92.49 68.72 57.30 43.49 92.45 78.26 65.50 87.33 84.73 84.10\nthe original example xand its perturbed version ˜x:\nL2(x,˜x) = ∥x−˜x∥2\n2\ndim(x) ∗\n(\nmax(x) −min(x)\n)2 .\nWe further notice that our attacks are generally performed\nwith clipping the outbounded pixel values at each step.\n5.2. CIFAR-10\nModel In the CIFAR-10 experiments, our model contains\n6 convolutional layers followed by 2 fully-connected layers.\nEvery layer is operated with batch normalization (Ioffe &\nSzegedy, 2015) and ReLU after. More details can be found\nin Appendix 7.\nScenario 1 (Direct Attack) We present in Fig. 1 the ef-\nfect of adversarial attacks with varying strengths (measured\nin the normalized L2 distance) on both the vanilla convolu-\ntional network (Baseline) and the proposed RaCNN’s with\nvarious settings. Across all ﬁve adversarial attacks, it is\nclear that the proposed RaCNN is more robust to adversarial\nexamples than the vanilla classiﬁer is. The proposed local\nmixup improves the robustness further, especially when the\nnumber of retrieved examples is small, i.e., K = 5. We\nconjecture that this is due to the quadratically increasing\nnumber of pairs, i.e., K(K−1)\n2 , for which local mixup must\ntake care of, with respect to K.\nScenario 2 (Retrieval Attack) In Table 1, we present the\naccuracies of both the baseline and RaCNN’s with varying\nstrengths of white-box attacks, when the feature extractor\nφ′for the retrieval engine is attacked. We observe that it is\nindeed possible to fool the proposed RaCNN by attacking\nthe retrieval process. Comparing Fig. 1 and Table 1, we\nhowever notice that the performance degradation is much\nless severe in this second scenario.\n5.3. SVHN\nModel We use the same architecture and hyper-parameter\nsetting as in the CIFAR-10 experiments.\nScenario 1 (Direct Attack) On SVHN, we observe a sim-\nilar trend from CIFAR-10. The proposed RaCNN is more\nFGSM\n iFGSM\nDeepFool\n L-BFGS\nBoundary\nFigure 2.The SVHN classiﬁers’\nrobustness to the adversarial at-\ntacks in the Scenario 1 (Direct\nAttack). The x-axis indicates\nthe strength of attack in terms\nof the normalized L2 distance.\nThe y-axis corresponds to the\naccuracy.\nrobust against all the adversarial attacks compared to the\nvanilla convolutional network. Similarly to CIFAR-10, the\nproposed approach is most robust to DeepFool and Bound-\nary, while it is most susceptible to L-BFGS. We however\nnotice that the impact of local mixup is larger with SVHN\nthan was with CIFAR-10.\nAnother noticeable difference is the impact of the number of\nretrieved examples on the classiﬁcation accuracy. In the case\nof CIFAR-10, the accuracies on the clean test examples (the\nﬁrst column in Table 1) between using 5 and 10 retrieved\nexamples differ signiﬁcantly, while it is much less so with\nSVHN (the ﬁrst column in Table 2.) We conjecture that\nthis is due to a lower level of variation in input examples\nin SVHN, which are pictures of house numbers taken from\nstreets, compared to those in CIFAR-10, which are pictures\nof general objects.\nRetrieval-augmented Convolutional Neural Networks against adversarial examples\nTable 3.The ImageNet classiﬁers’ robustness to the adversarial attacks in the Scenario 2 (Retrieval Attack).\nClean FGSM iFGSM DeepFool\nL2 0 1e-04 2e-04 4e-04 1e-05 2e-05 4e-05 1e-05 2e-05 4e-05\nBaseline 88.98 15 13.12 11.65 9.59 3.57 1.82 0.29 0.17 0.16\nRaCNN-K10-mixup 77.68 20.17 17.40 14.70 77.28 64.97 17.67 35.74 35.72 35.71\nFGSM\n iFGSM\nDeepFool\n L-BFGS\nBoundary\nFigure 3.The ImageNet classi-\nﬁers’ robustness to the adversar-\nial attacks in the Scenario 1 (Di-\nrect Attack). The x-axis indi-\ncates the strength of attack in\nterms of the normalized L2 dis-\ntance. The y-axis corresponds\nto the accuracy. The adversary\nutilizes top-5 accuracies for at-\ntacks.\nScenario 2 (Retrieval Attack) We observe a similar\ntrend between CIFAR-10 and SVHN, when the feature ex-\ntractor φ′for retrieval was attacked, as shown in Tables 1–2.\n5.4. ImageNet\nModel We use ResNet-18 (He et al., 2016). We pretrain\nit as a standalone classiﬁer on ImageNet and use the feature\nextractor part φ′ for retrieval. We use the same feature\nextractor φ= φ′for the RaCNN without updating it. The\nclassiﬁer gis initialized with g′and tuned during training.\nIn the case of ImageNet, we only try K = 10 retrieved\nexamples with local mixup. Due to the high computational\ncost of the L-BFGS and Boundary attacks, we evaluate\nboth the vanilla classiﬁer and RaCNN against these two\nattacks on 200 images drawn uniformly at random from the\nvalidation set. We use Accuracy@5 which is a standard\nmetric with ImageNet.\nScenario 1 (Direct Attack) A general trend with Ima-\ngeNet is similar to that with either CIFAR-10 or SVHN, as\ncan be seen in Fig. 3. The proposed RaCNN is more robust\nto adversarial attacks. We however do observe some dif-\nferences. First, iFGSM is better at compromising both the\nbaseline and RaCNN than L-BFGS is, in this case. Second,\nDeepFool is much more successful at fooling the baseline\nconvolutional network on ImageNet than on the other two\ndatasets, but is much less so at fooling the proposed RaCNN.\nScenario 2 (Retrieval Attack) Unlike CIFAR-10 and\nSVHN, we have observed that the retrieval attack is some-\ntimes more effective than the direct attack in the case of\nImageNet. For instance, FGSM can compromise the re-\ntrieval feature extractor φ′to decrease the accuracy from\n77.68 down to 0.20 at L2 = 10−4. We observed a similar\nbehavior with DeepFool, but not with iFGSM.\n5.5. Discussion\nIn summary, we have observed that the proposed RaCNN,\nwhen trained with the local mixup, is more robust to ad-\nversarial attacks, at least those ﬁve considered in the ex-\nperiments, than the vanilla convolutional network. More\nspeciﬁcally, the RaCNN was most robust to the black-box,\ndecision-based attach (Brendel et al., 2017), while it was\nmore easily compromised by white-box attacks, especially\nby the L-BFGS attack (Tabacof & Valle, 2016) which relies\non a strong, quasi-Newton optimizer. This suggests that\nthe RaCNN could be an attractive alternative to the vanilla\nconvolutional network when deployed, for instance, in a\ncloud-based environment.\nIn Fig. 4, we show retrieval results given a query image from\nImageNet. Although adversarial attack did indeed alter the\nretrieval engine’s behavior, we see that the semantics of\nthe original query image could still be maintained in those\nsets of retrieved images, suggesting two insights. First, the\nrobustness of the RaCNN is largely due to the robustness\nof the retrieval engine to small perturbation in the input.\nEven when the retrieval quality degrades, we observe that a\nmajority of retrieved examples are of the same, or a similar,\nclass. Second, we could further improve the robustness by\ndesigning the feature extractor φ′for the retrieval engine\nmore carefully. For instance, an identity function φ′(x) =\nx would correspond to retrieval based on the raw pixels,\nRetrieval-augmented Convolutional Neural Networks against adversarial examples\nClean\niFGSM (Scenario 1 – Direct Attack) with L2 = 2× 10−5\niFGSM (Scenario 2 – Retrieval Attack) with L2 = 2× 10−5\niFGSM (Scenario 1 – Direct Attack) with L2 = 4× 10−5\niFGSM (Scenario 2 – Retrieval Attack) with L2 = 4× 10−5\nFigure 4.On the left-most column shows the query image, and the next ten images have been retrieved byF. We show the retrieval results\nusing the original image and the adversarial images one row at a time. With the amount of injected noise high enough to fool any vanilla\nconvolutional network, the behavior of the retrieval engine changes however largely maintains the semantics of the query image. That is,\nmost of the retrieved images contain ﬁsh, although speciﬁc species may change.\nwhich would make the retrieval engine extremely robust\nto any adversarial attack imperceptible to humans. This\nmay however results in a lower accuracy on clean examples,\nwhich is a trade-off that needs to be determined per task.\nAs have been observed with the existing input transforma-\ntion based defense strategies, the robustness of the proposed\nRaCNN comes at the expense of the generalization perfor-\nmance on clean input examples. We have observed however\nthat this degradation could be controlled at the expense of\ncomputational overhead by varying the number of retrieved\nexamples per input. This controllability could be an impor-\ntant feature when deploying such a model in production.\n6. Conclusion\nIn this paper, we proposed a novel retrieval-augmented con-\nvolutional network classiﬁer (RaCNN) that integrates an off-\nthe-shelf retrieval engine to counter adversarial attacks. The\nRaCNN was designed to tackle both off- and on-manifold\nadversarial examples, and to do so, we use a retrieval engine\nto locally characterize the data manifold as a feature-space\nconvex hull and the attention mechanism to project the in-\nput onto this convex hull. The entire model, composed of\nthe retrieval engine and a deep convolutional network, is\ntrained jointly, and we introduced the local mixup learning\nstrategy to encourage the classiﬁer to behave linearly on the\nfeature-space convex hull.\nWe have evaluated the proposed approach on three stan-\ndard object recognition benchmarks–CIFAR-10, SVHN and\nImageNet– against four white-box adversarial attacks and\none black-box, decision-based attack. The experiments have\nrevealed that the proposed approach is indeed more robust\nthan the vanilla convolutional network in all the cases. The\nRaCNN was found to be especially robust to the black-box,\ndecision-based attack, suggesting its potential for the cloud-\nbased deployment scenario.\nThe proposed approach consists of three major components;\n(1) local characterization of data manifold, (2) data mani-\nfold projection and (3) regularized learning on the manifold.\nThere is a large room for improvement in each of these com-\nponents. For instance, a feature-space convex hull may be\nreplaced with a more sophisticated kernel estimator. Projec-\ntion onto the convex hull could be done better, and a learning\nalgorithm better than local mixup could further improve the\nrobustness against on-manifold adversarial examples. We\nleave these possibilities as future work.\nRetrieval-augmented Convolutional Neural Networks against adversarial examples\nReferences\nAman Sinha, Hongseok Namkoong, John Duchi. Certiﬁ-\nable distributional robustness with principled adversarial\ntraining. International Conference on Learning Repre-\nsentations, 2018.\nBahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua.\nNeural machine translation by jointly learning to align\nand translate. arXiv preprint arXiv:1409.0473, 2014.\nBengio, Yoshua, Mesnil, Gr ´egoire, Dauphin, Yann, and\nRifai, Salah. Better mixing via deep representations.\nIn Proceedings of the 30th International Conference on\nMachine Learning (ICML-13), 2013.\nBingham, Ella and Mannila, Heikki. Random projection in\ndimensionality reduction: applications to image and text\ndata. In KDD. ACM, 2001.\nBrendel, Wieland, Rauber, Jonas, and Bethge, Matthias.\nDecision-based adversarial attacks: Reliable attacks\nagainst black-box machine learning models. arXiv\npreprint arXiv:1712.04248, 2017.\nCisse, Moustapha, Bojanowski, Piotr, Grave, Edouard,\nDauphin, Yann, and Usunier, Nicolas. Parseval networks:\nImproving robustness to adversarial examples. In Inter-\nnational Conference on Machine Learning, 2017.\nDatar, Mayur, Immorlica, Nicole, Indyk, Piotr, and Mir-\nrokni, Vahab S. Locality-sensitive hashing scheme based\non p-stable distributions. In Proceedings of the twentieth\nannual symposium on Computational geometry. ACM,\n2004.\nDeng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai,\nand Fei-Fei, Li. Imagenet: A large-scale hierarchical\nimage database. In Computer Vision and Pattern Recog-\nnition, 2009. CVPR 2009. IEEE Conference on, 2009.\nDziugaite, Gintare Karolina, Ghahramani, Zoubin, and Roy,\nDaniel M. A study of the effect of jpg compression on\nadversarial images. arXiv preprint arXiv:1608.00853,\n2016.\nEfros, Alexei A and Freeman, William T. Image quilting for\ntexture synthesis and transfer. In Proceedings of the 28th\nannual conference on Computer graphics and interactive\ntechniques. ACM, 2001.\nGilmer, Justin, Metz, Luke, Faghri, Fartash, Schoenholz,\nSamuel S, Raghu, Maithra, Wattenberg, Martin, and\nGoodfellow, Ian. Adversarial spheres. arXiv preprint\narXiv:1801.02774, 2018.\nGoodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu,\nBing, Warde-Farley, David, Ozair, Sherjil, Courville,\nAaron, and Bengio, Yoshua. Generative adversarial nets.\nIn Advances in neural information processing systems,\n2014a.\nGoodfellow, Ian J, Shlens, Jonathon, and Szegedy, Christian.\nExplaining and harnessing adversarial examples. arXiv\npreprint arXiv:1412.6572, 2014b.\nGu, Jiatao, Wang, Yong, Cho, Kyunghyun, and Li, Vic-\ntor OK. Search engine guided non-parametric neural\nmachine translation. arXiv preprint arXiv:1705.07267,\n2017.\nGu, Shixiang and Rigazio, Luca. Towards deep neural net-\nwork architectures robust to adversarial examples. arXiv\npreprint arXiv:1412.5068, 2014.\nGuo, Chuan, Rana, Mayank, Cisse, Moustapha, and van der\nMaaten, Laurens. Countering adversarial images using\ninput transformations. arXiv preprint arXiv:1711.00117,\n2017.\nGuu, Kelvin, Hashimoto, Tatsunori B, Oren, Yonatan, and\nLiang, Percy. Generating sentences by editing prototypes.\narXiv preprint arXiv:1709.08878, 2017.\nHe, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,\nJian. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision\nand pattern recognition, 2016.\nIoffe, Sergey and Szegedy, Christian. Batch normalization:\nAccelerating deep network training by reducing internal\ncovariate shift. In International conference on machine\nlearning, 2015.\nJacob Buckman, Aurko Roy, Colin Raffel Ian Goodfellow.\nThermometer encoding: One hot way to resist adversarial\nexamples. International Conference on Learning Repre-\nsentations, 2018.\nJohnson, Jeff, Douze, Matthijs, and J´egou, Herv´e. Billion-\nscale similarity search with gpus. arXiv preprint\narXiv:1702.08734, 2017.\nKingma, Diederik P and Ba, Jimmy. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980,\n2014.\nKingma, Diederik P and Welling, Max. Auto-encoding\nvariational bayes. arXiv preprint arXiv:1312.6114, 2013.\nKrizhevsky, Alex and Hinton, Geoffrey. Learning multiple\nlayers of features from tiny images. Technical report,\n2009.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.\nImagenet classiﬁcation with deep convolutional neural\nnetworks. In Advances in neural information processing\nsystems, 2012.\nRetrieval-augmented Convolutional Neural Networks against adversarial examples\nKurakin, Alexey, Goodfellow, Ian, and Bengio, Samy. Ad-\nversarial examples in the physical world. arXiv preprint\narXiv:1607.02533, 2016.\nLeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner,\nPatrick. Gradient-based learning applied to document\nrecognition. Proceedings of the IEEE, 1998.\nLee, Hyeungill, Han, Sungyeob, and Lee, Jungwoo. Gen-\nerative adversarial trainer: Defense to adversarial per-\nturbations with gan. arXiv preprint arXiv:1705.03387,\n2017.\nLi, Xiaoqing, Zhang, Jiajun, and Zong, Chengqing. One\nsentence one model for neural machine translation. arXiv\npreprint arXiv:1609.06490, 2016.\nLu, Jiajun, Sibai, Hussein, Fabry, Evan, and Forsyth, David.\nNo need to worry about adversarial examples in ob-\nject detection in autonomous vehicles. arXiv preprint\narXiv:1707.03501, 2017.\nLuong, Minh-Thang, Pham, Hieu, and Manning, Christo-\npher D. Effective approaches to attention-based neural\nmachine translation. arXiv preprint arXiv:1508.04025,\n2015.\nMoosavi-Dezfooli, Seyed-Mohsen, Fawzi, Alhussein, and\nFrossard, Pascal. Deepfool: a simple and accurate method\nto fool deep neural networks. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\n2016.\nNetzer, Yuval, Wang, Tao, Coates, Adam, Bissacco, Alessan-\ndro, Wu, Bo, and Ng, Andrew Y . Reading digits in natural\nimages with unsupervised feature learning. In NIPS work-\nshop on deep learning and unsupervised feature learning,\n2011.\nPouya Samangouei, Maya Kabkab, Rama Chellappa.\nDefense-GAN: Protecting classiﬁers against adversarial\nattacks using generative models. International Confer-\nence on Learning Representations, 2018.\nRadford, Alec, Metz, Luke, and Chintala, Soumith. Un-\nsupervised representation learning with deep convolu-\ntional generative adversarial networks. arXiv preprint\narXiv:1511.06434, 2015.\nRauber, Jonas, Brendel, Wieland, and Bethge, Matthias.\nFoolbox v0. 8.0: A python toolbox to benchmark the\nrobustness of machine learning models. arXiv preprint\narXiv:1707.04131, 2017.\nRudin, Leonid I, Osher, Stanley, and Fatemi, Emad. Nonlin-\near total variation based noise removal algorithms. Phys-\nica D: nonlinear phenomena, 1992.\nRumelhart, David E, Hinton, Geoffrey E, and Williams,\nRonald J. Learning representations by back-propagating\nerrors. Nature, 1986.\nShin, R and Song, D. Jpeg-resistant adversarial images.\nIn MAchine LEarning and Computer Security Workshop,\n2017.\nSmith, Noah A and Tromble, Roy W. Sampling uniformly\nfrom the unit simplex. Johns Hopkins University, Tech.\nRep, 2004.\nSong, Yang, Kim, Taesup, Nowozin, Sebastian, Ermon,\nStefano, and Kushman, Nate. Pixeldefend: Leveraging\ngenerative models to understand and defend against ad-\nversarial examples. arXiv preprint arXiv:1710.10766,\n2017.\nSprechmann, Pablo, Jayakumar, Siddhant, Rae, Jack, Pritzel,\nAlexander, Badia, Adria Puigdomenech, Uria, Benigno,\nVinyals, Oriol, Hassabis, Demis, Pascanu, Razvan, and\nBlundell, Charles. Memory-based parameter adaptation.\nInternational Conference on Learning Representations,\n2018.\nSzegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya,\nBruna, Joan, Erhan, Dumitru, Goodfellow, Ian, and Fer-\ngus, Rob. Intriguing properties of neural networks. arXiv\npreprint arXiv:1312.6199, 2013.\nTabacof, Pedro and Valle, Eduardo. Exploring the space of\nadversarial images. In Neural Networks (IJCNN), 2016\nInternational Joint Conference on. IEEE, 2016.\nvan den Oord, Aaron, Kalchbrenner, Nal, Espeholt, Lasse,\nVinyals, Oriol, Graves, Alex, et al. Conditional image\ngeneration with pixelcnn decoders. InAdvances in Neural\nInformation Processing Systems, 2016.\nVincent, Pascal, Larochelle, Hugo, Lajoie, Isabelle, Bengio,\nYoshua, and Manzagol, Pierre-Antoine. Stacked denois-\ning autoencoders: Learning useful representations in a\ndeep network with a local denoising criterion. Journal of\nMachine Learning Research, 2010.\nWang, Zhiguo, Hamza, Wael, and Song, Linfeng. k-nearest\nneighbor augmented neural networks for text classiﬁca-\ntion. arXiv preprint arXiv:1708.07863, 2017.\nZhang, Hongyi, Cisse, Moustapha, Dauphin, Yann N, and\nLopez-Paz, David. mixup: Beyond empirical risk mini-\nmization. arXiv preprint arXiv:1710.09412, 2017.\nZhu, Ciyou, Byrd, Richard H, Lu, Peihuang, and Nocedal,\nJorge. Algorithm 778: L-BFGS-B: Fortran subroutines\nfor large-scale bound-constrained optimization. ACM\nTransactions on Mathematical Software (TOMS), 1997.\nRetrieval-augmented Convolutional Neural Networks against adversarial examples\n7. Appendix: Model details\nOur CIFAR-10 and SVHN model spec is listed in the fol-\nlowing table.\nStage Architecture Size\nFeature extractor 96 3x3 convolution 96 x 30 x 30\nφ batch normalization\n96 3x3 convolution 96 x 28 x 28\nbatch normalization & ReLU\n96 3x3 convolution with stride\n2x2\n96 x 13 x 13\nbatch normalization & ReLU\n192 3x3 convolution 192 x 11 x 11\nbatch normalization & ReLU\n192 3x3 convolution with stride\n2x2\n192 x 4 x 4\nbatch normalization\nAttention 256 4x4 convolution 256\nConvex-sum (with attention\nmechanism U or local mixup)\n256\nClassiﬁcation fully-connected layer 256 x 64 64\nbatch normalization & ReLU\nfully-connected layer 64 x 10 10\nThe pretrained retrieval index building φ′network is listed\nas follow:\nStage Architecture Size\nFeature extractor 96 3x3 convolution 96 x 30 x 30\nφ′ batch normalization & ReLU\n96 3x3 convolution 96 x 28 x 28\nbatch normalization & ReLU\n96 3x3 convolution with stride\n2x2\n96 x 13 x 13\nbatch normalization & ReLU\n192 3x3 convolution 192 x 11 x 11\nbatch normalization & ReLU\n192 3x3 convolution with stride\n2x2\n192 x 4 x 4\nbatch normalization\nClassiﬁcation fully-connected layer 3072 x 512 512\nUsed only batch normalization & ReLU\nin Scenario 2 fully-connected layer 512 x 128 128\nbatch normalization & ReLU\nfully-connected layer 128 x 10 10",
  "topic": "Convolutional neural network",
  "concepts": [
    {
      "name": "Convolutional neural network",
      "score": 0.7875526547431946
    },
    {
      "name": "Computer science",
      "score": 0.7815239429473877
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.758817732334137
    },
    {
      "name": "Classifier (UML)",
      "score": 0.6458463668823242
    },
    {
      "name": "Adversarial system",
      "score": 0.6208090782165527
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6198461651802063
    },
    {
      "name": "Manifold (fluid mechanics)",
      "score": 0.43810778856277466
    },
    {
      "name": "Machine learning",
      "score": 0.42445462942123413
    },
    {
      "name": "Deep neural networks",
      "score": 0.41376882791519165
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.40577247738838196
    },
    {
      "name": "Deep learning",
      "score": 0.3354305624961853
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    }
  ]
}