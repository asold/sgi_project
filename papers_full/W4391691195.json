{
  "title": "Retrieval Augmented Generation Enabled Generative Pre-Trained Transformer 4 (GPT-4) Performance for Clinical Trial Screening",
  "url": "https://openalex.org/W4391691195",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2801375744",
      "name": "Ozan Ünlü",
      "affiliations": [
        "Brigham and Women's Hospital",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2139208674",
      "name": "Ji-Yeon Shin",
      "affiliations": [
        "Mass General Brigham",
        "Brigham and Women's Hospital"
      ]
    },
    {
      "id": null,
      "name": "Charlotte J Mailly",
      "affiliations": [
        "Mass General Brigham",
        "Brigham and Women's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5040441958",
      "name": "Michael F Oates",
      "affiliations": [
        "Mass General Brigham",
        "Brigham and Women's Hospital"
      ]
    },
    {
      "id": null,
      "name": "Michela R Tucci",
      "affiliations": [
        "Brigham and Women's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2032973993",
      "name": "Matthew Varugheese",
      "affiliations": [
        "Brigham and Women's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A3179288997",
      "name": "Kavishwar Wagholikar",
      "affiliations": [
        "Political Research Associates",
        "Mass General Brigham",
        "Brigham and Women's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A1983569093",
      "name": "Fei Wang",
      "affiliations": [
        "Brigham and Women's Hospital",
        "Mass General Brigham"
      ]
    },
    {
      "id": "https://openalex.org/A201525477",
      "name": "Benjamin M. Scirica",
      "affiliations": [
        "Harvard University",
        "Brigham and Women's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2986320532",
      "name": "Alexander J Blood",
      "affiliations": [
        "Harvard University",
        "Brigham and Women's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2159475191",
      "name": "Samuel J. Aronson",
      "affiliations": [
        "Mass General Brigham",
        "Brigham and Women's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2801375744",
      "name": "Ozan Ünlü",
      "affiliations": [
        "Brigham and Women's Hospital",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2139208674",
      "name": "Ji-Yeon Shin",
      "affiliations": [
        "Brigham and Women's Hospital",
        "Mass General Brigham"
      ]
    },
    {
      "id": null,
      "name": "Charlotte J Mailly",
      "affiliations": [
        "Brigham and Women's Hospital",
        "Mass General Brigham"
      ]
    },
    {
      "id": "https://openalex.org/A5040441958",
      "name": "Michael F Oates",
      "affiliations": [
        "Mass General Brigham",
        "Brigham and Women's Hospital"
      ]
    },
    {
      "id": null,
      "name": "Michela R Tucci",
      "affiliations": [
        "Brigham and Women's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2032973993",
      "name": "Matthew Varugheese",
      "affiliations": [
        "Brigham and Women's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A3179288997",
      "name": "Kavishwar Wagholikar",
      "affiliations": [
        "Brigham and Women's Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A1983569093",
      "name": "Fei Wang",
      "affiliations": [
        "Brigham and Women's Hospital",
        "Mass General Brigham"
      ]
    },
    {
      "id": "https://openalex.org/A201525477",
      "name": "Benjamin M. Scirica",
      "affiliations": [
        "Brigham and Women's Hospital",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2986320532",
      "name": "Alexander J Blood",
      "affiliations": [
        "Brigham and Women's Hospital",
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2159475191",
      "name": "Samuel J. Aronson",
      "affiliations": [
        "Brigham and Women's Hospital",
        "Mass General Brigham"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2574781439",
    "https://openalex.org/W4281752845",
    "https://openalex.org/W2013432740",
    "https://openalex.org/W3210886649",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4366341464",
    "https://openalex.org/W4387241391",
    "https://openalex.org/W4389944785",
    "https://openalex.org/W3211421995",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4225576545",
    "https://openalex.org/W2999309192",
    "https://openalex.org/W3126232929",
    "https://openalex.org/W4388591773",
    "https://openalex.org/W4285734150",
    "https://openalex.org/W4386758072",
    "https://openalex.org/W4285800458",
    "https://openalex.org/W1980870383",
    "https://openalex.org/W3124912334"
  ],
  "abstract": "ABSTRACT Background Subject screening is a key aspect of all clinical trials; however, traditionally, it is a labor-intensive and error-prone task, demanding significant time and resources. With the advent of large language models (LLMs) and related technologies, a paradigm shift in natural language processing capabilities offers a promising avenue for increasing both quality and efficiency of screening efforts. This study aimed to test the Retrieval-Augmented Generation (RAG) process enabled Generative Pretrained Transformer Version 4 (GPT-4) to accurately identify and report on inclusion and exclusion criteria for a clinical trial. Methods The Co-Operative Program for Implementation of Optimal Therapy in Heart Failure (COPILOT-HF) trial aims to recruit patients with symptomatic heart failure. As part of the screening process, a list of potentially eligible patients is created through an electronic health record (EHR) query. Currently, structured data in the EHR can only be used to determine 5 out of 6 inclusion and 5 out of 17 exclusion criteria. Trained, but non-licensed, study staff complete manual chart review to determine patient eligibility and record their assessment of the inclusion and exclusion criteria. We obtained the structured assessments completed by the study staff and clinical notes for the past two years and developed a workflow of clinical note-based question answering system powered by RAG architecture and GPT-4 that we named RECTIFIER (RAG-Enabled Clinical Trial Infrastructure for Inclusion Exclusion Review). We used notes from 100 patients as a development dataset, 282 patients as a validation dataset, and 1894 patients as a test set. An expert clinician completed a blinded review of patients’ charts to answer the eligibility questions and determine the “gold standard” answers. We calculated the sensitivity, specificity, accuracy, and Matthews correlation coefficient (MCC) for each question and screening method. We also performed bootstrapping to calculate the confidence intervals for each statistic. Results Both RECTIFIER and study staff answers closely aligned with the expert clinician answers across criteria with accuracy ranging between 97.9% and 100% (MCC 0.837 and 1) for RECTIFIER and 91.7% and 100% (MCC 0.644 and 1) for study staff. RECTIFIER performed better than study staff to determine the inclusion criteria of “symptomatic heart failure” with an accuracy of 97.9% vs 91.7% and an MCC of 0.924 vs 0.721, respectively. Overall, the sensitivity and specificity of determining eligibility for the RECTIFIER was 92.3% (CI) and 93.9% (CI), and study staff was 90.1% (CI) and 83.6% (CI), respectively. Conclusion GPT-4 based solutions have the potential to improve efficiency and reduce costs in clinical trial screening. When incorporating new tools such as RECTIFIER, it is important to consider the potential hazards of automating the screening process and set up appropriate mitigation strategies such as final clinician review before patient engagement.",
  "full_text": "   \n \n1 \n \nRetrieval Augmented Generation Enabled Generative Pre-Trained Transformer 4 (GPT-4) \nPerformance for Clinical Trial Screening \nOzan Unlu1,2,3,6*, Jiyeon Shin1,4*, Charlotte J Mailly1,4, Michael F Oates1,4, Michela R Tucci1, Matthew \nVarugheese1, Kavishwar Wagholikar1,5, Fei Wang1,4, Benjamin M Scirica1,2,6, Alexander J Blood‡,1,2,6, \nSamuel J Aronson‡,1,4 \n*Dr. Ozan Unlu and Jiyeon Shin contributed equally to this work as co-first authors.  \n‡Dr. Alexander J Blood and Samuel Aronson contributed equally to this work as co-senior authors. \n \n1. Accelerator for Clinical Transformation, Brigham and Women’s Hospital, Boston, MA \n2. Division of Cardiovascular Medicine, Brigham and Women’s Hospital, Boston, MA \n3. Department of Biomedical Informatics, Harvard Medical School, Boston, MA \n4. Mass General Brigham Personalized Medicine, Cambridge, MA \n5. Research Information Science and Computing, Mass General Brigham, Somerville, MA \n6. Harvard Medical School, Boston, MA \n \nCorresponding Author: \nAlexander J. Blood, MD, MSc \nBrigham and Women’s Hospital \n75 Francis Street \nBoston, MA 02115 \nAblood@bwh.harvard.edu \n \nKeywords: Generative Pre-Trained Transformer 4, GPT-4, Retrieval Augmented Generation, RAG, \nClinical Trial, Screening, Heart failure, Digital health \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.08.24302376doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n   \n \n2 \n \nABSTRACT \nBackground: Subject screening is a key aspect of all clinical trials; however, traditionally, it is a labor-\nintensive and error-prone task, demanding significant time and resources. With the advent of large \nlanguage models (LLMs) and related technologies, a paradigm shift in natural language processing \ncapabilities offers a promising avenue for increasing both quality and efficiency of screening efforts. This \nstudy aimed to test the Retrieval-Augmented Generation (RAG) process enabled Generative Pretrained \nTransformer Version 4 (GPT-4) to accurately identify and report on inclusion and exclusion criteria for a \nclinical trial.  \nMethods: The Co-Operative Program for Implementation of Optimal Therapy in Heart Failure \n(COPILOT-HF) trial aims to recruit patients with symptomatic heart failure. As part of the screening \nprocess, a list of potentially eligible patients is created through an electronic health record (EHR) query. \nCurrently, structured data in the EHR can only be used to determine 5 out of 6 inclusion and 5 out of 17 \nexclusion criteria. Trained, but non-licensed, study staff complete manual chart review to determine \npatient eligibility and record their assessment of the inclusion and exclusion criteria. We obtained the \nstructured assessments completed by the study staff and clinical notes for the past two years and \ndeveloped a workflow of clinical note-based question answering system powered by RAG architecture \nand GPT-4 that we named RECTIFIER (RAG-Enabled Clinical Trial Infrastructure for Inclusion \nExclusion Review). We used notes from 100 patients as a development dataset, 282 patients as a \nvalidation dataset, and 1894 patients as a test set. An expert clinician completed a blinded review of \npatients’ charts to answer the eligibility questions and determine the “gold standard” answers. We \ncalculated the sensitivity, specificity, accuracy, and Matthews correlation coefficient (MCC) for each \nquestion and screening method. We also performed bootstrapping to calculate the confidence intervals for \neach statistic. \nResults: Both RECTIFIER and study staff answers closely aligned with the expert clinician answers \nacross criteria with accuracy ranging between 97.9% and 100% (MCC 0.837 and 1) for RECTIFIER and \n91.7% and 100% (MCC 0.644 and 1) for study staff. RECTIFIER performed better than study staff to \ndetermine the inclusion criteria of “symptomatic heart failure” with an accuracy of 97.9% vs 91.7% and \nan MCC of 0.924 vs 0.721, respectively. Overall, the sensitivity and specificity of determining eligibility \nfor the RECTIFIER was 92.3% (CI) and 93.9% (CI), and study staff was 90.1% (CI) and 83.6% (CI), \nrespectively.  \nConclusion: GPT-4 based solutions have the potential to improve efficiency and reduce costs in clinical \ntrial screening. When incorporating new tools such as RECTIFIER, it is important to consider the \npotential hazards of automating the screening process and set up appropriate mitigation strategies such as \nfinal clinician review before patient engagement. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.08.24302376doi: medRxiv preprint \n   \n \n3 \n \nINTRODUCTION \nA critical step in conducting a clinical trial is screening potential subjects to ensure they are eligible based \non study-specific inclusion and exclusion criteria. Traditionally, screening for clinical trials is a manual \nprocess, relying heavily on the judgment and diligence of study staff and healthcare professionals. This \napproach, while thorough, is prone to human error, which can lead to inappropriate participant selection \nor exclusion, thus affecting the overall integrity of the trial 1–3. Furthermore, manual screening requires \nsubstantial human resources and time, contributing to the high costs and lengthy durations of clinical \ntrials3. \n \nRecent advances in natural language processing (NLP) have improved the screening process for clinical \ntrials 4 . NLP technologies have the potential to automate the extraction and analysis of relevant data from \nelectronic health records (EHRs), literature, and other sources, thereby enhancing the efficiency and \naccuracy of participant selection4. However, traditional NLP methods have limitations, particularly in \nhandling complex, unstructured data commonly found in EHRs4 which are often the basis for key \ninclusion and exclusion criteria. \n \nThe advent of large language models (LLMs), such as Generative Pre-trained Transformer 4 (GPT-4)5, in \naddition to their generative capabilities, has revolutionized the field of NLP6. These models, with their \nadvanced capabilities in comprehending and generating human-like text, have shown great promise in \nvarious applications, including in the medical field7. GPT-4, in particular, exhibits unprecedented skill in \nprocessing and interpreting both structured and unstructured data, making it an ideal candidate for \nenhancing clinical trial screening processes5,8,9. \n \nIn this study, we investigate the application of GPT-4 Vision within a specialized framework known as \nRetrieval Augmented Generation (RAG), which enables the practical implementation of a clinical trial \nscreening application in real-world scenarios.  Hereafter, the framework will be referred to as the RAG-\nEnabled Clinical Trial Infrastructure for Inclusion Exclusion Review, or RECTIFIER. Specifically, we \nassess the efficacy of RECTIFIER in identifying eligible participants, particularly in scenarios where \nunstructured data is prevalent and structured data may be incomplete or inaccurate. This research aims to \nvalidate the utility of GPT-4 enabled RECTIFIER as a tool within a clinical trial screening process to \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.08.24302376doi: medRxiv preprint \n   \n \n4 \n \nimprove the efficiency, accuracy, and reliability of the clinical trial screening process, potentially \ntransforming the current paradigm in clinical research methodologies. \n \nMETHODS \nPatient Population \nWe evaluated the capabilities of RECTIFIER in the Co-Operative Program for Implementation of \nOptimal Therapy in Heart Failure (COPILOT-HF) trial, a pragmatic, randomized, open-label intervention \ntrial to investigate the comparative effectiveness of two remote care strategies on optimizing the \nprescription of guideline-directed medical therapy in patients with HF(NCT05734690). The current \nprocess for identifying the cohort for the trial involves querying the EHR through the Mass General \nBrigham (MGB) Enterprise Data Warehouse (EDW)10. Trained, non-clinically licensed study staff \nperform manual chart review to determine patient eligibility and record their assessment of 6 inclusion \nand 17 exclusion criteria for the study. We reviewed each criterion and identified 5 out of 6 inclusion and \n5 out of 17 exclusion criteria that can be determined reliably based on structured data in the EHR \n(Supplemental Table 1). To assess the ability of RECTIFIER to screen patients for the remaining  \n(1 inclusion and 12 exclusion), we excluded patients who met exclusion criteria based on information \nidentifiable through structured data.  \n \nPreparation of Data and Development of Datasets \nThe COPILOT-HF Study operations team uses Microsoft Dynamics 365 (Version 2023 Release Wave 2) \nto capture the inclusion and exclusion criteria obtained during the screening process for every patient11. \nThe values (yes/no) entered by the licensed study staff are stored in structured fields for each question. \nWe extracted the data for the 1 inclusion and 12 exclusion criteria questions determined by the study \nstaff’s review of the medical records. During screening, study staff stop reviewing an individual patient \nand mark them as ineligible as soon as they meet one of the 17 exclusion criteria. Because of this process, \nthere were missing answers for the remaining exclusion criteria. To compare RECTIFIER vs. study staff \nperformance for screening, an expert clinician completed a blinded review of all patients and answered \nthe questions previously answered by the study staff, thus establishing the “gold standard” answers.  \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.08.24302376doi: medRxiv preprint \n   \n \n5 \n \nWe prepared datasets for three phases: development, validation, and test.  During development, we \ndesigned and evaluated various prompts to optimize their performance in identifying each inclusion and \nexclusion criteria. In the validation phase, we confirmed or rejected the improvements observed during \nthe development phase, refining each prompt based on its performance on the larger dataset.  Finally, in \nthe test phase, we assessed the final prompts’ performance on a larger number of patients to assess their \ngeneralizability. \n \nWe identified 3,000 patients screened by study staff, each with documented expected answers for \nprogram inclusion/exclusion.  For the development phase, we used 100 patients (50% eligible, 50% \nineligible) to design and evaluate various prompts to optimize their performance in identifying each \ninclusion and exclusion criteria. We aimed to include 400 semi-randomly selected patients in the \nvalidation phase by ensuring that it represented both negative and positive cases for each exclusion \ncriteria. After removing the patients with no answers to any of the 13 questions, 282 patients were left in \nthe validation dataset which was used to confirm or reject observed improvements in prompt \nperformance, refining the prompt based on its performance on the larger dataset.  Finally, we selected \n2,500 previously untested patients from the remaining dataset to assess the final and optimized prompts \non a larger scale to present our final performance results. After removing the patients with no answers to \nany of the 13 questions, there were 1,894 patients left in the test dataset. Out of 1,894 patients, 1509 had \navailable answers sufficient to decide if a patient was eligible or ineligible (“Yes” to at least one \nexclusion criteria question, or “No” to at least one exclusion criteria question and “Yes” or “No” to \ninclusion criteria question). Expert clinician review classified 1,162 of these patients as eligible and 347 \nas ineligible.   \n \nCreation of Model Architecture \nWe used GPT-4 Vision (Model version 1015) in this study which is referred to as GPT-4 throughout this \nmanuscript since we only used language capabilities of the model. We identified Retrieval-Augmented \nGeneration (RAG) architecture12 as a suitable solution for this study. First, we needed GPT-4 to be able to \naccess external data, namely clinical notes of patients. Second, we only wanted to invoke GPT-4 using the \nrelevant portions of the clinical notes. Using a RAG architecture allows leveraging clinical notes as an \nexternal data source and filter them to only include relevant context rather than the entire content; this \ncapability offers several advantages. On average, a patient in our study had 120 clinical notes in the past 2 \nyears, ranging from a single paragraph to several pages. For some patients, feeding these notes into  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.08.24302376doi: medRxiv preprint \n   \n \n6 \n \nGPT-4 would exceed its token context limit. While combining a few notes at a time could circumvent this \nlimitation, processing all of the notes associated with the average patient would still be necessary until the \nrelevant information is identified or confirmed absent. This approach presents two major drawbacks. \nFirst, transmitting all notes, regardless of relevance, introduces unnecessary processing and potential \ndelays. By contrast, a focused approach transmits an average of 1,000 tokens (see results section), \ntargeting only information relevant to the specific query. Therefore, it reduces the processing time and \nenables faster response generation. Second, costs associated with GPT-4 usage scale proportionally with \nthe number of tokens consumed. Using only related content leads to substantial cost savings compared to \nsending all notes within specified time windows. Considering the average volume of clinical data within \nthe EHR, this cost reduction becomes even more significant. Therefore, we used an approach to identify \nand only provide the parts of the clinical data relevant to the specific question being asked.  \n \nThe workflow of our clinical note-based question answering system powered by the RAG architecture \nconsisted of four key stages: Data load, data split, vector embeddings, and question answering (Figure 1). \nWe used an internal REST API to retrieve a collection of clinical notes spanning the past two years.  The \nretrieval process was filtered to include only notes pertaining to specific types (progress note, discharge \nsummary, H&P, telephone encounter, note to patient via portal) and statuses (signed or addendum). We \ndeveloped a custom Python (Version 3.10) program to retrieve and store clinical notes organized by \npatient for a defined group of patients in batches. In addition to the notes, we extracted metadata for each \nnote, including file location, note ID, date of creation and service, type of note and author, and word \ncount which provides a better understanding of the nature of the clinical notes and enables future filtering \nfor optimization. To facilitate efficient processing and context-aware analysis, we segmented notes into \nsmaller chunks using LangChain’s recursive chunking strategy13 to preserve surrounding context while \navoiding mid-sentence or word truncation.  We also tracked the origin of each chunk, linking it back to its \noriginal note for future reference.  This approach naturally led to varying chunk sizes, reflecting the \ndiverse lengths of clinical notes, which can range from a single sentence to multiple pages. \n \nWe generated numerical vector representations (embeddings) for each chunk using Azure OpenAI’s ada-\n002 model14.  These embeddings capture the semantic meaning of the text, which allowed us to compare \nchunks quickly and efficiently when searching for relevant information. To optimize retrieval during the \nquestion and answering stage, we used Facebook’s AI Similarity Search (FAISS) library15.  Each patient’s \nembeddings were saved in a dedicated file using Python’s pickle module and then loaded into FAISS’  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.08.24302376doi: medRxiv preprint \n   \n \n7 \n \n \nFigure 1. The Workflow of Clinical Note-Based Question Answering System Powered by the RAG Architecture \n \nWorkflow of the patient Q&A system leveraging the RAG (Retrieval Augmented Generation) architecture. The workflow \nconsists of the following key steps: 1) Clinical note retrieval 2) Note segmentation 3) Vector embeddings 4) Similarity \nsearch, prompting, and generation. \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.08.24302376doi: medRxiv preprint \n   \n \n8 \n \nin-memory vector store as needed16 to optimize memory usage and loading efficiency.  This approach \nused FAISS’ built-in similarity search capabilities for faster retrieval and allowed us to reuse the same \nembeddings throughout development and validation phases, eliminating the need for repeated generation \nand saving computational resources. We then asked 13 questions for each patient, 1 inclusion and 12 \nexclusion criteria.  The embedding model transformed each question into a vector representation \n(embedding). These embeddings then acted as search queries against the vector store, using LangChain's \nRetrieval QA chain13, and retrieved the top 3 most relevant chunks based on their semantic similarity to \nthe question, along with links back to their original notes. These retrieved chunks, combined with a \nsystem prompt and the question itself, were fed to Azure OpenAI’s GPT-4 model with temperature 0, \nwhich generated concise “Yes” or “No” answers.  At times, GPT-4 returned an answer with a “.” \nappended.  We ignored these trailing periods in our data analysis.  Because the questions were designed to \nbe independent, we did not maintain a chat history to influence subsequent prompts, ensuring focused \nanalysis for each query. \n \nDetermination of the Chunk Size, Prompt Development, and Testing Consistency \nDuring development, we evaluated the impact of different patient note text chunk sizes (125, 250, 500, \n1,000, and 2,000 tokens) with 20% overlap on the retrieval of relevant context for GPT-4 responses.  Our \ninitial analysis suggested that chunk sizes of 500 and 1,000 provided a good balance between capturing \nsufficient relevant information and minimizing irrelevant context. We noted that smaller chunk sizes often \nmissed crucial information necessary for accurate information (e.g. missing valve replacement procedure \nto classify patient as having severe valve disease or failing to capture discontinuation of ambrisentan to \nconclude that the patient was still on disease specific therapy for pulmonary hypertension). To further \ninvestigate the optimal chunk sizes, we used the validation dataset and evaluated all 13 inclusion and \nexclusion criteria.  We compared the percentage agreement between RECTIFIER responses and expert \nclinician reviews for both chunk sizes of 500 and 1,000. Based on the results of these analyses, we \nultimately decided to perform all analyses using a chunk size of 1,000 tokens (see the results section for \nanalysis results). \n \nWe implemented an iterative approach to prompt development. Each iteration involved careful evaluation \nof the development set, where the prompt was run, and its retrieved chunks analyzed alongside the \ncorresponding patient's clinical notes. This allowed us to understand discrepancies between the expected \nanswers and the model's output, informing targeted adjustments to refine the prompt. To validate the \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.08.24302376doi: medRxiv preprint \n   \n \n9 \n \nefficacy of these refinements, we evaluated different prompt versions on the validation set, allowing us to \nconfirm or reject the observed improvements (Supplemental Table 2). Once we identified the final \nprompts for 13 eligibility criteria questions, we ran them through RECTIFIER one by one consecutively \nand also by combining inclusion and exclusion criteria questions into two prompts. \n \nFinally, to investigate the consistency of the outputs from RECTIFIER, we conducted a comparison \nanalysis of different RECTIFIER runs on the validation dataset. We ran all 13 criteria on the validation \ndataset five times and compared RECTIFIER responses to each other based on the questions answered by \nthe expert clinician. We assigned a score of one to each consistent answer based on the most prevalent \nanswer for a total maximum of five points per each question. The final consistency percentage was \ncalculated by dividing the total consistency points by the maximum possible points (number of questions \nx 5). We also analyzed the standard deviation of agreement scores to assess response variability.  \n \nData Privacy and HIPAA Compliance \nWe employed a multi-layered approach to ensure data privacy and security when using Azure OpenAI for \nsensitive healthcare data.  First, a private endpoint isolated the instance, restricting data access to our \nauthorized virtual network and preventing unauthorized access. Second, all persisted data resided solely \nwithin our secure on-premise corporate network. Third, we focused on storing and providing access to \nonly the minimum amount of protected health information required for these purposes.  The Azure AI \nAPIs used to call GPT-4 did not persist either prompts or responses.  Finally, the data was encrypted at \nrest and in transit, adhering to stringent security protocols for safeguarding patient information. \n \nFurthermore, MGB has established a formal Business Associate Agreement with Microsoft to ensure \ncompliance with HIPAA regulations. This agreement clearly defined the roles and responsibilities of both \nparties regarding the protection of sensitive healthcare data. Additionally, we have entered into a Master \nService Agreement and an Enterprise Agreement with Microsoft, further solidifying our commitment to \nsecurity and compliance in our use of Microsoft Azure infrastructure. Institutional Review Broad of Mass \nGeneral Brigham gave ethical approval for this work which was performed as part of the COPILOT-HF \nStudy. \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.08.24302376doi: medRxiv preprint \n   \n \n10 \n \nStatistical Analysis \nWe created confusion matrices for each of the 13 eligibility criteria for study staff and RECTIFIER \nanswers (both for single question and combined question strategy) using expert clinician review as a gold \nstandard. We then calculated the sensitivity, specificity, and accuracy for each eligibility criteria. We used \naccuracy as our metric for the optimization of chunk sizes and prompts and to test consistency. For the \nfinal analysis of performance in the test set, we chose Matthews correlation coefficient (MCC) as our \nprimary evaluation metric since it is considered to be the most robust metric in two class confusion \nmatrices with rare labels where performance on the positive and negative classification is equally \nimportant17,18.  Conversely, the Binary F1 metric, also often used for imbalanced datasets, places more \nweight on the positive class since it is independent from true negative classification 17.  \n \nWe used bootstrapping to estimate 95% confidence intervals (CI) for sensitivity, specificity, positive \npredictive value, negative predictive value, accuracy, and MCC for each question and screening method. \nWe randomly sampled the answers for each question with replacement to create 2,000 bootstrap samples \nfor each question. We then calculated the 95% CI for each statistic based on the bootstrap distribution by \ntaking the 2.5th and 97.5th percentiles of the bootstrap statistics as the lower and upper bounds of the \nconfidence intervals. To assess the statistical significance of the differences in metrics between \nRECTIFIER and study staff in the test set, we employed a permutation test with 2000 permutations. In \neach permutation, group labels were randomly shuffled for each question, maintaining the original group \nsize. We then recalculated the metrics for these permuted groups and computed the difference in metrics \nbetween the groups for each permutation. For each metric of each question, a p-value was calculated as \nthe proportion of permutations where the absolute difference in the metric was greater than or equal to the \nobserved absolute difference in the original (non-permuted) data. This approach allowed us to determine \nthe likelihood of observing the given metric differences under the null hypothesis of no difference \nbetween the RECTIFIER and study staff.  We initially set a predetermined alpha level of 0.05. Given that \nmultiple statistical tests (13 tests per patient per metric) were performed, we applied a Bonferroni \ncorrection to adjust for multiple comparisons, resulting in an adjusted alpha level of approximately 0.0038 \n(0.05/13) per test. This stringent threshold was used to deem the results statistically significant, thereby \nminimizing the likelihood of Type I errors due to multiple testing. Statistical analyses were conducted \nusing R (version 4.3.2)19. \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.08.24302376doi: medRxiv preprint \n   \n \n11 \n \nCost Analysis \nTo understand the real-world feasibility of integrating RECTIFIER into clinical trial screening \nworkflows, we conducted a cost analysis using the test dataset population and 13 eligibility questions. \nDuring each program execution, we automatically tracked the token usage of both the prompt and GPT’s \nresponse, providing a basis for estimating potential costs. \n \nWhile Microsoft provided complimentary access to GPT-4 (Model version 1015) for our research, its \nactual cost structure remained confidential. To address this gap, we used the publicly available pricing of \nGPT-4 Turbo (Model version 1106-Preview), considered the successor to our employed model 20. While \nnot identical, this approach provided an approximation of potential financial implications when deploying \nthe solution on a larger scale. Our analysis compared two approaches which included sending 13 \nquestions individually to GPT-4 and combining exclusion criteria into a single prompt and sending  \nGPT-4 a total of 2 questions (1 inclusion and 1 combined exclusion).  \n \nRESULTS \nThe word count in the validation set ranged from 8 to 7097 (roughly 13 pages), with 75.13% of notes \ncontaining 500 words or less and 92.01% under 1,500 words. We compared the accuracy of RECTIFIER \nresponses for chunk sizes of 500 and 1,000. Chunk size of 1,000 outperformed 500 in 10 out of 13 criteria \n(Supplemental Table 3). In the consistency analysis for RECTIFIER with five different runs on the \nvalidation dataset, consistency percentage ranged between 99.16% and 100% and the standard deviation \nof accuracy ranged from 0% to 0.86% with minimal variation and high overall consistency (Supplemental \nTable 4).  \n \nIn the test set, both study staff and RECTIFIER showed overall high sensitivity and specificity across 13 \neligibility questions. The sensitivity for individual questions ranged from 66.7% to 100% for study staff \nand 75% to 100% for RECTIFIER, specificity ranged from 82.1% to 100% for study staff and 92.1% to \n100% for RECTIFIER, and PPV ranged from 50% to 100% for study staff and 75% to 100% for \nRECTIFIER (Figure 2). Both study staff and RECTIFIER answers closely aligned with the expert \nclinician answers with accuracy ranging between 91.7% and 100% (MCC 0.644 and 1) for study staff and \n97.9% and 100% (MCC 0.837 and 1) for RECTIFIER (Table 1).  RECTIFIER performed similarly to \nstudy staff for all eligibility criteria except for the inclusion criteria of “symptomatic heart failure” where \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.08.24302376doi: medRxiv preprint \n   \n \n12 \n \nit performed better with an accuracy of 97.9% vs 91.7% and an MCC of 0.924 vs 0.721, respectively. \nOverall, the sensitivity and specificity of determining eligibility for a study staff was 90.1% and 83.6%, \nand for RECTIFIER was 92.3% and 93.9%, respectively. When inclusion and exclusion questions were \nasked in combination in a single prompt, RECTIFIER performed worse with a sensitivity of 73.7% and \nspecificity of 77.9% to determine overall eligibility (Supplemental Table 5).  Figure 3 shows confusion \nmatrices for both RECTIFIER and study staff. \nIn the cost analysis, the individual question approach incurred an average cost of 10 cents per patient with \nthe combined question approach incurring 2 cents per patient (Supplemental Table 6).  \n \nFigure 2. Comparison of Positive Predictive Value (Precision) and Sensitivity (Recall) of RECTIFIER vs Study Staff \nA. \n \nB. \n \nSolid lines indicate 95% confidence interval. AI: Aortic insufficiency; AS: Aortic stenosis; CI: Confidence Interval; DM: \nDiabetes Mellitus; HCM: Hypertrophic cardiomyopathy; MCC: Matthews Correlation Coefficient; PAH: Pulmonary arterial \nhypertension, * indicates p<0.001 \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.08.24302376doi: medRxiv preprint \n   \n \n13 \n \n \nTable 1. Comparison of Accuracy and Matthew Correlation Coefficient of RECTIFIER And Study Staff to Determine \nEach Eligibility Criteria and Overall Eligibility \n  Accuracy \n(95% CI) \nMCC \n(95% CI) \nCriteria Prevalence \nin Cohort Study Staff RECTIFIER P \nValue Study Staff RECTIFIER P Value \nActive \nChemotherapy \n72 /1469 \n(4.9%) \n0.980 \n(0.972-0.986) \n0.987  \n(0.982-0.993) 0.16 0.810  \n(0.743-0.871) \n0.852  \n(0.783-0.910) 0.41 \nActively \nundergoing dialysis \n3 /1414 \n(0.2%) \n0.999 \n(0.998-1.000) \n0.999  \n(0.998-1.000) >0.99 0.816 \n (0.577-1.000) \n0.866  \n(0.577-1.000) >0.99 \nCongenital Heart \nDisease \n2 /317 \n(0.6%) \n1.000 \n(1.000-1.000) \n1.000  \n(1.000-1.000) >0.99 1.000 \n (1.000-1.000) \n1.000  \n(1.000-1.000) >0.99 \nVentricular Assist \nDevice \n8 /1422 \n(0.6%) \n0.999 \n(0.998-1.000) \n1.000  \n(1.000-1.000) >0.99 0.942 \n (0.790-1.000) \n1.000  \n(1.000-1.000) 0.59 \nPregnant or \nbreastfeeding \n1 /1418 \n(0.1%) \n1.000 \n(1.000-1.000) \n1.000  \n(1.000-1.000) >0.99 1.000 \n (1.000-1.000) \n1.000  \n(1.000-1.000) >0.99 \nSymptomatic Heart \nFailure \n1374 /1654 \n(83.1%) \n0.917  \n(0.903-0.929) \n0.979  \n(0.972-0.985) <0.001 0.721 \n (0.675-0.761) \n0.924 \n (0.899-0.948) <0.001 \nHCM 40 /1203 \n(3.3%) \n0.997  \n(0.993-0.999) \n0.999  \n(0.998-1.000) 0.37 0.952 \n (0.900-0.989) \n0.987 \n (0.958-1.000) 0.22 \nGroup 1 PAH 6 /1483 \n(0.4%) \n0.996  \n(0.993-0.999) \n1.000  \n(1.000-1.000) 0.04 0.644 \n (0.312-0.881) \n1.000 \n (1.000-1.000) 0.05 \nHistory of \ntransplant and being \nevaluated for \ntransplant \n33 /1439 \n(2.3%) \n0.995  \n(0.991-0.999) \n0.999  \n(0.997-1.000) 0.18 0.890 \n (0.798-0.964) \n0.969 \n (0.919-1.000) 0.10 \nAmyloid heart \ndisease \n14 /1491 \n(0.9%) \n0.998  \n(0.995-1.000) \n1.000  \n(1.000-1.000) 0.25 0.896 \n (0.761-1.000) \n1.000 \n (1.000-1.000) 0.12 \nEnd-of-life care or \nhospice \n16 /1422 \n(1.1%) \n0.995  \n(0.991-0.999) \n0.999 \n (0.996-1.000) 0.19 0.801 \n (0.629-0.926) \n0.937 \n (0.830-1.000) 0.15 \nSevere AS or AI 25 /1482 \n(1.7%) \n0.991  \n(0.985-0.995) \n0.994  \n(0.990-0.997) 0.41 0.785 \n (0.672-0.883) \n0.837 \n (0.723-0.932) 0.52 \nType 1 DM 11 /1201 \n(0.9%) \n0.998  \n(0.994-1.000) \n0.998  \n(0.994-1.000) >0.99 0.857 \n (0.663-1.000) \n0.885 \n (0.737-1.000) 0.82 \nOverall Eligibility \n1162/1509 \n(77%) 0.891  \n(0.875-0.907) \n0.927  \n(0.913-0.940) <0.001 0.711 \n (0.668-0.749) \n0.813 \n (0.780-0.847) <0.001 \nAI: Aortic insufficiency; AS: Aortic stenosis; CI: Confidence Interval; DM: Diabetes Mellitus; HCM: Hypertrophic \ncardiomyopathy; MCC: Matthews Correlation Coefficient; PAH: Pulmonary arterial hypertension \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.08.24302376doi: medRxiv preprint \n   \n \n14 \n \n \n \n \nFigure 3. Performance Metrics of Study Staff and RECTIFIER for Overall Eligibility Determination \nA. \n \nB. \n \nA) Performance metrics of RECTIFIER and Study Staff to determine overall eligibility based on 13 questions in the \ntest set. Solid lines indicate 95% confidence interval. B) Confusion matrices of RECTIFIER and study staff against \nexpert clinician review for overall eligibility based on 13 questions in the test set. NPV: Negative Predictive Value, \nPPV: Positive Predictive Value, * indicates p<0.001 \n \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.08.24302376doi: medRxiv preprint \n   \n \n15 \n \nDISCUSSION \nThis study tested the ability of a scalable and cost-conscious architecture leveraging GPT-4 for clinical \ntrial screening. We found that RECTIFIER can accurately screen patients for clinical trials using an \nactively enrolling heart failure clinical trial as an example. In specific aspects, RECTIFIER performance \nfor clinical trial screening surpassed traditional methods with study staff. Furthermore, the cost of using \nRECTIFIER was only 10 cents per patient. We believe these findings can lead to substantial \nimprovements in the efficiency of patient recruitment for clinical trials. \n \nWe found that both sensitivity and specificity of RECTIFIER was high, while study staff had slightly \nlower sensitivity and significantly lower specificity. A lower specificity of study staff is mitigated in \nclinical trial screening processes, as there is always a final clinician review before a patient is enrolled in \na clinical trial. However, reducing the need for an upfront detailed review by study staff would save \nsignificant resources before the final review while improving the identification of potentially eligible \npatients given the slightly higher sensitivity.  \n \nNatural Language Processing (NLP) models have been employed previously in similar workflows in \nclinical trial operations such as screening and clinical endpoint adjudication4,21. However, conventional \nNLP models often depend on specific words or phrases or large training sets for accurate phenotyping. \nThe NLP approach particularly falls short in tasks that involve complex clinical scenarios22, such as \ndiscerning whether symptoms are related to heart failure. Pre-trained language models, such as GPT-4, \ndistinguish themselves in these tasks by their ability to synthesize conclusions from a combination of \nsources. This feature is particularly advantageous in reviewing and discerning complex clinical scenarios \nwhere even trained study personnel struggle. We found substantial evidence of this when we manually \nreviewed the patient charts where study staff and RECTIFIER disagreed. As an example, in an older \nfemale patient with obesity presenting with exertional dyspnea, lower extremity edema, elevated right and \nleft-sided ventricular filling pressures, and evidence of left ventricular diastolic dysfunction, the provider \nnoted that “the patient does not have heart failure” solely due to normal NT-proBNP levels. As expected, \nthe study staff reviewed the patient’s chart and concluded that the patient did not have heart failure due to \nthe provider’s note. However, as the thresholds for NT-proBNP in obesity are not well established, it \nalone should not be used as the sole reason to rule in or rule out heart failure23 in patients with obesity and \nrenal dysfunction24,25. RECTIFIER could correctly identify a clinical heart failure diagnosis in this patient \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.08.24302376doi: medRxiv preprint \n   \n \n16 \n \nbased on other data, however, as it is not possible to discern the reasoning of LLMs to answer a question, \nit is impossible to determine how GPT-4 arrived at that decision. \n \nWe leveraged GPT-4 as part of a RAG architecture that uses an embedding model to retrieve relevant \ncontext from the clinical notes. Therefore, it is possible that we underreported the performance of GPT-4 \nin this study. Providing the full chart to GPT-4 may have improved accuracy in some cases, but this \nwould have resulted in significantly increased cost. Therefore, we tested GPT-4 as part of an architecture \nthat is readily deployable from a cost perspective in a real use case. In addition to the architecture we used \nin this study, integrating structured EHR data into prompts may further refine the screening process. This \ncombination strategy could enhance the patient pool by not relying exclusively on diagnostic codes, \nespecially for clinical diagnoses such as heart failure. This approach might broaden the scope of potential \ncandidates and result in a more inclusive and accurate screening process. As an example, one could use \nthe structured data for chemotherapy obtained from the EHR to determine if a patient is on active \nchemotherapy and consequently use GPT-4 to determine if the patient is on chemotherapy for an active \nmalignancy. \n \nAlthough a more efficient and cost-saving strategy for clinical trial screening would be highly desirable, a \ncritical consideration is the potential hazards of an automated screening process. These are five examples \nof potential hazards associated with automating the clinical trial screening process:  1) the reliance on \nLLM for initial screening may lead to a loss of nuanced patient context that a study staff might gather \nsuch as values of the patient or the preferred method of contact, impacting the quality of the enrollment \nconversation. 2) Operational hazards might develop associated with LLM system downtime, which could \ndelay patient screening and enrollment, if there are no downtime strategies put in place. 3) From a clinical \nperspective, the LLM might overlook critical nuances in physician notes, such as a patient's maximum \ntolerated dosage in our use case, which is crucial for determining trial eligibility. 4) The integration of \nLLMs into clinical trial screening raises equity hazards which have not been investigated in this study. \nThe reliance on algorithms using LLMs could exacerbate access issues for people medically underserved, \npotentially leading to increased false positives or negatives in these groups. Such a discrepancy might be \ndue to less frequent healthcare interactions or variations in data representation within these populations, \nor implicit bias reflected in patient notes, which could skew the predictive accuracy of LLM. 5) There \nmight be hazards associated with changes in upstream data capture, LLM infrastructure or clinical \nprocesses which can significantly impact the performance of the LLM-integrated automated algorithms \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.08.24302376doi: medRxiv preprint \n   \n \n17 \n \nand, consequently, the screening results. Given these potential significant hazards, the integration of  \nGPT-4 into clinical trial screening necessitates a careful balance between embracing technological \nadvancements and mitigating the associated risks. This approach is crucial to preserve the trial's integrity \nand efficacy while safeguarding against patient harm and ensuring equitable treatment across diverse \npopulations. To mitigate these risks, the implementation of robust checks and balances, including a final \nreview by clinicians before patient enrollment, is essential. In addition, planned systematic analyses of \ndistribution of social determinants of health among those who are screened in or out by the LLM-\nintegrated algorithm might help early identification of equity hazards. \n \nThere are several future iterations to improve the performance of GPT-4 for clinical trial screening. \nCombining GPT with the RAG architecture, as detailed in this manuscript, offers significant efficiency \nand cost advantages for clinical trial recruitment.  However, while efficient, this approach has \nshortcomings, such as ensuring GPT receives relevant clinical context for accurate responses. To \nmaximize the value of RAG while minimizing effort, one can leverage several low-cost, quick-to \nimplement techniques such as metadata filtering to target more specific clinical notes, using a hybrid \nsearch to incorporate essential keywords (e.g., procedure names, medications) to refine searches, or \nreranking to prioritize results based on date sensitivity (e.g., current vs. historical condition). Furthermore, \nthese techniques can be implemented using various vector databases with built-in capabilities or existing \nframeworks like LangChain and LlamaIndex26.  These tools offer a faster and easier way to experiment \nand improve the process than advanced, but expensive and time-consuming techniques like fine-tuning \nembeddings or the LLM model itself. Although we had high consistency across several runs for the \nvalidation dataset, to further enhance the consistency of GPT-4 responses, one can leverage Microsoft \nOpenAI’s recommended “seed” parameter27.  Even though not completely deterministic, setting the same \nseed value across GPT runs reduces randomness, potentially leading to even greater reproducibility and \nconsistent performance.  Finally, while we found that per patient cost for screening was very low with the \nRECTIFIER, one can leverage several prompting approaches to decrease costs even further. We used one \nsuch approach using combined prompts for exclusion criteria questions which led to significantly lower \ncosts but also a significantly lower sensitivity and specificity for determining overall eligibility, \nhighlighting the importance of balancing the performance and cost of the model depending on the desired \noutcome. In our case, prioritizing higher accuracy might justify the additional cost of processing \nindividual criteria. It is important to recognize that these optimization steps involve inevitable trade-offs. \nFinding the right techniques requires a cost-aware approach that aligns with the specific use case.  To \ninform decision-making, automated monitoring, and evaluation of individual components and overall \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.08.24302376doi: medRxiv preprint \n   \n \n18 \n \nperformance will be essential. These data will provide valuable feedback to refine the system, mitigate \nrisks, and address limitations effectively. \n \nThis study had several strengths. Primarily, we included a large number of patients with coded inclusion \nand exclusion criteria ascertained previously by study staff and an expert clinician. Second, the extensive \ndataset used is derived from an ongoing randomized clinical trial, which is particularly valuable as it \nprovides a rich, authentic context for applying and evaluating GPT-4 in clinical trial screening, ensuring \nthat the findings are grounded in practical, real-world scenarios. Additionally, this approach allowed us to \nassess the study staff performance and compare it to the performance of GPT-4, which is critical in \nevaluating whether GPT-4 can assume the task of clinical trial screening from the study staff. Third, the \ncriteria we evaluated for screening in this study included those needing more complex clinical \nassessments such as “symptomatic heart failure”. Considering that most screening questions typically \nfocus on basic phenotyping to determine whether a condition is present, evaluating GPT-4 on more \nintricate tasks enhances the potential for generalizability in the findings of this study. Beyond those \npreviously mentioned, this study had several additional limitations. First, several eligibility questions \nwere unanswered by the study staff given they stopped answering other eligibility questions once they \ndetermined presence of any exclusion criteria. Second, our study cohort consisted predominantly of \npatients with a high prevalence of heart failure and exclusion criteria were relatively rare. This \ncomposition was due to the structured query methodology used to create the patient list for the \nCOPILOT-HF trial. While this approach is effective to narrow down the list of potentially eligible \npatients for the study and enable a more cost-effective approach to use LLMs, the results might exhibit \nvariability when applied to the 13 different conditions in populations with varying prevalence of these \nconditions. A third limitation is that a single expert clinician prepared the gold standard.  Including \nadditional clinicians will help characterize variations in the expert reviews, and additionally provide a \nquality assurance for the gold standard. \n \nThis suggests the necessity for initial validation of GPT-4 in a broader and more diverse potentially \neligible trial population before consideration of automating the screening task.  \n \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.08.24302376doi: medRxiv preprint \n   \n \n19 \n \nCONCLUSION \nGPT-4, when used to screen patients for clinical trials, has the potential to improve efficiency and reduce \ncosts significantly. Before the complete automation of the screening process, it is important to consider \ncarefully all potential hazards and deploy appropriate mitigation strategies. Furthermore, while this study \noffers promising insights and applications of GPT-4 in clinical trial screening, these must be interpreted \nwith an understanding of the context-specific nature of our findings. As we progress, we must continue \nrefining these technologies, ensuring their applicability across a spectrum of clinical scenarios while \naddressing potential challenges to ensure the continued integrity of clinical trials. \n \nACKNOWLEDGEMENTS \nWe appreciate the support and contributions of Elizabeth W. Karlson, MD, MS, Matthew S. Lebo, Ph.D., \nKalotina Machini Ph.D., Adam B. Landman, MD, Samantha Subramaniam, Marian McPartlin, Nallan \nSriraman, Matthew Butler, Jonathan Hamill, and Pranav Sriraman for this study. We also appreciate the \ncomplimentary access to Azure OpenAI GPT-4V provided by Microsoft. This work was conducted with \nsupport from Harvard Catalyst, The Harvard Clinical and Translational Science Center (National Center \nfor Advancing Translational Sciences, National Institutes of Health Awards UL1 TR002541 and \nR01HL151643, and financial contributions from Harvard University and its affiliated academic healthcare \ncenters.  \n \nDISCLOSURES \nFor this study, complimentary access to Azure OpenAI GPT-4V was provided by Microsoft. Microsoft \nhad no access to the data used and had no involvement in the analysis, interpretation of data, or writing of \nour study. Samuel J Aronson, Alexander J Blood, Charlotte J Mailly, Michael F Oates, Benjamin M \nScirica, Jiyeon Shin Michela R Tucci, Ozan Unlu, Matthew Varugheese, and Fei Wang report Research \nGrants and related funding via Brigham and Women’s Hospital: Better Therapeutics, Boehringer \nIngelheim, Eli Lilly, Milestone Pharmaceuticals and NovoNordisk. Samuel J Aronson reports consulting \nto Nest Genomics. Samuel Aronson, Charlotte J Mailly, Michael F Oates, Michela Tucci and Fei Wang \nalso report unrelated NIH and PCORI support. Alexander J. Blood reports consulting income from \nWalgreens Health, Color Health, Novo Nordisk, Medscape, and Arsenal Capital Partners, and equity \nholdings in Knownwell health. Benjamin M Scirica reports consulting fees from Abbvie (DSMB), \nAstraZeneca (DSMB), Boehringer Ingelheim (DSMB), Better Therapeutics, Elsevier Practice Update \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.08.24302376doi: medRxiv preprint \n   \n \n20 \n \nCardiology, Esperion, Hanmi (DSMB), Lexeo (DSMB), and NovoNordisk; and equity in Health [at] \nScale. Ozan Unlu receives funding from the National Heart Lung and Blood Institute under award number \nT32HL007604. \n  \nREFERENCES \n1. Dumitrache A, Aroyo L, Welty C. Crowdsourcing Ground Truth for Medical Relation Extraction. \nACM Trans Interact Intell Syst. 2018;8(2):11:1-11:20. doi:10.1145/3152889 \n2. Kim J, Quintana Y. Review of the Performance Metrics for Natural Language Systems for Clinical \nTrials Matching. Stud Health Technol Inform. 2022;290:641-644. doi:10.3233/SHTI220156 \n3. Elm JJ, Palesch Y, Easton JD, et al. Screen Failure Data in Clinical Trials: Are Screening Logs Worth \nIt? Clin Trials. 2014;11(4):467-472. doi:10.1177/1740774514538706 \n4. Idnay B, Dreisbach C, Weng C, Schnall R. A systematic review on natural language processing \nsystems for eligibility prescreening in clinical research. J Am Med Inform Assoc. 2021;29(1):197-206. \ndoi:10.1093/jamia/ocab228 \n5. OpenAI, Achiam J, Adler S, et al. GPT-4 Technical Report. Published online December 18, 2023. \ndoi:10.48550/arXiv.2303.08774 \n6. Min B, Ross H, Sulem E, et al. Recent Advances in Natural Language Processing via Large Pre-\ntrained Language Models: A Survey. ACM Comput Surv. 2023;56(2):30:1-30:40. \ndoi:10.1145/3605943 \n7. Thirunavukarasu AJ, Ting DSJ, Elangovan K, Gutierrez L, Tan TF, Ting DSW. Large language \nmodels in medicine. Nat Med. 2023;29(8):1930-1940. doi:10.1038/s41591-023-02448-8 \n8. Hamer DM den, Schoor P, Polak TB, Kapitan D. Improving Patient Pre-screening for Clinical Trials: \nAssisting Physicians with Large Language Models. Published online June 29, 2023. \ndoi:10.48550/arXiv.2304.07396 \n9. Perlis RH, Fihn SD. Evaluating the Application of Large Language Models in Clinical Research \nContexts. JAMA Network Open. 2023;6(10):e2335924. doi:10.1001/jamanetworkopen.2023.35924 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.08.24302376doi: medRxiv preprint \n   \n \n21 \n \n10. Unlu O, Blood AJ, Ostrominski JW, et al. Abstract 16521: Using Electronic Health Record Data \nto Identify and Prioritize Patients With Heart Failure With Reduced Ejection Fraction for a Remote \nMedication Optimization Program. Circulation. 2023;148(Suppl_1):A16521-A16521. \ndoi:10.1161/circ.148.suppl_1.16521 \n11. Gordon WJ, Blood AJ, Chaney K, et al. Workflow Automation for a Virtual Hypertension \nManagement Program. Appl Clin Inform. 2021;12(5):1041-1048. doi:10.1055/s-0041-1739195 \n12. Lewis P, Perez E, Piktus A, et al. Retrieval-Augmented Generation for Knowledge-Intensive NLP \nTasks. Published online April 12, 2021. doi:10.48550/arXiv.2005.11401 \n13. Retrieval QA | Langchain. Accessed January 9, 2024. \nhttps://js.langchain.com/docs/modules/chains/popular/vector_db_qa \n14. Neelakantan A, Xu T, Puri R, et al. Text and Code Embeddings by Contrastive Pre-Training. \nPublished online January 24, 2022. doi:10.48550/arXiv.2201.10005 \n15. Faiss: A library for efficient similarity search. Engineering at Meta. Published March 29, 2017. \nAccessed January 9, 2024. https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-\nfor-efficient-similarity-search/ \n16. pickle — Python object serialization. Python documentation. Accessed January 9, 2024. \nhttps://docs.python.org/3/library/pickle.html \n17. Chicco D, Jurman G. The advantages of the Matthews correlation coefficient (MCC) over F1 \nscore and accuracy in binary classification evaluation. BMC Genomics. 2020;21(1):6. \ndoi:10.1186/s12864-019-6413-7 \n18. Chicco D, Tötsch N, Jurman G. The Matthews correlation coefficient (MCC) is more reliable \nthan balanced accuracy, bookmaker informedness, and markedness in two-class confusion matrix \nevaluation. BioData Mining. 2021;14(1):13. doi:10.1186/s13040-021-00244-z \n19. R Core Team. R: A Language and Environment for Statistical Computing. R Foundation for \nStatistical Computing; 2023. https://www.R-project.org/ \n20. Azure OpenAI Service - Pricing | Microsoft Azure. Accessed February 5, 2024. \nhttps://azure.microsoft.com/en-us/pricing/details/cognitive-services/openai-service/ \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.08.24302376doi: medRxiv preprint \n   \n \n22 \n \n21. Cunningham JW, Singh P, Reeder C, et al. Natural Language Processing for Adjudication of \nHeart Failure in a Multicenter Clinical Trial: A Secondary Analysis of a Randomized Clinical Trial. \nJAMA Cardiol. Published online November 11, 2023:e234859. doi:10.1001/jamacardio.2023.4859 \n22. Lederman A, Lederman R, Verspoor K. Tasks as needs: reframing the paradigm of clinical \nnatural language processing research for real-world decision support. Journal of the American Medical \nInformatics Association. 2022;29(10):1810-1817. doi:10.1093/jamia/ocac121 \n23. Bayes-Genis A, Docherty KF, Petrie MC, et al. Practical algorithms for early diagnosis of heart \nfailure and heart stress using NT-proBNP: A clinical consensus statement from the Heart Failure \nAssociation of the ESC. Eur J Heart Fail. 2023;25(11):1891-1898. doi:10.1002/ejhf.3036 \n24. Kozhuharov N, Martin J, Wussler D, et al. Clinical effect of obesity on N-terminal pro-B-type \nnatriuretic peptide cut-off concentrations for the diagnosis of acute heart failure. Eur J Heart Fail. \n2022;24(9):1545-1554. doi:10.1002/ejhf.2618 \n25. deFilippi CR, Seliger SL, Maynard S, Christenson RH. Impact of renal disease on natriuretic \npeptide testing for diagnosing decompensated heart failure and predicting mortality. Clin Chem. \n2007;53(8):1511-1519. doi:10.1373/clinchem.2006.084533 \n26. LlamaIndex 0.9.44. Accessed February 5, 2024. https://docs.llamaindex.ai/en/stable/ \n27. mrbullwinkle. How to generate reproducible output with Azure OpenAI Service - Azure OpenAI. \nPublished December 7, 2023. Accessed February 5, 2024. https://learn.microsoft.com/en-us/azure/ai-\nservices/openai/how-to/reproducible-output \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.08.24302376doi: medRxiv preprint ",
  "topic": "Clinical trial",
  "concepts": [
    {
      "name": "Clinical trial",
      "score": 0.6457504034042358
    },
    {
      "name": "Workflow",
      "score": 0.6282588243484497
    },
    {
      "name": "Inclusion and exclusion criteria",
      "score": 0.5883810520172119
    },
    {
      "name": "Computer science",
      "score": 0.5750817060470581
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4442508816719055
    },
    {
      "name": "Transformer",
      "score": 0.43298494815826416
    },
    {
      "name": "Medical physics",
      "score": 0.34413599967956543
    },
    {
      "name": "Medicine",
      "score": 0.30544236302375793
    },
    {
      "name": "Engineering",
      "score": 0.1550939977169037
    },
    {
      "name": "Database",
      "score": 0.1530451476573944
    },
    {
      "name": "Internal medicine",
      "score": 0.11406892538070679
    },
    {
      "name": "Pathology",
      "score": 0.1101195216178894
    },
    {
      "name": "Alternative medicine",
      "score": 0.09645789861679077
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1283280774",
      "name": "Brigham and Women's Hospital",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I48633490",
      "name": "Mass General Brigham",
      "country": "US"
    }
  ],
  "cited_by": 35
}