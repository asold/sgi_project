{
  "title": "Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View",
  "url": "https://openalex.org/W2948981900",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1987524739",
      "name": "Lu Yiping",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227371052",
      "name": "Li, Zhuohan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098476997",
      "name": "He Di",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2360220517",
      "name": "Sun, Zhiqing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2011423952",
      "name": "Dong Bin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105471080",
      "name": "Qin Tao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1923131268",
      "name": "Wang Liwei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4209371238",
      "name": "Liu, Tie-Yan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2886490473",
    "https://openalex.org/W2124807415",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2949509589",
    "https://openalex.org/W2907121943",
    "https://openalex.org/W2963773075",
    "https://openalex.org/W2569134578",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2899297079",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2964156122",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W1679295048",
    "https://openalex.org/W2600297185",
    "https://openalex.org/W2049161806",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2044634997",
    "https://openalex.org/W2898318847",
    "https://openalex.org/W1452175245",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2788305176",
    "https://openalex.org/W2963755523",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2337199865",
    "https://openalex.org/W1597944220",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2767989436",
    "https://openalex.org/W3098011980",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2806311723",
    "https://openalex.org/W2109553965",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2952444318",
    "https://openalex.org/W2963302407",
    "https://openalex.org/W2964265128"
  ],
  "abstract": "The Transformer architecture is widely used in natural language processing. Despite its success, the design principle of the Transformer remains elusive. In this paper, we provide a novel perspective towards understanding the architecture: we show that the Transformer can be mathematically interpreted as a numerical Ordinary Differential Equation (ODE) solver for a convection-diffusion equation in a multi-particle dynamic system. In particular, how words in a sentence are abstracted into contexts by passing through the layers of the Transformer can be interpreted as approximating multiple particles' movement in the space using the Lie-Trotter splitting scheme and the Euler's method. Given this ODE's perspective, the rich literature of numerical analysis can be brought to guide us in designing effective structures beyond the Transformer. As an example, we propose to replace the Lie-Trotter splitting scheme by the Strang-Marchuk splitting scheme, a scheme that is more commonly used and with much lower local truncation errors. The Strang-Marchuk splitting scheme suggests that the self-attention and position-wise feed-forward network (FFN) sub-layers should not be treated equally. Instead, in each layer, two position-wise FFN sub-layers should be used, and the self-attention sub-layer is placed in between. This leads to a brand new architecture. Such an FFN-attention-FFN layer is \"Macaron-like\", and thus we call the network with this new architecture the Macaron Net. Through extensive experiments, we show that the Macaron Net is superior to the Transformer on both supervised and unsupervised learning tasks. The reproducible codes and pretrained models can be found at https://github.com/zhuohan123/macaron-net",
  "full_text": "Understanding and Improving Transformer\nFrom a Multi-Particle Dynamic System Point of View\nYiping Lu1,∗ Zhuohan Li1,∗ Di He1 Zhiqing Sun1\nBin Dong1 Tao Qin2 Liwei Wang1 Tie-Yan Liu2\n1Peking University 2Microsoft Research\n{luyiping9712,lizhouhan,di_he,zhiqing}@pku.edu.cn\ndongbin@math.pku.edu.cn wanglw@pku.edu.cn\n{taoqin, tyliu}@microsoft.com\nAbstract\nThe Transformer architecture is widely used in natural language processing. De-\nspite its success, the design principle of the Transformer remains elusive. In this\npaper, we provide a novel perspective towards understanding the architecture:\nwe show that the Transformer can be mathematically interpreted as a numerical\nOrdinary Differential Equation (ODE) solver for a convection-diffusion equation\nin a multi-particle dynamic system . In particular, how words in a sentence are\nabstracted into contexts by passing through the layers of the Transformer can be\ninterpreted as approximating multiple particles’ movement in the space using the\nLie-Trotter splitting scheme and the Euler’s method. Given this ODE’s perspective,\nthe rich literature of numerical analysis can be brought to guide us in designing\neffective structures beyond the Transformer. As an example, we propose to replace\nthe Lie-Trotter splitting scheme by the Strang-Marchuk splitting scheme, a scheme\nthat is more commonly used and with much lower local truncation errors. The\nStrang-Marchuk splitting scheme suggests that the self-attention and position-wise\nfeed-forward network (FFN) sub-layers should not be treated equally. Instead, in\neach layer, two position-wise FFN sub-layers should be used, and the self-attention\nsub-layer is placed in between. This leads to a brand new architecture. Such an\nFFN-attention-FFN layer is “Macaron-like\", and thus we call the network with this\nnew architecture the Macaron Net. Through extensive experiments, we show that\nthe Macaron Net is superior to the Transformer on both supervised and unsuper-\nvised learning tasks. The reproducible codes and pretrained models can be found\nat https://github.com/zhuohan123/macaron-net\n1 Introduction\nThe Transformer is one of the most commonly used neural network architectures in natural language\nprocessing. Variants of the Transformer have achieved state-of-the-art performance in many tasks\nincluding language modeling [ 11, 2] and machine translation [ 43, 12, 15]. Transformer-based\nunsupervised pre-trained models also show impressive performance in many downstream tasks\n[33, 13, 31].\nThe Transformer architecture is mainly built by stacking layers, each of which consists of two\nsub-layers with residual connections: the self-attention sub-layer and the position-wise feed-forward\nnetwork (FFN) sub-layer. For a given sentence, the self-attention sub-layer considers the semantics\nand dependencies of words at different positions and uses that information to capture the internal\n∗Equal contribution. Works done while interning at Microsoft Research Asia.\nPreprint. Under review.\narXiv:1906.02762v1  [cs.LG]  6 Jun 2019\nstructure and representations of the sentence. The position-wise FFN sub-layer is applied to each\nposition separately and identically to encode context at each position into higher-level representations.\nAlthough the Transformer has demonstrated promising results in many tasks, its design principle is\nnot fully understood, and thus the strength of the architecture is not fully exploited. As far as we\nknow, there is little work studying the foundation of the Transformer or different design choices.\nParticle (Word)\nParticles move in the space along time (Semantics encoded in stacked neural network layers)\nFigure 1: Physical interpretation of Transformer.\nIn this paper, we provide a novel perspective to-\nwards understanding the architecture. In particu-\nlar, we are the ﬁrst to show that the Transformer\narchitecture is inherently related to Multi-Particle\nDynamic System (MPDS) in physics. MPDS is\na well-established research ﬁeld which aims at\nmodeling how a collection of particles move in\nthe space using differential equations [ 28]. In\nMPDS, the behavior of each particle is usually\nmodeled by two factors separately. The ﬁrst fac-\ntor is the convection which concerns the mecha-\nnism of each particle regardless of other particles\nin the system, and the second factor is the diffu-\nsion which models the movement of the particle\nresulting from other particles in the system.\nInspired by the relationship between the ODE and neural networks [ 25, 8], we ﬁrst show that the\nTransformer layers can be naturally interpreted as a numerical ODE solver for a ﬁrst-order convection-\ndiffusion equation in MPDS. To be more speciﬁc, the self-attention sub-layer, which transforms\nthe semantics at one position by attending over all other positions, corresponds to the diffusion\nterm; The position-wise FFN sub-layer, which is applied to each position separately and identically,\ncorresponds to the convection term. The number of stacked layers in the Transformer corresponds to\nthe time dimension in ODE. In this way, the stack of self-attention sub-layers and position-wise FFN\nsub-layers with residual connections can be viewed as solving the ODE problem numerically using\nthe Lie-Trotter splitting scheme [ 17] and the Euler’s method [3]. By this interpretation, we have\na novel understanding of learning contextual representations of a sentence using the Transformer:\nthe feature (a.k.a, embedding) of words in a sequence can be considered as the initial positions of a\ncollection of particles, and the latent representations abstracted in stacked Transformer layers can be\nviewed as the location of particles moving in a high-dimensional space at different time points.\nSuch an interpretation not only provides a new perspective on the Transformer but also inspires us\nto design new structures by leveraging the rich literature of numerical analysis. The Lie-Trotter\nsplitting scheme is simple but not accurate and often leads to high approximation error [ 17]. The\nStrang-Marchuk splitting scheme [39] is developed to reduce the approximation error by a simple\nmodiﬁcation to the Lie-Trotter splitting scheme and is theoretically more accurate. Mapped to neural\nnetwork design, the Strang-Marchuk splitting scheme suggests that there should be three sub-layers:\ntwo position-wise feed-forward sub-layers with half-step residual connections and one self-attention\nsub-layer placed in between with a full-step residual connection. By doing so, the stacked layers will\nbe more accurate from the ODE’s perspective and will lead to better performance in deep learning.\nAs the FFN-attention-FFN layer is “Macaron-like\", we call it Macaron layer and call the network\ncomposed of Macaron layers the Macaron Net.\nWe conduct extensive experiments on both supervised and unsupervised learning tasks. For each task,\nwe replace Transformer layers by Macaron layers and keep the number of parameters to be the same.\nExperiments show that the Macaron Net can achieve higher accuracy than the Transformer on all\ntasks which, in a way, is consistent with the ODE theory.\n2 Background\n2.1 Relationship Between Neural Networks and ODE\nRecently, there are extensive studies to bridge deep neural networks with ordinary differential\nequations [46, 25, 19, 8, 51, 38, 42]. We here present a brief introduction to such a relationship\nand discuss how previous works borrow powerful tools from numerical analysis to help deep neural\nnetwork design.\n2\nA ﬁrst-order ODE problem is usually deﬁned as to solve the equation (i.e., calculate x(t) for any t)\nwhich satisﬁes the following ﬁrst-order derivative and the initial condition:\ndx(t)\ndt = f(x,t), x (t0) =w, (1)\nin which x(t) ∈Rd for all t≥t0. ODEs usually have physical interpretations. For example, x(t)\ncan be considered as the location of a particle moving in the d-dimensional space and the ﬁrst order\ntime derivative can be considered as the velocity of the particle.\nUsually there is no analytic solution to Eqn (1) and the problem has to be solved numerically. The\nsimplest numerical ODE solver is the Euler’s method [3]. The Euler’s method discretizes the time\nderivative dx(t)\ndt by its ﬁrst-order approximation x(t2)−x(t1)\nt2−t1\n≈f(x(t1),t1). By doing so, for the\nﬁxed time horizon T = t0 + γL, we can estimate x(T) from x0\n.= x(t0) by sequentially estimating\nxl+1\n.= x(tl+1) using\nxl+1 = xl + γf(xl,tl) (2)\nwhere l = 0,··· ,L −1, tl = t0 + γl is the time point corresponds to xl, and γ = (T −t0)/Lis\nthe step size. As we can see, this is mathematically equivalent to the ResNet architecture [ 25, 8]:\nThe function γf(xl,tl) can be considered as a neural-network block, and the second argument tl in\nthe function indicates the set of parameters in the l-th layer. The simple temporal discretization by\nEuler’s method naturally leads to the residual connection.\nObserving such a strong relationship, researchers use ODE theory to explain and improve the neural\nnetwork architectures mainly designed for computer vision tasks. [25, 8] show any parametric ODE\nsolver can be viewed as a deep residual network (probably with inﬁnite layers), and the parameters\nin the ODE can be optimized through backpropagation. Recent works discover that new neural\nnetworks inspired by sophisticated numerical ODE solvers can lead to better performance. For\nexample, [52] uses a high-precision Runge-Kutta method to design a neural network, and the new\narchitecture achieves higher accuracy. [19] uses a leap-frog method to construct a reversible neural\nnetwork. [24, 7] try to understand recurrent neural networks from the ODE’s perspective, and [41]\nuses non-local differential equations to model non-local neural networks.\n2.2 Transformer\nThe Transformer architecture is usually developed by stacking Transformer layers [ 43, 13]. A\nTransformer layer operates on a sequence of vectors and outputs a new sequence of the same shape.\nThe computation inside a layer is decomposed into two steps: the vectors ﬁrst pass through a (multi-\nhead) self-attention sub-layer and the output will be further put into a position-wise feed-forward\nnetwork sub-layer. Residual connection [ 20] and layer normalization [ 22] are employed for both\nsub-layers. The visualization of a Transformer layer is shown in Figure 2(a) and the two sub-layers\nare deﬁned as below.\nSelf-attention sub-layer The attention mechanism can be formulated as querying a dictionary\nwith key-value pairs [43], e.g., Attention(Q,K,V ) =softmax(QKT/√dmodel) ·V,where dmodel\nis the dimensionality of the hidden representations and Q(Query), K(Key), V (Value) are speciﬁed\nas the hidden representations of the previous layer in the so-called self-attention sub-layers in the\nTransformer architecture. The multi-head variant of attention allows the model to jointly attend to\ninformation from different representation subspaces, and is deﬁned as\nMulti-head(Q,K,V ) =Concat(head1,··· ,headH)WO, (3)\nheadk = Attention(QWQ\nk ,KW K\nk ,VW V\nk ), (4)\nwhere WQ\nk ∈Rdmodel×dK ,WK\nk ∈Rdmodel×dK ,WV\nk ∈Rdmodel×dV , and WO ∈RHdV ×dmodel are\nproject parameter matrices, H is the number of heads, and dK and dV are the dimensionalities of\nKey and Value.\nPosition-wise FFN sub-layer In addition to the self-attention sub-layer, each Transformer layer\nalso contains a fully connected feed-forward network, which is applied to each position separately\nand identically. This feed-forward network consists of two linear transformations with an activation\n3\nfunction σin between. Specially, given vectors h1,...,h n, a position-wise FFN sub-layer transforms\neach hi as FFN(hi) =σ(hiW1 + b1)W2 + b2, where W1,W2,b1 and b2 are parameters.\nIn this paper, we take the ﬁrst attempt to provide an understanding of the feature extraction process\nin natural language processing from the ODE’s viewpoint. As discussed in Section 2.1, several\nworks interpret the standard ResNet using the ODE theory. However, we found this interpretation\ncannot be directly applied to the Transformer architecture. First, different from vision applications\nwhose size of the input (e.g., an image) is usually predeﬁned and ﬁxed, the input (e.g., a sentence)\nin natural language processing is always of variable length, which makes the single-particle ODE\nformulation used in previous works not applicable. Second, the Transformer layer contains very\ndistinct sub-layers. The self-attention sub-layer takes the information from all positions as input\nwhile the position-wise feed-forward layer is applied to each position separately. How to interpret\nthese heterogeneous components by ODE is also not covered by previous works [41, 8].\n3 Reformulate Transformer Layers as an ODE Solver for Multi-Particle\nDynamic System\nIn this section, we ﬁrst introduce the general form of differential equations in MPDS and then\nreformulate the stacked Transformer layers to show they form a numerical ODE solver for a speciﬁc\nproblem. After that, we use advanced methods in the ODE theory to design new architectures.\n3.1 Multi-Particle ODE and Its Numerical Solver\nUnderstanding the dynamics of multiple particles’ movements in space is one of the important\nproblems in physics, especially in ﬂuid mechanics and astrophysics [ 28]. The behavior of each\nparticle is usually modeled by two factors: The ﬁrst factor concerns about the mechanism of its\nmovement regardless of other particles, e.g., caused by an external force outside of the system, which\nis usually referred to as the convection; The second factor concerns about the movement resulting\nfrom other particles, which is usually referred to as the diffusion. Mathematically, assume there are n\nparticles in d-dimensional space. Denote xi(t) ∈Rd as the location of i-th particle at time t. The\ndynamics of particle ican be formulated as\ndxi(t)\ndt = F(xi(t),[x1(t),··· ,xn(t)],t) +G(xi(t),t),\nxi(t0) =wi, i = 1,...,n. (5)\nFunction F(xi(t),[x1(t),··· ,xn(t)],t) represents the diffusion term which characterizes the inter-\naction between the particles. G(x,t) is a function which takes a location xand time tas input and\nrepresents the convection term.\nSplitting schemes As we can see, there are two coupled terms in the right-hand side of Eqn (5)\ndescribing different physical phenomena. Numerical methods of directly solving such ODEs can be\ncomplicated. The splitting method is a prevailing way of solving such coupled differential equations\nthat can be decomposed into a sum of differential operators [27]. Furthermore, splitting convection\nfrom diffusion is quite standard for many convection-diffusion equations [18, 17]. The Lie-Trotter\nsplitting scheme [17] is the simplest splitting method. It splits the right-hand side of Eqn (5) into\nfunction F(·) and G(·) and solves the individual dynamics alternatively. More precisely, to compute\nxi(t+ γ) from xi(t), the Lie-Trotter splitting scheme with the Euler’s method reads as\n˜xi(t) =xi(t) +γF(xi(t),[x1(t),x2(t),··· ,xn(t)],t), (6)\nxi(t+ γ) = ˜xi(t) +γG(˜xi(t),t). (7)\nFrom time tto time t+ γ, the Lie-Trotter splitting method ﬁrst solves the ODE with respect to F(·)\nand acquire an intermediate location ˜xi(t). Then, starting from ˜xi(t) , it solves the second ODE with\nrespect to G(·) to obtain xi(t+ γ).\n3.2 Physical interpretation of the Transformer\nWe reformulate the two sub-layers of the Transformer in order to match its form with the ODE\ndescribed above. Denote xl = (xl,1,...,x l,n) as the input to the l-th Transformer layer, where nis\nthe sequence length and xl,i is a real-valued vector in Rd for any i.\n4\nReformulation of the self-attention sub-layerDenote ˜xl,i as the output of the (multi-head) self-\nattention sub-layer at position iwith residual connections. The computation of ˜xl,i can be written\nas\n˜xl,i = xl,i + Concat (head1,..., headH) WO,l, (8)\nwhere headk =\nn∑\nj=1\nα(k)\nij [xl,jWV,l\nk ] =\nn∑\nj=1\n(\nexp(e(k)\nij )\n∑n\nq=1 exp(e(k)\niq )\n)\n[xl,jWV,l\nk ], (9)\nand e(k)\nij is computed as the dot product of input xl,i and xl,j with linear projection matrices WQ,l\nk\nand WK,l\nk , i.e., e(k)\nij = d−1/2\nmodel ·(xl,iWQ,l\nk )(xl,jWK,l\nk )T. Considering α(k)\nij as a normalized value of\nthe pair-wise dot product e(k)\nij over j, we can generally reformulate Eqn (8) as\n˜xl,i = xl,i + MultiHeadAttWl\natt\n(xl,i,[xl,1,xl,2,··· ,xl,n]), (10)\nwhere Wl\natt denotes all trainable parameters in the l-th self-attention sub-layer.\nReformulation of the position-wise FFN sub-layerNext, ˜xl,i is put into the position-wise FFN\nsub-layer with residual connections and output xl+1,i. The computation of xl+1,i can be written as\nxl+1,i = ˜xl,i + FFNWl\nﬀn\n(˜xl,i), (11)\nwhere Wl\nﬀn denotes all trainable parameters in the l-th position-wise FFN sub-layer.\nReformulation of Transformer layersCombining Eqn (10) and (11), we reformulate the Trans-\nformer layers2 as\n˜xl,i = xl,i + MultiHeadAttWl\natt\n(xl,i,[xl,1,xl,2,··· ,xl,n]), (12)\nxl+1,i = ˜xl,i + FFNWl\nﬀn\n(˜xl,i). (13)\nWe can see that the Transformer layers (Eqn (12-13)) resemble the multi-particle ODE solver in\nSection 3.1 (Eqn (6-7)). Indeed, we can formally establish the link between the ODE solver with\nsplitting scheme and stacked Transformer layers as below.\nClaim 1. Deﬁne γF∗(xl,i,[xl,1,··· ,xl,n],tl) = MultiHeadAttWl\natt\n(xl,i,[xl,1,··· ,xl,n]) and\nγG∗(xl,i,tl) = FFNWl\nﬀn\n(xl,i). The Transformer can be viewed as a numerical ODE solver us-\ning Lie-Trotter splitting scheme and the Euler’s method (with time stepγ) for Eqn (5) with F∗and\nG∗.\nThe above observation grants a physical interpretation of natural language processing and provides a\nnew perspective on the Transformer architecture. First, this perspective provides a uniﬁed view of the\nheterogeneous components in the Transformer. The self-attention sub-layer is viewed as a diffusion\nterm which characterizes the particle interactions while the position-wise feed-forward networks is\nviewed as a convection term. The two terms together naturally form the convection-diffusion equation\nin physics. Second, this interpretation advances our understanding of the latent representations of\nlanguage through the Transformer. Viewing the feature (a.k.a., embedding) of words in a sequence as\nthe initial position of particles, we can interpret the latent representations of the sentence abstracted\nby the Transformer as particles moving in a high-dimensional space as demonstrated in Figure 1 [50].\n3.3 Improving Transformer Via Strang-Marchuk Splitting Scheme\nIn the previous subsection, we have successfully mapped the Transformer architecture to a numerical\nODE solver for MPDS. However, we would like to point out that one of the key components in this\nODE solver, the Lie-Trotter splitting scheme, is the simplest one but has relatively high errors. In\nthis subsection, we incorporate one of the most popular and widely used splitting scheme [17], the\nStrang-Marchuk splitting scheme, into the design of the neural networks.\n2Layer normalization is sometimes applied to the sub-layers but recent work [49] shows that the normalization\ntrick is not essential and can be removed. One can still readily check that the reformulation (Eqn (12) and (13))\nstill holds with layer normalization.\n5\nMulti-HeadSelf Attention\nContextual Representation\nEmbeddingInputs\nPosition-wiseFeedForward\n×N+\n+\n+\nPositionalEncoding\nDiffusionStep\nConvectionStep\n(a) Original Transformer(b) Macaron Layers\nMulti-HeadSelf Attention\nPosition-wiseFeedForward\n×N\n+ DiffusionStep\nConvectionStep\nPosition-wiseFeedForward+ ×12\n×12\nConvectionStep\n+\nFigure 2: The Transformer and our Macaron architectures.\nThe Lie-Trotter splitting scheme solves the dynamics of F(·) and G(·) alternatively and exclusively\nin that order. This inevitably brings bias and leads to higher local truncation errors [17]. To mitigate\nthe bias, we use a simple modiﬁcation to the Lie-Trotter splitting scheme by dividing the one-step\nnumerical solver for G(·) into two half-steps: we put one half-step before solving F(·) and put the\nother half-step after solving F(·). This modiﬁed splitting scheme is known as the Strang-Marchuk\nsplitting scheme [39]. Mathematically, to compute xi(t+γ) from xi(t), the Strang-Marchuk splitting\nscheme reads as\n˜xi(t) =xi(t) +γ\n2 G(xi(t),t), (14)\nˆxi(t) = ˜xi(t) +γF(˜xi(t),[˜x1(t),˜x2(t),··· ,˜xn(t)],t), (15)\nxi(t+ γ) = ˆxi(t) +γ\n2 G\n(\nˆxi(t),t + γ\n2\n)\n. (16)\nThe Strang-Marchuk splitting scheme enjoys higher-order accuracy than the Lie-Trotter splitting\nscheme [5] in terms of the local truncation error [3], which measures the per-step distance between\nthe true solution and the approximated solution using numerical schemes. Mathematically, for\na differential equation dx(t)\ndt = f(x,t) and a numerical scheme A, the local truncation error of\nnumerical scheme Ais deﬁned as τ = x(t+ γ) −A(x(t),γ). For example, when Ais the Euler’s\nmethod, τEuler = x(t+ γ) −x(t) −γf(x(t),t). The order of local truncation error of the two\nschemes has been studied in [5], as shown in the following theorem.\nTheorem 1. [5] The local truncation error of the Lie-Trotter splitting scheme is second-order\n(O(γ2)) and the local truncation error of the Strang-Marchuk splitting scheme isthird-order (O(γ3)).\nFor completeness, we provide the formal theorem with proof in Appendix A. We can see from Eqn\n(14-16) that the Strang-Marchuk splitting scheme uses a three-step process to solve the ODE. Mapped\nto neural network design, the Strang-Marchuk splitting scheme (together with the Euler’s method)\nsuggests there should also be three sub-layers instead of the two sub-layers in Transformer. By\nreplacing function γF and γGby MultiHeadAtt and FFN, we have\n˜xl,i = xl,i + 1\n2FFNWl,down\nﬀn\n(xl,i), (17)\nˆxl,i = ˜xl,i + MultiHeadAttWl\natt\n(˜xl,i,[˜xl,1,˜xl,2,··· ,˜xl,n]), (18)\nxl+1,i = ˆxl,i + 1\n2FFNWl,up\nffn\n(ˆxl,i). (19)\nFrom Eqn (17-19), we can see that the new layer composes of three sub-layers. Each hidden vector\nat different positions will ﬁrst pass through the ﬁrst position-wise FFN sub-layer with a half-step\nresidual connection (“1\n2 ” in Eqn (17)), and then the output vectors will be feed into a self-attention\nsub-layer. In the last step, the vectors outputted from the self-attention sub-layer will be put into the\nsecond position-wise FFN sub-layer with a half-step residual connection. Since the FFN-attention-\nFFN structure is “Macaron”-like, we call the layer as Macaron layer and call the network using\n6\nTable 1: Translation performance (BLEU) on IWSLT14 De-En and WMT14 En-De testsets.\nIWSLT14 De-En WMT14 En-De\nMethod small base big\nTransformer [43] 34.4 27.3 28.4\nWeighted Transformer [1] / 28.4 28.9\nRelative Transformer [36] / 26.8 29.2\nUniversal Transformer [12] / 28.9 /\nScaling NMT [29] / / 29.3\nDynamic Conv [48] 35.2 / 29.7\nMacaron Net 35.4 28.9 30.2\nMacaron layers as Macaron Net, as shown in Figure 2(b). Previous works [25, 52] have successfully\ndemonstrated that the neural network architectures inspired from higher-order accurate numerical\nODE solvers will lead to better results in deep learning and we believe the Macaron Net can achieve\nbetter performance on practical natural language processing applications than the Transformer.\n4 Experiments\nWe test our proposed Macaron architectures in both supervised and unsupervised learning setting.\nFor supervised learning setting, we use IWLST14 and WMT14 machine translation datasets. For\nunsupervised learning setting, we pretrain the model using the same method as in [13] and test the\nlearned model over a set of downstream tasks. Due to space limitations, we put dataset description,\nmodel description, hyperparameter conﬁguration into Appendix B.\n4.1 Experiment Settings\nMachine Translation Machine translation is an important application for natural language process-\ning [43]. We evaluate our methods on two widely used public datasets: IWSLT14 German-to-English\n(De-En) and WMT14 English-to-German (En-De) dataset.\nFor the WMT14 dataset, the basic conﬁgurations of the Transformer architecture are the base and\nthe big settings [43]. Both of them consist of a 6-layer encoder and 6-layer decoder. The size of the\nhidden nodes and embeddings are set to 512 for base and 1024 for big. The number of heads are\n8 for base and 16 for big. Since the IWSLT14 dataset is much smaller than the WMT14 dataset,\nthe small setting is usually used, whose size of hidden states and embeddings is set to 512 and the\nnumber of heads is set to 4. For all settings, the dimensionality of the inner-layer of the position-wise\nFFN is four times of the dimensionality of the hidden states.\nFor each setting (base, big and small), we replace all Transformer layers by the Macaron layers3\nand obtain the base, big and small Macaron, each of which contains two position-wise feed-forward\nsub-layers in a layer. To make a fair comparison, we set the dimensionality of the inner-layer of the\ntwo FFN sub-layers in the Macaron layers to two times of the dimensionality of the hidden states. By\ndoing this, the base, big and small Macaron have the same number of parameters as the base, big\nand small Transformer respectively.\nUnsupervised Pretraining BERT [13] is the current state-of-the-art pre-trained contextual rep-\nresentation model based on a multi-layer Transformer encoder architecture and trained by masked\nlanguage modeling and next-sentence prediction tasks. We compare our proposed Macaron Net with\nthe base setting from the original BERT paper [13], which consists of 12 Transformer layers. The\nsize of hidden states and embeddings are set to 768, and the number of attention heads is set to 12.\nSimilarly, we replace the Transformer layers in BERTbase by the Macaron layers and reduce the\n3The translation model is based on the encoder-decoder framework. In the Transformer, the decoder layer\nhas a third sub-layer which performs multi-head attention over the output of the encoder stack (encoder-decoder-\nattention) and a mask to prevent positions from attending to subsequent positions. In our implementation of\nMacaron decoder, we also use masks and split the FFN into two sub-layers and thus our decoder layer is (FFN,\nself-attention, encoder-decoder-attention, and FFN).\n7\nTable 2: Test results on the GLUE benchmark (except WNLI).\nMethod CoLA SST-2 MRPC STS-B QQP MNLI -m/mm QNLI RTE GLUE\nExisting systems\nELMo [31] 33.6 90.4 84.4/78.0 74.2/72.3 63.1/84.3 74.1/74.5 79.8 58.9 70.0\nOpenAI GPT [32] 47.2 93.1 87.7/83.7 85.3/84.8 70.1/88.1 80.7/80.6 87.2 69.1 76.9\nBERTbase[13] 52.1 93.5 88.9/84.8 87.1/85.8 71.2/89.2 84.6/83.4 90.5 66.4 78.3\nOur systems\nBERTbase(ours) 52.8 92.8 87.3/83.0 81.2/80.0 70.2/88.4 84.4/83.7 90.4 64.9 77.4\nMacaron Netbase 57.6 94.0 88.4/84.4 87.5/86.3 70.8/89.0 85.4/84.5 91.6 70.5 79.7\ndimensionality of the inner-layer of the two FFN sub-layers by half, and thus we keep the number of\nparameters of our Macaron base same as BERT base.\n4.2 Experiment Results\nMachine Translation We use BLEU [ 30] as the evaluation measure for machine translation.\nFollowing common practice, we use tokenized case-sensitive BLEU and case-insensitive BLEU for\nWMT14 En-De and IWSLT14 De-En respectively.\nThe results for machine translation are shown in Table 1. For the IWSLT14 dataset, our Macaron\nsmall outperforms the Transformer small by 1.0 point in terms of BLEU. For the WMT14 dataset,\nour Macaron base outperforms its Transformer counterpart by 1.6 BLEU points. Furthermore, the\nperformance of our Macaron base model is even better than that of the Transformer big model\nreported in [ 43], but with much less number of parameters. Our Macaron big outperforms the\nTransformer big by 1.8 BLEU points. Comparing with other concurrent works, the improvements in\nour proposed method are still signiﬁcant.\nUnsupervised Pretraining Following [13], we evaluate all models by ﬁne-tuning them on 8\ndownstream tasks in the General Language Understanding Evaluation (GLUE) benchmark [ 44],\nincluding CoLA [45], SST-2 [37], MRPC [14], STS-B [6], QQP [9], MNLI [47], QNLI [34], and RTE\n[4]. More details about individual tasks and their evaluation metrics can be found in Appendix B.3\nand [44, 13]. To ﬁne-tune the models, we follow the hyperparameter search space in [ 13] for all\ndownstream tasks, including different batch sizes, learning rates, and numbers of epochs.\nThe GLUE results are presented in Table 2. We present the results of two BERTbase models. One\nis from [13], and the other is trained using our own data. Due to that some pieces of data used in\n[13] are no longer freely distributed, the two numbers are slightly different. We can see from the\ntable, our proposed Macaron Net base outperforms all baselines in terms of the general GLUE score.\nSpeciﬁcally, given the same dataset, our proposed model outperforms our trained BERT base model\nin all tasks. Even comparing with the BERT base model in [13], our model performs better in 6 out\nof 8 tasks and achieves close performance in the rest 2 tasks. Comparing our results with original\nBERT model, we can see that given a ﬁxed number of parameters and a ﬁxed number of attention\nsub-layers, stacking more FFN sub-layers can get better performance on downstream tasks.\nAs a summary, the improvement in both machine translation and GLUE tasks well aligns with the\nODE theory and our proposed architecture performs better than the Transformer in real practice.\n5 Conclusion and Future Work\nIn this paper, we interpret the Transformer as a numerical ODE solver for a convection-diffusion\nequation in a multi-particle dynamic system. Speciﬁcally, how words in a sentence are abstracted\ninto contexts by passing through the layers of the Transformer can be interpreted as approximating\nmultiple particles’ movement in the space using the Lie-Trotter splitting scheme and the Euler’s\nmethod. By replacing the Lie-Trotter splitting scheme with the more advanced Strang-Marchuk\nsplitting scheme, we obtain a new architecture. The improvements in real applications show the\neffectiveness of the new model and are consistent with the ODE theory. In the future, we will explore\ndeeper connections between the ODE theory and the Transformer models and use the ODE theory to\nimprove the individual components in the Transformer architecture such as attention modules.\n8\nAcknowledgements\nWe would like to thank Shuonan Wu for helpful discussions and Linyuan Gong for the help on\nunsupervised pretraining experiments.\nReferences\n[1] K. Ahmed, N. S. Keskar, and R. Socher. Weighted transformer network for machine translation.\narXiv preprint arXiv:1711.02132, 2017.\n[2] R. Al-Rfou, D. Choe, N. Constant, M. Guo, and L. Jones. Character-level language modeling\nwith deeper self-attention. arXiv preprint arXiv:1808.04444, 2018.\n[3] U. M. Ascher and L. R. Petzold. Computer methods for ordinary differential equations and\ndifferential-algebraic equations, volume 61. Siam, 1998.\n[4] L. Bentivogli, I. Dagan, H. T. Dang, D. Giampiccolo, and B. Magnini. The ﬁfth PASCAL\nrecognizing textual entailment challenge. 2009.\n[5] A. V . Bobylev and T. Ohwada. The error of the splitting scheme for solving evolutionary\nequations. Applied Mathematics Letters, 14(1):45–48, 2001.\n[6] D. Cer, M. Diab, E. Agirre, I. Lopez-Gazpio, and L. Specia. Semeval-2017 task 1: Se-\nmantic textual similarity-multilingual and cross-lingual focused evaluation. arXiv preprint\narXiv:1708.00055, 2017.\n[7] B. Chang, M. Chen, E. Haber, and E. H. Chi. Antisymmetricrnn: A dynamical system view on\nrecurrent neural networks. arXiv preprint arXiv:1902.09689, 2019.\n[8] T. Q. Chen, Y . Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential\nequations. In Advances in Neural Information Processing Systems, pages 6572–6583, 2018.\n[9] Z. Chen, H. Zhang, X. Zhang, and L. Zhao. Quora question pairs. 2018.\n[10] C. Chicone. Ordinary differential equations by vladimir i. arnold. Siam Review, 49(2):335–336,\n2007.\n[11] Z. Dai, Z. Yang, Y . Yang, W. W. Cohen, J. Carbonell, Q. V . Le, and R. Salakhutdinov.\nTransformer-xl: Attentive language models beyond a ﬁxed-length context. arXiv preprint\narXiv:1901.02860, 2019.\n[12] M. Dehghani, S. Gouws, O. Vinyals, J. Uszkoreit, and Ł. Kaiser. Universal transformers. arXiv\npreprint arXiv:1807.03819, 2018.\n[13] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[14] W. B. Dolan and C. Brockett. Automatically constructing a corpus of sentential paraphrases. In\nProceedings of the International Workshop on Paraphrasing., 2005.\n[15] S. Edunov, M. Ott, M. Auli, and D. Grangier. Understanding back-translation at scale. arXiv\npreprint arXiv:1808.09381, 2018.\n[16] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y . N. Dauphin. Convolutional Sequence to\nSequence Learning. In Proc. of ICML, 2017.\n[17] J. Geiser. Decomposition methods for differential equations: theory and applications. CRC\nPress, 2009.\n[18] R. Glowinski, S. J. Osher, and W. Yin. Splitting methods in communication, imaging, science,\nand engineering. Springer, 2017.\n[19] E. Haber and L. Ruthotto. Stable architectures for deep neural networks. Inverse Problems,\n34(1):014004, 2017.\n9\n[20] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European\nconference on computer vision, pages 630–645. Springer, 2016.\n[21] P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen,\nC. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. Moses: Open source\ntoolkit for statistical machine translation. In ACL, 2007.\n[22] J. Lei Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016.\n[23] H. J. Levesque, E. Davis, and L. Morgenstern. The Winograd schema challenge. In AAAI\nSpring Symposium: Logical Formalizations of Commonsense Reasoning., volume 46, page 47,\n2011.\n[24] Q. Liao and T. Poggio. Bridging the gaps between residual learning, recurrent neural networks\nand visual cortex. arXiv preprint arXiv:1604.03640, 2016.\n[25] Y . Lu, A. Zhong, Q. Li, and B. Dong. Beyond ﬁnite layer neural networks: Bridging deep\narchitectures and numerical differential equations. arXiv preprint arXiv:1710.10121, 2017.\n[26] B. W. Matthews. Comparison of the predicted and observed secondary structure of t4 phage\nlysozyme. Biochimica et Biophysica Acta (BBA)-Protein Structure, 405(2):442–451, 1975.\n[27] R. I. McLachlan and G. R. W. Quispel. Splitting methods. Acta Numerica, 11:341–434, 2002.\n[28] F. R. Moulton. An introduction to celestial mechanics. Courier Corporation, 2012.\n[29] M. Ott, S. Edunov, D. Grangier, and M. Auli. Scaling neural machine translation.arXiv preprint\narXiv:1806.00187, 2018.\n[30] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluation of\nmachine translation. In Proceedings of the 40th annual meeting on association for computational\nlinguistics, pages 311–318. Association for Computational Linguistics, 2002.\n[31] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep\ncontextualized word representations. arXiv preprint arXiv:1802.05365, 2018.\n[32] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding\nby generative pre-training.\n[33] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are\nunsupervised multitask learners. 2019.\n[34] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. SQuAD: 100,000+ questions for machine com-\nprehension of text. InProceedings of EMNLP ., pages 2383–2392. Association for Computational\nLinguistics, 2016.\n[35] R. Sennrich, B. Haddow, and A. Birch. Neural machine translation of rare words with subword\nunits. In ACL, 2016.\n[36] P. Shaw, J. Uszkoreit, and A. Vaswani. Self-attention with relative position representations.\narXiv preprint arXiv:1803.02155, 2018.\n[37] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts. Recursive deep\nmodels for semantic compositionality over a sentiment treebank. In Proceedings of EMNLP .,\npages 1631–1642, 2013.\n[38] S. Sonoda and N. Murata. Transport analysis of inﬁnitely deep neural network. The Journal of\nMachine Learning Research, 20(1):31–82, 2019.\n[39] G. Strang. On the construction and comparison of difference schemes. SIAM Journal on\nNumerical Analysis, 5(3):506–517, 1968.\n[40] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception archi-\ntecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 2818–2826, 2016.\n10\n[41] Y . Tao, Q. Sun, Q. Du, and W. Liu. Nonlocal neural networks, nonlocal diffusion and nonlocal\nmodeling. In Advances in Neural Information Processing Systems, pages 496–506, 2018.\n[42] M. Thorpe and Y . van Gennip. Deep limits of residual neural networks. arXiv preprint\narXiv:1810.11741, 2018.\n[43] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and\nI. Polosukhin. Attention is all you need. In Advances in neural information processing systems,\npages 5998–6008, 2017.\n[44] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. GLUE: A multi-task\nbenchmark and analysis platform for natural language understanding. 2019. In the Proceedings\nof ICLR.\n[45] A. Warstadt, A. Singh, and S. R. Bowman. Neural network acceptability judgments. arXiv\npreprint 1805.12471, 2018.\n[46] E. Weinan. A proposal on machine learning via dynamical systems. Communications in\nMathematics and Statistics, 5(1):1–11, 2017.\n[47] A. Williams, N. Nangia, and S. R. Bowman. A broad-coverage challenge corpus for sentence\nunderstanding through inference. In Proceedings of NAACL-HLT., 2018.\n[48] F. Wu, A. Fan, A. Baevski, Y . N. Dauphin, and M. Auli. Pay less attention with lightweight and\ndynamic convolutions. arXiv preprint arXiv:1901.10430, 2019.\n[49] H. Zhang, Y . N. Dauphin, and T. Ma. Fixup initialization: Residual learning without normaliza-\ntion. arXiv preprint arXiv:1901.09321, 2019.\n[50] W. Zhang. The bonﬁre of time. In Tragoidia, editor, Iris Long, 2019.\n[51] X. Zhang, Y . Lu, J. Liu, and B. Dong. Dynamically unfolding recurrent restorer: A moving\nendpoint control method for image restoration. In International Conference on Learning\nRepresentations, 2019.\n[52] M. Zhu, B. Chang, and C. Fu. Convolutional neural networks combined with runge-kutta\nmethods. arXiv preprint arXiv:1802.08831, 2018.\n[53] Y . Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning\nbooks and movies: Towards story-like visual explanations by watching movies and reading\nbooks. In arXiv preprint arXiv:1506.06724, 2015.\n11\nAppendix\nA Proof of the Theorem\nTheorem 2.([5]) We denote the true solution of equationdx\ndt = F(x),x(0) =y04 at time tas x(t) =\nSt\nF(y0). Simliarily, we can deﬁneSt\nGand St\nF+G. The local truncation error att= 0of the Lie-Trotter\nsplitting scheme is Sγ\nF+G(y0) −Sγ\nG[Sγ\nF(y0)] =γ2\n2 {F′(y0)(y0)G(y0) −G′(y0)(y0)F(y0)}+ O(γ3)\nwhich is second-order. The local truncation error at t= 0of the Strang-Marchuk splitting scheme\nSγ/2\nG {Sγ\nF[Sγ/2\nG (y0)]}is Sγ\nF+G(y0) −Sγ/2\nG {Sγ\nF[Sγ/2\nG (y0)]}= O(γ3) which is third-order.\nProof. Since\nSγ\nF(y0) =y0 + γF(y0) +γ2\n2 F′(y0)F(y0) +O(γ3),\nSγ\nG(y0) =y0 + γG(y0) +γ2\n2 G′(y0)G(y0) +O(γ3),\nwe have\nSγ\nG[Sγ\nF(y0)] =Sγ\nF(y0) +γG(Sγ\nF(y0)) +γ2\n2 G′(Sγ\nF(y0))G(Sγ\nF(y0)) +O(γ3).\nAt the same time, we have\nG(y0 + γF(y0) +O(γ2)) =G(y0) +γG′(y0)F(y0) +O(γ2),\nG′(Sγ\nF(y0))G(Sγ\nF(y0)) =G′(y0)G(y0) +O(γ).\nCombine the estimations we have\nSγ\nG[Sγ\nF(y0)] =y0 + γ[F(y0) +G(y0)]\n+ γ2\n2 [G′(y0)G(y0) +F′(y0)F(y0) + 2G′(y0)F(y0)] +O(γ3).\nAs a result, we can estimate the local truncation error of Lie-Trotter splitting scheme as\nSγ\nF+G(y0) −Sγ\nG[Sγ\nF(y0)]\n= y0 + γ(F(y0) +G(y0)) +γ2\n2 (F′(y0) +G′(y0))(F(y0) +G(y0)) +O(γ3)\n−(y0 + γ[F(y0) +G(y0)] +γ2\n2 [G′(y0)G(y0) +F′(y0)F(y0) + 2G′(y0)F(y0)] +O(γ3))\n= γ2\n2 {F′(y0)G(y0) −G′(y0)F(y0)}+ O(γ3).\nTo estimate the Strang-Marchuk splitting scheme’s local truncation error, we rewrite the Strang-\nMarchuk splitting scheme as\nSγ/2\nG {Sγ\nF[Sγ/2\nG (y0)]}= Sγ/2\nG {Sγ/2\nF {Sγ/2\nF [Sγ/2\nG (y0)]}}.\nFrom the previous estimation of Lie–Trotter splitting scheme we have\nSγ/2\nF+G(y0) −Sγ\nG[Sγ\nF(y0)] =γ2\n8 {F′(y0)G(y0) −G′(y0)F(y0)}+ O(γ3),\nSγ/2\nF+G(y0) −Sγ\nF[Sγ\nG(y0)] =γ2\n8 {G′(y0)F(y0) −F′(y0)G(y0)}+ O(γ3).\n4Since a time-dependent ODE can be formulated as a time-independent ODE by introducing an auxiliary\nvariable [10], the theorem here developed for time-independent ODEs can also be applied to time-dependent\nODEs without loss of generality.\n12\nCombine the two estimations, we have\nSγ/2\nG {Sγ/2\nF {Sγ/2\nF [Sγ/2\nG (y0)]}}= Sγ\nF+G(y0) +γ2\n8 {F′(y0)G(y0) −G′(y0)F(y0)}\n+ γ2\n8 {G′(y0)F(y0) −F′(y0)G(y0)}+ O(γ3)\n= Sγ\nF+G(y0) +O(γ3).\nB Experiment Settings\nB.1 Machine Translation\nDataset The training/validation/test sets of the IWSLT14 dataset contain about 153K/7K/7K sen-\ntence pairs, respectively. We use a vocabulary of 10K tokens based on a joint source and target\nbyte pair encoding (BPE) [35]. For WMT14 dataset, we replicate the setup of [43], which contains\n4.5M training parallel sentence pairs. Newstest2014 is used as the test set, and Newstest2013 is used\nas the validation set. The 37K vocabulary for WMT14 is based on a joint source and target BPE\nfactorization.\nModel For the WMT14 dataset, the basic conﬁgurations of the Transformer architecture are the\nbase and the big settings [43]. Both of them consist of a 6-layer encoder and 6-layer decoder. The\nsize of the hidden nodes and embeddings are set to 512 for base and 1024 for big. The number of\nheads are 8 for base and 16 for big. Since the IWSLT14 dataset is much smaller than the WMT14\ndataset, the small setting is usually used, whose size of hidden states and embeddings is set to 512\nand the number of heads is set to 4. For all settings, the dimensionality of the inner-layer of the\nposition-wise FFN is four times of the dimensionality of the hidden states.\nFor each setting (base, big and small), we replace all Transformer layers by the Macaron layers\nand obtain the base, big and small Macaron, each of which contains two position-wise feed-\nforward sub-layers in a layer. The translation model is based on the encoder-decoder framework.\nIn the Transformer, the decoder layer has a third sub-layer which performs multi-head attention\nover the output of the encoder stack (encoder-decoder-attention) and a mask to prevent positions\nfrom attending to subsequent positions. In our implementation of Macaron decoder, we also use\nmasks and split the FFN into two sub-layers and thus our decoder layer is (FFN, self-attention,\nencoder-decoder-attention and FFN).\nTo make a fair comparison, we set the dimensionality of the inner-layer of the two FFN sub-layers\nin the Macaron layers to two times of the dimensionality of the hidden states. By doing this, the\nbase, big and small Macaron have the same number of parameters as the base, big and small\nTransformer respectively.\nOptimizer and training We use the Adam optimizer and follow the optimizer setting and learning\nrate schedule in [43]. For the big setting, we enlarge the batch size and learning rate as suggested in\n[29] to accelerate training. We employ label smoothing of value ϵls = 0.1 [40] in all experiments.\nModels for WMT14/IWSLT14 are trained on 4/1 NVIDIA P40 GPUs respectively. Our code is based\non the open-sourced fairseq [16] code base in PyTorch toolkit.\nEvaluation We use BLEU5 [30] as the evaluation measure for machine translation. Following\ncommon practice, we use tokenized case-sensitive BLEU and case-insensitive BLEU for WMT14\nEn-De and IWSLT14 De-En respectively. During inference, we use beam search with beam size 4\nand length penalty 0.6 for WMT14, and beam size 5 and length penalty 1.0 for IWSLT14, following\n[43].\n5https://github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/multi-bleu.\nperl\n13\nB.2 Unsupervised Pretraining\nPre-training dataset We follow [13] to use English Wikipedia corpus and BookCorpus for pre-\ntraining. As the dataset BookCorpus [53] is no longer freely distributed. We follow the suggestions\nfrom [13] to crawl and collect BookCorpus6 on our own. The concatenation of two datasets includes\nroughly 3.4B words in total, which is comparable with the data corpus used in [13]. We ﬁrst segment\ndocuments into sentences with Spacy;7 Then, we normalize, lower-case, and tokenize texts using\nMoses[21] and apply BPE[35]. We randomly split documents into one training set and one validation\nset. The training-validation ratio for pre-training is 199:1.\nModel We compare our proposed Macaron Net with thebase setting from the original BERT paper\n[13], which consists of 12 Transformer layers. The size of hidden states and embeddings are set to\n768, and the number of attention heads is set to 12. Similarly, we replace the Transformer layers in\nBERT base by the Macaron layers and reduce the dimensionality of the inner-layer of the two FFN\nsub-layers by half, and thus we keep the number of parameters of our Macaron base as the same as\nBERT base.\nOptimizer and training We follow [13] to use two tasks to pretrain our model. One task is masked\nlanguage modeling, which masks some percentage of the input tokens at random, and then requires\nthe model to predict those masked tokens. Another task is next sentence prediction, which requires\nthe model to predict whether two sentences in a given sentence pair are consecutive. We use the\nAdam optimizer and and follow the optimizer setting and learning rate schedule in [13] and trained\nthe model on 4 NVIDIA P40 GPUs.\nB.3 GLUE Dataset\nWe provide a brief description of the tasks in the GLUE benchmark [44] and our ﬁne-tuning process\non the GLUE datasets.\nCoLA The Corpus of Linguistic Acceptability [45] consists of English acceptability judgments\ndrawn from books and journal articles on linguistic theory. The task is to predict whether an example\nis a grammatical English sentence. The performance is evaluated by Matthews correlation coefﬁcient\n[26].\nSST-2 The Stanford Sentiment Treebank [ 37] consists of sentences from movie reviews and\nhuman annotations of their sentiment. The task is to predict the sentiment of a given sentence\n(positive/negative). The performance is evaluated by the test accuracy.\nMRPC The Microsoft Research Paraphrase Corpus [14] is a corpus of sentence pairs automatically\nextracted from online news sources, with human annotations for whether the sentences in the pair are\nsemantically equivalent, and the task is to predict the equivalence. The performance is evaluated by\nboth the test accuracy and the test F1.\nSTS-B The Semantic Textual Similarity Benchmark [6] is a collection of sentence pairs drawn\nfrom news headlines, video and image captions, and natural language inference data. Each pair\nis human-annotated with a similarity score from 1 to 5; the task is to predict these scores. The\nperformance is evaluated by Pearson and Spearman correlation coefﬁcients.\nQQP The Quora Question Pairs 8 [9] dataset is a collection of question pairs from the commu-\nnity question-answering website Quora. The task is to determine whether a pair of questions are\nsemantically equivalent. The performance is evaluated by both the test accuracy and the test F1.\nMNLI The Multi-Genre Natural Language Inference Corpus [ 47] is a crowdsourced collection\nof sentence pairs with textual entailment annotations. Given a premise sentence and a hypothesis\nsentence, the task is to predict whether the premise entails the hypothesis (entailment ), contradicts\n6https://www.smashwords.com/\n7https://spacy.io\n8https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs\n14\nthe hypothesis (contradiction), or neither (neutral). The performance is evaluated by the test accuracy\non both matched (in-domain) and mismatched (cross-domain) sections of the test data.\nQNLI The Question-answering NLI dataset is converted from the Stanford Question Answering\nDataset (SQuAD) [34] to a classiﬁcation task. The performance is evaluated by the test accuracy.\nRTE The Recognizing Textual Entailment (RTE) datasets come from a series of annual textual\nentailment challenges [4]. The task is to predict whether sentences in a sentence pair are entailment.\nThe performance is evaluated by the test accuracy.\nWNLI The Winograd Schema Challenge [23] is a reading comprehension task in which a system\nmust read a sentence with a pronoun and select the referent of that pronoun from a list of choices. We\nfollow [13] to skip this task in our experiments, because few previous works do better than predicting\nthe majority class for this task.\nFine-tuning on GLUE tasksTo ﬁne-tune the models, following [13], we search the optimization\nhyperparmaters in a search space including different batch sizes (16/32), learning rates (5e-3/3e-5),\nnumber of epochs (3/4/5), and a set of different random seeds. We select the model for testing\naccording to their performance on the development set.\nTest data Note that the GLUE dataset distribution does not include the Test labels, and we only\nmade a single GLUE evaluation server submission9 for each of our models.\n9https://gluebenchmark.com\n15",
  "topic": "Ode",
  "concepts": [
    {
      "name": "Ode",
      "score": 0.6800971031188965
    },
    {
      "name": "Transformer",
      "score": 0.6131333708763123
    },
    {
      "name": "Computer science",
      "score": 0.5651299953460693
    },
    {
      "name": "Discretization",
      "score": 0.5261520743370056
    },
    {
      "name": "Architecture",
      "score": 0.46979913115501404
    },
    {
      "name": "Algorithm",
      "score": 0.39220041036605835
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3666072487831116
    },
    {
      "name": "Theoretical computer science",
      "score": 0.35215479135513306
    },
    {
      "name": "Mathematics",
      "score": 0.32245224714279175
    },
    {
      "name": "Applied mathematics",
      "score": 0.30696025490760803
    },
    {
      "name": "Mathematical analysis",
      "score": 0.1969403624534607
    },
    {
      "name": "Electrical engineering",
      "score": 0.1498166024684906
    },
    {
      "name": "Engineering",
      "score": 0.11481964588165283
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ]
}