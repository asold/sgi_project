{
    "title": "How Does ChatGPT Perform on the Medical Licensing Exams? The Implications of Large Language Models for Medical Education and Knowledge Assessment",
    "url": "https://openalex.org/W4313232683",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3134629398",
            "name": "Aidan Gilson",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A4284204345",
            "name": "Conrad Safranek",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A2101979092",
            "name": "Thomas Huang",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A2787252827",
            "name": "Vimig Socrates",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A1966855885",
            "name": "Ling Chi",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A2171721584",
            "name": "R. Andrew Taylor",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A2313261013",
            "name": "David Chartash",
            "affiliations": [
                "University College Dublin",
                "National University of Ireland",
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A3134629398",
            "name": "Aidan Gilson",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A4284204345",
            "name": "Conrad Safranek",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A2101979092",
            "name": "Thomas Huang",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A2787252827",
            "name": "Vimig Socrates",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A1966855885",
            "name": "Ling Chi",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A2171721584",
            "name": "R. Andrew Taylor",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A2313261013",
            "name": "David Chartash",
            "affiliations": [
                "Yale University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4285124505",
        "https://openalex.org/W2987501933",
        "https://openalex.org/W2058046738"
    ],
    "abstract": "ABSTRACT Background ChatGPT is a 175 billion parameter natural language processing model which can generate conversation style responses to user input. Objective To evaluate the performance of ChatGPT on questions within the scope of United States Medical Licensing Examination (USMLE) Step 1 and Step 2 exams, as well as analyze responses for user interpretability. Methods We used two novel sets of multiple choice questions to evaluate ChatGPT’s performance, each with questions pertaining to Step 1 and Step 2. The first was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the userbase. The second, was the National Board of Medical Examiners (NBME) Free 120-question exams. After prompting ChatGPT with each question, ChatGPT’s selected answer was recorded, and the text output evaluated across three qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question. Results On the four datasets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBMEFree-Step2, ChatGPT achieved accuracies of 44%, 42%, 64.4%, and 57.8%. The model demonstrated a significant decrease in performance as question difficulty increased (P=.012) within the AMBOSSStep1 dataset. We found logical justification for ChatGPT’s answer selection was present in 100% of outputs. Internal information to the question was present in &gt; 90% of all questions. The presence of information external to the question was respectively 54.5% and 27% lower for incorrect relative to correct answers on the NBME-Free-Step1 and NBME-Free-Step2 datasets (P&lt;=.001). Conclusion ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at greater than 60% threshold on the NBME-FreeStep-1 dataset we show that the model is comparable to a third year medical student. Additionally, due to the dialogic nature of the response to questions, we demonstrate ChatGPT’s ability to provide reasoning and informational context across the majority of answers. These facts taken together make a compelling case for the potential applications of ChatGPT as a medical education tool.",
    "full_text": "Ho w Does ChatGPT Perform on the Medical\nLicensing Exams? The Implications of Large\nLanguage Models for Medical Education and\nKnowledge Assessment\nAidan Gilson1,2, Conrad Safranek1, Thomas Huang2, Vimig Socrates1,3,\nLing Chi1, R. Andrew Taylor1,2, and David Chartash1,4\n1Section for Biomedical Informatics and Data Science, Yale University School of\nMedicine\n2Department of Emergency Medicine, Yale School of Medicine\n3Program of Computational Biology and Bioinformatics, Yale University\n4School of Medicine, University College Dublin - National University of Ireland, Dublin\nABSTRACT\nBac\nkground: ChatGPT is a 175 billion parameter natural language processing model which can gen-\nerate conversation style responses to user input.\nObjective: T o evaluate the performance of ChatGPT on questions within the scope of United States\nMedical Licensing Examination (USMLE) Step 1 and Step 2 exams, as well as analyze responses for\nuser interpretability .\nMethods: W e used two novel sets of multiple choice questions to evaluate ChatGPT’s performance,\neach with questions pertaining to Step 1 and Step 2. The first was derived from AMBOSS, a com-\nmonly used question bank for medical students, which also provides statistics on question diﬀiculty\nand the performance on an exam relative to the userbase. The second, was the National Board of\nMedical Examiners (NBME) F ree 120-question exams. After prompting ChatGPT with each question,\nChatGPT’s selected answer was recorded, and the text output evaluated across three qualitative met-\nrics: logical justification of the answer selected, presence of information internal to the question, and\npresence of information external to the question.\nResults: On the four datasets, AMBOSS-Step1, AMBOSS-Step2, NBME-F ree-Step1, and NBME-\nF ree-Step2, ChatGPT achieved accuracies of 44%, 42%, 64.4%, and 57.8%. The model demonstrated\na significant decrease in performance as question diﬀiculty increased (P=.012) within the AMBOSS-\nStep1 dataset. W e found logical justification for ChatGPT’s answer selection was present in 100% of\noutputs. Internal information to the question was present in >90% of all questions. The presence of\ninformation external to the question was respectively 54.5% and 27% lower for incorrect relative to\ncorrect answers on the NBME-F ree-Step1 and NBME-F ree-Step2 datasets (P<=.001).\nConclusion: ChatGPT marks a significant improvement in natural language processing models on the\ntasks of medical question answering. By performing at greater than 60% threshold on the NBME-F ree-\nStep-1 dataset we show that the model is comparable to a third year medical student. Additionally ,\ndue to the dialogic nature of the response to questions, we demonstrate ChatGPT’s ability to provide\nreasoning and informational context across the majority of answers. These facts taken together make\na compelling case for the potential applications of ChatGPT as a medical education tool.\nKeywords: Natural Language Processing; MedQA; GPT; Medical Education; Chatbots; Ar-\ntificial Intelligence; Education T echnology .\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 26, 2022. ; https://doi.org/10.1101/2022.12.23.22283901doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nINTR ODUCTION\nChatGPT, or Chat Generative Pre-trained T ransformer, is a 175 billion parameter natural\nlanguage processing (NLP) model that uses deep learning algorithms trained on vast amounts\nof data to generate human-like responses to user prompts. 1 As a general purpose dialogic agent,\nChatGPT is designed to be able to respond to a wide range of topics, potentially making it a\nuseful tool for customer service, chatbots, and a host of other applications. Since its release, it\nhas garnered significant press for both seemingly incredible feats like automated generation of\nresponses in the style of Shakespearean sonnets while also failing to answer simple mathematical\nquestions. 2–4\nWithin the medical domain, language models have been investigated as tools for personalized\npatient interaction 5 and consumer health education. 6 While demonstrating potential, these\nmodels have had limited success in areas testing clinical knowledge through generative question-\nanswering (QA) tasks. 7,8 ChatGPT builds on OpenAI’s previous GPT-3.5 language models\nwith the addition of both supervised and reinforcement learning techniques. 9 ChatGPT could\nrepresent the first in a new line of models which may better represent the combination of clinical\nknowledge and dialogic interaction. Although it is not an information retrieval tool like Google\nScholar, 10 UpT oDate,11 DynaMed, 12 or PubMed, 13 its response format of unique narrative\nreplies allows for novel use cases, including acting as a simulated patient, a brainstorming tool\nproviding individual feedback, and acting as a fellow classmate to simulate small-group style\nlearning. F or these applications to be useful, however, ChatGPT must exhibit a level of medical\nknowledge and reasoning that allows suﬀicient confidence in its responses.\nIn this paper we aim to assess the medical knowledge of ChatGPT by quantifying its performance\nthrough the use of two question sets centered around knowledge tested in the United States\nMedical Licensing Examination (USMLE) Step 1 and Step 2 exams. In addition, to further\nassess the ability of ChatGPT to serve as a virtual medical tutor, we qualitatively examine the\nintegrity of ChatGPT’s responses with regards to logical justification and use of intrinsic and\nextrinsic information.\nMETHODS\nMedical Education Datasets\nW e created two pairs of novel datasets to examine ChatGPT’s understanding of medical knowl-\nedge related to Step 1 and Step 2. W e first selected a subset of 100 questions from AMBOSS,14 a\nwidely used question bank which contains over 2700 Step 1 and 3150 Step 2 CK questions. The\nexisting performance statistics from previous AMBOSS users allows us to determine the models\nrelative performance. W e call these datasets AMBOSS-Step1 and AMBOSS-Step2, respectively .\nAMBOSS provides users with an Attending Tip when they have diﬀiculty with a question, as\nwell as a diﬀiculty rating (1-5). W e included these tips in our dataset to determine if additional\ncontext improves predictive performance.\nW e also used the list of 120 free Step 1 and Step 2 CK questions developed by the National\nBoard of Medical Examiners (NBME), which we call NBME-F ree-Step1 and NBME-F ree-Step2,\nto evaluate ChatGPT’s performance on questions most closely aligned with those from the true\nlicensure exams.\nPrompt Engineering\nDue to the significant impact that prompt engineering has been shown to have on generative\nLM output, we standardized the input formats of the AMBOSS and NBME-F ree datasets. First,\nwe removed any questions that include an image, as ChatGPT only accepts textual input. Next\nwe removed questions where the answer was formatted as a table. This was done so that the\naccuracy of ChatGPT’s answers was solely dependent on its ability to synthesize medical knowl-\nedge within narrative text, rather than parse complicated text inputs. Questions were formatted\nwith the question text followed by the direct question separated by newline. In the AMBOSS\ndatasets, the attending tip was inserted as a separate instance of the question. F ollowing the\n2/9\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 26, 2022. ; https://doi.org/10.1101/2022.12.23.22283901doi: medRxiv preprint \nquestion text and direct question, the multiple choice answers were provided, separated again\nby a newline. An example follows:\nExample\nQuestion:\nA 1-hour-old male newborn is examined in the hospital nursery . He was born at\nterm to a 22-year-old woman, gravida 1, para 1, with HIV infection. The mother\ntook antiretroviral therapy throughout her pregnancy , and her HIV viral load was\nconsistently <1000 copies/mL. She received intravenous zidovudine (AZT) during\nlabor. The patient’s Apgar scores were 8 and 9 at 1 and 5 minutes, respectively . He\nis at the 50th percentile for length, weight, and head circumference. He appears well.\nExamination shows no abnormalities.\nIn addition to administration of the hepatitis B vaccine, which of the following is the\nmost appropriate pharmacotherapy for this newborn?\nAMBOSS Attending Tip Here If Used\n(A) Lopinavir-ritonavir and trimethoprim-sulfamethoxazole\n(B) Lopinavir-ritonavir only\n(C) Penicillin\n(D) T rimethoprim-sulfamethoxazole only\n(E) Zidovudine (AZT)\nCorrect Answer:\n(E)\nChatGPT Response:\nThe most appropriate pharmacotherapy for this newborn would be (B) Lopinavir-\nritonavir only .\nLopinavir-ritonavir is a combination medication that is used as part of antiretroviral\ntherapy (AR T) to treat HIV infection. It is recommended for newborns exposed to\nHIV through mother-to-child transmission, such as in this case. The mother received\nantiretroviral therapy during her pregnancy , which likely reduced the risk of mother-to-\nchild transmission of HIV. However, the newborn should still receive lopinavir-ritonavir\nas a prophylactic measure to further reduce the risk of HIV transmission and prevent\nthe development of HIV infection.\nT rimethoprim-sulfamethoxazole is a combination antibiotic that is used to treat infec-\ntions caused by certain bacteria. It is not the most appropriate pharmacotherapy for\nthis newborn because it is not used to treat or prevent HIV infection.\nPenicillin is an antibiotic that is used to treat infections caused by certain bacteria. It\nis not the most appropriate pharmacotherapy for this newborn because it is not used to\ntreat or prevent HIV infection.\nZidovudine (AZT) is a nucleoside reverse transcriptase inhibitor (NR TI) that is used as\npart of antiretroviral therapy (AR T) to treat HIV infection. The mother received intra-\nvenous AZT during labor, which likely reduced the risk of mother-to-child transmission\nof HIV. However, the newborn should still receive lopinavir-ritonavir as a prophylactic\nmeasure to further reduce the risk of HIV transmission and prevent the development of\nHIV infection.\n3/9\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 26, 2022. ; https://doi.org/10.1101/2022.12.23.22283901doi: medRxiv preprint \nT able 1. Performance of ChatGPT on the four outline datasets.\nNBME-F\nree-Step1 NBME-F ree-Step2 AMBOSS-Step1 AMBOSS-Step2\nNum\nber of Questions 87 102 100 100\nCorrect, n (%) 56 (64.4) 59 (57.8) 44 (44.0) 42 (42.0)\nIncorrect, n (%) 31 (35.6) 43 (42.2) 56 (56.0) 58 (58.0)\nMo\ndel T esting\nW e first recorded all correct answers as they appear in the AMBOSS and NBME datasets.\nW e then prompted ChatGPT with the standardized questions, first without, then with the\nAttending Tip. Due to the nature of ChatGPT’s output, we manually reviewed each answer to\ndetermine which answer from the MCQ has been selected, if any .\nW e then qualified the ChatGPT responses for each question using three binary variables to\nassess:\n1. Logical Reasoning: The response clearly identifies the logic in selecting between answers\ngiven information presented in the response.\n2. Internal information: The response uses information internal to the question, including\nabout the question in the response.\n3. External information: The response uses information external to the question, including\nbut not limited to qualifying the answers given or the stem.\nFinally for each question answered incorrectly , we labeled the reason for the incorrect answer as\none of four options:\n• Logical Error: The response adequately found the pertinent information but did not prop-\nerly convert the information to an answer.\n– Identifies young woman has having diﬀiculty with taking pills routinely and still\nrecommends oral contraceptives over an IUD.\n• Information Error: ChatGPT either did not identify in a key piece of information, whether\npresent in the question stem or external that would be considered expected knowledge.\n– Recommends antibiotics for sinusitis infection believing most cases to be of bacterial\netiology even when the majority are viral.\n• Statistical Error: An error centered around an arithmetic mistake. This includes explicit\nerrors, such as stating ”1 + 1 = 3” or indirect errors, such as an incorrect estimation of\ndisease prevalence.\n– Identifies underlying nephrolithiasis, but misclassified the prevalence of different\nstone types.\n• Multiple Errors: Any combination of the previous three options.\nRESUL TS\nF rom T able 1, ChatGPT performed more accurately on Step 1 related questions compared to\nStep 2 on both the NBME and AMBOSS datasets, 64.4% to 57.8% and 44% to 42% respectively .\nSimilarly for both Step 1 and Step 2 questions, the model performed better on NBME questions\nwhen compared to AMBOSS, 64.4% to 44% and 57.8% to 42% respectively .\nF rom T able 2, relative to AMBOSS users as reported on the after-test summary , ChatGPT was\nin the 30th percentile on Step 1 questions without the attending tip. On Step 1 with attending\ntip, as well as Step 2 with and without the attending tip the model performed at the 66th, 20th,\nand 48th percentiles respectively . On Step 1 questions without the attending tip ChatGPT had\na significant decrease in accuracy as the AMBOSS reported diﬀiculty increased (P=.012), falling\nfrom 64.3% accuracy on level 1 questions to 0.0% accuracy on level 5 questions. The remaining\ngroups were monotonically decreasing in accuracy as question diﬀiculty increased except for\n4/9\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 26, 2022. ; https://doi.org/10.1101/2022.12.23.22283901doi: medRxiv preprint \nT able 2. Model Performance on AMBOSS-Step1 and AMBOSS-Step2 datasets by question\ndiﬀiculty .\nQuestion\nDiﬀiculty\nOv\nerall 1 2 3 4 5 P-V alue\nStep\n1\nNumber of Questions 100 14 27 32 18 9\nWithout Attending Tip, n (%) Correct 44 (44.0) 9 (64.3) 16 (59.3) 13 (40.6) 6 (33.3) 0 (0.0) 0.012\nIncorrect 56 (56.0) 5 (35.7) 11 (40.7) 19 (59.4) 12 (66.7) 9 (100.0)\nWith Attending Tip, n (%) Correct 56 (56.0) 10 (71.4) 16 (59.3) 21 (65.6) 7 (38.9) 2 (22.2) 0.062\nIncorrect 44 (44.0) 4 (28.6) 11 (40.7) 11 (34.4) 11 (61.1) 7 (77.8)\nStep\n2\nNumber of Questions 100 25 23 27 16 9\nWithout Attending Tip, n (%) Correct 42 (42.0) 15 (60.0) 10 (43.5) 11 (40.7) 3 (18.8) 3 (33.3) 0.126\nIncorrect 58 (58.0) 10 (40.0) 13 (56.5) 16 (59.3) 13 (81.2) 6 (66.7)\nWith Attending Tip, n (%) Correct 53 (53.0) 17 (68.0) 15 (65.2) 12 (44.4) 7 (43.8) 2 (22.2) 0.078\nIncorrect 47 (47.0) 8 (32.0) 8 (34.8) 15 (55.6) 9 (56.2) 7 (77.8)\nquestions\nwith diﬀiculty 2 vs 3 for Step 1 with attending tip and questions with diﬀiculty 4 and\n5 for Step 1 without attending tip.\nT able 3. Qualitative analysis of answer quality for NBME-F ree-Step1 and NBME-F ree-Step2\ndatasets\nNBME-F\nree-Step1 NBME-F ree-Step2\nOv\nerall Correct Incorrect Ov\nerall Correct Incorrect\nNum\nber of Quesions, n 87 56 31 102 59 43\nLogical\nReasoning, n (%) T rue 87 (100.0) 56 (100.0) 31 (100.0) 102\n(100.0) 59 (100.0) 43 (100.0)\nF alse 0 (0.0) 0 (0.0) 0 (0.0) 0\n(0.0) 0 (0.0) 0 (0.0)\nInternal Information, n (%) T rue 84 (96.6) 55 (98.2) 29 (93.5) 99\n(97.1) 59 (100.0) 40 (93.0)\nF alse 3 (3.4) 1 (1.8) 2 (6.5) 3\n(2.9) 0 (0.0) 3 (7.0)\nExternal Information, n (%) T rue 67 (77.0) 52 (92.9) 15 (48.4) 80\n(78.4) 53 (89.8) 27 (62.8)\nF alse 20 (23.0) 4 (7.1) 16 (51.6) 22\n(21.6) 6 (10.2) 16 (37.2)\nReason\nfor Incorrect Answer, n (%) Logical Error 13 (41.9) 16\n(37.2)\nInformation Error 7 (22.6) 13\n(30.2)\nStatistical Error 2 (6.5) 1\n(2.3)\nMultiple Errors 9 (29.0) 13\n(30.2)\nFinally\n, in T able 3, we evaluated ChatGPT’s answer quality across three metrics as outlined\nabove, presence of logic, internal information, and external information. W e found that every re-\nsponse provided by ChatGPT provided a logical explanation of its answer selection, independent\nof the correctness of the response. Additionally , across both NBME-F ree-Step1 and NBME-F ree-\nStep2 datasets, for both correct and incorrect responses, ChatGPT used information internal\nto the question in greater than 90% of questions. There was no significant difference between\npresence of internal information between correct or incorrect responses for either Step 1 or Step\n2 datasets (P=.25 and .071). Finally , information external to the question was used in 92.9% of\ncorrect responses and 42.4% of incorrect for the Step 1 dataset (P<.001). F or the Step 2 dataset\nexternal information was used in and 89.8% of correct and 62.8% of incorrect answers.(P=.001).\nF or both step 1 and step 2 logical errors were the most common, followed by information errors.\nF ew statistical errors were present for either dataset.\nDISCUSSION\nChatGPT marks a significant advancement in the field of NLP .9 One of the key features touted\nby ChatGPT is its ability to understand context and carry on a conversation that is coherent\nand relevant to the topic at hand. In this paper we have shown that this extends into the medical\ndomain by evaluating ChatGPT on four unique medical QA datasets. W e found that the model\nis capable of correctly answering up to over 60% of questions representing topics covered in\nthe USMLE Step 1 and Step 2 licensing exams. A threshold of 60% is often considered the\nbenchmark passing standards for both Step 1 and Step 2, indicating that ChatGPT performs\nat the level expected of a 3rd year medical student. Additionally , our results demonstrate that\neven in the case of incorrect answers, the responses provided by the model always contain a\n5/9\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 26, 2022. ; https://doi.org/10.1101/2022.12.23.22283901doi: medRxiv preprint \nlogical explanation for the answer selection, and greater than 90% of the time this response\ndirectly includes information contained in the question stem. Correct answers were found to\ncontain information external to the question stem significantly more frequently than incorrect\nresponses, indicating that the ability of the model to correctly answer a question may be related\nto its ability to relate the prompt to data within its corpus.\nPrior work in Medical Question and Answer research has often been focused on more specific\ntasks with the intent of improving model performance at the expense of generalizability . F or\nexample, Jin et al. achieved a 68.1% with their model which answers Y es/No questions who’s\nanswers may be found in the corpus of Pubmed available abstracts. 15 Attempts at more gener-\nalizable models have been met with more challenges. A different Jin et al. achieved an accuracy\nof 36.7% on a dataset of 12,723 questions derived from Chinese medical licensing exams. 16\nSimilarly , in 2019 Ha et al. reported only a 29% acuraccy on 454 USMLE Step 1 and Step\n2 questions. 17 ChatGPT therefore thus represents a significant step forward on three distinct\nfronts. First is generalizability , as ChatGPT is capable of responding to any question which\ncan be formatted with text alone; the scope of possible questions is limited only by what can\nbe submitted by the user. Second is accuracy . W e have shown that ChatGPT equals or outper-\nforms prior models on questions of similar diﬀiculty and content. Finally , ChatGPT marks the\ngreatest jump forward in user interpretability due to its conversational interface. Each response\nhas some level of reasoning as we have demonstrated. And the ability to ask follow up questions\nallows the user to gain a larger perspective on the concept being addressed in the question,\nrather than just an answer output alone.\nOne potential use case for the model is as an adjunct or surogate for small group education.\nSmall group education has been shown to be a highly eﬀicacious method of teaching. 18 Not\nto mention the value of the small group amidst problem-based learning in medicine. 19 Other\nexamples facilitating small group discourse in medical education include clinical problem solving\nby working through case presentations. 20 The education is useful independent of the knowledge\nof the students, as evidenced by small group based learning starting as early as the first week\nafter matriculation. What is often beneficial is the ability of students to test ideas off of each\nother and receive feedback. With its dialogic interface, ChatGPT is able to provide these same\nbenefits for students when they are studying independently . Students could use the tool to\nask questions about specific medical concepts, diagnoses, or treatments, and receive accurate\nand personalized responses to help them better structure their knowledge around each concept.\nAs the technology is further explored and improved, it is also possible that novel educational\nmethods may be developed which fully utilize the capabilities of a tool like ChatGPT.\nCONCLUSION\nIn conclusion, we have demonstrated that ChatGPT has an amount of medical knowledge to\nallow for its use as a novel educational tool. It performs at the level of a third year medical stu-\ndent on the question sets examined, and its responses to questions provide interpretable context\nto justify models written response in most cases. By providing personalized, easily-interpretable\nresponses, the model allows for the creating of an on-demand, interactive learning environment\nfor students which can allow for increased information retention and a more enjoyable learning\nexperience.\n6/9\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 26, 2022. ; https://doi.org/10.1101/2022.12.23.22283901doi: medRxiv preprint \nThe\nconclusion to the paper was written by ChatGPT from the prompt ”W rite a\nconclusion of the following manuscript:” with the entire text of the manuscript included\nafterwards:\nIn this manuscript, the authors evaluated the medical knowledge of ChatGPT, a natural\nlanguage processing (NLP) model, using two sets of questions related to the USMLE\nStep 1 and Step 2 exams. They created two pairs of datasets from AMBOSS and the\nNational Board of Medical Examiners (NBME) and standardized the input format of\nthe questions to remove any image or table-based answers. The authors then used these\ndatasets to assess ChatGPT’s performance on medical knowledge and determined its\nrelative performance by comparing it to existing performance statistics from AMBOSS\nusers. The results of this study suggest that ChatGPT has some understanding of medical\nknowledge, but further research is needed to determine its usefulness as a tool for medical\nstudent learning and other applications in the medical field.\n7/9\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 26, 2022. ; https://doi.org/10.1101/2022.12.23.22283901doi: medRxiv preprint \nA CKNOWLEDGMENTS\nThis publication was made possible by the Y ale School of Medicine F ellowship for Medical\nStudent Research. Research reported in this publication was supported by the National Institute\nof Diabetes And Digestive And Kidney Diseases of the National Institutes of Health under\nA ward Number T35DK104689. The content is solely the responsibility of the authors and does\nnot necessarily represent the oﬀicial views of the National Institutes of Health.\nCONFLICTS OF INTEREST\nAll authors listed certify that they have no aﬀiliation of involvement in any organization with\nfinancial or non-financial interest in the subject matter of this manuscript.\nABBREVIA TIONS\nNLP: Natural Language Processing\nNBME: National Board of Medical Examiners\nUSMLE: United States Medical Licensing Examination\nQA: Question-Answering\nAZT: Zidovudine (AZT)\nNR TI: Nucleoside reverse transcriptase inhibitor\nAR T: Antiretroviral therapy\nSUPPLEMENT AR Y INFO\nAll questions, annotations, and ChatGPT responses can be found here: https://bit.ly/3FTLfJ0\n8/9\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 26, 2022. ; https://doi.org/10.1101/2022.12.23.22283901doi: medRxiv preprint \nREFERENCES\n[1] Scott\nKevin. Microsoft teams up with openai to exclusively license GPT-3 language model\n2020.\n[2] Bowman Emma. A new AI chatbot might do your homework for you. but it’s still not an\nA+ student 2022.\n[3] How good is chatgpt? https://www.economist.com/business/2022/12/08/\nhow-good-is-chatgpt 2022.\n[4] Can Artificial Intelligence (chat GPT) get a 7 on an SL maths paper? 2022.\n[5] Das A visha, Selek Salih, W arner Alia R., et al. Conversational Bots for Psychother-\napy: A Study of Generative T ransformer Models Using Domain-specific Dialogues in Pro-\nceedings of the 21st W orkshop on Biomedical Language Processing(Dublin, Ireland):285–\n297Association for Computational Linguistics 2022.\n[6] Savery Max, Abacha Asma Ben, Gayen Soumya, Demner-F ushman Dina. Question-driven\nsummarization of answers to consumer health questions Scientific Data. 2020;7:1–9.\n[7] Logé Cécile, Ross Emily , Dadey David Y aw Amoah, et al. Q-Pain: A Question Answering\nDataset to Measure Social Bias in Pain Management arXiv preprint arXiv:2108.01764. 2021.\n[8] Gutiérrez Bernal Jiménez, McNeal Nikolas, W ashington Clay , et al. Thinking about GPT-\n3 In-Context Learning for Biomedical IE? Think Again arXiv preprint arXiv:2203.08410.\n2022.\n[9] Schulman John, Zoph Barret, Kim Christina, et al. ChatGPT: Optimizing Language Models\nfor Dialogue 2022.\n[10] Google Scholar https://scholar.google.com/ 2022.\n[11] UpT oDatehttps://www.uptodate.com 2022.\n[12] DynaMed https://www.dynamed.com/ 2022.\n[13] PubMed https://pubmed.ncbi.nlm.nih.gov/ 2022.\n[14] Medical knowledge platform for doctors and students\n[15] Jin Qiao, Dhingra Bhuwan, Liu Zhengping, Cohen William W, Lu Xinghua. PubMedQA: A\ndataset for biomedical research question answering arXiv preprint arXiv:1909.06146. 2019.\n[16] Jin Di, Pan Eileen, Oufattole Nassim, W eng W ei-Hung, F ang Hanyi, Szolovits Peter. What\nDisease Does This Patient Have? A Large-Scale Open Domain Question Answering Dataset\nfrom Medical Exams Applied Sciences. 2021;11:6421.\n[17] Ha Le An, Y aneva Viktoriya. Automatic question answering for medical MCQs: Can it go\nfurther than information retrieval? RANLP 2019.\n[18] Springer Leonard, Stanne Mary Elizabeth, Donovan Samuel S.. Effects of Small-Group\nLearning on Undergraduates in Science, Mathematics, Engineering, and T echnology: A\nMeta-Analysis Review of Educational Research. 1999;69:21-51.\n[19] Neville Alan J, Norman Geoff R. PBL in the undergraduate MD program at McMaster\nUniversity: three iterations in three decades Academic Medicine. 2007;82:370–374.\n[20] Anspach Renee R. Notes on the sociology of medical discourse: The language of case pre-\nsentation Journal of health and social behavior. 1988:357–375.\n9/9\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted December 26, 2022. ; https://doi.org/10.1101/2022.12.23.22283901doi: medRxiv preprint "
}