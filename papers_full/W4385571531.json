{
  "title": "How to Plant Trees in Language Models: Data and Architectural Effects on the Emergence of Syntactic Inductive Biases",
  "url": "https://openalex.org/W4385571531",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2108518500",
      "name": "Aaron MUELLER",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A817205692",
      "name": "Tal Linzen",
      "affiliations": [
        "New York University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4285256079",
    "https://openalex.org/W3200809495",
    "https://openalex.org/W3042795397",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W4308168008",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2120159535",
    "https://openalex.org/W3098613713",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2166735162",
    "https://openalex.org/W3166920165",
    "https://openalex.org/W2963351454",
    "https://openalex.org/W36434594",
    "https://openalex.org/W2996728628",
    "https://openalex.org/W2913340405",
    "https://openalex.org/W3037115370",
    "https://openalex.org/W2151834591",
    "https://openalex.org/W4301594491",
    "https://openalex.org/W3213014097",
    "https://openalex.org/W4285174271",
    "https://openalex.org/W3191350816",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2296073425",
    "https://openalex.org/W3034510440",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2137983211",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3035267217",
    "https://openalex.org/W2154600605",
    "https://openalex.org/W3035064549",
    "https://openalex.org/W2888922637",
    "https://openalex.org/W3034341146",
    "https://openalex.org/W3014415613",
    "https://openalex.org/W4288631803",
    "https://openalex.org/W2962961857",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W3202908475",
    "https://openalex.org/W3199241049",
    "https://openalex.org/W3124034626",
    "https://openalex.org/W3159684727",
    "https://openalex.org/W2788924045",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2889947987",
    "https://openalex.org/W2971016963"
  ],
  "abstract": "Accurate syntactic representations are essential for robust generalization in natural language. Recent work has found that pre-training can teach language models to rely on hierarchical syntactic features—as opposed to incorrect linear features—when performing tasks after fine-tuning. We test what aspects of pre-training are important for endowing encoder-decoder Transformers with an inductive bias that favors hierarchical syntactic generalizations. We focus on architectural features (depth, width, and number of parameters), as well as the genre and size of the pre-training corpus, diagnosing inductive biases using two syntactic transformation tasks: question formation and passivization, both in English. We find that the number of parameters alone does not explain hierarchical generalization: model depth plays greater role than model width. We also find that pre-training on simpler language, such as child-directed speech, induces a hierarchical bias using an order-of-magnitude less data than pre-training on more typical datasets based on web text or Wikipedia; this suggests that in cognitively plausible language acquisition settings, neural language models may be more data-efficient than previously thought.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 11237–11252\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nHow to Plant Trees\n in LMs: Data and Architectural\nEffects on the Emergence of Syntactic Inductive Biases\nAaron Mueller\nJohns Hopkins University\namueller@jhu.edu\nTal Linzen\nNew York University\nlinzen@nyu.edu\nAbstract\nAccurate syntactic representations are essential\nfor robust generalization in natural language.\nRecent work has found that pre-training can\nteach language models to rely on hierarchical\nsyntactic features—as opposed to incorrect lin-\near features—when performing tasks after fine-\ntuning. We test what aspects of pre-training\nare important for endowing encoder-decoder\nTransformers with an inductive bias that favors\nhierarchical syntactic generalizations. We fo-\ncus on architectural features (depth, width, and\nnumber of parameters), as well as the genre\nand size of the pre-training corpus, diagnos-\ning inductive biases using two syntactic trans-\nformation tasks: question formation and pas-\nsivization, both in English. We find that the\nnumber of parameters alone does not explain\nhierarchical generalization: model depth plays\ngreater role than model width. We also find\nthat pre-training on simpler language, such as\nchild-directed speech, induces a hierarchical\nbias using an order-of-magnitude less data than\npre-training on more typical datasets based on\nweb text or Wikipedia; this suggests that in\ncognitively plausible language acquisition set-\ntings, neural language models may be more\ndata-efficient than previously thought.\n1 Introduction\nAccurate syntactic representations are necessary\nfor robust generalization to new natural language\ninputs and for the generation of correct outputs.\nConsider the problem of identifying the subject of\n“said” in the following sentence:\n(1) Can you repeat what the senator next to the\ncats said?\nTypical language models (LMs), which receive lin-\near sequences of words as input, could conceivably\nrely on a linear or positional feature that usually,\nbut does not always, identifies the correct subject\nof a verb. An LM could learn, for example, that\nthe first noun in the sentence is always the subject.\nThis heuristic works for many simple sentences,\nbut fails in Ex. (1): here, the first noun is “ you”,\nand so this heuristic would lead the LM to incor-\nrectly interpret the sentence as meaning “Can you\nrepeat what you said?” The LM could also learn\nthat the subject of the verb is the noun closest to the\nverb in the linear order of the sentence, in which\ncase it would interpret Ex. (1) as “Can you repeat\nwhat the cats said?” By contrast, an LM that repre-\nsents the sentence as hierarchically structured will\ncorrectly identify senator as the subject of the em-\nbedded clause that contains the verb said. This\nexample demonstrates that a preference for syn-\ntactic features over linear features is required for\nrobust linguistic generalization.\nThe success of large-scale pre-training across\nfine-tuning tasks suggests that exposure to natural\nlanguage may teach models to rely on appropri-\nate syntactic features instead of heuristics (even\nthough models still often rely on heuristics; Mc-\nCoy et al. 2019). This hypothesis is supported by\nthe finding that, given minimal pairs of grammati-\ncal and ungrammatical sentences, the probability\ndistribution over sentences defined by LMs often fa-\nvors the grammatical sentence (Marvin and Linzen,\n2018; Hu et al., 2020). A related line of work has\nshown that, through pre-training, LMs can under\nsome circumstances acquire syntactic inductive bi-\nases which are then applied to fine-tuning tasks,\nwhereas models which have not been pre-trained\ndo not have such inductive biases (Warstadt and\nBowman 2020; Warstadt et al. 2020b; Lovering\net al. 2021; Mueller et al. 2022).\nWhen does pre-training endow LMs with a syn-\ntactic inductive bias? In this study, we address\ntwo specific sub-questions: (1) Which architec-\ntural features make a syntactic inductive bias more\nlikely to emerge in a Transformer LM? (2) How is\nthe inductive bias affected by the genre and size\nof the pre-training corpus? We investigate these\n11237\nquestions by evaluating a range of Transformer\nencoder-decoder models based on T5 (Raffel et al.,\n2020). We evaluate both existing publicly available\nmodels and models that we pre-train ourselves; we\nexplore a variety of model widths (embedding and\nhidden dimension, feed-forward layer size) and\ndepths (number of layers), and pre-train on cor-\npora of varying genres and sizes. We then evaluate\nmodels’ inductive biases by observing their out-\nof-distribution generalization when fine-tuned on\nsyntactic transformations tasks (§4). We find that\ndepth matters more than width for the acqui-\nsition of hierarchical biases (§5), and that pre-\ntraining on simpler language induces hierarchi-\ncal biases using far less data (§6 and §7). This\nlast finding suggests that in language acquisition\nsettings in which the training corpus more closely\nresembles the language that children are exposed to,\nTransformers may be more sample-efficient than\npreviously thought.\nOur code is available on GitHub.1\n2 Background and Motivation\nEvery finite training set is consistent with multiple\ngeneralizations. We use the term inductive bias to\nrefer to the set of assumptions that a model relies on\nwhen generalizing to new data. Our usage includes\nany factor that leads the model to generalize in one\nway rather than another (Mitchell, 1980); this can\ninclude not only the model’s architecture, but also\nrepresentations learned from prior or concurrent\ntraining on tasks that are related to the target task\n(Caruana, 1997), and in particular self-supervised\npre-training (Lovering et al., 2021).\nWe can infer a model’s inductive bias by observ-\ning how it generalizes out of distribution after train-\ning on a dataset that is compatible with multiple\ngeneralizations. Applying this methodology, Mc-\nCoy et al. (2018), McCoy et al. (2020), and Petty\nand Frank (2021) find that LSTM and Transformer\nencoder-decoder models trained from scratch (with-\nout pre-training) on syntactic transformations, such\nas converting a declarative sentence into a question\n(§3), do not generalize in a hierarchical manner.\nBy contrast, Mueller et al. (2022) find that certain\npre-trained encoder-decoder models—including\nT5 and BART (Lewis et al., 2020)— do general-\nize hierarchically after fine-tuning. Warstadt and\nBowman (2020) and Warstadt et al. (2020b) re-\n1https://github.com/aaronmueller/\nemergent-syntax\nport similar results for the pre-trained masked LM\nRoBERTa (Liu et al., 2019), though in their study a\nrobust syntactic inductive bias only emerged when\nthe training corpus was much larger than a human\nmight be exposed to.\nPrevious work on the effect of training corpus\nsize and genre on syntactic generalization includes\nHuebner et al. (2021), who find that masked LMs\nshow stronger syntactic abilities after training on a\nfew million words of child-directed speech than a\nsimilar amount of Wikipedia or news text; they do\nnot, however, explore whether similar abilities arise\nfrom training on a larger amount of Wikipedia text.\nVan Schijndel et al. (2019) report experimental re-\nsults suggesting that scaling the training corpus or\nmodel size is unlikely to result in human-like syn-\ntactic abilities for LSTM LMs, but they only vary\nmodel width and only train on Wikipedia data. We\nfill the gap between these studies by investigating\nthe influence of multiple component of the Trans-\nformer architecture and by training on corpora of\nvarying genres and sizes.\nOur work is related more broadly to the syntac-\ntic LM evaluation literature. In this style of work,\nevaluation is typically performed using minimal\npairs, where a grammatical and ungrammatical sen-\ntence or completion are provided to a model, and\nthe model is expected to assign a higher probability\nto the grammatical variant. Syntactic evaluations\nhave found that LSTM- (Hochreiter and Schmidhu-\nber, 1997) and Transformer-based (Vaswani et al.,\n2017) LMs are sensitive to grammatical number\nand gender in subject-verb agreement and reflex-\nives (Hu et al., 2020; Marvin and Linzen, 2018;\nGoldberg, 2019; Lakretz et al., 2021; Gauthier\net al., 2020). LMs are also sensitive to filler-gap\ndependencies (Wilcox et al., 2018) and, to a lesser,\nextent, negative polarity items (Marvin and Linzen,\n2018; Warstadt et al., 2020a). This holds across\nlanguages (Mueller et al., 2020; Ravfogel et al.,\n2018) and across grammatical/typological features\n(Ravfogel et al., 2019).\nOverall, prior work has shown that pre-training\ncan impart hierarchical inductive biases to LMs.\nThe goal of this study is to examine which aspects\nof pre-training—specifically, architecture and train-\ning data—contribute to the emergence of this bias.\n3 Syntactic Transformations\nTo evaluate the linguistic inductive biases of our\nmodels, we employ the poverty of the stimulus\n11238\nFigure 1: The syntactic transformations paradigm. A pre-trained model is fine-tuned on examples that are consistent\nwith both syntactic (hierarchical) and positional/word order (linear) explanations. Then, it is evaluated on examples\nwhere only a model with a syntactic inductive bias will produce the correct output. We investigate which components\nof pre-training induce hierarchical inductive biases. Adapted from Warstadt et al. (2020b) and Mueller et al. (2022).\ndesign (Wilson, 2006): we fine-tune a model on\nambiguous data and then evaluate it on out-of-\ndistribution examples where only the desired in-\nductive bias will result in the correct outputs. Here,\nwe use the syntactic transformations paradigm\n(Frank and Mathis, 2007) summarized in Figure 1,\nand observe whether models generalize accord-\ning to hierarchical linguistic rules or according to\nsurface heuristics based on word position or rela-\ntive word ordering. We evaluate on English ques-\ntion formation and passivization, using the English\ndatasets of Mueller et al. (2022) (themselves based\non McCoy et al. 2020).\n3.1 Question Formation\nHere, the task is to transform a declarative sentence\ninto a polar yes/no question by moving the auxiliary\nverb to the start of the sentence. The competing hy-\npotheses are MOVE -FIRST and MOVE -MAIN (see\nFigure 1 for examples). We train the models on\nsentences that are consistent with both hypotheses,\nwhere the main auxiliary is always the linearly first\nauxiliary in the input sentence. Then, in the gen-\neralization examples, we append a relative clause\n(RC) to the subject, such that the main auxiliary\nis now the linearly second auxiliary in the input.\nA model that acquired MOVE -MAIN —that is, one\nthat has a hierarchical inductive bias—will cor-\nrectly identify the main auxiliary verb and move it\nto the front, meaning that it should still produce the\ncorrect output. A model that learned MOVE -FIRST\nwill move the first auxiliary to the front, resulting\nin ungrammatical outputs (Figure 1).\n3.2 Passivization\nIn this task, the goal is to transform an active sen-\ntence into a passive one. This requires various\ninsertions, deletions, reinflections, and movements,\nmaking this task a potentially more difficult one\nthan question formation. Here, we evaluate the\nmovement of the object to the subject position. The\ncompeting hypotheses here are MOVE -SECOND\nand MOVE -MAIN . We train the models on sen-\ntences where the object is always the linearly sec-\nond noun in the sentence. Then, in the generaliza-\ntion examples, we append a prepositional phrase\n(PP) to the subject, such that the object is now the\nlinearly third noun. If a model acquires the gen-\neralization MOVE -MAIN (consistent with a hierar-\nchical inductive bias), it will detect the object and\nmove it to the front, producing the correct output.\nIf it acquires MOVE -SECOND , it will move the lin-\nearly second noun phrase even in the generalization\nexamples (where, again, the correct noun to move\nis actually the linearly third one), and as such will\noutput ungrammatical sequences. For example:\n(2) Passivization\na. Training: The raven observed the newts\n(near the yak). ⇒The newts (near the yak)\nwere observed by the raven.\nb. Generalization: The salamander behind the\nravens applauded the peacock. ⇒?\nc. MOVE -MAIN (correct): The peacock was\napplauded by the salamander behind the\nravens.\nd. MOVE -SECOND (incorrect): The ravens\nwere applauded by the salamander.\n3.3 Evaluation Metrics\nFor both syntactic transformations, we evaluate\nmodels’ outputs using two metrics. The first is\nsequence accuracy, which measures the percent-\nage of inputs for which the model’s full output se-\n11239\nquence is exactly correct. This is a strict metric that\ndoes not capture solely the syntactic phenomenon\nwe investigate, but also penalizes the model for\nother errors, such as word substitution errors. We\nalso report more targeted metrics for each of the\ntasks: main auxiliary accuracy for question for-\nmation, which measures how often the first word\nof the output sentence is the main auxiliary; and\nobject accuracy for passivization, which measures\nhow often the noun that gets moved to the start of\nthe sentence is the object.\n4 Overview of Experimental Paradigm\nAll of our experiments involve fine-tuning vari-\nants of T5, a Transformer encoder-decoder model\npre-trained using a span denoising objective: con-\ntiguous token sequences are masked in the input\nsequence and then reconstructed in the output se-\nquence. We either use the publicly available pre-\ntrained “efficient” T5 models released by Tay et al.\n(2022),2 or pre-train models ourselves using the\ntransformers library (Wolf et al., 2020).\nThe syntactic transformation datasets we fine-\ntune on are the English datasets of Mueller et al.\n(2022), which consist of 100,000 training exam-\nples; 10,000 in-distribution test examples, which\ntest whether the models have learned the task; and\n10,000 out-of-distribution generalization examples,\nwhich reveal models’ inductive biases.\nWe adopt Mueller et al.’s hyperparameters\n(App. A). We fine-tune for 10 epochs (approxi-\nmately 7500 training steps), and every 500 steps we\nsave a checkpoint and evaluate it. Across models,\naccuracy on the in-distribution test set generally\nreaches 100% within 500 steps (the first check-\npoint) and remains 100% throughout fine-tuning.\nBecause in-distribution test set accuracy may not\ncorrelate with generalization accuracy, it is unclear\nwhich checkpoint would yield the best accuracy on\nthe generalization set; we therefore report the mean\ngeneralization accuracy across all checkpoints.\n5 Architectural Effects\nWhich architectural features contribute to hierar-\nchical generalization? Given that language is struc-\n2The term “efficient” here contrasts the models of Tay et al.\n(2022) with the original T5 models of Raffel et al. (2020),\nwhich are Pareto-inefficient with respect to downstream per-\nformance and number of parameters. The “efficient” models\nachieve similar performance across tasks using fewer param-\neters by using a deeper (more layers) and narrower (smaller\nhidden size/feed-forward size) architecture.\nTiny Mini Small Base\n# Parameters 16M 31M 60M 220M\n# Layers (NL) 4 4 6 12\nFeedforward layer dimension (FF) 1024 1536 2048 3072\nEmbedding and hidden dimension (DM) 256 384 512 768\nKey/value projection matrix dim. (KV) 32 32 32 64\n# Heads per layer (NH) 4 8 8 12\nTable 1: Architectural details for T5 variants from Tay\net al. (2022). # Layers is the number of layers in each\nof the encoder and the decoder (multiply this number\nby 2 to obtain the total number of layers in the model).\nFigure 2: The Transformer encoder-decoder architec-\nture, annotated with the architectural hyperparameters\nwe vary.\ntured hierarchically, we hypothesize that model\ndepth (number of layers) will be the most impor-\ntant component, as deeper structure could more\neasily allow for representations of deeper hierar-\nchical structures (e.g., more complex syntax trees),\nwith recursive syntactic operations applied succes-\nsively across layers (Murty et al., 2023).\n5.1 Models\nWe fine-tune pre-trained models from Tay\net al. (2022), available on HuggingFace. We\ntrain two sets of models. The first set\nis google/t5-efficient-{tiny,mini,small,,\nbase}; see Table 1 for the hyperparameters of these\nmodels, and Figure 2 for a diagram of the Trans-\nformer architecture that illustrates these hyperpa-\nrameters. Note that multiple hyperparameter values\nchange at the same time when moving from, e.g.,\nT5small to T5base.\nThe second set of models we use from Tay et al.\n(2022) were derived from T5base by changing ex-\nactly one hyperparameter value. For these more\ncontrolled variants, we adopt Tay et al.’s nomen-\nclature, which is based on the particular hyperpa-\nrameter that is being changed, and its new value;\n11240\nfor example, T5 base-DM512 (which we abbrevi-\nate here to DM512) is identical to T5 base, except\nthe embedding/hidden dimension (DM) is reduced\nfrom 768 to 512. All of these models are trained\non approximately 34B words from the Colossal\nCleaned Common Crawl (C4) web text corpus.\n5.2 Depth, not Scale, Predicts Syntactic Bias\nWe start by asking whether scale alone can explain\nhierarchical generalization: Is there a monotonic\nrelationship between the number of parameters and\nin generalization accuracy? We find that the an-\nswer is no (Figure 3). For question formation, the\nSpearman rank-order correlation between the num-\nber of parameters and accuracy is 0.51 (sequence)\nand 0.58 (main auxiliary); for passivization, 0.75\n(sequence) and 0.43 (object). While these are sig-\nnificant correlations ( p < .05, except for object\naccuracy), if syntactic bias were predicted by scale\nalone, we would expect these to be close to 1. Thus,\nnumber of parameters alone is not sufficient to\nexplain the acquisition of a hierarchical bias .\nThis suggests that certain architectural components,\nwhich may be correlated with scale, are more im-\nportant than others.\nIndeed, we find that increasing model depth has\na much stronger impact on accuracy than scaling\nthe model up by increasing the value of other ar-\nchitectural hyperparameters (Figure 4): in a least\nsquares linear regression where the dependent vari-\nable is sequence accuracy and independent variable\nis number of parameters (normalized to the same\nrange as the accuracy values), the slope of the fitted\nline is 0.70 when varying over number of layers,\nbut only 0.13 for embedding/hidden size, and 0.25\nfor feed-forward layer width. In particular, the\nwide and shallow NL4 has more parameters than\nthe narrow and deep DM256, but achieves simi-\nlar performance as DM256 on question formation\nand significantly worse performance on passiviza-\ntion (as a reminder, NL4 is T5base with 4 encoder\nlayers and 4 decoder layers, and DM256 is T5base\nwith embedding/hidden size 256). This suggests\nthat when scaling the architecture, model depth\nis more important than other components for\nenabling hierarchical generalization.\nIs encoder depth or decoder depth more impor-\ntant for hierarchical generalization, or is total depth\nalone responsible for the patterns we find? We in-\nvestigate this in App. B, with mixed results: for\npassivization, reducing the depth of either compo-\n0.5 1.0 1.5 2.0\nParameters 1e8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nEnglish Question Formation\nMetric\nsequence\nmain aux\n0.5 1.0 1.5 2.0\nParameters 1e8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nEnglish Passivization\nMetric\nsequence\nobject noun\nFigure 3: Generalization accuracies on question\nformation (top) and passivization (bottom) using\nT5{tiny,mini,small,base}, as well as variants of T5base where\nwe vary the number of layers, number of attention heads\nper layer, embedding/hidden dimension, feed-forward\nwidth, or key-value projection dimension. There is a\npositive correlation between the number of parameters\nand accuracy, but the trend is not monotonic. Certain\narchitectural features may therefore play a more impor-\ntant role than others.\nnent leads to similar drops in generalization accu-\nracy, but for question formation, decoder depth has\na greater effect than encoder depth.\n5.3 Syntactic Bias Correlates with\nDownstream Performance\nHow well does syntactic generalization accuracy\ncorrelate with performance on other tasks? We\naddress this question by correlating main auxil-\niary accuracy with validation perplexity, question\nanswering accuracy on SQuAD (Rajpurkar et al.,\n2016), and scores on the SuperGLUE collection of\nnatural language understanding tasks (Wang et al.,\n2019), all provided by Tay et al. (2022). We do not\nreport correlations with passivization accuracy, as\nmost models achieve 100% accuracy on this task,\nwhich leaves little explainable variance.\nWe obtain Spearman correlations of 0.57 (p <.1)\nfor SuperGLUE, 0.34 (p >.1) for SQuAD, and 0.67\n11241\n0.5 1.0 1.5 2.0\nParameters 1e8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nDM256\nDM512\nFF1024 FF2048\nNL2\nNL4 NL8\nT5-base\nEnglish Question Formation\nComponent\nDM\nFF\nNL\n0.5 1.0 1.5 2.0\nParameters 1e8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nDM256\nDM512\nFF1024\nFF2048\nNL2\nNL4\nNL8\nT5-base\nEnglish Passivization\nComponent\nDM\nFF\nNL\nFigure 4: Generalization sequence accuracies on ques-\ntion formation (top) and passivization (bottom) using\narchitecturally modified versions of T5base. Decreasing\nmodel depth (NL) results in the greatest drop in perfor-\nmance, suggesting that model depth is more important\nfor learning syntax than other components. See App. C\nfor results when varying the number of attention heads\nor key/value projection dimension.\n(p <.05) for negative validation perplexity. In other\nwords, the correlation is weak but significant with\naverage SuperGLUE accuracy (Tay et al. do not\nreport accuracy for individual SuperGLUE tasks);\nnot significant with question answering; and rela-\ntively strong and significant with language model-\ning performance more broadly. We note that since\nthe number of models is relatively modest, correla-\ntions need to be quite strong to reach the statistical\nsignificance threshold.\nThese correlations do not indicate that syntac-\ntic abilities are causally implicated in the models’\nimproved performance on other tasks, but they do\nshow that the emergence of syntactic abilities\noften co-occurs with better language modeling\nperformance and downstream performance. Fu-\nture work could employ causal analysis methods to\nbetter understand how the emergence of syntactic\npreferences affects (or does not affect) performance\nacross NLP tasks.\n6 Corpus Genre\nLarge LMs are typically pre-trained on web text\nand/or Wikipedia data—genres that are distinct\nfrom the type of language that humans are exposed\nto during childhood. Could the domain of pre-\ntraining corpora explain why LMs require much\nmore data than humans to reach similar syntac-\ntic abilities (Warstadt et al., 2020b)? Huebner\net al. (2021) report experiments that support this\nhypothesis: they find that the RoBERTa masked\nLM achieves higher accuracies on linguistic accept-\nability judgment benchmarks when it is pre-trained\non child-directed speech as opposed to a similar\namount of Wikipedia data. In this section, we inves-\ntigate whether this applies to our paradigm by pre-\ntraining encoder-decoder models on child-directed\nspeech and a similar amount of text drawn from the\nEnglish Wikipedia.\n6.1 Models\nWe train models based on the T5 architecture\nand objective (see §4) on the English portion\nof CHILDES (MacWhinney, 2000), a 5M-word\nchild-directed speech corpus, and on an English\nWikipedia corpus from Huebner et al. (2021),\nwhich consists of a similar number of sentences\nas CHILDES. As Wikipedia sentences are longer,\nthe total number of words in the Wikipedia training\nset we use here is approximately 10M.\nWe train models with eight hyperparameter con-\nfigurations on each dataset (Table 2): we either vary\nthe number of layers (NL ∈{2, 4, 8, 16}), keeping\nother hyperparameters, such as embedding/hidden\ndimension and number of heads, constant; or we\nkeep the number of layers at 8 and vary other hy-\nperparameters. While we only pre-train each con-\nfiguration once, we fine-tune each configuration\nfive times, with a different random seed each time.\nFollowing Huebner et al., we modify the train-\ning hyperparameters to better suit the smaller and\nsimpler child-directed speech corpus: we reduce\nthe maximum sequence length to 128 and train a\nSentencePiece tokenizer (Kudo and Richardson,\n2018) with a reduced vocabulary size of 213 =\n8192; this is motivated by children’s vocabulary\nsize of approximately 5,000–6,000 lemmas at age 6\n(Biemiller, 2003). For the Wikipedia corpus, we\ntrain SentencePiece tokenizers using vocab sizes\n∈ {8192, 32768 }and take the best-performing\n11242\nData\nModelParameters NL FF DM KV NH CHILDES Wikipedia\nTiny 23M 8 1024 256 32 4 0.62 (.06) 0.07 (.02)Mini 50M 8 1536 384 32 8 0.68 (.07) 0.35 (.08)Small 75M 8 2048 512 32 8 0.73(.04)0.46(.10)Base 157M 8 3072 768 64 12 0.61 (.07) 0.45 (.09)Large 268M 8 4096 1024 64 16 0.57 (.09) 0.26 (.09)\nSmall 31M 2 2048 512 32 8 0.49 (.04) 0.08 (.01)Small 46M 4 2048 512 32 8 0.58 (.05) 0.35 (.08)Small 75M 8 2048 512 32 8 0.73(.04) 0.46 (.10)Small 134M 16 2048 512 32 8 0.70 (.06) 0.48(.08)\nTable 2: Main auxiliary accuracies averaged across 5\nfine-tuning random seeds (standard deviation across\nseeds in parentheses) on the question formation gener-\nalization dataset for various encoder-decoder models\npre-trained from scratch on 5M words of transcribed\nchild-directed speech or 10M words of Wikipedia text.\nmodel for each hyperparameter configuration,3,4 as\nit is not clear a prioriwhether a smaller vocabulary\nwould be beneficial for Wikipedia’s more complex\nand diverse language. We use sequence packing,\nwhere we concatenate multiple sentences from the\ncorpus into a single example such that the total\nlength of each training example is approximately\nequal to the maximum sequence length.\nWhen pre-training on child-directed speech, we\ncheckpoint every 10K training steps and find that\nthe best performance on our syntactic transforma-\ntions tasks is achieved at 130K steps. We train on\nthe Wikipedia corpus for the same number of steps.\n6.2 Results\nWe find that pre-training on child-directed\nspeech generally results in a greater ability to de-\ntect the main verb, as compared to pre-training\non Wikipedia (Table 2). This holds across model\nsizes and across model depths. The CHILDES-\npre-trained 8-layer variant of T5small performs best.\nWhen fixing NL at 8 and varying other components\naccording to each model size’s default settings (as\nin Table 1), we find that T5small performs best. In\nthe following experiment, we therefore focus on\nT5small modified to have 8 encoder layers and 8\ndecoder layers.\n7 How Much Data Leads to the\nEmergence of a Syntactic Bias?\nThe next experiment we report has two goals.\nFirst, we aim to replicate the finding that sim-\n332768 is the vocab size for T5 (Raffel et al., 2020).\n4The best vocab size varied depending on model size and\ncorpus size. V ocab size 8192 tends to work better for smaller\ncorpora and smaller models on average, and 32768 tends to\nwork better for larger corpora and larger models.\npler language gives rise to a stronger syntactic\nbias. Second, we expand the range of corpus sizes\nfor the genres where larger corpora are available;\nour goal is to determine how much data is neces-\nsary to induce a hierarchical bias from each genre.\nIn addition to child-directed speech and English\nWikipedia, which we included in the previous ex-\nperiment, we also pre-train models on the Colossal\nCleaned Common Crawl (C4) web text corpus (Raf-\nfel et al., 2020) and on Simple Wikipedia, which\ncontains text from the same domain as English\nWikipedia, but with a more limited vocabulary and\nsimpler sentence structures.\n7.1 Method\nWe collect English Wikipedia data using\nWikidumps.5 We use the witokit library6\nto preprocess the data. We pre-train on {1M,\n10M, 100M, 1B} words of English Wikipedia\ndata, where words are counted before being\ndivided into subwords by the tokenizer. Our {1M,\n10M}-word data is from Huebner et al. (2021);\nour {100M, 1B}-word data is a concatenation\nof their 10M-word dataset with the Wikidump\ndata that we download and preprocess. For C4,\nwe randomly shuffle the HuggingFace version\nof the dataset 7 and sample individual examples\nuntil we have reached 1B words. We then create\n{1M, 10M, 100M, 1B}-word datasets by uniformly\nsubsampling the data, ensuring that smaller\ndatasets are subsamples of larger datasets. For\nCHILDES, we only have access to 5M words,\nso we pre-train on {1M, 5M} words, where the\n1M-word dataset is a uniform subsample of the\n5M-word dataset.\nWe also download Simple Wikipedia\nWikidumps,8 and follow the same prepro-\ncessing pipeline we used for the English Wikipedia.\nSince we only have access to approximately 300M\nwords of Simple Wikipedia, we only pre-train on\n{1M, 10M, 100M} words, where smaller datasets\nare uniform subsamples of larger datasets.\nFor all genres and sizes, we use the best-\nperforming architecture from §6 (T5 small with 8\nencoder layers and 8 decoder layers), as well as the\nbest training hyperparameters from that experiment.\nWe tune over vocabulary size for each corpus style\nand size. See App. A for details.\n5https://dumps.wikimedia.org/enwiki/\n6https://github.com/akb89/witokit\n7https://huggingface.co/datasets/c4\n8https://dumps.wikimedia.org/simplewiki/\n11243\n1 10 100 1000\nNumber of words (millions)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Main auxiliary accuracy\nEnglish Question Formation\nCHILDES\nSimple Wikipedia\nWikipedia\nC4\n1 10 100 1000\nNumber of words (millions)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Object accuracy\nEnglish Passivization\nCHILDES\nSimple Wikipedia\nWikipedia\nC4\nFigure 5: Generalization accuracies for question forma-\ntion (top) and passivization (bottom) when pre-training\na small T5-like model (8 encoder layers and 8 decoder\nlayers) on corpora of various sizes and domains. Sim-\npler language induces syntactic generalization with less\ndata: CHILDES outperforms other datasets, and Simple\nWikipedia outperforms Wikipedia. Accuracies (points)\nand standard deviations (shaded regions) are measured\nacross 5 random seeds of fine-tuning.\n7.2 Results\nReplicating and extending our results from §6, we\nfind that pre-training on simpler language in-\nduces hierarchical generalization using less data\n(Figure 5). For question formation, transcribed\nchild-directed speech, the simplest language style\nwe use, induces hierarchical generalization in well\nover 50% of question formation generalization ex-\namples using just 5M words. For Simple Wikipedia\nand C4, 100M words are required to reach this ac-\ncuracy level; for Wikipedia, 1B words. Models\npre-trained on Simple Wikipedia generalize in a\nmuch more syntax-sensitive manner than models\npre-trained on a similar amount of Wikipedia data.\nFor passivization, generalization accuracies are\ngenerally much higher, though the qualitative\ntrends we observe for question formation still hold:\nchild-directed speech induces hierarchical gener-\nMain auxiliary Object\nDataset # Words accuracy accuracy\nWikipedia 100M 0.27 (.08) 0.98 (.01)\nWikipedia + CHILDES 105M 0.20 (.05) 0.99 (.01)\nC4 100M 0.67 (.04) 1.00 (.00)\nC4 + CHILDES 105M 0.60 (.05) 1.00 (.00)\nCHILDES 5M 0.73 (.04) 0.99 (.00)\nTable 3: Generalization accuracy for question forma-\ntion and passivization using the 100M-word versions\nof Wikipedia and C4, before and after concatenating\nCHILDES. Accuracy is averaged over five fine-tuning\nseeds (standard deviation over seeds in parentheses).\nalization using less data, and Simple Wikipedia\ninduces hierarchical generalization using less data\nthan Wikipedia.\nCould we narrow the gap between Wikipedia/C4\nand CHILDES by simply concatenating CHILDES\nto these datasets? The answer appears to be no:\nperformance does not significantly change when\nconcatenating CHILDES to Wikipedia, nor when\nconcatenating CHILDES to C4 (Table 3). Perhaps\nthe style of the different datasets is too dissimilar\nfor the model to form consistent generalizations\nwhen exposed to both distributions simultaneously.\nIt could be more beneficial to run a two-phase pre-\ntraining procedure, where we expose the model to\nthe simpler CHILDES dataset first, and then expose\nit to Wikipedia or C4 only after it has acquired\nthe hierarchical inductive bias. We discuss this\nhypothesis in more detail in §8.\n8 Discussion\nWhy does depth facilitate the emergence of a\nsyntactic bias? Our first set of experiments sug-\ngests that depth is the most important architectural\nfactor contributing to hierarchical generalization\nin Transformers. This finding is consistent with\nthe suggestion of Tay et al. (2022), who advocate\nfor deeper and narrower architectures for the best\nperformance across NLP tasks. Why are deeper\nmodels better in practice for many tasks and linguis-\ntic evaluations, when in theory an arbitrarily wide\nmodel can approximate any function with only two\nlayers (Hornik et al., 1989)?\nOne natural hypothesis is that Transformers gen-\neralize hierarchically on the basis of tree-structured\nrepresentations organized across layers, such that\nhigher layers represent larger constituents, and re-\ncursive syntactic operations are applied across suc-\ncessive layers; such a strategy arises more naturally\n11244\nin a deeper model. In recent work, Murty et al.\n(2023) find evidence that the internal organization\nof Transformer representations across layers be-\ncomes more tree-like over the course of training\non some tasks, and that this property predicts the\nmodel’s compositional generalization. While they\nfail to find a correlation between model depth and\nthe degree to which representations are tree-shaped,\nthis may be because they train relatively small mod-\nels from scratch on synthetic datasets. In future\nwork, methods such as those of Murty et al. (2023)\nmay be used to measure the tree-likeness of Trans-\nformers’ representations throughout pre-training\non natural language, and the degree to which the\ntree-likeness of the pre-trained model correlates\nwith the its syntactic inductive bias for fine-tuning.\nWhy does simpler language teach syntax more\neffectively? We find that pre-training on simpler\nlanguage, such as child-directed speech or Sim-\nple Wikipedia, enables hierarchical generalization\nfrom far less pre-training data than more com-\nplex language. Our findings from encoder-decoder\nmodels are consistent with previous findings from\nencoder-only masked LMs (Huebner et al., 2021),\nand with work on language understanding from\nspeech (Gelderloos et al., 2020). The advantage\nof child-directed speech may be attributable to re-\nduced lexical complexity, reduced syntactic com-\nplexity, or both (Soderstrom, 2007). Lower lexical\ncomplexity—in this case, fewer word types—may\nmake it possible to learn the distribution of, say,\nparts of speech from a smaller corpus, as the same\nwords would recur more often in different contexts.\nLower syntactic complexity could result in a higher\nproportion of short sentences with unambiguous\nsyntactic structure, which could help bootstrap syn-\ntactic learning. These two features are correlated in\nnatural child-directed speech, but could be disentan-\ngled in future work by independently manipulating\nthe lexical and syntactic distributions.\nSimpler language can be leveraged for more ef-\nficient pre-training. Our experiments show that\nnot all pre-training data is created equal, and mo-\ntivate further research on data curation for pre-\ntraining, and in particular on curriculum learning\n(Bengio et al., 2009). We conjecture that robust syn-\ntactic inductive biases will play a role not only in\nfine-tuning but also in pre-training, making it pos-\nsible for models to use additional pre-training sen-\ntences more efficiently. This motivates a two-phase\n“starting small” approach (Elman, 1993), where the\nmodel is first exposed a model to child-directed\nspeech until syntactic inductive biases emerge, and\nthen pre-training on a larger corpus proceeds as\nusual afterwards. This approach is related to, but\ndistinct from, the single-phase simple-to-complex\napproach, where a pre-training dataset is sorted\nfrom the simplest inputs to the most complex and\nthen presented to a model in order. The single-\nphase approach has demonstrated mixed results\n(Campos, 2021; Surkov et al., 2022), but to our\nknowledge, a syntax-focused two-phase approach\nhas not yet been attempted.\nTransformers may be more data-efficient than\npreviously thought. Our findings about the\namount of pre-training data required for the ac-\nquisition of syntactic biases also have implications\nfor cognitive modeling research. Humans learn\nlanguage from far fewer words than contemporary\nLMs, and at the same time generalize their lin-\nguistic knowledge to new settings more robustly;\nconversely, standard NLP evaluations, which do\nnot take the pre-training corpus into consideration,\nimplicitly reward architectures that learn well from\nvast amounts of data, raising the concern that those\narchitectures are suboptimal for cognitive model-\ning (Linzen, 2020). Our evaluation setup and em-\npirical results go some way towards addressing\nthese concerns: we show that pre-training on a\ndevelopmentally plausible amount of data can in-\nduce human-like inductive biases that improve out-\nof-distribution generalization. This suggests that\nTransformers, when trained in cognitively relevant\nregimes, may serve as fruitful models of human\nlanguage acquisition and processing (see also Hos-\nseini et al. 2022).\n9 Conclusions\nWe have analyzed the architectural and data fea-\ntures that contribute to the acquisition of syntactic\ninductive biases during the pre-training of encoder-\ndecoder Transformers. We find that model depth\nmatters more for hierarchical generalization than\nother model components (§5); that models more\nquickly learn that language is hierarchical given\nsimpler language (§6); and that it takes orders-of-\nmagnitude more data to induce hierarchical induc-\ntive biases when pre-training on genres such as\nWikipedia or web text, compared to simpler data\nsuch as child-directed speech (§7).\n11245\nAcknowledgements\nWe thank the authors of Tay et al. (2022) for facili-\ntating academic research on language model scal-\ning by releasing a large range of model checkpoints.\nWe also thank Alexandra DeLucia, Nathaniel Weir,\nand Daniel Khashabi for their thoughtful feedback\non earlier versions of this paper. This material is\nbased upon work supported by the National Science\nFoundation (NSF) under Grant No. BCS-2114505.\nAaron Mueller was supported by a National Sci-\nence Foundation Graduate Research Fellowship\n(Grant #1746891). This work was supported in\npart through the NYU IT High Performance Com-\nputing resources, services, and staff expertise.\nLimitations\nOur analyses are based on models with T5-like ar-\nchitectures and span denoising training objectives.\nThus, our findings may not generalize to other\ntypes of encoder-decoder models (e.g., BART), nor\nencoder-only and decoder-only models. We be-\nlieve this is unlikely, given that similar findings\nhave been shown for models with architectures and\nobjectives that differ significantly from T5’s (Hueb-\nner et al., 2021; Warstadt and Bowman, 2020).\nNonetheless, it cannot be ruled out.\nOur analyses are also based entirely in English,\nand only leverage two syntactic transformations.\nIt is possible that our findings will not generalize\nto other languages, given that certain grammati-\ncal features (e.g., more extensive case marking) in-\nduce more syntax-sensitive behavior given a similar\namount of training data across languages (Mueller\net al., 2020; Ravfogel et al., 2019); thus, perhaps\nless Wikipedia or C4 data is needed in these lan-\nguages for models to acquire hierarchical prefer-\nences. It is also possible that, within a language,\na model could adopt a hierarchical inductive bias\nfor one type of transformation, but not another—\nespecially if one transformation is much more fre-\nquent than the other. Indeed, the frequency of par-\nticular words positively correlates with syntactic\nevaluation accuracies (Wei et al., 2021; Newman\net al., 2021), and it would be reasonable to expect\na similar trend for the frequency of syntactic trans-\nformations. Thus, future work should investigate\nmore transformations in more languages to ensure\nthat these findings are consistent.\nReferences\nYoshua Bengio, Jérôme Louradour, Ronan Collobert,\nand Jason Weston. 2009. Curriculum learning. In\nProceedings of the 26th Annual International Confer-\nence on Machine Learning, ICML ’09, page 41–48,\nNew York, NY , USA. Association for Computing\nMachinery.\nAndrew Biemiller. 2003. V ocabulary: Needed if more\nchildren are to read well. Reading Psychology, 24(3-\n4):323–335.\nDaniel Campos. 2021. Curriculum learning for lan-\nguage modeling. Computing Research Repository,\narXiv:2108.02170.\nRich Caruana. 1997. Multitask learning. Machine\nLearning, 28(1):41–75.\nJeffrey L. Elman. 1993. Learning and development in\nneural networks: The importance of starting small.\nCognition, 48(1):71–99.\nRobert Frank and Donald Mathis. 2007. Transforma-\ntional networks. In Proceedings of the Workshop on\nPsychocomputational Models of Human Language\nAcquisition. Cognitive Science Society.\nJon Gauthier, Jennifer Hu, Ethan Wilcox, Peng Qian,\nand Roger Levy. 2020. SyntaxGym: An online plat-\nform for targeted evaluation of language models. In\nProceedings of the 58th Annual Meeting of the Associ-\nation for Computational Linguistics: System Demon-\nstrations, pages 70–76, Online. Association for Com-\nputational Linguistics.\nLieke Gelderloos, Grzegorz Chrupała, and Afra Al-\nishahi. 2020. Learning to understand child-directed\nand adult-directed speech. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 1–6, Online. Association\nfor Computational Linguistics.\nYoav Goldberg. 2019. Assessing BERT’s syntac-\ntic abilities. Computing Research Repository ,\narXiv:1901.05287.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation,\n9(8):1735–1780.\nKurt Hornik, Maxwell Stinchcombe, and Halbert White.\n1989. Multilayer feedforward networks are universal\napproximators. Neural Networks, 2(5):359–366.\nEghbal A. Hosseini, Martin Schrimpf, Yian Zhang,\nSamuel Bowman, Noga Zaslavsky, and Evelina Fe-\ndorenko. 2022. Artificial neural network language\nmodels align neurally and behaviorally with humans\neven after a developmentally realistic amount of train-\ning. bioRxiv.\nJennifer Hu, Jon Gauthier, Peng Qian, Ethan Wilcox,\nand Roger Levy. 2020. A systematic assessment\nof syntactic generalization in neural language mod-\nels. In Proceedings of the 58th Annual Meeting of\n11246\nthe Association for Computational Linguistics, pages\n1725–1744, Online. Association for Computational\nLinguistics.\nPhilip A. Huebner, Elior Sulem, Fisher Cynthia, and\nDan Roth. 2021. BabyBERTa: Learning more gram-\nmar with small-scale child-directed language. In Pro-\nceedings of the 25th Conference on Computational\nNatural Language Learning, pages 624–646, Online.\nAssociation for Computational Linguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nYair Lakretz, Dieuwke Hupkes, Alessandra Vergallito,\nMarco Marelli, Marco Baroni, and Stanislas Dehaene.\n2021. Mechanisms for handling nested dependen-\ncies in neural-network language models and humans.\nCognition, 213:104699. Special Issue in Honour of\nJacques Mehler, Cognition’s founding editor.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nTal Linzen. 2020. How can we accelerate progress\ntowards human-like linguistic generalization? In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5210–\n5217, Online. Association for Computational Lin-\nguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretrain-\ning approach. Computing Research Repository,\narXiv:1907.11692.\nCharles Lovering, Rohan Jha, Tal Linzen, and Ellie\nPavlick. 2021. Predicting inductive biases of pre-\ntrained models. In International Conference on learn-\ning representations.\nBrian MacWhinney. 2000. The CHILDES project: The\ndatabase, volume 2. Psychology Press.\nRebecca Marvin and Tal Linzen. 2018. Targeted syn-\ntactic evaluation of language models. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 1192–1202,\nBrussels, Belgium. Association for Computational\nLinguistics.\nR. Thomas McCoy, Robert Frank, and Tal Linzen. 2018.\nRevisiting the poverty of the stimulus: Hierarchical\ngeneralization without a hierarchical bias in recurrent\nneural networks. In Proceedings of the 40th An-\nnual Meeting of the Cognitive Science Society, pages\n2096–2101, Madison, Wisconsin. Cognitive Science\nSociety.\nR. Thomas McCoy, Robert Frank, and Tal Linzen. 2020.\nDoes syntax need to grow on trees? Sources of hier-\narchical inductive bias in sequence-to-sequence net-\nworks. Transactions of the Association for Computa-\ntional Linguistics, 8:125–140.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right\nfor the wrong reasons: Diagnosing syntactic heuris-\ntics in natural language inference. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 3428–3448, Florence,\nItaly. Association for Computational Linguistics.\nTom M Mitchell. 1980. The need for biases in learn-\ning generalizations (Rutgers Computer Science Tech.\nRept. CBM-TR-117). Rutgers University.\nAaron Mueller, Robert Frank, Tal Linzen, Luheng Wang,\nand Sebastian Schuster. 2022. Coloring the blank\nslate: Pre-training imparts a hierarchical inductive\nbias to sequence-to-sequence models. In Findings of\nthe Association for Computational Linguistics: ACL\n2022, pages 1352–1368, Dublin, Ireland. Association\nfor Computational Linguistics.\nAaron Mueller, Garrett Nicolai, Panayiota Petrou-\nZeniou, Natalia Talmina, and Tal Linzen. 2020.\nCross-linguistic syntactic evaluation of word predic-\ntion models. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5523–5539, Online. Association for Computa-\ntional Linguistics.\nShikhar Murty, Pratyusha Sharma, Jacob Andreas, and\nChristopher D. Manning. 2023. Characterizing in-\ntrinsic compositionality in transformers with tree pro-\njections. In International Conference on Learning\nRepresentations.\nBenjamin Newman, Kai-Siang Ang, Julia Gong, and\nJohn Hewitt. 2021. Refining targeted syntactic evalu-\nation of language models. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3710–3723, Online.\nAssociation for Computational Linguistics.\nJackson Petty and Robert Frank. 2021. Transformers\ngeneralize linearly. Computing Research Repository,\narXiv:2109.12036.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\n11247\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nShauli Ravfogel, Yoav Goldberg, and Tal Linzen. 2019.\nStudying the inductive biases of RNNs with synthetic\nvariations of natural languages. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 3532–3542, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nShauli Ravfogel, Yoav Goldberg, and Francis Tyers.\n2018. Can LSTM learn to capture agreement? the\ncase of Basque. In Proceedings of the 2018 EMNLP\nWorkshop BlackboxNLP: Analyzing and Interpreting\nNeural Networks for NLP, pages 98–107, Brussels,\nBelgium. Association for Computational Linguistics.\nMelanie Soderstrom. 2007. Beyond babytalk: Re-\nevaluating the nature and content of speech input to\npreverbal infants. Developmental Review, 27(4):501–\n532.\nMaxim Surkov, Vladislav Mosin, and Ivan\nYamshchikov. 2022. Do data-based curricula\nwork? In Proceedings of the Third Workshop on\nInsights from Negative Results in NLP, pages 119–\n128, Dublin, Ireland. Association for Computational\nLinguistics.\nYi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus,\nSamira Abnar, Hyung Won Chung, Sharan Narang,\nDani Yogatama, Ashish Vaswani, and Donald Met-\nzler. 2022. Scale efficiently: Insights from pretrain-\ning and finetuning transformers. In International\nConference on Learning Representations.\nMarten van Schijndel, Aaron Mueller, and Tal Linzen.\n2019. Quantity doesn’t buy quality syntax with neu-\nral language models. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 5831–5837, Hong Kong, China. As-\nsociation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. SuperGLUE: A stickier\nbenchmark for general-purpose language understand-\ning systems. In Advances in Neural Information\nProcessing Systems, volume 32. Curran Associates,\nInc.\nAlex Warstadt and Samuel R. Bowman. 2020. Can\nneural networks acquire a structural bias from raw\nlinguistic data? In Proceedings of the 42nd Annual\nMeeting of the Cognitive Science Society, Online.\nCognitive Science Society.\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R.\nBowman. 2020a. BLiMP: The benchmark of linguis-\ntic minimal pairs for English. Transactions of the\nAssociation for Computational Linguistics, 8:377–\n392.\nAlex Warstadt, Yian Zhang, Xiaocheng Li, Haokun Liu,\nand Samuel R. Bowman. 2020b. Learning which\nfeatures matter: RoBERTa acquires a preference for\nlinguistic generalizations (eventually). In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n217–235, Online. Association for Computational Lin-\nguistics.\nJason Wei, Dan Garrette, Tal Linzen, and Ellie Pavlick.\n2021. Frequency effects on syntactic rule learning\nin transformers. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 932–948, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nEthan Wilcox, Roger Levy, Takashi Morita, and Richard\nFutrell. 2018. What do RNN language models learn\nabout filler–gap dependencies? In Proceedings of\nthe 2018 EMNLP Workshop BlackboxNLP: Analyz-\ning and Interpreting Neural Networks for NLP, pages\n211–221, Brussels, Belgium. Association for Com-\nputational Linguistics.\nColin Wilson. 2006. Learning phonology with substan-\ntive bias: An experimental and computational study\nof velar palatalization. Cognitive Science, 30(5):945–\n982.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\n11248\nA Hyperparameters\nWhen fine-tuning models on syntactic transforma-\ntions, we use settings from Mueller et al. (2022):\nbatch size 128, window size 128, initial learning\nrate of 5 ×10−5, fine-tune for 10 epochs (≈7500\ntraining steps), checkpoint and evaluate every 500\nsteps.\nWhen pre-training models from scratch, we train\nfor 130K training steps, batch size 16 (except for\nthe 1B-word datasets, where we use batch size 128\nsuch that the model sees the entire dataset at least\nonce). We tune over the vocabulary size ∈{8192,\n32768} for each dataset and dataset size.\nB Is the Encoder or Decoder More\nImportant for Hierarchical\nGeneralization?\nIn §5, we found that model depth is more important\nthan model width for enabling LMs to acquire a\nhierarchical inductive bias. Here, we specifically\ninvestigate whether the encoder or decoder of the\nmodel is more important by varying the depth of\nthe encoder and decoder individually and observing\nchanges in generalization patterns. As in §5, our\nmodels are based on the T5base architecture, which\nhas 12 encoder and 12 decoder layers.\nIn our results (Figure 6), we observe that de-\ncreasing the depth of either component leads to\nsimilar losses in accuracy on passivization, though\ndecreasing decoder depth results in consistently\nlower accuracies for question formation. Thus, to-\ntal depth may be the most important factor, regard-\nless of where it is concentrated. Nonetheless, we\nobserve preliminary evidence for the decoder being\nslightly more important for acquiring a hierarchical\ninductive bias—or at least generating outputs that\nare consistent with this bias for question formation.\nFuture work could investigate other transforma-\ntions and other languages to test the consistency of\nthese findings.\nC All Architectural Variation Results\nIn §5 and App. B, we show that model depth is\nmore important than model width. However, we\ndid not show the performance of models where we\nvary the number of attention heads, nor the key-\nvalue projection matrix dimension. Here, we show\nthe full results (Figure 7).\nOverall, varying the number of attention heads\nhas little effect on the performance of the model.\n1.2 1.4 1.6 1.8 2.0 2.2\nParameters 1e8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nDL2\nDL4\nDL8\nEL2\nEL4 EL8\nT5-base\nEnglish Question Formation\nComponent\nDL\nEL\n1.2 1.4 1.6 1.8 2.0 2.2\nParameters 1e8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nDL2\nDL4\nDL8\nEL2\nEL4\nEL8 T5-base\nEnglish Passivization\nComponent\nDL\nEL\nFigure 6: Generalization sequence accuracies on ques-\ntion formation (top) and passivization (bottom) using\nT5base, as well as variants of T5base where we vary the\nencoder depth/decoder depth. Here, EL2 refers to an\narchitecture identical to T5base, except it has 2 encoder\nlayers (instead of 12). Likewise, DL2 is created from\nT5base by modifying the number of decoder layers, keep-\ning the number of encoder layers at the original 12.\nWe see the same trend for reductions in the size of\nthe key/value projection matrix. Thus, model depth\nstill appears to be the most important component\nin inducing hierarchy-sensitive generalizations.\n11249\n0.5 1.0 1.5 2.0\nParameters 1e8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nDM256\nDM512\nFF1024 FF2048\nKV16\nKV32NH8\nNL2\nNL4 NL8\nT5-base\nEnglish Question Formation\nComponent\nDM\nFF\nKV\nNH\nNL\n0.5 1.0 1.5 2.0\nParameters 1e8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nDM256\nDM512\nFF1024\nFF2048\nKV16\nKV32\nNH8\nNL2\nNL4\nNL8\nT5-base\nEnglish Passivization\nComponent\nDM\nFF\nKV\nNH\nNL\nFigure 7: Generalization sequence accuracies on ques-\ntion formation (top) and passivization (bottom) using all\narchitectural variants of T5base, as well as T5base itself.\n11250\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitations section at end of paper.\n□\u0017 A2. Did you discuss any potential risks of your work?\nWe evaluate the syntactic abilities of language models trained on small amounts of data. Our ﬁndings\nhave implications for model evaluations, but do not change the models or their behavior.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 3.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 3.\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nThe datasets and models we use all use highly permissive licenses, and we are using each as originally\nintended for research purposes.\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nSee B2.\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nThe data we use is synthetically generated using a CFG and contains only semantically implausible\nsentences. It contains no PII.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection 3.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 3.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n11251\nC □\u0013 Did you run computational experiments?\nSections 3, 4, 5.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nTable 1; Sections 3, 4, 5.\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSections 3, 4, 5.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 3.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSections 3, 4, 5.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n11252",
  "topic": "Inductive bias",
  "concepts": [
    {
      "name": "Inductive bias",
      "score": 0.8782752156257629
    },
    {
      "name": "Computer science",
      "score": 0.8245630860328674
    },
    {
      "name": "Generalization",
      "score": 0.6046246290206909
    },
    {
      "name": "Natural language processing",
      "score": 0.5832105278968811
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5679967403411865
    },
    {
      "name": "Language model",
      "score": 0.5500829815864563
    },
    {
      "name": "Focus (optics)",
      "score": 0.48365700244903564
    },
    {
      "name": "Transformer",
      "score": 0.4624583423137665
    },
    {
      "name": "Artificial neural network",
      "score": 0.4471759498119354
    },
    {
      "name": "Encoder",
      "score": 0.4208185374736786
    },
    {
      "name": "Multi-task learning",
      "score": 0.1416105329990387
    },
    {
      "name": "Mathematics",
      "score": 0.08310839533805847
    },
    {
      "name": "Task (project management)",
      "score": 0.07438793778419495
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}