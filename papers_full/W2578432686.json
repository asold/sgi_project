{
  "title": "Deep-Deep Neural Network Language Models for Predicting Mild Cognitive Impairment",
  "url": "https://openalex.org/W2578432686",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A2751927683",
      "name": "Orimaye, Sylvester Olubolu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4266712259",
      "name": "Jojo Sze-Meng Wong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2573725465",
      "name": "Judyanne Sharmini Gilbert Fernandez",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2076991840",
    "https://openalex.org/W1984994164",
    "https://openalex.org/W122584218",
    "https://openalex.org/W1862599395",
    "https://openalex.org/W146732357",
    "https://openalex.org/W2008176715",
    "https://openalex.org/W2132923627",
    "https://openalex.org/W2115017507",
    "https://openalex.org/W2046326055",
    "https://openalex.org/W2136914353",
    "https://openalex.org/W2160815625",
    "https://openalex.org/W1575384945",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W1970689298",
    "https://openalex.org/W2007553631",
    "https://openalex.org/W2063042856",
    "https://openalex.org/W2102062345"
  ],
  "abstract": null,
  "full_text": "Deep-Deep Neural Network Language Models for Predicting\nMild Cognitive Impairment\nSylvester Olubolu Orimaye, Jojo Sze-Meng Wong and Judyanne Sharmini Gilbert Fernandez\nIntelligent Health Research Group\nSchool of Information Technology, Monash University Malaysia\n{sylvester.orimaye,jojo.wong,judyanne.gilbert}@monash.edu\nAbstract\nEarly diagnosis of Mild Cognitive Impairment\n(MCI) is currently a challenge. Currently, MCI is\ndiagnosed using speciﬁc clinical diagnostic criteria\nand neuropsychological examinations. As such we\npropose an automated diagnostic technique using\na variant of deep neural networks language mod-\nels (DNNLM) on the verbal utterances of MCI pa-\ntients. Motivated by the success of DNNLM on\nnatural language tasks, we propose a combination\nof deep neural network and deep language models\n(D2NNLM) to predict MCI. Results on the Demen-\ntiaBank language transcript clinical dataset show\nthat D2NNLM sufﬁciently learned several linguis-\ntic biomarkers in the form of higher order n-grams\nand skip-grams to distinguish the MCI group from\nthe healthy group with reasonable accuracy, which\ncould help clinical diagnosis even in the absence of\nsufﬁcient training data.\n1 Introduction\nEarly diagnosis of Mild Cognitive Impairment (MCI) is cur-\nrently a challenge [Abbott, 2011 ]. More importantly, MCI\nhas been typically diagnosed through extensive neuropsycho-\nlogical examinations using a series of cognitive tests contain-\ning a set of questions and images [Mitolo et al., 2015]. For\nexample, the Mini-Mental State Examination (MMSE) and\nthe Montreal Cognitive Assessment (MoCA) screening tools\nare composed of a series of questions and cognitive tests that\nassess different cognitive abilities. The challenge with these\ncognitive tests is that the accuracy depends on the clinician’s\nlevel of experience and their ability to diagnose different sub-\ntypes of the disease [Damian et al., 2011]. Often, researchers\nand clinicians need to combine other cognitive tests with the\nMMSE [Mitchell, 2009 ], and in most cases wait for a rea-\nsonably long interval to ascertain diagnosis [Pozueta et al.,\n2011]. More recently, research has also shown that the re-\nliability of the MMSE as a tool for diagnosing MCI could\nbe limited [Kim and Caine, 2014]. The National Institute on\nAging and the Alzheimer’s Association has also called for\nseveral other clinical criteria that could be used to effectively\ndiagnose MCI and other similar disease in a non-invasive way\n[Albert et al., 2011].\nAs opposed to the ad hoc use of neuropsychological ex-\naminations, linguistic ability captured from verbal utterances\ncould be a good indication of MCI and other related diseases\n[Tillas, 2015]. The premise is that, MCI is characterized by\nthe deterioration of nerve cells that control cognitive, speech\nand language processes, which consequentially translates to\nhow patients compose verbal utterances. According to [Ball\net al., 2009], syntactic processing in acquired language disor-\nders such as Aphasia in adults, has shown promising ﬁndings,\nencouraging further study on identifying effective syntactic\ntechniques. Similarly, [Locke, 1997] emphasized the signiﬁ-\ncance of lexical-semantic components of a language, part of\nwhich is observable during utterance acquisition at a younger\nage. That work further highlighted that as the lexical capac-\nity increases, syntactic processing becomes automated, hence\nleading to lexical and syntactic changes in language.\nAs such, we are motivated by the effectiveness of deep neu-\nral networks language models (DNNLM) in modeling acous-\ntic signals for natural language tasks. In particular, we are in-\nspired by [Schwenk, 2007], which shows that a feed-forward\nneural network language model can be trained with low error\nrate and perplexity. Even with just 1 hidden layer, the perfor-\nmance of the NNLM was better than the conventional 4-gram\nlanguage model. The DNNLM improved on the NNLM with\nlower error rate and perplexity.\nThus, we explore deep-deep neural networks language\nmodels (D2NNLM) to learn the linguistic changes that dis-\ntinguish the language of patients with MCI from the healthy\ncontrols. The ordinary DNNLM uses lower order n-gram\nN dimensional sparse vectors as discrete feature representa-\ntions to learn the neural network with multiple hidden layers\n(DNN). In this paper, we maintain the same DNN architec-\nture and increase the depth of the language models by intro-\nducing higher order n-gram and skip-gram N dimensional\nsparse vectors as discrete inputs to the DNN rather than sin-\ngle word N dimensional sparse vectors. In other words, we\ncreate n-gram and skip-gram vocabulary spaces from which\nwe formed the N dimensional sparse vectors. The premise is\nthat clinical datasets are usually sparse and it is the same for\nthe DementiaBank1 dataset used in this paper. Thus, using\nlower order n-gram dimensional sparse vectors alone could\nlimit the vocabulary space and subsume the essential linguis-\n1http://talkbank.org/DementiaBank/\ntic changes and biomarkers, which could potentially distin-\nguish patients with MCI from the healthy controls. On the\nother hand, n-grams and skip-grams have been shown to be\ngood class predictors in several language modeling tasks on\nsparse data [Sidorov et al., 2014]. In addition, the DNN has\nbeen effective in learning discriminating features from sparse\nfeature representations [Li et al., 2015]. To the best of our\nknowledge, little work has considered deep neural network\nand deep language models for predicting MCI on sparse clin-\nical language datasets.\nThe rest of this paper is organized as follows. Section 2\ndiscusses related work. Section 3 describes the formal archi-\ntecture of the DNNLM. We explain our D2NNLM in Section\n4. Section 5 presents our experimental results. Finally, Sec-\ntion 7 concludes the paper.\n2 Related Work\nIn [Roark et al., 2011], the efﬁcacy of using complex syntac-\ntic features to classify MCI was demonstrated. In that work,\nspoken language characteristics were used to discriminate be-\ntween 37 patients with MCI and 37 in the healthy elderly\ngroup using 7 signiﬁcant pause and syntactic linguistic anno-\ntations as features to train Support Vector Machines (SVM).\nThat technique achieved 86.1% Area Under the ROC Curve\n(AUC). On the contrary, we use language models, which are\nmore representative of the language space of both the disease\nand healthy groups without using any handcrafted features.\nMore recently, [Prud’hommeaux and Roark, 2015 ] pro-\nposed a ‘graph-based content word summary score’ and\n‘graph-based content word word-level score’ to predict\nAlzheimer’s disease (AD), which is often preceded by MCI.\nUsing SVM on the same DementiaBank, that work achieved\n82.3% AUC. However, the graph based techniques require\nseparately built alignment models with sufﬁciently large\ndatasets.\nThis paper has two main contributions. (1) We introduce\ndeep language models in the form of higher order n-grams\nand skip-grams N dimensional sparse vectors as discrete in-\nputs to the DNN, hence we derived D2NNLM. (2) We show\nthat D2NNLM predicts MCI with less percentage error, per-\nplexity, and predictive accuracy compared to other baselines\nespecially on sparse clinical language datasets.\n3 Deep Neural Network Language Models\nThe DNNLM architecture has more than one hidden layer\nwith nonlinear activations [Arisoy et al., 2012], and it is built\non top of the original feed-forward NNLM architecture[Ben-\ngio et al., 2003]. Unlike the DNNLM, NNLM has only two\nhidden layers. The ﬁrst hidden layer has a linear activation\nand often referred to as the projection layer. The second\nhidden layer uses a nonlinear activation, hence making the\nNNLM a single hidden layer neural network [Bengio et al.,\n2003].\nIn this paper, we follow the notations used in [Schwenk,\n2007] and [Arisoy et al., 2012] to describe the components\nof the DNNLM architecture. Given a vocabulary space, each\nword in the vocabulary is denoted by aN dimensional sparse\nvector. In each vector, the index of that particular word is\nstored with 1 while other indices in the vector are stored with\n0s. As inputs to the neural network, the discrete feature repre-\nsentations are concatenated to contain then-1 previous words\nin the vocabulary space, which serves as the memory to the\nprevious words history. Given that N is the vocabulary size\nand P is the size of the projection layer, linear projections of\nall the concatenated words are used to create the ﬁrst hidden\nlayer of the network from every ith row of the N x P dimen-\nsional projection matrix. This is followed by the hidden layer\nH with hyperbolic tangent non-linear activation functions as\nfollows:\ndj = tanh\n\n\n(n−1)×P∑\nl=1\nMjlcl + bj\n\n∀j = 1,...,H (1)\nwhere H is the number of hidden layers, the weights be-\ntween the projection layer and the subsequent hidden layers\nare denoted with Mjl, and the biases of the hidden layers are\nrepresented with bj.\nNote that since the DNNLM follows the NNLM architec-\nture, other hidden layers with the same hyperbolic tangent\nnon-linear activation functions are added to make the network\ndeeper. The output layer uses a softmax function to simul-\ntaneously compute the language model probability of each\nword igiving its history, hj, thus P(wj = i|hj). We present\nthe details of the output layer and the language model proba-\nbility as follows:\noi =\nH∑\nj=1\nVijdj + ki∀i= 1,...,N (2)\npi = P(wj = i|hj) = exp(oi)∑N\nl=1 exp(ol)\n∀i= 1,...,N (3)\nwhere Vij denotes the weights between the hidden layers\nand the output layer, ki represents the biases of the output\nlayer, and the pi computes the language model probability\nfor every ith output neuron.\n4 Deep-Deep Neural Network Language\nModels\nOur D2NNLM uses the same architecture with DNNLM\ncomprising of multiple hyperbolic tangent non-linear activa-\ntion functions. On top of that, we make the vocabulary space\ndeeper by including additional n-gram and skip-gram vocab-\nulary spaces to the ordinary n-gram vocabulary space used in\nthe original DNNLM. Figure 1 shows the architecture of the\nthe D2NNLM.\nWith regard to predicting language utterances with MCI,\nit is of paramount importance to our D2NNLM that the lan-\nguage is modeled with a vocabulary space of substantial depth\ndue to the non-trivial nature of the problem [Roark et al.,\n2011; Fraser et al., 2014]. According to the study conducted\nby [Roark et al., 2011 ], many of the handcrafted language\nand speech measures that have been used in distinguishing\npatients with MCI from their respective healthy controls – in-\ncluding some statistically signiﬁcant measures – have shown\nOutput LayerProjection Layer Hidden Layers\no\ndj-n+1\nM V\nb k\nP(wj=1|hj)\nP(wj=i|hj)\nP(wj=N|hj)\n.   .  ..   .  .\n. . .\n0\n1\n0\n0\n:\n0\n1\n0\n0\n0\n:\n0\n0\n:\n0\n0\n1\n0\nWj-n+1\nWj-n+2\nWj--1\ndj-n+2 dj-1\nH\nP\nP\nP\n(n-1) x P\nDeep\nLanguage\nModels\nHigher order \nn-grams\n (n=6)\nskip-grams\n(1-skip-\ntrigrams)\nFigure 1: Deep-Deep Neural Network Language Models\nthe means and the standard deviations to be very close be-\ntween the MCI and healthy control groups. Thus, it is prob-\nable that very little linguistic deﬁcits will characterize either\ngroup. Even with DNNLM, which is based on simplen-gram\nlanguage models with embedded words as a continuous fea-\nture space, it is still challenging to generalize over unseen\ndata due to data sparseness problem [Arisoy et al., 2012]. As\nsuch, an alternate technique could be found in using a stacked\nmixture of language models for embedding the vocabulary in\na much deeper continuous space [Sarikaya et al., 2009]. As\nshown in Figure 1, we stacked higher ordern-gram and skip-\ngram language models to create deep language models for\ndeep neural network. We refer to such models as Deep-Deep\nNeural Network Language Models and our preliminary ex-\nperiments show that deeper language models potentially im-\nprove the performance of deep neural network for predicting\nMCI. We will describe the generation of then-gram and skip-\ngram vocabulary spaces in the following sections.\nSimilar to DNNLM, the computational complexity of\nD2NNLM is characterized by the output layer’s H×N ma-\ntrix multiplications. As such, we follow [Sarikaya et al.,\n2009] and performed Singular Value Decomposition (SVD)\nto produce a reduced-rank approximation of the n-gram and\nskip-gram vocabulary spaces before mapping the vocabular-\nies into the continuous feature space. This is then followed\nby the stacking together (or concatenation) of the projected\nvocabulary history vectors. The undecomposed feature space\ntypically has a large but sparse matrix containing few 1s and\na lot of 0s. As such, SVD becomes a straightforward op-\ntion to produce a compact approximation of the original fea-\nture space with optimal least-square as it sufﬁciently models\nthe frequently occurring 0s, which are often not informative\n[Sarikaya et al., 2009]. Thus, only the low dimensional vo-\ncabulary is used to learn the output targets. Note that the\nDNNLM assigns the probability mass to the output targets. A\nbackground language model was used to perform smoothing\nas done in [Schwenk, 2007]. We trained the neural network\nusing the standard back-propagation algorithm to minimize\nthe error function Er as follows:\nEr =\nN∑\ni=1\nti log pi + ϵ\n\n∑\njl\nM2\njl +\n∑\nij\nV2\nij\n\n∀j = 1,...,H\n(4)\nwhere ti is the target vector, parameterϵis determined em-\npirically using the validation set. Note that the ﬁrst half of the\nequation computes the cross entropy between the output and\nthe target probability masses, and the second half computes\nthe regularization term, which avoids overﬁtting the training\ndata.\n4.1 n-gram vocabulary space\nThe use of word n-gram is popular in NLP especially for de-\nveloping language models that are able to characterize the\nlexical usage of grammar in a dataset. A word n-gram is the\nsequence of words identiﬁed as an independent representation\nof a part of the grammar in an utterance or a sentence. ‘n’in\nthis case, represents the number of words in the sequence.\nFor instance, when nis 1, it is called a ‘unigram’, which has\nonly one word. Similarly, a ‘bigram’ and a ‘trigram’ haven\nequal to 2 and 3 respectively, and it is not uncommon to use\nhigher order n-grams (i.e. n ≥3) in learning tasks [Le et\nal., 2011]. In this paper, our n-gram vocabulary space con-\nsist of 6-grams (n-gram=6) features only, which are generated\nfrom the transcripts of both the MCI and the healthy control\ngroups. We believe that the 6-grams features could subsume\nother lower ordern-grams such as unigrams, bigrams, and tri-\ngrams [Sarikaya et al., 2009]. We put emphasis on higher or-\nder n-grams because they are known to have performed with\nreasonable accuracy in other NLP and ML tasks [Chen and\nChu, 2010].\n4.2 skip-gram vocabulary space\nSkip-grams are commonly used in statistical language mod-\nelling problems such as speech processing. Unlike the ordi-\nnary n-grams, word tokens are skipped intermittently while\ncreating the n-grams. For instance, in the sentence “take\nthe Cookie Jar”, there are three conventional bigrams: “take\nthe”, “the Cookie”, and “Cookie Jar”. With skip-gram,\none might skip one word intermittently for creating addi-\ntional bigrams, which include “take Cookie”, and “the jar”.\nWe believe such skip-grams could capture unique linguistic\nbiomarkers in verbal utterances of patients with MCI. Thus,\nas described [Orimaye et al., 2015], we used a compound of\nskip-grams to create our skip-gram vocabulary space. For\neach sentence S = {w1...wm}in a verbal dialogue, we de-\nﬁne k-skip-n-grams as a set of n-gram tokens Tngram =\n{wa,...,w a+n−k,...,w a+n,...,w m−n,...,w (m−n)+n−k,...,\nwm}, where nis the speciﬁed n-gram (e.g. 2 for bigramand\n3 for trigram), m is the number of word tokens in S, k is\nthe number of word skip between n-grams given that k<m ,\nand a = {1,...,m −n}. Thus for the sentence “take the\nCookie Jar from the cabinet”, 1-skip-2-grams will give{‘take\nCookie’, ‘the Jar’, ‘Cookie from’, ‘Jar the’, ‘from cabinet’}\nand 1-skip-3-grams will produce {‘take Cookie Jar’, ‘take\nthe Jar’, ‘the Jar from’, ‘the Cookie from’, ‘Cookie Jar the’,\n‘Cookie from the’, ‘Jar the cabinet’, ‘Jar from cabinet’}. In\nVocabulary MCI Control\n6-gram 1100 1163\n1-skip-trigram 2877 3011\ntotal 3977 4174\nTable 1: n-gram and skip-gram vocabularies from the MCI\nand Control groups.\nour experiments, we used only 1-skip-3-grams as some of its\nskip-grams often subsume 1-skip-2-grams.\n5 Experiment and Results\n5.1 Dataset and Baselines\nWe performed experiments on existing DementiaBank clin-\nical dataset. The dataset was created during a longitudinal\nstudy conducted by the University of Pittsburgh School of\nMedicine on Alzheimer’s disease (AD) and related Dementia,\nwhich was funded by the National Institute of Aging 2. The\ndataset contains transcripts of verbal interviews with healthy\ncontrols and patients that were diagnosed with AD and MCI,\nincluding those with related dementia. Interviews were con-\nducted in the English language and were based on the descrip-\ntion of the Cookie-Theft picture component, which is part of\nthe Boston Diagnostic Aphasia Examination. During the in-\nterview, patients were given the picture and were told to dis-\ncuss everything they could see happening in the picture. The\npatients’ verbal utterances were recorded and then transcribed\ninto a transcription format with the equivalent text. For our\nexperiments, we selected all the available 19 transcripts of\nMCI and an equivalent 19 transcripts of healthy controls.\nBecause many existing research works on MCI have\nmostly used different handcrafted features from self-collected\ndatasets [Roark et al., 2011 ], it is challenging to compare\nthe D2NNLM with those works because we could not test\nD2NNLM on their datasets due to ethics constraints. Never-\ntheless, we compared our model to the DNNLM and NNLM\nas baselines on the same DementiaBank dataset. Our goal\nis to show the efﬁcacy of the deep language model and deep\nneural network technique to the MCI prediction problem.\n5.2 D2NN Language Models Settings\nWe generated the vocabulary for the D2NNLM from all the\n38 transcript ﬁles containing 210 sentences for MCI and 236\nsentences for healthy controls. Table 1 shows the details\nof the 6-gram and 1-skip-3-grams vocabularies, which were\ngenerated from the dataset. The D2NNLM training data con-\nsist of 50% of the MCI and the control groups’ transcript ﬁles,\nwhile the test and validation sets consist of 25% of the tran-\nscript ﬁles, respectively.\nThe 50% training data for the MCI group has 104 sen-\ntences, 663 6-grams, and 1659 1-skip-trigrams. On the\nother hand, the training data for the Control group has 131\nsentences, 700 6-grams, and 1793 1-skip-trigrams. Having\nstacked the discrete continuous vocabulary spaces together,\nwe performed SVD and used the decomposed left-singular\nmatrix from the SVD to map the vocabulary histories into a\n2http://www.nia.nih.gov/\nlower dimensional continuous parameter space for the neu-\nral network. The language model smoothing for words out-\nside the output vocabulary was performed as described in\n[Schwenk, 2007]. The D2NNLM was trained with three hid-\nden layers excluding the projection layer. In order to avoid\nthe risk of reconstructing the identity function [Larochelle et\nal., 2009], we performed a grid search and set the sizes of the\nhidden layers to 70% of the input neurons as the number of\nhidden units. Because the D2NNLM performs a classiﬁca-\ntion task by discriminating between the MCI and the Control\nclasses, we performed ﬁnetuning in the form of backpropaga-\ntion instead of pretraining [Hinton et al., 2012]. For the clas-\nsiﬁcation task, the network parameters are used to estimate\nthe likelihood that a vocabulary feature sequence belongs to\neither the MCI or Control classes. While our training tech-\nnique is mostly similar to DNNLM [Arisoy et al., 2012], we\nset the initial learning rate to 0.01, momentum to 0.1, and the\nweight decay to 0.01, respectively. Finally, we trained the\nnetwork to convergence.\nWe estimated the percentage Mean Square Error (MSE)\nusing Pybrain’s implementation of the neural network and\nvalidates on a held-out test set. The percentage error is\nused often to evaluate neural network models [Arisoy et al.,\n2012]. We also estimated the language model perplexity of\nthe D2NNLM in comparison to DNNLM and NNLM. In\nlanguage modeling, perplexity measures how well a model\npredicts given examples using an information theoretic ap-\nproach. A better model minimizes the perplexity. We com-\npute the perplexity as 2B(q) as follows:\nB(q) =−1\nN\nN∑\ni=1\nlog2q(xi) (5)\nPerplexity = 2B(q) (6)\nwhere B(q) estimates the ability of the model to predict\nsigniﬁcant portion of the test samples.\n5.3 Result Analysis\nAs shown in Table 2, we performed experiments by\ncomparing the percentage error and perplexity between\nthe D2NNLM, DNNLM, and NNLM. We compared the\nD2NNLM with DNNLM and NNLM because both models\nconsolidate neural network with language models. Note that\nprevious work have shown that NNLM and DNNLM outper-\nform the conventional 4-gram language models [Schwenk,\n2007; Arisoy et al., 2012 ]. Similarly, in [Schwenk, 2007 ],\nNNLM outperformed the 4-gram back-off model even when\nthe modiﬁed Kneser-Ney smoothing is used. While it\nmakes sense to compare the D2NNLM with the Hierarchical\nPitman-Yor language models (HPYLM) [Huang and Renals,\n2007] and the sequence memoizer [Wood et al., 2011], nei-\nther the original NNLM nor DNNLM has made such compar-\nison even with very large datasets. Given the relatively small\nsize of the MCI clinical data (i.e. 19 MCI and 19 Control),\nthe parameters for the Pitman-Yor process (prior distribution)\ncould be complex to optimize for the optimal performance of\nboth HPYLM and sequence memoizer, which are Bayesian\nModels (%) Error Perplexity\nD2NNLM\n(ngram=6, skip-gram=1-skip-trigram) 12.5 1.6\nDNNLM (ngram=4) 62.5 3.1\nNNLM (ngram=4) 37.5 2.6\nTable 2: Percentage error and perplexity on held-out test set\nmodels. Nevertheless, we consider this as part of our plan for\nfuture work on a large clinical dataset.\nAs discussed in Section 3, the three models have differ-\nent architectures but used the same learning settings. For\nthe 3 models, we used 70% of the input neurons as the num-\nber of hidden units for the hidden layers. The D2NNLM (3\nhidden-layers) takes a stacked combination of 6-grams and 1-\nskip-trigram as inputs, while DNNLM (3 hidden layers) and\nNNLM (1 hidden layer) take only 4-grams as performed in\n[Arisoy et al., 2012].\nIn Table 2, we see that the D2NNLM has a better percent-\nage error of 12.5% and a much lesser perplexity of 1.6, which\nis comparably better than the DNNLM and NNLM. Interest-\ningly, the single hidden layer NNLM showed a better per-\ncentage error and perplexity than the DNNLM with 3 hidden\nlayers. One possible explanation for this behavior is that the\nhigher number of hidden layers in the DNNLM does not nec-\nessarily correspond to improved performance especially on\nsmall datasets [Arisoy et al., 2012]. On the other hand, in-\ncreasing the feature dimension could lead to improved perfor-\nmance [Bengio et al., 2003], which is why we introduced the\nD2NNLM with a much deeper language model vocabulary\nspace by stacking together higher order n-gram and skip-\ngram features. We also observed that D2NNLM needed much\nlonger training iterations (943 epochs) to converge. This is\nunderstandable considering the fact that the number of train-\ning sample is only 9 per category.\nFigure 2 shows a comparison between the perplexities of\nthe three models while varying the number of hidden units at\ntheir respective hidden layer(s). Note that all the three mod-\nels used the same number of hidden units at each step. We\nsee that the D2NNLM consistently shows lower perplexity\nwith respect to the increasing number of hidden units and sta-\nbilizes at 70% of the input neurons, which is why we chose\n70% of the input neurons as the number of hidden units in the\nprevious experiment.\nIn Table 3, we show the improvements on the DNNLM and\nNNLM by using a much deeper language model vocabulary\nspace instead of the 4-gram language model used in [Arisoy\net al., 2012](see Table 2). We see that both the percentage er-\nror and perplexity reduced considerably by using an increased\nfeature dimension with 6-grams and 1-skip-trigrams. For\nDNNLM, a deeper vocabulary space reduced the percentage\nerror by 60% and perplexity by 42%. Similarly for NNLM,\nthe percentage error was reduced by 33% and perplexity by\n27%. This further conﬁrms the hypothesis that increased fea-\nture dimension could lead to improved network performance\n[Bengio et al., 2003].\nWe also performed experiments to investigate the contribu-\ntions of three different hidden layers (H = 2, 3, and 4) to both\nD2NNLM and DNNLM. Recall that NNLM is a single-layer\n0.2 0.4 0.6 0.8\n2 4 6 8 10 12 14\n% Hidden units from inputs\nPerplexity\nD2NNLMDNNLMNNLM\nFigure 2: Comparing perplexity on held-out test set with\nvarying hidden units\nModels (%) Error Perplexity\nDNNLM\n(ngram=6, skip-gram=1-skip-trigram) 25 1.8\nNNLM\n(ngram=6, skip-gram=1-skip-trigram) 25 1.9\nTable 3: Improvements on DNNLM and NNLM as a result of\ndeeper vocabulary space\nnetwork, hence we cannot increase its hidden layer because\ndoing so will make it a DNNLM. In addition, we included a\nthird D2NNLM-6-grams without skip-gram features because\nwe wanted to observe the impact of theskip-gram vocabulary\nspace. Figure 3 shows the perplexity plot against different\nhidden layers of the models. We see that the D2NNLM has\na much lower perplexity plot between 2 and 3 hidden layers\ncompared to D2NNLM and D2NNLM-6-grams, albeit with\nan increased perplexity at the fourth hidden layer. We ob-\nserved the optimal number of hidden layers to be 3 on the\nMCI dataset. DNNLM has a marginally better perplexity with\n2 hidden layers but performed poorly with increased perplex-\nity at the third and fourth layers. We observed the absence of\nthe skip-gram vocabulary space to have substantial effect on\nthe D2NNLM-6-grams with increased perplexity above the\nfull D2NNLM. We believe that a combination of higher order\nn-grams and skip-grams with a maximum of 3 hidden layers\nled to an improved classiﬁcation on the MCI dataset.\nTo put the performance of D2NNLM in clinical perspec-\ntive, we achieved 87.5% accuracy on the held-out test set by\ncomputing the ratio of the correctly predicted samples to the\ntotal number of test samples. The performance is substantial\nconsidering the small size of the data set. Moreover, both\nDNNLM and NNLM gave comparatively lower accuracy of\n62.5% on the test set, respectively. In comparison to the re-\nsults in [Roark et al., 2011], the D2NNLM is in a better po-\nsition with better predictive accuracy because of its low error\nrate. Although other evaluations could be performed to sub-\nstantiate the efﬁcacy of our model in real clinical scenarios,\nnevertheless, we believe that our experimental results sufﬁ-\n2.0 2.5 3.0 3.5 4.0\n1 2 3 4 5 6\nHidden layers\nPerplexity\nD2NNLMD2NNLM6gramDNNLM\nFigure 3: Comparing perplexity on held-out test set with\nvarying hidden units\nciently show the potential of the deep-deep neural network\nlanguage models for predicting MCI on very sparse clinical\ndataset.\n6 Discussion and Limitations\nThe use of higher order n-gram and skip-gram features in\nthis study is limited to the description of the Cookie-Theft\npicture in the DementiaBank clinical dataset. This is under-\nstandable since the objects within the picture dictate the spe-\nciﬁc n-gram and skip-gram features in the language space of\nthe MCI and control individuals. Unless a picture with sim-\nilar objects in the Cookie-Theft picture is used for collecting\nthe speech transcript, the use of any other picture with differ-\nent objects is likely to generate a different set of n-gram and\nskip-gram features.\nWe believe that D2NNLM could be effective for predict-\ning other language and cognitive impairment related diseases\nsuch as Aphasia, Autism spectrum disorder, and Parkinson’s\ndisease. For example, linguistic defects could be more pro-\nnounced in Aphasia patients because Aphasia mainly affects\nlanguage. Thus, if D2NNLM can be sensitive to predict MCI,\nit could as well predict other pronounced language impair-\nments.\nAlso, our current work has not optimized D2NNLM to per-\nform as a general language model. We focus on MCI, and the\npossibility of predicting MCI on very sparse clinical dataset.\nNevertheless, we believe that future work could validate the\nD2NNLM on other natural language problems such as senti-\nment analysis.\nFinally, although the dataset used in this study is small,\nwe have used the DementiaBank dataset, which is the largest\nand publicly available clinical dataset on MCI to date. Most\nclinical datasets on MCI are self-collected over a short period\nof time and are mostly not publicly available largely due to\nethical constraints. We are currently conducting a longitudi-\nnal study to collect large speech samples from several MCI\npatients across two continents.\n7 Conclusion and Future work\nIn this paper, we proposed the combination of deep neural\nnetwork and deep language models to predict MCI from clini-\ncal language dataset. We learned deep language models using\nhigher order n-gram and skip-gram vocabulary spaces. Ex-\nperimental results show that the model predicts MCI with less\npercentage error and language model perplexity. In the fu-\nture, we will consider D2NNLM on other language impaired\nrelated diseases such as Aphasia and Parkinson’s disease. We\nwill also conduct further evaluations that could put our model\nto use on large speech samples in actual clinical scenarios.\nAcknowledgments\nThis work was supported by the Malaysian Ministry of Edu-\ncation via the Fundamental Research Grant Scheme (FRGS)\n- FRGS/2/2014/ICT07/MUSM/03/1.\nReferences\n[Abbott, 2011] Alison Abbott. Dementia: a problem for our\nage. Nature, 475(7355):S2–S4, 2011.\n[Albert et al., 2011] Marilyn S Albert, Steven T DeKosky,\nDennis Dickson, Bruno Dubois, Howard H Feldman,\nNick C Fox, Anthony Gamst, David M Holtzman,\nWilliam J Jagust, Ronald C Petersen, et al. The diagnosis\nof mild cognitive impairment due to alzheimers disease:\nRecommendations from the national institute on aging-\nalzheimers association workgroups on diagnostic guide-\nlines for alzheimer’s disease. Alzheimer’s & Dementia,\n7(3):270–279, 2011.\n[Arisoy et al., 2012] Ebru Arisoy, Tara N Sainath, Brian\nKingsbury, and Bhuvana Ramabhadran. Deep neural net-\nwork language models. InProceedings of the NAACL-HLT\n2012 Workshop: Will We Ever Really Replace the N-gram\nModel? On the Future of Language Modeling for HLT,\npages 20–28. Association for Computational Linguistics,\n2012.\n[Ball et al., 2009] Martin J Ball, Michael R Perkins, Nicole\nM¨uller, and Sara Howard. The handbook of clinical lin-\nguistics: vol 56. John Wiley & Sons, United States, 2009.\n[Bengio et al., 2003] Yoshua Bengio, R ´ejean Ducharme,\nPascal Vincent, and Christian Janvin. A neural probabilis-\ntic language model. The Journal of Machine Learning Re-\nsearch, 3:1137–1155, 2003.\n[Chen and Chu, 2010] Stanley F Chen and Stephen M Chu.\nEnhanced word classing for model m. In INTERSPEECH,\npages 1037–1040, 2010.\n[Damian et al., 2011] Anne M Damian, Sandra A Jacobson,\nJoseph G Hentz, Christine M Belden, Holly A Shill, Mar-\nwan N Sabbagh, John N Caviness, and Charles H Adler.\nThe montreal cognitive assessment and the mini-mental\nstate examination as screening instruments for cognitive\nimpairment: item analyses and threshold scores.Dementia\nand Geriatric Cognitive Disorders, 31(2):126–131, 2011.\n[Fraser et al., 2014] Kathleen C Fraser, Jed A Meltzer,\nNaida L Graham, Carol Leonard, Graeme Hirst, Sandra E\nBlack, and Elizabeth Rochon. Automated classiﬁcation\nof primary progressive aphasia subtypes from narrative\nspeech transcripts. Cortex, 55:43–60, 2014.\n[Hinton et al., 2012] Geoffrey Hinton, Li Deng, Dong Yu,\nGeorge E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,\nAndrew Senior, Vincent Vanhoucke, Patrick Nguyen,\nTara N Sainath, et al. Deep neural networks for acous-\ntic modeling in speech recognition: The shared views of\nfour research groups. Signal Processing Magazine, IEEE,\n29(6):82–97, 2012.\n[Huang and Renals, 2007] Songfang Huang and Steve Re-\nnals. Hierarchical pitman-yor language models for asr\nin meetings. In Automatic Speech Recognition & Under-\nstanding, 2007. ASRU. IEEE Workshop on, pages 124–\n129. IEEE, 2007.\n[Kim and Caine, 2014] Scott YH Kim and Eric D Caine.\nUtility and limits of the mini mental state examination in\nevaluating consent capacity in alzheimer’s disease. Psy-\nchiatric Services, 2014.\n[Larochelle et al., 2009] Hugo Larochelle, Yoshua Bengio,\nJ´erˆome Louradour, and Pascal Lamblin. Exploring strate-\ngies for training deep neural networks.The Journal of Ma-\nchine Learning Research, 10:1–40, 2009.\n[Le et al., 2011] Xuan Le, Ian Lancashire, Graeme Hirst, and\nRegina Jokel. Longitudinal detection of dementia through\nlexical and syntactic changes in writing: a case study of\nthree british novelists. Literary and Linguistic Computing,\npage fqr013, 2011.\n[Li et al., 2015] Yifeng Li, Chih-Yu Chen, and Wyeth W\nWasserman. Deep feature selection: Theory and ap-\nplication to identify enhancers and promoters. In Re-\nsearch in Computational Molecular Biology, pages 205–\n217. Springer, 2015.\n[Locke, 1997] John L Locke. A theory of neurolinguistic de-\nvelopment. Brain and Language, 58(2):265–326, 1997.\n[Mitchell, 2009] Alex J Mitchell. A meta-analysis of the ac-\ncuracy of the mini-mental state examination in the detec-\ntion of dementia and mild cognitive impairment. Journal\nof Psychiatric Research, 43(4):411–431, 2009.\n[Mitolo et al., 2015] Micaela Mitolo, Simona Gardini,\nPaolo Caffarra, Lucia Ronconi, Annalena Venneri, and\nFrancesca Pazzaglia. Relationship between spatial ability,\nvisuospatial working memory and self-assessed spatial\norientation ability: a study in older adults. Cognitive\nProcessing, 16(2):165–176, 2015.\n[Orimaye et al., 2015] Sylvester Olubolu Orimaye, Kah Yee\nTai, Jojo Sze-Meng Wong, and Chee Piau Wong. Learn-\ning linguistic biomarkers for predicting mild cognitive im-\npairment using compound skip-grams. arXiv preprint\narXiv:1511.02436, 2015.\n[Pozueta et al., 2011] Ana Pozueta, Eloy Rodr ´ıguez-\nRodr´ıguez, Jos ´e L Vazquez-Higuera, Ignacio Mateo,\nPascual S ´anchez-Juan, Soraya Gonz ´alez-Perez, Jos ´e\nBerciano, and Onofre Combarros. Detection of early\nalzheimer’s disease in mci patients by the combination\nof mmse and an episodic memory test. BMC Neurology,\n11(1):78, 2011.\n[Prud’hommeaux and Roark, 2015] Emily Prud’hommeaux\nand Brian Roark. Graph-based word alignment for clinical\nlanguage evaluation. Computational Linguistics, 2015.\n[Roark et al., 2011] Brian Roark, Margaret Mitchell, John-\nPaul Hosom, Kristy Hollingshead, and Jeffrey Kaye. Spo-\nken language derived measures for detecting mild cogni-\ntive impairment. Audio, Speech, and Language Process-\ning, IEEE Transactions on, 19(7):2081–2090, 2011.\n[Sarikaya et al., 2009] Ruhi Sarikaya, Mohamed Aﬁfy, and\nBrian Kingsbury. Tied-mixture language modeling in con-\ntinuous space. In Proceedings of Human Language Tech-\nnologies: The 2009 Annual Conference of the North Amer-\nican Chapter of the Association for Computational Lin-\nguistics, pages 459–467. Association for Computational\nLinguistics, 2009.\n[Schwenk, 2007] Holger Schwenk. Continuous space lan-\nguage models. Computer Speech & Language, 21(3):492–\n518, 2007.\n[Sidorov et al., 2014] Grigori Sidorov, Francisco Velasquez,\nEfstathios Stamatatos, Alexander Gelbukh, and Liliana\nChanona-Hern´andez. Syntactic n-grams as machine learn-\ning features for natural language processing. Expert Sys-\ntems with Applications, 41(3):853–860, 2014.\n[Tillas, 2015] Alexandros Tillas. Language as grist to the\nmill of cognition. Cognitive Processing, pages 1–25, 2015.\n[Wood et al., 2011] Frank Wood, Jan Gasthaus, C ´edric Ar-\nchambeau, Lancelot James, and Yee Whye Teh. The\nsequence memoizer. Communications of the ACM ,\n54(2):91–98, 2011.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.671362042427063
    },
    {
      "name": "Artificial neural network",
      "score": 0.5669522881507874
    },
    {
      "name": "Cognition",
      "score": 0.5553704500198364
    },
    {
      "name": "Artificial intelligence",
      "score": 0.517844021320343
    },
    {
      "name": "Cognitive impairment",
      "score": 0.49750521779060364
    },
    {
      "name": "Deep learning",
      "score": 0.4964204430580139
    },
    {
      "name": "Natural language processing",
      "score": 0.44899576902389526
    },
    {
      "name": "Deep neural networks",
      "score": 0.42888736724853516
    },
    {
      "name": "Machine learning",
      "score": 0.3400474190711975
    },
    {
      "name": "Psychology",
      "score": 0.19685333967208862
    },
    {
      "name": "Neuroscience",
      "score": 0.1349346935749054
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I11662577",
      "name": "Monash University Malaysia",
      "country": "MY"
    },
    {
      "id": "https://openalex.org/I56590836",
      "name": "Monash University",
      "country": "AU"
    }
  ]
}