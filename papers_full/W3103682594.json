{
  "title": "Long Range Arena: A Benchmark for Efficient Transformers",
  "url": "https://openalex.org/W3103682594",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222427123",
      "name": "Tay, Yi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2273124145",
      "name": "Dehghani, Mostafa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286755007",
      "name": "Abnar, Samira",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282814475",
      "name": "Shen, Yikang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222578404",
      "name": "Bahri, Dara",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288646555",
      "name": "Pham, Philip",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227174931",
      "name": "Rao, Jinfeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1983143503",
      "name": "Yang Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202039502",
      "name": "Ruder, Sebastian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3177605537",
      "name": "Metzler, Donald",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2787214294",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W3021293129",
    "https://openalex.org/W3006983028",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W2998108143",
    "https://openalex.org/W2911997761",
    "https://openalex.org/W2952136670",
    "https://openalex.org/W3091156754",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W3034561418",
    "https://openalex.org/W3033943443",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3087547017",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2953958347",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W2796705611",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2952125979",
    "https://openalex.org/W2964176953",
    "https://openalex.org/W2060277403",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W2178628967",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2970831648",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3019932981",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2536015822",
    "https://openalex.org/W2963015836",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2963866616",
    "https://openalex.org/W2997517014",
    "https://openalex.org/W3016697633",
    "https://openalex.org/W3007672467"
  ],
  "abstract": "Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. To this date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. This paper proposes a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. Our benchmark is a suite of tasks consisting of sequences ranging from $1K$ to $16K$ tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on our newly proposed benchmark suite. LRA paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle. Our benchmark code will be released at https://github.com/google-research/long-range-arena.",
  "full_text": "Preprint\nLONG RANGE ARENA : A BENCHMARK FOR EFFICIENT\nTRANSFORMERS\nYi Tay1∗, Mostafa Dehghani1∗, Samira Abnar1, Yikang Shen1, Dara Bahri1, Philip Pham1\nJinfeng Rao1, Liu Yang1, Sebastian Ruder2, Donald Metzler1\n1Google Research\n2Google DeepMind\n{yitay, dehghani}@google.com\nABSTRACT\nTransformers do not scale very well to long sequence lengths largely because\nof quadratic self-attention complexity. In the recent months, a wide spectrum\nof efﬁcient, fast Transformers have been proposed to tackle this problem, more\noften than not claiming superior or comparable model quality to vanilla Trans-\nformer models. To this date, there is no well-established consensus on how to\nevaluate this class of models. Moreover, inconsistent benchmarking on a wide\nspectrum of tasks and datasets makes it difﬁcult to assess relative model qual-\nity amongst many models. This paper proposes a systematic and uniﬁed bench-\nmark, Long-Range Arena, speciﬁcally focused on evaluating model quality un-\nder long-context scenarios. Our benchmark is a suite of tasks consisting of se-\nquences ranging from 1K to 16K tokens, encompassing a wide range of data\ntypes and modalities such as text, natural, synthetic images, and mathematical ex-\npressions requiring similarity, structural, and visual-spatial reasoning. We system-\natically evaluate ten well-established long-range Transformer models (Reformers,\nLinformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesiz-\ners, Sparse Transformers, and Longformers) on our newly proposed benchmark\nsuite. Long-Range Arena paves the way towards better understanding this class\nof efﬁcient Transformer models, facilitates more research in this direction, and\npresents new challenging tasks to tackle. Our benchmark code will be released at\nhttps://github.com/google-research/long-range-arena .\n1 I NTRODUCTION\nTransformers (Vaswani et al., 2017) are ubiquitously state-of-the-art across many modalities, from\nlanguage (Devlin et al., 2018; Raffel et al., 2019; Child et al., 2019) to images (Tan & Bansal, 2019;\nLu et al., 2019) to protein sequences (Rives et al., 2019). A common weakness of Transformers is\ntheir quadratic memory complexity within the self-attention mechanism that restricts their potential\napplication to domains requiring longer sequence lengths. To date, a dizzying number of efﬁcient\nTransformer models (‘xformers’) have been proposed to tackle this problem (Liu et al., 2018; Kitaev\net al., 2020; Wang et al., 2020; Tay et al., 2020b; Katharopoulos et al., 2020). Many of these models\ndemonstrate comparable performance to the vanilla Transformer model while successfully reducing\nthe memory complexity of the self-attention mechanism. An overview of this research area can be\nfound in (Tay et al., 2020c).\nComparing the evaluation and experimental setup of many of these papers, we can make the follow-\ning observations. Firstly, there is no unifying consensus on what makes an acceptable test bed for\nbenchmarking efﬁcient Transformers. There is also a large diversity in the types of tasks adopted—\nevery single model is evaluated on a different set of tasks and datasets, which makes comparison of\ndifferent models as well as an assessment of their relative strengths and weaknesses difﬁcult. Sec-\nondly, the benchmarks used for evaluation are often arbitrarily chosen, without much consideration\nto whether the task is suitable for evaluating long-range modeling. Thirdly, many papers tend to\nconﬂate the effectiveness of the inductive bias with the beneﬁts of pretraining (Ainslie et al., 2020;\n∗First two authors contributed equally.\n1\narXiv:2011.04006v1  [cs.LG]  8 Nov 2020\nPreprint\nZaheer et al., 2020; Wang et al., 2020), which tends to obfuscate the true value of the architecture.\nPretraining itself is a computationally expensive endeavour and de-coupling inductive bias research\nfrom pretraining would make xformer research more accessible.\nIn this paper, we propose a new benchmark, Long-Range Arena(LRA), for the purpose of bench-\nmarking sequence models under the long-context scenario. We design a benchmark suite com-\nprised of both synthetic probing tasks and real-world tasks and provide relative comparisons for\nten recently proposed efﬁcient Transformer models including Sparse Transformers (Child et al.,\n2019), Reformer (Kitaev et al., 2020), Linformer (Wang et al., 2020), Longformer (Beltagy et al.,\n2020), Sinkhorn Transformers (Tay et al., 2020b), Performers (Choromanski et al., 2020b), Synthe-\nsizers (Tay et al., 2020a), Linear Transformers (Katharopoulos et al., 2020), and BigBird (Zaheer\net al., 2020). This is the most comprehensive and extensives side-by-side evaluation of this class of\nmodels.\nWhile the focus of this benchmark is the ability of these architectures to reason in long-context sce-\nnarios, we are also fundamentally interested in understanding the capabilities and properties of these\nxformer architectures when exposed to different types of data and conditions. Hence, our benchmark\nis purposefully designed to be capability probing, i.e, we select datasets and tasks with certain innate\nstructure. For example, can these architectures model long sequences that are intrinsically hierar-\nchical or that contain some form of spatial structure? In general, we are especially interested in the\nrelative performance of these xformer models across diverse circumstances. We hope that under-\nstanding these better will inspire research on more efﬁcient architectures in the future. While the\nfocus of this paper is on efﬁcient Transformer models, our benchmark is also model agnostic and\ncan also serve as a benchmark for long-range sequence modeling.\nAside from comparing the quality of these models, we also conduct extensive efﬁciency and memory\nusage analysis of these models. We believe such a side-by-side performance benchmark will be\nvaluable to the community, providing deeper insight on the practical efﬁciency of these methods.\nOverall, we propose a uniﬁed framework for enabling easy side-by-side comparisons of efﬁcient\nTransformer models and broadly speaking, long-range sequence models in general. Our framework,\nwhich we open source, is written in JAX/FLAX1.\n2 L ONG -RANGE ARENA (LRA)\nThis section introduces the Long-Range Arena (LRA) benchmark (pronounced el-ra). We imple-\nment our benchmark (which includes the task, evaluators, and models) in Python 3 and Jax/Flax and\nopen-source our code2—making it easy to extend and to build on top of our work.\n2.1 D ESIDERATA\nFor creating the Long-Range Arena benchmark, we established a set of desiderata:\n1. Generality: All efﬁcient Transformers models should be applicable to our tasks. For instance,\ngiven that not all xformer models are able to perform autoregressive decoding (Wang et al.,\n2020), we include tasks that only require encoding.\n2. Simplicity: The tasks should have a simple setup. All factors that make comparisons dif-\nﬁcult should be removed. This encourages simple models instead of cumbersome pipelined\napproaches. For instance, we avoid including any particular data augmentation and consider\npretraining to be out of scope of this benchmark.\n3. Challenging: The tasks should be difﬁcult enough for current models to ensure there is room\nfor improvement to encourage future research in this direction.\n4. Long inputs: The input sequence lengths should be reasonably long since assessing how dif-\nferent models capture long-range dependencies is a core focus of LRA.\n5. Probing diverse aspects : The set of tasks should assess different capabilities of models like\ntheir ability to model relations and hierarchical/spatial structures, generalization capability, etc.\n1https://github.com/google/flax\n2https://github.com/google-research/long-range-arena\n2\nPreprint\n6. Non-resource intensive and accessible: The benchmarks should be deliberately designed to be\nlightweight so as to be accessible to researchers without industry-grade computing resources.\n2.2 T ASKS\nThis section describes the tasks in the LRA benchmark. Note that these tasks are speciﬁcally de-\nsigned for the purpose of assessing different aspects of efﬁcient Transformer models. Further details\nabout each task can be found in the appendix.\n2.2.1 L ONG LIST OPS\nIn this task, we are interested in the capability of modeling hierarchically structured data in a long-\ncontext scenario. This benchmark task is a longer variation of the standard ListOps task proposed\nin (Nangia & Bowman, 2018), which was designed to investigate the parsing ability of neural mod-\nels.\nThe dataset is comprised of sequences with a hierarchical structure and operators MAX , MEAN ,\nMEDIAN and SUM MOD that are enclosed by delimiters (brackets). An example (much shorter)\nsequence is as follows:\nINPUT: [MAX 4 3 [MIN 2 3 ] 1 0 [MEDIAN 1 5 8 9, 2]] OUTPUT: 5\nIn our task we use a version of ListOps of sequence lengths of up to 2K to test the ability to reason\nhierarchically while handling long contexts. Naturally, in the above example the model needs to\naccess all tokens and model the logical structure of the inputs in order to make a prediction. The\ntask is a ten-way classiﬁcation task and is considerably challenging.\n2.2.2 B YTE -LEVEL TEXT CLASSIFICATION\nThis task using real-world data represents a common use case of efﬁcient Transformers, which\nare often needed to process long documents. Text classiﬁcation in particular is associated with\nmany real-world applications such as spam, fraud, and bot detection and commercial document\nclassiﬁcation, among others (Howard & Ruder, 2018).\nThis task also benchmarks the ability of the models to deal with compositionality as it is required to\ncompose characters into words into higher-level phrases. Compared to ListOps, boundaries are less\nwell deﬁned and need to be learned from the data, which is a challenging problem in its own right\n(Kawakami et al., 2019).\nWe consider the byte/character-level setup of this task in order to simulate a longer input sequence,\nwhich also makes the task considerably more challenging.3 Note that this setup differs signiﬁcantly\nfrom character-level language modeling (char LM). In char LM, it would sufﬁce to read nearby\ncontext to determine the next character, e.g., a model is very likely to predict ‘e’after having seen\nthe preﬁx ‘appl’. For byte-level text classiﬁcation, the model needs to reason with compositional,\nunsegmented data in order to solve a meaningful real-world task. We use the IMDb reviews (Maas\net al., 2011) dataset, which is a commonly used dataset to benchmark document classiﬁcation. We\nuse a ﬁxed max length of 4K for this task, which is truncated or padded when necessary. This is a\nbinary classiﬁcation task with accuracy as the metric.\n2.2.3 B YTE -LEVEL DOCUMENT RETRIEVAL\nWe further evaluate a model’s ability to encode and store compressed representations that are useful\nfor matching and retrieval. Learning the similarity score between two vectors is a common problem\nin machine learning and is useful for a wide array of applications (Guo et al., 2016).\nHence, this task is mainly about modeling a similarity score between two documents in a ‘two tower\nsetup’ in which compressed representations are concatenated and passed into a linear classiﬁer. Note\nthat we deliberately prevent models from using cross attention. This task thus serves as a test of how\n3On the IMDb word-level task, models without pre-training achieve accuracies in the high 80s while the\nsame models score in the mid 60s on the character-level task (Tay et al., 2020b).\n3\nPreprint\nwell models are able to compress long sequences into representations suitable for similarity-based\nmatching.\nWe use the ACL Anthology Network (AAN; Radev et al., 2013) dataset, which identiﬁes if two\npapers have a citation link, a common setup used in long-form document matching (Jiang et al.,\n2019; Yang et al., 2020).\nSimilar to the text classiﬁcation setup, we use a byte/character level setup, which challenges the\nmodel to compose and aggregate information over longer contexts. We use a sequence length of4K\nfor each document, which makes the total text length8K for this task. This is a binary classiﬁcation\ntask with accuracy as the metric.\n2.2.4 I MAGE CLASSIFICATION ON SEQUENCES OF PIXELS\nThis task is an image classiﬁcation task, where the inputs are sequences of pixels. In other words,\nan N ×N image is ﬂattened to a sequence of length N2 pixels.\nSimilar to how the previous tasks require capturing the hierarchical structure in the data, this task\nrequires the model to learn the 2D spatial relations between input pixels, while presented as a 1D\nsequence of symbols.\nWe focus on assessing Transformer models that are designed to process a sequence of discrete\nsymbols, so we do not allow extra modules such as a CNN stem that embeds pixel-level inputs.\nTo simplify the setup, we map the input images to a single gray-scale channel where each pixel is\nrepresented with an 8-bit pixel intensity (vocabulary size of 256). In LRA, we use the CIFAR-10\ndataset (Krizhevsky, 2009) for the image classiﬁcation task.\n2.2.5 P ATHFINDER (LONG -RANGE SPATIAL DEPENDENCY )\n(a) A positive example.\n(b) A negative example.\nFigure 1: Samples of the Pathﬁnder\ntask.\nThe Pathﬁnder challenge (Linsley et al., 2018; Kim* et al.,\n2020) was ﬁrst introduced for learning long-range spatial\ndependencies. It is a synthetic visual task motivated by cog-\nnitive psychology (Houtkamp & Roelfsema, 2010).\nThe task requires a model to make a binary decision\nwhether two points represented as circles are connected by\na path consisting of dashes. We show a positive example of\ntwo connected points and a negative example of two uncon-\nnected points in Figure 1. The dataset also contains distrac-\ntor paths, which makes this setup challenging. We model\nthis task by treating images as sequences of pixels. In this\ntask, images are of dimensions (32 ×32), which make up a\nsequence length of 1024.\n2.2.6 P ATHFINDER -X (L ONG -RANGE\nSPATIAL DEPENDENCIES WITH EXTREME LENGTHS )\nFinally, we consider an extreme version of Pathﬁnder\n(Pathﬁnder-X) where examples consist of 16K pixels (i.e.,\nimages of 128 ×128).\nThe key goal here is to observe if a model would fail to\nsolve the 16K extreme version even if it can successfully\nlearn the standard version of 1024 tokens. This is an inter-\nesting litmus test to see if the same algorithmic challenges\nbear a different extent of difﬁculty when sequence lengths\nare much longer. We include this in our benchmark as Path-\nX.\n4\nPreprint\n2.3 R EQUIRED ATTENTION SPAN OF LRA TASKS\nOne of the main goals of the LRA benchmark is assessing the ability of different efﬁcient Trans-\nformer models to capture long-range dependencies. The tasks and setups are designed with this goal\nin mind. In order to have a quantitative estimate of the spatial extent needed to be considered by an\nattention mechanism to encode the inputs, we deﬁne required attention span.\nGiven a trained attention-based model and a sequence of tokens as inputs, the required attention span\nof an attention module is computed as the mean distance between the query token and the attended\ntokens, scaled by attention weights. Here, we compute the mean required attention spanover all\nattention modules in our best vanilla Transformer model for each task, averaged over 1K random\nsamples from the validation set.\nFigure 2: Required attention span on different tasks.\nFigure 2 summarizes the required attention span for each task in LRA. For all the tasks in LRA\nthe required attention span is rather high. This shows, a Transformer model needs to go beyond\ncombining only local information, while in many other tasks and datasets, attention mechanism\nmostly need to combine information from neighboring positions.\nGiven the purpose of LRA, we foundrequired attention spanserves as a good proxy for how difﬁcult\na task is for Transformer-based models.4\n3 E XPERIMENTAL RESULTS\n3.1 M ODELS\nThis section describes the models we evaluate on our LRA benchmark. We base our evaluation\non ten recently proposed efﬁcient Transformer models. Aside from the standard vanilla Trans-\nformer (Vaswani et al., 2017) and a simple local attention baseline, we compare Sparse Trans-\nformers (Child et al., 2019), Longformers (Beltagy et al., 2020), Linformers (Wang et al., 2020),\nReformers (Kitaev et al., 2020), Sinkhorn Transformers (Tay et al., 2020b), Synthesizers (Tay et al.,\n2020a), BigBird (Zaheer et al., 2020), Linear Transformers (Katharopoulos et al., 2020), and Per-\nformers (Choromanski et al., 2020a).\nWe believe these ten models to represent a diverse cross-section of recent efﬁcient Transformer\nmodels.\n3.2 P HILOSOPHY BEHIND THE BENCHMARK\nWe note that it is non-trivial and almost impossible to conduct a perfectly fair evaluation of all\nmodels. The large search space motivates us to follow a set of ﬁxed hyperparameters (number of\n4Note that this metric mainly provides an indication of the required attention span for a task and the relative\ndifferences between tasks based on a reasonably strong model; a better model might only need to attend to\nshorter ranges (Daniluk et al., 2017; Rae & Razavi, 2020).\n5\nPreprint\nModel ListOps Text Retrieval Image Pathﬁnder Path-X Avg\nTransformer 36.37 64.27 57.46 42.44 71.40 FAIL 54.39\nLocal Attention 15.82 52.98 53.39 41.46 66.63 FAIL 46.06\nSparse Trans. 17.07 63.58 59.59 44.24 71.71 FAIL 51.24\nLongformer 35.63 62.85 56.89 42.22 69.71 FAIL 53.46\nLinformer 35.70 53.94 52.27 38.56 76.34 FAIL 51.36\nReformer 37.27 56.10 53.40 38.07 68.50 FAIL 50.67\nSinkhorn Trans. 33.67 61.20 53.83 41.23 67.45 FAIL 51.39\nSynthesizer 36.99 61.68 54.67 41.61 69.45 FAIL 52.88\nBigBird 36.05 64.02 59.29 40.83 74.87 FAIL 55.01\nLinear Trans. 16.13 65.90 53.09 42.34 75.30 FAIL 50.55\nPerformer 18.01 65.40 53.82 42.77 77.05 FAIL 51.41\nTask Avg (Std) 29 (9.7) 61 (4.6) 55 (2.6) 41 (1.8) 72 (3.7) FAIL 52 (2.4)\nTable 1: Experimental results on Long-Range Arena benchmark. Best model is in boldface and sec-\nond best is underlined. All models do not learn anything on Path-X task, contrary to the Pathﬁnder\ntask and this is denoted by FAIL. This shows that increasing the sequence length can cause seriously\ndifﬁculties for model training. We leave Path-X on this benchmark for future challengers but do not\ninclude it on the Average score as it has no impact on relative performance.\nlayers, heads, embedding dimensions, etc.) for all models. The best performance and relative order\nof the models may change if we aggressively tune hyperparameters for all models. Hence, the results\nprovided in this paper are not meant to be a ﬁnal authoritative document on which xformer is the\nbest. Instead, we provide a starting point for future research and strive to be as fair as possible.\nIn order to do so, we plan to release the code with all the hyperparameters and implementation\ndetails. Additionally, we intend for our paper to be a living document and encourage researchers\n(authors and the broader community) to contribute and continue updating this paper (with rules and\nlimitations described in the appendix). We also implemented all models to the best of our abilities.\nWe often consulted with the original developers of the included models.\n3.3 Q UANTITATIVE RESULTS\nBased on our results, we observe that (1) all proposed tasks in LRA are considerably challenging\nand (2) there are meaningful differences in model performance across different xformer models.\nResults on ListOps The ListOps task (10-way classiﬁcation) has proven to be reasonably difﬁ-\ncult with the best models obtaining only 37%. The considerable gap to random chance shows that\nmodels are indeed learning the task. We notice that the inductive bias of the xformer model plays a\nsubstantial role on this task in which approximately half the xformer models are able to get > 30%\nperformance while the remainder of the models only get slightly above random chance. This may\nimply that certain efﬁciency-inspired inductive biases may be better at handling hierarchical data\nthan others. For instance, the results from our experiments seem to suggest that kernel-based mod-\nels (e.g., Performer, Linear Transformers) are possibly not as effective on hierarchically structured\ndata.\nResults on Text Classiﬁcation Byte-level classiﬁcation is shown to be difﬁcult and challenging\nespecially when no pretraining or contextual embeddings are used. The best model only obtains\n65.90 accuracy. The Linear Transformer performs well on this task, along with the Performer model.\nContrary to the ListOps task, it seems like fast kernel-based models do well on this task.\nResults on Retrieval The scores of different models on this task are also rather low (average of\n55%), indicating the difﬁculty of the task. The vanilla Transformer model only achieves 57.46%\naccuracy with some xformer variants scoring very close to random chance. The best performing\nmodel is the Sparse Transformer and the second best is BigBird. We ﬁnd that models that follow\nﬁxed sparse patterns to do well on this task. Models that are based on low-rank factorization and\nkernels perform relatively worse.\n6\nPreprint\nSteps per second Peak Memory Usage (GB)\nModel 1K 2K 3K 4K 1K 2K 3K 4K\nTransformer 8.1 4.9 2.3 1.4 0.85 2.65 5.51 9.48\nLocal Attention 9.2 (1.1x) 8.4 ( 1.7x) 7.4 ( 3.2x) 7.4 ( 5.3x) 0.42 0.76 1.06 1.37\nLinformer 9.3 (1.2x) 9.1 ( 1.9x) 8.5 ( 3.7x) 7.7 ( 5.5x) 0.37 0.55 0.99 0.99\nReformer 4.4 (0.5x) 2.2 ( 0.4x) 1.5 ( 0.7x) 1.1 ( 0.8x) 0.48 0.99 1.53 2.28\nSinkhorn Trans 9.1 (1.1x) 7.9 ( 1.6x) 6.6 ( 2.9x) 5.3 ( 3.8x) 0.47 0.83 1.13 1.48\nSynthesizer 8.7 (1.1x) 5.7 ( 1.2x) 6.6 ( 2.9x) 1.9 ( 1.4x) 0.65 1.98 4.09 6.99\nBigBird 7.4 (0.9x) 3.9 ( 0.8x) 2.7 ( 1.2x) 1.5 ( 1.1x) 0.77 1.49 2.18 2.88\nLinear Trans. 9.1 (1.1x) 9.3 (1.9x) 8.6 (3.7x) 7.8 (5.6x) 0.37 0.57 0.80 1.03\nPerformer 9.5 (1.2x) 9.4 (1.9x) 8.7 (3.8x) 8.0 (5.7x) 0.37 0.59 0.82 1.06\nTable 2: Benchmark results of all Xformer models with a consistent batch size of 32 across all\nmodels. We report relative speed increase/decrease in comparison with the vanilla Transformer in\nbrackets besides the steps per second. Memory usage refers to per device memory usage across each\nTPU device. Benchmarks are run on 4x4 TPU V3 Chips.\nResults on Image Classiﬁcation On the image classiﬁcation task, most models perform quite\nsimilarly (low variance amongst model performance). The best model on this task is the Sparse\nTransformer, followed by the Performer. Linformer and Reformers do not do well on this task. On\na related note, we also observed most of models struggle generalizing to the test even though they\nmanage to overﬁt the training set. While we extensively tried different regularization techniques on\nevery single model, there is a rather large gap between their performance on train and test set (More\ndetails in Appendix).\nResults on Pathﬁnder / Path-X Results show that all models achieve reasonable performance on\nthe Pathﬁnder task. The average performance is 72 and the best model Performer obtains 77.05%\naccuracy. The Local Attention model performs the worse out of all models.\nAll models failed to solve the Path-X task, achieving at best 50%. We ﬁnd this intriguing because\nthis is essentially an identical task to the standard Pathﬁnder, albeit with much longer sequence\nlengths. Hence, we observe that the extreme length of the task can signiﬁcantly obstruct a model\nfrom leaning anything meaningful. We leave Path-X in our benchmark suite, hoping to spur future\nprogress in modeling sequences at extreme lengths.\n3.4 E FFICIENCY BENCHMARKS\nIn this section, we report efﬁciency metrics of our runs. For simplicity, we use the byte-level text\nclassiﬁcation benchmark and report run times and memory consumption of the sequence lengths\n{1K, 2K, 3K, 4K}. We use a batch size of32 for all runs and conduct experiments on 4x4 TPU V3\nChips. We emphasize that this is again largely conditioned on hardware and implementation details\n(more details can be found in the appendix).\nResults on Speed Table 2 reports our efﬁciency benchmarks on the xformer models. We note\nthat low-rank and kernel-based models are generally the fastest. The overall fastest model is the\nPerformer model (Choromanski et al., 2020a), which is 5.7 ×faster than Transformers on the 4k\nsequence length. Linformer (Wang et al., 2020) and Linear Transformers (Katharopoulos et al.,\n2020) come in a close second and are almost as fast as Performers (at 5.5 to 5.6×faster). Based on\nour implementation, the slowest model is the Reformer model (Kitaev et al., 2020) that is about80%\nthe speed of vanilla Transformer at 4K sequence lengths and half the speed at 1K sequence length.\nResults on Memory Consumption The model with the smallest memory footprint in our bench-\nmarks is the Linformer model, coming in at 0.99GB per TPU device as compared to 9.48GB per\nTPU device for the vanilla Transformers at N = 4K. That is about a 10x reduction in memory\nfootprint. Similar to speed, Performers and Linear Transformers are also relatively compact and are\nalmost as compact as Linformers. Other models (Local Attention, Reformers, BigBird, Synthesiz-\ners) are still less memory hungry compared to vanilla Transformers but are relatively less efﬁcient\n7\nPreprint\n(memory consumption wise) compared to Linformers, Performers, and Linear Transformers. We\nalso notice that the memory consumption of models such as Linformer and Performer scales very\nwell, with the memory usgae at 3K and 4K being approximately equal.\n3.5 O VERALL RESULTS : N O ONE-SIZE -FITS -ALL\nBased on our analysis, the best qualitative performance in terms of LRA score, i.e. integrated\nacross all ﬁve tasks, is the BigBird model. While BigBird does not do extremely well on any\nindividual task compared to other models, it has consistently good performance across all tasks.\nPerformers and Linear Transformers have strong performance on some tasks but their average is\nlowered by the ListOps task. Figure 3 shows the trade-off between qualitative performance, model\nspeed, and memory footprint. While BigBird performs well, its speed is almost similar to the vanilla\nTransformer. On the other hand, a model like Local Attention is fast at the cost of lower quantitative\nperformance. Among these models, the kernel-based variants, i.e., Performer, Linformer, and linear\nTransformer seem to be able to make a better trade-off in terms of speed and performance, while\nhaving reasonable memory usage.\nFigure 3: Performance (y axis), speed (x axis), and memory footprint (size of the circles) of different\nmodels.\n4 R ELATED WORK\n4.1 E FFICIENT TRANSFORMERS\nThe pervasiveness of Transformer models, along with its well-known trait of being memory inten-\nsive, has spurred on a large number of innovations on this front. Early work in this area has typically\nconsidered a ﬁxed pattern (local window) approach (Liu et al., 2018; Parmar et al., 2018). More\nadvanced models have been proposed recently, including combined patterns (Child et al., 2019; Ho\net al., 2019; Beltagy et al., 2020; Zaheer et al., 2020), learned patterns (Kitaev et al., 2020; Roy\net al., 2020), and recent models based on kernels (Katharopoulos et al., 2020; Choromanski et al.,\n2020a) or low-rank approximations (Wang et al., 2020). For the sake of brevity, we refer interested\nreaders to (Tay et al., 2020c) for a detailed survey of this line of research.\n4.2 E XISTING BENCHMARKS\nGenerative Modeling / Language Modeling This generative modeling task requires predicting\nthe next character, word, or pixel and is a staple in xformer evaluations (Roy et al., 2020; Kitaev et al.,\n2020). However, it has been debated how much long-range signal such tasks actually encode (Rae\n& Razavi, 2020).\n8\nPreprint\nLSTM language models augmented with attention have been shown to rarely attend beyond seven\npreceding words of context (Daniluk et al., 2017) and samples from LSTM language models\nare known to quickly devolve into generic text. On the other hand, recent models such as the\nTransformer-XL (Dai et al., 2019) have been observed to be sensitive to a context of around 900\ntokens and samples from large-scale models (Radford et al., 2019) maintain a consistent theme over\nmuch longer sequences. Even such recent models, however, can be improved by limiting the range\nof attention (Rae & Razavi, 2020). In sum, while standard language modelling datasets contain\nsome long-range signal, which is required to perform long-range coreference resolution, reasoning\nwith events, discourse understanding, etc. (Ruder et al., 2019) it seems to be overshadowed by the\nmuch stronger signal of short-term word co-occurrences and is thus difﬁcult to evaluate.5\nQuestion Answering Another commonly used evaluation task is question answering (QA; Zaheer\net al., 2020). Open-domain QA in particular typically requires the model to answer questions based\non long contexts such as entire Wikipedia documents (Joshi et al., 2017; Kwiatkowski et al., 2019)\nor even books (Ko ˇcisk´y et al., 2018). Other datasets are explicitly designed to require multiple\n‘hops’ of reasoning (Welbl et al., 2018; Yang et al., 2018). Successful approaches are often highly\nengineered, computationally expensive systems that require pre-training and a separate retrieval\nmodel (Lee et al., 2019; Guu et al., 2020).\nNatural Language Understanding / GLUE tasks Evaluation on natural language understanding\n(NLU) tasks is also common (Wang et al., 2020). Examples in most of these datasets such as\nMultiNLI (Williams et al., 2018) and SST (Socher et al., 2013) consist of single sentences and less\nthan 100 tokens on average.\n5 C ONCLUSION\nWe proposed Long Range Arena (LRA), a new benchmark for evaluating progress on efﬁcient Trans-\nformer research. Our new benchmark is challenging and probes at model capabilities in dealing\nwith diverse data types and structures such as text, mathematics, and visual data. Our benchmark\ncomprises of tasks ranging from 1K to 16K tokens. For the ﬁrst time, we conduct an extensive\nside-by-side comparison of ten recently proposed efﬁcient Transformer models. The experimental\nresults show that these tasks are very challenging even for long-range Transformer models. The\noverall results show that there is no one-size-ﬁts-all solution and trade-offs have to be made in terms\nof model quality and speed/memory. We plan to open source our code and benchmarks to facilitate\nfuture benchmarking, research and model development.\n6 A CKNOWLEDGEMENTS\nWe would like to thank the following colleagues: Krzysztof Choromanski, Richard Song, Tamas\nSarlos for recommendations on Performer setups. David Dohan and Manzil Zaheer for help on the\nBigBird implementation. Anselm Levskaya for some useful reference code for Reformers. Orhan\nFirat for helpful pointers. Jiaxi Tang, Jai Gupta, Zhen Qin, Che Zheng, Zhe Zhao, Da-Cheng Juan,\nThomas Unterthiner, Marc Najork, Aurko Roy, Kevin Murphy, Ashish Vaswani, Niki Parmar, Mo-\nhammad Taghi Saffar, Noah Fiedel and Peter J Liu, for general feedback and discussions. We would\nalso like to thank Drew Linsley, who provided us with help and information for setting up the path-\nﬁnder benchmark.\nREFERENCES\nSamira Abnar and Willem Zuidema. Quantifying attention ﬂow in transformers. In Proceedings of\nthe 58th Annual Meeting of the Association for Computational Linguistics, 2020.\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Philip Pham, Anirudh Ravula, and Sumit Sanghai.\nEtc: Encoding long and structured data in transformers. arXiv preprint arXiv:2004.08483, 2020.\n5Datasets such as LAMBADA (Paperno et al., 2016) more explicitly test for context understanding but are\nstill restricted to comparatively short contexts of ﬁve sentences on average.\n9\nPreprint\nIz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150, 2020.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Jared Davis, Tamas\nSarlos, David Belanger, Lucy Colwell, and Adrian Weller. Masked language modeling for pro-\nteins via linearly scalable long-context transformers. arXiv preprint arXiv:2006.03555, 2020a.\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas\nSarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention\nwith performers. arXiv preprint arXiv:2009.14794, 2020b.\nZihang Dai, Zhilin Yang, Yiming Yang, William W. Cohen, Jaime Carbonell, Quoc V . Le, and\nRuslan Salakhutdinov. Transformer-XL: Attentive Language Models Beyond a Fixed-Length\nContext. In Proceedings of ACL 2019, 2019.\nMichał Daniluk, Tim Rockt, Johannes Welbl, and Sebastian Riedel. Frustratingly Short Attention\nSpans in Neural Language Modeling. In Proceedings of ICLR 2017, 2017.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nJiafeng Guo, Yixing Fan, Qingyao Ai, and W Bruce Croft. A deep relevance matching model for\nad-hoc retrieval. In Proceedings of the 25th ACM International on Conference on Information\nand Knowledge Management, pp. 55–64, 2016.\nKelvin Guu, Kenton Lee, Zora Tung, and Panupong Pasupat. REALM: Retrieval-Augmented Lan-\nguage Model Pre-Training. In Proceedings of ICML 2020, 2020.\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-\nmensional transformers. arXiv preprint arXiv:1912.12180, 2019.\nSara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020.\nR. Houtkamp and P. R. Roelfsema. Parallel and serial grouping of image elements in visual percep-\ntion. J Exp Psychol Hum Percept Perform,, 2010.\nJeremy Howard and Sebastian Ruder. Universal Language Model Fine-tuning for Text Classiﬁca-\ntion. In Proceedings of ACL 2018, 2018.\nJyun-Yu Jiang, Mingyang Zhang, Cheng Li, Michael Bendersky, Nadav Golbandi, and Marc Najork.\nSemantic text matching for long-form documents. In The World Wide Web Conference, pp. 795–\n806, 2019.\nMandar Joshi, Eunsol Choi, Daniel S Weld, Luke Zettlemoyer, and Paul G Allen. TriviaQA: A\nLarge Scale Distantly Supervised Challenge Dataset for Reading Comprehension. InProceedings\nof ACL 2017, 2017.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc ¸ois Fleuret. Transformers are\nrnns: Fast autoregressive transformers with linear attention. arXiv preprint arXiv:2006.16236,\n2020.\nKazuya Kawakami, Chris Dyer, and Phil Blunsom. Learning to discover, ground and use words with\nsegmental neural language models. In Proceedings of ACL 2019, pp. 6429–6441, 2019.\nJunkyung Kim*, Drew Linsley*, Kalpit Thakkar, and Thomas Serre. Disentangling neural mecha-\nnisms for perceptual grouping. In International Conference on Learning Representations, 2020.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer. In\nInternational Conference on Learning Representations, 2020. URL https://openreview.\nnet/forum?id=rkgNKkHtvB.\n10\nPreprint\nTom´aˇs Koˇcisk´y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G´abor Melis,\nand Edward Grefenstette. The NarrativeQA Reading Comprehension Challenge. Transactions of\nthe Association for Computational Linguistics, 2018.\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris\nAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin Kenton Lee, Kristina Toutanova, Llion\nJones Matthew Kelcey, Ming-Wei Chang, Andrew M Dai, Jakob Uszkoreit, Quoc Le, and Slav\nPetrov. Natural Questions: a Benchmark for Question Answering Research. In Transactions of\nthe ACL, 2019.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent Retrieval for Weakly Supervised\nOpen Domain Question Answering. In Proceedings of ACL 2019, 2019.\nDrew Linsley, Junkyung Kim, Vijay Veerabadran, Charles Windolf, and Thomas Serre. Learning\nlong-range spatial dependencies with horizontal gated recurrent units. In Advances in neural\ninformation processing systems, pp. 152–164, 2018.\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,\nand Noam Shazeer. Generating wikipedia by summarizing long sequences. arXiv preprint\narXiv:1801.10198, 2018.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguis-\ntic representations for vision-and-language tasks. In Advances in Neural Information Processing\nSystems, pp. 13–23, 2019.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y . Ng, and Christopher\nPotts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting\nof the Association for Computational Linguistics: Human Language Technologies, pp. 142–150,\nPortland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http:\n//www.aclweb.org/anthology/P11-1015.\nNikita Nangia and Samuel R Bowman. Listops: A diagnostic dataset for latent tree learning. arXiv\npreprint arXiv:1804.06028, 2018.\nDenis Paperno, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco\nBaroni, Gemma Boleda, and Raquel Fern. The LAMBADA dataset: Word prediction requiring a\nbroad discourse context. In Proceedings of ACL 2016, 2016.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. Image transformer. arXiv preprint arXiv:1802.05751, 2018.\nDragomir R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. The acl\nanthology network corpus. Language Resources and Evaluation, 47(4):919–944, 2013.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nModels are Unsupervised Multitask Learners. 2019.\nJack W Rae and Ali Razavi. Do Transformers Need Deep Long-Range Memory? In Proceedings\nof ACL 2020, pp. 7524–7529, 2020.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\nAlexander Rives, Siddharth Goyal, Joshua Meier, Demi Guo, Myle Ott, C Lawrence Zitnick, Jerry\nMa, and Rob Fergus. Biological structure and function emerge from scaling unsupervised learning\nto 250 million protein sequences. bioRxiv, pp. 622803, 2019.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efﬁcient content-based sparse\nattention with routing transformers. arXiv preprint arXiv:2003.05997, 2020.\n11\nPreprint\nSebastian Ruder, Matthew E Peters, Swabha Swayamdipta, and Thomas Wolf. Transfer learning\nin natural language processing. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Tutorials, pp. 15–18, 2019.\nRichard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\ntreebank. In Proceedings of EMNLP 2013, pp. 1631–1642. Citeseer, 2013.\nHao Tan and Mohit Bansal. LXMERT: Learning Cross-Modality Encoder Representations from\nTransformers. In Proceedings of EMNLP 2019, 2019.\nYi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer:\nRethinking self-attention in transformer models. arXiv preprint arXiv:2005.00743, 2020a.\nYi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention.\narXiv preprint arXiv:2002.11296, 2020b.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A survey.arXiv\npreprint arXiv:2009.06732, 2020c.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nSinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with\nlinear complexity. arXiv preprint arXiv:2006.04768, 2020.\nJohannes Welbl, Pontus Stenetorp, and Sebastian Riedel. Constructing Datasets for Multi-hop Read-\ning Comprehension Across Documents. In Transactions of the Association for Computational\nLinguistics, 2018.\nAdina Williams, Nikita Nangia, and Samuel R. Bowman. A Broad-Coverage Challenge Corpus for\nSentence Understanding through Inference. In Proceedings of NAACL-HLT 2018, 2018. URL\nhttp://arxiv.org/abs/1704.05426.\nLiu Yang, Mingyang Zhang, Cheng Li, Michael Bendersky, and Marc Najork. Beyond 512 tokens:\nSiamese multi-depth transformer-based hierarchical encoder for document matching. CoRR,\nabs/2004.12297, 2020. URL https://arxiv.org/abs/2004.12297.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov,\nand Christopher D. Manning. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question\nAnswering. In Proceedings of EMNLP 2018, 2018.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer\nsequences. arXiv preprint arXiv:2007.14062, 2020.\n12\nPreprint\nA A PPENDIX\nA.1 LRA TASKS\nThis section describes the details and hyperparameters of each task. We also plan to release the\nconﬁguration ﬁles along with the implementation of the models and benchmarks, that can be used\nto reproduce the results reported in the paper.\nA.1.1 L IST OPS\nFollowing the generation steps in (Nangia & Bowman, 2018), we generate our own long version of\nthis task. We use a sequence length of 2k for this task. All our xformer models have an embedding\ndimension of 512, 8 heads, 6 layers and a feed-forward dimensions of 2048. We train all models for\n5K steps. The [CLS] token is used and mapped into a 10 class Softmax layer for classiﬁcation.\nA.1.2 B YTE -LEVEL DOCUMENT CLASSIFICATION\nWe use the IMDb reviews dataset (Maas et al., 2011) and a sequence length of {1K, 2K, 3K, 4K}\ntokens for all models. We pick the best results across these four sequence lengths. We use a [cls]\ntoken for prediction. All the [cls] tokens from xformer encoders are passed into a two layered MLP\nwith ReLU activations. The MLP emits a 2-class logits for binary classiﬁcation. We optimize the\nsoftmax cross entropy loss function. All xformer models are parameterized by the same number of\nlayers, heads and hidden dimensions, namely 8 heads, 512 hidden dimensions and d = 2048 for\npositional FFN layers. We use 6 layers for all xformers. The learning rate is 0.05 with weight decay\nof 0.1. We use Adam with warmup. All models are trained for 20K steps and a batch size of 32.\nA.1.3 B YTE -LEVEL DOCUMENT MATCHING\nWe use the ACL anthology network for a related article matching task. We use a sequence length\nof 4K per document (8K tokens in total for two sequences). The two encoders share parameters.\nSimilar to document classiﬁcation, we use the [cls] token from xformer encoders. Let X1 be the\n[cls] token embedding from document 1 and X2 be the [cls] token embedding from document 2, the\nﬁnal score is computed via:\nY = MLP([X1, X2, X1 ∗X2, X1 −X2]) (1)\nwhere MLP(.) is a two layered MLP with relu activation functions. In lieu of the much longer\nsequence length, we use a batch size of32, embedding dimension of 128, 4 heads, a FFN dimension\nof 512 and 4 layers. Model is trained with Adam for 5K steps with a learning rate of 0.5.\nA.2 I MAGE CLASSIFICATION\nWe use the gray-scaled (single channel) CIFAR10 as the image classiﬁcation dataset, with 10 classes.\nThe resolution of input images is 32 ×32 and after ﬂattening the input images, we feed our xformer\nencoders with a sequence of1024 pixels. Similar to our other classiﬁcation tasks, there is a classiﬁer\nhead on top of the xformer encoder, consisting of a two-layer MLP with ReLU activation. Softmax\ncross-entropy has been used for optimizing the parameters of the models. We trained our models for\n200 epochs and have done extensive sweeps over different hyper-parameters and found the following\nvalues leading to the best average performance across all xformers: 3 layers, 4 heads, 128 as the\nhidden dimensions of FFN blocks, 64 as the query/key/value hidden dimensions, and ﬁnally the\nlearning rate of 0.01.\nA.2.1 G ENERALIZATION GAP\nFor the image classiﬁcation benchmark, in Section 3, we mentioned that most of the models struggle\ngeneralizing to the test set. Table 3 presents the train and test accuracy for different models and for\nalmost all these models, the gap between the two scores is considerably high.\nWhile this task can be simple to solve for convectional models (e.g., accuracy of wide-resnet on\ngray-scale CIFAR10 with no data augmentation is 89.21) it is rather difﬁcult for Transformer-based\n13\nPreprint\nModel test accuracy train accuracy\nTransformer 42.44 69.45\nLocal Attention 41.46 63.19\nSparse Trans. 44.24 66.74\nLongformer 42.22 71.65\nLinformer 38.56 97.23\nReformer 38.07 68.45\nSinkhorn Trans. 41.23 69.21\nSynthesizer 41.61 97.31\nBigBird 40.83 71.49\nLinear Trans. 42.34 65.61\nPerformer 42.77 73.90\nTable 3: Test and train accuracy of different models on Image Classiﬁcation task.\nmodels with this setup. Naturally, one can ﬁnd ways to improve the performance with a different\nsetup. For instance, in our setup, models are not informed about the oridinality of pixel intensities\nand consume them as independent symbols. We observed that learning embedding that reﬂects this\nproperty is rather hard for most of these models (Figure ). If we simply replace the embedding layer\nwith a CNN stem, we see imitate boost in the performance (e.g. replacing the embedding layer of a\nvanilla Transformer with a convectional stem, with 3 ×3 kernel, we get accuracy of 75.32).\nAnother modiﬁcation that can lead to better performance is to incorporate spatial representation that\nare translation invariant in Transformer models (e.g., adding 2D relative positional embedding to a\nvanilla transformer, we get accuracy of 61.72). However, adding these sorts of changes make the\nsetup digress from the original point of this task in our benchmark.\nA.2.2 V ISUALIZATIONS OF LEANED EMBEDDING BY A VANILLA TRANSFORMER\nFigure 4 presents visualizations for the pixel intensity and positional embedding that a vanilla trans-\nformer model learns for the image classiﬁcation task, on the gray-scaled CIFAR10 detest.\nFigure 4: Left: The cosine similarity between the embedding learned for each pixel intensity.Right:\nEach tile shows the cosine similarity between the position embedding of the pixel with the indicated\nrow and column and the position embeddings of all other pixels.\nOn the left, we can see the pairwise similarity of learned embeddings for pixel intensities. Although\nthere is a higher similarity for close pixel values, the patterns from these learned embeddings do\nnot perfectly reﬂect the ordinality of the pixel intensities. On the right, we can see the pairwise\nsimilarity of positional embeddings for different input positions. We can see that the lower the\n14\nPreprint\ndistance between two pixels is, the more similar are their learned positional embeddings. However,\nthe spatial closeness in y axis is more preserved in the learned embedding than the distances in the\nx axis.\nA.3 P ATHFINDER\nPathﬁnder task probes the ability of models to detect long range spatial dependencies between input\nfeatures. To solve the task, a model requires to identify the target contour and trace it from one\nend to the other. Although Pathﬁnder is visually a simple task, it has been show that the clutter and\nvariations in path shape makes the task difﬁcult for CNN models (Linsley et al., 2018; Kim* et al.,\n2020).\nThe Pathﬁnder task is a binary classiﬁcation task and the resolution of input images is 32 ×32.\nSimilar to image classiﬁcation task, we feed our xformer encoders with a sequence of 1024 pixels\nafter ﬂattening the input images. The classiﬁer head on top of the xformer encoder is also a two-\nlayer MLP with ReLU activation and we use Softmax cross-entropy loss for the optimization. We\ntrained our models for 200 epochs. The hyper-parameters used for the xformer model are as follow:\n4 layers, 8 heads, 128 as the hidden dimensions of FFN blocks, 128 as the query/key/value hidden\ndimensions, and the learning rate of 0.01.\nA.3.1 V ISUALIZATION OF THE ATTENTION MAPS FROM A VANILLA TRANSFORMER\nGiven that transformers have many units with global receptive ﬁeld, they have better potential for\nsolving the task, compared to models with local receptive ﬁelds. Figure 5 shows the attention distri-\nbutions for a set of examples given on token (CLS token) as the query. We can see that the attention\nmodule collects information from different positions in input to be able to trace the target path.\nFigure 5: Attention map for different examples from the Pathﬁnder task. Each map presents the\nattention distribution, given the CLS token at the ﬁnal layer as the query, averaged across all heads\nin a vanilla Transformer model. Note that for visualization, we use attention-rollout (Abnar &\nZuidema, 2020) for more precise input attribution.\nWe have also included a Pathﬁnder-X in LRA, which is similar to Pathﬁnder, but inputs are in higher\nresolutions, i.e. longer input sequences. On Pathﬁnder-X, we have tried two setups for training our\nmodels, ﬁrst training models from scratch, second evaluating models that are trained on Pathﬁnder.\nIn both cases, we found out none of the models are able to deal with/generalize to 16K input length.\n15\nPreprint\nB M ODELS AND IMPLEMENTATION\nThis section describes the details of our implementation. The code is primarily written in JAX and\nFLAX. In this section, we note speciﬁc details about certain implementations of models. We plan to\nrelease hyperparameters in a form of readme or script later.\nB.1 A B RIEF OVERVIEW OF MODEL IMPLEMENTATIONS\nWhile most of the ﬁne-grained details is planned to be available in the released code, we provide\na brief overview of some settings of the xformer models being evaluated. For local attention, we\ndo not use overlapping blocks. For local attention within Sinkhorn Transformer blocks, we do not\noverlap windows either. For Linformers, the projections are shared between key and values but not\nacross multiple layers. For Performer models, our implementation uses FA VOR+, the more recent\nversion in the paper Choromanski et al. (2020b).\nB.2 S PECIAL CASES OF OUR IMPLEMENTATION\nThis section describes several special cases in our implementation details. The diverse suite of\nTransformers come with a plethora of hardware constraints and implementation details. To succeed,\na Transformer model needs to also ‘win’ the hardware lottery (Hooker, 2020), i.e., having readily\nsupported ops, kernels or accelerator support to take advantage of its technical design. This section\ndiscusses some of the trade-offs and edge cases that make comparison of several models challenging.\nIn the end, we argue that simplicity is a virtue and not requiring any special support is a positive\nthing for an efﬁcient Transformer model.\nOn CUDA kernels CUDA kernels are cumbersome and are speciﬁc to GPU hardware, making it\ndifﬁcult to implement or use on TPU pods. Generally, these are considered to be undesirable and\ninconvenient in practical applications. Hence, Sparse Transformer and Longformer are implemented\nwith equivalent implementations to emulate for performance. This is by applying an equivalent\nmask. For this reason, we do not benchmark Sparse Transformer and Longformer for speed.\nReformer’s Implementation Having optimized ops to support many of Reformer’s functionality\nis crucial. Hence, Reformer is implemented slightly differently from other Transformer models.\nInstead of computing tensors with batch size dimensions B and head dimensions H, (i.e., B ×H ×\nN ×d), we compute the attention function for tensors of N ×d dimensions. After which, we\nparallelize this function via VMAP over the batch and head dimensions.\nC R ECOMMENDATIONS FOR FAIR COMPARISON\nWe welcome re-evaluation of our models on any task. However, we consider some hyperparameters\nto be immutable to ensure fair comparison with all models. In the case of proposing new models,\nthe LRA table in the paper can be copied as it is as long as (1) the model size remains unchanged,\n(2) no pretraining is conducted, (3) no alterations to the fundamental setups (e.g., changing char\nlevel to word level or adding spatial information to the image task). We will provide more details at\nhttps://github.com/google-research/long-range-arena .\n16",
  "topic": "Benchmark (surveying)",
  "concepts": [
    {
      "name": "Benchmark (surveying)",
      "score": 0.6358705163002014
    },
    {
      "name": "Transformer",
      "score": 0.6118996143341064
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.4800587594509125
    },
    {
      "name": "Computer science",
      "score": 0.3811284601688385
    },
    {
      "name": "Engineering",
      "score": 0.2944098711013794
    },
    {
      "name": "Electrical engineering",
      "score": 0.19204464554786682
    },
    {
      "name": "Geography",
      "score": 0.11637723445892334
    },
    {
      "name": "Cartography",
      "score": 0.07758229970932007
    },
    {
      "name": "Aerospace engineering",
      "score": 0.07294350862503052
    },
    {
      "name": "Voltage",
      "score": 0.052748292684555054
    }
  ]
}