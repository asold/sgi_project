{
    "title": "Neural Machine Translation with the Transformer and Multi-Source Romance Languages for the Biomedical WMT 2018 task",
    "url": "https://openalex.org/W2903533908",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A2902837430",
            "name": "Brian Tubay",
            "affiliations": [
                "Universitat Politècnica de Catalunya"
            ]
        },
        {
            "id": "https://openalex.org/A4227926241",
            "name": "Marta R. Costa‐jussà",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4298393544",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2802249775",
        "https://openalex.org/W2963506925",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W1665214252",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2964265128",
        "https://openalex.org/W2962824887",
        "https://openalex.org/W2964034111",
        "https://openalex.org/W2901607128",
        "https://openalex.org/W2613904329",
        "https://openalex.org/W2626792426",
        "https://openalex.org/W2550821151",
        "https://openalex.org/W2963602293",
        "https://openalex.org/W2963212250"
    ],
    "abstract": "The Transformer architecture has become the state-of-the-art in Machine Translation. This model, which relies on attention-based mechanisms, has outperformed previous neural machine translation architectures in several tasks. In this system description paper, we report details of training neural machine translation with multi-source Romance languages with the Transformer model and in the evaluation frame of the biomedical WMT 2018 task. Using multi-source languages from the same family allows improvements of over 6 BLEU points.",
    "full_text": "Proceedings of the Third Conference on Machine Translation (WMT), V olume 2: Shared Task Papers, pages 667–670\nBelgium, Brussels, October 31 - Novermber 1, 2018.c⃝2018 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/W18-64076\nNeural Machine Translation with the Transformer and Multi-Source\nRomance Languages for the Biomedical WMT 2018 task\nBrian Tubay and Marta R. Costa-juss`a\nTALP Research Center, Universitat Politcnica de Catalunya, Barcelona\nbrian.alcides.tubay.alvarez@alu-etsetb.upc.edu,marta.ruiz@upc.edu\nAbstract\nThe Transformer architecture has become the\nstate-of-the-art in Machine Translation. This\nmodel, which relies on attention-based mech-\nanisms, has outperformed previous neural ma-\nchine translation architectures in several tasks.\nIn this system description paper, we report\ndetails of training neural machine translation\nwith multi-source Romance languages with\nthe Transformer model and in the evaluation\nframe of the biomedical WMT 2018 task.\nUsing multi-source languages from the same\nfamily allows improvements of over 6 BLEU\npoints.\n1 Introduction\nNeural Machine Translation (NMT) (Bahdanau\net al., 2015) proved to be competitive with the\nencoder-decoder architecture based on recurrent\nneural networks and attention. After this archi-\ntecture, new proposals based on convolutional\nneural networks (Gehring et al., 2017) or only\nattention-based mechanisms (Vaswani et al., 2017)\nappeared. The latter architecture has achieved\ngreat success in Machine Translation (MT) and it\nhas already been extended to other tasks such as\nParsing (Kaiser et al., 2017), Speech Recognition\n1, Speech Translation (Cros et al., 2018), Chatbots\n(Costa-juss`a et al., 2018) among others.\nHowever, training with low resources is still a\nbig drawback for neural architectures and NMT\nis not an exception (Koehn and Knowles, 2017).\nTo face low resource scenarios, several techniques\nhave been proposed, like using multi-source (Zoph\nand Knight, 2016), multiple languages (Johnson\net al., 2017) or unsupervised techniques (Lample\net al., 2018; Artetxe et al., 2018), among many oth-\ners.\n1https://tensorflow.github.io/\ntensor2tensor/tutorials\\/asr$_$with$_\n$transformer.html\nIn this paper, we use the Transformer enhanced\nwith the multi-source technique to participate in\nthe Biomedical WMT 2018 task, which can be\nsomehow considered a low-resourced task, given\nthe large quantity of data that it is required for\nNMT. Our multi-source enhancement is done only\nwith Romance languages. The fact of using sim-\nilar languages in a multi-source system may be a\nfactor towards improving the ﬁnal system which\nends up with over 6 BLEU points of improvement\nover the single source system.\n2 The Transformer architecture\nThe Transformer model is the ﬁrst NMT model\nrelying entirely on self-attention to compute rep-\nresentations of its input and output without using\nrecurrent neural networks (RNN) or convolutional\nneural networks (CNN).\nRNNs read one word at a time, having to per-\nform multiple steps before generating an output\nthat depends on words that are far away. But\nit has been demonstrated that the more steps re-\nquired, the harder it is to the network to learn\nhow to make these decisions (Bahdanau et al.,\n2015). In addition, given the sequential nature\nof the RNNs, it is difﬁcult to fully take advan-\ntage of modern computing devices such as Tensor\nProcessing Units (TPUs) or Graphics Processing\nUnits (GPUs) which rely on parallel processing.\nThe Transformer is an encoder-decoder model that\nwas conceived to solve these problems.\nThe encoder is composed of three stages. In the\nﬁrst stage input words are projected into an em-\nbedded vector space. In order to capture the no-\ntion of token position within the sequence, a po-\nsitional encoding is added to the embedded input\nvectors. Without positional encodings, the output\nof the multi-head attention network would be the\nsame for the sentences “I love you more than her”\n667\nand “I love her more than you”. The second stage\nis a multi-head self-attention. Instead of comput-\ning a single attention, this stage computes multi-\nple attention blocks over the source, concatenates\nthem and projects them linearly back onto a space\nwith the initial dimensionality. The individual at-\ntention blocks compute the scaled dot-product at-\ntention with different linear projections. Finally\na position-wise fully connected feed-forward net-\nwork is used, which consists of two linear trans-\nformations with a ReLU activation (Vinod Nair,\n2010) in between.\nEmbedding \n& \nPositional Encoding \nEmbedding \n& \nPositional Encoding \nMulti-Head \nSelf-Attention \nMasked Multi-Head \nSelf-Attention \nFeed Forward\nFeed Forward\nMulti-Head \nAttention \nSoftmax\nInputs Targets\nOutput\nProbabilities\nEncoder Decoder\nFigure 1: Simpliﬁed diagram of the Transformer model\nThe decoder operates similarly, but generates\none word at a time, from left to right. It is com-\nposed of ﬁve stages. The ﬁrst two are similar to\nthe encoder: embedding and positional encoding\nand a masked multi-head self-attention, which un-\nlike in the encoder, forces to attend only to past\nwords. The third stage is a multi-head attention\nthat not only attends to these past words, but also\nto the ﬁnal representations generated by the en-\ncoder. The fourth stage is another position-wise\nfeed-forward network. Finally, a softmax layer al-\nlows to map target word scores into target word\nprobabilities. For more speciﬁc details about the\narchitecture, refer to the original paper (Vaswani\net al., 2017).\n3 Multi-Source translation\nMulti-source translation consists in exploiting\nmultiple text inputs to improve NMT (Zoph and\nKnight, 2016). In our case, we are using this ap-\nproach in the Transformer architecture described\nabove and using only inputs from the same lan-\nguage family.\n4 Experiments\nIn this section we report details on the database,\ntraining parameters and results.\n4.1 Databases and Preprocessing\nThe experimental framework is the Biomedical\nTranslation Task (WMT18) 2. The corpus used\nto train the model are the one provided for the\ntask for the selected languages pairs: Spanish-\nto-English (es2en), French-to-English (fr2en) and\nPortuguese-to-English (pt2en). Sources are\nmainly from Scielo and Medline and detailed in\nTable 3.\nTraining Scielo Medline Total\nes2en 713127 285358 998485\nfr2en 9127 612645 621772\npt2en 634438 74267 708705\nall2en 1356692 972270 2328962\nTable 3: Corpus Statistics (number of segments).\nValidation sets were taken from Khresmoi de-\nvelopment data3, as recommended in the task de-\nscription. Each validation dataset contains 500\nsentence pairs. Test sets were the ones provides\nby the task for the previous year competition\n(WMT174).\nPreprocessing relied on three basic steps: tok-\nenization, truecasing and limiting sentence length\nto 80 words. Words were segmented by means of\nByte-Pair Encoding (BPE) (Sennrich et al., 2015).\n4.2 Parameters\nThe system was implemented using OpenNMT in\nPyTorch (Klein et al., 2017) with the hyperparam-\neters suggested in the website 5. Other parame-\nters used in training are deﬁned in Table 4. Both\nsingle-language systems and multi-source system\n2http://www.statmt.org/wmt18/\nbiomedical-translation-task.html\n3https://lindat.mff.cuni.cz/\nrepository/xmlui/handle/11234/1-2122\n4http://www.statmt.org/wmt17/\nbiomedical-translation-task.html\n5http://opennmt.net/OpenNMT-py/FAQ.\nhtml\n668\nSystem es2en pt2en fr2en\nWMT17 WMT18 WMT17 WMT18 WMT17 WMT18\nBest performing system 37.49 43.31 43.88 42.58 - 25.78\nSingle-Language 39.35 39.06 44.31 38.54 31.75 19.42\nMulti-Language 40.11 40.49 45.55 39.49 38.31 25.78\nTable 1: Trained systems results for WMT17 and WMT18 ofﬁcial test sets.\nSpanish Utilizando la base de datos Epistemonikos, la cual es mantenida mediante bsquedas realizadas en 30 bases de datos,\nidentiﬁcamos seis revisiones sistemticas que en conjunto incluyen 36 estudios aleatorizados pertinentes a la pregunta.\nSingle-Language Using the Epistemonikos database, which is maintained through searches in 30 databases, we identiﬁed six systematic\nreviews including 36 randomized studies relevant to the question.\nMulti-Language Using the Epistemonikos database, which is maintained through searches in 30 databases, we identiﬁed six systematic\nreviews that altogether include 36 randomized studies relevant to the question.\nPortuguese Os resultados dos modelos de regresso mostraram associao entre os fatores de correo estimados e os indicadores de adequao\npropostos\nSingle-Language Regression models showed an association between estimated correction factors and the proposed adequacy indicators.\nMulti-Language The results of the regression models showed an association between the estimated correction factors and the proposed\nadequacy indicators.\nFrench (Traduit par Docteur Serge Messier).\nSingle-Language [Doctor Serge Messier].\nMulti-Language [(Translated by Doctor Serge Messier)].\nTable 2: Spanish/Portuguese/French to English examples for WMT18\nwere trained with same architecture and parame-\nters.\nHparam Text-to-Text\nEncoder layers 6\nDecoder layers 6\nBatch size 4096\nAdam optimizer β1 = 0.9 β2 = 0.998\nAttention heads 8\nTable 4: Training parameters.\nWe trained three single-language systems, one\nfor each language pair. We required 14 epochs for\nthe Spanish-to-English system (7 hours of train-\ning), 16 epochs for the French-to-English sys-\ntem (9 hours of training), and 17 epochs for the\nPortuguese-to-English system (7 hours of train-\ning). For the multi-source system, which con-\ncatenated the three parallel corpus together, we\nrequired 11 epochs (23 hours of training). We\nstopped training when the validation accuracy did\nnot increase in two consecutive epochs.\n4.3 Results\nBest ranking systems from WMT17 and WMT18\nare shown in Table 1, except for French-to-English\nWMT17 since the references for this set are not\navailable. For this pair, we used 1000 sentences\nfrom the Khresmoi development data. Table 1\nshows BLEU results for the baseline systems, the\nsingle-language and multi-source approaches.\nThe Transformer architecture outperforms\nWMT17 best system. Results become even better\nwith the system is trained with the common\ncorpus of Romance languages, what we call the\nmulti-source approach. The latter is consistent\nwith the universal truth that more data equals\nbetter results, even if the source language is not\nthe same.\nFinally, Table 2 shows some examples of the\noutput translations.\n5 Conclusions\nThe main conclusions of our experiments are that\nthe multi-source inputs of the same family applied\nto the Transformer architecture can improve the\nsingle input. Best improvements achieve an in-\ncrease of 6 BLEU points in translation quality.\nAcknowledgments\nAuthors would like to thank Noe Casas for his\nvaluable comments. This work is supported in\n669\npart by the Spanish Ministerio de Econom ´ıa y\nCompetitividad, the European Regional Devel-\nopment Fund and the Agencia Estatal de Inves-\ntigaci´on, through the postdoctoral senior grant\nRam´on y Cajal, the contract TEC2015-69266-P\n(MINECO/FEDER,EU) and the contract PCIN-\n2017-079 (AEI/MINECO).\nReferences\nMikel Artetxe, Gorka Labaka, Eneko Agirre, and\nKyunghyun Cho. 2018. Unsupervised neural ma-\nchine translation. In Proceedings of the Inter-\nnational Conference on Learning Representations\n(ICLR).\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. 2015. Neural machine translation by\njointly learning to align and translate. CoRR,\nabs/1409.0473.\nMarta R. Costa-juss`a, ´Alvaro Nuez, and Carlos Segura.\n2018. Experimental research on encoder-decoder\narchitectures with attention for chatbots. Com-\nputaci´on y Sistemas.\nLaura Cros, Carlos Escolano, Jos ´e A. R. Fonollosa,\nand Marta R. Costa-juss`a. 2018. End-to-end speech\ntranslation with the transformer. Submitted to Iber-\nSpeech.\nJonas Gehring, Michael Auli, David Grangier, De-\nnis Yarats, and Yann N. Dauphin. 2017. Con-\nvolutional sequence to sequence learning. CoRR,\nabs/1705.03122.\nMelvin Johnson, Mike Schuster, Quoc V . Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Vi´egas, Martin Wattenberg, Greg Corrado,\nMacduff Hughes, and Jeffrey Dean. 2017. Google’s\nmultilingual neural machine translation system: En-\nabling zero-shot translation. Transactions of the As-\nsociation for Computational Linguistics, 5:339–351.\nLukasz Kaiser, Aidan N Gomez, Noam Shazeer,\nAshish Vaswani, Niki Parmar, Llion Jones, and\nJakob Uszkoreit. 2017. One model to learn them\nall. arXiv preprint arXiv:1706.05137.\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean\nSenellart, and Alexander Rush. 2017. OpenNMT:\nOpen-source toolkit for neural machine translation.\nIn Proceedings of ACL 2017, System Demonstra-\ntions, pages 67–72. Association for Computational\nLinguistics.\nPhilipp Koehn and Rebecca Knowles. 2017. Six chal-\nlenges for neural machine translation. InProc. of the\n1st Workshop on Neural Machine Translation, pages\n28–39, Vancouver.\nGuillaume Lample, Ludovic Denoyer, and\nMarc’Aurelio Ranzato. 2018. Unsupervised ma-\nchine translation using monolingual corpora only.\nIn Proceedings of the International Conference on\nLearning Representations (ICLR).\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2015. Neural machine translation of rare words with\nsubword units. arXiv preprint arXiv:1508.07909.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 6000–6010. Curran As-\nsociates, Inc.\nGeoffrey E. Hinton Vinod Nair. 2010. Rectiﬁed lin-\near units improve restricted boltzmann machines. In\n27th International Conference on Machine Learn-\ning.\nBarret Zoph and Kevin Knight. 2016. Multi-source\nneural translation. In NAACL-HLT 2016, pages 30–\n34.\n670"
}