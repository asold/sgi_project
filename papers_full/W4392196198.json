{
  "title": "Comparative Evaluation of Commercial Large Language Models on PromptBench: An English and Chinese Perspective",
  "url": "https://openalex.org/W4392196198",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2097443957",
      "name": "Shiyu Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2161548545",
      "name": "Qian Ouyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097293095",
      "name": "Bing Wang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6854692045",
    "https://openalex.org/W3209721572",
    "https://openalex.org/W4318464200",
    "https://openalex.org/W6861062951",
    "https://openalex.org/W4389768226",
    "https://openalex.org/W4377864352",
    "https://openalex.org/W4391505467",
    "https://openalex.org/W4389068880",
    "https://openalex.org/W4386977773",
    "https://openalex.org/W4387425757",
    "https://openalex.org/W4322622443",
    "https://openalex.org/W4391012681",
    "https://openalex.org/W4391800721",
    "https://openalex.org/W4390528013",
    "https://openalex.org/W4390912975",
    "https://openalex.org/W6860041859",
    "https://openalex.org/W4387559778",
    "https://openalex.org/W4385889719",
    "https://openalex.org/W4389672268",
    "https://openalex.org/W4221141417",
    "https://openalex.org/W4382618460",
    "https://openalex.org/W4385373704",
    "https://openalex.org/W4391901128",
    "https://openalex.org/W4283366230",
    "https://openalex.org/W4380136538",
    "https://openalex.org/W4386794666",
    "https://openalex.org/W6857294608",
    "https://openalex.org/W6730822450",
    "https://openalex.org/W4388184238",
    "https://openalex.org/W4391591636",
    "https://openalex.org/W4391591805",
    "https://openalex.org/W4389223002",
    "https://openalex.org/W4389043118",
    "https://openalex.org/W4353113699",
    "https://openalex.org/W3199761064",
    "https://openalex.org/W4387436590",
    "https://openalex.org/W4308243242",
    "https://openalex.org/W4389217590",
    "https://openalex.org/W4388277090",
    "https://openalex.org/W4388964727",
    "https://openalex.org/W4390529182",
    "https://openalex.org/W4386794445",
    "https://openalex.org/W6856051742",
    "https://openalex.org/W6861581687",
    "https://openalex.org/W4389983939",
    "https://openalex.org/W4387034760",
    "https://openalex.org/W4386081573",
    "https://openalex.org/W4390961147",
    "https://openalex.org/W4283330306",
    "https://openalex.org/W4386290290",
    "https://openalex.org/W4391766565",
    "https://openalex.org/W4386701652",
    "https://openalex.org/W4391555991",
    "https://openalex.org/W4383605161",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W4388691863",
    "https://openalex.org/W4391590822",
    "https://openalex.org/W4387156634",
    "https://openalex.org/W4410342726",
    "https://openalex.org/W4390041933",
    "https://openalex.org/W4386081793",
    "https://openalex.org/W4387034804",
    "https://openalex.org/W4386293235",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W4389574762"
  ],
  "abstract": "<title>Abstract</title> This study embarks on an exploration of the performance disparities observed between English and Chinese in large language models (LLMs), motivated by the growing need for multilingual capabilities in artificial intelligence systems. Utilizing a comprehensive methodology that includes quantitative analysis of model outputs and qualitative assessment of language nuances, the research investigates the underlying reasons for these discrepancies. The findings reveal significant variations in the performance of LLMs across the two languages, with a pronounced challenge in accurately processing and generating text in Chinese. This performance gap underscores the limitations of current models in handling the complexities inherent in languages with distinct grammatical structures and cultural contexts. The implications of this research are far-reaching, suggesting a critical need for the development of more robust and inclusive models that can better accommodate linguistic diversity. This entails not only the enrichment of training datasets with a wider array of languages but also the refinement of model architectures to grasp the subtleties of different linguistic systems. Ultimately, this study contributes to the ongoing discourse on enhancing the multilingual capabilities of LLMs, aiming to pave the way for more equitable and effective artificial intelligence tools that cater to a global user base.",
  "full_text": "Comparative Evaluation of Commercial Large\nLanguage Models on PromptBench: An English and\nChinese Perspective\nShiyu Wang \nhttps://orcid.org/0009-0008-4014-6318\nQian Ouyang \nhttps://orcid.org/0009-0000-1950-8566\nBing Wang  \n \nhttps://orcid.org/0009-0003-3415-7612\nResearch Article\nKeywords: Arti\u0000cial Intelligence, Cross-Linguistic Analysis, Language Models Linguistic Diversity,\nMultilingual LLMs, Performance Evaluation\nPosted Date: February 27th, 2024\nDOI: https://doi.org/10.21203/rs.3.rs-3987793/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: The authors declare no competing interests.\n1\nComparative Evaluation of Commercial Large\nLanguage Models on PromptBench: An English and\nChinese Perspective\nShiyu W ang, Qian Ouyang, and Bing W ang\nAbstract—This study embarks on an exploration of the per-\nformance disparities observed between English and Chinese in\nlarge language models (LLMs), motivated by the growing need\nfor multilingual capabilities in artiﬁcial intelligence systems.\nUtilizing a comprehensive methodology that includes quantitative\nanalysis of model outputs and qualitative assessment of language\nnuances, the research investigates the underlying reasons for\nthese discrepancies. The ﬁndings reveal signiﬁcant variations in\nthe performance of LLMs across the two languages, with a pro-\nnounced challenge in accurately processing and generating text\nin Chinese. This performance gap underscores the limitations of\ncurrent models in handling the complexities inherent in languages\nwith distinct grammatical structures and cultural contexts. The\nimplications of this research are far-reaching, suggesting a critical\nneed for the development of more robust and inclusive models\nthat can better accommodate linguistic diversity . This entails not\nonly the enrichment of training datasets with a wider array\nof languages but also the reﬁnement of model architectures to\ngrasp the subtleties of different linguistic systems. Ultimately,\nthis study contributes to the ongoing discourse on enhancing the\nmultilingual capabilities of LLMs, aiming to pave the way for\nmore equitable and effective artiﬁcial intelligence tools that cater\nto a global user base.\nIndex T erms—Artiﬁcial Intelligence, Cross-Linguistic Analy-\nsis, Language Models Linguistic Diversity, Multilingual LLMs,\nPerformance Evaluation.\nI. I N T RO D U C T I O N\nThe advent of Large Language Models (LLMs) has marked\na transformative era in the ﬁeld of artiﬁcial intelligence,\noffering unprecedented capabilities in natural language pro-\ncessing, generation, and understanding [1]–[3]. These models,\nincluding notable examples such as ChatGPT -4, ChatGPT -3.5,\nGoogle’s Gemini, and Anthropic’s Claude, have demonstrated\nremarkable proﬁciency across a wide array of linguistic tasks,\npropelling advancements in both academic research and com-\nmercial applications [1], [3], [4]. However, the evaluation of\nthese models presents a complex challenge, necessitating com-\nprehensive benchmarks that can effectively measure their per-\nformance across diverse linguistic landscapes. The Microsoft\nLLM benchmark, PromptBench, emerges as a pivotal tool in\nthis context, designed to rigorously assess the capabilities of\nLLMs in understanding and generating responses to a variety\nof prompts in multiple languages [5]. This benchmark is\ncritical not only for benchmarking current models but also for\nguiding the future development of more advanced, nuanced,\nand culturally aware language models.\nThe evaluation of LLMs using benchmarks like Prompt-\nBench is essential for several reasons. Firstly , it provides\na standardized framework to compare the performance of\ndifferent models objectively . This is crucial for identifying the\nstrengths and weaknesses of each model, facilitating targeted\nimprovements and the development of more sophisticated\nlanguage processing capabilities. Secondly , such evaluations\nhelp in understanding how well these models can adapt to the\nnuances of human language, including idiomatic expressions,\ncultural references, and contextual understanding. This is par-\nticularly important as the utility of LLMs expands beyond En-\nglish to include a multitude of languages, each with its unique\nlinguistic characteristics and challenges. Thirdly , benchmarks\nlike PromptBench play a signiﬁcant role in ensuring that\nLLMs are being developed in an inclusive manner, catering\nto a global audience and reducing biases inherent in language\ntechnologies.\nDespite the notable successes of LLMs, our research has\nunveiled a consistent pattern of underperformance in Chinese\ncompared to English when evaluated using the PromptBench\nbenchmark. This discrepancy raises important questions about\nthe linguistic and cultural adaptability of current LLMs, high-\nlighting a critical area of concern for developers and re-\nsearchers alike. The observed performance gap underscores the\nimportance of developing models that are not only proﬁcient\nin processing the dominant languages but are also capable\nof handling the complexity and diversity of less commonly\nused languages with the same level of competence. Addressing\nthis issue is not merely a technical challenge but a necessary\nstep towards achieving truly global and inclusive language\ntechnologies.\nThis article aims to provide a comprehensive evaluation\nof commercial large language models using both the English\nand Chinese versions of the Microsoft LLM benchmark,\nPromptBench. By analyzing the performance of ChatGPT -\n4, ChatGPT -3.5, Google’s Gemini, and Anthropic’s Claude,\nwe seek to uncover the underlying factors contributing to the\nobserved performance discrepancies. Through this investiga-\ntion, we endeavor to contribute to the ongoing dialogue on\nthe development of more adaptable, equitable, and culturally\nsensitive language models. The insights gained from this study\nare intended to inform future model development efforts,\nensuring that advancements in LLM technologies are both\ninclusive and reﬂective of the diverse linguistic landscape that\ncharacterizes our world.\n2\nII. R E L AT E D WO R K\nThis section reviews existing literature on the evaluation of\nLLMs, with a particular focus on ChatGPT4, ChatGPT3.5,\nGoogle Gemini, and Anthropic Claude. It also examines the\ndevelopment and importance of benchmarks like PromptBench\nin assessing the capabilities and performance of these models\nacross different languages.\nA. Evaluations of Large Language Models\nA number of studies have systematically assessed the\nperformance and capabilities of large language models. A\nfew studies demonstrated the remarkable natural language\nunderstanding of ChatGPT3.5 across a range of benchmarks,\nhighlighting its advancement over previous iterations [4], [6]–\n[8]. Others compared the inference capabilities of ChatGPT4\nagainst its predecessors, revealing signiﬁcant improvements\nin reasoning and comprehension tasks [6], [9]–[13]. Google\nGemini was evaluated in a few studies, which showcased\nits superior handling of complex queries and its enhanced\nunderstanding of context [14]–[16]. Anthropic Claude was\nthe subject of studies that emphasized its ethical reasoning\nabilities and robustness against generating harmful content\n[17]–[19]. Further research highlighted the incremental ad-\nvancements in model performance, particularly in domain-\nspeciﬁc applications [17], [20], [21]. Comparative studies\nshowcased the differences in comprehension and generation\ncapabilities between models, identifying key areas for future\nimprovements [1], [22]. Other work examined the impact of\ntraining data diversity on the performance of these models,\nsuggesting a direct correlation with their linguistic and cultural\nunderstanding [23], [24].\nB. Development of Benchmarking T ools\nThe evolution of benchmarking tools has been critical in\nevaluating and advancing LLMs. The introductions of dif-\nferent LLM benchmarks were detailed in studies, marking\na signiﬁcant leap in assessing models’ performance across\ndiverse tasks [5], [25]–[29]. Subsequent studies expanded on\nthis by integrating multilingual capabilities into benchmarks,\nallowing for a broader evaluation of LLMs [25], [30], [31].\nResearch underscored the importance of benchmarks in un-\nderstanding models’ limitations, particularly in non-English\nlanguages [23], [27], [32]. Other studies introduced a novel\nmethodology for dynamically updating benchmarks to keep\npace with the rapid advancements in LLMs [1], [33]. The\nrole of benchmarks in highlighting ethical and societal con-\nsiderations was examined, emphasizing the need for models\nto perform responsibly across different cultural contexts [34].\nFurther research explored the comparative analysis of LLMs\nusing PromptBench and other benchmarks, offering insights\ninto the models’ evolving competencies and weaknesses [1],\n[27], [35].\nC. P erformance Discrepancy Across Languages\nThe disparity in LLMs’ performance across languages has\ngarnered signiﬁcant attention. Studies highlighted the chal-\nlenges faced by LLMs in comprehending and generating non-\nEnglish languages, using PromptBench for evaluation [35]–\n[37]. Other studies focused speciﬁcally on the Chinese lan-\nguage, revealing substantial performance gaps when compared\nto English [30], [38], [39]. Comparative studies analyzed the\nunderlying factors contributing to these discrepancies, pointing\nto differences in training data and linguistic structures [23],\n[24], [32], [40]. Further investigations explored the effective-\nness of various strategies in mitigating these performance gaps,\nsuch as ﬁne-tuning models with diverse language datasets\n[41]–[43]. Research also examined the role of cultural nuances\nin affecting model performance, underscoring the complexity\nof achieving language parity [44], [45]. Lastly , some studies\nproposed a framework for systematically improving LLMs’\nmultilingual capabilities, suggesting a path forward in address-\ning these challenges [46]–[54].\nIII. M E T H O D O L O G Y\nThis section outlines the methodology employed to eval-\nuate commercial large language models (LLMs) using the\nMicrosoft LLM benchmark, PromptBench, in both its English\nand Chinese versions. It includes a comprehensive description\nof the benchmark itself, the LLMs evaluated, and the metrics\napplied to assess their performance.\nA. Overview of PromptBench\nPromptBench is a comprehensive benchmark designed to\nevaluate the performance of large language models (LLMs)\nacross a broad spectrum of tasks that reﬂect real-world appli-\ncations [5]. Its design aims to assess not only the linguistic\ncapabilities of LLMs but also their reasoning, understand-\ning, and generation abilities within a controlled environment.\nPromptBench includes a diverse array of tasks, such as natural\nlanguage understanding, reading comprehension, translation,\nand summarization, to ensure a thorough evaluation of LLMs’\ncapacities. This benchmark distinguishes itself by offering a\nbalanced mix of qualitative and quantitative tasks, facilitating\na detailed assessment of LLM performance. For our study ,\nwe utilized both the English and Chinese versions of Prompt-\nBench to conduct a rigorous examination of model perfor-\nmance across linguistic boundaries. The tasks were carefully\nadapted into Chinese to preserve the benchmark’s integrity and\nensure comparability across languages.\nThe key features and characteristics of PromptBench, which\nset it apart as a benchmark for evaluating LLMs, include:\n• A wide-ranging set of tasks designed to test various\naspects of language understanding and generation, high-\nlighting the models’ capability to handle complex linguis-\ntic and cognitive challenges.\n• Incorporation of both qualitative and quantitative evalu-\nation metrics, offering a holistic view of model perfor-\nmance and its applicability to real-world scenarios.\n• The adaptation of tasks for multiple languages, specif-\nically English and Chinese in this study , to assess and\ncompare the cross-linguistic adaptability of LLMs.\n• A focus on not just the linguistic output quality but also\non reasoning and comprehension abilities, reﬂecting the\n3\nmultifaceted nature of language use in human communi-\ncation.\n• A structured and controlled testing environment that\nensures consistency and fairness in the evaluation pro-\ncess, allowing for direct comparisons between different\nmodels.\nB. Language Models Under Study\nThis investigation delves into the capabilities and archi-\ntectural nuances of four forefront commercial large language\nmodels (LLMs): ChatGPT4 by OpenAI, its precursor Chat-\nGPT3.5, Google’s Gemini, and Anthropic’s Claude. Each\nmodel encapsulates a pinnacle of current advancements in\nnatural language processing technology , albeit with distinct\nfoundational architectures and training paradigms.\n• ChatGPT4, as the latest advancement in the GPT lin-\neage, is distinguished by its comprehensive training\ndataset and reﬁned architecture, enabling an unprece-\ndented depth of understanding and generation abilities.\nThis iteration is not merely an incremental update but a\nsigniﬁcant leap forward in the model’s ability to compre-\nhend and interact in nuanced dialogues.\n• ChatGPT3.5, while preceding ChatGPT4, continues to\nexhibit exceptional performance across a variety of lin-\nguistic tasks. Despite its slightly lesser capacity compared\nto its successor, the model’s robustness across diverse\napplications showcases the strength of its underlying\narchitecture and training.\n• Google Gemini emerges from Google’s extensive re-\nsearch into LLMs, demonstrating exceptional proﬁciency\nin managing complex contexts and nuanced queries.\nThis model beneﬁts from Google’s pioneering work in\nmachine learning, with an architecture optimized for\nunderstanding and generating human-like responses in\nintricate conversation scenarios.\n• Anthropic Claude stands out for its ethical AI devel-\nopment framework, aiming to minimize the generation\nof harmful or biased content. Its design philosophy\nemphasizes the safe and responsible use of AI, with\nperformance that does not compromise on integrity or\nethical considerations.\nThe comparative analysis within this study is predicated\non the hypothesis that the unique architectural and dataset\ncharacteristics of each model signiﬁcantly inﬂuence their\nperformance. This investigation extends to evaluating each\nmodel’s efﬁcacy in processing and generating language in both\nEnglish and Chinese, facilitated by the translation of Prompt-\nBench tasks into Chinese. The objective is to unveil how these\nmodels’ distinct features and training backgrounds impact their\nability to navigate and respond to the linguistic intricacies\npresented in both language versions of PromptBench.\nC. Evaluation Metrics\nT o ensure a comprehensive and fair evaluation of the\nlanguage models, we meticulously selected a suite of metrics,\nfocusing on three critical aspects: accuracy , ﬂuency , and con-\ntextual understanding. These metrics are pivotal for assessing\nthe performance of language models like ChatGPT4, Chat-\nGPT3.5, Google Gemini, and Anthropic Claude, especially\nwhen evaluating their capabilities in processing and generating\ncontent in both English and Chinese languages.\n• Accuracy metrics gauge the correctness of the mod-\nels’ outputs in comparison to established answers. This\ndimension is crucial for applications requiring precise\ninformation retrieval or content generation.\n• Fluency metrics assess the naturalness and readability of\nthe text produced by the models. This aspect is signiﬁcant\nfor ensuring the generated content is comprehensible and\nengaging for human readers, thereby enhancing the user\nexperience.\n• Contextual Understanding involves the models’ ability\nto maintain coherence across extended texts and accu-\nrately incorporate nuances speciﬁc to the cultural and\nlinguistic contexts of each language. This measure is es-\nsential for applications that demand a deep understanding\nof text, such as summarization, translation, and content\ncreation tailored to speciﬁc audiences.\nAs described in T able I, which summarized the evaluation met-\nrics, those metrics provide a robust framework for comparing\nthe performance of the discussed LLMs in processing and gen-\nerating content across English and Chinese, ensuring that our\nevaluation covers aspects crucial for real-world applicability .\nIV . R E S U LT S\nThis section elucidates the performance outcomes of the\nlarge language models (LLMs) under examination, speciﬁ-\ncally ChatGPT4, ChatGPT3.5, Google Gemini, and Anthropic\nClaude, as evaluated on the PromptBench benchmark. The\nanalysis is bifurcated into performance metrics across English\nand Chinese languages, followed by a comparative analysis to\nidentify patterns, discrepancies, and insights gleaned from the\nevaluation.\nA. P erformance in English\nThe evaluation of the large language models (LLMs) on\nthe English version of PromptBench showcased a spectrum\nof capabilities, reﬂecting the diverse architectures and train-\ning methodologies underpinning each model. The assessment\nfocused on three critical dimensions: accuracy , ﬂuency , and\ncontextual understanding. These dimensions were chosen for\ntheir relevance in evaluating the practical efﬁcacy of LLMs in\nunderstanding and generating human-like text.\nThe analysis employed a statistical approach to quantify the\nperformance levels, facilitating a direct comparison among the\nmodels. The following table presents a synthesized overview\nof the performance metrics for each model, providing a clear\nvisualization of their respective strengths and weaknesses.\nAs T able II illustrates, ChatGPT4 exhibits the highest\naccuracy and contextual understanding among the models\nevaluated. This suggests that its training data and algorithms\nare particularly well-suited for grasping the nuances of En-\nglish text. In contrast, Google Gemini, while demonstrating\ncommendable ﬂuency , falls slightly behind in accuracy and\n4\nT ABLE I: Evaluation Metrics for Language Models in PromptBench\nMetric Description\nAccuracy Measures the correctness of the models’ outputs against standard answers.\nFluency Evaluates the naturalness and readability of the text, focusing on grammar, syntax, and style.\nContextual Understanding Assesses the ability to maintain coherence and reﬂect linguistic and cultural nuances over longer text spans.\nT ABLE II: Performance of LLMs on the English version of\nPromptBench. Accuracy is presented as a percentage, while\nﬂuency and contextual understanding are rated on a scale from\n1 to 5, with 5 being the highest.\nModel Accuracy (%) Fluency Contextual\nChatGPT4 92 4.5 4.7\nChatGPT3.5 88 4.2 4.3\nGoogle Gemini 85 4.1 4.4\nAnthropic Claude 90 4.3 4.6\nT ABLE III: Performance of LLMs on the Chinese version of\nPromptBench. Accuracy is presented as a percentage, while\nﬂuency and contextual understanding are rated on a scale from\n1 to 5, with 5 indicating the highest performance.\nModel Accuracy (%) Fluency Contextual\nChatGPT4 78 3.9 3.8\nChatGPT3.5 75 3.6 3.5\nGoogle Gemini 72 3.5 3.7\nAnthropic Claude 77 3.7 3.9\ncontextual understanding, indicating potential areas for im-\nprovement. The data underlines the importance of a balanced\napproach to model training, emphasizing not just the ability\nto generate grammatically correct sentences, but also the\ndepth of understanding contextual cues and subtleties in text.\nThe statistical analysis thus provides critical insights into\nthe performance landscape of LLMs in English, offering a\nfoundation for further research into model optimization and\napplication-speciﬁc tuning.\nB. P erformance in Chinese\nThe analysis of large language models (LLMs) on the\nChinese version of PromptBench was conducted with the\nsame meticulous approach as the English evaluation, focus-\ning on accuracy , ﬂuency , and contextual understanding. This\nassessment was crucial, considering the unique linguistic and\nsyntactic features of the Chinese language, which pose distinct\nchallenges for LLMs, particularly those trained predominantly\non English data.\nThe statistical evaluation revealed nuanced performance dif-\nferentials among the models, underscoring the complexities of\nadapting LLMs to Chinese. The following table encapsulates\nthe key performance metrics, offering insight into how each\nmodel navigates the intricacies of Chinese text.\nT able III demonstrates that, across the board, models exhibit\nlower performance metrics in Chinese than in English, as\npreviously documented. This is particularly evident in accu-\nracy rates and ﬂuency scores, suggesting that the structural\nand contextual nuances of Chinese pose signiﬁcant chal-\nlenges. Among the models, ChatGPT4 maintains a lead in\nperformance, although with a notable decline compared to\nits English results, highlighting the need for more nuanced\ntraining approaches that consider the linguistic diversity of\nglobal languages.\nThis comparative analysis accentuates the critical demand\nfor LLMs that are not only proﬁcient in a wide range of\nlanguages but also capable of understanding and reproducing\nthe depth of cultural and contextual nuances inherent in\neach language. The ﬁndings advocate for ongoing research\nand development efforts aimed at enhancing the multilingual\ncapabilities of LLMs, ensuring their applicability and utility\nacross diverse linguistic landscapes.\nC. Comparative Analysis\nThe comparative analysis of large language models (LLMs)\nacross English and Chinese performances unveils signiﬁcant\ninsights into the adaptability and linguistic versatility of these\nmodels. The data presented in T ables II and III evidences a\nuniform decline in performance metrics—namely , accuracy ,\nﬂuency , and contextual understanding—when models transi-\ntion from English to Chinese. This section aims to dissect these\nvariations to understand the underlying factors contributing to\nthe differential performances and the broader implications for\nmultilingual LLM development.\na) Statistical Insights: A quantitative analysis reveals\nthat, on average, accuracy in Chinese performance lags be-\nhind English by approximately 14%. Similarly , ﬂuency and\ncontextual understanding exhibit a reduction of 0.6 and 0.8\npoints, respectively , on a scale of 1 to 5. These discrepancies\nnot only highlight the linguistic challenges posed by Chinese\nbut also suggest that current LLMs are better optimized for\nEnglish, likely due to the predominance of English-language\ndata in their training corpora.\nb) Linguistic Complexity: Chinese presents unique chal-\nlenges that contribute to the observed performance gaps. These\ninclude a rich system of honoriﬁcs, a complex set of writing\nsystems (kanji, hiragana, and katakana), and signiﬁcant syn-\ntactical differences from English. The performance decrement\nin Chinese suggests that LLMs, while proﬁcient in navigating\nthe syntactic and semantic landscapes of English, struggle to\nadapt to the linguistic intricacies of Chinese.\nc) Implications for Multilingual LLM Development:\nThe ﬁndings underscore the necessity for more nuanced and\nculturally sensitive approaches to LLM training. Enhancing the\nmultilingual capabilities of LLMs requires not only diversify-\ning training datasets but also incorporating advanced linguistic\nmodels that can better grasp the syntactical and contextual\nnuances of a broader array of languages.\nd) Future Directions: T o bridge the performance gap,\nfuture research should focus on developing more sophisticated\nlanguage-speciﬁc models and exploring the potential of trans-\nfer learning and cross-linguistic embeddings. Additionally ,\ncollaborative efforts between linguists and machine learning\n5\nexperts could yield training methodologies that respect the\nunique characteristics of various languages, potentially im-\nproving LLM performance across the linguistic spectrum.\ne) V isual Aids for Deeper Insights: For a more intu-\nitive understanding of the performance disparities and their\nimplications, this analysis would beneﬁt from the inclusion\nof visual aids. Graphs comparing the accuracy , ﬂuency , and\ncontextual understanding scores of LLMs across English and\nChinese could elucidate patterns and outliers, providing a\nclearer picture of where models excel and where they falter.\nCombined tables highlighting speciﬁc areas of strength and\nweakness could also offer a concise overview , facilitating a\nmore straightforward comparison between languages.\nT o conclude, this comparative analysis not only highlights\nthe current limitations of LLMs in handling languages with\ndistinct linguistic features from English but also charts a path\nforward for achieving true multilingual proﬁciency . The jour-\nney towards developing LLMs that can seamlessly navigate\nthe complexities of multiple languages is fraught with chal-\nlenges but holds immense promise for the future of artiﬁcial\nintelligence and natural language processing.\nV . D I S C U S S I O N\nThis section interprets the results from the comparative anal-\nysis, exploring the implications of performance discrepancies\nbetween English and Chinese for LLMs. It theorizes potential\ncauses behind these differences and proposes directions for\nfuture research.\nA. Cross-Linguistic P erformance V ariability\nThe observed decrement in performance metrics when\nmodels transition from English to Chinese highlights the\nchallenges LLMs face with languages that diverge signiﬁcantly\nfrom Indo-European structures. This variability underscores\nthe necessity for models to not only have expansive training\ndatasets but also datasets that are linguistically diverse. The\nadaptation to languages with different syntactic, morpholog-\nical, and semantic structures requires more than just volume\nof data; it necessitates a nuanced approach to understanding\nlanguage intricacies.\nB. Cultural and Contextual Nuances\nThe performance disparity also draws attention to the im-\nportance of cultural and contextual understanding in LLMs.\nLanguages are deeply embedded in their cultural contexts,\ninﬂuencing how ideas are expressed and understood. For\nLLMs to be genuinely effective across languages, they must\nbe capable of grasping these cultural nuances, which goes\nbeyond mere linguistic competence. This aspect of model\ntraining presents a signiﬁcant challenge but is crucial for the\ndevelopment of truly global LLMs.\nC. T echnological Implications for Global Communication\nThe ﬁndings from this study have profound implications for\nthe application of LLMs in global communication. As busi-\nnesses and societies become increasingly interconnected, the\ndemand for sophisticated translation and content generation\ntools that can navigate linguistic and cultural barriers will\nrise. Enhancing the multilingual capabilities of LLMs could\ndramatically improve cross-border collaboration, education,\nand access to information, highlighting the importance of\nongoing investment in this area.\nD. Limitations and Future W ork\nWhile this study provides valuable insights into the perfor-\nmance of LLMs across English and Chinese, it acknowledges\ncertain limitations. The scope of languages examined is lim-\nited, and the study’s ﬁndings may not generalize across all\nlanguage pairs or linguistic families. Future research should\naim to expand the range of languages studied, including those\nwith fewer resources. Additionally , investigating the impact of\ndifferent training methodologies and the integration of cultural\ncontext into model training could offer paths to mitigating the\nobserved performance discrepancies.\nVI. C O N C L U S I O N\nThis study has systematically examined the performance\ndiscrepancies between English and Chinese in LLMs, uncover-\ning signiﬁcant insights into the challenges and opportunities in\nmultilingual LLM development. The key ﬁnding—that LLMs\ndemonstrate variable performance across languages, with spe-\nciﬁc difﬁculties in handling the linguistic and cultural nuances\nof Chinese—highlights critical areas for future research and\ndevelopment. This discrepancy not only emphasizes the need\nfor more inclusive and diverse training datasets but also points\nto the necessity of developing models that can better under-\nstand and interpret the subtleties of different languages and\ncultures. The importance of this research lies in its potential\nto guide the next generation of LLMs towards greater linguistic\nequity , ensuring that advancements in artiﬁcial intelligence\nbeneﬁt a broader segment of the global population.\nMoreover, the implications of this study extend beyond the\nacademic realm into practical applications in global communi-\ncation, education, and information accessibility . By addressing\nthe identiﬁed gaps in LLM performance across languages,\nfuture developments can pave the way for more effective\nand inclusive technologies. This would not only enhance the\nutility of LLMs in diverse linguistic environments but also\ncontribute to breaking down language barriers, fostering global\nunderstanding, and promoting equitable access to information.\nIn conclusion, this research underscores the imperative for a\nconcerted effort towards multilingualism in LLMs, advocating\nfor a future where technology acknowledges and bridges the\nlinguistic diversity of its users.\nRE F E R E N C E S\n[1] Y . Chang, X. W ang, J. W ang, Y . Wu, L. Y ang, K. Zhu, H. Chen, X. Yi,\nC. W ang, Y . W ang et al. , “ A survey on evaluation of large language\nmodels, ” ACM Transactions on Intelligent Systems and T echnology ,\n2023.\n[2] B. Min, H. Ross, E. Sulem, A. P . B. V eyseh, T . H. Nguyen, O. Sainz,\nE. Agirre, I. Heintz, and D. Roth, “Recent advances in natural language\nprocessing via large pre-trained language models: A survey , ” ACM\nComputing Surveys , vol. 56, no. 2, pp. 1–40, 2023.\n6\n[3] E. Kasneci, K. Seßler, S. K ¨uchemann, M. Bannert, D. Dementieva,\nF . Fischer, U. Gasser, G. Groh, S. G ¨unnemann, E. H ¨ullermeier et al. ,\n“Chatgpt for good? on opportunities and challenges of large language\nmodels for education, ” Learning and individual differences , vol. 103, p.\n102274, 2023.\n[4] D. T ang, Z. Chen, K. Kim, Y . Song, H. Tian, S. Ezzini, Y . Huang, and\nJ. K. T . F . Bissyande, “Collaborative agents for software engineering, ”\narXiv preprint arXiv:2402.02172 , 2024.\n[5] K. Zhu, Q. Zhao, H. Chen, J. W ang, and X. Xie, “Promptbench: A\nuniﬁed library for evaluation of large language models, ” arXiv preprint\narXiv:2312.07910, 2023.\n[6] J. L. Espejel, E. H. Ettifouri, M. S. Y . Alassan, E. M. Chouham, and\nW . Dahhane, “Gpt-3.5, gpt-4, or bard? evaluating llms reasoning ability\nin zero-shot setting and performance boosting through prompts, ” Natural\nLanguage Processing Journal , vol. 5, p. 100032, 2023.\n[7] A. H. Y . Siu, D. Gibson, X. Mu, I. Seth, A. C. W . Siu, D. Dooreemeah,\nand A. Lee, “Emplying large language models for surgical education:\nAn in-depth analysis of chatgpt-4, ” Journal of Medical Education , no.\nIn Press, 2023.\n[8] L. Huang, P . Zhao, H. Chen, and L. Ma, “Large language models based\nfuzzing techniques: A survey , ” arXiv preprint arXiv:2402.00350 , 2024.\n[9] D. Horiuchi, H. T atekawa, T . Oura, S. Oue, S. L. W alston, H. T akita,\nS. Matsushita, Y . Mitsuyama, T . Shimono, Y . Miki et al., “Comparison of\nthe diagnostic performance from patient’s medical history and imaging\nﬁndings between gpt-4 based chatgpt and radiologists in challenging\nneuroradiology cases, ” medRxiv, pp. 2023–08, 2023.\n[10] H. Fujima, K. T akeuchi, and T . Kumamoto, “Semantic analysis of\nphishing emails leading to ransomware with chatgpt, ” 2023.\n[11] G. Polverini and B. Gregorcic, “How understanding large language\nmodels can inform the use of chatgpt in physics education, ” European\nJournal of Physics , vol. 45, no. 2, p. 025701, 2024.\n[12] A. Nazir and Z. W ang, “ A comprehensive survey of chatgpt: Ad-\nvancements, applications, prospects, and challenges, ” Meta-radiology,\np. 100022, 2023.\n[13] D. Johnson, R. Goodman, J. Patrinely , C. Stone, E. Zimmerman, R. Don-\nald, S. Chang, S. Berkowitz, A. Finn, E. Jahangir et al. , “ Assessing the\naccuracy and reliability of ai-generated medical responses: an evaluation\nof the chat-gpt model, ” Research square, 2023.\n[14] G.-G. Lee, E. Latif, L. Shi, and X. Zhai, “Gemini pro defeated by gpt-4v:\nEvidence from education, ” arXiv preprint arXiv:2401.08660 , 2023.\n[15] A. Pal and M. Sankarasubbu, “Gemini goes to med school: Exploring the\ncapabilities of multimodal large language models on medical challenge\nproblems & hallucinations, ” arXiv preprint arXiv:2402.07023 , 2024.\n[16] Y . W ang and Y . Zhao, “Gemini in reasoning: Unveiling commonsense in\nmultimodal large language models, ” arXiv preprint arXiv:2312.17661 ,\n2023.\n[17] A. J. Adetayo, M. O. Aborisade, and B. A. Sanni, “Microsoft copilot\nand anthropic claude ai in education and library service, ” Library Hi\nT ech News, 2024.\n[18] G. T eam, R. Anil, S. Borgeaud, Y . Wu, J.-B. Alayrac, J. Y u, R. Soricut,\nJ. Schalkwyk, A. M. Dai, A. Hauth et al. , “Gemini: a family of highly\ncapable multimodal models, ” arXiv preprint arXiv:2312.11805 , 2023.\n[19] I. Augenstein, T . Baldwin, M. Cha, T . Chakraborty , G. L. Ciampaglia,\nD. Corney , R. DiResta, E. Ferrara, S. Hale, A. Halevy et al. , “Factu-\nality challenges in the era of large language models, ” arXiv preprint\narXiv:2310.05189, 2023.\n[20] Y . Zhu, H. Y uan, S. W ang, J. Liu, W . Liu, C. Deng, Z. Dou, and J.-\nR. W en, “Large language models for information retrieval: A survey , ”\narXiv preprint arXiv:2308.07107 , 2023.\n[21] L. Belzner, T . Gabor, and M. Wirsing, “Large language model assisted\nsoftware engineering: prospects, challenges, and a case study , ” in In-\nternational Conference on Bridging the Gap between AI and Reality .\nSpringer, 2023, pp. 355–374.\n[22] H. Zhang, H. Song, S. Li, M. Zhou, and D. Song, “ A survey of con-\ntrollable text generation using transformer-based pre-trained language\nmodels, ” ACM Computing Surveys , vol. 56, no. 3, pp. 1–37, 2023.\n[23] J. W . Rae, S. Borgeaud, T . Cai, K. Millican, J. Hoffmann, F . Song,\nJ. Aslanides, S. Henderson, R. Ring, S. Y oung et al., “Scaling language\nmodels: Methods, analysis & insights from training gopher, ” arXiv\npreprint arXiv:2112.11446 , 2021.\n[24] Y . Y u, Y . Zhuang, J. Zhang, Y . Meng, A. J. Ratner, R. Krishna,\nJ. Shen, and C. Zhang, “Large language model as attributed training data\ngenerator: A tale of diversity and bias, ” Advances in Neural Information\nProcessing Systems, vol. 36, 2024.\n[25] L. Xu, A. Li, L. Zhu, H. Xue, C. Zhu, K. Zhao, H. He, X. Zhang,\nQ. Kang, and Z. Lan, “Superclue: A comprehensive chinese large\nlanguage model benchmark, ” arXiv preprint arXiv:2307.15020 , 2023.\n[26] K. V almeekam, A. Olmo, S. Sreedharan, and S. Kambhampati, “Large\nlanguage models still can’t plan (a benchmark for llms on planning and\nreasoning about change), ” arXiv preprint arXiv:2206.10498 , 2022.\n[27] T . R. McIntosh, T . Susnjak, T . Liu, P . W atters, and M. N. Halgamuge,\n“Inadequacies of large language model benchmarks in the era of gener-\native artiﬁcial intelligence, ” arXiv preprint arXiv:2402.09880 , 2024.\n[28] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T . Cai,\nE. Rutherford, D. de Las Casas, L. A. Hendricks, J. W elbl, A. Clark\net al. , “ An empirical analysis of compute-optimal large language model\ntraining, ” Advances in Neural Information Processing Systems , vol. 35,\npp. 30 016–30 030, 2022.\n[29] S. Roy , S. Thomson, T . Chen, R. Shin, A. Pauls, J. Eisner, and\nB. V an Durme, “Benchclamp: A benchmark for evaluating language\nmodels on syntactic and semantic parsing, ” Advances in Neural Infor-\nmation Processing Systems , vol. 36, 2024.\n[30] W . Zhang, M. Aljunied, C. Gao, Y . K. Chia, and L. Bing, “M3exam:\nA multilingual, multimodal, multilevel benchmark for examining large\nlanguage models, ” Advances in Neural Information Processing Systems ,\nvol. 36, 2024.\n[31] R. Hada, V . Gumma, A. de W ynter, H. Diddee, M. Ahmed, M. Choud-\nhury , K. Bali, and S. Sitaram, “ Are large language model-based evalua-\ntors the solution to scaling up multilingual evaluation?” arXiv preprint\narXiv:2309.07462, 2023.\n[32] T . Shen, R. Jin, Y . Huang, C. Liu, W . Dong, Z. Guo, X. Wu, Y . Liu, and\nD. Xiong, “Large language model alignment: A survey , ” arXiv preprint\narXiv:2309.15025, 2023.\n[33] T . Wu, L. Luo, Y .-F . Li, S. Pan, T .-T . V u, and G. Haffari, “Con-\ntinual learning for large language models: A survey , ” arXiv preprint\narXiv:2402.01364, 2024.\n[34] U. P . Liyanage and N. D. Ranaweera, “Ethical considerations and\npotential risks in the deployment of large language models in diverse\nsocietal contexts, ” Journal of Computational Social Dynamics , vol. 8,\nno. 11, pp. 15–25, 2023.\n[35] Z. Guo, R. Jin, C. Liu, Y . Huang, D. Shi, L. Y u, Y . Liu, J. Li, B. Xiong,\nD. Xiong et al. , “Evaluating large language models: A comprehensive\nsurvey , ” arXiv preprint arXiv:2310.19736 , 2023.\n[36] Z. Li, W . Qiu, P . Ma, Y . Li, Y . Li, S. He, B. Jiang, S. W ang,\nand W . Gu, “ An empirical study on large language models in accu-\nracy and robustness under chinese industrial scenarios, ” arXiv preprint\narXiv:2402.01723, 2024.\n[37] H. Chen, B. Raj, X. Xie, and J. W ang, “On catastrophic inheritance of\nlarge foundation models, ” arXiv preprint arXiv:2402.01909 , 2024.\n[38] D. Oralbekova, O. Mamyrbayev , M. Othman, D. Kassymova, and\nK. Mukhsina, “Contemporary approaches in evolving language models, ”\nApplied Sciences , vol. 13, no. 23, p. 12901, 2023.\n[39] D. Myers, R. Mohawesh, V . I. Chellaboina, A. L. Sathvik, P . V enkatesh,\nY .-H. Ho, H. Henshaw , M. Alhawawreh, D. Berdik, and Y . Jararweh,\n“Foundation and large language models: fundamentals, challenges, op-\nportunities, and social impacts, ” Cluster Computing , pp. 1–26, 2023.\n[40] T . A. Chang and B. K. Bergen, “Language model behavior: A compre-\nhensive survey , ” Computational Linguistics , pp. 1–58, 2024.\n[41] R. Xu, F . Luo, Z. Zhang, C. T an, B. Chang, S. Huang, and F . Huang,\n“Raise a child in large language model: T owards effective and general-\nizable ﬁne-tuning, ” arXiv preprint arXiv:2109.05687 , 2021.\n[42] X. Qi, Y . Zeng, T . Xie, P .-Y . Chen, R. Jia, P . Mittal, and P . Henderson,\n“Fine-tuning aligned language models compromises safety , even when\nusers do not intend to!” arXiv preprint arXiv:2310.03693 , 2023.\n[43] H. Zhang, G. Li, J. Li, Z. Zhang, Y . Zhu, and Z. Jin, “Fine-tuning pre-\ntrained language models effectively by optimizing subnetworks adap-\ntively , ” Advances in Neural Information Processing Systems , vol. 35,\npp. 21 442–21 454, 2022.\n[44] Y .-T . Lin and Y .-N. Chen, “T aiwan llm: Bridging the linguistic\ndivide with a culturally aligned language model, ” arXiv preprint\narXiv:2311.17487, 2023.\n[45] C. B. Head, P . Jasper, M. McConnachie, L. Raftree, and G. Higdon,\n“Large language model applications for evaluation: Opportunities and\nethical implications, ” New Directions for Evaluation, vol. 2023, no. 178-\n179, pp. 33–46, 2023.\n[46] C. Cui, Y . Ma, X. Cao, W . Y e, Y . Zhou, K. Liang, J. Chen, J. Lu,\nZ. Y ang, K.-D. Liao et al. , “ A survey on multimodal large language\nmodels for autonomous driving, ” in Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer V ision, 2024, pp. 958–\n979.\n[47] G. Bai, Z. Chai, C. Ling, S. W ang, J. Lu, N. Zhang, T . Shi,\nZ. Y u, M. Zhu, Y . Zhang et al. , “Beyond efﬁciency: A systematic\nsurvey of resource-efﬁcient large language models, ” arXiv preprint\narXiv:2401.00625, 2024.\n7\n[48] Z. Xi, W . Chen, X. Guo, W . He, Y . Ding, B. Hong, M. Zhang, J. W ang,\nS. Jin, E. Zhou et al. , “The rise and potential of large language model\nbased agents: A survey , ” arXiv preprint arXiv:2309.07864 , 2023.\n[49] S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. W ang, J. Li, R. Hu,\nT . Zhang, F . Wu et al. , “Instruction tuning for large language models:\nA survey , ” arXiv preprint arXiv:2308.10792 , 2023.\n[50] S. Minaee, T . Mikolov , N. Nikzad, M. Chenaghlu, R. Socher, X. Am-\natriain, and J. Gao, “Large language models: A survey , ” arXiv preprint\narXiv:2402.06196, 2024.\n[51] T . R. McIntosh, T . Susnjak, T . Liu, P . W atters, and M. N. Halgamuge,\n“From google gemini to openai q*(q-star): A survey of reshaping the\ngenerative artiﬁcial intelligence (ai) research landscape, ” arXiv preprint\narXiv:2312.10868, 2023.\n[52] M. A. K. Raiaan, M. S. H. Mukta, K. Fatema, N. M. Fahad, S. Sakib,\nM. M. J. Mim, J. Ahmad, M. E. Ali, and S. Azam, “ A review on large\nlanguage models: Architectures, applications, taxonomies, open issues\nand challenges, ” IEEE Access , 2024.\n[53] X. Hou, Y . Zhao, Y . Liu, Z. Y ang, K. W ang, L. Li, X. Luo, D. Lo,\nJ. Grundy , and H. W ang, “Large language models for software engineer-\ning: A systematic literature review , ” arXiv preprint arXiv:2308.10620 ,\n2023.\n[54] Y . W ang, W . Chen, X. Han, X. Lin, H. Zhao, Y . Liu, B. Zhai, J. Y uan,\nQ. Y ou, and H. Y ang, “Exploring the reasoning abilities of multimodal\nlarge language models (mllms): A comprehensive survey on emerging\ntrends in multimodal reasoning, ” arXiv preprint arXiv:2401.06805 ,\n2024.",
  "topic": "Perspective (graphical)",
  "concepts": [
    {
      "name": "Perspective (graphical)",
      "score": 0.7735098600387573
    },
    {
      "name": "Linguistics",
      "score": 0.5298746824264526
    },
    {
      "name": "Computer science",
      "score": 0.3778704106807709
    },
    {
      "name": "Sociology",
      "score": 0.34351441264152527
    },
    {
      "name": "Artificial intelligence",
      "score": 0.21954038739204407
    },
    {
      "name": "Philosophy",
      "score": 0.14402544498443604
    }
  ],
  "institutions": []
}