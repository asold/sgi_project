{
  "title": "Are Pre-trained Language Models Knowledgeable to Ground Open Domain Dialogues?",
  "url": "https://openalex.org/W3102184948",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1593870275",
      "name": "Zhao Yu-fan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2086655376",
      "name": "Wu Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2027875661",
      "name": "Xu Can",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963475460",
    "https://openalex.org/W1591706642",
    "https://openalex.org/W2951508633",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2783549597",
    "https://openalex.org/W1975879668",
    "https://openalex.org/W3037026762",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2328886022",
    "https://openalex.org/W3007008027",
    "https://openalex.org/W2971236040",
    "https://openalex.org/W2914204778",
    "https://openalex.org/W2963360026",
    "https://openalex.org/W2950457956",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2949681671",
    "https://openalex.org/W2913755428",
    "https://openalex.org/W2584185835",
    "https://openalex.org/W2996227762",
    "https://openalex.org/W2962896208"
  ],
  "abstract": "We study knowledge-grounded dialogue generation with pre-trained language models. Instead of pursuing new state-of-the-art on benchmarks, we try to understand if the knowledge stored in parameters of the pre-trained models is already enough to ground open domain dialogues, and thus allows us to get rid of the dependency on external knowledge sources in generation. Through extensive experiments on benchmarks, we find that by fine-tuning with a few dialogues containing knowledge, the pre-trained language models can outperform the state-of-the-art model that requires external knowledge in automatic evaluation and human judgment, suggesting a positive answer to the question we raised.",
  "full_text": "arXiv:2011.09708v1  [cs.CL]  19 Nov 2020\nAre Pre-trained Language Models Knowledgeable to Ground Open\nDomain Dialogues?\nY ufan Zhao1, Wei Wu2, Can Xu1,\n1Microsoft Corporation, Beijing, China\n2Meituan, Beijing, China\n{yufzhao,caxu}@microsoft.com\nwuwei19850318@gmail.com\nAbstract\nW e study knowledge-grounded dialogue gen-\neration with pre-trained language models. In-\nstead of pursuing new state-of-the-art on\nbenchmarks, we try to understand if the knowl-\nedge stored in parameters of the pre-trained\nmodels is already enough to ground open do-\nmain dialogues, and thus allows us to get\nrid of the dependency on external knowledge\nsources in generation. Through extensive ex-\nperiments on benchmarks, we ﬁnd that by\nﬁne-tuning with a few dialogues containing\nknowledge, the pre-trained language models\ncan outperform the state-of-the-art model that\nrequires external knowledge in automatic eval-\nuation and human judgment, suggesting a pos-\nitive answer to the question we raised.\n1 Introduction\nWhile techniques of open domain dialogue gen-\neration (\nVinyals and Le , 2015; Xing et al. , 2017;\nZhang et al. , 2019a) have been applied in in-\ndustrial products ( Shum et al. , 2018; Ram et al. ,\n2018), people can still feel the gap between the\ndialogue systems and humans, especially when\nthey dive into a speciﬁc topic of interest. T o\nbridge the gap, researchers consider grounding\nopen domain dialogues by external knowledge\nwhich could be retrieved either from structured\nknowledge bases (\nZhou et al. , 2018a; Moon et al. ,\n2019; Tuan et al. , 2019) or from unstructured doc-\numents ( Dinan et al. , 2019). Dialogue generation\nnow is based on both conversation contexts and\nthe external knowledge which hints the generation\nmodel how to go deep for the topic in discussion.\nIn this work, we investigate if a large scale\npre-trained language model can instead serve as\na knowledge base in open domain dialogue gener-\nation. The work is motivated by two lines of re-\nsearch emerging recently: (1) interestingly , some\nrecent studies indicate that pre-trained language\nmodels have packed enough knowledge in their\nparameters, and thus they can do a good job in\nquestion-answering tasks without the need of ac-\ncess to external knowledge (\nPetroni et al. , 2019;\nRoberts et al. , 2020). Thus, we are curious if sim-\nilar results can be achieved in open domain di-\nalogue generation. If the answer is “yes”, then\nwe can get rid of the dependency on external\nknowledge sources, and obtain a simpler and more\nﬂexible architecture with a better generalization\nability inherited from pre-training with massive\ntext corpus (\nRadford et al. , 2019); and (2) pre-\ntraining techniques have exhibited compelling per-\nformance on the task of open domain dialogue\ngeneration (\nZhang et al. , 2019b; W olf et al. , 2019).\nParticularly , a recent paper ( Zhang et al. , 2019b)\nhas demonstrated with examples that a pre-trained\ngeneration model can reply with commonsense\nknowledge. Therefore, it is interesting to fur-\nther explore to what extend a pre-trained language\nmodel can keep conversation smooth, knowledge-\nable, and reasonable with in-depth analysis.\nThe test beds are benchmarks of knowledge-\ngrounded dialogue generation, including Wizard\nof Wikipedia (Wizard) (\nDinan et al. , 2019), CMU\nDocument Grounded Conversations (CMU DoG)\n(Zhou et al. , 2018b), and T opical-Chat (TC)\n(Gopalakrishnan et al. , 2019), in which we dis-\ncard the external knowledge passages. The\nremaining dialogues are basically deep discus-\nsions about speciﬁc topics between two partici-\npants, and thus are suitable probes. W e choose\nDialoGPT (\nZhang et al. , 2019b), GPT -2 finetune ,\nand DialoGPT finetune as the pre-trained language\nmodels for investigation, where GPT -2 finetune\nand DialoGPT finetune refer to the OpenAI GPT -2\nmodel ( Radford et al. , 2019) and DialoGPT ﬁne-\ntuned on the training data of the benchmarks re-\nspectively . Evaluation results on both automatic\nmetrics and human judgment indicate that with-\nout the aid of external knowledge, the ﬁne-tuned\nmodels are still capable of replying with proper\nand speciﬁc content based on the knowledge en-\ncoded in its parameters, though sometimes they\nmay make mistakes on details.\nOur contributions include: (1) the ﬁrst system-\natic study on the possibility of using a pre-trained\nlanguage model as a knowledge base to ground\nopen domain dialogues; and (2) insights and ideas\nfor future work from extensive experiments.\n2 Pre-trained Language Models\nW e choose GPT -2 as the backbone of the pre-\ntrained language models. Though the models have\nexhibited strong performance on a variety of lan-\nguage generation tasks (\nRadford et al. , 2019), they\nare not suitable for dialogue generation under a\nzero-shot setting, as the original models often syn-\nthesize a long paragraph after a conversation con-\ntext. For example, on the test sets of Wizard,\nwe observe that “responses” generated by the 24-\nlayer GPT -2 are made up of 402 tokens on average,\nwhich cannot be regarded as conversational replies\nany more\n1 . Therefore, we alternatively consider\nthe following variants of GPT -2 which are well\nadapted to dialogues.\nDialoGPT . The model follows the architecture\nof OpenAI GPT -2, and is trained (either from\nscratch or from OpenAI GPT -2) with 147M\nReddit dialogues (\nZhang et al. , 2019b). W e\nchoose the model trained from OpenAI GPT -\n2 with 345M parameters, as it shows the best\nperformance in the evaluation in (\nZhang et al. ,\n2019b). The model is implemented based on\nthe code shared at https://github.com/\nmicrosoft/DialoGPT.\nGPT -2finetune . W e ﬁne-tune the OpenAI GPT -2\nmodel with 345M parameters on the training data\nof the benchmarks. The model removes the inﬂu-\nence of the Reddit data, and the ﬁne-tuning step\nbiases the distribution of language to the conversa-\ntions of the crowd-workers who participate in the\nconstruction of the benchmarks.\nDialoGPTfinetune . W e further ﬁne-tune Di-\naloGPT ( 345M) with the training data of the\n1 W e have tried several heuristics, such as setting up an up-\nper bound for the length of the responses, as remedies, but\nin general the heuristics will make the responses ungrammat -\nical.\nbenchmarks. Through a comparison with Di-\naloGPT , we can look into how the small training\ndata affects the capability of the pre-trained model\nin terms of responding with knowledge.\n3 Experiments\n3.1 Datasets\nAll the three benchmark datasets are built with\ncrowd-sourcing on Amazon Mechanical Turk, and\nare split into training sets, validation sets, and test\nsets by the data owners. In Wizard and CMU\nDoG,\nknowledge is retrieved from Wikipedia, while in\nTC, besides wiki articles, W ashington Post arti-\ncles and Reddit fun facts are also utilized as the\nknowledge sources. Unlike CMU\nDoG that fo-\ncuses on the movie domain, both Wizard and TC\ncover a wide range of topics from multiple do-\nmains. V arious conﬁgurations are set up to sim-\nulate conversation scenarios in real world. In\nWizard, a wizard tells an apprentice about what\nhe/she learns from the knowledge about a speciﬁc\ntopic. In addition to wizard-apprentice conversa-\ntions, CMU\nDoG also contains conversations be-\ntween two workers who know the background doc-\numents and try to discuss the content in depth. In\nTC, participants play symmetric and asymmetric\nroles according to the knowledge they can access\nunder 5 settings. Dialogues under different con-\nﬁgurations are combined in each dataset, and only\nthe turns where knowledge is accessible are con-\nsidered in response generation. Wizard and TC\nfurther split the test sets into Seen/Frequent and\nUnseen/Rare where the former contains topics fre-\nquently appearing in the training sets and the latter\ncontains topics infrequently or never appearing in\nthe training sets. More details can be found in the\nsupplementary material.\n3.2 Implementation Details\nBoth GPT -2 finetune and DialoGPT finetune are\ntrained on 4 Nvidia V100 machines until there is\nno progress in validation loss. W e adopt Adam\nalgorithm (\nKingma and Ba , 2015) in optimiza-\ntion with a learning rate 1e-5 as in ( Zhang et al. ,\n2019b). For all the three models, we use the vocab-\nulary in OpenAI GPT -2 and DialoGPT that con-\nsists of 50,257 entries. The latest 128 tokens in a\ncontext are kept as input.\nCase-1\nContext Utter1: My favorite color is blue! It is important when considering things like decoration and art.\nUtter2: Blue is a great color especially in the summer\nDialoGPT It is !\nCase-2\nContext Utter1: Do you like Elvis Presley? he was a famous american singer\nUtter2: He is pretty good, I am from Nashville, So i’ve even been to gr aceland.\nDialoGPTf inetune. He died in 1969, where did you live when you visited? (wrong time)\nGPTf inetune. He moved to Nashville, T ennessee when he was 14 in 1926. (wrong time)\nCase-3\nContext\nUtter1: I am an accountant. Thanks to the establishment of the Itali an mathematician Luca Pacioli\nin 1494.\nUtter2: Oh? How was Luca Pacioli instrumental in the establishment of accountancy?\nGPTf inetune. He developed the concept of accounting in 1795. I am the custo mer of the banks and I provide the\nservice of being in the control of account receivables and in ventory .\nT able 1: Case study.\nDataset Model PPL F1 BLEU-1 BLEU-2 BLEU-3 BLEU-4 A verage Ext rema Greedy\nWizard-of-Wikipedia(T est Seen)\nTMN 66.5 15.9 0.184 0.073 0.033 0.017 0.844 0.427 0.658\nDRD 19.4 19.3 0.229 0.112 0.066 0.044 0.864 0.455 0.679\nDialoGPT . 84.0 8.4 0.073 0.020 0.006 0.003 0.797 0.363 0.603\nDialoGPTfinetune . 16.2 19.0 0.165 0.076 0.040 0.023 0.871 0.461 0.683\nGPT -2finetune . 15.0 14.4 0.140 0.049 0.020 0.010 0.860 0.417 0.649\nWizard-of-Wikipedia(T est Unseen)\nTMN 103.6 14.3 0.168 0.057 0.022 0.009 0.839 0.408 0.645\nDRD 23.0 17.9 0.221 0.102 0.057 0.037 0.862 0.444 0.671\nDialoGPT . 85.9 8.1 0.071 0.019 0.006 0.002 0.792 0.362 0.596\nDialoGPTfinetune . 20.4 17.6 0.158 0.067 0.032 0.017 0.869 0.451 0.674\nGPT -2finetune . 18.9 13.8 0.139 0.047 0.019 0.008 0.859 0.411 0.642\nCMU-DoG\nTMN 75.2 9.9 0.115 0.040 0.016 0.007 0.789 0.399 0.615\nDRD 54.4 10.7 0.150 0.057 0.025 0.012 0.809 0.413 0.633\nDialoGPT . 73.4 6.9 0.091 0.022 0.006 0.002 0.762 0.358 0.576\nDialoGPTfinetune . 15.9 13.7 0.161 0.064 0.030 0.015 0.812 0.430 0.641\nGPT -2finetune . 16.5 9.4 0.124 0.038 0.014 0.006 0.780 0.382 0.597\nT opic-Chat-Freq\nTMN 30.3 16.5 0.176 0.079 0.041 0.025 0.891 0.444 0.693\nDRD 25.9 15.2 0.203 0.088 0.050 0.033 0.893 0.408 0.681\nDialoGPT . 87.6 8.3 0.074 0.018 0.005 0.002 0.842 0.380 0.630\nDialoGPTfinetune . 13.2 17.1 0.161 0.069 0.036 0.020 0.906 0.446 0.705\nGPT -2finetune . 13.4 13.6 0.136 0.047 0.021 0.011 0.892 0.411 0.674\nT opic-Chat-Rare\nTMN 52.1 14.6 0.168 0.068 0.031 0.016 0.881 0.429 0.682\nDRD 28.0 15.1 0.190 0.083 0.046 0.030 0.874 0.398 0.667\nDialoGPT . 87.9 8.5 0.071 0.018 0.005 0.002 0.835 0.372 0.623\nDialoGPTfinetune . 15.7 16.7 0.156 0.063 0.030 0.015 0.900 0.437 0.697\nGPT -2finetune . 16.2 13.1 0.130 0.042 0.017 0.008 0.884 0.401 0.664\nT able 2: Automatic evaluation results. Numbers in bold indi cate the best performing models.\n3.3 Analysis with Automatic Metrics\nW e ﬁrst examine the models with au-\ntomatic metrics, including perplexity\n(PPL), BLEU ( Papineni et al. , 2002),\n{A verage,Extrema,Greedy} (Liu et al. , 2016),\nand F1 ( Dinan et al. , 2019). The last metric is cal-\nculated with the code shared at https://\ngithub.com/facebookresearch/\nParlAI/blob/master/parlai/core/\nmetrics.py\n, while the others are computed\nusing a public NLG evaluation project available\nat\nhttps://github.com/Maluuba/nlg-\neval. W e choose Transformer Memory Network\n(TMN) ( Dinan et al. , 2019) and Disentangled\nResponse Decoder (DRD) ( Zhao et al. , 2020)\nas baselines, where DRD exploits pre-training\ntechniques to handle the low-resource challenge\nin knowledge-grounded dialogue generation, and\nholds the state-of-the-art performance on Wizard\nwhen all parameters are ﬁne-tuned with the full\ntraining data.\nT able\n2 reports the evaluation results. Obvi-\nously , DialoGPT is much worse than the base-\nlines in terms of all metrics over all the three\ndatasets, due to the gap between conversations in\nReddit (at least those for training) and conversa-\ntions in the probes. The results indicate that Di-\naloGPT under a zero-shot setting is still not ca-\npable of going deep in discussions about a spe-\nciﬁc topic, even though its responses carry some\ncontent case-by-case as shown in (\nZhang et al. ,\n2019b). However, this does not mean that there is\n“no knowledge” in the pre-trained language mod-\nels, as after ﬁne-tuning on the training sets of\nDataset Models Utterance Knowledge κ\nFluency (%) Coherency (%) Existence (%) Relevance (%) Corre ctness (%)\nT est Seen\nDialoGPT . 94.0 62.0 33.2 26.0 21.0 0.64\nDialoGPTf inetune. 95.4 77.8 74.6 61.8 42.6 0.68\nGPT -2f inetune. 96.8 80.4 77.4 67.0 47.6 0.61\nDRD 84.2 74.6 67.8 57.6 50.4 0.62\nT est Unseen\nDialoGPT . 94.4 67.6 25.6 20.8 13.8 0.64\nDialoGPTf inetune. 96.4 78.4 68.8 56.6 36.0 0.63\nGPT -2f inetune. 97.0 82.2 74.6 64.2 41.4 0.61\nDRD 83.8 74.8 64.2 55.4 42.0 0.68\nT able 3: Human evaluation results. The ratios are calculate d by combining annotations from four judges together.\nthe probes, DialogGPT is comparable with the\nbaselines on most of the metrics. Note that we\nfollow (\nZhao et al. , 2020) and exploit a vocabu-\nlary with 60,000 words for the baselines. Al-\nthough the difference in vocabulary size is not sig-\nniﬁcant, PPL is still not directly comparable be-\ntween the pre-trained models and the baselines.\nIn terms of F1, DialogGPT finetune is compara-\nble with the baselines on Wizard, and outperforms\nthe baselines on the other data, indicating that\nthe model can leverage the knowledge in its pa-\nrameters and reply with relevant and speciﬁc con-\ntent along with topic of the dialogue contexts, at\nleast judged from unigram overlap with human\nresponses. The advantage of DialogGPT finetune\nover the baselines is even bigger on CMU\nDoG,\nindicating that pre-training is more crucial when\ntraining data is small in size. GPT -2 finetune is\nworse than DialogGPT finetune , probably due to\nthe small training size of the probes.\nOverall, it seems that pre-trained language mod-\nels can be used to ground open domain dialogues\nas long as we can ﬁnd a few dialogues carrying\nknowledge for ﬁne-tuning, though how to obtain\nsuch dialogues (e.g., without crowd-sourcing) for\na speciﬁc task could arise as a new problem. On\nthe other hand, it is well-known that there exist\none-to-many relations in open domain dialogues\nat both a knowledge-level and a response-level.\nTherefore, the automatic evaluation may not be\nsufﬁcient to support our conclusions, which mo-\ntivates us to further examine the pre-trained lan-\nguage models with human annotations.\n3.4 Further Analysis with Human Judgment\nSince human labor is expensive, we only com-\npare the pre-trained language models with DRD\non Wizard. For each of T est Seen and T est Un-\nseen, we randomly sample 500 contexts. Re-\nsponses from different models for these contexts\nare pooled, randomly shufﬂed, and presented to 4\nwell-educated native speakers as annotators. The\nannotators judge the quality of each response from\nﬁve aspects: ﬂuency , coherence regarding the con-\ntexts, if the response carries knowledge (knowl-\nedge existence), if the knowledge is relevant with\nthe contexts (knowledge relevance), and if the\nknowledge is correct (knowledge correctness). On\neach aspect, a binary label (0/1) is assigned to a re-\nsponse by an annotator, and in total each response\nreceives 4 labels per aspect. Annotators are en-\ncouraged to actively use search engines when they\nfeel difﬁcult to make a decision. Note that if an an-\nnotator judges there is no knowledge in a response,\nthen by default he/she will also assign 0s to the\nresponse on knowledge relevance and knowledge\ncorrectness. The agreement among the annotators\nis measured by Fleiss’ kappa (\nFleiss, 1971).\nT able 3 summarizes human evaluation results.\nAll kappa values exceed 0.6, indicating substan-\ntial agreement among the annotators. From the\ncomparison, we obtain the following insights: (1)\nresponses from DialoGPT are generally grammat-\nical, but they could digress from the contexts\nor be too generic sometimes. Moreover, the re-\nsponses are more like casual chat rather than in-\nformation exchange, as demonstrated by case-1\nin T able\n1; (2) after ﬁne-tuning, both coherence\nand knowledge-related metrics get improved. The\nﬁne-tuned models are good at delivering common-\nsense knowledge, but often make mistakes on de-\ntails such as numbers, time, locations, etc. Case-2\nin T able\n1 is such an example. Another observa-\ntion is that the ﬁne-tuned models sometimes glue\ndifferent pieces of knowledge together in an awk-\nward way , causing disorder within responses, as\ndemonstrated by case-3 in T able\n1; and (3) though\nDRD sometimes generates responses with repe-\ntition/grammatical errors/irrelevant facts (due to\nwrong knowledge selection), it does a better job\non details, thanks to the external knowledge. More\ncases are shown in the supplementary material.\nIn conclusion, the human evaluation further con-\nvinces us that with proper ﬁne-tuning material,\npre-trained language models are knowledgeable to\nground open domain dialogues. Despite this, com-\nbining external knowledge with the pre-trained\nlanguage models could still be an important future\ndirection (considering (2) and (3)).\n4 Conclusions\nW e inspect how knowledgeable pre-trained lan-\nguage models are in the task of open domain di-\nalogue generation. Evaluation results on bench-\nmarks indicate that by ﬁne-tuning on a few dia-\nlogues carrying knowledge, a pre-trained language\nmodel can generate proper and informative re-\nsponses without external knowledge sources.\nReferences\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason W eston. 2019. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. In ICLR.\nJoseph L Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological bulletin ,\n76(5):378.\nKarthik Gopalakrishnan, Behnam Hedayatnia, Qin-\nlang Chen, Anna Gottardi, Sanjeev Kwatra, Anu\nV enkatesh, Raefer Gabriel, Dilek Hakkani-T ¨ ur, and\nAmazon Alexa AI. 2019. T opical-chat: T o-\nwards knowledge-grounded open-domain conversa-\ntions. Proc. Interspeech 2019, pages 1891–1895.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR.\nZekang Li, Cheng Niu, Fandong Meng, Y ang Feng,\nQian Li, and Jie Zhou. 2019. Incremental trans-\nformer with deliberation decoder for document\ngrounded conversations. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 12–21.\nRongzhong Lian, Min Xie, Fan W ang, Jinhua Peng,\nand Hua Wu. 2019. Learning to select knowledge\nfor response generation in dialog systems. arXiv\npreprint arXiv:1902.04911.\nChia-W ei Liu, Ryan Lowe, Iulian Serban, Mike Nose-\nworthy, Laurent Charlin, and Joelle Pineau. 2016.\nHow not to evaluate your dialogue system: An em-\npirical study of unsupervised evaluation metrics for\ndialogue response generation. In Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2122–2132.\nSeungwhan Moon, Pararth Shah, Anuj Kumar, and Ra-\njen Subba. 2019. Opendialkg: Explainable conver-\nsational reasoning with attention-based walks over\nknowledge graphs. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 845–854.\nKishore Papineni, Salim Roukos, T odd W ard, and W ei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th annual meeting on association for compu-\ntational linguistics, pages 311–318. Association for\nComputational Linguistics.\nFabio Petroni, Tim Rockt¨ aschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Y uxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nAshwin Ram, Rohit Prasad, Chandra Khatri, Anu\nV enkatesh, Raefer Gabriel, Qing Liu, Jeff Nunn,\nBehnam Hedayatnia, Ming Cheng, Ashish Nagar,\net al. 2018. Conversational ai: The science behind\nthe alexa prize. arXiv preprint arXiv:1801.03604.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the pa-\nrameters of a language model? arXiv preprint\narXiv:2002.08910 .\nHeung-Y eung Shum, Xiaodong He, and Di Li. 2018.\nFrom eliza to xiaoice: Challenges and opportunities\nwith social chatbots. Frontiers of IT & EE, 19(1):10–\n26.\nY i-Lin Tuan, Y un-Nung Chen, and Hung-yi Lee.\n2019. Dykgchat: Benchmarking dialogue genera-\ntion grounding on dynamic knowledge graphs. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 1855–\n1865.\nOriol V inyals and Quoc Le. 2015. A neural conversa-\ntional model. arXiv preprint arXiv:1506.05869.\nThomas W olf, V ictor Sanh, Julien Chaumond, and\nClement Delangue. 2019. Transfertransfo: A\ntransfer learning approach for neural network\nbased conversational agents. arXiv preprint\narXiv:1901.08149 .\nChen Xing, W ei Wu, Y u Wu, Ming Zhou, Y alou Huang,\nand W ei-Y ing Ma. 2017. Hierarchical recurrent\nattention network for response generation. arXiv\npreprint arXiv:1701.07149.\nHainan Zhang, Y anyan Lan, Liang Pang, Jiafeng Guo,\nand Xueqi Cheng. 2019a. Recosa: Detecting the rel-\nevant contexts with self-attention for multi-turn di-\nalogue generation. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 3721–3730.\nY izhe Zhang, Siqi Sun, Michel Galley, Y en-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2019b. Dialogpt: Large-scale\ngenerative pre-training for conversational response\ngeneration. arXiv preprint arXiv:1911.00536.\nXueliang Zhao, W ei Wu, Chongyang T ao, Can Xu,\nDongyan Zhao, and Rui Y an. 2020. Low-resource\nknowledge-grounded dialogue generation. arXiv\npreprint arXiv:2002.10348.\nHao Zhou, T om Y oung, Minlie Huang, Haizhou Zhao,\nJingfang Xu, and Xiaoyan Zhu. 2018a. Com-\nmonsense knowledge aware conversation generation\nwith graph attention. In IJCAI, pages 4623–4629.\nKangyan Zhou, Shrimai Prabhumoye, and Alan W\nBlack. 2018b. A dataset for document grounded\nconversations. arXiv preprint arXiv:1809.07358.\nA More Details of the Benchmarks\nT able 4 reports some statistics of Wizard,\nCMU DOG, and T opic Chat.\nB More Baselines\nBesides Transformer Memory Network (TMN)\n(\nDinan et al. , 2019) and Disentangled Response\nDecoder (DRD) ( Zhao et al. , 2020), we also com-\npare the pre-trained models with Incremental\nTransformer with Deliberation Decoder (ITDD)\n(\nLi et al. , 2019) and Posterior Knowledge Selec-\ntion (PostKS) ( Lian et al. , 2019) on automatic met-\nrics. T able 5 reports the evaluation results. Basi-\ncally , the conclusions we have drawn in the main\npaper still hold here.\nC More Cases\nT able\n6 shows more examples from DialoGPT\nwhere the responses are good in a chat style but\nlack necessary information for extending the dis-\ncussions. In spite of this, we also see a few\ngood cases from DialoGPT , as shown in T able\n7,\nwhich remind us of the potential of pre-trained\nlanguage models in terms of replying with knowl-\nedge. T able\n8 and T able 10 give more cases where\nDialogGPTfinetune and GPT -2 finetune do an ex-\ncellent job respectively , and T able 9 and T able 11\nshow more examples where the two models make\nmistakes on details.\nWizard of Wikipedia CMU DoG T opic Chat\nTrain V alid T est Seen T est Unseen Train V alid T est Train V alid T est Freq T est Rare\nNumber of Dialogues 18,430 1,948 965 968 3,373 229 619 8,628 1,078 539 539\nA verage Utterances per Dialogue 9.0 9.1 9.0 9.1 22.2 21.8 22.0 21.8 21.7 21.8 21.8\nA verage W ords per Utterance 16.4 16.4 16.4 16.1 10.9 12.2 10.9 19.5 19.8 19.5 19.5\nT able 4: Statistics of the three datasets.\nDataset Model PPL F1 BLEU-1 BLEU-2 BLEU-3 BLEU-4 A verage Ext rema Greedy\nWizard-of-Wikipedia(T est Seen)\nITDD 17.8 16.2 0.158 0.071 0.041 0.025 0.841 0.425 0.654\nPostKS 68.5 17.9 0.167 0.076 0.040 0.024 0.774 0.406 0.625\nDialoGPT . 84.0 8.4 0.073 0.020 0.006 0.003 0.797 0.363 0.603\nDialoGPTfinetune . 16.2 19.0 0.165 0.076 0.040 0.023 0.871 0.461 0.683\nGPT -2finetune . 15.0 14.4 0.140 0.049 0.020 0.010 0.860 0.417 0.649\nWizard-of-Wikipedia(T est Unseen)\nITDD 44.8 11.4 0.134 0.047 0.021 0.011 0.826 0.364 0.624\nPostKS 106.7 14.1 0.149 0.056 0.024 0.011 0.735 0.355 0.589\nDialoGPT . 85.9 8.1 0.071 0.019 0.006 0.002 0.792 0.362 0.596\nDialoGPTfinetune . 20.4 17.6 0.158 0.067 0.032 0.017 0.869 0.451 0.674\nGPT -2finetune . 18.9 13.8 0.139 0.047 0.019 0.008 0.859 0.411 0.642\nCMU-DoG\nITDD 26.0 10.4 0.095 0.036 0.017 0.009 0.748 0.390 0.587\nPostKS 62.2 11.7 0.143 0.051 0.022 0.010 0.818 0.424 0.637\nDialoGPT . 73.4 6.9 0.091 0.022 0.006 0.002 0.762 0.358 0.576\nDialoGPTfinetune . 15.9 13.7 0.161 0.064 0.030 0.015 0.812 0.430 0.641\nGPT -2finetune . 16.5 9.4 0.124 0.038 0.014 0.006 0.780 0.382 0.597\nT opic-Chat-Freq\nITDD 21.4 15.8 0.163 0.074 0.041 0.026 0.887 0.426 0.680\nPostKS 48.0 15.4 0.167 0.069 0.033 0.018 0.884 0.419 0.676\nDialoGPT . 87.6 8.3 0.074 0.018 0.005 0.002 0.842 0.380 0.630\nDialoGPTfinetune . 13.2 17.1 0.161 0.069 0.036 0.020 0.906 0.446 0.705\nGPT -2finetune . 13.4 13.6 0.136 0.047 0.021 0.011 0.892 0.411 0.674\nT opic-Chat-Rare\nITDD 24.7 14.0 0.153 0.062 0.032 0.019 0.880 0.408 0.670\nPostKS 69.2 14.5 0.149 0.056 0.025 0.012 0.860 0.379 0.654\nDialoGPT . 87.9 8.5 0.071 0.018 0.005 0.002 0.835 0.372 0.623\nDialoGPTfinetune . 15.7 16.7 0.156 0.063 0.030 0.015 0.900 0.437 0.697\nGPT -2finetune . 16.2 13.1 0.130 0.042 0.017 0.008 0.884 0.401 0.664\nT able 5: Automatic evaluation results\nCase-1\nContext\nUtter1: I suffer from Hypochondriasis, a verious serious disease\nUtter2: I haven’t heard of that, what’s it like?\nUtter3: Is a condition in which a person is inordinately worried abo ut having a serious illness.\nUtter4: Ah, ok, I have heard of that. Just didn’t know the term. I’m su re that is frustrating.\nDialoGPT It’s exactly what it is .\nCase-2\nContext\nUtter1: I love Iguanas, I have a few as pets. Do you like lizards at all ?\nUtter2: Y es, I like them. They are interesting.and prehistoric loo king. I like turtles too.\nUtter3: I agree, they deﬁnitely have a prehistoric look to them. The re are also over 6000 species\nspread across the world.\nUtter4: Do they have teeth and does their bite hurt if they bite you?\nDialoGPT yes it hurts .\nCase-3\nContext Utter1: Hey have you ever been to Baton Rouge?\nUtter2: I haven’t. Is it a nice play to live or visit?\nDialoGPT It’s a nice town .\nT able 6: Responses from DialoGPT are more like chat.\nCase-1\nContext\nUtter1: Sometimes I am overcome by laziness, and I don’t want to work or do anything at all.\nUtter2: I know exactly how you feel. I can get pretty lazy .\nUtter3: More formally some call laziness a disinclination to activ ity or exertion despite having the\nability to\nUtter4: I am trying not to become too lazy .\nDialoGPT It’s better to be lazy . It means you’re more careful with your energy budget .\nCase-2\nContext\nUtter1: Instagram is a great way to share picture with my family and f riends. Do you use it?\nUtter2: I do not use Instagram, but I do use Facebook. Apparently Fac ebook owns Instagram.\nUtter3: Y a. That is why you get messages in FB about Instagram. I hear d the younger people like\nInstagram better than FB.\nDialoGPT Y ounger crowd were in Facebook before Instagram was popular .\nCase-3\nContext\nUtter1: I like to study , collect and race cars in my free time.\nUtter2: Same here ! Auto racing is also known as automobile racing\nUtter3:I think that sounds very old fashioned. lol. What cars do you race?\nDialoGPT I’ll drive a Maserati that I own and race my alfa one\nT able 7: Informative responses from DialoGPT .\nCase-1\nContext\nUtter1: I suffer from Hypochondriasis, a verious serious disease\nUtter2: I haven’t heard of that, what’s it like?\nUtter3: Is a condition in which a person is inordinately worried abo ut having a serious illness.\nUtter4: Ah, ok, I have heard of that. Just didn’t know the term. I’m su re that is frustrating.\nDialoGPTf inetune. It can cause memory loss, anxiety and depression\nCase-2\nContext\nUtter1: I love Jane Austen don‘t you!\nUtter2: She sure is, I have read all three volumes of her juvenile wri tings in manuscript and\neven her unﬁnished novel, The W atsons.\nUtter3: Do you know anything about her that might be of interest to me ?\nDialoGPTf inetune. Y es she was a romance novel writer, that’s why she wrote so man y with romantic themes in\nthem.\nCase-3\nContext Utter1: A taco can be made with a variety of ﬁllings! I love including pork and cheese\nUtter2: What culture are tacos from?\nDialoGPTf inetune. I think most people have a taco from Mexico, although some may be made with beef or pork or\neven ﬁsh!\nT able 8: Good cases of DialoGPT finetune .\nCase-1\nContext Utter1: Hey have you ever been to Baton Rouge?\nUtter2: I haven’t. Is it a nice play to live or visit?\nDialoGPTf inetune. It’s the 32nd largest metropolitan area. Y ou should visit! (wrong number)\nCase-2\nContext\nUtter1: Who even is Kurt Cobain?\nUtter2: Kurt Cobain was an american singer, and musician lead singe r of the band nirvana and\nquite a story\nUtter3: Ohh wow I didn’t know that he was on Nirvana.\nDialoGPTf inetune. Y es he was, he was born and raised in Los Angeles, California (wrong location)\nCase-3\nContext\nUtter1: I like to build tanks! An aquarium (plural: ”aquariums” or ” aquaria”) is a vivarium of\nany size having at least one transparent side\nUtter2: W ell, that is a pretty cool job! I bet it take a large amount of work to do?\nDialoGPTf inetune. It can, and would be just an aquarium. A word for an aquarium is ”tidal boulevard. ” there are no\nidentifying features such as plants, invertebrates, or fun gi (mixed knowledge)\nT able 9: Bad cases of DialoGPT finetune .\nCase-1\nContext\nUtter1: I love chihuahuas, I have 2 of them.\nUtter2: W ow chihuahua are something else. Tiny and the smallest bre ed of dog.\nUtter3: Y ep, they get a bad rap about their demeanor too. Most people say they’re mean but one of\nmine is the sweetest ever.\nGPTf inetune. Lol cool! Chihuahuas come from indigenous Mexican dogs.\nCase-2\nContext\nUtter1: The greenery in Ireland is quite lovely .\nUtter2: It is beautiful. It’s the second largest island in the Briti sh Isles. Great Britain is the largest.\nUtter3: That is good to know . What about the population?\nUtter4: It was about 6.6 million in 2011. It was ranked the second mos t populous island in Europe.\nUtter5: Will the Brexit movement change much there?\nGPTf inetune. I think so. Just before the 2008 ﬁnancial crisis, Ireland was one of the fastest growing economies.\nCase-3\nContext Utter1: I have never visited Chicago but I hear the University is a pr ivate research university\nlocated there.\nUtter2: Which university?\nGPTf inetune. W ell the University of Chicago is one of the top and oldest uni versities in the United States.\nT able 10: Good cases of GPT finetune .\nCase-1\nContext Utter1: Do you drink coffee? I occasionally drink coffee because th e caffeine is a great stimulant\nUtter2: I love coffee! Living in the south, I’m partial to iced coffe e. I particularly like cold brew .\nGPTf inetune. My favourite is arabica. It’s made from the seeds of the eryth rocopus macrocarpa (wrong fact)\nCase-2\nContext Utter1: Do you like Elvis Presley? he was a famous american singer.\nUtter2: He is pretty good, I am from Nashville, So i’ve even been to gr aceland.\nGPTf inetune. He moved to Nashville, T ennessee when he was 14 in 1926. (wrong time)\nCase-3\nContext Utter1: Man, ICP or Insane Clown Posse are my favorite band! Their hi p hop style is phenomenal.\nDo you like them?\nUtter2: I ﬁnd ICP to have some of the worst fans, they seem to be crazy d ruggies quite often\nGPTf inetune. That’s interesting because they’re a punk rock band. I loved their second album ”Stand Up” a lot.\n(wrong fact)\nT able 11: Bad cases of GPT finetune .",
  "topic": "Domain (mathematical analysis)",
  "concepts": [
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5715470910072327
    },
    {
      "name": "Computer science",
      "score": 0.4666837155818939
    },
    {
      "name": "Common ground",
      "score": 0.44802695512771606
    },
    {
      "name": "Natural language processing",
      "score": 0.4060930609703064
    },
    {
      "name": "Linguistics",
      "score": 0.34348493814468384
    },
    {
      "name": "Psychology",
      "score": 0.3141719698905945
    },
    {
      "name": "Communication",
      "score": 0.20177879929542542
    },
    {
      "name": "Mathematics",
      "score": 0.0765814483165741
    },
    {
      "name": "Philosophy",
      "score": 0.072447270154953
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ],
  "cited_by": 9
}