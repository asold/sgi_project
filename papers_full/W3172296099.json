{
  "title": "That Looks Hard: Characterizing Linguistic Complexity in Humans and Language Models",
  "url": "https://openalex.org/W3172296099",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5042129410",
      "name": "Gabriele Sarti",
      "affiliations": [
        "Institute for Computational Linguistics “A. Zampolli”",
        "Scuola Internazionale Superiore di Studi Avanzati",
        "University of Trieste"
      ]
    },
    {
      "id": "https://openalex.org/A5057659257",
      "name": "Dominique Brunato⋄",
      "affiliations": [
        "Institute for Computational Linguistics “A. Zampolli”"
      ]
    },
    {
      "id": "https://openalex.org/A5084812833",
      "name": "Felice Dell’Orletta⋄",
      "affiliations": [
        "Institute for Computational Linguistics “A. Zampolli”"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4236120890",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2788816349",
    "https://openalex.org/W158417997",
    "https://openalex.org/W2139450036",
    "https://openalex.org/W2806183494",
    "https://openalex.org/W131522978",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3118017018",
    "https://openalex.org/W2299976354",
    "https://openalex.org/W2891407789",
    "https://openalex.org/W2579343286",
    "https://openalex.org/W3117229574",
    "https://openalex.org/W2250263931",
    "https://openalex.org/W2913340405",
    "https://openalex.org/W2471684646",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3029139904",
    "https://openalex.org/W2013112874",
    "https://openalex.org/W3087970256",
    "https://openalex.org/W2914924671",
    "https://openalex.org/W3114406058",
    "https://openalex.org/W2964941017",
    "https://openalex.org/W2931720176",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2787873964",
    "https://openalex.org/W2128388135",
    "https://openalex.org/W2986138048",
    "https://openalex.org/W2970648593",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W2518578398",
    "https://openalex.org/W331019419",
    "https://openalex.org/W3134409176",
    "https://openalex.org/W2973047874",
    "https://openalex.org/W2394756230",
    "https://openalex.org/W2971057237",
    "https://openalex.org/W3103368673"
  ],
  "abstract": "This paper investigates the relationship between two complementary perspectives in the human assessment of sentence complexity and how they are modeled in a neural language model (NLM). The first perspective takes into account multiple online behavioral metrics obtained from eye-tracking recordings. The second one concerns the offline perception of complexity measured by explicit human judgments. Using a broad spectrum of linguistic features modeling lexical, morpho-syntactic, and syntactic properties of sentences, we perform a comprehensive analysis of linguistic phenomena associated with the two complexity viewpoints and report similarities and differences. We then show the effectiveness of linguistic features when explicitly leveraged by a regression model for predicting sentence complexity and compare its results with the ones obtained by a fine-tuned neural language model. We finally probe the NLM's linguistic competence before and after fine-tuning, highlighting how linguistic information encoded in representations changes when the model learns to predict complexity.",
  "full_text": "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 48–60\nOnline Event, June 10, 2021. ©2021 Association for Computational Linguistics\n48\nThat Looks Hard: Characterizing Linguistic Complexity\nin Humans and Language Models\nGabriele Sarti 1,2 Dominique Brunato 2\n1 University of Trieste, International School for Advanced Studies (SISSA), Trieste\n2 Istituto di Linguistica Computazionale \"Antonio Zampolli\" (ILC-CNR), Pisa\nItaliaNLP Lab – italianlp.it\ngabriele.sarti996@gmail.com\n{dominique.brunato, felice.dellorletta}@ilc.cnr.it\nFelice Dell’Orletta2\nAbstract\nThis paper investigates the relationship be-\ntween two complementary perspectives in the\nhuman assessment of sentence complexity and\nhow they are modeled in a neural language\nmodel (NLM). The ﬁrst perspective takes into\naccount multiple online behavioral metrics ob-\ntained from eye-tracking recordings. The sec-\nond one concerns the ofﬂine perception of\ncomplexity measured by explicit human judg-\nments. Using a broad spectrum of linguistic\nfeatures modeling lexical, morpho-syntactic,\nand syntactic properties of sentences, we per-\nform a comprehensive analysis of linguistic\nphenomena associated with the two complex-\nity viewpoints and report similarities and dif-\nferences. We then show the effectiveness of\nlinguistic features when explicitly leveraged\nby a regression model for predicting sentence\ncomplexity and compare its results with the\nones obtained by a ﬁne-tuned neural language\nmodel. We ﬁnally probe the NLM’s linguistic\ncompetence before and after ﬁne-tuning, high-\nlighting how linguistic information encoded\nin representations changes when the model\nlearns to predict complexity.\n1 Introduction\nFrom a human perspective, linguistic complexity\nconcerns difﬁculties encountered by a language\nuser during sentence comprehension. The source\nof such difﬁculties is commonly investigated us-\ning either ofﬂine measures or online behavioral\nmetrics. In the ofﬂine framework, complexity rat-\nings can be elicited either by assessing errors in\ncomprehension tests or collecting explicit com-\nplexity judgments from readers. Instead, in the\nonline paradigm, cognitive signals are collected\nmainly through specialized machinery (e.g., MRI\nscanners, eye-tracking systems) during natural or\ntask-oriented reading. Among the wide range of\nonline complexity metrics, gaze data are widely re-\ngarded as reliable proxies of processing difﬁculties,\nreﬂecting both low and high-level complexity fea-\ntures of the input (Rayner, 1998; Hahn and Keller,\n2016). Eye-tracking measures have recently con-\ntributed to signiﬁcant improvements across many\npopular NLP applications (Hollenstein et al., 2019a,\n2020) and in particular on tasks related to linguis-\ntic complexity such as automatic readability as-\nsessment (ARA) (Ambati et al., 2016; Singh et al.,\n2016; González-Garduño and Søgaard, 2018), ob-\ntaining meaningful results for sentence-level classi-\nﬁcation in easy and hard-to-read categories (Vajjala\nand Luˇci´c, 2018; Evaldo Leal et al., 2020; Mart-\ninc et al., 2021). However, readability levels are\nconceptually very different from cognitive process-\ning metrics since ARA corpora are usually built in\nan automated fashion from parallel documents at\ndifferent readability levels, without explicit eval-\nuations of complexity by target readers (Vajjala\nand Lu ˇci´c, 2019). A different approach to com-\nplexity assessment that directly accounts for the\nperspective of readers is presented in the corpus by\nBrunato et al. (2018), where sentences are individ-\nually labeled with the perception of complexity of\nannotators, which may better reﬂect the underlying\ncognitive processing required by readers to parse\nthe sentence. This consideration is supported by\nrecent results highlighting the unpredictability of\noutliers in perceived complexity annotations, es-\npecially for sentences having complex syntactic\nstructures (Sarti, 2020).\nGiven the relation between complexity judg-\nments elicited from annotators and online cogni-\ntive processing metrics, we investigate whether the\nconnection between the two perspectives can be\nhighlighted empirically in human annotations and\nlanguage model representations. We begin by lever-\naging linguistic features associated with a variety\nof sentence-level structural phenomena and analyz-\ning their correlation with ofﬂine and online com-\nplexity metrics. We then evaluate the performance\nof models using either complexity-related explicit\n49\nfeatures or contextualized word embeddings, fo-\ncusing mainly on the neural language model AL-\nBERT (Lan et al., 2020). In this context, we show\nhow both explicit features and learned representa-\ntions obtain comparable results when predicting\ncomplexity scores. Finally, we focus on studying\nhow complexity-related properties are encoded in\nthe representations of ALBERT. This perspective\ngoes in the direction of exploiting human process-\ning data to address the interpretability issues of un-\nsupervised language representations (Hollenstein\net al., 2019b; Gauthier and Levy, 2019; Abnar et al.,\n2019). To this end, we rely on the probing task ap-\nproach, a recently introduced technique within the\narea of NLMs interpretability consisting of training\ndiagnostic classiﬁers to probe the presence of en-\ncoded linguistic properties inside contextual repre-\nsentations (Conneau et al., 2018; Zhang and Bow-\nman, 2018). We observe that ﬁne-tuning on on-\nline and ofﬂine complexity produces a consequent\nincrease in probing performances for complexity-\nrelated features during our probing experiments.\nThis investigation has the speciﬁc purpose of study-\ning whether and how learning a new task affects\nthe linguistic properties encoded in pretrained rep-\nresentations. In fact, while pre-trained models have\nbeen widely studied using probing methods, the\neffect of ﬁne-tuning on encoded information was\nseldom investigated. For example, Merchant et al.\n(2020) found that ﬁne-tuning does not impact heav-\nily the linguistic information implicitly learned by\nthe model, especially when considering a super-\nvised probe closely related to a downstream task.\nMiaschi et al. (2020) further demonstrated a posi-\ntive correlation between the model’s ability to solve\na downstream task on a speciﬁc input sentence and\nthe related linguistic knowledge encoded in a lan-\nguage model. Nonetheless, to our knowledge, no\nprevious work has taken into account sentence com-\nplexity assessment as a ﬁne-tuning task for NLMs.\nOur results suggest that the model’s competencies\nduring training are interpretable from a linguistic\nperspective and are possibly related to its predictive\ncapabilities for complexity assessment.\nContributions To our best knowledge, this is the\nﬁrst work displaying the connection between online\nand ofﬂine complexity metrics and studying how\nthey are represented by a neural language model.\nWe a) provide a comprehensive analysis of linguis-\ntic phenomena correlated with eye-tracking data\nand human perception of complexity, addressing\nMetric Level Description Label\nOfﬂine\n(Perceptual)\nPerceived complexity annota-\ntion on a 1-to-7 Likert scale.\nPC\nOnline (Early) Duration of the ﬁrst reading\npass in milliseconds.\nFPD\nOnline\n(Late)\nTotal ﬁxation count FXC\nTotal duration of all ﬁxations in\nmilliseconds\nTFD\nOnline\n(Contextual)\nDuration of outbound regres-\nsive saccades in milliseconds\nTRD\nTable 1: Sentence-level complexity metrics. We refer\nto the entire set of gaze metrics as ET (eye-tracking).\nPerc. Complexity Eye-tracking\ndomain news articles literature\naggregation avg. annotators words sum +\navg. participants\nﬁltering IAA + duplicates min length\n# sentences 1115 4041\n# words 21723 52131\navg. sent. length 19.48 12.90\navg. word length 4.95 4.60\nTable 2: Descriptive statistics of the two sentence-level\ncorpora after the preprocessing procedure.\nsimilarities and differences from a linguistically-\nmotivated perspective across metrics and at dif-\nferent levels of granularity; b) compare the per-\nformance of models using both explicit features\nand unsupervised contextual representations when\npredicting online and ofﬂine sentence complexity;\nand c) show the natural emergence of complexity-\nrelated linguistic phenomena in the representations\nof language models trained on complexity metrics.1\n2 Data and Preprocessing\nOur study leverages two corpora, each capturing\ndifferent aspects of linguistic complexity:\nEye-tracking For online complexity metrics,\nwe used the monolingual English portion of\nGECO (Cop et al., 2017), an eye-tracking cor-\npus based on the novel “The Mysterious Case at\nStyles” by Agatha Christie. The corpus consists of\n5,386 sentences annotated at word-level with eye-\nmovement records of 14 English native speakers.\nWe select four online metrics spanning multiple\n1Code and data available at https://github.com/\ngsarti/interpreting-complexity\n50\nAnnotation Level Linguistic Feature Description Label\nRaw Text Sentence length (tokens), word length (characters) n_tokens, char_per_tok\nWords and lemmas type/token ratio ttr_form, ttr_lemma\nPOS Tagging\nDistribution of UD and language-speciﬁc POS tags upos_dist_*, xpos_dist_*\nLexical density lexical_density\nInﬂectional morphology of auxiliaries (mood, tense) aux_mood_*, aux_tense_*\nDependency Parsing\nSyntactic tree depth parse_depth\nAverage and maximum length of dependency links avg_links_len, max_links_len\nNumber and average length of prepositional chains n_prep_chains, prep_chain_len\nRelative ordering of main elements subj_pre, subj_post, obj_pre, obj_post\nDistribution of dependency relations dep_dist_*\nDistribution of verbal heads vb_head_per_sent\nDistribution of principal and subordinate clauses princ_prop_dist, sub_prop_dist\nAverage length of subordination chains sub_chain_len\nRelative ordering of subordinate clauses sub_post, sub_pre\nTable 3: Description of sentence-level linguistic features employed in our study.\nphases of cognitive processing, which are widely\nconsidered relevant proxies for linguistic process-\ning in the brain (Demberg and Keller, 2008; Va-\nsishth et al., 2013). We sum-aggregate those at\nsentence-level and average their values across par-\nticipants to obtain the four online metrics presented\nin Table 1. As a ﬁnal step to make the corpus\nmore suitable for linguistic complexity analysis,\nwe remove all utterances with fewer than 5 words.\nThis design choice is adopted to ensure consistency\nwith the perceived complexity corpus by Brunato\net al. (2018).\nPerceived Complexity For the ofﬂine evaluation\nof sentence complexity, we used the English por-\ntion of the corpus by Brunato et al. (2018). The cor-\npus contains 1,200 sentences taken from the Wall\nStreet Journal section of the Penn Treebank (Mc-\nDonald et al., 2013) with uniformly-distributed\nlengths ranging between 10 and 35 tokens. Each\nsentence is associated with 20 ratings of perceived-\ncomplexity on a 1-to-7 point scale. Ratings were\nassigned by English native speakers on the Crowd-\nFlower platform. To reduce the noise produced by\nthe annotation procedure, we removed duplicates\nand sentences for which less than half of the an-\nnotators agreed on a score in the range µn ±σn,\nwhere µn and σn are respectively the average and\nstandard deviation of all annotators’ judgments for\nsentence n. Again, we average scores across anno-\ntators to obtain a single metric for each sentence.\nTable 2 presents an overview of the two corpora\nafter preprocessing. The resulting eye-tracking\n(ET) corpus contains roughly four times more sen-\ntences than the perceived complexity (PC) one,\nwith shorter words and sentences on average.\n3 Analysis of Linguistic Phenomena\nAs a ﬁrst step to investigate the connection between\nthe two complexity paradigms, we evaluate the cor-\nrelation of online and ofﬂine complexity labels with\nlinguistic phenomena modeling a number of prop-\nerties of sentence structure. To this end, we rely\non the Proﬁling-UD tool (Brunato et al., 2020) to\nannotate each sentence in our corpora and extract\nfrom it ∼100 features representing their linguistic\nstructure according to the Universal Dependencies\nformalism (Nivre et al., 2016). These features cap-\nture a comprehensive set of phenomena, from ba-\nsic information (e.g. sentence and word length) to\nmore complex aspects of sentence structure (e.g.\nparse tree depth, verb arity), including properties\nrelated to sentence complexity at different levels of\ndescription. A summary of most relevant features\nin our analysis is presented in Table 3.\nFigure 1 reports correlation scores for features\nshowing a strong connection ( |ρ|> 0.3) with at\nleast one of the evaluated metrics. Features are\nranked using their Spearman’s correlation with\ncomplexity metrics, and scores are leveraged to\nhighlight the relation between linguistic phenom-\nena and complexity paradigms. We observe that\nfeatures showing a signiﬁcant correlation with eye-\ntracking metrics are twice as many as those corre-\nlating with PC scores and generally tend to have\nhigher coefﬁcients, except for total regression dura-\ntion (TRD). Nevertheless, the most correlated fea-\ntures are the same across all metrics. As expected,\nsentence length (n_tokens) and other related fea-\n51\nFigure 1: Ranking of the most correlated linguistic fea-\ntures for selected metrics. All Spearman’s correlation\ncoefﬁcients have p< 0.001.\ntures capturing aspects of structural complexity\noccupy the top positions in the ranking. Among\nthose, we also ﬁnd the length of dependency links\n(max_links_len, avg_links_len) and the depth of\nthe whole parse tree or selected sub-trees, i.e. nom-\ninal chains headed by a preposition (parse_depth,\nn_prep_chains). Similarly, the distribution of sub-\nordinate clauses (sub_prop_dist, sub_post) is posi-\ntively correlated with all metrics but with stronger\neffect for eye-tracking ones, especially in presence\nof longer embedded chains ( sub_chain_len). In-\nterestingly, the presence of numbers (upos_NUM,\ndep_nummod) affects only the explicit perception\nof complexity while it is never strongly correlated\nwith all eye-tracking metrics. This ﬁnding is ex-\npected since numbers are very short tokens and,\nlike other functional POS, were never found to be\nstrongly correlated with online reading in our re-\nsults. Conversely, numerical information has been\nidentiﬁed as a factor hampering sentence readabil-\nity and understanding (Rello et al., 2013).\nUnsurprisingly, sentence length is the most cor-\nrelated predictor for all complexity metrics. Since\nmany linguistic features highlighted in our analysis\nare strongly related to sentence length, we tested\nwhether they maintain a relevant inﬂuence when\nthis parameter is controlled. To this end, Spear-\nman’s correlation was computed between features\nand complexity tasks, but this time considering\nbins of sentences having approximately the same\nlength. Speciﬁcally, we split each corpus into 6\nbins of sentences with 10, 15, 20, 25, 30 and 35\ntokens respectively, with a range of ±1 tokens per\nbin to select a reasonable number of sentences for\nour analysis.\nFigure 2 reports the new rankings of the most\ncorrelated linguistic features within each bin across\ncomplexity metrics (|ρ|>0.2). Again, we observe\nthat features showing a signiﬁcant correlation with\ncomplexity scores are fewer for PC bins than for\neye-tracking ones. This fact depends on controlling\nfor sentence length but also on the small size of\nbins for the whole dataset. As in the coarse-grained\nanalysis, TRD is the eye-tracking metric less cor-\nrelated to linguistic features, while the other three\n(FXC, FPD, TFD) show a homogeneous behav-\nior across bins. For the latters, vocabulary-related\nfeatures (token-type ratio, average word length, lex-\nical density) are always ranked on top (and with\na positive correlation) in all bins, especially when\nconsidering shorter sentences (i.e. from 10 to 20\ntokens). For PC, this is true only for some of them\n(i.e. word length and lexical density). At the same\ntime, features encoding numerical information are\nstill highly correlated with the explicit perception\nof complexity in almost all bins. Interestingly, fea-\ntures modeling subordination phenomena extracted\nfrom ﬁxed-length sentences exhibit a reverse trend\nthan when extracted from the whole corpus, i.e.\nthey are negatively correlated with judgments. If,\non the one hand, we expect an increase in the pres-\nence of subordination for longer sentences (pos-\nsibly making sentences more convoluted), on the\nother hand, when length is controlled, our ﬁndings\nsuggest that subordinate structures are not necessar-\nily perceived as a symptom of sentence complex-\nity. Our analysis also highlights that PC’s relevant\nfeatures are signiﬁcantly different from those cor-\nrelated to online eye-tracking metrics when con-\ntrolling for sentence length. This aspect wasn’t\nevident from the previous coarse-grained analysis.\nWe note that, despite controlling sentence length,\n52\nFigure 2: Rankings of the most correlated linguistic features for metrics within length-binned subsets of the two\ncorpora. Coefﬁcients ≥0.2 or ≤-0.2 are highlighted, and have p< 0.001. (Bins from 10 to 35 have sizes of 173,\n163, 164, 151, 165, and 147 sentences for PC and 899, 568, 341, 215, 131, and 63 sentences for gaze metrics.)\ngaze measures are still signiﬁcantly connected to\nlength-related phenomena. This can be possibly\ndue to the ±1 margin applied for sentence selec-\ntion and the high sensitivity of behavioral metrics\nto small changes in the input.\n4 Predicting Online and Ofﬂine\nLinguistic Complexity\nGiven the high correlations reported above, we pro-\nceed to quantify the importance of explicit linguis-\ntic features from a modeling standpoint. Table 4\npresents the RMSE and R2 scores of predictions\nmade by baselines and models for the selected com-\nplexity metrics. Performances are tested with a 5-\nfold cross-validation regression with ﬁxed random\nseed on each metric. Our baselines use average\nmetric scores of all training sentences (Average)\nand average scores of sentences binned by their\nlength in # of tokens (Length-binned average) as\npredictions. The two linear SVM models leverage\nexplicit linguistic features, using respectively only\nn_tokens (SVM length) and the whole set of ∼100\nfeatures (SVM feats). Besides those, we also test\nthe performances of a state-of-the-art Transformer\nneural language model relying entirely on contex-\ntual word embeddings. We selected ALBERT as a\n53\nPC FXC FPD TFD TRD\nRMSE R2 RMSE R2 RMSE R2 RMSE R2 RMSE R2\nAverage .87 .00 6.17 .06 1078 .06 1297 .06 540 .03\nLength-binned average .53 .62 2.36 .86 374 .89 532 .85 403 .45\nSVM length .54 .62 2.19 .88 343 .90 494 .86 405 .45\nSVM feats .44 .74 1.77 .92 287 .93 435 .89 400 .46\nALBERT .44 .75 1.98 .91 302 .93 435 .90 382 .49\nTable 4: Average Root-Mean-Square Error andR2 for complexity predictions of two average baselines, two SVMs\nrelying on explicit features and a pretrained language model with contextualized word embeddings using 5-fold\ncross-validation. ALBERT learns eye-tracking metrics in a multitask setting over parallel annotations.\nlightweight yet effective alternative to BERT (De-\nvlin et al., 2019) for obtaining contextual word\nrepresentations, using its last-layer [CLS] sentence\nembedding as input for a linear regressor during\nﬁne-tuning and testing. We selected the last layer\nrepresentations, despite having strong evidence on\nthe importance of intermediate representation in\nencoding language properties, because we aim to\ninvestigate how ﬁnal layers encode complexity-\nrelated competences. Given the availability of par-\nallel eye-tracking annotations, we train ALBERT\nusing multitask learning with hard parameter shar-\ning (Caruana, 1997) on gaze metrics.2\nFrom results in Table 4 we note that: i) the\nlength-binned average baseline is very effective\nin predicting complexity scores and gaze metrics,\nwhich is unsurprising given the extreme correla-\ntion between length and complexity metrics pre-\nsented in Figure 1; ii) the SVM feats model shows\nconsiderable improvements if compared to the\nlength-only SVM model for all complexity met-\nrics, highlighting how length alone accounts for\nmuch but not for the entirety of variance in com-\nplexity scores; and iii) ALBERT performs on-par\nwith the SVM feats model on all complexity met-\nrics despite the small dimension of the ﬁne-tuning\ncorpora and the absence of explicit linguistic in-\nformation. A possible interpretation of ALBERT’s\nstrong performances is that the model implicitly\ndevelops competencies related to phenomena en-\ncoded by linguistic features while training on on-\nline and ofﬂine complexity prediction. We explore\nthis perspective in Section 5.\nAs a ﬁnal step in the study of feature-based\nmodels, we inspect the importance accorded by\nthe SVM feats model to features highlighted in\n2Additional information on parameters and chosen training\napproach is presented in Appendix A.\nprevious sections. Table 5 presents coefﬁcient\nranks produced by SVM feats for all sentences\nand for the 10±1 length bin, which was selected\nas the broadest subset. Despite evident similar-\nities with the previous correlation analysis, we\nencounter some differences that are possibly at-\ntributable to the model’s inability in modeling non-\nlinear relations. In particular, the SVM model\nstill ﬁnds sentence length and related structural\nfeatures highly relevant for all complexity met-\nrics. However, especially for PC, lexical features\nalso appear in the top positions (e.g. lexical den-\nsity, ttr_lemma, char_per_tok), as well as speciﬁc\nfeatures related to verbal predicate information\n(e.g. xpos_dist_VBZ,_VBN). This holds both for\nall sentences, and when considering single length-\nbinned subsets. While in the correlation analysis\neye-tracking metrics were almost indistinguishable,\nthose behave quite differently when considering\nhow linguistic features are used for inference by\nthe linear SVM model. In particular, the ﬁxation\ncount metric (FXC) consistently behaves in a differ-\nent way if compared to other gaze measures, even\nwhen controlling for length.\n5 Probing Linguistic Phenomena in\nALBERT Representations\nAs shown in Table 4, ALBERT performances on\nthe PC and eye-tracking corpora are comparable\nto those obtained using a linear SVM with explicit\nlinguistic features. To investigate if ALBERT en-\ncodes the linguistic knowledge that we identiﬁed\nas strongly correlated with online and perceived\nsentence complexity during training and prediction,\nwe adopt the probing task testing paradigm. The\naim of this analysis is two-fold: i) probing the pres-\nence of complexity-related information encoded by\nALBERT representations during the pre-training\n54\nAll Sentences Bin 10 ±1\nPC FXC FPD TFD TRD PC FXC FPD TFD TRD\nn_tokens 1 1 1 1 1 -36 5 1 1 2\nchar_per_tok 2 2 12 10 16 3 1 3 3 19\nxpos_dist_VBN 5 -37 76 77 75 28 9 26 21 42\navg_links_len 6 -6 7 7 7 11 -8 -23 -30 -46\nn_prep_chains 7 3 10 9 8 -44 16 50 41 48\ndep_dist_compound 9 7 58 61 49 13 12 60 51 47\nvb_head_per_sent 10 4 4 6 3 2 -9 31 36 -33\nmax_links_len 56 5 2 2 2 -32 -30 36 30 -39\nparse_depth 34 -36 3 3 4 -17 -1 22 24 12\nsub_post 28 -33 8 8 9 -28 -40 33 34 48\ndep_dist_conj 17 31 11 13 10 37 -37 46 56 -48\nupos_dist_NUM 15 39 70 72 72 4 / / / /\nttr_form -42 28 77 74 -26 17 2 3 2 1\nprep_chain_len 53 12 16 16 14 -48 -23 43 39 42\nsub_chain_len 24 -14 19 19 32 -30 -43 56 55 35\ndep_dist_nsubj 11 -16 -8 -8 -9 -2 31 -18 -19 -29\nupos_dist_PRON -16 -13 -7 -6 -8 -44 -21 -5 -8 -38\ndep_dist_punct -21 -3 -4 -4 -4 -20 -3 -2 -2 -2\ndep_dist_nmod -20 -2 55 50 50 -9 3 28 17 15\nxpos_dist_. -11 15 -1 -1 -1 -6 43 -24 -30 32\nxpos_dist_VBZ -9 20 82 -33 -30 24 14 20 40 -47\ndep_dist_aux -8 17 -30 -29 77 32 27 39 31 45\ndep_dist_case -7 -34 25 22 34 8 -6 62 44 -21\nttr_lemma -4 21 -22 -28 -11 -4 -45 4 4 9\ndep_dist_det -3 52 42 40 21 -27 -36 17 14 5\nsub_prop_dist -2 29 6 5 5 26 28 63 59 21\nlexical_density -1 -1 26 25 20 -37 -5 5 6 10\nTable 5: Rankings based on the coefﬁcients assigned by SVM feats for all metrics. Top ten positive and negative\nfeatures are marked with orange and cyan respectively. “/” marks features present in less than 5% of sentences.\nprocess, especially in relation to analyzed features;\nand ii) verifying whether, and in which respect, this\ncompetence is affected by a ﬁne-tuning on com-\nplexity assessment tasks.\nTo conduct the probing experiments, we aggre-\ngate three UD English treebanks representative of\ndifferent genres, namely: EWT, GUM and Par-\nTUT by Silveira et al. (2014); Zeldes (2017);\nSanguinetti and Bosco (2015), respectively. We\nthus obtain a corpus of 18,079 sentences and use\nthe Proﬁling-UD tool to extract nsentence-level\nlinguistic features Z= z1,...,z n from gold lin-\nguistic annotations. We then generate representa-\ntions A(x) of all sentences in the corpus using the\nlast-layer [CLS] embedding of a pretrained AL-\nBERT base model without additional ﬁne-tuning,\nand train nsingle-layer perceptron regressors gi :\nA(x) →zi that learn to map representations A(x)\nto each linguistic feature zi. We ﬁnally evaluate\nthe error and R2 scores of each gi as a proxy to\nthe quality of representations A(x) for encoding\ntheir respective linguistic feature zi. We repeat\nthe same evaluation for ALBERTs ﬁne-tuned re-\nspectively on perceived complexity (PC) and on all\neye-tracking labels with multitask learning (ET),\naveraging scores with 5-fold cross-validation. Re-\nsults are shown on the left side of Table 6.\nAs we can see, ALBERT’s last-layer sentence\nrepresentations have relatively low knowledge of\ncomplexity-related probes, but the performance on\nthem highly increases after ﬁne-tuning. Specif-\nically, a noticeable improvement is obtained on\nfeatures that were already better encoded in base\npretrained representation, i.e. sentence length and\nrelated features, suggesting that ﬁne-tuning possi-\nbly accentuates only properties already well-known\nby the model, regardless of the target task. To ver-\nify that this isn’t the case, we repeat the same ex-\nperiments on ALBERT models ﬁne-tuned on the\nsmallest length-binned subset (i.e. 10 ±1 tokens)\npresented in previous sections. The right side of\nTable 6 presents these results. We know from our\nlength-binned analysis of Figure 2 that PC scores\nare mostly uncorrelated with length phenomena,\n55\nBase PC ET PC Bin 10 ±1 ET Bin 10 ±1\nRMSE R2 RMSE R2 RMSE R2 RMSE R2 RMSE R2\nn_tokens 8.19 .26 4.66 .76 2.87 .91 8.66 .18 6.71 .51\nparse_depth 1.47 .18 1.18 .48 1.04 .60 1.50 .16 1.22 .43\nvb_head_per_sent 1.38 .15 1.26 .30 1.14 .42 1.44 .09 1.30 .25\nxpos_dist_. .05 .13 .04 .41 .04 .42 .04 .18 .04 .38\navg_links_len .58 .12 .53 .29 .52 .31 .59 .10 .56 .20\nmax_links_len 5.20 .12 4.08 .46 3.75 .54 5.24 .11 4.73 .28\nn_prep_chains .74 .11 .67 .26 .66 .29 .72 .14 .69 .21\nsub_prop_dist .35 .09 .33 .13 .31 .22 .34 .05 .32 .15\nupos_dist_PRON .08 .09 .08 .14 .08 .07 .07 .23 .08 .15\nupos_dist_NUM .05 .08 .05 .06 .05 .02 .05 .16 .05 .06\ndep_dist_nsubj .06 .08 .06 .10 .06 .05 .05 .17 .06 .11\nchar_per_tok .89 .07 .87 .12 .90 .05 .82 .22 .86 .14\nprep_chain_len .60 .07 .57 .17 .56 .19 .59 .12 .56 .18\nsub_chain_len .70 .07 .67 .15 .62 .26 .71 .04 .66 .16\ndep_dist_punct .07 .06 .07 .06 .07 .14 .07 .06 .07 .14\ndep_dist_nmod .05 .06 .05 .07 .05 .06 .05 .09 .05 .09\nsub_post .44 .05 .46 .12 .44 .18 .47 .05 .45 .14\ndep_dist_case .07 .05 .06 .06 .07 .08 .07 .07 .07 .10\nlexical_density .14 .05 .13 .03 .13 .03 .13 .13 .13 .13\ndep_dist_compound .06 .04 .06 .05 .06 .03 .06 .10 .06 .07\ndep_dist_conj .04 .03 .04 .04 .04 .04 .05 .02 .04 .03\nttr_form .08 .03 .08 .05 .08 .05 .08 .05 .08 .05\ndep_dist_det .06 .03 .06 .02 .06 .04 .06 .03 .06 .03\ndep_dist_aux .04 .02 .04 .01 .04 .01 .04 .06 .04 .04\nxpos_dist_VBN .03 .01 .03 .00 .03 .00 .03 .01 .03 .00\nxpos_dist_VBZ .04 .01 .04 .01 .04 .02 .04 .02 .04 .02\nttr_lemma .09 .01 .09 .06 .09 .06 .09 .04 .09 .03\nTable 6: RMSE and R2 scores for diagnostic regressors trained on ALBERT representations, respectively, without\nﬁne-tuning (Base), with PC and eye-tracking (ET) ﬁne-tuning on all data (left) and on the 10 ±1 length-binned\nsubset (right). Bold values highlight relevant increases in R2 from Base.\nwhile ET scores remain signiﬁcantly affected de-\nspite our controlling of sequence size. This also\nholds for length-binned probing task results, where\nthe PC model seems to neglect length-related prop-\nerties in favor of other ones, which were the same\nhighlighted in our ﬁne-grained correlation analy-\nsis (e.g. word length, numbers, explicit subjects).\nThe ET-trained model conﬁrms the same behav-\nior, retaining strong but lower performances for\nlength-related features. We note that, for all met-\nrics, features that were highly relevant only for the\nSVM predictions, such as those encoding verbal\ninﬂectional morphology or vocabulary-related ones\n(Table 5), are not affected by the ﬁne-tuning pro-\ncess. Despite obtaining the same accuracy of a\nSVM, the neural language model seem to address\nthe task more similarly to humans when accounting\nfor correlation scores (Figure 2). A more extensive\nanalysis of the relation between human behavior\nand predictions by different models is deemed in-\nteresting for future work.\nTo conclude, although higher probing tasks per-\nformances after ﬁne-tuning on complexity met-\nrics should not be interpreted as direct proof that\nthe neural language model is exploiting newly-\nacquired morpho-syntactic and syntactic informa-\ntion, they suggest an importance shift in NLM rep-\nresentation, triggered by ﬁne-tuning, that produces\nan encoding of linguistic properties able to better\nmodel the human assessment of complexity.\n6 Conclusion\nThis paper investigated the connection between\neye-tracking metrics and the explicit perception of\nsentence complexity from an experimental stand-\npoint. We performed an in-depth correlation analy-\nsis between complexity scores and sentence-level\nproperties at different granularity levels, highlight-\ning how all metrics are strongly connected to sen-\ntence length and related properties, but also re-\nvealing different behaviors when controlling for\nlength. We then evaluated models using explicit\n56\nlinguistic features and unsupervised word embed-\ndings to predict complexity, showing comparable\nperformances across metrics. We ﬁnally tested the\nencoding of linguistic properties in the contextual\nrepresentations of a neural language model, not-\ning the natural emergence of task-related linguistic\nproperties within the model’s representations after\nthe training process. We thus conjecture that a re-\nlation subsists between the linguistic knowledge\nacquired by the model during the training proce-\ndure and its downstream performances on tasks\nfor which the morphosyntactic and syntactic struc-\ntures play a relevant role. For the future, we would\nlike to test comprehensively the effectiveness of\ntasks inspired by the human language learning as\nintermediate steps to train more robust and parsi-\nmonious neural language models.\n7 Broader Impact and Ethical\nPerspectives\nThe ﬁndings described in this work are mostly\nintended to evaluate recent efforts in the compu-\ntational modeling of linguistic complexity. This\nsaid, some of the models and procedures described\ncan be clearly beneﬁcial to society. For exam-\nple, using models trained to predict reading pat-\nterns may be used in educational settings to iden-\ntify difﬁcult passages that can be simpliﬁed, im-\nproving reading comprehension for students in a\nfully-personalizable way. However, it is essen-\ntial to recognize the potentially malicious usage\nof such systems. The integration of eye-tracking\nsystems in mobile devices, paired with predictive\nmodels presented in this work, could be used to\nbuild harmful surveillance systems and advertise-\nment platforms using gaze predictions for extreme\nbehavioral manipulation. In terms of research im-\npact, the experiments presented in this work may\nprovide useful insights into the behavior of neural\nlanguage models for researchers working in the\nﬁelds of interpretability in NLP and computational\npsycholinguistics.\nReferences\nSamira Abnar, Lisa Beinborn, Rochelle Choenni, and\nWillem Zuidema. 2019. Blackbox meets blackbox:\nRepresentational similarity & stability analysis of\nneural language models and brains. In Proceedings\nof the 2019 ACL Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n191–203, Florence, Italy. Association for Computa-\ntional Linguistics.\nBharat Ram Ambati, Siva Reddy, and Mark Steedman.\n2016. Assessing relative sentence complexity us-\ning an incremental CCG parser. In Proceedings of\nthe 2016 Conference of the North American Chap-\nter of the Association for Computational Linguis-\ntics: Human Language Technologies , pages 1051–\n1057, San Diego, California. Association for Com-\nputational Linguistics.\nDominique Brunato, Andrea Cimino, Felice\nDell’Orletta, Giulia Venturi, and Simonetta\nMontemagni. 2020. Proﬁling-UD: a tool for linguis-\ntic proﬁling of texts. In Proceedings of the 12th\nLanguage Resources and Evaluation Conference ,\npages 7145–7151, Marseille, France. European\nLanguage Resources Association.\nDominique Brunato, Lorenzo De Mattei, Felice\nDell’Orletta, Benedetta Iavarone, and Giulia Ven-\nturi. 2018. Is this sentence difﬁcult? do you\nagree? In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2690–2699, Brussels, Belgium. Association\nfor Computational Linguistics.\nRich Caruana. 1997. Multitask learning. Machine\nLearning, 28:41–75.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Loïc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $&!#* vector: Probing\nsentence embeddings for linguistic properties. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2126–2136, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nUschi Cop, Nicolas Dirix, Denis Drieghe, and Wouter\nDuyck. 2017. Presenting GECO: An eyetracking\ncorpus of monolingual and bilingual sentence read-\ning. Behavior Research Methods, 49(2):602–615.\nDeepset. 2019. FARM: Framework for adapting rep-\nresentation models. GitHub repository: https://\ngithub.com/deepset-ai/FARM.\nVera Demberg and Frank Keller. 2008. Data from eye-\ntracking corpora as evidence for theories of syntac-\ntic processing complexity. Cognition, 109(2):193 –\n210.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nSidney Evaldo Leal, João Marcos Munguba Vieira, Er-\nica dos Santos Rodrigues, Elisângela Nogueira Teix-\neira, and Sandra Aluísio. 2020. Using eye-tracking\n57\ndata to predict the readability of Brazilian Por-\ntuguese sentences in single-task, multi-task and se-\nquential transfer learning approaches. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 5821–5831, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nJon Gauthier and Roger Levy. 2019. Linking artiﬁcial\nand human neural representations of language. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 529–\n539, Hong Kong, China. Association for Computa-\ntional Linguistics.\nAna Valeria González-Garduño and Anders Søgaard.\n2018. Learning to predict readability using eye-\nmovement data from natives and learners. In AAAI\nConference on Artiﬁcial Intelligence 2018 . AAAI\nConference on Artiﬁcial Intelligence.\nMichael Hahn and Frank Keller. 2016. Modeling hu-\nman reading with neural attention. In Proceedings\nof the 2016 Conference on Empirical Methods in\nNatural Language Processing, pages 85–95, Austin,\nTexas. Association for Computational Linguistics.\nNora Hollenstein, Maria Barrett, and Lisa Beinborn.\n2020. Towards best practices for leveraging human\nlanguage processing signals for natural language\nprocessing. In Proceedings of the Second Workshop\non Linguistic and Neurocognitive Resources , pages\n15–27, Marseille, France. European Language Re-\nsources Association.\nNora Hollenstein, Maria Barrett, Marius Troendle,\nFrancesco Bigiolli, Nicolas Langer, and Ce Zhang.\n2019a. Advancing nlp with cognitive language pro-\ncessing signals. arXiv preprint arXiv:1904.02682.\nNora Hollenstein, Antonio de la Torre, Nicolas Langer,\nand Ce Zhang. 2019b. CogniVal: A framework for\ncognitive word embedding evaluation. In Proceed-\nings of the 23rd Conference on Computational Nat-\nural Language Learning (CoNLL) , pages 538–549,\nHong Kong, China. Association for Computational\nLinguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A Lite BERT for self-supervised\nlearning of language representations. In Interna-\ntional Conference on Learning Representations.\nMatej Martinc, Senja Pollak, and Marko Robnik-\nŠikonja. 2021. Supervised and Unsupervised Neu-\nral Approaches to Text Readability. Computational\nLinguistics, 47(1):141–179.\nRyan McDonald, Joakim Nivre, Yvonne Quirmbach-\nBrundage, Yoav Goldberg, Dipanjan Das, Kuz-\nman Ganchev, Keith Hall, Slav Petrov, Hao\nZhang, Oscar Täckström, Claudia Bedini, Núria\nBertomeu Castelló, and Jungmee Lee. 2013. Uni-\nversal Dependency annotation for multilingual pars-\ning. In Proceedings of the 51st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 2: Short Papers), pages 92–97, Soﬁa, Bulgaria.\nAssociation for Computational Linguistics.\nAmil Merchant, Elahe Rahimtoroghi, Ellie Pavlick,\nand Ian Tenney. 2020. What happens to BERT em-\nbeddings during ﬁne-tuning? In Proceedings of the\nThird BlackboxNLP Workshop on Analyzing and In-\nterpreting Neural Networks for NLP , pages 33–44,\nOnline. Association for Computational Linguistics.\nAlessio Miaschi, Dominique Brunato, Felice\nDell’Orletta, and Giulia Venturi. 2020. Lin-\nguistic proﬁling of a neural language model. In\nProceedings of the 28th International Conference\non Computational Linguistics , pages 745–756,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Gin-\nter, Yoav Goldberg, Jan Hajiˇc, Christopher D. Man-\nning, Ryan McDonald, Slav Petrov, Sampo Pyysalo,\nNatalia Silveira, Reut Tsarfaty, and Daniel Zeman.\n2016. Universal Dependencies v1: A multilingual\ntreebank collection. In Proceedings of the Tenth In-\nternational Conference on Language Resources and\nEvaluation (LREC’16), pages 1659–1666, Portorož,\nSlovenia. European Language Resources Associa-\ntion (ELRA).\nKeith Rayner. 1998. Eye movements in reading and\ninformation processing: 20 years of research. Psy-\nchological bulletin, 124 3:372–422.\nLuz Rello, Susana Bautista, Ricardo Baeza-Yates,\nPablo Gervás, Raquel Hervás, and Horacio Saggion.\n2013. One half or 50%? an eye-tracking study\nof number representation readability. In Human-\nComputer Interaction – INTERACT 2013 , pages\n229–245, Berlin, Heidelberg. Springer Berlin Hei-\ndelberg.\nManuela Sanguinetti and Cristina Bosco. 2015. Part-\nTUT: The Turin University Parallel Treebank, pages\n51–69. Springer International Publishing, Cham.\nGabriele Sarti. 2020. UmBERTo-MTSA @ AcCompl-\nIt: Improving complexity and acceptability pre-\ndiction with multi-task learning on self-supervised\nannotations. In Proceedings of Seventh Evalua-\ntion Campaign of Natural Language Processing and\nSpeech Tools for Italian. Final Workshop (EVALITA\n2020), Online. CEUR.org.\nNatalia Silveira, Timothy Dozat, Marie-Catherine\nde Marneffe, Samuel Bowman, Miriam Connor,\nJohn Bauer, and Chris Manning. 2014. A gold stan-\ndard dependency corpus for English. InProceedings\nof the Ninth International Conference on Language\nResources and Evaluation (LREC’14), pages 2897–\n2904, Reykjavik, Iceland. European Language Re-\nsources Association (ELRA).\n58\nAbhinav Deep Singh, Poojan Mehta, Samar Husain,\nand Rajkumar Rajakrishnan. 2016. Quantifying\nsentence complexity based on eye-tracking mea-\nsures. In Proceedings of the Workshop on Com-\nputational Linguistics for Linguistic Complexity\n(CL4LC), pages 202–212, Osaka, Japan. The COL-\nING 2016 Organizing Committee.\nSowmya Vajjala and Ivana Lu ˇci´c. 2018. On-\neStopEnglish corpus: A new corpus for automatic\nreadability assessment and text simpliﬁcation. In\nProceedings of the Thirteenth Workshop on Innova-\ntive Use of NLP for Building Educational Applica-\ntions, pages 297–304, New Orleans, Louisiana. As-\nsociation for Computational Linguistics.\nSowmya Vajjala and Ivana Lu ˇci´c. 2019. On under-\nstanding the relation between expert annotations of\ntext readability and target reader comprehension. In\nProceedings of the Fourteenth Workshop on Innova-\ntive Use of NLP for Building Educational Applica-\ntions, pages 349–359, Florence, Italy. Association\nfor Computational Linguistics.\nShravan Vasishth, Titus von der Malsburg, and Felix\nEngelmann. 2013. What eye movements can tell\nus about sentence comprehension. Wiley interdisci-\nplinary reviews. Cognitive science, 4 2:125–134.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nAmir Zeldes. 2017. The GUM corpus: creating mul-\ntilayer resources in the classroom. Language Re-\nsources and Evaluation, 51:581–612.\nKelly Zhang and Samuel Bowman. 2018. Language\nmodeling teaches you more than translation does:\nLessons learned through auxiliary syntactic task\nanalysis. In Proceedings of the 2018 EMNLP Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP, pages 359–361, Brussels, Bel-\ngium. Association for Computational Linguistics.\n59\nModel & Tokenizer Parameters\nheads dimension 1-Layer Dense\nmax seq. length 128\nembed. dropout 0.1\nseed 42\nlowercasing \u0017\ntokenization SentencePiece\nvocab. size 30000\nTraining Parameters\nPC ET Probes\nﬁne-tuning standard multitask multitask\nfreeze LM w \u0017 \u0017 \u0013\nweighted loss - \u0013 \u0017\nCV folds 5 5 5\nearly stopping \u0013 \u0013 \u0017\ntraining epochs 15 15 5\npatience 5 5 -\nevaluation steps 20 40 -\nbatch size 32 32 32\nlearning rate 1e-5 1e-5 1e-5\nTable 7: Model, tokenizer and training parameters used\nfor ﬁne-tuning ALBERT on complexity metrics.\nA Parametrization and Fine-tuning\nDetails for ALBERT\nWe leverage the pretrained albert-base-v2 check-\npoint available in the HuggingFace’s Trans-\nformer framework (Wolf et al., 2020) and use\nadapted scripts and classes from the FARM frame-\nwork (Deepset, 2019) to perform multitask learn-\ning on eye-tracking metrics. Table 7 presents the\nparameters used to deﬁne models and training pro-\ncedures for experiments in Sections 4 and 5.\nDuring training we compute MSE loss scores\nfor task-speciﬁc heads for the four eye-tracking\nmetrics (ℓFXC , ℓFPD , ℓTFD , ℓTRD ) and perform\na weighted sum to obtain the overall loss scoreℓET\nto be optimized by the model:\nℓET = ℓFXC + ℓFPD + ℓTFD + (ℓTRD ×0.2)\nThe use of ℓTRD was shown to have a positive im-\npact on the overall predictive capabilities of the\nmodel only when weighted to prevent it from dom-\ninating the ℓET sum.\nProbing tasks on linguistic features are per-\nformed by freezing the language model weights\nand training 1-layer heads as probing regressors\nover the last-layer [CLS] token for each feature. In\nthis setting no loss weighting is applied, and the\nregressors are trained for 5 epochs without early\nstopping on the aggregated UD dataset.\nB Examples of Sentences from\nComplexity Corpora\nTable 8 presents examples of sentences randomly\nselected from the two corpora leveraged in this\nstudy. We highlight how eye-tracking scores show\na very consistent relation with sentence length,\nwhile PC scores are much more variable. This fact\nsuggests that the ofﬂine nature of PC judgments\nmakes them less related to surface properties and\nmore connected to syntax and semantics.\nC Models’ Performances on\nLength-binned Sentences\nSimilarly to the approach adopted in Section 3, we\ntest the performances of models on length-binned\ndata to verify if performances on length-controlled\nsequences are consistent with those achieved on the\nwhole corpora. RMSE scores averaged with 5-fold\ncross validation over the length-binned sentences\nsubsets are presented in Figure 3. We note that\nALBERT outperforms the SVM with linguistic fea-\ntures on nearly all lengths and metrics, showing the\nlargest gains on intermediate bins for PC and gaze\ndurations (FPD, TFD, TRD). Interestingly, overall\nperformances of models follow a length-dependent\nincreasing trend for eye-tracking metrics, but not\nfor PC. We believe this behavior can be explained\nin terms of the high sensibility to length previously\nhighlighted for online metrics, as well as the vari-\nability in bin dimensions (especially for the last bin\ncontaining only 63 sentences). We ﬁnally observe\nthat the SVM model based on explicit linguistic\nfeatures (SVM feats) performs poorly on larger\nbins for all tasks, sometimes being even worse than\nthe bin-average baseline. While we found this be-\nhavior surprising given the positive inﬂuence of\nfeatures highlighted in Table 4, we believe this is\nmostly due to the small dimension of longer bins,\nwhich negatively impacts the generalization capa-\nbilities of the regressor. The relatively better scores\nachieved by ALBERT in those, instead, support the\neffectiveness of information stored in pretrained\nlanguage representations when a limited number of\nexamples is available.\n60\nLength bin Sentence PC Score\nBin 10±1 It hasn’t made merger overtures to the board. 2.15\nBin 15±1 For most of the past 30 years, the marriage was one of convenience. 1.45\nBin 20±1 Shanghai Investment & Trust Co., known as Sitco, is the city’s main ﬁnancier for trading business. 3.35\nBin 25±1 For ﬁscal 1988, Ashland had net of $224 million, or $4.01 a share, on revenue of $7.8 billion. 4.55\nBin 30±1 C. Olivetti & Co., claiming it has won the race in Europe to introduce computers based on a\npowerful new microprocessor chip, unveiled its CP486 computer yesterday.\n4.25\nBin 35±1 The White House said he plans to hold a series of private White House meetings, mostly with\nSenate Democrats, to try to persuade lawmakers to fall in line behind the tax cut.\n2.9\nLength bin Sentence FPD FXC TFD TRD\nBin 10±1 Evidently there was a likelihood of John Cavendish being acquitted. 1429 7.69 1527 330\nBin 15±1 I come now to the events of the 16th and 17th of that month. 1704 9.71 1979 467\nBin 20±1 Who on earth but Poirot would have thought of a trial for murder as a\nrestorer of conjugal happiness!\n2745 15.38 3178 1003\nBin 25±1 He knew only too well how useless her gallant deﬁance was, since it was\nnot the object of the defence to deny this point.\n3489 19.77 4181 1012\nBin 30±1 I could have told him from the beginning that this obsession of his over\nthe coffee was bound to end in a blind alley, but I restrained my tongue.\n3638 21.36 4190 1010\nBin 35±1 There was a breathless hush, and every eye was ﬁxed on the famous\nLondon specialist, who was known to be one of the greatest authorities of\nthe day on the subject of toxicology.\n4126 23.14 4814 1631\nTable 8: Example of sentences selected from all the length-binned subset for the Perceived Complexity Corpus (top)\nand the GECO corpus (bottom). Scores are aggregated following the procedure described in Section 2. Reading\ntimes (FPD, TFD, TRD) are expressed in milliseconds.\nFigure 3: Average Root Mean Square Error (RMSE) scores for models in Table 4, performing 5-fold cross-\nvalidation on the same length-binned subsets used for the analysis of Figure 2. Lower scores are better.",
  "topic": "Linguistic sequence complexity",
  "concepts": [
    {
      "name": "Linguistic sequence complexity",
      "score": 0.7248094081878662
    },
    {
      "name": "Computer science",
      "score": 0.7176017165184021
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6110347509384155
    },
    {
      "name": "Natural language processing",
      "score": 0.5945627689361572
    },
    {
      "name": "Sentence",
      "score": 0.5524873733520508
    },
    {
      "name": "Viewpoints",
      "score": 0.48140406608581543
    },
    {
      "name": "Deep linguistic processing",
      "score": 0.4722815454006195
    },
    {
      "name": "Linguistics",
      "score": 0.45922935009002686
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.45723992586135864
    },
    {
      "name": "Perception",
      "score": 0.45282799005508423
    },
    {
      "name": "Language model",
      "score": 0.44412845373153687
    },
    {
      "name": "Linguistic competence",
      "score": 0.4126364588737488
    },
    {
      "name": "Psychology",
      "score": 0.20268580317497253
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210124522",
      "name": "Institute for Computational Linguistics “A. Zampolli”",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I138549579",
      "name": "Scuola Internazionale Superiore di Studi Avanzati",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I142444530",
      "name": "University of Trieste",
      "country": "IT"
    }
  ]
}