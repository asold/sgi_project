{
    "title": "Credible without Credit: Domain Experts Assess Generative Language Models",
    "url": "https://openalex.org/W4385572185",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5092596336",
            "name": "Denis Peskoff",
            "affiliations": [
                "Princeton University"
            ]
        },
        {
            "id": "https://openalex.org/A2109592939",
            "name": "Brandon Stewart",
            "affiliations": [
                "Princeton University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4317390716",
        "https://openalex.org/W3134312382",
        "https://openalex.org/W4285251400",
        "https://openalex.org/W2898970033",
        "https://openalex.org/W2996844929",
        "https://openalex.org/W3125036114",
        "https://openalex.org/W3035507081",
        "https://openalex.org/W3183624231",
        "https://openalex.org/W4389518954",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4313451803",
        "https://openalex.org/W3124478239",
        "https://openalex.org/W2886572631",
        "https://openalex.org/W2095932468",
        "https://openalex.org/W4281690148",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W1560729591"
    ],
    "abstract": "Language models have recently broken into the public consciousness with the release of the wildly popular ChatGPT. Commentators have argued that language models could replace search engines, make college essays obsolete, or even write academic research papers. All of these tasks rely on accuracy of specialized information which can be difficult to assess for non-experts. Using 10 domain experts across science and culture, we provide an initial assessment of the coherence, conciseness, accuracy, and sourcing of two language models across 100 expert-written questions. While we find the results are consistently cohesive and concise, we find that they are mixed in their accuracy. These results raise questions of the role language models should play in general-purpose and expert knowledge seeking.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 427–438\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nCredible Without Credit:\nDomain Experts Assess Generative Language Models\nDenis Peskoff\nPrinceton University\nOffice of Population Research\ndp2896@princeton.edu\nBrandon M. Stewart\nPrinceton University\nSociology and Office of Population Research\nbms4@princeton.edu\nAbstract\nLanguage models have recently broken into\nthe public consciousness with the release of\nthe wildly popular ChatGPT. Commentators\nhave argued that language models could re-\nplace search engines, make college essays ob-\nsolete, or even write academic research papers.\nAll of these tasks rely on accuracy of special-\nized information which can be difficult to as-\nsess for non-experts. Using 10 domain experts\nacross science and culture, we provide an ini-\ntial assessment of the coherence, conciseness,\naccuracy, and sourcing of two language models\nacross 100 expert-written questions. While we\nfind the results are consistently cohesive and\nconcise, we find that they are mixed in their\naccuracy. These results raise questions of the\nrole language models should play in general-\npurpose and expert knowledge seeking.\n1 Do Experts Agree with ChatGPT?\nSince its release in late November 2022, Chat-\nGPT has gained over 100 million users in just two\nmonths and been the subject of breathless coverage\nnews coverage which claims it threatens to \"replace\nsearch engines\" (Loten, 2022; Grant and Metz,\n2022), kill the college essay (Marche, 2022), and\nautomate the writing of scientific research (Stokel-\nWalker, 2023). These tasks are distinct from the\nkind usually evaluated in NLP because they all rely\non expert-level knowledge. In this paper, we survey\n10 experts to obtain subjective assessments of how\ntwo recent language models engage with questions\nin diverse domains.\nOur efforts build on prior work to evaluate\nthe capabilities of language models. Language\nmodels are now regularly subjected to extensive\nbenchmarks which cover a variety of standard\nNLP tasks (Wang et al., 2019; Brown et al., 2020;\nRibeiro et al., 2020; Srivastava et al., 2022). Re-\ncent efforts engage in domain-specific tasks such\nas taking the bar or medical licensing exams (Katz\nFigure 1: Average ratings by domain experts of lan-\nguage model generated answers to 100 questions across\n10 domains (higher values indicate better performance\nof the language model, 95% confidence intervals com-\nputed by expert-blocked bootstrap). Language models\nscore high on cohesion and conciseness, modest on ac-\ncuracy, and poor on sourcing of their answers. Both\nmodels are sometimes ranked ahead of Wikipedia.\net al., 2023; Kung et al., 2023) and making political\narguments (Palmer and Spirling, 2023). Liu et al.\n(2023), released on arXiv while this paper was\nunder review, evaluates the ability of generative\nsearch engines to answer a range of general knowl-\nedge queries. We complement these efforts by\nhaving experts craft their own information-seeking\nquestions and evaluate the generated responses.\nIn the next section, we briefly discuss the role\nof expertise in language models and our goals in\nevaluating it. We then describe our methodology\n427\n(Section 3). We find ChatGPT and YouChat to be\ncohesive and coherent (Section 4.1), generally ac-\ncurate with some misses (Section 4.2), and ubiqui-\ntously lacking in sources (Section 4.3). A majority\nof our experts recommend these models for general\npurpose questions but not for professional settings\n(Section 4.4). We conclude with implications and\ncontrast with the contemporaneous findings in Liu\net al. (2023) (Section 5).\n2 “Expertise” in Language Models\nIndividuals and companies are increasingly looking\nto language models as a source of expert question\nanswering. For example, Metzler et al. (2021) lays\nout a vision for search that involves language mod-\nels providing answers to user-generated questions.\nUnfortunately, the challenge for many language\nmodels is that they are trained to generate language,\nnot to have correct answers. As Shah and Bender\nwrite, “to the extent that [language models] some-\ntimes get the right answer to. . . questions [it] is\nonly because they happen to synthesize relevant\nstrings out of what was in their training data. No\nreasoning is involved\" (Shah and Bender, 2022, pg.\n222). This has led Narayanan and Kapoor (2022)\nto characterize ChatGPT as a “bullshit generator\"—\nplausible, but not accurate (Frankfurt, 2005). While\nlanguage models might incidentally produce accu-\nrate answers to simple and uncontested queries\n(e.g., “what is the capital of France?\"), we might be\nunderstandably skeptical that it will produce cor-\nrect answers to more nuanced questions. Generated\nlanguage reflects its training data and—to the ex-\ntent the training data is publicly known—it is more\nreflective of the web than expert speech (Bender\net al., 2021). By using experts evaluating material\nin their domain of choice, we provide an initial\nassessment of expertise provided by these models.\nUltimately what constitutes sufficient accuracy for\nbroader use depends on the use case.\n3 Methodology\nWe evaluate two recently-released language mod-\nels: OpenAI’s ChatGPT and You.com’s YouChat\n(Google’s Bard and many other options weren’t\nreleased at the time of initial submission). Ope-\nnAI’s ChatGPT is the wildly popular evolution of\nthe GPT-3 model (Brown et al., 2020) and YouChat\nis built specifically for search. Both systems have\na free and public option (at the time of writing)\nwhich makes them generally accessible.\nWe survey 10 experts across a range of\narbitrarily-chosen disciplines from quantum infor-\nmation to ballroom dance (see a complete list in the\nappendix). We recruited experts from our personal\nnetworks aiming to cover a wide-range of differ-\nent types of knowledge (with the understanding\nwe cannot be exhaustive or representative). The\nmajority hold a doctorate or medical degree.\nWe asked each expert to fill out an online survey\nwith their own description of their area of expertise,\ntwo Wikipedia pages pertinent to it, and five com-\nmon questions and five niche questions from their\ndomain (see Table 1 for examples).1 In a second\nwave of the survey, we provide answers generated\nfrom these questions using ChatGPT and YouChat\nand ask them to rank the answers on a 5-point\nLikert-type item for coherence, conciseness, accu-\nracy, sourcing, and quality of content relative to\nWikipedia (Likert, 1932). We ask for open-ended\nfeedback on answers and alternate which system\nthe experts evaluate first. Questions are designed to\nallow experts to focus on their own area of expertise\nwhile providing an opportunity to distinguish be-\ntween different levels of knowledge-specialization.\nThe survey took one hour on average. Six experts\nwere surveyed in January and four in May of 2023.\nThe survey design elicits subjective expert judg-\nment of system performance. We evaluate coher-\nence, conciseness, and accuracy as important prop-\nerties in information-seeking (Cambazoglu et al.,\n2021). Comparing assessments to Wikipedia pro-\nvides a difficult-to-beat baseline with which many\npeople are already familiar. We also ask whether\nthe language model provides a source for its infor-\nmation. Evaluating the source of the information in\nthe response is important not only for the purposes\nof giving credit, but also as a mechanism for ac-\ncountability (Bender et al., 2021; Liu et al., 2023).\nAfter all the questions, we directly ask whether the\nexpert would recommend the tool for general pur-\npose and professional use, and if the style of the\ncontent is obviously automated (Dou et al., 2022).\nWe make our data (including the full context for\nall quotes we use here) publicly available to help\nsupport future work.2\n1Prompts for the questions request different conditions\nsuch as length, uncertainty, and requesting a source. See the\nsupplement for the survey instrument.\n2https://denis.ai/bot_thoughts/\n428\nArea Example Common Question Example Niche Question\nFamily\nMedicine\nDoes everyone get cancer cells\nin their body?\nAre there some parts of cognitive function\nthat improve with age after age 40?\nRadiology Can imaging studies help diag-\nnose traumatic injuries?\nWhat is better to detect and diagnose necro-\ntizing enterocolitis in infants, x rays or ultra-\nsound?\nToxicology What are the most common\nlaboratory findings in a patient\nwith methanol poisoning?\nIn what scenario should methylene blue be\nadministered for methemoglobinemia?\nGeo-\narchaelogy\nWhat formed parallel and per-\npendicular straight water chan-\nnels in Mesoamerica?\nCould geological faulting create parallel\nchannels (streams or canals) that cross at\nright angles?\nNeuro-\nscience\nHow do neurons communicate\nin the brain?\nAre DREADDs a \"good\" method for manip-\nulating neural activity?\nQuantum\nInformation\nWhat speedups can quantum\ncomputers achieve over classi-\ncal computers?\nAre superconducting qubits the best plat-\nforms for realizing quantum computers\nwhen compared to trapped ion or semicon-\nductor or neutral-atom qubits?\nLinguistics Are there drawbacks to being\nbilingual?\nIs any aspect of syntax universal across all\nlanguages?\nDigital\nHumanities\nWhich was the first novel? What is the fate of formalism today in liter-\nary criticism?\nBallroom What are some of the most\npopular ballroom or Latin\nAmerican partner dances?\nShould a competitive dancer seek the opin-\nions of a wide variety of dance judges?\nOpera Which is the best version of\n\"Tannhäuser\"?\nWhy did Rossini stop composing operas at\nage 37?\nTable 1: A common and niche question, selected from one of 10, written by the experts from our 10 domains.\n4 Results Across Disciplines\nWe discuss two running examples: one about Maria\nCallas, a famous New York-born opera singer\nwith an international career often associated with\nGreece, and one about quantum information. Our\nopera expert asks, “Was Maria Callas American?”.\nChatGPT answers “No, Maria Callas was not\nAmerican. Maria Callas, whose full name was\nMaria Anna Cecilia Sofia Kalogeropoulos, was\nborn on December 2, 1923, in New York City,\nUnited States. However, she was of Greek de-\nscent. Her parents were Greek immigrants, and\nshe later acquired Greek citizenship. Maria Callas\nis renowned as one of the greatest opera singers of\nthe 20th century and was particularly known for\nher performances in the bel canto repertoire. ”\nAs a second example, a quantum expert asks\n“Can quantum computers be realized in the phys-\nical world?”. YouChat answers, “Yes, quantum\ncomputers can be realized in the physical world.\nResearchers have been able to build prototype sys-\ntems that demonstrate the principles of quantum\ncomputing, and there are a number of research\ngroups and companies working on building more\nadvanced versions. Additionally, several compa-\nnies have announced plans to build full-scale quan-\ntum computers in the next few years. ”\n4.1 Answers Are Credible\nAnswers are judged as coherent (avg 4.5), and con-\ncise (avg 4.2) by our experts. Responses generally\nrestate the question, provide relevant information,\nare grammatically correct, and are formal in tone.\nThe responses were quite stable on regeneration.\nAlthough results are relatively concise, they do\ndiffer in length. ChatGPT’s answer to the question\nabout Maria Callas is four sentences including a\nfinal sentence about her career that is completely\nunrelated (YouChat’s is 3). For the question on\nquantum information we gave above, ChatGPT\nprovided a three paragraph answer which our ex-\n429\npert described as “\"a well constructed and nuanced\nanswer that synthesizes information from multiple\nperspectives\"” while YouChat used three sentences.\n4.2 Uneven Accuracy\nWhile responses are fairly uniform in coherence,\nthey are uneven in terms of accuracy (with 111 of\nthe 200 responses marked as one of the two most ac-\ncurate categories and 38 marked in the two lowest\naccuracy categories). Surprisingly, niche questions\nwere only slightly less accurate than common ones\n(-.16). Examining the comments suggest that the\nrankings reflect fairly different standards for what\ncounts as accurate (expert ratings are included in\nparentheses below where a 1 is completely inac-\ncurate and a 5 is completely accurate). We urge\ncaution in interpreting the averages.\nOn the question about Maria Callas, ChatGPT\nasserts \"No\" while clarifying that she was born in\nNew York (1) while YouChat answers \"Yes\" (2).\nBoth comment on her additional Greek citizenship.\nOur expert gave both quantum information answers\ntop marks for accuracy (ChatGPT:5, YouChat:4).\nSeven experts gave at least one answer the lowest\naccuracy score suggesting it is completely wrong.\nFor example in a toxicology answer, ChatGPT\ngave “a list of causes of anion gap acidosis in-\nstead of NON-anion gap acidosis” (1). Similarly\nYouChat answered the wrong question from our\nballroom dance expert by confusing “the Viennese\nWaltz with the Waltz. The answer describes an en-\ntirely different dance from the dance the question\nis about” (1). Many other answers though were\nquite accurate. Our geoarcheologist expressed a\ncommon sentiment that the responses are “basic but\ngenerally correct” (4). Other answers were “excel-\nlent, nuanced” (5, toxicologist). The fairly uniform\ncoherence makes it difficult for a non-expert to\ndiscern the correct information from the noise.\nThe answers also varied in their ability to capture\nuncertainty in the field overall. Our neuroscientist\nnoted that ChatGPT “accurately captured the con-\ntroversy surrounding use of DREADDs” (5) but\nthat YouChat “was unable to capture the longstand-\ning controversy” (4). The toxicologist noted that\nChatGPT offered a “definitive answer to something\nthat is not totally agreed upon” on the subject of\ndialysis for lithium poisoning (3). By contrast, our\nlinguist observed on a niche question that “the re-\nsponse to the query about complex predicates is\nappropriately waffly” (5).\nWe close this section by noting that even for ex-\nperts, assessing accuracy can be complicated. Our\nlinguist notes “I would say that the response is\ninvalid, but there are linguists who would agree\nwith it and YouChat does flag the fact its controver-\nsial” (2) and the geoarcheologist cited overclaim-\ning, writing that YouChat “takes too strong a posi-\ntion that the evidence does not back up” (1). Such\ncases are difficult to adjudicate—what counts as\nsufficient evidence?—but the difficulty is inevitable\nwith complex questions.\n4.3 Sourcing is Almost Completely Absent\nOur clearest finding is that most answers by the\nlanguage models do not provide any source for their\ninformation. Only 11 out of 100 ChatGPT answers\nand 19 of 100 YouChat answers were scored more\nthan the lowest value for sourcing. Neither system\nprovides a source for Maria Callas’ biographical\ninformation nor concrete examples of physically-\nrealized quantum computers.\nWhen sources are provided, they are often vague,\nirrelevant, or misleading. The neuroscientist re-\nmarked on the first problem writing, “the refer-\nences are vague; it can cite the names of scientific\njournals and books but not specific articles or book\nchapters”. When the models provide a source we\nfound that it was often a (only tangentially relevant)\nWikipedia article (Figure 2 provides an anomalous\nexample). These are sometimes loosely related by\nkeywords, but still irrelevant such as a reference\nto Wikipedia’s article on post-traumatic epilepsy\nfor a question about using imaging to diagnose\ntraumatic injuries. In a question on quantum in-\nformation, a relevant Stephen Hawking paper was\nrecommended, but an unrelated link was provided.\nPerhaps the most serious concern is where an\nauthoritative source is invoked, but inaccurately.\nWhen asked “What should a radiologist recom-\nmend to a patient after the radiologist incidentally\ndetects a thyroid nodule on a chest CT scan done\nfor another reason?” ChatGPT claims, “The Amer-\nican Thyroid Association recommends that patients\nwith a thyroid nodule larger than 1 cm or with\nsuspicious features on imaging should undergo a\nfine-needle aspiration (FNA) biopsy. ”But, “neither\nthe ACR not ATA recommend that patients with\na thyroid nodule larger than 1 cm should categori-\ncally undergo fine-needle aspiration”! This echoes\nprevious findings in the domain of medicine, where\nwork evaluating previous generations of voice as-\n430\nFigure 2: The Wikipedia pages for Hello Muddah, Hello Fadduh (A Letter from Camp) and Mona Lisa are\nunsurprisingly not the correct sources for the legal name of a librettist, despite the YouChat interface’s suggestion.\nsistants has shown that they provided inaccurate\nmedical answers that could have proven fatal (Bick-\nmore et al., 2018). Our neuroscientist asked a niche\nquestion where YouChat identified a specific jour-\nnal article, but it appears to be made up (neither\nwe, nor she, were able to find it) although she did\njudge the answer as completely accurate.\n4.4 Mixed Recommendations for Use\nOnly 3 of the 10 experts would recommend us-\ning ChatGPT and 0 of the 10 would recommend\nYouChat in professional setting (rating of 4 or\nhigher, where 5 is \"full confidence\"). How-\never, the majority would endorse both systems\nfor general purposes questions about their domain\n(70% rating of 4 or higher)—more than would en-\ndorse Wikipedia for the same (60% rating of 4 or\nhigher). The family physician summarized a com-\nmon theme, “once again Wikipedia has extensive\narticles on life expectancy extension but nowhere\nnear as concise as this” and the linguist wrote on\nYouChat’s answer, “this is an excellent concise re-\nsponse, although wiki provides more information\n(as usual).”\n5 Discussion\nLanguage models were coherent, but undersourced\nand not always accurate. They were generally not\nendorsed for professional use, but were seen as\nvaluable by some experts as a source of knowledge\nfor people out of the domain. Providing sourcing\ncitations will be an important step in building confi-\ndence. Citations are sufficiently inconsistent when\nthey appear to merit verifying important results.\nOur findings are reinforced by the contempora-\nneous work of Liu et al. (2023) which provides a\nmore systematic audit of four generative search en-\ngines (including YouChat, but not ChatGPT) on a\ndiverse series of queries (including common google\nsearches and questions on Reddit) using 34 pre-\nscreened MTurk annotators. They also find that\nthese search engines are “credible without credit”—\nhaving high fluency and perceived utility, but insuf-\nficient sourcing. They find that about half of the\nresponses are fully supported by citations and three\nfourths of the citations given didn’t actually support\nthe sentence. One of their main findings is a neg-\native correlation between citation recall/precision\nand fluency/perceived utility. Sourcing is so ab-\nsent in our study that we observe no meaningful\ncorrelation with other variables and accuracy has\npositive correlation with cohesion and conciseness.\nFurther work could investigate if these discrepan-\ncies are due to differences in the systems evaluated,\nthe kinds of questions asked, or the judgments of\nexperts vs. annotators. This difference aside, their\nfindings resonate with ours that credibility with-\nout credit should make us cautious in looking to\nlanguage models as a source of expertise.\nLimitations\nOur study has three important limitations. First, our\nstudy is small in scope. By their nature, experts are\ndifficult to recruit and consequently the domains\nwe can cover are limited. The small sample also\nsuggests that the quantitative measures may not be\nstable in a larger or more representative sample.\n431\nSecond, our observation process was somewhat\nartificial. We generated replies for our experts and\ndid not to do any prompt tuning. This reflects\nthe way the expert chose to ask the question, but\ndoes not capture the ceiling of performance that\nwould be possible in a conversation. As the Family\nMedicine expert noted about our question compar-\ning Wikipedia to ChatGPT, “for more detail one\ncould spend more time with Wikipedia and to the\norganization themselves, but chat provides an im-\nmediate general summary and the opportunity to\ndrill down further with ongoing questions and con-\nversation.I have used chat GTP to do medical and\nbiological research In a matter of minutes which\nwould have taken me hours previously”. A more\nextensive study on information seeking behaviors\nwould be of interest and Liu et al. (2023) is a useful\nstep in that direction.\nThird, the responses across experts are not nec-\nessarily comparable. We allowed experts to choose\ntheir own questions and provide their own inter-\npretations of the key measures like coherence or\nconciseness. Comparability of scales across con-\ntexts is a long-standing problem in survey research\n(King and Wand, 2007) and we highlight some of\nthe concerns around the accuracy question above.\nNevertheless, we felt that asking a set of closed-\nended questions would help to provide some ag-\ngregate judgment, adding some systematic data\nto the anecdotes shared in public forums. While\nwe caution about drawing any binding conclusions\nfrom our preliminary work, we felt that given the\nfast-evolving nature of the technology, a quick as-\nsessment was merited. Our findings are broadly\nsupported using different questions and methodol-\nogy in Liu et al. (2023).\nOne important aspect that is out of scope in our\nanalysis is differential accuracy by question asker.\nLatanya Sweeney’s classic study of racial discrimi-\nnation in online ads (Sweeney, 2013) points to the\npossibility that how a question is asked or where it\nis asked from could result in inaccurate or harmful\nanswers for marginalized communities (see also\nNoble, 2018; Benjamin, 2019). We have also fo-\ncused exclusively on English language questions\nand answers, but differences in easily-available\ntraining data across languages can produce sub-\nstantial differences in the information offered. For\nexample, Yang and Roberts (2021) shows that em-\nbeddings trained on Baidu Baike—an online Chi-\nnese encyclopedia—encode substantially different\nassociations with sensitive historical events and\npeople than Chinese Language Wikipedia (which\nis regularly blocked in China). There is much more\nto understand about the degree to which large lan-\nguage models can mimic expertise.\nEthics Statement\nWork was approved by Princeton University’s IRB\nunder Proposal 15346. No deception was used in\nthe experiment and we screened language model\nresponses for any sensitive content before passing\nthem to the experts (although we did not encounter\nany). Participants were not compensated for par-\nticipation and gave consent to be identified. All\nappropriate IRB protocols in providing instructions\nand gathering consent were followed.\nAcknowledgements\nResearch reported in this publication was supported\nby The Eunice Kennedy Shriver National Institute\nof Child Health & Human Development of the\nNational Institutes of Health under Award Num-\nber P2CHD047879. Additionally, this material is\nbased upon work supported by the National Science\nFoundation under Grant # 2127309 to the Comput-\ning Research Association for the CIFellows 2021\nProject. This work would not have been possible\nwithout all the experts generously donating their\nbusy time. We are extremely grateful to them.\nReferences\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nRuha Benjamin. 2019. Race after technology: Aboli-\ntionist tools for the new jim code. Polity, Cambridge.\nTimothy W Bickmore, Ha Trinh, Stefan Olafsson,\nTeresa K O’Leary, Reza Asadi, Nathaniel M Rickles,\nand Ricardo Cruz. 2018. Patient and consumer safety\nrisks when using conversational assistants for medi-\ncal information: an observational study of siri, alexa,\nand google assistant. Journal of medical Internet\nresearch, 20(9):e11510.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\n432\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nB Barla Cambazoglu, Valeriia Baranova, Falk Scholer,\nMark Sanderson, Leila Tavakoli, and Bruce Croft.\n2021. Quantifying human-perceived answer utility\nin non-factoid question answering. In Proceedings\nof the 2021 Conference on Human Information Inter-\naction and Retrieval, pages 75–84.\nYao Dou, Maxwell Forbes, Rik Koncel-Kedziorski,\nNoah A. Smith, and Yejin Choi. 2022. Is GPT-3 text\nindistinguishable from human text? scarecrow: A\nframework for scrutinizing machine text. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 7250–7274, Dublin, Ireland. Association\nfor Computational Linguistics.\nHarry G Frankfurt. 2005. On bullshit. Princeton Uni-\nversity Press.\nNico Grant and Cate Metz. 2022. A new chat bot is\na ‘code red’ for google’s search business. The New\nYork Times.\nDaniel Martin Katz, Michael James Bommarito, Shang\nGao, and Pablo Arredondo. 2023. Gpt-4 passes the\nbar exam. Available at SSRN 4389233.\nGary King and Jonathan Wand. 2007. Comparing in-\ncomparable survey responses: Evaluating and select-\ning anchoring vignettes. Political Analysis, 15(1):46–\n66.\nTiffany H Kung, Morgan Cheatham, Arielle Medenilla,\nCzarina Sillos, Lorie De Leon, Camille Elepaño,\nMaria Madriaga, Rimel Aggabao, Giezel Diaz-\nCandido, James Maningo, et al. 2023. Performance\nof chatgpt on usmle: Potential for ai-assisted medical\neducation using large language models. PLoS digital\nhealth, 2(2):e0000198.\nRensis Likert. 1932. A technique for the measurement\nof attitudes. Archives of psychology, 22(140):5–55.\nNelson F Liu, Tianyi Zhang, and Percy Liang. 2023.\nEvaluating verifiability in generative search engines.\narXiv preprint arXiv:2304.09848.\nAndrew Loten. 2022. Chatty ai and protein-predicting\nalgorithm defined the year in emerging tech. The\nWall Street Journal.\nStephen Marche. 2022. The college essay is dead. The\nAtlantic.\nDonald Metzler, Yi Tay, Dara Bahri, and Marc Najork.\n2021. Rethinking search: making domain experts\nout of dilettantes. In ACM SIGIR Forum, volume 55,\npages 1–27. ACM New York, NY , USA.\nArvind Narayanan and Sayash Kapoor. 2022. Chatgpt\nis a bullshit generator. but it can still be amazingly\nuseful. AI Snake Oil.\nSafiya Umoja Noble. 2018. Algorithms of oppression.\nIn Algorithms of Oppression. New York University\nPress.\nAlexis Palmer and Arthur Spirling. 2023. Large lan-\nguage models can argue in convincing and novel\nways about politics: Evidence from experiments and\nhuman judgement. Technical report, Working paper),\nTechnical report.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,\nand Sameer Singh. 2020. Beyond accuracy: Be-\nhavioral testing of NLP models with CheckList. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4902–\n4912, Online. Association for Computational Lin-\nguistics.\nChirag Shah and Emily M Bender. 2022. Situating\nsearch. In ACM SIGIR Conference on Human Infor-\nmation Interaction and Retrieval, pages 221–232.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, and\net. al. 2022. Beyond the imitation game: Quantifying\nand extrapolating the capabilities of language models.\narXiv preprint arXiv:2206.04615.\nChris Stokel-Walker. 2023. Chatgpt listed as author on\nresearch papers: many scientists disapprove. Nature,\n613(7945):620–621.\nLatanya Sweeney. 2013. Discrimination in online ad\ndelivery. Communications of the ACM, 56(5):44–54.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th In-\nternational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\nOpenReview.net.\nEddie Yang and Margaret E Roberts. 2021. Censorship\nof online encyclopedias: Implications for nlp mod-\nels. In Proceedings of the 2021 ACM Conference on\nFairness, Accountability, and Transparency, pages\n537–548.\n433\nA Appendix\nA.1 Study Participants\nThe experts who wrote and evaluated these ques-\ntions are enumerated in Table 2.\nA.2 Reproducibility\nWe provided three reproductions of the first ques-\ntions for evaluation. An example is provided in\nTable 3.\nA.3 Data Examples\nWe provide example ChatGPT and YouChat an-\nswers for the opera and quantum information ex-\namples in Table 4.\nA.4 Data Collection\nThese questions were sourced through the form\nshown in Figures 3 and evaluated through the form\nin Figure 4.\nA.5 Additional Data\nWe release our data publicly at\nhttps://denis.ai/bot_thoughts/.\nSpeciality Name\nFamily Medicine Dr. D. Andrew MacFarlan\nRadiology Dr. Dionysios Koroulakis\nToxicology Dr. David Schaffer\nGeoarchaeology Dr. Timothy Beach\nNeuroscience Elizabeth Tawa\nQuantum Info Dr. Andrew Guo\nLinguistics Dr. Adele Goldberg\nDigital Humanities Dr. Ryan Heuser\nBallroom Dan Calloway\nOpera Fred Plotkin\nTable 2: Participants and their specialties.\nREGENERATION OF QUESTION\nHuman: “Can imaging studies help diagnose\ntraumatic injuries?”\nChatGPT: “Yes, imaging studies such as X-\nrays, CT scans, and MRI can be used to help\ndiagnose traumatic injuries. They can be used\nto identify broken bones, internal bleeding,\nand other injuries that may not be visible on\nthe surface of the body. Imaging studies can\nalso be used to monitor the healing process\nand determine the severity of an injury. ”\nChatGPT: “Imaging studies such as X-rays,\nCT scans, and MRIs can help diagnose trau-\nmatic injuries by providing detailed images\nof the affected area. These images can reveal\nbone fractures, dislocations, and injuries to\ninternal organs and soft tissue. They can also\nbe used to monitor the healing process and\ndetect complications. In some cases, a com-\nbination of imaging studies may be needed to\nfully evaluate the extent of an injury. ”\nChatGPT: “Yes, imaging studies such as X-\nrays, CT scans, and MRI scans can help di-\nagnose traumatic injuries. These imaging\ntechniques can provide detailed pictures of\nthe bones, soft tissue, and internal organs,\nwhich can help physicians identify and di-\nagnose injuries such as broken bones, head\ninjuries, internal bleeding, and other condi-\ntions. However, it’s important to note that\nimaging studies are not always necessary for\nthe diagnosis of traumatic injuries and the\ndecision to use imaging should be based on\nthe individual patient’s history and clinical\npresentation. ”\nTable 3: Regeneration of a question by ChatGPT.\n434\n(a) Prompts for common questions.\n (b) Prompts for niche questions.\n(a) Feedback is provided along a Likert scale for each question.\n (b) A summary judgement is made after all questions.\n435\nHuman: “Was Maria Callas American?”\nChatGPT: “No, Maria Callas was not American. Maria Callas, whose full name was Maria\nAnna Cecilia Sofia Kalogeropoulos, was born on December 2, 1923, in New York City, United\nStates. However, she was of Greek descent. Her parents were Greek immigrants, and she later\nacquired Greek citizenship. Maria Callas is renowned as one of the greatest opera singers of\nthe 20th century and was particularly known for her performances in the bel canto repertoire. ”\nYouChat: “Yes, Maria Callas was American-born. She was born in Manhattan, New York City,\nto Greek immigrant parents. However, she later established her career in Italy and became\nknown as a famous Greek soprano. ”\nHuman: “Can quantum computers be realized in the physical world?”\nChatGPT: “Quantum computers are a theoretical concept that has not yet been fully realized\nin the physical world. However, significant progress has been made in recent years in the\ndevelopment of quantum computing hardware, and several companies and research groups\nhave built prototype quantum computers that are capable of running small-scale quantum\nalgorithms. These prototypes are often referred to as \"Noisy Intermediate-Scale Quantum\"\n(NISQ) computers, as they are not yet capable of achieving the full potential of quantum\ncomputing, but they are able to perform some quantum computations that are beyond the\ncapabilities of classical computers.\nThere are several different physical implementations of quantum computing being researched,\nincluding superconducting qubits, trapped ions, topological qubits and so on. Each of these\napproaches has its own set of challenges and limitations, and it is not yet clear which approach\nwill ultimately be the most successful in building large-scale, practical quantum computers.\nIt’s worth to note that, currently, the most advanced quantum computers are still far from\nbeing able to perform any practical computations, and it’s still uncertain when, or even if,\nfully-functional, large-scale, fault-tolerant quantum computers will be built. ”\nYouChat: “Yes, quantum computers can be realized in the physical world. Researchers have\nbeen able to build prototype systems that demonstrate the principles of quantum computing,\nand there are a number of research groups and companies working on building more advanced\nversions. Additionally, several companies have announced plans to build full-scale quantum\ncomputers in the next few years. ”\nTable 4: The full ChatGPT and YouChat responses to the two in-paper examples.\n436\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLIMITATIONS\n□\u0013 A2. Did you discuss any potential risks of your work?\nETHICS\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract, 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNo response.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0017 Did you run computational experiments?\nLeft blank.\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nNo response.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n437\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nNo response.\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nNo response.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNo response.\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nEthics\n□\u0013 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nAppendix, Section 2\n□\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nEthics / IRB\n□\u0013 D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nEthics/ IRB\n□\u0013 D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nEthics\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Participants are identiﬁed directly. Appendix A\n438"
}