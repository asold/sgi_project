{
  "title": "Topological Planning with Transformers for Vision-and-Language Navigation",
  "url": "https://openalex.org/W3111566807",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2420348258",
      "name": "Chen Kevin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226836294",
      "name": "Chen, Junshen K.",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Chuang, Jo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2747924198",
      "name": "V√°zquez, Marynel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744670902",
      "name": "Savarese, Silvio",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3091037708",
    "https://openalex.org/W3192162805",
    "https://openalex.org/W3011144238",
    "https://openalex.org/W2030021468",
    "https://openalex.org/W2964043796",
    "https://openalex.org/W2964935470",
    "https://openalex.org/W2970972751",
    "https://openalex.org/W3035232877",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2884565639",
    "https://openalex.org/W2963477323",
    "https://openalex.org/W2593841437",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2777629900",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W2070789508",
    "https://openalex.org/W2963245725",
    "https://openalex.org/W3040922163",
    "https://openalex.org/W2963948945",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2553701371",
    "https://openalex.org/W2963800628",
    "https://openalex.org/W2970834748",
    "https://openalex.org/W2567015638",
    "https://openalex.org/W2154844948",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W1136193735",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2962744691",
    "https://openalex.org/W2805516822",
    "https://openalex.org/W2926977875",
    "https://openalex.org/W2962789679",
    "https://openalex.org/W2963726321",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2772545238",
    "https://openalex.org/W2995460200",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W3014266552",
    "https://openalex.org/W3046140993",
    "https://openalex.org/W3004691725",
    "https://openalex.org/W3034728521",
    "https://openalex.org/W2914608873",
    "https://openalex.org/W2962812366",
    "https://openalex.org/W2962879844",
    "https://openalex.org/W2978854499",
    "https://openalex.org/W2974759213"
  ],
  "abstract": "Conventional approaches to vision-and-language navigation (VLN) are trained end-to-end but struggle to perform well in freely traversable environments. Inspired by the robotics community, we propose a modular approach to VLN using topological maps. Given a natural language instruction and topological map, our approach leverages attention mechanisms to predict a navigation plan in the map. The plan is then executed with low-level actions (e.g. forward, rotate) using a robust controller. Experiments show that our method outperforms previous end-to-end approaches, generates interpretable navigation plans, and exhibits intelligent behaviors such as backtracking.",
  "full_text": "Topological Planning with Transformers for Vision-and-Language Navigation\nKevin Chen\nStanford University\nkevin.chen@cs.stanford.edu\nJunshen K. Chen\nStanford University\njkc1@stanford.edu\nJo Chuang\nStanford University\njochuang@stanford.edu\nMarynel V¬¥azquez\nYale University\nmarynel.vazquez@yale.edu\nSilvio Savarese\nStanford University\nssilvio@stanford.edu\nAbstract\nConventional approaches to vision-and-language navi-\ngation (VLN) are trained end-to-end but struggle to per-\nform well in freely traversable environments. Inspired by\nthe robotics community, we propose a modular approach\nto VLN using topological maps. Given a natural language\ninstruction and topological map, our approach leverages\nattention mechanisms to predict a navigation plan in the\nmap. The plan is then executed with low-level actions (e.g.\nFORWARD , ROTATE) using a robust controller. Experiments\nshow that our method outperforms previous end-to-end ap-\nproaches, generates interpretable navigation plans, and ex-\nhibits intelligent behaviors such as backtracking.\n1. Introduction\nEnabling robots to understand natural language and carry\nout communicated tasks has long been desired. A critical\nstep towards this goal in the context of mobile robotics is\nvision-and-language navigation (VLN) [4]. In VLN, the\nagent is provided with a navigation instruction such as:\n‚ÄúExit the bedroom, walk to the end of the hall, and enter\nthe kitchen on your right .‚Äù The agent is then expected to\nfollow the instruction and move to the speciÔ¨Åed destination.\nThe majority of VLN systems [18, 22, 27, 32, 33, 46, 51,\n57] are end-to-end deep learning models and utilize unstruc-\ntured memory such as LSTM [24]. These methods work\nwell when the movement is constrained to pre-deÔ¨Åned lo-\ncations, but performance drops signiÔ¨Åcantly when the agent\nis allowed to move freely [29]. Moreover, learning to per-\nform navigation, including mapping, planning, and control,\nin a fully end-to-end manner can be difÔ¨Åcult and expensive.\nSuch approaches often require millions of frames of expe-\nrience [11, 50, 52], and yet performance substantially de-\ngrades without ground truth odometry [11, 52].\nTo address the aforementioned issues, recent visual robot\nnavigation literature has explored using structured memory\n(e.g. metric maps, topological memory) and using a modu-\nlar approach, where the algorithm is explicitly divided into\nrelevant subcomponents such as mapping, planning, and\ncontrol [8, 9, 10, 17, 19, 26, 36, 42]. These approaches have\nbeen demonstrated to work well for tasks such as target im-\nage navigation and environment exploration. However, they\nhave not been well studied in the context of VLN.\nIn this work, we employ a modular approach and lever-\nage topological maps for VLN. Topological maps, in-\nspired in part by cognitive science, typically represent en-\nvironments as graphs where nodes correspond to places\nand edges denote environment connectivity or reachability.\nCompared with metric maps, topological maps eliminate\nthe need for meticulous map construction. They promote\nefÔ¨Åcient planning, interpretable navigation plans, and nav-\nigation robustness using cheaper sensors [36, 47]. In par-\nticular, the symbolic nature of topological maps lends them\nsuitable for navigation with language [35, 47], as the space\ndiscretization provided by the maps can facilitate learning a\nrelationship between instructions and spatial locations.\nImportantly, using topological maps synergizes well\nwith sequence prediction models. Predicting a naviga-\ntion plan in a topological map bears many similarities\nwith predicting sequences for language tasks such as lan-\nguage modeling and neural machine translation (NMT). By\ndrawing the parallel between navigation planning and lan-\nguage sequence prediction, we can leverage powerful atten-\ntion mechanisms [49] that have enabled signiÔ¨Åcant break-\nthroughs in language tasks for the navigation problem. Re-\ncently, these attention-based models have even been demon-\nstrated to achieve comparable performance to convolutional\nneural networks on image recognition tasks [16].\nWe propose using a cross-modal attention-based trans-\nformer to compute navigation plans in topological maps\nbased on language instructions. Whereas a language model\npredicts a word at a time, our transformer predicts one topo-\n1\narXiv:2012.05292v1  [cs.RO]  9 Dec 2020\n1\n3\n2\n1\n3\n2\nNatural Language Instruction\nTraversal Localization Traversal\n2. Control: Plan Execution1. Planning\nTopological Environment Map\nCross-modal \nTransformer \nPlanner\nNavigation Plan\n‚ÄúExit the bedroom and turn left. Go around the corner and down the \nhallway. Make a right turn and stop in the doorway on the right. ‚Äù\nGT start / goal\nPlanned path\n1\n3\n2\n1 23\n45\n6\nexit the bedroom... ...turn left...\nFigure 1: The agent uses the natural language instruction to generate a navigation plan in the topological map (left). A\ncontroller then executes the predicted plan by sequentially traversing to each subgoal node in the plan (right). As the agent\napproaches the subgoal node, it consumes that active node, and the subsequent node in the plan becomes the new active node.\nlogical map node in the navigation plan at a time. Struc-\nturing our model in this manner allows the agent to attend\nto relevant portions of the navigation instruction and rele-\nvant spatial regions of the navigation environment during\nthe navigation planning process. For example, this enables\nour model to relate the word ‚Äúbedroom‚Äù in a navigation in-\nstruction to the physical room in the environment.\nAltogether, we propose a full navigation system for\nVLN. Unlike much of the prior work in VLN, we use a\nmore challenging setup, allowing the agent to freely tra-\nverse the environment using low-level discrete actions. To\nthis end, we deÔ¨Åne a topological map representation that\ncan be constructed by the agent after it has freely explored\nthe environment. The maps are used as part of a modular\nnavigation framework which decomposes the problem into\nplanning and control (Fig. 1). For each navigation episode,\nour agent Ô¨Årst uses the cross-modal transformer to compute\na global navigation plan from the navigation instruction and\ntopological map. This navigation plan is executed by a ro-\nbust local controller that outputs low-level discrete actions.\nWe evaluate our approach using the VLN-CE dataset\n[29]. Our experiments show that cross-modal attention-\nbased planning is effective, and that our modular approach\nenables learning a robust controller capable of correcting\nfor navigation mistakes like moving in the wrong direction.\n2. Related Work\nLanguage modeling and pretraining. Recent language\nmodel work has beneÔ¨Åted from attention-based transformer\narchitectures [49] and generalized pre-training of language\nmodels for Ô¨Åne-tuning on speciÔ¨Åc tasks [14]. With enough\nscale, these pretrained models are adaptive to new tasks, in\nsome cases not even requiring any Ô¨Åne-tuning or gradient\nupdates [7]. These advances tie into the increasing interest\nin more effective methods for image recognition [15] and\nlanguage grounding with visual cues [31, 44, 45]. In addi-\ntion to capturing semantics that can only be visually demon-\nstrated, these models allow for a more diverse range of ap-\nplications than language-only approaches including VLN.\nVision-and-language navigation (VLN). Much of the\nprogress in VLN has been achieved using end-to-end mod-\nels [13, 18, 22, 27, 32, 33, 46, 51, 57] trained using the\nMatterport3D Simulator [4] in which the agent assumes a\npanoramic action space [18] that allows it teleport to and\nfrom a Ô¨Åxed set of pre-deÔ¨Åned locations in the environ-\nment. This setup promotes fast iteration and evaluation of\ndifferent VLN approaches. However, it ignores the prob-\nlem of associating action directions in the panorama with\nthe pre-deÔ¨Åned positions in the environment as well as the\nmotion feasibility problem of moving from one location to\nanother. As pointed out with the end-to-end approach by\nKrantz et al. [29], the VLN problem becomes signiÔ¨Åcantly\nharder when the agent is allowed to freely traverse the en-\nvironment. In contrast, in our work the agent builds a topo-\nlogical map of the environment from ground up using ex-\nploration trajectories. The agent predicts a navigation path\nin the map and uses the local controller to execute the pre-\ndicted path with low-level discrete actions, bringing the task\nof VLN one step closer to reality.\nAlthough they operate in the simpliÔ¨Åed action space,\nHao et al. [22] and Majumdar et al. [34] propose attention-\nbased transformer approaches that perform self-supervised\npre-training on a cross-modal network using visual lan-\nguage pairings, either from the web or from demonstrations\nof a typical VLN trajectory. By Ô¨Åne-tuning these pre-trained\nmodels on speciÔ¨Åc VLN tasks, they effectively achieve bet-\nter generalized performance across different variants of nav-\nigation tasks. Their approaches operate on top of an exist-\ning navigation graph and perform cross-modal attention be-\ntween image regions and language instead of spatial regions\nand language. Additionally, Majumdar et al. propose a tra-\njectory scoring mechanism and relies on other methods to\ngenerate candidate routes. On the other hand, our approach\npredicts routes and executes them with low-level actions.\n2\nExplore Sparsify Merge\nFigure 2: Topological map construction. The agent ex-\nplores the environment multiple times, sparsiÔ¨Åes the gener-\nated graph from each trajectory, and merges them together.\nMemory for visual navigation. Early learning-based ap-\nproaches to visual navigation were reactive [16, 58] or\nbased on unstructured memory [37, 38]. Later works inves-\ntigate explicit representations of environments, such as met-\nric map-based representations [8, 19, 20, 21, 40, 56]. For\nexample, [2] use metric maps for VLN, and [17] use trans-\nformers for explorative tasks by storing observations and\nposes as memory. Topological maps have also been demon-\nstrated across different navigation problems [9, 10, 36, 42].\nFor language navigation, [35] and [55] use topological maps\nbut do not use RGB cameras as in our work.\n3. Method\nProblem setup. Our setup allows the agent to explore the\nenvironments using exploration trajectories (Fig. 2) prior\nto executing the VLN tasks. This mimics settings closer to\nthe real world in which the agent has to build an understand-\ning of the indoor environment on its own rather than being\nhanded a pre-deÔ¨Åned map. After the exploration phase, the\nagent is expected to perform VLN and is provided with (1)\nthe instruction text and (2) the current RGBD panorama ob-\nservation. Our agent has orientation information (or head-\ning) but not positional information during navigation time.\nOverview of our approach. From the exploration phase,\nthe agent builds a topological map of the environment (Sec.\n3.1). The agent then uses this topological understanding of\nthe environment along with the instruction text and current\nobservation to execute the VLN task.\nFor executing the VLN task, we take inspiration from the\nrobotics community and use a modular approach. Specif-\nically, we separate our approach into planning and con-\ntrol. Using a modular approach has been proven to work\nwell [39, 48] and has numerous advantages including inter-\npretability, robustness, and Ô¨Çexibility [26].\nIn the planning stage, prior to taking any action steps\nin the environment, the agent Ô¨Årst uses the navigation in-\nstruction to generate aninterpretable global navigation plan\nthrough the environment (Sec. 3.2). The robot then tries to\nfollow the navigation plan using a repeating cycle of local-\nization and control until it reaches the destination (Sec. 3.3).\nWe use a hierarchical controller that Ô¨Årst predicts waypoint\nsubgoals. Then the waypoints are translated into low-level\nactions like FORWARD (0.25m) or ROTATE (15‚ó¶).\n3.1. Topological Map Representation\nBefore planning can take place, the agent must Ô¨Årst con-\nstruct a topological representation of the environment. For\nVLN, it would be desirable that the map 1) covers a large\nproportion of traversable area; 2) contains rich information\nin nodes and edges to enable localization as well as planning\nfrom language; and 3) consists of connected nodes that are\nreachable with high probability when executing the plan.\nSimilar to prior work [9, 36, 42], we represent the en-\nvironment as a graph in which each node is an observation\nand each edge represents the connectivity (or reachability)\nbetween two nodes. As the agent explores the environment,\nit places a node over its current location and connects this\nnode to the previous one.\nIn prior work [36, 42] the agent may end up placing mul-\ntiple nodes in the same location. For example, two observa-\ntions from the same position can look very different if they\nare oriented 180‚ó¶apart. This may result in dense topolog-\nical maps and exacerbate difÔ¨Åculties in localization, plan-\nning, and control as each node only contains partial infor-\nmation about each location. To reduce redundancy and miti-\ngate these issues, we represent the nodes as360‚ó¶panoramas\noriented in a consistent Ô¨Åxed global orientation.\nAn overview of the topological map construction is pre-\nsented in Fig. 2. We run multiple pre-deÔ¨Åned exploration\ntrajectories per environment. The graph from each explo-\nration trajectory is sparsiÔ¨Åed by a reachability estimator as\nproposed by Meng et al. [36] and then merged into a sin-\ngle graph that can be used for localization and planning.\nFurther details on the exploration trajectories and map con-\nstruction can be found in the appendix.\nWe append coarse geometric information to each di-\nrected edge in the map. To this end, we introduce a quan-\ntized polar coordinate system with 8 directions and 3 dis-\ntances (0-2m, 2-5m, >5m). Such information is useful for\nresolving ambiguities (e.g. symmetric environments) and\nmay facilitate language/spatial reasoning. Since mapping is\nnot our main focus, we use ground truth odometry to com-\npute the edge categories during map construction. However,\nthis assumption is relaxed during navigation test time and\nwe instead use a neural network for localization (Sec. 3.3).\n3.2. Cross-Modal Planning\nAs stated earlier, we modularize our approach into plan-\nning and control. For planning, the agent uses the con-\nstructed topological map and the navigation instruction to\nformulate a global navigation plan as a path in the map. This\npath, represented as a sequence of nodes, is then passed to\nthe controller at the end of the planning stage.\n3\nGNN\n...\nTopological Map\n...\n+\nMap Features\nTrajectory Position Encodings\n...\n...\n+\nWord Embeddings\nLanguage Position Encodings\nTokenizer + Embeddings\nMap Encoder Language Encoder\nCross-Modal Encoder\nClassification Head\nNatural Language Instruction\nFigure 3: Planner. The planner processes the topological\nmap and language instruction separately. The information is\nfused with a cross-modal transformer (map, language, and\ncross-modal encoder) to classify the next step in the plan.\nAs depicted in Fig. 3, our planner has two main compo-\nnents: a graph neural network (GNN) and a cross-modal\ntransformer (comprised of a map, language, and cross-\nmodal encoder). The GNN computes representations of the\nenvironment which capture visual appearance and environ-\nment connectivity. These representations are passed along\nwith the navigation instruction to the cross-modal trans-\nformer which selects the next node in the plan in a sequen-\ntial, auto-regressive fashion. This process repeats until the\nplanner classiÔ¨Åes that the end of the plan has been reached.\nRelationship to language modeling and translation.\nNotably, our problem setup and approach bear similari-\nties to language modeling and translation. In neural ma-\nchine translation (NMT) speciÔ¨Åcally, the model conditions\non the source sentence xin addition to its prior predictions\ny1,...,y t‚àí1 in the predicted sentence.\np(y|x) =\nT‚àè\nt=1\np(yt|y1,...,y t‚àí1,x) (1)\nAnalogously, in our setup the agent predicts each step of\nthe navigation plan yt conditioned on its previous predic-\ntions y1,...,y t‚àí1 as well as the topological map Gand the\nnavigation instruction L(i.e., x = (G,L)). This important\ninsight allows us to approach the VLN planning problem in\na manner similar to NMT and language modeling.\nThe following subsections illustrate the architectural\nsetup of our approach (Fig. 3). The planner has two\nbranches for encoding the map and the language instruc-\ntion. In the map branch, we use a graph neural network\n(GNN) to process the topological map and generate map\nfeatures (Sec. 3.2.1). In the language branch, the instruc-\ntion is mapped to word embeddings. Finally, the map fea-\ntures and word embeddings are passed into the cross-modal\ntransformer to produce a navigation plan (Sec. 3.2.2).\n3.2.1 Learning Environment Representations using\nGraph Neural Networks (GNNs)\nTo facilitate learning a mapping between language and\nphysical space, the map features passed to the transformer\nshould encapsulate visual appearance and environment con-\nnectivity. In our work, these map features are learned via\na graph neural network (GNN), which carries strong rela-\ntional inductive biases appropriate for topological maps.\nThe GNN is composed of sequential graph network (GN)\nblocks [6]. Each GN block takes as input a graph ÀúG =\n(Àúu,ÀúV, ÀúE) and produces an updated graph ÀúG‚Ä≤= (Àúu‚Ä≤,ÀúV‚Ä≤, ÀúE‚Ä≤)\nwhere the output features are computed according to the\ngraph structure. To do this, the GN block is comprised of\nupdate functions œÜv(¬∑), œÜe(¬∑), œÜu(¬∑) and aggregation func-\ntions œÅe‚Üív(¬∑), œÅe‚Üíu(¬∑), œÅv‚Üíu(¬∑). We implement the update\nfunctions as multi-layer perceptrons (MLP) and use sum-\nmation for the aggregation functions. For more details on\nGNNs, we refer the reader to Battaglia et al. [6].\nInput graph representation. The input to the GNN is\nthe topological map encoded as a directed graph G =\n(u,V,E ). To capture visual appearance and semantics, we\nencode each vertex as a ResNet152 feature [23] extracted\nfrom the corresponding Ô¨Åxed orientation RGB panorama.\nTo encode the relative geometry between nodes, we map\neach edge category (Sec. 3.1) to a learnable embedding.\nEach edge thus captures information about the relative ori-\nentation and distance between a pair of connecting nodes.\nLastly, the global feature u is also a learned embedding.\nOutput graph representation. At the Ô¨Ånal layer of the\nGNN, we extract the output node features ÀÜV = {ÀÜ vi}i=1:n\nas the environment map features. Due to the message pass-\ning nature of the GNN, this set of map features not only cap-\ntures visual appearance but also environment connectivity.\nThe features are passed along with the tokenized instruction\nto the cross-modal transformer described in the next section.\n3.2.2 Cross-Modal Transformer\nAt the core of our planner is a cross-modal transformer\nwhich takes as input the topological map encoded as map\nfeatures (modality 1; Sec. 3.2.1) and the navigation instruc-\ntion (modality 2). The start node is also provided. Each\nforward pass computes one step of the navigation plan.\nOur transformer is based on LXMERT [45] which has been\ndemonstrated to be effective for vision and language tasks.\n4\n1\n1\n3\n2 4\n75\n6\n8\n3 4 5 7 8 2 6\nTopological Map\n+\nPath\nNodes\nv1 v3 v4 v5 v7 v8 v2 v6Map Features\nE1 E2 E3 E4 E5 E6 ENT ENTPositional Encodings\n+ + + + + + + +\nFigure 4: Trajectory position encoding. Nodes in the pre-\ndicted trajectory (green) utilize a positional encoding for the\ncorresponding position, whereas non-trajectory (NT) nodes\n(blue) utilize the same positional embedding ENT .\nAttention. A fundamental component of this model is the\nself-attention layer [49]. SpeciÔ¨Åcally, an input sequence\nx‚ààRn√ód of length nand dimension dis linearly projected\ninto a set of keys K ‚ààRn√ódk , queries Q ‚ààRn√ódk , and\nvalues V ‚ààRn√ódv . The queries and keys are then used to\ncompute a weighted sum over the values.\nAttn(Q,K,V ) =softmax\n(QK‚ä§\n‚àödk\n)\nV (2)\nSingle modal encoder. Each modality (map and lan-\nguage) is encoded via its corresponding single-modality en-\ncoder. This encoder follows a standard transformer model\ncomprised of self-attention layers and feedforward layers.\nAs depicted in Fig. 3, the input to the map encoder is the\nsum of the map features (output from the GNN) with tra-\njectory positional encodings. The language branch pro-\ncesses word embeddings from the natural language instruc-\ntion summed with learned positional encodings.\nCross modal encoder. The cross-modal encoder [45] ex-\nchanges information between the two modalities. Each\nlayer of the cross-modal encoder is composed of cross-\nmodal attention, self-attention, and feed forward layers. In\nthe cross-modal attention layer, the query comes from one\nmodality, and the keys and values come from the other\nmodality. The calculation for two input sequences xA,xB\ncorresponding with modalities Aand Bis in Eq. 3.\nCrossAttn(xA,xB) =Attn(QA,KB,VB) (3)\nThe node features produced by the cross-modal encoder are\npassed to a classiÔ¨Åcation head, implemented as an MLP, to\ncompute the next step in the plan.\nTrajectory position encoding. Since the transformer pre-\ndicts a single node token at a time, it is essential that it keeps\ntrack of what has been predicted so far. To do this, we add\nConvolutional Encoder\nClassiÔ¨Åer Regressor\nDirection Prediction (ùúô) Distance Prediction (ùúå)\n.01 .03 .16 .37 .24 .04 .01 .00... 3.743\n-180 -120 +180-150 +150...\nSubgoal Node (Ok)Previous Node (Ok-1) Current Observation (Ocurr)\nFigure 5: High level controller . Chigh uses the agent‚Äôs\nobservation and the previous and current subgoal node to\npredict a waypoint relative to the agent in polar coordinates.\npositional embeddings to the node sequence as illustrated\nin Fig. 4. While sinusoidal positional encodings are com-\nmonly used [49], we found that learned positional embed-\ndings yielded higher planning accuracy.\nStop action. We use a [STOP] token to indicate the stop\naction. It is appended to the map features and classiÔ¨Åcation\nis performed over the nodes and the [STOP] token.\nTraining details. The planner (GNN and transformer) is\ntrained end-to-end with a cross entropy training objective.\nWe use the AdamW optimizer [28, 30] with a linear warmup\nschedule. Further architecture and implementation details\nare described in the appendix.\n3.3. Controller: Executing the Navigation Plan\nThe controller is responsible for converting the plan (a\ntopological path) into a series of low-level actions that take\nthe agent to the goal. The inputs of the controller are the\nRGBD panorama of each planned node in the sequence, the\ncurrent RGBD panoramic observation at each time step, and\nheading information. The output action space of the con-\ntroller is deÔ¨Åned as a set of parameterized low-level actions:\nFORWARD (0.25m), ROTATE LEFT /RIGHT (15‚ó¶).\nThe controller produces actions which move the agent\nfrom one node to another. To do this, it must also perform\nlocalization in order to determine when the current subgoal\nnode has been reached, at which point the agent can move\non to the next node in the plan. We abstract our controller\nC to two layers: a low level controller Clow that moves\nthe agent towards a geometric waypoint, and a high level\ncontroller Chigh that predicts such a waypoint. This high-\nlevel controller is used for localization.\nHigh level controller. The high-level controller uses three\npanoramas, including the current and subgoal observations,\n5\nto predict a waypoint to move towards in order to reach the\nsubgoal. We use a quantized polar coordinate system to de-\nscribe the waypoint relative to the agent. The polar coordi-\nnate is partitioned to 24 sectors, encompassing 15 degrees\neach. A sector index and a numeric distance deÔ¨Ånes the\nwaypoint position. The predicted distance is used for deter-\nmining whether the subgoal has been reached (localization).\nFigure 5 illustrates the design of the high level con-\ntroller Chigh. The partially observable agent state at any\ntime step is represented by a tuple of RGBD panoramas\n(ok‚àí1,ocurr,ok) where k is the index of the current sub-\ngoal. The observations correspond with the previous sub-\ngoal node, the current position, and the current subgoal\nnode, respectively. All observations are oriented in the same\nheading direction using the heading information. To op-\ntimize for panoramic observations, we use circular convo-\nlution layers [43] to encode the observations into features.\nThe features are then used to predict a classiÔ¨Åcation vector\nœÜ (direction of the subgoal node) and a scalar œÅ (distance\nbetween the agent and the subgoal node).\nBoth outputs are optimized simultaneously by imitation\nlearning using dataset aggregation (DAgger) [41], super-\nvised by A* expert waypoints of a Ô¨Åxed lookahead distance.\nThe controller is trained separately from the planner.\nHandling Oscillation. Because the agent‚Äôs state is de-\nÔ¨Åned by three panoramic observations only, there exists the\npossibility of perceptual aliasing in which the agent‚Äôs obser-\nvation is similar in multiple directions (e.g. a symmetric-\nlooking hallway with an anchor node at each end). In this\ncase, it is possible for the agent to move towards either di-\nrection in alternating time steps, resulting in no progress.\nTo alleviate this issue, we introduce ‚Äústubbornness‚Äù to\nthe predicted heading probabilities such that the predicted\ndirection is biased towards the prediction of the previous\ntime step. We deÔ¨Åne a bias function B(œÜ‚Ä≤\nt‚àí1,œÜt; œÉ2) to\nreweight the polar prediction at time tby multiplying each\nelement by a Gaussian distribution centered at the predicted\ndirection at the previous time step:\nœÜ‚Ä≤\nti = œÜti ‚àóN(i‚àíarg max\nj\n(œÜ‚Ä≤\nt‚àí1j ),œÉ2) (4)\nwhere i,j are indices into œÜ, and œÉ2 is the variance of the\nGaussian such that the lower the variance, the more biased\nthe waypoint is towards the previous predicted direction.\nMapping to low-level actions. After Chigh produces a\nwaypoint, a robot-speciÔ¨Åc low-level controller translates it\nto a robot action. For our problem setup, we deÔ¨Åne Clow as\na simple policy that maps œÜ‚Ä≤\nt to quantized low-level actions:\nFORWARD if agent is facing the direction of arg maxi œÜ‚Ä≤\nti ;\notherwise ROTATE towards that direction. This can be re-\nplaced by more sophisticated robot-speciÔ¨Åc controllers.\nLocalization and trajectory following. When the agent\ntraverses within a threshold distance d to the current sub-\ngoal node such that œÅ ‚â§ d, it consumes that node. The\nsubsequent node in the plan becomes the new subgoal. This\nprocess is repeated until no more nodes remain in the plan.\n4. Experiments\nWe evaluate our approach using the Interactive Gibson\nsimulator (iGibson) [53, 54]. The agent is equipped with\na 360‚ó¶panoramic RGBD camera and ground truth heading\ninformation. Prior to navigation, the agent explores each\nenvironment via 10 pre-deÔ¨Åned trajectories and constructs\ntopological maps denoted as agent-generated (AG) graphs.\nFor evaluation, we use the VLN-CE dataset [29] and evalu-\nate on environments which were seen (val-seen) and unseen\n(val-unseen) during training. We use the following metrics:\nsuccess rate (SR), oracle success rate (OS), and navigation\nerror from goal in meters (NE). Oracle success rate uses an\noracle to determine when to stop. A navigation episode is\nsuccessful if the agent stops within 3mof the goal position.\nDetails on these metrics can be found in [1, 4].\n4.1. Planner Evaluation\nIn this section, we evaluate the performance of the plan-\nner in isolation from the controller. In addition to the agent-\ngenerated maps, we also compare planning performance on\nthe pre-deÔ¨Åned navigation graphs from Room2Room (R2R)\n[4]. We compare our approach with a graph neural network\n(GNN) baseline as well as variants of our cross-modal trans-\nformer planner (CMTP) with different stop mechanisms:\n‚Äì Graph neural network (GNN): A GNN that directly pre-\ndicts the Ô¨Ånal navigation goal in the topological map when\ngiven the map, start node, and instruction. The instruction\nis encoded using a transformer and passed as the global\nfeature to the GNN. The GNN then performs classiÔ¨Åcation\nover the nodes. To extract a navigation plan, we compute\na shortest path from the start node to the predicted goal.\nThis baseline is similar in spirit to [13].\n‚Äì Cross-Modal Transformer Planner (CMTP): Our plan-\nner which predicts the stop action by performing classiÔ¨Å-\ncation over the [STOP] token and node tokens.\n‚Äì CMTP Binary Cross Entropy (CMTP-BCE) : Uses a\nseparate (binary) cross-entropy loss for the [STOP] to-\nken, trained jointly with the node classiÔ¨Åcation loss.\n‚Äì CMTP Repeated (CMTP-R) : Instead of using a\n[STOP] token, this CMTP is trained to indicate the stop\naction by predicting the same node consecutively.\n‚Äì CMTP No Stop (CMTP-NS) : Never produces a stop\ncommand and is instead used as an ablation. It provides\ninsight into planning performance when correctly timing\n6\nVal-Seen Val-Unseen\nSR ‚Üë OS ‚Üë NE ‚Üì SR ‚Üë OS ‚Üë NE ‚Üì\nGNN 30.6 45.6 8.04 23.5 40.7 8.15\nCMTP 37.9 54.0 6.54 26.0 39.1 7.62\nAG CMTP-BCE 33.6 48.1 7.11 27.3 42.9 8.13\nCMTP-R 37.4 52.6 6.28 26.5 41.3 7.66\nCMTP-NS - 66.2 - - 64.0 -\nGNN 27.3 51.2 9.62 20.7 49.5 11.21\nCMTP 38.6 49.1 6.76 28.7 37.6 7.74\nR2R CMTP-BCE 37.1 51.6 6.97 30.2 42.2 8.36\nCMTP-R 38.4 49.8 6.91 26.8 36.7 8.04\nCMTP-NS - 61.4 - - 60.5 -\nTable 1: Planner performance. Our CMTP models outper-\nform the baseline GNN and perform better on R2R graphs.\nCMTP-NS is provided as an ablation. See text for details.\nthe stop action is taken out of the picture. The episode\nterminates after 20 planning steps.\nQuantitative results are presented in Table 1. The trans-\nformer planner achieves higher success rates and lower nav-\nigation errors compared to the GNN. Across all models, the\nperformance on seen environments is signiÔ¨Åcantly higher\nthan unseen environments. This overÔ¨Åtting is most promi-\nnent in the CMTP model. On the other hand, the CMTP-\nBCE model has less overÔ¨Åtting at the cost of lower perfor-\nmance on seen environments.\nWe observe that the oracle success rate of CMTP-NS\nis signiÔ¨Åcantly higher than all models. In other words,\nthe transformer planner often navigates to the correct lo-\ncations but struggles to stop at the right time. These results\nhighlight the challenge of accurately stopping at the right\nmoment while illustrating the capability of our model for\nchoosing the correct places to navigate towards.\nDifferent topological maps also affect planning perfor-\nmance. For example, the transformer planners achieve\nhigher performance on the pre-deÔ¨Åned R2R graphs. This\nmay be due to better positioned nodes and edges compared\nto the AG maps, which sometimes has awkwardly placed\nnodes near walls or corners. Importantly, these results in-\ndicate the importance of (1) developing models which are\nable to navigate consistently well under different topolog-\nical maps and (2) exploring new memory representations\nwhich support effective and robust navigation.\nCross modal attention. The cross modal attention design\nof our planner facilitates informational Ô¨Çow between the\ntopological map and language domains, which is intended\nto ground correlated visual node features to their instruc-\ntional counterparts. An example is shown in Fig 6, which\nvisualizes the Ô¨Årst layer cross modal attention from pre-\n[CLS]\nhead\nforward\npast\nthe\nkitchen\nand\ntake\nthe\nfirst\nleft\nat\nthe\ndining\ntable\n.\nwait\nin\n1 2 3 4 5\n1 2\n3\n4\n5\n1 2 3 4 5\nthe\ndining\ntable\n.\nwait\nin\nthe\nhallway\nbetween\nthe\nliving\nroom\nand\ndining\nroom\n.\n[SEP]\nInstruction: Head forward past the kitchen and take the first left at the dining \ntable. Wait in the hallway between the living room and dining room.\nFigure 6: Cross modal attention (right) from nodes onto\ninstructions (top). Each column represents a node in the\npredicted plan (left), going from blue to red.\n(a) (b) (c) (d)\nFigure 7: Plan execution. The controller is able to follow\nthe navigation plans (a, c) as shown by the trajectories (b,\nd). It can also correct for mistakes via backtracking (d).\nUnbiased œÉ2 = 5 œÉ2 = 3 œÉ2 = 2\nPointgoal 99.8 / 99.5 99.9 / 99.4 99.9 / 99.7 99.6 / 99.2\nAG 83.3 / 82.5 89.4 / 87.2 93.1 / 89.6 77.7 / 75.9\nR2R 87.2 / 82.0 93.3 / 89.1 92.8 / 92.6 84.3 / 81.8\nTable 2: Controller success rate in [seen / unseen] envi-\nronments for random point goals (‚â§5maway) and for ran-\ndomly sampled plans from R2R or AG graphs (‚â§12 nodes\naway). Adding the bias greatly improves performance.\ndicted nodes to the input instruction. We see that words\nwhich encode the Ô¨Årst half of the instruction correspond\nwith earlier nodes in the trajectory, whereas words such as\n‚Äúhallway‚Äù and ‚Äúliving room‚Äù in the latter half of the sen-\ntence correspond to the last three nodes of the trajectory.\nWe note that the model will sometimes focus attention\non articles or punctuation. This behavior is in line with pre-\nvious observations [45] that the cross modal layer tends to\nfocus on articles over single noun references (e.g. attention\non ‚Äúthe‚Äù or ‚Äúliving‚Äù instead of ‚Äúroom‚Äù), using them as an-\nchor words while likely relying on the language encoder to\nconnect the articles to the speciÔ¨Åc objects they reference.\n4.2. Controller Evaluation\nWe evaluate the controller separate from our planner by\nexecuting episodes of random plans in the simulated envi-\nronment. A navigation success threshold distance of 1mis\n7\nVal-seen Val-unseen\nPS‚Üë CS‚Üë SR‚Üë OS‚Üë NE‚Üì SPL‚Üë PS‚Üë CS‚Üë SR‚Üë OS‚Üë NE‚Üì SPL‚Üë\nVLN-CE [29] - - 22.2 29.8 7.6 16.5 - - 19.7 27.2 7.8 14.1\nAG\nGNN 30.6 91.7 29.6 46.0 8.2 26.3 23.5 84.8 22.1 40.0 8.4 17.2\nCMTP 37.9 92.2 35.9 56.2 6.6 30.5 26.0 86.6 23.1 39.2 7.9 19.1\nCMTP-BCE 33.6 89.4 32.6 49.6 7.2 27.5 27.3 82.6 25.2 42.2 8.4 20.2\nCMTP-R 37.4 93.1 36.3 53.9 6.5 31.3 26.5 87.8 25.3 42.6 7.9 20.3\nR2R\nGNN 27.3 89.1 26.6 51.9 9.4 21.2 20.7 88.6 20.3 48.3 10.9 16.4\nCMTP 38.6 91.7 36.1 45.4 7.1 31.2 28.7 90.0 26.4 38.0 7.9 22.7\nCMTP-BCE 37.1 89.6 34.4 49.3 7.4 29.9 30.2 89.6 28.9 40.7 8.4 24.1\nCMTP-R 38.4 89.6 35.1 48.2 7.5 31.2 26.8 90.0 25.2 36.0 8.3 20.9\nTable 3: Integrated system performance. The modular approaches (GNN, CMTP) outperform the end-to-end VLN-CE\nmodel in success rate (SR), with our CMTP models achieving the best performance on seen and unseen environments.\nused when evaluating the controller alone.\nFig. 7 shows qualitative results of the controller exe-\ncuting navigation plans. The controller is able to navi-\ngate while avoiding collision, despite graph edges cutting\nthrough obstacles in the planned path. With the inclusion\nof the previous node into the controller input, the agent is\nable to learn intelligent backtracking behavior in which the\nagent backtracks to the previously known position when it\nmakes a mistake and deviates from the path.\nThe quantitative results reported in Table 2 support our\nobservations that the controller works well. In navigation to\npoint goals (row 1), it achieves high success rates even in\nunseen environments. Executing multi-node plans (rows 2-\n3) also produces high success rates, with better performance\non R2R graphs than AG graphs due to their better positioned\nnodes providing more informative observations.\n4.3. VLN Evaluation\nFinally, we evaluate our full navigation system. We com-\npare with the cross-modal approach in VLN-CE [29] since it\nis the only prior work, to our knowledge, that considers low-\nlevel actions like our work. VLN-CE is trained end-to-end\nand utilizes a gated recurrent unit (GRU) [12] to encode the\nagent‚Äôs history. In addition to prior metrics, we report suc-\ncess weighted by path length (SPL) [1], and planner success\n(PS) and controller success (CS) for modular approaches.\nTable 3 shows that our CMTP models achieve the high-\nest VLN success rates and SPL compared to the GNN and\nVLN-CE baselines. Moreover, the methods which use topo-\nlogical maps outperform the end-to-end VLN-CE baseline.\nWhile these results may be expected given that the former\nmethods pre-explore the environments, the gap in perfor-\nmance is also due to reducing the search space through the\ntopological maps. By combining our cross-modal planner\nand controller into an integrated system, we get the best\nof both worlds: the planner achieves high planning suc-\ncess rates while the controller is able to bring those plans\nFigure 8: Our full navigation system. We show the\nnavigation instruction (top-left), the panorama observations\n(right), and the predicted navigation plan (bottom-left) and\nexecution (bottom-middle). The arrows in each panorama\nindicate the predicted action at the given panorama.\nto fruition with low-level discrete actions.\nFig. 8 shows exciting qualitative results of our full nav-\nigation system. As illustrated by the agent‚Äôs predicted plan\nand executed trajectory, the modular approach makes it easy\nto interpret the agent‚Äôs intents and evaluate its ability to re-\nalize those intents. For instance, in Fig. 8 (top), the planner\npredicts a path between the pool table and couch as speci-\nÔ¨Åed by the instruction. The controller successfully executes\nthe plan by converting it into a smooth trajectory which tra-\nverses between the two objects rather than around them.\n8\n5. Conclusion\nWe introduced a novel approach for VLN with topo-\nlogical maps. By using a modular system in conjunction\nwith topological maps, we saw a boost in navigation perfor-\nmance compared to an end-to-end baseline. Potential exten-\nsions include demonstrating VLN on a real robot [3] and us-\ning topological maps constructed at test time [9]. We hope\nour work inspires further research in building robot systems\nwhich communicate effectively and operate robustly.\nAcknowledgments. This work was funded through the\nsupport of ONR MURI Award #W911NF-15-1-0479. We\nalso thank Stanford Vision and Learning Lab members for\ntheir constructive feedback, including Eric Chengshu Li and\nClaudia P¬¥erez D‚ÄôArpino.\nReferences\n[1] Peter Anderson, Angel Chang, Devendra Singh Chaplot,\nAlexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana\nKosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva,\net al. On evaluation of embodied navigation agents. arXiv\npreprint arXiv:1807.06757, 2018. 6, 8\n[2] Peter Anderson, Ayush Shrivastava, Devi Parikh, Dhruv Ba-\ntra, and Stefan Lee. Chasing ghosts: Instruction following as\nbayesian state tracking. In Advances in Neural Information\nProcessing Systems, pages 371‚Äì381, 2019. 3\n[3] Peter Anderson, Ayush Shrivastava, Joanne Truong, Arjun\nMajumdar, Devi Parikh, Dhruv Batra, and Stefan Lee. Sim-\nto-real transfer for vision-and-language navigation. arXiv\npreprint arXiv:2011.03807, 2020. 9\n[4] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark\nJohnson, Niko S ¬®underhauf, Ian Reid, Stephen Gould, and\nAnton van den Hengel. Vision-and-language navigation: In-\nterpreting visually-grounded navigation instructions in real\nenvironments. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pages 3674‚Äì\n3683, 2018. 1, 2, 6, 15, 18\n[5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016. 16\n[6] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Al-\nvaro Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Ma-\nlinowski, Andrea Tacchetti, David Raposo, Adam Santoro,\nRyan Faulkner, et al. Relational inductive biases, deep learn-\ning, and graph networks. arXiv preprint arXiv:1806.01261,\n2018. 4, 15\n[7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, Sand-\nhini Agarwal, Ariel Herbert-V oss, Gretchen Krueger, Tom\nHenighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Rad-\nford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners, 2020. 2\n[8] Devendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta,\nAbhinav Gupta, and Ruslan Salakhutdinov. Learning to ex-\nplore using active neural slam. In International Conference\non Learning Representations, 2019. 1, 3, 11\n[9] Devendra Singh Chaplot, Ruslan Salakhutdinov, Abhinav\nGupta, and Saurabh Gupta. Neural topological slam for vi-\nsual navigation. In CVPR, 2020. 1, 3, 9\n[10] Kevin Chen, Juan Pablo de Vicente, Gabriel Sepulveda, Fei\nXia, Alvaro Soto, Marynel V ÀúA¬°zquez, and Silvio Savarese.\nA behavioral approach to visual navigation with graph local-\nization networks. In Proceedings of Robotics: Science and\nSystems, FreiburgimBreisgau, Germany, June 2019. 1, 3\n[11] Tao Chen, Saurabh Gupta, and Abhinav Gupta. Learn-\ning exploration policies for navigation. arXiv preprint\narXiv:1903.01959, 2019. 1, 11\n[12] Kyunghyun Cho, Bart Van Merri ¬®enboer, Caglar Gulcehre,\nDzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and\nYoshua Bengio. Learning phrase representations using rnn\nencoder-decoder for statistical machine translation. arXiv\npreprint arXiv:1406.1078, 2014. 8\n[13] Zhiwei Deng, Karthik Narasimhan, and Olga Russakovsky.\nEvolving graphical planner: Contextual global plan-\nning for vision-and-language navigation. arXiv preprint\narXiv:2007.05655, 2020. 2, 6\n[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018. 2\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 2\n[16] Alexey Dosovitskiy and Vladlen Koltun. Learning to act by\npredicting the future. In International Conference on Learn-\ning Representations (ICLR), 2017. 1, 3\n[17] Kuan Fang, Alexander Toshev, Li Fei-Fei, and Silvio\nSavarese. Scene memory transformer for embodied agents\nin long-horizon tasks. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition , pages\n538‚Äì547, 2019. 1, 3\n[18] Daniel Fried, Ronghang Hu, V olkan Cirik, Anna Rohrbach,\nJacob Andreas, Louis-Philippe Morency, Taylor Berg-\nKirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell.\nSpeaker-follower models for vision-and-language naviga-\ntion. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,\nN. Cesa-Bianchi, and R. Garnett, editors, Advances in Neu-\nral Information Processing Systems 31 , pages 3314‚Äì3325.\nCurran Associates, Inc., 2018. 1, 2\n[19] Saurabh Gupta, James Davidson, Sergey Levine, Rahul Suk-\nthankar, and Jitendra Malik. Cognitive mapping and plan-\nning for visual navigation. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n2616‚Äì2625, 2017. 1, 3\n9\n[20] Saurabh Gupta, David Fouhey, Sergey Levine, and Jitendra\nMalik. Unifying map and landmark based representations for\nvisual navigation. arXiv preprint arXiv:1712.08125, 2017. 3\n[21] Saurabh Gupta, Varun Tolani, James Davidson, Sergey\nLevine, Rahul Sukthankar, and Jitendra Malik. Cognitive\nmapping and planning for visual navigation. International\nJournal of Computer Vision, 2019. 3\n[22] Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, and\nJianfeng Gao. Towards learning a generic agent for vision-\nand-language navigation via pre-training. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13137‚Äì13146, 2020. 1, 2\n[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770‚Äì778, 2016. 4, 15, 16\n[24] Sepp Hochreiter and J ¬®urgen Schmidhuber. Long short-term\nmemory. Neural computation, 9(8):1735‚Äì1780, 1997. 1\n[25] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. In International Conference on Machine Learn-\ning, pages 448‚Äì456, 2015. 15\n[26] Peter Karkus, Xiao Ma, David Hsu, Leslie Pack Kaelbling,\nWee Sun Lee, and Tom¬¥as Lozano-P¬¥erez. Differentiable algo-\nrithm networks for composable robot learning. In Proceed-\nings of Robotics: Science and Systems, 2019. 1, 3\n[27] Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman, Zhe\nGan, Jingjing Liu, Jianfeng Gao, Yejin Choi, and Siddhartha\nSrinivasa. Tactical rewind: Self-correction via backtrack-\ning in vision-and-language navigation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 6741‚Äì6749, 2019. 1, 2\n[28] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980 ,\n2014. 5, 16\n[29] Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra,\nand Stefan Lee. Beyond the nav-graph: Vision-and-language\nnavigation in continuous environments. 2020. 1, 2, 6, 8, 18\n[30] Ilya Loshchilov and Frank Hutter. Fixing weight decay reg-\nularization in adam. 2018. 5, 16\n[31] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vil-\nbert: Pretraining task-agnostic visiolinguistic representations\nfor vision-and-language tasks. In Advances in Neural Infor-\nmation Processing Systems, pages 13‚Äì23, 2019. 2\n[32] Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib,\nZsolt Kira, Richard Socher, and Caiming Xiong. Self-\nmonitoring navigation agent via auxiliary progress estima-\ntion. In Proceedings of the International Conference on\nLearning Representations (ICLR), 2019. 1, 2\n[33] Chih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caiming\nXiong, and Zsolt Kira. The regretful agent: Heuristic-\naided navigation through progress estimation. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 6732‚Äì6740, 2019. 1, 2\n[34] Arjun Majumdar, Ayush Shrivastava, Stefan Lee, Peter An-\nderson, Devi Parikh, and Dhruv Batra. Improving vision-\nand-language navigation with image-text pairs from the web,\n2020. 2\n[35] Cynthia Matuszek, Dieter Fox, and Karl Koscher. Follow-\ning directions using statistical machine translation. In 2010\n5th ACM/IEEE International Conference on Human-Robot\nInteraction (HRI), pages 251‚Äì258. IEEE, 2010. 1, 3\n[36] Xiangyun Meng, Nathan Ratliff, Yu Xiang, and Dieter Fox.\nScaling local control to large-scale topological navigation.\narXiv preprint arXiv:1909.12329, 2019. 1, 3, 11, 16\n[37] Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer,\nAndrew J Ballard, Andrea Banino, Misha Denil, Ross\nGoroshin, Laurent Sifre, Koray Kavukcuoglu, et al. Learn-\ning to navigate in complex environments. arXiv preprint\narXiv:1611.03673, 2016. 3\n[38] V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza,\nAlex Graves, Timothy Lillicrap, Tim Harley, David Silver,\nand Koray Kavukcuoglu. Asynchronous methods for deep\nreinforcement learning. In International conference on ma-\nchine learning, pages 1928‚Äì1937, 2016. 3\n[39] Nils J Nilsson. Shakey the robot. Technical report, SRI IN-\nTERNATIONAL MENLO PARK CA, 1984. 3\n[40] Emilio Parisotto and Ruslan Salakhutdinov. Neural map:\nStructured memory for deep reinforcement learning. In In-\nternational Conference on Learning Representations , 2018.\n3\n[41] St ¬¥ephane Ross, Geoffrey Gordon, and Drew Bagnell. A re-\nduction of imitation learning and structured prediction to no-\nregret online learning. InProceedings of the fourteenth inter-\nnational conference on artiÔ¨Åcial intelligence and statistics ,\npages 627‚Äì635, 2011. 6, 18\n[42] Nikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun.\nSemi-parametric topological memory for navigation. In In-\nternational Conference on Learning Representations (ICLR),\n2018. 1, 3\n[43] S. Schubert, P. Neubert, J. P ¬®oschmann, and P. Protzel. Cir-\ncular convolutional neural networks for panoramic images\nand laser data. In 2019 IEEE Intelligent Vehicles Symposium\n(IV), pages 653‚Äì660, 2019. 6\n[44] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu\nWei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-\nlinguistic representations. In International Conference on\nLearning Representations, 2020. 2\n[45] Hao Tan and Mohit Bansal. LXMERT: Learning cross-\nmodality encoder representations from transformers. In\nProceedings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pages 5100‚Äì5111, Hong Kong, China, Nov. 2019.\nAssociation for Computational Linguistics. 2, 4, 5, 7, 16\n[46] Hao Tan, Licheng Yu, and Mohit Bansal. Learning to nav-\nigate unseen environments: Back translation with environ-\nmental dropout. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), pages 2610‚Äì2621, 2019. 1, 2\n[47] Sebastian Thrun. Learning metric-topological maps for\nindoor mobile robot navigation. ArtiÔ¨Åcial Intelligence ,\n99(1):21‚Äì71, 1998. 1\n[48] Sebastian Thrun, Mike Montemerlo, Hendrik Dahlkamp,\nDavid Stavens, Andrei Aron, James Diebel, Philip Fong,\n10\nJohn Gale, Morgan Halpenny, Gabriel Hoffmann, et al. Stan-\nley: The robot that won the darpa grand challenge. Journal\nof Ô¨Åeld Robotics, 23(9):661‚Äì692, 2006. 3\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998‚Äì6008, 2017. 1,\n2, 5\n[50] Ayzaan Wahid, Austin Stone, Kevin Chen, Brian Ichter,\nand Alexander Toshev. Learning object-conditioned ex-\nploration using distributed soft actor critic. arXiv preprint\narXiv:2007.14545, 2020. 1\n[51] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao,\nDinghan Shen, Yuan-Fang Wang, William Yang Wang, and\nLei Zhang. Reinforced cross-modal matching and self-\nsupervised imitation learning for vision-language navigation.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 6629‚Äì6638, 2019. 1, 2\n[52] Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee,\nIrfan Essa, Devi Parikh, Manolis Savva, and Dhruv Batra.\nDd-ppo: Learning near-perfect pointgoal navigators from 2.5\nbillion frames. arXiv preprint arXiv:1911.00357, 2019. 1\n[53] Fei Xia, Chengshu Li, Kevin Chen, William B Shen, Roberto\nMartƒ±n-Martƒ±n, Noriaki Hirose, Amir R Zamir, and Li Fei-\nFei1 Silvio Savarese. Gibson env v2: Embodied simulation\nenvironments for interactive navigation. 2019. 6, 18\n[54] Fei Xia, William B Shen, Chengshu Li, Priya Kasimbeg, Mi-\ncael Edmond Tchapmi, Alexander Toshev, Roberto Mart ¬¥ƒ±n-\nMart¬¥ƒ±n, and Silvio Savarese. Interactive gibson benchmark:\nA benchmark for interactive navigation in cluttered environ-\nments. IEEE Robotics and Automation Letters , 5(2):713‚Äì\n720, 2020. 6, 18\n[55] Xiaoxue Zang, Ashwini Pokle, Marynel V ¬¥azquez, Kevin\nChen, Juan Carlos Niebles, Alvaro Soto, and Silvio Savarese.\nTranslating navigation instructions in natural language to a\nhigh-level plan for behavioral robot navigation. In Proceed-\nings of the 2018 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2657‚Äì2666, 2018. 3\n[56] Jingwei Zhang, Lei Tai, Joschka Boedecker, Wolfram Bur-\ngard, and Ming Liu. Neural slam: Learning to explore with\nexternal memory. arXiv preprint arXiv:1706.09520, 2017. 3\n[57] Fengda Zhu, Yi Zhu, Xiaojun Chang, and Xiaodan Liang.\nVision-language navigation with self-supervised auxiliary\nreasoning tasks. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition , pages\n10012‚Äì10022, 2020. 1, 2\n[58] Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Ab-\nhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven vi-\nsual navigation in indoor scenes using deep reinforcement\nlearning. In 2017 IEEE international conference on robotics\nand automation (ICRA), pages 3357‚Äì3364. IEEE, 2017. 3\nA. Topological Maps\nA.1. Exploration Trajectories\nAs a Ô¨Årst step prior to navigation, the agent builds an\nunderstanding of the environment in the form of a topolog-\nical map. To do this, it uses a set of exploration trajectories.\nMethods for exploring environments have been investigated\nin prior work (e.g. [8, 11]). In our work, we generate the\nexploration trajectories by sampling a set of waypoints in\nthe traversable areas of the environments using the ground\ntruth metric maps and then having the agent navigate to\neach of them. This allows us to create a Ô¨Åxed set of explo-\nration trajectories to make the experiments and evaluation\nacross baselines consistent. During the exploration process\nthe agent keeps track of its observation at each time step as\nwell as the odometry sensor reading of its position and ori-\nentation such that the agent is aware of the relative position\nbetween any two observations. The result is a dense trajec-\ntory throughout the environment as visualized in Fig. 9.\nA.2. Map Construction\nA.2.1 Reachability Estimator\nExamples of our agent-generated topological maps are\nshown in Fig. 10. We use a reachability estimator [36] to\nsparsify the exploration trajectories into a topological map.\nA reachability estimator is a function RE that predicts the\nprobability of any given controller to start from a certain po-\nsition and reach a certain target. In our work, we use an ora-\ncle reachability estimatorREthat exploits the traversability\nof the environment. Here, we deÔ¨Åne reachability between\ntwo positions p1 and p2 as:\nRE(p1,p2) =Œ¥(p1,p2) ‚àómax(dsp ‚àí‚à•p1 ‚àíp2‚à•\ndsp\n,0) (5)\nwhere Œ¥(p1,p2) represents line-of-sight between positions\np1 and p2 according to the traversability maps (as shown in\nwhite and gray in Fig. 9) and dsp is a parameter represent-\ning the sparsity of the resulting topological maps. In other\nwords, reachability is nonzero if an unobstructed line can\nbe drawn between p1 and p2 and if they are within dsp from\neach other.\nA.2.2 Sparsifying a dense trajectory\nGiven a dense exploration trajectory, for any starting\nposition pi, we may conÔ¨Ådently discard any position\npi+1...pj‚àí1 as long as RE(pi,pj) is sufÔ¨Åciently high [36].\nIn other words, we greedily choose the next node by\nmax j\ns.t. RE(pi,pk) >psparse,‚àÄk,i<k ‚â§j (6)\nwhere psparse is the threshold that guarantees a minimum\nreachability of any edge in the resultant graph.\n11\nEnvironment 1\nEnvironment 2\nEnvironment 3\nEnvironment 4\nA B C\nA B C\nA B C\nA B C\nFigure 9: Exploration trajectories. Each row shows different exploration trajectories, going from blue to red, for a single\nenvironment. White represents traversable regions, while gray represents untraversable areas.\n12\nEnvironment A, Ô¨Çoor 1-3\nEnvironment C, Ô¨Çoor 1 \nEnvironment B, Ô¨Çoor 1-2\nEnvironment D, Ô¨Çoor 1-2\nEnvironment E, Ô¨Çoor 1-3\nEnvironment F, Ô¨Çoor 1-3\nEnvironment G, Ô¨Çoor 1-2\nFigure 10: Agent-generated topological maps. The graphs are reasonably sparse but still cover a large portion of the\ntraversable space. Certain nodes may be difÔ¨Åcult to reach or close to walls, such as in Environment B Ô¨Çoor 1.\n13\nFigure 11: Comparison of agent-generated maps (left) and R2R predeÔ¨Åned graphs (right). In general, the agent-\ngenerated maps are sparser, whereas the R2R graphs may have dense clusters of nodes.\n14\nN\nS\nNE\nE\nSESW\nW\nNW\n0-2 meters\n2-5 meters\n5+ meters\nFigure 12: Directed edge labels for the topological map are\nrepresented as discretized polar coordinates. The chart is\nrepresented as a top-down view with the agent in the center,\nand orientations represented as compass directions.\nEach exploration trajectory produces one sparsiÔ¨Åed topo-\nlogical map. Since we perform multiple exploration runs\nper environment, we merge multiple topological maps into\none by adding edges between any two nodes iand j simi-\nlarly if RE(pi,pj) >psparse.\nFinally, we further sparsify the resultant singular graph\nby merging nodes iand j if RE(pi,pj) > pmerge, where\npmerge is a threshold above which two nodes are merged\ninto a single one. All neighbors of i and j become the\nneighbors of the new node, except the ones with reacha-\nbility dropping below psparse after the merge.\nIn our setup, we use dsp = 4m, psparse = 0, pmerge =\n0.5, such that edges are established between positions\nreachable in a straight line within 4m of each other, and\nnodes within 2mof each other are merged.\nA.2.3 Comparison to Pre-DeÔ¨Åned R2R Graphs\nWe illustrate differences between our agent-generated (AG)\nmaps and the predeÔ¨Åned Room2Room (R2R) graphs [4] in\nFig. 11. The AG maps are sparser and may have nodes\nwhich are very close to the walls. For instance, as shown in\nthe top row of Fig. 11, in the R2R graph the hallway along\nthe center of the map has nicely positioned nodes. However,\nthe nodes in the AG maps for the same space are positioned\nalong the walls, resulting in slightly more redundant nodes.\nWe also see large node clusters in open spaces in the R2R\ngraphs. In these locations, there are several nearby nodes in\nthe same area as well as edges which connect the nodes with\neach other. This can be seen in every R2R graph in Fig. 11.\nA.3. Topological Map Representation\nAs stated in Sec. 3.1, each topological map node is en-\ncoded as a ResNet152 feature [23], and each edge is mapped\nto an embedding corresponding with the edge category. The\nedge categories are in discretized polar coordinates which\nare visualized in Fig. 12.\nB. Method\nB.1. Cross-Modal Planning\nB.1.1 Graph Neural Network (GNN)\nOur graph neural network consists of 6 sequential graph net-\nwork (GN) blocks. Borrowing the notation from Battaglia\net al . [6], each GN block is comprised of update func-\ntions œÜv(¬∑), œÜe(¬∑), œÜu(¬∑) and aggregation functions œÅe‚Üív(¬∑),\nœÅe‚Üíu(¬∑), œÅv‚Üíu(¬∑).\nUpdate functions. We implement all update functions\nœÜ(¬∑) as multi-layer perceptrons (MLPs). Each MLP has 4\nfully-connected layers of hidden dimension 1024, each fol-\nlowed by batch normalization [25] and ReLU except the last\nlayer.\nAggregation functions. We implement all aggregation\nfunctions œÅ(¬∑) as element-wise summations.\nInputs to Ô¨Årst GN block. The Ô¨Årst GN block the same\ndimensions as the intermediate GN blocks. As input to the\nÔ¨Årst GN block, we use the following:\n‚Ä¢ Node features v(1)\ni ‚àà R1024 for the ith node in\nthe topological map. Each node embedding is a\nResNet152 embedding.\n‚Ä¢ Edge features e(1)\nj ‚ààR256 for the jth edge in the topo-\nlogical map. Each edge embedding is a learned feature\nfor the corresponding edge category (Fig. 12).\n‚Ä¢ A single global feature u(1) ‚ààR512 which is a learned\nembedding.\nIntermediate GN blocks. The intermediate GN blocks\nuse the following input and output dimensions for interme-\ndiate layer k:\n‚Ä¢ Node features v(k)\ni ‚àà R1024 for the ith node in the\ntopological map.\n‚Ä¢ Edge features e(k)\nj ‚ààR512 for the jth edge in the topo-\nlogical map.\n‚Ä¢ A single global feature u(k) ‚ààR512.\nOutputs of last GN block. The last GN block (index\nm) uses the same input dimension as the intermediate GN\nblocks. For the output dimensions, we use the following:\n‚Ä¢ Node features v(m)\ni ‚àà R768 for the ith node in the\ntopological map.\n15\nB.1.2 Cross-Modal Transformer\nArchitecture. Our cross-modal transformer uses a simi-\nlar architecture to LXMERT [45]. While LXMERT uses\nobject-level image embeddings which are encoded with ob-\nject position information via extra layers, we use node em-\nbeddings (from the GNN) and encode position using learn-\nable position embeddings as described in Sec. 3.2.2.\nFor the language encoding branch, we map each word to\na learnable embedding and each index position to a learn-\nable embedding as well. For words wi at index iof a navi-\ngation instruction, we encode the instructions as:\nhi = LayerNorm(WordEmbed(wi) +IdxEmbed(i)) (7)\nSingle-modality encoders. Each layer of the single-\nmodal encoders contains a self-attention sub-layer followed\nby a feed-forward sub-layer. Each sub-layer is followed by\na residual connection [23] and layer normalization [5].\nCross-modality encoders. Each layer of the cross-\nmodality encoder contains a cross-attention sub-layer, a\nself-attention sub-layer, and a feed-forward sub-layer. Each\nsub-layer is followed by a residual connection [23] and\nlayer normalization [5].\nNumber of layers. In total, we use 9 single-modal lan-\nguage encoding layers, 5 single-modal map encoder layers,\nand 5 cross-modal layers. For further details, we refer the\nreader to Tan and Bansal [45].\nClassiÔ¨Åcation head. We use a linear layer in the classiÔ¨Å-\ncation head to map from the 768-dimensional transformer\nnode outputs to the classiÔ¨Åcation logits.\nLoss function. We use a standard cross-entropy loss\nfor training the cross-modal transformer planner (CMTP),\nwhere classiÔ¨Åcation is done over the nodes and [STOP]\naction. The same loss is used for CMTP-Repeated (CMTP-\nR) except the classiÔ¨Åcation is done only over the nodes. For\nCMTP Binary Cross Entropy (CMTP-BCE), we use the fol-\nlowing loss function:\nL= 0.5CE(xnode,ynode) + 0.5BCE(xstop,ystop) (8)\nwhere CE represents cross-entropy loss over the node pre-\ndictions xnode and ground truth ynode, and BCE is bi-\nnary cross entropy over the predicted stop action xstop and\nground truth ystop.\nTraining details. For the training the planner, we use the\nAdamW optimizer [28, 30] with a learning rate of 2e-5 and\na linear warm-up schedule. We use a weight decay of 0.01\non all non-bias and layer normalization [5] weights. We use\na batch size of 16. All planners (GNN baseline and CMTP\nmodels) are trained from scratch without pre-training.\nB.2. Controller\nB.2.1 Logic\nThe controller is given a navigation plan in the form of a\npath in the topological map. It must then execute the plan\nby using the observations to predict an action at each time\nstep. SpeciÔ¨Åcally, Alg. 1 describes the logic in which the\ncontroller interacts with the environment.\nAlgorithm 1: Controller Logic\ninput : {o1,o2,...,o n}panoramas at planned nodes\nparam: œÉ2 variance of bias function\ndlocalization threshold\nœÜ‚Ä≤\n0 ‚Üê‚Éó1,k ‚Üê1,t ‚Üê1, OBSERVE o0\nwhile k‚â§ndo\nOBSERVE ocurr\nœÜt,œÅt ‚ÜêChigh(ok‚àí1,ocurr,ok)\nif œÅt ‚â§dthen\nk‚Üêk+ 1\nelse\nœÜ‚Ä≤\nt ‚ÜêB(œÜt,œÜ‚Ä≤\nt‚àí1; œÉ2)\nEXECUTE Clow(arg maxi œÜ‚Ä≤\nti )\nt‚Üêt+ 1\nend\nend\nThe controller Ô¨Årst retrieves the RGBD panoramic obser-\nvation for each node in the navigation plan. At the begin-\nning of an episode, the controller assumes that the agent is\nin proximity to the Ô¨Årst node o1 in the plan. It then selects\no2 as the subgoal. At each time step, the agent makes an\nobservation of the environment ocurr, and uses an odome-\ntry sensor reading of the agent‚Äôs orientation to rotate ocurr\nto align with the orientation of o1 and o2.\nChigh predicts a waypoint based on these three panora-\nmas, a bias function B is applied to the prediction to al-\nleviate perceptual aliasing, and then Clow translates that\nwaypoint into a robotic action. If the predicted waypoint is\nwithin a hyperparameterized proximity threshold such that\nœÅ‚â§d, the agent consumes the current subgoal, and the next\nnode in the plan becomes the new subgoal. This process is\nrepeated until there are no more nodes in the plan.\nB.2.2 High level controller ( Chigh)\nThe high level controller, similar to [36], is a robot-agnostic\nfunction that predicts a waypoint when given an agent state.\nAs speciÔ¨Åed above, the agent state is denote by an RGBD\npanoramic observation triplet < oprev,ocurr,osubgoal >.\n16\nPrevious anchor images (rgbd)\nOprev: 512 x 128 x 4\nConvolutional \nEncoder\n3584 x 5 3584 x 3\nClassifier\nPolar prediction œï\nCross entropy loss Proximity œÅ\nL2 Loss\nCurrent images (rgbd)\nOcurr: 512 x 128 x 4\nSubgoal images (rgbd)\nOk: 512 x 128 x 4\nConvolutional \nEncoder\nConvolutional \nEncoder\nzprev: 3584 x 1 zcurr: 3584 x 1 zk: 3584 x 1\nRegressor\n24 x 1 1 x 1\nFigure 13: High-level controller architecture. The high-level controller is composed of three main components: the convo-\nlutional encoder, classiÔ¨Åer, and and regressor. See the text for details.\nThe output of the high level controller is a waypoint, to-\nwards which the agent should move to in order to reach\nthe position of osubgoal. This waypoint is denoted in a\ndiscretized polar coordinate system deÔ¨Åned by < œÜ,œÅ >,\nwhere œÜis the index of one of 24 partitions of each span-\nning 15 degree, and œÅis a numeric distance.\nAs such, the high level controller is a function that maps\nthe agent state of observations to a waypoint:\nChigh(oprev,ocurr,osubgoal) ‚ÜíÀÜœÜ,ÀÜœÅ (9)\nArchitecture. As seen in Figure 13, our controller archi-\ntecture is a general 2-staged approach. We share the same\nconvolutional encoder to extract embeddings from each of\nour observations Oprev, Ocurr and Ok as explained in the\npaper. These embeddings are then passed to the classiÔ¨Åer\nand regressor. Note that the input to the classiÔ¨Åer is a con-\ncatenated 5-tuple of (zprev,zprev ‚àízcurr,zcurr,zcurr ‚àí\nzk,zk), and the input to the regressor is a concatenated 3-\ntuple of (zcurr,zcurr ‚àízk,zk), where each zis the 3584 di-\nmensional output from the convolutional encoder. We Ô¨Ånd\nthat including these ‚Äúdeltas‚Äù that are the difference between\nthe embeddings is helpful for training a better controller.\nLayer In Out Kernel Stride\nPConv 4 64 7 3\nBatch Norm 64 64\nReLU 64 64\nMax Pool 64 64 3 2\nPConv 64 128 5 2\nBatch Norm 128 128\nReLU 128 128\nMax Pool 128 128 2 2\nPConv 128 256 3 1\nBatch Norm 256 256\nReLU 256 256\nMax Pool 256 256 2 2\nPConv 256 512 3 1\nBatch Norm 512 512\nReLU 512 512\nMax Pool 512 512 2 2\nFlatten+FC 3584\nTable 4: Convolutional encoder architecture. Each row\nrepresents a layer in the network along with the number of\ninput channels (In) and number of output channels (Out).\n17\nLayer Shape In Shape Out\nFC 3584 x 5 512\nReLU 512 512\nFC 512 256\nReLU 256 256\nFC 256 128\nReLU 128 128\nFC 128 24\nTable 5: ClassiÔ¨Åer architecture. The classiÔ¨Åer predicts the\ndirection of the waypoint. See the text for details.\nLayer Type shape in shape out\nFC 3584 x 3 256\nReLU 256 256\nFC 256 128\nReLU 128 128\nFC 128 1\nTable 6: Regressor architecture. The regressor predicts\nthe waypoint distance from the agent.\nThe architectures for the convolutional encoder, classiÔ¨Åer,\nand regressor are depicted in Table 4, Table 5, and Table 6,\nrespectively.\nTraining. We perform the training of the high level\ncontroller network ofÔ¨Çine using a synthetically generated\ndataset of 2 million data samples. Each data sample con-\nsists of < oprev,ocurr,osubgoal >and its label < œÜ,œÅ >.\nBoth output values are simultaneously optimized, with the\ndirection index œÜoptimized by cross-entropy (CE), and dis-\ntance œÅby mean squared error (MSE):\nL= 0.5CE(œÜ,ÀÜœÜ) + 0.5MSE(œÅ,ÀÜœÅ) (10)\nDuring training, we also perform data augmentation by ran-\ndomly rotating all observation panoramas and œÅby a same\nrandom amount.\nExpert Waypoint. The expert waypoint is produced by\nground truth positions on a traversibility map (a grid). We\nÔ¨Årst build a traversal graph Gof this grid, with edges con-\nnecting neighboring positions weighted by their L2 distance\nplus a penalty if close to a wall. Then, given any agent posi-\ntion pcurr and a subgoal position psubgoal, we can calculate\na lowest cost path p = {p1,p2,...,p n}, where p1 = pcurr\nand pn = psubgoal.\nTo get an expert waypoint, we deÔ¨Åne a maximum looka-\nhead distance m, and we take the positionpk such that trav-\nelling from p1 to pk via the path is as far as possible, but\nno farther than m. Note that in this setting, when the agent\nposition is sufÔ¨Åciently close to the subgoal,pk = psubgoal is\nchosen such that the expert waypoint position is the subgoal\nposition.\nChigh is a simple function that produces a waypoint\nbased on observations, which can be trained completely of-\nÔ¨Çine given a sufÔ¨Åciently large and robust dataset of samples.\nWe also use DAgger [41] to further improve on the initially\ncollected dataset.\nB.2.3 Low level controller ( Clow)\nWe use a simple low-level controller Clow that rotates the\nagent to orient towards the predicted waypoint and walk in\na straight line towards it. Because of the naive nature of\nthis low-level controller, the waypoint predicted by Chigh\nmust be highly accurate in order to achieve good perfor-\nmance. Our experiments in Sec. 4 indicate that despite us-\ning a simple controller, the agent is still able to successfully\nfollow navigation plans with over 80% success rate. This\nperformance could be further improved by using more so-\nphisticated low-level controllers without requiring substan-\ntial changes to the rest of the overall approach due to our\nmodular setup.\nC. Experiments\nWe use the iGibson simulator [53, 54] along with the\nVLN-CE data [29] for our experiments. To perform train-\ning and evaluation, we convert the VLN-CE trajectories into\npaths in our agent-generated (AG) maps and pre-deÔ¨Åned\nRoom2Room (R2R) graphs [4]. For each episode, we com-\npared the ground truth 3D trajectory positions with the clos-\nest nodes in the appropriate topological maps. Episodes\nwhich contained start/goal positions that were > 2mfrom\nthe closest topological map node were discarded for both\ntrain and validation splits. The baseline GNN and VLN-CE\nmodels were trained in iGibson [54].\nC.1. VLN Qualitative Results\nWe provide more qualitative results of our integrated sys-\ntem (CMTP) in Fig. 15. As can be seen in the Ô¨Ågure, the\nplanner successfully generates plans which land near the\nground truth goal position. Many of the trajectories in-\nvolve going into multiple rooms or navigating past land-\nmarks speciÔ¨Åed in the instructions.\nWe also show example failures and inefÔ¨Åciencies in\nFig. 14. On the left side of Fig. 14, we see the planner\ntraverses past the ground truth goal position, failing to stop\nat the right time. On the other hand, the controller initially\nsuccessfully navigates into the bedroom. However, it mis-\ntakenly backtracks out of the room and after numerous self-\ncorrections, Ô¨Ånally correctly navigates into the bedroom and\nstops at the end of the navigation plan.\nIn the example on the right of Fig. 14, the provided in-\nstruction is too ambiguous. The instruction tells the agent to\n18\nA\n2\n4\n3\nInstruction: Walk past the living room(A) \ninto small hall area(B). Enter the bedroom(C) \nstraight ahead and wait in the doorway. \n1\n2\n4\n3\nInstruction: Move towards the couch(A). 1\nA A\nA\nA\n43\n2\n1\nB C\n1\n2\n3 4\nFigure 14: Failure cases. On the left, the planner overshoots the ground truth destination, failing to stop at the right time.\nThe controller successfully navigates into the room and accidentally backtracks out. However, it still reaches the planner\ndestination at the end of the episode. On the right, the navigation instruction is too ambiguous. There are many couches in\nthe environment and it is unclear which couch to move towards. See the text for more details.\nmove towards the couch, but there are several couches in the\nenvironment and it is unclear which couch is being referred\nto in the instruction. As a result, the planner predicts a plan\nthat moves towards a couch and the controller successfully\nexecutes the navigation plan, but this is not the same couch\nas speciÔ¨Åed in the instruction.\nC.2. Cross Modal Attention\nIn Fig. 16 we include additional examples of cross modal\nattention. These follow the same structure as Fig. 6. In the\ntop-left example, we see ‚Äúhallway‚Äù associated with nodes\nin the hallway, and the ‚Äúbedroom‚Äù word associated with the\nbedroom node (node 4).\nSimilarly, in the bottom-left example, we see that the\nword ‚Äúacross‚Äù has high correlation with the nodes involved\nin traversing across the room (nodes 2, 3, 4), while ‚Äúdou-\nble doors‚Äù is related to the last node. These results suggest\nthat the model is capable of aligning instruction words with\nspatial locations.\n19\nJunshen\nkevin\njo\nFigure 15: Integrated system VLN examples. We show 6 success cases of our CMTP approach. The agent successfully\npredicts plans according to the directions and landmarks speciÔ¨Åed in the instructions, while the controller executes the plans\nvia low-level actions.\n20\n1 2 3 4 5\n[CLS]\nwalk\nout\nthe\ndoor\nto\nyour\nleft\n.\nonce\nout\nthe\ndoor\n,\nwalk\nacross\nthe\nroom\nand\nstop\nin\nthe\ndoorway\nof\nthe\ndouble\ndoors\non\nyour\nleft\n.\n[SEP]\nInstruction:Walk out the door to your left. \nOnce out the door, walk across the room \nand stop in the doorway of the double \ndoors on your left.\n1\n2 3 4\n5\nInstruction: Walk into the living room and\nthrough the doorway to the right of the\nfireplace. Wait inside the dining room door.\n1 2 3 4 5\n[CLS]\nwalk\ninto\nthe\nliving\nroom\nand\nthrough\nthe\ndoorway\nto\nthe\nright\nof\nthe\nfireplace\n.\nwait\ninside\nthe\ndining\nroom\ndoor\n.\n[SEP]\n1\n2\n3 4\n5\nInstruction:Move past the bed into the hallway.\nGo past the bookcase and wait in the\ndoorway of the other bedroom.\n[CLS]\nmove\npast\nthe\nbed\ninto\nthe\nhallway\n.\ngo\npast\nthe\nbook\ncase\nand\nwait\nin\nthe\ndoorway\nof\nthe\nother\nbedroom\n.\n[SEP]\n1 2 3 4\n4\n1\n2\n3\n[CLS]\nwalk\ntowards\nthe\npool\n,\nwalk\nalong\nthe\nleft\nside\nof\nthe\npool\n,\nenter\nthe\nmassage\nroom\n.\n[SEP]\n1 2 3 4 5\nInstruction: Walk towards the pool, walk \nalong the left side of the pool, enter the \nmassage room.\n4\n5\n1 2\n3\nFigure 16: Cross modal attention. These visualizations follow the same format as Fig. 6. See the text for details.\n21",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6320135593414307
    },
    {
      "name": "Backtracking",
      "score": 0.6186224818229675
    },
    {
      "name": "Modular design",
      "score": 0.6175198554992676
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5942314267158508
    },
    {
      "name": "Robotics",
      "score": 0.5478116273880005
    },
    {
      "name": "Robot",
      "score": 0.5289694666862488
    },
    {
      "name": "Plan (archaeology)",
      "score": 0.4903445243835449
    },
    {
      "name": "Transformer",
      "score": 0.48771628737449646
    },
    {
      "name": "Topology (electrical circuits)",
      "score": 0.47466328740119934
    },
    {
      "name": "Natural language",
      "score": 0.4241175651550293
    },
    {
      "name": "Motion planning",
      "score": 0.41264617443084717
    },
    {
      "name": "Computer vision",
      "score": 0.4103381037712097
    },
    {
      "name": "Engineering",
      "score": 0.2004292905330658
    },
    {
      "name": "Geography",
      "score": 0.12284612655639648
    },
    {
      "name": "Algorithm",
      "score": 0.10547095537185669
    },
    {
      "name": "Programming language",
      "score": 0.10461711883544922
    },
    {
      "name": "Electrical engineering",
      "score": 0.09753569960594177
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 12
}