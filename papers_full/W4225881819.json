{
  "title": "UNIREX: A Unified Learning Framework for Language Model Rationale Extraction",
  "url": "https://openalex.org/W4225881819",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2183297351",
      "name": "Aaron Chan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2164522516",
      "name": "Maziar Sanjabi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2266316079",
      "name": "Lambert Mathias",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101128406",
      "name": "Liang Tan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2561852407",
      "name": "Shaoliang Nie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2214624239",
      "name": "Xiaochang Peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108009659",
      "name": "Xiang Ren",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2971547511",
      "name": "Hamed Firooz",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4287704453",
    "https://openalex.org/W3092292656",
    "https://openalex.org/W3035503910",
    "https://openalex.org/W2972918955",
    "https://openalex.org/W2898695519",
    "https://openalex.org/W3103035585",
    "https://openalex.org/W3035064231",
    "https://openalex.org/W4206831821",
    "https://openalex.org/W3173849000",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W2945976633",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3213444437",
    "https://openalex.org/W3035628711",
    "https://openalex.org/W3105868192",
    "https://openalex.org/W3196832757",
    "https://openalex.org/W3023690688",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W3034917890",
    "https://openalex.org/W1902674502",
    "https://openalex.org/W2949227999",
    "https://openalex.org/W3121076170",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2962833140",
    "https://openalex.org/W3152515526",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3208024750",
    "https://openalex.org/W2922580172",
    "https://openalex.org/W3214866469",
    "https://openalex.org/W2594475271",
    "https://openalex.org/W2970447476",
    "https://openalex.org/W2962851944",
    "https://openalex.org/W3035371891",
    "https://openalex.org/W2950831963",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2964159778",
    "https://openalex.org/W2292919134",
    "https://openalex.org/W2970155250",
    "https://openalex.org/W2562979205",
    "https://openalex.org/W2807333695",
    "https://openalex.org/W4300756893",
    "https://openalex.org/W2951936329",
    "https://openalex.org/W3139145960",
    "https://openalex.org/W1975879668",
    "https://openalex.org/W2956090150",
    "https://openalex.org/W3110300144",
    "https://openalex.org/W2605409611",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3015777882",
    "https://openalex.org/W2962862931",
    "https://openalex.org/W2804897457",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "Aaron Chan, Maziar Sanjabi, Lambert Mathias, Liang Tan, Shaoliang Nie, Xiaochang Peng, Xiang Ren, Hamed Firooz. Proceedings of BigScience Episode #5 -- Workshop on Challenges & Perspectives in Creating Large Language Models. 2022.",
  "full_text": "Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models, pages 51 - 67\nMay 27, 2022c⃝2022 Association for Computational Linguistics\nUNIREX: A Unified Learning Framework for\nLanguage Model Rationale Extraction\nAaron Chan1∗, Maziar Sanjabi2, Lambert Mathias2, Liang Tan2,\nShaoliang Nie2, Xiaochang Peng2, Xiang Ren1, Hamed Firooz2\n1University of Southern California, 2Meta AI\n{chanaaro,xiangren}@usc.edu,\n{maziars,mathiasl,liangtan,snie,xiaochang,mhfirooz}@fb.com\nAbstract\nAn extractive rationale explains a language\nmodel’s (LM’s) prediction on a given task in-\nstance by highlighting the text inputs that most\ninfluenced the prediction. Ideally, rationale ex-\ntraction should be faithful (reflective of LM’s\nactual behavior) and plausible (convincing to\nhumans), without compromising the LM’s (i.e.,\ntask model’s) task performance. Although attri-\nbution algorithms and select-predict pipelines\nare commonly used in rationale extraction, they\nboth rely on certain heuristics that hinder them\nfrom satisfying all three desiderata. In light of\nthis, we propose UNIREX , a flexible learning\nframework which generalizes rationale extrac-\ntor optimization as follows: (1) specify archi-\ntecture for a learned rationale extractor; (2) se-\nlect explainability objectives (i.e., faithfulness\nand plausibility criteria); and (3) jointly train\nthe task model and rationale extractor on the\ntask using selected objectives. UNIREX en-\nables replacing prior works’ heuristic design\nchoices with a generic learned rationale ex-\ntractor in (1) and optimizing it for all three\ndesiderata in (2)-(3). To facilitate comparison\nbetween methods w.r.t. multiple desiderata, we\nintroduce the Normalized Relative Gain (NRG)\nmetric. Across five English text classification\ndatasets, our best UNIREX configuration out-\nperforms the strongest baselines by an average\nof 32.9% NRG. Plus, we find that UNIREX -\ntrained rationale extractors’ faithfulness can\neven generalize to unseen datasets and tasks.\n1 Introduction\nLarge neural language models (LMs) have yielded\nstate-of-the-art performance on various natural lan-\nguage processing (NLP) tasks (Devlin et al., 2018;\nLiu et al., 2019). However, LMs’ complex rea-\nsoning processes are notoriously opaque (Rudin,\n2019), posing concerns about the societal implica-\ntions of using LMs for high-stakes decision-making\n∗Work done while AC was a research intern at Meta AI.\nFigure 1: Desiderata of Rationale Extraction. Unlike prior\nworks, UNIREX enables optimizing for all three desiderata.\n(Bender et al., 2021). Thus, explaining LMs’ behav-\nior is crucial for promoting trust, ethics, and safety\nin NLP systems (Doshi-Velez and Kim, 2017; Lip-\nton, 2018). Given a LM’s (i.e., task model’s) pre-\ndicted label on a text classification instance, an ex-\ntractive rationaleis a type of explanation that high-\nlights the tokens that most influenced the model\nto predict that label (Luo et al., 2021). Ideally, ra-\ntionale extraction should be faithful (Ismail et al.,\n2021; Jain et al., 2020) and plausible (DeYoung\net al., 2019), without hurting the LM’s task perfor-\nmance (DeYoung et al., 2019) (Fig. 1).\nConfiguring the rationale extractor and its train-\ning can greatly impact these desiderata, yet prior\nworks have commonly adopted two suboptimal\nheuristics. First, many works rely in some way on\nattribution algorithms(AAs), which extract ratio-\nnales via handcrafted functions (Sundararajan et al.,\n2017; Ismail et al., 2021; Situ et al., 2021). AAs\ncannot be directly trained and tend to be compute-\nintensive (Bastings and Filippova, 2020). Also,\nAAs can be a bottleneck for plausibility, as pro-\nducing human-like rationales is a complex objec-\n51\ntive requiring high capacity rationale extractors\n(Narang et al., 2020; DeYoung et al., 2019). Sec-\nond, many works use a specialized select-predict\npipeline (SPP), where a predictor module is trained\nto solve the task using only tokens chosen by a\nselector module (Jain et al., 2020; Yu et al., 2021;\nParanjape et al., 2020). Instead of faithfulness opti-\nmization, SPPs heuristically aim for “faithfulness\nby construction\" by treating the selected tokens as a\nrationale for the predictor’s output (which depends\nonly on those tokens). Still, SPPs typically have\nworse task performance than vanilla LMs since\nSPPs hide the full input from the predictor.\nTo tackle this challenge, we propose theUNIfied\nLearning Framework for Rationale EXtraction\n(UNIREX ), which generalizes rationale extractor\noptimization as follows: (1) specify architecture\nfor a learned rationale extractor; (2) select explain-\nability objectives (i.e., faithfulness and plausibil-\nity criteria); and (3) jointly train the task model\nand rationale extractor on the task using selected\nobjectives (Sec. 3). UNIREX enables replacing\nprior works’ heuristic design choices in (1) with a\ngeneric learned rationale extractor and optimizing\nit for all three desiderata in (2)-(3).\nUNIREX provides significant flexibility in per-\nforming (1)-(3). For (1), any model architecture is\napplicable, but we study Transformer LM based ra-\ntionale extractors in this work (Zaheer et al., 2020;\nDeYoung et al., 2019). We focus on two archi-\ntectures: (A) Dual LM, where task model and ra-\ntionale extractor are separate and (B) Shared LM,\nwhere task model and rationale extractor share pa-\nrameters. For (2), any faithfulness and plausibility\ncriteria can be used. Following DeYoung et al.\n(2019), we focus on comprehensiveness and suffi-\nciency as faithfulness criteria, while using similar-\nity to gold rationales as plausibility criteria. For (3),\ntrade-offs between the three desiderata can be eas-\nily managed during rationale extractor optimization\nby setting arbitrary loss weights for the faithfulness\nand plausibility objectives. Plus, though comput-\ning the faithfulness criteria involves discrete (non-\ndifferentiable) token selection, using Shared LM\ncan approximate end-to-end training and enable\nboth task model and rationale extractor to be opti-\nmized w.r.t. all three desiderata (Sec. 3.3).\nTo evaluate all three desiderata in aggregate, we\nintroduce the Normalized Relative Gain (NRG)\nmetric. Across five English text classification\ndatasets – SST, Movies, CoS-E, MultiRC, and e-\nSNLI (Carton et al., 2020; DeYoung et al., 2019) –\nour best UNIREX configuration outperforms the\nstrongest baselines by an average of 32.9% NRG\n(Sec. 4.2), showing that UNIREX can optimize\nrationale extractors for all three desiderata. In ad-\ndition, we verify our UNIREX design choices via\nextensive ablation studies (Sec. 4.3). Furthermore,\nUNIREX -trained extractors have high generaliza-\ntion power, yielding high plausiblity with minimal\ngold rationale supervision (Sec. 4.4) and high faith-\nfulness on unseen datasets and tasks (Sec. 4.5).\nFinally, our user study shows that humans judge\nUNIREX rationales as more plausible than ratio-\nnales extracted using other methods (Sec. 4.6).\n2 Problem Formulation\nRationale Extraction Let Ftask = ftask(fenc(·))\nbe a task model for M-class text classification\n(Sec. A.1), where fenc is the text encoder and\nftask is the task output head. Typically, Ftask has\na BERT-style architecture (Devlin et al., 2018), in\nwhich fenc is a Transformer (Vaswani et al., 2017)\nwhile ftask is a linear layer with softmax classi-\nfier. Let xi = [ xt\ni]n\nt=1 be the n-token input se-\nquence (e.g., a sentence) for task instance i, and\nFtask(xi) ∈RM be the logit vector for the output\nof the task model. Let ˆyi = arg maxj Ftask(xi)j\nbe the class predicted by Ftask. Given Ftask, xi,\nand ˆyi, the goal of rationale extraction is to output\nvector si = [st\ni]n\nt=1 ∈Rn, such that each st\ni ∈R is\nan importance scoreindicating how much token xt\ni\ninfluenced Ftask to predict class ˆyi. Let Fext be a ra-\ntionale extractor, such that si = Fext(Ftask,xi,ˆyi).\nFext can be a learned or heuristic function. In prac-\ntice, the final rationale is often obtained by bina-\nrizing si as ri ∈{0,1}n, via the top-k% strategy:\nrt\ni = 1 if st\ni is one of the top-k% scores in si; oth-\nerwise, rt\ni = 0 (DeYoung et al., 2019; Jain et al.,\n2020; Pruthi et al., 2020; Chan et al., 2021). For\ntop-k%, let r(k)\ni be the “important\" (i.e., ones) to-\nkens in ri, when using 0 ≤k≤100.\nFaithfulness means how well a rationale re-\nflects Ftask’s true reasoning process for predict-\ning ˆyi (Jacovi and Goldberg, 2020). Hence, faith-\nfulness metrics measure how much the r(k)\ni to-\nkens impact pˆyi(xi), which denotes Ftask’s confi-\ndence probability for ˆyi when using xi as input\n(DeYoung et al., 2019; Shrikumar et al., 2017;\nHooker et al., 2018; Pruthi et al., 2020). Recently,\ncomprehensiveness and sufficiency have emerged\nas popular faithfulness metrics (DeYoung et al.,\n52\n2019). Comprehensiveness (comp) measures the\nchange in pˆyi when r(k)\ni is removed from the in-\nput: comp = pˆyi(xi) −pˆyi(xi\\r(k)\ni ). Sufficiency\n(suff) measures the change in pˆyi when only r(k)\ni is\nkept in the input: suff = pˆyi(xi) −pˆyi(r(k)\ni ). High\nfaithfulness is signaled by high comp and low suff.\nPlausibility means how convincing a rationale\nis to humans (Jacovi and Goldberg, 2020). This\ncan be measured by automatically computing the\nsimilarity between Fext’s rationales (either si or ri)\nand human-annotated gold rationales (DeYoung\net al., 2019), or by asking human annotators to rate\nwhether Fext’s rationales make sense for predict-\ning ˆyi (Strout et al., 2019; Doshi-Velez and Kim,\n2017). Typically, a gold rationale is a binary vector\nr∗\ni ∈ {0,1}n, where ones/zeros indicate impor-\ntant/unimportant tokens (Lei et al., 2016).\nTask Performance, w.r.t. rationale extraction,\nconcerns how much Ftask’s task performance (on\ntest set) drops when Ftask is trained with explain-\nability objectives ( i.e., faithfulness, plausibility)\nfor Fext. As long as Ftask is trained with non-task\nlosses, Ftask’s task performance can be affected.\n3 UNIREX\nGiven task model Ftask, UNIREX generalizes\nrationale extractor optimization as follows: (1)\nchoose architecture for a learned rationale extrac-\ntor Fext; (2) select explainability objectives ( i.e.,\nfaithfulness loss Lfaith and plausibility loss Lplaus);\nand (3) jointly train Ftask and Fext using Ltask (task\nloss), Lfaith, and Lplaus. UNIREX training consists\nof two backpropagation paths (Fig. 2). The first\npath is used to update Ftask w.r.t. Ltask and Lfaith.\nWhereas Ltask is computed w.r.t. the task target\nyi, Lfaith is computed only using the task input xi\nand the top-k% important tokens r(k)\ni (obtained via\nFext), based on some combination of comp and\nsuff (Sec. 2). The second path is used to update\nFext w.r.t. Lplaus, which encourages importance\nscores si to approximate gold rationale r∗\ni. Thus,\nUNIREX frames rationale extraction as the follow-\ning optimization problem:\nmin\nFtask,Fext\nLtask(xi,yi; Ftask)\n+ αfLfaith(xi,r(k)\ni ; Ftask)\n+ αpLplaus(xi,r∗\ni; Fext),\n(1)\nwhere αf and αp are loss weights. If Ftask and\nFext share parameters, then the shared parameters\nwill be optimized w.r.t. all losses. During inference,\nfor task input xi, we first use Ftask to predict yi,\nthen use Fext to output a rationale ri for Ftask’s\nprediction ˆyi. Below, we discuss options for the\nrationale extractor and explainability objectives.\n3.1 Rationale Extractor\nIn UNIREX , Fext is a learned function by default.\nLearned Fext can be any model that transforms xt\ni\ninto st\ni. Given their success in NLP explainability\n(DeYoung et al., 2019), we focus on pre-trained\nTransformer LMs and highlight two architectures:\nDual LM (DLM) and Shared LM (SLM) (Fig. 3).\nFor DLM, Ftask and Fext are two separate Trans-\nformer LMs. DLM provides more dedicated capac-\nity for Fext, which can help Fext output plausible\nrationales. For SLM, Ftask and Fext are two Trans-\nformer LMs sharing encoder fenc, while Fext has\nits own output head fext. SLM leverages multitask\nlearning between Ftask and Fext, which can im-\nprove faithfulness since Fext gets more information\nabout Ftask’s reasoning process. Unlike heuristic\nFext (Sec. A.2), learned Fext can be optimized for\nfaithfulness/plausibility, but cannot be used out of\nthe box without training. Learned Fext is preferred\nif: (A) optimizing for both faithfulness and plau-\nsibility, and (B) gold rationales are available for\nplausibility optimization (Sec. A.3).\n3.2 Explainability Objectives\nAfter selecting Fext, we specify the explainabil-\nity objectives, which can be any combination of\nfaithfulness and plausibility criteria. In prior ap-\nproaches (e.g., AA, SPPs), the rationale extractor is\nnot optimized for both faithfulness and plausibility,\nbut UNIREX makes this possible. For any choice\nof learned Fext, UNIREX lets us easily “plug and\nplay\" different criteria and loss weights, based on\nour needs and domain knowledge, to find those that\nbest balance the rationale extraction desiderata.\nFaithfulness Evaluating rationale faithfulness\nis still an open problem with many existing metrics,\nand UNIREX is not tailored for any specific metric.\nStill, given the prevalence of comp/suff (Sec. 2),\nwe focus on comp/suff based objectives.\nRecall that comp measures the importance of to-\nkens in r(k)\ni as how pˆyi(ˆxi), Ftask’s predicted prob-\nability for class ˆyi, changes when those tokens are\nremoved fromxi. Intuitively, we wantpˆyi(ˆxi) to be\nhigher than pˆyi(xi\\r(k)\ni ), so higher comp is better.\nSince comp is defined for a single class’ probability\nrather than the label distribution, we can define the\ncomp loss Lcomp via cross-entropy loss LCE, as in\n53\nFigure 2: UNIREX Framework. UNIREX enables jointly optimizing the task model (Ftask) and rationale extractor (Fext),\nw.r.t. faithfulness (Lfaith), plausibility (Lplaus), and task performance (Ltask).\nFigure 3: Rationale Extractor Types.\nthe following difference criterionfor Lcomp:\nLcomp-diff = LCE(Ftask(xi),yi)\n−LCE(Ftask(xi\\r(k)\ni ),yi))\n(2)\nLCE(Ftask(xi),yi) =−yi log(Ftask(xi)) (3)\nFor training stability, we compute comp loss\nfor target class yi here instead of Ftask’s pre-\ndicted class ˆyi, since ˆyi is a moving target dur-\ning training. Using Lcomp-diff, it is possible for\nLCE(Ftask(xi\\r(k)\ni ),yi)) to become much larger\nthan LCE(Ftask(xi),yi), leading to arbitrarily neg-\native losses. To avoid this, we can add marginmc\nto the loss function, giving the margin criterion:\nLcomp-margin = max(−mc,LCE(Ftask(xi),yi)\n−LCE(Ftask(xi\\r(k)\ni ),yi)) +mc\n(4)\nRecall that suff measures the importance of to-\nkens in r(k)\ni as how pˆyi(ˆxi), Ftask’s predicted prob-\nability for class ˆyi, changes when they are the only\ntokens kept in xi. Based on suff’s definition, we\nwant pˆyi(r(k)\ni ) to be higher than pˆyi(ˆxi), so lower\nsuff is better. For suff loss Lsuff, we define the\ndifference and margin criteria analogously with\nmargin ms but the opposite sign (since lower suff\nis better):\nLsuff-diff = LCE(Ftask(r(k)\ni ),yi)\n−LCE(Ftask(xi),yi)\n(5)\nLsuff-margin = max(−ms,LCE(Ftask(r(k)\ni ),yi)\n−LCE(Ftask(xi),yi)) +ms\n(6)\nIn our experiments, we find that the margin-\nbased comp/suff criteria are effective (Sec. 4.3),\nthough others ( e.g., KL Div, MAE) can be used\ntoo (Sec. A.4.1). Note that r(k)\ni is computed via\ntop-k% thresholding (Sec. 2), so we also need to\nspecify a set Kof threshold values. We separately\ncompute the comp/suff losses for eachk∈K, then\nobtain the final comp/suff losses by averaging over\nall kvalues via area-over-precision-curve (AOPC)\n(DeYoung et al., 2019). To reflect this, we denote\nthe comp and suff losses asLcomp,K and Lsuff,K, re-\nspectively. Let αfLfaith = αcLcomp,K + αsLsuff,K,\nwhere αc and αs are loss weights.\nPlausibility Plausibility is defined as how con-\nvincing a rationale is to humans (Jacovi and Gold-\nberg, 2020), i.e., whether humans would agree the\nrationale supports the model’s prediction. While\noptimizing for plausibility should ideally involve\nhuman-in-the-loop feedback, this is prohibitive. In-\nstead, many works consider gold rationales as a\ncheaper form of plausibility annotation (DeYoung\net al., 2019; Narang et al., 2020; Jain et al., 2020).\nThus, if gold rationale supervision is available, then\n54\nwe can optimize for plausibility. With gold ratio-\nnale r∗\ni for input xi, plausibility optimization en-\ntails training Fext to predict binary importance la-\nbel r∗,t\ni for each token xt\ni. This is essentially token\nclassification, so one natural choice for Lplaus is the\ntoken-level binary cross-entropy (BCE) criterion:\nLplaus-BCE = −\n∑\nt\nr∗,t\ni log(Fext(xt\ni)) (7)\nBesides BCE loss, we can also consider other\ncriteria like sequence-level KL divergence and L1\nloss. See Sec. A.4.2 for discussion of these and\nother plausibility criteria.\n3.3 Training and Inference\nAfter setting Fext, Lfaith, and Lplaus, we can move\non to training Ftask and Fext. Since top- k% ratio-\nnale binarization (Sec. 3.2) is not differentiable,\nby default, we cannot backpropagate Lfaith through\nall of Fext’s parameters. Thus, Ftask is trained via\nLtask and Lfaith, while Fext is only trained viaLplaus.\nThis means Fext’s rationales ri are indirectly opti-\nmized for faithfulness by regularizing Ftask such\nthat its behavior aligns with ri. The exception is if\nwe are using the SLM variant, where encoder fenc\nis shared by Ftask and Fext. In this case, fenc is opti-\nmized w.r.t. all losses, ftask is optimized w.r.t. Ltask\nand Lfaith, and fext is optimized w.r.t. Lplaus. SLM\nis a simple way to approximate end-to-end training\nof Ftask and Fext. In contrast, past SPPs have used\nmore complex methods like reinforcement learning\n(Lei et al., 2016) and the reparameterization trick\n(Bastings et al., 2019), whose training instability\ncan hurt task performance (Jain et al., 2020).\nNow, we summarize the full learning objec-\ntive. Given that cross-entropy loss Ltask =\nLCE(Ftask(xi),yi) is used to train Ftask to predict\nyi, the full learning objective is:\nL= Ltask + αf Lfaith + αpLplaus\n= Ltask + αcLcomp,K + αsLsuff,K + αpLplaus. (8)\nDuring inference, we use Ftask to predict yi, then\nuse Fext to output ri for Ftask’s predicted label ˆyi.\n4 Experiments\nWe present empirical results demonstrating\nUNIREX ’s effectiveness in managing trade-offs\nbetween faithfulness, plausibility, and task per-\nformance during rationale extractor optimization.\nFirst, our main experiments compare methods w.r.t.\nfaithfulness, plausibility, and task performance\n(Sec. 4.2). Second, we perform various ablation\nstudies to verify our design choices for UNIREX\n(Sec. 4.3). Third, we present experiments high-\nlighting UNIREX ’s generalization ability, both in\nterms of limited gold rationale supervision (Sec.\n4.4) and zero-shot transfer (Sec. 4.5). Fourth, we\nconduct a user study to further evaluate UNIREX\nrationales’ plausibility, relative to those generated\nby other methods (Sec. 4.6). See Sec. A.5 for im-\nplementation details (LM architecture, AA settings,\ntraining).\n4.1 Experiment Setup\nDatasets We primarily use SST (Socher et al.,\n2013; Carton et al., 2020), Movies (Zaidan and\nEisner, 2008), CoS-E (Rajani et al., 2019), Mul-\ntiRC (Khashabi et al., 2018), and e-SNLI (Camburu\net al., 2018), all of which have gold rationale an-\nnotations. The latter four datasets were taken from\nthe ERASER benchmark (DeYoung et al., 2019).\nMetrics We use the metrics from the\nERASER explainability benchmark (DeYoung\net al., 2019). For faithfulness, we use compre-\nhensiveness (Comp) and sufficiency (Suff), for\nk = [1,5,10,20,50] (DeYoung et al., 2019). For\nplausibility, we use area under precision-recall\ncurve (AUPRC) and token F1 (TF1) to measure\nsimilarity to gold rationales (DeYoung et al., 2019;\nNarang et al., 2020). For task performance, we\nfollow (DeYoung et al., 2019) and (Carton et al.,\n2020) in using accuracy (SST, CoS-E) and macro\nF1 (Movies, MultiRC, e-SNLI).\nTo aggregately evaluate multiple desiderata, we\nintroduce the Normalized Relative Gain (NRG)\nmetric, which is based on the ARG metric from\nYe et al. (2021). NRG normalizes raw metrics\n(e.g., F1, sufficiency) to scores between 0 and\n1 (higher is better). Given a set of raw met-\nric scores Z = {z1,z2,...}(each from a differ-\nent method), NRG(zi) captures zi’s value rela-\ntive to min(Z) and max(Z). If higher values\nare better for the given metric (e.g., F1), then we\nhave: NRG(zi) = zi−min(Z)\nmax(Z)−min(Z) . If lower val-\nues are better ( e.g., sufficiency), then we have:\nNRG(zi) = max(Z)−zi\nmax(Z)−min(Z) . After computing NRG\nfor multiple raw metrics, we can aggregate them\nw.r.t. desiderata via averaging. Let FNRG, PNRG,\nand TNRG be the NRG values for faithfulness,\nplausibility, and task performance, respectively. Fi-\nnally, we compute the composite NRG as:CNRG =\nFNRG+PNRG+TNRG\n3 .\nResults Reporting For all results, we report\naverage over three seeds and the five kvalues. We\n55\nFigure 4: Composite NRG Comparison (w/o Plausibility Optimization). Composite NRG (CNRG) is the mean of the three\ndesiderata NRG scores. For each dataset, we use CNRG to compare methods that do notoptimize for plausibility.\nFigure 5: Composite NRG Comparison (w/ Plausibility Optimization). Composite NRG (CNRG) is the mean of the three\ndesiderata NRG scores. For each dataset, we use CNRG to compare methods that do optimize for plausibility.\ndenote each UNIREX configuration with “([ratio-\nnale extractor]-[explainability objectives])”. F, P,\nand FP denote faithfulness, plausibility, and faith-\nfulness+plausibility, respectively.\nBaselines The first category is AAs, which are\nnot trained: AA (Grad) (Simonyan et al., 2013), AA\n(Input*Grad) (Denil et al., 2014), AA (DeepLIFT)\n(Lundberg and Lee, 2017), AA (IG) (Sundarara-\njan et al., 2017). We also experiment with IG for\nL2E (Situ et al., 2021), which distills knowledge\nfrom an AA to an LM. The second category is\nSPPs: FRESH (Jain et al., 2020) and A2R (Yu\net al., 2021). For FRESH, we use a strong vari-\nant where IG rationales are directly given to the\npredictor, rather than output by a trained selector.\nA2R aims to improve SPP task performance by\nregularizing the predictor with an attention-based\npredictor that uses the full input. In addition, we\nintroduce FRESH+P and A2R+P, which augment\nFRESH and A2R, respectively, with plausibility\noptimization. The third category is AA-based reg-\nularization: SGT (Ismail et al., 2021), which uses\na sufficiency-based criterion to optimize for faith-\nfulness. We also consider SGT+P, which augments\nSGT with plausibility optimization.\n4.2 Main Results\nFig. 4-6 display the main results. In Fig. 4/5, we\ncompare the CNRG for all methods and datasets,\nwithout/with gold rationales. In both plots, we see\nthat UNIREX variants achieve the best CNRG\nacross all datasets, indicating that they are effec-\ntive in balancing the three desiderata. In partic-\nular, UNIREX (DLM-FP) and UNIREX (SLM-\n56\nFigure 6: NRG Comparison by Desiderata. We show FNRG, PNRG, and TNRG for all methods, averaged over all datasets.\nFP) have very high CNRG scores, both yielding\nmore than 30% improvement over the strongest\nbaselines. Fig. 6 compares methods w.r.t. desider-\nata NRG (i.e., FNRG, PNRG, TNRG). Here, the\nleft/right plots show methods without/with gold\nrationales. Again, we see that UNIREX variants\nachieve a good NRG balance of faithfulness, plau-\nsibility, and task performance. Meanwhile, many\nbaselines (e.g., AA (IG), A2R, SGT+P) do well on\nsome desiderata but very poorly on others.\n4.3 Ablation Studies\nWe present five ablation studies to validate the ef-\nfectiveness of our UNIREX design choices. The\nablation results are displayed in Table 1. In this\ntable, each of the five sections shows results for\na different ablation. Thus, all numbers within the\nsame section and column are comparable.\nExtractor Type In the Ext Type (F) section, we\ncompare four heuristic rationale extractors, using\nAA-F. Rand uses random importance scores, Gold\ndirectly uses the gold rationales, Inv uses the in-\nverse of the gold rationales, and IG uses IG. All\nheuristics yield similar task performance, but IG\ndominates on all faithfulness metrics. This makes\nsense because IG is computed using Ftask’s in-\nputs/parameters/outputs, while the others do not\nhave this information. For plausibility, Gold is the\nbest, Inv is the worst, and Rand and IG are about\nthe same, as none of the heuristics are optimized\nfor plausibility. In the Ext Type (FP) section, we\ncompare four learned rationale extractors. By de-\nfault, attribution algorithms’ dimension scores are\npooled into token scores via sum pooling. AA-FP\n(Sum) uses IG with sum pooling, while AA-FP\nAblation UNIREX ConfigFaithfulness Plausibility PerformanceComp (↑) Suff (↓) AUPRC (↑) Acc (↑)\nExt Type (F)\nAA-F (Rand) 0.171 (±0.040) 0.327 (±0.050) 44.92 (±0.00)94.05(±0.35)AA-F (Gold) 0.232 (±0.088) 0.249 (±0.021)100.00(±0.00) 93.81 (±0.54)AA-F (Inv) 0.242 (±0.010) 0.357 (±0.019) 20.49 (±0.00) 93.47 (±1.81)AA-F (IG)0.292(±0.051)0.171(±0.038) 48.13 (±1.14) 92.97 (±0.44)\nExt Type (FP)\nAA-FP (Sum) 0.296 (±0.067) 0.185 (±0.048) 47.60 (±2.44) 93.25 (±0.45)AA-FP (MLP) 0.285 (±0.051) 0.197 (±0.100) 54.82 (±1.97) 93.23 (±0.92)DLM-FP0.319(±0.090) 0.167 (±0.036)85.80(±0.74)93.81(±0.18)SLM-FP 0.302 (±0.039)0.113(±0.013) 82.55 (±0.84) 93.68 (±0.67)\nComp/Suff LossSLM-FP (Comp)0.350(±0.048) 0.310 (±0.049) 82.79 (±0.62) 93.59 (±0.11)SLM-FP (Suff) 0.166 (±0.003) 0.152 (±0.012)83.74(±0.84)94.16(±0.39)SLM-FP (Comp+Suff) 0.302 (±0.039)0.113(±0.013) 82.55 (±0.84) 93.68 (±0.67)\nSuff CriterionSLM-FP (KL Div)0.306(±0.098) 0.131 (±0.005) 82.62 (±0.88) 93.06 (±0.25)SLM-FP (MAE) 0.278 (±0.058) 0.143 (±0.008)82.66(±0.61)93.78(±0.13)SLM-FP (Margin) 0.302 (±0.039)0.113(±0.013) 82.55 (±0.84) 93.68 (±0.67)\nSLM Ext HeadSLM-FP (Linear) 0.302 (±0.039)0.113(±0.013) 82.55 (±0.84)93.68(±0.67)SLM-FP (MLP-2048-2)0.323(±0.071) 0.144 (±0.012) 83.82 (±0.77) 93.67 (±0.18)SLM-FP (MLP-4096-3) 0.295 (±0.057) 0.154 (±0.027)84.53(±0.61) 93.19 (±0.79)\nTable 1: UNIREX Ablation Studies on SST.\n(MLP) replaces the sum pooler with a MLP-based\npooler to increase capacity for plausibility opti-\nmization. Task performance for all four methods is\nsimilar, AA-FP (Sum) dominates on faithfulness,\nand DLM-FP and SLM-FP dominate on plausibil-\nity. AA-FP (MLP) does not perform as well on\nfaithfulness but slightly improves on plausibility\ncompared to AA-FP (Sum).\nComp/Suff Losses The Comp/Suff Loss sec-\ntion compares different combinations of Comp\nand Suff losses, using SLM-FP. Note that SLM-\nFP (Comp+Suff) is equivalent to SLM-FP shown\nin other tables/sections. As expected, SLM-\nFP (Comp) does best on Comp, but SLM-FP\n(Comp+Suff) actually does best on Suff. Mean-\nwhile, SLM-FP, (Suff) does second-best on Suff\nbut is much worse on Comp. This shows that Comp\nand Suff are complementary for optimization.\nSuff Criterion The Suff Criterion section com-\npares different Suff criteria, using SLM-FP. SLM-\nFP (KLDiv) uses the KL divergence criterion,\nSLM-FP (MAE) uses the MAE criterion, and SLM-\nFP (Margin) uses the margin criterion. SLM-FP\n(Margin) is equivalent to SLM-FP in other ta-\n57\nFigure 7: Gold Rationale Data Efficiency on SST.\nbles/sections. All criteria yield similar performance\nand plausibility, while Margin is slightly better on\nfaithfulness.\nSLM Extractor Head The SLM Ext Head\nsection compares different extractor heads, using\nSLM-FP. Linear is the default choice and uses a\nlinear layer. MLP-2048-2 uses a MLP with two\n2048-dim hidden layers. MLP-4096-3 uses a MLP\nwith three 4096-dim hidden layers. All three out-\nput head types yield similar performance, but de-\ncreasing head capacity yields better faithfulness,\nwhile increasing head capacity heads yields bet-\nter plausibility. This trades off faithfulness and\nplausibility, although larger heads will be more\ncompute-intensive.\n4.4 Gold Rationale Data Efficiency\nUNIREX supports arbitrary amounts of gold ratio-\nnale supervision and allows us to account for data\nefficiency. In Fig. 7, we compare plausibility (in\nAUPRC) for γ = [0.5,1,5,10,20,100] (i.e., % of\ntrain instances with gold rationales). We compare\nAA (IG) and four UNIREX variants (AA-F, AA-\nFP, DLM-FP, SLM-FP). AA (IG) and AA-F do not\nuse gold rationales and thus have the same AUPRC\nfor all γ. Standard deviation is shown by the error\nbands. UNIREX (DLM-FP) and UNIREX (SLM-\nFP) dominate across all γ values, with AUPRC\nslowly decreasing as γdecreases. Even at γ = 0.5,\nthey can still achieve high AUPRC. This suggests\nthat UNIREX ’s gold rationale batching procedure\n(Sec. A.3) is effective for learning from minimal\ngold rationale supervision and demonstrates how\nUNIREX enables us to manage this trade-off. See\nSec. A.6 for similar results on CoS-E.\nTask Dataset Method Faithfulness Task Performance\nComp (↑) Suff (↓) Perf ( ↑)\nSA\nSST AA (IG) 0.119 (±0.009) 0.258 (±0.031)93.81(±0.55)UNIREX (AA-F) 0.292 (±0.051) 0.171 (±0.038) 92.97 (±0.44)UNIREX (DLM-FP)0.319(±0.090)0.167(±0.036)93.81(±0.54)\nYelp AA (IG) 0.069 (±0.004) 0.219 (±0.028)92.50(±2.07)UNIREX (AA-F) 0.138 (±0.078) 0.126 (±0.059) 83.93 (±13.20)UNIREX (DLM-FP)0.265(±0.094)0.097(±0.033) 92.37 (±0.46)\nAmazon AA (IG) 0.076 (±0.010) 0.224 (±0.037)91.13(±0.28)UNIREX (AA-F) 0.130 (±0.077)0.073(±0.039) 77.90 (±13.12)UNIREX (DLM-FP)0.232(±0.072) 0.098 (±0.033) 89.35 (±2.22)\nHSD StormfrontAA (IG) 0.135 (±0.010) 0.245 (±0.059)10.48(±1.66)UNIREX (AA-F)0.219(±0.009)0.092(±0.025) 10.36 (±1.94)UNIREX (DLM-FP) 0.167 (±0.084) 0.115 (±0.059) 10.37 (±2.66)\nOSD OffenseEvalAA (IG) 0.097 (±0.009) 0.244 (±0.052) 33.51 (±0.99)UNIREX (AA-F) 0.074 (±0.040) 0.102 (±0.024) 32.62 (±4.85)UNIREX (DLM-FP)0.140(±0.049)0.087(±0.045)35.52(±1.26)\nID SemEval2018AA (IG) 0.128 (±0.014) 0.248 (±0.064) 29.63 (±4.72)UNIREX (AA-F) 0.069 (±0.041)0.096(±0.011)49.95(±8.31)UNIREX (DLM-FP)0.149(±0.052) 0.102 (±0.053) 31.97 (±2.80)\nTable 2: Zero-Shot Faithfulness Transfer from SST.\n4.5 Zero-Shot Faithfulness Transfer\nIn Table 2, we investigate if Fext’s faithfulness, via\nUNIREX training on some source dataset, can gen-\neralize to unseen target datasets/tasks in a zero-shot\nsetting (i.e., no fine-tuning on target datasets). Plau-\nsibility is not evaluated here, since these unseen\ndatasets do not have gold rationales. As the source\nmodel, we compare various SST-trained models:\nAA (IG) and UNIREX (AA-F, DLM-FP). First, we\nevaluate on unseen datasets for a seen task (senti-\nment analysis (SA)): Yelp (Zhang et al., 2015) and\nAmazon (McAuley and Leskovec, 2013). Second,\nwe evaluate on unseen datasets for unseen tasks:\nStormfront (hate speech detection (HSD), binary\nF1) (de Gibert et al., 2018), OffenseEval (offen-\nsive speech detection (OSD), macro F1) (Zampieri\net al., 2019), and SemEval2018 (irony detection\n(ID), binary F1) (Van Hee et al., 2018).\nWe want to show that, even ifFtask yields poor\ntask performance on unseen datasets, Fext’s ratio-\nnales can still be faithful. As expected, all meth-\nods achieve much lower task performance in the\nthird setting than in the first two settings. How-\never, faithfulness does not appear to be strongly\ncorrelated with task performance, as unseen tasks’\ncomp/suff scores are similar to seen tasks’. Across\nall datasets, DLM-FP has the best faithfulness and\nis the only method whose comp is always higher\nthan suff. AA-F is not as consistently strong as\nDLM-FP, but almost always beats AA (IG) on\ncomp and suff. Meanwhile, AA (IG) has the worst\ncomp and suff overall. Ultimately, these results\nsuggest that UNIREX -trained models’ faithfulness\n(i.e., alignment between Ftask’s and Fext’s outputs)\nis a dataset/task agnostic property ( i.e., can gen-\neralize across datasets/tasks), further establishing\nUNIREX’s utility in low-resource settings.\n58\nMethod Forward Simulation Subjective Rating\nAccuracy (%) Confidence (1-4) Alignment (1-5)\nNo Rationale 92.00 (±3.35) 3.02 (±0.39) -\nSGT+P 80.80 (±9.73) 2.34 (±0.31) 3.64 (±0.28)A2R+P 41.20 (±4.71)2.83(±0.28) 2.97 (±0.12)UNIREX (AA-FP) 72.00 (±7.78) 2.00 (±0.31) 3.26 (±0.31)UNIREX (DLM-FP)83.60(±5.41) 2.77 (±0.28) 3.96(±0.22)\nGold 81.20 (±3.03) 2.88 (±0.30) 4.00 (±0.20)\nTable 3: Plausibility User Study on SST.\n4.6 User Study on Plausibility\nGold rationale based plausibility evaluation is noisy\nbecause gold rationales are for the target label, not\na model’s predicted label. Thus, we conduct two\nfive-annotator user studies (Table 3) to get a better\nplausibility measurement. Given 50 random test in-\nstances from SST, we get the rationales for SGT+P,\nA2R+P, UNIREX (AA-FP), and UNIREX (DLM-\nFP), plus the gold rationales. For each instance, we\nthreshold all rationales to have the same number\nof positive tokens as the gold rationale. The first\nuser study is forward simulation (Hase and Bansal,\n2020; Jain et al., 2020). Here, the annotator is given\nan input and a rationale for some model’s predic-\ntion, then asked what (binary) sentiment label the\nmodel most likely predicted. For forward simu-\nlation, we also consider a No Rationale baseline,\nwhere no tokens are highlighted. For No Rationale\nand Gold, the target label is the correct choice. An-\nnotators are also asked to rate their confidence (4-\npoint Likert scale) in their answer to this question.\nThe second user study involves giving a subjective\nrating of how plausible the rationale is (Hase and\nBansal, 2020). Here, the annotator is given the\ninput, rationale, and model’s predicted label, then\nasked to rate (5-point Likert scale) how aligned the\nrationale is with the prediction.\nIn both forward simulation and subjective rat-\ning, we find that DLM-FP performs best among all\nnon-oracle methods and even beats Gold on accu-\nracy, further supporting that DLM-FP rationales are\nplausible. As expected, the fact that Gold does not\nachieve near-100% accuracy shows the discrepancy\nbetween evaluating plausibility based on the tar-\nget label (i.e., gold rationale similarity) and Ftask’s\npredicted label (forward simulation). Meanwhile,\nSGT+P and AA-FP, which had lower AUPRC/TF1\nin our automatic evaluation, also do worse in accu-\nracy/alignment. Also, users found SGT+P and AA-\nFP rationales harder to understand, as shown by\ntheir lower confidence scores. Meanwhile, A2R+P\nhad high AUPRC/TF1, but gets very low accu-\nracy/alignment because A2R+P’s predicted label\noften not the target label, leading to misalignment\nwith its gold-like rationale. A2R+P is a great ex-\nample of how automatic plausibility evaluation can\nbe misleading. For the accuracy, confidence, and\nalignment questions, we achieved Fleiss’ Kappa\n(Fleiss, 1971) inter-annotator agreement scores of\n0.2456 (fair), 0.1282 (slight), and, 0.1561 (slight),\nrespectively. This lack of agreement shows the\ndifficulty of measuring plausibility.\n5 Related Work\nFaithfulness Many prior works have tried to\nimprove the faithfulness of extractive rationales\nthrough the use of AAs (Bastings and Filippova,\n2020). Typically, this involves designing gradient-\nbased (Sundararajan et al., 2017; Denil et al.,\n2014; Lundberg and Lee, 2017; Li et al., 2015) or\nperturbation-based (Li et al., 2016; Poerner et al.,\n2018; Kádár et al., 2017) AAs. However, attribu-\ntion algorithms cannot be optimized and tend to\nbe compute-intensive (often requiring multiple LM\nforward/backward passes). Recently, Ismail et al.\n(2021) addressed the optimization issue by regu-\nlarizing the task model to yield faithful rationales\nvia the AA, while other works (Situ et al., 2021;\nSchwarzenberg et al., 2021) addressed the compute\ncost issue by training an LM (requiring only one\nforward pass) to mimic an AA’s behavior. Another\nline of work aims to produce faithful rationales\nby construction, via SPPs (Jain et al., 2020; Yu\net al., 2021; Paranjape et al., 2020; Bastings et al.,\n2019; Yu et al., 2019; Lei et al., 2016). Still, SPPs’\nfaithfulness can only guarantee sufficiency – not\ncomprehensiveness (DeYoung et al., 2019). Also,\nSPPs generally perform worse than vanilla LMs\nbecause they hide much of the original text input\nfrom the predictor and are hard to train end-to-end.\nPlausibility Existing approaches for improving\nextractive rationale plausibility typically involve su-\npervising LM-based extractors (Bhat et al., 2021)\nor SPPs (Jain et al., 2020; Paranjape et al., 2020;\nDeYoung et al., 2019) with gold rationales. How-\never, existing LM-based extractors have not been\ntrained for faithfulness, while SPPs’ faithfulness\nby construction comes at the great cost of task per-\nformance. Meanwhile, more existing works focus\non improving the plausibility of free-text rationales\n(Narang et al., 2020; Lakhotia et al., 2020; Cam-\nburu et al., 2018), often with task-specific pipelines\n(Rajani et al., 2019; Kumar and Talukdar, 2020).\nConnection to UNIREX Unlike prior works,\n59\nUNIREX enables both the task model and ratio-\nnale extractor to be jointly optimized for faithful-\nness, plausibility, and task performance. As a result,\nUNIREX -trained rationale extractors achieve a\nbetter balance of faithfulness and plausibility, with-\nout compromising the task model’s performance.\nAlso, by using a learned rationale extractor, which\ngenerally only requires one model forward pass,\nUNIREX does not have the computational ex-\npenses that limit many AAs.\nReferences\nJasmijn Bastings, Wilker Aziz, and Ivan Titov. 2019.\nInterpretable neural predictions with differentiable\nbinary variables. arXiv preprint arXiv:1905.08160.\nJasmijn Bastings and Katja Filippova. 2020. The ele-\nphant in the interpretability room: Why use atten-\ntion as explanation when we have saliency methods?\narXiv preprint arXiv:2010.05607.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big?. In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nMeghana Moorthy Bhat, Alessandro Sordoni, and Sub-\nhabrata Mukherjee. 2021. Self-training with few-shot\nrationalization: Teacher explanations aid student in\nfew-shot nlu. arXiv preprint arXiv:2109.08259.\nOana-Maria Camburu, Tim Rocktäschel, Thomas\nLukasiewicz, and Phil Blunsom. 2018. e-snli: Natu-\nral language inference with natural language expla-\nnations. arXiv preprint arXiv:1812.01193.\nSamuel Carton, Anirudh Rathore, and Chenhao Tan.\n2020. Evaluating and characterizing human ratio-\nnales. arXiv preprint arXiv:2010.04736.\nAaron Chan, Jiashu Xu, Boyuan Long, Soumya Sanyal,\nTanishq Gupta, and Xiang Ren. 2021. Salkg: Learn-\ning from knowledge graph explanations for common-\nsense reasoning. Advances in Neural Information\nProcessing Systems, 34.\nOna de Gibert, Naiara Perez, Aitor García-Pablos,\nand Montse Cuadros. 2018. Hate speech dataset\nfrom a white supremacy forum. arXiv preprint\narXiv:1809.04444.\nMisha Denil, Alban Demiraj, and Nando De Freitas.\n2014. Extraction of salient sentences from labelled\ndocuments. arXiv preprint arXiv:1412.6815.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani,\nEric Lehman, Caiming Xiong, Richard Socher, and\nByron C Wallace. 2019. Eraser: A benchmark to\nevaluate rationalized nlp models. arXiv preprint\narXiv:1911.03429.\nFinale Doshi-Velez and Been Kim. 2017. Towards a\nrigorous science of interpretable machine learning.\narXiv preprint arXiv:1702.08608.\nWilliam Falcon and The PyTorch Lightning team. 2019.\nPyTorch Lightning.\nJoseph L Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological bulletin,\n76(5):378.\nPeter Hase and Mohit Bansal. 2020. Evaluating ex-\nplainable ai: Which algorithmic explanations help\nusers predict model behavior? arXiv preprint\narXiv:2005.01831.\nSara Hooker, Dumitru Erhan, Pieter-Jan Kindermans,\nand Been Kim. 2018. A benchmark for interpretabil-\nity methods in deep neural networks. arXiv preprint\narXiv:1806.10758.\nAya Abdelsalam Ismail, Hector Corrada Bravo, and\nSoheil Feizi. 2021. Improving deep learning inter-\npretability by saliency guided training. Advances in\nNeural Information Processing Systems, 34.\nAlon Jacovi and Yoav Goldberg. 2020. Towards faith-\nfully interpretable nlp systems: How should we\ndefine and evaluate faithfulness? arXiv preprint\narXiv:2004.03685.\nSarthak Jain, Sarah Wiegreffe, Yuval Pinter, and\nByron C Wallace. 2020. Learning to faith-\nfully rationalize by construction. arXiv preprint\narXiv:2005.00115.\nAkos Kádár, Grzegorz Chrupała, and Afra Alishahi.\n2017. Representation of linguistic form and func-\ntion in recurrent neural networks. Computational\nLinguistics, 43(4):761–780.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 252–262.\nSawan Kumar and Partha Talukdar. 2020. Nile: Natu-\nral language inference with faithful natural language\nexplanations. arXiv preprint arXiv:2005.12116.\nKushal Lakhotia, Bhargavi Paranjape, Asish Ghoshal,\nWen-tau Yih, Yashar Mehdad, and Srinivasan Iyer.\n2020. Fid-ex: Improving sequence-to-sequence mod-\nels for extractive rationale generation. arXiv preprint\narXiv:2012.15482.\n60\nTao Lei, Regina Barzilay, and Tommi Jaakkola. 2016.\nRationalizing neural predictions. arXiv preprint\narXiv:1606.04155.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.\n2015. Visualizing and understanding neural models\nin nlp. arXiv preprint arXiv:1506.01066.\nJiwei Li, Will Monroe, and Dan Jurafsky. 2016. Un-\nderstanding neural networks through representation\nerasure. arXiv preprint arXiv:1612.08220.\nZachary C Lipton. 2018. The mythos of model inter-\npretability: In machine learning, the concept of in-\nterpretability is both important and slippery. Queue,\n16(3):31–57.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nScott M Lundberg and Su-In Lee. 2017. A unified ap-\nproach to interpreting model predictions. In Proceed-\nings of the 31st international conference on neural\ninformation processing systems, pages 4768–4777.\nSiwen Luo, Hamish Ivison, Caren Han, and Josiah Poon.\n2021. Local interpretations for explainable natu-\nral language processing: A survey. arXiv preprint\narXiv:2103.11072.\nJulian McAuley and Jure Leskovec. 2013. Hidden fac-\ntors and hidden topics: understanding rating dimen-\nsions with review text. In Proceedings of the 7th\nACM conference on Recommender systems, pages\n165–172.\nShervin Minaee, Nal Kalchbrenner, Erik Cambria, Nar-\njes Nikzad, Meysam Chenaghlu, and Jianfeng Gao.\n2021. Deep learning–based text classification: A\ncomprehensive review. ACM Computing Surveys\n(CSUR), 54(3):1–40.\nSharan Narang, Colin Raffel, Katherine Lee, Adam\nRoberts, Noah Fiedel, and Karishma Malkan. 2020.\nWt5?! training text-to-text models to explain their\npredictions. arXiv preprint arXiv:2004.14546.\nBhargavi Paranjape, Mandar Joshi, John Thickstun,\nHannaneh Hajishirzi, and Luke Zettlemoyer. 2020.\nAn information bottleneck approach for controlling\nconciseness in rationale extraction. arXiv preprint\narXiv:2005.00652.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. Advances\nin neural information processing systems, 32:8026–\n8037.\nNina Poerner, Benjamin Roth, and Hinrich Schütze.\n2018. Evaluating neural network explanation meth-\nods using hybrid documents and morphological\nagreement. arXiv preprint arXiv:1801.06422.\nDanish Pruthi, Bhuwan Dhingra, Livio Baldini Soares,\nMichael Collins, Zachary C Lipton, Graham Neubig,\nand William W Cohen. 2020. Evaluating explana-\ntions: How much do explanations from the teacher\naid students? arXiv preprint arXiv:2012.00893.\nNazneen Fatema Rajani, Bryan McCann, Caiming\nXiong, and Richard Socher. 2019. Explain your-\nself! leveraging language models for commonsense\nreasoning. arXiv preprint arXiv:1906.02361.\nCynthia Rudin. 2019. Stop explaining black box ma-\nchine learning models for high stakes decisions and\nuse interpretable models instead. Nature Machine\nIntelligence, 1(5):206–215.\nRobert Schwarzenberg, Nils Feldhus, and Sebastian\nMöller. 2021. Efficient explanations from empirical\nexplainers. arXiv preprint arXiv:2103.15429.\nAvanti Shrikumar, Peyton Greenside, and Anshul Kun-\ndaje. 2017. Learning important features through\npropagating activation differences. In International\nConference on Machine Learning, pages 3145–3153.\nPMLR.\nKaren Simonyan, Andrea Vedaldi, and Andrew Zis-\nserman. 2013. Deep inside convolutional networks:\nVisualising image classification models and saliency\nmaps. arXiv preprint arXiv:1312.6034.\nXuelin Situ, Ingrid Zukerman, Cecile Paris, Sameen\nMaruf, and Gholamreza Haffari. 2021. Learning\nto explain: Generating stable explanations fast. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 5340–\n5355.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empiri-\ncal methods in natural language processing, pages\n1631–1642.\nJulia Strout, Ye Zhang, and Raymond J Mooney. 2019.\nDo human rationales improve machine explanations?\narXiv preprint arXiv:1905.13714.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. In Inter-\nnational Conference on Machine Learning, pages\n3319–3328. PMLR.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2018. Commonsenseqa: A question\nanswering challenge targeting commonsense knowl-\nedge. arXiv preprint arXiv:1811.00937.\n61\nCynthia Van Hee, Els Lefever, and Véronique Hoste.\n2018. Semeval-2018 task 3: Irony detection in en-\nglish tweets. In Proceedings of The 12th Interna-\ntional Workshop on Semantic Evaluation, pages 39–\n50.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nQinyuan Ye, Bill Yuchen Lin, and Xiang Ren.\n2021. Crossfit: A few-shot learning challenge for\ncross-task generalization in nlp. arXiv preprint\narXiv:2104.08835.\nMo Yu, Shiyu Chang, Yang Zhang, and Tommi S\nJaakkola. 2019. Rethinking cooperative rationaliza-\ntion: Introspective extraction and complement con-\ntrol. arXiv preprint arXiv:1910.13294.\nMo Yu, Yang Zhang, Shiyu Chang, and Tommi Jaakkola.\n2021. Understanding interlocking dynamics of coop-\nerative rationalization. Advances in Neural Informa-\ntion Processing Systems, 34.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. In NeurIPS.\nOmar Zaidan and Jason Eisner. 2008. Modeling an-\nnotators: A generative approach to learning from\nannotator rationales. In Proceedings of the 2008 con-\nference on Empirical methods in natural language\nprocessing, pages 31–40.\nMarcos Zampieri, Shervin Malmasi, Preslav Nakov,\nSara Rosenthal, Noura Farra, and Ritesh Kumar.\n2019. Semeval-2019 task 6: Identifying and catego-\nrizing offensive language in social media (offenseval).\narXiv preprint arXiv:1903.08983.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level Convolutional Networks for Text\nClassification. arXiv:1509.01626 [cs].\nA Appendix\nA.1 Text Classification\nHere, we formalize the text classification prob-\nlem in more detail. Let D = {X,Y}N\ni=1 be a\ndataset, where X = {xi}N\ni=1 are the text inputs,\nY= {y∗\ni}N\ni=1 are the labels, and N is the number\nof instances (xi,y∗\ni) in D. We also assume Dcan\nbe partitioned into train set Dtrain, dev set Ddev, and\ntest set Dtest. Let Ftask = ftask(fenc(·)) be a task\nLM, where fenc is the text encoder, and ftask is the\ntask output head. Typically, Ftask has a BERT-style\narchitecture (Devlin et al., 2018), in which fenc is\na Transformer (Vaswani et al., 2017) while ftask\nis a linear layer. Below, we define the sequence\nclassification (SST, Movies, MultiRC, e-SNLI) and\nmulti-choice QA (CoS-E) tasks, which are different\ntypes of text classification.\nSequence Classification In sequence classifica-\ntion, xi is a token sequence ( e.g., a single sen-\ntence, a pair of sentences), while y∗\ni is the target\nclass for xi. Here, we assume a fixed label space\nY = {1,...,M }of size M, where y∗\ni ∈Y for all\ni. Thus, ftask outputs a vector of size M, such that\nFtask(xi) = ftask(fenc(xi)) = ˆyi ∈RM is the logit\nvector used to classify xi. Given ˆyi = [ˆyi,j]M\nj=1, let\nyi = arg maxj ˆyi,j be the class predicted by Ftask.\nThe goal of sequence classification is to learn Ftask\nsuch that y∗\ni = yi, for all (xi,y∗\ni) (Minaee et al.,\n2021).\nMulti-Choice QA Instead of a fixed label space,\nmulti-choice QA has a different (but fixed-size)\nset of answer choices per instance. For instance\ni, let qi be the question ( e.g., “A friend is greet-\ning me, what would they say?”) and Ai =\n{ai,j}M\nj=1 be the corresponding answer choices\n(e.g., {“say hello”, “greet”, “associate”, “social-\nize”, “smile”}), where M is now the number of\nanswer choices. Define xi,j = qi ⊕ai,j, where\n⊕denotes concatenation. In multi-choice QA, we\nhave xi = {xi,j}M\nj=1, while y∗\ni ∈Ai is the correct\nanswer for xi. Thus, ftask outputs a scalar, such\nthat Ftask(xi,j) = ftask(fenc(xi,j)) = ˆyi,j ∈ R\nis the logit for xi,j. Given ˆyi = [ˆyi,j]M\nj=1, let\nj′= arg maxj ˆyi,j, where yi = ai,j′ is the answer\npredicted by Ftask. The goal of multi-choice QA\nis to learn Ftask such that y∗\ni = yi, for all (xi,y∗\ni)\n(Talmor et al., 2018).\n62\nA.2 Heuristic Rationale Extractors\nA heuristic Ftask is an AA, which can be any hand-\ncrafted function that calculates an importance score\nst\ni for each input token xt\ni (Bastings and Filippova,\n2020). AAs are typically gradient-based (Sun-\ndararajan et al., 2017; Denil et al., 2014; Lundberg\nand Lee, 2017; Li et al., 2015) or perturbation-\nbased (Li et al., 2016; Poerner et al., 2018; Kádár\net al., 2017) methods. Gradient-based methods\ncompute st\ni via the gradient of Ftask’s output ˆyi\nw.r.t. xt\ni, via one or more Ftask backward passes.\nPerturbation-based methods measure st\ni as ˆyi’s\nchange when perturbing ( e.g., removing) xt\ni, via\nmultiple Ftask forward passes.\nAAs can be used out of the box without train-\ning and are designed to satisfy certain faithfulness-\nrelated axiomatic properties (Sundararajan et al.,\n2017; Lundberg and Lee, 2017). However, AAs’\nlack of learnable parameters means they cannot\nbe optimized for faithfulness/plausibility. Thus, if\nFtask is trained for explainability using AA-based\nrationales, then only Ftask is optimized. Also, faith-\nful AAs tend to be compute-intensive, requiring\nmany Ftask backward/forward passes per instance\n(Sundararajan et al., 2017; Lundberg and Lee, 2017;\nLi et al., 2016).\nA.3 Gold Rationale Supervision\nIf a learned rationale extractor is chosen,UNIREX\nenables users to specify how much gold rationale\nsupervision to use. Ideally, each train instance\nwould be annotated with a gold rationale. In this\ncase, we could directly minimize the plausibility\nloss for each train instance. However, since gold\nrationales can be expensive to annotate, UNIREX\nprovides a special batching procedure for training\nwith limited gold rationale supervision.\nGiven Ntrain = |Dtrain|train instances, let 0 <\nγ <100 be the percentage of train instances with\ngold rationales, Ngold = ⌈γ\n100 Ntrain⌉≥ 1 be the\nnumber of train instances with gold rationales, bbe\nthe desired train batch size, and β >1 be a scaling\nfactor. Define Dgold ⊆ Dtrain as the set of train\ninstances with gold rationales, where |Dgold| =\nNgold. Note that, if all train instances have gold\nrationales, then Dgold = Dtrain and γ = 100.\nEach batch is constructed as follows: (1) ran-\ndomly sample bgold = max(1, b\nβ) instances from\nDgold without replacement, then (2) randomly sam-\nple b−bgold instances from Dtrain\\Dgold without\nreplacement. This results in a batch with btotal\ntrain instances, bgold with gold rationales and the\nrest without. Since Ngold is generally small, we\nonly sample from Dgold without replacement for a\ngiven batch, but not a given epoch. Thus, instances\nfrom Dgold may appear more than once in the same\nepoch. However, we do sample from Dtrain\\Dgold\nwithout replacement for each batch and epoch, so\nevery instance in Dtrain\\Dgold appears exactly once\nper epoch.\nAfter constructing the batch, we compute\nthe plausibility loss for the batch as fol-\nlows: ∑b\ni=1 1 (xi,y∗\ni )∈Dgold Lplaus(Fext(xi),r∗\ni),\nwhere Lplaus is the plausibility loss for train in-\nstance (xi,y∗\ni). This function zeroes out the plau-\nsibility loss for instances without gold rationales,\nso that plausibility is only being optimized with\nrespect to instances with gold rationales. However,\nin Sec. ??, we show that it is possible to achieve\nhigh plausibility via rationale extractors trained on\nminimal gold rationale supervision.\nA.4 Explainability Objectives\nA.4.1 Faithfulness\nSufficiency In addition, to the criteria presented\nin Sec. 3.2, we consider two other sufficiency loss\nfunctions. The first is the KL divergence criterion\nused in (Ismail et al., 2021), which considers the\nentire label distribution and is defined asLsuff-KL =\nKL(Ftask(r(k)\ni )) ||Ftask(xi)). The second is the\nmean absolute error (MAE) criterion, which is\ndefined as Lsuff-MAE = |LCE(Ftask(r(k)\ni )),y∗\ni) −\nLCE(Ftask(xi),y∗\ni)|. Unlike the difference criterion\nLsuff-diff and margin criterion Lsuff-margin (Sec. 3.2),\nthe MAE criterion assumes that using r(k)\ni as input\nshould not yield better task performance than us-\ning xi as input. In our experiments, we find that\nLsuff-margin is effective, though others (e.g., KL di-\nvergence, MAE) can be used too.\nA.4.2 Plausibility\nSimilar to faithfulness, UNIREX places no re-\nstrictions on the choice of plausibility objective.\nAs described in Sec. 3.2, given gold rationale r∗\ni\nfor input xi, plausibility optimization entails train-\ning Fext to predict binary importance label r∗,t\ni for\neach token xt\ni. This is essentially binary token\nclassification, so one natural choice for Lplaus is\nthe token-level binary cross-entropy (BCE) crite-\nrion: Lplaus-BCE = −∑\ntr∗,t\ni log(Fext(xt\ni)) (Sec.\n3.2). Another option is the sequence-level KL di-\nvergence criterion, which is defined as:Lplaus-KL =\n63\nKL(Fext(xi) ||r∗\ni).\nAdditionally, we can directly penalize Fext(xi)\nin the logit space via a linear loss, defined as:\nLplaus-linear = Φ( r∗\ni) Fext(xi), where Φ(u) =\n−2u + 1 maps positive and negative tokens to\n−1 and +1, respectively. The linear loss directly\npushes the logits corresponding to positive/negative\ntokens to be higher/lower and increase the mar-\ngin between them. To prevent linear loss values\nfrom becoming arbitrarily negative, we can also\nlower bound the loss with a margin mp, yielding:\nLplaus-linear-margin = max(−mp,Lplaus-linear) + mp.\nA.5 Implementation Details\nLM Architecture While many prior works use\nBERT (Devlin et al., 2018) Transformer LMs,\nBERT is limited to having sequences with up\nto 512 tokens, which is problematic since many\ndatasets ( e.g., Movies) contain much longer se-\nquences. Meanwhile, BigBird (Zaheer et al., 2020)\nis a state-of-the-art Transformer LM designed to\nhandle long input sequences with up to 4096 tokens.\nThus, we use BigBird-Base, which is initialized\nwith RoBERTa-Base (Liu et al., 2019), in all of our\nexperiments (i.e., both baselines and UNIREX ).\nWe obtain the pre-trained BigBird-Base model\nfrom the Hugging Face Transformers library (Wolf\net al., 2019). Note that UNIREX is agnostic to\nthe choice of LM architecture, so RNNs, CNNs,\nand other Transformer LMs are also supported by\nUNIREX . However, we leave exploration of other\nLM architectures for future work.\nTraining Building upon Sec. ??, we discuss ad-\nditional training details here. We find thatαc = 0.5\nand αs = 0.5 are usually best. For the batching\nfactor β (Sec. A.3), we use 2. For model selec-\ntion, we choose the model with the best dev per-\nformance averaged over three seeds. We can also\nperform model selection based on dev explainabil-\nity metrics, but we leave this extended tuning for\nfuture work. All experiments are implemented us-\ning PyTorch-Lightning (Paszke et al., 2019; Falcon\nand The PyTorch Lightning team, 2019).\nA.6 Gold Rationale Data Efficiency\nFig. ?? shows the gold rationale data efficiency\nresults for CoS-E, using the same setup as Sec.\n??. Overall, we see that the CoS-E results are quite\nsimilar to the SST results. Again, UNIREX (DLM-\nFP) and UNIREX (SLM-FP) dominate across all\nγvalues, with AUPRC slowly decreasing as γde-\ncreases. Interestingly, UNIREX (AA-FP) yields a\nnoticeable dip in AUPRC for lowerγvalues. Since\nAA-FP has limited capacity (via the task model)\nfor plausibility optimization, it is possible that this\nfluctuation is due to random noise. We leave further\nanalysis of this for future work.\nFigure 8: Gold Rationale Data Efficiency on CoS-E.\nA.7 Additional Empirical Results\nIn this subsection, we present additional results\nfrom our experiments. Besides the aggregated re-\nsults shown in Sec. 4 of the main text, Tables 4-10\ncontain more detailed results, using both raw and\nNRG metrics. Specifically, Tables 4-8 show all\nraw/NRG results for each dataset, Table 9 shows\nthe ablation results for all raw metrics, and Table 10\nincludes the zero-shot explainability transfer results\nfor UNIREX (SLM-FP). Generally, the computa-\ntion of NRG should involve globally aggregating\nthe raw metrics for all available methods, as done\nin the main results. However, for a number of more\nfocused experiments (Tables 9-10), only a subset\nof the available methods are considered. Thus, to\nmake the faithfulness results in Tables 9-10 easier\nto digest, we introduce a metric called Comp-Suff\nDifference (CSD), which locally aggregates comp\nand suff as: CSD = comp −suff. Therefore, since\nhigher/lower comp/suff signal higher faithfulness,\nthen higher CSD signals higher faithfulness.\n64\nMethod Composite Faithfulness Plausibility Performance\nNRG (↑) NRG (↑) Comp (↑) Suff ( ↓) NRG (↑) AUPRC (↑) TF1 ( ↑) NRG (↑) Acc ( ↑)\nAA (Grad) 0.488 0.337 0.142 (±0.010) 0.256 (±0.006) 0.192 58.86 (±3.65) 27.40 (±0.00) 0.935 93.81 (±0.55)AA (Input*Grad) 0.420 0.107 0.078 (±0.013) 0.342 (±0.014) 0.218 44.16 (±1.43) 45.02 (±0.39) 0.935 93.81 (±0.55)AA (DeepLIFT) 0.453 0.122 0.085 (±0.006) 0.340 (±0.018) 0.302 46.50 (±1.32) 50.18 (±0.32) 0.935 93.81 (±0.55)AA (IG) 0.526 0.297 0.119 ( ±0.009) 0.258 (±0.031) 0.347 49.94 (±1.77) 50.75 (±0.54) 0.935 93.81 (±0.55)L2E 0.557 0.487 0.012 ( ±0.004) 0.009 (±0.024) 0.250 44.84 (±0.32) 47.24 (±0.87) 0.935 93.81 (±0.55)SGT 0.632 0.555 0.147 ( ±0.024) 0.113 (±0.031) 0.371 51.38 (±2.47) 51.35 (±1.64) 0.971 94.40 (±0.57)FRESH 0.330 0.837 0.219 ( ±0.057) 0.000 (±0.000) 0.152 42.06 (±8.84) 41.19 (±4.01) 0.000 78.78 (±6.48)A2R 0.479 0.941 0.283 ( ±0.104) 0.000 (±0.000) 0.457 63.36 (±6.01) 46.74 (±6.65) 0.038 79.39 (±11.67)UNIREX (AA-F) 0.639 0.706 0.292 (±0.051) 0.171 (±0.038) 0.329 48.13 (±1.14) 50.96 (±0.93) 0.882 92.97 (±0.44)\nSGT+P 0.596 0.507 0.139 ( ±0.032) 0.137 (±0.026) 0.355 50.38 (±1.45) 50.98 (±0.46) 0.928 93.70 (±0.88)FRESH+P 0.426 0.765 0.175 (±0.043) 0.000 (±0.000) 0.503 60.87 (±9.83) 53.55 (±8.27) 0.011 78.95 (±5.18)A2R+P 0.695 0.953 0.290 ( ±0.016) 0.000 (±0.000) 0.978 85.56 (±1.01) 70.97 (±1.03) 0.154 81.26 (±0.52)UNIREX (DLM-P) 0.770 0.339 0.142 (±0.008) 0.255 (±0.007) 0.970 84.35 (±0.87) 71.54 (±0.53) 1.000 94.86 (±0.41)UNIREX (AA-FP) 0.636 0.339 0.296 (±0.067) 0.185 (±0.048) 0.315 47.60 (±2.44) 50.23 (±2.26) 0.900 93.25 (±0.45)UNIREX (DLM-FP) 0.897 0.756 0.319 (±0.090) 0.167 (±0.036) 1.000 85.80 (±0.74) 72.76 (±0.19) 0.935 93.81 (±0.54)UNIREX (SLM-FP) 0.891 0.807 0.302 (±0.039) 0.113 (±0.013) 0.940 82.55 (±0.84) 70.65 (±0.44) 0.927 93.68 (±0.67)\nTable 4: Main Results on SST.\nMethod Composite Faithfulness Plausibility Performance\nNRG (↑) NRG (↑) Comp (↑) Suff ( ↓) NRG (↑) AUPRC (↑) TF1 ( ↑) NRG ( ↑) F1 ( ↑)\nAA (Grad) 0.481 0.457 0.184 (±0.023) 0.107 (±0.017) 0.028 13.31 (±0.91) 5.02 (±0.00) 0.957 95.33 (±0.65)AA (Input*Grad) 0.503 0.359 0.148 (±0.031) 0.137 (±0.019) 0.194 8.68 (±0.37) 37.58 (±0.55) 0.957 95.33 (±0.65)AA (DeepLIFT) 0.468 0.259 0.122 (±0.029) 0.172 (±0.022) 0.187 9.00 (±0.16) 36.15 (±1.45) 0.957 95.33 (±0.65)AA (IG) 0.439 0.173 0.134 ( ±0.016) 0.219 (±0.044) 0.188 8.88 (±0.21) 36.39 (±1.29) 0.957 95.33 (±0.65)L2E 0.550 0.445 0.000 ( ±0.007) 0.026 (±0.015) 0.248 16.68 (±10.20) 38.92 (±4.07) 0.957 95.33 (±0.65)SGT 0.553 0.474 0.124 ( ±0.053) 0.071 (±0.064) 0.184 10.05 (±1.23) 34.64 (±1.67) 1.000 96.33 (±0.76)FRESH 0.645 0.732 0.234 ( ±0.034) 0.000 (±0.000) 0.305 17.02 (±6.22) 48.26 (±5.87) 0.899 94.00 (±1.44)A2R 0.431 0.764 0.267 ( ±0.050) 0.000 (±0.000) 0.244 35.44 (±21.69) 19.78 (±25.56) 0.284 79.78 (±7.14)UNIREX (AA-F) 0.601 0.744 0.505 (±0.134) 0.122 (±0.100) 0.189 9.14 (±2.51) 36.28 (±1.84) 0.870 93.33 (±1.61)\nSGT+P 0.586 0.604 0.152 ( ±0.013) 0.022 (±0.004) 0.183 9.16 (±1.59) 35.33 (±0.41) 0.971 95.66 (±1.16)FRESH+P 0.491 0.691 0.193 (±0.062) 0.000 (±0.000) 0.710 65.78 (±11.16) 68.70 (±15.78) 0.070 74.84 (±12.22)A2R+P 0.585 0.764 0.267 ( ±0.076) 0.000 (±0.000) 0.991 93.53 (±0.93) 88.77 (±1.22) 0.000 73.22 (±0.75)UNIREX (DLM-P) 0.667 0.024 0.024 (±0.003) 0.238 (±0.004) 1.000 94.32 (±0.12) 89.53 (±1.63) 0.978 95.83 (±0.29)UNIREX (AA-FP) 0.543 0.514 0.428 (±0.174) 0.195 (±0.105) 0.193 8.53 (±0.46) 37.71 (±3.12) 0.921 94.50 (±1.00)UNIREX (DLM-FP) 0.744 0.326 0.283 (±0.217) 0.216 (±0.005) 0.991 93.65 (±0.36) 88.68 (±2.29) 0.913 94.33 (±1.61)UNIREX (SLM-FP) 0.754 0.362 0.313 (±0.059) 0.213 (±0.014) 0.965 91.70 (±1.84) 86.17 (±1.20) 0.935 94.83 (±0.76)\nTable 5: Main Results on Movies.\nMethod Composite Faithfulness Plausibility Performance\nNRG (↑) NRG (↑) Comp (↑) Suff ( ↓) NRG (↑) AUPRC (↑) TF1 ( ↑) NRG (↑) Acc ( ↑)\nAA (Grad) 0.537 0.504 0.331 (±0.012) 0.352 (±0.007) 0.130 37.33 (±0.62) 22.65 (±0.00) 0.977 63.56 (±1.27)AA (Input*Grad) 0.573 0.361 0.249 (±0.018) 0.385 (±0.008) 0.383 39.56 (±0.54) 44.43 (±0.40) 0.977 63.56 (±1.27)AA (DeepLIFT) 0.605 0.346 0.254 (±0.035) 0.403 (±0.042) 0.491 42.82 (±1.83) 51.72 (±1.26) 0.977 63.56 (±1.27)AA (IG) 0.578 0.327 0.216 ( ±0.007) 0.378 (±0.010) 0.429 40.07 (±5.47) 48.34 (±3.16) 0.977 63.56 (±1.27)L2E 0.544 0.493 0.005 ( ±0.003) 0.010 (±0.008) 0.161 23.56 (±1.09) 37.80 (±1.10) 0.977 63.56 (±1.27)SGT 0.618 0.367 0.197 ( ±0.040) 0.324 (±0.015) 0.491 43.68 (±4.68) 51.00 (±3.05) 0.995 64.35 (±0.46)FRESH 0.302 0.546 0.037 ( ±0.036) 0.000 (±0.000) 0.261 32.35 (±7.66) 39.37 (±0.70) 0.101 24.81 (±3.46)A2R 0.277 0.516 0.014 ( ±0.021) 0.000 (±0.000) 0.282 41.61 (±3.85) 33.12 (±9.06) 0.032 21.77 (±1.31)UNIREX (AA-F) 0.690 0.538 0.297 (±0.141) 0.286 (±0.084) 0.554 46.97 (±3.41) 53.99 (±1.66) 0.978 63.58 (±0.61)\nSGT+P 0.601 0.367 0.201 ( ±0.032) 0.328 (±0.022) 0.436 41.30 (±6.70) 47.95 (±1.65) 1.000 64.57 (±0.33)FRESH+P 0.374 0.515 0.013 (±0.021) 0.013 (±0.021) 0.606 53.40 (±12.87) 53.17 (±7.83) 0.000 20.36 (±0.66)A2R+P 0.488 0.500 0.001 ( ±0.001) 0.000 (±0.000) 0.951 73.59 (±0.81) 67.63 (±1.54) 0.012 20.91 (±0.48)UNIREX (DLM-P) 0.751 0.267 0.180 (±0.016) 0.390 (±0.035) 0.997 76.07 (±1.63) 69.76 (±0.27) 0.990 64.13 (±0.46)UNIREX (AA-FP) 0.685 0.551 0.395 (±0.109) 0.381 (±0.101) 0.537 45.21 (±4.46) 53.91 (±3.23) 0.968 63.14 (±0.33)UNIREX (DLM-FP) 0.814 0.492 0.293 (±0.043) 0.321 (±0.070) 0.997 76.38 (±0.57) 69.52 (±0.24) 0.953 62.50 (±1.34)UNIREX (SLM-FP) 0.807 0.494 0.390 (±0.087) 0.424 (±0.110) 0.983 75.12 (±0.41) 69.25 (±0.41) 0.944 62.09 (±2.12)\nTable 6: Main Results on CoS-E.\n65\nMethod Composite Faithfulness Plausibility Performance\nNRG (↑) NRG (↑) Comp (↑) Suff ( ↓) NRG ( ↑) AUPRC (↑) TF1 ( ↑) NRG (↑) F1 ( ↑)\nAA (Grad) 0.498 0.462 0.222 (±0.028) 0.120 (±0.018) 0.035 22.27 (±0.17) 13.81 (±0.00) 0.997 69.80 (±0.60)AA (Input*Grad) 0.506 0.289 0.225 (±0.048) 0.260 (±0.059) 0.231 18.51 (±0.23) 43.45 (±0.05) 0.997 69.80 (±0.60)AA (DeepLIFT) 0.493 0.249 0.225 (±0.012) 0.292 (±0.014) 0.234 18.80 (±0.19) 43.51 (±0.04) 0.997 69.80 (±0.60)AA (IG) 0.499 0.280 0.162 ( ±0.086) 0.222 (±0.086) 0.220 18.71 (±0.40) 41.79 (±1.33) 0.997 69.80 (±0.60)L2E 0.522 0.366 0.007 ( ±0.006) 0.042 (±0.024) 0.205 24.48 (±2.71) 32.63 (±6.12) 0.997 69.80 (±0.60)SGT 0.594 0.564 0.214 ( ±0.105) 0.033 (±0.077) 0.224 18.60 (±0.42) 42.42 (±0.51) 0.995 69.73 (±0.13)FRESH 0.675 0.571 0.176 ( ±0.029) 0.000 (±0.000) 0.617 24.68 (±7.98) 48.02 (±3.04) 0.838 64.47 (±3.41)A2R 0.217 0.404 -0.010 ( ±0.029) 0.000 (±0.000) 0.249 18.72 (±0.67) 45.45 (±0.02) 0.000 36.39 (±0.00)UNIREX (AA-F) 0.711 0.956 0.505 (±0.050) -0.071 (±0.020) 0.236 18.82 (±0.40) 43.68 (±0.38) 0.939 66.17 (±4.58)\nSGT+P 0.630 0.665 0.280 ( ±0.029) 0.283 (±0.039) 0.226 18.63 (±0.52) 42.71 (±0.39) 1.000 69.91 (±0.81)FRESH+P 0.404 0.413 0.000 ( ±0.013) 0.000 (±0.000) 0.739 55.87 (±10.13) 63.70 (±9.58) 0.060 38.41 (±5.34)A2R+P 0.516 0.422 0.011 ( ±0.024) 0.000 (±0.000) 0.977 70.86 (±1.30) 76.21 (±1.68) 0.150 41.42 (±8.73)UNIREX (DLM-P) 0.708 0.123 0.127 (±0.010) 0.322 (±0.017) 0.999 71.80 (±0.27) 77.94 (±0.57) 1.000 69.91 (±0.76)UNIREX (AA-FP) 0.706 1.000 0.545 (±0.045) -0.077 (±0.099) 0.231 19.13 (±0.71) 42.66 (±1.18) 0.888 66.17 (±4.58)UNIREX (DLM-FP) 0.751 0.327 0.135 (±0.072) 0.165 (±0.029) 0.998 71.89 (±0.41) 77.63 (±0.62) 0.929 67.53 (±1.06)UNIREX (SLM-FP) 0.784 0.377 0.198 (±0.038) 0.171 (±0.027) 0.997 71.69 (±0.21) 77.79 (±0.09) 0.979 69.20 (±1.58)\nTable 7: Main Results on MultiRC.\nMethod Composite Faithfulness Plausibility Performance\nNRG (↑) NRG (↑) Comp (↑) Suff ( ↓) NRG (↑) AUPRC (↑) TF1 ( ↑) NRG ( ↑) F1 ( ↑)\nAA (Grad) 0.587 0.518 0.313 (±0.009) 0.380 (±0.025) 0.244 59.80 (±1.32) 15.27 (±0.00) 0.999 90.78 (±0.27)AA (Input*Grad) 0.503 0.287 0.205 (±0.005) 0.446 (±0.020) 0.223 32.98 (±1.37) 43.13 (±0.86) 0.999 90.78 (±0.27)AA (DeepLIFT) 0.508 0.270 0.195 (±0.012) 0.448 (±0.014) 0.254 33.47 (±1.31) 46.44 (±0.04) 0.999 90.78 (±0.27)AA (IG) 0.596 0.473 0.308 ( ±0.011) 0.414 (±0.020) 0.317 47.83 (±1.04) 37.87 (±1.39) 0.999 90.78 (±0.27)L2E 0.606 0.460 0.009 ( ±0.015) 0.036 (±0.022) 0.358 58.11 (±0.97) 31.35 (±0.27) 0.999 90.78 (±0.27)SGT 0.595 0.503 0.288 ( ±0.025) 0.361 (±0.038) 0.298 42.46 (±3.03) 41.70 (±1.78) 0.985 90.23 (±0.16)FRESH 0.518 0.661 0.120 ( ±0.075) 0.000 (±0.000) 0.361 38.77 (±6.82) 53.71 (±3.30) 0.530 72.92 (±8.71)A2R 0.273 0.564 0.053 ( ±0.048) 0.000 (±0.000) 0.256 48.48 (±11.14) 29.54 (±24.72) 0.000 52.72 (±14.08)UNIREX (AA-F) 0.622 0.539 0.330 (±0.018) 0.383 (±0.055) 0.340 45.29 (±3.02) 43.69 (±1.98) 0.987 90.31 (±0.19)\nSGT+P 0.608 0.524 0.286 ( ±0.034) 0.339 (±0.032) 0.311 43.03 (±1.69) 42.59 (±1.63) 0.988 90.36 (±0.08)FRESH+P 0.614 0.695 0.143 (±0.072) 0.000 (±0.000) 0.603 56.21 (±10.47) 64.09 (±5.59) 0.544 73.44 (±12.88)A2R+P 0.800 0.751 0.182 ( ±0.097) 0.000 (±0.000) 0.992 87.30 (±0.44) 77.31 (±0.72) 0.656 77.31 (±0.72)UNIREX (DLM-P) 0.842 0.525 0.311 (±0.011) 0.371 (±0.032) 1.000 87.85 (±0.13) 77.63 (±0.35) 1.000 90.80 (±0.33)UNIREX (AA-FP) 0.626 0.529 0.341 (±0.008) 0.406 (±0.046) 0.363 44.79 (±0.81) 47.18 (±0.83) 0.985 90.21 (±0.08)UNIREX (DLM-FP) 0.857 0.588 0.335 (±0.018) 0.346 (±0.023) 0.991 86.99 (±0.40) 77.53 (±0.15) 0.992 90.51 (±0.12)UNIREX (SLM-FP) 0.864 0.603 0.353 (±0.017) 0.356 (±0.015) 0.994 87.58 (±0.14) 77.22 (±0.28) 0.994 90.59 (±0.09)\nTable 8: Main Results on e-SNLI.\nAblation Method Performance Faithfulness Plausibility\nAcc (↑) CSD ( ↑) Comp ( ↑) Suff ( ↓) AUPRC (↑) TF1 ( ↑)\nExt Type (F)\nUNIREX (AA-F, Rand) 94.05 (±0.35) -0.156 (±-0.156) 0.171 (±0.040) 0.327 (±0.050) 44.92 (±0.00) 46.15 (±0.00)UNIREX (AA-F, Gold) 93.81 (±0.54) -0.017 (±0.070) 0.232 (±0.088) 0.249 (±0.021) 100.00 (±0.00) 100.00 (±0.00)UNIREX (AA-F, Inv) 93.47 (±1.81) -0.115 (±0.018) 0.242 (±0.010) 0.357 (±0.019) 20.49 (±0.00) 0.00 (±0.00)UNIREX (AA-F, IG) 93.81 (±0.55) -0.138 (±0.040) 0.119 (±0.009) 0.258 (±0.031) 49.94 (±1.77) 50.75 (±0.54)\nExt Type (FP)\nUNIREX (AA-FP, Sum) 93.81 (±0.55) -0.138 (±0.040) 0.119 (±0.009) 0.258 (±0.031) 49.94 (±1.77) 50.75 (±0.54)UNIREX (AA-FP, MLP) 93.23 (±0.92) 0.087 (±0.134) 0.285 (±0.051) 0.197 (±0.100) 54.82 (±1.97) 49.62 (±0.65)UNIREX (DLM-FP) 93.81 (±0.18) 0.151 (±0.056) 0.319 (±0.090) 0.167 (±0.036) 85.80 (±0.74) 72.76 (±0.19)UNIREX (SLM-FP) 93.68 (±0.67) 0.189 (±0.030) 0.302 (±0.039) 0.113 (±0.013) 82.55 (±0.84) 70.65 (±0.44)\nComp/Suff LossUNIREX (SLM-FP, Comp) 93.59 (±0.11) 0.040 (±0.096) 0.350 (±0.048) 0.310 (±0.049) 82.79 (±0.62) 70.74 (±0.81)UNIREX (SLM-FP, Suff) 94.16 (±0.39) 0.014 (±0.010) 0.166 (±0.003) 0.152 (±0.012) 83.74 (±0.84) 70.94 (±0.86)UNIREX (SLM-FP, Comp+Suff) 93.68 (±0.67) 0.189 (±0.030) 0.302 (±0.039) 0.113 (±0.013) 82.55 (±0.84) 70.65 (±0.44)\nSuff CriterionUNIREX (SLM-FP, KL Div) 93.06 (±0.25) 0.174 (±0.100) 0.306 (±0.098) 0.131 (±0.005) 82.62 (±0.88) 70.43 (±0.65)UNIREX (SLM-FP, MAE) 93.78 (±0.13) 0.135 (±0.053) 0.278 (±0.058) 0.143 (±0.008) 82.66 (±0.61) 70.25 (±0.45)UNIREX (SLM-FP, Margin) 93.68 (±0.67) 0.189 (±0.030) 0.302 (±0.039) 0.113 (±0.013) 82.55 (±0.84) 70.65 (±0.44)\nSLM Ext HeadUNIREX (SLM-FP, Linear) 93.68 (±0.67) 0.189 (±0.030) 0.302 (±0.039) 0.113 (±0.013) 82.55 (±0.84) 70.65 (±0.44)UNIREX (SLM-FP, MLP-2048-2) 93.67 (±0.18) 0.179 (±0.060) 0.323 (±0.071) 0.144 (±0.012) 83.82 (±0.77) 70.93 (±0.87)UNIREX (SLM-FP, MLP-4096-3) 93.19 (±0.79) 0.141 (±0.030) 0.295 (±0.057) 0.154 (±0.027) 84.53 (±0.61) 71.41 (±0.91)\nTable 9: UNIREX Ablation Studies on SST.\n66\nTask Dataset Method Performance Faithfulness\nPerf (↑) CSD ( ↑) Comp ( ↑) Suff ( ↓)\nSentiment Analysis\nSST\nVanilla 93.81 (±0.74) -0.070 (±0.061) 0.145 (±0.023) 0.215 (±0.038)UNIREX (AA-F) 93.19 (±0.40) 0.360 (±0.055) 0.405 (±0.031) 0.045 (±0.024)UNIREX (DLM-FP) 93.81 (±0.18) 0.151 (±0.056) 0.319 (±0.090) 0.167 (±0.036)UNIREX (SLM-FP) 93.68 (±0.67) 0.189 (±0.030) 0.302 (±0.039) 0.113 (±0.013)\nYelp\nVanilla 92.50 (±2.07) -0.156 (±0.028) 0.067 (±0.004) 0.222 (±0.031)UNIREX (AA-F) 90.75 (±1.30) -0.138 (±0.120) 0.096 (±0.026) 0.233 (±0.096)UNIREX (DLM-FP) 92.37 (±0.46) 0.169 (±0.060) 0.265 (±0.094) 0.097 (±0.033)UNIREX (SLM-FP) 86.60 (±1.57) 0.114 (±0.056) 0.175 (±0.055) 0.060 (±0.001)\nAmazon\nVanilla 91.13 (±0.28) -0.120 (±0.038) 0.096 (±0.008) 0.217 (±0.033)UNIREX (AA-F) 86.60 (±0.95) -0.111 (±0.161) 0.100 (±0.042) 0.210 (±0.122)UNIREX (DLM-FP) 89.35 (±2.22) 0.133 (±0.039) 0.232 (±0.072) 0.098 (±0.033)UNIREX (SLM-FP) 81.82 (±7.62) 0.097 (±0.027) 0.147 (±0.012) 0.050 (±0.017)\nHate Speech Detection Stormfront\nVanilla 10.48 (±1.66) -0.066 (±0.072) 0.153 (±0.002) 0.219 (±0.071)UNIREX (AA-F) 9.43 (±1.45) 0.329 (±0.104) 0.337 (±0.073) 0.008 (±0.031)UNIREX (DLM-FP) 10.37 (±2.66) 0.052 (±0.027) 0.167 (±0.084) 0.115 (±0.059)UNIREX (SLM-FP) 4.51 (±1.87) 0.049 (±0.041) 0.110 (±0.039) 0.062 (±0.043)\nOffensive Speech Detection OffenseEval\nVanilla 33.51 (±0.99) -0.125 (±0.068) 0.104 (±0.007) 0.229 (±0.064)UNIREX (AA-F) 35.69 (±2.30) -0.028 (±0.084) 0.076 (±0.008) 0.104 (±0.076)UNIREX (DLM-FP) 35.52 (±1.26) 0.053 (±0.012) 0.140 (±0.049) 0.087 (±0.045)UNIREX (SLM-FP) 38.17 (±0.96) 0.039 (±0.031) 0.087 (±0.016) 0.048 (±0.024)\nIrony Detection SemEval2018-Irony\nVanilla 29.63 (±4.72) -0.058 (±0.075) 0.154 (±0.001) 0.212 (±0.074)UNIREX (AA-F) 47.99 (±6.33) 0.026 (±0.080) 0.087 (±0.022) 0.061 (±0.071)UNIREX (DLM-FP) 31.97 (±2.80) 0.047 (±0.017) 0.149 (±0.052) 0.102 (±0.053)UNIREX (SLM-FP) 17.42 (±4.04) 0.027 (±0.047) 0.091 (±0.027) 0.064 (±0.033)\nTable 10: Zero-Shot Explainability Transfer from SST to Unseen Datasets/Tasks.\n67",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.619055986404419
    },
    {
      "name": "Language model",
      "score": 0.5540930032730103
    },
    {
      "name": "Cognitive science",
      "score": 0.4173322319984436
    },
    {
      "name": "Artificial intelligence",
      "score": 0.395435094833374
    },
    {
      "name": "Natural language processing",
      "score": 0.374269962310791
    },
    {
      "name": "Programming language",
      "score": 0.32348114252090454
    },
    {
      "name": "Psychology",
      "score": 0.17946526408195496
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2800817003",
      "name": "Southern California University for Professional Studies",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    }
  ]
}