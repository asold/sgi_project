{
  "title": "Counterfactual Language Model Adaptation for Suggesting Phrases",
  "url": "https://openalex.org/W2761670104",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Arnold, Kenneth C.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4213521916",
      "name": "Chang, Kai-Wei",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Kalai, Adam T.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2059857707",
    "https://openalex.org/W1515760596",
    "https://openalex.org/W1518951372",
    "https://openalex.org/W1934019294",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2550821151",
    "https://openalex.org/W2165027873",
    "https://openalex.org/W2086206379",
    "https://openalex.org/W2251567929",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2539241874",
    "https://openalex.org/W2030960598"
  ],
  "abstract": "Mobile devices use language models to suggest words and phrases for use in text entry. Traditional language models are based on contextual word frequency in a static corpus of text. However, certain types of phrases, when offered to writers as suggestions, may be systematically chosen more often than their frequency would predict. In this paper, we propose the task of generating suggestions that writers accept, a related but distinct task to making accurate predictions. Although this task is fundamentally interactive, we propose a counterfactual setting that permits offline training and evaluation. We find that even a simple language model can capture text characteristics that improve acceptability.",
  "full_text": "Counterfactual Language Model Adaptation for Suggesting Phrases\nKenneth C. Arnold\nHarvard CS\nCambridge, MA\nKai-Wei Chang\nUniversity of California\nLos Angeles, CA\nAdam T. Kalai\nMicrosoft Research\nCambridge, MA\nAbstract\nMobile devices use language models to sug-\ngest words and phrases for use in text en-\ntry. Traditional language models are based\non contextual word frequency in a static\ncorpus of text. However, certain types of\nphrases, when offered to writers as sugges-\ntions, may be systematically chosen more\noften than their frequency would predict.\nIn this paper, we propose the task of gen-\nerating suggestions that writers accept, a\nrelated but distinct task to making accurate\npredictions. Although this task is funda-\nmentally interactive, we propose a counter-\nfactual setting that permits ofﬂine training\nand evaluation. We ﬁnd that even a simple\nlanguage model can capture text character-\nistics that improve acceptability.\n1 Introduction\nIntelligent systems help us write by proactively\nsuggesting words or phrases while we type. These\nsystems often build on a language model that picks\nmost likely phrases based on previous words in con-\ntext, in an attempt to increase entry speed and ac-\ncuracy. However, recent work (Arnold et al., 2016)\nhas shown that writers appreciate suggestions that\nhave creative wording, and can ﬁnd phrases sug-\ngested based on frequency alone to be boring. For\nexample, at the beginning of a restaurant review,\n“I love this place” is a reasonableprediction, but a\nreview writer might prefer a suggestion of a much\nless likely phrase such as “This was truly a wonder-\nful experience”—they may simply not have thought\nof this more enthusiastic phrase. Figure 1 shows\nanother example.\nWe propose a new task for NLP research: gener-\nate suggestions for writers. Doing well at this task\nrequires innovation in language generation but also\nFigure 1: We adapt a language model to offer sug-\ngestions during text composition. In above exam-\nple, even though the middle suggestion is predicted\nto be about 1,000 times more likely than the one\non the right, a user prefers the right one.\ninteraction with people: suggestions must be eval-\nuated by presenting them to actual writers. Since\nwriting is a highly contextual creative process, tra-\nditional batch methods for training and evaluat-\ning human-facing systems are insufﬁcient: ask-\ning someone whether they think something would\nmake a good suggestion in a given context is very\ndifferent from presenting them with a suggestion\nin a natural writing context and observing their re-\nsponse. But if evaluating every proposed parameter\nadjustment required interactive feedback from writ-\ners, research progress would be slow and limited\nto those with resources to run large-scale writing\nexperiments.\nIn this paper we propose a hybrid approach: we\nmaintain a natural human-centered objective, but\nintroduce a proxy task that provides an unbiased\nestimate of expected performance on human evalua-\ntions. Our approach involves developing a stochas-\ntic baseline system (which we call the reference\npolicy), logging data from how writers interact\nwith it, then estimating the performance of candi-\ndate policies by comparing how they would behave\narXiv:1710.01799v1  [cs.CL]  4 Oct 2017\nwith how the reference policy did behave in the\ncontexts logged. As long as the behavior of the\ncandidate policy is not too different from that of\nthe reference policy (in a sense that we formalize),\nthis approach replaces complex human-in-the-loop\nevaluation with a simple convex optimization prob-\nlem.\nThis paper demonstrates our approach: we col-\nlected data of how humans use suggestions made by\na reference policy while writing reviews of a well-\nknown restaurant. We then used logged interaction\ndata to optimize a simple discriminative language\nmodel, and ﬁnd that even this simple model gen-\nerates better suggestions than a baseline trained\nwithout interaction data. We also ran simulations\nto validate the estimation approach under a known\nmodel of human behavior.\nOur contributions are summarized below:\n•We present a new NLP task of phrase sugges-\ntion for writing.1\n•We show how to use counterfactual learning\nfor goal-directed training of language models\nfrom interaction data.\n•We show that a simple discriminative lan-\nguage model can be trained with ofﬂine in-\nteraction data to generate better suggestions\nin unseen contexts.\n2 Related Work\nLanguage models have a long history and play an\nimportant role in many NLP applications (Sordoni\net al., 2015; Rambow et al., 2001; Mani, 2001;\nJohnson et al., 2016). However, these models do\nnot model human preferences from interactions.\nExisting deployed keyboards use n-gram language\nmodels (Quinn and Zhai, 2016; Kneser and Ney,\n1995), or sometimes neural language models (Kim\net al., 2016), trained to predict the next word given\nrecent context. Recent advances in language model-\ning have increased the accuracy of these predictions\nby using additional context (Mikolov and Zweig,\n2012). But as argued in Arnold et al. (2016), these\nincreases in accuracy do not necessarily translate\ninto better suggestions.\nThe difference between suggestion and predic-\ntion is more pronounced when showing phrases\nrather than just words. Prior work has extended\npredictive language modeling to phrase prediction\n(Nandi and Jagadish, 2007) and sentence comple-\n1Code and data are available at https://github.\ncom/kcarnold/counterfactual-lm.\ntion (Bickel et al., 2005), but do not directly model\nhuman preferences. Google’s “Smart Reply” email\nresponse suggestion system (Kannan et al., 2016)\navoids showing a likely predicted response if it\nis too similar to one of the options already pre-\nsented, but the approach is heuristic, based on a\npriori similarity. Search engine query completion\nalso generates phrases that can function as sugges-\ntions, but is typically trained to predict what query\nis made (e.g., Jiang et al. (2014)).\n3 Counterfactual Learning for\nGenerating Suggestions\nWe consider the task of generating good words\nand phrases to present to writers. We choose a\npragmatic quality measure: a suggestion system is\ngood if it generates suggestions that writers accept.\nLet h denote a suggestion system, characterized\nby h(y|x), the probability that hwill suggest the\nword or phrase y when in context x(e.g., words\ntyped so far).2 We consider deploying hin an inter-\nactive interface such as Figure 1, which suggests\nphrases using a familiar predictive typing interface.\nLet δdenote a reward that a system receives from\nthat interaction; in our case, the number of words\naccepted.3 We deﬁne the overall quality of a sug-\ngestion system by its expected reward E[δ] over all\ncontexts.\nCounterfactual learning allows us to evaluate\nand ultimately learn models that differ from those\nthat were deployed to collect the data, so we can\ndeploy a single model and improve it based on the\ndata collected (Swaminathan and Joachims, 2015).\nIntuitively, if we deploy a model h0 and observe\nwhat actions it takes and what feedback it gets, we\ncould improve the model by making it more likely\nto suggest the phrases that got good feedback.\nSuppose we deploy a reference model4 h0 and\nlog a dataset\nD= {(x1,y1,δ1,p1),..., (xn,yn,δn,pn)}\nof contexts (words typed so far), actions (phrases\nsuggested), rewards, and propensities respectively,\nwhere pi ≡h0(yi|xi). Now consider deploying an\nalternative model hθ (we will show an example as\n2Our notation follows Swaminathan and Joachims (2015)\nbut uses “reward” rather than “loss.” Since h(y|x) has the\nform of a contextual language model, we will refer to it as a\n“model.”\n3Our setting admits alternative rewards, such as the speed\nthat a sentence was written, or an annotator’s rating of quality.\n4Some other literature calls h0 a logging policy.\nEq. (1) below). We can obtain an unbiased estimate\nof the reward that hθ would incur using importance\nsampling:\nˆR(hθ) = 1\nn\nn∑\ni=1\nδihθ(yi|xi)/pi.\nHowever, the variance of this estimate can\nbe unbounded because the importance weights\nhθ(yi|xi)/pi can be arbitrarily large for small\npi. Like Ionides (2008), we clip the importance\nweights to a maximum M:\nˆRM(h) = 1\nn\n∑n\ni=1\nδimin {M,hΘ(yi|xi)/pi}.\nThe improved model can be learned by optimizing\nˆhθ = argmaxh ˆRM(h).\nThis optimization problem is convex and differen-\ntiable; we solve it with BFGS. 5\n4 Demonstration Using Discriminative\nLanguage Modeling\nWe now demonstrate how counterfactual learning\ncan be used to evaluate and optimize the acceptabil-\nity of suggestions made by a language model. We\nstart with a traditional predictive language model\nh0 of any form, trained by maximum likelihood\non a given corpus. 6 This model can be used for\ngeneration: sampling from the model yields words\nor phrases that match the frequency statistics of\nthe corpus. However, rather than offering repre-\nsentative samples from h0, most deployed systems\ninstead sample from p(wi) ∝h0(wi)1/τ, where τ\nis a “temperature” parameter; τ = 1corresponds\nto sampling based on p0 (soft-max), while τ →0\ncorresponds to greedy maximum likelihood gener-\nation (hard-max), which many deployed keyboards\nuse (Quinn and Zhai, 2016). The effect is to skew\nthe sampling distribution towards more probable\nwords. This choice is based on a heuristic assump-\ntion that writers desire more probable suggestions;\nwhat if writers instead ﬁnd common phrases to be\noverly cliché and favor more descriptive phrases?\nTo capture these potential effects, we add features\nthat can emphasize various characteristics of the\n5We use the BFGS implementation in SciPy.\n6The model may take any form, but n-gram (Heaﬁeld et al.,\n2013) and neural language models (e.g., (Kim et al., 2016))\nare common, and it may be unconditional or conditioned on\nsome source features such as application, document, or topic\ncontext.\nLM weight = 1, all other weights zero:\ni didn’t see a sign for; i am a huge sucker for\nLM weight = 1,long-word bonus = 1.0:\nanother restaurant especially during sporting events\nLM weight = 1,POS adjective bonus = 3.0:\ngreat local bar and traditional southern\nTable 1: Example phrases generated by the log-\nlinear language model under various parameters.\nThe context is the beginning-of-review token; all\ntext is lowercased. Some phrases are not fully gram-\nmatical, but writers can accept a preﬁx.\ngenerated text, then use counterfactual learning to\nassign weights to those features that result in sug-\ngestions that writers prefer.\nWe consider locally-normalized log-linear lan-\nguage models of the form\nhθ(y|x) =\n|y|∏\ni=1\nexp θ·f(wi|c,w[:i−1])∑\nw′ exp θ·f(w′|c,w[:i−1]), (1)\nwhere y is a phrase and f(wi|x,w[:i−1]) is a fea-\nture vector for a candidate wordwigiven its context\nx. ( w[:i−1] is a shorthand for {w1,w2,...w i−1}.)\nModels of this form are commonly used in se-\nquence labeling tasks, where they are called Max-\nEntropy Markov Models (McCallum et al., 2000).\nOur approach generalizes to other models such as\nconditional random ﬁelds (Lafferty et al., 2001).\nThe feature vector can include a variety of fea-\ntures. By changing feature weights, we obtain lan-\nguage models with different characteristics. To il-\nlustrate, we describe a model with three features be-\nlow. The ﬁrst feature (LM) is the log likelihood un-\nder a base 5-gram language modelp0(wi|c,w[:i−1])\ntrained on the Yelp Dataset 7 with Kneser-Ney\nsmoothing (Heaﬁeld et al., 2013). The second and\nthird features “bonus” two characteristics of wi:\nlong-word is a binary indicator of long word\nlength (we arbitrarily choose ≥6 letters), and POS\nis a one-hot encoding of its most common POS tag.\nTable 1 shows examples of phrases generated with\ndifferent feature weights.\nNote that if we set the weight vector to zero ex-\ncept for a weight of 1/τ on LM, the model reduces\nto sampling from the base language model with\n“temperature”τ. The ﬁtted model weights of the\nlog-linear model in our experiments is shown in\nsupplementary material.\n7https://www.yelp.com/dataset_\nchallenge; we used only restaurant reviews\nReference modelh0. In counterfactual estima-\ntion, we deploy one reference model h0 to learn\nanother ˆh—but weight truncation will prevent ˆh\nfrom deviating too far from h0. So h0 must of-\nfer a broad range of types of suggestions, but they\nmust be of sufﬁciently quality that some are ulti-\nmately chosen. To balance these concerns, we use\ntemperature sampling with a temperature τ = 0.5):\np0(wi|c,w[:i−1])1/τ\n∑\nwp0(w|c,w[:i−1])1/τ.\nWe use our reference model h0 to generate 6-word\nsuggestions one word at a time, so pi is the product\nof the conditional probabilities of each word.\n4.1 Simulation Experiment\nWe present an illustrative model of suggestion ac-\nceptance behavior, and simulate acceptance behav-\nior under that model to validate our methodology.\nOur method successfully learns a suggestion model\nﬁtting writer preference.\nDesirability Model. We model the behavior of\na writer using the interface in Fig. 1, which dis-\nplays 3 suggestions at a time. At each timestep i\nthey can choose to accept one of the 3 suggestions\n{si\nj}3\nj=1, or reject the suggestions by tapping a key.\nLet {pi\nj}3\nj=1 denote the likelihood of suggestion si\nj\nunder a predictive model, and letpi\n∅= 1−∑3\nj=1 pi\nj\ndenote the probability of any other word. Let ai\nj de-\nnote the writer’s probability of choosing the corre-\nsponding suggestion, and ai\nj denote the probability\nof rejecting the suggestions offered. If the writer\ndecided exactly what to write before interacting\nwith the system and used suggestions for optimal\nefﬁciency, then ai\nj would equal pi\nj. But suppose the\nwriter ﬁnds certain suggestions desirable. Let Di\nj\ngive the desirability of a suggestion, e.g., Di\nj could\nbe the number of long words in suggestion si\nj. We\nmodel their behavior by adding the desirabilities to\nthe log probabilities of each suggestion:\na(i)\nj = p(i)\nj exp(D(i)\nj )/Z(i), a(i)\n∅ = p(i)\n∅ /Z(i)\nwhere Z(i) = 1−∑\nj p(i)\nj (1−exp(D(i)\nj )).The net\neffect is to move probability mass from the “reject”\naction ai\n∅to suggestions that are close enough to\nwhat the writer wanted to say but desirable.\nExperiment Settings and Results. We sample\n10% of the reviews in the Yelp Dataset, hold them\nout from training h0, and split them into an equal-\nsized training set and test set. We randomly sample\nsuggestion locations from the training set. We cut\noff that phrase and pretend to retype it. We gen-\nerate three phrases from the reference model h0,\nthen allow the simulated author to pick one phrase,\nsubject to their preference as modeled by the de-\nsirability model. We learn a customized language\nmodel and then evaluate it on an additional 500\nsentences from the test set.\nFor an illustrative example, we set the desirabil-\nity Dto the number of long words (≥6 characters)\nin the suggestion, multiplied by 10. Figure 3 shows\nthat counterfactual learning quickly ﬁnds model pa-\nrameters that make suggestions that are more likely\nto be accepted, and the counterfactual estimates\nare not only useful for learning but also correlate\nwell with the actual improvement. In fact, since\nweight truncation (controlled by M) acts as regu-\nlarization, the counterfactual estimate consistently\nunderestimates the actual reward.\n4.2 Experiments with Human Writers\nWe recruited 74 workers through MTurk to write re-\nviews of Chipotle Mexican Grill using the interface\nin Fig 1 from Arnold et al. (2016). For the sake of\nsimplicity, we assumed that all human writers have\nthe same preference. Based on pilot experiments,\nChipotle was chosen as a restaurant that many\ncrowd workers had dined at. User feedback was\nlargely positive, and users generally understood\nthe suggestions’ intent. The users’ engagement\nwith the suggestions varied greatly—some loved\nthe suggestions and their entire review consisted of\nnearly only words entered with suggestions while\nothers used very few suggestions. Several users\nreported that the suggestions helped them select\nwords to write down an idea or also gave them ideas\nof what to write. We did not systematically enforce\nquality, but informally we ﬁnd that most reviews\nwritten were grammatical and sensible, which indi-\ncates that participants evaluated suggestions before\ntaking them. The dataset contains 74 restaurant\nreviews typed with phrase suggestions. The mean\nword count is 69.3, std=25.70. In total, this data\ncomprises 5125 words, along with almost 30k sug-\ngestions made (including mid-word).\nEstimated Generation Performance. We learn\nan improved suggestion model by the estimated ex-\npected reward ( ˆRM). We ﬁx M = 10and evaluate\nthe performance of the learned parameters on held-\nFigure 2: Example reviews. A colored background indicates that the word was inserted by accepting a\nsuggestion. Consecutive words with the same color were inserted as part of a phrase.\n0 500 1000 1500 2000\n# training samples\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0Reward (# words accepted per suggestion)\nlogging policy h0\ncounterfactual estimate from training\nactual performance on testing\nFigure 3: We simulated learning a model based\non the behavior of a writer who prefers long words,\nthen presented suggestions from that learned model\nto the simulated writer. The model learned to make\ndesirable predictions by optimizing the counterfac-\ntual estimated reward. Regularization causes that\nestimate to be conservative; the reward actually\nachieved by the model exceeded the estimate.\nout data using 5-fold cross-validation. Figure 4\nshows that while the estimated performance of the\nnew model does vary with the M used when esti-\nmating the expected reward, the relationships are\nconsistent: the ﬁtted model consistently receives\nthe highest expected reward, followed by an ab-\nlated model that can only adjust the temperature\nparameter τ, and both outperform the reference\nmodel (with τ = 1). The ﬁtted model weights sug-\ngest that the workers seemed to prefer long words\nand pronouns, and eschewed punctuation.\n5 Discussion\nOur model assumed all writers have the same pref-\nerences. Modeling variations between writers, such\nas in style or vocabulary, could improve perfor-\nmance, as has been done in other domains (e.g.,\nLee et al. (2017)). Each review in our dataset was\nwritten by a different writer, so our dataset could be\n20 40 60 80\nTruncation factor M used in estimation\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\n3.00\n3.25\n3.50Estimated reward (words accepted per suggestion offered)\nBest reweighting of base LM\nFully adapted model\nLogging policy\nFigure 4: The customized model consistently im-\nproves expected reward over baselines (reference\nLM, and the best “temperature” reweighting LM) in\nheld-out data. Although the result is an estimated\nusing weight truncation at M, the improvement\nholds for all reasonable M.\nused to evaluate online personalization approaches.\nOur task of crowdsourced reviews of a single\nrestaurant may not be representative of other tasks\nor populations of users. However, the predictive\nlanguage model is a replaceable component, and a\nstronger model that incorporates more context (e.g.,\nSordoni et al. (2015)) could improve our baselines\nand extend our approach to other domains.\nFuture work can improve on the simple discrimi-\nnative language model presented here to increase\ngrammaticality and relevance, and thus acceptabil-\nity, of the suggestions that the customized language\nmodels generate.\nAcknowledgements Kai-Wei Chang was sup-\nported in part by National Science Foundation\nGrant IIS-1657193. Part of the work was done\nwhile Kai-Wei Chang and Kenneth C. Arnold vis-\nited Microsoft Research, Cambridge.\nReferences\nKenneth C. Arnold, Krzysztof Z. Gajos, and Adam T.\nKalai. 2016. On suggesting phrases vs. predicting\nwords for mobile text composition. In Proceedings\nof UIST ’16.\nSteffen Bickel, Peter Haider, and Tobias Scheffer. 2005.\nLearning to complete sentences. In Machine Learn-\ning: ECML 2005, pages 497–504. Springer.\nKenneth Heaﬁeld, Ivan Pouzyrevsky, Jonathan H.\nClark, and Philipp Koehn. 2013. Scalable modi-\nﬁed Kneser-Ney language model estimation. In Pro-\nceedings of the 51st Annual Meeting of the Associa-\ntion for Computational Linguistics , pages 690–696,\nSoﬁa, Bulgaria.\nEdward L Ionides. 2008. Truncated importance sam-\npling. Journal of Computational and Graphical\nStatistics, 17(2):295–311.\nJyun-Yu Jiang, Yen-Yu Ke, Pao-Yu Chien, and Pu-Jen\nCheng. 2014. Learning user reformulation behavior\nfor query auto-completion. In Proceedings of the\n37th International ACM SIGIR Conference on Re-\nsearch &#38; Development in Information Retrieval,\nSIGIR ’14, pages 445–454, New York, NY , USA.\nACM.\nMelvin Johnson, Mike Schuster, Quoc V . Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Tho-\nrat, Fernanda B. Viégas, Martin Wattenberg, Greg\nCorrado, Macduff Hughes, and Jeffrey Dean. 2016.\nGoogle’s multilingual neural machine translation\nsystem: Enabling zero-shot translation. CoRR,\nabs/1611.04558.\nAnjuli Kannan, Karol Kurach, Sujith Ravi, Tobias\nKaufmann, Andrew Tomkins, Balint Miklos, Greg\nCorrado, Laszlo Lukacs, Marina Ganea, Peter\nYoung, and Vivek Ramavajjala. 2016. Smart re-\nply: Automated response suggestion for email. In\nProceedings of the ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining (KDD).\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M Rush. 2016. Character-aware neural language\nmodels. In Proceedings of the National Conference\non Artiﬁcial Intelligence (AAAI).\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for m-gram language modeling. In Pro-\nceedings of the IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) .\nIEEE.\nJohn Lafferty, Andrew McCallum, and Fernando\nPereira. 2001. Conditional random ﬁelds: Prob-\nabilistic models for segmenting and labeling se-\nquence data. In Proceedings of the International\nConference on Machine Learning (ICML) , pages\n282–289.\nHung-Yi Lee, Bo-Hsiang Tseng, Tsung-Hsien Wen,\nYu Tsao, Hung-Yi Lee, Bo-Hsiang Tseng, Tsung-\nHsien Wen, and Yu Tsao. 2017. Personalizing\nrecurrent-neural-network-based language model by\nsocial network. IEEE/ACM Trans. Audio, Speech\nand Lang. Proc., 25(3):519–530.\nInderjeet Mani. 2001. Automatic Summarization, vol-\nume 3 of Natural Language Processing. John Ben-\njamins Publishing Company, Amsterdam/Philadel-\nphia.\nAndrew McCallum, Dayne Freitag, and Fernando\nPereira. 2000. Maximum entropy Markov models\nfor information extraction and segmentation. In\nProceedings of the International Conference on Ma-\nchine Learning (ICML).\nTomas Mikolov and Geoffrey Zweig. 2012. Context\ndependent recurrent neural network language model.\nIn SLT, pages 234–239.\nArnab Nandi and HV Jagadish. 2007. Effective phrase\nprediction. In Proceedings of the 33rd international\nconference on Very large data bases, pages 219–230.\nVLDB Endowment.\nPhilip Quinn and Shumin Zhai. 2016. A Cost-Beneﬁt\nStudy of Text Entry Suggestion Interaction. Pro-\nceedings of the 2016 CHI Conference on Human\nFactors in Computing Systems, pages 83–88.\nOwen Rambow, Srinivas Bangalore, and Marilyn\nWalker. 2001. Natural language generation in di-\nalog systems. In Proceedings of the ﬁrst interna-\ntional conference on Human language technology re-\nsearch, pages 1–4.\nAlessandro Sordoni, Michel Galley, Michael Auli,\nChris Brockett, Yangfeng Ji, Margaret Mitchell,\nJian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.\nA neural network approach to context-sensitive gen-\neration of conversational responses. In Proceed-\nings of the Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(NAACL).\nAdith Swaminathan and Thorsten Joachims. 2015.\nCounterfactual risk minimization. In Proceedings\nof the 24th International Conference on World\nWide Web Companion, pages 939–941. International\nWorld Wide Web Conferences Steering Committee.\nA Supplemental Material\nA.1 Experiment details\nCrowd workers were U.S.-based Mechanical Turk workers who were paid $3.50 to write a review of\nChipotle using the keyboard interface illustrated in Figure 1. They could elect to use the interface on either\na smartphone or on a personal computer. In the former case, the interaction was natural as it mimicked\na standard keyboard. In the latter case, users clicked with their mouses on the screen to simulate taps.\n(There did not seem to be signiﬁcant differences between these two groups.) The instructions are given\nbelow:\nGo to <URL> on your computer or phone.\nTry out our new keyboard by pretending you’re writing a restaurant review. For this\npart we just want you to play around -- it doesn’t matter what you type as long\nas you understand how it works. Click the submit button and enter the code here:\n____________\nDid you use your phone or did you use you computer?\nHow would you describe the new keyboard to a friend? How do you use the phrase\nsuggestions?\nNext, please go back to <URL> on your computer or phone (reload if necessary).\nNow please use the keyboard to write a fun review for Chipotle, the infamous chain\nMexican restaurant. The ideal review is well written (entertaining, colorful,\ninteresting), and has specific details about Chipotle menu items, service,\natmosphere, etc. Please do not randomly click on nonsense suggestions -- we all\nknow Chipotle doesn’t serve pizza or burgers. We will bonus our favorite review!\nA.2 Qualitative feedback\nWe include quotes from feedback from participants who used the system with suggestions generated by\nthe reference model h0, i.e., n-grams with temperature 1/2. Some users found the suggestions accurate at\npredicting what they intended to say, some found them useful in shaping one’s thoughts or ﬁnding the\n“right words,” and others found them overly simplistic or irrelevant.\n• “The phrases were helpful in giving ideas of where I wanted to go next in my writing, more as a jumping off point than\nword for word. I’ve always liked predictive text so the phrases are the next level of what I never knew I wanted.”\n• “Kind of easy to review but they also sometimes went totally tangent directions to the thoughts that I was trying to\naccomplish.”\n• “I was surprised how well the words matched up with what I was expecting to type.”\n• “I did like the phrase suggestions very much. They really came in handy when you knew what you wanted to say, but just\ncouldn’t ﬁnd the right words.”\n• “I thought they were very easy to use and helped me shape my thoughts as well! I think they may have been a bit too\nsimple in their own, but became more creative with my input.”\nA.3 Fitted model weights\nThe following table gives the ﬁtted weights for each feature in the log-linear model, averaged across\ndataset folds.\nbase LM is_long PUNCT ADJ ADP ADV CONJ DET NOUN NUM PRON PRT VERB\nmean 2.04 0.92 -1.16 1.03 1.45 0.45 0.91 0.36 0.96 0.87 1.68 0.23 0.79\nstd 0.16 0.14 0.26 0.61 0.38 0.55 0.26 0.22 0.14 0.27 0.20 1.00 0.32",
  "topic": "Counterfactual thinking",
  "concepts": [
    {
      "name": "Counterfactual thinking",
      "score": 0.8432719707489014
    },
    {
      "name": "Computer science",
      "score": 0.8250734806060791
    },
    {
      "name": "Task (project management)",
      "score": 0.7708826065063477
    },
    {
      "name": "Language model",
      "score": 0.6812847852706909
    },
    {
      "name": "Natural language processing",
      "score": 0.6571440100669861
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.6383519172668457
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5633873343467712
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.5231496095657349
    },
    {
      "name": "Word (group theory)",
      "score": 0.5194048881530762
    },
    {
      "name": "Linguistics",
      "score": 0.3180990219116211
    },
    {
      "name": "Psychology",
      "score": 0.14754581451416016
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 5
}