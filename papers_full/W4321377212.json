{
  "title": "DrugEx v3: scaffold-constrained drug design with graph transformer-based reinforcement learning",
  "url": "https://openalex.org/W4321377212",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2102261607",
      "name": "Xuhan Liu",
      "affiliations": [
        "Centre for Human Drug Research"
      ]
    },
    {
      "id": "https://openalex.org/A2101834232",
      "name": "Kai Ye",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A1828061262",
      "name": "Herman W.T. van Vlijmen",
      "affiliations": [
        "Centre for Human Drug Research",
        "Janssen (Belgium)"
      ]
    },
    {
      "id": "https://openalex.org/A1943172762",
      "name": "Adriaan P IJzerman",
      "affiliations": [
        "Centre for Human Drug Research"
      ]
    },
    {
      "id": "https://openalex.org/A302993782",
      "name": "Gerard J. P. van Westen",
      "affiliations": [
        "Centre for Human Drug Research"
      ]
    },
    {
      "id": "https://openalex.org/A2102261607",
      "name": "Xuhan Liu",
      "affiliations": [
        "Centre for Human Drug Research"
      ]
    },
    {
      "id": "https://openalex.org/A2101834232",
      "name": "Kai Ye",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A1828061262",
      "name": "Herman W.T. van Vlijmen",
      "affiliations": [
        "Centre for Human Drug Research",
        "Janssen (Belgium)"
      ]
    },
    {
      "id": "https://openalex.org/A1943172762",
      "name": "Adriaan P IJzerman",
      "affiliations": [
        "Centre for Human Drug Research"
      ]
    },
    {
      "id": "https://openalex.org/A302993782",
      "name": "Gerard J. P. van Westen",
      "affiliations": [
        "Centre for Human Drug Research"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2023818227",
    "https://openalex.org/W2046258407",
    "https://openalex.org/W2050535564",
    "https://openalex.org/W2797324131",
    "https://openalex.org/W1500036797",
    "https://openalex.org/W1531916394",
    "https://openalex.org/W2558217333",
    "https://openalex.org/W1979406691",
    "https://openalex.org/W1975391500",
    "https://openalex.org/W2122624834",
    "https://openalex.org/W2767357245",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W3080671367",
    "https://openalex.org/W2529996553",
    "https://openalex.org/W2578240541",
    "https://openalex.org/W2610148085",
    "https://openalex.org/W3094686696",
    "https://openalex.org/W2947950688",
    "https://openalex.org/W2992613109",
    "https://openalex.org/W3030948478",
    "https://openalex.org/W3044640842",
    "https://openalex.org/W2947161483",
    "https://openalex.org/W3158535672",
    "https://openalex.org/W2096541451",
    "https://openalex.org/W959778778",
    "https://openalex.org/W2740946158",
    "https://openalex.org/W2022476850",
    "https://openalex.org/W2034549041",
    "https://openalex.org/W2160592148",
    "https://openalex.org/W2051947720",
    "https://openalex.org/W1995530068",
    "https://openalex.org/W2992072991",
    "https://openalex.org/W3098269892"
  ],
  "abstract": "Abstract Rational drug design often starts from specific scaffolds to which side chains/substituents are added or modified due to the large drug-like chemical space available to search for novel drug-like molecules. With the rapid growth of deep learning in drug discovery, a variety of effective approaches have been developed for de novo drug design. In previous work we proposed a method named DrugEx , which can be applied in polypharmacology based on multi-objective deep reinforcement learning. However, the previous version is trained under fixed objectives and does not allow users to input any prior information ( i.e. a desired scaffold). In order to improve the general applicability, we updated DrugEx to design drug molecules based on scaffolds which consist of multiple fragments provided by users. Here, a Transformer model was employed to generate molecular structures. The Transformer is a multi-head self-attention deep learning model containing an encoder to receive scaffolds as input and a decoder to generate molecules as output. In order to deal with the graph representation of molecules a novel positional encoding for each atom and bond based on an adjacency matrix was proposed, extending the architecture of the Transformer. The graph Transformer model contains growing and connecting procedures for molecule generation starting from a given scaffold based on fragments. Moreover, the generator was trained under a reinforcement learning framework to increase the number of desired ligands. As a proof of concept, the method was applied to design ligands for the adenosine A 2A receptor (A 2A AR) and compared with SMILES-based methods. The results show that 100% of the generated molecules are valid and most of them had a high predicted affinity value towards A 2A AR with given scaffolds.",
  "full_text": "Liu et al. Journal of Cheminformatics           (2023) 15:24  \nhttps://doi.org/10.1186/s13321-023-00694-z\nRESEARCH\n© The Author(s) 2023. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nOpen Access\nJournal of Cheminformatics\nDrugEx v3: scaffold-constrained drug design \nwith graph transformer-based reinforcement \nlearning\nXuhan Liu1, Kai Ye2, Herman W. T. van Vlijmen1,3, Adriaan P . IJzerman1 and Gerard J. P . van Westen1* \nAbstract \nRational drug design often starts from specific scaffolds to which side chains/substituents are added or modified due \nto the large drug-like chemical space available to search for novel drug-like molecules. With the rapid growth of  deep \nlearning in drug discovery, a variety of effective approaches have been developed for de novo drug design. In previ-\nous work we proposed a method named DrugEx, which can be applied in polypharmacology based on multi-objec-\ntive deep reinforcement learning. However, the previous version is trained under fixed objectives and does not allow \nusers to input any prior information (i.e. a desired scaffold). In order to improve the general applicability, we updated \nDrugEx to design drug molecules based on scaffolds which consist of multiple fragments provided by users. Here, \na  Transformer model was employed to generate molecular structures. The Transformer is a multi-head self-attention \ndeep learning model containing an encoder to receive scaffolds as input and a decoder to generate molecules as \noutput. In order to deal with the graph representation of molecules a novel positional encoding for each atom and \nbond based on an adjacency matrix was proposed, extending the architecture of the Transformer. The graph Trans-\nformer model contains growing and connecting procedures for molecule generation starting from   a given scaffold \nbased on fragments. Moreover, the generator was trained under a reinforcement learning framework to increase the \nnumber of desired ligands. As a proof of concept, the method was applied to design ligands for the adenosine  A2A \nreceptor  (A2AAR) and compared with SMILES-based methods. The results show that 100% of the generated molecules \nare valid and most of them had a high predicted affinity value towards  A2AAR with given scaffolds.\nKeywords Deep learning, Reinforcement learning, Policy gradient, Drug design, Transformer, Multi-objective \noptimization, Adenosine  A2A receptor\nIntroduction\nDue to the size of drug-like chemical space (i.e. esti -\nmated at  1033–1060 organic molecules) [1] it is impossi -\nble to screen every corner of it to discover optimal drug \ncandidates. Commonly, specific scaffolds derived from \nendogenous substances, high throughput screening, or \na phenotypic assay [2] are taken as a starting point to \ndesign analogs while side chains/substituents are added \nor modified [3]. These fragments are used as “build -\ning blocks” to develop drug leads with e.g. combinato -\nrial chemistry such as growing, linking, and merging [4]. \nAfter a promising drug lead has been discovered it is \nfurther optimized by modifying side chains to improve \npotency towards the relevant targets, to improve selec -\ntivity over off-targets, and physicochemical properties \nwhich in turn can improve safety and tolerability [5].\n*Correspondence:\nGerard J. P . van Westen\ngerard@lacdr.leidenuniv.nl\n1 Drug Discovery and Safety, Leiden Academic Centre for Drug Research, \nEinsteinweg 55, Leiden, The Netherlands\n2 School of Electrics and Information Engineering, Xi’an Jiaotong \nUniversity, 28 XianningW Rd, Xi’an, China\n3 Janssen Pharmaceutica NV, Turnhoutseweg 30, B-2340 Beerse, Belgium\nPage 2 of 14Liu et al. Journal of Cheminformatics           (2023) 15:24 \nIn scaffold-based rational drug design it is generally \naccepted that a chemical space consisting of  109 diverse \nmolecules can be sampled with only  103 fragments [6]. \nFor instance, one well known class of drug targets are G \nProtein-coupled receptors (GPCRs), a family via which \napproximately 35% of drug exert their effect [7]. The \nadenosine receptors (ARs) form a family within rhodop -\nsin-like GPCRs and include four subtypes  (A1,  A2A,  A2B \nand  A3). Each of them has a unique pharmacological pro-\nfile, tissue distribution, and effector coupling [8, 9]. ARs \nare ubiquitously distributed throughout the human tis -\nsues, and involved in many biological processes and dis -\neases [10]. As adenosine is the endogenous agonist of the \nARs, a number of known ligands of the ARs are adeno -\nsine analogs and/or have a common scaffold. Examples \nof the latter include purines, xanthines, triazines, pyrimi -\ndines [11]. In this work, we aim to design novel ligands \nfor this family of receptors with deep generative neural \nnetworks.\nDeep learning methods have been gaining ground over \nthe last decade in computational drug discovery, includ -\ning de novo design [12]. Deep learning has achieved \nbreakthroughs in visual recognition, natural language \nprocessing, and other data-rich fields [13]. In drug dis -\ncovery the following developments rapidly followed each \nother. For distribution-directed issues, Gomez-Bom -\nbarelli et al. implemented variational autoencoders (VAE) \nto map molecules into a latent space where each point \ncan also be decoded into unique molecules inversely [14]. \nThey used recurrent neural networks (RNNs) to success -\nfully learn SMILES (simplified molecular-input line-entry \nsystem) grammar and construct a distribution of molec -\nular libraries [15]. For goal-directed issues, Sanchez-\nLengeling et  al. combined reinforcement learning and \ngenerative adversarial networks (GANs) to develop an \napproach named ORGANIC to design active compounds \nfor a given target [16]. Olivecrona et  al. proposed the \nREINVENT algorithm which updated this reinforce -\nment learning with a Bayesian approach and combined \nRNNs to generate SMILES-based desired molecules \n[17, 18]. Moreover, Lim et  al. proposed a method for \nscaffold-based molecular design with a graph generative \nmodel [19]. Li et  al. also used deep learning to develop \na tool named DeepScaffold for this issue [20]. Arús -Pous \net al. employed RNNs to develop a SMILES-based scaf -\nfold decorator for de novo drug design [21]. Finally, Yang \net al. used the Transformer model [22] to develop a tool \nnamed SyntaLinker for automatic fragment linking [23]. \nHere we continue to address this issue further with dif -\nferent molecular representations and deep learning \narchitectures.\nIn previous studies we investigated the performance of \nRNNs and proposed a method named DrugEx  that bal -\nances distribution-directed and goal-directed tasks in \nreinforcement learning [24]. Subsequently, DrugEx  was \nupdated with multi-objective reinforcement learning and \napplied in a polypharmacology use case [25]. However, \nthese models cannot receive any input data from users \nand can only produce a distribution of desired molecules \nwith fixed conditions. If the objectives are changed, the \nmodel needs to be trained again. Here, different end-\nto-end deep learning methods are compared to update \nthe DrugEx model to allow users to provide prior infor -\nmation, e.g. fragments that should occur in the gener -\nated molecules. Based on the extensive experience in \nour group with the  A2AAR, this target is again used as \nan  example to evaluate the performance  of these novel \nmethods. The Transformer model takes scaffolds com -\nposed of multiple fragments as input to generate desired \nmolecules which are predicted to be active on  A2AAR \nenabling scaffold-constrained drug design. All python \ncode developed in this study is freely available at https:// \ngithub. com/ CDDLe iden/ DrugEx.\nMaterials and methods\nData source\nThe ChEMBL set from DrugEx v2 was reused [25]. This \nset consisted of small molecule compounds downloaded \nfrom ChEMBL using a SMILES notation (version 27) \n[26]. After data preprocessing via RDKit ~ 1.7 million \nmolecules remained for model pre-training. Preprocess -\ning included neutralizing charges and removing metals \nand small fragments. In addition, 10,828 ligands and their \nbioactivity data on one or more of the four human adeno-\nsine receptors were extracted from ChEMBL to construct \nthe LIGAND set. The LIGAND  set structures were used \nfor fine-tuning the generative model. Moreover, mol -\necules with annotated  A2AAR activity were used to train \na bioactivity prediction model. If multiple measurements \nfor the same ligand existed, the average pChEMBL [27] \nvalue (pX, including pKi, pKd, pIC50 or pEC50) was cal -\nculated and duplicate items were removed. For the bio -\nactivity models the threshold of affinity was defined as \npX = 6.5 to predict if the compound was active (> = 6.5) \nor inactive (< 6.5). It was shown previously that this ena -\nbles the creation of a balanced classifier [28].\nThe dataset was constructed with an input–\noutput pair for each data point. Each molecule was \ndecomposed into a series of fragments with the \nBRICS method [29] in RDKit (Fig.  1A). If a molecule \ncontained more than four leaf fragments, the smaller \nfragments were ignored and a maximum of four larger \nPage 3 of 14\nLiu et al. Journal of Cheminformatics           (2023) 15:24 \n \nfragments were kept. For example, if one molecule M \ncontained four fragments, including A, B, C and D, \nthen there were 15 input–output pairs (A-M, B-M, \nC-M, D-M, AB-M, AC-M, AD-M, BC-M, BD-M, \nCD-M, ABC-M, ABD-M, ACD-M, BCD-M, ABCD-M) \nbased on the full combination. The order of input \nfragments was randomly selected. The SMILES of the \nfragments were joined with ‘. ’ as input data and a pair \nwas created with the full SMILES of molecules. The \nscaffold was defined as the combination of different \nfragments which can be either continuous (linked) or \ndiscrete (separated). The resulting scaffold-molecule \npairs formed the input and output data (Fig.  1B). \nAfter completion of the data pairs, the set was split \ninto a training set, a validation set, and a test set with \nthe ratio 8:1:1 based on the scaffolds. As a result, the \nChEMBL set contained 9,335,410 pairs in the training \nset, 1,104,125 pairs in the validation set, and 1,083,271 \npairs in the test set. In addition, in the LIGAND  set \nthere were 53,888 pairs in the training set, 7,380 pairs \nin the validation set, and 7,525 pairs in the test set. \nMoreover, the scaffolds in LIGAND  set were also split \ninto training set (11,836 samples), validation set (1,479 \nsamples), and test set (1,479 samples) with the ratio \n8:1:1 for reinforcement learning.\nMolecular representations\nTwo different molecular representations were tested: \nSMILES and graph. For SMILES representations each \nscaffold-molecule pair was transformed into two SMILES \nsequences which were then split into different tokens to \ndenote atoms, bonds, or tokens for grammar control (e.g. \nparentheses or numbers). All of these tokens were put \ntogether to form a vocabulary which recorded the index \nof each token (Fig.  1D). The same conversion procedure \nand vocabulary as in DrugEx  v2 was used [25]. Summa -\nrizing, for both input and output sequences of each pair, \na start token (GO) was put at the beginning and an end \ntoken (END) at the end. After sequence padding with a \nblank token at empty positions, each SMILES sequence \nwas rewritten as a series of token indices with a fixed \nlength to form the input and output matrix (Fig. 1E).\nFor the graph representation each molecule was repre -\nsented as a five-row matrix, in which the first two rows \nstand for the atom type and and bond types, respec -\ntively. The third and fourth rows represent the connected \natom index and current atom index, and the fifth row \nrepresents the fragment index (Fig.  1C). The columns \nof this matrix contain three sections to store the frag -\nment, growing part, and linking part. The fragment sec -\ntion starts with a start token in the first row and the last \nFig. 1 scaffold-molecule pair dataset construction. (A) Each molecule in the dataset is decomposed hierarchically into a series of fragments with \nthe BRICS algorithm. (B) Subsequently data pairs between input and output are created. Combinations of leaf fragments form the scaffold as input, \nwhile the whole molecule becomes the output. For clarity token colors alternate. (C) After conversion to an adjacency matrix, each molecule was \nrepresented as a graph matrix. The graph matrix contains five rows, standing for the atom type, bond type, connected atom index, atom index, \nand fragment index. Columns are divided in three parts to store the information of the fragment, the growing section and the linking section. (D) \nAll tokens are collected to construct the vocabularies for SMILES-based and graph-based generators, respectively. (E) An example of the input and \noutput matrices for the SMILES representation of scaffolds and molecules.\nPage 4 of 14Liu et al. Journal of Cheminformatics           (2023) 15:24 \nrow was labeled with the index of each fragment starting \nfrom one. The fragments of each molecule are put in the \nbeginning of the matrix, followed by the growing part for \nthe fragment, and the last part is the connecting bond \nbetween these growing fragments with single bonds. For \nthe growing and linking sections the last row was always \nzero and these two sections were separated by the col -\numn of the end token. It is worth noticing that the last \nrow was not directly involved in the training process. The \nvocabulary for graph representation was different from \nthe SMILES representation, containing 38 atom types \n(Additional file 1: Table S1), and four bond types (single, \ndouble, triple bonds and no bond). For each column, if an \natom is the first occurrence in a given fragment the type \nof the bond will be empty (indexed as 0 with token ‘*’). In \naddition, if the atom at the current position has occurred \nin the matrix, the type of the atom in this column will be \nempty. In order to grasp more details of the graph repre -\nsentation, the pseudocode for encoding (Additional file 1: \nTable  S2) and decoding (Additional file  1: Table  S3) is \nprovided.\nEnd‑to‑end deep learning\nHere, four different end-to-end DL architectures \nwere compared to deal with different molecular \nrepresentations of either graph or SMILES (Fig.  2). \nThese methods included: (A) a Graph Transformer, \n(B) an LSTM-based encoder-decoder model \n(LSTM-BASE), (C) an LSTM-based encoder-decoder \nmodel with an attention mechanism (LSTM + ATTN) \nand (D) a Sequential Transformer model. All of these \nDL models were constructed with PyTorch [ 30].\nFor the SMILES-based models, three different types \nwere constructed as follows (Fig.  2, right). The two \nLSTM-based models share similarities as will be out -\nlined here. In the LSTM-BASE model (Fig.  2B) the \nencoder and decoder had the same architecture as used \nin DrugEx v2, containing one embedding layer, three \nrecurrent layers, and one output layer. The number of \nneurons in the embedding and hidden layers were 128 \nand 512, respectively. The hidden states of the recurrent \nlayer in the encoder are directly sent to the decoder as \nthe initial states. The LSTM + ATTN modes is con -\nstructed on top of this LSTM-BASE model by adding \nan attention layer between the encoder and decoder \n(Fig.  2C). The attention layer calculates the weight \nfor each position of the input sequence to determine \nwhich position the decoder needs to focus on during \nthe decoding process. For each step the weighted sums \nof the output calculated by the encoder are combined \nwith the output of the embedding layer in the decoder \nto form the input for the recurrent layers. The output \nof the recurrent layers is dealt with by the output layer \nto generate the probability distribution of tokens in the \nvocabulary.\nFig. 2 Architectures of four different end-to-end deep learning models: A The Graph Transformer; B The LSTM-based encoder-decoder model \n(LSTM-BASE); C The LSTM-based encoder-decoder model with attention mechanisms (LSTM + ATTN); D The sequential Transformer model. The \nGraph Transformer accepts a graph representation as input and SMILES sequences are taken as input for the other three models\nPage 5 of 14\nLiu et al. Journal of Cheminformatics           (2023) 15:24 \n \nThe sequential Transformer has a distinct architec -\nture compared to the LSTM-BASE and LSTM + ATTN \nmodels although it also exploits an attention mecha -\nnism. For the embedding layers “position encodings” \nare added into the typical embedding structure as the \nfirst layer of the encoder and decoder. This ensures \nthat the model no longer needs to encode the input \nsequence token by token but can process all tokens in \nparallel. For the position embedding, sine and cosine \nfunctions are used to define its formula as follows:\nwhere PE(p, i) is the i th dimension of the position encod -\ning at position p. It has the same dimension d m = 512 \nas the typical embedding vectors so that the two can be \nsummed.\nIn addition, self-attention is used in the hidden layers \nto cope with long-range dependencies. For each hidden \nlayer in the encoder, it employs a residual connection \naround a multi-head self-attention sublayer and feed-\nforward sublayer followed by layer normalization.  Fur -\nthermore, a third sublayer with multi-head attention is \ninserted to capture the information from output of the \nencoder.\nThis self-attention mechanism is defined as the scaled \ndot-product attention with three vectors: queries (Q ), \nkeys (K) and values (V ), of which the dimensions are dq, \ndk, dv, respectively. The output matrix is computed as:\nInstead of a single attention function, the Trans -\nformer adopts multi-head attention to combine infor -\nmation from different representations at different \npositions which is defined as:\nwhere h is the number of heads. For each head, the atten-\ntion values were calculated by different and learned lin -\near projections with Q, K and V as follows:\nwhere WO, W Q, W K and W V are metrics of learned \nweights and we set h = 8 as the number of heads and \ndk = dv = 64 in this work.\nFor the graph representation of the molecules, the \nstructure of the sequential Transformer was updated \nto a Graph Transformer model (Fig.  2A). Similar to the \nPE (p,2i) = sin(pos/10000 2i/dm )\nPE (p,2i+1) = cos(pos/10000 2i/dm )\nAttention(Q , K , V ) = softmax\n(\nQK ⊺\n√\ndk\n)\nV\nMultiHead(Q,K,V ) = Concat(head1,... ,headh)W O\nheadi = Attention(QW Q\ni , KW K\ni , VW V\ni )\nsequential Transformer the Graph Transformer also \nrequires the encoding of both word and position as the \ninput. For the input word, the atom and bond cannot be \nprocessed simultaneously; therefore the indices of the \natom and the bond are combined together   as follows:\nThe index of the input word (W) for calculating word \nvectors is obtained by multiplying the atom type (T atom) \nby four (the total number of bond types defined) and sub-\nsequently add the bond index (T bond). Similarly, the posi-\ntion of each step cannot be used to calculate the position \nencoding directly. Faced with more complex data struc -\nture than sequential data, Dosovitskiy et  al. proposed a \nnew positional encoding scheme to define the position \nfor each patch in image data for image recognition [31]. \nInspired by their work the position encoding at each step \nwas defined as:\nThe input position (P) for calculating the position \nencoding was obtained by multiplying the current atom \nindex (I Atom) by the max length (L max) and then add -\ning the index of the connected atom (I Connected), which \nwas then processed with the same positional encod -\ning method as with the sequential Transformer. For the \ndecoder, the hidden vector from the Graph Transformer \nwas taken as the starting point to be decoded by a GRU-\nbased recurrent layer; and the probability of atom type, \nbond type, connected atom index, and current atom \nindex was decoded one by one sequentially.\nWhen graph-based molecules are generated, the chem-\nical valence rule is checked in every step. Invalid values \nof atom and bond types will be masked and an incorrect \nprevious or current position will be removed  to ensure \nthe validity of all generated molecules. It is worth notic -\ning that before being encoded, each molecule will be \nkekulized, meaning that the aromatic rings will be  trans -\nformed into either single or double bonds. The reason for \nthis is that aromatic bonds interfere with the calculation \nof the valence value for each atom.\nDuring the training process of SMILES-based mod -\nels, a negative log likelihood function was used to con -\nstruct the loss function. The loss function guarantees that \nthe probability of the token at each step in the output \nsequence becomes large enough in the probability dis -\ntribution of the vocabulary calculated by the deep learn -\ning model. In comparison, the loss function used by the \nGraph Transformer model also contains four parts for \natom type, bond type, connected atom index and current \natom index. Here the sum of these negative log probabil -\nity values is minimized to optimize the parameters in the \nmodel. For this, the Adam algorithm was used for the \nW = Tatom× 4 + Tbond\nP = IAtom × Lmax + IConnected\nPage 6 of 14Liu et al. Journal of Cheminformatics           (2023) 15:24 \noptimization of the loss function. Here, the learning rate \nwas set as  10–4, the batch size was 256, and training steps \nwere set to 20 epochs for pre-training and 1,000 epochs \nfor fine-tuning. In the end, optimal models were selected \nfrom the epoch in which the loss function achieved a \nminimum on the validation set. In the fine-tuning pro -\ncess, early stopping was evoked if the loss value did not \ndecrease after 100 epochs.\nMulti‑objective optimization\nIn order to combine multiple objectives, we exploited a \nPareto-based ranking algorithm with GPU acceleration \nas mentioned in DrugEx v2 [25]. Given two solutions m1 \nand m2 with their scores (x 1, x2, …, xn) and (y1, y2, …, yn), \nthen m1 is said to Pareto dominate m2 if and only if:\nOtherwise, neither m1 nor m 2 is dominates. After the \ndominance between all pairs of solutions has been deter -\nmined, the non-dominated scoring algorithm is exploited \nto obtain a rank of Pareto frontiers which consist of a set \nof solutions. In the same frontier, molecules were ranked \nbased on the average Tanimoto-distance to other mole -\ncules instead of the commonly used crowding distance in \nthe same frontier. Subsequently molecules with smaller \naverage distances were ranked on the top. The final \nreward R* is defined as:\nhere k is the index of the solution in the Pareto rank. \nRewards of undesired and desired solutions will be evenly \ndistributed in (0, 0.5] and (0.5, 0.1], respectively.\nIn this work, two objectives were considered: (1) the \nQED score [32] as implemented by RDKit (from 0 to 1) \nto evaluate the drug-likeness of each molecule (a larger \nvalue means more drug-like); (2) an affinity score towards \nthe  A2AAR which was implemented by a random for -\nest regression model with Scikit-Learn [33]. The input \ndescriptors consisted of 2048D ECFP6 fingerprints and \n19D physico-chemical descriptors (PhysChem). Phy -\nsChem included: molecular weight, logP , number of H \nbond acceptors and donors, number of rotatable bonds, \nnumber of amide bonds, number of bridge head atoms, \nnumber of hetero atoms, number of spiro atoms, num -\nber of heavy atoms, the fraction of SP3 hybridized \ncarbon atoms, number of aliphatic rings, number of \nsaturated rings, number of total rings, number of aro -\nmatic rings, number of heterocycles, number of valence \nelectrons, polar surface area, and Wildman-Crippen MR \nvalue. Again, it was determined if generated molecules \nare desired based on the Affinity score (larger than the \n∀j ∈ {1, ... ,n } :xj ≥ yj and ∃j ∈ {1, ... ,n } :xj;yj\nR ∗ =\n{\n0.5 + k−N undesired\n2N desired\n, if desired\nk\n2N undesired\n, if undesired\nthreshold = 6.5). In addition, the SA score [34] was also \nexploited an independent measurement to evaluate the \nsynthesizability of generated molecules, which is also cal-\nculated by RDKit.\nReinforcement learning\nA reinforcement learning framework was constructed \nbased on the interplay between a Graph Transformer \n(agent) and two scoring functions (environment). A \npolicy gradient method was implemented to train the \nreinforcement learning model, the objective function is \ndesignated as follows:\nFor each step t during the generation process the gen -\nerator (G) determines the probability of each token (y t) \nfrom the vocabulary  based on the generated sequence \nin previous steps (y 1:t-1). In the sequence-based models \nyt  is only  a token selected from the vocabulary to con -\nstruct SMILES while in the graph-based model it can be \ndifferent type of atoms or bonds or the atoms connected \nby the bond. The parameters in the objective function are \nupdated by  a policy gradient based on the expected end \nreward  (R*) received from the predictors. By maximiz -\ning this function, the parameter θ in the generator can be \noptimized to ensure that the generator designs desired \nmolecules which obtain a high reward score.\nIn order to improve the diversity and reliability of gen -\nerated molecules, the exploration strategy for molecule \ngeneration during the training loops was implemented. \nIn the training loop the generator is trained to produce \na chemical space as defined by the target of interest. In \nthis strategy there are two networks with the same archi -\ntectures, an exploitation net (G θ) and an exploration \nnet (G φ). Gφ did not require training as the parameters \nare always fixed and it is based on the general drug-\nlike chemical space for diverse targets obtained from \nChEMBL. The parameters in G θ on the other hand were \nupdated for each epoch based on the policy gradient. \nAgain, an exploring rate (ε) was defined with a range of \n[0.0, 1.0] to determine the percentage of scaffolds being \nrandomly selected as input by G φ to generate molecules. \nConversely Gθ generated molecules with other input \nscaffolds. After the training process was finished G φ was \nremoved and only G θ was left as the final model for mol -\necule generation.\nPerformance evaluation\nIn order to evaluate the performance of the generators, \nfive coefficients were calculated from the population \nJ(θ ) = E\n[\nR ∗(y1:T)|θ\n]\n=\nT∑\nt=1\nlogG\n(\nyt|y1:t−1\n)\n• R ∗(\ny1:T\n)\nPage 7 of 14\nLiu et al. Journal of Cheminformatics           (2023) 15:24 \n \nof generated molecules (validity, accuracy, desirability, \nuniqueness, and novelty) which are defined as:\nhere Ntotal is the total number of molecules, N valid is the \nnumber of molecules parsed as valid SMILES sequences, \nNaccurate is the number of molecules that contained all \ngiven scaffolds, N desired is the number of desired mol -\necules that reach all required objectives, N unique is the \nnumber of molecules which are different from others \nin the dataset, N novel is the number of generated unique \nmolecules that do not exist in the ChEMBL set.\nTo measure molecular diversity, we adopted the \nSolow Polasky measurement as in the previous work. \nThis approach was proposed by Solow and Polasky in \n1994 to estimate the diversity of a biological popula -\ntion in an eco-system [35]. The formula to calculate \ndiversity was redefined to normalize the range of values \nfrom [1, m] to (0, m] as follows:\nwhere A is a set of drug molecules with a size of |A| equal \nto m, e is an m-vector of 1’s and F(s)  = [f(dij))] is a non-\nsingular m × m distance matrix. Hereing f(d ij) stands for \nthe distance function of each pair of molecules provided \nas follows:\nHere θ was set to 0.5 as suggested in [35]. The dis -\ntance dij between molecules s i and s j was defined by \nusing the Tanimoto-distance with ECFP6 fingerprints \nas follows:\nValidity= N valid\nN total\nAccuracy = N accurate\nN total\nDesirability= N desired\nN total\nUniqueness = N unique\nN total\nNovelty = N novel\nN total\nI(A ) = 1\n|A |e⊺F(s)−1 e\nf (d ) = e−θ d ij\nd ij = d\n(\nsi,sj\n)\n= 1 −\n⏐⏐si ∩sj\n⏐⏐\n⏐⏐si ∪sj\n⏐⏐,\nwhere | s i ∩ sj | represents the number of common \nfingerprint bits, and | s i ∪ sj | is the number of union \nfingerprint bits.\nResults and discussion\nFragmentation of molecules\nEach molecule was decomposed into a series of \nfragments with the BRICS algorithm to construct a \nfragment-molecule pair with a compiled elaborate set \nof rules. For the ChEMBL and LIGAND  sets, 194,782 \nand 2,310   fragments were obtained, respectively. The \nLIGAND set was further split into three parts: active \nligands (LIGAND +, 2,638 compounds), inactive ligands \n(LIGAND−, 2,710 compounds) and undetermined \nligands (LIGAND 0, 5,480 compounds) based on the \npX of bioactivity for  A2AAR. The number of fragments \nper each molecule in these four datasets have a similar \ndistribution (Fig.  3A) and there are approximately five \nfragments on average for each molecule with a 95% \nconfidence between [1, 11] (Fig. 3A).\nIn the LIGAND  set the three subsets have a similar \nmolecular weight distribution of the fragments (Fig.  3B) \nwith an average of 164.3 Da, smaller than in the ChEMBL \nset (247.3  Da). The average similarity between training \nand test set is also slightly higher in the LIGAND set com-\npared to the ChEMBL set (Additional file  1: Figure S1). \nAs the structure of fragments is generally smaller than \nthe structure of molecules we used the Tanimoto similar-\nity calculation with ECFP4 (rather than ECFP6) descrip -\ntors between each pair of fragments in the same dataset \nto check the similarity of these fragments. It was found \nthat most of them were smaller than 0.5 indicating that \nthey are dissimilar to each other (Fig.  3C). Especially, the \nfragments in the LIGAND + set have the largest diversity. \nMoreover, the distribution of different fragments in these \nthree subsets of the LIGAND  set is shown in Fig.  3D. The \nmolecules in these three subsets have their unique frag -\nments and share some common substructures.\nGenerated molecules\nPre‑training and Fine‑tuning\nAfter finishing the dataset construction four models \nwere pre-trained on the ChEMBL set and fine-tuned \non the LIGAND  set. These models were benchmarked \non a server with Nvidia Tesla P100 GPUs. After the \ntraining process converged, each fragment in the test \nset was presented as input 20 times for both ChEMBL \nand LIGAND test sets to generate molecules. The \nperformance is shown in Table  1 (with additional \ntanimoto frequencies in the Additional file  1: Figure \nS2). Based on this benchmark, Transformer methods \noutperformed LSTM-based methods using SMILES. \nIn addition, the training of Transformer models was \nPage 8 of 14Liu et al. Journal of Cheminformatics           (2023) 15:24 \nfound to be faster but to consume more computational \nresources than LSTM-based methods with the same \nnumber of neurons in the hidden layers. Although the \nthree SMILES-based models improved after being \nfine-tuned they were still outperformed by the Graph \nTransformer because of the advantages of  graph \nrepresentation. To further check the accuracy of \ngenerated molecules the chemical space between the \ngenerated molecules and the compounds in the training \nset was compared with three different representations (1) \nMW ~ logP; (2) PCA with 19D PhysChem descriptors; \n(3) tSNE with 2048D ECFP6 fingerprints (Fig.  4). In \naddition, the loadings of PCA for each descriptor in \nPhysChem are provided in Additional file 1: Table S4. The \nregion occupied by molecules generated by the Graph \nTransformer overlapped completely with the compounds \nin both the ChEMBL and LIGAND  sets. In addition, \nthe average tanimoto similarity of molecules generated \nby the four methods in pre-training, fine-tuning, and \nreinforcement learning using the Graph Transformer are \nshown in supplementary information (Additional file  1: \nFigure S3).\nThe graph representation for molecules has advantages \nover the SMILES representation when dealing with \nfragment-based molecule design: (1) Invariance on a \nlocal scale: During the process of molecule generation, \nmultiple fragments in a given scaffold can be put into \nany position in the output matrix without changing \nFig. 3 Analysis of some properties of fragments in the ChEMBL set and three LIGAND subsets. A Violin plot for the distribution of the number of \nfragments per molecules; B Distribution of molecular weight of these fragments; C Distribution of the similarity of the fragments measured by the \nTanimoto-similarity with ECFP4 fingerprints; D Venn diagram for the intersection of the fragments existing in the three subsets of the LIGAND set\nPage 9 of 14\nLiu et al. Journal of Cheminformatics           (2023) 15:24 \n \nTable 1 The performance of four different generators with different number of neurons in hidden layers for pre-training and fine-\ntuning processes\nMethods Hidden Neurons Pre‑trained Model Fine‑tuned Model\nValidity Accuracy Novelty Uniqueness Validity Accuracy Novelty Uniqueness\nGraph Transformer 512 100.0% 99.3% 99.9% 99.4% 100.0% 99.2% 68.9% 82.9%\nSequential Transformer 128 91.8% 62.4% 90.2% 92.5% 94.5% 80.5% 8.6% 24.3%\n256 94.2% 69.3% 89.3% 91.4% 98.8% 89.5% 9.2% 26.6%\n512 96.7% 74.0% 89.1% 91.8% 99.3% 92.7% 8.9% 28.9%\n1024 97.1% 77.9% 89.5% 91.4% 99.4% 94.3% 8.2% 32.9%\nLSTM-BASE 128 87.1% 38.7% 83.2% 84.0% 85.2% 53.1% 9.9% 26.8%\n256 91.4% 48.8% 89.0% 91.2% 94.5% 75.8% 5.8% 21.2%\n512 93.9% 52.4% 84.3% 89.1% 98.7% 81.6% 3.9% 19.2%\n1024 95.7% 57.0% 79.6% 87.5% 99.6% 90.2% 2.1% 18.1%\nLSTM + AT TN 128 89.8% 57.0% 84.2% 85.0% 85.2% 64.8% 14.2% 27.8%\n256 92.6% 68.4% 87.1% 89.5% 94.9% 80.5% 8.9% 22.4%\n512 94.3% 72.8% 85.3% 89.7& 96.9% 85.9% 6.3% 20.7%\n1024 96.0% 75.0% 80.7% 89.4% 99.1% 92.9% 4.2% 20.2%\nFig. 4 The chemical space of generated molecules by the Graph Transformer. Shown are the molecules generated by the models pre-training on \nthe ChEMBL set (A, C and E) and fine-tuning on the LIGAND set (B, D and F). Chemical space was represented by either logP ~ MW (A and B) and the \nfirst two components from a PCA on PhysChem descriptors (C, D) and t-SNE on ECFP6 fingerprints (E and F)\nPage 10 of 14Liu et al. Journal of Cheminformatics           (2023) 15:24 \nthe order of atoms and bonds in that scaffold. (2) \nExtendibility on a global scale: When fragments in the \nscaffold are growing or being linked, they can be flexibly \nappended in the end column of the graph matrix while \nthe original data structure does not need changing. (3) \nFree of grammar: Unlike in SMILES sequences there \nis no explicit grammar to constrain the generation of \nmolecules, such as the parentheses for branches and \nthe numbers for rings in SMILES; (4) Accessibility \nof chemical rules: For each added atom or bond the \nalgorithm can detect if the valence of atoms is valid or \nnot and mask invalid atoms or bonds in the vocabulary to \nguarantee the whole generated matrix can be successfully \nparsed into a molecule. Due to these four advantages \nthe Graph Transformer generates molecules faster while \nusing less computational resources.\nHowever, after examining the QED scores and SA \nscores it was found that although the distribution of \nQED scores was similar between the methods (Fig.  5A \nand C), the synthesizability of the molecules generated \nby the Graph Transformer was not better than the \nSMILES-based generators. This was especially true \nwhen fine-tuning on the LIGAND  set. A possible reason \nis that molecules generated by the Graph Transformer \ncontain uncommon rings when the model dealt with \nlong-distance dependencies. In addition, because of \nmore complicated data structure and presence of more \nparameters in the model, the Graph Transformer did not \nFig. 5 The distribution of the QED score (A, C) and SA score (B, D) of desired ligands. Shown are the molecules generated from the ChEMBL set \nand LIGAND set and of molecules generated by four different generators. For the QED score, four generators had the same performance as the \nmolecules in both ChEMBL set (A) and the LIGAND set (C). For the SA score, Graph Transformer did not outperform three other SMILES-based \ngenerators in the ChEMBL set (B) and even worse in the LIGAND set (D).\nPage 11 of 14\nLiu et al. Journal of Cheminformatics           (2023) 15:24 \n \noutperform the other methods based on synthesizability \nof generated molecules while trained on a small dataset \n(e.g., the LIGAND set). It is also worth noticing that there \nstill was a small fraction of generated molecules that did \nnot contain the required scaffolds which is caused by a \nkekulization problem. For example, a scaffold ‘CCC’ \ncan be grown into ‘C1 = C(C)C = CC = C1’ . After being \nsanitized, it can be transformed into ‘c1c(C)cccc1’ . In this \nprocess one single bond in the scaffold is changed to an \naromatic bond, which causes a mismatch between the \nscaffold and the molecule. Currently DrugEx v3 cannot \nsolve this problem because if the aromatic bond is taken \ninto consideration, the valence of aromatic atoms is \ndifficult to calculate accurately. This would lead to the \ngeneration of invalid molecules. Therefore, there is no \naromatic bond provided in the vocabulary and all of the \naromatic rings are inferred automatically through the \nmolecule sanitization method in RDKit.\nPolicy gradient\nBecause the Graph Transformer generates with the best \nperformance it was chosen as the agent in the RL frame -\nwork. Two objectives were tested in the training process. \nThe first one was affinity towards  A2AAR, which is pre -\ndicted by the random forest-based regression model from \nDrugEx v2; the second one was the QED score calculated \nwith RDKit to measure how drug-like a generated mole -\ncule is. With the policy gradient method as the reinforce-\nment learning framework two cases were tested. On the \none hand, predicted affinity for  A2AAR was considered \nwithout the QED score. On the other hand, both objec -\ntives were used to optimize the model with Pareto rank -\ning. After the training process finished, each fragment in \nthe LIGAND test set was presented as input 20 times to \ngenerate molecules. In the first case 86.1% of the gener -\nated molecules were predicted active, while the percent -\nage of predicted active molecules in the second case was \n74.6%. Although the generator generated more active \nligands without the QED score constraint, most of them \nare not drug-like as they frequently have a molecular \nweight larger than 500  Da. However, when we checked \nthe chemical space represented by tSNE with ECFP6 \nfingerprints the overlap region between generated mol -\necules and ligands in the training set was not complete \nimplying that they fell out of the applicability domain of \nthe regression model.\nChanges to the exploration rate do not influence accu -\nracy and have a low effect on diversity. However, desir -\nability (finding active ligands) and uniqueness can be \ninfluenced significantly. Empirically determining an opti -\nmal value for a given chemical space is recommended.\nIn DrugEx v2, an exploration strategy simulated the \nidea of evolutionary algorithms such as crossover and \nmutation manipulations. However, when coupled to the \nGraph Transformer there were some difficulties and the \nstrategy had to be given up. Firstly, the mutation strategy \ndid not improve with different mutation rates. A possible \nreason is that before being generated, part the molecule \nwas fixed with a given scaffold counteracting the effect \nof mutation caused by the mutation net. Secondly, the \ncrossover strategy is computationally expensive in this \ncontext. This strategy needs the convergence of model \ntraining and iteratively updates the parameters in the \nagent. With multiple iterations, it takes a long period of \ntime beyond the computational resources available. As a \nresult, the exploration strategy was updated as mentioned \nin the Methods section with six different exploration \nrates: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5].\nAfter training the models, multiple scaffolds were input \n20 times to generate molecules. The results for accuracy, \ndesirability, uniqueness, novelty,  and diversity with \ndifferent exploration rates are shown in Table  2. With a \nlow ε the model generated more desired molecules, but \nthe uniqueness of the generated molecules decreased \nsignificantly. At ε = 0.3 the model generated the highest \npercentage of unique desired molecules (56.8%). Diversity \nwas always larger than 0.84 and the model achieved the \nlargest value (0.88) with ε = 0.0 or ε = 0.2. The chemical \nspace represented by tSNE with ECFP6 fingerprints \nconfirmed that the exploration strategy produced a set \nof generated molecules completely covering the region \noccupied by the LIGAND set (Fig. 6).\nIn the chemical space making up antagonists of  A2AAR \nthere are several well-known scaffolds. Examples include \nfuran, triazine, aminotriazole, and purine derivatives \nsuch as xanthine and azapurine. The Graph Transformer \nmodel produced active ligands for  A2AAR (inferred from \nthe predictors) with different combinations of these \nfragments as scaffolds. Taking these molecules generated \nby the Graph Transformer as an example, we filtered out \nthe molecules with potentially reactive groups (such as \nTable 2 The performance of the Graph Transformer with \ndifferent exploration rates in the RL framework\nε Accuracy Desirability Uniqueness Novelty Diversity\n0.0 99.7% 74.6% 60.7% 60.6% 0.879\n0.1 99.7% 66.8% 75.0% 74.6% 0.842\n0.2 99.8% 61.6% 80.2% 79.4% 0.879\n0.3 99.7% 56.8% 89.8% 88.8% 0.874\n0.4 99.7% 54.8% 88.8% 87.5% 0.859\n0.5 99.7% 46.8% 88.5% 86.4% 0.875\nPage 12 of 14Liu et al. Journal of Cheminformatics           (2023) 15:24 \naldehydes) and uncommon ring systems and listed 30 \ndesired molecules as putative  A2AAR ligands/antagonists \n(Fig. 7). For each scaffold five molecules were selected and \nassigned in the same row. These molecules are considered \na valid starting point for further considerations and work \n(e.g., molecular docking or simulation, or even synthesis).\nConclusions and future perspectives\nIn this study, DrugEx  was updated with the ability to \ndesign novel molecules based on scaffolds consisting \nof multiple fragments as input. In this version (v3), a \nnew positional encoding scheme for atoms and bonds \nwas proposed to make the Transformer model deal \nwith a molecular graph representation. With one \nmodel, multiple fragments in a given scaffold can be \ngrown at the same time and connected to generate a \nnew molecule. In addition, chemical rules on valence \nare enforced at each step of the process of molecule \ngeneration to ensure that all generated molecules are \nvalid. These advantages are impossible to be embodied \nin SMILES-based generation, as SMILES-based \nmolecules are constrained by grammar that allows a \n2D topology to be represented in a sequential way. \nWith multi-objective reinforcement learning the model \ngenerates drug-like ligands, in our case for the  A2AAR \ntarget.\nIn future work, the Graph Transformer will be \nextended to include other information as input to \ndesign drugs conditionally. For example, proteoch -\nemometric modelling (PCM) can take information for \nboth ligands and targets as input to predict the affin -\nity of their interactions, which allows generation of \ncompounds that are promiscuous (useful for e.g., viral \nmutants) or selective (useful for e.g., kinase inhibitors) \n[36]. The Transformer can then be used to construct \ninverse PCM models which take the protein informa -\ntion as input (e.g., sequences, structures, or descriptors) \nto design active ligands for a given protein target with -\nout known ligands. Moreover, the Transformer can also \nbe used for lead optimization. For instance, the input \ncan be a “hit” already, generating “optimized” ligands, \nor a “lead” with side effects to produce ligands with a \nbetter ADME/tox profile.\nFig. 6 The chemical space of generated molecules by the Graph Transformer trained with different exploration rates in the RL framework. The \nchemical space was represented by t-SNE on ECFP6 fingerprints\nPage 13 of 14\nLiu et al. Journal of Cheminformatics           (2023) 15:24 \n \nSupplementary Information\nThe online version contains supplementary material available at https:// doi. \norg/ 10. 1186/ s13321- 023- 00694-z.\nAdditional file 1: Table S1. Atoms in vocabulary for graph-based \nmolecule generation. Table S2. The pseudo code for encoding the \ngraph representation of molecules in DrugEx v3. Table S3. The pseudo \ncode for decoding the graph representation of molecules in DrugEx v3. \nTable S4. Loadings of the PCA results of PCA on PhysChem descriptors \nbetween the molecules generated by pre-trained and fine-tuned Graph \nTransformer and the ChEMBL set and the LIGAND set, respectively. Figure \nS1. The distribution of Tanimoto similarity within training and test set, and \nbetween the sets for both the ChEMBL set (A) and the LIGAND set (B). \nFigure S2. The distribution of frequency of generated molecules based \non the same fragments as input. These ligands were generated from \npre-training (A) and fine-tuning (B) process. The molecules generated \nby the Graph Transformer model in the reinforcement learning process \n(C) were also counted the frequency of the same molecules for the \nsame input fragments. Figure S3. The distribution of Tanimoto similarity \nbetween generated ligands and the molecules in the training set. These \nligands were generated from pre-training (A) and fine-tuning (B) with four \ndifferent models. The similarity was compared with the molecules in the \nChEMBL, LIGAND sets, respectively. In addition, the ligands generated by \nthe Graph Transformer model in the reinforcement learning (C) process \nwith different hyperparameter ε were also compared the similarity with \nthe LIGAND set.\nAcknowledgements\nThanks go to Dr. Xue Yang for verifying Additional file 1: Table S1 and Dr. \nAnthe Janssen checking the convergence of t-SNE. We also acknowledge Bert \nBeerkens for providing the common scaffolds used to generate molecules as \nan example.\nAuthor contributions\nXL and GJPvW conceived the study and performed the experimental work \nand analysis. KY, APIJ and HWTvV provided feedback and critical input. All \nauthors read, commented on and approved the final manuscript.\nFunding\nXL thanks Chinese Scholarship Council (CSC) for funding, GJPvW thanks the \nDutch Research Council and Stichting Technologie Wetenschappen (STW) for \nfinancial support (STW-Veni #14410).\nAvailability of data and materials\nThe data used in this study is publicly available ChEMBL data, the algorithm \npublished in this manuscript is made available at https:// github. com/ CDDLe \niden/ DrugEx.\nDeclarations\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 9 April 2022   Accepted: 6 February 2023\nFig. 7 Sample of  molecules generated with the Graph Transformer with different scaffolds. These scaffolds include: furan, triazine, aminotriazole, \nxanthine, and azapurine. The generated molecules based on the same scaffolds are aligned in the same row. The predicted pChEMBL value for the \nmolecule is shown in the bottom right\nPage 14 of 14Liu et al. Journal of Cheminformatics           (2023) 15:24 \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \nReferences\n 1. Polishchuk PG, Madzhidov TI, Varnek A (2013) Estimation of the size of \ndrug-like chemical space based on GDB-17 data. J Comput Aided Mol \nDes 27(8):675–679\n 2. Hajduk PJ, Greer J (2007) A decade of fragment-based drug design: \nstrategic advances and lessons learned. Nat Rev Drug Discov \n6(3):211–219\n 3. Card GL, Blasdel L, England BP , Zhang C, Suzuki Y, Gillette S, Fong D, \nIbrahim PN, Artis DR, Bollag G et al (2005) A family of phosphodiesterase \ninhibitors discovered by cocrystallography and scaffold-based drug \ndesign. Nat Biotechnol 23(2):201–207\n 4. Bian Y, Xie XS (2018) Computational fragment-based drug design: current \ntrends, strategies, and applications. AAPS J 20(3):59\n 5. Hughes JP , Rees S, Kalindjian SB, Philpott KL (2011) Principles of early drug \ndiscovery. Br J Pharmacol 162(6):1239–1249\n 6. Sheng C, Zhang W (2013) Fragment informatics and computational \nfragment-based drug design: an overview and update. Med Res Rev \n33(3):554–598\n 7. Santos R, Ursu O, Gaulton A, Bento AP , Donadi RS, Bologa CG, Karlsson \nA, Al-Lazikani B, Hersey A, Oprea TI et al (2017) A comprehensive map of \nmolecular drug targets. Nat Rev Drug Discov 16(1):19–34\n 8. Fredholm BB (2010) Adenosine receptors as drug targets. Exp Cell Res \n316(8):1284–1288\n 9. Chen JF, Eltzschig HK, Fredholm BB (2013) Adenosine receptors as drug \ntargets–what are the challenges? Nat Rev Drug Discov 12(4):265–286\n 10. Moro S, Gao ZG, Jacobson KA, Spalluto G (2006) Progress in the pursuit of \ntherapeutic adenosine receptor antagonists. Med Res Rev 26(2):131–159\n 11. Jespers W, Oliveira A, Prieto-Diaz R, Majellaro M, Aqvist J, Sotelo E, \nGutierrez-de-Teran H (2017) Structure-Based Design of Potent and \nSelective Ligands at the Four Adenosine Receptors. Molecules 22(11): \n1945.\n 12. LeCun Y, Bengio Y, Hinton G (2015) Deep learning. Nature \n521(7553):436–444\n 13. Liu X, IJzerman AP , van Westen GJP (2021) Computational approaches \nfor de novo drug design: past, present, and future. Methods Mol Biol \n2190:139–165\n 14. Gomez-Bombarelli R, Wei JN, Duvenaud D, Hernandez-Lobato JM, \nSanchez-Lengeling B, Sheberla D, Aguilera-Iparraguirre J, Hirzel TD, \nAdams RP , Aspuru-Guzik A (2018) Automatic chemical design using \na data-driven continuous representation of molecules. ACS Cent Sci \n4(2):268–276\n 15. Segler MHS, Kogej T, Tyrchan C, Waller MP (2018) Generating focused \nmolecule libraries for drug discovery with recurrent neural networks. ACS \nCent Sci 4(1):120–131\n 16. Benjamin S-L, Carlos O, Gabriel L. G, Alan A-G (2017) Optimizing \ndistributions over molecular space. An Objective-Reinforced Generative \nAdversarial Network for Inverse-design Chemistry (ORGANIC).\n 17. Olivecrona M, Blaschke T, Engkvist O, Chen H (2017) Molecular de-novo \ndesign through deep reinforcement learning. J Cheminform 9(1):48\n 18. Blaschke T, Arus-Pous J, Chen H, Margreitter C, Tyrchan C, Engkvist O, \nPapadopoulos K, Patronov A (2020) REINVENT 2.0: An AI tool for de novo \ndrug design. J Chem Inf Model 60(12):5918–5922\n 19. Lim J, Hwang SY, Moon S, Kim S, Kim WY (2019) Scaffold-based molecular \ndesign with a graph generative model. Chem Sci 11(4):1153–1164\n 20. Li Y, Hu J, Wang Y, Zhou J, Zhang L, Liu Z (2020) DeepScaffold: a \ncomprehensive tool for scaffold-based de novo drug discovery using \ndeep learning. J Chem Inf Model 60(1):77–91\n 21. Arus-Pous J, Patronov A, Bjerrum EJ, Tyrchan C, Reymond JL, Chen H, \nEngkvist O (2020) SMILES-based deep generative scaffold decorator for \nde-novo drug design. J Cheminform 12(1):38\n 22. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L, \nPolosukhin IJae-p (2017) Attention is all you need. In.: arXiv: 1706. 03762.\n 23. Yang Y, Zheng S, Su S, Zhao C, Xu J, Chen H (2020) SyntaLinker: automatic \nfragment linking with deep conditional transformer neural networks. \nChem Sci 11(31):8312–8322\n 24. Liu X, Ye K, van Vlijmen HWT, IJzerman AP , van Westen GJP , (2019) An \nexploration strategy improves the diversity of de novo ligands using \ndeep reinforcement learning: a case for the adenosine A2A receptor. J \nCheminform 11(1):35\n 25. Liu X, Ye K, van Vlijmen HWT, Emmerich MTM, IJzerman AP , van Westen \nGJP (2021) DrugEx v2: de novo design of drug molecules by Pareto-based \nmulti-objective reinforcement learning in polypharmacology. J \nCheminform 13(1):85\n 26. Gaulton A, Bellis LJ, Bento AP , Chambers J, Davies M, Hersey A, Light \nY, McGlinchey S, Michalovich D, Al-Lazikani B et al (2012) ChEMBL: a \nlarge-scale bioactivity database for drug discovery. Nucleic Acids Res \n40(Database issue):D1100-1107\n 27. Papadatos G, Gaulton A, Hersey A, Overington JP (2015) Activity, assay \nand target data curation and quality in the ChEMBL database. J Comput \nAided Mol Des 29(9):885–896\n 28. Lenselink EB, Ten Dijke N, Bongers B, Papadatos G, van Vlijmen HWT, \nKowalczyk W, IJzerman AP , van Westen GJP , (2017) Beyond the hype: \ndeep neural networks outperform established methods using a ChEMBL \nbioactivity benchmark set. J Cheminform 9(1):45\n 29. Degen J, Wegscheid-Gerlach C, Zaliani A, Rarey M (2008) On the \nart of compiling and using “drug-like” chemical fragment spaces. \nChemMedChem 3(10):1503–1507\n 30. PyTorch. https:// pytor ch. org/.\n 31. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner \nT, Dehghani M, Minderer M, Heigold G, Gelly S et al (2020) An image is \nworth 16x16 words: transformers for image recognition at scale. In.: arXiv: \n2010. 11929.\n 32. Bickerton GR, Paolini GV, Besnard J, Muresan S, Hopkins AL (2012) \nQuantifying the chemical beauty of drugs. Nat Chem 4(2):90–98\n 33. Scikit-Learn: machine learning in Python. http:// www. scikit- learn. org/.\n 34. Ertl P , Schuffenhauer A (2009) Estimation of synthetic accessibility score \nof drug-like molecules based on molecular complexity and fragment \ncontributions. J Cheminform 1(1):8\n 35. Solow AR, Polasky S (1994) Measuring biological diversity. Environ Ecol \nStat 1(2):95–103\n 36. van Westen GJ, Wegner JK, Geluykens P , Kwanten L, Vereycken I, Peeters A, \nIjzerman AP , van Vlijmen HW, Bender A (2011) Which compound to select \nin lead optimization? Prospectively validated proteochemometric models \nguide preclinical development. PLoS ONE 6(11):e27518\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Reinforcement learning",
  "concepts": [
    {
      "name": "Reinforcement learning",
      "score": 0.837026059627533
    },
    {
      "name": "Computer science",
      "score": 0.7659139037132263
    },
    {
      "name": "Transformer",
      "score": 0.611026406288147
    },
    {
      "name": "Encoder",
      "score": 0.4733583927154541
    },
    {
      "name": "Drug discovery",
      "score": 0.44307440519332886
    },
    {
      "name": "Scaffold",
      "score": 0.4400756061077118
    },
    {
      "name": "Chemical space",
      "score": 0.4200684130191803
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40053844451904297
    },
    {
      "name": "Chemistry",
      "score": 0.16978180408477783
    },
    {
      "name": "Programming language",
      "score": 0.10366129875183105
    },
    {
      "name": "Engineering",
      "score": 0.09666338562965393
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210133171",
      "name": "Centre for Human Drug Research",
      "country": "NL"
    },
    {
      "id": "https://openalex.org/I87445476",
      "name": "Xi'an Jiaotong University",
      "country": "CN"
    }
  ],
  "cited_by": 70
}