{
  "title": "TransformEHR: transformer-based encoder-decoder generative model to enhance prediction of disease outcomes using electronic health records",
  "url": "https://openalex.org/W4389132345",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2107222514",
      "name": "Zhichao Yang",
      "affiliations": [
        "Amherst College",
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A2536217553",
      "name": "Avijit Mitra",
      "affiliations": [
        "University of Massachusetts Amherst",
        "Amherst College"
      ]
    },
    {
      "id": "https://openalex.org/A2293447485",
      "name": "Weisong Liu",
      "affiliations": [
        "VA New England Healthcare System",
        "University of Massachusetts Lowell"
      ]
    },
    {
      "id": "https://openalex.org/A2608553023",
      "name": "Dan Berlowitz",
      "affiliations": [
        "VA New England Healthcare System",
        "University of Massachusetts Lowell"
      ]
    },
    {
      "id": "https://openalex.org/A2111112795",
      "name": "Hong Yu",
      "affiliations": [
        "Amherst College",
        "VA New England Healthcare System",
        "University of Massachusetts Lowell",
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A2107222514",
      "name": "Zhichao Yang",
      "affiliations": [
        "Amherst College",
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A2536217553",
      "name": "Avijit Mitra",
      "affiliations": [
        "University of Massachusetts Amherst",
        "Amherst College"
      ]
    },
    {
      "id": "https://openalex.org/A2293447485",
      "name": "Weisong Liu",
      "affiliations": [
        "Bedford VA Research Corporation",
        "VA New England Healthcare System",
        "University of Massachusetts Lowell"
      ]
    },
    {
      "id": "https://openalex.org/A2608553023",
      "name": "Dan Berlowitz",
      "affiliations": [
        "University of Massachusetts Lowell",
        "VA New England Healthcare System",
        "Bedford VA Research Corporation"
      ]
    },
    {
      "id": "https://openalex.org/A2111112795",
      "name": "Hong Yu",
      "affiliations": [
        "Bedford VA Research Corporation",
        "University of Massachusetts Lowell",
        "University of Massachusetts Amherst",
        "Amherst College",
        "VA New England Healthcare System"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3021687195",
    "https://openalex.org/W3183011410",
    "https://openalex.org/W3124926562",
    "https://openalex.org/W4385381606",
    "https://openalex.org/W2517259736",
    "https://openalex.org/W2769506033",
    "https://openalex.org/W2964959375",
    "https://openalex.org/W3043363778",
    "https://openalex.org/W2041674474",
    "https://openalex.org/W4205164650",
    "https://openalex.org/W2897007327",
    "https://openalex.org/W4312220150",
    "https://openalex.org/W3160137267",
    "https://openalex.org/W3017637887",
    "https://openalex.org/W3089168780",
    "https://openalex.org/W3213708588",
    "https://openalex.org/W2150845035",
    "https://openalex.org/W2087598211",
    "https://openalex.org/W2069724389",
    "https://openalex.org/W4327675663",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2784499877",
    "https://openalex.org/W3130608143",
    "https://openalex.org/W3137644897",
    "https://openalex.org/W2165698076",
    "https://openalex.org/W4313439128",
    "https://openalex.org/W3197559541",
    "https://openalex.org/W1500925570",
    "https://openalex.org/W3083277181",
    "https://openalex.org/W1966716734",
    "https://openalex.org/W2605512411",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2404901863",
    "https://openalex.org/W3136738164",
    "https://openalex.org/W2129112927",
    "https://openalex.org/W2094043475",
    "https://openalex.org/W2792170027",
    "https://openalex.org/W2804266670",
    "https://openalex.org/W2752429071",
    "https://openalex.org/W3201540941",
    "https://openalex.org/W3005713150",
    "https://openalex.org/W1965454346",
    "https://openalex.org/W3007372240",
    "https://openalex.org/W4293162168",
    "https://openalex.org/W4375858857",
    "https://openalex.org/W2070605700",
    "https://openalex.org/W2609573654",
    "https://openalex.org/W2655730963",
    "https://openalex.org/W2899980768",
    "https://openalex.org/W2985209668",
    "https://openalex.org/W2479768810",
    "https://openalex.org/W3133650345",
    "https://openalex.org/W3098949126",
    "https://openalex.org/W2072727975"
  ],
  "abstract": "Abstract Deep learning transformer-based models using longitudinal electronic health records (EHRs) have shown a great success in prediction of clinical diseases or outcomes. Pretraining on a large dataset can help such models map the input space better and boost their performance on relevant tasks through finetuning with limited data. In this study, we present TransformEHR, a generative encoder-decoder model with transformer that is pretrained using a new pretraining objective—predicting all diseases and outcomes of a patient at a future visit from previous visits. TransformEHR’s encoder-decoder framework, paired with the novel pretraining objective, helps it achieve the new state-of-the-art performance on multiple clinical prediction tasks. Comparing with the previous model, TransformEHR improves area under the precision–recall curve by 2% ( p &lt; 0.001) for pancreatic cancer onset and by 24% ( p = 0.007) for intentional self-harm in patients with post-traumatic stress disorder. The high performance in predicting intentional self-harm shows the potential of TransformEHR in building effective clinical intervention systems. TransformEHR is also generalizable and can be easily finetuned for clinical prediction tasks with limited data.",
  "full_text": "Article https://doi.org/10.1038/s41467-023-43715-z\nTransformEHR: transformer-based encoder-\ndecoder generative model to enhance\nprediction of disease outcomes using\nelectronic health records\nZhichao Yang 1, Avijit Mitra1, Weisong Liu2,3,D a nB e r l o w i t z3,4 &\nHong Yu 1,2,3,5\nDeep learning transformer-based models using longitudinal electronic health\nrecords (EHRs) have shown a great successin prediction of clinical diseases or\noutcomes. Pretraining on a large dataset can help such models map the input\nspace better and boost their performance on relevant tasks throughﬁnetuning\nwith limited data. In this study, we present TransformEHR, a generative\nencoder-decoder model with transform e rt h a ti sp r e t r a i n e du s i n gan e wp r e -\ntraining objective— predicting all diseases and outcomes of a patient at a future\nvisit from previous visits. TransformEHR’s encoder-decoder framework,\npaired with the novel pretraining objective, helps it achieve the new state-of-\nthe-art performance on multiple clinical prediction tasks. Comparing with the\nprevious model, TransformEHR improves area under the precision–recall\ncurve by 2% (p < 0.001) for pancreatic cancer onset and by 24% (p =0 . 0 0 7 )f o r\nintentional self-harm in patients with post-traumatic stress disorder. The high\nperformance in predicting intentional self-harm shows the potential of\nTransformEHR in building effective clinical intervention systems. Transfor-\nmEHR is also generalizable and can be easilyﬁnetuned for clinical prediction\ntasks with limited data.\nThe widespread adoption of electronic health records (EHRs) among\nthe US hospitals has led to the development and adoption of numer-\nous data mining and statistical techniques for EHRs. Longitudinal EHRs\nhave been successfully used to predict clinical diseases or outcomes\n1–4.\nEarly work applied regression and traditional machine learning (ML)\nbased models (e.g., support vectors machines, random forest, and\ngradient boosting) to predict single disease or outcome. Examples\ninclude congestive heart failure\n5, sepsis mortality 6, mechanical\nventilation6,s e p t i cs h o c k7, type 2 diabetes8,a n dd e v e l o p m e n to fp o s t -\ntraumatic stress disorder (PTSD)9, among others.\nWith the availability of large cohorts and computational resour-\nces, deep learning based models can outperform traditional ML\nmodels\n10–16. State-of-the-art (SOTA) models in EHR-based predictive\nmodeling achieved this through the pretrain-ﬁnetune paradigm - a\ntwo-step process where the model isﬁrst trained on large-scale long-\nitudinal EHRs to learn the representations of clinical features such as,\nInternational Classiﬁcation of Diseases (ICD) codes (pretrain) and then\nfurther trained to adapt to speciﬁc tasks e.g., outcome prediction\n(ﬁnetune). Models such as Med-BERT\n13,B E H R T14,a n dB R L T M15 fall in\nthis category. However, their pretraining objectives were limited in\nReceived: 3 May 2023\nAccepted: 17 November 2023\nCheck for updates\n1College of Information and Computer Science, University of Massachusetts Amherst, Amherst, MA, USA.2School of Computer & Information Sciences,\nUniversity of Massachusetts Lowell, Lowell, MA, USA.3Center for Healthcare Organization and Implementation Research, VA Bedford Health Care System,\nBedford, MA, USA.4Department of Public Health, University of Massachusetts Lowell, Lowell, MA, USA.5Center for Biomedical and Health Research in Data\nSciences, University of Massachusetts Lowell, Lowell, MA, USA.e-mail: Hong_Yu@uml.edu\nNature Communications|         (2023) 14:7857 1\n1234567890():,;\n1234567890():,;\npredicting a fraction of ICD codes within each visit. In reality, most\npatients have multiple diseases or outcomes at once17,m a n yo fw h i c h\nare highly correlated (such as obesity, diabetes, and hypertension18–20)\nand thus collectively contribute to the disease or outcome trajectories.\nTherefore, a novel pretraining strategy, which predicts the complete\nset of diseases and outcomes within a visit, might improve clinical\npredictive modeling.\nIn this study, we propose TransformEHR, an innovative denoising\nsequence to sequence transformer\n21 model that was pretrained on 6.5\nmillion patients’ EHRs to predict complete ICD codes of a visit.\nTransformEHR can be furtherﬁnetuned for single disease or outcome\npredictions. Unlike previous EHR-based models13–16 which rely on the\nbidirectional (left-to-right and right-to-left) encoder representation\nfrom transformers (BERT) framework\n22,T r a n s f o r m E H Ru s e da\ntransformer-based encoder-decoder generative framework to predict\nfuture ICD codes during pretraining. The unidirectional (left-to-right)\ndecoder in such an encoder-decoder framework is more similar to the\nuse case of future disease or outcome predictions based on history of\npast diseases or outcomes (past-to-future) compared to the bidirec-\ntional encoder-only framework.\nAlthough the encoder-decoder framework was originally\ndesigned to generate next sentence given previous sentences as\ncontext\n23,24, we repurposed the framework for TransformEHR to gen-\nerate the ICD codes of the next visit given previous EHRs (context).\nTransformEHR can utilize cross-attention\n21 by identifying relevant ICD\ncodes from previous visits to predict future ICD codes. The decoder\nthen predicts ICD codes one after another by using already predicted\ndiagnostic ICD codes to predict next ICD codes. Furthermore, Trans-\nformEHR includes date of each visit to integrate temporal information,\nwhereas previous transformer-based predictive models only included\ntheir sequential order\n13–16.S p e c iﬁc date of each visit is an important\nfeature in predictive modeling as importance of predictor in a visit can\nvary over time\n1,25–27.\nWe evaluated TransformEHR for a broad range of disease and\noutcome predictions. In addition to predictions of ICD codes, we\nevaluated TransformEHR on two challenging and clinically important\ndisease and outcome prediction tasks: pancreatic cancer prediction\nand intentional self-harm prediction among PTSD patients. In sum-\nmary, our key contributions are as follows:\nFirst, we propose a new pretraining objective that predicts all\ndiseases or outcomes of a future visit using longitudinal information\nfrom the previous visits. Such a pretraining objective helps Transfor-\nmEHR uncover the complex interrelations among different diseases\nand outcomes.\nSecond, this is theﬁrst study that explored a generative encoder-\ndecoder framework to predict patients’ ICD codes using their long-\nitudinal EHRs. Our encoder-decoder framework outperformed the\nencoder-based models in part due to the decoder self-attention and\ncross-attention mechanisms. TransformEHR outperformed SOTA\nBERT models on both common and uncommon ICD code predictions.\nIn particular, the improvements for uncommon ICD code predictions\nwere substantial.\nThird, TransformEHR achieved a positive predictive value (PPV) of\n8.8% for prediction of intentional self-harm among the top 10% PTSD\npatients at high predicted risk. A recent study has shown that a prac-\ntical suicide prevention tool must achieve above 1.7% PPV to be con-\nsidered as cost-effective: balance the costs of providing the\nintervention against the potential health care related costs if self-harm\noccurs\n28. A PPV of 8.8% is substantially above the threshold of 1.7%\nneeded to balance the cost in clinical practice. This shows the potential\nof deploying TransformEHR for clinical screening and interventions.\nFinally, we validated the generalizability of TransformEHR using\nboth internal and external datasets. Internally, we evaluated Trans-\nformEHR on never-seen in-domain EHR data from Veterans Health\nAdministrations (VHA) facilities. Externally, we evaluated\nTransformEHR on out-of-domain data from a non-VHA hospital. Our\nresults demonstrated a strong transfer learning\n29 capability of Trans-\nformEHR, which can greatly beneﬁt hospitals with limited data and\ncomputing resources.\nResults\nData\nOur pretraining cohort comprises 6,475,218 patients who received\ncare from more than 1200 health care facilities of the US VHA from 1/1/\n2016 to 12/31/2019. To evaluate pretrained models, we created two\ndisease/outcome agnostic prediction (DOAP) datasets— one for com-\nmon and one for uncommon diseases/outcomes. We selected 10 ICD-\n10CM codes with the highest prevalence (prevalence ratio >2%) in our\npretraining cohort for our common disease/outcome DOAP dataset.\nAs for the set of uncommon diseases/outcomes, we followed the FDA\nguidelines\n30 to randomly select 10 ICD-10CM codes with a prevalence\nratio ranging from 0.04% to 0.05% in our pretraining cohort. The lists\nof common and uncommon diseases/outcomes are shown in Table1.\nThese codes were assigned by VHA medical record technician and\nserved several important purposes including clinical studies, perfor-\nmance measurement, workload capture and operation, cost determi-\nnation, and billing. To assess the generalizability, we evaluated\nTransformEHR on out-of-domain non-VHA EHR data. We used the\nMIMIC-IV dataset\n31 to build a non-VHA DOAP dataset. The MIMIC-IV\ndataset includes intensive care unit patients admitted to the Beth Israel\nDeaconess Medical Center in Boston, Massachusetts. Since the dataset\ncontains information from 2008 to 2019 but the implementation of\nICD-10CM started from October 2015, we only selected patients with\nthe ICD-10CM records to match our implementation for the cohorts\nfrom VHA, resulting in a dataset of 29,482 patients.\nTo evaluate ﬁnetuned models for single disease and outcome\npredictions with low prevalence ratio, we created two EHR datasets for\ntwo important prediction tasks: pancreatic cancer (single disease) and\nintentional self-harm among patients with PTSD (single outcome).\nEarly screening, early diagnosis and early treatment of pancreatic\nTable 1 | Disease or outcome deﬁnitions in this study\nTask Disease or Outcome (ICD-10CM code)\nPrediction of single disease or\noutcome\nNew onset pancreatic cancer (C25)\nIntentional self-harm among patients\nwith PTSD\nDisease/outcome agnostic\nprediction - common\nChronic post-traumatic stress disorder\n(F43.12)\nType 2 diabetes (E11.9)\nHyperlipidemia (E78.5)\nLoin pain (R10.3)\nLow back pain (M54.50)\nO b s t r u c t i v es l e e pa p n e a( G 4 7 . 3 3 )\nDepression (F33.9)\nObstructive airway disease (J44.9)\nGastroesophageal reﬂux disease (K21.9)\nArteriosclerosis (I25.10)\nDisease/outcome agnostic\nprediction - uncommon\nBenign neoplasm of connective tissue of\neyelid (D21.0)\nRefractory anemia (D46.4)\nMelanocytic nevi of upper limb (D22.6)\nBenign neoplasm of skin of upper eyelid\n(D23.10)\nCutaneous abscess of axilla (L02.41)\nAnkle and foot subacute osteomyelitis\n(M86.27)\nCortical hemisphere nontraumatic hemor-\nrhage intracerebral (I61.1)\nMalignant neoplasm of head of pancreas\n(C25.0)\nOther complication of kidney transplant\n(T86.19)\nNonexudative age-related macular degen-\neration (H35.31)\nArticle https://doi.org/10.1038/s41467-023-43715-z\nNature Communications|         (2023) 14:7857 2\ncancer are critical for this deadly disease32, 33. Med-BERT13 has been\nused to predict pancreatic cancer. Intentional self-harm is common\namong the US military Veterans with PTSD\n34. The detailed pretraining\nand ﬁnetuning cohort deﬁnitions are presented in Methods section.\nWe compare these cohorts in Supplementary Table 1. To assess the\ngeneralizability, we also evaluated TransformEHR on Veterans with\nPTSD from VHA facilities not included in the pretraining cohort.\nLongitudinal EHRs\nAs shown in Fig.1, TransformEHR takes longitudinal EHRs as input. To\ncompare TransformEHR with the previous SOTA EHR-based models\nusing BERT\n13,15, we included demographic information and ICD-10CM\ncodes as predictors. Demographic information includes gender, age,\nrace, and marital status. The attributes (e.g., male) of each category\n(e.g., gender) were appended as individual predictors. Following pre-\nvious work\n13, we grouped ICD codes at visit level. Within a visit, we\nordered ICD codes from high to low priority, as assigned by health care\nproviders, where the primary diagnosis is typically given the highest\npriority, followed by the secondary diagnosis and so on.\nMultiple visits of each patient formed a time-stamped (by date of\nvisit) input of a sequence of ICD-10CM code groups (Fig.2a). We used\nmulti-level embeddings\n13. Embeddings are trainableﬁxed-dimensional\nvectors that were used to represent predictors in a continuous vector\nspace and were learned during the pretraining process (details in the\nnext section). We represented each visit in an input sequence using a\nvisit embedding and each ICD code using a code embedding. To\nembed the time, we applied sinusoidal position embedding\n21 to the\nnumerical format of visit date (date-speciﬁc). However, the use of\nactual visit dates, which is protected health information sensitive, may\nimpact the deployability of the model. Thus, we also explored using\nt h er e l a t i v et i m ei n f o r m a t i o n— days difference to embed time (day-\nsdiff). Speciﬁcally, we calculate the days difference between a certain\nvisit and the last visit in the EHR. Finally, each input embedding was\nconstructed by summing up the corresponding code embedding, visit\nembedding, and time embedding (Fig.2a).\nPretrain-Finetune paradigm\nWith longitudinal EHRs as input, weﬁrst pretrained models on the\npretraining cohort of 6,475,218 patients and thenﬁnetuned the model\nfor single disease or outcome prediction, as shown in Fig.3. During the\npretraining, the model was trained to recover the original longitudinal\nEHRs from corrupted (masked) longitudinal EHRs.\nPrevious EHR-based BERT models corrupted longitudinal EHRs by\nrandomly sampling 25% ICD codes and replacing them with mask\n(code masking)\n13–15. TransformEHR, on the other hand, masked all ICD\ncodes in a single visit (visit masking). A comparative example is illu-\nstrated in Fig.2b. In other words, TransformEHR was pretrained to\npredict the complete set of ICD codes of a patient’s future visit, given\ndemographic information and longitudinal ICD codes up to the cur-\nrent visit.\nTransformEHR architecture\nTransformEHR uses an encoder–decoder transformer architecture23,24.\nThe encoder processes the input embeddings and generates a set of\nhidden representations for each predictor. Unlike the encoder-only\ntransformer architecture used by existing EHR-based BERT models\n13–15,\nTransformEHR performs cross-attention over the hidden representa-\ntions from the encoder and assigns an attention weight for each\nrepresentation. These weighted representations are then fed to the\ndecoder, which generates ICD codes of the future visit as illustrated in\nFig.3. The decoder generates ICD codes following the sequential order\nof code priority within a visit. In other words, itﬁrst generates primary\ndiagnosis, and then generates secondary diagnosis based on primary\ndiagnosis. This step is repeated until all diagnoses of a future visit are\nAbdominal\nPain\nFibromatosis\nDepressive\nEpisode\nDiabetes Mellitus\nVisit 1 Visit 2 Visit 3 Visit 4\n?\nAge: 51\nGender: M\nRace: White\nNow2016-01-01 2016-11-042016-05-23\nDiabetes MellitusExam to\nEducational\nInstitution\nFig. 1 | Exemplar EHR sequence of a patient (white, male, age 51).This EHR\ncontains demographic and ICD codes from 3 previous visits, including Z-codes such\nas exam to educational institution Z02.0. The pretraining objective is to predict all\ndiseases and outcomes in the next visit. All these values are artiﬁcial and for illus-\ntration purposes.\nRace \nWhite\nVisit\nEmbeddings\nTime\nEmbeddings\n2017-03-12\n0 days\n1\n2016-01-01\n436 days\n2016-05-23\n293 days\n2017-03-12\n0 days\n3 42\nGender \nMale\n2016-11-04\n128 days\nR10.9\nM72.0\nF43.12\nH53.8\nM54.5\nE11.9\nZ02.0\n+ + + + ++ ++\n+ + + +\n+\nCode/Demographic\nEmbeddings\na.\nb.\nR10.9\nM72.0\nMASK\nM54.5\nMASK\nZ02.0\nE11.9\nBERT\nMASK\nE11.9\nZ02.0\nE11.9\nF43.12\nR10.9\nM72.0\nF43.12\nH53.8\nM54.5\nE11.9\nZ02.0\nR10.9\nM72.0\nF43.12\nAge\n51\nE11.9\n4\n+\nTransformEHR\nFig. 2 | How model learns the correlation of ICD codes by recovering the\nmasked ICD codes to its original ICD codes. aillustrates the preprocessing of\ninput data from 4 patient visits, with visit embeddings capturing chronological\ninformation, time embeddings capturing the speciﬁc date, and ICD codes along\nwith demographic data encoded as embeddings.b displays various medical history\nmasking schemes: BERT-style masking random codes and TransformEHR-style\nmasking the all codes in a visit.\nArticle https://doi.org/10.1038/s41467-023-43715-z\nNature Communications|         (2023) 14:7857 3\ncompleted. Our results showed that the TransformEHR architecture\noutperformed the encoder-only architecture.\nEvaluation metrics\nWe reported PPV (precision), area under the receiver operating char-\nacteristic curve (AUROC), and area under the precision recall curve\n(AUPRC) to measure models’ performance on single disease or out-\ncome predictions. PPV is the fraction of true positives from all pre-\ndicted positives. AUROC is the area under sensitivity and false positive\nrate curve. Sensitivity (or recall) is the number of true positives divided\nby the number of ground truth positives. False positive rate is the\nnumber of false positives divided by total number of negatives. AUPRC\nis the area under PPV and sensitivity curve, and it has shown to be an\neffective measure for highly imbalanced binary classiﬁcation pro-\nblems, which include self-harm prediction\n35,36.W ec o m p a r e dT r a n s -\nformEHR with four strong baseline models: logistic regression, long\nshort-term memory (LSTM)\n37,B E R T22 without pretraining, and BERT22\npretrained on VHA cohort.\nPretraining evaluation: disease or outcome agnostic prediction\nDOAP is the task of predicting the ICD codes of a patient’s future visit\nbased on patient’s demographic information and longitudinal ICD\ncodes up to the current visit. Compared to BERT, TransformEHR\nimproved the AUROC in all prediction subtasks, regardless of disease/\noutcome category (common or uncommon) and occurrence type (new\nor recurrent), as shown in Table2. We found a 3.96% relative increase\nin common diseases/outcomes and 5.92% in uncommon diseases/\noutcomes.\nOur TransformEHR contains three unique components compared\nto previous medical BERT-based models: (1) visit masking, (2) encoder-\ndecoder architecture, and (3) time embedding. We performed ablation\nanalysis to evaluate the effectiveness of each component. First, we\ncompared an encoder-decoder model that uses visit masking, i.e.,\nmasking all ICD codes of a visit, to another encoder-decoder model\nthat uses code masking, i.e., masking a randomly selected fragment of\nICD codes of a visit. Ourﬁndings revealed that the visit masking per-\nformed better (showing an improvement of 95%CI: 2.52%–2.96%,\nVisit: 23\nMigraine\nBack Pain\n[MASK]\n1\nCataract\nDepressive\nEpisode\n4 Back Pain\nFever\nEncoder\nDecoder\nCross Attention\nEmbedding\nBefore\nPretrain\nPTSD\nStep 1:\nBig Data Pretrain\nMajor Depressive Recurrent Epi., Severe\nAlcohol Dependence\n...\nRheumatic Bone\nKlinefelter\nEncoder Decoder\nStep 2:\nDownstream Finetune\nEncoder Decoder\nIntentional self-harm before next visit?\nNo\nYes\nNo\nYes\nIntentional self-harm before next visit?\nMajor Depressive Recurrent Epi., Severe\nF33.3\nAlcohol Dependence\nF10.2\nMajor Depressive Single Epi., Mild\nF32.0\nAggressive Personality\nF60.3\nProbability distribution for next icd code in this visit\nChronic Migraine Without Aura, Intractable\nG43.71\nGeneralized Anxiety Disorder\nF41.1\nHeadache\nSneezing\nInput Source\nActual disease patient had 1.00.0\nHeadache\nR51.0\nEHR\nRecords:\nTop Indicators\nAfter\nPretrain\nOur Model\nTransformEHR\nTransformEHR\nPreviously predicted\ncomorbidities\nBack Pain\nTransformEHR\nFig. 3 | TransformEHR architecture and pretrain-ﬁnetune paradigm.During\nStep 1, TransformEHR was pretrained with generative encoder-decoder transfor-\nmer on a large set of longitudinal EHR data. TransformEHR learned the probability\ndistribution of ICD codes (vs. random distribution) through correlation of cross\nattention. DuringStep 2,w et h e nﬁnetuned TransformEHR to the predictions of\nsingle disease or outcome. Through attention weights, TransformEHR was able to\nidentify top indicators for the predictions. Encoder is colored in green, decoder is\ncolored in red, and cross attention that connects both is colored in blue.\nArticle https://doi.org/10.1038/s41467-023-43715-z\nNature Communications|         (2023) 14:7857 4\np < 0.001 in AUROC) compared to code masking for all diseases/out-\ncomes tested as shown in Supplementary Table 2. These results\ndemonstrated that pretraining for prediction of all diseases and out-\ncomes outperform traditional pretraining objectives.\nNext, we compared encoder-decoder model with encoder-only\nmodel (BERT) on DOAP. Our ﬁndings showed that the encoder-\ndecoder model outperformed the encoder-only model (with an\nimprovement of 95%CI: 0.74%–1.16%, p < 0.001 in AUROC) across all\ndiseases/outcomes tested (Supplementary Table 2).\nFinally, we conducted an experiment by excluding time embed-\nding from the implementation of TransformEHR. Time embedding\nallows TransformEHR to capture the temporal information of prior\nvisits. Our results indicated that TransformEHR with date-speciﬁct i m e\nembedding exhibits a moderate improvement (with an increase of\n95% CI: 0.01%, 0.43%,p = 0.021 in AUROC) than TransformEHR without\ntime embedding across most diseases/outcomes (Supplementary\nTable 2). Instead of speciﬁcd a t ea st i m ee m b e d d i n g ,w ea l s ot r i e dd a y s\ndifference between visits as time embedding, TransformEHR with\ndate-speciﬁc embedding showed no signiﬁcant improvement than\nTransformEHR with days difference embedding. Thus, we chose days\ndifference embedding for later experiments as it provides better\nprotection to patient health information compared to the spe-\nciﬁcd a t e .\nFinetuning evaluation: single disease or outcome prediction\nResults of pancreatic cancer onset prediction are shown in Table3.O n\nthe metric of AUROC, TransformEHR achieved 81.95 (95% CI: 81.06,\n82.85) outperforming both the logistic regression model (73.64; 95%\nCI: 71.39, 75.90, p < 0.001), LSTM (76.98; 95% CI: 76.44, 77.52,\np < 0.001) and BERT (79.22; 95% CI: 78.75, 79.69,p < 0.001). A similar\ntrend is observed on AUPRC. TransformEHR achieved AUPRC of 78.64\n(95% CI: 77.80, 79.49) outperforming both the logistic regression\nmodel (68.95; 95% CI: 66.81, 71.08,p < 0.001), LSTM (73.48; 95% CI:\n72.93, 74.03, p < 0.001) and BERT (76.89; 95% CI: 76.41,\n77.37, p < 0.001).\nPredicting intentional self-harm in patients with PTSD is challen-\nging because of its low prevalence (1.9%). As shown in Table4,A U P R C\nof TransformEHR was 16.67 (95% CI, 15.11, 18.23,p = 0.007), out-\nperforming BERT (13.34; 95% CI: 12.00, 15.11,p < 0.001), LSTM (8.36;\n95% CI: 7.56, 9.16,p < 0.001), and logistic regression (3.15; 95% CI: 2.39,\n3.92, p < 0.001) by 24%, 97%, and 422% respectively. A similar trend is\nobserved for AUROC. We also calculated sensitivity and PPV for a\nvariety of thresholds. As shown in Supplementary Table 3, The PPV of\nTransformEHR ranges from 3.14 to 8.80 for 10% to 60% threshold.\nSubgroup analyses among different demographics. The results of\nthe subgroup analyses are shown in Supplementary Table 4. AUPRC\nwas consistent among different genders, ages, races, and marital sta-\ntus. For example, the AUPRC of patients who were more than 80-year-\nold (4.86% of the cohort) was 18.08 (95% CI, 16.05, 19.66), which was\nnot statistically different from the AUPRC of patients whose ages range\nfrom 30 to 39 (19.58% of the cohort): 16.29 (95% CI, 14.31, 18.27).\nEffect of historical EHR length on performance. To examine the\nimpact of how patients’prior history impacts model predictions, we\nconducted two experiments -“self-harm with short history” (included\nonly the ﬁve most recent visits prior to self-harm) and“self-harm”\nincluded all visits prior to self-harm. As shown in Table4,T h eA U P R Co f\nTransformEHR using only theﬁve prior visits was 13.77. Using all visits,\nthe AUPRC improved by 19% to 16.67.\nTable 3 | Performance of models for pancreatic cancer\nprediction\nModels AUROC AUPRC\nWithout Pretraining Logistic regression 73.64 ±2.26 68.95 ±2.14\nLSTM 76.98 ±0.54 73.48 ±0.55\nBERT without pretraining 77.27 ±0.45 74.00 ±0.31\nWith Pretraining BERT 79.22 ±0.47 76.89 ±0.48\nTransformEHR (ours) 81.95 ±0.90 78.64 ±0.85\nResult is calculated from best hyperparameters with 5 randomized seeds each. ± represents\nstandard deviation.\nTable 4 | Performance (and standard deviation) of predictive\nmodels for intentional self-harm\nModels Self-Harm w/\nShort History\nSelf-Harm w/ Full\nHistory\nAUPRC AUROC AUPRC AUROC\nWithout\nPretraining\nLogistic\nregression\n6.89 66.87 3.15 64.60\n±1.55 ±0.60 ±0.77 ±3.73\nLSTM 9.13 71.46 8.36 69.36\n±0.74 ±0.13 ±0.80 ±0.83\nBERT without\npretraining\n9.39 71.78 10.98 72.53\n±0.30 ±0.18 ±0.66 ±0.69\nWith\nPretraining\nBERT 10.30 71.87 13.34 78.02\n±0.83 ±0.79 ±1.34 ±1.84\nTransformerEHR 13.77 74.89 16.67 79.90\n±0.69 ±0.77 ±1.56 ±1.73\n“Self-Harm w/ Full History” refers to cases where the prediction is based on the original EHR\n(mean: 10.1 visits, st.dev.: 3.3 visits) prior to predicting intentional self-harm.“Self-Harm w/ Short\nHistory” includes only the 5 most recent visits. ± represents standard deviation.\nResult is calculated from best hyperparameters with 5 randomized seeds each.\nTable 2 | Disease/outcome agnostic prediction: AUROC\nscores on different pretraining objectives for the 10 common\nand 10 uncommon diseases in Table 1\nModels BERT TransformEHR\nChronic PTSD R 81.04 ±0.11 83.73 ±0.07\n0 76.74 ±0.17 77.95 ±0.12\nType 2 diabetes R 85.00 ±0.10 85.72 ±0.07\n0 79.97 ±0.04 81.84 ±0.05\nHyperlipidemia R 86.78 ±0.03 88.04 ±0.05\n0 81.28 ±0.08 83.42 ±0.08\nLoin pain R 81.47 ±0.04 88.24 ±0.05\n0 76.88 ±0.12 85.37 ±0.08\nLow back pain R 85.43 ±0.07 86.94 ±0.03\n0 80.16 ±0.07 82.30 ±0.10\nObstructive sleep apnea R 80.74 ±0.17 82.25 ±0.16\n0 73.06 ±0.08 74.69 ±0.19\nDepression R 86.73 ±0.05 87.66 ±0.12\n0 82.60 ±0.12 83.85 ±0.11\nObstructive airway\ndisease\nR 83.57 ±0.14 86.19 ±0.07\n0 76.99 ±0.08 80.27 ±0.07\nGastroesophageal reﬂux R 84.98 ±0.28 91.07 ±0.11\n0 76.29 ±0.36 83.41 ±0.33\nArteriosclerosis R 82.21 ±0.06 88.79 ±0.10\n0 75.78 ±0.08 80.03 ±0.20\nUncommon disease/\noutcome\n0 75.63 ±0.12 80.11 ±0.12\nMany common diseases are chronic in nature. We therefore study whether prior history of the\nsame disease has an impact on prediction performance, where R is recurrent and 0 is new\ndisease onset. ± represents standard deviation.\nArticle https://doi.org/10.1038/s41467-023-43715-z\nNature Communications|         (2023) 14:7857 5\nGeneralizability evaluation\nWe validated the generalizability of TransformEHR both internally and\nexternally. Out of 1239 VHA health care facilities, we found that EHR\ndata from 121 facilities were not included in the pretraining cohort. We\ncreated an internal generalizability evaluation dataset using this subset\nof EHR data for the intentional self-harm prediction task among PTSD\np a t i e n t s .U p o ne v a l u a t i n gT r a n s f o r m E H Ro nt h i sd a t a s e t ,o u rr e s u l t s\nshowed no statistical difference in AUPRC between data from unseen\nfacilities (1st bar on the left of Supplementary Fig. 1) and data from\nfacilities where at least some of the data were included in the pre-\ntraining cohort (other bars of Supplementary Fig. 1).\nTo evaluate TransformEHR’s generalizability on the external\ndataset, we furtherﬁnetuned the pretrained TransformEHR and BERT\non the training set of a non-VHA DOAP dataset and evaluated them on\nthe testing set. The distribution of the ICD codes assigned to an ICU\npopulation would be different from the ICD distribution in the VA, as\nshown in Supplementary Fig. 2. The evaluation result is shown in\nSupplementary Table 5. Compared to TransformEHR without pre-\ntraining, TransformEHR with pretraining improved AUROC by 2.3%\n(95% CI, 0.8%, 3.6%,p = 0.005). In comparison, BERT with pretraining\noutperformed BERT without pretraining in AUROC by 1.2% (95% CI,\n−0.3%, 2.7%,p = 0.103). The comparison of performance gain shows\nthat TransformEHR offers better generalizability on the external\ndataset compared to BERT.\nDiscussion\nIn this study we introduced TransformEHR, a generative deep neural\nnetwork model for the prediction of diseases and outcomes using\npatients’ longitudinal EHRs. Byﬁrst pretraining TransformEHR on a\nlarge collection of EHRs (255 million visits from 6.5 million patients\nb e t w e e n2 0 1 6a n d2 0 1 9 )a n dt h e nﬁnetuning for speciﬁc clinical\napplications, we found that TransformEHR outperformed existing\nSOTA models on a wide range of disease or outcome predictions. The\nperformance gain was substantial on intentional self-harm prediction\namong PTSD patients. As shown in Table4, TransformEHR was the\nbest-performing model and outperformed BERT, LSTM, and logistic\nregression models by 24%, 97%, and 422% respectively. The afore-\nmentioned results were not surprising, as deep-learning-based models\nare known to capture the salient information of EHR data to create\npowerful feature representations\n38. In addition, pretraining on large\ndata and using the encoder-decoder framework have both been shown\nto be SOTA strategies for sequence-to-sequence applications\n23.\nTransformEHR outperformed BERT for prediction of both single\nand multiple diseases/outcomes, as shown in Tables2, 3,a n d4.I n\ncontrast to BERT, TransformEHR learned the representation of each\nclinical variable by predicting the complete diseases and outcomes\nwith the help of cross-attention and decoder self-attention. Cross-\nattention allowed TransformEHR to selectively focus on sections of\npast visit ICD codes that were most relevant to predicting each ICD\ncode in the future visit, and the decoder self-attention helped Trans-\nformEHR in predicting complete diseases and outcomes by leveraging\nalready predicted primary diagnostic ICD codes to predict secondary\ncodes that are less common. Thus, the performance gains were the\nhighest among uncommon disease predictions. As shown in Table2,\ncompared with the BERT model, TransformEHR improved the AUROC\nby an average of 3.96% in predicting 10 common diseases, and by an\naverage of 5.92% in predicting 10 uncommon diseases. TransformEHR\nalso substantially improved prediction for intentional self-harm pre-\ndiction (AUPRC from 13.34 to 16.67,p = 0.007) and pancreatic cancer\nprediction (AUPRC from 76.89 to 78.64,p <0 . 0 0 1 ) .\nP r e t r a i n i n gp l a y e dak e yr o l ei ni m p r o v i n gt h ep e r f o r m a n c eo fo u r\ndeep-learning-based models. To demonstrate, we chose intentional\nself-harm prediction and compared the performance between a BERT\nmodel pretrained on our pretraining cohort (as described in the Data\nsection) and a BERT model with no pretraining (parameters were\nrandomly initialized beforeﬁne-tuning). Pretrained BERT substantially\noutperformed the non-pretrained BERT (23% higher AUPRC and 7%\nhigher AUROC), as shown in Table4. In particular, pretraining helps\nimprove the latent representations of EHR data compared to non-\npretrained models. This helped improve the probability distribution of\ncandidate diseases or outcomes. As shown in Fig.3, after pretraining\non a large EHR cohort, the probability distribution for the next visit ICD\ncode changed from a random distribution to a learned representation\nof clinically relevant diseases or outcomes. While a pretrained model\ncan capture the probability distribution at a large cohort level,ﬁne-\ntuning it further can improve the performance for a speci ﬁc\napplication.\nAttention-based models beneﬁt from longitudinal EHRs with long\nhistories (approximately 10 prior visits). As shown in Table4,o ft h e\nattention-based models (TransformEHR and BERT)ﬁnetuned on more\nthan 5 visits, the AUPRC scores improved by 19% and 31%, respectively,\nin comparison with modelsﬁnetuned on onlyﬁve most recent visits.\nHowever, with the same experimental setup, the AUPRC scores of non-\nattention-based models such as LSTM and logistic regression\ndecreased by 8% and 54%, respectively, whenﬁnetuned on more than\nﬁve prior visits (on average 10) compared with the modelsﬁnetuned\non only theﬁve most recent visits. Theseﬁndings were consistent with\nprevious research\n39, which showed that LSTM, although good at miti-\ngating the vanishing gradient challenge of RNN, remains suboptimal\nwith long-time dependencies.\nOur work is also related to predictive model studies focused on\nintentional self-harm\n40–44. Typically, these approaches sample thou-\nsands of patients and use probability tables, decision trees, and logistic\nregression to predict intentional self-harm. PPV plays a crucial role\nhere in estimating the potential beneﬁt of any intensive case man-\nagement intervention, if one were to be implemented based on any\nsuch approach. PPV indicates the percentage of patients receiving an\nintervention who would otherwise attempt self-harm. Hartl et al.\n42.\npredicted intentional self-harm among PTSD patients (prevalence ratio\n5%, PPV 0%). Simon et al.\n44. integrated EHR data and questionnaires for\n2,960,929 patients to predict suicide attempts (prevalence ratio 1%)\nwithin 90 days of a mental health visit. The most successful model\ndemonstrated a PPV of 5%. TransformEHR, on the other hand, achieved\na PPV of 8.80% at 10% threshold for prediction of intentional self-harm\namong PTSD patients, which substantially outperformed the baseline\n(Supplementary Table 3). In other words, out of the 100 highest-\npredicted-risk patients from 1000 previously diagnosed PTSD\npatients, 9 patients would attempt intentional self-harm for theﬁrst\ntime before the next visit. In comparison, for logistic regression the\nPPV at the 10% threshold was 3.31%. A practical suicide prevention tool\nmust have a relatively high PPV to minimize the resource cost and\nintrusion directed at patients who will never attempt self-harm\n45.F o l -\nlowing the previous work28, we calculated the incremental cost-\neffectiveness ratio (ICER) of self-harm risk prediction and intervention:\nthe ratio of its incremental cost to its incremental quality-adjusted life-\nyears (QALYs) compared with usual care. Previous studies in the US\nhave suggested that ICER thresholds under $150 k per QALY can be\nused to determine the cost-effectiveness of healthcare interventions in\n2014\n46. When using cognitive behavioral therapy as intervention, BERT\nachieves ICER of $123 k per QALY, and TransformEHR could reduce\nICER to $109 k per QALY. Therefore, TransformEHR may be a reliable\nand feasible option for designing an effective suicide prevention\nsystem.\nIn conclusion, our results have multiple clinical implications. First,\nexisting predictive models tend to focus on single diseases. Yet the\nfocus in clinical care, particularly in older people, is often on managing\ncomorbidities\n17. A predictive model that can predict multiple diseases\nor outcomes may be useful in designing complex treatment plans.\nSecond, a disease-speciﬁc approach could require building hundreds\nof different predictive models, an inefﬁcient and costly use of\nArticle https://doi.org/10.1038/s41467-023-43715-z\nNature Communications|         (2023) 14:7857 6\nresources whereas TransformEHR’s unique pretraining objective\nenables it to predict all diseases and outcomes of a visit, immediately\nafter the pretraining. Third, TransformEHR outperformed existing\nSOTA predictive models with a substantial performance gain, espe-\ncially for uncommon disease/outcome predictions. Thus, our\napproach could assist in the development of screening algorithms to\ndetect uncommon conditions. Fourth, TransformEHR can be easily\ngeneralized on an out-of-domain dataset with a signiﬁcantly smaller\ntraining data. TransformEHR’s strong transfer learning capability\nmakes it a goodﬁt for hospital settings with limited data and com-\nputing resources.\nDespite the merits of this study, there are several limitations that\nprovide opportunities for future improvements. First, we followed\nprevious work on model pretraining and included only diagnostic ICD\ncodes and demographic information\n13. Other information such as\nprocedure codes, medications, lab results, and phenotypical infor-\nmation extracted from notes can be added to further improve the\nperformance\n47. Combining all these codes together would result in a\nlarge vocabulary, forcing the model to have a huge embedding matrix\nas the input and output layer. The computation on this matrix would\ncause both a signiﬁcant increase of memory and time complexity. In\nfuture work, we will increase the GPUs resource to mitigate this com-\nputational challenge. Second, our prediction of single disease was\nlimited to pancreatic cancer and single outcome was limited to\nintentional self-harm with PTSD. Future works would expand the set of\ndiseases and outcomes. Third, while we followed the previous studies\nto use ICD-10-CM codes to identify intentional self-harm\n44,48,49,w e\nacknowledge that the ICD-10-CM representation of self-harm would\nmiss some, although a small percentage of patients who conducted\nself-harm\n50. In addition, the date of the encounter may not be the\nactual date of the self-harm. To determine the viability of using\nTransformEHR in a pragmatic trial, a tailored cost-effectiveness ana-\nlysis that addresses these issues would be necessary for future work.\nMethods\nVHA cohort\nUsing the VHA Corporate Data Warehouse (CDW), weﬁrst identiﬁed a\ntotal of 8,308,742 patients who received care from more than 1200\nhealth care facilities of the US VHA from 1/1/2016 to 12/31/2019. For\ninclusion, we required each patient to have at least two visits: one\noutcome visit and at least one prior visit to be used for prediction of\nthe outcome. This resulted in a total of 6,829,064 patients.\nPretraining andﬁnetuning cohorts\nFollowing the standard 95–5s p l i t13,22, we randomly took 95% of the\npatients (6,475,218) to create our pretraining cohort and used the\nremaining 5% (353,846) to create other datasets toﬁnetune for DOAP\nand single disease or outcome predictions. The detailed patient cohort\nselections and diseases or outcomes for this study are shown in Sup-\nplementary Fig. 3. The study protocol was approved by the Institu-\ntional Review Board at the VA Bedford Healthcare System.\nWe identiﬁed two use cases to evaluate TransformEHR.\nPancreatic cancer\nAlthough pancreatic cancer is relatively uncommon, it is deadly, pro-\njecting to become the second leading cause of cancer-related mortality\nby 2030\n32. Since effective screening is not available for pancreatic\ncancer, most patients can seek medical attention only after being\ndiagnosed with locally advanced or metastatic cancer. Therefore,\naccurate prediction of pancreatic cancer can help early detection of\npancreatic cancer. Previous research\n13 has evaluated BERT for pan-\ncreatic cancer prediction. Following the same criteria in previous\nstudies\n13,51,w ed eﬁned pancreatic cancer with theﬁrst 3 digit ICD-10CM\ncodes as C25. The case included 4639 patients of 45 years or older with\nno report of any other cancer disease before theirﬁrst pancreatic\ncancer diagnosis and diagnosis made between 12 and 36 months after\ntheir last visit, and control patients comprised 5089 patients of 45\nyears or older without any cancer diagnosis.\nPrediction of intentional self-harm\nPTSD is considered as a hallmark injury of US Post-9/11 Veterans,\nwith a prevalence estimated to be up to 23%\n52. Individuals with PTSD\nalso have co-occurring physical health (e.g., chronic pain53), mental\nhealth (e.g., depression and anxiety54,55), and behavioral conditions\n(e.g., substance use disorder56,57). People with PTSD have 5.3–13\ntimes the rate of suicide than people without PTSD58. Hence, we also\nevaluated TransformEHR for prediction of intentional self-harm for\npatients with PTSD. Out of 70,967 the patients who had been\ndiagnosed with PTSD from 353,846 patients not in the pretraining\ncohort, 1342 patients who would attempt self-harm for theﬁrst-time\n(from VHA CDW and Suicide Prevention Applications Network)\nwere collected as cases. Following previous research that used the\nICD-9 codes for intentional self-harm\n44,48,49, we converted those ICD-\n9 into ICD-10CM and deﬁned intentional self-harm using ICD-10CM\ncodes as in Supplementary Data 1. A recent study found that self-\nharm related ICD-10-CM codes only miss a small proportion of\nactual self-harm incidents\n50. Another 14 patients were considered as\ncontrols who were diagnosed with PTSD but would not attempt self-\nharm in six months after their last visit. The observed self-harm rate\nwas 1.9% among PTSD patients. Further details of the cohort deﬁ-\nnition are available in Supplementary Fig. 4. All inpatient and out-\npatient ICD codes were included in our data.\nImplementation details\nFor pretraining, we tasked TransformEHR to predict the next visit\nICD codes recursively. Speci ﬁcally, for each patient ’si n p u t\nsequence, we used theﬁrst visit to predict the second visit, and then\nused the ﬁrst two visits to predict the third visit, and so on. This\nprocess was repeated until the last visit as recorded in the input\nsequence was predicted. ICD codes were ordered by their priorities\nwithin a visit. We set the maximum sequence length (number of ICD\ncodes) to be 512. To prevent the model from simply memorizing the\npretraining data, we randomly drop 15% of visits from each input\nduring pretraining. Other hyperparameters include a warmup ratio\nof 0.1, a learning rate of 1e− 3, a dropout rate of 0.1, and an L2\nweight decay of 1e− 3 with fp16. To make a fair comparison, we\npretrained the baseline BERT model on the same pretraining cohort,\ninstead of using existing models pretrained on other cohorts. We\nused 4 Nvidia Tesla P40 GPU of 22 GB graphics memory capacity\nand trained each model for 6 days with more than 280 k steps and\nbatch size of 48.\nDuringﬁnetuning, we added a task-speciﬁc classiﬁer layer (a linear\nlayer) on top of TransformEHR and BERT to predict disease or out-\ncome as a binary classiﬁcation task. For a fair comparison with other\nmedical BERT models\n13–15, TransformEHR used six layers in both\nencoder and decoder modules to best match the amount of para-\nmeters. All models wereﬁnetuned withﬁve random seeds and statis-\ntical tests were carried out among these models. To ensure a fair\ncomparison, the same feature transformation, L2 regularization, and\nhyperparameter tuning strategies, were used toﬁnetune Transfor-\nmEHR and all baseline models. For each outcome prediction, we built\ntraining, validation, and test datasets by the ratio of 7:1:2. One sided\nstudent’s t test was used to determine if TransformEHR outperforms\nbaseline models. All ﬁnetuning experiments were conducted with\nNvidia Tesla P40 GPU, and each single disease or outcome prediction\nwas ﬁnetuned within 12 h.\nReporting summary\nFurther information on research design is available in the Nature\nPortfolio Reporting Summary linked to this article.\nArticle https://doi.org/10.1038/s41467-023-43715-z\nNature Communications|         (2023) 14:7857 7\nData availability\nThe study protocol was approved by the Institutional Review Board at\nthe VA Bedford Healthcare System under the waiver of informed\nconsent. The study was exempted because the research involves only\ninformation collection and analysis involving the investigator’su s eo f\nidentiﬁable information when that use is regulated under 45 Code of\nFederal Regulations (CFR) parts 160 and 164, subparts A and E, for the\npurposes of health care operations or research as those terms are\ndeﬁned at 45 CFR 164.512(b). The VHA EHR data are available under\nrestricted access for Veterans’privacy and data security laws, access\ncan be obtained by relevant approvals through VA Informatics and\nComputing Infrastructure (VINCI) (contact: VINCI@va.gov). Indivi-\nduals who wish to use this data for research purposes must fulﬁll the\nresearch credentialing requirements as outlined by the VA Ofﬁce of\nResearch and Development, with the approval process expected to\ntake from 1 month to 1 year. The MIMIC-IV raw data is publicly available\nthrough Physionet. aiming to utilize this data for research will be\nrequired to meet research credentialing requirements as outlined at\nthe Physionet’s web site:https://physionet.org/content/mimiciv/2.2/.\nRequests are normally processed within a week. We release a minimum\ndataset to illustrate the training process on https://github.com/\nwhaleloops/TransformEHR.\nCode availability\nOur ﬁnetuning code is publicly available on https://github.com/\nwhaleloops/TransformEHR/. Experiments were conducted using\nPython version 3.8, torch version 1.9.0, transformer library version\n4.16.2. Visualization was obtained using Python packages matplotlib\nversion 3.3.2.\nReferences\n1. Kessler, R. C. et al. Using administrative data to predict suicide after\npsychiatric hospitalization in the veterans health administration\nsystem. Front. Psychiatry11, 390 (2020).\n2. Zhao, W., Jiang, W. & Qiu, X. Deep learning for COVID-19 detection\nbased on CT images.Sci. Rep.11, 14353 (2021).\n3. Goh, K. H. et al. Artiﬁcial intelligence in sepsis early prediction and\ndiagnosis using unstructured data in healthcare.Nat. Commun.12,\n711 (2021).\n4. Wornow, M. et al. The shaky foundations of large language models\nand foundation models for electronic health records.NPJ Digit.\nMed. 6, 135 (2023).\n5. Choi, E. et al. RETAIN: an interpretable predictive model for\nhealthcare using reverse time attention mechanism. In30th Annual\nConference on Neural Information Processing Systems (NIPS 2016).\nAdvances in Neural Information Processing Systems\n3512–3520 (2016).\n6. Wu, M. et al. Beyond sparsity: tree regularization of deep models for\ninterpretability. InThirty-Second AAAI Conference on Artiﬁcial\nIntelligence. Association for the Advancement of Artiﬁcial Intelli-\ngence 1670–1678 (2017).\n7 . Z h a n g ,Y . ,Y a n g ,X . ,I v y ,J .S .&C h i ,M .A T T A I N :a t t e n t i o n - b a s e dT i m e -\nAware LSTM networks for disease progression modeling. InPro-\nceedings of the Twenty-Eighth International Joint Conference on\nArtiﬁcial Intelligence Main track. International Joint Conference on\nArtiﬁcial Intelligence4369–4375 (2019).\n8. Kopitar, L., Kokol, P. & Stiglic, G. Early detection of type 2 diabetes\nmellitus using machine learning-based prediction models.Sci. Rep.\n10, 11981 (2020).\n9. Galatzer-Levy, I. R., Karstoft, K.-I., Statnikov, A. R. & Shalev, A. Y.\nQuantitative forecasting of PTSD from early trauma responses: a\nmachine learning application.J. Psychiatr. Res.59,6 8–76 (2014).\n10. Rajpurkar, P., Chen, E., Banerjee, O. & Topol, E. J. AI in health and\nmedicine.Nat. Med.28,3 1–38 (2022).\n11. Zhang, J., Kowsari, K., Harrison, J. H., Lobo, J. M. & Barnes, L. E.\nPatient2Vec: a personalized interpretable deep representation of\nthe longitudinal electronic health record.6, 65333–65346 (2018).\n12. Yang, X. et al. A large language model for electronic health records.\nNPJ Digit. Med.5, 194 (2022).\n1 3 . R a s m y ,L . ,X i a n g ,Y . ,X i e ,Z . ,T a o ,C .&Z h i ,D .M e d - B E R T :p r e t r a i n e d\ncontextualized embeddings on large-scale structured electronic\nhealth records for disease prediction.Npj Digit. Med.4,8 6( 2 0 2 1 ) .\n14. Li, Y. et al. BEHRT: transformer for electronic health records.Sci.\nRep. 10,7 1 5 5( 2 0 2 0 ) .\n15. Meng, Y., Speier, W., Ong, M. & Arnold, C. Bidirectional repre-\nsentation learning from transformers using multimodal electronic\nhealth record data to predict depression.IEEE J. Biomed. Health\nInform. 25,3 1 2 1–3129 (2021).\n16. Pang, C. et al. CEHR-BERT: incorporating temporal information from\nstructured EHR data to improve prediction tasks. InProceedings of\nMachine Learning for Health, volume 158 of Proceedings of Machine\nLearning Research,2 3 9–260 (2021).\n17. Valderas, J. M., Starﬁeld, B., Sibbald, B., Salisbury, C. & Roland, M.\nDeﬁning comorbidity: implications for understanding health and\nhealth services.Ann. Fam. Med.7,3 5 7–363 (2009).\n18. Long, A. N. & Dagogo-Jack, S. Comorbidities of diabetes and\nhypertension: mechanisms and approach to target organ protec-\ntion. J. Clin. Hypertens.13,2 4 4–251 (2011).\n19. Colosia, A. D., Palencia, R. & Khan, S. Prevalence of hypertension\nand obesity in patients with type 2 diabetes mellitus in observa-\ntional studies: a systematic literature review.Diabetes Metab. Syndr.\nObes. Targets Ther.6,3 2 7–338 (2013).\n20. Powell, N. R. et al. Clinically important alterations in pharmacogene\nexpression in histologically severe nonalcoholic fatty liver disease.\nNat. Commun.14, 1474 (2023).\n21. Vaswani, A. et al. Attention is All you Need. inAdvances in Neural\nInformation Processing Systems 30(eds. Guyon, I. et al.)\n5998–6008 (Curran Associates, Inc., 2017).\n22. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: Pre-training\nof Deep Bidirectional Transformers for Language Understanding. in\nProceedings of the 2019 Conference of the North {A}merican\nChapter of the Association for Computational Linguistics: Human\nLanguage Technologies4171–4186 (Association for Computational\nLinguistics, 2019).\n23. Lewis, M. et al. BART: Denoising Sequence-to-Sequence Pre-\ntraining for Natural Language Generation, Translation, and Com-\nprehension. inProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics7871–7880 (Association\nfor Computational Linguistics, 2020).https://doi.org/10.18653/v1/\n2020.acl-main.703.\n24. Raffel, C. et al. Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer.Journal of Machine Learning Research21,\n1–67 (2020).\n25. Rajkomar, A. et al. Scalable and accurate deep learning with elec-\ntronic health records.Npj Digit. Med1,1 8( 2 0 1 8 ) .\n26. Yuan, W. et al. Temporal bias in case-control design: preventing\nreliable predictions of the future.Nat. Commun.12, 1107 (2021).\n2 7 . M c D e r m o t t ,M .B .A . ,N e s t o r ,B .A . ,A r g a w ,P .N .&K o h a n e ,I .S .E v e n t\nStream GPT: a data pre-processing and modeling library for gen-\nerative, pre-trained transformers over continuous-time sequences\nof complex events.ArXiv\nabs/2306.11547, (2023).\n28. Ross, E. L. et al. Accuracy requirements for cost-effective suicide\nrisk prediction among primary care patients in the US.JAMA Psy-\nchiatry 78, 642–650 (2021).\n29. Pan, S. J. & Yang, Q. A survey on transfer learning.IEEE Trans. Knowl.\nData Eng.22,1 3 4 5–1359 (2010).\n30. US Food and Drug Administration.Orphan Drug Act97–414.\n(1983).\nArticle https://doi.org/10.1038/s41467-023-43715-z\nNature Communications|         (2023) 14:7857 8\n31. Johnson, A. E. W. et al. MIMIC-IV, a freely accessible electronic\nhealth record dataset.Sci. Data10, 1 (2023).\n32. Park, W., Chawla, A. & O’R e i l l y ,E .M .P a n c r e a t i cc a n c e r :ar e v i e w .\nJAMA 326,8 5 1–862 (2021).9.\n3 3 . M e l o ,S .A .e ta l .G l y p i c a n - 1i d e n t iﬁes cancer exosomes and detects\nearly pancreatic cancer.Nature 523,1 7 7–182 (2015).\n34. Raudales, A. M., Weiss, N. H., Goncharenko, S., Forkus, S. R. &\nContractor, A. A. Posttraumatic stress disorder and deliberate self-\nharm among military veterans: Indirect effects through negative\nand positive emotion dysregulation.Psychol. Trauma Theory Res.\nPract. Policy12,7 0 7–715 (2020).\n3 5 . S a i t o ,T .&R e h m s m e i e r ,M .T h ep recision-recall plot is more infor-\nmative than the ROC plot when evaluating binary classiﬁers on\nimbalanced datasets.PLoS One10, e0118432 (2015).\n36. Walsh, C. G., Ribeiro, J. D. & Franklin, J. C. Predicting risk of suicide\nattempts over time through machine learning.C l i n .P s y c h o l .S c i .5,\n457–469 (2017).\n37. Hochreiter, S. & Schmidhuber, J. Long short-term memory.Neural.\nComput. 9,1 7 3 5–1780 (1997).\n3 8 . M i o t t o ,R . ,L i ,L . ,K i d d ,B .A .&D u d l e y ,J .T .D e e pp a t i e n t :a nu n s u -\npervised representation to predict the future of patients from the\nelectronic health records.Sci. Rep.6, 26094 (2016).\n39. Pascanu, R., Mikolov, T. & Bengio, Y. On the difﬁculty of training\nrecurrent neural networks. inInternational Conference on Machine\nLearning1310–1318 (2013).\n40. Tsui, F. R. et al. Natural language processing and machine learning\nof electronic health records for prediction ofﬁrst-time suicide\nattempts.JAMIA Open4, ooab011 (2021).\n41. Galfalvy, H. C., Oquendo, M. A. & Mann, J. J. Evaluation of clinical\nprognostic models for suicide attempts after a major depressive\nepisode.Acta Psychiatr. Scand. 117,2 4 4–252 (2008).\n4 2 . H a r t l ,T .L . ,R o s e n ,C . ,D r e s c h e r ,K . ,L e e ,T .T .&G u s m a n ,F .P r e d i c t i n g\nhigh-risk behaviors in veterans with posttraumatic stress disorder.\nJ. Nerv. Ment. Dis. 193,4 6 4–472 (2005).\n43. Nock, M. K. et al. Risk factors for the transition from suicide ideation\nto suicide attempt: results from the army study to assess risk and\nresilience in servicemembers (Army STARRS).J. Abnorm. Psychol.\n127,1 3 9–49 (2018).\n44. Simon, G. E. et al. Predicting suicide attempts and suicide deaths\nfollowing outpatient visits using electronic health records.Am. J.\nPsychiatry175,9 5 1–960 (2018).\n45. Large, M. et al. Suicide risk assessment among psychiatric inpa-\ntients: a systematic review and meta-analysis of high-risk cate-\ngories. Psychol. Med.48, 1119–1127 (2017).\n46. Anderson, J. L. et al. ACC/AHA statement on cost/value metho-\ndology in clinical practice guidelines and performance measures: a\nreport of the American College of Cardiology/American Heart\nAssociation Task Force on Performance Measures and Task Force.\nPract. Guidel. J. Am. Coll. Cardiol.63,2 3 0 4–2322 (2014).21.\n47. Rongali, S. et al. Learning latent space representations to predict\npatient outcomes: model development and validation.J. Med.\nInternet Res.22,e 1 6 3 7 4( 2 0 2 0 ) .\n48. Patrick, A. R. et al. Identiﬁcation of hospitalizations for intentional\nself-harm when E-Codes are incompletely recorded.Pharmacoe-\npidemiol. Drug Saf.19,1 2 6 3–1275 (2010).\n49. Zheng, L. et al. Development of an early-warning system for high-\nrisk patients for suicide attempt using deep learning and electronic\nhealth records.Transl. Psychiatry10,7 2( 2 0 2 0 ) .\n50. Simon, G. E. et al. Accuracy of ICD-10-CM encounter diagnoses\nfrom health records for identifying self-harm events.J. Am. Med.\nInform. Assoc. JAMIA29,2 0 2 3–2031 (2022).\n51. Placido, D. et al. A deep learning algorithm to predict risk of pan-\ncreatic cancer from disease trajectories.Nat. Med.29,\n1113–1122 (2023).\n52. Fulton, J. J. et al. The prevalence of posttraumatic stress disorder in\nOperation Enduring Freedom/Operation Iraqi Freedom (OEF/OIF)\nVeterans: a meta-analysis.J. Anxiety Disord.31,9 8–107 (2015).\n53. Seal, K. H. et al. Association of traumatic brain injury with chronic\npain in Iraq and Afghanistan veterans: effect of comorbid mental\nhealth conditions.Arch. Phys. Med. Rehabil.98,1 6 3 6–1645\n(2017).\n54. Pugh, M. J. et al. Traumatic brain injury severity, comorbidity, social\nsupport, family functioning, and community reintegration among\nveterans of the Afghanistan and Iraq Wars.Arch. Phys. Med. Rehabil.\n99,S 4 0–S49 (2018).\n5 5 . D i s m u k e - G r e e r ,C .E .e ta l. Comorbid TBI-depression costs in\nveterans: a chronic effect of neurotrauma consortium (CENC)\nstudy.Brain Inj.1 –7( 2 0 1 8 )https://doi.org/10.1080/02699052.2018.\n1542508.\n56. Greer, N. et al. Relationship of deployment-related mild traumatic\nbrain injury to posttraumatic stress disorder, depressive disorders,\nsubstance use disorders, suicidalideation, and anxiety disorders: a\nsystematic review. (Department of Veterans Affairs (US), 2019).\n57. McHugo, G. J. et al. The prevalence of traumatic brain injury among\npeople with co-occurring mentalhealth and substance use dis-\norders. J. Head. Trauma Rehabil.32,E 6 5–E74 (2017).\n58. Gradus, J. PTSD and death from suicide.Natl Cent. Posttraumatic\nStress Disord.28,1 0 5 0–1835 (2017).\nAcknowledgements\nSupport for VA data was provided by the VA Health Services Research\nand Development Service. H.Y. was supported by R01MH125027,\nR01DA056470, and R01AG080670, all of which are from the National\nInstitutes of Health (NIH). In addition, she was also supported by\nI01HX003711 from the US Department of Veterans Affair Veterans Health\nAdministration. Z.Y. was supported by R01MH125027 from NIH. A.M. was\nsupported by R01MH125027 and R01DA056470 from NIH. D.B. was\nsupported by R01AG080670 from NIH. The content is solely the\nresponsibility of the authors and does not necessarily represent the\nofﬁcial views of the National Institutes of Health.\nAuthor contributions\nH.Y. initialized the conceptualization of the project. Z.Y. designed the\nstudy, implemented the methods, and performed the data analysis. W.L.\nchecked the validity of the data. Z.Y. and A.M. interpretate the results\nwith substantial input from D.B. and H.Y. All authors contributed to\nmanuscript preparation.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-023-43715-z.\nCorrespondenceand requests for materials should be addressed to\nHong Yu.\nPeer review informationNature Communicationsthanks Laila Rasmy\nBekhet, and the other, anonymous, reviewer(s) for their contribution to\nthe peer review of this work. A peer reviewﬁle is available.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jur-\nisdictional claims in published maps and institutional afﬁliations.\nArticle https://doi.org/10.1038/s41467-023-43715-z\nNature Communications|         (2023) 14:7857 9\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate if\nchanges were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visithttp://creativecommons.org/\nlicenses/by/4.0/.\nThis is a U.S. Government work and not under copyright protection in the\nUS; foreign copyright protection may apply 2023\nArticle https://doi.org/10.1038/s41467-023-43715-z\nNature Communications|         (2023) 14:7857 10",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7800406217575073
    },
    {
      "name": "Encoder",
      "score": 0.6984435319900513
    },
    {
      "name": "Transformer",
      "score": 0.6527726650238037
    },
    {
      "name": "Health records",
      "score": 0.580623984336853
    },
    {
      "name": "Recall",
      "score": 0.5746057629585266
    },
    {
      "name": "Machine learning",
      "score": 0.5478869676589966
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5324578881263733
    },
    {
      "name": "Harm",
      "score": 0.48150691390037537
    },
    {
      "name": "Generative grammar",
      "score": 0.45217281579971313
    },
    {
      "name": "Cognitive psychology",
      "score": 0.14351630210876465
    },
    {
      "name": "Health care",
      "score": 0.12217888236045837
    },
    {
      "name": "Psychology",
      "score": 0.11579930782318115
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}