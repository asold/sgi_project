{
  "title": "BLESS: Benchmarking Large Language Models on Sentence Simplification",
  "url": "https://openalex.org/W4389524467",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2997754973",
      "name": "Tannon Kew",
      "affiliations": [
        "University of Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2554480353",
      "name": "Alison Chi",
      "affiliations": [
        "National Tsing Hua University"
      ]
    },
    {
      "id": "https://openalex.org/A5081529594",
      "name": "Laura Vásquez-Rodríguez",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105928631",
      "name": "Sweta Agrawal",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2971822299",
      "name": "Dennis Aumiller",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4213678622",
      "name": "Fernando Alva-Manchego",
      "affiliations": [
        "Cardiff University"
      ]
    },
    {
      "id": "https://openalex.org/A2250494766",
      "name": "Matthew Shardlow",
      "affiliations": [
        "Manchester Metropolitan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3205068155",
    "https://openalex.org/W3202658980",
    "https://openalex.org/W4287207937",
    "https://openalex.org/W3194727116",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W4388778203",
    "https://openalex.org/W3002330681",
    "https://openalex.org/W4389437528",
    "https://openalex.org/W2997833324",
    "https://openalex.org/W4318719246",
    "https://openalex.org/W4385570956",
    "https://openalex.org/W4312205996",
    "https://openalex.org/W4304697829",
    "https://openalex.org/W2605243085",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2534253848",
    "https://openalex.org/W4285265395",
    "https://openalex.org/W4307225507",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W2968888988",
    "https://openalex.org/W4389520065",
    "https://openalex.org/W2576026769",
    "https://openalex.org/W3176093981",
    "https://openalex.org/W3037308680",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4389519254",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W3173957073",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4292119927",
    "https://openalex.org/W4385570969",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3034639488",
    "https://openalex.org/W4319863236",
    "https://openalex.org/W3153637831",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4287779338",
    "https://openalex.org/W4382317561",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4386339125",
    "https://openalex.org/W1507711477",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4385573594",
    "https://openalex.org/W4385570666",
    "https://openalex.org/W3035008906",
    "https://openalex.org/W4386339084",
    "https://openalex.org/W3197876970",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W4385570941",
    "https://openalex.org/W4285290744",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4285225959",
    "https://openalex.org/W1970026646",
    "https://openalex.org/W4224275713",
    "https://openalex.org/W1746111881",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2109802560",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3196642073",
    "https://openalex.org/W2988249555",
    "https://openalex.org/W4321854628",
    "https://openalex.org/W4385302156",
    "https://openalex.org/W3184869503",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4386566815",
    "https://openalex.org/W4287674181",
    "https://openalex.org/W4296672794"
  ],
  "abstract": "We present BLESS, a comprehensive performance benchmark of the most recent state-of-the-art Large Language Models (LLMs) on the task of text simplification (TS). We examine how well off-the-shelf LLMs can solve this challenging task, assessing a total of 44 models, differing in size, architecture, pre-training methods, and accessibility, on three test sets from different domains (Wikipedia, news, and medical) under a few-shot setting. Our analysis considers a suite of automatic metrics, as well as a large-scale quantitative investigation into the types of common edit operations performed by the different models. Furthermore, we perform a manual qualitative analysis on a subset of model outputs to better gauge the quality of the generated simplifications. Our evaluation indicates that the best LLMs, despite not being trained on TS perform comparably with state-of-the-art TS baselines. Additionally, we find that certain LLMs demonstrate a greater range and diversity of edit operations. Our performance benchmark will be available as a resource for the development of future TS methods and evaluation metrics.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13291–13309\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nBLESS: Benchmarking Large Language Models on Sentence Simplification\nTannon Kew1,†, Alison Chi2,†, Laura Vásquez-Rodríguez3,4,†,∗,\nSweta Agrawal5, Dennis Aumiller6, Fernando Alva-Manchego7, Matthew Shardlow8\n1University of Zurich, Switzerland 2National Tsing Hua University, Taiwan\n3Idiap Research Institute, Switzerland 4University of Manchester, UK\n5University of Maryland, US 6Cohere, US 7Cardiff University, UK\n8Manchester Metropolitan University, UK\nkew@cl.uzh.ch, achi@gapp.nthu.edu.tw, laura.vasquez@idiap.ch\nsweagraw@umd.edu, dennisaumiller@cohere.com\nalvamanchegof@cardiff.ac.uk, m.shardlow@mmu.ac.uk\nAbstract\nWe present BLESS, a comprehensive perfor-\nmance benchmark of the most recent state-of-\nthe-art large language models (LLMs) on the\ntask of text simplification ( TS). We examine\nhow well off-the-shelf LLMs can solve this\nchallenging task, assessing a total of 44 mod-\nels, differing in size, architecture, pre-training\nmethods, and accessibility, on three test sets\nfrom different domains (Wikipedia, news, and\nmedical) under a few-shot setting. Our anal-\nysis considers a suite of automatic metrics as\nwell as a large-scale quantitative investigation\ninto the types of common edit operations per-\nformed by the different models. Furthermore,\nwe perform a manual qualitative analysis on\na subset of model outputs to better gauge the\nquality of the generated simplifications. Our\nevaluation indicates that the best LLMs, despite\nnot being trained on TS, perform comparably\nwith state-of-the-art TS baselines. Addition-\nally, we find that certain LLMs demonstrate a\ngreater range and diversity of edit operations.\nOur performance benchmark will be available\nas a resource for the development of future TS\nmethods and evaluation metrics.1\n1 Introduction\nLarge pre-trained language models (LLMs) have\ndemonstrated strong performance on a wide range\nof NLP tasks without the need for task-specific\nfine-tuning, leading to a prevailing conventional\nwisdom that LLMs can solve any task. This has\nmotivated the development of benchmarks to better\nunderstand the abilities of LLMs in specific do-\nmains such as healthcare (Sallam, 2023), finance\n(Dowling and Lucey, 2023), education (Baidoo-\nAnu and Owusu Ansah, 2023), engineering (Soba-\n1We make our code and the generated system outputs\navailable at https://github.com/ZurichNLP/BLESS.\n†These authors contributed equally.\n*Work done as a PhD student at the University of Manch-\nester, United Kingdom.\nnia et al., 2023), and ethics (Zhuo et al., 2023),\nas well as for specific NLP tasks (Li et al., 2022;\nWang et al., 2023; Liu et al., 2023).\nHowever, it remains unclear how well current\nLLMs can perform on the challenging task of text\nsimplification (TS). In this paper, we focus on sen-\ntence simplification in English, which typically\ninvolves rephrasing part or all of a sentence into\nlanguage which is more accessible and easier to\nunderstand. While recent work has focused on\nevaluating TS abilities of select models, such as\nGPT-3.5-Turbo (Feng et al., 2023) and mT5 (Ryan\net al., 2023), there is currently no large-scale and\ndetailed analysis of the simplification capabilities\nof different LLMs.\nIn this study, we expand both the breadth and\ndepth of the knowledge base on TS with LLMs,\nevaluating a wider variety of models on three dif-\nferent TS datasets: ASSET (Alva-Manchego et al.,\n2020a), NEWSELA (Jiang et al., 2020) and MED-\nEAS I (Basu et al., 2023). We select these datasets\nto cover a variety of domains (Wikipedia, news,\nand medical) and a diverse set of TS operations\n(e.g. paraphrasing, splitting, and elaboration).\nSpecifically, we use in-context learning (ICL)\nand assess LLMs in a few-shot setting, experi-\nmenting with three different prompts. We select\n44 widely used generative models (both open and\nclosed-weight) and evaluate their abilities from\nthree distinct angles. First, we rely on automatic\nevaluation metrics commonly used in the TS liter-\nature. Second, we quantify and compare the edit\noperations performed by the LLMs during simplifi-\ncation. Finally, we perform a targeted qualitative\nanalysis to validate our findings and to better under-\nstand the quality of the generated simplifications.\nOur findings reveal that closed-weight models pro-\nvide significant gains over open-weight alternatives\nunder a few-shot setting, establishing them as a\n13291\nstrong baseline for future work on TS. We summa-\nrize our contributions as follows:\n1. BLESS ( Benchmarking Large language\nmodEls on Sentence Simplification), a per-\nformance evaluation benchmark of 44 LLMs\nin a few-shot setting (Section 3).\n2. An evaluation that includes both widely used\nautomatic metrics and an analysis of the TS\nedit operations performed by the models (Sec-\ntion 4).\n3. A qualitative analysis of the results, with man-\nual annotation of simplification operations and\nan examination of the relationships between\nselected evaluation metrics (Section 5).\n2 Related Work\nText Simplification Benchmarks Most simpli-\nfication work treats the task as a monolingual\nmachine translation problem, training models\non datasets containing complex-simple sentence\npairs (Zhu et al., 2010). Alva-Manchego et al.\n(2020b) performed a standardized evaluation of\ngeneral data-driven simplification systems, using\nWikipedia-based datasets and NEWSELA . At the\ndocument level, Alva-Manchego et al. (2019b) con-\nducted a systematic analysis of simplification op-\nerations to demonstrate the limitations and disrup-\ntions that occur when multiple sentences are in-\nvolved. Benchmarks have also been established for\nmore specific kinds of simplification: for example,\nboth non-neural (Paetzold and Specia, 2016) and\nneural (Stajner et al., 2022; Saggion et al., 2022)\napproaches to lexical simplification, which aims to\nreplace complex words with simpler alternatives.\nLLM-based Simplification LLMs such as\nGPT-3.5-Turbo, the model behind early versions\nof ChatGPT2, are often used out-of-the-box with-\nout any further training for a given domain or\ntask. Some previous works have investigated sim-\nplification capabilities of select LLMs in order\nto benchmark performance against dedicated ap-\nproaches (Aumiller and Gertz, 2022; Vásquez-\nRodríguez et al., 2022; Ryan et al., 2023; Sun et al.,\n2023; Chi et al., 2023). Meanwhile, Feng et al.\n(2023) explored the TS abilities of the two strong-\nperforming OpenAI models, GPT-3.5-Turbo and\nDavinci-003. However, despite these efforts, we\nonly have results from a very limited number of\nLLMs and evaluation metrics. Thus, it remains un-\n2https://chat.openai.com/\nDataset Domain Size # Words # R TER\nC S\nASSET Wikipedia 359 22.57 18.87 10 16.79\nMED-EASI Medical 300 26.48 27.42 1 25.03\nNEWSELA News 256 26.44 24.82 4 23.17\nTable 1: Dataset Statistics. C: Complex; S: Simple; R:\nReferences. TER refers to Translation Error Rate, a\nmeasurement of the average edit distance between the\nsource and reference texts (see https://www.cs.umd.\nedu/~snover/tercom).\nclear how a wider spectrum of models, differing in\narchitecture and training strategy, perform on differ-\nent domains and in response to different prompts.\nWe aim to fill this gap and study the simplification\nabilities of 44 LLMs in order to highlight potential\nweaknesses and determine areas for further devel-\nopment. To the best of our knowledge, we are the\nfirst to focus on establishing the performance of\nrecent LLMs on the task of TS.\n3 BLESS: Benchmarking Large\nLanguage Models on Sentence\nSimplification\n3.1 Datasets\nOur assessment establishes the performance of cur-\nrent LLMs on TS according to three datasets, cov-\nering different domains and styles. Table 1 summa-\nrizes these datasets.\nASSET (Alva-Manchego et al., 2020a) com-\nprises 2,359 sentences from English Wikipedia\npaired with 10 simplified references. We use the\nofficial test split (359 sentences) for evaluation.\nThese references were created by crowdworkers\nwho were instructed to use edit operations such as\nreplacement, splitting, and deletion.\nMED-EAS I (Basu et al., 2023) is a simplifica-\ntion dataset for short medical texts containing 1,979\ncomplex (expert) - simple (layman) pairs. Each\ntext contains one or more sentences. In this dataset,\nsimplified texts are composed using four types of\noperations: elaboration, replacement, deletion, and\ninsertion. We use the released test split (300 in-\nstances) for our evaluation. Unlike the other two\ndatasets, simplifications in MED-EAS I are slightly\nlonger than the complex source texts, due to ex-\nplanation and decomposition of complex medical\nterms.\nNEWSELA (Xu et al., 2015) contains 1,130 long-\nform news articles that have been professionally\n13292\nrewritten according to four different graded read-\nability levels. For our benchmarking experiments,\nwe opt for the Newsela-Manual test set (Jiang et al.,\n2020). We extract all aligned and partially aligned\nsentence pairs between a complex source sentence\n(level 0) and the four simplified article versions\n(levels 1-4), keeping only those sentences for which\nwe have a reference for all four simplification lev-\nels.3 This results in 256 test examples. Using\nthis small subset of NEWSELA data ensures that\nsentence-level alignments are of high quality and\ncapture important edit operations such as splitting.\n3.2 LLM Types\nWe investigate a total of 44 LLMs with different\nsizes, architectures, and training objectives. The\nmodels we consider range from 60 million to 176\nbillion parameters and are all based on the trans-\nformer architecture (Vaswani et al., 2017), consist-\ning of either an encoder-decoder or a standalone\ndecoder. Furthermore, all have undergone a self-\nsupervised pre-training stage. Nine of these mod-\nels leverage instruction-tuning, which fine-tunes\na pre-trained base model on labeled instruction-\nresponse pairs from a diverse set of tasks. Finally,\njust three of these models have received additional\ntraining through reinforcement learning with hu-\nman feedback (RLHF) to better align the model’s\nresponses with human preferences (Stiennon et al.,\n2020; Ouyang et al., 2022). Evaluating a wide vari-\nety of currently available models should serve as a\nbroad baseline and give sufficient information on\nwhich models perform best in which domains as\nwell as where key challenges remain.\nWe broadly distinguish between open- and\nclosed-weight models. The former pertains to mod-\nels for which the trained weights are accessible and\nthus allow for self-hosting. Typically, these models\nare considered to be “open-source.” However, we\nnote that this obfuscates specific licensing agree-\nments attached to some models and whether or not\nthe training data and code are also made available.\nIn comparison, closed-weight models refer to those\nwhose weights are kept private and can be queried\nonly through APIs. Our open-weight models in-\nclude variants of the T5 family (Raffel et al., 2020),\nGPT-style models (Radford et al., 2019; Wang and\nKomatsuzaki, 2021), OPT (Zhang et al., 2022c)\n3These articles are simplified as a whole to match the\ndesired school grade; therefore, there is no guarantee that\nthere will be an exact match for all the sentences in the text\nacross all grade levels.\nand LLaMA models (Touvron et al., 2023), and the\nBLOOM family (Scao et al., 2022). For closed-weight\nmodels, we focus on those developed by OpenAI.\nDetails on each model family are provided in Ap-\npendix A.\n3.3 Prompts\nTo simplify sentences with LLMs without addi-\ntional fine-tuning, we use in-context learning (ICL).\nICL is a prompting technique that utilizes a small\nnumber of input-output examples to demonstrate a\ntask (Brown et al., 2020). Previous work on related\ntasks has demonstrated that LLMs are sensitive to\nwhich input prompts and few-shot examples are\nused (Zhang et al., 2022b; Lu et al., 2022; Agrawal\net al., 2023). To account for this, we construct\nthree stylistically distinct prompts that consist of\na task instruction and N few-shot examples (see\nFigure 1). For all generation settings, we set N=3\nand randomly sample complex-source pairs from\nthe corresponding validation sets. We leave a de-\ntailed investigation of optimal in-context learning\nstrategies for TS to future work.\n3.4 Inference Settings\nFor open-weight models, we run inference on local\nGPUs using the Transformers library (Wolf et al.,\n2020). We load the models with 8-bit quantiza-\ntion (Dettmers et al., 2022), which allows us to run\ninference efficiently on as few as 5 A100 80GB\nGPUs. For closed-weight models, we use the APIs\nprovided by OpenAI. As generation hyperparam-\neters, we use Nucleus Sampling (Holtzman et al.,\n2020) with a probability threshold of 0.9, a temper-\nature of 1.0, and a maximum output length of 100\ntokens. To account for the stochastic generation\nsettings, we perform each inference run with 3 dif-\nferent random seeds and aggregate the results for\neach metric.\n3.5 Baselines\nWe use the MUSS (Martin et al., 2022) model\nas our main baseline since it has been shown\nto achieve state-of-the-art performance. MUSS\nfine-tunes a BART-large (Lewis et al., 2020)\nmodel with ACCESS control tokens (Martin\net al., 2020) extracted from labeled TS datasets\nand/or mined paraphrases to train both su-\npervised ( MUSS-wiki-mined) and unsupervised\n(MUSS-mined) TS systems. We use the suggested\nhyperparameters from the original paper to set the\ncontrol tokens for simplification generation.\n13293\nRewrite the complex sentence with\nsimple sentence(s). Keep the meaning\nsame, but make it simpler.\nComplex: {complex example}\nSimple: {simple example}\nComplex: {input}\nN ×few-shot examples\n(a) Prompt 0 uses a basic instruction adapted from (Feng et al.,\n2023) followed by a list of N few-shot examples before the\ninput sentence to be simplified.\nRewrite the complex sentence with\nsimple sentence(s). Keep the meaning\nsame, but make it simpler.\nThe sentence ‘{complex example}’\ncan be simplified as follows: ‘{simple\nexample}’\nThe sentence ‘{input}’ can be simpli-\nfied as follows:\nN ×few-shot examples\n(b) Prompt 1 uses the same basic task instruction as prompt 0,\nbut presents few-shot examples in an inline, continuous text\nformat.\nPlease rewrite the following complex\nsentence in order to make it easier\nto understand by non-native speakers\nof English. You can do so by replac-\ning complex words with simpler syn-\nonyms (i.e. paraphrasing), deleting\nunimportant information (i.e. compres-\nsion), and/or splitting a long complex\nsentence into several simpler ones.\nThe final simplified sentence needs to\nbe grammatical, fluent, and retain the\nmain ideas of its original counterpart\nwithout altering its meaning.\nComplex: {complex example}\nSimple: {simple example}\nComplex: {input}\nN ×few-shot examples\n(c) Prompt 2 repurposes the instructions from (Alva-Manchego\net al., 2020a) that were provided to crowdworkers in the cre-\nation of the ASSET dataset. Similarly to prompt 0, few-shot\nexamples are presented in a structured format.\nFigure 1: Prompts used for LLM text simplification.\nThe blue boxes contain the task instructions. Orange\nboxes show how the few-shot examples are presented to\nthe model and yellow boxes contain the prefix for the\nmodel to continue.\n3.6 Automatic Metrics\nTo assess how well LLMs can perform TS, we\nevaluate all the model outputs using a suite of au-\ntomatic metrics. 4 We measure simplicity using\nSARI (Xu et al., 2016), meaning preservation using\nBERTScore (Zhang et al., 2020), and readability\nusing FKGL (Kincaid et al., 1975). These metrics\nare computed using the EASSE package (Alva-\nManchego et al., 2019a).5 Additionally, we report\nLENS (Maddela et al., 2023), a recently proposed\nlearned metric, which considers both the seman-\ntic similarity and the degree of simplification per-\nformed by the system with respect to the source\nsentence and references.6 Where possible, we also\nestablish the performance of ‘gold’ simplifications\nby evaluating available reference sentences using\na ‘leave-one-out’ strategy. That is, in cases where\nmultiple references are available, we select one\nat random and evaluate it against the remaining\nreferences.\n4 Automatic Evaluation Results\nIn this section, we present the results of our auto-\nmatic evaluation of simplification outputs and sum-\nmarize our main findings. First, we perform an ex-\nhaustive assessment using automatic metrics (Sec-\ntion 3.6). For brevity, we report the results of the\nbest-performing LLMs with SARI and BERTScore\nin Table 2 and provide the complete results for\nall 44 models and metrics in Appendix B. Then,\nwe compute edit distance statistics to quantify the\nsimplification operations performed by each of the\nLLMs (Section 4.1). We begin by assessing the\nimpact of the different prompt formats.\nStructured prompting improves performance.\nFigure 2 reveals that prompts 0 and 2 both offer\na slight advantage over prompt 1, especially in re-\ngard to meaning preservation. This confirms that\nproviding a structured template for few-shot exam-\nples instead of embedding them within sentences is\nthe most beneficial. Hence, we focus on prompt 2\nfor all our analysis, as it provides the most detailed\ndescription of the task and has also been used in\nprior work (Maddela et al., 2023).\nTraining method matters more than size. Ta-\nble 2 presents the performance according to SARI\n4See Appendix B.1 for details on each evaluation metric.\n5https://github.com/feralvam/easse\n6We compute LENS using its original implementation:\nhttps://github.com/Yao-Dou/LENS.\n13294\nFigure 2: Impact of prompt selection on SARI and BERTScore for all models on ASSET . Prompts 0 and 2 achieve\nimproved meaning preservation over prompt 1.\nASSET M ED-EAS I NEWSELA\nSARI ↑ BERT↑ SARI↑ BERT↑ SARI↑ BERT↑\nBaselines\nGold References 45.27 78.89 100 100 60.11 87.66\nMUSS-mined 42.29 79.86 35.15 42.55 38.40 72.14\nMUSS-wiki-mined 44.90 77.71 35.12 43.07 41.24 74.1\nLLMs\nAda-001* 33.97 81.76 36.52 33.95 34.42 70.33\nBabbage-001* 38.44 82.46 36.6 37.95 36.41 62.99\nCurie-001* 39.87 82.75 38.22 39.31 37.53 69.17\nDavinci-002* 42.84 85.91 36.34 43.67 40.25 73.62\nDavinci-003* 46.60 79.66 39.81 40.83 37.76 61.56\nGPT-3.5-Turbo* 47.69 79.39 40.14 40.67 37.29 60.19\nBLOOM 39.72 76.63 37.72 11.95 37.48 61.17\nBLOOMZ 37.63 82.06 36.6 12.9 37.06 69.55\nOPT-1.3b 33.01 75.57 34 3.82 34.76 50.78\nOPT-30b 38.04 77.22 35.08 9.96 37.58 61.79\nOPT-IML-MAX-1.3b 36.00 79.73 37.01 11.85 37.08 62.68\nOPT-IML-MAX-30b 42.03 79.39 35.8 11.73 39.59 66.39\nFlan-T5-small 38.57 77.26 36.65 38.6 37.72 68.15\nFlan-T5-base 41.40 79.7 36.79 40.63 38.67 68.09\nFlan-T5-large 42.17 80.44 35.71 41.31 39.08 70.27\nFlan-T5-xl 41.07 85.06 33.21 44.12 37.51 75.5\nFlan-T5-xxl 41.75 84.13 34.27 43.43 39.42 73.05\nFlan-UL2 42.83 84.34 35.31 42.8 40.27 73.23\nTable 2: For brevity, we report automatic metrics for simplification (SARI) and meaning preservation (BERTScore)\nfor select models using Prompt 2. ‘*’ indicates closed-weights. The full list of results is available in Tables 6, 7, and\n8 in the Appendix.\nand BERTScore for the top-performing LLMs.\nScaling LLMs has revealed strong benefits in\nfew-shot settings (Brown et al., 2020; Chowdh-\nery et al., 2022); however, in our evaluation, we\nobserve numerous exceptions to this rule. For\nexample, Flan-T5-large (770 million parame-\nters) consistently attains higher SARI scores on\nASSET than Flan-T5-xl (3 billion parameters)\nand Flan-T5-xxl (11 billion parameters).7 Mean-\nwhile, we observe that training strategies such as\n7We include a wider comparison of selected LLMs on\nASSET in Figure 7 in the Appendix.\ninstruction-tuning and RLHF help to deliver greater\nimprovements, especially for meaning preserva-\ntion, as measured by BERTScore. This agrees with\nprevious findings that demonstrate the benefits of\ninstruction-based adaption strategies for improved\ngeneralization abilities (Schick and Schütze, 2021;\nZhang et al., 2022a; Chung et al., 2022).\nASSET On Wikipedia-style data, OpenAI’s\nDavinci-003 and GPT-3.5-Turbo outperform all\nother tested LLMs by a considerable margin accord-\ning to SARI. Strikingly, these models also outper-\nform the ground truth references, which are closely\n13295\napproximated by the previous state-of-the-art MUSS\nmodels. This is notable since MUSS-wiki-mined\nwas trained on the in-domain TS dataset of Wiki-\nLarge (Zhang and Lapata, 2017). Meanwhile, for\nopen-weight contenders, we can see in Table 2 that\nonly a small number of models are competitive,\nnamely OPT-IML-Max-30b, Flan-T5-large, and\nFlan-UL2, which scores the best balance between\nsimplicity and meaning preservation according to\nautomatic metrics.\nMED-EAS I For medical-related texts, we ob-\nserve that the majority of the models consistently\nfail to preserve meaning (our qualitative analysis\nin Section 5 confirms this, see Table 3). The drop\nin meaning preservation can likely be explained by\nthe fact that models are known to produce inade-\nquate generations in out-of-domain settings (Müller\net al., 2020; Singhal et al., 2023). The models that\ndo strike a reasonable balance with both SARI and\nBERTScore are again OpenAI’s more powerful\nofferings and the Flan models. Notably, we also\nobserve that the two MUSS models are able to per-\nform competitively with the Flan models despite\nbeing multiple orders of magnitude smaller.\nNEWSELA Evaluating LLMs on professionally\nwritten simplifications from NEWSELA reveals that\neven the best LLMs are not able to match hu-\nman performance. This is observable through\nthe clear margins of around 20 SARI points and\n14 BERTScore points between the best perform-\ners and the gold simplifications. On this dataset,\nMUSS-wiki-mined remains a strong baseline, out-\nperforming all LLMs on both metrics, while\nDavinci-002, Flan-UL2, and Flan-T5-xxl show\nthe strongest performances among the LLMs.\n4.1 Analysis of Edit Operations\nTo identify the main token-level edit operations per-\nformed by LLMs, we use an adaptation of the Wag-\nner–Fischer algorithm (Wagner and Fischer, 1974),\nfollowing previous work by Vásquez-Rodríguez\net al. (2021a). Specifically, we calculate the por-\ntion of insertion, replacement, deletion, and keep\noperations between the input source sentence and\neach of the system outputs for each dataset.\nFigure 3 shows the distribution of token-level\nedit operations for the best-performing LLMs on\nASSET (for a more comprehensive view across\nall datasets and models, see Figure 5 in the Ap-\npendix). Most models perform all four operations\nto differing degrees; however, similar to the gold\nFigure 3: Distribution of token-level edit operations\nproduced by the best-performing LLMs.\nreferences, the keep operation is by far the most\nprominent in this dataset. Notably, Davinci-003\nand GPT-3.5-Turbo perform the most diverse set\nof operations, with fewer additions and more re-\nplacements than other models. Insertions are typ-\nically less frequent, suggesting that the majority\nof the models avoid adding new and potentially\nirrelevant content. We observe that most LLMs are\nwithin the range of the gold references in terms of\nthe amount of information they delete when simpli-\nfying.\n5 Qualitative Analysis\nAutomatic metrics are known to have blind\nspots and are not always entirely reliable (Alva-\nManchego et al., 2021; He et al., 2022). To com-\npensate for this, we perform a qualitative analysis\non a total of 300 system outputs.\nFirst, we check whether or not each output is a\nvalid simplification and highlight common failure\ncases such as inappropriate changes to the meaning\nof the original text, ungrammatical outputs, and\nthe occurrence of hallucinations. Then, we an-\nnotate occurrences of common simplification edit\noperations such as lexical simplification, deletion,\nsentence splitting, reordering, and paraphrasing.8\nFor our annotations, we select model outputs\nfrom the top five systems ranked according to per-\n8All annotations were completed by one of the authors and\nvalidated separately by another.\n13296\nModel outputs %S↑ %MP↑ %L+ %P+ %D+ %Sp+ %R+ %H ↓\nAll 61.67 67.33 30.33 28.33 35.0 4.33 4.67 12.33\nTop 5 SARI 72.0 68.0 48.0 34.66 37.33 6.67 6.67 8.0\nTop 5 BERT 62.67 84.0 17.33 29.33 34.67 5.33 10.67 2.67\nTop 5 FKGL 34.67 40.0 14.66 17.33 26.67 0.0 0.0 36.0\nTop 5 LENS 77.33 77.33 41.33 32.0 41.33 5.33 1.33 2.67\nOpen-Weight 58.58 64.55 29.47 22.76 36.94 3.36 3.73 13.81\nClosed-Weight 87.50 90.63 37.50 75.0 18.75 12.50 12.50 0.0\nOn ASSET 77.0 82.0 31.0 54.0 33.0 8.0 4.0 10.0\nOn NEWSELA 54.0 70.0 34.0 9.0 38.0 5.0 7.0 17.0\nOn MED-EAS I 54.0 50.0 26.0 22.0 34.0 0.0 3.0 10.0\nTable 3: Results of our manual analysis. The annotation schema includes the following annotation features: S ↑:\naccepted simplification, MP↑: meaning preserved, L+: lexical simplification, P+: paraphrasing, R+: reordering (no\nchanges), D+: deletion, Sp+: sentence splitting, H↓: hallucination.\nformance on the individual evaluation metrics of\nSARI, BERTScore, FKGL, and LENS. In each\nranking set, we randomly select five complex-\nsimple pairs from all generation settings. To evalu-\nate a unique set of models for greater diversity, if a\nsystem is repeated in the ranking (e.g. two different\nprompt types from the same model appear in the\ntop five), we choose the next best system for analy-\nsis. An example of our annotated outputs is shown\nin Table 9 in the Appendix. Table 3 shows results\nfrom this analysis, which we describe according to\ndifferent criteria below.\nBy Automatic Metric Overall, we find that sim-\nplicity and meaning preservation are fairly bal-\nanced. However, there is a clear trade-off between\nthese two axes when we consider the top 5 models\naccording to SARI and BERTScore. This agrees\nwith earlier findings from Schwarzer and Kauchak\n(2018). Along with a higher degree of simplicity,\nthe top 5 SARI models exhibit more diverse edit op-\nerations than those ranked highly by BERTScore.\nLENS, however, does not trade off simplicity\nand meaning preservation and even achieves a\nhigher simplicity score than SARI along with its in-\ncreased level of deletion. This result is in line with\nthe previous finding that LENS achieves stronger\ncorrelations with human judgments compared to\nexisting TS metrics (Maddela et al., 2023). The\ntop 5 models ranked by FKGL, on the other hand,\nproduce outputs with low simplicity and meaning\npreservation and an especially high amount of hal-\nlucinations. This result supports the previous find-\ning that FKGL can be easily gamed by degenera-\ntions (Tanprasert and Kauchak, 2021) and is there-\nfore an unsuitable metric for evaluating the outputs\nof automatic TS systems.\nBy Open-Status Open-weight models most fre-\nquently use the operations of lexical simplification,\nparaphrasing, and deletion, while structural opera-\ntions such as sentence splitting and reordering are\noften neglected. Many only achieve high meaning\npreservation by directly copying the input sentence.\nHowever, the closed-weight models investigated\nhere behave very differently: they produce close\nto 10% more splitting, lexical simplification, and\nre-ordering than open-weight ones, while simulta-\nneously performing fewer deletions. This leads to\na greater degree of paraphrasing.\nBy Domain When comparing performance be-\ntween different domains, we observe that all LLMs\ndo significantly better on general encyclopedic\ntexts in ASSET in terms of both simplicity and\nmeaning preservation, while also exhibiting a di-\nverse set of edit operations. Although outputs from\nNEWSELA contain more hallucinations, meaning\npreservation is still fairly high. Outputs from MED-\nEAS I, on the other hand, have the lowest meaning\npreservation by far and the least diverse set of edit\noperations. We find that MED-EAS I outputs, along\nwith others that do not preserve meaning, often con-\ntain repetitions, hallucinations, and in some cases\neven copy the input prompt, demonstrating a ten-\ndency to disregard the instruction and thus fail to\ncomplete the task. These failure modes are most\nfrequently observed from the smaller T5 models,\nbut are also exhibited by models such as LLaMA\nwhen evaluated on MED-EAS I.\n6 Discussion\nWe discuss our results around the following aspects:\nthe access level of the simplification models (open-\nvs. closed-weight), the training strategies (general\n13297\nFigure 4: BERTScore, computed between the system output and reference sentence(s), correlates strongly with\nLevenshtein similarity, computed between the source sentence and system outputs. This indicates that BERTScore\ntends to reward minimally edited sentences. Levenshtein similarity is computed with the EASSE package (Alva-\nManchego et al., 2019a).\npre-training vs. general fine-tuning strategies), and\nthe utility of automatic metrics.\nAccess Level Among the OpenAI models, we\nobserve that all models perform particularly well\non meaning preservation according to BERTScore\nbut exhibit considerable differences in their abil-\nity to simplify, as indicated by SARI on ‘weaker’\nmodels such as Ada-001. Among the evaluated\nopen-weight models, we observe that the Flan mod-\nels (T5 and UL2) typically perform competitively,\npunching well above their weight in terms of pa-\nrameter counts with much larger decoder-only mod-\nels. This is a promising finding for the category of\nopen-weight models, and we hope that this encour-\nages future work to continue investigating different\nmethods regardless of the model size.\nTraining Strategies Within model families,\nwhen comparing base models to their instruc-\ntion fine-tuned counterparts, we observe that\ninstruction-tuning typically leads to better perfor-\nmance in our few-shot ICL setting for TS. We find\nthis to be particularly encouraging since TS is one\ntask often hindered by the lack of high-quality la-\nbeled training data (Stajner, 2021).\nNevertheless, improvement is not always guar-\nanteed, as seen when comparing BLOOM vs BLOOMZ.\nIn this case, instruction fine-tuning leads to better-\nmeaning preservation but a reduction in the degree\nof simplification, indicating that the instruction-\ntuning method used to derive the multilingual\nBLOOMZ may be less suitable for English TS. This\nstands in stark contrast to the Flan instruction\ntuning method, which delivers considerable gains\nin both SARI and BERTScore despite sharing\nthe same underlying instruction-tuning dataset as\nBLOOMZ. Therefore, we hypothesize that this drop\nin performance may be influenced by the multi-\nlingual instruction tuning setup that is unique to\nBLOOMZ.\nUtility of Automatic Metrics Overall, we find\nSARI and BERTScore to be useful automatic eval-\nuation metrics for inspecting the trade-off between\nsimplicity and meaning preservation (see Figure 6\nin the Appendix). In general, closed-weight models\noften strike a more optimal balance. This is also\nsupported by our qualitative analysis, which con-\nfirmed that these models rely less on deletion, an\noft-overused operation (Devaraj et al., 2022), and\nmore on other edits (e.g. paraphrasing or splitting).\nFurthermore, our qualitative analysis shows that\noutputs with higher BERTScores tend to be min-\nimally simplified, often copying the entire input\ntext. We validate this by studying the relation-\nship between BERTScore (computed between the\nsystem output and the reference sentence(s)) and\nLevenshtein similarity (computed between the sys-\ntem output and the original input sentence). Fig-\nure 4 reveals a strong positive correlation across\nall datasets, indicating that BERTScore tends to re-\nward minimally simplified responses. For some\nof the closed-models, which tend to perform a\ngreater degree of paraphrasing, this leads to lower\nBERTScores, while models that perform more\ncopying are rewarded. Overall, the results from\nour qualitative study generally showed agreement\nwith those from our automatic evaluation metrics,\nparticularly SARI, BERTScore, and LENS. It also\nenabled us to pinpoint specific operations, such as\nre-ordering, and identify issues, notably hallucina-\ntions, in system outputs.\n13298\n7 Conclusion\nIn this paper, we provided a comprehensive as-\nsessment of how well out-of-the-box LLMs per-\nform on the task of TS with few-shot in-context\nlearning. We found that the best LLMs outperform\nstate-of-the-art supervised TS baselines while also\nproducing a more diverse set of simplification op-\nerations. We also established that closed-weight\nmodels perform better than open-weight ones and\nthat general instruction-tuning often improves a\nmodel’s abilities on TS. Furthermore, we empiri-\ncally validated the trade-off between simplicity and\nmeaning preservation through automatic evaluation\nand a manual analysis. Our analyses of multiple\nfew-shot prompting strategies revealed that a more\nstructured prompting format produces better results\nthan presenting source-target examples in continu-\nous text.\nOur performance benchmark, BLESS, provides\na strong foundation for future work. For example,\nit remains an open question as to which expressions\nand instructions are optimal for prompting LLMs to\nsimplify texts. Furthermore, this work exclusively\nfocused on few-shot in-context learning. Future\nwork could explore the capabilities of these systems\nin zero-shot, fine-tuned, or retrieval-based settings.\nLimitations\nIn this section, we discuss a few limitations of\nour work. First, we only considered English TS\ndatasets, and it still remains to be seen how these\nTS abilities transfer to languages other than En-\nglish. Additionally, we selected only a handful of\noutput samples for manual analysis for the three\ntest datasets considered, and all annotations were\nperformed by one of the authors and subsequently\nvalidated by another author independently. It will\nbe necessary to perform this at a larger scale to\nmore accurately characterize the capabilities of\neach model for each domain and prompt. We fur-\nther acknowledge the limits of the evaluation set\nitself. While we purposefully chose the test splits\nto cover a variety of domains, test splits for all\nthree corpora amount to 915 samples, which could\npotentially limit the statistical power of results ob-\ntained from the assessment. Additionally, two out\nof the three test sets contain only sentences as input,\nwhile the third contains short multi-sentence texts,\nso this assessment mostly applies to the subtask\nof sentence simplification. Finally, our findings\nconfirm that proprietary, closed-source models can\nachieve a new state-of-the-art performance on the\ntask of text simplification. However, very little is\nknown about their training data, alignment strate-\ngies, and implementation behind paywalled APIs.\nTherefore, the comparison to open-source models,\nwhich contain no explicit training on the task and\nan extremely bare-bones implementation is poten-\ntially unfair.\nEthics Statement\nThis work is conducted in full awareness of and in\nline with the ACL Ethics Policy. Particularly, this\nwork contributes to the transparency and fairness\nof evaluation methodologies in line with Sections\n1.1, 1.2, 2.7, and 2.9 of the code, which innately\nleads to avoiding seen and unseen harms (Section\n1.2, 1.4). We contribute to improving expertise in\nthe domain of text simplification (Section 2.6). All\nmodels, datasets, and compute resources are used\nwith permission and with concern to the appropri-\nate access rights and licenses (Section 2.8). Our\nwork contributes to the professional development\nof the research team (Section 3.5) and more widely\nbenefits the research community and wider society\n(Section 3.1) by augmenting the understanding of\nthe capacity of LLMs on the specific task of TS.\nAcknowledgements\nWe would like to thank Sian Gooding for her ini-\ntiative in motivating this project, as well as Hoang\nNguyen Hung Van, Jan Trienes, and everyone in\nthe text simplification research community who\njoined our discussions during this journey. Thank\nyou also to the anonymous reviewers for providing\nvaluable feedback. This work was facilitated by\nthe infrastructure services provided by S3IT, the\nService and Support for Science IT team at the\nUniversity of Zurich. Laura Vásquez-Rodríguez’s\nwork was funded by the Kilburn Scholarship from\nthe University of Manchester.\nReferences\nSweta Agrawal, Chunting Zhou, Mike Lewis, Luke\nZettlemoyer, and Marjan Ghazvininejad. 2023. In-\ncontext examples selection for machine translation.\nIn Findings of the Association for Computational\nLinguistics: ACL 2023, pages 8857–8873, Toronto,\nCanada. Association for Computational Linguistics.\nFernando Alva-Manchego, Louis Martin, Antoine Bor-\ndes, Carolina Scarton, Benoît Sagot, and Lucia Spe-\ncia. 2020a. ASSET: A dataset for tuning and evalua-\n13299\ntion of sentence simplification models with multiple\nrewriting transformations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 4668–4679, Online. Association\nfor Computational Linguistics.\nFernando Alva-Manchego, Louis Martin, Carolina Scar-\nton, and Lucia Specia. 2019a. EASSE: Easier auto-\nmatic sentence simplification evaluation. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP): System Demonstrations,\npages 49–54, Hong Kong, China. Association for\nComputational Linguistics.\nFernando Alva-Manchego, Carolina Scarton, and Lucia\nSpecia. 2019b. Cross-sentence transformations in\ntext simplification. In Proceedings of the 2019 Work-\nshop on Widening NLP , pages 181–184, Florence,\nItaly. Association for Computational Linguistics.\nFernando Alva-Manchego, Carolina Scarton, and Lucia\nSpecia. 2020b. Data-driven sentence simplification:\nSurvey and benchmark. Computational Linguistics,\n46(1):135–187.\nFernando Alva-Manchego, Carolina Scarton, and Lucia\nSpecia. 2021. The (un)suitability of automatic evalu-\nation metrics for text simplification. Computational\nLinguistics, 47(4):861–889.\nDennis Aumiller and Michael Gertz. 2022. UniHD at\nTSAR-2022 shared task: Is compute all we need for\nlexical simplification? In Proceedings of the Work-\nshop on Text Simplification, Accessibility, and Read-\nability (TSAR-2022) , pages 251–258, Abu Dhabi,\nUnited Arab Emirates (Virtual). Association for Com-\nputational Linguistics.\nDavid Baidoo-Anu and Leticia Owusu Ansah. 2023. Ed-\nucation in the era of generative artificial intelligence\n(ai): Understanding the potential benefits of chatgpt\nin promoting teaching and learning. Available at\nSSRN 4337484.\nChandrayee Basu, Rosni Vasu, Michihiro Yasunaga,\nand Qian Yang. 2023. Med-easi: Finely annotated\ndataset and models for controllable simplification\nof medical texts. In Proceedings of the Thirty-\nSeventh AAAI Conference on Artificial Intelligence\nand Thirty-Fifth Conference on Innovative Applica-\ntions of Artificial Intelligence and Thirteenth Sympo-\nsium on Educational Advances in Artificial Intelli-\ngence, AAAI’23/IAAI’23/EAAI’23. AAAI Press.\nJason Baumgartner, Savvas Zannettou, Brian Keegan,\nMegan Squire, and Jeremy Blackburn. 2020. The\npushshift reddit dataset. In Proceedings of the inter-\nnational AAAI conference on web and social media,\nvolume 14, pages 830–839.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace\nHe, Connor Leahy, Kyle McDonell, Jason Phang,\nMichael Pieler, Usvsn Sai Prashanth, Shivanshu Puro-\nhit, Laria Reynolds, Jonathan Tow, Ben Wang, and\nSamuel Weinbach. 2022. GPT-NeoX-20B: An open-\nsource autoregressive language model. In Proceed-\nings of BigScience Episode #5 – Workshop on Chal-\nlenges & Perspectives in Creating Large Language\nModels, pages 95–136, virtual+Dublin. Association\nfor Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAlison Chi, Li-Kuang Chen, Yi-Chen Chang, Shu-Hui\nLee, and Jason S. Chang. 2023. Learning to Para-\nphrase Sentences to Different Complexity Levels.\narXiv preprint arXiv:2308.02226.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nTim Dettmers, Mike Lewis, Younes Belkada, and\nLuke Zettlemoyer. 2022. LLM.int8(): 8-bit\nMatrix Multiplication for Transformers at Scale.\nArXiv:2208.07339 [cs].\nAshwin Devaraj, William Sheffield, Byron Wallace, and\nJunyi Jessy Li. 2022. Evaluating factuality in text\nsimplification. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 7331–7345,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nMichael Dowling and Brian Lucey. 2023. Chatgpt for\n(finance) research: The bananarama conjecture. Fi-\nnance Research Letters, 53:103662.\nYutao Feng, Jipeng Qiang, Yun Li, Yunhao Yuan, and\nYi Zhu. 2023. Sentence Simplification via Large\nLanguage Models. ArXiv:2302.11957 [cs].\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nTianxing He, Jingyu Zhang, Tianle Wang, Sachin\nKumar, Kyunghyun Cho, James Glass, and Yulia\nTsvetkov. 2022. On the Blind Spots of Model-\nBased Evaluation Metrics for Text Generation.\nArXiv:2212.10020 [cs].\n13300\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The Curious Case of Neural Text\nDegeneration. arXiv:1904.09751 [cs].\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster,\nTianlu Wang, Qing Liu, Punit Singh Koura, Xian Li,\nBrian O’Horo, Gabriel Pereyra, Jeff Wang, Christo-\npher Dewan, Asli Celikyilmaz, Luke Zettlemoyer,\nand Ves Stoyanov. 2023. Opt-iml: Scaling language\nmodel instruction meta learning through the lens of\ngeneralization.\nChao Jiang, Mounica Maddela, Wuwei Lan, Yang\nZhong, and Wei Xu. 2020. Neural CRF model for\nsentence alignment in text simplification. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 7943–7960, On-\nline. Association for Computational Linguistics.\nJ. Peter Kincaid, Robert P. Fishburne, R L Rogers, and\nBrad S. Chissom. 1975. Derivation of new readabil-\nity formulas (automated readability index, fog count\nand flesch reading ease formula) for navy enlisted\npersonnel. In Institute for Simulation and Training,\npages 1–49.\nHugo Laurençon, Lucile Saulnier, Thomas Wang,\nChristopher Akiki, Albert Villanova del Moral, Teven\nLe Scao, Leandro V on Werra, Chenghao Mou, Ed-\nuardo González Ponferrada, Huu Nguyen, et al. 2022.\nThe bigscience roots corpus: A 1.6 tb composite mul-\ntilingual dataset. Advances in Neural Information\nProcessing Systems, 35:31809–31826.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nQuentin Lhoest, Albert Villanova del Moral, Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\nLewis Tunstall, et al. 2021. Datasets: A commu-\nnity library for natural language processing. arXiv\npreprint arXiv:2109.02846.\nYanyang Li, Jianqiao Zhao, Michael Lyu, and Li-\nwei Wang. 2022. Eliciting knowledge from large\npre-trained models for unsupervised knowledge-\ngrounded conversation. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 10551–10564, Abu Dhabi,\nUnited Arab Emirates. Association for Computa-\ntional Linguistics.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. G-eval:\nNlg evaluation using gpt-4 with better human align-\nment. arXiv 2303.16634.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Fantastically Ordered\nPrompts and Where to Find Them: Overcoming Few-\nShot Prompt Order Sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086–8098, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nMounica Maddela, Yao Dou, David Heineman, and Wei\nXu. 2023. LENS: A learnable evaluation metric for\ntext simplification. In Proceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 16383–\n16408, Toronto, Canada. Association for Computa-\ntional Linguistics.\nLouis Martin, Éric de la Clergerie, Benoît Sagot, and\nAntoine Bordes. 2020. Controllable sentence sim-\nplification. In Proceedings of the Twelfth Language\nResources and Evaluation Conference, pages 4689–\n4698, Marseille, France. European Language Re-\nsources Association.\nLouis Martin, Angela Fan, Éric de la Clergerie, Antoine\nBordes, and Benoît Sagot. 2022. MUSS: Multilin-\ngual unsupervised sentence simplification by mining\nparaphrases. In Proceedings of the Thirteenth Lan-\nguage Resources and Evaluation Conference, pages\n1651–1664, Marseille, France. European Language\nResources Association.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey\nSchoelkopf, et al. 2022. Crosslingual generaliza-\ntion through multitask finetuning. arXiv preprint\narXiv:2211.01786.\nMathias Müller, Annette Rios, and Rico Sennrich. 2020.\nDomain robustness in neural machine translation. In\nProceedings of the 14th Conference of the Associa-\ntion for Machine Translation in the Americas (Volume\n1: Research Track), pages 151–164, Virtual. Associa-\ntion for Machine Translation in the Americas.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nGustavo Paetzold and Lucia Specia. 2016. Benchmark-\ning lexical simplification systems. In Proceedings\n13301\nof the Tenth International Conference on Language\nResources and Evaluation (LREC’16), pages 3074–\n3080, Portorož, Slovenia. European Language Re-\nsources Association (ELRA).\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training. Technical report,\nOpenAI.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, and others. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nMichael Ryan, Tarek Naous, and Wei Xu. 2023. Re-\nvisiting non-English text simplification: A unified\nmultilingual benchmark. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 4898–\n4927, Toronto, Canada. Association for Computa-\ntional Linguistics.\nHoracio Saggion, Sanja Štajner, Daniel Ferrés,\nKim Cheng Sheang, Matthew Shardlow, Kai North,\nand Marcos Zampieri. 2022. Findings of the TSAR-\n2022 shared task on multilingual lexical simplifica-\ntion. In Proceedings of the Workshop on Text Simpli-\nfication, Accessibility, and Readability (TSAR-2022),\npages 271–283, Abu Dhabi, United Arab Emirates\n(Virtual). Association for Computational Linguistics.\nMalik Sallam. 2023. Chatgpt utility in healthcare ed-\nucation, research, and practice: Systematic review\non the promising perspectives and valid concerns.\nHealthcare, 11(6).\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Févry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M. Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022. OpenReview.net.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nTimo Schick and Hinrich Schütze. 2021. It’s not just\nsize that matters: Small language models are also few-\nshot learners. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2339–2352, Online. Association\nfor Computational Linguistics.\nMax Schwarzer and David Kauchak. 2018. Human\nEvaluation for Text Simplification: The Simplicity-\nAdequacy Tradeoff. Technical report, SoCal NLP\nSymposium.\nPrasann Singhal, Jarad Forristal, Xi Ye, and Greg Dur-\nrett. 2023. Assessing out-of-domain language model\nperformance from few examples. In Proceedings\nof the 17th Conference of the European Chapter\nof the Association for Computational Linguistics ,\npages 2385–2397, Dubrovnik, Croatia. Association\nfor Computational Linguistics.\nDominik Sobania, Martin Briesch, Caril Hanna, and\nJustyna Petke. 2023. An analysis of the auto-\nmatic bug fixing performance of chatgpt. In 2023\nIEEE/ACM International Workshop on Automated\nProgram Repair (APR), pages 23–30, Los Alamitos,\nCA, USA. IEEE Computer Society.\nSanja Stajner. 2021. Automatic text simplification for\nsocial good: Progress and challenges. In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 2637–2652, Online. Association\nfor Computational Linguistics.\nSanja Stajner, Daniel Ferrés, Matthew Shardlow, Kai\nNorth, Marcos Zampieri, and Horacio Saggion. 2022.\nLexical simplification benchmarks for English, Por-\ntuguese, and Spanish. Frontiers in Artificial Intelli-\ngence, 5.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel\nZiegler, Ryan Lowe, Chelsea V oss, Alec Radford,\nDario Amodei, and Paul F Christiano. 2020. Learn-\ning to summarize with human feedback. In Advances\nin neural information processing systems, volume 33,\npages 3008–3021. Curran Associates, Inc.\nRenliang Sun, Wei Xu, and Xiaojun Wan. 2023. Teach-\ning the pre-trained model to generate simple texts\nfor text simplification. In Findings of the Associa-\ntion for Computational Linguistics: ACL 2023, pages\n9345–9355, Toronto, Canada. Association for Com-\nputational Linguistics.\n13302\nTeerapaun Tanprasert and David Kauchak. 2021.\nFlesch-kincaid is not a text simplification evaluation\nmetric. In Proceedings of the 1st Workshop on Natu-\nral Language Generation, Evaluation, and Metrics\n(GEM 2021), pages 1–14, Online. Association for\nComputational Linguistics.\nYi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia,\nJason Wei, Xuezhi Wang, Hyung Won Chung, Dara\nBahri, Tal Schuster, Steven Zheng, Denny Zhou, Neil\nHoulsby, and Donald Metzler. 2023. UL2: Unifying\nlanguage learning paradigms. In The Eleventh Inter-\nnational Conference on Learning Representations.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nLaura Vásquez-Rodríguez, Nhung Nguyen,\nMatthew Shardlow, and Sophia Ananiadou.\n2022. UoM&MMU at TSAR-2022 shared task:\nPrompt learning for lexical simplification. In\nProceedings of the Workshop on Text Simplification,\nAccessibility, and Readability (TSAR-2022) , pages\n218–224, Abu Dhabi, United Arab Emirates (Virtual).\nAssociation for Computational Linguistics.\nLaura Vásquez-Rodríguez, Matthew Shardlow, Piotr\nPrzybyła, and Sophia Ananiadou. 2021a. Investi-\ngating text simplification evaluation. In Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 876–882, Online. Association\nfor Computational Linguistics.\nLaura Vásquez-Rodríguez, Matthew Shardlow, Piotr\nPrzybyła, and Sophia Ananiadou. 2021b. The role\nof text simplification operations in evaluation. In\nProceedings of the First Workshop on Current Trends\nin Text Simplification (CTTS-2021), pages 57–69.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nRobert A. Wagner and Michael J. Fischer. 1974. The\nString-to-String Correction Problem. Journal of the\nACM (JACM), 21(1):168–173.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\n6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/\nmesh-transformer-jax.\nLongyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang,\nDian Yu, Shuming Shi, and Zhaopeng Tu. 2023.\nDocument-level machine translation with large lan-\nguage models.\nYizhong Wang, Swaroop Mishra, Pegah Alipoor-\nmolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan\nDhanasekaran, Atharva Naik, David Stap, Eshaan\nPathak, Giannis Karamanolakis, Haizhi Gary Lai, Is-\nhan Purohit, Ishani Mondal, Jacob Anderson, Kirby\nKuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar\nPal, M. Moradshahi, Mihir Parmar, Mirali Purohit,\nNeeraj Varshney, Phani Rohitha Kaza, Pulkit Verma,\nRavsehaj Singh Puri, Rushang Karia, Shailaja Keyur\nSampat, Savan Doshi, Siddharth Deepak Mishra, Su-\njan Reddy, Sumanta Patro, Tanay Dixit, Xudong\nShen, Chitta Baral, Yejin Choi, Hannaneh Hajishirzi,\nNoah A. Smith, and Daniel Khashabi. 2022. Bench-\nmarking generalization via in-context instructions on\n1, 600+ language tasks. ArXiv, abs/2204.07705.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V Le. 2022. Finetuned language mod-\nels are zero-shot learners. In International Confer-\nence on Learning Representations.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nWei Xu, Chris Callison-Burch, and Courtney Napoles.\n2015. Problems in current text simplification re-\nsearch: New data can help. Transactions of the Asso-\nciation for Computational Linguistics, 3:283–297.\nWei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen,\nand Chris Callison-Burch. 2016. Optimizing sta-\ntistical machine translation for text simplification.\nTransactions of the Association for Computational\nLinguistics, 4:401–415.\nNingyu Zhang, Luoqiu Li, Xiang Chen, Shumin Deng,\nZhen Bi, Chuanqi Tan, Fei Huang, and Huajun\nChen. 2022a. Differentiable Prompt Makes Pre-\ntrained Language Models Better Few-shot Learners.\nArXiv:2108.13161 [cs].\nRongzhi Zhang, Yue Yu, Pranav Shetty, Le Song, and\nChao Zhang. 2022b. Prompt-based rule discovery\nand boosting for interactive weakly-supervised learn-\ning. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) , pages 745–758, Dublin, Ireland.\nAssociation for Computational Linguistics.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n2022c. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068.\n13303\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Evalu-\nating text generation with BERT. In8th International\nConference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\nview.net.\nXingxing Zhang and Mirella Lapata. 2017. Sentence\nsimplification with deep reinforcement learning. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, pages 584–\n594, Copenhagen, Denmark. Association for Compu-\ntational Linguistics.\nZhemin Zhu, Delphine Bernhard, and Iryna Gurevych.\n2010. A monolingual tree-based translation model\nfor sentence simplification. In Proceedings of the\n23rd International Conference on Computational Lin-\nguistics (Coling 2010) , pages 1353–1361, Beijing,\nChina. Coling 2010 Organizing Committee.\nTerry Yue Zhuo, Yujin Huang, Chunyang Chen, and\nZhenchang Xing. 2023. Red teaming ChatGPT via\nJailbreaking: Bias, Robustness, Reliability and Toxi-\ncity. arXiv e-prints, page arXiv:2301.12867.\nA Model Details\nIn this section, we describe each type of LLM we\nuse in our experiments.\nA.1 Open-weight Models\nAs a brief disclaimer, we note that some listed mod-\nels are not truly “open-weight\" and may require spe-\ncial permission to obtain weights for self-hosting.\nFurther, in our descriptions, we do not distinguish\nbetween different variations of the same model. We\nprovide the details of the training data and model\nsizes in Table 4. We consider both encoder-decoder\nand decoder-only models for our evaluation as dis-\ncussed below.\nA.1.1 Encoder-Decoder Models\nT5 Family We evaluate a range of model variants\nderived from the original T5 models (Raffel et al.,\n2020). Originally, training recipes for T5 employ\npre-training with a span-infilling objective and are\nthus not suitable for left-to-right generation tasks\noff the shelf. We thus use theT5-LM-adapted mod-\nels from (Lester et al., 2021) which have undergone\ncontinued pre-training using a standard LM objec-\ntive.\nOne later derivation includes the instruction-\ntuned variant Flan-T5 (Chung et al., 2022),\nwhich continues training from the aforementioned\nT5-LM-adapted checkpoints and uses a wide vari-\nety of labeled data for instruction fine-tuning. No-\ntably, the dataset description by Chung et al. (2022)\ndoes not include any reference to simplification-\nrelated tasks. Similar parallel efforts lead to the\ncreation of the T0 models (Sanh et al., 2022).\nFinally, UL2 (Tay et al., 2023) proposes a more\ndiverse set of pre-training objectives beyond simple\nspan corruption. Additional tasks include sequence\ndistortion and extreme span corruption.\nA.1.2 Decoder-only Models\nGPT-J/GPT-X Early reproduction efforts of large-\nscale GPT-style models started following the surge\nin popularity of GPT-2 (Radford et al., 2019). For\nour benchmark, we include models published by\nEleutherAI, namely the 6 billion parameter variant\nof GPT-J (Wang and Komatsuzaki, 2021) and the\n20 billion parameter version of GPT-NeoX (Black\net al., 2022). Both models were trained with a\nstandard LM pre-training objective and were not\nfine-tuned to follow instructions.\n13304\nModels Type Sizes Training Data\nBLOOM D 560M, 1b1, 3b, 7b, 175b ROOTS (Laurençon et al., 2022), Huggingface\nDatasets (Lhoest et al., 2021)\nBLOOMZ D 560M, 1b1, 3b, 7b, 175b P3 (Sanh et al., 2022), xP3\nLLaMA D 7b, 13b, 30b, 65b CommonCrawl, C4 (Raffel et al., 2020), Github,\nWikipedia, ArXiv, StackExchange\nOPT D 1.3b, 6.7b, 13b, 30b, 66b Pile (Gao et al., 2020), Reddit (Baumgartner et al.,\n2020)\nOPT-IML D 1.3b, 30b, OPT-IML Benchmark (Iyer et al., 2023)\nGPT-J D 6b, 20b Pile (Gao et al., 2020)\nT5 E-D 60m (small), 220m (base), 770m (large), 3b (xl),\n11b (xxl)\nC4 (Raffel et al., 2020)\nT0, T0pp E-D 3b, 11b P3 (Sanh et al., 2022)\nFlan-T5 E-D 60m (small), 220m (base), 770m (large), 3b (xl),\n11b (xxl)\nMuffin (Wei et al., 2022), P3 (Sanh et al., 2022),\nNIV2 (Wang et al., 2022)\nUL2 E-D 20b\nFlan-UL2 E-D 20b Muffin (Wei et al., 2022), P3 (Sanh et al., 2022),\nNIV2 (Wang et al., 2022)\nTable 4: Description of Open-Weight models. Model type \"D\" refers to decoder-only models, \"E-D\" for models\nbased on an encoder-decoder architecture.\nOPT/LLaMA Reproduction efforts of large-scale\ndecoder-only models conducted by researchers at\nMeta AI were released under the OPT label (Zhang\net al., 2022c) and more recently under the LLaMA\nlabel (Touvron et al., 2023). Besides a differ-\nent composition in training data and some im-\nplementation choices relating to hardware perfor-\nmance, they otherwise share similar architectures\nand training objectives with the previously men-\ntioned GPT-like models. Iyer et al. (2023) experi-\nmented with instruction tuning the OPT models to\nprovide OPT-IML checkpoints, which we also use\nin BLESS.\nBLOOM The result of an open collaboration, the\nBLOOM model family (Scao et al., 2022) represents\nthe largest open-weight models available at the\ntime of writing, up to the full 176 billion parameter\nscale of GPT-3 (Brown et al., 2020). The original\nmodel was only trained with a standard LM pre-\ntraining objective. BLOOMZ models (Muennighoff\net al., 2022) extend these models with instruction\nfine-tuning.\nA.2 Closed-Weight Models\nAs the current primary choice for commer-\ncial solutions, we benchmark a range of mod-\nels by OpenAI. Previous publications regarding\nthe GPT family (Radford et al., 2018, 2019;\nBrown et al., 2020) establish that these models\n(Ada/Babbage/Curie/Davinci) are decoder-only,\nwith varying numbers of parameters. Table 5 shows\nthe API inference costs of our experiments with\nOpenAI’s models.\nModel $/1k\ntokens ASSET M ED-EASI NEWSELA\nAda-001 0.0004 0.35 0.41 0.28\nBabbage-001 0.0005 0.44 0.51 0.35\nCurie-001 0.002 1.76 2.01 1.41\nGPT-3.5-Turbo0.002 1.75 1.95 1.37\nDavinci-002 0.02 17.62 20.06 14.10\nDavinci-003 0.02 17.52 19.90 13.96\nTotal – 39.54 44.84 31.47\nTable 5: Pricing information for OpenAI’s API models.\nHere we report the total costs incurred for all three\ninference prompts and three seeded runs, totalling nine\ninference runs per dataset. Prices listed correspond to\nthose for the API-based models available from April\nthrough June, 2023. All prices are in USD.\nB Supplemental Results\nTables 6, 7, and 8 show full results for these on\nASSET , MED-EAS I, and NEWSELA respectively.\nB.1 Details on Evaluation Metrics\nA variety of automatic evaluation methods have\nbeen proposed. Commonly used automatic metrics\nlike BLEU (Papineni et al., 2002) and SARI (Xu\net al., 2016) can provide insights into how similar\na model’s outputs are to a set of gold reference\nsimplifications. However, to more precisely under-\nstand a model’s strengths and weaknesses, finer-\ngrained evaluation is often required. For example,\ncalculating the distribution of edit simplification\noperations (e.g. additions and deletions) (Vásquez-\n13305\nRodríguez et al., 2021a,b) can yield more insights\ninto the capabilities of these systems. We eval-\nuate model outputs according to multiple metrics.\nWhile we focus on reporting SARI and BERTScore\nin order to relate our findings with previous work,\nwe also compute additional evaluation metrics for\nmore fine-grained analyses and perform a qualita-\ntive analysis. Specifically, we report:\n1. SARI (Xu et al., 2016): SARI (System output\nAgainst References and against the Input sen-\ntence) is a holistic metric for simplification\nquality. It computes the F1 score for n-grams\nadded, kept, and deleted, with respect to the\ninput (source) and reference sentences.\n2. BERTScore (Zhang et al., 2020): We com-\npute the BERTScore precision, recall and F1\nof the predictions against both the reference\nand source sentences, totaling in 6 different\nscores. Results reported in the paper use\nBERTScore F1 computed between system out-\nput simplifications and the gold reference sen-\ntence(s).\n3. FKGL (Kincaid et al., 1975): FKGL (Flesch-\nKincaid Grade Level) is a weighted score\nbased on sentence length and syllable infor-\nmation. The lower the FKGL, the simpler the\noutput, and the lowest possible score is -3.40.\nHowever, for a given test set, we consider the\nbest FKGL to be the score that is closest to\nthe FKGL of the gold references.\n4. LENS (Maddela et al., 2023): LENS\n(Learnable Evaluation Metric for Text\nSimplification) is a score between 0 and 100\nestimated by a model trained on complex-\nsimple pairs annotated with human ratings.\nWe report the average LENS score for each\ndataset.\nTable 6: Simplification Results on ASSET\nSARI↑ FKGL↓ BERT↑ LENS↑\nBaselines\nGold References 45.27 6.53 78.89 65.58\nMUSS-mined 42.29 8.18 79.86 61.36\nMUSS-wiki-mined44.90 5.29 77.71 69.23\nLLMs\nAda-001 33.97 9.06 81.76 56.41\nBabbage-001 38.44 8.65 82.46 61.39\nCurie-001 39.87 8.33 82.75 63.02\nDavinci-002 42.84 7.77 85.91 67.09\nDavinci-003 46.6 7.74 79.66 67.39\nGPT-3.5-Turbo 47.69 7.51 79.39 69.17\nBLOOM-560m 36.14 8.01 50.11 42.68\nBLOOM-1b1 34.08 8.18 68.60 51.23\nBLOOM-3b 37.15 7.92 72.28 54.34\nBLOOM-7b1 36.96 8.17 77.82 57.37\nBLOOM 39.72 7.78 76.63 60.37\nBLOOMZ-560m 35.12 7.52 41.21 39.52\nBLOOMZ-1b1 35.00 8.42 76.66 54.86\nBLOOMZ-3b 35.74 8.73 75.86 56.78\nBLOOMZ-7b1 37.05 8.56 79.09 59.14\nBLOOMZ 37.63 8.27 82.06 61.07\nGPT-J-6b 38.86 7.83 76.48 60.13\nGPT-NeoX-20b 39.04 8.04 75.81 60.87\nLLaMA-7b 40.70 7.39 75.52 62.80\nLLaMA-13b 40.45 7.33 76.13 62.95\nLLaMA-30b 39.14 7.32 78.74 62.73\nLLaMA-65b 38.59 8.07 81.59 62.90\nOPT-1.3b 33.01 8.61 75.57 57.08\nOPT-6.7b 38.64 7.79 76.62 61.26\nOPT-13b 38.78 8.03 79.08 60.51\nOPT-30b 38.04 8.17 77.22 60.01\nOPT-66b 39.64 7.76 76.72 61.68\nOPT-IML-Max-1.3b36.00 7.66 79.73 61.31\nOPT-IML-Max-30b42.03 6.62 79.39 65.29\nT0-3b 35.16 8.90 54.92 50.38\nT0 36.49 8.56 55.32 48.71\nT0pp 35.05 8.65 47.69 44.67\nT5-small-LM-adapt33.89 6.61 10.27 14.56\nT5-base-LM-adapt34.70 6.80 19.63 14.27\nT5-large-LM-adapt31.12 6.88 37.82 15.21\nT5-xl-LM-adapt 29.12 7.06 48.25 23.39\nT5-xxl-LM-adapt33.17 6.85 46.59 25.43\nFlan-T5-small 38.57 7.58 77.26 54.80\nFlan-T5-base 41.40 7.32 79.70 62.75\nFlan-T5-large 42.17 6.78 80.44 63.35\nFlan-T5-xl 41.07 7.16 85.06 64.74\nFlan-T5-xxl 41.75 7.27 84.13 66.08\nUL2 35.65 7.65 37.01 15.99\nFlan-UL2 42.83 6.85 84.34 67.36\n13306\nTable 7: Simplification Results on MED-EAS I\nModel SARI↑ FKGL↓ BERT↑ LENS↑\nBaselines\nGold References 100 9.59 100 65.89\nMUSS-mined 35.15 9.29 42.55 52.48\nMUSS-wiki-mined35.12 8.04 43.07 59.12\nLLMs\nAda-001 36.52 10.62 33.95 41.43\nBabbage-001 36.60 10.49 37.95 53.91\nCurie-001 38.22 10.15 39.31 56.10\nDavinci-002 36.34 10.05 43.67 57.71\nDavinci-003 39.81 9.31 40.83 60.71\nGPT-3.5-Turbo 40.14 8.93 40.67 63.80\nBLOOM-560m 35.37 7.58 -2.60 36.27\nBLOOM-1b1 35.86 7.37 1.63 40.47\nBLOOM-3b 35.48 7.40 5.94 42.21\nBLOOM-7b1 37.47 7.23 9.53 44.17\nBLOOM 37.72 7.11 11.95 47.50\nBLOOMZ-560m 33.14 6.83 -3.08 38.32\nBLOOMZ-1b1 35.65 6.99 6.40 43.69\nBLOOMZ-3b 35.68 7.17 8.56 44.79\nBLOOMZ-7b1 36.78 7.08 9.43 47.15\nBLOOMZ 36.60 7.08 12.90 47.67\nGPT-J-6b 36.20 7.01 10.53 46.67\nGPT-NeoX-20b 36.02 7.07 10.62 46.46\nLLaMA-7b 36.95 6.62 10.28 48.42\nLLaMA-13b 36.98 6.73 11.43 48.63\nLLaMA-30b 37.56 6.89 12.21 47.92\nLLaMA-65b 37.86 6.85 12.20 47.45\nOPT-1.3b 34.00 7.17 3.82 43.64\nOPT-6.7b 34.73 7.02 8.86 47.72\nOPT-13b 34.69 6.96 8.73 47.16\nOPT-30b 35.08 7.02 9.96 46.96\nOPT-66b 35.72 6.96 11.42 47.28\nOPT-IML-Max-1.3b37.01 7.12 11.85 46.80\nOPT-IML-Max-30b35.80 6.78 11.73 49.28\nT0-3b 38.16 10.34 17.83 42.02\nT0 35.67 10.81 15.93 42.76\nT0pp 35.61 10.67 11.58 36.60\nT5-small-LM-adapt34.71 8.87 -4.15 12.68\nT5-base-LM-adapt34.70 8.47 -0.92 16.41\nT5-large-LM-adapt36.69 8.62 10.27 19.34\nT5-xl-LM-adapt 33.65 8.83 18.91 22.59\nT5-xxl-LM-adapt32.61 9.10 21.69 28.21\nFlan-T5-small 36.65 8.99 38.60 45.37\nFlan-T5-base 36.79 9.05 40.63 51.95\nFlan-T5-large 35.71 8.70 41.31 52.59\nFlan-T5-xl 33.21 9.11 44.12 54.75\nFlan-T5-xxl 34.27 9.13 43.43 54.70\nUL2 35.89 9.28 17.15 19.79\nFlan-UL2 35.31 8.52 42.80 57.95\nTable 8: Simplification Results on NEWSELA\nModel SARI↑ FKGL↓ BERT↑ LENS↑\nBaselines\nGold References 60.11 5.88 87.66 71.02\nMUSS-mined 38.40 7.86 72.14 61.49\nMUSS-wiki-mined41.24 6.12 74.10 67.61\nLLMs\nAda-001 34.42 8.66 70.33 55.06\nBabbage-001 36.41 8.32 62.99 60.91\nCurie-001 37.53 8.23 69.17 64.35\nDavinci-002 40.25 7.46 73.62 68.58\nDavinci-003 37.76 7.75 61.56 66.20\nGPT-3.5-Turbo 37.29 7.80 60.19 67.97\nBLOOM-560m 33.41 7.76 31.85 38.58\nBLOOM-1b1 35.37 7.99 48.52 46.54\nBLOOM-3b 35.85 8.22 55.33 51.71\nBLOOM-7b1 36.12 7.96 61.00 54.16\nBLOOM 37.48 7.49 61.17 60.98\nBLOOMZ-560m 28.55 7.53 17.56 34.21\nBLOOMZ-1b1 35.22 7.47 54.19 53.05\nBLOOMZ-3b 34.75 8.51 52.51 52.37\nBLOOMZ-7b1 36.21 8.29 59.53 59.36\nBLOOMZ 37.06 8.41 69.55 62.07\nGPT-J-6b 36.8 7.47 59.59 58.98\nGPT-NeoX-20b 36.87 7.62 56.85 59.71\nLLaMA-7b 36.70 6.28 55.31 62.43\nLLaMA-13b 37.16 6.42 59.61 63.32\nLLaMA-30b 37.50 6.75 63.89 64.30\nLLaMA-65b 38.59 7.10 67.82 64.24\nOPT-1.3b 34.76 7.96 50.78 55.35\nOPT-6.7b 36.58 7.76 58.68 60.28\nOPT-13b 37.67 7.16 60.65 61.31\nOPT-30b 37.58 7.75 61.79 61.91\nOPT-66b 37.45 7.25 60.43 62.98\nOPT-IML-Max-1.3b37.08 7.32 62.68 60.47\nOPT-IML-Max-30b39.59 6.09 66.39 64.74\nT0-3b 33.37 8.50 36.56 50.64\nT0 32.83 7.58 30.96 53.23\nT0pp 33.02 8.20 30.66 47.62\nT5-small-LM-adapt30.54 6.33 4.85 16.85\nT5-base-LM-adapt32.94 6.00 12.13 17.80\nT5-large-LM-adapt33.48 6.35 30.99 20.07\nT5-xl-LM-adapt 32.85 6.62 42.51 25.48\nT5-xxl-LM-adapt33.44 6.67 44.15 29.49\nFlan-T5-small 37.72 7.61 68.15 53.61\nFlan-T5-base 38.67 7.21 68.09 59.11\nFlan-T5-large 39.08 6.90 70.27 62.70\nFlan-T5-xl 37.51 7.25 75.50 64.39\nFlan-T5-xxl 39.42 7.32 73.05 65.13\nUL2 35.22 6.92 37.07 21.62\nFlan-UL2 40.27 6.86 73.23 66.42\n13307\nModel Sentence Annotation\nComplex They are rivaled as biological materials in toughness only by\nchitin.\n-\nReference They are rivaled only by chitin in toughness. -\nGPT-3.5-Turbo Chitin is the only biological material thatrivalsthem intoughness. S↑ P+ L+\nDavinci-003 Chitin is the only biological materialtougherthan them. S ↑ P+ L+\nDavinci-002 They are tough like chitin, which is thetoughestknown biological\nmaterial.\nMP↓P+ L+\nFlan-UL2 They are onlysecondto chitin for biological materials. MP ↓P+ L+\nFlan-T5-large Chitin is better than human materials intoughness. MP ↓P+ L+\nTable 9: Annotation examples from a SARI-based model ranking. S: Simplification, P: Paraphrasing, L: Lexical\nSimplification, and MP: meaning preservation. We highlight lexical simplification in bold and conflicts in meaning\npreservation in red.\nFigure 5: Token-level edit operations computed for all models and test sets using prompt 2. For most models, the\nedit operations performed in ASSET and NEWSELA reflect those in the gold reference simplifications. However,\non the MED-EAS I dataset, we observe a sudden spike in insertions from all LLMs except for OpenAI and Flan\nmodels. These additions indicate the presence of potentially unrelated hallucinated tokens and endless generations,\nwhich aligns with the low BERTScore results. We regard this failure case to be related to the fact that MED-EAS I\npresents a challenging domain which is out of the distribution of most general-purpose models.\n13308\nFigure 6: Adequacy-simplicity trade-off as exhibited by a limited set of models on each of the three datasets. On\nASSET , higher SARI is associated with lower BERTScore. In the case of MED-EAS I, we can see that smaller\nmodels, which often tend to copy the input sentence, are rewarded by BERTScore but punished by SARI. Here,\nonly the closed-weight OpenAI models exhibit a favorable balance between the two metrics. On NEWSELA , the\nrelationship is more linear. We suspect that this is influenced by the fact that reference sentences are taken from\nmultiple simplification levels (1-4) and therefore cover a broader range of possible rewrites, some with more\nsimplifying edit operations (rewarded by SARI) and some with fewer (rewarded by BERTScore).\nFigure 7: Visualizing LLM performance for select models, generated using prompt 2. This visualization corresponds\nto the results reported in Table 2 for ASSET . Models on the x-axis are ordered by model family, and within each\nmodel family, they are ordered by size (ascending).\n13309",
  "topic": "Benchmarking",
  "concepts": [
    {
      "name": "Benchmarking",
      "score": 0.6836445331573486
    },
    {
      "name": "Computer science",
      "score": 0.6165539622306824
    },
    {
      "name": "Natural language processing",
      "score": 0.4972229301929474
    },
    {
      "name": "Sentence",
      "score": 0.48910075426101685
    },
    {
      "name": "Linguistics",
      "score": 0.4657982289791107
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42439544200897217
    },
    {
      "name": "Natural language",
      "score": 0.41561904549598694
    },
    {
      "name": "Philosophy",
      "score": 0.24933195114135742
    },
    {
      "name": "Management",
      "score": 0.10220053791999817
    },
    {
      "name": "Economics",
      "score": 0.06262904405593872
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I202697423",
      "name": "University of Zurich",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I25846049",
      "name": "National Tsing Hua University",
      "country": "TW"
    },
    {
      "id": "https://openalex.org/I79510175",
      "name": "Cardiff University",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I11983389",
      "name": "Manchester Metropolitan University",
      "country": "GB"
    }
  ],
  "cited_by": 11
}