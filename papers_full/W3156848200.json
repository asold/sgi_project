{
  "title": "Classifying Drug Ratings Using User Reviews with Transformer-Based Language Models",
  "url": "https://openalex.org/W3156848200",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4297764876",
      "name": "Akhil Shiju",
      "affiliations": [
        "Florida State University"
      ]
    },
    {
      "id": "https://openalex.org/A2098223376",
      "name": "Zhe He",
      "affiliations": [
        "Florida State University"
      ]
    },
    {
      "id": "https://openalex.org/A4297764876",
      "name": "Akhil Shiju",
      "affiliations": [
        "Florida State University"
      ]
    },
    {
      "id": "https://openalex.org/A2098223376",
      "name": "Zhe He",
      "affiliations": [
        "Florida State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2017857169",
    "https://openalex.org/W1972110417",
    "https://openalex.org/W3006162251",
    "https://openalex.org/W2109575571",
    "https://openalex.org/W2065561264",
    "https://openalex.org/W979396035",
    "https://openalex.org/W2974738057",
    "https://openalex.org/W2143144980",
    "https://openalex.org/W2906083215",
    "https://openalex.org/W3071563765",
    "https://openalex.org/W2108611837",
    "https://openalex.org/W2910485119",
    "https://openalex.org/W2023071006",
    "https://openalex.org/W3135084718",
    "https://openalex.org/W6838173123",
    "https://openalex.org/W2802337645",
    "https://openalex.org/W2070903049",
    "https://openalex.org/W1731355431",
    "https://openalex.org/W2126682177",
    "https://openalex.org/W3013493966",
    "https://openalex.org/W2798859316",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3118897201",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W2070027903",
    "https://openalex.org/W3126934640",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3118480041",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2954835819",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "Abstract Drugs.com provides users’ textual reviews and numeric ratings of drugs. However, text reviews may not always be consistent with the numeric ratings. Overly positive or negative rating may be misleading. In this project, to classify user ratings of drugs with their textual reviews, we built classification models using traditional machine learning and deep learning approaches. Machine learning models including Random Forest and Naive Bayesian classifiers were built using TF-IDF features as input. Also, transformer-based neural network models including BERT, BioBERT, RoBERTa, XLNet, ELECTRA, and ALBERT were built using the raw text as input. Overall, BioBERT model outperformed the other models with an overall accuracy of 87%. We further identified UMLS concepts from the postings and analyzed their semantic types in the postings stratified by the classification result. This research demonstrated that transformer-based models can be used to classify drug reviews and identify reviews that are inconsistent with the ratings.",
  "full_text": "  \nClassifying Drug Ratings Using User Reviews with Transformer-Based \nLanguage Models \nAkhil Shiju1, Zhe He, Ph.D.1 \n1Florida State University, Tallahassee, Florida, United States  \nAbstract \nDrugs.com provides users’ textual reviews and numeric ratings of drugs. However, text reviews may not always be \nconsistent with the numeric ratings. Overly positive or negative rating may be misleading. In this project, to classify \nuser ratings of drugs with their textual reviews, we built classification models using traditional machine learning and \ndeep learning approaches. Machine learning models including Random Forest and Naive Bayesian classifiers were \nbuilt using TF-IDF features as input. Also, transformer -based neural network models including BERT, BioBERT, \nRoBERTa, XLNet, ELECTRA, and ALBERT were built  using the raw text as input. Overall, BioBERT model \noutperformed the other models with an overall accuracy of 87%. We further identified UMLS concepts from the \npostings and analyzed the ir semantic types  in t he postings stratified by the classification result. This research \ndemonstrated that transformer -based models can be used to classify drug reviews and identify reviews that are \ninconsistent with the ratings. \nIntroduction \nThe evaluation of the efficacy and safety of drugs heavily relies on randomized controlled trials with rigorous inclusion \nand exclusion criteria.1 However, such processes are limited to a small number of individuals enrolled in the study and \nare constrained to participants in the target population who meet possibly restrictive eligibility criteria, limiting the \npopulation representativeness and subsequent study generalizability.2,3 The ramifications of these acclimations could \npotentially have resulted in the overestimat ion of the efficacy of the product and misidentification of adverse \nevents/side effe cts in the diverse population .4 To counter such issues, approaches such as post -marketing drug \nsurveillance have been introduced to optimize the safety of the drug after its regulatory approval and mass production.5  \nThere are two major forms of post-marketing drug surveillance. Some are formed by government regulators such as \nthe Vaccine Adverse Event Reporting System (VAERS) by the United States Food and Drug Administration6 or the \nYellow Card Scheme by the United Kingdom Medicines and Healthcare Products Regulatory Agency .7 Also, \npublic/private organizations have a system to monitor drug side-effects such as the Research on Adverse Drug events \nAnd Reports .8 Existing methods for identifying adverse events typically focused on  analyzing molecular drug \ncomposition,9 query logs,10 VAERS records,11 or clinical notes in the medical records12 but did not analyze specifically \nthe sentiment of the consumers using their reviews of the drug.13 The application of post-market drug surveillance has \nbeen successfully applied in the identification of adverse  events through safety reports by the introduction of deep \nlearning-based methods including the extraction of temporal events, the procedure performed, and social \ncircumstance14.  \nIn the era of Web 2.0, the Internet has opened up new pathways to obtain information directly from consumers about \ntheir drug reviews in an elaborative format. Publicly available information on the Internet offers an easily attainable \nresource that could be leveraged to gain a deep understanding of t he drug reviews by the users. Entire user reviews \nare fully available on drug review websites, on which users can comment on their personal experiences of the drugs \nthey have taken for a specific condition. Unlike many other forms of medical data, this inf ormation is not filtered \nthrough medical professionals. Since these reviews are given by anonymous users, there is no risk of patient health \nrecord violation for confidentiality.  These reviews contain a plethora of information regarding individual experiences \nassociated with the drugs  such as symptoms, adverse events, and interactions with other drugs . Such reviews have \nalso contained an extensive amount of user sentiment related to a particular condition, which could be leveraged to \ndetect the side effects and efficacy of drugs.15  \nHowever, many barriers exist in the extraction of sentiment from these online medical reviews. For instance, user \nreviews of drugs in such online forms are typically unconventional and most reviewers lack medical knowledge , \nposing barriers for extracting meaningful information from them. In addition, many review websites use some form \nof numerical rating that has served the role of quantifying such a sentiment, but they do not provide a clear guideline \nfor giving a certain numeric rating. As such, these review websites may have introduced biases as individual users \nmay have different perception as to what a high score means versus what would have constituted a low score. Users \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 8, 2021. ; https://doi.org/10.1101/2021.04.15.21255573doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n  \nhave tended to reduce the effort required in reporting values by rating all qualities as highly important, thus resulting \nin overly positive ratings.16 This could lead to an unintended positive view of the overrated drugs by the general public, \nalbeit less effective for certain population subgroups. Prior research  has fo und that web -based reviews have the \npotential to be viewed as an applicable source of information for analysis, but the direct reliance on consumer ratings \ncould be biased by the consumer experience. For example, addictive drugs have been observed to be typically highly \nrated in comparison to other drugs which have treated the same condition, even if these additive drugs \nunderperformed.17 Thus, the ratings of those drugs may be skewed, thus a potential solution could be analyzing the \nrelationship between the review and the rating and identify skewed ratings based on the textual review.18    \nThe application of machine learning, especially through transformer -based language models pre -trained with an \nenormous amount of data, offers a unique approach to classify textual information.19 In this project, we evaluated the \nfeasibility of leveraging machine learning and natural language processing to classify user rati ngs based on their \ntextual review to identify the locations of contingency. In addition,  the constructed models can then be tested to \nidentify overly positive and overly negative instances. To provide some interpretability of the classification res ults, \nwe used an interpretation tool called Eli5 to highlight phrases in the text that have a positive or negative impact on the \nclassification results. The overly positive (false negative) or overly negative ( false positive) scores (user rating that \nwas incorrectly classified by the model) were further analyzed with QuickUMLS to identify semantic type patterns \nassociated with these classifications. \nMethods  \nDataset \nWe obtained the dataset from the UCI Machine Learning Repository.20 These instances were collected from Drugs.com \nusing Beautiful Soup. The dataset used for this study  consists of user drug reviews, drug names, related medical \nconditions, and a 10 -point rating. The rating were integer values ranging from 1 to 10 with 10 being the highest \npossible rating. Table 1 shows example records of the dataset. Figure 1 shows the distribution of reviews by ratings.  \nThe ratings were shown to be skewed to the left to suggest that most drugs received a relatively high score. Prior \nanalysis of this dataset focused primarily on the sentiment analysis 21 and classification of reviews used an n -gram \ntechnique which used unequal classes, thus skewing accuracy22. Neither was there an emphasis on the error analysis \nof the models.  In total, the dataset consists of 215,063 instances. The numeric ratings had a mean of 7.00 with a \nstandard deviation of 3.27. There are 836 classified medical conditions in the dataset. \nTable 1. Two examples of a high-rating review versus a low-rating review with condition, drug name, and rating. \nDrug \nName \nCondition Review Rating \nChantix Smoking \nCessation \nI smoked for 50+ years. Took it for one week and that was it. I didn’t think it \nwas possible for me to quit. It has been 6 years now. Great product. \n10.0 \nExcedrin Migraine Does not work for people sensitive to caffeine. I was jittery and nervous and \nqueasy after using a single dose. \n2.0 \n \n \nFigure 1. Total number of reviews in the dataset.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 8, 2021. ; https://doi.org/10.1101/2021.04.15.21255573doi: medRxiv preprint \n  \nReview Rating Classification \nSince the primary focus of this study was to classify textual reviews, the data was broken down using a median of the \nratings: ratings 8 or above were considered above average, and below 8 were considered as below average. The binary \nclassification was chosen over multiple classes since the other objective of this project was to identify overly positive \nand overly negative ratings which could be identified by misclassification of the system versus the actual score. Thus, \nmisclassification identification in its simplest sense would only be possible with a binary system. Instances in which \nthe reviews contained more than 514 tokens were removed from the study due to the input size limit of the transformer-\nbased language models. \nThe common methodology for transfer learning has been applied through the application of pre-training on a large \nunannotated corpus that was capable of understanding the composition of the data type such as patterns in the \nlanguage. This process could be considered as self -supervised learning. This pre-trained model is then followed by \nthe fine-tuning process which focused on the training on an application-specific dataset.  \nBERT: Some common language models are pre-trained by predicting the next word in a sequence, but Bidirectional \nencoder representation from transformer (BERT) looked at bidirectional predicting context masked intermediate text \ntokens in the pretraining from Wikipedia and BookCorpus and next sentence prediction. Bert-base-uncased was used \nfor this project23.   \nBioBERT: The BERT model has been pre-trained with a medical corpus from publicly available data from PubMed \nand PMC.24 The model which was used was from Huggingface labeled Bio_ClinicalBERT. \nALBERT: A Lite BERT (ALBERT) is a model which focused on being a less memory -heavy and faster version of \nBERT through the separation of the word embedding into two matrixes and by cross-layer parameter sharing.25 Albert-\nbase-v2 was used for this model.  \nRoBERTa: Robustly Optimized BERT Approach (RoBERTa) has been considered a pretraining model that eliminates \nthe next sentence prediction task and adapts a novel approach of dynamic masking which randomized the masked \ntoken between training epochs.26 RoBERTa outperformed BERT of multiple results such as GLUE, RACE, and \nSQuAD. Roberta-base was the model selected for this project.  \nXLNet: As a more computationally expensive model, the Generalized Auto-Regressive model (XLNet) implemented \na system where it applies an autoencoder language model.27  \nELECTRA: Efficiently Learning an Encoder that Classifies Token Replacements Accurately (ELECTRA) replaced \nthe masked language task with a generator and pre-trains the model to identify which token has been replaced.28 The \nElectra-base-discriminator was used for this project. \nWe split the dataset into a training set (60%), a validation set (20%), and the test set (20%). These datasets were further \nclassified into lists which were then converted into Transformer datasets that could be trained by a neural network to \ngenerate a model. \nWe constructed these transformer-based text classification models utilizing the Huggingface transformers using the \nPython k-train pipeline wrapper class for text classification. The models used for this project consisted of BioBERT, \nELECTRA, RoBERTa, XLNet, ALBERT, and BERT. The  parameter included a 514 max token length, a 5e^ -5 \nlearning rate, and a batch size of 6. The train test dataset was fed into the neural network trained to minimize validation \ndata loss. After the training was completed, a confusion matrix of the test data was generated to determine F1 scores \nfor the classes and the accuracy in comparison to the user ratings.  \nAs a baseline approach for evaluating the transformer-based models, bag-of-words (BOW) models were constructed \nbased on term frequency and inverse document frequency (TF -IDF). The textual reviews were converted into a bag \nof words representation. Afterward, a term TF-IDF score matrix was computed for the bag of words repres entation. \nWe trained and evaluated a Random Forest classifier and a Naïve-Bayes classifier with the BOW features. \nThe test data was stratified for the top 10  conditions based on the test data user reviews as seen in Table 2. The \ntransformer models were then used to classify each of the different conditions to determine condition-specific F1 score \nand accuracy.  The overall workflow is outlined in Figure 2. \n \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 8, 2021. ; https://doi.org/10.1101/2021.04.15.21255573doi: medRxiv preprint \n  \nTable 2. Condition-specific statistics. Birth Control was the most common condition which users reviewed followed \nby depression, pain, anxiety, and acne. \nTop 10 Conditions # of Instances Condition-Specific \nMean Rating \nCondition-Specific \nMedian Rating \nBirth Control 38345 6.08 7 \nDepression 12129 7.10 8 \nPain 8241 7.62 9 \nAnxiety 7795 7.69 9 \nAcne 7411 7.37 9 \nBipolar Disorder 5598 7.12 8 \nInsomnia 4895 6.69 8 \nWeight Loss 4852 8.03 9 \nObesity 4748 7.73 9 \nADHD 4497 7.31 8 \n \nModel Interpretation \nAfter the best-performing transformer model was selected, to provide some interpretability for the model, Eli5 metrics \nwere applied to the model. Eli5 has been used to understand why a certain classification through the identification of \nimportant features such as h ighlighting significant text features .29 This is accomplished by inspecting the model \nparameters to discover the global implications. This was performed for many reviews to establish some sense of how \nthe model performed these classifications. Top scores were also computed through the Eli5 metrics. \nError Analysis  \nAn analysis of the potential relationship between false positives, false negatives, true positives, true negatives from \nthe best overall performing models was conducted by analyzing the occurrences of certain semantic types of the \nUnified Medical Language System (UMLS) Metathesaurus, which links terms to biomedical concepts .30 We would \nlike to see whether certain error types had deviation in the semantic types present in the review in comparison to the \nother conditional cases. This was conducted using the QuickUMLS package, an unsupervised tool for biomedical term \nextraction using simstring.31 We chose 8 semantic types that were most prevalent in the dataset and had some medical \nsignificance, including Sign or Symptom, Disease or Syndrome, Organism Function, Pathologic Function, Body \nSubstance, Body Location, or Region, Body Part, Organ, or Organ Component, and Health Care Activity. Only \ninstances with a 1.0 Jaccard similarity were retained, and the best matching CUIs were selected.  After the semantic \ntypes were extracted for all the reviews, the means were calculated by true positive, true negative, false positive and \nfalse negative (class type). A one-way ANOVA was employed to determine whether there was a significant difference \nbased on the mean value of the number of concepts of a certain semantic type per post across different class types. \n \nFigure 2. The workflow of the project. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 8, 2021. ; https://doi.org/10.1101/2021.04.15.21255573doi: medRxiv preprint \n  \nResults \nClassification of the Drug Review Rating \nOverall, the model generated by the BioBERT and ELECTRA outperformed the other models on a variety of metrics \nas displayed in Table 3. The BOW models showed lower accuracy compared to the other constructions. XLNet had \nthe longest training time compared to the other models.  Table 4 provides the condition-specific statistics for the top \n10 conditions. The ratings of the reviews pertaining to the Birth Control, Depression, Pain drugs were classified with \nhigh accuracy than the ratings of the drugs for other conditions. The conditions with lower instances had lower \naccuracy than the conditions with higher instances. However, there are many notable deviations present such as the \npain and obesity models’ lower accuracy or the higher accuracy for the ADHD model. \nTable 3. Overall condition validation from the test dataset for the minimized loss for the top-performing models.  \nModel Above Average F1 Below Average F1 Accuracy \nBERT 0.84 0.84 0.84 \nRoBERTa 0.83 0.83 0.84 \nXLNet 0.84 0.84 0.84 \nBioBERT 0.87 0.87 0.87 \nELECTRA 0.85 0.87 0.86 \nALBERT 0.75 0.81 0.78 \nRandom Forest (BOW) 0.77 0.45 0.68 \nNaïve Bayes (BOW) 0.76 0.03 0.61 \n \nTable 4. Results of condition-specific classifications for the top 10 conditions.  \nCondition Model Above Average \nF1 \nBelow \nAverage F1 \nAccuracy \nBirth Control ELECTRA 0.89 0.94 0.92 \n BioBERT 0.86 0.92 0.90 \nDepression ELECTRA 0.88 0.88 0.88 \n BioBERT 0.87 0.87 0.87 \nPain ELECTRA 0.83 0.81 0.82 \n BioBERT 0.85 0.83 0.84 \nAnxiety ELECTRA 0.87 0.82 0.85 \n BioBERT 0.87 0.81 0.85 \nAcne ELECTRA 0.90 0.88 0.89 \n BioBERT 0.86 0.84 0.85 \nBipolar Disorder ELECTRA 0.88 0.84 0.86 \n BioBERT 0.84 0.87 0.86 \nInsomnia ELECTRA 0.82 0.86 0.84 \n BioBERT 0.82 0.84 0.83 \nWeight Loss ELECTRA 0.87 0.80 0.85 \n BioBERT 0.89 0.79 0.86 \nObesity ELECTRA 0.82 0.77 0.80 \n BioBERT 0.85 0.79 0.83 \nADHD ELECTRA 0.86 0.88 0.87 \n BioBERT 0.88 0.87 0.87 \n \nError Analysis \nThe results generated in Table 5 are produced by applying the Eli5 toolkit to the BioBERT trained model. There is a \nclear relationship between the words highlighted and the classification that was made by the model. Terms highlighted \nin green supports the classification generated by the model, while terms generated in red oppose the predictions. The \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 8, 2021. ; https://doi.org/10.1101/2021.04.15.21255573doi: medRxiv preprint \n  \nshade of the color represents the level of importance at which a word contributes to the classification. Phrases related \nto side-effects were typically highlighted as below-average features such as being a “bit moody” or “sore”. The \npositive effects of the drug were highlighted as above-average features such as “my pain almost totally disappeared”. \nSpecific highlighted terms by the ELi5 metric could potentially be subjected to an incorrect sentiment association. For \nexample, the phrase “my cramps disappeared” in Table 5 for the false negative adverse event review was shown to \nsupport the prediction of a negative  below average feature. However, this phrase would  usually have a p ositive \nconnotation associated with it unlike what is suggested by the model. In general, the major reason for a miss prediction \nby the model was primarily due to the presence of a mixture of positive and negative elements present in the text. This \ncould have resulted from the presence of multiple medications, changes in the effectiveness of the medication over \ntime, the extent of the medicinal effects, or treatment experience which could work in both sentimental directions to \nresult in false positives or false. Adverse Events, in general, could result in false positive or false negatives depending \non the extent to which the side-effect concerned someone. In addition, if a classification error occurred, a lower number \nof adverse events tended to be classified as fals e positive versus false negative. Overall, the interpretability of \nmisclassifications, through the Eli5 tool kit revealed an important aspect of how the model used specific keywords.   \nTable 5. Examples of false positives and false negatives in BioBERT model with the important words highlighted in \ngreen (positively impacting the classification results) and red (negatively impacting the classification results) by the \nEli5 toolkit.  \nClass type/reason Review \nFalse \nnegative/multiple \nmedications \nfor me, vyvanse has the “smoothest” feeling of the adhd medicines i have tried. i have found t\nhat concerta (methylphenidate) and focalin (dexmethylphenidate) create an anxious feeling. v\nyvanse does not make me feel this way. downside: it can be outrageously expensive. \nFalse \npositive/Temporal \neffectiveness \nWhen i first started lyrica, my pain almost totally disappeared. after about 3 weeks, my pain s\ntarted returning. my tongue started to tingle and was sore. \nFalse \nnegative/adverse \nevents \nlove this. cleared my skin up, made my period so light and my cramps disappear. i was a bit \nmoody for the first month, but that went away. \nFalse positive/ \nmedication \nineffectiveness \n\"i have cysteine stones...huge! passed 9 small stones within 30 mins after taking. and with ver\ny little pain in the uretha but doesn’t help much with the ureter pain.  \nFalse \npositive/treatment \nexperience \nit gets the job done. tastes gross and i personally had a hard time keeping it down but i manag\ned. it took about 2 hours for the first dose to kick in and I’ve been going since. took the 2nd d\nose an hour ago and almost clear! \n \nMost semantic types were found to have a p-value < 0.05 for most of the class types  based on the results shown in \nTable 6. The physiologic function semantic type for the BioBERT model and clinical drug name semantic type  for \nboth models were found to be insignificant. This suggests that both models tend not to heavily rely on the name of the \nclinical drug in predicting a score but could also be due to the lack of clinical drug names present in the user reviews. \nThis idea is further supported based on the results of the ELi5 which shows many clinical drugs highlighted less \nimpactful (lighter) to the classification than other terms in general .  Based on results of the ELECTRA model, the \naverage number of concepts of most sematic types (e.g., Sign or Symptom) in true negative instances is greater than \nthat of true positive instances; and the average number s of concepts of most sematic types in the  false positive and \nfalse negative instances are between that of true positive and true negative instances  \nHowever, the BioBERT model has some more significant deviation from the most common class distribution in the \nELECTRA model. Significant deviation from this class type distribution in the BioBERT model occur for Disease or \nSyndrome, Organism function, Pathologic function, Body region or Location. \nAll the implementation code and more examples of the misclassifications can be found in the GitHub repository.32 \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 8, 2021. ; https://doi.org/10.1101/2021.04.15.21255573doi: medRxiv preprint \n  \nTable 6. The average mean number of semantic types reported for each class type based on the classification results \nof the overall BioBERT and ELECTRA models . P-values were generated using a 1 -way ANOVA test for each \ncondition using semantic type as the independent variable. Semantic types that differed from the most common pattern \nand were significant (α=0.05) are in bold.  \nModel Semantic \nTypes \nSign or \nSymptom \nDisease \nor \nSyndrome \nOrganism \nFunction \nPathologic \nFunction \nBody \nSubstance \nBody \nLocation \nor \nRegion \nBody Part, \nOrgan, or \nOrgan \nComponent \nHealth \nCare \nActivity \nELECTRA True \nPositive \n1.167 0.595 0.522 0.577 0.079 0.256 0.579 0.382 \nTrue \nNegative \n2.317 0.925 0.867 0.879 0.153 0.435 0.944 0.534 \nFalse \nPositive \n1.512 0.663 0.592 0.638 0.083 0.276 0.659 0.465 \nFalse \nNegative \n1.733 0.750 0.778 0.716 0.137 0.366 0.814 0.516 \nP-values < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 \nBIOBERT True \nPositive \n1.374 0.697 0.623 0.675 0.097 0.307 0.688 1.374 \nTrue \nNegative \n1.993 0.784 0.734 0.752 0.130 0.369 0.802 1.993 \nFalse \nPositive \n1.705 0.784 0.725 0.706 0.111 0.345 0.781 1.705 \nFalse \nNegative \n1.806 0.733 0.747 0.698 0.123 0.329 0.784 1.806  \nP-values < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 < 0.001 \n \nDiscussion \nIn this study, we built multiple classification models to classify drug review rating using the review text. Afterward, \nthe Eli5 toolkit was applied to explain the models’ classification by highlighting the words that positively or negatively \nimpacted the classification result. Informed by these experiments, it was clear that the consumers' online drug reviews \ncontain a vast quantity of information related to the sentiment expressed by the user. Transformer-based models have \nthe potential to serve as a methodology to discriminate between overly positive and truly positive scores. Overall, this \nresearch outlined a potential process of identifying consumer drug review bias. This was consistent with other studies \nwhich found that subjective effects are often distorted in rating systems.33  \nWhen comparing different transformer-based models, the BioBERT and ELECTRA models outperformed the other \nmodels when the same amount of information was present . One possible reason for BioBERT to outperform other \nmodels is that BioBERT model was pre -trained with biomedical texts which were topically related to the drug \nreviews.34 According to the error analysis, ELECTRA model followed the pattern that the average numbers of concepts \nof most semantic types per post in true negative posts were greater than those in true positives; and those of false \npositive and false negative instances falling between those of true cases. This is further cemented by the fact that the \nELi5 toolkit highlighted these terms as more important contributors in general. The BioBERT model tends to evade \nthis classification for certain sema ntic types as previously stated in the error analysis. BioBERT trained with  \nbiomedical text allowed it to find more intricate relationships among the terms, allowing it to reach a better prediction \naccuracy. However, BioBERT and ELECTRA did significantly follow a similar pattern for the semantic types such \nas Sign or Symptom, Body substance, Body part organ or Organ component, and Health care activity. As it is clear \nthat both models relied significantly on these factors to decern the sentiment of a user review, it is likely that user also \nweighted these factors higher than other semantic types when deciding their rating of the drugs. As such, higher \nnumber of possible adverse events (sign or symptoms), the need for more possible medical interventions (healthcare \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 8, 2021. ; https://doi.org/10.1101/2021.04.15.21255573doi: medRxiv preprint \n  \nactivities), and more reference to bodily fluids and organs (body substances and body part organ or organ component) \ntend to result in a lower rating.  \nOverly positive and negative scores given by the users could be detected by this model. Overly positive tend to suggest \nthat the review given by the user does not ref lect the positive rating that the user gave and conversely for overly \nnegative scores. Disparities between user reviews and ratings may signify a knowledge gap for the score criteria or a \ncertain level of subjectivity of the scoring.  \nTransformer models provide an automated, fast, and economic system to classify the sentiment of reviews from \nindividuals for specific medications. Furthermore, transformer models' capability to generate a suggestion of a score \nsolely based on user reviews can be utilized as a point of comparison to user -generated reviews. In a clinical study, \nthis discrepancy could potentially contribute towards advancing a conversation with the reviewer to further investigate \nthe cause for such variations. In addition, ELi5 is an easy tool to understand what noteworthy terms contribute to the \nmodel, and potentially reviewers relied on providing more clarity on the logic behind the score. The identification of \nsignificant term contributors through the ELi5 metrics could hint at factors such as adverse events that are important \nin post-market drug surveillance. The binary classification approach of BioBERT and other transformer models could \naid in potentially finding negative drug reviews in data that lacks a numeric score. This filtration of reviews delivers \na vital step to simplify the process in the identification of adverse events, side effects, and poss ible medical \ninteractions. A fast-paced system sentiment score prediction attests to its impact in analyzing large social media drug \ndatasets providing a manageable tool to separate reviews into separate classes. The classified social media data can \nthen be adopted for different purposes such as topic modeling by sentiment types.  \nLimitations and Future Work  \nAlthough this model was able to successfully classify reviews in a binary system, the ability for large class \nidentification is still unknown and warrants further investigation. One important issue with many transformer models \nwas the issue of over-fitting.35 In addition, many transformer models such as XLNet are computationally expensive \nwhich may result in a long training time. Additional research will concentrate on the utilization of transformer models \non non-scored-based social media data. In addition, another area of focus could be to expand this model for multi -\nclass identification as this may be more advantageous in the determination of highly negative reviews. \nConclusions \nThis study presents the construction of transformer -based model s for the classification of drug reviews from \ndrugs.com. The most successful model in this project was the BioBERT model with the highest F1 score. Overall, the \ntransformer models outperformed the traditional machine learning models using bag-of-words features. These binary \ntransformer models tended to be effective at decerning highly optimistic reviews from reviews that contain a mixture \nof positive and negative feedback.   \nAcknowledgements \nThis study was partially supported by the National Institute on Aging (NIA) of the National Institutes of Health (NIH) \nunder Award Number R21AG061431; and in part by Florida State University -University of Florida Clini cal and \nTranslational Science Award funded by National Center for Advancing Translational Sciences under Award Number \nUL1TR001427. The first author would like to thank eHealth Lab at FSU and the Undergraduate Research Opportunity \nProgram at Florida State University for the mentorship and guidance.   \nReferences \n1.  Califf RM. Characteristics of clinical trials registered in clinicaltrials.gov, 2007-2010. JAMA;307(17):1838. \n2.  Farmer KC. Methods for measuring and monitoring medication regimen adherence in clinical trials and clinical \npractice. Clinical Therapeutic.1999;21(6):1074–90. \n3.  He Z, Tang X, Yang X, Guo Y, George TJ, Charness N, et al. Clinical trial generalizability assessment in the big \ndata era: a review. Clin Transl Sci. 2020;13(4):675–84.  \n4.  Mills EJ, Seely D, Rachlis B, Griffith L, Wu P, Wilson K, et al. Barriers to participation in clinical trials of cancer: \na meta-analysis and systematic review of patient-reported factors. The Lancet Oncology. 2006;7(2):141–8.  \n5.  Crombie I. The role of record linkage in post -marketing drug surveillance. British Journal of Clinical \nPharmacology. 1986;22(S1):77S-82S.  \n6.  Shimabukuro TT, Nguyen M, Martin D, DeStefano F. Safety monitoring in the vaccine adverse event reporting \nsystem (vaers). Vaccine. 2015;33(36):4398–405.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 8, 2021. ; https://doi.org/10.1101/2021.04.15.21255573doi: medRxiv preprint \n  \n7.  O’ Donovan B, Rodgers RM, Cox AR, Krska J. Making medicines safer: analysis of patient reports to the uk’s \nyellow card scheme. expert opinion on drug safety. 2019;18(12):1237–43.  \n8.  Yom-Tov E, Gabrilovich E. Postmarket drug surveillance without trial costs: discovery of adverse drug reactions \nthrough large-scale analysis of web search queries. J Med Internet Res. 2013;15(6):e124. \n9.  Dey S, Luo H, Fokoue A, Hu J, Zhang P. Predicting adverse drug reactions through interpretable deep learning \nframework. BMC Bioinformatics. 2018;19(S21):476.  \n10.  Ahmad F, Abbasi A, Kitchens B, Adjeroh DA, Zeng D. Deep  learning for a dverse event detection from web \nsearch. IEEE Trans Knowl Data Eng. 2020;1–1.  \n11.  Moro PL, Arana J, Cano M, Lewis P, Shimabukuro TT. Deaths reported to the vaccine adverse event reporting \nsystem, united states, 1997–2013. Plotkin SA, editor. Clin Infect Dis. 2015;61(6):980–7.  \n12.  Dandala B, Joopu di V, Devarakonda M. Adverse  drug events detection in clinical notes by jointly modeling \nentities and relations using neural networks. Drug Saf. 2019;42(1):135–46. \n13.  Kulldorff M, Davis RL, Kolczak† M, Lewis E, Lieu T, Platt R. A maximized sequential probability ratio test for \ndrug and vaccine safety surveillance. Sequential Analysis. 2011;30(1):58–78. \n14.  Du J, Xiang Y, Sankaranarayanapillai M, Zhang M, Wang J, Si Y, et al. Extracting postmarketing adverse events \nfrom safety reports in the vaccine adverse event reporting system (vaers) using deep learning. Journal of the \nAmerican Medical Informatics Association. 2021; \n15.  Dinh T, Chakraborty G. Detecting side effects and evaluating the effectiveness of drugs from customers’ online \nreviews using text analytics, sentiment analysis, and machine learning models . sas-global-forum-proceedings. \n2020;1–23. \n16.  Hino A, Imai R. Ranking and rating: neglected biases in factor analysis of postmaterialist values . International \nJournal of Public Opinion Research. 2019;31(2):368–81. \n17.  Tanabe P, Buschmann M. A prospective study of ed pain management practices and the patient’s perspective . \nJournal of Emergency Nursing. 1999;25(3):171–7. \n18.  Adusumalli S, Lee H, Hoi Q, Koo S-L, Tan IB, Ng PC. Assessment of web-based consumer reviews as a resource \nfor drug performance. J Med Internet Res. 2015;17(8): e211. \n19.  Lewis DD. Challenges in machine learning for text classification. In: Proceedings of the ninth annual conference \non Computational learning theory  - COLT ’96. Desenzano del Garda, Italy: ACM Press. 1996;1-ff.  \n20.  UCI. drug review dataset (drugs.com) data set . Available from: \nhttps://archive.ics.uci.edu/ml/datasets/Drug+Review+Dataset+%28Drugs.com%29 \n21.  Sairamvinay Vijayaraghavan, Debraj Basu. Sentiment analysis in drug reviews using supervised machine \nlearning algorithms. Available from: https://arxiv.org/abs/2003.11643 \n22.  Gräßer F, Kallumadi S, Malberg H, Zaunseder S. Aspect -based sentiment analysis of drug reviews applying \ncross-domain and cross-data learning. In: Proceedings of the 2018 International Conference on Digital Health . \nLyon France: ACM. 2018;121–5.  \n23.  Devlin J, Chang M-W, Lee K, Toutanova K. BERT: pre-training of deep bidirectional transformers for language \nunderstanding. arXiv:181004805. 2019; \n24.  Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, et al. BioBERT: a pre-trained biomedical language representation \nmodel for biomedical text mining. Wren J, editor. Bioinformatics. 2019; \n25.  Lan Z, Chen M, Goodman S, Gimpel K, Sharma P, Soricut R. ALBERT: a lite bert for self-supervised learning \nof language representations. arXiv:190911942. 2020; \n26.  Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, et al. RoBERTa: a robustly optimized bert pretraining approach. \narXiv:190711692. 2019; \n27.  Yang Z, Dai Z, Yang Y, Carbonell J, Salakhutdinov R, Le QV. XLNET: generalized autoregressive pretraining \nfor language understanding. arXiv:190608237. 2020; \n28.  Clark K, Luong M-T, Le QV, Manning CD. ELECTRA: pre-training text encoders as discriminators rather than \ngenerators. arXiv:200310555. 2020; \n29.  Agarwal N, Das S. Interpretable machine learning tools: a survey. 2020 IEEE Symposium Series on \nComputational Intelligence (SSCI). Canberra, ACT, Australia: IEEE. 2020;1528–34. \n30.  Bodenreider O. The unified medical language system (umls): integrating biomedical terminology. Nucleic Acids \nResearch. 2004;32(90001):267D – 270. \n31.  Luca Soldaini, Nazli Goharian. QuickUMLS: a fast, unsupervised approach for medical concept extraction. 2016; \nAvailable from: http://medir2016.imag.fr/data/MEDIR_2016_paper_16.pdf \n32.  akhilfsu/Classifying-Drug-Ratings-Using-User-Reviews-with-Transformer-Based-Language-Models. Available \nfrom: https://github.com/akhilfsu/Classifying -Drug-Ratings-Using-User-Reviews-with-Transformer-Based-\nLanguage-Models \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 8, 2021. ; https://doi.org/10.1101/2021.04.15.21255573doi: medRxiv preprint \n  \n33.  Abou Taam M, Rossard C, Cantaloube L, Bouscaren N, Roche G, Pochard L, et al. Analysis of patients’ narratives \nposted on social media websites on benfluorex’s (Mediator ® ) withdrawal in France. J Clin Pharm Ther [Internet]. \n2014;39(1):53–5. \n34.  Pipalia K, Bhadja R, Shukla M. Comparative analysis of different transformer based architectures used in \nsentiment analysis. In: 2020 9th International Conference System Modeling and Advancement in Research \nTrends (SMART). Moradabad, India: IEEE. 2020;411–5. \n35.  Komatsuzaki A. One epoch is all you need. arXiv:190606669. 2019;  \n \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted September 8, 2021. ; https://doi.org/10.1101/2021.04.15.21255573doi: medRxiv preprint ",
  "topic": "Random forest",
  "concepts": [
    {
      "name": "Random forest",
      "score": 0.7435302734375
    },
    {
      "name": "Computer science",
      "score": 0.7109278440475464
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6727750301361084
    },
    {
      "name": "Transformer",
      "score": 0.6364519596099854
    },
    {
      "name": "Machine learning",
      "score": 0.602056086063385
    },
    {
      "name": "Natural language processing",
      "score": 0.5775891542434692
    },
    {
      "name": "Naive Bayes classifier",
      "score": 0.5068197846412659
    },
    {
      "name": "Bayesian probability",
      "score": 0.43584030866622925
    },
    {
      "name": "Artificial neural network",
      "score": 0.4242873787879944
    },
    {
      "name": "Information retrieval",
      "score": 0.3243445158004761
    },
    {
      "name": "Support vector machine",
      "score": 0.1429019570350647
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}