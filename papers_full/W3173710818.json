{
    "title": "On the Interplay Between Fine-tuning and Composition in Transformers",
    "url": "https://openalex.org/W3173710818",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2097310566",
            "name": "Lang Yu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2404566579",
            "name": "Allyson Ettinger",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2962736243",
        "https://openalex.org/W4288379066",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4287646350",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2982756474",
        "https://openalex.org/W2963859254",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3104136798",
        "https://openalex.org/W2067438047",
        "https://openalex.org/W3100475986",
        "https://openalex.org/W3092448486",
        "https://openalex.org/W2964165804",
        "https://openalex.org/W3037725254",
        "https://openalex.org/W2932893307",
        "https://openalex.org/W3104739822",
        "https://openalex.org/W2962694015",
        "https://openalex.org/W3098300729",
        "https://openalex.org/W2952984539",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2970565456",
        "https://openalex.org/W2954604190",
        "https://openalex.org/W2970853769",
        "https://openalex.org/W2804339109",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W3101449015",
        "https://openalex.org/W2972778456",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W2251044566",
        "https://openalex.org/W2970752815",
        "https://openalex.org/W3104350794",
        "https://openalex.org/W4288573351",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2996132992",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W3037626499",
        "https://openalex.org/W3124174361",
        "https://openalex.org/W4300618906",
        "https://openalex.org/W2790235966",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W3154165903",
        "https://openalex.org/W1854884267",
        "https://openalex.org/W4298392964",
        "https://openalex.org/W2964204621",
        "https://openalex.org/W3035160860",
        "https://openalex.org/W2888329843",
        "https://openalex.org/W2963025830",
        "https://openalex.org/W2962727366",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2799124508",
        "https://openalex.org/W2951286828",
        "https://openalex.org/W2946359678",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W3037530970",
        "https://openalex.org/W2963969878",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W4288351520",
        "https://openalex.org/W2788751659",
        "https://openalex.org/W2515741950",
        "https://openalex.org/W2251882135",
        "https://openalex.org/W2996851481",
        "https://openalex.org/W2962843521",
        "https://openalex.org/W3004346089",
        "https://openalex.org/W2516090925",
        "https://openalex.org/W3102659883",
        "https://openalex.org/W2053921957",
        "https://openalex.org/W4288255289",
        "https://openalex.org/W2963366649",
        "https://openalex.org/W3118017018",
        "https://openalex.org/W3104763958",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W3098680936",
        "https://openalex.org/W2983040767",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W2962776659",
        "https://openalex.org/W2941666437",
        "https://openalex.org/W2972498556",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2964060387"
    ],
    "abstract": "Pre-trained transformer language models have shown remarkable performance on a variety of NLP tasks.However, recent research has suggested that phrase-level representations in these models reflect heavy influences of lexical content, but lack evidence of sophisticated, compositional phrase information (Yu and Ettinger, 2020).Here we investigate the impact of fine-tuning on the capacity of contextualized embeddings to capture phrase meaning information beyond lexical content.Specifically, we fine-tune models on an adversarial paraphrase classification task with high lexical overlap, and on a sentiment classification task.After fine-tuning, we analyze phrasal representations in controlled settings following prior work.We find that fine-tuning largely fails to benefit compositionality in these representations, though training on sentiment yields a small, localized benefit for certain models.In follow-up analyses, we identify confounding cues in the paraphrase dataset that may explain the lack of composition benefits from that task, and we discuss potential factors underlying the localized benefits from sentiment training.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2279–2293\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n2279\nOn the Interplay Between Fine-tuning and Composition in Transformers\nLang Yu\nDeptartment of Computer Science\nUniversity of Chicago\nlangyu@uchicago.edu\nAllyson Ettinger\nDepartment of Linguistics\nUniversity of Chicago\naettinger@uchicago.edu\nAbstract\nPre-trained transformer language models have\nshown remarkable performance on a variety\nof NLP tasks. However, recent research has\nsuggested that phrase-level representations in\nthese models reﬂect heavy inﬂuences of lexi-\ncal content, but lack evidence of sophisticated,\ncompositional phrase information (Yu and Et-\ntinger, 2020). Here we investigate the impact\nof ﬁne-tuning on the capacity of contextual-\nized embeddings to capture phrase meaning\ninformation beyond lexical content. Speciﬁ-\ncally, we ﬁne-tune models on an adversarial\nparaphrase classiﬁcation task with high lexical\noverlap, and on a sentiment classiﬁcation task.\nAfter ﬁne-tuning, we analyze phrasal represen-\ntations in controlled settings following prior\nwork. We ﬁnd that ﬁne-tuning largely fails\nto beneﬁt compositionality in these represen-\ntations, though training on sentiment yields a\nsmall, localized beneﬁt for certain models. In\nfollow-up analyses, we identify confounding\ncues in the paraphrase dataset that may explain\nthe lack of composition beneﬁts from that task,\nand we discuss potential factors underlying the\nlocalized beneﬁts from sentiment training.\n1 Introduction\nTransformer language models like BERT (Devlin\net al., 2019), GPT (Radford et al., 2018, 2019) and\nXLNet (Yang et al., 2019b), have improved the\nstate-of-art in many NLP tasks since their introduc-\ntion. The versatility of these pre-trained models\nsuggests that they may acquire fairly robust linguis-\ntic knowledge and capacity for natural language\n“understanding”. However, an emerging body of\nanalysis demonstrates a level of superﬁciality in\nthese models’ handling of language (Niven and\nKao, 2019; Kim and Linzen, 2020; McCoy et al.,\n2019; Ettinger, 2020; Yu and Ettinger, 2020).\nIn particular, although composition—a model’s\ncapacity to combine meaning units into more com-\nplex units reﬂecting phrase meanings—is an indis-\npensable component of language understanding,\nwhen testing for composition in pre-trained trans-\nformer representations, Yu and Ettinger (2020) re-\nport that these representations reﬂect word content\nof phrases, but don’t show signs of more sophisti-\ncated humanlike composition beyond word content.\nIn the present paper we perform a direct follow-\nup of that study, asking whether models will show\nbetter evidence of composition after ﬁne-tuning\non tasks that are good candidates for requiring\ncomposition: 1) the Quora Question Pairs dataset\nin Paraphrase Adversaries from Word Scrambling\n(PAWS-QQP) (Zhang et al., 2019a), an adversar-\nial paraphrase dataset forcing models to classify\nparaphrases with high lexical overlap, and 2) the\nStanford Sentiment Treebank (Socher et al., 2013),\na sentiment dataset with ﬁne-grained phrase labels\nto promote composition. We base our analysis on\nthe tests proposed by Yu and Ettinger (2020), which\nrely on alignment with human judgments of phrase\npair similarities, and which leverage control of lexi-\ncal overlap to target compositionality. We ﬁne-tune\nand evaluate the same models and representation\ntypes tested in that paper, for optimal comparison.\nWe ﬁnd that across the board, ﬁne-tuning on\nPAWS-QQP does not improve compositionality—\nif anything, performance on composition metrics\ntends to degrade. Composition performance also\nremains low after training on SST, but we do see\nsome localized improvements for certain models.\nAnalyzing the PAWS-QQP dataset, we ﬁnd reli-\nable superﬁcial cues to paraphrase labels (distance\nof word swap), explaining in part why ﬁne-tuning\non that task might fail to improve composition—\nand reinforcing the need for caution in interpreting\ndifﬁculty of NLP tasks. We also discuss the con-\ntribution of variation in size of labeled phrases in\nSST, with respect to the beneﬁts that result from\nﬁne-tuning on that task. All experimental code and\n2280\ndata are made available for further testing.1\n2 Related work\nExtensive work has studied the nature of learned\nrepresentations in NLP models (Adi et al., 2016;\nConneau et al., 2018; Ettinger et al., 2016; Dur-\nrani et al., 2020). Our work builds in particular\non analysis of contextualized representations (Ba-\ncon and Regier, 2019; Tenney et al., 2019; Peters\net al., 2018; Hewitt and Manning, 2019; Klafka\nand Ettinger, 2020; Toshniwal et al., 2020). Other\nwork that has focused on transformers, as we do,\nhas often focused on analyzing the attention mech-\nanism (Vig and Belinkov, 2019; Clark et al., 2019),\nlearned parameters (Roberts et al., 2020; Radford\net al., 2019; Raffel et al., 2020) and redundancy\n(Dalvi et al., 2020; V oita et al., 2019; Michel et al.,\n2019). The evaluation that we use here follows\nthe paradigm of classiﬁcation-based probing (Kim\net al., 2019; Wang et al., 2018; Zhang et al., 2019b;\nYang et al., 2019a) and correlation with similar-\nity judgments (Finkelstein et al., 2001; Gerz et al.,\n2016; Hill et al., 2015; Conneau and Kiela, 2018).\nThe current paper also builds on work subject-\ning trained NLP models to adversarial inputs, to\nhighlight model weaknesses. One body of work\napproaches the problem by applying heuristic rules\nof perturbation to input sequences (Wallace et al.,\n2019; Jia and Liang, 2017; Zhang et al., 2019a),\nwhile another uses neural models to construct ad-\nversarial examples (Li et al., 2020, 2018) or ma-\nnipulate inputs in embedding space (Jin et al.,\n2020). Our work also contributes to efforts to un-\nderstand impacts and outcomes of the ﬁne-tuning\nprocess (Miaschi et al., 2020; Mosbach et al., 2020;\nWang et al., 2020; Perez-Mayos et al., 2021).\nPhrase and sentence composition has drawn fre-\nquent attention in analysis of neural models, often\nfocusing on analysis of internal representations and\ndownstream task behavior (Ettinger et al., 2018;\nConneau et al., 2019; Nandakumar et al., 2019;\nMcCoy et al., 2019; Yu and Ettinger, 2020; Bha-\nthena et al., 2020; Mu and Andreas, 2020; Andreas,\n2019). Some work investigates compositionality\nvia constructing linguistic (Keysers et al., 2019)\nand non-linguistic (Liˇska et al., 2018; Hupkes et al.,\n2018; Baan et al., 2019) synthetic datasets.\nMost related to our work here is the ﬁnding of Yu\n1Datasets and code available at\nhttps://github.com/yulang/ﬁne-tuning-and-composition-\nin-transformers\nand Ettinger (2020). They test for composition in\ntwo-word phrase representations from transform-\ners, via similarity correlations and paraphrase de-\ntection. They ﬁnd that baseline performance on\nthese tasks is high, but once they control for amount\nof word overlap, performance drops dramatically,\nsuggesting that observed correspondences rely on\nword content rather than phrase composition. We\nbuild directly on this work, testing whether these\npatterns will still hold after ﬁne-tuning on tasks\nintended to encourage composition.\n3 Fine-tuning Pre-trained Transformers\nIn response to the weaknesses observed by Yu and\nEttinger (2020), we select two different datasets\nwith promising characteristics for addressing these\nweaknesses. We ﬁne-tune on these tasks, then per-\nform layer-wise testing on contextualized repre-\nsentations from the ﬁne-tuned models, comparing\nagainst results on the pre-trained models. Here we\ndescribe the two ﬁne-tuning datasets.\n3.1 PA WS: ﬁne-tuning on high word overlap\nThe core of the Yu and Ettinger (2020) ﬁnding\nis that model performance on the selected com-\nposition tests degrades signiﬁcantly when cues of\nlexical overlap are controlled. It stands to reason,\nthen, that a model trained to discern meaning dif-\nferences under conditions of high lexical overlap\nmay improve on these overlap-controlled compo-\nsition tests. This drives our selection of the Para-\nphrase Adversaries from Word Scrambling (PAWS)\ndataset (Zhang et al., 2019b), which consists of sen-\ntence pairs with high lexical overlap. The task is\nformulated as binary classiﬁcation of whether two\nsentences are paraphrases or not. State-of-the-art\nmodels achieve only < 40% accuracy before train-\ning on the dataset (Zhang et al., 2019a). Table 1\nshows examples from this dataset. Due to the high\nlexical overlap, we might expect that in order to\nachieve non-trivial accuracy on this task, models\nmust attend to more sophisticated meaning infor-\nmation than simple word content.\n3.2 SST: ﬁne-tuning on hierarchical labels\nAnother dataset that has been associated with train-\ning and evaluation of phrasal composition is the\nStanford Sentiment Treebank, which contains syn-\ntactic phrases of various lengths, together with ﬁne-\ngrained human-annotated sentiment labels for these\nphrases. Because this dataset contains annotations\n2281\nSentence 1 Sentence 2 Label\nThere are also speciﬁc discussions , public\nproﬁle debates and project discussions .\nThere are also public discussions , proﬁle spe-\nciﬁc discussions , and project discussions . 0\nShe worked and lived in Stuttgart , Berlin (\nGermany ) and in Vienna ( Austria ) .\nShe worked and lived in Germany ( Stuttgart ,\nBerlin ) and in Vienna ( Austria ) . 1\nTable 1: Example pairs from PAWS-QQP. Both positive and negative pairs have high bag-of-words overlap.\nof composed phrases of various sizes, we can rea-\nsonably expect that training on this dataset may fos-\nter an increased sensitivity to compositional phrase\nmeaning. We formulate the ﬁne-tuning task as\na 5-class classiﬁcation task following the setup\nin Socher et al. (2013). The models are trained\nto predict sentiment labels given phrases as input.\n4 Representation evaluation\nFor optimal comparison of the effects of ﬁne-tuning\non the above tasks, we replicate the tests, represen-\ntation types, and models reported on by Yu and\nEttinger. Here we brieﬂy describe these methods.\nFor more details on the evaluation dataset and task\nsetup, please refer to Yu and Ettinger (2020).\n4.1 Evaluation tasks\nYu and Ettinger propose two analyses for measur-\ning composition: similarity correlations and para-\nphrase classiﬁcation. They focus on two-word\nphrases, using the BiRD bigram relatedness dataset\n(Asaadi et al., 2019) for similarity correlations, and\nthe PPDB 2.0 paraphrase database (Ganitkevitch\net al., 2013; Pavlick et al., 2015) for paraphrase\nclassiﬁcation. BiRD contains 3,345 bigram pairs,\nwith source phrases paired with numerous target\nphrases, and human-annotated similarity scores\nranging from 0 to 1. For similarity correlation,\nYu and Ettinger take layer-wise correlations be-\ntween these human phrase similarity scores and\nthe cosine similarities of model representations for\nthe same phrases. For paraphrase classiﬁcation,\nYu and Ettinger train a multi-layer perceptron clas-\nsiﬁer to label whether two phrase representations\nare paraphrases, drawing their positive phrase pairs\nfrom PPDB 2.0—which contains paraphrases with\nscores generated by a regression model—and ran-\ndomly sampling negative pairs from the rest of the\ndataset. We replicate all of these procedures.\nFor both task types, Yu and Ettinger compare be-\ntween “uncontrolled” and “controlled” tests, with\nthe latter ﬁltering the data to control word over-\nlap within phrase pairs, such that amount of word\noverlap between two phrases can no longer be used\nas a cue for how similar the meanings are. It is\non these controlled settings that Yu and Ettinger\nobserve the signiﬁcant drop in performance, sug-\ngesting that model representations lack the compo-\nsitional knowledge to discern phrase meaning be-\nyond word content. Below we will report results for\nboth settings, with particular focus on controlled\nsettings.\n4.2 Representation types\nFollowing Yu and Ettinger, for each input phrase\nwe test as a potential representation 1) CLS to-\nken, 2) average of tokens within the phrase (Avg-\nPhrase), 3) average of all input tokens (Avg-All),\n4) embedding of the second word of the phrase,\nintended to approximate the semantic head (Head-\nWord), and 5) SEP token. We test each of these\nrepresentations at every layer of each model.2\n5 Experimental setup\nWe ﬁne-tune and analyze the same models that Yu\nand Ettinger test in pre-trained form: BERT (De-\nvlin et al., 2019), RoBERTa (Liu et al., 2019), Dis-\ntilBERT (Sanh et al., 2019), XLNet (Yang et al.,\n2019b) and XLM-RoBERTa (Conneau et al., 2019).\nIn each case, the pre-trained “base” version is used\nas the starting point for ﬁne-tuning. We use the\nimplementation of Wolf et al. (2019) 3 based on\nPyTorch (Paszke et al., 2019).\nWe ﬁne-tune these models on the two datasets\ndescribed in Section 3. The Quora Question\nPairs dataset in Paraphrase Adversaries from Word\nScrambling (PAWS-QQP)4 consists of a training\nset with 11,988 sentence pairs, and a dev/test set\nwith 677 sentence pairs. Tuning on PAWS-QQP\nis formulated as binary classiﬁcation. Sentences\nare passed as input and models are trained to pre-\n2Like Yu and Ettinger, we also test both phrase-only input\n(encoder input consists only of two-word phrase plus spe-\ncial CLS/SEP tokens), as well as inputs in which phrases are\nembedded in sentence contexts.\n3https://github.com/huggingface/\ntransformers\n4https://github.com/\ngoogle-research-datasets/paws\n2282\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\nCorrelation\nBERT\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7 RoBERTa\n0 1 2 3 4 5 6−0.1\n0.1\n0.3\n0.5\n0.7 DistilBERT\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7 XLM-RoBERTa\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\nCLS\nXLNet\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\nCorrelation\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\nHT\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\nCorrelation\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\nSEP\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\nCorrelation\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\nAP\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n−0.1\n0.1\n0.3\n0.5\n0.7\nCorrelation\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6\nLayer\n−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n−0.1\n0.1\n0.3\n0.5\n0.7\nAA\nPretrained PAWS -tuned SST-tuned\nFigure 1: Similarity correlation on uncontrolled BiRD dataset, with phrase-only input. Columns correspond\nto models, and rows correspond to representation types (“HT” = Head-token, “AP” = Avg-Phrase and “AA” =\nAvg-All). For each model and representation type, the corresponding subplot shows correlations for pre-trained,\nPAWS-tuned and SST-tuned settings, respectively. For each subplot, X-axis corresponds to layer index, and Y-axis\ncorresponds to correlation value. Layer 0 corresponds to input embeddings passed to the model.\ndict whether the input sentences are paraphrases\nor not. Models are trained on the training set, and\nvalidated on the dev/test set for convergence.\nThe Stanford Sentiment Treebank (SST) 5\n(Socher et al., 2013) contains 215,154 phrases.\n15% of the data is reserved for validation. The\nﬁne-tuning task is formulated as 5-class classiﬁca-\ntion on sentiment labels, where models are given\nphrases as input, and asked to predict sentiment. In\nboth tasks, we use the Adam optimizer (Kingma\nand Ba, 2014) with default weight decay. We train\nthe models until convergence on the validation set.\nThe evaluation tasks consist of correlation analy-\nsis and paraphrase classiﬁcation. For correlation in\nthe uncontrolled setting, we use the complete BiRD\ndataset, containing 3,345 phrase pairs.6 For the con-\ntrolled test, we ﬁlter the complete dataset following\nthe criteria in Yu and Ettinger (2020), resulting in\n410 “AB-BA” mirror-image pairs with 100% word\noverlap (e.g., law school / school law). For the\nclassiﬁcation tasks, we use the preprocessed data\nreleased by Yu and Ettinger (2020). 7 We collect\n12,036 source-target phrase pairs from the prepro-\n5https://nlp.stanford.edu/sentiment/\ntreebank.html\n6http://saifmohammad.com/WebPages/BiRD.\nhtml\n7https://github.com/yulang/\nphrasal-composition-in-transformers\ncessed dataset for our uncontrolled classiﬁcation\nsetting, and for the controlled classiﬁcation setting,\nwe collect 11,772 phrase pairs with exactly 50%\nword overlap in each pair, following the procedure\nfrom the original paper.\n6 Results after ﬁne-tuning\n6.1 Full datasets\nFigure 1 presents the original results from Yu and\nEttinger (2020) on pre-trained models, alongside\nour new results after ﬁne-tuning, on the full BiRD\ndataset. Since this is prior to the control of word\noverlap, these correlations can be expected to re-\nﬂect effects of lexical content encoding, without yet\nhaving isolated effects of composition. We ﬁnd that\nafter ﬁne-tuning on SST, most models and represen-\ntation types show small improvements in peak cor-\nrelations across layers, while ﬁne-tuning on PAWS\nalso yields improvements in peak correlations—\nalbeit even smaller—in models other than BERT\nand XLM-RoBERTa. Overall, within a given\nrepresentation type, improvements are generally\nstronger after ﬁne-tuning on SST than on PAWS.\nBetween representation types, Avg-Phrase and Avg-\nAll remain consistently at the highest correlations\nafter ﬁne-tuning. Additionally, we see that the de-\ncline in correlation at later layers in pre-trained\nBERT, RoBERTa and XLM-RoBERTa is mitigated\n2283\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\nCorrelation\nBERT\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4 RoBERTa\n0 1 2 3 4 5 6−0.4\n−0.2\n0.0\n0.2\n0.4 DistilBERT\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4 XLM-RoBERTa\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\nCLS\nXLNet\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\nCorrelation\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\nHT\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\nCorrelation\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\nSEP\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\nCorrelation\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\nAP\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n−0.4\n−0.2\n0.0\n0.2\n0.4\nCorrelation\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6\nLayer\n−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n−0.4\n−0.2\n0.0\n0.2\n0.4\nAA\nPretrained PAWS -tuned SST-tuned\nFigure 2: Similarity correlation on controlled BiRD dataset (AB-BA setting), with phrase-only input.\nafter ﬁne-tuning. Model-wise, we see the most\nsigniﬁcant improvements in the RoBERTa model,\nfor which the correlations become more consistent\nacross layers for most representation types. As\nwe discuss below, we take this as indication that\nthe ﬁne-tuning promotes more robust retention of\nword content information across layers, if not more\nrobust phrasal composition.\nFor the sake of space, we present the plots of\nthe uncontrolled paraphrase classiﬁcation setting\nin Figure 7 of the Appendix. The overall improve-\nments are even smaller than those seen in the corre-\nlations, but we do see comparable patterns in these\nparaphrase classiﬁcation results, in particular with\nSST showing slightly stronger beneﬁts than PAWS.\n6.2 Controlled datasets\nAbove we see small beneﬁts of ﬁne-tuning for\nperformance on the full, uncontrolled datasets.\nHowever, the critical question for our purposes\nis whether correlations also show improvements in\nword-overlap controlled settings, which better iso-\nlate effects of composition. Figure 2 shows correla-\ntions for all models on the controlled AB-BA (full\nword overlap) correlation test. Figure 3 shows the\nresults for the controlled paraphrase classiﬁcation\nsetting, where both paraphrase and non-paraphrase\npairs have exactly 50% word overlap.\nThe ﬁrst comparison to note is between original\nand controlled settings, which allows us to establish\nthe contributions of overlap information as opposed\nto composition. Comparing between Figure 1 and\nFigure 2, it is clear that ﬁne-tuned models still show\nsubstantial reduction in correlation when overlap\ncues are removed. The same goes for Figure 3 (by\ncomparison to Figure 7 of the Appendix)—we see\nthat on the controlled dataset, accuracies hover just\nabove chance-level performance both before and\nafter ﬁne-tuning, compared to over 90% accuracy\non the uncontrolled dataset. This gap in perfor-\nmance between the original and controlled datasets\nmirrors the ﬁndings of Yu and Ettinger (2020), and\nsuggests that even after ﬁne-tuning, the majority of\ncorrespondence between model phrase representa-\ntions and human meaning similarity judgments can\nbe attributed to capturing of word content informa-\ntion rather than phrasal composition.\nThe second key comparison is between pre-\ntrained and ﬁne-tuned models within the overlap-\ncontrolled settings. While the prior comparison\ntells us that similarity correspondence is still domi-\nnated by word content effects, this second compar-\nison can tell us whether ﬁne-tuning shows at least\nsome boost in meaning composition relative to pre-\ntraining. Comparing performance of pre-trained\nand ﬁne-tuned models in Figure 2, we see that ﬁne-\ntuning on PAWS-QQP actually slightly degrades\ncorrelations at many layers for a majority of mod-\nels and representation types—with improvements\nlargely restricted to XLM-RoBERTa and XLNet\n(perhaps notably, mostly in cases where pre-trained\ncorrelations are negative). This is despite the fact\nthat models achieve strong validation performance\non PAWS-QQP (as shown in Table 2), suggesting\n2284\n0 1 2 3 4 5 6 7 8 91011120.4\n0.5\n0.6Accuracy\nBERT\n0 1 2 3 4 5 6 7 8 91011120.4\n0.5\n0.6\nRoBERTa\n0 1 2 3 4 5 60.4\n0.5\n0.6\nDistilBERT\n0 1 2 3 4 5 6 7 8 91011120.4\n0.5\n0.6\nXLM-RoBERTa\n0 1 2 3 4 5 6 7 8 91011120.4\n0.5\n0.6\nCLS\nXLNet\n0 1 2 3 4 5 6 7 8 91011120.4\n0.5\n0.6Accuracy\n0 1 2 3 4 5 6 7 8 91011120.4\n0.5\n0.6\n0 1 2 3 4 5 60.4\n0.5\n0.6\n0 1 2 3 4 5 6 7 8 91011120.4\n0.5\n0.6\n0 1 2 3 4 5 6 7 8 91011120.4\n0.5\n0.6\nHT\n0 1 2 3 4 5 6 7 8 91011120.4\n0.5\n0.6Accuracy\n0 1 2 3 4 5 6 7 8 91011120.4\n0.5\n0.6\n0 1 2 3 4 5 60.4\n0.5\n0.6\n0 1 2 3 4 5 6 7 8 91011120.4\n0.5\n0.6\n0 1 2 3 4 5 6 7 8 91011120.4\n0.5\n0.6\nSEP\n0 1 2 3 4 5 6 7 8 91011120.4\n0.5\n0.6Accuracy\n0 1 2 3 4 5 6 7 8 91011120.4\n0.5\n0.6\n0 1 2 3 4 5 60.4\n0.5\n0.6\n0 1 2 3 4 5 6 7 8 91011120.4\n0.5\n0.6\n0 1 2 3 4 5 6 7 8 91011120.4\n0.5\n0.6\nAP\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n0.4\n0.5\n0.6Accuracy\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n0.4\n0.5\n0.6\n0 1 2 3 4 5 6\nLayer\n0.4\n0.5\n0.6\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n0.4\n0.5\n0.6\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n0.4\n0.5\n0.6\nAA\nPretrained PAWS-tuned SST-tuned\nFigure 3: Paraphrase classiﬁcation accuracy on controlled PPDB dataset (50% word overlap setting) with phrase-\nonly input. Y-axis range is smaller relative to Figure 7, to make changes from pre-training more visible.\nthat learning this task does little to improve compo-\nsition. We will explore the reasons for this below.\nIn Figure 3, we see that ﬁne-tuning also does\nlittle to improve paraphrase classiﬁcation accura-\ncies in the controlled setting—though each model\nshows slight improvement in peak accuracy across\nlayers and representation types (e.g., RoBERTa\nshows ∼3% increase in peak accuracy with SST\ntuning, and 2% with PAWS tuning). Even so, the\nbest accuracies across models continue to be only\nmarginally above chance. This, too, fails to provide\nevidence of any substantial composition improve-\nment resulting from the ﬁne-tuning process.\nThe story changes slightly when we turn to im-\npacts of SST ﬁne-tuning on correlations in Figure\n2. While all correlations remain low after SST ﬁne-\ntuning, we do see that correlations for BERT, XLM-\nRoBERTa and XLNet show some non-trivial bene-\nﬁts even in the controlled setting. In particular, SST\ntuning consistently improves correlation among all\nrepresentation types in BERT (except for minor\ndegradation in later layers for Head-token), boost-\ning the highest correlation from ∼0.2 to ∼0.39.\nBetween representation types, the greatest change\nis in the CLS token, with the most dramatic point\nof improvement being an abrupt correlation peak\nfor CLS at BERT’s fourth layer. We will discuss\nmore below about this localized beneﬁt.\nA ﬁnal important observation is that ﬁne-tuning\non either dataset produces clear degradation in cor-\nrelations for all representation types in RoBERTa\nModel Accuracy(%)\nBERT 80.13\nRoBERTa 90.81\nDistilBERT 81.98\nXLM-RoBERTa 91.18\nXLNet 88.24\nLinear CLF 71.34\nTable 2: Accuracy of ﬁne-tuned models on PAWS-QQP\ndev/test set. Linear CLF is a baseline classiﬁer with\nrelative swapping distance as the only input feature.\nunder the controlled setting, by contrast to the gen-\neral improvements seen for that and other models\nin the uncontrolled setting. This suggests that at\nleast for that model, ﬁne-tuning encourages reten-\ntion or enhancement of lexical information, but\nmay degrade compositional phrase information.8\n7 Analyzing impact of ﬁne-tuning\nThe presented results suggest that despite com-\npelling reasons to think that ﬁne-tuning on the se-\nlected tasks may improve composition of phrase\nmeaning, these models mostly do not exhibit note-\nworthy beneﬁts from ﬁne-tuning. In particular, ﬁne-\n8Following Yu and Ettinger (2020), in addition to phrase-\nonly inputs we also try embedding target phrases in sentence\ncontexts. Consistent with the ﬁndings of Yu and Ettinger\n(2020), we see that presence of context words does boost over-\nall correlation and accuracy, but does not alter the general\ntrends. Moreover, models still show relatively weak perfor-\nmance on controlled tasks even with context available (see\nFigure 8 and Figure 9 in the Appendix for details).\n2285\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n−0.4\n−0.2\n0.0\n0.2\n0.4\nCorrelation\nLength 2\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n−0.4\n−0.2\n0.0\n0.2\n0.4 Length 3\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n−0.4\n−0.2\n0.0\n0.2\n0.4 Length 4\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n−0.4\n−0.2\n0.0\n0.2\n0.4\nCorrelation\nLength 5\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n−0.4\n−0.2\n0.0\n0.2\n0.4 Length 6\n0 1 2 3 4 5 6 7 8 9 10 11 12\nLayer\n−0.4\n−0.2\n0.0\n0.2\n0.4 Full Data et\nCLS Head Word SEP Avg Phra e Avg All\nFigure 4: Layer-wise correlation of BERT ﬁne-tuned on phrases of different lengths in SST.\n0.00 0.25 0.50 0.75\n20\n40#samples\nBERT\n0.0 0.2 0.4 0.6\n20\n40\nRoBERTa\n0.00 0.25 0.50 0.75\n20\n40\nDistilBERT\n0.00 0.25 0.50 0.75\n10\n20\n30#samples\nXLM-RoBERTa\n0.00 0.25 0.50 0.75\n20\n40\nXLNet\n0.00 0.25 0.50 0.75\n20\n40\nTest set statistics\nRelative swapping distance distribution\npos_prediction neg_prediction\nFigure 5: Distribution of positive and negative predic-\ntions made by tuned models. Last plot shows the statis-\ntics in the PAWS-QQP dev/test set. X-axis corresponds\nto relative swapping distance; Y-axis shows number of\nsamples in the speciﬁc relative swapping distance bin.\ntuning on the PAWS-QQP dataset often degrades\nperformance on the controlled datasets taken to be\nmost indicative of compositionality. As for SST,\nthe beneﬁts are minimal, but in localized cases like\nBERT’s CLS token, we do see signs of improved\ncompositionality. In this section, we conduct fur-\nther analysis on the impacts of ﬁne-tuning, and\ndiscuss why tuned models behave as they do.\n7.1 Failure of PA WS-QQP\nTable 2 shows accuracy of ﬁne-tuned models on\nthe dev/test set of PAWS-QQP.9 It is clear that the\nmodels are learning to perform well on this dataset,\nbut our results above indicate that this does not\ntranslate to improved composition sensitivity.\nWe explore the possibility that this discrepancy\nmay be caused by trivial cues arising during the\n9The performance of BERT in the table is different from\nprevious work mainly due to the fact that models in Zhang et al.\n(2019a) are tuned on concatenation of QQP and PAWS-QQP\ndatasets rather than PAWS-QQP only.\nconstruction of the dataset, enabling models to in-\nfer paraphrase labels without needing to improve\ntheir understanding of the meaning of the sentence\npair (c.f., Poliak et al., 2018; Gururangan et al.,\n2018). Sentence pairs in PAWS are generated via\nword swapping and back translation to ensure high\nbag-of-words overlap (Zhang et al., 2019a). We\nhypothesize that models may be able to achieve\nhigh performance in this task based on distance of\nthe word swap alone, without requiring any sophis-\nticated representation of sentence meaning.\nTo test this, given a sentence pair (s1, s2) with\nword counts l1, l2, respectively, we deﬁne “relative\nswapping distance” as\ndistrelative = distswap\nmax(l1, l2)\nwhere distswap is deﬁned as the index difference\nof the ﬁrst swapping word in s1 and s2. For the\nexample shown in the ﬁrst row of Table 1, the ﬁrst\nswapping word is “speciﬁc”, with distswap = 4.\nNote that with this measure we focus on informa-\ntion from one word swap only, while some pairs\nin PAWS-QQP have multiple swapped words—so\nin reality, swapping distance information may be\neven stronger than our results below indicate.\nIn the last plot of Figure 5, we show an asso-\nciation between relative swapping distance and\nparaphrase labels in the PAWS dev/test set: sen-\ntence pairs with small swapping distance tend to\nbe positive samples, while large swapping distance\nassociates with negative labels. The other plots in\nFigure 5 show distribution of positive and negative\npredictions generated by each ﬁne-tuned model\nwith respect to relative swapping distance. We see\na similar pattern, with models tending to generate\nnegative labels when swapping distance is larger.\n2286\n0 1 2 3 4 5 6 7 8 9 10 11 12\nCLS\nHead Word\nSEP\nAvg Phrase\nAvg All\nPAWS-Tuned\n0 1 2 3 4 5 6 7 8 9 10 11 12\nCLS\nHead Word\nSEP\nAvg Phrase\nAvg All\nSentiment-Tuned\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nFigure 6: Average layer-wise embedding similarity between ﬁne-tuned and pre-trained BERT. The upper half\nshows the comparison between PAWS-QQP tuned and pre-trained BERT. And the lower half presents Sentiment\nTreebank-tuned v.s. pre-trained. Embeddings are evaluated using full BiRD dataset for input.\nTo verify the viability of this cue, we train a sim-\nple linear classiﬁer on PAWS-QQP, with relative\nswapping distance as the only input feature. The re-\nsults are reported as “Linear CLF” in Table 2. Even\nwithout access to the content of the sentences, we\nsee that this simple model is able to achieve non-\ntrivial and comparably good classiﬁcation accuracy\non the dev/test set. The strong performance of the\nlinear classiﬁer and the distribution of predictions\nare consistent with the hypothesis that when we\ntune on PAWS-QQP, rather than forcing models\nto learn nuanced meaning in the absence of word\noverlap cues, we may instead encourage models to\nfocus on lower-level information having little to do\nwith the sentence meaning, further degrading their\nperformance on the composition tasks.\n7.2 Localized impacts of SST\nFine-tuning on sentiment shows a bit of a different\npattern—while it mostly shows only minor changes\nfrom pre-training, and the correlations and classiﬁ-\ncation accuracies remain at decidedly low levels on\nthe controlled settings, we do see in certain mod-\nels some distinctive changes in levels of similarity\ncorrelation as a result of tuning on SST. Notably,\nsince these improvement patterns are seen in the\nsimilarity correlations but not in the classiﬁcation\naccuracies, this suggests that these two tasks are\npicking up on slightly different aspects of phrasal\ncompositionality. To investigate these effects fur-\nther, we focus our attention on BERT, which shows\nthe most distinctive improvement in correlations.\nThe obvious candidate for the source of the local-\nized SST beneﬁt is the dataset’s inclusion of labeled\nsyntactic phrases of various sizes. The beneﬁts\nseen from SST tuning suggest that this may indeed\nencourage models to gain some ﬁner-grained sensi-\ntivity to compositional impacts of phrase structure\n(at least those relevant for sentiment). To examine\nthis further, we ﬁlter the SST dataset to subsets\nwith phrases of the same length, from 2 to 6 words,\nand tune pre-trained BERT on each subset.\nFigure 4 shows the correlations for BERT, ﬁne-\ntuned on each phrase length, on the overlap-\ncontrolled BiRD dataset. We see that tuning\non the full dataset (mixed phrase lengths) gives\nthe strongest fourth-layer boost in CLS correla-\ntion performance—but among the size subsets, a\nsemblance of the fourth-layer CLS peak is seen\nacross phrase lengths, with length-2 training yield-\ning the strongest peak, and length-6 training the\nsmallest. This suggests an amount of size-based\nspecialization—sentiment training on phrases of\n(or closer to) two words has more positive im-\npact on similarity correlations for our two-word\nphrases.10 However, we also see that phrases of\n10Although subset size can potentially contribute to correla-\ntion performance, we ﬁnd that subset size does not correlate\n2287\nother sizes contribute non-trivially to the ultimate\ncorrelation improvement observed from training on\nthe full dataset. This is consistent with the notion\nthat training on diverse phrase sizes encourages\nﬁne-grained attention to compositionality, while\ntraining on phrases of similar size may have slightly\nmore direct beneﬁt.\nRepresentation changes For further compari-\nson of ﬁne-tuning effects between tasks, we ana-\nlyze changes in BERT representations at each layer\nbefore and after the ﬁne-tuning process. Figure 6\nshows the average layer-wise representation sim-\nilarity between ﬁne-tuned and pre-trained BERT\ngiven identical input. We see substantial differ-\nences between tasks in terms of representation\nchanges: while SST ﬁne-tuning produces signif-\nicant changes across representations and layers,\nPAWS ﬁne-tuning leaves representations largely\nunchanged (further supporting the notion that this\ntask can be solved fairly trivially). We also see that\nafter SST tuning, BERT’s CLS token shows robust\nsimilarity to pre-trained representations until the\nﬁfth layer, followed by a rapid drop in similarity.\nThis suggests that the fourth-layer correlation peak\nmay be enabled in part by retention of key informa-\ntion from pre-training, combined with heightened\nphrase sensitivity from ﬁne-tuning. We leave in-\ndepth exploration of this dynamic for future work.\n8 Discussion\nThe results of our experiments indicate that de-\nspite the promise of PAWS-QQP and SST tasks\nfor improving models’ phrasal composition, ﬁne-\ntuning on these tasks falls far short of resolving\nthe composition weaknesses observed by Yu and\nEttinger (2020). The majority of correspondence\nwith human judgments can still be attributed to\nword overlap effects—disappearing once overlap\nis controlled—and improvements on the controlled\nsettings are absent, very small, or highly localized\nto particular models, layers and representations.\nThis outcome aligns with the increasing body of\nevidence that NLP datasets often do not require of\nmodels the level of linguistic sophistication that we\nmight hope for—and in particular, our identiﬁca-\ntion of a strong spurious cue in the PAWS-QQP\ndataset adds to the growing number of ﬁndings em-\nphasizing that NLP datasets often have artifacts that\nwith the performance patterns we observe here. Phrase count\nof each subset: length 2 - 11,499; length 3 - 11,779; length 4 -\n15,050; length 5 - 11,816; length 6 - 9,935.\ncan inﬂate performance (Poliak et al., 2018; Guru-\nrangan et al., 2018; Kaushik and Lipton, 2018).\nWe do see a ray of promise in the small, lo-\ncalized beneﬁts for certain models from tuning on\nSST. These improvements do not extend to all mod-\nels, and are fairly small in the models that do see\nbeneﬁts—but as we discuss above, it appears that\ntraining on ﬁne-grained syntactic phrase distinc-\ntions may indeed confer some enhancement of com-\npositional meaning in phrase representations—at\nleast when model conditions are amenable. Since\nsentiment information constitutes only a very lim-\nited aspect of phrase meaning, we anticipate that\ntraining on ﬁne-grained phrase labels that reﬂect\nricher and more diverse meaning information could\nbe a promising direction for promoting composi-\ntion more robustly in these models.\n9 Conclusions and future directions\nWe have tested effects of ﬁne-tuning on phrase\nmeaning composition in transformer representa-\ntions. Although we select tasks with promise to\naddress composition weaknesses and reliance on\nword overlap, we ﬁnd that representations in the\nﬁne-tuned models show little improvement on con-\ntrolled composition tests, or show only very local-\nized improvements. Follow-up analyses suggest\nthat the PAWS-QQP dataset contains spurious cues\nthat undermine learning of sophisticated meaning\nproperties when training on that task. However,\nresults from SST tuning suggest that training on\nlabeled phrases of various sizes could prove effec-\ntive for learning composition. Future work should\ninvestigate how model properties interact with ﬁne-\ntuning to produce improvements in particular mod-\nels and layers—and should move toward phrase-\nlevel training with meaning-rich annotations, which\nwe predict will be a promising direction for improv-\ning models’ phrase meaning composition.\nAcknowledgments\nWe would like to thank three anonymous reviewers\nfor valuable feedback for improving this paper. We\nalso thank members of the University of Chicago\nCompLing Lab for helpful comments and sugges-\ntions on this work. This material is based upon\nwork supported by the National Science Founda-\ntion under Award No. 1941160.\n2288\nReferences\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer\nLavi, and Yoav Goldberg. 2016. Fine-grained anal-\nysis of sentence embeddings using auxiliary predic-\ntion tasks. arXiv preprint arXiv:1608.04207.\nJacob Andreas. 2019. Measuring compositional-\nity in representation learning. arXiv preprint\narXiv:1902.07181.\nShima Asaadi, Saif Mohammad, and Svetlana Kir-\nitchenko. 2019. Big bird: A large, ﬁne-grained,\nbigram relatedness dataset for examining semantic\ncomposition. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 505–516.\nJoris Baan, Jana Leible, Mitja Nikolaus, David Rau,\nDennis Ulmer, Tim Baumg¨artner, Dieuwke Hupkes,\nand Elia Bruni. 2019. On the realization of compo-\nsitionality in neural networks. In Proceedings of the\n2019 ACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP , pages 127–\n137.\nGeoff Bacon and Terry Regier. 2019. Does bert\nagree? evaluating knowledge of structure depen-\ndence through agreement relations. arXiv preprint\narXiv:1908.09892.\nHanoz Bhathena, Angelica Willis, and Nathan Dass.\n2020. Evaluating compositionality of sentence rep-\nresentation models. ACL 2020, page 185.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D Manning. 2019. What does bert look\nat? an analysis of bert’s attention. In Proceedings of\nthe 2019 ACL Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n276–286.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nAlexis Conneau and Douwe Kiela. 2018. Senteval: An\nevaluation toolkit for universal sentence representa-\ntions. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018).\nAlexis Conneau, Germ´an Kruszewski, Guillaume Lam-\nple, Lo¨ıc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $ &!#* vector: Probing\nsentence embeddings for linguistic properties. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2126–2136.\nFahim Dalvi, Hassan Sajjad, Nadir Durrani, and\nYonatan Belinkov. 2020. Analyzing redundancy in\npretrained transformer models. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 4908–\n4926.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nNadir Durrani, Hassan Sajjad, Fahim Dalvi, and\nYonatan Belinkov. 2020. Analyzing individual neu-\nrons in pre-trained language models. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4865–4880.\nAllyson Ettinger. 2020. What bert is not: Lessons from\na new suite of psycholinguistic diagnostics for lan-\nguage models. Transactions of the Association for\nComputational Linguistics, 8:34–48.\nAllyson Ettinger, Ahmed Elgohary, Colin Phillips, and\nPhilip Resnik. 2018. Assessing composition in sen-\ntence vector representations. In Proceedings of\nthe 27th International Conference on Computational\nLinguistics, pages 1790–1801.\nAllyson Ettinger, Ahmed Elgohary, and Philip Resnik.\n2016. Probing for semantic evidence of composition\nby means of simple classiﬁcation tasks. In Proceed-\nings of the 1st Workshop on Evaluating Vector-Space\nRepresentations for NLP, pages 134–139.\nLev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,\nEhud Rivlin, Zach Solan, Gadi Wolfman, and Ey-\ntan Ruppin. 2001. Placing search in context: The\nconcept revisited. In Proceedings of the 10th inter-\nnational conference on World Wide Web, pages 406–\n414.\nJuri Ganitkevitch, Benjamin Van Durme, and Chris\nCallison-Burch. 2013. Ppdb: The paraphrase\ndatabase. In Proceedings of the 2013 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 758–764.\nDaniela Gerz, Ivan Vuli´c, Felix Hill, Roi Reichart, and\nAnna Korhonen. 2016. Simverb-3500: A large-\nscale evaluation set of verb similarity. In Proceed-\nings of the 2016 Conference on Empirical Methods\nin Natural Language Processing, pages 2173–2182.\nSuchin Gururangan, Swabha Swayamdipta, Omer\nLevy, Roy Schwartz, Samuel Bowman, and Noah A\nSmith. 2018. Annotation artifacts in natural lan-\nguage inference data. In Proceedings of the 2018\nConference of the North American Chapter of the\n2289\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers) ,\npages 107–112.\nJohn Hewitt and Christopher D Manning. 2019. A\nstructural probe for ﬁnding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4129–4138.\nFelix Hill, Roi Reichart, and Anna Korhonen. 2015.\nSimlex-999: Evaluating semantic models with (gen-\nuine) similarity estimation. Computational Linguis-\ntics, 41(4):665–695.\nDieuwke Hupkes, Anand Singh, Kris Korrel, German\nKruszewski, and Elia Bruni. 2018. Learning compo-\nsitionally through attentive guidance. arXiv preprint\narXiv:1805.09657.\nRobin Jia and Percy Liang. 2017. Adversarial exam-\nples for evaluating reading comprehension systems.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2021–2031.\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter\nSzolovits. 2020. Is bert really robust? a strong base-\nline for natural language attack on text classiﬁcation\nand entailment. In Proceedings of the AAAI con-\nference on artiﬁcial intelligence , volume 34, pages\n8018–8025.\nDivyansh Kaushik and Zachary C Lipton. 2018.\nHow much reading does reading comprehension re-\nquire? a critical investigation of popular bench-\nmarks. arXiv preprint arXiv:1808.04926.\nDaniel Keysers, Nathanael Sch ¨arli, Nathan Scales,\nHylke Buisman, Daniel Furrer, Sergii Kashubin,\nNikola Momchev, Danila Sinopalnikov, Lukasz\nStaﬁniak, Tibor Tihon, et al. 2019. Measuring com-\npositional generalization: A comprehensive method\non realistic data. arXiv preprint arXiv:1912.09713.\nNajoung Kim and Tal Linzen. 2020. Cogs: A composi-\ntional generalization challenge based on semantic in-\nterpretation. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 9087–9105.\nNajoung Kim, Roma Patelϕ, Adam Poliak, Alex Wang,\nPatrick Xia, R Thomas McCoy, Ian Tenney, Alexis\nRoss, Tal Linzen, Benjamin Van Durme, et al. 2019.\nProbing what different nlp tasks teach machines\nabout function word comprehension. NAACL HLT\n2019, page 235.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nJosef Klafka and Allyson Ettinger. 2020. Spying on\nyour neighbors: Fine-grained probing of contex-\ntual embeddings for information about surrounding\nwords. arXiv preprint arXiv:2005.01810.\nJinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting\nWang. 2018. Textbugger: Generating adversarial\ntext against real-world applications. arXiv preprint\narXiv:1812.05271.\nLinyang Li, Ruotian Ma, Qipeng Guo, Xiangyang\nXue, and Xipeng Qiu. 2020. Bert-attack: Adver-\nsarial attack against bert using bert. arXiv preprint\narXiv:2004.09984.\nAdam Liˇska, Germ ´an Kruszewski, and Marco Baroni.\n2018. Memorize or generalize? searching for a\ncompositional rnn in a haystack. arXiv preprint\narXiv:1802.06467.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 3428–3448.\nAlessio Miaschi, Dominique Brunato, Felice\nDell’Orletta, and Giulia Venturi. 2020. Lin-\nguistic proﬁling of a neural language model. arXiv\npreprint arXiv:2010.01869.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In Ad-\nvances in Neural Information Processing Systems ,\npages 14014–14024.\nMarius Mosbach, Anna Khokhlova, Michael A Hed-\nderich, and Dietrich Klakow. 2020. On the inter-\nplay between ﬁne-tuning and sentence-level probing\nfor linguistic knowledge in pre-trained transformers.\narXiv preprint arXiv:2010.02616.\nJesse Mu and Jacob Andreas. 2020. Composi-\ntional explanations of neurons. arXiv preprint\narXiv:2006.14032.\nNavnita Nandakumar, Timothy Baldwin, and Bahar\nSalehi. 2019. How well do embedding models cap-\nture non-compositionality? a view from multiword\nexpressions. In Proceedings of the 3rd Workshop on\nEvaluating Vector Space Representations for NLP ,\npages 27–34.\nTimothy Niven and Hung-Yu Kao. 2019. Probing neu-\nral network comprehension of natural language argu-\nments. arXiv preprint arXiv:1907.07355.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\n2290\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. In Ad-\nvances in Neural Information Processing Systems ,\npages 8024–8035.\nEllie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch,\nBenjamin Van Durme, and Chris Callison-Burch.\n2015. Ppdb 2.0: Better paraphrase ranking, ﬁne-\ngrained entailment relations, word embeddings, and\nstyle classiﬁcation. In Proceedings of the 53rd An-\nnual Meeting of the Association for Computational\nLinguistics and the 7th International Joint Confer-\nence on Natural Language Processing (Volume 2:\nShort Papers), pages 425–430.\nLaura Perez-Mayos, Roberto Carlini, Miguel Balles-\nteros, and Leo Wanner. 2021. On the evolution of\nsyntactic information encoded by bert’s contextual-\nized representations.\nMatthew Peters, Mark Neumann, Luke Zettlemoyer,\nand Wen-tau Yih. 2018. Dissecting contextual word\nembeddings: Architecture and representation. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing , pages\n1499–1509.\nAdam Poliak, Jason Naradowsky, Aparajita Haldar,\nRachel Rudinger, and Benjamin Van Durme. 2018.\nHypothesis only baselines in natural language infer-\nence. NAACL HLT 2018, page 180.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the pa-\nrameters of a language model? arXiv preprint\narXiv:2002.08910.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing ,\npages 1631–1642.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel Bowman, Dipanjan\nDas, et al. 2019. What do you learn from con-\ntext? probing for sentence structure in contextual-\nized word representations. In 7th International Con-\nference on Learning Representations, ICLR 2019.\nShubham Toshniwal, Haoyue Shi, Bowen Shi, Lingyu\nGao, Karen Livescu, and Kevin Gimpel. 2020.\nA cross-task analysis of text span representations.\narXiv preprint arXiv:2006.03866.\nJesse Vig and Yonatan Belinkov. 2019. Analyzing\nthe structure of attention in a transformer language\nmodel. In Proceedings of the 2019 ACL Workshop\nBlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, pages 63–76.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5797–5808.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner,\nand Sameer Singh. 2019. Universal adversarial trig-\ngers for attacking and analyzing nlp. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2153–2162.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks for NLP ,\npages 353–355.\nChengyu Wang, Minghui Qiu, Jun Huang, and Xi-\naofeng He. 2020. Meta ﬁne-tuning neural lan-\nguage models for multi-domain text mining. arXiv\npreprint arXiv:2003.13003.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason\nBaldridge. 2019a. PAWS-X: A Cross-lingual Adver-\nsarial Dataset for Paraphrase Identiﬁcation. In Proc.\nof EMNLP.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le.\n2019b. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. In Advances in\n2291\nneural information processing systems, pages 5754–\n5764.\nLang Yu and Allyson Ettinger. 2020. Assessing phrasal\nrepresentation and composition in transformers. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4896–4907.\nYuan Zhang, Jason Baldridge, and Luheng He. 2019a.\nPaws: Paraphrase adversaries from word scrambling.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers) , pages 1298–\n1308.\nYuan Zhang, Jason Baldridge, and Luheng He. 2019b.\nPAWS: Paraphrase Adversaries from Word Scram-\nbling. In Proc. of NAACL.\nA Appendix\n2292\n0 1 2 3 4 5 6 7 8 91011120.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nBERT\n0 1 2 3 4 5 6 7 8 91011120.0\n0.2\n0.4\n0.6\n0.8\n1.0 RoBERTa\n0 1 2 3 4 5 60.0\n0.2\n0.4\n0.6\n0.8\n1.0 DistilBERT\n0 1 2 3 4 5 6 7 8 91011120.0\n0.2\n0.4\n0.6\n0.8\n1.0 XLM-RoBERTa\n0 1 2 3 4 5 6 7 8 91011120.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCLS\nXLNet\n0 1 2 3 4 5 6 7 8 91011120.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\n0 1 2 3 4 5 6 7 8 91011120.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 1 2 3 4 5 60.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 1 2 3 4 5 6 7 8 91011120.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 1 2 3 4 5 6 7 8 91011120.0\n0.2\n0.4\n0.6\n0.8\n1.0\nHT\n0 1 2 3 4 5 6 7 8 91011120.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\n0 1 2 3 4 5 6 7 8 91011120.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 1 2 3 4 5 60.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 1 2 3 4 5 6 7 8 91011120.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 1 2 3 4 5 6 7 8 91011120.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSEP\n0 1 2 3 4 5 6 7 8 91011120.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\n0 1 2 3 4 5 6 7 8 91011120.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 1 2 3 4 5 60.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 1 2 3 4 5 6 7 8 91011120.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 1 2 3 4 5 6 7 8 91011120.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAP\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 1 2 3 4 5 6\nLayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAA\nPretrained PAWS-tuned SST-tuned\nFigure 7: Paraphrase classiﬁcation accuracy on uncontrolled PPDB dataset, with phrase-only input. Columns\ncorrespond to models, and rows correspond to representation types (“HT” = Head-Token, “AP” = Avg-Phrase and\n“AA” = Avg-All). For each model and representation type, the corresponding subplot shows accuracies for pre-\ntrained, PAWS-tuned and SST-tuned settings, respectively. For each subplot, X-axis corresponds to layer index,\nand Y-axis corresponds to accuracy value. Layer 0 corresponds to input embeddings passed to the model.\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\nCorrelation\nBERT\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\nRoBERTa\n0 1 2 3 4 5 6−0.1\n0.1\n0.3\n0.5\n0.7\nDistilBERT\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\nXLM-RoBERTa\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\nCLS\nXLNet\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\nCorrelation\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\nHT\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\nCorrelation\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\nSEP\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\nCorrelation\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6 7 8 9101112−0.1\n0.1\n0.3\n0.5\n0.7\nAP\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n−0.1\n0.1\n0.3\n0.5\n0.7\nCorrelation\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6\nLayer\n−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n−0.1\n0.1\n0.3\n0.5\n0.7\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n−0.1\n0.1\n0.3\n0.5\n0.7\nAA\nPretrained PAWS -tuned SST-tuned\nFigure 8: Similarity correlation on full BiRD dataset with phrases embedded in context sentence (context-available\ninput).\n2293\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\nCorrelation\nBERT\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4 RoBERTa\n0 1 2 3 4 5 6−0.4\n−0.2\n0.0\n0.2\n0.4 DistilBERT\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4 XLM-RoBERTa\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\nCLS\nXLNet\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\nCorrelation\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\nHT\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\nCorrelation\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\nSEP\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\nCorrelation\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6 7 8 9101112−0.4\n−0.2\n0.0\n0.2\n0.4\nAP\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n−0.4\n−0.2\n0.0\n0.2\n0.4\nCorrelation\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6\nLayer\n−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n−0.4\n−0.2\n0.0\n0.2\n0.4\n0 1 2 3 4 5 6 7 8 9101112\nLayer\n−0.4\n−0.2\n0.0\n0.2\n0.4\nAA\nPretrained PAWS -tuned SST-tuned\nFigure 9: Similarity correlation on controlled BiRD dataset (AB-BA setting) with phrases embedded in context\nsentence (context-available input)."
}