{
  "title": "An Exploratory Approach to Find a Novel Metric Based Optimum Language Model for Automatic Bangla Word Prediction",
  "url": "https://openalex.org/W2794139335",
  "year": 2018,
  "authors": [
    {
      "id": null,
      "name": "Department of CSE, Daffodil International University, Dhaka, Bangladesh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2497467123",
      "name": "Md. Tarek Habib",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1870866583",
      "name": "Abdullah Al Mamun",
      "affiliations": [
        "Daffodil International University"
      ]
    },
    {
      "id": "https://openalex.org/A2204697389",
      "name": "Md Sadekur Rahman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2603551736",
      "name": "Shah Md. Tanvir Siddiquee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2183867041",
      "name": "Farruk Ahmed",
      "affiliations": [
        "Independent University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2604361511",
    "https://openalex.org/W2596764381",
    "https://openalex.org/W2563681206",
    "https://openalex.org/W4293869204",
    "https://openalex.org/W2123842387",
    "https://openalex.org/W1599268073",
    "https://openalex.org/W2127357421",
    "https://openalex.org/W2563525636",
    "https://openalex.org/W6676214546",
    "https://openalex.org/W1508812494",
    "https://openalex.org/W2171139677",
    "https://openalex.org/W2646417138",
    "https://openalex.org/W2162039701",
    "https://openalex.org/W2124059530",
    "https://openalex.org/W6630725953",
    "https://openalex.org/W6678539225",
    "https://openalex.org/W2093245971",
    "https://openalex.org/W2482589566",
    "https://openalex.org/W2108484244",
    "https://openalex.org/W1515760596",
    "https://openalex.org/W2124814993",
    "https://openalex.org/W1565377632"
  ],
  "abstract": "Word completion and word prediction are two important phenomena in typing that have intense effect on aiding disable people and students while using keyboard or other similar devices.Such auto completion technique also helps students significantly during learning process through constructing proper keywords during web searching.A lot of works are conducted for English language, but for Bangla, it is still very inadequate as well as the metrics used for performance computation is not rigorous yet.Bangla is one of the mostly spoken languages (3.05% of world population) and ranked as seventh among all the languages in the world.In this paper, word prediction on Bangla sentence by using stochastic, i.e.N-gram based language models are proposed for auto completing a sentence by predicting a set of words rather than a single word, which was done in previous work.A novel approach is proposed in order to find the optimum language model based on performance metric.In addition, for finding out better performance, a large Bangla corpus of different word types is used.",
  "full_text": "I.J. Intelligent Systems and Applications, 2018, 2, 47-54 \nPublished Online February 2018 in MECS (http://www.mecs-press.org/) \nDOI: 10.5815/ijisa.2018.02.05 \nCopyright © 2018 MECS                                                             I.J. Intelligent Systems and Applications, 2018, 2, 47-54 \nAn Exploratory Approach to Find a Novel Metric \nBased Optimum Language Model for Automatic \nBangla Word Prediction \n \nMd. Tarek Habib \nDepartment of CSE, Daffodil International University, Dhaka, Bangladesh \nE-mail: md.tarekhabib@yahoo.com \n \nAbdullah Al-Mamun, Md. Sadekur Rahman, Shah Md. Tanvir Siddiquee \nDepartment of CSE, Daffodil International University, Dhaka, Bangladesh \nE-mail: mamun.education@gmail.com, sadekur738@gmail.com, tanvir.cse@diu.edu.bd \n \nFarruk Ahmed \nDepartment of CSE, Independent University Bangladesh, Dhaka, Bangladesh \nE-mail: farruk60@gmail.com \n \nReceived: 16 April 2017; Accepted: 15 September 2017; Published: 08 February 2018 \n \n \nAbstract—Word completion and word prediction are two \nimportant phenomena in typing that have intense effect \non aiding d isable people and students while using \nkeyboard or other similar devices. Such auto completion \ntechnique also helps students significantly during \nlearning process through constructing proper keywords \nduring web searching. A lot of works are conducted for \nEnglish language, but for Bangla, it is still very \ninadequate as well as the metrics used for performance \ncomputation is not rigorous yet. Bangla is one of the \nmostly spoken languages (3.05% of world population) \nand ranked as seventh among all the languages  in the \nworld. In this paper, word prediction on Bangla sentence \nby using stochastic, i.e. N-gram based language models \nare proposed for auto completing a sentence by predicting \na set of words rather than a single word, which was done \nin previous work. A n ovel approach is proposed in order \nto find the optimum language model based on \nperformance metric. In addition, for finding out better \nperformance, a large Bangla corpus of different word \ntypes is used. \n \nIndex Terms —Word prediction, performance metric, \nnatural language processing, N-gram, language model, \ncorpus, machine learning, eager learning. \n \nI.  INTRODUCTION \nWriting and typing aids for the disabled are so \nimportant. A person having disability can live a \ncomfortable life if he or she has the opportunity  of \nleaving a note or typing an email easily being aided \nthrough automatic sentence completion technique by the \nprocess of word prediction. In addition, for the early \nlearners in any field (i.e. students, novice researchers) the \nautomatic sentence completi on technique could be \nbeneficial during the learning process by searching new \nthings; as word prediction might help them by providing \nmost suitable suggestions while searching for new topics \nwith keywords. Besides, word prediction is an \n\"intelligent\" word processing feature that can alleviate \nwriting breakdowns simply by reducing the number of \nkeystrokes necessary for typing words, as with the inputs \nof a letter, the software presents a list of possible words \nbeginning with that letter. As each additional l etter is \nadded, the list is refined. When the intended word appears \nin the list, the person selects it, often by clicking on it or \ntyping its number, which inserts the word into the \ndocument. This requires a rigorous performance metric \nrather than traditio nal performance metric, i.e. accuracy \nwhich only considers whether the predicted word matches \nwith the intended word. Auto -completion has been \nextensively implemented in most modern text editors, \nbrowsers and search engines. In predictive auto -\ncompletion s ystems, the candidates are matched against \nthe prefix on -the-fly using information retrieval and \nnatural language processing (NLP) techniques. NLP not \nonly encompasses the problems related to text, such as \ntext documents classification [1], text translatio n [2], etc., \nbut also the problems related to speech, such as [3]. In \nrecent years, machine learning is widely being practiced \nin NLP. In this paper, a novel approach is proposed in \norder to find the optimum language model based on a \nnovel performance metr ic for automatic sentence \ncompletion using supervised machine learning technique \nbased on popular N-gram language model for Bangla \nlanguage. \nThe rest of the paper is organized as follows. Section II \ndescribes the current state of solution to address the \nproblem of automated Bangla word prediction. Section \nIII describes the formal problem formulation of the \n48 An Exploratory Approach to Find a Novel Metric Based Optimum Language  \nModel for Automatic Bangla Word Prediction \nCopyright © 2018 MECS                                                             I.J. Intelligent Systems and Applications, 2018, 2, 47-54 \nautomated Bangla word prediction. In Section IV, comes \nthe description of our approach to solve the problem. \nSection V describes how we apply our entire  \nmethodology and what results are achieved.  In Section \nVI, we investigate results obtained in order to develop an \nunderstanding about the merits of our proposed approach. \nFinally, we summarize our work along with limitations, \nand discuss the scope for future work in Section VII. \n \nII.  RELATED WORKS \nVery few efforts have been made for word prediction \nfor a recent couple of years, specially focused on the \nlanguage Bangla. In the previous work in Bangla [4], \nword prediction on sentence by using stochastic, i. e. N-\ngram based language models such as unigram, bigram, \ntrigram, linear interpolation and backoff models are \nproposed for auto completing a sentence by predicting a \nsingle word in a sentence which is different than the work \npresented in this paper because now the prediction is built \nwith a set of words instead of a single one during finding \nout the best language model. In [5], the authors developed \na sentence completion method in both German and \nEnglish based on N-gram language models and they \nderived a k best Viterbi beam search decoder for strongly \ncompleting a sentence. The use of Artificial Intelligence \nfor word prediction in Spanish is also observed in [6], in \nwhich using the chart bottom -up technique, syntactic and \nsemantic analysis is done for word p rediction. In [7], an \neffective method of word prediction in English is \npresented using machine learning and new feature \nextraction and selection techniques adapted from Mutual \nInformation (MI) and Chi square ( 2). Nagalaviand and \nHanumanthappa [8] have ap plied N-gram based word \nprediction model in order to establish the link between \ndifferent blocks of a piece of writing in e -newspaper in \nEnglish retaining with the sentence reading order. Some \nresearchers use N-gram language model for word \ncompletion in Ur du language [9] and in Hindi language \n[10] for detecting disambiguation in Hindi word. Some \nrelated works in Bangla language, e.g. Bangla grammar \nchecker [11] using N-gram language model, checking the \ncorrectness of Bangla word [12], verification of Bangla  \nsentence structure [13], and validity determination of \nBangla sentences [14] are also conducted. There are \ndifferent word prediction tools such as AutoComplete by \nMicrosoft, AutoFill by Google Chrome, TypingAid, \nLetMeType etc. Some software developed to p rovide \nonly word completion features but do not offer word \nprediction or sentence completion features.  \nAt [15] software with improved training and recall \nalgorithms are suggested to solve the sentence completion \nproblem using the cogent confabulation model, which can \nremember sentences with 100% accuracy  in the training \nfiles. In addition, it helps in filling up missing words in \nsimple sentences or based on some given initial words \ndeliver meaningful sentences. In [16], a N-gram model is \nconstructed which  was used to compute 30 alternative \nwords for a given low frequency word in a sentence, and \nhuman judges then picked the 4 best impostor words, \nbased on a set of provided guidelines. They also used the \nCMU language modeling toolkit to build a 4 -gram \nlanguage model using Good-Turing smoothing. However, \nduring their experiment though the human grooming \nshows good accuracy (91%) but the N-gram models \nresults(Generating model -31%, Smoothed 3 -gram- 36%, \nSmoothed 4 -gram-39%, Simple 4 -gram-34% and LSA \nsimilarity m odel-49%) do not actually meet the \nexpectation. \nFor sentence -completion task an index -based retrieval \nalgorithm and a cluster -based approach are proposed at \n[17]. Bickel et al. [18] learned a linearly interpolated N-\ngram model for sentence completion. For generating and \nranking auto-completion candidates for partial queries in \nthe absence of search logs, Bhatia et al. [19] extracted \nfrequently occurring phrases and N-grams from text \ncollections and deployed them. For the purpose of \nlearning to personalize a uto-completion rankings based \non a new approach is proposed in [20]. While previous \nauto-completion models rely on aggregated statistics \nacross all (or demo-graphic groups of) users, they showed \nthat user-specific and demographic -based features can be \nused together under the same framework. They also \nintroduced a strategy for extracting training labels from \nprevious logs and showed that it can be used for training \nauto-completion rankers. Though they considered their \nlabels to be binary, it would be interes ting to investigate \nhow multi -graded labels. For the future work they left \ntheir model, which can be extended by allowing more \nthan one relevant candidate per ranking if they are closely \nrelated. However, the word prediction process using  N-\ngram language model presented in this paper by shows \nsignificantly more accuracy than the above mentioned \nworks. In addition, during the word prediction for the \nsentence completion process a novel approach is followed \nin which a set of words (collected and sorted based on a \nranking mechanism) is used as list of suggestions for \nsentence completion instead of single word (binary \nprediction). \n \nIII.  PROBLEM DESCRIPTION \nThe problem addressed in this paper is about \nstochastically predicting a suitable word to complete an \nincomplete sentence which consists of some words. Let \nw1w2 w3 … wm-1wm be a sentence (i.e. sequence of words) \nwhere w1w2 w3 … w m-1has already been typed. The \nproblem of the task is to build a language model which \ntakes w1 w2 w3 … wm-1 as input and predicts an  n-tuple of \nwords (vm1,vm2, vm3, …vmn) as output in order to match an \nelement (possibly first) of the n-tuple with wm(intended \nword), as shown in Fig. 1.  \nThe performance of each language model is measured  \nby taking both the matching of predicted word with \nintended word as well as the order of matching into \naccount. Therefore,  accuracy and failure rate  are used in \norder to address this issue.  \nSuppose wm matches with vmi, (i.e. wm equals vmi, where \n1 ≤ i ≤ n + 1), then the equation of accuracy is as follows:\n An Exploratory Approach to Find a Novel Metric Based Optimum Language 49 \nModel for Automatic Bangla Word Prediction \nCopyright © 2018 MECS                                                             I.J. Intelligent Systems and Applications, 2018, 2, 47-54 \n%1001  n\ninAcuracy\n                    (1) \n \nFailure occurs when i equals n + 1. ( n + 1) -th match \nmeans no match has taken place, i.e. accuracy equals 0. If \nin an experiment a language model fails to predict f times, \ni.e. f failures occur, out of p predictions, then the failure \nrate is likewise: \n \n%100\np\nfRateFailure\n                        (2) \n \nAnother aspect of the problem is empirical. Given a \nnumber of language models, we need to co me up with the \none which outperforms all other models in terms of \naccuracy for possibly small value of n. \n \n \nFig.1. Proposed model. \n \nIV.  PROPOSED APPROACH \nWe begin with five language models, namely unigram, \nbigram, trigram, backoff and linear interpolatio n. All \nthese language models are based on N-gram \napproximation. The general equation for this N-gram \napproximation to the conditional probability of the next \nword in a sequence is: \n \n   \n11\n11|| nn\nn n n NP w w P w w\n\n            (3) \n \nEquation 3  shows that the prob ability of a word \nnw\ngiven all the previous words can be approximated by \nthe probability given only the previous N words. \nWe train a language model based on a corpus setting n, \nthe prediction length, with 1. Then accuracy of the trained \nmodel is tested. The value of n is increased by 1 and the \nlanguage model is trained and tested. The process \ncontinues until insignificant change in accuracy occurs \nand the value exceeds the average word length of corpus, \n|\nw |. Here is to mention that as the value of n increases, so \nis for accuracy too. Although larger value of n involves \nbetter accuracy, it increases the value of i, the number of \nposition in n-tuple at which prediction matches. Thus it \nalso involves larger number of  key strokes required. This \nis why the average word length of corpus is used in \nlooping condition. In this way, n*, the considerable \noptimum value of n is automatically calculated for every \nlanguage model stated earlier, which is given as \npseudocode in Algorithm 1.  \nIn Algorithm 1  “considerable optimum value of n” \nmeans that the minimum value of n, for which the \naccuracy is the maximum among the accuracies for all \nvalues of n throughout the range of for loop iteration in \nAlgorithm 1. \nAfter computing n* as w ell as accuracy for all \nlanguage models, the maximum accuracy is calculated. \nThe language model, which shows the maximum \naccuracy, is selected as optimum model. If there are more \nthan one language models, which show the same or \nalmost same accuracy, the la nguage model with the \nminimum failure rate is selected as the optimum model. If \nthere are more than one language models with the \nminimum or near about minimum failure rate, the \nlanguage model, which corresponds with the most \npositively skewed curve for the  accuracy distribution \namong different values of i-th match (1 ≤ i ≤ n*), is \nselected as the optimum language model. The entire \nmethodology is shown in Fig. 2. \n \n \nFig.2. Proposed process offinding best language model (LM). \n50 An Exploratory Approach to Find a Novel Metric Based Optimum Language  \nModel for Automatic Bangla Word Prediction \nCopyright © 2018 MECS                                                             I.J. Intelligent Systems and Applications, 2018, 2, 47-54 \nAlgorithm 1.  Algorithm for cal culating n*, the \nconsiderable optimum value of n for each language \nmodel (LM), where  is a small positive number working \nas the stopping criterion and |\nw | is the average word \nlength of corpus. \n \nSet n* to 0 and the value of minimum accuracy to \naccuracy of LM using n = 1 \nfor i ← 2 to |\nw | \n   Calculate the accuracy of LM using n = i \n   if current accuracy  minimum accuracy\nSet the value of minimum accuracy to \ncurrent accuracy \n   Set to the nonnegative difference of current \nand previous accuracy \n   if ≤\n     Set n* to the value of i, for which \naccuracy is minimum \nExit for loop \n \nV.  IMPLEMENTATION \nA set of training modules of word prediction were \ndeveloped to compute unigram, bigram, trigram, backoff \nas well as linear interpolation based on N-gram. The \nimplementation is different in respect to the previous \nwork [4] as the prediction is built with a set of words \ninstead of a single one during finding out the best \nlanguage model among these language mode ls. These \nmodels are used to determine different probabilities by \ncounting frequencies of words in a very large corpus, \nwhich has been constructed from the popular Bangla \nnewspaper the “Daily Prothom Alo”. The corpus contains \nmore than 11 million (11,203,7 90) words and about 1 \nmillion (937,349) sentences, where total number of \nunique words is 294,371 and average \nword length (|\nw |) is 7. \nDuring this work, the entire corpus is divided into two \nparts, namely training part and testing part. The holdout \nmethod [21] is used to split the corpus at the proportion \nof two -thirds for training and one -third for testing. \nTherefore, this work starts with a training corpus of size \nmore than six (6) hundred thousand sentences. In order to \navoid model ove r-fitting problem (i.e. to have lower \ntraining error but higher generalization error), a \nvalidation set is used. In accordance with this approach, \nthe original training data is divided into two smaller \nsubsets. One of the subsets is used for training, whil e the \nother one (i.e. the validation set) is used for calculating \nthe generalization error. Two -thirds of the training set is \nfixed for model building while the remaining one -third is \nused for error estimation.  \nThe holdout method is repeated for five time s in order \nto find the best model. After finding out the best model, \nthe accuracy of the model is computed using the test set, \nthrough which the considerable optimum prediction \nlength ( n*) is determined automatically based on \nAlgorithm 1. The optimum predi ction length ( n*) along \nwith the accuracy of each model is shown on Table 1. \nFrom the Table 1, it can be seen that the optimum \nprediction length ( n*) is seven for all of the models \nexcept the bigram (n*=5). The accuracy comparison of all \nthe models is pres ented in Fig. 3, where the optimum \nprediction length ( n*) of each model is also marked with \nyellow dot. \nTable 1. Optimum prediction length (n*) of all language models \nLanguage Model \nPrediction Length Optimum \nvalue of n \n(n* ) n = 1 n = 2 n = 3 n = 4 n = 5 n = 6 n = 7 \nUnigram 2.70% 11% 19.50% 22% 28.50% 32.50% 34% 7 \nBigram 35% 39% 45.50% 48.33% 55% 50% 50% 5 \nTrigram 59.50% 66% 67% 68.50% 70% 74.50% 75% 7 \nBackoff 60% 66.10% 67% 68% 71% 74.50% 75.90% 7 \nLinear \nInterpolation 61% 66.50% 67.50% 69% 71.50% 75.60% 77% 7 \n \n \nFig.3. All models' accuracy comparison against the prediction size. \nIn addition, a detailed investigation is conducted \n(shown on Table 2) to evaluate the performance of the \nclassifier for all models  by varying the length of test \nsentences, i.e. unigram, bigram, trigram, backoff and \nlinear interpolation. \nThe comparison of the top three model’s accuracy \nagainst the average availability of the words in the test \nsentences is shown on Fig. 4. \nAfter finding out the different accuracy rate of top  \nthree model’s with the test set consists of sentences with \ndifferent lengths, the average accuracy of the model’s (i.e. \ntrigram, backoff and linear interpolation) is computed \n(see Fig. 5) which might lead us in finding out the best \nlanguage model. During finding out the accuracy of each \nmodel, it is noticed that, sometimes models show almost \n An Exploratory Approach to Find a Novel Metric Based Optimum Language 51 \nModel for Automatic Bangla Word Prediction \nCopyright © 2018 MECS                                                             I.J. Intelligent Systems and Applications, 2018, 2, 47-54 \nsame accuracy during the process of predicting the \nsuitable word. Therefore, keeping track of the failure rate \nis considered as a significant task, as some models mig ht \nshow same accuracy b ut with different failure rate.  In \nTable 3, the failure rate of all the models is presented. \nTable 2. All models’ accuracy across the availability of words \nAvailable \nWords in Test \nSentences \nAccuracy of Language Model \nTrigram Backoff Linear \nInterpolation \n1 39.00% 40.25% 40.5% \n2 46.30% 47.30% 47.4% \n3 48.60% 49.00% 49% \n4 58.00% 58.50% 58.8% \n5 60.90% 59.30% 59.5% \n6 62.50% 62.70% 62% \n7 65.70% 66.60% 66.7% \n8 67.27% 67.50% 67% \n9 69.60% 69.90% 70.2% \n10 71.50% 70.20% 70.5% \n11 72.35% 72.50% 71% \n12 75.50% 75.63% 75.63% \n13 78.00% 78.50% 79% \n14 80.50% 80.70% 80.9% \n15 81.15% 81.40% 81.5% \n \n \nFig.4. All models’ accuracy comparison against the  \navailability of words. \n \nFig.5. Average accuracy of language models. \nTable 3. All models’ failure rate with the availability of the  \nwords in test sentences \nAvailable \nWords in Test \nSentences \nFailure Rate of Language Model \nTrigram Backoff Linear \nInterpolation \n1 38.53% 40.15% 31% \n2 33.23% 31.93% 30.3% \n3 32.93% 30.23% 28% \n4 28.03% 26.73% 25.5% \n5 28.43% 25.53% 25% \n6 26.93% 24.94% 23.9% \n7 24.83% 23.09% 21.5% \n8 23.23% 21.33% 20.5% \n9 22.93% 19.23% 18.43% \n10 20.53% 16.65% 13.8% \n11 16.17% 13.33% 10.67% \n12 12.9% 12.2% 8.6% \n13 9.5% 7.8% 6.15% \n14 5% 4.3% 3.7% \n15 2.3% 2.1% 2% \n \n \nFig.6. All models’ failure rate comparison against the  \navailability of words. \n \nFig.7. Average failure rate of language models. \n \nThe comparison of the top three model’s failure rate \nagainst the average availability of the words in the test \nsentences is shown on Fig. 6. \nAfter finding out the different failure rate of top three \n52 An Exploratory Approach to Find a Novel Metric Based Optimum Language  \nModel for Automatic Bangla Word Prediction \nCopyright © 2018 MECS                                                             I.J. Intelligent Systems and Applications, 2018, 2, 47-54 \nmodel’s with the test set consists of sentences with \ndifferent lengths, the average failure rate (see Fig. 7) of \nthe top model’s (i.e. trigram, backoff and linear \ninterpolation) is computed which might lead us in finding \nout the best language model; as during the process of \nfinding out the accuracy some models have shown almost \nsimilar accuracy which makes  the selection process \ndifficult. \n \nVI.  DISCUSSION \nDuring the initial  experiment, as shown o n Table 1 , it \nis noticeable, the top three models have shown good \naccuracy among all the models (see Fig. 3), though linear \ninterpolation model shows slightly better performance in \nterms of predicting next possible word with optimum \nprediction length seven, i.e. n* = 7. Therefore, to find out \nthe best model,  in the second phase, a further deep \ninvestigation is conducted, as shown on Table 2, to find \nout, how the top three models  behave against the test sets \nwith different sizes (average) of sentences. From the \nexperiment in second phase, all the top models behave \nsimilar like before (see Fig. 4). Consequently, a  third \nphase is required, in which the failure rate of the top \nmodels is computed (see Table  3). Though, in some cases \nthe trigram, backoff and linear interpolation method show \nalmost same accuracy, but the failure rate of the other two \nmodels (trigram and backoff) is higher compared to the \nlinear interpolation  (see Fig. 6). Moreover, from the \naverage accuracy and average failure rate of all models \n(Fig. 5 and Fig. 7 respectively) it is obvious to come up \nwith the final decision that linear interpolation model \naccomplishes most accuracy among all other models \nduring the word prediction process. The accuracy rate \nalong with the increment of the predictio n length of the \nlinear interpolation model is shown on Table 4 and Fig. 8. \nTable 4. Accuracy along with the prediction length of Linear \nInterpolation model. \nPrediction length (n) Accuracy \n1 61% \n2 66.50% \n3 67.50% \n4 69% \n5 71.50% \n6 75.60% \n7 77% \n \nAlthough the linear interpolation model has shown \nbetter performance than other top models (77% with n* = \n7) and the experiment result is promising; it is still \nnecessary to test with larger training corpus to \naccomplish more than 90% accuracy with less predict ion \nlength (n*). \n \nFig.8. Accuracy along with the prediction length of Linear  \nInterpolation model. \n \nVII.  CONCLUSION \nThe focus of this research was modeling, training and \nrecall techniques for automatic sentence completion using \nsupervised machine learnin g technique based on popular \nN-gram language model.  N-gram based word prediction \nworks well for English, but for Bangla language, it is \nfound more challenging to get very good, e.g. more than \n90% accuracy, performance as it depends on training \ncorpus of size more than six hundred thousand sentences.  \nThough during the several phases of experiments, the top \nthree models show almost same level of accuracy, but in \nterms of both accuracy and failure rate, the linear \ninterpolation outperforms the other models. \nFor the future work, a further testing with the present \nmodels is planned with larger corpus. In addition, it is \nperceived to find out  a language model  with a better \nalgorithm that will make use of character level N-gram in \ncombination with word level N-gram, or make use of \nBangla grammatical rules along with word level N-gram. \nExtensive work is in progress to devise such a language \nmodel using a very large Bangla corpus. \nREFERENCES \n[1] I. S. I. Abuhaiba and H. M. Dawoud,  \"Combining \nDifferent Approaches to Improv e Arabic Text Documents \nClassification\", International Journal of Intelligent \nSystems and Applications(IJISA) , Vol.9, No.4, pp.39 -52, \n2017. \n[2] G. Chandra, S. K. Dwivedi, \"Assessing Query Translation \nQuality Using Back Translation in Hindi -English CLIR,\" \nInternational Journal of Intelligent Systems and \nApplications (IJISA), Vol.9, No.3, pp.51-59, 2017. \n[3] K. K . Ravi  and P.V. Subbaiah,  \"A Survey on Speech \nEnhancement Methodologies,\" International Journal of \nIntelligent Systems and Applications (IJISA), Vol.8, No.12, \npp.37-45, 2016. \n \n \n \n \n An Exploratory Approach to Find a Novel Metric Based Optimum Language 53 \nModel for Automatic Bangla Word Prediction \nCopyright © 2018 MECS                                                             I.J. Intelligent Systems and Applications, 2018, 2, 47-54 \n[4] M. M. Haque, M. T Habib and M. M . Rahman, \n“Automated Word Prediction in Bangla Language Using \nStochastic Language Models ,” Academy & Industry \nResearch Collaboration Center (AIRCC) International \nJournal in Foundations of Computer Scie nce & \nTechnology, vol. 5, no. 6, pp. 67–75, November 2015. \n[5] S. Bickel, P. Haider and T.  Scheffer, (2005), \"Predicting \nSentences using N-Gram Language Models,\" In \nProceedings of Conference on Empirical Methods in \nNatural language Processing. \n[6] N. Garay -Vitoria and J.  Gonzalez-Abascal, (2005), \n\"Application of Artificial Intelligence Methods in a Word-\nPrediction Aid,\" Laboratory of Huma n-Computer \nInteraction for Special Needs. \n[7] H. Al-Mubaid, \"A Learning -Classification Based \nApproach for Word Prediction,\" The Inte rnational Arab \nJournal of Information Technology, Vol. 4, No. 3, 2007. \n[8] D. Nagalaviand and M. Hanumanthappa, “ N-gram Word \nprediction language models to identify the sequence of \narticle blocks in English e-newspapers,” In Proceedings of \nInternational Confere nce on Computation System and \nInformation Technology for Sustainable Solutions \n(CSITSS), 2016. \n[9] Q. Abbas, (2014), \"A Stochastic Prediction Interface for \nUrdu\", Intelligent Systems and Applications , Vol.7, No.1, \npp 94-100 .  \n[10] U. P. Singh, V. Goyal and A.  Rani, (2014), \n\"Disambiguating Hindi Words Using N-Gram Smoothing \nModels\", International Journal of Engineering Sciences , \nVol.10, Issue June, pp 26-29. \n[11] J. Alam, N. Uzzaman and M.  khan,  (2006), \" N-gram \nbased Statistical Grammar Checker for Bangla and \nEnglish\", In Proceedings of  International Conference on \nComputer and Information Technology. \n[12] N. H. Khan, G . C. Saha, B . Sarker and M. H . Rahman, \n(2014), \"Checking the Correctness of Bangla Words using \nN-Gram\", International Journal of Computer Application , \nVol. 89, No. 11. \n[13] N. H. Khan, M. F. Khan, M. M. Islam, M. H. Rahman and \nB. Sarker, \"Verification of Bangla Sentence Structure \nusing N-Gram,\" Global Journal of Computer Science and \nTechnology, vol. 14, issue 1, 2014. \n[14] M. R. Rahman, M. T. Habib, M. S. Rahman, S. B. Shuvo \nand M . S. Uddin, “ An Investigative Design Based \nStatistical Approach for Determining Bangla Sentence \nValidity,” International Journal of Computer Science and \nNetwork Security , vol. 16, no. 11, pp. 30 –37, November \n2016. \n[15] Q. Qiu et al., \"Confabulation ba sed sentence completion \nfor machine reading,\" 2011 IEEE Symposium on \nComputational Intelligence, Cognitive Algorithms, Mind, \nand Brain (CCMB), Paris, 2011, pp. 1-8. \n[16] G. Zweig, C. J. C. Burges . (2011). Tech report: “The \nMicrosoft Research Sentence Completion Challenge”. \n[17] K. Grabski and T. Scheffer. Sentence completion. In Proc. \nSIGIR, pages 433–439, Sheffield, United Kingdom, 2004. \n[18] S. Bickel, P. Haider, and T. Scheffer.  Learning to com -\nplete sentences. In Proceedings. ECML, volume 3720 of \nLecture Notes in Com puter Science, pages 497{504. \nSpringer, 2005}. \n[19] S. Bhatia, D. Majumdar, and P. Mitra. Query suggestions \nin the absence of query logs. In Proceedings. SIGIR, pp. \n795-804, Beijing, China, 2011. \n[20] M. Shokouhi. 2013. Learning to personalize query auto -\ncompletion. In Proceedings of the 36 th international ACM \nSIGIR conference on Research and development in \ninformation retrieval (SIGIR '13). ACM, New York, NY, \nUSA.  \n[21] P.-N. Tan, M. Steinbach, and V. Kumar, “ Introduction to \nData Mining,” Addison-Wesley, 2006. \n \n \n \nAuthors’ Profiles \n \nMd. Tarek Habib  is pursuing his Ph.D. \ndegree at the Department of Computer \nScience and Engineering in Jahangirnagar \nUniversity. He obtained his B.Sc. degree \nin Computer Science from BRAC \nUniversity in 2006. Then he got M.S. \ndegree in Computer S cience and \nEngineering (Major in Intelligent Systems Engineering) from \nNorth South University in 2009. He is an Assistant Professor at \nthe Department of Computer Science and Engineering in \nDaffodil International University. His research interest is in \nArtificial Intelligence, especially Artificial Neural Netw orks, \nMachine Learning , Computer Vision and Natural Language \nProcessing. \n \n \nAbdullah Al -Mamun attained his B.Sc. \ndegree in Computer Science and \nEngineering from American International \nUniversity Bangl adesh. He received his \njoint M.Sc. in Computer Science (double \ndegree) jointly from University of Trento, \nItaly and RWTH Aachen University, \nGermany under the Erasmus Mundus \nDouble Degree masters program. At present he is working as a  \nSenior Lecturer at the  Department of Computer Science and \nEngineering in Da ffodil International University . His research \ninterest includes Data M ining, Artificial Intelligence , Web \nSearch Visualization, Interactive Information Retrieval, Human \nComputer Interaction, Visual Analy tics and Natural Language \nProcessing.  \n \n \nMd. Sadekur Rahman  is pursuing his \nPh.D. at University Sains Islam Malaysia \non Ontology and knowledge management. \nHe obtained his B.Sc. and M.Sc. degree in \nApplied Mathematics & Informatics from \nPeoples' Friendship University of Russia. \nNow he is working as a Senior Lecturer at \nthe Department of Computer Science and \nEngineering in Daffodil International University. He has a \nnumber of publications in international and national journals \nand conference proceedings. His research interest includes Data \nMining, Artificial Intelligence, Pattern Recognition, and Natural \nLanguage Processing.  \n \n \nShah Md. Tanvir Siddique e is a Senior \nLecturer in Department of Computer \nEngineering under the faculty of Science \nand Information Tec hnology, Daffodil \nInternational University, Dhaka, \nBangladesh. He holds a B.Sc. in \nComputer Science and Engineering from \nHajee Mohammad Danesh Science and \nTechnology University, Dinajpur, Bangladesh and M.Sc. in \nComputer Science from South Asian University  (SAARC \n\n54 An Exploratory Approach to Find a Novel Metric Based Optimum Language  \nModel for Automatic Bangla Word Prediction \nCopyright © 2018 MECS                                                             I.J. Intelligent Systems and Applications, 2018, 2, 47-54 \nUniversity), New Delhi, India. His research interests in Cloud \nComputing, Cloud Security using open source cloud \nenvironment \n \n \nFarruk Ahmed  is a Professor at the \nDepartment of Computer Science and \nEngineering in Independent University, \nBangladesh. He is an eminent professor \nof Electrical Engineering and Computer \nScience in Bangladesh. He has had much \nmore than 100 publications in \ninternational and national journals and conference proceedings. \nHis areas of research interests in Computer Science are  \nMicroprocessor, Microcomputer Based Systems, Natural \nLanguage Processing, Speech Processing, Image Processing and \nPattern Recognition, Interfacing and Peripherals, Computer \nNetworks, and Information Systems Design. \n \n \n \nHow to cite this paper: Md. Tarek Habib, Abdullah Al-Mamun, \nMd. Sadekur Rahman , Shah Md. Tanvir Siddiquee, Farruk \nAhmed, \" An Exploratory Approach to Find a Novel Metric \nBased Optimum Language Model for Automatic Bangla Word \nPrediction\", International Journal of Intelligent Systems and \nApplications(IJISA), Vol.10, No.2, pp. 47-54, 201 8. DOI: \n10.5815/ijisa.2018.02.05 ",
  "topic": "Bengali",
  "concepts": [
    {
      "name": "Bengali",
      "score": 0.9580928087234497
    },
    {
      "name": "Computer science",
      "score": 0.8912132382392883
    },
    {
      "name": "Word (group theory)",
      "score": 0.7057045698165894
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6804602742195129
    },
    {
      "name": "Sentence",
      "score": 0.6677590012550354
    },
    {
      "name": "Natural language processing",
      "score": 0.6462284922599792
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5660578608512878
    },
    {
      "name": "Metric (unit)",
      "score": 0.5416936874389648
    },
    {
      "name": "Population",
      "score": 0.44603705406188965
    },
    {
      "name": "Speech recognition",
      "score": 0.32888108491897583
    },
    {
      "name": "Linguistics",
      "score": 0.14463981986045837
    },
    {
      "name": "Programming language",
      "score": 0.08794641494750977
    },
    {
      "name": "Demography",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    }
  ]
}