{
  "title": "Impact of Co-occurrence on Factual Knowledge of Large Language Models",
  "url": "https://openalex.org/W4389519983",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2987476463",
      "name": "Cheongwoong Kang",
      "affiliations": [
        "Kootenay Association for Science & Technology",
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2107736885",
      "name": "Jae-Sik Choi",
      "affiliations": [
        "Kootenay Association for Science & Technology",
        "Korea Advanced Institute of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4226142937",
    "https://openalex.org/W3110879614",
    "https://openalex.org/W3173673636",
    "https://openalex.org/W2107598941",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W2051150309",
    "https://openalex.org/W2076332451",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3202712981",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3101204082",
    "https://openalex.org/W2247119764",
    "https://openalex.org/W4285107714",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W2946930197",
    "https://openalex.org/W4308410295",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W4318351203",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W4309217888",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4385573668",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W1593045043",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3035261884",
    "https://openalex.org/W3200809495",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4226137521",
    "https://openalex.org/W4288804650",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W3034383590",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W2952604841",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4385573636",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W3100283070",
    "https://openalex.org/W4285247752",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W3048045781",
    "https://openalex.org/W3115587447",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3156170450",
    "https://openalex.org/W3177765786",
    "https://openalex.org/W3199709353",
    "https://openalex.org/W3213407400",
    "https://openalex.org/W4226455589",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W2962736243",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W3034995113"
  ],
  "abstract": "Large language models (LLMs) often make factually incorrect responses despite their success in various applications. In this paper, we hypothesize that relying heavily on simple co-occurrence statistics of the pre-training corpora is one of the main factors that cause factual errors. Our results reveal that LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently co-occurred words over the correct answer. Consequently, LLMs struggle to recall facts whose subject and object rarely co-occur in the pre-training dataset although they are seen during finetuning. We show that co-occurrence bias remains despite scaling up model sizes or finetuning. Therefore, we suggest finetuning on a debiased dataset to mitigate the bias by filtering out biased samples whose subject-object co-occurrence count is high. Although debiased finetuning allows LLMs to memorize rare facts in the training set, it is not effective in recalling rare facts unseen during finetuning. Further research in mitigation will help build reliable language models by preventing potential errors. The code is available at https://github.com/CheongWoong/impact_of_cooccurrence.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7721–7735\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nImpact of Co-occurrence on Factual Knowledge of Large Language Models\nCheongwoong Kang\nKAIST\ncw.kang@kaist.ac.kr\nJaesik Choi\nKAIST\njaesik.choi@kaist.ac.kr\nAbstract\nLarge language models (LLMs) often make\nfactually incorrect responses despite their suc-\ncess in various applications. In this paper, we\nhypothesize that relying heavily on simple co-\noccurrence statistics of the pre-training corpora\nis one of the main factors that cause factual\nerrors. Our results reveal that LLMs are vul-\nnerable to the co-occurrence bias, defined as\npreferring frequently co-occurred words over\nthe correct answer. Consequently, LLMs strug-\ngle to recall facts whose subject and object\nrarely co-occur in the pre-training dataset al-\nthough they are seen during finetuning. We\nshow that co-occurrence bias remains despite\nscaling up model sizes or finetuning. Therefore,\nwe suggest finetuning on a debiased dataset to\nmitigate the bias by filtering out biased sam-\nples whose subject-object co-occurrence count\nis high. Although debiased finetuning allows\nLLMs to memorize rare facts in the training set,\nit is not effective in recalling rare facts unseen\nduring finetuning. Further research in mitiga-\ntion will help build reliable language models by\npreventing potential errors. The code is avail-\nable at https://github.com/CheongWoong/\nimpact_of_cooccurrence.\n1 Introduction\nNatural language processing has seen significant\nprogress in recent years with the advent of large lan-\nguage models (LLMs) (Devlin et al., 2019; Brown\net al., 2020; Raffel et al., 2020). Factual knowl-\nedge probing benchmarks like LAMA have demon-\nstrated that LLMs have a high capacity to recall\nfactual knowledge (Petroni et al., 2019; Jiang et al.,\n2020; Roberts et al., 2020; Shin et al., 2020; Zhong\net al., 2021). However, factual knowledge stored\nin LLMs may not always be correct (Elazar et al.,\n2021; Cao et al., 2021). Understanding the reasons\nbehind such inaccuracies is critical for developing\nmore accurate and reliable language models. Re-\ncent studies point out that LLMs often learn short-\nThe capital of Canada is [MASK]\nLanguage Model\nPre-training data\nToronto\nOttawa\nSubject Object Count\nCanada Toronto 246\nCanada Ottawa 19\n… … …\nCanada London 8\nFigure 1: This figure shows an overall framework of our\ncorrelation analysis between co-occurrence counts and\nfactual knowledge of LLMs. We assume that if the target\nmodel heavily relies on subject-object co-occurrence, it\nis more likely to recall the most co-occurring word with-\nout accurate semantic understanding. For instance, in\nthis hypothetical example, the model fails to answer the\nquestion about the capital of Canada by generating the\nmost frequently co-occurring word ‘Toronto’, while the\ncorrect answer is ‘Ottawa’. This indicates that relying\nheavily on co-occurrence statistics may have potential\nerrors.\ncuts relying on spurious features rather than under-\nstanding language (Wallace et al., 2019; McCoy\net al., 2019; Poerner et al., 2020; Ettinger, 2020;\nKassner and Schütze, 2020; Cao et al., 2021; Elazar\net al., 2021; Bender et al., 2021). We suspect that re-\nlying on co-occurrence statistics of the pre-training\ncorpora is one of the main factors that cause such\nbehaviors (Razeghi et al., 2022; Li et al., 2022;\nElazar et al., 2022; Kandpal et al., 2023; Kazemi\net al., 2023).\nIn this work, we investigate the effects of co-\noccurrence statistics of the pre-training data on\nfactual knowledge in LLMs. First, we adopt the\nLAMA dataset (Petroni et al., 2019) to probe fac-\ntual knowledge, represented as a subject-relation-\nobject triple. Then, we analyze the correlation be-\ntween co-occurrence statistics and performance on\n7721\nfactual knowledge probing. Specifically, we count\nco-occurrences of word pairs in the pre-training\ncorpora. We focus on subject-object co-occurrence,\nmotivated by the concept of distant supervision,\nwhich shows that a sentence often contains the\ntriple if it contains a subject and an object of a triple\n(Mintz et al., 2009). Figure 1 illustrates an overall\nframework of our correlation analysis between co-\noccurrence counts and factual knowledge of LLMs.\nWe hypothesize that the target model would gener-\nate the most frequently co-occurring word if it heav-\nily relies on co-occurrence. In this simulated ex-\nample, given the fact ‘Canada’-‘capital’-‘Ottawa’,\nthe target model generates the most frequently co-\noccurring word ‘Toronto’, which is not the correct\nanswer.\nWe test our hypothesis with GPT-Neo (Black\net al., 2021) and GPT-J (Wang and Komatsuzaki,\n2021), which are open-source versions of GPT-3\n(Brown et al., 2020). We compute co-occurrence\nstatistics of the Pile dataset (Gao et al., 2020), on\nwhich the target models are pre-trained. We show\nthat the factual probing accuracy of LLMs highly\ncorrelates with subject-object co-occurrence, lead-\ning to failures in recalling rare facts. Although\nscaling up model sizes or finetuning boosts the\noverall performance on factual knowledge prob-\ning, they do not resolve co-occurrence bias, in\nwhich frequently co-occurred words are preferred\nover the correct answer. Besides, we find that a\nsignificant portion of facts in the LAMA dataset\ncan be recalled by simply generating the object\nwith the highest co-occurrence count. Although co-\noccurrence is necessary to recall factual knowledge,\nit is not sufficient. Therefore, relying heavily on\nco-occurrence is inappropriate for understanding\nthe accurate meaning behind words.\nRelying heavily on the co-occurrence statistics\nmay lead to hallucinations (Fish, 2009; Maynez\net al., 2020; Ji et al., 2023) if the co-occurrence\nstatistics reflect factually incorrect information.\nTherefore, we suggest finetuning LLMs on the\ndebiased LAMA, constructed by filtering out bi-\nased samples whose subject-object co-occurrence\ncount is high. Although finetuning on the debiased\ndataset allows LLMs to learn rare facts that appear\nin the training set, it is not generalizable to test\ncases.\nIn summary, we show that factual knowledge\nprobing accuracy correlates with subject-object\nco-occurrence. In addition, we present novel ev-\nidence and insights by providing a more detailed\npicture. Specifically, we demonstrate that LLMs\nprefer frequently co-occurring words, which often\noverride the correct answer, especially when the\ncorrect answer rarely co-occurs with the subject.\nWhile existing studies only show that the perfor-\nmance of LLMs correlates with co-occurrence, our\nresults provide evidence and reasons for that. We\nhope our results spur future work on mitigating\nco-occurrence bias to build accurate and reliable\nlanguage models.\n2 Related Work\n2.1 Prompt Tuning and Finetuning\nThere have been recent attempts to tune input\nprompts to improve the performance of LLMs\nfurther (Liu et al., 2023b; Lester et al., 2021;\nLi and Liang, 2021; Qin and Eisner, 2021; Liu\net al., 2022, 2023a). However, directly optimiz-\ning prompts is not trivial since changes in the in-\nput space may cause non-monotonic performance\nchanges (Hu et al., 2022). Especially, Fichtel et al.\n(2021) demonstrate that finetuned LMs outperform\nprompt-tuned LMs on factual knowledge probing\ntasks. Although LLMs, such as GPT-3 and T0,\nwere primitively designed to perform well on var-\nious tasks without finetuning (Brown et al., 2020;\nSanh et al., 2022), recent work shows that finetun-\ning improves the linguistic capabilities of LLMs\nsubstantially (Ouyang et al., 2022; Wei et al., 2022).\nTherefore, we consider finetuned LMs for analysis.\n2.2 Term Frequency and Model Behaviors\nThere have been several approaches to understand-\ning the effects of training data on model behaviors.\nSpecifically, recent studies observe a correlation\nbetween pre-training term frequency and model be-\nhaviors (Kassner et al., 2020; Wei et al., 2021; Li\net al., 2022; Razeghi et al., 2022; Kandpal et al.,\n2023; Elazar et al., 2022). Our work offers unique\ncontributions by providing additional evidence and\ninsights with in-depth analysis. Specifically, we\nverify that (1) LLMs learn co-occurrence bias\nfrom the pre-training data, preferring frequently\nco-occurred words over the correct answer, which\nis especially problematic when recalling rare facts,\nand (2) co-occurrence bias is not overcome either\nby scaling up model sizes or finetuning.\n7722\n2.3 Spurious Features\nA spurious correlation refers to a relationship in\nwhich variables are correlated but does not imply\ncausation due to a coincidence or a confounding\nfactor (Simon, 1954). LMs often learn shortcuts\nrelying on spurious features, such as word overlap,\ntype matching, misprimes, and surface form, which\nmostly come from dataset bias (Gururangan et al.,\n2018; McCoy et al., 2019; Wallace et al., 2019;\nKassner and Schütze, 2020; Poerner et al., 2020;\nWang et al., 2022). For example, if a heuristic\n(e.g. word overlap, surface form) frequently co-\noccurs with specific labels, the models may learn\nthe shortcut relying on the heuristic to make deci-\nsions. Although spurious features may be helpful\nin generating plausible responses, it is not appro-\npriate for accurate semantic understanding. Our\nwork suggests that co-occurrence statistics of the\npre-training data may work as spurious features,\ncausing hallucinations (Fish, 2009; Maynez et al.,\n2020; Ji et al., 2023) or biased responses (Boluk-\nbasi et al., 2016; Caliskan et al., 2017; Bommasani\net al., 2021).\n2.4 Memorization\nLLMs have been shown to memorize information\nin training data and generate it verbatim at test\ntime (Emami et al., 2020; Feldman and Zhang,\n2020; McCoy et al., 2023; Zhang et al., 2021;\nLee et al., 2022; Akyurek et al., 2022; Magar and\nSchwartz, 2022; Carlini et al., 2023). Memoriza-\ntion implies that LLMs recall memorized informa-\ntion rather than generalizing to new inputs based\non learned knowledge. Although recent studies in-\ndicate that memorization poses privacy risks (Song\nand Shmatikov, 2019; Carlini et al., 2019, 2021;\nKandpal et al., 2022), it is necessary for near-\noptimal generalization when learning from a long-\ntail distribution (Feldman, 2020). Our work also\nsuggests that memorization is essential for accu-\nrately recalling facts, since factual knowledge may\nnot be inferred based on prior knowledge of other\nfacts. However, we demonstrate that LLMs often\nstruggle to memorize facts, as the correct answer is\noverridden by co-occurrence statistics.\n3 Factual Knowledge Probing\nThis section describes the overall framework to test\nfactual knowledge of LLMs. With this framework,\nwe aim to investigate the effects of model sizes,\nfinetuning, and co-occurrence statistics.\n3.1 The LAMA Probe\nWe adopt the LAMA-TREx dataset (Elsahar et al.,\n2018; Petroni et al., 2019), which consists of 41 re-\nlations, to probe factual knowledge of LLMs. Facts\nare represented as subject-relation-object triples.\nEach fact is converted to a natural language form\nby utilizing a pre-defined set of templates for rela-\ntions, provided in the original LAMA dataset. For\nexample, a fact ‘Canada’-‘capital’-‘Ottawa’ is con-\nverted to the sentence “The capital of Canada is\nOttawa”. Then, each fact is converted to a Cloze\nstatement by masking an object (e.g. “The cap-\nital of Canada is [MASK]”), to query the target\nLM for the masked word. To query unidirectional\nLMs, we use a sentence truncated right before the\nmask token (e.g. “The capital of Canada is”) in\nthe zero-shot setting while utilizing a full sentence\nin the finetuned setting. The details of finetuning\nare included in Appendix A. We assume that the\ntarget model knows a fact if it correctly predicts\nthe masked word.\nWe preprocess the original LAMA-TREx dataset\nfor our experiments. First, we filter out samples\nwhose answer is not in the intersection of the vo-\ncabularies of target models. Since the dataset was\noriginally designed for zero-shot knowledge prob-\ning, we split the dataset into training and test sets\nwith a ratio of 70:30 to study the effects of finetun-\ning. The data descriptions and statistics, including\ntemplates and the number of samples for each rela-\ntion, are shown in Table 4 in Appendix B.\n3.2 Metrics\nFollowing the knowledge base completion litera-\nture (Bordes et al., 2011, 2013), we consider two\nrank-based metrics, hits@1 and MRR (mean recip-\nrocal rank), to evaluate the performance on factual\nknowledge probing. The models that rank ground-\ntruth objects higher are considered more knowled-\nable. Hits@1 is 1 if the correct answer is ranked\nin the top 1 prediction, otherwise 0. MRR is the\naverage reciprocal rank of the correct answer in the\nprediction. When computing hits@1, we remove\nother valid objects for a subject-relation pair other\nthan the one we test, following the original setup\nof LAMA.\n3.3 Restricted Candidate Sets\nSince LLMs are not trained to act as knowledge\nbases, we use restricted output candidate sets\nfollowing the recent work (Xiong et al., 2020;\n7723\nRavichander et al., 2020; Kassner et al., 2021;\nElazar et al., 2021). Specifically, we use three\ndifferent settings to restrict output vocabularies to\nstudy whether LLMs are capable of recognizing\nappropriate object candidates or not: (1) remove\nstopwords, (2) gold objects and (3) gold objects\n(relation-wise). The remove stopwords removes\nstopwords in the stopword list of NLTK (Bird et al.,\n2009) from the output candidates. The gold objects\nrestricts the output vocabulary to the set of gold\nobjects in the whole dataset. Similarly, the gold ob-\njects (relation-wise) restricts the output candidates\nto the set of gold objects for each relation in the\ndataset.\n4 Factual Knowledge Probing with\nCo-occurrence Statistics\nThis section describes our framework to analyze\nthe impact of pre-training co-occurrence statis-\ntics on factual knowledge of LLMs. We first test\nhow much factual knowledge can be recalled with\nterm frequency statistics, including co-occurrence.\nThen, we analyze the correlation between co-\noccurrence statistics and factual predictions.\n4.1 Co-occurrence Statistics\nWe consider the co-occurrence statistics of the pre-\ntraining dataset of target models. Since it is in-\ntractable to count co-occurrences of every n-gram\npair, we only count co-occurrences between pairs\nin a minimal sufficient set. This set is initialized\nas a set of subject entities in the LAMA-TREx\ndataset and words in the target model’s vocabulary,\nwhich are object candidates. For text normaliza-\ntion, we tokenize words based on Penn Treebank\n(Marcus et al., 1993) and remove stopwords in the\nresulting tokens. Then, we filter out entities those\nare composed of more than three tokens. Due to\ncomputational burdens from the large amount of\ndocuments, we count whether an entity pair ap-\npears in the same document or not, instead of using\na sliding window approach.\n4.2 Term Frequency Baselines\nWe test how much factual knowledge can be re-\ncalled with simple term frequency statistics by\nmeasuring the performance of three different term\nfrequency baselines: (1) marginal probability, (2)\njoint probability and (3) PMI. For a subject and re-\nlation pair, the marginal probability baseline ranks\nobject candidates based on how frequently they ap-\npear in the pre-training dataset. The joint probabil-\nity ranks object candidates based on how frequently\nthey appear with the subject in the pre-training\ndataset. Following the definition of PMI (pointwise\nmutual information) (Church and Hanks, 1990),\nthe PMI baseline normalizes Ppretrain(obj|subj),\nthe conditional probability of objects given a sub-\nject, by Ppretrain(obj), the marginal probability\nof objects. We measure hits@1 and MRR of the\nbaselines and compare them with the target LLMs.\n4.3 Correlation Metrics\nTo analyze the correlation between factual knowl-\nedge of LLMs and co-occurrence statistics, we plot\nhits@1 of the target LLMs against subject-object\nco-occurrence counts. Here, we consider two types\nof measures for co-occurrence: (1) the reciprocal\nrank of subject-object co-occurrence counts and\n(2) the conditional probability of the gold object\ngiven a subject. The former is a relative measure\nsince it considers a reciprocal rank of the gold ob-\nject among output candidates, while the latter is an\nabsolute measure as it uses conditional probability\nregardless of other output candidates. Here, we\nuse the co-occurrence statistics of the pre-training\ncorpora to compute co-occurrence counts and con-\nditional probabilities.\n5 Experiments\n5.1 Target Models\nWe test open-source versions of GPT-3 (Brown\net al., 2020) with four different model sizes: GPT-\nNeo 125M, GPT-Neo 1.3B, GPT-Neo 2.7B, and\nGPT-J 6B (Black et al., 2021; Wang and Komat-\nsuzaki, 2021), which are publicly available on Hug-\ngingface’s transformers (Wolf et al., 2020). These\nmodels are pre-trained on the Pile (Gao et al.,\n2020), which is a publicly available dataset that\nconsists of 800GB of high-quality texts from 22\ndifferent sources.\n5.2 Results\n5.2.1 Factual Knowledge Probing\nThe results of micro-average hits@1 on the test\nset are reported in Figure 2. Figure 2a shows the\nresults in the zero-shot setting. We observe that\nhits@1 is higher as the model is larger and as we re-\nstrict the output candidates to a smaller set of gold\nobjects. In other words, scaling up model sizes\ncan improve the performance on factual knowledge\n7724\n(a) zero-shot\n (b) finetuned\nFigure 2: Effects of model sizes and restricted candidate sets: We plot micro-average hits@1 on the test set.\n(a) In the zero-shot setting, we observe that hits@1 is higher as the model is larger and as the output vocabulary\nis restricted to a smaller set. (b) In the finetuned setting, we observe that the effect of model sizes and restricted\ncandidate sets is marginal. Effects of finetuning: We observe that finetuning boosts the overall performance.\nFigure 3: Memorization capacity of finetuned LLMs:\nWe show micro-average hits@1 of the finetuned models\non the training set. We observe that the models are\ncapable of memorizing most of the seen facts during\nfinetuning except for the smallest model.\nFigure 4: The results of term frequency baselines:\nWe report micro-average hits@1 on the test set. We\nobserve that a large portion (about 60%) of the facts\ncan be recalled with the joint probability when the out-\nput candidates are tightly restricted in the gold objects\n(relation-wise) setting. Note that co-occurrence is useful\nbut not sufficient to recall facts.\nprobing, and LLMs struggle to recognize appro-\npriate object candidates. Figure 2b presents the\nresults in the finetuned setting. We find that the\neffect of model sizes is marginal. Different from\nthe zero-shot setting, the effect of restricting the\noutput candidates is also marginal, implying that\nthe models may learn appropriate candidate sets\nduring finetuning. We also observe that finetun-\ning improves factual knowledge probing accuracy\nsubstantially. The results of MRR are shown in\nFigure 10 in Appendix C as they exhibit a similar\ntendency.\nFigure 3 shows the hits@1 results of finetuned\nmodels on the training set. We observe that the\nmodels except for the smallest one are capable of\nmemorizing most of the seen facts. This implies\nthat memorization is necessary to recall facts since\nfactual knowledge in the test set may not be in-\nferred based on prior knowledge of other facts in\nthe training set.\n5.2.2 Term Frequency Baselines\nFigure 4 shows how much factual knowledge can\nbe recalled with term frequency statistics of the\npre-training data. We observe that a large por-\ntion (about 60%) of the facts can be recalled with\nthe joint probability baseline when the output can-\ndidates are tightly restricted in the gold objects\n(relation-wise) setting. The joint probability base-\nline performs as well as GPT-J 6B, the largest\nmodel considered in our experiments. The results\nencourage us to consider the co-occurrence statis-\ntics when evaluating language models as it may in-\nflate model performance. Although co-occurrence\n7725\n(a) zero-shot\n (b) finetuned\nFigure 5: The correlation between co-occurrence statistics and factual knowledge probing accuracy: We plot\nhits@1 against Ppretrain(obj|subj), the conditional probability of the gold object given a subject, on the test set\nin the remove stopwords setting. In both (a) zero-shot and (b) finetuned settings, we observe a strong correlation:\nhits@1 is lower as the co-occurrence count is lower. As a result, LLMs struggle to recall rare facts. We observe that\nsuch correlation remains despite scaling up model sizes or finetuning.\nFigure 6: Correlational analysis of larger models:\nWe test GPT-3.5 175B and ChatGPT on the subset of\ntest data in the remove stopwords setting, verifying that\ncorrelation remains despite scaling up model sizes.\nFigure 7: Correlational analysis of finetuned models\non the training set: We also report the results of fine-\ntuned models on the training set in theremove stopwords\nsetting. Surprisingly, we observe a similar trend to the\nresults on the test set, indicating that LLMs struggle to\nmemorize seen facts if they are rare in the pre-training\ncorpora.\nhelps recall facts, it may not be appropriate to un-\nderstand the semantics behind words. The results\nof MRR, shown in Figure 11 in Appendix C, show\na similar tendency.\n5.2.3 Correlation Analysis\nThis section reports the results of the correlation\nbetween co-occurrence statistics and factual knowl-\nedge of LLMs. Note that we exclude facts whose\nco-occurrence count is unknown (e.g. composed\nof an entity with more than three tokens), which\namounts to less than 6% of the total samples. In\nthis section, we analyze the effects of (1) finetuning\nand (2) scaling up model sizes.\nIn Figure 5, we plot hits@1 of the target models\nagainst Ppretrain(obj|subj), the conditional prob-\nability of the gold object given a subject. In both\nzero-shot and finetuned settings, we observe that\nhits@1 is lower as the co-occurrence count is lower.\nConsequently, LLMs suffer from generalizing to\nrecalling rare facts. Comparing Figure 5a and 5b,\nwe observe that finetuning does not resolve co-\noccurrence bias despite improving the overall per-\nformance. In both zero-shot and finetuned settings,\nwe observe that such correlation exists regardless\nof model sizes. We further test larger models: GPT-\n3.5 (InstructGPT) 175B (Ouyang et al., 2022) and\nChatGPT12, a GPT optimized to a dialogue system.\nFor ChatGPT, we test two variants with different\nsizes: (1) GPT-3.5-turbo and (2) GPT-4. Since\nthe vocabulary of ChatGPT is different from the\n1https://openai.com/blog/chatgpt\n2Note that the pre-training data of GPT-3.5 and ChatGPT\nare not the same as the Pile, but we use the results as a proxy.\n7726\nTable 1: The failure cases of GPT-J 6B, preferring words\nwith higher co-occurrence counts over the correct an-\nswers. The numbers in parentheses represent the co-\noccurrence counts.\nQuery Groundtruth Prediction\nTim Mitchellwas born in Detroit(19) London(246)\nLa Promessewas created inBelgium(87) France(3420)\nYutaka Abedied in Kyoto(14) Tokyo(43)\nBell Labsis owned by Nokia(1744) Google(5167)\nWere Iluis located in Ethiophia(129) Israel(254)\nTable 2: The quantitative failure analysis of GPT-J 6B,\ncounting how often a word with higher co-occurrence is\npreferred over the correct answer. We report the ratio of\nbiased cases to the total failure cases in each frequency\nbin.\nFrequency bin Ratio\n1/1 0%\n1/2 15%\n1/4 42%\n1/8 56%\n1/16 70%\n1/32 78%\n1/64 85%\n0 95%\nTotal 38%\nopen-source target models, we report the results on\nthe subset of test data, which filters out samples\nwhose answer is not in the vocabulary of ChatGPT.\nThe results in Figure 6 verify that scaling up model\nsizes does not resolve co-occurrence bias while\nimproving the overall performance.\nIn Figure 7, we investigate the results of seen\nfacts during finetuning. Interestingly, LLMs strug-\ngle to learn facts that rarely appear in the pre-\ntraining corpora although they are explicitly given\nduring finetuning. The results of hits@1 against\nthe reciprocal rank of subject-object co-occurrence\ncounts are shown in Figure 12, 13 in Appendix C.\n5.2.4 Failure Analysis\nCo-occurrence statistics are necessary but not suffi-\ncient to recall facts, as shown in Figure 4. There-\nfore, a heavy reliance on co-occurrence may be\nproblematic. Table 1 showcases failure cases of\nGPT-J 6B in the gold objects (relation-wise) set-\nting. The examples demonstrate that the model\nfails to recall facts by selecting words with higher\nco-occurrence counts over the correct answers.\nThis implies that co-occurrence statistics may often\nwork as spurious features, leading to hallucinations\nTable 3: The failure analysis of GPT-J 6B, com-\nparing the conditional probability of predictions,\nPpretrain( ˆobj|subj), and the conditional probability of\nthe groundtruth objects, Ppretrain(obj|subj). We re-\nport the mean and standard deviation of conditional\nprobabilities in each frequency bin.\nFrequency bin P( ˆobj|subj) P(obj|subj)\n1/1 0.42±0.31 1.00 ±0.00\n1/2 0.38±0.28 0.72 ±0.14\n1/4 0.37±0.27 0.37 ±0.07\n1/8 0.31±0.26 0.18 ±0.04\n1/16 0.29±0.29 0.09 ±0.02\n1/32 0.30±0.31 0.05 ±0.01\n1/64 0.26±0.32 0.02 ±0.00\n0 0.26±0.30 0.01 ±0.00\nTotal 0.35±0.29 0.46 ±0.32\nand biased responses.\nWe also quantitatively measure how often the\ncorrect answer is overridden by a word with higher\nco-occurrence counts. Here, we count in each ques-\ntion whether the model’s generated answer has\nhigher co-occurrence counts than the correct an-\nswer when the model fails to answer correctly. We\ndefine a biased case as when the correct answer is\noverridden by a word with higher co-occurrence\ncounts. Table 2 reports the ratio of GPT-J 6B’s\nbiased cases to the total failure cases in each fre-\nquency bin. We observe that a word with higher\nco-occurrence counts overrides the correct answer\nin a total of 38% of the failure cases. The results\nof different frequency bins indicate that the co-\noccurrence bias is more problematic when recall-\ning rare facts. Additionally, Table 3 compares the\nconditional probability of the generated objects,\nPpretrain( ˆobj|subj), and the conditional probabil-\nity of the gold objects, Ppretrain(obj|subj). The\nresults show that LLMs prefer to generate words\nthat co-occur with the subject frequently enough\n(Ppretrain( ˆobj|subj) ≥0.26). In other words, re-\ncalling rare facts is especially difficult since words\nwith low co-occurrence counts are hardly gener-\nated.\n6 Mitigation\nDebiasing with Undersampling Considering\nfacts whose subject-object co-occurrence count\nis high as biased samples, we suggest finetuning\nLMs on a debiased dataset, constructed by filtering\nout biased samples from the training set. Given a\ndataset D with samples xi and corresponding bias\n7727\n(a) Debiased finetuning vs random filtering\n (b) Effects of filtering ratios on debiased finetuning\nFigure 8: (a) Effects of debiased finetuning: We report the micro-average hits@1 of GPT-J 6B on the original\ntraining set in the remove stopwords setting, comparing debiased finetuning and the random filtering baseline. The\nresults show that debiased finetuning helps to learn rare facts but hampers learning frequent facts. (b) Effects of\nfiltering ratios: We observe that higher filtering ratios cause performance degradation on frequent facts but marginal\nimprovement on rare facts. Therefore, a filtering ratio needs to be properly tuned to maximize the performance\ngains on rare facts while keeping the performance on frequent facts.\nFigure 9: Effects of debiased finetuning on the test set:\nWe also analyze the effects of debiased finetuning on\nthe test set. The results show that the effect of debiased\nfinetuning is marginal at test time.\nscores scorebias(xi), and a filtering ratio p, we dis-\ncard p% of the total samples with the highest bias\nscores. We compute scorebias(xi) as the condi-\ntional probability Ppretrain(obji|subji). Since the\nnumber of samples is reduced, we consider a ran-\ndom filtering baseline that randomly filters out p%\nof the total samples for a fair comparison. Note that\nwe apply the filtering algorithms for each relation\nseparately to prevent discarding nearly all samples\nof a highly biased relation. We test three different\nfiltering ratios: 0.1, 0.3, and 0.5.\nThe results of GPT-J 6B on the original train-\ning set are shown in Figure 8. Figure 8a compares\nthe debiased model with the random filtering base-\nline with a filtering ratio 0.5. The debiased model\nsurpasses the baseline on rare facts with sacrifices\non frequent ones. Figure 8b compares the effects\nof different filtering ratios on debiased finetuning.\nWe observe that the performance on frequent facts\nsignificantly degrades while improvements on rare\nones are marginal as more samples are filtered out.\nFurthermore, we investigate the effects of debiased\nfinetuning on the test set in Figure 9. We observe\nthat the performance of the debiased model and\nthe baseline are similar regardless of co-occurrence\ncounts, implying that the effect of debiased fine-\ntuning is not generalizable. Since it is non-trivial\nto directly fix the cause of co-occurrence bias, de-\nsigning a more sophisticated debiasing algorithm\nor using other approaches may be beneficial to\ncomplement the proposed debiased finetuning. We\nleave further investigation in this direction as future\nwork.\n7 Conclusion\nIn this work, we investigate the impact of co-\noccurrence statistics of the pre-training corpora\non factual knowledge of LLMs. We reveal the\nco-occurrence bias of LLMs, in which they pre-\nfer words that frequently co-occur with the subject\nentity over the correct answer. As a result, LLMs\nstruggle to recall facts whose subject and object\nrarely co-occur in the pre-training corpora although\nthey are seen during finetuning. Although scaling\nup model sizes or finetuning substantially improves\nthe overall performance, the co-occurrence bias re-\nmains. Therefore, we suggest further investigation\non mitigating co-occurrence bias to ensure the reli-\nability of language models.\n7728\nLimitations\nDue to the requirement of large computational re-\nsources and the availability of pre-training corpora,\nwe are only able to test a limited set of LLMs. Al-\nthough we believe that testing other LLMs does not\ninvalidate our claims, it would be good to verify the\nscalability of the results to strengthen our claims.\nAnother limitation is that our work only focuses on\na factual knowledge probing test, which may not\nbe aligned with real-world scenarios. It would be\nbeneficial to investigate how our results generalize\nto downstream tasks, such as question answering\nand summarization.\nEthics Statement\nIn light of the growing prevalence of LLMs, eth-\nical concerns and challenges have also emerged.\nFor example, LLMs often generate factually in-\ncorrect or biased responses. Within this context,\nour work shows that LLMs strongly correlate with\nsimple co-occurrence statistics of the pre-training\ncorpora, implying that they may generate the most\nco-occurring word without truly understanding the\nmeaning behind words. We believe that our work\nhas a positive social impact as it suggests a direc-\ntion toward mitigating potential harms to ensure\nreliability.\nAcknowledgements\nThis work was supported by Institute of Informa-\ntion & communications Technology Planning &\nEvaluation (IITP) grant funded by the Korea gov-\nernment(MSIT) (No.2022-0-00984, Development\nof Artificial Intelligence Technology for Personal-\nized Plug-and-Play Explanation and Verification\nof Explanation), (No.2022-0-00184, Development\nand Study of AI Technologies to Inexpensively\nConform to Evolving Policy on Ethics), (No.2019-\n0-00075, Artificial Intelligence Graduate School\nProgram(KAIST))\nReferences\nEkin Akyurek, Tolga Bolukbasi, Frederick Liu, Bin-\nbin Xiong, Ian Tenney, Jacob Andreas, and Kelvin\nGuu. 2022. Towards tracing knowledge in language\nmodels back to the training data. In Findings of the\nConference on Empirical Methods in Natural Lan-\nguage Processing.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the ACM conference\non fairness, accountability, and transparency.\nSteven Bird, Ewan Klein, and Edward Loper. 2009.Nat-\nural language processing with Python: analyzing text\nwith the natural language toolkit.\nSid Black, Leo Gao, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021. GPT-Neo: Large\nScale Autoregressive Language Modeling with Mesh-\nTensorflow.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016. Man\nis to computer programmer as woman is to home-\nmaker? debiasing word embeddings. Advances in\nNeural Information Processing Systems.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-\nDuran, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. Advances in Neural Information Pro-\ncessing Systems.\nAntoine Bordes, Jason Weston, Ronan Collobert, and\nYoshua Bengio. 2011. Learning structured embed-\ndings of knowledge bases. In Proceedings of the\nAAAI Conference on Artificial Intelligence.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in Neural Information Processing\nSystems.\nAylin Caliskan, Joanna J Bryson, and Arvind Narayanan.\n2017. Semantics derived automatically from lan-\nguage corpora contain human-like biases. Science.\nBoxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingy-\nong Yan, Meng Liao, Tong Xue, and Jin Xu. 2021.\nKnowledgeable or educated guess? revisiting lan-\nguage models as knowledge bases. In Proceedings\nof the Annual Meeting of the Association for Com-\nputational Linguistics and the International Joint\nConference on Natural Language Processing.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramer, and Chiyuan Zhang.\n2023. Quantifying memorization across neural lan-\nguage models. In International Conference on Learn-\ning Representations.\nNicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej\nKos, and Dawn Song. 2019. The secret sharer: Eval-\nuating and testing unintended memorization in neural\nnetworks. In USENIX Security Symposium.\n7729\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom B Brown, Dawn Song, Ul-\nfar Erlingsson, et al. 2021. Extracting training data\nfrom large language models. In USENIX Security\nSymposium.\nKenneth Church and Patrick Hanks. 1990. Word associ-\nation norms, mutual information, and lexicography.\nComputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Amir\nFeder, Abhilasha Ravichander, Marius Mosbach,\nYonatan Belinkov, Hinrich Schütze, and Yoav Gold-\nberg. 2022. Measuring causal effects of data statis-\ntics on language model’s ‘factual’ predictions. arXiv\npreprint arXiv:2207.14251.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha\nRavichander, Eduard Hovy, Hinrich Schütze, and\nYoav Goldberg. 2021. Measuring and improving con-\nsistency in pretrained language models. Transactions\nof the Association for Computational Linguistics.\nHady Elsahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique Lafor-\nest, and Elena Simperl. 2018. T-REx: A large scale\nalignment of natural language with knowledge base\ntriples. In Proceedings of the International Confer-\nence on Language Resources and Evaluation.\nAli Emami, Kaheer Suleman, Adam Trischler, and\nJackie Chi Kit Cheung. 2020. An analysis of dataset\noverlap on winograd-style tasks. In Proceedings of\nthe International Conference on Computational Lin-\nguistics.\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association for\nComputational Linguistics.\nVitaly Feldman. 2020. Does learning require memoriza-\ntion? a short tale about a long tail. In Proceedings of\nthe ACM SIGACT Symposium on Theory of Comput-\ning.\nVitaly Feldman and Chiyuan Zhang. 2020. What neural\nnetworks memorize and why: Discovering the long\ntail via influence estimation. Advances in Neural\nInformation Processing Systems.\nLeandra Fichtel, Jan-Christoph Kalo, and Wolf-\nTilo Balke. 2021. Prompt tuning or fine-tuning-\ninvestigating relational knowledge in pre-trained lan-\nguage models. In Conference on Automated Knowl-\nedge Base Construction.\nWilliam Fish. 2009. Perception, hallucination, and\nillusion.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800GB dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy,\nRoy Schwartz, Samuel Bowman, and Noah A Smith.\n2018. Annotation artifacts in natural language in-\nference data. In Proceedings of the Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. 2022. LoRA: Low-rank adaptation of large\nlanguage models. In International Conference on\nLearning Representations.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics.\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\nWallace, and Colin Raffel. 2023. Large language\nmodels struggle to learn long-tail knowledge. In\nInternational Conference on Machine Learning.\nNikhil Kandpal, Eric Wallace, and Colin Raffel. 2022.\nDeduplicating training data mitigates privacy risks in\nlanguage models. Proceedings of Machine Learning\nResearch.\nNora Kassner, Philipp Dufter, and Hinrich Schütze.\n2021. Multilingual LAMA: Investigating knowledge\nin multilingual pretrained language models. In Pro-\nceedings of the Conference of the European Chapter\nof the Association for Computational Linguistics.\nNora Kassner, Benno Krojer, and Hinrich Schütze. 2020.\nAre pretrained language models symbolic reasoners\nover knowledge? In Proceedings of the Conference\non Computational Natural Language Learning.\nNora Kassner and Hinrich Schütze. 2020. Negated and\nmisprimed probes for pretrained language models:\nBirds can talk, but cannot fly. In Proceedings of the\nAssociation for Computational Linguistics.\nMehran Kazemi, Sid Mittal, and Deepak Ramachan-\ndran. 2023. Understanding finetuning for factual\nknowledge extraction from language models. arXiv\npreprint arXiv:2301.11293.\n7730\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. 2022. Deduplicating training\ndata makes language models better. In Proceedings\nof the Association for Computational Linguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the Conference on Empiri-\ncal Methods in Natural Language Processing.\nShaobo Li, Xiaoguang Li, Lifeng Shang, Zhenhua Dong,\nCheng-Jie Sun, Bingquan Liu, Zhenzhou Ji, Xin\nJiang, and Qun Liu. 2022. How pre-trained language\nmodels capture factual knowledge? a causal-inspired\nanalysis. In Findings of the Association for Compu-\ntational Linguistics.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the Annual Meeting of the Association\nfor Computational Linguistics and the International\nJoint Conference on Natural Language Processing.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023a. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengx-\niao Du, Zhilin Yang, and Jie Tang. 2022. P-tuning:\nPrompt tuning can be comparable to fine-tuning\nacross scales and tasks. In Proceedings of the As-\nsociation for Computational Linguistics.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2023b. GPT\nunderstands, too. AI Open.\nInbal Magar and Roy Schwartz. 2022. Data contami-\nnation: From memorization to exploitation. In Pro-\nceedings of the Association for Computational Lin-\nguistics.\nMitch Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a large annotated\ncorpus of English: The Penn Treebank. Computa-\ntional Linguistics.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factual-\nity in abstractive summarization. In Proceedings of\nthe Association for Computational Linguistics.\nR Thomas McCoy, Paul Smolensky, Tal Linzen, Jian-\nfeng Gao, and Asli Celikyilmaz. 2023. How much do\nlanguage models copy from their training data? evalu-\nating linguistic novelty in text generation using raven.\nTransactions of the Association for Computational\nLinguistics.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right\nfor the wrong reasons: Diagnosing syntactic heuris-\ntics in natural language inference. In Proceedings of\nthe Association for Computational Linguistics.\nMike Mintz, Steven Bills, Rion Snow, and Dan Juraf-\nsky. 2009. Distant supervision for relation extraction\nwithout labeled data. In Proceedings of the Joint\nConference of the Annual Meeting of the Association\nfor Computational Linguistics and the International\nJoint Conference on Natural Language Processing.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing\nand the International Joint Conference on Natural\nLanguage Processing.\nNina Poerner, Ulli Waltinger, and Hinrich Schütze. 2020.\nE-BERT: Efficient-yet-effective entity embeddings\nfor BERT. In Findings of the Conference on Empiri-\ncal Methods in Natural Language Processing.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying LMs with mixtures of soft prompts.\nIn Proceedings of the Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research.\nAbhilasha Ravichander, Eduard Hovy, Kaheer Suleman,\nAdam Trischler, and Jackie Chi Kit Cheung. 2020.\nOn the systematicity of probing contextualized word\nrepresentations: The case of hypernymy in BERT. In\nProceedings of the Joint Conference on Lexical and\nComputational Semantics.\nYasaman Razeghi, Robert L Logan IV , Matt Gardner,\nand Sameer Singh. 2022. Impact of pretraining term\nfrequencies on few-shot numerical reasoning. In\nFindings of the Conference on Empirical Methods in\nNatural Language Processing.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\nConference on Empirical Methods in Natural Lan-\nguage Processing.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\n7731\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In International Conference on Learning\nRepresentations.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting knowledge from language models with au-\ntomatically generated prompts. In Proceedings of the\nConference on Empirical Methods in Natural Lan-\nguage Processing.\nHerbert A Simon. 1954. Spurious correlation: A causal\ninterpretation. Journal of the American Statistical\nAssociation.\nCongzheng Song and Vitaly Shmatikov. 2019. Audit-\ning data provenance in text-generation models. In\nProceedings of the ACM SIGKDD International Con-\nference on Knowledge Discovery & Data Mining.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-\nner, and Sameer Singh. 2019. Universal adversarial\ntriggers for attacking and analyzing NLP. In Pro-\nceedings of the Conference on Empirical Methods in\nNatural Language Processing and the International\nJoint Conference on Natural Language Processing.\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-\n6B: A 6 Billion Parameter Autoregressive Lan-\nguage Model. https://github.com/kingoflolz/\nmesh-transformer-jax.\nTianlu Wang, Rohit Sridhar, Diyi Yang, and Xuezhi\nWang. 2022. Identifying and mitigating spurious cor-\nrelations for improving robustness in NLP models.\nIn Findings of the Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V Le. 2022. Finetuned language mod-\nels are zero-shot learners. In International Confer-\nence on Learning Representations.\nJason Wei, Dan Garrette, Tal Linzen, and Ellie Pavlick.\n2021. Frequency effects on syntactic rule learning in\ntransformers. In Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transformers:\nState-of-the-art natural language processing. In Pro-\nceedings of the Conference on Empirical Methods in\nNatural Language Processing: System Demonstra-\ntions.\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2020. Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language\nmodel. In International Conference on Learning\nRepresentations.\nChiyuan Zhang, Daphne Ippolito, Katherine Lee,\nMatthew Jagielski, Florian Tramèr, and Nicholas Car-\nlini. 2021. Counterfactual memorization in neural\nlanguage models. arXiv preprint arXiv:2112.12938.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: Learning vs. learning to\nrecall. In Proceedings of the Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies.\n7732\nA Details of Finetuning\nWe train models for 3 epochs with a learning rate of 2e-5 and a batch size of 128, padding sequences to\na fixed length of 128. Models are trained to predict the masked word only, rather than the whole input\nprompt. The other hyperparameters are the same as the default hyperparameters of a training script of\ncausal language modeling in Huggingface’s transformers (Wolf et al., 2020).\nFor finetuning uni-directional LMs, we use a manually designed prompt, “### Input:\\n {X}\\n\\n###\nResponse:”, where X is replaced with a masked fact (e.g. “The capital of Canada is [MASK] .”).\nB Factual Knowledge Probing Dataset\nTable 4: Descriptions and statistics of the LAMA-TREx dataset.\nRelation ID Label Template Type Train Test\nP17 country [X] is located in [Y] . N-1 650 262\nP19 place of birth [X] was born in [Y] . N-1 537 243\nP20 place of death [X] died in [Y] . N-1 582 235\nP27 country of citizenship [X] is [Y] citizen . N-M 691 267\nP30 continent [X] is located in [Y] . N-1 657 302\nP31 instance of [X] is a [Y] . N-M 608 274\nP36 capital The capital of [X] is [Y] . 1-1 330 141\nP37 official language The official language of [X] is [Y] . N-1 620 280\nP39 position held [X] has the position of [Y] . N-M 330 155\nP47 shares border with [X] shares border with [Y] . N-M 448 203\nP101 field of work [X] works in the field of [Y] . N-M 409 164\nP103 native language The native language of [X] is [Y] . N-1 635 284\nP106 occupation [X] is a [Y] by profession . N-M 569 252\nP108 employer [X] works for [Y] . N-M 274 104\nP127 owned by [X] is owned by [Y] . N-1 424 195\nP131 located in the administrative territorial entity [X] is located in [Y] . N-1 535 240\nP136 genre [X] plays [Y] music . N-1 616 243\nP138 named after [X] is named after [Y] . N-1 327 140\nP140 religion [X] is affiliated with the [Y] religion . N-1 299 135\nP159 headquarters location The headquarter of [X] is in [Y] . N-1 565 236\nP176 manufacturer [X] is produced by [Y] . N-1 666 291\nP178 developer [X] is developed by [Y] . N-M 411 177\nP190 twinned administrative body [X] and [Y] are twin cities . N-M 454 217\nP264 record label [X] is represented by music label [Y] . N-1 43 10\nP276 location [X] is located in [Y] . N-1 515 251\nP279 subclass of [X] is a subclass of [Y] . N-1 623 280\nP361 part of [X] is part of [Y] . N-1 533 223\nP364 original language of film or TV show The original language of [X] is [Y] . N-1 531 225\nP407 language of work or name [X] was written in [Y] . N-1 598 259\nP413 position played on team / speciality [X] plays in [Y] position . N-1 675 277\nP449 original network [X] was originally aired on [Y] . N-1 585 223\nP463 member of [X] is a member of [Y] . N-M 153 50\nP495 country of origin [X] was created in [Y] . N-1 652 253\nP527 has part [X] consists of [Y] . N-M 661 295\nP530 diplomatic relation [X] maintains diplomatic relations with [Y] . N-M 667 283\nP740 location of formation [X] was founded in [Y] . N-1 599 244\nP937 work location [X] used to work in [Y] . N-M 592 261\nP1001 applies to jurisdiction [X] is a legal term in [Y] . N-M 461 203\nP1303 instrument [X] plays [Y] . N-M 352 161\nP1376 capital of [X] is the capital of [Y] . 1-1 120 59\nP1412 languages spoken, written or signed [X] used to communicate in [Y] . N-M 665 259\nTotal 20662 8856\n7733\nC Additional Results\n(a) zero-shot\n (b) finetuned\nFigure 10: Effects of model sizes and restricted candidates sets: We show micro-average MRR on the test set.\n(a) In the zero-shot setting, we observe that MRR is higher as the model is bigger and as the output vocabulary\nis restricted to a smaller set. (b) In the finetuned setting, we observe that the effect of model sizes and restricted\ncandidate sets is marginal. Effects of finetuning: We observe that finetuning boosts MRR on factual knowledge\nprobing.\nFigure 11: The results of term frequency baselines: We plot micro-average MRR on the test set. In the gold\nobjects (relation-wise) setting, we observe that the joint probability performs as well as GPT-J 6B, which is the\nlargest model considered in our experiments.\n(a) zero-shot\n (b) finetuned\nFigure 12: The correlation between co-occurrence statistics and factual knowledge probing accuracy: We plot\nhits@1 against the reciprocal rank of subject-object co-occurrence counts on the test set in the remove stopwords\nsetting. In both settings (a) and (b), we observe a strong correlation between co-occurrence and hits@1.\n7734\nFigure 13: Correlational analysis of finetuned models on the training set: We also plot hits@1 of finetuned\nmodels against the reciprocal rank of subject-object co-occurrence counts on the training set in theremove stopwords\nsetting. Surprisingly, we observe a similar trend to the results on the test set, indicating that LLMs struggle to\nmemorize seen facts if they are rare in the pre-training corpora.\n7735",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6807146668434143
    },
    {
      "name": "Subject (documents)",
      "score": 0.5640769004821777
    },
    {
      "name": "Recall",
      "score": 0.4988579750061035
    },
    {
      "name": "Object (grammar)",
      "score": 0.47994568943977356
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.45776277780532837
    },
    {
      "name": "Natural language processing",
      "score": 0.43977367877960205
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43567854166030884
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3588687777519226
    },
    {
      "name": "Machine learning",
      "score": 0.34185925126075745
    },
    {
      "name": "Psychology",
      "score": 0.20808982849121094
    },
    {
      "name": "World Wide Web",
      "score": 0.08952105045318604
    },
    {
      "name": "Programming language",
      "score": 0.07407602667808533
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210099236",
      "name": "Kootenay Association for Science & Technology",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I157485424",
      "name": "Korea Advanced Institute of Science and Technology",
      "country": "KR"
    }
  ],
  "cited_by": 7
}