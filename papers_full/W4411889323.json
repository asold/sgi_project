{
    "title": "From concept to manufacturing: evaluating vision-language models for engineering design",
    "url": "https://openalex.org/W4411889323",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A5039731391",
            "name": "Cyril Picard",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5054193933",
            "name": "Kristen M. Edwards",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5093321387",
            "name": "Anna C. Doris",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5104301716",
            "name": "Brandon Man",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5099262026",
            "name": "Giorgio Giannone",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5035835155",
            "name": "Md Ferdous Alam",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5040705509",
            "name": "Faez Ahmed",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2898964810",
        "https://openalex.org/W4392875681",
        "https://openalex.org/W2038333915",
        "https://openalex.org/W4385495347",
        "https://openalex.org/W2724418474",
        "https://openalex.org/W4324316215",
        "https://openalex.org/W4254882155",
        "https://openalex.org/W2092801729",
        "https://openalex.org/W2069697210",
        "https://openalex.org/W2898501751",
        "https://openalex.org/W3128175389",
        "https://openalex.org/W2233566691",
        "https://openalex.org/W3209019340",
        "https://openalex.org/W4387521449",
        "https://openalex.org/W4393934186",
        "https://openalex.org/W3096092761",
        "https://openalex.org/W4391136507",
        "https://openalex.org/W2011634477",
        "https://openalex.org/W2938716636",
        "https://openalex.org/W4220950075",
        "https://openalex.org/W4404782964",
        "https://openalex.org/W4405064339",
        "https://openalex.org/W6610102793",
        "https://openalex.org/W4226096770",
        "https://openalex.org/W4390490537",
        "https://openalex.org/W2165764797",
        "https://openalex.org/W4206448857",
        "https://openalex.org/W2488503856",
        "https://openalex.org/W4327519588",
        "https://openalex.org/W6600669965",
        "https://openalex.org/W4403081466",
        "https://openalex.org/W6604577539",
        "https://openalex.org/W1976036879",
        "https://openalex.org/W4393221457",
        "https://openalex.org/W3095120293",
        "https://openalex.org/W2964118266",
        "https://openalex.org/W4212913949",
        "https://openalex.org/W4385704495",
        "https://openalex.org/W4308649813",
        "https://openalex.org/W1982827541",
        "https://openalex.org/W4411889323",
        "https://openalex.org/W2060970103",
        "https://openalex.org/W4388896754",
        "https://openalex.org/W4389618734",
        "https://openalex.org/W154592414",
        "https://openalex.org/W2019998121",
        "https://openalex.org/W2153524324",
        "https://openalex.org/W6601462914",
        "https://openalex.org/W3215596691",
        "https://openalex.org/W2220999736",
        "https://openalex.org/W4315648806",
        "https://openalex.org/W4388134253",
        "https://openalex.org/W2519260192",
        "https://openalex.org/W4379645258",
        "https://openalex.org/W4388873639",
        "https://openalex.org/W4380685958",
        "https://openalex.org/W2033674995",
        "https://openalex.org/W2261191959",
        "https://openalex.org/W2903261488",
        "https://openalex.org/W6600577311",
        "https://openalex.org/W6628863115",
        "https://openalex.org/W4298156653",
        "https://openalex.org/W4402670290",
        "https://openalex.org/W3021167275",
        "https://openalex.org/W2040806210",
        "https://openalex.org/W6600648412",
        "https://openalex.org/W3199321767",
        "https://openalex.org/W2797161036",
        "https://openalex.org/W4313396343",
        "https://openalex.org/W4388161737",
        "https://openalex.org/W4292495108"
    ],
    "abstract": "Abstract Engineering design is undergoing a transformative shift with the advent of AI, marking a new era in how we approach product, system, and service planning. Large language models have demonstrated impressive capabilities in enabling this shift. Yet, with text as their only input modality, they cannot leverage the large body of visual artifacts that engineers have used for centuries and are accustomed to. This gap is addressed with the release of multimodal vision-language models (VLMs), such as GPT-4V, enabling AI to impact many more types of tasks. Our work presents a comprehensive evaluation of VLMs across a spectrum of engineering design tasks, categorized into four main areas: Conceptual Design, System-Level and Detailed Design, Manufacturing and Inspection, and Engineering Education Tasks. Specifically in this paper, we assess the capabilities of two VLMs, GPT-4V and LLaVA 1.6 34B, in design tasks such as sketch similarity analysis, CAD generation, topology optimization, manufacturability assessment, and engineering textbook problems. Through this structured evaluation, we not only explore VLMs’ proficiency in handling complex design challenges but also identify their limitations in complex engineering design applications. Our research establishes a foundation for future assessments of vision language models. It also contributes a set of benchmark testing datasets, with more than 1000 queries, for ongoing advancements and applications in this field.",
    "full_text": "Accepted: 30 May 2025 / Published online: 1 July 2025\n© The Author(s) 2025\nCyril Picard and Kristen M. Edwards have contributed equally to this study.\n \r Cyril Picard\ncyrilp@mit.edu\n \r Kristen M. Edwards\nkme@mit.edu\n1 Department of Mechanical Engineering, Massachusetts Institute of Technology, Cambridge, \nMA 02139, USA\n2 Department of Applied Mathematics and Computer Science, Technical University of \nDenmark, 2800 Lyngby, Denmark\nFrom concept to manufacturing: evaluating vision-language \nmodels for engineering design\nCyril Picard1  · Kristen M. Edwards1  · Anna C. Doris1  · Brandon Man1  · \nGiorgio Giannone1,2  · Md Ferdous Alam1  · Faez Ahmed1\nArtificial Intelligence Review (2025) 58:288\nhttps://doi.org/10.1007/s10462-025-11290-y\nAbstract\nEngineering design is undergoing a transformative shift with the advent of AI, marking \na new era in how we approach product, system, and service planning. Large language \nmodels have demonstrated impressive capabilities in enabling this shift. Yet, with text \nas their only input modality, they cannot leverage the large body of visual artifacts that \nengineers have used for centuries and are accustomed to. This gap is addressed with \nthe release of multimodal vision-language models (VLMs), such as GPT-4V , enabling AI \nto impact many more types of tasks. Our work presents a comprehensive evaluation of \nVLMs across a spectrum of engineering design tasks, categorized into four main areas: \nConceptual Design, System-Level and Detailed Design, Manufacturing and Inspection, \nand Engineering Education Tasks. Specifically in this paper, we assess the capabilities of \ntwo VLMs, GPT-4V and LLaV A 1.6 34B, in design tasks such as sketch similarity analy -\nsis, CAD generation, topology optimization, manufacturability assessment, and engineer -\ning textbook problems. Through this structured evaluation, we not only explore VLMs’ \nproficiency in handling complex design challenges but also identify their limitations in \ncomplex engineering design applications. Our research establishes a foundation for future \nassessments of vision language models. It also contributes a set of benchmark testing data -\nsets, with more than 1000 queries, for ongoing advancements and applications in this field.\n et al. [full author details at the end of the article]\n1 3\nC. Picard et al.\n1 Introduction\nLarge language models (LLMs) have shown promising performance in domains ranging from \nmedicine (Arkoudas 2023), to law (Katz et al. 2023), to mathematics and coding (Bubeck \net al. 2023). The chat-like interfaces offered by tools such as Google’s Bard (Manyika and \nHsiao 2023) or OpenAI’s ChatGPT (OpenAI 2023) have enabled millions of users to query \nand leverage their immense implicit knowledge to assist in tasks ranging from diagnosing \ndiseases to creating manufacturing drawings (Makatura et al. 2024), to supporting the con-\nceptual design stage for a robotic arm (Stella et al. 2023). Their use of natural language as \nthe input modality offers an intuitive interface for humans to express a variety of problems, \noften in their mother tongue and without the need for technical jargon (Bubeck et al. 2023). \nThe diversity of possibilities is immense and a review identified many tasks within engi -\nneering design that could be automated using natural language processing tools (Siddharth \net al. 2022). Examples include automated design knowledge retrieval, discovery of user \nneeds, and documentation generation.\nHowever, expressing some tasks in text alone is prohibitively complex. In certain \ndomains, especially those involving spatial information, a picture is truly worth one thou -\nsand words. Imagine if one had to write assembly instructions for IKEA furniture using \ntext instructions alone. One would have to describe each part, define its orientation, and \ninstruct where to put screws repeatedly using text similar to “Insert screw A in the third hole \nstarting from the top, where the top of the board is the shortest edge that has a slot through \nits length.” Unsurprisingly, engineers have relied on visual artifacts to communicate and \nsupport their work for centuries (Henderson 1999). From sketches that highlight the key \nworking principles of mechanisms to manufacturing drawings that elicit all the information \nneeded to manufacture, assemble, and inspect parts, visual representations are ubiquitous \nin engineering.\nRecently, powerful multimodal language models have been proposed. In particular, text \nand vision models like GPT-4V (OpenAI 2023), LLaV A 1.6 34B (Liu et al. 2023b, 2024), \nand Fuyu-8B (Bavishi et al. 2023) have shown immense promise since their public release. \nThese vision-language models (VLMs) can take images and text as input and generate text \nas output. Specifically, most VLMs are based on a LLM and extend its capabilities by add-\ning the embedding from a vision encoder as token the LLM’s input context. For example, \nGPT-4V builds upon the leading LLM, GPT-4V . Researchers have begun exploring the \ncapabilities of VLMs in several application domains. Examples include image understand -\ning and reasoning (Yang et al. 2023b), image and language association (Liu et al. 2023a), \nand optical character recognition (Shi et al. 2023).\nTo better understand LLMs’ capabilities, researchers have investigated the performance \nof LLMs on standardized tests in specific fields such as law (Katz et al. 2023), or medi -\ncine (Nori et al. 2023; Rosoł et al. 2023; Takagi et al. 2023), or on specific skills, such as rea-\nsoning or mathematics (Arkoudas 2023; Bubeck et al. 2023). However, there is interest in \nevaluating LLMs and VLMs on their performance on tasks outside of existing standardized \nexams. Doing this requires creating assessment questions and answers that accurately reflect \nthe task of interest. Feng et al. (2024) evaluate GPT-4 on an upper-level undergraduate com-\nputer science course. The authors compiled and classified a dataset of past assessment ques-\ntions, and released their benchmark dataset. Other researchers have focused on evaluating \nthe applicability of LLMs through their chat interface for specific topics, including design \n1 3\n288 Page 2 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nfor manufacturing (Makatura et al. 2024), design for additive manufacturing (Badini et al. \n2023), and the design of cementitious composites (Cai et al. 2024).\nOne challenge for comparing different LLMs or VLMs accurately is that most evaluation \nresearch is looking at a select-few models on a specific task with a specific dataset and met-\nric. If models’ performance are not compared on the same tasks with the same datasets using \nthe same metrics, it is difficult to draw accurate conclusions about how the two models com-\npare. To address this, Chang et al. (2024) survey current LLM research to understand how \nand where LLMs are being evaluated. They compile 46 popular benchmarks ranging from \ngeneral language tasks to domain-specific question and answering tasks. Further, Chang \net al. outline the major areas where LLMs have been evaluated thus far. These are: natural \nlanguage processing tasks; robustness, ethics, biases and trustworthiness; social sciences; \nnatural science and engineering; and medical applications. Chang et al. identify nearly 100 \npublications in these categories. They point out, however, that there remains a lack of stan-\ndardization even within each category, but this can be improved by the development of \nstandardized benchmarks and automatic evaluation.\nThe challenging tradeoff between automatic and human evaluation is quite present in this \nfield (Chang et al. 2024; Zheng et al. 2023; Zhu et al. 2024). For certain tasks around human \npreferences, automatic evaluation techniques may be insufficient, however human ratings \nare expensive and hard to scale. Researchers are exploring the use of LLMs as judges, \nfocusing on their agreement with human evaluations and identifying their biases (Zheng \net al. 2023). New standardized and automatic benchmarks are also being developed to \naddress this. PromptBench, for example, is a unified Python library for evaluating LLMs \nwith modular options for different LLMs, tasks, datasets, benchmarks, prompts, and prompt \nengineering techniques (Zhu et al. 2024). Still, many of these benchmarks and tasks are \ndeveloped for language-only models.\nLooking to evaluate vision-language models rather than language-only models, \nMMBench (Liu et al. 2023c) is a work that specifically addresses benchmarking VLMs via \n“an objective evaluation pipeline of 2,974 multiple-choice questions covering 20 ability \ndimensions.” This work highlights the importance of developing VLM benchmarks that are \nrelevant to a given field. However, the 20 ability dimensions in Liu et al. (2023c) are quite \ngeneral, and not specialized to engineering design tasks.\nIn this work, we aim to create an evaluation methodology including curated tasks, data -\nsets, and benchmarks to answer the question “Can VLMs effectively perform tasks or assist \nengineers in engineering design?”\nEngineering design encompasses a broad range of tasks within the product design pro -\ncess, as shown in Fig. 1. These include: (1) generating and selecting concepts, (2) choosing \nbetween modular and integral structures, (3) sizing components and selecting materials, and \n(4) prototyping, manufacturing, and inspection.\nObjectives and contributions This work is a preliminary and broad exploration of the \ncapabilities of VLMs for engineering design tasks that require textual and visual inputs \nand sets to establish standardized evaluation tasks. We select diverse engineering tasks that \ncould be automated by VLMs and perform qualitative and quantitative analyses of GPT-4V , \nsummarized in Fig. 1. We then use our quantitative benchmark to evaluate a leading open-\nsource VLM, LLaV A 1.6 34B (Liu et al. 2024). We discuss our findings and implications \nfor using VLMs within engineering design. We aimed, wherever possible, for larger sample \n1 3\nPage 3 of 75 288\nC. Picard et al.\nsizes and quantitative analyses, despite the lack of API at the start of our investigation. Spe-\ncifically, our overall contributions are:\n ● We developed and performed quantitative experiments through more than 1000 queries \nto evaluate GPT-4V and create benchmarks for future VLMs related to (i) design simi-\nlarity, (ii) early-stage sketch description, (iii) understanding of engineering drawing and \nCAD generation, (iv) topology optimization result analysis, (v) assessment of manufac-\nturability, (vi) machining feature identification, (vii) defect identification, (viii) textbook \nproblems, and (ix) spatial reasoning.\nFig. 1 We explored GPT-4V and LLaV A 1.6 34B’s ability to perform numerous engineering design tasks \nthat utilize both visual and textual information. Panel “Textbook Problems” adapted from Frey and Gos-\nsard (2009) under CC BY-NC-SA 4.0. Panel “Material Inspection” adapted from Mundt et al. ( 2019) \nunder its specific license\n \n1 3\n288 Page 4 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\n ● We developed and performed qualitative case studies of GPT-4V’s performance related \nto (i) early-stage sketch description generation, (ii) material selection, and (iii) topology \noptimization understanding.\n ● We created and released datasets for future evaluations, including the input images, in-\nput prompts, and answers for all eight quantitative experiments described above on the \nproject webpage https://decode.mit.edu/projects/vlms4design/.\n ● We ran comparative experiments using the open-source VLM LLaV A 1.6 34B to dem-\nonstrate the use of these tasks and datasets for benchmarking VLMs in engineering \ndesign.\nOutline of the work Section 3 delves into conceptual design, focusing on design sketches \nand text descriptions. Section 4 focuses on tasks related to the detailed design stage. In par-\nticular, we discuss material selection, engineering drawing analysis and CAD generation. \nWe also use the specific case of topology optimization to the model’s general understanding \nand the support it can offer for result analysis. Section 5 assesses the general knowledge of \nmanufacturing and tests the performance on inspection tasks. Section 6 investigates the per-\nformance of GPT-4V on textbook problems, providing some insights into its over-arching \nengineering knowledge and skills. Section 7 investigates the performance of GPT-4V on \nspatial reasoning tests, looking into an important skill across design tasks. Finally, Sect. 8 \ncompares the quantitative results from GPT-4V and LLaV A 1.6 34B, and Sect. 9 offers a \nbroader perspective of the capabilities of VLMs for engineering design and discusses its \nlimitations.\n2 Experimental setup\nWe developed most of our prompts within the ChatGPT user interface, specifically harness-\ning the capabilities of the September 25, 2023 update . It should be noted that the princi -\npal content of the paper and the primary experiments were conducted using this particular \nversion,1 with a specific focus on the vision feature. We do not include internet access, \nplugins, or a code interpreter unless explicitly mentioned (for example see Sect. 4.3.1). By \nthe nature of using the ChatGPT web interface, we do not control or vary the “temperature” \nor “top-k” parameters of the responses. Each repeated experiment was run in a new chat, \nwhich we denote as context in this work. When using the GPT-4V API and LLaV A 1.6 34B \nwe used a temperature and top-k of 1.0.\nOur methodology is outlined by the following key points: \n(1) Emphasis on images. Our primary focus in utilizing VLMs is for image-based tasks. \nThis is where our efforts and resources are concentrated, aiming to explore the model’s \nunderstanding and analysis of visual data.\n(2) Short text prompts.  We utilize short, straightforward prompts that prioritize vision \ntasks. This is to ensure a focus on visual analysis over the complexity of prompt engi -\nneering or creating lengthy custom instructions.\n1 The majority of the research and experimental work for the initial version of this paper was carried out \nbefore November 2, 2023, leveraging GPT-4V released on September 25, 2023.\n1 3\nPage 5 of 75 288\nC. Picard et al.\n(3) Transparent prompts and answers. In each section, we provide examples of our exact \nprompts and the exact response from the model, unless shortened with ellipses.\nBy maintaining these standards, we hope to provide clear experiments that can be used as \nbenchmarks for future VLM evaluations in engineering design. Each section contains the \nadditional methodological aspects related to each topic.\n3 Conceptual design\nOverview and motivation Conceptual design is an early stage of the product development \nprocess, during which designers identify customer needs, establish target specifications, \nand then generate, select, and test many concepts (Ulrich et al. 2020). Experts estimate that \n70–80% of a product’s life-cycle costs are determined during the early design stages (Cor -\nbett and Crookall 1986; Pahl et al. 2007), highlighting the importance of decisions made \nduring this stage. There exists a rich body of research examining concept generation during \nthis stage (Bryant et al. 2005; Toh and Miller 2019; Das and Yang 2022), creativity evalu-\nation in conceptual design (Amabile 1982; Shah et al. 2003; John and Sharon 2009; Baer \nand Kaufman 2019; Cseh and Jeffries 2019), cost estimates and environmental assessments \nduring conceptual design (Saravi et al. 2008; Fu et al. 2020), and, recently, multimodal \nmachine learning in conceptual design (Yuan et al. 2021; Edwards et al. 2021; Song et al. \n2023a; Su et al. 2023).\nIn conceptual design, two of the primary design modalities are hand-drawn sketches and \ntextual descriptions. Often these modalities are combined, and early-stage designs are rep -\nresented as a sketch with accompanying text. A vision language model (VLM) such as GPT-\n4V has great potential to be used as a tool during conceptual design. Some of the main tasks \nin conceptual design include generating sketches and descriptions of design ideas, compar-\ning and iterating upon those ideas, and ultimately selecting a design to move forward with. \nIn the following experiments, we explore how GPT-4V can act as an aide for these tasks. \nSpecifically, we see how the multimodal capabilities enable GPT-4V to perform engineering \ntasks when both design sketches and text prompts are included as input. For effective mul -\ntimodal learning, it is important to have designs with both image and text modalities, and \nsufficient datasets of these designs. There exist a few multimodal conceptual design data -\nsets that we use in the following experiments. However, an overarching theme in machine \nlearning within the engineering domain is that most datasets are small, which poses a chal-\nlenge for data-driven models. Large pretrained models like GPT-4V can help overcome this \nchallenge because they have been trained on a plethora of information (although the exact \ntraining information of GPT-4V is not yet released), meaning that they have general knowl-\nedge about the world. Another challenge is faced during concept selection. Experts suggest \ngenerating a multitude of conceptual designs and then down-selecting through design evalu-\nation (Ulrich et al. 2020); however, the evaluation step is often performed by experts, which \ntakes time and resources (Baer and Kaufman 2019). GPT-4V may be able to help engineers \nduring the conceptual design stage by utilizing general knowledge and sketch understanding \nto interpret and compare designs, move between design representations (text and image), \nand perform concept selection tasks. We aim to evaluate these capabilities in the following \nexperiments.\n1 3\n288 Page 6 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\n3.1 Design similarity\nDetermining if two designs are similar is an important part of conceptual design. Assess -\ning design similarity can act as a proxy for assessing design novelty, which is a common \ncriterion in concept selection (Ahmed et al. 2018). Novelty expresses that a concept is rare, \ningenious, imaginative, or surprising, as well as radical or transformational (Verhaegen \net al. 2012). However, novelty evaluation is often subjective. It can be easier for humans \nto articulate why they might rate designs as similar than why they would rate one design \nas more novel than another (Ahmed et al. 2018). For this reason, past work has studied \nhow humans assess similarity of concepts (Ahmed et al. 2018) as a method to build idea \nmaps and identify novelty. Similarity comparisons can also help explore the design space \nby identifying clusters of similar ideas, potentially helping with faster design space explora-\ntion. Recently, researchers have compared how human evaluations of similarity compare \nto computationally determined similarity, and found that they diverge based on the level of \nabstraction of a product (Nandy and Goucher-Lambert 2022).\nThree main challenges arise with human evaluations of similarity: \n1. Evaluation speed and cost Human evaluations are very expensive. Both the time and \nthe cost of these evaluations are exacerbated as the number of designs increases since \nthe number of similarity queries scales with n2 the number of designs.\n2. Self-consistency Humans may make different similarity assessments when they repeat \nthe same evaluation.\n3. Transitive violations Given designs A, B, and C, one cannot say that A is more similar \nto B, B is more similar to C, but that C is also more similar to A. This would violate the \ntransitive property of inequality, since if AB < AC (where AB is a measure of similarity \nbetween design A and design B) and BC < BA, then these imply that CB < CA, so say-\ning CA < CB cannot be true. A violation of this sort can be tested when the same three \ndesigns, which we call a triplet, are assessed for similarity multiple times.\nIn the following experiments, we evaluate if GPT-4V can effectively assess the similarity \nof designs, i.e. with high self-consistency and a low number of transitive violations. We are \nreplicating this experiment which was completed by human raters in Ahmed et al. ( 2018), \nand here novelty is defined as distance from other designs, and is not impacted by relevance \nto the design task. Our methodology, an example of our prompt, and the table of results are \nprovided below.\nMethodology We tasked GPT-4V with performing the same experiment that eleven \nhuman raters performed in Ahmed et al. ( 2018). We have 10 early-stage design sketches \nfrom Starkey et al. (2016) and Toh and Miller ( 2016) as shown in Fig. 2. Each design is \ndefined by a hand-drawn sketch, possibly annotated, and a handwritten description, referred \nto as “text description.” We group them into groups of three, which we call triplets. As \nshown in Fig. 3, we provide a triplet of design sketches labeled A, B, and C, and ask GPT-\n4V which design is most similar to A. Since we have 10 designs, we can make 360 triplets \nsuch that each design is considered design A for all 36 combinations of the other 9 designs \nas designs B and C. When given 360 of these triplet examples, we assessed if GPT-4V \ncommits transitive violations. We then repeated 50 of the examples to evaluate GPT-4V’s \nself-consistency.\n1 3\nPage 7 of 75 288\nC. Picard et al.\nIn addition, we utilize the generalized non-metric multidimensional scaling (GNMDS) \ntechnique introduced by Agarwal et al. ( 2007) to find 2-D embeddings of design sketches \nusing triplet responses and generate a visualization of the ten designs where designs that \nare closer to each other are considered more similar. This technique is the same as used \nin Ahmed et al. ( 2018) to generate a map of these designs from human ratings. The resul -\ntant map, referred to as the idea map of GPT-4V , is shown in Fig. 4. The idea map shows a \nstriking resemblance to human-generated idea maps reported in literature. We observe that \nthe three designs that show cups filled with milk are grouped together (designs 2, 5, and \n6) as well as the two bicycle-based designs (designs 3 and 4). This clustering of similar \ndesigns was also observed in the idea map of all human raters combined, shown in Fig. 8 \nin Ahmed et al. (2018). GPT-4V’s idea map also places Sketch 0 further away from all other \nsketches, denoting that it was perceived as most dissimilar. Coincidentally, the most novel \nsketch identified by the aggregated human ratings was also Sketch 0. Sketch 0, proposing \nFig. 2 Ten conceptual designs of novel milk frothers. We task GPT-4V with assessing the similarity of \nthese designs to one another. The handwritten descriptions at the bottom of each design after the typed \n“Idea Description:” are referred to as “text descriptions” throughout this work. For example, the text \ndescription of design 0 is “countertop jet engine”\n \n1 3\n288 Page 8 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nFig. 4 A map of the milk frother design sketches where sketches that are closer to each other are more \nsimilar. These are based on the responses by GPT-4V for 360 triplet similarity queries. We observe that \nthe map clusters similar designs together and places unique designs further away from other designs\n \nFig. 3 Assess design similarity\n \n1 3\nPage 9 of 75 288\nC. Picard et al.\na countertop jet turbine to froth milk was also the most novel sketch rated by the expert in \ntheir work. This serves as a validity check, demonstrating to us that GPT-4V’s assessment \nof the similarity of sketches aligns with human ratings. We note that in past work, each \nhuman rater has a different map, and GPT-4V creates a unique map as well. The variability \nin individual human idea maps is likely influenced by diverse criteria for judging similarity. \nConsequently, establishing a definitive standard for sketch similarity is challenging. There-\nfore, we compared our results to the aggregated map by eleven human raters to gauge how \nGPT-4V’s assessments conformed with collective human wisdom.\nThese initial findings pave the way for future research avenues. First, GPT-4V’s capa -\nbility to create idea maps is not only scalable, and ideal for evaluating a large number of \ndesign items, but also overcomes limitations faced by previous studies reliant on time-con-\nsuming human ratings. Second, the use of triplet queries for generating idea maps extends \nbeyond sketches to other design forms like prototypes, 3D models, or multimedia, offering \na novel approach to evaluating design similarity. These maps are valuable tools for design-\ners, enabling them to better understand their design domain and leverage GPT-4V for more \neffective exploration of the design space.\nDiscussion: design similarity  From the quantitative experiments using a total of 410 \nqueries, with results summarized in Table 1, we find that GPT-4V is able to assess the \nsimilarity of designs with greater or equal performance as human raters. In the 360 trials, \nGPT-4V made only five transitive violations, which matches the lowest number of transi -\ntive violations made by any of the eleven human raters. Additionally, in our trials, GPT-4V \nwas self-consistent 94% of the time, which is greater than any of the human raters. A naive \nmodel could still be self-consistent without actually understanding much about a design \nsketch. So to gain insight into GPT-4V’s similarity assessment, we plot the designs using \nthe GNMDS embedding technique, shown in Fig. 4. We observe sensible clustering of three \ndesign sketches whose major features are cups with milk, and two design sketches whose \nmajor features are bicycles. While future work should explore how well VLMs perform \nat this task for other datasets, these results offer a promising suggestion that GPT-4V can \neffectively assess the similarity of conceptual design sketches. Future VLMs can be evalu -\nated using these same methods to compare them against this version of GPT-4V . We have \nprovided the dataset including all triplets for this purpose. These experiments were meant \nto test understanding, analysis, and evaluation. The results suggest that GPT-4V is able to \nunderstand and analyze design sketches in order to assess their similarity. Furthermore, \nRater Self-consistency (%) Transitive Violations\n1 91.6 5\n2 50.0 5\n3 83.3 5\n4 75.0 10\n5 58.3 10\n6 58.3 20\n7 41.6 8\n8 41.6 20\n9 58.3 11\n10 75.0 12\n11 58.3 5\nGPT-4V 94.0 5\nTable 1 A summary of the \nself-consistency and number of \ntransitive violations of GPT-4V \n(highlighted in bold) when \nevaluating 360 triplets for which \ndesigns are more similar\nWe compare the results with \nthe corresponding values \nfor 11 human raters reported \nin Ahmed et al. ( 2018). GPT-4V \nhas higher self-consistency than \nthe human raters and its number \nof transitive violations equals \nthe lowest human rater value\n \n1 3\n288 Page 10 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nassessing similarity among design triplets is a mundane and repetitive task. GPT-4V’s abil-\nity to perform this task may mean that human raters do not have to, which can save time and \nresources both in dataset creation and in the design process.\n3.2 Design descriptions\nThrough these experiments, we aim to evaluate how well GPT-4V understands the different \nrepresentations of a design, in this case, textual and sketch. Understanding sketches is a first \nstep in being able to evaluate and compare them, which is one of the end goals of the con -\nceptual design phase (Ulrich et al. 2020). We task GPT-4V with matching a design to its cor-\nrect description given a number of options, and we also task GPT-4V with generating textual \ndescriptions of a design given just a sketch. We specifically chose to perform description \nmatching (Sect. 3.2.1) in the form of multiple-choice questions because this allows for \nquantitative analysis. Additionally, we provide the exact questions and results so that future \nVLMs can be similarly evaluated. In fact, we evaluate GPT-4V , and LLaV A 1.6 34B in this \nmanner; results are shown in Tables 2 and 20. While these description matching and gener-\nating tasks do not directly translate to common engineering tasks during the design process, \nwe believe that GPT-4V’s performance on them sheds light on its ability to understand \ninformation from one modality (sketch) and then synthesize information in another modal-\nity (generating text). Furthermore, if a tool can automatically generate accurate and useful \ntextual descriptions of conceptual design sketches, this could allow engineers to (1) create \nan easily-searchable catalog of early-stage designs, and (2) more easily generate multi -\nmodal datasets of paired sketches and text descriptions, which are necessary for multimodal \nmachine learning in the engineering domain (Song et al. 2023b). Automatically generating \nrelevant textual descriptions for hand-drawn sketches can also help communicate design \nideas to design team members and potential stakeholders, which is a primary role of sketch-\ning (Das and Yang 2022). Lastly, it can help human raters judge design ideas for creativity, \nnovelty, quality, and other common design metrics (Shah et al. 2003).\nCorrect \nanswer\nWith text \ndescription\nNo text \ndescription\nNo text \ndescription, \nno “None of \nthe above”\n0 B B B B\n1 A A A A\n2 C C D B\n3 A A A A\n4 B B B B\n5 C C D B\n6 C C D C\n7 C C D D\n8 B B A A\n9 B B B B\nTrial 1 score 10/10 5/10 6/10\nTrial 2 score 10/10 5/10 8/10\nTrial 3 ccore 10/10 6/10 7/10\nAverage 100% 53.3% 70%\nTable 2 Results for the three \nmultiple-choice design descrip-\ntion matching experiments\n90 queries were run across three \ntrials, 0 queries for each of the \nthree cases. The full results for \nTrial 1 are displayed, as well as \nthe scores for all three trials\n \n1 3\nPage 11 of 75 288\nC. Picard et al.\n3.2.1 Match a description to a design\nGiven an image of an early-stage design sketch, and four different design description \noptions, we test if GPT-4V can identify the correct description. We analyze GPT-4V’s per-\nformance on these simple tasks to gain a basic understanding of whether more challenging \ndescription generation tasks are possible. Our methodology, four examples of our prompts, \nand a table of our results are provided below.\nMethodology We assessed if GPT-4V can match a design sketch to its correct text \ndescription for three different cases: \n1. We provide the whole image including the handwritten text description, as well as four \ndescription options including “None of the above” (Fig. 5 upper left).\n2. We provide the image with the handwritten text description removed, as well as four \ndescription options including “None of the above” (Fig. 5 upper right).\n3. We provide the image with the handwritten text description removed, and only three \ntext description options, removing the “None of the above” option (Fig. 5 lower left and \nlower right).\nWe ran 90 total queries: three trials each comprised of ten multiple choice questions for each \nof three different cases2 We included one multiple-choice question for each of the designs in \nFig. 2. Examples of these questions are shown in Fig. 5. As the answer options, we provided \n2 Trial 1 used the September 25th update of GPT-4V; Trials 2 and 3 used the November 6th update of GPT-4V .\nFig. 5 Match a design to the correct description\n \n1 3\n288 Page 12 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nthree text descriptions from among the 10 design sketches. Table 2 displays the results for \neach of the three cases, as well as the final score out of 10.\nGPT-4V perfectly matches designs to their descriptions when the handwritten description \nis provided in the sketch. When the handwritten description is removed, GPT-4V’s errors \nare typically a result of selecting “None of the above,” and when that option is removed, \nand it is forced to select a description, its performance increases (from 5.33/10 to 7/10). \nThese results, combined with the results of LLaV A 1.6 (see Table 20), suggest that the pres-\nence of textual context in sketches significantly enhances model accuracy, underscoring \nthe importance of integrating text with visual data. The variability in model performance, \nparticularly GPT-4V’s superiority in scenarios without handwritten descriptions, suggests \nthat model choice should be tailored to specific task requirements. Additionally, GPT-4V’s \ntendency to select “None of the above” when uncertain highlights its cautious approach in \nambiguous situations, reflecting a strategy to manage uncertainty. This behavior, along with \nthe contrasting error patterns between GPT-4V and LLaV A 1.6, points to the need for deeper \nunderstanding and improvement in how different models process and interpret visual infor-\nmation, especially in the absence of textual cues. These findings are crucial for optimizing \nthe use of VLMs in conceptual design contexts.\n3.2.2 Generate a design description from a sketch\nGiven an image of an early-stage design sketch, can GPT-4V generate a relevant and accu-\nrate design description?\nWe performed this experiment for five early-stage design sketches with varying drawing \nscores. The drawing scores are based on a Consensual Assessment Technique evaluation of \nstudents’ milk frother designs (Starkey et al. 2016; Toh and Miller 2016). The scores can \nrange from 1–7, but within the dataset of sketches, the scores range from 1–6. Table 3 shows \nthe results for these design sketches. The selected sketches were chosen at random from \namong all sketches with a similar drawing score. We show how GPT-4V responds when \nsimply asked to describe the design, versus when provided with a description of the original \ndesign task given to the students and then asked to describe the design. For brevity, in both \ncases, we prompt GPT-4V to respond in three sentences. The exact prompts are provided in \nthe column headers of Table 3.\nDiscussion: design descriptions In this section, we aimed to assess GPT-4V’s ability to \nmatch different representations of an early-stage conceptual design and to generate one rep-\nresentation from another. The tasks we generated to explore these capabilities were: match \na design sketch with its correct textual description, and generate a textual design descrip -\ntion from a sketch. For each of these tasks, we gave various forms of the sketch to under -\nstand how the amount of handwritten text and the drawing skill in each sketch would affect \nthese results. The quantitative results from the description matching experiment, shown in \nTable 2, provided a basic understanding of whether the later description generation tasks \nwere possible at all. The results showed that given an entire design sketch including the \nhandwritten text description, GPT-4V was able to match the sketch to the text description \nfor 10/10 of the questions across all three trials. This result essentially just assured us that \nGPT-4V could comprehend the hand-written text in these drawings.\nWith this point verified, we next tested description matching if we removed the handwrit-\nten text description from the image. In this case, GPT-4V was able to match the sketch to the \n1 3\nPage 13 of 75 288\nC. Picard et al.\ntext description for 5.33/10 of the questions on average. While this is still better than ran -\ndom chance, which would be an average of 2.5/10, this result demonstrated how important \nproviding both modalities, text, and image, is in this design stage. We noticed that many of \nthe incorrect answers were GPT-4V selecting the “None of the above” option, suggesting \nthat none of the descriptions matched the design. In fact, 4/5 of the incorrect answers for \nTrial 1 and 2, and 3/4 incorrect answers for Trial 3 occurred this way. This is sensible to us, \nas the design sketches are often visually simple compared to their textual descriptions. An \nexample of this is design 5 in Fig. 2, which visually looks like a cup of milk, but whose text \ndescription is “Centrifuge of milk.” With these results in mind, we tested how well GPT-4V \nwould match the sketch to the description if we removed the “None of the above” option. \nThis led to an improved average of 7/10 correct. One interesting example is shown in the \nlower right image of Fig. 5, in which GPT-4V generated its own option because the model \ndetermined that the sketch did not match any of the provided options, despite one being \ncorrect.\nWe further explored GPT-4V’s capability to generate textual descriptions from design \nsketches. The results are shown in Table 3. We explore how well the model generates design \ndescriptions for three designs with different “drawing scores,” or levels of perceived draw-\ning ability. For each design, we task GPT-4V with generating a description using two differ-\nent prompts, one with more information: \n1. Please describe this design in 3 sentences.\n2. A student was asked to develop a new innovative product that froths milk in a short \namount of time. Please describe this design in 3 sentences.\nTable 3 GPT-4V generated descriptions from design sketches\nWe also include the expert-rated drawing score of each sketch. The sketches are ordered by descending \ndrawing score\n1 3\n288 Page 14 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nQualitatively, we assess that the model is able to generate useful and accurate text descrip -\ntions of designs even for sketches with very low drawing scores. We highlight in green \nthe parts of the description that we believe showed understanding and relevance. GPT-4V \nextrapolates beyond just the given text to describe both the form and the function of the \ndesigns. This result once again highlights the importance of having both the text and image \nmodalities in the design, as the generated descriptions pull heavily from the handwritten \ntext to contextualize the sketch and explain how the design might function. Figure 6 dem-\nonstrates GPT-4V’s ability to understand, infer, extrapolate, and generate information about \na design from its sketch. For example, the sketch includes a labeled belt and pulley sys -\ntem, and GPT-4V includes this in the generated description: “ Belt and pulleys:  This part \nFig. 6 Generate a design description\n \n1 3\nPage 15 of 75 288\nC. Picard et al.\nseems to be connected to an AC Motor. The belt and pulley system likely drives the turbine \nmechanism, converting the motor’s rotational movement into the desired frothing action.” \nWe assess that GPT-4V is able to effectively generate textual descriptions when provided \nwith a detailed sketch including handwritten text. This is true for both prompt types, the one \nthat just requests “Please describe this design in 3 sentences,” as well as the prompt that \nprovides additional information about the task. We do not observe a difference in the design \ndescriptions that result from these different prompts, but further research could explore this \nmore. It is important to note that LLMs, like GPT-4V , may hallucinate. This means GPT-4V \nmay perceive patterns or objects in the input that are imperceptible to humans, producing \noutputs that do not make sense. Engineers should perform checks on generated descriptions, \nand additional research should be done regarding generating text descriptions for sketches \nin many domains. However, from our results, GPT-4V is able to generate sensible text \ndescriptions from early-stage sketches. This ability may help engineers (1) create an easily-\nsearchable catalog of early-stage designs, and (2) more easily generate multimodal datasets \nof paired sketches and text descriptions.\n3.3 Concept selection\nA core component of conceptual design is concept selection (Okudan and Tauhid 2009; \nMiller et al. 2021). There are various concept selection methods, ranging from those based \non decision matrices, to uncertainty modeling, to heuristics (Okudan and Tauhid 2009). One \nof the most widely used concept selection methods for engineers is the Pugh Evaluation \nMatrix, sometimes called the Pugh chart (Pugh 1991, 1995).\nA Pugh Chart, also known as a Pugh Matrix, is a decision-making tool used in engineer-\ning and design. It involves comparing multiple options against a set of criteria, using a base-\nline for reference, to determine the most suitable choice. Each option is scored based on how \nwell it meets each criterion compared to the baseline, facilitating an objective evaluation of \nalternatives. The first step in creating a Pugh Chart is defining selection criteria, which will \nbe used to evaluate and compare concepts. The method may vary, but a common practice is \nto then select a benchmark design, and score all other designs qualitatively based on how \nthey compare to the benchmark for each of the selection criteria.\nIn the following experiments, we explore GPT-4V’s ability to both generate selection \ncriteria given a design prompt, and use concept selection methods on design sketches. \nWe utilize a case study presented in the Concept Selection chapter of Product Design and \nDevelopment by Ulrich et al. (2020). This case study provides a design task:\n“A medical supply company retained a product design firm to develop a reusable syringe \nwith precise dosage control for outpatient use.”\nSeven design sketches of reusable syringes are also included along with a Pugh Chart and \nselection criteria. We use these as qualitative benchmarks with which we compare GPT-\n4V’s ability to generate selection criteria and a Pugh Chart.\n1 3\n288 Page 16 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\n3.3.1 Generating selection criteria\nIn Fig. 7, we explore if GPT-4V can generate selection criteria that would be used to evalu-\nate designs, given a description of a design task. As a baseline to assess the generated cri -\nteria, we utilized the selection criteria and Pugh Chart provided in Ulrich et al. ( 2020); \nthese were made for this design task and used the same seven design concepts that we used. \nTable 4 shows the ground truth selection criteria from the textbook, and the matching selec-\ntion criteria generated by GPT-4V (if applicable). As seen in Table 4, GPT-4V generated \nmatching selection criterion for each of the ground truth criterion, sometimes via multiple \ncriteria. For example for the ground truth criterion “Ease of manufacture,” the two matching \nTextbook selection criteria GPT-4V equivalent\nEase of handling Ease of use: Ergonomic grip and \nhandling\nEase of use Ease of use\nReadability of dose settings Ease of use: Clear markings and \nindicators for dosage\nDose metering accuracy Precision and accuracy\nDurability Durability and reusability\nEase of manufacture Cleaning and sterilization: Ease \nof disassembly and reassembly for \ncleaning Scalability: Design should \nbe adaptable for mass production\nPortability Storage and portability\nTable 4 Selection criteria pro-\nvided by the Product Design and \nDevelopment textbook and the \nGPT-4V equivalent\n \nFig. 7 Generating selection criteria for a design task\n \n1 3\nPage 17 of 75 288\nC. Picard et al.\ncriteria generated by GPT-4V are “Cleaning and Sterilization: Ease of disassembly and reas-\nsembly for cleaning” and “Scalability: Design should be adaptable for mass production.”\n3.3.2 Creating a Pugh Chart\nIn Fig. 8, we explore GPT-4V’s ability to perform concept selection by generating a Pugh \nChart. In particular, given selection criteria and images of various designs, can GPT-4V \nanalyze and evaluate the designs and format this evaluation in a Pugh Chart? The results are \ndiscussed in the following section.\nDiscussion: concept selection Through these experiments, we explored GPT-4V’s abil-\nity to perform two common concept selection tasks: generating selection criteria given a \ndesign task and making a Pugh chart given several design concepts. We found that GPT-4V \nwas able to assess a design task and generate many relevant selection criteria, as shown in \nFig. 8 Generating a Pugh Chart for concept selection\n \n1 3\n288 Page 18 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nFig. 7. For example, given this design task “A medical supply company retained a product \ndesign firm to develop a reusable syringe with precise dosage control for outpatient use,” \nGPT-4V generated selection criteria such as “Safety and Biocompatibility,” “Ease of Use,” \nand “Precision and Accuracy.” These criteria highlight that the design must be user-centered \nand safe in healthcare settings.\nTable 4 shows the baseline selection criteria provided for the design task from Ulrich \net al. (2020), as well as the equivalent selection criteria generated by GPT-4V , if applicable. \nWe observe that for each of the seven criteria that the baseline Pugh Chart used, GPT-4V \noutputs an equivalent criterion. It is important to note that some of GPT-4V’s equivalent \ncriteria were subcategories, such as GPT-4V’s “Ease of Use: Clear markings and indicators \nfor dosage,” which mapped to the textbook’s “Readability of dose settings.” These results \ndemonstrate that GPT-4V is able to generate many relevant selection criteria, but that an \nengineer should still read the raw output and select relevant criteria as well as separate sub-\ncategories of certain criteria.\nWhen tasked with creating a Pugh Chart given selection criteria and several designs, we \nfound that GPT-4V understood what a Pugh Chart was and how to generate one, however, \nit was often reluctant to create one given the limited information, as shown in Fig. 8. GPT-\n4V was able to generate an empty Pugh Chart with the correct matrix format (not shown \nin Fig. 8, which switched the typical rows and columns), and also understood that it would \nbe filled with qualitative comparisons of concepts, with one reference concept. However, \nit would only fill in the Pugh chart with hypothetical values given the lack of informa -\ntion about each concept. For example, in one trial, GPT-4V stated “Since I cannot physi -\ncally interact with the concepts to evaluate them against the criteria you’ve provided, such \nas “Ease of handling” or “Durability”, I am unable to make informed decisions about the \nscores. These criteria often require subjective assessments or empirical testing that cannot \nbe performed without direct interaction with the designs.” In every instance of running the \ntask in Fig. 8, GPT-4V refused to fill in the Pugh chart with anything other than random \nhypothetical values. Perhaps if an engineer provided more information about each concept, \nGPT-4V would have been able to generate an accurate Pugh chart, however, it failed to do \nso within our task format.\nOverall, our findings suggest that GPT-4V can be potentially effective in assisting human \ndesigners in identifying key factors that should be considered in the design process. How -\never, while GPT-4V can generate equivalent criteria to those used in traditional methods, its \noutputs may need refinement, such as categorizing subcriteria. In terms of creating a Pugh \nchart, GPT-4V understands the concept and can format the chart correctly, but its reluctance \nto fill in the chart without extensive information indicates a limitation. This suggests that \nwhile GPT-4V can be a useful tool for structuring and initiating the concept selection pro -\ncess, human input remains crucial for detailed analysis and decision-making. The implica -\ntions for practitioners are clear: VLMs such as GPT-4V can be a valuable aid in the initial \nstages of design concept evaluation, but they may require careful oversight and additional \ninformation to realize their full potential in more complex decision-making tasks.\n3.4 Summary\nA concise summary of our assessment areas for GPT-4V’s conceptual design abilities, and \nour findings for each are provided below. \n1 3\nPage 19 of 75 288\nC. Picard et al.\n(i) Assessing design similarity Section 3.1—How does GPT-4V’s consistency in assess-\ning design sketch similarity compare to human benchmarks?\n ● We measure consistency using two measures from Ahmed et al. (2018)—self-con-\nsistency and transitive violations in assessing sketch triplet queries. The model is \nable to assess design similarity with higher self-consistency than human raters (94% \ncompared to 62.8% average for human raters) and as few transitive violations as the \ntop human raters.\n(ii) Matching design representations  Section 3.2.1—Can GPT-4V accurately match \ndesign sketches with their text descriptions under varying information conditions?\n ● We ran 90 queries: three trials each comprised of ten multiple choice questions \nfor each of three different cases. We found the following results. When provided \nthe entire design sketch including a handwritten description, the model matched \na design sketch to its appropriate text description 10/10 times for all three trials, \nhowever with the handwritten description removed, the score dropped to an aver -\nage of 5.33/10, which we can compare to a score of 2.5/10 for randomly matching. \nIncorrect answers were often in the form of choosing “None of the above,” so when \ngiven the same task without the “None of the above” option, the score rose to an \naverage of 7/10.\n(iii) Generating design descriptions  Section 3.2.2—Is GPT-4V capable of generating \neffective descriptions for early-stage design sketches?\n ● Qualitatively, we find that GPT-4V is able to generate accurate and useful design \ndescriptions given hand-drawn sketches.\n(iv) Generating selection criteria Section 3.3.1—How effectively does GPT-4V generate \nconcept selection criteria in engineering design?\n ● In our case study, we find that when provided a design task GPT-4V generates useful \nselection criteria that match those generated by design professionals.\n(v) Generating a Pugh Chart  Section 3.3.2—What is the extent and limitation of GPT-\n4V’s ability to generate Pugh charts for conceptual design evaluation?\n ● GPT-4V understands what a Pugh chart is, and can provide examples of the format-\nting, but often will not fill in the Pugh chart, or simply provide a “looks-like” Pugh \nchart given just a design task and design sketches. The model cites that it cannot fill \nin the Pugh chart without additional context about the designs, suggesting it may be \nable to if provided with more information.\n1 3\n288 Page 20 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\n4 System-level and detail design\nOverview and motivation After conceptual design, engineers move into the system-level \nand detail design portion of the product development process (Ulrich et al. 2020). During \nthis phase, engineers flesh out the specifics of the design as they select materials, develop \nCAD models, iterate to optimize the design, abide by constraints, and create engineering \ndrawings for subsequent manufacturing steps. For example, an experienced mechanical \nengineer, embarking on the design of a new lightweight bicycle frame, would necessarily \ngo through a material selection phase—to identify materials that are both strong and light—\na CAD generation phase—to develop the geometric details of the design—and a design \noptimization phase—to refine the design to best meet a set of objectives. These steps are \nvery common and critical to the detail design stage and inform the structure of this section.\nThe generation of system-level and detail designs draws upon many skill sets, including \nspatial reasoning, knowledge of specific CAD software programs, and physics-based prin -\nciples. Many of the tasks that engineers perform during this step of the product development \nprocess are inherently visual. They must consult material charts and graphs, read and create \nengineering drawings, develop 3D geometric models in CAD GUIs, and visually check for \ngeometric constraint violations. Given VLMs’ image-processing capabilities and emergent \ntechnical knowledge, we test, in the follow sections, the models’ abilities to support visual-\nbased detail design tasks.\n4.1 Material selection\nOftentimes, the selection of material comes early-on in the detail design phase, as material \nchoice informs both the design and the utilized manufacturing method. Material selection \nrequires balancing various constraints and requirements, such as material strength, stiff -\nness, cost, density, embodied energy, electrical resistivity, and thermal conductivity (Ashby \n2016). Choosing a material that meets an extensive list of requirements and constraints \noften requires cross-referencing multiple tables or charts, such as Ashby charts (Ashby \n2016). Ashby charts enable engineers to visually represent the trade-offs between various \nmaterial properties for different families of materials. Provided with these charts for dif -\nferent material properties, LLMs have the potential to condense material information and \nidentify materials that meet certain criteria. Several groups have explored GPT’s ability to \nassist with material considerations. In Saka et al. (2023), the authors used the GPT API to \nintegrate ChatGPT into the building information modeling process to assist with material \nselection for components of a building. In Makatura et al. (2024), the authors looked at \nGPT-4’s ability to propose a manufacturing process for a part based on material selected. In \nBuehler (2023), the authors trained a model, MeLM (Mechanics Language Model), which \nwas used for material-related tasks, like proposing microstructure designs that meet certain \nstress–strain responses. In this section, we conduct three independent experiments involving \nAshby charts and material selection. To analyze the consistency of responses, each experi -\nment is repeated three times.\nMethodology For the material selection experiments, we provide GPT-4V with various \nAshby charts, which are commonly used by engineers to evaluate trade-offs between differ-\nent materials. We conduct three different types of experiments: Ashby chart look-up, Ashby \nchart cross-referencing, and Ashby chart material selection for a beam. Each experiment \n1 3\nPage 21 of 75 288\nC. Picard et al.\ntype is repeated three times. One of the repetitions of each of the three experiment types \ncan be seen in Fig. 9 (look-up), Fig. 10 (cross-referencing), and Fig. 11 (material selection \nfor a beam).\nFor the look-up experiment, we provide the model a density vs. Young’s modulus Ashby \nchart. We ask the model to identify materials that have a density between 7–10 Mg\nm3  and a \nFig. 9 Ashby chart look-up: Identifying materials that meet stiffness and density constraints\n \n1 3\n288 Page 22 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nYoung’s Modulus greater than 100 GPa. The purpose of this experiment is to assess whether \nGPT-4V can perform a simple “look-up” of feasible materials from the chart.\nFor the cross-referencing experiment, we give GPT-4V two Ashby charts, one showing \ndensity vs. Young’s modulus and another showing density vs. strength. We then ask GPT-4V \nto cross-reference the two charts, identifying materials that have a density in between 1–3 \nMg\nm3 , a Young’s modulus between 0.01 and 0.1 GPa, and a strength of 3 MPa. The purpose \nof this experiment is to understand if GPT-4V can synthesize information from two material \ncharts together.\nFor the material selection for a beam experiment, we provide the model a density vs. \nYoung’s modulus Ashby chart. We ask GPT-4V to help us select a material for a hypo -\nthetical beam, given general requirements that the beam must be both stiff and light. The \npurpose of this experiment is to understand if GPT-4V can translate the general require -\nFig. 10 Ashby chart cross-referencing: Identifying materials that meet stiffness, strength, and density \nconstraints\n \n1 3\nPage 23 of 75 288\nC. Picard et al.\nments into material requirements and propose appropriate material families based on those \nrequirements.\nDiscussion: material selection  Overall, we conclude that GPT-4V performs well at \nidentifying broad material families that exhibit general properties (e.g. low density), but \nperforms less well when given specific requirements or constraints (e.g., density between \n1.0 and 3.0). This finding is illustrated by the results of the three experiments. The responses \nfrom all experiments and repetitions can be seen in Table 5. One repetition of each experi-\nment is displayed in-full in Figs. 9, 10, and 11. For the Ashby Chart Look-Up experiment \n(see Fig. 9), we would expect the correct answer—materials that have a density between 7 \nand 10 Mg\nm3  and that have a Young’s modulus greater than 100 GPa—to be steel, Ni-alloys, \nFig. 11 Application of Ashby charts to the material selection for a beam\n \n1 3\n288 Page 24 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nand Cu-alloys. (Zn-alloys and Mo-alloys lie on the border of the feasible region). For all \nthree repetitions, GPT-4V correctly answered that steels would be feasible materials. For \ntwo out of the three times, it also mentioned that Ni-alloys would meet the specified require-\nments. However, in all three iterations it also included materials in its answer—either Ti-\nalloys or WC-Co—that do not meet our specifications; Ti-alloys have a density less than 7 \nMg\nm3  and WC-Co materials have a density greater than 10 Mg\nm3 . In none of the iterations did \nGPT-4V mention Cu-alloys in its answer, although this material group meets both the stiff-\nness and density specifications.\nGPT-4V performed poorly in our Ashby Chart Cross-Referencing Experiment (see \nFig. 10 for the full response from one of the repetitions). The correct answers to the question \nwere the soft butyl and elastomer materials, which have densities between 1.0 to 3.0 Mg\nm3 , \nYoung’s moduli between 0.01 and 0.1, and a strength of 3 MPa. Across the three repetitions, \nGPT-4V never identified either of these materials as meeting our requirements. Overall, \nit tends to conclude that polymer foams, foams, polymers, or woods would be suitable \nchoices, but these materials do not meet our specifications. Polymer foams, for example, \ndon’t meet the density requirement; many polymer foams have densities between 0.1 and \n0.3 Mg\nm3 , suggesting that GPT-4V possibly confuses our 1.0–3.0 Mg\nm3  density specification \nwith this 0.1 to 0.3 range. It is important to note that for two of the three repetitions of this \nexperiment, GPT-4V was hesitant to provide an answer due to the “resolution” of the pro -\nvided images.\nOur Ashby Chart Look-Up and Cross-Referencing experiment reveals possible areas of \nimprovement needed in handling precise numerical data and complex information synthe -\nsis. The model’s struggle with accurately interpreting numerical constraints, as evidenced \nby these two experiments, highlights a shortfall in applying exact numerical ranges. Further-\nmore, its inability to effectively cross-reference and synthesize data from multiple sources \nunderscores a challenge in processing multi-dimensional information. This issue is particu-\nlarly pertinent in engineering where precision and multi-faceted data analysis are crucial.\nGPT-4V performs much better when asked to propose potential material families for a \nbeam that needs to be both lightweight and stiff (see Fig. 11). For all three experiment rep-\netitions, GPT-4V correctly translates the stiffness specification into a high Young’s modulus \nrequirement and the light-weight specification into a low-density requirement. For all tri -\nals, it correctly asserts that materials we would want to consider are towards the top left \nTable 5 Summarized results for the GPT-4V material selection experiments\n*For these responses, GPT-4V expressed concern about making definitive material choices based on the \nresolution of the provided Ashby charts\n1 3\nPage 25 of 75 288\nC. Picard et al.\nof the provided Ashby chart and proposes engineering composites and engineering alloys \n(and for two of the three repetitions, also proposes engineering ceramics and wood prod -\nucts). We conclude that while GPT-4V struggles to identify materials that meet specific \nnumerical requirements, it is much better at proposing material families that meet general \nspecifications.\nOverall, the use of GPT-4V in material selection in engineering design showcases its \npotential as a supportive tool in the preliminary stages of decision-making and as an edu -\ncational aid in materials science. Its ability to suggest material families based on general \nrequirements can streamline the initial phases of design, allowing engineers to focus on \nfiner details. This integration points towards a future where AI complements traditional \nengineering tools, enhancing the efficiency of design workflows. However, it also raises \nimportant ethical and practical considerations, such as over-reliance on VLM models with-\nout knowing their limitations and ensuring AI-generated recommendations of material align \nwith safety standards and environmental concerns. For example, the varying answers across \nthe repeats highlight the non-deterministic nature of ChatGPT’s interface, which engineers \nneed to consider.\nTransitioning from the exploration of GPT-4V’s capabilities in material selection, the \nresearch now shifts focus to another critical aspect of engineering design: VLM’s ability \nto interpret complex engineering drawings and contribute to the generation of Computer-\nAided Design (CAD) models.\n4.2 Engineering drawing analysis and CAD generation\nA critical step of the detailed design process is the generation of 3D models. Computer-\naided design (CAD) software enables the creation of detailed solid models, allowing engi -\nneers to encode precise part dimensions and assembly relations between parts (Nyemba \n2022). These CAD models pave the way for design for manufacturing, since detailed engi-\nneering drawings with manufacturing specifications are typically created from the 3D mod-\nels (Nyemba 2022). CAD models are also useful for the different ways in which designs \nand parts can be visualized (e.g. cross-sections, wireframe views, etc.), enabling engineers \nto easily consider different aspects of their design (Nyemba 2022). We hypothesized that \nGPT-4 with vision would be better able to assist with CAD generation and engineering \ndrawing analysis than GPT-4 since these two design forms—CAD models and engineering \ndrawings—are inherently visual mediums.\nWe gain inspiration from the work of researchers who have explored the potential of \nGPT to assist with converting text into CAD (Makatura et al. 2024; Nelson et al. 2023). For \nexample, Makatura et al. devoted a large section of their work to the exploration of GPT-4’s \nability to generate CAD designs from text. They looked at text to scripts that would generate \n2D designs (DXFs and SVGs), demonstrating relative success in the design of 2D pieces of \na cabinet. Another work then performed several case studies to illustrate GPT-4’s ability to \nconvert text into scripts for 3D designs, using both CSG-based CAD languages and sketch-\nbased CAD languages (Makatura et al. 2024). These experiments showcased mixed suc -\ncess, oftentimes requiring prompts to be engineered with specific function signatures. The \nauthors noted reasoning challenges, specifically when it came to spatial reasoning. They \nalso cited iterative ability as both a capability and a drawback of GPT-4, as they found the \nmodel was sometimes able to correct errors through continued chat iterations but that more \n1 3\n288 Page 26 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\niterations also led to limited memory of previous information in a chat. However, a key limi-\ntation of past work is that it relied on text-only LLMs, while CAD is inherently a task that \nhas significant visual aspects. In our study, we focus on evaluating the capabilities of VLMs.\nMethodology To assess GPT-4V’s ability to analyze engineering drawings and gener -\nate CAD, we utilized a seven-prompt experiment structure (carried out in the same context \nwindow). An example of a full experiment can be seen in Fig. 12. The first two prompts (P1 \nand P2) of each experiment assess GPT-4V’s ability to analyze engineering drawings. We \nFig. 12 Overview of the prompts and answers for one experiment of engineering drawing analysis and \nCAD generation using OpenSCAD. Only the three first CAD generation iterations are shown\n \n1 3\nPage 27 of 75 288\nC. Picard et al.\ntest the model on two aspects: 1) its ability to describe a part based on an engineering draw-\ning and 2) its ability to extract dimensions from an engineering drawing. We correct any \nresponses to these questions that the model answers incorrectly, so we can independently \nscore the next part of the experiment. In prompts three through seven (P3–P7), we evaluate \nthe model’s ability to generate a script that encodes the CAD of a part. We ask the model to \ndo this based on the previously provided engineering drawing, the previously extracted part \ndimensions, and a CAD scripting language that we specify. In this part of the experiment, \nwe score the model on the CAD that its script generates. If the CAD is not correct on the first \nattempt, we feed it back views of the generated CAD and ask it to iterate to fix any discrep-\nancies it sees between the generated CAD and the original engineering drawing. In this way, \nour feedback is much like the model’s built-in Code Interpreter, but instead for CAD. We \nrepeat this iterative process until five different CAD generation attempts have been made.\nIn total, we ran nine experiments, each with seven prompts (P1-P7) conducted sequen -\ntially in a single chat context. Three groups of three experiments used identical prompts \n(conducted for repetition), and the difference between the three groups of experiments is in \nthe CAD scripting language specified. We now further elaborate on the experiment structure \nand the method for scoring each experiment:\nThe prompts for these queries are identical across all nine experiments.\n ● Part description from an engineering drawing —Prompt 1 (P1). GPT-4V is given \nan engineering drawing of a block with a blind hole, as seen in Fig. 12, P1. We ask the \nmodel to provide a description of the part. We chose to use this block-with-blind-hole \npart as the subject of our experiments since it represents one of the most basic yet func-\ntional parts that can be created using CAD, necessitating only two sketches and basic \ncut/extrude operations. The drawing follows typical engineering drawing conventions \nand was created in an undergraduate-level engineering course.3 As such, it is a drawing \nthat we would expect undergraduate-level mechanical engineers to readily understand.\n – Scoring (1 point possible): We assign 1 point if GPT-4V correctly mentions that \nthe part is a “block with a hole” or “block with a blind hole.” Any mention of a \n“through” hole receives no points as it shows an incorrect understanding of the \nunderlying geometry.\n ● Dimension extraction from an engineering drawing —Prompt 2 (P2). We next ask \nGPT-4V to extract the dimensions shown in the engineering drawing, assigning them \nappropriate names. We specifically ask GPT-4V to not extrapolate to create dimensions \nthat are not explicitly called out in the drawing.\n – Scoring (10 points possible): 1 point is awarded for each of the five numbers \nshown on the drawing (8.00, 5.00, 12.00, 4.00, and ø5.00) that GPT-4V success -\nfully extracts. Another point is awarded for each of the five dimensions to which it \nassigns an appropriate name. For the block dimensions—8.00, 5.00, and 12.00—we \naccept any assignment of [length, width, height] or [depth, width, height] to the \nthree dimensions since the assignment of these labels depends on how the block is \n3 The engineering drawing of the block with blind hole shown in Fig. 4.2.1 was created as an assignment for \nthe “ENME272: Introduction to Computer Aided Design” course at the University of Maryland.\n1 3\n288 Page 28 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\noriented. For the 4.00 dimension and the ø5.00 dimension, we expect labels of “hole \ndepth” and “hole diameter” respectively, or equivalent names. For any dimensions \nGPT-4V lists beyond those shown in the drawing, we subtract 1 point for not fol -\nlowing instructions.\nThe prompts for the following queries vary across the nine experiments, as indicated below.\n ● CAD Generation 1—Prompt 3 (P3). Continuing in the same context window where P2 \nleft off, we correct any dimensions that GPT-4V extracted from the drawing incorrectly, \nand we then ask GPT-4V to generate a CAD script of the block-with-hole part based on \nthe engineering drawing provided in P1 and the dimensions it extracted in P2. For three \nof the experiments (experiments 1–3), we ask GPT-4V to do this using the CadQuery \nscripting language; for another three experiments (experiments 4–6), we ask GPT-4V to \ndo this with a different scripting language, FeatureScript; and for the last three experi -\nments (experiments 7–9), we ask GPT-4V to use the CAD scripting language Open -\nSCAD. Note that each language offers unique features and advantages:\n – CadQuery: An open-source CAD scripting module built-in Python, CadQuery is \neasy to use for those already familiar with Python.\n – FeatureScript: The scripting language of the CAD software Onshape—a free cloud-\nbased CAD software—FeatureScript is integrated into Onshape, enabling both tra -\nditional CAD modeling and custom, script-defined, parametric modeling.\n – OpenSCAD: Another open-source CAD scripting language built in C++, Open -\nSCAD is integrated into the CAD software FreeCAD and provides granular control \nover a model.\n By utilizing these three scripting languages, our research aims to comprehensively assess \nGPT-4V’s ability to adapt to different CAD scripting environments and to evaluate its ver -\nsatility in translating engineering drawings into functional CAD models.\n – Scoring (6 points possible): We assign 1 point if the generated script has no errors \nwhen run. We award 1 point for each of the following features that the generated \nCAD possesses: the block has correct dimensions; the CAD has a hole on the larg -\nest block face; the hole is centered on a face; the hole has the correct depth; and the \nhole has the correct diameter. We subtract a point for each extra, incorrect feature \nthat is present in the generated CAD (e.g. a second hole, a cutout in the block, etc.).\n ● CAD Generation 2–5 : Prompts 4–7 (P4–P7). If the CAD generated by the previous \nprompt has a syntax error when the code is run, we provide it to GPT-4V and ask it to \nfix the script. If the script runs but the generated CAD doesn’t have a perfect score, we \nask GPT-4V to correct discrepancies between the generated CAD and the engineering \ndrawing. We ask it to do this by providing it with an image with four views of the CAD \ngenerated from the previously provided script (see 4.2.1 P4 for an example). These \nviews show the CAD with hidden lines and coordinate systems visible for each view. \nIf the CAD is still not receiving a perfect score by P7 (CAD Generation 5), we end the \nexperiment.\n1 3\nPage 29 of 75 288\nC. Picard et al.\n – Scoring (6 points possible for each prompt, P4–P7): Scoring for these prompts is \nidentical to the scoring for P3.\nPlease note that the scoring system used here primarily serves as an illustrative example to \nsimplify the understanding of an aggregate score for readers. Practitioners may adjust the \nweights assigned to each question type based on specific requirements. We have made the \nraw data for each response available, facilitating benchmarking and allowing for flexibility \nin evaluating performance when varying the importance of different factors.\nDiscussion: engineering drawing analysis and CAD generation  Based on the results \nfrom P1 and P2 (see Table 6), which quantify GPT-4V’s ability to analyze an engineering \ndrawing, we conclude that the model generally understands the content in the drawing, but \nstruggles with interpreting the drawing’s details. For P1 in eight out of the nine experi -\nments, GPT-4V incorrectly describes the part as a block with a hole “through” it. While it \nunderstands the part generally—a block with a hole—it does not pick up on the notation \nin the drawing that indicates the hole is blind rather than through. In the one experiment \n(experiment 4) where it received a correct score for the part description, it called the part a \n“rectangular block with a cylindrical hole or recess in it” and a “generic block with a hole.” \nWhile this qualifies as an accurate description, it does not demonstrate whether GPT-4V \nrecognizes the blind hole in the drawing or not.\nAfter being told that the part in question is a block with a blind hole (P2), GPT-4V is gen-\nerally good at extracting dimensions from the drawing, receiving a perfect P2 score for six \nout of nine experiments (see Table 6). Across all nine experiments, GPT-4V always extracts \nall five dimensions from the drawing. Two-thirds of the time it assigns all of the dimensions \nappropriate labels. It has the most trouble naming the hole depth dimension: in experiment \n1 it calls it the “height of the block (from the bottom right view)” and in experiment 9 it \ncalls it the “width of the block.” The relative difficulty in understanding what the 4.0 dimen-\nsion represents in the drawing is consistent with its initial lack of understanding (in P1) that \nthe drawing represents a block with a blind hole. It is also interesting to note that GPT-4V \nis inconsistent in the labels it chooses for the three block dimensions—varying between \nheight/width/length, depth/height/width, height/depth/width, and depth/height/length—per-\nhaps reflecting a lack of consistent spatial reasoning or a lack of consistent norms used to \nlabel dimensions.\nFrom the responses to P3-P7, we evaluate GPT-4V’s ability to generate CAD using CAD \nscripting languages. We observe, see Fig. 13, that GPT-4V rarely generates accurate CAD \nTable 6 Summarized results from Sect. 4.2\nExperiment name Exp 1 Exp 2 Exp 3 Exp 4 Exp 5 Exp 6 Exp 7 Exp 8 Exp 9\n(P1) Part Description 0/1 0/1 0/1 1/1 0/1 0/1 0/1 0/1 0/1\n8.0 Dim. extraction and label 2 2 2 2 2 2 2 2 1a\n5.0 Dim. Extraction and Label 2 2 2 2 2 2 2 2 2\n12.0 Dim. extraction and label 2 2 2 2 2 2 2 2 2\nø5.0 Dim. extraction 2 2 2 2 2 2 2 2 2\n4.0 Dim. extraction and label 1a 2 2 2 2 2 2 2 1a\nAdditional dim. extraction 0 − 1 0 0 0 0 0 0 0\n(P2) Dimension extraction 9/10 9/10 10/10 10/10 10/10 10/10 10/10 10/10 8/10\naThese entries which have a score of 1 always correspond with successful dimension extraction but \nincorrect label assignment\n1 3\n288 Page 30 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\non the first attempt (P3), and CAD iterations (P4–P7) do not improve the CAD. For P3, \nonly one out of the nine experiments (experiment 3, using CadQuery) leads to correctly \ngenerated CAD on the first attempt. For FeatureScript, GPT-4V cannot get out of syntax \nand function implementation errors for all five CAD generation iterations. The most com -\nmon issue for P3 is not putting the hole on the correct face. We noticed this is because the \nhole extrusion direction is always linked with the dimension to which GPT-4V assigns the \n“height” label. The 5.0 block dimension is only assigned the height label three times, one of \nwhich is the sole experiment (experiment 3) where perfect score CAD is generated in P3.\nFrom the results from P4-P7, we conclude that visual feedback of the generated design \nfrom the previous prompt does not improve GPT-4V’s CAD scripting ability. In fact, if \nFig. 13 Results of the CAD Generation prompts, CAD Generation 1—CAD Generation 5 (P3–P7). Ex -\nperiments 1–3 were generated using CadQuery and Experiments 7–9 were generated using OpenSCAD. \nExperiment 3 was the only experiment that generated perfect score CAD on the first iteration. Experi -\nments 4–6, the FeatureScript experiments, are not shown here, since they had persistent code errors and \nnever generated viable CAD\n \n1 3\nPage 31 of 75 288\nC. Picard et al.\nGPT-4V generated incorrect CAD in P3, P4-P7 will never fully rectify the problematic \nCAD, and CAD Generation 5 (P7) will have a worse score than CAD Generation 1 (P3). \nA visualization of this finding can be seen in Fig. 13. For the CadQuery and OpenSCAD \nexperiments, a general reduction in CAD score occurs in CAD Generation 3 (P5), where \nGPT-4V consistently forgets the dimensions that it extracted in the original engineering \ndrawing.\nIn summary, we find that GPT-4V can pick up many aspects of the provided engineering \ndrawing (e.g. general part depicted, many of the dimensions shown, etc.), but can struggle \nwhen it comes to understanding the details (e.g. recognizing the through hole, labeling the \nthrough hole dimension, etc.). GPT-4V performs poorly when it comes to CAD generation, \nand we demonstrate that our attempts at visual, iterative improvements are unsuccessful. \nThese findings imply that while GPT-4V can offer some assistance in preliminary design \ntasks, its current capabilities are not yet sufficient for detailed, precision-driven CAD work. \nBuilding on the findings from the evaluation of GPT-4V , future research should focus on \nenhancing the model’s ability to interpret and process detailed engineering information. \nAnother critical area for development is in CAD generation, where GPT-4V currently shows \nlimitations. Future work should explore methods to improve the model’s accuracy and effi-\nciency in creating detailed CAD models, perhaps through advanced training techniques or \nintegration with specialized CAD software. Additionally, there’s a need to investigate how \niterative feedback mechanisms can be better utilized by GPT-4V to make meaningful cor -\nrections and improvements in successive design iterations. Addressing these areas will be \ncrucial in expanding the applicability of GPT-4V and similar VLM tools in more advanced \nand precision-dependent stages of the engineering design process.\nIn tandem with CAD generation and engineering drawing creation, engineers frequently \naim to improve the design of the part, using iterative optimization approaches (e.g., using \ncommercial tools such as nTop or SOLIDWORKS Simulation software). One of the com -\nmonly used iterative optimization approaches is structural topology optimization, which can \nhelp a designer reduce material usage while meeting some design requirements. In the next \nsection, we turn to GPT-4V’s ability to assist with topology optimization.\n4.3 Topology optimization analysis\nStructural topology optimization (TO) is a numerical approach for optimizing material dis-\ntribution in structures under specific constraints, aiming for efficient material use while \nmaintaining performance. The SIMP (Solid Isotropic Material with Penalization) method, a \ndominant approach in TO, models material properties using a density field, adjusting it itera-\ntively to optimize design and adhere to stress or deformation constraints (Bendsøe 1989). \nIn mechanical systems, the minimum compliance problem  focuses on finding a material \ndensity distribution, x ∈ Rn, to minimize deformation under forces and boundary condi -\ntions (Bendsøe and Kikuchi 1988). The problem is formulated as:\n \nmin\nx∈[0,1]\nc(x)= FT U(x)\ns.t. v(x)= vT x < ¯v\n (1)\n1 3\n288 Page 32 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nHere, the objective is to minimize the compliance c(x), with F being the external load forces, \nU(x) the displacements of nodes and solution of the equilibrium equation K(x)U(x)= F, \nand K(x) the stiffness matrix which depends on the material distribution. The constraints \ninclude maintaining the volume fraction v(x) below a specified limit ¯v and ensuring the \ndesign variables x remain within the bounds of 0 and 1, allowing for a gradient of material \ndistribution from void to solid (Bendsøe and Kikuchi 1988; Sigmund and Maute 2013).\nOptimal topologies are, however, often challenging to analyze for human experts. The \ntopologies that result from the optimization process may be mathematically optimal, but \npresent practical challenges in manufacturability and analysis and can be non-intuitive for \nhuman designers. We consider the analysis of topology optimization images as a test case to \nevaluate the use of visual-language models to assist humans in interpreting complex topolo-\ngies. Note, in these experiments we do not ask the VLM to perform topology optimization, \nbut rather to determine the inputs to or interpret the results of topology optimizations.\n4.3.1 Volume fraction estimation\nIn this experiment, we task the model with calculating the volume fraction from an optimized \ntopology depicted in an image. This involves measuring the proportion of black material in \nthe given domain and determining the relevant ratio. The challenge is initially approached \nusing GPT-4V’s visual analysis capabilities alone. Following this, GPT-4V employs its code \ninterpretation abilities to address the task. We aim to obtain an accurate answer within a 5% \nerror threshold.\nMethodology In this experiment, we provide a VLM with an image of an optimized \ntopology, and prompt the VLM to estimate the volume fraction of the structure. We provide \nthe following contextual information: “Consider that white means the absence of material \nand the initial domain is a square of size 256x256,” as shown in the prompt in Fig. 14. We \nperform this experiment for 100 optimized designs with no floating material as well as 50 \nnon-optimized designs which have floating material. We run the experiment with these dif-\nferent prompting strategies, as seen in Table 7: without domain-specific prompting (w/o \nExpertise), incorporating domain expertise in the prompt (w/ Expertise, i.e. “you are an \nexpert engineer in the field of topology optimization”), and enriching the expert prompt \nwith Chain-of-Thoughts rationale (w/ CoT, i.e. “answer using a step-by-step rationale”). \nThe quantitative results are shown in Table 7.\nWe provide quantitative experiments in Table 7, where the aim is to estimate V olume \nFraction (VF) and the presence of Floating Material (FM). Table 7 presents the error metrics \nacross 100 optimized topologies (first and second columns) and a balanced set of 50 opti -\nmized alongside 50 un-optimized topologies with randomized floating material (third col -\numn). It focuses on V olume Fraction Error (VFE) for assessing the accuracy in estimating \nmaterial usage and Floating Material Error (FME) for identifying disconnected components \nin designs, with a baseline error expectation set at 50% for random chance. We can compare \nour three prompting strategies (w/o Expertise, w/ Expertise, and w/ CoT). Integrating engi-\nneering expertise into the model’s prompting strategy (w/ Expertise) improves the results, \nas seen by lower VFE than w/o Expertise in Table 7. However, the addition of step-by-step \nexplanations (CoT) yields only marginal improvements in accuracy. Overall, the capacity \nto estimate V olume Fraction and detect Floating Material is relatively poor. This observa -\ntion underlines the limitations of relying solely on a vision encoder for precise topology \n1 3\nPage 33 of 75 288\nC. Picard et al.\nFig. 14 V olume fraction estimation\n \n1 3\n288 Page 34 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\noptimization tasks, such as estimating V olume Fraction and detecting Floating Material. \nIt highlights the necessity of employing external analytical tools, like a code interpreter or \ndedicated vision modules, to achieve more accurate and reliable outcomes.\nDiscussion: volume fraction estimation\nIn Fig. 14, we challenge the model to quantitatively estimate the volume fraction of an \noptimized topology. The model accurately defines the task as “ evaluating the percentage \nof the black area (material presence) relative to the total area of the square .” However, \nits initial attempts to count black pixels and calculate the material percentage yield highly \ninaccurate results. This inconsistency persists across multiple trials, each providing different \nand incorrect answers.\nTo address these limitations, we introduce a code interpreter (third prompt in Fig. 14), \nenabling the model to use a Python script for the estimation. This approach significantly \nimproves accuracy, bringing the estimate close to the target within a reasonable margin of \nerror. This experiment highlights two key insights: Firstly, it underscores the limitations \nof the vision encoder (at least for the version of GPT-4V used in this study) in handling \nprecise quantitative assessments based on images. Secondly, it demonstrates the effective -\nness of integrating coding tools in overcoming these limitations, showcasing the synergistic \npotential of combining AI’s interpretive capabilities with precise, code-based calculations \nfor more accurate and reliable results.\n4.3.2 Technical caption generation and analysis\nMethodology In this experiment, we task a VLM with captioning a technical diagram using \na basic prompt, with the diagram inspired by the experiment in Fig. 7 from Woldseth et al. \n(2022). Initially, we employ a generic prompt for captioning. Then, we enhance the task by \nincorporating details about the system’s technical expertise, providing a more in-depth and \nknowledgeable description of the diagram. These can be seen in Fig. 15. We would like to \nsee that the model understands that small variations of constraint configurations (in this case \nload direction) can greatly change the optimized topology.\nAdditionally, we conducted a quantitative experiment to assess how effectively a VLM \nprocesses textual information within images, such as descriptive captions (Table 8). We \ncreated ten problem setups modeled after the example in Fig. 15, varying aspects of the \nimage such as the number of elements in the x and y axes, volume fraction, load magnitude, \nand angle of application, along with the positions and directions of loads and the types of \nboundary conditions.\nOur focus was to evaluate GPT-4V’s ability to extract specific details from captions–\ninformation that exists solely as text within the image. We also tested the model’s accuracy \nin identifying the positions and directions of loads, as well as the types and placements of \nTable 7 V olume Fraction Error (VFE) is the percent error when predicting the volume fraction of an opti -\nmized design\n↓ VFE (%) ↓ FME (%) ↓ FME (50/50) (%)\nw/o Expertise 45.71 ± 2.45 47.15 ± 1.11 42.08 ± 2.83\nw/ Expertise 44.13 ± 1.11 44.71 ± 1.15 39.75 ± 2.54\nw/ CoT 43.62 ± 0.77 43.90 ± 0.23 39.50 ± 2.49\nFloating Material Error is for the classification task of determining if a design has floating material or not \n(50% error would be a random guess)\n1 3\nPage 35 of 75 288\nC. Picard et al.\nFig. 15 Technical captioning of TO \nimages\n \n1 3\n288 Page 36 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nboundary conditions. Additionally, we examined the model’s capability to generate a func-\ntional topology optimization Python script based on the given data.\nThe findings indicate that GPT-4V performs well in interpreting and retrieving data from \nimage captions. However, it shows limitations in accurately locating loads and boundary \nconditions. The Python scripts generated by the model were structurally sound but typically \nincluded minor mistakes in indexing and variable names, reflecting areas needing improve-\nment in the model’s programming language generation capabilities.\nTable 8 presents the results from testing the GPT-4V model on ten different problem \nsetups, which are modifications of the example shown in Fig. 15. These modifications \ninclude variations in load positions and directions, domain sizes, volume fractions, and \nload application angles (represented as ϕ). The table evaluates the model’s performance in \nthree critical areas: (i) Caption Analysis. We evaluate how accurately the model interprets \ntext within images, such as captions. (ii) Load and Boundary Positioning. We evaluate the \nmodel’s precision in identifying the correct positions for loads and boundary conditions. \n(iii) Code Validation for Topology Optimization. We evaluate the effectiveness in generating \na usable Python script for minimum compliance topology optimization, highlighting issues \nlike indexing errors or incorrect variable naming which prevent the script from running. \nEach entry in the table is scored as 1 (correct answer) or 0 (wrong answer, partially correct, \nor non-functional script), summarizing the model’s capability to handle textual and spatial \ninformation in images under varying complexity and setup parameters. nelx: number of \nelements in x-direction; nely: number of elements in y-direction; | F|: force module; VF: \nvolume fraction; R: filter radius; ϕ: angle of application for the load.\nDiscussion: technical caption generation and analysis\nIn Fig. 15, we evaluate the model’s ability to interpret a complex diagram. The model \ndelivers a comprehensive and accurate analysis, adeptly linking forces, their angles of \napplication, and optimized topology. It accurately identifies the image as “ a structural or \nmechanical analysis, demonstrating how the structure or material responds to varying load \nangles.” This insight into how loading direction affects topology is a correct deduction of a \nTable 8 This table presents whether the model was successful (a score of 1) or not (a score of 0) at caption \nanalysis, loads and boundary positioning, and code validation when analyzing a topology optimization image\nProblem Caption analysis Position Validation\nnelx nely |F| VF R ϕ Load BC Code Runs\nP 1 1 1 1 1 1 1 0 0 0\nP 2 1 1 1 1 1 1 0 0 0\nP 3 1 1 1 1 1 1 0 1 0\nP 4 1 1 1 1 1 1 0 0 0\nP 5 1 1 1 1 1 1 1 1 0\nP 6 1 1 1 1 1 1 1 1 0\nP 7 1 1 1 1 1 1 1 1 0\nP 8 1 1 1 1 1 1 0 0 0\nP 9 1 1 1 1 1 1 1 0 0\nP 10 1 1 1 1 1 1 0 0 0\nAvg 1.0 1.0 1.0 1.0 1.0 1.0 0.4 0.4 0.0\nFor caption analysis, the terms nelx and nely are the number of elements in x-direction and y-direction \nrespectively;| F| is the force module; VF is the volume fraction; R is the filter radius; ϕ is the angle of \napplication for the load\n1 3\nPage 37 of 75 288\nC. Picard et al.\ndifficult physical problem, showcasing the model’s proficiency in understanding boundary \nconditions, loads, and their impact on structures.\nHowever, the model encounters difficulty with the boundary conditions at the bottom \ncenter and right of the diagram, mistakenly interpreting them as “ two hanging weights ,” \nwhich is an incorrect assessment of the boundary sketch. This misinterpretation is unex -\npected, particularly given the overall high-quality response and the accurate grasp of the \nproblem’s essence.\nFurther refining the prompt to emphasize engineering concepts, the model again provides \na largely accurate response, delving deeper into topics like loads, volume fraction, and fil -\ntering radius. Yet, it repeats the same error concerning the boundary conditions, suggest -\ning “Two external point loads, represented by weights, are applied at the bottom corners.” \nThis persistent mistake indicates a gap in the model’s global understanding of the scenario, \nrevealing a vulnerability to misconceptions in specific contexts.\n4.3.3 Invalid design\nMethodology The task involves identifying invalid designs, specifically floating materi -\nals, based on a given prompt. The objective is for the model to recognize, independently \nand without prior information, the presence of a disconnected component within a low-\nresolution design (64 × 64). Following this recognition, the model is expected to assess \nthe design’s overall validity and the quality of the low-resolution grid. Lastly, we test if the \nmodel is capable of suggesting potential improvements to rectify the identified issues.\nDiscussion: invalid design In Fig. 16, we task the model with identifying the presence \nof floating material in a design, specifically a detached triangle in the top right corner. The \nmodel accurately recognizes the issue as a result of a topology optimization process, cor -\nrectly noting the disconnection of the “ isolated triangular shape from the main structure ” \nin the top right.\nWhen queried about the structure’s validity, the model identifies the floating material but \nits response lacks full clarity on the implications of such a flaw. While it correctly points out \nthat “this isolated feature might be problematic to manufacture, and its disconnected nature \nmight render it ineffective in a load-bearing role ,” it fails to emphatically state that a dis -\nconnected component invariably compromises structural integrity and manufacturability. In \nTable 7, we present a quantitative analysis of our experiment on detecting Floating Material \nusing the vision encoder, illustrating the complexities and challenges inherent in accurately \nidentifying such presence or absence of small disconnected components in a broad range \nof cases.\nRegarding the design’s optimization objectives, the model suggests a focus on “minimal \nmaterial usage.” This is a common requirement in topology optimization, but it oversimpli-\nfies the broader range of performance requirements typically involved in such processes.\nWhen asked about improving the design, the model sensibly proposes “ integrating the \nisolated triangle into the main structure or removing it if it lacks functional benefit .” This \nis a valid solution to address the floating material issue. However, its subsequent recom -\nmendation to “ re-evaluate boundary conditions and load cases ” as a method to eliminate \ndisconnected components is somewhat misguided. The more appropriate approach would \ninvolve refined optimization strategies and post-processing techniques.\n1 3\n288 Page 38 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nFig. 16 Invalid design \nidentification\n \n1 3\nPage 39 of 75 288\nC. Picard et al.\nIn summary, while GPT-4V effectively identifies floating material and offers viable \nsolutions, it falls short of fully understanding the criticality of disconnected components. \nFloating material or disconnected parts in a topology-optimized design invariably render \nit structurally unsound from an engineering point of view or unmanufacturable without \nfurther optimization or processing.\n4.4 Summary\nIn order to assess GPT-4V’s performance in select detail design tasks, we performed evalua-\ntions on material selection using an Ashby chart, on engineering drawing analysis and CAD \ngeneration, and on supporting the understanding and analysis of topology optimization. Our \nfindings are discussed below. \n(i) Material selection Section 4.1—Can GPT-4V effectively assist in material selection \nbased on property charts and design requirements?\n ● We find that GPT-4V can be helpful at pointing out material families that meet \ngeneral specifications, but that it struggles to identify materials that match specific \nnumerical requirements.\n(ii) Engineering drawing analysis and CAD generation  Section 4.2—How accurately \ncan GPT-4V extract and interpret information from engineering drawings? What is \nGPT-4V’s proficiency in generating and iteratively improving CAD scripts from engi-\nneering drawings?\n ● Only 11% of the time was GPT-4V successful in describing a block with a blind hole \npart. Once told it was a block with a blind hole, GPT-4V was successful 67% of the \ntime in extracting all dimensions from the engineering drawing and assigning the \ndimensions appropriate names.\n ● Preliminary findings suggest that GPT-4V struggles with CAD generation of the \nblock-with-blind-hole part, as it only succeeded once in nine attempts to generate \ncorrect CAD on the first try. Also, its iterative ability for CAD correction appears \nlimited, as it doesn’t successfully correct incorrect CAD in subsequent iterations.\n(iii) Topology optimization analysis Section 4.3—Can VLMs properly analyze aspects of \nan optimized topology, such as the volume fraction and the presence of floating mate -\nrial? Furthermore, can VLMs interpret images with technical captions to identify the \ncorrect positions for loads and boundary conditions?\n ● While GPT-4V correctly defines the concepts of volume fraction and floating mate-\nrials in terms of topology optimization, Table 7 shows that GPT-4V has high error in \nestimating volume fraction or identifying floating material from images.\n ● In terms of analyzing images and captions to identify the number and location of \nelements like forces and boundary conditions, Table 8 shows that GPT-4V could \nconsistently extract values from captions, however, could only identify the position \n1 3\n288 Page 40 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nof loads and boundary conditions from images 40% of the time. Lastly, GPT-4V \ncould never generate executable topology optimization code.\nWe highlight the strengths and weaknesses of GPT-4V’s response by marking the more \nrelevant and high-quality sections in green, and the incorrect, out-of-context, or low-quality \nparts in brown.\n5 Manufacturing and inspection\nOverview and motivation  Here we focus on assessing the performance of GPT-4V in \nmanufacturing and inspection-related tasks. Our motivation relies on the visual cues that \nengineers often use to understand the practical aspects of manufacturing complex geometric \nartifacts. This multimodal information requires expertise in understanding images as well as \nmanufacturing knowledge. As GPT-4V shows potential for task-specific image analysis, we \nevaluate its potential for manufacturing and inspection. The field of manufacturing is broad \nand discussing the complete potential of multimodal LLMs for all manufacturing tasks is \nout of the scope of our work. To this end, we focus on selective manufacturing tasks that can \nprovide useful insights in assessing the capabilities of these multimodal LLMs. Specifically, \nwe focus on design for manufacturing (DfM) and post-manufacturing inspection tasks. Both \nof these topics are critical for manufacturing applications in industry and demand extensive \ndomain-specific knowledge. We draw particular attention to understanding the manufac -\nturability of 3D CAD models only from images. Note that manufacturability traditionally \nrefers to the relative ease with which a part can be manufactured (Budinoff 2019; Budinoff  \nand McMains 2021; Yang and Zhao 2015). Ensuring the manufacturability of a new part is \na major challenge and requires careful analysis and expertise. The potential of an automated \ntool for this purpose would increase manufacturing productivity by a large margin. Multi -\nmodal LLMs may help industries build next-generation tools for automating these types of \ntasks. Our analysis can be thought of as an early evaluation of multimodal LLMs and their \nmanufacturing knowledge and reasoning. For brevity, we divide the Design for Manufactur-\ning section into two parts: additive and subtractive manufacturing. Based on existing litera-\nture, we query GPT-4V with images of 3D CAD models and assess its manufacturability \nresponse against the ground truth.\n5.1 Design for manufacturing\nDesign for manufacturing (DfM) is a popular concept that studies the manufacturability \nof an engineering design (Webb 2008). The DfM field is broad, as manufacturability is \ndependent on the materials used, the specific manufacturing method employed (e.g. addi -\ntive, subtractive, etc.), and the particular tools utilized for manufacturing (e.g. which type of \n3D printer). We explore GPT-4V’s ability to assist with DfM for two popular manufacturing \nmethods: additive and subtractive.\n1 3\nPage 41 of 75 288\nC. Picard et al.\n5.1.1 Design for additive manufacturing (DfAM)\nAdditive manufacturing (AM) has become increasingly popular as a fabrication method \nin recent years (Attaran 2017). AM first became popular because of its usefulness in rapid \nprototyping, but it is also utilized for low quantities of design-varying parts in aerospace \nand automotive component manufacturing (Attaran 2017). Design constraints for AM vary \nconsiderably by the additive system used, and assessing manufacturability of a design often \nrequires some experimentation. However, through manufacturing experience, engineers and \nmachinists often develop manufacturing guidelines or rules for designing a part to be manu-\nfactured with a specific process. It would be challenging to quantitatively assess GPT-4V’s \nability to predict the 3D-printability of part, because just as understanding of manufactur -\nability varies from person to person, it would vary from person to model. However, we can \nprovide the model with a set of unambiguous rules pertaining to 3D printability and ask the \nmodel to assess the printability of a part based on those rules. Hubs, a ProtoLabs company \nthat offers on-demand manufacturing, created a chart, entitled “Design rules for 3D Print -\ning,” encoding common design rules for AM based on printer type (see Fig. 19) (Hubs \n2023). There are ten specific design rules for FDM-printed parts. For example, one rule \nstates that supported walls for an FDM printer can have a minimum thickness of 0.8 mm. \nThese rules are heuristics and exceptions can be found, but the chart enables us to assess \nGPT-4V’s ability to apply common fabrication rules to a design.\nMethodology We assess GPT-4V’s ability to understand and apply AM design rules by \nasking the model to predict success in 3D-printing various designs. For this task, we cre -\nated a set of 20 designs, split into two sets: one set of problematic designs and another set \nof manufacturable designs, see Fig. 17. For each of the ten design rules in The Hubs chart \nthat pertain to FDM manufacturing, we created the ten problematic designs, each of which \nviolates one of the ten rules. The other ten designs comprising the manufacturable designs \nset are similar to problematic designs but actually pass all of the FDM rules in The Hubs \nchart. To confirm the intended manufacturability of the ten designs in the manufacturable \nset, we 3D-printed them using a Carbon X1 Bambu printer. All ten designs were printed \nsuccessfully as shown in Fig. 18.\nWe carried out 20 queries, each in a new context window and each corresponding with \none of the 20 designs. For each query, we provided GPT-4V with the chart of the design \nrules and a dimensioned image—one of the twenty images shown in Fig. 17—of the design \nwe desired to print. We then asked GPT-4V , based on the provided design rules, to predict \nthe success of 3D printing the part using an FDM printer. We asked the model to point to the \nspecific design rule(s) violated if it believed the part would not print successfully. Sample \nqueries can be seen in Fig. 19. To check repeatability, each of these queries was repeated \nthree times for a total of 60 queries. We scored each response as follows: \n1. Manufacturable? (max score 1): If GPT-4V correctly answered if the part was manu-\nfacturable or not, we assigned a score of 1, otherwise, 0.\n2. Correct rule (max score 1):  This scoring metric is only applicable to designs in \nthe problematic design set. If the rule that was in violation was named in GPT-4V’s \nresponse, we assigned a score of 1, otherwise, 0.\n3. # Incorrect rules (max score 0):  This scoring metric is only applicable to designs in \nthe problematic design set. The number of rules GPT-4V mentioned that the model \n1 3\n288 Page 42 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nbelieved were violated, but which were truly not violated, corresponds with the nega -\ntive value of this score. For example, if three rules were mentioned by GPT-4V which \nwere not violated by the design, the score for this metric would be -3.\nA summary of all our results can be seen in Table 9.\n5.1.2 Design for subtractive manfuacturing\nSubtractive manufacturing is the most widely used manufacturing technology in the indus-\ntries for manufacturing complex parts. This design process requires careful attention to the \nFig. 18 Parts from Fig. 17 (K–T) \nthat we 3D-printed using a Car-\nbon X1 Bambu printer\n \nFig. 17 (A–J) Ten problematic designs, where each design violates one of the FDM AM rules on The \nHub’s chart for “Design rules for 3D printing.” The specific rule violated is noted below each design. \n(K–T) Ten manufacturable designs, each based on one of the problematic designs\n \n1 3\nPage 43 of 75 288\nC. Picard et al.\nmanufacturability of a part and typically this process is iterative. This is particularly chal -\nlenging for parts with interacting features (Gao and Shah 1998). Unfortunately, there is a \nvery limited number of datasets for this task in the literature. Recently, deep learning-based \napproaches have been implemented to identify machining features using synthetic CAD \ndatasets (Cao et al. 2020; Zhang et al. 2018). These datasets are created using a curated set \nof design principles. To this end, we utilize the MFCAD dataset (Cao et al. 2020) to query \nGPT-4V for manufacturing feature recognition from the image of a CAD model.\nFig. 19 Example prompts and answers for the design for additive manufacturing\n \n1 3\n288 Page 44 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nTable 9 Scores achieved by GPT-4V on the design for additive manufacturing experiments across three trials\nDesign # Manufacturable? Correct Rule # Incorrect Rules Score\nTrial # 1 2 3 1 2 3 1 2 3 1 2 3\nDesign A 1 1 1 0 0 0 − 4 − 3 − 5 − 3 − 2 − 4\nDesign B 1 1 1 1 0 1 − 1 − 2 − 2 1 − 2 0\nDesign C 1 1 1 1 1 1 − 2 − 1 − 3 0 1 − 1\nDesign D 1 1 1 0 0 0 − 3 − 3 − 3 − 2 − 2 − 2\nDesign E 1 1 1 1 1 1 − 1 − 1 − 2 1 1 0\nDesign F 1 1 1 1 1 1 − 1 − 1 − 1 1 1 1\nDesign G 1 1 1 1 1 1 − 2 − 4 − 2 0 − 2 0\nDesign H 1 1 1 0 0 0 − 2 − 1 − 3 − 1 0 − 2\nDesign I 1 1 1 0 0 0 − 2 − 3 − 2 − 1 − 2 − 1\nDesign J 1 1 1 0 0 0 − 1 − 2 − 2 0 − 1 − 1\nScores for Designs K-T are not shown in the above table, since GPT-4V always (across all three trials) incorrectly predicted those designs to be not manufacturable, even \nthough they came from the manufacturable set. As such, the Manufacturable?  score for Designs K-T for all trials is 0\n1 3\nPage 45 of 75 288\nC. Picard et al.\nMethodology We perform a quantitative study based on multiple queries to GPT-4V . We \nrandomly pick 50 samples from the MFCAD dataset and create the images of each CAD \nmodel. Each of these images corresponds to a ground truth that assigns machining features \nto each surface of the CAD model. In general, there are 15 possible machining features that \nwe test in all of the experiments which excludes the stock material block. The list of machin-\ning features is the following: rectangular through slot, triangular through slot, rectangular \npassage, triangular passage, 6 sided passage, rectangular through step, 2 sided through step, \nslanted through step, rectangular blind step, triangular blind step, rectangular blind slot, \nrectangular pocket, triangular pocket, 6 sided pocket, chamfer. We query GPT-4V with each \nof these images and ask for the machining features that are present in the design. First, we \nprovide an initial prompt to focus on design for manufacturing. Next, we query GPT-4V \nabout each of the images, as shown in Fig. 20, sequentially. Figure 21 shows two example \nprompts and the corresponding responses from GPT-4V .\nThe dataset used in this experiment is provided with this document as an open-source \nsmall-scale evaluation dataset for vision-language LLMs. This design for the subtractive \nmanufacturing dataset is mainly based on the MFCAD dataset (Cao et al. 2020) and consists \nof 15, 488 images of CAD models and their corresponding machining features as labels. \nWe only show results for 8 image-label pairs in this document as the inaccuracy of GPT-4V \nmakes it difficult to quantify the performance. We anticipate that this dataset can be useful \nin evaluating a more capable VLM that can understand 3D geometry and engineering design \nimages.\n ● Initial prompt: I am going to ask you a series of questions about some machining feature \nrecognition from an image of a stock of material.\n ● Image prompt: Here are the machining features, from an image you need to identify \nwhich machining features are present in the stock of material in the image. – List of ma-\nchining features: Rectangular through slot, Triangular through slot, Rectangular pas -\nsage, Triangular passage, 6 sided passage, Rectangular through step, 2 sided through \nstep, Slanted through step, Rectangular blind step, Triangular blind step, Rectangular \nblind slot, Rectangular pocket, Triangular pocket, 6 sided pocket, Chamfer, Stock\nWe repeat these 50 queries three times and obtain similar responses. GPT-4V identifies at \nleast one feature in most images but fails to consistently identify features.\nDiscussion: design for manufacturing Overall, we note that GPT-4V never successfully \nanswers all parts of any of our DfM queries. While the model is able to correctly answer \npieces of our questions (e.g. that the design breaks an AM rule, that the design contains a \ncertain machining feature), its answers are never fully accurate. In particular, we notice that \nGPT-4V sometimes struggles to or forgets to follow directions specified in a prompt, and its \nperformance deteriorates as the complexity of designs increases.\nIn terms of its ability to predict the success of additive manufacturability based on pro -\nvided design rules, GPT-4V always states that the provided design will not be able to be suc-\ncessfully produced using AM. The response that the part would not be able to be 3D-printed \nsuccessfully was consistent across all 60 queries, for both the problematic and manufactur-\nable design sets. In other words, for all the designs that were manufacturable—and which \nviolated no FDM design rules in the chart—GPT-4V hypothesized that they would break \none of the 3D printing design rules. This consistently negative response to printability \n1 3\n288 Page 46 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nlikely reflects a cautious posture on the part of the model. We also observed from the data \nin Table 9 that GPT-4V always maintains that multiple design rules are broken, while all \ndesigns in the problematic design set violate just one of the rules listed in the Hub design \nrule chart. As such, GPT-4V is never fully correct in answering any of our questions about \nadditive manufacturability based on the provided design rules. GPT-4V’s listing of many \nrules in response to our question about rules violated could further reflect its tendency to \ntake on a cautious position. Less than half the time (13/30 queries) is GPT-4V able to cor -\nrectly identify the violated rule for the problematic designs. We also note that the model \nsometimes seems to get confused by and/or forgets our ask to name the rules by the numbers \nwe assigned to each one in our prompt (see Fig. 19). As seen in Fig. 19, the model lists the \nfirst rule by the correct number but then lists the second rule by an incorrect number.\nOverall, in the context of additive manufacturing tasks, the implications of using GPT-4V \nare nuanced. The model consistently predicts that designs will not be successfully produced \nusing additive manufacturing methods, regardless of their actual manufacturability. This \nuniform negativity indicates a cautious approach, likely to avoid over-optimistic assess -\nments, but also leads to an overestimation of manufacturing challenges. GPT-4V’s tendency \nto list multiple broken design rules, even when only one is violated, further reflects its \ncaution. However, this approach can be misleading in real-world scenarios where precise \nand accurate manufacturability assessments are crucial. The model’s difficulty in following \nspecific prompt instructions, such as correctly identifying rules by assigned numbers, points \nto a need for further development in its ability to process and respond to detailed addi -\ntive manufacturing queries. While GPT-4V’s partial answering capability suggests a basic \nunderstanding of additive manufacturing principles, its current limitations underscore that \nit is not yet a reliable tool for comprehensive and accurate manufacturability assessments \nin additive processes.\nIn subtractive manufacturing tasks, GPT-4V demonstrates an ability to identify at least \none machining feature in most images (12/20), but its performance is inconsistent, particu -\nlarly with more complex designs. For example, GPT-4V often identifies ‘triangular through \nFig. 20 Machining feature recognition from CAD images: results are shown for eight selected samples \nwhere each ground truth (GT) is also shown corresponding to the GPT-4V response\n \n1 3\nPage 47 of 75 288\nC. Picard et al.\nslot’ instead of ‘rectangular through slot’ and ‘2-sided through step’ instead of ‘6-sided pas-\nsage’. The model also misidentifies distinct features, such as confusing a ’triangular through \nslot’ with a ’rectangular through slot’, and exhibits challenges in understanding more intri-\ncate geometric features. This inconsistency in feature identification can lead to unreliable \nassessments in scenarios where precision in subtractive manufacturing is essential. While \nGPT-4V seems to fare better with simpler geometric objects, its difficulty with complex \nobjects suggests that its current use might be more suitable for preliminary assessments or \neducational purposes, rather than for detailed, technical manufacturing evaluations. The \nsomewhat random nature of its explanations and inability to satisfy detailed engineering \ndesign concerns indicates that significant improvements are necessary before GPT-4V can \nFig. 21 Example prompts and answers for the feature identification in subtractive manufacturing\n \n1 3\n288 Page 48 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nfunction as a stand-alone tool in subtractive manufacturing tasks. As such, while GPT-4V \ncan provide some support in these tasks, it requires careful human oversight and verification \nto ensure accuracy and relevance in practical manufacturing scenarios.\nBased on the study’s insights into GPT-4V’s performance in Design for Manufacturing \ntasks, future work should focus on enhancing the model’s precision and depth of understand-\ning in both additive and subtractive manufacturing processes. For additive manufacturing, \nresearch should aim to calibrate GPT-4V’s cautious approach, enabling it to differentiate \nbetween manufacturable and non-manufacturable designs more accurately, and follow spe-\ncific guidelines more precisely. In the realm of subtractive manufacturing, efforts need to be \ndirected toward improving GPT-4V’s ability to consistently and correctly identify complex \nmachining features. This includes training the model to handle a broader range of geome -\ntries and intricate design elements, thus reducing its current limitations in assessing detailed \nand technical aspects of manufacturing designs. Additionally, developing a better way for \nAI models to understand 3D geometry could enhance GPT-4V’s interpretative capabili -\nties, leading to more reliable and practical applications in the manufacturing sector. These \nadvancements would not only make GPT-4V a more robust tool for manufacturing design \nbut also pave the way for its broader application in automated manufacturing processes. \nIn the next section, we turn to another application of GPT-4V for manufacturability: post-\nmanufacturing inspection.\n5.2 Post-manufacturing inspection\nEngineering inspection constitutes a whole domain within itself: parts must be inspected \nafter they are fabricated to ensure that they meet certain technical requirements, and for \ncritical components, inspection can continue into the lifetime of the part. Inspection is a \nkey aspect of the engineering design process, as it may help in improving the next iteration \nof the design. Oftentimes, inspection necessitates a visual component (e.g., detection of a \ndefect through an image, X-ray, graph of collected data, etc.) alongside extensive engineer-\ning knowledge of detailed documents, like engineering standards. As such, we are curious to \nunderstand if GPT-4V , with multimodal capabilities, can aid engineers with defect detection \nin images.\nMethodology For our analysis, we use the CODEBRIM (Concrete DEfect BRidge \nIMage) dataset, released by Mundt et al. (2019). It entails images of structural concrete \nfrom bridges that contain none or some of the following defects: cracks, spallations, efflo -\nrescence, exposed bars, and corrosion strains. Sample images from the dataset can be seen \nin Fig. 22. We chose a subset of 23 images from the CODEBRIM dataset for our experi -\nments. The images were chosen such that each of the five defect types was present in at \nleast five images. Five of the 23 images were “background” images, containing no defects. \nWe provided GPT-4V each image in a separate context window 4 and asked the model to \nidentify any of the five defects it could find. If the model was hesitant to respond—due to \nimage resolution or safety concerns—but still suggested certain defects, we counted that \nas a response. To understand repeatability, each of the 23 image experiments was repeated \nthree times, for a total of 69 queries. Two queries and responses can be seen in Fig. 23. The \nresults for all experiments can be seen in Tables  10, 11, 12, 13, 14.\n4 This experiment was conducted using the Nov. 6, 2023 version through the API interface.\n1 3\nPage 49 of 75 288\nC. Picard et al.\nDiscussion The confusion matrices (Tables 10, 11, 12, 13, 14) provided for different \ndefect types in structural concrete offer insights into GPT-4V’s defect detection capabilities. \nFor 12 experiments (8 different images), GPT-4V would not provide an answer to our ques-\ntion, citing resolution issues, safety concerns, or just plainly stating that it could not assist \nwith the request. When it did answer, GPT-4V did not perform particularly well in predict-\ning types of defects, as seen by the relatively low F1 scores for each class of defect. We also \nnote that GPT-4V tends to over-predict the crack defect; this is evidenced by the relatively \nhigh recall (true positive rate) score of 0.79 and a relatively low specificity (true negative \nrate) score of 0.44 when compared with the other classes. Perhaps most familiar with the \ncrack defect, GPT-4V may over-predict cracks out of an abundance of caution and the safety \nimplications of missing a true positive defect in concrete images of a bridge. For 14 out of \nthe 69 queries, GPT-4V had a perfect defect prediction of all defect classes in the image. \nIntriguingly, half of these perfect scores were for images without any defects, suggesting \nthat GPT-4V might be more adept at discerning the absence of defects rather than accurately \nclassifying the type of defect present.\nThe findings of this study have significant implications for the application of AI in engi-\nneering inspection tasks. While GPT-4V demonstrates potential in identifying defects in \nstructural concrete, its moderate performance underlines the need for further model refine -\nment and continued reliance on human expertise or more specialized machine learning \ntools. The model’s ability to discern the absence of defects could be leveraged in prelimi -\nnary inspections to streamline processes, yet the necessity for human verification remains \nparamount, especially in safety-critical assessments. These results suggest avenues for \nfuture research focused on improving AI accuracy through diverse training datasets and \napproaches of expert feedback.\n5.3 Summary\nTo assess the manufacturing-related knowledge of GPT-4V , we performed three types of \nexperiments. A concise summary of our findings for each are provided below. \nFig. 22 Bridge structural concrete images adapted from the CODEBRIM dataset (Mundt et al. 2019) under \nits specific license. From left to right, as named in the original dataset: (1) image_0000005_crop_0000001.\npng—contains efflorescence and corrosion stain defects. (2) image_0000046_crop_0000001.png—con-\ntains crack defects. (3) image_0000109_crop_0000003.png—contains spallations and corrosion strain \ndefects. (4) image image_0001189_crop_0000004.png—contains exposed bar defects\n \n1 3\n288 Page 50 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nTrue positive True negative\nPredicted positive 11 3\nPredicted negative 8 35\nF1 score is 0.67\nTable 11 Confusion matrix for \nspallation defects as predicted by \nGPT-4V\n \nTrue positive True negative\nPredicted positive 11 24\nPredicted negative 3 19\nF1 score is 0.45\nTable 10 Confusion matrix for \ncrack defects as predicted by \nGPT-4V\n \nFig. 23 Example prompts and answers for the concrete defect classification\n \n1 3\nPage 51 of 75 288\nC. Picard et al.\n(i) Design for additive manufacturing  Section 5.1.1—In the realm of additive manufac-\nturing, does GPT-4V consistently predict the 3D printability of a design based on a set \nof provided DfAM rules?\n ● GPT-4V uniformly (in all instances) indicated that designs would not be suitable for \n3D printing. This conclusion was drawn irrespective of whether the designs actually \nconformed to the specified additive manufacturing rules.\n(ii) Design for subtractive manufacturing  Section 5.1.2—Is GPT-4V capable of identi-\nfying manufacturing features in subtractive manufacturing designs?\n ● GPT-4V exhibited a basic grasp of feature geometries but lacked consistency in its \nresponses. The model struggled to differentiate between similar features and fre -\nquently resorted to making arbitrary guesses.\n(iii)  Post-manufacturing inspection Section 5.2—To what extent can GPT-4V accurately \nclassify different types of defects in images, specifically in the context of identifying \nconcrete defects in manufacturing?\n ● Based on our experiments with concrete defect classification, we find that GPT-4V \nmay have the potential to distinguish between images that have defects and images \nthat do not. However, it was unable to consistently and accurately classify different \ntypes of concrete defects.\nTrue positive True negative\nPredicted positive 15 10\nPredicted negative 6 26\nF1 score is 0.65\nTable 14 Confusion matrix \nfor corrosion stain defects as \npredicted by GPT-4V\n \nTrue positive True negative\nPredicted positive 9 5\nPredicted negative 6 37\nF1 score is 0.62\nTable 13 Confusion matrix for \nexposed bar defects as predicted \nby GPT-4V\n \nTrue positive True negative\nPredicted positive 3 4\nPredicted negative 12 38\nF1 score is 0.27\nTable 12 Confusion matrix for \nefflorescence defects as predicted \nby GPT-4V\n \n1 3\n288 Page 52 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\n6 Engineering textbook problems\nOverview and motivation In this last section, we take a step back from the product devel-\nopment process and investigate GPT-4V’s abilities to solve problems that are present in \nengineering curricula. During their curriculum, students are regularly asked to solve engi -\nneering design problems that require them to interpret sketches, graphs, tables, and images \nto answer a related question. As such, students need to integrate their natural language pro-\ncessing and visual information understanding skills with their domain knowledge to solve \nthis type of problem. The underlying idea is that these are tasks and assignments used to \nevaluate humans’ readiness to be engineers. Consequently, they may enable us to draw \nsome comparison with GPT-4V’s readiness to support engineering tasks. Textbook prob -\nlems, exam questions, and standardized tests have been quite popular ways to evaluate \nLLMs (Katz et al. 2023; Wang et al. 2023). These problems are often well-defined, self-\ncontained, and mostly closed-form type questions (Taraban 2011), supporting replicabil -\nity (Zong and Krishnamachari 2023). For example, for text input only, SciBench (Wang \net al. 2023) features 695 collegiate-level textbook problems drawn from physics, chemistry, \nand mathematics. Using this benchmark, SciBench (Wang et al. 2023) aimed at evaluating \nthe reasoning and problem-solving skills of LLMs.\nFollowing a similar approach, we propose to use engineering textbook problems requir-\ning visual information to evaluate GPT-4V’s understanding and problem-solving capabili -\nties through the pairing of different visual and textual information.\nMethodology We gathered questions from two undergraduate engineering design classes \npublicly available under CC-BY-NC-SA on MIT OpenCourseWare (Frey and Gossard 2009; \nChun and Kim 2004). The class materials include problem sets and exams. All class materi-\nals come with model solutions, which we use as ground truth. To ensure that we are evalu -\nating GPT-4V’s multimodal capabilities, we select questions that reference one or more \npictures in the question prompt. We ignore questions that require students to annotate an \nimage as GPT-4V cannot generate images, but questions asking for sketches are included. \nIndeed, sketches can be parameterized and drawn using coding languages.\nTo ensure independence, we reset GPT-4V’s context window for each question except for \nmulti-part questions, where we prompt GPT-4V sequentially. For multi-part questions that \nhave multiple images, we supplement each sub-question with only the images required to \nsolve that particular question to avoid confusing GPT-4V with superfluous information. For \nexample, consider a multi-part question has two images X and Y, and part (a) only requires \nX to solve, part (b) requires Y to solve and part (c) requires both X and Y to solve, we would \nsupplement question (a) with X, question (b) with Y and question (c) with both X and Y.\nWe evaluate GPT-4V’s correctness based on a binary scale: one point is only given \nfor fully correct answers, and none otherwise. Being “fully correct” means outputting an \nanswer that is semantically similar to the ground truth for free-text questions. For questions \ninvolving calculations, the correct numerical answer must be provided and the intermediate \nsteps should reasonably lead to the correct solution. For multi-part questions, we award a \npoint for each correct part. We group the errors into three categories:\n ● Reasoning: Incorrect explanation or calculation.\n ● Inference: Incorrect information extraction from the image.\n ● Imprecise: Vague answer or explanation without execution.\n1 3\nPage 53 of 75 288\nC. Picard et al.\nThe questions were repeated three times to account for the variability of the model and, \noverall, a question was considered correctly answered if at least two repeats were correct.\nCan GPT-4V Solve Engineering Textbook Problems?  We extracted 21 questions from \ntwo classes, resulting in a total of 44 questions when counting each sub-part individually. \nWe observe that GPT-4V can answer 16 of these 44 questions correctly, giving an aver -\nage 36% accuracy. An overview of all the repeats and questions is provided in Table 17. \nRelatively to the type of image, GPT-4V answered correctly most questions involving 3D \nmodels and tables (63% and 67% respectively) but had a lower success rate for photographs \n(33%), diagrams (29%) and graphs (0%), see Table 15. In terms of question format, GPT-\n4V performed slightly better on free-text questions (44%), than on any other format, see \nTable 16. Overall, we observe that GPT-4V makes mostly reasoning errors (20), followed \nby imprecise answers (5) and inference errors (3). Thus, it seems to be more helpful for \nquestions that require explanations and for problems that ask questions about tables or 3D \nmodels.\nSelected questions and answers In the following, we reproduce selected questions and \nanswers to illustrate the type of questions, as well as the type of errors in the answers.\nFirst, we look at an example of imprecise answers. Figure 24 shows the question and \nanswer to Q1 (a–c). Although GPT-4V can describe relationships between stall torque, no-\nload speed, and maximum power, it fails to provide the exact proportions by which the rela-\ntionships increase or decrease. For Q1, the expected solution is that by doubling the number \nof windings, the stall torque doubles, the no-load speed is cut in half and the maximum \npower stays constant. Noteworthy, the provided answer also contains additional explana -\ntions that were not asked for.\nNext, we look at Q2, which is the only multiple-choice question, see Fig. 25. The correct \nanswer to this question is (d) since the capacitor is already charged up to the supply volt -\nage and cannot unload through the LED. GPT-4V seems unable to understand the circuit \nbased on the photograph and thus, bases its responses solely on the provided text. While it \nunderstood that the LED and the capacitor were in series, it got some basic physics concepts \nincorrect.\nFinally, we look at a question that requires extracting values from a table and performing \ncalculations using them, see Fig. 26. While arguments can be made to input tabular data as \ntext, tables are often inconvenient to input and are more convenient for users to input as \nimages. In this particular answer (repeat 3), GPT-4V is able to correctly extract the values \nand calculate the center distance, the torque, and the reaction force. It is worth keeping in \nTable 15 Summary of GPT-4V’s score on textbook problems grouped by type of image\nPhoto Diagram Graph 3D Table Overall\nCorrect 1 8 0 5 2 16\nTotal 3 28 2 8 3 44\nAvg. (%) 33 29 0 63 67 36\nFree text MCQ Numerical Draw Overall\nCorrect 7 0 8 1 15\nTotal 16 1 24 3 44\nAccuracy (%) 44 0 33 33 36\nTable 16 Summary of GPT-4V’s \nscore on textbook problems \ngrouped by question format\n \n1 3\n288 Page 54 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nmind, see Table 17 that the two other repeats were not as successful, showing how challeng-\ning such a task is.\n6.1 Discussion: Textbook Problems\nAs previously mentioned, GPT-4V makes three types of mistakes: reasoning, image misin-\nterpretation, and imprecision. We go into detail below.\nReasoning GPT-4V can sometimes provide incorrect answers as a result of reasoning \nerrors. This was especially apparent with multi-step reasoning tasks. When asked what hap-\npens to a system after a series of actions are performed, such as the one given in Fig. 25, \nGPT-4V often makes a mistake in reasoning, such as hallucinating a fact, and derails its \nchain of thought. Furthermore, When provided a question to compute a numerical answer, \nGPT-4V can sometimes have the correct methodology for arriving at the correct answer but \nprovides an incorrect answer due to an incorrect numerical approximation. For example, \nwhen asked to compute e0.3π, GPT-4V instead used eπ as an approximation and arrived at \nan incorrect answer even though its previous steps were correct. This issue is not difficult to \nFig. 24 A diagram-based textbook problem about motor parameters. Q1—Repeat 1\n \n1 3\nPage 55 of 75 288\nC. Picard et al.\nFig. 26 Calculating answers using information from a table. Q9—Repeat 3\n \nFig. 25 Reasoning on an electric circuit based on a photograph. Q2—Repeat 1\n \n1 3\n288 Page 56 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nTable 17 Detailed list of questions including image type and question format and answers by GPT-4V for \neach trial, along with the type of error\nImage Type Format #1 / Error #2 / Error #3 / Error Overall\nQ1-a Diagram Free text Imprecise Imprecise Imprecise ×\nQ1-b Diagram Free text Imprecise Imprecise Imprecise ×\nQ1-c Diagram Free text Imprecise Imprecise Imprecise ×\nQ2 Photograph MCQ Reasoning Reasoning Reasoning ×\nQ3-a 3D-model Free text ✓ ✓ ✓ ✓\nQ3-b 3D-model Free text ✓ ✓ ✓ ✓\nQ4-a Diagram Numerical ✓ Reasoning Reasoning ×\nQ4-b Diagram Numerical Inference Inference Inference ×\nQ5 Diagram Draw Imprecise Imprecise Imprecise ×\nQ6 Photograph Numerical ✓ ✓ ✓ ✓\nQ7 Photograph Numerical Reasoning Reasoning Reasoning ×\nQ8-a 3D-model Free text ✓ Reasoning Reasoning ×\nQ8-b 3D-model Free text Reasoning Reasoning ✓ ×\nQ8-c 3D-model Draw Imprecise Imprecise Imprecise ×\nQ9-a Table Numerical ✓ Inference ✓ ✓\nQ9-b Table Numerical Reasoning ✓ ✓ ✓\nQ9-c Table Numerical Reasoning Reasoning ✓ ×\nQ10 3D-Model Free text ✓ ✓ ✓ ✓\nQ11 Diagram Numerical Reasoning Reasoning Reasoning ×\nQ12-a Diagram Numerical Reasoning Reasoning Reasoning ×\nQ12-b Diagram Numerical Reasoning Reasoning Reasoning ×\nQ12-c Diagram Numerical Reasoning Reasoning Reasoning ×\nQ12-d Diagram Free text ✓ ✓ ✓ ✓\nQ13 Diagram Numerical Reasoning Reasoning Reasoning ×\nQ14-a Diagram Numerical Reasoning Reasoning Reasoning ×\nQ14-b Diagram Free text Reasoning Reasoning Reasoning ×\nQ14-c Diagram Free text ✓ ✓ ✓ ✓\nQ15-a Diagram Numerical Reasoning Reasoning Reasoning ×\nQ15-b Diagram Numerical Reasoning Reasoning Reasoning ×\nQ16-a Diagram Draw ✓ ✓ ✓ ✓\nQ16-b Diagram Free text ✓ ✓ ✓ ✓\nQ16-c Graph Free text ✓ Inference Inference ×\nQ16-d Diagram Numerical Reasoning ✓ ✓ ✓\nQ16-e Diagram Numerical Reasoning Reasoning Reasoning ×\nQ17-a Diagram Numerical ✓ ✓ ✓ ✓\nQ17-b Diagram Numerical Reasoning Reasoning Reasoning ×\nQ17-c Diagram Numerical ✓ ✓ ✓ ✓\nQ18-a Diagram Numerical Reasoning ✓ Reasoning ×\nQ18-b Diagram Numerical Reasoning Reasoning Reasoning ×\nQ19-a Diagram Free text Reasoning Reasoning Reasoning ×\nQ20-a Diagram Free text ✓ ✓ ✓ ✓\nQ20-b Diagram Free text Inference Inference Inference ×\nQ21-a 3D-Model Numerical ✓ ✓ ✓ ✓\nQ21-b 3D-Model Numerical ✓ ✓ ✓ ✓\nOverall score 16 (36%)\n1 3\nPage 57 of 75 288\nC. Picard et al.\nalleviate, however, as previous papers have shown that leveraging tools such as calculators \ncan enable GPT-4V to perform numerical reasoning tasks better.\nImage misinterpretation GPT-4V can have trouble understanding and inferring infor -\nmation in images. In Fig. 25, GPT-4V fails to interpret the circuit shown as a photograph. \nAs a result of this misinterpretation, GPT-4V cannot leverage the information present in it \nand only answers based on the provided text.\nImprecision While GPT-4V has reasonable success in providing qualitative answers, it \nsometimes fails to do so as its answers are too vague or do not capture the main idea of a \nquestion. In a question that asked about potential errors in an injection molding experiment, \nGPT-4V provided a long list of potential issues, but not issues that were specific to the ques-\ntion. GPT-4V also had issues with relating exact numerical relationships between different \nvariables as seen from Fig. 24.\n6.2 Summary\nHere we provide a summary of our findings regarding GPT-4V’s ability to solve engineering \ntextbook problems. \n(i) High level reasoning Is GPT-4V able to reason about domain-specific at a high level?\n ● We evaluated GPT-4V on 16 free-text questions, which ask about high-level rea -\nsoning knowledge on Mechanical Engineering topics. Of the 5 types of questions, \nGPT-4V performs relatively better in this area compared to other tasks. Free-text \nquestions tend to be more open-ended and require less precise reasoning than other \ntypes of questions. As a result, GPT-4V can provide answers that are considered cor-\nrect even though they may not fully correspond to the model solution.\n(ii) Numerical reasoning How strong is GPT-4V at numerical reasoning tasks?\n ● We evaluated GPT-4V on 24 different numerical reasoning tasks and it answered 8 \nof them correctly. We note that GPT-4V’s inability to answer numerical questions \ncomes from two sources. First, it is unable to precisely compute answers, leading \nfuture steps in the computation to deviate from the answer. Second, it is unable to \nlogically incorporate domain knowledge during its reasoning process, causing it to \nuse incorrect formulas.\n(iii) Failure modes When does GPT-4V fail to provide satisfactory answers?\n ● We classified the failure modes of GPT-4V into three cases: reasoning, inference, \nand imprecision. Reasoning errors happen either due to failing complex, precise \nlogical steps or incorporating domain knowledge. This was especially apparent for \nmulti-step reasoning tasks. Inference errors happen when GPT-4V fails to incorpo-\nrate image information into its answers. Imprecision errors happen when GPT-4V is \nunable to calculate correct answers as alluded to in the above section.\n1 3\n288 Page 58 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\n7 Spatial reasoning abilities\n7.1 Mental rotation and packing tests\nSpatial reasoning is the ability of humans to perform mental spatial operations: rotations, \ntranslation, projection, and orientation. Spatial reasoning is at play when humans read maps, \nnavigate their homes at night without light, or solve pretty much any problem in the fields \nof science, technology, engineering, and mathematics (STEM) (Maeda and Yoon 2013). \nSpatial reasoning skills are considered essential skills for understanding graphs, diagrams, \nplots, 3D objects, and representations. Indeed, multiple studies have found that spatial abili-\nties are a good predictor of academic success (Shea et al. 2001; Berkowitz and Stern 2018). \nConsequently, spatial reasoning skills have been well studied in humans, and many stan -\ndardized tests exist, e.g., The Revised Purdue Spatial Visualization Test: Visualization of \nRotations (PSVT:R) (Yoon 2011), the Mental Cutting Test “Schnitte” (Quaiser-Pohl 2003), \nor the Novel Spatial Ability Tests (Berkowitz et al. 2021).\nFollowing some of our observations on the apparent struggles of GPT-4V regarding spa-\ntial understanding, we specifically tested its spatial abilities in order to provide additional \ninsights. Spatial reasoning tests are also good candidates to evaluate vision language models \nsince they focus on inherently visual tasks and are often not publicly available, to maintain \ntheir validity. They are thus unlikely to be part of the training data.\nMethodology We assessed GPT-4V’s spatial reasoning skills using the openly accessible \npacking test (part of the Novel Spatial Ability Tests (Berkowitz et al. 2021)), and the MechE \nRotation Test (Picard 2023). While the first one is openly accessible, the latter is released \npublicly for the first time in parallel to this work.\nThe MechE Rotation Test follows the general principles of the PSVT:R, but uses objects \nwith features typically seen on mechanical parts. It measures the ability of participants to \nvisualize one or two rotations applied to a reference object and apply them to another object. \nFor each question, five possible configurations of the object are shown and the participants \nselect the correct one. The test is composed of an example—for which the correct answer is \ngiven to the participant—followed by ten questions of increasing difficulty. The packing test \nrequires participants to evaluate if shapes can be composed of or decomposed into smaller \nsub-shapes. The packing test is split into two parts: packing and unpacking. In the first part, \nparticipants have to choose among four options which set of sub-shapes can be packed \ntogether to form a larger shape. In the second part, participants do the opposite and select \namong four large shapes, which can be decomposed into the provided smaller shapes. The \nexample questions for these tests are shown in Fig. 27.\nIn this work, we have GPT-4V take the tests as they are given to human participants: We \ntype the exact textual instructions from these standardized tests and include the correspond-\ning images. Each questionnaire is passed in a single context, sequentially going through the \nexamples and the questions, and providing the instructions and images. To account for some \nstochasticity, each questionnaire is repeated five times. In addition, and inspired by Yang \net al. ( 2023a), we evaluate if adding visual marks—reference coordinates and coloring \nfaces—improved the performance of the model on the MechE rotation test. The runs using \nthe original test are referred to as Run H, while the ones with the marks are called Run P.\nThe full set of prompts is made available as a benchmark for any future vision language \nmodel (Fig. 28).\n1 3\nPage 59 of 75 288\nC. Picard et al.\nScores on the spatial reasoning tests The answers of GPT-4V for the packing test and \nthe MechE rotation test  are provided in Tables 18 and 19, respectively. Starting with the \npacking test, GPT-4V obtains an average score across five runs of 36%, slightly higher than \nthe expected average score if answering at random (25%). Interestingly, all five questions \nthat have been answered correctly have been done so by at least two runs, further suggesting \nthat GPT-4V is not answering at random. In comparison to humans, however, it remains sig-\nnificantly lower than the average score of undergraduate (66%) and graduate (73%) students \nreported by Berkowitz et al. ( 2021). For the MechE rotation test, the average scores (16% \nand 20%) are lower and closer to the expected score for random answering (20%). While \nslightly higher, it is unclear if the visual prompting supports GPT-4V . While no human \nresults have been published for this test, average scores between 60% and 70% are expected \nbased on internal tests and by comparison to the revised PSVT:R test.\nFig. 27 Example questions from the considered spatial reasoning tests\n \n1 3\n288 Page 60 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nTo gain more insights, run P1 is reproduced in Fig. 29. GPT-4V’s answers seem to indi-\ncate that the nature of the test and the task is well understood. However, while the answer is \ncorrect, the reasoning is incorrect. Based on the provided coordinate system, the reference \nobject is rotated by 90◦ around the X-axis, and not the Z-axis as stated by GPT-4V . This type \nof behavior has already been reported regarding numerical reasoning (Stechly et al. 2023). \nAs such, it seems like additional visual or textual instructions are needed to properly root \nthe model within a spatial system.\nDiscussion Overall, our evaluation of the spatial abilities of GPT-4V using standard -\nized (human) tests tends to suggest that, compared to humans, GPT-4V has some, although \n(Correct) Run 1 Run 2 Run 3 Run 4 Run 5\nPart 1 Q1 (4) 3 2 3 3 3\nPart 1 Q2 (3) 4 1 4 4 4\nPart 1 Q3 (3) 3 3 3 3 3\nPart 1 Q4 (2) 2 1 2 2 2\nPart 1 Q5 (1) 3 4 4 4 3\nPart 2 Q1 (3) 3 3 2 3 3\nPart 2 Q2 (3) 4 1 1 4 1\nPart 2 Q3 (2) 2 4 3 2 2\nPart 2 Q4 (2) 1 3 1 3 1\nPart 2 Q5 (3) 3 1 4 4 3\nScores 50% 20% 20% 40% 50%\nAverage 36%\nTable 18 Answers and scores \nfor the Packing Test (Berkowitz \net al. 2021)\nCorrect answers are in bold. \nEach run was conducted within \nthe same context\n \nFig. 28 Example of the MechE rotation test with the additional visual prompts to support the model\n \n1 3\nPage 61 of 75 288\nC. Picard et al.\nlimited, spatial reasoning capabilities. Indeed, while these visualization tasks are hard and \nconstructed to be somewhat deceptive, most untrained undergraduate students in science \nand technical fields answer at least half of the questions correctly (Yoon 2011; Berkowitz \net al. 2021). Unfortunately, this seeming lack could, in part, explain GPT-4V’s limitations in \nperforming engineering design tasks, such as CAD generation, see Sect. 4.2. These results \nalso corroborate findings recently reported in the literature (Wen et al. 2023).\n7.2 Summary\nOur evaluation of GPT-4V’s performance on standardized spatial reasoning tests is sum -\nmarized below. \n(i) Standardized spatial reasoning tests  Can GPT-4V correctly answer multiple-choice \nquestionnaires created to assess spatial reasoning skills in humans?\n ● We repeated the MechE rotation and the packing tests five times each for a total \nof 100 multiple-choice questions. The model’s answers are similar success rates \nexpected for the number of options in these questions.\n ● Further, we investigated a visual prompting approach based on (Yang et al. 2023a) \non the MechE rotation test aiming to assist the model. Repeated five times, we see \nno improvement in model performance over the unmodified test.\n8 Benchmarking LLaVA\nWe further explored how well VLMs perform engineering design tasks by performing the \nquantitative experiments with LLaV A 1.6 34B. It is an open-source VLM with both a chat-\nbot and API interface, and for our experiments, we utilized the API interface with a “tem -\nperature” and “top-k” of 1.0. The results of the quantitative experiments using both GPT-4V \nand LLaV A 1.6 34B are shown in Table 20.\nTable 19 Answers and scores to the MechE Rotation Test (Picard 2023)\n(Correct) H1 H2 H3 H4 H5 P1 P2 P3 P4 P5\nQ1 (D) C C C D D D D C C B\nQ2 (A) D B A D C D B D B D\nQ3 (C) C C C E B C D B C C\nQ4 (E) C D A A C C C D B D\nQ5 (C) C B A B B D B B C C\nQ6 (B) E D C D C D E E D E\nQ7 (C) E E D C D E C C B D\nQ8 (A) B C C B E C D B C C\nQ9 (E) D B A C B C A C B E\nQ10 (A) C C C B D B E B C B\nScores 20% 10% 20% 20% 10% 20% 20% 10% 20% 30%\nAverage 16% 20%\nCorrect answers are in bold. Each run was conducted within the same context\n1 3\n288 Page 62 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nFig. 29 First two prompts for the run P1 of the MechE rotation test, with the corresponding answers by \nGPT-4V . Both prompts are executed consecutively in the same context\n \n1 3\nPage 63 of 75 288\nC. Picard et al.\nAcross all 12 relevant quantitative experiments, GPT-4V outperforms LLaV A 1.6 34B. It \nis important to note that LLaV A 1.6 34B is only able to intake one input image per context, \ntherefore the tasks that required two or more input images cannot be run with this model. \nWe denote this in Table 20 where applicable, such as for the spatial reasoning experiments. \nOne quantitative experiment that we ran with only GPT-4V was the design similarity triplet \nexperiments. The reason being that LLaV A 1.6 34B often output nonsensical answers. For \nexample when presented with three designs labeled A, B, and C, and asked which design is \nmost similar to A, LLaV A 1.6 34B often answered “Design D,” or other irrelevant answers \nlike “The image in the middle.” Therefore we could not use the triplet similarity results to \nmeasure self-consistency or transitive violations. We note, however, that GPT-4V did not \nhave this problem.\nOverall, this section demonstrates how the datasets and experiments published as part of \nthis work can be used to benchmark future VLMs on engineering design tasks. Furthermore, \nthese results reinforce the need for field-specific relevant datasets to evaluate the perfor -\nmance of machine-learning models. Indeed, our results strongly contrast with the perfor -\nmance reported by LLaV A 1.6 34B’s authors, showing that the model outperforms GPT-4V \nand Gemini Pro on several benchmarks (Liu et al. 2024).\n9 Discussion\nIn this paper, we aimed to assess the capabilities of VLMs in several engineering design \ntasks ranging from conceptual design to manufacturing, and develop a reusable benchmark \nto evaluate future VLMs. Below, we present a detailed discussion of our findings from each \nsection.\nMax GPT-4V LLaV A 1.6 34B\nDesign description\n With text description 30 30 26\n No text description 30 16 14\n No text descr., no N/A 30 21 13\n Engg. drawing analysis 99 86 29\n CAD generation (1st try) 54 28 3\n Topology optimization 90 68 43\nDesign for manufacturing\n Additive 90 − 22 N/A\n Machining features 60 0 0\n Crack inspections 345 204 172\n Textbook questions 135 51 26a\nSpatial reasoning\n Rotation 100 18 N/A\n Packing 50 18 N/A\n Total 1113 540 326\nTable 20 Total score for both \nGPT-4V and LLaV A 1.6 34B on \nthe quantitative experiments in \nthis work\nThe maximum possible score is \nprovided as the “Max.” column\na15 of the 135 textbook \nquestions involved two input \nimages, which LLaVA 1.6 34B \ndoes not support\n \n1 3\n288 Page 64 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\n9.1 Conceptual design\nWe examined design similarity analysis, sketch descriptions, and concept selection. We \ndiscovered that GPT-4V could evaluate design similarity with high self-consistency and \nminimal transitive violations. It was also consistent with human-generated idea maps in \nidentifying unique sketches and groups of similar sketches. Additionally, it effectively \nmatched design sketches to their descriptions when provided with the entire sketch (an \naverage score of 10/10), including a handwritten description, but without the description, it \noften chose “None of the above,” and therefore performed worse (average score of 5.33/10). \nWhen “None of the above” was not an option, GPT-4V performed better (an average score \nof 7/10). This suggests a level of “caution,” so when GPT-4V has the chance to not be \nincorrect, it takes it. GPT-4V could generate useful and accurate text descriptions of designs \neven for sketches with very low drawing scores. Lastly, the model generated appropriate \nselection criteria but did not generate Pugh charts when only provided with design sketches. \nOverall, GPT-4V shows the great potential of VLMs for design sketch analysis and sup -\nporting the conceptual design stage beyond what has been identified in previous work (Sid-\ndharth et al. 2022; Stella et al. 2023).\n9.2 System-level and detailed design\nWe investigated GPT-4V’s ability to use several Ashby diagrams to suggest appropriate \nmaterials, analyze engineering drawings, and generate CAD scripts. We found that GPT-4V \ncould correctly respond where to look for materials in Ashby diagrams, but made errors \nwhen asked to be specific. The model faced difficulty in understanding the nuances of a \nblock-with-blind-hole engineering drawing, but it was able to extract most dimensions and \nassign them appropriate labels. In terms of CAD generation ability, GPT-4V was successful \nin generating correct CAD scripts on the first attempt only in one out of nine experiments, \nand our iterations to fix the scripts did not improve the results. LLaV A 1.6 34B scored lower \non the engineering drawing analysis and was unable to make a correct CAD script.\nWe investigated GPT-4V’s ability to understand and analyze structures resulting from \ntopology optimization (TO). GPT-4V showed understanding of high-level TO principles \nlike volume fraction, and proposed realistic locations for where boundary conditions may \nbe. However, when provided with an image, it struggled to estimate the volume fraction of \na design or identify the presence of floating material (Table 8). When allowed to use a code \ninterpreter, however, GPT-4V was able to estimate volume fraction much more effectively. \nThis suggests that for certain tasks, specifically those using spatial reasoning, engineers may \nbenefit by integrating VLMs with other tools or plug-ins.\n9.3 Manufacturing and inspection\nIn the manufacturing stage, we tested GPT-4V’s understanding of design for manufacturing \n(DfM) for subtractive and additive manufacturing operations. GPT-4V was, as we interpret \nit, cautious, and suggested that none of the additive manufacturing parts were printable, \neven when the parts were well within the provided guidelines. On the feature identification \ntask for subtractive manufacturing, GPT-4V was able to identify at least one feature 12 out \nof 20 times, but never all of them. The provided explanations were most of the time incon -\n1 3\nPage 65 of 75 288\nC. Picard et al.\nsistent and confused different technical terms. Furthermore, we assessed GPT-4V’s ability \nto inspect images to find and identify defects. For the evaluated cases, GPT-4V tended to \noverly predict the presence of defects and was inconsistent in identifying the type of defect. \nEven more than in the detailed design stage, design for manufacturing and inspection is all \nabout precision, and GPT-4V overall fails to deliver reliable and consistent performance on \nthe evaluated task.\n9.4 Engineering textbook problems\nWe evaluated GPT-4V’s ability on textbook problems in engineering. Overall, GPT-4V \nachieves rather low scores with a 36% accuracy. It performed the best for textbook problems \nasking for explanations (free text answers) but struggled for numerical questions, both in the \nreasoning and in the numerical value extraction from the provided images.\n9.5 Spatial reasoning\nLastly, we evaluated GPT-4V’s spatial reasoning abilities through specific tests typically \nused to evaluate humans. Overall, GPT-4V achieves rather low scores with 36%, and 18% \naccuracy over the packing test, and the MechE rotation test respectively. GPT-4V’s scores \nare indistinguishable from random answer picking and the provided explanations did not \nmatch the visual representations. Given these low scores, the spatial reasoning tests could \nbecome a competitive benchmark to evaluate future multimodal LLMs. However, the cur -\nrent protocol for these tests, following the testing protocol for humans, requires models to \naccept multiple images, a capability most models don’t currently have.\n9.6 Techniques for enhancing VLM performance\nThere are a number of techniques that can be applied during prompt construction or model \ninference, including in-context learning (ICL), retrieval-augmented generation (RAG), and \nthe integration of other computational tools. For example, in-context learning is a prompt -\ning technique where the model is given a few input–output demonstrations of a task within \nthe prompt itself, enabling it to generalize to new examples without additional fine-tun -\ning Brown et al. (2020). This approach is especially useful in data-scarce scenarios, as it \nrequires only a handful of examples to adapt the model to a new downstream task Luo et al. \n(2024); Dong et al. (2024). In the engineering design domain, researchers have explored the \nefficacy of ICL to better enable VLMs to match expert design evaluations Edwards et al. \n(2025). Their results indicate that the incorporation of ICL improves the model’s ability to \nmatch an expert, even outperforming a trained novice. However, the type of context matters: \ntextual and image-based prompts can yield different results.\nAnother approach to improve LLM or VLM performance on domain-specific tasks is to \nincorporate retrieval-augmented generation (RAG) into the inference pipeline Lewis et al. \n(2020). RAG enables models to access information that they may have had limited or no \nexposure to during pretraining. RAG works by chunking documents into small sections, \nwhich are then embedded. During inference, a user’s query is also embedded, and cosine \nsimilarity is computed between the query and each document chunk to identify the N-most \n1 3\n288 Page 66 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nrelevant chunks. These are appended to the user’s query and fed into the model as input for \nresponse generation.\nRAG has demonstrated potential to improve model performance on domain-specific \nquestions. For example, Xiong et al. (2024) show that coupling GPT-3.5 and Mixtral with a \nRAG framework improves their performance by up to 18% on medical question-answering. \nAlawwad et al. ( 2024) combine supervised fine-tuning and RAG to enhance LLM perfor -\nmance on textbook questions. Applying a RAG system to the questions in this study would \nlikely improve model responses. However, prior work highlights limitations in RAG’s prac-\ntical utility. For instance, Doris et al. ( 2025) used an off-the-shelf RAG system to provide \nVLMs with domain-specific technical content but found that it often failed to retrieve the \nmost relevant sections. Even when provided with curated, relevant context–essentially “ide-\nalized” RAG–LLaV A-1.5 still struggled to answer questions due to inherent VLM limita -\ntions, such as inaccurate image analysis and limited engineering knowledge. Moreover, \nwhile some questions in our study lend themselves well to RAG (e.g., textbook problems), \nothers (e.g., engineering drawing analysis) lack obvious reference documents. Future work \nmight explore more targeted RAG systems–such as Siriwardhana et al. (2023), which are \ndesigned for domain-specific question answering–to improve performance on these tasks.\nWhile this paper does not explore each prompting technique in depth, our goal was to use \nminimal, human-readable prompts to assess VLM performance, and to provide a benchmark \nthat would enable future comparisons. Future work could iteratively implement these tech-\nniques to identify the most effective strategies. To support such efforts, we have released all \noriginal prompts and VLM outputs via the Harvard Dataverse Picard et al. (2024).\n9.7 AI agents for engineering design\nCurrent work on AI agents is still an emerging field. For example, one study mimics the \ntraditional engineering design process by framing it as a multi-agent system Ocker et al. \n(2025). Another system uses multiple agents to support car design workflows, assigning \ndifferent responsibilities to agents such as styling or CFD simulations Elrefaie et al. (2025). \nYet another approach explores tool-augmented agents, enabling an LLM to use engineering \ntools via a Python API to solve problems such as CAD question-answering, sketch con -\nstraining, and parameterization Mallis et al. (2025).\nWhile AI agents hold promise for assisting engineers, many commonly used tools are \nGUI-based and cannot be invoked via an API. GUI navigation, even for relatively simple \ntasks like performing a Google search, remains a significant challenge, and current agent \nframeworks are unlikely to scale to the complexity of professional engineering software. \nUnlike other UI navigation tasks, engineering workflows often require strong spatial rea -\nsoning to interpret graphs, 3D models, and other domain-specific visualizations. As a result, \nthere remains a significant gap before AI agents can robustly support complex engineering \ndesign tasks.\n9.8 Looking forward: vision-language models in engineering design\n ● Future VLMs that are meant to assist with engineering tasks must understand dimen -\nsions, scales, and spatial reasoning. The visuals in engineering tasks often provide \n1 3\nPage 67 of 75 288\nC. Picard et al.\ncritical spatial information. However, current VLMs show weak performance when in-\nterpreting images for their precise spatial information. For instance, in Sect. 7, which \nexplores spatial reasoning tasks, GPT-4V performed with about the accuracy of random \nguessing. Furthermore, in Sect. 4.2, GPT-4V was only successful in distinguishing that \na design had a blind hole rather than a through hole from an engineering drawing 11% \nof the time. As shown in Sect. 4.3, both models in this study performed poorly when \nestimating the volume fraction of an image. Understanding relative dimensions, scales \nand orientations within a visual will greatly increase a VLM’s utility for engineering \ntasks and should be an area of future development.\n ● To best serve engineering tasks, future models should be designed to process multiple \nimages and text simultaneously. Many engineering tasks require viewing visuals in con-\ntext with each other. This allows engineers to cross-reference charts, compare designs, \netc. The most useful VLMs will also have this functionality so that they can more seam-\nlessly integrate into an existing engineering workflow.\n ● A useful model must understand and follow instructions. Especially if an engineer is \nusing a VLM for the benefit of automation at large scales, this benefit is undermined \nif the VLM does not properly follow instructions. For example, if instructed to select \nfrom options A, B, and C, the model should not generate a fourth option D. This is a \nproblem that arose in Sect. 3 in both matching design descriptions and answering the \ntriplet similarity questions.\n ● Engineers should be aware that current models may require users to iteratively adjust \ntheir prompts and images to obtain desired results. For certain tasks, this iteration and \ncorrection of the VLM may render its benefits moot. For other tasks, iterative feedback \ncould be useful for exploring different solution strategies, or for ensuring that the user \nhas mastery of a subject, and can therefore correct a VLM. The need for iteration is \ndemonstrated in Makatura et al. (2024), which shows that LLMs have great potential to \ngenerate valid CAD scripts, however these results require users to iteratively guide the \nmodel to a desired output.\n ● If a VLM is unsure of an answer, it has two primary options for its response: indicate \nthat it is unsure and perhaps request more information, or make up a response that \nsounds correct. For engineers, we propose that a more useful VLM will indicate that \nit is unsure and ask for additional information. This occurred in Sect. 3 when trying to \nmake a Pugh chart for concept selection. In this case the model requests more informa-\ntion about each design. In one trial, GPT-4V stated “Since I cannot physically interact \nwith the concepts to evaluate them against the criteria you’ve provided, such as “Ease \nof handling” or “Durability,” I am unable to make informed decisions about the scores. \nThese criteria often require subjective assessments or empirical testing that cannot be \nperformed without direct interaction with the designs. However, I can guide you on how \nyou might consider each criterion for scoring:...” This indicates that practitioners should \nbe prepared to engage in an iterative information exchange with AI tools.\nWhile this work investigates GPT-4V in-depth, we proposed a set of benchmark cases that \ncould be used to assess other current and future models and that could inform about their \nperformance for engineering design. In particular, we provide our datasets, prompts, and \nspecific questions, where applicable. To demonstrate this work’s utility for benchmarking \nother VLMs, we perform comparative experiments with LLaV A  1.6 34B, and show the \n1 3\n288 Page 68 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nresults in Table 20. Other open-source vision language models have been released recently, \nsuch as Fuyu-8B (Bavishi et al. 2023). Unfortunately, most of these models have not yet \nbeen fine-tuned and aligned to the level of GPT-4V , making them less practical out-of-the-\nbox. As we have shown in Sect. 8, LLaV A 1.6 34B obtains slightly worse results across all \nof the quantitative experiments in this work. These open-source models, however, can be \nparticularly useful as base models to develop custom fine-tuned models, for example, to \nspecifically target the needs of engineers.\n9.9 Limitations of this study\nEvaluating the performance of GPT-4V faces the same challenges and concerns raised in \nprevious studies of LLMs (Mao et al. 2023). Below, we highlight a few of the limitations.\nSpecificity of Engineering Problems: While we attempted to cover a wide range of engi-\nneering tasks, the study still focuses on a subset of engineering design problems. This could \nlimit the applicability of the findings to other challenges encountered in the broad spectrum \nof the field.\nDependency on Prompt Engineering: The results might be highly sensitive to how \nprompts are engineered. Subtle variations in prompt structure or wording could lead to \nmarkedly different responses from the model, affecting the reliability of the evaluations. As \ndetailed in the Discussion, there are numerous prompting techniques that can be explored to \nimprove results, as well as experiments that vary the quantity and quality of input images. \nHowever, we note that our contribution is a benchmark, with released datasets of prompts, \nimages, and outputs, designed to enable future research to compare against our work under \nconsistent conditions.\nDataset Representativeness: The selected benchmark datasets, their quality, diversity, \nand representativeness can significantly impact the model’s performance and our results. \nWhile we created a large set of evaluation problems for VLMs, we recognize that these data-\nsets might not fully capture the diversity and complexity of real-world engineering scenar -\nios. This could affect the generalizability of the results to practical engineering applications.\nBlack-box and evolving models: Model changes, including data leaks, and the lack of \ncontrol when using the chat interface, mean that we cannot fully define the experimen -\ntal environment and some of the results may be different if reevaluated. However, for the \nassessments, we strived to create larger benchmarks within the limitation of the chat inter -\nface and repeated the experiments to obtain a better sample of the model’s performance. \nFurther, since AI models are frequently updated, our results may not hold for long. Yet, \nwhile new VLMs will enable new capabilities, we believe that this study provides a lot of \nvalue by demonstrating the tasks that future models should be evaluated on as well as pro -\nviding these tasks in our datasets. We release all our quantitative datasets to measure how \nmuch future models improve for different engineering problems.\nHuman-AI Interaction: An important part of the engineering design process involves \nhow humans interact with design. In this study, we did not test the capabilities of how \nhuman designers may interact with VLMs and how this interaction influences the problem-\nsolving process is crucial, as human biases, trust, and interpretations can affect the out -\ncomes (Zhang et al. 2023).\nIn conclusion, while this study offers valuable insights into the capabilities of VLMs \nin addressing engineering design problems, it is essential to recognize these limitations as \n1 3\nPage 69 of 75 288\nC. Picard et al.\nan integral part of our findings. They highlight the areas needing further exploration and \nremind us of the cautious approach required when generalizing AI capabilities to broader \nreal-world applications. Our research is a step in an ongoing journey, contributing to the \nevolving dialogue on the role and effectiveness of AI in complex, multifaceted fields like \nengineering design.\n10 Conclusion and future work\nThe first avenue for future research following this study involves expanding the scope and \ndepth of the engineering problems evaluated. This can be achieved by incorporating a wider \nvariety of engineering challenges, particularly from domains that were less represented in \nour initial study. We believe that industry can play a crucial role in this step, by provid -\ning representative problems for different types of engineering design tasks faced by them. \nSuch diversity in problem selection will provide a more comprehensive understanding of \nVLM’s capabilities across the engineering design process. Alongside this, there is a need for \nenhanced dataset curation. Developing more robust datasets that closely mirror complex, \nreal-world engineering scenarios can significantly improve the model’s evaluation. These \ndatasets should capture the multifaceted and multidimensional nature of engineering tasks, \nallowing for a more nuanced assessment of multimodal VLMs’ applicability and effective-\nness. An effort should also be made to avoid publicly available datasets, to limit challenges \nwith evaluation data leaking into future model training.\nAnother critical area of future work lies in the realm of human-AI collaboration. It is \nimperative to study how engineers interact with VLMs in real-world design scenarios. Such \nstudies can shed light on practical utility, user trust, and the integration of AI into engineer-\ning workflows. This includes understanding how engineers’ biases and decision-making \nprocesses interact with AI-generated solutions. Additionally, conducting longitudinal stud-\nies to monitor the impact of model evolution over time on its performance in engineering \ntasks will be highly beneficial. Given the rapid developments in AI, understanding how \nupdates and changes affect its applicability and effectiveness is crucial. This will help in \nkeeping the AI applications in engineering up-to-date and relevant, ensuring that they con -\ntinue to meet the evolving demands of the field.\nAuthor contributions C.P. and K.M.E supervised the projects, assisted all co-authors in developing their \nsections, and wrote the introduction and the discussion. C.P. developed the spatial reasoning experiments and \nthe evaluation of LLaV A 1.6. K.M.E developed the conceptual design experiments. A.C.D. developed the \nsystem-level and detail design, the design for additive manufacturing, and the post-manufacturing inspection \nsections. B.M. developed the textbook problems section. G.G. developed the topology optimization section. \nM.F.A. developed the subtractive manufacturing section. F.A. provided guidance across all sections and \nassisted in manuscript preparation and editing. All authors discussed the results and contributed to the final \nmanuscript.\nFunding 'Open Access funding provided by the MIT Libraries'. This study was supported by Swiss National \nScience Foundation (Postdoc.Mobility Fellowship P500PT_206937) and National Science Foundation Grad-\nuate Research Fellowship.\nData availability Data needed for future evaluations, including the input images, input prompts, and answers \nfor all quantitative experiments is available online: https://doi.org/10.7910/DVN/FLHZQE\n1 3\n288 Page 70 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nDeclarations\nCompeting interests The authors declare no competing interests.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons \nlicence, and indicate if changes were made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. \nIf material is not included in the article’s Creative Commons licence and your intended use is not permitted \nby statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the \ncopyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\nAgarwal S, Wills J, Cayton L, Lanckriet G, Kriegman D, Belongie S (2007) Generalized non-metric multi -\ndimensional scaling. In: Proceedings of the eleventh international conference on artificial intelligence \nand statistics. Proceedings of machine learning research, vol 2, pp 11–18, San Juan, Puerto Rico, 21–24 \nMarch 2007.  h t t p s :  / / p r o  c e e d i n  g s . m  l r . p r  e s s / v  2 / a g a r  w a l 0  7 a . h t m l\nAhmed F, Ramachandran SK, Fuge M, Hunter S, Miller S (2018) Interpreting idea maps: pairwise compari-\nsons reveal what makes ideas novel. J Mech Des 141(2):021102, 12. https://doi.org/10.1115/1.4041856\nAlawwad HA, Alhothali A, Naseem U, Alkhathlan A, Jamal A (2024) Enhancing textbook question answer-\ning task with large language models and retrieval augmented generation. SSRN 4761601\nAmabile TM (1982) Social psychology of creativity: a consensual assessment technique. J Pers Soc Psychol \n43(5):997. https://doi.org/10.1037/0022-3514.43.5.997\nArkoudas K (2023) GPT-4 can’t reason. https://arxiv.org/abs/2308.03762\nAshby MF (2016) Materials selection in mechanical design. Elsevier, Amsterdam\nAttaran M (2017) The rise of 3-D printing: the advantages of additive manufacturing over traditional manu-\nfacturing. Bus Horiz 60(5):677–688. https://doi.org/10.1016/j.bushor.2017.05.011\nBadini S, Regondi S, Frontoni E, Pugliese R (2023) Assessing the capabilities of chatgpt to improve additive \nmanufacturing troubleshooting. Adv Ind Eng Polym Res 6(3):278–287.  h t t p s : / / d o i . o r g / 1 0 . 1 0 1 6 / j . a i e p r \n. 2 0 2 3 . 0 3 . 0 0 3       \nBaer J, Kaufman JC (2019) Assessing creativity with the consensual assessment technique. In: The Palgrave \nhandbook of social creativity research. Springer, pp 27–37. https://doi.org/10.1007/978-3-319-95498-1_3\nBavishi R, Elsen E, Hawthorne C, Nye M, Odena A, Somani A, Taşırlar S (2023) Introducing our multimodal \nmodels. https://www.adept.ai/blog/fuyu-8b\nBendsøe MP (1989) Optimal shape design as a material distribution problem. Struct Optim 1(4):193–202. \nhttps://doi.org/10.1007/BF01650949\nBendsøe MP, Kikuchi N (1988) Generating optimal topologies in structural design using a homogenization \nmethod. Comput Methods Appl Mech Eng 71(2):197–224. https://doi.org/10.1016/0045-7825(88)90086-2\nBerkowitz M, Stern E (2018) Which cognitive abilities make the difference? Predicting academic achieve -\nments in advanced STEM studies. J Intell 6(4):48. https://doi.org/10.3390/jintelligence6040048\nBerkowitz M, Gerber A, Thurn CM, Emo B, Hoelscher C, Stern E (2021) Spatial abilities for architecture: \ncross sectional and longitudinal assessment with novel and existing spatial ability tests. Front Psychol \n11:4096. https://doi.org/10.3389/fpsyg.2020.609363\nBrown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, Neelakantan A, Shyam P, Sastry G, Askell \nA, Agarwal S, Herbert-V oss Ariel, Krueger G, Henighan T, Child R, Ramesh A, Ziegler DM, Wu J, \nWinter C, Hesse C, Chen M, Sigler E, Litwin M, Gray S, Chess B, Clark J, Berner C, McCandlish S, \nRadford Alec, Sutskever I, Amodei Dario (2020) Language models are few-shot learners. In: Proceed-\nings of the 34th international conference on neural information processing systems, NIPS ’20. Curran \nAssociates, Red Hook\nBryant C, Stone R, Mcadams D, Kurtoglu T, Campbell M (2005) Concept generation from the functional \nbasis of design. In: Proceedings ICED 05, the 15th international conference on engineering design, 01\nBubeck S, Chandrasekaran V , Eldan R, Gehrke J, Horvitz E, Kamar E, Lee P, Lee YT, Li Y , Lundberg S, Nori \nH, Palangi H, Ribeiro MT, Zhang Y (2023) Sparks of artificial general intelligence: early experiments \nwith GPT-4, April 2023. https://arxiv.org/abs/2303.12712\n1 3\nPage 71 of 75 288\nC. Picard et al.\nBudinoff HD (2019) Geometric manufacturability analysis for additive manufacturing. PhD thesis, Univer -\nsity of California, Berkeley\nBudinoff HD, McMains S (2021) Will it print: a manufacturability toolbox for 3D printing. Int J Interact Des \nManuf 15(4):613–630. https://doi.org/10.1007/s12008-021-00786-w\nBuehler MJ (2023) MeLM, a generative pretrained language modeling framework that solves forward and \ninverse mechanics problems. J Mech Phys Solids 181:105454.  h t t p s : / / d o i . o r g / 1 0 . 1 0 1 6 / j . j m p s . 2 0 2 3 . 1 0 \n5 4 5 4       \nCai J, Yuan Y , Sui X, Lin Y , Zhuang K, Yun X, Zhang Q, Ukrainczyk N, Xie T (2024) Chatting about chatgpt: \nhow does chatgpt 4.0 perform on the understanding and design of cementitious composite? Constr Build \nMater 425:135965.  h t t p s :  / / d o i  . o r g / 1  0 . 1 0  1 6 / j .  c o n b u  i l d m a t  . 2 0 2  4 . 1 3 5 9 6 5\nCao W, Robinson T, Hua Y , Boussuge F, Colligan AR, Pan W (2020) Graph representation of 3D CAD mod-\nels for machining feature recognition with deep learning. In: vol 11A: 46th Design automation confer -\nence (DAC) of International design engineering technical conferences and computers and information \nin engineering conference, August 2020. https://doi.org/10.1115/DETC2020-22355\nChang Y , Wang X, Wang J, Wu Y , Zhu K, Chen H, Yang L, Yi X, Wang C, Wang Y , Ye W, Zhang Y , Chang Y , \nYu PS, Yang Q, Xie X (2024) A survey on evaluation of large language models. ACM Trans Intell Syst \nTechnol (TIST). Accepted\nChun J-H, Kim S-G (2004) 2.008 Design and manufacturing II, Spring 2004. Massachusetts Institute of \nTechnology: MIT OpenCouseWare.  h t t p s :  / / o c w  . m i t . e  d u / c  o u r s e  s / 2 - 0  0 8 - d e s  i g n -  a n d - m  a n u f a  c t u r i n  g - i i  - s p \nr i n g - 2 0 0 4. Accessed 20 Oct 2023\nCorbett J, Crookall JR (1986) Design for economic manufacture. CIRP Ann 35(1):93–97.  h t t p s :  / / d o i  . o r g / 1  0 \n. 1 0  1 6 / S 0 0 0 7 - 8 5 0 6 ( 0 7 ) 6 1 8 4 6 - 0\nCseh GM, Jeffries KK (2019) A scattered CAT: a critical evaluation of the consensual assessment technique \nfor creativity research. Psychol Aesthet Creat Arts 13(2):159. https://doi.org/10.1037/aca0000220\nDas M, Yang MC (2022) Assessing early stage design sketches and reflections on prototyping. J Mech Des \n144(4):041403, 02. https://doi.org/10.1115/1.4053463\nDong Q, Li L, Dai D, Zheng C, Ma J, Li R, Xia H, Xu J, Wu Z, Chang B, Sun X, Li L, Sui Z (2024) A survey \non in-context learning. In: Al-Onaizan Y , Bansal M, Chen Y-N (eds) Proceedings of the 2024 conference \non empirical methods in natural language processing, Miami, Florida, USA, November. Association for \nComputational Linguistics, pp 1107–1128.  h t t p s :  / / d o i  . o r g / 1  0 . 1 8  6 5 3 / v  1 / 2 0 2  4 . e m n l  p - m a  i n . 6 4.  h t t p s : / / a c \nl a n t h o l o g y . o r g / 2 0 2 4 . e m n l p - m a i n . 6 4       \nDoris AC, Grandi D, Tomich R, Alam MF, Ataei M, Cheong H, Ahmed F (2025) DesignQA: a multimodal \nbenchmark for evaluating large language models’ understanding of engineering documentation. J Com-\nput Inf Sci Eng 25(2):021009\nDrela M, Hall S, Lagace PA, Lundqvist IK, Naeser G, Perry H, Radovitzky R, Waitz IA (2005a) Unified \nengineering I, II, III, & IV (supplementary notes for lectures m17-m20). Massachusetts Institute of \nTechnology: MIT OpenCouseWare.  h t t p s :  / / o c w  . m i t . e  d u / c  o u r s e  s / 1 6 -  0 1 - u n i  fi  e d  - e n g i  n e e r i  n g - i - i  i - i i  i - i v -  f \na l l -  2 0 0 5 - s  p r i n  g - 2 0 0 6 / r e s o u r c e s / z m 1 7 _ 2 0 /. Accessed 18 Nov 2023\nDrela M, Hall S, Lagace PA, Lundqvist IK, Naeser G, Perry H, Radovitzky R, Waitz IA, Young P, Craig JL \n(2005b) Unified engineering I, II, III, & IV (lecture notes). Massachusetts Institute of Technology: MIT \nOpenCouseWare.  h t t p s :  / / o c w  . m i t . e  d u / c  o u r s e  s / 1 6 -  0 1 - u n i  fi  e d  - e n g i  n e e r i  n g - i - i  i - i i  i - i v -  f a l l -  2 0 0 5 - s  p r i n  g - 2 0 \n0 6 / r e s o u r c e s / z m 2 1 /. Accessed 18 Nov 2023\nEdwards KM, Tehranchi F, Miller SR, Ahmed F (2025) AI judges in design: statistical perspectives on \nachieving human expert equivalence with vision-language models. https://arxiv.org/abs/2504.00938\nEdwards KM, Peng A, Miller SR, Ahmed F (2021) If a picture is worth 1000 words, is a word worth 1000 fea-\ntures for design metric estimation? J Mech Des 144(4):041402, 12. https://doi.org/10.1115/1.4053130\nElrefaie M, Qian J, Wu R, Chen Q, Dai A, Ahmed F (2025) AI agents in engineering design: a multi-agent \nframework for aesthetic and aerodynamic car design. https://arxiv.org/abs/2503.23315\nFeng TH, Denny P, Wuensche B, Luxton-Reilly A, Hooper S (2024) More than meets the ai: Evaluating the \nperformance of gpt-4 on computer graphics assessment questions. In: Proceedings of the 26th Austral-\nasian computing education conference, ACE ’24, New York, NY , USA. Association for Computing \nMachinery, pp 182–191. https://doi.org/10.1145/3636243.3636263\nFrey D, Gossard D (2009) 2.007 Design and manufacturing I, Spring 2009. Massachusetts Institute of Tech-\nnology: MIT OpenCouseWare.  h t t p s :  / / o c w  . m i t . e  d u / c  o u r s e  s / 2 - 0  0 7 - d e s  i g n -  a n d - m  a n u f a  c t u r i n  g - i -  s p r i n g \n- 2 0 0 9. Accessed 20 Oct 2023\nGao S, Shah JJ (1998) Automatic recognition of interacting machining features based on minimal condition \nsubgraph. Comput Aided Des 30(9):727–739.  h t t p s :  / / d o i  . o r g / 1  0 . 1 0  1 6 / S 0 0 1 0 - 4 4 8 5 ( 9 8 ) 0 0 0 3 3 - 5\nGerhard P, Wolfgang B, Joerg F, Karl-Heinrich G (2007) Engineering design: a systematic approach. \nSpringer, London. https://doi.org/10.1007/978-1-84628-319-2\nHenderson K (1999) On line and on paper: visual representations, visual culture, and computer graphics in \ndesign engineering. Inside Technology. MIT Press, Cambridge\n1 3\n288 Page 72 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nHubs (2023) What are the key design elements for 3D printing?  h t t p s :  / / w w w  . h u b s .  c o m /  k n o w l  e d g e -  b a s e / k  e \ny - d  e s i g n  - c o n s  i d e r a t  i o n s  - 3 d - p r i n t i n g /. Accessed 18 Nov 2023\nJohn B, Sharon SM (2009) Assessing creativity using the consensual assessment technique. In: Handbook \nof research on assessment technologies, methods, and applications in higher education. IGI Global, pp \n65–77.  h t t p s :  / / d o i  . o r g / 1  0 . 4 0  1 8 / 9 7  8 - 1 - 6  0 5 6 6 - 6  6 7 - 9  . c h 0 0 4\nKatz DM, Bommarito MJ, Gao S, Arredondo P (2023) GPT-4 passes the bar exam. SSRN 4389233.  h t t p s : / / \nd o i . o r g / 1 0 . 2 1 3 9 / s s r n . 4 3 8 9 2 3 3       \nLewis P, Perez E, Piktus A, Petroni F, Karpukhin V , Goyal N, Küttler H, Lewis M, Yih W, Rocktäschel T et \nal (2020) Retrieval-augmented generation for knowledge-intensive nlp tasks. Adv Neural Inf Process \nSyst 33:9459–9474\nLiu F, Guan T, Li Z, Chen L, Yacoob Y , Manocha D, Zhou T (2023a) HallusionBench: You see what you \nthink? or you think what you see? an image-context reasoning benchmark challenging for GPT-\n4V(ision), LLaV A-1.5, and other multi-modality models. URL https://arxiv.org/abs/2310.14566\nLiu H, Li C, Li Y , Lee YJ (2023b) Improved Baselines with Visual Instruction Tuning, 10.  h t t p s : / / a r x i v . o r g \n/ a b s / 2 3 1 0 . 0 3 7 4 4       \nLiu Y , Duan H, Zhang Y , Li B, Zhnag S, Zhao W, Yuan Y , Wang J, He C, Liu Z, Chen K, Lin D (2023c) \nMmbench: is your multi-modal model an all-around player? arXiv:2307.06281\nLiu H, Li C, Li Y , Li B, Zhang Y , Shen S, Lee YJ (2024) Llava-next: improved reasoning, ocr, and world \nknowledge, January.  h t t p s :  / / l l a  v a - v l .  g i t h  u b . i o  / b l o g  / 2 0 2 4 -  0 1 - 3  0 - l l a v a - n e x t /\nLuo M, Xu X, Liu Y , Pasupat P, Kazemi M (2024) In-context learning with retrieved demonstrations for \nlanguage models: a survey. https://arxiv.org/abs/2401.11624\nMaeda Y , Yoon SY (2013) A meta-analysis on gender differences in mental rotation ability measured by the \nPurdue spatial visualization tests: visualization of rotations (PSVT:R). Educ Psychol Rev 25(1):69–94. \nhttps://doi.org/10.1007/s10648-012-9215-x\nMakatura L, Foshey M, Wang B, Hähnlein F, Ma P, Deng B, Tjandrasuwita M, Spielberg A, Owens CE, \nChen PY , Zhao A, Zhu A, Norton WJ, Gu E, Jacob J, Li Y , Schulz A, Matusik W (2024) Large language \nmodels for design and manufacturing. An MIT exploration of generative AI, March 27.  h t t p s : / / m i t - g e n \na i . p u b p u b . o r g / p u b / n m y p m n h s       \nMallis, D, Karadeniz AS, Cavada S, Rukhovich D, Foteinopoulou N, Cherenkova K, Kacem A, Aouada D \n(2025) CAD-assistant: tool-augmented vllms as generic cad task solvers.  h t t p s : / / a r x i v . o r g / a b s / 2 4 1 2 . 1 3 \n8 1 0       \nManyika J, Hsiao S (2023) An overview of Bard: an early experiment with generative AI.  h t t p s :  / / a i .  g o o g l e  / s \nt a  t i c / d  o c u m e  n t s / g o  o g l e  - a b o u t - b a r d . p d f\nMao, R, Chen G, Zhang X, Guerin F, Cambria E (2023) GPTEval: a survey on assessments of ChatGPT and \nGPT-4. https://arxiv.org/abs/2308.12488\nMiller SR, Hunter ST, Starkey E, Ramachandran S, Ahmed F, Fuge M (2021) How should we measure \ncreativity in engineering design? A comparison between social science and engineering approaches. J \nMech Des 143(3):031404, 01. https://doi.org/10.1115/1.4049061\nMundt M, Majumder S, Murali S, Panetsos P, Ramesh V (2019) Meta-learning convolutional neural archi -\ntectures for multi-target concrete defect classification with the concrete defect bridge image dataset. In: \nProceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp 11196–11205\nNandy A, Goucher-Lambert K (2022) Do human and computational evaluations of similarity align? An \nempirical study of product function. J Mech Des 144(4):041404, 03. https://doi.org/10.1115/1.4053858\nNelson MD, Goenner BL, Gale BK (2023) Utilizing ChatGPT to assist CAD design for microfluidic devices. \nLab Chip 23(17):3778–3784. https://doi.org/10.1039/D3LC00518F\nNori H, King N, McKinney SM, Carignan D, Horvitz E (2023) Capabilities of GPT-4 on medical challenge \nproblems. https://arxiv.org/abs/2303.13375\nNyemba WR (2022) Computer aided design: engineering design and modeling using AutoCAD. CRC Press, \nBoca Raton. https://doi.org/10.1201/9781003288626\nOcker F, Menzel S, Sadik A, Rios T (2025) From idea to CAD: a language model-driven multi-agent system \nfor collaborative design. https://arxiv.org/abs/2503.04417\nOkudan GE, Tauhid S (2009) Concept selection methods—a literature review from 1980 to 2008. Int J Des \nEng 2(3):243–277. https://doi.org/10.1504/IJDE.2008.023764\nOpenAI (2023) GPT-4V(ision) system card. Technical report, OpenAI\nPicard C (2023) MechE rotation test, 11. ETH, Zurich\nPicard C, Edwards KM, Doris AC, Man B, Giannone G, Alam MF, Ahmed F (2024) Data for evaluating \nvision-language models for engineering design. https://doi.org/10.7910/DVN/FLHZQE\nPugh S (1991) Total design. Addison-Wesley, Boston\nPugh S (1995) Concept selection—a method that works. In: International conference of engineering design, \npp 497–506\n1 3\nPage 73 of 75 288\nC. Picard et al.\nQuaiser-Pohl C (2003) The mental cutting test “Schnitte” and the picture rotation test-two new measures to \nassess spatial ability. Int J Test 3(3):219–231. https://doi.org/10.1207/S15327574IJT0303_2\nRosoł M, Gąsior JS, Łaba J, Korzeniewski K, Młyńczak M (2023) Evaluation of the performance of GPT-3.5 \nand GPT-4 on the polish medical final examination. Sci Rep 13(1):20512.  h t t p s : / / d o i . o r g / 1 0 . 1 0 3 8 / s 4 1 5 \n9 8 - 0 2 3 - 4 6 9 9 5 - z     . (ISSN 2045-2322)\nSaka A, Taiwo R, Saka N, Salami B, Ajayi S, Akande K, Kazemi H (2023) GPT models in construction indus-\ntry: opportunities, limitations, and a use case validation. https://arxiv.org/abs/2305.18997\nSaravi M, Newnes L, Mileham AR, Goh YM (2008) Estimating cost at the conceptual design stage to optimize \ndesign in terms of performance and cost. In: Collaborative product and service life cycle management \nfor a sustainable world. Springer, London, pp 123–130. https://doi.org/10.1007/978-1-84800-972-1_11\nShah JJ, Smith SM, Vargas-Hernandez N (2003) Metrics for measuring ideation effectiveness. Des Stud \n24(2):111–134.  h t t p s :  / / d o i  . o r g / 1  0 . 1 0  1 6 / S 0 1 4 2 - 6 9 4 X ( 0 2 ) 0 0 0 3 4 - 0\nShea DL, Lubinski D, Benbow CP (2001) Importance of assessing spatial ability in intellectually talented \nyoung adolescents: a 20-year longitudinal study. J Educ Psychol 93(3):604–614.  h t t p s : / / d o i . o r g / 1 0 . 1 0 \n3 7 / 0 0 2 2 - 0 6 6 3 . 9 3 . 3 . 6 0 4       \nShi Y , Peng D, Liao W, Lin Z, Chen X, Liu C, Zhang Y , Jin L (2023) Exploring OCR capabilities of GPT-\n4V(ision): a quantitative and in-depth evaluation. https://arxiv.org/abs/2310.16809\nSiddharth L, Blessing L, Luo J (2022) Natural language processing in-and-for design research. Des Sci 8:e21. \nhttps://doi.org/10.1017/dsj.2022.16\nSigmund O, Maute K (2013) Topology optimization approaches. Struct Multidiscip Optim 48(6):1031–1055. \nhttps://doi.org/10.1007/s00158-013-0978-6\nSiriwardhana S, Weerasekera R, Wen E, Kaluarachchi T, Rana R, Nanayakkara S (2023) Improving the \ndomain adaptation of retrieval augmented generation (RAG) models for open domain question answer-\ning. Trans Assoc Comput Ling 11:1–17\nSong B, Miller S, Ahmed F (2023a) Attention-enhanced multimodal learning for conceptual design evalua -\ntions. J Mech Des 145(4):041410, 02. https://doi.org/10.1115/1.4056669\nSong B, Zhou R, Ahmed F (2023b) Multi-modal machine learning in engineering design: a review and future \ndirections. https://arxiv.org/abs/2302.10909\nStarkey E, Toh CA, Miller SR (2016) Abandoning creativity: the evolution of creative ideas in engineering \ndesign course projects. Des Stud 47:47–72. https://doi.org/10.1016/j.destud.2016.08.003\nStechly K, Marquez M, Kambhampati S (2023) GPT-4 doesn’t know it’s wrong: an analysis of iterative \nprompting for reasoning problems, 10. https://arxiv.org/abs/2310.12397\nStella F, Santina CD, Hughes J (2023) How can LLMs transform the robotic design process? Nat Mach Intell \n5(6):561–564. https://doi.org/10.1038/s42256-023-00669-7\nSu H, Song B, Ahmed F (2023) Multi-modal machine learning for vehicle rating predictions using image, \ntext, and parametric data. In: Proceedings of the international design engineering technical conferences \n& computers and information in engineering conference, Boston, MA. ASME\nTakagi S, Watari T, Erabi A, Sakaguchi K (2023) Performance of GPT-3.5 and GPT-4 on the Japanese medi-\ncal licensing examination: comparison study. JMIR Med Educ 9:e48002. https://doi.org/10.2196/48002. \nURL https://mededu.jmir.org/2023/1/e48002\nTaraban R (2011) Information fluency growth through engineering curricula: analysis of Students’ text-pro-\ncessing skills and beliefs. J Eng Educ 100(2):397–416.  h t t p s :  / / d o i  . o r g / 1  0 . 1 0  0 2 / j .  2 1 6 8 -  9 8 3 0 . 2  0 1 1 .  t b 0 0 \n0 1 9 . x\nToh CA, Miller SR (2016) Choosing creativity: the role of individual risk and ambiguity aversion on creative \nconcept selection in engineering design. Res Eng Des 27:195–219.  h t t p s : / / d o i . o r g / 1 0 . 1 0 0 7 / s 0 0 1 6 3 - 0 1 \n5 - 0 2 1 2 - 1       \nToh C, Miller SR (2019) Does the preferences for creativity scale predict engineering students’ ability to \ngenerate and select creative design alternatives? J Mech Des 141(6):062001, 04.  h t t p s : / / d o i . o r g / 1 0 . 1 1 \n1 5 / 1 . 4 0 4 2 1 5 4       \nUlrich KT, Eppinger SD, Yang MC (2020) Product design and development, 7th edn. McGraw-Hill, New \nYork\nVerhaegen P-A, Vandevenne D, Duflou JR (2012) Originality and novelty: a different universe. In: Interna -\ntional design conference—DESIGN 2012, Dubrovnik, Croatia, 21–24 May 2012, pp 1961–1966\nWang X, Hu Z, Lu P, Zhu Y , Zhang J, Subramaniam S, Loomba AR, Zhang S, Sun Y , Wang W (2023) Sci-\nBench: evaluating college-level scientific problem-solving abilities of large language models.  h t t p s : / / a \nr x i v . o r g / a b s / 2 3 0 7 . 1 0 6 3 5       \nWebb C (2008) 45nm design for manufacturing. Intel Technol J 12(2):121–130\nWen L, Yang X, Fu D, Wang X, Cai P, Li X, Ma T, Li Y , Xu L, Shang D, Zhu Z, Sun S, Bai Y , Cai X, Dou M, \nHu S, Shi B (2023) On the road with GPT-4V(ision): early explorations of visual-language model on \nautonomous driving. https://arxiv.org/abs/2311.05332\n1 3\n288 Page 74 of 75\nFrom concept to manufacturing: evaluating vision-language models for…\nWoldseth RV , Niels Aage J, Bærentzen A, Sigmund O (2022) On the use of artificial neural networks in topol-\nogy optimisation. Struct Multidiscip Optim 65(10):294. https://doi.org/10.1007/s00158-022-03347-1\nXiong G, Jin Q, Zhiyong L, Zhang A (2024) Benchmarking retrieval-augmented generation for medicine. In: \nFindings of the Association for Computational Linguistics ACL 2024, pp 6233–6251\nYan F, Wang L, Li L (2020) Conceptual design scheme automatic generation and decision-making consider-\ning green demand. Procedia Manuf 43:407–414. https://doi.org/10.1016/j.promfg.2020.02.194\nYang S, Zhao YF (2015) Additive manufacturing-enabled design theory and methodology: a critical review. \nInt J Adv Manuf Technol 80:327–342. https://doi.org/10.1007/s00170-015-6994-5\nYang Z, Li L, Lin K, Wang J, Lin C-C, Liu Z, Wang L (2023b) The dawn of LLMs: Preliminary explorations \nwith GPT-4V(ision). URL https://arxiv.org/abs/2309.17421\nYang J, Zhang H, Li F, Zou X, Li C, Gao J (2023a) Set-of-mark prompting unleashes extraordinary visual \ngrounding in GPT-4V , 10. https://arxiv.org/abs/2310.11441\nYoon SY (2011) Psychometric properties of the revised purdue spatial visualization tests: visualization of \nrotations (the revised PSVT:R). PhD thesis, Purdue University\nYuan C, Marion T, Moghaddam M (2021) Leveraging end-user data for enhanced design concept evaluation: \na multimodal deep regression model. J Mech Des 144(2):021403, 09. https://doi.org/10.1115/1.4052366\nZhang Z, Jaiswal P, Rai R (2018) FeatureNet: machining feature recognition based on 3D convolution neural \nnetwork. Comput Aided Des 101:12–22. https://doi.org/10.1016/j.cad.2018.03.006\nZhang G, Raina A, Brownell E, Cagan J (2023) Artificial intelligence impersonating a human: the impact of \ndesign facilitator identity on human designers. J Mech Des 145(5):051404, 01.  h t t p s : / / d o i . o r g / 1 0 . 1 1 1 5 \n/ 1 . 4 0 5 6 4 9 9       \nZheng L, Chiang W-L, Sheng Y , Zhuang S, Wu Z, Zhuang Y , Lin Z, Li Z, Li D, Xing E, Zhang H, Gonzalez \nJE, Stoica I (2023) Judging llm-as-a-judge with mt-bench and chatbot arena. In: Oh A, Naumann T, \nGloberson A, Saenko K, Hardt M, Levine S (eds) Advances in neural information processing systems, \nvol 36. Curran Associates, pp 46595–46623.  h t t p s :   /  / p r o c e e d i n g  s .  n e u r i  p  s .  c c  / p a p  e  r _ fi    l e s /  p a  p e r  /  2 0 2 3  /  fi  l  e / \n9 1 f  1 8 a 1 2  8 7 b 3 9 8  d 3 7 8  e f 2 2 5  0 5 b f  4  1 8 3 2 -   P a p e  r - D a  t a s e  t s _ a n d _ B e n  c h m a r k s . p d f\nZhu K, Zhao Q, Chen H, Wang J, Xie X (2024) Promptbench: a unified library for evaluation of large lan -\nguage models. J Mach Learn Res (JMLR) 25:1–22\nZong M, Krishnamachari B (2023) Solving math word problems concerning systems of equations with GPT \nmodels. Mach Learn Appl 14:100506. https://doi.org/10.1016/j.mlwa.2023.100506\nPublisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\n1 3\nPage 75 of 75 288"
}