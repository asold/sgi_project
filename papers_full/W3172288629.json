{
  "title": "MLPruning: A Multilevel Structured Pruning Framework for Transformer-based Models",
  "url": "https://openalex.org/W3172288629",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5102915910",
      "name": "Zhewei Yao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5084195463",
      "name": "Linjian Ma",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100784818",
      "name": "Sheng Shen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5047285420",
      "name": "Kurt Keutzer",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5033006662",
      "name": "Michael W. Mahoney",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3111747337",
    "https://openalex.org/W2805003733",
    "https://openalex.org/W2924902521",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W3024171804",
    "https://openalex.org/W2894740066",
    "https://openalex.org/W2945767825",
    "https://openalex.org/W2767785892",
    "https://openalex.org/W3017022649",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3000514857",
    "https://openalex.org/W3098576111",
    "https://openalex.org/W3125953778",
    "https://openalex.org/W2964233199",
    "https://openalex.org/W2979314664",
    "https://openalex.org/W3015298864",
    "https://openalex.org/W3005692288",
    "https://openalex.org/W2963145730",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W3114304470",
    "https://openalex.org/W3104263050",
    "https://openalex.org/W2916954108",
    "https://openalex.org/W2754526845",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2974875810",
    "https://openalex.org/W3015233032",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2947946877",
    "https://openalex.org/W2915589364",
    "https://openalex.org/W2952344559",
    "https://openalex.org/W2969515962",
    "https://openalex.org/W3104216863",
    "https://openalex.org/W3020268419",
    "https://openalex.org/W3022969335",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W3036463250",
    "https://openalex.org/W2607892599",
    "https://openalex.org/W2948130861"
  ],
  "abstract": "Pruning is an effective method to reduce the memory footprint and computational cost associated with large natural language processing models. However, current approaches either only explore head pruning, which has a limited pruning ratio, or only focus on unstructured pruning, which has negligible effects on the real inference time and/or power consumption. To address these challenges, we develop a novel MultiLevel structured Pruning (MLPruning) framework, which uses three different levels of structured pruning: head pruning, row pruning, and block-wise sparse pruning. We propose using a learnable Top-k threshold, which employs an adaptive regularization to adjust the regularization magnitude adaptively, to select appropriate pruning ratios for different weight matrices. We also propose a two-step pipeline to combine block-wise pruning with head/row pruning to achieve high structured pruning ratios with minimum accuracy degradation. Our empirical results show that for \\bertbase, with \\textapprox20\\% of remaining weights, \\OURS can achieve an accuracy that is comparable to the full model on QQP/MNLI/\\squad, with up to \\textapprox3.69x speedup. Our framework has been open sourced~\\cite{codebase}.",
  "full_text": "LEAP: Learnable Pruning for Transformer-based Models\nZhewei Yao* 1 Xiaoxia Wu* 1 Linjian Ma 2 Sheng Shen 3 Kurt Keutzer 3 Michael W. Mahoney4 Yuxiong He1\nAbstract\nPruning is an effective method to reduce the mem-\nory footprint and computational cost associated\nwith large natural language processing models.\nHowever, current pruning algorithms either only\nfocus on one pruning category, e.g., structured\npruning and unstructured, or need extensive hy-\nperparameter tuning in order to get reasonable ac-\ncuracy performance. To address these challenges,\nwe propose LEArnable Pruning (LEAP), an effec-\ntive method to gradually prune the model based\non thresholds learned by gradient descent. Dif-\nferent than previous learnable pruning methods,\nwhich utilize L0 or L1 penalty to indirectly affect\nthe ﬁnal pruning ratio, LEAP introduces a novel\nregularization function, that directly interacts with\nthe preset target pruning ratio. Moreover, in order\nto reduce hyperparameter tuning, a novel adaptive\nregularization coefﬁcient is deployed to control\nthe regularization penalty adaptively. With the\nnew regularization term and its associated adap-\ntive regularization coefﬁcient, LEAP is able to\nbe applied for different pruning granularity, in-\ncluding unstructured pruning, structured pruning,\nand hybrid pruning, with minimal hyperparam-\neter tuning. We apply LEAP for BERT mod-\nels on QQP/MNLI/SQuAD for different pruning\nsettings. Our result shows that for all datasets,\npruning granularity, and pruning ratios, LEAP\nachieves on-par or better results as compared to\nprevious heavily hand-tuned methods.\n1. Introduction\nSince the development of transformer models (Vaswani\net al., 2017), the number of parameters for natural lan-\nguage processing (NLP) models has become much larger,\n*Equal contribution 1Microsoft, Bellevue, WA 2University\nof Illinois Urbana-Champaign 3University of California,\nBerkeley 4International Computer Science Institute and Uni-\nversity of California, Berkeley. Correspondence to: Zhewei\nYao <zheweiyao@microsoft.com>, Xiaoxia Wu <xiaoxi-\nawu@microsoft.com>.\ne.g., BERTlarge (330M) (Devlin et al., 2019), Megatron-\nLM (8.3B) (Shoeybi et al., 2019), T5 (11B) (Raffel et al.,\n2019), GPT3 (170B) (Brown et al., 2020), and MT-NLG\n(530B) (Microsoft & Nvidia, 2021). Although larger models\ntend to exhibit better generalization ability for downstream\ntasks, the inference time and associated power consumption\nbecome critical bottlenecks for deploying those models on\nboth cloud and edge devices.\nOne of the promising approaches for addressing the infer-\nence time and power consumption issues of these large\nmodels is pruning (Sanh et al., 2020; Michel et al., 2019;\nWang et al., 2020a). As the nature of neural networks (NNs),\ndifferent pruning granularity exists, e.g., structured pruning\n(head pruning for transformers and block-wise pruning for\nweight matrices) and unstructured pruning (purely sparse-\nbased pruning). Different pruning methods are proposed,\nbut they generally only target one set of pruning granularity.\nAs such, when a new scenario comes, e.g., hybrid pruning, a\ncombination of structured pruning and unstructured pruning,\nit is unclear how to choose the proper method.\nMeanwhile, existing work sometimes sets the same pruning\nratio for all layers. However, it is challenging to prune\nthe same amount of parameters of all weights of a general\nNNs to ultra-low density without signiﬁcant accuracy loss.\nThis is because not all the layers of an NN allow the same\npruning level. A possible approach to address this is to use\ndifferent pruning ratios. A higher density ratio is needed for\ncertain “sensitive” layers of the network, and a lower density\nratio for “non-sensitive” layers. However, manually setting\nsuch multi-level pruning ratios is infeasible. Regularization\nmethod, e.g., (Sanh et al., 2020), is proposed to address\nmulti-level pruning ratio issue. However, it introduces two\ndrawbacks: (i) a careful hand-tuned threshold schedule is\nneeded to improves the performance, especially in high\nsparsity regimes; and (ii) due to the regularization term is not\ndirectly applied to the ﬁnal pruning ratio, the regularization\nmagnitude also needs heavily tuning to get desired density\nratio.\nMotivated by these issues, we propose an effective LEArn-\nable Pruning (LEAP) method to gradually prune the weight\nmatrices based on corresponding thresholds that are learned\nby gradient descent. We summarize our contributions below,\n• LEAP sets a group of learnable pruning ratio parameters,\narXiv:2105.14636v2  [cs.CL]  23 May 2022\nLearnable Pruning\nwhich can be learned by the stochastic gradient descent,\nfor the weight matrices, with a purpose to set a high prun-\ning ratio for insensitive layers and vice versa. As the NN\nprefers a high-density ratio for higher accuracy and low\nloss, we introduce a novel regularization function that can\ndirectly control the preset target pruning ratio. As such,\nLEAP can easily achieve the desired compression ratio un-\nlike those L0 or L1 penalty-based regularization methods,\nwhose target pruning ratio needs careful hyperparameter\ntuning.\n• To ease hyperparameter search, we design an adaptive reg-\nularization magnitude λreg to adaptively control the con-\ntribution to the ﬁnal loss from the regularization penalty.\nThe coefﬁcient λreg is automatically adjusted to be large\n(small) when the current pruning ratio is far away (close\nto) the target ratio.\n• We apply LEAP for BERT base on three datasets, i.e.,\nQQP/MNLI/SQuAD, under different pruning granularity,\nincluding structured, hybrid, and unstructured pruning,\nwith various pruning ratios. Our results demonstrate that\nLEAP can consistently achieve on-par or better perfor-\nmance as compared to previous heavily tuned methods,\nwith minimal hyperparameter tuning.\n• We show that LEAP is less sensitive to the hyperparame-\nters introduced by our learnable pruning thresholds, and\ndemonstrate the advance of our adaptive regularization\nmagnitude over constant magnitude. Also, by analyzing\nthe ﬁnal pruned models, two clear observations can be\nmade for BERT pruning: (1) early layers are more sensi-\ntive to pruning, which results in a higher density ratio at\nthe end; and (2) fully connected layers are more insensi-\ntive to pruning, which results in a much higher pruning\nratio than multi-head attention layers.\n2. Related Work\nDifferent approaches have been proposed to compress large\npre-trained NLP models. These efforts can be generally\ncategorized as follows: (i) knowledge distillation (Jiao et al.,\n2019; Tang et al., 2019; Sanh et al., 2019; Sun et al., 2019);\n(ii) quantization (Bhandare et al., 2019; Zafrir et al., 2019;\nShen et al., 2020; Fan et al., 2020; Zadeh et al., 2020; Zhang\net al., 2020; Bai et al., 2020; Esser et al., 2019); (iii) new\narchitecture design (Sun et al., 2020; Iandola et al., 2020;\nLan et al., 2019; Kitaev et al., 2020; Wang et al., 2020b);\nand (iv) pruning. Pruning can be broadly categorized into\nunstructured pruning (Dong et al., 2017; Lee et al., 2018;\nXiao et al., 2019; Park et al., 2020; Han et al., 2016; Sanh\net al., 2020) and structured pruning (Luo et al., 2017; He\net al., 2018; Yu et al., 2018; Lin et al., 2018; Huang & Wang,\n2018; Zhao et al., 2019; Yu et al., 2021; Michel et al., 2019).\nHere, we brieﬂy discuss the related pruning work in NLP.\nFor unstructured pruning, (Yu et al., 2019; Chen et al., 2020;\nPrasanna et al., 2020; Shen et al., 2021) explore the lottery-\nticket hypothesis (Frankle & Carbin, 2018) for transformer-\nbased models; (Zhao et al., 2020) shows that pruning is an\nalternative effective way to ﬁne-tune pre-trained language\nmodels on downstream tasks; and (Sanh et al., 2020) pro-\nposes the so-called movement pruning, which considers the\nchanges in weights during ﬁne-tuning for a better pruning\nstrategy, and which achieves signiﬁcant accuracy improve-\nments in high sparsity regimes. However, as an extension\nof (Narang et al., 2017), (Sanh et al., 2020) requires non-\ntrivial hyperparameter tuning to achieve better performance\nas well as desired pruning ratio.\nFor structured pruning, (Fan et al., 2019; Sajjad et al.,\n2020) uses LayerDrop to train the model and observes that\nsmall/efﬁcient models can be extracted from the pre-trained\nmodel; (Wang et al., 2019) uses a low-rank factorization of\nthe weight matrix and adaptively removes rank-1 compo-\nnents during training; and (Michel et al., 2019) tests head\ndrop for multi-head attention and concludes that a large\npercentage of attention heads can be removed during infer-\nence without signiﬁcantly affecting the performance. More\nrecently, (Lagunas et al., 2021) extends (Sanh et al., 2020)\nfrom unstructured pruning to block-wise structured pruning.\nAs a continuing work of (Sanh et al., 2020; Narang et al.,\n2017), hyperparameter tuning is also critical for (Lagunas\net al., 2021).\nAlthough fruitful pruning algorithms are proposed, most\nmethods generally only work for speciﬁc pruning scenarios,\ne.g., unstructured or structured pruning. Also, a lot of algo-\nrithms either (i) need a hand-tuned threshold (aka pruning\nratio) to achieve good performances; or (ii) require care-\nful regularization magnitude/schedule to control the ﬁnal\npruning ratio and retain the model quality. Our LEAP is\na general pruning algorithm that achieves on-par or even\nbetter performance under similar pruning ratio across var-\nious pruning scenarios as compared to previous methods,\nand LEAP achieves this with very minimal hyperparame-\nter tuning by introducing a new regularization term and a\nself-adaptive regularization magnitude.\n3. Methodology\nRegardless of pruning granularity, in order to prune a neu-\nral network (NN) there are two approaches: (i) one-time\npruning (Yu et al., 2021; Michel et al., 2019) and (ii) multi-\nstage pruning (Han et al., 2016; Lagunas et al., 2021). The\nmain difference between the two is that one-time pruning\ndirectly prunes the NN to a target ratio within one pruning\ncycle. However, one-time pruning oftentimes requires a\npre-trained model on downstream tasks and leads to worse\nperformance as compared to multi-stage pruning. For multi-\nstage pruning, two main categories are used: (i) one needs\nmultiple rounds for pruning and ﬁnetuing (Han et al., 2016);\nLearnable Pruning\nand (ii) another gradually increases pruning ratio within\none run (Sanh et al., 2020; Lagunas et al., 2021). Here, we\nfocus on the latter case, where the pruning ratio gradually\nincreases until it reaches the preset target.\n3.1. Background and Problems of Existing Pruning\nMethods\nAssume the NN consists of n weight matrices, W =\n{W1,...,W n}. To compress W, gradual pruning consists\nof the following two stages:\n• (S1) For each Wi, we initialize a corresponding all-one\nmask Mi and denote M= {M1,...,M n}as the whole\nset of masks.\n• (S2) We train the network with the objective function,\nmin\nW\nLpure(M⊙W ),\nwhere M⊙W means Wi ⊙Mi for all i= 1,...n , and\nLpure is the standard training objective function of the\nassociated task, e.g., the ﬁnite sum problem with cross-\nentropy loss. As the training proceeds, the mask Mi is\ngradually updated with more zero, i.e., the cardinality,\n|Mi|= si\nt at iteration t, becomes smaller.\nHere si\nt in (S2) could be a simple linear decaying function\nor more generally a polynomial function based on the user’s\nrequirement. Such method is called hard/soft-threshold\npruning. In (Zhu & Gupta, 2017; Sanh et al., 2020; La-\ngunas et al., 2021), si\nt is set to be the same across all the\nweight matrices, i.e., si\nt := st and they use a cubic sparsity\nscheduling for the target sparsity sf given a total iterations\nof tf :\nst =\n\n\n\ns0 0 ≤t<t 0,\nsf + (s0 −sf )(1 − t−(t0+tc)\ntf −(t0+tc) )3 t0 ≤t<t f −tc,\nsf t≥tc.\n(1)\nAlthough threshold methods achieve reasonably advanced\npruning ratios along with high model qualities, they also ex-\nhibit various issues. Here we dive deep into those problems.\nCommon issues Both hard- and soft- threshold pruning\nintroduce three hyperparameters: the initial sparsity value\ns0, the warmup step t0, and the cool-down steps tc. As a\ncommon practical issue, more hyperparameters need more\ntuning efforts, and the question, how to choose the hyperpa-\nrameters v0, t0 and tf , is by no means resolved.\nIssues of hard-threshold pruning It is natural for weight\nmatrices to have different tolerances/sensitivities to pruning,\nwhich means that a high pruning ratio needs to be applied\nfor insensitive layers, and vice versa. However, for hard-\nthreshold pruning, which sorts the weight in one layer by\nabsolute values and masks the smaller portion (i.e., si\nt) to\nzero, it uses the same pruning ratio across all layers. That\noftentimes leads to a sub-optimal solution for hard-threshold\npruning.\nAs such instead of using a single st schedule, a more suit-\nable way is to use different si\nt, i= 1,...n for each weight\nmatrix Wi. However, this leads the number of tuning hyper-\nparameters to be a linear function as the number of weight\nmatrices, i.e., 3n. For instance, there are 3 ×6 ×12 = 216\nhyperparameters for the popular NLP model–BERTbase, a\n12-layer encoder-only Transformer of which each layer con-\nsists of 6 weight matrices (Devlin et al., 2019). Extensively\nsearching for these many hyperparameters over a large space\nis impractical.\nExcept for the single threshold issue, the hard-threshold\nmethod is hard to extend to different pruning scenarios, e.g.,\nblock-pruning, head pruning for attention heads, and ﬁlter\npruning for fully connected layers. The reason is that the\nimportance of those structured patterns cannot be simply\ndetermined by their sum of absolute values or other norms\nsuch as the Euclidean norm.\nIssues of soft-threshold pruning One way to resolve the\nabove issues is through soft-threshold methods. Instead of\nusing the magnitude (aka absolute value) of the weight ma-\ntrix to generate the mask, soft-threshold methods introduce\na regularization (penalty) function Lreg(S) to control the\nsparsity of the weight parameters (for instance, Lp-norm,\nLreg = ∥·∥p, with p= 0or p= 1). Here, S:= {Si}n\ni=1\nand each Si refers to the associated importance score ma-\ntrix of Wi, which is learnable during training. Particularly,\n(i) this Si can be adopted to different pruning granularity,\ne.g., structured and unstructured pruning, and (ii) the ﬁnal\npruning ratio of each weight matrix can be varied thanks to\nthe learnable nature of Si.\nFor soft-threshold pruning, the mask, Mi, is generated by\nthe learnable importance score Si and st1 using the com-\nparison function, Mi = f(Si) > st.2 Where f(·) is any\nfunction that maps real values to[0, 1]. As f(Si) will prefer\nlarger values as smaller loss will be introduced to training\nprocedure, a regularization term is added to the training\nobjective,\nLobj(M⊙W ) =Lpure(M⊙W ) +λregLreg(f(S)) (2)\nThe coefﬁcient λreg is used to adjust the magnitude of the\npenalty (the larger λreg, the sparser the W). Although soft-\nthreshold pruning methods achieve better performance as\ncompared to hard-threshold pruning methods, it introduces\nanother hyperparameter λreg. More importantly, as the ﬁnal\n1When all si\nt are the same, we drop the superscript for simplic-\nity.\n2Here both the function f(·) and the comparison are element\nwise and the comparison returns either 1 or 0.\nLearnable Pruning\nsparsity is controlled indirectly by the regularization term, it\nrequires sizable laborious experiments to achieve the desired\ncompression ratio.\nShort summary For both hard- and soft- threshold prun-\ning, users have to design sparsity scheduling which raises\nhyperparameters search issues. Hard-threshold pruning can\nhardly extend to different pruning granularity, which likely\nleads to sub-optimal solutions by setting the same pruning\nratio for all layers. While soft threshold methods could be\na possible solution to resolve part of the problems, it in-\ntroduces another extra hyperparameter, λreg, and there are\ncritical concerns on how to obtain the target sparse ratio.\nWe address the above challenges in the coming section by\ndesigning learnable thresholds with (i) a simple yet effective\nregularization function that can help the users to achieve\ntheir target sparse ratio, and (ii) an adaptive regularization\nmagnitude, λreg to alleviate the hyperparameter tuning.\n3.2. LEAP with A New Regularization\nIn order to address the previously mentioned challenges\nin Section 3.1, we propose our LEArnable Pruning (LEAP)\nwith a new regularization. We denote the learnable threshold\nvector σ = [σi,...,σ n] and each σi associates with the\ntuple (Wi,Mi,Si). With a general importance score S\nand learnable threshold vector σ, LEAP can be smoothly\nincorporated to Top-k pruning method (Zhu & Gupta, 2017;\nSanh et al., 2020).3\nRecall the Top-K pruning uses the score matrix set Sto\ncompute M, i.e., Mi = Top-K(Si) with K ∈[0,100] in a\nunit of percentage. By sorting the elements of the matrix\nSi, Top-Kset the mask Mi for the top K% to be 1, and the\nbottom (100 −K)% to 0. Mathematically, it expresses as\nTop-K(x) =\n{\n1 x∈sort(Si,K%),\n0 o.w., (3)\nwhere sort(Si,K%) contains the Top K% of the sorted\nmatrix Si. Here K is determined by the users, and thus\nfollows various kinds of schedules such as the cubic sparsity\nscheduling, Eq. 1. As described in Section 3.1, such a\nschedule usually requires extensive engineering tuning in\norder to achieve state-of-the-art performance. Moreover,\nin (Zhu & Gupta, 2017), the Top-K(·) threshold is ﬁxed\nfor all weight matrices. However, different weight matrices\nhave different tolerances/sensitivities to pruning, meaning\nthat a low pruning ratio needs to be applied for sensitive\nlayers, and vice versa. In order to resolve those issues, we\npropose an algorithm to automatically adjust their thresholds\n3Our methods can thus be easily applied to magnitude-based\npruning methods by setting Sto be identical to W(Han et al.,\n2015).\nfor all weight matrices. More speciﬁcally, we deﬁne Kas\nK(σi) := 100·k(σi),\nk(σi) =Sigmoid(σi/T), for i= 1,...,n (4)\nwhere the Sigmoid function is used to map σto be in the\nrange of (0,1). T is a temperature value which critically\ncontrols the speed of k transitioning from 1 to 0 as σ de-\ncreases. We remark that Sigmoid could be replaced with\nany continuous function that maps any positive or negative\nvalues to [0,1]. Investigating for various such functions\ncould be an interesting future direction.\nFor a mask Mi ∈Rdi\nin×di\nout , its density ratio |Mi|/(di\nin ×\ndi\nout) = k(σi) is uniquely determined by σi. However,\ndirectly applying this for our objective function will tend\nto make k(σi) always close to 1, since the model prefers\nno pruning to achieve lower training loss. Therefore, we\nintroduce a novel regularization term to compensate for\nthis. Denote R(σ) the remaining ratio of weight parameter,\nwhich is a function of σ (more details of how to calculate\nR(σ) are given later). Suppose that our target pruning ratio\nis Rtarget. We propose the following simple yet effective\nregularization loss,\nLreg(σ) =\n{\n(R(σ) −Rtarget)2 R(σ) ≥Rtarget,\n0 else. (5)\nEquipped with Eq. 3, 4, and 5, we then rewrite the training\nobjective as\nLobj(Mσ ⊙W) =Lpure(Mσ ⊙W) +λregLreg(σ) (6)\nwhere the masks Mσ is written in an abstract manner, mean-\ning that each mask Mi is determined by Top-K (deﬁned\nin Eq. 3). As the Top-koperator is not a smooth operator,\nwe use the so-called Straight-through Estimator (Bengio\net al., 2013) to compute the gradient with respect to both σ\nand S. That is to say, the gradient through Top-Koperator\nis artiﬁcially set to be 1.\nWith such a regularization deﬁned in Eq. 6, there exits “com-\npetition\" between σi in Lpure and σi in Lreg. Particularly, σi\nin Lpure tends to make k(σi) close to 1 as the dense model\ngenerally gives better accuracy performance, while σi in\nLreg makes k(σi) close to the target ratio Rtarget. Notably,\nour regularization method is fundamentally different from\nthose soft-threshold methods by using L0 or L1 regular-\nization. While they apply a penalty to the score matrices\nwith indirect control on ﬁnal sparsity, our method focus\non learnable sparsity thresholds σi. Thus, we could easily\nachieve our target compression ratios. On the other hand,\none may add L0 or L1 regularization to Eq. 6 as the two are\ncomplementary.\nCritical term R(σ) We now delve into the calculation of\nR(σ). For simplicity, we consider that all three matrices\nLearnable Pruning\nMi, Wi, and Si follow the same dimensionsdi\nin ×di\nout. Then\nR(σ) =Nremain(σ)/Ntotal,\nwhere the total number of weight parameters Ntotal =∑n\ni=1(di\nin ×di\nout), and the number of remaining parame-\nters Nremain(σ) =∑n\ni=1 k(σi)(di\nin ×di\nout).\nAdaptive regularization coefﬁcient λreg Generally, for\nregularization-based (e.g., L1 or L0 regularization) pruning\nmethods, λreg needs to be carefully tuned (Sanh et al., 2020).\nTo resolve this tuning issue, we propose an adaptive formula\nto choose the value of λreg,\nλreg = max\n{\nλmax\nLreg\n(1 −Rtarget)2 ,λmin\n}\n, (7)\nwhere λmax and λmin are pre-chosen hyperparameters. 4\nWe have found that our results are not sensitive to the choice\nof these hyper-parameters. The idea is that when R(σ) is\nfar away from the Rtarget, the new coefﬁcient λreg in Eq. 7\nis close to λmax (when R(σ) = 1, it is indeed λmax) so\nthat we can have a strong regularization effect; and when R\nis close to Rtarget, the penalty can be less heavy in Eq. 6.\nDetailed comparison between constant and our proposed\nadaptive regularization are referred to Section 5.\n4. Experimental Setup\nWe apply LEAP with task-speciﬁc pruning for BERT base,\na 12-layer encoder-only Transformer model (Devlin et al.,\n2019), with approximately 85M parameters excluding the\nﬁrst embedding layer. We focus on three monolingual (En-\nglish) tasks: question answer (SQuAD v1.1) (Rajpurkar\net al., 2016); sentence similarity (QQP) (Iyer et al., 2017);\nand natural language inference (MNLI) (Williams et al.,\n2017). For SQuAD, QQP, and MNLI, there are 88K, 364K,\n392K training examples respectively.\nIn order to do a fair comparison with Soft MvP (Lagunas\net al., 2021), for all tasks, we perform logit distillation to\nboost the performance (Hinton et al., 2014). That is,\nLobj = αLds + (1−α)Lce(M(σ) ⊙W) +λregLreg(σ) (8)\nHere, Lds is the KL-divergence between the predictions of\nthe student and the teacher, Lce is the original cross entropy\nloss function between the student and the true label, and α\nis the hyperparameter that balances the cross-entropy loss\nand the distillation loss. We let α = 0.9 by default for\nfair comparison (One might be able to improve the results\nfurther with more careful hyperparameter tuning and more\nsophisticated distillation methods).\n4Here Lreg is just used as a scalar value, which does not affect\nthe computation of gradient. For instance, in PyTorch/TensorFlow,\nwe need to detach it from the computational graph.\nTable 1.Summary of different pruning settings. Here the ﬁrst\ncolumn shows the abbreviate name we will refer to later, the second\ncolumn shows the block size used for multi-head attention (MHA),\nand the third column shows the block size used for fully-connected\nlayers (FC).\nPruning Settings MHA FC\nHybrid (H32) 32×32 1 ×1\nStructure (S32) 32×32 32 ×32\nStructure (S16) 16×16 16 ×16\nStructure (S8) 8×8 8 ×8\nUnstructure (S1) 1×1 1 ×1\nStructured/Unstructured/Hybrid pruning The basic\ningredients of the transformer-based layer consist of multi-\nheaded attention (MHA) and fully connected (FC) sub-\nlayers. We denote the sets of weight matrices Watt for MHA\nand Wfc for FC.\nBefore we give details on the three pruning settings (Struc-\ntured, Unstructured, and Hybrid), we ﬁrst explain square\nd×dblock-wise pruning. Consider an output matrix as\nW ∈Rdin×dout , where din = drand dout = dc(here rand c\nare integer by design). We will deﬁne a maskM and a score\nS with the dimension of r×cfor the matrix W. Given a\nscore [S]i,j, if the Top-K operator returns [M]i,j = 0, then\nall d2 elements in the (i,j)-th block of W will be set to 0;\notherwise, [M]i,j = 1means keeping those elements.\nIn our experiments, structured pruningrefers to applying\nblock-wise pruning to both sets, i.e., Watt and Wfc. In ad-\ndition, we make the square block size the same in both\nMHA and FC sub-layers and we choose d= 32. Unstruc-\ntured pruning is using d = 1 for MHA and FC. Hybrid\npruning means using structured pruning for MHA (setting\nthe block size to be d = 32) and using unstructured one\nfor FC ( d = 1). As such, there are three different sets\nof experiments and we summarize them in Table 1. We\ntest our methods with the scores Sdescribed in movement\npruning (Sanh et al., 2020; Lagunas et al., 2021) over three\ndatasets across unstructured, hybrid, and structured pruning\nsetups. Moreover, we follow strictly (Lagunas et al., 2021)\n(referred to as Soft Pruning in later text) on the learning\nrate including warm-up and decaying schedules as well as\nthe total training epochs to make sure the comparison is\nfair. Let LEAP-l and soft MvP-1 denote a double-epoch\ntraining setup compared to LEAP and Soft Pruning (Sanh\net al., 2020). For more training details, see Appendix A.\n5. Results\nIn this section, we present our results for unstructured prun-\ning and structured block-wise pruning, and compare with\n(Sanh et al., 2020; Lagunas et al., 2021) and not include\nother methods for the two reasons: (1) our training setup\nLearnable Pruning\nTable 2. Different density ratios for unstructured pruning. Here\nSoft MvP is referred to (Lagunas et al., 2021). Here LEAP uses\nexactly training strategies as (Lagunas et al., 2021) and LEAP-l\ndoubles the training epochs. For QQP, we report accuracy and F1\nsocre; for MNLI, we report the accuracy of match and mis-match\nsets; for SQuAD, we report exact match and F1 score.\nMethods Density 1 Density 2 Density 3\n9∼10% 3 ∼4% 1 ∼2%\nSoft MvP 90.2/86.8 89.1/85.5 N/A\nLEAP 90.4/87.1 90.3/87.0 89.6/86.0\nQQPLEAP-l 90.9/87.9 90.6 /87.4 90.4 /87.1\n12∼13% 10 ∼11% 2 ∼3%\nSoft MvP N/A 81.2/81.8 79.5/80.1\nLEAP 81.3/81.5 80.6/81.0 79.0/79.3\nMNLILEAP-l 82.2/82.2 81.7 /81.7 80.3 /80.2\n9∼10% 5 ∼6% 3 ∼4%\nSoft MvP 76.6/84.9 N/A 72.7/82.3\nLEAP 77.0/85.4 74.3/83.3 72.9/82.5\nSQuADLEAP-l 78.7/86.7 75.9 /84.5 75.7 /84.5\nare close to them and they are the current stat-of-the-art\nmethods for BERT models. (2) in (Sanh et al., 2020; Lagu-\nnas et al., 2021), there already exist extensive comparisons\nbetween different methods, including hard-threshold andL0\nregularization\n5.1. Unstructured pruning\nUnstructured pruning is one of the most effective ways\nto reduce the memory footprint of NNs with minimal im-\npact on model quality. Here, we show the performance\nof LEAP under different density ratios of BERT base on\nQQP/MNLI/SQuAD.\nAs can be seen, compared to Soft MvP, LEAP achieves\nbetter performances on 4 out of 6 direct comparisons (as\nSoft MvP only provides two pruning ratios per task). Par-\nticularly, for QQP, LEAP is able to reduce the density ratio\nto 1∼2% while achieving similar performance as Soft MvP\nwith 3∼4% density ratio; for MNLI, although LEAP is\nslightly worse than Soft MvP, the performance gap is within\n0.6 for all cases. Also, recall that in order to achieve dif-\nferent level of pruning ratios as well as good model quality,\nSoft MvP needs to be carefully tuned st in Eq. 1 and λreg\nin Eq. 2. However, for LEAP, the tuning is much friendly\nand stable (see Section 6).\nWe also list the results of LEAP-l, which utilizes more train-\ning epochs to boost the performance, in Table 2. Note that\nfor all 9 scenarios, the performance of LEAP-l is much bet-\nter than LEAP, particularly for extreme compression. For\ninstance, for MNLI (SQuAD) with 2∼3% (3∼4%) density\nratio, longer training brings 1.3/0.9 (2.8/2.0) extra perfor-\nTable 3.Hybrid and structured pruning comparison between LEAP\nand Soft MvP (Lagunas et al., 2021). For QQP, we report accuracy\nand F1 socre; for MNLI, we report the accuracy of match and\nmis-match sets; for SQuAD, we report exact match and F1 score.\nLEAP-1 (MvP-1) means the training iterations is twice larger than\nLEAP (MvP).\nMethods Density 1 Density 2 Density 3\n27∼30% 21 ∼25% 11 ∼15%\nsoft MvP-1 (H32) NA/87.6 NA/87.1 NA/ 86.8\nQQPLEAP-1 (H32) 91.2/ 88.0 91.0/87.9 90.7/85.5\nLEAP-1 (S32) 91.0/87.9 90.9/87.7 90.5/87.3\n27∼30% 17 ∼21% 11 ∼15%\nsoft MvP-1 (H32) 83.0/83.6 82.3 /82.7 81.1/81.5\nMNLI\nLEAP-1 (H32) 83.0/83.2 82.2/82.4 81.5/81.5\nLEAP-1 (S32) 82.0/82.1 81.0/81.0 80.0/80.0\n27∼30% 21 ∼25% 15 ∼19%\nsoft MvP-1 (H32) 80.5/88.7 79.3 /86.9 78.8/86.6\nLEAP-1 (H32) 80.1/87.6 79.3/87.0 78.2/86.1\nsoft MvP-1 (S32) 77.9/85.6 77.1/85.2 N/A\nSQuADLEAP-1 (S32) 77.9/85.9 77.2 /86.4 N/A\nLEAP-1 (S16) 78.1/86.1 77.8/86.1 N/A\nLEAP-1 (S8) 78.7/86.7 78.3/86.3 N/A\nmance as compared to LEAP. Meanwhile, for all tasks,\nLEAP-l demonstrates a better performance as compared to\nSoft MvP.\nOne hypothesis to explain why longer training can signiﬁ-\ncantly boost the performance of LEAP is that LEAP intro-\nduces both more learnable hyperparameters and the adaptive\nregularization magnitude. As such, those extra parameters\nneed more iterations to reach the “optimal” values (which is\nalso illustrated in Section 6).\n5.2. Hybrid and structure pruning\nWe start with hybrid pruning and compare LEAP-1 with\nSoft MvP-1. The results are shown in Table 3. Again, as can\nbe seen, for different tasks with various pruning ratio, the\noverall performance of LEAP-1 is similar to Soft MvP-1,\nwhich demonstrates the easy adoption feature of LEAP.\nWe also present structured pruning results in Table 3. The\nﬁrst noticeable ﬁnding as expected here is that the accuracy\ndrop of structured pruning is much higher than hybrid mode,\nespecially for a low density ratio. Compared to Soft MvP-1,\nLEAP-1 achieves slightly better performance on SQuAD.\nFor more results, see Appendix B.\nWe reiterate that Table 2 and Table 3 are not about beat-\ning the state-of-the-art results but emphasizing that LEAP\nrequires much less hyper-parameter tuning but achieves sim-\nilar performance as Soft MvP that involved a large set of the\nhyper-parameter sweep. For details about hyper-parameter\ntuning of Soft MvP-1 and LEAP, see Section B.\nLearnable Pruning\n6. Analysis\nAs mentioned, LEAP is a learnable pruning method with a\nminimal requirement of hyperparameter tuning. In order to\ndemonstrate this, we analyze LEAP by delving into the key\ncomponents of LEAP: the initialization of our thresholds σ,\nthe temperature T, and the regularization term λreg.\nFigure 1.Effect of temperatureT for unstructured pruning on QQP.\nThe density ratio is set to be 1%.\nTemperature T As T deﬁned in Eq. 4, it plays a critical\nrole in determining the rate at which the threshold curve\nk(σi) falls. In addition, T also directly links to the ini-\ntialization of σi which is set to be 5T for all i such that\nSigmoid(σi/T) ≈1. This allows the model to have sufﬁ-\ncient time to identify the layers which are insensitive for\naggressive pruning and vice versa. To understand how T\ninﬂuences the performances of the Bert model, we conduct\nan unstructured pruning on the QQP dataset by varying\nT ∈{64,48,32,16}and keeping all other hyperparameters\nto be the same. We plot the objective loss Lobj (loss), the\nregularization loss Lreg (regu_loss), the density ratio R(σ),\nand F1 accuracy, with respect to the iterations in Figure 1.\nTable 4.Unstructured pruning on SQuAD with epoch 10 using\nvarious values of regularization coefﬁcient λreg in Eq. 6. It shows\nthat our LEAP is not too sensitive to the hyper-parameter choices\nT and λreg.\nTemperature λreg= 50 λreg= 160 λreg= 320acc/f1 density acc/f1 density acc/f1 density\nT= 16 76.58/84.94 10.66 76.49/85.01 10.27 76.33/84.85 10.2T= 32 77.11/85.49 11.46 76.97/85.47 10.39 76.96/85.36 10.23\nAmong the four curves in Figure 1, T = 48gives the best\nFigure 2.Effect of adaptive regularization T for Hybrid pruning\n(H32) on MNLI with a target dense ratio of 30%. Note that in the\nplot of density ratio with respect to epochs (left bottom), the purple\n(blue) and orange (green) curves are overlapped. Also in the right\ntop bottom, blue and green curves are overlapped.\nF1 accuracy while achieving ∼1% density, which clearly\ndemonstrates the signiﬁcance of T for LEAP. Meanwhile,\nwe see that the gaps between the performance for all T ex-\ncept 64 are close, thus it shows that LEAP is not sensitive\nto T. A possible explanation why T = 64gives the worse\nperformance is that the density ratio of T = 64decays rela-\ntively slower compared to rest curves. As such, when it is\nclose to the desired pruning regime, the learning rate is rela-\ntively small and so it cannot be able to recover the accuracy.\nOn the other hand, it is interesting to note that using the tem-\nperature T = 16(orange curve), the density ratio increases\nafter around ﬁve epochs and keeps increasing to the end 5,\nwhich results in a much better performance even though it\nexperiences the most accuracy drop in the beginning. This\nin some scenes illustrates the “competition\" between σi in\nLpure and σi in Lreg mentioned in Section 3.2: the accuracy\nincreases at epoch 5 meaning that Lpure is decreasing effec-\ntively and the Lreg increases (compromises). Compared to\nthose manual scheduling thresholds, this increasing phenom-\nena of σi also shows the advantage of learnable thresholds\nverifying that the model can ﬁgure out automatically when\nto prune and when not.\nRobustness of hyper-parameter tuningT and λreg We\nsee in the previous section that given the same λreg, various\nvalues of the temperature T lead to similar results although\n5Please note that the y-axis of the density plot is in logarithmic\nscale. Even T = 16slightly increases the density ratio, it is still\nvery close to 1%.\nLearnable Pruning\nFigure 3.The density ratio k(σi) to all the weight matrices for structured, hybrid and unstructured pruning on SQuAD, of which the total\ndensity ratios are respectively 20% , 16%, and 8%.\ntuning is necessary to achieve the best one. Here we study\nhow robust the coefﬁcient ofλreg in our proposed regulariza-\ntion Lreg. We prune BERTbase on the SQuAD task with a tar-\nget ratio 10% with a combination of λreg ∈{50,160,320}\nand T ∈{16,32}, for which the results is in Table 6.\nFor a given T, it indicates that the results are not highly sen-\nsitive to different λregs as there is only about 0.1 variation\nfor accuracy. It is worth noticing that a smaller λreg (here\nλreg = 50) can indeed affect achieving our target sparse\nratio. However, the most off pruning ratio is 11.46%, which\nis reasonably close to the desired target of 10%.\nFor a given λreg, larger T leads both the accuracy and the\ndensity ratio higher as expected. The reason is that the\ndensity ratio function, i.e., Sigmoid(σi/T), becomes ﬂatter\nfor larger T, which leads to a higher density ratio by using\nthe same value of σ (Generally, σ is negative to achieve\n< 50% density ratio). And higher density ratio results in\nhigher accuracy.\nOverall, we can see that LEAP is robust to both T and λreg.\nThe regularization coefﬁcient λreg To better understand\nthe effect of adaptive λreg (Eq. 7), we set λmax ∈\n{160,320}and ﬁx λmin = 10(same as Section 5) to prune\nBERTbase on the MNLI task with a target ratio 30%. In\naddition, we also compare this adaptive coefﬁcient with\ntheir constant counterparts λreg ∈{160,320}. We plots the\nλreg (lambda_reg), the regularization loss Lreg (regu_loss),\nthe density ratio R(σ), and accuracy, with respect to the\niterations in Figure 2. First of all, we see that our adaptive\ncoefﬁcient λreg decreases in a quadratic manner and reach-\ning to the λmin = 10after 4 epochs, which slows down the\npruning activities after 4 epochs. Also, note that the curves\nof different λmax are actually overlapped with each other,\nwhich also indicates that LEAP is not vulnerable to λreg.\nMeanwhile, as λreg quickly reaches λmin, the importance\nscore Shas more time to ﬁgure out the pruning parame-\nters for the last small portion. As such, this slowness can\nin turn decrease the drop of accuracy and thus eventually\nrecover a much better accuracy than that of the constant\nregularization.\nThe effect of learnable pruning for different weight ma-\ntrices As mentioned, the sensitivities of different weight\nmatrices are different. Therefore, a high pruning ratio should\nbe set for insensitive layers, and a low pruning ratio needs\nto be used for sensitive layers. To demonstrate LEAP can\nautomatically achieve this, we plot the remaining parame-\nters per layer for different pruning granularity on SQuAD\nin Figure 3. As can be seen, different layers receive different\npruning ratios. Particularly, (i) as compared to MHA layers,\nFC layers are generally pruned more, which results in a\nlower density ratio. This might indicate that FC layers are\nless sensitive as compared to MHA layers; (ii) there is a\nclear trend that shallow layers (close to inputs) have higher\ndensity ratios as compared to deep layers (close to outputs).\nThis ﬁnding is very intuitive. If the pruning ratio is too high\nfor shallow layers, the information loss might be too high,\nand it is hard for the model to propagate the information to\nthe output layer successfully. Therefore, the pruning ratio\nof shallow layers is smaller.\n7. Conclusions\nIn this work, we present LEAP, a learnable pruning frame-\nwork for transformer-based models. To alleviate the hy-\nperparameter tuning effort, LEAP (i) introduces a novel\nregularization function to achieve desired pruning ratio with\nlearnable pruning ratios for different weight matrices, and\n(ii) designs an adaptive regularization magnitude coefﬁcient\nto control the regularization loss adaptively. By combining\nthese two techniques, LEAP achieves on-par or even better\nperformance for various pruning scenarios as compared to\nprevious methods. Also, we demonstrate that LEAP less\nsensitive to the newly introduced hyperparameters and show\nthe advance of the proposed adaptive regularization coef-\nLearnable Pruning\nﬁcient. Finally, we also show that there is clear pruning\nsensitivity associated with the depth and the component of\nthe network.\nReferences\nBai, H., Zhang, W., Hou, L., Shang, L., Jin, J., Jiang, X., Liu,\nQ., Lyu, M., and King, I. BinaryBERT: Pushing the limit\nof BERT quantization. arXiv preprint arXiv:2012.15701,\n2020.\nBengio, Y ., Léonard, N., and Courville, A. Estimating or\npropagating gradients through stochastic neurons for con-\nditional computation. arXiv preprint arXiv:1308.3432,\n2013.\nBhandare, A., Sripathi, V ., Karkada, D., Menon, V ., Choi, S.,\nDatta, K., and Saletore, V . Efﬁcient 8-bit quantization of\ntransformer neural machine language translation model.\narXiv preprint arXiv:1906.00532, 2019.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,\nJ., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165, 2020.\nChen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y .,\nWang, Z., and Carbin, M. The lottery ticket hypoth-\nesis for pre-trained BERT networks. arXiv preprint\narXiv:2007.12223, 2020.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. In NAACL-HLT, 2019.\nDong, X., Chen, S., and Pan, S. Learning to prune deep\nneural networks via layer-wise optimal brain surgeon. In\nAdvances in Neural Information Processing Systems, pp.\n4857–4867, 2017.\nEsser, S. K., McKinstry, J. L., Bablani, D., Appuswamy, R.,\nand Modha, D. S. Learned step size quantization. arXiv\npreprint arXiv:1902.08153, 2019.\nFan, A., Grave, E., and Joulin, A. Reducing transformer\ndepth on demand with structured dropout. arXiv preprint\narXiv:1909.11556, 2019.\nFan, A., Stock, P., Graham, B., Grave, E., Gribonval, R.,\nJegou, H., and Joulin, A. Training with quantization\nnoise for extreme ﬁxed-point compression.arXiv preprint\narXiv:2004.07320, 2020.\nFrankle, J. and Carbin, M. The lottery ticket hypothesis:\nFinding sparse, trainable neural networks. arXiv preprint\narXiv:1803.03635, 2018.\nHan, S., Pool, J., Tran, J., and Dally, W. Learning both\nweights and connections for efﬁcient neural network. In\nAdvances in neural information processing systems, pp.\n1135–1143, 2015.\nHan, S., Mao, H., and Dally, W. J. Deep compression:\nCompressing deep neural networks with pruning, trained\nquantization and huffman coding. International Confer-\nence on Learning Representations, 2016.\nHe, Y ., Lin, J., Liu, Z., Wang, H., Li, L.-J., and Han, S. Amc:\nAutoml for model compression and acceleration on mo-\nbile devices. In Proceedings of the European Conference\non Computer Vision (ECCV), pp. 784–800, 2018.\nHinton, G., Vinyals, O., and Dean, J. Distilling the knowl-\nedge in a neural network. Workshop paper in NIPS, 2014.\nHuang, Z. and Wang, N. Data-driven sparse structure se-\nlection for deep neural networks. In Proceedings of the\nEuropean conference on computer vision (ECCV), pp.\n304–320, 2018.\nIandola, F. N., Shaw, A. E., Krishna, R., and Keutzer,\nK. W. SqueezeBERT: What can computer vision teach\nNLP about efﬁcient neural networks? arXiv preprint\narXiv:2006.11316, 2020.\nIyer, S., Dandekar, N., and Csernai, K. First quora dataset\nrelease: Question pairs, 2017. URL https://data. quora.\ncom/First-Quora-Dataset-Release-Question-Pairs, 2017.\nJiao, X., Yin, Y ., Shang, L., Jiang, X., Chen, X., Li, L.,\nWang, F., and Liu, Q. TinyBERT: Distilling BERT\nfor natural language understanding. arXiv preprint\narXiv:1909.10351, 2019.\nKitaev, N., Kaiser, Ł., and Levskaya, A. Reformer: The\nefﬁcient transformer. arXiv preprint arXiv:2001.04451,\n2020.\nLagunas, F., Charlaix, E., Sanh, V ., and Rush, A. M. Block\npruning for faster transformers. In Proceedings of the\n2021 Conference on Empirical Methods in Natural Lan-\nguage Processing, pp. 10619–10629, 2021.\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P.,\nand Soricut, R. ALBERT: A lite bert for self-supervised\nlearning of language representations. In International\nConference on Learning Representations, 2019.\nLee, N., Ajanthan, T., and Torr, P. H. Snip: Single-shot\nnetwork pruning based on connection sensitivity. arXiv\npreprint arXiv:1810.02340, 2018.\nLin, S., Ji, R., Li, Y ., Wu, Y ., Huang, F., and Zhang, B. Ac-\ncelerating convolutional networks via global & dynamic\nﬁlter pruning. In IJCAI, pp. 2425–2432, 2018.\nLearnable Pruning\nLuo, J.-H., Wu, J., and Lin, W. Thinet: A ﬁlter level pruning\nmethod for deep neural network compression. InProceed-\nings of the IEEE international conference on computer\nvision, pp. 5058–5066, 2017.\nMichel, P., Levy, O., and Neubig, G. Are sixteen heads\nreally better than one? arXiv preprint arXiv:1905.10650,\n2019.\nMicrosoft and Nvidia. Using DeepSpeed and Megatron to\nTrain Megatron-Turing NLG 530B, the World’s Largest\nand Most Powerful Generative Language Model.https:\n//developer.nvidia.com/blog/using-de\nepspeed-and-megatron-to-train-megatr\non-turing-nlg-530b-the-worlds-larges\nt-and-most-powerful-generative-langu\nage-model/, 2021.\nNarang, S., Undersander, E., and Diamos, G. Block-\nsparse recurrent neural networks. arXiv preprint\narXiv:1711.02782, 2017.\nPark, S., Lee, J., Mo, S., and Shin, J. Lookahead: a far-\nsighted alternative of magnitude-based pruning. arXiv\npreprint arXiv:2002.04809, 2020.\nPrasanna, S., Rogers, A., and Rumshisky, A. When BERT\nplays the lottery, all tickets are winning. arXiv preprint\narXiv:2005.00561, 2020.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad:\n100,000+ questions for machine comprehension of text.\narXiv preprint arXiv:1606.05250, 2016.\nSajjad, H., Dalvi, F., Durrani, N., and Nakov, P. Poor man’s\nBERT: Smaller and faster transformer models. arXiv\npreprint arXiv:2004.03844, 2020.\nSanh, V ., Debut, L., Chaumond, J., and Wolf, T. DistilBERT,\na distilled version of bert: smaller, faster, cheaper and\nlighter. arXiv preprint arXiv:1910.01108, 2019.\nSanh, V ., Wolf, T., and Rush, A. M. Movement prun-\ning: Adaptive sparsity by ﬁne-tuning. arXiv preprint\narXiv:2005.07683, 2020.\nShen, S., Dong, Z., Ye, J., Ma, L., Yao, Z., Gholami, A.,\nMahoney, M. W., and Keutzer, K. Q-BERT: Hessian\nbased ultra low precision quantization of bert. In AAAI,\npp. 8815–8821, 2020.\nShen, S., Yao, Z., Kiela, D., Keutzer, K., and Mahoney,\nM. W. What’s hidden in a one-layer randomly weighted\ntransformer? arXiv preprint arXiv:2109.03939, 2021.\nShoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J.,\nand Catanzaro, B. Megatron-LM: Training multi-billion\nparameter language models using gpu model parallelism.\narXiv preprint arXiv:1909.08053, 2019.\nSun, S., Cheng, Y ., Gan, Z., and Liu, J. Patient knowledge\ndistillation for bert model compression. arXiv preprint\narXiv:1908.09355, 2019.\nSun, Z., Yu, H., Song, X., Liu, R., Yang, Y ., and\nZhou, D. MobileBERT: a compact task-agnostic\nBERT for resource-limited devices. arXiv preprint\narXiv:2004.02984, 2020.\nTang, R., Lu, Y ., Liu, L., Mou, L., Vechtomova, O., and Lin,\nJ. Distilling task-speciﬁc knowledge from BERT into sim-\nple neural networks. arXiv preprint arXiv:1903.12136,\n2019.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Atten-\ntion is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nWang, H., Zhang, Z., and Han, S. Spatten: Efﬁcient sparse\nattention architecture with cascade token and head prun-\ning. arXiv preprint arXiv:2012.09852, 2020a.\nWang, S., Li, B., Khabsa, M., Fang, H., and Ma, H. Lin-\nformer: Self-attention with linear complexity. arXiv\npreprint arXiv:2006.04768, 2020b.\nWang, Z., Wohlwend, J., and Lei, T. Structured pruning of\nlarge language models. arXiv preprint arXiv:1910.04732,\n2019.\nWilliams, A., Nangia, N., and Bowman, S. R. A broad-\ncoverage challenge corpus for sentence understanding\nthrough inference. arXiv preprint arXiv:1704.05426,\n2017.\nXiao, X., Wang, Z., and Rajasekaran, S. Autoprune: Auto-\nmatic network pruning by regularizing auxiliary param-\neters. In Advances in Neural Information Processing\nSystems, pp. 13681–13691, 2019.\nYu, H., Edunov, S., Tian, Y ., and Morcos, A. S. Playing\nthe lottery with rewards and multiple languages: lottery\ntickets in rl and nlp. arXiv preprint arXiv:1906.02768,\n2019.\nYu, R., Li, A., Chen, C.-F., Lai, J.-H., Morariu, V . I., Han,\nX., Gao, M., Lin, C.-Y ., and Davis, L. S. Nisp: Pruning\nnetworks using neuron importance score propagation. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 9194–9203, 2018.\nLearnable Pruning\nYu, S., Yao, Z., Gholami, A., Dong, Z., Mahoney, M. W.,\nand Keutzer, K. Hessian-aware pruning and optimal\nneural implant. arXiv preprint arXiv:2101.08940, 2021.\nZadeh, A. H., Edo, I., Awad, O. M., and Moshovos, A.\nGobo: Quantizing attention-based nlp models for low la-\ntency and energy efﬁcient inference. In 2020 53rd Annual\nIEEE/ACM International Symposium on Microarchitec-\nture (MICRO), pp. 811–824. IEEE, 2020.\nZafrir, O., Boudoukh, G., Izsak, P., and Wasserblat,\nM. Q8BERT: Quantized 8bit bert. arXiv preprint\narXiv:1910.06188, 2019.\nZhang, W., Hou, L., Yin, Y ., Shang, L., Chen, X., Jiang, X.,\nand Liu, Q. Ternarybert: Distillation-aware ultra-low bit\nbert. arXiv preprint arXiv:2009.12812, 2020.\nZhao, C., Ni, B., Zhang, J., Zhao, Q., Zhang, W., and Tian,\nQ. Variational convolutional neural network pruning. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pp. 2780–2789, 2019.\nZhao, M., Lin, T., Mi, F., Jaggi, M., and Schütze, H. Mask-\ning as an efﬁcient alternative to ﬁnetuning for pretrained\nlanguage models. arXiv preprint arXiv:2004.12406,\n2020.\nZhu, M. and Gupta, S. To prune, or not to prune: exploring\nthe efﬁcacy of pruning for model compression. arXiv\npreprint arXiv:1710.01878, 2017.\nLearnable Pruning\nA. Training Details\nFor all three tasks, the temperature parameter T for k(σi) is chosen between {16,32,48,64}and λmax varies between\n{40,80,160,320}with λmin = 10. For initialization of σi, we set it to be 5T. We use a batch size 32, a sequence 128 for\nQQP/MNLI and 11000/12000 warmup steps (about 1 epoch) for learning rate. As for SQuAD, we use a batch size 16 and a\nsequence 384, and we use 5400 warmup steps (about 1 epoch). We use a learning rate of 3e-5 (1e-2) for the original weights\n(for pruning-rated parameters, i.e., Sand σ). We set all the training to be deterministic with the random seed of 17. All\nthe models are trained using FP32 with PyTorch on a single V100 GPU. Note that these conﬁgurations strictly follow the\nexperimental setup in (Sanh et al., 2020; Lagunas et al., 2021); readers could check more details there. For the results in\nTable 1, the entire epoch using LEAP is 10, 6, and 10, respectively, for QQP, MNLI, and SQuAD. For the results of LEAP-1\nand the results in Table 2, we simply double training epochs correspondingly (i.e., 20, 12, and 20).\nB. Results Details\nSmaller tasks. Note larger datasets (QQP/MNLI/SQuAD) to evaluate the performance of the pruning method is very\ncommon due to the evaluation robustness. However, to illustrate the generalization ability of LEAP, we also tested its\nperformance on two smaller datasets STS-B and MPRC, using block pruning with size 32x32. The results are shown in\nTable 1. As can be seen, with around 20% density ratio, LEAP still achieves marginal accuracy degradation compared to\nbaseline.\nTable 5. Results for STS-B (baseline is 88.71) and MRPC (baseline is 87.01%) with different temperature T and adaptive λmax for\nstructure pruning (block size 32x32).\nSpearman correlation Density ratio\nSTS-B T=1 T=2 T=4 T=1 T=2 T=4\nλmax=80 85.68 85.86 85.96 20.0 20.1 22.5\nλmax=160 85.73 85.91 86.19 20.0 20.1 26.0\nλmax=320 85.72 86.01 86.46 20.0 20.3 28.5\nAccuracy Density ratio\nMRPC T=1 T=2 T=4 T=1 T=2 T=4\nλmax=80 82.1 82.6 79.16 20.0 20.0 20.3\nλmax=160 81.37 82.35 79.65 20.0 20.0 21.3\nλmax=320 80.88 81.37 78.67 20.0 20.0 22.7\nHyper-parameter. We emphasize again the results of soft mvp is a strong baseline, and our goal is not to purely beat soft\nmvp from accuracy perspective. However, their results require extensive hyperparameter tuning (see directory), while ours\nrequire to only tune T. To show the generalization of the best hyperparameter, we include the results for various λmax and\nT on multiple tasks in Table 3. Note that when T is ﬁxed, different λmax gives similar results over various tasks.\nTable 6. Results for QQP and MRPC with different temperature T and adaptive λmax for structure pruning (block size 32x32).\nAccuracy Density ratio\nQQP T=16 T=32 T=48 T=16 T=32 T=48\nλmax=160 90.68 90.87 90.7 20.07 20.07 20.1\nλmax=320 90.79 90.78 90.6 20.08 20.06 20.1\nAccuracy (MNLI/MNLI-MM) Density ratio\nMNLI T=16 T=32 T=48 T=16 T=32 T=48\nλmax=40 80.41/81.06 80.79/81.12 80.87/81.22 21.07 21.52 22.25\nλmax=160 80.56/80.98 80.88/80.81 81.02/81.17 21.07 21.46 22.15",
  "topic": "Pruning",
  "concepts": [
    {
      "name": "Pruning",
      "score": 0.7152769565582275
    },
    {
      "name": "Computer science",
      "score": 0.6658501625061035
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.46750032901763916
    },
    {
      "name": "Speedup",
      "score": 0.4206504821777344
    },
    {
      "name": "Machine learning",
      "score": 0.35948777198791504
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3373563885688782
    },
    {
      "name": "Algorithm",
      "score": 0.32633456587791443
    },
    {
      "name": "Parallel computing",
      "score": 0.19634884595870972
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Agronomy",
      "score": 0.0
    }
  ],
  "cited_by": 7
}