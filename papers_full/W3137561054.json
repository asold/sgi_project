{
  "title": "UNETR: Transformers for 3D Medical Image Segmentation",
  "url": "https://openalex.org/W3137561054",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221854506",
      "name": "Hatamizadeh, Ali",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2368893787",
      "name": "Tang Yucheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222507664",
      "name": "Nath, Vishwesh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1975478623",
      "name": "Yang Dong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222987583",
      "name": "Myronenko, Andriy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2190297699",
      "name": "Landman, Bennett",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3103523507",
      "name": "Roth, Holger",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3102047246",
      "name": "Xu, Daguang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2965994994",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3112701542",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2604785265",
    "https://openalex.org/W3132503749",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W2608631154",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2804078698",
    "https://openalex.org/W3204563069",
    "https://openalex.org/W3025870037",
    "https://openalex.org/W3120885796",
    "https://openalex.org/W2952234052",
    "https://openalex.org/W2798122215",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2791680898",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W2604790786",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W3024821730",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2769910914",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3130695101",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2900298334",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3112256294",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W2964227007",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W3203841574",
    "https://openalex.org/W2933796343",
    "https://openalex.org/W2463818697",
    "https://openalex.org/W3103010481",
    "https://openalex.org/W2963046541",
    "https://openalex.org/W2618677231",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W2285838348",
    "https://openalex.org/W3134689216",
    "https://openalex.org/W2301358467",
    "https://openalex.org/W3211432419",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W2963122731",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2899279931",
    "https://openalex.org/W2915126261"
  ],
  "abstract": "Fully Convolutional Neural Networks (FCNNs) with contracting and expanding paths have shown prominence for the majority of medical image segmentation applications since the past decade. In FCNNs, the encoder plays an integral role by learning both global and local features and contextual representations which can be utilized for semantic output prediction by the decoder. Despite their success, the locality of convolutional layers in FCNNs, limits the capability of learning long-range spatial dependencies. Inspired by the recent success of transformers for Natural Language Processing (NLP) in long-range sequence learning, we reformulate the task of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem. We introduce a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information, while also following the successful \"U-shaped\" network design for the encoder and decoder. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the final semantic segmentation output. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV leaderboard. Code: https://monai.io/research/unetr",
  "full_text": "UNETR: Transformers for 3D Medical Image Segmentation\nAli Hatamizadeh\nNVIDIA\nYucheng Tang\nVanderbilt University\nVishwesh Nath\nNVIDIA\nDong Yang\nNVIDIA\nAndriy Myronenko\nNVIDIA\nBennett Landman\nVanderbilt University\nHolger R. Roth\nNVIDIA\nDaguang Xu\nNVIDIA\nAbstract\nFully Convolutional Neural Networks (FCNNs) with con-\ntracting and expanding paths have shown prominence for the\nmajority of medical image segmentation applications since\nthe past decade. In FCNNs, the encoder plays an integral\nrole by learning both global and local features and contextual\nrepresentations which can be utilized for semantic output\nprediction by the decoder. Despite their success, the locality\nof convolutional layers in FCNNs, limits the capability of\nlearning long-range spatial dependencies. Inspired by the\nrecent success of transformers for Natural Language Process-\ning (NLP) in long-range sequence learning, we reformulate\nthe task of volumetric (3D) medical image segmentation as\na sequence-to-sequence prediction problem. We introduce a\nnovel architecture, dubbed as UNEt TRansformers (UNETR),\nthat utilizes a transformer as the encoder to learn sequence\nrepresentations of the input volume and effectively capture\nthe global multi-scale information, while also following the\nsuccessful ‚ÄúU-shaped‚Äù network design for the encoder and\ndecoder. The transformer encoder is directly connected to\na decoder via skip connections at different resolutions to\ncompute the Ô¨Ånal semantic segmentation output. We have\nvalidated the performance of our method on the Multi Atlas\nLabeling Beyond The Cranial Vault (BTCV) dataset for multi-\norgan segmentation and the Medical Segmentation Decathlon\n(MSD) dataset for brain tumor and spleen segmentation tasks.\nOur benchmarks demonstrate new state-of-the-art perfor-\nmance on the BTCV leaderboard.\nCode: https://monai.io/research/unetr\n1. Introduction\nImage segmentation plays an integral role in quantitative\nmedical image analysis as it is often the Ô¨Årst step for analysis\nof anatomical structures [33]. Since the advent of deep learn-\ning, FCNNs and in particular ‚ÄúU-shaped‚Äú encoder-decoder ar-\nTransformer  Encoder\nLinear Projection of Flattened Patches\n9864321 5 70 *\nDecoder\nùêª√óùëä √óùê∑ √óùê∂\nSegmentation\nOutput\n3D Patches\nFigure 1. Overview of UNETR. Our proposed model consists\nof a transformer encoder that directly utilizes 3D patches and is\nconnected to a CNN-based decoder via skip connection.\nchitectures [22, 23, 21] have achieved state-of-the-art results\nin various medical semantic segmentation tasks [2, 38, 19]. In\na typical U-Net [36] architecture, the encoder is responsible\nfor learning global contextual representations by gradually\ndownsampling the extracted features, while the decoder up-\nsamples the extracted representations to the input resolution\nfor pixel/voxel-wise semantic prediction. In addition, skip\nconnections merge the output of the encoder with decoder\nat different resolutions, hence allowing for recovering spatial\ninformation that is lost during downsampling.\nAlthough such FCNN-based approaches have powerful\nrepresentation learning capabilities, their performance in\nlearning long-range dependencies is limited to their localized\nreceptive Ô¨Åelds [ 20, 35]. As a result, such a deÔ¨Åciency\nin capturing multi-scale information leads to sub-optimal\nsegmentation of structures with variable shapes and scales\n(e.g. brain lesions with different sizes). Several efforts have\nused atrous convolutional layers [9, 27, 18] to enlarge the\n1\narXiv:2103.10504v3  [eess.IV]  9 Oct 2021\nreceptive Ô¨Åelds. However, locality of the receptive Ô¨Åelds in\nconvolutional layers still limits their learning capabilities to\nrelatively small regions. Combining self-attention modules\nwith convolutional layers [45, 50, 16] has been proposed to\nimprove the non-local modeling capability.\nIn Natural Language Processing (NLP), transformer-based\nmodels [ 42, 13] achieve state-of-the-art benchmarks in\nvarious tasks. The self-attention mechanism of transformers\nallows to dynamically highlight the important features of\nword sequences. Additionally, in computer vision, using\ntransformers as a backbone encoder is beneÔ¨Åcial due to their\ngreat capability of modeling long-range dependencies and\ncapturing global context [14, 4]. SpeciÔ¨Åcally, unlike the local\nformulation of convolutions, transformers encode images as\na sequence of 1D patch embeddings and utilize self-attention\nmodules to learn a weighted sum of values that are calculated\nfrom hidden layers. As a result, this Ô¨Çexible formulation\nallows to effectively learn the long-range information.\nFurthermore, Vision Transformer (ViT) [14] and its variants\nhave shown excellent capabilities in learning pre-text tasks\nthat can be transferred to down-stream applications [40, 6, 3].\nIn this work, we propose to leverage the power of\ntransformers for volumetric medical image segmentation and\nintroduce a novel architecture dubbed as UNEt TRansformers\n(UNETR). In particular, we reformulate the task of 3D seg-\nmentation as a 1D sequence-to-sequence prediction problem\nand use a transformer as the encoder to learn contextual\ninformation from the embedded input patches. The extracted\nrepresentations from the transformer encoder are merged\nwith the CNN-based decoder via skip connections at multiple\nresolutions to predict the segmentation outputs. Instead of\nusing transformers in the decoder, our proposed framework\nuses a CNN-based decoder. This is due to the fact that trans-\nformers are unable to properly capture localized information,\ndespite their great capability of learning global information.\nWe validate the effectiveness of our method on 3D CT\nand MRI segmentation tasks using Beyond the Cranial\nVault (BTCV) [ 26] and Medical Segmentation Decathlon\n(MSD) [ 38] datasets. In BTCV dataset, UNETR achieves\nnew state-of-the-art performance on both Standard and\nFree Competition sections on its leaderboard. UNETR\noutperforms the state-of-the-art methodologies on both brain\ntumor and spleen segmentation tasks in MSD dataset.\nour main contributions of this work are as follows::\n‚Ä¢ We propose a novel transformer-based model for\nvolumetric medical image segmentation.\n‚Ä¢ To this end, we propose a novel architecture in which (1)\na transformer encoder directly utilizes the embedded 3D\nvolumes to effectively capture long-range dependencies;\n(2) a skip-connected decoder combines the extracted\nrepresentations at different resolutions and predicts the\nsegmentation output.\n‚Ä¢ We validate the effectiveness of our proposed model for\ndifferent volumetric segmentation tasks on two public\ndatasets: BTCV [26] and MSD [38]. UNETR achieves\nnew state-of-the-art performance on leaderboard of\nBTCV dataset and outperforms competing approaches\non the MSD dataset.\n2. Related Work\nCNN-based Segmentation Networks : Since the intro-\nduction of the seminal U-Net [ 36], CNN-based networks\nhave achieved state-of-the-art results on various 2D and 3D\nvarious medical image segmentation tasks [15, 54, 49, 17, 28].\nFor volume-wise segmentation, tri-planar architectures\nare sometimes used to combine three-view slices for each\nvoxel, also known for 2.5D methods [ 28, 29, 46]. In\ncontrast, 3D approaches directly utilize the full volumetric\nimage represented by a sequence of 2D slices or modalities.\nThe intuition of employing varying sizes was followed\nby multi-scan, multi-path models [ 24, 25, 8] to capture\ndownsampled features of the image. In addition, to exploit\n3D context and to cope with limitation of computational\nresource, researchers investigated hierarchical frameworks.\nSome efforts proposed to extract features at multiple scales\nor assembled frameworks [ 21]. Roth et al. [37] proposed\na multi-scale framework to obtain varying resolution\ninformation in pancreas segmentation. These methods\nprovide pioneer studies of 3D medical image segmentation\nat multiple levels, which reduces problems in spatial context\nand low-resolution condition. Despite their success, a\nlimitation of these networks is their poor performance in\nlearning global context and long-range spatial dependencies,\nwhich can severely impact the segmentation performance for\nchallenging tasks.\nVision Transformers : Vision transformers have recently\ngained traction for computer vision tasks. Dosovitskiy\net al. [14] demonstrated state-of-the-art performance on\nimage classiÔ¨Åcation datasets by large-scale pre-training and\nÔ¨Åne-tuning of a pure transformer. In object detection, end-\nto-end transformer-based models have shown prominence\non several benchmarks [5, 55]. Recently, hierarchical vision\ntransformers with varying resolutions and spatial embed-\ndings [30, 44, 12, 48] have been proposed. These method-\nologies gradually decrease the resolution of features in the\ntransformer layers and utilize sub-sampled attention modules.\nUnlike these approaches, the size of representation in UNETR\nencoder remains Ô¨Åxed in all transformer layers. However, as\ndescribed in Sec. 3, deconvolutional and convolutional oper-\nations are used to change the resolution of extracted features.\nRecently, multiple methods were proposed that explore the\npossibility of using transformer-based models for the task of\n2D image segmentation [52, 7, 41, 51]. Zheng et al. [52] in-\ntroduced the SETR model in which a pre-trained transformer\n2\nencoder with different variations of CNN-based decoders\nwere proposed for the task of semantic segmentation. Chenet\nal. [7] proposed a methodology for multi-organ segmentation\nby employing a transformer as an additional layer in the bottle-\nneck of a U-Net architecture. Zhanget al.[51] proposed to use\nCNNs and transformers in separate streams and fuse their out-\nputs. Valanarasuet al. [41] proposed a transformer-based ax-\nial attention mechanism for 2D medical image segmentation.\nThere are key differences between our model and these efforts:\n(1) UNETR is tailored for 3D segmentation and directly uti-\nlizes volumetric data; (2) UNETR employs the transformer as\nthe main encoder of a segmentation network and directly con-\nnects it to the decoder via skip connections, as opposed to us-\ning it as an attention layer within the segmentation network (3)\nUNETR does not rely on a backbone CNN for generating the\ninput sequences and directly utilizes the tokenized patches.\nFor 3D medical image segmentation, Xie et al. [47]\nproposed a framework that utilizes a backbone CNN for\nfeature extraction, a transformer to process the encoded\nrepresentation and a CNN decoder for predicting the\nsegmentation outputs. Similarly, Wanget al. [43] proposed to\nuse a transformer in the bottleneck of a 3D encoder-decoder\nCNN for the task of semantic brain tumor segmentation. In\ncontrast to these approaches, our method directly connects\nthe encoded representation from the transformer to decoder\nby using skip connections.\n3. Methodology\n3.1. Architecture\nWe have presented an overview of the proposed model\nin Fig. 2. UNETR utilizes a contracting-expanding pat-\ntern consisting of a stack of transformers as the encoder\nwhich is connected to a decoder via skip connections. As\ncommonly used in NLP, the transformers operate on 1D\nsequence of input embeddings. Similarly, we create a 1D\nsequence of a 3D input volume x ‚ààRH√óW√óD√óC with\nresolution (H,W,D) and C input channels by dividing it into\nÔ¨Çattened uniform non-overlapping patches xv ‚ààRN√ó(P3.C)\nwhere (P, P, P) denotes the resolution of each patch and\nN =(H√óW √óD)/P3 is the length of the sequence.\nSubsequently, we use a linear layer to project the patches\ninto a K dimensional embedding space, which remains\nconstant throughout the transformer layers. In order to\npreserve the spatial information of the extracted patches, we\nadd a 1D learnable positional embeddingEpos ‚ààRN√óK to\nthe projected patch embeddingE‚ààR(P3.C)√óK according to\nz0 =[x1\nvE;x2\nvE;...;xN\nv E]+Epos, (1)\nNote that the learnable [class] token is not added to\nthe sequence of embeddings since our transformer backbone\nis designed for semantic segmentation. After the embedding\nlayer, we utilize a stack of transformer blocks [42, 14] com-\nprising of multi-head self-attention (MSA) and multilayer\nperceptron (MLP) sublayers according to\nz‚Ä≤\ni =MSA(Norm(zi‚àí1))+zi‚àí1, i =1...L, (2)\nzi =MLP(Norm(z‚Ä≤\ni))+z‚Ä≤\ni, i =1...L, (3)\nwhere Norm() denotes layer normalization [1], MLP com-\nprises of two linear layers with GELU activation functions,\ni is the intermediate block identiÔ¨Åer, andL is the number of\ntransformer layers.\nA MSA sublayer comprises of n parallel self-attention\n(SA) heads. SpeciÔ¨Åcally, the SA block, is a parameterized\nfunction that learns the mapping between a query ( q) and\nthe corresponding key ( k) and value ( v) representations\nin a sequence z ‚ààRN√óK. The attention weights ( A) are\ncomputed by measuring the similarity between two elements\nin z and their key-value pairs according to\nA=Softmax( qk‚ä§\n‚àöKh\n), (4)\nwhere Kh = K/n is a scaling factor for maintaining the\nnumber of parameters to a constant value with different\nvalues of the keyk. Using the computed attention weights,\nthe output ofSA for valuesv in the sequencez is computed as\nSA(z)= Av, (5)\nHere, v denotes the values in the input sequence and\nKh = K/n is a scaling factor. Furthermore, the output of\nMSA is deÔ¨Åned as\nMSA(z)=[ SA1(z);SA2(z);...;SAn(z)]Wmsa, (6)\nwhere Wmsa ‚àà Rn.Kh√óK represents the multi-headed\ntrainable parameter weights.\nInspired by architectures that are similar to U-Net [ 36],\nwhere features from multiple resolutions of the encoder are\nmerged with the decoder, we extract a sequence representa-\ntion zi (i ‚àà{3,6,9,12}), with size H√óW√óD\nP3 √óK, from the\ntransformer and reshape them into aH\nP √óW\nP √óD\nP √óK tensor.\nA representation in our deÔ¨Ånition is in the embedding space\nafter it has been reshaped as an output of the transformer\nwith feature size of K (i.e. transformer‚Äôs embedding size).\nFurthermore, as shown in Fig. 2, at each resolution we project\nthe reshaped tensors from the embedding space into the input\nspace by utilizing consecutive3√ó3√ó3 convolutional layers\nthat are followed by normalization layers.\nAt the bottleneck of our encoder (i.e. output of trans-\nformer‚Äôs last layer), we apply a deconvolutional layer to the\ntransformed feature map to increase its resolution by a factor\nof 2. We then concatenate the resized feature map with the\n3\nEmbedded\nPatches\nNorm\nMulti-Head\nAttention\n+\nNorm\nMLP\n+\nLinear\nProjection\nùëß3\nC\nC\nC\nC\nConv 3√ó3√ó3, BN, ReLU\nDeconv 2√ó2√ó2, Conv 3√ó3√ó3, BN, ReLU\nDeconv 2√ó2√ó2\nConv 1√ó1√ó1\nùëß6\nùëß9\nùëß12\n√ó12\nùêª\n16√ó ùëä\n16√ó ùê∑\n16√ó768\nùêª\n16√ó ùëä\n16√ó ùê∑\n16√ó768\nùêª\n16√ó ùëä\n16√ó ùê∑\n16√ó768\nùêª\n16√ó ùëä\n16√ó ùê∑\n16√ó768\nùêª√óùëä √óùê∑ √ó4\nùêª\n4 √óùëä\n4 √óùê∑\n4 √ó256\nùêª\n8 √óùëä\n8 √óùê∑\n8 √ó512\nùêª\n2 √óùëä\n2 √óùê∑\n2 √ó128\nùêª\n2 √óùëä\n2 √óùê∑\n2 √ó128\nùêª\n4 √óùëä\n4 √óùê∑\n4 √ó256\nùêª√óùëä √óùê∑ √ó64\nùêª√óùëä √óùê∑ √ó64\nInput \nùêª√óùëä √óùê∑ √ó3\nOutput\nFigure 2. Overview of UNETR architecture. A 3D input volume (e.g.C=4 channels for MRI images), is divided into a sequence of uniform\nnon-overlapping patches and projected into an embedding space using a linear layer. The sequence is added with a position embedding and\nused as an input to a transformer model. The encoded representations of different layers in the transformer are extracted and merged with a\ndecoder via skip connections to predict the Ô¨Ånal segmentation. Output sizes are given for patch resolutionP =16 and embedding sizeK=768.\nfeature map of the previous transformer output (e.g.z9), and\nfeed them into consecutive 3 √ó3 √ó3 convolutional layers\nand upsample the output using a deconvolutional layer. This\nprocess is repeated for all the other subsequent layers up\nto the original input resolution where the Ô¨Ånal output is fed\ninto a 1√ó1√ó1 convolutional layer with a softmax activation\nfunction to generate voxel-wise semantic predictions.\n3.2. Loss Function\nOur loss function is a combination of soft dice loss [32]\nand cross-entropy loss, and it can be computed in a voxel-wise\nmanner according to\nL(G,Y )=1 ‚àí2\nJ\nJ‚àë\nj=1\n‚àëI\ni=1Gi,jYi,j\n‚àëI\ni=1G2\ni,j +‚àëI\ni=1Y 2\ni,j\n‚àí\n‚àí1\nI\nI‚àë\ni=1\nJ‚àë\nj=1\nGi,jlogYi,j.\n(7)\nwhere I is the number of voxels;J is the number of classes;\nYi,j and Gi,j denote the probability output and one-hot\nencoded ground truth for classj at voxeli, respectively.\n4. Experiments\n4.1. Datasets\nTo validate the effectiveness of our method, we utilize\nBTCV [ 26] and MSD [ 38] datasets for three different\nsegmentation tasks in CT and MRI imaging modalities.\nBTCV (CT):The BTCV dataset [26] consists of 30 sub-\njects with abdominal CT scans where 13 organs were anno-\ntated by interpreters under supervision of clinical radiologists\nat Vanderbilt University Medical Center. Each CT scan was\nacquired with contrast enhancement in portal venous phase\nand consists of80 to 225 slices with512√ó512 pixels and slice\nthickness ranging from 1 to 6 mm. Each volume has been\npre-processed independently by normalizing the intensities\nin the range of [-1000,1000] HU to [0,1]. All images are\nresampled into the isotropic voxel spacing of 1.0mm during\npre-processing. The multi-organ segmentation problem is for-\nmulated as a 13 class segmentation task with 1-channel input.\nMSD (MRI/CT):For the brain tumor segmentation task,\nthe entire training set of 484 multi-modal multi-site MRI\ndata (FLAIR, T1w, T1gd, T2w) with ground truth labels\nof gliomas segmentation necrotic/active tumor and oedema\nis utilized for model training. The voxel spacing of MRI\nimages in this tasks is 1.0 √ó1.0 √ó1.0 mm3. The voxel\nintensities are pre-processed with z-score normalization. The\n4\nMethods Spl RKid LKid Gall Eso Liv Sto Aor IVC Veins Pan AG Avg.\nSETR NUP [52] 0.931 0.890 0.897 0.652 0.760 0.952 0.809 0.867 0.745 0.717 0.719 0.620 0.796\nSETR PUP [52] 0.929 0.893 0.892 0.649 0.764 0.954 0.822 0.869 0.742 0.715 0.714 0.618 0.797\nSETR MLA [52] 0.930 0.889 0.894 0.650 0.762 0.953 0.819 0.872 0.739 0.720 0.716 0.614 0.796\nnnUNet [21] 0.942 0.894 0.910 0.704 0.723 0.948 0.824 0.877 0.782 0.720 0.680 0.616 0.802\nASPP [10] 0.935 0.892 0.914 0.689 0.760 0.953 0.812 0.918 0.807 0.695 0.720 0.629 0.811\nTransUNet [7] 0.952 0.927 0.929 0.662 0.757 0.969 0.889 0.920 0.833 0.791 0.775 0.637 0.838\nCoTr w/o CNN encoder [47] 0.941 0.894 0.909 0.705 0.723 0.948 0.815 0.876 0.784 0.723 0.671 0.623 0.801\nCoTr* [47] 0.943 0.924 0.929 0.687 0.762 0.962 0.894 0.914 0.838 0.796 0.783 0.647 0.841\nCoTr [47] 0.958 0.921 0.936 0.700 0.764 0.963 0.854 0.920 0.838 0.787 0.775 0.694 0.844\nUNETR 0.968 0.924 0.941 0.750 0.766 0.971 0.913 0.890 0.847 0.788 0.767 0.741 0.856\nRandomPatch [39] 0.963 0.912 0.921 0.749 0.760 0.962 0.870 0.889 0.846 0.786 0.762 0.712 0.844\nPaNN [53] 0.966 0.927 0.952 0.732 0.791 0.973 0.891 0.914 0.850 0.805 0.802 0.652 0.854\nnnUNet-v2 [21] 0.972 0.924 0.958 0.780 0.841 0.976 0.922 0.921 0.872 0.831 0.842 0.775 0.884\nnnUNet-dys3 [21] 0.967 0.924 0.957 0.814 0.832 0.975 0.925 0.928 0.870 0.832 0.849 0.784 0.888\nUNETR 0.972 0.942 0.954 0.825 0.864 0.983 0.945 0.948 0.890 0.858 0.799 0.812 0.891\nTable 1. Quantitative comparisons of segmentation performance in BTCV test set. Top and bottom sections represent the benchmarks of\nStandard and Free Competitions respectively. Our method is compared against current state-of-the-art models. All SETR [52] baselines use ViT-\nB-16 [14] backbone. Note: Spl: spleen, RKid: right kidney, LKid: left kidney, Gall: gallbladder, Eso: esophagus, Liv: liver, Sto: stomach, Aor:\naorta IVC: inferior vena cava, Veins: portal and splenic veins, Pan: pancreas, AG: adrenal gland. All results obtained from BTCV leaderboard.\nTask/Modality Spleen Segmentation (CT) Brain tumor Segmentation (MRI)\nAnatomy Spleen WT ET TC All\nMetrics Dice HD95 Dice HD95 Dice HD95 Dice HD95 Dice HD95\nUNet [36] 0.953 4.087 0.766 9.205 0.561 11.122 0.665 10.243 0.664 10.190\nAttUNet [34] 0.951 4.091 0.767 9.004 0.543 10.447 0.683 10.463 0.665 9.971\nSETR NUP [52] 0.947 4.124 0.697 14.419 0.544 11.723 0.669 15.192 0.637 13.778\nSETR PUP [52] 0.949 4.107 0.696 15.245 0.549 11.759 0.670 15.023 0.638 14.009\nSETR MLA [52] 0.950 4.091 0.698 15.503 0.554 10.237 0.665 14.716 0.639 13.485\nTransUNet [7] 0.950 4.031 0.706 14.027 0.542 10.421 0.684 14.501 0.644 12.983\nTransBTS [43] - - 0.779 10.030 0.574 9.969 0.735 8.950 0.696 9.650\nCoTr w/o CNN encoder [47] 0.946 4.748 0.712 11.492 0.523 9.592 0.698 12.581 0.6444 11.221\nCoTr [47] 0.954 3.860 0.746 9.198 0.557 9.447 0.748 10.445 0.683 9.697\nUNETR 0.964 1.333 0.789 8.266 0.585 9.354 0.761 8.845 0.711 8.822\nTable 2. Quantitative comparisons of the segmentation performance in brain tumor and spleen segmentation tasks of the MSD dataset. WT,\nET and TC denote Whole Tumor, Enhancing tumor and Tumor Core sub-regions respectively.\nproblem of brain tumor segmentation is formulated as a 3\nclass segmentation task with 4-channel input.\nFor the spleen segmentation task, 41 CT volumes with\nspleen body annotation are used. The resolution/spacing of\nvolumes in task 9 ranges from0.613√ó0.613√ó1.50 mm3 to\n0.977√ó0.977√ó8.0 mm3. All volumes are re-sampled into\nthe isotropic voxel spacing of 1.0mm during pre-processing.\nThe voxel intensities of the images are normalized to the\nrange [0,1] according to 5th and 95th percentile of overall fore-\nground intensities. Spleen segmentation is formulated as a bi-\nnary segmentation task with 1-channel input. For multi-organ\nand spleen segmentation tasks, we randomly sample the input\nimages with volume sizes of[96,96,96]. For brain segmenta-\ntion task, we randomly sample the input images with volume\nsizes of [128,128,128]. For all experiments, the random\npatches of foreground/background are sampled at ratio1:1 .\n4.2. Evaluation Metrics\nWe use Dice score and 95% Hausdorff Distance (HD) to\nevaluate the accuracy of segmentation in our experiments.\nFor a given semantic class, letGi and Pi denote the ground\ntruth and prediction values for voxeli and G‚Ä≤and P‚Ä≤denote\nground truth and prediction surface point sets respectively.\nThe Dice score and HD metrics are deÔ¨Åned as\nDice(G,P)= 2‚àëI\ni=1GiPi\n‚àëI\ni=1Gi+‚àëI\ni=1Pi\n, (8)\nHD(G‚Ä≤,P‚Ä≤)=max {max\ng‚Ä≤‚ààG‚Ä≤\nmin\np‚Ä≤‚ààP‚Ä≤\n‚à•g‚Ä≤‚àíp‚Ä≤‚à•,\nmax\np‚Ä≤‚ààP‚Ä≤\nmin\ng‚Ä≤‚ààG‚Ä≤\n‚à•p‚Ä≤‚àíg‚Ä≤‚à•}. (9)\nThe 95% HD uses the 95th percentile of the distances\nbetween ground truth and prediction surface point sets. As\na result, the impact of a very small subset of outliers is\nminimized when calculating HD.\n4.3. Implementation Details\nWe implement UNETR in PyTorch1 and MONAI2. The\nmodel was trained using a NVIDIA DGX-1 server. All models\nwere trained with the batch size of6, using the AdamW opti-\nmizer [31] with initial learning rate of0.0001 for 20,000 iter-\nations. For the speciÔ¨Åed batch size, the average training time\n1http://pytorch.org/\n2https://monai.io/\n5\nGT\n0.86 0.82 0.80\nnnUNetTransUNetCoTrUNETRCT image  \nSpleen Right kidney Left Kidney Gallbladder Esophagus Liver\nStomach Aorta IVC S&P Veins Pancreas Adrenal glands\n0.83\n0.85 0.76 0.810.82\n0.84 0.78 0.760.81\n0.83 0.80 0.770.81\n0.79 0.74 0.760.76\nFigure 3. Qualitative comparison of different baselines in BTCV cross-validation. The Ô¨Årst row shows a complete representative CT slice.\nWe exhibit four zoomed-in subjects (row 2 to 5), where our method shows visual improvement on segmentation of kidney and spleen (row 2),\npancreas and adrenal gland (row 3), gallbladder (row 4) and portal vein (row 5). The subject-wise average Dice score is shown on each sample.\nwas 10 hours for 20,000 iterations. Our transformer-based\nencoder follows the ViT-B16 [14] architecture with L = 12\nlayers, an embedding size ofK =768. We used a patch resolu-\ntion of 16√ó16√ó16. For inference, we used a sliding window\napproach with an overlap portion of 0.5 between the neighbor-\ning patches and the same resolution as speciÔ¨Åed in Sec. 4.1.\nWe did not use any pre-trained weights for our transformer\nbackbone (e.g. ViT on ImageNet) since it did not demonstrate\nany performance improvements. For BTCV dataset, we have\nevaluated our model and other baselines in the Standard and\nFree Competitions of its leaderboard. Additional data from\nthe same cohort was used for the Free Competition increasing\nthe number of training cases to 80 volumes. For all experi-\nments, we employed Ô¨Åve-fold cross validation with a ratio of\n95:5. In addition, we used data augmentation strategies such\nas random rotation of 90, 180 and 270 degrees, random Ô¨Çip in\naxial, sagittal and coronal views and random scale and shift\nintensity. We used ensembling to fuse the outputs of models\nfrom four different Ô¨Åve-fold cross-validations. For brain and\nspleen segmentation tasks in MSD dataset, we split the data\ninto training, validation and test with a ratio of 80:15:5.\n4.4. Quantitative Evaluations\nUNETR outperforms the state-of-the-art methods\nfor both Standard and Free Competitions on the BTCV\nleaderboard. As shown in Table 1, in the Free Competition,\nUNETR achieves an overall average Dice score of 0.899\nand outperforms the second, third and fourth top-ranked\nmethodologies by 1.238%, 1.696% and 5.269% respectively.\nIn the Standard Competition, we compared the perfor-\nmance of UNETR against CNN and transformer-based\nbaselines. UNETR achieves a new state-of-the-art perfor-\nmance with an average Dice score of 85.3% on all organs.\nSpeciÔ¨Åcally, on large organs, such as spleen, liver and\nstomach, our method outperforms the second best baselines\nby 1.043%, 0.830% and 2.125% respectively,in terms of\nDice score. Furthermore, in segmentation of small organs,\nour method signiÔ¨Åcantly outperforms the second best\n6\nGround Truth\n0.86\nUNetTransBTS CoTrUNETR\n0.83\nFigure 4. UNETR effectively captures the Ô¨Åne-grained details in segmentation outputs. The Whole Tumor (WT) encompasses a union of red, blue\nand green regions. The Tumor Core (TC) includes the union of red and blue regions. The Enhancing Tumor core (ET) denotes the green regions.\nbaselines by 6.382% and 6.772% on gallbladder and adrenal\nglands respectively, in terms of Dice score.\nIn Table 2, we compare the performance of UNETR\nagainst CNN and transformer-based methodologies for brain\ntumor and spleen segmentation tasks on MSD dataset. For\nbrain segmentation, UNETR outperforms the closest baseline\nby 1.5% on average over all semantic classes. In particular,\nUNETR performs considerably better in segmenting tumor\ncore (TC) subregion. Similarly for spleen segmentation,\nUNETR outperforms the best competing methodology by\nleast 1.0% in terms of Dice score.\n4.5. Qualitative Results\nQualitative multi-organ segmentation comparisons are pre-\nsented in Fig. 3. UNETR shows improved segmentation per-\nformance for abdomen organs. Our model‚Äôs capability of\nlearning long-range dependencies is evident in row 3 (from\nthe top), in which nnUNet confuses liver with stomach tissues,\nwhile UNETR successfully delineates the boundaries of these\norgans. In Fig. 3, rows 2 and 4 demonstrate a clear detection of\nkidney and adrenal glands against surrounding tissues, which\nindicate that UNETR captures better spatial context. In com-\nparison to 2D transformer-based models, UNETR exhibits\nhigher boundary segmentation accuracy as it accurately identi-\nÔ¨Åes the boundaries between kidney and spleen. This is evident\nfor gallbladder in row 2, liver and stomach in row 3, and portal\nvein against liver in row 5. In Fig. 4, we present qualitative\nsegmentation comparisons for brain tumor segmentation on\nthe MSD dataset. SpeciÔ¨Åcally, our model demonstrates better\nperformance in capturing the Ô¨Åne-grained details of tumors.\n5. Discussion\nOur experiments in all datasets demonstrate superior per-\nformance of UNETR over both CNN and transformer-based\nsegmentation models. SpeciÔ¨Åcally, UNETR achieves better\nsegmentation accuracy by capturing both global and local\ndependencies. In qualitative comparisons, this is illustrated\nin various cases in which UNETR effectively captures\nlong-range dependencies (e.g. accurate segmentation of the\npancreas tail in Fig. 3).\nMoreover, the segmentation performance of UNETR on\nthe BTCV leaderboard demonstrates new state-of-the-art\nbenchmarks and validates its effectiveness. SpeciÔ¨Åcally\nfor small anatomies, UNETR outperforms both CNN and\ntransformer-based models. Although 3D models already\ndemonstrate high segmentation accuracy for small organs\nsuch as gallbladder, adrenal glands, UNETR can still\noutperform the best competing model by a signiÔ¨Åcant margin\n(See Table 1). This is also observed in Fig. 3, in which\n7\nOrgan Spleen Brain\nDecoder Spleen WT ET TC All\nNUP 0.932 0.721 0.527 0.660 0.636\nPUP 0.941 0.749 0.558 0.698 0.668\nMLA 0.950 0.757 0.563 0.732 0.684\nUNETR 0.964 0.789 0.585 0.761 0.711\nTable 3. Effect of the decoder architecture on segmentation\nperformance. NUP, PUP and MLA denote Naive UpSampling,\nProgressive UpSampling and Multi-scale Aggregation.\nUNETR has a signiÔ¨Åcantly better segmentation accuracy for\nleft and right adrenal glands, and UNETR is the only model\nto correctly detect branches of the adrenal glands. For more\nchallenging tissues, such as gallbladder in row 4 and portal\nvein in row 5, which have low contrast with the surrounding\nliver tissue, UNETR is still capable of segmenting clear\nconnected boundaries.\n6. Ablation\nDecoder Choice In Table 3, we evaluate the effectiveness\nof our decoder by comparing the performance of UNETR\nwith other decoder architectures on two representative\nsegmentation tasks from MRI and CT modalities. In these ex-\nperiments, we employ the encoder of UNETR but replaced the\ndecoder with 3D counterparts of Naive UPsampling (NUP),\nProgressive UPsampling (PUP) and MuLti-scale Aggregation\n(MLA) [52]. We observe that these decoder architectures\nyield sub-optimal performance, despite MLA marginally\noutperforming both NUP and PUP. For brain tumor segmen-\ntation, UNETR outperforms its variants with MLA, PUP\nand NUP decoders by 2.7%, 4.3% and 7.5% on average\nDice score. Similarly, for spleen segmentation, UNETR\nouterforms MLA, PUP and NUP by1.4%, 2.3% and 3.2%.\nPatch Resolution A lower input patch resolution leads\nto a higher sequence length, and therefore higher memory\nconsumption, since it is inversely correlated to the cube of the\nresolution. As shown in Table 4, our experiments demonstrate\nthat decreasing the resolution leads to consistently improved\nperformance. SpeciÔ¨Åcally, decreasing the patch resolution\nfrom 32 to 16 improves the performance by 1.1% and\n0.8% in terms of average Dice score in spleen and brain\nsegmentation tasks respectively. We did not experiment with\nlower resolutions due to memory constraints.\nModel and Computational Complexity In Table 5,\nwe present number of FLOPs, parameters and averaged\ninference time of the models in BTCV benchmarks. Number\nof FLOPs and inference time are calculated based on an input\nsize of 96 √ó96 √ó96 and using a sliding window approach.\nAccording to our benchmarks, UNETR is a moderate-sized\nOrgan Spleen Brain\nResolution Spleen WT ET TC All\n32 0.953 0.776 0.579 0.756 0.703\n16 0.964 0.789 0.585 0.761 0.711\nTable 4. Effect of patch resolution on segmentation performance.\nModels #Params (M) FLOPs (G) Inference Time (s)\nnnUNet [21] 19.07 412.65 10.28\nCoTr [47] 46.51 399.21 19.21\nTransUNet [7] 96.07 48.34 26.97\nASPP [11] 47.92 44.87 25.47\nSETR [52] 86.03 43.49 24.86\nUNETR 92.58 41.19 12.08\nTable 5. Comparison of number of parameters, FLOPs and averaged\ninference time for various models in BTCV experiments.\nmodel with 92.58M parameters and 41.19G FLOPs. For com-\nparison, other transformer-based methods such as CoTr [47],\nTransUNet [7] and SETR [ 52] have 46.51M, 96.07M and\n86.03M parameters and 399.21G, 48.24G and 43.49G FLOPs,\nrespectively. UNETR shows comparable model complexity\nwhile outperforming these models by a large margin in\nBTCV benchmarks. CNN-based segmentation models of\nnnUNet [ 21] and ASPP [ 10] have 19.07M and 47.92M\nparameters and 412.65G and 44.87G FLOPs, respectively.\nSimilarly, UNETR outperforms these CNN-based models\nwhile having a moderate model complexity. In addition,\nUNETR has the second lowest averaged inference time after\nnnUNet [21] and is signiÔ¨Åcantly faster than transformer-based\nmodels such as SETR [52], TransUNet [7] and CoTr [47].\n7. Conclusion\nThis paper introduces a novel transformer-based archi-\ntecture, dubbed as UNETR, for semantic segmentation of\nvolumetric medical images by reformulating this task as a 1D\nsequence-to-sequence prediction problem. We proposed to\nuse a transformer encoder to increase the model‚Äôs capability\nfor learning long-range dependencies and effectively\ncapturing global contextual representation at multiple scales.\nWe validated the effectiveness of UNETR on different\nvolumetric segmentation tasks in CT and MRI modalities.\nUNETR achieves new state-of-the-art performance in both\nStandard and Free Competitions on the BTCV leaderboard\nfor the multi-organ segmentation and outperforms competing\napproaches for brain tumor and spleen segmentation on the\nMSD dataset. In conclusion, UNETR has shown the potential\nto effectively learn the critical anatomical relationships\nrepresented in medical images. The proposed method could\nbe the foundation for a new class of transformer-based\nsegmentation models in medical images analysis.\n8\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer normalization. 2016.\n[2] Spyridon Bakas, Mauricio Reyes, et Int, and Bjoern\nMenze. Identifying the best machine learning algorithms\nfor brain tumor segmentation, progression assessment, and\noverall survival prediction in the BRATS challenge. In\narXiv:1811.02629, 2018.\n[3] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of\nimage transformers. arXiv preprint arXiv:2106.08254, 2021.\n[4] Irwan Bello. Lambdanetworks: Modeling long-range\ninteractions without attention. In International Conference\non Learning Representations, 2020.\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. InEuropean Confer-\nence on Computer Vision, pages 213‚Äì229. Springer, 2020.\n[6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv¬¥e J¬¥egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. arXiv\npreprint arXiv:2104.14294, 2021.\n[7] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan\nAdeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin Zhou.\nTransunet: Transformers make strong encoders for medical\nimage segmentation. arXiv preprint arXiv:2102.04306, 2021.\n[8] Jianxu Chen, Lin Yang, Yizhe Zhang, Mark Alber, and\nDanny Z Chen. Combining fully convolutional and recurrent\nneural networks for 3d biomedical image segmentation. In\nProceedings of the 30th International Conference on Neural\nInformation Processing Systems, pages 3044‚Äì3052, 2016.\n[9] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. Deeplab: Semantic image\nsegmentation with deep convolutional nets, atrous convolution,\nand fully connected crfs. IEEE transactions on pattern\nanalysis and machine intelligence, 40(4):834‚Äì848, 2017.\n[10] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\nseparable convolution for semantic image segmentation. In\nProceedings of the European conference on computer vision\n(ECCV), pages 801‚Äì818, 2018.\n[11] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\nseparable convolution for semantic image segmentation.\narXiv:1802.02611, 2018.\n[12] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing\nRen, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins:\nRevisiting the design of spatial attention in vision transformers.\narXiv preprint arXiv:2104.13840, 1(2):3, 2021.\n[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018.\n[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa\nDehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\net al. An image is worth 16x16 words: Transformers for image\nrecognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[15] Qi Dou, Hao Chen, Yueming Jin, Lequan Yu, Jing Qin, and\nPheng-Ann Heng. 3d deeply supervised network for automatic\nliver segmentation from ct volumes. In International con-\nference on medical image computing and computer-assisted\nintervention, pages 149‚Äì157. Springer, 2016.\n[16] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei\nFang, and Hanqing Lu. Dual attention network for scene\nsegmentation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 3146‚Äì3154,\n2019.\n[17] Eli Gibson, Francesco Giganti, Yipeng Hu, Ester Bonmati,\nSteve Bandula, Kurinchi Gurusamy, Brian Davidson,\nStephen P Pereira, Matthew J Clarkson, and Dean C Barratt.\nAutomatic multi-organ segmentation on abdominal ct with\ndense v-networks. IEEE transactions on medical imaging,\n37(8):1822‚Äì1834, 2018.\n[18] Zaiwang Gu, Jun Cheng, Huazhu Fu, Kang Zhou, Huaying\nHao, Yitian Zhao, Tianyang Zhang, Shenghua Gao, and\nJiang Liu. Ce-net: Context encoder network for 2d medical\nimage segmentation. IEEE transactions on medical imaging,\n38(10):2281‚Äì2292, 2019.\n[19] Nicholas Heller, Niranjan Sathianathen, Arveen Kalapara,\nEdward Walczak, Keenan Moore, Heather Kaluzniak, Joel\nRosenberg, Paul Blake, Zachary Rengel, Makinna Oestreich,\net al. The kits19 challenge data: 300 kidney tumor cases\nwith clinical context, ct semantic segmentations, and surgical\noutcomes. arXiv preprint arXiv:1904.00445, 2019.\n[20] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local\nrelation networks for image recognition. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision,\npages 3464‚Äì3473, 2019.\n[21] Fabian Isensee, Paul F Jaeger, Simon AA Kohl, Jens Petersen,\nand Klaus H Maier-Hein. nnu-net: a self-conÔ¨Åguring method\nfor deep learning-based biomedical image segmentation.\nNature Methods, 18(2):203‚Äì211, 2021.\n[22] Fabian Isensee and Klaus H Maier-Hein. An attempt at beating\nthe 3d u-net. arXiv preprint arXiv:1908.02182, 2019.\n[23] Qiangguo Jin, Zhaopeng Meng, Changming Sun, Hui Cui, and\nRan Su. Ra-unet: A hybrid deep attention-aware network to\nextract liver and tumor in ct scans.Frontiers in Bioengineering\nand Biotechnology, 8:1471, 2020.\n[24] Konstantinos Kamnitsas, Liang Chen, Christian Ledig, Daniel\nRueckert, and Ben Glocker. Multi-scale 3d convolutional\nneural networks for lesion segmentation in brain mri.Ischemic\nstroke lesion segmentation, 13:46, 2015.\n[25] Konstantinos Kamnitsas, Christian Ledig, Virginia FJ New-\ncombe, Joanna P Simpson, Andrew D Kane, David K Menon,\nDaniel Rueckert, and Ben Glocker. EfÔ¨Åcient multi-scale\n3d cnn with fully connected crf for accurate brain lesion\nsegmentation. Medical image analysis, 36:61‚Äì78, 2017.\n[26] B Landman, Z Xu, J Igelsias, M Styner, T Langerak, and\nA Klein. Miccai multi-atlas labeling beyond the cranial\nvault‚Äìworkshop and challenge. InProc. MICCAI Multi-Atlas\nLabeling Beyond Cranial Vault‚ÄîWorkshop Challenge, 2015.\n[27] Wenqi Li, Guotai Wang, Lucas Fidon, Sebastien Ourselin,\nM Jorge Cardoso, and Tom Vercauteren. On the compactness,\nefÔ¨Åciency, and representation of 3d convolutional networks:\n9\nbrain parcellation as a pretext task. InInternational conference\non information processing in medical imaging, pages 348‚Äì360.\nSpringer, 2017.\n[28] Xiaomeng Li, Hao Chen, Xiaojuan Qi, Qi Dou, Chi-Wing Fu,\nand Pheng-Ann Heng. H-denseunet: hybrid densely connected\nunet for liver and tumor segmentation from ct volumes.IEEE\ntransactions on medical imaging, 37(12):2663‚Äì2674, 2018.\n[29] Siqi Liu, Daguang Xu, S Kevin Zhou, Olivier Pauly, Sasa\nGrbic, Thomas Mertelmeier, Julia Wicklein, Anna Jerebko,\nWeidong Cai, and Dorin Comaniciu. 3d anisotropic hybrid\nnetwork: Transferring convolutional features from 2d images\nto 3d anisotropic volumes. In International Conference\non Medical Image Computing and Computer-Assisted\nIntervention, pages 851‚Äì858. Springer, 2018.\n[30] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows.arXiv\npreprint arXiv:2103.14030, 2021.\n[31] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017.\n[32] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.\nV-net: Fully convolutional neural networks for volumetric\nmedical image segmentation. In 2016 fourth international\nconference on 3D vision (3DV), pages 565‚Äì571. IEEE, 2016.\n[33] Miguel Monteiro, Virginia FJ Newcombe, Francois Mathieu,\nKrishma Adatia, Konstantinos Kamnitsas, Enzo Ferrante,\nTilak Das, Daniel Whitehouse, Daniel Rueckert, David K\nMenon, et al. Multiclass semantic segmentation and quantiÔ¨Å-\ncation of traumatic brain injury lesions on head ct using deep\nlearning: an algorithm development and multicentre validation\nstudy. The Lancet Digital Health, 2(6):e314‚Äìe322, 2020.\n[34] Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew\nLee, Mattias Heinrich, Kazunari Misawa, Kensaku Mori,\nSteven McDonagh, Nils Y Hammerla, Bernhard Kainz, et al.\nAttention u-net: Learning where to look for the pancreas.\narXiv preprint arXiv:1804.03999, 2018.\n[35] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jonathon Shlens. Stand-\nalone self-attention in vision models. arXiv preprint\narXiv:1906.05909, 2019.\n[36] O. Ronneberger, P.Fischer, and T. Brox. U-net: Convolutional\nnetworks for biomedical image segmentation. In Proc.\nMICCAI, volume 9351 ofLNCS, pages 234‚Äì241, 2015.\n[37] Holger R Roth, Hirohisa Oda, Yuichiro Hayashi, Masahiro\nOda, Natsuki Shimizu, Michitaka Fujiwara, Kazunari Misawa,\nand Kensaku Mori. Hierarchical 3d fully convolutional\nnetworks for multi-organ segmentation. arXiv preprint\narXiv:1704.06382, 2017.\n[38] Amber L Simpson, Michela Antonelli, Spyridon Bakas,\nMichel Bilello, Keyvan Farahani, Bram Van Ginneken,\nAnnette Kopp-Schneider, Bennett A Landman, Geert Litjens,\nBjoern Menze, et al. A large annotated medical image\ndataset for the development and evaluation of segmentation\nalgorithms. arXiv preprint arXiv:1902.09063, 2019.\n[39] Yucheng Tang, Riqiang Gao, Ho Hin Lee, Shizhong Han, Yun-\nqiang Chen, Dashan Gao, Vishwesh Nath, Camilo Bermudez,\nMichael R Savona, Richard G Abramson, et al. High-\nresolution 3d abdominal segmentation with random patch\nnetwork fusion. Medical Image Analysis, 69:101894, 2021.\n[40] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv¬¥e J¬¥egou. Training\ndata-efÔ¨Åcient image transformers & distillation through\nattention. In International Conference on Machine Learning,\npages 10347‚Äì10357. PMLR, 2021.\n[41] Jeya Maria Jose Valanarasu, Poojan Oza, Ilker Hacihaliloglu,\nand Vishal M Patel. Medical transformer: Gated axial-\nattention for medical image segmentation. arXiv preprint\narXiv:2102.10662, 2021.\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. InAdvances in neural\ninformation processing systems, pages 5998‚Äì6008, 2017.\n[43] Wenxuan Wang, Chen Chen, Meng Ding, Jiangyun Li, Hong\nYu, and Sen Zha. Transbts: Multimodal brain tumor segmen-\ntation using transformer. arXiv preprint arXiv:2103.04430,\n2021.\n[44] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid\nvision transformer: A versatile backbone for dense prediction\nwithout convolutions. arXiv preprint arXiv:2102.12122, 2021.\n[45] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming\nHe. Non-local neural networks. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n7794‚Äì7803, 2018.\n[46] Yingda Xia, Fengze Liu, Dong Yang, Jinzheng Cai, Lequan\nYu, Zhuotun Zhu, Daguang Xu, Alan Yuille, and Holger\nRoth. 3d semi-supervised learning with uncertainty-aware\nmulti-view co-training. In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision, pages\n3646‚Äì3655, 2020.\n[47] Yutong Xie, Jianpeng Zhang, Chunhua Shen, and Yong Xia.\nCotr: EfÔ¨Åciently bridging cnn and transformer for 3d medical\nimage segmentation. arXiv preprint arXiv:2103.03024, 2021.\n[48] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu.\nCo-scale conv-attentional image transformers. arXiv preprint\narXiv:2104.06399, 2021.\n[49] Lequan Yu, Xin Yang, Hao Chen, Jing Qin, and Pheng Ann\nHeng. V olumetric convnets with mixed residual connections\nfor automated prostate segmentation from 3d mr images. In\nProceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence,\nvolume 31, 2017.\n[50] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus\nOdena. Self-attention generative adversarial networks.\nIn International conference on machine learning , pages\n7354‚Äì7363. PMLR, 2019.\n[51] Yundong Zhang, Huiye Liu, and Qiang Hu. Transfuse: Fusing\ntransformers and cnns for medical image segmentation.arXiv\npreprint arXiv:2102.08005, 2021.\n[52] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip HS Torr, et al. Rethinking semantic segmentation\nfrom a sequence-to-sequence perspective with transformers.\narXiv preprint arXiv:2012.15840, 2020.\n[53] Yuyin Zhou, Zhe Li, Song Bai, Chong Wang, Xinlei Chen, Mei\nHan, Elliot Fishman, and Alan L Yuille. Prior-aware neural\n10\nnetwork for partially-supervised multi-organ segmentation.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 10672‚Äì10681, 2019.\n[54] Qikui Zhu, Bo Du, Baris Turkbey, Peter L Choyke, and\nPingkun Yan. Deeply-supervised cnn for prostate segmen-\ntation. In 2017 international joint conference on neural\nnetworks (IJCNN), pages 178‚Äì184. IEEE, 2017.\n[55] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable detr: Deformable transformers for\nend-to-end object detection. arXiv preprint arXiv:2010.04159,\n2020.\n11",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7587356567382812
    },
    {
      "name": "Segmentation",
      "score": 0.732144296169281
    },
    {
      "name": "Encoder",
      "score": 0.6630244851112366
    },
    {
      "name": "Transformer",
      "score": 0.6129216551780701
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5908229947090149
    },
    {
      "name": "Artificial intelligence",
      "score": 0.587472140789032
    },
    {
      "name": "Deep learning",
      "score": 0.5068705677986145
    },
    {
      "name": "Image segmentation",
      "score": 0.4335881769657135
    },
    {
      "name": "Recurrent neural network",
      "score": 0.4201076626777649
    },
    {
      "name": "Feature learning",
      "score": 0.41126880049705505
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.40321868658065796
    },
    {
      "name": "Artificial neural network",
      "score": 0.34610995650291443
    },
    {
      "name": "Computer vision",
      "score": 0.33975639939308167
    },
    {
      "name": "Engineering",
      "score": 0.1022803783416748
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": [],
  "cited_by": 206
}