{
    "title": "Towards Pareto Optimal Throughput in Small Language Model Serving",
    "url": "https://openalex.org/W4394007410",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5026630988",
            "name": "Pol G. Recasens",
            "affiliations": [
                "Universitat Polit√®cnica de Catalunya",
                "Barcelona Supercomputing Center"
            ]
        },
        {
            "id": "https://openalex.org/A2108041374",
            "name": "Yue Zhu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2096334414",
            "name": "Chen Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2114364457",
            "name": "Eun Kyung Lee",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A284463012",
            "name": "Olivier Tardieu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2043613386",
            "name": "Alaa Youssef",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2105376436",
            "name": "Jordi Torres",
            "affiliations": [
                "Barcelona Supercomputing Center",
                "Universitat Polit√®cnica de Catalunya"
            ]
        },
        {
            "id": "https://openalex.org/A2026048867",
            "name": "Josep Ll. Berral",
            "affiliations": [
                "Barcelona Supercomputing Center",
                "Universitat Polit√®cnica de Catalunya"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4321636575",
        "https://openalex.org/W6810081322",
        "https://openalex.org/W4387321091",
        "https://openalex.org/W4390189088",
        "https://openalex.org/W3159727696",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W4224308101"
    ],
    "abstract": "Large language models (LLMs) have revolutionized the state-of-the-art of many different natural language processing tasks. Although serving LLMs is computationally and memory demanding, the rise of Small Language Models (SLMs) offers new opportunities for resource-constrained users, who now are able to serve small models with cutting-edge performance. In this paper, we present a set of experiments designed to benchmark SLM inference at performance and energy levels. Our analysis provides a new perspective in serving, highlighting that the small memory footprint of SLMs allows for reaching the Pareto-optimal throughput within the resource capacity of a single accelerator. In this regard, we present an initial set of findings demonstrating how model replication can effectively improve resource utilization for serving SLMs.",
    "full_text": "Towards Pareto Optimal Throughput in\nSmall Language Model Serving\nPol G. Recasens\nBarcelona Supercomputing Center\npol.garcia@bsc.es\nYue Zhu\nIBM Research\nYue.Zhu@ibm.com\nChen Wang\nIBM Research\nChen.Wang1@ibm.com\nEun Kyung Lee\nIBM Research\neunkyung.lee@us.ibm.com\nOlivier Tardieu\nIBM Research\ntardieu@us.ibm.com\nAlaa Youssef\nIBM Research\nasyousse@us.ibm.com\nJordi Torres\nBarcelona Supercomputing Center\nUniversitat Polit√®cnica de Catalunya\njordi.torres@bsc.es\nJosep Ll. Berral\nUniversitat Polit√®cnica de Catalunya\nBarcelona Supercomputing Center\njosep.ll.berral@upc.edu\nAbstract\nLarge language models (LLMs) have revolutionized the state-\nof-the-art of many different natural language processing\ntasks. Although serving LLMs is computationally and mem-\nory demanding, the rise of Small Language Models (SLMs)\noffers new opportunities for resource-constrained users, who\nnow are able to serve small models with cutting-edge per-\nformance. In this paper, we present a set of experiments\ndesigned to benchmark SLM inference at performance and\nenergy levels. Our analysis provides a new perspective in\nserving, highlighting that the small memory footprint of\nSLMs allows for reaching the Pareto-optimal throughput\nwithin the resource capacity of a single accelerator. In this\nregard, we present an initial set of findings demonstrating\nhow model replication can effectively improve resource uti-\nlization for serving SLMs.\n1 Introduction\nLarge language models (LLMs) have revolutionized the state-\nof-the-art of many natural language processing tasks, and\nshow impressive zero-shot and few-shot capabilities in a\nwide range of applications. However, deploying language\nmodels is computationally and memory-intensive, often de-\nmanding multiple accelerators to host the model weights. For\ninstance, the largest model from the OPT family has 175B\nparameters [35], occupying 350GB of memory space and\nrequiring the distribution of the model across multiple de-\nvices. Although the success of LLMs was traditionally attrib-\nuted to scale, recent research suggests that a curated dataset\nmight play an important role in training high-performance\nmodels [5, 8, 9]. This paradigm shift, coupled with new serv-\ning optimization strategies, holds a substantial impact for a\nresource-constrained user, that is now able to serve SOTA\nsmall models. This rise of Small Language Models (SLMs)\nrepresents a significant step forward in making AI more\naccessible.\nDespite the smaller size of SLMs, the incremental decoding\nof autoregressive language models limits the serving perfor-\nmance. Due to data dependencies in the self-attention layer,\nwe process a single token per iteration, leading to matrix-\nvector operations. This, coupled with the large cost of loading\nthe model weights from memory, leads to very low arithmetic\nintensity during single-batch inference [12]. One way to in-\ncrease the arithmetic intensity, defined as the ratio between\narithmetic operations and bytes accessed, is to batch requests\nand compute multiple tokens for the same transfer of weights.\nAs long as the memory-transfer time overlaps the compute\ntime we can potentially improve the serving throughput by\nincreasing the batch size. Still, after a certain size the com-\npute time becomes non-negligible, and might grow larger\nthan the memory transfer time. Large batches have been pre-\nviously associated with compute bound scenarios [1, 11, 12],\nbut due to the substantial memory demands of LLMs they\nare rarely reached in practice. How large batches affect the\nserving performance of the less memory-demanding SLMs\nhas yet to be explored.\nHowever, batching techniques demand more memory to\nstore key-value pairs of previously processed tokens. The\nspace in memory dedicated to store the intermediate results\nof previous tokens is known as KV cache [20], and handling\nit naively leads to memory fragmentation [14]. PagedAtten-\ntion [14] algorithm identified this challenge and effectively\nreduced memory waste by dividing the KV cache in blocks, al-\nlowing to store KV pairs in non-contiguous memory space. In\nour experiments, we leverage vLLM [14], a high-throughput\nonline serving engine based on PagedAttention, to guarantee\nachieving the maximum batch size from our computational\nresources.\nIn this paper, we provide a set of experiments to bench-\nmark SLM inference at performance and energy levels. In\nthis regard, we serve OPT [35] models ranging from 125M\narXiv:2404.03353v3  [cs.CL]  7 Aug 2025\nPol G. Recasens, et al.\nto 13B parameters in various online scenarios, sending re-\nquests generated from the ShareGPT dataset [23]. We char-\nacterise the throughput and latency trade-off when the small\nmemory footprint allows for large batches of requests, and\ncomplement the study with internal GPU metrics, highlight-\ning the effect of SLM inference on energy consumption. To\nthe best of our knowledge, this provides a novel perspec-\ntive, as previous inference benchmarking works are limited\nand primarily focused on large-scale serving [3, 22]. From\nour results, we observe that the Pareto-optimal throughput\nwith small models is reached within the resource capacity\nof a single accelerator. This paves the way to new optimiza-\ntions, such as partitioning of GPU resources in multi-model\nserving. In this context, we present an initial set of findings\ndemonstrating how model replication can improve resource\nutilization for serving SLMs.\n2 Background\n2.1 KV Cache in Autoregressive Models\nThe autoregressive generation of decoder-only Transformer\nmodels can be decomposed in two phases. Given an input\nprompt (ùë•1,...,ùë• ùë°), the model first generates the intermediate\nkeys and values of the prompt tokens. This phase is named\nprefill phase, and can be efficiently computed as each position\ncan be processed in parallel. Then, in theautoregressive phase,\nthe model generates one token per iteration until completing\nthe generation or reaching a maximum sequence length. This\nautoregressive behaviour is inherent to the neural attention\nfunction [2], applied through different self-attention layers\nin the model. This operation linearly transforms an input\nvector ùë•ùë° into query, key and value vectors ùëûùë°,ùëòùë°,ùë£ùë°.\nùëûùë° = ùëäùëÑ\nùë° ‚àóùë•ùë°,ùëòùë° = ùëäùêæ\nùë° ‚àóùë•ùë°,ùë£ùë° = ùëäùëâ\nùë° ‚àóùë•ùë° (1)\nThe output vector ùëúùë° of the layer results from a weighted\naverage between values from previous positions (ùë£1,...,ùë£ ùë°),\neach weighted with its attention score ùëé = ùëûùë° ‚àóùëòùëá. There-\nfore, each token attends to every token in the sequence up\nto its position. During training, the computation of the self-\nattention layer can be efficiently implemented as we know\nthe ground-truth target sequence [ 24]. However, data de-\npendencies prevent a parallel computation during inference,\nresulting in the mentioned incremental computation. In this\ncontext, we are interested in storing the intermediate keys\nand values of each token from one iteration to the next, avoid-\ning re-computations. The space in the GPU high-bandwidth\nmemory (HBM) dedicated to store these intermediate results\nis named KV cache [20], and grows with the model size and\nthe number of tokens in the batch. Specifically, the KV cache\nof a token is calculated as 2 (FP16) * 2 (key and value tensors)\n* (hidden dimensions) * (number of layers) [14]. For instance,\nin the case of OPT 1.3B, a request with 512 input tokens and\n256 output tokens requires 50 MB. Therefore, a batch of 512\nrequests demands at most 25.6 GB of memory space to store\nthe KV cache.\n2.2 Batching techniques\nThe low arithmetic intensity of the autoregressive phase\nprompts the adoption of optimization techniques to increase\nthe resource utilization. Batching techniques improve serv-\ning performance by computing multiple requests at a time,\nthereby increasing the arithmetic operations per iteration, at\nthe cost of transferring weights from the HBM to the on-chip\nSRAM once. It is worth noting that this also increments the\namount of memory transferred due to the KV cache.Dynamic\nbatching involves waiting a predefined window of time to\ncollect a group of requests sent to the engine. This approach\noperates at request-level granularity, leading to inefficiencies\ndue to the waiting window and the delay in processing subse-\nquent batch requests until all sequences in the current batch\nare completed. Continuous batching addresses the problem\nby operating at iteration-level granularity, with the sched-\nuler deciding at each forward pass which requests join or\nleave the batch. This solves the fragmentation and increases\nthe overall throughput. However, it faces challenges, particu-\nlarly in batching attention operations of requests running at\ndifferent output indices. In this regard, Orca [34] proposed\nselective batching, batching requests on linear operations and\nsequentially computing them during attention operations.\n2.3 Understanding performance limiters\nPerformance of an inference step on a given processor can be\nmemory-IO bound , limited by the time spent accessing mem-\nory, or compute bound , limited by the time spent computing\noperations. The metric used to measure the limiting fac-\ntor is the arithmetic intensity , defined as the ratio ops:byte\nbetween compute operations and bytes transferred from\nHBM memory. Due to the low arithmetic intensity in the\nautoregressive generation phase, the performance of single-\nbatch inference is commonly classified as memory-IO bound\n[12, 13, 18]. This scenario is frequently found in memory-\nlimited scenarios, with large models and small batch sizes.\nTherefore, as long as memory-IO time overlaps compute\ntime, we can theoretically increase the arithmetic intensity\nand improve the serving throughput without affecting end-\nto-end latency.\nHowever, as we increase the inner size of the matrix-\nmatrix operations with larger batches, compute might be-\ncome important. For instance, a linear layer with a large\nbatch is usually limited by arithmetic [18]. This might also\nbe influenced by how continuous batching sequentially pro-\ncesses attention operations of different requests (see Discus-\nsion 6). With large batches compute time grows to be larger\nthe memory-IO time, reaching a Pareto-optimal throughput\nfrontier. Beyond that point, further increasing of the batch\nsize does not improve the serving performance. The compute\noperations in a forward pass N parameter decoder model\nTowards Pareto Optimal Throughput in Small Language Model Serving\ncan be estimated 2ùëÅ +6ùêøùêªùëÑùëá add-multiply FLOPs per to-\nken seen, with L representing the number of layers, H the\nnumber of heads, Q the head dimension and T the sequence\nlength [4].\n2 1\n 20 21 22 23 24 25 26\nLatency (s)\n2 2\n2 1\n20\n21\n22\n23\n24\nThroughput (req/s)\n1\n2\n4\n8\n16\n32 64\n128 256 512\n256\n128\n64\n16\nOPT-125M\nOPT-1.3B\nOPT-2.7B\nOPT-6.7B\nOPT-13B\n1 GPU\n2 GPUs\n4 GPUs\nFigure 1. Throughput and latency trade-off for models of\nincreasing size and batch sizes in powers of two. With small\nmodels we observe that throughput reaches a Pareto-optimal\nfrontier within the resource capacity of a single accelerator.\n3 Experimental setup\nIn this section, we provide an overview of the hardware setup\nand experiment configurations employed to evaluate the\ninference performance of SLMs. The serving engine utilized\nfor these experiments is vLLM1 [14], an open-source library\ndesigned for high-throughput online serving of language\nmodels. This engine incorporates PagedAttention, a novel\nattention mechanism that reduces memory fragmentation in\nthe KV cache to the minimum. This allows us to benchmark\nacross various scenarios with the maximum possible batch\nsize.\nThe goal of our experiments is to characterise the perfor-\nmance of serving SLMs. Since these small models require sig-\nnificantly less memory than larger LLMs, we expect to batch\nenough requests to reach the throughput frontier within the\nmemory available in a high-end accelerator. In our experi-\nments, we increasingly allocate more memory to the serving\nsystem via distributed inference to find the Pareto optimality\npoint. Other optimization techniques, such as quantization,\ndistillation, or sparsity, can be coupled with batching. These\ntechniques aim to reduce the memory footprint of the model,\nthereby creating space for larger batch sizes. We leave as\nfuture work incorporating these techniques with batching.\n3.1 Models and hardware configuration\nWe employ models from the OPT [ 35] family, with sizes\nranging from 125M to 13B parameters. Introduced by MetaAI\nin May 2022, OPT belongs to decoder-only architectures such\nas GPT-3. It was pretrained using the self-supervised causal\n1We used version v0.1.7.\nlanguage modeling objective mainly over a large amount\nof English text. The weights of the models are provided\nby the Transformers library [32] from HuggingFace. Also,\nexperiments on LLaMa 7B [28], MPT 7B [27] and Chat-GPT\nJ 6B [30] are included for consistency.\nWe use an internal IBM cluster of machines composed of\n4 NVIDIA A100 GPU‚Äôs interconnected with NVLink. Each\nGPU has 40GB of HBM and a GPU memory bandwidth of\n1555GB/s. The vLLM engine reserves 90% of the HBM by\ndefault for parameters and the KV cache, resulting in a max-\nimum memory allocation of 36GB for a single device. In\ndistributed inference, vLLM leverages the Megatron-LM‚Äôs\ntensor parallel algorithm. This involves distributing the com-\nputation and memory usage across devices [26] using tensor\nand pipeline parallelism. In our context, we only use tensor\nparallelism, slicing and distributing the computation of the\nself-attention and MLP blocks to the workers.\n3.2 Workload\nWe generate 500 requests from the ShareGPT dataset [23], a\ncollection of real conversations with ChatGPT. The prompt\nof each request is composed by 512 input tokens, and we\nlimit the generation to 256 output tokens. Therefore, each re-\nquest requires storing KV pairs of up to 768 tokens in the KV\ncache, divided in blocks of 16 tokens allocated on-demand.\nWe opt not to apply beam search so we are greedily choosing\nthe token with highest probability, and we set top_p to 1 to\nconsider all tokens. This synthetic workload is intentionally\nrestricted to study the effect of larger batches in a straight-\nforward and consistent manner, providing an estimate of the\namount of memory required to reach the optimal throughput.\nThen, the serving system can adapt to a realistic scenario\nwith requests of varying input and output lengths. Each re-\nquest is tokenized and sent to the vLLM engine through an\nhttp request, simulating a real-world deployment scenario.\nAlthough we include experiments with different arrival rates,\nwe primarily focus on sending all the requests concurrently\nfor various batch settings, helping to evaluate different batch\nconfigurations. It is worth noting that models larger than\nOPT-125m can not process the 500 requests concurrently\nwith a single GPU.\n3.3 Metrics\nBatching techniques are employed to increase the serving\nthroughput and improve resource utilization. We measure\nthroughput, defined as the number of requests processed per\nsecond by the engine, and latency, defined as the average\nend-to-end processing time across all requests. Good serv-\ning performance should maximize the system throughput\nwhile providing low latency to users. In our main experimen-\ntation we consider requests of the same length, therefore\nthe throughput and latency per token is equivalent to the\nthroughput and latency per request normalized by the num-\nber of tokens.\nPol G. Recasens, et al.\nTable 1. Maximum batch size in a 40 GB A100 GPU, capped\nat 512, for OPT models ranging in size from 125M to 13B\nparameters. We consider sizes in powers of two.\nModel Parameters Maximum batch size\nOPT-125M 250 MB 512\nOPT-1.3B 2.6 GB 256\nOPT-2.7B 5.4 GB 128\nOPT-6.7B 13.4 GB 64\nOPT-13 B 26 GB 16\nAlso, we analyze the performance of the system at sys-\ntem‚Äôs level, relying on NVIDIA internal metrics. Whilenvidia-\nsmi is frequently used, it is not the optimal choice for per-\nformance measurements. For instance, the gpu-utilization\nmetric shows the percentage of time where one or more ker-\nnels where executed on the GPU, without considering the\nnumber of streaming multiprocessors (SMs) being utilized\nduring that time. Therefore, we use NVIDIA Data Center\nGPU Manager (DCGM) and the following metrics [19]:\n‚Ä¢DCGM_FI_DEV_GPU_UTIL: Fraction of time that the\nGPU is not idle.\n‚Ä¢DCGM_FI_PROF_SM_ACTIVE: Fraction of time that\nat least one warp is active on an SM, averaged across\nall SMs. A thread warp is considered to be active if\nit scheduled, regardless of whether it is actively com-\nputing or waiting for resources.\n‚Ä¢DCGM_FI_PROF_SM_OCCUPANCY: Fraction of resi-\ndent warps on a SM, relative to the maximum number\nof concurrent warps supported.\n‚Ä¢DCGM_FI_DEV_POWER_USAGE: Power usage for\nthe device in Watts.\n4 Results\n4.1 Finding the optimal batch size\nWe are interested in observing how large batches affect to\nthe inference performance. While it proves challenging to\nbatch even a small number of requests in LLM serving, SLMs\nintroduce a unique scenario where a single accelerator can\nmanage the memory requirements for storing a larger num-\nber. The performance implications of this aspect have yet to\nbe explored. In this first analysis, we benchmark the system‚Äôs\nperformance when serving small models of increasing size\nfor different batch sizes. If the batch size cannot be achieved\nwith a single accelerator, we distribute the serving across\nmultiple GPUs to increase available memory. Additionally,\nwe characterize in detail the effects of these variations on\nthe GPU with DCGM metrics.\n4.1.1 Throughput/latency analysis. Table 1 shows the\nmaximum batch size that can fit in 40GB of HBM memory\nfrom a A100 GPU for each model size, and Figure 1 the\nthroughput and latency trade-off for the different models.\nEach data point corresponds to a batch size ranging from 1 to\nthe maximum attainable batch size on 4 A100 GPUs, limited\nat 512. In Figure 1, we notice a decline in throughput when\ndistributing the serving to multiple GPUs, that we mainly\nattribute to the overhead in memory transfer across GPUs.\nIn Figure 1, it is evident that with large batches we reach\na throughput Pareto-optimal frontier. One observation that\nstands out is that for OPT 125M, OPT 1.3B and OPT 2.7B, this\nthroughput frontier appears within the resource capacity of a\nsingle accelerator. Beyond this point doubling the size of the\nbatch results in minimal or no improvement. For instance, in\nthe case of OPT 125M and OPT 1.3B, after a batch size of 128\nwe observe a marginal improvement in the throughput at the\ncost of a considerable degradation in latency. However, in\nthe case of a larger model such as OPT 13B, distributing the\nserving from 2 to 4 devices is still improving the throughput.\nThis is due to the memory requirements of larger models,\nwhich scale up the KV cache size needed to reach the fron-\ntier. Following what we discussed in section 2.3, batching\nincreases the arithmetic intensity and improves the serv-\ning performance. However, with large batches the compute\ntime grows to be larger than the memory-IO time, shifting\nthe performance limitation towards a compute-bound sce-\nnario. In the case of SLMs, this performance upper-bound\nopens the door to a potential slicing of the GPU resources in\nmulti-model serving, where a scheduler assigns the optimal\namount to each model depending on its individual charac-\nteristics. In that case, we have the guarantee that allocating\nmore memory to serving a model, therefore allowing larger\nbatch sizes, do not incur better performance.\nIn addition, Figure 2 (right) shows the effect of output\nlength in performance when serving OPT 6.7B, sending re-\nquests with different arrival rates. The waiting time for each\nrequest is sampled from a Poisson distribution. If the arrival\nrate is ùëñùëõùëì all requests are sent instantly. We observe how\nincreasing the number of output tokens leads to a decrease\nin throughput. Since the maximum amount of tokens that\nwe can store in KV cache is defined by the hardware, increas-\ning the output length reduces the batch size. Also, there is\na slightly increase in latency. This might be related to the\noverhead in self-attention computation, which depends on\nthe sentence length (see Section 2.3). In Figure 2 (left), we\nobserve how the throughput and latency curve is consistent\nfor different families of models.\n4.1.2 System analysis. Figure 3 shows the SM active and\nSM occupancy metrics for different batch sizes and models\nof increasing size. We show the maximum and the average\nvalues of the metrics over all cycles of the execution. The\nmaximum SM occupancy remains low, barely reaching a 40%,\nand the maximum SM activity increases with the batch size\nand reaches the limit. vLLM assigns one SM per sequence\n[29] in the attention computation of the batch, leading to\nTowards Pareto Optimal Throughput in Small Language Model Serving\n21 22\nLatency (s)\n2 1\n20\n21\n22\nThroughput (req/s)\n64\nOPT-6.7B\nMPT-7B\nGPT-J-6B\nLlaMA-7B\n2 2\n 2 1\n 20 21 22\nLatency (s)\n23\n24\nThroughput (req/s)\n 256\n32 64\n128\n256\ninf\n100\n10\nFigure 2. (Left) Throughput and latency trade-off for families\nof decoder models of similar size. (Right) Throughput and\nlatency trade-off when serving OPT 6.7B with output lengths\nranging from 32 to 256 in powers of two.\nhigher SM activity with larger batches. Although the maxi-\nmum SM activity reaches the limit (at least one cycle in all\nthe execution), the average SM activity remains low, which\nmotivates model replication to increase the utilization of\nresources.\nOPT-125M OPT-1.3B OPT-2.7B OPT-6.7B OPT-13B\nModel\n0\n50\n100\n150\n200\n250Average Power Usage (Watts)\nNVIDIA A100 GPU Power Usage\nFigure 4. Average power usage in Watts of a NVIDIA A100\nGPU. The value corresponds to the average power usage\nacross all timesteps. We set the batch size of the model to\nthe maximum in powers of two (see Table 1).\nIt is also interesting to study the impact of model size on\nenergy costs. Figure 4 shows the average GPU power usage\nconsumed by an A100 GPU when serving models of increas-\ning size, with the maximum possible batch size (see Table 1).\nEven though we batch the maximum number of requests, the\nenergy consumption when serving OPT 1.3B and OPT 2.7B\nis considerably lower than the energy consumption when\nserving larger models. This suggests both underutilization of\nresources and possible opportunities in limiting the power\nto reduce energy costs. Also, we observe a slight increase\non energy consumption between OPT 6.7B and OPT 13B,\ncompared to a larger increase in GPU utilization. This spe-\ncific behaviour might stem from NVIDIA‚Äôs thermal throttling\nmechanism, which underclocks the card and restricts power\nconsumption and heat generation.\n4.2 Model replication\nGPU resources are scarce, and our previous results show that\nover provisioning memory to SLMs, therefore increasing the\nbatch size, does not necessarily correlate to a performance\nimprovement. With this knowledge in hand, we can limit the\nmemory allocated to each model, and run different models in\nthe same accelerator, or replicate the same model with multi-\nple instances. In this section, we provide a first set of findings\non how we can leverage model replication to improve end-to-\nend serving performance. We limit the amount of memory\nallocated by each vLLM instance, and run multiple instances\nof OPT-125M, OPT-1.3B and OPT-2.7B simultaneously in the\nsame device. Following the initial experimental setting, we\ngenerate 500 requests, but in this case they are distributed\nevenly across the replicas. The overall throughput is the sum\nof each replica‚Äôs throughput, and the overall latency is the\nlargest latency among replicas. For the sake of simplicity, we\nlimit the batch size to the maximum attainable batch size in\none device (see Table 1) divided by the number of replicas.\nOPT-125M OPT-1.3B OPT-2.7B\nModel\n0\n10\n20\n30\n40\n50\n60Throughput (req/s)\n1 replica\n2 replicas\n3 replicas\nOPT-125M OPT-1.3B OPT-2.7B\nModel\n0\n10\n20\n30E2E Latency (s)\nFigure 5. Throughput and latency trade-off when serving\nup to 3 replicas per model. We assign the same percentage\nof memory to each vLLM instance, all adding to 90%.\nFollowing the intuition build from the results of the pre-\nvious section, we partition the memory by the number of\ninstances and observe how it affects to the overall perfor-\nmance. The allocated memory for each instance is indepen-\ndent, and the compute is shared among processes in a First\nCome First Serve (FCFS) manner. Figure 5 shows an im-\nprovement in both the throughput and the latency when\nrunning multiple instances simultaneously. We observe a\nconsiderable improvement via model replication for OPT\n125M, which suggest a clear underutilization at all levels of\nthe GPU when serving really small models. Although in the\ncase of OPT 2.7B, serving two models in the same device\nslightly improves the throughput, it has a positive impact on\nthe end-to-end latency.\nRegarding the GPU performance, Figure 6 shows the im-\npact of model replication in the average GPU utilization.\nWe observe a considerable increase in GPU utilization in the\nthree models as we increase the number of replicas. Similarly,\nin Figure 7 we can observe how the average power usage\nincreases with the number of replicas in the case of OPT\n125M and OPT 1.3B, while remaining constant for OPT 2.7B.\nSimilarly to the observation made in Section 4.1.2, this might\nbe due to the internal thermal throttling mechanism of the\nPol G. Recasens, et al.\n128 256 512\nBatch Size\n0.00\n0.25\n0.50\n0.75\n1.00Utilization\nSM_ACTIVE (MAX)\n128 256 512\nBatch Size\n0.00\n0.25\n0.50\n0.75\n1.00\nSM_ACTIVE (AVG)\n128 256 512\nBatch Size\n0.00\n0.25\n0.50\n0.75\n1.00\nSM_OCCUPANCY (MAX)\n128 256 512\nBatch Size\n0.00\n0.25\n0.50\n0.75\n1.00\nSM_OCCUPANCY (AVG)\nOPT-125M OPT-1.3B OPT-2.7B OPT-6.7B\nFigure 3. SM active and SM occupancy metrics when serving models ranging from 125M to 6.7B with batches in powers of\ntwo on one device. We show the mean and maximum values over all time steps, ranging from 1 to the maximum batch size.\nGPU. Therefore, we observe a positive impact of model repli-\ncation on both the GPU performance and the throughput\nand latency of the system.\nOPT-125M OPT-1.3B OPT-2.7B\nNumber of replicas\n0\n20\n40\n60\n80\n100Average GPU Utilization (\\%)\nNVIDIA A100 GPU Utilization\n1 replicas\n2 replicas\n3 replicas\nFigure 6. Average GPU utilization when serving OPT 125M,\nOPT 1.3B and OPT 2.7B in an NVIDIA A100 GPU. The model\nis replicated up to three times, if possible.\nOPT-125M OPT-1.3B OPT-2.7B\nNumber of replicas\n0\n50\n100\n150\n200\n250Average Power Usage (Watts)\nNVIDIA A100 GPU Power Usage\n1 replicas\n2 replicas\n3 replicas\nFigure 7. Average power usage in Watts when serving OPT\n125M, OPT 1.3B and OPT 2.7B in an NVIDIA A100 GPU. The\nmodel is replicated up to three times, if possible.\n5 Related work\nServing language models.With LLMs already estab-\nlished as state-of-the-art for many different tasks, there is\na growing interest in efficiently deploying and serving lan-\nguage models. In this regard, general online serving systems\nhave emerged, such Orca [ 34], Text Generation Inference\n(TGI) [10], DeepSpeed-FastGen [1, 16] or vLLM [14], which\nimprove serving performance through different optimiza-\ntions. Other serving systems, like AlpaServe [15], optimize\nmulti-model serving via statistical multiplexing of multiple\ndevices. Additionally, engines such as FasterTransformer\n[17] provide a highly optimized implementation of language\nmodels written in C++/CUDA.\nMemory optimizations.Memory size is lagging behind\nthe increasing compute capabilities in modern accelerators.\nTherefore, several approaches aim to reduce the resource\nrequirements for serving language models. In this regard,\nquantization [6, 25, 33] minimizes the memory footprint\nby compressing the model weights, sparsity [ 31] prunes\nunimportant tokens and heads, and offloading techniques\nsuch as [25] leverage memory from CPU and disk in offline\nserving scenarios. We leave as further work benchmarking\nthe systems performance while coupling batching with these\noptimizations, that indirectly increase the effective batch\nsize.\nManaging the KV cache.The management of the KV\ncache in LLM serving is challenging due to the unpredictable\nnumber of tokens generated per request, and it limits the\nnumber of requests that can be effectively batched. Orca‚Äôs\n[34] initial approach pre-allocates contiguous GPU mem-\nory for the maximum possible output length, introducing\nmemory fragmentations. Recent works identified and ad-\ndressed this problem. S3 [11] proposes to predict the number\nof output tokens of each request, adjusting the amount of\nmemory that is pre-allocated. On the other hand, vLLM [14]\nintroduces PagedAttention, an attention algorithm inspired\non OS paging that divides the KV cache in blocks of tokens,\nand stores them in non-contiguous memory blocks. This ap-\nproach dynamically allocates blocks on-demand, and allows\nmemory sharing in complex decoding scenarios (e.g parallel\nsampling). The KV cache can be further optimized to reuse\nattention states across different requests. In this regard, the\nprompt cache [7] precomputes attention states of frequently\nvisited text segments and reuses them for different sequences.\nAlso, RadixAttention mantains a radix tree on the CPU to\nreuse the KV cache during runtime. For our experiments, we\nadhere to vLLM and PagedAttention to control the KV cache\nsize for each request.\nTowards Pareto Optimal Throughput in Small Language Model Serving\n6 Discussion\nThe growing interest in the topic of LLM inference leads\nto never-ending improvements in serving systems. There-\nfore, our experiments are linked to the performance of the\ncurrent implementation of vLLM. Although we do not cou-\nple batching with other optimization techniques to mitigate\npotential issues, there are some inherent limitations in the\nimplementation. For instance, PyTorch networks that are\nrunning on high-throughput GPUs are frequently limited\nby CPU overheads [21], that might take up to a 50% of the\nlatency [29]. This leads to unusual behaviour in GPU uti-\nlization metric: while increasing the batch size in increases\nthe overall throughput (see Figure 8), the GPU utilization\nremains constant, and decreases with large batches. How-\never, this behaviour is orthogonal to the goal of the paper,\nwhere we aim to characterize the Pareto-optimal through-\nput for small models. Further versions of vLLM introduce\nCUDA Graphs, launching multiple GPU operations through\na single CPU operation, therefore reducing the launching\noverheads [21]. Although we could have used a serving sys-\ntem written in optimized C++ with less overheads, such as\nFasterTransformer [17], we would sacrifice PagedAttention.\nIn this regard, we leave as future work analyzing at kernel\nlevel the performance of the system to investigate further in\nthis limitation.\n2 1\n 20 21 22 23 24\nThroughput (req/s)\n0\n20\n40\n60\n80\n100Average GPU Utilization (%)\n512\n256\n128\n1 2 4 8 16\n32\n64\nOPT-125M\nOPT-1.3B\nOPT-2.7B\nOPT-6.7B\nFigure 8. Average GPU utilization and throughput for each\nmodel size and batch size.\nMoreover, our baseline device is a 40GB NVIDIA A100\nGPU, which may not be affordable for the average user. While\nthis device offers enough memory to reach a throughput limit\nfor small models, cheaper devices might not. Given the op-\ntimal amount of memory that we need to allocate to reach\nthe throughput frontier, we must consider a cost trade-off\nbetween serving SLMs on smaller devices or replicate the\nmodel on larger devices. It is also worth noting that in our\nexperiments we set the input length of each request to be\n512 tokens, and we limit the output length to 256. Although\nthis restricted configuration allows us to characterize the\nPareto optimal throughput, in more realistic scenarios re-\nquests might vary in input and output length. We leave as\na future analysis the benchmark of SLM serving with het-\nerogeneous devices, and with heterogeneous requests. Also,\nthe study of more suitable alternatives for model replication\nsuch as Multi-Process Service (MPS) or Multi-Instance GPU\n(MIG), providing multiple users with separate GPU resources\nfor optimal GPU utilization.\n7 Conclusion\nThis paper characterizes the serving performance of SLMs,\nhighlighting the implications of memory allocation on infer-\nence throughput. Our analysis shows that for small models\na single high-end accelerator has enough memory to reach a\nPareto-optimal throughput frontier given a large batch of re-\nquests. Beyond that point, allocating more memory results in\nminimal or no improvements. In light of our results, we pave\nthe way for new optimizations in model serving, presenting\nan initial set of findings that show how model replication\non a single device improves overall inference performance.\nFurther analysis should consider a more realistic serving sce-\nnario with heterogeneous requests and devices, and explore\nmodel replication with more suitable techniques.\nAcknowledgments\nThis work has been partially financed by grant agreement EU-\nHORIZON GA.101095717 and by the EU-HORIZON MSCA\nprogramme under grant agreement EU-HORIZON MSCA\nGA.101086248. Also, it has been partially financed by Gener-\nalitat de Catalunya (AGAUR) under grant agreement 2021-\nSGR-00478, and by the Spanish Ministry of Science (MICINN),\nthe Research State Agency (AEI) and European Regional\nDevelopment Funds (ERDF/FEDER) under grant agreement\nPID2021-126248OB-I00, MCIN/AEI/ 10.13039/ 501100011033/\nFEDER, UE.\nReferences\n[1] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan,\nCheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia\nZhang, Jeff Rasley, et al. 2022. Deepspeed-inference: enabling efficient\ninference of transformer models at unprecedented scale. In SC22:\nInternational Conference for High Performance Computing, Networking,\nStorage and Analysis . IEEE, 1‚Äì15.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural\nmachine translation by jointly learning to align and translate. arXiv\npreprint arXiv:1409.0473 (2014).\n[3] Guangji Bai, Zheng Chai, Chen Ling, Shiyu Wang, Jiaying Lu, Nan\nZhang, Tingwei Shi, Ziyang Yu, Mengdan Zhu, Yifei Zhang, et al. 2024.\nBeyond Efficiency: A Systematic Survey of Resource-Efficient Large\nLanguage Models. arXiv preprint arXiv:2401.00625 (2024).\n[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma,\nGaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung,\nCharles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling lan-\nguage modeling with pathways. Journal of Machine Learning Research\n24, 240 (2023), 1‚Äì113.\n[5] Ronen Eldan and Yuanzhi Li. 2023. TinyStories: How Small Can\nLanguage Models Be and Still Speak Coherent English? arXiv preprint\narXiv:2305.07759 (2023).\nPol G. Recasens, et al.\n[6] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2022.\nGptq: Accurate post-training quantization for generative pre-trained\ntransformers. arXiv preprint arXiv:2210.17323 (2022).\n[7] In Gim, Guojun Chen, Seung-seob Lee, Nikhil Sarda, Anurag Khan-\ndelwal, and Lin Zhong. 2023. Prompt cache: Modular attention reuse\nfor low-latency inference. arXiv preprint arXiv:2311.04934 (2023).\n[8] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C√©sar Teodoro Mendes,\nAllie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann,\nGustavo de Rosa, Olli Saarikivi, et al . 2023. Textbooks Are All You\nNeed. arXiv preprint arXiv:2306.11644 (2023).\n[9] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena\nBuchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas,\nLisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al . 2022.\nTraining compute-optimal large language models. arXiv preprint\narXiv:2203.15556 (2022).\n[10] HuggingFace. 2023. Text Generation Inference. https://huggingface.\nco/docs/text-generation-inference/index.\n[11] Yunho Jin, Chun-Feng Wu, David Brooks, and Gu-Yeon Wei. 2023.ùëÜ3:\nIncreasing GPU Utilization during Generative Inference for Higher\nThroughput. arXiv preprint arXiv:2306.06000 (2023).\n[12] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xi-\nuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer.\n2023. SqueezeLLM: Dense-and-Sparse Quantization. arXiv preprint\narXiv:2306.07629 (2023).\n[13] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo\nKang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt\nKeutzer, Michael W Mahoney, et al . 2023. Full stack optimization\nof transformer inference: a survey. arXiv preprint arXiv:2302.14017\n(2023).\n[14] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin\nZheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica.\n2023. Efficient memory management for large language model serv-\ning with pagedattention. In Proceedings of the 29th Symposium on\nOperating Systems Principles . 611‚Äì626.\n[15] Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng,\nXin Jin, Yanping Huang, Zhifeng Chen, Hao Zhang, Joseph E Gonzalez,\net al. 2023. AlpaServe: Statistical Multiplexing with Model Parallelism\nfor Deep Learning Serving. arXiv preprint arXiv:2302.11665 (2023).\n[16] Microsoft. 2023. DeepSpeed-FastGen. https://github.com/microsoft/\nDeepSpeed/tree/master/blogs/deepspeed-fastgen.\n[17] NVIDIA. 2023. FasterTransformer. https://github.com/NVIDIA/\nFasterTransformer.\n[18] NVIDIA. 2023. GPU Performance Background User‚Äôs Guide.\nhttps://docs.nvidia.com/deeplearning/performance/dl-performance-\ngpu-background/index.html#understand-perf .\n[19] NVIDIA. 2024. DCGM User guide: Feature overview.\nhttps://docs.nvidia.com/datacenter/dcgm/latest/user-guide/feature-\noverview.html.\n[20] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin,\nJames Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and\nJeff Dean. 2023. Efficiently scaling transformer inference. Proceedings\nof Machine Learning and Systems 5 (2023).\n[21] PyTorch. 2024. Accelerating PyTorch with CUDA Graphs. https:\n//pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/ .\n[22] Siddharth Samsi, Dan Zhao, Joseph McDonald, Baolin Li, Adam\nMichaleas, Michael Jones, William Bergeron, Jeremy Kepner, Devesh\nTiwari, and Vijay Gadepally. 2023. From Words to Watts: Benchmark-\ning the Energy Costs of Large Language Model Inference. In2023 IEEE\nHigh Performance Extreme Computing Conference (HPEC) . IEEE, 1‚Äì9.\n[23] ShareGPT. 2023. ShareGPT. https://sharegpt.com/.\n[24] Noam Shazeer. 2019. Fast transformer decoding: One write-head is all\nyou need. arXiv preprint arXiv:1911.02150 (2019).\n[25] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max\nRyabinin, Beidi Chen, Percy Liang, Christopher R√©, Ion Stoica, and\nCe Zhang. 2023. Flexgen: High-throughput generative inference of\nlarge language models with a single gpu. In International Conference\non Machine Learning . PMLR, 31094‚Äì31116.\n[26] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley,\nJared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training\nmulti-billion parameter language models using model parallelism.\narXiv preprint arXiv:1909.08053 (2019).\n[27] MosaicML NLP Team. 2023. Introducing MPT-7B: A New Standard for\nOpen-Source, Commercially Usable LLMs . www.mosaicml.com/blog/\nmpt-7b Accessed: 2023-05-05.\n[28] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-\nAnne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric\nHambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971 (2023).\n[29] vLLM. 2024. Project update. https://docs.google.com/presentation/\nd/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/\nmobilepresent?slide=id.g2b46085d608_1_0.\n[30] Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Param-\neter Autoregressive Language Model. https://github.com/kingoflolz/\nmesh-transformer-jax .\n[31] Hanrui Wang, Zhekai Zhang, and Song Han. 2021. Spatten: Efficient\nsparse attention architecture with cascade token and head pruning.\nIn 2021 IEEE International Symposium on High-Performance Computer\nArchitecture (HPCA). IEEE, 97‚Äì110.\n[32] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,\nClement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R√©mi\nLouf, Morgan Funtowicz, et al. 2020. Transformers: State-of-the-art\nnatural language processing. In Proceedings of the 2020 conference on\nempirical methods in natural language processing: system demonstra-\ntions. 38‚Äì45.\n[33] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and\nSong Han. 2023. Smoothquant: Accurate and efficient post-training\nquantization for large language models. In International Conference\non Machine Learning . PMLR, 38087‚Äì38099.\n[34] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and\nByung-Gon Chun. 2022. Orca: A distributed serving system for\nTransformer-Based generative models. In 16th USENIX Symposium on\nOperating Systems Design and Implementation (OSDI 22) . 521‚Äì538.\n[35] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya\nChen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Vic-\ntoria Lin, et al . 2022. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068 (2022)."
}