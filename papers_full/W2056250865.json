{
  "title": "Factored language models and generalized parallel backoff",
  "url": "https://openalex.org/W2056250865",
  "year": 2003,
  "authors": [
    {
      "id": "https://openalex.org/A2116964878",
      "name": "Jeff A. Bilmes",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2068133273",
      "name": "Katrin Kirchhoff",
      "affiliations": [
        "University of Washington"
      ]
    },
    {
      "id": "https://openalex.org/A2116964878",
      "name": "Jeff A. Bilmes",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2068133273",
      "name": "Katrin Kirchhoff",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1895315011",
    "https://openalex.org/W2075201173",
    "https://openalex.org/W2100506586",
    "https://openalex.org/W1508165687",
    "https://openalex.org/W1482911759",
    "https://openalex.org/W4211150175",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W2096044236",
    "https://openalex.org/W1727996342",
    "https://openalex.org/W3148140880",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W2097927681"
  ],
  "abstract": "We introduce factored language models (FLMs) and generalized parallel backoff (GPB). An FLM represents words as bundles of features (e.g., morphological classes, stems, data-driven clusters, etc.), and induces a probability model covering sequences of bundles rather than just words. GPB extends standard backoff to general conditional probability tables where variables might be heterogeneous types, where no obvious natural (temporal) backoff order exists, and where multiple dynamic backoff strategies are allowed. These methodologies were implemented during the JHU 2002 workshop as extensions to the SRI language modeling toolkit. This paper provides initial perplexity results on both CallHome Arabic and on Penn Treebank Wall Street Journal articles. Significantly, FLMs with GPB can produce bigrams with significantly lower perplexity, sometimes lower than highly-optimized baseline trigrams. In a multi-pass speech recognition context, where bigrams are used to create first-pass bigram lattices or N-best lists, these results are highly relevant.",
  "full_text": null,
  "topic": "Bigram",
  "concepts": [
    {
      "name": "Bigram",
      "score": 0.9856097102165222
    },
    {
      "name": "Perplexity",
      "score": 0.9851781129837036
    },
    {
      "name": "Trigram",
      "score": 0.8655694723129272
    },
    {
      "name": "Treebank",
      "score": 0.8231512308120728
    },
    {
      "name": "Computer science",
      "score": 0.7520993947982788
    },
    {
      "name": "Language model",
      "score": 0.7298504710197449
    },
    {
      "name": "Natural language processing",
      "score": 0.5942944288253784
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5680872797966003
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49262529611587524
    },
    {
      "name": "Speech recognition",
      "score": 0.3937416970729828
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Parsing",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    }
  ]
}