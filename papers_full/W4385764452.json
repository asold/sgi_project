{
  "title": "On Efficient Transformer-Based Image Pre-training for Low-Level Vision",
  "url": "https://openalex.org/W4385764452",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2103550463",
      "name": "Wenbo Li",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2067267901",
      "name": "Xin Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2968143215",
      "name": "Shengju Qian",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2130275747",
      "name": "Jiangbo Lu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6864014924",
    "https://openalex.org/W6863994431",
    "https://openalex.org/W2741137940",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W1920328734",
    "https://openalex.org/W2056370875",
    "https://openalex.org/W2954930822",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2102605133",
    "https://openalex.org/W2112552549",
    "https://openalex.org/W2780930362",
    "https://openalex.org/W1930824406",
    "https://openalex.org/W2147800946",
    "https://openalex.org/W6803771590",
    "https://openalex.org/W2466666260",
    "https://openalex.org/W2884068670",
    "https://openalex.org/W3196057788",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4221163966",
    "https://openalex.org/W2209874411",
    "https://openalex.org/W2799269579",
    "https://openalex.org/W3174531399",
    "https://openalex.org/W2912435603",
    "https://openalex.org/W6759363029",
    "https://openalex.org/W2734663976",
    "https://openalex.org/W2971719842",
    "https://openalex.org/W2891158090",
    "https://openalex.org/W2930755307",
    "https://openalex.org/W3023003829",
    "https://openalex.org/W6863071542",
    "https://openalex.org/W2963878020",
    "https://openalex.org/W2913360047",
    "https://openalex.org/W3212228063",
    "https://openalex.org/W6791858558",
    "https://openalex.org/W2508457857",
    "https://openalex.org/W3081639259",
    "https://openalex.org/W2963703197",
    "https://openalex.org/W4386083034",
    "https://openalex.org/W3035250394",
    "https://openalex.org/W3127775241",
    "https://openalex.org/W4287682995",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2962843773",
    "https://openalex.org/W4312812783",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W4226440503",
    "https://openalex.org/W4287022992",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3175544090",
    "https://openalex.org/W2866634454",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W4306979388",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W3104725225",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W2740982616",
    "https://openalex.org/W4239072543",
    "https://openalex.org/W2509784253",
    "https://openalex.org/W4225672218"
  ],
  "abstract": "Pre-training has marked numerous state of the arts in high-level computer vision, while few attempts have ever been made to investigate how pre-training acts in image processing systems. In this paper, we tailor transformer-based pre-training regimes that boost various low-level tasks. To comprehensively diagnose the influence of pre-training, we design a whole set of principled evaluation tools that uncover its effects on internal representations. The observations demonstrate that pre-training plays strikingly different roles in low-level tasks. For example, pre-training introduces more local information to intermediate layers in super-resolution (SR), yielding significant performance gains, while pre-training hardly affects internal feature representations in denoising, resulting in limited gains. Further, we explore different methods of pre-training, revealing that multi-related-task pre-training is more effective and data-efficient than other alternatives. Finally, we extend our study to varying data scales and model sizes, as well as comparisons between transformers and CNNs. Based on the study, we successfully develop state-of-the-art models for multiple low-level tasks.",
  "full_text": "On EfÔ¨Åcient Transformer-Based Image Pre-training for Low-Level Vision\nWenbo Li1\u0003 , Xin Lu2* , Shengju Qian1 and Jiangbo Lu3\n1The Chinese University of Hong Kong\n2Deeproute.ai\n3SmartMore Corporation\nfwenboli,sjqiang@cse.cuhk.edu.hk, luxin941027@gmail.com, jiangbo.lu@gmail.com\nAbstract\nPre-training has marked numerous state of the arts\nin high-level computer vision, while few attempts\nhave ever been made to investigate how pre-training\nacts in image processing systems. In this paper,\nwe tailor transformer-based pre-training regimes\nthat boost various low-level tasks. To compre-\nhensively diagnose the inÔ¨Çuence of pre-training,\nwe design a whole set of principled evaluation\ntools that uncover its effects on internal represen-\ntations. The observations demonstrate that pre-\ntraining plays strikingly different roles in low-level\ntasks. For example, pre-training introduces more\nlocal information to intermediate layers in super-\nresolution (SR), yielding signiÔ¨Åcant performance\ngains, while pre-training hardly affects internal fea-\nture representations in denoising, resulting in lim-\nited gains. Further, we explore different methods of\npre-training, revealing that multi-related-task pre-\ntraining is more effective and data-efÔ¨Åcient than\nother alternatives. Finally, we extend our study to\nvarying data scales and model sizes, as well as com-\nparisons between transformers and CNNs. Based\non the study, we successfully develop state-of-the-\nart models for multiple low-level tasks.\n1 Introduction\nImage pre-training has received great attention in computer\nvision, especially prevalent in object detection and segmenta-\ntion [Girshick et al., 2014; Girshick, 2015; Chenet al., 2017].\nWhen task-speciÔ¨Åc data is limited, pre-training helps mod-\nels see large-scale data, thus vastly enhancing their capabil-\nities. In the Ô¨Åeld of high-level vision, previous work [Ko-\nrnblith et al., 2019; Sun et al., 2017; Mahajan et al., 2018;\nKolesnikov et al., 2020] has shown that ConvNets pre-trained\non ImageNet [Deng et al., 2009 ] classiÔ¨Åcation yield signiÔ¨Å-\ncant improvements on a wide spectrum of downstream tasks.\nAs for image processing tasks, e.g., super-resolution (SR)\nand deraining, the widely used datasets typically contain only\na few thousand images, pointing out the potential of pre-\ntraining. However, its crucial role in low-level vision is com-\n\u0003Equal contribution\nmonly omitted. To the best of our knowledge, the sole pio-\nneer exploring this point is IPT [Chen et al., 2021 ]. Hence,\nthere still lacks principled analysis on understanding how pre-\ntraining acts and how to perform effective pre-training.\nPrevious image processing systems majorly leverage con-\nvolutional neural networks (CNNs) [LeCun et al., 1989 ].\nMore recently, transformer architectures [Dosovitskiy et al.,\n2020; Liu et al., 2021; Wang et al., 2021a ], initially pro-\nposed in NLP [Vaswani et al., 2017], have achieved promis-\ning results in vision tasks, demonstrating the potential of us-\ning transformers as a primary backbone for vision applica-\ntions. Moreover, the stronger modeling capability of trans-\nformers allows for large-scale and sophisticated pre-training,\nwhich has shown great success in both NLP and computer\nvision [Radford et al., 2018; Radford et al., 2019; Brown et\nal., 2020; Devlinet al., 2018; Heet al., 2021; Liuet al., 2022;\nZamir et al., 2022; Chenet al., 2022]. However, it remains in-\nfeasible to directly exploit structure designs and data utiliza-\ntion on the full-attention transformers for low-level vision.\nFor example, due to the massive amount of parameters (e.g.,\n116M for IPT [Chen et al., 2021 ]) and huge computational\ncost, it is prohibitively hard to explore various pre-training\ndesign choices based on IPT and further apply them in prac-\ntice. Instead of following the full-attention pipeline, we ex-\nplore the other window-based variants [Liang et al., 2021;\nWang et al., 2021b ], which are more computationally ef-\nÔ¨Åcient while leading to impressive performance. Along\nthis line, we develop an encoder-decoder-based transformer\n(EDT) that is powerful yet efÔ¨Åcient in data exploitation and\ncomputation. We mainly adopt EDT as a representative for\nefÔ¨Åcient computation, since our observations generalize well\nto other frameworks, as shown in Sec. 3.4.\nIn this paper, we systematically explore and evaluate how\nimage pre-training performs in window-based transformers.\nUsing centered kernel alignment [Kornblith et al., 2019;\nCortes et al., 2012 ] as a network ‚Äúdiagnosing‚Äù measure, we\nhave designed a set of pre-training strategies, and thoroughly\ntested them with different image processing tasks. As a re-\nsult, we uncover their respective effects on internal network\nrepresentations, and draw useful guidelines for applying pre-\ntraining to low-level vision. The key Ô¨Åndings and contribu-\ntions of this study can be summarized as follows,\n‚Ä¢ Internal representations of transformers. We Ô¨Ånd\nstriking differences in low-level tasks, e.g., SR and de-\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n1089\nCB‚Üì CB‚Üì\nTS\nCB CB\nHigh-Res.\nCB CB\nCB‚Üë CB‚Üë\nUp\n‚ãØ\nDeraining\nDenoising\n‚ãÆ\nSR\n‚ãÆ\nLow-Res. \nùêª2√óùëä2 ùêª4√óùëä4 ùêª4√óùëä4 ùêª2√óùëä2\nùêªùë†√óùëäùë† ùêªùë†√óùëäùë† ùêªùë†√óùëäùë† ùêªùë†√óùëäùë†\n‚ãÆ\n‚ãÆ\n‚®Å\n‚®Å\nBilinear\nùêªùë†√óùëäùë†\nùêª√óùëä ùêª√óùëäCBConvolution BlockTSTransformer StageUpUpsamplerBilinearBilinear InterpolationInput Output\nFigure 1: The proposed encoder-decoder-based transformer (EDT). It processes high-resolution (e.g., in denoising) and low-resolution (e.g.,\nin SR, sis the scale) inputs using different paths, modeling long-range interactions at a low resolution, for efÔ¨Åcient computation.\nraining models show clear stages, containing more local\ninformation in early layers while more global informa-\ntion in higher layers. The denoising model presents a\nrelatively uniform structure Ô¨Ålled with local information.\n‚Ä¢ Effects of pre-training . We Ô¨Ånd that pre-training im-\nproves the model performance by introducing different\ndegrees of local information, treated as a kind of induc-\ntive bias, to the intermediate layers.\n‚Ä¢ Pre-training guidelines . Examining different pre-\ntraining strategies, we suggest a favorablemulti-related-\ntask setup that brings more improvements and could be\napplied to multiple downstream tasks. Also, we Ô¨Ånd this\nperforming strategy is more data-efÔ¨Åcient than purely in-\ncreasing the data scale. Besides, a larger model capacity\nusually gets more out of pre-training.\n‚Ä¢ Transformers v.s. CNNs . We observe that both\ntransformers and CNNs beneÔ¨Åt from pre-training, while\ntransformers obtain greater improvements.\n‚Ä¢ SOTA models. Based on the comprehensive study of\npre-training, we provide a series of pre-trained models\nwith state-of-the-art performance for multiple tasks, in-\ncluding super-resolution, denoising and deraining.\n2 Encoder-Decoder-Based Transformer\nSeveral transformers [Chen et al., 2021; Liang et al., 2021;\nWang et al. , 2021b ] are tailored to low-level tasks, among\nwhich window-based architectures [Liang et al., 2021; Wang\net al. , 2021b ] show competitive performance under con-\nstrained parameters and computational complexity. Built\nupon the existing work, we make several modiÔ¨Åcations\nand present an efÔ¨Åcient encoder-decoder-based transformer\n(EDT) in Fig. 1. It achieves state-of-the-art results on mul-\ntiple low-level tasks (see Sec. 4), especially for those with\nheavy degradation. For example, EDT yields 0.49dB im-\nprovement in \u00024 SR on the Urban100 [Huang et al., 2015]\nbenchmark compared to IPT, while our \u00024 SR model size\n(11.6M) is only 10.0% of IPT (115.6M) and only requires\n200K images ( 15.6% of IPT) for pre-training. Also, our\ndenoising model obtains superior performance in level-50\nGaussian denoising, with 38 GFLOPs for 192 \u0002192 inputs,\nfar less than SwinIR [Liang et al., 2021] (451 GFLOPs), ac-\ncounting for only 8.4%. And the inference speed of EDT\n(51.9ms) is much faster than SwinIR (271.9ms). It should be\npointed out that designing a novel framework is not our main\npurpose. Noticing similar pre-training effects on transformers\nin Sec. 3.4, we adopt EDT for fast pre-training in this paper.\n2.1 Overall Architecture\nAs shown in Fig. 1, our EDT is composed of a lightweight\nconvolutional encoder and decoder as well as a transformer-\nbased body, for modeling long-range interactions.\nTo improve the encoding efÔ¨Åciency, images are Ô¨Årst down-\nsampled to 1=4 size with strided convolutions for tasks with\nhigh-resolution inputs (e.g., denoising or deraining), while\nbeing processed under the original size for those with low-\nresolution inputs (e.g., SR). The stack of early convolutions\nis also proven useful for stabling the optimization [Xiao et\nal., 2021]. Then, there follow multiple stages of transformer\nblocks, achieving a large receptive Ô¨Åeld at a low computa-\ntional cost. It is noted that we improve the structure of trans-\nformer blocks through a series of ablations and provide more\ndetails in the supplementary Ô¨Åle. During the decoding phase,\nwe upsample the feature back to the input size using trans-\nposed convolutions for denoising or deraining while main-\ntaining the size for SR. Besides, skip connections are intro-\nduced to enable fast convergence during training. In particu-\nlar, there is an additional convolutional upsampler before the\noutput for super-resolution.\n2.2 Architecture Variants\nWe develop four variants of EDT with different model sizes,\nrendering our framework easily applied in various scenarios.\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n1090\nModels EDT-T EDT-S EDT-B EDT-L\n#Channels 60 120 180 240\n#Stages 4 5 6 12\n#Heads 6 6 6 8\n#Param.\n(\u0002106, M) 0.9 4.2 11.5 40.2\nFLOPs (\n\u0002109, G) 2.8 12.4 37.6 136.4\nTable 1: ConÔ¨Ågurations of four variants of EDT. The parameter\nnumbers and FLOPs are counted in denoising at 192 \u0002192 size.\nAs shown in Table 1, apart from the base model (EDT-B),\nwe also provide EDT-T (Tiny), EDT-S (Small) and EDT-L\n(Large). The main differences lie in the channel number,\nstage number and head number in the transformer body. We\nuniformly set the block number in each transformer stage to\n6, the expansion ratio of the feed-forward network (FFN) to\n2 and the window size to (6;24).\n3 Study of Image Pre-training\n3.1 Pre-training on ImageNet\nFollowing [Chen et al., 2021], we adopt the ImageNet [Deng\net al., 2009 ] dataset in the pre-training stage. Unless spec-\niÔ¨Åed otherwise, we only use 200K images for fast pre-\ntraining. We choose three representative low-level tasks in-\ncluding super-resolution (SR), denoising and deraining. Re-\nferring to [Chen et al., 2021; Agustsson and Timofte, 2017;\nGu et al., 2017 ], we simulate the degradation procedure to\nsynthesize low quality images. In terms of SR, we utilize\nbicubic interpolation to obtain low-resolution images. As for\ndenoising and deraining, Gaussian noises (on RGB space)\nand rain streaks are directly added to the clean images. In this\nwork, we explore \u00022/\u00023/\u00024 settings in SR, 15/25/50 noise\nlevels in denoising and light/heavy rain streaks in deraining.\nWe explore three pre-training strategies: on a single task,\non unrelated tasks and on related tasks. (1) Single-task pre-\ntraining refers to training a single model on a speciÔ¨Åc task\n(e.g., \u00022 SR). (2) The second is to train a single model on\nmultiple yet unrelated tasks (e.g.,\u00022 SR, level-15 denoising),\nwhile (3) the last contains highly related tasks (e.g., \u00022, \u00023\nSR). Following[Chen et al., 2021], we adopt a multi-encoder,\nmulti-decoder, shared-body architecture for the latter two se-\ntups. The Ô¨Åne-tuning is performed on a single task, where the\nmodel is initialized with the pre-trained task-speciÔ¨Åc encoder\nand decoder as well as the shared transformer body. Training\ndetails are provided in the supplementary Ô¨Åle.\n3.2 Centered Kernel Alignment\nWe introduce centered kernel alignment (CKA)[Kornblith et\nal., 2019; Cortes et al., 2012; Raghu et al., 2021 ] to study\nrepresentation similarity of network hidden layers, support-\ning quantitative comparisons within and across networks. In\ndetail, given m data points, we calculate the activations of\ntwo layers X 2Rm\u0002p1 and Y 2Rm\u0002p2 , having p1 and p2\nneurons respectively. We use the Gram matrices K = XX>\nand L = YY>to compute CKA:\nCKA(K;L) = HSIC(K;L)\np\nHSIC(K;K)HSIC(L;L)\n; (1)\nwhere HSIC is the Hilbert-Schmidt independence crite-\nrion [Gretton et al., 2007]. Given the centering matrix H =\nIn \u00001\nn11>, K\n0\n= HKH and L\n0\n= HLH are centered\nGram matrices, then we have HSIC(K;L) = vec( K\n0\n) \u0001\nvec(L\n0\n)=(m\u00001)2. Thanks to the properties of CKA, invari-\nant to orthogonal transformation and isotropic scaling, we are\nable to conduct a meaningful analysis of neural network rep-\nresentations. However, naive computation of CKA requires\nmaintaining the activations across the entire dataset in mem-\nory, causing much memory consumption. To avoid this, we\nuse minibatch estimators of CKA[Nguyen et al., 2020], with\na minibatch of 300 by iterating over the test dataset 10 times.\n3.3 Representation Structure of EDT\nWe begin our investigation by studying the internal repre-\nsentation structure of our models. How are representations\npropagated within models in different low-level tasks? To an-\nswer this intriguing question, we compute CKA similarities\nbetween every pair of layers within a model. Apart from the\nconvolutional head and tail, we include outputs of attention\nand FFN after residual connections in the transformer body.\nWe observe a block-diagonal structure in the CKA similar-\nity maps in Fig. 2. As for the SR and deraining models in\nFig. 2 (a)-(b), we Ô¨Ånd there are roughly four groups, among\nwhich a range of transformer layers are of high similarity.\nThe Ô¨Årst and last group structures (from left to right) corre-\nspond to the model head and tail, while the second and third\ngroup structures account for the transformer body. As for the\ndenoising task (Fig. 2 (c)), there are only three obvious group\nstructures, where the second one (transformer body) is domi-\nnated. Finally, from the cross-model comparison in Fig. 2 (d)\nand (h), we Ô¨Ånd higher similarity scores between denoising\nbody layers and the second group SR layers, while showing\nsigniÔ¨Åcant differences compared to the third group SR layers.\nWe also explore the impact of single-task pre-training on\nthe internal representations. As for SR and deraining in Fig. 2\n(e)-(f), the representations of the model head and tail remain\nbasically unchanged. Meanwhile, we observe obvious repre-\nsentation changes in the transition regions between the sec-\nond and third groups. In terms of denoising in Fig. 2 (g), the\ninternal representations do not change too much, consistent\nwith the Ô¨Ånding in Table 4 that denoising tasks obtain fewer\nimprovements, compared to SR and deraining tasks.\nKey Findings: (1) SR and deraining models show clear\nstages in the internal representations of the transformer body,\nwhile the denoising model presents a relatively uniform struc-\nture; (2) the denoising model layers show more similarity to\nthe lower layers of SR models, containing more local infor-\nmation, as veriÔ¨Åed in Sec. 3.4; (3) single-task pre-training\nmainly affects the intermediate layers of SR and deraining\nmodels but has limited impact on the denoising model.\n3.4 Single- and Multi-Task Pre-training\nIn the previous section, we observe that the transformer body\nof SR models is clearly composed of two group structures\nand pre-training mainly changes the representations of higher\nlayers. What is the difference between these two partitions?\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n1091\n(a) (b) (c) (d)\n(e) (g) (h)\n(f)\nSimilarity\n1st 2nd 3rd 4th 1st 2nd 3rd 4th 1st 2nd 3rd\nFigure 2: Sub-Ô¨Ågures (a)-(c) show CKA similarities between all pairs of layers in \u00022 SR, light streak deraining and level-15 denoising\nEDT-B models with single-task pre-training, and the corresponding similarities between with and without pre-training are shown in (e)-(g).\nSub-Ô¨Ågure (d) shows the cross-model comparison between SR and denoising models and (h) shows the ratios of layer similarity larger than\n0.6 for input images, where ‚Äús‚Äù means the similarity between the current layer in SR and any layer in denoising.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nSet5 Set14 Urban100 Manga109\nPSNR(dB) Improvement\n single\n multi-unrelated\n multi-related\nFigure 3: PSNR improvements of single-task, multi-unrelated-task\nand multi-related-task pre-training for EDT-B in \u00022 SR.\nHow does the pre-training, especially multi-task pre-training,\naffect the behaviors of models?\nWe conjecture that one possible reason causing the parti-\ntion lies with the difference of ability to incorporate local or\nglobal information between different layers. We start by an-\nalyzing self-attention layers for their mechanism of dynam-\nically aggregating information from other spatial locations,\nwhich is quite different from the Ô¨Åxed receptive Ô¨Åeld of the\nFFN layer. To represent the range of attentive Ô¨Åelds, we aver-\nage pixel distances between the queries and keys using atten-\ntion weights for each head over 170,000 data points, where\na larger distance usually refers to using more global infor-\nmation. We do not record attention distances of shifted local\nwindows, because the shift operation narrows down boundary\nwindows and hence can not reÔ¨Çect real distances.\nAs shown in Fig. 4 (e)-(h), for the second group structure\n(counted from the head, same as Sec. 3.3), the standard devi-\nation of attention distances (shown as the blue area) is large\nand the mean value is small, indicating the attention mod-\nules in this group structure area have a mix of local heads\n(relatively small distances) and global heads (relatively large\ndistances). On the contrary, the third group structure only\ncontains global heads, showing more global information are\naggregated in this stage.\nCompared to single-task pre-training (\u00022 SR, Fig. 4 (b)\nand (f)), multi-unrelated-task setup (\u00022, \u00023 SR, g15 denois-\ning, in Fig. 4 (c) and (g)) converts more global representa-\ntions (in red box) of the third group to local ones, increasing\nthe scope of the second group. In consequence, as shown in\nFig. 3, we observe obvious PSNR improvements on all bench-\nmarks. When replacing the g15 denoising with highly related\n\u00024 SR (\u00022, \u00023, \u00024 SR, in Fig. 4 (d) and (h)), we observe\nmore changes in global representations, along with further\nimprovements in Fig. 3. The inferiority of multi-unrelated-\ntask setup is mainly due to the representation mismatch of un-\nrelated tasks, as shown in Sec. 3.3. We also provide detailed\nquantitative comparisons for all tasks and different batch size\nsettings in the supplementary material.\nKey Findings: (1) the representations of SR models contain\nmore local information in early layers while more global in-\nformation in higher layers; (2) all three pre-training methods\ncan greatly improve the performance by introducing different\ndegrees of local information, treated as a kind of inductive\nbias, to the intermediate layers of the model, among which\nmulti-related-task pre-training performs best.\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n1092\n(a)\n (b)\n (d)\n(c)\n(e) (f) (h)(g)\nSimilarity\nFigure 4: Sub-Ô¨Ågures (a)-(d) show CKA similarities of \u00022 SR models, without pre-training as well as with pre-training ona single task (\u00022),\nunrelated tasks (\u00022, \u00023 SR, g15 denoising) and highly related tasks (\u00022, \u00023, \u00024 SR). Sub-Ô¨Ågures (e)-(h) show the corresponding attention\nhead mean distances of transformer blocks. We do not plot shifted local windows in (e)-(h) so that the last blue dotted line (‚Äú---‚Äù) has no\nmatching point. The red boxes indicate the same attention modules.\nAverage: 5.33 Average: 5.10\nFigure 5: Attention head mean distances of transformer blocks in\nSwinIR with and without pre-training.\nTo validate whether the Ô¨Ånding that pre-training brings\nmore local information to the model also Ô¨Åt other window-\nbased frameworks, we show the attention head distances of\nSwinIR [Liang et al., 2021 ] in Fig. 5. Without pre-training,\nthe Ô¨Årst few blocks (1-15) tend to be local while the last ones\n(16-18) are more global. And pre-training brings more local\nrepresentations, matching our observation before.\n3.5 Effect of Data Scale on Pre-training\nIn this section, we investigate how pre-training data scale af-\nfects the super-resolution performance. As shown in Table 2,\nModel Data Set5 Set14\nUrban100 Manga109\nEDT-B 0 38.45 34.57\n33.80 39.93\nEDT-By 50K 38.53 34.66\n33.86 40.14\nEDT-By 100K 38.55 34.68\n33.90 40.18\nEDT-By 200K 38.56 34.71\n33.95 40.25\nEDT-By 400K 38.61 34.75\n34.05 40.37\nEDT-B? 200K 38.63 34.80\n34.27 40.37\nTable 2: PSNR(dB) results of different pre-training data scales in\u00022\nSR. ‚ÄúEDT-By‚Äù refers to the base model with single-task (\u00022 SR)\npre-training and ‚ÄúEDT-B?‚Äù represents the base model with multi-\nrelated-task (\u00022, \u00023, \u00024 SR) pre-training.\nwith regard to the EDT-B model, we obviously observe incre-\nmental PSNR improvements on multiple SR benchmarks by\nincreasing the data scale from 50K to 400K during single-task\npre-training. It is noted that we double the pre-training itera-\ntions for the data scale of 400K so that the data can be fully\nfunctional. However, a longer pre-training period largely in-\ncreases the training burden.\nOn the contrary, as shown in Table 2, multi-related-task\npre-training (with much fewer training iterations) success-\nfully breaks through the limit. Our EDT-B model with multi-\nrelated-task pre-training on 200K images achieves new state\nof the arts on all benchmarks, though a smaller data scale is\nadopted, revealing that simply increasing the data scale may\nnot be the optimal option. Thus, we suggest multi-related-\ntask pre-training is more effective and data-efÔ¨Åcient.\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n1093\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nSet5 Set14 Urban100 Manga109\nPSNR(dB) Improvement\nDataset\n EDT-T\n EDT-S\n EDT-B\n EDT-L\nFigure 6: PSNR improvements of four variants of EDT models using\nsingle-task pre-training in \u00022 SR. ‚ÄúT‚Äù, ‚ÄúS‚Äù, ‚ÄúB‚Äù and ‚ÄúL‚Äù refer to\ntiny, small, base and large models. The improvement of EDT-T on\nUrban100 is 0.00dB, thus we do not plot the bar.\nFigure 7: CKA similarities between all pairs of layers in EDT-S,\nEDT-B and EDT-L models using single-task pre-training in\u00022 SR.\n3.6 Effect of Model Size on Pre-training\nWe conduct experiments to compare the performance of\nsingle-task pre-training for four model variants in the \u00022 SR\ntask. As shown in Fig. 6, we visualize PSNR improvements\nof models with pre-training over counterparts trained from\nscratch. It is observed that models with larger capacities gen-\nerally obtain more improvements. Especially, we Ô¨Ånd pre-\ntraining can still improve a lot upon already strong EDT-L\nmodels, showing the potential of pre-training. The quantita-\ntive results are provided in the supplementary Ô¨Åle.\nHere we visualize the CKA maps of the EDT-S, EDT-B\nand EDT-L models in Fig 7. As illustrated in Sec. 3.3, we\nalready know there are roughly four group structures in the\nCKA maps of SR models, among which the second and third\ngroup structures account for the transformer body. The pro-\nportion of the third part is positively correlated with the model\nsize. Especially, compared to the other two, the third group\nstructure of EDT-L account for the vast majority and show\nhigh similarities, which reÔ¨Çects the redundancy of the model.\n3.7 EDT v.s. ConvNets with Pre-training\nWe further explore the pre-training performance of EDT\nand CNNs-based models (RRDB [Wang et al., 2018 ] and\nRCAN [Zhang et al., 2018b ]). Fig. 8 demonstrates that our\nEDT-B obtains greater or comparable improvements from\npre-training, giving higher baselines with fewer parame-\nters. From the representation comparisons between EDT and\nCNNs-based models exhibited in the supplementary material,\nwe argue that the superiority of transformers may come from\nthe utilization of global information.\n33.27 \n33.33 33.34 \n33.44 \n33.80 \n33.95 \n33.00\n33.20\n33.40\n33.60\n33.80\n34.00\nW/o Pre-training W/ Pre-training\nUrban100\n39.61 \n39.86 \n39.79 \n40.05 \n39.93 \n40.25 \n39.40\n39.60\n39.80\n40.00\n40.20\n40.40\nW/o Pre-training W/ Pre-training\nManga109\nRRDB (16.7M) RCAN (15.4M) EDT-B (11.5M)\nPSNR(dB)\nPSNR(dB)\nFigure 8: Quantitative comparison between ConvNets (RRDB and\nRCAN) and our EDT-B without (‚ÄúW/o‚Äù) and with (‚ÄúW/‚Äù) single-\ntask pre-training in \u00022 SR.\n4 Experiments\nFollowing the pre-training guidelines, we conduct experi-\nments in super-resolution (SR), denoising and detraining.\nAs aforementioned, we observe that multi-related-task pre-\ntraining is highly effective and data-efÔ¨Åcient. Thus, we adopt\nthis pre-training strategy in all the tests. The involved pre-\ntraining tasks of SR include \u00022, \u00023 and \u00024, those of denois-\ning include g15, g25 and g50, and those of deraining include\nlight and heavy rain streaks. More experimental settings and\nvisual comparisons are given in the supplementary Ô¨Åle.\n4.1 Super-Resolution Results\nFor the super-resolution (SR) task, we test our models on two\nsettings, classical and lightweight SR, where the latter gen-\nerally refers to models with < 1M parameters. The results\nof \u00023 classical SR and lightweight SR are provided in the\nsupplementary material due to the space limit.\nWe compare our EDT with state-of-the-art CNNs-based\nmethods as well as transformer-based methods. As shown in\nTable 3, while the proposed EDT-B serves as a strong base-\nline, achieving nearly 0.1dB gains on multiple datasets over\nSwinIR [Liang et al., 2021 ], pre-training still brings signiÔ¨Å-\ncant improvements on \u00022 and \u00024 scales. For example, we\nobserve up to 0.46dB and 0.45dB improvements on high-\nresolution benchmark Urban100 and Manga109, manifesting\nthe effectiveness of our pre-training strategy.\n4.2 Denoising Results\nIn Table 4, we present our three models: (1) EDT-B without\npre-training; (2) EDT-B with pre-training; (3) EDT-B without\ndownsampling and pre-training.\nIt is worthwhile to note that, unlike SR models that ben-\neÔ¨Åt a lot from pre-training, denoising models only achieve\n0.02-0.11dB gains. One possible reason is that we use a large\ntraining dataset in denoising tasks, which already provides\nsufÔ¨Åcient data to make the capacity of our models into full\nplay. On the other hand, pre-training hardly affects the in-\nternal feature representation of models, discussed in Sec. 3.3.\nTherefore, we suggest that the Gaussian denoising task may\nnot need a large amount of training data.\nBesides, we Ô¨Ånd our framework is well performed on high\nnoise levels (e.g., \u001b = 50), while yielding slightly inferior\nperformance on low noise levels (e.g., \u001b = 15). This could\nbe caused by the downsampling operation in EDT. To ver-\nify this assumption, we train another EDT-B model without\ndownsampling. As shown in Table 4, it does obtain better\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n1094\nScale Method #Param. Set5 Set14 BSDS100 Urban100 Manga109\n(\u0002106) PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM\n\u00022\nRCAN [Zhang et al.\n, 2018b] 15.4 38.27 0.9614 34.12 0.9216 32.41 0.9027 33.34 0.9384 39.44 0.9786\nSAN [Dai et al.\n, 2019] 15.7 38.31 0.9620 34.07 0.9213 32.42 0.9028 33.10 0.9370 39.32 0.9792\nNLSA [Mei et al.\n, 2021] 31.9 38.34 0.9618 34.08 0.9231 32.43 0.9027 33.42 0.9394 39.59 0.9789\nIPTy[Chen et al.\n, 2021] 115.5 38.37 - 34.43 - 32.48 - 33.76 - - -\nSwinIR [Liang et al.\n, 2021] 11.8 38.42 0.9622 34.48 0.9252 32.50 0.9038 33.70 0.9418 39.81 0.9796\nSwinIRz[Liang et al.\n, 2021] 11.8 38.42 0.9623 34.46 0.9250 32.53 0.9041 33.81 0.9427 39.92 0.9797\nEDT-B(Ours) 11.5 38.45 0.9624 34.57 0.9258 32.52 0.9041 33.80 0.9425 39.93 0.9800\nEDT-By(Ours) 11.5 38.63 0.9632 34.80 0.9273 32.62 0.9052 34.27 0.9456 40.37 0.9811\n\u00024\nRCAN [Zhang et al.\n, 2018b] 15.6 32.63 0.9002 28.87 0.7889 27.77 0.7436 26.82 0.8087 31.22 0.9173\nSAN [Dai et al.\n, 2019] 15.9 32.64 0.9003 28.92 0.7888 27.78 0.7436 26.79 0.8068 31.18 0.9169\nNLSA [Mei et al.\n, 2021] 44.2 32.59 0.9000 28.87 0.7891 27.78 0.7444 26.96 0.8109 31.27 0.9184\nIPTy[Chen et al.\n, 2021] 115.6 32.64 - 29.01 - 27.82 - 27.26 - - -\nSwinIR [Liang et al.\n, 2021] 11.9 32.74 0.9020 29.06 0.7939 27.89 0.7479 27.37 0.8233 31.93 0.9246\nSwinIRz[Liang et al.\n, 2021] 11.9 32.92 0.9044 29.09 0.7950 27.92 0.7489 27.45 0.8254 32.03 0.9260\nEDT-B(Ours) 11.6 32.82 0.9031 29.09 0.7939 27.91 0.7483 27.46 0.8246 32.05 0.9254\nEDT-By(Ours) 11.6 33.06 0.9055 29.23 0.7971 27.99 0.7510 27.75 0.8317 32.39 0.9283\nTable 3: Quantitative comparison for classical SR on PSNR(dB)/SSIM on the Y channel from the YCbCr space. ‚Äúz‚Äù means the \u00024 model of\nSwinIR are pre-trained on the \u00022 setup and training patch size is 64 \u000264 (ours is 48 \u000248). ‚Äúy‚Äù indicates methods with a pre-training. Best\nand second best results are in red and blue colors.\nDataset \u001b BM3D DnCNN FFDNet BRDNet IPTy DRUNet SwinIRz EDT-B EDT-By EDT-B\u0003\n[Dabov et\nal., 2007] [Zhang et al.\n, 2017] [Zhang et al.\n, 2018a] [Tian et\nal., 2020] [Chen et al.\n, 2021] [Zhang et al.\n, 2021] [Liang et al.\n, 2021] (Ours) (Ours) (Ours)\nCBSD68\n15 33.52 33.90 33.87 34.10 - 34.30 34.42 34.33 34.38 34.39\n25 30.71 31.24 31.21 31.43 - 31.69 31.78 31.73 31.76 31.76\n50 27.38 27.95 27.96 28.16 28.39 28.51 28.56 28.55 28.57 28.56\nKodak24\n15 34.28 34.60 34.63 34.88 - 35.31 35.34 35.25 35.31 35.37\n25 32.15 32.14 32.13 32.41 - 32.89 32.89 32.84 32.89 32.94\n50 28.46 28.95 28.98 29.22 29.64 29.86 29.79 29.81 29.83 29.87\nMcMaster\n15 34.06 33.45 34.66 35.08 - 35.40 35.61 35.43 35.51 35.61\n25 31.66 31.52 32.35 32.75 - 33.14 33.20 33.20 33.26 33.34\n50 28.51 28.62 29.18 29.52 29.98 30.08 30.22 30.21 30.25 30.25\nUrban100\n15 33.93 32.98 33.83 34.42 - 34.81 35.13 34.93 35.04 35.22\n25 31.36 30.81 31.40 31.99 - 32.60 32.90 32.78 32.86 33.07\n50 27.93 27.59 28.05 28.56 29.71 29.61 29.82 29.93 29.98 30.16\nTable 4: Quantitative comparison for color image denoising on PSNR(dB) on RGB channels. ‚Äúz‚Äù means the \u001b = 25=50 models of SwinIR\nare pre-trained on the \u001b= 15level. ‚Äúy‚Äù indicates methods with pre-training. ‚Äú\u0003‚Äù means our model without pre-training and downsampling.\nperformance on the low level noises. Nonetheless, we sug-\ngest that the proposed EDT model is still a good choice for\ndenoising tasks since it strikes a sweet point between per-\nformance and computational complexity. For example, the\nFLOPs of EDT-B (38G) is only 8.4% of SwinIR (451G).\n4.3 Deraining Results\nWe evaluate the performance of our EDT on Rain100L[Yang\net al., 2019] and Rain100H [Yang et al., 2019] two datasets,\naccounting for light and heavy rain streaks. As shown in Ta-\nble 5, though the model size of our EDT-B (11.5M) for de-\nraining is far smaller than IPT (116M), it still outperforms\nIPT by 0.52dB on the light rain setting. Meanwhile, our\nmodel reaches signiÔ¨Åcantly superior results by 2.66dB gain\non the heavy rain setting, compared to the second-best RCD-\nNet [Wang et al., 2020], supporting that EDT performs well\nfor restoration tasks with heavy degradation.\n5 Conclusion\nBased on the proposed framework, we perform an in-depth\nanalysis of transformer-based image pre-training in low-level\nvision. We Ô¨Ånd pre-training plays the central role of de-\nveloping stronger intermediate representations by incorporat-\ning more local information. Also, we Ô¨Ånd the effect of pre-\nMethod RAIN100L RAIN100H\nPSRN SSIM PSNR SSIM\nDSC [Luo et al.\n, 2015] 27.34 0.8494 13.77 0.3199\nGMM [Li et al.\n, 2016] 29.05 0.8717 15.23 0.4498\nJCAS [Gu et al.\n, 2017] 28.54 0.8524 14.62 0.4510\nClear [Fu et al.\n, 2017a] 30.24 0.9344 15.33 0.7421\nDDN [Fu et al.\n, 2017b] 32.38 0.9258 22.85 0.7250\nRESCAN [Li et al.\n, 2018] 38.52 0.9812 29.62 0.8720\nPReNet [Ren et al.\n, 2019] 37.45 0.9790 30.11 0.9053\nSPANet [W\nang et al., 2019] 35.33 0.9694 25.11 0.8332\nJORDER E [Yang et\nal., 2019] 38.59 0.9834 30.50 0.8967\nSSIR [Wei et\nal., 2019] 32.37 0.9258 22.47 0.7164\nRCDNet [Wang et\nal., 2020] 40.00 0.9860 31.28 0.9093\nIPTy[Chen et al.\n, 2021] 41.62 0.9880 - -\nEDT-By(Ours) 42.14 0.9903 34.02 0.9406\nTable 5: PSNR(dB)/SSIM results for image deraining on the Y chan-\nnel. ‚Äúy‚Äù indicates methods with pre-training.\ntraining is task-speciÔ¨Åc, leading to signiÔ¨Åcant improvements\non SR and deraining while limited gains on denoising. Then,\nwe suggest multi-related-task pre-training exhibits great po-\ntential in digging image priors, far more efÔ¨Åcient than using\nlarger pre-training datasets. Finally, we show how data scale\nand model size affect the performance of pre-training and\npresent comparisons between transformers and ConvNets.\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n1095\nAcknowledgments\nThis work is partially supported by Shenzhen Science and\nTechnology Program KQTD20210811090149095 and also\nthe Pearl River Talent Recruitment Program 2019QN01X226.\nReferences\n[Agustsson and Timofte, 2017] Eirikur Agustsson and Radu\nTimofte. Ntire 2017 challenge on single image super-\nresolution: Dataset and study. In CVPRW, pages 126‚Äì135,\n2017.\n[Brown et al., 2020] Tom B Brown, Benjamin Mann, Nick\nRyder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, et al. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165, 2020.\n[Chen et al., 2017] Liang-Chieh Chen, George Papandreou,\nIasonas Kokkinos, Kevin Murphy, and Alan L Yuille.\nDeeplab: Semantic image segmentation with deep convo-\nlutional nets, atrous convolution, and fully connected crfs.\nPAMI, 40(4):834‚Äì848, 2017.\n[Chen et al., 2021] Hanting Chen, Yunhe Wang, Tianyu\nGuo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma,\nChunjing Xu, Chao Xu, and Wen Gao. Pre-trained image\nprocessing transformer. In CVPR, pages 12299‚Äì12310,\n2021.\n[Chen et al., 2022] X Chen, X Wang, J Zhou, and C Dong.\nActivating more pixels in image super-resolution trans-\nformer. arxiv 2022. arXiv preprint arXiv:2205.04437,\n2022.\n[Cortes et al., 2012] Corinna Cortes, Mehryar Mohri, and\nAfshin Rostamizadeh. Algorithms for learning kernels\nbased on centered alignment. The Journal of Machine\nLearning Research, 13(1):795‚Äì828, 2012.\n[Dabov et al., 2007] Kostadin Dabov, Alessandro Foi,\nVladimir Katkovnik, and Karen Egiazarian. Image\ndenoising by sparse 3-d transform-domain collaborative\nÔ¨Åltering. TIP, 16(8):2080‚Äì2095, 2007.\n[Dai et al., 2019] Tao Dai, Jianrui Cai, Yongbing Zhang,\nShu-Tao Xia, and Lei Zhang. Second-order attention net-\nwork for single image super-resolution. In CVPR, pages\n11065‚Äì11074, 2019.\n[Deng et al., 2009] Jia Deng, Wei Dong, Richard Socher, Li-\nJia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In CVPR, pages 248‚Äì255.\nIeee, 2009.\n[Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805, 2018.\n[Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition\nat scale. In ICLR, 2020.\n[Fu et al., 2017a] Xueyang Fu, Jiabin Huang, Xinghao Ding,\nYinghao Liao, and John Paisley. Clearing the skies: A\ndeep network architecture for single-image rain removal.\nTIP, 26(6):2944‚Äì2956, 2017.\n[Fu et al., 2017b] Xueyang Fu, Jiabin Huang, Delu Zeng,\nYue Huang, Xinghao Ding, and John Paisley. Remov-\ning rain from single images via a deep detail network. In\nCVPR, pages 3855‚Äì3863, 2017.\n[Girshick et al., 2014] Ross Girshick, Jeff Donahue, Trevor\nDarrell, and Jitendra Malik. Rich feature hierarchies for\naccurate object detection and semantic segmentation. In\nCVPR, pages 580‚Äì587, 2014.\n[Girshick, 2015] Ross Girshick. Fast r-cnn. In ICCV, pages\n1440‚Äì1448, 2015.\n[Gretton et al., 2007] Arthur Gretton, Kenji Fukumizu,\nChoon Hui Teo, Le Song, Bernhard Sch ¬®olkopf, Alexan-\nder J Smola, et al. A kernel statistical test of independence.\nIn Nips, volume 20, pages 585‚Äì592. Citeseer, 2007.\n[Gu et al., 2017] Shuhang Gu, Deyu Meng, Wangmeng Zuo,\nand Lei Zhang. Joint convolutional analysis and synthesis\nsparse representation for single image layer separation. In\nICCV, pages 1708‚Äì1716, 2017.\n[He et al., 2021] Kaiming He, Xinlei Chen, Saining Xie,\nYanghao Li, Piotr Doll ¬¥ar, and Ross Girshick. Masked\nautoencoders are scalable vision learners. arXiv preprint\narXiv:2111.06377, 2021.\n[Huang et al., 2015] Jia-Bin Huang, Abhishek Singh, and\nNarendra Ahuja. Single image super-resolution from\ntransformed self-exemplars. In CVPR, pages 5197‚Äì5206,\n2015.\n[Kolesnikov et al., 2020] Alexander Kolesnikov, Lucas\nBeyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung,\nSylvain Gelly, and Neil Houlsby. Big transfer (bit):\nGeneral visual representation learning. In ECCV, pages\n491‚Äì507. Springer, 2020.\n[Kornblith et al., 2019] Simon Kornblith, Jonathon Shlens,\nand Quoc V Le. Do better imagenet models transfer bet-\nter? In CVPR, pages 2661‚Äì2671, 2019.\n[LeCun et al., 1989] Yann LeCun, Bernhard Boser, John S\nDenker, Donnie Henderson, Richard E Howard, Wayne\nHubbard, and Lawrence D Jackel. Backpropagation ap-\nplied to handwritten zip code recognition. Neural compu-\ntation, 1(4):541‚Äì551, 1989.\n[Li et al., 2016] Yu Li, Robby T Tan, Xiaojie Guo, Jiangbo\nLu, and Michael S Brown. Rain streak removal using layer\npriors. In CVPR, pages 2736‚Äì2744, 2016.\n[Li et al., 2018] Xia Li, Jianlong Wu, Zhouchen Lin, Hong\nLiu, and Hongbin Zha. Recurrent squeeze-and-excitation\ncontext aggregation net for single image deraining. In\nECCV, pages 254‚Äì269, 2018.\n[Liang et al., 2021] Jingyun Liang, Jiezhang Cao, Guolei\nSun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir:\nImage restoration using swin transformer. In ICCVW,\npages 1833‚Äì1844, 2021.\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n1096\n[Liu et al., 2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu,\nYixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using\nshifted windows. ICCV, 2021.\n[Liu et al., 2022] Lin Liu, Lingxi Xie, Xiaopeng Zhang,\nShanxin Yuan, Xiangyu Chen, Wengang Zhou, Houqiang\nLi, and Qi Tian. Tape: Task-agnostic prior embedding\nfor image restoration. In ECCV, pages 447‚Äì464. Springer,\n2022.\n[Luo et al., 2015] Yu Luo, Yong Xu, and Hui Ji. Removing\nrain from a single image via discriminative sparse coding.\nIn ICCV, pages 3397‚Äì3405, 2015.\n[Mahajan et al., 2018] Dhruv Mahajan, Ross Girshick, Vig-\nnesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan\nLi, Ashwin Bharambe, and Laurens Van Der Maaten. Ex-\nploring the limits of weakly supervised pretraining. In\nECCV, pages 181‚Äì196, 2018.\n[Mei et al., 2021] Yiqun Mei, Yuchen Fan, and Yuqian Zhou.\nImage super-resolution with non-local sparse attention. In\nCVPR, pages 3517‚Äì3526, 2021.\n[Nguyen et al., 2020] Thao Nguyen, Maithra Raghu, and Si-\nmon Kornblith. Do wide and deep networks learn the\nsame things? uncovering how neural network repre-\nsentations vary with width and depth. arXiv preprint\narXiv:2010.15327, 2020.\n[Radford et al., 2018] Alec Radford, Karthik Narasimhan,\nTim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training. 2018.\n[Radford et al., 2019] Alec Radford, Jeffrey Wu, Rewon\nChild, David Luan, Dario Amodei, Ilya Sutskever, et al.\nLanguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9, 2019.\n[Raghu et al., 2021] Maithra Raghu, Thomas Unterthiner,\nSimon Kornblith, Chiyuan Zhang, and Alexey Dosovit-\nskiy. Do vision transformers see like convolutional neural\nnetworks? arXiv preprint arXiv:2108.08810, 2021.\n[Ren et al., 2019] Dongwei Ren, Wangmeng Zuo, Qinghua\nHu, Pengfei Zhu, and Deyu Meng. Progressive image de-\nraining networks: A better and simpler baseline. In CVPR,\npages 3937‚Äì3946, 2019.\n[Sun et al., 2017] Chen Sun, Abhinav Shrivastava, Saurabh\nSingh, and Abhinav Gupta. Revisiting unreasonable ef-\nfectiveness of data in deep learning era. In ICCV, pages\n843‚Äì852, 2017.\n[Tian et al., 2020] Chunwei Tian, Yong Xu, and Wangmeng\nZuo. Image denoising using deep cnn with batch renor-\nmalization. Neural Networks, 121:461‚Äì473, 2020.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NIPS, pages 5998‚Äì6008, 2017.\n[Wang et al., 2018] Xintao Wang, Ke Yu, Shixiang Wu, Jin-\njin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen\nChange Loy. Esrgan: Enhanced super-resolution gener-\native adversarial networks. In ECCVW, pages 0‚Äì0, 2018.\n[Wang et al., 2019] Tianyu Wang, Xin Yang, Ke Xu,\nShaozhe Chen, Qiang Zhang, and Rynson WH Lau. Spa-\ntial attentive single-image deraining with a high quality\nreal rain dataset. In CVPR, pages 12270‚Äì12279, 2019.\n[Wang et al., 2020] Hong Wang, Qi Xie, Qian Zhao, and\nDeyu Meng. A model-driven deep neural network for sin-\ngle image rain removal. InCVPR, pages 3103‚Äì3112, 2020.\n[Wang et al., 2021a] Wenhai Wang, Enze Xie, Xiang Li,\nDeng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping\nLuo, and Ling Shao. Pyramid vision transformer: A ver-\nsatile backbone for dense prediction without convolutions.\narXiv preprint arXiv:2102.12122, 2021.\n[Wang et al., 2021b] Zhendong Wang, Xiaodong Cun, Jian-\nmin Bao, and Jianzhuang Liu. Uformer: A general u-\nshaped transformer for image restoration. arXiv preprint\narXiv:2106.03106, 2021.\n[Wei et al., 2019] Wei Wei, Deyu Meng, Qian Zhao, Zong-\nben Xu, and Ying Wu. Semi-supervised transfer learning\nfor image rain removal. InCVPR, pages 3877‚Äì3886, 2019.\n[Xiao et al., 2021] Tete Xiao, Mannat Singh, Eric Mintun,\nTrevor Darrell, Piotr Doll ¬¥ar, and Ross Girshick. Early\nconvolutions help transformers see better. arXiv preprint\narXiv:2106.14881, 2021.\n[Yang et al., 2019] Wenhan Yang, Robby T Tan, Jiashi Feng,\nZongming Guo, Shuicheng Yan, and Jiaying Liu. Joint\nrain detection and removal from a single image with con-\ntextualized deep networks. PAMI, 42(6):1377‚Äì1393, 2019.\n[Zamir et al., 2022] Syed Waqas Zamir, Aditya Arora,\nSalman Khan, Munawar Hayat, Fahad Shahbaz Khan, and\nMing-Hsuan Yang. Restormer: EfÔ¨Åcient transformer for\nhigh-resolution image restoration. In CVPR, pages 5728‚Äì\n5739, 2022.\n[Zhang et al., 2017] Kai Zhang, Wangmeng Zuo, Yunjin\nChen, Deyu Meng, and Lei Zhang. Beyond a gaussian de-\nnoiser: Residual learning of deep cnn for image denoising.\nTIP, 26(7):3142‚Äì3155, 2017.\n[Zhang et al., 2018a] Kai Zhang, Wangmeng Zuo, and Lei\nZhang. Ffdnet: Toward a fast and Ô¨Çexible solution for\ncnn-based image denoising. TIP, 27(9):4608‚Äì4622, 2018.\n[Zhang et al., 2018b] Yulun Zhang, Kunpeng Li, Kai Li,\nLichen Wang, Bineng Zhong, and Yun Fu. Image super-\nresolution using very deep residual channel attention net-\nworks. In ECCV, pages 286‚Äì301, 2018.\n[Zhang et al., 2021] Kai Zhang, Yawei Li, Wangmeng Zuo,\nLei Zhang, Luc Van Gool, and Radu Timofte. Plug-and-\nplay image restoration with deep denoiser prior. PAMI,\n2021.\nProceedings of the Thirty-Second International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-23)\n1097",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7814321517944336
    },
    {
      "name": "Transformer",
      "score": 0.6953297853469849
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5628899335861206
    },
    {
      "name": "Training set",
      "score": 0.5506908893585205
    },
    {
      "name": "Machine learning",
      "score": 0.5099962949752808
    },
    {
      "name": "Training (meteorology)",
      "score": 0.43499237298965454
    },
    {
      "name": "Task analysis",
      "score": 0.42087966203689575
    },
    {
      "name": "Task (project management)",
      "score": 0.3980615735054016
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.33074408769607544
    },
    {
      "name": "Voltage",
      "score": 0.15316471457481384
    },
    {
      "name": "Engineering",
      "score": 0.10464149713516235
    },
    {
      "name": "Meteorology",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    }
  ]
}