{
    "title": "Transformer Encoder with Protein Language Model for Protein Secondary Structure Prediction",
    "url": "https://openalex.org/W4393900843",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5094362206",
            "name": "Ammar Kazm",
            "affiliations": [
                "University of Technology Malaysia",
                "University of Wasit"
            ]
        },
        {
            "id": "https://openalex.org/A2171868399",
            "name": "Aida Ali",
            "affiliations": [
                "University of Technology Malaysia"
            ]
        },
        {
            "id": "https://openalex.org/A2119792020",
            "name": "Haslina Hashim",
            "affiliations": [
                "University of Technology Malaysia"
            ]
        },
        {
            "id": "https://openalex.org/A5094362206",
            "name": "Ammar Kazm",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2171868399",
            "name": "Aida Ali",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2119792020",
            "name": "Haslina Hashim",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4213347131",
        "https://openalex.org/W4225919313",
        "https://openalex.org/W4210830887",
        "https://openalex.org/W3216165545",
        "https://openalex.org/W2008708467",
        "https://openalex.org/W4360603156",
        "https://openalex.org/W2153187042",
        "https://openalex.org/W2151831732",
        "https://openalex.org/W3146944767",
        "https://openalex.org/W3177500196",
        "https://openalex.org/W3156431688",
        "https://openalex.org/W4210535996",
        "https://openalex.org/W4242765109",
        "https://openalex.org/W4303645584",
        "https://openalex.org/W4225891318",
        "https://openalex.org/W4281993476",
        "https://openalex.org/W4229451461",
        "https://openalex.org/W1989447327",
        "https://openalex.org/W2987946433",
        "https://openalex.org/W2119571185",
        "https://openalex.org/W2041673452",
        "https://openalex.org/W1969573163",
        "https://openalex.org/W1758266623",
        "https://openalex.org/W2104641434",
        "https://openalex.org/W2099063328",
        "https://openalex.org/W2029476353",
        "https://openalex.org/W2147645629",
        "https://openalex.org/W2345051977",
        "https://openalex.org/W2594661604",
        "https://openalex.org/W2607268717",
        "https://openalex.org/W2951092885",
        "https://openalex.org/W2791790018",
        "https://openalex.org/W3027554922",
        "https://openalex.org/W2905446269",
        "https://openalex.org/W2950374603",
        "https://openalex.org/W4306889100",
        "https://openalex.org/W4317374308",
        "https://openalex.org/W4387641258",
        "https://openalex.org/W2953008890",
        "https://openalex.org/W2102461176",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2147905080",
        "https://openalex.org/W2900952222"
    ],
    "abstract": "In bioinformatics, protein secondary structure prediction plays a significant role in understanding protein function and interactions. This study presents the TE_SS approach, which uses a transformer encoder-based model and the Ankh protein language model to predict protein secondary structures. The research focuses on the prediction of nine classes of structures, according to the Dictionary of Secondary Structure of Proteins (DSSP) version 4. The model's performance was rigorously evaluated using various datasets. Additionally, this study compares the model with the state-of-the-art methods in the prediction of eight structure classes. The findings reveal that TE_SS excels in nine- and three-class structure predictions while also showing remarkable proficiency in the eight-class category. This is underscored by its performance in Qs and SOV evaluation metrics, demonstrating its capability to discern complex protein sequence patterns. This advancement provides a significant tool for protein structure analysis, thereby enriching the field of bioinformatics.",
    "full_text": "Engineering, Technology & Applied Science Research  Vol. 14, No. 2, 2024, 13124-13132  13124   \n \nwww.etasr.com Kazm et al.: Transformer Encoder with  Protein Language Model for Protein Secondary Structure … \n \nTransformer Encoder with Protein Language \nModel for Protein Secondary Structure \nPrediction \n \nAmmar Kazm \nSchool of Computing, Faculty of Engineering, Univer siti Teknologi Malaysia, Malaysia | College of \nEducation for Pure Sciences, Wasit University, Iraq \nawadkazm@graduate.utm.my (corresponding author) \n \nAida Ali \nSchool of Computing, Faculty of Engineering, Universiti Teknologi Malaysia, Malaysia \naida@utm.my \n \nHaslina Hashim \nSchool of Computing, Faculty of Engineering, Universiti Teknologi Malaysia, Malaysia \nhaslinah@utm.my \nReceived: 2 January 2024 | Revised: 15 January 2024 | Accepted: 17 January 2024 \nLicensed under a CC-BY 4.0 license | Copyright (c) by the authors | DOI: https://doi.org/10.48084/etasr.6855 \nABSTRACT \nIn bioinformatics, protein secondary structure pred iction plays a significant role in understanding pr otein \nfunction and interactions. This study presents the TE_SS approach, which uses a transformer encoder-\nbased model and the Ankh protein language model to predict protein secondary structures. The research \nfocuses on the prediction of nine classes of struct ures, according to the Dictionary of Secondary Stru cture \nof Proteins (DSSP) version 4. The model's performan ce was rigorously evaluated using various datasets.  \nAdditionally, this study compares the model with th e state-of-the-art methods in the prediction of eig ht \nstructure classes. The findings reveal that TE_SS excels in nine- and three-class structure predictions while \nalso showing remarkable proficiency in the eight-cl ass category. This is underscored by its performanc e in \nQs and SOV evaluation metrics, demonstrating its capability to discern complex protein sequence patterns. \nThis advancement provides a significant tool for pr otein structure analysis, thereby enriching the fie ld of \nbioinformatics.  \nKeywords-protein secondary structure prediction; bi oinformatics; nine-class protein prediction; transf ormer \nmodel; Ankh protein language model  \nI.  INTRODUCTION  \nProteins are made up of chains of amino acids. By a ltering \ntheir arrangement, 20 different types of acids can create a wide \nrange of proteins. The primary structure of a prote in is \nrepresented by one sequence, comprising the specifi c order in \nwhich the amino acids are arranged [1], and is referred to as 1D \nstructure. Tertiary structures, often referred to a s three shapes, \nare formed in living organisms through the interactions, among \namino acids. These interactions play a crucial role  in \ndetermining the function of proteins [2]. To fully comprehend \nthe relationship between the tertiary structures of a protein, it is \nimportant to predict its secondary structure [3]. T he use of \nefficient techniques to forecast protein structures  has become \nessential in closing the disparity between the numb er of \nrecognized protein sequences and the determined structures due \nto the limitations that experimental procedures ent ail, such as \ntime requirements and the substantial costs involved [4]. These \npredictive models are instrumental in enhancing our  \ncomprehension of protein functions and may be utili zed in \napplications like drug development and disease control [5]. \nSecondary structure in proteins refers to the folde d patterns \nthat occur within a chain of acids as a result of f orces like \nhydrogen and van der Waals bonds. To precisely defi ne \nstructure, the Dictionary of Secondary Structure of  Proteins \n(DSSP) was devised [6]. This program analyzes the coordinates \nof proteins with known structures to identify patte rns of \nhydrogen bonding and geometric characteristics. DSSP assigns \na type of secondary structure to each residue in the protein. The \noriginal classification consisted of eight classes: G (310 helix), \nH ( α-helix), I ( π-helix), B (isolated β-strand), E (extended \nstrand), S (bend), T (turn), and L (irregular struc ture). These \ncategories are often grouped into three group classes: H, G, I to \nEngineering, Technology & Applied Science Research  Vol. 14, No. 2, 2024, 13124-13132  13125   \n \nwww.etasr.com Kazm et al.: Transformer Encoder with  Protein Language Model for Protein Secondary Structure … \n \nhelix (H), (B, E) to strand (E), and T, S, L to coi l (C). The \nlatest iteration of DSSP, version 4.0, which was re leased in \n2021, marks a significant update in the field of pr otein \nsecondary structure classification. This version ex tends the \nconventional eight types of secondary structures to  include a \nninth type, known as the poly-proline helix (P) [7] . The task of \nProtein Secondary Structure Prediction (PSSP) invol ves \nassigning classes of structures, such as alpha heli ces, beta \nsheets, and coils to each individual amino acid in a protein \nchain. For computational methods to predict a struc ture, it is \nnecessary to represent acids as numeric vectors. One-hot vector \napproach uses 21-encoding for each amino acid in pr otein \nsequence, which includes the 20 standard amino acid s that \nmake up the proteins and 1 non-standard amino acid \nrepresented by X to indicate an unknown or unspecif ied amino \nacid. However, this method has shown limited accura cy, in \nprediction. Another used technique involves utilizi ng PSSM \nprofile features [8] or HHM profile features [9]. T hese profile \nfeatures incorporate information derived from analy zing \nsequence alignments obtained from a large protein s equence \ndatabase. \nCreating Hidden Markov Models (HMMs) or Position \nSpecific Scoring Matrices (PSSMs) for each template sequence \ncan be a time-consuming process, especially when dealing with \nproteins that have no sequences. To overcome this h urdle, \nrecent advancements have introduced novel protein \nrepresentation techniques inspired by methods used in natural \nlanguage processing [10-13]. These techniques invol ve the \nusage of pretrained protein language models, follow ed by fine \ntuning for specific tasks. These models can achieve  \nperformance even with limited task specific data av ailable, \nwhere embedding from a language model pretrained on  a large \ncorpus of protein sequences effectively replaces ev olutionary \ninformation. The implementation of this approach ha s \ndemonstrated encouraging outcomes in several protei n-related \nsubsequent studies [4, 7, 14-19]. In the early stag es of PSSP \nresearch statistical approaches were predominantly used. These \nmethods focused on determining the likelihood of amino acids, \nin protein structures [20]. Initially these predict ors were \ndesigned for a three-class secondary structure prediction due to \ntraining data availability and computational constr aints of that \ntime. However, the particular methods encountered c hallenges \nin achieving high Q3 accuracy because they struggle d to \nextract information from primary protein structure sequences. \nTo overcome this limitation and improve their perfo rmance, \nresearchers started incorporating information and p osition \nspecific scoring matrices into the prediction proce ss. This \nadvancement proved significant, leading to a Q3 acc uracy \nexceeding 70% [21]. \nVarious machine learning techniques have been used for \nperforming coarse-grained prediction, including dec ision trees \n[22], support vector machines [23], Neural Networks  (NN) \n[24], HMMs [25], probabilistic graph models [26], a nd k-\nnearest neighbors [27]. The methodologies in this a rea \nprimarily utilize a fixed-size sliding window appro ach. This \nmethod was employed to forecast the secondary struc ture \ncategory of the essential amino acid residue in a g iven \nsequence. JPred4 [28] and PSIPRED V3.0 [29] were no table \namong the initial prediction algorithms. These tech niques laid \nthe foundation for further progress in the field, d emonstrating \nthe effectiveness of machine learning in understand ing and \npredicting protein structures. The increased availa bility of data \nhas led to the dominance of sequence-to-sequence de ep model \npredictions, which have achieved state-of-the-art p erformance. \nInnovations in this area include DCRNN [30], which uses \ncascaded Convolutional and Recursive NN to extract both \nmultiscale local and global contextual features. Ot her \nsignificant contributions include multiscale chaine d \nconvolutional architecture for improved eight-state  prediction \n[31]. SPIDER3 [32] uses LSTM BRNNs to capture compl ex \namino acid interactions, DeepACLSTM [33] integrates  \nnetworks with LSTM units and utilizes specific dime nsions in \nprotein sequence feature vectors. MUFOLD SS [34] an d \nSAINT [35] both employ Deep inception-inside-incept ion \nnetworks with MUFOLD SS emphasizing inception modul es \nwhile SAINT incorporates self-attention mechanisms.  SPOD \n1D [36] combines LSTM BRNN and ResNet models with \nresidue contact maps for its predictions. NetSurfP 2.0 [37] \nemploys convolutional and LSTM networks, while \nShuffleNet_SS [5] focuses on a lightweight convolut ional NN. \nAnother important development is the introduction o f the \nprotein encoder [38]. This method employs a two-ste p process, \nbeginning with an unsupervised autoencoder for feat ure \nextraction, followed by an ensemble of feature sele ction \nmethods. A common element in earlier prediction mod els is \ntheir reliance on profile features, which are prima rily obtained \nfrom Multiple Sequence Alignments (MSA). Neverthele ss, the \nspecific process, especially considering the rapidl y expanding \nprotein sequence databases, poses a significant tim e constraint. \nIn response to this challenge, recent research has shifted toward \nleveraging embedding features extracted from pretra ined \nprotein language models. For instance, DML_SS [4] a pplies \nlearning through a deep centroid model, for its pre dictions. \nSPOT 1D LM [19] synergizes embeddings from language  \nmodels with one hot encoding techniques. LIFT SS [7 ] focuses \non tuning pretrained protein language models.  \nIn the field of predicting protein secondary struct ures, it is \ninteresting to note that most current predictors ap art from \nLIFT_SS [7] rely on the eight class assignments of structures \nfrom the previous version of the DSSP program for t heir \ntraining and evaluation data. Notably, even subsequ ent studies \npublished after the introduction of DSSP 4.0 have c ontinued to \nrely on eight-class secondary structure information , rather than \nadopting the more recent nine-class secondary struc ture \nclassification. This trend indicates that these methodologies are \nbeing trained and evaluated using potentially outda ted labeling \ninformation. In this study, the latest edition DSSP  4, a \ncomprehensive database for secondary structure sequ ences, \nwas utilized. The use of DSSP 4 ensured the trainin g and \nevaluation data were based on a detailed classifica tion of \nprotein structures. Additionally, the Ankh protein language \nmodel [39] was adopted for obtaining protein embedd ings, \nleveraging its capability to accurately represent p rotein \nsequences and replace the need for more computation ally \nintensive evolutionary information. TE_SS, a deep \ntransformer-based model [40], specifically designed  to discern \ncomplex relationships between distant and proximal amino acid \nsequences in proteins, is proposed. This model is s pecifically \nEngineering, Technology & Applied Science Research  Vol. 14, No. 2, 2024, 13124-13132  13126   \n \nwww.etasr.com Kazm et al.: Transformer Encoder with  Protein Language Model for Protein Secondary Structure … \n \ndesigned to discern complex relationships between l ocal and \nnonlocal amino acid sequences in proteins, processi ng \nsequential features in parallel, in contrast to exi sting models \nthat extract features sequentially. The architecture of this model \nenables it to capture patterns and interactions wit hin protein \nstructures, enhancing the prediction of secondary structure. \nII.  METHODS \nA.  Dataset \nIn this research, a collection of protein training data was \ngenerated using the PISCES server [38]. The latter is well \nknown for creating curated lists of sequence subset s from the \nProtein Data Bank (PDB). To assess protein structur e \nprediction algorithms, criteria and parameters rela ted to \nsequence identity were applied. The PISCES server u tilizes a \nfilter based on predefined protein parameters. Subs equently it \nsends the resulting lists and sequence files directly to the email \naddress provided. To ensure the dataset’s reliabili ty and \nusefulness, the PISCES server was configured with s ettings, \nincluding a maximum resolution of 2.0 Å, an upper l imit R \nvalue of 2.0 and a requirement that there will be n o more than \n50% sequence identity between any pair of protein s equences. \nInitially the dataset suggested consisted of 16,225  proteins. \nHowever, 188 proteins from this collection were exc luded \nbecause they lacked corresponding information. In a ddition, to \nmaintain the integrity of the performed analysis an d avoid data \ncontamination, any proteins that overlapped with th e proposed \ntest dataset were eliminated.  Following these rigorous filtering \ncriteria, a refined dataset, labeled as 16,037, con sisting of \n16,037 proteins was successfully curated. This data set was \nstrategically partitioned, with 15,037 proteins des ignated for \nthe training set and the remaining 1000 proteins al located for \nvalidation purposes. \nThis study involves datasets for secondary structur e \nanalysis of protein based on the 9-class classifica tion provided \nby DSSP4. The DSSP software generates a DSSP file f or each \nprotein with an established structure, which contai ns detailed \nsecondary structure information derived from the pr otein’s \nthree-dimensional structural data recorded in the PDB database. \nIn the performed methodology, the Biopython library  was \ninitially employed to retrieve the PDB file corresp onding to a \ngiven protein chain. This file is accessed from the PDB website \nusing the specific PDB ID and the chain ID of the p rotein \nchain. The occurrence of nonstandard amino acids in  these \nfiles, including modified residues was observed. A notable \nexample includes the representation of methionine ( MET) and \nselenomethionine (MSE) by the one-letter code M. To  address \nthis issue, a conversion process in which the three -letter amino \nacid codes in the PDB file were translated to their  one-letter \nequivalents, with nonstandard amino acids denoted a s X, was \nimplemented. This modified sequence is referred to as the \ntarget primary sequence. Subsequently, the identica l DSSP file \nwas acquired on the basis of the PDB file. Contiguo us \nfragments of amino acid residues and their associat ed \nsecondary structures were extracted from this file,  guided by \nthe chain ID and the residue sequence number. Howev er, \nextracting a primary sequence from the DSSP file th at exactly \nmatches the target primary sequence in terms of seq uence \ncomposition or length is often not feasible [6]. To  accurately \nrepresent the sequence of protein structure, the pr imary \nsequence of interest is matched with the sequence o btained \nfrom the DSSP file. During this alignment process, any gaps \nthat occur are filled with the letter X to indicate  unassigned \ntypes of structures. To achieve this alignment, the  Pairwise2 \nalignment algorithm from the BioPython package was utilized \n[39]. \nPerformance metrics for nine-class prediction were \nassessed on diverse datasets. Three editions of the  CASP \ncompetition, namely CASP12, CASP13, and CASP14, wer e \nutilized. These datasets encompass a selection of 47, 41, and 33 \nprotein chains, respectively, carefully chosen to r epresent real-\nworld challenges in protein structure prediction. A dditionally, \nthe CB433 test data [4], a curated and filtered sub set of the \nwidely used CB513 dataset, comprising 433 protein s tructures \nwas considered. Evaluating the proposed model fairl y against \nexisting models necessitates the use of datasets th at adhere to \nthe same 8-class system. For this purpose, two well-known test \ndatasets, TEST2016 and TEST2018 [34], containing 12 13 and \n250 protein sequences accordingly, were chosen. The se are in \nline with the training and validation sets, which i nclude 10029 \nand 983 proteins, respectively. Across all four dat asets, the \nmaximum length of any protein sequence does not exc eed 700. \nAdditionally, the primary and secondary structure s equence \ndata, which is the standard for these datasets, is also utilized. \nB.  Embedding \nPretrained models that focus on protein language (P rotein \nLanguage Models-PLMs) have become a tool, in biolog ical \napplications serving as a strong foundation for mod eling \nprotein related tasks \n. While most approaches rely on these \nmodels for extracting features, this study takes an  approach by \nutilizing the Ankh model [39], which is a  large unsupervised \nPLM. Ankh, built on a transformer-based architectur e and has \nbeen trained on the BFD [41] and UniRef50 [42] data set. It \nachieves state of the art performance while using less than 10% \nof the parameters compared to models. This impressi ve \nefficiency opens up possibilities for accessible an d scalable \nprotein modeling applications. One of the strengths of the Ankh \nmodel lies in its ability to extract high quality e mbedding \nfeatures that represent proteins accurately. These features are \nrepresentations of protein sequences that capture i nformation \nabout their structure, function and evolutionary re lationships. \nTo acquire the embedding feature of a specific prot ein chain \nusing Ankh we input its sequence into the model enc oder and \nretrieve its output. Each amino acid, in a protein sequence is \nassigned a 1536-feature vector through the output e mbedding, \nwhich captures its information. For every protein s equence L \nthis model generates an embedding vector of size (L *1536). \nThe embeddings acquired from the embedding process  were \nused as the input for the model. \nC.  Model Architecture \nA novel model has been developed to predict protein  \nstructures. It heavily relies on transformer archit ectures. These \ntransformers are great, at identifying both distant  relationships \nwithin protein sequences by utilizing self-attention mechanisms \nand feed forward layers. The initial input for this  model is a \ntwo-dimensional embedding, with (L, 1536) sequence \nEngineering, Technology & Applied Science Research  Vol. 14, No. 2, 2024, 13124-13132  13127   \n \nwww.etasr.com Kazm et al.: Transformer Encoder with  Protein Language Model for Protein Secondary Structure … \n \ndimensions. This embedding is generated using the t raining \nAnkh model. Additionally, the model incorporates encoding for \neach amino acid in the sequence to enhance informat ion \nrepresentation. After that, a series of N transform er encoders \nprocess the input as shown in Figure 1. This approa ch \ndemonstrates how effective the model is at capturin g patterns, \nwithin protein sequences leading to accurate predic tions of \nprotein structures. \n \n \nFig. 1.  Transformer-based model architecture for protein se condary \nstructure prediction. \n1)  Positional Encoding  \nTo ensure the proposed model effectively takes into  \naccount the nature of protein sequences, positional  encoding \nwas incorporated. This approach produces data on th e precise \nlocations of amino acids throughout the protein seq uence. By \ncombining positional encoding with amino acid embed dings, \nnot only can this model comprehend the unique chara cteristics \nof each amino acid, but also their contextual relat ionships \nwithin the sequence. This approach is crucial for c apturing the \nspatial details of amino acids, which are essential for accurately \npredicting protein secondary structure. Positional encoding \n(PESS) is defined as follows: \nPESS \u0004\u0005\u0006\u0007,\t\n\u000b \f  sin\u0011psn/10000 \t\n/\u0016\n\u0017 \u0018\u0019\u001a\u001b\u001c \u001d (1) \nPESS \u0004\u0005\u0006\u0007,\t\n\u001e\u001f\u000b \f  cos\u0011psn/10000 \t\n/\u0016\n\u0017 \u0018\u0019\u001a\u001b\u001c \u001d (2) \nwhere psn  represents the position of an amino acid in the \nsequence and \" is its dimension in the encoding space, whereas \n#\"$ \u0017%\u0016&'  refers to the dimensionality of the model [43]. In  (1) \nthe encoding for odd sequence positions is addressed, while (2) \npertains to the encoding for even sequence positions. \nIn (3) the positional encoding obtained from (1) an d (2) is \nillustrated and added into the input embeddings. \n()$* \n+ \f ()$, \n - PESS    (3) \nwhere ()$* \n+  is positionally encoded embedding for \" amino \nacid and ()$, \n is \" amino acid's embedding obtained from the \nAnkh model. \nBy including these data the suggested model acquire s a \ncomprehension of the protein’s arrangement, which i mproves \nits predictive abilities, for the secondary structure. \n2)  Transformer Encoder in Protein Secondary Structure \nPrediction \nThe transformer encoder is a component of the trans former \narchitecture [43] used for processing sequences in parallel. It is \ncomposed of layers, each of which has two sublayers : the \nPosition-Wise Feed Forward  Network and the Multi Head Self \nAttention Mechanism. \nThe key elements of the Transformer Encoder are: \na)  Multi-head Attention \nThe multi head self-attention mechanism plays a role, in the \nencoder by allowing the model to evaluate and adjus t the \nimportance of segments within an input sequence. It  creates \nthree vector representations, i.e. query (Q), key ( K), and value \n(V) for each input element. By measuring the simila rity \nbetween Q and K, the attention scores are calculate d to \ndetermine a sum of V vectors highlighting most rele vant \ninformation. This process is performed across multi ple heads \nenabling focus on different aspects of the sequence . The \nmathematical formulation, for this process is [43]: \n.//)0/\"10 \u00042, 3, 4\u000b \f 516/$78 9\n:; <\n=\u0016>\n? 4 (4) \nwhere =#;  serves as a scaling factor to ensure values for \nsequences. \nb)  Feed–Forward Networks \nAfter the self-attention mechanism, the data pass t hrough a \nfeed-forward NN, which is applied to each position separately \nand identically. This network consists of fully connected layers \nwith activation functions and is responsible for fu rther \ntransforming the representation. \nc)  Layer Normalization  \nEach sublayer of both self-attention and feed-forwa rd \nnetworks in the transformer encoder has a residual connection \naround it, followed by layer normalization. The res idual \nconnections help mitigate the vanishing gradient pr oblem, \nenabling the training of very deep models. \nd)  Stacking of Layers  \nThe self-attention, multi-head attention, and feed- forward \nlayers are stacked together, forming multiple encod er layers. \nEach layer builds upon the previous one, gradually extracting \nincreasingly complex and higher-level representatio ns of the \nsequence. \n\nEngineering, Technology & Applied Science Research  Vol. 14, No. 2, 2024, 13124-13132  13128   \n \nwww.etasr.com Kazm et al.: Transformer Encoder with  Protein Language Model for Protein Secondary Structure … \n \n3)  Convolution 1D Layer  \nTo further augment the model’s capability to extrac t \ninformative features, a 1D convolutional layer foll ows the \ntransformer encoder. This layer operates along the feature \ndimension, applying learnable filters to capture lo cal patterns \nand dependencies within the feature space. Mathematically, the \nexpression of the one-dimensional convolution opera tion can \nbe formally articulated as: \n@ \f  6\u0004A ⊛  ( -  *\u000b     (5) \nwhere Y is the output feature map, X is the input feature map, w \nis the convolutional filter, b is the bias term, ⊛ is the \nconvolution operator, and f  is the activation function. \n4)  Final Fully Connected Layer \nThe architecture concludes with a fully connected l ayer, \nwhich serves as the classification component of the model. This \nlayer translates the processed features into predic tions of the \nprotein's secondary structure. \nD.  Evaluation Metrics \nTo evaluate the effectiveness of the proposed approach, two \nemployed metrics were utilized; Q\ns accuracy and Segment \nOverlap (SOV) [44]. Qs accuracy measures how the predicted \nsecondary structure aligns, with the determined sec ondary \nstructure specifically looking at the proportion of  residues that \nmatch. Meanwhile, SOV assesses how closely the pred icted \nand experimentally determined secondary structure s egments \nresemble each other. In addition to these metrics, F1, Precision, \nand Recall were also employed to evaluate the propo sed \nmodel’s performance on the selected test dataset. Qs accuracy \nquantifies the proportion of residues where the pre dicted \nsecondary structure aligns with the findings. This metric plays \na role in assessing a model’s ability to accurately classify types \nof secondary structures found in proteins. Precision is indicated \nby how residues are correctly predicted for their c orresponding \nsecondary structures. It expands on the conventiona l Q3 \naccuracy measure S = (H, E, C) by categorizing seco ndary \nstructures into nine categories: S = (H, G, I, P, B , E, T, S, L). \nTo compute Qs we divide the number of correctly predicted \nresidues, in state 5 (ns) by the total number of residues actually \nin state 5 (Ns), with s representing each state within the set S. \nThis is formally represented in (6): \n2 C \f\nDE\nFE\n × 100  , 5 ∈  S   (6)  \nTo calculate the overall accuracy for per residue p rediction \nall ( ns) values for each state 5 in set S are summed up and \ndivided by the sum of all ( Ns) values for each state 5 in set S \n[4]: \n2|J|  \f  \n∑ DE E∈L\n∑ FE E∈L\n × 100     (7) \nThe SOV metric is crucial when evaluating the precision of \nprotein secondary structure predictions. Unlike acc uracy \nmeasures, SOV provides a detailed evaluation by con sidering \nboth length and overlap between the predicted and a ctual \nsegments. This metric is useful when assessing pred ictions for \nstructure elements like alpha helices and beta shee ts which can \nvary significantly in length. SOV compares how well predicted \nsegments align with segments in terms of length and overlap. It \ntakes into account variations, in segment size maki ng it a \ncomprehensive and realistic measure to assess predi ction \nperformance for complex proteins that exhibit diver se \nsecondary structures. \nE.  Implementation Details \nPyTorch framework was used as it offers a graph, \nimperative execution style and a wide range of tool s and \nlibraries. To ensure training and avoid overfitting  to data \npatterns, the minibatch size was set to 8 and rando m sampling \nwas employed to create minibatches. For optimizing the \nsuggested models, the AdamW optimizer was used with  a \nweight decay value of 0.0001. Throughout the traini ng process \na fixed learning rate of 0.00005 was maintained. To  enhance \nthe proposed model’s performance, a custom cross lo ss \nfunction that handles class imbalances by allowing optional \nweights, for different classes was implemented. Thi s function \ncalculates the loss for each instance without reduction, and then \naverages it across the minibatch while considering the provided \nclass weights. This approach ensures an impact of each class on \nthe models learning process. Moreover, a stopping c riterion \nwas implemented. The particular criterion halts training if there \nis no improvement in Q\ns accuracy, on the validation set for 5 \nconsecutive epochs. This study experiments were con ducted \nusing an NVIDIA Tesla V100 GPU with 16 GB VRAM and 32 \nGB system memory. The transformer encoder architecture used \nin this study consisted of 5 layers, each equipped with 8 \nattention heads. In these layers, a dropout rate of  0.2 was \nincorporated. The dimension of the feed forward net work was \nset to 2048. The model’s convolutional layers produ ced an \noutput with 1024 channels. \nIII.  RESULTS \nA.  Ablation Study  \nWe comprehensively evaluated the performance of the  \nproposed method through a series of experiments on the CB433 \ntest set and our validation set. These experiments were \nmeticulously designed to analyze the influence of k ey \nhyperparameters, specifically the number of transfo rmer \nencoder layers, the number of attention heads, and the learning \nrate, on the model’s effectiveness. \n1)  Number of Encoder Layers \nTo examine the effect of encoder layer depth on mod el \nperformance, this study experimented with architect ures \nranging from 1 to 7 layers, each coupled with a fix ed \nconfiguration of 8 attention heads. The validation and test \nresults on the CB433 dataset, as depicted in Figure  2, indicate \nthat the architecture with 5 encoder layers achieved the highest \n9-class accuracy (Q9). Deeper models can learn more  complex \ncontextual representations and better capture long- range \ndependencies in protein sequences, performance plat eaus, but \nthis ability slightly declines beyond 5 layers. Thi s could be \nindicative of overfitting or vanishing gradients, a ffecting the \nmodel's generalizability and learning efficacy. \nEngineering, Technology & Applied Science Research  Vol. 14, No. 2, 2024, 13124-13132  13129   \n \nwww.etasr.com Kazm et al.: Transformer Encoder with  Protein Language Model for Protein Secondary Structure … \n \n \nFig. 2.  Performance evaluation of transformer models with v arying \nnumbers of encoder layers. \n2)  Number of Attention Heads \nTo investigate the optimal configuration for protei n \nsecondary structure prediction, there was a focus on the number \nof attention heads in the transformer encoder layer . \nConfigurations with 1, 2, 3, 4, 6, 8, 12, and 16 at tention heads \nwere tested for their impact on the model’s perform ance to be \ndetermined. As shown in Figure 3, the model with 8 attention \nheads proved most effective in both validation and testing sets, \nparticularly on the CB433 dataset, achieving signif icant \nimprovements in 9-class accuracy, pointing to the o ptimal \nbalance between the granularity and breadth of atte ntion \nmechanisms. While increasing the number of attentio n heads \ngenerally improves model’s ability to discern intri cate \nrelationships within protein structures, a threshol d exists \nbeyond which additional heads may not enhance or co uld even \nreduce predictive accuracy. This highlights the imp ortance of \nfine-tuning attention mechanisms in transformer mod els for \nspecialized bioinformatics tasks mechanisms in tran sformer \nmodels for specialized bioinformatics tasks. \n \n \nFig. 3.  Comparative analysis of prediction accuracy across different \nnumbers of attention heads. \n3)  Learning Rate \nThe study examined the best hyperparameters for pro tein \nsecondary structure prediction and found that the l earning rate \nhad a significant impact on model accuracy. The for mer \nrigorously assessed the model’s performance through out a \nrange of learning rates, as shown in Figure 4: 0.00 1, 0.0005, \n0.0001, 0.00005, 0.00001, and 0.000005. In the test ing and \nvalidation stages, a learning rate of 0.00005 produ ced the best \n9-class accuracy, especially when employing the CB4 33 \ndataset. Interestingly, there was a clear trend in the model's \nperformance: the accuracy decreased dramatically at  higher \nlearning rates (0.001 and 0.0005) pointing the detr imental \neffect of rapid weight adjustments. However, as the  learning \nrate was gradually reduced, a notable improvement in accuracy \nwas observed, culminating in the optimal performanc e at \n0.00005. \n \n \nFig. 4.  Impact of learning rate on model accuracy. \nB.  Comparative Analysis on Eight-State Prediction \nThis section provides a comparative analysis of the  \nproposed method against a selection of state-of-the -art \npredictors methods, specifically focusing on two di stinct test \ndata called, TEST2016 and TEST2018. To ensure an eq uitable \ncomparison, data for existing predictors were sourc ed from the \nliterature [4, 7, 9]. The comparison encompasses a variety of \nmethods: 10 predictors based on profile features an d 3 \npredictors based on embedding features (Table I). F or the \nLIFT_SS method, the most accurate results were sele cted from \nthree lightweight fine-tuning approaches. In the co nducted \nanalysis the TE_SS method was evaluated, against th ese 13 \npredictors through the employment of different metr ics on two \ntest datasets. These metrics include Q8 and SOV8 fo r \npredictions in 8 classes and Q3 and SOV3 for predic tions in 3 \nclasses. The detailed results can be found in Table  I, which \nshowcases the performance of the proposed method al ongside \nthe 13 methods for each metric. Table I clearly dem onstrates \nthat the TE_SS method outperforms the others in pre dicting \nprotein secondary structures in both 8- and 3- clas s formats. \nNot only does this comprehensive analysis reveal th e strength \nof the TE_SS model, but also its advancement over e xisting \nstate of the art methods setting a new standard, in  protein \nsecondary structure prediction. \nC.  Comparative Analysis on Nine-State Prediction  \nTo assess the effectiveness of the TE_SS framework \nexperiments were conducted using four widely used benchmark \ndatasets, in the field of protein structure analysi s. TE_SS was \ncompared against two leading methods for predicting  9 class \nprotein structures; DML_SSembed and LIFT_SS. Both t hese \nmethods utilize embeddings derived from ProtTrans, a trained \nPL). These two methods were selected based on their  \nutilization of the 9 class predictor from DSSP4. \nDML_SSembed employs a centroid model for sequence t o \nsequence prediction. It assigns a centroid in the e mbedding \n\nEngineering, Technology & Applied Science Research  Vol. 14, No. 2, 2024, 13124-13132  13130   \n \nwww.etasr.com Kazm et al.: Transformer Encoder with  Protein Language Model for Protein Secondary Structure … \n \nspace to each structure category and aims to maximi ze the \nsimilarity between each amino acid and its correspo nding \ncentroid. This approach enhances the accuracy of se condary \nstructure prediction. In contrast, LIFT_SS utilizes a fine tuning \nstrategy on the pre trained PLM by employing 7 state of the art \nfine tuning techniques. This enables LIFT_SS to pre dict \nstructures accurately by introducing new parameters during the \nembedding process. The results of these comparisons including \npredictions, for both 9- and 3- class scenarios are  presented in \nTables II and III. Notably, the highest metric values were taken \nfrom the 7 fine-tuning techniques used by LIFT_SS. The data \nfor existing predictors were obtained from [7]. It is worth \nmentioning that the TE_SS model consistently outper formed \nboth DML_SSembed and LIFT_SS exhibiting its accurac y and \neffectiveness, in predicting protein structure. \nTABLE I.  COMPARISON (Q8, Q3, SOV8, AND SOV3 ACCURACY) WITH STATE-OF-THE-ART METHODS \nMethod TEST2016  TEST2018  \nQ8 SOV8 Q3 SOV3 Q8 SOV8 Q3 SOV3 \nCNN_BIGRU  [45]  73.91  70.92  85.04  81.61  72.78  68.75  84.17  79.41  \nDeepACLSTM  [33]  75.19  73.67  85.62  82.6  73.42  71.32  84.66  80.05  \nDCRNN [30] 72.19 68.63 83.72 78.39 70.6 65.82 82.75  75.1 \nDeepCNN [31] 74.54 71.56 85.14 79.31 72.75 69.18 84 .16 76.83 \nMUFold-SS [34] 76.03 73.67 85.97 81.98 74.29 71 84. 63 79.53 \nNetSurfP-2.0 [37] - - - - 73.81 71.14 85.31 78.58 \nSPOD -1D  [36]  76.03  73.88  86.67  79.52  74.26  71.45  85.66  78.77  \nSAINT [35] 76.23 - - - 74.48 - - - \nSPIDER-3 [32] - - 84.66 75.62 - - 83.84 73.89 \nDML_SS [4] 76.62 74.6 86.1 82.72 74.82 72.23 84.83 80.5 \nSPOT-1D-LM [19] - - - - 76.47 - 86.74 - \nDML_SSembed [4] 78.03 75.9 87.41 84.51 76.48 73.44 86.82 82.43 \nLIFT_SS  [7]  78.7  76.79  87.84  84.76  76.86  74.24  87.13  82.32  \nTE_SS 79.08 77.02 87.99 84.81 77.57 74.60 87.31 82.47 \nTABLE II.  COMPARATIVE 9-CLASS PSSP RESULTS ON THE TEST DATASETS \nMethod  CASP12 CASP13 CASP14 CB433 \nQ9 SOV9 Q9 SOV9 Q9 SOV9 Q9 SOV9 \nDML_SSembed [4] 74.81 70.07 72.32 67.25 65.4 58.43 75.59 73.35 \nLIFT_SS [7] 75.82 70.81 73.07 68.06 66.59 59.71 76. 87 74.36 \nTE_SS 76.44 71.03 75.62 71.51 67.47 60.29 78.95 76.37 \nTABLE III.  COMPARATIVE  3-CLASS PSSP RESULTS ON THE TEST DATASETS \nMethods  CASP12  CASP13  CASP14  CB433  \nQ3 SOV3 Q3 SOV3 Q3 SOV3 Q3 SOV3 \nDML_SSembed [4] 84.56 78.48 82.75 74.99 77.7 68.82 85.9 80.63 \nLIFT_SS [7] 85.24 79.38 83.56 77.3 78.38 68.23 86.6 9 81.28 \nTE_SS 85.55 79.84 84.69 77.50 78.56 69.71 87.35 81.64 \n \nD.  Multi-Metric Evaluation \nTo thoroughly evaluate models performance, an appro ach \nwas adopted by considering evaluation metrics, such  as F1 \nscore, Precision, and Recall. These metrics were ap plied to \nCB433, CASP12, CASP13, and CASP14 datasets. This \nrigorous evaluation strategy ensured that the model ’s \neffectiveness in predicting protein structure was r eliable and \napplicable to a range of protein sequences. Table IV provides a \nsummary of the models performance on these metrics \nhighlighting its accuracy in predicting protein str ucture. The \nproposed model consistently performed satisfactorily across all \ndatasets indicating its potential, for various prot ein structure \nprediction tasks. \nTABLE IV.  PERFORMANCE OF TE_SS ON TEST DATASETS  \nDataset F1 Precision Recall \nCASP12  61.28  68.66  57.45  \nCASP13 58.95 64.24 56.56 \nCASP14 45.11 53.06 42.94 \nCB433 67.72 73.16 65.55 \nIV.  CONCLUSIONS  \nIn this study, the effectiveness of the transformer -based \nTE_SS model in predicting protein structures has be en \ndemonstrated. Utilizing the Ankh protein language m odel for \nfeature embedding, the TE_SS model achieves accurat e \npredictions of protein structures in both nine and eight \nclassification systems. The model's performance in predicting \n9-class structures was evaluated on CASP12, CASP13,  \nCASP14, and CB433 test datasets. Also, the model, t rained on \ndata containing 8 classes, was evaluated on two pub licly \navailable test datasets, TEST2016 and TEST2018. The  \nexperimental results indicate improved accuracy com pared to \nthe other models. A notable advancement of TE_SS is  its \nadeptness in capturing both short-range and long-ra nge \ndependencies among residues in proteins. The abilit y of this \ntransformer-based model to process sequence data in  parallel \ndemonstrates its efficiency and effectiveness in an alyzing \ncomplex protein structures. However, it is worth noting that the \nproposed method has limitations in terms of its dem anding \nresources and GPU memory requirements. Moreover, th e \nEngineering, Technology & Applied Science Research  Vol. 14, No. 2, 2024, 13124-13132  13131   \n \nwww.etasr.com Kazm et al.: Transformer Encoder with  Protein Language Model for Protein Secondary Structure … \n \nmodel currently lacks the ability to provide inform ation \nregarding the reliability or confidence level of it s predictions. \nThis shortcoming is especially evident when the mod el \nencounters specific types of proteins or disordered  regions \nwithin proteins, where its predictions may be less accurate or \nreliable. For future work, it is imperative to addr ess these \nlimitations, potentially by developing methods to e stimate \nprediction reliability and optimizing the model for  reduced \nresource consumption. \nREFERENCES \n[1]  S. Damodaran and K. L. Parkin, Eds., \"Amino Acids, Peptides, and \nProteins,\" in Fennema’s Food Chemistry , 5th ed., Boca Raton, FL, USA: \nCRC Press, 2017. \n[2]  S. Tahzeeb and S. Hasan, \"A Neural Network-Based Mu lti-Label \nClassifier for Protein Function Prediction,\" Engineering, Technology & \nApplied Science Research , vol. 12, no. 1, pp. 7974–7981, Feb. 2022, \nhttps://doi.org/10.48084/etasr.4597. \n[3]  M. Zubair et al. , \"A Deep Learning Approach for Prediction of Prote in \nSecondary Structure,\" Computers, Materials & Continua , vol. 72, no. 2, \npp. 3705–3718, Mar. 2022, https://doi.org/10.32604/cmc.2022.026408. \n[4]  W. Yang, Y. Liu, and C. Xiao, \"Deep metric learning  for accurate \nprotein secondary structure prediction,\" Knowledge-Based Systems , vol. \n242, Apr. 2022, Art. no. 108356, https://doi.org/10 .1016/j.knosys. \n2022.108356. \n[5]  W. Yang, Z. Hu, L. Zhou, and Y. Jin, \"Protein secon dary structure \nprediction using a lightweight convolutional networ k and label \ndistribution aware margin loss,\" Knowledge-Based Systems , vol. 237, \nFeb. 2022, Art. no. 107771, https://doi.org/10.1016 /j.knosys.2021. \n107771. \n[6]  W. Kabsch and C. Sander, \"Dictionary of protein sec ondary structure: \nPattern recognition of hydrogen-bonded and geometri cal features,\" \nBiopolymers , vol. 22, no. 12, pp. 2577–2637, 1983, https://doi .org/ \n10.1002/bip.360221211. \n[7]  W. Yang, C. Liu, and Z. Li, \"Lightweight Fine-tunin g a Pretrained \nProtein Language Model for Protein Secondary Struct ure Prediction.\" \nbioRxiv, Mar. 23, 2023, https://doi.org/10.1101/2023.03.22.530066. \n[8]  D. T. Jones, \"Protein secondary structure predictio n based on position-\nspecific scoring matrices11Edited by G. Von Heijne, \" Journal of \nMolecular Biology , vol. 292, no. 2, pp. 195–202, Sep. 1999, \nhttps://doi.org/10.1006/jmbi.1999.3091. \n[9]  S. R. Eddy, \"Profile hidden Markov models.,\" Bioinformatics , vol. 14, \nno. 9, pp. 755–763, Jan. 1998, https://doi.org/10.1 093/bioinformatics/ \n14.9.755. \n[10]  A. Rives et al. , \"Biological structure and function emerge from sc aling \nunsupervised learning to 250 million protein sequences,\" Proceedings of \nthe National Academy of Sciences , vol. 118, no. 15, Apr. 2021, Art. no. \ne2016239118, https://doi.org/10.1073/pnas.2016239118. \n[11]  A. Elnaggar et al. , \"ProtTrans: Toward Understanding the Language of \nLife Through Self-Supervised Learning,\" IEEE Transactions on Pattern \nAnalysis and Machine Intelligence , vol. 44, no. 10, pp. 7112–7127, Jul. \n2022, https://doi.org/10.1109/TPAMI.2021.3095381. \n[12]  Z. Lin et al. , \"Language models of protein sequences at the scal e of \nevolution enable accurate structure prediction.\" bi oRxiv, Jul. 21, 2022, \nhttps://doi.org/10.1101/2022.07.20.500902. \n[13]  B. Ahmed, G. Ali, A. Hussain, A. Baseer, and J. Ahm ed, \"Analysis of \nText Feature Extractors using Deep Learning on Fake  News,\" \nEngineering, Technology & Applied Science Research , vol. 11, no. 2, \npp. 7001–7005, Apr. 2021, https://doi.org/10.48084/etasr.4069. \n[14]  J. Singh, T. Litfin, J. Singh, K. Paliwal, and Y. Z hou, \"SPOT-Contact-\nLM: improving single-sequence-based prediction of protein contact map \nusing a transformer language model,\" Bioinformatics , vol. 38, no. 7, pp. \n1888–1894, Mar. 2022, https://doi.org/10.1093/bioinformatics/btac053. \n[15]  H. Stark, C. Dallago, M. Heinzinger, and B. Rost, \" Light attention \npredicts protein location from the language of life ,\" Bioinformatics \nAdvances , vol. 1, no. 1, Jan. 2021, Art. no. vbab035, https ://doi.org/ \n10.1093/bioadv/vbab035. \n[16]  S. Pokharel, P. Pratyush, M. Heinzinger, R. H. Newm an, and D. B. Kc, \n\"Improving protein succinylation sites prediction u sing embeddings \nfrom protein language model,\" Scientific Reports , vol. 12, no. 1, Oct. \n2022, Art. no. 16933, https://doi.org/10.1038/s41598-022-21366-2. \n[17]  A. Villegas-Morcillo, A. M. Gomez, and V. Sanchez, \"An analysis of \nprotein language model embeddings for fold predicti on,\" Briefings in \nBioinformatics , vol. 23, no. 3, May 2022, Art. no. bbac142, \nhttps://doi.org/10.1093/bib/bbac142. \n[18]  M. H. Hoie et al. , \"NetSurfP-3.0: accurate and fast prediction of pr otein \nstructural features by protein language models and deep learning,\" \nNucleic Acids Research , vol. 50, no. W1, pp. W510–W515, Jul. 2022, \nhttps://doi.org/10.1093/nar/gkac439. \n[19]  J. Singh, K. Paliwal, T. Litfin, J. Singh, and Y. Z hou, \"Reaching \nalignment-profile-based accuracy in predicting prot ein secondary and \ntertiary structural properties without alignment,\" Scientific Reports , vol. \n12, no. 1, May 2022, Art. no. 7607, https://doi.org /10.1038/s41598-022-\n11684-w. \n[20]  M. Levitt and C. Chothia, \"Structural patterns in g lobular proteins,\" \nNature , vol. 261, no. 5561, pp. 552–558, Jun. 1976, https ://doi.org/ \n10.1038/261552a0. \n[21]  P. Kumar, S. Bankapur, and N. Patil, \"An enhanced p rotein secondary \nstructure prediction using deep learning framework on hybrid profile \nbased features,\" Applied Soft Computing , vol. 86, Jan. 2020, Art. no. \n105926, https://doi.org/10.1016/j.asoc.2019.105926. \n[22]  J. Selbig, T. Mevissen, and T. Lengauer, \"Decision tree-based formation \nof consensus protein secondary structure prediction ,\" Bioinformatics , \nvol. 15, no. 12, pp. 1039–1046, Dec. 1999, https:// doi.org/10.1093/ \nbioinformatics/15.12.1039. \n[23]  B. Yang, Q. Wu, Z. Ying, and H. Sui, \"Predicting pr otein secondary \nstructure using a mixed-modal SVM method in a compo und pyramid \nmodel,\" Knowledge-Based Systems , vol. 24, no. 2, pp. 304-313, Mar. \n2011, https://doi.org/10.1016/j.knosys.2010.10.002. \n[24]  M. H. Zangooei and S. Jalili, \"PSSP with dynamic we ighted kernel \nfusion based on SVM-PHGS,\" Knowledge-Based Systems , vol. 27, pp. \n424–442, Mar. 2012, https://doi.org/10.1016/j.knosys.2011.11.002. \n[25]  Z. Aydin, Y. Altunbasak, and M. Borodovsky, \"Protei n secondary \nstructure prediction for a single-sequence using hi dden semi-Markov \nmodels,\" BMC Bioinformatics , vol. 7, no. 1, Mar. 2006, Art. no. 178, \nhttps://doi.org/10.1186/1471-2105-7-178. \n[26]  J. Martin, J.-F. Gibrat, and F. Rodolphe, \"Analysis  of an optimal hidden \nMarkov model for secondary structure prediction,\" BMC Structural \nBiology , vol. 6, no. 1, Dec. 2006, Art. no. 25, https://do i.org/ \n10.1186/1472-6807-6-25. \n[27]  W. Yang, K. Wang, and W. Zuo, \"Prediction of protei n secondary \nstructure using large margin nearest neighbour clas sification,\" \nInternational Journal of Bioinformatics Research an d Applications , vol. \n9, no. 2, pp. 207–219, Jan. 2013, https://doi.org/1 0.1504/IJBRA.2013. \n052445. \n[28]  A. Drozdetskiy, C. Cole, J. Procter, and G. J. Bart on, \"JPred4: a protein \nsecondary structure prediction server,\" Nucleic Acids Research , vol. 43, \nno. W1, pp. W389–W394, Jul. 2015, https://doi.org/10.1093/nar/gkv332. \n[29]  D. W. A. Buchan, S. M. Ward, A. E. Lobley, T. C. O.  Nugent, K. \nBryson, and D. T. Jones, \"Protein annotation and mo delling servers at \nUniversity College London,\" Nucleic Acids Research , vol. 38, no. \nsuppl_2, pp. W563–W568, Jul. 2010, https://doi.org/ 10.1093/ \nnar/gkq427. \n[30]  Z. Li and Y. Yu, \"Protein Secondary Structure Predi ction Using \nCascaded Convolutional and Recurrent Neural Network s.\" arXiv, Apr. \n25, 2016, https://doi.org/10.48550/arXiv.1604.07176. \n[31]  A. Busia and N. Jaitly, \"Next-Step Conditioned Deep  Convolutional \nNeural Networks Improve Protein Secondary Structure  Prediction.\" \narXiv, Feb. 13, 2017, https://doi.org/10.48550/arXiv.1702.03865. \n[32]  R. Heffernan, Y. Yang, K. Paliwal, and Y. Zhou, \"Ca pturing non-local \ninteractions by long short-term memory bidirectiona l recurrent neural \nnetworks for improving prediction of protein second ary structure, \nbackbone angles, contact numbers and solvent access ibility,\" \nEngineering, Technology & Applied Science Research  Vol. 14, No. 2, 2024, 13124-13132  13132   \n \nwww.etasr.com Kazm et al.: Transformer Encoder with  Protein Language Model for Protein Secondary Structure … \n \nBioinformatics , vol. 33, no. 18, pp. 2842–2849, Sep. 2017, \nhttps://doi.org/10.1093/bioinformatics/btx218. \n[33]  Y. Guo, W. Li, B. Wang, H. Liu, and D. Zhou, \"DeepA CLSTM: deep \nasymmetric convolutional long short-term memory neu ral models for \nprotein secondary structure prediction,\" BMC Bioinformatics , vol. 20, \nno. 1, Jun. 2019, Art. no. 341, https://doi.org/10. 1186/s12859-019-2940-\n0. \n[34]  C. Fang, Y. Shang, and D. Xu, \"MUFOLD-SS: New deep inception-\ninside-inception networks for protein secondary str ucture prediction,\" \nProteins: Structure, Function, and Bioinformatics , vol. 86, no. 5, pp. \n592–598, 2018, https://doi.org/10.1002/prot.25487. \n[35]  M. R. Uddin, S. Mahbub, M. S. Rahman, and M. S. Bay zid, \"SAINT: \nself-attention augmented inception-inside-inception  network improves \nprotein secondary structure prediction,\" Bioinformatics , vol. 36, no. 17, \npp. 4599–4608, Nov. 2020, https://doi.org/10.1093/b ioinformatics/ \nbtaa531. \n[36]  J. Hanson, K. Paliwal, T. Litfin, Y. Yang, and Y. Z hou, \"Improving \nprediction of protein secondary structure, backbone  angles, solvent \naccessibility and contact numbers by using predicte d contact maps and \nan ensemble of recurrent and residual convolutional  neural networks,\" \nBioinformatics , vol. 35, no. 14, pp. 2403–2410, Jul. 2019, \nhttps://doi.org/10.1093/bioinformatics/bty1006. \n[37]  M. S. Klausen et al. , \"NetSurfP-2.0: Improved prediction of protein \nstructural features by integrated deep learning,\" Proteins: Structure, \nFunction, and Bioinformatics , vol. 87, no. 6, pp. 520–527, 2019, \nhttps://doi.org/10.1002/prot.25674. \n[38]  Uzma, U. Manzoor, and Z. Halim, \"Protein encoder: A n autoencoder-\nbased ensemble feature selection scheme to predict protein secondary \nstructure,\" Expert Systems with Applications , vol. 213, Mar. 2023, Art. \nno. 119081, https://doi.org/10.1016/j.eswa.2022.119081. \n[39]  A. Elnaggar et al. , \"Ankh ☥ : Optimized Protein Language Model \nUnlocks General-Purpose Modelling.\" bioRxiv, Jan. 1 8, 2023, \nhttps://doi.org/10.1101/2023.01.16.524265. \n[40]  T. S. Mian, \"Evaluation of Stock Closing Prices usi ng Transformer \nLearning,\" Engineering, Technology & Applied Science Research , vol. \n13, no. 5, pp. 11635–11642, Oct. 2023, https://doi. org/10.48084/ \netasr.6017. \n[41]  M. Steinegger and J. Soding, \"Clustering huge prote in sequence sets in \nlinear time,\" Nature Communications , vol. 9, no. 1, Jun. 2018, Art. no. \n2542, https://doi.org/10.1038/s41467-018-04964-5. \n[42]  B. E. Suzek, Y. Wang, H. Huang, P. B. McGarvey, and  C. H. Wu, \n\"UniRef clusters: a comprehensive and scalable alte rnative for \nimproving sequence similarity searches,\" Bioinformatics , vol. 31, no. 6, \npp. 926–932, Mar. 2015, https://doi.org/10.1093/bioinformatics/btu739. \n[43]  A. Vaswani et al. , \"Attention is All you Need,\" in 31st Conference on \nNeural Information Processing Systems , Long Beach, CA, USA, Dec. \n2017, vol. 30, pp. 1–15. \n[44]  A. Zemla, C. Venclovas, K. Fidelis, and B. Rost, \"A  modified definition \nof Sov, a segment-based measure for protein seconda ry structure \nprediction assessment,\" Proteins: Structure, Function, and \nBioinformatics , vol. 34, no. 2, pp. 220–223, 1999, https://doi.or g/ \n10.1002/(SICI)1097-0134(19990201)34:2<220::AID-PROT7>3.0.CO;2-\nK. \n[45]  I. Drori et al. , \"High Quality Prediction of Protein Q8 Secondary \nStructure by Diverse Neural Network Architectures.\"  arXiv, Nov. 17, \n2018, https://doi.org/10.48550/arXiv.1811.07143. \n "
}