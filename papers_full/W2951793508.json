{
  "title": "Scaling Recurrent Neural Network Language Models",
  "url": "https://openalex.org/W2951793508",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4305102854",
      "name": "Williams, Will",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4305102855",
      "name": "Prasad, Niranjani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4305102856",
      "name": "Mrva, David",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4305102857",
      "name": "Ash, Tom",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2963374451",
      "name": "Robinson, Tony",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2253807446",
    "https://openalex.org/W2271177914",
    "https://openalex.org/W1926502259",
    "https://openalex.org/W2117278770",
    "https://openalex.org/W196214544",
    "https://openalex.org/W2251502575",
    "https://openalex.org/W2008398646",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2134800885",
    "https://openalex.org/W2474824677"
  ],
  "abstract": "This paper investigates the scaling properties of Recurrent Neural Network Language Models (RNNLMs). We discuss how to train very large RNNs on GPUs and address the questions of how RNNLMs scale with respect to model size, training-set size, computational costs and memory. Our analysis shows that despite being more costly to train, RNNLMs obtain much lower perplexities on standard benchmarks than n-gram models. We train the largest known RNNs and present relative word error rates gains of 18% on an ASR task. We also present the new lowest perplexities on the recently released billion word language modelling benchmark, 1 BLEU point gain on machine translation and a 17% relative hit rate gain in word prediction.",
  "full_text": "SCALING RECURRENT NEURAL NETWORK LANGUAGE MODELS\nWill Williams, Niranjani Prasad, David Mrva, Tom Ash, Tony Robinson\nCantab Research, Cambridge, UK\n{willw,tonyr}@cantabResearch.com\nABSTRACT\nThis paper investigates the scaling properties of Recurrent Neu-\nral Network Language Models (RNNLMs). We discuss how\nto train very large RNNs on GPUs and address the questions\nof how RNNLMs scale with respect to model size, training-set\nsize, computational costs and memory. Our analysis shows that\ndespite being more costly to train, RNNLMs obtain much lower\nperplexities on standard benchmarks than n-gram models. We\ntrain the largest known RNNs and present relative word error\nrates gains of 18% on an ASR task. We also present the new\nlowest perplexities on the recently released billion word lan-\nguage modelling benchmark, 1 BLEU point gain on machine\ntranslation and a 17% relative hit rate gain in word prediction.\nIndex Terms— recurrent neural network, language mod-\nelling, GPU, speech recognition, RNNLM\n1. INTRODUCTION\nStatistical language models form a crucial component of many\napplications such as automatic speech recognition (ASR), ma-\nchine translation (MT) and prediction for mobile phone text in-\nput. One such class of models, Recurrent Neural Network Lan-\nguage Models (RNNLMs), provide a rich and powerful way to\nmodel sequential language data. Despite an initial ﬂurry of in-\nterest in RNNs in the early ’90s for acoustic modelling [1, 2,\n3], the computational cost and memory overheads of training\nlarge RNNs proved prohibitive. Since the earliest days of large\nvocabulary speech recognition, n-gram language models have\nbeen the dominant paradigm. However, recent work by Mikolov\n[4] on RNNLMs has shown that modestly sized RNNLMs can\nnow be trained and have been shown to be competitive with n-\ngrams. Mikolov trained RNNLMs with 800 hidden state units;\nGoogle’s language modelling benchmark [5] subsequently es-\ntablished baseline RNNLM results with 1024 hidden state units.\nMany of the most recent ASR evaluations involve RNNLMs,\nhighlighting their widespread popularity.\nAlthough RNNs have replaced n-grams as state-of-the-art,\nthe question remains whether these architectures will scale.\nSingle CPU-based implementations have suffered from compu-\ntational limitations and have been unable to scale to the large\nnumber of parameters now commonplace in the neural net lit-\nerature. Concretely, we address the questions of how RNNLMs\nscale with respect to model size, training-set size, processing\npower and memory. Our primary performance metric here is\nThis paper summarises the results of SMART award ”Large scale neural\nnetwork language models” which ran from March 2013 to August 2014.\nperplexity and therefore all discussion on performance will\nrelate speciﬁcally to the ability to reduce perplexity. The re-\nsults presented here show the largest reductions in perplexity\nreported so far over KN 5-grams.\n2. DATA SETS\n2.1. Training Data\nThe main training corpus we use throughout this paper is an in-\nternally collated and processed data set. All data predates Nov.\n2013 and totals approximately 8bn words, comprising:\n1. [880m] Spoken News: Derived from transcripts of radio\nand news shows.\n2. [1.7bn] Wikipedia: Copy of Wikipedia articles.\n3. [6.1bn] Written News: Derived from a web crawl of\nmany popular news sites.\nWe used a rolling buffer of minimum size of 140 characters\nsplit on sentences to deduplicate the corpora. We used multiple\nhash functions to store past occurrences efﬁciently and excluded\na negligibly small proportion of text due to false positives. We\nthen put the corpus through a typical ASR normalisation and\ntokenisation pipeline - expanding out numbers into digit strings,\nremoving most punctuation, casting to lower case and inserting\nsentence markers.\n2.2. Test Data\nUnless otherwise stated, all our testing was done on a standard\ntest benchmark: TED test data IWSLT14.SLT.tst2010 1. We\nchose this out-of-domain test set to exclude any possibility of\ntext overlap between training and test sets, and also because of\nthe availability of a publicly available KALDI recipe2.\n2.3. Entropy Filtering\nTo ﬁlter our corpus we used cross-entropy difference scoring\n[6, 7]. Typically, sentences are used as the natural choice for\nsegments in this style of ﬁltering. However, for the compos-\nite corpus, we wanted to maintain between-sentence context\nfor the beneﬁt of the RNNLM training, and so did not want\nto ﬁlter our corpus on a sentence by sentence basis. We in-\nstead implemented a rolling-buffer solution: a cross-entropy\ndifference score was calculated across rolling batches of 16 sen-\ntences. If a sentence was ever part of a rolling buffer with\n1http://hltshare.fbk.eu/IWSLT2014/IWSLT14.SLT.\ntst2010.en-fr.en.tgz\n2Kaldi Revision 4084, recipe: http://sourceforge.net/p/\nkaldi/code/HEAD/tree/trunk/egs/tedlium/s5/\narXiv:1502.00512v1  [cs.CL]  2 Feb 2015\ncross-entropy difference below the threshold, it was maintained\nin the output corpus. This led to output that was more likely\nto be drawn from consecutive sentences, maintaining between-\nsentence information. The entropy threshold was then set at\nempirically determined levels to produce ﬁltered corpora of the\ndesired size, i.e. 1\n2 , 1\n4 etc. of the original corpus size, as de-\ntermined by word count. For typical in-domain data we used\nIWSLT13.ASR.train.en3.\n3. SCALING PROPERTIES OF N-GRAMS\nn-gram language models have maintained their dominance in\nstatistical language modelling largely due to their simplicity and\nease of computation. Although n-grams can be modelled and\nqueried comparatively fast, they exhibit diminishing gains when\nbuilt on more data.\n108 109 1010 1011 1012 1013\n100\n125\n150\n175\nTraining Words\nPerplexity\n5-gram-RS\nFig. 1. Effect of increasing training data size on a randomly\nselected (RS) subsets of our training corpus with a 5-gram. The\ndashed line is extrapolation.\nn-grams can be computed on larger data sets more easily\nthan RNNLMs but as the extrapolation in Figure 1 indicates,\neach order of magnitude increase in the training data above\n1012 words gives a reduction in perplexity of less than 6%. The\nlargest current text corpora such as Google n-grams4 and Com-\nmonCrawl [8] are about size 1012. The asymptote of an ex-\nponential curve ﬁtted to Figure 1 is approximately perplexity\n73; these values represent a hard limit on the performance of\n5-grams on this test set. n-grams also scale very poorly with\nrespect to memory. At 8bn words the KN 5-gram already takes\nup 362GB in ARPA format and 69GB in KenLM [9] binary\ntrie format - already impractically large for current commercial\nASR. In comparison to RNNLMs, n-grams take up massively\nmore space for modest entropy improvements and therefore do\nnot scale well with respect to data size.\nAdditionally, increasing the order of then-gram model gives\nvanishingly small gains for anything above a 5-gram trained on\n3http://hltshare.fbk.eu/IWSLT2013/IWSLT13.ASR.\ntrain.en\n4http://googleresearch.blogspot.co.uk/2006/08/all-\nour-n-gram-are-belong-to-you.html\ncurrently available corpora. The number of parameters of an n-\ngram increases signiﬁcantly with its order, making attempts to\nincrease the order impractical.\n4. RNNLMS ON GPUS\n4.1. Background\nRecent attempts to train very large networks (on the order of\nbillions of parameters) have required many CPU cores to work\nin concert [5, 10]. High-end Graphics Processing Units (GPUs)\nrepresent a viable alternative, being both affordable and capable\nof extremely high computational throughput. GPUs have there-\nfore been one of the key elements in the resurgence of neural\nnetworks over the last decade. RNNLMs are highly amenable\nto parallelisation on GPUs due to their predominant reliance on\nlinear algebra operations (such as matrix multiplies) for which\nhighly optimised GPU-based libraries are publicly available.\nIn contrast to n-gram models, which simply count the oc-\ncurrences of particular combinations of words, RNNs learn a\ndistributed hidden representation for each context from which a\nprobability distribution over all words in a given vocabulary can\nbe obtained.\nWe use a standard RNN architecture [11] but dispense with\nbias units to maximise efﬁciency on the GPU and because, in\nour experience, they do not provide any practical beneﬁt. Re-\ncent work has improved our understanding of how to effec-\ntively train RNNs [12]. However, many of these improvements -\nsuch as optimisation and regularisation techniques - make large\nclaims on resources. Very large RNNs with a large number of\nhidden state units, nstate, can only be trained efﬁciently on cur-\nrent generation GPU hardware if one simpliﬁes both the archi-\ntecture and training algorithm as far as possible. We believe our\nsetup from 2013 in this paper gives better speedups than previ-\nously reported elsewhere [13, 14, 15, 16, 17].\n4.2. Implementation\nTo achieve high throughput and utilisation on GPUs we train a\nstandard RNN with stochastic gradient descent and rmsprop[18].\nWhere possible we use CUDA’s highly optimised linear algebra\nlibrary cuBLAS to make SGEMM calls. Where this wasn’t\npossible we wrote and optimised our own custom CUDA ker-\nnels. We use ﬂoats everywhere; the precision which doubles\noffer is unnecessary to learn good representations. We pack our\ninput data using a data offset scheme which indexes the input\ncorpus at a number of different points, denotednoffset. This en-\ntails managing noffset by minibatch size different hidden states\nthroughout training. Typically we use noffset 128, similar to\n[16]. Empirically, we found a minibatch size of ∼ 8-10 repre-\nsents a good trade-off between memory usage and perplexity.\nIt also allows us to marshal a large and very efﬁcient matrix\nmultiply to perform all the hidden state-to-state and gradient\ncalculations in one cuBLAS SGEMM operation. We use a very\nlarge rmsprop smoothing constant (0.9995). For the input-to-\nstate and state-to-output matrices we approximate the majority\n0.25 0.5 1 2 4 8\n60\n80\n100\n120\n140\n160\nTraining words (bn)\nPerplexity\n5-gram-RS\n5-gram-EF\nRNN-2048-RS\nRNN-2048-EF\nFig. 2. Performance of 5-grams against nstate 2048 RNNs with\nincreasing training data size. We test on Randomly Selected\n(RS) splits and Entropy Filtered (EF) splits of the 8bn corpus.\n128 256 512 1024 2048 4096 8192\n60\n70\n80\n90\n100\n108\nnstate\nPerplexity\nRNN\nRNN+5gram\nFig. 3. Scaling nstate trained on 1bn words of the entropy ﬁl-\ntered 8bn corpus. Dashed line is the 5-gram baseline.\nof the rmsprop values by averaging over nstate units such that\neach word has just one rmsprop value rather thann values. This\nalmost halves the total memory usage. We train using Noise\nContrastive Estimation [19]. When reporting perplexities we\nexplicitly normalise the output distribution. When performing\nlattice rescoring we ﬁnd empirically that the small amount of\nnormalisation noise does not signiﬁcantly change the accuracy,\nwhilst considerably faster than standard class based modelling.\n4.3. Analysis\nFigure 3 shows that as we scale nstate we observe a near lin-\near reduction in log perplexity with log training words. Given\nthat n-grams scale very poorly with respect to their order it\nis clear that, on a ﬁxed-size data set, RNNs scale much bet-\nter with model size. For network sizes over nstate 1024, our\nModel # Params\n[millions] Training Time Perplexity\nKN 5-gram 1,740 30m 66.9\nRNN - 128 16.4 6h 60.8\nRNN - 256 32.8 16h 57.3\nRNN - 512 65.8 1d2h 53.2\nRNN - 1024 132 2d2h 48.9\nRNN - 2048 266 4d7h 45.2\nRNN - 4096 541 14d5h 42.4\nTable 1. Perplexities on shard-0 (’news.en.heldout-00000-of-\n00050’) from [5].\nimplementations on an Nvidia GeForce GTX Titan give 100x\nspeedups against the baseline single core Mikolov implementa-\ntion5 on a 3.4GHz Intel i7 CPU. Despite much improved train-\ning times on the GPU, our larger RNNs take on the order of\ndays to train rather than hours which n-grams require. How-\never, more compute power will favour RNNs since largernstate\nRNNs can leverage the extra computation to scale the model\nsize with nstate - something which yields a much smaller per-\nformance gain for n-grams. With respect to scaling the training\nset size, Figure 2 shows that the nstate 2048 saturates (i.e. can\nnot make good use of more data) on 4bn words when trained on\na randomly chosen splits. We can, however, increase the model\nsize (i.e. nstate) to mitigate this problem - an approach which,\nas already discussed, is impractical for n-grams. With an nstate\n8192 RNN from Figure 3 we have already reduced perplexity\nto 57.5, which is below the 73 we believe to be the asymp-\ntotic minimum for a 5-gram on this task. It therefore seems\nunlikely that even a 5-gram with unlimited data and a further\n15% reduction from entropy ﬁltering could ever outperform an\nRNNLM. In addition, the nstate 8192 RNN corresponds to a\n48% reduction over the 5-gram. Despite being so much better\nin terms of perplexity on the same amount of data, the RNN uses\nonly 886M parameters - approximately half the number of the\n5-gram. Moreover, we can match the perplexity of the 5-gram\nwith an nstate 128 RNN which uses under 1% of the parame-\nters. In summary, although training time is much larger for an\nRNN and we have to increase nstate when scaling the training\nset size, RNNs make much better use of the additional data than\nn-grams and use far fewer parameters to do so.\n5. RESULTS\n5.1. Language Modelling\nWe present our RNNLM results on the recently released billion\nword language modelling benchmark[5]. The vocabulary size\nfor this task is very large (770K) - we therefore train the RNNs\non a much smaller 64k vocabulary and interpolate them with\na full size 5-gram to ﬁll in rare word probabilities. RNN per-\nplexities in Table 1 are interpolated with the KN 5-gram. The\nnstate 4096 RNN is larger in state size than those in [5] and\n5http://rnnlm.org/\nn/a 128 256 512 1024 2048 4096 8192\n8\n10\n12\n14\n16\nnstate\nWER %\nIWSLT-unseg\nIWSLT-seg\nen-US-unseg\nen-US-seg\nSM-unseg\nFig. 4. Rescoring Kaldi Lattices with RNNLMs. ‘n/a’ refers to\nrescoring with n-gram only and is the n-gram baseline.\nwe achieve a better perplexity (42.4) with that one single model\nthan the combination of interpolated models in [5] whilst also\nusing only 3% of the total parameters.\n5.2. ASR\nWe evaluated the RNNLMs by rescoring lattices on three differ-\nent systems, using the IWSLT14.SLT.tst2010 data both with and\nwithout the supplied segmentation. The ‘IWSLT’ system uses\nthe Kaldi TEDLIUM [20] recipe for acoustic models and lan-\nguage models built on [5]6. The nstate 2048 models overtrain on\nthe small entropy ﬁltered corpus. The ‘en-US’ system uses the\nsame framework but with the RNNLMs from Figure 3 and inter-\nnal acoustic models. The ‘SM’ system also uses the RNNLMs\nfrom Figure 3, but within the commercial service available at\nspeechmatics.com. Lattices were rescored using a highly\nefﬁcient internal lattice rescoring tool which operates in consid-\nerably less than real time. Over both the ‘en-US’ and ‘SM’ task\nwe see an average reduction in WER of 18% relative to rescor-\ning with the n-gram alone by using nstate 8192 RNNLMs.\n5.3. Machine Translation\nIn our MT experiments we rescored the WFST lattices from\nthe WMT13 EN-RU system developed at Cambridge University\nEngineering Department [21]. The evaluation measure com-\nmonly used in statistical machine translation, BLEU score, was\n32.34 with our baseline n-gram model and increased to 33.45\nwith an nstate 3520 RNNLM (bigger is better for BLEU scores).\nThe improvement of 1 BLEU point over the baseline demon-\nstrates the utility of RNNLMs in not just ASR but also statistical\nMT systems.\n6The 12.8% IWSLT result is a competitive baseline that complies with\nIWSLT rules, using freely redistributable sources and can be recreated from\nthe Kaldi TEDLIUM s5 recipe and http://cantabResearch.com/\ncantab-TEDLIUM.tar.bz2.\n101 102 103\n26\n28\n30\n32\n34\n36\n38\n101\nLM total size (mb)\nHit-Rate %\nRNN-1024s\nKatz 5-grams\nKN n-grams\nFig. 5. Hit rates of Katz n-grams with different prune thresh-\nolds, RNNs quantised to different numbers of bits per weight\nand KN n-grams of increasing order. 7-bit quantisation pro-\nduces an anomalously high hit-rate - we are unsure what is in-\nteracting to product this effect.\n5.4. Word prediction\nWord prediction involves predicting the next word in a string,\nwith performance determined by percentage of times the target\nword is in the top 3 words predicted (known as ‘hit-rate’). For\nthis task we train on 100M words from the BNC corpus [22]\nusing a vocabulary size of 10K and the RNN using a shortlist\nof 100 candidate words generated from a heavily pruned 2 MB\nn-gram. For our task of word prediction on mobile phones we\nhave tight memory restrictions and therefore choosenstate 1024\nas an appropriate RNN size. We compress the RNNs by in-\nserting layers with 512 units above and below the hidden state\nand then train end-to-end. Additionally, we tie the input-state\nand state-output weights as their transpose and quantise all the\nweights. At 10 MB the RNN achieves a 17% relative hit-rate\ngain over the Katz-5 n-gram, proving the utility of RNNs in\nmemory constrained settings.\n6. CONCLUSION\nWe have shown that large RNNLMs can be trained efﬁciently\non GPUs by exploiting data parallelism and minimising the\nnumber of extra parameters required during training. Such\nRNNs reduce the perplexity on standard benchmarks by over\n40% against 5-grams, whilst using a fraction of the parameters.\nWe believe RNNs now offer a lower perplexity than 5-grams for\nany amount of training data. In addition, we showed that state\nof the art ASR systems can be trained with Kaldi and high-end\nGPUs by rescoring with an RNNLM. Despite being both com-\npute and GPU memory bound, RNNLMs are comfortably ahead\nof n-grams at present. We believe that future developments in\ncompute power and memory capacity will further favour them.\n7. REFERENCES\n[1] Anthony J. Robinson, “An application of recurrent nets\nto phone probability estimation,” Neural Networks, IEEE\nTransactions on, vol. 5, no. 2, pp. 298–305, 1994.\n[2] Ronald J. Williams, “Complexity of exact gradient com-\nputation algorithms for recurrent neural networks,” Tech.\nRep., 1989.\n[3] Mike Schuster and Kuldip K. Paliwal, “Bidirectional re-\ncurrent neural networks,” Signal Processing, IEEE Trans-\nactions on, vol. 45, no. 11, pp. 2673–2681, 1997.\n[4] Tom ´aˇs Mikolov, Statistical language models based on\nneural networks, Ph.D. thesis, Brno University of Tech-\nnology, 2012.\n[5] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robinson,\n“One Billion Word Benchmark for measuring progress in\nstatistical language modeling,” .\n[6] Dietrich Klakow, “Selecting articles from the language\nmodel training corpus,” in Acoustics, Speech, and Signal\nProcessing, 2000. ICASSP’00. Proceedings. 2000 IEEE\nInternational Conference on. IEEE, 2000, vol. 3, pp.\n1695–1698.\n[7] Robert C. Moore and William Lewis, “Intelligent selec-\ntion of language model training data,” in Proceedings of\nthe ACL 2010 Conference Short Papers. Association for\nComputational Linguistics, 2010, pp. 220–224.\n[8] Christian Buck, Kenneth Heaﬁeld, and Bas van Ooyen,\n“N-gram counts and language models from the common\ncrawl,” LREC, 2014.\n[9] Kenneth Heaﬁeld, “KenLM: Faster and smaller language\nmodel queries,” in Proceedings of the Sixth Workshop on\nStatistical Machine Translation. Association for Compu-\ntational Linguistics, 2011, pp. 187–197.\n[10] Quoc V Le, Rajat Monga, Matthieu Devin, Kai Chen,\nGreg S. Corrado, Jeff Dean, and Andrew Y . Ng, “Build-\ning high-level features using large scale unsupervised\nlearning,” in Acoustics, Speech and Signal Processing\n(ICASSP), 2013 IEEE International Conference on. IEEE,\n2013, pp. 8595–8598.\n[11] Tom ´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur, “Recurrent neural net-\nwork based language model,” in Proceedings of the 11th\nAnnual Conference of the International Speech Commu-\nnication Association (INTERSPEECH), 2010, pp. 1045–\n1048.\n[12] Yoshua Bengio, Nicolas Boulanger-Lewandowski, and\nRazvan Pascanu, “Advances in optimizing recurrent net-\nworks,” in Acoustics, Speech and Signal Processing\n(ICASSP), 2013 IEEE International Conference on. IEEE,\n2013, pp. 8624–8628.\n[13] Tomas Mikolov, Stefan Kombrink, Anoop Deoras, Lukar\nBurget, and J Cernocky, “Rnnlm-recurrent neural network\nlanguage modeling toolkit,” .\n[14] Ilya Sutskever, James Martens, and Geoffrey E Hinton,\n“Generating text with recurrent neural networks,” in Pro-\nceedings of the 28th International Conference on Machine\nLearning (ICML-11), 2011, pp. 1017–1024.\n[15] Zhiheng Huang, Geoffrey Zweig, Michael Levit, Benoit\nDumoulin, Barlas Oguz, and Shawn Chang, “Accelerat-\ning recurrent neural network training via two stage classes\nand parallelization,” inAutomatic Speech Recognition and\nUnderstanding (ASRU), 2013 IEEE Workshop on. IEEE,\n2013, pp. 326–331.\n[16] Xie Chen, Yongqiang Wang, Xunying Liu, Mark JF Gales,\nand Philip C Woodland, “Efﬁcient GPU-based training of\nrecurrent neural network language models using spliced\nsentence bunch,” Proc. ISCA Interspeech, 2014.\n[17] Boxun Li, Erjin Zhou, Bo Huang, Jiayi Duan, Yu Wang,\nNingyi Xu, Jiaxing Zhang, and Huazhong Yang, “Large\nscale recurrent neural network on gpu,” in Neural Net-\nworks (IJCNN), 2014 International Joint Conference on.\nIEEE, 2014, pp. 4062–4069.\n[18] T. Tieleman and G. Hinton, “Lecture 6.5-rmsprop: Divide\nthe gradient by a running average of its recent magnitude,”\nCOURSERA: Neural Networks for Machine Learning, vol.\n4, 2012.\n[19] Andriy Mnih and Koray Kavukcuoglu, “Learning word\nembeddings efﬁciently with noise-contrastive estimation,”\nin Advances in Neural Information Processing Systems,\n2013, pp. 2265–2273.\n[20] Anthony Rousseau, Paul Del ´eglise, and Yannick Est `eve,\n“TED-LIUM: an Automatic Speech Recognition dedi-\ncated corpus,” in LREC, 2012, pp. 125–129.\n[21] J. Pino, A. Waite, T. Xiao, A. de Gispert, F. Flego, and\nW. Byrne, “The University of Cambridge Russian-English\nsystem at WMT13,” in Proceedings of the Eighth Work-\nshop on Statistical Machine Translation, 2013, pp. 198–\n203.\n[22] BNC Consortium et al., “The British National Corpus,\nversion 3 (BNC XML Edition), distributed by Oxford Uni-\nversity Computing Services on behalf of the BNC Consor-\ntium,” 2007.",
  "topic": "Language model",
  "concepts": [
    {
      "name": "Language model",
      "score": 0.8204054832458496
    },
    {
      "name": "Computer science",
      "score": 0.7515060901641846
    },
    {
      "name": "Recurrent neural network",
      "score": 0.7243649959564209
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.703660249710083
    },
    {
      "name": "Scaling",
      "score": 0.6798985004425049
    },
    {
      "name": "Machine translation",
      "score": 0.6350423097610474
    },
    {
      "name": "Word (group theory)",
      "score": 0.6202611327171326
    },
    {
      "name": "Task (project management)",
      "score": 0.583820104598999
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5654102563858032
    },
    {
      "name": "Word error rate",
      "score": 0.5588722825050354
    },
    {
      "name": "Natural language processing",
      "score": 0.5061885714530945
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.48016634583473206
    },
    {
      "name": "Artificial neural network",
      "score": 0.47397321462631226
    },
    {
      "name": "Mathematics",
      "score": 0.09838199615478516
    },
    {
      "name": "Engineering",
      "score": 0.06571066379547119
    },
    {
      "name": "Programming language",
      "score": 0.06221780180931091
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ]
}