{
  "title": "ChIP-GPT: a managed large language model for robust data extraction from biomedical database records",
  "url": "https://openalex.org/W4391544525",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A334184517",
      "name": "Olivier Cinquin",
      "affiliations": [
        "University of California, Irvine"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1605960500",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W4385573954",
    "https://openalex.org/W4385573087",
    "https://openalex.org/W2165446840",
    "https://openalex.org/W6713154993",
    "https://openalex.org/W2900310474",
    "https://openalex.org/W4221102522",
    "https://openalex.org/W2118258530",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W2930139824",
    "https://openalex.org/W3110528765",
    "https://openalex.org/W6636248600",
    "https://openalex.org/W6760761205",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385574038",
    "https://openalex.org/W2949555952",
    "https://openalex.org/W4286985375",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2930291217",
    "https://openalex.org/W2401834729",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4322718191"
  ],
  "abstract": "Abstract Increasing volumes of biomedical data are amassing in databases. Large-scale analyses of these data have wide-ranging applications in biology and medicine. Such analyses require tools to characterize and process entries at scale. However, existing tools, mainly centered on extracting predefined fields, often fail to comprehensively process database entries or correct evident errors—a task humans can easily perform. These tools also lack the ability to reason like domain experts, hindering their robustness and analytical depth. Recent advances with large language models (LLMs) provide a fundamentally new way to query databases. But while a tool such as ChatGPT is adept at answering questions about manually input records, challenges arise when scaling up this process. First, interactions with the LLM need to be automated. Second, limitations on input length may require a record pruning or summarization pre-processing step. Third, to behave reliably as desired, the LLM needs either well-designed, short, ‘few-shot’ examples, or fine-tuning based on a larger set of well-curated examples. Here, we report ChIP-GPT, based on fine-tuning of the generative pre-trained transformer (GPT) model Llama and on a program prompting the model iteratively and handling its generation of answer text. This model is designed to extract metadata from the Sequence Read Archive, emphasizing the identification of chromatin immunoprecipitation (ChIP) targets and cell lines. When trained with 100 examples, ChIP-GPT demonstrates 90–94% accuracy. Notably, it can seamlessly extract data from records with typos or absent field labels. Our proposed method is easily adaptable to customized questions and different databases.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.843374490737915
    },
    {
      "name": "Automatic summarization",
      "score": 0.6414506435394287
    },
    {
      "name": "Metadata",
      "score": 0.5650620460510254
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.49330297112464905
    },
    {
      "name": "Language model",
      "score": 0.4665284752845764
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4552004933357239
    },
    {
      "name": "Data mining",
      "score": 0.43026190996170044
    },
    {
      "name": "Information retrieval",
      "score": 0.4142891764640808
    },
    {
      "name": "Machine learning",
      "score": 0.4003419876098633
    },
    {
      "name": "Database",
      "score": 0.36224183440208435
    },
    {
      "name": "Natural language processing",
      "score": 0.34683549404144287
    },
    {
      "name": "World Wide Web",
      "score": 0.13172829151153564
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ]
}