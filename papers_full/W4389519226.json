{
    "title": "LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models",
    "url": "https://openalex.org/W4389519226",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2165474851",
            "name": "Huiqiang Jiang",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        },
        {
            "id": "https://openalex.org/A2162740970",
            "name": "Qianhui Wu",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        },
        {
            "id": "https://openalex.org/A3143182302",
            "name": "Chin-Yew Lin",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        },
        {
            "id": "https://openalex.org/A2094451174",
            "name": "Yuqing Yang",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        },
        {
            "id": "https://openalex.org/A2121935375",
            "name": "Lili Qiu",
            "affiliations": [
                "Microsoft (Finland)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4309591680",
        "https://openalex.org/W4205725534",
        "https://openalex.org/W4309386164",
        "https://openalex.org/W4302011807",
        "https://openalex.org/W4285199320",
        "https://openalex.org/W4367000321",
        "https://openalex.org/W4313484599",
        "https://openalex.org/W4378770449",
        "https://openalex.org/W4287901267",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W4384392961",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W4367628408",
        "https://openalex.org/W4390493562",
        "https://openalex.org/W4385571157",
        "https://openalex.org/W4306886919",
        "https://openalex.org/W4223945106",
        "https://openalex.org/W4389524473",
        "https://openalex.org/W3168124404",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4385567182",
        "https://openalex.org/W3174708387",
        "https://openalex.org/W4367000491",
        "https://openalex.org/W4310632189",
        "https://openalex.org/W2046419776",
        "https://openalex.org/W4286892945",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W4386908184",
        "https://openalex.org/W4366330736",
        "https://openalex.org/W4385571597",
        "https://openalex.org/W2079145130",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W1519253855",
        "https://openalex.org/W4364385701"
    ],
    "abstract": "Large language models (LLMs) have been applied in various applications due to their astonishing capabilities. With advancements in technologies such as chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of tokens. To accelerate model inference and reduce cost, this paper presents LLMLingua, a coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models. We conduct experiments and analysis over four datasets from different scenarios, i.e., GSM8K, BBH, ShareGPT, and Arxiv-March23; showing that the proposed approach yields state-of-the-art performance and allows for up to 20x compression with little performance loss.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13358â€“13376\nDecember 6-10, 2023 Â©2023 Association for Computational Linguistics\nLLMLingua: Compressing Prompts for Accelerated Inference\nof Large Language Models\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, Lili Qiu\nMicrosoft Corporation\n{hjiang, qianhuiwu, cyl, yuqing.yang, liliqiu}@microsoft.com\nAbstract\nLarge language models (LLMs) have been ap-\nplied in various applications due to their as-\ntonishing capabilities. With advancements in\ntechnologies such as chain-of-thought (CoT)\nprompting and in-context learning (ICL), the\nprompts fed to LLMs are becoming increas-\ningly lengthy, even exceeding tens of thousands\nof tokens. To accelerate model inference and\nreduce cost, this paper presents LLMLingua,\na coarse-to-fine prompt compression method\nthat involves a budget controller to maintain\nsemantic integrity under high compression ra-\ntios, a token-level iterative compression algo-\nrithm to better model the interdependence be-\ntween compressed contents, and an instruc-\ntion tuning based method for distribution align-\nment between language models. We conduct\nexperiments and analysis over four datasets\nfrom different scenarios, i.e., GSM8K, BBH,\nShareGPT, and Arxiv-March23; showing that\nthe proposed approach yields state-of-the-art\nperformance and allows for up to 20x compres-\nsion with little performance loss.1\n1 Introduction\nThe widespread adoption of ChatGPT has trans-\nformed numerous scenarios by harnessing the pow-\nerful generalization and reasoning capabilities of\nlarge language models (LLMs). In practical ap-\nplications, crafting suitable prompts is crucial\nand usually involves techniques such as chain-of-\nthought, in-context learning, and retrieving related\ndocuments or historical conversations (Wei et al.,\n2022; Chase, 2022). While these methods can\nelicit highly effective generations by activating\nLLMsâ€™ domain-specific knowledge, they often re-\nquire longer prompts. Therefore, striking a balance\nbetween the massive computational demands of\nLLMs and the need for longer prompts has become\nan urgent issue. Some studies attempt to accelerate\nmodel inference by modifying the parameters of\n1Our code is available at https://aka.ms/LLMLingua.\nLLMs through quantization (Dettmers et al., 2022;\nXiao et al., 2023), compression (Frantar and Alis-\ntarh, 2023), etc. However, these approaches may\nbe not suitable when the LLMs can be accessed via\nAPIs only.\nApproaches that attempt to reduce the length of\noriginal prompts while preserving essential infor-\nmation have emerged lately. These approaches are\ngrounded in the concept that natural language is\ninherently redundant (Shannon, 1951) and thus can\nbe compressed. Gilbert et al. (2023) also indicate\nthat LLMs can effectively reconstruct source code\nfrom compressed text descriptions while maintain-\ning a high level of functional accuracy. Therefore,\nwe follow this line of studies to compress a long\nprompt into a shorter one without any gradient flow\nthrough the LLMs to support applications based on\na larger range of LLMs.\nIn terms of information entropy, tokens with\nlower perplexity (PPL) contribute less to the over-\nall entropy gains of the language model. In other\nwords, removing tokens with lower perplexity has\na relatively minor impact on the LLMâ€™s compre-\nhension of the context. Motivated by this, Li\n(2023) propose Selective-Context, which first em-\nploys a small language model to compute the self-\ninformation of each lexical unit (such as sentences,\nphrases, or tokens) in original prompts, and then\ndrops the less informative content for prompt com-\npression. However, this method not only ignores\nthe interdependence between the compressed con-\ntents but also neglects the correspondence between\nthe LLM being targeted and the small language\nmodel used for prompt compression.\nThis paper proposes LLMLingua, a coarse-\nto-fine prompt compression method, to address\nthe aforementioned issues. Specifically, we first\npresent a budget controller to dynamically allo-\ncate different compression ratios to various com-\nponents in original prompts such as the instruction,\ndemonstrations, and the question, and meanwhile,\n13358\nperform coarse-grained, demonstration-level com-\npression to maintain semantic integrity under high\ncompression ratios. We further introduce a token-\nlevel iterative algorithm for fine-grained prompt\ncompression. Compared with Selective Context, it\ncan better preserve the key information within the\nprompt by taking into account the conditional de-\npendencies between tokens. Additionally, we pose\nthe challenge of distribution discrepancy between\nthe target LLM and the small language model used\nfor prompt compression, and further propose an\ninstruction tuning based method to align the distri-\nbution of both language models.\nWe validate the effectiveness of our approach on\nfour datasets from different domains, i.e., GSM8K\nand BBH for reasoning and ICL, ShareGPT for con-\nversation, and Arxiv-March23 for summarization.\nThe results show that our method yields state-of-\nthe-art performance across the board. Furthermore,\nwe conduct extensive experiments and discussions\nto analyze why our approach attains superior per-\nformance. To our best knowledge, we are the first\nto evaluate reasoning and ICL capabilities in the\ndomain of efficient LLMs.\n2 Related Work\n2.1 Efficient LLMs\nEfficient large language models have gained sig-\nnificant attention in recent research community,\nespecially with the growing prominence of Chat-\nGPT. Most of these methods aim to reduce the\ncosts of inference and fine-tuning by modifying the\nmodel parameters through quantization (Dettmers\net al., 2022; Frantar et al., 2023; Xiao et al., 2023),\ncompression (Frantar and Alistarh, 2023), instruct\ntuning (Taori et al., 2023; Chiang et al., 2023; Xu\net al., 2023), or delta tuning (Hu et al., 2022).\nA line of studies attempt to optimize inference\ncosts from the perspective of the input prompts.\nMotivated by the observation of the abundance of\nidentical text spans between the input and the gener-\nated result, Yang et al. (2023) directly copy tokens\nfrom prompts for decoding to accelerate the infer-\nence process of LLMs. Some approaches focus on\ncompressing prompts, specifically, learning special\ntokens via prompt tuning of LLMs to reduce the\nnumber of tokens to be processed during inference\n(Mu et al., 2023; Ge et al., 2022; Wingate et al.,\n2022; Chevalier et al., 2023; Ge et al., 2023). Un-\nfortunately, these methods are usually tailored to\nparticular tasks and some of them (Mu et al., 2023;\nChevalier et al., 2023) even require to fine-tune the\nwhole language model, which severely limits their\napplication scenarios. Furthermore, there are some\nstudies (Chase, 2022; Zhang et al., 2023) that at-\ntempt to utilize LLMs to summarize dialog or data,\nthereby forming memory and knowledge. How-\never, these approaches require multiple invocations\nof LLMs, which are quite costly.\nSome methods reduce the prompt length by se-\nlecting a subset of demonstrations. For example,\nZhou et al. (2023) introduces a reinforcement learn-\ning based algorithm to allocate a specific number\nof demonstrations for each question. Some other\nmethods focus on token pruning (Goyal et al., 2020;\nKim and Cho, 2021; Kim et al., 2022; Rao et al.,\n2021; Modarressi et al., 2022) and token merg-\ning (Bolya et al., 2023). However, these approaches\nare proposed for smaller models such as BERT, ViT.\nMoreover, they depend on fine-tuning the models\nor obtaining intermediate results during inference.\nThe most similar work to this paper is Selective-\nContext (Li, 2023), which evaluates the infor-\nmativeness of lexical units by computing self-\ninformation with a small language model, and\ndrops the less informative content for prompt com-\npression. This paper is inspired by Selective-\nContext and further proposes a coarse-to-fine\nframework to address its limitations.\n2.2 Out-of-Distribution (OoD) Detection\nRecently, a series of studies have been proposed\nfor unsupervised OoD detection. With only in-\ndistribution texts available for learning, these meth-\nods either fine-tune a pre-trained language model\n(Arora et al., 2021) or train a language model from\nscratch (Mai et al., 2022). Wu et al. (2023) analyze\nthe characteristics of these methods and leverage\nmulti-level knowledge distillation to integrate their\nstrengths while mitigating their limitations. Finally,\nperplexity output by the resulting language model\nis used as the indication of an example being OoD.\nThis paper also regards perplexity as a measure-\nment of how well a language model predicts a sam-\nple. In contrast to out-of-distribution detection,\nwhich identifies examples with high perplexities\nas indicative of unreliable predictions, we consider\ntokens with higher perplexity to be more influential\nduring the inference process of language models.\n2.3 LLMs as a Compressor\nRecently, many perspectives have interpreted\nlarge language models and unsupervised learn-\n13359\nOriginal Prompt LLMLingua\nCompressed Prompt\nInstruction: Follow the given examples \nand answer the question.\nDemonstration 1: Q: In a certain \nschool, 2/3 of the male students like to \nplay basketball, â€¦. What percent of the \npopulation of the school do not like to \nplay basketball if the ratio of the male to \nfemale students is 3:2 and there are \n1000 students? Let's think step by step\nThe students are divided into 3 + 2 = 5\nEach part represents 1000/5 = 200 \nstudents. So, there are 3 x 200 = 600 \nmales. And there are 2 x 200 = 400. \nâ€¦basketball is 520/1000 * 100 = 52. \nThe answer is 52.\nDemonstration 2:\nâ€¦\nDemonstration 8: Q: Sam bought a \ndozen boxes, each with 30 highlighter \npens inside,â€¦ The answer is 115.\nQuestion: Janetâ€™s ducks lay 16 eggs per \ndayâ€¦.. How much in dollars does she \nmake every day at the farmers' market?\nSmall \nModel\nBlack-box LLMs\n2366 tokens\n: Sam bought a dozen boxes each 30 \nhighl pens inside, $10 each. ... thelters \nseparately at the of three $2. much make \ntotal,\\nLets think step\\nbought boxes x0 \noflters\\nHe 2 3ters in\\nSam then boxes \n6lters/box 0ters\\nHe sold these boxes \n5\\nAfterelling these boxes there \n36030lters\\nese00 of three\\nsold groups2 \neach so made *2 $20 from\\nIn total, \nhe015\\nSince his he $ - $120 = $115 in \nprofit.\\nThe answer is 115\n117 tokens\nI  Budget   \nController\nII  Iterative T oken-\nLevel Prompt \nCompression\n0  Distribution \n    Alignment\nIII  Compressed \nPrompt Execution\nOriginal Prompt LLMLingua\nCompressed Prompt\nInstruction: Follow the given examples \nand answer the question.\nDemonstration 1: Q: In a certain \nschool, 2/3 of the male students like to \nplay basketball, â€¦. What percent of the \npopulation of the school do not like to \nplay basketball if the ratio of the male to \nfemale students is 3:2 and there are \n1000 students? Let's think step by step\nThe students are divided into 3 + 2 = 5\nEach part represents 1000/5 = 200 \nstudents. So, there are 3 x 200 = 600 \nmales. And there are 2 x 200 = 400. \nâ€¦basketball is 520/1000 * 100 = 52. \nThe answer is 52.\nDemonstration 2:\nâ€¦\nDemonstration 8: Q: Sam bought a \ndozen boxes, each with 30 highlighter \npens inside,â€¦ The answer is 115.\nQuestion: Janetâ€™s ducks lay 16 eggs per \ndayâ€¦.. How much in dollars does she \nmake every day at the farmers' market?\nSmall \nModel\nBlack-box LLMs\n2366 tokens\n: Sam bought a dozen boxes each 30 \nhighl pens inside, $10 each. ... thelters \nseparately at the of three $2. much make \ntotal,\\nLets think step\\nbought boxes x0 \noflters\\nHe 2 3ters in\\nSam then boxes \n6lters/box 0ters\\nHe sold these boxes \n5\\nAfterelling these boxes there \n36030lters\\nese00 of three\\nsold groups2 \neach so made *2 $20 from\\nIn total, \nhe015\\nSince his he $ - $120 = $115 in \nprofit.\\nThe answer is 115\n117 tokens\nI  Budget   \nController\nII  Iterative T oken-\nLevel Prompt \nCompression\n0  Distribution \n    Alignment\nIII  Compressed \nPrompt Execution\nFigure 1: Framework of the proposed approach LLMLingua.\ning as a kind of compressor for world knowl-\nedge (Sutskever, 2023; DelÃ©tang et al., 2023), by\nusing arithmetic coding (Rissanen, 1976; Pasco,\n1976). Our research can be viewed as an endeavor\nto further compress information within prompts by\ncapitalizing on the compression-like characteristics\nof large language models.\n3 Problem Formulation\nA prompt compression system is designed to gen-\nerate a compressed prompt Ëœx = {Ëœğ‘¥ğ‘–}Ëœğ¿\nğ‘–=1 from\na given original prompt x = (xins, xdems, xque),\nwhere xins = {ğ‘¥ins\nğ‘– }ğ¿ins\nğ‘–=1 , xdems = {ğ‘¥dems\nğ‘– }ğ¿dems\nğ‘–=1 , and\nxque = {ğ‘¥que\nğ‘– }ğ¿que\nğ‘–=1 denote the instruction, demon-\nstrations, and the question in the original prompt\nx. Ëœğ¿, ğ¿ins, ğ¿dems, and ğ¿que represent the numbers\nof tokens in Ëœx, xins, xdems, and xque, respectively.\nLet ğ¿ = ğ¿ins +ğ¿dems +ğ¿que denote the total se-\nquence length of x, the compression rate is defined\nas ğœ =Ëœğ¿/ğ¿, ğœ âˆˆ[0, 1], and the compression ratio\nis 1/ğœ. A smaller value of ğœ implies a lower infer-\nence cost, which is preferable. Let Ëœxğº represent\nthe LLM-generated results derived by Ëœxand xğº\ndenotes the tokens derived by x, the distribution of\nËœxğº is expected to be as similar to xğº as possible.\nThis can be formulated as:\nmin\nËœx,ğœ\nKL(ğ‘ƒ(Ëœxğº|Ëœx), ğ‘ƒ(xğº|x)), (1)\n4 Methodology\nIn this section, we elaborate on the proposed coarse-\nto-fine prompt compression approach, LLMLingua.\nFirst, we introduce a budget controller to dynam-\nically allocate different compression ratios to var-\nious components in prompts and meanwhile, per-\nform coarse-grained, demonstration-level compres-\nsion to maintain semantic integrity under high com-\npression ratios. Next, we describe the proposed it-\nerative prompt algorithm designed to retain knowl-\nedge from the prompt while compressing. Finally,\nwe introduce alignment to address the distribution\ngap between the small model and black-box large\nmodels. Figure 1 show the framework.\n4.1 Budget Controller\nThe budget controller here is designed to allocate\ndifferent budgets, i.e., compression ratio, to differ-\nent components in a prompt such as instructions,\ndemonstrations, and questions, at the sentence or\ndemonstration level. There are two considerations:\n(i) In general, the instruction and the question in\na prompt have a direct influence on the generated\nresults, as they should contain all the necessary\nknowledge to generate the following answer. On\nthe contrary, if there are multiple demonstrations\nin the original prompt, the conveyed information\nmay be redundant. Therefore, a tailored budget\ncontroller is required to allocate more budget (i.e.,\n13360\nAlgorithm 1 Pseudo code of Budget Controller.\nInput: A small language model Mğ‘ ; the original prompt\nx =(xins, xdems, xque).\n1: Set the selected demonstration set D=ğœ™.\n2: Get demonstration compression rate ğœdem by Eq.(2).\n3: Calculate the perplexity of each demonstration via Mğ‘ .\n4: Rank all demonstrations in descending order of their per-\nplexity as a list (xdem\n(1), ...,xdem\n(ğ‘)), where ğ‘ is the number\nof demonstrations, xdem\n(ğ‘–) is the ğ‘–-th demonstration.\n5: for ğ‘– =1 do\n6: if Ëœğ¿D> ğ‘˜Â·ğœdems ğ¿dems then\n7: Break.\n8: end if\n9: Append xdem\n(ğ‘–) to D.\n10: ğ‘– =ğ‘– +1\n11: end for\n12: Allocate remaining budget to xins and xque via Eq. (3).\nOutput: The subset of demonstrations Dobtained from\ncoarse-grained compression; Additional budget Î”ğœins,que for\nthe instruction and the question.\nsmaller compression ratios) for instructions and\nquestions, and less budget for demonstrations.\n(ii) When a high compression ratio is required,\ntoken-level dropout as in Li (2023) might make\nthe compressed prompts too trivial and thus lose\nvital information from the original prompt. Con-\nsequently, sentence-level dropout should be em-\nployed instead to preserve a certain degree of lin-\nguistic integrity. Especially in the case of multiple\nredundant demonstrations, we can even perform\ndemonstration-level control to meet the compres-\nsion requirement.\nAlgorithm 1 illustrates the overall procedure of\nthe budget controller.\nDerive compression ratio for demonstrations.\nWe first compute the compression rate for demon-\nstrations ğœdems according to the target overall com-\npression rate ğœ and the pre-defined compression\nrate for instructions and questions, i.e., ğœins and\nğœque, respectively.\nğœdems = ğœğ¿ âˆ’(ğœins ğ¿ins +ğœque ğ¿que)\nğ¿dems\n. (2)\nDemonstration-level prompt compression.\nWith the derived ğœdems for demonstrations, we\nthen perform a coarse-grained demonstration-level\nprompt compression: we construct D, a subset of\ndemonstrations from xdems.\nSpecifically, we first employ a small language\nmodel Mğ‘ , such as GPT-2 or LLaMA, to com-\npute the perplexity of each demonstration in xdems.\nThen, we select demonstrations in descending order\nof their perplexity values, until adding one more\nAlgorithm 2 Pseudo code of Iterative Token-level\nPrompt Compression (ITPC).\nInput: A small language model Mğ‘ ; the prompt from budget\ncontroller xâ€² = (xins, xD, xque); target compression rate ğœ,\nadjusted compression rate â–³ğœins,que.\n1: Set the selected token set T=ğœ™\n2: Get segment set S.\n3: for ğ‘– =1, 2, . . . , ğ‘šdo\n4: Get the conditional probabilities ğ‘(sğ‘–)via Eq.(5)\n5: Get the compression threshold ğ›¾ğ‘– with Eq. (6).\n6: Append the compressed token to Tvia Eq.(7).\n7: end for\n8: Concatenate all tokens in Tas Ëœx.\nOutput: The compressed prompt Ëœx.\ndemonstration to Dwill make the total number of\ntokens in Dexceed maximum tokens ğ‘˜ Â·ğœdems ğ¿dems,\nwhere ğ‘˜ is the granular control coefficient.\nAdjust compression ratios for instruction and\nquestion. After obtaining the coarse-grained\ncompression result D={ğ‘¥ğ‘–}\nËœğ¿D\nğ‘–=1, we allocate the re-\nmaining budget to the instruction and the question:\nÎ”ğœ = ğ‘˜ Â·ğœdems ğ¿dems âˆ’Ëœğ¿D\nğ¿ins +ğ¿que\n, (3)\nwhere Ëœğ¿Ddenote the total number of tokens in D.\n4.2 Iterative Token-level Prompt Compression\nUtilizing perplexity for prompt compression en-\ncounters the intrinsic limitation, i.e., the indepen-\ndence assumption, similar to the shortcomings of\nthe Mask Language Model (Yang et al., 2019) as:\nğ‘(Ëœx)=\nËœğ¿/âˆšï¸âˆšï¸‚oductdiâˆšï¸„âˆšï¸lay.ï£¶\nğ‘–=1\nğ‘(Ëœğ‘¥ğ‘–|Ëœğ‘¥<ğ‘–)\nâ‰ˆğ‘(xâ€²)=\nğ¿â€²\n/âˆšï¸âˆšï¸‚oductdiâˆšï¸„âˆšï¸lay.ï£¶\nğ‘–=1\nğ‘(ğ‘¥ğ‘–|Ëœğ‘¥<ğ‘–, ğ‘¥<ğ‘–),\n(4)\nwhere xâ€²=(xins, xD, xque)is the original prompt\nafter demonstration-level compression; xDis the\nconcatenation of all demonstrations in D; Ëœğ‘¥ is the\nfinal compressed prompt; Ëœğ‘¥<ğ‘– and ğ‘¥<ğ‘– denote the\npreserved and compressed tokens before the ğ‘–-th\ntoken ğ‘¥ğ‘–; ğ¿â€²and Ëœğ¿ denote the numbers of all tokens\nin xâ€²and Ëœx, respectively.\nHere we propose an iterative token-level prompt\ncompression (ITPC) algorithm to mitigate the inac-\ncuracy introduced by the conditional independence\nassumption. Algorithm 2 shows the pseudo codes.\nSpecifically, we first divide the target prompt xâ€²\ninto several segments S= {s1, s2, ...,sğ‘š}. And\n13361\nthen, we use the smaller model Mğ‘  to obtain the\nperplexity distribution of all segments. The com-\npressed prompt obtained from each segment is\nconcatenated to the subsequent segment, enabling\nmore accurate estimation of the conditional prob-\nability. The corresponding probability estimation\nfunction can be formulated as:\nğ‘(Ëœsğ‘—)=\n/âˆšï¸„ummationtext.ï£¶ğ‘—\nğ‘˜ Ëœğ¿ğ‘ ,ğ‘˜/âˆšï¸âˆšï¸‚oductdiâˆšï¸„âˆšï¸lay.ï£¶\nğ‘–=1\nğ‘(Ëœğ‘ ğ‘—,ğ‘–|Ëœğ‘ ğ‘—,<ğ‘–, Ëœs<ğ‘—)\nâ‰ˆ\nğ¿ğ‘ , ğ‘—+/âˆšï¸„ummationtext.ï£¶ğ‘—âˆ’1\nğ‘˜ Ëœğ¿ğ‘ ,ğ‘˜/âˆšï¸âˆšï¸‚oductdiâˆšï¸„âˆšï¸lay.ï£¶\nğ‘–=1\nğ‘(ğ‘ ğ‘—,ğ‘–|ğ‘ ğ‘—,<ğ‘–, Ëœs<ğ‘—),\n(5)\nwhere ğ‘ ğ‘—,ğ‘– denotes the ğ‘–-th token in the ğ‘—-th seg-\nment, ğ¿ğ‘ ,ğ‘— and Ëœğ¿ğ‘ ,ğ‘— represent the token length of ğ‘—-\nth original and compressed segment, respectively.\nWhen the conditional probabilities for each seg-\nment ğ‘(sğ‘—)are obtained, the compression ratio\nthreshold ğ›¾ğ‘— w.r.t. sğ‘— are dynamically calculated\nbased on the PPL distribution and the correspond-\ning compression ratio ğœsğ‘— , where\nğœsğ‘— =\nï£±ï£´ï£´ï£´ ï£²\nï£´ï£´ï£´ï£³\nğœins +Î”ğœ, if sğ‘— from xins,\nğœdems, if sğ‘— from xD,\nğœque +Î”ğœ, if sğ‘— from xque.\n(6)\nFinally, tokens in each sğ‘— with the PPL greater\nthan ğ›¾ğ‘— are retained in the compressed prompt.\nËœsğ‘— ={ğ‘ ğ‘—,ğ‘–|ğ‘(ğ‘ ğ‘—,ğ‘–)> ğ›¾ğ‘—} (7)\n4.3 Distribution Alignment\nTo narrow the gap between the distribution of the\nLLM and that of the small language model used\nfor prompt compression, here we align the two\ndistributions via instruction tuning.\nSpecifically, we start from a pre-trained small\nlanguage model Mğ‘  and use the data generated by\nthe LLM to perform instruction tuning onMğ‘ . The\noptimization of Mğ‘  can be formulated as:\nmin\nÎ¸ğ‘ \nE\n[\n1\nğ‘\nğ‘âˆ‘ï¸\nğ‘–=1\nL(xğ‘–, yğ‘–,LLM; Î¸Mğ‘ \n)\n]\n, (8)\nwhere ğœƒMğ‘  denotes the parameters of Mğ‘ ,\n(xğ‘–, yLLM\nğ‘– )denotes the pair of instruction xğ‘– and\nthe LLM generated texts yLLM\nğ‘– , ğ‘ is the number of\nall examples used for instruction tuning.\n5 Experiments\n5.1 Settings\nDatasets To comprehensively assess the effec-\ntiveness of compressed prompts in retaining LLM\nabilities, we evaluated their performance across\nfour datasets. For reasoning and in-context learn-\ning (ICL), we use GSM8K (Cobbe et al., 2021)\nand BBH (Suzgun et al., 2022). As for contextual\nunderstanding, we use ShareGPT (sha, 2023) for\nconversation and Arxiv-March23 (Li, 2023) for\nsummarization. Itâ€™s worth noting that neither the\nsmall LM nor the target LLMs used in this paper\nhave seen any of the evaluation datasets, especially\nthe last two which were newly collected this year.\nWe followed the experimental setup of previous\nwork (Fu et al., 2023a; Li, 2023) for the usage of\nthese datasets. Please refer to Appendix A.1 for\ndetailed information.\nEvaluation Following Cobbe et al. (2021), Fu\net al. (2023a), and Li (2023), we utilize the Ex-\nact Match as the evaluation metric for GSM8K\nand BBH. We use BLEU (Papineni et al., 2002),\nROUGE (Lin, 2004), and BERTScore (Zhang et al.,\n2020) as the evaluation metrics for ShareGPT and\nArxiv-March23.\nImplementation Details In this paper, we em-\nploy the GPT-3.5-Turbo-0301 and the Claude-v1.3\nas the target LLMs, which can be accessed via Ope-\nnAI2 and Claude API 3. To improve the stability\nof outputs produced by LLMs we apply greedy\ndecoding with a temperature of 0 across all exper-\niments. The Alpaca dataset (Taori et al., 2023) is\nexclusively employed for aligning small language\nmodels with black-box LLMs, and is not utilized\nin the evaluation process. In our experiments, we\nutilize either Alpaca-7B4 or GPT2-Alpaca as the\nsmall pre-trained language model Mğ‘  for compres-\nsion. We implement our approach based on Py-\nTorch 1.12.05 and Huggingfaceâ€™s Transformers6.\nWe set the granular control coefficient ğ‘˜ to 2. We\nuse the pre-defined compression rates ğœins =0.85\nand ğœque =0.9 for instructions and questions. The\nsegment size used in the iterative token-level com-\npression is set to 100.\n2https://platform.openai.com/\n3https://anthropic.com/\n4https://github.com/tatsu-lab/stanford_alpaca\n5https://pytorch.org/\n6https://github.com/huggingface/transformers\n13362\nMethods ShareGPT Arxiv-March23\nBLEU Rouge1 Rouge2 RougeL BS F1 Tokens1/ğœ BLEU Rouge1 Rouge2 RougeL BS F1 Tokens1/ğœ\nConstraint I 2x constraint 350 tokens constraint\nSentence Selection28.59 46.11 31.07 37.94 88.64 388 1.5x 22.77 50.1 25.93 33.63 88.21 379 4x\nSelective-Context25.42 46.47 29.09 36.99 88.92 307 1.9x 21.41 51.3 27.94 36.73 89.60 356 4x\nOurs 27.36 48.87 30.32 38.55 89.52 304 1.9x23.15 54.21 32.66 42.74 90.33 345 4x\nConstraint II 3x constraint 175 tokens constraint\nSentence Selection18.94 35.17 18.96 26.75 85.63 255 2.3x 12.41 38.91 14.25 26.72 87.09 229 7x\nSelective-Context15.79 38.42 20.55 28.89 87.12 180 3.3x 12.23 42.47 19.48 29.47 88.16 185 8x\nOurs 19.55 40.81 22.68 30.98 87.70 177 3.3x13.45 44.36 24.86 34.94 89.03 176 9x\nTable 1: Performance of different methods under different target compression ratios on the conversation (ShareGPT)\nand summarization (Arxiv-March23) task.\nMethods GSM8K BBH\nEM Tokens1/ğœ EM Tokens1/ğœ\nFull-shot 78.85 2,366 - 70.07 774 -\n1-shot constraint\n1-shot 77.10 422 6x 69.60 284 3x\nSelective-Context53.98 452 5x 54.27 276 3x\nGPT4 Generation71.87 496 5x 27.13 260 3x\nOurs 79.08 446 5x 70.11 288 3x\nhalf-shot constraint\nSentence Selection72.33 230 10x 39.56 175 4x\nSelective-Context52.99 218 11x 54.02 155 5x\nGPT4 Generation68.61 223 11x 27.09 161 5x\nOurs 77.41 171 14x 61.60 171 5x\nquarter-shot constraint\nSentence Selection66.67 195 12x 46.00 109 7x\nSelective-Context44.20 157 15x 47.37 108 7x\nGPT4 Generation56.33 188 20x 26.81 101 8x\nOurs 77.33 117 20x 56.85 110 7x\nzero-shot 48.75â€  11 215x 32.32 16 48x\nSimple Prompt 74.9 691 3x - - -\nTable 2: Performance of different methods under differ-\nent target compression ratios on the GSM8K mathemati-\ncal reasoning and Big-bench Hard (BBH) datasets. â€ We\nalso include the instruction of the prompt in zero-shot\nexperiments for a vertical comparison.\nBaselines We consider the following baselines:\nâ€¢ GPT4-Generation: Instruct GPT-4 to com-\npress the original prompt. We used ten sets\nof instructions here and reported the best re-\nsults. Appendix C displays the instructions\nwe employed.\nâ€¢ Random Selection: Random select the demon-\nstrations or sentences of the original prompt.\nâ€¢ Selective-Context (Li, 2023): Use the phrase-\nlevel self-information from a small language\nmodel to filter out less informative content.\nWe use the same small LM, i.e., Alpaca-7B\nfor a fair comparison.\n5.2 Main Results\nTable 1 and 2 report the results of our approach\nalongside those baseline methods on GSM8K,\nBBH, ShareGPT, and Arxiv-March23. It can be\nseen that our proposed method consistently out-\nperforms the prior methods by a large margin in\nalmost all experiments.\nSpecifically, on GSM8K and BBH, the reason-\ning and in-context learning-related benchmark, our\nmethod even achieves slightly higher results than\nthe full-shot approach, while also delivering im-\npressive compression ratios ( 1/ğœ) of 5x and 3x\nrespectively, with the 1-shot constraint. This well\ndemonstrates that our compressed prompts effec-\ntively retain the reasoning information contained\nin the original prompt. As the compression ratio\nincreases, i.e., under the half-shot and quarter-shot\nconstraints, the performance experiences a slight\ndecline. For instance, on GSM8K, the EM scores\nwill decrease by 1.44 and 1.52, respectively, de-\nspite compression ratios as high as 14x and 20x.\nOn BBH, our approach achieves compression ra-\ntios of 5x and 7x with the EM score decreasing\nby 8.5 and 13.2 points, respectively. In fact, this\nperformance is already quite satisfactory, as it ap-\nproaches the score of 62.0 achieved by PaLM-540B\nin half-shot constraint. Our case study reveals\nthat this declined performance on BBH is mainly\ndue to challenging reasoning tasks, such as track-\ning_shuffled_objects_seven_objects.\nMoreover, on ShareGPT and Arxiv-March23,\ntwo contextual understanding benchmarks, we can\nsee that our approach achieves acceleration ratios\nof 9x and 3.3x with a high BERTScore F1, indi-\ncating that our approach successfully retains the\nsemantic information of the initial prompts.\n5.3 Analysis on Reasoning & ICL Tasks.\nHere we analyze the performance of our approach\nand baseline methods on the difficult reasoning and\nin-context learning (ICL) benchmarks GSM8K and\nBBH.\nWe notice that our approach shows significant\n13363\nperformance improvements over the strong base-\nline Selective-Context under all settings. We\nconjecture that, as relying on phrase-level self-\ninformation, Selective-Context is prone to lose\ncritical reasoning information during the chain-of-\nthought process. Especially on GSM8K, its perfor-\nmance is lower than ours by 33.10 points at a com-\npression ratio of 20x. The inferior performance of\nSentence Selection suggests that it may face similar\nissues of fragmentary reasoning logic. Surprisingly,\nthough GPT-4 has demonstrated its strong text gen-\neration capability, the suboptimal performance on\nprompt compression indicates that the generated\nprompts may omit crucial details from the original\nprompt, particularly reasoning steps.\nIn addition to the findings mentioned above, the\nexperiments also demonstrate that our method can\npreserve the ICL capacity of prompts for LLMs.\nCompared to the zero-shot results, our approach\nexhibits significant performance improvements of\n51.55 and 24.53 even with the largest compression\nratios. Notably, on GSM8K, our 20x compressed\nprompt outperforms the 8-shot 3-step CoT by 2.43,\nfurther suggesting that our method can effectively\nretain the reasoning information.\n5.4 Ablation\nTo validate the contributions of different compo-\nnents in our approach, we introduce five variants\nof our model for ablation study: i) Ours w/o It-\nerative Token-level Compression, which performs\ntoken-level compression in a single inference rather\nthan iteratively. ii) Ours w/o Budget Controller ,\nwhich directly employs ITPC with the same com-\npression ratio for all components. iii) Ours w/o\nDynamic Compression Ratio, which uses the same\ncompression ratio for all components. iv) Ours\nw/ Random Selection in Budget Controller, which\nrandomly selects demonstrations or sentences for\ndemonstration-level prompt compression. v) Ours\nw/o Distribution Alignment , which removes the\ndistribution alignment module of our approach and\ndirectly use the pre-trained LLaMA-7B as the small\nlanguage model. vi) Ours w/ Remove Stop Words,\nwhich removes the stop words in original prompts\nusing NLTK7. Table 3 shows the results.\nComparing Ours with w/o Iterative Token-level\nPrompt Compression, we observe a significant de-\ncline in Exact Match when the conditional depen-\ndence between compressed tokens is not consid-\n7https://www.nltk.org/\nEM Tokens1/ğœ\nOurs 79.08439 5x\n- w/o Iterative Token-level Prompt Compression 72.93 453 5x\n- w/o Budget Controller 73.62 486 5x\n- w/o Dynamic Compression Ratio 77.26 457 5x\n- w/ Random Selection in Budget Controller 72.78 477 5x\n- w/o Distribution Alignment 78.62 452 5x\n- w/ Remove Stop Words 76.27 1,882 1.3x\nTable 3: Ablation study on GSM8K in 1-shot constraint.\nered. We conjecture this variant may lose essen-\ntial information in the prompt, especially for low-\nfrequency keywords that frequently appear in the\ngiven prompt. When comparing Ours with w/o\nDynamic Compression Ratio and with w/o Bud-\nget Controller, it reveals that different components\nof the prompt exhibit varying sensitivity. Instruc-\ntions and questions necessitate a lower compres-\nsion ratio. To balance the relationship between\ncompression ratio and language integrity, introduc-\ning a demonstration or sentence-level filter better\npreserves sufficient linguistic information, even at\nhigher compression ratios. Ours w/ Random Selec-\ntion in Budget Controller indicates that selecting\nsentences or demonstrations based on perplexity\ncan better identify information-rich sentences for\ntarget LLMs. Distribution Alignment allows small\nLMs to generate distributions that more closely re-\nsemble those of target LLMs, resulting in a further\nimprovement of 0.56 on GSM8K.\n5.5 Discussion\nDifferent Target LLMs Here we test our method\nwith Claude-v1.3 as the target LLM to demon-\nstrate its generalizability across different black-box\nLLMs in addition to the GPT series models. Due\nto the limitation of API cost, we only consider the\nscenarios with one-shot constraint and half-shot\nconstraint. Similarly, we employe Alpaca-7B as\nthe small language model for the challenges in col-\nlecting alignment data. As shown in Table 4, our\nmethod can achieve improvements over the simple\nprompt by 0.8 and 1.7 EM points with compression\nratios of 5x and 14x, respectively.\nEM Tokens1/ğœ\nOursin 1-shot constraint 83.51 439 5x\nOursin half-shot constraint 82.61 171 14x\nSimple Prompt 81.8 691 3x\nTable 4: Ours method on GSM8K using Claude-v1.3.\n13364\nDifferent Small LMs We further test our ap-\nproach with different small language models: we\nfine-tune the GPT2-small on the Alpaca dataset and\nuse it as the small LM for our system. As shown in\nTable 5, the results obtained by Alpaca finetuned\nGPT2-small are weaker than those obtained by\nAlpaca-7B with a performance drop of 2.06, 0.99,\nand 1.06 EM points at different compression ratios.\nThis is due to the significant distribution discrep-\nancy between the small LM and the target LLM.\nEven with distribution alignment, it is still diffi-\ncult to directly estimate the target LLM using the\ndistribution from the small language model. Sim-\nilar observations have been reported in Li (2023).\nHowever, benefiting from the proposed budget con-\ntroller and the iterative token-level prompt com-\npression algorithm, our approach achieves satisfac-\ntory results in difficult tasks such as reasoning even\nwith the less powerful GPT2-Small as the small\nlanguage model.\nEM Tokens1/ğœ\nOurswith GPT2 in 1-shot constraint 77.02 447 5x\nOurswith GPT2 in half-shot constraint 76.42 173 14x\nOurswith GPT2 in quarter-shot constraint 76.27 128 18x\nTable 5: Our method on GSM8K with GPT2-Alpaca as\nthe small language model.\nThe Generation Results of Compressed Prompt\nAppendix E displays several compressed prompts\nalong with following generation texts. It is evident\nthat the compressed prompts can still guide the gen-\neration of multi-step reasoning outcomes similar to\nthe original ones. In contrast, prompts compressed\nusing Selective-Context exhibit errors in reason-\ning logic. This highlights the effectiveness of our\nmethod in preserving crucial semantic information\nwhile retaining reasoning capabilities.\nAs depicted in Figure 2, we also analyze the re-\nlationship between the compression ratio and the\nlength of the corresponding generated texts. It can\nbe observed that as the compression ratio increases,\nthe text length produced by target LLMs tends to\ndecrease, albeit with varying degrees across differ-\nent datasets. This indicates that prompt compres-\nsion not only saves computational resources in the\ninput but also contributes to computational savings\nin the generation stage.\nOverhead of LLMLingua We explore two key\nfactors to study the computation overhead of LLM-\n5 10 15 20\nCompreession Ratio\n100\n200\n300\n400Generation Token Length\nGSM8K\nBBH\nShareGPT\nArxiv\nFigure 2: The distribution of generated token lengths at\nvarying compression ratios (1/ğœ).\nLingua: the number of tokens involved in computa-\ntion and the end-to-end latency.\nThe overall computation of our system is the\nsum of the prompt compression and the following\ninference. This can be formulated as:\nğ‘ =(ğ¿ +ğ‘˜ ğ¿/ğœ +ğ¿/ğœ)Â· ğ‘small +ğ¿/ğœ Â·ğ‘LLMs, (9)\nwhere ğ‘small and ğ‘LLMs represent the per token com-\nputation load of the small LM and LLM, respec-\ntively. ğ¿, ğ‘˜ ğ¿/ğœ, and ğ¿/ğœ are the numbers of to-\nken inferences for the budget controller, the per-\nplexity calculation of tokens to compress in ITPC,\nand the conditioned perplexity calculation of com-\npressed results in ITPC (using KV cache), respec-\ntively. Assuming that the small LM has the same\nsystem optimizations as the LLMs, such as the\nuse of FasterTransformer8 and quantization tech-\nniques, we can estimate the ratio between ğ‘small\nand ğ‘LLMs based on model parameters: ğ‘small â‰ˆ\n7/175ğ‘LLMs =1/25ğ‘LLMs. When ğœ =5, we have\nğ‘ â‰ˆ0.264 Â·ğ¿ğ‘LLMs â‰ˆ1/4 Â·ğ¿ğ‘LLMs. That is, we\ncan achieve nearly 4x savings in computational re-\nsources when using the smaller LM with a prompt\ncompression rate of 5x.\n1/ğœ 1x 2x 5x 10x\nEnd-to-End w/o LLMLingua 8.6 - - -\nEnd-to-End w/ LLMLingua - 4.9(1.7x) 2.3(3.3x) 1.3(5.7x)\nLLMLingua - 0.8 0.3 0.2\nTable 6: Latency (s) comparison on GSM8K.\nTable 6 shows the end-to-end latency of different\nsystems on a V100-32G GPU with a compression\nrate from 1x to 10x. We can see that LLMLingua\nhas a relatively small computation overhead and\ncan achieve a speedup ranging from 1.7x to 5.7x.\n8https://github.com/NVIDIA/FasterTransformer\n13365\nRecovering the Compressed Prompt using\nLLMs Appendix D shows some examples re-\nstored from the compressed prompts by using GPT-\n49. It is evident that LLMs can effectively compre-\nhend the semantic information in the compressed\nprompts, even if it might be challenging for humans.\nAdditionally, we notice that how much information\nGPT-4 can recover depends on the compression\nratio and the small language model we use. For\ninstance, in Figure 4, the prompt compressed using\nAlpaca-7B is restored to its complete 9-step rea-\nsoning process, while in Figure 5, the prompt com-\npressed with GPT2-Alpaca can only be restored to\na 7-step reasoning process, with some calculation\nerrors.\nCompare with Generation-based Methods We\ndo not develop our approach based on LLM gen-\neration primarily for three reasons: i) The content\nand length of the generated text are uncontrollable.\nUncontrollable length requires more iterations to\nsatisfy the constraint of the compression ratio. Un-\ncontrollable content leads to low overlap between\nthe generated text and the original prompt, par-\nticularly for complex prompts with multi-step in-\nference, which may lose significant amounts of\nreasoning paths or even generate completely unre-\nlated demonstrations. ii) The computational cost\nis high. Small language models struggle to handle\nsuch complex tasks, and using models like GPT-4\nfor compression would further increase computa-\ntional overhead. Moreover, even powerful genera-\ntion models like GPT-4 struggle to retain effective\ninformation from prompts as shown in Table 2.\niii) The compressed prompts obtained from genera-\ntion models are complete and continuous sentences,\nusually resulting in a lower compression ratio com-\npared to our coarse-to-fine method.\nCompare with Prompt Engineering methods\nOur method is orthogonal to Prompt Engineering\nmethods, such as prompt retrieval and prompt or-\ndering. Our work focuses on compressing well-\ndesigned prompts, and it performs well on com-\nplex and fine-tuned prompts like GSM8K. More-\nover, the perplexity-based demonstration filtering\nmethod used in our budget controller can also be\napplied to scenarios such as prompt retrieval. This\n9An intriguing observation is that GPT-3.5-Turbo struggles\nto reconstruct compressed prompts, while GPT-4 has demon-\nstrated an ability to do so. This contrast in performance could\nsuggest that recovering compressed prompts is an emergent\nability that arises in more advanced language models.\ndemonstrates the compatibility and adaptability of\nour approach in various LLMs settings.\n6 Conclusion\nWe introduce a coarse-to-fine algorithm for prompt\ncompression, named LLMLingua, which is based\non the small LMâ€™s PPL for black-box LLMs. Our\napproach consists of three modules: Budget Con-\ntroller, Iterative Token-level Compression, and\nAlignment. We validate the effectiveness of our\napproach on 4 datasets from different domains, i.e.,\nGSM8K, BBH, ShareGPT, and Arxiv-March23,\ndemonstrating that our method achieves state-of-\nthe-art performance across all datasets, with up\nto 20x compression with only a 1.5 point perfor-\nmance drop. Moreover, we observe that LLMs\ncan effectively restore compressed prompts, and\nprompt compression contributes to a reduction in\ngenerated text length. Our approach holds substan-\ntial practical implications, as it not only reduces\ncomputational costs but also offers a potential solu-\ntion for accommodating longer contexts in LLMs.\nThe method of compressing prompts has the po-\ntential to enhance downstream task performance\nby compressing longer prompts and to improve the\nLLMsâ€™s inference efficiency by compressing the\nKV cache.\nLimitations\nThere are also some limitations in our approach.\nFor instance, we might observe a notable per-\nformance drop when trying to achieve exces-\nsively high compression ratios such as 25x-30x\non GSM8K, as shown in Figure 3.\n5 10 15 20 25 30\nCompreession Ratio\n50\n60\n70\n78.85Exact Match\nOurs\nRandom Selection\nGPT4-Generation\nSelective-Context\nOurs w/ GPT2\nFigure 3: The performance of various prompt com-\npression methods at different compression ratios (1/ğœ)\non GSM8K. The dashed line corresponds to the Exact\nMatch score obtained from the full-shot prompt.\n13366\nIt is shown that as the compression ratio in-\ncreases especially around 25x-30x, all methods as\nwell as ours will experience a substantial perfor-\nmance drop. In comparison with other methods,\nthis performance drop derived from our approach\nis significantly shifted to much higher compression\nratios. We owe this to the Budget Controller and\nthe Iterative Token-level Prompt Compression al-\ngorithm, which enable our method to maintain the\noriginal prompt information even at some extreme\ncompression ratios. The upper limit of the compres-\nsion ratio for different prompts varies, depending\non factors such as prompt length, task type, and the\nnumber of sentences involved.\nAdditionally, there may be subtle differences\nbetween the tokenizers used by the small language\nmodel and the black-box LLM, which may result\nin an underestimation of the promptâ€™s token length.\nReferences\n2023. Sharegpt. https://sharegpt.com/.\nUdit Arora, William Huang, and He He. 2021. Types\nof out-of-distribution texts and how to detect them.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n10687â€“10701, Online and Punta Cana, Dominican\nRepublic. Association for Computational Linguistics.\nDaniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao\nZhang, Christoph Feichtenhofer, and Judy Hoffman.\n2023. Token merging: Your vit but faster. In The\nEleventh International Conference on Learning Rep-\nresentations.\nHarrison Chase. 2022. LangChain.\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith, and\nDanqi Chen. 2023. Adapting language models to\ncompress contexts. ArXiv preprint, abs/2305.14788.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. ArXiv preprint, abs/2110.14168.\nGrÃ©goire DelÃ©tang, Anian Ruoss, Paul-Ambroise\nDuquenne, Elliot Catt, Tim Genewein, Christo-\npher Mattern, Jordi Grau-Moya, Li Kevin Wenliang,\nMatthew Aitchison, Laurent Orseau, et al. 2023. Lan-\nguage modeling is compression. ArXiv preprint ,\nabs/2309.10668.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke\nZettlemoyer. 2022. GPT3.int8(): 8-bit matrix mul-\ntiplication for transformers at scale. In Advances in\nNeural Information Processing Systems.\nElias Frantar and Dan Alistarh. 2023. SparseGPT: Mas-\nsive language models can be accurately pruned in\none-shot. In International Conference on Machine\nLearning.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan\nAlistarh. 2023. OPTQ: Accurate quantization for\ngenerative pre-trained transformers. In The Eleventh\nInternational Conference on Learning Representa-\ntions.\nYao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao\nPeng, and Tushar Khot. 2023a. Chain-of-thought\nhub: A continuous effort to measure large language\nmodelsâ€™ reasoning performance. ArXiv preprint ,\nabs/2305.17306.\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and\nTushar Khot. 2023b. Complexity-based prompting\nfor multi-step reasoning. In The Eleventh Interna-\ntional Conference on Learning Representations.\nTao Ge, Jing Hu, Li Dong, Shaoguang Mao, Yan Xia,\nXun Wang, Si-Qing Chen, and Furu Wei. 2022.\nExtensible prompts for language models. ArXiv\npreprint, abs/2212.00616.\nTao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu\nWei. 2023. In-context autoencoder for context com-\npression in a large language model. ArXiv preprint,\nabs/2307.06945.\nHenry Gilbert, Michael Sandborn, Douglas C Schmidt,\nJesse Spencer-Smith, and Jules White. 2023. Seman-\ntic compression with large language models. ArXiv\npreprint, abs/2304.12512.\nSaurabh Goyal, Anamitra Roy Choudhury, Saurabh\nRaje, Venkatesan T. Chakaravarthy, Yogish Sabhar-\nwal, and Ashish Verma. 2020. Power-bert: Accel-\nerating BERT inference via progressive word-vector\nelimination. In Proceedings of the 37th International\nConference on Machine Learning, ICML 2020, 13-18\nJuly 2020, Virtual Event, volume 119 of Proceedings\nof Machine Learning Research , pages 3690â€“3699.\nPMLR.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. 2022. LoRA: Low-rank adaptation of large\nlanguage models. In International Conference on\nLearning Representations.\nGyuwan Kim and Kyunghyun Cho. 2021. Length-\nadaptive transformer: Train once with length drop,\nuse anytime with search. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 6501â€“6511, Online. Association\nfor Computational Linguistics.\n13367\nSehoon Kim, Sheng Shen, David Thorsley, Amir Gho-\nlami, Woosuk Kwon, Joseph Hassoun, and Kurt\nKeutzer. 2022. Learned token pruning for transform-\ners. In Proceedings of the 28th ACM SIGKDD Con-\nference on Knowledge Discovery and Data Mining,\npages 784â€“794.\nYucheng Li. 2023. Unlocking context constraints of\nllms: Enhancing context efficiency of llms with self-\ninformation-based content filtering. ArXiv preprint,\nabs/2304.12102.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74â€“81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In 7th International\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019 . OpenRe-\nview.net.\nKimberly T Mai, Toby Davies, and Lewis D Griffin.\n2022. Self-supervised losses for one-class textual\nanomaly detection. ArXiv preprint, abs/2204.05695.\nAli Modarressi, Hosein Mohebbi, and Moham-\nmad Taher Pilehvar. 2022. AdapLeR: Speeding up\ninference by adaptive length reduction. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 1â€“15, Dublin, Ireland. Association for\nComputational Linguistics.\nJesse Mu, Xiang Lisa Li, and Noah Goodman. 2023.\nLearning to compress prompts with gist tokens.\nArXiv preprint, abs/2304.08467.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311â€“318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nRichard Clark Pasco. 1976. Source coding algorithms\nfor fast data compression. Ph.D. thesis, Citeseer.\nYongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu,\nJie Zhou, and Cho-Jui Hsieh. 2021. Dynamicvit: Ef-\nficient vision transformers with dynamic token spar-\nsification. In Advances in Neural Information Pro-\ncessing Systems.\nJorma J Rissanen. 1976. Generalized kraft inequality\nand arithmetic coding. IBM Journal of research and\ndevelopment, 20(3):198â€“203.\nClaude E Shannon. 1951. Prediction and entropy\nof printed english. Bell system technical journal ,\n30(1):50â€“64.\nIlya Sutskever. 2023. A theory of unsupervised\nlearning. https://simons.berkeley.edu/talks/\nilya-sutskever-openai-2023-08-14 .\nMirac Suzgun, Nathan Scales, Nathanael SchÃ¤rli, Se-\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny\nZhou, , and Jason Wei. 2022. Challenging big-bench\ntasks and whether chain-of-thought can solve them.\nArXiv preprint, abs/2210.09261.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nDavid Wingate, Mohammad Shoeybi, and Taylor\nSorensen. 2022. Prompt compression and contrastive\nconditioning for controllability and toxicity reduction\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, pages\n5621â€“5634, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nQianhui Wu, Huqiang Jiang, Haonan Yin, BÃ¶rje F. Karls-\nson, and Chin-Yew Lin. 2023. Multi-level knowledge\ndistillation for out-of-distribution detection in text.\nIn Proceedings of the 61th Annual Meeting of the\nAssociation for Computational Linguistics (Long Pa-\npers).\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Julien De-\nmouth, and Song Han. 2023. Smoothquant: Accu-\nrate and efficient post-training quantization for large\nlanguage models. In International Conference on\nMachine Learning.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2023. Wizardlm: Empowering large lan-\nguage models to follow complex instructions. ArXiv\npreprint, abs/2304.12244.\nNan Yang, Tao Ge, Liang Wang, Binxing Jiao, Daxin\nJiang, Linjun Yang, Rangan Majumder, and Furu\nWei. 2023. Inference with reference: Lossless ac-\nceleration of large language models. ArXiv preprint,\nabs/2304.04487.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems 32: Annual Confer-\nence on Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada, pages 5754â€“5764.\nLei Zhang, Yuge Zhang, Kan Ren, Dongsheng Li, and\nYuqing Yang. 2023. Mlcopilot: Unleashing the\npower of large language models in solving machine\nlearning tasks. ArXiv preprint, abs/2304.14979.\n13368\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Evalu-\nating text generation with BERT. In8th International\nConference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\nview.net.\nWangchunshu Zhou, Yuchen Eleanor Jiang, Ryan Cot-\nterell, and Mrinmaya Sachan. 2023. Efficient prompt-\ning via dynamic in-context learning. ArXiv preprint,\nabs/2305.11170.\nA Experiment Details\nA.1 Dataset Details\nGSM8K A widely used math reasoning dataset\ncomprising 8,000 problems, including a 1,300 prob-\nlems test set that assesses modelsâ€™ capabilities in\narithmetic reasoning and formulating mathematical\nsteps using language (Cobbe et al., 2021). For this\ndataset, we employ the complex multi-step CoT\nprompt (Fu et al., 2023b)10 as the original prompt.\nBBH A suite of language and symbolic reason-\ning tasks, consisting of 6,500 problems across 23\nsubsets, specifically designed to evaluate chain-of-\nthought prompting. In our experiment, we adopt\nthe 3-shot CoT prompt11 as the original prompts,\nfollowing the approach described by Suzgun et al.\n(2022).\nShareGPT A conversation dataset from\nShareGPT.com platform (sha, 2023) which in-\ncludes users sharing conversations with ChatGPT\nin different languages and in various scenarios\n(e.g., coding, chitchat, writing assistant, etc.). We\nuse a dataset of 575 samples provided by Li (2023)\nas our test set. We use all dialogues except the\nfinal round as the prompt and generate results with\nGPT-3.5-Turbo as the reference.\nArxiv-March23 A dataset consisting of latest\nacademic papers created in March 2023 from the\narXiv preprint repository. We use 500 data items\ncollected by Li (2023) as the test set. Due to the\nexcessive length of some articles, we take the first\nfive sections of each article and truncate each sec-\ntion to 10,000 characters. Then, we concatenate\nthese sections to form the original prompt and use\nGPT-3.5-Turbo to generate the summary as the ref-\nerence.\n10https://github.com/FranxYao/chain-of-thought-hub\n11https://github.com/suzgunmirac/BIG-Bench-Hard\nA.2 Other Implementation Details\nAll experiments were conducted using a Tesla\nV100 (32GB). We trained the GPT2-Alpaca model\non the Alpaca dataset 12 for eight epochs using\na learning rate of 1e-4 and the AdamW opti-\nmizer (Loshchilov and Hutter, 2019). The training\nprocess took approximately 150 minutes to com-\nplete. We use tiktoken13 and GPT-3.5-Turbo model\nto count all the tokens.\nB Economic Cost\nGSM8K BBH ShareGPT Arxiv\nOriginal 5.2 12.8 0.7 1.3\nOurs 0.5 4.8 0.3 0.2\nTable 7: The inference costs($) for various datasets\nusing GPT-3.5-Turbo.\nTable 7 displays the estimated inference costs\nfor various datasets, according to the pricing of\nGPT-3.5-Turbo. Our approach showcases signifi-\ncant savings in computational resources and mon-\netary expenditures, with cost reductions of $4.7,\n$8.0, $0.4, and $0.8 observed in the GSM8K, BBH,\nShareGPT, and Arxiv datasets, respectively.\nC Instructions used in GPT-4 Generation\nThe instructions we used in the GPT-4 Generation\nare shown below:\n1. Could you please rephrase the paragraph to\nmake it short, and keep 5% tokens?\n2. Condense the passage to retain only 5% of its\noriginal tokens, while preserving its meaning.\n3. Short the sentences to 200 tokens.\n4. Trim the text down to 200 tokens in total.\n5. Please provide a concise summary of the given\nexamples in several sentences, ensuring that\nall reasoning information is included.\n6. Summarize the provided examples in a few\nsentences, maintaining all essential reasoning\naspects.\n7. Remove redundancy and express the text con-\ncisely in English, ensuring that all key in-\nformation and reasoning processes are pre-\nserved.\n12https://github.com/tatsu-lab/stanford_alpaca\n13https://github.com/openai/tiktoken\n13369\n8. Eliminate repetitive elements and present the\ntext concisely, ensuring that key details and\nlogical processes are retained.\n9. Follow these steps to shorten the given text\ncontent: 1. First, calculate the amount of\ninformation contained in each sentence, and\nremove sentences with less information. 2.\nNext, further condense the text by removing\nstop words, unnecessary punctuation, and re-\ndundant expressions. Refine the content while\nensuring that all key information is retained.\nLetâ€™s do it step by step.\n10. To shorten the given text, follow these steps:\na) Determine the information value of each\nsentence and remove those with lower value.\nb) Further reduce the text by removing stop\nwords, unneeded punctuation, and superflu-\nous expressions, while making sure to keep\nall vital information intact. Letâ€™s do it step by\nstep.\nD Recovering Compressed Prompts with\nLarge Language Model\nIn this section, we showcase several examples of\nemploying black-box LLMs to reconstruct com-\npressed prompts. Specifically, we have selected\nthree compressed prompts with varying compres-\nsion ratios, produced by distinct small language\nmodels, on different datasets. These prompts, ac-\ncompanied by guiding instructions, will serve as\ninput for the GPT-4 model.\nE Cases Study\nWe present various cases from multiple datasets,\nencompassing compressed prompts, outcomes de-\nrived from original prompts, outcomes derived\nfrom compressed prompts, and results achieved\nutilizing the selective-context approach.\n13370\nOriginal Prompt(9-steps Chain-of-Thought):\nQuestion: Sam bought a dozen boxes, each with 30 highlighter pens inside, for $10 each box. He\nrearranged five of these boxes into packages of six highlighters each and sold them for $3 per\npackage. He sold the rest of the highlighters separately at the rate of three pens for $2. How much\nprofit did he make in total, in dollars?\nLetâ€™s think step by step\nSam bought 12 boxes x $10 = $120 worth of highlighters.\nHe bought 12 * 30 = 360 highlighters in total.\nSam then took 5 boxes Ã—6 highlighters/box = 30 highlighters.\nHe sold these boxes for 5 * $3 = $15\nAfter selling these 5 boxes there were 360 - 30 = 330 highlighters remaining.\nThese form 330 / 3 = 110 groups of three pens.\nHe sold each of these groups for $2 each, so made 110 * 2 = $220 from them.\nIn total, then, he earned $220 + $15 = $235.\nSince his original cost was $120, he earned $235 - $120 = $115 in profit.\nThe answer is 115\nCompressed Prompt:\n: Sam bought a dozen boxes each 30 highl pens inside, $10 each. He reanged five of boxes into of\nsix each $3 per. He sold the thelters separately at the of three $2. much make total,\nLets think step\nbought boxes x0 oflters\nHe 2 3ters in\nSam then boxes 6lters/box 0ters\nHe sold these boxes 5\nAfterelling these boxes there 36030lters\nese00 of three\nsold groups2 each so made *2 $20 from\nIn total, he015\nSince his he $ - $120 = $115 in profit.\nThe answer is 115\nRecovered Compressed Prompt(9-steps Chain-of-Thought):\nQuestion: Sam purchased a dozen boxes, each containing 30 highlighter pens, for $10 each. He\nrearranged five of these boxes to have six pens each and sold them for $3 per box. He then sold\nthe remaining highlighters individually in groups of three for $2 each. Letâ€™s calculate the total profit.\nInitially, Sam bought 12 boxes with 30 highlighters each, which equals 360 highlighters.\nHe spent $10 per box, so his total cost was $120.\nSam then rearranged 5 boxes to have 6 highlighters per box, which equals 30 highlighters. He sold\nthese boxes for $3 each, making $15.\nAfter selling these boxes, he had 330 highlighters left (360 - 30). He sold these in groups of three,\nwhich equals 110 groups. He sold each group for $2, making $220 from this.\nIn total, Sam made $235 ($15 + $220). Since his initial cost was $120, he earned a\nprofit of $115 ($235 - $120).\nThe answer is $115.\nFigure 4: Recovering the compressed prompt(1/ğœ=17x, Alpaca-7B as small language model) from GSM8K using\nGPT-4.\n13371\nCompressed Prompt:\nQuestion: Sam a dozen boxes with 30ighter pens, each Heanged into of3 the separately the of\nthree. much total, in\nâ€™s think step bought boxes $120l\nbought 12 30 = 360\nSam took boxes 6ighters for $15\nthese boxes were 360 - =ighters\nThese form 330 = 110 of three these so made * $220 from them.\nIn total, then, he earned $220 + $15 = $235.\nSince his original cost was $120, he earned $235 - $120 = $115 in profit.\nThe answer is 115\nRecovered Compressed Prompt(7-steps Chain-of-Thought):\nQuestion: Sam bought a dozen boxes with 30 lighters each. He arranged them into groups of three\nand sold them separately. In total, he spent $120 on the lighters. How much profit did he make?\nStep 1: Sam bought 12 boxes * 30 lighters each = 360 lighters in total.\nStep 2: Sam took 6 boxes of lighters and sold them for $15.\nStep 3: The remaining lighters are 360 - 6 boxes * 30 lighters each = 330 lighters.\nStep 4: These 330 lighters form groups of three, so he made 110 groups * $2 per group = $220\nfrom them.\nStep 5: In total, he earned $220 + $15 = $235.\nStep 6: Since his original cost was $120, he earned $235 - $120 = $115 in profit.\nThe answer is $115.\nFigure 5: Recovering the compressed prompt (1/ğœ=19x, GPT2-Alpaca as small language model) from GSM8K\nusing GPT-4.\nOriginal Prompt:\nQuestions that involve enumerating objects and asking the model to count them.\nQ: I have a blackberry, a clarinet, a nectarine, a plum, a strawberry, a banana, a flute, an orange,\nand a violin. How many fruits do I have?\nA: Letâ€™s think step by step.\nWe first identify the fruits on the list and include their quantity in parentheses:\n- blackberry (1)\\n - nectarine (1)\\n - plum (1)\\n - strawberry (1)\\n - banana (1)\\n - orange (1)\nNow, letâ€™s add the numbers in parentheses: 1 + 1 + 1 + 1 + 1 + 1 = 6. So the answer is 6.\nCompressed Prompt:\nQuestions that involve enumerating objects and asking the model to count them.\n: a blackberry, a a ne a a a a, many have\n:â€™s think\nWe first theruits the list and include their in\n- (â€“\nâ€™s the numbers in parentheses:1 + 1 = 6. So the answer is 6.\nRecovered Compressed Prompt:\nQuestion: In a basket, there is a blackberry, an apple, a pear, a plum, a peach, and an orange. How\nmany fruits are in the basket?\nLetâ€™s think step by step. We first identify the fruits in the list and include their individual count in\nparentheses: a blackberry (1), an apple (1), a pear (1), a plum (1), a peach (1), and an orange (1).\nNow, letâ€™s add the numbers in parentheses: 1 + 1 + 1 + 1 + 1 + 1 = 6. So the answer is 6. There are\n6 fruits in the basket.\nFigure 6: Recovering the compressed prompt( 1/ğœ=7x, Alpaca-7B as small language model) from BBH using\nGPT-4.\n13372\nCompressed Prompt:\nFollow the given examples and answer the question.\n: Sam bought a dozen boxes each 30 highl pens inside, $10 each. He reanged five of boxes into of\nsix each $3 per. He sold the thelters separately at the of three $2. much make total,\nLets think step\nbought boxes x0 oflters\nHe 2 3ters in\nSam then boxes 6lters/box 0ters\nHe sold these boxes 5\nAfterelling these boxes there 36030lters\nese00 of three\nsold groups2 each so made *2 $20 from\nIn total, he015\nSince his he $ - $120 = $115 in profit.\nThe answer is 115\nQ: Toula went to the bakery and bought various types of pastries. She bought 3 dozen\ndonuts which cost $68 per dozen, 2 dozen mini cupcakes which cost $80 per dozen, and 6 dozen\nmini cheesecakes for $55 per dozen. How much was the total cost?\nReference Answer(4-steps):\nThe total charge for the doughnuts was 3 x $68 = $Â«3*68=204Â»204.\nThe total charge for the mini cupcakes was 2 x $80 = $Â«2*80=160Â»160.\nThe total charge for the mini cheesecakes was 6 x $55 = $Â«6*55=330Â»330.\nTherefore the total amount Toula paid for the pastries was $204 + $160 + $330 =\n$Â«204+160+330=694Â» 694 .\nThe Generation of Compressed Prompt(4-steps):\nToula bought 3 dozen donuts which cost $68 per dozen. So, she spent 3 x 68 = $204 on donuts.\nShe also bought 2 dozen mini cupcakes which cost $80 per dozen. So, she spent 2 x 80 = $160 on\nmini cupcakes.\nFinally, she bought 6 dozen mini cheesecakes for $55 per dozen. So, she spent 6 x 55 = $330 on\nmini cheesecakes.\nTherefore, the total cost was 204 + 160 + 330 = $694.\nThe answer is 694 .\nThe Generation of Selective-Context:\nA: Toula bought 3 x 12 = Â«3*12=36Â»36 donuts.\nThen, the total cost of the donuts is 36 x $68 = $Â«36*68=2448Â»2448.\nHow many mini cupcakes did Toula buy in total?\nShe bought 2 x 12 = Â«2*12=24Â»24 mini cupcakes.\nSo, the total cost of the mini cupcakes is 24 x $80 = $Â«24*80=1920Â»1920.\nHow many mini cheesecakes did Toula buy in total?\nShe bought 6 x 12 = Â«6*12=72Â»72 mini cheesecakes.\nThus, the total cost of the mini cheesecakes is 72 x $55 = $Â«72*55=3960Â»3960.\nTherefore, the total cost of all the pastries is $2448 + $1920 + $3960 =\n$Â«2448+1920+3960=8328Â»8328. Answer: 8328 .\nFigure 7: Cases study on GSM8K math reasoning dataset in half-shot constraint.\n13373\nCompressed Prompt:\nEvaluate a random boolean function expressed as a word problem.\nQ: Question: Fidel tells the truth.\nJerry saysidel tells truth.\nVina says Jerry\nMill saysina\nRaymond saysent\n(3)ina says Jerry\n() saysina\n) saysent\nWe know from ( that Raymond truth.\nSo the answer is Yes.\nQ: Question: Raymond tells the truth. Sal says Raymond lies. Alexis says Sal lies. Helene says\nAlexis lies. Elanor says Helene lies. Does Elanor tell the truth?\nA: Letâ€™s think step by step.\nThe Generation of Full-shot Prompt(5-steps):\n(1) Raymond tells the truth. So, we know that Raymond tells the truth.\n(2) Sal says Raymond lies. Since we know from (1) that Raymond tells the truth, if Sal says\nRaymond lies, then Sal lies.\n(3) Alexis says Sal lies. Since we know from (2) that Sal lies, if Alexis says Sal lies, then Alexis\ntells the truth.\n(4) Helene says Alexis lies. Since we know from (3) that Alexis tells the truth, if Helene says\nAlexis lies, then Helene lies.\n(5) Elanor says Helene lies. Since we know from (4) that Helene lies, if Elanor says Helene lies,\nthen Elanor tells the truth.\nNow, the question asks: Does Elanor tell the truth? We know from (5) that Elanor tells the truth.\nSo the answer is ğ‘Œğ‘’ğ‘  .\nThe Generation of Compressed Prompt(5-steps):\n1. Raymond tells the truth.\n2. Sal says Raymond lies. This means Sal lies.\n3. Alexis says Sal lies. This means Alexis tells the truth.\n4. Helene says Alexis lies. This means Helene lies.\n5. Elanor says Helene lies. This means Elanor tells the truth.\nSo the answer is ğ‘Œğ‘’ğ‘  .\nThe Generation of Selective-Context:\n1. Raymond says Millicent lies. If Raymond tells the truth, then Millicent lies. If Raymond lies,\nthen Millicent tells the truth. We donâ€™t know which one is true.\n2. Millie says Kristian lies. If Millie tells the truth, then Kristian lies. If Millie lies, then Kristian\ntells the truth. We donâ€™t know which one is true.\n3. Michaela says Kristian lies. If Michaela tells the truth, then Kristian lies. If Michaela lies, then\nKristian tells the truth. We donâ€™t know which one is true.\n4. Osvaldo says Raymond tells the truth. If Osvaldo tells the truth, then Raymond tells the truth. If\nOsvaldo lies, then Raymond lies. We donâ€™t know which one is true.\n5. Jamey is not mentioned in any of the previous statements, so we donâ€™t know if he tells the truth\nor not.\nNow letâ€™s move on to the second question.\nFigure 8: Cases study on web_of_lies of BBH reasoning dataset in quarter-shot constraint.\n13374\nCompressed Prompt:\nPlease provide the example of the radiation beam spectrum in the TOPAS format, and I will learn\nfrom it.\nThe beam energy spectrum is composed of 12 energy levels with corresponding weights.\ndv:So/SARRPbeam/BeamEnergySpectrumValues: Lists the energy values in the spectrum,\nseparated by tabs.\nFrom the provided data, we have the following energy values and corresponding weights:\nEnergy (V)Weight202.15E-0520.12.29E-0520.22.43E0520.32.580520.42.74052.52.9152.63.0\n952.7.29052.8.49E-0520.93.71E05213.94E-0521.14.15E05Please let me know if you have any\nquestions or would like me to assist you further with this data.\nNow Iâ€™ll give you a table with the spectrum of an ElektaAC and I want you to transform it to the\ntopas format:\nPlease provide the table with the spectrum of the Elekta LINAC, and I will transform it into theAS\nformat for you.\nE (MeV) Elekta (6 MV)\n0.25 0.173Eâ€”04(1.5%)\\n .5 .100E03(0.5%)\\n .75 .110E03(.5%)\\n 1 0.952Eâ€”04(0.5%)\\n 1.25\n.827E04(.5\\n .5 .721E04(.\\n . .63E04(.\\n 2 .55E04(.%)\\n . .493E04(0.5%)\\n 2.5 .437E04(.6\\n\n2.5 .386E04(.6\\n 3 .34E4(.6\\n . .30E4(.6\\n . .27E4(.7\\n 3.7 0.240Eâ€”04(.7\\n 4 .21E04(.7\\n .\n.18E(.8\\n . .16E(.8\\n .14E(.9\\n 5 .121E04(.9\\n 5.2 .101E4(1.0\\n . .81E0(.1\\n . .64E.2\\n 6 .45E.4\\n\n.0.300Eâ€”05(1.8\\n 6.5 .15E0(.4\\n .7 .73E0(.4\\n 7 .29E0(.0\\n . .88E0(.\\n .18Eâ€”07(19.8%)\\n 7.75\n0.277Eâ€”08(50.0%)\\n 8 0.159Eâ€”08(70.8%)\nThe Generation of Full-shot Prompt:\nThank you for providing the table with the Elekta LINAC spectrum. Hereâ€™s the TOPAS format for\nthe same:\n# Spectrum\ns:...Type \"Discrete\"\ndv:...Values\n0.25 0.5 0.75 1 1.25 1.5 1.75 2\n2.25 2.5 2.75 3 3.25 3.5 3.75 4\n4.25 4.5 4.75 5 5.25 5.5 5.75 6\n6.25 6.5 6.75 7 7.25 7.5 7.75 8\nuv:...Weights\n0.173E-04 0.100E-03 0.110E-03 0.952E-04 0.827E-04 0.721E-04 0.635E-04 0.557E-04\n0.493E-04 0.437E-04 0.386E-04 0.345E-04 0.302E-04 0.270E-04 0.240E-04 0.210E-04\n0.186E-04 0.164E-04 0.142E-04 0.121E-04 0.101E-04 0.817E-05 0.645E-05 0.455E-05\n0.300E-05 0.156E-05 0.734E-06 0.294E-06 0.882E-07 0.188E-07 0.277E-08 0.159E-08\nI hope this helps. Let me know if you have any questions or if you need further assistance.\nThe Generation of Compressed Prompt:\nThank you for providing the table of the EletaAC spectrum. Hereâ€™s how it can be formatted in the\nTOPAS format:\n# Spectrum\ns:...Type \"Discrete\"\ndv:...Values 0.25 1.5 2 3.2 4 5 6 7\n7.25 7.5 7.75 8\nuv:...Weights 173E04 .10E-0 .11E1 .950 .82 .63E-0 .55E0 .4943\n.345E0 .30E0 .27 .24E0 .16E0 .714E52 .64E0 6.45E0\n.30E0 .15E0 .73E0 .294E-06 .882E-07 .188E-07 .277E-08 .159E-08\nPlease let me know if you have any questions or if you need further assistance with this data.\nThe Generation of Selective-Context:\nIâ€™m sorry, but I cannot see the table you are referring to. Can you please provide me with the table\nso I can assist you in transforming it into the TOPAS format?\nFigure 9: Cases study on ShareGPT conversation dataset in 2x constraint.\n13375\nCompressed Prompt:\nRedshift drift in a universe with structure II: Light rays propagated through aian Nbody simulation\nkoksbang@cp3.sdk\nCP3-Orins,markâ€ Mmark\nTheshiftift is computed alongs propagating through aulated universe on the Newtonian\nNbody simulation code GGET2 with a perturbedmann-Lreer metric in theian gauge.\nIt is that the meanshiftift is to theift of the meanshift to the precision of the numericalations and\nthat this is to a high of cancellation between two dominant components theshiftift.\nThis result is contrary to earlier findings based on inhomogeneous cosmological models exhibiting\ncosmic backreaction.\nFor simplicity, the results neglect contributions from optical drift.\nBased on a study of the redshift drift in a Lemaitre-Tolman-Bondi model, the optical drift effects\nare estimated to be at most of order 10% of the redshift drift signal.\nThe Generation of Full-shot Prompt:\nThe article discusses the computation of redshift drift along light rays in a simulated universe\nbased on the Newtonian N-body simulation code GADGET-2 combined with a perturbed\nFriedmann-Lemaitre-Robertson-Walker metric in the Newtonian gauge. The study finds that the\nmean redshift drift is equal to the drift of the mean redshift to the precision of the numerical\ncomputations, and this is due to a high degree of cancellation between two dominant components\nof the redshift drift. The study neglects contributions from optical drift, but it is estimated that\nthese effects are at most of order 10% of the redshift drift signal. The study also finds that the\nredshift drift contribution from peculiar acceleration of the emitter is negligible in the simulation\nsetup, but it is expected that this contribution will be larger for real observations.\nThe Generation of Compressed Prompt:\nThe document discusses the computation of redshift drift in a simulated universe using the Nbody\nsimulation code GGET2 with a perturbed LemaÃ®tre-LemaÃ®tre metric. The results show that the\nmean shift is equivalent to the shift of the mean to a high degree of cancellation between two\ndominant components. This contradicts earlier findings based on inhomogeneous cosmological\nmodels exhibiting cosmic backreaction. The document also estimates that optical drift effects are\nat most 10% of the redshift drift signal.\nThe Generation of Selective-Context:\nThe document discusses the use of redshift drift measurements, which were first proposed in 1962.\nThe study involves analyzing non-radial light rays observed by three different observers with\nnon-radial lines of sight. The present-day density profile of the LTB model is shown in a figure,\nand quadri-linear interpolation is used to interpolate between snapshots. The observation time is\nset to 30 years, and the mean and fluctuations of the individual components of the redshift drift are\nshown in another figure. The study does not address the effects of peculiar acceleration.\nFigure 10: Cases study on Arxiv-March23 summarization dataset in 200 tokens constraint.\n13376"
}