{
  "title": "Publicly-Detectable Watermarking for Language Models",
  "url": "https://openalex.org/W4406309654",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A3108698244",
      "name": "Jaiden Fairoze",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2111572205",
      "name": "Sanjam Garg",
      "affiliations": [
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A2193269139",
      "name": "Somesh Jha",
      "affiliations": [
        "University of Wisconsin–Madison"
      ]
    },
    {
      "id": "https://openalex.org/A2766377478",
      "name": "Saeed Mahloujifar",
      "affiliations": [
        "Artificial Intelligence in Medicine (Canada)"
      ]
    },
    {
      "id": "https://openalex.org/A2318129066",
      "name": "Mohammad Mahmoody",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A2161978436",
      "name": "Mingyuan Wang",
      "affiliations": [
        "New York University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4293077571",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W4389325647",
    "https://openalex.org/W4353007481",
    "https://openalex.org/W4368755014",
    "https://openalex.org/W4387561528",
    "https://openalex.org/W1479681406",
    "https://openalex.org/W4288333737",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4385473933",
    "https://openalex.org/W2963044831",
    "https://openalex.org/W3156309620",
    "https://openalex.org/W3211484452",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W4319831446",
    "https://openalex.org/W1527181506",
    "https://openalex.org/W4318351452",
    "https://openalex.org/W4392745757",
    "https://openalex.org/W2968297680",
    "https://openalex.org/W4362598574",
    "https://openalex.org/W4380994421",
    "https://openalex.org/W4226317937",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4385437823",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W1610688607",
    "https://openalex.org/W2122014146",
    "https://openalex.org/W4391421113",
    "https://openalex.org/W204891295",
    "https://openalex.org/W2136175650",
    "https://openalex.org/W4364387756",
    "https://openalex.org/W4360891421",
    "https://openalex.org/W2103012681",
    "https://openalex.org/W2499568709",
    "https://openalex.org/W3172958259",
    "https://openalex.org/W2052267638",
    "https://openalex.org/W4399425477",
    "https://openalex.org/W4287610330",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4318149317",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W4299567010"
  ],
  "abstract": "We present a publicly-detectable watermarking scheme for LMs: the detection algorithm contains no secret information, and it is executable by anyone. We embed a publicly-verifiable cryptographic signature into LM output using rejection sampling and prove that this produces unforgeable and distortion-free (i.e., undetectable without access to the public key) text output. We make use of error-correction to overcome periods of low entropy, a barrier for all prior watermarking schemes. We implement our scheme and find that our formal claims are met in practice.",
  "full_text": "IACR Communications in Cryptology\nISSN 3006-5496, Vol. 1, No. 4, 34 pages.\nhttps://doi.org/10.62056/ahmpdkp10\nCheck for updates\nPublicly-Detectable Watermarking for\nLanguage Models\nJaiden Fairoze1, Sanjam Garg1 , Somesh Jha2, Saeed Mahloujifar3 ,\nMohammad Mahmoody4 and Mingyuan Wang5\n1 University of California, Berkeley, USA\n2 University of Wisconsin–Madison, USA\n3 Fundamental Artificial Intelligence Research at Meta, USA\n4 University of Virginia, USA\n5 New York University Shanghai, China\nAbstract. We present a publicly-detectable watermarking scheme for LMs: the\ndetection algorithm contains no secret information, and it is executable by anyone.\nWe embed a publicly-verifiable cryptographic signature into LM output using re-\njection sampling and prove that this produces unforgeable and distortion-free (i.e.,\nundetectable without access to the public key) text output. We make use of error-\ncorrection to overcome periods of low entropy, a barrier for all prior watermarking\nschemes. We implement our scheme and find that our formal claims are met in\npractice.\nKeywords: LMwatermarking · languagemodels · unforgeablewatermark · distortion-\nfree sampling\n1 Introduction\nGenerative AI (GenAI) technologies, such as language models (LMs) and diffusion models,\nhave impressive capabilities. These capabilities include in-context learning, code completion,\ntext-to-image generation, and document and code chat. However, GenAI technologies are\nalso being used for nefarious purposes (e.g., generating fake tweets, generating attacks,\nand harmful prose). To protect against such use cases, a large body of work has focused\non detecting AI-generated content [LUY08; Ber16; GSR19; ZHR+19; MLK+23; GPT23;\nHAAL23]. The problem is: given contentc, isc generated by a specific GenAI tool, e.g.,\nGPT-4 [Ope23], Gemini [Goo24], or Stable Diffusion [RBL+22]? Informally, we want a\n“GenAI Turing Test.”\nAt present, the main approach when trying to detect arbitrary AI-generated text is\nto train yet another AI model to perform the detection [ZHR+19; MLK+23; GPT23;\nHAAL23]. This method makes a critical assumption: that AI-generated text has embedded\nfeatures that are identifiable by AI. The key problem with this assumption is that generative\nmodels are explicitly designed to produce realistic content that is difficult to distinguish\nfrom natural content (generated by a human or nature). As a result, any “black-box”\ndetection scheme will suffer from high false positive and/or false negative rates as generative\nmodels improve. Available detectors such as GPTZero [GPT23] have no guarantee of\nFull code available at:https://github.com/jfairoze/publicly-detectable-watermark\nE-mail: fairoze@berkeley.edu (Jaiden Fairoze),sanjamg@berkeley.edu (Sanjam Garg),jha@cs.w\nisc.edu (Somesh Jha), saeedm@meta.com (Saeed Mahloujifar), mohammad@virginia.edu (Mohammad\nMahmoody), mingyuan.wang@nyu.edu (Mingyuan Wang)\nThis work is licensed under a “CC BY 4.0” license.\nReceived: 2024-10-09 Accepted: 2024-12-03\n2 Publicly-Detectable Watermarking for Language Models\ncorrectness—e.g., the authors state outright that detection results from their tool should\nnot be used to reprimand students.\nTo circumvent this fundamental issue, a recent line of work [Aar23; KGW+23; CGZ24;\nKTHL24] has taken a different approach to detecting AI content. These watermarking\ntechniques alter the generation process to embed a “signal” in the generated content.\nThe detection process measures the signal: if the signal is sufficiently strong, the content\nwas likely watermarked. In particular, the cryptographic approach of Christ, Gunn, and\nZamir [CGZ24] achieves formal notions of completeness (any watermarked text will be\ndetected), soundness (one cannot watermark a text without knowing the secret), and\ndistortion-freeness (watermarking does not change the output distribution). Finally, these\nwatermarking schemes are “keyed” in the sense that the signal is a function of a secret key.\nThe same key is used to generate and measure the signal.\nThe aforementioned watermarking approaches have one problem in common: the model\nprovider and the detector both need to know a shared secret key. This is acceptable in\nscenarios where the entity trying to detect the watermark is the same entity generating\nthe content. For example, an entity that provides a chat API may be able to provide a\ndetection API as well. However, such a setup has limitations:\n1. Lack of privacy:The entity that wants to check the integrity of the content might\nnot be willing to share it with the detector. For example, someone looking to identify\nwhether their medical record summary is AI-generated may not want to share the\nsummary itself.\n2. Conflict of interest:The entity providing the detection API might not be trusted\nin certain cases. For instance, consider a case where an entity is accused of generating\ninappropriate text and is brought to a court of law. It is not reasonable to ask the\nsame entity to tell whether the text is watermarked.\nOne solution could be sharing the secret with the world so everyone can run the\ndetection. However, this raises another important problem: anyone can now embed the\nwatermark to any content, AI-generated or not. This is unacceptable because it introduces\nthe possibility of denial-of-service attacks: an attacker can create masses of watermarked\ncontent that is not AI-generated to undermine the dependability of the detector. Consider\nthe effect on one of the main applications of watermarking: an entity may want to use\nthe watermark as a signature for their content. Such signatures are useful when (a) the\ngenerated content needs to come with proof of a credible generator, and (b) the entity\nneeds to refute an accusation about a generated content; i.e., it should not be accountable\nfor a content without its watermark. This application is rendered impossible in a world\nwhere attacks based on the availability of the secret key can be launched.\nIn this paper, we aim to solve the aforementioned problems for LLMs that produce\ntext. We ask:\nIs it possible to construct apublicly-detectable watermarking scheme with\ncryptographic detectability and distortion-freeness?\nWe find that the answer is yes: we construct a publicly-detectable scheme that provably\nresolves the trust issue—users can cryptographically verify the presence of a watermark.\nFurther, they have a guarantee that the only entity capable of embedding the watermark\nis the model provider, resolving the privacy and conflict of interest issues above. We state\nthe properties for public detectability below:\n1. Cryptographic detectability:To guarantee a user is convinced that a watermark\nis detected, the watermarking scheme must achieve cryptographic detectability: false\npositives or negatives must never occur in practice.\nFairoze et al. 3\n2. Weak robustness: It is possible that text obtained from LMs is modified—to\nsome extent—before publication. The watermark detector should be able to detect a\nwatermark so long as the cryptographic signature is still embedded in the text. Prior\nwork in the secret key setting aimed forstrong robustness where detection should\nbe possible even if the LM output has changed substantially but text semantics are\npreserved. Strong robustness has since been shown to be impossible in the general\ncase [ZEF+24] and we focus on ensuring high detectability as a first step.\n3. Distortion-freeness: The watermarking scheme should not degrade the quality of\nthe LM output. No probabilistic polynomial-time (PPT) adversary should be able\nto distinguish between watermarked and non-watermarked text without access to\nthe public key.\n4. Model agnosticity: The watermarking scheme should use the model as a black\nbox, i.e., it should not rely on any specific model weights or configurations.\n5. Public-verifiablity: Without access to the model weights or secret material of\nthe watermarking scheme, the detector should still be able to determine whether a\ncandidate text is watermarked.\n2 Security Model\nThis section defines what it means for a publicly-detectable watermarking scheme to be\nsecure. We will eventually prove that our construction satisfies these definitions.\n2.1 Preliminaries\nLet a ∥b denote the concatenation ofa to b. We use log(·) to take logarithms base 2.\nLet ϵ denote an empty list or empty string. Letai denote thei-th bit of vectora. We\nuse Python slicing notation throughout:a[−i] refers to thei-th last element of a list and\na[j : k] extracts the elementsai for i∈[j,k). We use\n$\n←to denote a random sample, e.g.,\nr\n$\n←{0,1}n to samplen random bits. We use an asterisk to denote an arbitrary-length\nstring of tokens from a set of possible tokens, e.g.,S∗for a given setS.\nFor the cryptographic primitives in this paper, we useλ for the security parameter.\nA negligible functionnegl(λ) in λ are those functions that decay faster than the inverse\nof any polynomials. That is, for allpoly(λ), it holds thatnegl(λ) < 1\npoly(λ) for all large\nenough λ.\nDefinition 1(Auto-regressive Model). An auto-regressive modelModel over token vocab-\nulary T is a deterministic algorithm that takes in a promptρ∈T ∗and tokens previously\noutput by the modelt∈T ∗and outputs a probability distributionp= Model(ρ,t) over T.\nGenModel wrapsaround Model toimplementagenerativemodelasshowninAlgorithm1.\nWe useModel and GenModel for subsequent definitions and proofs. We use subscript\nnotation as shorthand for then input, i.e.,GenModeln(ρ) = GenModel(n,ρ).\nWe rely on a public-key signature scheme with the following properties.\nDefinition 2(Public-Key Signature Scheme). A public-key signature schemeS is a tuple\nof algorithmsS = (Gen,Sign,Verify) where:\n• Gen(1λ) →(sk,pk) outputs a key pair(sk,pk) with respect to the security parameter\nλ.\n• Signsk(m) →σ produces a signatureσ, given a messagem, using the secret signing\nkey sk. We denote the signature size|σ|by λσ.\n4 Publicly-Detectable Watermarking for Language Models\nAlgorithm 1GenModel\n1: input: n, ρ\n2: t←ϵ\n3: for i= 1 to n do\n4: t←t∥LMDecode(Model(ρ,t))\n5: output: t\nGenModel iteratively generates n tokens. LMDecode is the specific decoding method.\nThroughout this paper, we fixLMDecode to multinomial sampling, though any decoding\nalgorithm that satisfies Assumption 1 would suffice.\n• Verifypk(m,σ) →{true,false}outputs true or false, given a candidate message\nm and signatureσ, using the public verification key.\nDefinition 3(Unforgeability). For every adversaryA, we have\nPr\n[\nVerifypk(m∗,σ∗) = true : (pk,sk) ←Gen(1λ)\n(m∗,σ∗) ←ASignsk(·)(pk)\n]\n≤negl(λ).\nHere, the adversary gets oracle access to the signing oracleSignsk(·), butm∗in the final\nforgery output (m∗,σ∗) must have never been queried using the signing oracle. As a\nsignature scheme, we require this property to guarantee it is hard to forge a watermark.\nDefinition 4(Hamming Distance). For alphabetΣ and x,y ∈Σn, define the Hamming\ndistance betweenx and y as\nHamming(x,y) := |{i∈[n] : xi ̸= yi}|.\nDefinition 5(Error-Correcting Code). For an alphabetΣ, an[n,k,d ]Σ error-correcting\ncode is a 2-tuple(Encode,Decode) algorithm where Encode : Σk →Σn is an encoding\nalgorithm such that for allm,m′∈Σk where m̸= m′,\nHamming(Encode(m),Encode(m′)) ≥d\nand Decode : Σn →Σk is the decoding algorithm such that, for all messagesm∈Σk and\nerroneouscodewords c∈Σn, we have\nHamming(Encode(m),c) ≤γmax =⇒ Decode(c) = m\nwhere γmax ≤(d−1)/2 is the maximum number of erroneous symbols thatDecode can\ncorrect. We denote the codeword size|c|by λc.\n2.2 Assumptions\nWe assume that any contiguous block ofℓ tokens contains at leastα bits of min-entropy,\ni.e., no particular sample is more than2−α likely to happen.1 This assumption allows us\nto capture security properties and present our protocol concisely. In addition,ℓ effectively\nserves as a parameter to tune the trade-off between robustness and distortion-freeness.\nHigher ℓ values lead to more distortion-free text at the cost of robustness and vice versa.\nAssumption 1.For any promptρand tokenst, the new tokenst′←GenModelℓ(ρ,t) ∈T ℓ\nwere sampled from distributions with min-entropy at leastα.\n1Formally, the min-entropyH∞(D) of a distributionD is defined as−log\n(\nmaxω∈Supp(D) Pr[D = ω]\n)\n.\nFairoze et al. 5\nIf this assumption is met, distortion-freeness is guaranteed. However, our construction\nmakes novel use of error-correcting codes (ECC) to weaken the entropy requirement in\npractice—our protocol can tolerate a fixed number of periods where the min entropy is\nbelow α. The maximum number of low-entropy periods our scheme can tolerate is exactly\nthe maximum number of errors that the underlying ECC scheme can correct.\n2.3 Entity Interaction\nWe refer to two distinct entities in our security model:\nModel provider The model provider provides the LM service: given a prompt, it\nreturns the LM output for that prompt and the given LM configuration. An honest model\nprovider will run the watermarking protocol at text generation time. This entity has\nwhite-box access to the model weights in addition to any secret material specific to the\nwatermarking protocol, e.g., a secret watermarking key.\nUser Users generate prompts which are sent to the model provider in exchange for\nthe model output. Users may test text for the presence of a watermark by running the\ndetection algorithm on candidate text and an LM provider’s public key. The user should\nbe convinced that the watermark is present or not, i.e., the detector must provide a “proof\nof watermark” that can be verified without model weights or secret material pertaining to\nthe watermarking protocol.\n2.4 Definitions\nIn this section, we formally define a publicly detectable watermarking scheme, which should\nsatisfy (a) completeness, (b) soundness, (c) robustness, and (d) distortion-freeness. We\nprove our scheme meets these definitions.\nDefinition 6(Publicly-Detectable Watermarking Scheme). A (δs,δc,δr,ϵ)-publicly de-\ntectable watermarking scheme PDWS for an auto-regressive model Model over token\nvocabulary T is a tuple of algorithmsPDWS = (Setup,Watermark,Detect) where:\n• Setup(1λ) →(sk,pk) outputs a public key pair(sk,pk) with respect to the security\nparameter λ.\n• Watermarksk(ρ)\n$\n→tproduces response textt∈T ∗ given a promptρ∈T ∗ using\nthe secret keysk.\n• Detectpk(t∗) →{true,false}outputs true or false given a candidate watermarked\ntext t∗.\nA PDWS scheme is considered secure if the following security definitions are met.\nDefinition 7(Completeness). A PDWS is δc-complete if for every promptρand token\nsequence t∈T ∗of length|t|≥ δc, it holds that\nPr\n[\nDetectpk(t) = false : (sk,pk) ←Setup(1λ)\nt←Watermarksk(ρ)\n]\n≤negl(λ).\nδc-completeness ensures that text of sufficient length that was watermarked with the\nhonest protocol results in non-detection with negligible probability. This definition is an\nasymmetric-key analogue of the symmetric-key completeness definition in [CGZ24].\n6 Publicly-Detectable Watermarking for Language Models\nDefinition 8(Soundness/Unforgeability). A PDWS is δs-sound if any adversaryAcannot\ngenerate a watermarked text given the public detection key and any polynomial number\nof genuinely-watermarked texts. Formally, the following must be satisfied:\nPr\n[\nDetectpk(t∗) = true ∧\nnon_overlappingk(t∗,t1,t2,... ) = true : (sk,pk) ←Setup(1λ)\nt∗←AWatermarksk(·)(pk)\n]\n≤negl(λ).\nHere, the adversary is allowed to make a polynomial number of queries to the ora-\ncle Watermarksk(·). We use t1,t2,... to denote the watermarked text that the adver-\nsary receives as output when she queries the model Watermarksk(·). The predicate\nnon_overlappingδs(t∗,t1,t2,... ) outputs true if t∗ does not share a δs-length window\nof tokens with any of the genuinely-watermarked textst1,t2,... and outputsfalse other-\nwise.\nOn the unforgeability of our schemeIntuitively, our soundness definition says the\nfollowing. If the adversary manages to output a textt∗that is labeled as watermarked, it\nmust be the case that she copied a sufficiently long sequence of tokens from the genuinely-\nwatermarked texts she received from the model (i.e.,t1,t2,... ). This implies that any\nattempted forgery of a watermarked message must contain an overwhelming portion of\ntokens from genuine watermarked text. We emphasize that this notion of unforgeability is\nparametrized(by the overlapping lengthδs). Intuitively, the largerδs is the more sound\nour scheme is. Looking ahead, our main construction is flexible in that, for any desired\noverlapping parameterδs, our construction can be adapted to meet the corresponding\nsoundness guarantee.\nDefinition 9(Robustness). A publicly-detectable watermarking scheme isδr-robust if,\nfor every promptρand security parameterλ,\nPr\n[\nDetectpk(A(t)) = false : (sk,pk) ←Setup(1λ)\nt←Watermarksk(ρ)\n]\n≤negl(λ)\nwhere the adversary is allowed to transform the input textthowever she pleases so long\nas aδr-length contiguous sequence of tokens remains. Formally, lett∗be the adversarially-\nmodified text (i.e.t∗←A(t)). Then, there must exist aδr-length window of tokens int∗\nthat exactly matches aδr-length window int.\nIntuitively, the robustness definition claims that as long as aδr-length contiguous\nsequence of tokens is preserved, the watermarked is also preserved.\nThe relationship betweenδs, δc, andδr We remark that it must be thatδs ≤δc ≤δr.\nIntuitively, the watermarking scheme requiresδc tokens to embed a watermark. Anyδr\nconsecutive tokens are guaranteed to contain a segment ofδc tokens that embeds the\nwatermark. Additionally, any adversary who forges an accepting watermarked text must\ncopy a segment of≥δs tokens from the observed watermarked test.\nDefinition 10(Distortion-freeness). A PDWS is (computationally)ϵ-distortion-free if,\nfor all PPT distinguishersD,\n⏐⏐⏐Pr\n[\nDModel,GenModel(1λ) →1\n]\n− Pr\n(sk,pk)←Setup(1λ)\n[\nDModel,Watermarksk (1λ) →1\n]⏐⏐⏐≤ϵ.\nThis means distortion-freeness ensures that the watermarking algorithm does not noticeably\nchange the quality of the model output, i.e., without the public detection key, no PPT\nmachine can distinguish plain LM output from watermarked LM output. Again, this\ndefinition is the public-key analogue of Christ, Gunn, and Zamir [CGZ24]’s undetectability\ndefinition. Moreover, we denote it as distortion-freeness to avoid confusion with public-\ndetectability.\nFairoze et al. 7\n3 Protocol\n3.1 Technical Overview\nℓm ℓc·λc\nt1 ··· tℓm\nH1(t),H2(t)\nh1,h2\n1. Extract\nsc1,1\n1,1 ···sc1,ℓc\n1,ℓc ··· scλc,1\nλc,1 ···scλc,ℓc\nλc,ℓc\n3. Plant\n∀i∈λc,H3\n(\nsci,1\ni,1 ···sci,ℓ\ni,ℓ\n)\n=ci\nc←h2 ⊕Encodeγ(Signsk(h1))\n2. Sign, encode, and randomize\nFigure 1: Our core gadget. Embedding is a three-step process as designated by (1) through\n(3). First (1),ℓm tokens are sampled natively from the LM. These tokenstare hashed\ntwice with two different hash functions, producingh1 ←H1(t) and h2 ←H2(t). Second\n(2), h2 is signed with the secret keysk, error-corrected, and randomized withh1. The final\nproduct is a pseudorandom bitstringc←h2 ⊕Encodeγ(Signsk(h1)). Lastly (3), each bit\nci the randomized codeword is embedded into the nextℓc tokens by rejection sampling.\nThat is, thei-th block ofℓc tokens are sampled such that the hash of the block yields the\ni-th bit of the randomized codeword, i.e.,∀i∈λc,H3\n(\nsci,1\ni,1 ···sci,ℓc\ni,ℓc\n)\n= ci where eachs is\none token.\nWe first give an overview of the key ideas in our construction before expanding on\nspecifics of the scheme in the remainder of this section. Refer to Figure 1 for a visual\nrepresentation.\nLet t:= t1,t2,...,t ℓ be bit samples from probability distributionsp1,p2,...,p ℓ where\neach pi is a probability distribution from an auto-regressive model. Our scheme assumes\nthat any consecutiveℓ tokens output by the language model contain sufficient entropy\n(formally captured by Assumption 1). Hence, we know that∑ℓ\ni=1 −ln pi(ti) ≥α for some\nreasonably largeα. That is, theℓ tokens were sampled from distributions with at least\nα cumulative bits of entropy. Lett denote the firstℓ tokens sampled from the model\n(denote tas the message) and letc←h2 ⊕Encodeγ(Signsk(h1)) where h1 ←H1(t) and\nh2 ←H2(t).2 We can embed theλc-bit codeword c = c1,c2,...,c λc in a contiguous\nsequence of tokens from the auto-regressive model as follows: for each of the nextℓ·λc\ntokens sampled from the model, ensure that thei-th block of ℓ tokens hashes to the\ncorresponding i-th bit inc, i.e.,H3(ti+1,ti+2,...,t i+ℓ) = ci for i∈[λc]. After this process,\na complete message-signature pair is embedded into a contiguous sequence of generated\ntokens. We remark that without knowledge of the public key, our watermarked output is\n(computationally) indistinguishable from the original output: as long as there is sufficient\nentropy at generation time, no PPT algorithm can tell if a text completion came from the\nwatermarking algorithm or the plain algorithm.\nTo detect the presence of a watermark, the detector needs to recover the message-\nsignature pair. The detector first recovers the messaget by looking at the firstℓ to-\nkens. Next, the detector recovers each bit of the signature codeword by computing\nci = H3(ti+1,ti+2,...,t i+ℓ) for i∈[λc] and letc= (c1,...,c λc). It can then decode and\nverify the signature by computingVerifypk(H1(t),Decodeγ(H2(t) ⊕c)) using the public\nverification key: if the signature verifies, then the text was watermarked.\n2Here, H1, H2, and H3 are cryptographic hash functions with different image lengths,Sign is the\nsigning algorithm of a digital signature scheme, andEncode is the encoding algorithm of an error-correcting\ncode (refer to Section 2.1).\n8 Publicly-Detectable Watermarking for Language Models\nAlgorithm 2Setup\n1: input: 1λ\n2: r\n$\n←{0,1}λ\n3: sk,pk ←Gen(1λ)\n4: output: (sk,r),(pk,r)\nSetup is the watermark key generation algorithm. It produces a secret keysk and public\nkey pk obtained by runningGen, a key generation algorithm for a digital signature scheme.\nAdditionally,Setup also returns a random stringr—this string will seed the hash functions\nto ensure distortion-freeness.\nDealing with low entropy sequencesAs in the private key setting, our protocol needs\nto handle sequences with limited entropy. Kaptchuk, Jois, Green, and Rubin [KJGR21]\nprovide an illustrative example: given the inputs “The largest carnivore of the Cretaceous\nperiod was the Tyrannosaurus,” the next token is almost certainly going to be “Rex.”\nAssuming that “Rex” is a whole token and it does not hash to the desired bit, text\ngeneration cannot continue.\nWeovercomethisproblembyleveragingstandarderrorcorrection. Insteadofembedding\nσ:= Signsk(H1(t)) directly, we can instead embedc := Encodeγ(σ) where c is a codeword\nof lengthλc >λσ that allows for correction of up toγ errors. Now, at generation time, we\ncan tolerate up toγ periods of low entropy—when such a scenario is encountered, we can\nplant tokens that do not satisfy the rejection sampling condition. At detection time, we\ncan correct these planted errors so long as they do not exceed the maximum amountγ.\nLastly, the codewordcis no longer guaranteed to be pseudorandom—this can be addressed\neasily by re-randomizing the codeword with a pseudorandom one-time padH2(t).\nWe now present an(ℓ,ℓ+ℓ·λc,2(ℓ+ℓ·λc),exp(−Ω(α)))-publicly-detectable watermark\nin Algorithm 2, Algorithm 3, and Algorithm 6. Prior to watermarking or detection, the\nSetup algorithm is used to initialize the secret key(sk,r) and public key(pk,r) where sk\nand pk are generated by the native signature key generation algorithm andris a uniformly\nrandom string. We now describe the watermarking and detection algorithms in detail.\n3.2 Private Generation Algorithm\nWe present our watermarking scheme in Algorithm 3. The core idea is to embed a message\nand a corresponding publicly-verifiable signature in the generated text. The message-\nsignature pair should be extractable during detection. Once extracted, it can be verified\nusing the public key.\nTo explain our scheme, we describe how to embed one message-signature pair in LM\noutput—the construction can be applied repeatedly to generate arbitrarily long LM output\n(i.e., Line 4 in Algorithm 3). Refer to Figure 1 for a simplified visual presentation of the\nconstruction.\nTo perform watermarking, the first step is to sample a fixed number of tokens such that\nthe entropy used at generation time to produce those tokens is sufficient for watermarking.\nThis is captured in Line 2 of Algorithm 4. By Assumption 1, we know thatℓ tokens were\nsampled from distributions with at leastα bits of entropy. Denote theseℓ tokens as the\nmessage t. Once thas been sampled, it is hashed, signed, and error-corrected (Lines 3-4).\nNow, any error-correcting codeword is not a pseudorandom string; therefore, directly\nembedding a codeword distorts the distribution of the output. However, we can regain\npseudorandomness by using the message hash as a one-time pad to mask the codeword.\nSpecifically, we encodec := H2(r ∥t) ⊕Encode(σ) where H2(r ∥·) is a different hash\nfunction than the one used to originally hash the message sinceH1(r ∥·) and H2(r ∥·)\nFairoze et al. 9\nAlgorithm 3Watermark\n1: constants: (sk,r), n, ℓ, λc, β, amax, γmax\n2: input: ρ\n3: t←ϵ\n4: while |t|+ (ℓ+ ℓ·λc) <n do\n5: t←GenerateMessageSignaturePair(ρ,t)\n6: if |t|<n then\n7: t←t∥GenModeln−|t|(ρ,t)\n8: output: t\nWatermark is the main watermarking algorithm. It generates a text completion for input\nprompt ρconsisting ofn watermarked tokens.\nAlgorithm 4GenerateMessageSignaturePair\n1: input: ρ, t\n2: t←t∥GenModelℓ(ρ,t)\n3: σ←Signsk(H1(r∥t[−ℓ:]))\n4: c ←H2(r∥t[−ℓ:]) ⊕Encodeγ(σ)\n5: m,cprev ←ϵ,ϵ\n6: γ ←0\n7: while c ̸= ϵ do\n8: c,c ←c[0 : β],c[β :]\n9: t,m,cprev ←RejectSampleTokens(c,t,m,cprev)\n10: output: t\nGenerateMessageSignaturePair plants the message-signature pair gadget intoℓ+ℓ·λc tokens.\nFirst, theℓ-length message is sampled naively from the underlying model and the error-\ncorrected signaturec is computed. c is then iteratively embedded intoℓ·λc tokens using\nrejection sampling.\nAlgorithm 5RejectSampleTokens\n1: input: c,t,m,cprev\n2: a←0\n3: xbest,dbest ←ϵ,∞\n4: repeat\n5: x ←GenModelℓ(ρ,t)\n6: a←a+ 1\n7: d←Hamming(H3(r∥m ∥x ∥cprev),c)\n8: if d<d best then\n9: dbest,xbest ←d,x\n10: if (a>a max ∧ γ <γmax) then\n11: x ←xbest\n12: γ ←γ+ 1\n13: break\n14: until H3(r∥m ∥x ∥cprev) = c\n15: m ←m ∥x\n16: t←t∥x\n17: cprev ←cprev ∥c\n18: output: t,m,cprev\nRejectSampleTokens controls the rejection sampling loop. It generatesℓ tokens such that\neach contiguous block ofℓ tokens encodesc: one bit of information.\n10 Publicly-Detectable Watermarking for Language Models\nAlgorithm 6Detect\n1: input: (pk,r), n, ℓ, λc, β, γ, t′\n2: for i∈{0,...,n −(ℓ+ ℓ·λc)}do\n3: t←H1(r∥t′[i: i+ ℓ])\n4: m,c ←ϵ,ϵ\n5: for j ∈{0,...,λ c −1}do\n6: m ←m ∥t′[(i+ ℓ+ 1) + (j·ℓ) : (i+ ℓ+ 1) + ((j+ 1) ·ℓ)]\n7: c ←c ∥H3(r∥m ∥c)\n8: σ←Decodeγ(H2(r∥t′[i: i+ ℓ]) ⊕c)\n9: if Verifypk(t,σ) = true then\n10: output: true\n11: output: false\nDetect is the watermark detection algorithm. Given potentially watermarked textt′, it\nexhaustively searches for an embedded message-signature pair that passes authentication.\nIf one such pair is found, the input text is flagged as watermarked.\nmap to a different range of bits. Note thatr serves as a seed for bothH1 and H2—this\nensures that without the public-key(pk,r), the output of the hashes (modeled as public\nrandom oracles) are unpredictable.\nOnce the pseudorandom signature codewordc is computed, the next step is to embed\nit into natural language. The key idea is to embed bits into a block of tokens such that the\nblock of tokens hashes to the target bit. In particular, the construction embedsβ bits into\neach ℓtokens. In Lines 4-14 in Algorithm 5, we sampleℓmore tokens using the native LM\ndecoder and check if there is a hash collision. In particular, we tryamax times to find the\nbest nextℓ tokens that hash to the nextβ embedded bits, where optimality is measured\nby Hamming distance. Note that the hash depends on all previous inputs to hashes for the\ncurrent signature codeword. Once we find the optimal output, we accept the token block\nand move on to the nextβ bits of the signature codeword. Otherwise, reject the tokens\nand freshly sample a new block of lengthℓ. At the end of the rejection sampling process,\nthe signature will be embedded inℓ·λc tokens whereλc is the length of the signature\ncodeword—one message-signature pair is embedded in generated text. This process can be\nrepeated to embed multiple pairs for added resilience.\n3.3 Public Detection Algorithm\nTo detect if a watermark is present in candidate text, it suffices to extract one message-\nsignature pair and verify it using the public key. In Line 2 in Algorithm 6, we iterate over\nall potential token blocks of lengthℓ (adjusting byλc = λσ\nβ to account for the signature\ncodeword length). Once the messagetis assigned, the signature is iteratively reconstructed\nin Lines 5-8. Notably, since we employ an error-correcting code to handle the cases where\nthe entropy is low to embed bits, we must invoke the error-correction algorithm to correctly\ndecode the signature embedded in the (potentially) erroneous codeword. This is exactly\nwhat Line 8 does. If the signature verifies, we know with overwhelming probability the\ntext was watermarked (See Lemma 2). Otherwise, move on to the next candidate block\nand try again. If no message-signature pair is verified, we conclude that the text was not\nwatermarked (See Lemma 3)\n3.4 Formal Guarantees of Our Construction\nWe will prove the following theorem:\nFairoze et al. 11\nTheorem 1.The schemePDWS defined in Algorithms 2, 3 and 6 is an(ℓ,ℓ + ℓ·λc,2(ℓ+\nℓ·λc),exp(−Ω(α)))-publicly-detectable watermark.\nProof of Theorem 1 follows immediately from the proof of Lemmas 1 to 4. Before\nproving each lemma below, we briefly discuss our model. Our construction uses random\noracle Oto model a cryptographic hash function H. A random oracle is a random\nfunction drawn uniformly randomly from the set of all possible functions (over specific\ninput and output domains). Random oracle models are commonly used in cryptographic\nconstruction [BR93]. Constructions that are provably secure in the random oracle model\nare heuristically assumed to be also secure when one instantiates the random oracleO\nwith a cryptographic hash functionH. We useOand H interchangeably in the proof.\nAssume that each block ofℓ tokens has at leastα bits of entropy. We model the\ncryptographic hashH(·) as a random oracle, denotedO(·). Without loss of generality,\nthe proof is written for the case ofβ = 1, i.e., we embed one bit into eachℓ tokens. It\ngeneralizes to anyβ.\nLemma 1(Distortion-freeness). Let H2 and H3 be random oracles.PDWS is a compu-\ntationally distortion-free publicly-detectable watermarking scheme assuming everyℓ tokens\ngenerated by the LM containsα bits of entropy.\nProof. Our proof relies on the following claim.\nClaim (Balanced Partition). Let Dbe any distribution with min-entropy≥α. It holds\nthat\nPr\nH3\n[⏐⏐⏐Pr\nD\n[H3(D) = 1] −1/2\n⏐⏐⏐≥1\n2 ·√α·2−α/2\n]\n≤2 ·2−α.\nThat is, a randomly sampled hash functionH3 will result in a balanced bi-partitionH−1\n3 (0)\nand H−1\n3 (1) on the support ofDwith overwhelming probability.\nProof of Claim 3.4.Since Dcontains at leastα bits of min-entropy, the support ofD\ncontains at least2α elements; let us denote them byx1,...,x u, whereu≥2α. Let Xi be\nthe random variable defined as\nXi =\n{\nPr [D= xi] when H3(xi) = 1\n0 when H3(xi) = 0 .\nClearly, Pr\nD\n[H3(D) = 1] = ∑u\ni=1 Xi. Observe thatXi are independent random variables\nsatisfying 0 ≤Xi ≤2−α. By the Hoeffiding inequality (Theorem 2), we have\nPr\nH3\n[⏐⏐⏐Pr\nD\n[H3(D) = 1] −1/2\n⏐⏐⏐≥1\n2 ·√α·2−α/2\n]\n≤2 ·2−α·2−α\n2−α = 2 ·2−α.\nHere, we use the fact that∑\ni(bi−ai)2 = ∑\nib2\ni ≤(maxibi) ·∑\nibi = maxibi ≤2−α. This\ncompletes the proof.\nNow we proceed to prove our theorem. The only difference between our sampling\nalgorithm and the original sampling algorithm is the following: The original sampling\nalgorithm (i.e., multinomial sampling) samples the nextℓ tokens directly from some\ndistribution D. Our sampling algorithm first samples a bitb and then samples the next\nbatch ofℓtokens according toD, but conditioned on that its hash is consistent withb. We\njust need to prove that these two sampling processes are computationally indistinguishable.\nBy the randomness of the output of the random oracleH2, each embedded bitb is\ncomputationally indistinguishable from a truly random bit. Therefore, it suffices to prove\nthat Dis close to first sampling a truly random bitband then sampling fromDconditioned\non the hash beingb.\n12 Publicly-Detectable Watermarking for Language Models\nObserve that, ifH−1\n3 (0) and H−1\n3 (1) gives aperfectbalanced partition on the support\nof D(i.e., the probability of the hash of a sample fromDis perfectly uniformly random),\nthen these two ways of sampling fromDis identical. Now, our Claim 3.4 states that, for\nevery ℓ tokens that the LM outputs, as long as it containsα bits of entropy, a randomly\nsampled hash functionH3 will not give a well-balanced partition withexponentially small\nprobability. Suppose the LM outputs a total ofm sets ofℓ tokens. By a simple union\nbound over all such sets, a randomly sampled hash functionH3 will give a well-balanced\npartition onall these m distributions with probability1 −m·exp(−Ω(α)). Conditioned\non that hash function gives a well-balanced distribution on all thesem distributions, the\ndistribution that our sampling process gives and the distribution that the original LM\noutputs are indeedexp(−Ω(α))-close by our Claim 3.4.\nThis completes the proof that our sampling process is computationally indistinguishable\nfrom the original sampling process; hence, our scheme is computationallyϵ-distortion-free,\nwhere ϵ= exp(−Ω(α)).\nLemma 2(Completeness). PDWS is a(ℓ+ ℓ·λc)-complete publicly detectable water-\nmarking scheme.\nProof. For any long enough outputt, it is easy to see that if the watermarking scheme\nsuccessfully embeds in a message/signature pair, the detection algorithm will mark the\ntext as “watermarked”. The only possibility that the watermarking fails is if the rejection\nsampling algorithm fails to find the next batch of tokens whose hash is consistent with the\ntarget bit.\nFor anyℓ consecutive tokens that containα bits of entropy, by our analysis of the\ndistortion-freeness, each sampling of the next batch of tokens will have a uniformly random\nhash bit. Consequently, each sampling attempt will succeed in finding a consistent hash\nwith probability 1/2. Afterλc attempts, our rejection sampling will find the next batch of\ntokens with probability1 −2−λc.\nAdditionally, if there exists a fewℓ consecutive tokens that does not contain enough\nentropy, our watermarking scheme can also handle these by error correction. Namely, our\nembedding algorithm will stop trying to embed the given bit at those locations and simply\nembed an arbitrary bit. As long as the number of such occurrences is fewer thanγmax the\nmaximum number of errors that we can error-correct, the detection algorithm will still be\nable to recover the signature and output successful detection. This brings the final success\nprobability to1 −2−(λc+γmax).\nLemma 3(Soundness). PDWS is anℓ-sound publicly-detectable watermarking scheme.\nProof. The soundness of our watermarking scheme is based on the unforgeability of\nthe signature scheme—if there exists a PPT adversary that can find a text labeled as\nwatermarked, it must mean that this watermarked text has a valid message/signature pair\nembedded inside. Then, one may extract this pair, which constitutes a forgery attack\nagainst the underlying signature scheme.\nMore formally, given an adversaryAthat breaks theℓ-soundness of our watermarking\nscheme, we will reduce it to an adversaryA′that breaks the unforgeability of the underlying\nsignature scheme. A′simulates LLM watermarking oracle withAfrom the watermarking\nsoundness game. A′will rely on the external signing oracle to obtain new signatures. In\nthe end, with a non-negligible probability,Awill break theℓ-soundness. This has two\nimplications. First, Aoutputs a watermarked text and, by definition, the watermarking\ndetector successfully extracts a valid(msg,sig) pair from the forgery. Second, any ℓ-\nsubstring ofA’s output is not a substring of any of the watermarked texts that she received\nper her query. Sincemsg is always an output of a random oracle onℓ consecutive tokens,\nthis means (w.h.p.) that themsg from (msg,sig) pair extracted must be a new message\nthat has not appeared in any ofA′’s oracle queries. This breaks the unforgeability of the\nunderlying signature scheme.\nFairoze et al. 13\nLemma 4(Robustness). PDWS is an2(ℓ+ℓ·λc)-robust publicly detectable watermarking\nscheme.\nProof. The robustness of our scheme is rather easy to see. Lett be the output of the\nLM. If the adversary’s outputA(t) contains 2δ consecutive tokens from the originalt, it\nmust mean that there is aδ consecutive tokens, which embeds a message/signature pair,\nis preserved inA(t). The detection algorithm will recover this consecutive sequence by an\nexhaustive search, resulting in a successful detection output.\n4 Empirical Evaluation\nWe implement both our publicly-detectable protocol and Christ, Gunn, and Zamir\n[CGZ24]’s privately-detectable protocol—the only schemes with both cryptographic de-\ntectability and distortion-freeness at the time of writing. Our full source code is available\nat: https://github.com/jfairoze/publicly-detectable-watermark . We focus our\nevaluation on assessing whether distortion-freeness is met in practice. In particular, we\nneed to verify that Assumption 1 on min-entropy is realistic. Note that our other formal\nproperties, detectability and weak robustness, are immediate from our construction: de-\ntectability is inherited from the underlying signature scheme (BLS signatures [BLS01]), and\nweak robustness follows from the fact that the adversary does can only destroy all-but-one\nmessage-signature pairs, meaning that at least one message-signature pair is extractable at\ndetection time. We additionally evaluate real-world performance under varying conditions.\nConcretely, we (a) present a range of generation examples for varying parameters in our\nprotocol alongside examples from the other protocols, (b) quantify the distortion-freeness of\nthe text completions using GPT-4 Turbo as a judge, (c) measure generation times against\nbaseline (plain generation without any watermarking) and detection times for Christ,\nGunn, and Zamir [CGZ24] and our protocol, and (d) compare generation times for varying\nparameters in our protocol.\nHereafter, we will refer to the four generation algorithms using the following aliases:\n1. plain. Standard text decoding.\n2. plain with bits. Standard text decoding but with the arbitrary-to-binary vocabulary\nreduction of Christ, Gunn, and Zamir [CGZ24] applied.\n3. symmetric. The base (non-substring complete) version of the Christ, Gunn, and\nZamir [CGZ24] private key watermarking protocol.\n4. asymmetric. Our public key watermarking protocol. We vary our protocol over\nthree parameters: signature segment lengthℓ, bit sizeβ, and planted error limitγ.\nFollowing prior watermarking evaluations, we use samples from the news-like subset\nof the C4 dateset [RSR+20] as prompts. We implement our publicly detectable protocol\nand the base (non-substring-complete) version of the Christ, Gunn, and Zamir [CGZ24]\nsymmetric protocol. For asymmetric, cryptographic keys are sampled fresh for each\nparameter configuration, and we embed a single message-signature pair into the generated\ntext. For symmetric, the key is fixed throughout. Our implementation is written in\nPython 3 with PyTorch [PGM+19] and wraps around the Hugging Facetransformers\ninterface for transformer models [Fac23]. We focus on the openly available Mistral 7B\nmodel [JSM+23] for quality analysis. We additionally provide examples from the semi-open\nLlama 2 [TMS+23] 70B and 13B models in Table 2 and Table 3, respectively. We use\nthe 2.7B parameter OPT model [ZRG+22] for runtime analysis. Refer to Section C for\nextensive completion examples.\nBenchmarking procedure. The benchmarking script selects a fixed number of\nprompts at random from the C4 dataset, skipping prompts that mention specific products.\n14 Publicly-Detectable Watermarking for Language Models\nWe runasymmetric first, then use the number of tokens from the execution in the subsequent\nalgorithms. This ensures that all algorithms produce the same number of tokens. We force\ngeneration length to be as long as needed to encode the signature, i.e., we explicitly block\nthe model from outputting the stop token before the signature is embedded.\nEmbedding in characters instead of tokens.Note that throughout the paper we\nhave discussed embedding the signature intokens for simplicity and alignment with prior\nwork. However, in our implementation, we plant the watermark directly on plain text\nrather than tokens to avoid inconsistencies in encoding then decoding (or vice versa) using\nany given tokenizer: tokenizers do not guarantee that encoding the same string twice will\noutput the same tokens. Thus, theℓ= 16 and 32 in our subsequent discussion and figures\nwill denotecharacters, not tokens.\nConcrete parameters. When ℓ = 32 and β = 2 , our gadget was embedded in\n≈ 2,000 tokens during the experiment. When ℓ = 16 and β = 1 or 2, the gadget\nwas embedded in≈1,000 tokens. For either case, λ = 328 or 360 bits of data were\nembedded depending on ifγ = 0 or 2. Consequently, this implies that we instantiate\n(ℓ,ℓ + ℓ·λc,2(ℓ+ ℓ·λc),exp(−Ω(α)))-publicly-detectable watermarks forℓ∈{16,32}and\nλc ∈{328,360}.\nTable 1: Example completions from Mistral 7B [JSM+23]. For the Christ, Gunn, and\nZamir [CGZ24] scheme, we set security parameterλ= 16. For our scheme, we set signature\nsegment lengthℓ = 16, bit sizeb = 2, and maximum number of planted errorsγ = 2.\nCompletions are truncated to the first 200 characters. See Table 5 for more completions\nunder the same conditions.\n# Prompt Plain (tokens) Plain (bits) Christ et al. This work\n1 Windthorst pulled offa sweep of CollinsvilleTuesday, while ArcherCity and Hollidaywere unable to ad-vance.\\nCHICO — Witha chance to square offagainst the defendingchamps later this week,No. 7 Windthorst tookcare of business Tuesdaynight.\\nThe Trojanettes\ndefeated Collinsville 25-16, 25-18, 23-25, 25-21 tosecure an area title andpunched their ticket tothe Region I-2A quarter-finals with a trip to thecontroversial West Texastown of Iredell on the li\n, seeded second, sweptCollinsville in threegames to set up aquarterfinal date withtop-seeded Trent.Windthorst defeatedCollinsville 25-16, 25-17and 25-20.\\nAssumptionsqueaked past Brock,22-25,\nswept Collinsville to ad-vance to the area roundof the postseason andface Olney. They’rehoping to turn the ta-bles on the Ladycatsfor whom they lost inthe second round a yearago.\\n“They beat us la\ntraveled to No. 10Collinsville and sweptthe Lady Collie Cardi-nals in game one, set-ting up a chance totake on Amarillo RiverRoad later this week ina highly-anticipated Re-gion I-2A area roundrema\n2 Eight Sussex skiers willtake to the slopes tobattle it out for thehonour of being crownedNational Championat the English AlpineSki Championshipswhich start this week-end.\\nBurgess Hillsisters, 18-year-old Samand 16-year-old HelenTodd-Saunders andCrawley\n/Fernhill Heath duo,Martin Mellon andOliver James will donnational kit to competein Giant Slalom (GS)and Slalom (SL) eventsin Yongpyong, SouthKorea.\\nPerformances in14 events will be rankedto\n’s Amy Mertens, just 13,will go head-to-head withsome of the country’sbest racers at the widevariety of disciplines onoffer.\\nWith almost 20races in multiple disci-plines it is the most di-verse s\n’s Callum Adams (18),Ross Guest (25) and MaxGreen (17) all make upthe squad for the cham-pionships.\\nWorthing’sDanny Williams (17),Syd Wilson (16) andEast Grinstead’s NaomiWilkinson (21) also re\n’s Amy Crocket willall compete in the Se-nior National Champi-onships, racing at theTrois Vallees ski resort inthe French Alps, alongwith Booker’s Mollie Dar-ling and Dylan Jetsun;Rye’s William Phil\n4.1 Text Completion Examples\nWe show how text completions vary over six benchmarking runs with different generation\nparameters. We primarily use the Mistral 7B model due to its high quality output for its\nsize class. We display a couple text completions for each algorithm in Table 1. See Table 4\nthrough Table 9 in Section C for the full collection of text completions—each table shows\none text completion per generation algorithm for 5 distinct prompts. We additionally\ninclude a few completion examples from larger models (Llama 2 70B and 13B) in Table 2\nand Table 3. In the next section, we discuss the quality of these examples.\nFairoze et al. 15\nFigure 2:Aggregated text quality score assignments from GPT-4 Turbo for each generation\nalgorithm configuration over the Mistral 7B model [JSM+23]. Forasymmetric, the configu-\nrations from left to right represent the most compact (lowest quality) to least compact\n(highest quality) parameters. Each bar is the aggregation of GPT-4 Turbo-assigned quality\nscores for 250 distinct prompt completions. The error bars show the 95% interval data\nspread. Observe that no protocol clearly outperforms the others: the mean score falls\nbetween 27 and 40 for all protocols, and each one exhibits large quality spreads. Note\nthat even the baseline decoder,plain, follows this pattern. This suggests the watermarking\nprotocols are indeed distortion-free.\n4.2 Zero-Shot Quality Measurements with GPT-4 Turbo\nFollowing many works in the NLP literature (e.g., [CWJ+23; PSF+23], we automatically\nassign a quality score to each text completion using an established LM. We do not use\nmodel perplexity as it is known to assign arbitrary scores in some cases—for example, it\ncan favor repetitive text [HBD+19; WKR+19; PSF+23]. In particular, we use zero-shot\nprompting of GPT-4 Turbo [Ope23]. For each batch of four generations (one from each\nalgorithm), our prompt template asks the model to: (a) rate the text completion by giving\nit a score from 0 (worst) to 100 (best), and (b) give reasoning for the assigned score in list\nform.\nIn theory, all the algorithms should be computationally distortion-free if their underlying\nassumptions are satisfied. Recall distorion-free means no PPT algorithm can distinguish\nbetween watermarked and non-watermarked text. We see in Figure 2 that GPT-4 Turbo-\nassigned scores have similar means and high variance—there is no statistically-significant\nsignal that any particular generation algorithm outperforms the others. This provides\nevidence toward real-world distortion-freeness.\nOn embedding compactness.One of the main limitations of our protocol is that it\ntakes a relatively large number of characters (tokens) to embed the message-signature pair\n(greater than 1,000 tokens for our most compact parameter configuration whereℓ= 16,\nβ = 2). Our GPT-4 Turbo quality scores are comparable to theplain baseline even for the\nmost compact parameters, suggesting that we can encode more information in even less\ntokens at the cost of increased runtime. That is, we can decreaseℓ(holding β constant) or\nincrease β (holding ℓ constant).\n4.3 Generation and Detection Runtimes\nIn this section, we discuss the generation and detection runtimes shown in Figure 3.\nText generation. plain generation without any watermarking or bit reduction is,\nas expected, the fastest—we use this setting as our control against which we compare\nthe performance of the watermarking schemes.plain with bitsand symmetric are closely\n16 Publicly-Detectable Watermarking for Language Models\nFigure 3:Generation and detection runtimes for each generation algorithm over the Mistral\n7B model [JSM+23]. Five distinct generations were aggregated for each of the 10 random\nprompts from the news-like portion of the C4 dataset [RSR+20]. The error bars show the\n95% interval data spread. On average, the fastest to slowest generation runtimes were\nfor: plain (expected as this is the baseline),asymmetric, thensymmetric and plain with bits\n(the latter two are about equal with the dominant cost being the reduction to a binary\nvocabulary). For detection,asymmetric runs much faster thansymmetric which is expected\ngiven they run in linear vs. quadratic time, respectively, in the number of tokensn.\ncorrelated, implying that the dominating cost of Christ, Gunn, and Zamir [CGZ24]’s\nwatermarking scheme is the arbitrary-to-binary vocabulary reduction. Theasymmetric\nscheme ran approximately twice as fast on each prompt for the parameters we used in this\nexperiment. We note that both watermarking methods are implemented without advanced\noptimization; therefore, the concrete time measurements should be interpreted relative\nto theplain baseline. In particular, theasymmetric protocol’s rejection sampling loop is\nparallelizable—this would drastically improve generation times (see Section B.3).\nWatermark detection.We seeasymmetric detection runs significantly faster than\nsymmetric detection. asymmetric consistently runs near 0.01s andsymmetric varies between\n10 and 10,000s for Mistral 7B. This aligns with performance expectations. Detecting an\nasymmetric watermark takes constant time in our implementation because we know the\nstarting index of the signature. Note that in extensions of this implementation where the\nlocation of the signature in the text is unknown, detecting an asymmetric watermark would\ntake linear time in the generated text lengthn using a sliding window (see Section B.1).\nOn the other hand, detecting a symmetric watermark runs in quadratic time based on the\ngenerated text lengthn [CGZ24].\n4.4 Asymmetric Parameters Optimized for Runtime\nExpected generation time is proportional to the average number of characters needed to\nencode the watermark:Eλ(ℓ,β) = 2β ·λ\nβ ·ℓ, where2β is the expected number of attempts\nto pass the rejection sampling,λ\nβ the number of signature segments required, andℓ is\nthe number of characters per signature segment. Holding all else constant, observe that\n(a) higherℓ means more entropic flexibility but increases runtime by a linear factor, and\n(b) higherβ also means more entropic flexibility but increases runtime by a factor of2β\nβ .\nWe see in Figure 4 that the empirical results line up with the expectations above\nwhen comparing the runtimes across differing parameters. Runtimes forℓ = 16,β = 1\nand ℓ = 16,β = 2 are close to each other and approximately double the runtimes for\nℓ= 32,β = 2.\nFairoze et al. 17\nFigure 4: Generation runtimes for each variant of our protocol over the OPT-2.7B\nmodel [ZRG+22]. Generation runtimes for different parameter instantiations of our\nprotocol. 5 completions were generated for each of the 10 random prompts from the news-\nlike portion of the C4 dataset [RSR+20]. ℓdenotes the signature segment length,β denotes\nthe bit size, andγ denotes the maximum number of planted errors. The error bars show\nthe 95% interval data spread. Comparing non-error-corrected (γ = 0) vs. error-corrected\n(γ = 2) runtimes for prompts with high variance (prompts 4, 5, 6, 8, and 9 for parameters\nℓ= 32, β= 2 and ℓ= 16, β= 2), we can see a clear reduction in the variance and mean\nruntime when error correction is applied to overcome low entropy periods. Note that we\nexpect to sampleEλ(ℓ,β) = 2β ·λ\nβ ·ℓ characters to embed the signature codeword. Thus,\nin expectation,Eλ(16,1) = Eλ(16,2) <Eλ(32,2) where λ= 328 or 360 depending on if\nγ = 0 or 2. Our empirical runtimes align with this.\n4.5 The Effect of Error-Correction\nFigure 4 presents the generation time performance of our publicly-detectable protocol for\nsix different parameter settings. We see that across the board, allowing the algorithm to\nplant up toγ = 2 errors resolves high runtime spread. Generations that took a relatively\nlong time got stuck in the rejection sampling loop trying to find a hash collision. The\nversion of our protocol that plants errors was designed to allow the algorithm to break\nout of this loop by settling for the closest hash (as measured by Hamming distance to\nthe target bit sequence). In particular, we see significant reductions in runtime spread for\nprompts 4, 5, 6, 8, and 9.\n5 Related Work\n5.1 Text Distinguishers\nWe discuss key approaches for detecting AI-generated text without introducing any changes\nto text generation. See [JAL20] for a comprehensive survey.\nEarly approaches to detecting AI-generated text revolve around looking for features\nof AI-generated text that are not present in human-generated text—if you can or cannot\nidentify such features, you can conclude the text was or was not AI-generated. Examples of\nfeatures include relative entropy scoring [LUY08], perplexity [Ber16], and other statistical\nsignals [GSR19]. We refer the reader to [Ber16] for a survey.\nAnother common method is to train another model to automatically identify distin-\nguishing features. Research of this nature [ZHR+19; MLK+23; GPT23; HAAL23] uses\ndeep learning as a binary classifier.\nThe problem with this idea is that it relies on AI-generated text being fundamentally\n18 Publicly-Detectable Watermarking for Language Models\ndifferent from human-generated text. This reliance is at odds with the core goal of LMs:\nto produce human-like text. As models get better, statistical features of AI-generated text\nwill decay. In particular, GPT-4 [Ope23] and other cutting edge models are quickly closing\nthis gap. Chakraborty, Bedi, Zhu, An, Manocha, and Huang [CBZ+23] formally show that\nas AI-generated text approaches human quality, text distinguishers demand longer text\nsamples.\nBeyondrelyingonandiminishingassumption, textdistinguisherslackformalguarantees—\nthe detector’s correctness is only empirically validated, so any validation performed is only\nrelevant to the exact model, its configuration, and prompting context during experimenta-\ntion.\nOther work has shown that it is possible to train models to transform text such that it\nfools text distinguishers [KSK+23; SKB+23].\n5.2 Watermarking Schemes\nThere is a recent line of work using ML to perform watermarking [AF21; QZL+23; YAJK23;\nMTDZ24; LPH+23]. Notably, Liu, Pan, Hu, Li, Wen, King, and Yu [LPH+23] address the\nsame problem as this paper: their approach is to train two models—one for embedding\na signal and one for detecting it. This is analogous to using asymmetric keys. Crucially,\nall schemes in this category are entirely empirical and have no formal guarantees such as\ncorrectness, soundness, or distortion-freeness.\nRecently, Kirchenbauer, Geiping, Wen, Katz, Miers, and Goldstein [KGW+23] gave the\nfirst watermarking scheme with formal guarantees. They showed that when model entropy\nis high, a watermark can be planted by hashing previous tokens to embed a watermark\nsignal in the next token. Crucially, hashing tokens to zero or one effectively assigns a binary\nlabel to potential next tokens. By ensuring that only tokens with label “zero” appear in\ngenerated text, the watermark can be detected after text generation by recomputing the\nhash. Kirchenbauer, Geiping, Wen, Katz, Miers, and Goldstein [KGW+23] bound the\ndistortions introduced by the watermark by measuring perplexity: the difference between\nthe distribution produced by the plain model and the one produced by the model with\nwatermarking.\nThe Gumbel softmax scheme of Aaronson [Aar23] is another approach to LM water-\nmarking. The scheme uses exponential minimum sampling to sample from the model using\nrandomness based on previous tokens (via hashing). This scheme is distortion-free so long\nas no two output texts that share a common substring are public [CGZ24]. This is unlikely\nfor a widely-used LM.\nKuditipudi, Thickstun, Hashimoto, and Liang [KTHL24] design a family of watermark-\ning schemes that aim to maximize robustness. The main idea of their scheme is to use a key\nthat is as large as the generated text output—this permits statistical distortion-freeness\nas opposed to the cryptographic distortion-freeness of this paper and Christ, Gunn, and\nZamir [CGZ24] at the cost of computation that scales with the generated text output.\nThe long key is then “aligned” with the generated text by computing an alignment cost—\nthis alignment cost can be (re)computed at detection time with the detection key and\nthe generated text. Text that has been watermarked will be statistically likely to have\nlow alignment cost at detection time. Their scheme has the desirable property that the\nalignment cost is a measure of edit distance and thus a watermark may persist even if text\nis inserted or deleted from the original watermarked text. Their scheme is not provably\ncomplete or sound.\nWe remark that prior watermarking schemes differ in trade-offs when token length\nincreases (or decreases). Schemes in the private key setting ([KGW+23; CGZ24; KTHL24])\ngain stronger soundness as token length increases and vice versa. In this work, strong\nsoundness is achieved so long as there are sufficiently many tokens to embed our message-\nsignature gadget, i.e., there is a sharp threshold. This is because soundness in our scheme\nFairoze et al. 19\nstems from unforgeability of the signature, rather than strength of an embedded statistical\nsignal.\nPiet, Sitawarin, Fang, Mu, and Wagner [PSF+23] systematically analyze watermarking\nschemes in the secret key setting. Their study focuses on assessing generation quality and\nrobustness to minor edits for practical protocol parameters. They state that the Kirchen-\nbauer, Geiping, Wen, Katz, Miers, and Goldstein [KGW+23] scheme produces the best\nwatermark even though the protocol is distortion inducing. Furthermore, they conclude\nthat distortion-freeness is too strong a property for practice. This conclusion was drawn\nfrom quality assessment performed by the chat version of Llama 2 7B [TMS+23]. We\nremark that Llama 2 7B’s quality assessment likely does not generalize—higher fidelity\nmodels may reveal weaknesses in distortion-inducing watermarking schemes. In contrast,\nno (probabilistic, polynomial time) algorithm can distinguish between non-watermarked\ntext and distortion-free watermarked text so long as protocol assumptions hold.\nZhang, Edelman, Francati, Venturi, Ateniese, and Barak [ZEF+24] formally proved\nthat “strong” robustness is impossible in watermarking schemes. The further demonstrated\nthat their attack works in practice against a range of secret-key watermarking schemes\n(including the Kuditipudi, Thickstun, Hashimoto, and Liang [KTHL24] scheme). That is,\nit is possible to remove watermarks with low computational effort whilst preserving text\nquality. Our scheme comes under their “weak watermarking scheme” definition and thus\ntheir impossibility result does not apply.\nQu, Yin, He, Zou, Tao, Jia, and Zhang [QYH+24] developed a watermarking scheme\nfor LLMs that also makes use of ECC. They use ECC to gain robustness: this is distinct\nfrom this work, which uses ECC to overcome low entropy periods when generating text.\n5.3 Linguistic Steganography\nThe main goal of linguistic steganography is to embed a hidden message in natural language\ntext. A steganographic protocol provides formal security if an adversary cannot determine\nwhether a message is from the original distribution or the distorted distribution that embeds\na hidden message [HLV02]. The key difference in this setting compared to watermarking\nis that distortions to the distribution are permitted so long as some notion of semantic\nsimilarity is preserved. Furthermore, there are critical differences in the problem model\nbetween linguistic steganography and LM watermarking. In LM watermarking, prompts\nare adversarially chosen and the watermarking protocol should be agnostic to the plain text\ndistribution. The focus of linguistic steganography is to achieve undetectability. In LM\nwatermarking, undetectability is not important—what is important is that the text is of\nsimilar (ideally, the same) quality as unwatermarked text, i.e., it should be distortion-free.\nThat is, watermarked text should still be usable for the same downstream tasks for which\nunwatermarked text is useful.\nWe note that prior work has applied public-key cryptography to the watermarking\nproblem. They commonly work by hiding cryptographic objects (i.e., a signature or\nencryption) within an image or text. While our work uses a similar approach, prior\nuses of public-key cryptography for watermarking [WM01; SC05] were motivated by\napplications to copyright protection and crucially embed the watermark to existing content,\nas opposed to the generation-time watermark of this work. Prior steganographic work has\nalso made use of rejection sampling. Cachin [Cac98]’s protocol uses rejection sampling\nto sample stegotexts that “look like” covertexts where each stegotext embeds one bit\nof information. Hopper, Langford, and Von Ahn [HLV02] gave a complexity-theoretic\ntreatment of steganography and used rejection sampling to sample from an oracle to meet a\nspecific target, i.e., samplec←M until a conditionF(c) = xis satisfied wherexis a target.\nIn contrast, this paper applies rejection sampling for the special case of embedding bits\nin generated content. Beyond steganography, rejection sampling also appears broadly in\nother cryptography subfields such as post quantum-secure encryption [ZWQ21; GHJ+22].\n20 Publicly-Detectable Watermarking for Language Models\n6 Conclusion\nIn this paper, we construct an LM watermarking scheme that is simultaneously publicly-\ndetectable and unforgeable. We conclude with a summary of our scheme’s limitations and\npotential avenues for future work.\nLimitations and future work The main barriers for general use of our protocol are\n(1) embedding compactness: it takes many tokens to embed a whole digital signature,\nand (2) robustness: removing the watermark is easy for an unrestricted adversary to\ndestroy. Resolving either of these issues is nontrivial and weaker settings suffer from\nsimilar issues, e.g., the secret-key setting. However, for specific use-cases where generation\noutput is long-form and robustness is not a concern, our watermark can be readily applied.\nWe believe there is considerable space for future work to build toward more practical\npublic watermarks with strong formal properties. In a recent paper, [GM24] constructed\na watermarking scheme in the public setting that is edit-distance robust—future work\ncould seek to bring down the concrete complexity of an edit-distance robust watermark or\ndevelop new techniques that permit other notions of robustness. In particular, it would be\ninteresting to uncover the theoretical limitations of what properties a publicly-detectable\nwatermark could hope to achieve.\nAcknowledgments\nJaiden Fairoze and Sanjam Garg are supported in part by an AFOSR Award FA9550-24-1-\n0156 and research grants by Bakar Fund, J.P. Morgan Faculty Research Award, Supra Inc.,\nVisa Inc, and the Stellar Development Foundation. Somesh Jha is partially supported\nby Air Force Grant FA9550-18-1-0166, the National Science Foundation (NSF) Grants\nCCF-FMitF-1836978, IIS-2008559, SaTC-Frontiers-1804648, and ARO grant number\nW911NF-17-1-0405. Mingyuan Wang is supported in part by an NYU startup fund and\ncompleted part of this work while a postdoc at UC Berkeley. We would like to express our\ngratitude to Miranda Christ, Keewoo Lee and the anonymous reviewers for their valuable\neditorial suggestions that have improved the presentation of this work.\nReferences\n[Aar23] Scott Aaronson. Neurocryptography. Invited Plenary Talk at Crypto’2023,\n2023. url: https://www.scottaaronson.com/talks/neurocrypt.pptx.\n[AF21] Sahar Abdelnabi and Mario Fritz. Adversarial watermarking transformer:\nTowards tracing text provenance with data hiding. In2021 IEEE Symposium\non Security and Privacy (SP), 2021.doi: 10.1109/SP40001.2021.00083.\n[Ber16] Daria Beresneva. Computer-generated text detection using machine learning:\nA systematic review. InNatural Language Processing and Information Sys-\ntems: 21st International Conference on Applications of Natural Language to\nInformation Systems, 2016.doi: 10.1007/978-3-319-41754-7_43 .\n[BLS01] Dan Boneh, Ben Lynn, and Hovav Shacham. Short signatures from the\nWeil pairing. InInternational Conference on the Theory and Application of\nCryptology and Information Security, 2001.doi: 10.1007/3-540-45682-1_3\n0.\n[BR93] Mihir Bellare and Phillip Rogaway. Random oracles are practical: A paradigm\nfor designing efficient protocols. InProceedings of the 1st ACM Conference\non Computer and Communications Security, 1993.doi: 10.1145/168588.16\n8596.\nFairoze et al. 21\n[Cac98] Christian Cachin. An information-theoretic model for steganography. In\nInternational Workshop on Information Hiding, 1998.doi: 10.1007/3-540-\n49380-8_21.\n[CBZ+23] Souradip Chakraborty, Amrit Singh Bedi, Sicheng Zhu, Bang An, Dinesh\nManocha, and Furong Huang. On the possibilities of AI-generated text\ndetection. arXiv preprint arXiv:2304.04736, 2023.doi: 10.48550/arXiv.23\n04.04736.\n[CGZ24] Miranda Christ, Sam Gunn, and Or Zamir. Undetectable watermarks for\nlanguage models. InProceedings of Thirty Seventh Conference on Learning\nTheory, 2024.url: https://proceedings.mlr.press/v247/christ24a.ht\nml.\n[CWJ+23] Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng Xu. Exploring\nthe use of large language models for reference-free text quality evaluation:\nA preliminary empirical study.arXiv preprint arXiv:2304.00723, 2023.doi:\n10.48550/arXiv.2304.00723.\n[Fac23] Hugging Face. Hugging face transformers.https://huggingface.co/docs\n/transformers/index, 2023. Accessed: 2023-10-08.\n[GHJ+22] Qian Guo, Clemens Hlauschek, Thomas Johansson, Norman Lahr, Alexander\nNilsson, and Robin Leander Schröder. Don’t reject this: key-recovery timing\nattacks due to rejection-sampling in HQC and BIKE.IACR Transactions on\nCryptographic Hardware and Embedded Systems, 2022.doi: 10.46586/tches\n.v2022.i3.223-263.\n[GM24] Noah Golowich and Ankur Moitra. Edit distance robust watermarks for\nlanguage models.arXiv preprint arXiv:2406.02633, 2024.doi: 10.48550/ar\nXiv.2406.02633.\n[Goo24] Google DeepMind. Our next-generation model: Gemini 1.5.https://blog\n.google/technology/ai/google-gemini-next-generation-model-febr\nuary-2024/, 2024. Accessed: 2024-02-20.\n[GPT23] GPTZero. GPTZero | The Trusted AI Detector for ChatGPT, GPT-4, &\nMore. https://gptzero.me/, 2023. Accessed: 2023-10-05.\n[GSR19] Sebastian Gehrmann, Hendrik Strobelt, and Alexander M Rush. GLTR:\nStatistical detection and visualization of generated text. arXiv preprint\narXiv:1906.04043, 2019.doi: 10.48550/arXiv.1906.04043.\n[GW17] Venkatesan Guruswami and Carol Wang. Deletion codes in the high-noise\nand high-rate regimes.IEEE Transactions on Information Theory, 2017.doi:\n10.1109/TIT.2017.2659765.\n[HAAL23] Jan Hendrik Kirchner, Lama Ahmad, Scott Aaronson, and Jan Leike. New\nAI classifier for indicating AI-written text.https://openai.com/blog/ne\nw-ai-classifier-for-indicating-ai-\\written-text , 2023. Accessed:\n2023-10-05.\n[HBD+19] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious\ncase of neural text degeneration.arXiv preprint arXiv:1904.09751, 2019.doi:\n10.48550/arXiv.1904.09751.\n[HLV02] Nicholas J Hopper, John Langford, and Luis Von Ahn. Provably secure\nsteganography. InAdvances in Cryptology—CRYPTO 2002: 22nd Annual\nInternational Cryptology Conference, 2002.doi: 10.1007/3-540-45708-9_6 .\n[Hoe94] Wassily Hoeffding. Probability inequalities for sums of bounded random\nvariables. The Collected Works of Wassily Hoeffding, 1994.doi: 10.1007/97\n8-1-4612-0865-5_26 .\n22 Publicly-Detectable Watermarking for Language Models\n[JAL20] Ganesh Jawahar, Muhammad Abdul-Mageed, and Laks VS Lakshmanan.\nAutomatic detection of machine generated text: A critical survey.arXiv\npreprint arXiv:2011.01314, 2020.doi: 10.48550/arXiv.2011.01314.\n[JSM+23] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna\nLengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7B.arXiv preprint\narXiv:2310.06825, 2023.doi: 10.48550/arXiv.2310.06825.\n[KGW+23] John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers,\nand Tom Goldstein. A watermark for large language models. InProceedings\nof the 40th International Conference on Machine Learning, 2023.url: https\n://proceedings.mlr.press/v202/kirchenbauer23a.html.\n[KJGR21] Gabriel Kaptchuk, Tushar M Jois, Matthew Green, and Aviel D Rubin.\nMeteor: Cryptographically secure steganography for realistic distributions.\nIn Proceedings of the 2021 ACM SIGSAC Conference on Computer and\nCommunications Security, 2021.doi: 10.1145/3460120.3484550.\n[KSK+23] Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit\nIyyer. Paraphrasing evades detectors of AI-generated text, but retrieval is\nan effective defense.Advances in Neural Information Processing Systems, 36,\n2023. url: https://proceedings.neurips.cc/paper_files/paper/2023\n/file/575c450013d0e99e4b0ecf82bd1afaa4-Paper-Conference.pdf.\n[KTHL24] Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, and Percy Liang.\nRobust distortion-free watermarks for language models.Transactions on\nMachine Learning Research, 2024.url: https://openreview.net/forum?i\nd=FpaCL1MO2C.\n[LPH+23] Aiwei Liu, Leyi Pan, Xuming Hu, Shu’ang Li, Lijie Wen, Irwin King, and\nPhilip S Yu. A private watermark for large language models.arXiv preprint\narXiv:2307.16230, 2023.doi: 10.48550/arXiv.2307.16230.\n[LUY08] Thomas Lavergne, Tanguy Urvoy, and François Yvon. Detecting fake content\nwithrelativeentropyscoring. Proceedings of the 2008 International Conference\non Uncovering Plagiarism, Authorship and Social Software Misuse, 2008.url:\nhttps://ceur-ws.org/Vol-377/paper4.pdf.\n[MLK+23] Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning,\nand Chelsea Finn. DetectGPT: zero-shot machine-generated text detection\nusing probability curvature.Proceedings of the 40th International Conference\non Machine Learning, 2023.url: https://dl.acm.org/doi/10.5555/3618\n408.3619446.\n[MTDZ24] Travis Munyer, Abdullah All Tanvir, Arjon Das, and Xin Zhong. Deep-\nTextMark: A Deep learning-driven text watermarking approach for identify-\ning large language model generated text.IEEE Access, 2024.doi: 10.1109\n/ACCESS.2024.3376693.\n[Ope23] OpenAI. GPT-4 technical report.arXiv preprint arXiv:2303.08774, 2023.\ndoi: 10.48550/arXiv.2303.08774.\n[PGM+19] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury,\nGregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. PyTorch: An imperative style, high-performance deep learning\nlibrary. Advances in Neural Information Processing Systems, 2019. url:\nhttps://proceedings.neurips.cc/paper_files/paper/2019/file/bdb\nca288fee7f92f2bfa9f7012727740-Paper.pdf.\nFairoze et al. 23\n[PSF+23] Julien Piet, Chawin Sitawarin, Vivian Fang, Norman Mu, and David Wagner.\nMark my words: Analyzing and evaluating language model watermarks.arXiv\npreprint arXiv:2312.00273, 2023.doi: 10.48550/arXiv.2312.00273.\n[QYH+24] Wenjie Qu, Dong Yin, Zixin He, Wei Zou, Tianyang Tao, Jinyuan Jia, and\nJiaheng Zhang. Provably robust multi-bit watermarking for AI-generated\ntext via error correction code.arXiv preprint arXiv:2401.16820, 2024.doi:\n10.48550/arXiv.2401.16820.\n[QZL+23] Jipeng Qiang, Shiyu Zhu, Yun Li, Yi Zhu, Yunhao Yuan, and Xindong Wu.\nNatural language watermarking via paraphraser-based lexical substitution.\nArtificial Intelligence, 2023.doi: 10.1016/j.artint.2023.103859.\n[RBL+22] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and\nBjörn Ommer. High-resolution image synthesis with latent diffusion models.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2022.url: https://openaccess.thecvf.com/conte\nnt/CVPR2022/papers/Rombach_High-Resolution_Image_Synthesis_Wit\nh_Latent_Diffusion_Models_CVPR_2022_paper.pdf.\n[RSR+20] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits\nof transfer learning with a unified text-to-text transformer.The Journal of\nMachine Learning Research, 2020.url: https://jmlr.org/papers/v21/20\n-074.html.\n[SC05] Qibin Sun and Shih-Fu Chang. A secure and robust digital signature scheme\nfor JPEG2000 image authentication.IEEE Transactions on Multimedia, 2005.\ndoi: 10.1109/TMM.2005.846776.\n[SKB+23] Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao\nWang, and Soheil Feizi. Can AI-generated text be reliably detected?arXiv\npreprint arXiv:2303.11156, 2023.doi: 10.48550/arXiv.2303.11156.\n[TMS+23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,\nYasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,\nShruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023.doi: 10.48550/arXiv.2307.09288.\n[WKR+19] Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho,\nand Jason Weston. Neural text generation with unlikelihood training. In\nInternational Conference on Learning Representations, 2019. url: https:\n//openreview.net/forum?id=SJeYe0NtvH.\n[WM01] Ping Wah Wong and Nasir Memon. Secret and public key image water-\nmarking schemes for image authentication and ownership verification.IEEE\ntransactions on image processing, 2001.doi: 10.1109/83.951543.\n[YAJK23] KiYoon Yoo, Wonhyuk Ahn, Jiho Jang, and Nojun Kwak. Robust natural lan-\nguagewatermarkingthroughinvariantfeatures. arXiv preprint arXiv:2305.01904,\n2023. doi: 10.48550/arXiv.2305.01904.\n[ZEF+24] Hanlin Zhang, Benjamin L. Edelman, Danilo Francati, Daniele Venturi,\nGiuseppe Ateniese, and Boaz Barak. Watermarks in the sand: Impossibility\nof strong watermarking for language models. In Proceedings of the 41st\nInternational Conference on Machine Learning, 2024.url: https://procee\ndings.mlr.press/v235/zhang24o.html.\n24 Publicly-Detectable Watermarking for Language Models\n[ZHR+19] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi,\nFranziska Roesner, and Yejin Choi. Defending against neural fake news.\nAdvances in Neural Information Processing Systems, 2019.url: https://pr\noceedings.neurips.cc/paper_files/paper/2019/file/3e9f0fc9b2f89\ne043bc6233994dfcf76-Paper.pdf.\n[ZRG+22] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen,\nShuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,\net al. Opt: Open pre-trained transformer language models.arXiv preprint\narXiv:2205.01068, 2022.doi: 10.48550/arXiv.2205.01068.\n[ZWQ21] Zhongxiang Zheng, Anyu Wang, and Lingyue Qin. Rejection sampling revisit:\nhow to choose parameters in lattice-based signature.Mathematical Problems\nin Engineering, 2021.doi: 10.1155/2021/9948618.\nA Additional Preliminary\nOur proof relies on the following standard concentration bound, known as Hoeffiding\ninequality.\nTheorem 2([Hoe94]). Let X1,...,X n be independent random variables such thatai ≤\nXi ≤bi for all i. LetSn = ∑n\ni=1 Xi. For anyt> 0, it holds that\nPr [|Sn −E [Sn]|≥ t·E [Sn]] ≤2 ·e\n− 2t2∑n\ni=1\n(bi−ai)2\n.\nB Extra Discussion\nB.1 Minor Protocol Discussion\nThere are a number of extensions one can add to the protocol to make well-defined\nimprovements with no significant cost.\nt1 c1\n↓ t2 ←c1[−ℓ:]\nt2 c2\n↓ t3 ←c2[−ℓ:]\nt3 c3\n······\nFigure 5: Tiling structure to compress multiple message-signature pairs. This is possible\nbecause the signature codeword itself is pseudorandom.\nEmbedding multiple watermarked blocksFigure 1 above embeds one signature in a\nfixed number of output bits. To extend this scheme to support arbitrarily large output, we\ncan tile the block structure defined above sequentially until the desired length is reached.\nWhen n is large enough to permit multiple message-signature pairs, we can leverage\nthe pseudorandomness of the masked signature codeword to use significantly fewer tokens.\nSpecifically, to embedk message-signature segments, we only needk·(ℓ+ ℓ·λc) −(k−1) ·ℓ\nFairoze et al. 25\ntokens given λc ≥ℓ. Note that in practice, λσ = 328 ≤λc bits (inherited from BLS\nsignatures [BLS01]) and ℓ ≃16 characters. The idea is to use the last ℓ bits of the\nsignature from the previous message-signature pair as the message for the next segment.\nThis preserves full distortion-freeness since the signature codeword is re-randomized with\na pseudorandom mask. See Figure 5 for a visual depiction of this process.\nTuning the robustness vs. distortion-freeness trade-off We assume that each\nblock of ℓ tokens has sufficient entropy (as defined by the parameterα) to ensure our\nformal notion of distortion-freeness is met. However, in practice, one can setℓ to a\nconcrete value (e.g., determined empirically for a specific model and hyperparameters) to\ntweak robustness—ℓ should be set as low as possible such that the text quality remains\nsimilar3 to non-watermarked text in order to get more robustness. Whenℓ is low, each\nmessage-signature segment requires fewer total tokens, meaning more segments can be\nembedded inn tokens. So long as one message-signature pair remains after text edits, the\nwatermark is detectable.\nChaining embedded bits The plain gadget depicted in Figure 1 embeds one bit of\nthe signature into a block ofℓ tokens. Observe that the scheme is susceptible to the\nfollowing attack. If an adversary knows that a specific portion of text corresponds to\na message-signature pair, she can construct a different message that still verifies. By\ndefinition of the random oracle, any freshly sampled token has1/2 probability of hashing\nto 0 or 1. The adversary replaces a codewordc∈T ℓ·λc with a new messagec′∈T ℓ·λc by\nchanging ℓtokens at a time and checking that the new block still hashes to the same value\nas the old block. If the hash is inconsistent, sample a new block ofℓ tokens and try again.\nThis process can be repeated for allλc bits of the signature codeword. In addition, since\nthe blocks are independent, the adversary can replace each block with adversarial text in\na modular fashion.\nWe make this attack more difficult by introducing dependencies across adjacent blocks.\nFor eachi∈λc success condition for rejection hashing changes from\nH\n(\nsci,1\ni,1 ···sci,ℓ\ni,ℓ\n)\n= ci\nto\nH\n\n\ni⨁\nj=1\nscj,1\nj,1 ···scj,ℓ\nj,ℓ\n\n= ci\nwhere ⨁i\nj=1 vj stands for concatenation as⨁i\nj=1 vj := v1 ∥v2 ∥... ∥vi. Now, for the\nadversary to perform the same signature replacement attack, she can no longer change\neach ℓ-length token block independently.\nDynamic entropy Through this paper, we assume that there is sufficient entropy in\nevery ℓ tokens sampled from the LM in order to simplify presentation and analysis. We\ncan relax this assumption in the real world—rather than setting a global lengthℓ that\nis expected to be of sufficient entropy, we can empirically measure how much entropy is\navailable from the LM at generation time. This idea originates from Christ, Gunn, and\nZamir [CGZ24] where they use it to transform their private-key scheme into a substring-\ncomplete version, i.e., a version that embeds independent detectable segments to gain\nrobustness. We can employ the core idea in the asymmetric setting as follows:\nGiven a distributionp←Model(·,·), one can measure the entropy of sampling tokent\nfrom pas −log(p(t)). Thus, each time a new token is sampled, the generation algorithm can\n3Our Theorem 1 proves that this similarity is closely related to how much entropy eachℓ tokens carry.\n26 Publicly-Detectable Watermarking for Language Models\nkeep track of how much collective entropy has been seen up to that point, i.e., cumulative\nentropy can be measured as∑j\ni=1 −log(pi(ti)) for a contiguous block ofj tokens—this\nsum can be updated incrementally as more tokens are sampled. To use this optimization,\nthe generation algorithm simply waits for sufficiently many token samples such that the\ncumulative entropy sum is large enough. Once the threshold is reached, all sampled tokens\nare taken as the message, and the protocol proceeds as before4. At detection time, the\nmessage is no longer of fixed size, so it no longer suffices to iterate all windows of fixed\nlength in the text. Thus, the detector will incur quadratic performance costs by searching\nover all possible message strings from the input text.\nB.2 Outsourcing Watermarking Itself\nOneofthemainbenefitsofapubliclydetectablewatermarkingschemeisthatthewatermark\ndetector is outsourceable—the entity providing a “detection service” is different from the\none providing the model. Besides outsourcing detection, we remark that our protocol also\nnaturally supports outsourcing the watermarking process itself, i.e., anexternal entitycan\nembed a publicly detectable watermark in text generated by aprivate model. In contrast\nwith all known prior watermarking schemes, our protocol does not necessarily need to\nknow the distribution from which to sample each token—it suffices to obtain a list of\n“next best tokens” given by the prompt and prior generated tokens. This means that our\nwatermarking scheme can operate over API access to private LMs.\nAssume that an LM provider supports an API that returns the topℓ next tokens for\na given prompt. The watermarking process itself can be outsourced by replacing direct\nmodel calls with API calls. This has the downside of requiring linearly many requests in\nthe output length—we acknowledge that this is likely a preventative expense for many use\ncases.5\nB.3 Performance Optimizations\nThe main computational bottleneck in our scheme is rejection sampling. There are\ntwo straightforward optimizations to this process that would greatly improve concrete\nperformance. First, rejection sampling naturally lends itself to parallelism. Instead of\nsequentially searching for a signature collision (on the CPU), this process can be performed\nin parallel on the GPU to take advantage of (a) faster inference and (b) faster hashing.\nSecond, consider the case whereℓ= 1 for simplicity. Whenever a token is rejected (i.e.,\nthe token hash did not match the signature hash), this information can be used to modify\nmodel logits to prevent sampling the same token repeatedly. This has a large impact on\nthe expected running time of rejection sampling since if the token was sampled, it is likely\nto be sampled again. Preventing this situation drastically improves the time needed to find\na matching token. This idea can be generalized toℓ> 1, however, care would be necessary\nto ensure that only the specific token sequence becomes improbable after each rejection.\nB.4 Embedding Codewords for Robustness\nIn contrast to prior watermarking schemes [KGW+23; CGZ24; KTHL24], our framework\nof embedding extractable bits into the generated textreadily allows for further improvement\nin robustness.\nInstead of embedding a message-signature pair (which contains no redundancy) into the\ngenerated text, one may embed an “error-corrected encoding” of the message and signature\n4Note that this optimization can only apply to the message portion of the embedding—applying it to\nthe signature component would result in an exponential blowup of the detector’s runtime.\n5At the time of writing, OpenAI charges US$0.03 per 1000 input tokens and US$0.06 per 1000 output\ntokens for GPT-4 API access with 8k token context.\nFairoze et al. 27\ninto the generated text. Given an input text, the detection algorithm will first extract the\npotentially-erroneous codeword from the text and then apply error correction to recover\nthe original message-signature pair. Apparently, the robustness of this new watermarking\nscheme will inherit the robustness of the error-correction scheme. For instance, if the\nerror-correcting algorithm allows for 10% of errors, our watermarking scheme will be\nresilient to 10% of word replacements.\nWe emphasize that the robustness guarantee provided in this scheme can beformally\nproven, unlike the strong robustness claims of non-cryptographic watermarks, which are\nbased on experimental validation and heuristics.\nOne potential barrier to this proposed scheme is efficiency. The efficiency of the scheme\ndepends on the efficiency of the error-correcting encoding. Normally, in the context of text\nediting, one aims for resilience against insertions and deletions. However, existing state-of-\nthe-art error-correcting codes against insertions and deletions [GW17] have worse efficiency\ncompared to their counterparts for Hamming error correction. However, we emphasize\nthat our framework is modular. Any improvement in the construction of error-correcting\ncodes will directly give improvement to the efficiency of this proposed scheme.\nC Completion Examples\nOur evaluation primarily involved the Mistral 7B [JSM+23] completions discussed above,\nwe ran the following benchmarks on the larger Llama 2 13B and 70B models [TMS+23] on\na smaller scale. All model inference was performed with full precision—no quantization\nwas employed. Evaluation was computed using NVIDIA A100 GPUs with 40GB VRAM\n(from 1 to 8 in parallel depending on the model size) for all benchmarks except those\ninvolving OPT 2.7B, which ran on a single NVIDIA RTX 3090 Ti GPU.\nTable 2:Example completions from Llama 2 70B [TMS+23]. For the Christ, Gunn, and\nZamir [CGZ24] scheme, we set security parameterλ= 16. For our scheme, we set signature\nsegment lengthℓ = 16, bit sizeb = 1, and maximum number of planted errorsγ = 2.\nCompletions are truncated to the first 200 characters.\n# Prompt Plain (tokens) Plain (bits) Christ et al. This work\n1 Windthorst pulled offa sweep of CollinsvilleTuesday, while ArcherCity and Hollidaywere unable to ad-vance.\\nCHICO — Witha chance to square offagainst the defendingchamps later this week,No. 7 Windthorst tookcare of business Tuesdaynight.\\nThe Trojanettes\nheld off a pesky Chicosquad en route to a 64-48victory in the Region II-2A girls basketball semi-finals.\\nNow they’ll geta rematch with No. 1Cooper in the region ti-tle game.\\nWindthorst ismedal-bo\nswept No. 21 Collinsville25-19, 25-19, 25-19to win their Class2A Region II quarter-final at Chico HighSchool.\\nWindthorst(26-9) advances to regionsemifinals that will beheld in Dangerfield Frida\nwon behind 19 kills fromAudrey Lopez to takeGame 1, 25-14, 25-17, 25-21, over Collinsville inthe Region II-2A quarter-final match, setting upa semifinal tilt againstCrawford for Friday af-ternoon i\nunleashed some offenseand fended off a Bearcats’comeback attempt to fin-ish off a 67-58 win andsweep of Collinsville inthe Class 2A girls areaplayoffs.\\n\"I’d like to seeus play with that type of\n2 Eight Sussex skiers willtake to the slopes tobattle it out for thehonour of being crownedNational Championat the English AlpineSki Championshipswhich start this week-end.\\nBurgess Hillsisters, 18-year-old Samand 16-year-old HelenTodd-Saunders andCrawley\n’s Olivia Gillespie, 17,will race in the girls’events at at Whaka-papa Ski Area in NewZealand.\\nTheboyscom-peting for the honourof becoming NationalChampions are 19-year-old Ivan Ukri, of Coldean\n’s Nicholas Moynihan, 21,are part of the 11-strongTeam Evil who head toBormio, Italy, tomorrow(Tuesday) for a week’straining ahead of thechampionships. The triowill be joined by threeother Loo\n-based Sol Buchler (17)are travelling out toMontalbert, Francefor the Championshipswhich run from March21st to March 24th dueto lack of snow condi-tions in the UK.\\nSamTodd-Saunders. Pic byActi\n’s Trevor McColganhave all previouslywon national titlesand will be looking tocontinue their winningways.\\nHowever, theywill be up against astrong field of competi-tors, before one girl andone\n28 Publicly-Detectable Watermarking for Language Models\nTable 3:Example completions from Llama 2 13B [TMS+23]. For the Christ, Gunn, and\nZamir [CGZ24] scheme, we set security parameterλ= 16. For our scheme, we set signature\nsegment lengthℓ = 16, bit sizeb = 1, and maximum number of planted errorsγ = 2.\nCompletions are truncated to the first 200 characters.\n# Prompt Plain (tokens) Plain (bits) Christ et al. This work\n1 Windthorst pulled offa sweep of CollinsvilleTuesday, while ArcherCity and Hollidaywere unable to ad-vance.\\nCHICO — Witha chance to square offagainst the defendingchamps later this week,No. 7 Windthorst tookcare of business Tuesdaynight.\\nThe Trojanettes\ndowned No. 10Collinsville 75-27 ina Class (1A) bi-districtbasketball playoff gameTuesday as part ofa quadruple-comboevent at Chico HighSchool.\\nWindthorst(22-8) jumped out to an11-4 lead afte\n(19-7-2) defeatedCollinsville, 6-3, in theRegion I-1A semifinalsat Lion Field. Theywill face Honey Grove,27-4, in the regionalfinal at Noon Friday inStephenville.\\nNo. 5Honey Grove defeatedVa\nbeat the Class 2A RegionI power Collinsville 4-2Tuesday at the Big Coun-try Soccer Complex.\\nItwas one of four gamesin the opening round ofthe Region I-2A tourna-ment, with just threewins enough t\nstayed on course foranother appearancein Friday’s Class 1Astate championship,pulling off a 3-1 sweep ofCollinsville to advanceto the state tournamentsemifinals.\\n“I think themain thing that h\n2 Eight Sussex skiers willtake to the slopes tobattle it out for thehonour of being crownedNational Championat the English AlpineSki Championshipswhich start this week-end.\\nBurgess Hillsisters, 18-year-old Samand 16-year-old HelenTodd-Saunders andCrawley\nsiblings, 18-year-oldOllie O’Sullivan and14-year-old Jack, areamong those earmarkedto shine in the eventbased at Essex Snows-ports.\\nTheir handler,Alton snow sports coach,Liz Baird said: “It’s a\n’s Amy Conroy (pic-tured), 18, will representSussex against some ofthe fastest alpine racingfemale representativesin the country.\\nJohnGardner, father of 19-year-old Eliza and Tom,both pictured,\n’s 14-year-old HelenCullen all enjoy sup-porting each other atcompetitions and train-ing.\\nBoth Cullen andTodd-Saunders lookedon in admiration as Samwon the Salomon super-glast season snatching vi\nbrothers Harry Roylance,16, and Charlie Roylance,13 of Oxted’s CollegeSevenoaks have all beenselectedto headoff totheslopes of Austria for thecompetition.\\nThe eventincludes a full programm\nFairoze et al. 29\nTable 4: Example completions from Mistral 7B [JSM+23]. For the Christ, Gunn, and\nZamir [CGZ24] scheme, we set security parameterλ= 16. For our scheme, we set signature\nsegment lengthℓ = 16, bit sizeb = 2, and maximum number of planted errorsγ = 0.\nCompletions are truncated to the first 200 characters.\n# Prompt Plain (tokens) Plain (bits) Christ et al. This work\n1 Windthorst pulled offa sweep of CollinsvilleTuesday, while ArcherCity and Hollidaywere unable to ad-vance.\\nCHICO — Witha chance to square offagainst the defendingchamps later this week,No. 7 Windthorst tookcare of business Tuesdaynight.\\nThe Trojanettes\nbeat Collinsville, 6-1, 7-0,to secure a season sweepof the former Division Ichamps and advance tothe Region II-1A quarter-finalslaterthisweek.\\n“Ithought, going into thistournament, one of the\nknocked off Collinsvillein two five-set matchesto win a Class 1A bi-district volleyball play-off series.\\nWindthorst isscheduled to visit fourth-ranked Mark, a fellowDistrict 8-2A squad, Fri-day at\nopened play Tuesday inthe Class A Region I1A regional quarterfi-nals with a tough 25-20, 25-20, 25-22 sweep ofCollinsville.\\nUnfortunately for theTrojans and No. 13Archer City, Westbrookbeat the\npulled off a sweepat Lamar Nice Fieldas they handled No.17 Collinsville in theArea Championship,winning 10-0 in fiveinnings and 5-1 in a verytightly contested secondgame.\\nWindthorst willfac\n2 Eight Sussex skiers willtake to the slopes tobattle it out for thehonour of being crownedNational Championat the English AlpineSki Championshipswhich start this week-end.\\nBurgess Hillsisters, 18-year-old Samand 16-year-old HelenTodd-Saunders andCrawley\ngymnast Paige McKenziewill represent England atgirls combined categorywhilst Crawley siblingsPhilippa and JamesHorton will competein the boys hopefulsevent.\\nSteyning pairPhoebe Pratten and La\nstudents, 20-year-old Al-ice Reidy, 18-year-old Fe-lix Rogers, 16-year-oldTom Eagleson and 15-year-old Patrick Lane, allracers for the BrightonSnozone Academy, willall be looking to grab thetitle.\nskier, 14-year-old JoeRoberts will be com-peting in the event inMeribel, France, from5th March.\\nThe restof the team is madeup of Alexandra Leff(Worthing), Jack Hatton,Phoebe Smith (Both 18,Has\nGirls’ School’s 17-year-old Lucy Finlayson, whois also the under-21British Slalom Cham-pion, willgoheadtoheadwith some of the bestskiers in the country ina series of races takingplace on Ste\n3 It’s 14 degrees andsnowing. But at one ofMoscow’s new cooper-ative clothing markets,business is booming.Muffled against the cold,vendors shout promo-tions for their paltryofferings while othersmingle more discreetlywith the crowd, hawkingFrench perfume\n, jewelry, and other just-arrived wares, all at aprice.\\n\\n\"Get a look atthese earrings!\" shoutsone such trader, whip-ping in a customer withhis pendulum-like swingsof his color-plastic ear-rings.\\\nand Italian ear-rings.\\n\\nIt’s not all asinnocent as it sounds.All of the clothing arestolen from retailersand vendors have to becareful because many ofthem are not only policebut private detect\nand Jean-Paul Gaultiersweaters under thetable.\\n\\nThe businessmay be illicit, but thescene is all too familiarin Eastern Europe’snewest attempt at capi-talism: the Communistfree market.\\n\\n“I m\n, Chinese sneakers andSwedish sweaters. Withcash prizes up for grabsfor the best deals, this isRussian capitalism at itsmost jaded. \"We only goto places like that now,\"a university vice-chancel\n4 Now Finally TakingShape: A World WithEver Less Decently-PaidWork for Most. ButMust This Be a Prob-lem?\\nFor centuries,experts/futurists havepredicted that machineswould someday makeworkers obsolete. Andnow it’s finally happen-ing, sporadically, hereand\nthere. But is this any-thing to worry about? Isthis a problem? The rootof these hard-fought dis-agreements about it istheunspokenassumptionthat (at least) the work-ing class is supposed tocomprise\nthere,\\nBut let’s be hon-est here – pre-retireeswho know there’s littlehope that they’ll be ableto fill large gaps in thedecades remaining before& after retirement – theydon’t know what to do f\nthere, little by little.And while it isn’t muchof a shock to most ofus to hear that sophisti-cated algorithms and ma-chines are better than usat many types of fairlyroutine work (or thatsexy/futur\nthere–the interestingstructural aspect is thatthe service sector (low-tech), not the high-techsector is leading thecharge.\\nAs I outlined inthe first installment, thejobless-growth paradoxis\n5 In our “always on” cul-ture, the office mantrais: work late, stayconnected. The problemis that working harderand longer doesn’t nec-essarily make you moreproductive. Researchshows that gettingaway—especially on ajourney that engagesyour mind and body,\nnot just your eyes—is es-sential to achieve power-ful rebooting and offerdeep drives toward yourown life goals.\\n\\nYouwill need a “model ofjourneying,” wholly em-bracingtheunfoldinghol-iday experie\nslows you down,and helps you re-set—revitalizes andenergizes.\\n\\n>Get outof your rut – find theultimate WildernessLodge Villa Package,and escape to par-adise.\\n\\nWalt DisneyWorld Resort vacation\nmaybe even encouragesyou to step outsideyour comfort zone—canspur productivity, cre-ative thinking, andinnovation.\\n\\nByLeo Babauta of ZenHabits\\n\\n“Be herenow.” This isn’t a sen-tence that needs\nand forces you to un-plug—can encourageyour productivity whenyou get back.\\n\\nInfact, nothing nurturesthe drive to excel morethan seeing your passionplayed out in a differentcontext—say, by exp\n30 Publicly-Detectable Watermarking for Language Models\nTable 5: Example completions from Mistral 7B [JSM+23]. For the Christ, Gunn, and\nZamir [CGZ24] scheme, we set security parameterλ= 16. For our scheme, we set signature\nsegment lengthℓ = 16, bit sizeb = 2, and maximum number of planted errorsγ = 2.\nCompletions are truncated to the first 200 characters.\n# Prompt Plain (tokens) Plain (bits) Christ et al. This work\n1 Windthorst pulled offa sweep of CollinsvilleTuesday, while ArcherCity and Hollidaywere unable to ad-vance.\\nCHICO — Witha chance to square offagainst the defendingchamps later this week,No. 7 Windthorst tookcare of business Tuesdaynight.\\nThe Trojanettes\ndefeated Collinsville 25-16, 25-18, 23-25, 25-21 tosecure an area title andpunched their ticket tothe Region I-2A quarter-finals with a trip to thecontroversial West Texastown of Iredell on the li\n, seeded second, sweptCollinsville in threegames to set up aquarterfinal date withtop-seeded Trent.Windthorst defeatedCollinsville 25-16, 25-17and 25-20.\\nAssumptionsqueaked past Brock,22-25,\nswept Collinsville to ad-vance to the area roundof the postseason andface Olney. They’rehoping to turn the ta-bles on the Ladycatsfor whom they lost inthe second round a yearago.\\n“They beat us la\ntraveled to No. 10Collinsville and sweptthe Lady Collie Cardi-nals in game one, set-ting up a chance totake on Amarillo RiverRoad later this week ina highly-anticipated Re-gion I-2A area roundrema\n2 Eight Sussex skiers willtake to the slopes tobattle it out for thehonour of being crownedNational Championat the English AlpineSki Championshipswhich start this week-end.\\nBurgess Hillsisters, 18-year-old Samand 16-year-old HelenTodd-Saunders andCrawley\n/Fernhill Heath duo,Martin Mellon andOliver James will donnational kit to competein Giant Slalom (GS)and Slalom (SL) eventsin Yongpyong, SouthKorea.\\nPerformances in14 events will be rankedto\n’s Amy Mertens, just 13,will go head-to-head withsome of the country’sbest racers at the widevariety of disciplines onoffer.\\nWith almost 20races in multiple disci-plines it is the most di-verse s\n’s Callum Adams (18),Ross Guest (25) and MaxGreen (17) all make upthe squad for the cham-pionships.\\nWorthing’sDanny Williams (17),Syd Wilson (16) andEast Grinstead’s NaomiWilkinson (21) also re\n’s Amy Crocket willall compete in the Se-nior National Champi-onships, racing at theTrois Vallees ski resort inthe French Alps, alongwith Booker’s Mollie Dar-ling and Dylan Jetsun;Rye’s William Phil\n3 It’s 14 degrees andsnowing. But at one ofMoscow’s new cooper-ative clothing markets,business is booming.Muffled against the cold,vendors shout promo-tions for their paltryofferings while othersmingle more discreetlywith the crowd, hawkingFrench perfume\nand warm Palestinianwraps. – Peggy Randall,Moscow resident MILAN-Thepast20yearsofItal-ian history have come vi-olently to life as I beganreading Turino: la storianel presente (\"Turin: His-tor\n.\\n\\nSome sellers havefinally located a rarecommodity: bras forsmall women, as wellas perfume. (One fra-grance is dubbed \"Hap-piness.\")\\n\\nThe marketin Beschastovo, north-east Moscow, is the latest\n, Greek olive oil, Italiancognac.\\n\\nThis hush-hush fare all has onething in common: It ison sale outside the offi-cialgovernmentquotaforforeign-currency imports.It is on the black mar-ket.\\n\\nH\nand Japanese ski suits -both sold in rubles at lo-cal markets only a fewyears ago.\\n\\nClothingexperienced a boom insummer 1990, when newsupplies arrived in retailmarkets priced in hardcurrency\n4 Now Finally TakingShape: A World WithEver Less Decently-PaidWork for Most. ButMust This Be a Prob-lem?\\nFor centuries,experts/futurists havepredicted that machineswould someday makeworkers obsolete. Andnow it’s finally happen-ing, sporadically, hereand\nthere–perhaps five per-cent of workers at anygiven time–and it’s con-tributing to the un-employment and socialbreakdown that’s gettingincreasing media atten-tion. But do we have areason to think thi\nthere. But many peopledon’t want to admitthat it is happening.First Software, NextRobots.\\n\\nBut thesoon-to-be-likely realityis that many jobs willbe automated and manypeople will be happier a\nthere. Is this a disas-ter to be avoided–or wel-comed?\\n\\n#367 #use-Canada As Canada un-covers 88 secret RCAFbases that housed 15,973MIAs. Fact— all of themcaptives of the Secret Oc-cupation Governm\nthere in this majortrend: The Destructionof Work/Jobs. Thoughmachines might takejobs/work away, what ifthese inventions are not\"the end\", but the hinge,the pivot of a new era?A period of less\n5 In our “always on” cul-ture, the office mantrais: work late, stayconnected. The problemis that working harderand longer doesn’t nec-essarily make you moreproductive. Researchshows that gettingaway—especially on ajourney that engagesyour mind and body,\nexperiences new peopleand cultures, and cap-tures inspiring sights,sounds and smells, reju-venates you, making youmore productive and cre-ative when you return.Around the world, travelplanners and h\nperhaps in creating newcuriosities–canbethekeyto productivity. One ofthe ways to do that is togo immersed into a NewPlace.\\n\\nWith offices inHong Kong, New Yorkand San Francisco, Sig-nature J\nand takes you far awayfrom your daily sched-ule—achieves a greaterreset than going onvacation.\\n\\nGlobe-trotting can positivelyimpact creativity as well.Divergent thinking is anessential part of\nand promotes creativeconnections—deliverswhat McKinsey Co.CEO Doonan Stewartcalls “the invaluablerenewal to our psycheand brains.”\\n\\n###Section 5: CustomerResearch\\n\\nBeen withmy current cu\nFairoze et al. 31\nTable 6: Example completions from Mistral 7B [JSM+23]. For the Christ, Gunn, and\nZamir [CGZ24] scheme, we set security parameterλ= 16. For our scheme, we set signature\nsegment lengthℓ = 16, bit sizeb = 1, and maximum number of planted errorsγ = 0.\nCompletions are truncated to the first 200 characters.\n# Prompt Plain (tokens) Plain (bits) Christ et al. This work\n1 Windthorst pulled offa sweep of CollinsvilleTuesday, while ArcherCity and Hollidaywere unable to ad-vance.\\nCHICO — Witha chance to square offagainst the defendingchamps later this week,No. 7 Windthorst tookcare of business Tuesdaynight.\\nThe Trojanettes\nadvanced to the RegionI-2A final by sweepingCollinsville, 25-12, 25-11, 25-13, at Chico HighSchool. The win sets upa four-match all-VernonDistrict series against No.2 Brock that will decidewho\nrolled to a 35-11 victoryover Collinsville in thearea round of the play-offs, setting up a datewith No. 3 Parsonsin the regional quarter-finals at 7 p.m. Fridayin Archer City.\\nThat iswhen postsea\npulled off a sweep ofCollinsville, 25-18, 25-13,25-22 to move into Thurs-day’s Region I-2A finalagainst Post (24-15), thedefending Region I-2Achampion.\\nThe winnerfaces the Region II-2Awinner on\ntook a large halftime leadand were never reallythreatened in a 58-46 vic-tory at Collinsville inthe title game at Freer’sHall.\\nWindthorst willtravel to Bellmead thisweekend to take on Dis-trict\n2 Eight Sussex skiers willtake to the slopes tobattle it out for thehonour of being crownedNational Championat the English AlpineSki Championshipswhich start this week-end.\\nBurgess Hillsisters, 18-year-old Samand 16-year-old HelenTodd-Saunders andCrawley\ndynamo, 17-year-oldHamish Lovegrove willcompete against 214other skiers in Busillatsto be crowned NationalChampion.\\nBritishdevelopment squadmember, Isaac Brownand 15-year-old AmberPyrah, fro\nteenager Bradley Leechhave made it throughto finals in Bansko, Bul-garia.\\nThe trio, whorepresent Mid Sussex SkiRacing Club, are among119 university studentsandamateurstakingpartin events tha\n’s 18-year-old HenryRees, will compete inthe giant slalom on theDa Jaunne slope at theChaudanne in Les Arcs,whilst 14-year-old SolSteed (Chichester) willcompete in Downhill 1and Yusuf Sardar S\nteam-mates, 16-year-oldJimmy Simpkin and 17-year-old Maria Brooks-bank lead the way inthe teenage categories,but they face competi-tion from many of thecountry’s best racers overthe four day event\n3 It’s 14 degrees andsnowing. But at one ofMoscow’s new cooper-ative clothing markets,business is booming.Muffled against the cold,vendors shout promo-tions for their paltryofferings while othersmingle more discreetlywith the crowd, hawkingFrench perfume\nand Italian shoes theybought in Paris. Eachtakes the risk of negotia-tion as the market’s sec-ondary vendors only ac-cept rubles for merchan-dise that is against thelaw to sell. Little returnsto these\nand women’s lin-gerie.\\n\\nOn a marketwall, multi-colored tex-tured bags emblazonedwith the trademark of ayoung designer line thelength of a refrigerator.A vendor tells me that awoman sold 30 of th\nand knockoff designerclothes. They havebeen lured here by bet-ter money.\\n\\nThursdayshould be a holiday forthe people of Caucasus,Georgia’s breakaway re-public of Abkhazia, Abk-hazia has a war-damag\nand Japanese crys-tal.\\n\\nRussian Presi-dent Boris N. Yeltsin’seffort this year to reshuf-fle the Kremlin andthe government and todraft the nation’s newnational budget have ledto a blistering shake\n4 Now Finally TakingShape: A World WithEver Less Decently-PaidWork for Most. ButMust This Be a Prob-lem?\\nFor centuries,experts/futurists havepredicted that machineswould someday makeworkers obsolete. Andnow it’s finally happen-ing, sporadically, hereand\nthere–and in splurgy1st-world nations likeCanada, it’s gettingharder and harder to geta job and if you’re luckyenough to get one, thepay is lousy.\\n- RobotsStart Stealing Jobs FromImmigrants\nthere. Politicians andthe media repeatedlyshow hosts and stu-dio personnel workingon futuristic-looking self-driving cars that need nohuman driver, and othervehicles taking us shortdistances in d\nthere. So while somestill cry ’it can’t hap-pen’ or ’but it will cre-ate so much wealth foreveryone’/’shouldn’t wetry to extend the bene-fits of computers/i-techto as many people world-wide as possibl\nthere. But isthis really a prob-lem?\\n\\nEconomics/class/globalization:\\nThe Roleof the US Border Patrolin the Racial Inequal-ity Plaguing the SanDiego Region. Blackand Latino areas havesignifican\n5 In our “always on” cul-ture, the office mantrais: work late, stayconnected. The problemis that working harderand longer doesn’t nec-essarily make you moreproductive. Researchshows that gettingaway—especially on ajourney that engagesyour mind and body,\nwith long-term healthbenefits and a chanceto nurture your creativ-ity—can boost produc-tivity and enhance yourcareer. Nearly three outof four (74%) of workersbelieve spending time onvacation insid\nand has a clear achieve-ment in sight—helpsus attain productivity,greater mental clarity,and a renewed senseof self.\\n\\nMore thana vacation, expeditionexperiences provide away for your employees\nimproves your memory,boosts creativity, and fos-tersinnovation. Whethertraveling is for busi-ness or leisure, thereare ways to incorpo-rate fitness and mind-fulness into your timeaway.\\n\\n# Why Fitne\nlike with a ski trip—canhelp us better under-stand who we are andwhere we want to gonext.\\n\\nHere at the As-pen Journalism Podcast,we can’t emphasize thenecessity of unpluggingenough. Whether you\n32 Publicly-Detectable Watermarking for Language Models\nTable 7: Example completions from Mistral 7B [JSM+23]. For the Christ, Gunn, and\nZamir [CGZ24] scheme, we set security parameterλ= 16. For our scheme, we set signature\nsegment lengthℓ = 16, bit sizeb = 1, and maximum number of planted errorsγ = 2.\nCompletions are truncated to the first 200 characters.\n# Prompt Plain (tokens) Plain (bits) Christ et al. This work\n1 Windthorst pulled offa sweep of CollinsvilleTuesday, while ArcherCity and Hollidaywere unable to ad-vance.\\nCHICO — Witha chance to square offagainst the defendingchamps later this week,No. 7 Windthorst tookcare of business Tuesdaynight.\\nThe Trojanettes\nswept past Collinsville25-20, 25-17, 25-20in the Region I-2Asecond-round playoffmatch at Chico HighSchool. Windthorst(32-16) earned the 25-point victory with twobuzzer-beating kills thatfell s\nswept Collinsville in theRegion II-A quarterfi-nals, beatingtheLadyLi-ons, 25-15, 25-21, 25-23.After two straight must-win, Region II-B semifi-nal matches, Archer Cityand Holliday were bothousted\ndefeated Collinsville 58-57 in Game 1 of the Class2A Regional I final dou-bleheader at Chico onTuesday and swept thepivotal series with a 56-45 win in Game 2.\\nItwas a hilariously close fin-ish to t\nswept Collinsville 25-13and 25-15 on Tuesday,advancing to a rematchwith No. 2 DivineChild Academy.\\n\\nThetwo teams met in groupplay last week at theWindthorst tournament,with DCA winning both\n2 Eight Sussex skiers willtake to the slopes tobattle it out for thehonour of being crownedNational Championat the English AlpineSki Championshipswhich start this week-end.\\nBurgess Hillsisters, 18-year-old Samand 16-year-old HelenTodd-Saunders andCrawley\n’s 17-year-old FreddieNairn head to Alta Ba-dia, Italy along withformer Welsh interna-tional Mackenzie Hughes,22, London Welsh HilaryGrant, 14, London skiingsuperstar Grace Coombs,aged 22, and Max\n’s Anugreen Sefi areall set to compete aspart of a 100 strongEnglish team whichwill be aiming forglory at Pralognan-la-Vanoise.\\nSnow-loversstarted their journey tothe French resort of theway\n’s Sophie Smith are allin the GS Ladies’ Under19s while Guin Bacon,from Eastbourne, racesin the GS Women’s Un-der 21s.\\nThe final En-glish ladies and men’ssquads for the upcomingFIS World Junior C\n’s girls’ team will spear-head the Brighton &Hove City Ski Team(BHCST) hopes, withthe cross country teamalso likely to figureprominently.\\nSam, aformer pupil at BurgessHill School for Girls who\n3 It’s 14 degrees andsnowing. But at one ofMoscow’s new cooper-ative clothing markets,business is booming.Muffled against the cold,vendors shout promo-tions for their paltryofferings while othersmingle more discreetlywith the crowd, hawkingFrench perfume\n, unregistered guns ofvarious awkward calibersor pills which mightbe anything from cran-berry antioxidants to Vi-agra.\\n\\nAfter the So-viet Union collapsed in1991, the Soviet economyslipped into a d\nand luxury watchesat discountprices.\\n\\nWelcometo the well-heeled worldof \"gray\" market, whichboth provides muchneeded relief for Russia’sovertaxed consumersector and spotlightsmajor ineffici\nand knockoff handbagsto the agog shoppers. Of-ficial statistics show thathalf of all Russians nowlive chronically on thebrink of poverty–with anincome well below theminimum clothing needsfor th\nand German refrigera-tors.\\n\\nDespite an all-out advertising storm,which covered apartmentwalls with huge lectureposters proclaiming pro-ductivity and dignitythrough hard work, nu-merous Soviet trad\n4 Now Finally TakingShape: A World WithEver Less Decently-PaidWork for Most. ButMust This Be a Prob-lem?\\nFor centuries,experts/futurists havepredicted that machineswould someday makeworkers obsolete. Andnow it’s finally happen-ing, sporadically, hereand\nthere, largely because ofthe Internet. And hereat Atrios, a group ofsomewhat leftish Amer-icans have welcomedthis development. Butwhy?\\n\\nFor a longtime these pundits haveworried that, either in\nthere, as it always willincreasingly - except,unfortunately, the ex-perts/futurists didn’tthink to anticipatethat machines wouldalso be the solution totheir forecasted prob-lem.\\n\\nUnfortunately,\nthere, and many ob-servers fear that the eco-nomic future is bleak.We are already seeingthis with automation(i.e., robots) replacingworkers in retail jobs,warehousing jobs, manu-facturing jobs in pl\nthere, and thatmeans more joblosses/underpay/zero-hour-or-part-time workfor most. It doesn’t ex-actly feel like a triumph.So at least some expertsworry that white collarknowledge jobs are alsoc\n5 In our “always on” cul-ture, the office mantrais: work late, stayconnected. The problemis that working harderand longer doesn’t nec-essarily make you moreproductive. Researchshows that gettingaway—especially on ajourney that engagesyour mind and body,\nand encourages a moreexpansive thinking—isenormously invigorating.These are the kindsof journeys JTB Spe-cial Interest Groups of-fer.\\n\\n### We HaveFun:\\n\\nWe offer numer-ous ways to connect withfe\nnot just yourbrain—increases in-novation and creativity,which leads to better per-formance when workersreturn, according to stud-ies cited by the WorldTravel and TourismCouncil.\\n\\nEscapesthat comb\nand provides ampletime to think—fuelsyour brain to solvecomplicated problems.So, while productivitymay not look productive,your brain will be work-ing overtime during yourdowntime.\\n\\nFind aGea\nsuch as a bicycletour—can improve yourperformance when youreturn.\\n\\nCyclingopens up your imagina-tion by focusing yourattention on one of themost fundamental hu-man activities—moving.With no ga\nFairoze et al. 33\nTable 8: Example completions from Mistral 7B [JSM+23]. For the Christ, Gunn, and\nZamir [CGZ24] scheme, we set security parameterλ= 16. For our scheme, we set signature\nsegment lengthℓ = 32, bit sizeb = 2, and maximum number of planted errorsγ = 0.\nCompletions are truncated to the first 200 characters.\n# Prompt Plain (tokens) Plain (bits) Christ et al. This work\n1 Windthorst pulled offa sweep of CollinsvilleTuesday, while ArcherCity and Hollidaywere unable to ad-vance.\\nCHICO — Witha chance to square offagainst the defendingchamps later this week,No. 7 Windthorst tookcare of business Tuesdaynight.\\nThe Trojanettes\ncrushed No. 27Collinsville in a sweepat Mineral Wells HighSchool, 25-10, 25-18,25-22. However, contro-versy preceded the finalscores.\\nAfter the firstgame, the Collinsvillecoach took umbrage wit\nbeat up on AngelinaCounty’s Collinsville,25-10, 25-15, at PikeProvident Bank Gym asthe only Nolan CountySchool District No.12 team still alive inthe Region I-2A tour-ney.\\nMPVP MARLIHOUSER spen\nswept Holliday in twogames, winning by thescores of 25-9, 25-12 tomove on to Thursday’sregional semifinal gamein China Spring againstthe winner of BrookshireRoyal and WaxahachieLife Christian.\nswept the CollinsvilleLady Lions 25-15, 25-16,25-16 in the Region II-2Aplayoffs to advance to thearea round. Windthorstwas the only team in Dis-trict 9-2A to beat de-fending state championFriona\n2 Eight Sussex skiers willtake to the slopes tobattle it out for thehonour of being crownedNational Championat the English AlpineSki Championshipswhich start this week-end.\\nBurgess Hillsisters, 18-year-old Samand 16-year-old HelenTodd-Saunders andCrawley\n’-based Louise D’Arcy,will be among theyoungest in a field com-prised of some of the bestjunior slalom skiers fromacross the UK.\\n\\nTheChampionships takeplace over four days atthe Les 2 Alpes ski\n’s Karl Holland (17) willcompete over a six dayperiod in Tignes, France,braving the ‘weapons ofmass destruction’ thatFrance has become fa-mous for.\\nLaura Rees(20 – Poole), AlabasterJones (17 –\nsisters Ella and ImogenKowal are joined by 15year-olds Sophie Gwarun-ski from Burgess Hilland Chichester’s LizzieKuzmenko and for-timesNational Champion Re-becca Burns, who is 17and from Lewes.\\nF\n’s Nancy Webb, 13, RudiHudman, 15, Jack Auger,16 and Ben Harsant, 20and 19-year-old Harro-gate skier Lucy Try, willcompete in England’slargest ski race of the sea-son which starts at theAlpe d’Hue\n3 It’s 14 degrees andsnowing. But at one ofMoscow’s new cooper-ative clothing markets,business is booming.Muffled against the cold,vendors shout promo-tions for their paltryofferings while othersmingle more discreetlywith the crowd, hawkingFrench perfume\nand expensive cosmet-ics.\\n\\nThey defy thelaw. underneath the ta-ble trade is illegal, butthe bearded young manswiftly managing the ex-otic goods, Darwin, hasnothing to fear. He’s apoliceman.\\n\\nDar\nand fake Rolex watchesrather than the cheapfleeces and caps thatdominate the mar-ket.\\n\\nOn a clear day,when the snow liesslushy on the sidewalk,patrolling cops evict thesuccessful and respected\npackaged in Russianboxes.\\n\\n## Sur-vivors Of Morocco GasPlane Crash Feted InParis\\n\\nDecember 03,2012\\nhttp://www.huffingtonpost.com/2012/11/30/morocco-gas-plane-crash_n_2212279.html\\n\\nParis, Fra\n, Italian pens and En-glish socks. On the fringeof this human maze threeyoung women show offa collection of sherry-brown cotton skirts, dot-ted with small medal-lions. Ostentatiously,they all have \"b\n4 Now Finally TakingShape: A World WithEver Less Decently-PaidWork for Most. ButMust This Be a Prob-lem?\\nFor centuries,experts/futurists havepredicted that machineswould someday makeworkers obsolete. Andnow it’s finally happen-ing, sporadically, hereand\nthere, in small bites,and unpredictably. Butit’s truly hard to evenimagine what this willmean for our future, bar-ring unexpected large-scale changes in attitudesabout the nature of workand in ou\nthere, in certain fields.As noted elsewhere manytimes over the yearshere, we’re seeing thismostly already in high-margin/sexy fields first(as in finance, movie in-dustry, design) and con-tinuing to\nthere, as an increasingpercentage of workers arelosing their jobs to ma-chines, and to corporategreed.\\n\\nIs this reallya problem? TaskRab-bit CEO says not, thatthere’s no class war, andhe’s hopin\nthere, but still mostly intraditional manufactur-ing. (’You Build It, WeJust Take the Money’:Afraid of your resumÁ©?ApplyatMcDonald’s!: Iswork becoming a dyinginstitution? Who actu-ally works t\n5 In our “always on” cul-ture, the office mantrais: work late, stayconnected. The problemis that working harderand longer doesn’t nec-essarily make you moreproductive. Researchshows that gettingaway—especially on ajourney that engagesyour mind and body,\nsuch as a Disney vaca-tion— can actually re-lieve the symptoms ofstress.\\n\\nWhat betterplace to reconnect withyour “inner you” thanthe peace, pamperingand productivity of aWalt Disney World®va-cat\none that unplugs youfrom your daily rou-tine—can bring real ben-efits that last long aftercrusty sandals have beendiscarded.\\n\\n“Travel isabout having a differ-ent experience,” says Dr.John Pencavel\nchallenges you to pushbeyond your comfortzone—can help you re-cover from burnout andmake you a more effec-tive, productive worker.Work breaks are increas-ingly popular, giving peo-ple an opportunity t\nand provides a changeof scenery—can leaveyou more energized andbetter able to focus whenyou return.\\n\\n###Passages Anamcara MiniRetreat\\n\\nThese five-hour luxurious retreatsoffer an exquisite\n34 Publicly-Detectable Watermarking for Language Models\nTable 9: Example completions from Mistral 7B [JSM+23]. For the Christ, Gunn, and\nZamir [CGZ24] scheme, we set security parameterλ= 16. For our scheme, we set signature\nsegment lengthℓ = 32, bit sizeb = 2, and maximum number of planted errorsγ = 2.\nCompletions are truncated to the first 200 characters.\n# Prompt Plain (tokens) Plain (bits) Christ et al. This work\n1 Windthorst pulled offa sweep of CollinsvilleTuesday, while ArcherCity and Hollidaywere unable to ad-vance.\\nCHICO — Witha chance to square offagainst the defendingchamps later this week,No. 7 Windthorst tookcare of business Tuesdaynight.\\nThe Trojanettes\nclipped the No. 25 LadyPanthers of Collinsvillein three sets, but in quitea display. The Girls fin-ished the night 25-17, 25-11 and 25-8.\\nWas thedisplay impressive? Well,let’s just say it was the\nmade quick work ofChico, sweeping theWildcats easily in theopening round of theChico district tourna-ment.\\nIn the ClassA championship todayat Chico High School,Windy will face Petrolia,who ups\nswept their Class 1ARegion II Area rivalCollinsville, 25-16, 25-11, 25-19 to advance tothe regional quarterfinalsand pull a step closer to arematch with defendingstate champion and areafoe Chil\n, victorious overCollinsville 3-0, willplay No. 2 MLK in theRegion I-3A volleyballsemifinals at 12:30 p.m.Friday in Austin.\\nTheLady Saints were up-ended 3-1 by Wellingtonin Tuesday’s remaining\n2 Eight Sussex skiers willtake to the slopes tobattle it out for thehonour of being crownedNational Championat the English AlpineSki Championshipswhich start this week-end.\\nBurgess Hillsisters, 18-year-old Samand 16-year-old HelenTodd-Saunders andCrawley\n’s 20-year-old rowerturned skier CameronBourke, are all deter-mined to do the clubproud.\\nThe Nationalsare a sell-out eventattracting athletes fromall over the country andsees around 400 competi\ntwins, 18-year-old sisters,Ellie and Rachael Bellare all racing at the En-glish Alpine Ski Cham-pionships over the nexttwo weeks hoping to takehome two crystal ski tro-phies to represent South-ern and\n’s Jake Moffat (17) haveall been selected tocompete in the oppen-ing races to decide thetop five challenged for2014.\\nThe squads se-lected to join the threeyoung slalom and giantslalom skiers are:\ndownhill skier FrancescaPoulton will competewith the national teamin the Under 21 cat-egories.\\nNigel Thomp-son, coach of BirchwoodSki Club in Brightonwhich develops skiers,said: “We are really p\n3 It’s 14 degrees andsnowing. But at one ofMoscow’s new cooper-ative clothing markets,business is booming.Muffled against the cold,vendors shout promo-tions for their paltryofferings while othersmingle more discreetlywith the crowd, hawkingFrench perfume\n(1,500 rubles a bottle),stolen army uniforms(50 rubles) and Chinese-made T-shirts toppedwith knock-off de-signer logos (4 rubleseach).\\n\\n\"It’s goodbusiness,\" says ElenaGumerova, who runs thesta\nor fake designer jeans.In a tent filled withsuitcases piled withhandbags, watches andbottles of perfume,Russian, Chinese andEast European womenbuy.\\n\\nMany of theitems are fake. Even thismost\n, Japanese designer suitsand the finest Hungar-ian chocolates. (Thisis typically a big weekfor Moscow retailers, asRussian women usuallywait until the December7th Day of Reconciliationand the Ne\nand jeans. \"GreatBartknows his stuff–the thun-der roll will be the gui-tar hook. And the cho-rus turns it into one greatone-word question. -RoatMarked I’ll. But thecold snap is threateningto pierc\n4 Now Finally TakingShape: A World WithEver Less Decently-PaidWork for Most. ButMust This Be a Prob-lem?\\nFor centuries,experts/futurists havepredicted that machineswould someday makeworkers obsolete. Andnow it’s finally happen-ing, sporadically, hereand\nthere, and particularly indeveloped nations. Yes-terday Switzerland be-came the first employerof an army of robots(geolocated in the na-tion’s food courts andbrick-and-mortar retailoutlets), the fir\nthere, and the generalpublic is not yet atall ready to cope withit. Humans are be-ing pushed out of evermore work. Africans toohave already found theirminds and fists no matchfor automated produ\nthere in earnest, andmore often in job ca-sualization and gigifi-cation.\\nHere in NorthAmerica and western Eu-rope we’re seeing tens ofmillions of jobs just van-ish or be photographedor programmed or\nthere. But what doesthis say about progress–and the ability of mostto keep from fallingpermanently into theranks of the unem-ployed and marginally-employed?\\nSky-HighMarginal Taxes ArePushed by\n5 In our “always on” cul-ture, the office mantrais: work late, stayconnected. The problemis that working harderand longer doesn’t nec-essarily make you moreproductive. Researchshows that gettingaway—especially on ajourney that engagesyour mind and body,\nhelps to sharpen focus,relax and reset. Outsidethe hotel lobby and chainrestaurants, the water-front and hiking trails ofthis historic city inviteconnection and encour-age a fresh perspective.Here\nenhances your creativity,and revitalizes yourenergy—puts you on thepath toward sustainedsuccess.\\n\\nVirgin’sBranson Group isoffering leadership devel-opment trips to exoticlocations that break o\nwhether it’s a retreat inthe desert or a weekendon the water —creates apersonal and professionalrecharge.\\n\\nHow? Youpause and get perspec-tive on where you’re at,where you’re going, andwhat mat\nlike a climbing trip—canspur your creativity,stoke your innovation,and make you feelproductive and fulfilled.By taking the trip, yourecharge emotionally,physically, and evencognitively. Yes,",
  "topic": "Digital watermarking",
  "concepts": [
    {
      "name": "Digital watermarking",
      "score": 0.7765864133834839
    },
    {
      "name": "Computer science",
      "score": 0.5436283349990845
    },
    {
      "name": "Computer security",
      "score": 0.3576218783855438
    },
    {
      "name": "Artificial intelligence",
      "score": 0.19624987244606018
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I135310074",
      "name": "University of Wisconsin–Madison",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210164862",
      "name": "Artificial Intelligence in Medicine (Canada)",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I51556381",
      "name": "University of Virginia",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    }
  ]
}