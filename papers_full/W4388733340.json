{
  "title": "Benefits and Harms of Large Language Models in Digital Mental Health",
  "url": "https://openalex.org/W4388733340",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2123618928",
      "name": "Munmun De Choudhury",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2923596521",
      "name": "Sachin R Pendse",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2108620924",
      "name": "Neha Kumar",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6600560973",
    "https://openalex.org/W3114790994",
    "https://openalex.org/W3040472729",
    "https://openalex.org/W6600002382",
    "https://openalex.org/W6607077144",
    "https://openalex.org/W2921659045",
    "https://openalex.org/W6600451642",
    "https://openalex.org/W6600025664",
    "https://openalex.org/W175233344",
    "https://openalex.org/W4236644011",
    "https://openalex.org/W4234281470",
    "https://openalex.org/W2085798593",
    "https://openalex.org/W3132495213",
    "https://openalex.org/W2017675395",
    "https://openalex.org/W6600151197",
    "https://openalex.org/W6609907923",
    "https://openalex.org/W6678746234",
    "https://openalex.org/W6601013545",
    "https://openalex.org/W6606440526",
    "https://openalex.org/W6825607749",
    "https://openalex.org/W6742851896",
    "https://openalex.org/W6604788247",
    "https://openalex.org/W6600195515",
    "https://openalex.org/W6609689914",
    "https://openalex.org/W6818248265",
    "https://openalex.org/W6776981881",
    "https://openalex.org/W6629414214",
    "https://openalex.org/W6602409367",
    "https://openalex.org/W6818032520",
    "https://openalex.org/W6600234944",
    "https://openalex.org/W6602141972",
    "https://openalex.org/W6825285808",
    "https://openalex.org/W6602334313",
    "https://openalex.org/W6610186178",
    "https://openalex.org/W6600505529",
    "https://openalex.org/W6606698219",
    "https://openalex.org/W2022613111",
    "https://openalex.org/W2162051395",
    "https://openalex.org/W4310938578",
    "https://openalex.org/W4304112108",
    "https://openalex.org/W4200087658",
    "https://openalex.org/W72496981",
    "https://openalex.org/W3112116031",
    "https://openalex.org/W4386913016",
    "https://openalex.org/W2977128309",
    "https://openalex.org/W4309442889",
    "https://openalex.org/W2088057669",
    "https://openalex.org/W4385571694",
    "https://openalex.org/W2047538252",
    "https://openalex.org/W3090130288",
    "https://openalex.org/W3174252437",
    "https://openalex.org/W2132084060",
    "https://openalex.org/W4385468994",
    "https://openalex.org/W3201412806",
    "https://openalex.org/W4383186888",
    "https://openalex.org/W2773170638",
    "https://openalex.org/W2999286925",
    "https://openalex.org/W4246938759",
    "https://openalex.org/W2086793406",
    "https://openalex.org/W4293003888",
    "https://openalex.org/W4286975337",
    "https://openalex.org/W4380627488",
    "https://openalex.org/W4382992448",
    "https://openalex.org/W2091909921",
    "https://openalex.org/W2318723339",
    "https://openalex.org/W1996783985",
    "https://openalex.org/W4387405016",
    "https://openalex.org/W3163562865",
    "https://openalex.org/W2465078555",
    "https://openalex.org/W2067121749",
    "https://openalex.org/W4385266429",
    "https://openalex.org/W4304112197",
    "https://openalex.org/W2922711788",
    "https://openalex.org/W4383346782",
    "https://openalex.org/W4385246972",
    "https://openalex.org/W2978761732",
    "https://openalex.org/W3007934595",
    "https://openalex.org/W2976367414",
    "https://openalex.org/W4317757464",
    "https://openalex.org/W3164886736",
    "https://openalex.org/W2973379954",
    "https://openalex.org/W4313219932",
    "https://openalex.org/W4318671461",
    "https://openalex.org/W3205879657",
    "https://openalex.org/W4322397507",
    "https://openalex.org/W4383500963",
    "https://openalex.org/W4379599615",
    "https://openalex.org/W2735406191",
    "https://openalex.org/W2112591624",
    "https://openalex.org/W4386501849",
    "https://openalex.org/W2883860074",
    "https://openalex.org/W4226353085",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W4366588626",
    "https://openalex.org/W4248086932",
    "https://openalex.org/W4383301640",
    "https://openalex.org/W2931958159",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W2318994447",
    "https://openalex.org/W2964262738",
    "https://openalex.org/W4387329158",
    "https://openalex.org/W4385381606",
    "https://openalex.org/W3175135058",
    "https://openalex.org/W1984982865",
    "https://openalex.org/W2743226859",
    "https://openalex.org/W4246454792",
    "https://openalex.org/W4200223551",
    "https://openalex.org/W4389636360",
    "https://openalex.org/W3154272574",
    "https://openalex.org/W1964771253",
    "https://openalex.org/W4386575491",
    "https://openalex.org/W4365816082",
    "https://openalex.org/W3201511368",
    "https://openalex.org/W2059216172",
    "https://openalex.org/W1902254884",
    "https://openalex.org/W3165373184",
    "https://openalex.org/W2953417263",
    "https://openalex.org/W4312220150",
    "https://openalex.org/W2546928310",
    "https://openalex.org/W28333651",
    "https://openalex.org/W3160567393",
    "https://openalex.org/W3212304951",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W4220763927",
    "https://openalex.org/W4280489099",
    "https://openalex.org/W4281489185",
    "https://openalex.org/W4387823073",
    "https://openalex.org/W4385572158",
    "https://openalex.org/W4386272586",
    "https://openalex.org/W4296056389",
    "https://openalex.org/W1982698797",
    "https://openalex.org/W4376988648",
    "https://openalex.org/W4387356406",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W3013908145",
    "https://openalex.org/W4384457683",
    "https://openalex.org/W4225002570",
    "https://openalex.org/W4387500346",
    "https://openalex.org/W4385473492",
    "https://openalex.org/W2405042511",
    "https://openalex.org/W3011449107",
    "https://openalex.org/W4396218868",
    "https://openalex.org/W4387928375",
    "https://openalex.org/W4361251463",
    "https://openalex.org/W2123930025",
    "https://openalex.org/W2889391310",
    "https://openalex.org/W3169148746",
    "https://openalex.org/W2975086838",
    "https://openalex.org/W2981869278",
    "https://openalex.org/W2327046676",
    "https://openalex.org/W4288029087",
    "https://openalex.org/W4387617694",
    "https://openalex.org/W2154967513",
    "https://openalex.org/W2948999923",
    "https://openalex.org/W4229913394",
    "https://openalex.org/W4377865288"
  ],
  "abstract": "The past decade has been transformative for mental health research and practice. The ability to harness large repositories of data, whether from electronic health records (EHR), mobile devices, or social media, has revealed a potential for valuable insights into patient experiences, promising early, proactive interventions, as well as personalized treatment plans. Recent developments in generative artificial intelligence, particularly large language models (LLMs), show promise in leading digital mental health to uncharted territory. Patients are arriving at doctors' appointments with information sourced from chatbots, state-of-the-art LLMs are being incorporated in medical software and EHR systems, and chatbots from an ever-increasing number of startups promise to serve as AI companions, friends, and partners. This article presents contemporary perspectives on the opportunities and risks posed by LLMs in the design, development, and implementation of digital mental health tools. We adopt an ecological framework and draw on the affordances offered by LLMs to discuss four application areas---care-seeking behaviors from individuals in need of care, community care provision, institutional and medical care provision, and larger care ecologies at the societal level. We engage in a thoughtful consideration of whether and how LLM-based technologies could or should be employed for enhancing mental health. The benefits and harms our article surfaces could serve to help shape future research, advocacy, and regulatory efforts focused on creating more responsible, user-friendly, equitable, and secure LLM-based tools for mental health treatment and intervention.",
  "full_text": "Benefits and Harms of Large Language Models\nin Digital Mental Health\nMunmun De Choudhury†, Sachin R. Pendse, Neha Kumar\nSchool of Interactive Computing, Georgia Institute of Technology, Atlanta, GA, USA\n{munmund, sachin.r.pendse, neha.kumar}@gatech.edu\n† Corresponding author\nAbstract\nThe past decade has been transformative for mental health research and practice. The abil-\nity to harness large repositories of data, whether from electronic health records (EHR), mobile\ndevices, or social media, has revealed a potential for valuable insights into patient experiences,\npromising early, proactive interventions, as well as personalized treatment plans. Recent devel-\nopments in generative artificial intelligence, particularly large language models (LLMs), show\npromise in leading digital mental health to uncharted territory. Patients are arriving at doctors’\nappointments with information sourced from chatbots, state-of-the-art LLMs are being incor-\nporated in medical software and EHR systems, and chatbots from an ever-increasing number\nof startups promise to serve as AI companions, friends, and partners. This article presents\ncontemporary perspectives on the opportunities and risks posed by LLMs in the design, devel-\nopment, and implementation of digital mental health tools. We adopt an ecological framework\nand draw on the affordances offered by LLMs to discuss four application areas—care-seeking\nbehaviors from individuals in need of care, community care provision, institutional and med-\nical care provision, and larger care ecologies at the societal level. We engage in a thoughtful\nconsideration of whether and how LLM-based technologies could or should be employed for\nenhancing mental health. The benefits and harms our article surfaces could serve to help shape\nfuture research, advocacy, and regulatory efforts focused on creating more responsible, user-\nfriendly, equitable, and secure LLM-based tools for mental health treatment and intervention.\n1 Introduction\n“When I use ChatGPT to talk things through and vent about how I feel, it goes on to tell me\nto get help and that I’m not alone. But why does it feel as if it’s mocking me? It feels as if it’s\nhaving a laugh at my expense. ”– A paraphrased social media post\nIn November 2022 [1], OpenAI released ChatGPT. ChatGPT followed the mold of past chatbots by\nproviding a simple interface for people to easily interact with a conversational agent. However, unlike past\npublicly accessible chatbots, ChatGPT was powered by OpenAI’s proprietary language generation model,\noften called Large Language Models (LLMs). OpenAI’s LLM (named GPT, for Generative Pre-trained\nManuscript under peer review.\n2\nTransformers [2]) was created through a large-scale collection of text from the Internet combined with man-\nual review through a process often called Reinforcement Learning From Human Feedback (RLHF) [3].\nChatGPT’s underlying language and simple interface astonished users with answers that were surprisingly\ncoherent and wide-ranging. Since then, conversations across academic, medical, industry, and policy do-\nmains have begun to discuss how LLMs could offer new possibilities for diagnosis, treatment, and patient\ncare in mental health.\nOver the past decade, there has been increased conversation around the growing potential for digital\ntechnologies, artificial intelligence (AI), and machine learning to contribute value to mental health research\nand practice. Research has demonstrated some of this potential. For example, methods from natural lan-\nguage processing (such as sentiment analysis) have been used to assess people’s emotional states from their\ntext, speech, or social media language [4, 5]. These studies have consistently shown that computational or\npredictive analyses of digital data can accurately detect mood [6], mental health states [7], and even the risk\nof potential harm [8] and suicide [9]. Collectively, the implications of this research include the potential for\nvaluable insights into daily patient experiences, a paving of the way for early and proactive interventions,\nand the design of personalized treatment plans. However, as people further rely on online tools to seek\ncare for their mental health, researchers and activists have sounded the alarm about the potential for harm\nif digital mental health interventions are staged without the awareness or consent of people experiencing\ndistress [10, 11]. Scholars have also expressed concern that the use of predictive analytics in mental health\ncould compromise patient and clinician agency [12], exacerbate systemic disparities encoded in the training\ndata of AI models [10, 13], and propagate insights poor in clinical grounding or construct validity [14].\nThe challenges in implementing an AI-informed mental health care model have further invited criticism and\nskepticism around the role of AI in this field [15], even as researchers have increasingly sought to draw upon\nboth computational and psychiatric expertise and paradigms to advocate for an “(AI) model-based psychi-\natry” [16]. With the rapid introduction of LLMs in new parts of everyday life, initiatives in digital mental\nhealth are likely to be similarly disrupted, opening doors to new opportunities for mental healthcare, while\nalso setting the stage for new and previously unconsidered harms.\nThis discussion of benefits and harms can already be seen unfolding in popular discourse around LLMs\nand mental health. For instance, ChatGPT was not created as a mental health support tool—however, people\nin distress have started to use ChatGPT for mental health support and non-judgmental guidance, as the\nopening quote to this Introduction shows. As discussed in online communities (such as Reddit [17]), users\nhave turned to ChatGPT in moments of suicidal ideation, and expressed their belief that ChatGPT saved\ntheir life in moments of despair. Other users have described their approach to carefully training LLM-based\nchatbots to behave as therapists from different theoretical orientations, such as prompting chatbots to take\non the role of an Acceptance and Commitment (ACT) therapist. Several mental healthcare organizations and\ncompanies [18–20] have also begun to research the integration of LLMs into the design of their services.\nThis increased use of LLMs in service delivery has been met with excitement [21], but also with justified\nskepticism, given potential racial or gender biases [22, 23] and unexpected outputs [24] from LLM-based\nchatbots. For example, in June 2023, the National Eating Disorder Association was forced to shut down a\nchatbot created to provide clinically validated information after the chatbot provided harmful and dangerous\nadvice to users, including diet and weight loss advice [24, 25]. Unbeknownst to the administrative staff at\nNEDA, the chatbot company that provided its services to NEDA had “rolled out an AI component to its\nchatbots” [24]. The chatbot was introduced soon after several pivotal events, including an attempt by NEDA\nhelpline volunteers to unionize and the closing of the NEDA helpline, raising awareness of the potential\nthat nascent technology may be used to replace human staff [25]. The harms of irresponsible uses of LLM-\nbased technologies can even be lethal—according to a report from the Belgian news outlet La Libre (via\nBelga News Agency) [26], a Belgian man is said to have tragically ended his own life after engaging in\nconversations with an AI chatbot, for six weeks, discussing climate change.\nManuscript under peer review.\n3\nThe CareseekerInteract with an LLM-powered chatbot without stigma or fear of judgment\nThe CaregiverUse LLMs to respond to individuals in crisis with contextually-and culturally appropriate advice\nThe InstitutionLeverage LLMs to learn from large data to aid evidence-based decision-making\nThe SocietyEnhance tele-mental health services with patient-centered provider training and optimizing care delivery paradigms\nFigure 1: An ecological conceptualization of the use of large language models in digital mental\nhealth, based on the Social Ecological Model [34].\nMany of the arguments that highlight the potential harms of LLMs are warranted. LLMs are trained on\nan Internet that is largely devoid of fact-checking. As a result, LLMs often reproduce convincing misinfor-\nmation [27], and in the context of the COVID-19 pandemic, were found to be capable of generating highly\npersuasive, difficult to discern health misinformation about COVID-19’s precautionary and prevention mea-\nsures [28]. Some have described LLMs as being similar to super-powerful auto-completion tools, as it can be\nhard to systematically control for specific outputs [29]. This can be problematic in a mental health context,\nwhere the success of interventions can be highly dependent on the nature of a provider’s response [30–32].\nLLMs have been described as a “blurry JPEG image” [33] of the rest of the Internet, similarly containing\nboth substantial utility as well as the potential for harm to users. Today, just as in digital mental health\nspaces, there are widespread debates around the consequences of LLMs for truthful public discourse on one\nhand, and productivity and efficiency on the other.\nThis article is situated against the backdrop of this larger debate. We present a state-of-the-art informed\nperspective on contexts where LLMs can potentially be beneficial, and where there may be significant risk\nof harm. We conclude with directions and recommendations that can help amplify benefits while also\nminimizing harms.\n2 Theoretical Frameworks\nDrawing on Insel’s [35] four key areas where LLMs could influence mental health, we conceptualize bene-\nfits and harms in the use of LLMs for administering tele-mental healthcare, for supporting crisis response,\nfor providing clinical decision support in psychiatric settings, and for psychotherapy. To organize our dis-\ncussion, we adopt the Social Ecological Model widely used in the public health field [34] and map each of\nInsel’s potential application contexts into a four-tier framework. This ecological analysis allows us to un-\nderstand the complex interplay between individuals and the broader social and environmental contexts they\nare embedded in—contexts that influence and shape people’s mental health as well as contexts in which\nLLM-based technologies could exist.\nFigure 1 shows this organization of the potential uses of LLMs in mental health. The innermost layer\nfocuses on the individual seeking mental healthcare and considers personal factors such as values, beliefs,\nknowledge, skills, and experiences in how an individual might appropriate LLMs to seek help and support\nto overcome their distress. The next layer is the caregiver layer, where LLMs might be appropriated by\nsupporters, counsellors, crisis volunteers, and others to respond to an individual in crisis. The third layer\ncenters around institutional efforts and initiatives in caregiving, where we discuss the role that LLMs can\nManuscript under peer review.\n4\nplay in enabling decision-making within health systems. At the final layer, we discuss potential futures\nwhere LLMs could transform telehealth paradigms.\nIn addition, to systematically identify and discuss the factors that can support or hinder caregiving at\neach level resulting from LLMs, we adopt the concept of affordances [36]. Affordances refer to the potential\nactions or uses that an object, environment, or system offers to individuals based on their perceived charac-\nteristics or features [37]. To understand the benefits and harms of LLMs in digital mental health, we begin\nby describing the various affordances of LLMs that are relevant for the above four-tiered ecological frame-\nwork. First, LLMs enable natural language understanding—users can input text-based queries, prompts,\nor commands in natural language, and the model interprets and responds to them coherently. Second, by\noffering end users the ability to engage in open dialogue, and by parsing the semantics of the user input,\nLLMs allow users to interact with conversational agents in a natural environment [38]. These affordances\nare relevant to the use of LLMs in psychotherapy. Next, LLMs are extremely powerfulinformation retrieval\nsystems and are able to parse very large repositories of data efficiently—users can thus request information\non a wide range of topics by asking questions or requesting explanations and the model can provide answers,\nsummaries, or context on various subjects. This affordance is relevant to the use of LLMs in crisis response.\nThird, LLMs can provide predictions of outcomes of interest by learning from its underlying data—users\ncan seek answers, recommendations, and suggestions from the LLM for various scenarios and use cases,\nmaking this affordance relevant to clinical decision support. Finally, LLMs also allow efficienttext summa-\nrization—users can request concise summaries of longer texts, making it easier to grasp the main points of\nan article, document, or report. Complementarily, LLMs can be used for content generation and machine\ntranslation tasks. Together, these features can be highly valuable for envisioning new models of telehealth.\n3 For Careseekers: Psychotherapy Chatbot\nPerhaps one of the most widely spoken of applications of LLMs in mental health centers around their use\nfor psychotherapy; Insel discusses the potential for LLMs, particularly GPT-4, to serve as autonomous\ntherapists [35]. In fact, LLM-powered chatbots like Replika are already claiming to be an “AI comparison,\n[...] always ready to chat when [a person] needs an empathetic friend” [39]. However, for decades, the idea of\nusing a machine to serve as a therapy bot has met with significant heated debate and controversy, specifically\naround their value to individuals in distress. For example, the Rogerian therapy chatbot ELIZA was first\ncreated by Joseph Weizenbaum as a commentary on the irreplaceability of human interaction in mental\nhealth support [40]. After careseekers in distress found ELIZA to provide useful support, some mental\nhealth professionals argued that chatbots may be able to scale up basic forms of mental health support [41].\nWeizenbaum was shocked by this enthusiasm, and responded that “no humane therapy of any kind” can or\nshould be done via a chatbot computer program [42], grounding his argument in Rogers’s own ideas around\nperson-centered therapy. These debates continue today, and this section synthesizes the salient points in\nthese conversations.\n3.1 Potential Benefits\nImproving the Reach of Therapeutic Care. Mental health conversational agents have historically\nbeen rule-based, meaning they would engage with users based on predetermined scripts [43]. This limitation\nmade it challenging for conversational agents to deliver human-like interactions, as they could not engage in\nopen-ended conversations that were tailored to users’ emotional requirements. LLMs (grounded in genera-\ntive AI) have demonstrated impressive performance in participating in realistic human-like conversations in\na coherent manner, following practically any type of prompt from the end user [44].\nManuscript under peer review.\n5\nA subsequent benefit offered by this type of naturalistic LLM-powered chatbots in psychotherapy is\naccessibility. Chatbots are available around the clock, providing users with a convenient and immediate\nsource of support. This can be especially valuable for individuals who have difficulty accessing traditional\nin-person therapy due to geographical or scheduling constraints [45], those living in underserved areas, such\nas mental health professional shortage areas [46, 47], or those who might be otherwise marginalized in\nconventional mental health care [48, 49]—over half of U.S. counties lack a single psychiatrist [46], and a\nrecent survey revealed that 60% of mental health professionals have had no available slots for new patients\nfollowing the COVID-19 pandemic [50]. Research has demonstrated that online and self-guided single-\nsession interventions can reduce depressive symptoms [51, 52]. The naturalistic and accessible interface\nassociated with LLM-powered chatbots may allow for a new modality to deliver efficacious and self-guided\npsychotherapy interventions.\nImproving Therapeutic Quality through Personalization and Evidence-based Adaptation.\nLLM-based chatbots are trained on large amounts of historical data, to the order of many terabytes of\ndata [53]. Current and future LLMs could be further fine-tuned to be better at providing support based on\nwhether past support responses (from humans or the chatbot) were evaluated by humans as being efficacious.\nThis might include the specific emotional patterns, style, or tonality of response that align best with the\nneeds and expectations of a given client or patient. Researchers have demonstrated that natural language\nprocessing techniques that harness the power of LLMs can empower and equip supporters with writing\nsuggestions during practice training sessions. For example, Hsu et al. [54] create the CARE system for mock\nchats, which uses LLMs to offer suggestions to online peer supporters as they undergo training. Participants\nfound recommendations from the CARE system to be helpful, particularly when faced with uncertainty\naround how best to respond during the mock training session. Learnings from LLM-based systems can be\ndirectly applied when supporters are helping people in distress without the aid of any computational system.\nSimilarly, Sharma et al. [55] developed HAILEY , an AI-powered agent that offers real-time guidance to\nenhance the empathetic responses of peer supporters as they assist those in need of support. The research\nfound that peer supporters can effectively incorporate AI feedback with this system, benefiting from both\ndirect and indirect AI assistance, without developing an excessive reliance on AI. In contrast, supporters\nreported enhanced self-efficacy after receiving AI feedback, underscoring the promise of LLMs as teaching\naids for supporters. LLMs can further allow for an enhanced ability to use data for actionable and evidence-\nbased insights into the nature of support, including how to adapt support for the individual and their context.\nDestigmatizing the Seeking of Care. LLM-based chatbots can be built to draw on evidence-based\npsychotherapy techniques to deliver helpful support exercises for a user, with potentially less stigma than\ntraditional psychotherapy. Recent empirical research has demonstrated that language models can be utilized\nto help individuals work to reframe negative thoughts and beliefs through in-context learning [56], if trained\non relevant data. For example, Ziems et al. [57] draw upon six theoretically grounded reframing strategies to\ndemonstrate that language models can be used to identify and reframe various types of cognitive distortions.\nSimilarly, Sharma et al. [56] utilize a dataset consisting of potential thoughts and corresponding reframed\nthoughts (validated by practitioners) to train an LLM that generates reframed thoughts for new contexts. 1\nThis work demonstrates the promise of LLMs in providing CBT-based exercises that are personalized to a\nuser’s context, which may lower the barriers to seeking treatment or support, often stemming from stigma,\nshame, or structural factors, as has been argued for AI as well [58, 59]. LLM-based chatbots may thus\nmotivate more people to reach out for mental health needs, and help normalize the process of seeking\ncare through self-disclosure and processing of thoughts, whether with other humans or chatbots. Such\n1This system is publicly testable at bit.ly/changing-thoughts.\nManuscript under peer review.\n6\nnormalization of care-seeking has been identified to be very important to fight the prevailing societal stigma\nsurrounding mental illness [60].\n3.2 Potential Harms\nBlack [61] has noted how those working at the intersection of AI and care are quick to explain that “a\nchatbot will never equal, let alone surpass, the abilities of a human therapist or counselor.” However, a\ngrowing number of commercial initiatives are building LLM-informed psychotherapy chatbots, with a belief\nthat “mental health chatbots [can be] instantly and all but universally available at a fraction of the cost of a\ntherapist” [61]. Given the pace of development, while there may be benefits for those in distress, a careful\nconsideration of harms is crucial.\nAn Erosion of the Therapeutic Alliance. The therapeutic alliance, characterized by trust, mutual\nrespect, and emotional connection, is a key predictor of psychotherapy outcomes. The emotional connection\nbuilt between psychotherapists and their clients plays a pivotal role in developing this alliance [62]. Patients\nare more likely to engage actively in psychotherapy sessions and work towards making behavior changes\nwhen they perceive a sense of genuine care and understanding from their therapist [63].\nChatbots may be trained on billions of documents that describe rich emotional experiences, but they\nlack the capacity for genuine emotional connection. Psychotherapists are trained to not only understand\na client’s emotional state, but to also empathize with a client’s feelings through their own lived emotional\nexperiences. Psychotherapists discern the nuances of complex emotional experiences, including co-occuring\nsadness, anger, and fear, and respond in a compassionate manner. They can also pick up on non-verbal cues\n– body language, facial expressions, tone of voice, and even pauses in speech are all non-verbal (or textual)\ncues that psychotherapists interpret [64]. LLM powered chatbots, in contrast, are limited in their capacity to\nunderstand or interpret the broader context of the client’s experiences, as expressed through these non-verbal\ncues. This limitation makes it more difficult for a connection to be built between chatbot and user.\nThe emotional connection between therapist and client is also foundational to building trust and rapport,\nparticularly when clients are assured that their expressions of distress and vulnerability are confidential [65,\n66]. This trust is essential for successful psychotherapy, as it encourages open and honest communication.\nPsychotherapy excels in an environment when patients are comfortable sharing their deepest concerns, fos-\ntering a therapeutic alliance [67]. To date, it is unclear if chatbots can build the type of trust and rapport\nthat is essential to successful therapy, particularly given the potential for digital mental health data to be\nleaked, sold, or even legally used for targeted advertising [68, 69]. LLM chatbots risk taking away the hu-\nman element of psychotherapy. Existing studies have demonstrated that users express negative reactions to\nunhelpful and risky chatbot responses [70], which is dangerous – first experiences with mental healthcare\ncan quickly set the tone for future engagements (or disengagements) with mental healthcare [71].\nA Lack of Appropriate Safeguards for Patient Safety. The excitement for LLM-based therapy\nbots needs to be tempered with the reality and safety issues they pose to clients and patients. Generative\nAI-based conversational agents have been found to be unable to recognize, and respond appropriately to,\nsigns of distress [70]. Most commercial chatbots for mental health claim psychology-trained workers to\ncarefully write and evaluate the dialogue for these bots [61]. However, in the interest of safety, chatbots can\nbe constrained by predefined scripts [24] and may struggle to adapt to the intricate and evolving emotional\nstates of clients. Research has also shown that while chatbots can offer structured interventions that can\nbe easily scaled up (such as self-guided exercises from CBT), sustained user engagement often requires\nthe involvement of human psychotherapists [72]. It is also important to note that many mental health issues\ninvolve complex and multifaceted emotional experiences. Patients dealing with trauma, grief, or deep-seated\nManuscript under peer review.\n7\nemotional struggles often require nuanced, empathetic support that goes beyond providing information or\nstructured interventions [73]. Human therapists can adapt and respond to the evolving needs of their clients,\nmaintaining their motivation and commitment to therapy, and tailor their responses and interventions to the\nunique needs of each individual [74]. LLM chatbots may lack capacity for such personalized adaption of\ntherapeutic techniques in response to unique patient needs.\n4 For Care Providers: Crisis Response\nThe introduction of the 988 crisis response number in July 2022 was a significant step forward in the field\nof mental health [75, 76]. This new service provides individuals in crisis with immediate access to men-\ntal health support, as an alternative to calling 911 for police or fire department assistance. However, one\nchallenge that remains is the scarcity of a trained workforce to respond effectively to mental health crises,\noften resulting in calls being transferred to 911 and risking additional harm to callers [77]. Between 2015\nto 2020, 23% of people who were killed by police in the U.S. were experiencing mental health crises at the\ntime [78]. Insel [35] highlights the potential of LLMs in addressing this issue, emphasizing their dual role\nin potentially detecting the severity of a crisis and supporting 988 in providing real-time assistance. Below,\nwe explore the benefits and harms of using LLMs in this context.\n4.1 Potential Benefits\nMatching Users to Contextually Relevant Volunteers. Meta-reviews of existing crisis line ser-\nvices have revealed a lack of effectiveness, especially of distal outcomes, such as reduction in symptoms\nor feelings of distress in callers in a followup period after the call [79]. Empirical studies of helplines have\ninvestigated the reasons driving this phenomenon, finding that they stem from dissatisfied callers and re-\nsponders inability to attend to callers’ diverse needs [80]. LLMs could strengthen infrastructures like 988\nby helping to effectively route people in distress to helpline volunteers based on an assessment of their\nneeds. Through their proficiency in natural language understanding, LLMs could be utilized to analyze\nthe language of distress to match people to the types of context-specific support and specialized volunteers\nthat they may need. This rapid assessment can help ensure that individuals in immediate need receive the\ncontextually relevant forms of support they need promptly. In turn, this could reduce the burden on cri-\nsis responders by prioritizing high-risk cases. These possibilities have already been demonstrated in prior\nresearch – Althoff et al. [81] used data from an SMS texting-based counseling service where people in cri-\nsis engaged in therapeutic conversations with counselors, to build computational approaches that described\nwhich types of language of volunteers elicited better conversational outcomes.\nCulturally-Sensitive Vetted Guidance. LLM technologies can be harnessed to bolster the expertise of\ncrisis responders and volunteers through real-time linguistic framing and support. This potential is supported\nby recent research that has trained language models towards semantic, issue-based, and lexical reframing of\nopinions, arguments, as well as unhelpful thoughts [82, 83]. Additionally, LLMs could help by surfacing\nguidance and recommendations to specific crisis situations which have been previously vetted (by human\nexperts), to be helpful in mitigating crisis; Sharma and De Choudhury [84] highlighted this potential through\nmodels that learn from positive support seeking and support provisioning engagements on online forums.\nLLMs could help to suggest appropriate interventions, coping strategies, and de-escalation techniques based\non the information provided by the caller and matching this to similar crisis intervention scenarios in his-\ntorical data. This real-time assistance can be invaluable in calming the situation and ensuring the safety\nof the individual in crisis. Further, we discussed in the previous section that LLMs can be programmed to\nrespond in linguistically diverse ways; in a crisis scenario, culturally resonating support can have significant\nManuscript under peer review.\n8\nimpact on the caller’s mental health outcomes. Prior research has shown how language barriers can hinder\neffective crisis intervention [80]; thus by empowering crisis volunteers with language tailored to the identity\nand culture of the caller, LLMs could help promote greater inclusivity on helplines, including 988 as well\nas those in more resource-constrained settings.\n4.2 Potential Harms\nA Lack of Contextual Understanding. Although AI-powered crisis response has been advocated to\nbe particularly helpful during rapidly evolving ad well as protracted societal crises like mass shootings [85]\nand the COVID-19 pandemic [86] due to their ability to be deployed quickly and at scale, crisis response is\nan extremely high-stakes domain, and thus risk and harms could have debilitating impacts on stakeholders\ninvolved, especially the callers. First, multiple factors influence what precipitates a crisis as well as what\nstrategies could help mitigate it [87]. Such factors could exist outside the realm of the training data used\nto build the LLMs, often perhaps in messy offline contexts—contexts in which LLMs may have little to\nno insight. Further, the “black box” nature of such AIs make identifying contextual gaps inscrutable [88].\nWhile a human crisis responder could be well-equipped, trained, or use their awareness of the situation to\nprobe those unobserved factors behind the crisis, LLMs may provide inappropriate or hallucinated responses\nor those without sufficient empathy, potentially leading to an ineffective, harmful, or non-consensual crisis\nresponse that perhaps even worsens the caller’s emotional state. Even with prompt engineering, it can be\nhard to control what an LLM may say to an individual in crisis—the harmful outputs produced by the AI-\nassisted NEDA chatbot, as introduced in the Introduction, demonstrates how harmful directly exposing crisis\nresponse service users to LLMs may be.\nThe Complexities of Data Use and Consent. Ultimately, an LLM is only as good as its training\ndata [29]. Scholars have repeatedly discussed how by learning from large datasets on the internet, LLMs\ncould “overrepresent hegemonic viewpoints and encode biases” [29], creating ethically contentious out-\ncomes potentially extending or even exacerbating inequities in care [89]. However, beyond diversity, the\nscale and scope of the training data also matters, especially in an application domain like crisis response.\nThe successful use of LLMs in crisis responses hinges on being able to train them on copious amounts of\ndata spanning caller-volunteer conversations. Normally, these conversations are seldom recorded or tran-\nscribed beyond service optimization purposes, both to protect confidentiality of the data, as well as to ensure\ncallers find the crisis resource to allow more disinhibited exchange with the volunteer [90]. It is known that\nknowledge of being tracked or monitored could create a “Hawthorne effect” [91] leading to people being\nless truthful of their thoughts and feelings, and perhaps feeling silenced and fearful of the consequences of\nsurveillance. Since privacy is often “contextual” [92], in a crisis setting, callers might be concerned about\nhow their data is collected, stored, and perhaps most importantly, who does what with this data. An emerg-\ning crisis may also present challenges to a caller’s capacity to recognize these potential harms and to make\nthe most rational decision for themselves. The best example of this might be the Crisis Text Line scandal\nfrom 2022, where the efforts of the organization to collect and share conversational data with a for-profit\nspinoff without user consent alarmed many [93]. Thus efforts to collect conversational data going forward,\nto train LLMs, may undermine the goals of adequate assessment of a caller’s experience and deploying the\nmost appropriate intervention. Sourcing training data without adequate informed consent or participatory\ninvolvement of the data producers (e.g., the people in distress seeking help) may further complicate their\nuse in LLMs, by reducing their agency in controlling “what data is captured, how it is used, or who it\nbenefits” [94] and by rendering their data labor invisible [95].\nManuscript under peer review.\n9\n5 For Institutions: Clinical Decision Support\nIn his article, Tom Insel argued that LLMs can provide clinicians with comprehensive and up-to-date in-\nformation, aiding in the decision-making process [35]. We examine the benefits and potential harms of\nincorporating LLMs into clinical decision support.\n5.1 Potential Benefits\nUnlocking Vast Medical Knowledge. One of the most significant advantages of using LLMs in clin-\nical decision support is the ability to access a wealth of information. These models can learn from vast\namounts of medical literature, offering clinicians insights on various conditions, treatments, and potential\nside effects, including that is latest in the medical field. Side effects of psychiatric medications in particular\nare often very nuanced and demonstrate patient heterogeneity in effects [96, 97]. LLMs can not only surface\nhow similar patients have responded to specific treatments but also can help inform health professionals\nabout previously unknown potential side effects by learning from complex drug interactions spanning thou-\nsands of clinical trials [98] and online discussions around interactions [99]. This knowledge can assist\nhealthcare professionals in making well-informed decisions. The knowledge ingested by LLMs can also be\nutilized toward predictive analytic approaches, in order to augment decision support about patient outcomes,\nhospital readmission risk, and disease progression—all of which have been shown to be outcome predictable\nusing machine learning techniques [100–102]. LLMs could both improve the precision of these predictions\nand aid in proactive patient management and resource allocation.\nProviding Individually-Tailored Recommendations. LLMs can provide tailored recommendations\nbased on the patient’s unique circumstances that is gleaned from their historical electronic health records,\nclinical notes, or hospital discharge summaries, which together can significantly impact patient care and\nimprove patient outcomes [103]. LLMs can easily and quickly ingest diverse types of conventional health\n(EHR) and health-adjacent data (e.g., smartphone or wearable use, social media activities) of patients to\ndevelop such personalized models [104], and utilize it for differential diagnosis [105]. When it comes to\npersonalized treatment, differential diagnosis is perhaps one of the biggest strengths offered by LLMs [105].\nWith this knowledge, clinicians may be empowered to reduce the risk of misdiagnosis [106]; misdiagnosis\nhurts the efficacy of therapeutic and pharmacologic treatments down the road, and can enable individuals\nto function better in their personal and professional lives, maintain relationships, and achieve their life\ngoals. LLMs can importantly democratize the medical knowledge encoded in interactions amongst health\nprofessionals by providing information not only to clinicians but also to patients and their families [107].\nInformed patients can engage in shared decision-making with providers, fostering a collaborative approach\nto healthcare and improving health literacy [108].\n5.2 Potential Harms\nPerpetuating Misinformation and Contextually Uninformed Decisions. Relying solely on LLMs\nfor clinical decision support without verification from human experts can lead to the dissemination of mis-\ninformation, potentially harming patients’ health and well-being [109]. Jin and Chandra et al [110] recently\nshowed that while GPT-4 like LLMs are largely adept at providing accurate responses to a variety of health\nqueries, for some types of queries they produce incorrect information. In fact, Zhou et al. showed that\nGPT models (a type of LLM built by Open AI) could be prompt engineered relatively easily to reproduce\nmedically incorrect information [28]. Due to the complex and sensitive nature of mental health issues, clini-\ncal decision-making demands nuanced, context-specific understanding and personalized care. LLMs, while\nManuscript under peer review.\n10\npowerful, lack the ability to grasp the intricacies of an individual’s mental state and history, especially fac-\ntors ans aspects that may not be apparent in its training data such as from EHRs [111]. Given the lack of\n“objective” medical measures of mental illness, clinicians utilize a variety of collateral information in their\ndecision-making [112], for instance, through interactions with the patients’ family members or relying on\nnon-clinical insights. LLMs are likely to miss opportunities to learn from such collateral information that\ntend to be heavily individual-specific and unique. By relying on specific types of biased training data stem-\nming from the lived experience of specific (majority) populations, LLMs might overlook the subtleties in\nlanguage related to mental health, such as expressions tied to traumatic experiences or coping mechanisms,\nwhich shape a person’s own conceptualization of their mental health [113, 114]. As Harrigian et al [115]\nnoted, when these nuances are not considered during the training of predictive models (here, LLMs), there is\na risk of these signals generating numerous false alarms in decision-making when applied to different popu-\nlations. This may be exacerbated by temporal artifacts as also noted by Harrigian et al [115]. That is, when\nthere are group-level differences in temporal alignment of the data between model training and deployment,\nit can exert an impact on predictive performance of LLMs dedicated for psychiatric decision-making.\nMoreover, existing commercial LLMs have been demonstrated to not generalize well to non-English\nhealth contexts [110], producing to poor quality, non-comprehensive, and hallucinated information [116],\nthus disadvantaging non-English speaking patients. In psychiatry, where accurate diagnosis and timely,\nculturally-sensitive treatment is paramount for success of care paradigms [10], relying solely on automated\nsystems can be particularly perilous, that can lead to poor response to treatment or prolonged duration of\nuntreated mental illness.\nSuggesting Clinically Unverified or Incorrect Treatments. The above problems can be exacer-\nbated when considering individuals with serious mental illnesses, such as schizophrenia or bipolar disorder.\nThese conditions often require highly individualized care and diagnosis, as symptoms can manifest differ-\nently from person to person. Misinformation from an LLM could lead to inappropriate treatment plans,\nexacerbating the suffering of already vulnerable individuals. LLMs learn from web and social media data,\nwhich is often considered a strength, but such data (e.g., Reddit health conversations) has also been shown\nto include scientifically unsupported, clinically unverified, sometimes dangerous treatments [117, 118]. Un-\ncritical use of such data for model training may result in such medically unverified strategies to trickle into an\nLLM’s decision-making [119]. Furthermore, researchers have shown that in supervised classification tasks,\nLLMs often fail to outperform existing fine-tuned traditional machine learning models [120]. In resource-\nscare settings where access to mental health professionals and services is limited [47], these systems may\nbe one of the few available resources for support [119], making false positives or false negatives in clinical\ndecisions make the potential for harm even more significant. False negatives in predicting adverse mental\nhealth events may leave marginalized individuals without the support they need. False positives, on the other\nhand, may exacerbate stigma, perpetuate bias, harassment and marginalization, and importantly, diminish\ntrust in care systems [121].\nEthical Issues in Automated Decision-Making. The use of LLMs for clinical decision-making in\nmental health also raises ethical and legal questions, particularly in matters of liability [122, 123]. If a\nclinical decision goes awry based on recommendations from an LLM, who bears responsibility? When\nclinicians rely on LLMs for diagnosis, treatment, or advice without proper verification, they could be held\naccountable for any resulting harm to patients. Legal questions may emerge regarding their duty of care,\nprofessional negligence, and the informed use of technology [124]. Unlike traditional human healthcare\nproviders, LLMs lack the capacity for judgment and accountability [29, 125], which makes determining\nliability in cases of misinformation or harm resulting from improper decisions challenging. Liability in\nLLM-based clinical decision-making in mental health—“a field where classifications of diseases as well\nManuscript under peer review.\n11\nas definitions of what is and what is not a disease are in a state of constant flux” [126] – may further be\ncomplicated by the often unpredictable and context-dependent nature of mental disorders. Administrative\nmodels have been demonstrated to be accurate in predicting suicidal behaviors at only 50% rate [127], with\nscholars often considering this task “unpredictable” [128] or suicide events to be “random” [129]. Even if\nLLMs promise better performance in such prediction tasks, as has been the case with other types of AI [130],\nliability issues complicate realizing their practical clinical value.\n6 For Society: Telehealth 2.0\nFinally, Insel envisions generative AI to revolutionize telehealth services [35] through what he calls “Tele-\nhealth 2.0”. LLMs in telehealth may offer a variety of opportunities and challenges.\n6.1 Potential Benefits\nEnhancing Efficiency of Care-Delivery. Perhaps the most promising opportunity pertains to the ef-\nficiency and automation provided by LLMs in the context of telehealth. One of the significant benefits of\nLLMs in mental health can be the automation of various tasks. Since LLMs can be used to transcribe and\nsummarize large volumes of text data, such as stemming from therapy sessions, it can help to reduce the\nadministrative burden on clinicians and healthcare workers [131]. This, in turn, could allow clinicians to\nfocus more on providing quality patient care. Automation may also lead to more consistent and thorough\ndocumentation of patient interactions, which can be invaluable in maintaining continuity of care [132]. In\nfact, by identifying patterns, trends, and insights in patient-clinician interactions, the chances of patient con-\ncerns being overlooked can be reduced – an issue widely recognized in traditional healthcare models, and\nparticularly among racial and gender minorities [133–136]. It can instill confidence in the patient that their\nvoices and concerns are more likely to be heard.\nEnriching Counselor/Provider Training with Cultural Competence. Next, LLMs can play a\nvaluable role in counselor/provider training in the context of telehealth. LLMs can be employed to curate\nand compile extensive resources, including textbooks, research papers, case studies, and guidelines relevant\nto counseling. These can serve as a comprehensive knowledge base for trainees [137]. LLMs can also\ngenerate realistic case scenarios, reflecting various mental health issues, patient backgrounds, and cultural\ncontexts. Trainees can engage with these scenarios to practice their counseling skills [138]. Notably, LLMs\ncan analyze and provide feedback on the trainee’s counseling sessions. As shown in Sharma et al’s work [55,\n139], by processing the dialogue and the dynamics of the conversation, AI systems can offer insights on\ncommunication effectiveness, active listening, and therapeutic rapport. This feedback can be invaluable for\ntrainees to identify areas for improvement. Moreover, such a scalable approach can ensure that a broader\ngroup of individuals is equipped to respond effectively to those in need. Furthermore, as discussed by Pendse\net al [10], one of the biggest impediments to accessing care lies in a poor alignment between a patient’s\ncultural understandings of their experience of mental health, and that of the counsellor/provider’s. LLMs\ncan assist in cultural competency training by providing trainees with information about various cultures,\ntheir values, and belief systems.\n6.2 Potential Harms\nDehumanizing Mental Health Treatment Paradigms. Automation by learning from vast datasets\nhas been touted to be a inimitable strength of LLMs [140]. However, while automation can be efficient, it\nManuscript under peer review.\n12\nmay also lead to a depersonalized healthcare experience and by rendering human labor (of healthcare work-\ners) increasingly obsolescent. Mental health care is not only about diagnosing and treating the underlying\nillnesses but also about providing emotional support, empathy, and comfort to patients [141]. These human\naspects of care in an LLM-informed telehealth model may not be fully replicated by machines [142]. More-\nover, the success of many pharmacological and therapeutic approaches to mental health treatment hinges\nupon trust and respect between patients and providers [143]. A dehumanized healthcare experience punctu-\nated by AI may erode this trust, as patients might feel that their care is being delivered by algorithms that\nlack a comprehensive understanding of their individual needs. Careless automation of telehealth paradigms\nmay also lead to disinvestment in care work and to a shift away from patients’ values and preferences –\ninterfering with the goals of a patient-centered model of care [144]. By removing humans from the care\ndelivery loop, increased LLM or generative AI based automation may dissuade individuals from pursuing\nthe healthcare profession altogether or demotivate healthcare workers from persisting in a profession where\ntheir jobs may be at risk of displacement. The obsolescence of healthcare workers could have economic and\nsocial implications, including job losses, economic dislocation, and potential labor market disruptions.\nThreats to Data Privacy. The most prominent risk perhaps centers around that the analysis of sensitive\npatient data by LLMs – an approach that can threaten privacy [125]. This is especially critical if the utility\nof LLMs is viewed to be centered around gleaning meaning from electronic health records, which contain\npatient health information (PHI) [145]. It has been noted in recent research that LLMs can accidentally di-\nvulge private information if prompted in specific ways [27]. This is because, in learning from vast data, these\nmodels learn relationships between personally identifiable information or PII (e.g., names, addresses etc.)\nand other linguistic elements. These attributes of LLMs can make them vulnerable to data breaches [146] to\nunauthorized parties or to bad actors employing them for contexts beyond the intended use.\nDemographic and Representational Biases. Language models have, for some years, been shown to\nreplicate human-like as well as systemic biases, whether around gender [147–149] or racial sterotypes [150–\n152]. In a telehealth context, if used to provide recommendations, such as to clinicians or trainees based\non historical patient-clinician interactions or therapy sessions, LLMs can inadvertently introduce biases\nbased on any underlying skewness in the training data. If not carefully monitored, this can lead to unfair or\ninaccurate representations of patient-provider interactions. For instance, if the training data skews the rep-\nresentativeness of one demographic group versus another, LLM-based suggestions in telehealth could lead\nto or exacerbate gender, racial, or ethnic inequities. This concern is not merely theoretical; commercially\naccessible LLMs have exhibited racial and gender biases in non-medical contexts, and these very models\nhave been found to propagate stereotypes related to race within the field of medicine [23]. Biases may\nalso arise from LLMs learning stigmatizing representations of language in training data [153], and prop-\nagating those through inappropriate language or portraying mental health issues in a negative light. It is\nalready known that stigmatizing language often surfaces in EHR patient notes [154]. If used for knowledge\nsummarization in a telehealth context, LLMs may thus over-pathologize common emotions or behaviors,\ncausing undue alarm. Stigma already hinders support-seeking in mental health, and this is known to pro-\nlong the duration of untreated mental illness [155]. Additional concerns of harm stem from LLMs creating\ninformational or perspective “echo chambers”, where LLMs inadvertently perpetuate pre-existing beliefs\nand biases held by providers or patients, as they exist in biased training data. For example, it is already\nknown that screening tools in psychiatry may be biased in ways that tend to over-diagnose Black patients\nwith schizophrenia [156]. LLMs may pathologize these biases by learning from such data. An over-reliance\non LLMs for summarizing EHR information or providing training materials could result in not challenging\nend users sufficiently to consider a variety of approaches to providing mental healthcare, rather than an\nLLM-recommended approach.\nManuscript under peer review.\n13\n7 Conclusion, Recommendations, and Future Directions\nLong before LLMs were put in the hands of the lay internet user through ChatGPT, Bender and Koller [157]\nnoted that it will be crucial to acknowledge the limitations of LLMs and place their strengths within a\nbroader perspective. This approach can serve to moderate exaggerated expectations, which can lead both\nthe general public and researchers astray in terms of the capabilities of these technologies. At the same time,\nthese understandings have the potential to stimulate fresh research pathways that are not solely reliant on\nthe utilization of ever bigger language models in every possible domain of societal interest. Throughout this\narticle, we discussed the many dimensions of the debate centered around the use of LLMs in digital mental\nhealth applications. We offer some reflections and specific considerations for future research.\n7.1 Reflections and Lessons Learned\nWhat is apparent from our above discussion is that, however fine-tuned and tailored LLMs may be to data\nstemming from real-world mental health contexts, LLM-powered chatbots or decision-support tools cannot\nserve as a replacement for human psychotherapists or health workers. Neither can a machine alone be a\nsurrogate to a real person during moments of distress. The significance of the human therapist may be further\nunderscored by the fact that the appeal of chatbots in therapy may vary among different age groups. For\ndigital natives, who are more accustomed to interacting with technology, the appeal of a machine therapist\nmight be greater that of previous generations; but across age groups, preferences for a human therapist is\nlikely to remain strong. Thus, what are the boundaries of the role of LLMs in digital mental health, for\nwho, and what responsibilities do developers and healthcare providers have in ensuring their ethical use? A\nrealistic “safety-first” approach might be to use them as surrogates, rather than as standalone AI therapists.\nFor such a safety-first approach, it will be essential to strike a balance between harnessing the potential\nof LLMs and ensuring that human experts remain integral to the decision-making process in psychiatry\nand mental health, particularly when dealing with the most vulnerable and resource-scarce populations. An\napproach, such as a human-in-the-loop or more preferably, an AI-in-the-loop system, may help to combine\nthe cognitive strengths of healthcare providers with the analytical capabilities of LLMs. Horvitz’s [158]\nconceptualization of “mixed initiative” systems might be particularly pertinent to mental health applications\nwhere, based on the situation, users can take the lead when they have a specific goal or intention, while also\nallowing AI to take the initiative when it can provide value or assist the user. In critical uses surrounding\nclinical decision-support or crisis response, mixed initiatives between the human and the AI (LLM) can\nenable them to work together as partners, with the system actively seeking input from the user and the user\nhaving the ability to request information, clarification, or assistance from the LLM component. To this\nend, it will be essential to establish clear protocols for verifying the underlying LLM’s recommendations\nof clinical decisions or crisis intervention strategies, ensuring human experts remain responsible for final\ndecisions and that providers maintain their ethical and legal obligations to prioritize patient safety and well-\nbeing. In psychotherapy or telehealth, human oversight will ensure that there are appropriate safeguards in\nplace that prepare for potential harms when the underlying “stochastic parrots” [29], provide inappropriate,\nincorrect, or misinformed outcomes, because of the non-deterministic nature of these technologies.\nIn addition, if LLMs are to be utilized in this high stakes domain, thoughtful investments to create or\ngather realistic training data will also be needed, that do not compromise the very mission and values that\nunderlie psychotherapeutic practices or crisis intervention. For instance, creating keystone datasets has been\nadvocated to help advance psychological research using LLMs [159]. Consent, awareness, and literacy\nregarding how specific data (e.g., EHRs, crisis helpline call logs, or psychotherapy chat transcripts) is used\nin specific LLMs, how, and by whom will be equally important considerations in ensuring governance of\nthese systems. On that note, for any type of mental health-relevant data that can be made available for LLM\ntraining, utmost care will need to be employed to prevent unauthorized access or data breaches, which could\nManuscript under peer review.\n14\nlead to severe harm, including identity theft or emotional distress for individuals in distress or in crisis.\nWithin the telehealth context, in employing LLMs to summarize patient-provider interactions or therapy\nsessions to support clinical care or to train counsellors, it will be paramount to protect PHI and PII in EHR\ndata, ensuring not only compliance with data protection regulations, but also to maintain patient trust their\npersonal information is secure and will not be misused.\nBroadly speaking, there is a pressing requirement for increased empirical research and advocacy efforts\naimed at helping mental health service users and practitioners differentiate the quality, usability, and efficacy\nof LLMs, as well as identifying the suitable use cases, scenarios, and target populations that stand to gain\nfrom their application.\n7.2 Recommendations\nBuilding on these reflections, we suggest technical, ethical, and human-centered recommendations to ensure\nthat the use of LLMs in mental health settings carefully balances effectiveness and responsibility.\nDevelopers of LLM-based mental health tools should shoulder the responsibility in keeping applications\nsafe for end users of these tools. This can be implemented through self-accountability frameworks, such\nas, by reporting to the public, or through outcomes of red teaming efforts. Additional accountability can\ncome from companies describing what types of training data was used to build the models, how they were\nevaluated following training, what performance metrics were used to assess performance, and the extent\nto which performance was tests in various contexts and scenarios. Several efforts in these lines have been\nproposed in the algorithmic fairness and accountability literature, such as Datasheets for describing the\ncapabilities and limits of datasets used for building AI models [160], Model Cards for bringing transparency\nto how complex models work [161], and disclosures of ethics practices to demonstrate how model builders\nremain accountable for the outcomes of AI [162]. Some stakeholders have argued that this can be achieved\nif the creators of domain-specific LLMs open source their models [163], others advocating for a regulatory\nsolution [164]. And yet, some have critiqued both approaches because of the risk of open LLMs being\nexploited by malicious actors or regulation resulting in concentrating power in the hands of a few that stifles\ninnovation and competition [165]. Nevertheless, what this debate underscores is a need for continuous\nmonitoring and evaluation mechanisms to ensure responsible usage and adherence to ethical guidelines\nwithin a high stakes domain like mental health.\nThere has been some policy movement around developing standards and tools to help ensure that AI\ntechnologies are safe, secure, and trustworthy; e.g., US President Joe Biden’s 2023 executive order on\nAI [166] or the Group of Seven (G7) announcement of a new code of conduct and international guiding\nprinciples on AI [167]. However, scholars have argued that a one-size-fits-all regulatory model for generative\nAI will be unsuitable for specific health applications [168], and an adaptable approach to oversight will be\nneeded that can evolve with the rapidly and ever-evolving capabilities of this technology. This adaptive\napproach will need to go hand in hand with continuous monitoring, external auditing, and benchmarked\nevaluation of these systems to ensure responsible usage and adherence to ethical guidelines.\nAutonomous bodies to enforce oversight would also need to be created in the digital mental health field\nto establish what types of standards might be suitable for the four types of uses of LLMs in mental health.\nFor instance, what standards would ensure safety if LLMs were to be integrated into the 988 system or\nwithin psychotherapy contexts? Pharmacological treatments undergo a clearance process with the FDA and\nscholars have long advocated for a need to consider similar regulatory processes for digital mental health\nas well. After many years of research, Empatica is one of the few digital mental health technologies that\nhas received clearance from the FDA for medical use [169, 170]. This conversation may be extended to\nLLM-based mental health technologies too, to ensure that a reasonable level of safety is promised in any\napplication that reaches mental health support seekers.\nManuscript under peer review.\n15\nFurthermore, it will be essential for legal, infrastructure, privacy, and security teams to review organiza-\ntional policies and procedures to guarantee adherence to state and federal laws and regulations, particularly\nin the context of personal health information exchange protocols, accountability, liability, service reimburse-\nment, and clinical workflows. Concurrently, there is a demand for the creation of educational curricula and\nmethods to instruct people with lived experience of mental illness, mental health professionals and care-\ngivers, and health system administrators in fundamentals of generative AI, its practical application, and its\nrole in enhancing how care is extended.\nIn the book The Soul of Care [171], Arthur Kleinman describes how “the work of the doctor has moved\naway from hands-on practice to high-technology diagnosis and treatment,” which has distanced doctors from\nengaging with the human experiences of their patients. Medical enterprises, digital healthcare services, and\nhealthcare institutions have initiated the integration of LLMs into their core operations [168]. In this piece,\nwe emphasize the need to attend to immediate challenges in the use of this paradigm-shifting technology,\ngiven its rapid clinical roll-out [168]. As Arthur Kleinman discusses, the importance of emotionally en-\ngaged human doctors in the practice of care can never be replaced, and future LLM-augmented medical\ntechnologies must be cognizant of the importance of human connection in care.\nAcknowledgments and Disclosures\nThis work was supported by NIMH grants R01MH117172 (PI: De Choudhury) and P50MH115838 (Co-I:\nDe Choudhury), and a grant from the American Foundation for Suicide Prevention (PI: De Choudhury).\nThis content is solely the responsibility of the authors and does not necessarily represent the official views\nof the National Institutes of Mental Health or AFSP. We thank the members of the Social Dynamics and\nWell-Being Lab at Georgia Tech for helping to shape early discussions on the topic.\nReferences\n1. OpenAI. Introducing ChatGPT. https://openai.com/blog/chatgpt (2022).\n2. Radford, A., Narasimhan, K., Salimans, T. & Sutskever, I. Improving language understand-\ning by generative pre-training (2018).\n3. Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P. &\nIrving, G. Fine-tuning language models from human preferences.arXiv preprint arXiv:1909.08593\n(2019).\n4. Moura, I., Teles, A., Viana, D., Marques, J., Coutinho, L. & Silva, F. Digital phenotyping\nof mental health using multimodal sensing of multiple situations of interest: A systematic\nliterature review. Journal of Biomedical Informatics, 104278 (2022).\n5. Chancellor, S. & De Choudhury, M. Methods in predictive techniques for mental health\nstatus on social media: a critical review. NPJ digital medicine 3, 43 (2020).\n6. De Choudhury, M., Counts, S. & Horvitz, E. Predicting postpartum changes in emotion and\nbehavior via social media in Proceedings of the SIGCHI conference on human factors in\ncomputing systems (2013), 3267–3276.\n7. De Choudhury, M., Gamon, M., Counts, S. & Horvitz, E. Predicting depression via social\nmedia in ICWSM (2013).\nManuscript under peer review.\n16\n8. De Choudhury, M., Kiciman, E., Dredze, M., Coppersmith, G. & Kumar, M. Discovering\nshifts to suicidal ideation from mental health content in social media in Proceedings of the\n2016 CHI Conference on Human Factors in Computing Systems (2016), 2098–2110.\n9. Choi, D., Sumner, S. A., Holland, K. M., Draper, J., Murphy, S., Bowen, D. A., Zwald, M.,\nWang, J., Law, R., Taylor, J.,et al. Development of a machine learning model using multiple,\nheterogeneous data sources to estimate weekly US suicide fatalities.JAMA network open 3,\ne2030932–e2030932 (2020).\n10. Pendse, S. R., Nkemelu, D., Bidwell, N. J., Jadhav, S., Pathare, S., De Choudhury, M. &\nKumar, N. From treatment to healing: envisioning a decolonial digital mental health in\nProceedings of the 2022 CHI Conference on Human Factors in Computing Systems(2022),\n1–23.\n11. Bossewitch, J., Brown, L. X., Gooding, P. M., Harris, L., Horton, J., Katterl, S., Myrick, K.,\nUbozoh, K. & Vasquez Encalada, A. Digital Futures in Mind: Reflecting on Technological\nExperiments in Mental Health & Crisis Support. Available at SSRN 4215994 (2022).\n12. Chancellor, S., Baumer, E. P. & De Choudhury, M. Who is the ”Human” in Human-Centered\nMachine Learning: The Case of Predicting Mental Health from Social Media. PACM HCI\n(2019).\n13. Thieme, A., Belgrave, D. & Doherty, G. Machine learning in mental health: A systematic\nreview of the HCI literature to support the development of effective and implementable ML\nsystems. ACM Transactions on Computer-Human Interaction (TOCHI)27, 1–53 (2020).\n14. Ernala, S. K., Rizvi, A. F., Birnbaum, M. L., Kane, J. M. & De Choudhury, M. Linguis-\ntic markers indicating therapeutic outcomes of social media disclosures of schizophrenia.\nProceedings of the ACM on Human-Computer Interaction 1, 1–27 (2017).\n15. Koutsouleris, N., Hauser, T. U., Skvortsova, V . & De Choudhury, M. From promise to prac-\ntice: towards the realisation of AI-informed mental health care. The Lancet Digital Health\n4, e829–e840 (2022).\n16. Hauser, T. U., Skvortsova, V ., De Choudhury, M. & Koutsouleris, N. The promise of a\nmodel-based psychiatry: building computational models of mental ill health. The Lancet\nDigital Health 4, e816–e828 (2022).\n17. Reardon, S. AI Chatbots Could Help Provide Therapy, but Caution Is Needed. Scientific\nAmerican. Accessed: 2023-11-03. https://www.scientificamerican.com/\narticle/ai-chatbots-could-help-provide-therapy-but-caution-\nis-needed/ (2023).\n18. Replika - Virtual AI Friend https://apps.apple.com/us/app/replika-\nvirtual-ai-friend/id1158555867 . [Internet]. App Store. [cited 2023 Mar 14].\n2023.\n19. Crasto, R., Dias, L., Miranda, D. & Kayande, D. CareBot: A Mental Health ChatBotin 2021\n2nd International Conference for Emerging Technology (INCET)[Internet]. [cited 2023 Mar\n14] (IEEE, Belagavi, India, 2021), 1–5.\n20. O’Leary, K. Human–AI collaboration boosts mental health support. Nat Med. https :\n//www.nature.com/articles/d41591-023-00022-w (2023).\nManuscript under peer review.\n17\n21. Ayers, J. W., Zhu, Z., Poliak, A., Leas, E. C., Dredze, M., Hogarth, M. & Smith, D. M.\nEvaluating Artificial Intelligence Responses to Public Health Questions. JAMA Network\nOpen 6, e2317517–e2317517 (2023).\n22. Zack, T., Lehman, E., Suzgun, M., Rodriguez, J. A., Celi, L. A., Gichoya, J., Jurafsky, D.,\nSzolovits, P., Bates, D. W., Abdulnour, R.-E. E., et al. Coding Inequity: Assessing GPT-\n4’s Potential for Perpetuating Racial and Gender Biases in Healthcare. medRxiv, 2023–07\n(2023).\n23. Omiye, J. A., Lester, J., Spichak, S., Rotemberg, V . & Daneshjou, R. Beyond the hype: large\nlanguage models propagate race-based medicine. medRxiv, 2023–07 (2023).\n24. Jargon, J. How a Chatbot Went Rogue. Wall Street Journal. https://www.wsj.com/\narticles/how-a-chatbot-went-rogue-431ff9f9 (2023).\n25. Xiang, C. Eating Disorder Helpline Fires Staff, Transitions to Chatbot After Unionization\n2023. https://www.vice.com/en/article/n7ezkm/eating-disorder-\nhelpline-fires-staff-transitions-to-chatbot-after-unionization .\n26. Xiang, C. ’He Would Still Be Here’: Man Dies by Suicide After Talking with AI Chatbot,\nWidow Says 2023. https://www.vice.com/en/article/pkadgm/man-dies-\nby-suicide-after-talking-with-ai-chatbot-widow-says .\n27. Chen, Y ., Mendes, E., Das, S., Xu, W. & Ritter, A. Can Language Models be Instructed to\nProtect Personal Information? arXiv preprint arXiv:2310.02224 (2023).\n28. Zhou, J., Zhang, Y ., Luo, Q., Parker, A. G. & De Choudhury, M. Synthetic lies: Under-\nstanding ai-generated misinformation and evaluating algorithmic and human solutions in\nProceedings of the 2023 CHI Conference on Human Factors in Computing Systems(2023),\n1–20.\n29. Bender, E. M., Gebru, T., McMillan-Major, A. & Shmitchell, S. On the dangers of stochastic\nparrots: Can language models be too big? in Proceedings of the 2021 ACM conference on\nfairness, accountability, and transparency (2021), 610–623.\n30. Glorioso, C. Fake news? ChatGPT has a knack for making up phony anonymous sources\nhttps://www.nbcnewyork.com/investigations/fake-news-chatgpt-\nhas-a-knack-for-making-up-phony-anonymous-sources/4120307/ .\nAccessed: 2023-11-03. 2023.\n31. Hsu, T. & Thompson, S. A. Disinformation researchers raise alarms about A.I. Chatbots\nThe New York Times. 2023.\n32. Klepper, D. It turns out that ChatGPT is really good at creating online propaganda: ’I think\nwhat’s clear is that in the wrong hands there’s going to be a lot of trouble’Fortune. 2023.\n33. Chiang, T. ChatGPT Is a Blurry JPEG of the Web https://www.newyorker.com/\ntech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-\nweb.\n34. Reupert, A. A socio-ecological framework for mental health and well-being 2017.\n35. Insel, T. in AI Anthology (ed Horvitz, E.) (2023). https://unlocked.microsoft.\ncom/ai-anthology/tom-insel.\nManuscript under peer review.\n18\n36. Gibson, J. J. The theory of affordances. Hilldale, USA 1, 67–82 (1977).\n37. Norman, D. A. Affordance, conventions, and design. interactions 6, 38–43 (1999).\n38. O’Leary, D. E. Massive data language models and conversational artificial intelligence:\nEmerging issues. Intelligent Systems in Accounting, Finance and Management 29, 182–\n198 (2022).\n39. Khullar, D. Can A.I. Treat Mental Illness? — newyorker.comhttps://www.newyorker.\ncom/magazine/2023/03/06/can-ai-treat-mental-illness . [Accessed\n06-11-2023].\n40. Weizenbaum, J. Computer power and human reason: From judgment to calculation. (1976).\n41. Colby, K. M., Watt, J. B. & Gilbert, J. P. A computer method of psychotherapy: Preliminary\ncommunication. The Journal of Nervous and Mental Disease 142, 148–152 (1966).\n42. Weizenbaum, J. Computers as” Therapists”. Science 198, 354–354 (1977).\n43. Vaidyam, A. N., Wisniewski, H., Halamka, J. D., Kashavan, M. S. & Torous, J. B. Chat-\nbots and conversational agents in mental health: a review of the psychiatric landscape. The\nCanadian Journal of Psychiatry 64, 456–464 (2019).\n44. Ma, Z., Mei, Y . & Su, Z. Understanding the benefits and challenges of using large lan-\nguage model-based conversational agents for mental well-being support. arXiv preprint\narXiv:2307.15810 (2023).\n45. Vall ´ee, J., Shareck, M., Kestens, Y . & Frohlich, K. L. Everyday geography and service ac-\ncessibility: the contours of disadvantage in relation to mental health.Annals of the American\nAssociation of Geographers 112, 931–947 (2022).\n46. Thomas, K. C., Ellis, A. R., Konrad, T. R., Holzer, C. E. & Morrissey, J. P. County-level\nestimates of mental health professional shortage in the United States. Psychiatric services\n60, 1323–1328 (2009).\n47. Rochford, B., Pendse, S., Kumar, N. & De Choudhury, M. Leveraging symptom search data\nto understand disparities in US mental health care: demographic analysis of search engine\ntrace data. JMIR Mental Health 10, e43253 (2023).\n48. Scrutton, A. P. in The Routledge handbook of epistemic injustice 347–355 (Routledge,\n2017).\n49. Pendse, S. R., Kumar, N. & De Choudhury, M. Marginalization and the Construction of\nMental Illness Narratives Online: Foregrounding Institutions in Technology-Mediated Care.\nProceedings of the ACM on Human-Computer Interaction 7, 1–30 (2023).\n50. Kuehn, B. M. Clinician shortage exacerbates pandemic-fueled “mental health crisis”. JAMA\n327, 2179–2181 (2022).\n51. Schleider, J. L., Mullarkey, M. C., Fox, K. R., Dobias, M. L., Shroff, A., Hart, E. A. &\nRoulston, C. A. A randomized trial of online single-session interventions for adolescent\ndepression during COVID-19. Nature Human Behaviour 6, 258–268 (2022).\n52. Schleider, J. Little Treatments, Big Effects: How to Build Meaningful Moments that Can\nTransform Your Mental Health(Robinson, 2023).\nManuscript under peer review.\n19\n53. Thirunavukarasu, A. J., Ting, D. S. J., Elangovan, K., Gutierrez, L., Tan, T. F. & Ting,\nD. S. W. Large language models in medicine. Nature medicine 29, 1930–1940 (2023).\n54. Hsu, S.-L., Shah, R. S., Senthil, P., Ashktorab, Z., Dugan, C., Geyer, W. & Yang, D. Helping\nthe Helper: Supporting Peer Counselors via AI-Empowered Practice and Feedback. arXiv\npreprint arXiv:2305.08982 (2023).\n55. Sharma, A., Lin, I. W., Miner, A. S., Atkins, D. C. & Althoff, T. Human–AI collabora-\ntion enables more empathic conversations in text-based peer-to-peer mental health support.\nNature Machine Intelligence 5, 46–57 (2023).\n56. Sharma, A., Rushton, K., Lin, I. W., Wadden, D., Lucas, K. G., Miner, A. S., Nguyen, T.\n& Althoff, T. Cognitive Reframing of Negative Thoughts through Human-Language Model\nInteraction. arXiv preprint arXiv:2305.02466 (2023).\n57. Ziems, C., Li, M., Zhang, A. & Yang, D. Inducing positive perspectives with text reframing.\narXiv preprint arXiv:2204.02952 (2022).\n58. Butler, A. C., Chapman, J. E., Forman, E. M. & Beck, A. T. The empirical status of cognitive-\nbehavioral therapy: A review of meta-analyses.Clinical psychology review26, 17–31 (2006).\n59. Palmer, A. & Schwan, D. Beneficent dehumanization: Employing artificial intelligence and\ncarebots to mitigate shame-induced barriers to medical care. Bioethics 36, 187–193 (2022).\n60. Wright, J. H. Cognitive-behavior therapy for severe mental illness: An illustrated guide\n(American Psychiatric Pub, 2009).\n61. Black, V . E. De-humanizing Care: An Ethnography of Mental Health Artificial Intelligence\nPhD thesis (University of California, Berkeley, 2023).\n62. Stanhope, V ., Barrenger, S. L., Salzer, M. S. & Marcus, S. C. Examining the relationship\nbetween choice, therapeutic alliance and outcomes in mental health services. Journal of\nPersonalized Medicine 3, 191–202 (2013).\n63. Krupnick, J. L., Sotsky, S. M., Elkin, I., Simmens, S., Moyer, J., Watkins, J. & Pilkonis,\nP. A. The role of the therapeutic alliance in psychotherapy and pharmacotherapy outcome:\nFindings in the National Institute of Mental Health Treatment of Depression Collaborative\nResearch Program. Focus 64, 532–277 (2006).\n64. Gladstein, G. A. Nonverbal communication and counseling/psychotherapy: A review. The\nCounseling Psychologist 4, 34–57 (1974).\n65. Price, B. Developing patient rapport, trust and therapeutic relationships. Nursing Standard\n31 (2017).\n66. Greenberg, L. S. & Pascual-Leone, A. Emotion in psychotherapy: A practice-friendly re-\nsearch review. Journal of clinical psychology 62, 611–630 (2006).\n67. Lambert, M. J. & Barley, D. E. Research summary on the therapeutic relationship and psy-\nchotherapy outcome. Psychotherapy: Theory, research, practice, training38, 357 (2001).\n68. Kim, J. Data Brokers and the Sale of Americans’ Mental Health Data: The Exchange of Our\nMost Sensitive Data and What It Means for Personal Privacy. Duke Sanford Cyber Policy\nProgram (2023).\nManuscript under peer review.\n20\n69. Federal Trade Commission. FTC to Ban BetterHelp from Revealing Consumers’ Data, In-\ncluding Sensitive Mental Health Information, to Facebook and Others for Targeted Adver-\ntising. Press Release, Federal Trade Commission(2023).\n70. De Freitas, J., U ˘guralp, A. K., O˘guz-U˘guralp, Z. & Puntoni, S. Chatbots and Mental Health:\nInsights into the Safety of Generative AI. Journal of Consumer Psychology (2022).\n71. Pendse, S. R., Sharma, A., Vashistha, A., De Choudhury, M. & Kumar, N. “Can I not be\nsuicidal on a Sunday?”: understanding technology-mediated pathways to mental health\nsupport in Proceedings of the 2021 CHI Conference on Human Factors in Computing Sys-\ntems (2021), 1–16.\n72. Abd-Alrazaq, A. A., Alajlani, M., Alalwan, A. A., Bewick, B. M., Gardner, P. & Househ, M.\nAn overview of the features of chatbots in mental health: A scoping review. International\nJournal of Medical Informatics 132, 103978 (2019).\n73. Andersson, G. & Titov, N. Advantages and limitations of Internet-based interventions for\ncommon mental disorders. World Psychiatry 13, 4–11 (2014).\n74. Norcross, J. C. & Wampold, B. E. What works for whom: Tailoring psychotherapy to the\nperson. Journal of clinical psychology 67, 127–132 (2011).\n75. Suran, M. How the new 988 lifeline is helping millions in mental health crisis. JAMA (2023).\n76. Matthews, S., Cantor, J. H., Holliday, S. B., Bialas, A., Eberhart, N. K., Breslau, J. &\nMcBain, R. K. National preparedness for 988—the new mental health emergency hotline in\nthe United States. Preventive medicine reports 33, 102208 (2023).\n77. Zabelski, S., Kaniuka, A. R., A. Robertson, R. & Cramer, R. J. Crisis lines: current status\nand recommendations for research and policy. Psychiatric services 74, 505–512 (2023).\n78. Skorburg, J. A. & Friesen, P. Mind the Gaps: Ethical and Epistemic Issues in the Digital\nMental Health Response to Covid-19. Hastings Center Report 51, 23 (2021).\n79. Hoffberg, A. S., Stearns-Yoder, K. A. & Brenner, L. A. The effectiveness of crisis line\nservices: a systematic review. Frontiers in public health 7, 399 (2020).\n80. Pendse, S. R., Lalani, F. M., De Choudhury, M., Sharma, A. & Kumar, N. ” Like Shock Ab-\nsorbers”: understanding the human infrastructures of technology-mediated mental health\nsupport in Proceedings of the 2020 CHI Conference on Human Factors in Computing Sys-\ntems (2020), 1–14.\n81. Althoff, T., Clark, K. & Leskovec, J. Large-scale analysis of counseling conversations: An\napplication of natural language processing to mental health.Transactions of the Association\nfor Computational Linguistics 4, 463–476 (2016).\n82. Chakrabarty, T., Hidey, C. & Muresan, S. ENTRUST: Argument reframing with language\nmodels and entailment. arXiv preprint arXiv:2103.06758 (2021).\n83. Maddela, M., Ung, M., Xu, J., Madotto, A., Foran, H. & Boureau, Y .-L. Training Models to\nGenerate, Recognize, and Reframe Unhelpful Thoughts. arXiv preprint arXiv:2307.02768\n(2023).\n84. Sharma, E. & De Choudhury, M. Mental Health Support and its Relationship to Linguistic\nAccommodation in Online Communities in CHI (2018).\nManuscript under peer review.\n21\n85. Cheng, Y . & Jiang, H. AI-Powered mental health chatbots: Examining users’ motivations,\nactive communicative action and engagement after mass-shooting disasters.Journal of Con-\ntingencies and Crisis Management 28, 339–354 (2020).\n86. Abbas, J., Wang, D., Su, Z. & Ziapour, A. The role of social media in the advent of COVID-\n19 pandemic: crisis management, mental health challenges and implications. Risk manage-\nment and healthcare policy, 1917–1932 (2021).\n87. Murphy, S. M., Irving, C. B., Adams, C. E. & Waqar, M. Crisis intervention for people with\nsevere mental illnesses. Cochrane Database of Systematic Reviews (2015).\n88. Ehsan, U., Wintersberger, P., Liao, Q. V ., Watkins, E. A., Manger, C., Daum´e III, H., Riener,\nA. & Riedl, M. O. Human-Centered Explainable AI (HCXAI): beyond opening the black-\nbox of AI in CHI conference on human factors in computing systems extended abstracts\n(2022), 1–7.\n89. Obermeyer, Z., Powers, B., V ogeli, C. & Mullainathan, S. Dissecting racial bias in an algo-\nrithm used to manage the health of populations. Science 366, 447–453 (2019).\n90. Turkington, R., Mulvenna, M. D., Bond, R. R., O’Neill, S., Potts, C., Armour, C., Ennis, E.\n& Millman, C. Why do people call crisis helplines? Identifying taxonomies of presenting\nreasons and discovering associations between these reasons. Health informatics journal 26,\n2597–2613 (2020).\n91. McCambridge, J., Witton, J. & Elbourne, D. R. Systematic review of the Hawthorne ef-\nfect: new concepts are needed to study research participation effects. Journal of clinical\nepidemiology 67, 267–277 (2014).\n92. Nissenbaum, H. Privacy as contextual integrity. Wash. L. Rev.79, 119 (2004).\n93. Hendel, J. Crisis Text Line ends data-sharing relationship with for-profit spinoff. https:\n//www.politico.com/news/2022/01/31/crisis-text-line-ends-\ndata-sharing-00004001 (2022).\n94. Li, H., Vincent, N., Chancellor, S. & Hecht, B. The Dimensions of Data Labor: A Road Map\nfor Researchers, Activists, and Policymakers to Empower Data Producers in Proceedings\nof the 2023 ACM Conference on Fairness, Accountability, and Transparency(2023), 1151–\n1161.\n95. Yoo, D. W., Woo, H., Pendse, S. R., Lu, N., Birnbaum, M. L., Abowd, G. & De Choudhury,\nM. Missed Opportunities for Human-Centered AI Research: Understanding Stakeholder\nCollaboration in Mental Health AI Research.Proceedings of the ACM on Human-Computer\nInteraction (2024).\n96. Saha, K., Sugar, B., Torous, J., Abrahao, B., Kıcıman, E. & De Choudhury, M. A social\nmedia study on the effects of psychiatric medication usein Proceedings of the International\nAAAI Conference on Web and Social Media13 (2019), 440–451.\n97. Saha, K., Torous, J., Kiciman, E., De Choudhury, M., et al. Understanding side effects of\nantidepressants: Large-scale longitudinal study on social media data.JMIR mental health 8,\ne26589 (2021).\nManuscript under peer review.\n22\n98. Ayvaz, S., Horn, J., Hassanzadeh, O., Zhu, Q., Stan, J., Tatonetti, N. P., Vilar, S., Brochhausen,\nM., Samwald, M., Rastegar-Mojarad, M., et al. Toward a complete dataset of drug–drug in-\nteraction information from publicly available sources.Journal of biomedical informatics55,\n206–217 (2015).\n99. Papoutsaki, A., So, S., Kenderova, G., Shapiro, B. & Epstein, D. A. Understanding De-\nlivery of Collectively Built Protocols in an Online Health Community for Discontinuation\nof Psychiatric Drugs. Proceedings of the ACM on Human-Computer Interaction 5, 1–29\n(2021).\n100. Lyons, J. S., Stutesman, J., Neme, J., Vessey, J. T., O’Mahoney, M. T. & Camper, H. J.\nPredicting psychiatric emergency admissions and hospital outcome.Medical care, 792–800\n(1997).\n101. Alonso, S. G., de La Torre-D ´ıez, I., Hamrioui, S., L ´opez-Coronado, M., Barreno, D. C.,\nNozaleda, L. M. & Franco, M. Data mining algorithms and techniques in mental health: a\nsystematic review. Journal of medical systems 42, 1–15 (2018).\n102. Birnbaum, M. L., Ernala, S. K., Rizvi, A., Arenare, E., R. Van Meter, A, De Choudhury, M &\nKane, J. M. Detecting relapse in youth with psychotic disorders utilizing patient-generated\nand patient-contributed digital data from facebook. NPJ schizophrenia 5, 17 (2019).\n103. Yang, X., Chen, A., PourNejatian, N., Shin, H. C., Smith, K. E., Parisien, C., Compas, C.,\nMartin, C., Costa, A. B., Flores, M. G., et al. A large language model for electronic health\nrecords. NPJ Digital Medicine 5, 194 (2022).\n104. Steinberg, E., Jung, K., Fries, J. A., Corbin, C. K., Pfohl, S. R. & Shah, N. H. Language\nmodels are an effective representation learning technique for electronic health record data.\nJournal of biomedical informatics 113, 103637 (2021).\n105. Kottlors, J., Bratke, G., Rauen, P., Kabbasch, C., Persigehl, T., Schlamann, M. & Lennartz,\nS. Feasibility of differential diagnosis based on imaging patterns using a large language\nmodel. Radiology 308, e231167 (2023).\n106. Gala, D. & Makaryus, A. N. The utility of language models in cardiology: a narrative re-\nview of the benefits and concerns of ChatGPT-4. International Journal of Environmental\nResearch and Public Health 20, 6438 (2023).\n107. Clusmann, J., Kolbinger, F. R., Muti, H. S., Carrero, Z. I., Eckardt, J.-N., Laleh, N. G.,\nL¨offler, C. M. L., Schwarzkopf, S.-C., Unger, M., Veldhuizen, G. P.,et al. The future land-\nscape of large language models in medicine. Communications Medicine 3, 141 (2023).\n108. Botelho, F., Tshimula, J. M. & Poenaru, D. Leveraging ChatGPT to Democratize and De-\ncolonize Global Surgery: Large Language Models for Small Healthcare Budgets. World\nJournal of Surgery, 1–2 (2023).\n109. Verma, G., Bhardwaj, A., Aledavood, T., De Choudhury, M. & Kumar, S. Examining the\nimpact of sharing COVID-19 misinformation online on mental health.Scientific Reports 12,\n8045 (2022).\n110. Jin, Y ., Chandra, M., Verma, G., Hu, Y ., Choudhury, M. D. & Kumar, S. Better to Ask\nin English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries\n2023. arXiv: 2310.13132 [cs.CL].\nManuscript under peer review.\n23\n111. Bhatt, S., Goyal, P., Dandapat, S., Choudhury, M. & Sitaram, S. On the universality of deep\ncontextual language models. arXiv preprint arXiv:2109.07140 (2021).\n112. Petrik, M. L., Billera, M., Kaplan, Y ., Matarazzo, B. & Wortzel, H. Balancing patient care\nand confidentiality: considerations in obtaining collateral information. Journal of Psychi-\natric Practice® 21, 220–224 (2015).\n113. Nichter, M. Idioms of distress revisited. Culture, Medicine, and Psychiatry 34, 401–416\n(2010).\n114. Kleinman, A. The illness narratives: Suffering, healing, and the human condition (Basic\nbooks, 1988).\n115. Harrigian, K., Aguirre, C. & Dredze, M. Do models of mental health based on social media\ndata generalize? in Findings of the association for computational linguistics: EMNLP 2020\n(2020), 3774–3788.\n116. Zhang, Y ., Li, Y ., Cui, L., Cai, D., Liu, L., Fu, T., Huang, X., Zhao, E., Zhang, Y ., Chen, Y .,\net al. Siren’s Song in the AI Ocean: A Survey on Hallucination in Large Language Models.\narXiv preprint arXiv:2309.01219 (2023).\n117. Chancellor, S., Nitzburg, G., Hu, A., Zampieri, F. & De Choudhury, M. Discovering Alter-\nnative Treatments for Opioid Use Recovery Using Social Media in Proc. CHI (2019).\n118. ElSherief, M., Sumner, S. A., Jones, C. M., Law, R. K., Kacha-Ochana, A., Shieber, L.,\nCordier, L., Holton, K. & De Choudhury, M. Characterizing and identifying the prevalence\nof web-based misinformation relating to medication for opioid use disorder: Machine learn-\ning approach. Journal of medical Internet research 23, e30753 (2021).\n119. Tate, S., Fouladvand, S., Chen, J. H. & Chen, C.-Y . A. The ChatGPT therapist will see you\nnow: Navigating generative artificial intelligence’s potential in addiction medicine research\nand patient care 2023.\n120. Ziems, C., Held, W., Shaikh, O., Chen, J., Zhang, Z. & Yang, D. Can Large Language\nModels Transform Computational Social Science?arXiv preprint arXiv:2305.03514(2023).\n121. Balcombe, L. AI Chatbots in Digital Mental Health in Informatics 10 (2023), 82.\n122. Yoo, D. W., Ernala, S. K., Saket, B., Weir, D., Arenare, E., Ali, A. F., Van Meter, A. R., Birn-\nbaum, M. L., Abowd, G. D. & De Choudhury, M. Clinician perspectives on using compu-\ntational mental health insights from patients’ social media activities: design and qualitative\nevaluation of a prototype. JMIR Mental Health 8, e25455 (2021).\n123. Duffourc, M. & Gerke, S. Generative AI in health care and liability risks for physicians and\nsafety concerns for patients. Jama (2023).\n124. Saenz, A. D., Harned, Z., Banerjee, O., Abr `amoff, M. D. & Rajpurkar, P. Autonomous AI\nsystems in the face of liability, regulations and costs. NPJ digital medicine 6, 185 (2023).\n125. Weidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P.-S., Cheng, M., Glaese,\nM., Balle, B., Kasirzadeh, A., et al. Ethical and social risks of harm from language models.\narXiv preprint arXiv:2112.04359 (2021).\n126. Halleck, S. L. & Halleck, S. L. The Psychiatrist’s Liability for Negligent Diagnosis. Law in\nthe Practice of Psychiatry: A Handbook for Clinicians, 65–82 (1980).\nManuscript under peer review.\n24\n127. Kessler, R. C., Bossarte, R. M., Luedtke, A., Zaslavsky, A. M. & Zubizarreta, J. R. Suicide\nprediction models: a critical review of recent research with recommendations for the way\nforward. Molecular psychiatry 25, 168–179 (2020).\n128. Large, M. M. The role of prediction in suicide prevention. Dialogues in clinical neuro-\nscience (2018).\n129. Soper, C., Malo Ocejo, P & Large, M. M. On the randomness of suicide: An evolutionary,\nclinical call to transcend suicide risk assessment.Evolutionary psychiatry: evolutionary per-\nspectives on mental health, 134–152 (2022).\n130. Coppersmith, G., Leary, R., Crutchley, P. & Fine, A. Natural language processing of social\nmedia as screening for suicide risk.Biomedical informatics insights 10, 1178222618792860\n(2018).\n131. Sezgin, E. Artificial intelligence in healthcare: Complementing, not replacing, doctors and\nhealthcare providers. Digital Health 9, 20552076231186520 (2023).\n132. Khanbhai, M., Warren, L., Symons, J., Flott, K., Harrison-White, S., Manton, D., Darzi,\nA. & Mayer, E. Using natural language processing to understand, facilitate and maintain\ncontinuity in patient experience across transitions of care. International journal of medical\ninformatics 157, 104642 (2022).\n133. Hoffman, K. M., Trawalter, S., Axt, J. R. & Oliver, M. N. Racial bias in pain assessment and\ntreatment recommendations, and false beliefs about biological differences between blacks\nand whites. Proceedings of the National Academy of Sciences 113, 4296–4301 (2016).\n134. Chen, E. H., Shofer, F. S., Dean, A. J., Hollander, J. E., Baxt, W. G., Robey, J. L., Sease,\nK. L. & Mills, A. M. Gender disparity in analgesic treatment of emergency department\npatients with acute abdominal pain. Academic Emergency Medicine 15, 414–418 (2008).\n135. Bougie, O., Yap, M. I., Sikora, L., Flaxman, T. & Singh, S. Influence of race/ethnicity\non prevalence and presentation of endometriosis: a systematic review and meta-analysis.\nBJOG: An International Journal of Obstetrics & Gynaecology 126, 1104–1115 (2019).\n136. Lee, P., Le Saux, M., Siegel, R., Goyal, M., Chen, C., Ma, Y . & Meltzer, A. C. Racial and\nethnic disparities in the management of acute pain in US emergency departments: meta-\nanalysis and systematic review. The American journal of emergency medicine 37, 1770–\n1777 (2019).\n137. Ravi, A., Neinstein, A. & Murray, S. G. Large Language Models and Medical Education:\nPreparing for a Rapid Transformation in How Trainees Will Learn to Be Doctors. ATS\nScholar, ats–scholar (2023).\n138. Safranek, C. W., Sidamon-Eristoff, A. E., Gilson, A. & Chartash, D. The role of large lan-\nguage models in medical education: applications and implications 2023.\n139. Sharma, A., Lin, I. W., Miner, A. S., Atkins, D. C. & Althoff, T. Towards facilitating em-\npathic conversations in online mental health support: A reinforcement learning approachin\nProceedings of the Web Conference 2021(2021), 194–205.\n140. Li, J., Dada, A., Kleesiek, J. & Egger, J. ChatGPT in Healthcare: A Taxonomy and System-\natic Review. medRxiv, 2023–03 (2023).\nManuscript under peer review.\n25\n141. Gateshill, G., Kucharska-Pietura, K. & Wattis, J. Attitudes towards mental disorders and\nemotional empathy in mental health and other healthcare professionals. The Psychiatrist\n35, 101–105 (2011).\n142. Montemayor, C., Halpern, J. & Fairweather, A. In principle obstacles for empathic AI: why\nwe can’t replace human empathy in healthcare. AI & society 37, 1353–1359 (2022).\n143. Brown, P., Calnan, M., Scrivener, A. & Szmukler, G. Trust in mental health services: A\nneglected concept. Journal of Mental Health 18, 449–458 (2009).\n144. Carey Prof, T. A. Beyond patient-centered care: enhancing the patient experience in men-\ntal health services through patient-perspective care. Patient Experience Journal 3, 46–49\n(2016).\n145. Wornow, M., Xu, Y ., Thapa, R., Patel, B., Steinberg, E., Fleming, S., Pfeffer, M. A., Fries, J.\n& Shah, N. H. The shaky foundations of large language models and foundation models for\nelectronic health records. npj Digital Medicine 6, 135 (2023).\n146. Kasneci, E., Seßler, K., K ¨uchemann, S., Bannert, M., Dementieva, D., Fischer, F., Gasser,\nU., Groh, G., G ¨unnemann, S., H ¨ullermeier, E., et al. ChatGPT for good? On opportunities\nand challenges of large language models for education.Learning and individual differences\n103, 102274 (2023).\n147. Bartl, M., Nissim, M. & Gatt, A. Unmasking Contextual Stereotypes: Measuring and Mit-\nigating BERT’s Gender Bias in Proceedings of the Second Workshop on Gender Bias in\nNatural Language Processing (eds Costa-juss`a, M. R., Hardmeier, C., Radford, W. & Web-\nster, K.) (Association for Computational Linguistics, Barcelona, Spain (Online), Dec. 2020),\n1–16. https://aclanthology.org/2020.gebnlp-1.1.\n148. Bhardwaj, R., Majumder, N. & Poria, S. Investigating gender bias in bert. Cognitive Com-\nputation 13, 1008–1018 (2021).\n149. Kurita, K., Vyas, N., Pareek, A., Black, A. W. & Tsvetkov, Y . Measuring Bias in Con-\ntextualized Word Representations in Proceedings of the First Workshop on Gender Bias in\nNatural Language Processing (eds Costa-juss`a, M. R., Hardmeier, C., Radford, W. & Web-\nster, K.) (Association for Computational Linguistics, Florence, Italy, Aug. 2019), 166–172.\nhttps://aclanthology.org/W19-3823.\n150. Tan, Y . C. & Celis, L. E. Assessing social and intersectional biases in contextualized word\nrepresentations. Advances in neural information processing systems 32 (2019).\n151. Caliskan, A., Bryson, J. J. & Narayanan, A. Semantics derived automatically from language\ncorpora contain human-like biases. Science 356, 183–186 (2017).\n152. Kiritchenko, S. & Mohammad, S. Examining Gender and Race Bias in Two Hundred Senti-\nment Analysis Systems in Proceedings of the Seventh Joint Conference on Lexical and Com-\nputational Semantics (eds Nissim, M., Berant, J. & Lenci, A.) (Association for Computa-\ntional Linguistics, New Orleans, Louisiana, June 2018), 43–53.https://aclanthology.\norg/S18-2005.\n153. Tate, S. Generative Artificial Intelligence Tools in Medicine Will Amplify Stigmatizing\nLanguage. Journal of Addiction Medicine, 10–1097 (2023).\nManuscript under peer review.\n26\n154. Weiner, S. G., Lo, Y .-C., Carroll, A. D., Zhou, L., Ngo, A., Hathaway, D. B., Rodriguez,\nC. P. & Wakeman, S. E. The incidence and disparities in use of stigmatizing language in\nclinical notes for patients with substance use disorder. Journal of addiction medicine, 10–\n1097 (2023).\n155. Mueser, K. T., DeTore, N. R., Kredlow, M. A., Bourgeois, M. L., Penn, D. L. & Hintz,\nK. Clinical and demographic correlates of stigma in first-episode psychosis: The impact of\nduration of untreated psychosis. Acta Psychiatrica Scandinavica 141, 157–166 (2020).\n156. Barnes, A. Race and schizophrenia diagnoses in four types of hospitals. Journal of Black\nStudies 44, 665–681 (2013).\n157. Bender, E. M. & Koller, A. Climbing towards NLU: On meaning, form, and understand-\ning in the age of data in Proceedings of the 58th annual meeting of the association for\ncomputational linguistics (2020), 5185–5198.\n158. Horvitz, E. Principles of mixed-initiative user interfacesin Proceedings of the SIGCHI con-\nference on Human Factors in Computing Systems (1999), 159–166.\n159. Demszky, D., Yang, D., Yeager, D. S., Bryan, C. J., Clapper, M., Chandhok, S., Eichstaedt,\nJ. C., Hecht, C., Jamieson, J., Johnson, M., et al. Using large language models in psychol-\nogy. Nature Reviews Psychology, 1–14 (2023).\n160. Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J. W., Wallach, H., Iii, H. D. & Craw-\nford, K. Datasheets for datasets. Communications of the ACM 64, 86–92 (2021).\n161. Mitchell, M., Wu, S., Zaldivar, A., Barnes, P., Vasserman, L., Hutchinson, B., Spitzer, E.,\nRaji, I. D. & Gebru, T. Model cards for model reporting in Proceedings of the conference\non fairness, accountability, and transparency (2019), 220–229.\n162. Ajmani, L. H., Chancellor, S., Mehta, B., Fiesler, C., Zimmer, M. & De Choudhury, M. A\nSystematic Review of Ethics Disclosures in Predictive Mental Health Research in Proceed-\nings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (2023),\n1311–1323.\n163. Guaglione, S. The case for and against open-source large language models for use in\nnewsrooms — digiday.com https://digiday.com/media/the- case- for-\nand-against-open-source-large-language-models-for-use-in-\nnewsrooms/. [Accessed 05-11-2023].\n164. Harrer, S. Council Post: From Boring And Safe To Exciting And Dangerous: Why Large\nLanguage Models Need To Be Regulated — forbes.com https://www.forbes.com/\nsites/forbestechcouncil/2023/03/22/from-boring-and-safe-to-\nexciting-and-dangerous-why-large-language-models-need-to-be-\nregulated/. [Accessed 05-11-2023].\n165. Joint Statement on AI Safety and Openness — open.mozilla.orghttps://open.mozilla.\norg/letter/. [Accessed 05-11-2023].\nManuscript under peer review.\n27\n166. Biden, J. Executive Order on the Safe, Secure, and Trustworthy Development and Use\nof Artificial Intelligence The White House. Accessed: 2023-11-03. 2023. https : / /\nwww.whitehouse.gov/briefing-room/presidential-actions/2023/\n10/30/executive-order-on-the-safe-secure-and-trustworthy-\ndevelopment-and-use-of-artificial-intelligence/ .\n167. Press corner — ec.europa.euhttps://ec.europa.eu/commission/presscorner/\ndetail/en/ip_23_5379. [Accessed 06-11-2023].\n168. Mesk ´o, B. & Topol, E. J. The imperative for regulatory oversight of large language models\n(or generative AI) in healthcare. npj Digital Medicine 6, 120 (2023).\n169. Empatica. Empatica’s Platform Receives New FDA Clearance for Cardiac Digital Biomark-\ners 2023. https://www.prnewswire.com/news- releases/empaticas-\nplatform - receives - new - fda - clearance - for - cardiac - digital -\nbiomarkers-301975974.html.\n170. Empatica. Empatica Receives New FDA Clearance for Its Health Monitoring Platform and\nAnnounces Series B Financing 2022. https://www.prnewswire.com/news-\nreleases/empatica-receives-new-fda-clearance-for-its-health-\nmonitoring-platform-and-announces-series-b-financing-301685344.\nhtml.\n171. Kleinman, A. The soul of care: the moral education of a husband and a doctor (Penguin,\n2020).\nManuscript under peer review.",
  "topic": "Mental health",
  "concepts": [
    {
      "name": "Mental health",
      "score": 0.6045069694519043
    },
    {
      "name": "Transformative learning",
      "score": 0.6005294919013977
    },
    {
      "name": "Affordance",
      "score": 0.5406626462936401
    },
    {
      "name": "Psychological intervention",
      "score": 0.5104331374168396
    },
    {
      "name": "Public relations",
      "score": 0.4868437647819519
    },
    {
      "name": "Health care",
      "score": 0.4801054298877716
    },
    {
      "name": "Digital health",
      "score": 0.4263094663619995
    },
    {
      "name": "Internet privacy",
      "score": 0.3690745532512665
    },
    {
      "name": "Psychology",
      "score": 0.3508874177932739
    },
    {
      "name": "Political science",
      "score": 0.32687655091285706
    },
    {
      "name": "Medicine",
      "score": 0.31818512082099915
    },
    {
      "name": "Nursing",
      "score": 0.23211050033569336
    },
    {
      "name": "Computer science",
      "score": 0.14418953657150269
    },
    {
      "name": "Psychiatry",
      "score": 0.12822368741035461
    },
    {
      "name": "Pedagogy",
      "score": 0.0
    },
    {
      "name": "Cognitive psychology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    }
  ]
}