{
  "title": "Transformer-Based Multi-Aspect Multi-Granularity Non-Native English Speaker Pronunciation Assessment",
  "url": "https://openalex.org/W4224928163",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2117679203",
      "name": "Gong Yuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2107939812",
      "name": "Chen, Ziyi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286857807",
      "name": "Chu, Iek Heng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102841036",
      "name": "Chang Peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2745591341",
      "name": "Glass, James",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2888030130",
    "https://openalex.org/W1988687075",
    "https://openalex.org/W6607544360",
    "https://openalex.org/W2167642311",
    "https://openalex.org/W1967624650",
    "https://openalex.org/W2559844874",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3197742413",
    "https://openalex.org/W3096544436",
    "https://openalex.org/W2139008940",
    "https://openalex.org/W6636649193",
    "https://openalex.org/W149618481",
    "https://openalex.org/W2137469438",
    "https://openalex.org/W2091856355",
    "https://openalex.org/W2152334841",
    "https://openalex.org/W1979364015",
    "https://openalex.org/W3096674206",
    "https://openalex.org/W2016114400",
    "https://openalex.org/W3196525293",
    "https://openalex.org/W2017743450",
    "https://openalex.org/W3197938691",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6785749098",
    "https://openalex.org/W2963448630",
    "https://openalex.org/W4287241492",
    "https://openalex.org/W2395107031",
    "https://openalex.org/W185021866",
    "https://openalex.org/W3101648800",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Automatic pronunciation assessment is an important technology to help self-directed language learners. While pronunciation quality has multiple aspects including accuracy, fluency, completeness, and prosody, previous efforts typically only model one aspect (e.g., accuracy) at one granularity (e.g., at the phoneme-level). In this work, we explore modeling multi-aspect pronunciation assessment at multiple granularities. Specifically, we train a Goodness Of Pronunciation feature-based Transformer (GOPT) with multi-task learning. Experiments show that GOPT achieves the best results on speechocean762 with a public automatic speech recognition (ASR) acoustic model trained on Librispeech.",
  "full_text": "TRANSFORMER-BASED MULTI-ASPECT MULTI-GRANULARITY\nNON-NATIVE ENGLISH SPEAKER PRONUNCIATION ASSESSMENT\nYuan Gong1, Ziyi Chen2, Iek-Heng Chu2, Peng Chang2, James Glass1\n1MIT CSAIL, Cambridge, MA 02139, USA 2PAII Inc., Palo Alto, CA 94306, USA\n{yuangong,glass}@mit.edu {chenziyi253,zhuyixing276,changpeng805}@pingan.com.cn\nABSTRACT\nAutomatic pronunciation assessment is an important technol-\nogy to help self-directed language learners. While pronuncia-\ntion quality has multiple aspects including accuracy, ﬂuency,\ncompleteness, and prosody, previous efforts typically only\nmodel one aspect (e.g., accuracy) at one granularity (e.g.,\nat the phoneme-level). In this work, we explore modeling\nmulti-aspect pronunciation assessment at multiple granular-\nities. Speciﬁcally, we train a Goodness Of Pronunciation\nfeature-based Transformer (GOPT) with multi-task learn-\ning. Experiments show that GOPT achieves the best results\non speechocean762 with a public automatic speech recogni-\ntion (ASR) acoustic model trained on Librispeech. Code at\nhttps://github.com/YuanGongND/gopt.\nIndex Terms— Pronunciation assessment, Transformer\n1. INTRODUCTION\nComputer assisted pronunciation training (CAPT) is an im-\nportant technology for self-directed language learning [1,\n2, 3], which facilitates non-native (L2) speakers to learn\nforeign spoken (L1) languages. Compared with conven-\ntional classes, CAPT is more economical and convenient, and\nalso allows language learners to receive immediate feedback\non their pronunciation. Due to its usefulness, CAPT has\nbeen extensively studied, with the majority of these efforts\nfocusing on scoring phoneme-level pronunciation quality\n(e.g., [4, 5, 6, 7, 8, 9, 10]). Overall pronunciation quality\nincludes many other aspects such as word- and utterance-\nlevel ﬂuency, prosody, stress, etc., which have been typically\nmodeled separately (e.g., [11, 12, 13, 14, 15, 16]). However,\nphoneme-, word-, and utterance-level scores of accuracy,\nﬂuency, prosody, and stress are potentially correlated, there-\nfore modeling them jointly instead of separately may allow\na machine learning model to learn a more comprehensive\nrepresentation and in turn improve its performance. In reality,\nit is also desirable to have a single model that can assess\nmultiple aspects of pronunciation simultaneously.\nAs a step in this direction, in this paper we propose a new\npronunciation assessment model, named GOPT, based on\nGoodness of Pronunciation (GOP) features and a Transformer\nself-attention architecture [17]. We use the open-source spee-\nchocean762 dataset [18] that contains one phoneme-level,\nthree word-level, and ﬁve utterance-level labels including\naccuracy, prosody, and ﬂuency and apply multi-aspect multi-\ngrained supervision for GOPT training. This not only en-\nables GOPT to measure multiple aspects of pronunciation\nquality, but also boosts its performance for each assessment\ntask. In addition, the Transformer architecture captures the\ncontextual information between phonemes and words of an\nutterance. As a consequence, GOPT noticeably outperforms\nprevious methods on the speechocean762 benchmark for both\nphoneme- and utterance-level assessment tasks (there is no\nprevious work reporting word-level scores). To our knowl-\nedge, this is the ﬁrst work studying multi-aspect L2 speaker\npronunciation assessment in a multi-granularity fashion.\n2. RELATED WORK\nAs mentioned, CAPT has been extensively studied with a\nlong history. One major focus of this area is automatic\nmispronunciation detection, where GOP [4] and its variants\n(e.g., [10, 5, 7, 8]) are dominant methods. To capture the\ncorrelation between phonemes and words of an utterance,\nself-attention based models such as Transformer [17] have\nbeen added on top of GOP features for score modeling to\nimprove performance [9, 19]. There are also some non-GOP\nbased methods such as a wav2vec2-based method [20] and\na deep feature based method [21] where transfer learning is\nusually needed due to the limited L2 training material.\nConversely, automatic assessment of other aspects of pro-\nnunciation quality are usually modeled independently, e.g.,\nﬂuency [11, 12], prosody [13, 14], intonation [15, 16]. There\nare only a few previous efforts on multi-granularity pronun-\nciation assessment [19, 22]. In these works, however, only a\nsingle score is considered for each granularity. In addition,\nthe hierarchical architecture in [19] requires a relatively so-\nphisticated training scheme to optimize.\nTo the best of our knowledge, this paper is the ﬁrst to si-\nmultaneously consider multiple pronunciation quality aspects\n(accuracy, ﬂuency, prosody, etc) along with multiple granu-\nlarities (phoneme, word, utterance). In addition, we show that\na BERT-style [23] non-hierarchical standard Transformer ar-\nchitecture can perform well on most assessment tasks. Unlike\nmany previous efforts using non-public datasets or acoustic\nmodels, in this work, we intentionally use a public acoustic\narXiv:2205.03432v1  [cs.SD]  6 May 2022\nTransformer \nEncoder\nAcoustic \nModel\nUtterance \nAccuracy \nHead\nUtterance \nFluency \nHead\nUtterance \nComplete \nHead\nIH \nT \nS \n| \nN \nEY \nM\n(Its \nName)\nUtterance \nProsodic \nHead\nUtterance \nTotal   \nHead\nPos[0]\nCanonical \nTranscription\nPos[1]\nPos[2]\nPos[3]\nPos[4]\nPos[5]\nPos[6]\nPos[7]\nPos[8]\nPos[9]\nPos[10]\nCLS \n[utt-acc]\nCLS \n[utt-flu]\nCLS \n[utt-comp]\nCLS \n[utt-pros]\nCLS \n[utt-total]\nPhn[IH]\nPhn[T]\nPhn[S]\nPhn[N]\nPhn[EY]\nPhn[M]\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\nAudio\n+\n+\n+\n+\n+\n+\nGoP[IH]\nGoP[T]\nGoP[S]\nGoP[N]\nGoP[EY]\nGoP[M]\nPhone \nProjection \nLayer\nUtterance \nScores\nAccuracy \nScore\nFluency \nScore\nCompleteness \nScore\nProsodic \nScore\nTotal \nScore\nPhn \nHead\nWord \nHead\nPhn \nHead\nWord \nHead\nPhn \nHead\nWord \nHead\nPhn \nHead\nWord \nHead\nPhn \nHead\nWord \nHead\nPhn \nHead\nWord \nHead\nPhone \n[IH]\nScore\nPhone  \n[T]\nScore\nPhone  \n[S]\nScore\nPhone  \n[N]\nScore\nPhone  \n[EY]\nScore\nPhone  \n[M]\nScore\n Word [Its] Accuracy/Stress/Total Score   \nClassification \nTokens\nPositional \nEmbedding\nCanonical \nPhoneme \nEmbedding\nGoP \nFeatures\nInput \nPhoneme \nScores\n Word Scores Word [Name] Accuracy/Stress/Total Score   \nGoP \nProjection \nLayer\n(84-dim)\n(24-dim)\nOne-hot \nEncoding\nFig. 1. Illustration of the proposed GOPT architecture with a sample utterance “Its Name”, actual utterances used are longer.\nmodel and dataset for our main experiments (which achieves\nstate-of-the-art results) for easy reproduction and comparison.\n3. GOODNESS OF PRONUNCIATION\nTRANSFORMER\n3.1. Speechocean762 Dataset\nSpeechocean762 [18] is a free open-source dataset designed\nfor pronunciation assessment, consisting of a total of 5,000\nEnglish utterances collected from 250 non-native speakers.\nOne major advantage of speechocean762 is that it provides\nrich label information. Speciﬁcally, for each utterance, it pro-\nvides ﬁve utterance-level aspect scores: accuracy, ﬂuency,\ncompleteness, prosody, and total score (ranging from 0-10).\nFor each word, it provides three word-level aspect scores: ac-\ncuracy, stress, and total score (ranging from 0-10). It also pro-\nvides an accuracy score for each phoneme (ranging from 0-2).\nEach score is annotated by ﬁve experts. Thus, it provides a\ntotal of 8 labels for different granularities and pronunciation\nquality aspects. However, the rich annotation has not been\nfully utilized by previous work. We re-scale utterance and\nword-level scores to 0-2, making them on the same scale as\nthe phoneme scores. The training set consists of 2,500 utter-\nances, 15,849 words, and 47076 phones; the test set consists\nof 2,500 utterances, 15,967 words, and 47,369 phones.\n3.2. GOPT Architecture Overview\nAn overview of the GOPT architecture is shown in Figure 1.\nFor the pronunciation assessment task, the canonical tran-\nscription is known. We ﬁrst input the audio and correspond-\ning canonical transcription to the acoustic module to get a se-\nquence of frame-level phonetic posterior-probabilities, which\nare then force-aligned at the phoneme-level and converted\nto 84-dimensional goodness of pronunciation (GOP) features\n(discussed in Section 3.3). The GOP feature is then projected\nto 24-dimensions with a dense layer. In parallel, we gener-\nate a sequence of canonical phoneme embeddings (also at the\nphoneme-level) by ﬁrst converting each canonical phoneme\nto a one-hot encoding and then projecting it to the same 24-\ndimensions as the projected GOP feature. The reason for\nusing a canonical phoneme embedding is because different\nphonemes have different characteristics and thus the canoni-\ncal phoneme provides useful information to the Transformer\nmodel [21]. We then add the projected GOP feature, canoni-\ncal phoneme embedding, and a 24-dimensional trainable po-\nsitional embedding together and input it to the Transformer\nencoder. For simplicity, we intentionally follow the original\nTransformer encoder architecture [17] as close as possible but\nscale it down to 3 layers and an embedding dimension of 24.\nUnlike previous work [19, 21] that use a hierarchical ar-\nchitecture to get utterance level representations, we prepend a\nset of ﬁve trainable [cls] tokens to the phoneme-level input\nsequence in a similar way as BERT [23], each corresponding\nto one utterance aspect label, and use the output of the Trans-\nformer encoder of these [cls] aspect tokens as the corre-\nsponding utterance-level representations. The reason why this\nregime works is that the Transformer can learn the correlation\nbetween the utterance-level tokens and phoneme-level tokens\nthrough the attention mechanism.\nDuring training we apply multi-task learning to the\nmodel. Speciﬁcally, we use one regression head for each\nphoneme, word, and utterance label (eight in total). Each\nregression head is a 24 ×1 dense layer with layer normal-\nization. Utterance-level regression heads are added on top\nof the output of the Transformer of the corresponding utter-\nance [cls] tokens. Phoneme- and word-level regression\nheads are added on top of the Transformer output of each\ncorresponding phoneme. We propagate the word score to\neach of its phonemes during training and average the out-\nput of phonemes that belong to the word in inference. We\nuse mean squared error (MSE) loss for each assessment\ntask. Since we normalize the scores to the same scale,\nfor simplicity, we ﬁrst average the losses of each granu-\nlarity and then sum them up with the same weight, i.e.,\nL = Lutterance + Lword + Lphoneme, where Lutterance\nPhoneme Score Word Score (PCC) Utterance Score (PCC)Model MSE ↓ PCC ↑ Accuracy ↑ Stress ↑ Total ↑ Accuracy ↑ Completeness ↑ Fluency ↑ Prosodic ↑ Total ↑\nRF [18] 0.130 0.440 - - - - - - - -\nSVR [18] 0.160 0.450 - - - - - - -\nLin et.al [21] - - - - - - - - - 0.720\nLSTM 0.089\n±0.000\n0.591\n±0.003\n0.514\n±0.003\n0.294\n±0.012\n0.531\n±0.004\n0.720\n±0.002\n0.076\n±0.086\n0.745\n±0.002\n0.747\n±0.005\n0.741\n±0.002\nGOPT\n(Librispeech)\n0.085\n±0.001\n0.612\n±0.003\n0.533\n±0.004\n0.291\n±0.030\n0.549\n±0.002\n0.714\n±0.004\n0.155\n±0.039\n0.753\n±0.008\n0.760\n±0.006\n0.742\n±0.005\nGOPT\n(PAII-A)\n0.069\n±0.000\n0.679\n±0.001\n0.588\n±0.004\n0.146\n±0.004\n0.601\n±0.003\n0.727\n±0.004\n0.011\n±0.069\n0.692\n±0.015\n0.694\n±0.009\n0.732\n±0.006\nTable 1. Comparing the performance of various pronunciation assessment tasks between GOPT and baseline models. GOPT\n(PAII-A) depends on a different acoustic model so its results (shown in grey) cannot be directly compared with other models.\nand Lword are averaged utterance and word level losses of\nﬁve utterance-level labels and three word-level labels, respec-\ntively; Lphoneme is the phoneme loss. The entire network\n(except the acoustic model) is trained end-to-end.\n3.3. Acoustic Model and GOP Feature\nFor our main experiment we use a public ASR acoustic\nmodel1 trained with Librispeech [24] 960-hour data. The\nmodel is based on the factorized time-delay neural network\n(TDNN-F) and trained with the Kaldi Librispeech S5 recipe.\nAcoustic model trained on both L1 and L2 speech gen-\nerates better alignment for L2 speech and may output better\nGOP features [25]. To explore if GOPT works with different\nacoustic models, we also test with two PAII internal acous-\ntic models PAII-A and PAII-B, both are also TDNN-F mod-\nels. PAII-A is trained with 452 hours L1 TED-LIUM 3 [26]\ndata and 1,696 hours of L2 data collected from 5,994 non-\nnative speakers; PAII-B is trained with 995 hours of L1 data\n(from WSJ [27], TED-LIUM 3 [26], and Librispeech [24])\nand 6,591 hours of L2 data from 672k non-native speakers.\nIn this work, we use the log phone posterior (LPP) and log\nposterior ratio (LPR) deﬁned in [8] as GOP features. Speciﬁ-\ncally, the LPP of a phone p is deﬁned as follows:\nLPP (p) ≈ 1\nte −ts + 1\nte∑\nt=ts\nlog p(p|ot) (1)\np(p|ot) =\n∑\ns∈p\np(s|ot) (2)\nwhere ts and te are the start and end frame indexes; ot is the\ninput observation of the framet, s is the state belonging to the\nphone p. LPR of a phone pj versus pi is deﬁned as:\nLPR(pj|pi) = logp(pj|o; ts, te) −log p(pi|o; ts, te) (3)\nThe Librispeech acoustic model we use has a total of 42 pure\nphones, thus the GOP feature of phone p can be deﬁned as a\n84-dimensional vector as follows:\n[LPP (p1)..., LPP(p42), LPR(p1|p)..., LPR(p42|p)] (4)\n1https://kaldi-asr.org/models/m13\n4. EXPERIMENTS\nFor all experiments, we train the model with an Adam opti-\nmizer, an initial learning rate of 1e-3, a batch size of 25, and\nMSE loss for 100 epochs using the ofﬁcial speechocean762\ntraining set, and evaluate on the ofﬁcial test set. The learn-\ning rate is cut in half every ﬁve epochs after the 20th epoch,\nand the result of the last epoch is reported. We repeat each\nexperiment ﬁve times with different random seeds and re-\nport the mean and standard deviation of the results. Since\nthe speechocean762 labels are imbalanced (biased towards\nhigh scores), we use the Pearson correlation coefﬁcient (PCC)\nas the main evaluation metric but also report MSE of the\nphoneme accuracy score to make a comparison with previ-\nous work. Note that while we re-scale the utterance and word\nlevel scores, PCCs and phoneme-level MSE are not impacted.\n4.1. Main Results\nWe compare the following six models: 1) Random forest\nregression (RF) model implemented in the code repository\nof [18]; 2) Support vector regressor (SVR) based model\nin [18]; 3) Deep feature and transfer learning-based model\npresented in [21]; 4) An LSTM based model implemented by\nus. To make a fair comparison, the LSTM model has the same\ndepth and embedding dimension as the GOPT model and is\ntrained with the same setting. The output of the last token\nis used as the utterance representation and, as with GOPT,\nthe word score is propagated to its phones; 5) The proposed\nGOPT model with the Librispeech acoustic model. 6) The\nproposed GOPT model with the PAII-A acoustic model. It is\nworth mentioning that models 1-5 are all based on acoustic\nmodels trained with the same Librispeech data, and models\n1,2,4,5, and 6 use the same GOP features (model 3 does not\nuse GOP features but deep transfer learning). Therefore, we\nmake a fair comparison and the performance difference is not\ndue to the acoustic model and GOP features.\nWe show the results in Table 1. The key ﬁndings are as\nfollows: First, the proposed GOPT model can perform well\non most assessment tasks except word stress score and sen-\ntence completeness score assessment, demonstrating that it\nSetting Phoneme Word Utterance\nTraining Task\nOnly Phoneme 0.605 ±0.002 - -\nOnly Word - 0.536 ±0.004 -\nOnly Utterance - - 0.736 ±0.011\nJoint* 0.612±0.003 0.549 ±0.002 0.742 ±0.005\nCanonical Phoneme Embedding\nw/o Phn Embed 0.512=0.006 0.472 ±0.006 0.719=0.002\nw/ Phn Embed* 0.612±0.003 0.549 ±0.002 0.742 ±0.005\n# Transformer Layer (ASR params not included in#params)\n3* (27K Params) 0.612±0.003 0.549 ±0.002 0.742 ±0.005\n6 (48K Params) 0.605 ±0.003 0.543 ±0.004 0.731 ±0.003\nEmbedding Dimension (ASR params not included in#params)\n12 (8K Params) 0.608 ±0.003 0.544 ±0.008 0.741 ±0.011\n24* (27K Params) 0.612±0.003 0.549 ±0.002 0.742 ±0.005\n48 (94K Params) 0.605 ±0.003 0.545 ±0.006 0.738 ±0.004\n96 (355K Params) 0.586 ±0.006 0.530 ±0.006 0.725 ±0.004\nTable 2. The ablation results, we only show the PCC of\nphoneme, word, and utterance total scores due to space limi-\ntation. * denotes the setting used in the base GOPT model.\nis possible to have a single model for multi-aspect and multi-\ngranularity pronunciation assessment. Speciﬁcally, the GOPT\nachieves 0.085 MSE and 0.612 PCC for the phoneme accu-\nracy score assessment, noticeably outperforming the models\nin [18]; GOPT achieves 0.742 PCC for the utterance-level\nscore assessment, noticeably outperforming the model in [21]\nwhich uses more sophisticated features than GOP. We hypoth-\nesize that the poor utterance completeness assessment per-\nformance is due to the highly imbalanced distribution of the\ncompleteness score in the training data. Second, the multi-\ntask learning scheme can be also applied to an LSTM, which\nachieves similar results for utterance assessment with GOPT.\nHowever, the performance of the LSTM for phoneme-level\nand word-level assessment are worse than the GOPT, demon-\nstrating that the Transformer architecture is better at model-\ning ﬁne-grained pronunciation units. Third, using the PAII-A\nacoustic model trained on both L1 and L2 speech can fur-\nther boost the phoneme and word assessment performance\nby around 10%, but the utterance-level performance is worse\nthan just using the Librispeech acoustic model. We also eval-\nuate GOPT with PAII-B acoustic model, it leads to similar\nresults with GOPT with PAII-A acoustic model.\n4.2. Ablations\nWe conduct a set of ablation studies to show the performance\nimpact of various factors. We set the GOPT model mentioned\nin Section 3 with three Transformer layers, embedding dimen-\nsion of 24, canonical phoneme embedding, and trained with\nScoring\nModel\nAcoustic Model\nLibrispeech PAII-A PAII-B\nMSE ↓ PCC ↑ MSE ↓ PCC ↑ MSE ↓ PCC ↑\nSVR 0.160 0.450 0.118 0.538 0.115 0.561\nGOPT 0.085\n±0.001\n0.612\n±0.003\n0.069\n±0.000\n0.679\n±0.001\n0.071\n±0.001\n0.662\n±0.001\nTable 3. Comparing the phoneme assessment performance\nbetween the SVR based [18] model and proposed GOPT\nmodel with various acoustic models.\nall phoneme, word, and utterance assessment tasks as the base\nGOPT model, and then change one factor at a time to observe\nthe performance change.\nWe show the results in Table 2. First, we see that the\nGOPT trained with multi-task learning achieves better re-\nsults than any single-task learning model, demonstrating that\nmulti-task learning not only allows the model to conduct\nmulti-aspect and multi-granularity pronunciation assessment\nsimultaneously, but also improves the performance of each\nindividual task. Second, we see that the canonical phoneme\nembedding is crucial to the performance as the model trained\nwithout it performs much worse for all tasks. However, it is\nworth mentioning that canonical phoneme embedding is not\nthe reason why GOPT outperforms previous methods since\ncanonical phoneme embedding is also used in [21]. In [18],\neach phoneme has a separate classiﬁer, which serves a sim-\nilar function as a canonical phoneme embedding. Third, we\nexplore the performance impact of the size of GOPT model,\nand see that increasing either the width or depth of the net-\nwork cannot further improve the performance, indicating that\na small model is preferred with the relatively small dataset.\nFurther, although the GOP feature is 84-dimensional, we\nshow that an embedding size of 24 is sufﬁcient to represent\npronunciation quality with a Transformer.\nFinally, in Table 3, we compare the phoneme assessment\nperformance between the SVR [18] model and the proposed\nGOPT model with various acoustic models. We show that\nthe proposed GOPT consistently leads to a signiﬁcant perfor-\nmance improvement regardless of the acoustic model, demon-\nstrating that the GOPT is model agnostic and can be used with\ndifferent acoustic models.\n5. CONCLUSION\nIn this paper, we present the Transformer-based multi-aspect\nmulti-granularity pronunciation assessment model GOPT. We\nshow that with the multi-task learning scheme, a single GOPT\nmodel can conduct multiple pronunciation tasks simultane-\nously, and its performance is better than the same model\ntrained with a single task. Experiments show the GOPT can\nnoticeably outperform previous methods on speechocean762.\n6. REFERENCES\n[1] Maxine Eskenazi, “An overview of spoken language\ntechnology for education,” Speech Communication,\n2009.\n[2] Klaus Zechner, Derrick Higgins, et al., “Automatic scor-\ning of non-native spontaneous speech in tests of spoken\nenglish,” Speech Communication, 2009.\n[3] Silke M Witt, “Automatic error detection in pronuncia-\ntion training: Where we are and where we need to go,”\nin International Symposium on Automatic Detection on\nErrors in Pronunciation Training, 2012.\n[4] S.M Witt and S.J Young, “Phone-level pronunciation\nscoring and assessment for interactive language learn-\ning,” Speech Communication, 2000.\n[5] Feng Zhang, Chao Huang, et al., “Automatic mispro-\nnunciation detection for mandarin,” in ICASSP, 2008.\n[6] Dean Luo, Yu Qiao, et al., “Analysis and utilization of\nmllr speaker adaptation technique for learners’ pronun-\nciation evaluation,” in Interspeech, 2009.\n[7] Yow-Bang Wang and Lin-Shan Lee, “Improved ap-\nproaches of modeling and detecting error patterns with\nempirical analysis for computer-aided pronunciation\ntraining,” in ICASSP, 2012.\n[8] Wenping Hu, Yao Qian, et al., “Improved mispronuncia-\ntion detection with deep neural network trained acoustic\nmodels and transfer learning based logistic regression\nclassiﬁers,” Speech Communication, 2015.\n[9] Jiatong Shi, Nan Huo, and Qin Jin, “Context-aware\ngoodness of pronunciation for computer-assisted pro-\nnunciation training,” in Interspeech, 2020.\n[10] Joost van Doremalen, C Cucchiarini, and H Strik, “Us-\ning non-native error patterns to improve pronunciation\nveriﬁcation,” in Interspeech, 2010.\n[11] Catia Cucchiarini, Helmer Strik, and LWJ Boves,\n“Quantitative assessment of second language learners’\nﬂuency: an automatic approach,” in ICSLP, 1998.\n[12] Catia Cucchiarini, Helmer Strik, and Lou Boves,\n“Quantitative assessment of second language learners’\nﬂuency by means of automatic speech recognition tech-\nnology,” The Journal of the Acoustical Society of Amer-\nica, 2000.\n[13] Paul Christopher Bagshaw, Automatic prosodic analy-\nsis for computer aided pronunciation teaching, Ph.D.\nthesis, 1994.\n[14] Joseph Tepperman and Shrikanth Narayanan, “Auto-\nmatic syllable stress detection using prosodic features\nfor pronunciation evaluation of language learners,” in\nICASSP, 2005.\n[15] Juan Pablo Arias, Nestor Becerra Yoma, and Hiram\nVivanco, “Automatic intonation assessment for com-\nputer aided language learning,”Speech Communication,\n2010.\n[16] Kun Li, Xixin Wu, and Helen Meng, “Intonation clas-\nsiﬁcation for l2 english speech using multi-distribution\ndeep neural networks,” Computer Speech and Lan-\nguage, 2017.\n[17] Ashish Vaswani, Noam Shazeer, et al., “Attention is all\nyou need,” in Advances in Neural Information Process-\ning Systems, 2017.\n[18] Junbo Zhang, Zhiwen Zhang, et al., “speechocean762:\nAn open-source non-native english speech corpus for\npronunciation assessment,” in Interspeech, 2021.\n[19] Binghuai Lin, Liyuan Wang, Xiaoli Feng, and Jinsong\nZhang, “Automatic scoring at multi-granularity for l2\npronunciation.,” in Interspeech, 2020.\n[20] Xiaoshuo Xu, Yueteng Kang, Songjun Cao, Binghuai\nLin, et al., “Explore wav2vec 2.0 for mispronunciation\ndetection,” Interspeech, 2021.\n[21] Binghuai Lin and Liyuan Wang, “Deep feature transfer\nlearning for automatic pronunciation assessment,” In-\nterspeech, 2021.\n[22] Tobias Cincarek, Rainer Gruhn, et al., “Automatic pro-\nnunciation scoring of words and sentences independent\nfrom the non-native’s ﬁrst language,”Computer Speech\nand Language, 2009.\n[23] Jacob Devlin, Ming-Wei Chang, et al., “Bert: Pre-\ntraining of deep bidirectional transformers for language\nunderstanding,” ACL, 2018.\n[24] Vassil Panayotov, Guoguo Chen, et al., “Librispeech:\nan asr corpus based on public domain audio books,” in\nICASSP, 2015.\n[25] Ming Tu, Anna Grabek, et al., “Investigating the role of\nl1 in automatic pronunciation evaluation of l2 speech,”\nin Interspeech, 2018.\n[26] Franc ¸ois Hernandez, Vincent Nguyen, et al., “Ted-lium\n3: twice as much data and corpus repartition for experi-\nments on speaker adaptation,” in SPECOM, 2018.\n[27] Mitchell P Marcus, Mary Ann Marcinkiewicz, et al.,\n“Building a large annotated corpus of english: the penn\ntreebank,” Computational Linguistics, 1993.",
  "topic": "Pronunciation",
  "concepts": [
    {
      "name": "Pronunciation",
      "score": 0.8810969591140747
    },
    {
      "name": "Computer science",
      "score": 0.8077623844146729
    },
    {
      "name": "Transformer",
      "score": 0.7022995948791504
    },
    {
      "name": "Fluency",
      "score": 0.646789014339447
    },
    {
      "name": "Prosody",
      "score": 0.6256771087646484
    },
    {
      "name": "Granularity",
      "score": 0.6159944534301758
    },
    {
      "name": "Speech recognition",
      "score": 0.5948981046676636
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5168402791023254
    },
    {
      "name": "Natural language processing",
      "score": 0.4744371473789215
    },
    {
      "name": "Linguistics",
      "score": 0.1531931459903717
    },
    {
      "name": "Engineering",
      "score": 0.07817766070365906
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": []
}