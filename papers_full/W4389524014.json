{
  "title": "Using Captum to Explain Generative Language Models",
  "url": "https://openalex.org/W4389524014",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2554856477",
      "name": "Vivek Miglani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2910525744",
      "name": "Aobo Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2465656519",
      "name": "Aram Markosyan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4295638335",
      "name": "Diego Garcia-Olano",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1989131412",
      "name": "Narine Kokhlikyan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3176390156",
    "https://openalex.org/W2605409611",
    "https://openalex.org/W2073231946",
    "https://openalex.org/W2953073956",
    "https://openalex.org/W2962862931",
    "https://openalex.org/W3085380432",
    "https://openalex.org/W3105604018",
    "https://openalex.org/W1787224781",
    "https://openalex.org/W4321649851",
    "https://openalex.org/W3174057701",
    "https://openalex.org/W2962851944",
    "https://openalex.org/W3175982906",
    "https://openalex.org/W3174150157",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W4385573981",
    "https://openalex.org/W4385570354",
    "https://openalex.org/W4226119086",
    "https://openalex.org/W4362720788"
  ],
  "abstract": "Captum is a comprehensive library for model explainability in PyTorch, offering a range of methods from the interpretability literature to enhance users’ understanding of PyTorch models. In this paper, we introduce new features in Captum that are specifically designed to analyze the behavior of generative language models. We provide an overview of the available functionalities and example applications of their potential for understanding learned associations within generative language models.",
  "full_text": "Proceedings of the 3rd Workshop for Natural Language Processing Open Source Software (NLP-OSS 2023), pages 165–173\nDecember 6, 2023 ©2023 Association for Computational Linguistics\nUsing Captum to Explain Generative Language Models\nVivek Miglani*, Aobo Yang*, Aram H. Markosyan, Diego Garcia-Olano, Narine Kokhlikyan\nMeta AI\n{vivekm, aoboyang, amarkos, diegoolano, narine}@meta.com\nAbstract\nCaptum is a comprehensive library for model\nexplainability in PyTorch, offering a range of\nmethods from the interpretability literature to\nenhance users’ understanding of PyTorch mod-\nels. In this paper, we introduce new features\nin Captum1 that are specifically designed to\nanalyze the behavior of generative language\nmodels. We provide an overview of the avail-\nable functionalities and example applications\nof their potential for understanding learned as-\nsociations within generative language models.\n1 Introduction\nModel interpretability and explainability have be-\ncome significantly more important as machine\nlearning models are used in critical domains such\nas healthcare and law. It is insufficient to simply\nmake a prediction through a black-box model and\nimportant to better understand why the model made\na particular decision.\nInterest in Large Language Models (LLMs) has\nalso grown exponentially in the past few years with\nthe release of increasingly large and more powerful\nmodels such as GPT-4 (OpenAI, 2023). A lack of\nexplainability continues to exist despite larger mod-\nels, and with the use of these models expanding\nto more and more use-cases, it is increasingly im-\nportant to have access to tooling providing model\nexplanations.\nCaptum is an open-source model explainability\nlibrary for PyTorch providing a variety of generic\ninterpretability methods proposed in recent litera-\nture such as Integrated Gradients, LIME, DeepLift,\nTracIn, TCA V and more (Kokhlikyan et al., 2020).\nIn this work, we discuss newly open-sourced\nfunctionalities in Captum v0.7 to easily apply ex-\nplainability methods to large generative language\nmodels, such as GPT-3.\n*Denotes equal contribution\n1https://captum.ai\n2 Attribution Methods\nOne important class of explainability methods is\nattribution or feature importance methods, which\noutput a score corresponding to each input feature’s\ncontribution or impact to a model’s final output.\nFormally, given a function f : Rd →R, where\nf ∈F and X ∈Rd is a single input vector con-\nsisting of ddimensions or features, an attribution\nmethod is defined as a function ϕ: F ×Rd →Rd.\nEach element in the attribution output corresponds\nto a score of the contribution of corresponding fea-\nture i∈D, where Ddenotes the set of all feature\nindices D= {1,2,...,d }.\nMany attribution methods also require a baseline\nor reference input B ∈Rd defining a comparison\ninput point to measure feature importance with\nrespect to.\nWe utilize the notation XS to denote selecting\nthe feature values with indices from the set S ⊆D\nand the remaining indices from B. Formally, the\nvalue of feature i in XS is (XS)i = Ii∈SXi +\nIi/∈SBi, where I is the indicator function.\nIn this section, we provide background con-\ntext on attribution methods available in Captum.\nThese methods can be categorized broadly into\n(i) perturbation-based methods, which utilize re-\npeated evaluations of a black-box model on per-\nturbed inputs to estimate attribution scores, and\n(ii) gradient-based methods, which utilize back-\npropagated gradient information to estimate attri-\nbution scores. Perturbation-based methods do not\nrequire access to model weights, while gradient-\nbased models do.\n2.1 Perturbation-Based Methods\n2.1.1 Feature Ablation\nThe most straightforward attribution is feature ab-\nlation, where each feature is substituted with the\ncorresponding element of the baseline feature vec-\ntor to estimate the corresponding importance.\n165\nFormally, this method is defined as\nϕi(f,X) =f(X) −f(XD\\{i}) (1)\nFeature Ablation has clear advantages as a sim-\nple and straightforward method, but the resulting\nattributions may not fully capture the impacts of\nfeature interactions since features are ablated indi-\nvidually.\n2.1.2 Shapley Value Sampling\nShapley Values originated from cooperative game\ntheory as an approach to distribute payouts fairly\namong players in a cooperative game. Analogously,\nin the attribution setting, Shapley Values assign\nscores to input features, with payouts correspond-\ning to a feature’s contribution to the model output.\nShapley Values satisfy a variety of theoretical prop-\nerties including efficiency, symmetry and linearity.\nFormally, this method is defined as\nϕi(f,X) =\n∑\nS⊆D\\{i}\n[|S|!(|D|−|S|−1)!\n|D|!\nf(XS∪{i}) −f(XS)\n]\n(2)\nWhile computing this quantity exactly requires\nan exponential number of evaluations in the num-\nber of features, we can estimate this quantity using\na sampling approach (Castro et al., 2009). The\napproach involves selecting a permutation of the\ndfeatures and adding the features one-by-one to\nthe original baseline. The output change as a result\nof adding each feature accounts for its contribu-\ntion, and averaging this over sampled perturbations\nresults in an unbiased estimate of Shapley Values.\n2.1.3 LIME\nLIME or Locally Interpretable Model Explanations\nproposes a generic approach to sample points in\nthe neighborhood of the input point X and train an\ninterpretable model (such as a linear model) based\non the results of the local evaluations (Ribeiro et al.,\n2016).\nThis method proposes reparametrizing the in-\nput space to construct interpretable features such\nas super-pixels in images and then evaluating the\noriginal model on a variety of perturbations of the\ninterpretable features. The method can be utilized\nwith any perturbation sampling and weighting ap-\nproaches and interpretable model / regularization\nparameters. The interpretable model can then be\nused as an explanation of the model’s behavior in\nthe local region surrounding the target input point.\nFor a linear model, the coefficients of this model\ncan be considered as attribution scores for the cor-\nresponding feature.\n2.1.4 Kernel SHAP\nKernel SHAP is a special case of the LIME\nframework, which sets the sampling approach, in-\ntepretable model, and regularization in a specific\nway such that the results theoretically approximate\nShapley Values (Lundberg and Lee, 2017).\n2.2 Gradient Based Methods\n2.2.1 Saliency\nSaliency is a simple gradient-based approach, uti-\nlizing the model’s gradient at the input point as the\ncorresponding feature attribution (Simonyan et al.,\n2013). This method can be understood as taking a\nfirst order approximation of the function, in which\nthe gradients would serve as the coefficients of each\nfeature in the model.\nϕi(f,X) =f′(X) (3)\n2.2.2 Integrated Gradients\nIntegrated Gradients estimates attribution by com-\nputing the path integral of model gradients between\nthe baseline point and input point (Sundararajan\net al., 2017). This approach has been shown to sat-\nisfy desirable theoretical properties including sen-\nsitivity and implementation invariance. Formally,\nthe method can be defined as\nϕi(f,X) = (Xi −Bi)\n×\n∫1\nα=0\nf′(B+ (X−B)α)\n∂xi\ndα\n(4)\n2.2.3 Other Gradient-Based Methods\nOther popular gradient-based attribution methods\ninclude DeepLift and Layerwise Relevance Pro-\npogation (LRP) (Shrikumar et al., 2017; Bach et al.,\n2015). These methods both require a backward\npass of the model on the original inputs but cus-\ntomize the backward propagation of specific func-\ntions, instead of using their default gradient func-\ntions.\n3 Language Model Attribution\nIn Captum v0.7, we propose new functionalities\nto apply the attribution methods within Captum to\nanalyze the behaviors of LLMs. Users can choose\nany interested tokens or segments from the input\n166\nOUTPUT 1:  attribution for most likely   \n                  decoded sequence \nDavid lives in Palm Coast, FL and …\nplaying  golf,  hiking, and cooking.\n  -.204          1.081    -0.8918  -0.0498   -.2699\n Feature Importance Scores Relative to Selected Input\nOUTPUT 2:  attribution for user provided \n                  target string\nDavid lives in Palm Coast, FL and ..\nOpen sourcing explainable tech..\n  .7051        -2.341             -.1851              -.2834                                   \n Feature Importance Scores Relative to Selected Input\nCase 2\nPrompt:  David lives in Palm \nCoast, FL and is a lawyer.  His \npersonal interests include \nTarget: open sourcing \nexplainable techniques for \ngenerative LLMs and golf.\nCase 1\nPrompt:  David lives in Palm \nCoast, FL and is a lawyer.  His \npersonal interests include \nTarget: None\nLLM + \nCAPTUM \nATTRIBUTION \nMETHOD\nLLM + \nCAPTUM \nATTRIBUTION \nMETHOD\nFigure 1: Example of applying Captum attribution methods to analyze the input prompt’s effect on the output of\nLLMs, showing two types of target strings accepted by Captum attribution API and token level attribution outputs\nfor both with respect to the input \"Palm Coast\". In Case 1, no Target string is provided, so attributions are provided\nfor the most likely decoded sequence. In Case 2, attributions are provided for the chosen target output.\nprompt as features, e.g., \"Palm Coast\" in the exam-\nple shown in Figure 1, and use attribution methods\nto quantify their impacts to the generation targets,\nwhich can be either a specified output sequence or\na likely generation from the model.\n3.1 Perturbation-Based Methods\nWe introduce simple APIs to experiment with\nperturbation-based attribution methods including\nFeature Ablation, Lime, Kernel SHAP and Shapley\nValue Sampling.\nWe prioritize ease-of-use and flexibility, allow-\ning users to customize the chosen features for attri-\nbution, mask features into groups as necessary, and\ndefine appropriate baselines to ensure perturbed\ninputs remain within the natural data distribution.\nIn Figure 2, we demonstrate an example usage\nof the LLMAttribution API for the simple prompt\n\"Dave lives in Palm Coast, FL and is a lawyer. His\npersonal interests include\". Providing this input\nprompt to a language model to generate the most\nlikely subsequent tokens, we can apply Captum\nto understand the impact of different parts of the\nprompt string on the model generation. Figure 3\npresents a more customized usage where we use\nthe same function to understand a specific output\nwe are interested in ( \"playing golf, hiking, and\ncooking.\").\n3.1.1 Defining Features\nUsers are able to define and customize ’features’\nfor attribution in the prompt text. The simplest ap-\nproach would be defining the features as individual\ntokens.\nUnfortunately, in many cases, it doesn’t make\nsense to perturb individual tokens, since this may\nno longer form a valid sentence in the natural distri-\nbution of potential input sequences. For example,\nperturbing the token \"Palm\" in the above sentence\nwould result in a city name that is not in the natu-\nral distribution of potential cities in Florida, which\nmay lead to unexpected impacts on the perturbed\nmodel output. Moreover, tokenizers used in mod-\nern LLMs may further break a single word in many\ncases. For example, the tokenizer can break the\nword \"spending\" into \"_sp\" and \"ending\".\nThe API provides flexibility to define units of\nattribution as custom interpretable features which\ncould be individual words, tokens, phrases, or even\nfull sentences. For example, in Figure 2, we select\nthe relevant features to be the name, city, state,\noccupation, and pronoun in the sentence prompt\nand desire to determine the relative contribution of\nthese contextual features on the model’s predicted\nsentence completion.\nUsers can define the units for attribution as a list\nor dictionary of features and provide a format string\nor function to define a mapping from the attribution\nunits to the full input prompt as shown in Figure 3.\n3.1.2 Baselines\nThe baseline choice is particularly important for\ncomputing attribution for text features, as it serves\nas the reference value used when perturbing the\nchosen feature. The perturbation-based feature API\nallows defining custom baselines corresponding to\n167\nfrom captum . attr import FeatureAblation , LLMAttribution , TextTemplateFeature\nfa = FeatureAblation ( model )\nllm_attr = LLMAttribution (fa , tokenizer )\ninp = TextTemplateFeature (\n# the text template\n\"{} lives in {} , {} and is a {} . {} personal interests include \",\n# the values of the features\n[\" Dave \", \" Palm Coast \", \"FL\", \" lawyer \", \" His \"],\n# the reference baseline values of the features\nbaselines =[\" Sarah \", \" Seattle \", \"WA\", \" doctor \", \" Her \"],\n)\nllm_attr . attribute ( inp )\nFigure 2: Example of applying Captum with a list of features in a text template\ninp = TextTemplateFeature (\n\"{ name } lives in { city }, { state } and is a { occupation }. { pronoun } personal\ninterests include \",\n{\" name \":\" Dave \", \" city \": \" Palm Coast \", \" state \": \"FL\", \" occupation \":\" lawyer \", \"\npronoun \":\" His \"},\nbaselines = {\" name \":\" Sarah \", \" city \": \" Seattle \", \" state \": \"WA\", \" occupation \":\" doctor\n\", \" pronoun \":\" Her \"}\n)\nattr_result = llm_attr . attribute (inp , target =\" playing golf , hiking , and cooking .\")\nattr_result . plot_token_attr ()\nFigure 3: Example of applying Captum with a dictionary of features in a text template and a specific target, and\nvisualize the token attribution\neach input feature.\nIt is recommended to select a baseline which fits\nthe context of the original text and remains within\nthe natural data distribution. For example, replac-\ning the name of a city with another city ensures\nthe sentence remains naturally coherent, but allows\nmeasuring the contribution of the particular city\nselected.\nIn addition to a single baseline, the Captum API\nalso supports providing a distribution of baselines,\nprovided as either a list or function to sample a\nreplacement option. For example, in the example\nabove, the name \"Dave\" could be replaced with\na sample from the distribution of common first\nnames to measure any impact of the name \"Dave\"\nin comparison to the chosen random distribution as\nshown in Figure 6.\n3.1.3 Masking Features\nSimilar to the underlying Captum attribution meth-\nods, we support feature masking, which enables\ngrouping features together to perturb as a single\nunit and obtain a combined, single attribution score.\nThis functionality may be utilized for highly corre-\nlated features in the text input, where it often makes\nsense to ablate these features together, rather than\nindependently.\nFor example, in Figure 2, the feature pairs (city,\nstate) and (name, pronoun) are often highly corre-\nlated, and thus it may make sense to group them\nand consider them as a single feature.\n3.1.4 Target Selection\nFor any attribution method, it is also necessary to\nselect the target function output for which attribu-\ntion outputs are computed. Since language models\ntypically output a probability distribution over the\nspace of tokens for each subsequently generated to-\nken, there are numerous choices for the appropriate\ntarget.\nBy default, when no target is provided, the target\nfunction behavior is for the attribution method to\nreturn attributions with respect to the most likely\ndecoded token sequence.\nWhen a target string is provided, the target func-\ntion is the log probability of the output sequence\nfrom the language model, given the input prompt.\nFor a sequence with multiple tokens, this is numer-\nically computed through the sum of the log proba-\nbilities of each token in the target string. Figure 1\nshows these two input use cases and shows token\nlevel attribution relative to an input subsequence\n168\nfor both.\nWe also support providing a custom function on\nthe output logit distribution, which allows attribu-\ntion with respect to an alternate quantity such as\nthe entropy of the output token distribution.\n3.2 Gradient-Based Methods\nCaptum 0.7 also provides a simple API to apply\ngradient-based methods to LLMs. Applying these\nmethods to language models is typically more chal-\nlenging than for models with dense feature inputs,\nsince embedding lookups in LLMs are typically\nnon-differentiable functions, and gradient-based\nattributions need to be obtained with respect to\nembedding outputs. Captum allows these attribu-\ntions to be aggregated across embedding dimen-\nsions to obtain per-token attribution scores. Figure\n4 demonstrates an example of applying Integrated\nGradients on a sample input prompt.\n3.3 Visualization\nWe also open source utilities for easily visualizing\nthe attribution outputs from language models. Fig-\nure 3 shows how to use the utilities to visualize the\nattribution result, and Figure 5 demonstrates the\nheatmap plotted with the prompt along the top, the\ntarget string along the left side and feature impor-\ntance scores in each cell.\n(Feature) Value Golfing Hiking Cooking\n(Name) Dave 0.4660 -0.2640 -0.4515\n(City) Palm Coast 1.0810 -0.8762 -0.2699\n(State) FL 0.6070 -0.3620 -0.3513\n(Occupation) lawyer 0.7584 -0.1966 0.0331\n(Pronoun) His 0.2217 -0.0650 -0.2577\nTable 1: Associations between input and generated text\nfeatures\n4 Applications\nIn this section, we discuss two applications of the\nattribution methods described above in different\ncontexts. These applications provide additional\ntransparency as well as contribute to a better un-\nderstanding of a model’s learned associations and\nrobustness.\n4.1 Understanding Model Associations\nThis perturbation-based tooling can be particularly\nuseful for improved understanding of learned asso-\nciations in LLMs.\nConsider the example prompt:\n“David lives in Palm Coast, FL and is a lawyer.\nHis personal interests include ”\nWe can define features including gender, city,\nstate and occupation. Obtaining attributions on\nthese features against the subsequent target\n“playing golf, hiking, and cooking. ”\nallows us to better understand why the model pre-\ndicted these personal interests and how each feature\ncorrelates with each of these interests. The model\nmight be associating this activity as a common\nhobby for residents in the specific city or as an ac-\ntivity common to lawyers. Through this choice of\nfeatures, we can obtain a better understanding of\nwhy the model predicted these particular hobbies\nand how this associates with the context provided\nin the prompt.\nWe apply Shapley Value Sampling to better un-\nderstand how each of the features contributed to\nthe prediction. The corresponding code snippet\nis shown in the Appendix in Figure 6. Table 1\npresents the effects of each feature on the LLM’s\nprobability of outputting the corresponding inter-\nest, with positive and negative values indicating\nincreases and decreases of the probability respec-\ntively. We can therefore identify some interesting\nand even potentially biased associations. For ex-\nample, the male name and pronoun, i.e., \"Dave\"\nand \"His\", have positive attribution to \"golfing\" but\nnegative attribution to \"cooking\".\n4.2 Evaluating Effectiveness of Few-Shot\nPrompts\nSignificant prior literature has demonstrated the\nability of LLMs to serve as few-shot learners\n(Brown et al., 2020). We utilize Captum’s attri-\nbution functionality to better understand the impact\nand contributions of few-shot examples to model\npredictions. Table 2 demonstrates four example\nfew shot prompts and corresponding attribution\nscores when predicting positive sentiment for \"I\nreally liked the movie, it had a captivating plot!\"\nmovie review.\nHere we aim to understand the impact of each ex-\nample prompt on the Positive sentiment prediction.\nThe LLM is asked to predict positive or negative\nsentiment using the following prompt:\n“Decide if the following movie review enclosed\nin quotes is Positive or Negative. Output only\neither Positive or Negative:\n169\nfrom captum . attr import LayerIntegratedGradients , TextTokenFeature\nig = LayerIntegratedGradients ( model , \" model . embed_tokens \")\nllm_attr = LLMGradientAttribution (ig , tokenizer )\ninp = TextTokenFeature (\" Dave lives in Palm Coast , FL and is a lawyer . His personal\ninterests include \", tokenizer )\nllm_attr . attribute ( inp )\nFigure 4: Example of applying Captum with a gradient-based approach\nFigure 5: Text Attribution Visualization Example\n‘I really liked the movie, it had a captivating plot!’\n”\nWe consider each of the provided example\nprompts as features and we utilize zero-shot prompt\nas a baseline in the attribution algorithm. The de-\ntailed implementation can be found in Appendix in\nFigure 7.\nWe obtain results as shown in Table 2 by ap-\nplying Shapley Values. Surprisingly, the results\nsuggest that all the provided examples actually re-\nduced confidence in the prediction of \"Positive\".\nExample Shapley\nValue\n’The movie was ok, the actors weren’t\ngreat’ -> Negative\n-0.0413\n’I loved it, it was an amazing story!’\n-> Positive\n-0.2751\n’Total waste of time!!’ -> Negative -0.2085\n’Won’t recommend’ -> Negative -0.0399\nTable 2: Example prompts’ contribution to model re-\nsponse \"Positive.\"\n5 Related Work\nNumerous prior works have developed and inves-\ntigated attribution methods with a variety of prop-\nerties, but few efforts have been made to develop\nopen-source interpretability tools providing a vari-\nety of available methods, particularly for the text\ndomain. Captum was initially developed to fill\nthis gap and provide a centralized resource for re-\ncent interpretability methods proposed in literature\n(Kokhlikyan et al., 2020).\nEcco and inseq are two libraries that have pro-\nvided attribution functionalities for text / language\nmodels (Sarti et al., 2023; Alammar, 2021), and\nboth libraries are built on top of the attribution\nmethods available in Captum. These libraries pri-\nmarily focus on gradient-based attribution methods,\nwhich provide token-level attribution based on gra-\ndient information.\nIn contrast, our main contribution in this work\nhas been a focus on perturbation-based methods\nand providing flexibility on aspects such as feature\ndefinition, baseline choice and masking. We do not\nnecessarily expect that these attribution methods\nprovide a score for each token individually, which\nis typically the case for gradient-based methods.\nThis shift in structure allows us to generalize to\na variety of cases and allows the users to define\nfeatures for attribution as it fits best.\nSome prior work on attribution methods have\nalso demonstrated limitations and counterexamples\nof these methods in explaining a model’s reliance\non input features, particularly with gradient-based\nattribution methods (Adebayo et al., 2018).\nPerturbation-based methods generally have an\nadvantage of being justifiable through the model’s\noutput on counterfactual perturbed inputs. But per-\nturbing features by removing individual tokens or\nreplacing them with pad or other baseline tokens\nmay result in inputs outside of the natural data dis-\ntribution, and thus, model outputs in this region\nmay not be accurate. The tools developed have\n170\nbeen designed to make it easier for developers to\nselect features, baselines, and masks which can en-\nsure perturbations remain within the natural data\ndistribution in order to obtain more reliable feature\nattribution results.\nRecent advances in data augmentation (Plušˇcec\nand Šnajder, 2023) for natural language processing\nhave led to the development of a number of open-\nsource libraries (Wang et al., 2021; Papakipos and\nBitton, 2022; Zeng et al., 2021; Morris et al., 2020;\nMa, 2019; Dhole et al., 2022; Wu et al., 2021).\nAmong many functionalities, these libraries pro-\nvide a rich set of text perturbations. Some libraries\nhave specific focus, e.g. perturbing demographic\nreferences (Qian et al., 2022). An interesting di-\nrection of future work will be the extension of our\npresent API to provide fully automated feature and\nbaseline selections, allowing users to simply pro-\nvide an input string and automatically identify ap-\npropriate text features and corresponding baselines\nfor attribution.\n6 Conclusion\nIn this work, we introduce new features in the\nopen source library Captum that are specifically de-\nsigned to analyze the behavior of generative LLMs.\nWe provide an overview of the available functional-\nities and example applications of their potential in\nunderstanding learned associations and evaluating\neffectiveness of few-shot prompts within generative\nLLMs. We demonstrate examples for using pertur-\nbation and gradient-based attribution methods with\nCaptum which highlight the API’s flexibility on\naspects such as feature definition, baseline choice\nand masking. This flexibility in structure allows\nusers to generalize to a variety of cases, simplifying\ntheir ability to conduct explainability experiments\non generative LLMs.\nIn the future, we plan to expand our API for\nadditional automation in baseline and feature selec-\ntion as well as incorporate other categories of inter-\npretability techniques for language models. Run-\ntime performance optimization of attribution algo-\nrithms is another area of research that could be\nbeneficial for the OSS community.\nReferences\nJulius Adebayo, Justin Gilmer, Michael Muelly, Ian\nGoodfellow, Moritz Hardt, and Been Kim. 2018. San-\nity checks for saliency maps.\nJ Alammar. 2021. Ecco: An open source library for the\nexplainability of transformer language models. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing: System Demonstrations. Association for\nComputational Linguistics.\nSebastian Bach, Alexander Binder, Grégoire Montavon,\nFrederick Klauschen, Klaus-Robert Müller, and Wo-\njciech Samek. 2015. On pixel-wise explanations\nfor non-linear classifier decisions by layer-wise rele-\nvance propagation. PloS one, 10(7):e0130140.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nJavier Castro, Daniel Gómez, and Juan Tejada. 2009.\nPolynomial calculation of the shapley value based\non sampling. Computers & Operations Research,\n36(5):1726–1730.\nKaustubh D. Dhole, Varun Gangal, Sebastian\nGehrmann, Aadesh Gupta, Zhenhao Li, Saad Ma-\nhamood, Abinaya Mahendiran, Simon Mille, Ashish\nShrivastava, Samson Tan, Tongshuang Wu, Jascha\nSohl-Dickstein, Jinho D. Choi, Eduard Hovy, On-\ndrej Dusek, Sebastian Ruder, Sajant Anand, Na-\ngender Aneja, Rabin Banjade, Lisa Barthe, Hanna\nBehnke, Ian Berlot-Attwell, Connor Boyle, Car-\noline Brun, Marco Antonio Sobrevilla Cabezudo,\nSamuel Cahyawijaya, Emile Chapuis, Wanxiang\nChe, Mukund Choudhary, Christian Clauss, Pierre\nColombo, Filip Cornell, Gautier Dagan, Mayukh\nDas, Tanay Dixit, Thomas Dopierre, Paul-Alexis\nDray, Suchitra Dubey, Tatiana Ekeinhor, Marco Di\nGiovanni, Tanya Goyal, Rishabh Gupta, Rishabh\nGupta, Louanes Hamla, Sang Han, Fabrice Harel-\nCanada, Antoine Honore, Ishan Jindal, Przemys-\nlaw K. Joniak, Denis Kleyko, Venelin Kovatchev,\nKalpesh Krishna, Ashutosh Kumar, Stefan Langer,\nSeungjae Ryan Lee, Corey James Levinson, Hualou\nLiang, Kaizhao Liang, Zhexiong Liu, Andrey Lukya-\nnenko, Vukosi Marivate, Gerard de Melo, Simon\nMeoni, Maxime Meyer, Afnan Mir, Nafise Sadat\nMoosavi, Niklas Muennighoff, Timothy Sum Hon\nMun, Kenton Murray, Marcin Namysl, Maria Obed-\nkova, Priti Oli, Nivranshu Pasricha, Jan Pfister,\nRichard Plant, Vinay Prabhu, Vasile Pais, Libo Qin,\nShahab Raji, Pawan Kumar Rajpoot, Vikas Rau-\nnak, Roy Rinberg, Nicolas Roberts, Juan Diego\nRodriguez, Claude Roux, Vasconcellos P. H. S.,\nAnanya B. Sai, Robin M. Schmidt, Thomas Scialom,\nTshephisho Sefara, Saqib N. Shamsi, Xudong Shen,\nHaoyue Shi, Yiwen Shi, Anna Shvets, Nick Siegel,\nDamien Sileo, Jamie Simon, Chandan Singh, Ro-\nman Sitelew, Priyank Soni, Taylor Sorensen, William\nSoto, Aman Srivastava, KV Aditya Srivatsa, Tony\nSun, Mukund Varma T, A Tabassum, Fiona Anting\nTan, Ryan Teehan, Mo Tiwari, Marie Tolkiehn,\n171\nAthena Wang, Zijian Wang, Gloria Wang, Zijie J.\nWang, Fuxuan Wei, Bryan Wilie, Genta Indra Winata,\nXinyi Wu, Witold Wydma´nski, Tianbao Xie, Usama\nYaseen, Michael A. Yee, Jing Zhang, and Yue Zhang.\n2022. Nl-augmenter: A framework for task-sensitive\nnatural language augmentation.\nNarine Kokhlikyan, Vivek Miglani, Miguel Martin,\nEdward Wang, Bilal Alsallakh, Jonathan Reynolds,\nAlexander Melnikov, Natalia Kliushkina, Carlos\nAraya, Siqi Yan, and Orion Reblitz-Richardson. 2020.\nCaptum: A unified and generic model interpretability\nlibrary for pytorch.\nScott Lundberg and Su-In Lee. 2017. A unified ap-\nproach to interpreting model predictions.\nEdward Ma. 2019. Nlp augmentation.\nhttps://github.com/makcedward/nlpaug.\nJohn X. Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby,\nDi Jin, and Yanjun Qi. 2020. Textattack: A frame-\nwork for adversarial attacks, data augmentation, and\nadversarial training in nlp.\nOpenAI. 2023. Gpt-4 technical report.\nZoe Papakipos and Joanna Bitton. 2022. Augly: Data\naugmentations for robustness.\nDomagoj Plušˇcec and Jan Šnajder. 2023. Data augmen-\ntation for neural nlp.\nRebecca Qian, Candace Ross, Jude Fernandes,\nEric Michael Smith, Douwe Kiela, and Adina\nWilliams. 2022. Perturbation augmentation for fairer\nNLP. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9496–9521, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016. \"why should i trust you?\": Explain-\ning the predictions of any classifier.\nGabriele Sarti, Nils Feldhus, Ludwig Sickert, Oskar\nvan der Wal, Malvina Nissim, and Arianna Bisazza.\n2023. Inseq: An interpretability toolkit for sequence\ngeneration models. ArXiv, abs/2302.13942.\nAvanti Shrikumar, Peyton Greenside, and Anshul Kun-\ndaje. 2017. Learning important features through\npropagating activation differences. In International\nconference on machine learning, pages 3145–3153.\nPMLR.\nKaren Simonyan, Andrea Vedaldi, and Andrew Zis-\nserman. 2013. Deep inside convolutional networks:\nVisualising image classification models and saliency\nmaps. arXiv preprint arXiv:1312.6034.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. In Interna-\ntional conference on machine learning, pages 3319–\n3328. PMLR.\nXiao Wang, Qin Liu, Tao Gui, Qi Zhang, Yicheng\nZou, Xin Zhou, Jiacheng Ye, Yongxin Zhang, Rui\nZheng, Zexiong Pang, Qinzhuo Wu, Zhengyan Li,\nChong Zhang, Ruotian Ma, Zichu Fei, Ruijian Cai,\nJun Zhao, Xingwu Hu, Zhiheng Yan, Yiding Tan,\nYuan Hu, Qiyuan Bian, Zhihua Liu, Shan Qin, Bolin\nZhu, Xiaoyu Xing, Jinlan Fu, Yue Zhang, Minlong\nPeng, Xiaoqing Zheng, Yaqian Zhou, Zhongyu Wei,\nXipeng Qiu, and Xuanjing Huang. 2021. TextFlint:\nUnified multilingual robustness evaluation toolkit for\nnatural language processing. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing: System\nDemonstrations, pages 347–355, Online. Association\nfor Computational Linguistics.\nTongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, and\nDaniel S Weld. 2021. Polyjuice: Generating coun-\nterfactuals for explaining, evaluating, and improving\nmodels. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 6707–6723.\nGuoyang Zeng, Fanchao Qi, Qianrui Zhou, Tingji\nZhang, Zixian Ma, Bairu Hou, Yuan Zang, Zhiyuan\nLiu, and Maosong Sun. 2021. OpenAttack: An open-\nsource textual adversarial attack toolkit. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing:\nSystem Demonstrations. Association for Computa-\ntional Linguistics.\nA Appendix\n172\nfrom captum . attr import ShapleyValueSampling , LLMAttribution , TextTemplateFeature ,\nProductBaselines\nsvs = ShapleyValueSampling ( model )\nbaselines = ProductBaselines (\n{\n(\" name \", \" pronoun \"): [(\" Sarah \", \" Her \"), (\" John \", \" His \")],\n\" city \": [\" Seattle \", \" Boston \"],\n\" state \": [\"WA\", \"MA\"],\n\" occupation \": [\" doctor \", \" engineer \", \" teacher \", \" technician \", \" plumber \"],\n}\n)\nllm_attr = LLMAttribution (svs , tokenizer )\ninp = TextTemplateFeature (\n\"{ name } lives in { city }, { state } and is a { occupation }. { pronoun } personal\ninterests include \",\n{\" name \":\" Dave \", \" city \": \" Palm Coast \", \" state \": \"FL\", \" occupation \":\" lawyer \", \"\npronoun \":\" His \"},\nbaselines = baselines ,\n)\nattr_result = llm_attr . attribute (inp , target =\" playing golf , hiking , and cooking .\")\nFigure 6: Applying Captum for the model associations example\nfrom captum . attr import ShapleyValues , LLMAttribution , TextTemplateFeature\nsv = ShapleyValues ( model )\nllm_attr = LLMAttribution (sv , tokenizer )\ndef prompt_fn (* examples ):\nmain_prompt = \" Decide if the following movie review enclosed in quotes is\nPositive or Negative :\\n 'I really liked\nthe Avengers , it had a captivating\nplot !'\\ nReply only Positive or\nNegative .\"\nsubset = [ elem for elem in examples if elem ]\nif not subset :\nprompt = main_prompt\nelse :\nprefix = \" Here are some examples of movie reviews and classification of\nwhether they were Positive or\nNegative :\\n\"\nprompt = prefix + \"\\n\". join ( subset ) + \"\\n\" + main_prompt\nreturn \"[ INST ] \" + prompt + \"[/ INST ]\"\ninput_examples = [\n\"'The movie was ok , the actors weren 't great ' -> Negative \",\n\"'I loved it , it was an amazing story ! ' -> Positive \",\n\"'Total waste of time !! ' -> Negative \",\n\"'Won 't recommend ' -> Negative \",\n]\ninp = TextTemplateFeature ( prompt_fn , input_examples )\nattr_result = llm_attr . attribute ( inp )\nFigure 7: Applying Captum for the few-shot prompt example\n173",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.9152202606201172
    },
    {
      "name": "Generative grammar",
      "score": 0.8477758169174194
    },
    {
      "name": "Computer science",
      "score": 0.8148361444473267
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6093904376029968
    },
    {
      "name": "Language model",
      "score": 0.5714198350906372
    },
    {
      "name": "Language understanding",
      "score": 0.5291951894760132
    },
    {
      "name": "Generative model",
      "score": 0.48657548427581787
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.4716426730155945
    },
    {
      "name": "Natural language processing",
      "score": 0.43455198407173157
    },
    {
      "name": "Engineering",
      "score": 0.06365850567817688
    },
    {
      "name": "Aerospace engineering",
      "score": 0.0
    }
  ]
}