{
    "title": "Nob-MIAs: Non-biased Membership Inference Attacks Assessment on Large Language Models with Ex-Post Dataset Construction",
    "url": "https://openalex.org/W4401688307",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5070235439",
            "name": "Cédric Eichler",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5106602253",
            "name": "Nathan Champeil",
            "affiliations": [
                null,
                "École Nationale Supérieure de Techniques Avancées"
            ]
        },
        {
            "id": "https://openalex.org/A5048401513",
            "name": "Nicolas Anciaux",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5043315649",
            "name": "Alexandra Bensamoun",
            "affiliations": [
                "Université Paris-Saclay"
            ]
        },
        {
            "id": "https://openalex.org/A5068017939",
            "name": "Héber H. Arcolezi",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5091681007",
            "name": "José M. de Fuentes",
            "affiliations": [
                "Universidad Carlos III de Madrid"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4362655426",
        "https://openalex.org/W3112689365",
        "https://openalex.org/W4389519044",
        "https://openalex.org/W6600146248",
        "https://openalex.org/W6600113797",
        "https://openalex.org/W4402683484",
        "https://openalex.org/W6788175385",
        "https://openalex.org/W4400723399",
        "https://openalex.org/W6604317845",
        "https://openalex.org/W4404782737",
        "https://openalex.org/W6857316800",
        "https://openalex.org/W2535690855",
        "https://openalex.org/W6786113699",
        "https://openalex.org/W4402683760",
        "https://openalex.org/W2795435272"
    ],
    "abstract": null,
    "full_text": "Nob-MIAs: Non-biased Membership Inference\nAttacks Assessment on Large Language Models\nwith Ex-Post Dataset Construction\nCédric Eichler1,2\u0000 [0000−0003−3026−1749], Nathan Champeil3,2,\nNicolas Anciaux2,1[0009−0003−7537−295X], Alexandra Bensamoun4,\nHéber H. Arcolezi2[0000−0001−8059−7094], and\nJosé Maria De Fuentes5[0000−0002−4023−3197]\n1 LIFO, INSA Centre Val de Loire, Université d’Orléans, Bourges, France\n2 Inria Saclay, France<firstname.lastname@inria.fr>\n3 ENSTA Paris, Palaiseau, France\n4 Université Paris-Saclay<alexandra.bensamoun@universite-paris-saclay.fr>\n5 Universidad Carlos III de Madrid,<josemaria.defuentes@uc3m.es>\nAbstract. The rise of Large Language Models (LLMs) has triggered\nlegal and ethical concerns, especially regarding the unauthorized use of\ncopyrighted materials in their training datasets. This has led to law-\nsuits against tech companies accused of using protected content without\npermission. Membership Inference Attacks (MIAs) aim to detect whether\nspecific documents were used in a given LLM pretraining, but their effec-\ntiveness is undermined by biases such as time-shifts and n-gram overlaps.\nThis paper addresses the evaluation of MIAs on LLMs with partially\ninferable training sets, under the ex-post hypothesis, which acknowl-\nedges inherent distributional biases between members and non-members\ndatasets. We propose and validate algorithms to create “non-biased” and\n“non-classifiable” datasets for fairer MIA assessment. Experiments using\nthe Gutenberg dataset on OpenLLaMA and Pythia show that neutraliz-\ning known biases alone is insufficient. Our methods produce non-biased\nex-post datasets on which MIAs achieve AUC-ROC scores comparable\nto those previously obtained on genuinely random datasets, validating\nour approach. Globally, MIAs yield results close to random, with only\none Meta-Classifier-based MIA being effective on both random and our\ndatasets, but its performance decreases when bias is removed.\nKeywords: Membership Inference Attack· LLM · Assessment · Bias.\n1 Introduction\nThe proliferation of Large Language Models (LLMs) has ignited significant le-\ngal and ethical debates, particularly concerning copyright infringement. These\nmodels often do not document their training data sources, leading to disputes\nover unauthorized use of copyrighted material. For instance, lawsuits are piling\nup against OpenAI accused of training ChatGPT using articles and other books\narXiv:2408.05968v2  [cs.CR]  26 Sep 2024\n2 C. Eichler et al.\nwithout permission6. Similar accusations have been leveled at Meta or Google\nfor allegedly using protected content7. These issues underscore the societal, eco-\nnomic and legal implications of LLM training practices.\nRecent surveys highlight the challenges of respecting copyright in AI training\nacross different jurisdictions, such as the USA and France. In the USA, the use of\ncopyrighted data is generally prohibited without the rights holder’s permission\nunless it falls under “fair use” [24]. For the culture and media sectors, LLM train-\ning could not be exempted from this limitation due to the conditions associated\nwith it. More than twenty lawsuits are pending in the USA. In the European\nUnion, the Directive 2019/790 on copyright and related rights in the digital sin-\ngle market introduces a “text and data mining” exception, for any purpose, that\ncould correspond to the use of protected content for LLM training. However, its\nbenefit is conditional on lawful access to copyrighted data and the absence of an\nopt-out by rights holders. However, not only do the training databases contain\ninfringing works, but most of the rights holders have exercised their opt-out.\nHowever, the opacity of the process compromises the return to exclusive rights.\nSo, to provide leverage, the AI Act (European Regulation 2024/1689), the first\ncomprehensive regulation on AI, has required LLM providers, on the one hand,\nto put in place an internal policy aimed at respecting copyright and, on the\nother, to be transparent about the sources of training. The AI Office will pro-\nvide a template on this point. The issue of knowledge of the use of content by\nthe LLM is therefore crucial for rights holders.\nObjective. From a technical perspective, determining whether a specific doc-\nument was a member of the training set of a machine learning (ML) model\nbased on the model’s output, is a Membership Inference Attacks (MIAs) prob-\nlem, highlighted in 2017 by Shokri et al. [26]. However, the effectiveness of MIAs\nin the context of LLMs is subject to debate.\nLimits of existing solutions. To determine whether a particular document\nhas been used to train an AI model, MIAs rely on overfitting, which results in\nstronger predictions when applied to training data. While some research shows\nthat MIAs can achieve high accuracy [19,18], other studies [7,6] question their\nvalidity due to inherent biases in the datasets of members and non-members used\nfor their assessment. For example, biases like time shifts and n-gram overlaps can\nlead to over-interpretation of results [7]. Additionally, studies indicate that some\nbiases can be exploited to make a “blind” classifier, without model access, more\neffective than MIAs [6]. This raises doubts about the robustness and practical\nrelevance of current MIA techniques, and recent surveys like [13] point out the\nlack of rigor and practical relevance of current proposals.\nResearch question and contributions.This paper aims to address the problem\nof assessing MIA effectiveness on LLMs which do not disclose their full training\ndatasets but where part of the training dataset can be inferred. The research\n6 See, e.g., “The copyright lawsuits against OpenAI are piling up as the tech company\nseeks data to train its AI”, Jun 30, 2024\n7 See, e.g., “Congress Senate Tech Companies Pay AI Training Data”, July 2, 2024\nNob-MIAs: Non-biased MIAs Assessment on LLMs with Ex-Post Datasets 3\nquestion we seek to answer is:“How can we construct an unbiased dataset for\nevaluating MIAs on LLMs with partially inferable training sets?”\nWeproposeandevaluatetwoapproaches:(1)Creatingdatasetsthatare“non-\nbiased” by design with respect to known biases, and (2) Constructing datasets\nthat cannot be classified, ensuring fairer assessment. Our contributions can be\nsummarized as follows:\n– We provide algorithms for constructing ex-post datasets of two types:No −\nNgram (“NoN-grambias”)and No−Class (“nonclassifiable”),eachdesigned\nto mitigate specific types of biases for MIA assessment.\n– We validate our algorithms and compare our proposed methods in the as-\nsessment of existing MIAs.\n– We demonstrate that neutralizing known biases (e.g., time shifts, n-gram\nbiases) is insufficient for accurate MIA assessment; we also show that several\nexisting MIAs, which are presumed to be effective, are not efficient when\nevaluated using non-classifiable datasets. For instance, our experiments show\nthat the TPR@10%FPR and ROC AUC of the best performing MIA out\nof the 6 assessed drop by 40% and 14.3% respectively when evaluated on\ndatasets produced with our approach rather than on randomly sampled ones.\nOutline. Section 2 reviews related work and positions our research. Section 3\nsynthesizes our hypotheses and the addressed problem. Section 4 presents our\nproposed solutions and algorithms, detailing ex-post (i.e., a posteriori) construc-\ntion of unbiased datasets. Section 5 provides a comparative experimental eval-\nuation of the proposed solutions. Finally, Section 6 concludes the paper and\nsuggests directions for future research.\n2 Related Work and Positioning\nMIA techniques, originally developed for machine learning classification algo-\nrithms [26], have recently been adapted to the context of LLMs. The baseline\ntechnique uselikelihood-basedmetrics such as loss [31] and perplexity [2] to dis-\ntinguish betweenmembers (documents which were used for LLM training) and\nnon-members (not used in training). Several studies show that likelihood-based\nMIAs applied to LLMs are effective, with high AUC-ROC values. For exam-\nple, [19] reports an AUC-ROC of0.856 for the Gutenberg dataset [23] (also used\nfor evaluation in our paper). Other metrics, such as Min-k%Prob [25], are based\non the premise that if the text has been read by the LLM, it is more likely to ap-\npear. More precisely, Min-k%Prob selects the k% tokens of the document with\nthe minimum probabilities returned by the LLM and computes their average\nlog-likelihood.\nNeighboring-based MIAs calibrate the perplexity score using either neighbor-\ning models M′ or neighboring documentsD′. Classification between members\nand non-members is then obtained by comparing the likelihood of the target\nmodel M anddocument D withthatoftheneighboringmodel M′ anddocument\nD, or the modelM and neighboring documentsD′. Neighboring models hence\n4 C. Eichler et al.\nassume access to a reference model trained on a disjoint data set drawn from a\nsimilar distribution, which is often unrealistic [7]. Neighboring documents [9] are\nmore realistic but present slightly lower performance and additional difficulties\nin correctly setting noise parameters.\nIn cases where LLMs do not output likelihood information, complementary\nmetrics can be used with acceptable performance penalty. For example, [14] pro-\nposes a MIA method called SaMIA, which measures the similarity between input\nsamples from a documentD and the rest of the text inD using ROUGE [16].\nSaMIA demonstrates an AUC-ROC of0.64 on subsets of The Pile dataset [10].\nRecent studies [7,18] challenge the high-performance claims of MIAs on\nLLMs. They identify several biases that may skew results, such as timeshift\nbetween members and non-members, leading to different distributions of dates\nand word usage. Re-evaluating some MIAs on Pythia [1], trained on genuinely\nrandom train/test splits of The Pile [10] (and hence have no bias), shows de-\ncreased AUC-ROC measures, questioning the apparent success of some MIAs.\nSome studies suggest that naive classifiers can distinguish members from non-\nmembers with good results based on these biases [6,20]. These studies conclude\nthat MIAs must be evaluated on random datasets taken from a same distri-\nbution. While this is possible with open LLMs like Pythia, which reveal their\ntraining and test data sources, it is not feasible for LLMs that do not disclose\ntheir sources (those of interest in copyright cases). As shown in the literature\n(see, e.g., [7,18,21]), the same MIAs yield different performance (accuracy and\nrelevance)resultsondifferentLLMs/datasets.Therefore,theassessmentofMIAs\non open LLMs cannot be directly transposed to LLMs that do not disclose their\ntraining dataset. This confirms the need for techniques like the one we propose.\nA technique inspired by the Regression Discontinuity Design from causal\ninference, originally used to study treatment effects based on a cutoff date, is\nproposed in [20]. However, documents added just before or after the cutoff date\nmust be known and sufficiently numerous. Additionally, declared dates often\ndeviate from reality [4], making this approach impractical.\nMany other works are based on MIA attacks on LLMs but rely on different\nhypotheses, leading to solutions not applicable to our context. [15] introduces a\nframework using loss gap variation during fine-tuning to detect if a document\nhas been seen, though this is not generalizable to initial training documents and\nalso requires an assesment using unbiased datasets. Related works on copyright\naspects include copyright issues in LLM outputs [22,17] to reduce copyrighted\ntext generation and protect users from potential plagiarism, and watermarking\ntechniques [21,29] for detecting violations in LLM pretraining data, or LLM\nfine-tuning data [30], but these works are not transposable to our context.\n3 Problem Statement\nA Membership Inference Attack on a large language modelM is a binary clas-\nsification task aimed at determining whether a specific textual documentD was\nincluded in the training datasetDtrain used to buildM. The goal of this attack\nNob-MIAs: Non-biased MIAs Assessment on LLMs with Ex-Post Datasets 5\nis to design a functionMIA : D → {0, 1} that can ascertain the truth value of\nD ∈ Dtrain for any document in the document spaceD.\nIn the context of copyright checks, our goal is to detect ex-post potential\nviolations involving protected texts in the LLM’s pretraining dataset. Our hy-\npotheses H1 to H3 stem from this context, acknowledging that the LLM may\ntry to obscure the use of these texts:\nH1: Self-Assessment.We assume that a reliable assessment of the MIA must\nbe performed on the target LLMM itself. As shown in the literature (see,\ne.g., [7,18]), the same MIAs yield different performance (accuracy and rele-\nvance) results on different LLMs/datasets.\nH2: Partial Member Knowledge.The training datasetDtrain of M is par-\ntially inferable, i.e., a subsetDknown\ntrain of Dtrain can be inferred by an attacker.\nFor example, it is know that OpenAI models like GPT-4 have memorized\nsome precise collections of copyright protected books [3].\nH3: Bias Recognition.A subset Dknown\nnm of non-members (i.e., Dknown\nnm ⊂\nD\\Dtrain) is (obviously) known. Traditionally, such a subset is constructed\nby considering documents not available at the time the target LLM was re-\nleased. This creates inherent biases in the ex-post context, where members\nand non-members are not drawn from the same random distribution.\nWe address the problem of producing datasets of membersDNob\ntrain ⊂ Dknown\ntrain\nand non-membersDNob\nnm ⊂ Dknown\nnm which aim to minimize bias (hence the name\n“Nob” for Non-biased). These datasets ensure a reliable evaluation of MIAs on\nLLMs while satisfying these three hypotheses.\n4 Neutralizing Bias in Ex-Post Dataset Construction\nInthissection,wepresentourapproachtoidentifyingandmitigatingspecificbias\nin the construction of datasets used for the assessment of MIAs. Our methodol-\nogy operates in two phases: first, addressing bias caused by low n-gram overlap,\nwhich has been shown to significantly affect the assessment of MIAs [7]; and\nsecond, mitigating additional biases that go beyond n-gram overlap.\n4.1 Methodology for Identifying and Mitigating Bias\nWe begin by targeting n-gram bias, as previous work has demonstrated that\nn-gram overlaps between members and non-members can distort MIA bench-\nmarks [7]. To counteract this, we propose theNo − Ngram algorithm, which\naims to generate members and non-members sets with similar distributions of\nn-gram overlaps.\nNext, we leverage traditional classifiers, which we refer to as “LLM-Agnostic”\nclassifiers, to identify and mitigate bias beyond n-gram overlap. These classifiers\noperate without any prior knowledge of the target language modelM or the\ntraining dataset Dtrain. Our approach uses these classifiers to create member\nand non-member datasets that resist effective classification. TheNo − Class\nalgorithm further neutralizes detectable biases, hindering the classifier’s ability\nto distinguish between members and non-members.\n6 C. Eichler et al.\n4.2 Neutralizing N-gram Bias\nThe impact of n-gram distribution on MIA performance has been extensively\ndocumented. For example, time-shifted datasets often exhibit variations in n-\ngram distribution due to changes in dates, but also language, vocabulary and\ntopics of interest over time [7]. A significant difference in n-gram overlap between\nnon-members and left-out members can lead to an inflated evaluation of MIA\nperformance. To mitigate this, we propose the No − Ngram algorithm (see\nAlgorithm 1), which generates member and non-member sets with distributions\nof n-gram overlap w.r.t. left-out members that closely match.\nAlgorithm 1No − Ngram\nInput: Dknown\ntrain set of known members,Dknown\nnm set of non-members, integern the size\nof the output datasets,Dist distance between two vectors\nOutput: DNob\ntrain ⊂ Dknown\ntrain a set of members,DNob\nnm ⊂ Dknown\nnm non-members, minimiz-\ning N-gram bias\nRequire: n ≤ 1/2 ∗ |Dknown\ntrain |\nEnsure: n = |DNob\ntrain| = |DNob\nnm |\n1: DNob\ntrain ← random sample ofDknown\ntrain of sizen\n2: Dremain\ntrain ← Dknown\ntrain \\DNob\ntrain\n3: distribtrain ← distribution(DNob\ntrain, Dremain\ntrain ) ▷ Compute n-gram overlap\ndistribution\n4: DNob\nnm ← ∅\n5: for i = 1to n do ▷ Select document minimizing overlap distributions distance\n6: Dremain\nnm ← Dknown\nnm \\DNob\nnm\n7: D ← arg min\nD∈Dremainnm\n(dKolmogorov−Sminrov (distribution({D} ∪ DNob\nnm ), distribtrain)\n8: DNob\nnm ← {D} ∪ DNob\nnm\n9: end for\nAlgorithm overview.Algorithm 1 operates in three steps:\n– Initial sampling: The algorithm begins by selecting an arbitrary sample\nDNob\ntrain of Dknown\ntrain of an appropriate size (line 1).\n– Overlap distribution computation: It then computes the distribution of n-\ngramoverlapbetweentheselectedmembersandremainingones(line3)using\nthe functiondistribution (described below). This distribution represents the\ntarget overlap distribution that the non-member set should mirror to be\nindistinguishable from selected members.\n– Greedy construction: Afterwards, the non-member dataset is constructed\ndocument by document, in a greedy fashion: at each step (lines 5 to 9),\nthe document that minimizes the Kolmogorov-Smirnov distance (see below)\nbetween the n-gram overlap distributions is selected.\nThe Kolmogorov-Smirnov (KS) distance8 used in the algorithm is a widely\nused metric for measuring the distance between (real, non parametric) distribu-\n8 Other distance metrics could be used. Exploring them is planned for future work.\nNob-MIAs: Non-biased MIAs Assessment on LLMs with Ex-Post Datasets 7\ntions. In the context of LLMs, it is particularly useful for comparing distributions\nof generated verbatim text as it appears in the training data or prompts [27].\nOther distance could be used, which is considered future work.\nThe distribution function produces the distribution of n-gram overlap of a\ndataset with reference to another. For a given documentD, which is considered\nas a sequence ofk tokens (such as letters or words), an n-gram is defined as\na continuous sequence ofn tokens. The overlap of n-grams from a document\nD with reference to a set of documentsDref is computed as the percentage of\nn-grams inD that appear in any document ofDref . The resulting distribution\nis comprised of the overlap scores of each document in the first dataset.\n4.3 Constructing a Non-Classifiable Dataset\nTo mitigate bias indicated by the ability of agnostic classifiers to distinguish\nbetween members and non-members, we introduce theNo − Class algorithm\n(see Algorithm 2). This algorithm is designed to produce datasets where the\nperformance of classifiers is minimized, effectively neutralizing their ability to\ndifferentiate between members and non-members.\nAlgorithm 2No − Class Dataset Generation\nInput Dknown\ntrain , Dknown\nnm , integern the size of the output datasets,(Ci)i∈[1,N] a vector\nof agnostic classifiers outputingP[Ci(D)] the probability ofD being a member\nOutput DNob\ntrain ⊂ Dknown\ntrain , DNob\nnm ⊂ Dknown\nnm\nRequire: n ≤ 1/4 × |Dknown\ntrain | & n ≤ 1/4 × |Dknown\nnm |\n1: Dm ← random sample ofDknown\ntrain of sizen\n2: Dnm ← random sample ofDknown\nnm of sizen\n3: train each(Ci)i∈[1,N] on Dm ∪ Dnm\n4: DNob\ntrain ← ∅\n5: Dremain\ntrain ← Dknown\ntrain \\Dm\n6: DNob\nnm ← ∅\n7: Dremain\nnm ← Dknown\nnm \\Dnm\n8: for i = 1to n do ▷ populating members and non-members minimizing confidence\n9: Dm ← arg min\nDm∈Dtrain\nremain\n\r\r(Ci(Dm) − 0.5)i∈[1,N]\n\r\r\n2\n10: Dnm ← arg min\nDnm∈Dnm\nremain\n\r\r(Ci(Dnm) − 0.5)i∈[1,N]\n\r\r\n2\n11: DNob\ntrain ← DNob\ntrain ∪ {Dm}\n12: Dremain\ntrain ← Dremain\ntrain \\{Dm}\n13: DNob\nnm ← DNob\nnm ∪ {Dnm}\n14: Dremain\nnm ← Dremain\nnm \\{Dnm}\n15: end for\nAlgorithm overview. In Algorithm 2, we consider a vector ofN classifiers\n(Ci)i∈[1,N], each of which, once trained, assigns a probability in the range[0, 1]\nto indicate the likelihood of a document being a member. The closer to1 (re-\nspectively,0), the more confident the classifier is that the document is a member\n8 C. Eichler et al.\n(resp., non-member). The intuition behind our algorithm is to exploit the con-\nfidence to ensures that the constructed datasets are as challenging as possible\nfor the classifiers. It is worth noting that other variants of this algorithm have\nbeen implemented, balancing the number of false positives, false negatives, true\npositives, and true negatives in each member/non-member class. The algorithm\noperates in two main steps:\n– Sampling and training:The algorithm begins by randomly sampling known\nmembers and non-members from the dataset (lines 1-2). These samples are\nthen used to train a set ofN agnostic classifiers(Ci)i∈[1,N] (line 3).\n– Confidence minimization: Using the classifiers (Ci)i∈[1,N] on the left-out\nmembers, we then construct DNob\ntrain and DNob\nnm (lines 8 to 14) minimizing\nthe overall confidence of the classifiers. Since the furtherCi(D) is from0.5,\nthe more confidentCi is in its assessment of documentD, we consider the\nvector (Ci(D) − 0.5)i∈[1,N] representing the confidence of each classifiers.\nAt each step, we add the elementD that minimizes the l2-norm of this\nconfidence vector. For instance, considering the construction of considered\nmembers: (1) among the members that have neither been selected as “Non-\nbiased” (Nob) nor used to train the classifiers (D ∈ Dremain\ntrain ), the one that\nminimizes the l2-norm of the confidence vector (i.e.,\n\r\r(Ci(x) − 0.5)i∈[1,N]\n\r\r\n2)\nis selected (line 9); (2) the aforementioned element is inserted in the set (line\n11); (3) the set of remaining candidates is updated (line 12).\n5 Experimental Validation of Our Approach\nIn this section, we apply and evaluate our proposal with reference to the Guten-\nberg dataset. Experimental settings are described in Sec. 5.1, detailing how the\ncandidate datasets are constructed to avoid a priori bias, how bias are assessed,\nas well as the MIAs and LLMs assessed on the datasets produced following our\nproposal. In spite of known members and non-members being constructed to\ncircumvent bias, Sec. 5.2 shows that random samples exhibit n-gram bias. Such\nbias are addressed in Sec 5.3 by producing datasets following theNo − Ngram\nalgorithm (Alg. 1), which still exhibit residual bias exploitable by an agnos-\ntic classifier. Section 5.4 assesses the last pair of datasets produced following the\nNo −Class algorithm (Alg. 2). Finally, Sec. 5.5 presents the assessment of MIAs\nusing the produced datasets and discusses the impact of bias in the evaluation\nof MIAs. All the code, resulting analysis and dataset are available online9.\n5.1 Experimental Setting\nDataset: Gutenberg Project.The project Gutenberg10 offers a high-quality\nopen dataset of over 70,000 books, continuously expanding. PG-19 [23], a subset\nof 28,752 books extracted in 2019, has been included in RedPajama-Data [5] and\n9 https://github.com/ceichler/MIA-bias-removal\n10 https://www.gutenberg.org/\nNob-MIAs: Non-biased MIAs Assessment on LLMs with Ex-Post Datasets 9\nThe Pile [10] and used to train LLMs such as Pythia [1] and OpenLLaMA [11].\nIt is also widely used to evaluate MIAs (e.g., [19], [18]). We use documents from\nProjectGutenberginourexperimentsbecauseofitsquality,recognizedrelevance\nin MIA research and the availability of methods [19] to minimize bias.\nWe assume an LLM trained on PG-19 [23] and draw our members from\nthis dataset. Regardingnon-members, since Project Gutenberg is ongoing, with\nbooks continuously added, all English books added after the publication of\nPG-19 are potential non-members. To circumvent the potential for temporal bias\nbetween the member and non-member sample, we adhere to the methodology\nproposed by Meeus et al. [19] and restrict our analysis to books published\nbetween 1850 and 1910. This leads to final setsDknown\ntrain and Dknown\nnm of 7300 and\n2400 books, respectively. Note also that Das et al. [6] identified a potential bias\nin datasets constructed following this methodology. Indeed, they showed that\nthe format of the preface metadata that project Gutenberg adds to books has\nchanged since 2019. To circumvent this, we discard such metadata. Therefore,\nour starting sets of members Dknown\ntrain and non-members Dknown\nnm are chosen\nbecause, a priori, there is no (known) bias affecting them.\nBias assessment. The agnostic classifiers employed in this study utilize a\nBayes algorithm for multinomially distributed data, focusing on the distribution\nof 1 to 3-grams. These classifiers are trained and applied using scikit-learn,\nchosen for its robustness in handling such data distributions. For then-gram\nanalysis, characters are treated as tokens when computing n-grams. We focus on\nn=7, as previous research [7] has shown that 7-grams reveal the most significant\ndistributional differences. The n-gram analysis is conducted using a Bloom\nfilter based on the implementation of [12], ensuring efficient and accurate bias\ndetection.\nLLMs. We conduct our experiments on two autoregressive large language\nmodels: OpenLLaMA [11] and Pythia [1]. OpenLLaMA is a series of 3B, 7B\nand 13B open-source models trained on 1T tokens that aims to emulate Meta’s\nLLaMA [28]. OpenLLaMA is trained on RedPajama-Data [5], an open-source\nreproduction of the original LLaMA training dataset. Pythia is an open and\ntransparent suite of LLMs ranging in size from 70M to 12B parameters that\nhas been specifically released to enable research. The language models in\nPythia have been trained on The Pile [10]. In this work, we have used the\nOpenLLaMA-3B and Pythia-2.8B models. Both The Pile and RedPajama-Data\ninclude PG-19 [23].\nMIAs. We conduct our experiments adapting the codes provided by [18] with\nthe following state-of-the-art MIAs:\n– Min-k% Probis based on the likelihood of thek% of tokens in a sequence\nD that have the lowest probabilities, based on the preceding tokens [25].\n10 C. Eichler et al.\n– Max-k% Probis the inverse metric of Min-k% Prob, based on the tokens\nthat have the highest probabilities. We usek = 10 for both Min-k% Prob\nand Max-k% Prob.\n– zlib Ratio identifies potential member when having a low ratio of the\nmodel’s perplexity to the entropy of the text [2]. This entropy is calculated\nas the number of bits required to compress the sequence using [8].\n– Perplexity (ppl)leverages perplexity [2] as scores and then threshold them\nto classify samples as members or non-members.\n– Meta_MIA is based on the work of [18], which aggregates 52 MIAs (in-\ncluding Min-k% Prob, perplexity, zlib Ratio, etc.) to create a single feature\nvector. A linear regressor is trained to learn the importance of weights for\nthe different MIA attacks and thus classify their membership status.\n5.2 Assuming No bias: Random Sample\nBy construction, Dknown\ntrain and Dknown\nnm are exempt of bias related to meta-data\nand time-shift. Since there is no reason to suspect a bias, we constructDNob\ntrain\nand DNob\nnm through a random sample. We compute the distributions of n-gram\noverlap of these two sets with reference to the left out members (Dknown\ntrain \\DNob\ntrain)\nas described in Sec. 4.2. The result is depicted as histograms in Fig. 1.\n(a) Randomly selected members.\n (b) Randomly selected non-members.\nFig.1: Histograms of n-gram overlap w.r.t. left out members (KS-dist = 0.222).\nSurprisingly, in-spite of the absence of time-shift and metadata bias, the set\nof members and non-members exhibit significant distributional difference of n-\ngram overlap, with a KS distance of 0.222. This bias can be exploited by an\nagnostic classifier achieving an AUC ROC of 0.84 (reported hereafter).\nConclusion. The text of books written in the same time interval but added to\nprojectGutenbergatdifferentdates(beforeoraftertheextractionofPG-19)still\nexhibit n-gram shifts and a random sampling produce heavily biased datasets.\nNob-MIAs: Non-biased MIAs Assessment on LLMs with Ex-Post Datasets 11\n(a) No − Ngram members.\n (b) No − Ngram non-members.\nFig.2: Histograms of n-gram overlap w.r.t. left out members (KS-dist = 0.034).\n(a) Trained onNo − Ngram sets.\n (b) Trained on randomly sampled set.\nFig.3: ROC of 5-folds agnostic classifiers.\n5.3 No − Ngram to Minimize N-gram Bias\nTo address the highlighted n-gram overlap bias, we produce new samplesDNob\ntrain\nand DNob\nnm following theNo −Ngram algorithm. The corresponding distributions\nof n-gram overlap are depicted in Fig. 2. Their KS distance is 0.034, a drastic\n84% drop from 0.222, the distance achieved with random samples. Notably, non-\nmembers exhibit high n-gram overlap with the left out members, only 5 non-\nmember books having an overlap score lesser than 0.8.\nWe further assess residual bias by training a classifier onDNob\ntrain and DNob\nnm .\nThe ROC of each fold is illustrated in Fig. 3a. As a reference, the evaluation\nof a classifier trained on randomly sampled datasets is depicted in Fig. 3b. The\nagnostic classifier achieves on average over 5 folds 0.82 AUC ROC and 5%,\n26%, and 49%TPR at 1%, 5%, and 10% FPR, respectively. This remains highly\naccurate and the accuracy loss is marginal when compared to random samples\nwhere an agnostic classifier achieves 0.84 AUC ROC and 3%, 30%, and 63%\nTPR at 1%, 5%, and 10% FPR, respectively.\nConclusion. While theNo − Ngram algorithm has successfully reduced (if not\neliminated) the bias in n-gram overlap, the produced sets can still be discrimi-\n12 C. Eichler et al.\nnated with high accuracy by an agnostic classifier. Contrarily to previous pro-\nposal [7], this suggests that distributional difference in n-gram overlap is insuf-\nficient as a metric of MIA benchmark difficulty.\n5.4 Sampling Unclassifiable Datasets\nThe datasets being classifiable even when n-gram bias are minimized, we apply\nNo − Class11 using a single classifier trained on randomly sampled set whose\nevaluation is presented in Fig. 3b. To evaluate residual bias, we train a new\nagnostic classifier on the resulting sets whose evaluation is shown in Fig. 4.\nFig.4: ROC of 5-folds agnostic classifier trained onNo − Class sets.\nOnaverage,theagnosticclassifierachieves6%,13%,and20%TPRat1%,5%,\nand 10% FPR, respectively. Interestingly, the 5th fold achieves lower TPR than\na random guess for FPR in the 30-45% interval. Overall, performance is slightly\nbetter than random, particularly at low FPR, but significantly worse than pre-\nvious settings. Indeed, the TPR at 5% and 10% FPR decrease by roughly 56%\nand 72% when compared to a classifier trained on random samples, respectively.\nSimilarly, the AUC ROC drops from 0.84 to 0.58, denoting a 76% decrease of\nthe distance to the AUC ROC of a random guess.\nConclusion. These results indicate that hard-to-classify sets also resist training,\nshowing minimal exploitable bias for agnostic classifiers.\n5.5 Assesment of MIAs\nWe assess 6 state of the art MIAs on 2 LLMs and the datasets random,No −\nNgram , andNo −Class presented in Sec. 5.2, 5.3, and 5.4, respectively. Tables 1\nand 2 report their TPR at 10%FPR and AUC ROC averaged over 5 runs.\nOverall, no MIA manage to outperform an agnostic classifier on the ran-\ndom andNo − Ngram datasets. Only Meta-MIA outperforms the classifier on\n11 Since we use a single classifier, we also ensure the same number of false positive,\nfalse negative, true positive and true negative in the selected sets.\nNob-MIAs: Non-biased MIAs Assessment on LLMs with Ex-Post Datasets 13\nModel Dataset Meta_MIA Max10%Prob ppl zlib_ratio Max10%Prob\nPythia-2.8B random 0.314 0.085 0.087 0.111 0.062\nPythia-2.8B No − Ngram 0.350 0.149 0.050 0.145 0.124\nPythia-2.8B No − Class 0.314 0.313 0.127 0.140 0.162\nOpenLLaMA-3B random 0.371 0.111 0.059 0.139 0.122\nOpenLLaMA-3B No − Ngram 0.386 0.156 0.074 0.131 0.222\nOpenLLaMA-3B No − Class 0.224 0.070 0.046 0.132 0.081\nTable 1: TPR values at 10%FPR. Bold values outperform agnostic classifiers.\nModel Dataset Meta_MIA Min10%Prob ppl zlib_ratio Max10%Prob\nPythia-2.8B random 0.692 0.544 0.554 0.494 0.490\nPythia-2.8B No − Ngram 0.688 0.538 0.477 0.544 0.557\nPythia-2.8B No − Class 0.670 0.665 0.583 0.531 0.578\nOpenLLaMA-3B random 0.740 0.523 0.493 0.501 0.576\nOpenLLaMA-3B No − Ngram 0.744 0.543 0.545 0.506 0.642\nOpenLLaMA-3B No − Class 0.634 0.520 0.503 0.523 0.494\nTable 2: AUC ROC values. Bold values outperform an agnostic classifier.\nNo − Class on both OpenLLaMA-3B and Pythia-2.8B, while 10%_min_probs\noutperforms it solely onPythia-2.8B according to the TPR@10%FPR.\nMeta-MIA is consistently significantly above a random guess and the best\nMIA across all settings and metric. It achieves its best results onPythia-2.8B,\nwith 37.1% TPR@10%FPR and a AUC ROC of 0.74 for the random biased\ndataset. OnNo −Class, these values drop to 22.4% and 0.634, denoting a ratio\nNo − Class/random of 0.60 and 0.857.\nConclusion. Meta-MIA is consistently the best out of the 6 MIAs evaluated on\nour datasets. Yet, its TPR@10%FPR and AUC ROC drop by 40% and 14.3%\nrespectively when evaluated on datasets produced using our approach rather\nthan on randomly sampled ones. This underlines the importance of our approach\nto accurately estimate MIAs performances.\n6 Conclusion\nAs LLMs are trained leveraging myriads of data items, including copyrighted\nones, it is key to ascertain whether a piece of data has been used in this process.\nYet, the effectiveness of MIAs has been recently questioned, due to the existence\nof biases in datasets constructed ex-post. This work introducesNob − MIAs , a\nset of algorithms to build unbiased datasets, thus setting a more solid ground\nfor MIA assessment. Our experiments on the Gutenberg dataset confirms that\nour approach significantly reduces bias (e.g., an 84% reduction of difference in n-\ngram overlap distribution) and impacts on MIA evaluation, with TPR@10%FPR\nand ROC AUC of the best-performing MIA (out of 6) decreasing by 40% and\n14.3% respectively, compared to evaluations on randomly sampled datasets.\n14 C. Eichler et al.\nThis work opens several future research avenues, including extending the\nalgorithms to detect and mitigate residual biases and applying this approach to\nnon-textual MIAs, where ex-post dataset construction is also common.\nAcknowledgments. Thisworkwassupportedbythe“ANR22-PECY-0002” IPoP(In-\nterdisciplinary Project on Privacy) project of the Cybersecurity PEPR and DATAIA.\nJose Maria de Fuentes has also received support from the Spanish National Cyberse-\ncurity Institute (INCIBE) grant APAMciber within the framework of the Recovery,\nTransformation and Resilience Plan funds, financed by the European Union (Next\nGeneration); and from UC3M’s Requalification programme, funded by the Spanish\nMinisterio de Ciencia, Innovacion y Universidades with EU recovery funds (Convo-\ncatoria de la Universidad Carlos III de Madrid de Ayudas para la recualificación del\nsistema universitario español para 2021-2023, de 1 de julio de 2021).\nReferences\n1. Biderman, S., Schoelkopf, H., Anthony, Q., Bradley, H., O’Brien, K., Hallahan, E.,\nKhan, M.A., Purohit, S., Prashanth, U.S., Raff, E., Skowron, A., Sutawika, L., Van\nDerWal,O.:Pythia:asuiteforanalyzinglargelanguagemodelsacrosstrainingand\nscaling. In: Proceedings of the 40th International Conference on Machine Learning.\nICML’23, JMLR.org (2023)\n2. Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K.,\nRoberts, A., Brown, T., Song, D., Erlingsson, U., et al.: Extracting training data\nfrom large language models. In: 30th USENIX Security Symposium (USENIX Se-\ncurity 21). pp. 2633–2650 (2021)\n3. Chang, K.K., Cramer, M., Soni, S., Bamman, D.: Speak, memory: An archaeology\nof books known to chatgpt/gpt-4. arXiv preprint arXiv:2305.00118 (2023)\n4. Cheng, J., Marone, M., Weller, O., Lawrie, D., Khashabi, D., Van Durme, B.:\nDated data: Tracing knowledge cutoffs in large language models. arXiv preprint\narXiv:2403.12958 (2024)\n5. Computer, T.: Redpajama-data: An open source recipe to reproduce llama training\ndataset (2023),https://github.com/togethercomputer/RedPajama-Data\n6. Das, D., Zhang, J., Tramèr, F.: Blind baselines beat membership inference attacks\nfor foundation models. arXiv preprint arXiv:2406.16201 (2024)\n7. Duan, M., Suri, A., Mireshghallah, N., Min, S., Shi, W., Zettlemoyer, L., Tsvetkov,\nY., Choi, Y., Evans, D., Hajishirzi, H.: Do membership inference attacks work on\nlarge language models? arXiv preprint arXiv:2402.07841 (2024)\n8. Gailly, J.l., Adler, M.: Zlib compression library (2004)\n9. Galli, F., Melis, L., Cucinotta, T.: Noisy neighbors: Efficient membership inference\nattacks against llms. arXiv preprint arXiv:2406.16565 (2024)\n10. Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He,\nH., Thite, A., Nabeshima, N., Presser, S., Leahy, C.: The pile: An 800gb dataset\nof diverse text for language modeling (2020)\n11. Geng, X., Liu, H.: Openllama: An open reproduction of llama (May 2023),https:\n//github.com/openlm-research/open_llama\n12. Groeneveld, D., Ha, C., Magnusson, I.: Bff: The big friendly filter (2023),https:\n//github.com/allenai/bff\n13. Jedrzejewski, F.V., Thode, L., Fischbach, J., Gorschek, T., Mendez, D., Laves-\nson, N.: Adversarial machine learning in industry: A systematic literature review.\nComputers & Security p. 103988 (2024)\nNob-MIAs: Non-biased MIAs Assessment on LLMs with Ex-Post Datasets 15\n14. Kaneko, M., Ma, Y., Wata, Y., Okazaki, N.: Sampling-based pseudo-likelihood for\nmembership inference attacks. arXiv preprint arXiv:2404.11262 (2024)\n15. Li, H., Deng, G., Liu, Y., Wang, K., Li, Y., Zhang, T., Liu, Y., Xu, G., Xu, G.,\nWang, H.: Digger: Detecting copyright content mis-usage in large language model\ntraining. arXiv preprint arXiv:2401.00676 (2024)\n16. Lin, C.Y.: Rouge: A package for automatic evaluation of summaries. In: Text sum-\nmarization branches out. pp. 74–81 (2004)\n17. Liu, X., Sun, T., Xu, T., Wu, F., Wang, C., Wang, X., Gao, J.: Shield: Evalua-\ntion and defense strategies for copyright compliance in llm text generation. arXiv\npreprint arXiv:2406.12975 (2024)\n18. Maini, P., Jia, H., Papernot, N., Dziedzic, A.: Llm dataset inference: Did you train\non my dataset? arXiv preprint arXiv:2406.06443 (2024)\n19. Meeus, M., Jain, S., Rei, M., de Montjoye, Y.: Did the neurons read your book?\ndocument-level membership inference for large language models. In: Balzarotti,\nD., Xu, W. (eds.) 33rd USENIX Security Symposium, USENIX Security 2024,\nPhiladelphia, PA, USA, August 14-16, 2024. USENIX Association (2024)\n20. Meeus, M., Jain, S., Rei, M., de Montjoye, Y.A.: Inherent challenges of post-hoc\nmembership inference for large language models. arXiv preprint arXiv:2406.17975\n(2024)\n21. Meeus, M., Shilov, I., Faysse, M., de Montjoye, Y.A.: Copyright traps for large\nlanguage models. In: 41st International Conference on Machine Learning (2024)\n22. Panaitescu-Liess, M.A., Che, Z., An, B., Xu, Y., Pathmanathan, P., Chakraborty,\nS., Zhu, S., Goldstein, T., Huang, F.: Can watermarking large language mod-\nels prevent copyrighted text generation and hide training data? arXiv preprint\narXiv:2407.17417 (2024)\n23. Rae, J.W., Potapenko, A., Jayakumar, S.M., Lillicrap, T.P.: Compressive trans-\nformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507 (2019)\n24. Reuel, A., Bucknall, B., Casper, S., Fist, T., Soder, L., Aarne, O., Hammond, L.,\nIbrahim, L., Chan, A., Wills, P., et al.: Open problems in technical ai governance.\narXiv preprint arXiv:2407.14981 (2024)\n25. Shi, W., Ajith, A., Xia, M., Huang, Y., Liu, D., Blevins, T., Chen, D., Zettle-\nmoyer, L.: Detecting pretraining data from large language models. In: The Twelfth\nInternational Conference on Learning Representations (2024)\n26. Shokri, R., Stronati, M., Song, C., Shmatikov, V.: Membership inference attacks\nagainstmachinelearningmodels.In:2017IEEEsymposiumonsecurityandprivacy\n(SP). pp. 3–18. IEEE (2017)\n27. Sonkar, S., Baraniuk, R.G.: Many-shot regurgitation (msr) prompting. arXiv\npreprint arXiv:2405.08134 (2024)\n28. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,\nRozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient\nfoundation language models. arXiv preprint arXiv:2302.13971 (2023)\n29. Wei, J.T.Z., Wang, R.Y., Jia, R.: Proving membership in llm pretraining data via\ndata watermarks. arXiv preprint arXiv:2402.10892 (2024)\n30. Yan, B., Li, K., Xu, M., Dong, Y., Zhang, Y., Ren, Z., Cheng, X.: On protect-\ning the data privacy of large language models (llms): A survey. arXiv preprint\narXiv:2403.05156 (2024)\n31. Yeom, S., Giacomelli, I., Fredrikson, M., Jha, S.: Privacy risk in machine learning:\nAnalyzing the connection to overfitting. In: 2018 IEEE 31st computer security\nfoundations symposium (CSF). pp. 268–282. IEEE (2018)"
}