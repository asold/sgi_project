{
  "title": "Tabi: An Efficient Multi-Level Inference System for Large Language Models",
  "url": "https://openalex.org/W4388874804",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A30152912",
      "name": "Wang YiDing",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1809448432",
      "name": "Chen Kai",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2209678436",
      "name": "Tan, Haisheng",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2118883104",
      "name": "Guo Kun",
      "affiliations": [
        "Fuzhou University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3130689885",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2739542029",
    "https://openalex.org/W4212883601",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2565600385",
    "https://openalex.org/W3021636956",
    "https://openalex.org/W3104939451",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3046754651",
    "https://openalex.org/W3130716829",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W3007007518",
    "https://openalex.org/W3034742519",
    "https://openalex.org/W3033527233",
    "https://openalex.org/W6739651123",
    "https://openalex.org/W4282577879",
    "https://openalex.org/W6771626834",
    "https://openalex.org/W2128073546",
    "https://openalex.org/W2810469995",
    "https://openalex.org/W2971118045",
    "https://openalex.org/W2986193249",
    "https://openalex.org/W2896958211",
    "https://openalex.org/W2606492274",
    "https://openalex.org/W6639048329",
    "https://openalex.org/W3181029726",
    "https://openalex.org/W6762945437",
    "https://openalex.org/W3196976833",
    "https://openalex.org/W3164703200",
    "https://openalex.org/W3168124404",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W6798686915",
    "https://openalex.org/W6777017071",
    "https://openalex.org/W3214897310",
    "https://openalex.org/W2982157693",
    "https://openalex.org/W2963516811",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W2962677625",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3159727696",
    "https://openalex.org/W3153004963",
    "https://openalex.org/W4225729912",
    "https://openalex.org/W2951333013",
    "https://openalex.org/W3035038672",
    "https://openalex.org/W2952357537",
    "https://openalex.org/W4220738176",
    "https://openalex.org/W6781533629",
    "https://openalex.org/W3172942063",
    "https://openalex.org/W2956461999",
    "https://openalex.org/W4290991649",
    "https://openalex.org/W3165698711",
    "https://openalex.org/W3033737024",
    "https://openalex.org/W95608104",
    "https://openalex.org/W4288347855",
    "https://openalex.org/W1850527962",
    "https://openalex.org/W3102725307",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W4287777801",
    "https://openalex.org/W3101163004",
    "https://openalex.org/W2750779823",
    "https://openalex.org/W2734941459"
  ],
  "abstract": "Today's trend of building ever larger language models (LLMs), while pushing the performance of natural language processing, adds significant latency to the inference stage. We observe that due to the diminishing returns of adding parameters to LLMs, a smaller model could make the same prediction as a costly LLM for a majority of queries. Based on this observation, we design Tabi, an inference system with a multi-level inference engine that serves queries using small models and optional LLMs for demanding applications. Tabi is optimized for discriminative models (i.e., not generative LLMs) in a serving framework. Tabi uses the calibrated confidence score to decide whether to return the accurate results of small models extremely fast or re-route them to LLMs. For re-routed queries, it uses attention-based word pruning and weighted ensemble techniques to offset the system overhead and accuracy loss. We implement and evaluate Tabi with multiple tasks and models. Our result shows that Tabi achieves 21%-40% average latency reduction (with comparable tail latency) over the state-of-the-art while meeting LLM-grade high accuracy targets.",
  "full_text": "Tabi: An Efficient Multi-Level Inference System for Large Language Models\nWang, Yiding; Chen, Kai; Tan, Haisheng; Guo, Kun\nEuroSys '23: Proceedings of the Eighteenth European Conference on Computer \nSystems, / Association for Computing Machinery. New York, NY : Association for \nComputing Machinery, 2023, p. 233-248\nAccepted Version\n10.1145/3552326.3587438\nAssociation for Computing Machinery\nÂ© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nEuroSys '23: Eighteenth European Conference on Computer Systems, Rome, Italy, \n8-12 May 2023\nTabi: An Efficient Multi-Level Inference System for\nLarge Language Models\nYiding Wang1, Kai Chen1, Haisheng Tan2, Kun Guo3\n1iSING Lab, Hong Kong University of Science and Technology\n2University of Science and Technology of China 3Fuzhou University\nAbstract\nTodayâ€™s trend of building ever larger language models (LLMs),\nwhile pushing the performance of natural language pro-\ncessing, adds significant latency to the inference stage. We\nobserve that due to the diminishing returns of adding pa-\nrameters to LLMs, a smaller model could make the same\nprediction as a costly LLM for a majority of queries. Based\non this observation, we design Tabi, an inference system\nwith a multi-level inference engine that serves queries using\nsmall models and optional LLMs for demanding applications.\nTabi is optimized for discriminative models (i.e., not genera-\ntive LLMs) in a serving framework. Tabi uses the calibrated\nconfidence score to decide whether to return the accurate\nresults of small models extremely fast or re-route them to\nLLMs. For re-routed queries, it uses attention-based word\npruning and weighted ensemble techniques to offset the sys-\ntem overhead and accuracy loss. We implement and evaluate\nTabi with multiple tasks and models. Our result shows that\nTabi achieves 21%-40% average latency reduction (with com-\nparable tail latency) over the state-of-the-art while meeting\nLLM-grade high accuracy targets.\n1 Introduction\nTransformer-based language models, such as BERT [ 18],\nhave achieved strong performance in various natural lan-\nguage processing (NLP) tasks. One key to their advancement\nis the sheer volume of parameters and we are seeing a trend\nof building ever larger language models (LLMs) to push the\nlimit of NLP performance [ 9, 34]. LLMs are computation-\nally intensive, resulting in high inference latency [ 80, 89],\nwhereas low latency is crucial to the user experience of NLP\napplications.\nGiven the high computational demands of LLMs, it is\ncommon to use the cloud infrastructure to deliver their\nbreakthroughs [71]. Machine learning (ML) inference sys-\ntems [15, 31, 57] serve incoming queries of various appli-\ncations with trained deep neural networks (DNNs) on the\nserver. They select appropriate models for the user to meet\ntasksâ€™ accuracy and latency requirements so users do not\nneed to, letting users deploy trained models efficiently with-\nout mastering ML or system optimizations (i.e., model-less\ninference [57]).\nHowever, the current inference systems mostly overlook\nthe concerning status quo of NLP:LLMs trade a huge amount\nof DNN capacity for top-grade accuracy, while the return di-\nminishes [11]. Studies find that the pace of model growth\nfar exceeds the resulting increase in model performance and\nmore resources are required to improve language models\nby making them larger [61]. Model compression [28, 59, 75]\nhas attracted considerable research attention, but the con-\nsequent accuracy loss prevents compressed models from\nserving accuracy-demanding tasks. Adaptive inference tech-\nniques (e.g., early-exit[80, 89] and token pruning [ 29, 74])\nrequire special DNN designing and training, thus are hard\nto fit in with todayâ€™s model-less inference systems.\nWhen serving accuracy-demanding applications that do\nrequire LLMs, the current inference systems select the same\nmodels to serve all queries of a task homogeneously [31, 57].\nThey ignore the variance of difficulty between individual\ndata samples within an NLP application [10]. This is because\nthere is no easy way to set per-query requirements in ad-\nvance. Essentially, inference systems treat each DNN query\nas a black-box1 without understanding its specific needs and\nfocus more on resource scheduling instead.\nWe argue that optimizing for all queries homogeneously\nwhen serving NLP applications is sub-optimal because the\nLLMs selected could be a huge overkill for most less demand-\ning data samples. For example, we find that to serve a text\nclassification task with a strict accuracy target, INFaaS [57]\nneeds to select a highly accurate but heavy LLM. Meanwhile,\na faster model with only 1/4 of parameters can correctly\nfulfill over 90% of the queries. Yet it will not get selected\nfor its below-target accuracy. Such huge resource overheads\nfollowing todayâ€™s trend of developing larger models moti-\nvate us to revisit the coarse-grained model selection when\nserving LLMs and optimize single data inference.\nWe propose Tabi, an inference system featuring a novel\nmulti-level inference engine driven by individual per-query\nfeedback and employing recent advances of ML to optimize\nthe inference latency of LLMs for discriminative tasks. Unlike\ntodayâ€™s inference systems that serve the whole application\nwith the same models, Tabi incorporates multiple DNNs each\noptimized for efficiency or accuracy to handle heterogeneous\nqueries within a task and avoid invoking the costly LLMs\nwhen smaller models can suffice.\n1The previous white-box DNN inference engine [45] focuses on a different\naspect of reusing low-level operations to improve computational efficiency.\n1\nEuroSys â€™23, May 08â€“12, 2023, Rome, Italy Wang et al.\nIn Tabi, each query will first go through a level of effi-\ncient DNN, obtaining predictions and a confidence score\nwell-calibrated with temperature scaling [32] that accurately\nindicates the inference quality. We use a probability-based\ndispatcher to decide a queryâ€™s DNN capacity: For a majority\nof highly confident predictions, we directly return the re-\nsults and achieve extremely low inference latency; if not sure,\nTabi re-routes those challenging queries to the next level of\na more advanced model for high-quality inference (Â§4.1).\nTabi employs ML techniques and reuses results of the\nsmall model to optimize the system overhead of occasionally\nrunning extra DNNs. For re-routed queries, we directly prune\nthe words that contribute little to the task from the input data\nusing the previous DNNâ€™s attention weights [69], offsetting\nthe small modelâ€™s overhead by reducing the LLMâ€™s latency\ninstead (Â§4.2). Attention weights are intermediate variables\nof language models. An encoded token is semantically im-\nportant if it receives a lot of attention from other tokens [79].\nSince natural language is highly redundant (due to less mean-\ningful elements such as prepositions and punctuation marks),\nremoving low-attention words can accelerate inference while\nkeeping accuracy [29, 74]. When outputting the results, Tabi\nfurther improves the LLMâ€™s accuracy by combining existing\npredictions using weighted ensemble without requiring ex-\ntra DNN computations (Â§4.3). Through designs inspired by\nrecent advances of ML/NLP, Tabi achieves fine-grained im-\nprovements over state-of-the-art (SOTA) inference systems.\nWe abstract a set of Tabi model levels and configurations\nas a model candidate , which is logically equivalent to single\nmodels and compatible with inference scaling systems (e.g.,\nINFaaS). Meanwhile, we focus on single data inference (i.e.,\neach query runs separately), which is fundamental to live\ndata analytics and can benefit complex workloads [73]. To\nnavigate the large optimization space brought about by the\ngeneral system architecture, Tabi conducts efficient offline\nprofiling to generate differently optimized candidates and\nselects the optimal candidate for incoming tasks (Â§5).\nWe evaluate Tabi using multiple NLP benchmarks and\npopular language models. Tabi achieves a 21%-40% reduc-\ntion on average latency and good tail performance compared\nto SOTA inference systems while meeting the LLM-grade\ndemanding accuracy targets. Tabi can work together with\nML-optimized models (e.g., via compression) by using them\nin candidates and can switch to them for relaxed accuracy\ntargets. Compared to ML research on adaptive inference\n(e.g., early-exit[80, 89] and token pruning [29, 74]), Tabi can\naccelerate various generic language models for model-less\ninference and achieve even better performance without re-\nquiring customization and extra training for every model.\nTo summarize, the contributions of Tabi include: (1) propos-\ning the first inference system for the resource overhead issue\namong increasingly large language models; (2) designing an\nefficient multi-level system empowered by ML advancements\n50 60 70 80 90 95\nCorrect prediction (%)\n0\n1\n2\n3Number of parameters\n1e8\noverheads\nwrong prediction\nBERT-small\nDistilBERT\nBERT-base\nRoBERT a-base\nRoBERT a-large\n(a) Resource overheads of LLMs.\nBERT-small\nDistilBERT\nBERT-base\nRoBERTa-base\nRoBERTa-large\nQuery (b) An imaginary MLFQ.\nFigure 1. Each color-filled barâ€™s right edge shows its accu-\nracy. A barâ€™s width shows the accuracy improvement over\nthe previous smaller DNNs, i.e., the percentage of queries\nthat can be correctly served by a model but not by the ones\non its left. The height shows the model size. The gray area\nis the resource overheads compared to an ideal scenario.\nthat achieves significant latency reduction while meeting de-\nmanding accuracy targets; and (3) optimizing for generality\nacross models without requiring customization.\n2 Background and Motivation\n2.1 Resource Overhead of LLMs\nWe are in the paradigm of large language models (LLMs)\ntoday. BERT [18], published in 2018, impressively improved\nthe GLUE (General Language Understanding Evaluation)\nbenchmark consisting of 9 tasks [72] from no higher than 69\nout of 100 to 80. Since then, masked language modeling [18]\nproposed by BERT and other self-supervised pre-training\nobjectives enable the utilization of large-scale datasets con-\nsisting of billions of words.\nLearning from massive amounts of data to substantially\nunderstand the language requires large-scale models. For ex-\nample, BERT-large with 340 million parameters was one of\nthe largest DNNs when proposed. Switch Transformer [24]\nuses more than a trillion parameters to achieve SOTA re-\nsults on several cognitive tasks. In recent years, LLMs with\nthe attention mechanism [69] and similar architectures as\nBERT have passed the human baseline and pushed the GLUE\nleaderboard above 90.\nOn the other hand, achieving SOTA performance requires\nmore and more computational resources while the return is\ndiminishing [61]. For example, a SOTA Transformer model\nspends 50% of its training footprint solely to achieve a de-\ncrease of 0.3 of the error rate [53]; the recent DeBERTa [35]\nuses 36Ã—more parameters than ALBERT [43] but only im-\nproves the accuracy by 6%. GPT-3 [13], which has 175 billion\nparameters, 100Ã—of its predecessor, suggests that the current\nLLMs â€œmay eventually run into the limits of the pre-training\nobjectiveâ€.\n2\nTabi: An Efficient Multi-Level Inference System for Large Language Models EuroSys â€™23, May 08â€“12, 2023, Rome, Italy\nFigure 1a shows the accuracies and sizes of five DNNs from\nsmall to large for text classification. We can see that the ac-\ncuracy improvements over previous models (widths of bars)\nbecome less noticeable even when adding more parameters\n(height differences of bars). Our finding echoes sustainable\nML research that alarms the vast carbon footprint of training\nand deploying LLMs [61, 65]. Research also suggests that for\nless demanding tasks, especially on edge devices, such large\nDNNs are not necessary when much smaller models might\nsuffice [27, 70].\nThe NLP status quo of developing larger models will cause\nhuge resource overheads when delivering their SOTA perfor-\nmance using the current DNN inference systems. To under-\nstand the issue, let us look at how they serve a text classifi-\ncation task (SST-2 [72]) with a target accuracy of 95% in Fig-\nure 1a. To meet the demanding target (vertical dashed line),\nthe inference system chooses RoBERTa-large from available\ntrained models, which has an accuracy of 96.3% and 355 mil-\nlion parameters. The workloadâ€™s resource usage is generally\nequivalent to the area of the whole figure, serving all queries\n(width) with the selected DNNâ€™s capacity (height).\nHowever, when analyzing the resultspost hoc, we find that\nother more efficient models can make the exact prediction as\nRoBERTa-large does for over 90% of queries (e.g., DistilBERT,\nthe second smallest model in Figure 1a, has an accuracy of\n91%), although none of them meet the accuracy target to get\nselected (see how model selection works in Â§2.2).\nIdeally speaking, if a query can be served correctly by a\nsmall model, we should not invoke larger ones. 2 This sce-\nnario is similar to going through a multi-level feedback queue\n(MLFQ) of models from small to large as Figure 1b while\nknowing the shortcut (the dashed arrows) to the right level.\nThus, the optimal resource usage of a workload is the lower\nright area of all bars in Figure 1a, which is only 11% of the\nactual usage! We define easy queries as those obtaining the\nsame results no matter served with small or large DNNs.\nWhile the left gray area is the overhead of unnecessarily\nserving easy queries with over-powerful LLMs.\nIn reality, we do not know a queryâ€™s minimum require-\nments beforehand, and thus there is no shortcut to the most\nefficient model; besides, going through multiple models adds\nextra latency. This is why the current inference systems\nmatch performance targets at an application level and fo-\ncus on resource/model scaling instead. Nevertheless, the\nremarkable potential of latency reduction, cost saving, and\nenvironmental impact motivates us to optimize the inference\nof each single query for LLMs in a model-less system.\n2.2 DNN Inference Systems\nRecent inference systems [31, 57] let users only specify high-\nlevel performance requirements (e.g., accuracy and latency\n2As the Occamâ€™s razor principle goes,entities should not be multiplied beyond\nnecessity.\nfor a sentiment analysis task) rather than specific models and\ndeploy trained models without mastering ML or system op-\ntimizations, termed as model-less inference. The system first\nselects appropriate resources (e.g., GPU or CPU) and DNNs\n(e.g., optimized for accuracy or efficiency) from registered\nmodels. Then it routes queries to the selected models, runs\nDNN inference, and returns the results. This paper focuses\non model selection and single data inference. In addition,\ninference systems can also scale the resources and models\nto adapt to workload changes.\nThe current inference systems make various optimizations.\nINFaaS [57] selects the best single models automatically opti-\nmized for different devices and batch sizes for inference and\nquickly adapts to workload changes. Cocktail [31] employs\nensemble learning with multiple small models to reduce la-\ntency through parallel execution; it also dynamically adjusts\nmodel ensembles to minimize cost.\nSOTA inference systems set performance targets for an\napplication and select models to serve the whole workload\nhomogeneously without inspecting whether the selected mod-\nels are overkill for individual queries. Although Cocktail [31]\nhas a monitoring process using ground-truth labels which\nare rare for real-world tasks, it cannot achieve fine-grained\nmodel selection. This one-model-fits-all design is not a con-\ncerning problem for many tasks when the DNN performance\nimproves notably as the model grows larger (i.e., less per-\nquery overheads), which is not the case for LLMs.\nWe observe an apparent variance of needs for DNN ca-\npacity within an NLP application: The natural workload is\na mix of difficult and easy samples and many easy samples\ncan be served correctly by a much smaller language model.\nML research has been exploring the variance in training and\ninference settings: Hard sample mining [37, 63] spends more\ntraining resources on complex data, and adaptive predic-\ntion [10, 68, 80] develops specialized DNNs that only run\nearly layers for easy data. Tabi tackles the overhead issue of\nserving LLMs from a model-less inference perspective.\n2.3 Pay Attention to Transformers\nTabi focuses on serving discriminative LLMs including BERT\nand 25 of the 30 most downloaded models [2]. Such models\nsummarize the inputs, learn unique proprieties, and make\npredictions, which are different from generative models that\ngenerate new tokens like GPT-3 [13] and machine transla-\ntion [69]. Such pre-trained language models power various\nNLP tasks that feed on encoded text representation, includ-\ning non-Latin [16], programming code [25], and clinical [36]\nlanguages, such as sentiment analysis [48], natural language\ninference [72], question answering [ 54], and applications\nincluding review understanding [ 81] and content moder-\nation [58]. Tabi does not optimize for generative models\nwhose Transformer decoder architecture does not support\nour confidence score and word pruning design.\n3\nEuroSys â€™23, May 08â€“12, 2023, Rome, Italy Wang et al.\nToken Embedding\nFC for  Q, K, V\n QKT/ dk\nSoftmax\nAttention Weight â‹… V\nFFN\nSentiment Classiï¬er\nAttention\nâ€œIâ€ â€likeâ€ â€youâ€\nOutput: positive\nHÃ— heads\nNÃ— encoder \nblocks â€¦\nâ€¦\nResidual and Norm \nFigure 2. Illustration of the attention module in BERT, which\nis similar to those in other language models.\nLanguage model architecture. Discriminative language\nmodels consist of a series of Transformer encoder blocks . We\nfirst turn words into a numeric representation. Language\nmodels split an input sentence into sub-word tokens and\nmap them to numeric vectors before feeding them to the\nTransformer blocks. This step is calledtokenization. A Trans-\nformer block is a particular neural network: As shown in\nFigure 2, the input vector will go through a self-attention\nmodule [69] connected to a feed-forward network (FFN). The\ninputs of attention consist of Query (ğ‘„), Key (ğ¾), and Value\n(ğ‘‰) with the same dimension (ğ‘‘ğ‘˜), obtained by multiplying\nthe input vector by their weights, each split into multiple\nheads. ğ‘„ğ¾ğ‘‰ are transformed into the intermediate attention\noutput. Then, an FFN will apply to the attention output and\nproduce input for the next Transformer block. This process\nwill repeat as going through the hidden layers of LLMs.\nAttention mechanism. Language models build on the\nattention mechanism. Conceptually, attention encodes the\nrelationship between tokens, generates context-dependent\nembeddings, and lets models know the relevant tokens when\nprocessing a vector. First, we calculateğ‘„ğ¾ğ‘‡/âˆšğ‘‘ğ‘˜ to produce\nthe attention score, which shows how close two tokens are\nrelated. Intuitively, attention works like searching and re-\ntrieving in a database, which is why it uses scaled dot-product\nto measure the similarity between tokens represented by ğ‘„\nand ğ¾. Then we apply softmax to obtain the attention weight\nof a query to enlarge the score for highly-related token pairs.\nMultiplying the attention weights with ğ‘‰ gives the result of\none head. To summarize, the attention mechanism works as:\nğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘› (ğ‘„,ğ¾,ğ‘‰ )= ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ (ğ‘„ğ¾ğ‘‡\nâˆšğ‘‘ğ‘˜\n)ğ‘‰ (1)\nthispaper is beautifullywrittenandenjoyableto read\nthis\npaper\nis\nbeautifully\nwritten\nand\nenjoyable\nto\nread\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.78 0.79 0.73 0.99 0.79 0.63 1.5 0.52 0.73\nFigure 3. The attention weight matrix and the importance\nvector. Tokens with top-50% weights are inbold. They are\nmore important to the task. Special tokens are omitted.\nThe multi-headed mechanism concatenates the output of\neach attention head and lets the model focus on different\npositions in parallel [51].\nAttention weights. The attention weight ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ (ğ‘„ğ¾ğ‘‡\nâˆšğ‘‘ğ‘˜\n)\nnormalizes the â€œattentionâ€ paid from one token to all the\nother tokens in an input sequence to a probability distribu-\ntion that sums to 1 (rows in Figure 3). Thus summing up\nthe attention weights received from all tokens will produce\nan importance vector (bottom of Figure 3), in which more\nimpactful tokens that elicit more attention have higher weights .\nFor example, Figure 3 shows the attention weight matrix\nof an input sentence produced by a sentiment analysis BERT\nmodel. Tokens that express strong emotion (e.g., â€œbeautifullyâ€\nand â€œenjoyableâ€) have the highest weights in the importance\nvector, while adpositions (e.g., â€œandâ€ and â€œtoâ€) receive the\nleast attention. Nevertheless, all tokens, whether important\nor not, contribute to the computational complexity of atten-\ntion quadratically regarding the sequence length [69].\nML research finds that dropping unimportant vectors\nacross layers can reduce computation cost [29, 74] but they\nrequire specially designed and trained models. Tabi acceler-\nates generic LLMs without code modification or re-training\nby directly reducing the input data size (rather than per-layer\npruning) using the attention weights available in the system.\n3 Tabi Overview\nMotivated by the huge overheads of serving LLMs with to-\ndayâ€™s model-less inference systems and new optimization\nopportunities brought about by the attention mechanism,\n4\nTabi: An Efficient Multi-Level Inference System for Large Language Models EuroSys â€™23, May 08â€“12, 2023, Rome, Italy\nController\nWorker\nModel-1 \nprocess\nRepository\nModel store\nModel-2 \nprocess\nModel-n \nprocess\nApp \nqueries \nwith \ntargets\nReturned \nDNN \nresults\nCandidate \nmetadata\nMulti-\nlevel inf. \nengine\nTabi-aware \noï¬„ine \nproï¬ling\nModel \nselection\nInf. ctrl. \nlogic\nFigure 4. Tabi workflow. Highlighted components are op-\ntimized in this work. Components in dashed lines form the\nlogical multi-level inference engine (Â§4).\nTabi proposes a novel multi-level inference engine to achieve\nfine-grained latency reduction efficiently for generic models\ndriven by real-time per-query feedback.\nFigure 4 illustrates the workflow and components of Tabi.\nThe model repository stores the trained DNNs submitted\nand maintained by the user. Tabi conducts offline profiling\nto obtain the statistics of single models and various multi-\nlevel DNN configurations (i.e.,candidates, details in Â§5). For a\nnew task with performance targets, the controller selects the\noptimal candidate and runs DNN inference on the worker.\nThe inference control logic consists of the three modules in\nÂ§4 and manages the DNNs used on the worker. We make\ncontributions in model selection and single data inference\nwhile the design of Tabi has no substantial difficulty to work\nwith resource/model scaling systems [31, 57].\nModel repository. Tabi maintains a model repository that\nstores the registered DNNs and their metadata, including\nthe accuracy, inference latency, memory utilization, and in-\ntermediate results including attention weights and softmax\noutputs. In addition to single models, we efficiently profile\nvarious configurations of Tabi formed by available models\nand view them as logical model candidates.\nModel selection. As discussed in Â§2.1, the theoretically\nideal solution ignores the prohibitively large cost of enumer-\nating all DNNs to find the perfectly fitting one for a single\nquery. To handle this in practice, Tabi generates differently\noptimized candidates to bound\nthe overhead and balance the performance. For queries\nof a specific task, Tabi selects the optimal model candidate\nregarding the the target and runs inference. We further elab-\norate the criterion in Â§5.\n4 Multi-Level Inference Engine\nThe core of Tabi is a novel multi-level inference engine that\nemploys both efficient and highly accurate DNNs to serve\nheterogeneous NLP queries with corresponding resources,\nas shown in Figure 5. In addition to the DNNs, the inference\nengine includes (1) a probability-based dispatcher that uses\nthe calibrated confidence score to return accurate predictions\nRun small DNN only; \nGet softmax outputs, \nattention weights, and \ncalibrated conï¬dence.\nReturn or \ndispatch query \nwith a probability.\nRun LLM faster; \nGet softmax \noutputs.\nAttention-based \nword pruning.\n (Â§4.2)\nQuery Outputs\nreturn\nre-\nroute\nconï¬-\ndence\nattention weights\nWeighted dual \nensemble.  (Â§4.3)\nsoftmax \noutputs\nsoftmax \noutputs\nreturn\n (Â§4.1)\nFigure 5. Overview of multi-level inference engine with two\nmodel levels. 3 system modules are highlighted. Generated\nvariables are in italic, and system decisions are in bold.\nfast; (2) input data pruning using existing attention weights\nto accelerate LLM inference for re-routed queries and reduce\nsystem overheads; and (3) a weighted multi-level ensemble\nof activated models to further increase the final accuracy.\nCompared to ML-native optimizations. Tabi concep-\ntually echoes the ML research on adaptive inference (e.g.,\nearly-exit[80, 89] and token pruning [ 29, 74]) in terms of\nconsuming only necessary DNN resources. We find that let-\nting a DNN decide which layer to return results or which\ntokens to drop requires non-trivial and, more importantly,\nper-model non-scalable labor, including modification to the\nmodel architecture and additional training efforts. This is not\nideal because the booming of pre-trained language models\nand model-less inference systems were meant to save efforts\nfor both ML development and deployment. Instead, Tabi\nachieves low inference latency with DNNs straight from\nmodel zoos without customization. Tabi implements the\nattention-aware multi-level design at the system level in-\nspired by ML advancements, so that we can directly serve\ngeneric or generated models (e.g., via compression [28]) with\noptimized performance and usability of model-less inference.\nWe evaluate Tabi compared to early-exit and optimized sin-\ngle models in Â§7.5.\n4.1 Probabilistic Dispatcher\nIn applications that handle complex natural languages, DNNs\nshould not only be accurate but also need to indicate when\nthey are likely to be incorrect. In Tabi, returning the incorrect\nresults of small models early could nullify any advantages of\nLLMs and risk the violation of accuracy targets. We design\na probabilistic dispatcher to decide when to return outputs\nearly and when to continue the inference.\nA straightforward way to get the confidence of predic-\ntion is using the softmax probability, which sums to 1. This\nmethod is used by the popular Hugging Face deployment\npipeline [22] and its model zoo. The probability of a class ğ‘–\namong the set of all classes ğ¾ is calculated as:\nğ‘ƒ(ğ‘¦ğ‘–)= ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ (ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡ğ‘ ğ‘–)= exp (ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡ğ‘ ğ‘–)Ã\nğ‘—âˆˆğ¾ exp (ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡ğ‘ ğ‘—) (2)\n5\nEuroSys â€™23, May 08â€“12, 2023, Rome, Italy Wang et al.\n0.6 0.8 1.0\nConfidence score\n0.6\n0.7\n0.8\n0.9\n1.0Average accuracy\nCalibrated\nconfidence\nUncalibrated\n0.9 1.00.5 0.6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Re-routing prob.\nRe-routing\nprobability\nReference\nConfidence\nCDF\nConfidence score\nFigure 6. Confidence score buckets and their average ac-\ncuracy for SST-2 (left). Confidence calibration allows us to\nmake accurate dispatching decisions. Our re-routing proba-\nbility curve (right) can balance the accuracy and efficiency.\nwhere ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡ğ‘  is the raw model output.\nHowever, modern DNNs, including language models, are\nâ€œover-confidentâ€ [17, 32]. In our test, small models constantly\nmake predictions with softmax probabilities over 0.9, which\nare higher than their actual accuracies; besides, even wrong\npredictions are often made with high softmax probabilities.\nIn the left part of Figure 6, we group the results of a bi-\nnary classification workload by confidence score (Equation 2,\nrange from 0.5 to 1) into five buckets and show their average\naccuracies as the patterned orange bars. The red diagonal\nline illustrates the ideal relationship between the confidence\nscore and the actual inference accuracy when they are equal.\nClearly, they do not match and high softmax probabilities\n(i.e., above 0.8) do not guarantee high accuracies, making it\nhard to understand the inference quality.\nTo dispatch queries reliably and balance accuracy and\nefficiency, we use temperature scaling [32] to calibrate the\nsoftmax confidence score ğ‘ğ‘œğ‘›(ğ‘¦)so that it can match the\nactual inference accuracy:\nğ‘ğ‘œğ‘›(ğ‘¦)= max\nâˆ€ğ‘–âˆˆğ¾\nexp (ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡ğ‘ ğ‘–/ğ‘‡)Ã\nğ‘—âˆˆğ¾ exp (ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡ğ‘ ğ‘—/ğ‘‡) (3)\nwhere ğ‘‡ is the temperature set through a quick fitting.\nAs a result, the calibrated confidence score (blue bars in\nFigure 6) matches the ideal distribution of accuracy, provid-\ning Tabi reliable basis to decide when to return the correct\nresults of the efficient model level quickly.\nCalibrating with Equation 3 does not require DNN re-\ntraining. A quick fitting that minimizes the negative log-\nlikelihood loss can set the optimalğ‘‡. We calibrate the trained\nmodels in the model repository together with the offline\nprofiling. We use appropriate techniques such as multi-class\ncalibration [42] and regression calibration [41].\nWith the calibrated confidence scoreğ‘ğ‘œğ‘›(ğ‘¦), we define the\nprobability of re-routing a query to the next level in Equa-\ntion 4, which is a combination of the normalized confidence,\nthe ReLU, and the scaled sigmoid activation functions:\nğ‘ƒğ‘™ğ‘ğ‘Ÿğ‘”ğ‘’(ğ‘¦)=\nï£±ï£´ï£´ ï£²\nï£´ï£´ï£³\n1\n1 +exp (ğ‘˜Â·ğ‘›ğ‘œğ‘Ÿğ‘š(ğ‘ğ‘œğ‘›(ğ‘¦))) ğ‘ğ‘œğ‘›(ğ‘¦)> ğ‘\n1 ğ‘ğ‘œğ‘›(ğ‘¦)â‰¤ ğ‘\n,\nğ‘›ğ‘œğ‘Ÿğ‘š(ğ‘ğ‘œğ‘›(ğ‘¦))= ğ‘ğ‘œğ‘›(ğ‘¦)âˆ’ğ‘\n1 âˆ’ğ‘ âˆ’1\n2\n(4)\nwhere ğ‘ is the cut-off threshold close to 1 and ğ‘˜ is a scaling\nparameter for the slope of the sigmoid. Both ğ‘ and ğ‘˜ are\ntask-specific for the various distribution of difficulty vari-\nance. Tabi offline-profiles them with the DNN models and\nwill be set at the model selection phase to achieve optimal\nperformance (Â§5). Equation 4 combines soft probability and\nhard cut-off to balance accuracy (high recall of challeng-\ning queries) and latency (high precision of re-routing). We\nevaluate how hyperparameters affect different tasks in Â§7.4.\nThe right-hand side of Figure 6 illustrates Equation 4 with\nğ‘ = 0.9 and two classes. For queries whose confidence is\nlower than the cut-off threshold, the dispatcher always re-\nroutes them to the next-level model. For the more certain\nqueries, we use thescaled sigmoid function to prioritize those\nwith near-100% confidence for returning and adjust the re-\nrouting probability of queries whose confidence is just above\nğ‘ for better accuracy (solid line). Compared to linearly de-\ncreasing the re-routing probability (dashed line), our scheme\ncan return more queries early for lower latency and achieves\nhigher accuracy among the returned ones.\nAlthough the probability of returning early (area above\nthe solid line) seems limited, we find that the CDF of queries\n(green dotted line) usually concentrates around the high-\nconfidence side, so the potential is huge. In our evaluation,\nthe dispatcher can return around 50% to 70% of the queries\nafter the first efficient model. This is because our model\nselection process can use a competent and efficient model as\nthe first level to share the workload of LLMs.\n4.2 Attention-Based Word Pruning\nAfter the efficient level and dispatching, those challenging\nqueries still require LLMs. A concern about serving such\nqueries with extra small models is that re-routed queries\nwould increase the tail latency even though we improve the\naverage and median performance. On the contrary, we find\nthat the small modelâ€™s results can reduce the latency. We use\nthe real-time semantic understanding (i.e., attention weights)\nof a query made by the small model to accelerate its LLM\ninference by pruning unnecessary words from the input data\nand offset the multi-level latency overhead.\nNatural languages contain redundant tokens such as prepo-\nsitions and postpositions that contribute less to the NLP\ntask [14, 29]. Pruning such words in a query can accelerate\ninference because the computational complexity of the at-\ntention mechanism grows quadratically with the sequence\nlength [69]. Recent ML research develops specialized DNN\n6\nTabi: An Efficient Multi-Level Inference System for Large Language Models EuroSys â€™23, May 08â€“12, 2023, Rome, Italy\nstructures to adaptively prune tokens across layers based\non attention weights [29, 74]. Nevertheless, such optimiza-\ntion requires specialized models and extra training, so it\ndoes not fit into model-less inference systems. Instead, we\ndirectly optimize the input texts of the LLM at the model\nlevel rather than layer level through the attention weights\nreadily provided by the previous small model.\nAs introduced in Â§2.3, language models build on the atten-\ntion mechanism, which indicates the semantic importance\nof each sub-word token. We obtain the importance vector ğ¼\n(as illustrated in Figure 3) of a query by extracting and accu-\nmulating the attention weights of each layer and head of the\nprevious model, as shown in Figure 5. Attention layers and\nheads handle different aspects of the language [14, 56]; thus,\naccumulation makes the importance vector more reliable.\nThe attention weight is an intermediate variable produced\nduring inference, so this summation process is fast.\nTo select the significant tokens and generate a pruning\nmask, we normalize ğ¼ to relative z-scores and convert them\nto 1s and 0s (to be pruned) with a binary step:\nğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’_ğ‘šğ‘ğ‘ ğ‘˜(ğ¼)=\n(\n1 ğ¼ > ğ›¼Â·ğœ‡\n0 ğ¼ â‰¤ğ›¼Â·ğœ‡ (5)\nwhere ğœ‡is the mean value of ğ¼ and ğ›¼ is the hyperparameter\nof relative pruning degree. We setğ›¼through model profiling\nand selection (Â§5). We always keep special tokens.\nHaving the token pruning mask is not enough because dif-\nferent language models usually have different numeric rep-\nresentation methods of tokens calledtokenizers. For example,\nsome tokenizers will split and encode the word â€œbeautifullyâ€\ninto two tokens, â€œbeautifulâ€ and â€œ-lyâ€, while others encode\nit as a whole. Besides, tokenizers with different vocabular-\nies will also map the same word to different numeric word\nids. We find that the only universal data passing interface\nbetween models is the text itself.\nOur inference system should optimize for generality across\nmodels with different architectures and embedding formats.\nInstead of pruning tokens across layers [29], Tabi generates\na word pruning mask and applies it on the raw input texts to\nthe LLM, e.g., â€œthis paper beautifully written enjoyabeâ€ is the\noptimized query from the toy example in Figure 3. We prune\na word only if all of its sub-word tokens are 0s in Equation 5,\nusing tokenâ€™s positional information from the tokenizer. For\ninstance, if â€œbeautifulâ€ should be preserved while â€œ-lyâ€ is\nmasked, Tabi keeps the whole word for accuracy.\nAttention-based semantic understanding is universal across\nlanguage models and we experimentally validate the cor-\nrectness of word pruning using another modelâ€™s attention\nweights. We find that smaller models produce similar dis-\ntributions of importance vectors compared to large ones,\ntested on SST-2 dataset: 89% of the top-50% important tokens\nare the same between a small and a large model, indicat-\ning Tabi can maintain the necessary tokens. The difference\nis that LLMs are better at clearly distinguishing high- and\nlow-importance tokens. Theoretical research suggests that\nlayers and heads in LLMs work as an ensemble of low-rank\nattention layers [27, 51]; removing some (i.e., using small\nmodels) does not drastically alter the attention outputs [56].\n4.3 Weighted Multi-Level Ensemble\nFor those re-routed queries, rather than directly outputting\nthe prediction of the final-level LLM, we employ weighted\nensemble learning to combine it with the previous levelsâ€™\npredictions to improve accuracy. This technique does not add\nextra ML computations since all the intermediate softmax\noutputs are readily available in the multi-level structure.\nAn ensemble is a set of models whose individual decisions\nare combined with weights to make predictions jointly. It\nhas been proved that adding independent models, even weak\nones, to an existing large model can be more accurate than\na single large model alone because of the reduced variance\nand bias [40, 49].\nWhen the LLM outputs its results, Tabi forms an weighted\naverage prediction ğ‘ (ğ‘¦)(i.e., soft voting ) with existing pre-\ndictions of the efficient model. For example, a two-level Tabi\nconfiguration outputs:\nğ‘ (ğ‘¦)= ğ‘¤ Â·ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ 2 (ğ‘¦)+( 1 âˆ’ğ‘¤)Â·ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥ 1 (ğ‘¦) (6)\nwhere ğ‘¤ is the weight parameter of the final LLM in the en-\nsemble algorithm [12]. We use the standard method to setğ‘¤\nat the model selection stage: Weights are proportional to the\naccuracy on the re-routed data during offline profiling [19].\nThe recent inference system Cocktail [ 31] proposes an\nensemble of smaller DNNs to achieve parallel execution and\nreduce latency, which essentially makes LLMs shallower but\nwider. Different from Cocktail, Tabi uses ensemble to make\nup for the slight accuracy loss during the previous latency-\nfocused optimizations at almost no additional cost. Latest\nbiomedical research proposes to adaptively set the ensemble\nweight using each data sampleâ€™s confidence score [50] which\nis available in Tabi, but we find the overall improvement\nlimited and decide to keep the system simple.\n5 Model Candidate Profiling and Selection\nTo navigate Tabiâ€™s optimization space, we need to properly\nconfigure the system, e.g., how many levels and which mod-\nels should we use, how confident to return predictions early,\nand how unimportant a word is for pruning. We abstract\na set of Tabi configurations as a model candidate with its\naccuracy and latency performance. A candidate is logically\nequivalent to other DNN serving units, e.g., single models in\nINFaaS [57] and ensembles in Cocktail [31], such that Tabi\ncan be compatible with existing inference scaling systems.\nTabi includes an offline profiling and an online selection\nprocess. Profiling generates a range of differently optimized\ncandidates. When a task arrives, Tabi selects a candidate\nwhose combined performance can meet the targets with the\n7\nEuroSys â€™23, May 08â€“12, 2023, Rome, Italy Wang et al.\nlowest latency. Tabiâ€™s candidate profiling and selection gen-\nerally follow the SOTA inference pipeline [ 31, 57] while\naccommodating the internal mechanism of Tabi.\nTabi-aware offline profiling. The performance of a can-\ndidate is jointly decided by the base models and Tabi hy-\nperparameters (e.g., ğ‘, ğ›¼, and ğ‘¤). First, we profile the regis-\ntered single models using representative datasets as INFaaS\ndoes. We save their statistics (e.g., accuracy and latency)\nand Tabi-specific intermediate variables (Figure 5), including\nmodelsâ€™ softmax outputs (#class-length vectors), calibrated\nconfidence scores (floats), and importance weights from at-\ntention (#token-length vectors). These variables add negligi-\nble storage overheads (i.e., a few megabytes per model).\nNext, we obtain the combined performance of candidates\nwith early stopping for efficiency. Tabi automatically gener-\nates a list of possible multi-level DNN combinations starting\nfrom 2 levels with accuracy in ascending order using regis-\ntered models. To remove impractical choices (e.g., putting\nLLMs at all levels or using too many models), we only keep\ncandidates whose 75th tail latency is within 1.5Ã—of the slow-\nest model used. For each combination, we test a range of\nhyperparameter values. For dispatcher cut-off ğ‘, testing dif-\nferent values does not invoke additional DNN computations\nbecause they essentially rearrange the saved softmax re-\nsults. We decrease ğ‘ from 0.95 (1 means always using LLMs)\nwith step size 0.05 and stop when the combined accuracy is\nlower than the average of the models used. We set ensemble\nweight ğ‘¤ to be proportional to each modelâ€™s accuracy (see\nÂ§4.3). Since word pruning modifies the input, profiling the\npruning scale ğ›¼requires extra rounds of DNN inference only\nfor LLMs. We use the saved attention weights of a represen-\ntative efficient model DistilBERT and increase ğ›¼ from 0.5 (0\nmeans no pruning) with step size 0.1 to generate the pruning\nmask until an LLMâ€™s accuracy loss exceeds 2%. These designs\nreduce the overhead over the standard one-time profiling\n(discussed in Â§7.5). Finally, we save the metadata of Tabi\ncandidates at the model repository just like single models.\nOnline candidate selection. With the profiled perfor-\nmance, Tabi selects candidates for online tasks using the\nsaved metadata from the model repository following exist-\ning criteria [57]. We choose the candidate that achieves the\nlowest latency that can meet the performance target (accu-\nracy and latency) of a task. Then the controller launches the\nselected model instances on the worker machine to serve\nqueries of this application. Our work focuses on optimiz-\ning single data inference while existing resource and model\nscaling systems still apply. For example, a Tabi candidate\ncould horizontally scale to multiple instances or vertically\nswitch to different internal models or parameters to handle\nworkload changes like single models (Â§7.5).\n6 Implementation\nWe build Tabi using Python and libraries including PyTorch [5]\nas the base ML framework, HuggingFace Transformers [4]\nfor the NLP toolbox and model implementations, and Torch-\nServe [7] as the inference system backbone.\nClients can send text queries to the inference REST API,\nand developers can register trained DNNs using the man-\nagement API, both hosted by the controller. We implement\nthe model selection algorithm on the controller. Each active\nlanguage model runs in a process, whether using GPU or\nCPU, on the worker machine. The model repository stores\ntrained models on networked storage via GlusterFS [1] and\nconducts offline profiling on an available worker. It saves the\nmetadata of candidates as Python dictionary for fast lookup\nand can upgrade to distributed Redis [6] for scalability.\nThe multi-level inference engine is a logical entity, as\nshown in Figure 4. The probabilistic dispatcher (Â§4.1), attention-\nbased word pruning (Â§4.2), and weighted multi-level ensem-\nble (Â§4.3) all reside in the controller, exchanging the control\nlogic and intermediate results with the DNNs on the worker.\nImplementing attention-related operations uses Hugging-\nFace standard APIs without requiring model changes. The\nseparation of control and compute ensures the scalability\nof Tabi, as models for the same task do not have to run on\nthe same physical machine. Besides latency, a monitoring\nprocess updates the CPU/GPU and memory utilization of the\nworker to the controller for extensible resource scheduling\nwhich is not the focus of this paper (Â§2.1).\n7 Evaluation\nWe first evaluate Tabiâ€™s overall performance of reducing in-\nference latency (including average and tail) while meeting\nLLM-grade target accuracy compared to SOTA inference sys-\ntems (Â§7.2). To demonstrate the effectiveness of Tabiâ€™s design\nchoices, we dive into the candidate profiling and selection,\nthe three system components of the multi-level inference en-\ngine (Â§7.3), and the impact of hyperparameters (Â§7.4). Finally,\nwe discuss the system overhead, ML-native optimizations,\nand generality (Â§7.5). The main takeaways are:\nâ€¢Tabi reduces inference latency by 21%-40% on average\nfor accuracy-demanding applications;\nâ€¢Tabi minimizes the system overhead and achieves com-\nparable tail latency performance;\nâ€¢Tabi shows advantages over ML-native optimizations\nand is compatible with other models/systems.\n7.1 Methodology\nTestbed setup. We evaluate Tabi using a server with 2\nNVIDIA V100 GPUs, 40 CPU cores, and 128 GB memory,\nrunning Ubuntu 18.04. The server is network-attached to\nhard disks as the model store. The logical controller and\nworker are located on the same server in our evaluation.\n8\nTabi: An Efficient Multi-Level Inference System for Large Language Models EuroSys â€™23, May 08â€“12, 2023, Rome, Italy\nModel #Parameters\n(million)\nLatency\n(ms)\nAccuracy\n(%) Pf\nBERT-small 28 6 72.1 6\nDistilBERT* [59] 66 7 83.2 5\nALBERT* [43] 11 14 84.5 5\nPruneBERT* [60] 110 15 81.2 4\nDeBERTa-small 142 12 86.9 4\nBERT-base [18] 110 17 84.1 4\nRoBERTa-base [47] 124 19 86.3 4\nDeBERTa-base [35] 184 20 88.8 3\nBERT-large 340 24 86.7 2\nRoBERTa-large 355 26 90.6 2\nDeBERTa-large 406 29 91.3 2\nDeBERTa-xlarge 886 38 91.7 1\nTable 1. Summary of the language model architectures\nranked by inference latency for MNLI. The latency is the\nDNN execution time. For other tasks/datasets, the relative\nperformance is similar. We consider the bottom four to be\nLLMs because of their large sizes. A lower packing factor\n(Pf ) suggests a higher per-model cost. * denotes ML-native\noptimized models.\nTasks, datasets, and models. We use the GLUE bench-\nmark [72], MASSIVE [26], and CLINC150 [ 44]. GLUE is a\nstandard evaluation method that covers 9 classification and\nregression datasets. MASSIVE (en-US) for scenario detec-\ntion and CLINC150 for intent classification each have 18 and\n150 classes. The validation (dev) sets we use have mostly\nthousands of data points. Each query contains a single data\nsample without zero padding (i.e., batch size = 1) and arrives\nafter the previous one finishes. We select 12 representative\nand generic language models in Table 1, ranging from highly\nefficient to having top-grade accuracies. Three models noted\nwith * are each optimized with knowledge distillation, pa-\nrameter sharing, and pruning. Most of the fine-tuned models\nare publicly available [2]; if not, we fine-tune them using the\npre-trained backbone.\nModel storage. The size of models in Table 1 is 32bit Ã—\n#parameters; the model zoo also lists modelsâ€™ size [2]. For ex-\nample, DistilBERT (66M), BERT-base (110M), and RoBERTa-\nlarge (355M) each proportionally take 268MB, 440MB, and\n1.43GB. Models fine-tuned for different tasks in Table 2 have\nalmost the same size (<1%) since the task-specific classifier\nhas much fewer parameters than Transformer blocks (Fig-\nure 2). Selecting from available models is a standard practice\nin model-less inference [57], and not all models will be de-\nployed. Besides, inference systems are not the only user of\nstored models [33, 52]. We curate various models to show\nTabiâ€™s generality, while the user-maintained repository size\ndepends on usersâ€™ deployment scale. Tabi is not designed for\nedge devices, as distributing models is much more costly [88].\nBaselines. We compare Tabi with INFaaS [57] and Cock-\ntail [31]. INFaaS selects the best single model, and Cocktail\nforms a voting ensemble of smaller models to reduce latency.\nBoth systems propose resource scheduling designs, while\nwe focus on optimizing the latency of single data inference\nand estimate the cost and supported throughput in a static\nsetting. We set Tabi and Cocktail to use both GPUs to execute\nmultiple models while INFaaS uses a single GPU.\nMetrics. The main latency metric is the average DNN\ninference time per query, including Tabiâ€™s control logic. Data\npre-processing time (e.g., tokenization) is excluded because it\nis a static and small factor which is highly optimized [3] but\nnot by Tabi. We show multiple percentiles of the inference\nlatency given the variance of tasks. The accuracy metric is\nthe percentage of queries that obtain the correct results. We\naverage the results of three runs. To understand the cloud\nserving cost on our testbed, we use the packing factor (Pf )\nintroduced in Cocktail as a proxy of unit price, which is\nthe number of models that can be executed concurrently on\na single GPU without latency degradation (>10%). Larger\nmodels have lower packing factors, thus the per-model cost\n(1/ğ‘ƒğ‘“ ) is higher. The estimated cost of serving a dataset ( Â¯ğ¶)\nis the sum of the product of each model (ğ‘š)â€™s cost (1/ğ‘ƒğ‘“ğ‘š),\naverage time of the queries it serves (ğ‘‡ğ‘š), and percentage of\nworkload (ğ‘Šğ‘š): Â¯ğ¶ = Ã\nğ‘šâˆˆğ‘€ 1/ğ‘ƒğ‘“ğ‘š Ã—ğ‘‡ğ‘š Ã—ğ‘Šğ‘š. For INFaaS,\nthere is one model, and ğ‘Š is 100%; Cocktail uses multiple\nmodels (ğ‘€), and each ğ‘Šğ‘š is 100% without auto-scaling; for\nTabi, the workload handled by the small model (ğ‘Šğ‘ ) is 100%\nwhile the large modelâ€™s ğ‘Šğ‘™ < 100% (see Â§7.3.2). We test the\nthroughput (queries per second) using the workload pattern\ndescribed before which has no queuing, batching, or scaling.\n7.2 Overall Performance\nTable 2 summarizes the evaluation results. We set the target\naccuracy of tasks to be rather high to reflect Tabiâ€™s scope of\nserving accuracy-demanding applications that require LLMs.\nAccuracy. For all tasks, Tabi achieves the LLM-grade ac-\ncuracy targets. Compared to INFaaS, which uses LLMs to\nserve all queries, and Cocktail, which uses an ensemble of\nmodels, Tabiâ€™s accuracy difference is within 1% on average.\nTabi can balance latency and accuracy by accurately offload-\ning the confident queries. As a result, early returned queries\nhave a similar accuracy whether served by efficient models\nor LLMs (detailed analysis in Â§7.3.2). For the challenging\nqueries which would have degraded quality if served by\nsmall models, Tabi utilizes LLMs and ensemble learning to\nensure accuracy (details in Â§7.3.3 and Â§7.3.4).\nLatency. Tabi drastically reduces the average inference la-\ntency by 21% to 40% compared to INFaaS across various tasks.\nCompared to the recent Cocktail, Tabi also achieves an 11%-\n26% average latency reduction. Figure 7 shows the detailed\n9\nEuroSys â€™23, May 08â€“12, 2023, Rome, Italy Wang et al.\nMethod SST-2 MNLI (-mm) RTE QQP MRPC CoLA QNLI STS-B MASSIVE CLINC\nTgt. acc. (%) - 95 90 85 92 90 65 94 92 92 97\nAccuracy\n(%)\nINFaaS 96.1 91.2 86.6 92.3 90.9 67.6 94.7 92.4 92.5 97.3\nCocktail 95.4 90.4 85.2 92.1 90.0 65.1 94.3 92.1 92.1 97.0\nTabi 95.6 90.4 86.0 92.1 90.1 65.2 94.6 92.0 92.1 97.0\nLatency\n(ms)\nINFaaS 22.0 25.8 38.1 25.4 24.9 22.5 26.0 21.2 20.9 21.3\nCocktail 17.8 22.9 34.7 20.8 20.2 18.2 22.3 17.5 15.4 17.2\nTabi 13.2 20.3 30.0 16.0 16.5 15.7 18.7 13.4 13.5 14.8\nEstimated\ncost & tput\nINFaaS 11.6/42.7 13.3/36.1 19.5/24.6 13.2/36.3 13.2/36.4 11.8/40.7 13.5/35.5 11.4/41.9 11.2/42.4 11.5/41.8\nCocktail 9.4/53.2 15.3/40.0 20.3/26.8 11.7/43.7 12.1/44.0 10.7/50.1 12.8/40.6 9.6/53.0 9.5/55.9 10.5/52.9\nTabi 5.8/63.8 9.3/42.1 14.2/29.2 7.2/53.5 7.9/52.2 6.5/53.8 8.8/46.5 6.0/62.2 5.9/61.9 6.3/59.3\nLatency\nreduction (%) - 40/26 22/11 21/12 37/23 34/18 30/14 28/16 37/23 35/12 30/14\nTable 2. Summary of evaluation tasks. Compared to INFaaS, Tabi achieves 21%-40% average inference latency reduction while\nmeeting the demanding accuracy target. Compared to Cocktail, Tabi also reduces the latency by 11%-26%. In our static setting,\nTabi can reduce the estimated cost by 27%-50% and 30%-39% and increase the throughput by 16%-49% and 5%-20% respectively.\nThe costs are estimated using the packing factor (Pf ).\nlatency performance. The 75% and 99% tail latencies are im-\nportant to large-scale systems, while the 25% and median\nlatencies are also crucial to ML analytics since many appli-\ncations favor â€œfreshnessâ€ when the follow-up analysis can\nbe updated when new results arrive [21]. Tabi significantly\nreduces the 25% and median latencies by up to 62% and 45%\ncompared to INFaaS and Cocktail, thus is especially effective\nfor agile NLP applications. We also achieve better tail latency\nthan INFaaS and close to Cocktail which expressly focuses\non parallel execution. Tabi works well for tasks with 10+\nclasses (e.g., MASSIVE and CLINC150) since the dispatcher\ncan well handle multi-class confidence calibration.\nEstimated cost and throughput. Tabi can reduce the\ncloud serving cost and support higher throughput by running\nfewer LLMs. As explained in Â§7.1, we estimate the cost of\nmodels normalized by datasets using the same GPU testbed\nassuming the workloads and hardware are well scheduled\nso that the cost of running each model is relative to its us-\nage fraction of the device. Tabi reduces the cost Â¯ğ¶ by 27%\nto 50% and 30% to 39% compared to INFaaS and Cocktail\nrespectively. Cocktail sometimes gets higher costs than IN-\nFaaS since more base-size models are used; Cocktail uses\nauto-scaling for dynamic workloads to reduce costs which is\nnot activated with our static query pattern. For single data\ninference workload, Tabi generally improves the throughput;\ncompared to INFaaS and Cocktail, we can serve 16% to 49%\nand 5% to 20% more queries.\n7.3 Performance Breakdown\nWe deep dive into the performance of each system compo-\nnent on two representative datasets, SST-2 and MNLI, to\nexplain our design choices. We define task difficulty as the\nINFaaS Cocktail T abiINFaaS Cocktail T abi\n0\n10\n20\n30\n40Latency (ms)\nSST-2\nINFaaS Cocktail T abiINFaaS Cocktail T abi\n0\n10\n20\n30\n40 MNLI\nFigure 7. Latency breakdown. Each box shows the 25%,\nmedian, and 75% latencies; whiskers plot the 1% and 99%\nnon-outlier values. Tabi largely reduces the median and 25%\nlatency and maintains the tail latency compared to INFaaS.\naccuracy gap between using small models and LLMs rather\nthan the absolute accuracy. SST-2 is a relatively simple sen-\ntiment analysis task with two labels, while MNLI is a more\nchallenging natural language inference (NLU) task with three\nlabels where LLMs perform much better than small models.\n7.3.1 Candidate Selection and Hyperparameters. We\nfind that for all the evaluated tasks, Tabi always selects the\ninference engine with two levels : an efficient DNN and an LLM,\nskipping the base-size models in Table 1. For example, for\nSST-2 Tabi selects DistilBERT and RoBERTa-large, while for\nMNLI, Tabi selects DistilBERT and DeBERTa-large. No candi-\ndate uses DeBERTa-xlarge for its very limited improvement\nover the large version, nor BERT-large over other modern ar-\nchitectures. Similarly, INFaaS mostly selects RoBERTa-large,\nand Cocktail selects multiple base and small models.\nOur model profiling and selection bound Tabiâ€™s overhead;\nthus, it favors a two-level structure over more levels. As\nshown in Figure 7, using two models can strike a balance\n10\nTabi: An Efficient Multi-Level Inference System for Large Language Models EuroSys â€™23, May 08â€“12, 2023, Rome, Italy\nbetween workload sharing (reducing average latency) and\nsystem overhead (maintaining a comparable tail latency). We\nanalyze the performance of using three models in Â§7.5.\nIn addition to models, this step sets the Tabi hyperparame-\nters. For example, it chooses the dispatcher cut-off threshold\nğ‘ of 0.95 for SST-2 and 0.85 for MNLI and pruning scale ğ›¼ of\n0.7 and 0.9. A larger ğ‘ lets the first model level return more\nqueries with high confidence: 89% of queries of SST-2 have\nconfidence above 0.9, while only 52% have such high con-\nfidence for MNLI. A larger ğ›¼ prunes more words for larger\nlatency reduction. We test the sensitivity in detail in Â§7.4.\n7.3.2 Workload Sharing with Dispatcher. After going\nthrough the first-level DNN, the probabilistic dispatcher\nreads the calibrated confidence score and decides whether to\nreturn the outputs directly. A good dispatcher should return\nas many easy queries as possible to produce fresh results.\nMeanwhile, the accuracy gap between using efficient models\nand LLMs should be small for early returned queries and\nlarge for re-routed ones so that they can get what they need.\nFor SST-2, Tabi serves 69.8% of queries only using a small\nDNN while achieving 97.7% and 91.4% accuracies for re-\nturned and re-routed queries. To understand this perfor-\nmance, we also test alternatively serving early returned\nqueries with an LLM (as INFaaS does) and serving re-routed\nqueries with a small model and find the accuracies to be\n98.7% and 76.2%. Their 1.0% and 15.2% gaps show that we\ncorrectly assign the right DNNs to heterogeneous queries.\nFor the more challenging MNLI task, Tabi early returns\n49.4% of the queries. This is because LLMs have a large ad-\nvantage over efficient DNNs for more difficult tasks. Tabi\nachieves 96.1% and 85.5% accuracies for each level, while\nif we switch the models, the accuracy would be 96.8% and\n71.3%; the gaps are ideally 0.7% and 14.2%. To summarize,\nTabi achieves minimal accuracy loss (within 1%) for early re-\nturned queries, suggesting that our dispatcher can correctly\ndistinguish the easy queries from the rest.\nFigure 7 also shows the modest drawback of Tabi: Com-\npared to the largely optimized 25% and median latencies, the\ntail latency does not improve much compared to INFaaS and\nis not as good as Cocktail. This is because Tabi sequentially\nruns DNNs for a limited number of queries while Cocktail\nalways runs in parallel. That being said, our attention-aware\npruning successfully reduces overheads and dismisses the\nconcern of slowing down the tail performance of LLMs.\n7.3.3 Tail Latency Reduction With Attention. By re-\nusing the attention weights of the small DNN to prune input\nwords, we can accelerate LLM inference of the re-routed\nqueries and thus reduce the tail latency. Table 3 shows the\nperformance of word pruning. For SST-2, by pruning the\n14.2% least-attended words from 26 words per query on av-\nerage to 22.3 words, we can reduce the execution latency of\nLLM by 17%. The 0.3% accuracy loss on the re-routed queries\ncan be remedied by the weighted ensemble (see Â§7.3.4).\nTask Word pruning\nratio (%)\nLLM latency\nreduction (%)\nLLM accuracy\nloss (%)\nSST-2 14.2 17 0.3\nMNLI 13.6 15.4 0.3\nTable 3. Attention-based word pruning reduces the inference\nlatency of LLMs with minimal accuracy loss and offsets the\nextra small DNN overhead in the tail performance.\nFor MNLI, the performance of word pruning is similar.\nOne difference is that the NLU task of MNLI consists of\ntwo separate sentences in a query rather than a single one.\nWe design Tabi for generality so that it can serve various\ninput formats and tokenizers by focusing on words rather\nthan tokens. As a result, optimizing the re-routed queries by\nâˆ¼14% can approximately offset the overhead of sequentially\nexecuting an efficient DNN (compared to INFaaS in Figure 7).\n7.3.4 Weighted Multi-Level Ensemble. Rather than di-\nrectly outputting the LLM predictions for re-routed queries,\nwe form a weighted ensemble of multi-level models whose\nresults are already available. We find that by weighted ensem-\nbling both levels, we can improve the LLM accuracy by 0.4%\n(91.4% to 91.8%) and 0.3% (85.5% to 85.8%) on these datasets\nwithout extra DNN computations, neutralizing the accuracy\nloss of the attention-based word pruning (see Table 3).\n7.4 Sensitivity Analysis\nTabi automatically selects the hyperparameters in a candi-\ndate to achieve a balance: meeting the accuracy target while\nhaving the lowest latency. Here we analyze the accuracy-\nlatency impact of ğ‘and ğ›¼ using alternative values, while the\nensemble weight ğ‘¤ is directly set by ML guidelines (Â§4.3).\nDispatcher cut-off ğ‘. Reducing ğ‘ from 1 will monoton-\nically but non-linearly increase the early-return ratio. In\nFigure 8, by movingğ‘from 0.75 to 0.85 and 0.95, 92.4%, 87.4%,\nand 69.8% of queries in SST-2 are early returned, thus increas-\ning both accuracy and latency. Tabi sets ğ‘ to 0.95 to meet\nthe accuracy target (95%). For MNLI, the patterns are similar:\n59.4%, 49.4%, and 16.0% of queries are early returned, but the\naccuracy does not significantly improve when ğ‘ increases to\n0.95, which is why Tabi sets ğ‘ to 0.85 to balance latency.\nWord pruning ratio ğ›¼. Increasing ğ›¼ will prune more\nwords from LLMsâ€™ inputs, e.g., 3.8%, 14.2%, and 62.5% of\nwords are removed using ğ›¼ = 0.5, 0.7, and 0.9 for SST-2. For\nSST-2, there is an exception that the accuracy when ğ›¼ = 0.7\nis slightly higher than ğ›¼ = 0.5. We assume this is because\npruning a few words removes the noise in the data, and\nthe accuracy resumes decreasing when ğ›¼ = 0.8 when some\nuseful words are affected (not shown in the figure).\n11\nEuroSys â€™23, May 08â€“12, 2023, Rome, Italy Wang et al.\n0.75 0.85 0.95\n80\n90\n100Accuracy (%)\n92.9% 94.0%\n95.6%\nSST-2\n0.75 0.85 0.95\n89.1% 90.4% 90.5%\nMNLI\n10.4 11.6\n13.2\n10\n20\n30\nLatency (ms)\n17.4\n20.3\n26.7\nAccuracy\nLatency\nCut-off c\nFigure 8. Impact of different ğ‘ on the combined accuracy\nand latency. Selected values are in bold.\n0.5 0.7 0.9\n80\n90\n100Accuracy (%)\n91.2% 91.4%\n87.5%\nSST-2\nAccuracy\nLatency\n0.5 0.7 0.9\n85.8% 85.7% 85.5%\nMNLI\n22.5 22.1\n19.8\n15\n25\n35\nLatency (ms)\n30.4 29.2 27.7\nPruning scale \nFigure 9. Impact of different ğ›¼ on the re-routed accuracy\nand latency. Selected values are in bold.\n7.5 Discussion\nSystem overheads. Tabiâ€™s major overhead is running an\nextra DNN for re-routed queries, which has been analyzed in\ndetail. We find that non-DNN overheads are minimal: Tabiâ€™s\ncontrol logic (e.g., Equation 4, 5, and 6) constantly takes less\nthan or around 5ms per query, mostly on obtaining attention\nweights, plus small tensor (e.g., softmax logits) operations.\nCompared to the standard profiling, Tabi runs each model\nâˆ¼2.5 times on average to test ğ›¼ with our variable reusing\ndesign at the same level of tens of minutes as INFaaS.\nCompared to ML-native optimizations. We differenti-\nate Tabi from model compression and adaptive inference\nby providing LLM-grade performance in model-less infer-\nence without per-model customization. Tabi works well with\noptimized models (Table 1) by using them as the efficient\nlevel, exploiting their speed and making up for their accuracy\nlosses. We evaluate an early-exit optimization, DeeBERT [80],\nas shown in Table 4. While DeeBERT provides two optimized\nbase-size models out of the box which cannot meet our tar-\nget accuracy, it shows 11%-40% latency reduction and <1%\naccuracy loss compared to its own baselines. Tabi performs\nbetter than the stock DeeBERT because early-exit requires\ncustomization expertise and fine-tuning, and thus has limited\nmodel choices. In addition, we customize and train an LLM-\nsize DeeBERT model with RoBERTa-large, which in theory\nshould have higher efficiency given Tabiâ€™s system design.\nWe find that the early-exit LLM does not show a clear ad-\nvantage in Table 4 because Tabi achieves extra gain through\nTask Tabi DeeBERT-\nBERT-base\nDeeBERT-\nRoBERTa-base\nDeeBERT*-\nRoBERTa-L\nSST-2 95.6/ 40% 93/40% 94.4/26% 95.9/38%\nMNLI 90.4/22% 83.9/14% 87/19% 90.4/24%\nTable 4. Accuracy (%) and latency reduction of Tabi and\nDeeBERT [80]. Tabi has similar performance even compared\nto a customized LLM. * denotes requiring ML expertise.\nAccuracy\n(%)\nMean\nlatency\nMedian\nlatency\n99%\nlatency\nLevel return\ndistribution\n90.2\n(-0.2%)\n22.0\n(+8.4%)\n12.7\n(-2.3%)\n49.4\n(+70.3%)\n45.6%/36.8%\n/17.6%\nTable 5. Compared to Tabiâ€™s two-level decision, using three\nmodels invokes the LLM less but prohibitively increases the\ntail latency by 70.3%, and so does the mean.\nattention-based word pruning. Nevertheless, early-exit will\nperform better when accuracy targets are relaxed and not\nrequiring LLMs, as discussed in the next paragraph. Users\ncan apply a cost-benefit analyzer (e.g., LiteReconfig [83]) on\ntop of our profiling module to choose from system-based\nand ML-native designs regarding the use case or incorpo-\nrate it into Tabiâ€™s controller to select candidates including\noptimized models and benefit from Tabiâ€™s architecture.\nRelaxed target accuracy. For relaxed accuracy targets\nthat do not require LLMs and are out of our scope, we can\ngracefully switch Tabi candidates to efficient single mod-\nels. We test that the break-even points for SST-2 and MNLI\nare 92% and 86% when Tabi has the same accuracy/latency\nas a modern model, DeBERTa-small. Similarly, early-exit-\noptimized RoBERTa-large can achieve 3% more latency re-\nduction than Tabi only when accuracy targets are relaxed\nunder 91% and 87% respectively at the cost of giving up\nmodel-less inference. Therefore, early-exit works best for\ndedicated tasks with less strict accuracy requirements.\nUsing three levels. In our evaluation, Tabi rules out using\nthree or more models for exceeding tail latency. Here we\nanalyze the latency performance of using three models (with\nDeBERTa-base) in Tabi for MNLI and the same setting. We\nfind that although the middle-sized model can further reduce\nthe workload of the LLM from 50.6% to 17.6%, the 99% tail\nlatency increases by 70.3% for serving challenging queries\nwith an extra model, making token pruning irrelevant and\nthis candidate impractical.\n8 Related Work and Potential Extensions\nDNN inference systems. INFaaS [57] and Cocktail [31]\nare all-around inference systems that have been discussed in\n12\nTabi: An Efficient Multi-Level Inference System for Large Language Models EuroSys â€™23, May 08â€“12, 2023, Rome, Italy\ndetail. Clipper [15] is an early generalized inference system\nthat serves different application-level targets. PRETZEL [45],\nNexus [62], and TurboTransformers [23] focus on low-level\nDNN execution efficiency which are complementary to our\nwork. sensAI [73] uses pruning to decompose a large model\ninto multiple binary classifiers to utilize more computation\ndevices. Clockwork [30] reduces the variability of GPU in-\nference latency by re-ordering queries based on their targets\nand avoiding interference. MArk [86] focuses on cost-aware\nresource procurement policies while managing the objec-\ntives of the task. Many research works exploit serverless\ncloud computing for DNN inference [8, 64]. LiteFlow [87]\ndesigns a kernel-space fast path for efficient model inference.\nML optimizations. Research on adaptive inference, in-\ncluding early-exit [80, 89] which reduces the depth of DNN\ncomputation, and token pruning [29, 74] which shrinks the\ndata span, has been discussed in detail (Â§4 and Â§7.5). The sim-\nilar ideas of optimizing data (i.e., words or pixels) have been\ndiscussed in CV systems [46, 76â€“78]. Some works focus on re-\nducing the computational cost of the attention [66, 84] which\nare orthogonal to our work. New model architectures [38, 59]\nand compression techniques [28, 67] (e.g., quantization) try\nto reduce the cost of LLMs while preserving their accuracy.\nTabi can serve them as candidates.\nPotential extensions. Vision Transformers (ViTs) are a\nnew group of models that adapt this NLP architecture to\ncomputer vision (CV) tasks like image classification [ 20].\nTabi should achieve comparable speedup when serving ViTs,\nsince they scale up similarly to LLMs [85], and our techni-\ncal dependencies, especially attention-based token pruning,\nhave been recently explored on ViTs [39, 55]. For other mod-\nels (e.g., CNNs), the benefits of Tabi will not be significant,\nand the tail latency may not get offset, as our designs are\nmotivated by the scaling characteristics and the attention\nmechanism that are unique to Transformers.\n9 Conclusion\nDelivering LLMsâ€™ top-grade accuracy with model-less infer-\nence systems causes huge overheads. Tabi can reduce the\nlatency of serving LLMs while maintaining accuracy by shar-\ning LLMsâ€™ heterogeneous workload with efficient models\nbased on per-query feedback. Tabi uses attention to reduce\nsystem overheads and tail latency. We evaluate Tabi on multi-\nple datasets and find that Tabi reduces the average inference\nlatency of LLMs by 21%-40% regarding SOTA systems.\nAcknowledgment\nWe thank the anonymous EuroSys reviewers and our shep-\nherd Dr. Somali Chaterji for their constructive feedback and\nsuggestions. This work is supported in part by the Key-Area\nR&D Program of Guangdong Province (2021B0101400001),\nthe Hong Kong RGC TRS T41-603/20-R, GRF-16213621, ITF-\nACCESS, the NSFC Grant 62062005, and the Turing AI Com-\nputing Cloud (TACC) [82]. Haisheng Tan is partly supported\nby the NSFC Grant 62132009, and Kun Guo is partly sup-\nported by the Natural Science Foundation of Fujian Province\nGrant No.2022J01118. We thank Yilun Jin and Han Tian for\nproviding valuable feedback regarding the early idea. Kai\nChen is the corresponding author.\nReferences\n[1] GlusterFS. https://www.gluster.org/.\n[2] HuggingFace models. https://huggingface.co/models.\n[3] HuggingFace Tokenizers. https://github.com/huggingface/tokenizers/.\n[4] HuggingFace Transformers. https://github.com/huggingface/\ntransformers/.\n[5] PyTorch. https://pytorch.org/.\n[6] Redis. https://redis.io/.\n[7] TorchServe. https://github.com/pytorch/serve.\n[8] Ahsan Ali, Riccardo Pinciroli, Feng Yan, and Evgenia Smirni. Batch:\nmachine learning inference serving on serverless platforms with adap-\ntive batching. In SC20: International Conference for High Performance\nComputing, Networking, Storage and Analysis , pages 1â€“15. IEEE, 2020.\n[9] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmar-\ngaret Shmitchell. On the dangers of stochastic parrots: Can language\nmodels be too big? In Proceedings of the 2021 ACM Conference on\nFairness, Accountability, and Transparency , pages 610â€“623, 2021.\n[10] Tolga Bolukbasi, Joseph Wang, Ofer Dekel, and Venkatesh Saligrama.\nAdaptive neural networks for efficient inference. In International\nConference on Machine Learning , pages 527â€“536. PMLR, 2017.\n[11] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran\nArora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine\nBosselut, Emma Brunskill, et al. On the opportunities and risks of\nfoundation models. arXiv preprint arXiv:2108.07258 , 2021.\n[12] Leo Breiman. Bagging predictors. Machine learning , 24(2):123â€“140,\n1996.\n[13] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared\nKaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish\nSastry, Amanda Askell, et al. Language models are few-shot learners.\narXiv preprint arXiv:2005.14165 , 2020.\n[14] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D\nManning. What does bert look at? an analysis of bertâ€™s attention. In\nProceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP , pages 276â€“286, 2019.\n[15] Daniel Crankshaw, Xin Wang, Guilio Zhou, Michael J Franklin,\nJoseph E Gonzalez, and Ion Stoica. Clipper: A low-latency online pre-\ndiction serving system. In 14th {USENIX}Symposium on Networked\nSystems Design and Implementation ( {NSDI}17), pages 613â€“627, 2017.\n[16] Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and\nGuoping Hu. Revisiting pre-trained models for Chinese natural lan-\nguage processing. In Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: Findings , pages 657â€“668, On-\nline, November 2020. Association for Computational Linguistics.\n[17] Shrey Desai and Greg Durrett. Calibration of pre-trained transformers.\nIn Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP) , pages 295â€“302, 2020.\n[18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBert: Pre-training of deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:1810.04805 , 2018.\n[19] Thomas G Dietterich. Ensemble methods in machine learning. In\nInternational workshop on multiple classifier systems , pages 1â€“15, 2000.\n[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis-\nsenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,\n13\nEuroSys â€™23, May 08â€“12, 2023, Rome, Italy Wang et al.\nMatthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image\nis worth 16x16 words: Transformers for image recognition at scale. In\nInternational Conference on Learning Representations , 2020.\n[21] Kuntai Du, Ahsan Pervaiz, Xin Yuan, Aakanksha Chowdhery, Qizheng\nZhang, Henry Hoffmann, and Junchen Jiang. Server-driven video\nstreaming for deep learning inference. In Proceedings of the Annual\nconference of the ACM Special Interest Group on Data Communication on\nthe applications, technologies, architectures, and protocols for computer\ncommunication, pages 557â€“570, 2020.\n[22] Hugging Face. Pipelines - Hugging Face, 2021. https://huggingface.co/\ndocs/transformers/main_classes/pipelines.\n[23] Jiarui Fang, Yang Yu, Chengduo Zhao, and Jie Zhou. Turbotrans-\nformers: an efficient gpu serving system for transformer models. In\nProceedings of the 26th ACM SIGPLAN Symposium on Principles and\nPractice of Parallel Programming , pages 389â€“402, 2021.\n[24] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers:\nScaling to trillion parameter models with simple and efficient sparsity.\narXiv preprint arXiv:2101.03961 , 2021.\n[25] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng,\nMing Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. Code-\nbert: A pre-trained model for programming and natural languages. In\nFindings of the Association for Computational Linguistics: EMNLP 2020 ,\npages 1536â€“1547, 2020.\n[26] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mackie, Kay\nRottmann, Ana Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala,\nRicha Singh, et al. Massive: A 1m-example multilingual natural lan-\nguage understanding dataset with 51 typologically-diverse languages.\narXiv preprint arXiv:2204.08582 , 2022.\n[27] Jonathan Frankle and Michael Carbin. The lottery ticket hypoth-\nesis: Finding sparse, trainable neural networks. arXiv preprint\narXiv:1803.03635, 2018.\n[28] Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang,\nHassan Sajjad, Preslav Nakov, Deming Chen, and Marianne Winslett.\nCompressing large-scale transformer-based models: A case study on\nbert. Transactions of the Association for Computational Linguistics ,\n9:1061â€“1080, 2021.\n[29] Saurabh Goyal, Anamitra Roy Choudhury, Saurabh Raje, Venkatesan\nChakaravarthy, Yogish Sabharwal, and Ashish Verma. Power-bert:\nAccelerating bert inference via progressive word-vector elimination.\nIn International Conference on Machine Learning , pages 3690â€“3699.\nPMLR, 2020.\n[30] Arpan Gujarati, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kauf-\nmann, Ymir Vigfusson, and Jonathan Mace. Serving dnns like clock-\nwork: Performance predictability from the bottom up. In 14th\n{USENIX}Symposium on Operating Systems Design and Implemen-\ntation ( {OSDI}20), pages 443â€“462, 2020.\n[31] Jashwant Raj Gunasekaran, Cyan Subhra Mishra, Prashanth Thi-\nnakaran, Mahmut Taylan Kandemir, and Chita R Das. Cocktail: A\nmultidimensional optimization for model serving in cloud. In 19th\nUSENIX Symposium on Networked Systems Design and Implementation\n(NSDI 22) , April 2022.\n[32] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On cal-\nibration of modern neural networks. In International Conference on\nMachine Learning , pages 1321â€“1330. PMLR, 2017.\n[33] Peizhen Guo, Bo Hu, and Wenjun Hu. Sommelier: Curating dnn models\nfor the masses. In Proceedings of the 2022 International Conference on\nManagement of Data , pages 1876â€“1890, 2022.\n[34] Abhishek Gupta. The imperative for sustainable ai systems. The\nGradient, 2021.\n[35] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta:\nDecoding-enhanced bert with disentangled attention. arXiv preprint\narXiv:2006.03654, 2020.\n[36] Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. Clinicalbert:\nModeling clinical notes and predicting hospital readmission. arXiv\npreprint arXiv:1904.05342, 2019.\n[37] Angela H Jiang, Daniel L-K Wong, Giulio Zhou, David G Andersen, Jef-\nfrey Dean, Gregory R Ganger, Gauri Joshi, Michael Kaminksy, Michael\nKozuch, Zachary C Lipton, et al. Accelerating deep learning by focus-\ning on the biggest losers. arXiv preprint arXiv:1910.00762 , 2019.\n[38] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The\nefficient transformer. In International Conference on Learning Repre-\nsentations, 2019.\n[39] Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei Niu, Meng-\nshu Sun, Bin Ren, Minghai Qin, Hao Tang, and Yanzhi Wang. Spvit:\nEnabling faster vision transformers via soft token pruning. arXiv\npreprint arXiv:2112.13890, 2021.\n[40] Anders Krogh, Jesper Vedelsby, et al. Neural network ensembles,\ncross validation, and active learning. Advances in neural information\nprocessing systems , 7:231â€“238, 1995.\n[41] Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate\nuncertainties for deep learning using calibrated regression. In Interna-\ntional Conference on Machine Learning , pages 2796â€“2804, 2018.\n[42] Meelis Kull, Miquel Perello Nieto, Markus KÃ¤ngsepp, Telmo Silva Filho,\nHao Song, and Peter Flach. Beyond temperature scaling: Obtaining\nwell-calibrated multi-class probabilities with dirichlet calibration. Ad-\nvances in Neural Information Processing Systems , 32:12316â€“12326, 2019.\n[43] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,\nPiyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised\nlearning of language representations. arXiv preprint arXiv:1909.11942 ,\n2019.\n[44] Stefan Larson, Anish Mahendran, Joseph J Peper, Christopher Clarke,\nAndrew Lee, Parker Hill, Jonathan K Kummerfeld, Kevin Leach,\nMichael A Laurenzano, Lingjia Tang, et al. An evaluation dataset\nfor intent classification and out-of-scope prediction. In Proceedings\nof the 2019 Conference on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP) , pages 1311â€“1316, 2019.\n[45] Yunseong Lee, Alberto Scolari, Byung-Gon Chun, Marco Domenico\nSantambrogio, Markus Weimer, and Matteo Interlandi. PRETZEL:\nOpening the black box of machine learning prediction serving sys-\ntems. In 13th USENIX Symposium on Operating Systems Design and\nImplementation (OSDI 18) , pages 611â€“626. USENIX Association, 2018.\n[46] Xiaoxiao Li, Ziwei Liu, Ping Luo, Chen Change Loy, and Xiaoou Tang.\nNot all pixels are equal: Difficulty-aware semantic segmentation via\ndeep layer cascade. In Proceedings of the IEEE conference on computer\nvision and pattern recognition , pages 3193â€“3202, 2017.\n[47] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi\nChen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoy-\nanov. Roberta: A robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692, 2019.\n[48] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y\nNg, and Christopher Potts. Learning word vectors for sentiment\nanalysis. In Proceedings of the 49th annual meeting of the association for\ncomputational linguistics: Human language technologies , pages 142â€“150,\n2011.\n[49] Richard Maclin and David Opitz. An empirical evaluation of bagging\nand boosting. AAAI/IAAI, 1997:546â€“551, 1997.\n[50] Ankur Manna, Rohit Kundu, Dmitrii Kaplun, Aleksandr Sinitca, and\nRam Sarkar. A fuzzy rank-based ensemble of cnn models for classifi-\ncation of cervical cytology. Scientific Reports , 11(1):1â€“18, 2021.\n[51] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really\nbetter than one? Advances in Neural Information Processing Systems ,\n32:14014â€“14024, 2019.\n[52] Laurel Orr, Atindriyo Sanyal, Xiao Ling, Karan Goel, and Megan\nLeszczynski. Managing ml pipelines: feature stores and the coming\nwave of embedding ecosystems. Proceedings of the VLDB Endowment ,\n14(12):3178â€“3181, 2021.\n14\nTabi: An Efficient Multi-Level Inference System for Large Language Models EuroSys â€™23, May 08â€“12, 2023, Rome, Italy\n[53] Titouan Parcollet and Mirco Ravanelli. The energy and carbon foot-\nprint of training end-to-end speech recognizers. 2021.\n[54] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.\nSquad: 100,000+ questions for machine comprehension of text. arXiv\npreprint arXiv:1606.05250, 2016.\n[55] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and\nCho-Jui Hsieh. Dynamicvit: Efficient vision transformers with dynamic\ntoken sparsification. Advances in neural information processing systems ,\n34:13937â€“13949, 2021.\n[56] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in\nbertology: What we know about how bert works. Transactions of the\nAssociation for Computational Linguistics , 8:842â€“866, 2020.\n[57] Francisco Romero, Qian Li, Neeraja J Yadwadkar, and Christos\nKozyrakis. INFaaS: Automated model-less inference serving. In 2021\nUSENIX Annual Technical Conference (USENIX ATC 21) , July 2021.\n[58] Matthew J. Salganik and Robin C. Lee. To apply machine learn-\ning responsibly, we use it in moderation - New York Times\nOpen, 2020. https://open.nytimes.com/to-apply-machine-learning-\nresponsibly-we-use-it-in-moderation-d001f49e0644 .\n[59] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.\nDistilbert, a distilled version of bert: smaller, faster, cheaper and lighter.\narXiv preprint arXiv:1910.01108 , 2019.\n[60] Victor Sanh, Thomas Wolf, and Alexander M. Rush. Movement prun-\ning: Adaptive sparsity by fine-tuning. 2020.\n[61] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. Green\nAI. Communications of the ACM , 63(12):54â€“63, 2020.\n[62] Haichen Shen, Lequn Chen, Yuchen Jin, Liangyu Zhao, Bingyu Kong,\nMatthai Philipose, Arvind Krishnamurthy, and Ravi Sundaram. Nexus:\na gpu cluster engine for accelerating dnn-based video analysis. In Pro-\nceedings of the 27th ACM Symposium on Operating Systems Principles ,\npages 322â€“337, 2019.\n[63] Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick. Training\nregion-based object detectors with online hard example mining. In\nProceedings of the IEEE conference on computer vision and pattern recog-\nnition, pages 761â€“769, 2016.\n[64] Vikram Sreekanti, Harikaran Subbaraj, Chenggang Wu, Joseph E Gon-\nzalez, and Joseph M Hellerstein. Optimizing prediction serving on\nlow-latency serverless dataflow. arXiv preprint arXiv:2007.05832 , 2020.\n[65] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy\nand policy considerations for deep learning in nlp. arXiv preprint\narXiv:1906.02243, 2019.\n[66] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand\nJoulin. Adaptive attention span in transformers. arXiv preprint\narXiv:1905.07799, 2019.\n[67] Siqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. Patient knowledge\ndistillation for bert model compression. In Proceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4323â€“4332, 2019.\n[68] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung.\nBranchynet: Fast inference via early exiting from deep neural networks.\nIn 2016 23rd International Conference on Pattern Recognition (ICPR) ,\npages 2464â€“2469. IEEE, 2016.\n[69] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention\nis all you need. In Advances in neural information processing systems ,\npages 5998â€“6008, 2017.\n[70] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan\nTitov. Analyzing multi-head self-attention: Specialized heads do the\nheavy lifting, the rest can be pruned. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics , pages 5797â€“\n5808, 2019.\n[71] Mariko Wakabayashi. Speeding up transformer CPU inference in\nGoogle Cloud - Twitter, 2021. https://blog.twitter.com/engineering/\nen_us/topics/insights/2021/speeding-up-transformer-cpu-inference-\nin-google-cloud .\n[72] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. Glue: A multi-task benchmark and analysis\nplatform for natural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP , pages 353â€“355, 2018.\n[73] Guanhua Wang, Zhuang Liu, Brandon Hsieh, Siyuan Zhuang, Joseph\nGonzalez, Trevor Darrell, and Ion Stoica. sensai: Convnets decomposi-\ntion via class parallelism for fast inference on live data. Proceedings of\nMachine Learning and Systems , 3, 2021.\n[74] Hanrui Wang, Zhekai Zhang, and Song Han. SpAtten: Efficient sparse\nattention architecture with cascade token and head pruning. In 2021\nIEEE International Symposium on High-Performance Computer Archi-\ntecture (HPCA), pages 97â€“110. IEEE, 2021.\n[75] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming\nZhou. Minilm: Deep self-attention distillation for task-agnostic com-\npression of pre-trained transformers. Advances in Neural Information\nProcessing Systems , 33:5776â€“5788, 2020.\n[76] Yiding Wang, Weiyan Wang, Duowen Liu, Xin Jin, Junchen Jiang, and\nKai Chen. Enabling edge-cloud video analytics for robotics applica-\ntions. In Proceedings of the IEEE International Conference on Computer\nCommunications, Virtual Conference , pages 10â€“13, 2021.\n[77] Yiding Wang, Weiyan Wang, Duowen Liu, Xin Jin, Junchen Jiang, and\nKai Chen. Enabling edge-cloud video analytics for robotics applica-\ntions. IEEE Transactions on Cloud Computing , 2022.\n[78] Yiding Wang, Weiyan Wang, Junxue Zhang, Junchen Jiang, and Kai\nChen. Bridging the edge-cloud barrier for real-time advanced vision\nanalytics. In HotCloud, 2019.\n[79] Lilian Weng. Attention? attention! 2018. https://lilianweng.github.io/\nlil-log/2018/06/24/attention-attention .html.\n[80] Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. Deebert:\nDynamic early exiting for accelerating bert inference. InProceedings of\nthe 58th Annual Meeting of the Association for Computational Linguistics ,\npages 2246â€“2251, 2020.\n[81] Hu Xu, Bing Liu, Lei Shu, and S Yu Philip. Bert post-training for\nreview reading comprehension and aspect-based sentiment analysis.\nIn Proceedings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) , pages 2324â€“2335, 2019.\n[82] Kaiqiang Xu, Xinchen Wan, Hao Wang, Zhenghang Ren, Xudong Liao,\nDecang Sun, Chaoliang Zeng, and Kai Chen. Tacc: A full-stack cloud\ncomputing infrastructure for machine learning tasks. arXiv preprint\narXiv:2110.01556, 2021.\n[83] Ran Xu, Jayoung Lee, Pengcheng Wang, Saurabh Bagchi, Yin Li, and\nSomali Chaterji. Litereconfig: cost and content aware reconfiguration\nof video object detection systems for mobile gpus. InProceedings of the\nSeventeenth European Conference on Computer Systems , pages 334â€“351,\n2022.\n[84] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua\nAinslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula,\nQifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences.\nIn NeurIPS, 2020.\n[85] Xiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer.\nScaling vision transformers. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition , pages 12104â€“12113, 2022.\n[86] Chengliang Zhang, Minchen Yu, Wei Wang, and Feng Yan. Mark: Ex-\nploiting cloud services for cost-effective, slo-aware machine learning\ninference serving. In 2019 {USENIX}Annual Technical Conference\n({USENIX}{ATC}19), pages 1049â€“1062, 2019.\n[87] Junxue Zhang, Chaoliang Zeng, Hong Zhang, Shuihai Hu, and Kai\nChen. Liteflow: towards high-performance adaptive neural networks\nfor kernel datapath. In Proceedings of the ACM SIGCOMM 2022 Confer-\nence, pages 414â€“427, 2022.\n15\nEuroSys â€™23, May 08â€“12, 2023, Rome, Italy Wang et al.\n[88] Li Lyna Zhang, Shihao Han, Jianyu Wei, Ningxin Zheng, Ting Cao,\nYuqing Yang, and Yunxin Liu. nn-meter: towards accurate latency\nprediction of deep-learning model inference on diverse edge devices.\nIn Proceedings of the 19th Annual International Conference on Mobile\nSystems, Applications, and Services , pages 81â€“93, 2021.\n[89] Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian McAuley, Ke Xu, and\nFuru Wei. Bert loses patience: Fast and robust inference with early\nexit. Advances in Neural Information Processing Systems , 33, 2020.\n16",
  "topic": "Inference",
  "concepts": [
    {
      "name": "Inference",
      "score": 0.8013357520103455
    },
    {
      "name": "Computer science",
      "score": 0.7291878461837769
    },
    {
      "name": "Discriminative model",
      "score": 0.5384656190872192
    },
    {
      "name": "Pruning",
      "score": 0.5216389298439026
    },
    {
      "name": "Generative model",
      "score": 0.4848405122756958
    },
    {
      "name": "Offset (computer science)",
      "score": 0.4839799404144287
    },
    {
      "name": "Latency (audio)",
      "score": 0.46985188126564026
    },
    {
      "name": "Generative grammar",
      "score": 0.42258888483047485
    },
    {
      "name": "Machine learning",
      "score": 0.39352524280548096
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35907724499702454
    },
    {
      "name": "Telecommunications",
      "score": 0.08909836411476135
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Agronomy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}