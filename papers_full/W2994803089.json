{
  "title": "Extending Machine Language Models toward Human-Level Language Understanding",
  "url": "https://openalex.org/W2994803089",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A4287747454",
      "name": "McClelland, James",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202179581",
      "name": "Hill, Felix",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221385001",
      "name": "Rudolph, Maja",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226671789",
      "name": "Baldridge, Jason",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2764041618",
      "name": "Schütze, Hinrich",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2052417512",
    "https://openalex.org/W1525136198",
    "https://openalex.org/W3008360102",
    "https://openalex.org/W2108503207",
    "https://openalex.org/W2949178656",
    "https://openalex.org/W2070811448",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W1977531436",
    "https://openalex.org/W2080915067",
    "https://openalex.org/W2008353316",
    "https://openalex.org/W2118373646",
    "https://openalex.org/W2530887700",
    "https://openalex.org/W3207021134",
    "https://openalex.org/W2142565290",
    "https://openalex.org/W2184677",
    "https://openalex.org/W2047057213",
    "https://openalex.org/W2157904933",
    "https://openalex.org/W2963267799",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2158899491",
    "https://openalex.org/W266716723",
    "https://openalex.org/W2620290674",
    "https://openalex.org/W2073257493",
    "https://openalex.org/W2090839168",
    "https://openalex.org/W2020755048",
    "https://openalex.org/W3103536442",
    "https://openalex.org/W2003232287",
    "https://openalex.org/W3014837850",
    "https://openalex.org/W2137351172",
    "https://openalex.org/W2060208675",
    "https://openalex.org/W3031914912",
    "https://openalex.org/W2087946919",
    "https://openalex.org/W2126278534",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2119051448",
    "https://openalex.org/W1974525874"
  ],
  "abstract": "Language is central to human intelligence. We review recent break- throughs in machine language processing and consider what re- mains to be achieved. Recent approaches rely on domain general principles of learning and representation captured in artificial neu- ral networks. Most current models, however, focus too closely on language itself. In humans, language is part of a larger system for acquiring, representing, and communicating about objects and sit- uations in the physical and social world, and future machine lan- guage models should emulate such a system. We describe exist- ing machine models linking language to concrete situations, and point toward extensions to address more abstract cases. Human language processing exploits complementary learning systems, in- cluding a deep neural network-like learning system that learns grad- ually as machine systems do, as well as a fast-learning system that supports learning new information quickly. Adding such a system to machine language models will be an important further step toward truly human-like language understanding.",
  "full_text": "Extending Machine Language Models toward\nHuman-Level Language Understanding\nJames L. McClellanda,b,2, Felix Hillb,2, Maja Rudolphc,2, Jason Baldridged,1,2, and Hinrich Schützee,1,2\naStanford University, Stanford, CA 94305, USA;bDeepMind, London N1C 4AG, UK; cBosch Center for Artiﬁcial Intelligence, Renningen, 71272, Germany; dGoogle Research,\nAustin, TX 78701, USA; eLMU Munich, Munich, 80538, Germany\nThis manuscript was compiled on December 13, 2019\nLanguage is central to human intelligence. We review recent break-\nthroughs in machine language processing and consider what re-\nmains to be achieved. Recent approaches rely on domain general\nprinciples of learning and representation captured in artiﬁcial neu-\nral networks. Most current models, however, focus too closely on\nlanguage itself. In humans, language is part of a larger system for\nacquiring, representing, and communicating about objects and sit-\nuations in the physical and social world, and future machine lan-\nguage models should emulate such a system. We describe exist-\ning machine models linking language to concrete situations, and\npoint toward extensions to address more abstract cases. Human\nlanguage processing exploits complementary learning systems, in-\ncluding a deep neural network-like learning system that learns grad-\nually as machine systems do, as well as a fast-learning system that\nsupports learning new information quickly. Adding such a system to\nmachine language models will be an important further step toward\ntruly human-like language understanding.\nLanguage Understanding | Natural Language Processing| Situation\nModels | Machine Language Models| Brain System for Understanding\nMany of the most impressive recent successes of machine in-\ntelligence have appeared in the domain of language. Machines\ncan now better identify the words we speak and respond in\never more natural sounding voices. More impressive still is\nmodern machine translation (1). Anyone with a smartphone\nhas access to applications that allow them to say a sentence in\none language and then see and hear its translation in another.\nHuman ability still far exceeds machines in most language\ntasks, but these systems work well enough to be used by\nbillions of people everyday.\nWhat underlies these successes, and what limitations do\nthese systems face? We argue that successes to date come from\never more eﬀective methods for exploiting principles of neural\ncomputation that human language users also exploit. We then\nnote that the work remains limited in that it largely treats\nlanguage separately from the larger task of understanding\nthe world around us. This leads us to propose an integrated\napproach to building a system that truly understands, in which\nlanguage plays a key role in concert with other sources of input.\nWe discuss the challenges facing further development of the\napproach and propose future steps toward addressing these\nchallenges.\nPrinciples of Neural Computation\nThe principles of neural computation are domain general prin-\nciples inspired by the human brain. They were ﬁrst articulated\nin the 1950s (2) and further developed in the 1980s in the\nParallel Distributed Processing (PDP) framework for modeling\ncognition (3). A central idea of this approach is that struc-\nture in language and other cognitive domains is an emergent\nphenomenon, captured in learned connection weights and re-\nsulting in context-sensitive, distributed representations whose\ncharacteristics reﬂect a gradual, input-statistics dependent,\nlearning process (4). The models treat the symbols and rules\nof classical linguistic theory as consequences of processing and\nlearning, not entities whose structure must be built in. Instead\nof discrete symbols for linguistic units, these models rely on\npatterns of activity often calledembeddings over arrays of\nneuron-like processing units. Instead of explicit systems of\nrules, they rely on learned matrices of connection weights to\nmap patterns on one set of units into patterns on others.\nAnother key principle is mutual constraint satisfaction (5).\nFor example, the meaning of a sentence depends on its struc-\nture (its organization into constituent phrases); but so too can\nthe structure depend on the meaning. Consider the sentenceA\nboy hit a man with a __. If the missing word isbat, with a bat\nis read as part of a verb phrase headed byhit, and speciﬁes the\ninstrument used to carry out the action. But ifbeardﬁlls the\nblank, with a beardis a part of a noun phrase describing who\nwas hit. Even the segmentation of spoken or written language\ninto elementary segments (e.g., letters) depends in part on\nmeaning and context, as illustrated in Fig. 1. Rumelhart (5)\nsketched an interactive model of language understanding in\nwhich estimates of probabilities about all aspects of an input\nconstrain estimates of the probability of every other aspect.\nThe idea was captured in a model of context eﬀects in per-\nception (6). Later work (7, 8) linked these ideas to energy\nminimization in statistical physics. Neural language modeling\nresearch, which we now describe, incorporates these principles.\nNeural Language Modeling\nAn Early Neural Language Model.Elman (9) built on the prin-\nciples above to demonstrate how neural models can capture\nkey characteristics of language structure through learning, a\nfeat once considered impossible (10). The model provides a\nstarting point for understanding recent developments. Elman\nused the recurrent neural network shown in Fig. 2a. The\nnetwork was trained to predict the next word in a sequence\n(w(t+ 1)) based on the current word (w(t)) and its ownhidden\n(that is, learned internal) representation from the previous\ntime step (h(t − 1)). These two inputs are each multiplied\nby a matrix of connection weights (represented by the arrows\nlabeled Whi and Whh) to produce vectors that are added to\nproduce a vector of inputs to the hidden layer of units. The\nJM, FH, MR, JB, and HS wrote the paper.\nThe authors declare no conﬂict of interest.\n1J.B. and H.S. contributed equally.\n2 To whom correspondence should be addressed. E-mail: jlmccstanford.edu, felixhill@google.com,\nmarirudolph@gmail.com, jasonbaldridge@google.com or inquiries@cislmu.org\n1\narXiv:1912.05877v1  [cs.CL]  12 Dec 2019\nFig. 1. Two handwritten sentences illustrating how context inﬂuences the identiﬁcation\nof letters in written text. The visual input we read as went in the ﬁrst sentence and\nevent in the second is the same bit of Rumelhart’s handwriting, cut and pasted into\neach of the two contexts. Reprinted from (5).\nelements of this vector undergo a transformation limiting the\nrange of activation values, resulting in the hidden layer repre-\nsentation. This in turn is multiplied with the matrix of weights\nto the output layer from the hidden layer (Woh) to generate a\nvector used to predict the probability of each of the possible\nsuccessor words. Learning in this and other neural models is\nbased on the discrepancy between the network’s output and\nthe actual next word; the values of the connection weights\nare adjusted by a small amount to reduce the discrepancy.\nThe network is called recurrent because the same connection\nweights (denoted by arrows in the ﬁgure) are used to process\neach successive word; the hidden representationh(t) becomes\nthe context representationh(t − 1) for the next time step.\nElman demonstrated two crucial ﬁndings. First, after train-\ninghisnetworktopredictthenextwordinsimplesentenceslike\nman eats bread, dog chases cat, andgirl sleeps, the network’s\nrepresentations captured the syntactic distinction between\nnouns and verbs (9). It also captured interpretable subcate-\ngories, as shown by a hierarchical clustering of the hidden-layer\npattern (h(t)) for each word in the training materials (Fig. 2b).\nThis illustrates a key feature of learned representations in neu-\nral models: they capture speciﬁc as well as general or abstract\ninformation. By using a diﬀerent learned representation for\neach word, the speciﬁc predictive consequences of that word\ncan be exploited. Because representations for words that make\nsimilar predictions are similar, and because neural networks\nexploit similarity, the network can share knowledge about\npredictions among related words. Second, Elman (11) used\nboth simple sentences likeboy chases dogsand more complex\nones likeboy who sees girls chases dogs. In the more complex\ncase, the verbchases must agree with the ﬁrst noun (boy), not\nthe closest noun (girls), since the sentence contains a main\nclause (boy chases dogs) interrupted by an embedded clause\n(boy [who] sees girls). The model learned to predict the verb\nform correctly despite the intervening clause, showing that it\nacquired sensitivity to the syntactic structure of language, and\nnot just local co-occurrence statistics in its learned connections\nand distributed representations.\nScaling Up to Process Natural Text.Elman’s task—predicting\nthe next word in a sequence—has been central to neural lan-\nguage modeling. However, Elman trained his networks with\nonly tiny fragments of what were eﬀectively toy languages.\nFor many years, it seemed they would not scale up. Beginning\nabout 10 years ago, advances in machine language process-\ning began to overcome this limitation for neural models. We\ndescribe two crucial developments next.\nLong-distance dependencies and pretrained word embeddings. A\nchallenge for language prediction is the indeﬁnite length of\nFig. 2. (a) Elman’s (1990) simple recurrent network and (b) his hierarchical clustering\nof the representations it learned, reprinted from (9).\nthe context that might be relevant. Consider this passage:\nJohn put some beer in a cooler and went out with his\nfriends to play volleyball. Soon after he left, someone\ntook the beer out of the cooler. John and his friends\nwere thirsty after the game, and went back to his\nplace for some beers. When John opened the cooler,\nhe discovered that the beer was ___.\nHere a reader expects the missing word to begone. Yet if\nwe replacedsomeone took the beerwith someone took the ice\nthe expected word would bewarm instead. Furthermore, any\namount of additional text betweenbeer and gone would not\nchange the predictive relationship. Elman’s network could\ntake only a few words of context into account, reﬂecting a\nlarger challenge known as thevanishing gradientproblem (12).\nIn essence, the magnitude of the learning signal that deter-\nmines the adjustments to connection weights tends to decrease\nexponentially as the number of layers of weights between an\ninput and an output increases. The development of neural\nnetwork modules called Long-Short-Term-Memory (LSTM)\nmodules (13) that partially overcame this limitation was there-\nfore crucial, greatly increasing the contextual range of neural\nmodels. In another crucial development, researchers began\nto use pre-trained word embeddings derived from learning\npredictive relationships among words (14, 15). When training\na neural model for a speciﬁc task, such embeddings could then\nbe used to directly represent training words in the model’s\ninput. The embeddings were based on the aggregate statistics\nof large text corpora, and captured both general and speciﬁc\npredictive relationships, supporting generalization at both gen-\neral and speciﬁc levels. Using these embeddings, task-focused\nmodels trained with relatively small data sets could generalize\nwhat they learned from training on frequent words (such as\nsofa) to infrequent words with similar predictive relationships\n(such assettee).\nA limitation of the above approach is that the same rep-\nresentation of a word is used every time it occurs, regardless\nof context. However, in line with the principle of mutual\nconstraint satisfaction, humans interpret words, including am-\nbiguous words likebank, in accordance with the context (16),\nrapidly assigning a contextually appropriate meaning to each\nword based on all other words. A ﬁxed embedding also limits\npredicting other words; the predictive implication of the word\nbank depends on which kind of bank is involved. Initial steps\nMcClelland et al.: Language understanding in humans and machines 2\nFig. 3. High-level depiction of one stage of the bidirectional attention architecture,\nshown constructing a contextually appropriate representation of bank based on other\nwords in the same sentence. Diagonal lines show how inputs from other positions\nreach bank’s position; the same occurs at all word positions. See text for details.\ntoward context sensitivity (17) recognize this limitation. We\nconsider a fuller solution in the next section.\nAttention and fully contextualized embeddings. Breakthroughs in\nneural language modeling have come from recent models that\nconstruct fully contextualized word representations (18–22).\nThe models represent words via a mutual constraint satis-\nfaction process in which each word in a text span inﬂuences\nthe representation of every other word. BERT (20) is a key\nmodel in this class, illustrated in Fig. 3 as it encodes the end\nof the sentenceJohn reached the bank of the river(example\nfrom (23)). An initial context-independent representation of\neach word is ﬁrst combined with a positional representation\n(bottom row of boxes in the ﬁgure). Then, a separate copy of\nthe same neural network module updates the representation in\neach position with input from all other positions. This process\nuses queries (red) at each position that are compared to keys\n(yellow) at all positions to form weightings (mauve boxes) that\ndetermine how strongly the values (blue) from each position\ncontribute to the combined attention vectors (grey boxes) that\nprovide context-sensitivity. The computation iterates over\nmany layers, allowing words whose representations have been\ninﬂuenced by their context to inﬂuence the representations of\nother words. The process allows selection among alternative\ndistinct meanings of a word likebank as well as graded shading\nof word meaning by context, for example assigning diﬀerent\nemotional valance to the dogs inthe dog wagged its tailand\nthe dog snarled.\nKey ingredients of the contemporary models are (i)bidi-\nrectionality of information ﬂow during processing and (ii) an\nattention mechanism, which replaces the LSTM mechanism to\nenhance the exploitation of context. Bidirectionality matters\nbecause the meaning of a word in context depends on what\ncomes after it as well as what comes before; in the example\nsentence, the last word,river, determines the meaning ofbank.\nWhile it is remarkable how much can be done with strictly left-\nto-right constraint propagation (24), bidirectionality allows\nneural models to implement a mutual constraint satisfaction\nprocess in which the representation of each word depends on\nall other words. The attention mechanism distinguishes these\nmodels from earlier LSTM-based neural language models, and\nis used in both bidirectional and left-to-right models. Atten-\ntion has proven to be even more eﬀective than the LSTM\nmechanism in allowing networks to capture long-distance de-\npendencies. Rather than requiring information about a context\nword to reach a target word through an iteration of the LSTM\nfor every intervening word, the context reaches the target\ndirectly. Likewise, gradient learning signals skip over the inter-\nvening words, avoiding the dissipation of learning signal that\nwould otherwise occur.\nBERT-based models have produced remarkable improve-\nments on a wide range of language tasks (25, 26). The models\ncan be pre-trained on massive text corpora, providing useful\nrepresentations for subsequent tasks for which little speciﬁc\ntraining data exists (27). These models seem to capture syn-\ntactic, semantic and world knowledge and they are beginning\nto address tasks once thought beyond their reach. For ex-\nample, Winograd Schema challenge(28) requires determining\nthe referent of a pronoun (hereit) in a sentence such asThe\ntrophy did not ﬁt in the suitcase because it was too ___. For a\nperson, world knowledge tells us that if the missing word isbig\nthe referent must be the trophy, but if it issmall the referent\nmust be the suitcase. The latest models achieve ever-higher\nscores on benchmarks including variants of the Winograd chal-\nlenge (29). However, variants of test materials that do not\nfool humans continue to stymie even the best models (30), and\nfurther reﬁnements in the models and their assessment will be\nrequired before it will be clear what such models can achieve.\nThe Human Integrated Understanding System (IUS)\nSituations and objects. Despite the successes of neural lan-\nguage modeling, an important limitation is that these models\nare purely language based. We need models in which lan-\nguage is a part of anintegrated understanding system(IUS)\nfor understanding and communicating about the situations\nwe encounter and the objects that participate in them. Rep-\nresentations of situations constitute our models of our world\nand guide our behavior and our interpretation of language.\nIndeed, resolving the referent of the pronoun in a Winograd\nsentence would follow from building a representation of the\nsituation the sentence describes. In the situation in which a\ntrombone does not ﬁt in a suitcase, the natural reason would\nbe that the trombone is too big or the suitcase too small; the\nidentity of the referent of the pronoun follows from realizing\nthis. Thus, solving the Winograd Schema challenge is a natu-\nral byproduct of the human language understanding process.\nIn short, we argue that language evolved for communication\nabout situations and our systems should address this goal.\nSituations can be concrete and static, such as one where\na cat is on a mat, or they may be events such as one where\na boy hits a ball. They can be conceptual, social or legal,\nsuch as one where a court invalidates a law. They may even\nbe imaginary. The objects may be real or ﬁctitious physical\nobjects or locations; animals, persons, groups or organizations;\nbeliefs or other states of mind; or entities such as theories,\nlaws or constitutions. Here we focus on concrete situations,\nconsidering other cases below. Our proposal builds on classic\nwork in linguistics (31, 32), human cognition (33), artiﬁcial\nintelligence (34), and an early PDP model (35) and dovetails\nwith an emerging perspective in cognitive neuroscience (36).\nAs humans process language, we construct a representation\nof the situation the language describes from the stream of\nwords and other available information. Words and their se-\nquencing serve asclues to meaning(37) that jointly constrain\nthe understanding of the situation and each object participat-\ning in it (35). Consider this passage:\nJohn spread jam on a slice of bread. The knife had\nMcClelland et al.: Language understanding in humans and machines 3\nbeen dipped in poison. John ate the bread and soon\nbegan to feel sick.\nWe can make many inferences here: that the jam was spread\nwith the poisoned knife, that some of the poison was trans-\nferred to the bread, and that this may have led to John’s\nsickness. Note that the entities here are objects, not words,\nand the situation could instead be conveyed by a silent movie.\nEvidence that humans construct situation representations\ncomes from classic work by Bransford and colleagues (33, 38).\nThis work demonstrates that (1) we understand and remember\ntexts better when we can relate the statements in the text\nto a familiar situation; (2) information that conveys aspects\nof the situation can be provided by a picture accompanying\nthe text; (3) the characteristics of the objects we remember\ndepend on the situations in which they occurred in a text;\n(4) we represent in memory objects not explicitly mentioned\nin texts; and (5) after hearing a sentence describing spatial\nor conceptual relationships among objects, we retain mem-\nory for these relationships rather than the linguistic input.\nFurther, evidence from eye movements shows that people use\nlinguistic and non-linguistic input jointly and immediately as\nthey process language in context (39). For example, just after\nhearing The man will drink ...participants look at a full wine\nglass rather than an empty beer glass (40). After hearingThe\nman drank, they look at the empty beer glass. Thus, language\nunderstanding involves constructing–in real time–a represen-\ntation of the situation being described by language input,\nincluding the objects involved and their spatial relationships\nwith each other, using visual and linguistic inputs.\nThe understanding system in the brain. Fig. 4 presents a de-\npiction of our proposed integrated understanding system.Our\nproposal is both a theory of the brain basis of understanding\nand a proposed architecture for future language understanding\nresearch. It is largely consistent with proposals in (36). First,\nwe focus on a part of the system, called the neocortical sys-\ntem, that is suﬃcient to combine linguistic and non-linguistic\ninput to understand the object and situation referred to upon\nhearing a sentence containing the wordbat while observing\na corresponding situation in the world. This system consists\nof the blue ovals (corresponding to pools of neurons in the\nbrain) and blue arrows (connections between these pools) in\nthe ﬁgure. One population subserves a visual representa-\ntion/embedding of the given situation, and another subserves\na non-semantic linguistic representation capturing the sound\nstructure (phonology) of co-occurring spoken language. The\nthird represents objects participating in the situation, and\nthe fourth represents the overall situation itself. Within each\npool, and between each connected pair of pools, the neurons\nare reciprocally interconnected via learning-dependent path-\nways allowing mutual constraint satisfaction among all of the\nelements of each of the embedding types. Brain regions for\nrepresenting visual and linguistic inputs are well-established,\nand the evidence for their involvement in a mutual constraint\nsatisfaction process is substantial (41). Here we focus on the\nevidence for object and situation representations in the brain.\nObject representations. A brain area near the front of the tempo-\nral lobe houses neurons whose activity provides an embedding\ncapturing the properties of an object someone is considering\n(42). Damage to this area impairs the ability to name objects,\nto grasp objects correctly in service of their intended use, to\nFig. 4. Proposed integrated understanding system (IUS). The blue box contains\nthe neocortical system, with each oval forming an embedding (representation) of a\nspeciﬁc kind of information. Blue arrows represent learned connections that allow\nthe embeddings to constrain each other. The red box contains the medial temporal\nlobe system, thought to provide a network that stores an integrated embedding of the\nneocortical system state. The red arrow represents fast-learning connections that\nbind the elements of this embedding together for later reactivation and use. Green\narrows connecting the red and blue ovals support bidirectional inﬂuences between\nthe two systems. (A) and (B) are two example inputs discussed in the main text.\nmatch objects with their names or the sounds that they make,\nand to pair objects that go together with each other, either\nfrom their names or from pictures. Models that capture these\nﬁndings (43) treat this area as the hidden layer of an interac-\ntive, recurrent network with bidirectional connections to other\nlayers corresponding to brain areas that represent diﬀerent\ntypes of object properties including the object’s name. In these\nmodels, an input to any of these other layers activates the cor-\nresponding pattern in the hidden layer, which in turn activates\nthe corresponding patterns in the other layers, supporting, for\nexample, the ability to produce the name of an object from\nvisual input. Damage (simulated by removal of neurons in the\nhidden layer) degrades the model’s representations, capturing\nthe patterns of errors made by patients with the condition.\nSituation representations. The situation representation speciﬁes\nthe event or situation conveyed by visual and/or language\ninput. Evidence from behavioral studies indicates that con-\nstruction of a situation representation can occur with or with-\nout language input (44) or through the convergent inﬂuence\nof both sources of information (40). Cognitive neuroscience\nresearch supports the idea that the situation representation\narises in a set of interconnected brain areas primarily located\nin the frontal and parietal lobes (36, 45). In recent work, brain\nimaging data is used to analyze the time-varying patterns\nof neural activity that arise during the processing of a tem-\nporally extended event sequence. The activity patterns that\nMcClelland et al.: Language understanding in humans and machines 4\nrepresent corresponding events in a sequence are largely the\nsame, whether the information about the sequence comes from\nwatching a movie, hearing or reading a narrative description,\nor recalling the movie after having seen it (46, 47).\nSituation-speciﬁc constraints. An important feature of our brain-\ninspired proposal is the use of distinct situation and object\nrepresentations, and the idea that the constraints on the par-\nticipating objects are mediated by the situation representation.\nThe advantage of this is that it allows these constraints to\nbe situation-speciﬁc. For example, the dogs in the events de-\nscribed by the sentencesthe boy ranto the dogand the boy ran\nfrom the dogare likely to be diﬀerent, and a comprehender will\nrepresent them diﬀerently (38). In general, context-sensitivity\nis best captured by a mediating representation rather than\ndirect associations among constituents (48). While BERT-like\nmodels might partially capture such constraints implicitly, an\nintegrated situation representation may be more eﬀective.\nIn summary, the brain contains distinct areas that represent\neach input modality and the objects and situations conveyed\nthrough them, computing these representations through a\nmutual constraint satisfaction process combining language and\nother inputs. Emulating this architecture in machines could\ncontribute to achieving human-level language understanding.\nWhat would a computational instantiation of our system look\nlike? It is likely that a biologically realistic version would\ndiﬀer in some ways from the most eﬀective machine version.\nContemporary attention-based language models can deploy\nattention over tens to thousands of words kept in their current\nsystem state, but evidence from brain imaging data collected\nduring movie comprehension suggests that activation states in\nvisual, speech, and object areas change rapidly as events unfold,\nwhile the brain state tends to be more constant, changing only\nat event boundaries in brain areas associated with situation\nrepresentations (47). In humans, spanning longer temporal\nwindows, including multi-event narratives, appears to require\nthe complementary learning system we consider next.\nComplementary Learning Systems. Learning plays a crucial\nrole in understanding. The knowledge in the connection\nweights in the neural networks we have described is acquired\nthrough the accumulation of very small adjustments based on\neach experience. The connection weights gradually become\nsensitive to subtle higher-order statistical relationships, taking\nmore and more context into account as learning continues (49),\nand exhibiting sensitivity both to general and recurring speciﬁc\ninformation (e.g., names of close friends and famous people).\nIn our proposed architecture, this gradual process occurs in\nall the pathways represented by the blue arrows in Fig. 4, just\nas it does in the artiﬁcial neural language models considered\nabove. However, this learning mechanism is not well suited to\nacquiring new information rapidly, and attempting to learn\nspeciﬁc new information quickly by focused repetition leads\nto catastrophic interference with what is already known (50).\nYet, humans can often rely on information presented just\nonce at an arbitrary time in the past to inform our current\nunderstanding. Returning to the beer John left in the cooler,\nto anticipate that John will not ﬁnd the beer when he opens\nthe cooler again, we must rely on information acquired when\nwe ﬁrst heard about the beer being stolen. Such situations\nare ubiquitous, and a learning system must be able to exploit\nsuch information, but BERT and the other models described\npreviously are limited in this way. Though some models hold\nlong word sequences in an active state, when one text is\nreplaced with another, only the small connection adjustments\ndescribed above remain, leaving these systems without access\nto the speciﬁcs of the prior information.\nThe human brain contains a system that addresses this\nlimitation. Consider a situation in which someone sees a pre-\nviously unfamiliar object and hears a spoken statement about\nit, as illustrated in Fig. 4B. The visual input provides one\nsource of information about the object (a previously unfamiliar\nanimal), while the linguistic input provides its name. Humans\nshow robust learning after just two brief exposures to such\npairings (51). This form of learning depends on the hippocam-\npus and adjacent areas in the medial temporal lobes (MTL)\nof the brain (51). While details of the role of the MTL in\nlearning and memory continue to be debated (52, 53), there is\nconsensus that the MTL is crucial for the initial formation of\nnew memories, including memories for speciﬁc events and their\nconstituent objects and situations, while general knowledge,\nthe ability to understand language, and previously acquired\nskills are unaﬀected by MTL damage.\nThe evidence from MTL damage suggests there is a fast\nlearning system in the MTL. According to complementary\nlearning systems theory (CLST) (54–56) this system (shown\nin red in Fig. 4) provides an integrated representation of the\nunderstanding system state, and employs modiﬁable connec-\ntions within the MTL (red arrow) that can change rapidly to\nsupport new learning based on a single experience. The green\narrows represent connections that carry information between\nthe neocortical (blue) and MTL (red) systems so the systems\ncan inﬂuence each other.\nLet us consider how, according to CLST, a human can learn\nabout the numbat (see Fig. 4B) from an experience seeing\nit and hearing a sentence about it (56). The input to the\nMTL is thought to be an embedding that captures (i.e., can\nbe used to reconstruct) the patterns in the neocortical areas\nthat arise from the experience. Networks within the MTL (not\nshown) map the MTL input representation to a sparser one\ndeep inside the MTL, maximizing distinctness and minimizing\ninterference among experiences (54). Large connection weight\nchanges within the MTL associate the elements of the sparse\nrepresentation with each other and with the MTL input repre-\nsentation. When the person hears the wordnumbat in a later\nsituation, connections to the MTL from the neocortex activate\nneurons in the MTL input representation. The weight changes\nthat occurred on prior exposure support the reconstruction\nof the complete MTL representation, and return connections\nto the neocortex then support approximate reconstruction of\nthe visual, speech, object and situation representations formed\nduring the initial exposure to the numbat. These representa-\ntions are the explicit memory for the prior experience, allowing\nthe cortical network to use what it learned from one prior\nexposure to contribute to understanding the new situation.\nIntegrating information into the neocortex. How can knowledge ini-\ntially dependent on the MTL be integrated into the neocor-\ntex? According to CLST (55), the neocortex learns gradually\nthrough interleaved presentations of new and familiar items;\nthis process avoids interference of new items with what is al-\nready known. Interleaved learning can occur through ongoing\nexperience, as would happen if, for example, we acquire a pet\nnumbat that we then see every day, while continuing to have\nMcClelland et al.: Language understanding in humans and machines 5\nother experiences. Interleaving may also occur during rest\nor sleep through reactivation and replay of patterns stored\nin the MTL: Indeed, spontaneous replay of short snippets of\npreviously experienced episodes occurs within the MTL during\nsleep and between behavioral episodes (see (56) for review).\nIn summary, the human brain contains complementary\nlearning systems that support the simultaneous use of many\nsources of information as we seek to understand an experienced\nsituation. One of these systems acquires an integrated system\nof knowledge gradually through interleaved learning, includ-\ning our knowledge of the meanings of words, the properties\nof frequently-encountered objects, and the characteristics of\nfamiliar situations. The other complements this system to\nallow information from speciﬁc experiences to be brought to\nbear on the interpretation of a current situation.\nToward an Artiﬁcial Integrated Understanding System\nHere we consider current deep learning research that is taking\nsteps consistent with our IUS proposal, and point toward fu-\nture directions that will be needed to achieve a truly integrated\nand fully functional understanding system. We begin within\nthe context of language grounded within concrete visual and\nphysical situations, then consider the role of memory, and\nﬁnally turn to the extension of the approach to address under-\nstanding of more abstract objects, situations, and relations.\nMapping vision and language to representations of objects.\nHow might a model learn about situations that can occur\nin the world? The need for an artiﬁcial system of language\nunderstanding to be grounded in the external world has long\nbeen discussed. An early example is Winograd’s SHRDLU\nsystem (57), which produced and responded to language about\na simulated physical world. Deep learning has enabledjoint,\nend-to-end training of perceptual input and language (i.e., in\na single synchronous optimization process). Recent advances\nwith such models have greatly improved performance, resulting\nin applications transforming user experiences. When presented\nwith a photograph, networks can now answer questions such\nas what is the man holding? or what color is the woman’s\nshirt? (58), demonstrating an ability to combine information\nfrom vision and language to understand a class of situations.\nA very recent model (59) explicitly represents the objects in\na scene, their properties, and their relations to other objects in\na designedscene graphwith slots for objects, their properties,\nand relations. It encodes questions as a series of instructions\nto ﬁnd a target object or relation by searching the graph to\nanswer a query. For example, the questionwhat is the object\nbeside the yellow bowl?can be answered by ﬁnding the yellow\nbowl, ﬁnding an object linked to it with the ‘beside’ relation,\nand then reading out this object’s identity. The approach\nadvances the state of the art, though a large gap relative\nto humans remains. The model shares important properties\nwith our proposal in that it explicitly treats language input\nas querying the model’s representation of the objects in the\nscene, and their conceptual properties. A natural extension\nconsistent with IUS would be to build up scene representations\nusing a combination of visual input and language, allowing\ntext to enrich the representations of objects and relations.\nA question this work raises is whether to build structured\nrepresentations into one’s model. This is advocated in (59),\nbut natural structure exhibits ﬂexible embedding relationships\nand is often only approximately characterized by explicit tax-\nonomies, motivating use of emergent connection-based rather\nthan hard-coded representational structures (60). A challenge,\nthen, is to achieve comparable performance with models in\nwhich these concept-based object and relational representa-\ntions emerge through learning.\nEmbodied models for language understanding. Beyond the\nintegration of vision and language, as illustrated in Fig. 4, we\nsee progress coming from an even fuller integration of many\nadditional information sources. Every source provides a basis\nfor distinct learning objectives and enables information that\nis salient in one source to bootstrap learning and inference in\nthe other. Important additional sources of information include\nnon-language sound, touch and force-sensing, and information\nabout one’s own actions.\nIncorporating additional information sources will allow an\nIUS to go beyond answering questions about static images.\nSince image data has no temporal aspect, such models lack\nexperience of events or processes. While models that jointly\nprocess video and language (61) may acquire some sensitiv-\nity to event structure and commonplace causal relationships,\nthese systems do not make choices aﬀecting the world they\nobserve. Ultimately, an ability to link one’s actions to their\nconsequences as one intervenes in the observed ﬂow of events\nand interacts with other agents should provide the strongest\nbasis for acquiring notions of cause and eﬀect, of agency, and\nof self and others.\nThese considerations motivate recent work on agent-\nbased language learning in simulated interactive 3D environ-\nments (62–65). In (66), an agent was trained to identify, lift,\ncarry and place objects relative to other objects in a virtual\nroom, as speciﬁed by simpliﬁed language instructions. At each\ntime step, the agent received a ﬁrst-person visual observation\n(pixel-based image) that it processed to produce a representa-\ntion of the scene. This was concatenated to the ﬁnal state of\nan LSTM that processed the instruction, then passed to an\nintegrative LSTM whose output was used to select a motor\naction. The agent gradually learned to follow instructions of\nthe formﬁnd a pencil, lift up a basketballand put the teddy\nbear on the bed, encompassing 50 objects, and requiring up\nto 70 action steps to complete. Such instructions require the\nconstruction of representations based on language stimuli that\nenable the identiﬁcation of objects and relations across space\nand time, and the integration of this information to inform mo-\ntor behaviors. Importantly, without building in explicit object\nrepresentations, the system supported the interpretation of\nnovel instructions. For instance, an agent trained to lift a set\nof 20 objects in the environment, but only trained to put 10 of\nthose in a speciﬁc location could place the remaining objects\nin the same location on command with over 90% accuracy.\nNeural models often fail to exhibit systematic generaliza-\ntion, leading some to propose that more structure should be\nbuilt in (67). While the agent’s level of systematicity does\nnot reach human levels, this work suggests that grounding lan-\nguage learning can help support systematicitywithout building\nit in. Critically, the agent’s systematicity was contingent on\nthe ego-centric, multimodal and temporally-extended expe-\nrience of the agent. On the same set of generalization tests,\nboth an alternative agent with a ﬁxed perspective on a 2D grid\nworld and a static neural network classiﬁer that received only\nindividual still image stimuli exhibited signiﬁcantly worse gen-\nMcClelland et al.: Language understanding in humans and machines 6\nFig. 5. Left: the (allocentric) agent perspective in a 2D grid-world. The text indicates\nthe language instruction, requiring the agent (white striped cell) to visit one of the\nred ﬁgures and move it to the white square. Right: the ﬁrst-person perspective of the\nsituated agent, addressing an equivalent task to the one posed in the grid world.\neralization that a fully situated agent (Fig. 5). This underlines\nhow aﬀording neural networks access to rich, multi-modal\ninteractive environments can stimulate the development of\ncapacities that are essential for language learning.\nDespite these promising signs, achieving fully human levels\nof generalization remains an important challenge. We propose\nthat incorporating an MTL-like fast learning system will help\naddress this by allowing new words to be linked to the corre-\nsponding object from just a single episode supporting use of\nthe word to refer to the referent in other situations.\nAn artiﬁcial fast learning system. What might a fast learning\nsystem in an implementation of an integrated understanding\nsystem look like? The memory system in the diﬀerentiable\nneural computer (DNC) (68) is one possibility. These systems\nstore embeddings derived from past episodes in slots that\ncould store Integrated System State representations like those\nwe attribute to the human MTL. Alternatively, they could\nstore the entire ensemble of states across the visual, speech,\nobject, and situation representations. Though we do not\nbelieve the brain has a separate slot for each memory, it can\nbe useful to model it as though it does (56), and artiﬁcial\nsystems with indeﬁnite capacity could exceed human abilities\nin this regard. How might the retrieval of relevant information\nwork in such a system? The DNC employs a querying system\nsimilar to the one in BERT and to proposals in the human\nmemory literature, whereby the representation retrieved from\nthe MTL is weighted by the degree of match between a query\n(which we would treat as coming from the neocortex) and each\nvector stored in memory. Close matches are favored in this\ncomputation (69), so that when there is a unique match (such\nas a single memory containing a once seen word likenumbat),\nthe corresponding object and situation representation could\nbe retrieved. Retrieval could be based on a combination of\ncontext and item information, similar to human memory (70).\nWorking out the details of such a system presents an exciting\nresearch direction for the future.\nBeyond concrete situations. Our discussion has focused pri-\nmarily on concrete situations. However, language allows us to\ndiscuss abstract ideas and situations, where grounding in the\nphysical world can be very indirect and our learning about\nit comes primarily from language-based materials. Consider,\nfor example, an understanding ofBrexit. Concrete events\ninvolving actual people have occurred, but the issues and ques-\ntions under consideration can only be communicated through\nlanguage. What is the way forward toward developing models\nthat can understand such a complex situation?\nLanguage may have evolved in part to support transmission\nof complex, hierarchical knowledge about tools (71). How-\never, utterances also had to support abstraction and complex\ndependencies between concrete objects, as well as social re-\nlationships between speakers. Words themselves provided a\nnew abstract substrate for characterizing other words (72).\nWord embeddings are one implementation of this substrate:\nthey can characterize abstract words likejustice and represents\nwithout directly grounding them. In humans, encyclopedic\nknowledge grounded in such representations can be acquired\nin an MTL-dependent way from reading an encyclopedia arti-\ncle just once. For machines, forming integrated system state\nrepresentations capturing the content and using a DNC-like\nsystem for their storage and retrieval might provide a starting\nplace for enabling such knowledge to be acquired and used\neﬀectively.\nThatsaid, wordsareutteredinrealworldcontextsandthere\nis a continuum between grounding and language-based linking\nfor diﬀerent words and diﬀerent uses of words. For example,\ncareeris not only linked to other abstract words likework and\nspecialization but also to more grounded concepts such aspath\nand its extended metaphorical use for discussing the means to\nachieve goals (72). Embodied, simulation-based approaches\nto meaning (73, 74) build on this observation to bridge from\nconcrete to abstract situations via metaphor. They posit\nthat understanding words likegrasp is directly linked to neural\nrepresentations of the action of grabbing and that this circuitry\nis recruited for understanding the word in contexts such as\ngrasping an idea. We consider situated agents as a critical\ncatalyst for learning about how to represent and compose\nconcepts pertaining to spatial, physical and other perceptually\nimmediate phenomena—thereby providing a grounded ediﬁce\nthat can connect to both the low level brain circuitry for motor\naction and to representations derived primarily from language.\nConclusion. Language does not stand alone. The integrated\nunderstanding system in the brain connects language to rep-\nresentations of objects and situations and enhances language\nunderstanding by exploiting the full range of our multi-sensory\nexperience of the world, our representations of our motor\nactions, and our memory of previous situations. We have ar-\ngued that the next generation language understanding system\nshould emulate this system in the brain and we have sketched\nsome aspects of the form such a system might take. While\nwe have emphasized understanding of concrete situations, we\nhave argued that understanding more abstract language builds\nupon this concrete foundation, pointing toward the possibility\nthat it may someday be possible to build artiﬁcial systems that\nunderstand abstract situations far beyond the concrete and\nthe here-and-now. In sum, we have proposed that modeling\nthe integrated understanding system in the brain will take us\ncloser to capturing human-level language understanding and\nintelligence.\nACKNOWLEDGMENTS. This article grew out of a workshop\norganized by HS atMeaning in Context 3, Stanford University,\nSeptember 2017. We thank Chris Potts for discussion. HS was\nsupported by ERC Advanced Grant #740516.\nMcClelland et al.: Language understanding in humans and machines 7\n1. Wu Y , et al. (2016) Google’s neural machine translation system: Bridging the gap between\nhuman and machine translation. -.\n2. Rosenblatt F (1961) Principles of neurodynamics. perceptrons and the theory of brain mech-\nanisms. (Spartan Books, Cornell University, Ithaca, New Y ork).\n3. Rumelhart DE, McClelland JL, the PDP research group (1986) Parallel Distributed Process-\ning. Explorations in the Microstructure of Cognition. Volume 1: Foundations. (The MIT Press,\nCambridge, MA).\n4. Rumelhart DE, McClelland JL (1986) On learning the past tenses of English verbs in Parallel\nDistributed Processing. Explorations in the Microstructure of Cognition. Volume 2: Psycho-\nlogical and Biological Models, eds. McClelland JL, Rumelhart DE, the PDP Research Group.\n(MIT Press), pp. 216–271.\n5. Rumelhart DE (1977) Toward an interactive model of reading in Attention & Performance VI,\ned. Dornic S. (LEA, Hillsdale, NJ), pp. 573–603.\n6. McClelland JL, Rumelhart DE (1981) An interactive activation model of context effects in letter\nperception: I. an account of basic ﬁndings. Psychological review 88(5):375.\n7. Hopﬁeld JJ (1982) Neural networks and physical systems with emergent collective computa-\ntional abilities. Proceedings of the national academy of sciences 79(8):2554–2558.\n8. Ackley DH, Hinton GE, Sejnowski TJ (1985) A learning algorithm for boltzmann machines.\nCognitive science 9(1):147–169.\n9. Elman JL (1990) Finding structure in time. Cognitive Science 14:179–211.\n10. Gold EM (1967) Language identiﬁcation in the limit. Information and control 10(5):447–474.\n11. Elman JL (1991) Distributed representations, simple recurrent networks, and grammatical\nstructure. Mach. Learn. 7(2/3):195–225.\n12. Hochreiter S, Bengio Y , Frasconi P , Schmidhuber J (2001) Gradient ﬂow in recurrent nets: the\ndifﬁculty of learning long-term dependencies in A Field Guide to Dynamical Recurrent Neural\nNetworks, eds. Kremer SC, Kolen JF . (IEEE Press).\n13. Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural computation\n9(8):1735–1780.\n14. Collobert R, et al. (2011) Natural language processing (almost) from scratch. Journal of\nMachine Learning Research 12(Aug):2493–2537.\n15. Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J (2013) Distributed representations of\nwords and phrases and their compositionality in Neural Information Processing Systems. pp.\n3111–3119.\n16. Simpson GB (1981) Meaning dominance and semantic context in the processing of lexical\nambiguity. Journal of verbal learning and verbal behavior 20(1):120–136.\n17. Rudolph M, Ruiz F , Blei D (2017) Structured embedding models for grouped data inAdvances\nin Neural Information Processing Systems.\n18. Peters ME, et al. (2018) Deep contextualized word representations. CoRR abs/1802.05365.\n19. Vaswani A, et al. (2017) Attention is all you need in Advances in Neural Information Process-\ning Systems 30. (Curran Associates, Inc.), pp. 5998–6008.\n20. Devlin J, Chang MW, Lee K, Toutanova K (2018) BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. ArXiv e-prints.\n21. Y ang Z, et al. (2019) Xlnet: Generalized autoregressive pretraining for language understand-\ning. CoRR abs/1906.08237.\n22. Dehghani M, Gouws S, Vinyals O, Uszkoreit J, Kaiser L (2018) Universal transformers. CoRR\nabs/1807.03819.\n23. Uszkoreit J (2017) Transformer: A novel neural network architecture for language understand-\ning, https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html.\n24. Radford A, Narasimhan K, Salimans T, Sutskever I (2018) Improving language understanding\nby generative pre-training. OpenAI Preprint.\n25. Devlin J, Chang MW, Lee K, Toutanova K (2018) Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805.\n26. Wang A, et al. (2018) Glue: A multi-task benchmark and analysis platform for natural lan-\nguage understanding. arXiv preprint arXiv:1804.07461.\n27. Wang A, et al. (2019) Superglue: A stickier benchmark for general-purpose language under-\nstanding systems. arXiv preprint arXiv:1905.00537.\n28. Levesque H, Davis E, Morgenstern L (2012) The winograd schema challenge in Thirteenth\nInternational Conference on the Principles of Knowledge Representation and Reasoning.\n29. Raffel C, Shazeer N, Roberts A, , et al. (2019) Exploring the limits of transfer learning with a\nuniﬁed text-to-text transformer. arXiv preprint arXiv:1910.10683.\n30. Jia R, Liang P (2017) Adversarial examples for evaluating reading comprehension systems in\nProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing.\n(Association for Computational Linguistics), pp. 2021–2031.\n31. Lakoff G (1987) Women, Fire, and Dangerous Things . (The University of Chicago Press,\nChicago).\n32. Langacker RW (1987) Foundations of cognitive grammar: Theoretical prerequisites . (Stan-\nford university press) Vol. 1.\n33. Bransford JD, Johnson MK (1972) Contextual prerequisites for understanding: Some in-\nvestigations of comprehension and recall. Journal of verbal learning and verbal behavior\n11(6):717–726.\n34. Schank RC (1983) Dynamic memory: A theory of reminding and learning in computers and\npeople. (Cambridge University Press).\n35. John MFS, McClelland JL (1990) Learning and applying contextual constraints in sentence\ncomprehension. Artiﬁcial Intelligence 46(1):217 – 257.\n36. Hasson U, Egidi G, Marelli M, Willems RM (2018) Grounding the neurobiology of language\nin ﬁrst principles: The necessity of non-language-centric explanations for language compre-\nhension. Cognition 180:135–157.\n37. Rumelhart DE (1979) Some problems with the notion that words have literal meanings in\nMetaphor and thought, ed. Ortony A. (Cambridge Univ. Press, Cambridge, UK), pp. 71–82.\n38. Barclay J, Bransford JD, Franks JJ, McCarrell NS, Nitsch K (1974) Comprehension and se-\nmantic ﬂexibility. Journal of Verbal Learning and Verbal Behavior 13(4):471 – 481.\n39. Tanenhaus MK, Spivey-Knowlton MJ, Eberhard KM, Sedivy JC (1995) Integration of visual\nand linguistic information in spoken language comprehension. Science pp. 1632–1634.\n40. Altmann GT, Kamide Y (2007) The real-time mediation of visual attention by language and\nworld knowledge: Linking anticipatory (and other) eye movements to linguistic processing.\nJournal of Memory and Language 57(4):502–518.\n41. McClelland JL, Mirman D, Bolger DJ, Khaitan P (2014) Interactive activation and mutual con-\nstraint satisfaction in perception and cognition. Cognitive science 38(6):1139–1189.\n42. Patterson K, Nestor PJ, Rogers TT (2007) Where do you know what you know? the represen-\ntation of semantic knowledge in the human brain. Nature Reviews Neuroscience 8(12):976.\n43. Rogers TT, et al. (2004) Structure and deterioration of semantic memory: a neuropsycholog-\nical and computational investigation. Psychological review 111(1):205–235.\n44. Zwaan RA, Radvansky GA (1998) Situation models in language comprehension and memory.\nPsychological bulletin 123(2):162–185.\n45. Ranganath C, Ritchey M (2012) Two cortical systems for memory-guided behaviour. Nature\nReviews Neuroscience 13:713. Review Article.\n46. Zadbood A, Chen J, Leong Y , Norman K, Hasson U (2017) How we transmit memories to\nother brains: Constructing shared neural representations via communication. Cerebral Cor-\ntex 27(10):4988–5000.\n47. Baldassano C, et al. (2017) Discovering event structure in continuous narrative perception\nand memory. Neuron 95(3):709–721.\n48. Hinton GE (1981) Implementing semantic networks in parallel hardware in Parallel Models of\nAssociative Memory, eds. Hinton GE, Anderson JA. (Erlbaum), pp. 161–187.\n49. Cleeremans A, McClelland JL (1991) Learning the structure of event sequences. Journal of\nExperimental Psychology: General 120(3):235.\n50. McCloskey M, Cohen NJ (1989) Catastrophic interference in connectionist networks: The\nsequential learning problem in Psychology of learning and motivation. (Elsevier) Vol. 24, pp.\n109–165.\n51. Warren DE, Duff MC (2019) Fast mappers, slow learners: Word learning without hippocam-\npus is slow and sparse irrespective of methodology. Cognitive neuroscience pp. 1–3.\n52. Squire LR (1992) Memory and the hippocampus: a synthesis from ﬁndings with rats, mon-\nkeys, and humans. Psychological review 99(2):195.\n53. Y onelinas A, Ranganath C, Ekstrom A, Wiltgen B (2019) A contextual binding theory of\nepisodic memory: systems consolidation reconsidered. Nature Reviews Neuroscience.\n54. Marr D (1971) Simple memory: a theory for archicortex. Philosophical Transactions of the\nRoyal Society of London B: Biological Sciences 262(841):23–81.\n55. McClelland JL, McNaughton BL, O’Reilly RC (1995) Why there are complementary learning\nsystems in the hippocampus and neocortex: Insights from the successes and failures of\nconnectionist models of learning and memory. Psychological Review 102(3):419–457.\n56. Kumaran D, Hassabis D, McClelland JL (2016) What learning systems do intelligent agents\nneed? Complementary learning systems theory updated. Trends in Cognitive Sciences\n20(7):512–534.\n57. Winograd T (1972) Understanding natural language. Cognitive psychology 3(1):1–191.\n58. MacLeod H, Bennett CL, Morris MR, Cutrell E (2017) Understanding blind people’s experi-\nences with computer-generated captions of social media images in Proceedings of the 2017\nCHI Conference on Human Factors in Computing Systems. (ACM), pp. 5988–5999.\n59. Hudson DA, Manning CD (2019) Learning by abstraction: The neural state machine. arXiv\npreprint arXiv:1907.03950.\n60. Rumelhart DE, Smolensky P , McClelland JL, Hinton G (1986) Pdp models of schemata and\nsequential thought processes in pdp models. Parallel distributed processing: Explorations in\nthe microstructure of cognition 2:3–57.\n61. Yu H, Wang J, Huang Z, Y ang Y , Xu W (2016) Video paragraph captioning using hierarchical\nrecurrent neural networks in Proceedings of the IEEE conference on computer vision and\npattern recognition. pp. 4584–4593.\n62. Hermann KM, et al. (2017) Grounded language learning in a simulated 3d world. arXiv\npreprint arXiv:1706.06551.\n63. Das R, Zaheer M, Reddy S, McCallum A (2017) Question answering on knowledge bases\nand text using universal schema and memory networks in Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistic. pp. 358–365.\n64. Chaplot DS, Sathyendra KM, Pasumarthi RK, Rajagopal D, Salakhutdinov R (2017)\nGated-attention architectures for task-oriented language grounding. arXiv preprint\narXiv:1706.07230.\n65. Oh J, Singh S, Lee H, Kohli P (2017) Zero-shot task generalization with multi-task deep rein-\nforcement learning inProceedings of the 34th International Conference on Machine Learning-\nVolume 70. (JMLR. org), pp. 2661–2670.\n66. Hill F , et al. (2019) Emergent systematic generalization in a situated agent. arXiv preprint\narXiv:1910.00571.\n67. Lake BM, Salakhutdinov R, Tenenbaum JB (2015) Human-level concept learning through\nprobabilistic program induction. Science 350(6266):1332–1338.\n68. Graves A, et al. (2016) Hybrid computing using a neural network with dynamic external mem-\nory. Nature 538(7626):471–476.\n69. Hintzman DL (1984) Minerva 2: A simulation model of human memory. Behavior Research\nMethods, Instruments, & Computers 16(2):96–101.\n70. Polyn SM, Norman KA, Kahana MJ (2009) A context maintenance and retrieval model of\norganizational processes in free recall. Psychological review 116(1):129.\n71. Stout D, Chaminade T (2012) Stone tools, language and the brain in human evolution. Philos\nTrans R Soc Lond B Biol Sci 367(1585):75–87.\n72. Bryson J (2008) Embodiment vs. memetics. Mind and Society 7(1):77–94.\n73. Lakoff G, Johnson M (1980) Metaphors We Live By. (University of Chicago, Chicago, IL).\n74. Feldman J, Narayanan S (2004) Embodied meaning in a neural theory of language. Brain\nand Language 89:385–392.\nMcClelland et al.: Language understanding in humans and machines 8",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8062930107116699
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6333221197128296
    },
    {
      "name": "Focus (optics)",
      "score": 0.5298687815666199
    },
    {
      "name": "Language model",
      "score": 0.5017969608306885
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4948374629020691
    },
    {
      "name": "Cache language model",
      "score": 0.4816259741783142
    },
    {
      "name": "Representation (politics)",
      "score": 0.4685339033603668
    },
    {
      "name": "Natural language processing",
      "score": 0.4559119939804077
    },
    {
      "name": "Point (geometry)",
      "score": 0.43718981742858887
    },
    {
      "name": "Language primitive",
      "score": 0.43670111894607544
    },
    {
      "name": "Language identification",
      "score": 0.4316139221191406
    },
    {
      "name": "Human language",
      "score": 0.42257770895957947
    },
    {
      "name": "Exploit",
      "score": 0.4136689007282257
    },
    {
      "name": "Universal Networking Language",
      "score": 0.3781208395957947
    },
    {
      "name": "Natural language",
      "score": 0.37236279249191284
    },
    {
      "name": "Comprehension approach",
      "score": 0.22844666242599487
    },
    {
      "name": "High-level programming language",
      "score": 0.21271780133247375
    },
    {
      "name": "Programming language",
      "score": 0.1091298758983612
    },
    {
      "name": "Programming paradigm",
      "score": 0.08234411478042603
    },
    {
      "name": "Linguistics",
      "score": 0.07964223623275757
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ]
}