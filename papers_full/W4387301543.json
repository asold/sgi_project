{
    "title": "A Large Language Model-Based Generative Natural Language Processing Framework Finetuned on Clinical Notes Accurately Extracts Headache Frequency from Electronic Health Records",
    "url": "https://openalex.org/W4387301543",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3083190073",
            "name": "Chia-Chun Chiang",
            "affiliations": [
                "Mayo Clinic in Arizona"
            ]
        },
        {
            "id": "https://openalex.org/A2103670821",
            "name": "Man Luo",
            "affiliations": [
                "Mayo Clinic Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2760884149",
            "name": "Gina Dumkrieger",
            "affiliations": [
                "Mayo Clinic Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2933101254",
            "name": "Shubham Trivedi",
            "affiliations": [
                "Mayo Clinic Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2580597186",
            "name": "Yi-Chieh Chen",
            "affiliations": [
                "Mayo Clinic in Arizona"
            ]
        },
        {
            "id": "https://openalex.org/A4202403729",
            "name": "Chieh-Ju Chao",
            "affiliations": [
                "Mayo Clinic in Arizona"
            ]
        },
        {
            "id": "https://openalex.org/A827242392",
            "name": "Todd J. Schwedt",
            "affiliations": [
                "Mayo Clinic Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2165699372",
            "name": "Abeed Sarker",
            "affiliations": [
                "Emory University"
            ]
        },
        {
            "id": "https://openalex.org/A2122079672",
            "name": "Imon Banerjee",
            "affiliations": [
                "Arizona State University",
                "Mayo Clinic Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A3083190073",
            "name": "Chia-Chun Chiang",
            "affiliations": [
                "Mayo Clinic in Arizona"
            ]
        },
        {
            "id": "https://openalex.org/A2103670821",
            "name": "Man Luo",
            "affiliations": [
                "Mayo Clinic Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2760884149",
            "name": "Gina Dumkrieger",
            "affiliations": [
                "Mayo Clinic Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2933101254",
            "name": "Shubham Trivedi",
            "affiliations": [
                "Mayo Clinic Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2580597186",
            "name": "Yi-Chieh Chen",
            "affiliations": [
                "Mayo Clinic in Arizona"
            ]
        },
        {
            "id": "https://openalex.org/A4202403729",
            "name": "Chieh-Ju Chao",
            "affiliations": [
                "Mayo Clinic in Arizona"
            ]
        },
        {
            "id": "https://openalex.org/A827242392",
            "name": "Todd J. Schwedt",
            "affiliations": [
                "Mayo Clinic Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2165699372",
            "name": "Abeed Sarker",
            "affiliations": [
                "Emory University"
            ]
        },
        {
            "id": "https://openalex.org/A2122079672",
            "name": "Imon Banerjee",
            "affiliations": [
                "Mayo Clinic Hospital",
                "Arizona State University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2047041272",
        "https://openalex.org/W4298144341",
        "https://openalex.org/W4312206450",
        "https://openalex.org/W4296712220",
        "https://openalex.org/W2396881363",
        "https://openalex.org/W2925863688",
        "https://openalex.org/W2937845937",
        "https://openalex.org/W2963716420",
        "https://openalex.org/W4294971360",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W4292779060"
    ],
    "abstract": "Abstract Background Headache frequency, defined as the number of days with any headache in a month (or four weeks), remains a key parameter in the evaluation of treatment response to migraine preventive medications. However, due to the variations and inconsistencies in documentation by clinicians, significant challenges exist to accurately extract headache frequency from the electronic health record (EHR) by traditional natural language processing (NLP) algorithms. Methods This was a retrospective cross-sectional study with human subjects identified from three tertiary headache referral centers-Mayo Clinic Arizona, Florida, and Rochester. All neurology consultation notes written by more than 10 headache specialists between 2012 to 2022 were extracted and 1915 notes were used for model fine-tuning (90%) and testing (10%). We employed four different NLP frameworks: (1) ClinicalBERT (Bidirectional Encoder Representations from Transformers) regression model (2) Generative Pre-Trained Transformer-2 ( GPT-2) Question Answering (QA) Model zero-shot (3) GPT-2 QA model few-shot training fine-tuned on Mayo Clinic notes; and (4) GPT-2 generative model few-shot training fine-tuned on Mayo Clinic notes to generate the answer by considering the context of included text. Results The GPT-2 generative model was the best-performing model with an accuracy of 0.92[0.91 – 0.93] and R 2 score of 0.89[0.87, 0.9], and all GPT2-based models outperformed the ClinicalBERT model in terms of the exact matching accuracy. Although the ClinicalBERT regression model had the lowest accuracy 0.27[0.26 – 0.28], it demonstrated a high R 2 score 0.88[0.85, 0.89], suggesting the ClinicalBERT model can reasonably predict the headache frequency within a range of ≤ ± 3 days, and the R 2 score was higher than the GPT-2 QA zero-shot model or GPT-2 QA model few-shot training fine-tuned model. Conclusion We developed a robust model based on a state-of-the-art large language model (LLM)-a GPT-2 generative model that can extract headache frequency from EHR free-text clinical notes with high accuracy and R 2 score. It overcame several challenges related to different ways clinicians document headache frequency that were not easily achieved by traditional NLP models. We also showed that GPT2-based frameworks outperformed ClinicalBERT in terms of accuracy in extracting headache frequency from clinical notes. To facilitate research in the field, we released the GPT-2 generative model and inference code with open-source license of community use in GitHub.",
    "full_text": "A Large Language Model-Based Generative Natural Language Processing Framework \nFinetuned on Clinical Notes Accurately Extracts Headache Frequency from Electronic \nHealth Records  \n \nChia-Chun Chiang, MD1*, Man Luo, PhD2*, Gina Dumkrieger, PhD3, Shubham Trivedi4, \nYi-Chieh Chen, Pharm.D, BCPS5, Chieh-Ju Chao, MD6, Todd J. Schwedt, MD3, Abeed \nSarker, PhD7, Imon Banerjee, PhD2,8 \n \n*Dr. Chiang and Dr. Luo contributed equally \n1Department of Neurology, Mayo Clinic, Rochester, MN \n2Department of Radiology, Mayo Clinic, Phoenix, AZ  \n3Department of Neurology, Mayo Clinic, Phoenix, AZ \n4Department of Radiology, Mayo Clinic, Phoenix, AZ  \n5Department of Pharmacy, Mayo Clinic, Rochester, MN \n6Department of Cardiology, Mayo Clinic, Rochester, MN \n7Department of Biomedical Informatics, School of Medicine, Emory University, Atlanta, \nGA  \n8School of Computing and Augmented Intelligence, Arizona State University \n \nCorresponding author: \nChia-Chun Chiang, MD \nDepartment of Neurology, Mayo Clinic, Rochester, MN \nChiang.Chia-Chun@mayo.edu \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 3, 2023. ; https://doi.org/10.1101/2023.10.02.23296403doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nKeywords: migraine, headache frequency, artificial intelligence, natural language processing, \nlarge language model \n \nAbstract \nBackground \nHeadache frequency, defined as the number of days with any headache in a month (or four \nweeks), remains a key parameter in the evaluation of treatment response to migraine preventive \nmedications. However, due to the variations and inconsistencies in documentation by clinicians, \nsignificant challenges exist to accurately extract headache frequency from the electronic health \nrecord (EHR) by traditional natural language processing (NLP) algorithms. \nMethods \nThis was a retrospective cross -sectional study with human subjects identified from three tertiary \nheadache referral centers - Mayo Clinic Arizona, Florida, and Rochester. All neurology \nconsultation notes written by more than 10 headache specialists between 2012 to 2022 were \nextracted and 1915 notes were used for model fine-tuning (90%) and testing (10%). We employed \nfour different NLP frameworks: (1) ClinicalBERT (Bidirectional Encoder Representations from \nTransformers) regression model  (2) Generative Pre -Trained Transformer -2 ( GPT-2) Question \nAnswering (QA) Model zero -shot (3) GPT -2 QA model few -shot training fine-tuned on Mayo \nClinic notes; and (4) GPT-2 generative model few-shot training fine-tuned on Mayo Clinic notes \nto generate the answer by considering the context of included text. \nResults \nThe GPT-2 generative model was the best-performing model with an accuracy of 0.92[0.91 – 0.93] \nand R2 score of 0.89[0.87, 0.9], and all GPT2-based models outperformed the ClinicalBERT model \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 3, 2023. ; https://doi.org/10.1101/2023.10.02.23296403doi: medRxiv preprint \nin terms of the exact matching accuracy. Although the ClinicalBERT regression model had the \nlowest accuracy 0.27[0.26 – 0.28], it demonstrated a high R2 score 0.88[0.85, 0.89], suggesting \nthe ClinicalBERT model can reasonably predict the headache frequency within a range of £ ± 3 \ndays, and the R2 score was higher than the GPT-2 QA zero-shot model or GPT-2 QA model few-\nshot training fine-tuned model. \nConclusion  \nWe developed a robust model based on a state-of-the-art large language model (LLM)- a GPT-2 \ngenerative model that can extract headache frequency from EHR free-text clinical notes with \nhigh accuracy and R2 score. It overcame several challenges related to different ways clinicians \ndocument headache frequency that were not easily achieved by traditional NLP models. We also \nshowed that GPT2-based frameworks outperformed ClinicalBERT in terms of accuracy in \nextracting headache frequency from clinical notes. To facilitate research in the field, we released \nthe GPT-2 generative model and inference code with open-source license of community use in \nGitHub. \n \nIntroduction \nHeadache medicine is a unique field where the diagnosis, treatment, and outcome assessments rely \nheavily on human natural language rather than test results. Important clinical information for \ndiagnosis and evaluation of treatment responses, including detailed headache description, \nmigraine-associated symptoms, aura, prior treatment trials, and the frequency a nd severity of \nmigraine attacks and headaches are often documented as free text in clinical notes in the Electronic \nHealth Records (EHRs). Migraine is a highly prevalent and disabling neurological condition that \naffects around 16% of the U nited States population and more than 1 billion people worldwide, \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 3, 2023. ; https://doi.org/10.1101/2023.10.02.23296403doi: medRxiv preprint \ncausing significant disability and loss of productivity [1]. For patients with frequent migraine \nattacks, preventive therapeutic interventions are recommended with the  aim of  reducing their \nheadache frequency and severity. Currently, the gold standard metric for evaluation of treatment \nresponse to migraine preventive medications is the change in headache frequency, often \ndocumented as headache days per month or every four weeks in clinician s’ notes. Varying \nsyntactic reporting of headache frequency poses significant challenges when large-scale extraction \nof such da ta is needed, and conducting chart review s by human s reading the notes  is time -\nconsuming and not practical when a large amount of data is needed for research. \n \nNatural Language Processing (NLP) is a branch of Artificial Intelligence (AI) that concerns the \nability of machines to understand, interpret, and generate human language. Previous attempts have \nused NLP to extract headache-related information from various sources to perform tasks including \nanalyzing patient self-written narratives to distinguish between migraine and cluster headache[2], \ndeveloping a generalizable NLP model to identify users with self -reported migraine on various \nsocial media platforms[3], and distinguishing between migraine versus other headache as well as \nidentifying headache-associated symptoms[4]. Although headache frequency is one of the most \ncommonly used gold standard metrics in the evaluation of treatment responses, currently, there are \nno tools reported to accurately and reliably extract headache frequency from various clinic notes \nwithin the EHR. Possible explanations include 1) Headache frequency is not always documented \nfor all patients being evaluated for migraine, especially when patients were seen by non-specialists; \n2) Even if the information exists, there is a lack of consistency in th e documentation of headache \nfrequency, making it challenging to extract such information based on simple rule -based NLP \nmodels. Here we list several examples of variations of documentation of headache frequency  in \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 3, 2023. ; https://doi.org/10.1101/2023.10.02.23296403doi: medRxiv preprint \nclinical notes, defined as days with any headache in a 4-week period (28 days).“She experiences \ndaily headache with 10 severe migraine days per month” : it requires the recognition of “daily” \nheadache (e.g., 28 days per month with headache)  instead of picking up the number 10, “He \nreports having headaches 3 days per week” -requires calculation 3 x 4 weeks= 12 days , and \n“Overall, she has 2 headache-free days per month”-requires calculation 28-2=26 days. \n \nWith the advances in NLP techniques, the recent emergence of generative large language models \n(LLMs), and the confluence of AI, there arises an opportunity to elevate the granularity and \naccuracy of automated data extraction from EHRs. We aim to develop robust modeling \nframeworks that can accurately extract headache frequency data from free-text clinical notes in \nthe EHR. We performed a comparative analysis of a traditional transformer-based NLP model, \nnotably a ClinicalBERT regression model, against the recent advancement of open source, \ngenerative LLM models, namely three Generative Pre-Trained Transformer-2 (GPT-2) based \nmodels with various architectures - (a) zero-shot versus few-shot; (b) question-answering versus \ngenerative. We hypothesized that novel LLM frameworks could overcome the challenges in \nheadache frequency extraction that would be otherwise hard to achieve with traditional \ntransformer-based NLP models. \n \nMethods \nThis was a retrospective cross-sectional study with human subjects identified from Mayo Clinic in \nArizona, Florida, and Rochester. The Mayo Clinic Institutional Review Board approved an \nexemption for this study, and written informed consent was not require d. We extracted 34369 \nneurology consultation notes documented by more than 10 headache specialists within 2012 - 2022 \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 3, 2023. ; https://doi.org/10.1101/2023.10.02.23296403doi: medRxiv preprint \nacross 3 geographically distributed sites and tertiary referral Headache Centers in Rochester, MN, \nScottsdale, AZ, and Jacksonville, FL. We randomly selected 1,950 notes for model development. \nAfter excluding irrelevant notes, we curated 1,915 notes for model fine -tuning (90%) and testing \n(10%). Three individual readers annotated the headache frequency from the curated cohort. All \nheadache frequency annotations were validated by headache specialist s (CC and TS)  to ensure \naccuracy and used as the gold standard for model development. \n \nExperimental Design. Figure 1 shows the overall framework for our experimental design. After \nName Entity Recognition (NER) extraction, four different state -of-the-art NLP modeling \narchitectures were used to directly read the full neurology consultation notes selected for model \ndevelopment and to extract documented headache frequencies. We then compare the performance \nmetrics against the manual annotations confirmed by headache specialists. The subsections below \ndetail each component of the modeling framework including a brief description of the model \narchitecture variations. \n \nFig 1. Nature Language Processing (NLP) framework for extracting headache frequency \nfrom neurology consultation notes - four parallel modeling schemes: (1) ClinicalBERT \nregression model: encoder-based regression model pre-trained on MIMIC-III; (2) GPT-2 QA \nmodel zero-shot: decoder QA model trained on the generic web scraped data; (3) GPT-2 QA \nmodel few-shot training: decoder model trained on the generic web scraped data and fine-tuned \non Mayo Clinic notes; and (4) GPT-2 generative model few-shot training:  decoder model \ntrained on the generic web scraped data and fine-tuned on Mayo Clinic notes to generate directly \nthe answer by considering the context text. Abbreviations: Name Entity Recognition (NER); The \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 3, 2023. ; https://doi.org/10.1101/2023.10.02.23296403doi: medRxiv preprint \nMedical Information Mart for Intensive Care III (MIMIC-III); Generative Pre-Trained \nTransformer-2 (GPT-2); Question Answering (QA) \n \n \n \nNER extractions: To capture the vocabulary for the intended task, we compiled the following two \ncomplementary dictionaries: the target term list, which was a publicly available terminology \nprogram (Clinical Event Recognizer) extended with 10 additional terms for migraine that were \nprimarily captured by analyzing the training set , including migraine, headache, headaches, days \nper month, frequency; and the modifier list, which was a list of modifier terms, including clinical \nterms related to negations (eg, no, rule out), temporality (eg, history, current), family (eg, mother, \nsister) and discussion (eg, risk of, may introduce). Finally, a keyword -based sentence retrieval \nmethod was applied to each clinic note, which selected only the sentences that contained a t least \none of the migraine/headache -related terms as a named entity and generated a text snippet by \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 3, 2023. ; https://doi.org/10.1101/2023.10.02.23296403doi: medRxiv preprint \ncombining the sentences extracted from the whole notes. Based on the modifiers, we dropped all \nthe historical reporting and discussion.  \n \nLanguage modeling \nIn parallel, we compared four state-of-the-art strategies for extracting headache frequency from \nthe selected text block - (1) ClinicalBERT regression model: an encoder-based regression model \npre-trained on the Medical Information Mart for Intensive Care III ( MIMIC-III), which is a large \ndatabase with deidentified health-related data associated with over 40,000 patients who stayed in \ncritical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012 [5]; (2) \nGPT2-Question Answering ( QA) Model zero-shot: a decoder model trained on the generic web \nscraped data to predict relevant answer text position within the context text; (3) GPT-2 QA model \nfew-shot training: a decoder model trained on the generic web scraped data and fine-tuned on the \nselected 1,915 Mayo Clinic neurology consultation notes with annotations confirmed by headache \nspecialists to predict relevant answer text position within the context text ; and (4) GPT-2 \ngenerative model few-shot training:  a decoder model trained on the generic web scraped data and \nfine-tuned on the same 1,915 Mayo Clinic notes to generate the answer directly by considering the \ncontext of the included text which does not require the answer to be included in the sentences used. \nWe chose GPT-2 based models as those are open -source models that can be downloaded locally \nand fine-tuned as needed without the risk of uploading sensitive information to a third party.  \n \nClinicalBERT regression model  - ClinicalBERT[6] has a bidirectional encoder representation \nsimilar to the BERT, where ClinicalBERT is pre-trained on clinical data - MIMIC-III clinic notes, \nand BERT is trained on generic domain - Wikipedia and BookCorpus.  Both models are designed \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 3, 2023. ; https://doi.org/10.1101/2023.10.02.23296403doi: medRxiv preprint \nto understand the context of words in a sentence by looking at the words that come both before \nand after each word in a sentence.  They are trained via masked language modeling (MLM) - an \nunsupervised learning approach that can utilize large amounts of una nnotated free text. MLM \ntraining masks some random words in the input and the model is trained to predict missing words \nin a sentence. We fine -tuned ClinicalBERT and trained a regression model. More specifically, \ngiven an input sentence, we used ClinicalBERT to encode the sentence into a sequence of tokens \nstarting with a special token [CLS] [7], Similar to any other BERT model, such that the [CLS] \nmodel is a global representation of the input sentence. During the fine-tuning phase of the model, \nwe initialized the model with the weights of a pre -trained model \n\"emilyalsentzer/Bio_ClinicalBERT\"[6,8], which is pre -trained on MIMIC -III corpus. On top of \nClinicalBERT, we formulated the headache frequency detection as an ordinal regression problem \nwhere each input text will obtain a score with the interval [-1,28], -1 being no headache frequency \ndetected in the note and 28 indicating having daily headache, using 28 days (4 weeks) as a unit to \ncapture headache days per month. We use mean square loss as the objective function to train the \nregressor. The learning rate is set 5e-5, and total training epoch is 50, batch size of 5. The optimizer \nchosen for this task was AdamW[9,10]  \n \nGPT2-QA Model zero-shot and few-shot training - GPT-2 is a decoder-only architecture, which \nis an autoregression model . The main difference between the autoregression and bidirectional \nencoder, like the BERT model, is that the autoregressive model generates sentences word by word, \neach time predicting the next word based on all the previous words in the sentence rather than both \nprevious and following words as in the bidirectional model. Leveraging unsupervised trai ning \nstrategy, GPT-2 is trained on a large  training corpus of web-scraped data but with the next word \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 3, 2023. ; https://doi.org/10.1101/2023.10.02.23296403doi: medRxiv preprint \nprediction task, which means the model is trained to predict the next word in a sentence given all \nthe previous words. The GPT-2 model is good at text generation tasks given the nature of how it \nwas pre-trained. We frame GPT -2 as an extractive QA model. For such a QA task, the model is \ngiven a context and a question, and the goal is to extract the answer to the question fro m the \ncontext.  \nTo extract the answer, the GPT-2 QA first tokenizes an input sentence into a sequence of tokens. \nEvery token is represented by a vector embedding. Then the model predicts which token is the \nstart token of an answer and which token is an end token of the answer. To achieve this, on top of \nthe GPT-2 model, we add a linear layer that takes each token as input and predicts two probabilities \nof this token: being the start token of the answer and being the end token of the answer. Finally, \nthe token that has the highest probability of being the start token is the beginning of the answer, \nlike the end token of the answer. Considering the following example, the context is “Over the past \nfour weeks, she reports having had 15 headache days.”, and the question is “ what is the monthly \nfrequency of the headache for this patient? ” The label of this example is (11, 12) since token 11 \nand token 12 represent the answer “15” in the context. During the training, the model is trained to \npredict the highest probability (i.e. label 1) of token 11 to be the start token, and the low \nprobabilities of every other token (label 0). Similarly, token 12 has the highest probability of being \nthe end (label 1) and the probability of being the end token for every other token is low (label 0). \nWe use cross entropy loss as the training objective. \n \nWe initialize the model with the pretrained weight s of \"anas-awadalla/gpt2-span-head-few-shot-\nk-32-finetuned-squad-seed-4\", which is fine-tuned on the Squad dataset[11].  The learning rate is \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 3, 2023. ; https://doi.org/10.1101/2023.10.02.23296403doi: medRxiv preprint \nset 2e-5, and total training epoch is 10, batch size of 16. The optimizer chosen for this task was \nAdamW[9,10].  \n \nFirst, we perform a zero -shot testing of the QA model \"anas -awadalla/gpt2-span-head-few-shot-\nk-32-finetuned-squad-seed-4\". Since the model is trained on a question -answering task before, \ntheoretically, we can use it to answer any question. One concern is t he domain shift issue which \ncharacterizes the change in statistical distribution or semantic organization of the data, e.g. the \nmodel trained on politics, is prompted to answer questions regarding clinical problems; however, \nour target task does not require any clinical or biomedical background to answer the question, thus, \nwe hypothesize that the QA model can do reasonable zero -shot prediction. In this zero -shot \nevaluation setting, we provided the question: \"What is the headache frequency per month?'\" to test \nthe pre-trained QA model. From our analysis, which will be detailed below, we found that while \nthe exact matching score is not high, the model was able to predict a span which contained the \nanswer. For example, the ground truth is “12”, and the model generates an answer “a headache for \n12 days out of 28”; another example: the ground truth is “28”, and the model generates an answer \n“a daily headache”. Semantically, the model’s prediction is correct ; however, by exact matching, \nthe model will be judged as incorrect. To resolve this evaluation problem and fair comparison, we \nmapped the zero -shot model prediction to a digit answer by extracting the digit value in the \nprediction, or if the words “daily” o r “every day” mentioned in the prediction, we map the \nprediction to “28”; if there is no digit mentioned in the prediction, we map the prediction to “ -1”. \n  \nIn addition, we have applied a few-shot learning strategy where we showed some examples \nto the GPT -2 QA model for fine -tuning the answer space to extract more targeted answers as \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 3, 2023. ; https://doi.org/10.1101/2023.10.02.23296403doi: medRxiv preprint \nexpected by the experts. We fine -tuned the same model using the 1,915 annotated notes and \nreported the performance on the test set.   \n \nGPT-2 Generative Model - The GPT-2 QA model assumes that the answer is always a span in the \ninput text; however, it fails when the answer is not directly given in the input. For example, the \ngiven text can be “ the patient has 4 headache -free days a month.”, then the actual answer is 28 -\n4=24 days which is not present within the context. To resolve such a challenge, we train a GPT-2 \ngenerative model to generate the actual frequency instead of the index of the answer in the given \ntext. Furthermore, since the possible answers are discrete values ranging from -1 to 28, where -1 \nrepresents there is no frequency mentioned in the given text, we add all the possible answers, all \nnumbers from -1 to 28, as new tokens to the model’s vocabulary if they are not already part of it. \nDuring the training, the model is optimized by negative log-likelihood of the frequency label. Here, \nwe use the same example given in the previous section to illustrate the main difference between \nthe extractive QA counter part model. Given the same context and the question, the label for the \nGPT-2 generative model is “15”. On top of GPT -2, we add a linear layer with the embedding as \ninput and cross entropy loss as the training objective. To fairly compare with the GPT-2 QA model, \nwe initialize the GPT generative model with the same pretrained weig hts and same \nhyperparameters during the training time.  \n \nResults \nCohort. A total of 1,915 neurology consultation notes written by headache specialists were used \nfor model development. Three individual readers extracted the headache frequency from the \ncurated cohort and inter-rater reliability was >0.85.  1723   \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 3, 2023. ; https://doi.org/10.1101/2023.10.02.23296403doi: medRxiv preprint \n \nTable 1. Quantitative performance of the parallel architectures. Optimal performance is \nhighlighted in bold. 95% confidence interval is calculated using auto-bootstrapping and \nrepresented as [x, y]. Same test set is used for all the model evaluations. \nModel Accuracy R2 score \nClinicalBert regression 0.27 [0.26, 0.28] 0.88 [0.85, 0.89] \nGPT2 zeroshot 0.57 [0.57, 0.59] -0.014 [ -0.81, -0.01] \nGPT2 QA model 0.87 [0.85 , 0.87] 0.53 [0.45, 0.55] \nGPT2 generative model 0.92 [0.91, 0.93] 0.89 [0.87, 0.9] \n \nQuantitative performance.  \nWe compared the model extracted frequencies against the manual labeled ground truth and \nreported the performance for all four models in Table 1. Overall, the GPT-2 generative model \nwas the best-performing model, with an accuracy of 0.92 [0.91 – 0.93] and R2 score of 0.89 \n[0.87, 0.9], followed by the GPT-2 QA model (accuracy 0.87 [0.85 , 0.87], R2 0.53 [0.45, 0.55]), \nGPT-2 zero-shot model (accuracy 0.57 [0.57, 0.59], R2 -0.014 [ -0.81, -0.01]), then the \nClinicalBERT regression model (accuracy 0.27 [0.26 – 0.28], R2 0.88 [0.85, 0.89]), though the \nClinicalBERT model demonstrated a high R2 score. \n \nThe results show that GPT-2 type models outperformed the ClinicalBERT model in terms of the \nexact matching accuracy. Notably, even the zero-shot QA model performed significantly better \nthan the ClinicalBERT model in terms of classification accuracy. This suggests that classifying \none label from 30 classes (i.e. discrete value of [-1, 28]) in ClinicalBERT is not an effective \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 3, 2023. ; https://doi.org/10.1101/2023.10.02.23296403doi: medRxiv preprint \napproach for our task. In contrast, the R2 score of ClinicalBERT is high, which suggests that the \nClinicalBERT model can predict the label within a reasonable range (£ ± 3) but the GPT-2 QA \nfine-tuned model only scored 0.53 in R2 score. Comparing the GPT-2 QA and GPT-2 generative \nmodels, the latter performs better than the QA model in terms of both scores. The advantage of \ngenerative models is that when the answer directly appears in the text, the generative model can \nstill do well, but the extractive QA model might fail.  \n \nIn order to further analyze the model performance, we visualize d the scatter plot of true and \npredicted values in Fig. 2 and Bland-Altman plot in Fig. 3. GPT-2 zero-shot, even after post -\nprocessing, is under -estimating the migraine frequency (mean -7.15) and not able to extract the \nfrequency if it is not reported directly using simple language. As seen in Figures 2 and 3, the fine-\ntuned GPT-2 QA model trained with cross -entropy loss is able to estimate the frequency with \nmoderate alignment and mean difference is 2.37 (+/-12 1.96 std), while fine-tuned ClinicalBERT \nachieved decent alignment and mean difference is 0.78 (+/ -6.6 1.96 std). However, the GPT-2 \ngenerative model outperformed all the architecture and achieved 0.46 mean  difference (+/- 7.1 \n1.96 std). \nFig 2. Scatter plot visualization of true frequency reported in clinic notes (x-axis) and \npredicted frequency by the model (y-axis) in log scale. Blue line shows the perfect alignment \n(R2 = 1.0).  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 3, 2023. ; https://doi.org/10.1101/2023.10.02.23296403doi: medRxiv preprint \n  \n(a) ClinicalBERT (R2 0.88) (b) GPT-2 QA zero-shot (R2 -0.014) \n  \n(c) GPT-2 QA few-shot (R2 0.53) (d) GPT-2 generative few-shot (R2 0.89) \n \nFig 3. Bland-Altman test to compare each metric computed from the NLP model against \nthe ground truth. There are 192 data elements in total for each subfigure, with each point \nrepresenting one note in the validation dataset. Mean and standard deviation is also calculated. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 3, 2023. ; https://doi.org/10.1101/2023.10.02.23296403doi: medRxiv preprint \n  \n(a) ClinicalBERT (b) GPT-2 QA zero-shot \n  \n(c) GPT-2 QA few-shot (d) GPT-2 generative few-shot \n*The (b) GPT-2 QA zero-shot model incorrectly predicted 550 days for a sentence that includes \n“naproxen 550 mg”. Therefore, the range of prediction (X-axis) is much larger than other \nmodels. \n \nError Analysis:  \nWhile the overall performance is 92% accuracy, the GPT-2 generative model still makes \nmistakes on 8% of cases when more complicated reasoning is required. For example, “The \npatient generally has two headache-free days per week” The correct answer would be (7-2) x 4 = \n20; however, the model predicted 2 days per month. When headache frequency is documented as \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 3, 2023. ; https://doi.org/10.1101/2023.10.02.23296403doi: medRxiv preprint \na range, we labeled the maximum frequency as the ground truth, while the model often extracts \nthe minimum headache frequency. For example, ’On average, she estimates 7-10 headache days \nper month.’ The model reports 7 days which was also correct, while the ground truth label was \n10 days. More examples are presented in Table 2.  \nTable 2. Example cases selected for error analysis; We provide examples of errors from the \nGPT-2 generated model and also highlight the reasoning for analysis \nSentence Ground Truth GPT-2 Generative \nModel Prediction \nComplicated \nReasoning \nThe patient generally \nhas two headache-\nfree days per week \n20 2 2 headache-free \nmeans the patient has \n5 days headache per \nweek, and a month \nhas 4 weeks, thus the \nfinal answer is (7-2) x \n4 = 20  \nHer headaches last \napproximately 2 days \nand occur once per \nweek.  \n8 2 A headache lasts for 2 \ndays and occurs once \nper week, therefore \nthe headache \nfrequency is 2 x 4=8 \nshe has had some \ntype of pain every \nday since 2008 but \nhas more moderate-\nto-severe headaches \nabout 20 days per \nmonth. \n28 20 Ambiguity in note \ndescription- “some \ntype of pain” every \nday. A clinician \nwould assume this \ninfers to “head pain”, \nalthough it could be \npain from other \nlocations. The model \nidentified headaches \n20 days per month \nBy his headache \ndiaries, he is \ncontinuing to have a \n28 28(correct) Similar instance, but \nthe model accurately \nidentified “headache \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 3, 2023. ; https://doi.org/10.1101/2023.10.02.23296403doi: medRxiv preprint \nheadache every day \nwith about one severe \nheadache day each \nmonth and about ten \nmoderate headache \ndays each month.   \nevery day” and not 1 \nor 10 headache days \neach month, showing \nthe model was able to \nidentify “headache \nevery day” rather than \ndays with severe \nheadache \nthe headaches became \ndaily about six \nmonths ago and are \nnow daily and \nshowing increasing \nseverity with an \nincreasing proportion \nof the headaches \nreaching at least \nmoderate levels of \nintensity. \n28 6 Incorrect prediction \nof “6” in the sentence \n“6 months ago” , \ninstead of selecting \n“daily”, which yields \nthe answer of 28 \nduring the summer of \n2019, headaches \nworsened in severity, \nstill around 8-11 days \nper month, but \nseverity increased \nparticularly in august. \n11 8 In fact, the model \npredicts correctly, but \nour dataset only \nlabels the maximun \nnumber as the ground \ntruth.  This might be \nrelated to the \nrelatively small \nnumber of notes that \ncontains a range. \n \n \nDiscussion \nWe experimented with several state-of-the-art models that can reliably extract headache frequency \n(monthly headache days) based on clinical notes in the EHR with high performance and showed \nthat GPT-2 based models, specifically the GPT-2 generative model, with an accuracy of 0.92 [0.91 \n– 0.93] and R 2 score of 0.89 [0.87, 0.9], outperformed the traditional ClinicalBERT regression \nmodel (accuracy 0.27 [0.26 – 0.28], R2 0.88 [0.85, 0.89]). Even though ClinicalBERT had lower \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 3, 2023. ; https://doi.org/10.1101/2023.10.02.23296403doi: medRxiv preprint \naccuracy, it achieved a high R2 score, indicating it can reasonably predict headache frequency for \na given text within a reasonable range (£ ± 3).  \n \nWhile the GPT-2 zero-shot model did not perform well (accuracy 0.57 [0.57, 0.59], R2 -0.014 [ -\n0.81, -0.01]), the results are decent given that no training data is needed for the model. This result \nis aligned with the literature where researchers found that GPT -2 is a good zero-shot learner[12]. \nWe applied few -shot in -context learning with the same GPT -2 model which improves the \nperformance from the zero-shot (57% - 87% accuracy) (accuracy 0.87 [0.85, 0.87], R2 0.53 [0.45, \n0.55] for the GPT -2 QA model), but it failed to extract the headache frequency when the exact \nanswer is not present and/or additional reasoning is required.  \n \nIn developing the model to extract headache frequency documented in clinical notes, we were able \nto overcome challenges related to inconsistencies in documenting headache frequencies, which \ncan present in various forms in the note. While sentences like “Over the past 4 weeks, he \nexperiences 13 headache days per month”, and “Headache frequency: 26 days'' are \nstraightforward for all models to extract the number documented in the note, we noticed the \nClinicalBERT model could not reliably capture and often underestimates descriptions of daily or \nconstant headache, which represents the majority of patients seen at tertiary headache centers. \nExamples include “She continues to experience constant headaches ”- answer is 28, and “His \nheadache is still constant and daily with severe headaches occurring on 10 days per month ”- \nanswer is 28 and not 10. We overcame these challenges by fine -tuning the LLM models QA and \ngenerative paradigm by expert -annotated clinic notes, and the model learned the patterns and \ngenerated the correct answers.  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 3, 2023. ; https://doi.org/10.1101/2023.10.02.23296403doi: medRxiv preprint \n \nIn other instances, the accurate answer does not exist within the sentence and requires calculation \nbased on the content of the sentence. For example, “She has 5 headache -free days per month - \nanswer is 28 -5=23”, and “On average, she has 4 headache days per week -answer is 4x4=16” . \nWe were able to overcome those challenges by employing a GPT -2 generative model to \ncomprehend, calculate and generate the answer instead of extracting the index of the answer in the \ngiven text. \n \nAlthough previous studies have utilized NLP to extract migraine characteristics from the \nEHR[2,4], accurate extraction of headache frequency has been a challenging task, yet of great \nimportance. To our knowledge, this is the first study that reports  NLP framework s that can \naccurately extract headache frequency using LLMs. Extraction of headache frequency is of \nparticular importance since change in headache frequency is a commonly used measure for \ndetermining the effectiveness of migraine preventive treatment. The strengths of our study include \nemploying and comparing several LLM -based models with diffe rent training architectures - QA \nversus generative, and strategies- zero-shot versus few-show inference to overcome the challenges \nof variation in clinical documentation of headache frequency. We achieved high accuracy and R 2 \nscore with the GPT -2 generative model. Even though we were able to achieve 92% accuracy in \nextracting headache frequency, the study has several limitations. First, the models were fine-tuned \nand tested only on clinic notes written by headache specialists at Mayo Clinic. While the test s et \nincludes notes documented by more than 10 different headache specialists from different sites \n(Rochester, Arizona, and Florida) with various syntactic documentation patterns and preferences, \nthe model may need additional fine -tuning for an outside organization  with different practice \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 3, 2023. ; https://doi.org/10.1101/2023.10.02.23296403doi: medRxiv preprint \npatterns and patient populations . Additionally, the model has only been tested on the neurology \nconsultation notes. Extracting headache frequency documented by other specialties or from data \nsources other than clinic notes might require additional fine-tuning of the model.     \n \nConclusion \nWe leverage the generative capacity of LLM and developed high-performing, state-of-the-art \nmodels with high accuracy and R2 score to accurately extract headache frequency documented in \nclinical notes in the EHR. We showed that GPT-2 based models outperformed traditional \ntransformer model-  ClinicalBERT regression, and compared different LLM-based frameworks \nwith GPT-2 zero-shot, few-shot, question answering model, and generative models. Our results \nshowed that the GPT-2 generative model was the best-performing model that could recognize \nvarious ways of describing headache frequency in clinical notes. We developed a powerful tool \nfor EHR-based headache research as changes in headache frequency remain the gold standard for \nevaluation of treatment outcomes to migraine preventive medications. To facilitate research in \nthe field, we released the GPT-2 generative model and inference code with open-source license \nof community use in GitHub.  https://github.com/imonban/MigraneFreq_extract/tree/main \n \nFunding source: This study is partially supported by NIH/NCI, U01 CA269264-01-1, Flexible \nNLP toolkit for automatic curation of outcomes for breast cancer patients. This research was \nfunded in part by the Fisher Benefactor Fund for Migraine and Headache Research at Mayo \nClinic. \n \nFinancial Disclosures: \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 3, 2023. ; https://doi.org/10.1101/2023.10.02.23296403doi: medRxiv preprint \nIn the prior 24 months, CC has received personal compensation for consulting with Satsuma and \neNeura. She also receives research support from American Heart Association with grants paid to \nher institution. \n \nIn the prior 24 months, TJS has received personal compensation for consulting with Abbvie, \nAllergan, Amgen, Axsome, Biodelivery Science, Biohaven, Collegium, Eli Lilly, Linpharma, \nLundbeck, Satsuma, and Theranica; research grant support from American Heart Association, \nAmerican Migraine Foundation, Amgen, Henry Jackson Foundation, NIH, PCORI, SPARK \nNeuro, United States Department of Defense; royalties from UpToDate; stock options from \nAural Analytics and Nocira. \n \nOther authors report no financial disclosure. \n \nReferences \n \n[1] Burch RC, Loder S, Loder E, Smitherman TA. The Prevalence and Burden of Migraine and \nSevere Headache in the United States: Updated Statistics From Government Health Surveillance \nStudies. Headache J Head Face Pain 2015;55:21–34. https://doi.org/10.1111/head.12482. \n[2] Vandenbussche N, Hee CV, Hoste V, Paemeleire K. Using natural language processing to \nautomatically classify written self-reported narratives by patients with migraine or cluster \nheadache. J Headache Pain 2022;23:129. https://doi.org/10.1186/s10194-022-01490-0. \n[3] Guo Y, Rajwal S, Lakamana S, Chiang C-C, Menell PC, Shahid AH, et al. Generalizable \nNatural Language Processing Framework for Migraine Reporting from Social Media. AMIA Jt \nSummits Transl Sci Proc AMIA Jt Summits Transl Sci 2023;2023:261–70. \n[4] Hindiyeh NA, Riskin D, Alexander K, Cady R, Kymes S. Development and validation of a \nnovel model for characterizing migraine outcomes within real-world data. J Headache Pain \n2022;23:124. https://doi.org/10.1186/s10194-022-01493-x. \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 3, 2023. ; https://doi.org/10.1101/2023.10.02.23296403doi: medRxiv preprint \n[5] Johnson AEW, Pollard TJ, Shen L, Lehman LH, Feng M, Ghassemi M, et al. MIMIC-III, a \nfreely accessible critical care database. Sci Data 2016;3:160035. \nhttps://doi.org/10.1038/sdata.2016.35. \n[6] Alsentzer E, Murphy JR, Boag W, Weng W-H, Jin D, Naumann T, et al. Publicly Available \nClinical BERT Embeddings. ArXiv 2019. https://doi.org/10.48550/arxiv.1904.03323.  \n[7] Huang K, Altosaar J, Ranganath R. ClinicalBERT: Modeling Clinical Notes and Predicting \nHospital Readmission. ArXiv 2019. https://doi.org/10.48550/arxiv.1904.05342. \n[8] Alsentzer E, Murphy J, Boag W, Weng W-H, Jindi D, Naumann T, et al. Publicly Available \nClinical BERT Embeddings. 2019 In Proceedings of the 2nd Clinical Natural Language \nProcessing Workshop, Pages 72–78, Minneapolis, Minnesota, USA Association for \nComputational Linguistics n.d. \n[9] Bjorck J, Weinberger K, Gomes C. Understanding Decoupled and Early Weight Decay. \nArXiv 2020. https://doi.org/10.48550/arxiv.2012.13841. \n[10] Loshchilov I, Hutter F. Decoupled Weight Decay Regularization. ArXiv 2017. \nhttps://doi.org/10.48550/arxiv.1711.05101. \n[11] Rajpurkar P, Zhang J, Lopyrev K, Liang P. SQuAD: 100,000+ Questions for Machine \nComprehension of Text. InProceedings of the 2016 Conference on Empirical Methods in Natural \nLanguage Processing 2016 Nov (pp. 2383-2392). \n[12] Brown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, et al. Language Models \nare Few-Shot Learners. Advances in neural information processing systems. 2020;33:1877-901. \nArXiv 2020. https://doi.org/10.48550/arxiv.2005.14165. \n  \n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 3, 2023. ; https://doi.org/10.1101/2023.10.02.23296403doi: medRxiv preprint "
}