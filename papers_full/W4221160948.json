{
  "title": "RFormer: Transformer-Based Generative Adversarial Network for Real Fundus Image Restoration on a New Clinical Benchmark",
  "url": "https://openalex.org/W4221160948",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2097342843",
      "name": "Zhuo Deng",
      "affiliations": [
        "University Town of Shenzhen",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2617638058",
      "name": "Yuanhao Cai",
      "affiliations": [
        "Tsinghua University",
        "University Town of Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A2098210174",
      "name": "Lu Chen",
      "affiliations": [
        "Jinan University"
      ]
    },
    {
      "id": "https://openalex.org/A2102511683",
      "name": "Zheng Gong",
      "affiliations": [
        "University Town of Shenzhen",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2766312260",
      "name": "Qiqi Bao",
      "affiliations": [
        "Tsinghua University",
        "University Town of Shenzhen"
      ]
    },
    {
      "id": "https://openalex.org/A2187972768",
      "name": "Xue Yao",
      "affiliations": [
        "Jinan University"
      ]
    },
    {
      "id": "https://openalex.org/A2086678938",
      "name": "Dong Fang",
      "affiliations": [
        "Jinan University"
      ]
    },
    {
      "id": "https://openalex.org/A2119736559",
      "name": "Wenming Yang",
      "affiliations": [
        "University Town of Shenzhen",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2118020757",
      "name": "Shaochong Zhang",
      "affiliations": [
        "Jinan University"
      ]
    },
    {
      "id": "https://openalex.org/A2109964608",
      "name": "Lan Ma",
      "affiliations": [
        "University Town of Shenzhen",
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3202802159",
    "https://openalex.org/W3112383662",
    "https://openalex.org/W2293295816",
    "https://openalex.org/W2974705525",
    "https://openalex.org/W2981699788",
    "https://openalex.org/W3196736930",
    "https://openalex.org/W3085574514",
    "https://openalex.org/W3107910567",
    "https://openalex.org/W2783507708",
    "https://openalex.org/W2966096374",
    "https://openalex.org/W2901158775",
    "https://openalex.org/W2426274714",
    "https://openalex.org/W2066168964",
    "https://openalex.org/W2037760632",
    "https://openalex.org/W2149049515",
    "https://openalex.org/W2989221291",
    "https://openalex.org/W2963073614",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W2963470893",
    "https://openalex.org/W2891158090",
    "https://openalex.org/W6803606285",
    "https://openalex.org/W3106593688",
    "https://openalex.org/W2979741709",
    "https://openalex.org/W3171613875",
    "https://openalex.org/W2963800716",
    "https://openalex.org/W3035484352",
    "https://openalex.org/W3106758205",
    "https://openalex.org/W3170697543",
    "https://openalex.org/W3176096490",
    "https://openalex.org/W3107405705",
    "https://openalex.org/W3121661546",
    "https://openalex.org/W3034347506",
    "https://openalex.org/W2755908738",
    "https://openalex.org/W2901354392",
    "https://openalex.org/W3033845331",
    "https://openalex.org/W3098903345",
    "https://openalex.org/W3214281692",
    "https://openalex.org/W3205385190",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2019646504",
    "https://openalex.org/W2011592399",
    "https://openalex.org/W2777750897",
    "https://openalex.org/W2468596194",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2798876216",
    "https://openalex.org/W3191334516",
    "https://openalex.org/W3043547428",
    "https://openalex.org/W2963255313",
    "https://openalex.org/W2275363859",
    "https://openalex.org/W2983541695",
    "https://openalex.org/W3109637563",
    "https://openalex.org/W3091618893",
    "https://openalex.org/W4220675554",
    "https://openalex.org/W2962903125",
    "https://openalex.org/W2798844427",
    "https://openalex.org/W6795892075",
    "https://openalex.org/W4214612132",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6803916128",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3204538018",
    "https://openalex.org/W4321232185",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3176892444",
    "https://openalex.org/W3203925315",
    "https://openalex.org/W4214508443",
    "https://openalex.org/W3113950706",
    "https://openalex.org/W3188225995",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3203003533",
    "https://openalex.org/W6784094891",
    "https://openalex.org/W4225672218",
    "https://openalex.org/W6796568838",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W4312812783",
    "https://openalex.org/W4224294196",
    "https://openalex.org/W6838305376",
    "https://openalex.org/W4226277663",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W2962579127",
    "https://openalex.org/W2106923440",
    "https://openalex.org/W2766620038",
    "https://openalex.org/W3034785019",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6726497184",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W6755808572",
    "https://openalex.org/W2782364420",
    "https://openalex.org/W2150769593",
    "https://openalex.org/W1974969377",
    "https://openalex.org/W3101507774",
    "https://openalex.org/W4281383159",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3211771658",
    "https://openalex.org/W3169612303",
    "https://openalex.org/W3171206729",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3101566148",
    "https://openalex.org/W2896194744"
  ],
  "abstract": "Ophthalmologists have used fundus images to screen and diagnose eye diseases. However, different equipments and ophthalmologists pose large variations to the quality of fundus images. Low-quality (LQ) degraded fundus images easily lead to uncertainty in clinical screening and generally increase the risk of misdiagnosis. Thus, real fundus image restoration is worth studying. Unfortunately, real clinical benchmark has not been explored for this task so far. In this paper, we investigate the real clinical fundus image restoration problem. Firstly, We establish a clinical dataset, Real Fundus (RF), including 120 low- and high-quality (HQ) image pairs. Then we propose a novel Transformer-based Generative Adversarial Network (RFormer) to restore the real degradation of clinical fundus images. The key component in our network is the Window-based Self-Attention Block (WSAB) which captures non-local self-similarity and long-range dependencies. To produce more visually pleasant results, a Transformer-based discriminator is introduced. Extensive experiments on our clinical benchmark show that the proposed RFormer significantly outperforms the state-of-the-art (SOTA) methods. In addition, experiments of downstream tasks such as vessel segmentation and optic disc/cup detection demonstrate that our proposed RFormer benefits clinical fundus image analysis and applications.",
  "full_text": "IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMA TICS, VOL. 26, NO. 9, SEPTEMBER 2022 4645\nRFormer: T ransformer-Based Generative\nAdversarial Network for Real Fundus Image\nRestoration on a New Clinical Benchmark\nZhuo Deng , Yuanhao Cai , Lu Chen, Zheng Gong, Qiqi Bao , Xue Y ao, Dong Fang,\nWenming Y ang, Senior Member, IEEE, Shaochong Zhang, and Lan Ma\nAbstract— Ophthalmologists have used fundus images\nto screen and diagnose eye diseases. However, different\nequipments and ophthalmologists pose large variations to\nthe quality of fundus images. Low-quality (LQ) degraded\nfundus images easily lead to uncertainty in clinical screen-\ning and generally increase the risk of misdiagnosis. Thus,\nreal fundus image restoration is worth studying. Unfor-\ntunately, real clinical benchmark has not been explored\nfor this task so far. In this paper, we investigate the real\nclinical fundus image restoration problem. Firstly, We es-\ntablish a clinical dataset, Real Fundus (RF), including 120\nlow- and high-quality (HQ) image pairs. Then we propose\na novel Transformer-based Generative Adversarial Network\n(RFormer) to restore the real degradation of clinical fun-\ndus images. The key component in our network is the\nWindow-based Self-Attention Block (WSAB) which cap-\ntures non-local self-similarity and long-range depen-\ndencies. To produce more visually pleasant results, a\nTransformer-based discriminator is introduced. Extensive\nexperiments on our clinical benchmark show that the pro-\nposed RFormer signiﬁcantly outperforms the state-of-the-\nart (SOTA) methods. In addition, experiments of down-\nstream tasks such as vessel segmentation and optic\ndisc/cup detection demonstrate that our proposed RFormer\nbeneﬁts clinical fundus image analysis and applications.\nIndex Terms — Real Fundus Image Restoration,\ntransformer, generative Adversarial Network, self-\nAttention.\nManuscript received 4 January 2022; revised 27 April 2022 and 19\nJune 2022; accepted 22 June 2022. Date of publication 29 June 2022;\ndate of current version 9 September 2022. This work was supported\nin part by the Shenzhen Bay Laboratory and the Shenzhen Interna-\ntional Science and T echnology Information Center according to the\nresearch project under Grant KCXFZ20211020163813019.(Zhuo Deng\nand Yuanhao Cai are co-ﬁrst authors.) (Corresponding authors: Lan Ma;\nShaochong Zhang; Wenming Y ang.)\nZhuo Deng, Yuanhao Cai, Zheng Gong, Qiqi Bao, Wenming Y ang,\nand Lan Ma are with the Shenzhen International Graduate School,\nTsinghua University, Shenzhen 518055, China (e-mail: dz20@mails.\ntsinghua.edu.cn; 3359145729@qq.com; gz20@mails.tsinghua.edu.cn;\nbqq19@mails.tsinghua.edu.cn; yang.wenming@sz.tsinghua.edu.cn;\nmalan@sz.tsinghua.edu.cn).\nLu Chen, Xue Y ao, Dong Fang, and Shaochong Zhang are with the\nShenzhen Eye Hospital afﬁliated to Jinan University, Shenzhen 518040,\nChina (e-mail: chenludoc@outlook.com; 18925257121@163.com;\ndora.eye@hotmail.com; shaochongzhang@outlook.com).\nThe dataset, code, and models will be made publicly available at https:\n//github.com/dengzhuo-AI/Real-Fundus\nDigital Object Identiﬁer 10.1109/JBHI.2022.3187103\nI. INTRODUCTION\nD\nUE to the safety and cost-effectiveness in acquiring, fun-\ndus images are widely used by ophthalmologists for early\neye disease detection and diagnosis, including glaucoma [3]–[5],\ndiabetic retinopathy [6]–[8], cataract [9], [10], and age-related\nmacular degeneration [11], [12]. However, different equipments\nand ophthalmologists pose large variations to the quality of\nfundus images. A screening study of 5,575 patients found that\nabout 12% of fundus images are of inadequate quality to be\nreadable by ophthalmologists [13]. We analyze the factors caus-\ning the degradation in real fundus image capturing. Firstly,\npatients, especially infant patients, do not cooperate with the\ncapturing process of fundus images. Speciﬁcally, most patients\nare reluctant to undergo pupil dilation, which causes poorly\nlit and blurred fundus images. Besides, infant patients usually\ncan not resist the eye-closing reﬂex caused by a bright light\nduring ﬂash photography. Secondly, in practice, spatial pixel\nmisalignment, color, and brightness mismatch are inevitable\ndue to the changes in light conditions and misoperations of\ninexpert ophthalmologists. Thirdly, high-quality (HQ) fundus\nimages can be collected in hospitals of developed areas using\nhigh-precision fundus cameras. However, these equipments are\nexpensive and unaffordable for hospitals in some remote ar-\neas of under-developed or developing countries. As a result,\nlow-precision and portable fundus cameras are used to capture\nlow-quality (LQ) fundus images. These LQ fundus images easily\nmislead the clinical diagnosis and lead to unsatisfactory results\nof downstream tasks like blood vessels segmentation. Various\nbiomarkers of the retina ( e.g., hemorrhage, microaneurysm,\nexudate, optic nerve and optic cup) are essential in different\ndiseases. Therefore, it is necessary to ensure the prominence and\nvisibility of each marker for precise clinical diagnosis. Thus,\nwhen LQ fundus images are captured in clinical diagnosis,\nophthalmologists often repeat dozens of shots until HQ fun-\ndus images are obtained. Nonetheless, this repeated capturing\nprocess harms patients, degrades hospital efﬁciency, prevents\nreliable diagnosis of ophthalmologists, and impacts automated\nimage analysis systems.\nWe observe the clinical fundus images and ﬁnd that the main\ndegradation types of LQ images include out-of-focus blur, mo-\ntion blur, artifact, over-exposure, and over-darkness. Compared\nwith other types of degradation, blur, especially out-of-focus\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4646 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMA TICS, VOL. 26, NO. 9, SEPTEMBER 2022\nFig. 1. Our Real Fundus vs. Synthetic Dataset. (a) The pipeline of\nestablishing our clinical fundus image benchmark, Real Fundus (RF).\n(b) Artiﬁcial degradation models are used to synthesize low-quality (LQ)\nfundus images from their high-quality (HQ) counterpart. (c) Compar-\nisons of synthetic LQ fundus image and our real clinical LQ image.\n(d) Restoration results of models trained with synthetic data on RF .\nThe two CNN-based methods, I-SECRET [1] and Cofe-Net [2], fail to\nreconstruct the real clinical degraded fundus images.\nblur, poses the most severe threat to image analysis and clinical\ndiagnosis. An example is shown in Fig. 2 , where the uneven\nillumination, haze, and out-of-focus blur degradation are pre-\nsented in (a), (b), and (c), respectively. It can be observed that\nthe performance of blood vessel segmentation only collapses on\nout-of-focus blurred fundus images.\nTraditional fundus image restoration methods [14], [15] are\nmainly based on handcrafted priors. However, these model-\nbased methods achieve unsatisfactory performance and gen-\nerality due to the poor representing capacity. Recently, deep\nConvolutional Neural Networks (CNNs) have been widely used\nin natural image restoration and enhancement [16]–[18], e.g.,\nsuper resolution [19]–[25], deraining [26], deblurring [27]–[31],\nenlighten [32], [33], etc. Inspired by the success of natural\nimage restoration, CNNs have also been applied to fundus image\nrestoration [1], [2], [22], [23], [34]–[39]. Although impressive\nresults have been achieved, CNN-based methods show limita-\ntions in capturing long-range dependencies. In recent years, the\nnatural language processing (NLP) model, Transformer [40] has\nbeen introduced into computer vision and outperformed CNN-\nbased methods in many tasks. The Multi-head Self-Attention\n(MSA) in Transformer excels at modeling non-local similarity\nand long-range dependencies. This advantage of Transformer\nFig. 2. Fundus images of different degeneration types and their blood\nvessel segmentation results. From top to bottom are the low-quality (LQ)\nfundus images, blood vessel segmentation of the LQ fundus images,\nthe ground-truth high-quality (HQ) fundus images, and blood vessel\nsegmentation of the ground-truth HQ fundus images.\nmay provide a possibility to address the limitations of CNN-\nbased methods.\nExisting deep learning methods rely on a large amount of LQ\nand HQ fundus image pairs. Unfortunately, real clinical bench-\nmark has not been explored for fundus image reconstruction.\nThere remains a data-hungry problem. As shown in Fig. 1(b),t o\nget more image pairs, artiﬁcially designed degradation models\nsuch as Gaussian ﬁlter are used to synthesize degraded fun-\ndus images from their high-quality counterparts. However, as\ndepicted in Fig. 1(c) , artiﬁcial degradation is fundamentally\ndifferent from clinical degradation. As shown in Fig. 1(d) ,t h e\ntwo CNN-based methods [1], [2] trained with synthesized data\nfail in real fundus image restoration.\nIn this paper, we investigate the real fundus image\nrestoration problem, which has not been studied in the\nliterature. Our work is the ﬁrst attempt. To begin with,\nwe establish a clinical benchmark, Real Fundus (RF),\nincluding 120 LQ and HQ real clinical fundus image\npairs to alleviate the data-hungry issue. Based on this\ndataset, we propose a novel method, namely Transformer-\nbased Generative Adversarial Network (RFormer), for\nreal fundus image restoration. Speciﬁcally, the generator\nand discriminator are built up by the basic unit, Window-based\nDENG et al.: RFORMER: TRANSFORMER-BASED GENERA TIVE ADVERSARIAL NETWORK FOR REAL FUNDUS IMAGE 4647\nSelf-Attention Blocks (WSABs). The self-attention mechanism\nequipped with each basic block excels at capturing the\nnon-local self-similarity and long-range dependencies, which\nare the main limitations of existing CNN-based methods. In\nparticular, the generator adopts a U-shape structure to aggregate\nmulti-resolution contextual information. Unlike previous\nCNN-based Generative Adversarial Networks (GANs), we\nadopt a Transformer-based discriminator to extract non-local\nimage prior information and thus improve the ability of\ndiscriminator to distinguish restored fundus images from\nthe ground-truth HQ fundus images. Our Transformer-based\nadversarial training scheme encourages the generator to create\nmore plausible-looking natural and visually-pleasant images\nwith more detailed contents and structural textures.\nOur contributions can be summarized as follows:\nr We establish a new clinical benchmark, RF, to evaluate\nalgorithms in real fundus image restoration. To the best of\nour knowledge, this is the ﬁrst real fundus image dataset.\nr We propose a novel Transformer-based method, RFormer,\nfor real fundus image restoration. To the best of our\nknowledge, it is the ﬁrst attempt to explore the potential\nof Transformer for this task in the literature.\nr Comprehensive quantitative and qualitative experiments\ndemonstrate that our RFormer signiﬁcantly outperforms\nSOTA algorithms. Extensive experiments of down-\nstream tasks further validate the effectiveness of our\nmethod.\nII. RELA TEDWORK\nA. Fundus Image Restoration\nTraditional fundus image restoration and enhancement meth-\nods [14], [15] are mainly based on hand-crafted priors. For\nexample, Setiawan etal . [41] apply contrast limited adaptive\nhistogram equalization (CLAHE) to fundus image enhance-\nment. Some methods [42]–[44] decompose the reﬂection and\nillumination, achieving image enhancement and correction by\nestimating the solution in an alternate minimization scheme.\nHowever, these model-based methods achieve unsatisfactory\nperformance and generality due to the poor representing ca-\npacity. With the development of deep learning, fundus image\nrestoration has witnessed a signiﬁcant progress. CNNs [1],\n[2], [22], [23], [34]–[39] apply a powerful learning model to\nrestore LQ fundus images. For instance, Zhao etal . [23] pro-\npose an end-to-end deep CNN to remove the lesions on the\nfundus images of cataract patients. However, the cataract lesions\nare not caused by clinical fundus imaging. Sourya etal . [22],\nShen etal . [2], and Raj etal . [39] customize different synthetic\ndegradation models to better simulate the degradation types in\nactual clinical practice. However, real fundus image degradation\nis more sophisticated than synthesized degradation. It is hard\nto simulate real degradation by artiﬁcial degradation models\ncompletely. Thus, models trained on synthesized data easily fail\nin real fundus image restoration. In addition, the CNN-based\nmethods show limitations in capturing non-local self-similarity\nand long-rang dependencies, which are critical for fundus image\nreconstruction.\nB. Generative Adversarial Network\nGenerative Adversarial Network (GAN) is ﬁrstly introduced\nin [45] and has been proven successful in image synthesis\n[16]–[18], and translation [17], [18]. Subsequently, GAN is\napplied to image restoration and enhancement, e.g., super reso-\nlution [19]–[25], deraining [26], deblurring [27], enlighten [32],\n[33], dehazing [46], [47], image inpainting [48], [49], style trans-\nfer [18], [50], image editing [51], [52], medical image enhance-\nment [22], [23], [53], [54], and mobile photo enhancement [55],\n[56]. Although GAN is widely applied in low-level vision tasks,\nfew works are dedicated to improving the underlying framework\nof GAN, such as replacing the traditional CNN framework with\nTransformer. Jiang etal . [57] propose the ﬁrst Transformer-\nbased GAN, TransGAN, for image generation. Nonetheless, to\nthe best of our knowledge, the Transformer-based GAN has not\nbeen involved in fundus image restoration.\nC. Vision T ransformer\nTransformer is proposed by [40] for machine translation.\nRecently, Transformer has achieved great success in high-level\nvision, such as image classiﬁcation [58]–[62], semantic seg-\nmentation [61]–[64], human pose estimation [65]–[70], object\ndetection [61], [71]–[74], etc.Due to the advantage of capturing\nlong-range dependencies and excellent performance in many\nhigh-level vision tasks, Transformer has also been introduced\ninto low-level vision [75]–[81]. SwinIR [78] uses Swin Trans-\nformer [61] blocks to build up a residual network and achieve\nSOTA results in natural image restoration. Chen etal .[77] pro-\npose a large model IPT pre-trained on large-scale datasets with\na multitask learning scheme. MST [82] presents a spectral-wise\nTransformer for HSI reconstruction. Although Transformer has\nachieved impressive results in many tasks, its potential in fundus\nimage restoration remains under-explored.\nIII. METHODOLOGY\nA. RFormer Architecture\nThe architecture of RFormer is shown in Fig. 3, where (a) and\n(b) depict the generator and discriminator. Fig. 3(c) illustrates\nthe proposed Window-based Self-Attention Blocks (WSABs),\nwhich consists of a Feed-Forward Network (FFN) (detailed\nin Fig. 3(d) ), a Window-based Multi-head Self-Attention (W-\nMSA), and two layer normalization.\nThe generator adopts a U-shaped [83] architecture including\nan encoder, a bottleneck, and a decoder. The input LQ image\nis denoted as ILQ ∈ RH×W×3. Firstly, the generator exploits a\nprojection layer consisting of a 3×3 convolution ( conv) and\nLeakyReLU to extract shallow feature I0 ∈ RH×W×C. Sec-\nondly, 4 encoder stages are used for deep feature extraction\non I0. Each stage is composed of two consecutive WSABs and\none downsampling layer. We adopt a 4×4conv with stride 2 as\nthe downsampling layer to downscale the spatial size of feature\nmaps and double the channel dimension. Thus, the feature of the\ni-th stage in the encoder is denoted as Xi ∈ R\nH\n2i ×H\n2i ×2iC. Here,\ni =0 ,1,2,3 indicates the four stages. Thirdly, X3 undergoes\nthe bottleneck that consists of two WSABs. Subsequently,\n4648 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMA TICS, VOL. 26, NO. 9, SEPTEMBER 2022\nFig. 3. Architecture of our RFormer.(a) The generator adopts a U-shaped structure, including an encoder, a bottleneck, and a decoder.(b) Different\nfrom CNN-based GANs, our discriminator is T ransformer-based.(c) The basic unit of our RFormer is Window-based Multi-head Self-Attention Block.\n(d) Feed-Forward Network consits of two 1×1 conv layers, two GLEU activations, and a depth-wise 3×3 conv layer. .\nfollowing the spirits of U-Net, we customize a symmetrical\ndecoder, which also contains 4 stages. Each stage of the decoder\nis also composed of two WSABs and one upsampling layer.\nSimilarly, the feature maps of the i-th stage in the decoder\nis denoted as X′\ni ∈ R\nH\n2i ×H\n2i ×2i+1C. The upsampling layer is a\nbilinear interpolation followed by a 3 ×3 conv layer. To alleviate\nthe information loss caused by downsampling in the encoder,\nskip connections are used for feature fusion between the encoder\nand decoder. Finally, after undergoing the decoder, the feature\nmaps pass through a 3 ×3 conv layer to generate a residual image\nI′∈ RH×W×3. The restored fundus image can be obtained by\nIR = ILQ +I′.\nAs shown in Fig. 3(b) , the discriminator aims to distinguish\nthe restored fundus images from ground-truth high-quality fun-\ndus images. As analyzed in [17], patch-level GAN is more\neffective than image-level GAN in capturing high-resolution and\nﬁne-grained image information, which is critical for restoring\nclinical fundus image restoration. Hence, we follow the adver-\nsarial training scheme based on image patches as PatchGAN\nand further propose a Transformer-based discriminator. More\nspeciﬁcally, the discriminator employs the same architecture\nas the encoder in the generator followed by a 3 ×3 conv layer.\nThe restored fundus image IR ∈ RH×W×3 concatenated with\nthe ground-truth HQ fundus image IHQ ∈ RH×W×3 undergoes\nour proposed Transformer-based discriminator to generate the\npredicted map F ∈ RN×N×1.\nB. Window-Based Self-Attention Block\nThe emergence of Transformer provides an alternative to\naddress the limitations of CNN-based methods in modeling\nnon-local self-similarity and long-range dependencies. How-\never, as analyzed in Swin Transformer [61], the computational\ncost of the standard global Transformer is quadratic to the spatial\nsize of the input feature ( HW). This burden is nontrivial and\nsometimes unaffordable. To tackle this problem, we adopt the\nFig. 4. Illustration of the feature map partition. The feature maps\nare partitioned into non-overlapping windows, where the window-based\nmulti-head self-attention (W-MSA) is conducted.\nWindow-based Multi-head Self-Attention (W-MSA) [61] as\nthe self-attention mechanism and integrate it with the basic\nTransformer unit. The computational complexity of W-MSA\nis linear to the spatial size, which is much cheaper than that of\nstandard global MSA. Inspired by Swin Transformer [61], we\nadd window shift operations(WSO) in our proposed Window-\nbased Multi-head Self-Attention Block (WSAB) to introduce\ncross-window connections. The components of our proposed\nWSAB are shown in Fig. 3(c) . WSAB consists of a W-MSA,\nan FFN, and two layer normalization. The details of FFN are\nshown in Fig. 3(d). Then WSAB can be formulated as\nF′= W-MSA(LN(Fin))+ Fin,\nFout = FFN(LN(F′))+ F′, (1)\nwhere Fin represents the input feature maps of a WSAB. LN (·)\nrepresents the layer normalization. F′and Fout denote the output\nfeature of W-MSA and FFN respectively.\n1) Window-Based Multi-Head Self-Attention: Instead of us-\ning global correspondence, we partition the feature map into\nnon-overlapping windows. Subsequently, the token interactions\nare calculated inside each window. As shown in Fig. 4 ,g i v e n\nthe input feature map Xin ∈ RH×W×C with H and W be-\ning the height and the width. We partition Xin into L×L\nDENG et al.: RFORMER: TRANSFORMER-BASED GENERA TIVE ADVERSARIAL NETWORK FOR REAL FUNDUS IMAGE 4649\nnon-overlapping windows. The feature of the i-th window is\ndenoted as Xi\nin ∈ RL×L×C, where i ∈{ 1,2,...,N }and N =\nHW/L2. Xi\nin is ﬂattened and transposed into Xi ∈ RL2×C.W e\nconduct MSA on Xi. Firstly, Xi is linearly projected into query\nQi, key Ki, and value Vi ∈ RL2×C:\nQi = XiWQ,Ki = XiWK,Vi = XiWV , (2)\nwhere WQ,WK,WV ∈ RC×C are learnable parameters, de-\nnoting the projection matrices of the query, key and value.W e\nrespectively split Qi, Ki and Vi into k heads along the chan-\nnel dimension: Qi =[ Qi\n1,..., Qi\nk], Ki =[ Ki\n1,..., Ki\nk] and\nVi =[ Vi\n1,..., Vi\nk]. The dimension of each head is dk = C/k.\nThe Self-Attention (SA) for head j is formulated as\nSA(Qi\nj,Ki\nj,Vi\nj)= softmax\n(\nQi\njKi\nj\nT\n√dk\n)\nVi\nj, (3)\nwhere Qi\nj,Ki\nj and Vi\nj respectively represent the query, key and\nvalue of head j respectively. The output tokens Xi\no ∈ RL2×C of\nthe i-th window can be obtained by\nXi\no =\nk\nConcat\nj=1\n(\nSA(Qi\nj,Ki\nj,Vi\nj)\n)\nWO +B, (4)\nwhere Concat (·) denotes the concatenating operation, B ∈\nRL2×C represents the position embedding, and WO ∈ RC×C\nare learnable parameters. We reshape Xi\no to obtain the output\nwindow feature map Xi\nout ∈ RL×L×C.Finally, we merge all the\npatch representations {X1\nout,X2\nout,X3\nout,..., XN\nout} to obtain\nthe output feature maps Xout ∈ RH×W×C.\n2) Feed-Forward Network: As depicted in Fig. 3(d) ,t h e\nFeed-Forward Network (FFN) consists of a 1×1 conv layer\nwith a GELU activation, a depth-wise 3×3 conv layer with a\nGELU activation, and another 1×1conv layer.\nC. Loss Functions\nDuring the training procedure, we exploit the weighted sum\nof four loss functions as the overall training objective. They are\ndescribed and analyzed in the following part.\n1) Charbonnier Loss: The ﬁrst loss function is the Charbon-\nnier loss between the restored and ground-truth HQ images:\nL1(IR,IHQ)=\n√\n∥IR −IHQ∥2 +ε2 (5)\nwhere IR denotes the restored fundus image, IHQ represents the\nground-truth HQ fundus image, and εdenotes a constant which\nis empirically set to 10−3 for all the experiments.\n2) Fundus Quality Perception Loss:Unlike natural images,\nfundus images have speciﬁc acquisition process and anatomical\nstructures. This indicates fundus images have highly similar\nstyles. Therefore, we exploit high-level feature constraints to\nimprove the perceptual quality and encourage the network to\ncapture the fundus anatomical structures and styles. To this end,\nwe propose Fundus Quality Perception Loss (FQPLoss). More\nspeciﬁcally, we adopt VGG-19 [84] as the perception network\nand pre-train it on the fundus image quality evaluation dataset,\nEye-Q [85] with the fundus image quality classiﬁcation task. The\nEye-Q dataset has 28,792 fundus images with three-level quality\ngrading. The perception network trained on the Eye-Q dataset\nis capable of extracting the difference of high-level features\nbetween different qualities of fundus images. Subsequently, our\nFQPLoss can be formulated as\nLfqp = 1\nHW\nH∑\ni=1\nW∑\nj=1\n(φ(IR)(i,j)−φ(IHQ)(i,j))2 (6)\nwhere Hand W denote the height and width of the fundus image.\nφ(·) denotes the feature extraction function of the pre-trained\nperception network. Our FQPLoss is customized to assess a\nsolution with respect to perceptually relevant characteristics.\nBy minimizing the FQPLoss Lfqp , the model is encouraged\nto capture more high-level discriminative features and generate\nmore visually-pleasant results.\n3) Adversarial Loss: Our proposed Transformer-based dis-\ncriminator aims to distinguish the restored fundus images from\nground-truth HQ fundus images. More speciﬁcally, the discrim-\ninator outputs a patch score map F ∈ RNH×NW ×1, where NH\nand NW denote the height and width. The score of each position\nindicates how realistic the corresponding fundus image patch is.\nThen the adversarial loss is formulated as\nLD\nadv = 1\nNH ×NW\nNH×NW∑\ni=1\n(D(IR)[i])2 +(D(IHQ)[i]−1)2,\nLG\nadv = 1\nNH ×NW\nNH×NW∑\ni=1\n(D(IR)[i]−1)2, (7)\nwhere D(·) denotes the mapping function of our proposed\nTransformer-based discriminator. LG\nadv trains the generator to\nfool the discriminator by generating more realistic restored\nfundus images. In contrast, LD\nadv encourages the discriminator\nto distinguish the restored images from real images.\n4) Edge Loss: To enhance the high-frequency edge details,\nwe exploit the edge loss function that focuses on the gradient in-\nformation of images and enhances edge textures. To be speciﬁc,\nthe edge loss function is formulated as\nLedge(IR,IHQ)=\n√\n∥Δ(IR)−Δ(IHQ)∥2 +ε2, (8)\nwhere Δ(·) represents the Laplacian operator.\n5) The Overall Loss Function: Finally, the overall training\nobjective is the weighted sum of the above four loss functions:\nL = L1 +λ1Lfqp +λ2Ledge +λ3(LG\nadv +LD\nadv), (9)\nwhere λ1,λ2,λ3 are three hyper-parameters controlling the\nimportance balance of different loss functions. Our proposed\nRFormer is end-to-end trained by minimizing L. The weights\nof the perception network are ﬁxed. Each mini-batch training\nprocedure can be divided into two steps: (i) Fix the discrim-\ninator and train the generator. (ii) Fix the generator and train\nthe discriminator. This adversarial training scheme encourages\nthe reconstructed fundus images to be more photo-realistic and\ncloser to the real clinical HQ fundus image manifold.\n4650 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMA TICS, VOL. 26, NO. 9, SEPTEMBER 2022\nIV . REAL FUNDUS\nThis section introduces our clinical benchmark, Real Fundus\n(RF). It consists of 120 LQ and HQ clinical fundus image pairs\nwith the spatial size of 2560×2560. The training and testing\nsubsets are split in proportional to 3:1. Since blur signiﬁcantly\nimpacts clinical diagnosis and automated image analyzing sys-\ntems, it is set to the primary degradation type of LQ fundus\nimages. Besides, there are other degradation types such as arti-\nfacts and uneven illumination which are inevitably introduced\nin the fundus image capturing process.\nA. Data Collecting Process\nThe collection process of our RF obtains the exemption deter-\nmination from Shenzhen Eye Hospital and contains three steps:\ncapturing, selecting, and calibrating fundus images.\n1) Capturing: Instead of exploiting artiﬁcial degradation\nmodels (e.g., Gaussian Filter.) to synthesize LQ fundus images as\nshown in Fig. 1(b), we directly use the degraded fundus images\nfrom the fail cases in practical capturing. As depicted in Fig. 1(a),\nthe fundus images are captured by ophthalmologists using a\nZEISS VISUCAM200 fundus camera, which is a mainstream\nproduct of fundus camera. The price of ZEISS VISUCAM200\nfundus camera is about 350,000 RMB. We select clinical fundus\nimages from patients of different ages and different fundus states\n(e.g., leopard fundus, hemorrhage, microaneurysms, and drusen)\nto expand the scope of our RF.\n2) Selecting: When LQ fundus images are captured in\npractice, the operator will repeat capturing until HQ fundus\nimages are obtained. Subsequently, we manually select LQ and\nHQ fundus image pairs of the same eye. To ensure the diversity\nof RF and avoid similar data, only one image pair is selected\nwith one eye. Note that only HQ clear fundus images captured\nby experienced ophthalmologists can be used as the ground\ntruths of degraded LQ images. Based on these strict criteria, we\nﬁnally select 120 LQ and HQ fundus image pairs from the eye\nhospital database containing more than 30,00 eye instances.\nEach instance contains multiple fundus images.\n3) Calibrating: After selecting fundus image pairs, we ob-\nserve two issues in raw unprocessed fundus data. Firstly, the LQ\nand HQ fundus images are spatially misaligned (as illustrated\nin Fig. 1(a) ). Secondly, there is a large black area around the\neyeball. This black area is uninformative and may easily degrade\nthe performance of the restoration model during the training\nprocedure. Thus, to improve the quality of our RF, we calibrate\nthe collected dataset using the software, Photoshop. Speciﬁcally,\nwe ﬁrst spatially align the image pairs and then cut off the black\narea around the eyeball.\nB. Comparisons With Synthetic Dataset\n1) LQ Fundus Images:We compare the LQ images from our\nRF and synthetic dataset in Fig. 1(c) . As can be seen from the\nzoom-in patches that the artiﬁcially synthesized degradation is\nfundamentally different from the real clinical degradation.\n2) Domain Discrepancy: To validate the huge domain dis-\ncrepancy between the synthetic and real clinical datasets, we\nT ABLE I\nQUANTIT A TIVECOMP ARISONSWITH SOT A ALGORITHMS ON OUR RF AND\nCROSS-VALIDA TIONRESULTS OFRFORMER\nadopt two CNN-based fundus image restoration methods, I-\nSECRET [1] and Cofe-Net [2], to conduct ablation study. We\ntrain them with the synthetic data and then test them on our RF.\nAs shown in Fig. 1(d), the two models fail to reconstruct the real\nclinical LQ fundus images. They either yield over-smooth results\nsacriﬁcing detailed contents, or introduce visually unpleasant\nartifacts. Since the synthetic data can not be applied to real\nfundus image restoration, it still remains a severe data-hungry\nissue. To meet with this research requirement, we establish a\nlarge scale clinical dataset, RF. To the best of our knowledge,\nthis is the ﬁrst work contributing a real clinical fundus image\nrestoration benchmark.\nV. EXPERIMENTS\nA. Implementation Details\nDuring the training procedure, fundus images are ﬁrst cropped\ninto the patches with the size of 128 ×128. Then the patches\nare fed into our proposed RFormer. The Adam [89] optimizer\n(β1=0.9, β2=0.999) is adopted. The initial learning rate is set\nto 1×10−4. The cosine annealing strategy [90] is employed\nto steadily decrease the learning rate from the initial value to\n1×10−6 during the training procedure. Our RFormer is imple-\nmented by PyTorch. It takes about 12 h using an NVIDIA RTX\n3090 GPU to train for 100 epochs. The mini-batch size is set to\n4. Random ﬂipping and rotation are used for data augmentation.\nIn the testing phase, the input is the whole image with the size\nof 2560 ×2560 for fair comparison with other methods. We\nadopt peak signal-to-noise ratio (PSNR) and structural simi-\nlarity (SSIM) [91] as the metrics to evaluate the fundus image\nreconstruction performance.\nB. Comparisons With State-of-The-Art Methods\nWe provide quantitative comparisons between our RFormer\nwith seven SOTA methods including two model-based meth-\nods (GLCAE [87] and Bicubic+RL [86]), four CNN-based\nmethods (RealSR [88], ESRGAN [20], I-SECRET [1], and\nCofe-Net [2]), and one Transformer-based method (MST [82]).\nThe quantitative comparisons on our RF are shown in Table I,\nthe proposed RFormer outperforms other competitors in terms\nof PSNR and SSIM. Speciﬁcally, RFormer achieves 0.33 and\nDENG et al.: RFORMER: TRANSFORMER-BASED GENERA TIVE ADVERSARIAL NETWORK FOR REAL FUNDUS IMAGE 4651\nFig. 5. Restored fundus image comparisons on our RF . Six SOT A methods and our proposed RFormer are included.(a) LQ fundus image.\n(b) Bicubic+RL [86].(c) GLCAE [87].(d) Cofe-Net [2].(e) ESRGAN [20].(f) RealSR [88].(g) I-SECRET [1].(h) Our proposed RFormer.(i) Ground-\ntruth HQ fundus image. Our RFormer reconstructs more detailed contents and structural testures.\nFig. 6. CNN-based vs. T ransformer-based discriminators.(a) LQ fun-\ndus image.(b) Using CNN-based PixelGAN [17].(c) Using CNN-based\nPatchGAN [17]. (d) Using both CNN-based PixelGAN and PatchGAN.\n(e) Equipped with our proposed T ransformer-based discriminator.(f)\nGround truth HQ fundus image. Our T ransformer-based discriminator\nsigniﬁcantly surpasses traditional CNN-based discriminators in terms of\nrecovering detailed contents and preserving the anatomical structure.\n1.59 dB improvement in PSNR when compared to RealSR [88]\nand ESRGAN [20].\nTo verify the robustness of our RFormer, we conduct 5-fold\nand 10-fold cross-validation. The results are shown in Table I.\nAs can be seen that our Rformer still achieves robust results,\ne.g., 28.15 dB in 5-fold cross-validation and 28.38 dB in 10-fold\ncross-validation. The small gap in performance suggests that\nthe overﬁtting is moderate while the effectiveness of RFormer\nis reliable and promising.\nFig. 5 depicts the qualitative comparisons on RF. It can be\nobserved that Bicubic+RL [86], GLCAE [87], I-SECRET [1],\nand Cofe-Net [2] yield over-smooth results and fail to restore\nthe LQ blurry fundus images. Although ESRGAN [20] and\nRealSR [88] can reconstruct more high-frequency edge details,\nFig. 7. Ablation study of our FQPLoss.(a) shows the LQ fundus image.\n(b) and (c) depict the images restored by RFormers without and with\nusing FQPLoss, respectively.(d) illustrates the ground-truth HQ fundus\nimage. With our FQPLoss, the model restores more detailed anatomical\nstructure contents and high-frequency textures.\nT ABLE II\nABLA TIONSTUDY OF THELOSS FUNCTIONS AND WINDOW SHIFT\nOPERA TIONS(WSO)\nT ABLE III\nCNN-BASED VS. OUR TRANSFORMER-BASED DISCRIMINA TORS\nT ABLE IV\nABLA TIONSTUDY OF THEPAT C HSIZE OF OUR DISCRIMINA TOR\n4652 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMA TICS, VOL. 26, NO. 9, SEPTEMBER 2022\nT ABLE V\nTHE RESULTS OFIMAGE QUALITY EVALUA TION INUSER STUDY\nthey do not maintain the authenticity and anatomical structure\nof the original fundus. Some undesired artifacts are introduced\nto the restored images, which may severely mislead the clinical\ndiagnosis. In contrast, our RFormer is capable of restoring more\nﬁne-grained contents and structural details without introducing\nartifacts. Thus, the fundus anatomical structure can be well\npreserved.\nC. Ablation Study\n1) Fqploss: We adopt RFormer as the restoration model to\nconduct an ablation study to validate the effect of our FQPLoss.\nAs listed in Table II, when the FQPLoss is applied, the PSNR\nand SSIM are increased by 1.07 dB and 0.129, respectively. In\naddition, we provide visual comparisons in Fig. 7 . As depicted\nin Fig. 7(b), the model yields an over-smooth fundus image and\nfails in reconstructing the ﬁne-grained vessel details without\nFQPLoss. As shown in Fig. 7(c), when using our FQPLoss, the\nmodel restores more detailed anatomical structure contents and\nhigh-frequency textures.\n2) Discriminator: We conduct ablation study to compare\nour proposed Transformer-based discriminator with Traditional\nCNN-based discriminators. Please note that the Transformer-\nbased generator remains unchanged. The results are reported\nin Table III . Compared with the discriminators in CNN-\nbased PatchGAN [17], PixelGAN [17], etc, our Transformer-\nbased discriminator yields the best performance. We provide\nqualitative comparisons in Fig. 6 . It can be observed that\nour Transformer-based discriminator signiﬁcantly outperforms\nCNN-based discriminators in terms of recovering detailed con-\ntents and preserving the anatomical structure consistency.\n3) Patch Size: We experimentally analyze the effect of the\npatch size set in the Transformer-based discriminator. The results\nare shown in Table IV. Our RFormer achieves the best restoration\nresult with the patch size of 40 ×40.\n4) Window Size: We change the window size of W-MSA and\nconduct experiments to study its effect. The results are reported\nin Table VI. It can be observed that our RFormer yields the best\nresult when the window size is set to 8 ×8.\n5) Window Shift Operations: We conduct ablation study to\nanalyze the effect of the window shift operations. The results are\nreported in Table II. The results indicate that the window shift\noperations can build cross-window connections and improve the\nperformance of RFormer.\nD. Clinical Image Analysis and Applications\nThe ultimate goal of restoring and enhancing fundus images\nis to serve the real clinical tasks better and improve the ac-\ncuracy of clinical diagnosis. To validate the effectiveness of\nour proposed RFormer, we use it as a pre-processing technique\nfor downstream clinical image analysis tasks, including vessel\nT ABLE VI\nABLA TIONSTUDY OF THEWINDOW SIZE OF OUR PROPOSED WSAB\nsegmentation and optic disc/cup detection. LadderNet [92] and\nM-Net [93] are employed as the segmentation baselines.\n1) User Study: Since the restored fundus images should\nmeet the requirements of ophthalmologists, we adopt 30 LQ\nfundus images for user study. We use different image restoration\nmethods to enhance these LQ images. Subsequently, we display\nthese results in random order and ask the experienced ophthal-\nmologists to score the quality of the restored images based on\ntheir extensive clinical experience. The score ranges from 0 to\n100, larger values are better. The suppression of artifacts and\npreservation of lesions are taken into account. Finally, we collect\nresponses from ﬁve ophthalmologists. The score results for each\nmethod are shown in Table V. Our RFormer receives the highest\nscore for best restored results.\n2) Vessel Segmentation: We test LadderNet [92] pre-trained\non DRIVE [94] dataset for vessel segmentation on our collected\nRF. Please note that the vessel segmentation maps of real HQ\nfundus images serve as the references for comparison due to the\nlack of segmentation labels on our RF. The vessel segmentation\nresults are shown in the third row of Fig. 8. As can be seen that\nLadderNet fails in segmenting the blood vessels of clinical LQ\nfundus images. In contrast, LadderNet extracts obvious vessel\nstructure of the fundus images restored by our RFormer, which\nis closest to the segmentation results of real HQ fundus images.\nThese results clearly suggest the effectiveness of our proposed\nmethod.\n3) Optic Disc/Cup Detection: We also evaluate the effect of\nour RFormer for the downstream disc/cup detection task. We\ntest M-Net [93] pre-trained on ORIGA [95] dataset for optic\ndisc/cup detection on our collected RF. Similar to the vessel\nsegmentation task, the optic disc/cup detection results of real\nHQ fundus images function as the references due to the lack\nof segmentation labels. The qualitative comparisons of different\nfundus image restoration methods are depicted in Fig. 8 .T h e\nfourth line is the optic disc/cup detection map and the ﬁfth\nline depicts the zoom-in patches of the fourth line. It can be\nobserved that M-Net fails to detect the disc/cup on clinical LQ\nfundus images. On the contrary, M-Net detects the optic cup\nand disc more accurately on the fundus images reconstructed by\nour RFormer. This evidence veriﬁes that our method beneﬁts the\noptic disc/cup detection task.\nDENG et al.: RFORMER: TRANSFORMER-BASED GENERA TIVE ADVERSARIAL NETWORK FOR REAL FUNDUS IMAGE 4653\nFig. 8. Vessel segmentation and optic disc/cup detection on our RF . From top to bottom are fundus images, optic disc/cup patches, vessel\nsegmentation maps, optic disc/cup detection maps, and the zoom-in patches of the optic disc/cup detection maps. (a) LQ fundus image.\n(b) Bicubic+RL [86]. (c) GLCAE [87]. (d) Cofe-Net [2]. (e) ESRGAN [20]. (f) RealSR [88]. (g) I-SECRET [1]. (h) Our RFormer. (i) HQ fundus\nimage. Our restoration method can improve the performance of vessel segmentation and optic disc/cup detection.\nFig. 9. Fail cases of Rformer on our RF . In the(a) and (e) column, from top to bottom are low-quality fundus image, restored fundus image, and\nhigh-quality fundus image.(b), (c), and(d) are all the zoom-in patches of(a). (f), (g), and(h) are all the zoom-in patches of(e).\nE. Fail Cases\nAlthough RFormer achieves good performance, it may not\nwork in some scenes. Fig. 9 shows some fail cases of RFormer\non our RF dataset. In the (a) and (e) column, from top to bottom\nare LQ fundus image, restored fundus image, and HQ fundus\nimage. (b), (c), and (d) are three zoom-in patches of (a). (f),\n(g), and (h) are three zoom-in patches of (e). It can be clearly\nobserved from 9(c), (f), and (g) that our RFormer fails to remove\nthe bright spots. As can be seen from 9(b), (d), and (h) that our\nRFormer fails in enhancing the low-lights regions. It is difﬁcult\nfor RFormer to learn the feature in areas with insufﬁcient contrast\nand brightness. We will continue to improve our work according\nto these fail cases.\nVI. CONCLUSION\nIn this paper, we establish the ﬁrst real clinical fundus image\nrestoration benchmark, Real Fundus, which contains LQ and HQ\nfundus image pairs to alleviate the data-hungry issue. Our dataset\ncan help better evaluate restoration algorithms in clinical scenes.\nBased on this dataset, we propose a novel Transformer-based\nmethod, RFormer, for clinical fundus image restoration. To\nthe best of our knowledge, it is the ﬁrst attempt to explore\nthe potential of Transformer in this task. Comprehensive qual-\nitative and quantitative results demonstrate that our RFormer\nsigniﬁcantly outperforms a series of SOTA methods. Extensive\nexperiments verify that the proposed RFormer serving as a\ndata pre-processing technique can boost the performance of\n4654 IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMA TICS, VOL. 26, NO. 9, SEPTEMBER 2022\ndifferent downstream tasks, such as vessel segmentation and\noptic disc/cup detection. We hope this work can serve as a\nbaseline for real clinical fundus image restoration and beneﬁt\nthe community of medical imaging.\nREFERENCES\n[1] P. Cheng, L. Lin, Y . Huang, J. Lyu, and X. Tang, “I-secret: Importance-\nguided fundus image enhancement via semi-supervised contrastive con-\nstraining,” in Proc. Int. Conf. Med. Image Comput. Comput.- Assist.\nIntervention, 2021, pp. 87–96.\n[2] Z. Shen, H. Fu, J. Shen, and L. Shao, “Modeling and enhancing low-\nquality retinal fundus images,” IEEE Trans. Med. Imag., vol. 40, no. 3,\npp. 996–1006, Mar. 2021.\n[3] X. Chen, Y . Xu, D. W. K. Wong, T. Y . Wong, and J. Liu, “Glaucoma\ndetection based on deep convolutional neural network,” inProc. 37th Annu.\nInt. Conf. IEEE Eng. Med. Biol. Soc. (EMBC), 2015, pp. 715–718.\n[4] N. Mojab, V . Noroozi, S. Y . Philip, and J. A. Hallak, “Deep multi-task\nlearning for interpretable glaucoma detection,” in Proc. IEEE 20th Int.\nConf. Inf. Reuse Integration Data Sci. (IRI), 2019, pp. 167–174.\n[5] W. Liao, B. Zou, R. Zhao, Y . Chen, Z. He, and M. Zhou, “Clinical inter-\npretable deep learning model for glaucoma diagnosis,” IEEE J. Biomed.\nHealth Inform., vol. 24, no. 5, pp. 1405–1412, May 2020.\n[6] S. Majumder and N. Kehtarnavaz, “Multitasking deep learning model\nfor detection of ﬁve stages of diabetic retinopathy,” IEEE Access,v o l .9 ,\npp. 123220–123230, 2021.\n[7] A. He, T. Li, N. Li, K. Wang, and H. Fu, “CABNet: Category attention\nblock for imbalanced diabetic retinopathy grading,” IEEE Trans. Med.\nImag., vol. 40, no. 1, pp. 143–153, Jan. 2021.\n[8] C.-H. Hua et al., “Convolutional network with twofold feature augmenta-\ntion for diabetic retinopathy recognition from multi-modal images,” IEEE\nJ. Biomed. Health Inform., vol. 25, no. 7, pp. 2686–2697, Jul. 2021.\n[9] Y . Dong, Q. Zhang, Z. Qiao, and J.-J. Yang, “Classiﬁcation of cataract\nfundus image based on deep learning,” in Proc. IEEE Int. Conf. Imag.\nSyst. Techn. (IST), 2017, pp. 1–5.\n[10] H. Zhang, K. Niu, Y . Xiong, W. Yang, Z. He, and H. Song, “Automatic\ncataract grading methods based on deep learning,” Comput. Methods\nPrograms Biomed., vol. 182, 2019, Art. no. 104978.\n[11] Y . Peng et al., “Deepseenet: A deep learning model for automated classi-\nﬁcation of patient-based age-related macular degeneration severity from\ncolor fundus photographs,” Ophthalmol., vol. 126, no. 4, pp. 565–575,\n2019.\n[12] P. Burlina, D. E. Freund, N. Joshi, Y . Wolfson, and N. M. Bressler,\n“Detection of age-related macular degeneration via deep learning,” inProc.\nIEEE 13th Int. Symp. Biomed. Imag. (ISBI), 2016, pp. 184–188.\n[13] S. Philip, L. Cowie, and J. Olson, “The impact of the health technology\nboard for Scotland’s grading model on referrals to ophthalmology ser-\nvices,”Brit. J. Ophthalmol., vol. 89, no. 7, pp. 891–896, 2005.\n[14] M. Foracchia, E. Grisan, and A. Ruggeri, “Luminosity and contrast nor-\nmalization in retinal images,” Med. Image Anal., vol. 9, no. 3, pp. 179–190,\n2005.\n[15] S. J. Hwang, A. Kapoor, and S. B. Kang, “Context-based automatic local\nimage enhancement,” in Proc. Eur. Conf. Comput. Vis., 2012, pp. 569–582.\n[16] X. Gong, S. Chang, Y . Jiang, and Z. Wang, “Autogan: Neural architecture\nsearch for generative adversarial networks,” in Proc. IEEE/CVF Int. Conf.\nComput. Vis., 2019, pp. 3224–3234.\n[17] P. Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation\nwith conditional adversarial networks,” in Proc. IEEE Conf. Comput. Vis.\npattern Recognit., 2017, pp. 1125–1134.\n[18] J.-Y . Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image\ntranslation using cycle-consistent adversarial networks,” in Proc. IEEE\nInt. Conf. Comput. Vis., 2017, pp. 2223–2232.\n[19] C. Ledig et al., “Photo-realistic single image super-resolution using a\ngenerative adversarial network,” in Proc. IEEE Conf. Comput. Vis. pattern\nRecognit., 2017, pp. 4681–4690.\n[20] X. Wang et al., “ESRGAN: Enhanced super-resolution generative adver-\nsarial networks,” in Proc. Eur. Conf. Comput. Vis. (ECCV) Workshops,\n2018, pp. 63–79.\n[21] Y . Cai, X. Hu, H. Wang, Y . Zhang, H. Pﬁster, and D. Wei, “Learning\nto generate realistic noisy images via pixel-level noise-aware adversarial\ntraining,”Adv. Neural Inf. Process. Syst., vol. 34, pp. 3259–3270, 2021.\n[22] S. Sengupta, A. Wong, A. Singh, J. Zelek, and V . Lakshminarayanan,\n“DeSupGAN: Multi-scale feature averaging generative adversarial net-\nwork for simultaneous de-blurring and super-resolution of retinal fundus\nimages,” in Proc. Int. Workshop Ophthalmic Med. Image Anal., 2020,\npp. 32–41.\n[23] H. Zhao, B. Yang, L. Cao, and H. Li, “Data-driven enhancement of blurry\nretinal images via generative adversarial networks,” in Proc. Int. Conf.\nMed. Image Comput. Comput.- Assist. Intervention, 2019, pp. 75–83.\n[24] X. Hu, H. Wang, Y . Cai, X. Zhao, and Y . Zhang, “Pyramid orthogonal\nattention network based on dual self-similarity for accurate mr image\nsuper-resolution,” in Proc. IEEE Int. Conf. Multimedia Expo (ICME),\n2021, pp. 1–6.\n[25] X. Hu, Y . Cai, H. Wang, Y . Peng, and Y . Zhang, “Egˆ2n: Enhanced gradient\nguiding network for single mr image super-resolution,” in Proc. Optoelec-\ntron. Imag. Multimedia Technol. VII, vol. 11550, 2020, Art. no. 115500I.\n[26] R. Qian, R. T. Tan, W. Yang, J. Su, and J. Liu, “Attentive generative\nadversarial network for raindrop removal from a single image,” in Proc.\nIEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 2482–2491.\n[27] K. Zhang et al., “Deblurring by realistic blurring,” in Proc. IEEE/CVF\nConf. Comput. Vis. Pattern Recognit., 2020, pp. 2737–2746.\n[28] S. W. Zamir et al., “Learning enriched features for real image restora-\ntion and enhancement,” in Eur. Conf. Comput. Vis.. Springer, 2020,\npp. 492–511.\n[29] S.W. Zamir et al., “Multi-stage progressive image restoration,” in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2021, pp. 14821–14831.\n[30] L. Chen, X. Lu, J. Zhang, X. Chu, and C. Chen, “HINet: Half instance\nnormalization network for image restoration,” in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit., 2021, pp. 182–192.\n[31] D. Park, D. U. Kang, J. Kim, and S. Y . Chun, “Multi-temporal recurrent\nneural networks for progressive non-uniform single image deblurring with\nincremental temporal training,” in Proc. Eur. Conf. Comput. Vis., 2020,\npp. 327–343.\n[32] Y . Jiang et al., “Enlightengan: Deep light enhancement without paired\nsupervision,” IEEE Trans. Image Process., vol. 30, pp. 2340–2349,\n2021.\n[33] W. Yang, S. Wang, Y . Fang, Y . Wang, and J. Liu, “From ﬁdelity to\nperceptual quality: A semi-supervised approach for low-light image en-\nhancement,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.,\n2020, pp. 3063–3072.\n[34] B. M. Williams et al., “Fast blur detection and parametric deconvolution\nof retinal fundus images,” in Proc. Int. Workshop Ophthalmic Med. Image\nAnal., 2017, pp. 194–201.\n[35] D. Mahapatra, B. Bozorgtabar, and R. Garnavi, “Image super-resolution\nusing progressive generative adversarial networks for medical image anal-\nysis,”Computerized Med. Imag. Graph., vol. 71, pp. 30–39, 2019.\n[36] Y . Luo et al., “Dehaze of cataractous retinal images using an unpaired\ngenerative adversarial network,” IEEE J. Biomed. Health Inform., vol. 24,\nno. 12, pp. 3374–3383, Dec 2020.\n[37] J. Wang, Y .-J. Li, and K.-F. Yang, “Retinal fundus image enhancement\nwith image decomposition and visual adaptation,” Comput. Biol. Med.,\nvol. 128, 2021, Art. no. 104116.\n[38] S. Zhang, C. A. Webers, and T. T. Berendschot, “A double-pass fundus\nreﬂection model for efﬁcient single retinal image enhancement,” Signal\nProcess., vol. 192, 2022, Art. no. 108400.\n[39] A. Raj, N. A. Shah, and A. K. Tiwari, “A novel approach for fundus\nimage enhancement,” Biomed. Signal Process. Control, vol. 71, 2022,\nArt. no. 103208.\n[40] A. Vaswani et al., “Attention is all you need,” in Proc. Adv. Neural Inf.\nProcess. Syst., 2017, pp. 5998–6008.\n[41] A. W. Setiawan, T. R. Mengko, O. S. Santoso, and A. B. Suksmono, “Color\nretinal image enhancement using CLAHE,” in Proc. Int. Conf. ICT Smart\nSoc., 2013, pp. 1–3.\n[42] M. K. Ng and W. Wang, “A total variation model for retinex,” SIAM J.\nImag. Sci., vol. 4, no. 1, pp. 345–365, 2011.\n[43] W. Wang and M. K. Ng, “A nonlocal total variation model for image\ndecomposition: Illumination and reﬂectance,” Numer. Math.: Theory,\nMethods Appl., vol. 7, no. 3, pp. 334–355, 2014.\n[44] X. Fu, D. Zeng, Y . Huang, X.-P. Zhang, and X. Ding, “A weighted\nvariational model for simultaneous reﬂectance and illumination esti-\nmation,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016,\npp. 2782–2790.\n[45] I. Goodfellow et al., “Generative adversarial nets,” in Proc. Adv. Neural\nInf. Process. Syst., 2014, pp. 2672–2680.\nDENG et al.: RFORMER: TRANSFORMER-BASED GENERA TIVE ADVERSARIAL NETWORK FOR REAL FUNDUS IMAGE 4655\n[46] R. Li, J. Pan, Z. Li, and J. Tang, “Single image dehazing via conditional\ngenerative adversarial network,” in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., 2018, pp. 8202–8211.\n[47] X. Hu, Y . Cai, Z. Liu, H. Wang, and Y . Zhang, “Multi-scale selective\nfeedback network with dual loss for real image denoising,” in Proc. 30th\nInt. Joint Conf. Artif. Intell., 2021, pp. 729–735.\n[48] J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, and T. S. Huang, “Generative image\ninpainting with contextual attention,” in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., 2018, pp. 5505–5514.\n[49] C. Zheng, T.-J. Cham, and J. Cai, “Pluralistic image completion,”\nin Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. , 2019,\npp. 1438–1447.\n[50] C. Li and M. Wand, “Combining Markov random ﬁelds and convolutional\nneural networks for image synthesis,” in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., 2016, pp. 2479–2486.\n[51] S. Yang, Z. Wang, Z. Wang, N. Xu, J. Liu, and Z. Guo, “Controllable\nartistic text style transfer via shape-matching GAN,” in Proc. IEEE/CVF\nInt. Conf. Comput. Vis., 2019, pp. 4442–4451.\n[52] S. Yang, Z. Wang, J. Liu, and Z. Guo, “Deep plastic surgery: Robust\nand controllable image editing with human-drawn sketches,” in Proc. Eur.\nConf. Comput. Vis., 2020, pp. 601–617.\n[53] Y . Ma et al., “Structure and illumination constrained GAN for med-\nical image enhancement,” IEEE Trans. Med. Imag., vol. 40, no. 12,\npp. 3955–3967, Dec. 2021.\n[54] S. Chen, Q. Zhou, and H. Zou, “A novel un-supervised GA for fundus\nimage enhancement with classiﬁcation prior loss,” Electronics, vol. 11,\nno. 7, 2022.\n[55] Y . Yuan, S. Liu, J. Zhang, Y . Zhang, C. Dong, and L. Lin, “Unsuper-\nvised image super-resolution using cycle-in-cycle generative adversarial\nnetworks,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops,\n2018, pp. 701–710.\n[56] Y .-S. Chen, Y .-C. Wang, M.-H. Kao, and Y .-Y . Chuang, “Deep photo\nenhancer: Unpaired learning for image enhancement from photographs\nwith GANS,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018,\npp. 6306–6314.\n[57] Y . Jiang, S. Chang, and Z. Wang, “TransGAN: Two pure transformers can\nmake one strong GAN, and that can scale up,” in Proc. Adv. Neural Inf.\nProcess. Syst., vol. 34, 2021, pp. 14745-14758.\n[58] A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Luˇ ci´c, and C. Schmid,\n“Vivit: A video vision transformer,” inProc. IEEE/CVF Int. Conf. Comput.\nVis., 2021, pp. 6836–6846.\n[59] A. Dosovitskiy et al., “An image is worth 16x16 words: Transformers for\nimage recognition at scale,” 2020, arXiv:2010.11929.\n[60] A. Ali et al., “Xcit: Cross-covariance image transformers,” Adv. Neural\nInf. Process. Syst., vol. 34, pp. 20014–20027, 2021.\n[61] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using\nshifted windows,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2021,\npp. 10012–10022.\n[62] B. Wu et al., “Visual transformers: Token-based image representation and\nprocessing for computer vision,” 2020, arXiv:2006.03677.\n[63] H. Cao et al., “Swin-unet: Unet-like pure transformer for medical image\nsegmentation,” 2021, arXiv:2105.05537.\n[64] S. Zheng et al., “Rethinking semantic segmentation from a sequence-\nto-sequence perspective with transformers,” in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit., 2021, pp. 6881–6890.\n[65] K. Li, S. Wang, X. Zhang, Y . Xu, W. Xu, and Z. Tu, “Pose recognition\nwith cascade transformers,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit., 2021, pp. 1944–1953.\n[66] Y . Li et al., “Tokenpose: Learning keypoint tokens for human pose estima-\ntion,” inProc. IEEE/CVF Int. Conf. Comput. Vis., 2021, pp. 11313–11322.\n[67] S. Yang, Z. Quan, M. Nie, and W. Yang, “Transpose: Keypoint localiza-\ntion via transformer,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2021,\npp. 11802–11812.\n[68] Y . Cai et al., “Learning delicate local representations for multi-person pose\nestimation,” in Proc. Eur. Conf. Comput. Vis., 2020, pp. 455–472.\n[69] Z. Luo et al., “Efﬁcient human pose estimation by learning deeply aggre-\ngated representations,” in Proc. IEEE Int. Conf. Multimedia Expo (ICME),\n2021, pp. 1–6.\n[70] Y . Cai et al., “Res-steps-net for multi-person pose estimation,” in Proc.\nJoint COCO Mapillary Workshop ICCV, 2019, vol. 3.\n[71] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S.\nZagoruyko, “End-to-end object detection with transformers,” in Proc. Eur.\nConf. Comput. Vis., 2020, pp. 213–229.\n[72] X. Dai, Y . Chen, J. Yang, P. Zhang, L. Yuan, and L. Zhang, “Dynamic detr:\nEnd-to-end object detection with dynamic attention,” in Proc. IEEE/CVF\nInt. Conf. Comput. Vis., 2021, pp. 2988–2997.\n[73] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable DETR:\nDeformable transformers for end-to-end object detection,” in Proc. Int.\nConf. Learn. Representations, 2020.\n[74] J. Huang et al., “Coco keypoint challenge track technical report: Udp,” in\nProc. Joint COCO LVIS Workshop ECCV, 2020.\n[75] S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, and M.-H. Yang,\n“Restormer: Efﬁcient transformer for high-resolution image restoration,”\nin Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2022, pp. 5728–\n5739.\n[76] J. Cao, Y . Li, K. Zhang, and L. Van Gool, “Video super-resolution trans-\nformer,” 2021, arXiv:2106.06847.\n[77] H. Chen et al., “Pre-trained image processing transformer,” in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2021, pp. 12299–12310.\n[78] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte, “Swinir:\nImage restoration using swin transformer,” in Proc. IEEE/CVF Int. Conf.\nComput. Vis., 2021, pp. 1833–1844.\n[79] Z. Wang, X. Cun, J. Bao, and J. Liu, “Uformer: A general u-shaped\ntransformer for image restoration,” in Proc. IEEE/CVF Conf. Comput.\nVis. Pattern Recognit., 2022, pp. 17683–17693.\n[80] Y . Cai et al., “Mst : Multi-stage spectral-wise transformer for efﬁcient\nspectral reconstruction,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit., 2022, pp. 745–755.\n[81] Y . Cai et al., “Degradation-aware unfolding half-shufﬂe transformer for\nspectral compressive imaging,” 2022, arXiv:2205.10102.\n[82] Y . Cai et al., “Mask-guided spectral-wise transformer for efﬁcient hyper-\nspectral image reconstruction,” in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., 2022, pp. 17502–17511.\n[83] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks\nfor biomedical image segmentation,” in Proc. Int. Conf. Med. Iimage\nComput. Comput.- Assist. Intervention, 2015, pp. 234–241.\n[84] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” 2014, arXiv:1409.1556.\n[85] H. Fu et al., “Evaluation of retinal image quality assessment networks in\ndifferent color-spaces,” in Proc. Int. Conf. Med. Image Comput. Comput.-\nAssist. Intervention, 2019, pp. 48–56.\n[86] Y .-W. Tai, P. Tan, and M. S. Brown, “Richardson-lucy deblurring for scenes\nunder a projective motion path,” IEEE Trans. Pattern Anal. Mach. Intell.,\nvol. 33, no. 8, pp. 1603–1618, Aug. 2011.\n[87] Q.-C. Tian and L. D. Cohen, “Global and local contrast adaptive enhance-\nment for non-uniform illumination color images,” in Proc. IEEE Int. Conf.\nComput. Vis. Workshops, 2017, pp. 3023–3030.\n[88] X. Ji, Y . Cao, Y . Tai, C. Wang, J. Li, and F. Huang, “Real-world\nsuper-resolution via kernel estimation and noise injection,” in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops , 2020,\npp. 466–467.\n[89] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\nin Proc. Int. Conf. Learn. Representations, 2015.\n[90] I. Loshchilov and F. Hutter, “Sgdr: Stochastic gradient descent with warm\nrestarts,” 2016, arXiv:1608.03983.\n[91] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality\nassessment: From error visibility to structural similarity,” IEEE Trans.\nImage Process., vol. 13, no. 4, pp. 600–612, Apr. 2004.\n[92] J. Zhuang, “LadderNet: Multi-path networks based on U-Net for medical\nimage segmentation,” 2018, arXiv:1810.07810.\n[93] H. Fu, J. Cheng, Y . Xu, D. W. K. Wong, J. Liu, and X. Cao, “Joint optic\ndisc and cup segmentation based on multi-label deep network and polar\ntransformation,” IEEE Trans. Med. Imag., vol. 37, no. 7, pp. 1597–1605,\nJul. 2018.\n[94] J. Staal, M. D. Abràmoff, M. Niemeijer, M. A. Viergever, and B. Van\nGinneken, “Ridge-based vessel segmentation in color images of the retina,”\nIEEE Trans. Med. Imag., vol. 23, no. 4, pp. 501–509, Apr. 2004.\n[95] Z. Zhang et al., “Origa-light: An online retinal fundus image database for\nglaucoma analysis and research,” in Proc. Annu. Int. Conf. IEEE Eng. Med.\nBiol., 2010, pp. 3065–3068.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7301128506660461
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7289639115333557
    },
    {
      "name": "Discriminator",
      "score": 0.6941301822662354
    },
    {
      "name": "Fundus (uterus)",
      "score": 0.6553373336791992
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5702750086784363
    },
    {
      "name": "Computer vision",
      "score": 0.5626245737075806
    },
    {
      "name": "Transformer",
      "score": 0.548251211643219
    },
    {
      "name": "Image restoration",
      "score": 0.4868606925010681
    },
    {
      "name": "Generative adversarial network",
      "score": 0.46365317702293396
    },
    {
      "name": "Image quality",
      "score": 0.46039149165153503
    },
    {
      "name": "Segmentation",
      "score": 0.44711795449256897
    },
    {
      "name": "Image processing",
      "score": 0.2679903507232666
    },
    {
      "name": "Deep learning",
      "score": 0.26264601945877075
    },
    {
      "name": "Image (mathematics)",
      "score": 0.24992841482162476
    },
    {
      "name": "Medicine",
      "score": 0.0890507698059082
    },
    {
      "name": "Ophthalmology",
      "score": 0.08828568458557129
    },
    {
      "name": "Engineering",
      "score": 0.074677973985672
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Detector",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I3131625388",
      "name": "University Town of Shenzhen",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I159948400",
      "name": "Jinan University",
      "country": "CN"
    }
  ],
  "cited_by": 71
}