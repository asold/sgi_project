{
    "title": "Mapping Brains with Language Models: A Survey",
    "url": "https://openalex.org/W4385571979",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5002734392",
            "name": "Antonia Karamolegkou",
            "affiliations": [
                "University of Copenhagen"
            ]
        },
        {
            "id": "https://openalex.org/A5021284917",
            "name": "Mostafa Abdou",
            "affiliations": [
                "Princeton University"
            ]
        },
        {
            "id": "https://openalex.org/A5018138946",
            "name": "Anders Søgaard",
            "affiliations": [
                "University of Copenhagen"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2808197282",
        "https://openalex.org/W2947899826",
        "https://openalex.org/W2343218976",
        "https://openalex.org/W2160654481",
        "https://openalex.org/W2130167591",
        "https://openalex.org/W4304465123",
        "https://openalex.org/W3210923133",
        "https://openalex.org/W1992570774",
        "https://openalex.org/W4312107867",
        "https://openalex.org/W3108672584",
        "https://openalex.org/W2902687238",
        "https://openalex.org/W4212828284",
        "https://openalex.org/W4310631979",
        "https://openalex.org/W3118443283",
        "https://openalex.org/W2782213998",
        "https://openalex.org/W4293030666",
        "https://openalex.org/W4226320018",
        "https://openalex.org/W3163073193",
        "https://openalex.org/W4285228148",
        "https://openalex.org/W3087970256",
        "https://openalex.org/W2344975321",
        "https://openalex.org/W2170167574",
        "https://openalex.org/W4283643570",
        "https://openalex.org/W4311991162",
        "https://openalex.org/W2986138048",
        "https://openalex.org/W3104727373",
        "https://openalex.org/W2511680308",
        "https://openalex.org/W2949616263",
        "https://openalex.org/W4224928099",
        "https://openalex.org/W2168217710",
        "https://openalex.org/W4286850069",
        "https://openalex.org/W3203937348",
        "https://openalex.org/W4308341600",
        "https://openalex.org/W4307364527",
        "https://openalex.org/W3211949750",
        "https://openalex.org/W2963249739",
        "https://openalex.org/W4281765823",
        "https://openalex.org/W4288427514",
        "https://openalex.org/W2517394272",
        "https://openalex.org/W4287854368",
        "https://openalex.org/W2749357784",
        "https://openalex.org/W4211074922",
        "https://openalex.org/W2970648593",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3209721572",
        "https://openalex.org/W4304465621",
        "https://openalex.org/W2973047874",
        "https://openalex.org/W2963959059",
        "https://openalex.org/W2892147425",
        "https://openalex.org/W1849775727",
        "https://openalex.org/W2564636125",
        "https://openalex.org/W2141599568",
        "https://openalex.org/W4293463483",
        "https://openalex.org/W2805003518",
        "https://openalex.org/W3171589458",
        "https://openalex.org/W2592280765",
        "https://openalex.org/W3034503779",
        "https://openalex.org/W2904996081",
        "https://openalex.org/W3022154375",
        "https://openalex.org/W4297995647",
        "https://openalex.org/W4287122435",
        "https://openalex.org/W2151803133",
        "https://openalex.org/W1878625794"
    ],
    "abstract": "Over the years, many researchers have seemingly made the same observation: Brain and language model activations exhibit some structural similarities, enabling linear partial mappings between features extracted from neural recordings and computational language models. In an attempt to evaluate how much evidence has been accumulated for this observation, we survey over 30 studies spanning 10 datasets and 8 metrics. How much evidence has been accumulated, and what, if anything, is missing before we can draw conclusions? Our analysis of the evaluation methods used in the literature reveals that some of the metrics are less conservative. We also find that the accumulated evidence, for now, remains ambiguous, but correlations with model size and quality provide grounds for cautious optimism.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 9748–9762\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nMapping Brains with Language Models: A Survey\nAntonia Karamolegkou1, Mostafa Abdou2, Anders Søgaard1\n1University of Copenhagen, 2 Princeton University\nantka@di.ku.dk, ma4231@princeton.edu, soegaard@di.ku.dk\nAbstract\nOver the years, many researchers have seem-\ningly made the same observation: Brain and\nlanguage model activations exhibit some struc-\ntural similarities, enabling linear partial map-\npings between features extracted from neural\nrecordings and computational language models.\nIn an attempt to evaluate how much evidence\nhas been accumulated for this observation, we\nsurvey over 30 studies spanning 10 datasets\nand 8 metrics. How much evidence has been\naccumulated, and what, if anything, is missing\nbefore we can draw conclusions? Our analysis\nof the evaluation methods used in the literature\nreveals that some of the metrics are less con-\nservative. We also find that the accumulated\nevidence, for now, remains ambiguous, but cor-\nrelations with model size and quality provide\ngrounds for cautious optimism.\n1 Introduction\nAdvances in neuroimaging technologies have made\nit possible to better approximate the spatiotem-\nporal profile of the computations responsible for\nlanguage in the brain (Poldrack and Farah, 2015;\nAvberšek and Repovš, 2022). At the same time,\nadvances in natural language processing have pro-\nduced language models (LMs) with high perfor-\nmance in many tasks (Min et al., 2021).\nThis progress has motivated scientists to start\nusing state-of-the-art LMs to study neural activ-\nity in the human brain during language processing\n(Wehbe et al., 2014b; Huth et al., 2016; Schrimpf\net al., 2021; Toneva et al., 2022b; Caucheteux and\nKing, 2022). Conversely, it has also prompted NLP\nresearchers to start using neuroimaging data to eval-\nuate and improve their models (Søgaard, 2016; Bin-\ngel et al., 2016; Toneva and Wehbe, 2019; Hollen-\nstein et al., 2019; Aw and Toneva, 2023).\nAt the conceptual core of these studies lies the\nsuggestion that representations extracted from NLP\nmodels can (partially) explain the signal found in\nneural data. These representations can be based on\nco-occurrence counts (Mitchell et al., 2008; Pereira\net al., 2013; Huth et al., 2016) or syntactic and dis-\ncourse features (Wehbe et al., 2014a,b). Later stud-\nies use dense representations such as word embed-\ndings (Anderson et al., 2017; Pereira et al., 2018;\nToneva and Wehbe, 2019; Hollenstein et al., 2019)\nand recurrent neural networks to extract contex-\ntual stimuli representations (Qian et al., 2016; Jain\nand Huth, 2018; Sun et al., 2019). More recently,\ntransformer-based architectures have been shown\nto align even better with neural activity data (Gau-\nthier and Levy, 2019; Schrimpf et al., 2021; Oota\net al., 2022b).\nSuch work shows that LMs can be trained to in-\nduce representations that are seemingly predictive\nof neural recordings or features thereof. However,\npursuing the literature, it quickly becomes clear\nthat these papers all rely on different experimen-\ntal protocols and different metrics (Minnema and\nHerbelot, 2019; Hollenstein et al., 2020; Beinborn\net al., 2023). So questions are: How much evidence\nhas really been accumulated in support of structural\nsimilarities between brains and LMs? And more\nimportantly, what exactly, if anything, drives this\nalignment, and what are we to understand from it?\nAfter gathering all the studies, we examine their\nevaluation metrics and their interrelationships, pro-\nviding discussions on the corresponding findings.\nContributions Our study provides four major\ncontributions for the wider NLP audience: (a) a de-\ntailed review of the literature on mappings between\nfMRI/MEG recordings and representations from\nlanguage models; (b) an overview of the datasets\nand mapping methods; (c) an analysis of the eval-\nuation setups that have been used to link neural\nsignals with language models and how they relate;\n(d) a discussion of what drives this representational\nalignment and what we, as a field, can make of it\ngoing forward.\n9748\nTerminology First, a brief note on terminology:\nNeural response measurements refer to recordings\nof the brain activity of subjects reading or lis-\ntening to language. We focus on (a) functional\nmagnetic resonance imaging (fMRI), which mea-\nsures neuronal activity via blood oxygenation level-\ndependent contrast and has a high spatial resolution\nbut poor temporal resolution (3–6s) and (b) mag-\nnetoencephalography (MEG), which involves the\nmeasurement of the magnetic field generated by\nthe electrical activity of neurons in the cortex, pro-\nviding a more accurate resolution of the timing of\nneuronal activity.\nV oxels refer to the smallest unit of data in a\nneuroimage, being the three-dimensional equiva-\nlent of a pixel in two-dimensional images (Ger-\nber and Peterson, 2008). Finally, we use brain\ndecoding to refer to predicting stimuli from brain\nresponses (i.e. reading the brain). Brain encod-\ning will then refer to predicting brain responses\nfrom stimuli. Whereas decoding models serve as\na test for the presence of information in neural re-\nsponses, encoding models can be interpreted as\nprocess models constraining brain-computational\ntheories (Kriegeskorte and Douglas, 2019).\n2 Datasets\nTo infer a mapping between language models and\nbrains, researchers rely on datasets in which brain\nactivity is recorded in response to linguistic stim-\nuli. In some studies, the stimuli are single words\n(Mitchell et al., 2008; Anderson et al., 2017) or sen-\ntences displayed on a screen (Pereira et al., 2018).\nIn others, participants read longer stories (Wehbe\net al., 2014a; Bhattasali et al., 2020; Nastase et al.,\n2021) or listened to speech or podcasts (Huth et al.,\n2016; Antonello et al., 2021). Table 1 lists pub-\nlicly available datasets that have been used in the\ncontext of mapping language models to and from\nrecordings of brain response. Differences between\nthe datasets –the number of participants, the equip-\nment, the experimental setup, pre-processing steps,\nand probabilistic corrections – should lead us to\nexpect some variation in what researchers have con-\ncluded (Hollenstein et al., 2020).\n3 How to predict brain activity?\nIn this section, we survey work in which neural\nresponses are predicted from linguistic represen-\ntations. Such work typically aims to shed light\non how language functions in the brain. One of\nthe earliest studies exploring the mapping between\nbrain and language representations is by Mitchell\net al. (2008), who trained a linear regression model\non a set of word representations extracted from 60\nnouns using 115 semantic features based on co-\noccurrence statistics, to predict the corresponding\nfMRI representations of the same nouns. They use\npair-wise matching accuracy evaluation, extracting\ntwo words w and w′for evaluation, and showed\nthat the predicted fMRI for a word wwas closer to\nthe real fMRI image for wthan to the real fMRI\nimage for w′, at above-chance levels. Mitchell et al.\n(2008) also report percentile rank results, ranking\npredicted fMRI images by similarity with the real\nimage of w. We discuss how the metrics relate in\n§6.\nThe dataset of Mitchell et al. (2008)is also used\nby Murphy et al. (2012), who extract linguistic\nfeatures from part-of-speech taggers, stemmers,\nand dependency parsers, showing that dependency\nparsers are the most successful in predicting brain\nactivity. They also use leave-2-out pair-matching\nas their performance metric.\nLater on, Wehbe et al. (2014a) moved on to pre-\ndicting brain activation patterns for entire sentences\nrather than for isolated words. They recorded fMRI\nneural response measurements while participants\nread a chapter fromHarry Potter and the Sorcerer’s\nStone, then extracted a set of 195 features for each\nword (ranging from semantic, syntactic properties\nto visual and discourse-level features) to train a\ncomprehensive generative model that would then\npredict the time series of the fMRI activity ob-\nserved when the participants read that passage.\nLeave-2-out pair-matching accuracy is used for\nevaluation.\nHuth et al. (2016), in contrast, use fMRI record-\nings of participants listening to spoken narrative\nstories, representing each word in the corpus as a\n985-dimensional vector encoding semantic infor-\nmation driven by co-occurrence statistics. They\ntrain per-voxel linear regression models and evalu-\nate their predicted per-word fMRI images by their\nper-voxel Pearson correlation with the real fMRI\nimages, showing that 3-4 dimensions explained a\nsignificant amount of variance in the FMRI data.\nWehbe et al. (2014b) are among the first to use\nneural language models, using recurrent models to\ncompute contextualized embeddings, hidden state\nvectors of previous words, and word probabilities.\nThey run their experiments of MEG recordings\n9749\nData Authors Method N subjects\n1 60 Nouns Mitchell et al. (2008) fMRI 9\n2 Harry Potter Dataset Wehbe et al. (2014a) fMRI 8\n3 Harry Potter Dataset Wehbe et al. (2014b) MEG 8\n4 The Moth Radio Hour Dataset Huth et al. (2016) fMRI 7\n5 Pereira Dataset Pereira et al. (2018) fMRI 16\n6 Mother of all Unification Studies Schoffelen et al. (2019) fMRI, MEG 204\n7 Natural Stories Audio Dataset Zhang et al. (2020) fMRI 19\n8 Narratives Nastase et al. (2021) fMRI 345\n9 Podcast dataset Antonello et al. (2021) fMRI 5\n10 The Little Prince Datasets Li et al. (2022) fMRI 112\nTable 1: The opensource datasets that were used in the studies surveyed. Numbering used in Table 2.\nof participants reading Harry Potter, obtained in a\nfollow-up study to Wehbe et al. (2014a). From the\nthree sets of representations, they then train linear\nregression models to predict the MEG vectors cor-\nresponding to each word, and the regression mod-\nels are then evaluated by computing pair-matching\naccuracy.\nSimilarly, Søgaard (2016) evaluates static word\nembeddings on the data from Wehbe et al. (2014a),\nlearning linear transformation from word embed-\ndings into an fMRI vector space. The predictions\nare evaluated through mean squared error (MSE).\nJain and Huth (2018) evaluate recurrent language\nmodels against the fMRI dataset from Huth et al.\n(2016). Their findings show that contextual lan-\nguage model representations align significantly bet-\nter (to brain response) compared to static word\nembedding models. Their evaluation metric is the\ntotal sum of explained variance1\nFollowing this, Schwartz et al. (2019) use\nattention-based transformer language models for\nbrain mapping. They finetune BERT (Devlin\net al., 2019)to predict neural response measure-\nments from the Harry Potter dataset, showing that\nthe fine-tuned models have representations that en-\ncode more brain-activity-relevant language infor-\nmation than the non-finetuned models. They rely\non pair-matching accuracy as their performance\nmetric.\nAs in Søgaard (2016), Zhang et al. (2020) map\nstatic word embeddings into the vector space of\nthe neural response measurements (fMRI). They\nintroduce a new dataset of such measurements from\nsubjects listening to natural stories. They rely on\nexplained variance as their performance metric.\nToneva and Wehbe (2019) evaluate word and se-\nquence embeddings from 4 recurrent and attention-\n1The squared Pearson correlation coefficient. We will not\ndistinguish between studies using Pearson correlation and\nstudies using explained variance. See Appendix A.2.\nbased transformer language models, using the\nHarry Potter fMRI dataset. They evaluate mod-\nels across layers, context lengths, and attention\ntypes, using pairwise matching accuracy as their\nperformance metric. In a later study, Toneva et al.\n(2022a) induce compositional semantic represen-\ntations of \"supra-word meaning\" which they then\nuse to predict neural responses across regions of\ninterest, evaluating their models using Pearson cor-\nrelation.\nAlso using the Harry Potter data, Abnar et al.\n(2019) evaluate five models, one static and four\ncontextualized, relying on a variant of representa-\ntional similarity analysis (Kriegeskorte et al., 2008).\nThe results suggest that models provide represen-\ntations of local contexts that are well-aligned to\nneural measurements. However, as information\nfrom further away context is integrated by the mod-\nels, representations become less aligned to neural\nmeasurements.\nIn a large-scale study, Schrimpf et al. (2021)\nexamine the relationships between 43 diverse state-\nof-the-art neural network models (including embed-\nding models, recurrent models, and transformers)\nacross three datasets (two fMRI, one electrocardio-\ngraphy). They rely on a metric they term Brain\nScore which involves normalising the Pearson cor-\nrelation by a noise ceiling. Their results show that\ntransformer-based models perform better than re-\ncurrent or static models, and larger models perform\nbetter than smaller ones.\nSimilarly, in Caucheteux and King (2022), the\nSchoffelen et al. (2019) fMRI and MEG datasets\nare used to compare a variety of transformer ar-\nchitectures. They study how architectural details,\ntraining settings, and the linguistic performance of\nthese models independently account for the gener-\nation of brain correspondent representations. The\nresults suggest that the better language models are\nat predicting words from context, the better their\n9750\nactivations linearly map onto those of the brain.\nAntonello et al. (2021) evaluate three static and\nfive attention-based transformer models, in combi-\nnation with four fine-tuning tasks and two machine\ntranslation models. They train linear regression\nmodels to evaluate their word-level representations\nagainst a new fMRI dataset from participants listen-\ning to podcast stories. They find a low-dimensional\nstructure in language representations that can pre-\ndict brain responses. In a similar setting, Antonello\nand Huth (2022) examine why some features fit the\nbrain data better arguing that the reason is that they\ncapture various linguistic phenomena.\nReddy and Wehbe (2021) evaluate syntactic fea-\ntures in conjunction with BERT representations,\nfinding that syntax explains additional variance in\nbrain activity in various parts of the language sys-\ntem, even while controlling for complexity metrics\nthat capture processing load.\nIn a series of studies Caucheteux et al. (2021,\n2022b,a) investigate GPT2’s activations in predict-\ning brain signals using the Nastase et al. (2021)\ndataset. Their evaluation metric is Brain Score\n(Schrimpf et al., 2018). To determine which factors\naffect the brain encoding Pasquiou et al. (2022)\nexamine the impact of test loss, training corpus,\nmodel architecture, and fine-tuning in various mod-\nels using the Li et al. (2022) dataset. They evaluate\nmodel performance using Pearson Correlation.\nOota et al. (2022a) study the impact of con-\ntext size in language models on how they align\nwith neural response measurements. They use\nthe Nastase et al. (2021) dataset and evaluate re-\ncurrent and attention-based transformer architec-\ntures. In a later study, Oota et al. (2022b) use the\nPereira et al. (2018) dataset and evaluate BERT-\nbase models (fine-tuned for various NLP tasks).\nThey showed that neural response predictions from\nridge regression with BERT-base models fine-tuned\nfor coreference resolution, NER, and shallow syn-\ntactic parsing explained more variance for Pereira\net al. (2018) response measurements. On the other\nhand, tasks such as paraphrase generation, sum-\nmarization, and natural language inference led to\nbetter encoding performance for the Nastase et al.\n(2021) data (audio). Using the same dataset, in\nOota et al. (2022c) it is shown that the presence\nof surface, syntactic, and semantic linguistic in-\nformation is crucial for the alignment across all\nlayers of the language model. They use pairwise\nmatching accuracy and/or Pearson correlation as\ntheir performance metrics in these studies.\nAw and Toneva (2023) extract feature represen-\ntations from four attention-based transformer mod-\nels. They evaluate the impact of fine-tuning on the\nBookSum dataset (Kryscinski et al., 2021). All\nmodels are used to predict brain activity on the\nHarry Potter data. Pairwise matching accuracy and\nPearson correlation are their performance metrics.\nMerlin and Toneva (2022) focus more narrowly on\nvariants of GPT-2, showing that improvements in\nalignment with brain recordings are probably not\nbecause of the next-word prediction task or word-\nlevel semantics, but due to multi-word semantics.\nTheir reported metric is Pearson correlation.\nIntermediate summary The above studies dif-\nfer in many respects. Several metrics are used:\npairwise-matching accuracy,2 Pearson correlation\n(or Brain Score), mean squared error, and represen-\ntational similarity analysis. Even studies that report\nthe same performance metrics are not directly com-\nparable because they often report on results on dif-\nferent datasets and use slightly different protocols,\ne.g., Murphy et al. (2012) and Wehbe et al. (2014b).\nBeinborn et al. (2023) compare various encoding\nexperiments and receive very diverse results for dif-\nferent evaluation metrics. The diversity of metrics\nand data renders a direct comparison difficult. To\nremedy this, we consider how the metrics compare\nin §6.\n4 How to predict linguistic stimuli?\nDecoding models work in the other direction and\naim to predict linguistic features of the stimuli from\nrecordings of brain response. Pereira et al. (2018)\nintroduce a decoder that predicts stimuli representa-\ntion of semantic features given fMRI data.They in-\ntroduce a novel dataset of neural responses aligned\nwith annotation of concrete and abstract semantic\ncategories (such as pleasure, ignorance, cooking\netc.). They evaluate static word embeddings by\napplying ridge regression to predict per-word fMRI\nvectors. A separate regression model is trained per\ndimension, allowing for dimension-wise regulariza-\ntion. The model is evaluated in terms of pairwise\nmatching accuracy, but also in terms of percentile\nrank, adapted to the decoding scenario.\n2Some papers (Wehbe et al., 2014b; Toneva and We-\nhbe, 2019; Aw and Toneva, 2023) use a variant of pairwise-\nmatching accuracy, in which the model has to discriminate\nbetween two averages of 20 random predicted neural response\nmeasurements. We do not distinguish between the two vari-\nants.\n9751\nGauthier and Levy (2019) also train linear re-\ngression models which map from the response mea-\nsurements in Pereira et al. (2018), but to representa-\ntions of the same sentences produced by the BERT\nlanguage model finetuned on different natural lan-\nguage understanding tasks. The regression models\nare evaluated using two metrics: mean squared\nerror and average percentile rank. Their results\nshow that fine-tuning with different NLU objec-\ntives leads to worse alignment and that, somewhat\nsurprisingly, the only objective which does lead\nto better alignment is a scrambled language mod-\neling task where the model is trained to predict\nscrambled sentences.\nMinnema and Herbelot (2019) re-examine the\nwork of Pereira et al. (2018) using various metrics\n(pairwise matching accuracy, percentile rank, co-\nsine distance, R2, RSA), comparing decoder mod-\nels (ridge regression, perceptron, and convolutional\nneural networks).3 They show that positive results\nare only obtained using pairwise matching accu-\nracy.\nAbdou et al. (2021) investigate whether align-\ning language models with brain recordings can be\nimproved by biasing their attention with annota-\ntions from syntactic or semantic formalisms. They\nfine-tune the BERT models using several syntacto-\nsemantic formalisms and evaluate their alignment\nwith brain activity measurements from the Wehbe\net al. (2014a) and Pereira et al. (2018) datasets.\nTheir results – obtained using Pearson correlation\nas performance metric – are positive for two in\nthree formalisms.\nZou et al. (2022) propose a new evaluation\nmethod for decoding, a so-calledcross-modal cloze\ntask. They generate the data for the task from the\nneural response measures in Mitchell et al. (2008)\nand Wehbe et al. (2014a). The task itself amounts\nto a cloze task in which the context is prefixed by\nthe fMRI image of the masked word. They evalu-\nate models using precision@k. Note how this task\nis considerably easier than linearly mapping from\nlanguage model representations into fMRI images,\nand precision@kresults therefore cannot be com-\npared to those obtained in other settings. Their\nbest precision@1 scores are around 0.3, but only\nmarginally (0.03) better than a unimodal LM.\nFinally, Pascual et al. (2022) try a more realis-\ntic setup by predicting language from fMRI scans\n3Only the former two are linear and relevant for this meta-\nstudy.\nof subjects not included in the training. They use\nthe (Pereira et al., 2018) dataset and evaluate the\nregression models based on pairwise accuracy and\nprecision@k (or top-k accuracy). They propose\nevaluating with direct classification as a more de-\nmanding setup to evaluate and understand current\nbrain decoding models.\nIntermediate summary Decoding studies also\ndiffer in many respects. Several metrics are used:\npairwise-matching accuracy, Pearson correlation,\npercentile rank, cosine distance, precision@k, and\nrepresentational similarity analysis; and several\ndatasets are used. Gauthier and Ivanova (2018) crit-\nicize the evaluation techniques of decoding studies\nand suggest adopting task and mechanism explicit\nmodels. It is of particular interest to our study\nthat both Minnema and Herbelot (2019) only re-\nport positive results for pairwise matching accuracy\ncompared to other metrics. This suggests pairwise\nmatching accuracy is a less conservative metric\n(and maybe less reliable).\n5 Performance Metrics\nWe present the evaluation metrics used in the above\nstudies and discuss how they relate. See Table 2 for\na summary of metrics and corresponding studies.\nMitchell et al. (2008) introduce pairwise match-\ning accuracy. Because of their small sample size,\nthey use a leave-2-out cross-validation, which later\nwork also adopted. The metric is a binary classi-\nfication accuracy metric on a balanced dataset, so\na random baseline converges toward 0.5. Many\nstudies have relied on this metric, both in encoding\nand decoding (see Table 2).4\nPearson correlationPearson correlation is an-\nother widely used metric in the studies surveyed\nabove, measuring the linear relationship between\nvariables, and providing insight into the strength\nand direction of their association. Huth et al.\n(2016), compute Pearson correlation between pre-\ndicted and actual brain responses using Gaussian\nrandom vectors to test statistical significance. Re-\nsulting p-values are corrected for multiple com-\nparisons within each subject using false discovery\nrate (FDR) (Benjamini and Hochberg, 1995). Oth-\ners have used Bonferroni correction (Huth et al.,\n2016) or block-wise permutation test (Adolf et al.,\n2014) to evaluate the statistical significance of the\n4The method is often referred to as 2v2 Accuracy. The\nvariant that averages across 20 images, is then referred to as\n20v20 Accuracy.\n9752\nAuthors Data E/D Acc P/B Rank MSE RSA Cos Sim P@ k\nMitchell et al. (2008) 1 E ✓ ✓ ✓\nMurphy et al. (2012) 1 E ✓\nWehbe et al. (2014a,b) 2,3 E ✓ ✓\nHuth et al. (2016) 4 E ✓\nSøgaard (2016) 2 E ✓\nPereira et al. (2018) 5 D ✓ ✓\nJain and Huth (2018) 4 E ✓\nToneva and Wehbe (2019) 2 E ✓\nGauthier and Levy (2019) 5 D ✓ ✓ ✓\nMinnema and Herbelot (2019) 5 D ✓ ✓ ✓ ✓\nSchwartz et al. (2019) 3 E ✓\nAbnar et al. (2019) 2 E ✓\nZhang et al. (2020) 7 E ✓ ✓\nSchrimpf et al. (2021) 5 E ✓ ✓\nAbdou et al. (2021) 2,5 D ✓\nReddy and Wehbe (2021) 2 E ✓\nAntonello et al. (2021) 9 E ✓\nAntonello and Huth (2022) 9 E ✓\nCaucheteux et al. (2021, 2022b,a) 8 E ✓\nCaucheteux and King (2022) 6 E ✓\nZou et al. (2022) 1,2 D ✓\nOota et al. (2022a,b,c) 8 E ✓ ✓\nPasquiou et al. (2022) 10 E ✓\nToneva et al. (2022a) 2 E ✓\nMerlin and Toneva (2022) 2 E ✓\nPascual et al. (2022) 5 D ✓ ✓\nAw and Toneva (2023) 2 E ✓ ✓\nTable 2: Overview of what studies rely on what data and what performance metrics. See Table 1 for dataset\nnumbering. E/D: Encoding or decoding model Acc: Pairwise matching accuracy. P/B: Pearson correlation or\nBrainScore. Rank: percentile rank. MSE: mean squared error. RSA: representational similarity analysis. CosSim:\ncosine similarity. P@k: precision@k. Schrimpf et al. (2021) used a non-public fMRI dataset, too.\ncorrelation (Zhang et al., 2020). Some report R2\n(explained variance) instead of or in addition to cor-\nrelation coefficients (Minnema and Herbelot, 2019;\nReddy and Wehbe, 2021). Others have adopted a\nmore elaborate extension of Pearson correlation,\nnamely BrainScore (Schrimpf et al., 2018). Brain-\nScore is estimated on held-out test data, calculating\nPearson’s correlation between model predictions\nand neural recordings divided by the estimated ceil-\ning and averaged across voxels and participants.\nPercentile rank was first used for encoding\n(Mitchell et al., 2008), but can also be used for\ndecoding (Pereira et al., 2018; Gauthier and Levy,\n2019; Minnema and Herbelot, 2019). In encoding,\nthe predicted brain image for wis ranked along the\npredicted images for a set of candidate wordsw′by\ntheir similarity to the real (ground truth) image for\nw. The average rank is then reported. For decoding,\nthey rank word vectors rather than neural response\nimages. Note the similarity metric is unspecified,\nbut typically cosine distance is used.\nMean squared error, the average of the squared\ndifferences between word vectors and neural re-\nsponses, was first used for encoding in Søgaard\n(2016) on a held-out test split. It was also used by\nGauthier and Levy (2019).\nRepresentational similarity analysis (RSA)\nwas introduced in Kriegeskorte et al. (2008) as\na non-parametric way to characterize structural\nalignment between the geometries of representa-\ntions derived from disparate modalities. RSA ab-\nstracts away from activity patterns themselves and\ninstead computes representational similarity ma-\ntrices (RSMs), which characterize the information\ncarried by a given representation method through\nglobal similarity structure. A rank correlation co-\nefficient is computed between RSMs derived from\nthe two spaces, providing a summary statistic in-\ndicative of the overall representational alignment\nbetween them. Being non-parametric, RSA circum-\nvents many of the various methodological weak-\nnesses (such as over fitting, etc.). Gauthier and\nLevy (2019), Minnema and Herbelot (2019), and\nAbnar et al. (2019) apply (variations of) RSA to\ninvestigate the relations between different model\ncomponents, and then to study the alignment of\nthese components with brain response.\nCosine similarity was used in Mitchell et al.\n9753\n(2008) to select between the candidate images in\npairwise matching accuracy, as well as in percentile\nrank and RSA, but the raw cosine similarities be-\ntween predicted and real images or embeddings can\nalso be used as a metric. Minnema and Herbelot\n(2019) use this metric to quantify how close the\npredicted word vectors are to the target. Finally,\nZou et al. (2022) use precision@k, a standard met-\nric in other mapping problems, e.g., cross-lingual\nword embeddings (Søgaard et al., 2019).\nComparisons Most metrics are used to evalu-\nate both encoding and decoding models (pairwise\nmatching accuracy, Pearson correlation, percentile\nrank, MSE, RSA, cosine distance). Results for\ntwo of the most widely used metrics – pairwise\nmatching accuracy5 and percentile rank – tend to\nbe around 0.7–0.8 with generally better results for\nmore recent architectures and larger LMs. To draw\nconclusions across studies relying on different met-\nrics, we need to investigate which metrics are more\nconservative, and how different metrics relate.\nPairwise matching accuracy vs. Pearson cor-\nrelation It seems that pairwise matching accu-\nracy tends to increase monotonically with Pear-\nson correlation. Consider three sets of distances\nover corresponding point sets, A, B, and C. If\nA and B are more strongly linearly correlated\nthan A and C, under an optimal linear mapping\nΩ (minimizing point-wise squared error distance),\nE[(a−bΩ)2] > E[(a−cΩ)2]. Even in this con-\nservative setting in our synthetic experiments in\nAppendix A.1, the correlation between matching\naccuracy and percentile rank was very high, ~0.9.\nPairwise matching accuracy vs. percentile rank\nBoth metrics have random baseline scores of 0.5,\nand they will converge in the limit. If ahas a per-\ncentile rank of pin a list A, it will be higher than a\nrandom member of Appercent of the time. In our\nexperiments in Appendix A.1, the correlation con-\nverges toward 1.0, with values consistently higher\nthan 0.8 for N = 100.\nPairwise matching accuracy vs. precision@k\nare also positively correlated. Perfect score in one\nentails perfect score in the other, but precision@k\ncan of course be very small for very high values of\npairwise matching accuracy (especially if the set of\ncandidate words is big). Conversely, we can have\n5When discriminating averages over 20 images (Wehbe\net al., 2014b), scores are naturally lower.\nsaturation for high values of k, because matching\naccuracies higher than n−k\nn will mean near-perfect\nprecision@kscores. In practice, precision@k(for\nlow values of k) will be much more conservative,\nhowever. The correlation coefficient for N = 100\n(see Appendix A.1) tends to lie around 0.7.\nRelative strength Pairwise Matching Accuracy\nis a relatively permissible performance metric. To\nsee this, consider the scenario in which all target\nwords can be divided into two equal-sized buckets\nbased on word length (number of characters). Say\nthe neural responses capture nothing but this binary\ndistinction between long and short words, but do so\nperfectly. Moreover, our mapping method, e.g., lin-\near regression, learns this from training data. Now,\nfrom this alone, the pairwise matching accuracy\nwill converge toward µ = 0.75, since our model\nwill do perfectly (1.0) on half of the data, and ex-\nhibit random performance (0.5) on the other half. If\nthe neural responses tracked word length (and not\njust the distinction between short and long words),\nperformance would be even better. In other words,\nPairwise Matching Accuracy scores around 0.7-0.8\n(observed in the studies above) may only reflect\nvery shallow processing characteristics. The fact\nthat Minnema and Herbelot (2019) only observed\ngood results with this metric, led them to adopt a\nrather critical stance, for good reasons.\nOther metrics are clearly more conservative. For\na set of n candidate words, a random mapping\nwill induce a precision@1-score of 1\nn . While hubs\nmay inflate scores for larger values, the metric\nis extremely conservative for small values of k.\nHowever, only Zou et al. (2022) use this metric,\nand they modify the experimental protocol substan-\ntially, making the task much easier by providing\nadditional input to a non-linear model. The small\nimprovement from adding neural response input is\ninteresting, but could potentially be explained by\nshallow processing characteristics.\nThey argue that analogy testing would provide a\nbetter evaluation protocol:\none would ideally use standard metrics\nsuch as semantic relatedness judgment\ntasks, analogy tasks, etc. [but] this is not\npossible due to the limited vocabulary\nsizes of the available brain datasets\nSuch evaluation is possible on small scale,\nthough, and increasingly larger fMRI datasets are\nbecoming available (see above). Zhang et al. (2020)\n9754\nhave identified analogical reasoning in fMRI brain\nactivation spaces. The analogies are computed us-\ning vector offset and probe the systematicity of\nhow semantic relations are encoded. If a model\nencodes the capital-of relation systematically, we\ncan retrieve the capital of Germany by subtracting\nthe fMRI vector for ’Paris’ from the sum of our\nthe fMRI vectors for Germany and France. This\nis the same kind of analogical reasoning found in\nlanguage models (Mikolov et al., 2013). Garneau\net al. (2021) show that the more language models\nsatisfy analogies, the more isomorphic they are.\nSo far, it seems that, with the possible excep-\ntion of Zhang et al. (2020), there is little evidence\nfor structural similarities, beyond what could be\ninduced by shallow processing characteristics, but\nwhat about all the studies that report strong Pearson\ncorrelations? Per-voxel correlation coefficients are\nlow on average, but across the above studies, typi-\ncally only around 4-40% of the voxels exhibit sig-\nnificant correlations (Huth et al., 2016; Caucheteux\nand King, 2022). Since these correlations have\nbeen replicated across different datasets, they are\ngenerally not disputed, but could still reflect rather\nshallow processing characteristics.\nOn a more positive note, several studies show\nthat larger (and better) language models align bet-\nter with neural response measurements (Schrimpf\net al., 2021; Caucheteux and King, 2022). This\nsuggests that language models in the future may\nalign even better with such measurements, possibly\nreflecting properties of deep processing. Such cor-\nrelations with model quality and size are positive,\nmaking the results reported above more credible.\nGenerally, the conclusions we can draw from the\nabove studies are somewhat vague. There are two\nreasons for this: (i) Past studies have relied on per-\nmissible (pairwise matching accuracy) and ambigu-\nous (Pearson correlation) performance metrics; and\n(ii) past studies have relied on small-sized datasets.\nWe believe that this calls for a meta-analysis of\nthe above studies. To provide grounds for such a\nmeta-analysis, we have in this section taken steps\nto compare the metrics used in these studies. We\nleave it for future work to explore various ways\neffect sizes can be computed across these studies.\n6 Discussion\nMany studies, summarized above, aim to com-\npare language model representations with neural\nresponse measurements using linear mapping mod-\nels. Our main reason to focus on linear mapping\nmodels is that they quantify the degree of struc-\ntural similarity (isomorphism). Overall, results sug-\ngest that structural similarities between language\nmodels and neural responses exist. Furthermore,\nthere is good evidence that alignment has correlated\npositively with model quality and model size, sug-\ngesting a certain level of convergence as language\nmodels improve.\nWhat drives alignment? Is alignment driven by\ndeep processing characteristics or by shallow tex-\ntual characteristics? Classical candidates for shal-\nlow ones would be word length, frequency, regu-\nlarity, and part of speech. Mitchell et al. (2008),\nfor example, only controlled for part of speech.\nSome authors have presented results to suggest\nthat alignments are driven by syntactic or seman-\ntic factors (Abdou et al., 2021; Reddy and We-\nhbe, 2021; Caucheteux et al., 2021; Zhang et al.,\n2020), whereas others have claimed some similari-\nties reflect semantic phenomena (Huth et al., 2016;\nCaucheteux et al., 2021). Others suggest that align-\nments reflect deeper similarities between model ob-\njectives and predictive processing in human brains\n(Schrimpf et al., 2018; Caucheteux et al., 2022a;\nGoldstein et al., 2021), but see Antonello and Huth\n(2022) for a critical discussion of such work.\nLinguistically-transparent models that allow for\na principled decomposition of a model’s compo-\nnents into smaller linguistically meaningful units\nand models that move towards possible neurobio-\nlogical implementations of neural computation are\nlikely to be key for answering this question (Hale\net al., 2022; Ten Oever et al., 2022). Given the\nplethora of interpretability methods recently devel-\noped, however, we believe that even models which\nare not intrinsically interpretable can be useful to-\nward this goal.\nDo some models align better?Most studies ob-\nserve that better and larger, contextual models align\nbetter with neural responses (Jain and Huth, 2018;\nCaucheteux and King, 2022). Other improvements\ninclude fine-tuning on specific tasks (Oota et al.,\n2022b; Aw and Toneva, 2023). Pasquiou et al.\n(2022) outline the impact of model training choices.\nWhat metrics? The inconsistent use of perfor-\nmance metrics makes it hard to compare and inter-\npret the results reported in the literature (Beinborn\net al., 2023). We have shown that some metrics\nare perhaps too permissible to detect structural sim-\n9755\nilarities between language models and neural re-\nsponses. We have argued that precision@kis more\nconservative than most other metrics. Minnema\nand Herbelot (2019) have proposed using analogy\nscores. In the limit (given sufficient analogies),\nperfect analogical accuracy implies isomorphism\n(Garneau et al., 2021). So do perfect precision@1\nand perfect RSA scores. We, therefore, propose\ngiving priority to these performance metrics, not\nto conflate shallow processing characteristics with\ndeeper, more semantic properties.\nMeta-analysis? Proper meta-analysis is cur-\nrently hindered by the use of different metrics, but\nwe have taken steps to relate these.\n7 Conclusions\nWe surveyed work on linear mappings between\nneural response measurements and language model\nrepresentations, with a focus on metrics. In par-\nticular, we surveyed a broad range of 30 studies\nspanning across 10 datasets and 8 metrics. By\nexamining the metrics, and relating them to one\nanother, we attempt to critically assess the accumu-\nlated evidence for structural similarity between neu-\nral responses and language model representations.\nWe find that similarities with existing models are\nlimited to moderate, and there is a possibility they\nmight be explained by shallow processing charac-\nteristics since there is no standardised methodology\nfor employing controls, but also that positive cor-\nrelations with model quality and size suggest that\nlanguage models may exhibit deeper similarities\nwith neural responses in years to come.\nLimitations\nThis work focuses on a specific view of the whole\nneuro-computational modeling field. We exclude\nspecific angles of research such as non-linear mod-\nels (Ruan et al., 2016; Qian et al., 2016; Bingel\net al., 2016; Anderson et al., 2017; Oota et al.,\n2018) since we want to evaluate the accumulated\nevidence for structural similarity (isomorphism)\nbetween neural responses and language models.\n(Ivanova et al., 2022) mention several advantages\nof using linear mapping models, they are more in-\nterpretable and more biologically plausible. They\nalso provide an insightful discussion on mapping\nmodel choice, emphasizing the importance of esti-\nmating models’ complexity over categorizing them\nas purely linear or nonlinear.\nAnother limitation is that we do not include\nspeech models (Vaidya et al., 2022; Défossez et al.,\n2022; Millet et al., 2022) that have been used to\nmap brain representations mostly due to coherency\nand page-limit restrictions. The survey is also lim-\nited to fMRI and MEG data instead of other modal-\nities for two many reasons: (i) fMRI and MEG are\nused as a combination in many studies (Caucheteux\nand King, 2022; Schrimpf et al., 2021; Toneva et al.,\n2022a), and (ii) they offer high spatial resolution\nand signal reliability (fMRI) and better temporal\nand spatial resolution (MEG), making them suit-\nable for NLP (Hollenstein et al., 2020). For a sur-\nvey in encoding and decoding models in cognitive\nelectrophysiology, see Holdgraf et al. (2017).\nEthics Statement\nThe use of publicly available data in this survey\nensures compliance with ethical guidelines while\nacknowledging the initial consent provided by the\nparticipants for data capture and sharing. Partic-\nipants’ consent is a crucial ethical consideration\nin the collection and sharing of fMRI and MEG\ndata, and the preservation of legal and ethical rights\nshould always be prioritized. By upholding ethical\nprinciples, researchers can responsibly contribute\nto the field of brain encoding and decoding, advanc-\ning our understanding of neural processes without\ncompromising individual rights and privacy. Re-\nsearchers should ensure secure storage, anonymiza-\ntion, and limited access to sensitive neuroimaging\ndata, adhering to data protection regulations and\nguidelines.\nFurthermore, it is essential to prioritize the dis-\nsemination of research findings in a responsible\nmanner, with clear and accurate communication\nthat respects the limits and uncertainties of scien-\ntific knowledge. Openness and transparency in\nreporting methods, results, and interpretations con-\ntribute to the overall integrity of the research field.\nAdditionally, fostering a culture of collaboration,\nrespect, and acknowledgment of the contributions\nof participants, colleagues, and the wider scientific\ncommunity promotes ethical conduct and respon-\nsible research practices in brain encoding and de-\ncoding. By adhering to these ethical principles,\nresearchers not only advance scientific knowledge\nbut also build public trust, enhance the societal\nimpact of their work, and ensure the long-term sus-\ntainability and progress of the field.\n9756\nAcknowledgements\nThis work is supported by the Novo Nordisk Foun-\ndation. Antonia Karamolegkou was supported by\nthe Onassis Foundation - Scholarship ID: F ZP\n017-2/2022-2023’.\nReferences\nMostafa Abdou, Ana Valeria González, Mariya Toneva,\nDaniel Hershcovich, and Anders Søgaard. 2021.\nDoes injecting linguistic structure into language mod-\nels lead to better alignment with brain recordings?\nArXiv, abs/2101.12608.\nSamira Abnar, Lisa Beinborn, Rochelle Choenni, and\nWillem Zuidema. 2019. Blackbox meets blackbox:\nRepresentational similarity & stability analysis of\nneural language models and brains. In Proceedings\nof the 2019 ACL Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n191–203, Florence, Italy. Association for Computa-\ntional Linguistics.\nDaniela Adolf, Snezhana Weston, Sebastian Baecke,\nMichael Luchtmann, Johannes Bernarding, and\nSiegfried Kropf. 2014. Increasing the reliability of\ndata analysis of functional magnetic resonance imag-\ning by applying a new blockwise permutation method.\nFront. Neuroinform., 8:72.\nAndrew J. Anderson, Douwe Kiela, Stephen Clark, and\nMassimo Poesio. 2017. Visually grounded and tex-\ntual semantic models differentially decode brain ac-\ntivity associated with concrete and abstract nouns.\nTransactions of the Association for Computational\nLinguistics, 5:17–30.\nRichard Antonello and Alexander Huth. 2022. Predic-\ntive Coding or Just Feature Discovery? An Alter-\nnative Account of Why Language Models Fit Brain\nData. Neurobiology of Language, pages 1–16.\nRichard Antonello, Javier S. Turek, Vy Ai V o, and\nAlexander Huth. 2021. Low-dimensional structure in\nthe space of language representations is reflected in\nbrain responses. In Advances in Neural Information\nProcessing Systems 34: Annual Conference on Neu-\nral Information Processing Systems 2021, NeurIPS\n2021, December 6-14, 2021, virtual , pages 8332–\n8344.\nLev Kiar Avberšek and Grega Repovš. 2022. Deep\nlearning in neuroimaging data analysis: Applications,\nchallenges, and solutions.\nKhai Loong Aw and Mariya Toneva. 2023. Training\nlanguage models to summarize narratives improves\nbrain alignment. In The Eleventh International Con-\nference on Learning Representations.\nLisa Beinborn, Samira Abnar, and Rochelle Choenni.\n2023. Robust evaluation of language-brain encoding\nexperiments. In Gelbukh, A. (eds) Computational\nLinguistics and Intelligent Text Processing. CICLing\n2019., volume 13451 of Lecture Notes in Computer\nScience. Springer, Cham.\nYoav Benjamini and Yosef Hochberg. 1995. Controlling\nthe false discovery rate - a practical and powerful\napproach to multiple testing. J. Royal Statist. Soc.,\nSeries B, 57:289 – 300.\nShohini Bhattasali, Jonathan R. Brennan, Wen-Ming\nLuh, Berta Franzluebbers, and John T. Hale. 2020.\n\"the alice dataset: fmri dataset to study natural lan-\nguage comprehension in the brain\".\nJoachim Bingel, Maria Barrett, and Anders Søgaard.\n2016. Extracting token-level signals of syntactic\nprocessing from fMRI - with an application to PoS\ninduction. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers) , pages 747–755, Berlin,\nGermany. Association for Computational Linguis-\ntics.\nCharlotte Caucheteux, Alexandre Gramfort, and Jean-\nRemi King. 2021. Disentangling syntax and seman-\ntics in the brain with deep networks. In Proceedings\nof the 38th International Conference on Machine\nLearning, volume 139 of Proceedings of Machine\nLearning Research, pages 1336–1348. PMLR.\nCharlotte Caucheteux, Alexandre Gramfort, and Jean-\nRémi King. 2022a. Long-range and hierarchical lan-\nguage predictions in brains and algorithms. Nature\nHuman Behaviour, abs/2111.14232.\nCharlotte Caucheteux, Alexandre Gramfort, and Jean-\nRémi King. 2022b. Deep language algorithms pre-\ndict semantic comprehension from brain activity. Na-\nture Scientific Reports, 12.\nCharlotte Caucheteux and Jean-Rémi King. 2022.\nBrains and algorithms partially converge in natu-\nral language processing. Communications Biology,\n5:134.\nAlexandre Défossez, Charlotte Caucheteux, Jeremy\nRapin, Ori Kabeli, and Jean-Rémi King. 2022. De-\ncoding speech from non-invasive brain recordings.\nWorking paper or preprint.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nNicolas Garneau, Mareike Hartmann, Anders Sandholm,\nSebastian Ruder, Ivan Vulic, and Anders Søgaard.\n2021. Analogy training multilingual encoders. Pro-\nceedings of the AAAI Conference on Artificial Intelli-\ngence, 35(14):12884–12892.\n9757\nJon Gauthier and Anna A. Ivanova. 2018. Does the\nbrain represent words? an evaluation of brain de-\ncoding studies of language understanding. ArXiv,\nabs/1806.00591.\nJon Gauthier and Roger Levy. 2019. Linking artificial\nand human neural representations of language. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 529–539, Hong\nKong, China. Association for Computational Linguis-\ntics.\nAndrew J Gerber and Bradley S Peterson. 2008. What\nis an image? Journal of the American Academy of\nChild and Adolescent Psychiatry, 47(3):245–248.\nAriel Goldstein, Zaid Zada, Eliav Buchnik, Mariano\nSchain, Amy Price, Bobbi Aubrey, Samuel A. Nas-\ntase, Amir Feder, Dotan Emanuel, Alon Cohen, Aren\nJansen, Harshvardhan Gazula, Gina Choe, Aditi Rao,\nSe Catherine Kim, Colton Casto, Lora Fanda, Werner\nDoyle, Daniel Friedman, Patricia Dugan, Lucia Mel-\nloni, Roi Reichart, Sasha Devore, Adeen Flinker, Liat\nHasenfratz, Omer Levy, Avinatan Hassidim, Michael\nBrenner, Yossi Matias, Kenneth A. Norman, Orrin\nDevinsky, and Uri Hasson. 2021. Thinking ahead:\nspontaneous prediction in context as a keystone of\nlanguage in humans and machines. bioRxiv.\nJohn T. Hale, Luca Campanelli, Jixing Li, Shohini Bhat-\ntasali, Christophe Pallier, and Jonathan R. Brennan.\n2022. Neurocomputational models of language pro-\ncessing. Annual Review of Linguistics, 8(1):427–446.\nChristopher R Holdgraf, Jochem W Rieger, Cristiano\nMicheli, Stephanie Martin, Robert T Knight, and\nFrederic E Theunissen. 2017. Encoding and decod-\ning models in cognitive electrophysiology. Frontiers\nin Systems Neuroscience, 11:61.\nNora Hollenstein, Maria Barrett, and Lisa Beinborn.\n2020. Towards best practices for leveraging human\nlanguage processing signals for natural language pro-\ncessing. In Proceedings of the Second Workshop on\nLinguistic and Neurocognitive Resources, pages 15–\n27, Marseille, France. European Language Resources\nAssociation.\nNora Hollenstein, Antonio de la Torre, Nicolas Langer,\nand Ce Zhang. 2019. CogniVal: A framework for\ncognitive word embedding evaluation. In Proceed-\nings of the 23rd Conference on Computational Nat-\nural Language Learning (CoNLL), pages 538–549,\nHong Kong, China. Association for Computational\nLinguistics.\nAlexander G Huth, Wendy A de Heer, Thomas L Grif-\nfiths, Frédéric E Theunissen, and Jack L Gallant.\n2016. Natural speech reveals the semantic maps that\ntile human cerebral cortex. Nature, 532(7600):453–\n458.\nAnna A Ivanova, Martin Schrimpf, Stefano Anzellotti,\nNoga Zaslavsky, Evelina Fedorenko, and Leyla Isik.\n2022. Beyond linear regression: mapping models\nin cognitive neuroscience should align with research\ngoals. Neurons, Behavior, Data analysis, and Theory,\n1.\nShailee Jain and Alexander Huth. 2018. Incorporating\ncontext into language encoding models for fmri. In\nAdvances in Neural Information Processing Systems,\nvolume 31. Curran Associates, Inc.\nNikolaus Kriegeskorte and Pamela Douglas. 2019. In-\nterpreting encoding and decoding models. Current\nopinion in neurobiology, 55, page 67–179.\nNikolaus Kriegeskorte, Marieke Mur, and Peter Ban-\ndettini. 2008. Representational similarity analysis\n- connecting the branches of systems neuroscience.\nFrontiers in Systems Neuroscience, 2:4.\nWojciech Kryscinski, Nazneen Rajani, Divyansh Agar-\nwal, Caiming Xiong, and Dragomir R. Radev. 2021.\nBooksum: A collection of datasets for long-form\nnarrative summarization. ArXiv, abs/2105.08209.\nJixing Li, Shohini Bhattasali, Shulin Zhang, Berta\nFranzluebbers, Wen-Ming Luh, R Nathan Spreng,\nJonathan R Brennan, Yiming Yang, Christophe Pal-\nlier, and John Hale. 2022. Le petit prince multilingual\nnaturalistic fMRI corpus. Scientific Data, 9(1):530.\nG. Merlin and M. Toneva. 2022. Language models and\nbrain alignment: beyond word-level semantics and\nprediction. arXiv.\nTomas Mikolov, Scott Wen-tau Yih, and Geoffrey\nZweig. 2013. Linguistic regularities in continuous\nspace word representations. In Proceedings of the\n2013 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies (NAACL-HLT-2013).\nAssociation for Computational Linguistics.\nJuliette Millet, Charlotte Caucheteux, Pierre Orhan,\nYves Boubenec, Alexandre Gramfort, Ewan Dun-\nbar, Christophe Pallier, and Jean-Remi King. 2022.\nToward a realistic model of speech processing in the\nbrain with self-supervised learning. In Advances in\nNeural Information Processing Systems.\nBonan Min, Hayley Ross, Elior Sulem, Amir Pouran\nBen Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko\nAgirre, Ilana Heinz, and Dan Roth. 2021. Recent\nadvances in natural language processing via large pre-\ntrained language models: A survey. Workingpaper.\nGosse Minnema and Aurélie Herbelot. 2019. From\nbrain space to distributional space: The perilous jour-\nneys of fMRI decoding. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics: Student Research Workshop, pages 155–\n161, Florence, Italy. Association for Computational\nLinguistics.\n9758\nTom M. Mitchell, Svetlana V . Shinkareva, Andrew Carl-\nson, Kai-Min Chang, Vicente L. Malave, Robert A.\nMason, and Marcel Adam Just. 2008. Predicting hu-\nman brain activity associated with the meanings of\nnouns. Science, 320(5880):1191–1195.\nBrian Murphy, Partha Talukdar, and Tom Mitchell. 2012.\nSelecting corpus-semantic models for neurolinguistic\ndecoding. In *SEM 2012: The First Joint Conference\non Lexical and Computational Semantics – Volume 1:\nProceedings of the main conference and the shared\ntask, and Volume 2: Proceedings of the Sixth Interna-\ntional Workshop on Semantic Evaluation (SemEval\n2012), pages 114–123, Montréal, Canada. Associa-\ntion for Computational Linguistics.\nSamuel Nastase, Yun-Fei Liu, Hanna Hillman, Asieh\nZadbood, Liat Hasenfratz, Neggin Keshavarzian, Jan-\nice Chen, Christopher Honey, Yaara Yeshurun, Mor\nRegev, Mai Nguyen, Claire H. C. Chang, Christopher\nBaldassano, Olga Lositsky, Erez Simony, Michael\nChow, Yuan Leong, Paula Brooks, Emily Micciche,\nand Uri Hasson. 2021. The \"narratives\" fmri dataset\nfor evaluating models of naturalistic language com-\nprehension. Scientific Data, 8:250.\nSubba Reddy Oota, Frederic Alexandre, and Xavier\nHinaut. 2022a. Long-term plausibility of language\nmodels and neural dynamics during narrative listen-\ning. In Proceedings of the Annual Meeting of the\nCognitive Science Society, volume 44.\nSubba Reddy Oota, Jashn Arora, Veeral Agarwal,\nMounika Marreddy, Manish Gupta, and Bapi Suram-\npudi. 2022b. Neural language taskonomy: Which\nNLP tasks are the most predictive of fMRI brain ac-\ntivity? In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 3220–3237, Seattle, United States.\nAssociation for Computational Linguistics.\nSubba Reddy Oota, Manish Gupta, and Mariya Toneva.\n2022c. Joint processing of linguistic properties in\nbrains and language models. arXiv.\nSubba Reddy Oota, Naresh Manwani, and Raju S. Bapi.\n2018. fmri semantic category decoding using lin-\nguistic encoding of word embeddings. In Neural\nInformation Processing, pages 3–15, Cham. Springer\nInternational Publishing.\nDamian Pascual, Béni Egressy, Nicolas Affolter, Yim-\ning Cai, Oliver Richter, and Roger Wattenhofer. 2022.\nImproving brain decoding methods and evaluation.\nIn ICASSP 2022 - 2022 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 1476–1480.\nAlexandre Pasquiou, Yair Lakretz, John Hale, Bertrand\nThirion, and Christophe Pallier. 2022. Neural lan-\nguage models are not born equal to fit brain data,\nbut training helps. In ICML 2022 - 39th Interna-\ntional Conference on Machine Learning , page 18,\nBaltimore, United States.\nFrancisco Pereira, Matthew M. Botvinick, and Greg De-\ntre. 2013. Using wikipedia to learn semantic feature\nrepresentations of concrete concepts in neuroimaging\nexperiments. Artificial intelligence, 194:240–252.\nFrancisco Pereira, Bin Lou, Brianna Pritchett, Samuel\nRitter, Samuel Gershman, Nancy Kanwisher,\nMatthew Botvinick, and Evelina Fedorenko. 2018.\nToward a universal decoder of linguistic meaning\nfrom brain activation. Nature Communications, 9.\nRussell A. Poldrack and Martha J. Farah. 2015.\nProgress and challenges in probing the human brain.\nNature, 526(7573):371–379.\nPeng Qian, Xipeng Qiu, and Xuanjing Huang. 2016.\nBridging lstm architecture and the neural dynamics\nduring reading. In Proceedings of the Twenty-Fifth\nInternational Joint Conference on Artificial Intelli-\ngence, p. 1953–1959, abs/1604.06635.\nAniketh Janardhan Reddy and Leila Wehbe. 2021. Can\nfmri reveal the representation of syntactic structure\nin the brain? In Advances in Neural Information\nProcessing Systems, volume 34, pages 9843–9856.\nCurran Associates, Inc.\nYu-Ping Ruan, Zhen-Hua Ling, and Yu Hu. 2016. Ex-\nploring semantic representation in brain activity us-\ning word embeddings. In Proceedings of the 2016\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 669–679, Austin, Texas.\nAssociation for Computational Linguistics.\nJ.M. (Jan Mathijs) Schoffelen, Robert Oostenveld, Niet-\nzsche Lam, Julia Udden, Annika Hultén, and P. (Pe-\nter) Hagoort. 2019. Mother of unification studies,\na 204-subject multimodal neuroimaging dataset to\nstudy language processing.\nMartin Schrimpf, Idan Asher Blank, Greta Tuckute, Ca-\nrina Kauf, Eghbal A. Hosseini, Nancy Kanwisher,\nJoshua B. Tenenbaum, and Evelina Fedorenko. 2021.\nThe neural architecture of language: Integrative\nmodeling converges on predictive processing. Pro-\nceedings of the National Academy of Sciences ,\n118(45):e2105646118.\nMartin Schrimpf, Jonas Kubilius, Ha Hong, Najib Majaj,\nRishi Rajalingham, Elias B. Issa, Kohitij Kar, Pouya\nBashivan, Jonathan Prescott-Roy, Kailyn Schmidt,\nDaniel L. K. Yamins, and James J. DiCarlo. 2018.\nBrain-score: Which artificial neural network for ob-\nject recognition is most brain-like? bioRxiv.\nDan Schwartz, Mariya Toneva, and Leila Wehbe. 2019.\nInducing Brain-Relevant Bias in Natural Language\nProcessing Models. Proceedings of the 33rd Interna-\ntional Conference on Neural Information Processing\nSystems, Red Hook, NY , USA.\nAnders Søgaard. 2016. Evaluating word embeddings\nwith fMRI and eye-tracking. In Proceedings of the\n1st Workshop on Evaluating Vector-Space Represen-\ntations for NLP , pages 116–121, Berlin, Germany.\nAssociation for Computational Linguistics.\n9759\nAnders Søgaard, Ivan Vuli´c, Sebastian Ruder, and Man-\naal Faruqui. 2019. Cross-Lingual Word Embed-\ndings, 2 edition. Synthesis Lectures on Human Lan-\nguage Technologies. Morgan & Claypool Publishers,\nUnited States.\nJingyuan Sun, Shaonan Wang, Jiajun Zhang, and\nChengqing Zong. 2019. Towards sentence-level\nbrain decoding with distributed representations. In\nProceedings of the Thirty-Third AAAI Conference on\nArtificial Intelligence and Thirty-First Innovative Ap-\nplications of Artificial Intelligence Conference and\nNinth AAAI Symposium on Educational Advances in\nArtificial Intelligence, AAAI’19/IAAI’19/EAAI’19.\nAAAI Press.\nSanne Ten Oever, Karthikeya Kaushik, and Andrea E\nMartin. 2022. Inferring the nature of linguistic com-\nputations in the brain. PLoS computational biology,\n18(7):e1010269.\nMariya Toneva, Tom M. Mitchell, and Leila Wehbe.\n2022a. Combining computational controls with natu-\nral text reveals new aspects of meaning composition.\nNature Computational Science volume 2.\nMariya Toneva and Leila Wehbe. 2019. Interpreting\nand Improving Natural-Language Processing (in Ma-\nchines) with Natural Language-Processing (in the\nBrain). Curran Associates Inc., Red Hook, NY , USA.\nMariya Toneva, Jennifer Williams, Anand Bollu,\nChristoph Dann, and Leila Wehbe. 2022b. Same\ncause; different effects in the brain.\nAditya R. Vaidya, Shailee Jain, and Alexander G. Huth.\n2022. Self-supervised models of audio effectively\nexplain human cortical responses to speech. In Inter-\nnational Conference on Machine Learning.\nLeila Wehbe, Brian Murphy, Partha Talukdar, Alona\nFyshe, Aaditya Ramdas, and Tom Mitchell. 2014a.\nSimultaneously uncovering the patterns of brain re-\ngions involved in different story reading subpro-\ncesses. PLoS One, 9(11):e112575.\nLeila Wehbe, Ashish Vaswani, Kevin Knight, and Tom\nMitchell. 2014b. Aligning context-based statistical\nmodels of language with brain activity during reading.\nIn Proceedings of the 2014 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 233–243, Doha, Qatar. Association for Com-\nputational Linguistics.\nYizhen Zhang, Kuan Han, Robert Worth, and Zhong-\nming Liu. 2020. Connecting concepts in the brain\nby mapping cortical representations of semantic rela-\ntions. bioRxiv.\nShuxian Zou, Shaonan Wang, Jiajun Zhang, and\nChengqing Zong. 2022. Cross-modal cloze task: A\nnew task to brain-to-word decoding. In Findings of\nthe Association for Computational Linguistics: ACL\n2022, pages 648–657, Dublin, Ireland. Association\nfor Computational Linguistics.\nA Appendix\nA.1 Metric Correlations\nWe used the following synthetic experiment to es-\ntimate the correlations between some of the most\nwidely used performance metrics:\n(i) Generate nrandom numbers and sort them to\nproduce the list A.\n(ii) Sample n\n10 items Bof Aat random.\n(iii) For ϵ ∈ {1\n100 ,..., 100\n100 }, evaluate µb∈B for\n⟨b,ϵ ·b⟩for all metrics.\nIn other words, for a noise level ϵ, we evaluate\npredicted images or word vectors ϵ·bagainst true\nimages or word vectors b relative to a set of tar-\nget images/vectors of 99 candidate words. This\nexperiment is easily repeated to estimate reliable\ncoefficients.\nA.2 Correlation - Explained Variance\nIn this study, we do not distinguish between stud-\nies using Pearson correlation and studies using ex-\nplained variance.\nPearson Correlation can be defined as:\nr=\n∑n\ni=1(xi −x)(yi −y)√∑n\ni=1(xi −x)2√∑n\ni=1(yi −y)2\nwhere:\n• r is the Pearson correlation coefficient be-\ntween variables X and Y\n• x_i and y_i are individual data points for vari-\nables X and Y\n• xand yare the means of variables X and Y\n• n is the sample size.\nThe proportion of variance explained by the cor-\nrelation is represented byr2. The correlation coeffi-\ncient (r) measures the strength and direction of the\nlinear relationship, while the coefficient of determi-\nnation (R2 = r2) represents the proportion of the\nvariance explained by the independent variable(s)\nin the dependent variable.\n9760\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n8\n□\u0013 A2. Did you discuss any potential risks of your work?\nLeft blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nLeft blank.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNo response.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0017 Did you run computational experiments?\nLeft blank.\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nNo response.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n9761\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nNo response.\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nNo response.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNo response.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n9762"
}