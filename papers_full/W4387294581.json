{
  "title": "A Large Language Model Approach to Educational Survey Feedback Analysis",
  "url": "https://openalex.org/W4387294581",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2099167784",
      "name": "Michael J. Parker",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Anderson, Caitlin",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Stone, Claire",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Oh, YeaRim",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3183336689",
    "https://openalex.org/W4287328210",
    "https://openalex.org/W2091259418",
    "https://openalex.org/W2080582775",
    "https://openalex.org/W2774397945",
    "https://openalex.org/W2965096648",
    "https://openalex.org/W2044723364",
    "https://openalex.org/W4312001989",
    "https://openalex.org/W4361866031",
    "https://openalex.org/W4327811957",
    "https://openalex.org/W2901951144",
    "https://openalex.org/W3039503982",
    "https://openalex.org/W4380985699",
    "https://openalex.org/W2109277446",
    "https://openalex.org/W4362508231",
    "https://openalex.org/W4288253152",
    "https://openalex.org/W2024792608",
    "https://openalex.org/W2040403808",
    "https://openalex.org/W3006059096",
    "https://openalex.org/W3180966651",
    "https://openalex.org/W2122838852",
    "https://openalex.org/W4362700315",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2958619243",
    "https://openalex.org/W4386142022",
    "https://openalex.org/W3198743574",
    "https://openalex.org/W4379255800",
    "https://openalex.org/W4367692219",
    "https://openalex.org/W2107747210",
    "https://openalex.org/W3035269559",
    "https://openalex.org/W4362707064",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W4380626958",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W2960680116",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2146085743",
    "https://openalex.org/W2921181970",
    "https://openalex.org/W3036054098",
    "https://openalex.org/W2943485424",
    "https://openalex.org/W4392617925",
    "https://openalex.org/W4285140393",
    "https://openalex.org/W2010149969",
    "https://openalex.org/W3107721628",
    "https://openalex.org/W2936503027",
    "https://openalex.org/W3080996308",
    "https://openalex.org/W2125827881",
    "https://openalex.org/W4365601444",
    "https://openalex.org/W4386730372",
    "https://openalex.org/W4380715494",
    "https://openalex.org/W3168624591",
    "https://openalex.org/W2027049304",
    "https://openalex.org/W122653547",
    "https://openalex.org/W4225364564",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4321649710",
    "https://openalex.org/W2511397010",
    "https://openalex.org/W2188323193",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W2062038813",
    "https://openalex.org/W4384807943",
    "https://openalex.org/W2034592824",
    "https://openalex.org/W285141978",
    "https://openalex.org/W3022935508",
    "https://openalex.org/W3127028307",
    "https://openalex.org/W4281560629",
    "https://openalex.org/W2894916285",
    "https://openalex.org/W1979290264",
    "https://openalex.org/W4392538354",
    "https://openalex.org/W2806072296",
    "https://openalex.org/W4388430418",
    "https://openalex.org/W4280545690",
    "https://openalex.org/W3135087240",
    "https://openalex.org/W2747512048",
    "https://openalex.org/W4391964508",
    "https://openalex.org/W3012480406",
    "https://openalex.org/W4389713899",
    "https://openalex.org/W2161307025",
    "https://openalex.org/W4296959557",
    "https://openalex.org/W2037270872",
    "https://openalex.org/W3038127753"
  ],
  "abstract": "This paper assesses the potential for the large language models (LLMs) GPT-4 and GPT-3.5 to aid in deriving insight from education feedback surveys. Exploration of LLM use cases in education has focused on teaching and learning, with less exploration of capabilities in education feedback analysis. Survey analysis in education involves goals such as finding gaps in curricula or evaluating teachers, often requiring time-consuming manual processing of textual responses. LLMs have the potential to provide a flexible means of achieving these goals without specialized machine learning models or fine-tuning. We demonstrate a versatile approach to such goals by treating them as sequences of natural language processing (NLP) tasks including classification (multi-label, multi-class, and binary), extraction, thematic analysis, and sentiment analysis, each performed by LLM. We apply these workflows to a real-world dataset of 2500 end-of-course survey comments from biomedical science courses, and evaluate a zero-shot approach (i.e., requiring no examples or labeled training data) across all tasks, reflecting education settings, where labeled data is often scarce. By applying effective prompting practices, we achieve human-level performance on multiple tasks with GPT-4, enabling workflows necessary to achieve typical goals. We also show the potential of inspecting LLMs' chain-of-thought (CoT) reasoning for providing insight that may foster confidence in practice. Moreover, this study features development of a versatile set of classification categories, suitable for various course types (online, hybrid, or in-person) and amenable to customization. Our results suggest that LLMs can be used to derive a range of insights from survey text.",
  "full_text": "Vol.:(0123456789)\nInternational Journal of Artificial Intelligence in Education\nhttps://doi.org/10.1007/s40593-024-00414-0\n1 3\nARTICLE\nA Large Language Model Approach to Educational Survey \nFeedback Analysis\nMichael J. Parker1  · Caitlin Anderson1 · Claire Stone2 · YeaRim Oh2\nAccepted: 8 June 2024 \n© The Author(s) 2024\nAbstract\nThis paper assesses the potential for the large language models (LLMs) GPT-4 and \nGPT-3.5 to aid in deriving insight from education feedback surveys. Exploration of \nLLM use cases in education has focused on teaching and learning, with less explo-\nration of capabilities in education feedback analysis. Survey analysis in education \ninvolves goals such as finding gaps in curricula or evaluating teachers, often requir -\ning time-consuming manual processing of textual responses. LLMs have the poten-\ntial to provide a flexible means of achieving these goals without specialized machine \nlearning models or fine-tuning. We demonstrate a versatile approach to such goals \nby treating them as sequences of natural language processing (NLP) tasks includ-\ning classification (multi-label, multi-class, and binary), extraction, thematic analy -\nsis, and sentiment analysis, each performed by LLM. We apply these workflows \nto a real-world dataset of 2500 end-of-course survey comments from biomedical \nscience courses, and evaluate a zero-shot approach (i.e., requiring no examples or \nlabeled training data) across all tasks, reflecting education settings, where labeled \ndata is often scarce. By applying effective prompting practices, we achieve human-\nlevel performance on multiple tasks with GPT-4, enabling workflows necessary to \nachieve typical goals. We also show the potential of inspecting LLMs’ chain-of-\nthought (CoT) reasoning for providing insight that may foster confidence in practice. \nMoreover, this study features development of a versatile set of classification catego-\nries, suitable for various course types (online, hybrid, or in-person) and amenable \nto customization. Our results suggest that LLMs can be used to derive a range of \ninsights from survey text.\nKeywords Large language model · Survey analysis · GPT-4 · GPT-3.5 · ChatGPT · \nQualitative methodology\nExtended author information available on the last page of the article\n International Journal of Artificial Intelligence in Education\n1 3\nIntroduction\nSurveys in education have long been used for course evaluation and structured \nevaluation of teaching (SET) (Diaz et al., 2022; Dommeyer et al., 2004). The his-\ntory of education surveys has seen examination of the pros and cons, with some \nnoting the subjectivity of students’ perspectives and others pointing to the diffi-\nculties in gathering responses unbiased by low response rates and the challenges \nwith analyzing qualitative survey responses in an objective way (McGourty et al., \n2002; Shah & Pabel, 2019; Spooren et  al., 2013; Stowell et  al., 2012; Wallace \net  al., 2019; Wongsurawat, 2011). However, evidence has supported the valid-\nity of surveys, including qualitative and mixed methods approaches, for guid-\ning changes to teaching practice and course design (Ferren & Aylesworth, 2001; \nJohnson & Onwuegbuzie, 2004; Lattuca & Domagal-Goldman, 2007; Marks \net al., 2017; McKeachie, 1997; Mentkowski, 1991; Spooren et al., 2013).\nCommon use cases for end-of-course evaluations (the type we focus on in this \npaper) include improving teaching and learning outcomes, measuring course qual-\nity, evaluating teachers, and informing decision-making (Diaz et al., 2022; Flodén, \n2017; Marsh & Roche, 1993; Moss & Hendry, 2002; Schulz et  al., 2014). Marsh \nand Roche (1993) found that university teachers receiving feedback and evaluations \nimproved significantly more than a control group. Teachers can use feedback to tar -\nget specific areas for improvement, with the potential to lead to structural course \nchanges (Flodén, 2017; Schulz et al., 2014). Flodén (2017) observed positive effects \nof student feedback, tending to push teaching towards more interactive formats like \nseminars and group work rather than lectures. Education administrators have used \nfeedback for quality assurance and to drive strategic decision-making (Ferren & \nAylesworth, 2001; Marginson & Considine, 2000; Mazzarol et al., 2003). The scope \nis not limited to higher education. Student feedback surveys have played an increas-\ning role in teacher development and evaluation in K-12 in recent years (Schulz et al., \n2014). A large study found student surveys to be reliable and predictive of a teach-\ner’s ability to improve student achievement (Kane et al., 2013).\nWhile quantitative feedback may have advantages for simplicity of analysis, \nthe qualitative feedback gathered through student comments has additional value. \nIn a higher education setting, Alhija and Fresko (2009) examined written student \ncomments from end-of-course evaluations. They found that qualitative comments \nfocused on the course, the instructor, and the context, capturing information not \nfound in quantitative ratings. Comments covered unique aspects and provided more \nspecific feedback on the strengths and weaknesses of a course. They concluded that \nqualitative feedback can provide a more comprehensive view of a course’s teach-\ning. Shah and Pabel (2019) used qualitative student feedback comments to compare \nthe experiences of online and in-person students at their institution. Based on their \ninsights, they conclude that universities must analyze qualitative student comments, \nnot just quantitative ratings, to truly understand and enhance the student experi-\nence, especially given the growth in online education. The importance of qualitative \nfeedback has been examined across online, in-person, and blended or hybrid format \ncourses (Aldeman & Branoff, 2021; Alhija & Fresko, 2009; Onan, 2021a).\n1 3\nInternational Journal of Artificial Intelligence in Education \nDespite the utility of qualitative student feedback, significant challenges remain \nin putting its usage into practice (Richardson, 2005). Shah and Pabel (2019) note the \nsuccess in use of quantitative data but point to limited prior progress in the analy -\nsis and practical use of qualitative feedback. Approaches for analysis have included \ntraditional, manual approaches such as thematic analysis that rely on manual effort \nof annotating and coding survey responses (Braun & Clarke, 2006; Riger & Sig-\nurvinsdottir, 2016). Manually coding and analyzing large volumes of open-ended \nsurvey responses or student feedback comments is extremely time-consuming and \nlabor-intensive and may not always provide actionable suggestions for improvement \n(Shaik et al., 2023; Mattimoe et al., 2021; Nanda et al., 2021; Shah & Pabel, 2019). \nMaintaining consistent coding across large educational datasets can be challenging \nwhen done manually, especially if multiple researchers are involved (Shaik et  al., \n2023; Mattimoe et al., 2021).\nEmploying crowdworkers, for example via Amazon’s Mechanical Turk platform, \nreduces the cost and time of manual annotation of qualitative data but does not solve \nsome of the other issues. The quality of results may vary, particularly in cases where \nsome degree of domain expertise is needed (Gilardi et al., 2023; Rother et al., 2021). \nQualitative annotation tasks can be inherently subjective, leading to disagreements \namong crowdworkers and low inter-annotator agreement (Pradhan et  al., 2022; \nRother et al., 2021). In addition, a recent study (Veselovsky et al., 2023) provided \nevidence that a substantial fraction of crowdworkers used generative AI (Large Lan-\nguage Models, or LLMs) to assist with a summarization task, leading to a mix of \nresults from humans and LLMs and raising doubt that crowdworkers will continue \nto be a reliable source of human annotations.\nMore recently, automated methods for analysis of qualitative data have relied on \na variety of machine learning models, (Deepa et al., 2019; Onan, 2021b; Smith & \nHumphreys, 2006; Zhang et al., 2020). Machine learning approaches (discussed fur-\nther below)—including unsupervised semantic mapping, topic modeling, and using \nof different forms of neural networks—have been applied to a range of tasks like \nclustering, summarization, entity extraction, and sentiment analysis (Gottipati et al., \n2018; Hamzah et al., 2020; Nanda et al., 2021; Patil et al., 2019; Shaik et al., 2023). \nThese approaches have shown promise in aiding analysis, but often require condi-\ntions that make their use less feasible to most educators, such as the need for signifi-\ncant technical resources, fine-tuning of models on volumes of pre-existing labeled \ndata, use of separate models for the natural language processing tasks involved \n(impeding broader analyses), or the need for use of specialized software (Fan et al., \n2015; Gottipati et al., 2018; Orescanin et al., 2023; Pyasi et al., 2018; Smith & Hum-\nphreys, 2006). These models have therefore generally been the domain of research, \nwith a gap in widely accessible, practical methods for qualitative analysis of end-of-\ncourse surveys.\nLarge language models with generative AI capabilities have become more widely \navailable, capable, and accessible (easier-to-use) in the last one to two years, but are \nunderexplored in terms of use in survey analysis versus more specialized machine \nlearning models. LLMs have the potential to circumvent many of the problems \nassociated with specialized machine learning approaches and potentially democra-\ntize access to high quality qualitative survey analysis. For example, the ability to \n International Journal of Artificial Intelligence in Education\n1 3\nuse such models through natural language instructions, via simple web interfaces, \nor at scale through application programming interfaces (APIs), lessens the need for \ndedicated machines or expensive software. However, a thorough analysis of the fea-\nsibility and evaluation of the quality of LLMs’ results has not been performed to \nestablish reliability and rigor in common qualitative survey analysis tasks. Our main \nresearch question was: are large language models are at a stage where they can be \neffectively used across the broad range of tasks that are part of survey analysis? To \nanswer this main question, we propose the following related research questions:\n• Research question 1 (RQ1): Can LLMs be used to perform multiple unstruc-\ntured text analysis tasks on educational survey responses, including multi-label \nclassification, multi-class classification, binary classification, extraction, induc-\ntive thematic analysis, and sentiment analysis?\n• Research question 2 (RQ2): Can LLMs’ chain-of-thought (a demonstration of \nthe intermediate steps of how they arrive at their answers) be captured to provide \na degree of transparency that may help foster confidence in real world usage? \nCan we demonstrate examples that show the potential for this use case?\n• Research question 3 (RQ3): Is a zero-shot approach (not needing to provide \nhand-labeled examples) across all tasks, a scenario that mimics many real-world \npractical use cases in the education setting, capable of achieving performance \ncomparable to human annotation?\nAs part of the evaluation process, we also developed a set of classification cat-\negories that can be applied to a variety of course types (online, hybrid, or in-person), \nand which are amenable to customization depending on specific requirements.\nBackground\nTypes of Tasks Associated with Analysis of Unstructured Survey Data\nAnalyzing survey textual responses to explore the high-level goals of educational \nstakeholders requires chaining together natural language processing (NLP) tasks in \nthe form of workflows. Such workflows can be implemented with NLP tasks, includ-\ning classification, extraction, and sentiment analysis, that form composable building \nblocks for similar workflows.\nClassification of comments may be single-label (binary or multi-class, the latter \ninvolving classifying into one of a set of tags) or multi-label (classification of each \ncomment with one or more of a set of tags). The tags (also called labels, classes, or \ncategories) are frequently custom-chosen, reflecting the goals of a particular analysis \n(Goštautaitė & Sakalauskas, 2022). Often those doing the analysis have a specific \nobjective or goal focus that they are investigating (e.g., suggestions for improve-\nment), and text extraction is a useful technique for this purpose. Sentiment analysis \ncan be used to lend nuance and insight to the quantitative ratings that are gathered \nthrough Likert scales or “star” ratings (Gottipati et al., 2017; Nitin et al., 2015).\nA high-level breakdown of objectives and NLP tasks is shown in Table 1.\n1 3\nInternational Journal of Artificial Intelligence in Education \nTable 1  NLP tasks that may be used for analysis of textual survey responses\nObjective Question NLP Tasks Notes\nHigh-level initial analysis What did students say (and how did they \nfeel about the course)?\nMulti-label classification, inductive \nthematic analysis, sentiment analysis\nDepends on whether analysis is top-down \n(using pre-determined labels or areas \nof interest) or bottom-up (deriving \nthemes from scratch based on student \ncomments)\nAnswering a focused question What did students say about x (particular \nfocus)?\nExtraction Results are amenable to multi-class clas-\nsification or inductive thematic analysis\nQuantification of textual survey \nresponses\nHow many comments were there on \neach aspect?\nClassification (binary, multi-label, or \nmulti-class)\nHelps the person performing analysis find \nthemes of greater importance\n International Journal of Artificial Intelligence in Education\n1 3\nFor discussion on the background of automated means of qualitative survey anal-\nysis, it is helpful to divide methods into those developed prior to the broad avail-\nability of the most recent generative AI versus those that make use of the recent \ngenerative AI advances in the form of LLMs. It is a potentially useful simplification \nto think of the pre-generative AI approaches as “language in, numbers out” and the \nlater generative AI approaches as “language in, language out” in terms of how one \ninteracts with the models. These approaches are distinguished below.\nPrevious Machine Learning Approaches and Challenges in Analyzing Education \nFeedback\nFor feature extraction from text, techniques like TF-IDF, and Word2Vec have been \napplied for short text classification and sentiment analysis (Deepa et  al., 2019; \nDevlin et al., 2018; Onan, 2021a; Zhang et al., 2020). Topic modeling using latent \nsemantic analysis or latent Dirichlet allocation has been useful for discovering \nthemes and trends in collections of student feedback (Cunningham-Nelson et  al., \n2019; Perez-Encinas & Rodriguez-Pomeda, 2018; Unankard & Nadee, 2020). For \nevaluating text, sentiment analysis techniques like CNN and Bi-LSTM models \nhave been used to classify student evaluations (Sindhu et  al., 2019; Sutoyo et  al., \n2021). Overall, these techniques have shown utility for gaining insights from student \nfeedback.\nWith the advent of recent machine learning (ML) techniques, great strides have \nbeen made in dealing with unstructured text. BERT (Bidirectional Encoder Rep-\nresentations from Transformers, Devlin et  al., 2018) and related models allow for \ntransformation of text passages into numerical formats (high dimensional dense vec-\ntors called embeddings) that are then amenable to classification via conventional ML \nmethods such as logistic regression. Good results have been achieved in certain con-\ntexts using such models (Meidinger & Aßenmacher, 2021). Despite such advances, \nchallenges remain that present obstacles to routine use of such models in practice.\nSpecialized ML models often require a “fine-tuning” process using labeled data \n(data that human annotators have classified) to best adapt to a specific use case. \nDepending on the amount of human labeling needed, this aspect may provide a stum-\nbling block based on the time and effort involved. Although there are many examples \nof labeled datasets (Papers With Code datasets, n.d.; Hugging Face datasets, n.d.; \nKastrati et al., 2020a, b), real-world use cases often rely on custom labels for which \nthere is no pre-existing labeled data for fine-tuning. Even supposing such fine-tuning \ntakes place, there are additional barriers to practical use of this technology.\nOne such barrier is that multiple distinct AI models may be needed, depending on \nthe range of tasks. The model that is suitable for classification may not be the same \none that performs text extraction, and each model may need its own fine-tuning or \nadaptation.\nEven for a core task like classification, there are a number of challenges. Diffi-\nculty of classification increases in situations where multiple labels may concurrently \nbe assigned to the same survey comment, often leading to a degree of inter-rater dis-\nagreement even among highly-skilled human annotators who have high familiarity \n1 3\nInternational Journal of Artificial Intelligence in Education \nwith the domain. Other challenges include data imbalance, multi-topic comments, \nand domain-specific terminology (Edalati et al., 2022; Shaik et al., 2022).\nIn classifying unstructured textual feedback, data imbalance exists when the \nlabels chosen are not attributable in equal proportions across a dataset; some labels \nmay be comparatively rare. If there are few examples of particular labels, this scar -\ncity can create difficulties in training machine learning models that classify new \ncomments. If human labeling is being used as ground truth, rarity of certain labels \nmay require labeling a larger set of feedback to enable training an ML classifier. \nTechniques addressing data imbalance include synthesizing new training examples \nfor the minority class through data augmentation or by oversampling the minority \nclass instances through techniques like SMOTE (Kennedy et al., 2024). Other meth-\nods involve modifying the learning algorithms to assign higher misclassification \ncosts to minority class examples, such that the model parameters are affected more \nby rare class examples, and ensemble methods trained on resampled versions of the \ndata (Johnson & Khoshgoftaar, 2019; Shah & Ali, 2023).\nAnother challenge is that of multi-topic comments. Depending on how feed-\nback is collected and how open-ended the survey questions are, students may pro-\nvide feedback that encompasses multiple topics (for example, “I found the quizzes \nincredibly difficult, but the teacher was great and I felt I got what I paid for. If I had \nhad more time to complete the course, this would have been even better.”). Such \nmulti-topic comments present a challenge for ML techniques based on embeddings \n(dense vector representations) derived from models such as BERT (or BERT related, \nsuch as Sentence-BERT, Reimers & Gurevych, 2019), given that the embedding of \na comment is related to the comment’s semantic meaning. A comment with multi-\nple topics may have an embedding that doesn’t adequately localize to the seman-\ntic “neighborhood” of any of the individual topics associated with that comment, \ndecreasing the performance of downstream classifiers.\nUse of context-specific, specialized terms in the text data, known as domain-spe-\ncific language, can also decrease the performance of ML techniques. Deep learning \nmodels like BERT that perform feature selection by creating embeddings have been \npre-trained on a large corpus of text, usually publicly accessible and mostly from the \ninternet. Depending on the pre-training, terms specific to a specialized domain such \nas immunology or biomedical engineering may not have been seen during training, \nor seen only in very limited quantities. In those cases, the pre-trained model cannot \nadequately capture the semantics of such terms via its embeddings, again impact-\ning the performance of downstream applications such as classification and cluster -\ning that may rely on those embeddings. For example, Lee et al. (2020) discuss how \nthe original BERT was pre-trained on general domain corpora like Wikipedia and \nBookCorpus, which lack sufficient technical biomedical vocabulary and writing \nstyles. This leads to poor performance on biomedical NLP tasks. Gu et al. (2021) \nanalyze how continual pre-training of BERT on biomedical corpora like PubMed \nabstracts and PMC full-text articles significantly improves performance on down-\nstream biomedical tasks compared to the general BERT model. The key reasons \ncited for why BERT models may struggle with domain-specific language are the \nlack of domain-specific vocabulary, writing conventions, and linguistic patterns \nin the original BERT pre-training corpus, which leads to poor representations for \n International Journal of Artificial Intelligence in Education\n1 3\ntechnical terminology and jargon when applied to domain tasks without additional \nin-domain pre-training.\nIn sentiment analysis, pre-trained sentiment analysis models may not adapt well \nto settings where it is important to take into account the context. For example, in \nanalyzing comments from biomedical science courses that cover cancer as a topic, \nlearners’ comments may include the words ‘cancer’ or ‘oncology’ or ‘tumor’, simply \nas referring to parts of the curriculum. These comments may end up being classified \nas negative even by a state-of-the-art existing model, given that discussions of can-\ncers and tumors in many training datasets (often from internet settings) may be in \nthe context of negative emotions being expressed.\nFinally, a common challenge is that of lack of interpretability of results coming \nfrom specialized machine learning models (Hassija et  al., 2024). Although there \nhas been significant work on approaches like visualizing factors that contribute to \na neural network-based model’s predictions, complex models may still be viewed \nas “black boxes” by downstream users in areas like education, with this perception \npotentially inhibiting usage.\nLLM Background and Related Research\nEducation feedback analysis seeks to extract insights from open-ended written \nresponses, such as student surveys or teacher evaluations, and automated techniques \ncan be seen as a particular application of the broader field of natural language pro-\ncessing (Shaik et al., 2022). The introduction of transformer-based neural network \narchitectures in 2017 led to an explosion of new AI models for NLP with increas-\ning capabilities. BERT (mentioned above) was developed shortly thereafter (2018), \nwith multiple related models (e.g., RoBERTa) being further developed over the last \nfive years, with effectiveness at various NLP tasks that often exceeded those of pre-\ntransformer models. Such models have been applied to a wide range of tasks, both \nwith fine-tuning and without.\nLarge language models are neural networks based on transformer architectures, \nincluding not only those in the BERT lineage but also other models such as GPT-\n2, GPT-3, T5, and many others, with tremendous scale in terms of the number of \nmodel parameters (billions and sometimes trillions) and the internet scale volume \nof text on which they are trained (billions or even trillions of tokens, with tokens \nbeing numerical representations of words or parts of words). BERT (the large vari-\nant) has approximately 345 million parameters and was trained on about 3.3 billion \nwords; in comparison, GPT-3 has 175 billion parameters and was trained on approx-\nimately 500 billion tokens (approximately 375 billion words). Models like GPT-3.5 \nand GPT-4 are proprietary, and the number of parameters and the amount of training \ndata are unknown, although there are estimates that GPT-4 uses approximately 1.8 \ntrillion parameters and was trained on approximately 13 trillion tokens, with GPT-\n3.5 somewhere in between GPT-3 and GPT-4 (Schreiner, 2023).\nIt is important as well to distinguish between BERT, along with related models like \nRoBERTa and SentenceTransformers, and generative models like GPT-3.5 and GPT-\n4. While all of these are transformer models (a type of neural network architecture), \n1 3\nInternational Journal of Artificial Intelligence in Education \nBERT is known as an “encoder-only” model, more suitable for feature extraction, clus-\ntering, and a variety of use cases making use of the resulting embeddings (numeri-\ncal representations of language, discussed above). The GPT models and other recent \ngenerative models are “decoder-only, auto-regressive” models, with different strengths, \nincluding the ability to generate complex text. These models are also trained on an \norder of magnitude more text, which lends to their capabilities as well.\nThese generative AI models have the capability to do tasks like summarization, \ntranslation, and generation of high-quality text output. As their scale has grown, the \nrange of tasks of which they have shown to be capable has increased, along with a \nlevel of performance that has surprised many. With the recent popularization and \nwider spread availability of LLMs, in part due to ChatGPT, with its underlying \nGPT-3.5 and GPT-4 models, as well as other LLMs like Claude (Anthropic), Com-\nmand (Cohere), Bard/Gemini (Google), Llama (Meta), and a range of open-source \nmodels, interest has grown in applying these to use cases like analysis of short text \ncomments such as are seen in Tweets (Törnberg, 2023), customer feedback, and sur-\nvey feedback (Jansen et al., 2023; Masala et al., 2021).\nMultiple recent studies have examined using ChatGPT for text annotation and \nclassification tasks, with mixed results based on variations in prompts, datasets, \nparameters, and complexity of tasks. Reiss (2023) focused on sensitivity to the \nprompts and parameters used in classification, in the context of performing classi-\nfication on a German dataset. Pangakis et al. (2023) argues that researchers using \nLLMs for annotation must validate against human annotation to show that LLMs are \neffective for particular tasks and types of datasets, given that there is variation in the \nquality of prompts, the complexity of the data, and the difficulty of the tasks. Other \nstudies (Gilardi et al., 2023; Huang et al., 2023) demonstrate the potential for Chat-\nGPT to perform text annotation or provide natural language explanations at levels \napproaching or matching those of humans.\nDespite explorations like those mentioned above, research to date has not focused \non the feasibility and quality of LLMs’ results in performing a broad array of com-\nmon qualitative education survey analysis tasks, leaving a gap that we focus on in \nthis study. For example, a review published in 2024 focusing on natural language \nprocessing of students’ feedback to instructors makes no mention of studies using \nLLMs for this purpose (Sunar & Khalid, 2024). Prior work has primarily focused on \nthe use of encoder models like BERT for their clustering and feature extraction capa-\nbilities and have not explored the current generation of decoder-only auto-regressive \nmodels like the GPT models mentioned above. Our contribution is to explore and \nevaluate the capabilities of the most recent generation of LLMs for educational sur -\nvey feedback analysis.\nMethods\nSurvey Data Used for Evaluation\n2,500 survey responses (625 for each of four open-ended questions) were selected \nat random from a larger set of survey responses (50,525 total responses as of April \n International Journal of Artificial Intelligence in Education\n1 3\n16, 2023, when data collection was performed, with 14,359 responses for Q1 (see \nbelow), 13,654 responses for Q2 (see below), 12,825 responses for Q3 (see below), \nand 9,687 responses for Q4 (see below)) received as end-of-course feedback on a \nrange of online biomedical science courses. The courses included 365 course itera-\ntions total across 15 different courses (e.g. Pharmacology, Genetics, Immunology, \netc.). An additional 2,000 survey comments were chosen as a development set that \ncould be used for LLM prompt tuning. The courses all use a single, uniform end-\nof-course survey. In addition to quantitative ratings (e.g., net promoter scores) and \noptional demographic data, the survey included open-ended text responses to four \nquestions/directives:\n– “Please describe the best parts of this course.” [Q1]\n– “What parts of the experience enhanced your learning of the concepts most?” \n[Q2]\n– “What can we do to improve this course?” [Q3]\n– “Please provide any further suggestions, comments, or ideas you have for this \nseries.” [Q4]\nOn average, learners answered approximately two of the four questions. The \nshortest responses containing content were one word, and the longest responses \nwere several paragraphs. Example survey responses are shown in Table 2.\nSurvey responses were collected via Qualtrics, with minor processing with Pan-\ndas 2.0.1 for elimination of leading and trailing white space and automated removal \nof responses with no content (NA or None variants).\nSurvey responses were inspected manually and via named entity recognition \n(NER), running locally, to ensure that no private or sensitive information was trans-\nmitted to publicly available LLMs.\nDevelopment of Course Tagging System\nWe spent considerable time developing and testing a set of labels that would work \nwell not only for online courses like those that the survey responses in this paper \nwere a part of, but also other types of educational offerings. The motivation for the \nchoice of labels was based on the functional areas of the team creating and deliv -\nering the courses. This team had separate functions (sub-teams) for curriculum/\nteaching, logistics and operations (e.g. enrollment and technical aspects of course \ndelivery), creative media/visual asset development (art and video creation), tech-\nnology (learning management system and platform development and maintenance), \nadministration/leadership (analysis of feedback for improvement and future feature/\ncourse requests), and support (solving learner issues and handling inquiries). While \nmost course teams at universities or online learning providers may not have all these \nfunctions separately, they are generally present in some fashion even if a single indi-\nvidual (for example, an instructor) covers multiple functions. Tags were therefore \ndesigned to cover functional areas and enable identification of feedback of interest \nfor each team’s quality improvement and planning processes.\n1 3\nInternational Journal of Artificial Intelligence in Education \nTable 2  Example actual survey responses\n[Q1] responses [Q2] responses [Q3] responses [Q4] responses\n“The teachers they are incred-\nible and their fascination \nabout this topic make it more \ninteresting.”\n“the whole concept, the short videos with the \nexplanations written down and then the interac-\ntive modules”\n“Implement more checkpoints that review previ-\nous material throughout the course.”\n“I really enjoyed the course and \nlearned a lot of applicable \ninformation for my job. I would \nhave like a little more time \nbetween new releases of infor-\nmation. It would also be nice \nto have a live question/answer \nsession.”\n“the structure of this course is \njust great. however i would \nlove to have the chance to \nrepeat all the modules as \ni am from a very different \nbackground.”\n“The quizzes after each module made me think \nabout the material I just learned.”\n“The course was fantastic and informative. \nHowever, I had to rewatch the videos several \ntimes to write down everything that is said. I \nlearn best by looking at the words. The videos \nshould come with either a transcript or writ-\nten words or some sort that convey the same \ninformation”\n“A visit to meet the tutors and a \nsummary discussion on loca-\ntion would be fabulous—I am \naware not many people would \nmake it, but a thought nonethe-\nless.”\n International Journal of Artificial Intelligence in Education\n1 3\nThe label development process started with a much larger set of labels (71 total). \nGiven that each survey response could cover multiple topics, the task was to assign \nas many labels to each survey response as were applicable (a multi-label classifica-\ntion task). The four authors (all of whom have been involved in either course devel-\nopment or delivery for multiple years and can be considered domain experts in the \ncourse resources and processes) each labeled a test set of 2,000 survey responses \n(from the same educational program overall, but distinct from the set of 2,500 com-\nments ultimately labeled), with resulting relatively low inter-rater agreement. Based \non this experiment, tag categories were combined to arrive at a much smaller set of \ngeneralizable tags (see Table 3). These were still applied in a multi-label classifica-\ntion approach, with each survey response potentially receiving multiple labels. For \nexample, the example comment mentioned earlier (“I found the quizzes incredibly \ndifficult, but the teacher was great and I felt I got what I paid for. If I had had more \ntime to complete the course, this would have been even better.”) might simultane-\nously receive labels of “assessment”, “teaching”, and “course logistics and fit”. In \naddition, best practices were followed to ensure generalizability (University of Wis-\nconsin—Madison, n.d.; UC Berkeley Center for Teaching & Learning, n.d.; Bren-\nnan & Williams, 2004; Medina et al., 2019).\nA one to three sentence description of each tag was created to provide guidance \nso that tags could be applied appropriately in testing rounds. The intent is also that \nothers can adapt these same tags by modifying the description portion for their own \nTable 3  Final tags and descriptions\nTag Description\nCourse logistics and fit course delivery (policy, support), cost, difficulty, time commitment, \ngrading, credit, schedule, user fit, access, background (e.g., prereqs and \nappropriateness of course level)\nCurriculum course content, curriculum, specific topics, course structure. This focuses \non the content and the pedagogical structure of the content, including \nflow and organization. This also includes applied material such as clinical \ncases and case studies. Includes references to pre-recorded discussions \nbetween experts or between a doctor and a patient. Includes specific sug-\ngestions for additional courses or content\nTeaching modality video, visual, interactive, animation, step-by-step, deep dive, background \nbuilder (the format rather than the content/topic)\nTeaching instructors, quality of teaching and explanations\nAssessment quizzes, exams\nResources note taking tools, study guides, notepads, readings. Includes other potential \nstatic resources like downloadable video transcripts\nPeer and teacher interaction includes chances for the student to interact with another person in the \ncourse (teacher or student). This includes discussion forums, teacher-\nstudent or student–student interactions. Includes requests for live sessions \nwith teachers or live office hours\nOther catch-all for the rarer aspects that we’ll encounter and also the ‘na’, ‘thank \nyou’, etc. comments that don’t really belong in the above bins. Also for \nsufficiently general comments like ‘all the course was terrific’ that can’t \nbe narrowed down to one of the other categories\n1 3\nInternational Journal of Artificial Intelligence in Education \npurposes. The same descriptions that served as context for the human annotators \nwere also used in the prompts for the LLMs in the multi-label classification task as a \nform of deductive thematic analysis.\nWe then iteratively tested the new, much smaller set of tags on several sets of 100 \nsurvey responses, with all four authors independently tagging the same entries, fol-\nlowed by examination of inter-rater agreement. This yielded good results. With this \nset of tags, we then independently labeled 2,500 survey responses, and evaluated \ninter-rater agreement using Jaccard similarity coefficient between pairs of raters and \naveraged across all pairs of raters. Jaccard similarity was chosen because it is one of \nthe most common metrics for multi-label classification. Other common multi-label \nclassification metrics (e.g., Hamming distance) were also calculated and are pro-\nvided in the appendix (see discussion of metrics below).\nLLM Processing\nAll LLM tasks were performed via calls to the OpenAI API endpoints. GPT-3.5 \n(model: gpt-3.5-turbo-0301) and GPT-4 (model: gpt-4–0314) were used for the \nmulti-label classification task; all other tasks described used GPT-3.5 (model: gpt-\n3.5-turbo-0613) and GPT-4 (model: gpt-4–0613). All tests were run with calls to the \nmodels’ asynchronous endpoints and used a temperature of 0 with other parameters \nset to their default values, other than the functions parameter and the function_call \nparameter, which were set to specify the applicable function schema and the func-\ntion name where applicable. “Function calling”, a capability specific to these mod-\nels, was used to generate the JSON structured output for all tasks. Prompts used (see \nElectronic Supplement) involved function schemas, as well as the system and user \nmessages to the model.\nFor the LLM approach to the multi-label classification task, the multi-class clas-\nsification task for extracted excerpts, the binary classification task, and the sentiment \nanalysis task, zero-shot chain-of-thought (CoT) prompting was used (where a model \nis prompted to reason step-by-step but without examples of such reasoning pro-\nvided) (Kojima et al., 2022; Wei et al., 2022). In addition to use of CoT enhancing \nthe accuracy of the model output, the reasoning was included in the output to allow \nfor error analysis and prompt tuning, as well as to allow inspection of the model’s \nreasoning, something potentially helpful for those using the results in practice. For \nsentiment analysis, we had the LLM output a sentiment classification based on the \npossible categories ‘negative’, ‘slightly negative’, ‘neutral’, ‘slightly positive’, and \n‘positive’, along with its reasoning.\nFor the LLM approach to inductive thematic analysis of survey responses, a two-\nstep approach was used. The first step involved prompting the LLM to derive themes \nrepresenting feedback from multiple students and summarize the themes. This step \nwas run in parallel on batches of survey responses that would fit within the model’s \ncontext window. The second step involved prompting the LLM to coalesce derived \nthemes based on similarity to arrive at a final set of themes and descriptions. These \nsteps could be considered analogous to part of the human inductive thematic analy -\nsis qualitative analysis workflow (Braun & Clarke, 2006).\n International Journal of Artificial Intelligence in Education\n1 3\nVarious prompting techniques were used to improve the results. These included:\n1) Zero-shot CoT—This technique involves asking the model to think step-by-step \nto arrive at a correct result and to provide its detailed reasoning. In the absence \nof providing examples of CoT reasoning in the prompt, this type of prompting is \ncategorized as zero-shot.\n2) Prompt tuning via inspection of CoT reasoning—In testing, error analysis was \nsupplemented with inspection of CoT reasoning to help discern where prompts \nmight need refinement. As prompts were updated, we observed corresponding \nchanges in the output and the stated reasoning, with improvement in the develop-\nment set metrics.\n3) Additional descriptive context for labels—Given that there was no fine-tuning \nto allow the model to learn the appropriate context and meaning of labels, we \nadded context to prompts in the form of definitions for each label and the types \nof elements for which each label applied.\n4) Additional context through injection of the survey questions into the prompt—\nInclusion of additional context, such as the survey question that a given comment \nis in reply to, may improve the performance of LLMs and was used in this study.\n5) Use of function calling for reliable structured output—This technique is specific \nto the GPT-3.5 and GPT-4 models, for which models from the June 2023 check-\npoint (0613) onward have been fine-tuned to enable structured output (e.g., JSON) \nwhen provided with information about a function schema that could be called \nwith the output.\n6) Memetic proxy, also known as the persona pattern (Reynolds & McDonell, 2021; \nWhite et al., 2023)—Asking the LLM to act as a certain persona, for example as \nan expert in survey analysis tasks, has been described as another way to improve \nresults, potentially by helping the model access a portion of its memory that holds \nhigher quality examples of the task at hand.\nOther Models\nIn addition to comparison to human ground truth labels, for multi-label classifica-\ntion, comparison was made to SetFit (Tunstall et al., 2022), a SentenceTransformers \nfinetuning approach based on Sentence-BERT and requiring very little labeled data; \nfor sentiment analysis, comparison was made to a publicly available RoBERTa-\nbased model trained on 124 M Tweets. Both are encoder-only models (distinct from \ngenerative AI models like GPT-4 and GPT-3.5), with the embeddings used in fine-\ntuning to enable classification. These comparisons provide some context for the \nLLMs’ performance relative to recent specialized models.\nEvaluation Metrics\nScikit-learn 1.2.0 was used for statistical tests, along with numpy 1.23.5 and Pandas \n2.0.1 for data analysis. Weights & Biases was used for tracking of model evaluation \nresults.\n1 3\nInternational Journal of Artificial Intelligence in Education \nFor the multi-label classification task, model results were compared to the \nhuman ground truth labels. Two ways were used to arrive at ground truth labels \naggregating results from multiple annotators: 1) using consensus rows: only the \nsubset of survey responses (dataset rows) where all four annotators had majority \nagreement on all selected tags were kept; and 2) using consensus labels: all sur -\nvey responses were kept but only labels with majority agreement were chosen as \nselected.\nTo fine-tune the SetFit model, we used a portion of each ground truth dataset \n(the first 20 examples for each label). Those examples were omitted from the test \nset, leaving 2,359 rows in the consensus labels test set and 1,489 rows in the con-\nsensus rows test set.\nFor each of the above scenarios, model results for multi-label classification \nwere evaluated against aggregated human annotator results via the following met-\nrics: 1) Jaccard similarity coefficient, comparing the model against aggregated \nhuman results for each row (survey response) and then averaged over all rows; 2) \naverage precision per tag; 3) average recall per tag; 4) macro average precision, \nrecall, and F1 score across all tags; 5) micro average precision, recall, and F1 \nscore across all tags; 6) Hamming loss; and 7) subset accuracy.\nFor the binary classification task, accuracy, precision, recall, and F1 score \nwere calculated, comparing the model results to one expert human annotator.\nFor the extraction task, extracted excerpts were evaluated by GPT-4 using a \nrubric created specifically for this task, examining performance on multiple \naspects of performance, including the presence of excerpts that were not exact \nquotes from the original (part of the original extraction instructions), the com-\npleteness of capturing relevant excerpts, the presence of excerpts irrelevant to the \ninitial goal focus, the inclusion of relevant context from the original comment, \nand several others. The results were also evaluated by human annotation to deter -\nmine the presence of hallucinations (excerpts that were substantial changes from \nthe original survey responses, rather than just changes in punctuation, spelling, or \ncapitalization), with the percent of the total number of excerpts representing hal-\nlucinations being reported.\nFor the inductive thematic analysis task, there is not an accepted evaluation \nmethod given that this is a complex, compound task, and evaluation consisted of \ninspecting the derived themes and descriptions as well as inspecting the results of \nthe associated multi-label classification step.\nThe sentiment analysis results of GPT-3.5 and GPT-4 were compared to those \nof a RoBERTa sentiment classifier trained on ~ 124 million tweets (Hugging Face \ncardiffnlp/twitter-roberta-base-sentiment-latest, 2022; Loureiro et al., 2022), as well \nas to results from a human annotator, with accuracy, precision, recall, and F1 scores \nreported for the prediction of sentiment as negative, neutral, or positive. Comparison \nwas made by grouping ‘negative’ and ‘slightly negative’ into a single class, keep-\ning ‘neutral’ as its own class, and grouping ‘positive’ and ‘slightly positive’ into a \nsingle class to allow for comparison across sentiment analysis methods. The RoB-\nERTa classifier produced a dictionary with negative, neutral, and positive classes, \nwith probabilities summing to 1.0. The class with the maximum probability score \nwas chosen as the label for comparison to the human annotations.\n International Journal of Artificial Intelligence in Education\n1 3\nResults\nIn this section, we organize the results in relation to the three research questions. The \nfirst research question pertains to whether LLMs can perform multiple text analysis \ntasks on unstructured survey responses, including multi-label classification, multi-\nclass classification, binary classification, extraction, inductive thematic analysis, and \nsentiment analysis. The section below on the approach to LLM workflows answers \nthis research question. The second research question explores whether LLMs’ chain-\nof-thought (a demonstration of the intermediate steps of how they arrive at their \nanswers) can be captured to provide a degree of transparency that may help foster \nconfidence in real world usage. In the section below on chain-of-thought reasoning, \nwe demonstrate examples that show the potential for this use case. The final research \nquestion asks whether a zero-shot approach (not needing to provide hand-labeled \nexamples) across all tasks can achieve performance comparable to human annota-\ntion. The section below on individual NLP task evaluations answers this research \nquestion.\nFor the examples, we use GPT-4 as the LLM; the evaluations compare GPT-4 \nand GPT-3.5 as well as the other models used.\nApproach to LLM Workflows\nThe main types of workflows demonstrated support the goals shown in Table  1 of \n1) high-level analysis, in which the desire is to understand the main categories and \nareas of emphasis across all student feedback, or 2) more focused analysis, e.g., \nanswering specific questions about a particular aspect of a course. In both cases, \nquantification of results is a consideration, which is supported by classification \ntasks.\nFor initial, high-level analysis across the entire set of survey comments, we dem-\nonstrate two approaches: 1) inductive thematic analysis, a “bottom-up” approach \nsupporting the use case where no predetermined labels (areas of interest) have been \ndefined, similar to topic modeling, and 2) multi-label classification using predefined \nlabels, a “top-down” approach, also referred to as deductive thematic analysis. When \ncategories of interest are known in advance, multi-label classification is an appropri-\nate first step, binning survey responses into relevant categories that provide a sense \nof the type of feedback learners are providing. These categories also provide group-\nings of comments for further focused analysis (e.g., via extraction), as well as allow \nfor quantification based on the number of comments labeled with each category.\nFor focused analysis, in which there is a specific question or goal for the analysis, \nnot necessarily known in advance, we demonstrate extraction as a key step, followed \nby either a classification step or thematic analysis. To provide output for further \ndownstream analysis and quantification, multi-class classification can be used as a \nstep, as demonstrated here with the generalizable set of labels used in this study, \nor with an adapted or fully customized version for one’s own use case. This step \nis shown used after extraction, given that short excerpts are more likely to be ade-\nquately classified with a single label versus multi-sentence comments. The output of \n1 3\nInternational Journal of Artificial Intelligence in Education \nother forms of classification (binary or multi-label) also lends itself well to quantifi-\ncation of results.\nSentiment analysis was applied as a final step for workflows where finding posi-\ntive or negative excerpts was of interest, as demonstrated in the example related to \nthe level of difficulty of the course.\nAlthough the full model responses were in JSON format, only the relevant output \ntext is shown for brevity and clarity.\nWorkflow Examples\nExample 1—High‑Level Analysis by Inductive Thematic Analysis (“Bottom‑Up” \nApproach)\nA workflow for finding and summarizing the main themes (ideas expressed by mul-\ntiple students) of survey responses is shown in Fig.  1, and consists of three LLM \nsteps: 1) themes are first derived and summarized for batches of comments, each \nof which is sized to fit within the context window of the model used; 2) comments \nare classified using the derived themes; and 3) sets of themes from these batches are \ncoalesced to arrive at a final set of themes. Additionally, label counts are aggregated \nfrom the themes that were combined. In qualitative research, steps 1 and 3 are called \ninductive thematic analysis; this is similar to topic modeling, in that themes are \ninductively derived from comments. In general, depending on the input size (context \nwindow) for the model used (8 K tokens in this example) and the number of com-\nments being analyzed, dividing into batches and coalescing the themes from each \nbatch may be unnecessary.\nResults for running this process on the 625 comments from Q1 (‘Please describe \nthe best parts of this course’) are shown in Fig.  1. The number of comments that \nthe LLM identified as corresponding to each theme is shown, along with the theme \ntitles and descriptions.\nExample 2—High‑Level Analysis by Categorizing Student Comments (“Top‑Down” \nApproach)\nMulti-label classification of survey responses, using the set of predetermined labels \ndeveloped for this study (Table  3) was run on the 625 comments from Q1 (‘Please \ndescribe the best parts of this course’) and results are shown in Fig.  2. The catego-\nrized comments can be used for analysis (for example, comparing the categorization \nof responses to ‘Please describe the best parts of this course’ to the categorization \nof responses to ‘What can we do to improve this course?’) or as a starting point for \nfurther downstream tasks.\nExample 3—Finding Suggestions for Improvement\nA workflow for finding and quantifying suggestions for course improvement is shown \nin Fig.  3, and consists of extraction of relevant excerpts, followed by multi-class \n International Journal of Artificial Intelligence in Education\n1 3\nclassification, based on the labels in Table 3, to facilitate quantification as well as rout-\ning of comments to the appropriate stakeholders. Excerpts resulting from the extraction \nstep were assumed to be focused enough that they could each be categorized with a \nsingle class from among the pre-existing labels in Table 3.\nResults for several representative real comments from the larger set of survey com-\nments are shown in Fig.  3. The model’s CoT reasoning for each step is shown else-\nwhere, but is omitted here for clarity.\nFig. 1  Derivation of themes from student comments (results shown using GPT-4)\n1 3\nInternational Journal of Artificial Intelligence in Education \nExample 4—What Other Content or Topics Were Students Interested in Seeing \nCovered?\nA common goal in analyzing student feedback is to better understand the gaps in \ncourse content, in order to decide whether to develop additional material or even \nnew courses. To see if this type of information could reliably be derived from survey \nresponses, we focused on responses to relevant survey questions (Q3 and Q4) for \nimmunology courses with the workflow shown in Fig.  4. Results for several repre-\nsentative real comments are shown. First, just the portions containing new content or \ntopic area suggestions are extracted from the survey responses. Content suggestion \nthemes are then derived and summarized from the excerpts; this is done in batches \nif they cannot be fit within a single prompt to the LLM (i.e., if there are too many \nexcerpts to fit in the model’s maximum context size). Multi-class classification is \nperformed on the excerpts with the themes from each batch. If thematic analysis is \ndone in batches, sets of themes from these batches are then coalesced to arrive at \na final set of content themes. The results suggest that GPT-4 is capable of finding \ncontent suggestions despite many being specific to the biomedical domain. This may \nbe due to the volume and diversity of the model’s pre-training data (although this \ntraining mixture has not been disclosed). Immunology is used as an example, but the \nworkflow is not specific to the type of course.\nExample 5—What Feedback Did Students Give About The Teaching \nand Explanations?\nFeedback about teachers and the quality of teaching and explanations in a course is \na frequent objective of academic course surveys. Here, we show a workflow where \nmulti-label classification has already been run as an initial step in high-level analy -\nsis, and we use the results of that classification as our initial filter to focus on the \nidentified subset of comments related to teaching (corresponding directly to one of \nFig. 2  Multi-label classification of student comments (results shown using GPT-4)\n International Journal of Artificial Intelligence in Education\n1 3\nthe pre-existing labels), with extraction used to further narrow the output of analy -\nsis. The workflow, shown in Fig. 5, consisted of multi-label classification, using the \npre-existing labels developed (Table  3) followed by extraction of relevant excerpts \nfrom the comments that were classified into the ‘teaching’ category (9% of total \ncomments). If multi-label classification hadn’t previously been run, extraction could \nhave been performed on the broader group of comments as the initial step. For our \ndataset, which includes numerous multi-topic comments, the extraction step was \nused to further filter the information to only content related to the goal. Results \nfor several representative real comments (de-identified in pre-processing) from the \nlarger set of survey comments are shown in Fig.  5, including one where the model \nimproperly filtered out the comment despite it containing a reference to the quality \nFig. 3  Finding suggestions for improvement from student comments (results shown using GPT-4)\n1 3\nInternational Journal of Artificial Intelligence in Education \nof explanations. An error such as the one shown could be considered somewhat sub-\ntle and highlights the need with zero-shot prompting of LLMs for clear specification \nof the goal of the extraction.\nChain‑of‑Thought Reasoning\nThe prompts for binary classification, multi-label classification, multi-class clas-\nsification, sentiment analysis, and evaluation of extraction results all used zero-\nshot chain-of-thought (CoT) to enhance the quality of the results while maintain-\ning the zero-shot conditions of this study. The CoT reasoning was included in the \nFig. 4  Finding suggestions for new immunology content from student comments (results shown using \nGPT-4)\n International Journal of Artificial Intelligence in Education\n1 3\nstructured output, allowing for inspection. Only the reasoning from GPT-4 was \nconsistently reliable, and examples are shown here.\nExample results for binary and multi-class classification tasks are shown in \nFig.  6 and Fig.  7, and reasoning for sentiment analysis is also shown in Fig.  7. \nThe reasoning, inspected manually over several hundred comments, is consistent \nwith the classification results and appears to provide logical justification that is \ngrounded in the contextual information (e.g., labels and descriptions) included \nas part of the prompts (see Electronic Supplement). This suggests that the CoT \nreasoning from GPT-4 meets a threshold of consistency and logic that allows for \npotential downstream use cases such as prompt tuning and insight into reason-\ning for end-users. Potential benefits and caveats of such uses are explored in the \nDiscussion.\nEvaluation of the extraction task used a custom LLM evaluation (see Electronic \nSupplement), developed for this study. In order to refine the evaluation to align \nresults with human preferences, we inspected the CoT reasoning along with the \nstructured eval results for the separate development set of survey responses and \nmade modifications to the evaluation prompts in an iterative fashion. An example \nof the CoT output for GPT-4 is shown in Fig.  8. As prompts were altered based \non human review, the eval results changed in a consistent fashion, suggesting that \nGPT-4 provided CoT reasoning may be useful in refining LLM evaluations.\nFig. 5  Feedback about teaching and explanations (results shown using GPT-4). The red ‘x’ indicates an \nerror by the model\n1 3\nInternational Journal of Artificial Intelligence in Education \nIndividual NLP Task Evaluations\nTo better assess the reliability of workflows such as those shown in the examples \nand to answer Research Question 3, we evaluated the individual tasks, including \nmulti-label classification, binary classification, extraction, and sentiment analysis.\nMulti‑Label Classification Metrics\nThe difficulty of multi-label classification tasks varies widely (Meidinger & Aßen-\nmacher, 2021), depending on the content to which the labels are being applied, the \ndesign of the labels (for example, the clarity of their specification and the potential \nfor overlap), and the number of labels. To put the LLM results in context, we show \nthe inter-rater agreement for application of the eight-label set (Table  3) to our data-\nset and also compare the LLM results to SetFit, another classification technique.\nFig. 6  Examples of GPT-4 CoT reasoning for binary classification and multi-class classification related \nto the task of finding suggestions for improvement\n International Journal of Artificial Intelligence in Education\n1 3\nInter‑rater Agreement 1,413 (57%) of 2,500 rows had all 4 human raters in agree-\nment across all selected labels and 1,572 (63%) had majority (3 of 4) agreement \non all selected labels. The average Jaccard similarity coefficient including all 2,500 \nrows (averaged across the six unique pairings of the four human raters for all rows) \nwas 81.24% (Table  4), suggesting that this was a challenging task even for expert \nhuman annotators who developed the custom label set in close collaboration. GPT-4 \nagreement with human annotators is shown; the average across all pairings includ-\ning GPT-4 was 80.60%.\nLLM and SetFit Evaluation In addition to evaluating the GPT models, we also per -\nformed multi-label classification using SetFit (Tables 5 and 6).\nFig. 7  Examples of GPT-4 CoT reasoning for multi-label classification, binary classification, and senti-\nment analysis related to the task of finding how students felt about the level of difficulty of the course\n1 3\nInternational Journal of Artificial Intelligence in Education \nFor the consensus rows evaluation, the zero-shot results for GPT-4 are similar to \nwhat might be expected of fine-tuned classifiers (Meidinger & Aßenmacher, 2021). \nThe other models have strengths and weaknesses, with SetFit having relatively high \nprecision and lower recall, and GPT-3.5 following the converse pattern. The overall \nresults for SetFit and GPT-3.5, focusing on Jaccard coefficient and F1 scores, are \nsimilar. The results emphasize 1) the fact that fine-tuning is desirable when feasible, \nFig. 8  Example of GPT-4 CoT reasoning for extraction evaluation\nTable 4  Inter-rater Jaccard similarity coefficients, including human annotators and GPT-4 as another \nrater/annotator (human pairs average = 81.24%; all pairs average = 80.60%)\nAnnotator1 Annotator2 GPT-4 Annotator3 Annotator4\nAnnotator1 - 81.27 80.18 83.37 82.35\nAnnotator2 81.27 - 79.40 80.84 78.42\nGPT-4 80.18 79.40 - 80.74 78.22\nAnnotator3 83.37 80.84 80.74 - 81.18\nAnnotator4 82.35 78.42 78.22 81.18 -\nTable 5  Evaluation on consensus rows, with majority agreement on all tags (1,572 rows for LLMs, 1,489 \nrows for SetFit)\nMacro average Micro average\nJaccard Average precision Precision Recall F1 Precision Recall F1\nGPT-4 92.97 93.91 89.88 90.59 89.78 93.66 93.26 93.46\nGPT-3.5 72.61 74.79 69.34 82.18 72.63 72.36 84.48 77.96\nSetFit 73.86 78.01 84.37 57.59 66.85 91.92 71.43 80.39\n International Journal of Artificial Intelligence in Education\n1 3\napproaching the performance of powerful LLMs like GPT-3.5 even with a few-shot \nfine-tuning approach; and 2) the quality of the zero-shot performance of GPT-4.\nBinary Classification Metrics\n1,250 comments (randomly taken from the original selected 2,500 comments, evenly \ndistributed across Q1 through Q4) were classified based on the question “does this \ncomment contain suggestions for improvement?”, with the model assigning a ‘yes’ or \n‘no’ to each comment, and results were compared against one expert human annota-\ntor—as the gold standard—assigning ‘yes’ or ‘no’ to each comment. Binary classifica-\ntion could be considered the simplest of the evaluated NLP tasks, and both LLM mod-\nels exhibited good performance (Table 7).\nExtraction Evaluation\nUsing ‘suggestions for improvement’ as an example target of extraction, comments \nwere first classified via GPT-4 as containing the target or not (see binary classification \ntask above). Of the 1,250 comments, 716 were labeled by binary classification as con-\ntaining suggestions for improvement. These comments were then run through extrac-\ntion to find the individual excerpts. The quality of the extraction was then scored by \nthe following method, employing the concept of LLM-as-a-judge (Huang et al., 2024) \nwhere custom LLM-based evaluations are used for cases where standardized evalua-\ntion methods don’t exist or are inadequate. For each comment’s excerpt(s), GPT-4 was \nused to apply a custom evaluation rubric with nine questions. Only GPT-4 was capable \nof applying the evaluation reliably and was therefore used. An example of one of the \nrubric questions was “Did the program extract any irrelevant excerpts? (yes or no)” \n(see the extraction evaluation prompt in the Electronic Supplement for all questions in \nthe rubric). That question is abbreviated as the “Irrelevant Excerpts” category in the \ntable below, showing the percentage of “yes” answers. Given that each of the rubric \nTable 6  Evaluation on all rows using consensus labels (2500 rows for LLMs, 2359 rows for SetFit)\nMacro average Micro average\nJaccard Average precision Precision Recall F1 Precision Recall F1\nGPT-4 80.17 81.53 73.91 88.38 79.69 78.32 89.70 83.63\nGPT-3.5 63.00 65.18 60.42 79.79 65.75 60.31 83.45 70.02\nSetFit 62.72 67.52 73.22 53.08 59.61 79.40 65.14 71.57\nTable 7  Binary classification \ntask performance Model Accuracy Precision Recall F1\nGPT-4 95.20 96.20 95.39 95.79\nGPT-3.5 90.16 89.01 93.35 91.14\n1 3\nInternational Journal of Artificial Intelligence in Education \nquestions had a yes/no answer, with a “yes” answer indicating a failure of the aspect \nof the model’s extraction based on that question, the error rate or percentage of fail-\nures for each aspect could be determined. The extracted excerpts were also examined \nby a human annotator to determine the percentage of the 716 rows that contained hal-\nlucinations in the excerpts, as defined by substantial edits or complete fabrication of \nadditional language not present in the original comment. That percentage is shown in \nthe last row of Table 8, with the other rows’ results representing the scores (error rates) \ndetermined by GPT-4 judging the extraction results from GPT-4 and GPT-3.5.\nThe GPT-4 model included some ambiguous excerpts; however, those were most \ncommonly due to lack of context in the comment itself, rather than the model failing \nto extract that context. GPT-4 followed directions very closely, and its results did not \ncontain hallucinations. In contrast, the output of GPT-3.5 contained hallucinations at \na rate of about 4% and edits to comments at a rate of about 6%. GPT-3.5 also missed \nrelevant excerpts significantly more frequently than GPT-4. Additional prompt tun-\ning may reduce the rate of these errors; nonetheless, the results suggest that a degree \nof caution should be applied in using GPT-3.5 for extraction.\nSentiment Analysis Metrics\nUsing GPT-4 and GPT-3.5, comments related to course suggestions and improve-\nment (Q3 and Q4) were classified as ‘negative’, ‘slightly negative’, ‘neutral’, \n‘slightly positive’, or ‘positive’. Table 9 shows accuracy, and macro precision, recall, \nand F1 scores for three models; comparison was made by grouping ‘negative’ and \nTable 8  Error rate (%) of \nextraction for ‘suggestions for \nimprovement’ from comments \nclassified as containing \n‘suggestions for improvement’ \n(all metrics from rubric \nand human annotation for \nhallucinations)\nMetric GPT-4 GPT-3.5\nMissed Excerpts 2.37 7.82\nAmbiguous Excerpts 4.61 4.75\nMissed Existing Context 0.28 0.84\nIrrelevant Excerpts 0.14 0.84\nIrrelevant Context 0.00 0.14\nImplied Goal Focus 3.07 2.79\nNon-Quotes 0.00 6.01\nNon-Contiguous Excerpts 0.00 0.14\nRedundant Excerpts 0.28 2.79\nHallucinations 0.00 3.91\nTable 9  Classification of comments as negative, positive, or neutral relative to human annotator\nModel Accuracy Precision (macro) Recall (macro) F1 (macro)\nGPT-4 80.86 82.65 80.28 80.78\nGPT-3.5 65.17 73.68 66.44 64.88\ntwitter-roberta-base-\nsentiment-latest\n66.69 71.38 64.86 61.10\n International Journal of Artificial Intelligence in Education\n1 3\n‘slightly negative’ into a single negative class, keeping ‘neutral’ as its own class, and \ngrouping ‘positive’ and ‘slightly positive’ into a single positive class.\nGPT-4 is substantially better on each metric than the other models; however, the \nresults are lower than what has been seen for fine-tuned models on in-domain data-\nsets, indicating that the sentiment expressed in student course feedback may differ \nfrom the range of sentiment expressed in the internet training data of these models. \nThe negative class was the most challenging for all models, suggesting that negative \ncourse feedback may differ significantly from negative internet feedback.\nLLM Cost and Time\nThe cost of using the OpenAI APIs for GPT-4 and GPT-3.5 depends on the num-\nber of prompt tokens and number of completion tokens. For the final prompts and \ntasks used in this study, the average price of running 100 comments is shown in \nTable 10 for each model for different tasks (cost as of June 2023, when the results \nwere run). The tasks include the CoT reasoning in the completion (output), signifi-\ncantly increasing the number of completion tokens. These provide an approximate \ngauge given that comments vary in length. Total API cost for this study including \nprompt tuning was approximately $300.\nThe time for model calls for GPT-4, the slower of the OpenAI models, was \napproximately 10 s for running 100 comments in parallel for most tasks listed. For \nthe extraction evaluation, it took approximately 1 min to run 100 comments in paral-\nlel. For batches, sleep intervals were also incorporated to stay conservatively within \nmaximum token rates. A small percentage of API calls received errors and auto-\nmatic retries were used after wait intervals.\nDiscussion\nAnalysis of education feedback, in the form of unstructured data from survey \nresponses, is a staple for improvement of courses (Diaz et al., 2022; Flodén, 2017; \nMarsh & Roche, 1993; Moss & Hendry, 2002; Schulz et al., 2014). However, this \ntask can be time-consuming, costly, and imprecise, hampering the ability for educa-\ntors and other stakeholders to make decisions based on insights from the data (Shaik \nTable 10  Cost per 100 \ncomments for GPT-4 and \nGPT-3.5\nTask GPT-4 GPT-3.5\nBinary classification $0.93 $0.04\nMulti-label classification $2.63 $0.12\nMulti-class classification $2.13 $0.10\nText extraction $1.10 $0.05\nText extraction evaluation $3.01 $0.13\nSentiment analysis $1.17 $0.05\nInductive thematic analysis $0.13 $0.006\n1 3\nInternational Journal of Artificial Intelligence in Education \net al., 2023; Mattimoe et al., 2021; Nanda et al., 2021; Shah & Pabel, 2019). Large \nlanguage models with generative AI capabilities have become widely accessible \nrecently but remain underexplored in analysis of qualitative data from educational \nfeedback surveys.\nOur research questions focused on whether these new tools can 1) be successfully \nused perform a wide variety of natural language processing tasks on survey results \n(RQ1); 2) offer a degree of transparency based on capturing chain-of-thought inter -\nmediate output (RQ2); and 3) perform at a level comparable to human performance \nacross all tasks without needing to be provided with hand-labeled examples (RQ3).\nWe were able to create reliable, reproducible workflows that put together multiple \nanalysis tasks, running them in minutes on more than one thousand survey responses. \nThe results demonstrate that these workflows can provide insight into a variety of \nquestions that may be asked by educators, including finding suggestions for improve-\nment, identifying themes in students’ feedback, and quantifying such results through \nclassification, including multi-label classification. The zero-shot approach (no hand-\nlabeled examples, other than for evaluation metrics) provides flexibility; in other \nwords, tasks can rapidly be adjusted through changes in the LLM prompts, and new \ntasks (for example, using extraction to find information about a different focus) can \nbe added without need for model fine-tuning or labeling new examples.\nWe show that chain-of-thought prompting, which was used to increase accuracy, may \nalso provide insight into the model’s reasoning or trajectory. It is possible that the LLM \nis imitating plausible reasoning rather than providing insight into how it actually arrived \nat its answer; however, this distinction may be immaterial given that 1) GPT-4’s reason-\ning was logical and highly consistent with the results, displaying elements of causal rea-\nsoning; and 2) when prompts were changed, reasoning results changed accordingly. This \nhas been discussed in recent work; GPT-4 has been shown to score highly on evaluations \nof causal reasoning (Kıcıman et al., 2023). In Peng et al., 2023, GPT-4 was used for eval-\nuation of other LLMs and was able to provide consistent scores as well as detailed expla-\nnations for those scores. While specialized non-LLM models can provide signals like \nconfidence scores in individual classes, they lack more detailed explanations of results; \nwe believe that seeing a version of logical reasoning behind complex output can foster \nconfidence and reduce the perception of these models as black boxes. Furthermore, it is \nimportant to consider that having human annotators reliably provide consistent, logical \njustification for each annotation is prohibitive for datasets of any appreciable scale.\nThe evaluation metrics for each individual task show a human or near-human level \nof performance across a range of tasks for GPT-4. GPT-3.5 does not reach this level of \naccuracy. Our tasks and dataset were drawn from real-world data and actual use cases, \nwith some of the tasks (e.g., multi-label classification) proving challenging even for \nexpert annotators. The human-like level of GPT-4 can be seen in examples of the rea-\nsoning results as well (Figs.  6, 7, and 8 ). In addition, it outperformed label-efficient \nfine-tuned classifiers like SetFit. For workflows that chain together two or more NLP \ntasks, like those examined in this study, it is important that the performance on each \ntask is reliable enough such that errors do not accrue in the process of obtaining a final \nresult. It is likely that at the time of publication there are other models (e.g., the latest \nversion of Claude or the top-performing open-source models) that perform at similar or \nhigher levels than the version of GPT-4 used in this study.\n International Journal of Artificial Intelligence in Education\n1 3\nOverall, we found that large language models are at a stage where they can be effec-\ntively used across the broad range of tasks that are part of survey analysis. Use of LLMs \nfor this purpose has implications for education. These implications include:\n1) the potential to democratize access to high quality qualitative survey analysis. The \nparadigm of “language in, language out” allows non-machine learning experts to \ncreate workflows that can handle a range of tasks.\n2) a drastic reduction in the time and effort involved for challenging survey analysis \ntasks, shortening the feedback loop. In many cases where there is a high volume \nof survey responses, the effort and expertise necessary to arrive at data-driven \nresults may otherwise be prohibitive. The goal is to be able to analyze unstruc-\ntured text survey data as easily as one might analyze quantitative results like those \nfrom Likert scales.\n3) making educators and education leaders more ambitious in terms of the questions \nthey can answer based on student feedback. For example, doing thematic analysis \non a volume of survey responses may have been infeasible previously.\nWe look forward to people using these new tools to effectively learn from student \nfeedback.\nLimitations\nThe data used in this study was from a specific domain (online biomedical science \ncourses) and was in English. Other domains and languages were not tested.\nLLM results were highly prompt-dependent, and others may achieve even more \naccurate results than those we have shown. Even within the most capable models, we \nobserved that prompting techniques and prompt tuning made a significant difference. \nThere is considerable literature on effective methods of prompting. There is an inter-\nplay of prompting techniques with the behavior of instruction-tuned models in a way \nthat may or may not fully elicit the capabilities of each model, with prompts being seen \nas a form of hyperparameter to the model and with responses changing depending on \nupdates to model training (Chen et al., 2023).\nOther than the comparison to SetFit and to the RoBERTa sentiment analysis model, \nwe limited our exploration to recent OpenAI models; future work may expand this to \ninclude other models such as the most capable proprietary models (e.g., Claude and \nGemini), and the most capable open-source models.\nConcluding Remarks\nWhile LLM analysis approaches are being used in other fields, like customer \nreviews and user feedback (Morbidoni, 2023; Abdali et al., 2023), there has not \nyet been rigorous demonstration of their utility and accuracy in student feedback \nsurveys. Our contributions are to demonstrate the viability of using LLMs for \n1 3\nInternational Journal of Artificial Intelligence in Education \nthis purpose, and to perform a thorough analysis of the feasibility and evaluation \nof the quality of LLMs’ results to show reliability in common qualitative survey \nanalysis tasks.\nFuture research could incorporate additional prompting techniques to improve \ncapabilities and accuracy. For example, self-consistency (Wang et  al., 2022), \nreflection (iterative self-refinement, Madaan et al., 2023), and few-shot learning \nhave been studied and shown to be reliable means of improving performance on \ndifficult tasks; these were out-of-scope for the zero-shot premise of this article \nbut are worth exploring. In addition, the ability to compose survey analysis work -\nflows is also amenable to the use of agents (Shen et al., 2023; Weng, 2023; Yao \net al., 2022). An educator or other stakeholder analyzing survey feedback should \nbe able to state a goal or intent to an LLM-powered agent, with the agent picking \nand running tasks as a chain to get the desired analysis. Such an agent could also \nincorporate non-LLM tools, for example if a fine-tuned model is available that \nexcels on a given task and is well-matched to the dataset at hand. Ideally, users of \nsuch tools should be able to operate by stating intent rather than tuning prompts \nor fine-tuning specialized models.\nWe look forward to future progress in exploring the capabilities of these new \ntools; the models will continue to improve but even in their current state, they can \nbe powerful tools for survey analysis.\nAppendix\nAdditional Metrics for Multi‑Label Classification\nConsensus Rows—1572 Rows Dataset (1489 for SetFit)\nPrecision, recall, and F1 score are shown for each tag in multi-label classification \nfor the consensus rows condition, along with macro averages for each metric, in \nTable 11 for GPT-4. Hamming loss and subset accuracy are shown in Table 12.\nTable 11  Individual label scores \nfor multi-label classification \nwith GPT-4, consensus rows\nTag Precision Recall F1 Score\nCourse logistics and fit 89.83 64.63 75.18\nCurriculum 95.89 91.48 93.64\nTeaching modality 97.78 96.70 97.24\nTeaching 76.04 91.25 82.95\nAssessment 97.50 93.41 95.41\nResources 92.31 97.30 94.74\nPeer and teacher interaction 77.08 92.50 84.09\nOther 92.60 97.47 94.97\nMacro average 89.88 90.59 89.78\n International Journal of Artificial Intelligence in Education\n1 3\nConsensus Labels—2500 Rows Dataset (2359 for SetFit)\nPrecision, recall, and F1 score are shown for each tag in multi-label classification \nfor the consensus labels condition, along with macro averages for each metric, in \nTable 13 for GPT-4. Hamming loss and subset accuracy are shown in Table 14.\nSupplementary Information The online version contains supplementary material available at https:// doi. \norg/ 10. 1007/ s40593- 024- 00414-0.\nAcknowledgements We wish to thank members of the HMX team for their contributions to creating and \nadministering the surveys used in this study.\nAuthor Contributions Conceptualization, methodology, software, analysis, and drafting of the manu-\nscript were performed by Michael J. Parker. Development of labels, annotation/labeling for multi-class \nclassification, and refinement of the manuscript were performed by all authors (equal contributions).\nFunding This research received no external funding.\nTable 12  Hamming loss and \nsubset accuracy for multi-label \nclassification, consensus rows\nModel Hamming loss Subset accuracy\nGPT-4 0.0194 0.8849\nGPT-3.5 0.0710 0.5948\nSetFit 0.0508 0.6797\nTable 13  Individual label scores \nfor multi-label classification \nwith GPT-4, consensus labels\nTag Precision Recall F1 Score\nCourse logistics and fit 74.81 57.71 65.16\nCurriculum 82.86 86.38 84.58\nTeaching modality 78.35 95.94 86.26\nTeaching 56.70 87.59 68.83\nAssessment 87.36 94.82 90.94\nResources 70.95 95.49 81.41\nPeer and teacher interaction 60.58 94.03 73.68\nOther 79.64 95.10 86.68\nMacro average 73.91 88.38 79.69\nTable 14  Hamming loss and \nsubset accuracy for multi-label \nclassification, consensus labels\nModel Hamming loss Subset accuracy\nGPT-4 0.0503 0.7168\nGPT-3.5 0.10235 0.4628\nSetFit 0.07238 0.5774\n1 3\nInternational Journal of Artificial Intelligence in Education \nAvailability of Data and Materials The prompts and the function schemas used in this study are shared in \nsupplementary material (Electronic Supplements 1 and 2). To help preserve the anonymity of students \nand of feedback about teachers, the survey responses are not included in an open-access repository. The \ndata may be provided upon request to the authors and approval of the university research ethics board.\nDeclarations \nEthics Approval and Consent to Participate This study was determined not to be human subjects research \nby the Harvard Medical School Office of Human Research Administration.\nCompeting Interests The authors have no relevant financial or non-financial interests to disclose.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is \nnot permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission \ndirectly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen \nses/ by/4. 0/.\nReferences\nAbdali, S., Parikh, A., Lim, S. & Kiciman, E. (2023). Extracting self-consistent causal insights from \nusers feedback with LLMs and in-context learning. In arXiv [cs.AI]. arXiv. Retrieved April 5, 2024, \nfrom http:// arxiv. org/ abs/ 2312. 06820\nAldeman, M., & Branoff, T. J. (2021). Impact of course modality on student course evaluations. Paper \npresented at 2021 ASEE Virtual Annual Conference Content Access, Virtual Online. Retrieved \nAugust 21, 2023, from https:// peer. asee. org/ 37275. pdf\nAlhija, F.N.-A., & Fresko, B. (2009). Student evaluation of instruction: What can be learned from stu-\ndents’ written comments? Studies in Educational Evaluation, 35(1), 37–44. https:// doi. org/ 10. \n1016/j. stued uc. 2009. 01. 002\nBraun, V., & Clarke, V. (2006). Using thematic analysis in psychology. Qualitative Research in Psychol-\nogy, 3(2), 77–101. https:// doi. org/ 10. 1191/ 14780 88706 qp063 oa\nBrennan, J., & Williams, R. (2004). Collecting and using student feedback. A guide to good practice. \nLearning and Teaching Support Network. Retrieved August 21, 2023, from https:// www. advan ce- \nhe. ac. uk/ knowl edge- hub/ colle cting- and- using- stude nt- feedb ack- guide- good- pract ice\ncardiffnlp/twitter-roberta-base-sentiment-latest. (2022). Retrieved August 21, 2023, from https:// huggi \nngface. co/ cardi ffnlp/ twitt er- rober ta- base- senti ment- latest.\nChen, L., Zaharia, M., & Zou, J. (2023). How is ChatGPT’s behavior changing over time? arXiv [cs.CL]. \nRetrieved August 21, 2023, from https:// arxiv. org/ abs/ 2307. 09009\nCunningham-Nelson, S., Baktashmotlagh, M., & Boles, W. (2019). Visualizing student opinion through text \nanalysis. IEEE Transactions on Education, 62(4), 305–311. https:// doi. org/ 10. 1109/ TE. 2019. 29243 85\nDeepa, D., Raaji, & Tamilarasi, A. (2019). Sentiment analysis using feature extraction and dictionary-\nbased approaches. In 2019 Third International conference on I-SMAC (IoT in Social, Mobile, Ana-\nlytics and Cloud) (I-SMAC), 786–790. https:// doi. org/ 10. 1109/I- SMAC4 7947. 2019. 90324 56\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). BERT: Pre-training of deep bidirectional \ntransformers for language understanding. arXiv [cs.CL]. Retrieved August 21, 2023, from http:// \narxiv. org/ abs/ 1810. 04805\nDiaz, N. P., Walker, J. P., Rocconi, L. M., Morrow, J. A., Skolits, G. J., Osborne, J. D., & Parlier, T. R. \n(2022). Faculty use of end-of-course evaluations. International Journal of Teaching and Learning in \nHigher Education, 33(3), 285–297.\n International Journal of Artificial Intelligence in Education\n1 3\nDommeyer, C. J., Baum, P., Hanna, R. W., & Chapman, K. S. (2004). Gathering faculty teaching evalua-\ntions by in-class and online surveys: Their effects on response rates and evaluations. Assessment & \nEvaluation in Higher Education, 29(5), 611–623. https:// doi. org/ 10. 1080/ 02602 93041 00016 89171\nEdalati, M., Imran, A. S., Kastrati, Z., & Daudpota, S. M. (2022). The potential of machine learn-\ning algorithms for sentiment classification of students’ feedback on MOOC. In Intelligent Sys-\ntems and Applications (pp. 11–22). Springer International Publishing. https:// doi. org/ 10. 1007/ \n978-3- 030- 82199-9_2\nFan, X., Luo, W., Menekse, M., Litman, D. & Wang, J. (2015). CourseMIRROR: Enhancing large class-\nroom instructor-student interactions via mobile interfaces and natural language processing. Proceed-\nings of the 33rd Annual ACM Conference Extended Abstracts on Human Factors in Computing Sys-\ntems, 1473–1478. https:// doi. org/ 10. 1145/ 27026 13. 27328 53\nFerren, A. S., & Aylesworth, M. S. (2001). Using qualitative and quantitative information in academic \ndecision making. New Directions for Institutional Research, 2001(112), 67–83. https:// doi. org/ 10. \n1002/ ir. 29\nFlodén, J. (2017). The impact of student feedback on teaching in higher education. Assessment & Evalua-\ntion in Higher Education, 42(7), 1054–1068. https:// doi. org/ 10. 1080/ 02602 938. 2016. 12249 97\nGilardi, F., Alizadeh, M., & Kubli, M. (2023). ChatGPT outperforms crowd-workers for text-annotation \ntasks. arXiv [cs.CL]. Retrieved August 21, 2023, from http:// arxiv. org/ abs/ 2303. 15056\nGoštautaitė, D., & Sakalauskas, L. (2022). Multi-label classification and explanation methods for stu-\ndents’ learning style prediction and interpretation. NATO Advanced Science Institutes Series e: \nApplied Sciences, 12(11), 5396. https:// doi. org/ 10. 3390/ app12 115396\nGottipati, S., Shankararaman, V., & Lin, J. R. (2018). Text analytics approach to extract course improve-\nment suggestions from students’ feedback. Research and Practice in Technology Enhanced Learn-\ning, 13(1), 6. https:// doi. org/ 10. 1186/ s41039- 018- 0073-0\nGottipati, S., Shankararaman, V. & Gan, S. (2017). A conceptual framework for analyzing students’ feedback. \n2017 IEEE Frontiers in Education Conference (FIE), 1–8. https:// doi. org/ 10. 1109/ FIE. 2017. 81907 03\nGu, Y., Tinn, R., Cheng, H., Lucas, M., Usuyama, N., Liu, X., Naumann, T., Gao, J., & Poon, H. (2021). \nDomain-specific language model pretraining for biomedical natural language processing. ACM \nTransactions on Computing for Healthcare, 3(1), 1–23. https:// doi. org/ 10. 1145/ 34587 54\nHamzah, A., Hidayatullah, A. F., & Persada, A. G. (2020). Discovering trends of mobile learning research \nusing topic modelling approach. International Journal of Interactive Mobile Technologies (iJIM), \n14(09), 4. https:// doi. org/ 10. 3991/ ijim. v14i09. 11069\nHassija, V., Chamola, V., Mahapatra, A., Singal, A., Goel, D., Huang, K., Scardapane, S., Spinelli, I., \nMahmud, M., & Hussain, A. (2024). Interpreting black-box models: A review on explainable artifi-\ncial intelligence. Cognitive Computation, 16(1), 45–74. https:// doi. org/ 10. 1007/ s12559- 023- 10179-8\nHuang, F., Kwak, H., & An, J. (2023). Is ChatGPT better than human annotators? Potential and limita-\ntions of ChatGPT in explaining implicit hate speech. arXiv [cs.CL]. Retrieved August 21, 2023, \nfrom http:// arxiv. org/ abs/ 2302. 07736\nHuang, H., Qu, Y., Liu, J., Yang, M., Zhao, T. (2024). An empirical study of LLM-as-a-judge for LLM \nevaluation: Fine-tuned judge models are task-specific classifiers. arXiv [cs.CL]. Retrieved April 12, \n2024, from http:// arxiv. org/ abs/ 2403. 02839\nHugging Face – The AI community building the future. (n.d.). Retrieved August 21, 2023, from https:// \nhuggi ngface. co/ datas ets? task_ categ ories= task_ categ ories: zero- shot- class ifica tion& sort= trend ing.\nJansen, B. J., Jung, S.-G., & Salminen, J. (2023). Employing large language models in survey research. \nNatural Language Processing Journal, 4, 100020. https:// doi. org/ 10. 1016/j. nlp. 2023. 100020\nJohnson, J. M., & Khoshgoftaar, T. M. (2019). Survey on deep learning with class imbalance. Journal of \nBig Data, 6(1), 1–54. https:// doi. org/ 10. 1186/ s40537- 019- 0192-5\nJohnson, R. B., & Onwuegbuzie, A. J. (2004). Mixed methods research: A research paradigm whose time \nhas come. Educational Researcher, 33(7), 14–26. https:// doi. org/ 10. 3102/ 00131 89X03 30070 14\nKane, T. J., McCaffrey, D., Miller, T. & Staiger, D. (2013). Have we identified effective teachers? Validat-\ning measures of effective teaching using random assignment. Research paper. MET project. Bill & \nMelinda Gates Foundation. Retrieved April 9, 2024, from https:// eric. ed. gov/? id= ED540 959\nKastrati, Z., Imran, A. S., & Kurti, A. (2020b). Weakly supervised framework for aspect-based sentiment \nanalysis on students’ reviews of MOOCs. IEEE Access, 8, 106799–106810. https:// doi. org/ 10. 1109/ \nACCESS. 2020. 30007 39\nKastrati, Z., Arifaj, B., Lubishtani, A., Gashi, F., & Nishliu, E. (2020a). Aspect-based opinion mining \nof students’ reviews on online courses. In Proceedings of the 2020 6th International Conference \n1 3\nInternational Journal of Artificial Intelligence in Education \non Computing and Artificial Intelligence (ICCAI ’20) (pp. 510–514). Association for Computing \nMachinery. https:// doi. org/ 10. 1145/ 34045 55. 34046 33\nKennedy, R. K. L., Villanustre, F., Khoshgoftaar, T. M., & Salekshahrezaee, Z. (2024). Synthesizing \nclass labels for highly imbalanced credit card fraud detection data. Journal of Big Data, 11(1), 1–22. \nhttps:// doi. org/ 10. 1186/ s40537- 024- 00897-7\nKıcıman, E., Ness, R., Sharma, A., & Tan, C. (2023). Causal reasoning and large language models: Open-\ning a new frontier for causality. arXiv [cs.AI]. Retrieved August 21, 2023, from http:// arxiv. org/ abs/ \n2305. 00050\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y. & Iwasawa, Y. (2022). Large language models are zero-shot \nreasoners. In arXiv [cs.CL]. arXiv. http:// arxiv. org/ abs/ 2205. 11916.\nLattuca, L. R., & Domagal-Goldman, J. M. (2007). Using qualitative methods to assess teaching effec-\ntiveness. New Directions for Institutional Research. https:// doi. org/ 10. 1002/ ir. 233\nLee, J., Yoon, W., Kim, S., Kim, D., Kim, S., So, C. H., & Kang, J. (2020). BioBERT: A pre-trained \nbiomedical language representation model for biomedical text mining. Bioinformatics, 36(4), 1234–\n1240. https:// doi. org/ 10. 1093/ bioin forma tics/ btz682\nLoureiro, D., Barbieri, F., Neves, L., et al. (2022). TimeLMs: Diachronic language models from Twitter. \narXiv [cs.CL]. https:// doi. org/ 10. 48550/ arXiv. 2202. 03829\nMadaan, A., Tandon, N., Gupta, P., et  al. (2023). Self-refine: Iterative refinement with self-feedback. \narXiv [cs.CL]. https:// doi. org/ 10. 48550/ arXiv. 2303. 17651\nMarginson, S. & Considine, M. (2000). The enterprise university: Power, governance and reinvention in \nAustralia. Cambridge University Press. Retrieved April 5, 2024, from https:// play. google. com/ store/ \nbooks/ detai ls? id= SLljl FVJVO sC\nMarks, A., Al-Ali, M., Majdalawieh, M., & Bani-Hani, A. (2017). Improving academic decision-making \nthrough course evaluation technology. International Journal of Emerging Technologies in Learning, \n12(11), 4. https:// doi. org/ 10. 3991/ ijet. v12. i11. 6987\nMarsh, H. W., & Roche, L. (1993). The use of students’ evaluations and an individually structured inter -\nvention to enhance university teaching effectiveness. American Educational Research Journal, \n30(1), 217–251. https:// doi. org/ 10. 3102/ 00028 31203 00012 17\nMasala, M., Ruseti, S., Dascalu, M., & Dobre, C. (2021). Extracting and clustering main ideas from \nstudent feedback using language models. In Artificial Intelligence in Education (pp. 282–292). \nSpringer International Publishing. https:// doi. org/ 10. 1007/ 978-3- 030- 78292-4_ 23\nMattimoe, R., Hayden, M. T., Murphy, B. & Ballantine, J. (2021). Approaches to analysis of qualitative \nresearch data: A reflection on the manual and technological approaches. In Accounting, Finance & \nGovernance Review. https:// doi. org/ 10. 52399/ 001c. 22026\nMazzarol, T., Geoffrey, N. S., & Michael, S. Y. S. (2003). The third wave: Future trends in international \neducation. International Journal of Educational Management, 17(3), 90–99. https:// doi. org/ 10. \n1108/ 09513 54031 04677 78\nMcGourty, J., Scoles, K., & Thorpe, S. (2002). Web-based student evaluation of instruction: Promises \nand pitfalls. In 42nd Annual Forum of the Association for Institutional Research, Toronto, Ontario. \nRetrieved April 5, 2024, from http:// web. augsb urg. edu/ ~kraje wsk/ educa use20 04/ webev al. pdf\nMcKeachie, W. J. (1997). Student ratings: The validity of use. The American Psychologist, 52(11), 1218–\n1225. https:// doi. org/ 10. 1037/ 0003- 066X. 52. 11. 1218\nMedina, M. S., Smith, W. T., Kolluru, S., et al. (2019). A review of strategies for designing, administer -\ning, and using student ratings of instruction. American Journal of Pharmaceutical Education, 83, \n7177. https:// doi. org/ 10. 5688/ ajpe7 177\nMeidinger, M., & Aßenmacher, M. (2021). A new benchmark for NLP in social sciences: Evaluating \nthe usefulness of pre-trained language models for classifying open-ended survey responses. In Pro -\nceedings of the 13th International Conference on Agents and Artificial Intelligence (pp. 866–873). \nSCITEPRESS - Science and Technology Publications. https:// doi. org/ 10. 5220/ 00102 55108 660873\nMentkowski, M. (1991). Creating a context where institutional assessment yields educational improve-\nment. The Journal of General Education, 40, 255–283. Retrieved April 5, 2024, from http:// www. \njstor. org/ stable/ 27797 140\nMorbidoni, C. (2023). Poster: LLMs for online customer reviews analysis: oracles or tools? Experiments \nwith GPT 3.5. Proceedings of the 15th Biannual Conference of the Italian SIGCHI Chapter, 1–4. \nhttps:// doi. org/ 10. 1145/ 36053 90. 36108 10\nMoss, J., & Hendry, G. (2002). Use of electronic surveys in course evaluation. British Journal of Educa-\ntional Technology: Journal of the Council for Educational Technology, 33(5), 583–592. https:// doi. \norg/ 10. 1111/ 1467- 8535. 00293\n International Journal of Artificial Intelligence in Education\n1 3\nNanda, G., Douglas, K. A., Waller, D. R., Merzdorf, H. E., & Goldwasser, D. (2021). Analyzing large collec-\ntions of open-ended feedback from MOOC learners using LDA topic modeling and qualitative analysis. \nIEEE Transactions on Learning Technologies, 14(2), 146–160. https:// doi. org/ 10. 1109/ TLT. 2021. 30647 98\nNitin, G. I., Swapna, G., & Shankararaman, V. (2015). Analyzing educational comments for topics and \nsentiments: A text analytics approach. IEEE Frontiers in Education Conference (FIE), 2015, 1–9. \nhttps:// doi. org/ 10. 1109/ FIE. 2015. 73442 96\nOnan, A. (2021a). Sentiment analysis on massive open online course evaluations: A text mining and deep \nlearning approach. Computer Applications in Engineering Education, 29(3), 572–589. https:// doi. org/ \n10. 1002/ cae. 22253\nOnan, A. (2021b). Sentiment analysis on product reviews based on weighted word embeddings and deep \nneural networks. Concurrency and Computation: Practice & Experience, 33(23). https:// doi. org/ 10. \n1002/ cpe. 5909\nOrescanin, M., Smith, L. N., Sahu, S., Goyal, P., & Chhetri, S. R. (2023). Editorial: Deep learning with lim-\nited labeled data for vision, audio, and text. Frontiers in Artificial Intelligence, 6, 1213419. https:// doi. \norg/ 10. 3389/ frai. 2023. 12134 19\nPangakis, N., Wolken, S., & Fasching, N. (2023). Automated annotation with generative AI requires valida-\ntion. arXiv [cs.CL]. Retrieved April 5, 2024, from http:// arxiv. org/ abs/ 2306. 00176\nPapers with Code - Machine Learning Datasets. (n.d.). Retrieved August 21, 2023, from https:// paper swith \ncode. com/ datas ets? task= text- class ifica tion.\nPatil, P. P., Phansalkar, S. & Kryssanov, V. V. (2019). Topic modelling for aspect-level sentiment analysis. \nProceedings of the 2nd International Conference on Data Engineering and Communication Technol-\nogy, 221–229. https:// doi. org/ 10. 1007/ 978- 981- 13- 1610-4_ 23\nPeng, B., Li, C., He, P., et al. (2023). Instruction tuning with GPT-4. arXiv [cs.CL]. https:// doi. org/ 10. 48550/ \narXiv. 2304. 03277\nPerez-Encinas, A., & Rodriguez-Pomeda, J. (2018). International students’ perceptions of their needs when \ngoing abroad: Services on demand. Journal of Studies in International Education, 22(1), 20–36. https:// \ndoi. org/ 10. 1177/ 10283 15317 724556\nPradhan, V. K., Schaekermann, M., & Lease, M. (2022). In search of ambiguity: A three-stage workflow \ndesign to clarify annotation guidelines for crowd workers. Frontiers in Artificial Intelligence, 5, 828187. \nhttps:// doi. org/ 10. 3389/ frai. 2022. 828187\nPyasi, S., Gottipati, S. & Shankararaman, V. (2018). SUFAT: An analytics tool for gaining insights from \nstudent feedback comments. (2018). 2018 Frontiers in Education Conference 48th FIE: San Jose, CA, \nOctober 3–6: Proceedings, 1–9. Retrieved April 5, 2024, from https:// core. ac. uk/ downl oad/ pdf/ 20025 \n4353. pdf\nReimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence embeddings using Siamese BERT-networks. \narXiv [cs.CL]. https:// doi. org/ 10. 48550/ arxiv. 1908. 10084\nReiss, M. V. (2023). Testing the reliability of ChatGPT for text annotation and classification: A cautionary \nremark. arXiv [cs.CL]. Retrieved August 21, 2023, from http:// arxiv. org/ abs/ 2304. 11085\nReynolds, L., & McDonell, K. (2021). Prompt programming for large language models: Beyond the few-shot \nparadigm. arXiv [cs.CL]. https:// doi. org/ 10. 48550/ arXiv. 2102. 07350\nRichardson, J. T. E. (2005). Instruments for obtaining student feedback: A review of the literature. Assess-\nment & Evaluation in Higher Education, 30(4), 387–415. https:// doi. org/ 10. 1080/ 02602 93050 00991 93\nRiger, S. & Sigurvinsdottir, R. (2016). Thematic Analysis. In Jason, L., & Glenwick, D. (Eds.), Handbook of \nmethodological approaches to community-based research: Qualitative, quantitative, and mixed meth-\nods (pp. 33–41). Oxford university press.\nRother, A., Niemann, U., Hielscher, T., Völzke, H., Ittermann, T., & Spiliopoulou, M. (2021). Assessing the \ndifficulty of annotating medical data in crowdworking with help of experiments. PLoS ONE, 16(7), \ne0254764. https:// doi. org/ 10. 1371/ journ al. pone. 02547 64\nSchreiner, M. (2023). GPT-4 architecture, datasets, costs and more leaked. Retrieved April 5, 2024, from \nhttps:// the- decod er. com/ gpt-4- archi tectu re- datas ets- costs- and- more- leaked/\nSchulz, J., Sud, G. & Crowe, B. (2014). Lessons from the field: The role of student surveys in teacher evalu-\nation and development. Bellwether Education Partners. Retrieved April 5, 2024, from https:// eric. ed. \ngov/? id= ED553 986\nShah, M. & Ali, H. (2023). Imbalanced data in machine learning: A comprehensive review. https:// doi. org/ \n10. 13140/ RG.2. 2. 18456. 98564\nShah, M., & Pabel, A. (2019). Making the student voice count: Using qualitative student feedback to enhance \nthe student experience. Journal of Applied Research in Higher Education, 12(2), 194–209. https:// doi. \norg/ 10. 1108/ JARHE- 02- 2019- 0030\n1 3\nInternational Journal of Artificial Intelligence in Education \nShaik, T., Tao, X., Li, Y., Dann, C., McDonald, J., Redmond, P., & Galligan, L. (2022). A review of the \ntrends and challenges in adopting natural language processing methods for education feedback analysis. \nIEEE Access, 10, 56720–56739. https:// doi. org/ 10. 1109/ ACCESS. 2022. 31777 52\nShaik, T., Tao, X., Dann, C., Xie, H., Li, Y. & Galligan, L. (2023). Sentiment analysis and opinion mining on \neducational data: A survey. In arXiv [cs.CL]. arXiv. Retrieved April 4, 2024, from http:// arxiv. org/ abs/ \n2302. 04359\nShen, Y., Song, K., Tan, X., et al. (2023). HuggingGPT: Solving AI tasks with ChatGPT and its friends in \nHugging Face. arXiv [cs.CL]. https:// doi. org/ 10. 48550/ arXiv. 2303. 17580\nSindhu, I., Muhammad, S., Badar, K., Bakhtyar, M., Baber, J., & Nurunnabi, M. (2019). Aspect-based opin-\nion mining on student’s feedback for faculty teaching performance evaluation. IEEE Access, 7, 108729–\n108741. https:// doi. org/ 10. 1109/ ACCESS. 2019. 29288 72\nSmith, A. E., & Humphreys, M. S. (2006). Evaluation of unsupervised semantic mapping of natural language \nwith Leximancer concept mapping. Behavior Research Methods, 38(2), 262–279. https:// doi. org/ 10. \n3758/ bf031 92778\nSpooren, P., Brockx, B., & Mortelmans, D. (2013). On the validity of student evaluation of teaching: The \nstate of the art. Review of Educational Research, 83(4), 598–642. https:// doi. org/ 10. 3102/ 00346 54313 \n496870\nStowell, J. R., Addison, W. E., & Smith, J. L. (2012). Comparison of online and classroom-based student \nevaluations of instruction. Assessment & Evaluation in Higher Education, 37(4), 465–473. https:// doi. \norg/ 10. 1080/ 02602 938. 2010. 545869\nSunar, A. S., & Khalid, M. S. (2024). Natural language processing of student’s feedback to instructors: A \nsystematic review. IEEE Transactions on Learning Technologies, 17, 741–753. https:// doi. org/ 10. 1109/ \nTLT. 2023. 33305 31\nSutoyo, E., Almaarif, A., & Yanto, I. T. R. (2021). Sentiment analysis of student evaluations of teaching \nusing deep learning approach. In International Conference on Emerging Applications and Technologies \nfor Industry 4.0 (EATI’2020) (pp. 272–281). Springer International Publishing. https:// doi. org/ 10. 1007/ \n978-3- 030- 80216-5_ 20\nTörnberg, P. (2023). ChatGPT-4 outperforms experts and crowd workers in annotating political twitter mes-\nsages with zero-shot learning. In arXiv [cs.CL]. arXiv. Retrieved August 21, 2023, from http:// arxiv. \norg/ abs/ 2304. 06588\nTunstall, L., Reimers, N., Jo, U. E. S., Bates, L., Korat, D., Wasserblat, M., & Pereg, O. (2022). Efficient \nfew-shot learning without prompts. In arXiv [cs.CL]. arXiv. https:// doi. org/ 10. 48550/ arXiv. 2209. 11055\nUC Berkeley Center for Teaching & Learning. (n.d.). Course evaluations question bank. Retrieved August \n21, 2023, from https:// teach ing. berke ley. edu/ course- evalu ations- quest ion- bank\nUnankard, S., & Nadee, W. (2020). Topic detection for online course feedback using LDA. In Emerging \nTechnologies for Education (pp. 133–142). Springer International Publishing. https:// doi. org/ 10. 1007/ \n978-3- 030- 38778-5_ 16\nUniversity of Wisconsin—Madison. (n.d.). Best practices and sample questions for course evaluation sur-\nveys. In Student learning assessment. Retrieved August 21, 2023, from https:// asses sment. wisc. edu/ \nbest- pract ices- and- sample- quest ions- for- course- evalu ation- surve ys/\nVeselovsky, V., Ribeiro, M. H., & West, R. (2023). Artificial artificial artificial intelligence: Crowd workers \nwidely use large language models for text production tasks. arXiv [cs.CL]. https:// doi. org/ 10. 48550/ \narXiv. 2306. 07899\nWallace, S. L., Lewis, A. K., & Allen, M. D. (2019). The state of the literature on student evaluations of \nteaching and an exploratory analysis of written comments: Who benefits most? College Teaching, \n67(1), 1–14. https:// doi. org/ 10. 1080/ 87567 555. 2018. 14833 17\nWang, X., Wei, J., Schuurmans, D., et al. (2022). Self-consistency improves chain of thought reasoning in \nlanguage models. arXiv [cs.CL]. https:// doi. org/ 10. 48550/ arXiv. 2203. 11171\nWei, J., Wang, X., Schuurmans, D., et al. (2022). Chain-of-thought prompting elicits reasoning in large lan-\nguage models. arXiv [cs.CL], 24824–24837. https:// doi. org/ 10. 48550/ arXiv. 2201. 11903\nWeng, L. (2023). LLM-powered autonomous agents. Lil’Log. Retrieved August 21, 2023, from https:// lilia \nnweng. github. io/ posts/ 2023- 06- 23- agent/\nWhite, J., Fu, Q., Hays, S., et al. (2023). A prompt pattern catalog to enhance prompt engineering with Chat-\nGPT. arXiv [cs.SE]. https:// doi. org/ 10. 48550/ arXiv. 2302. 11382\nWongsurawat, W. (2011). What’s a comment worth? How to better understand student evaluations of teach-\ning. Quality Assurance in Education, 19(1), 67–83. https:// doi. org/ 10. 1108/ 09684 88111 11077 62\nYao, S., Zhao, J., Yu, D., et al. (2022). ReAct: Synergizing reasoning and acting in language models. arXiv \n[cs.CL]. https:// doi. org/ 10. 48550/ arXiv. 2210. 03629\n International Journal of Artificial Intelligence in Education\n1 3\nZhang, H., Dong, J., Min, L., & Bi, P. (2020). A BERT fine-tuning model for targeted sentiment analysis \nof Chinese online course reviews. International Journal of Artificial Intelligence Tools: Architectures, \nLanguages, Algorithms, 29(07n08), 2040018. https:// doi. org/ 10. 1142/ S0218 21302 04001 87\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nAuthors and Affiliations\nMichael J. Parker1  · Caitlin Anderson1 · Claire Stone2 · YeaRim Oh2\n * Michael J. Parker \n michael_parker@hms.harvard.edu\n Caitlin Anderson \n caitlin_anderson@hms.harvard.edu\n Claire Stone \n claire_stone@hms.harvard.edu\n YeaRim Oh \n yearim_oh@hms.harvard.edu\n1 Office of Online Learning, Harvard Medical School, HMX, 4 Blackfan Circle, Boston, MA, \nUSA\n2 Office of External Education, Harvard Medical School, Boston, MA, USA",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6856469511985779
    },
    {
      "name": "Workflow",
      "score": 0.5116643905639648
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.48575451970100403
    },
    {
      "name": "Class (philosophy)",
      "score": 0.46616795659065247
    },
    {
      "name": "Thematic analysis",
      "score": 0.4649679958820343
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45064693689346313
    },
    {
      "name": "Curriculum",
      "score": 0.44327396154403687
    },
    {
      "name": "Personalization",
      "score": 0.4140256345272064
    },
    {
      "name": "Data science",
      "score": 0.3835896849632263
    },
    {
      "name": "World Wide Web",
      "score": 0.17687657475471497
    },
    {
      "name": "Psychology",
      "score": 0.1462881863117218
    },
    {
      "name": "Pedagogy",
      "score": 0.1327698528766632
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Qualitative research",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    }
  ],
  "cited_by": 6
}