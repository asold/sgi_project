{
    "title": "ADEPT: Adapter-based Efficient Prompt Tuning Approach for Language Models",
    "url": "https://openalex.org/W4385571322",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2266882273",
            "name": "Aditya Shah",
            "affiliations": [
                "Virginia Tech"
            ]
        },
        {
            "id": "https://openalex.org/A3046931858",
            "name": "Surendrabikram Thapa",
            "affiliations": [
                "Virginia Tech"
            ]
        },
        {
            "id": "https://openalex.org/A5067111844",
            "name": "Aneesh Jain",
            "affiliations": [
                "Virginia Tech"
            ]
        },
        {
            "id": "https://openalex.org/A2166497444",
            "name": "Lifu Huang",
            "affiliations": [
                "Virginia Tech"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3093517588",
        "https://openalex.org/W3205717164",
        "https://openalex.org/W4319915529",
        "https://openalex.org/W3173777717",
        "https://openalex.org/W3044438666",
        "https://openalex.org/W3153427360",
        "https://openalex.org/W3174784402",
        "https://openalex.org/W3098267758",
        "https://openalex.org/W4229506649",
        "https://openalex.org/W3212706150",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3026404337",
        "https://openalex.org/W3166986030",
        "https://openalex.org/W2970925270",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W4206178588",
        "https://openalex.org/W3153675281",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4287863810",
        "https://openalex.org/W4287328210",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W2953271402",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2113459411",
        "https://openalex.org/W3166846774",
        "https://openalex.org/W2891555348",
        "https://openalex.org/W3176693010",
        "https://openalex.org/W2943552823",
        "https://openalex.org/W4297801719",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W3088409176",
        "https://openalex.org/W4309811444",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W4205991051"
    ],
    "abstract": "Fine-tuning large pre-trained models for downstream tasks can be really expensive.In the past, researchers have proposed various alternatives like adapter and prompt-based methods for tuning these large language models using minimal parameters.However, applying prompt-tuning for smaller language models has not been effective so far and not much work is done in pushing forward soft prompting for these smaller models.To improve the training efficiency of the language models and reduce the size of tuned parameters, we propose a novel Adapter-based Efficient Prompt Tuning approach (ADEPT).In this paper, we show that tuning the parameters of soft prompts with adapter modules while keeping the rest of the model frozen can be a promising method to optimize smaller language models for downstream tasks.Our method achieves up to 98% performance of full fine-tuning while using only 0.02% of total model parameters.",
    "full_text": "Proceedings of The Fourth Workshop on Simple and Efficient Natural Language Processing (SustaiNLP), pages 121–128\nJuly 13, 2023 ©2023 Association for Computational Linguistics\nADEPT: Adapter-based Efficient Prompt Tuning\nApproach for Language Models\nAditya Shah, Surendrabikram Thapa, Aneesh Jain, Lifu Huang\nDepartment of Computer Science, Virginia Tech\n{aditya31, surendrabikram, aneeshj, lifuh}@vt.edu\nAbstract\nFine-tuning large pre-trained models for down-\nstream tasks can be really expensive. In the\npast, researchers have proposed various alter-\nnatives like adapter and prompt-based meth-\nods for tuning these large language models us-\ning minimal parameters. However, applying\nprompt-tuning for smaller language models has\nnot been effective so far and not much work\nis done in pushing forward soft prompting for\nthese smaller models. To improve the train-\ning efficiency of the language models and re-\nduce the size of tuned parameters, we propose\na novel Adapter-based Efficient Prompt Tun-\ning approach (ADEPT). In this paper, we show\nthat tuning the parameters of soft prompts with\nadapter modules while keeping the rest of the\nmodel frozen can be a promising method to\noptimize smaller language models for down-\nstream tasks. Our method achieves up to 98%\nperformance of full fine-tuning while using only\n0.02% of total model parameters.\n1 Introduction\nWith the rapid advancement in computational fa-\ncilities and the research in the field of Natural\nLanguage Processing (NLP), pre-trained language\nmodels (Peters et al., 2018; Conneau et al., 2018;\nDevlin et al., 2019; Yang et al., 2019; Raffel et al.,\n2020; Qiu et al., 2020; Xue et al., 2021; Dodda-\npaneni et al., 2021) have been used widely in var-\nious tasks. These models use different statistical\nand probabilistic methods to decide the likelihood\nof a given sequence of words occurring in a sen-\ntence. To efficiently use the language models, re-\nsearchers fine-tune these pre-trained language mod-\nels on downstream tasks. With the conventional\npractices of fine-tuning the models, new parame-\nters are generally introduced for every downstream\ntask. However, this approach of fine-tuning lan-\nguage models becomes difficult especially when\nthere are a lot of trainable parameters. With lan-\nguage models becoming larger and larger (Brown\net al., 2020), we can often anticipate challenges\nrelated to maintaining multiple copies of model\nparameters for inference, training time, and lack\nof necessary computing power. The concept of\nFigure 1: Prompt-based tuning using discrete prompt.\nThe prompt “ Experience was ” with a [MASK] is\nprepended to the input text.\nprompt-tuning was introduced to improve parame-\nter efficiency in the downstream tasks. In prompt\ntuning, there’s no need for new parameters as we\nconvert our problem into a language modeling task.\nThis method can be promising, especially when\nthere are very few training examples for e.g. few\nshot learning (Gao et al., 2021), where the standard\nfine-tuning would not be efficient.\nA prompt is usually a sequence of words or pa-\nrameters that are appended or prepended to the\ninput so that the given downstream task can be\nconstructed as a language modeling problem (Liu\net al., 2021a). An example is shown in Figure 1.\nIn order to classify the sentiment of a given text,\nlike an amazon product review “The delivery was\nbad. The items were broken. ”, we can prepend\na prompt, such as “ Experience was” or “It was”,\nwith a [MASK] to the sentence and anticipate the\nlanguage model to predict a negative adjective such\nas “awful” for the [MASK] position.\nAs shown by Lester et al. (2021); Kim et al.\n(2021), soft prompt-tuning for smaller language\nmodels do not perform well when compared\nto traditional fine-tuning. This approach only\nworks for significantly larger models (BERT large,\nRoBERTalarge , etc). In this paper, we propose a\nnovel approach to leverage soft prompt tuning for\nsmaller language models. We insert adapter mod-\nules in the language model, then jointly fine-tune\nthe parameters of this adapter module along with\nsoft prompts while keeping the rest of the model\nfrozen. Through empirical results on 3 benchmark\ndatasets from SuperGLUE (Wang et al., 2019) and\n4 text classification datasets, we demonstrate the\neffectiveness of our proposed approach for these\n121\nsmaller LM models (RoBERTa (Liu et al., 2019)\nand BERT (Devlin et al., 2019)). Our method opti-\nmizes only 0.02% of total model parameters during\ntraining and yet achieves better performance than\nother tuning strategies while being competitive to\nfine-tuning.\nOur main contributions can be summarised as\nfollows:\n• a new Adapter-based Efficient Prompt Tuning\n(ADEPT) approach to leverage soft prompts\nfor smaller language models - \"roberta-base\"\nand \"bert-base-cased\".\n• analyze the effectiveness of our approach with\nrespect to other soft prompt-tuning and fine-\ntuning methods.\n• an ablation study to investigate the importance\nof the number of prompt tokens and adapter\nhidden size.\n2 Related Work\nThe effectiveness of prompt tuning was demon-\nstrated by (Brown et al., 2020) where the authors\nshowed that GPT-3 model could handle wide va-\nriety of tasks using only a few training examples.\nThe use of prompts was first proposed by (Radford\nand Narasimhan, 2018). The authors showed that\nlanguage models can perform well in few-shot and\nzero-shot settings through these natural language\nprompts. More recently, Jiang et al. (2020) pro-\nposed an approach to automatically discover bet-\nter prompts in order to improve the factual knowl-\nedge retrieval from these language models. More-\nover, Schick and Schütze (2021) introduced Pattern\nExploiting Training (PET) which uses cloze-style\nphrases and achieves state-of-the-art performance\non few supervised and semi-supervised tasks -\nclassification on Yelp Reviews, AG’s News, Ya-\nhoo Questions (Zhang et al., 2015) and MNLI\n(Williams et al., 2018). This work was further\nimproved by (Tam et al., 2021) for few-shot natural\nlanguage understanding without using any unla-\nbeled data. In all of these approaches, prompts\nwere manually designed in the form of discrete\ntokens. Thus, in such scenarios, it is important\nto design appropriate prompts based on different\ndownstream tasks. The importance of prompt en-\ngineering and the complete paradigm of prompt\ntuning is summarized in Liu et al. (2021a).\nIn contrast to discrete prompts (Shin et al., 2020;\nHambardzumyan et al., 2021; Gao et al., 2021;\nReynolds and McDonell, 2021), soft prompts are\nrandomly initialized vectors that are prepended\nor appended to the input text. The parameters\nof the entire language model are fixed and only\nthe prompt parameters are fine-tuned. Liu et al.\n(2021b) showed that automatically searching bet-\nter prompts in the continuous space gives com-\npetitive performance for natural language under-\nstanding. Soft prompts were initially proposed by\nZhong et al. (2021) where OptiPrompt was pro-\nposed and outperformed discrete prompts on knowl-\nedge probing tasks. Li and Liang (2021) and Qin\nand Eisner (2021) used a similar idea for generation\ntasks where they prepended task-specific prompts\nin the input text and achieved comparable perfor-\nmance as the original model fine-tuning. Han et al.\n(2021) proposed prompt-tuning with rules (PTR)\nwhich significantly outperformed state-of-the-art\nbaselines for relation classification tasks. The ef-\nfectiveness of soft prompt-tuning was further lever-\naged by Lester et al. (2021) where they applied\nsoft prompts on the T5 model and achieved a good\nperformance on the SuperGLUE benchmark. Fur-\nthermore, Su et al. (2021); Vu et al. (2022) studied\nthe transferability of prompt tuning across different\ntasks and models. They showed that soft prompts\ncan be transferred to similar tasks without training\nand can be a good initialization for the underlying\nlanguage model.\n3 Approach\nWe propose a novel adapter-based prompt-tuning\narchitecture for downstream classification tasks.\nFigure 2 shows an overview of the model. We\nuse the pre-trained BERT (Devlin et al., 2019) and\nRoBERTa (Liu et al., 2019) from Hugging Face as\nthe encoder. For soft prompt-tuning, we append\nsoft prompt embedding in the input text using the\nsame approach as Lester et al. (2021). Given an\ninput text X = [x1, ..., xn], where xi is the i−th\ntoken in the text andn is the length of the sequence,\nwe append prompt tokens [p1, p2, ... pm] where m\nis the number of prompt tokens. We initialize these\nprompt tokens with embeddings obtained from ran-\ndom words in the vocabulary and update them dur-\ning training, which shows better performance on\ndownstream tasks than initializing them with ran-\ndom vectors. We found that initializing prompt\ntokens with random word embeddings instead of\nrandom vectors helps the model converge better\non downstream tasks. The resulting input to the\n122\nmodel becomes: [p1, ..., pm, x1, ..., xn]. We do not\nuse any separator token between prompt vectors\nand tokens.\nFigure 2: Overview of the ADEPT model. The adapter\nmodule is inserted between encoder layer 8 and layer 9.\nParameters of prompt embeddings and this adapter mod-\nule are tuned while the rest of the complete model and\ninput text embeddings are frozen. ADEPT approach us-\ning RoBERTa/BERT tunes only28K parameters (0.02%\nof 123M total parameters)\nInspired by the work of Bapna and Firat (2019);\nHoulsby et al. (2019); Pfeiffer et al. (2021); Rücklé\net al. (2021); He et al. (2021), we insert adapter\nmodules in the form of Multi-Layer Perceptron\n(MLP) between the encoder layers of the base trans-\nformer model. To reduce the number of parameters,\nwe choose a bottleneck architecture for the adapter\nmodule. In a bottleneck architecture, the hidden\nlayer also called as bottleneck size is much smaller\nthan the input layer. If d is the input size (number\nof neurons) of the MLP layer and b is the bottle-\nneck size (adapter hidden size), then the bottleneck\narchitecture would require 2·d·b+d+b parameters\n. When b << d, the total parameters are greatly\nreduced compared to a general MLP layer with\nd · d + d parameters. If the embedding dimension\nsize is e and the prompt length is m, then the train-\nable parameters for prompt embeddings is m . e .\nSo the total trainable parameters for the ADEPT\nmodel are just [2 · d · b + (d + b)] +m · e. We\nuse one adapter module with hidden size b = 8\nand prompt length m = 20in our implementation\nwhich is inserted between the 8th and 9th encoder\nlayer 1. For RoBERTa and BERT models, we have\nd = e = 768. So, total trainable parameters for\nthe ADEPT model is only 28k while in standard\nfine-tuning, we update all the parameters ( 123M\nparameters for roberta-base model.)\n4 Experimental Setup\nWe conduct experiments on four classification\ndatasets - IMDB (Maas et al., 2011), AG’s News\n(Zhang et al., 2015) , Yahoo Answers (Zhang et al.,\n2015), Yelp - 5 (Zhang et al., 2015) and three\ndatasets from SuperGLUE benchmark - Boolq\n(Clark et al., 2019), CommitmentBank (de Marn-\neffe et al., 2019), and Recognizing Textual Entail-\nment (RTE) . More details on data statistics and\nimplementation details are described in Appendix\nA\nTo show the efficiency of our adapter-based\nprompt tuning approach, we compare it with sev-\neral other training paradigms:\n• Prompt-tuning (PT): fine-tune soft prompt\nembeddings while keeping the entire model\nfrozen. The randomly initialized parameters\nof the classification head are also fixed.\n• Head-tuning (HT): only fine-tune the classi-\nfication head. The base transformer model is\nfrozen.\n• Prompt + Head-tuning (PHT):fine-tune soft\nprompt embeddings along with the parame-\nters of the classification head. The base trans-\nformer model is frozen.\n• Fine-tuning (FT): traditional fine-tuning\nwhere the entire model along with the clas-\nsification head is trained.\n1We experimented with different positions for the adapter\nmodule: between layer 4 and 5; between layer 6 and 7; be-\ntween layer 8 and 9; between layer 10 and 11. We achieved\nthe best results when the adapter module was inserted between\nlayer 8 and layer 9. It stands out to the reason that the lower\nlayers of BERT account for sentence and coarse linguistic\nstructure while the higher layers are more domain-specific and\nhelp to learn task-specific parameters (Rogers et al., 2020).\nAttaching an adapter module between higher layers helps the\nadapter module parameters to converge better to downstream\ntasks\n123\nModel Method IMDB AG’s News Yahoo Yelp-5 BoolQ CB RTE\nBERT\nPrompt (PT) 0.91 0 .86 0 .66 0 .57 0.62 0 .66 0 .47\nHead (HT) 0.89 0 .88 0 .67 0 .60 0.61 0 .72 0 .53\nPrompt + Head (PHT) 0.92 0 .92 0 .70 0 .63 0.67 0 .71 0 .54\nFine-tune (FT) 0.94 0 .94 0 .72 0 .69 0.73 0 .83 0 .61\nAdapter + Prompt (ADEPT) 0.92 0 .93 0 .71 0 .67 0.69 0 .80 0 .58\nRoBERTa\nPrompt (PT) 0.90 0 .87 0 .65 0 .59 0.62 0 .64 0 .50\nHead (HT) 0.91 0 .90 0 .68 0 .61 0.63 0 .70 0 .54\nPrompt + Head (PHT) 0.94 0 .93 0 .72 0 .66 0.64 0 .71 0 .55\nFine-tune (FT) 0.96 0 .95 0 .73 0 .71 0.80 0 .86 0 .71\nAdapter + Prompt (ADEPT) 0.95 0 .94 0 .72 0 .68 0.71 0 .77 0 .60\nTable 1: Performance on all evaluation tasks. Each experiment is run for 3 trials and the average result is\nreported. For text classification tasks (IMDB, AG’s News, Yahoo, Yelp-5), we report the test F-score; for Boolq,\nCommitmentBank (CB), and Recognizing Textual Entailment (RTE), we report the test accuracy. ADEPT approach\nresults in better performance than Head-tuning (HT) and Prompt + Head-tuning (PHT) while using significantly\nlower parameters and is competitive to standard fine-tuning\n• Adapter + Prompt-tuning (ADEPT): fine-\ntune soft prompt embeddings and adapter\nmodule parameters that are inserted in the en-\ncoder layer. The rest of the model and the\nrandomly initialized parameters of the classi-\nfication head are kept fixed.\n5 Results\nTable 1 shows the F-score and accuracy on all eval-\nuation tasks under different training settings. Ta-\nble 2 compares the model parameters and training\nmetrics. We can see that ADEPT outperforms all\nthe other methods which just tune the prompt or\nhead layers using significantly lower parameters.\nBy just tuning 0.02% parameters, ADEPT shows\ncomparable performance as the method that fine-\ntunes all the parameters, demonstrating its signif-\nicance in improving the training efficiency. It is\ninteresting to see that although Head-tuning (HT)\nand Prompt + Head-tuning (PHT) methods have\nlarger parameters to be optimized, the ADEPT\nmethod still achieves better results on the Super-\nGLUE dataset and requires lesser training time\ncompared to the standard fine-tuning approach. We\nalso observe that standard prompt-tuning requires\nabout 2.5 times more steps to converge compared\nto fine-tuning approach and does not perform well\nfor multi-class classification. A similar result was\nreported by Lester et al. (2021), where the authors\nclaim that soft prompt based-tuning does not per-\nform well for smaller language models.\nOverall, attaching adapter modules between the\nencoder layers helps to retain the knowledge from\npre-trained LMs and efficiently learn the required\nparameters to further improve the performance on\ndownstream tasks. This helps the language model\nto better adapt to downstream tasks using minimal\nparameters. It achieves comparable performance\nas standard fine-tuning while being highly param-\neter efficient and requiring much lower training\ntime. All these demonstrate the effectiveness of\nour proposed approach in improving the training\nand resource efficiency of language models.\nMethod # params % params time Convergence\nPT 13K 0.01% 0.9 t 2.5 x steps\nHT 600K 0.48% 0.5 t 2 x steps\nPHT 620K 0.49% 0.8 t 2 x steps\nFT 123M 100% t x steps\nADEPT 28K 0.02% 0.7 t 1.5 x steps\nTable 2: Model parameters and training metrics. t and\nx refer to training time and training steps respectively\nas required by standard fine-tuning. # params denotes\nthe number of trainable parameters for every method.\n% params denotes % of parameters to be optimized\ncompared to standard fine-tuning.\n6 Conclusion\nWe proposed a novel adapter-based prompt-tuning\napproach for fine-tuning language models. Our re-\nsults demonstrate the effectiveness of the approach\non seven benchmark datasets. ADEPT achieves\na significant performance boost over PT, HT, and\nPHT approaches and is comparable to the standard\nfine-tuning while using only 0.02% of the model\nparameters. Since adapter modules learn the re-\nquired parameters for various NLU tasks, they help\nin retaining the knowledge of the pre-trained LM\nmodel when the encoder is frozen. Our work aims\nto facilitate the further research direction of using\nadapter-based prompt methods for tuning LMs.\n124\nLimitations\nDue to the limited resources, we could not experi-\nment with this approach for larger language models\nsuch as roberta-largeand bert-large. It would be in-\nteresting to investigate the performance of ADEPT\nwith larger LMs.\nIn addition, we only evaluate the ADEPT ap-\nproach on seven downstream tasks. It would also be\ninteresting to test it on more broad natural language\nprocessing tasks, such as information extraction,\nnatural language generation, question answering,\nand so on.\nBroader Impact\nAs discussed earlier, fine-tuning large pre-trained\nmodels for downstream tasks can be really expen-\nsive. The ADEPT approach can help AI practition-\ners to assess the abilities of LM without using a\nlot of resources. This approach of prompt tuning\ncan also help smaller end users to take advantage\nof harnessing the power of LM with the minimal\nresources they have. It can be used by social sci-\nentists, Non-profit organizations, etc. to create a\npositive impact in society in spite of limited com-\nputing resources.\nReproducibility\nThe code and resources for this work are available\nat our GitHub repository2. The code for training,\ntesting, and producing plots are made available.\nThe details on the model and hyperparameters are\ngiven in Appendix A.2. A brief introduction to the\ndataset used in this paper is given in Appendix A.1.\nOur implementation approach specific to all the\ndataset used are also explained.\nReferences\nAnkur Bapna and Orhan Firat. 2019. Simple, scal-\nable adaptation for neural machine translation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1538–\n1548, Hong Kong, China. Association for Computa-\ntional Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\n2https://github.com/Aditya-shahh/ADEPT\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions. In NAACL.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina\nWilliams, Samuel Bowman, Holger Schwenk, and\nVeselin Stoyanov. 2018. XNLI: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2475–2485, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nMarie-Catherine de Marneffe, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The commitmentbank: Investi-\ngating projection in naturally occurring discourse.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nSumanth Doddapaneni, Gowtham Ramesh, Mitesh M.\nKhapra, Anoop Kunchukuttan, and Pratyush Kumar.\n2021. A primer on pretrained multilingual language\nmodels.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nKaren Hambardzumyan, Hrant Khachatrian, and\nJonathan May. 2021. W ARP: Word-level Adversarial\nReProgramming. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 4921–4933, Online. Association for\nComputational Linguistics.\nXu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and\nMaosong Sun. 2021. Ptr: Prompt tuning with rules\nfor text classification.\n125\nRuidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng\nDing, Liying Cheng, Jia-Wei Low, Lidong Bing, and\nLuo Si. 2021. On the effectiveness of adapter-based\ntuning for pretrained language model adaptation.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In\nProceedings of the 36th International Conference\non Machine Learning , volume 97 of Proceedings\nof Machine Learning Research , pages 2790–2799.\nPMLR.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know?\nBoseop Kim, HyoungSeok Kim, Sang-Woo Lee,\nGichang Lee, Donghyun Kwak, Jeon Dong Hyeon,\nSunghyun Park, Sungju Kim, Seonhoon Kim, Dong-\npil Seo, Heungsub Lee, Minyoung Jeong, Sungjae\nLee, Minsub Kim, Suk Hyun Ko, Seokhun Kim,\nTaeyong Park, Jinuk Kim, Soyoung Kang, Na-Hyeon\nRyu, Kang Min Yoo, Minsuk Chang, Soobin Suh,\nSookyo In, Jinseong Park, Kyungduk Kim, Hiun\nKim, Jisu Jeong, Yong Goo Yeo, Donghoon Ham,\nDongju Park, Min Young Lee, Jaewook Kang, Inho\nKang, Jung-Woo Ha, Woomyoung Park, and Nako\nSung. 2021. What changes can large-scale language\nmodels bring? intensive study on HyperCLOV A:\nBillions-scale Korean generative pretrained trans-\nformers. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 3405–3424, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021a. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021b. GPT\nunderstands, too. CoRR, abs/2103.10385.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 142–150, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227–2237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nJonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé,\nKyunghyun Cho, and Iryna Gurevych. 2021.\nAdapterFusion: Non-destructive task composition\nfor transfer learning. In Proceedings of the 16th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Main Volume, pages\n487–503, Online. Association for Computational Lin-\nguistics.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying LMs with mixtures of soft prompts.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5203–5212, Online. Association for Computa-\ntional Linguistics.\nXiPeng Qiu, TianXiang Sun, YiGe Xu, YunFan\nShao, Ning Dai, and XuanJing Huang. 2020. Pre-\ntrained models for natural language processing: A\nsurvey. Science China Technological Sciences ,\n63(10):1872–1897.\nAlec Radford and Karthik Narasimhan. 2018. Im-\nproving language understanding by generative pre-\ntraining.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(1).\nLaria Reynolds and Kyle McDonell. 2021. Prompt\nprogramming for large language models: Beyond the\nfew-shot paradigm.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in bertology: What we know about\nhow bert works.\nAndreas Rücklé, Gregor Geigle, Max Glockner, Tilman\nBeck, Jonas Pfeiffer, Nils Reimers, and Iryna\nGurevych. 2021. AdapterDrop: On the efficiency\nof adapters in transformers. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\n126\nLanguage Processing, pages 7930–7946, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nTimo Schick and Hinrich Schütze. 2021. Exploiting\ncloze questions for few shot text classification and\nnatural language inference.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric\nWallace, and Sameer Singh. 2020. AutoPrompt: Elic-\niting Knowledge from Language Models with Auto-\nmatically Generated Prompts. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4222–4235,\nOnline. Association for Computational Linguistics.\nYusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan,\nYankai Lin, Zhiyuan Liu, Peng Li, Juanzi Li, Lei Hou,\nMaosong Sun, and Jie Zhou. 2021. On transferability\nof prompt tuning for natural language understanding.\nDerek Tam, Rakesh R Menon, Mohit Bansal, Shashank\nSrivastava, and Colin Raffel. 2021. Improving and\nsimplifying pattern exploiting training.\nTu Vu, Brian Lester, Noah Constant, Rami Al-Rfou’,\nand Daniel Cer. 2022. SPoT: Better frozen model\nadaptation through soft prompt transfer. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 5039–5059, Dublin, Ireland. Association\nfor Computational Linguistics.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems. In Advances in Neural Information\nProcessing Systems, volume 32. Curran Associates,\nInc.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498, On-\nline. Association for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In Advances in Neural Infor-\nmation Processing Systems, volume 32. Curran Asso-\nciates, Inc.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. CoRR, abs/1509.01626.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [mask]: Learning vs. learning to\nrecall.\nA Appendix\nA.1 Dataset\nThe data statistics along with number of examples\nand labels is shown in Table 3.\nDataset # train # val # test # labels\nIMDB 40,000 5,000 5,000 2\nAG’s News 102,000 18,000 7,600 4\nYahoo 1,300,000 60,000 100,000 10\nYelp-5 520,000 130,000 50,000 5\nBoolq 9,427 3,270 3,245 3\nCB 250 57 250 3\nRTE 2,500 278 300 2\nTable 3: Data statistics\nIMDB: It is a binary sentiment analysis dataset\nthat has a movie review and its respective label.\nThe dataset consist of 2 labels - “positive“ and\n“negative“.\nAG’s News: A news classification dataset where\nevery example consists of a headline h, a text body\nb, and a label associated with it. The dataset con-\nsist of 4 labels - “World“, “Sports“, “Business“,\nand “Science/Tech“. For our implementation, we\nconcatenate the headline and body - [h : b] and use\nthe concatenated text for the prediction.\nYahoo Answers: A topic classification dataset\nthat consists of a question q, an answer a, and a\nlabel associated with it. The dataset consists of 10\nlabels - “Society“, “Science“, “Health“, “Educa-\ntion“, “Computer“, “Sports“, “Business“, “En-\ntertainment“, “Relationship“, and “Politics“. For\nour implementation, we concatenate the question\nand answer - [q : a] and use the concatenated text\nfor the prediction.\nYelp-5 A review classification dataset where ev-\nery example consists of a review and a label associ-\nated with it. The dataset consists of 5 labels in the\nform of numbers - (1 to 5).\nBoolq Boolq is a question answering dataset for\nyes/no questions from the SuperGLUE benchmark.\nEach example is a triplet of (question, passage, and\n127\n(a) Varying adapter size\n (b) Varying # of prompt tokens\nFigure 3: Test F-scores on the classification datasets using RoBERTa with ADEPT model In (a), number of prompt\ntokens is kept fixed (m) = 20and adapter hidden size is varied. In (b), the adapter hidden size is kept fixed (d) = 8\nand number of prompt tokens is varied.\nanswer). For our implementation, we concatenate\nthe question and passage - [q : p] and use the\nconcatenated text for the prediction. The dataset\nconsist of 2 labels - “yes“ and “no“\nCB CommitmentBank (CB) is a three-class tex-\ntual entailment dataset from the SuperGLUE bench-\nmark. Each example is a triplet of (premise, hy-\npothesis, and label). For our implementation, we\nconcatenate the premise and hypothesis - [p : h]\nand use the concatenated text for the prediction.\nThe dataset consist of 3 labels - “contradiction“,\n“neutral“, and “entailment“.\nRTE Recognizing Textual Entailment (RTE) is\na binary textual entailment dataset from the Su-\nperGLUE benchmark. Each example is a triplet of\n(premise, hypothesis, and label). For our implemen-\ntation, we concatenate the premise and hypothesis -\n[p : h] and use the concatenated text for the predic-\ntion. The dataset consist of 2 labels - “entailment“\nand “not_entailment“.\nA.2 Implementation details\nWe use roberta-base and bert-base-cased from\nHugging Face3. For our experiment, the adapter\nmodule is inserted between the 8th encoder layer\nand 9th encoder layer of both models. We run\neach experiment for 3 trials and the average re-\nsult is reported. The learning rate is 8e − 4\nfor ADEPT, prompt-tuning (PT), prompt-tuning\n+ head (PHT), head-tuning (HT), and 2e − 5 for\nfine-tuning method. The batch size is 16 for all\nthe methods. We train fine-tuning method for 10\n3https://huggingface.co/\nepochs, the ADEPT model for 15 epochs, prompt-\ntuning + head and head-tuning for 20 epochs, and\nprompt-tuning for 25 epochs. We use AdamW op-\ntimizer and linear scheduler with 6% warmup steps\nfor all the methods. For the ADEPT model, we\nuse an adapter hidden size of 8 and the number\nof prompt tokens is 20 for ADEPT, PT, and PHT\nmethods.\nA.3 Ablation Study\nWe analyze RoBERTa with the ADEPT model to\nshow the impact of adapter hidden size and number\nof prompt tokens. We conduct two sets of exper-\niments Figure 3a - for different adapter sizes and\nFigure 3b - for different prompt tokens.\nAdapter Hidden Size: We train the ADEPT\nmodel for varying adapter sizes - (4, 8, 32, 64, 128),\nand the number of prompt tokens is kept as 20. As\nFigure 3a shows, the F-score is slightly improved\nas we increase the adapter size. Simpler data like\nIMDB do not benefit much from the increase in\nadapter size. Increasing adapter size beyond 32\nbrings 2 to 3% improvement for Yahoo and Yelp-5\ndataset\nNumber of Prompt Tokens: We train the\nADEPT model by varying the number of prompt\ntokens - (10, 20, 50, 100, 200) and keep the adapter\nhidden size fixed as 8. When the number of prompt\ntokens is beyond 50, the performance is decreased,\nindicating that adding trainable parameters in the\ninput does not help much beyond a certain point.\n128"
}