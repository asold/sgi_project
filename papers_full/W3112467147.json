{
  "title": "Audio Captioning using Pre-Trained Large-Scale Language Model Guided by Audio-based Similar Caption Retrieval",
  "url": "https://openalex.org/W3112467147",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2595854816",
      "name": "Koizumi, Yuma",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2591718619",
      "name": "Ohishi, Yasunori",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224379713",
      "name": "Niizumi, Daisuke",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2562394231",
      "name": "Takeuchi, Daiki",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2601353469",
      "name": "Yasuda, Masahiro",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2939574508",
    "https://openalex.org/W2982669287",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2593116425",
    "https://openalex.org/W3099206234",
    "https://openalex.org/W3097791920",
    "https://openalex.org/W1975517671",
    "https://openalex.org/W2996403597",
    "https://openalex.org/W3015700860",
    "https://openalex.org/W2907225497",
    "https://openalex.org/W3015591594",
    "https://openalex.org/W3121952123",
    "https://openalex.org/W1533561824",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3035312337",
    "https://openalex.org/W2964213897",
    "https://openalex.org/W3015300171",
    "https://openalex.org/W2945761034",
    "https://openalex.org/W3038899388",
    "https://openalex.org/W2103235956",
    "https://openalex.org/W2916103538",
    "https://openalex.org/W2130942839"
  ],
  "abstract": "The goal of audio captioning is to translate input audio into its description using natural language. One of the problems in audio captioning is the lack of training data due to the difficulty in collecting audio-caption pairs by crawling the web. In this study, to overcome this problem, we propose to use a pre-trained large-scale language model. Since an audio input cannot be directly inputted into such a language model, we utilize guidance captions retrieved from a training dataset based on similarities that may exist in different audio. Then, the caption of the audio input is generated by using a pre-trained language model while referring to the guidance captions. Experimental results show that (i) the proposed method has succeeded to use a pre-trained language model for audio captioning, and (ii) the oracle performance of the pre-trained model-based caption generator was clearly better than that of the conventional method trained from scratch.",
  "full_text": "AUDIO CAPTIONING USING PRE-TRAINED LARGE-SCALE LANGUAGE MODEL\nGUIDED BY AUDIO-BASED SIMILAR CAPTION RETRIEV AL\nYuma Koizumi, Yasunori Ohishi, Daisuke Niizumi, Daiki Takeuchi, and Masahiro Yasuda\nNTT Corporation, Tokyo, Japan\nABSTRACT\nThe goal of audio captioning is to translate input audio into its de-\nscription using natural language. One of the problems in audio cap-\ntioning is the lack of training data due to the difﬁculty in collecting\naudio-caption pairs by crawling the web. In this study, to overcome\nthis problem, we propose to use a pre-trained large-scale language\nmodel. Since an audio input cannot be directly inputted into such a\nlanguage model, we utilize guidance captions retrieved from a train-\ning dataset based on similarities that may exist in different audio.\nThen, the caption of the audio input is generated by using a pre-\ntrained language model while referring to the guidance captions. Ex-\nperimental results show that (i) the proposed method has succeeded\nto use a pre-trained language model for audio captioning, and (ii) the\noracle performance of the pre-trained model-based caption generator\nwas clearly better than that of the conventional method trained from\nscratch.\nIndex Terms— Audio captioning, pre-trained language model,\nGPT-2, and crossmodal retrieval.\n1. INTRODUCTION\nAudio captioning is a crossmodal translation task when translating\ninput audio into its description using natural language [1–8]. This\ntask potentially enhances the level of sound environment under-\nstanding from merely tagging events [9, 10] (e.g. alarm), scenes [11]\n(e.g. kitchen) and conditions [12] (e.g. normal/anomaly) to higher\ncontextual information including concepts and/or high-level knowl-\nedge. For instance, a smart speaker with an audio captioning system\nwill be able to output “ a digital alarm in the kitchen has gone off\nthree times,” and might be the ﬁrst step for giving us more intelligent\nrecommendations such as “turn the gas range off.”\nOne of the challenges in audio captioning is to address the lack\nof training data [8]. Typical datasets in audio captioning, Audio-\nCaps [4] and Clotho [5], contain only 49,838 and 14,465 training\ncaptions, respectively, whereas there are 36M training sentence-pairs\nin the WMT 2014 English-French dataset for machine translation. It\nis due to the difﬁculty in collecting audio and the corresponding cap-\ntions by crawling the web.\nGenerally, to overcome such a problem, task-speciﬁc ﬁne-tuning\nof a pre-trained model is a promising strategy. In audio event de-\ntection and scene classiﬁcation, several pre-trained models, such as\nVGGish [13], L3-Net [14], and COALA [15], have been published\nto achieve better results with less training data. Similarly, in NLP,\nlarge-scale pre-trained language models, such as the Bidirectional\nEncoder Representations from Transformers (BERT) [16] and Gen-\nerative Pre-trained Transformer (GPT) [17,18], also improve the per-\nformance of various tasks.\nUnfortunately, it is difﬁcult to straightforwardly adopt such pow-\nerful pre-trained language models for audio captioning. This is be-\ncause such models assume that the input is also a word sequence,\nTraining data\nInput\nAudio similarity \ncalculation\nCaption\nAudio\nBirds are chirping \nin the backgroundBirds are chirping \nin the backgroundBirds are chirping \nin the backgroundBirds are chirping \nin the backgroundBirds are chirping \nin the background\nGPT-2\nToken\nprediction\nGPT-2\n<BOS>\nModule 1: Guidance caption retrieval\nAttention \nblock\nModule 2: Caption generation\nGuidance caption\nFig. 1. Overview of the proposed audio captioning system. The or-\nange blocks have trainable blocks and the blue blocks denote frozen\npre-trained models.\nnot an audio sequence. To beneﬁt from such pre-trained language\nmodels in crossmodal translation tasks such as audio captioning, we\nstate a research question as ”how can another modal information be\ninputted to a pre-trained language model?”\nThis paper is the ﬁrst study to utilize a large-scale pre-trained\nlanguage model for the audio caption task. The proposed method\nis a cascaded system consisting of two modules as shown in Fig. 1.\nThe ﬁrst module works like an encoder; this module outputs guid-\nance captions for the next module in which these captions are re-\ntrieved from the training dataset based on the similarity between the\ninput audio and training samples. The second module works like a\ndecoder; this module generates the caption of the input audio using a\npre-trained language model while referring to the retrieved captions.\nThis strategy enables us to avoid directly inputting the audio to the\npre-trained language model, and to utilize it for audio captioning.\n2. PRELIMINARIES OF AUDIO CAPTIONING\nAudio captioning is a task to translate an input audio sequence Φ =\n(φ1,..., φT) into a (sub-)word token sequence (w1,...,w N). Here,\nφt ∈RDa is a set of acoustic features at time index t, and T is the\nlength of the input sequence. Each element of the output wn ∈N\ndenotes the n-th token’s index in the word vocabulary, andN is the\nlength of the output sequence.\nPrevious studies addressed audio captioning through the Encoder-\nDecoder framework such as sequence-to-sequence models (seq2seq)\n[8, 19, 20] and/or Transformer [7, 21]. First, the encoder embeds Φ\ninto a feature-space as ν. Then, the decoder predicts the posterior\nprobability of the n-th token while using conditioning information\nνand 1st to (n−1)-th outputs recursively as\nν= Enc (Φ) , (1)\np(wn|Φ,wn−1) = Dec (ν,wn−1) , (2)\narXiv:2012.07331v1  [eess.AS]  14 Dec 2020\nVGGish\nEmbedding\nnetwork\nAudio Caption\nBERTscore\nCaption\nTraining data\nSimilar Not similar\nCaption\nAnchor Similar Not similar\nTriplet loss\nTransformer\nencoder\nReshape\nNormalize\nEmbedding network\n(a) Similar/not-similar labeling (b) Guidance caption retrieval training\nAudio\nCaption\nCaption\nCaption\nCaption\nCaption\nCaption\nCaption\nCaption\nCaption\n Caption\nFig. 2. Training procedure of audio-based guidance caption retrieval.\nwhere wn−1 = (w1,...,w n−1), and wn is estimated from the\nposterior. To improve the word prediction accuracy, some studies\nestimate additional information such as keywords [4, 7], and pass\nit to the decoder as conditioning information. Then, (2) becomes\np(wn|Φ,wn−1) = Decθd (ν,c,wn−1) where cis the additional\ncondioning information.\nThe decoder is an autoregressive language model for generat-\ning a caption while conditioned by Φ. It is known that the sentence\ngeneration accuracy can be dramatically improved by using an au-\ntoregressive pre-trained language model such as GPT [17,18]. How-\never, previous audio captioning studies did not use them and trained\nthe decoder from scratch [1–8]. This is because such a pre-trained\nlanguage model estimates p(wn|wn−1), that is, it is not able to con-\ndition the posterior by using other modal’s information such asν.\n3. PROPOSED METHOD\n3.1. Basic strategy\nWe construct an encoder to convert an input audio sequence to a\ntoken sequence, and use it for conditioning a pre-trained language\nmodel. That is, in contrast to the conventional audio caption frame-\nwork such as (1) and (2), the encoder outputs a reference token se-\nquence wref = (wref\n1 ,...,w ref\nM) and the decoder predicts the posterior\nwhile using wref as conditioning information as\nwref = Enc (Φ) , (3)\np(wn|Φ,wn−1) = Dec\n(\nwn−1,wref\n)\n, (4)\nwhere M is the length of wref, and a part of the decoder consists of\na pre-trained language model.\nHere, the encoder does not necessarily need to “generate” a to-\nken sequence, rather only needs to output token sentences that are\nuseful for the decoder conditioning. Based on this idea, we con-\nstruct the encoder to retrieve appropriated (similar) captions from\ngiven training dataset based on audio similarity as shown in Fig. 1.\nHereafter, we call such retrieved captions as “guidance captions”. In\nthe following sections, we describe the “encoder” step in Sec. 3.2\nand the “decoder” step in Sec. 3.3.\n3.2. Audio-based guidance caption retrieval\nThe goal of the “encoder” step is to retrieve guidance captions from\nthe training dataset based on audio similarity. This audio similarity\nshould take high value when the two captions are similar even if its\ncorresponding audios are not similar. In order to achieve this require-\nment, the training of this step consists of (i) deﬁning the sentence\nsimilarity between captions in the training dataset, and (ii) training a\nDNN to predict the similarity from the audio.\nAs the similarity between two captions, we use BERTScore [22]\nas shown in Fig. 2 (a), because the BERTScore correlates good with\nhuman judgments in evaluation for image captioning. The detailed\nprocedure is followings:\n1. Calculating the BERTScore between all the possible caption\npairs in the training dataset.\n2. Normalizing the BERTScore so that the maximum and mini-\nmum values are 1 and 0, respectively.\n3. Labeling captions whose normalized BERTScore is larger\nthan a pre-deﬁned threshold as “similar”, and “not similar”\nfor others. We used 0.7 as the threshold.\nThe procedure of guidance caption retrieval is followings. First,\nthe audio input in the time-domainxis converted into the embedded\nspace using a DNN, and then the distances between the input and\nall audio samples in the training dataset are calculated. The DNN\nconsists of freezed VGGish [13] and a trainable embedding network\nas shown in Fig. 2 (b). First, xis converted to its feature sequence\nΦ as\nΦ = VGGish(x), (5)\nwhere Φ ∈RDa×T and Da = 128 in VGGish. Then, Φ is con-\nverted to a ﬁxed dimension vector eas\ne= Embed(Φ). (6)\nThe trainable layer of Embed(·) is only one Transformer-encoder\nlayer which consists of a multi-head self-attention layer and a feed-\nforward block. After the Transformer-encoder layer, we use a re-\nshape function RDa×T →RDaT. Finally, we normalize eso that\n|e|= 1. In this study, we assume the length of the time-domain\ninput is always the same, thus the dimension of DaT of the input\nand all training samples is also always the same. To retrieve guid-\nance captions, we use ℓ2 distances D(a,b) =∥a−b∥2\n2 between the\nembedded features of the input audio and all the audio in the train-\ning dataset. Then, this module outputs the top K = 5captions with\nsmaller distances as wref.\nTo train the embedding network, we adopted the triplet-based\ntraining [23] according to the success of this strategy in crossmodal\nretrieval [24, 25]. The network is trained to minimize the triplet\nloss [23] as\nL= max (0,D(ea,ep) −D(ea,en) +α) , (7)\nwhere α= 0.3 is a margin parameter,ea, ep, and enare the embed-\ndings of the anchor, positive, and negative samples, respectively. In\nthis study, the anchor is the input audio and the positive is a randomly\nselected audio in training data which labeled as “similar” to the an-\nchor in accordance with the above BERTScore-based similarity. As\nthe negative sample, we selected a semi-hard negative sample [26]\nfrom “not similar” samples which satisfy with\nD(ea,ep) ≤D(ea,en) <D(ea,ep) +α. (8)\n3.3. Caption generation using GPT-2 decoder\nAs the pre-trained model in the decoder, we use GPT-2 [18] because\nof its powerful sentence generation capability. In addition, GPT-2\n<BOS>Siml. captionSiml. captionSiml. captionSiml. caption\nGPT-2GPT-2\nToken prediction\nConcat. Concat.\nInput\nSiml. caption\nLinear\nOutput\nMHA\nMHA\n+\nLinear\naud. refs hyps\nLinear\nAttention block\nVGGish\nAttention\nblock\n refs\nLinear( \u0000 )  hyps\nn \u0000 1\n n \u0000 1 + ⌥ n \u0000 1\nFig. 3. Flowchart of caption generation using GPT-2. MHA denotes\nmulti-head attention layer.\nis easy to use as a part of the decoder because it is an autoregres-\nsive language model. The most intuitive and simplest way is that\nGPT-2 predicts the next sentence of concatenated K guidance cap-\ntions. However, this strategy does not work well. The reason is that\nsince guidance captions are independent K sentences, the concate-\nnated guidance captions will seem to be a strange document. Such\nstrange documents might confuse GPT-2 that trained using natural\ndocuments, and be a cause to generate strange caption.\nFigure 3 shows the ﬂowchart of caption generation. We use\nGPT-2 without the ﬁnal token prediction layer as a feature extrac-\ntor. That is, the concatenated guidance captions and target captions\npassed to the frozen GPT-2 independently. Then, we integrate them\nusing trainable multi-head attention layers. In more detail, we ﬁrst\npass wref and wn−1 to GPT-2 independently as\nΨrefs = GPT-2\n(\nwref\n)\n, (9)\nΨhyps\nn−1 = GPT-2 (wn−1) , (10)\nwhere Ψrefs ∈RDl×M and Ψhyps\nn−1 ∈RDl×(n−1), here Dl = 768in\nthe case of the standard GPT-2 (a.k.a. 117M). Then, these matrices\nare integrated by using a multi-head attention layer as\nΨn−1 = MultiHeadAttention\n(\nΨhyps\nn−1,Ψrefs\n)\n, (11)\nwhere MultiHeadAttention(a,b) uses a as query and b as key\nand value, and Ψn−1 ∈RDl×(n−1). In addition, we also integrate\nthe audio-based feature into Ψn−1 using the a multi-head attention\nlayer. In order to reduce the number of parameters, the dimension of\nboth Ψn−1 and Φ are reduced to Dr = 60 using Linear layers as\nΨ′\nn−1 and Φ′, then passed them to the multi-head attention layer as\nΥn−1 = Linear\n(\nMultiHeadAttention\n(\nΨ′\nn−1,Φ′))\n, (12)\nwhere Linear(·) adjusts the dimension to RDl . Finally, the inte-\ngrated feature is passed to the pre-trained token prediction layer\nLMHead(·) to obtain the posterior as\np(wn|Φ,wn−1) = LMHead (Ψn−1 + Υn−1) . (13)\nNote that LMHead(·) is also trainable to ﬁt the decoder for statistics\nof the words available in training captions.\nIn the training phase, we used randomly selected and or-\ndered K = 5 captions from “similar” labeled captions in the\ntraining dataset as wref. The loss function was the smoothed\ncross-entropy with teacher forcing, that is, we calculated poste-\nrior p(wn|Φ,wn−1) using the ground-truth wn−1 and minimize\nthe cross-entropy between the true n-th word wn and the posterior\nwith label smoothing parameter λ= 0.1.\n4. EXPERIMENTS\n4.1. Experimental setup\nDataset and metrics: We evaluated the proposed method on the\nAudioCaps dataset [4], which consists of audio clips from the Au-\ndioSet [27] and their captions. We used the standard split of this\ndataset; 49838, 495, and 975 audio clips with their captions are\nprovided as training, validation, and testing samples, respectively.\nWe evaluated the proposed method on the same metrics used in the\ndataset paper [4], i.e., BLEU-1, BLEU-2, BLEU-3, BLEU-4, ME-\nTEOR, CIDEr, ROUGE-L, and SPICE.\nComparison methods:We compared the proposed method with the\nsystems described in the dataset paper [4] on three scopes; (i) the\nsystem accuracy, (ii) the caption retrieval accuracy, and (iii) the per-\nformance upper-bound.\nScope (i): We investigated whether the proposed method en-\nables to generate captions while using a pre-trained language model\nby comparing it to the state-of-the-art system [4]. As the compari-\nson method, we usedTopDown-AlignedAtt(1NN) which is the\nbest system proposed in the dataset paper [4]. This system retrieved\nthe nearest training audio from a subset of AudioSet and transfered\nits labels as conditioning information of the decoder.\nScope (ii): We evaluated the accuracy of guidance caption\nretrieval. The proposed method is the 1-nearest search with the\ntriplet-based embeddings e (Triplet-Retrieval), whereas\nthe conventional method is that with raw VGGish embeddings Φ\n(1NN-VGGish). Both methods ﬁnd the closest training audio using\nthe ℓ2 distance on the embedded features and return its caption as a\nprediction. As a reference, we also evaluated the scores of the best\nBERTScore caption as Top1-BERTScore.\nScope (iii): We evaluated the performance upper-bound of the\nproposed method through the accuracy when the estimation accuracy\nof the audio-based guidance caption retrieval is perfect. We used\ntop-Kcaption in BERTScores aswref. The comparison method was\nAlignedAtt(GT)-VGGish-LSTM which the ground-truth Au-\ndioSet labels as the conditioning feature of the decoder. As a refer-\nence, we also listed the scores of human’s captions asHuman which\nis the cross validation on the ﬁve ground-truth captions [4].\nTraining details (guidance caption retrieval):All trainable param-\neters were initialized using a random number from N(0,0.02) [28].\nWe used dropout before the Transformer-encoder layer with prob-\nability 0.3, and the AdaBound optimizer [29] with the initial and\nﬁnal learning rate were 1e-4 and 0.1, respectively. We train for 200\nepochs on minibatches of 128 randomly sampled, and the best vali-\ndation model was used as the ﬁnal output.\nTraining details (caption generation):All captions were tokenized\nusing the word tokenizer of GPT-2, thus the vocabulary size was\n50,257. All trainable parameters were initialized using a random\nnumber from N(0,0.02) [28]. We used dropout after both multi-\nhead attention layers with probability 0.3. We used the Adam opti-\nmizer [30] with β1 = 0.9, β2 = 0.999, and ϵ= 10−8 and varied the\nTable 1. Experimental results on AudioCaps dataset\nScope Method B-1 B-2 B-3 B-4 METEOR CIDEr ROUGE-L SPICE\n(i) TopDown-AlignedAtt(1NN) [4] 61.4 44.6 31.7 21.9 20.3 59.3 45.0 14.4\nOurs 63.8 45.8 31.8 20.4 19.9 50.3 43.4 13.9\n(ii)\n1NN-VGGish [4] 44.2 26.5 15.8 9.0 15.1 25.2 31.2 9.2\nTriplet-Retrieval 47.5 28.8 17.7 10.4 16.2 30.6 33.1 10.9\nTop1-BERTScore 74.0 59.9 48.2 38.1 29.1 95.2 59.2 18.6\n(iii)\nAlignedAtt(GT)-VGGish-LSTM [4] 69.1 52.3 36.4 26.1 23.6 77.7 49.6 17.2\nOurs (GT) 75.1 60.4 48.0 37.5 26.8 86.1 56.3 17.9\nHuman 65.4 48.9 37.3 29.1 28.8 91.3 49.6 21.6\nlearning rate was annealed using a cosine schedule where the period\nwas 20 epochs and the maximum and minimum values were 1e-4\nand 1e-6, respectively. We train for 200 epochs on minibatches of\n512 randomly sampled, and the best validation model was used as\nthe ﬁnal output. We used the beam-search decoding for generating\ntokens from posteriors where the beamsize was 4.\n4.2. Results\nTable 1 shows the evaluation results on the AudioCaps dataset.\nThese results suggest the following:\nScope (i): The proposed method achieved results similar to\nthose of the conventional method which used a carefully designed\nDNN architecture by the authors of the dataset, even though the\nproposed method added only a few additional trainable layers after\nthe pre-trained models. In particular, the proposed method outper-\nformed the conventional method for BLEU-1 and 2 which represent\nthe accuracy of the uni- and bi-grams, and achieved quite close\nscores for BLEU-3 and METEOR which are that of trigram and the\nharmonic mean of unigram precision and recall, respectively. These\nresults suggest that the words in guidance captions can be accurately\nused to generate captions. Meanwhile, the proposed method was\nclearly worse than the conventional method for BLEU-4 and CIDEr\nwhich are affected by higher order (longer) n-grams concordance\nrate. This might be because there is a difference between the higher\norder n-grams of frequently appear in captions and that of the “free-\nstyle” texts that exist on the web and were used in GPT-2 training.\nAs a future work, this problem might be solved by ﬁne tuning GPT-2\nusing training captions.\nScope (ii): The proposed method outperformed the conventional\nmethod for all metrics. The difference between two methods is only\nthe embedding network trained using the triplet-loss. Therefore, it\nis effective to train an additional embedding network so as to de-\ncrease the distance between the embeddings of an audio pair where\nits captions’ BERTScore is high.\nScope (iii): The proposed method clearly outperformed the con-\nventional method for all metrics. In addition, the proposed method\nalso outperformed the scores of human’s caption except three met-\nrics. This result suggests that adopting a large-scale pre-trained lan-\nguage model to audio captioning has the potential to greatly increase\nthe accuracy of caption generation.\nBy considering the results of all scopes simultaneously, the\nblock with the greatest promise of improved accuracy is audio-\nbased guidance caption retrieval. In scope (ii), there is a huge\ndifference in scores between the proposed method and top-1 caption\nof BERTScore, and there is also a large difference in scores between\nOurs and Ours (GT). These results suggest that there is a sig-\nniﬁcant room for improvement in the performance of audio-based\nguidance caption retrieval, and these improvements would improve\nthe performance of caption generation using a large-scale pre-trained\nlanguage model. Thus, in the future, we will improve the network\narchitecture and the training methods to achieve higher accuracy in\ncaption retrieval.\nWe show an example of ﬁve ground-truth captions, top-5\nBERTScore captions, K = 5 retrieved guidance captions, and the\ngenerated caption. Audio-based guidance caption retrieval success-\nfully searched “engine” and “honk”, however, incorrectly searched\n“vehicle” instead of “bus”, that results in “vehicle” became the\nsubject of the generated caption.\nGround truth: An engine rumbles loudly, then an air horn honk\nthree times. Humming of an engine followed by some honks of a\nhorn. A bus engine running followed by a bus horn honking. A bus\nengine accelerating followed by a bus horn honking while plastic\nclacks. A bus engine running followed by a vehicle horn honking.\nBERTScore top-5: A vehicle running followed by a horn honk-\ning. A bus engine running followed by a horn honking and a person\nlaughing. A mufﬂed bus engine running followed by a vehicle horn\nhonking then a kid laughing. An engine running followed by a loud\nhorn honking. A man talking followed by a vehicle horn honking.\nGuidance captions: Trafﬁc ﬂows, brakes squeak, a car horn is\nhonked. A vehicle engine running as a loud horn honk followed by\na softer horn honking. Vehicle running and honking horn. Some ve-\nhicles move and horn is triggered. Humming of an engine followed\nby honking of a car horn.\nGenerated caption: A vehicle engine running followed by a horn\nhonking.\n5. CONCLUSIONS\nIn this study, we examined the use of a pre-trained large-scale lan-\nguage model in audio captioning. In order to overcome the lack of\nthe amount of training data problem, we used GPT-2 as a part of the\ncaption generation, i.e. decoder. The proposed method consisted of\ntwo modules. The ﬁrst module retrieved guidance captions for the\ndecoder based on audio similarity. The second module generated a\ncaption using GPT-2 while referring to the retrieved captions. Exper-\nimental results showed that (i) the proposed method had succeeded\nto use a pre-trained language model for audio captioning, and (ii)\nthe oracle performance of the pre-trained model-based caption gen-\nerator was clearly superior to the conventional method trained from\nscratch. Thus, we conclude that the use of a pre-trained language\nmodels is a promising strategy with room for improving the caption\ngeneration accuracy.\n6. REFERENCES\n[1] K. Drossos, S. Adavanne, and T. Virtanen, “Automated Au-\ndio Captioning with Recurrent Neural Networks,” in Proc.\nIEEE Workshop Appl. Signal Process. Audio Acoust. (WAS-\nPAA), 2017.\n[2] S. Ikawa and K. Kashino, “Neural Audio Captioning based on\nConditional Sequence-to-Sequence Model,” in Proc. Detect.\nClassif. Acoust. Scenes Events (DCASE) Workshop, 2019.\n[3] M. Wu, H. Dinkel, and K. Yu, “Audio Caption: Listen\nand Tell,” in Proc. Int. Conf. Acoust. Speech Signal Process.\n(ICASSP), 2019.\n[4] C. D. Kim, B. Kim, H. Lee, and G. Kim, “AudioCaps: Generat-\ning Captions for Audios in The Wild,” inProc. N. Am. Chapter\nAssoc. Comput. Linguist.: Hum. Lang. Tech. (NAACL-HLT),\n2019.\n[5] K. Drossos, S. Lipping, and T. Virtanen, “Clotho: An Audio\nCaptioning Dataset,” in Proc. Int. Conf. Acoust. Speech Signal\nProcess. (ICASSP), 2020.\n[6] Y . Koizumi, D. Takeuchi, Y . Ohishi, N. Harada, and K.\nKashino, “The NTT DCASE2020 Challenge Task 6 System:\nAutomated Audio Captioning with Keywords and Sentence\nLength Estimation,” in Tech. Rep. Detect. Classif. Acoust,\nScenes Events Chall., 2020.\n[7] Y . Koizumi, R. Masumura, K. Nishida, M. Yasuda, and S.\nSaito, “A Transformer-based Audio Captioning Model with\nKeyword Estimation,” inProc. Interspeech,2020.\n[8] D. Takeuchi, Y . Koizumi, Y . Ohishi, N. Harada, and K.\nKashino, “Effects of Word-frequency based Pre- and Post-\nProcessings,” in Proc. Detect. Classif. Acoust. Scenes Events\n(DCASE) Workshop, 2020.\n[9] A. Mesaros, T. Heittola, A. Eronen, and T. Virtanen, “Acous-\ntic Event Detection in Real Life Recordings,” in Proc. Euro.\nSignal Process. Conf. (EUSIPCO), 2010.\n[10] K. Imoto, N. Tonami, Y . Koizumi, M. Yasuda, R. Yaman-\nishi, and Y . Yamashita, “Sound Event Detection By Multitask\nLearning of Sound Events and Scenes with Soft Scene Labels,”\nin Proc. Int. Conf. Acoust. Speech Signal Process. (ICASSP),\n2020.\n[11] D. Barchiesi, D. Giannoulis, D. Stowell, and M. D. Plumb-\nley, “Acoustic Scene Classiﬁcation: Classifying Environments\nfrom the Sounds they Produce,” IEEE Signal Process. Mag.,\n2015.\n[12] Y . Koizumi, S. Saito, H. Uematsu, Y . Kawachi, and N. Harada,\n“Unsupervised Detection of Anomalous Sound based on Deep\nLearning and the Neyman-Pearson Lemma,”IEEE/ACM Tran.\nAudio, Speech, and Lang. Process., 2019.\n[13] S. Hershey, S. Chaudhuri, D. P. W. Ellis, J. F. Gemmeke, A.\nJansen, R. C. Moore, M. Plakal, DvPlatt, R. A. Saurous, B.\nSeybold, M. Slaney, R. Weiss, and K. Wilson, “CNN Architec-\ntures for LargeScale Audio Classiﬁcation,” in Proc. Int. Conf.\nAcoust. Speech Signal Process. (ICASSP), 2017.\n[14] J. Cramer, H.-H. Wu, J. Salamon, and J. P. Bello, “Look, Lis-\nten and Learn More: Design Choices for Deep Audio Em-\nbeddings,” in Proc. Int. Conf. Acoust. Speech Signal Process.\n(ICASSP), 2019.\n[15] X. Favory, K. Drossos, T. Virtanen, and X. Serra “COALA:\nCo-Aligned Autoencoders for Learning Semantically En-\nriched Audio Representations,” inWorkshop Self-superv. Audio\nSpeech at Int. Conf. Mach. Learn. (ICML), 2020.\n[16] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT:\nPre-training of Deep Bidirectional Transformers for Language\nUnderstanding,” in Proc. N. Am. Chapter Assoc. Comput. Lin-\nguist.: Hum. Lang. Tech. (NAACL-HLT), 2019.\n[17] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever,\n“Improving Language Understanding with Unsupervised\nLearning,”Tech. rep., OpenAI,2018.\n[18] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and\nI. Sutskever. “Language models are unsupervised multitask\nlearners,”Tech. rep., OpenAI,2019.\n[19] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to Sequence\nLearning with Neural Networks,” inProc. Adv. Neural Inf. Pro-\ncess. Syst. (NIPS),2014.\n[20] M. T. Luong, H. Pham, and C. D. Manning “Effective Ap-\nproaches to Attention-based Neural Machine Translation,” in\nProc. Empir. Methods Nat. Lang. Process. (EMNLP),2015.\n[21] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.\nN. Gomez, L. Kaiser, and I. Polosukhin, “Attention Is All You\nNeed,” inProc. Adv. Neural Inf. Process. Syst. (NIPS), 2017.\n[22] T. Zhang, V . Kishore, F. Wu, K. Q. Weinberger, and Y . Artzi,\n“BERTScore: Evaluating Text Generation with BERT,” in\nProc. of Int. Conf. Learn. Representations (ICLR), 2020.\n[23] J. Wang, Y . Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin,\nB. Chen, and Y . Wu. “Learning Fine-grained Image Similarity\nwith Deep Ranking,” in Proc. IEEE Int. Conf. Comput. Vis.\nPattern Recognit. (CVPR), 2014.\n[24] Y . Ohishi, A. Kimura, T. Kawanishi, K. Kashino, D. Harwath\nand J. Glass “Trilingual Semantic Embeddings of Visually\nGrounded Speech with Self-attention Mechanisms” in Proc.\nInt. Conf. Acoust. Speech Signal Process. (ICASSP), 2020.\n[25] M. Yasuda, Y . Ohishi, Y . Koizumi, and N. Harada, “rossmodal\nSound Retrieval based on Speciﬁc Target Co-occurrence\nDenoted with Weak Labels,” inProc. Interspeech,2020.\n[26] F. Schroff, D. Kalenichenko, and J. Philbin, “FaceNet: A Uni-\nﬁed Embedding for Face Recognition and Clustering’ in Proc.\nIEEE Int. Conf. on Comput. Vis. Pattern Recognit. (CVPR),\n2016.\n[27] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen, W.\nLawrence, R. C. Moore, M. Plakal, and M. Ritter, “Audio Set:\nAn Ontology and Human-Labeled Dataset for Audio Events,”\nin Proc. Int. Conf. Acoust. Speech Signal Process. (ICASSP),\n2017.\n[28] A. Radford, K. Narasimhan, T. Salimans, and I.\nSutskever, “Improving Language Understanding by Gen-\nerative Pre-Training,” https://blog.openai.com/\nlanguage-unsupervised, 2018.\n[29] L. Luo,Y . Xiong, Y . Liu, and X. Sun, “Adaptive Gradient Meth-\nods with Dynamic Bound of Learning Rate,” inProc. Int. Conf.\nLearn. Representations (ICLR), 2019.\n[30] D. P. Kingma and J. L. Ba, “Adam: A method for stochastic op-\ntimization,” inProc. Int. Conf. Learn. Representations (ICLR),\n2015.",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.9617577791213989
    },
    {
      "name": "Computer science",
      "score": 0.8600670099258423
    },
    {
      "name": "Language model",
      "score": 0.6542046666145325
    },
    {
      "name": "Oracle",
      "score": 0.5283152461051941
    },
    {
      "name": "Natural language processing",
      "score": 0.5228467583656311
    },
    {
      "name": "Speech recognition",
      "score": 0.50654137134552
    },
    {
      "name": "Natural language",
      "score": 0.49132075905799866
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4552953541278839
    },
    {
      "name": "Image (mathematics)",
      "score": 0.10367530584335327
    },
    {
      "name": "Software engineering",
      "score": 0.0
    }
  ]
}