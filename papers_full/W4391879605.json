{
  "title": "Difficulty in chirality recognition for Transformer architectures learning chemical structures from string representations",
  "url": "https://openalex.org/W4391879605",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5004040969",
      "name": "Yasuhiro Yoshikai",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2600683187",
      "name": "Tadahaya Mizuno",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A3149342371",
      "name": "Shumpei Nemoto",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2043024742",
      "name": "Hiroyuki Kusuhara",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A5004040969",
      "name": "Yasuhiro Yoshikai",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2600683187",
      "name": "Tadahaya Mizuno",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A3149342371",
      "name": "Shumpei Nemoto",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2043024742",
      "name": "Hiroyuki Kusuhara",
      "affiliations": [
        "The University of Tokyo"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2790808809",
    "https://openalex.org/W2887896328",
    "https://openalex.org/W2884430236",
    "https://openalex.org/W3204865406",
    "https://openalex.org/W2066150117",
    "https://openalex.org/W2753962198",
    "https://openalex.org/W2592262780",
    "https://openalex.org/W2529996553",
    "https://openalex.org/W2913617083",
    "https://openalex.org/W2901476322",
    "https://openalex.org/W1757990252",
    "https://openalex.org/W2177317049",
    "https://openalex.org/W2900090807",
    "https://openalex.org/W2200017991",
    "https://openalex.org/W1988037271",
    "https://openalex.org/W3085821739",
    "https://openalex.org/W3209056694",
    "https://openalex.org/W4318609278",
    "https://openalex.org/W4206355546",
    "https://openalex.org/W3137903636",
    "https://openalex.org/W3214740101",
    "https://openalex.org/W2970764640",
    "https://openalex.org/W2972608805",
    "https://openalex.org/W2994678679",
    "https://openalex.org/W3088265803",
    "https://openalex.org/W3175633940",
    "https://openalex.org/W6774009640",
    "https://openalex.org/W4385567824",
    "https://openalex.org/W2947722296",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4226159083",
    "https://openalex.org/W2886791556",
    "https://openalex.org/W4365150514",
    "https://openalex.org/W2594183968",
    "https://openalex.org/W2173027866",
    "https://openalex.org/W2016589492",
    "https://openalex.org/W4321616023",
    "https://openalex.org/W6772381481",
    "https://openalex.org/W4312018564",
    "https://openalex.org/W3131648425",
    "https://openalex.org/W2949676527",
    "https://openalex.org/W4323304388",
    "https://openalex.org/W6892464278",
    "https://openalex.org/W3094771832",
    "https://openalex.org/W3098269892",
    "https://openalex.org/W4391879605"
  ],
  "abstract": null,
  "full_text": "Article https://doi.org/10.1038/s41467-024-45102-8\nDifﬁc u l t yi nc h i r a l i t yr e c o g n i t i o nf o r\nTransformer architectures learning chemical\nstructures from string representations\nYasuhiro Yoshikai1,2, Tadahaya Mizuno1,2 , Shumpei Nemoto1 &\nHiroyuki Kusuhara1\nRecent years have seen rapid development of descriptor generation based on\nrepresentation learning ofextremely diverse molecules, especially those that\napply natural language processing (NLP) models to SMILES, a literal repre-\nsentation of molecular structure. However, little research has been done on\nhow these models understand chemicalstructure. To address this black box,\nwe investigated the relationship between the learning progress of SMILES and\nchemical structure using a representative NLP model, the Transformer. We\nshow that while the Transformer learns partial structures of molecules quickly,\nit requires extended training to understand overall structures. Consistently,\nthe accuracy of molecular property predictions using descriptors generated\nfrom models at different learning steps was similar from the beginning to the\nend of training. Furthermore, we foundthat the Transformer requires parti-\ncularly long training to learn chirality and sometimes stagnates with low per-\nformance due to misunderstanding of enantiomers. Theseﬁndings are\nexpected to deepen the understanding of NLP models in chemistry.\nRecent advancements in machine learning have inﬂuenced various\nstudies in chemistry such as molecular property prediction, energy\ncalculation, and structure generation\n1– 6. To utilize machine learning\nmethods in chemistry, weﬁrst need to make computers recognize\nchemical structures. One of the most popular approaches is to use\nchemical language models, which are natural language processing\n(NLP) models fed with strings representing chemical structures such as\nsimpliﬁed molecular input line entry speciﬁcation (SMILES)\n7.I n2 0 1 6 ,\nGó mez-Bombarelli et al. applied a chemical language model using a\nneural network for descriptor generation and created a trend8– 10.I n\nthis approach, a neural NLP model such as a recurrent neural network\n(RNN) learns an extremely wide variety of SMILES from public\ndatabases\n11– 13, converts the string into a low-dimensional vector,\ndecodes it back to the original SMILES, and then the intermediate\nvector is drawn out as a descriptor. The obtained descriptor is superior\nto the conventionalﬁngerprints, such as MACCS keys\n14 and ECFP15,i n\ncontinuous and thus highly expressive natures, and that the original\nstructures can be restored from the descriptor by the decoder16.O n\nthe other hand, the presented approach also has the disadvantage that\nit obscures the process of descriptor generation and that the meanings\nof each value in the descriptor are hard to interpret. It is scarcely\nstudied how chemical language models understand structures of\nextremely diverse molecules and connect chemical structures and\ndescriptors. Related works are described in Supplementary Note 1.\nIn this study, we tackle with addressing this black box by com-\nparing the performance of the model and its descriptor at various\nsteps of training, which clariﬁes what types of molecular features are\neasily incorporated into the descriptor and what types are not. Parti-\ncularly, we focus on the most prevalent NLP model, the Transformer, a\nwell utilized architecture for descriptor generation and other chemical\nlanguage tasks these days\n17– 32.T ob es p e c iﬁc, we train a Transformer\nmodel to translate SMILES strings and then compare perfect agree-\nment and similarity of molecularﬁngerprints between prediction and\ntarget at different training steps. We also conduct 6 molecular\nReceived: 23 March 2023\nAccepted: 11 January 2024\nCheck for updates\n1Laboratory of Molecular Pharmacokinetics, Graduate School of Pharmaceutical Sciences, The University of Tokyo, 7-3-1 Hongo, Bunkyo, Tokyo, Japan. 2These\nauthors contributed equally: Yasuhiro Yoshikai, Tadahaya Mizuno.e-mail: tadahaya@gmail.com\nNature Communications|         (2024) 15:1197 1\n1234567890():,;\n1234567890():,;\nproperty prediction tasks with descriptors generated by models at\ndifferent steps in training and studied what kinds of tasks are easily\nsolved. We furtherﬁnd that the translation accuracy of the Transfor-\nmer sometimes stagnates at a low level for a while and then suddenly\nsurges. To clarify the cause of this, we compare translation accuracy\nfor each character of SMILES. Finally, we search for and found methods\nto prevent stagnation and stabilize learning.\nResults\nPartial/overall structure recognition of the Transformer in\nlearning progress\nTo understand how the Transformer model learns the diverse chemi-\ncal structures, weﬁrst researched the relationship between the learn-\ning procedure and the model performance by comparing the models\nat various training steps. In this study, we trained the Transformer to\npredict canonical SMILES of molecules based on their randomized\nSMILES\n32– 34. For models at various steps of training, we calculated\nperfect accuracy and partial accuracy of predicted SMILES\nexpression35. We supposed perfect accuracy, which evaluates the\ncomplete consistency of target and prediction, represents how much\nthe models understand the connectivity of atoms constituting overall\nmolecular structures, whereas partial accuracy, which measures\nposition-wise accuracy of prediction, indicates recognition of the\nconnectivity of atoms in partial structures. The result showed that\npartial accuracy rapidly converged to 1.0, meaning almost complete\ntranslation, whereas perfect accuracy gradually increased as the\nlearning proceeded (Fig.1a). This result suggests that the Transformer\nmodel recognizes partial structures of molecules at quite an early\nstage of training when overall structures are yet to be understood well.\nTo further evaluate partial and overall recognition of molecules, we\nprepared the models when perfect accuracy surpassed 0.2, 0.5, 0.7,\n0.9, 0.95, and 0.98 and at steps 0, 4000, and 80,000 (end of training).\nFor models at these steps, we computed MACCS keys\n14 and ECFP15\n(radius R = 1, 2, 3) of predicted/target molecules; and calculated the\nTanimoto similarity for each prepared model. As these descriptors are\nwidely accepted to represent typical partial structures of molecules,\ntheir similarity between target and prediction will help the compre-\nhension of the model on partial structures of molecules. As a result, the\nTanimoto similarity of molecularﬁngerprints saturated at nearly 1.0,\nmeaning complete correspondence ofﬁngerprint between prediction\nand target, when perfect accuracy was merely about 0.3 (Fig1a, b). We\nalso compared the Tanimoto similarity with the loss function (Fig.1b),\nand it was shown that theﬁngerprints corresponded almost com-\npletely when the loss had yet to be converged. Fig.1c shows valid\nexamples of predicted molecules with their targets at early phase of\ntraining (step 4000). These results also support the early recognition\nof partial structures and late recognition of the overall structure of\nmolecules by the Transformer model. We previously found that the\nGRU model, derived from NLP, has a similar tendency as thisﬁnding\n35.\nIt is then suggested that NLP models, when trained to chemical\nstructures by learning SMILES, recognize partial structures of mole-\ncules at the early stage of training, regardless of their architecture.\nThese ﬁndings can be explained as follows: When translating rando-\nmized SMILES into canonical SMILES, the model needs to reproduce\nthe numbers of atoms in the molecule with each atomic type and how\nthey are bound together. Assuming that the numbers of atoms are\nFig. 1 | Partial/overall structure recognition of Transformer in learning pro-\ngress. aTemporal change of perfect accuracy and partial accuracy.b Temporal\nchange of Tanimoto similarity between the indicatedﬁngerprints of predicted and\ntarget Simpliﬁed Molecular Input Line Entry System (SMILES), with the loss for\ncomparison. Each gray dot indicates the loss of each batch.c Examples of\npredicted/target molecules at step 4000. Valid SMILES wasﬁltered from SMILES in\nthe test set predicted at step 4000, and theﬁgure shows the randomly sampled\nexamples of valid SMILES. Each of the molecules in the upper row is predicted\ntargeted to the directly below molecule. Source data are provided as a Source\nData ﬁle.\nArticle https://doi.org/10.1038/s41467-024-45102-8\nNature Communications|         (2024) 15:1197 2\neasily learned as simple frequencydistributions, a large part of the\ntraining is spent learning and reproducing the bonds between atoms.\nThe model in the early phase of training can only reproduce easy-to-\nunderstand bonds between atoms (such as those with high frequency\nand few other options), and those partially connected atoms are\nobserved as partial structure. The model in the subsequent phase of\ntraining learns the remaining bonds which are left to be challenging\ntasks for the model but necessary for reconstructing the overall\nstructures.\nWe also investigated what type of substructure is easily or hardly\nunderstood by the model using dimension-wise similarities of MACCS\nkeys, whose details are written in Supplementary Note 2. No remark-\nable tendency was observed in the results, and the similarities of most\nof the dimensions converged rapidly.\nDownstream task performance in the learning progress\nMolecular descriptors are frequently used in solving cheminformatics\ntasks. Therefore, in many cases, the performance of descriptor gen-\neration methods is evaluated by how much downstream tasks, such as\nprediction of molecular properties, are solved from their descriptor.\nOn the other hand, we have shown in a previous study that in the case\nof a descriptor generated by chemical language model based on GRU,\ndownstream task performance is mainly related to the recognition of\npartial structures of molecules\n35, and here we worked on the evaluation\nof the downstream task performance over the learning progress about\nthe Transformer model. To be speciﬁc, we predicted the molecular\nproperties from intermediate representation of molecules during\ntranslation in the Transformer at different steps. The details of\ndescriptor generation and conditions of prediction are described in\nMolecular property prediction Section. Note that to evaluate the per-\nformance of memory expression itself, rather than the inherent\narchitecture of the model, we did not conductﬁne-tuning. We used\nbenchmark datasets from MoleculeNet\n36 summarized in Table1.\nFigure 2 and Supplementary Figs. 1 and 2 show the prediction\nscores of each descriptor (also summarized in Table2). The results\nshowed that descriptors of models at an early phase, or even at the\nbeginning of training, can perform just as well as that of the fully\ntrained model, except for the prediction of Lipophilicity, although the\nscore for this task saturated at an early phase (step 6000). Duvenaud et\nal.\n37 showed that neuralﬁngerprint (NFP), a deep-learning and graph-\nbased descriptor, correlated to ECFP and was able to predict molecular\nproperties without training. Similarly, one of the explanations of the\npresented result is that the Transformer model, even with its initial\nweights, generates a meaningful descriptor by its inherent mechanism\nsuch as self-attention. This implies that the modifying structure of the\nmodel is more helpful for improving the performance than changing\nwhat data the model isﬁne-tuned on. Note that the performance of the\ndescriptor pooled from the Transformer memory is almost similar to\nthat of ECFP and is slightly lower than that of CDDD. One of explana-\ntions of slightly low performance can be that the pooling process in\ndescriptor generation omitted part of the structural information of\nmolecules which is scattered in the whole memory. The potential for\nfurther structural enhancements in prediction, like co-learning of\nmolecular properties, is worth noting although it falls outside the\nscope of this current study.\nStagnation of perfect accuracy in learning chemical structures\nWe experimented with different random seeds to reproduce the\nresults in Partial/overall structure recognition of the Transformer in\nlearning progress. It was then observed that the perfect accuracy of the\nTransformer sometimes stagnated at a low level for a while and then\nabruptly increased at a certain step. We are interested in this phe-\nnomenon and conducted some experiments changing the randomly\ndetermined conditions. To be speciﬁc, we trained the model with 14\ndifferent initial weights and 2 different orders of iteration on the\ntraining dataset. Figure3a shows the perfect accuracy in these differ-\nent conditions. The ﬁgure shows that while perfect accuracy\nuneventfully converges in many cases perfect accuracy sometimes\nstayed at ~0.6 from approximately 10,000 to 70,000 steps and then\nsurged to nearly 1.0 or even maintained a low accuracy until the end of\ntraining. Figure3b shows change of loss in conditions in which stag-\nnation did or did not occur. This shows that the loss sharply decreased\na tt h es a m et i m ea sa c c u r a c ys u r g e d .\nTo specify the determinative factor of the stagnation, we\nobtained the steps when accuracy exceeded 0.7 and 0.95, named\nstep-0.7 and step-0.95 respectively. Based on Fig.3a, we considered\nstep-0.7 to represent the step when stagnation was resolved, and\nstep-0.95 is the step when learning was almost completed. Supple-\nmentary Fig. 3a, b shows the relationship betweenstep-0.7/0.95 of\nthe same seed and different iteration orders. The result shows that\nthe trend of learning progress is similar for different iteration orders\nwhen the same initial weight was used. Supplementary Fig. 3c, d\nshows the averagestep-0.7/0.95 of each iteration order, and no sig-\nniﬁcant difference ofstep-0.7/0.95 was observed. These results sug-\ngest that whether the stagnation occurs or not depends on initial\nweight, rather than iteration order.\nWe replicated the experiments in Sections Partial/overall struc-\nture recognition of the Transformer in learning progress and Down-\nstream task performance in the learning progress for one training in\nwhich stagnation occurred. We investigated the agreement ofﬁnger-\nprints and performance on downstream tasks at different steps of the\nlearning with stagnation. As a result, the tendencies in found in the\nprevious sections was conserved even when stagnation occurred,\nreinforcing ourﬁndings in the previous sections. Details about these\nexperiments are shown in Supplementary Note 3.\nCause of stagnation in learning chemical structures\nWhat is the cause of this stagnation? We investigated the model per-\nformance on each character of SMILES strings using 2 metrics. Theﬁrst\none is the perfect accuracy when each character is masked. This is\ncalculated like perfect accuracy deﬁned in Partial/overall structure\nrecognition of the Transformer in learning progress Section except\nTable 1 | Summary of datasets for downstream tasks\nDataset # Total molecules # Used molecues Splitting Task type Recommended metric\ntask\nESOL 1128 1108 Scaffold Regression RMSE\nFreeSolv 642 628 Random Regression RMSE\nLipophilicity 4200 4190 Scaffold Regression RMSE\nBACE 1513 1471 Scaffold Classi ﬁcation ROC-AUC\nBBBP 2050 1899 Scaffold Classi ﬁcation ROC-AUC\nClinTox CT_TOX 1484 1341 Random Classi ﬁcation ROC-AUC\nFDA_APPROVED 1484 1341 Classi ﬁcation ROC-AUC\nArticle https://doi.org/10.1038/s41467-024-45102-8\nNature Communications|         (2024) 15:1197 3\nthat prediction for a certain type of characters in the target is not\nconsidered. This value is expected to rise when a difﬁcult, or com-\nmonly mistaken character is masked. The second metric is the accu-\nracy of each character of the target when teacher forcing is used. In the\ntest phase, as the model usually predicts each letter of SMILES from a\npreviously predicted string, the model is likely to make an additional\nmistake when it has already made one. This means characters that\nappear more in the later positions (like“)” compared with“(”)t e n dt o\nshow low accuracy. To remedy this, we adopted teacher-forcing\n38 to\npredict the SMILES, meaning the model always predicts each letter\nfrom the correct SMILES string, when computing the accuracy of each\ncharacter.\nFigure 4a shows the transition of masked accuracy about training\nwith or without stagnation. The results show that when the model is in\nstagnation, predictions of“@” and “@@” are wrong by a large number.\nThese 2 characters are used to describe chirality in SMILES repre-\nsentation (Fig.4b). It suggests that stagnation was caused by confusion\nin discriminating enantiomers, and the subsequent surge of perfect\naccuracy was the result of the resolution of this confusion. We also\ninvestigated the ratio of correct predictions, wrong predictions due\nFig. 2 | Performance of descriptors on molecular property prediction. aRMSE\nscore of prediction on ESOL dataset from descriptors of the model at different\nsteps of training, for 4 different ways of pooling. Blue, mean; yellow, latent repre-\nsentation of theﬁrst token; red, concatenation of the indicated 4 aggregation\nmethods; navy, concatenation of the indicated 6 aggregation methods.b AUROC\nscore of prediction on BACE dataset from descriptors of the model at different\nsteps of training for 4 different ways of pooling. Mean, unbiased standard deviation\nand data distribution of experiments for 5 folds split by recommended method in\nDeepChem\n50 are shown as bar height, error bar length and gray dots, respectively.\nThe metrics were determined based on MoleculeNet36. The perfect accuracy at each\nstep is written down on the horizontal axis. Source data are provided as a Source\nData ﬁle. RMSE Root Mean Squared Error, ESOL Estimated Solubility, AUROC Area\nUnder Receiver Operating Characteristic.\nTable 2 | Performance of each descriptor onmolecular property prediction (Summary)\nDescriptor ESOL (RMSE) FreeSolv (RMSE) Lipophilicity\n(RMSE)\nBACE (AUROC) BBBP (AUROC) ClinTox\nSteps CT_TOX (AUROC) FDA_APPROVED\n(AUROC)\nrandom 1.060 ± 0.186 1.023 ± 0.070 1.002 ± 0.026 0.497 ± 0.025 0.482 ± 0.040 0.475 ± 0.038 0.467 ± 0.098\nECFP(R = 2) 0.947 ± 0.199 0.463 ± 0.065 0.749 ± 0.053 0.856± 0.035 0.852 ± 0.035 0.875 ± 0.041 0.834 ± 0.103\nCDDD 0.715 ± 0.272 0.320 ± 0.032 0.677 ± 0.038 0.826 ± 0.032 0.874 ± 0.054 0.895 ± 0.016 0.882 ± 0.041\nUni-Mol 0.456± 0.082 0.295 ± 0.032 0.505 ± 0.053 0.847 ± 0.029 0.861 ± 0.042 0.874 ± 0.048 0.875 ± 0.053\nTransformer 0 0.548 ± 0.065 0.485 ± 0.053 0.897 ± 0.022 0.776 ± 0.037 0.845 ± 0.067 0.859 ± 0.045 0.780 ± 0.047\n4000 0.571 ± 0.151 0.398 ± 0.029 0.821 ± 0.041 0.791 ± 0.029 0.862 ± 0.057 0.899 ± 0.032 0.883 ± 0.051\n6000 0.566 ± 0.111 0.424 ± 0.051 0.775 ± 0.030 0.815 ± 0.006 0.881± 0.032 0.869 ± 0.035 0.862 ± 0.033\n8000 0.579 ± 0.074 0.464 ± 0.068 0.774 ± 0.034 0.821 ± 0.019 0.875 ± 0.045 0.871 ± 0.042 0.868 ± 0.066\n10000 0.570 ± 0.063 0.459 ± 0.065 0.782 ± 0.034 0.829 ± 0.025 0.859 ± 0.064 0.837 ± 0.062 0.819 ± 0.065\n16000 0.567 ± 0.091 0.486 ± 0.064 0.804 ± 0.033 0.823 ± 0.020 0.876 ± 0.047 0.876 ± 0.023 0.804 ± 0.088\n30000 0.581 ± 0.091 0.497 ± 0.104 0.793 ± 0.024 0.825 ± 0.012 0.863 ± 0.082 0.845 ± 0.032 0.799 ± 0.049\n48000 0.594 ± 0.086 0.496 ± 0.047 0.801 ± 0.033 0.824 ± 0.019 0.875 ± 0.043 0.888 ± 0.025 0.850 ± 0.049\n80000 0.585 ± 0.059 0.461 ± 0.058 0.771 ± 0.024 0.835 ± 0.024 0.861 ± 0.062 0.904± 0.024 0.894 ± 0.046\nBold ﬁgures are the best score for each dataset among the models.\nArticle https://doi.org/10.1038/s41467-024-45102-8\nNature Communications|         (2024) 15:1197 4\nsolely to chirality, and wrong predictions due to other reasons. Fig-\nure 4c shows that most of the mistakes in stagnation are due to chir-\nality, which supports the impact of chirality confusion in stagnation.\nIt should be noted that errors occurred in both directions, invol-\nving mistakes of the“@“ token for “@@“, and vice versa (Supple-\nmentary Fig. 4a). Additionally, we conducted an analysis of the\naccumulated count of“@“ and “@@“ tokens in the training set. As\ndepicted in Supplementary Fig. 4b, the occurrence of the“@@“ token\nsurpasses that of the “@“ t o k e ni nt h et a r g e tS M I L E S( c a n o n i c a l\nSMILES), while the difference appears to be marginal. In light of this\nobservation, it is worth noting that stagnation did occur in some cases\neven when we took measures to train the model on a dataset that was\nwell-balanced in terms of “@“ and “@@“ tokens (Supplementary\nNote 4). Theseﬁndings underscore that bias regarding chiral token is\nnot the primary cause of stagnation.\nThe true challenge for a chemical language model lies not in\nmastering the numerous elementary atom-bonding rules, but rather in\ncomprehending the difﬁcult rules that persist even after the basics\nhave been acquired, which is consistent with Ucak’sw o r kr e g a r d i n gt h e\nreconstruction of molecular representations fromﬁngerprints\n39.T h e\nabove ﬁndings indicate that learning chirality, which is the cause of\nstagnation, is the very difﬁcult task for the Transformer model learning\ndiverse chemical structures. Notably, the examination of the accuracy\ntransition for each character not only conﬁrmed that“@“ and “@@“\nshow a slower increase of accuracy than the other characters, but also\nrevealed the difﬁculty of chiral tokens even when learning proceeds\nsmoothly without stagnation (Supplementary Fig. 5). This trend holds\ntrue considering character frequencies (Supplementary Figs. 6 and 7).\nIn addition, the accuracy for chiral tokens is relatively low even when\nthe model reaches a plateau, a perfect accuracy of 0.9. It should be\nnoted that the accuracy increase of“\\” token, related to geometrical\nisomers, was also slow (Supplementary Fig. 5). This suggests that the\ntokens associated with stereochemistry pose a general challenge for\nthe Transformer model in terms of learning, while the inﬂuence of\ngeometrical isomers remains relatively modest, owing to their infre-\nquent occurrence (approximately 1/100th of the chiral tokens).\nThese ﬁndings indicate that chirality of molecules is difﬁcult for\nthe Transformer to understand, and sometimes the model is confused\na b o u ti tf o ral o n gp e r i o d ,c a u s i n gs t a g n a t i o n .\nSolution of stagnation in learning chemical structures\nThen, how can we facilitate the understanding of the Transformer on\nchirality? To answer this question, we applied the following pertur-\nbation to the learning process and evaluated its effect on stagnation.\n1. Increase chirality in training dataset: It is possible that learning\nmore enantiomers encourages the model to understand chirality.\nWe therefore omitted half of the molecules in the training set\nwhose SMILES has neither“@” nor “@@” and trained the model\nwith the data in which chirality appears more frequently.\n2. Introduce AdamW: In deep-learning studies, one of the possible\nexplanations for this kind of stagnation is that the model is stuck\nto a local optimum, and changing the optimizer can be a solution\nto avoid such stagnation. We have been using the Adam optimizer\nbased on Vaswani et al.\n31 so far, but here we tried the AdamW40\noptimizer. The AdamW optimizer is a reﬁned optimizer of Adam\nwith L2 normalization in the loss function. Loshchilov et al.40\nshowed that this optimizer can be adopted to a wider range of\nlearnings than Adam.\n3. He normal initialization: Experiments in 4.3 suggested that stag-\nnation occurs depending on the initial weight of models. Thus,\nchanging the initialization of model weight would stabilize\nlearning. Here we introduced He normal initialization, which is\nFig. 3 | Stagnation of perfect accuracy at different initial weights. a, b Temporal\nchange of perfect accuracy for 14 different seeds and 2 different iteration orders.\nLines with the same color corresponds to trainings from the same initial weight.\nc, d Perfect accuracy in the trainings with/without stagnation compared to loss\nfunction. Each gray dot indicates the loss of each batch. Source data are provided as\na Source Dataﬁle.\nArticle https://doi.org/10.1038/s41467-024-45102-8\nNature Communications|         (2024) 15:1197 5\nreferred to as suitable for the ReLU activation function in the\nTransformer.\n4. pre-LN structure: Pre-LN is a structural modi ﬁcation of the\nTransformer ﬁrst proposed in Xiong et al.41 to stabilize learning.\nThis method prevents vanishing gradients in the lower layer of the\nTransformer by ensuring that the residual connection is not\naffected by layer normalization, which can cause vanishing\ngradients. This method has been shown to stabilize the learning\nof the Transformer\n41.\nAll these perturbations were respectively introduced to the\nbaseline model, and training was conducted 14 times with different\ninitial weights for each modiﬁcation, except for the introduction of He\nnormal, which showed a signiﬁcant delay in learning and was aborted\nwhen 5 studies wereﬁnished. Considering the computational cost, we\nstopped training when perfect accuracy reached 0.95.\nSupplementary Fig. 8a, b shows the average ofstep-0.7/0.95.I n\nsome cases where the accuracy did not reach 0.7/0.95,step-0.7/0.95\nwas deﬁned as 80,000 (end of training). The result showed that the\nintroduction of pre-LN signiﬁcantly accelerated the learning speed,\nwhereas other modiﬁcations did not achieve improvement. Figure5a\nshows the changes in accuracy over time in the 14 trainings with pre-\nLN, compared with those about the control model. Thisﬁgure also\ndemonstrates that pre-LN strongly stabilizes learning.\nThen, does pre-LN facilitate understanding of chirality, or simply\naccelerate overall learning? Fig.5b and Supplementary Fig. 9 show the\nmasked accuracy and accuracy for each character in one of the studies\nin which pre-LN was adopted. The results show that while learning on\nall tokens is accelerated,“@” and “@@” is relatively slow to be learned\neven in the model with pre-LN, suggesting that pre-LN accelerates the\nlearning of not only chirality but also molecular structure in general.\nInvestigation with another chemical language\nFinally, to clarify the generalizability of ourﬁndings about the Trans-\nformer, we trained the model with another expression of molecules.\nInstead of SMILES, here we used InChI, an alternative literal repre-\nsentation of molecules adopted in some cheminformatics studies with\nchemical language models. Although the performances of chemical\nFig. 4 | Difﬁculty in learning chirality for Transformer. a, b Temporal change of\nperfect accuracy when each one of the characters in Simpliﬁed Molecular Input\nLine Entry System (SMILES) was masked for trainings in which stagnation did/did\nnot occur. Rare tokens which did not appear in the test set are not shown.\nc Examples of target and predicted molecules during stagnation (at step 10,000).\nEach of the molecules in the upper row is predicted targeted to the directly below\nmolecule. d Ratio of correct predictions, predictions with only mistakes of“@”\ntoken for“@@” token and“@@” token for“@” token (mistakes attributed to\nchirality), and predictions with other mistakes in the test set. Source data are\nprovided as a Source Dataﬁle.\nArticle https://doi.org/10.1038/s41467-024-45102-8\nNature Communications|         (2024) 15:1197 6\nlanguage models fed with InChI are reported to be inferior to those\nwith SMILES10,42, and therefore InChI expression is not used frequently,\nthe translation task between InChI and SMILES guarantees that the\nmodel learns to extract essential connectivity of atoms constituting\nmolecules because these representations are completely different,\nwhereas randomized SMILES and canonical SMILES follows the same\ngrammar. The details about InChI-to-SMILES translation are written in\nSupplementary Note 5.\nThe result showed early saturation of partial accuracy andﬁn-\ngerprint similarity compared to perfect accuracy and loss function,\nindicating that the recognition of partial structures is easier than\noverall structures in InChI-to-SMILES translation (Fig.6a, b). The\nperformance on downstream tasks was not improved by training\n(Supplementary Figs. 10 and 11). The results also revealed that the\nstagnation did occur in InChI-to-SMILES translation (Fig.6a), and\ncharacter-wise analysis exhibited that confusion in discriminating\nenantiomers caused it (Fig. 6c, d and Supplementary Fig. 12). In\naddition, pre-LN introduction relieved the stagnation (Fig.6e), and\ncharacter-wise accuracy showed overall acceleration of saturation\nand relatively low accuracy of chiral tokens when pre-LN was adop-\nted, indicating that this structure generally accelerates under-\nstanding of chemical structures (Supplementary Fig. 13). These\nresults suggest that what we have found about the Transformer\nmodel trained by random-to-canonical SMILES translation is an\ninnate property of Transformer, rather than a grammatical or pro-\ncessive problem speciﬁc to SMILES.\nDiscussion\nIn recent years, a newﬁeld of research has been established in which\nNLP models, especially the Transformer model, is applied to literal\nrepresentations of molecules like SMILES to solve various tasks\nhandling molecular structures: chemical language models with neural\nnetwork\n7. In this paper, as a basic study of chemical language models\nfor descriptor generation, we investigated how a Transformer model\nunderstands diverse chemical structures during the learning progress.\nWe compared the agreement betweenthe output and the target, and\nthe ﬁngerprints related to substructures, for the models in the process\nof learning. The performance of the descriptor generated by the model\nunder training was also examined on the downstream tasks of pre-\ndicting molecular properties. We further found that perfect accuracy\nof translation sometimes stagnates at a low level depending on the\ninitial weight of the model. Toﬁnd the cause of this phenomenon, we\ncompared the accuracy per each character of SMILES, and we experi-\nmented with 4 alterations to prevent stagnation. The majorﬁndings in\nthis paper are as follows:\n1. In the Transformer model, partial structures of molecules are\nrecognized in the early steps of training, whereas recognition of\nthe overall structures requires more training. Together with our\nprevious study about RNN models\n35,t h i sﬁnding can be general-\nized for various NLP models fed with SMILES strings. Therefore,\nenabling the Transformer model to refer to overall structural\ninformation as an auxiliary task in its structure would help\nimprove the descriptor generation model.\n2. For molecular property prediction, the performance of the\ndescriptor generated by the Transformer model may already have\nbeen saturated before it was trained, and it was not improved by\nthe subsequent training. This suggests that the descriptor of the\ninitial model already contains enough information for down-\nstream tasks, which is perhaps the partial structures of molecules.\nOn the other hand, it is also possible that downstream tasks like\nproperty prediction of molecules are too easy for the Transformer\nand inappropriate for evaluating Transformer-based descriptor\ngeneration methods\n31.\nFig. 5 | Improvement of stagnation and recognition of chirality by the intro-\nduction of pre-LN. a, b Temporal change of perfect accuracy started from 14\ndifferent initial weights with post/pre-layer normalization(post-LN/pre-LN) struc-\nture. Lines with the same color corresponds to training from the same initial weight.\na is reproduced from Fig.3a for comparison.c, d Temporal change of perfect\naccuracy when each one of the characters in Simpliﬁed Molecular Input Line Entry\nSystem (SMILES) was masked for trainings with post-LN/pre-LN structure.c is\nreproduced from Fig.4b for comparison. Source data are provided as a Source\nData ﬁle.\nArticle https://doi.org/10.1038/s41467-024-45102-8\nNature Communications|         (2024) 15:1197 7\n3. Translation performance of the Transformer concerning chirality\nis relatively slow to rise compared to the other factors, such as\noverall structures or other partial structures, and the model is\nsometimes confused about chirality for a long period, causing\npersistent stagnation in whole structure recognition. This suggests\nthat additional structures or tasks that teach chirality to the model\ncan improve the performance ofthe model and its descriptor.\n4. Introducing the pre-LN structure accelerates and stabilizes\nlearning, including chirality.\nThese discoveries contribute to clarify the black box in chemical\nlanguage models and are expected to activate thisﬁeld. It is an intri-\nguing future task to investigate whether theseﬁndings hold true in\nchemical language models for other applications with supervised\nnatures such as structure generation and end-to-end property pre-\ndiction, although we focused on descriptor generation in this study\nbecause this task makes the model purely learn chemical structures in\nan unsupervised manner. Chemical language models would be\nincreasingly developed, as NLP is one of the most advancedﬁelds in\nFig. 6 | Experiments with the Transformer model trained by InChI-to-SMILES\ntranslation. aTemporal change of perfect accuracy of the model trained by InChI-\nto-SMILES translation started from 5 different initial weights.b Temporal change of\npartial accuracy of the model trained by InChI-to-SMILES translation started from 5\ndifferent initial weights.c Temporal change of Tanimoto similarity between the\nindicatedﬁngerprints of predicted and target SMILES, with the loss for comparison.\nEach gray dot indicates the loss of each batch.d Temporal change of perfect\naccuracy when each one of the characters in SMILES was masked for one of the\ntrainings. Rare tokens which did not appear in the test set are not shown.e Ratio of\nmolecules in the test set which were correctly predicted, and molecules which were\nmistakenly predicted for each reason. Wrong predictions were classiﬁed by whe-\nther they only have“@”-to-“@@” or “@@”-to-“@” mistakes, or also have other\ntypes of mistakes.f Temporal change of perfect accuracy and partial accuracy\nstarted from 5 different initial weights with pre-LN structure. Initial weights were\nnot shared between post/pre-LN here. Source data are provided as a Source Data\nﬁle. InChI International Chemical Identiﬁer, SMILES Simpliﬁed Molecular Input Line\nEntry System, Pre-LN pre-Layer Normalization.\nArticle https://doi.org/10.1038/s41467-024-45102-8\nNature Communications|         (2024) 15:1197 8\ndeep learning. On the other hand, there are many unknowns in the\nrelationship between language models and chemical structures com-\npared to prevalent neural network models in theﬁeld of chemistry,\nsuch as graph neural networks\n43,44. Further basic research on the\nrelationship between NLP models and chemical structures is expected\nto further clarify the black box about how NLP models evolve and\nrecognize chemical structures, leading to the development and per-\nformance improvement of chemical language models for various tasks\nin chemistry.\nMethods\nTraining a Transformer\nDataset. To pretrain a Transformer model, molecules were sampled\nfrom the ZINC-15 dataset11, which contains approximately 1 billion\nmolecules. After rough sampling of molecules from the whole data-\nset, Molecules with atoms other than H, B, C, N, O, F, P, S, Cl, Br, or I,\nand molecules that have more than 50 or less than 3 heavy atoms\nwere ﬁltered out, 30,190,532 (approximately 30 M) molecules were\nsampled for train and test set. The molecules were sampled not\nrandomly, but in a stratiﬁed way in terms of SMILES length to reduce\nthe bias of molecular weights and SMILES lengths in the training set,\ni.e., all molecules in ZINC-15 were classiﬁed according to lengths of\nSMILES strings, and 460,000 molecules were sampled from each\nlength. All molecules were sampled for lengths which do not contain\nmore than 460,000 molecules. The initial rough sampling was also\nstratiﬁed similarly not to omit molecules with rare length of SMILES.\nAbout 3% (9057) molecules were sampled as the test set, and the\nremaining molecules were used for training. Note that random\nsampling is widely used instead of this stratiﬁed sampling, although\nwe believe that our sampling strategy enables fair training of mole-\ncular structures with regard to molecular weight and SMILES length.\nWe therefore conducted the key experiments with the dataset pre-\npared by random sampling to conﬁrm generality and obtained\nsimilar results. The details about these experiments are written in\nSupplementary Note 6.\nThe remaining molecules were then stripped of small fragments,\nsuch as salts, and canonical SMILES and randomized SMILES of them\nwere generated. SMILES is a one-dimensional representation of a\nmolecule, and because any atom can be the starting point, multiple\nSMILES representations correspond to a single molecule. To identify\none speciﬁc representation, there is a rule for selecting the initial\natom, and the SMILES representation identiﬁed based on this rule is\ncalled canonical SMILES, whereas the others are referred to here as\nrandomized SMILES. Translation between randomized and canonical\nSMILES is used as a training task in\n32,34. We generated randomized\nSMILES by renumbering all atoms in molecule32,33. InChI, another\nstring representation of molecules were also generated for transla-\ntion. All of these processes were conducted using the RDKit library\n(ver. 2022.03.02).\nModel architecture\nWe implemented the model with PyTorch framework (ver. 1.8, except\nfor the model with pre-LN structure). Parameters and model archi-\ntecture were determined according to the original Transformer in\nref. 31; the dimension of the model was 512, the dimension of the feed-\nforward layer was 2048, and the number of layers in the encoder and\ndecoder was 6. We used ReLU activation, and the dropout ratio was 0.1\nin both the encoder and the decoder.\nLearning procedure\nRandomized SMILES strings of molecules in the training dataset were\ntokenized and then fed into the encoder of the Transformer. Toke-\nnization was conducted with the vocabulary shown in Supplemen-\ntary Table 1.“<s>” and “<\\s>” tokens are added to the beginning end\nand of the token sequences, respectively, and“<pad>” tokens are\nadded to some of the sequences to arrange the length of them in a\nbatch. Positional encoding was added to the embedded SMILES\ntokens. The input of the decoder was canonical SMILES strings of the\nsame molecule, and the model was forced to predict the same\ncanonical SMILES shifted by one character before, with the attention\nfrom posterior tokens being masked. Hence, the model was forced to\npredict each token of SMILES based on its prior tokens (teacher-\nforcing\n38). We calculated cross-entropy loss for each token except\nthe padding token, and the mean loss along all tokens was used\nas the loss function. As for translation from InChI to canonical\nSMILES, the vocabulary in Supplementary Table 2 was used for\ntokenization.\n25,000 tokens were inputted per step following\n31. Due to resource\nrestriction, the batch size was set to 12,500 tokens, and the optimizer\nwas stepped after every 2 batches. We introduced bucketing\n10,31,t h a ti s ,\nwe classiﬁed SMILES strings in training data into several ranges of their\nlengths, and generated batches from SMILES in the same range of\nlength to reduce padding. The number of batches in total amounted to\n147,946 (for 73,973 steps). We used Adam optimizer\n45 with a warmup\nscheduler (warmup step = 4000) and continued training up to\n80,000 steps (slightly longer than one epoch), although training was\naborted whenperfect accuracy(described below) reached 0.95 in some\nstudies to save time in training.\nMetrics\nWe measured the translation performance of each model by 2 metrics:\nperfect accuracy and partial accuracy35. Perfect accuracy means the\nratio of molecules whose target SMILES strings were completely\ntranslated by the model, except those after end-of-string tokens (i.e.,\npadding tokens).Partial accuracymeans the character-wise ratio of\ncoincidence between the target and predicted strings.\nTo evaluate recognition of the model about partial structures of\nmolecules, we calculated MACCS keys and ECFP for both target and\npredicted SMILES and calculated the Tanimoto similarity of these\nﬁngerprints between them for the test set. The radius of ECFP was\nvaried from 1 to 3. Because prediction of the Transformer does not\nalways output valid SMILES, molecules for which the Transformer\npredicted invalid SMILES at any step were omitted when calculating\nTanimoto similarity. Note that a valid SMILES means a grammatically\ncorrect SMILES which encodes a molecule that satis ﬁes the\noctet rule.\nWe calculated the agreement of MACCS keys between the pre-\ndicted and target molecules in each dimension. For each dimension,\nthe percentage of molecules was calculated for which the model makes\nvalid predictions and for which the predicted molecules have MACCS\nkeys of 1, in all molecules that have MACCS keys of 1. The same per-\ncentage for bit 0 was also calculated.\nMolecular property prediction\nDataset. Physical and biological property data of molecules was\nobtained from MoleculeNet36 with the DeepChem module46.T a b l e1\nshows the datasets we used and their information. The sameﬁltering\nand preprocessing were applied as in Training a Transformer Section\nto molecules in each dataset, although too long or short SMILES were\nnot removed in order not to overestimate the prediction performance,\nand then duplicated SMILES were omitted. The model was trained and\nevaluated for 5 train-validation-test folds split by method recom-\nmended by DeepChem.\nMolecular property prediction task\nWe tested the property prediction ability for models at steps when\nperfect accuracy reached 0.2, 0.5, 0.7, 0.9, 0.95, and 0.98 and at\nsteps 0, 4000, and 80,000 (end of training). In the property pre-\ndiction experiments, only the encoder of the Transformer was used.\nRandomized SMILES were inputted into it, and then the memory\nArticle https://doi.org/10.1038/s41467-024-45102-8\nNature Communications|         (2024) 15:1197 9\n(=output of encoder) was pooled and used as the descriptor of\nmolecules. To minimize the effect of the pooling procedure, we\ntested 4 pooling methods: 1) average all memory along the axis of\nSMILES length, 2) extract memory corresponding to theﬁrst token in\nSMILES, 3) obtain the average and maximum memory along the axis\nof the SMILES length and concatenate them with the memories of the\nﬁrst and last token\n34, 4) concatenate average, maximum, minimum,\nstandard deviation, beginning, and end of memory. Note that the\ndimensions of pooled descriptors are not equal: 1) 512, 2) 512, 3)\n2048, and 4) 3072.\nFrom these pooled descriptors, we predicted the target molecular\nproperties with SVM, XGBoost, and MLP in our preliminary study and\nchose XGBoost which showed the best performance. For each of the\n5 splits, we searched hyperparameters by Bayesian optimization using\nOptuna\n47. As baseline descriptors, we calculated ECFP (R = 2, dimen-\nsion = 2048) and CDDD10 (dimension = 512) for molecules in Molecu-\nleNet datasets and measured the property prediction accuracy.\nRandom values from a uniform distribution in [0, 1] were generated\nand also used as baseline descriptors (dimension = 2048). Further-\nmore, we compared the prediction performance with Uni-Mol\n48,o n eo f\nthe state-of-the-art models in molecular property prediction. The\nexperiments were conducted for 5 folds split by the methods recom-\nmended by DeepChem, and the accuracy of prediction was measured\nby the recommended metric by MoleculeNet.\nExperiment in different initial weight and iteration order\n14 different initial weights were randomly generated with different\nrandom seeds in PyTorch, and 2 different orders of data iteration were\nmade by randomly sampling molecules for each batch and randomly\nshufﬂing the batch iteration order with 2 different seeds. All hyper-\nparameters wereﬁxed during these 28 experiments in total. To per-\nform numerous experiments, we aborted the experiments when\naccuracy reached 0.95 instead of continuing until step 80,000, except\n4 experiments to see saturation. We calculated perfect accuracy and\npartial accuracy on validation set for every 2000 steps, and the step\nwhen perfect accuracyﬁrst reached 0.7/0.95 was calledstep-0.7/0.95,\nrespectively. The mean step-0.7 and step-0.95 were compared\nbetween 2 iteration orders by two-sided Welch’st - t e s t .\nResearch for the cause of stagnation\nFor each character in the vocabulary, we calculated the perfect accu-\nracy with characters of each kind masked. This means we did not check\nwhether the characters of the selected kind were correctly predicted\nby the Transformer when calculating perfect accuracy. We computed\npartial accuracy for each character as well. Because the Transformer\npredicts each character from memory and previous characters it pre-\ndicted, it is more likely to produce wrong predictions after it once\nmade a mistake. We therefore adopted teacher-forcing when calcu-\nlating this metric, meaning the model predicts each character with the\ncorrect preceding characters\n38. We examined the percentage of\nSMILES that could not be translated completely, for which the mistake\nwas solely attributable to chirality. Since chirality of molecules is\nrepresented by“@” and “@@” tokens in SMILES, the percentage of\nmolecules that were answered correctly except that“@” was mistaken\nfor “@@”, “@@” was mistaken for“@”, or both, among all molecules\nwere calculated.\nStructural modiﬁcations of the model to prevent stagnation\nFor AdamW, He normal initialization, and pre-LN (pre-layer normal-\nization) structures, we used PyTorch implementation. As pre-LN is not\nimplemented in PyTorch version 1.8, we conducted experiments with\nthe pre-LN structure in version 1.10. For experiments with more“@”\nand “@@”, training data was sampled again from the training dataset\nwe prepared in Training a Transformer Section. SMILES strings that\nhave either“@” or “@@” were sampled at 100% probability, and those\nthat do not were sampled at 50%. The new training dataset contained\nabout 135,000 molecules (about 67,500 steps). We did not change the\ntest set.\nThese modiﬁcations were introduced respectively, and the model\nwas trained from 14 initial weights. The number of steps the model\ntook until perfect accuracy reached 0.7 and 0.95 was compared to the\ncontrol experiment with no modiﬁcation by two-sided Welch’s t-test\nwith Bonferroni correction. Since we had to conduct many experi-\nments in this section, we aborted experiments when accuracy reached\n0.95 instead of continuing until step 80,000.\nReporting summary\nFurther information on research design is available in the Nature\nPortfolio Reporting Summary linked to this article.\nData availability\nZINC-1511 dataset used to train and evaluate the model was downloaded\nfrom https://zinc15.docking.org/MoleculeNet36 dataset used to eval-\nuate performance of molecular property prediction was downloaded\nthrough DeepChem\n46 module (https://deepchem.io/). Source data are\nprovided with this paper.\nCode availability\nUsed codes and trained models are available at:https://github.com/\nmizuno-group/ChiralityMisunderstanding49.\nReferences\n1 . C h e n ,H . ,E n g k v i s t ,O . ,W a n g ,Y . ,O l i v e c r o n a ,M .&B l a s c h k e ,T .T h e\nrise of deep learning in drug discovery.Drug Discov. Today23,\n1241– 1250 (2018).\n2. Wu, Y. & Wang, G. Machine learning based toxicity prediction: From\nchemical structural descriptiont ot r a n s c r i p t o m ea n a l y s i s .Int J. Mol.\nSci. 19,2 3 5 8( 2 0 1 8 ) .\n3 . B u t l e r ,K .T . ,D a v i e s ,D .W . ,C a r t w r i g h t ,H . ,I s a y e v ,O .&W a l s h ,A .\nMachine learning for molecular and materials science.Nature 559,\n547– 555 (2018).\n4. Danishuddin, Kumar, V., Faheem, M. & Woo Lee, K. A decade of\nmachine learning-based predictive models for human pharmaco-\nkinetics: Advances and challenges.Drug Discov. Today27,\n529– 537 (2022).\n5. Khamis, M. A., Gomaa, W. & Ahmed, W. F. Machine learning in\ncomputational docking.Artif. Intell. Med.63,1 3 5– 152 (2015).\n6. Faber, F. A. et al. Prediction Errors of Molecular Machine Learning\nModels Lower than Hybrid DFT Error.J. Chem. Theory Comput.13,\n5255– 5264 (2017).\n7. Ikebata, H., Hongo, K., Isomura, T., Maezono, R. & Yoshida, R.\nBayesian molecular design with a chemical language model.J.\nComput. Aided Mol. Des.31,3 7 9– 391 (2017).\n8. Gómez-Bombarelli, R. et al. Automatic chemical design using a\ndata-driven continuous representation of molecules.ACS Cent. Sci.\n4,2 6 8– 276 (2018).\n9. Quan, Z. et al. A System for Learning Atoms Based on Long Short-\nTerm Memory RecurrentNeural Networks. In2018 IEEE International\nConference on Bioinformatics and Biomedicine (BIBM)728– 733\n(IEEE, 2018).\n1 0 . W i n t e r ,R . ,M o n t a n a r i ,F . ,N o é ,F .&C l e v e r t ,D . - A .L e a r n i n gc o n -\ntinuous and data-driven molecular descriptors by translating\nequivalent chemical representations.Chem. Sci.10,\n1692– 1701 (2019).\n11. Sterling, T. & Irwin, J. J. ZINC 15– ligand discovery for everyone.J.\nChem. Inf. Model55,2 3 2 4– 2337 (2015).\n12. Kim, S. et al. PubChem substance and compound databases.\nNucleic Acids Res.44,D 1 2 0 2– D1213 (2016).\n13. Mendez, D. et al. ChEMBL: towards direct deposition of bioassay\ndata. Nucleic Acids Res.47,D 9 3 0–\nD940 (2019).\nArticle https://doi.org/10.1038/s41467-024-45102-8\nNature Communications|         (2024) 15:1197 10\n1 4 . D u r a n t ,J .L . ,L e l a n d ,B .A . ,H e n r y ,D .R .&N o u r s e ,J .G .R e o p t i m i z a -\ntion of MDL Keys for Use in Drug Discovery.J. Chem. Inf. Comput\nSci. 42,1 2 7 3– 1280 (2002).\n15. Rogers, D. & Hahn, M. Extended-Connectivity Fingerprints.J. Chem.\nInf. Model50,7 4 2– 754 (2010).\n1 6 . L e ,T . ,W i n t e r ,R . ,N o é ,F .&C l e v e r t ,D . - A .N e u r a l d e c i p h e r - r e v e r s e -\nengineering extended-connectivityﬁngerprints (ECFPs) to their\nmolecular structures.Chem. Sci.11,1 0 3 7 8– 10389 (2020).\n1 7 . B a g a l ,V . ,A g g a r w a l ,R . ,V i n o d ,P .K .&P r i y a k u m a r ,U .D .M o l G P T :\nmolecular generation using a transformer-decoder model.J. Chem.\nInf. Model62,2 0 6 4– 2076 (2021).\n18. Hong, Y.-B., Lee, K.-J., Heo, D. & Choi, H. Molecule Generation for\nDrug Discovery with New Transformer Architecture. Preprint at\nhttps://ssrn.com/abstract=4195528(2022).\n19. Rahimovich, D. R., Qaxramon O’g’li, A. S., & Abdiqayum O'g'li, S. R.\nApplication of transformer model architecture in the new drugs\ndesign. In2021 International Conferenceon Information Science and\nCommunications Technologies (ICISCT)1– 3( I E E E ,2 0 2 1 ) .\n20. Shin, B., Park, S., Bak, J. & Ho, J. C. Controlled molecule generator\nfor optimizing multiple chemical properties.Proc. Conf. Health\nInference Learn.2021,1 4 6– 153 (2021).\n21. Kim, H., Na, J. & Lee, W. B. Generative chemical transformer: neural\nmachine learning of molecular geometric structures from chemical\nlanguage via attention.J. Chem. Inf. Model61, 5804– 5814 (2021).\n22. Yang, Q. et al. Molecular transformer uniﬁes reaction prediction and\nretrosynthesis across pharma chemical space.Chem. Commun.55,\n12152– 12155 (2019).\n23. Karpov, P., Godin, G. & Tetko, I. V. A transformer model for retro-\nsynthesis. InInternational Conference on Artiﬁcial Neural Networks\n817– 830 (Springer International Publishing, 2019).\n24. Zheng, S., Rao, J., Zhang, Z., Xu, J. & Yang, Y. Predicting retro-\nsynthetic reactions using self-corrected transformer neural net-\nworks. J. Chem. Inf. Model60,4 7– 55 (2019).\n25. Tetko, I. V., Karpov, P., van Deursen, R. & Godin, G. State-of-the-art\naugmented NLP transformer models for direct and single-step\nretrosynthesis.Nat. Commun.11,5 5 7 5( 2 0 2 0 ) .\n26. Mao, K. et al. Molecular graph enhanced transformer for retro-\nsynthesis prediction.Neurocomputing457\n,1 9 3– 202 (2021).\n27. Maziarka, Ł. et al. Molecule attention transformer. Preprint at\nhttps://arxiv.org/abs/2002.08264(2020).\n28. Zhu, J. et al. Dual-view Molecular Pre-training. InProceedings of the\nACM SIGKDD International Conference on Knowledge Discovery and\nData Mining(Association for Computing Machinery, 2023).\n29. Shin, B., Park, S., Kang, K. & Ho, J. C. Self-Attention Based Molecule\nRepresentation for Predicting Drug-Target Interaction.Mach. Learn.\nHealthc. Conf.106,2 3 0– 248 (2019).\n30. Chen, B., Barzilay, R. & Jaakkola, T. Path-augmented graph trans-\nformer network. Preprint athttps://arxiv.org/abs/1905.12712(2019).\n31. Vaswani, A. et al. Attention Is All You Need. InAdvances in Neural\nInformation Processing Systems(NIPS, 2017).\n32. Irwin, R., Dimitriadis, S., He, J. & Bjerrum, E. J. Chemformer: A pre-\ntrained transformer for computational chemistry.Mach. Learn Sci.\nTechnol.3, 015022 (2022).\n33. Bjerrum, E. J. & Sattarov, B. Improving chemical autoencoder latent\nspace and molecular de novo generation diversity with hetero-\nencoders.Biomolecules8,1 3 1( 2 0 1 8 ) .\n34. Honda, S., Shi, S. & Ueda, H. R. SMILES Transformer: Pre-trained\nMolecular Fingerprint for Low Data Drug Discovery. Preprint at\nhttp://arxiv.org/abs/1911.04738(2019).\n35. Nemoto, S., Mizuno, T. & Kusuhara, H. Investigation of chemical\nstructure recognition by encoder– decoder models in learning\nprogress.J. Cheminform15,4 5( 2 0 2 3 ) .\n36. Wu, Z. et al. MoleculeNet: A benchmark for molecular machine\nlearning.Chem. Sci.9,5 1 3– 530 (2018).\n37. Duvenaud, D. et al. Convolutional Networks on Graphs for Learning\nMolecular Fingerprints. InAdvances in Neural Information Proces-\nsing Systems(The MIT Press, 2015).\n38. Williams, R. J. & Zipser, D. A Learning Algorithm for Continually\nRunning Fully Recurrent Neural Networks.Neural Comput1,\n270– 280 (1989).\n39. Ucak, U. V., Ashyrmamatov, I. & Lee, J. Reconstruction of lossless\nmolecular representations fromﬁngerprints.J. Cheminform15,\n26 (2023).\n40. Loshchilov, I. & Hutter, F. Decoupled weight decay regularization. In\nICLR Workshop on Representation Learning on Graphs and Mani-\nfolds (2019).\n41. Xiong, R. et al. On layer normalization in the transformer archi-\ntecture. InInternational Conference on Machine Learning\n10524– 10533 (JMLR, 2020).\n42. Omote, Y., Matsushita, K., Iwakura, T., Tamura, A. & Ninomiya, T.\nTransformer-based approach for predicting chemical compound\nstructures. InProceedings of the 1st Conference of the Asia-Paciﬁc\nChapter of the Association for Computational Linguistics and the\n10th International Joint Conference on Natural Language Processing\n154– 162 (Association for Computational Linguistics, 2020).\n43. Wang, Y. et al. Identiﬁ\ncation of vital chemical information via\nvisualization of graph neural networks.Brief. Bioinform24,\nbbac577 (2023).\n44. Jiménez-Luna, J., Skalic, M., Weskamp, N. & Schneider, G. Color-\ning Molecules with Explainable Artiﬁcial Intelligence for Pre-\nclinical Relevance Assessment.J. Chem. Inf. Model61,\n1083– 1094 (2021).\n45. Kingma, D. P. & Ba, J. Adam: A Method for Stochastic Optimization.\nIn International Conference on Learning Representations(2015).\n46. Ramsundar, B. et al.Deep Learning for the Life Sciences(O’Reilly\nMedia, 2019).\n47. Akiba, T., Sano, S., Yanase, T., Ohta, T. & Koyama, M. Optuna: A next-\ngeneration hyperparameter optimization framework. InProceed-\nings of the 25th ACM SIGKDD international conference on knowl-\nedge discovery & data mining,2 6 2 3– 2631 (ACM, 2019).\n48. Zhou, G. et al. Uni-Mol: a universal 3D molecular representation\nlearning framework. InInternational Conference on Learning\nRepresentations(2023).\n49. Yoshikai, Y., Mizuno, T., Nemoto, S. & Kusuhara, H. Difﬁculty in\nchirality recognition for Transformer architectures learning chemi-\ncal structures from string representations. mizuno-group/Chir-\nalityMisunderstanding.Zenodo, https://doi.org/10.5281/zenodo.\n10389855(2023).\n50. Ramsundar, B.Molecular machine learning with deepchem. http://\npurl.stanford.edu/js264hd4826(2018).\nAcknowledgements\nWe thank all those who contributed to the construction of the following\ndata sets employed in the present study such as ZINC and MoleculeNet.\nThis work was supported by AMED under Grant Number JP\n23ak0101199h0001 (T.M.) and JP22mk0101250h0001 (T.M.), and the\nJSPS KAKENHI Grant-in-Aid for Scientiﬁc Research (C) (grant number\n21K06663, T.M.) from the Japan Society for the Promotion of Science.\nAuthor contributions\nYasuhiro Yoshikai: Methodology, Software, Investigation, Writing– Ori-\nginal Draft, Visualization. Tadahaya Mizuno: Conceptualization, Resour-\nces, Supervision, Project administration, Writing– Original Draft, Writing\n– Review & Editing, Funding acquisition. Shumpei Nemoto: Writing–\nReview & Editing. Hiroyuki Kusuhara: Writing– Review & Editing.\nCompeting interests\nThe authors declare no competing interests.\nArticle https://doi.org/10.1038/s41467-024-45102-8\nNature Communications|         (2024) 15:1197 11\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-024-45102-8.\nCorrespondenceand requests for materials should be addressed to\nTadahaya Mizuno.\nPeer review informationNature Communicationsthanks Joao Aires de\nSousa, and the other, anonymous, reviewer(s) for their contribution to\nthe peer review of this work. A peer reviewﬁle is available.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jur-\nisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons license, and indicate if\nchanges were made. The images or other third party material in this\narticle are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons license and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this license, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2024\nArticle https://doi.org/10.1038/s41467-024-45102-8\nNature Communications|         (2024) 15:1197 12",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6914318203926086
    },
    {
      "name": "Computer science",
      "score": 0.6527104377746582
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5481916666030884
    },
    {
      "name": "Natural language processing",
      "score": 0.49935197830200195
    },
    {
      "name": "Representation (politics)",
      "score": 0.4415076971054077
    },
    {
      "name": "Machine learning",
      "score": 0.33171334862709045
    },
    {
      "name": "Physics",
      "score": 0.1431085169315338
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    }
  ]
}