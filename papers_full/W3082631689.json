{
  "title": "A Comparison of Pre-trained Vision-and-Language Models for Multimodal Representation Learning across Medical Images and Reports",
  "url": "https://openalex.org/W3082631689",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5001704304",
      "name": "Yikuan Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5013719827",
      "name": "Hanyin Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100452550",
      "name": "Yuan Luo",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W90362830",
    "https://openalex.org/W2963954913",
    "https://openalex.org/W2963686907",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2962964995",
    "https://openalex.org/W3024464497",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W3101156210",
    "https://openalex.org/W2963967185",
    "https://openalex.org/W2950104027",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W2963921132",
    "https://openalex.org/W2901770990",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W2963466845",
    "https://openalex.org/W3015803803",
    "https://openalex.org/W2969862959",
    "https://openalex.org/W2913279579",
    "https://openalex.org/W2953328958",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W3098232790",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W3014611590",
    "https://openalex.org/W2969624041",
    "https://openalex.org/W2073982987",
    "https://openalex.org/W2965818302",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2732016772",
    "https://openalex.org/W2938082352",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2901466771",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2152772232",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2109586012",
    "https://openalex.org/W2963373823",
    "https://openalex.org/W3020257313",
    "https://openalex.org/W2962749469",
    "https://openalex.org/W2990263762",
    "https://openalex.org/W2965729401",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2412400526",
    "https://openalex.org/W2950279864",
    "https://openalex.org/W2770241596",
    "https://openalex.org/W2963075078",
    "https://openalex.org/W2975501350"
  ],
  "abstract": "Joint image-text embedding extracted from medical images and associated contextual reports is the bedrock for most biomedical vision-and-language (V+L) tasks, including medical visual question answering, clinical image-text retrieval, clinical report auto-generation. In this study, we adopt four pre-trained V+L models: LXMERT, VisualBERT, UNIER and PixelBERT to learn multimodal representation from MIMIC-CXR radiographs and associated reports. The extrinsic evaluation on OpenI dataset shows that in comparison to the pioneering CNN-RNN model, the joint embedding learned by pre-trained V+L models demonstrate performance improvement in the thoracic findings classification task. We conduct an ablation study to analyze the contribution of certain model components and validate the advantage of joint embedding over text-only embedding. We also visualize attention maps to illustrate the attention mechanism of V+L models.",
  "full_text": "A COMPARISON OF PRE-TRAINED VISION -AND -LANGUAGE\nMODELS FOR MULTIMODAL REPRESENTATION LEARNING\nACROSS MEDICAL IMAGES AND REPORTS\nYikuan Li\nFeinberg School of Medicine\nNorthwestern University\nChicago, IL 60611\nyikuanli2018@u.northwestern.edu\nHanyin Wang\nFeinberg School of Medicine\nNorthwestern University\nChicago, IL 60611\nhanyinwang2022@u.northwestern.edu\nYuan Luo‚àó\nFeinberg School of Medicine\nNorthwestern University\nChicago, IL 60611\nyuan.luo@northwestern.edu\nABSTRACT\nJoint image-text embedding extracted from medical images and associated contextual reports is the\nbedrock for most biomedical vision-and-language (V+L) tasks, including medical visual question\nanswering, clinical image-text retrieval, clinical report auto-generation. In this study, we adopt\nfour pre-trained V+L models: LXMERT, VisualBERT, UNIER and PixelBERT to learn multimodal\nrepresentation from MIMIC-CXR radiographs and associated reports. The extrinsic evaluation on\nOpenI dataset shows that in comparison to the pioneering CNN-RNN model, the joint embedding\nlearned by pre-trained V+L models demonstrate performance improvement in the thoracic Ô¨Åndings\nclassiÔ¨Åcation task. We conduct an ablation study to analyze the contribution of certain model\ncomponents and validate the advantage of joint embedding over text-only embedding. We also\nvisualize attention maps to illustrate the attention mechanism of V+L models.\n1 Introduction\nThere has been a long history of learning visual and semantic information to solve vision and language tasks. Various\npioneering approaches [1, 2, 3, 4, 5] have been proposed to boost the performance of visual question answering (VQA)\n[6], image captioning, image-text matching, visual reasoning [7] and textual grounding, respectively. These models\nlearned image representations from powerful backbone convolutional neural networks (CNN), text representations from\nrecurrent neural networks (RNN), and a fusion modality to achieve joint image-text embedding. For simplicity, these\ngenres of models are often referred to as CNN+RNN models. Most of these models were designed for speciÔ¨Åc vision-\nan-language tasks and therefore had relatively poor generalizability. Recently, inspired by the success of large-scale\npre-trained language models, such as BERT [8], XLNET [9], more researchers focus on generating image-text join\nembedding from pre-training Transformer-based [10] model on V+L datasets. The joint embedding is then Ô¨Åne-tuned to\nvarious V+L tasks and achieves state-of-the-art results. The main difference between these models lies in pre-training\nstrategies and cross-modality architecture. To be speciÔ¨Åc, UNITER [11] and VisualBert [12] applied a single stream of\nTransformer to jointly learn image-text embedding. LXMERT [13] and ViLBERT [14] used two separated Transformer\nblocks on image and text input and a third fusion Transformer block for cross-modality. Masked language modeling,\n‚àóCorresponding author\nOur implementation can be found at: https://github.com/YIKUAN8/Transformers-VQA\narXiv:2009.01523v1  [cs.CV]  3 Sep 2020\nA PREPRINT - SEPTEMBER 4, 2020\nmasked region modeling, image-text matching, word-region alignment, and other pre-training tasks are experimented in\nthose models.\nThe joint image-text embedding are less discussed in the biomedical domain due to the shortage of large-scale annotated\nV+L datasets. TandemNet [15], which adopts a CNN+LSTM architecture, classiÔ¨Åes pathological bladder cancer from\nimages with the semantic clues in diagnostic reports. Lau et al. [ 16] introduce the Ô¨Årst manually constructed VQA\ndataset in radiology, named VQA-RAD. In their study, the performances of 2 well-known CNN+LSTM architecture\n(MCB [17] and SAN [18]) are compared with human radiologists on the VQA-RAD dataset. For drug label identiÔ¨Åcation\ntasks, the image-text embedding (DLI-IT) [ 19] derived from an VGG+BiLSTM architecture, greatly outperforms\nimage-based or text-based identiÔ¨Åcation methods. Yan et al. [20] won the Ô¨Årst place in ImageCLEF 2019 VQA-Med\n[21] competition using the \"VGG16+BERT+MFB\" model. Unlike conventional architectures using RNN for text\nembedding, they encode the semantic questions using the last and penultimate hidden states of the BERT model.\nChest X-Ray datasets are widely used in V+L researches. The joint image-text embedding can be learned from easily\naccessible Chest X-ray images and free-text radiology reports. TieNet [22], an end-to-end CNN+RNN architecture,\nis developed to learn a blend of distinctive image and text representations from the ChestXray14 [23] dataset and are\nfurther used for thoracic Ô¨Åndings classiÔ¨Åcation and automatic X-ray report generation. Zhang et al. [ 24] achieve better\ninterpretation for OpenI [25] Chest X-Ray dataset by using dual attention mechanisms to distill visual features with\nsemantic features. Hsu et al. [26] explore supervised and unsupervised methods to learn joint image-text embedding\nfrom MIMIC-CXR [27] dataset. The joint representations are further examined by a document retrieval task.\nWe observe that there is a signiÔ¨Åcant gap between the models for joint embedding generation. Transformer-based pre-\ntrained models are widely used in the general domain while in the biomedical domain, CNN-RNN based architectures\nare still predominating. Our contributions are three-fold: (1) We compare 4 pre-trained Transformer-based V+L\nmodels of learning joint image-text embedding from chest radiographs and associated reports; (2) We conduct extrinsic\nevaluation to compare these V+L models with a pioneering CNN-RNN model (TieNet); (3) We explore the advantage\nof joint embedding over text-only embedding, pre-training over train-from-scratch and other model components by a\ndetailed ablation study.\n2 Methodology\nIn this section, we introduce how to Ô¨Åne-tune pre-trained V+L models for a multi-label thoracic Ô¨Åndings classiÔ¨Åer from\nChest X-ray images and associated reports. This experiment can be considered as a visual question answering (VQA)\ntask. The workÔ¨Çow is illustrated in Fig. 1.\n2.1 Data\n2.1.1 MIMIC-CXR\nMIMIC-CXR[27] is the largest publicly available Chest X-ray dataset that contains 377,110 radiographs and 227,827\nfree-text radiology associated reports. 13 thoracic Ô¨Åndings (detailed in the ‚ÄòFinding‚Äô column of Table 3) and ‚Äò No\nFinding‚Äô (absence of all 13 Ô¨Åndings) are derived from radiology reports by using 2 auto-annotators: CheXpert [28] and\nNegBio [29]. Each label is categorized into one of four classes: positive, negative, uncertain and missing.\nWe pre-process the data as follows. First, as one radiology report can be associated with multiple radiographs, we\nonly keep the Ô¨Årst frontal view radiograph in chronological order. Those reports without any frontal view radiographs\nare discarded. Then, we re-categorize the labels of radiology reports from 4 classes: positive, negative, uncertain\nand missing to 2 classes: positive and all others. This step helps us keep our processed labels consistent with other\npublic chest X-ray datasets (e.g. ChestXRay14, Open I). In total, we have 222,713 image-report pairs and each pair\ncorresponds to 14 binary labels. The sample numbers and prevalence of each Ô¨Ånding are shown in the #,% columns in\nTable 3.\n2.2 Open I\nOpenI[30] is another publicly available chest X-ray dataset collected by Indiana University. The dataset contains 3,996\nradiology reports associated with 8,121 images. Unlike MIMIC-CXR, which is annotated by auto-annotators, OpenI\ndataset is labeled by medical professionals manually using Medical Subject Heading [31] (MeSH) indexing. We apply\nthe same pre-processing steps as TieNet and yielded 3,684 image-report pairs. Each pair is assigned multiple MeSH\nterms by human-annotators. For comparison purposes, we only keep the Ô¨Åndings that also present in MIMIC-CXR. The\npositive pair sample numbers and prevalence of selected Ô¨Åndings are shown in the #,% columns in Table 2.\n2\nA PREPRINT - SEPTEMBER 4, 2020\nFigure 1: Overview of applying V+L models to learn joint image-text embedding from CXR images and associated\nreports for thoracic Ô¨Åndings identiÔ¨Åcation.\nfinal report indication : ___ year old man with respiratory failure and multifocal pnas/p pea arrest // interval change comparison : radiographs from ___ a ___ a.m.. impression : support lines and tubes are unchanged in position. heart size is upper limits of normal but stable. there is persistent pulmonary edema with a more focal area of increased density in the left long which has increased since prior. there is a developing left retrocardiac opacity. no pneumothoraxes.\nMIMIC CXR / Open I\nReports\nRadiographs\nTokenizer\nVisual Extractor\nWordPiece\nBUTD or CheXNet\nTokenEmbedding[CLS][final][report][final]\n[##ra][##xes][SEP]‚Ä¶\nVisual TokenEmbedding‚Ä¶\nPositionEmbedding[0][1][2][3]\n[107][108][109]‚Ä¶\n++++\n+++\nLocationEmbedding[ùíôùüèùüé,ùíöùüèùüé,ùíôùüêùüé,ùíöùüêùüé]\n[ùíôùüèùüè,ùíöùüèùüè,ùíôùüêùüè,ùíöùüêùüè]\n[ùíôùüèùüëùüí,ùíöùüèùüëùüí,ùíôùüêùüëùüí,ùíöùüêùüëùüí]\n[ùíôùüèùüëùüì,ùíöùüèùüëùüì,ùíôùüêùüëùüì,ùíöùüêùüëùüì]\n+\n+\n+\n+\nTypeEmbedding[Text][Text]\n[Text]\n[Text]\n[Text]‚Ä¶\n++++\n+++ [Text]\n[Text]\n‚Ä¶\nTypeEmbedding[Image]\n[Image]\n[Image]\n[Image]\n+\n+\n+\n+ ‚Ä¶\nVisualBERT/ LXMERT / UNITER / PixelBERT\nPre-trainedV+L modelsImage-TextJoint EmbeddingClassification Head‚Ä¶\n‚úïEnlarged Cardiomediastinum\n‚úîCardiomegaly\n‚úîAirspace Opacity‚úïLung Lesion\n‚úîEdema‚úïConsolidation‚úïPneumonia‚úïAtelectasis‚úïPneumothorax‚úïPleural Effusion‚úïPleural Other‚úïFracture\n‚úîSupport Devices\nTable 1: A comparison across four pre-trained vision-and-language models and two language representation models.\nModel PretrainingDataset Pretraining Tasks* Visual Encoder Location Features Model Architecture**TransformerStreams BaselinePerformance***\nUNITER[11] COCO [32]+VG [33]+CC [34]+SBU [35] MLM+MRM+ITM+WRA BUTD [36] [top, left, bottom,right, width, height,area] of ROIs 12 BertLayers Single 72.70 (COCO)\nLXMERT[13] COCO+VG+VQA2.0 [6]+GQA [37]+VG-QA [38]MLM+MRM+ITM+IQA BUTD [top, left, bottom,right] of ROIs\n9 BertLayers for language,5 BertLayers for vision,5 CrossAttLayers forcross-modality\nDouble 72.40 (COCO)\nVisualBERT[12] COCO+VQA2.0 MLM+ITM+Task-speciÔ¨ÅcPretraining BUTD - 12 BertLayers Single 70.80 (COCO)\nPixelBERT[39] COCO+VG MLM+ITM ResNeXt [40](Trainable) - 12 BertLayers Single 74.45 (COCO)\nBERT[41] BookCorpus+English-Wiki[42] MLM+NSP - - 12 BertLayers Single 77.6 (MedNLI)\nClinicalBERT[43] MIMIC-III [44] MLM+NSP - - 12 BertLayers Single 80.8 (MedNLI)\n*MLM: Mask Languae Modeling; MRM: Mask Region Modeling: ITM: Image-Text Matching; WRA: Word-Region Alignment;IQA: Image Question Answering; NSP: Next sentence prediction.**BertLayer is a self-attention layer followed by a feed-forward layer; CrossAttLayer is a cross-attention layer followed by a feed-forward layer.***For V+L models, we report the accuracy performed on test-dev of COCO; For language representation models, we report the accuray performed on MedNLI [45]\n2.3 Models\nWe investigate 4 trending vision-and-language models: VisualBERT, UNITER, LXMERT, and PixelBERT. A detailed\ncomparison of these models is outlined in Table 1. In summary, all 4 V+L models use the COCO [32] dataset as their\npre-training material. Some other data sources, such as Visual Genome [ 33], VQA 2.0 [21] are added to enrich the\ntraining materials. Inspired by the masked language modeling (MLM) and next-sentence prediction of the BERT model,\nMLM conditioned on images and image-text matching (IMT) are adopted by all 4 V+L models. Only VisualBERT\nrequires task-speciÔ¨Åc training to improve the performance on downstream tasks. Bottom-up top-down (BUTD) [36],\nadopted by VisualBERT, UNITER, and LXMERT, is the most common region-based visual encoder in V+L models.\nHowever, region-based visual encoders are designed for speciÔ¨Åc visual tasks and are limited by the given categories of\nthe object detection tasks. PixelBERT creatively solves the limitation of region-based visual representation by proposing\na CNN-based visual encoder. Note that the self-attention mechanism in the Transformer model is order-less, thus\n3\nA PREPRINT - SEPTEMBER 4, 2020\nTable 2: AUCs of identifying 7 thoracic Ô¨Åndings from OpenI dataset using TieNet, 4 pre-trained vision-and-language\nmodels, and 2 language representation models.\nFindings Image+Report Report # %TieNet-I+R VisualBERT LXMERT UNITER PixelBERT TieNet-R BERT ClinicalBERT\nCardiomegaly 0.962 0.977 0.980 0.978 0.970 0.944 0.976 0.969 315 8.55Edema 0.995 0.982 0.995 0.989 0.995 0.984 0.987 0.976 40 1.09Consolidation 0.989 0.996 0.993 0.998 0.975 0.969 0.978 0.982 28 0.76Pneumonia 0.994 0.990 0.990 0.988 0.969 0.983 0.970 0.982 36 0.98Atelectasis 0.972 0.992 0.989 0.982 0.959 0.981 0.927 0.947 293 7.95Pneumothorax 0.960 0.988 0.958 0.983 0.982 0.960 0.930 0.973 22 0.60Pleural Effusion 0.977 0.985 0.983 0.983 0.970 0.968 0.961 0.976 140 3.80\nAverage 0.978 0.987 0.984 0.986 0.974 0.970 0.961 0.972\n#w Average* 0.971 0.985 0.984 0.982 0.968 0.965 0.956 0.964\n*#w Averaged denotes the sample number weighted average of AUC.\nUNITER and LXMERT employ one layer of location features in addition to the original visual features. LXMERT is the\nonly model that has two transformer streams that learns visual and text embedding separately, and further incorporates\nthem with cross-attention layers to achieve joint embedding.\nPreliminary results show that TieNet-R, an attention-based LSTM model that only needs contextual input, also yielded\na competitive result. This is in line with expectations since radiology reports are generated by experienced radiologists\nthat contain richer and more interpretative information compared with images. Moreover, the Ô¨Ånding labels are\nauto-annotated by CheXpert, a rule-based text classiÔ¨Åer, on radiology reports. Therefore, it is natural that applying\ntext-only embedding methods also achieves a good performance. To this end, BERT and ClinicalBERT [ 43], two\npre-trained language representation models, are also involved in our study for the purpose of exploring the advantages of\nimage-text joint embedding over text-only embedding. These two models have the same Transformer-based architecture\nas PixelBERT, VisualBERT, and UNITER. BERT obtained state-of-the-art results on 11 NLP tasks when released.\nClinicalBERT, trained on MIMIC-III [44] dataset, enhances adaptability of BERT for the clinical domain and yields\nperformance improvements on clinical NLP tasks.\n2.4 Implementations\nWe Ô¨Årst Ô¨Åne-tune the 4 V+L models to learn a joint image-text embedding (the last hidden-state of [CLS] token) from\nthe training set of MIMIC-CXR. A classiÔ¨Åcation head is added on top of the joint embedding to generate the probability\nestimates of 13 thoracic Ô¨Åndings. We use the held-out testing set of MIMIC-CXR for internal evaluation and entire\nOpenI dataset for external evaluation. It should be noted that we do not follow the ofÔ¨Åcial splits of MIMIC-CXR.\nInstead, we perform stratiÔ¨Åed split and use 80% of the entire dataset as the training set, 10% as developing set for\nTable 3: AUCs of identifying 13 thoracic Ô¨Åndings from MIMIC-CXR dataset by Ô¨Åne-tuning 4 pre-trained vision-and-\nlanguage models and 2 language representation models. Multiple conÔ¨Ågurations of each model are applied for ablation\nstudies usage.\nFindings Image+Report Report # %LXMERT VisualBERT UNITER PixelBERT BERT Clinical-BERT\nConÔ¨Åg*Default FS CB Default FS CB Default FS CB Default NT\nEnlarged-Cardiomediastinum0.974 0.971 0.971 0.981 0.856 0.977 0.979 0.866 0.962 0.870 0.724 0.747 0.966 7,025 3.15\nCardiomegaly 0.987 0.988 0.988 0.991 0.968 0.990 0.989 0.974 0.981 0.965 0.814 0.954 0.979 43,931 19.73Airspace Opacity 0.988 0.988 0.9880.991 0.977 0.989 0.989 0.981 0.984 0.971 0.922 0.972 0.978 50,238 22.56Lung Lesion 0.983 0.963 0.963 0.985 0.947 0.983 0.981 0.948 0.962 0.951 0.723 0.911 0.972 6,312 2.83Edema 0.988 0.989 0.988 0.991 0.975 0.990 0.990 0.978 0.985 0.958 0.790 0.956 0.979 26,309 11.81Consolidation 0.985 0.986 0.986 0.989 0.967 0.986 0.988 0.974 0.981 0.973 0.772 0.947 0.979 10,507 4.72Pneumonia 0.969 0.971 0.971 0.977 0.932 0.973 0.974 0.948 0.961 0.942 0.741 0.886 0.962 16,539 7.43Atelectasis 0.985 0.986 0.986 0.988 0.975 0.986 0.987 0.978 0.983 0.971 0.797 0.965 0.976 44,887 20.15Pneumothorax 0.990 0.985 0.985 0.992 0.975 0.991 0.991 0.981 0.985 0.942 0.827 0.942 0.979 10,432 4.68Pleural Effusion 0.990 0.991 0.991 0.993 0.982 0.992 0.992 0.984 0.988 0.972 0.945 0.967 0.981 52,890 23.75Pleural Others 0.983 0.969 0.969 0.981 0.946 0.973 0.973 0.956 0.963 0.930 0.735 0.815 0.964 1,949 0.88Fracture 0.979 0.976 0.976 0.976 0.960 0.975 0.977 0.956 0.961 0.941 0.602 0.873 0.958 4,425 1.99Support Devices 0.993 0.992 0.992 0.995 0.985 0.993 0.994 0.986 0.990 0.979 0.949 0.970 0.983 65,235 29.29\nAverage 0.984 0.981 0.981 0.987 0.957 0.984 0.985 0.962 0.976 0.953 0.795 0.916 0.974\n*ConÔ¨Ågurations for ablation studies: Default: default conÔ¨Åguration; FS: training from scratch without loading pre-trained model weights;CB: replacing the tokenizer, vocabulary and embedding layer with those of ClinicalBERT; NT: freezing weights of the visual backbone.\n4\nA PREPRINT - SEPTEMBER 4, 2020\nFigure 2: Visualization of the selected attention maps learned by the VisualBERT model.\n0\n0.1\n0.2\n0.3\n0.4ImageText ImageText\nText Text Text\nImage Image Image\nImageText\na) Attention map of head #12 at the 1stLayerb) Attention map of head #3 at the 6thLayerc) Attention map of head #3 at the 11thLayer\nhyper-parameter selection, and the rest 10% for held-out internal evaluation. TieNet [22], a pioneering joint embedding\nmodel used RNN-CNN architecture, is set as a baseline in our external evaluation. Areas Under Receiver Operating\nCurves (AUC) is computed based on the probability estimates and annotated labels to evaluate the model performance.\nWe follow the evaluations in TieNet and report averaged AUC and sample number weighted average of AUC for\ncomparison.\nFor detailed implementation, each text sequence is truncated or padded to 128 tokens in length. We apply a special\nedition of BUTD visual encoder for UNITER, LXMERT and VisualBERT, which consistently extracts 36 objects\nfrom a single image. As the pre-trained weights of PixelBERT are not open-sourced, we adopt CheXNet [ 46] as a\nsubstitute of the ResNeXt as the visual backbone and initialize the BertLayers with pre-trained BERT weights. The\nfeature map (7x7x1024) of CheXNet is Ô¨Årst Ô¨Çattened by spatial dimensions (49x1024) and then down-sampled to 36\n1024-long visual features. We follow the same setting as [ 39] that trains the PixelBERT for 18 epochs. All other 3\nV+L models as well as BERT and ClinicalBERT are Ô¨Åne-tuned for 6 epochs as the pre-trained weights can be loaded.\nSGD optimizer, with weight decay 5e-4 and learning rate 0.01 scheduled decay by 10 at the 12 th and 16th epoch, is\nused to optimize the CheXNet backbone in PixelBERT. AdamW is used to optimize the Transformer block(s) in all\nmodels. Each model can be Ô¨Åt into 1 Tesla K40 GPU when using a batch size of 16. Our implementation can be found\nat: https://github.com/YIKUAN8/Transformers-VQA\n3 Results\nThe model performance on OpenI dataset are shown in Table 2. All V+L models yield a high AUC of over 0.958 for all\nÔ¨Åndings. VisualBERT, LXMERT and UNITER achieve higher averaged AUC (0.984+) and sample number weighted\nAUC (0.982+) than TieNet-I+R (0.978 and 0.971) in OpenI dataset. While PixelBERT, without any pre-training,\nperforms slightly worse than TieNet-I+R. The supreme performance demonstrates all V+L models‚Äô great adaptability to\nbiomedical domain. When digging into a particular Ô¨Ånding, TieNet only outperforms V+L models in the classiÔ¨Åcation\nof edema and pneumonia.\nWe use the held-out test split of MIMIC-CXR to make a more comprehensive comparison of 4 V+L models. The results\nare revealed in the default column of Table 3. VisualBERT yields the best averaged AUC of 0.987, followed by 0.985\nusing UNITER and 0.984 using LXMERT. PixelBERT only achieves a moderate AUC of 0.953 due to the absence\nof the pre-training. When comparing the performance on different Ô¨Åndings, we surprisingly Ô¨Ånd that VisualBERT\nachieves the best results on 11 out of 13 Ô¨Åndings. Although VisualBERT adopts the least pre-training tasks and the\nsimplest model architecture, we believe the introduction of task-speciÔ¨Åc training for VQA contributes to this optimal\nperformance. LXMERT outperforms others in Pleural Others and Fracture, which are the two least prevalent Ô¨Åndings.\nWe also notice that it is harder to identifyEnlarged Cardiomediastinum compared with other abnormalities. We attribute\nthis to the absence of lateral view X-ray images in our training set in that the cardiomediastinum contour is easier to\nrecognize from a lateral view. Moreover, it is as expected that UNITER does not have a supreme performance in our\nstudy as we adopt the base version of UNITER. UNITERlarger is much larger than UNITERbase in both number of layers\nand trainable parameters. We believe that UNITERlarger can achieve better results given its phenomenal performance in\ngeneral domain.\n5\nA PREPRINT - SEPTEMBER 4, 2020\nFigure 3: Results of three classiÔ¨Åcation samples. Top 10 tokens with highest attention scores are highlighted in each\nsample. Words in grey are truncated during tokenization.\n[CLS] There is a large hydropneumothoraxwithin the left chest. There is essentially complete collapseof the left lung. Within the right lung, there are increased interstitial opacities within the medial right lung base and right upper lobe, with patchy airspace opacity within the right lung apex. At the right lung apex, there is a more focal ovoid lucencywhich measures approximately 1.3 cm. This could indicate cavitation. Left-sided cardiomediastinalcontours are obscured by collapse of the left lung. No convincing acute bony findings. [SEP] 1. Large left hydropneumothorax, with complete collapse of the left lung. 2. Airspace and interstitial opacity within the right upper and lower lobes. Possible apical cavitation. Tuberculosis should be excluded clinically\n[CLS] Cardiomegaly is present. There is interstitial pulmonary edema with the presenceof XXXX B-lines. There is no pneumothorax. There is an oval, 17 mm nodular opacityprojecting between the posterior left 5th and 6th ribs. There isa10 mm nodular density projecting over the right posterior 4th rib. There is a XXXX posterior effusion. Normal mediastinal silhouette. T-spineosteophytes. 1. Cardiomegaly with mild interstitial edema and XXXX posterior[SEP]pleural effusion. 2. 17 mm nodular opacity in the left lung and 10 mm nodular opacity in the right lung. These lesions are XXXX and could be followed up radiographically after treatment of edema, or could be further characterized with CT.vtok#8\nb)  Top 10 tokens of a false positive case\nc)  Top 10 tokens of a false negative case\nFindings: AtelectasisIdentified Findings by VisualBERT: Atelectasis and Pneumothorax\nFindings: Cardiomegaly, Edema and Pleural Effusion  Identified Findings by VisualBERT: Edema\n[CLS] The left lung is grossly clear. The right lung demonstrates a large right pleural effusion with associated atelectatic collapseof the right middle lobe and partial collapse of the right lower lobe. XXXX opacities are seen within the aerated right lung, XXXX subsegmental atelectasis. No focal consolidation orpneumothorax identified. No acute osseous abnormality. Cardio mediastinal silhouette is stable compared to prior examinations. Large right pleural effusion with associated passive atelectasis of the right middle and lower lobes. Grossly clear [SEP] left lungvtok#9\nvtok#6\na)  Top 10 tokens of a correctly identified case\nFindings: Atelectasis and Pleural Effusion Identified Findings by VisualBERT: Atelectasis and Pleural Effusion \nvtok#2\nNext, we compare the advantage of using image-text joint embedding over text-only embedding. When using joint\nembedding, TieNet only receives less than 1% performance improvement over the text-only embedding in the OpenI\ndataset. While, VisualBERT, LXMERT, and UNITER receives more than 2 % improvement over BERT and 1 %\nover ClinicalBERT, respectively. This larger performance improvement demonstrates that transformer-based models\nare better at image-text interaction compared with the CNN-RNN architecture. In the test split of MIMIC-CXR, the\nimprovement of V+L models over BERT model is even larger (more than 6%).\nAdditionally, although VisualBERT, LXMERT, and UNITER have different model architectures, embedding, and\npre-training strategies, they all yield comparable results in both internal held-out evaluation and external data resources.\nThis is in line with expectation because these V+L models all have supreme performance and rank top 10 in the general\ndomain (evaluated by coco test-dev). This also indicates that although BUTD is pre-trained in general domain from\nVisual Genome, it could also extract useful ROIs from biomedical images.\n6\nA PREPRINT - SEPTEMBER 4, 2020\nTable 4: Layer-level modality importance scores. A lower score indicates that [CLS] token spends less attention on the\nmodality.\nLayer# 1 2 3 4 5 6 7 8 9 10 11 12\nTextual Modality Importance 0.828 0.993 0.941 0.824 0.844 0.908 0.724 0.733 0.812 0.864 0.922 0.673Visual Modality Importance 0.170 0.007 0.055 0.103 0.147 0.082 0.254 0.206 0.172 0.134 0.075 0.324\n4 Discussion\nIn this section, we Ô¨Årst present ablation studies to analyze the contribution of certain model components. Then, we use\nvisualization to illustrate the attention details. Finally, we discuss the limitations of our current work and our plans for\nfurther studies.\n4.1 Pre-training versus Training From Scratch\nWe explore whether pre-training strategies and tasks that are utilized in the general domain contribute to our joint\nrepresentation learning in the biomedical domain. As a comparative study, we do not load the pre-trained weights and\nrandomly initialize the weights for VisualBERT, LXMERT, and UNITER. These 3 V+L models are directly trained to\nclassify thoracic Ô¨Åndings without any pre-training. We train these models for 12 epochs. The results of training from\nscratch are shown in the FS columns of Table 2 and are compared to the Default columns. The pre-training procedure\nimproves the averaged AUC of VisualBERT by 3%, UNITER by 2.3% and LXMERT by 2%. This conÔ¨Årms that the\npre-training tasks contribute to generating more meaningful joint embedding. In the contrast, PixelBERT achieves\nstate-of-the-art results in many NLP tasks in general domain, our train-from-scratch PixelBERT underperforms the\nother 3 pre-trained model signiÔ¨Åcantly. This also indicates that pre-training is of great importance to multimodal\nrepresentation learning.\n4.2 BERT versus ClinicalBERT\nWe observe that ClinicalBERT outperforms BERT by 5.8% in MIMIC-CXR and 1.1% in OpenI. It is as expected\nin that ClinicalBERT is pre-trained using MIMIC-III which is also released by PhysioNet. Compared with BERT,\nClinicalBERT adapts better to biomedical data. Inspired by this, we replace the BERT vocabulary and embedding\nlayer in VisualBERT, LXMERT and UNITER by clinicalBERT. In terms of implementation, we load all the pre-trained\nparameters except the text embedding layer to those 3 V+L models. We also train the models for 12 epochs in hope of\nmore training epochs will help V+L models adapt to the new vocabulary and embedding layer. The results of using\nClinicalBERT components are shown in the CB columns of Table 3. Compared with the default conÔ¨Åguration, we do\nnot observe any performance boosting. We believe the 12 training epochs are still not sufÔ¨Åcient to adjust the parameters\nto adapt the a new language representation model.\n4.3 Trainable Visual Backbone versus Frozen Visual Backbone\nWe are also interested in whether a trainable visual CNN backbone is beneÔ¨Åcial. This experiment is also conducted in the\noriginal work of PixelBERT. We adopt another conÔ¨Åguration of PixelBERT by freezing all the layers of CheXNet, the\nvisual backbone CNN. All other hyper-parameters and technical details remain the same. The results of the PixelBERT\nwith a frozen CNN backbone can be found in the NT column of Table 3. PixelBERT with frozen CheXNet only yields\na moderate averaged AUC of less than 0.8, which underperforms the default conÔ¨Åguration of more than 15%. This\nindicates that it is very important to adjust the weights of the visual backbone model when applying feature map of the\nCNN model to generate image representations.\n4.4 Visualization\nSimilar to [47], we Ô¨Årst visualize selected attention maps of the best performed VisualBERT to illustrate the attention\nmechanism. In a), b) and c) of Fig. 2, we observe several interesting patterns. Fig. 2a shows a block pattern. Only\nintra-modality (text self-attention and image self-attention) can be observed in this attention map. Fig. 2b has a\nvertical pattern that illustrates Text-to-Image and Text-to-Text attentions in this layer. Besides, we can see some special\ntokens are heavily attended. Fig. 2c visualizes the attention scores of a head in the penultimate layer. Text-to-Image,\nImage-to-Text and Text self-attention can be found in this attention map. When comparing the difference of these 3\nattention maps, we observe there is a trending that the deeper layers demonstrate more interaction between image and\ntext, while self-attention predominate in the beginning several layers. Furthermore, to quantitatively analyze the [CLS]\nattention trace, we calculate the layer-level Modality Importance (MI) scores introduced in [48] of textual and visual\n7\nA PREPRINT - SEPTEMBER 4, 2020\nmodality, respectively. The result is shown in Table 4. We can see that layer-level MI scores for text modality are\nsigniÔ¨Åcantly higher than those for visual modality, especially at intermediate layers. This suggests that more attention\nheads learn useful knowledge from the textual modality than image modality.\n4.5 Error Analysis\nFor error analysis analysis, the top 10 tokens across all heads and layers with highest attention scores attended by the\n[CLS] token are highlighted in the Fig. 3. All three cases are sampled from OpenI. In Fig. 3a, a sample diagnosed\nwith Atelectasis, Pleural Effusion is correctly classiÔ¨Åed. We can observe that keywords, such as atelectatic collapse,\nopacity, effusion, display high attention by the classiÔ¨Åcation token. Visual token #9, which precisely targets the contour\nof the collapse of the right middle lobe, also greatly contributes to the classiÔ¨Åcation. On the contrary, Fig. 3b illustrates\na false negative sample. This sample is wrongly classiÔ¨Åed to positive Pneumothorax by our model. As shown in the\nradiology report of this sample, there is a large hydropneumothorax within the left lung. Given that all V+L models\nuse WordPiece, which is a subword algorithm, hydropneumothorax is tokenized to a superset of pneumothorax. The\ntokenization mechanism of V+L models results in this kind of false positive classiÔ¨Åcation. Fig. 3c demonstrates a false\nnegative sample. This patient is diagnosed with Cardiomegaly, Edema, and Pleural Effusion. However, only Edema is\nidentiÔ¨Åed by our classiÔ¨Åer. When digging into the speciÔ¨Åc tokens, no Ô¨Ånding-related words but nodular opacity receive\na high attention score. Pleural Effusion in the context is truncated as we set the maximum text sequence to 128. While,\nwe can see that visual token #8 precisely targets thoracic-spine and T-spine in radiology report also has a high attention\nscore. This demonstrates the strong image-text alignment ability of V+L models.\n4.6 Limitation and Future Work\nOur study has several limitations, each of which brings a great opportunity for future work. First, we barely tune\nthe hyper-parameters or CNN backbone architectures of the V+L models, which has the potential of achieving better\nperformance. For example, we could apply deeper CNN backbones, such as ResNet152 to extract visual features\nfor PixelBERT. Additionally, due to the limitation of computation resources, we are unable to implement domain-\nspeciÔ¨Åc pre-training. According to the previous studies [12, 13], domain-speciÔ¨Åc pre-training will enhance the model‚Äôs\nadaptability and therefore improve the performance of domain-speciÔ¨Åc tasks. Finally, we only apply the pre-trained\nV+L models to chest X-ray datasets. We are planning to examine these models on a wider range of biomedical V+L\ndatasets and tasks, such as BCIDR [15], VQA-Med [21] with more conÔ¨Ågurations as our next step.\n5 Conclusion\nIn this study, we examine the ability of four transformer-based V+L models in learning joint image-text embedding from\nchest X-ray images and associated reports. The extrinsic evaluation shows that pre-trained V+L models outperform\ntraditional CNN-RNN methods. Ablation studies demonstrate the advantage of joint embedding over text embedding\nand the beneÔ¨Åts of the pre-training procedures. Future studies will focus on pre-training V+L models using biomedical\ndataset and down-stream the pre-trained models to more biomedical V+L tasks.\n6 Acknowledgment\nThis study is supported in part by NIH grant 1R01LM013337.\nReferences\n[1] Dongfei Yu, Jianlong Fu, Tao Mei, and Yong Rui. Multi-level attention networks for visual question answering.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4709‚Äì4717, 2017.\n[2] Jyoti Aneja, Aditya Deshpande, and Alexander G Schwing. Convolutional image captioning. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition, pages 5561‚Äì5570, 2018.\n[3] Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He. Stacked cross attention for image-text\nmatching. In Proceedings of the European Conference on Computer Vision (ECCV), pages 201‚Äì216, 2018.\n[4] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with\na general conditioning layer. In Thirty-Second AAAI Conference on ArtiÔ¨Åcial Intelligence, 2018.\n[5] Anna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor Darrell, and Bernt Schiele. Grounding of textual phrases\nin images by reconstruction. In European Conference on Computer Vision, pages 817‚Äì834. Springer, 2016.\n8\nA PREPRINT - SEPTEMBER 4, 2020\n[6] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi\nParikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision,\npages 2425‚Äì2433, 2015.\n[7] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 3128‚Äì3137, 2015.\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional\ntransformers for language understanding. CoRR, abs/1810.04805, 2018.\n[9] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V . Le. Xlnet:\nGeneralized autoregressive pretraining for language understanding. CoRR, abs/1906.08237, 2019.\n[10] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and\nIllia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017.\n[11] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu.\nUniter: Learning universal image-text representations. arXiv preprint arXiv:1909.11740, 2019.\n[12] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and\nperformant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019.\n[13] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers. arXiv\npreprint arXiv:1908.07490, 2019.\n[14] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic repre-\nsentations for vision-and-language tasks. In Advances in Neural Information Processing Systems, pages 13‚Äì23,\n2019.\n[15] Zizhao Zhang, Pingjun Chen, Manish Sapkota, and Lin Yang. Tandemnet: Distilling knowledge from medical\nimages using diagnostic reports as optional semantic references. In International Conference on Medical Image\nComputing and Computer-Assisted Intervention, pages 320‚Äì328. Springer, 2017.\n[16] Jason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. A dataset of clinically generated\nvisual questions and answers about radiology images. ScientiÔ¨Åc data, 5(1):1‚Äì10, 2018.\n[17] Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. Multimodal\ncompact bilinear pooling for visual question answering and visual grounding. arXiv preprint arXiv:1606.01847,\n2016.\n[18] Zichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image\nquestion answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n21‚Äì29, 2016.\n[19] Xiangwen Liu, Joe Meehan, Weida Tong, Leihong Wu, Xiaowei Xu, and Joshua Xu. Dli-it: a deep learning\napproach to drug label identiÔ¨Åcation through image and text embedding. BMC Medical Informatics and Decision\nMaking, 20:1‚Äì9, 2020.\n[20] Xin Yan, Lin Li, Chulin Xie, Jun Xiao, and Lin Gu. Zhejiang university at imageclef 2019 visual question\nanswering in the medical domain. In CLEF (Working Notes), 2019.\n[21] Asma Ben Abacha, Sadid A Hasan, Vivek V Datla, Joey Liu, Dina Demner-Fushman, and Henning M√ºller.\nVqa-med: Overview of the medical visual question answering task at imageclef 2019. In CLEF (Working Notes),\n2019.\n[22] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, and Ronald M Summers. Tienet: Text-image embedding\nnetwork for common thorax disease classiÔ¨Åcation and reporting in chest x-rays. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 9049‚Äì9058, 2018.\n[23] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers. Chestx-ray8:\nHospital-scale chest x-ray database and benchmarks on weakly-supervised classiÔ¨Åcation and localization of\ncommon thorax diseases. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 2097‚Äì2106, 2017.\n[24] Zizhao Zhang, Pingjun Chen, Xiaoshuang Shi, and Lin Yang. Text-guided neural network training for image\nrecognition in natural scenes and medicine. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n2019.\n[25] Dina Demner-Fushman, Sameer Antani, Matthew Simpson, and George R Thoma. Design and development\nof a multimodal biomedical information retrieval system. Journal of Computing Science and Engineering ,\n6(2):168‚Äì177, 2012.\n9\nA PREPRINT - SEPTEMBER 4, 2020\n[26] Tzu-Ming Harry Hsu, Wei-Hung Weng, Willie Boag, Matthew McDermott, and Peter Szolovits. Unsupervised\nmultimodal representation learning across medical images and reports. arXiv preprint arXiv:1811.08615, 2018.\n[27] Alistair EW Johnson, Tom J Pollard, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Yifan Peng,\nZhiyong Lu, Roger G Mark, Seth J Berkowitz, and Steven Horng. Mimic-cxr-jpg, a large publicly available\ndatabase of labeled chest radiographs. arXiv preprint arXiv:1901.07042, 2019.\n[28] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik Marklund,\nBehzad Haghgoo, Robyn Ball, Katie Shpanskaya, et al. Chexpert: A large chest radiograph dataset with uncertainty\nlabels and expert comparison. In Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence, volume 33, pages\n590‚Äì597, 2019.\n[29] Yifan Peng, Xiaosong Wang, Le Lu, Mohammadhadi Bagheri, Ronald Summers, and Zhiyong Lu. Negbio: a\nhigh-performance tool for negation and uncertainty detection in radiology reports.AMIA Summits on Translational\nScience Proceedings, 2018:188, 2018.\n[30] Dina Demner-Fushman, Marc D Kohli, Marc B Rosenman, Sonya E Shooshan, Laritza Rodriguez, Sameer Antani,\nGeorge R Thoma, and Clement J McDonald. Preparing a collection of radiology examinations for distribution and\nretrieval. Journal of the American Medical Informatics Association, 23(2):304‚Äì310, 2016.\n[31] Carolyn E Lipscomb. Medical subject headings (mesh). Bulletin of the Medical Library Association, 88(3):265,\n2000.\n[32] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, and\nC Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision,\npages 740‚Äì755. Springer, 2014.\n[33] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis\nKalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced\ndense image annotations. International journal of computer vision, 123(1):32‚Äì73, 2017.\n[34] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed,\nimage alt-text dataset for automatic image captioning. InProceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 2556‚Äì2565, 2018.\n[35] Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering in images.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4995‚Äì5004, 2016.\n[36] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang.\nBottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 6077‚Äì6086, 2018.\n[37] Drew A Hudson and Christopher D Manning. Gqa: a new dataset for compositional question answering over\nreal-world images. arXiv preprint arXiv:1902.09506, 3(8), 2019.\n[38] Vicente Ordonez, Girish Kulkarni, and Tamara L Berg. Im2text: Describing images using 1 million captioned\nphotographs. In Advances in neural information processing systems, pages 1143‚Äì1151, 2011.\n[39] Zhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. Pixel-bert: Aligning image pixels with\ntext by deep multi-modal transformers, 2020.\n[40] Saining Xie, Ross B. Girshick, Piotr Doll√°r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations\nfor deep neural networks. CoRR, abs/1611.05431, 2016.\n[41] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[42] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler.\nAligning books and movies: Towards story-like visual explanations by watching movies and reading books. In\nProceedings of the IEEE international conference on computer vision, pages 19‚Äì27, 2015.\n[43] Emily Alsentzer, John R Murphy, Willie Boag, Wei-Hung Weng, Di Jin, Tristan Naumann, and Matthew\nMcDermott. Publicly available clinical bert embeddings. arXiv preprint arXiv:1904.03323, 2019.\n[44] Alistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-Wei, Mengling Feng, Mohammad Ghassemi,\nBenjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical\ncare database. ScientiÔ¨Åc data, 3(1):1‚Äì9, 2016.\n[45] Alexey Romanov and Chaitanya Shivade. Lessons from natural language inference in the clinical domain. arXiv\npreprint arXiv:1808.06752, 2018.\n10\nA PREPRINT - SEPTEMBER 4, 2020\n[46] Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony Duan, Daisy Yi Ding, Aarti\nBagul, Curtis Langlotz, Katie S. Shpanskaya, Matthew P. Lungren, and Andrew Y . Ng. Chexnet: Radiologist-level\npneumonia detection on chest x-rays with deep learning. CoRR, abs/1711.05225, 2017.\n[47] Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. Revealing the dark secrets of bert.arXiv\npreprint arXiv:1908.08593, 2019.\n[48] Jize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-Chun Chen, and Jingjing Liu. Behind the scene: Revealing the\nsecrets of pre-trained vision-and-language models. arXiv preprint arXiv:2005.07310, 2020.\n11",
  "topic": "Embedding",
  "concepts": [
    {
      "name": "Embedding",
      "score": 0.8162128925323486
    },
    {
      "name": "Computer science",
      "score": 0.7076825499534607
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6483643651008606
    },
    {
      "name": "Representation (politics)",
      "score": 0.5682094693183899
    },
    {
      "name": "Natural language processing",
      "score": 0.5552011132240295
    },
    {
      "name": "Joint (building)",
      "score": 0.5475708842277527
    },
    {
      "name": "Multimodal learning",
      "score": 0.5032541155815125
    },
    {
      "name": "Task (project management)",
      "score": 0.48022347688674927
    },
    {
      "name": "Image (mathematics)",
      "score": 0.4256865978240967
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4003225564956665
    },
    {
      "name": "Machine learning",
      "score": 0.3522011339664459
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}