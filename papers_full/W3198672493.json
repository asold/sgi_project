{
  "title": "An Empirical Study on Few-shot Knowledge Probing for Pretrained Language Models",
  "url": "https://openalex.org/W3198672493",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2745057305",
      "name": "He, Tianxing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2753220617",
      "name": "Cho, Kyunghyun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2745591341",
      "name": "Glass, James",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3176828726",
    "https://openalex.org/W2962886429",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W3137573489",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3152497014",
    "https://openalex.org/W3152956381",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W3146844750",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W3154903254",
    "https://openalex.org/W3100283070",
    "https://openalex.org/W3132736064",
    "https://openalex.org/W2971869958",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3156170450"
  ],
  "abstract": "Prompt-based knowledge probing for 1-hop relations has been used to measure how much world knowledge is stored in pretrained language models. Existing work uses considerable amounts of data to tune the prompts for better performance. In this work, we compare a variety of approaches under a few-shot knowledge probing setting, where only a small number (e.g., 10 or 20) of example triples are available. In addition, we create a new dataset named TREx-2p, which contains 2-hop relations. We report that few-shot examples can strongly boost the probing performance for both 1-hop and 2-hop relations. In particular, we find that a simple-yet-effective approach of finetuning the bias vectors in the model outperforms existing prompt-engineering methods. Our dataset and code are available at \\url{https://github.com/cloudygoose/fewshot_lama}.",
  "full_text": "An Empirical Study on Few-shot Knowledge Probing for\nPretrained Language Models\nTianxing He\nMIT\ntianxing@mit.edu\nKyunghyun Cho\nNew York University\nkyunghyun.cho@nyu.edu\nJames Glass\nMIT\nglass@mit.edu\nAbstract\nPrompt-based knowledge probing for 1-hop re-\nlations has been used to measure how much\nworld knowledge is stored in pretrained lan-\nguage models. Existing work uses consider-\nable amounts of data to tune the prompts for\nbetter performance. In this work, we com-\npare a variety of approaches under a few-\nshot knowledge probing setting, where only\na small number (e.g., 10 or 20) of example\ntriples are available. In addition, we create\na new dataset named TREx-2p, which con-\ntains 2-hop relations. We report that few-\nshot examples can strongly boost the probing\nperformance for both 1-hop and 2-hop rela-\ntions. In particular, we ﬁnd that a simple-yet-\neffective approach of ﬁnetuning the bias vec-\ntors in the model outperforms existing prompt-\nengineering methods. Our dataset and code\nare available at https://github.com/\ncloudygoose/fewshot_lama.\n1 Introduction\nLarge-scale unsupervised pretraining (Peters et al.,\n2018; Devlin et al., 2018; Song et al., 2019; Yang\net al., 2019; Liu et al., 2019) of language models\n(LMs) has been shown to greatly boost the per-\nformance of a wide variety of natural language\nprocessing (NLP) tasks. It is interesting to wonder\nhow much world knowledge is embedded within\nthese pretrained LMs. In the LAMA benchmark\n(Petroni et al., 2019), templates (e.g., “[X] was born\nin <mask> .”) are used to create natural-language\nprompts for 1-hop relations in an existing knowl-\nedge graph. The accuracy of whether the model\ncan predict the right object token is treated as a\nproxy for how knowledgeable the pretrained LM\nis. This line of investigation (Poerner et al., 2020;\nKassner and Schütze, 2020; Kassner et al., 2021;\nHeinzerling and Inui, 2021) also points to an excit-\ning potential application of using a pretrained LM\nas an implicit knowledge base.\nFigure 1: Few-shot examples can potentially correct\nthe model’s prediction in knowledge probing.\nUnfortunately, the zero-shot performance of\nthe manually created templates from LAMA is\nlow. For example, the BERT-large model only\nhas around 30% accuracy on the T-REx dataset.\nIn our preliminary examinations, we found that in\nmany error cases, the LM predicts the wrong type\nof objects. We illustrate this in Figure 1.\nMotivated by this observation, in this work we\nexplore few-shot knowledge probing, where only a\nsmall number (e.g., 10 or 20) of example triples are\navailable to tune the prompts or model for better\nperformance. This setting is attractive because: (1)\nIntuitively, a few examples are usually enough for\nhumans to infer the precise relation type of interest;\n(2) Few-shot examples enable us to probe for new\nor rare relation types.\nIn our experiments, we conduct a comprehensive\ncomparison of different approaches in the context\nof few-shot knowledge probing. We brieﬂy sum-\nmarize our contributions as follows: (1) We create\na new knowledge probing dataset named TREx-2p,\nwhich contains more challenging 2-hop relations.\n(2) For both 1-hop and 2-hop relations, few-shot\nexamples strongly boost the knowledge probing\nperformance for a pretrained LM. In particular, we\nﬁnd that a simple-yet-effective approach of ﬁne-\ntuning the bias vectors in the model outperforms\narXiv:2109.02772v2  [cs.AI]  11 Sep 2021\nexisting prompt-engineering methods.\n2 Few-shot Knowledge Probing\nWe begin by establishing notations. We denote the\nparameters of a pretrained masked language model\nas θ, and the vocabulary as V. For each relation\ntype r, the probing dataset has a set of knowledge-\nbase triples Dr “txx,r,y yu, where xand yrefers\nto the subject and object, respectively. Since we\nare considering a few-shot setting, each Dr is split\nto Dr\nfewshot and Dr\ntest, where Dr\nfewshot only contain a\nsmall number (e.g., 10 or 20) of triples. Most of\nour approaches involve hyper-parameter tuning, in\nwhich case we further split Dr\nfewshot into Dr\ntrain and\nDr\ndev. Dr\ndev can also be used to prevent over-ﬁtting\n(via early stopping).\nThe task of LM knowledge probing (Petroni\net al., 2019) is to query a pretrained LM for y,\nby feeding it information about xand r. To do so,\na converter function f (to be described below) will\nbe used to convert xand r into a query sentence\nwith exactly one mask token in it, which is then\nfed into the LM. We denote the model’s output dis-\ntribution for the masked token as PLMp¨| fpx,rqq,\nand the performance is reﬂected by the rank of yin\nthat distribution.\nNext, we review available template options, and\ndescribe approaches which utilize the available few-\nshot training and development data to improve the\nperformance of probing. Concrete examples are\nshown in Table 1.\nTemplate Options In Petroni et al. (2019), the\nconverter function (denoted by fmanT) is imple-\nmented via manually created templates, that are\nhand-crafted for each relation type. In Jiang et al.\n(2020), mining or paraphrasing-based methods are\nused to automatically ﬁnd alternative templates\nfor a given relation. They released the generated\ntemplates by the name LPAQA (LM Prompt And\nQuery Archive), which are in the same format as\nin Petroni et al. (2019). For each relation r, we\nselect the best-performing template by comparing\nthe performance of each template on Dr\nfewshot, and\nuse it for the convert function denoted by fmineT.\nBoth manT and mineT require human labor or\nexternal resources, therefore in the few-shot con-\ntext setting we consider, it is reasonable to question\nwhether such manual work is necessary. To explore\nthis question, we follow Brown et al. (2020) and\ncreate a default template of “[X] => [Y]”, which\ncan be applied for any relation type. We denote it\nby defT, and it can be used in the in-context learn-\ning approach described next.\nIn-context Learning As shown by (Brown et al.,\n2020), pretrained LMs are able to learn from the\nexamples included in the input. To implement this\napproach, we concatenate converted triples from\nDr\ntrain to be a long preﬁx, and prepend it to our\nqueries. We denote it by f*+ctx, where ˚is a place-\nholder for the template option (e.g., manT). In our\nexperiments we ﬁnd that the order of the preﬁx\nexamples will affect the performance. Therefore\nfor each relation type, we tune the ordering as a\nhyper-parameter via Dr\ndev.\nOptimized Prompts It is attractive to think\nof approaches which can automatically design\nprompts, minimizing human effort. AutoPrompt\n(Shin et al., 2020) and BERTese (Haviv et al., 2021)\nuse gradient-based search to automatically ﬁnd tem-\nplates, in the form of discrete tokens, that maximize\nthe model’s performance on a training set. Very\nrecently, OptiPrompt (Zhong et al., 2021) general-\nizes to continuous vectors, and achieves better per-\nformance than AutoPrompt. In OptiPrompt, ﬁve\nrelation vectors are put between the subject and\nthe mask token, before being fed into the model.1\nThese relation vectors are trained to minimize the\ncross-entropy loss for the object, with stochastic\ngradient descent (SGD):\nLr “´ 1\n|Dr\ntrain|\nÿ\nxx,yyPDr\ntrain\nlog PLMpy|foptiPpx,rqq.\n(1)\nBy default, we initialize the relation vectors to\nbe the mean of the input embeddings of the ﬁrst\n10,000 most frequent tokens that are stored in the\npretrained LM. We could also align the relation vec-\ntors with the manual template, and initialize them\nto be the embedding of the corresponding token in\nthe template (denoted by optiP+manT).\nThese studies utilize a considerable number of\nexample triples (around 1000 samples per relation\ntype) to train the prompts, and their performance\nunder a few-shot setting is unknown.\nModel Finetuning All the approaches discussed\nabove engineer the input while the pretrained LM\nis kept ﬁxed. Therefore, it is natural to consider\nﬁnetuning the model with the available templates\nor relation vectors as input. The major shortcoming\nis that we would need to store a copy of the entire\n1We have also tried with 8 or 10 relation vectors, but only\nobserve very little improvements.\nf fpx, rq\nmanT/mineT Andrea Alciato was born in <mask> . / Andrea Alciato lived in <mask> .\ndefT Andrea Alciato => <mask> .\nmanT+ctx Joan Dickson was born in Edinburgh. Charles Helou was born in Beirut. Andrea Alciato was born in <mask>.\noptiPrompts Andrea Alciato <V0> <V1> <V2> <V3> <V4> <mask>\noptiP+manT Andrea Alciato <V0>:=was <V1>:=born <V2>:=in <mask> <V3>:=.\nTable 1: Examples of how different types of converters f form a input for the masked language model. The\nrelation ris “place of birth”, and the xbeing queried is Andrea Alciato. The few-shot examples Dr\ntrain consists of\nxJoan Dickson, Edinburghyand xCharles Helou, Beiruty, which can be used for in-context learning.\nmodel for each relation type (Lester et al., 2021; Li\nand Liang, 2021).\nModel Bias Finetuning To mitigate the storage\nissue, Ben-Zaken et al. (2020) proposes to ﬁnetune\nonly the bias vectors in the encoder. This approach\nis named BitFit, and is shown to be very competi-\ntive on the GLUE benchmark (Wang et al., 2018).\nFurther details and a storage cost comparison are\ngiven in Appendix A. In our experiments, we test\nits performance under few-shot knowledge probing,\nand compare it with full-model ﬁnetuning.\n3 Datasets\nFollowing Zhong et al. (2021) and Shin et al.\n(2020), we use the T-REx (Elsahar et al., 2018)\ndataset, which is included in the LAMA bench-\nmark (Petroni et al., 2019). It contains 41 Wikidata\nrelation types, and each relation type has up to 1000\ntriples. We will refer to it as TREx-1p as it focuses\non 1-hop relations.\nIn addition to memory of 1-hop relations, hu-\nmans also possess the capability of multi-hop rea-\nsoning (Yang et al., 2018; Xiong et al., 2017). For\nexample, given two known facts of \"[X] works for\n[Y].\" and \"[Y] produces [Z].\", there is clearly a 2-\nhop link between X and Z (e.g., X being Steve Jobs,\nY being Apple, and Z being iPhone). To probe\nwhether the pretrained LM also possesses this kind\nof “indirect” knowledge, we create a 2-hop vari-\nant of the T-REx dataset, named TREx-2p.2 We\nmanually examine the 2-hop link existing in the\nknowledge graph of TREx-1p, and select eight 2-\nhop relation types that make sense to humans.\nAs in LAMA, we manually create natural-\nlanguage templates for relations in TREx-2p. We\nshow them in Table 4 (Appendix B). To encode the\n2-hop relations, these templates are syntactically\nmore complicated (e.g., “[X] works for a company\nthat developed [Y] .”). Therefore, we expect the\nzero-shot probing performance of TREx-2p with\nmanual templates to be low.\n2We will release the data and code used in this work in the\npublic version of this manuscript.\n4 Experiments\nOur experiments focus on the Roberta-large model\n(Liu et al., 2019), a 24-layer transformer LM with\na hidden-dimension of 1024. Our code is based on\nHuggingFace (Wolf et al., 2020) and the released\ncode from LAMA. The few-shot development set\n(Ddev) are used for hyper-parameter tuning. We\nﬁnd that ﬁnetuning with the few-shot training ex-\namples are very prone to over-ﬁtting. Therefore,\nduring SGD ﬁnetuning we do early stopping by\nmonitoring the loss on the development set every\n10 iterations. More details are in Appendix A.\nIn addition to accuracy (Precision@1), we also\nreport mean reciprocal rank (MRR), to account\nfor cases with multiple valid targets. Following\nearlier work (Petroni et al., 2019), we report macro-\naveraged numbers across different relation types.\nThe few-shot examples are randomly selected\nfrom the dataset for each relation type, and the rest\nof the samples are used for evaluation. We compare\nthe performance of different approaches under set-\ntings where 10/20/40 example triples are available.\nOut of the available examples, 5/10/10 samples are\ntaken out as a development set, leaving the rest for\ntraining. The same training/development sets are\nused across different approaches.\nThe accuracy results are shown in Table 2. Ob-\nservations from results measured by MRR are\nhighly similar, and we defer them to Table 5 (Ap-\npendix B) to save space. In general, we observe that\nfor both 1-hop and 2-hop relations, large gains can\nbe achieved with as few as 10 available examples\nin comparison to the zero-shot performance.\nFor prompt engineering, OptiPrompt greatly out-\nperforms manual or LPAQA (mineT) templates,\nwhich agrees with the non-few-shot results in\nZhong et al. (2021). This conﬁrms the advantage\nof a continuous prompt as opposed to discrete to-\nkens. Next, in-context learning is competitive in the\n10/20-shot setting. However, its performance satu-\nrates quickly, and is outperformed by OptiPrompt\nin the 40-shot setting.\nAccuracy(%) Prompt Engineering In-context Learning Model FT BitFit\nTREx-1p manT mineT optiP optiP+manT manT defT mineT manT defT manT defT optiP+manT\n5T+5D 0-shot:\n25.8\n34.9 40.0 49.4 49.0 47.3 48.9 49.1 44.8 49.2 45.4 49.8\n10T+10D 36.3 47.9 49.7 50.3 51.1 51.6 51.3 49.4 52.4 48.9 52.1\n30T+10D 37.0 52.3 52.5 50.0 52.1 51.0 54.1 53.2 54.5 53.3 54.0\nTREx-2p manT / optiP optiP+manT manT defT / manT defT manT defT optiP\n5T+5D 0-shot:\n14.4\n/ 43.2 41.3 47.5 45.0 / 45.6 48.1 44.6 46.9 48.0\n10T+10D / 50.1 46.7 44.0 44.0 / 50.1 48.9 51.4 51.5 50.1\n30T+10D / 51.8 52.0 53.0 50.3 / 53.5 54.2 53.6 53.5 55.7\nTable 2: The accuracy performance of different approaches for the TREx-1/2p datasets. “5T+5D” means that 5\nexamples are used for training and 5 examples are used as a development set. Some combination of approaches\n(such as model ﬁnetuning with OptiPrompt) are deferred to Appendix B due to lack of space.\n10 15 20 25 30 35 40 45 50\nNumber of Few-shot Examples\n37.5\n40.0\n42.5\n45.0\n47.5\n50.0\n52.5\n55.0Accuracy(%)\nmanT+bitfit\noptiP+manT\nmanT+finalhiddenOffset\nmanT+vocabOffset\nFigure 2: (TREx-1p) Only ﬁnetuning the bias param-\neter in the output layer or the ﬁnal hidden layer gives\nworse performance than BitFit or OptiPrompt.\nDirect ﬁnetuning a large model with only a few\ntraining examples is usually considered difﬁcult\ndue to over-ﬁtting. Interestingly, we ﬁnd that early\nstopping with the tiny development set can effec-\ntively regularize the training, and model ﬁntun-\ning gives better accuracy than OptiPrompt in most\ncases. More excitingly, BitFit, which only tunes\nthe bias parameters, achieves similar or even bet-\nter accuracy than full-model ﬁnetuning. In some\ncases, BitFit can beneﬁt from OptiPrompt as in-\nput for the extra ﬂexibility. Lastly, we observe\nthat manual templates perform better than the de-\nfault template for OptiPrompt, model ﬁnetuning\nand BitFit, showing a complementary effect to the\nfew-shot examples.\nWill more examples give better performance? In\nAppendix B, we show that the performance satu-\nrates at around 200 examples with an accuracy of\n57.5%.\nFor TREx-2p, the general observations are simi-\nlar to TREx-1p. We mention two differences: (1)\nThe zero-shot performance of manual templates\nfor TREx-2p is poor (only 14.4%), which is ex-\npected as 2-hop templates are syntactically more\ncomplicated; (2) Possibly due to the same reason,\nOptiPrompt does not beneﬁt from manual-template\ninitialization for TREx-2p.\nFinally, we introduce two control baselines for\nBitFit: (1) Only the length- |V|ﬁnal bias vector\nin the output layer is ﬁnetuned; (2) Only the bias\nvector in the ﬁnal hidden layer is ﬁnetuned. Re-\nsults are shown in Figure 2. We observe that both\ncontrol baselines are outperformed by BitFit and\nOptiPrompt by a large margin. This shows that the\nperformance gain is not only from simply biasing\nthe model to a certain group of output tokens, and\nthe inner representations are also changed to better\nexpose the stored knowledge.\n5 Related Works\nHow to effectively adapt a pretrained LM to a spe-\nciﬁc task in a few-shot setting has been an impor-\ntant topic in recent NLP research (Zhang et al.,\n2021). The idea of in-context learning is popular-\nized by GPT-3 (Brown et al., 2020), which shows\nthat a ﬁxed pretrained LM can beprimed to conduct\ndifferent tasks via in-context examples. Recently,\nZhao et al. (2021) points out some caveats about\nin-context learning, and how to better calibrate it.\nClosely related to template-based knowledge\nprobing, Schick and Schütze (2021) proposes Pat-\ntern Exploiting Training (PET) for NLU tasks,\nwhere inputs are converted into cloze-style ques-\ntions (e.g., “Awful pizza! It was <mask>.”), and\ngradient-based optimization is conducted. PET\nand its variants iPET and ADAPET (Schick and\nSchütze, 2020; Tam et al., 2021) are shown to\nbe more effective than vanilla in-context learning.\nAlso along this line of work, Gao et al. (2020) pro-\npose an automatic framework of prompt generation\nand demonstration selection.\nLast but not least, Li and Liang (2021), followed\nby Lester et al. (2021), proposepreﬁx tuning, where\ncontinuous task-speciﬁc input vectors are tuned\nwhile the model is kept ﬁxed. It is very similar to\nthe OptiPrompt approach considered in this work.\nReferences\nElad Ben-Zaken, Shauli Ravfogel, and Yoav Goldberg.\n2020. Bitﬁt: Simple parameter-efﬁcient ﬁne-tuning\nfor transformer-based masked language-models.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nHady Elsahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique\nLaforest, and Elena Simperl. 2018. T-REx: A large\nscale alignment of natural language with knowledge\nbase triples. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC 2018) , Miyazaki, Japan. European\nLanguage Resources Association (ELRA).\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020.\nMaking pre-trained language models better few-shot\nlearners. CoRR, abs/2012.15723.\nAdi Haviv, Jonathan Berant, and Amir Globerson.\n2021. Bertese: Learning to speak to BERT. CoRR,\nabs/2103.05327.\nBenjamin Heinzerling and Kentaro Inui. 2021. Lan-\nguage models as knowledge bases: On entity\nrepresentations, storage capacity, and paraphrased\nqueries. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Com-\nputational Linguistics: Main Volume , pages 1772–\n1791, Online. Association for Computational Lin-\nguistics.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nNora Kassner, Philipp Dufter, and Hinrich Schütze.\n2021. Multilingual LAMA: Investigating knowl-\nedge in multilingual pretrained language models. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 3250–3258, Online.\nAssociation for Computational Linguistics.\nNora Kassner and Hinrich Schütze. 2020. Negated and\nmisprimed probes for pretrained language models:\nBirds can talk, but cannot ﬂy. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7811–7818, Online. As-\nsociation for Computational Linguistics.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efﬁcient prompt\ntuning. CoRR, abs/2104.08691.\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-\ntuning: Optimizing continuous prompts for genera-\ntion. CoRR, abs/2101.00190.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Con-\nference on Learning Representations.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proc. of NAACL.\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton\nBakhtin, Yuxiang Wu, Alexander H. Miller, and Se-\nbastian Riedel. 2019. Language models as knowl-\nedge bases?\nNina Poerner, Ulli Waltinger, and Hinrich Schütze.\n2020. E-BERT: Efﬁcient-yet-effective entity em-\nbeddings for BERT. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2020 ,\npages 803–818, Online. Association for Computa-\ntional Linguistics.\nTimo Schick and Hinrich Schütze. 2020. It’s not just\nsize that matters: Small language models are also\nfew-shot learners. CoRR, abs/2009.07118.\nTimo Schick and Hinrich Schütze. 2021. Exploiting\ncloze-questions for few-shot text classiﬁcation and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 255–269, Online. Association for Com-\nputational Linguistics.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting Knowledge from Language Models with\nAutomatically Generated Prompts. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4222–4235, Online. Association for Computational\nLinguistics.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. Mass: Masked sequence to sequence\npre-training for language generation. arXiv preprint\narXiv:1905.02450.\nDerek Tam, Rakesh R. Menon, Mohit Bansal,\nShashank Srivastava, and Colin Raffel. 2021. Im-\nproving and simplifying pattern exploiting training.\nCoRR, abs/2103.11955.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP , pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nWenhan Xiong, Thien Hoang, and William Yang\nWang. 2017. Deeppath: A reinforcement learn-\ning method for knowledge graph reasoning. CoRR,\nabs/1707.06690.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. CoRR, abs/1906.08237.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 2369–2380, Brussels, Belgium. Association\nfor Computational Linguistics.\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q\nWeinberger, and Yoav Artzi. 2021. Revisiting few-\nsample {bert} ﬁne-tuning. In International Confer-\nence on Learning Representations.\nTony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein,\nand Sameer Singh. 2021. Calibrate before use: Im-\nproving few-shot performance of language models.\nCoRR, abs/2102.09690.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: learning vs. learning to\nrecall. CoRR, abs/2104.05240.\nAppendices\nA Implementation Details\nFor model or relation vector ﬁnetuning, we use the\nAdamW optimizer (Loshchilov and Hutter, 2019;\nZhang et al., 2021). Since in most of our few-shot\nexperiments, the training data only consists of a\nsmall number (e.g., 10 or 20) of samples, we di-\nrectly do full-batch training. We tune the learning-\nrates in a log-scale using Dr\ndev. Typically, we ﬁnd\nthat a small learning rate (e.g., 1e-06) works well\nfor full-model ﬁnetuning, while a relatively large\nlearning rate (e.g., 0.01) works well for OptiPrompt\nor BitFit. For in-context learning, we try 20 differ-\nent random orders (of the examples in the context)\nfor each relation type, and use the ordering which\ngives best performance on Dr\ndev.\nFor the implementation of BitFit, we follow the\nBitFit-Bvariant in Ben-Zaken et al. (2020), where\naround half of the bias vectors are tuned. To be\nspeciﬁc, for each transformer layer, the bias for\nthe attention query (of length 1024), and the bias\nfor the intermediate layer (of length 4096) of the\ntransformer block, are tuned. We summarize and\ncompare the storage cost of different approaches in\nTable 3.\nFinally, we mention two differences between\nour implementation and the code from the LAMA\nbenchmark: (1) The original code uses a common\nvocab, which is a intersection of the vocabularies\nfrom various pretrained LMs. In this work since we\nfocus on the Roberta-large model, we just use the\nwhole Roberta vocab. If we switch to the common\nvocab, the zero-short accuracy on the TREx-1p\ndataset will be improved from 25.8% to 31.9%.\n(2) In the original code, inside each relation, if a\nsubject has multiple valid objects, they are still\ntreated as separate triples. As a consequence, for\nthe accuracy metric, it is impossible for the LM get\nall triples right because only the top-1 prediction is\nconsidered. In our implementation, we merge them\ninto one test case with multiple valid targets, and\nreport both accuracy and MRR.\nB Auxiliary Results\nTemplates created for TREx-2p are shown in Table\n4. These 2-hop relations are manually selected\nfrom the knowledge graph of TREx-1p.\nIn Table 5, MRR performance of different ap-\nproaches are shown. The observations are similar\nto the accuracy results (Table 2).\nApproach Param. Number\nIn-context Leanring 0\nOptiPrompt 5 ˆ1024\nBitFit 24 ˆ5120\nModel Finetuning 355M\nFinalHiddenOffset 1024\nV ocabOffset |V| = 50325\nTable 3: The number of extra parameters to be saved\nfor each relation type.\n50 100 150 200 250\nNumber of Examples\n54.0\n54.5\n55.0\n55.5\n56.0\n56.5\n57.0\n57.5\n58.0Accuracy(%)\nmanT+modelFT\nmanT+bitfit\nFigure 3: (TREx-1p) Accuracy results with more avail-\nable samples for each relation, the performance satu-\nrates at around 200 samples.\nIn Figure 3, accuracy results with more available\nsamples are shown. We observe that the perfor-\nmance saturates at around 200 samples at around\n57.5%. When the number of available samples is\nlarger than or equal to 100, we use 20 samples for\ndevelopment.\nIn Table 6, a complete set of results of model ﬁn-\ntuning/BitFit for the TREx-1/2p datasets are shown.\nOptiPrompt and the model (or the bias vectors in\nthe model) can be jointly trained for the extra ﬂexi-\nbility in the input. In some cases the performance\nis improved, but the gain is not large.\nP159 The headquarter of [X] (Virtue Party) is in [Y] (Ankara) . | P1376 [X] (Ankara) is the capital of [Y] (Turkey) .\nThe headquarter of [X] (Virtue Party) is in the country of [Y] (Turkey) .\nP108 [X] (Steve Jobs) works for [Y] (Apple) . | P178 [X] (macOS) is developed by [Y] (Apple) .\n[X] (Steve Jobs) works for a company that developed [Y] (macOS) .\nP178 [X] (macOS) is developed by [Y] (Apple) . | P178 [X] (MessagePad) is developed by [Y] (Apple) .\n[X] (macOS) and [Y] (MessagePad) are developed by the same company .\nP31 [X] (Wick Airport) is a [Y] (airport) . | P361 [X] (runway) is part of [Y] (airport) .\nOne component of [X] (Wick Airport) is [Y] (runway) .\nP361 [X] (geometry) is part of [Y] (mathematics) . | P361 [X] (arithmetic) is part of [Y] (mathematics) .\n[X] (geometry) and [Y] (arithmetic) are part of the same thing .\nP361 [X] (whey) is part of [Y] (milk) . | P527 [X] (yogurt) consists of [Y] (milk) .\n[X] (whey) is a low-level part of [Y] (yogurt) .\nP527 [X] (gelato) consists of [Y] (milk) . | P527 [X] (yogurt) consists of [Y] (milk) .\n[X] (gelato) and [Y] (yogurt) share at least one element .\nP37 The ofﬁcial language of [X] (Scotland) is [Y] (English) . | P19 [X] (Paul Mounsey) was born in [Y] (Scotland) .\nThe ofﬁcial language of the country where [X] (Paul Mounsey) was born is [Y] (English) .\nTable 4: Examples of the TREx-2p dataset, and the manual templates we created. The relation ids from the origin\nT-REx dataset are also shown.\nMRR Prompt Engineering In-context Learning Model Finetuning BitFit\nTREx-1p manT mineT optiP optiP+manT manT defT mineT manT defT manT defT optiP+manT\n5T+5D 0-shot:\n.340\n.436 .487 .572 .568 .559 .572 .576 .538 .577 .538 .580\n10T+10D .450 .559 .577 .583 .596 .596 .594 .580 .600 .574 .600\n30T+10D .458 .603 .608 .583 .603 .595 .626 .616 .625 .617 .625\nTREx-2p manT / optiP optiP+manT manT defT / manT defT manT defT optiP\n5T+5D 0-shot:\n.166\n/ 35.8 35.0 44.9 42.7 / 39.3 40.0 38.0 38.9 43.1\n10T+10D / 42.6 42.3 41.3 41.2 / 44.6 43.4 46.5 44.3 43.3\n30T+10D / 44.4 43.7 44.2 44.5 / 45.8 45.9 46.3 44.8 46.6\nTable 5: The MRR performance of different approaches for the TREx-1/2p datasets. The leading zeros are omitted.\nThe observations are similar to the accuracy results.\nAccuracy(%) Model Finetuning Bitﬁt\nTREx-1p manT defT mineT optiT optiT+manT manT defT mineT optiT optiP+manT\n5T+5D 49.1 44.8 48.7 42.6 49.0 49.2 45.4 48.8 44.3 49.8\n10T+10D 51.3 49.4 51.1 49.1 51.2 52.4 48.9 51.1 47.7 52.2\n30T+10D 54.1 53.2 54.1 53.3 54.2 54.5 53.3 54.5 53.1 54.0\nTREx-2p manT defT mineT optiT optiT+manT manT defT mineT optiT optiP+manT\n5T+5D 45.6 48.1 / 44.3 46.4 44.6 46.9 / 48.0 45.3\n10T+10D 50.1 48.9 / 50.0 48.8 51.4 51.5 / 50.1 50.6\n30T+10D 53.5 54.2 / 52.5 53.6 53.6 53.5 / 55.7 53.6\nMRR Model Finetuning Bitﬁt\nTREx-1p manT defT mineT optiT optiT+manT manT defT mineT optiT optiP+manT\n5T+5D .576 .538 .569 .514 .575 .577 .538 .570 .532 .580\n10T+10D .594 .580 .590 .570 .594 .600 .574 .590 .562 .600\n30T+10D .626 .616 .623 .613 .627 .625 .617 .626 .612 .625\nTREx-2p manT defT mineT optiT optiT+manT manT defT mineT optiT optiP+manT\n5T+5D 39.3 40.0 / 37.3 39.6 38.0 38.9 / 43.1 39.3\n10T+10D 44.6 43.4 / 42.8 43.2 46.5 44.3 / 43.3 46.0\n30T+10D 45.8 45.9 / 44.7 45.6 46.3 44.8 / 46.6 45.9\nTable 6: A complete set of results of model ﬁntuning and BitFit for the TREx-1/2p datasets.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8147314786911011
    },
    {
      "name": "Hop (telecommunications)",
      "score": 0.6064895391464233
    },
    {
      "name": "Shot (pellet)",
      "score": 0.5605836510658264
    },
    {
      "name": "Code (set theory)",
      "score": 0.5342758297920227
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.5191932916641235
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.481374055147171
    },
    {
      "name": "Measure (data warehouse)",
      "score": 0.46291616559028625
    },
    {
      "name": "One shot",
      "score": 0.4242500960826874
    },
    {
      "name": "Source code",
      "score": 0.42129233479499817
    },
    {
      "name": "Language model",
      "score": 0.4202693700790405
    },
    {
      "name": "Natural language processing",
      "score": 0.38553687930107117
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38384002447128296
    },
    {
      "name": "Machine learning",
      "score": 0.3583596348762512
    },
    {
      "name": "Data mining",
      "score": 0.3053118586540222
    },
    {
      "name": "Programming language",
      "score": 0.1442764401435852
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.05706608295440674
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ]
}