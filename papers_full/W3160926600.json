{
    "title": "MOTR: End-to-End Multiple-Object Tracking with Transformer",
    "url": "https://openalex.org/W3160926600",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4361164659",
            "name": "Zeng, Fangao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2011423952",
            "name": "Dong Bin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2224432451",
            "name": "Zhang, Yuang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2315442368",
            "name": "Wang TianCai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1895367665",
            "name": "Zhang, Xiangyu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3188424656",
            "name": "Wei, Yichen",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3165926952",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W3106763294",
        "https://openalex.org/W2981393651",
        "https://openalex.org/W2474389331",
        "https://openalex.org/W2963351448",
        "https://openalex.org/W2145938889",
        "https://openalex.org/W2796347433",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3015834770",
        "https://openalex.org/W2964015640",
        "https://openalex.org/W2237765446",
        "https://openalex.org/W2962803115",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W3190647944",
        "https://openalex.org/W3104778224",
        "https://openalex.org/W3109319753",
        "https://openalex.org/W2935837427",
        "https://openalex.org/W2913841731",
        "https://openalex.org/W3095753995",
        "https://openalex.org/W2194187530",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963313370",
        "https://openalex.org/W3115390238",
        "https://openalex.org/W2963940252",
        "https://openalex.org/W2252355370",
        "https://openalex.org/W2962766617",
        "https://openalex.org/W3122239467",
        "https://openalex.org/W2222512263",
        "https://openalex.org/W3041270645",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2547098537",
        "https://openalex.org/W3167949052",
        "https://openalex.org/W2951418500",
        "https://openalex.org/W2291627510",
        "https://openalex.org/W3016298629",
        "https://openalex.org/W2616924599",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2964333437",
        "https://openalex.org/W1521019969",
        "https://openalex.org/W2950465199",
        "https://openalex.org/W2903739847",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2168356304",
        "https://openalex.org/W3034765865",
        "https://openalex.org/W3099887740",
        "https://openalex.org/W2766984662",
        "https://openalex.org/W3119686997",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2603203130",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W3042823511",
        "https://openalex.org/W2798542761",
        "https://openalex.org/W3096068180"
    ],
    "abstract": "Temporal modeling of objects is a key challenge in multiple object tracking (MOT). Existing methods track by associating detections through motion-based and appearance-based similarity heuristics. The post-processing nature of association prevents end-to-end exploitation of temporal variations in video sequence. In this paper, we propose MOTR, which extends DETR and introduces track query to model the tracked instances in the entire video. Track query is transferred and updated frame-by-frame to perform iterative prediction over time. We propose tracklet-aware label assignment to train track queries and newborn object queries. We further propose temporal aggregation network and collective average loss to enhance temporal relation modeling. Experimental results on DanceTrack show that MOTR significantly outperforms state-of-the-art method, ByteTrack by 6.5% on HOTA metric. On MOT17, MOTR outperforms our concurrent works, TrackFormer and TransTrack, on association performance. MOTR can serve as a stronger baseline for future research on temporal modeling and Transformer-based trackers. Code is available at https://github.com/megvii-research/MOTR.",
    "full_text": "MOTR: End-to-End Multiple-Object Tracking\nwith Transformer\nFangao Zeng1‚ãÜ, Bin Dong1‚ãÜ, Yuang Zhang2‚ãÜ, Tiancai Wang1‚ãÜ‚ãÜ,\nXiangyu Zhang1, and Yichen Wei1\n1 MEGVII Technology\n2 Shanghai Jiao Tong University\nAbstract. Temporal modeling of objects is a key challenge in multiple-\nobject tracking (MOT). Existing methods track by associating detections\nthrough motion-based and appearance-based similarity heuristics. The\npost-processing nature of association prevents end-to-end exploitation of\ntemporal variations in video sequence.\nIn this paper, we propose MOTR, which extends DETR [6] and intro-\nduces ‚Äútrack query‚Äù to model the tracked instances in the entire video.\nTrack query is transferred and updated frame-by-frame to perform iter-\native prediction over time. We propose tracklet-aware label assignment\nto train track queries and newborn object queries. We further propose\ntemporal aggregation network and collective average loss to enhance tem-\nporal relation modeling. Experimental results on DanceTrack show that\nMOTR significantly outperforms state-of-the-art method, ByteTrack [42]\nby 6.5% on HOTA metric. On MOT17, MOTR outperforms our concur-\nrent works, TrackFormer [18] and TransTrack [29], on association per-\nformance. MOTR can serve as a stronger baseline for future research on\ntemporal modeling and Transformer-based trackers. Code is available at\nhttps://github.com/megvii-research/MOTR.\nKeywords: Multiple-Object Tracking, Transformer, End-to-End\n1 Introduction\nMultiple-object tracking (MOT) predicts the trajectories of instances in con-\ntinuous image sequences [39,2]. Most of existing methods separate the MOT\ntemporal association into appearance and motion: appearance variance is usu-\nally measured by pair-wise Re-ID similarity [37,43] while motion is modeled via\nIoU [4] or Kalman Filtering [3] heuristic. These methods require similarity-based\nmatching for post-processing, which becomes the bottleneck of temporal infor-\nmation flow across frames. In this paper, we aim to introduce a fully end-to-end\nMOT framework featuring joint motion and appearance modeling.\nRecently, DETR [6,45] was proposed for end-to-end object detection. It for-\nmulates object detection as a set prediction problem. As shown in Fig. 1(a),\n‚ãÜ Equal contribution.\n‚ãÜ‚ãÜ Corresponding author. Email: wangtiancai@megvii.com\narXiv:2105.03247v4  [cs.CV]  19 Jul 2022\n2 F. Zeng et al.\nImagefeature\n(a) One-shot prediction in DETR.\nBox-iBox-jBox-k DETR Decoder\nMulti-framefeatures\nIterativeUpdate(b) Iterativeprediction of box sequences in MOTR.\nTrack-i‚Ä¶Track-jTrack-kObjectqueries Trackqueries ‚Ä¶‚Ä¶\nùëá!ùëá\"‚Ä¶DETR Decoder\nFig. 1: (a) DETR achieves end-to-end detection by interacting object queries\nwith image features and performs one-to-one assignment between the updated\nqueries and objects. (b) MOTR performs set of sequence prediction by updating\nthe track queries. Each track query represents a track. Best viewed in color.\nobject queries, served as a decoupled representation of objects, are fed into the\nTransformer decoder and interacted with the image feature to update their rep-\nresentation. Bipartite matching is further adopted to achieve one-to-one assign-\nment between the object queries and ground-truths, eliminating post-processes,\nlike NMS. Different from object detection, MOT can be regarded as a sequence\nprediction problem. The way to perform sequence prediction in the end-to-end\nDETR system is an open question.\nIterative prediction is popular in machine translation [30,31]. The output\ncontext is represented by a hidden state, and sentence features iteratively interact\nwith the hidden state in the decoder to predict the translated words. Inspired by\nthese advances in machine translation, we intuitively regard MOT as a problem\nof set of sequence predictionsince MOT requires a set of object sequences. Each\nsequence corresponds to an object trajectory. Technically, we extend object query\nin DETR to track queryfor predicting object sequences. Track queries are served\nas the hidden states of object tracks. The representations of track queries are\nupdated in the Transformer decoder and used to predict the object trajectory\niteratively, as shown in Fig. 1(b). Specifically, track queries are updated through\nself-attention and cross-attention by frame features. The updated track queries\nare further used to predict the bounding boxes. The track of one object can be\nobtained from all predictions of one track query in different frames.\nTo achieve the goal above, we need to solve two problems: 1) track one\nobject by one track query; 2) deal with newborn and terminated objects. To\nsolve the first problem, we introduce tracklet-aware label assignment (TALA).\nIt means that predictions of one track query are supervised by bounding box\nsequences with the same identity. To solve the second problem, we maintain\na track query set of variable lengths. Queries of newborn objects are merged\ninto this set while queries of terminated objects are removed. We name this\nprocess the entrance and exit mechanism. In this way, MOTR does not require\nexplicit track associations during inference. Moreover, the iterative update of\ntrack queries enables temporal modeling regarding both appearance and motion.\nTo enhance the temporal modeling, we further propose collective average loss\n(CAL) and temporal aggregation network (TAN). With the CAL, MOTR takes\nMOTR: End-to-End Multiple-Object Tracking with Transformer 3\nvideo clips as input during training. The parameters of MOTR are updated\nbased on the overall loss calculated for the whole video clip. TAN introduces a\nshortcut for track query to aggregate the historical information from its previous\nstates via the key-query mechanism in Transformer.\nMOTR is a simple online tracker. It is easy to develop based on DETR\nwith minor modifications on label assignment. It is a truly end-to-end MOT\nframework, requiring no post-processes, such as the track NMS or IoU match-\ning employed in our concurrent works, TransTrack [29], and TrackFormer [18].\nExperimental results on MOT17 and DanceTrack datasets show that MOTR\nachieves promising performance. On DanceTrack [28], MOTR outperforms the\nstate-of-the-art ByteTrack [42] by 6.5% on HOTA metric and 8.1% on AssA.\nTo summarize, our contributions are listed as below:\n‚Äì We present a fully end-to-end MOT framework, named MOTR. MOTR can\nimplicitly learn the appearance and position variances in a joint manner.\n‚Äì We formulate MOT as a problem of set of sequence prediction. We generate\ntrack query from previous hidden states for iterative update and prediction.\n‚Äì We propose tracklet-aware label assignment for one-to-one assignment be-\ntween track queries and objects. An entrance and exit mechanism is intro-\nduced to deal with newborn and terminated tracks.\n‚Äì We further propose CAL and TAN to enhance the temporal modeling.\n2 Related Work\nTransformer-based Architectures.Transformer [31] was first introduced to\naggregate information from the entire input sequence for machine translation.\nIt mainly involves self-attention and cross-attention mechanisms. Since that,\nit was gradually introduced to many fields, such as speech processing [13,7]\nand computer vision [34,5]. Recently, DETR [6] combined convolutional neu-\nral network (CNN), Transformer and bipartite matching to perform end-to-end\nobject detection. To achieve the fast convergence, Deformable DETR [45] intro-\nduced deformable attention module into Transformer encoder and Transformer\ndecoder. ViT [9] built a pure Transformer architecture for image classification.\nFurther, Swin Transformer [16] proposed shifted windowing scheme to perform\nself-attention within local windows, bringing greater efficiency. VisTR [36] em-\nployed a direct end-to-end parallel sequence prediction framework to perform\nvideo instance segmentation.\nMultiple-Object Tracking. Dominant MOT methods mainly followed the\ntracking-by-detection paradigm [3,12,22,24,39]. These approaches usually first\nemploy object detectors to localize objects in each frame and then perform track\nassociation between adjacent frames to generate the tracking results. SORT [3]\nconducted track association by combining Kalman Filter [38] and Hungarian\nalgorithm [11]. DeepSORT [39] and Tracktor [2] introduced an extra cosine dis-\ntance and compute the appearance similarity for track association. Track-RCNN\n[26], JDE [37] and FairMOT [43] further added a Re-ID branch on top of object\ndetector in a joint training framework, incorporating object detection and Re-ID\n4 F. Zeng et al.\nfeature learning. TransMOT [8] builds a spatial-temporal graph transformer for\nassociation. Our concurrent works, TransTrack [29] and TrackFormer [18] also\ndevelop Transformer-based frameworks for MOT. For direct comparison with\nthem, please refer to Sec. 3.7.\nIterative Sequence Prediction.Predicting sequence via sequence-to-sequence\n(seq2seq) with encoder-decoder architecture is popular in machine translation\n[30,31] and text recognition [25]. In seq2seq framework, the encoder network\nencodes the input into intermediate representation. Then, a hidden state with\ntask-specific context information is introduced and iteratively interacted with\nthe intermediate representation to generate the target sequence through the\ndecoder network. The iterative decode process contains several iterations. In\neach iteration, hidden state decodes one element of target sequence.\n3 Method\n3.1 Query in Object Detection\nDETR [6] introduced a fixed-length set of object queries to detect objects. Ob-\nject queries are fed into the Transformer decoder and interacted with image\nfeatures, extracted from Transformer encoder to update their representation.\nBipartite matching is further adopted to achieve one-to-one assignment between\nthe updated object queries and ground-truths. Here, we simply write the object\nquery as ‚Äúdetect query‚Äù to specify the query used for object detection.\n3.2 Detect Query and Track Query\nWhen adapting DETR from object detection to MOT, two main problems arise:\n1) how to track one object by one track query; 2) how to handle newborn and\nterminated objects. We extend detect queries to track queries in this paper.\nTrack query set is updated dynamically, and the length is variable. As shown\nin Fig. 2, the track query set is initialized to be empty, and the detect queries\nin DETR are used to detect newborn objects (object 3 at T2). Hidden states of\ndetected objects produces track queries for the next frame; track queries assigned\nto terminated objects are removed from the track query set (object 2 at T4).\n3.3 Tracklet-Aware Label Assignment\nIn DETR, one detect (object) query may be assigned to any object in the im-\nage since the label assignment is determined by performing bipartite matching\nbetween all detect queries and ground-truths. While in MOTR, detect queries\nare only used to detect newborn objects while track queries predict all tracked\nobjects. Here, we introduce the tracklet-aware label assignment (TALA) to solve\nthis problem.\nGenerally, TALA consists of two strategies. For detect queries, we modify the\nassignment strategy in DETR as newborn-only, where bipartite matching is\nMOTR: End-to-End Multiple-Object Tracking with Transformer 5\nDetectQueryTrackQuery\nObject 1Object 2Object 3\nùëá! ùëá\" ùëá# ùëá$ùëá! ùëá\" ùëá# ùëá$\nFig. 2: Update process of detect (object) queries and track queries under some\ntypical MOT cases. Track query set is updated dynamically, and the length\nis variable. Track query set is initialized to be empty, and the detect queries\nare used to detect newborn objects. Hidden states of all detected objects are\nconcatenated to produce track queries for the next frame. Track queries assigned\nto terminated objects are removed from the track query set.\nconducted between the detect queries and the ground-truths of newborn objects.\nFor track queries, we design an target-consistent assignment strategy. Track\nqueries follow the same assignment of previous frames and are therefore excluded\nfrom the aforementioned bipartite matching.\nFormally, we denote the predictions of track queries as bYtr and predictions of\ndetect queries as bYdet. Ynew is the ground-truths of newborn objects. The label\nassignment results for track queries and detect queries can be written as œâtr and\nœâdet. For frame i, label assignment for detect queries is obtained from bipartite\nmatching among detect queries and newborn objects, i.e.,\nœâi\ndet = arg min\nœâi\ndet‚àà‚Ñ¶i\nL(bY i\ndet|œâi\ndet\n, Yi\nnew), (1)\nwhere L is the pair-wise matching cost defined in DETR and ‚Ñ¶i is the space\nof all bipartite matches among detect queries and newborn objects. For track\nquery assignment, we merge the assignments for newborn objects and tracked\nobjects from the last frame, i.e., for i >1:\nœâi\ntr = œâi‚àí1\ntr ‚à™ œâi‚àí1\ndet . (2)\nFor the first frame ( i = 1), track query assignment œâ1\ntr is an empty set ‚àÖ since\nthere are no tracked objects for the first frame. For successive frames ( i >1),\nthe track query assignment œâi\ntr is the concatenation of previous track query\nassignment œâi‚àí1\ntr and newborn object assignment œâi‚àí1\ndet .\nIn practice, the TALA strategy is simple and effective thanks to the powerful\nattention mechanism in Transformer. For each frame, detect queries and track\nqueries are concatenated and fed into the Transformer decoder to update their\nrepresentation. Detect queries will only detect newborn objects since query inter-\naction by self-attention in the Transformer decoder will suppress detect queries\nthat detect tracked objects. This mechanism is similar to duplicate removal in\nDETR that duplicate boxes are suppressed with low scores.\n6 F. Zeng et al.\nùëá! ùëá\" ùëá#‚Ä¶ ‚Ä¶Video Stream\nEncDec\nùëå!\" ùëå\"#ùëå#\"Predictions\nEncDecC EncDecDetect Query\nTrack QueryQIM ‚Ä¶ ‚Ä¶\nC‚Ä¶ ‚Ä¶ùëû$%# QIMùëû$%&\nùëû' ùëû' ùëû'\nFig. 3: The overall architecture of MOTR. ‚ÄúEnc‚Äù represents a convolutional neu-\nral network backbone and the Transformer encoder that extracts image features\nfor each frame. The concatenation of detect queries qd and track queries qtr is\nfed into the Deformable DETR decoder (Dec) to produce the hidden states. The\nhidden states are used to generate the prediction bY of newborn and tracked\nobjects. The query interaction module (QIM) takes the hidden states as input\nand produces track queries for the next frame.\n3.4 MOTR Architecture\nThe overall architecture of MOTR is shown in Fig. 3. Video sequences are fed into\nthe convolutional neural network (CNN) (e.g. ResNet-50 [10]) and Deformable\nDETR [45] encoder to extract frame features.\nFor the first frame, there are no track query and we only feed the fixed-\nlength learnable detect queries ( qd in Fig. 3) into the Deformable DETR [45]\ndecoder. For successive frames, we feed the concatenation of track queries from\nthe previous frame and the learnable detect queries into the decoder. These\nqueries interact with image feature in the decoder to generate the hidden state for\nbounding box prediction. The hidden state is also fed into the query interaction\nmodule (QIM) to generate the track queries for the next frame.\nDuring training phase, the label assignment for each frame is described in\nSec. 3.3. All predictions of the video clip are collected into a prediction bank{bY1,\nbY2, . . . ,bYN }, and we use the proposed collective average loss (CAL) described in\nSec. 3.6 for supervision. During inference time, the video stream can be processed\nonline and generate the prediction for each frame.\n3.5 Query Interaction Module\nIn this section, we describe query interaction module (QIM). QIM includes object\nentrance and exit mechanism and temporal aggregation network (TAN).\nObject Entrance and Exit.As mentioned above, some objects in video se-\nquences may appear or disappear at intermediate frames. Here, we introduce the\nMOTR: End-to-End Multiple-Object Tracking with Transformer 7\n3\n21\nfilter3\n31filter1(b) Object Exit\n(a) Object Entrance\nC\n0.150.840.09\n0.870.23\n1Tracked object\nscore\nTAN\nVAdd & NormFFNAdd & Norm\nùëû!\"#$%\nQuery Interaction Module\nsplit MHAK Qùëû!\"# +TAN2Exited object3Newborn objectDetectTrack\nFig. 4: The structure of query interaction module (QIM). The inputs of QIM\nare the hidden state produced by Transformer decoder and the corresponding\nprediction scores. In the inference stage, we keep newborn objects and drop\nexited objects based on the confidence scores. Temporal aggregation network\n(TAN) enhances long-range temporal modeling.\nway we deal with the newborn and terminated objects in our method. For any\nframe, track queries are concatenated with the detect queries and input to the\nTransformer decoder, producing the hidden state (see the left side of Fig. 4).\nDuring training, hidden states of terminated objects are removed if the\nmatched objects disappeared in ground-truths or the intersection-over-union\n(IoU) between predicted bounding box and target is below a threshold of 0.5.\nIt means that the corresponding hidden states will be filtered if these objects\ndisappear at current frame while the rest hidden states are reserved. For new-\nborn objects, the corresponding hidden states are kept based on the assignment\nof newborn object œâi\ndet defined in Eq. 1.\nFor inference, we use the predicted classification scores to determine appear-\nance of newborn objects and disappearance of tracked objects, as shown in Fig. 4.\nFor object queries, predictions whose classification scores are higher than the en-\ntrance threshold œÑen are kept while other hidden states are removed. For track\nqueries, predictions whose classification scores are lower than the exit threshold\nœÑex for consecutive M frames are removed while other hidden states are kept.\nTemporal Aggregation Network.Here, we introduce the temporal aggrega-\ntion network (TAN) in QIM to enhance temporal relation modeling and provide\ncontextual priors for tracked objects.\nAs shown in Fig. 4, the input of TAN is the filtered hidden state for tracked\nobjects (object ‚Äú1‚Äù). We also collect the track query qi\ntr from the last frame\nfor temporal aggregation. TAN is a modified Transformer decoder layer. The\ntrack query from the last frame and the filtered hidden state are summed to\nbe the key and query components of the multi-head self-attention (MHA). The\n8 F. Zeng et al.\nhidden state alone is the value component of MHA. After MHA, we apply a\nfeed-forward network (FFN) and the results are concatenated with the hidden\nstate for newborn objects (object ‚Äú3‚Äù) to produce the track query set qi+1\ntr for\nthe next frame.\n3.6 Collective Average Loss\nTraining samples are important for temporal modeling of track since MOTR\nlearns temporal variances from data rather than hand-crafted heuristics like\nKalman Filtering. Common training strategies, like training within two frames,\nfail to generate training samples of long-range object motion. Different from\nthem, MOTR takes video clips as input. In this way, training samples of long-\nrange object motion can be generated for temporal learning.\nInstead of calculating the loss frame-by-frame, our collective average loss\n(CAL) collects the multiple predictions bY = {bYi}N\ni=1. Then the loss within the\nwhole video sequence is calculated by ground-truthsY = {Yi}N\ni=1 and the match-\ning results œâ = {œâi}N\ni=1. CAL is the overall loss of the whole video sequence,\nnormalized by the number of objects:\nLo(bY |œâ, Y) =\nNP\nn=1\n(L(bY i\ntr|œâi\ntr\n, Yi\ntr) + L(bY i\ndet|œâi\ndet\n, Yi\ndet))\nNP\nn=1\n(Vi)\n(3)\nwhere Vi = V i\ntr +V i\ndet denotes the total number of ground-truths objects at frame\ni. V i\ntr and V i\ndet are the numbers of tracked objects and newborn objects at frame\ni, respectively. L is the loss of single frame, which is similar to the detection loss\nin DETR. The single-frame loss L can be formulated as:\nL(bYi|œâi , Yi) = ŒªclsLcls + Œªl1 Ll1 + ŒªgiouLgiou (4)\nwhere Lcls is the focal loss [14]. Ll1 denotes the L1 loss and Lgiou is the general-\nized IoU loss [21]. Œªcls, Œªl1 and Œªgiou are the corresponding weight coefficients.\n3.7 Discussion\nBased on DETR, our concurrent works, TransTrack [29] and TrackFormer [18]\nalso develop the Transformer-based frameworks for MOT. However, our method\nshows large differences compared to them:\nTransTrack models a full track as a combination of several independent\nshort tracklets. Similar to the track-by-detection paradigm, TransTrack decou-\nples MOT as two sub-tasks: 1) detect object pairs as short tracklets within\ntwo adjacent frames; 2) associate short tracklets as full tracks by IoU-matching.\nWhile for MOTR, we model a full track in an end-to-end manner through the\niterative update of track query, requiring no IoU-matching.\nMOTR: End-to-End Multiple-Object Tracking with Transformer 9\nTable 1: Comparison with other MOT\nmethods based on Transformer.\nMethod IoU match NMS ReID\nTransTrack [29] !\nTrackFormer [18] ! !\nMOTR (ours)\nTable 2: Statistics of chosen datasets\nfor evaluation.\nDatasets Class Frame Video ID\nDanceTrack [28] 1 106k 100 990\nMOT17 [19] 1 11k 14 1342\nBDD100K [41] 8 318k 1400 131k\nTrackFormershares the idea of track query with us. However, TrackFormer\nstill learns within two adjacent frames. As discussed in Sec. 3.6, learning within\nshort-range will result in relatively weak temporal learning. Therefore, Track-\nFormer employs heuristics, such as Track NMS and Re-ID features, to filter out\nduplicate tracks. Different from TrackFormer, MOTR learns stronger temporal\nmotion with CAL and TAN, removing the need of those heuristics. For direct\ncomparison with TransTrack and TrackFormer, please refer to the Table 1.\nHere, we clarify that we started this work independently long before Track-\nFormer and TransTrack appear on arXiv. Adding that they are not formally\npublished, we treat them as concurrent and independentworks instead of previ-\nous works on which our work is built upon.\n4 Experiments\n4.1 Datasets and Metrics\nDatasets. For comprehensive evaluation, we conducted experiments on three\ndatasets: DanceTrack [28], MOT17 [19], and BDD100k [41]. MOT17 [19] contains\n7 training sequences and 7 test sequences. DanceTrack [28] is a recent multi-\nobject tracking dataset featuring uniform appearance and diverse motion. It\ncontains more videos for training and evaluation thus providing a better choice\nto verify the tracking performance. BDD100k [41] is an autonomous driving\ndataset with an MOT track featuring multiple object classes. For more details,\nplease refer to the statistics of datasets, shown in Table 2.\nEvaluation Metrics.We follow the standard evaluation protocols to evaluate\nour method. The common metrics include Higher Order Metric for Evaluat-\ning Multi-object Tracking [17] (HOTA, AssA, DetA), Multiple-Object Tracking\nAccuracy (MOTA), Identity Switches (IDS) and Identity F1 Score (IDF1).\n4.2 Implementation Details\nFollowing the settings in CenterTrack [44], MOTR adopts several data augmen-\ntation methods, such as random flip and random crop. The shorter side of the\ninput image is resized to 800 and the maximum size is restricted to 1536. The\ninference speed on Tesla V100 at this resolution is about 7.5 FPS. We sample\nkeyframes with random intervals to solve the problem of variable frame rates.\nBesides, we erase the tracked queries with the probabilitypdrop to generate more\n10 F. Zeng et al.\nsamples for newborn objects and insert track queries of false positives with the\nprobability pinsert to simulate the terminated objects. All the experiments are\nconducted on PyTorch with 8 NVIDIA Tesla V100 GPUs. We also provide a\nmemory-optimized version that can be trained on NVIDIA 2080 Ti GPUs.\nWe built MOTR upon Deformable-DETR [45] with ResNet50 [10] for fast\nconvergence. The batch size is set to 1 and each batch contains a video clip of 5\nframes. We train our model with the AdamW optimizer with the initial learning\nrate of 2.0¬∑10‚àí4. For all datasets, we initialize MOTR with the official Deformable\nDETR [45] weights pre-trained on the COCO [15] dataset. On MOT17, we\ntrain MOTR for 200 epochs and the learning rate decays by a factor of 10 at\nthe 100th epoch. For state-of-the-art comparison, we train on the joint dataset\n(MOT17 training set and CrowdHuman [23] val set). For ‚àº5k static images\nin CrowdHuman val set, we apply random shift as in [44] to generate video\nclips with pseudo tracks. The initial length of video clip is 2 and we gradually\nincrease it to 3,4,5 at the 50 th,90th,150th epochs, respectively. The progressive\nincrement of video clip length improves the training efficiency and stability.\nFor the ablation study, we train MOTR on the MOT17 training set without\nusing the CrowdHuman dataset and validate on the 2DMOT15 training set. On\nDanceTrack, we train for 20 epochs on the train set and learning rate decays\nat the 10 th epoch. We gradually increase the clip length from 2 to 3,4,5 at the\n5th,9th,15th epochs. On BDD100k, we train for 20 epochs on the train set and\nlearning rate decays at the 16 th epoch. We gradually increase the clip length\nfrom 2 to 3 and 4 at the 6 th and 12th epochs.\n4.3 State-of-the-art Comparison on MOT17\nTable 3 compares our approach with state-of-the-art methods on MOT17 test\nset. We mainly compare MOTR with our concurrent works based on Trans-\nformer: TrackFormer [18] and TransTrack [29]. Our method gets higher IDF1\nscores, surpassing TransTrack and TrackFormer by 4.5%. The performance of\nMOTR on the HOTA metric is much higher than TransTrack by 3.1%. For the\nMOTA metric, our method achieves much better performance than TrackFormer\n(71.9% vs. 65.0%). Interestingly, we find that the performance of TransTrack is\nbetter than our MOTR on MOTA. We suppose the decoupling of detection and\ntracking branches in TransTrack indeed improves the object detection perfor-\nmance. While in MOTR, detect and track queries are learned through a shared\nTransformer decoder. Detect queries are suppressed on detecting tracked objects,\nlimiting the detection performance on newborn objects.\nIf we compare the performance with other state-of-the-art methods, like Byte-\nTrack [42], it shows that MOTR is frustratingly inferior to them on the\nMOT17 dataset. Usually, state-of-the-art performance on the MOT17 dataset\nis dominated by trackers with good detection performance to cope with various\nappearance distributions. Also, different trackers tend to employ different detec-\ntors for object detection. It is pretty difficult for us to fairly verify the motion\nperformance of various trackers. Therefore, we argue that the MOT17 dataset\nalone is not enoughto fully evaluate the tracking performance of MOTR. We\nMOTR: End-to-End Multiple-Object Tracking with Transformer 11\nTable 3: Performance comparison between MOTR and existing methods on the\nMOT17 dataset under the private detection protocols. The number is marked in\nbold if it is the best among the Transformer-based methods.\nMethods HOTA‚Üë AssA‚Üë DetA‚Üë IDF1‚Üë MOTA‚Üë IDS‚Üì\nCNN-based:\nTracktor++[2] 44.8 45.1 44.9 52.3 53.5 2072\nCenterTrack[44] 52.2 51.0 53.8 64.7 67.8 3039\nTraDeS [40] 52.7 50.8 55.2 63.9 69.1 3555\nQDTrack [20] 53.9 52.7 55.6 66.3 68.7 3378\nGSDT [35] 55.5 54.8 56.4 68.7 66.2 3318\nFairMOT[43] 59.3 58.0 60.9 72.3 73.7 3303\nCorrTracker [32] 60.7 58.9 62.9 73.6 76.5 3369\nGRTU [33] 62.0 62.1 62.1 75.0 74.9 1812\nMAATrack [27] 62.0 60.2 64.2 75.9 79.4 1452\nByteTrack [42] 63.1 62.0 64.5 77.3 80.3 2196\nTransformer-based:\nTrackFormer [18] / / / 63.9 65.0 3528\nTransTrack[29] 54.1 47.9 61.6 63.9 74.5 3663\nMOTR (ours) 57.8 55.7 60.3 68.6 73.4 2439\nfurther evaluate the tracking performance on DanceTrack [28] dataset with uni-\nform appearance and diverse motion, as described next.\n4.4 State-of-the-art Comparison on DanceTrack\nRecently, DanceTrack [28], a dataset with uniform appearance and diverse mo-\ntion, is introduced (see Tab. 2). It contains much more videos for evaluation\nand provides a better choice to verify the tracking performance. We further\nconduct the experiments on the DanceTrack dataset and perform the perfor-\nmance comparison with state-of-the-art methods in Tab. 4. It shows that MOTR\nachieves much better performance on DanceTrack dataset. Our method gets a\nmuch higher HOTA score, surpassing ByteTrack by 6.5%. For the AssA metric,\nour method also achieves much better performance than ByteTrack (40.2% vs.\n32.1%). While for the DetA metric, MOTR is inferior to some state-of-the-art\nmethods. It means that MOTR performs well on temporal motion learning while\nthe detection performance is not that good. The large improvements on HOTA\nare mainly from the temporal aggregation network and collective average loss.\n4.5 Generalization on Multi-Class Scene\nRe-ID based methods, like FairMOT [43], tend to regard each tracked object\n(e.g., person) as a class and associate the detection results by the feature similar-\nity. However, the association will be difficult when the number of tracked objects\nis very large. Different from them, each object is denoted as one track query in\nMOTR and the track query set is of dynamic length. MOTR can easily deal with\n12 F. Zeng et al.\nTable 4: Performance comparison between MOTR and existing methods on the\nDanceTrack[28] dataset. Results for existing methods are from DanceTrack [28].\nMethods HOTA AssA DetA MOTA IDF1\nCenterTrack [44] 41.8 22.6 78.1 86.8 35.7\nFairMOT [43] 39.7 23.8 66.7 82.2 40.8\nQDTrack [20] 45.7 29.2 72.1 83.0 44.8\nTransTrack [29] 45.5 27.5 75.9 88.4 45.2\nTraDes [40] 43.3 25.4 74.5 86.2 41.2\nByteTrack [42] 47.7 32.1 71.0 89.6 53.9\nMOTR (ours) 54.2 40.2 73.5 79.7 51.5\nTable 5: Performance comparison between MOTR and existing methods on the\nBDD100k[41] validation set.\nMethods mMOTA mIDF1 IDSw\nYu et al. [41] 25.9 44.5 8315\nDeepBlueAI [1] 26.9 / 13366\nMOTR (ours) 32.0 43.5 3493\nthe multi-class prediction problem, by simply modifying the class number of the\nclassification branch. To verify the performance of MOTR on multi-class scenes,\nwe further conduct the experiments on the BDD100k dataset (see Tab. 5). Re-\nsults on bdd100k validation set show that MOTR performs well on multi-class\nscenes and achieves promising performance with fewer ID switches.\n4.6 Ablation Study\nMOTR Components.Table 6a shows the impact of integrating different com-\nponents. Integrating our components into the baseline can gradually improve\noverall performance. Using only object query of as original leads to numerous\nIDs since most objects are treated as entrance objects. With track query in-\ntroduced, the baseline is able to handle tracking association and improve IDF1\nfrom 1.2 to 49.8. Further, adding TAN to the baseline improves MOTA by 7.8%\nand IDF1 by 13.6%. When using CAL during training, there are extra 8.3% and\n7.1% improvements in MOTA and IDF1, respectively. It demonstrates that TAN\ncombined with CAL can enhance the learning of temporal motion.\nCollective Average Loss. Here, we explored the impact of video sequence\nlength on the tracking performance in CAL. As shown in Table 6b, when the\nlength of the video clip gradually increases from 2 to 5, MOTA and IDF1 metrics\nare improved by 8.3% and 7.1%, respectively. Thus, multi-frame CAL can greatly\nboost the tracking performance. We explained that multiple frames CAL can help\nthe network to handle some hard cases such as occlusion scenes. We observed\nthat duplicated boxes, ID switches, and object missing in occluded scenes are\nsignificantly reduced. To verify it, we provide some visualizations in Fig. 5.\nMOTR: End-to-End Multiple-Object Tracking with Transformer 13\nTable 6: Ablation studies on our proposed MOTR. All experiments use the\nsingle-level C5 feature in ResNet50.\n(a) The effect of our contributions. TrackQ:\ntrack query. TAN: temporal aggregation net-\nwork. CAL: collective average loss.\nTrackQ TAN CAL MOTA‚Üë IDF1‚Üë IDS‚Üì\n- 1.2 33198\n‚úì 37.1 49.8 562\n‚úì ‚úì 44.9 63.4 257\n‚úì ‚úì 47.5 56.1 417\n‚úì ‚úì ‚úì 53.2 70.5 155\n(b) The impact of increasing video clip\nlength in Collective Average Loss dur-\ning training on tracking performance.\nLength MOTA ‚Üë IDF1‚Üë IDS‚Üì\n2 44.9 63.4 257\n3 51.6 59.4 424\n4 50.6 64.0 314\n5 53.2 70.5 155\n(c) Analysis on random track query eras-\ning probability pdrop during training.\npdrop MOTA‚Üë IDF1‚Üë IDS‚Üì\n5e-2 49.0 60.4 411\n0.1 53.2 70.5 155\n0.3 51.1 69.0 180\n0.5 48.5 62.0 302\n(d) Effect of random false positive inserting\nprobability pinsert during training.\npinsert MOTA‚Üë IDF1‚Üë IDS‚Üì\n0.1 51.2 71.7 148\n0.3 53.2 70.5 155\n0.5 52.1 62.0 345\n0.7 50.7 57.7 444\n(e) The exploration of different combinations of\nœÑex and œÑen in QIM network.\nœÑex 0.6 0.6 0.6 0.5 0.6 0.7\nœÑen 0.7 0.8 0.9 0.8 0.8 0.8\nMOTA‚Üë 52.7 53.2 53.1 53.5 53.2 52.8\nIDF1‚Üë 69.8 70.5 70.1 70.5 70.5 68.3\nIDS‚Üì 181 155 142 153 155 181\n(f) The effect of random sampling in-\nterval on tracking performance.\nIntervals MOTA ‚Üë IDF1‚Üë IDS‚Üì\n3 53.2 64.8 218\n5 50.8 62.8 324\n10 53.2 70.5 155\n12 53.1 69 158\nErasing and Inserting Track Query.In MOT datasets, there are few train-\ning samples for two cases: entrance objects and exit objects in video sequences.\nTherefore, we adopt track query erasing and inserting to simulate these two cases\nwith probability pdrop and pinsert, respectively. Table 6c reports the performance\nusing different value of pdrop during training. MOTR achieves the best perfor-\nmance when pdrop is set to 0.1. Similar to the entrance objects, track queries\ntransferred from the previous frame, whose predictions are false positives, are\ninserted into the current frame to simulate the case of object exit. In Table 6d,\nwe explore the impact on tracking performance of different pinsert. When pro-\ngressively increasing pinsert from 0.1 to 0.7, our MOTR achieves the highest\nscore on MOTA when pinsert is set to 0.3 while the IDF1 score is decreasing.\nObject Entrance and Exit Threshold.Table 6e investigates the impact of\ndifferent combination of object entrance threshold œÑen and exit threshold œÑex\nin QIM. As we vary the object entrance threshold œÑen, we can see that the\nperformance is not that sensitive to œÑen (within 0.5% on MOTA) and using an\n14 F. Zeng et al.\n!\"## $%&#'\"()*\n+,-.\")/(0+,-.\")/(0\n+,-.\")/(0\n(a) (b)\nFig. 5: The effect of CAL on solving (a) duplicated boxes and (b) ID switch\nproblems. Top and bottom rows are the tracking results without and with CAL,\nrespectively.\nentrance threshold of 0.8 produces relatively better performance. We also further\nconduct experiments by varying the object exit threshold œÑex. It is shown that\nusing a threshold of 0.5 results in slightly better performance than that of 0.6.\nIn our practice, œÑen with 0.6 shows better performance on the MOT17 test set.\nSampling Interval. In Table 6f, we evaluate the effect of random sampling\ninterval on tracking performance during training. When the sampling interval\nincreases from 2 to 10, the IDS decreases significantly from 209 to 155. During\ntraining, the network is easy to fall into a local optimal solution when the frames\nare sampled in a small interval. Appropriate increment on sampling interval can\nsimulate real scenes. When the random sampling interval is greater than 10,\nthe tracking framework fails to capture such long-range dynamics, leading to\nrelatively worse tracking performance.\n5 Limitations\nMOTR, an online tracker, achieves end-to-end multiple-object tracking. It im-\nplicitly learns the appearance and position variances in a joint manner thanks to\nthe DETR architecture as well as the tracklet-aware label assignment. However,\nit also has several shortcomings. First, the performance of detecting newborn ob-\njects is far from satisfactory (the result on the MOTA metric is not good enough).\nAs we analyzed above, detect queries are suppressed on detecting tracked ob-\njects, which may go against the nature of object query and limits the detection\nperformance on newborn objects. Second, the query passing in MOTR is per-\nformed frame-by-frame, limiting the efficiency of model learning during training.\nIn our practice, the parallel decoding in VisTR [36] fails to deal with the com-\nplex scenarios in MOT. Solving these two problems above will be an important\nresearch topic for Transformer-based MOT frameworks.\nMOTR: End-to-End Multiple-Object Tracking with Transformer 15\nAcknowledgements: This research was supported by National Key R&D Pro-\ngram of China (No. 2017YFA0700800) and Beijing Academy of Artificial Intel-\nligence (BAAI).\nReferences\n1. CodaLab Competition - CVPR 2020 BDD100K Multiple Object Tracking\nChallenge (Jul 2022), https://competitions.codalab.org/competitions/24910,\n[Online; accessed 19. Jul. 2022] 12\n2. Bergmann, P., Meinhardt, T., Leal-Taixe, L.: Tracking without bells and whistles.\nIn: ICCV (2019) 1, 3, 11\n3. Bewley, A., Ge, Z., Ott, L., Ramos, F., Upcroft, B.: Simple online and realtime\ntracking. In: ICIP (2016) 1, 3\n4. Bochinski, E., Eiselein, V., Sikora, T.: High-speed tracking-by-detection without\nusing image information. In: AVSS (2017) 1\n5. Camgoz, N.C., Koller, O., Hadfield, S., Bowden, R.: Sign language transformers:\nJoint end-to-end sign language recognition and translation. In: CVPR (2020) 3\n6. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-\nto-end object detection with transformers. In: ECCV (2020) 1, 3, 4\n7. Chang, X., Zhang, W., Qian, Y., Le Roux, J., Watanabe, S.: End-to-end multi-\nspeaker speech recognition with transformer. In: ICASSP (2020) 3\n8. Chu, P., Wang, J., You, Q., Ling, H., Liu, Z.: Transmot: Spatial-temporal graph\ntransformer for multiple object tracking. arXiv preprint arXiv:2104.00194 (2021)\n4\n9. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:\nAn image is worth 16x16 words: Transformers for image recognition at scale. In:\nICLR (2021) 3\n10. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: CVPR (2016) 6, 10\n11. Kuhn, H.W.: The hungarian method for the assignment problem. Naval research\nlogistics quarterly 2(1-2), 83‚Äì97 (1955) 3\n12. Leal-Taix¬¥ e, L., Canton-Ferrer, C., Schindler, K.: Learning by tracking: Siamese cnn\nfor robust target association. In: CVPRW (2016) 3\n13. Li, N., Liu, S., Liu, Y., Zhao, S., Liu, M.: Neural speech synthesis with transformer\nnetwork. In: AAAI (2019) 3\n14. Lin, T.Y., Goyal, P., Girshick, R., He, K., Doll¬¥ ar, P.: Focal loss for dense object\ndetection. In: ICCV (2017) 8\n15. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll¬¥ ar, P.,\nZitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014) 10\n16. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv preprint\narXiv:2103.14030 (2021) 3\n17. Luiten, J., Osep, A., Dendorfer, P., Torr, P., Geiger, A., Leal-Taix¬¥ e, L., Leibe, B.:\nHota: A higher order metric for evaluating multi-object tracking. IJCV 129(2),\n548‚Äì578 (2021) 9\n18. Meinhardt, T., Kirillov, A., Leal-Taixe, L., Feichtenhofer, C.: Trackformer: Multi-\nobject tracking with transformers. arXiv preprint arXiv:2101.02702 (2021) 1, 3, 4,\n8, 9, 10, 11\n16 F. Zeng et al.\n19. Milan, A., Leal-Taix¬¥ e, L., Reid, I., Roth, S., Schindler, K.: Mot16: A benchmark\nfor multi-object tracking. arXiv preprint arXiv:1603.00831 (2016) 9\n20. Pang, J., Qiu, L., Li, X., Chen, H., Li, Q., Darrell, T., Yu, F.: Quasi-dense similarity\nlearning for multiple object tracking. In: CVPR (2021) 11, 12\n21. Rezatofighi, H., Tsoi, N., Gwak, J., Sadeghian, A., Reid, I., Savarese, S.: General-\nized intersection over union: A metric and a loss for bounding box regression. In:\nCVPR (2019) 8\n22. Schulter, S., Vernaza, P., Choi, W., Chandraker, M.: Deep network flow for multi-\nobject tracking. In: CVPR (2017) 3\n23. Shao, S., Zhao, Z., Li, B., Xiao, T., Yu, G., Zhang, X., Sun, J.: Crowdhuman:\nA benchmark for detecting human in a crowd. arXiv preprint arXiv:1805.00123\n(2018) 10\n24. Sharma, S., Ansari, J.A., Murthy, J.K., Krishna, K.M.: Beyond pixels: Leveraging\ngeometry and shape cues for online multi-object tracking. In: ICRA (2018) 3\n25. Shi, B., Bai, X., Yao, C.: An end-to-end trainable neural network for image-based\nsequence recognition and its application to scene text recognition. TPAMI 39(11),\n2298‚Äì2304 (2016) 4\n26. Shuai, B., Berneshawi, A.G., Modolo, D., Tighe, J.: Multi-object tracking with\nsiamese track-rcnn. arXiv preprint arXiv:2004.07786 (2020) 3\n27. Stadler, D., Beyerer, J.: Modelling ambiguous assignments for multi-person track-\ning in crowds. In: Proceedings of the IEEE/CVF Winter Conference on Applica-\ntions of Computer Vision. pp. 133‚Äì142 (2022) 11\n28. Sun, P., Cao, J., Jiang, Y., Yuan, Z., Bai, S., Kitani, K., Luo, P.: Dancetrack:\nMulti-object tracking in uniform appearance and diverse motion. arXiv preprint\narXiv:2111.14690 (2021) 3, 9, 11, 12\n29. Sun, P., Jiang, Y., Zhang, R., Xie, E., Cao, J., Hu, X., Kong, T., Yuan, Z., Wang,\nC., Luo, P.: Transtrack: Multiple-object tracking with transformer. arXiv preprint\narXiv: 2012.15460 (2020) 1, 3, 4, 8, 9, 10, 11, 12\n30. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural\nnetworks. In: NeurlPS (2014) 2, 4\n31. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. In: NeurlPS (2017) 2, 3, 4\n32. Wang, Q., Zheng, Y., Pan, P., Xu, Y.: Multiple object tracking with correlation\nlearning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition. pp. 3876‚Äì3886 (2021) 11\n33. Wang, S., Sheng, H., Zhang, Y., Wu, Y., Xiong, Z.: A general recurrent track-\ning framework without real data. In: Proceedings of the IEEE/CVF International\nConference on Computer Vision. pp. 13219‚Äì13228 (2021) 11\n34. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: CVPR\n(2018) 3\n35. Wang, Y., Kitani, K., Weng, X.: Joint object detection and multi-object tracking\nwith graph neural networks. In: 2021 IEEE International Conference on Robotics\nand Automation (ICRA). pp. 13708‚Äì13715. IEEE (2021) 11\n36. Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., Xia, H.: End-to-end\nvideo instance segmentation with transformers. In: CVPR (2021) 3, 14\n37. Wang, Z., Zheng, L., Liu, Y., Li, Y., Wang, S.: Towards real-time multi-object\ntracking. In: ECCV (2020) 1, 3\n38. Welch, G., Bishop, G., et al.: An introduction to the kalman filter (1995) 3\n39. Wojke, N., Bewley, A., Paulus, D.: Simple online and realtime tracking with a deep\nassociation metric. In: ICIP (2017) 1, 3\nMOTR: End-to-End Multiple-Object Tracking with Transformer 17\n40. Wu, J., Cao, J., Song, L., Wang, Y., Yang, M., Yuan, J.: Track to detect and\nsegment: An online multi-object tracker. In: CVPR (2021) 11, 12\n41. Yu, F., Chen, H., Wang, X., Xian, W., Chen, Y., Liu, F., Madhavan, V., Darrell,\nT.: Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In:\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n(June 2020) 9, 12\n42. Zhang, Y., Sun, P., Jiang, Y., Yu, D., Yuan, Z., Luo, P., Liu, W., Wang, X.:\nBytetrack: Multi-object tracking by associating every detection box. arXiv preprint\narXiv:2110.06864 (2021) 1, 3, 10, 11, 12\n43. Zhang, Y., Wang, C., Wang, X., Zeng, W., Liu, W.: Fairmot: On the fairness of\ndetection and re-identification in multiple object tracking. IJCV pp. 1‚Äì19 (2021)\n1, 3, 11, 12\n44. Zhou, X., Koltun, V., Kr¬® ahenb¬® uhl, P.: Tracking objects as points. In: ECCV (2020)\n9, 10, 11, 12\n45. Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr: Deformable\ntransformers for end-to-end object detection. In: ICLR (2020) 1, 3, 6, 10"
}