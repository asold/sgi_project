{
  "title": "Two Stage Transformer Model for COVID-19 Fake News Detection and Fact Checking",
  "url": "https://openalex.org/W3109746361",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Vijjali, Rutvik",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227936278",
      "name": "Potluri, Prathyush",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2611653886",
      "name": "Kumar, Siddharth",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288523260",
      "name": "Teki, Sundeep",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2798416089",
    "https://openalex.org/W2789566302",
    "https://openalex.org/W1520352740",
    "https://openalex.org/W3015298864",
    "https://openalex.org/W2794520057",
    "https://openalex.org/W3109495579",
    "https://openalex.org/W2735017898",
    "https://openalex.org/W2250334791",
    "https://openalex.org/W2011645945",
    "https://openalex.org/W3039643360",
    "https://openalex.org/W2950782805",
    "https://openalex.org/W2964068236",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2977526300",
    "https://openalex.org/W3015622078",
    "https://openalex.org/W3125492565",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2159505618",
    "https://openalex.org/W2154474435",
    "https://openalex.org/W2250539671"
  ],
  "abstract": "The rapid advancement of technology in online communication via social media platforms has led to a prolific rise in the spread of misinformation and fake news. Fake news is especially rampant in the current COVID-19 pandemic, leading to people believing in false and potentially harmful claims and stories. Detecting fake news quickly can alleviate the spread of panic, chaos and potential health hazards. We developed a two stage automated pipeline for COVID-19 fake news detection using state of the art machine learning models for natural language processing. The first model leverages a novel fact checking algorithm that retrieves the most relevant facts concerning user claims about particular COVID-19 claims. The second model verifies the level of truth in the claim by computing the textual entailment between the claim and the true facts retrieved from a manually curated COVID-19 dataset. The dataset is based on a publicly available knowledge source consisting of more than 5000 COVID-19 false claims and verified explanations, a subset of which was internally annotated and cross-validated to train and evaluate our models. We evaluate a series of models based on classical text-based features to more contextual Transformer based models and observe that a model pipeline based on BERT and ALBERT for the two stages respectively yields the best results.",
  "full_text": "Two Stage Transformer Model for COVID-19 Fake News Detection and\nFact Checking\nRutvik Vijjali∗\nrutvikvijjali30@gmail.com\nPrathyush Potluri∗\npotluri.prathyush@gmail.com\nSiddharth Kumar∗\nkumar.sidiyer@gmail.com\nSundeep Teki\nsundeep.teki@gmail.com\nAbstract\nThe rapid advancement of technology in online communication via social media platforms has\nled to a proliﬁc rise in the spread of misinformation and fake news. Fake news is especially\nrampant in the current COVID-19 pandemic, leading to people believing in false and potentially\nharmful claims and stories. Detecting fake news quickly can alleviate the spread of panic, chaos\nand potential health hazards. We developed a two stage automated pipeline for COVID-19 fake\nnews detection using state of the art machine learning models for natural language processing.\nThe ﬁrst model leverages a novel fact checking algorithm that retrieves the most relevant facts\nconcerning user claims about particular COVID-19 claims. The second model veriﬁes the level of\n“truth” in the claim by computing the textual entailment between the claim and the true facts re-\ntrieved from a manually curated COVID-19 dataset. The dataset is based on a publicly available\nknowledge source consisting of more than 5000 COVID-19 false claims and veriﬁed explana-\ntions, a subset of which was internally annotated and cross-validated to train and evaluate our\nmodels. We evaluate a series of models based on classical text-based features to more contextual\nTransformer based models and observe that a model pipeline based on BERT and ALBERT for\nthe two stages respectively yields the best results.\n1 Introduction\nElectronic means of communication have helped to eliminate time and distance barriers to sharing and\nbroadcasting information. However, despite all its advantages, faster means of communication has also\nresulted in extensive spread of misinformation. The world is currently going through the deadly COVID-\n19 pandemic and fake news regarding the disease, its cures, its prevention and causes have been broadcast\nwidely to millions of people. The spread of fake news and misinformation during such precarious times\ncan have grave consequences leading to widespread panic and ampliﬁcation of the threat of the pandemic\nitself. It is therefore of paramount importance to limit the spread of fake news and ensure that accurate\nknowledge is disseminated to the public.\nIn this work, we propose a robust, dynamic fake news detection system, that can not only estimate the\n“correctness” of a claim but also provides users with pertinent information regarding the said claim. This\nis achieved using a knowledge base of veriﬁed information that can be constantly updated. Previous\nwork on fake news detection has primarily focused on evaluating the relationship measured via a textual\nentailment task between a header and the body of the article. However, such a method is insufﬁcient\nfor identifying speciﬁc fake news claims without any knowledge of the facts relevant to the claim. This\nwarrants the use of a new dataset speciﬁc to the COVID-19 pandemic.\nDeveloping a solution for such a task involves generating a database of factual explanations, which be-\ncomes our knowledge base, that serves as ground truth for any given claim. We compute the entailment\nbetween any given claim and explanation to verify if the claim is true or not. Querying for claim, ex-\nplanation pairs for each explanation in our knowledge base is computationally expensive and slow, so\n∗* These authors contributed equally.\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://\ncreativecommons.org/licenses/by/4.0/.\narXiv:2011.13253v1  [cs.CL]  26 Nov 2020\nwe propose generating a set of candidate explanations which are contextually similar to the claim. We\nachieve this by using a model trained with relevant and irrelevant claim explanation pairs, and using a\nsimilarity metric between the two to match them. Therefore, in the pipeline, ﬁrstly, for a given claim, a\nset of candidate explanations from the knowledge base have to be fetched in real-time. Then, the claim\nvalidated for truth using relevant candidate explanations. In this work, we have explored the use of\nTransformer (Vaswani et al., 2017) based models to both fetch relevant explanations as well as measure\nthe entailment between a given claim and a factual explanation.\nWe then evaluate our model on the basis of count of relevant explanation results fetched as well as the\naccuracy in verifying a given claim. We demonstrate the effectiveness of pre-trained multi-attention mod-\nels in terms of overall accuracy when compared with other natural language processing (NLP) baselines\nwhile maintaining near real-time performance.\n2 Related Work\nThe paper (Riedel et al., 2017) uses traditional approaches with a simple classiﬁer model that makes use\nof Term Frequency(TF), Term Frequency and Inverse Document Frequency(TF-IDF), and cosine simi-\nlarity between vectors as features to classify fake news. They have provided a baseline for fake news\nstance detection on Fake News Challenge (FNC-1) dataset 1. We have implemented their approach on\nour dataset and results can be seen in Table 3.\nIn (Nie et al., 2019), the authors present a connected system consisting of three homogeneous neural\nsemantic matching models that perform document retrieval, sentence selection, and claim veriﬁcation\non the FEVER Dataset (Thorne et al., 2018) jointly for fact extraction and veriﬁcation. Their Neural\nSemantic Matching Network (NSMN) is a modiﬁcation of the Enhanced Sequential Inference Model\n(ESIM) (Chen et al., 2017), where they add skip connections from input to matching layer and change\noutput layer to only max-pool plus one afﬁne layer with ReLU activation. They use three stage pipeline\nin which given a claim, they ﬁrst retrieve candidate documents from the corpus, followed by retriev-\ning candidate sentences from the selected candidate documents and ﬁnally, the last stage classiﬁes the\nsentence in to one of three classes. They used Bidirectional LSTM (BiLSTM) to encode the claim and\nsentences using GloVe (Pennington et al., 2014) and ELMO (Peters et al., 2018) embeddings. However,\nthese tasks are concerned with static or slowly evolving domains, on topics that do not require domain\nexpertise to annotate.\nDespite the partial success of the above methods, there were still certain shortcomings in terms of the\naccuracy of the results. Bidirectional encoder representations from Transformers (BERT) (Devlin et al.,\n2018) is a pre-trained language model trained on a large corpus comprising the Wikipedia and Toronto\nBook Corpus, and is shown to perform well on several natural language tasks like GLUE (Wang et al.,\n2018). Transformer based pretrained models achieved state of the art results in several NLP subtasks,\ntheir ease of ﬁne-tuning makes them adaptable to newer tasks. In (Jwa et al., 2019), the authors propose\na model based on the BERT architecture to detect fake news by analyzing the contextual relationship\nbetween the headline and the body text of news. They demonstrate that using Transformer based mod-\nels like BERT and ﬁne-tuning them for the task of fake news detection gives better results than other\nmodels like stackLSTM (Hanselowski et al., 2018; Hermans and Schrauwen, 2013) and featMLP (Davis\nand Proctor, 2017). They further enhanced their model performance by pre-training with domain spe-\nciﬁc news and articles, and countered the class imbalance for the ﬁnal classiﬁcation task by the use of a\nweighted cross entropy loss function. However, this approach can not be adapted for our task of fetching\nrelevant explanations for each claim.\n(Naud´e, 2020) and (Bullock et al., 2020) extensively studied the use of machine learning strategies to\naddress various issues regarding COVID-19. The latter also describes fake news and misinformation as\na major issue in the ongoing pandemic and further highlights the problem as solving an infodemic. In\n(Gallotti et al., 2020), the authors develop an Infodemic Risk Index (IRI) after analyzing Twitter posts\n1http://www.fakenewschallenge.org/\nFigure 1: Cross validated data examples\nacross various languages and calculate the rate at which a particular user from a locality comes across\nunreliable posts from different classes of users like veriﬁed humans, unveriﬁed humans, veriﬁed bots,\nand unveriﬁed bots. In (Mejova et al., 2018), the authors examine Facebook advertisements across 64\ncountries and ﬁnd that around 5% of advertisements contained possible errors or misinformation. But\nnone of these mentioned works tackle the problem of misinformation by reasoning out the given fake\nclaim with an explanation.\n3 Dataset\nUsing an existing misinformation dataset will not serve as a reliable knowledge base for training and\nevaluating the models due to the recent and uncommon nature i.e., the vocabulary used to describe the\ndisease and the terms associated with the COVID-19 pandemic. It is important to generate real and\ntimely datasets to ensure accurate and consistent evaluation of the methods. To overcome this drawback,\nwe manually curated a dataset speciﬁc to COVID-19. Our proposed dataset consists of 5500 claim and\nexplanation pairs. We describe the collection and annotation process in Section 3.1.\n3.1 Covid-19 Claims Dataset\nThere are multiple sources on the web that are regularly identifying and debunking fake news on COVID-\n19. We scraped data from “Poynter”2 , a fact checking website which collects fake news and debunks or\nfact-checks them with supporting articles from more than 70 countries, covering more than 40 languages.\nThe Poynter website has a database exclusively for COVID-19 with over 7000 fact checks. Each fact\ncheck contains the corresponding claim that is being checked, the rating indicating the type of the claim,\nfor example - ’False’, ’Mostly False’ or ’Misleading’, the name of the fact checker, the explanation given\nfor the current claim, the location of origin of the claim and the date when the fact check was done. This\ndata can be used to update our ”explanation” look-ups in a timely fashion so our database is constantly\nevolving as we learn more about the virus and the facts change.\nFor each fact check, we collect only the ”claim” and the corresponding “explanation” from this\ndatabase which were rated as ’False’ or ’Misleading’. In this way, we collected about 5500 false-claim\nand explanation pairs. We further manually rephrase these false claims to generate true claims, as the\nones that align with the explanation so as to create an equal proportion of true-claim and explanation\npairs. We have taken claim-explanation pairs from the time period between January 1,2020 to May\n2https://www.poynter.org/ifcn-covid-19-misinformation/\nDataset Number of sentence pairs\nFalse claim - Explanation pairs 5500\nCross validated False claim - True claim - Explanation pairs for train data 1000\nCross validated False claim - True claim - Explanation pairs for test data 200\nTable 1: Dataset Information\n15,2020 for our training data and from May 18,2020 to July 1,2020 for our test data. In this way, we\nevaluate the generalization of the model on completely new and unseen data. Our collected data follows\nthe structure:\n[false claim, explanation]\nThe subset of the data that we annotated and cross validated follows the structure:\n[false claim, true claim, explanation]\nFigure 1 shows some examples of the cross validated data.\n3.2 Dataset Statistics\nThe current proposed Covid-19 dataset contains cross-validated claim-explanation sentence pairs. Statis-\ntics about the distribution of labels are provided in Table 1. This is a dynamic dataset and we are contin-\nually collecting and curating additional claim-explanation pairs. We plan to open source this dataset to\nfacilitate more research in this domain.\n4 Methodology\nThe architecture consists of a two stage model, we will refer to the ﬁrst model as “Model A” and the\nsecond model as “Model B”. The objective of Model A is to fetch the candidate “true facts” or expla-\nnations for a given claim, which are then evaluated for entailment using the Model B. Next, we describe\nthe training procedure as well as intended run time behaviour for both Model A and Model B.\nFigure 2: Block diagram of our two stage model pipeline\n4.1 Model A\nFirst, to fetch relevant explanations, we train our Transformer model on a binary sentence entailment\ntask, where the claims and explanations are the two sentences fed in as input separated by a [SEP] tag.\nWe generate negative claim-explanation pairs through random sampling to ensure that equal proportions\nof positive and negative pairs are present. Training multi-attention network with our COVID-19 speciﬁc\ndata enables the model to capture long-range correlations between the vector representations of claims\nand explanations of similar contexts. We train our models with a base encoder and a sequence classiﬁca-\ntion head on top for binary classiﬁcation of the labels. The model is trained to optimise the cross entropy\nloss.\nThrough our experiments, we ﬁnd that, on this trained model, if we generate embeddings for a single sen-\ntence (either claim or explanation individually) and compare matching [claim, explanation] embeddings\nusing the cosine similarity metric, there is a distinction in the distribution of similarity scores between re-\nlated and unrelated [claim,explanation] pairs. Therefore, for faster near real-time performance, we cache\nthe embeddings for all our explanations (knowledge base) beforehand, and compute the cosine similarity\nbetween the claim and the cached embeddings of the explanations. The vector of the [CLS](the start of\nsentence) token of the ﬁnal layer works as a strong representation of the entire sentence, although we\nfound that taking element-wise mean over all the token vectors leads to better performance. We fetch\nthe top explanations for any given claim exceeding a certain threshold of sentence similarity as there\ncould be several explanations relevant for a given claim. This threshold is determined on the basis of\nthe summary statistics of the cosine similarity metric between the claim and relevant explanations in the\nvalidation set, as described in Section 5.3. These retrieved explanations serve as candidates for verifying\nthe accuracy of the claim through Model B.\n4.2 Model B\nThe second part of the pipeline is to identify the veracity of a given claim. Model A fetches the candidate\nexplanations while Model B is used to verify whether the given claim aligns with our set of candidate\nexplanations or not. We can therefore treat this task as a textual entailment problem (Dagan et al., 2013;\nAdler et al., 2012). To train the Model B, we use a smaller subset of “false claim” and “explanation” pairs\nfrom our original dataset, and cross validate each sample with “true claim” or in other words, claims that\nalign with the factual explanation. However, this small annotated data is not sufﬁcient to train the model\neffectively. Therefore, the parameters of the Model A, which was trained on a much larger dataset were\nused as initial parameters for Model B, and ﬁne-tuned further using our cross validated dataset.\nWe trained Model B in a similar fashion as Model A i.e. as a sequence classiﬁcation problem with cross\nentropy loss. Once we have the candidate explanations for a given claim, we use Model B to estimate\nthe probabilities of alignment of claim with each of the candidate explanations. We used the statistic of\nmean probability score and standard deviation of aligning and non-aligning claim and explanation pairs\nin the validation set to determine the thresholds for Model B classiﬁcation. We trained and evaluated\nboth Model A and Model B using several approaches based on classical NLP methods as well as more\nsophisticated pre-trained Transformer models. The ﬂow of the Model A + Model B pipeline is shown in\nFigure 2.\n5 Experiments\n5.1 Baseline Models\nFor baselining our model on classical NLP approaches, we use two simple bag-of-words (BOW) rep-\nresentations for the text inputs: term frequency (TF) and term frequency-inverse document frequency\n(TF-IDF). We followed the architecture proposed by (Riedel et al., 2017). The representations and fea-\ntures extracted from the claim and explanation pairs consist of the following: • The TF vector of the\nclaim; • The TF vector of the explanation; • The cosine similarity between the 2-normalised TF-IDF\nvectors of the claim and explanation.\nFor the TF vectors, we extract a vocabulary of the 5,000 most frequent words in the training set and\nexclude stop words (the NLTK (Bird et al., 2009) stop words for the English language). For the TF-IDF\nvectors, we use the same vocabulary and set of stop words. The TF vectors and the TF-IDF cosine sim-\nilarity values are concatenated to form a feature vector with total dimension of 10,001 and is fed to the\nclassiﬁer.\nThe classiﬁer takes an input of 10,001 dimensional vector followed by a feed-forward network with 50,\n20 and 2 dimensional dense layers respectively with each hidden layer having tanh activation and the\nlast layer is a softmax layer. We trained in mini-batches over the entire training set using the Adam\nModel Model A Val Accuracy Model B Val Accuracy\nTF-IDF 0.832 0.799\nGloVe 0.781 0.777\nMobileBERT 0.921 0.877\nALBERT 0.927 0.956\nBERT 0.944 0.927\nTable 2: Model Performance on Validation Set\noptimiser (Kingma and Ba, 2014) with a learning rate of 0.001.\nThe second approach involves use of word vectors for which we used 300-dimensional GloVe (Penning-\nton et al., 2014) embeddings, pretrained on 2014-Wikipedia and Gigaword, and averaged over token\nembeddings to compute sentence vectors. So, for a given claim and explanation pair, we have a 300-\ndimension vector for claim as well as the explanation, both of which are concatenated to form a 600-\ndimensional vector that serves as input to our dense layer classiﬁer. This model is a simple feed-forward\nneural network with 4 hidden layers having 200, 100, 50 and 2 hidden units respectively with the ﬁrst\nthree layers having ReLU activation while the last layer is a softmax layer. We trained in mini-batches\nof 32 over the entire training set with back-propagation using the Adam optimizer with a learning rate of\n0.001.\n5.2 Transformer Models\nWe trained and evaluated three Transformer based pre-trained models for both Model A and Model B us-\ning the training strategy described in Section 4. As our focus was to ensure that the proposed pipeline can\nbe deployed effectively in a near real-time scenario, we restricted our experiments to models that could\nefﬁciently be deployed using inexpensive compute. We chose the following three models - BERT(base),\nALBERT (Lan et al., 2019) and MobileBERT (Sun et al., 2020). The authors of MobileBERT demon-\nstrated that using a teacher-student learning technique for progressive knowledge transfer from the BERT\nto MobileBERT model helps them achieve a task-agnostic model similar to BERT and can be deployed\neasily on resource limited devices due to faster inference speeds and lower memory consumption. The\nALBERT model was proposed to increase the training and inference speed of BERT besides lowering the\nmemory consumption. The authors demonstrate that the use of their parametric reduction techniques and\na custom self supervised loss helps it to achieve results similar to BERT while having fewer parameters.\nModel A was trained on 5000 claim-explanation pairs on the sequence classiﬁcation task to optimize the\nsoftmax cross entropy loss using a learning rate of 3e-5. This trained model was then validated on a test\nset comprising of 1000 unseen claim-explanation pairs. The training data structure here looks like:\n[claim, relevant explanation, 1]\n[claim, irrelevant explanation, 0]\nModel B was trained on a smaller subset of 800 cross validated [claim,explanation,label] data, on the\nsame sequence classiﬁcation task, where the label was assigned based on whether the claim aligned with\nthe explanation - 1 or not - 0. This was validated on 200 unseen data-points. The loss function used was\nsoftmax cross-entropy with a uniform learning rate of 1e-5. The training data structure here looks like:\n[true claim, relevant explanation, 1]\n[false claim, relevant explanation, 0]\n5.3 Evaluation Metrics\nFor evaluating the performance of the overall pipeline model, we ﬁrst evaluate the performance of Model\nA in its ability to retrieve relevant explanation. For this we use Mean Reciprocal Rank(MRR) (Craswell,\n2009) and Mean Recall @10 (Malheiros et al., 2012), that is the proportion of claims for which the\nrelevant explanation was present in the top 10 most contextual explanation by cosine similarity and their\nmean inverse rank. Equation 1 shows the MRR formula for our evaluation.\nMRR = 1\nC\nC∑\ni=1\n1\nranki\n(1)\nwhere ranki is the position of the explanation that is relevant to a particular claim according to test data\nand C is the total number of claims.\nEquation 2 shows the Recall@10 formula for our evaluation.\nRecall@10 = 1\nC\nC∑\ni=1\nf(top-10 explanations) (2)\nwhere\nf(x) =\n{\n1 if true exp ∈ x\n0 otherwise\nHere, true exp is the actual relevant explanation for a particular claim according to test data and C is\nthe total number of claims.\nOnce, Model A has retrieved relevant explanations, we evaluate the performance of Model B on comput-\ning the veracity of the claim. Here, we only used explanations that exceed an empirically deﬁned thresh-\nold in cosine similarity between the claim and the explanation. Through our experiments, we found that\na threshold of mean - standard deviation of cosine similarity over the validation data worked well for\npicking relevant explanations. For evaluating the accuracy, we take a mean of the output probabilities for\neach claim, explanationi, deﬁned by the Equation 3.\nptruth = 1\nn\n∑\ni\nmodelB(claim, expi ) (3)\nwhere\nexp ∈ expA | cos(c vecA, expvecA) > t\nHere, expA are the top-10 explanations returned by Model A. c vecA and exp vecA are the vec-\ntor representations generated by running claim and explanations individually through Model A and t\nrefers to the threshold.\n5.4 Results and Discussion\nThe performance of several Transformer based models as well as classical NLP models were compared\nusing the evaluation metrics described in Section 5.3. The results of the experiments on the test set are\nsummarised in Table 3.\nModel MRR Recall@10 Accuracy\nTF-IDF 0.477 0.635 0.525\nGloVe 0.182 0.410 0.579\nMobileBERT 0.561 0.735 0.710\nBERT 0.632 0.795 0.810\nALBERT 0.582 0.675 0.825\nBERT+ALBERT 0.632 0.795 0.855\nTable 3: Model Performance on test set\nThe results in Table 3 clearly illustrate that Transformer based models are signiﬁcantly better than\nclassical NLP models. An interesting observation was that some models are better at retrieval of relevant\nModel\nLatency\nper claim\n(in seconds)\nMemory\n(in MB)\nTF-IDF 0.108 16\nGloVe 0.003 990\nMobileBERT 0.607 1200\nALBERT 2.376 942\nBERT 3.106 1910\nBERT + ALBERT 2.471 1398\nTable 4: Model Compute performance and Memory usage\nexplanations while others have a better classiﬁcation performance. We ﬁnd that a combination of the\nbest performing Model A (BERT) and best performing Model B (ALBERT) yielded the highest MRR,\nRecall@10 and Accuracy on the test set for fact checking. We however do acknowledge that our models\ncould still make errors of two kinds: ﬁrstly, Model A might not fetch a relevant explanation which au-\ntomatically means that the prediction provided by Model B is irrelevant, and secondly, Model A might\nhave fetched the correct explanation(s) but Model B classiﬁes it incorrectly. We show some of the errors\nour models made in Table 5.\nTable 4 shows the memory usage and latencies of the implemented models. The memory consumption\nand latency per claim in the classical NLP models was observed to be quite low in comparison to the\nTransformer based models. This is expected due to the lower parameter size of the TF-IDF and GloVe\nmodels. Among the Transformer based models, MobileBERT had the least latency per claim as ex-\npected and explained in Section 5.2 while ALBERT consumed the least memory. The best performing\nBERT+ALBERT model utilized a memory of 1398MB and fetched relevant explanations of each claim\nin 2.471 seconds. The model latencies and memory usage were evaluated on an Intel Xeon - 2.3GHz\nSingle core - 2 thread CPU.\nClaim: Cannabis could help prevent coronavirus infection .\nTrue Explanation: The study claiming that marijuana can cure coronavirus did not pass the peer\nreview , it was conducted on artiﬁcial human tissues and not on real organisms . It is a classic\npreliminary research that may even fail . The authors themselves speak of the need for further\nstudies and research .\nTop Fetched Explanation: The vaccine can provide stable immunity , while the presence of\nantibodies does not prevent reinfection .\nRemark: Model A fetched irrelevant explanation for this claim\nClaim: The vaccine is not the ﬁnal solution against the novel coronavirus but antibodies are .\nExplanation: The vaccine can provide stable immunity , while the presence of antibodies does not\nprevent reinfection .\nProbability score: 0.340\nRemark: Model B misclassiﬁed this claim-explanation pair as True\nClaim: WHO recommends wearing masks in public spaces to slow down the spread of coronavirus\nExplanation: The WHO changed its position about masks by now recommending community\nmasks in areas with many infections . And it says that masks have to be used properly and alone\ncan’t protect you from COVID-19 .\nProbability score: 0.686\nRemark: Model B misclassiﬁed this claim-explanation pair as False\nTable 5: BERT+ALBERT Model Faulty predictions\n6 Conclusions and Future work\nIn this work, we have demonstrated the use and effectiveness of pre-trained Transformer based language\nmodels in retrieving and classifying fake news in a highly specialized domain of COVID-19. Our pro-\nposed two stage model performs signiﬁcantly better than other baseline NLP approaches. Our knowledge\nbase, that we prepare through collecting factual data from reliable sources from the web can be dynamic\nand change to a large extent, without having to retrain our models again for as long as the distribution is\nconsistent. All of our proposed models can run in near real-time with moderately inexpensive compute.\nOur work is based on the assumption that our knowledge base is accurate and timely. This assumption\nmight not always be true in a scenario such as COVID-19 where “facts” are changing as we learn more\nabout the virus and its effects. Therefore a more systematic approach is needed for retrieving and classi-\nfying claims using this dynamic knowledge base. Our future work consists of weighting our knowledge\nbase on the basis of the duration of the claims and benchmarking each claim against novel sources of\nground truth.\nOur model performance can be further boosted by better pre-training, through domain speciﬁc knowl-\nedge. In one of the more recent work by (Guo et al., 2020), the authors propose a novel semantic textual\nsimilarity dataset speciﬁc to COVID-19. Pre-training our models using such speciﬁc datasets could help\nin better understanding of the domain and ultimately better performance.\nFake news and misinformation is an increasingly important and a difﬁcult problem to solve, especially\nin an unforeseen situation like the COVID-19 pandemic. Leveraging state of the art machine learning\nand deep learning algorithms along with preparation and curation of novel datasets can help address the\nchallenge of fake news related to COVID-19 and other public health crises.\nReferences\nMeni Adler, Jonathan Berant, and Ido Dagan. 2012. Entailment-based text exploration with application to the\nhealth-care domain. In Proceedings of the ACL 2012 System Demonstrations, pages 79–84.\nSteven Bird, Ewan Klein, and Edward Loper. 2009. Natural language processing with Python: analyzing text with\nthe natural language toolkit. ” O’Reilly Media, Inc.”.\nJoseph Bullock, Katherine Hoffmann Pham, Cynthia Sin Nga Lam, Miguel Luengo-Oroz, et al. 2020. Mapping\nthe landscape of artiﬁcial intelligence applications against covid-19. arXiv preprint arXiv:2003.11336.\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Diana Inkpen, and Si Wei. 2017. Neural natural language inference\nmodels enhanced with external knowledge. arXiv preprint arXiv:1711.04289.\nNick Craswell. 2009. Mean reciprocal rank. Encyclopedia of database systems, 1703.\nIdo Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto. 2013. Recognizing textual entailment:\nModels and applications. Synthesis Lectures on Human Language Technologies, 6(4):1–220.\nRichard Davis and Chris Proctor. 2017. Fake news, real consequences: Recruiting neural networks for the ﬁght\nagainst fake news.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805.\nRiccardo Gallotti, Francesco Valle, Nicola Castaldo, Pierluigi Sacco, and Manlio De Domenico. 2020. Assessing\nthe risks of” infodemics” in response to covid-19 epidemics. arXiv preprint arXiv:2004.03997.\nXiao Guo, Hengameh Mirzaalian, Ekraam Sabir, Aysush Jaiswal, and Wael Abd-Almageed. 2020. Cord19sts:\nCovid-19 semantic textual similarity dataset. arXiv preprint arXiv:2007.02461.\nAndreas Hanselowski, Avinesh PVS, Benjamin Schiller, Felix Caspelherr, Debanjan Chaudhuri, Christian M\nMeyer, and Iryna Gurevych. 2018. A retrospective analysis of the fake news challenge stance detection task.\narXiv preprint arXiv:1806.05180.\nMichiel Hermans and Benjamin Schrauwen. 2013. Training and analysing deep recurrent neural networks. In\nAdvances in neural information processing systems, pages 190–198.\nHeejung Jwa, Dongsuk Oh, Kinam Park, Jang Mook Kang, and Heuiseok Lim. 2019. exbake: Automatic fake\nnews detection model based on bidirectional encoder representations from transformers (bert). Applied Sci-\nences, 9(19):4062.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019.\nAlbert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942.\nYuri Malheiros, Alan Moraes, Cleyton Trindade, and Silvio Meira. 2012. A source code recommender system to\nsupport newcomers. In 2012 IEEE 36th Annual Computer Software and Applications Conference, pages 19–24.\nIEEE.\nYelena Mejova, Ingmar Weber, and Luis Fernandez-Luque. 2018. Online health monitoring using facebook\nadvertisement audience estimates in the united states: evaluation study. JMIR public health and surveillance ,\n4(1):e30.\nWim Naud´e. 2020. Artiﬁcial intelligence against covid-19: An early review.\nYixin Nie, Haonan Chen, and Mohit Bansal. 2019. Combining fact extraction and veriﬁcation with neural semantic\nmatching networks. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 6859–\n6866.\nJeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word represen-\ntation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP),\npages 1532–1543.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. 2018. Deep contextualized word representations. In Proc. of NAACL.\nBenjamin Riedel, Isabelle Augenstein, Georgios P Spithourakis, and Sebastian Riedel. 2017. A simple but tough-\nto-beat baseline for the fake news challenge stance detection task. arXiv preprint arXiv:1707.03264.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. 2020. Mobilebert: a\ncompact task-agnostic bert for resource-limited devices. arXiv preprint arXiv:2004.02984.\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. Fever: a large-scale dataset\nfor fact extraction and veriﬁcation. arXiv preprint arXiv:1803.05355.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and\nIllia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages\n5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint\narXiv:1804.07461.",
  "topic": "Misinformation",
  "concepts": [
    {
      "name": "Misinformation",
      "score": 0.8667618632316589
    },
    {
      "name": "Computer science",
      "score": 0.7444361448287964
    },
    {
      "name": "Coronavirus disease 2019 (COVID-19)",
      "score": 0.605418860912323
    },
    {
      "name": "Social media",
      "score": 0.5616886615753174
    },
    {
      "name": "Transformer",
      "score": 0.5569590330123901
    },
    {
      "name": "Textual entailment",
      "score": 0.519360363483429
    },
    {
      "name": "Pipeline (software)",
      "score": 0.5085922479629517
    },
    {
      "name": "News aggregator",
      "score": 0.4637497067451477
    },
    {
      "name": "Fake news",
      "score": 0.45062050223350525
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44035810232162476
    },
    {
      "name": "Language model",
      "score": 0.41639888286590576
    },
    {
      "name": "Natural language processing",
      "score": 0.32331061363220215
    },
    {
      "name": "Computer security",
      "score": 0.22960570454597473
    },
    {
      "name": "Logical consequence",
      "score": 0.19003239274024963
    },
    {
      "name": "World Wide Web",
      "score": 0.18609720468521118
    },
    {
      "name": "Internet privacy",
      "score": 0.17994141578674316
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Infectious disease (medical specialty)",
      "score": 0.0
    },
    {
      "name": "Disease",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}