{
  "title": "Multimodal Motion Prediction with Stacked Transformers",
  "url": "https://openalex.org/W3139279104",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2134074473",
      "name": "Liu Yi-cheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2385129061",
      "name": "Zhang Jing-huai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202137365",
      "name": "Fang, Liangji",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2367655678",
      "name": "Jiang, Qinhong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2962448985",
      "name": "Zhou, Bolei",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2955189650",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3034722190",
    "https://openalex.org/W2964121718",
    "https://openalex.org/W2962687116",
    "https://openalex.org/W3048875849",
    "https://openalex.org/W3090166818",
    "https://openalex.org/W2618011341",
    "https://openalex.org/W2988499426",
    "https://openalex.org/W2963759562",
    "https://openalex.org/W2997470645",
    "https://openalex.org/W3010427722",
    "https://openalex.org/W3062588417",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3103005696",
    "https://openalex.org/W2970116586",
    "https://openalex.org/W2963914175",
    "https://openalex.org/W2963001155",
    "https://openalex.org/W3035671534",
    "https://openalex.org/W3011344527",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2963818059",
    "https://openalex.org/W2963906196",
    "https://openalex.org/W3081405043",
    "https://openalex.org/W2424778531",
    "https://openalex.org/W2963677766",
    "https://openalex.org/W3108486966",
    "https://openalex.org/W2134089414",
    "https://openalex.org/W3035054225",
    "https://openalex.org/W2975065512",
    "https://openalex.org/W2983397630",
    "https://openalex.org/W2607296803",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W2980160556",
    "https://openalex.org/W2886617718"
  ],
  "abstract": "Predicting multiple plausible future trajectories of the nearby vehicles is crucial for the safety of autonomous driving. Recent motion prediction approaches attempt to achieve such multimodal motion prediction by implicitly regularizing the feature or explicitly generating multiple candidate proposals. However, it remains challenging since the latent features may concentrate on the most frequent mode of the data while the proposal-based methods depend largely on the prior knowledge to generate and select the proposals. In this work, we propose a novel transformer framework for multimodal motion prediction, termed as mmTransformer. A novel network architecture based on stacked transformers is designed to model the multimodality at feature level with a set of fixed independent proposals. A region-based training strategy is then developed to induce the multimodality of the generated proposals. Experiments on Argoverse dataset show that the proposed model achieves the state-of-the-art performance on motion prediction, substantially improving the diversity and the accuracy of the predicted trajectories. Demo video and code are available at https://decisionforce.github.io/mmTransformer.",
  "full_text": "Multimodal Motion Prediction with Stacked Transformers\nYicheng Liu1⋆ Jinghuai Zhang2⋆ Liangji Fang2 Qinhong Jiang2 Bolei Zhou1\nThe Chinese University of Hong Kong1 SenseTime Research2\nAbstract\nPredicting multiple plausible future trajectories of the\nnearby vehicles is crucial for the safety of autonomous\ndriving. Recent motion prediction approaches attempt to\nachieve such multimodal motion prediction by implicitly\nregularizing the feature or explicitly generating multiple\ncandidate proposals. However, it remains challenging since\nthe latent features may concentrate on the most frequent\nmode of the data while the proposal-based methods depend\nlargely on the prior knowledge to generate and select the\nproposals. In this work, we propose a novel transformer\nframework for multimodal motion prediction, termed as\nmmTransformer. A novel network architecture based on\nstacked transformers is designed to model the multimodal-\nity at feature level with a set of ﬁxed independent propos-\nals. A region-based training strategy is then developed to\ninduce the multimodality of the generated proposals. Ex-\nperiments on Argoverse dataset show that the proposed\nmodel achieves the state-of-the-art performance on mo-\ntion prediction, substantially improving the diversity and\nthe accuracy of the predicted trajectories. Demo video\nand code are available at https://decisionforce.\ngithub.io/mmTransformer.\n1. Introduction\nPredicting the future trajectories of nearby vehicles is\ncritical for the Autonomous Vehicle systems to understand\nthe surrounding and make informative decisions. Multi-\nmodal prediction, which aims to generate multiple plausible\ntrajectories of the target vehicle, plays a key role to handle\nthe uncertainty in motion prediction and improve the safety\nof motion planning. Due to the uncertain future events, traf-\nﬁc vehicles could perform differently even under the same\nscene. However, there is only one ground truth trajectory\ncollected in each driving scene. Hence one challenge for\nenabling multimodal prediction lies in how to learn to cover\nall the possible outcomes in a given scene with limited train-\ning samples.\n⋆ Co-ﬁrst authors with equal contributions.\nFigure 1. Examples of multimodal motion prediction in complex\ndriving scenarios. For each moving vehicle near the ego car, three\nplausible future trajectories are predicted by the proposed model.\nRecent motion prediction methods mainly follow prob-\nabilistic approaches [20, 17, 30] or proposal-based ap-\nproaches [35, 28, 5, 11] to address the aforementioned is-\nsue. The probabilistic approaches implicitly model the un-\ncertainty of the trajectory through deﬁning the underlying\npossible models as a latent variable. They either achieve the\nmultimodal prediction with generator conditioned on differ-\nent latent variables, or directly constrain the output over a\nprobability distribution(e.g., GMM) to get diverse results.\nThese methods depend heavily on the predeﬁned prior dis-\ntribution and the well-designed loss function, which might\nbe prone to the optimization instability and the mode col-\nlapse issue. Unlike probabilistic approaches which gen-\nerate multimodal outputs through modeling the latent dis-\ntribution of the modality, the proposal-based approaches\n[11, 35, 5, 27] perform in an alternative way, which ﬁrst de-\nﬁnes candidate points or trajectories as proposals, and then\nregress or classify these proposals to the ground truth. With\npredeﬁned proposals, these methods alleviate optimization\nburden and narrow down the feasible space of solutions. Al-\nthough these methods achieve good performance, they still\nhave the following two issues: 1) The result relies heavily\non the quality of the predeﬁned anchors since the heuristic\nmethods are applied to sample the candidate points. 2) The\nmultimodal prediction can not be guaranteed since multi-\nmodal nature of trajectory prediction is not well captured\nwith only one ground truth provided during the training.\nIn this work, we propose a novel end-to-end multimodal\nmotion prediction framework called MultiModal Trans-\n1\narXiv:2103.11624v2  [cs.CV]  24 Mar 2021\nformer ( mmTransformer), where the proposals are ﬁrst\nrandomly initialized and then reﬁned to incorporate con-\ntextual information. mmTransformer is designed based on\nthe transformer architecture, which proves to be effective\nin modeling sequential data. The whole model can be\nviewed as stacked transformers in which the past trajec-\ntories, the road information, and the social interaction are\naggregated hierarchically with several transformer encoder-\ndecoder modules. Two multimodal prediction examples of\nthe whole trafﬁc scenes are shown in Fig 1.\nWe develop two new mechanisms to ameliorate the uni-\nmodal effects brought by identical features. First, we in-\ntroduce a trajectory proposal mechanism to the ﬁeld of mo-\ntion prediction. Speciﬁcally, queries in the decoders of mm-\nTransformer are represented as trajectory proposals, which\nasymptotically aggregate multiple channels of contextual\ninformation from encoders, and make independent predic-\ntions. Since these proposals are orthogonal with each other,\neach of them will carry customized features, which pro-\nmotes the diversity and multimodality. Second, a region-\nbased training strategy (RTS) is developed to explicitly en-\nsure the multimodality, which negotiates the conﬂicts be-\ntween the uniqueness of ground truth and multimodal na-\nture of predictions. We divide the surrounding space into\nseveral regions and group trajectory proposals into differ-\nent sets, with each set being assigned to one region. During\ntraining, only the set of proposals assigned to the region\nwhere ground truth locates will be utilized to optimize the\nframework. This new strategy enforces individual proposal\nto focus on a speciﬁc mode, without compromising the la-\ntent features learned by other proposals.\nThe contributions of this paper are summarized as fol-\nlows: (1) To the best of our knowledge, mmTransformer\nis the ﬁrst model using stacked transformers for trajectory\nproposals to aggregate multiple channels of contextual in-\nformation and achieve multimodal prediction. (2) To pre-\nserve the multimodal nature of motion forecasting, we de-\nsign a novel region-based training strategy, which ensures\nthat each individual proposal is capable of capturing a spe-\nciﬁc mode. (3) Extensive experiments show the substantial\nimprovement brought by the proposed model architecture\nand the tailored region-based training strategy. Our model\nranked the 1st on the Leaderboard of Argoverse benchmark\ndated on 16 Nov 2020, and remains competitive on the\nleaderboard.\n2. Related Work\nMotion Prediction. Recently deep learning-based meth-\nods have been widely used for motion prediction [1, 15, 8,\n2, 11]. The typical pipeline is to ﬁrst create the input rep-\nresentation by rasterizing [10, 34] or vectorizing [7, 22, 12]\nsurrounding information and then use deep neural network\n(e.g., CNN, Long Short-Term Memory(LSTM), graph neu-\nral network[33, 16]) to extract informative features. Finally,\nthe trajectory is directly generated by model [12] or based\non prior knowledge [11, 35, 5].\nTo this end, motion prediction methods can be roughly\ndivided into two categories: feature-based and proposal-\nbased methods. For the ﬁrst kind of methods, most of\nthem focus on how to extract useful information from the\nenvironment. CNN Encoder-Decoder is proposed in [33]\nto extract features from vehicles’ past positions and direc-\ntions and directly regress the future positions. Graph neu-\nral networks [22] have emerged in response to problems\nthat scenes cannot be easily represented by matrices of pix-\nels or simple vectors. SoPhie [29] have leveraged features\nextracted from the physical environment, attention mecha-\nnisms, and adversarial training. However, for these feature-\nbased methods it is difﬁcult to guarantee the multimodal\nprediction of the model. As for another kinds of models\n[35, 28, 5, 11], where the candidate trajectory set is ﬁrst gen-\nerated based on prior knowledge and candidate point and\nthen optimize and reduce these candidate trajectory by de-\nsigned cost function or post-processing to obtain ﬁnal pre-\ndiction. Although these methods successfully modeling the\nmultimodality, there are still many unsolved problems. As\nthese methods focus on the manipulation of the predicted\ntrajectories or candidates, the meanings of latent features\nare typically neglected, which may hurts the stability of the\nmodel. As a result, performance will be signiﬁcantly in-\nﬂuenced by the robustness of prior hypothesis made by the\nauthor.\nDifferent from above, our model achieves the multi-\nmodal prediction at both feature and proposal levels. The\nregion-based training strategy further reﬁnes the proposals\nwhich reduces the correlation of the proposals and guaran-\ntees the diversity of the predicted trajectories. Meanwhile,\nby explicitly considering modality into proposal during\ntraining process, mmTransformer has a more interpretable\npipeline.\nTransformer. Transformer is a novel attention-based\nmethod which was ﬁrstly introduced in [31]. It has been\nsuccessfully deployed in several applications ( e.g., neural\nmachine translation and image caption generation [9, 25]).\nThe most important part of transformer is the self-attention\nmechanism. The advantage of the attention mechanism in\ntransformer lies in its capability of learning high quality\nfeatures through taking the whole context into considera-\ntion. Some of the recent methods in the ﬁeld of trajectory\nforecasting adopt the attention mechanisms in sequence and\ninteraction modeling [21, 13, 26]. For example, an interac-\ntion transformer [21] is introduced to model the interaction\nbetween trafﬁc vehicles. Ind-TF [13] replaces RNN with\nvanilla transformer to model the trajectory sequences. Un-\nlike these methods that use transformer as a part of their\n2\nFigure 2. Overview of mmTransformer: The proposed mmTransformer uses stacked transformers as the backbone, which aggregates\nthe contextual information on the ﬁxed trajectory proposals. Proposal feature decoder further generates the trajectories and corresponding\nconﬁdence scores by decoding each proposal feature through the trajectory generator and selector, respectively. Besides, the trajectory\ngenerator and trajectory selector are two feed-forward network, which have the same structure with the FFN in the transformer [31]. The\ndetailed network architecture can be found in appendix.\nfeature extractor, a fully transformer based architecture is\nused in our case to solve the multimodal motion prediction\nproblem.\n3. Multimodal Motion Prediction Framework\nMotion prediction aims to accurately predict the future\nmotion of target vehicles, given the history trajectories of\ntrafﬁc vehicles in the scene and other contextual informa-\ntion such as road and trafﬁc information. To tackle the mul-\ntimodal motion prediction, we ﬁrstly learn a feature set Y\ncomprising various proposal features y ∈ Y. Each y is\ngenerated from Fθ(x), where x is the scene information,\ninvolving motion history and surrounding context. With the\nset of proposal features, we can generate multiple future tra-\njectories S ={si ∈RT×2 : 1≤i ≤K}by Gφ(y), in which\nT denotes future horizon andKdenotes the total number of\npredictions. Additionally, prediction set S ⊂S where Sis\nthe entire space of possible S, and Fθ(·) and Gφ(·) are pa-\nrameterized by θand φ, respectively.\nWe aim to construct an appropriate set of proposal fea-\ntures Y ⊂ Yto ensure both accuracy and multimodal-\nity. Therefore, we introduce a novel mmTransformer to ob-\ntain informative proposals and then apply the region-based\ntraining strategy to ensure the multimodality of proposals.\n3.1. Stacked Transformers\nTransformer has demonstrated outstanding performance\nin dealing with sequential data. In order to apply trans-\nformer to trajectory prediction, we need to extend the model\nto incorporate a variety of the contextual information, be-\ncause the vanilla transformer only supports encoding single\ntype of data ( e.g., the corpus token in the language trans-\nformer [9], and image in the visual transformer [4]). A\nnaive solution is to concatenate all types of inputs such as\npast trajectory and lane information into a sequence of con-\ntextual embeddings and input them to the transformer. As\nthe transformer requires a ﬁxed size of the input, a naive\nsolution will consume a large amount of resources. Addi-\ntionally, since different types of information will compound\nin such design and be aggregated by the attention layers, the\nquality of the latent feature might be compromised. There-\nfore, we consider the alternative of incorporating multiple\nchannels of information separately.\nUnder the circumstance of different inputs, the challenge\nlies in how to incorporate multiple channels of information\nas input to the transformer. we propose to deﬁne the queries\nof transformer decoder as trajectory proposals, tailored to\nour multimodal trajectory prediction task. This design is in-\nspired by the parallel version of transformer used in [4].\nIts strength is that parallel trajectory proposals can inte-\ngrate the information from the encoder independently, al-\nlowing each single proposal to carry disentangled modality\ninformation. The stacked architecture adapts to the multi-\ninput circumstance with several tailored feature extractors,\nintegrating different contextual information hierarchically.\nSpeciﬁcally, the structure of the stacked transformers con-\nsists of three individual transformer units, motion extractor,\nmap aggregator, and social constructor respectively, each\n3\ntaking the updated trajectory proposals from the previous\ntransformer as the input of its decoder to reﬁne the propos-\nals. The framework is illustrated in Fig 2.\nFor simplicity, the transformer modules retain the struc-\nture introduced by [4]. Since the transformer decoder is\npermutation-invariant, the K proposals must be distinct to\neach other in order to represent different modes and gener-\nate different trajectories. A learned positional encoding is\nadded before each decoder layer of the transformer mod-\nules. We illustrate the three components in more detail as\nfollows.\nMotion Extractor. The encoder input of motion extractor is\nthe history trajectories of observed vehicles as H = {hi ∈\nRTobs×2 : 1≤i≤Nvehicle}, where Nvehicle is the number\nof the observed vehicles, including the target vehicle, and\nTobsis the length of history observation. The decoder inputs\nare the trajectory proposals Y = {yi ∈Rn : 1≤i≤K},\ntextcolorredwhich are initialized by a set of learnable po-\nsitional encoding, feature size of each encoding is n. The\noutputs of decoder can be considered as proposal features.\nIt is noted that all the observed vehicles share the same ar-\nchitecture of motion extractor, the same for map aggregator.\nMap Aggregator. As the behaviors of vehicles depend\nlargely on the topology of the map, such as road structure,\nwe utilize this transformer to fuse the geometric and seman-\ntic information from high deﬁnition map to reﬁne features\nof the input proposals. Following [12], we encode each cen-\nterline segment into vectorized representation, and use the\nsubgraph module in [12] to process each vectorized poly-\nlines. After that, the latent features of the polylines are fed\nto the map aggregator to explicitly model the scene. Bene-\nﬁting from the interpretability of encoder-decoder attention\nmodule inside transformer, the proposal can retrieve the cor-\nresponding map features based on its preassigned modality.\nSocial Constructor. Unlike the previous transformers, so-\ncial constructor encodes the vehicle features among all ob-\nserved vehicles, aiming to model the interactions between\nthem. In particular, the vehicle feature of each observed\nvehicle is obtained by summarizing all the proposals for\neach of vehicles via a multi-layer perception (MLP). The\nvehicle feature can be also viewed as distribution of future\nmovements for each trafﬁc vehicle. Since our objective is to\nforecast the future trajectory of the target vehicle, we only\nutilize the decoder of social constructor to update the pro-\nposals for target vehicles, instead of all vehicles, in pursuit\nof higher efﬁciency.\nAs a whole, the motivation behind our framework is to\nestablish the intra-relation inside data ( e.g., extracting the\nmap topology with the encoder of map aggregator), and\nintegrate contextual information from different encoders\nasymptotically to update each proposal and highlight its\npre-assigned modality.\nIntuitively, stacked transformers can be divided into two\nFigure 3. Overview of the region-based training strategy. We ﬁrst\ndistribute each proposal to one of the M regions. These propos-\nals, shown in colored rectangles, learn corresponding proposal fea-\nture through the mmTransformer. Then we select the proposals\nassigned to the region where the GT endpoints locate, generate\ntheir trajectories as well as their associated conﬁdence scores, and\ncalculate the losses for them.\nparts. First part only encodes the information of each ve-\nhicle individually by the motion extractor and the map\naggregator, without any interaction information being in-\nvolved. Then the social constructor is applied to aggregate\nthe nearby information and model the dependency among\nall the vehicles. Thus, the order is logically sequential, i.e.,\nthe social relation should be constructed based on individual\nvehicle features. Additionally, the order of the other trans-\nformers has been veriﬁed empirically by experimental re-\nsults shown in Table 2. Based on this stacked architecture,\nour model can capture the latent connection between con-\ntextual information and the diverse proposal features which\nensures the multimodal predictions.\n3.2. Proposal Feature Decoder\nThe ﬁnal Proposal Feature Decoder comprises two\nbranches, namely, the Trajectory Generator for trajectory\nprediction and the Trajectory Selector branch for proposal\nscoring. For each of the K target proposals, we apply a\nthree-layer MLP G(·) to generate the prediction as follows,\nS ={si ∈RT×2 : si = G(yi),yi ∈Y,1 ≤i ≤K} (1)\nwhere yi ∈Y is the ith proposal feature generated from\nsocial constructor, s is a tensor of predicted trajectory, T is\nthe number of future time steps.\nFor scoring, we apply the MLP, W(·), with the same\nstructure as regression branch to generate the Kconﬁdence\nscores for each of the trajectory proposals.\nC ={ci ∈R : ci = W(yi),1 ≤i ≤K,} (2)\n3.3. Region-based Training Strategy\nAs shown in previous work [35], direct regression for all\nthe trajectory proposals leads to the mode average problem,\n4\nwhich hampers the multimodalily property of the model.\nOne feasible solution to overcome the mode average prob-\nlem is to calculate the regression loss and classiﬁcation loss\nonly using the proposal with the minimum ﬁnal displace-\nment error. We term this as the vanilla training strategy.\nAlthough our model achieves competitive results under this\ntraining strategy using 6 trajectory proposals, the predic-\ntion results are conﬁned to a local region around the ground\ntruth trajectory due to the small number of proposals are\nused. However, the modality collapsing issue occurs when\nwe attempt to increase the number of proposals to improve\ndiversity.\nTo address the limitation of the vanilla training strategy,\nwe propose a novel training strategy called region-based\ntraining strategy (RTS), which groups trajectory proposals\ninto several spatial clusters based on the spatial distribution\nof the ground truth endpoints, and optimizes the framework\nto improve the prediction results within each cluster.\nThe strategy is illustrated in Fig 3. For each single sce-\nnario, we ﬁrst rotate the scene to align the heading direction\nof the target vehicle to +y axis, and make all coordinates\ncentered at the last observation point of the target vehicle.\nBased on that, we partition the sample space of target ve-\nhicles into M regions, without any overlaps between them.\nThe detailed analysis of region shape and region number\nis illustrated in Section 4.3 and the procedure of the par-\ntition is illustrated in appendix. After that, we equally di-\nvide the total Kproposals of mmTransformer into M parts,\nwith each of them assigned to a speciﬁc region. As a re-\nsult, each region will possessNindividual proposals, where\nN = K/M. It is noted that the pre-processing in our work\nensures that all the samples can share the same partition\nmap.\nDuring the training, we utilize the regression loss and\nclassiﬁcation loss in a similar way to vanilla training strat-\negy. The difference is that we calculate the loss for all the\nproposals that are assigned to the region where ground truth\nendpoint locate, rather than the one closest to the ground\ntruth. In this way, we improve the multimodal results in\na region-based manner, which optimizes the predictions in\none region without affecting any other regions.\n3.4. Training Objective\nSince all the modules are differentiable, our framework\nyields fully supervised end-to-end training. The ﬁnal loss\nof our model is composed of regression loss, scoring loss,\nand an auxiliary loss for multi-task learning. They are the\nHuber loss for regression, the KL divergence for scoring tra-\njectories, and an auxiliary loss for the region classiﬁcation\nrespectively. The detail of losses is as follows:\nRegression Loss. Lreg is the Huber loss over per-step co-\nordinate offsets.\nLreg = 1\nN\nN∑\ni=1\nLHuber(si,sgt), (3)\nwhere si is the i-th predicted trajectory generated by pro-\nposal feature decoder, and thesgt is the ground truth trajec-\ntory.\nConﬁdence Loss. To assign each trajectory a conﬁdence\nscore, we follow the [35] to tackle this scoring problem of\ntrajectory prediction via a maximum entropy model,\nτ(y) = exp (W(y))∑N\ni=1 exp (W(yi))\n, (4)\nYregion = {y1,..., yN},\nwhere Yregion ⊂Y is a subset of proposal features selected\nby the region-based training strategy from Y.\nλ(s) = exp (−D(s,sgt))∑N\ni=1 exp (−D(si,sgt))\n, (5)\nwhere D(si) of each predicted trajectory si is deﬁned\nby the L2 distance of its endpoint to ground truth endpoint,\nD(si,sj) =∥si,T −sj,T∥2.\nLconf = 1\nN\nN∑\ni\nDKL(λ(si)||τ(yi)). (6)\nAs we want the distribution of the predicted score to stay\nclose to the target distribution calculated in Eq 5, We use\nthe Kullback-Leibler Divergence as the loss function.\nClassiﬁcation Loss. We introduces region classiﬁcation\nloss to ensure model can identify correct region. When the\nproposal number K is large, we ﬁnd that using such auxil-\niary loss Lcls helps to regularize the conﬁdence loss, which\naccelerates the convergence of mmTransformer + RTS. The\ndetails of Lcls is provided in appendix.\nIntermediate Layer Losses. In order to accelerate the\ntraining process, we add Proposal Feature Decoder and the\naforementioned combination of losses after each decoder\nlayer of social constructor.\nTotal Loss. Since the total loss function can be viewed as\nthe summation of multiple distinct tasks, we use multi-task\nlearning approach in [18, 23] to balance them.\nL= 1\nσ2\n1\nLreg + 1\nσ2\n2\nLconf + 1\nσ2\n3\nLcls +\n3∑\ni=1\nlog(σi + 1), (7)\nwhere σi,i ∈{1,2,3}are learnable loss weights.\n5\nMethods minADE minFDE MR(%)\nNN [6] 1.7129 3.2870 53.69\nLSTM ED [6] 2.34 5.44 -\nTNT [35] (4th) 0.9358 1.5384 13.28\nLGCN [22] (7th) 0.8679 1.3640 16.34\nLA [27] (21th) 0.9436 1.5486 21.79\nWIMF [19] (8th) 0.8995 1.4220 16.69\nmmTrans. (5th) 0.8435 1.3383 15.42\nmmTrans.+RTS (1st) 0.8704 1.3688 13.00\nTable 1. Comparison with state-of-the-art methods on the Argov-\nerse test set (K=6). Here, mmTrans. stands for 6-proposal mm-\nTransformer, while mmTrans.+RTS stands for 36-proposal mm-\nTransformer trained with RTS.\n3.5. Inference\nDuring inference, all the K proposals are used to gener-\nate ﬁnal results. In order to merge multimodal predictions, a\nselection algorithm inspired by the non-maximum suppres-\nsion algorithm is used to reject near-duplicate trajectories\nbased on the euclidean distances of endpoints (the detailed\nprocedure can be found in appendix).\n4. Experiments\n4.1. Experimental Setup\nDataset. We perform experiments on Argoverse motion\nforecasting benchmark [6], which involves 340k 5s long\ntrajectory sequences and corresponding contextual informa-\ntion. The sequences are split into 205,942 training, 39,472\nvalidation and 78,143 testing cases, respectively. Given a 2-\nsecond history trajectory and the context as inputs, the goal\nis to forecast the future movements of the target vehicle over\nthe next 3 seconds. For each scenario, local map informa-\ntion can be represented as a set of centerline-based poly-\nlines from HD map. Besides, past trajectories and locations\nof the adjacent vehicles and the ego-car are also included in\norder to model the interaction between them.\nMetrics. We evaluate our model in terms of the widely used\nAverage Displacement Error (ADE) and Final Displace-\nment Error (FDE). Due to the multimodal nature of trajec-\ntory prediction, minADE, minFDE and miss rate (MR) of\nthe top K (K=6) trajectories are also reported following the\nevaluation criteria of the Argoverse benchmark.\n4.2. Results\nWe compare our model with the state-of-the-art methods\nin the test set of Argoverse. The scores of different meth-\nods in Table 1 were extracted before the CVPR submission\ndeadline (16/11/2020) from the Argoverse Leaderboard.\nAs shown in Table 1, we include the results of vanilla\nmmTransformer and mmTransformer model trained with\nMotion Map Social RTS Proposal minADE minFDE MR(%)\n✓ 6 0.915 1.681 23.3\n✓ ✓ 6 0.794 1.284 14.4\n✓ ✓ 6 0.826 1.418 17.3\n✓ ✓ ✓ 6 0.713 1.153 10.6\n✓ ✓ ✓ 36 0.833 1.453 17.6\n✓ ✓ ✓ ✓ 36 0.721 1.211 9.2\nTable 2. Ablation study on the effectiveness of different com-\nponents of mmTransformer on the Argoverse validation dataset.\nAs shown in the last two rows of Table 2, same model without\nRTS shows a poorer performance when other condition remain the\nsame.\nRTS. It shows that our models achieve the best perfor-\nmance in terms of all the metrics, which indicates that our\nmethod is capable of learning high quality proposal features\nby employing stacked transformers, and achieving promis-\ning multimodal results by using the RTS. Compared to the\n6-proposal mmTransformer without RTS , we observe a\nslightly drop of minADE and minFDE in the 36-proposal\nmmTransformer with RTS. It is the large number of pro-\nposals that leads to the drop of minADE and minFDE.\nWe explain the reason using outcome of 36-proposal mm-\nTransformer as an example: Limitted by the ﬁxed num-\nber (6) of ﬁnal outputs, we discard the redundant candi-\ndate proposals to retain the diversity( i.e., MR) during the\npost-processing. As a side effect, the number of selected\nproposals in GT region is decreased, which may hurt the\naccuracy, i.e. minFDE and minADE. We regard this as a\ntrade-off between accuracy and diversity. Besides, the ac-\ncruacy drop caused by post-processing is a common issue in\nmachine learning community [3], especially when selecting\nﬁnal predictions from a large candidate set.\nVisualization of Multimodal Motion Prediction.In Fig 4,\nwe showcase multimodal prediction results of mmTrans-\nformer on the Argoverse validation set. We can see that\nmmTransformer generates trajectories covering all the plau-\nsible modes in each driving scenario. Although mmTrans-\nformer itself is capable to make reasonable predictions (col-\numn 1,2) with trajectory proposals, it fails to achieve com-\nparable performance as mmTransformer+RTS in more chal-\nlenging scenarios (column 3,4), as the modalities may con-\ncentrate in a speciﬁc area.\n4.3. Ablation Study\nWe ﬁrst conduct ablation study to analyze the impor-\ntance of each component in our model. Then, we evaluate\ntwo different partition methods, K-means and manual parti-\ntion. We ﬁnally measure impact of the number of region M\nand the number of proposal in each region N on mmTrans-\nformer+RTS.\nImportance of Each Transformer Module and RTS. To\n6\nResults of 36-proposal mmTransformer+ RTS filtered by confidence score\nResults of 6-proposal mmTransformer\nFigure 4. Qualitative comparison between mmTransformer (6 proposals) and mmTransformer+RTS (36 proposals) on four driving scenarios\nfrom Argoverse validation set. Green rectangle represents the target vehicle. The red and yellow arrows indicate the groundtruth and\npredicted trajectories with high conﬁdence scores. For a clear visualization, we ﬁlter the trajectories with scores lower than the uniform\nprobability in each scenario. It can be seen that most of plausible areas are covered by our prediction results.\nanalyze the importance of each component in mmTrans-\nformer, we compare the results of several models on Ar-\ngoverse validation set. We consider the motion extractor in\nmmTransformer as the baseline model, and progressively\nadd other transformer modules to aggregate contextual in-\nformation, and utilize RTS to encourage multimodal pre-\ndiction. Observations can be drawn from the experimental\nresults shown in Table 2.\nFirstly, all the structural components contribute to the\nperformance of the framework. We observe that the MR\nis improved from 23.3% to 10.6% by applying all the trans-\nformer modules (row 1-4). With the contextual informa-\ntion being captured and incorporated by each module, the\nmodel gains more comprehensive understanding of the sce-\nnario. For example, the map information captured by map\naggregator brings useful road features (e.g. layout of lane\nlines), which beneﬁts the overall performance (from 23.3%\nto 14.4%). Also, The model with map aggregator and social\nconstructor further promotes the MR to 10.6%. It is noted\nthat we stack these modules hierarchically rather than fol-\nlowing a parallel design because of the logical relationship\nbetween different contextual information.\nBesides, RTS facilities the ﬁnal results by increasing\nthe number of proposals to encourage multimodal predic-\ntion. As shown in Table 2, the region-based training strat-\negy boosts the performance by a large margin, from 17.6%\nto 9.2% in MR. We attribute the large improvement to the\nlarge amount of proposals used by the RTS. However, train-\ning large number of proposals with vanilla strategy can not\nresult in the comparable performance, since the optimiza-\nPartition method minADE minFDE MR(%)\nK-means 0.72 1.21 9.21\nManual partition 0.73 1.23 9.13\nTable 3. Impact of different partition algorithms on the Argoverse\nvalidation dataset.\ntion of a single proposal compromises the others under this\nsetting. The comparison results demonstrate that RTS helps\nto preserve the modality information.\nSpatial partitions. We evaluate two ways to divide the sur-\nrounding space for RTS. The ﬁrst one adopts constrained\nK-means [32], while we manually split the space into fan-\nshaped regions(similar with Fig 3) for the another one.\nTraining samples are evenly distributed in each region for\ndata balance. For a fair comparison, we partition the space\ninto 6 regions, according to the number of regions, and as-\nsign 6 proposals (represented as regional proposals) to each\nregion. Compared to K-means based partition algorithm,\nmanual partition can successfully divide some blurry sam-\nples to correct region. Since we assume that the misclassi-\nﬁed samples may perturb the learning of regional proposal,\nmanual partition can, therefore, achieve slightly higher per-\nformance, as shown in Table 3. How to classify the samples\nto help training remains to be an open topic.\nNumber of Proposals. We further conduct experiments to\nexplore the appropriate number of proposals and regions\nutilized in mmTransformer+RTS. We hypothesis that the\nnumber of regions (M) and the number of proposals in each\n7\nR0R1R3 R4R2R5\nMR Matrix of Region Proposals Proposals  assign to each regionRegion\nFigure 5. Visualization of the multimodal prediction results on Argoverse validation set. We utilize all trajectory proposals of mmTrans-\nformer to generate multiple trajectories for each scenario and visualize all the predicted endpoints in the ﬁgures (left three columns). For\nclear illustration, we ﬁlter the points with conﬁdences lower than the uniform probability( 1/K). The background represents all the pre-\ndicted results and colored points indicate the prediction results of a speciﬁc group of proposals (regional proposals). We observe that the\nendpoints generated by each group of regional proposals are within the associated region. Miss Rate (MR) matrix of regional proposals is\nshown on the upper right, where the value in each cell (i, j) represents the MR calculated by proposals assigned to region i and ground\ntruth in region j. The proposals possess high accuracy when the GT is located in their region. For reference, the layout of the regions\nproduced by constrained K-means[32] is shown in the bottom right.\nN\nMR(%) M 3 6 9\n1 28.87 20.67 23.86\n6 11.65 9.21 9.37\n8 12.96 9.23 9.31\nTable 4. Impact of number of region M and the number of pro-\nposal in each region N on the Argoverse validation dataset.\nregion (N) jointly control the concentration and coverage of\npredicted trajectories. We ﬁnd that the ratio betweenMand\nN affects the performance signiﬁcantly when total num-\nber of proposal is not very large. Speciﬁcally, the perfor-\nmance drops when the ratio is far way from 1. However, the\nperformance increases marginally and even becomes worse\nwhen the total number of proposal is large, regardless of the\nchanging of ratio. According to our experiments, the model\ngives the most desirable performance when M and N both\nequal to 6.\nVisualization of Region-based Training Strategy. Fig 5\nillustrates the effectiveness of RTS (36 proposals). We con-\nduct the experiments on Argoverse validation set. As shown\nin the MR matrix of Fig 5, cell (i,j) represents the missing\nrate of proposals assigned to i-th region (named as region\nproposals) in predicting all cases that belong to j-th region.\nThe low MR in diagonal indicates that the regional pro-\nposals have learned specialization in assigned region. We\nobserve that each proposal tends to generate the trajectory\nwhich ends in the preassigned region, which demonstrates\nthat mmTransformer has learned different modalities in a\nregion-based manner.\n5. Conclusion\nWe develop a transformer-based motion prediction\nmodel called mmTransformer for accurate multimodal pre-\ndiction. A novel partition training method is introduced to\nimprove the multimodal prediction. The experiments show\nthe competitive result on the Argoverse benchmark.\nAcknowledgement: This project was partially supported\nby the Centre for Perceptual and Interactive Intelligence\n(CPII) Ltd under the Innovation and Technology Fund. The\nauthors would like to thank Xin Zhang for his insightful dis-\ncussions.\nAppendix\nA. Implementation Details\nFollowing previous transformer-based approaches [4,\n31], we utilize AdamW[24] as the optimizer, with the ini-\ntial learning rate, weight decay and gradient max norm set\nto 1 ×10−3, 1 ×10−4 and 0.1 respectively. All parameters\nin mmTransformer are initialized using Xavier initialization\n[14]. All transformer modules in mmTransformer contain\n128 hidden units. Each transformer module has two encoder\nlayers and two decoder layers, except for the decoder of so-\ncial constructor, which contains four layers. The map infor-\n8\nFigure 6. The detailed structure of the transformer module.\nmation in our implementation covers the 65m×65mlocal\nregion, centered at the position of target agent at the last ob-\nservation point. Besides, the heading direction of the target\nagent at last observation point is aligned to+yaxis, as men-\ntioned before. To enhance the robustness of the model, we\nfurther conduct data augmentation by ﬂipping the trajecto-\nries horizontally and randomly masking the trajectories at\nthe ﬁrst ten time steps.\nB. Detailed Architecture\nThe detailed architecture of the transformer used in mm-\nTransformer is visualized in Fig 6. Surrounding information\nis passed to the encoder to derive memory of the contextual\nfeatures. Besides, spatial positional encodings are added to\nthe queries and keys at each multi-head self-attention layer.\nThen, the decoder receives proposals (randomly initialized),\npositional encoding of proposals, as well as encoder mem-\nory, and produces the reﬁned trajectory proposals through\nmultiple multi-head self-attention and decoder-encoder at-\ntention layers. It is noted that the ﬁrst self-attention layer in\nthe ﬁrst decoder layer of motion extractor can be skipped.\nC. Classiﬁcation Loss\nThe idea is to encourage the trajectory proposals which\nare assigned to the ground truth region to have higher\nscores. Speciﬁcally, each logit in loss term is the sum of\nthe scores belonging to the corresponding region\nP = {pi ∈R : pi =\ni+N∑\nj=i\ncj}, (8)\nStep 1 Step 2 Step 3\nFigure 7. Visualization of the partition procedure.\nwhere i ∈{1,N + 1,2N + 1,..., (M −1)N + 1}, then\nwe apply the cross entropy loss to calculate the penalty as\nLcls = −\nM∑\ni=1\nδ(i −gt) log pi, (9)\nwhere the gt is the ground truth region index and δ is a\nindicator function. The auxiliary loss beneﬁts to the gener-\nalization and convergence of our model.\nD. The Procedure of Partition Algorithm\nLet’s take K-means as a example to described partition\nprocedure. Step1: Extract all normalized GT trajectory end-\npoints, using normalization described in line 435 of our pa-\nper. Step2: Apply constrained K-means [32] to divide these\nsamples into M clusters equally. Step3: Find the vertices\nof each region with convex hull algorithm; gather these ver-\ntices to form the regions. The procedure is visualized in\nFig 7\nE. Inference\nDuring the inference stage, we utilize NMS algorithm to\nﬁlter duplicated trajectories. The detail of NMS algorithm\ngoes as follow: we ﬁrst sort the predicted trajectories ac-\ncording to their conﬁdence scores in descending order, and\nthen pick them greedily. Speciﬁcally, we set a threshold\nand exclude trajectories that are close to any of the selected\ntrajectories. We keep repeating above two steps until col-\nlecting sufﬁcient predicted trajectories.\nReferences\n[1] Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan,\nAlexandre Robicquet, Li Fei-Fei, and Silvio Savarese. So-\ncial lstm: Human trajectory prediction in crowded spaces. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 961–971, 2016. 2\n[2] Florent Altch ´e and Arnaud de La Fortelle. An lstm network\nfor highway trajectory prediction. In 2017 IEEE 20th Inter-\nnational Conference on Intelligent Transportation Systems\n(ITSC), pages 353–359. IEEE, 2017. 2\n[3] Navaneeth Bodla, Bharat Singh, Rama Chellappa, and\nLarry S Davis. Soft-nms–improving object detection with\none line of code. In Proceedings of the IEEE international\nconference on computer vision, pages 5561–5569, 2017. 6\n9\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. arXiv preprint\narXiv:2005.12872, 2020. 3, 4, 8\n[5] Yuning Chai, Benjamin Sapp, Mayank Bansal, and Dragomir\nAnguelov. Multipath: Multiple probabilistic anchor tra-\njectory hypotheses for behavior prediction. arXiv preprint\narXiv:1910.05449, 2019. 1, 2\n[6] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jag-\njeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Pe-\nter Carr, Simon Lucey, Deva Ramanan, et al. Argoverse:\n3d tracking and forecasting with rich maps. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 8748–8757, 2019. 6\n[7] Hang Chu, Daiqing Li, David Acuna, Amlan Kar, Maria\nShugrina, Xinkai Wei, Ming-Yu Liu, Antonio Torralba, and\nSanja Fidler. Neural turtle graphics for modeling city road\nlayouts. In Proceedings of the IEEE International Confer-\nence on Computer Vision, pages 4522–4530, 2019. 2\n[8] Nachiket Deo and Mohan M Trivedi. Convolutional social\npooling for vehicle trajectory prediction. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition Workshops, pages 1468–1476, 2018. 2\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018. 2, 3\n[10] Nemanja Djuric, Vladan Radosavljevic, Henggang Cui, Thi\nNguyen, Fang-Chieh Chou, Tsung-Han Lin, and Jeff Schnei-\nder. Motion prediction of trafﬁc actors for autonomous\ndriving using deep convolutional networks. arXiv preprint\narXiv:1808.05819, 2018. 2\n[11] Liangji Fang, Qinhong Jiang, Jianping Shi, and Bolei Zhou.\nTpnet: Trajectory proposal network for motion prediction.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 6797–6806, 2020. 1,\n2\n[12] Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, Dragomir\nAnguelov, Congcong Li, and Cordelia Schmid. Vectornet:\nEncoding hd maps and agent dynamics from vectorized rep-\nresentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 11525–\n11533, 2020. 2, 4\n[13] Francesco Giuliari, Irtiza Hasan, Marco Cristani, and Fabio\nGalasso. Transformer networks for trajectory forecasting.\narXiv preprint arXiv:2003.08111, 2020. 2\n[14] Xavier Glorot and Yoshua Bengio. Understanding the difﬁ-\nculty of training deep feedforward neural networks. In Pro-\nceedings of the thirteenth international conference on artiﬁ-\ncial intelligence and statistics, pages 249–256, 2010. 8\n[15] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese,\nand Alexandre Alahi. Social gan: Socially acceptable tra-\njectories with generative adversarial networks. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 2255–2264, 2018. 2\n[16] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term\nmemory. Neural computation, 9(8):1735–1780, 1997. 2\n[17] Joey Hong, Benjamin Sapp, and James Philbin. Rules of the\nroad: Predicting driving behavior with a convolutional model\nof semantic interactions. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, pages\n8454–8462, 2019. 1\n[18] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task\nlearning using uncertainty to weigh losses for scene geome-\ntry and semantics. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 7482–7491,\n2018. 5\n[19] Siddhesh Khandelwal, William Qi, Jagjeet Singh, Andrew\nHartnett, and Deva Ramanan. What-if motion prediction\nfor autonomous driving. arXiv preprint arXiv:2008.10587,\n2020. 6\n[20] Namhoon Lee, Wongun Choi, Paul Vernaza, Christopher B\nChoy, Philip HS Torr, and Manmohan Chandraker. Desire:\nDistant future prediction in dynamic scenes with interacting\nagents. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 336–345, 2017. 1\n[21] Lingyun Luke Li, Bin Yang, Ming Liang, Wenyuan Zeng,\nMengye Ren, Sean Segal, and Raquel Urtasun. End-to-end\ncontextual perception and prediction with interaction trans-\nformer. arXiv preprint arXiv:2008.05927, 2020. 2\n[22] Ming Liang, Bin Yang, Rui Hu, Yun Chen, Renjie Liao, Song\nFeng, and Raquel Urtasun. Learning lane graph representa-\ntions for motion forecasting. In European Conference on\nComputer Vision, pages 541–556. Springer, 2020. 2, 6\n[23] Lukas Liebel and Marco K ¨orner. Auxiliary tasks in multi-\ntask learning. arXiv preprint arXiv:1805.06334, 2018. 5\n[24] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 8\n[25] Christoph L ¨uscher, Eugen Beck, Kazuki Irie, Markus Kitza,\nWilfried Michel, Albert Zeyer, Ralf Schl ¨uter, and Hermann\nNey. Rwth asr systems for librispeech: Hybrid vs attention–\nw/o data augmentation. arXiv preprint arXiv:1905.03072 ,\n2019. 2\n[26] Jean Mercat, Thomas Gilles, Nicole El Zoghby, Guil-\nlaume Sandou, Dominique Beauvois, and Guillermo Pita\nGil. Multi-head attention for multi-modal joint vehicle mo-\ntion forecasting. In 2020 IEEE International Conference on\nRobotics and Automation (ICRA) , pages 9638–9644. IEEE,\n2020. 2\n[27] Jiacheng Pan, Hongyi Sun, Kecheng Xu, Yifei Jiang, Xi-\nangquan Xiao, Jiangtao Hu, and Jinghao Miao. Lane at-\ntention: Predicting vehicles’ moving trajectories by learning\ntheir attention over lanes. arXiv preprint arXiv:1909.13377,\n2019. 1, 6\n[28] Tung Phan-Minh, Elena Corina Grigore, Freddy A Boulton,\nOscar Beijbom, and Eric M Wolff. Covernet: Multimodal\nbehavior prediction using trajectory sets. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 14074–14083, 2020. 1, 2\n[29] Amir Sadeghian, Vineet Kosaraju, Ali Sadeghian, Noriaki\nHirose, Hamid Rezatoﬁghi, and Silvio Savarese. Sophie:\nAn attentive gan for predicting paths compliant to social and\nphysical constraints. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , pages 1349–\n1358, 2019. 2\n10\n[30] Charlie Tang and Russ R Salakhutdinov. Multiple futures\nprediction. In Advances in Neural Information Processing\nSystems, pages 15424–15434, 2019. 1\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998–6008, 2017. 2,\n3, 8\n[32] Kiri Wagstaff, Claire Cardie, Seth Rogers, Stefan Schr ¨odl,\net al. Constrained k-means clustering with background\nknowledge. In Icml, volume 1, pages 577–584, 2001. 7,\n8, 9\n[33] Takuma Yagi, Karttikeya Mangalam, Ryo Yonetani, and\nYoichi Sato. Future person localization in ﬁrst-person\nvideos. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 7593–7602, 2018. 2\n[34] Yifan Zhang, Jinghuai Zhang, Jindi Zhang, Jianping Wang,\nKejie Lu, and Jeff Hong. A novel learning framework for\nsampling-based motion planning in autonomous driving. In\nProceedings of the AAAI Conference on Artiﬁcial Intelli-\ngence, volume 34, pages 1202–1209, 2020. 2\n[35] Hang Zhao, Jiyang Gao, Tian Lan, Chen Sun, Benjamin\nSapp, Balakrishnan Varadarajan, Yue Shen, Yi Shen, Yuning\nChai, Cordelia Schmid, et al. Tnt: Target-driven trajectory\nprediction. arXiv preprint arXiv:2008.08294, 2020. 1, 2, 4,\n5, 6\n11",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7976961135864258
    },
    {
      "name": "Multimodality",
      "score": 0.7591830492019653
    },
    {
      "name": "Transformer",
      "score": 0.6368733048439026
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5273780822753906
    },
    {
      "name": "Motion (physics)",
      "score": 0.5035714507102966
    },
    {
      "name": "Machine learning",
      "score": 0.4999561309814453
    },
    {
      "name": "Architecture",
      "score": 0.4957636296749115
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.47152480483055115
    },
    {
      "name": "Exploit",
      "score": 0.45649591088294983
    },
    {
      "name": "Engineering",
      "score": 0.1011858880519867
    },
    {
      "name": "Voltage",
      "score": 0.07834163308143616
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "World Wide Web",
      "score": 0.0
    }
  ],
  "topic": "Computer science"
}