{
  "title": "Evaluating Large Language Models in extracting cognitive exam dates and scores",
  "url": "https://openalex.org/W4405281112",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2095945326",
      "name": "Hao Zhang",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2995026561",
      "name": "Neil Jethani",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2102875672",
      "name": "Simon Jones",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2070565107",
      "name": "Nicholas Genes",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2096206670",
      "name": "Vincent J. Major",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2889047133",
      "name": "Ian S Jaffe",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A3015094690",
      "name": "Anthony B. Cardillo",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A3202113543",
      "name": "Noah Heilenbach",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A4283182090",
      "name": "Nadia Fazal Ali",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A5019737350",
      "name": "Luke J. Bonanni",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2988780533",
      "name": "Andrew J. Clayburn",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A4296685757",
      "name": "Zain Khera",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A4384039988",
      "name": "Erica C. Sadler",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2809483708",
      "name": "Jaideep Prasad",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A4220215879",
      "name": "Jamie Schlacter",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2100631594",
      "name": "Kevin Liu",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2125265398",
      "name": "Benjamin Silva",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A3092633817",
      "name": "Sophie Montgomery",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2115713152",
      "name": "Eric J. Kim",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2538475737",
      "name": "Jacob Lester",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2762440835",
      "name": "Theodore M Hill",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A3021378894",
      "name": "Alba Avoricani",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A3115361973",
      "name": "Ethan Chervonski",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A4384039994",
      "name": "James Davydov",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2025926024",
      "name": "William Small",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2791518936",
      "name": "Eesha Chakravartty",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2139396761",
      "name": "Himanshu Grover",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2182650040",
      "name": "John A. Dodson",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2183521394",
      "name": "Abraham A. Brody",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A1850176264",
      "name": "Yindalon Aphinyanaphongs",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A4221759198",
      "name": "Arjun Masurkar",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2078741054",
      "name": "Narges Razavian",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2095945326",
      "name": "Hao Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2995026561",
      "name": "Neil Jethani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102875672",
      "name": "Simon Jones",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2070565107",
      "name": "Nicholas Genes",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096206670",
      "name": "Vincent J. Major",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2889047133",
      "name": "Ian S Jaffe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3015094690",
      "name": "Anthony B. Cardillo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3202113543",
      "name": "Noah Heilenbach",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4283182090",
      "name": "Nadia Fazal Ali",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5019737350",
      "name": "Luke J. Bonanni",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2988780533",
      "name": "Andrew J. Clayburn",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4296685757",
      "name": "Zain Khera",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4384039988",
      "name": "Erica C. Sadler",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2809483708",
      "name": "Jaideep Prasad",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4220215879",
      "name": "Jamie Schlacter",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100631594",
      "name": "Kevin Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2125265398",
      "name": "Benjamin Silva",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3092633817",
      "name": "Sophie Montgomery",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115713152",
      "name": "Eric J. Kim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2538475737",
      "name": "Jacob Lester",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2762440835",
      "name": "Theodore M Hill",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3021378894",
      "name": "Alba Avoricani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3115361973",
      "name": "Ethan Chervonski",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4384039994",
      "name": "James Davydov",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2025926024",
      "name": "William Small",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2791518936",
      "name": "Eesha Chakravartty",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2139396761",
      "name": "Himanshu Grover",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2182650040",
      "name": "John A. Dodson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2183521394",
      "name": "Abraham A. Brody",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A1850176264",
      "name": "Yindalon Aphinyanaphongs",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221759198",
      "name": "Arjun Masurkar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2078741054",
      "name": "Narges Razavian",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2168845544",
    "https://openalex.org/W4361289889",
    "https://openalex.org/W3022003261",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4380786006",
    "https://openalex.org/W4319062614",
    "https://openalex.org/W4362521774",
    "https://openalex.org/W4372231834",
    "https://openalex.org/W4367310920",
    "https://openalex.org/W4323050332",
    "https://openalex.org/W4323347604",
    "https://openalex.org/W42510333",
    "https://openalex.org/W4318263917",
    "https://openalex.org/W6848849931",
    "https://openalex.org/W2122402213",
    "https://openalex.org/W2769851464",
    "https://openalex.org/W2778088240",
    "https://openalex.org/W2146089916",
    "https://openalex.org/W2139865360",
    "https://openalex.org/W2977613223",
    "https://openalex.org/W3089862415",
    "https://openalex.org/W2772121968",
    "https://openalex.org/W4379769651",
    "https://openalex.org/W3036909358",
    "https://openalex.org/W1779612606",
    "https://openalex.org/W2897815444",
    "https://openalex.org/W3021953657",
    "https://openalex.org/W4375949262",
    "https://openalex.org/W3034383590",
    "https://openalex.org/W3146388672",
    "https://openalex.org/W6787393329",
    "https://openalex.org/W2616393094",
    "https://openalex.org/W2972048113",
    "https://openalex.org/W3046275966",
    "https://openalex.org/W2912533302",
    "https://openalex.org/W3204089331",
    "https://openalex.org/W6675837603",
    "https://openalex.org/W1991952617",
    "https://openalex.org/W2019694480",
    "https://openalex.org/W2165758561",
    "https://openalex.org/W1975879668",
    "https://openalex.org/W2155243985",
    "https://openalex.org/W2167563976",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3171757072",
    "https://openalex.org/W4377009978",
    "https://openalex.org/W2949709152",
    "https://openalex.org/W4360891289",
    "https://openalex.org/W4367191144",
    "https://openalex.org/W3037343796",
    "https://openalex.org/W3101156210",
    "https://openalex.org/W4317390716",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4287813795",
    "https://openalex.org/W3089638105",
    "https://openalex.org/W4318035272",
    "https://openalex.org/W2963466845",
    "https://openalex.org/W2913961733"
  ],
  "abstract": "Ensuring reliability of Large Language Models (LLMs) in clinical tasks is crucial. Our study assesses two state-of-the-art LLMs (ChatGPT and LlaMA-2) for extracting clinical information, focusing on cognitive tests like MMSE and CDR. Our data consisted of 135,307 clinical notes (Jan 12th, 2010 to May 24th, 2023) mentioning MMSE, CDR, or MoCA. After applying inclusion criteria 34,465 notes remained, of which 765 underwent ChatGPT (GPT-4) and LlaMA-2, and 22 experts reviewed the responses. ChatGPT successfully extracted MMSE and CDR instances with dates from 742 notes. We used 20 notes for fine-tuning and training the reviewers. The remaining 722 were assigned to reviewers, with 309 each assigned to two reviewers simultaneously. Inter-rater-agreement (Fleiss’ Kappa), precision, recall, true/false negative rates, and accuracy were calculated. Our study follows TRIPOD reporting guidelines for model validation. For MMSE information extraction, ChatGPT (vs. LlaMA-2) achieved accuracy of 83% (vs. 66.4%), sensitivity of 89.7% (vs. 69.9%), true-negative rates of 96% (vs 60.0%), and precision of 82.7% (vs 62.2%). For CDR the results were lower overall, with accuracy of 87.1% (vs. 74.5%), sensitivity of 84.3% (vs. 39.7%), true-negative rates of 99.8% (98.4%), and precision of 48.3% (vs. 16.1%). We qualitatively evaluated the MMSE errors of ChatGPT and LlaMA-2 on double-reviewed notes. LlaMA-2 errors included 27 cases of total hallucination, 19 cases of reporting other scores instead of MMSE, 25 missed scores, and 23 cases of reporting only the wrong date. In comparison, ChatGPT’s errors included only 3 cases of total hallucination, 17 cases of wrong test reported instead of MMSE, and 19 cases of reporting a wrong date. In this diagnostic/prognostic study of ChatGPT and LlaMA-2 for extracting cognitive exam dates and scores from clinical notes, ChatGPT exhibited high accuracy, with better performance compared to LlaMA-2. The use of LLMs could benefit dementia research and clinical care, by identifying eligible patients for treatments initialization or clinical trial enrollments. Rigorous evaluation of LLMs is crucial to understanding their capabilities and limitations.",
  "full_text": "RESEA RCH ARTICL E\nEvaluating Large Language Models in\nextracting cognitive exam dates and scores\nHao Zhang\nID\n1\n, Neil Jethani\nID\n1\n, Simon Jones\n1\n, Nicholas Genes\nID\n1\n, Vincent J. Major\nID\n1\n,\nIan S. Jaffe\nID\n1\n, Anthony B. Cardillo\nID\n1\n, Noah Heilenbach\nID\n1\n, Nadia Fazal Ali\nID\n1\n,\nLuke J. Bonanni\nID\n1\n, Andrew J. Clayburn\nID\n1\n, Zain Khera\nID\n1\n, Erica C. Sadler\nID\n1\n,\nJaideep Prasad\nID\n1\n, Jamie Schlacter\nID\n1\n, Kevin Liu\nID\n1\n, Benjamin Silva\nID\n1\n,\nSophie Montgomery\nID\n1\n, Eric J. Kim\nID\n1\n, Jacob Lester\nID\n1\n, Theodore M. Hill\nID\n1\n,\nAlba Avoricani\nID\n1\n, Ethan Chervonski\nID\n1\n, James Davydov\n1\n, William Small\nID\n1\n,\nEesha Chakravartty\nID\n1\n, Himanshu Grover\n1\n, John A. Dodson\n1\n, Abraham A. Brody\nID\n1,2\n,\nYindalon Aphinyanaphon gs\nID\n1\n, Arjun Masurkar\nID\n1\n, Narges Razavian\nID\n1\n*\n1 NYU Grossman School of Medicine, New York, New York, United States of America, 2 Rory Meyers\nCollege of Nursing, New York Universit y, New York, New York, United States of America\n* narges.r azavian@ny ulangone.or g\nAbstract\nEnsuring reliability of Large Language Models (LLMs) in clinical tasks is crucial. Our study\nassesses two state-of-the-art LLMs (ChatGPT and LlaMA-2) for extracting clinical informa-\ntion, focusing on cognitive tests like MMSE and CDR. Our data consisted of 135,307 clinical\nnotes (Jan 12th, 2010 to May 24th, 2023) mentioning MMSE, CDR, or MoCA. After applying\ninclusion criteria 34,465 notes remained, of which 765 underwent ChatGPT (GPT-4) and\nLlaMA-2, and 22 experts reviewed the responses. ChatGPT successfully extracted MMSE\nand CDR instances with dates from 742 notes. We used 20 notes for fine-tuning and training\nthe reviewers. The remaining 722 were assigned to reviewers, with 309 each assigned to\ntwo reviewers simultaneously. Inter-rater-agreem ent (Fleiss’ Kappa), precision, recall, true/\nfalse negative rates, and accuracy were calculated. Our study follows TRIPOD reporting\nguidelines for model validation. For MMSE information extraction, ChatGPT (vs. LlaMA-2)\nachieved accuracy of 83% (vs. 66.4%), sensitivity of 89.7% (vs. 69.9%), true-negative rates\nof 96% (vs 60.0%), and precision of 82.7% (vs 62.2%). For CDR the results were lower over-\nall, with accuracy of 87.1% (vs. 74.5%), sensitivity of 84.3% (vs. 39.7%), true-negative rates\nof 99.8% (98.4%), and precision of 48.3% (vs. 16.1%). We qualitatively evaluated the\nMMSE errors of ChatGPT and LlaMA-2 on double-revie wed notes. LlaMA-2 errors included\n27 cases of total hallucination, 19 cases of reporting other scores instead of MMSE, 25\nmissed scores, and 23 cases of reporting only the wrong date. In comparison, ChatGPT’s\nerrors included only 3 cases of total hallucination, 17 cases of wrong test reported instead of\nMMSE, and 19 cases of reporting a wrong date. In this diagnostic/prognost ic study of\nChatGPT and LlaMA-2 for extracting cognitive exam dates and scores from clinical notes,\nChatGPT exhibited high accuracy, with better performanc e compared to LlaMA-2. The use\nof LLMs could benefit dementia research and clinical care, by identifying eligible patients for\ntreatments initialization or clinical trial enrollments. Rigorous evaluation of LLMs is crucial to\nunderstanding their capabilities and limitations.\nPLOS DIGI TAL HEALT H\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000068 5 Decemb er 11, 2024 1 / 16\na1111111111\na1111111111\na1111111111\na1111111111\na1111111111\nOPEN ACCESS\nCitation: Zhang H, Jethani N, Jones S, Genes N,\nMajor VJ, Jaffe IS, et al. (2024) Evaluating Large\nLanguage Models in extracting cognitive exam\ndates and scores. PLOS Digit Health 3(12):\ne0000685. https://d oi.org/10.1371/j ournal.\npdig.00006 85\nEditor: Imon Banerjee, Mayo Clinic, Arizona,\nUNITED STATES OF AMERICA\nReceived: March 1, 2024\nAccepted: October 28, 2024\nPublished: December 11, 2024\nCopyright: © 2024 Zhang et al. This is an open\naccess article distributed under the terms of the\nCreative Commons Attribution License, which\npermits unrestricte d use, distribu tion, and\nreproduction in any medium, provided the original\nauthor and source are credited.\nData Availabilit y Statement: The clinical notes\nused for this study were collected from the NYU\nLangone Health System EHR maintained by the\nNYULH Datacore team. These clinical notes contain\npotentially identifying or sensitive patient\ninformation, and according to the Institutional\nReview Board and Data Sharing Committee at NYU\nLangone, cannot be made publicly available.\nResearch ers interested in the data used in this\nstudy should submit a reasonable request to the\ndata sharing committee datasharing@ nyulangone.\norg, and the request will undergo institutional\nAuthor summary\nLarge-scale language models (LLMs) have emerged as powerful tools in natural language\nprocessing (NLP), capable of performing diverse tasks when prompted. Since reliable per-\nformance of LLMs in clinical tasks is essential, our study evaluates two advanced LLMs—\nChatGPT and LlaMA-2—for their ability to extract clinical information from health rec-\nords, particularly cognitive test results: MMSE (Mini-Mental State Examination) and\nCDR (Clinical Dementia Rating). We analyzed 765 clinical notes from over a decade,\nfocusing on how well these models could identify and date specific test mentions.\nChatGPT accurately extracted MMSE and CDR details from most notes, demonstrating\ngreater accuracy, sensitivity. Our findings also reveal common errors, with ChatGPT gen-\nerally producing fewer inaccuracies. This research emphasizes the potential for LLMs to\nenhance dementia research and patient care by improving the identification of eligible\npatients for treatment or clinical trials. However, a thorough understanding of these mod-\nels’ strengths and weaknesses is essential for their effective application in real-world clini-\ncal settings.\nIntroduction\nLarge-scale language models (LLMs) [1–4] have emerged as powerful tools in natural language\nprocessing (NLP), capable of performing diverse tasks when prompted [5,6]. These models\nhave demonstrated impressive clinical reasoning abilities [7], successfully passing medical\nlicensing exams [8–10] and generating medical advice on distinct subjects, including cardio-\nvascular disease [11], breast cancer [12], colonoscopy [13], and general health inquiries [6,14–\n16]. These models can produce clinical notes [16] and assist in writing research articles [16].\nMedical journals have begun developing policies around use of LLMs in writing [17–22] and\nreviewing. Examples of such LLMs include ChatGPT [1,2], Med-PALM-2 [3], LlaMA-2 [4],\nand open-source models actively produced by the community [23].\nIn this study, we focus on evaluating information extraction abilities of Large Language\nModels from clinical notes, specifically focusing on proprietary ChatGPT (powered by GPT-4\n[2]), and open source LlaMA-2 [4] LLMs. Information extraction involves the retrieval of spe-\ncific bits of information from unstructured clinical notes, a task historically handled by rule-\nbased systems [24–30] or language models explicitly trained on datasets annotated by human\nexperts [31–36]. Rule-based systems lack a contextual understanding and struggle with com-\nplex sentence structures, ambiguous language, and long-distance dependencies, often leading\nto high false positive rates and low sensitivities [37–40]. Additionally, training a new model for\nthis task can be computationally demanding and require substantial human effort. In contrast,\nLLMs, such as ChatGPT or LlaMA-2, operate at “zero-shot” capacity [41–43], i.e., only requir-\ning a prompt describing the desired information to be extracted.\nDespite their promise, LLMs also have a potential limitation—the generation of factually\nincorrect yet highly convincing outputs, commonly known as “hallucination.” The massive\narchitectures and complex training schemes of LLMs hamper “model explanation” and the\nability to intrinsically guarantee behavior. This issue has been extensively discussed in the liter-\nature, emphasizing the need for cautious interpretation and validation of information gener-\nated by LLMs [2,44,45].\nOne area where LLMs may greatly benefit healthcare is in the identification of memory\nproblems and other symptoms indicative of Alzheimer’s Disease and Alzheimer’s Disease\nPLOS DIGI TAL HEALT H\nEvaluating Large Language Models in extracting cognitive exam dates and scores\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000068 5 Decemb er 11, 2024 2 / 16\nreview and will be subject to local and national\nethical approvals.\nFunding: This study was supported by NYU\nLangone Medical Center Information Technology\n(MCIT) center, National Institute On Aging, of the\nNational Institutes of Health (R01AG0856 17 to NR\nand AM, P30AG0665 12 to NR and AM,\nP30AG06 6512 to HZ, SJ, VJM, JAD, AAB, YA, AM,\nand NR). The funders had no role in study design,\ndata collection and analysis , decision to publish, or\npreparation of the manuscript.\nCompeting interests : The authors have declared\nthat no competing interests exist.\nRelated Dementias (AD/ADRD) within clinical notes. AD/ADRD is commonly underdiag-\nnosed or diagnosed later in the disease trajectory, particularly in racial and ethnic minoritized\ngroups [46–51]. The precise extraction of cognitive test scores holds significant importance in\nthe development and clinical validation of tools that can facilitate early detection [52] of AD/\nADRD in the clinic. Earlier identification can lead to a host of benefits, including assisting\nwith advanced care planning, performing secondary cardiovascular disease prevention, which\nmay reduce worsening of cognitive impairment [53,54], identification for serving in research\ntrials [55–57], and with the rapid advancement in biologic therapeutics, the opportunity to\nreceive potentially disease modifying drugs [57,58]. Accurately extracting cognitive exam\nscores (often buried in clinical notes and not documented in any structured field), enables vali-\ndation, training and fine-tuning of models at a much larger scale in a clinical setting for a\nmuch more racial/ethnically diverse patient population set compared to current research\ncohorts.\nThe primary focus of this paper is therefore on the validation of two state-of-the-art LLMs\n(ChatGPT powered by GPT-4, and LlaMA-2), for information extraction related to cognitive\ntests, specifically the Mini-Mental State Examination (MMSE) [59] and Clinical Dementia Rat-\ning (CDR) [60], from clinical notes of a racially and ethnically diverse patient population. Our\nobjective is to accurately extract all instances of (the exam score, and the date when the exam\nwas administered) using these LLMs.\nThis study represents a large-scale formal evaluation of two state of the art LLMs (ChatGPT,\nand LlaMA-2) performance in information extraction from clinical notes. Going forward, we\nintend to employ this benchmark dataset to validate other (open or closed-source) LLMs. Fur-\nthermore, we plan to adopt a similar approach to validate LLMs for information extraction\nacross various clinical use cases. By prioritizing prompt engineering with ChatGPT and\nLlaMA-2 for extracting clinical information, this research aims to enhance our understanding\nof the potential of LLMs in healthcare and facilitate the development of reliable and robust\nclinical information extraction tools.\nMethods\nThis study is approved under IRB i20-01095, “Understanding and predicting Alzheimer’s Dis-\nease.” NYU DataCore services were utilized to prepare the data as described below. A HIPAA-\ncompliant private instance of ChatGPT (Microsoft Azure OpenAI Service) was utilized for this\nstudy. LlaMA-2 (“Llama-2-70b-chat” version) was evaluated on two A100 Nvidia GPUs on\nour local high performance computing servers. This Diagnostic/Prognostic study designed to\nvalidate the diagnostic accuracy of two LLMs (ChatGPT and LlaMA-2) in extracting cognitive\nexam dates and scores, follows the TRIPOD Prediction Model Validation reporting guidelines\n(S1 Checklist) [61].\nDataset\nAn original cohort of 135,307 clinical notes corresponding to inpatient, outpatient, and emer-\ngency department visits between January 12th 2010 and May 24th 2023, which included any of\nthe following keywords (‘MMSE’, ‘CDR,’ or ‘MoCA’ case-insensitive) were identified (see\nFig 1). MMSE stands for Mini Mental State Exam, CDR stands for Cognitive Dementia Rating,\nand MoCA stands for Montreal Cognitive Assessment [62]. These notes belonged to 52,948\npatients. From among these patients, 26,355 had a non-contrast brain Magnetic Resonance\nImaging (MRI) in the system. Limiting the clinical notes to those who had an MRI in the sys-\ntem resulted in 77,547 notes. After extracting the notes, we further excluded 43,082 notes that\nonly mentioned MoCA, yielding 34,465 clinical notes for analysis.\nPLOS DIGI TAL HEALT H\nEvaluating Large Language Models in extracting cognitive exam dates and scores\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000068 5 Decemb er 11, 2024 3 / 16\nThe choice for requiring patients to have a brain MRI as well as MMSE and/or CDR enables\nus to have a similar level of granularity as the Alzheimer’s Disease Neuro-Imaging Initiative\n(ADNI) [63], which also uses MMSE and CDR for definition of mild cognitive impairment\nand dementia stages. This further enables us to harmonize our clinical dataset with these large\nFig 1. Flowchar t of clinical notes evaluated for inclusion in the final sample of GPT-analyz ed notes.\nhttps:// doi.org/10.1371 /journal.pdig .0000685.g001\nPLOS DIGI TAL HEALT H\nEvaluating Large Language Models in extracting cognitive exam dates and scores\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000068 5 Decemb er 11, 2024 4 / 16\nresearch cohorts. To elucidate the impact of this choice (restriction of cohort to those with\nMRI) on the racial breakdown of our study, we include a demographics comparison between\nthe two sets (original 52,948 patients, and the 26,355 with an MRI) in S1 Section. Similarly, the\nchoice to ignore MoCA was due to the lack of inclusion of MoCA in standard definition for\nstages of cognitive impairment in ADNI. The mild cognitive impairment and (mild, moderate\nor severe) dementia definition criteria utilized in ADNI are included in S1 Table. Data harmo-\nnization is beyond the scope of this paper, although information extraction plays a substantial\nrole in enabling it.\nFrom among 34,465 notes that fit the inclusion criteria, a random selection of 765 notes\nwas identified to undergo information extraction via ChatGPT and manual evaluation. 765\nwas the total number of the notes needed to satisfy two conditions: 1) Each reviewer not being\nassigned more than 50 notes to review, and 2) at least around 15 notes per reviewer being dou-\nble-reviewed by another random reviewer. From among these 765 notes, ChatGPT encoun-\ntered application programming interface (API) errors in 23 cases (3%). These errors arose\nfrom “Azure content management violations” [64] (17 cases), API timeouts (5 cases), and\nmaximum length limit errors (1 case). S2 Table includes a more detailed description of these\nerrors. The remaining 742 were considered for assignment to domain expert reviewers, and\nunderwent analysis by LlaMA-2.\nGenerative AI, ChatGPT\nA private, HIPAA-compliant instance of ChatGPT (GPT-4, API version “2023-03-15-pre-\nview”) was used on these 765 notes to extract all instances of the cognitive tests—MMSE and\nCDR—along with the dates at which the tests were mentioned to have been administered.\nExamples of our task are provided in the S2 Section. Inference was successful for 742 notes.\nThe complete API call, along with the exact prompt, the temperature, and other hyper-param-\neters are included in S3 Table. The prompt included a request to return these results in a JSON\nformat. ChatGPT’s response (full), as well as the JSON formatted dialogue response were\nrecorded in one session on June 9th 2023. The notes sent to ChatGPT were text-only, stripped\nof the rich-text formatting (RTF) native to our EHR system (Epic Systems, Verona, WI). This\nreduced token count by approximately ten-fold, enabling notes to fit into the GPT4-8K input\nwindow and removing a substantial source of confusion for the LLM in prompt tuning. The\ndate that the encounter was recorded in Epic was appended at the beginning of the note, pro-\nceeding with a column (“:”) then the note text. See S3 Table for the API request, including the\nprompt.\nGenerative AI, LlaMA-2\nWe used LlaMA-2 (version “Llama-2-70b-chat\") on all the notes which ChatGPT produced\nvalid answer. All pre-processing steps on the notes were similar to that of ChatGPT. The con-\ntext window was limited to the first 3696 tokens. The complete API call, along with the exact\nprompt, the temperature, and other hyper-parameters are included in S4 Table.\nHyper-parameter and prompt tuning\nFor both ChatGPT and LlaMA-2, we assigned 20 notes out of the 742 as our hyper-parameter\nand prompt tuning set. For ChatGPT, an interactive cloud-based environment (i.e play-\nground) was utilized initially to fine-tune the prompt. After initial exploratory analysis using\nthese 20 notes, they were scored via the API using the best prompt and hyper-parameter found\nin the interactive mode. For LlaMA-2, the exploration was performed locally, on the same 20\nnotes. For both models, we explored the following model parameters: max_token_length,\nPLOS DIGI TAL HEALT H\nEvaluating Large Language Models in extracting cognitive exam dates and scores\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000068 5 Decemb er 11, 2024 5 / 16\ntemperature. All human expert reviewers (detailed below) were instructed to first review the\nChatGPT results of the 20 cases in a RedCap survey. The goal of this step was to train the\nreviewers, refine the information presented in RedCap, improve clarification of the questions,\nand potentially refine the prompt. These 20 notes were then excluded from any additional\nanalysis.\nHuman expert reviewers\nOur team included 22 medically trained expert reviewers who volunteered and were trained to\nreview an (HTML formatted) note, provide ground truth, and judge the correctness and com-\npleteness of ChatGPT answers for each cognitive test. Fully (HTML) formatted notes were\npulled using an Epic web service, and were fed into the RedCap survey. Redcap survey ren-\ndered the note’s HTML formatting, to ensure notes could be displayed to users in the same for-\nmat as the readers are accustomed to seeing them clinically, rather than the text-only,\ncomputer-friendly format provided to GPT. To generate ground-truth, the reviewers used\nChatGPT responses as the basis, and corrected any errors ChatGPT made.\nFor 21 of these reviewers, each reviewer was assigned approximately 50 clinical notes to\nevaluate. From among each reviewer’s 50 assigned notes, about 15 notes were assigned to\nanother random reviewer. The assignment algorithm randomly selected a pair of reviewers for\neach of our 309 double-reviewed notes and assigned the remaining notes to a randomly\nselected reviewer until each reviewer reached 50 notes or we fully assigned all notes. This ran-\ndom assignment was a necessary step for ensuring correctness of Fleiss’ Kappa [65] metric for\ninter-rater-agreement. As a result, there was a slight variation in the total number of assigned\nnotes for each reviewer.\nOverall, 722 notes were assigned to these 21 reviewers, of which 309 were double-reviewed\nand 413 were solo-reviewed. The double-reviewed 309 notes were utilized in reporting inter-\nrater-agreement metrics. After the review, 69 out of 309 notes had at least one disagreement\nbetween the two reviewers based on one of the four questions: Whether ChatGPT’s response on\nMMSE was correct; whether ChatGPT’s response on MMSE included all instances of MMSE\nfound in the clinical note; whether ChatGPT’s response on CDR was correct; and whether\nChatGPT’s response on CDR included all instances of CDR found in the clinical note. A 22nd\nreviewer was then tasked to review these 69 notes again to provide a third review. Majority\nvote was then employed to identify the final answer and the ground truth provided by the\nreviewer whose answer was in the majority vote was used to calculate detailed precision/recall\nmetrics. When both reviewers fully agreed and their JSON results were both valid for analysis,\nwe randomly selected one to compute the precision and recall. Details of the parsing of the\nJSON result are included in the S3 Section. These expert-provided ground truth results were\nthe basis for evaluating LlaMA-2.\nStatistical approach\nWe reported Fleiss’ Kappa [65] as a measure of inter-rater-agreement for double-reviewed\nnotes. We reported this metric for the four questions on ChatGPT-generated responses (Is\nMMSE complete/correct, and is CDR complete/correct). Additionally, for double-reviewed\nnotes, we derived inter-rather-agreement by computing 2-way Fleiss’ Kappa for MMSE and\nCDR lists of (outcome and date) tuples extracted from the JSON responses by expert reviewers.\nFleiss’ Kappa is useful when the assignment of a note to reviewer pairs has been random (uni-\nform), and each note has been reviewed by a subset of reviewers [66,67]. Only exact outcome\nand date tuple matches were considered to be in agreement between raters (i.e [MMSE-27/30,\ndate “10-10-2010”], with [MMSE-26/30, date “10-10-2010”] is just as bad as [MMSE-5/30, date\nPLOS DIGI TAL HEALT H\nEvaluating Large Language Models in extracting cognitive exam dates and scores\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000068 5 Decemb er 11, 2024 6 / 16\n“10-10-2012”]). We also report a 3-way Fleiss’ Kappa on the entries of MMSE and CDR results\nextracted from the JSON results, computing the joint agreement between the results of\nChatGPT and the results provided by two human reviewers.\nWe also report per test type (MMSE and CDR), Accuracy, True and False Negative Rates,\nMicro- and Macro-Precision and Micro- and Macro-Recall for both ChatGPT and LlaMA-2.\nAccuracy is defined as the percentage of correct results (at clinical note level), correct being\ndefined as the list of (Value/Date) tuples in the JSON entries for the LLM and Ground Truth\nbeing fully identical. Macro-Precision for MMSE (or CDR) is the average (at the note level) of\npercentage of correct MMSE (or CDR) tuples extracted (correct both in date and score values\ncompared to an entry mentioned in the ground truth for MMSE (or CDR)). Macro-Recall for\nMMSE (or CDR) is the average (at the note level) of the percentage of the MMSE items in the\nground truth that are extracted by the LLM. Micro-precision is calculated as percentage of cor-\nrect MMSE (or CDR) items extracted by the LLM, from among all extracted MMSE (or CDR)\nitems by that LLM, and is calculated as one number across all notes combining all notes’\nentries. Micro-recall is similarly calculated as the percentage of all MMSE (or CDR) items\nmentioned in the ground truth that were extracted by the LLM.\nResults\nChatGPT analyzed 765 notes for extraction of Mini Mental Status Exam (MMSE) and Cogni-\ntive Dementia Rating (CDR) scores and exam dates. Of these, 23 encountered API error (3%),\nand 20 were used to fine-tune prompt and hyper-parameters. The remaining 722 notes were\nassigned to human expert reviewers who manually reviewed (and provided ground truth for)\nthese notes. LlaMA-2 analyzed these 722 notes as well. Characteristics of these 722 notes and\nassociated patients are included in Table 1.\nOf the double-reviewed 309 notes, 69 had at least one disagreement between the responses\nto the four questions (if ChatGPT’s response for MMSE/CDR is correct/complete) and were\nassigned to a new reviewer for a third opinion. Among the responses with disagreement, 9 dis-\nagreed about correctness of MMSE answers, 40 disagreed about completeness of MMSE\nanswers, 17 disagreed about correctness of CDR answers, and 22 disagreed about completeness\nof CDR answers. The average response (at the note level) by the included reviews for the four\nyes/no questions are included in Table 2. Overall reviewers considered ChatGPT’s response to\nbe 96.5% and 98% correct for MMSE and CDR respectively. The assessment for whether\nChatGPT’s answers are also complete (i.e. they do not miss anything) was slightly lower aver-\naging about 84% and 83% for MMSE and CDR respectively.\nThe inter-rater-agreements between reviewers were calculated based on Fleiss’ Kappa and\nare summarized in Table 3. In addition to measuring Fleiss’ Kappa between reviewers based\non double-reviewed notes (reported as 2-way Fleiss’ Kappa in Table 3), we also report agree-\nment between ChatGPT, and the two human reviewers (reported as 3-way Fleiss’ Kappa in\nTable 3). The 2-way agreement on the yes/no questions was high (94% agreement between\nreviewers for MMSE and 89% agreement for CDR). There was some disagreement in judging\nthe completeness of the answer, leading to a Kappa value of 75% for MMSE (and 85% for\nCDR). More notably, when analyzing the elements of the ground truth JSON, the 2-way agree-\nment was excellent both for scores (83% for MMSE and 80% for CDR) and for dates (93% for\nMMSE and 79% for CDR). When measuring the 3-way agreement, there was an increase in all\nthe metrics except MMSE dates. The accuracy and results of JSON formatting of the responses\nare included in S4 Section.\nChatGPT had an excellent True Negative Rate—over 96% for MMSE and 100% for CDR in\ndouble-reviewed notes (Table 4). Both results had high recall (sensitivity), reaching 89.7% for\nPLOS DIGI TAL HEALT H\nEvaluating Large Language Models in extracting cognitive exam dates and scores\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000068 5 Decemb er 11, 2024 7 / 16\nMMSE (macro-recall) and 91.3% for CDR (macro-recall). MMSE was more frequently men-\ntioned in the notes and ChatGPT’s macro precision (PPV) was 82.7%. CDR, on the other\nhand, was less frequent, and we observed that ChatGPT hallucinates (factitiously generates)\nresults occasionally leading to a macro precision of only 57.5%. LlaMA-2 results were signifi-\ncantly lower than that of ChatGPT across all metrics. A detailed qualitative analysis of the\nChatGPT errors for both CDR and MMSE, and LlaMA-2 results for MMSE are included in\nS5 Section. The majority of the errors corresponded to ChatGPT presenting results of another\ntest instead of the one indicated as the answer. LlaMA-2 had higher rate of unexplained hallu-\ncinations. Taking positive and negative results into account, overall, ChatGPT had the highest\nperformance with MMSE and CDR results being 83% and 89% accurate according to the dou-\nble-reviewed notes.\nTable 2. Average response (at the note level) of the responses of reviewers in judging if ChatGPT’s answers for\nMMSE and CDR are correct and/or complete.\nAll notes (N = 722) Double reviewed notes (N = 309)\nIs ChatGPT’s answer for MMSE correct? (%) 96.5 (sd 18.2) 96.4 (sd 18.5)\nIs ChatGPT’s answer for MMSE complet e? (%) 85.0 (sd 35.7) 84.7 (sd 36.0)\nIs ChatGPT’s answer for CDR correct? (%) 98.0 (sd 13.7) 99.6 (sd 5.6)\nIs ChatGPT’s answer for CDR complete? (%) 80.4 (sd 39.6) 83.4 (sd 37.1)\nhttps://d oi.org/10.1371/j ournal.pdig. 0000685.t0 02\nTable 1. Characteri stics of 722 notes which are manually evaluated, and their correspond ing patients.\nFeature All notes (N = 722 notes from 458\npatients)\nDouble reviewed notes (N = 309 notes from\n236 patients)\nPatient demograp hics\nAge at time of note (mean\n(sd))\n72.64 (14.01) 73.68 (14.01)\nGender\nFemale (%) 242 (52.84%) 124 (52.54%)\nMale(%) 216 (47.16%) 112 (47.46%)\nRace\nAsian 27 (5.90%) 10 (4.24%)\nBlack 39 (8.52%) 17 (7.20%)\nWhite 334 (72.93%) 178 (75.42%)\nAmerican Indian 1 (0.22%) 0 (0.00%)\nUnknown 57 (12.45%) 31 (13.14%)\nNote characteristic s\nDate ranges (min to max) 2011/11/2 1 to 2023/05/10 2011/11/21 to 2023/05/10\nLength (in words) (mean\n(SD))\n8428.2 (3822.3) 8306.2 (3851.1)\nChatGPT (Prompt Tokens) 2212.93 (1002.9) 2174.9 (992.3)\nChatGPT (Completion\nTokens)\n64.3 (49.6) 64.2 (46.5)\nChatGPT (Total Tokens) 2277.3 (1017.9) 2239.1 (1005.0)\nLlama2 (Prompt Tokens) 2860.8 (1224.2) 2810.4 (1208.4)\nLlama2 (Completio n\nTokens)\n140.2 (112.8) 146.9 (125.3)\nLlama2 (Total Tokens) 3000.9 (1276.7) 2957.4 (1270.8)\nhttps://d oi.org/10.1371/j ournal.pdig. 0000685.t0 01\nPLOS DIGI TAL HEALT H\nEvaluating Large Language Models in extracting cognitive exam dates and scores\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000068 5 Decemb er 11, 2024 8 / 16\nDiscussion\nIn this study, our primary objective was to evaluate the performance of two state of the art\nLLMs (ChatGPT and LlaMA-2), in extracting information from clinical notes, specifically\nfocusing on cognitive tests such as the Mini-Mental State Examination (MMSE) and Clinical\nDementia Rating (CDR). Our results revealed that ChatGPT achieves high accuracy in extract-\ning relevant information for MMSE and CDR scores, as well as their associated dates, with\nhigh recall, capturing nearly all of the pertinent details present in the clinical notes. The overall\naccuracy of ChatGPT in information extraction for MMSE and CDR were 83% and 89%\nrespectively. The extraction was highly and had outstanding true-negative-rates. The precision\nof the extracted information was also high for MMSE although in the case of CDR, we\nobserved that ChatGPT occasionally mistook other tests for CDR. Based on the ground-truth\nprovided by our reviewers, 89.1% of the notes included an MMSE documentation instance,\nwhereas only 14.3% of the notes included a CDR documentation instance. This, combined\nwith our analysis of the errors, explain lower precision in the CDR case, and suggest combin-\ning ChatGPT with basic NLP preprocessing may improve the LLM performance further. Com-\npared to ChatGPT, the open-source state of the art LLM (LlaMA-2) achieved lower\nperformance across all metrics. The substantial inter-rater-agreement among our expert\nreviewers further supported the robustness and validity of our findings, and the reviewers con-\nsidered ChatGPT’s responses correct and complete.\nThe findings of our study demonstrate that ChatGPT (powered by GPT-4), offer a prom-\nising solution for extracting valuable clinical information from unstructured notes. This\napproach provides a more efficient and scalable approach compared to previous methods\nthat either rely on rigid rule-based systems or involve training resource intensive task spe-\ncific models. Validated and accurate LLMs such as ChatGPT can be effortlessly applied to\nenhance the value of clinical data for research, enable harmonization with disease registries\nTable 3. Fleiss’ kappa inter-rat er-agreeme nt metric between reviewers (2-way) and reviewers and ChatGPT\n(3-way) over the double-review ed notes.\n2-way Fleiss’ kappa (Among human\nreviewers)\nOn N = 309 double-rev iewed notes,\nn = 21 reviewer s (%)\n3-way Fleiss’ Kappa (between ChatGP T\nand two human reviewers)\nOn N = 309 double-re viewed notes, n = 21\nreviewer s (%)\nBinary Questions\nIs MMSE list generate d by\nChatGPT correct?\n94.2 NA\nIs MMSE list generate d by\nChatGPT complete?\n75.2 NA\nIs CDR list generate d by\nChatGPT correct?\n89.0 NA\nIs CDR list generate d by\nChatGPT complete?\n85.8 NA\nIndividual (value/ date) tuples from ChatGPT and Ground-Trut h JSON results.\nMMSE values (of the scores in\nthe note)\n83.6 93.7\nMMSE dates (of the scores in\nthe note)\n93.3 87.2\nCDR values (of the scores in\nthe note)\n80.5 87.0\nCDR dates (of the scores in the\nnote)\n79.0 82.5\nhttps://d oi.org/10.1371/j ournal.pdig. 0000685.t0 03\nPLOS DIGI TAL HEALT H\nEvaluating Large Language Models in extracting cognitive exam dates and scores\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000068 5 Decemb er 11, 2024 9 / 16\nand biobanks, improve outreach programs within health centers, and contribute to the\nadvancement of precision medicine. Additionally, the availability of large labeled datasets\nresulting from this information extraction process can also enable AI models to be trained\nfor a wide variety of tasks.\nFurthermore, our findings have implications for future AD/ADRD research. Currently, the\nmajority of research in scalable development and validation of AI tools for early AD/ADRD\ndetection rely on research cohorts. These cohorts are overwhelmingly white (NACC cohort is\n83% white [68] ADNI cohort is 92% white [63], and do not represent true at-risk populations\nwho tend to have higher comorbid disease burden [50]. Due to late detection and diagnosis of\nAD/ADRD [46–49], clinical data often lacked the details necessary for accurate case identifica-\ntion (i.e. structured data such as ICD codes would yield low sensitivities). Using LLMs to\nextract data from clinical notes has the potential to improve the quality of clinical data, paving\nthe way for clinical validation and development of clinically applicable novel AI tools and per-\nforming cognitive-health precision medicine at scale.\nTable 4. Aggregate Accurac y, True Negativ e Rate, (Micro- and Macro-) Precision and Recall for MMSE and CDR scores extracted by ChatGPT and LlaMA-2.\nAll notes with parsed JSON\n(N = 710)\nDouble-revie wed notes with\nparsed JSON\n(N = 306)\nChatGPT LlaMA- 2 ChatGPT LlaMA- 2\nMMSE\nTotal notes without any MMSE (in ground truth) 115 48\nTotal notes without any MMSE (in GPT results) 77 110 25 46\nTotal correctly predicted empty MMSEs 76 66 24 23\nMMSE True Negative Rate (%) 98.7 60.0 96 50.0\nMMSE False Negativ e Rate(%) 1.2 40.0 4 50.0\nRemaining notes with un-empty GPT response undergon e Precision/Re call calculation for MMSE 633 600 281 260\nTotal MMSE instances predicted 831 957 366 410\nMMSE Macro Precision (mean % (sd %)) 82.9 (sd 36.2) 62.2(sd 45.5) 82.7 (sd 36.8) 63.4 (sd 44.9)\nMMSE Macro Recall (mean % (sd %)) 87.8 (sd 30.4) 69.9 (sd 43.5) 89.7 (sd 28.3) 71.8 (sd 42.1)\nMMSE Micro Precision (%) 83.8 57.7 84.1 59.3\nMMSE Micro Recall (%) 83.7 68.1 87.5 69.0\nTotal notes with any error MMSE result 121 238 52 98\nOverall accuracy of MMSE (%) 82.9 66.4 83.0 68.0\nCDR\nTotal notes without CDR (in ground truth) 608 260\nTotal notes without CDR (in GPT results) 533 497 233 215\nTotal correctly predicted empty CDR 532 489 233 212\nCDR True Negativ e Rate (%) 99.8 98.4 100 98.6\nCDR False Negative Rate (%) 0.2 1.6 0 1.4\nRemaining notes with un-empty GPT response undergon e Precision/Re call calculation for CDR 177 213 73 153\nTotal CDR instances predicte d 256 344 92 153\nCDR Macro Precision (mean % sd %) 48.3 (sd 49.9) 16.1 (sd 35.5) 57.5 (sd 49.4) 18.1 (sd 36.9)\nCDR Macro Recall (mean % sd %) 84.3 (sd 36.3) 39.7 (sd 48.7) 91.3 (sd 28.1) 43.5 (sd 49.6)\nCDR Micro Precision (%) 36.3 12.0 51.0 13.2\nCDR Micro Recall (%) 85.3 37.6 92.1 39.2\nTotal notes with any error CDR result 91 181 31 76\nOverall accuracy of CDR (%) 87.1 74.5 89.8 75.4\nhttps://do i.org/10.1371/j ournal.pdig. 0000685.t00 4\nPLOS DIGI TAL HEALT H\nEvaluating Large Language Models in extracting cognitive exam dates and scores\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000068 5 Decemb er 11, 2024 10 / 16\nLimitations\nOur focus was on evaluating the information extraction capabilities of two current state of the\nart LLMs, specifically ChatGPT powered by GPT-4, and LlaMA-2, rather than comparing it to\nall other LLMs or NLP methods. We believe that our results may be enhanced with better\nprompt engineering and combining LLMs with standard NLP. In the future, we hope to\ninclude other LLMs to evaluate this task. One limitation during the labeling stage is that we\ngenerated ChatGPT responses first before the experts review and correct to create the ground\ntruth. While this could introduce potential bias towards ChatGPT, we believed that ground\ntruth is still valid to evaluate other models. ChatGPT might also not be reliable 100% of the\ntime, as we have seen that it failed to generate responses for a small fraction of the notes. In\nproduction, it is critical to have back-up plans in place such as alternative LLMs to ensure the\nsystem can reliably extract scores for all notes. Additionally, we conducted a large-scale human\nevaluation for a single dementia use case, prioritizing result reliability over assessing various\nclinical scenarios. It is also important to note that our findings pertain specifically to informa-\ntion retrieval from clinical notes and do not predict how LLMs will perform on medical tasks\nrequiring diagnosis, treatment recommendation, or summarization. For the scope of the\nstudy, we focused on patients with an MRI exam, and we have seen that there is a distribution\ndifference in patients with an MRI than those tho do not (S1 Table). There might be a potential\ndifference on how clinicians document cognitive scores in the two populations. In future stud-\nies, we would like to explore how this difference could affect model performance. Finally, these\nlarge language model requires extensive hardware resources, meaning carbon footprint is\nlarger compared to traditional NLP methods. However, as the scores were often discussed in\nnatural language, where the information (type of test, date of test, and test scores) can be far\napart from each other, traditional NLP methods are not viable for this particular task without\nextensive efforts and large amount of training samples. We have included a few examples of\nclinical texts in S2 Table to demonstrate the heterogeneity of the texts.\nConclusions\nIn this diagnostic/prognostic study of ChatGPT and LlaMA-2 for extracting cognitive exam\ndates and scores from clinical notes, ChatGPT exhibited high accuracy in extracting MMSE\nscores and dates, with better performance compared to LlaMA-2. The use of LLMs could bene-\nfit dementia research and clinical care, by identifying eligible patients for treatments initializa-\ntion or clinical trial enrollments. Rigorous evaluation of LLMs is crucial to understanding\ntheir capabilities and limitations.\nSupporting information\nS1 Checklist. TRIPOD checklist: Prediction model development.\n(PDF)\nS1 Section. Characteristics of patients with cognitive tests with and without MRI in the sys-\ntem, vs the subset of those with an MRI in the system.\n(DOCX)\nS2 Section. Examples of the notes and corresponding output that is produced by\nChatGPT.(Dates for each patient note and ChatGPT responses are shifted by a random\nyear and month, to preserve anonymous nature of notes. All note shifts for one patients are\nconsistent).\n(DOCX)\nPLOS DIGI TAL HEALT H\nEvaluating Large Language Models in extracting cognitive exam dates and scores\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000068 5 Decemb er 11, 2024 11 / 16\nS3 Section. Parsing the JSON results.\n(DOCX)\nS4 Section. JSON responses.\n(DOCX)\nS5 Section. Qualitative analysis of error instances of ChatGPT.\n(DOCX)\nS1 Table. Diagnosis criteria for cognitively normal, mild cognitive impairment and mild\nAlzheimer’s disease dementia in ADNI cohorts.\n(DOCX)\nS2 Table. Description of the errors encountered by ChatGPT API.\n(DOCX)\nS3 Table. The prompt (and the full request JSON for the task) for ChatGPT. CLINICAL_-\nNOTE would include the date of the note (from EPIC) + “:” + the text-only content of the\nnotes.\n(DOCX)\nS4 Table. The prompt (and the full request JSON for the task) for LlaMA-2. CLINICAL_-\nNOTE would include the date of the note (from EPIC) + “:” + the text-only content of the\nnotes.\n(DOCX)\nAcknowledgmen ts\nThe following 22 authors are our clinical reviewers who also contributed to reviewing and\nauthorship of the manuscript: N.G, I.S.J, A.B.C, N.H., N.F.A, L.J.B., A.J.C., Z.K., E.C.S., J. P., J.\nS., K.L., B.S., S.M., E.J.K., J.L., T.M.H, A.A., E.C., J.D., W.S., E.C.; Authors N.J., V.J.M. H.G.,\nand Y.A., provided significant contributions to dataset construction, Redcap evaluation design\nand analysis and writing. Author Simon Jones performed statistical analysis. Authors J.A.D.,\nA.A.B., and A.M. provided significant domain expertise in conceptualization and assistance in\nwriting. Author N.R. led the study, assembled the team, and supervised the full execution of\nthe study and is the corresponding author. Author H.Z. completed all Llama-2 analysis and\nhelped in writing.\nAuthor Contributions\nConceptualization: Narges Razavian.\nData curation: Vincent J. Major, Himanshu Grover, Yindalon Aphinyanaphongs, Narges\nRazavian.\nFormal analysis: Hao Zhang, Simon Jones, Nicholas Genes, Ian S. Jaffe, Anthony B. Cardillo,\nNoah Heilenbach, Nadia Fazal Ali, Luke J. Bonanni, Andrew J. Clayburn, Zain Khera, Erica\nC. Sadler, Jaideep Prasad, Jamie Schlacter, Kevin Liu, Benjamin Silva, Sophie Montgomery,\nEric J. Kim, Jacob Lester, Theodore M. Hill, Alba Avoricani, Ethan Chervonski, James\nDavydov, William Small, Eesha Chakravartty, Narges Razavian.\nFunding acquisition: Narges Razavian.\nInvestigation: Narges Razavian.\nMethodology: Narges Razavian.\nPLOS DIGI TAL HEALT H\nEvaluating Large Language Models in extracting cognitive exam dates and scores\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000068 5 Decemb er 11, 2024 12 / 16\nProject administration: Neil Jethani, Narges Razavian.\nResources: Narges Razavian.\nSoftware: Hao Zhang, Narges Razavian.\nSupervision: Neil Jethani, John A. Dodson, Abraham A. Brody, Arjun Masurkar, Narges\nRazavian.\nValidation: Narges Razavian.\nWriting – original draft: Neil Jethani, Narges Razavian.\nWriting – review & editing: Simon Jones, Nicholas Genes, Vincent J. Major, Anthony B. Car-\ndillo, Noah Heilenbach, Nadia Fazal Ali, Luke J. Bonanni, Andrew J. Clayburn, Zain Khera,\nErica C. Sadler, Jaideep Prasad, Jamie Schlacter, Kevin Liu, Benjamin Silva, Sophie Mont-\ngomery, Eric J. Kim, Jacob Lester, John A. Dodson, Abraham A. Brody, Yindalon Aphinya-\nnaphongs, Arjun Masurkar, Narges Razavian.\nReferences\n1. OpenAI. ChatGP T. 2023 [cited 3 Jul 2023]. Available: http://open ai.com/c hatgpt (accessed June 2023)\n2. OpenAI. GPT-4 Technical Report. arXiv [cs.CL]. 2023. Available: http://arxiv .org/abs/2 303.08774\n3. Singhal K, Tu T, Gottweis J, Sayres R, Wulczyn E, Hou L, et al. Towards Expert-Level Medical Questio n\nAnswerin g with Large Langua ge Models. arXiv [cs.CL]. 2023. Available: http://arxiv .org/abs/2 305.\n09617\n4. Touvron H, Louis Martin, Stone K, Albert P, Almaha iri A, et al. \"Llama 2: Open foundati on and fine-\ntuned chat models.\" arXiv preprint arXiv:2307.09 288 (2023).\n5. Bubeck S, Chandrase karan V, Eldan R, Gehrke J, Horvitz E, Kamar E, et al. Sparks of Artificial General\nIntelligenc e: Early experime nts with GPT-4. arXiv [cs.CL]. 2023. Available: http://arxiv .org/abs/2 303.\n12712\n6. Nori H, King N, McKinney SM, Carignan D, Horvitz E. Capabiliti es of GPT-4 on Medical Challenge Prob-\nlems. arXiv [cs.CL]. 2023. Available: http://arxiv.or g/abs/230 3.13375\n7. Lee P, Bubeck S, Petro J, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine. N Engl J Med.\n2023; 388: 1233–1239.\n8. Kung TH, Cheatham M, Medenil la A, Sillos C, De Leon L, Elepaño C, et al. Perform ance of ChatGP T\non USMLE: Potential for AI-assisted medical education using large language models. PLOS Digit\nHealth. 2023; 2: e00001 98. https://doi.or g/10.137 1/journal.pd ig.0000198 PMID: 36812645\n9. Giannos P. Evaluating the limits of AI in medical specialisatio n: ChatGPT’s performanc e on the UK Neu-\nrology Special ty Certificate Examinati on. BMJ Neurolo gy Open. 2023; 5. https:// doi.org/10.11 36/bmjno -\n2023-00045 1 PMID: 373375 31\n10. Matias Y. Our latest health AI research updates. In: Google [Interne t]. 14 Mar 2023 [cited 3 Jul 2023].\nAvailable: https:// blog.google/te chnolog y/health/ai -llm-medpalm -research-th echeckup/\n11. Sarraju A, Bruemme r D, Van Iterson E, Cho L, Rodriguez F, Laffin L. Appropriat eness of Cardiovasc ular\nDisease Preven tion Recommen dations Obtaine d From a Popular Online Chat-Base d Artificial Intelli-\ngence Model. JAMA. 2023; 329: 842–844. https://doi.or g/10.100 1/jama.20 23.1044 PMID: 367352 64\n12. Haver HL, Ambinder EB, Bahl M, Oluyemi ET, Jeudy J, Yi PH. Appropria teness of Breast Cancer Pre-\nvention and Screening Recommen dations Provided by ChatGP T. Radiolog y. 2023; 307: e230424.\nhttps://doi.or g/10.114 8/radiol.230 424 PMID: 370142 39\n13. Lee T-C, Staller K, Botoman V, Pathip ati MP, Varma S, Kuo B. ChatGPT Answers Common Patient\nQuestions About Colonoscopy . Gastroenter ology. 2023. https://doi.o rg/10.1053/j.g astro.202 3.04.033\nPMID: 371504 70\n14. Ayers JW, Poliak A, Dredze M, Leas EC, Zhu Z, Kelley JB, et al. Comp aring physician and artificia l intel-\nligence chatbot response s to patient questions posted to a public social media forum. JAMA Intern\nMed. 2023; 183: 589–596 . https://doi.or g/10.1001/ jamainternm ed.2023.1838 PMID: 37115527\n15. Dash D, Thapa R, Banda JM, Swaminatha n A, Cheatham M, Kashyap M, et al. Evaluation of GPT-3.5\nand GPT-4 for supporti ng real-world information needs in healthcar e delivery. arXiv [cs.AI]. 2023. Avail-\nable: http://arxiv .org/abs/2304. 13714\nPLOS DIGI TAL HEALT H\nEvaluating Large Language Models in extracting cognitive exam dates and scores\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000068 5 Decemb er 11, 2024 13 / 16\n16. Cascella M, Montomoli J, Bellini V, Bignam i E. Evaluating the Feasibili ty of ChatGP T in Healthcare : An\nAnalysis of Multiple Clinical and Resea rch Scenarios. J Med Syst. 2023; 47: 33. https://doi.or g/10.100 7/\ns10916-023 -01925-4 PMID: 36869927\n17. Koo M. The Importance of Proper Use of ChatGP T in Medical Writing . Radiolog y. 2023; 307: e230312.\nhttps://doi.or g/10.114 8/radiol.230 312 PMID: 368809 46\n18. Stokel-W alker C. ChatGPT listed as author on research papers: many scientist s disapprov e. In: Nature\nPublishing Group UK [Interne t]. 18 Jan 2023 [cited 4 Jul 2023]. https://d oi.org/10.103 8/d4158 6-023-\n00107-z PMID: 36653617\n19. Thorp HH. ChatGP T is fun, but not an author. Science. 2023; 379: 313–313. https:// doi.org/10.11 26/\nscience.adg7 879 PMID: 36701446\n20. Nature. Authorship. In: Nature Authorship [Interne t]. Springer Nature; 2023 [cited 4 Jul 2023]. Availab le:\nhttps://www .nature.co m/nature/e ditorial-po licies/autho rship\n21. JAMA. Instructions for Authors . In: JAMA Authorship Guideline s [Interne t]. 4 Jul 2023 [cited 4 Jul 2023].\nAvailable: https:// jamanetwor k.com/journa ls/jama/pag es/inst ructions-for-a uthors\n22. Hosseini M, Rasmussen LM, Resnik DB. Using AI to write scholarly publicati ons. Account Res. 2023;\n1–9. https://do i.org/10.1080 /0898962 1.2023.216853 5 PMID: 36697395\n23. Park D. Open LLM Leaderboar d. In: Open LLM Leaderboar d [Interne t]. 4 Jul 2023 [cited 4 Jul 2023].\nAvailable: https:// huggingface. co/spaces/Hu ggingFa ceH4/open_l lm_leaderboa rd\n24. Aronson AR, Lang F-M. An overview of MetaMap: historical perspective and recent advances. J Am\nMed Inform Assoc. 2010; 17: 229–236. https://do i.org/10.1136 /jamia.200 9.002733 PMID: 20442139\n25. Soysal E, Wang J, Jiang M, Wu Y, Pakhomov S, Liu H, et al. CLAMP—a toolkit for efficiently building\ncustomize d clinical natural langua ge processing pipelines. J Am Med Inform Assoc. 2018; 25: 331–336.\nhttps://doi.or g/10.109 3/jamia/ocx13 2 PMID: 29186491\n26. Wu H, Toti G, Morley KI, Ibrahim ZM, Folari n A, Jackson R, et al. SemEHR : A general-pu rpose seman-\ntic search system to surface semantic data from clinical notes for tailored care, trial recruitme nt, and\nclinical research. J Am Med Inform Assoc. 2018; 25: 530–537. https://doi.o rg/10.1093/ja mia/ocx1 60\nPMID: 293610 77\n27. Savova GK, Masanz JJ, Ogren PV, Zheng J, Sohn S, Kipper-S chuler KC, et al. Mayo clinical Text Anal-\nysis and Knowled ge Extraction System (cTAKES) : architectur e, component evaluatio n and applica-\ntions. J Am Med Inform Assoc. 2010; 17: 507–513. https://doi.or g/10.1136/ jamia.2009.0 01560 PMID:\n20819853\n28. Chapman WW, Bridewe ll W, Hanbury P, Cooper GF, Buchan an BG. A Simple Algorithm for Identifying\nNegated Findings and Diseases in Dischar ge Summaries . J Biomed Inform. 2001; 34: 301–310. https://\ndoi.org/10.10 06/jbin.2001.1 029 PMID: 12123149\n29. Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. Chestx-ray 8: Hospital-scale chest x-ray data-\nbase and benchmarks on weakly-s upervised classif ication and localizatio n of commo n thorax diseases.\nProceedings of the IEEE conferenc e on computer vision and pattern recognition. 2017. pp. 2097–2106.\n30. Irvin J, Rajpurkar P, Ko M, Yu Y, Ciurea- Ilcus S, Chute C, et al. Chexpert: A large chest radiograph data-\nset with uncertai nty labels and expert comparison . arXiv preprint arXiv:19 01 07031. 2019. Available:\nhttps://www .aaai.org/Pa pers/AA AI/2019/AAA I-IrvinJ.65 37.pdf\n31. Smit A, Jain S, Rajpurkar P, Pareek A, Ng AY, Lungren MP. CheXbert : Combining Automatic Labelers\nand Expert Annota tions for Accurate Radiolog y Report Labeling Using BERT. arXiv [cs.CL]. 2020.\nAvailable: http://arxiv .org/abs/2 004.09167\n32. McDermo tt MBA, Hsu TMH, Weng W-H, Ghassemi M, Szolovits P. CheXper t++: Approxim ating the\nCheXper t labeler for Speed ,Differentia bility, and Probabilistic Output. arXiv [cs.LG]. 2020. Availab le:\nhttp://arxiv.o rg/abs/2006.1 5229\n33. Le Glaz A, Haralambo us Y, Kim-Dufor D-H, Lenca P, Billot R, Ryan TC, et al. Machine Learning and\nNatural Language Processin g in Mental Health: System atic Review. J Med Internet Res. 2021; 23:\ne15708. https:// doi.org/10.21 96/1570 8 PMID: 33944788\n34. Weng W-H, Waghol ikar KB, McCray AT, Szolovits P, Chueh HC. Medical subdomain classificat ion of\nclinical notes using a machine learning-b ased natural language processing approach. BMC Med Inform\nDecis Mak. 2017; 17: 1–13.\n35. Jiang LY, Liu XC, Nejatian NP, Nasir-M oin M, Wang D, Abidin A, et al. Health system-sc ale language\nmodels are all-purpose prediction engines . Nature. 2023; 1–6. https://doi. org/10.1038/s 41586-023-\n06160-y PMID: 37286606\n36. Leiter RE, Santus E, Jin Z, Lee KC, Yusufov M, Chien I, et al. Deep Natural Langua ge Processin g to\nIdentify Symptom Docume ntation in Clinical Notes for Patients With Heart Failure Undergoing Cardiac\nResynchr onization Therapy. J Pain Symptom Manage. 2020; 60: 948–958.e3. https://doi.or g/10.1016/ j.\njpainsymm an.2020.06.0 10 PMID: 32585181\nPLOS DIGI TAL HEALT H\nEvaluating Large Language Models in extracting cognitive exam dates and scores\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000068 5 Decemb er 11, 2024 14 / 16\n37. Wei W-Q, Teixeira PL, Mo H, Cronin RM, Warner JL, Denny JC. Comb ining billing codes, clinical notes,\nand medication s from electronic health records provide s superior phenotyp ing performanc e. J Am Med\nInform Assoc. 2015; 23: e20–e2 7. https://doi.or g/10.1093 /jamia/ocv130 PMID: 263382 19\n38. Taggart M, Chapman WW, Steinberg BA, Ruckel S, Pregenz er-Wenzler A, Du Y, et al. Comparison of 2\nNatural Language Processin g Methods for Identificati on of Bleeding Among Critically Ill Patients. JAMA\nNetw Open. 2018; 1: e183451–e18 3451. https://doi.or g/10.1001/ jamanetwor kopen.20 18.3451 PMID:\n30646240\n39. Wu Y, Denny JC, Trent Rosenbloom S, Miller RA, Giuse DA, Xu H. A comparative study of current clini-\ncal natural languag e processing systems on handling abbreviations in discharge summa ries. AMIA\nAnnu Symp Proc. 2012; 2012: 997. PMID: 23304375\n40. Fan Y, Wen A, Shen F, Sohn S, Liu H, Wang L. Evaluating the Impact of Dictionary Updates on Auto-\nmatic Annotations Based on Clinical NLP System s. AMIA Summits Transl Sci Proc. 2019; 2019: 714.\nPMID: 312590 28\n41. Larochelle H, Erhan D, Bengio Y. Zero-data learning of new tasks. Proceedings of the 23rd national\nconferen ce on Artificial intelligenc e—Volume 2. AAAI Press; 2008. pp. 646–65 1.\n42. Wei J, Bosma M, Zhao VY, Guu K, Yu AW, Lester B, et al. Finetune d language models are zero-sho t\nlearners. arXiv [cs.CL]. 2021. Availab le: https://resear ch.google /pubs/pub 51119/\n43. Rezaei M, Shahidi M. Zero-sho t learning and its applicati ons from autonomo us vehicles to COVID-19\ndiagnos is: A review. Intelligence-B ased Medicine. 2020; 3–4: 100005. https://doi.or g/10.1016/ j.ibmed.\n2020.10000 5 PMID: 33043311\n44. Borji A. A Categorica l Archive of ChatGP T Failures. arXiv [cs.CL]. 2023. Available: http://arxiv. org/abs/\n2302.03494\n45. Maynez J, Narayan S, Bohnet B, McDona ld R. On Faithfuln ess and Factuality in Abstractive Summari -\nzation. Proceedings of the 58th Annual Meeting of the Association for Computa tional Linguistics.\nOnline: Association for Computat ional Linguistics; 2020. pp. 1906–1 919.\n46. Tsoy E, Kiekhofe r RE, Guterman EL, Tee BL, Windon CC, Dorsman KA, et al. Assessme nt of Racial/\nEthnic Disparities in Timeliness and Comprehen siveness of Dementia Diagnosis in Californi a. JAMA\nNeurol. 2021; 78: 657–665. https:/ /doi.org/10.10 01/jaman eurol.2021.0 399 PMID: 33779684\n47. Lin P-J, Daly A, Olchanski N, Cohen JT, Neumann PJ, Faul JD, et al. Deme ntia diagnosis disparities by\nrace and ethnicity. Alzheime rs Deme nt. 2020; 16. https:// doi.org/10.10 02/alz.043 183\n48. Saadi A, Himmelste in DU, Woolha ndler S, Mejia NI. Racial disparities in neurologic health care access\nand utilizatio n in the United States. Neurolo gy. 2017; 88: 2268–2275. https:// doi.org/10.12 12/WNL.\n00000000000 04025 PMID: 28515272\n49. Drabo EF, Barthold D, Joyce G, Ferido P, Chang Chui H, Zissimopoulo s J. Longitudinal analysis of\ndementia diagnosis and specialty care among racially diverse Medicare beneficia ries. Alzheime rs\nDement. 2019; 15: 1402–1411. https:// doi.org/10.10 16/j.jalz.2 019.07.0 05 PMID: 31494079\n50. Livingston G, Huntley J, Sommerl ad A, Ames D, Ballard C, Banerjee S, et al. Deme ntia prevention,\nintervent ion, and care: 2020 report of the Lancet Commission . Lancet. 2020; 396: 413–446. https:// doi.\norg/10.1016/ S0140-673 6(20)3036 7-6 PMID: 32738937\n51. Harper LC. 2022 Alzheime r’s Association Facts and Figures. https. Available: https:// www.cambridg e.\norg/core/s ervices/aop -cambrid ge-core/cont ent/view/915A 476B938D 0AF39A218D 34852AF645/\n97810093251 89mem _205-207.pd f/resource s.pdf\n52. US Dept of Health and Human Services. National Plan to Addres s Alzheime r’s Disease: 2020 Update.\n2021 [cited 1 Nov 2021]. Available: https://aspe.hhs .gov/repo rts/national-pl an-address -alzheimers -\ndisease-2 020-update- 0\n53. SPRINT MIND Investigato rs for the SPRINT Researc h Group, Williamson JD, Pajewski NM, Auchus\nAP, Bryan RN, Chelune G, et al. Effect of Intensive vs Standard Blood Pressure Control on Probable\nDementia: A Random ized Clinical Trial. JAMA. 2019; 321: 553–561. https://doi.or g/10.1001/ jama.2018.\n21442 PMID: 306889 79\n54. Pragmatic Evaluation of Events And Benefit s of Lipid-loweri ng in Older Adults—F ull Text View—Clin i-\ncalTrials.G ov. [cited 27 Oct 2021]. Available: https://c linicaltrials.gov /ct2/show/NC T04262206\n55. NIA. NIA-funde d active Alzheime r’s and related dementia s clinical trials and studies. In: NIA [Interne t].\n2021 [cited 20 Apr 2021]. Available: https:// www.nia.ni h.gov/resea rch/ongoing- AD-trials\n56. Science. In: AAAS [Internet] . [cited 10 Jul 2023]. Available: https:// www.scienc e.org/cont ent/article/\nanother-al zheimers- drug-flops-p ivotal-cli nical-trial\n57. Drug Approv al Package: Aduhelm (aducanu mab-avwa). [cited 31 Oct 2021]. Availab le: https://www .\naccessda ta.fda.gov/dr ugsatfda_ docs/nda/20 21/7611 78Orig1s00 0TOC.cfm\n58. Manly JJ, Glymour MM. What the Aducanum ab Approval Reveals About Alzheime r Disease Research.\nJAMA Neurol. 2021. https://doi.or g/10.1001/ jamaneurol.20 21.3404 PMID: 34605885\nPLOS DIGI TAL HEALT H\nEvaluating Large Language Models in extracting cognitive exam dates and scores\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000068 5 Decemb er 11, 2024 15 / 16\n59. Folstein MF, Folstein SE, McHugh PR. Mini-Men tal State Examinati on. J Psychiatr Res. 1975. https://\ndoi.org/10.10 37/t07757-00 0\n60. Morris JC. The Clinical Dementia Rating (CDR): Current version and scoring rules. Neurolo gy.\n1993. pp. 2412–2 412. https://doi.or g/10.121 2/wnl.43.11 .2412-a PMID: 823297 2\n61. Collins G.S., Reitsma J.B., Altman D.G. et al. Transparent reporting of a multivaria ble prediction model\nfor individual prognos is or diagnosis (TRIPOD) : the TRIPOD Statemen t. BMC Med 13, 1 (2015).\nhttps://doi.or g/10.118 6/s12916 -014-0241-z PMID: 25563062\n62. Nasreddin e ZS, Phillips NA, Be ´ dirian V, Charbonne au S, Whitehead V, Collin I, et al. The Montreal Cog-\nnitive Asses sment, MoCA: a brief screening tool for mild cognitive impairme nt. J Am Geriatr Soc. 2005;\n53: 695–699. https://doi.or g/10.1111/ j.1532-5415 .2005.53221 .x PMID: 158170 19\n63. ADNI. 2021 [cited 1 Nov 2021]. Available: http://adni .loni.usc.edu /data-samp les/adni- participant-\ndemogra phic/\n64. Azure OpenAI Service content filtering—A zure OpenAI. [cited 10 Jul 2023]. Availab le: https://lear n.\nmicrosoft.c om/en-us/a zure/cogn itive-servi ces/openai /concepts/ content-filter\n65. Fleiss JL. Measurin g nominal scale agreem ent among many raters. Psychol Bull. 1971; 76: 378–38 2.\n66. Hallgren KA. Comp uting Inter-Rate r Reliabili ty for Observa tional Data: An Overview and Tutorial. Tutor\nQuant Methods Psychol. 2012; 8: 23. https://doi.or g/10.209 82/tqmp.08 .1.p023 PMID: 22833776\n67. Maxwell AE. Coeffic ients of Agreem ent Between Observers and Their Interpre tation. Br J Psychiatry.\n1977; 130: 79–83. https://doi.or g/10.119 2/bjp.130.1.79 PMID: 831913\n68. Beekly DL, Ramo s EM, van Belle G, Deitrich W, Clark AD, Jacka ME, et al. The National Alzheime r’s\nCoordin ating Center (NACC) Database: an Alzheime r disease database. Alzheime r Dis Assoc Disord.\n2004; 18: 270–27 7. PMID: 15592144\nPLOS DIGI TAL HEALT H\nEvaluating Large Language Models in extracting cognitive exam dates and scores\nPLOS Digital Health | https://doi.or g/10.137 1/journal.pd ig.000068 5 Decemb er 11, 2024 16 / 16",
  "topic": "Natural language processing",
  "concepts": [
    {
      "name": "Natural language processing",
      "score": 0.5557422637939453
    },
    {
      "name": "Cognition",
      "score": 0.5342333316802979
    },
    {
      "name": "Computer science",
      "score": 0.44438838958740234
    },
    {
      "name": "Psychology",
      "score": 0.39384347200393677
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35439053177833557
    },
    {
      "name": "Psychiatry",
      "score": 0.06041312217712402
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    }
  ],
  "cited_by": 7
}