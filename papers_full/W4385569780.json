{
    "title": "Evaluating Open-Domain Question Answering in the Era of Large Language Models",
    "url": "https://openalex.org/W4385569780",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2963155088",
            "name": "Ehsan Kamalloo",
            "affiliations": [
                "Allen Institute for Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2803702289",
            "name": "Nouha Dziri",
            "affiliations": [
                "Allen Institute for Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2021138190",
            "name": "Charles Clarke",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A272765103",
            "name": "Davood Rafiei",
            "affiliations": [
                "Allen Institute for Artificial Intelligence"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4287122359",
        "https://openalex.org/W4287854593",
        "https://openalex.org/W2962718483",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2889787757",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4306294746",
        "https://openalex.org/W4320813768",
        "https://openalex.org/W4225992558",
        "https://openalex.org/W4296557505",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3206455169",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3214608568",
        "https://openalex.org/W4287185415",
        "https://openalex.org/W3199493219",
        "https://openalex.org/W4205456754",
        "https://openalex.org/W2919420119",
        "https://openalex.org/W2983309655",
        "https://openalex.org/W2987553933",
        "https://openalex.org/W3176182290",
        "https://openalex.org/W3034383590",
        "https://openalex.org/W4226157795",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W3102659883",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W1801866228",
        "https://openalex.org/W3118668786",
        "https://openalex.org/W3156789018",
        "https://openalex.org/W3196194504",
        "https://openalex.org/W2083581617",
        "https://openalex.org/W2990928880",
        "https://openalex.org/W4289865932",
        "https://openalex.org/W4385572714",
        "https://openalex.org/W3099700870",
        "https://openalex.org/W3118423943",
        "https://openalex.org/W2051167396",
        "https://openalex.org/W4285225959",
        "https://openalex.org/W2951434086",
        "https://openalex.org/W348133106"
    ],
    "abstract": "Lexical matching remains the de facto evaluation method for open-domain question answering (QA). Unfortunately, lexical matching fails completely when a plausible candidate answer does not appear in the list of gold answers, which is increasingly the case as we shift from extractive to generative models. The recent success of large language models (LLMs) for QA aggravates lexical matching failures since candidate answers become longer, thereby making matching with the gold answers even more challenging. Without accurate evaluation, the true progress in open-domain QA remains unknown. In this paper, we conduct a thorough analysis of various open-domain QA models, including LLMs, by manually evaluating their answers on a subset of NQ-open, a popular benchmark. Our assessments reveal that while the true performance of all models is significantly underestimated, the performance of the InstructGPT (zero-shot) LLM increases by nearly +60%, making it on par with existing top models, and the InstructGPT (few-shot) model actually achieves a new state-of-the-art on NQ-open. We also find that more than 50% of lexical matching failures are attributed to semantically equivalent answers. We further demonstrate that regex matching ranks QA models consistent with human judgments, although still suffering from unnecessary strictness. Finally, we demonstrate that automated evaluation models are a reasonable surrogate for lexical matching in some circumstances, but not for long-form answers generated by LLMs. The automated models struggle in detecting hallucinations in LLM answers and are thus unable to evaluate LLMs. At this time, there appears to be no substitute for human evaluation.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 5591–5606\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nEvaluating Open-Domain Question Answering\nin the Era of Large Language Models\nEhsan Kamalloo ♢ ♣ Nouha Dziri ♠ Charles L. A. Clarke ♣ Davood Rafiei♢\n♢ University of Alberta ♣ University of Waterloo\n♠ Allen Institute for Artificial Intelligence\nekamalloo@uwaterloo.ca\nAbstract\nLexical matching remains the de facto eval-\nuation method for open-domain question an-\nswering (QA). Unfortunately, lexical matching\nfails completely when a plausible candidate an-\nswer does not appear in the list of gold answers,\nwhich is increasingly the case as we shift from\nextractive to generative models. The recent suc-\ncess of large language models (LLMs) for QA\naggravates lexical matching failures since can-\ndidate answers become longer, thereby making\nmatching with the gold answers even more chal-\nlenging. Without accurate evaluation, the true\nprogress in open-domain QA remains unknown.\nIn this paper, we conduct a thorough analysis\nof various open-domain QA models, including\nLLMs, by manually evaluating their answers\non a subset of NQ-OPEN , a popular benchmark.\nOur assessments reveal that while the true per-\nformance of all models is significantly under-\nestimated, the performance of the InstructGPT\n(zero-shot) LLM increases by nearly +60%,\nmaking it on par with existing top models,\nand the InstructGPT (few-shot) model actually\nachieves a new state-of-the-art on NQ- OPEN .\nWe also find that more than 50% of lexical\nmatching failures are attributed to semantically\nequivalent answers. We further demonstrate\nthat regex matching ranks QA models consis-\ntent with human judgments, although still suf-\nfering from unnecessary strictness. Finally, we\ndemonstrate that automated evaluation models\nare a reasonable surrogate for lexical matching\nin some circumstances, but not for long-form\nanswers generated by LLMs. The automated\nmodels struggle in detecting hallucinations in\nLLM answers and are thus unable to evaluate\nLLMs. At this time, there appears to be no\nsubstitute for human evaluation.1\n1 Introduction\nReliable benchmarks have been a bedrock to mea-\nsuring progress in open-domain QA, the task of an-\n1Code and data are released at https://github.com/\nehsk/OpenQA-eval.\nWhat was the city of Beijing previously known as?\nPeking Jicheng\nLexical \nMatch\nQA Model\nWho won the Oscar for best actor in 1975?\nQA ModelArt Carney\nJack Nicholson won \nthe Oscar for Best \nActor in 1975 for \nhis performance in \nOne Flew Over the \nCuckoo's Nest.\nLexical \nMatch\nFigure 1: Examples of failures in open-domain QA eval-\nuation. Top: Jicheng is a credible answer although not\npresent in the list of gold answers. Existing automated\nevaluation mechanisms fail to identify it as correct. Bot-\ntom: A seemingly correct but unattributable answer\nfrom InstructGPT (Ouyang et al., 2022) for which auto-\nmatic evaluation goes astray.\nswering information-seeking questions over a mas-\nsive text corpus. In recent years, we have seen great\nstrides in open-domain QA by novel models (Chen\net al. 2017; Wang et al. 2018; Clark and Gardner\n2018; Lee et al. 2019; Asai et al. 2020; Izacard and\nGrave 2021b,a; Khattab et al. 2021; Singh et al.\n2021; Asai et al. 2022; inter alia) that continue\nto raise state-of-the-art on well-established bench-\nmarks such as Natural Questions-OPEN (Lee et al.,\n2019). The standard procedure for evaluating open-\ndomain QA models, borrowed from reading com-\nprehension (Rajpurkar et al., 2016), is to perform\nlexical matching between gold answers provided in\nthe benchmark and models’ predictions. However,\nas the performance of open-domain QA approaches\nthat of humans,2 these classic evaluation methods\nbegin to fail. Such failures largely stem from the\nincomplete list of gold answers that do not fully\ncover all plausible answers. For example, in Fig-\nure 1, “Jicheng” is a correct answer to what was\nthe city of Beijing previously known as? while not\nannotated as a gold answer in Natural Questions-\n2typically equipped with a search engine\n5591\nOPEN (NQ-OPEN ; Lee et al. 2019).\nWith the recent success of generative QA sys-\ntems in the open-domain setting (Izacard and\nGrave, 2021b; Roberts et al., 2020), it becomes\nharder for lexical matching to recognize correct\nanswers, and in turn for us, to recognize perfor-\nmance differences between models. The problem\nis exacerbated by a tendency of Large Language\nModels(LLM)-based systems (Brown et al. 2020;\nChowdhery et al. 2022; Zhang et al. 2022; Black\net al. 2022; inter alia) to occasionally hallucinate\nplausible but incorrect answers (Dziri et al., 2022;\nYe and Durrett, 2022). For instance, in Figure 1,\nInstructGPT (Ouyang et al., 2022) generates “Jack\nNicholson” in great details to answer who won the\noscar for best actor in 1975? but although looks\nnatural, the answer is not factually correct (he won\nin 1976). Therefore, human confirmation of answer\ncorrectness demands additional effort and care due\nto the ability of LLMs to formulate these answers\nas complete and seemingly authoritative.\nWhile it might be assumed that improved per-\nformance under lexical matching would reflect im-\nproved performance in an absolute sense, even if\nsome correct answers are missed, we show this as-\nsumption does not hold. For this purpose, we man-\nually re-evaluate several open-domain QA models\non a random subset of NQ-OPEN (Lee et al., 2019),\nan established benchmark. Not only is true perfor-\nmance substantially underestimated by this bench-\nmark, but the relative performance of the models\nalters after re-evaluation: InstructGPT (zero-shot)\nachieves an accuracy of 12.6% on our NQ- OPEN\nsubset, but our human judgment reveals its true\nperformance to be 71.4%, a nearly +60% improve-\nment. Our linguistic analysis of the failure cases of\nlexical matching, an extension of a similar study by\nMin et al. (2021), shows that the mismatches are\nmostly linguistically shallow and could be captured\nby simple patterns, such as regular expressions.\nIn contrast, automated evaluation mechanisms\nsuch as BEM (Bulian et al., 2022) based on seman-\ntic matching between the gold answers and gener-\nated answers produce a relative performance that is\nmostly consistent with human evaluation, although\nthe absolute improvements are lower. However,\nlong-form answers, generated by LLMs, introduce\na new challenge that did not occur on prior models;\nthey are prone to carry unattributable information\n(Rashkin et al., 2021). Automated evaluation mod-\nels often deem the hallucinated responses correct,\nwhich is why, InstructGPT (zero-shot) is overes-\ntimated under these models, compared to human\njudgment.\nWe repeated this experiment with the 20-year-\nold CuratedTREC dataset (V oorhees, 2003) that\nprovides its gold answers in the form of regular ex-\npressions. We observe that the relative performance\nof models remains mostly consistent under all three\nevaluation mechanisms, i.e., regular expressions,\nhuman evaluation, and semantic matching, with\nonly slight differences in absolute performance.\nHowever, the ranking discrepancy still persists be-\ntween the two LLMs, i.e., InstructGPT (zero-shot)\nand InstructGPT (few-shot). Also, only under hu-\nman judgment does the absolute performance of\nLLMs exceed that of the heavily engineered sta-\ntistical NLP systems from 20 years ago on this\ncollection. Until recently, the best of these classi-\ncal systems has been substantially superior to even\nthe best of the modern neural models. In light of\nour observations, we highlight that while semantic\nmatching against exact answers would have been\nsufficient for QA evaluation prior to LLMs, they\ncannot accurately evaluate LLMs.\n2 Related Work\nAnswer Equivalence in QA. One way to tackle\nthis task is through the automatic collection of alter-\nnative plausible answers from auxiliary knowledge\nsources such as a knowledge base (Si et al., 2021).\nHowever, the effectiveness of this approach is heav-\nily contingent on the presence of answers in the\nknowledge source, which is often not the case. For\ninstance, numerical answers or common phrases\nare unlikely to be found in a knowledge base. More-\nover, matching gold answers with knowledge base\nentries can also be problematic as their surface\nforms may not be identical. Thus, these approaches\nfail to scale for various types of answers. Another\nline of work focuses on building models to perform\nsemantic similarity between candidate answers and\ngold answers, which can supersede lexical match-\ning for verifying answers (Chen et al., 2019, 2020;\nRisch et al., 2021; Bulian et al., 2022). These meth-\nods indeed work well in reading comprehension\nbecause the presence of an input context often cur-\ntails the possibilities of models’ generated answers.\nHowever, they are susceptible to failure in open-\ndomain QA where questions should be answered\nwithout any additional context. Similarly, unsu-\npervised semantic similarity-based evaluation met-\n5592\nrics such as BERTScore (Zhang et al., 2020) that\nrely on token-level matching of contextualized rep-\nresentations exhibit poor correlation with human\njudgment in QA evaluation (Chen et al., 2019) and\nlack the ability to capture attributability (Maynez\net al., 2020).\nHuman Judgment in QA. Many works (Roberts\net al., 2020; Min et al., 2021) resort to human eval-\nuation to assess QA models. Although using hu-\nmans for evaluation is expensive and not scalable,\nMin et al. (2021) find that the performance of QA\nsystems bumps up 23% on average using human\njudgment. The substantial gap between the true per-\nformance and token-based metrics showcases the\nlong known strictness problem of lexical matching.\n3 Open-domain QA Evaluation\nThe task of open-domain QA is referred to finding\nanswers for information-seeking questions given\na massive knowledge source such as Wikipedia\n(V oorhees and Tice, 2000). The questions are typ-\nically factoid with short answers and acontextual\n(Rogers et al., 2022). Open-domain QA datasets\nencompass questions with their annotated gold an-\nswers that serve as a reference for evaluation. Fol-\nlowing reading comprehension (Rajpurkar et al.,\n2016), evaluation is carried out via lexical match-\ning using the following two widely used metrics to\nmeasure the performance of models:\n• Exact-Match accuracy (EM): A candidate\nanswer is deemed correct iff it can be found\nin the set of gold answers. The ratio of correct\nanswers in the test collection is reported as\nEM accuracy.\n• F1 score: Considering answers as bags of\ntokens, a candidate answer receives a partial\nscore (F1) iff its tokens overlap with those\nof a gold answer. The maximum F 1 score\nover a set of gold answers is assigned to the\ncandidate answer. The final metric at corpus-\nlevel is measured via averaging F1 scores over\nthe test collection.\nBased on the implementation of Rajpurkar et al.\n(2016), answers are normalized (i.e., case-folded,\nand punctuation and articles are discarded) to com-\npute these metrics.\n3.1 Models\nWe select open-domain QA models with publicly\navailable codebase and reproduce their reported re-\nsults. For all models, the “base” flavors are chosen\nfor the experiments. In total, we use 12 models.\nRetriever-Reader Models. DPR (Karpukhin\net al., 2020) is a well-known open-domain QA\nmodel that consists of a bi-encoder retriever and\nleverages an extractive reader. In addition to DPR,\nwe pair several retrievers with Fusion-In-Decoder\n(FiD; Izacard and Grave 2021b), a prominent gener-\native model that condition generating an answer on\na list of passages: ANCE (Xiong et al., 2021), Con-\ntriever3 (Izacard et al., 2022) RocketQAv2 (Ren\net al., 2021), and FiD-KD (Izacard and Grave,\n2021a). Further, we leverage GAR (Mao et al.,\n2021), a sparse retrieval model that augments ques-\ntions with relevant contextual information gener-\nated by a fine-tuned T5 (Raffel et al., 2020). We\nfuse ANCE and GAR results with BM25, namely\nANCE+ and GAR+, as they led to better results.\nWe also use R2-D2 (Fajcik et al., 2021) that com-\nbines extractive and generative readers.\nEnd-to-End Models. EMDR2 (Singh et al.,\n2021) is an end-to-end model that jointly trains\na dense retriever with a FiD-style reader. We also\nuse EviGen (Asai et al., 2022) that jointly learns to\npredict the evidentiality of passages and to generate\nthe final answer in a multi-task fashion.\nClosed-book Models. We use InstructGPT 4\n(Ouyang et al., 2022) in two settings, following\nBrown et al. (2020): zero-shot and few-shot where\nthe prompt includes 64 question/answer pairs, ran-\ndomly sampled from the NQ-OPEN training data.\n3.2 Dataset\nWe select questions from NQ- OPEN (Lee et al.,\n2019), a popular open-domain QA benchmark, that\nconsists of 3610 questions in the test set. We ran-\ndomly sample 301 questions from NQ-OPEN . An-\nswers are generated via the prominent open-domain\nQA models, described in §3.1, for the selected ques-\ntions. In total, the number of unique answers gen-\nerated by the 12 models for 301 questions amounts\nto 1490 question/answer pairs. Our experiments\nare done on Wikipedia, following the same settings\nprovided by Karpukhin et al. (2020).\n3https://huggingface.co/facebook/\ncontriever-msmarco\n4text-davinci-003, the details about this model\nare available at https://beta.openai.com/docs/\nmodel-index-for-researchers .\n5593\n4 Strategies for Evaluating Open-domain\nQA Models\nOur goal is to shed light on the discrepancies be-\ntween the actual and the measured accuracy of\nopen-domain QA models. To this end, we adopt\nthree evaluation mechanisms in addition to lexical\nmatching to assess 12 open-domain QA models\nand draw a comparison between their estimated\naccuracy and the token-based performance.\n4.1 Supervised Evaluation via Semantic\nSimilarity\nA common paradigm to evaluate QA systems is\nto cast evaluation as a classification task where\nthe goal is to decide whether gold answers and\ncandidate answers are semantically equivalent or\nnot (Risch et al., 2021; Bulian et al., 2022). To this\nend, we use a recent BERT-based model, namely\nBEM (Bulian et al., 2022), that is trained on a\nhuman-annotated collection of answer pairs given\na question, derived from SQuAD (Rajpurkar et al.,\n2016). For evaluation, we feed a question along\nwith a gold answer and a candidate answer to BEM\nand take its prediction. For questions with multiple\ngold answers, each gold answer is independently\ntested with a candidate answer. Once matched with\neither of the gold answers, a candidate answer is\ndeemed correct.\n4.2 Zero-shot Evaluation via Prompting\nWe also test the ability of LLMs for evaluating\nQA models. In open-domain QA, the task of an-\nswer equivalence requires supplementary informa-\ntion in the absence of a given context, e.g., match-\ning “Jicheng” with “Peking” in Figure 1; therefore,\nLLMs are a reasonable choice here because they are\nequipped with an implicit memory that encompass\nknowledge (Roberts et al., 2020), serving thus as\nan auxiliary information. To use LLMs for evaluat-\ning models, we elicit the following prompt through\nInstructGPT (Ouyang et al., 2022):\nQuestion: what was the city of Beijing\npreviously known as?\nAnswer: Peking\nCandidate: Jicheng\nIs candidate correct?\nWe include the gold answer along with the candi-\ndate answer in the prompt, akin to the semantic\nsimilarity mechanism, as the objective here is to\nverify the correctness of the candidate. We call this\nevaluation method, InstructGPT-eval. We also test\nGPT-4 (OpenAI, 2023) using the same evaluation\nmethod, namely GPT4-eval, and observe that its\nresults, reported in §A, closely resemble to those\nobtained from InstructGPT-eval.\n4.3 Human Evaluation\nHuman evaluation reflects the true performance of\na model and serves as a basis for checking the fea-\nsibility of other evaluation mechanisms. For this\npurpose, we ask two human annotators5 to judge\nwhether a given answer to a question is correct\nor not. We present only question/answer pairs to\nhuman annotators to avoid any inadvertent biases,\ni.e., the annotators do not know which answers cor-\nrespond to which model nor do they know if an\nanswer is a gold answer. Annotators are allowed to\nuse a search engine to find evidence that supports\nor rejects a candidate answer. Our annotation pro-\ncedure is specifically geared towards open-domain\nQA unlike those of Risch et al. (2021) and Bulian\net al. (2022) that are designed for reading com-\nprehension where annotators decide equivalence\nbetween a pair of answers given a question and a\ncontext.\nThe Fleiss’ Kappa score between the two anno-\ntators is 72.8%, i.e., 202 disagreements out of 1490\ncases (13.6%), indicating substantial agreement.\nMost disagreements arise from questions that are\nmore likely to possess subjective answers. They\nmainly fall into three categories: ambiguous (e.g.,\n“what is the corporate tax rate in great britain ”),\nlist-style (e.g. “who dies in the lost city of z”), and\ntime-dependent (e.g. “ latest series of keeping up\nwith the kardashians”) questions. We ask a third\nannotator to judge the 202 cases where the two\nannotators diverged and take a majority vote to de-\ntermine the correctness. The accepted answers by\nthe annotators are then added to the set of gold an-\nswers for the selected questions. We compute the\naccuracy of the 12 models after amending the gold\nanswers and compare it with the original accuracy\nthat is computed via lexical matching.\n4.4 Results and Discussion\nTable 1 presents the accuracy of the open-domain\nQA models, computed using the three evaluation\nmechanisms, BEM, InstructGPT-eval, and Human,\ncompared to the de facto EM accuracy. The ac-\ncuracy of all models consistently surges across all\n5The human annotators are the authors of this paper.\n5594\nModel K Entire Data(3.6K) Sampled (301) BEM InstructGPT-eval Human\nEM F 1 EM F 1 Acc ∆ Acc ∆ Acc ∆\nInstructGPT (zero-shot) - 14.6 - 12.6 27.5 63.5 +50.9 77.1 +64.5 71.4 +58.8\nInstructGPT (few-shot) - 29.9 - 33.9 50.5 59.5 +25.6 67.8 +33.9 75.8 +41.9\nDPR 50 40.9 47.8 45.9 52.3 52.5 +6.6 55.1 +9.2 58.8 +12.9\nFiD 100 46.5 53.7 47.8 55.4 58.1 +10.3 61.5 +13.7 64.8 +17.0\nANCE+ & FiD 50 47.3 54.8 48.2 55.9 59.5 +11.3 63.1 +14.9 65.8 +17.6\nRocketQAv2 & FiD 100 47.7 55.6 49.8 58.7 62.5 +12.7 66.1 +16.3 70.1 +20.3\nContriever & FiD 100 47.9 55.4 46.5 55.9 60.8 +14.3 63.1 +16.6 66.5 +20.0\nFiD-KD 100 49.6 57.4 50.8 61.2 65.8 +15.0 70.4 +19.6 73.1 +22.3\nGAR+ & FiD 100 49.8 57.4 50.8 59.7 63.1 +12.3 67.1 +16.3 69.4 +18.2\nEviGen 20 49.8 57.0 51.8 59.5 62.1 +10.3 64.8 +13.0 67.1 +15.3\nEMDR2 50 51.5 59.5 53.2 62.6 64.5 +11.3 68.4 +15.2 73.1 +19.9\nR2-D2 25 52.4 59.0 52.8 61.4 63.8 +11.0 68.4 +15.6 71.4 +18.6\nTable 1: Accuracy of several open-domain QA models on a randomly sampled subset of 301 questions from\nNQ-OPEN using lexical matching and the three evaluation mechanisms along with the reported results of these\nmodels on the entire dataset. K refers to the number of passages fed to a model to generate an answer. InstructGPT\n(zero-shot) and InstructGPT (few-shot) achieve the highest raise in accuracy across all three additional evaluation\nmethods. Only under human assessment does InstructGPT (few shot) outperform all other models.\n12.6\n63.5\n33.9\n59.5\n45.9\n52.5\n46.5\n60.8\n47.5\n58.1\n48.2\n59.5\n49.8\n62.5\n50.8\n63.1\n50.8\n65.8\n51.8\n62.1\n52.8\n63.8\n53.2\n64.5\nInstructGPT (zero-shot)InstructGPT (few-shot)DPRContriever & FiDFiD ANCE+ & FiDRocketQA\nv2 & FiD\nGAR+ & FiDFiD-KDEviGenR2-D2EMDR2\n10\n20\n30\n40\n50\n60\n70\n80\n(a) BEM\n12.6\n77.1\n33.9\n67.8\n45.9\n55.1\n46.5\n63.1\n47.5\n61.5\n48.2\n63.1\n49.8\n66.1\n50.8\n67.1\n50.8\n70.4\n51.8\n64.8\n52.8\n68.4\n53.2\n68.4\nInstructGPT (zero-shot)InstructGPT (few-shot)DPRContriever & FiDFiD ANCE+ & FiDRocketQA\nv2 & FiD\nGAR+ & FiDFiD-KDEviGenR2-D2EMDR2\n10\n20\n30\n40\n50\n60\n70\n80 (b) InstructGPT-eval\n12.6\n71.4\n33.9\n75.8\n45.9\n58.8\n46.5\n66.5\n47.8\n64.8\n48.2\n65.8\n49.8\n70.1\n50.8\n69.4\n50.8\n73.1\n51.8\n67.1\n52.8\n71.4\n53.2\n73.1\nInstructGPT (zero-shot)InstructGPT (few-shot)DPRContriever & FiDFiD ANCE+ & FiDRocketQA\nv2 & FiD\nGAR+ & FiDFiD-KDEviGenR2-D2EMDR2\n10\n20\n30\n40\n50\n60\n70\n80 (c) Human\nFigure 2: Accuracy of 12 open-domain QA models on the NQ- OPEN subset of 301 questions using EM (purple\npoints) and the three evaluation mechanisms (green points). For LLMs, the ranking of models under BEM and\nInstructGPT-eval is not consistent with human evaluation, while the rest of the models are ranked similarly under\nthe two evaluation method. InstructGPT (few shot) outperforms other models only under human assessment.\nthree evaluation mechanisms, i.e., 16%, 21%, and\n24% on average for BEM, InstructGPT-eval, and\nHuman, respectively. InstructGPT (zero-shot) and\nInstructGPT (few-shot) are the top 2 models with\nthe highest raise in accuracy across the evaluation\nmechanisms, whereas the amended result of DPR\nachieves the lowest increase. Moreover, the accu-\nracy reported using BEM and InstructGPT-eval are\nyet lower than that of human judgment, i.e., trailing\n7.6% and 2.9% on average across all open-domain\nQA models, respectively.\nMore importantly, the ranking of models is read-\njusted by applying the three evaluation mecha-\nnisms. Figure 2 visualizes the accuracy of the\nopen-domain QA models before (using only EM)\nand after our evaluation. EMDR 2, originally the\nbest performing model, loses the top spot to In-\nstructGPT (few-shot) by a nearly +3% margin us-\ning human evaluation. BEM picks FiD-KD as the\nbest model, whereas the LLM-based evaluation\nmethod estimates the highest accuracy for Instruct-\nGPT (zero-shot). Also, the Kendall’s τ correlation\nof InstructGPT-eval, and BEM with human evalu-\nation is 0.75, and 0.70, respectively, whereas EM\nand F1 show a significantly weaker correlation of\n0.23 and 0.37.\nIn contrast to human evaluation, BEM and\nInstructGPT-eval show that InstructGPT (zero-\nshot) has 4%, and 9% advantage, respectively, over\nInstructGPT (few-shot). To further investigate this\nphenomenon, we manually examine the Instruct-\nGPT (zero-shot) generated answers that are deemed\nincorrect by humans. We identify 47 unattributable\nanswers out of 86 answers. The generated an-\nswers of InstructGPT (zero-shot) tend to be long\nstatements that offer supplementary information,\nwhich raises the risk of containing hallucinated con-\ntent. InstructGPT-eval accepts 30 of those answers\n5595\n(∼10% error over the 301 questions), whereas\nBEM incorrectly predicts 18 (∼6% error) answers\nas correct. Interestingly, GPT4-eval performs bet-\nter and misidentifies only 9 cases (∼3% error). Yet,\nthese results highlight that the automated methods\nare prone to misjudging hallucinated long answers,\nessentially rendering them unreliable against an-\nswers generated by LLMs.\n5 Linguistic Analysis of Correct Answers\nIn this section, we aim to examine model answers\nthat are not considered correct based on EM, but\nare in fact acceptable according to our assessment.\nMin et al. (2021) conducted a similar analysis on\n50 questions for the participating models in the\nEfficientQA competition at NeurIPS 2020. In line\nwith this work, we provide an in-depth analysis on\na broader scale using more recent models to em-\nphasize the drawbacks of widely used lexical-based\nevaluation metrics and semantic similarity meth-\nods. We further dissect the categories presented by\nMin et al. (2021) into more detailed sub-categories.\nSpecifically, we group the 493 question/answer\npairs that are deemed correct by humans while can-\nnot be matched with gold answers into hierarchical\ncategories as follows:6\nSemantic Equivalence: Model predictions and\ngold answers convey the same meaning while not\nmatching verbatim:\n(i) Multinominal entities, e.g., “Bhimrao Ramji\nAmbedkar” and “B. R. Ambedkar.”\n(ii) Synonymous answers, e.g., “a virtual reality\nsimulator” and “a virtual reality world.”\n(iii) More elaborate answers , e.g., “ Typically ,\nno” and “not required in all jurisdictions.”\n(iv) Exact-Match in explanatory answers, e.g.,\n“1995” and “Michael Jordan returned to the\nNBA in 1995.”\n(v) Bridging/Abridging, e.g., “citizens” vs. “or-\ndinary citizens” or “in the Gospel of Luke” vs.\n“Gospel of Luke.”\n(vi) Tokenization mismatches, especially in the\npresence of punctuation marks, e.g., “s-block”\nand “s - block.”\n6Long answers, generated by LLMs, are annotated based\nsolely on the parts that candidate answers are mentioned.\nSymbolic Equivalence: In case of numeric an-\nswers, gold answers and predicted ones can be sym-\nbolically identical either exactly or approximately\nwhile their surface text differs, e.g., “ about 3.99\ndegrees” vs. “3.97 degrees” or the year “1524” vs.\n“the 16th century.”\nIntrinsic Ambiguity in Questions: Ambiguous\nquestions have several interpretations, each of\nwhich can lead to different answers. Min et al.\n(2020) found that ambiguity is prevalent in NQ-\nOPEN . Unlike other categories, mismatches that\nstem from ambiguity are not rooted in answers and\ninstead, arise from questions themselves. For in-\nstance, “ when does the next episode of iZombie\nair?” presupposes a reference point in time that\ncan only be clarified within a context. Thus, both\n“May 07, 2018” and “February 26, 2018” are cor-\nrect, depending on when the question is asked.\nGranularity Discrepancies: Predicted answers\nmay appear at different granularity levels than the\ngold answers. This case often arises for answers\nindicating spatial or temporal references. Indeed,\nunder different presuppositions, some granularity\nlevels are more preferable than others. Nonetheless,\nboth predictions and gold answers are valid. We\nfurther categorize this discrepancy into:\n(i) Temporal granularity discrepancy , e.g.,\n“when was the 50th star added to the united\nstates flag?” can be answered by both “1960”\nand “July 4, 1960.”\n(ii) Spatial granularity discrepancy, e.g., both\n“Camping World Stadium ” and “ Orlando,\nFlorida” answer the question “ where is the\ncitrus bowl held this year?”\nList-style Questions: Actual answers to these\nkinds of questions encompass a set of plausible\nanswers that is not fully specified in gold answers.\nFor these questions, model answers are deemed\ncorrect if they are among at least one gold answer.\nWe broke this group down into:\n(i) List questions, e.g., gold answers to “list of\nstrict nature reserve in the Philippines” con-\nsist of six locations that is by no means com-\nprehensive.\n(ii) Open-ended questions such as “what is an\nexample of a government monopoly in the\nUnited States? ” where “ the United States\n5596\nFigure 3: Statistics of exact-match failure modes deter-\nmined via our linguistic analysis\nPostal Service,” not listed among gold an-\nswers, is a correct answer.\n(iii) Compound questions ask about multiple\npieces of information in one question. They\nare a special case of multi-hop questions\n(Yang et al., 2018), e.g., “when was the cana-\ndian pacific railway started and finished? ”\nwhere the gold answer is “between 1881 and\n1885” vs. “ Started in 1881 and finished in\n1885.” that is a correct answer.\nIncorrect Gold Answers: Models produce cor-\nrect answers, but gold annotations are incorrect.\nMismatches in this category are a byproduct of\ndata quality issues. For example, the answer to\n“what is the largest ethnic group in Mexico today?”\nis annotated “K’iche’”, whereas the correct answer\nis “Mestizos.”\n5.1 Discussion\nThe statistics for each category are presented in\nFigure 3. Semantic equivalence (50.3%) is the\nmost common failure mode of exact matching. The\nmost frequent subcategories within this category\nare bridging/abridging (11.4%), EM in explanatory\nanswers (10.1%), and multinominal entities (9.3%).\nOther top frequent failure modes are list-style ques-\ntions (20.6%) and granularity discrepancy (15.0%).\nInterestingly, most of these failure cases are related\nto syntactical variations of answers, which is why\nspecifying gold answers via regular expressions can\nSemantic Eq.Symbolic Eq.\nGranularity Discr.Ambiguous Q.\nList Q.\nIncorrect Gold\n0\n10\n20\n30\n40\n50\nGPT4-eval InstructGPT-eval BEM EM\nFigure 4: Percentage of high-level failure modes for\neach evaluation method on NQ-OPEN .\nbe useful in capturing these variations. Moreover,\n14% of EM failures are attributed to data quality\nissues, i.e., ambiguity and incorrect gold answers.\nError Analysis of Automated Evaluation Meth-\nods. The answers that InstructGPT-eval and BEM\nreject but humans consider correct are a subset of\nEM failures.7 More precisely, InstructGPT-eval\nand BEM reduce the 493 failure cases of EM to 149\n(70% ↓) and 217 (56% ↓), respectively. For GPT4-\neval, the number of failure cases is 137 (72% ↓),\nonly slightly lower than InstructGPT-eval. The\nbreakdown of the high-level failure categories for\neach evaluation method is shown in Figure 4. The\nthree automated evaluation methods are able to\nfix most of the failures corresponding to semantic\nequivalence, granularity discrepancy, and symbolic\nequivalence. However, they do not perform that\nwell on list-style questions where InstructGPT-eval\nand GPT4-eval still fail on more than 10% of the\nEM failures, and BEM falls short on 14%. They\nalso perform nearly on par with EM on data quality-\nrelated failure cases, i.e., incorrect gold answers\nand ambiguous questions.\n6 Regex Matching on CuratedTREC\nAn alternative to lexical matching between gold\nanswers and predicted answers during evaluation\nis to specify gold answers as regular expression\npatterns. Regex matching allows for capturing syn-\ntactical answer variations where exact-match falls\nshort. In this section, our main goal is to highlight\n7With only 3 exceptions: InstructGPT-eval rejects only\n2 actually correct answers matching with gold answers that\ncorrespond to list questions where candidate answers appear\nin the middle of the gold answers. Moving the candidate\nanswer to the top of the gold answer list would fix the issue.\nSimilarly, BEM rejects only 1 exactly matched correct answer,\ni.e., “P-A-D-A-W-A-N.” while the gold answer is “Padawan”.\n5597\n39.2\n44.8 47.347.3\n52.3\n57.2\n54.054.0 54.3\n59.9\n57.9\n63.5\n61.3\n66.7\n62.462.4\n71.0\n83.3\n73.4\n80.0 88.188.1\nDPRuwmtB3RocketQA\nv2 & FiD\nexactanswerFiD Contriever & FiDANCE+ & FiDpris2002InstructGPT (zero-shot)InstructGPT (few-shot)LCCmain2002\n40\n50\n60\n70\n80\n90\n(a) BEM\n39.2\n43.2 47.347.3\n52.3\n55.4\n54.054.0 54.3\n59.0\n57.9\n61.7\n61.3\n65.1\n62.462.4\n71.0\n86.5\n73.4\n83.3 88.188.1\nDPRuwmtB3RocketQA\nv2 & FiD\nexactanswerFiD Contriever & FiDANCE+ & FiDpris2002InstructGPT (zero-shot)InstructGPT (few-shot)LCCmain2002\n40\n50\n60\n70\n80\n90 (b) InstructGPT-eval\n39.2\n44.8 47.347.3\n52.3\n58.1\n54.054.0 54.3\n60.8\n57.9\n64.0\n61.3\n68.0\n62.462.4\n71.0\n91.0\n73.4\n92.0\n88.188.1\nDPRuwmtB3RocketQA\nv2 & FiD\nexactanswerFiD Contriever & FiDANCE+ & FiDpris2002InstructGPT (zero-shot)InstructGPT (few-shot)LCCmain2002\n40\n50\n60\n70\n80\n90 (c) Human\nFigure 5: Accuracy of several open-domain QA models on CuratedTREC 2002, computed via regex matching,\nalong with the results of three evaluation mechanisms. Purple points represent the EM accuracy, and green points\ndepict accuracy achieved via BEM, InstructGPT-eval, and human judgment. Classic statistical models from TREC\nQA 2002 are shown as orange stars. InstructGPT (few shot) outperforms the best of these classic models only under\nhuman assessment.\nthe advantages and pitfalls of using answer patterns\nin QA evaluation by comparing its results with our\nthree evaluation mechanisms, described in §3.1.\nDataset. We make a comparison across open-\ndomain QA models on CuratedTREC 2002 (Baudiš\nand Šediv`y, 2015), a dataset whose gold answers\nare specified by regular expressions. The questions\nin CuratedTREC are derived from the dataset in\nthe QA tracks (V oorhees, 2003) of TREC 2001\nto 2003 after a manual review to discard ambigu-\nous or outdated questions. The knowledge source\nfor TREC QA is originally English news text,\nnamely AQUAINT, from three news sources (AP,\nNYTimes, and Xinhua), dating back to the late 90s.\nHere, we opt for the original knowledge source to\nreplicate the same environment as TREC QA 2002\nso as to quantitatively gauge progress over two\ndecade by comparing recent models with the mod-\nels that took part in the QA track in 2002. This ex-\nperiment is an out-of-distribution test for the neural\nmodels to check whether they are actually capable\nof using the knowledge source to answer questions\nor they answer from memory because the old news\narticles is less likely to have appeared in the pre-\ntraining corpus. However, LLMs inevitably do not\nuse the knowledge source as they perform the task\nfrom their memory in a closed-book fashion. Cu-\nratedTREC 2002 consists of 444 questions whose\nanswers are looked up in the AQAUINT corpus,\ncomprising around 1M news articles. We follow\nKarpukhin et al. (2020) to split the articles into non-\noverlapping passages of 100 words, which amounts\nto over 4M passages in total.\nModels. Out of the 12 models, we keep the\nones that do not require further training on Cu-\nratedTREC 2002, leaving us with 7 models. These\nmodels produce 1872 unique answers on Curat-\nedTREC 2002. We also obtained the submitted run\nfiles of the participants in the TREC QA 2002 track\nfrom TREC organizers to compute their accuracy\non CuratedTREC 2002. We include top 4 teams\nas baselines: LCCmain2002 (88.1%; Pasca and\nHarabagiu 2001), pris2002 (62.4%), exactanswer\n(54.0%), and uwmtB3 (47.3%).\nSimilar to NQ-OPEN , we ask two annotators to\njudge 1872 question/answer pairs, followed by a\nthird annotator who evaluates the diverging cases.\nThe Fleiss’ Kappa score between the two anno-\ntators is 83.5%, i.e., 150 disagreements (8.0%),\nindicating an almost perfect agreement.\nThe results are shown in Figure 5. Interestingly,\nthe ranking of models via regex matching is left\nunchanged by all three evaluation mechanisms, ex-\ncept for InstructGPT (zero-shot) and InstructGPT\n(few-shot). Consistent with our observation on NQ-\nOPEN , both BEM and InstructGPT-eval assign a\nhigher accuracy to InstructGPT (zero-shot) over In-\nstructGPT (few-shot). However, in contrast to NQ-\nOPEN , they do not overestimate InstructGPT (zero-\nshot). Human evaluation shows that InstructGPT\n(few-shot), by scoring 92%, is the best performing\nmodel, analogous to NQ- OPEN . Among the non-\nLLM models, ANCE+ and Contriever consistently\nsurpass other models. Similar to EM, regex match-\ning is too rigid albeit to a lesser extent. In particular,\nthe accuracy is underestimated by 6.6%, 6.4%, and\n9.9% on average via BEM, InstructGPT-eval, and\nhuman evaluation, respectively.\nWe note that LCCmain2002, an original TREC\nrun, outperforms all models prior to our assessment.\nHuman evaluation highlights that both InstructGPT\n5598\nmodels are superior to LCCmain2002 by +1.9%\n(for zero-shot) and +2.9% (for few-shot). How-\never, BEM and InstructGPT-eval fail to reflect this\nresult. For other non-LLM models, ANCE+ and\nContriever surpass pris2002 via all three evaluation\nmethods (with the exception of Contriever using\nInstructGPT-eval). An interesting finding here is\nthat although neural open-domain QA models are\nrepeatedly proven to be powerful in accomplish-\ning state-of-the-art, LCCmain2002, a heavily engi-\nneered statistical method from 20 years ago, ruffles\ntheir feathers by a substantial margin of 20%. Only\nunder human judgment does the absolute perfor-\nmance of LLMs surpass LCCmain2002.\n7 Conclusion\nDespite the simplicity and ubiquity of lexical\nmatching as an evaluation metric in open-domain\nQA, it is unnecessarily rigid because plausible can-\ndidate answers are likely not to appear in the list\nof gold answers. This flaw has been long known,\nbut the efforts to circumvent it have been mostly\nartisanal. In this paper, we report a systematic\nstudy of lexical matching by manually judging an-\nswers generated by several prominent open-domain\nQA models. We found that LLMs achieve state-\nof-the-art on NQ-OPEN . The accuracy of models\nis severely underestimated, with most EM failure\ncases stemming from syntactical variations of an-\nswers. Moreover, a zero-shot prompting method\ncan be a reasonable substitute for human evalua-\ntion although it cannot detect unattributability in\nlong-form answers. Our insights and analysis in\nthis paper will hopefully underpin the development\nof solid evaluation techniques in open-domain QA.\nLimitations\nOur main focus in this work is limited to fac-\ntoid information-seeking questions that typically\nprompt short answers. However, lexical match-\ning is adopted by more complicated forms of QA\nthat require complex reasoning. More precisely,\nQA tasks such as multi-hop reasoning (Yang et al.,\n2018), discrete reasoning (Dua et al., 2019), and\ncausal relations (Lin et al., 2019) also warrant sim-\nilar systematic analysis as studied in this paper.\nAcknowledgements\nWe thank the anonymous reviewers for their con-\nstructive feedback.\nReferences\nAkari Asai, Matt Gardner, and Hannaneh Ha-\njishirzi. 2022. Evidentiality-guided generation for\nknowledge-intensive NLP tasks. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2226–2243,\nSeattle, United States. Association for Computational\nLinguistics.\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2020. Learning\nto retrieve reasoning paths over wikipedia graph for\nquestion answering. In International Conference on\nLearning Representations.\nPetr Baudiš and Jan Šediv `y. 2015. Modeling of the\nquestion answering task in the YodaQA system. In\nInternational Conference of the cross-language eval-\nuation Forum for European languages , CLEF’15,\npages 222–228. Springer-Verlag.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace\nHe, Connor Leahy, Kyle McDonell, Jason Phang,\nMichael Pieler, Usvsn Sai Prashanth, Shivanshu Puro-\nhit, Laria Reynolds, Jonathan Tow, Ben Wang, and\nSamuel Weinbach. 2022. GPT-NeoX-20B: An open-\nsource autoregressive language model. In Proceed-\nings of BigScience Episode #5 – Workshop on Chal-\nlenges & Perspectives in Creating Large Language\nModels, pages 95–136, virtual+Dublin. Association\nfor Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D. Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nJannis Bulian, Christian Buck, Wojciech Gajewski, Ben-\njamin Börschinger, and Tal Schuster. 2022. Tomayto,\ntomahto. beyond token-level answer equivalence for\nquestion answering evaluation. In Proceedings of the\n2022 Conference on Empirical Methods in Natural\nLanguage Processing, pages 291–305, Abu Dhabi,\nUnited Arab Emirates. Association for Computa-\ntional Linguistics.\nAnthony Chen, Gabriel Stanovsky, Sameer Singh, and\nMatt Gardner. 2019. Evaluating question answer-\ning evaluation. In Proceedings of the 2nd Workshop\non Machine Reading for Question Answering, pages\n119–124, Hong Kong, China. Association for Com-\nputational Linguistics.\n5599\nAnthony Chen, Gabriel Stanovsky, Sameer Singh, and\nMatt Gardner. 2020. MOCHA: A dataset for train-\ning and evaluating generative reading comprehension\nmetrics. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 6521–6532, Online. Association for\nComputational Linguistics.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1870–1879,\nVancouver, Canada. Association for Computational\nLinguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Mor-\neira, Rewon Child, Oleksandr Polozov, Katherine\nLee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta,\nMark Diaz, Orhan Firat, Michele Catasta, Jason Wei,\nKathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav\nPetrov, and Noah Fiedel. 2022. PaLM: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nChristopher Clark and Matt Gardner. 2018. Simple and\neffective multi-paragraph reading comprehension. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 845–855, Melbourne, Australia.\nAssociation for Computational Linguistics.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 2368–2378, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nNouha Dziri, Sivan Milton, Mo Yu, Osmar Zaiane, and\nSiva Reddy. 2022. On the origin of hallucinations\nin conversational models: Is it the datasets or the\nmodels? In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5271–5285, Seattle, United States.\nAssociation for Computational Linguistics.\nMartin Fajcik, Martin Docekal, Karel Ondrej, and Pavel\nSmrz. 2021. R2-D2: A modular baseline for open-\ndomain question answering. In Findings of the As-\nsociation for Computational Linguistics: EMNLP\n2021, pages 854–870, Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-\ntian Riedel, Piotr Bojanowski, Armand Joulin, and\nEdouard Grave. 2022. Unsupervised dense informa-\ntion retrieval with contrastive learning. Transactions\non Machine Learning Research.\nGautier Izacard and Edouard Grave. 2021a. Distilling\nknowledge from reader to retriever for question an-\nswering. In International Conference on Learning\nRepresentations.\nGautier Izacard and Edouard Grave. 2021b. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874–880, Online. Association for Computa-\ntional Linguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nOmar Khattab, Christopher Potts, and Matei Zaharia.\n2021. Relevance-guided supervision for OpenQA\nwith ColBERT. Transactions of the Association for\nComputational Linguistics, 9:929–944.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 6086–6096, Florence, Italy.\nAssociation for Computational Linguistics.\nKevin Lin, Oyvind Tafjord, Peter Clark, and Matt Gard-\nner. 2019. Reasoning over paragraph effects in situ-\nations. In Proceedings of the 2nd Workshop on Ma-\nchine Reading for Question Answering, pages 58–62,\nHong Kong, China. Association for Computational\nLinguistics.\nYuning Mao, Pengcheng He, Xiaodong Liu, Yelong\nShen, Jianfeng Gao, Jiawei Han, and Weizhu Chen.\n2021. Generation-augmented retrieval for open-\ndomain question answering. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 4089–4100, Online. As-\nsociation for Computational Linguistics.\n5600\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1906–1919, On-\nline. Association for Computational Linguistics.\nSewon Min, Jordan Boyd-Graber, Chris Alberti,\nDanqi Chen, Eunsol Choi, Michael Collins, Kelvin\nGuu, Hannaneh Hajishirzi, Kenton Lee, Jenni-\nmaria Palomaki, Colin Raffel, Adam Roberts, Tom\nKwiatkowski, Patrick Lewis, Yuxiang Wu, Hein-\nrich Küttler, Linqing Liu, Pasquale Minervini, Pon-\ntus Stenetorp, Sebastian Riedel, Sohee Yang, Min-\njoon Seo, Gautier Izacard, Fabio Petroni, Lucas Hos-\nseini, Nicola De Cao, Edouard Grave, Ikuya Ya-\nmada, Sonse Shimaoka, Masatoshi Suzuki, Shumpei\nMiyawaki, Shun Sato, Ryo Takahashi, Jun Suzuki,\nMartin Fajcik, Martin Docekal, Karel Ondrej, Pavel\nSmrz, Hao Cheng, Yelong Shen, Xiaodong Liu,\nPengcheng He, Weizhu Chen, Jianfeng Gao, Bar-\nlas Oguz, Xilun Chen, Vladimir Karpukhin, Stan\nPeshterliev, Dmytro Okhonko, Michael Schlichtkrull,\nSonal Gupta, Yashar Mehdad, and Wen-tau Yih. 2021.\nNeurIPS 2020 EfficientQA competition: Systems,\nanalyses and lessons learned. volume 133 of Pro-\nceedings of Machine Learning Research, pages 86–\n111. PMLR.\nSewon Min, Julian Michael, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2020. AmbigQA: Answering am-\nbiguous open-domain questions. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 5783–\n5797, Online. Association for Computational Lin-\nguistics.\nOpenAI. 2023. GPT-4 technical report. Technical re-\nport.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Gray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems, pages 27730–27744. Curran As-\nsociates, Inc.\nMarius A. Pasca and Sandra M. Harabagiu. 2001. High\nperformance question/answering. In Proceedings of\nthe 24th Annual International ACM SIGIR Confer-\nence on Research and Development in Information\nRetrieval, SIGIR ’01, page 366–374, New York, NY ,\nUSA. Association for Computing Machinery.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,\nMichael Collins, Dipanjan Das, Slav Petrov, Gau-\nrav Singh Tomar, Iulia Turc, and David Reitter. 2021.\nMeasuring attribution in natural language generation\nmodels. arXiv preprint arXiv:2112.12870.\nRuiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao,\nQiaoQiao She, Hua Wu, Haifeng Wang, and Ji-Rong\nWen. 2021. RocketQAv2: A joint training method\nfor dense passage retrieval and passage re-ranking.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2825–2835, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nJulian Risch, Timo Möller, Julian Gutsch, and Malte\nPietsch. 2021. Semantic answer similarity for evalu-\nating question answering models. In Proceedings of\nthe 3rd Workshop on Machine Reading for Question\nAnswering, pages 149–157, Punta Cana, Dominican\nRepublic. Association for Computational Linguistics.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nAnna Rogers, Matt Gardner, and Isabelle Augenstein.\n2022. QA dataset explosion: A taxonomy of NLP\nresources for question answering and reading com-\nprehension. ACM Computing Surveys, 55(10):1–45.\nChenglei Si, Chen Zhao, and Jordan Boyd-Graber. 2021.\nWhat’s in a name? answer equivalence for open-\ndomain question answering. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 9623–9629, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nDevendra Singh, Siva Reddy, Will Hamilton, Chris\nDyer, and Dani Yogatama. 2021. End-to-end train-\ning of multi-document reader and retriever for open-\ndomain question answering. In Advances in Neural\nInformation Processing Systems, volume 34, pages\n25968–25981.\nEllen M. V oorhees. 2003. Overview of the TREC 2002\nquestion answering track. In TREC.\nEllen M. V oorhees and Dawn M. Tice. 2000. The TREC-\n8 question answering track. In Proceedings of the\nSecond International Conference on Language Re-\nsources and Evaluation (LREC’00), Athens, Greece.\nEuropean Language Resources Association (ELRA).\n5601\nShuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo\nWang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry\nTesauro, Bowen Zhou, and Jing Jiang. 2018. Rˆ3:\nReinforced ranker-reader for open-domain question\nanswering. In Proceedings of the AAAI Conference\non Artificial Intelligence.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\nArnold Overwijk. 2021. Approximate nearest neigh-\nbor negative contrastive learning for dense text re-\ntrieval. In International Conference on Learning\nRepresentations.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369–2380, Brussels, Belgium. Association for Com-\nputational Linguistics.\nXi Ye and Greg Durrett. 2022. The unreliability of\nexplanations in few-shot prompting for textual rea-\nsoning. In Advances in Neural Information Process-\ning Systems, volume 35, pages 30378–30392. Curran\nAssociates, Inc.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. OPT: Open pre-\ntrained transformer language models. arXiv preprint\narXiv:2205.01068.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. BERTScore:\nEvaluating text generation with bert. In International\nConference on Learning Representations.\n5602\nA Zero-shot Evaluation using GPT-4\nFor the sake of completeness, we test the ability of\nGPT-4 (OpenAI, 2023) for evaluating QA models\nas explained in §4.2. We find that GPT4-eval re-\nsults aligns with the trends observed in InstructGPT-\neval, albeit displaying marginal improvements. Fol-\nlowing the Table 1 layout, Table 2 presents the\naccuracy of the open-domain QA models, com-\nputed using GPT4-eval in conjunction with lexical\nmatching, InstructGPT-eval, and human judgment\nas reference points. The accuracy of all models\nconsistently increases by an average of 20% using\nGPT4-eval, which is similar to the increase level ob-\nserved in InstructGPT-eval. Moreover, analogous\nto InstructGPT-eval, the GPT4-eval accuracies are,\non average, 3.3% lower than those of human judg-\nment.\nFigure 6 visualizes the accuracy of the open-\ndomain QA models on NQ- OPEN using EM\nand GPT4-eval, similar to Figure 2. Unlike\nInstructGPT-eval, GPT4-eval estimates the high-\nest accuracy for FiD-KD, followed by InstructGPT\n(zero-shot), InstructGPT (few-shot), and EMDR2.\nAlso, the Kendall’sτ correlation of GPT4-eval with\nhuman judgment is 0.79, slightly higher than 0.75\nof InstructGPT-eval.\n12.6\n68.8\n33.9\n68.8\n45.9\n56.5\n46.5\n64.8\n47.5\n62.1\n48.2\n62.5\n49.8\n67.1\n50.8\n67.4\n50.8\n69.4\n51.8\n66.1\n52.8\n65.8\n53.2\n68.4\nInstructGPT (zero-shot)InstructGPT (few-shot)DPRContriever & FiDFiD ANCE+ & FiDRocketQA\nv2 & FiD\nGAR+ & FiDFiD-KDEviGenR2-D2EMDR2\n10\n20\n30\n40\n50\n60\n70\n80\nFigure 6: Accuracy of 12 open-domain QA models on\nthe NQ-OPEN subset of 301 questions using EM (purple\npoints) and GPT4-eval (green points).\nError Analysis: As illustrated in Figure 4,\nGPT4-eval errors closely resemble the errors found\nin InstructGPT-eval. However, for a small number\nof cases, GPT4-eval demonstrates unique erratic\nbehaviours. First, for 2 cases, the model exhibits\noverconfidence in its internal memory and disre-\ngards gold answers that can be simply matched\nusing EM. For example, GPT4-eval incorrectly\nrejects the candidate answer “ Jermaine Jackson”\n(that is also a gold answer) to the question “ Who\nsings Somebody’s Watching Me with Michael Jack-\nson?” We also observe the contradictory response\nof “No, the candidate is correct” for 2 candidate\nanswers that are correct, but are not included in the\ngold answers. Moreover, GPT4-eval incorrectly ab-\nstains from evaluating 2 candidate answers because\nit thinks more context is needed. For instance, it\nfalsely utters\n“I cannot determine if the candidate is\ncorrect, as there is not enough informa-\ntion provided about the show \"Fall\" and\nthe character Rose. Valene Kane is an\nactress, but without more context, it is\nunclear if she is related to this specific\nshow or character.”\nas a response to the question “Who is Rose in the\nFall season 2?” and the candidate answer “Rose is\na new character introduced in the second season\nof the show Fall. She is a mysterious woman who\nis connected to the supernatural events occurring\nin the town.” that is entirely fabricated.\nResults on CuratedTREC 2002: As shown in\nFigure 7, GPT4-eval follows closely InstructGPT-\neval on CuratedTREC 2002. Specifically, it indi-\ncates a higher accuracy for InstructGPT (zero-shot)\ncompared to InstructGPT (few-shot) and ranks LC-\nCmain2002 ahead of both InstructGPT models de-\nspite human evaluation suggesting otherwise.\n39.2\n43.9 47.347.3\n52.3\n57.4\n54.054.0 54.3\n60.4\n57.9\n62.8\n61.3\n66.7\n62.462.4\n71.0\n85.8\n73.4\n85.4 88.188.1\nDPRuwmtB3RocketQA\nv2 & FiD\nexactanswerFiD Contriever & FiDANCE+ & FiDpris2002InstructGPT (zero-shot)InstructGPT (few-shot)LCCmain2002\n40\n50\n60\n70\n80\n90\nFigure 7: Accuracy of several open-domain QA models\non CuratedTREC 2002, computed via regex matching\n(purple points), along with the results of GPT4-eval\n(green points), similar to Figure 5. Classic statistical\nmodels from TREC QA 2002 are shown as orange stars.\n5603\nModel Sampled (301) InstructGPT-eval GPT4-eval Human\nEM F 1 Acc ∆ Acc ∆ Acc ∆\nInstructGPT (zero-shot) 12.6 27.5 77.1 +64.5 68.8 +56.2 71.4 +58.8\nInstructGPT (few-shot) 33.9 50.5 67.8 +33.9 68.8 +34.9 75.8 +41.9\nDPR 45.9 52.3 55.1 +9.2 56.5 +10.6 58.8 +12.9\nFiD 47.8 55.5 61.5 +13.7 61.8 +14.0 64.8 +17.0\nANCE+ & FiD 48.2 55.9 63.1 +14.9 62.5 +14.3 65.8 +17.6\nRocketQAv2 & FiD 49.8 58.7 66.1 +16.3 67.1 +17.3 70.1 +20.3\nContriever & FiD 46.5 55.9 63.1 +16.6 64.8 +18.3 66.5 +20.0\nFiD-KD 51.2 61.6 70.4 +19.6 69.4 +18.6 73.1 +22.3\nGAR+ & FiD 50.8 59.7 67.1 +16.3 67.4 +16.6 69.4 +18.2\nEviGen 51.8 59.5 64.8 +13.0 66.1 +14.3 67.1 +15.3\nEMDR 2 53.2 62.6 68.4 +15.2 68.4 +15.2 73.1 +19.9\nR2-D2 52.8 61.4 68.4 +15.6 65.8 +13.0 71.4 +18.6\nTable 2: Accuracy of several open-domain QA models on a randomly sampled subset of 301 questions from\nNQ-OPEN using lexical matching, GPT4-eval, human evaluation. Only GPT4-eval results are new here. The\nrest of the results are already reported in Table 1 and copied here solely as a reference. GPT4-eval demonstrates\napproximately similar behaviour as InstructGPT-eval when ranking the models.\n5604\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nIn an unnumbered section after Section 7\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nOur ﬁndings are summarized in the abstract, Section 1 (Introduction), and Section 7 (Conclusion).\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nYes, we used Natural Questions-open (Section 3.2) and CuratedTREC 2002 (Section 6).\n□\u0017 B1. Did you cite the creators of artifacts you used?\nSection 3.2 and Section 6\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNatural Questions-open is a subset of Natural Questions that is licensed under Creative Commons\nAttribution 4.0 International (CC BY 4.0) license. CuratedTREC 2002 is a subset of TREC QA 2002\nwhose access is governed by TREC organizers. Both datasets are public and freely available for\nresearch purposes.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nSection 3.2 and Section 6\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nWe only worked with the test dataset; the details are stated in Section 3.2 and Section 6.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n5605\nC □\u0013 Did you run computational experiments?\nWe used publicly available code and pre-trained models from previous work to reproduce their results.\nWe didn’t train or ﬁne-tune any models ourselves.\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nNot applicable. Left blank.\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSections 3.1, 3.2 and 6\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 3\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nWe reproduced previous work, cited in Section 3.1. The metrics are described in Section 3.\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nWe, the authors, did the annotations, described in the paper.\n□\u0013 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nSection 4.3\n□\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nSection 4.3 (footnote): the authors were the annotators\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n5606"
}