{
  "title": "Evaluating General Vision-Language Models for Clinical Medicine",
  "url": "https://openalex.org/W4394797911",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2122463621",
      "name": "Yixing Jiang",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2995228781",
      "name": "Jesutofunmi A. Omiye",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A3092411595",
      "name": "Cyril Zakka",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2142940066",
      "name": "Michael Moor",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A3093988608",
      "name": "Haiwen Gui",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A4304211958",
      "name": "Shayan Alipour",
      "affiliations": [
        "Sapienza University of Rome"
      ]
    },
    {
      "id": "https://openalex.org/A2374195516",
      "name": "Seyed Shahabeddin Mousavi",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2300145783",
      "name": "Jonathan H Chen",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2222836187",
      "name": "Pranav Rajpurkar",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2141760147",
      "name": "Roxana Daneshjou",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2122463621",
      "name": "Yixing Jiang",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2995228781",
      "name": "Jesutofunmi A. Omiye",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A3092411595",
      "name": "Cyril Zakka",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2142940066",
      "name": "Michael Moor",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A3093988608",
      "name": "Haiwen Gui",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A4304211958",
      "name": "Shayan Alipour",
      "affiliations": [
        "Sapienza University of Rome"
      ]
    },
    {
      "id": "https://openalex.org/A2374195516",
      "name": "Seyed Shahabeddin Mousavi",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2300145783",
      "name": "Jonathan H Chen",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2222836187",
      "name": "Pranav Rajpurkar",
      "affiliations": [
        "Harvard University"
      ]
    },
    {
      "id": "https://openalex.org/A2141760147",
      "name": "Roxana Daneshjou",
      "affiliations": [
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6602430550",
    "https://openalex.org/W6612613465",
    "https://openalex.org/W6840334356",
    "https://openalex.org/W6841071471",
    "https://openalex.org/W4391301614",
    "https://openalex.org/W4365143687",
    "https://openalex.org/W4388998820",
    "https://openalex.org/W4290852327",
    "https://openalex.org/W4387775965",
    "https://openalex.org/W4388585663",
    "https://openalex.org/W4361289889",
    "https://openalex.org/W4389156617"
  ],
  "abstract": "Abstract Recently emerging large multimodal models (LMMs) utilize various types of data modalities, including text and visual inputs to generate outputs. The incorporation of LMMs into clinical medicine presents unique challenges, including accuracy, reliability, and clinical relevance. Here, we explore clinical applications of GPT-4V, an LMM that has been proposed for use in medicine, in gastroenterology, radiology, dermatology, and United States Medical Licensing Examination (USMLE) test questions. We used standardized robust datasets with thousands of endoscopy images, chest x-ray, and skin lesions to benchmark GPT-4V’s ability to predict diagnoses. To assess bias, we also explored GPT-4V’s ability to determine Fitzpatrick skin tones with dermatology images. We found that GPT-4V is limited in performance across all four domains, resulting in decreased performance compared to previously published baseline models. The macro-average precision, recall, and F1-score for gastroenterology were 11.2%, 9.1% and 6.8% respectively. For radiology, the best performing task of identifying cardiomegaly had precision, recall, and F1-score of 28%, 94%, and 43% respectively. In dermatology, GPT-4V had an overall top-1 and top-3 diagnostic accuracy of 6.2% and 21% respectively. There was a significant accuracy drop when predicting images of darker skin tones ( p&lt; 0.001 ). GPT-4V accurately identified Fitzpatrick skin tones for 56.5% of images. For the multiple-choice-styled USMLE image-based test questions, GPT-4V had an accuracy of 59%. Our findings demonstrate that the current version of GPT-4V is limited in its diagnostic abilities across multiple image-based medical specialties. Future work should be done to explore LMM’s sensitivity to prompting as well as hybrid models that can combine LMM’s capabilities with other robust models.",
  "full_text": "Evaluating General Vision-Language Models for\nClinical Medicine\nYixing Jiang1†, Jesutofunmi A. Omiye1†, Cyril Zakka1,\nMichael Moor1, Haiwen Gui1, Shayan Alipour2,\nSeyed Shahabeddin Mousavi1, Jonathan H. Chen1,\nPranav Rajpurkar3, Roxana Daneshjou1*\n1Stanford University, Stanford, CA, USA.\n2Sapienza University of Rome, Rome, Rome, Italy.\n3Harvard Medical School, Boston, MA, USA.\n*Corresponding author(s). E-mail(s): roxanad@stanford.edu;\n†These authors contributed equally to this work.\nAbstract\nRecently emerging large multimodal models (LMMs) utilize various types of\ndata modalities, including text and visual inputs to generate outputs. The incor-\nporation of LMMs into clinical medicine presents unique challenges, including\naccuracy, reliability, and clinical relevance. Here, we explore clinical applications\nof GPT-4V, an LMM that has been proposed for use in medicine, in gastroenterol-\nogy, radiology, dermatology, and United States Medical Licensing Examination\n(USMLE) test questions. We used standardized robust datasets with thousands of\nendoscopy images, chest x-ray, and skin lesions to benchmark GPT-4V’s ability to\npredict diagnoses. To assess bias, we also explored GPT-4V’s ability to determine\nFitzpatrick skin tones with dermatology images. We found that GPT-4V is lim-\nited in performance across all four domains, resulting in decreased performance\ncompared to previously-published baseline models. The macro-average precision,\nrecall, and F1-score for gastroenterology was 11.2%, 9.1% and 6.8% respectively.\nFor radiology, the best performing task of identifying cardiomegaly had precision,\nrecall, and F1-score of 28%, 94%, and 43% respectively. In dermatology, GPT-4V\nhad an overall top-1 and top-3 diagnostic accuracy of 6.2% and 21% respectively.\nThere was a significant accuracy drop when predicting on images of darker skin\ntones (p<0.001). GPT-4V accurately identified Fitzpatrick skin tones for 56.5%\nof images. For the multiple-choice styled USMLE image-based test questions,\nGPT-4V had an accuracy of 59%. Our findings demonstrate that the current ver-\nsion of GPT-4V is limited in its diagnostic abilities across multiple image-based\n1\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 14, 2024. ; https://doi.org/10.1101/2024.04.12.24305744doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nmedical specialties. Future work should be done to explore LMM’s sensitivity to\nprompting as well as hybrid models that can combine LMM’s capabilities with\nother robust models.\n1 Introduction\nLarge language models (LLMs) have recently demonstrated impressive capabilities in\ndiverse tasks related to language processing and generation [1–3]. These advancements\nhave had significant implications in the field of clinical medicine, as these models\nhave also been applied to cases such as guideline recommendations, patient encounter\nsummarization, and clinical note synthesis [4–7]. However, medicine is multi-modal,\nand images play a crucial role in medical decision-making. In response to this need,\nthe introduction of large multimodal models (LMMs) has extended the capabilities\nof existing models to include visual understanding, as demonstrated by platforms like\nGPT-4V(vision) (an enhanced GPT4 model with vision capabilities), LLaVA-Med,\nand Med-Flamingo [8–10].\nDespite these advancements, the integration of LMMs into clinical medicine, espe-\ncially in image-heavy specialties like radiology and dermatology, presents unique\nchallenges. These include considerations related to accuracy, reliability, and clinical\nrelevance. Moreover, the interpretability of model outputs and the ability to provide\nreasoning that aligns with clinical expectations remain critical for gaining the trust\nand utility of LMMs in medical practice.\nHere, we explore the clinical applications of a general-purpose multimodal model\nthat has been proposed for use in medicine: GPT-4V. We start our evaluation by\ngenerating clinical reports across three core domains including gastroenterology, radi-\nology, and dermatology. We also benchmark GPT-4V with standard robust datasets\nwith thousands of images and evaluate its ability to predict a diagnosis, differential\ndiagnoses, and Fitzpatrick skin tone. To assess bias, we analyze its robustness in mak-\ning predictions on various skin tones. Finally, we evaluate its role as a screening tool\nand compare its performance to that of medical experts.\nOur study is particularly focused on the implications of these technologies for\npatient care, ethical considerations in AI deployment, and the future landscape of\ngeneral-purpose AI-assisted medicine. As these models continue to evolve, their poten-\ntial integration into healthcare systems necessitates a multidisciplinary evaluation\napproach involving clinicians, data scientists, ethicists, and patients, ensuring that\nthese technologies augment medical practice without causing harm [11].\n2 Results\nHere we show the results of GPT-4V across four medical domains: Gastroenterology,\nRadiology, Dermatology, and the visual United States Medical Licensing Examination\n(USMLE) questions.\n2\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 14, 2024. ; https://doi.org/10.1101/2024.04.12.24305744doi: medRxiv preprint \n2.1 Gastroenterology\nGPT-4V’s performance on the Gastrovision test dataset reveals some important\ninsights into the limitations of LMMs in gastroenterology. Gastrovision is an endoscopy\nimage dataset that is meant to assess computer vision capabilities for detecting gas-\ntrointestinal diseases [12]. GPT4-V demonstrates a macro precision of 11.15%, macro\nrecall of 9.12%, and a macro F1 score of 6.81% (Table1), indicating a general challenge\nin accurately predicting across diverse conditions. The micro-level metrics, represent-\ning an aggregated performance across all classifications, stand at 20.30% for precision,\nrecall, and F1 score alike, suggesting modest predictive consistency despite the pres-\nence of class imbalances. Similarly, the Matthews Correlation Coefficient (MCC),\nwhich measures the overall quality of multi-class classification by taking into account\ntrue and false positives and negatives, was calculated to be 0.1478, a modest improve-\nment over random guessing. The previous best performing algorithm, Gastrovision\nDenseNet-121 demonstrated superior performance, with macro precision of 73.9%, and\nmacro recall of 62.3% [12].\nTable 1 Results for all classification experiments on the Gastrovision dataset.\nMethod Macro Average Micro Average MCC\nPrec. Recall F1 Prec. Recall F1\nGPT-4V 0.1115 0.0912 0.0681 0.2030 0.2030 0.2030 0.1478\nGastrovision DenseNet-121 [15] 0.7388 0.6231 0.6504 0.8203 0.8203 0.8203 0.7987\n2.2 Radiology\nIn general, GPT-4V does not perform well on this dataset. Table 2 shows the eval-\nuation results using CheXpert dataset, a large, publicly available dataset for chest\nradiograph interpretation [13]. Among the two largest classes, GPT-4V resulted in\na 0.56 sensitivity, 0.34 specificity, 0.24 precision, and 0.33 F-1 score for atelectasis\ndetection. For cardiomegaly identification, GPT-4V resulted in a 0.94 sensitivity, 0.15\nspecificity, 0.28 precision, and 0.43 F-1 score. Comparatively, the baseline CNN-based\nmodel reported by Irvin et. al achieved a ∼0.75 sensitivity at a ∼0.60 precision for\natelectasis detection and a ∼0.95 sensitivity with ∼0.50 precision for cardiomegaly\ndetection [13]. Radiologist performance on this dataset, as previously reported by Irvin\net. al revealed similar performances to the baseline model (0.89 sensitivity at 0.64\nprecision for atelectasis detection; 0.75 sensitivity at 0.80 precision for cardiomegaly\ndetection) [13].\n2.3 Dermatology\nIn this task, GPT-4V has an overall top-1 accuracy of 6.2% and top-3 accuracy of 21%\nwhen asked to make a diagnosis. Most of its predictions are likely to be malignancies\ncompared to the ground truth. For example, GPT-4V predicts melanoma in situ at a\nfrequency of 37.8% compared to the true frequency of 0.91%. Also, when evaluating\n3\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 14, 2024. ; https://doi.org/10.1101/2024.04.12.24305744doi: medRxiv preprint \nTable 2 Results of GPT-4V on the CheXpert dataset.\nDisease Sensitivity Specificity Precision F1-score Number of Positive Cases\nAtelectasis 0.56 0.34 0.24 0.33 178\nCardiomegaly 0.94 0.15 0.28 0.43 175\nConsolidation 0.14 0.92 0.09 0.11 35\nEdema 0.35 0.84 0.25 0.29 85\nPleural Effusion 0.92 0.16 0.19 0.32 120\nthe top predictions for GPT-4V, most are malignant dermatological conditions. Figure\n1 shows the direct comparisons of the top 5 diagnoses from the DDI dataset based\non the top-3 predictions of GPT-4V compared to the true labels. The distribution of\nthe top 5 predicted diagnoses across top-1 and top-3 results did not change except for\nmelanoma acral lentiginous, although accuracy significantly improved. GPT-4V was\nless likely to predict rare conditions on the DDI dataset which is shown in the other\ncategory section in Figure 1. We show the complete evaluation metrics of GPT-4V’s\ntop predictions in Table 3. Here, we use the top-3 predictions as a comprehensive\nevaluation of GPT-4V’s diagnostic ability.\nFig. 1 This shows the top 5 diagnoses from the DDI dataset and the top 5 predictions from GPT-\n4V. There is a higher representation of malignant conditions in GPT-4V predictions compared to the\nground truth. We utilize the top-3 predictions from GPT-4V for this figure.\nTable 3 Evaluation Metrics of GPT-4V on the DDI Dataset. Note: Sensitivity and Accuracy\nvalues were found to be identical for each disease.\nDisease Sensitivity Specificity Precision F1-score\nBasal cell carcinoma 0.83 0.35 0.08 0.14\nMelanoma in situ 0.50 0.52 0.01 0.02\nSquamous cell carcinoma in-situ (SCCIS) 0.68 0.63 0.08 0.14\nSquamous cell carcinoma 0.59 0.68 0.05 0.09\nMelanocytic nevi 0.39 0.75 0.26 0.31\n4\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 14, 2024. ; https://doi.org/10.1101/2024.04.12.24305744doi: medRxiv preprint \nFor Fitzpatrick skin tone (FST) prediction, GPT-4V only provided skin tones for\n603 images and reported there was not enough information for the remaining 53 images\nto provide a skin tone analysis. For the 603 images, GPT-4V had an accuracy of 56.5%\nfor predicting the skin tone. Table 4 shows the complete evaluation metrics of the three\nskin tone groups. We also show the confusion matrix of the prediction task in Figure 2.\nTable 4 Fitzpatrick Skin Tone (FST) Classification report from GPT-4V.\nClass Precision Recall F1-score Number of cases\n12.0 0.55 0.85 0.67 186\n34.0 0.41 0.49 0.44 226\n56.0 0.96 0.24 0.38 191\nFig. 2 This shows the confusion matrix for Fitzpatrick skin tone (FST) prediction. We see that\nGPT-4V misses many of the darker skin tones and performs worse in this group.\nWhen evaluating the robustness of GPT-4V’s predictions to skin tone, we found\ntop-1 accuracies of 5.28% (95% CI: 2.67% - 9.27%), 8.71% (95% CI: 5.47% - 13.01%),\nand 4.35% (95% CI: 2.01% - 8.09%) for FST I-II, III-IV, and V-VI groups respectively.\nTop-3 accuracies increased slightly to 16.82% (95% CI: 12.01% - 22.62%), 29.46%\n(95% CI: 23.78% - 35.65%), and 15.94% (95% CI: 11.24% - 21.65%). There was no\n5\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 14, 2024. ; https://doi.org/10.1101/2024.04.12.24305744doi: medRxiv preprint \nstatistically significant difference across the FST groups for the top-1 (0.13). However,\nthere was a significant difference for the top-3 predictions (3 .98 × 10−4).\nWhen comparing dermatologists to GPT-4V’s malignancy predictions, we found\nthat dermatologists had an accuracy of 67.99% (95% CI: 64.27% - 71.55%) compared\nto GPT-4V’s accuracy of 39.62% (95% CI: 35.41% - 43.95%). This difference was\nstatistically significant with a p-value of 3.93×10−21. However, GPT-4V outperformed\nin other metrics including a sensitivity of 0.86 compared to dermatologists’ 0.71. A\ncomplete evaluation of this comparison is shown in Table 5.\nTable 5 Dermatologists vs GPT-4V Predictions.\nType Sensitivity Specificity Precision F1-score\nDermatologist 0.71 0.67 0.43 0.54\nGPT-4V 0.86 0.23 0.28 0.42\n2.4 USMLE\nWe encountered six parse errors (GPT-4V refused to answer that question). Among\nthe remaining 612 questions, GPT-4V answers correctly for 361 questions, leading\nto an accuracy of 59.0%. Given this dataset was originally designed for open-ended\nanswering, there is no baseline performance in terms of accuracy reported.\n3 Methods\nWe run all the images to evaluate GPT-4V using a Python script through the Chat-\nGPT web interface. This was processed in Oct 2023 and a new session was used for\neach query. Prompts were predefined and specific to the image challenge. We also add\nchoices in the prompts to prevent unnecessary GPT-4V hallucinations and aid ease of\nanalysis. 95% confidence interval was generated using the Clopper-Pearson interval,\nand statistical significance was tested using the chi-square test and z-test statistic.\nSpecific methodologies are described in the respective image sections.\n3.1 Gastroenterology\n3.1.1 Dataset\nTo evaluate GPT-4V’s capabilities in gastroenterology, we make use of the test split\nfrom the Gastrovision [12] dataset comprising of 1587 endoscopic images acquired\nfrom Bærum Hospital in Norway and Karolinska University Hospital in Sweden. This\ndataset includes upper and lower GI tracts with three classification categories (normal\nfindings, anatomical landmarks, and pathological findings ), and spanning 27 distinct\nclasses. Ground truth labels are determined by a team of expert gastroenterologists,\none junior doctor, and two computational scientists.\n6\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 14, 2024. ; https://doi.org/10.1101/2024.04.12.24305744doi: medRxiv preprint \n3.1.2 Prompt\nThe following prompt is used as input to GPT-4V:\nYou are an AI doctor specializing in Gastroenterology. You are given a frame\nfrom a patient’s endoscopy procedure and a list of possible diagnosis choices.\nSelect only the single correct choice, and give the answer as a short response.\nIf none of the choices is correct, output none. Do not explain.\nChoices: Accessory tools, Angiectasia, Barrett’s esophagus, Blood in lumen,\nCecum, Colon diverticula, Colon polyps, Colorectal cancer, Duodenal bulb,\nDyed-lifted-polyps, Dyed-resection-margins, Erythema, Esophageal varices,\nEsophagitis, Gastric polyps, Gastroesophageal_junction_normal z-line,\nIleocecal valve, Mucosal inflammation large bowel, Normal esophagus,\nNormal mucosa and vascular pattern in the large bowel, Normal stomach,\nPylorus, Resected polyps, Resection margins, Retroflex rectum,\nSmall bowel terminal ileum, Ulcer.\n3.1.3 Evaluation metric\nWe use the standard multi-class classification metrics, such as Matthews Correlation\nCoefficient (MCC), micro and macro averages of recall/sensitivity, precision, and F1-\nscore, to validate the performance of GPT-4V on this dataset.\n3.2 Radiology\n3.2.1 Dataset\nWe use the CheXpert [13] dataset for evaluating GPT-4V’s capabilities on radiology\ndiagnosis. This is a publicly available dataset that contains 224,316 chest radiographs\nof 65,240 patients, labeled for the presence of 14 common chest radiographic observa-\ntions. We follow the setting from CheXpert competition, which limited classification\nto five diseases based on clinical relevance: (a) Atelectasis, (b) Cardiomegaly, (c) Con-\nsolidation, (d) Edema, and (e) Pleural Effusion. The test split of the CheXpert dataset\nis used, and it contains 668 images from 500 patients. The ground truth labels are\ndecided by the majority vote of five board-certified radiologists.\n3.2.2 Prompt\nThe following is the prompt used as the input to GPT-4V.\nYou are an AI doctor specializing in radiology. You are given the patient’s chest\nradiograph and a list of possible diagnosis choices. Select all the correct choice(s),\nand give the answer as a short response. If none of the choices is correct, output none.\nDo not explain.\nChoices: Atelectasis, Cardiomegaly, Consolidation, Edema, Pleural Effusion\n7\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 14, 2024. ; https://doi.org/10.1101/2024.04.12.24305744doi: medRxiv preprint \n3.2.3 Evaluation metric\nUnlike the CheXpert competition which computed receiver operator curves and\nprecision-recall curves, we elected to use sensitivity, specificity, precision and F1-score\nbecause the predictions from GPT-4V are binary.\n3.3 Dermatology\n3.3.1 Dataset\nFor evaluation of GPT-4V on the diagnosis of skin diseases, we utilize the Diverse Der-\nmatology Images (DDI) Dataset [14]. This is a publicly available histopathologically\nconfirmed dataset that is representative of diverse skin tones. DDI contains 656 clin-\nical images obtained from the Stanford Clinic and includes some rare dermatological\nconditions that have previously been described in literature [14].\nThe Fitzpatrick skin tone (FST) was carefully labeled based on in-person visit\ndocumentation, demographic photo, and image of the lesion. They are represented as\ngroups of two i.e. FST I-II (light skin tone), FST III-IV, and FST V-VI (dark skin\ntone). There are 208, 241, and 207 clinical images across the three groups respectively.\nThe choice of this dataset enables us to evaluate GPT-4V’s robustness to analyz-\ning medical images of various skin tones. Beyond the fine-grained diagnostic labels,\nDDI also contains a label for malignancy or not, which we utilize in our downstream\nanalysis.\n3.3.2 Prompt\nThe following is the prompt used as the input to GPT-4V while evaluating it on DDI:\nBased on this image, pretend you are a dermatologist and write me a report that\ndescribes the Fitzpatrick skin tone and your top 3 diagnoses in order of concern.\nChoices: melanoma-in-situ, mycosis-fungoides, squamous-cell-carcinoma-in-situ,\nbasal-cell-carcinoma, squamous-cell-carcinoma, melanoma-acral-lentiginous,\nbasal-cell-carcinoma-superficial, squamous-cell-carcinoma-keratoacanthoma,\nsubcutaneous-t-cell-lymphoma, melanocytic-nevi, seborrheic-keratosis-irritated,\nfocal-acral-hyperkeratosis, hyperpigmentation, lipoma, foreign-body-granuloma,\nblue-nevus, verruca-vulgaris, acrochordon, wart, epidermal-nevus,\nabrasions-ulcerations-and-physical-injuries, basal-cell-carcinoma-nodular,\nepidermal-cyst, acquired-digital-fibrokeratoma,\nseborrheic-keratosis, trichilemmoma, pyogenic-granuloma, neurofibroma,\nsyringocystadenoma-papilliferum, nevus-lipomatosus-superficialis, benign-keratosis,\ninverted-follicular-keratosis, onychomycosis, dermatofibroma, trichofolliculoma,\nlymphocytic-infiltrations, prurigo-nodularis, kaposi-sarcoma, scar, eccrine-poroma,\nangioleiomyoma, keloid, hematoma, metastatic-carcinoma, melanoma, angioma,\nfolliculitis, atypical-spindle-cell-nevus-of-reed, xanthogranuloma,\neczema-spongiotic-dermatitis, arteriovenous-hemangioma, acne-cystic,\nverruciform-xanthoma, molluscum-contagiosum, condyloma-accuminatum, morphea,\nneuroma, dysplastic-nevus, nodular-melanoma-(nm), actinic-keratosis,\npigmented-spindle-cell-nevus-of-reed, dermatomyositis, glomangioma,\n8\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 14, 2024. ; https://doi.org/10.1101/2024.04.12.24305744doi: medRxiv preprint \ncellular-neurothekeoma, fibrous-papule, graft-vs-host-disease, lichenoid-keratosis,\nreactive-lymphoid-hyperplasia, coccidioidomycosis, leukemia-cutis,\nsebaceous-carcinoma, chondroid-syringoma, tinea-pedis, solar-lentigo,\nclear-cell-acanthoma, abscess, blastic-plasmacytoid-dendritic-cell-neoplasm,\nacral-melanotic-macule.\nWe develop the above choices from the unique diagnoses present in the DDI dataset.\n3.3.3 Evaluation metric\nWe pre-process the GPT-4V reports to extract the top-3 diagnoses and Fitzpatrick\nskin tone. We further process inconsistencies with the skin tone groups for better\ncomparison with the ground truth. For example, FST’s that are provided as one of\nsingle numbers FST I-VI are converted into a skin tone group of either 12, 34, or 56.\nIn some cases where GPT-4V outputs a wrong group like 23 or 45, we transform it\nto 34 or 56. Our evaluation of the models’ performances was based on three prongs.\nFirst, we evaluate on clinical diagnosis based on just the image and prompt described\nabove. Then we proceed to evaluate its ability to diagnose FST. The last step of\nour evaluation was to compare GPT-4V’s diagnostic performance to board-certified\ndermatologists.\nClinical Diagnosis\nFor the diagnosis, we use the top-1 accuracy, alongside macro averages of sensitivity,\nprecision, and F1-score to validate the performance of GPT-4V. Since GPT-4V gives\na differential diagnosis, we also use the top-3 accuracy, macro averages of sensitivity,\nprecision, and F1-score to further validate GPT-4V. Furthermore, for the top-3 accu-\nracy, we evaluate if there are any performance changes across the three FST groups.\nFitzpatrick Skin Tone\nWe use the accuracy metric to evaluate GPT-4V’s performance in detecting the FST.\nAs stated above, we also stratify the diagnostic performance across FST.\nDermatologists vs GPT-4V\nHere, we focus on comparing an ensemble of three board-certified dermatologists’\nperformance with GPT-4V. We first evaluate on fine-grained diagnostic performance\nand proceed to malignancy detection. We utilize three dermatologists to generate\nproportions for our analysis. Since GPT-4V was not asked to detect malignancy in the\nprompt, we covert its differential diagnosis outputs into binary labels of malignancy\nor not. For the comparison, we use the top-3 predictions and threshold it at 0 .5 for\nboth dermatologists and GPT-4V. We use the accuracy, sensitivity, precision, and\nF1-scores for comparison. For diagnostic performance and malignancy detection, 95%\nconfidence intervals are generated by bootstrapping 1000 times, and p-values through\nthe chi-squared test.\n9\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 14, 2024. ; https://doi.org/10.1101/2024.04.12.24305744doi: medRxiv preprint \n3.4 USMLE\n3.4.1 Dataset\nWe use the Visual USMLE [10] dataset for evaluating GPT-4V’s capabilities on general\nmedicine question answering. The Visual USMLE dataset was created by adapting\nproblems from the Amboss platform (using licensed user access), and it contains 618\nquestions. There is one image associated with each question. Each question can have\nup to 10 answer choices, but only one of them is correct.\n3.4.2 Prompt\nThe following is the prompt used as the input to GPT-4V.\nYou are an AI doctor trained in general medicine. You are given a medical\nquestion and an image as context, together with a list of possible answer choices.\nOnly one of the choices is correct. Select the correct choice, and\ngive the answer as a short response.\nDo not explain.\nQuestion: {question}\nChoices: {choices}\n3.4.3 Evaluation metric\nGiven Visual USMLE questions are multi-choice questions, we use accuracy as the\nmetric. Besides accuracy, we also report the number of errors such as unable to parse\na single answer from GPT-4V’s response.\n4 Discussion\nOur study presents the largest comprehensive medical evaluation- to date- of GPT-\n4V, a large general-purpose multimodal model. Specifically, we evaluate across various\nmedical domains like gastroenterology, radiology, and dermatology using thousands\nof images across diverse datasets. This study builds on a recently emerging body of\nliterature evaluating GPT-4V for medical applications [15, 16]. Here, we present a\nrigorous approach by expanding the clinical domains, enhancing tested samples, and\nproviding direct comparisons to medical specialists. We also evaluate the robustness of\nGPT-4V on various skin tones and additionally experiment on a medical-specific VLM.\nOverall, our study provides valuable insights into the capabilities and limitations of a\ngeneral-purpose vision-language model (VLM) for medical applications.\nFor gastroenterology, GPT-4V is greatly limited in performance in accurately iden-\ntifying normal findings, anatomical landmarks, and pathological findings in endoscopic\nimages. When compared to previous baseline CNN-based models, namely DenseNet-\n121, GPT-4V had significant room for improvement. Uniquely compared to the\nGastrovision baseline, our study utilized all classes, including those with fewer than 25\nsamples. These sparse classes performed similarly to all other classes, which may sug-\ngest an avenue of exploration for rare diseases and images with one-shot or few-shot\nlearning.\n10\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 14, 2024. ; https://doi.org/10.1101/2024.04.12.24305744doi: medRxiv preprint \nIn the radiology domain, GPT-4V also does not achieve satisfactory performances\nin interpreting chest X-ray images. The baseline supervised model can achieve the\nbest AUC on Pleural Effusion (0.97), and the worst on Atelectasis (0.85), while the\nAUCs of all other observations are at least 0.9, as reported by Irvin et. al[13]. This\nindicates that the current version of GPT-4V might not be suitable for augmenting\nradiologists in their decision-making. Compared to GPT-4 which has been praised for\nits medical capabilities on textual data alone [17], GPT-4V significantly falls short of\nstate-of-the-art (SOTA) model performances for this task.\nFor dermatology, GPT-4V was able to generate a coherent comprehensive report\nbased on the images provided. However, it has a high propensity to predict malignant\nconditions, as evidenced by its top-3 predictions of basal cell carcinoma, melanoma\nin situ, squamous cell carcinoma in situ compared to DDI’s melanocytic nevi, sebor-\nrheic keratosis, and verucca vulgaris. This could be due to most of its training data\nbeing more likely to be malignant conditions or a tendency to be more cautious by\nprioritizing life-threatening diagnoses. However, the accuracy on this task is too low\nto warrant any real clinical or educational usage. In addition, we show that GPT-\n4V had the worst performance in identifying darker skin tones as shown in Table 4.\nA striking finding is the significant drop in accuracy when predicting on images of\ndarker skin tones (FST V-VI). This aligns with the literature of both special and gen-\neral purpose models performing worse on darker tones [14], and provides an avenue\nfor significant improvement. Furthermore, the comparison with board-certified der-\nmatologists reveals a nuanced picture of GPT-4V’s role in healthcare. In contrast to\nthe study by Buckley et. al [16], we show that human experts in the form of derma-\ntologists significantly outperform GPT-4V on accurately predicting malignancy with\np<0.05. However, GPT-4V had a higher sensitivity probably due its predilection for\nmalignancy. This makes a small case for GPT-4V as a possible screening tool, although\nmore robust analysis is required to evaluate its role for this purpose.\nOur study is not without limitations. First, we evaluate GPT-4V with a zero-shot\nprompting strategy and did not comprehensively evaluate its sensitivity to various\nprompting techniques. More advanced prompting techniques have been shown to sig-\nnificantly improve LLMs accuracy [18] and that could have affected the results of our\nexperiments. Second, as the GPT-4V model is closed, we are unaware of the data\nused in training and validating the model. However, our results show very poor per-\nformance across these domains making it less likely that these datasets were included\nin training the model. We provide all the generated reports used in our analysis in the\naccompanying supplementary materials.\nLooking forward, our study opens avenues for further research into evaluating\ngeneral-purpose multi-modal AI models for medical applications. Future work includes\nevaluating GPT-4V’s sensitivity to prompts and more robust evaluation of vari-\nous VLMs on diverse images. Additionally, exploring hybrid models that combine\nthe emerging multi-modal AI capabilities with the nuanced understanding of human\nexperts could yield more reliable and effective diagnostic tools in the future. Although,\nchallenges like accuracy and healthcare bias need to be resolved before the deployment\nof these models in medicine.\n11\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 14, 2024. ; https://doi.org/10.1101/2024.04.12.24305744doi: medRxiv preprint \nDeclarations\nCompeting interests\nR.D. has served as an advisor to MDAlgorithms and Revea and received consulting\nfees from Pfizer, L’Oreal, Frazier Healthcare Partners, and DWA, and research funding\nfrom UCB. All other authors declare no competing interests.\nEthics approval\nThis study did not require ethical approval.\nData availability\nThe data used in this study is available in the supplementary material.\nCode Availability\nThe code used in this study is available here: https://github.com/jyx-su/\nGPT4V-Automation.\nAuthors’ contributions\nY.J., J.A.O., C.Z., M.M., J.H.C., P.R., and R.D. were responsible for study conceptu-\nalization. Y.J., J.A.O., C.Z., M.M., H.G. S.A., and S.S.M. were responsible for data\ncollection, methodology, and other analyses. Y.J., J.A.O., C.Z., M.M., and H.G. wrote\nthe original draft. All authors reviewed and edited the manuscript. J.H.C., P.R., and\nR.D. supervised the study.\nReferences\n[1] Brown, T. B. et al. Language models are few-shot learners (2020). 2005.14165.\n[2] Wei, J. et al. Emergent abilities of large language models (2022). 2206.07682.\n[3] Kojima, T., Gu, S. S., Reid, M., Matsuo, Y. & Iwasawa, Y. Large language\nmodels are zero-shot reasoners (2022). 2205.11916.\n[4] Singhal, K. et al. Large language models encode clinical knowledge (2022). 2212.\n13138.\n[5] Zakka, C. et al. Almanac: Retrieval-augmented language models for clinical\nmedicine (2023). 2303.01229.\n[6] Ramesh, V., Chi, N. A. & Rajpurkar, P. Improving radiology report generation\nsystems by removing hallucinated references to non-existent priors (2022). 2210.\n06340.\n12\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 14, 2024. ; https://doi.org/10.1101/2024.04.12.24305744doi: medRxiv preprint \n[7] Omiye, J. A., Gui, H., Rezaei, S. J., Zou, J. & Daneshjou, R. Large language\nmodels in medicine: The potentials and pitfalls: A narrative review. Annals of\nInternal Medicine 177, 210–220 (2024).\n[8] Gpt-4v(ision) system card (2023). URL https://api.semanticscholar.org/\nCorpusID:263218031.\n[9] Li, C. et al. Llava-med: Training a large language-and-vision assistant for\nbiomedicine in one day. ArXiv abs/2306.00890 (2023). URL https://api.\nsemanticscholar.org/CorpusID:258999820.\n[10] Moor, M. et al. Med-flamingo: a multimodal medical few-shot learner , 353–367\n(PMLR, 2023).\n[11] Moor, M. et al. Foundation models for generalist medical artificial intelligence.\nNature 616, 259–265 (2023).\n[12] Jha, D. et al. Gastrovision: A multi-class endoscopy image dataset for computer\naided gastrointestinal disease detection (2023). 2307.08140.\n[13] Irvin, J. et al. Chexpert: A large chest radiograph dataset with uncertainty labels\nand expert comparison, Vol. 33, 590–597 (2019).\n[14] Daneshjou, R. et al. Disparities in dermatology ai performance on a diverse,\ncurated clinical image set. Science advances 8, eabq6147 (2022).\n[15] Wu, C. et al. Can gpt-4v (ision) serve medical applications? case studies on gpt-4v\nfor multimodal medical diagnosis. arXiv preprint arXiv:2310.09909 (2023).\n[16] Buckley, T., Diao, J. A., Rodman, A. & Manrai, A. K. Accuracy of a vision-\nlanguage model on challenging medical cases. arXiv preprint arXiv:2311.05591\n(2023).\n[17] Lee, P., Bubeck, S. & Petro, J. Benefits, limits, and risks of gpt-4 as an ai chatbot\nfor medicine. New England Journal of Medicine 388, 1233–1239 (2023).\n[18] Nori, H. et al. Can generalist foundation models outcompete special-purpose\ntuning? case study in medicine. arXiv preprint arXiv:2311.16452 (2023).\n13\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprintthis version posted April 14, 2024. ; https://doi.org/10.1101/2024.04.12.24305744doi: medRxiv preprint ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.40136629343032837
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34336990118026733
    },
    {
      "name": "Natural language processing",
      "score": 0.33835166692733765
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I861853513",
      "name": "Sapienza University of Rome",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I136199984",
      "name": "Harvard University",
      "country": "US"
    }
  ]
}