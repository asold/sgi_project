{
    "title": "Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering",
    "url": "https://openalex.org/W4389519321",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2116155728",
            "name": "Fangkai Yang",
            "affiliations": [
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2106563948",
            "name": "Pu Zhao",
            "affiliations": [
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2106126490",
            "name": "Wang Zezhong",
            "affiliations": [
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2099626634",
            "name": "Lu Wang",
            "affiliations": [
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A1985327412",
            "name": "Bo Qiao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2147025158",
            "name": "Jue Zhang",
            "affiliations": [
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2111148789",
            "name": "Mohit Garg",
            "affiliations": [
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2045251856",
            "name": "Qing-Wei Lin",
            "affiliations": [
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A4283110780",
            "name": "Saravan Rajmohan",
            "affiliations": [
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2097661864",
            "name": "Dongmei Zhang",
            "affiliations": [
                "Chinese University of Hong Kong"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4245015270",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W1484558966",
        "https://openalex.org/W3034671305",
        "https://openalex.org/W4309674289",
        "https://openalex.org/W3169841173",
        "https://openalex.org/W1964162497",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4221154592",
        "https://openalex.org/W4301243929",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W4312091890",
        "https://openalex.org/W4327564965",
        "https://openalex.org/W3099700870",
        "https://openalex.org/W4361866031",
        "https://openalex.org/W4378770815",
        "https://openalex.org/W72839301",
        "https://openalex.org/W2123301721",
        "https://openalex.org/W4252076394",
        "https://openalex.org/W4323717348",
        "https://openalex.org/W4205508242",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W4226101361",
        "https://openalex.org/W2970482702",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4318719006",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W2950681488",
        "https://openalex.org/W4309088836",
        "https://openalex.org/W4322757547",
        "https://openalex.org/W4360836968",
        "https://openalex.org/W1975466487",
        "https://openalex.org/W4362707064",
        "https://openalex.org/W4322718421",
        "https://openalex.org/W4283773758",
        "https://openalex.org/W4321177655",
        "https://openalex.org/W4361289889"
    ],
    "abstract": "Fangkai Yang, Pu Zhao, Zezhong Wang, Lu Wang, Bo Qiao, Jue Zhang, Mohit Garg, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track. 2023.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 294–312\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nEmpower Large Language Model to Perform Better\non Industrial Domain-Specific Question Answering\nFangkai Yang1 Pu Zhao1 Zezhong Wang2∗ Lu Wang1\nJue Zhang1 Mohit Garg1 Qingwei Lin1 Saravan Rajmohan1 Dongmei Zhang1\n1Microsoft\n2The Chinese University of Hong Kong\nAbstract\nLarge Language Model (LLM) has gained\npopularity and achieved remarkable results in\nopen-domain tasks, but its performance in real\nindustrial domain-specific scenarios is aver-\nage due to its lack of specific domain knowl-\nedge. This issue has attracted widespread at-\ntention, but there are few relevant benchmarks\navailable. In this paper, we provide a bench-\nmark Question Answering (QA) dataset named\nMSQA, centered around Microsoft products\nand IT technical problems encountered by cus-\ntomers. This dataset contains industry cloud-\nspecific QA knowledge, an area not exten-\nsively covered in general LLMs, making it\nwell-suited for evaluating methods aiming to\nenhance LLMs’ domain-specific capabilities.\nIn addition, we propose a new model inter-\naction paradigm that can empower LLM to\nachieve better performance on domain-specific\ntasks where it is not proficient. Extensive ex-\nperiments demonstrate that the approach fol-\nlowing our method outperforms the commonly\nused LLM with retrieval methods. We make\nour source code and sample data available at:\nhttps://aka.ms/Microsoft_QA.\n1 Introduction\nRecent advancements in large language models\n(LLMs), including OpenAI’s GPT-3.5 (Ouyang\net al., 2022), GPT-4 (OpenAI, 2023), Google’s\nPaLM (Chowdhery et al., 2022), and other bench-\nmark models (Touvron et al., 2023; Taori, 2023;\nTeam, 2023), have demonstrated impressive perfor-\nmance across various natural language processing\n(NLP) tasks. These models are pretrained on ex-\ntensive data, which imbues them with remarkable\nlanguage understanding and generation capabili-\nties (Bubeck et al., 2023). However, when it comes\nto domain-specific problems, LLMs exhibit limited\nperformance due to their insufficient pretraining\non domain knowledge, where the overwhelming\n∗ Work done during the internship at Microsoft.\npresence of domain-general data causes them to\nprioritize common knowledge, leading to a poten-\ntial oversight of crucial domain-specific informa-\ntion (Lee et al., 2023; Castro Nascimento and Pi-\nmentel, 2023; Lecler et al., 2023). Fine-tuning and\nmaintaining LLMs to incorporate domain-specific\nknowledge can be expensive for most companies\nand researchers. Moreover, the availability of\ndomain-specific data is often restricted and confi-\ndential, introducing risks of potential data leakage\nduring fine-tuning of LLMs (Mauran, 2023).\nThe existing works primarily focus on enhanc-\ning the performance of LLMs in specific domains\nby employing retrieval-based methods (Liu, 2022;\nShi et al., 2023; Peng et al., 2023a) or external\nmodules (Wu et al., 2023; Auto-GPT, 2023) to ex-\ntract domain-specific knowledge. However, these\napproaches suffer from certain limitations. Firstly,\nretrieval-based methods face challenges in handling\ncomplex queries as they may not retrieve all the nec-\nessary domain-specific context, leading to incom-\nplete information. Additionally, retrieved chunks\ncan suffer from the issue of “quote out of context”\nwhere the retrieved information may not fully cap-\nture the intended meaning (Engel, 1982). As the\nexample shown in Figure 1, retrieved chunks con-\ntain keywords or spans of the question, but they do\nnot understand the question, resulting in generating\ninaccurate answers. Secondly, due to the length\nlimitation of prompts, it becomes challenging to\nincorporate all retrieved data for in-context learn-\ning. This poses a constraint on these methods in\nleveraging domain-specific knowledge.\nHumans integrate domain-specific knowledge\nwith domain-general knowledge through interac-\ntions (Siegler, 1989; Penner and Klahr, 1996; Li\net al., 2014). For example, Penner et al. (Penner\nand Klahr, 1996) conducted an experiment where\nchildren inferred factors affecting the sinking rates\nof objects. Initially, children believed weight alone\ndetermined sinking, but the experiment helped\n294\nGrounded Answer:\nIf the machine is Hybrid AAD \njoined, a cloud only user won’t be \nable to join, as while you login to a \nHybrid AAD join machine, …\nQuestion:\nWe are planning to use hybrid\njoined devices (AD&AAD) as one \nstep to a cloud-only approach … \nCan a cloud-only user logon to a \nhybrid joined computer? …\n… Hybrid Azure AD joined \n|\\n\\nOn an Azure AD-joined or \nregistered device …\n… ### Hybrid service \naccounts\\n\\nSome hybrid …\n…\nA cloud-only account can't logon \nto a hybrid joined computer. You \nneed to have an on-premises AD \naccount to logon to a hybrid \njoined computer.\nRetrieved Chunks:\nOurs:\nResponse: \nYes, a cloud-only user can log \non to a hybrid joined computer. \nThere are some prerequisites \nthat need to be met …\nResponse:\nNo, a cloud-only user cannot \nlogon to a hybrid joined \ncomputer. To logon to a hybrid \njoined computer, you need to \nhave an on-premises AD \naccount. If your company plans \nto remove on-premises AD…\nFigure 1: An example1from MSQA dataset shows retrieval-based methods’ limitations in complex question handling.\nThe retrieved chunks contain matching keywords (highlighted in green) but failed to retrieve essential information\nneeded to answer the question correctly. Our model generates more accurate answers by understanding the question\nand leveraging domain-specific knowledge (highlighted in yellow). Case details are in Table 17 in Appendix I.\nthem understand the effects of object shape and\nmaterial on sinking rates. This domain-specific\nknowledge was extracted and learned through in-\nteractive experiences with various objects, rather\nthan being conveyed through formal, abstract rules.\nInspired by this, we introduce a novel model inter-\naction paradigm that bridges domain-general and\ndomain-specific knowledge. Our approach involves\nfine-tuning a smaller LLM, i.e., LLaMA (Touvron\net al., 2023), using domain documentation to align\nit with domain-specific knowledge. At runtime, our\nfine-tuned model provides domain-specific knowl-\nedge to LLMs. This paradigm replaces traditional\nretrieval modules with the generation of domain-\nspecific knowledge, enabling easy maintenance and\nprivacy protection within the specific domain.\nIn this paper, we focus on the cloud domain and\nspecifically address question-answering (QA) tasks\nusing our proposed model interaction paradigm.\nWhile LLMs have demonstrated their effectiveness\nin QA tasks, there is limited exploration and eval-\nuation of LLMs in domain-specific QA tasks in-\nvolving long-form answers. Our contributions are\nsummarized as follows:\n• We release a cloud-domain QA dataset that\ncontains 32k QA pairs from the Microsoft\nQ&A forum2. To the best of our knowledge,\nthis is the first cloud-domain QA dataset. We\nbelieve that this benchmarking dataset will\nassist the research community in evaluating\ntheir models in domain-specific scenarios.\n1The QA example details can be found inhttps://learn.\nmicrosoft.com/en-us/answers/questions/2096/\n2The data is collected and post-processed from the\nMicrosoft Q&A forum ( https://learn.microsoft.com/\nen-us/answers/questions/), which is publicly available.\n• We propose a new model interaction paradigm\nthat empowers the LLM with generated\ndomain-specific knowledge. Evaluation re-\nsults highlight the significant performance of\nour model interaction paradigm in generating\nanswers enriched with domain-specific knowl-\nedge, compared with retrieval-based methods.\n• We propose novel evaluation metrics for as-\nsessing long-form answers in QA tasks, which\nare aligned with human evaluations and have\nthe potential for automation evaluation.\n2 Related Work\n2.1 Question Answering Datasets\nQuestion answering (QA) (Hirschman and\nGaizauskas, 2001) aims to provide answers based\non knowledge or given context. Recent advance-\nments in LLMs have shown promising results\nin various QA datasets (Wang, 2022). However,\nexisting evaluations mainly focus on answer\ntypes like multiple-choice or span extraction,\nwhich are comparatively easier to assess LLM\nperformance. Evaluating long-form question\nanswering (LFQA) (Fan et al., 2019; Krishna et al.,\n2021; Nakano et al., 2021; Su et al., 2022) poses\nchallenges due to limited datasets and appropriate\nevaluation metrics. In particular, LLMs are often\nnot evaluated in specific domains, and available\ndomain-specific QA datasets, such as medical (Pal\net al., 2022; Jin et al., 2019), financial (Chen et al.,\n2021), and legal domains (Zheng et al., 2021),\ntypically include questions, answers, and relevant\nparagraphs. However, in practical QA scenarios,\nthis additional contextual information may not\nalways be available. Our paper addresses this by\n295\nreleasing an LFQA dataset specific to the cloud\ndomain, along with new evaluation metrics. Our\napproach eliminates the need for an additional\nparagraph to extract domain-specific knowledge,\nmaking it suitable for industrial applications while\nensuring data privacy.\n2.2 Augmented Large Language Models\nRecent efforts have been made to enhance the con-\ntext generation ability of LLMs in specific domains\nby incorporating external knowledge (Mialon et al.,\n2023). One group of approaches leverages exter-\nnal modules, such as Visual ChatGPT (Wu et al.,\n2023), HuggingGPT (Shen et al., 2023) and Auto-\nGPT (Auto-GPT, 2023). They highly rely on the\nLLM’s prompting management and the availability\nof external tools or applications. However, such\nexternal modules are not always available when\nit comes to domain-specific scenarios. Another\ngroup of approaches is retrieval-augmented (Liu,\n2022; Guu et al., 2020; Izacard et al., 2022; Shi\net al., 2023), which leverages retrieval-based meth-\nods like BM25 (Robertson et al., 2009) and dense\npassage retrieval (DPR) (Karpukhin et al., 2020).\nThis approach retrieves relevant data or text chunks,\nwhich are then used as additional context to in-\ncorporate domain-specific knowledge with LLMs,\nthus improving their performance. However, they\nmay not be able to handle complex questions\nthat require information from multiple sources or\nmodalities. Our method is able to comprehend com-\nplex questions and provide comprehensive domain-\nspecific knowledge without the “quote out of con-\ntext” issue.\n3 MSQA Dataset Creation\nCurrent public Q&A forums, such as Quora, Red-\ndit, Stack Overflow, contain responses to a variety\nof open-ended questions. However, there are lim-\nited Q&A forums dedicated to specific domains\nthat have a large number of active users. In light\nof this, we chose to focus on the publicly available\nMicrosoft Q&A forum 3 for our dataset creation,\nprimarily due to its extensive collection of avail-\nable questions and corresponding answers. These\ndomain-specific QAs cover a wide range of Mi-\ncrosoft technologies and products, such as Azure\nand Microsoft 365. Additionally, Microsoft of-\nfers publicly available and well-documented doc-\numentation, which serves as a valuable external\n3https://learn.microsoft.com/en-us/answers/\nresource for extracting domain-specific knowledge.\nWe make our MSQA dataset openly accessible to\nthe NLP community. We hope this resource could\nfacilitate the exploration of LLM’s capabilities in\nhandling industrial domain-specific questions.\n3.1 Data Collection and Post-Processing\nWe select questions and answers spanning from\nthe Microsoft Q&A forum from October 2019 to\nMay 2023. These QA pairs went through a filtering\nprocess based on user ratings. Firstly, we retain QA\npairs where the answers were marked as ‘Accepted’.\nSecondly, we exclude QA pairs involving multi-\nturn discussions, as they are outside the scope of\nthis paper. Additionally, we focus on text-based\nQA pairs and discard samples containing image\nattachments, leaving multi-modality QA tasks for\nfuture work. Furthermore, we gather metadata of\neach QA pair, including the number of up-votes\nreceived by both the question and answer, question\ntags, and other relevant information.\nThe QA pairs obtained through the aforemen-\ntioned collection process may contain noisy in-\nformation, particularly in human-written answers.\nThis noise stems from the inclusion of irrelevant\ndetails like user names, IDs, decoration symbols,\nand platform-generated context. They introduce\nunwanted noise during the fine-tuning process. To\nmitigate this, we conduct additional data post-\nprocessing, following a set of principles detailed in\nAppendix A.\n3.2 Statistics\nFollowing data post-processing, our dataset con-\nsists of 32k QA pairs. Table 1 summarizes the\nstatistics. Each question within the dataset is ac-\ncompanied by a diverse range of relevant topic\ntags, comprising a total of 332 distinct tags, such\nas Azure Virtual Machine, PowerPoint, Windows\nServer. These tags serve to categorize and provide\ncontextual information for the questions. To gain\na preliminary understanding of the different types\nof questions, we employ a categorization approach\nbased on the first interrogative words. The major-\nity of questions fall into the “Is” category, which\nseeks judgments (Is it possible to ...), while others\nrequire explanations from answers, such as “How”\nor “Why”. Interestingly, even “Is” questions often\nelicit explanatory answers. Table 8 in Appendix B\nshows randomly sampled examples of MSQA ques-\ntions based on their types.\n296\nQuestion Tag (%) 1st Question word (%)\nAzure 28.55 Is 19.18\nWindows 16.73 How 11.91\nM365 15.14 Why 10.75\nOther 39.58 Do 7.14\nAvg # of token Can 6.57\nQuestion 347.15 What 5.94\nAnswer 382.18 Other 38.33\nTable 1: Statistics of MSQA\n4 Methodology\nThe model interaction paradigm (shown in Fig-\nure 2) involves two key steps: (a) obtaining a\ndomain-specific model that incorporates aligned\nknowledge, (b) providing the generated domain-\nspecific knowledge to LLMs, enabling them to gen-\nerate answers enriched with domain knowledge.\nIn the first step, we pre-train small language mod-\nels4 using the publicly available Azure documenta-\ntion5. This documentation serves as a comprehen-\nsive knowledge base for Microsoft Azure, contain-\ning cloud-domain knowledge of Microsoft’s cloud\nproducts. Note that Microsoft maintains extensive\ndocumentation covering various product offerings.\nHowever, we specifically focus on Azure documen-\ntation as it aligns with the prevalent tags related to\nAzure found in the Q&A forum, which captures the\nmost frequently asked questions (shown in Table 1).\nBy narrowing down our focus to Azure, we aim\nto evaluate the efficacy of our model interaction\nparadigm within a well-defined domain.\nAfter completing the pre-training phase, we\nimbue the small language models with domain-\nspecific knowledge from the Azure knowledge base.\nWe then adapt the model to the LFQA task through\ninstruction tuning (Ouyang et al., 2022), allow-\ning it to specialize and become more accurate in\nthe QA task. To facilitate instruction tuning, we\nconstruct instructions from the training set of the\nMSQA dataset. Each instruction consists of a three-\nelement tuple, including an instruction prompt, an\ninput query or statement, and a corresponding re-\nsponse. The instruction template is shown in Ta-\nble 2 (Appendix C shows an example instruction).\nThe details of the training setup and parameters can\nbe found in Appendix D.\n4We use LLaMA-7B (Touvron et al., 2023) in this paper.\n5https://github.com/MicrosoftDocs/azure-docs\nLM Pre-trained LM Domain-specific LM\nDomain-specific \ndocumentation\n Instructions\nOffline\nStep 1: obtain the domain-specific model\nStep 2: LLM incorporates domain knowledge and generate answer\nDomain knowledge\nLLM\nAnswer\nOnline\nQuestion\nDomain-specific LM\nFigure 2: The model interaction framework.\nBelow is an instruction that describes a task. Write a\nresponse that appropriately completes the request.\nInstruction: Please answer the following questions\nconcerning {Tags}.\nInput: {Question}\nResponse: {Answer}\nTable 2: The instruction template.\nBy engaging in instruction tuning, the pre-\ntrained small LM learns and assimilates domain-\nspecific knowledge, enabling it to generate rele-\nvant responses when encountering domain-specific\nqueries or statements.\nIn the second step, the fine-tuned domain-\nspecific language model serves as an expert in\nAzure. During runtime, the domain-specific LM\nleverages its knowledge to provide domain-relevant\ninformation in response to a given question. Then\nthe LLM takes both the question and the domain\nknowledge to generate the final response. By en-\nriching the LLMs with domain-specific knowledge,\ntheir comprehension of the question context is en-\nhanced, resulting in more accurate and contextually\nappropriate responses. Note that our approach does\nnot propose replacing the LLM with a domain-\nspecific LM. Instead, we propose a model inter-\naction paradigm, leveraging the domain-specific\nLM as an expert to provide knowledge. Through\nour application practice, we have observed that\ndomain-specific knowledge may not excel in lan-\nguage expression and general question answering,\nas questions may contain both Azure-related and\ngeneral queries. Additionally, our domain-specific\nmodel can function as a compatible plugin within\nthe existing retrieval-based system, offering supple-\nmentary information beyond just chunks.\n297\n5 Experiment\n5.1 Baselines\nWe leveraged two LLMs, namely GPT-3.5\n(gpt-35-turbo) and GPT-4 (gpt-4), as the back-\nbone to output the answer by taking the extra in-\nformation from either the data-retrieval methods or\nour approach. We utilize two data retrieval meth-\nods, i.e., BM25 (Robertson et al., 2009) and dense\npassage retrieval (DPR) (Karpukhin et al., 2020).\nThese methods were employed to retrieve the top-3\nrelevant information chunks from Azure documen-\ntation, which were then used as supplementary in-\nformation for the backbone LLMs during answer\ngeneration. We make the below baselines:\nRaw LLM (LLM). Questions were directly posed\nto the backbone LLMs without providing any addi-\ntional information.\nLLM+BM25/+DPR. The LLM incorporated both\nthe question and retrieved chunks using BM25 and\nDPR, respectively.\nLLM+EXP. The LLM utilized the domain knowl-\nedge from our domain-specific LM as extra infor-\nmation to generate answers.\nAppendix E shows the baseline prompt details.\n5.2 Evaluation Metrics\nEvaluating long-form generated answers lacks an\nautomatic metric, and thus, we employ standard\nmetrics, our proposed metrics, and human evalua-\ntion to assess the quality of the generated answers.\nLexical-Overlap-Based Metrics . We em-\nploy BLEU (Papineni et al., 2002), ROUGE-1,\nROUGE-2, and ROUGE-L (Lin, 2004), and ME-\nTEOR (Banerjee and Lavie, 2005), as the lexical-\noverlap-based metrics to measure the N-gram\nalignment between the generated answers and the\ngrounded answers.\nSemantic-Overlap-Based Metrics. To evaluate\nthe semantic overlap between the generated an-\nswers and the ground truth, we utilize BERT-\nScore (Zhang et al., 2020) and report F1 score.\nAdditionally, we calculate the similarity between\nthe embedding of the grounded answer and the em-\nbedding of the generated answer, referred to as the\nSIM metric.\nBesides the above metrics, we propose\nthree novel metrics for the LFQA scenario:\nKeyword/Span-Hit-Rate (KHR). We extract key-\nwords or spans from the grounded answer, remov-\ning those presented in the question. This yields\na distinct keyword/span set, and we measure the\nrate of hits in the generated response (Table 12 in\nAppendix E shows the prompt).\nCan-Answer-Rate (CAR). To prevent answer hal-\nlucinations, we require the backbone LLMs to an-\nswer only when confident. CAR represents the\npercentage of answerable questions and evaluates\nthe informativeness of extra information provided\nby data-retrieval methods or our approach.\nLLM-based Metrics. LLMs have demonstrated\nimpressive performance as evaluators and annota-\ntors in recent studies (Wang et al., 2022; Chiang\net al., 2023; Peng et al., 2023b). In our work, we\nemploy an LLM as an evaluator to compare and\nrank two responses based on their similarity to the\ngrounded answer (see full prompt in Appendix F.1).\nHowever, concerns have been raised regarding the\nreliability of LLMs as evaluators due to their sen-\nsitivity to response positions (Wang et al., 2023).\nTo address this issue, we incorporate the chain-of-\nthought concept (Wei et al., 2022) in our prompt\ndesign, which involves providing detailed explana-\ntions before scoring the responses. Moreover, we\npropose a rule where we trust the LLM evaluator\nonly when the score gap exceeds 1 (excluding 1),\nallowing for a single round of scoring. Otherwise,\nwe conduct two scoring rounds, switching response\npositions, and rank them based on the average score\nof the two rounds. Note that GPT-4 exhibits signif-\nicantly fewer conflict cases compared to GPT-3.5,\nleading us to select GPT-4 as the evaluator. Fur-\nther details of the score gap study are available in\nAppendix F.2.\nHuman Evaluation. There still requires human\nevaluation as there is a lack of good metrics of\nlong-form answers (Fan et al., 2019; Krishna et al.,\n2021). We evaluate a small subset of test samples\n(30 randomly sampled QA pairs). Five evaluators\nwith domain knowledge are given QA pairs and\nthree responses from different methods. They are\nasked to rank these three responses based on their\nsimilarity with the grounded answer. The evalua-\ntion setup and user interface are in Appendix G.\n6 Results\nAs suggested in (Krishna et al., 2021; Ji et al., 2023)\nand our experiments, the lexical-overlap-based met-\nrics are not an informative way to evaluate the\nquality of LLM-generated answers due to their\npoor correlation with grounded human-written an-\nswers. As shown in Table 15 and 16 in Appendix H,\nthe lexical-overlap-based scores demonstrate fewer\n298\nMetrics (%)LLM LLM+BM25 LLM+DPR LLM+EXP\nBERT-Score52.47 53.83 54.94 56.21\nSIM 61.84 62.46 64.87 67.08\nKHR 22.53 23.25 24.30 24.61\nCAR 98.37 92.07 95.34 99.77\nTable 3: The results of semantic-overlap-based metrics\nover different methods with GPT-3.5 as backbone LLM.\nMetrics (%)LLM LLM+BM25 LLM+DPR LLM+EXP\nBERT-Score51.79 52.33 54.83 56.91\nSIM 67.94 68.30 68.78 71.19\nKHR 30.40 32.15 32.50 33.13\nCAR 76.22 73.89 87.41 99.30\nTable 4: The results of semantic-overlap-based metrics\nover different methods with GPT-4 as backbone LLM.\nvariations across different methods, and the scores\nare low in general, suggesting that these metrics\nare not suited.\nTable 3 and 4 show the results of semantic-\noverlap-based metrics, i.e. BERT-Score and SIM,\nwith GPT-3.5 and GPT-4 serving as the back-\nbone LLMs for answer generation, respectively.\nThe worst performance is observed for Raw LLM,\nhighlighting the usefulness of extra information\nprovided through data-retrieval methods or our\nmethod. LLM+DPR has better performance than\nLLM+BM25, and our LLM+EXP achieves the best\nperformance. Note that the difference between\nRaw LLM and other baselines is relatively small,\npossibly due to the pre-training of LLMs, which\nalready contains some knowledge related to Mi-\ncrosoft Azure. Our KHR metric has a similar\npattern as the lexical-overlap-based metric. How-\never, we observe that CAR is initially high for Raw\nLLM with GPT-3.5 (Table 3), but decreases when\nextra information from data-retrieval methods is\nprovided. This suggests that GPT-3.5 may exhibit\nblind confidence, leading to potential answer hal-\nlucinations. By incorporating extra information,\nit gains access to relevant information and is not\nsolely reliant on its own knowledge. In contrast,\nGPT-4 demonstrates superior performance and is\nnot blindly confident in its answers, even without\nextra information (76.22% CAR in Table 4). Note\nthat responses that cannot answer the question, e.g.,\n“Sorry, I cannot give a confident answer. ”, are ex-\ncluded when calculating other metrics.\nLLM+DPR performs better than LLM+BM25,\nas indicated by the previous analysis. Hence, we se-\nlect LLM+DPR as the representative data-retrieval\nmethod for both LLM-based metric evaluation and\nhuman evaluation to optimize resources and reduce\nhuman efforts. In the LLM-based metric evalua-\nLLM LLM+DPR LLM+EXP\nMost Favor (%)51.98 52.45 68.76\nAvg Rank 1.33 1.29 1.05\nTable 5: The results of LLM-based metric. Ranks: 1\n(highest), 2 (second), and 3 (lowest). Ranks can be tied.\nLLM LLM+DPR LLM+EXP\nMost Favor (%)13.33 20.00 76.67\nAvg Rank 2.19 2.07 1.34\nDon’t Know 0.13 0.10 0.03\nTable 6: The results of human evaluation.\ntion, we compare methods pairwise three times and\nexclude samples with circular preferences or rank\nconflicts (17.97% conflict rate over the test set).\nTable 5 demonstrates that LLM+EXP outperforms\nbaselines, achieving the highest favor rate and the\naverage rank. The favor rate means the percent-\nage of a certain method selected as the best over\nthe test set. Table 6 shows the human evaluation\nresult with at least two agreements among evalua-\ntors. Similar to the LLM-based metric, LLM+EXP\nshows the best performance in the favor rate and\nthe average ranking. Moreover, LLM+EXP has\nthe least “Don’t Know” rate, representing the con-\nfidence of the human evaluators. The agreement\nanalysis in Appendix G.2 shows that human eval-\nuation is reliable and consistent among evaluators.\nThe results align with the LLM-based metric, high-\nlighting the significant performance of our method\nand the potential of using the LLM-based metric as\nan automation evaluation. We present case studies\nin Appendix I to give a comprehensive comparison\nof different methods. The retrieved-based methods\ntend to provide scattered and often “quote out of\ncontext” chunks. In contrast, the domain knowl-\nedge from our method offers more concise and\nrelevant information, with a significantly shorter\nlength compared with the retrieved chunks.\n7 Conclusion\nIn this paper, we deal with the challenge of em-\npowering LLMs with domain-specific knowledge,\nenabling them to accurately answer questions in\nindustrial scenarios. Due to the limited avail-\nability of relevant benchmarks, we introduce the\nMSQA dataset, tailored for cloud domain QA.\nOur novel model interaction paradigm effectively\nequips LLMs with domain-specific knowledge,\nbridging the gap between general models and indus-\ntry demands. Experiments demonstrate and high-\nlight the effectiveness of our proposed paradigm in\nstandard and newly proposed metrics.\n299\nLimitations\nIt is essential to discuss the limitations of this pa-\nper. One primary limitation is the dataset used for\nexperimentation is confined to Microsoft Azure. It\npotentially impacts the generalizability of the pro-\nposed model interaction paradigm in other domain-\nspecific scenarios. Another limitation is the pa-\nrameter tuning in instruction tuning. It is unlike\npre-training, where we have a large amount of data\nto perform a few epochs to make the model imbue\nAzure domain knowledge. In instruction tuning, it\nis challenging to set the number of epochs prop-\nerly. There still lacks a well-defined and automated\nmetric to evaluate LFQA in order to select good\ncheckpoints with less effort. From our practice,\nsetting a large max token length and more epochs\ndoes not necessarily make a better model. More-\nover, this paper focuses on text-based QA, exclud-\ning QA scenarios with image attachments. Lastly,\nthe proposed model is trained and evaluated exclu-\nsively in English, while the Microsoft Q&A forum\nincludes QAs in other languages. These limitations\nconstrain the applicability of our model to other\nlanguages and multi-modality scenarios.\nEthics Statement\nAlthough we use language models trained on data\ncollected from the web, which have been shown\nto have issues with gender bias and abusive lan-\nguage, we have taken significant precautions to\nensure the ethical integrity of our research. Our\npre-training and instruction-tuning data have been\ncarefully verified to exclude any toxic text, and we\ncollected the data from the Microsoft Q&A forum,\nwhere engineering experts and administrators take\nmoderation and inspection. We have implemented\nrigorous filtering mechanisms and conducted thor-\nough validation to remove inappropriate content\nand any user information. All data used, includ-\ning human evaluation data, is anonymized and pro-\ncessed in compliance with privacy regulations, with\nno disclosure of personally identifiable information.\nWhile acknowledging the limitations and the need\nfor ongoing research, we are dedicated to advanc-\ning responsible and unbiased AI technologies and\nwelcome any inquiries regarding the ethical aspects\nof our work.\nReferences\nAuto-GPT. 2023. Auto-gpt: An autonomous\ngpt-4 experiment. https://github.com/\nSignificant-Gravitas/Auto-GPT. Accessed:\n2023-05-15.\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An\nautomatic metric for mt evaluation with improved cor-\nrelation with human judgments. In Proceedings of\nthe acl workshop on intrinsic and extrinsic evaluation\nmeasures for machine translation and/or summariza-\ntion, pages 65–72.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nCayque Monteiro Castro Nascimento and André Silva\nPimentel. 2023. Do large language models un-\nderstand chemistry? a conversation with chatgpt.\nJournal of Chemical Information and Modeling ,\n63(6):1649–1655.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\nMatt Beane, Ting-Hao Huang, Bryan Routledge, et al.\n2021. Finqa: A dataset of numerical reasoning over\nfinancial data. arXiv preprint arXiv:2109.00122.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.\n2023. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%* chatgpt quality. See https://vicuna.\nlmsys. org (accessed 14 April 2023).\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nS Morris Engel. 1982. With good reason: An introduc-\ntion to informal fallacies.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\nier, Jason Weston, and Michael Auli. 2019. ELI5:\nlong form question answering. In Proceedings of\nthe 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n3558–3567. Association for Computational Linguis-\ntics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International confer-\nence on machine learning, pages 3929–3938. PMLR.\nLynette Hirschman and Robert Gaizauskas. 2001. Natu-\nral language question answering: the view from here.\nnatural language engineering, 7(4):275–300.\n300\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):1–38.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William W\nCohen, and Xinghua Lu. 2019. Pubmedqa: A dataset\nfor biomedical research question answering. arXiv\npreprint arXiv:1909.06146.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nS. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020, pages 6769–6781. Associa-\ntion for Computational Linguistics.\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.\nHurdles to progress in long-form question answering.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021, pages\n4940–4957. Association for Computational Linguis-\ntics.\nAugustin Lecler, Loïc Duron, and Philippe Soyer. 2023.\nRevolutionizing radiology with gpt-based models:\nCurrent applications, future possibilities and limi-\ntations of chatgpt. Diagnostic and Interventional\nImaging.\nPeter Lee, Sebastien Bubeck, and Joseph Petro. 2023.\nBenefits, limits, and risks of gpt-4 as an ai chatbot\nfor medicine. New England Journal of Medicine ,\n388(13):1233–1239.\nDawei Li, Shawn E Christ, and Nelson Cowan. 2014.\nDomain-general and domain-specific functional net-\nworks in working memory. Neuroimage, 102:646–\n656.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nJerry Liu. 2022. LlamaIndex.\nCecily Mauran. 2023. Whoops, samsung work-\ners accidentally leaked trade secrets via\nchatgpt. https://mashable.com/article/\nsamsung-chatgpt-leak-details. Accessed:\n2023-05-15.\nGrégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-\nforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,\nBaptiste Rozière, Timo Schick, Jane Dwivedi-Yu,\nAsli Celikyilmaz, et al. 2023. Augmented language\nmodels: a survey. arXiv preprint arXiv:2302.07842.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021. Webgpt: Browser-assisted question-\nanswering with human feedback. arXiv preprint\narXiv:2112.09332.\nOpenAI. 2023. Gpt-4 technical report. ArXiv,\nabs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nAnkit Pal, Logesh Kumar Umapathi, and Malaikan-\nnan Sankarasubbu. 2022. Medmcqa: A large-scale\nmulti-subject multi-choice dataset for medical do-\nmain question answering. In Conference on Health,\nInference, and Learning, pages 248–260. PMLR.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng,\nYujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\nYu, Weizhu Chen, et al. 2023a. Check your facts\nand try again: Improving large language models with\nexternal knowledge and automated feedback. arXiv\npreprint arXiv:2302.12813.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-\nley, and Jianfeng Gao. 2023b. Instruction tuning with\ngpt-4. arXiv preprint arXiv:2304.03277.\nDavid E Penner and David Klahr. 1996. The interaction\nof domain-specific knowledge and domain-general\ndiscovery strategies: A study with sinking objects.\nChild development, 67(6):2709–2727.\nStephen Robertson, Hugo Zaragoza, et al. 2009. The\nprobabilistic relevance framework: Bm25 and be-\nyond. Foundations and Trends® in Information Re-\ntrieval, 3(4):333–389.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhuggingface. arXiv preprint arXiv:2303.17580.\n301\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2023. Replug: Retrieval-\naugmented black-box language models. arXiv\npreprint arXiv:2301.12652.\nRobert S Siegler. 1989. How domain-general and\ndomain-specific knowledge interact to produce strat-\negy choices. Merrill-Palmer Quarterly (1982-) ,\npages 1–26.\nDan Su, Xiaoguang Li, Jindi Zhang, Lifeng Shang, Xin\nJiang, Qun Liu, and Pascale Fung. 2022. Read before\ngenerate! faithful long form question answering with\nmachine reading. In Findings of the Association for\nComputational Linguistics: ACL 2022, Dublin, Ire-\nland, May 22-27, 2022, pages 744–756. Association\nfor Computational Linguistics.\nRohan Taori. 2023. Alpaca: A strong, replicable\ninstruction-following model. Accessed: 2023-03-13.\nThe Vicuna Team. 2023. Vicuna: An open-source\nchatbot impressing gpt-4 with 90% chatgpt quality.\nhttps://lmsys.org/blog/2023-03-30-vicuna/.\nAccessed: 2023-05-15.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai\nLin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\n2023. Large language models are not fair evaluators.\narXiv preprint arXiv:2305.17926.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\nguage model with self generated instructions. arXiv\npreprint arXiv:2212.10560.\nZhen Wang. 2022. Modern question answering\ndatasets and benchmarks: A survey. arXiv preprint\narXiv:2206.15030.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nChenfei Wu, Shengming Yin, Weizhen Qi, Xi-\naodong Wang, Zecheng Tang, and Nan Duan.\n2023. Visual chatgpt: Talking, drawing and edit-\ning with visual foundation models. arXiv preprint\narXiv:2303.04671.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Evalu-\nating text generation with bert.\nLucia Zheng, Neel Guha, Brandon R Anderson, Peter\nHenderson, and Daniel E Ho. 2021. When does pre-\ntraining help? assessing self-supervised learning for\nlaw and the casehold dataset of 53,000+ legal hold-\nings. In Proceedings of the eighteenth international\nconference on artificial intelligence and law, pages\n159–168.\nA Data Post-Processing\nDue to the fact that the data is collected from an on-\nline Q&A forum, the context of answers is usually\ncomplex and includes a large number of decorative\nsymbols and platform-generated content, which\nmakes the data not easy to use and causes poten-\ntial noise in fine-tuning. To address this issue, we\nconducted a deep sampling of the collected data\nin order to summarize the existing problems and\nidentify their patterns. We design the following\ndata filtering pipeline:\n• Remove user-related information, such as\nusernames and IDs, e.g., bob@1234567, as\nthese personal details are irrelevant to the QA\ncontent and contain noise. For example, in-\ncluding such information in the instruction-\ntuning data would make fine-tuned model out-\nput answers starts with hallucinated user name\nand IDs. Additionally, removing this informa-\ntion helps protect privacy.\n• Standardize all links appearing in the data ac-\ncording to the Markdown link reference syn-\ntax, organizing them into a unified format,i.e.,\n[description](link). We find these links\nare also meaningful, and the model could ex-\ntract information from the context of the links.\nThe fine-tuned model generates relevant and\nvalid links in the response.\n• Remove platform-generated contents, such as\n\"–please don’t forget to\nupvote and Accept as answer\nif the reply is helpful–\"\n• Remove irregular decorative symbols added\nby users, such as \"****\" for separation.\n• Address different types of line breaks and han-\ndling consecutive multiple line breaks. We\nadopted a strategy to replace consecutive mul-\ntiple line breaks with a single line break, while\npreserving the integrity of code blocks by not\nmerging multiple spaces within them.\n302\n• Detect image links in questions and remove\nsamples with screenshots. This dataset fo-\ncuses solely on plain text, with multimodal\ndata exploration reserved for future work.\n• Detect the length of questions and specifically\nlabel samples with questions exceeding 8192\ntokens, as these may require special handling\nor truncation for current models.\nFor reference, Figure 3 provides an example of\ndata post-processing, showcasing the removal of\nuser names and platform-generated context.\nFigure 3: An example of data post-processing. The\nhighlighted red part is removed in post-processing.\nBy implementing this data filtering pipeline, we\naim to improve the quality and usability of the\ncollected data for research purposes.\nB Statistics and Question Samples\nAs shown in Table 7, the average length of ques-\ntions is 347.15 tokens, whereas the average length\nof answers is 382.18 tokens. Additionally, based on\nthe analysis conducted, the average upvote counts\nare 0.05 for questions, 0.26 for answers, and 0.32\nfor samples. Upvotes serve as an indicator of the\ncontent’s perceived value by other users within the\ncommunity, and these counts have been collected\nand calculated independently.\nWe randomly sampled questions based on their\ntypes as shown in Table 8.\nC Instruction Tuning\nBelow is an instruction example used in the instruc-\ntion tuning:\n\"Below is an instruction that\ndescribes a task. Write\na response that appropriately\ncompletes the request\nStatistic Value\n#data 32252\n#tags 332\nAvg. #questions per tag 97.36\nAvg. #tags per question 1.35\nAvg. #tokens per question 347.15\nAvg. #tokens per answer 382.18\nAvg. #upvotes per question 0.05\nAvg. #upvotes per answer 0.26\nAvg. #upvotes per sample 0.32\nTable 7: Additional statistics of MSQA\nInstruction: Please answer the\nfollowing questions concerning\nAzure Virtual Machines.\nInput: I have set Auto shut down\ntime for my VM as 00:30 local\ntime. I have updated the time on\none day to 01:00 at 00:14. Though\nthe modification took affect from\nnext day. Is this expected?\nResponse: Yes, this is the\nexpected behavior. If you update\nthe auto shutdown schedule for\nyour VM within 30 minutes of\nthe previously scheduled shutdown\ntime, the new shutdown time takes\neffect the next day.\"\nD Training Setup and Parameters\nThe source code, configurations, and data asso-\nciated with our work can be accessed at: https:\n//aka.ms/Microsoft_QA. Both the pre-training\nand instruction tuning are conducted in a V100\n32GB node with 8 GPUs. The DeepSpeed6 frame-\nwork was employed for the training process.\nDuring the pre-training phase, we employed un-\nsupervised learning with a next-word prediction\ntraining approach. We split the Azure documen-\ntation into 184,655 samples for pre-training. The\npre-training parameters were set as follows: 8 train-\ning epochs, a maximum token length of 512, a\nbatch size of 64, and a learning rate of 2e−5 with\na cosine decaying scheduler. To ensure efficient\nprocessing, the Azure documentation was divided\ninto separate samples, each with a maximum to-\nken length of 512 and no overlap. Image links and\nrelative links to other Azure markdown files were\nremoved, while clickable links were retained.\n6https://github.com/microsoft/DeepSpeed\n303\nIs\nLooking at migration and also backup/restore options for ConfigMgr. Historically, Microsoft\ndo not support either of the below for ConfigMgr Primary servers:\n• **Migration to VM method**; Physical to Virtual (P2V)\n• **Backup method**; VM snapshots\nIs that still the case for both these scenarios?\nHow\nHow to have administrator reset password for ADB2C user? I’m trying to reset passwords for\nusers inside of the Azure ADB2C blade but when trying the temporary password I always get\n\"The password has expired\" error message.\nWhy\nPassword Writeback General Questions. So, I’m trying to understand some more intricate\nworkings of PasswordResetService. Unlike the pass-through authentication feature, there is no\nWindows service that runs for password writeback. It is my understanding that password write-\nback uses a service bus relay that’s specific to the tenant. Why do I not see that in my tenant,\nand how is this working under-the-hood? Is there no need for multiple \"instances\" like there is\nfor pass-through Authentication? Is it a WCF service, and if so, what is that doing, and how is\nhigh availability accounted for?\nDo\nI changed my app service plan level and that led to a change of the inbound IP address. Now\nI have several apps running there where the domain is handled externally. I had no problems\nchanging the A record for these.\nHowever, I also have an \"app service domain\" managed by Azure pointing\nthere. Do I have to do anything for this domain, or is the change propagated automatically?\nIf I have to do something, where do I find the documentation, because I can’t find any.\nCan\nFor my Windows 11 laptop, can I use the same Windows product key on my VMs, without\nhaving to pay a license for each VM?\nWhat\nWe have a Hybrid Exchange environment, and many user mailboxes are still on-prem. However,\nmany users already have an E3 license. The issue occurs when a user is logged into OWA\nand is connecting to our on-premise exchange server. When they receive an O365 link (e.g.,\n[https://forms.office.com/](https://forms.office.com/)), the user gets the error, ’You do not have\npermission to view or respond to this form’, even though the user has an E3 license. When they\nopen up an incognito window and sign into O365 with the same credentials, everything works\nflawlessly. If someone could explain the theory behind how this works, that would be great.\nWhat is the difference between these two credentials even though the credentials are exactly\nthe same? Thank you.\nTable 8: Examples of questions randomly sampled by their types. The questions are highlighted .\n304\nIn the instruction-tuning phase, we selected QA\npairs that had tags related to Azure, resulting in\na dataset of 10,089 samples. To split the data\ninto train and test sets, we computed the TF-IDF\nsimilarity between each pair of questions and ex-\ncluded questions with high similarity from the test\nset. Consequently, the training set comprised 9,517\nsamples, while the test set contained 572 samples.\nWe restrict the number of the test set considering\nthe generation and evaluation cost with LLMs. The\ninstruction tuning parameters were set as follows:\n3 epochs, a maximum token length of 512, a batch\nsize of 64, and a learning rate of1e−5 with a cosine\ndecaying scheduler. Note that we utilized a smaller\nnumber of epochs in the instruction-tuning process\ncompared to pre-training to mitigate the risk of\noverfitting the training questions and answers.\nE Baseline and Metric Prompts\nIn this section, we list the prompts of baselines:\nLLM, LLM+BM25/DPR, and LLM+EXP from Ta-\nble 9 to Table 11.\n[System]\nAs a helpful assistant, your task is to create responses\nto the user’s questions. If you cannot be sure about\nthe user’s intention, please say, \"Sorry, I do not\nunderstand your question\"; If you cannot give a\nconfident answer, please say, \"Sorry, I cannot give a\nconfident answer\"\n[User]\n{question}\nTable 9: The prompt of the raw LLM method.\nThe prompt to extract keywords and spans in the\nKHR metric is shown in Table 12.\nF LLMs as Evaluators\nF.1 Evaluator Prompt\nTable 13 shows the prompt of scoring two re-\nsponses. The LLM is tasked with comparing these\nresponses to a grounded answer and providing eval-\nuation explanations. Then LLM scores two re-\nsponses ranging from 1 to 10.\nF.2 Evaluator Sensitivity\nTo evaluate the sensitivity of LLM evaluators to\nthe positions of responses, we performed an exper-\niment involving 200 randomly sampled response\npairs from different methods. Each sample con-\nsisted of two responses from two different methods.\n[System]\nAs a helpful assistant, your task is to create responses\nto the user’s questions. We have retrieved some\nchunks from the documents. These chunks are\nincomplete paragraphs and may not be relevant to the\nquestion. Please first determine whether these chunks\nare related to the user’s question and disregard\nthose you deem irrelevant. For the helpful chunks,\nintegrate the useful content from these chunks into\nyour answer without quoting them. If you cannot be\nsure about the user’s intention, please say, \"Sorry, I\ndo not understand your question\"; If you cannot give\na confident answer, please say, \"Sorry, I cannot give\na confident answer\". Below are the chunks:\n<CHUNK>\n{chunk 1}\n<CHUNK>\n{chunk 2}\n<CHUNK>\n{chunk 3}\n[User]\n{question}\nTable 10: The prompt of the LLM+BM25/+DPR\nmethod.\n[System]\nAs a helpful assistant, your task is to create responses\nto the user’s questions. We have retrieved one\nresponse from another LLM. This answer may not\nbe relevant to the question. If you think the LLM\nresponse is helpful, integrate the useful information\ninto your answer without quoting them. Otherwise,\nyou can ignore the LLM response. If you cannot be\nsure about the user’s intention, please say, \"Sorry, I\ndo not understand your question\"; If you cannot give\na confident answer, please say, \"Sorry, I cannot give\na confident answer\". Below are the LLM response:\n<LLM RESPONSE>\n{llama response}\n[User]\n{question}\nTable 11: The prompt of the LLM+EXP method.\nWe conducted two rounds of scoring by switching\nthe positions of the responses. The responses were\nranked based on their scores, and we assigned three\ntags: better, tied, and worse to represent the rank-\ning relationship. If the ranks are different in two\nrounds, we say there is a conflict. We observed\nscoring conflicts in the evaluations conducted by\nthe LLM evaluator before and after switching the\npositions of the responses. In some cases, the LLM\n305\n[System]\nAs a helpful assistant, your task is to extract the\nkeywords or important spans from the provided text\nin <TEXT>. Focus on identifying significant words\nor phrases that are central to the topic or convey\nessential information. Take into account relevant con-\ntext and consider both single words and multi-word\nexpressions as potential keywords. Phrases follow\nthe subject-verb or subject-verb-object pattern. The\nphrases should state if the verb is possible or not.\nPlease provide a list of the extracted keywords or\nspans, separated by a comma. Below is the text:\n[User]\n<TEXT>: {grounded answer}\nTable 12: The prompt to extract keywords and spans\nfrom the grounded answer in the KHR metric.\n[System]\nYou are a helpful and precise assistant for checking\nthe quality of the answer. We would like to invite you\nto provide feedback on the performance of two AI as-\nsistants in answering a user’s question in <Question>,\ncompared with the <Grounded Answer>written\nby humans. Please rate the helpfulness, relevance,\naccuracy, and level of detail of their responses. Each\nassistant receives an overall score on a scale of 1\nto 10, where a higher score indicates better overall\nperformance.\nPlease first provide a comprehensive explanation\nof your evaluation, avoiding any potential bias and\nensuring that the order in which the responses were\npresented does not affect your judgment.\nThen, output two lines indicating the scores for\nAssistant 1 and 2, respectively.\nOutput with the following format:\nEvaluation evidence: <your evaluation explanation\nhere>\nScore of Assistant 1’s response: <score>\nScore of Assistant 2’s response: <score>\n[User]\n<Question>: {question}\n<Grounded Answer>: {grounded_answer}\nAssistant 1’s Response: {response_1}\nAssistant 2’s Response: {response_2}\nTable 13: The prompt of the LLM evaluator generates\nan evaluation explanation first and then gives scores on\ntwo response candidates.\nexhibited a preference for the response located in\nthe first position, resulting in inconsistent rankings\nbetween the two rounds of scoring.\nFigure 4: The statistics of score gaps. GPT-3.5 evalua-\ntor has 46.33% conflict rate, and GPT-4 evaluator has\n29.70% conflict rate.\nEvaluator Conflict/Non-Conflict Ratio\n0 1 2 3 4 5\nGPT-3.5 5.63 1.01 0.45 0.39 0.80 0\nGPT-4 1.27 0.63 0.26 0.15 0.07 0.07\nTable 14: The conflict versus non-conflict ratio of each\nscore gap.\nWe introduce the concept of the score gap, which\nrepresents the absolute difference in scores between\ntwo responses within a single scoring round. Fig-\nure 4 shows the percentage of conflict and non-\nconflict cases when using GPT-3.5 and GPT-4 as\nevaluators, respectively. Note that each sample has\ntwo scoring rounds, and if a conflict arises between\nthese two rounds, both rounds are labeled as con-\nflict. Notably, the GPT-4 evaluator exhibits a sig-\nnificantly lower conflict rate compared to GPT-3.5.\nThen, we select GPT-4 as our preferred evaluator.\nFurthermore, we observe that conflicts mostly oc-\ncur within a score gap range of 0-2. On the other\nhand, we calculate the conflict/non-conflict ratio\nfor each score gap value (see Table 14). When the\nscore gap is 0 or 1, the ratio indicates a high proba-\nbility of conflict. Based on these observations, we\npropose a rule where we trust the LLM evaluator\nonly when the score gap exceeds 1. Otherwise, we\nconduct two scoring rounds by switching response\npositions and rank them based on the average score\nof two rounds. This approach mitigates scoring\n306\nconflicts and ensures a reliable and efficient evalua-\ntion process, primarily relying on a single scoring\nround for most cases.\nG Human Evaluation\nG.1 Evaluation Setup and User Interface\nTo ensure reliable evaluations, we randomly select\na small subset of test samples consisting of 30 QA\npairs. During the selection process, we exclude\nQA pairs that have grounded answers containing\nlinks or phrases such as \"the answer is not sup-\nported in Microsoft Q&A forum.\" These types of\ngrounded answers are not suitable for meaningful\ncomparisons.\nTo conduct the evaluations, we engage crowd-\nworkers who possess expertise in the cloud domain\nand are familiar with Microsoft products. We em-\nploy five such evaluators. Each sample receives five\nindependent evaluations from these qualified eval-\nuators. We consider an evaluation reliable when\nthere is agreement among at least two out of the\nfive evaluators.\nFigure 5 shows the user interface (UI) of human\nevaluation in our study. The UI presents the com-\nponents involved in evaluating a single sample. It\nbegins with the display of a QA pair, followed by\nthree responses generated by different methods. To\nminimize any potential bias, the positions of the\nresponses are shuffled randomly for each sample\nevaluation. This ensures that the evaluator does\nnot implicitly associate a particular response with a\nspecific method. The evaluator’s task is to rank the\nposition of each response based on its similarity to\nthe grounded answer. As the rank is assigned to\neach response individually, it is possible for two\nresponses to receive the same rank. For example,\nboth Response 1 and Response 2 can be assigned\nRank 1 if they are equally similar to the grounded\nanswer. This flexibility allows for a more nuanced\nevaluation and accommodates cases where multi-\nple responses are equally relevant or accurate. The\nevaluators are also provided the \"I don’t know\" op-\ntion if they do not have a confident evaluation of\nthe sample. Before proceeding with the evaluation\nof the 30 test samples, each evaluator is given a sep-\narate test sample to familiarize themselves with the\nevaluation process. This preliminary test sample\nserves as a practice round, allowing the evaluators\nto become acquainted with the evaluation criteria\nand interface.\nFigure 5: The user interface template of human evalu-\nation. The details of QA and responses are not shown\ndue to the space limit.\nG.2 Evaluation Agreements\nAs shown in Figure 6, all evaluated methods consis-\ntently exhibit a nearly 100% ratio of at-least-two-\nagreement. In particular, the LLM+EXP method\nstands out with a higher agreement compared\nto other approaches when considering agreement\ncounts larger than 2. The results highlight the relia-\nbility of the human evaluation in achieving agree-\nment across multiple annotations.\n307\nFigure 6: The statistics of agreements among human\nevaluators.\nH Results of Lexical-overlap-based\nMetrics\nAccording to findings in (Krishna et al., 2021; Ji\net al., 2023), as well as our own experimental ob-\nservations, lexical-overlap-based metrics are inad-\nequate for evaluating the quality of generated an-\nswers. This is evident from the results presented\nin Table 15 and Table 16, where ROUGE scores\ndemonstrate minimal variations across different\nmethods. Although BLEU and METEOR indicate\nbetter performance for LLM+EXP, the differences\nare not significant. Additionally, the low values of\nBLEU and METEOR scores suggest that lexical-\noverlap-based metrics are not suited for compar-\ning LLM-generated answers with human-written\ngrounded answers.\nMetrics LLM LLM+BM25 LLM+DPR LLM+EXP\nBLEU 3.49 3.57 4.37 4.49\nROUGE-131.04 31.40 31.31 30.49\nROUGE-28.37 8.80 9.10 8.63\nROUGE-L18.12 18.19 18.02 17.92\nMETEOR17.77 18.07 20.50 20.67\nTable 15: The results of lexical-overlap-based metrics\nover different methods with GPT-3.5 as the backbone\nLLM.\nMetrics LLM LLM+BM25 LLM+DPR LLM+EXP\nBLEU 3.78 3.74 4.64 5.55\nROUGE-130.62 32.62 31.39 31.62\nROUGE-28.87 9.92 9.67 9.29\nROUGE-L17.37 18.76 18.12 18.34\nMETEOR23.03 22.02 22.83 23.63\nTable 16: The results of lexical-overlap-based metrics\nover different methods with GPT-4 as the backbone\nLLM.\nI Case Study\nWe present two case studies to offer a detailed com-\nparison of different methods.\nTable 17 presents a question inquiring about\nwhether cloud-only users can log on to a hybrid\njoined computer. The grounded answer is negative,\nas only users with both on-prem and cloud presence\ncan log on to hybrid AAD. We compare the results\nobtained from retrieved chunks and our domain\nknowledge generated from the domain-specific LM.\nThe data-retrieved chunks contain scattered infor-\nmation related to keywords in the question, such as\nAAD joined devices and configurations of hybrid\nAAD joined devices. However, they do not directly\naddress the question of whether cloud-only users\ncan log on to hybrid AAD joined devices. On the\nother hand, our domain knowledge provides a di-\nrect answer, stating that users require an on-prem\naccount to log on to hybrid joined devices. Incorpo-\nrating this extra information results in significantly\ndifferent responses, with data-retrieval+LLM sug-\ngesting it is a configuration issue.\nTable 18 presents another question asking about\nobtaining metrics for specific blob containers’\ntransactions and the affected files. The grounded\nanswer recommends using Log Analytics to ag-\ngregate statistics from logs, a method not avail-\nable in the Azure portal. The retrieved chunks\ngathered information on Azure Storage and Azure\nMonitor Metrics but did not offer a clear approach\nfor obtaining specific metrics for blob containers.\nConsequently, data-retrieval+LLM responses sug-\ngested that the metrics could be obtained through\nthe Azure portal and erroneously mentioned Azure\nStorage Analytics as a possible solution. On the\nother hand, our response from the domain-specific\nLM explicitly suggests utilizing Azure Log Analyt-\nics and provides a step-by-step guideline, aligning\nwith the grounded answer.\n308\nQuestion\nCloud-only account on hybrid joined device?\nHello everyone,\nwe are planning to use hybrid joined devices (AD & AAD) in future as one step (of\nmany) to a cloud-only approach.\nWhat we are currently wondering (because it’s not working in the lab environment):\nCan a cloud-only user logon to a hybrid joined computer? Currently in our test\nenvironment it’s not working. The company want’s to have the On-Premise AD\nUser Accounts removed in near future and use cloud-only accounts.\nThanks in advance\nGrounded\nAnswer\nIf the machine is **Hybrid AAD joined,** a cloud only user wont be able to join,\nas while you login to a Hybrid AAD join machine, usually the user is present in\nboth the on-prem and the cloud and only that user can login, because while logging\ninto a Domain joined machine, the user first has to get authenticated by the on-prem\n**Domain Controller** before AAD checks the credentials. Second option is to go\nahead with **Azure AD Joined** machine.\nBM25\nChunks\nDeploy Azure AD joined VMs in Azure Virtual Desktop-Azure. User accounts\ncan be cloud-only or synced users from the same Azure AD tenant. ## Known\nlimitations The following known limitations may affect access to your on-premises\nor Active Directory domain · · ·\nArchive for What’s new in Azure Active Directory? Users can now use FIDO2\nsecurity keys to sign in to their Hybrid Azure AD joined Windows 10 devices and\nget seamless sign-on to their on-premises and cloud resources. · · ·\nWhat is an Azure AD joined device? | Azure AD Join | Description | | - - - | - - - | | *\n* Definition * * | Joined only to Azure AD requiring organizational account to sign\nin to the device · · ·\nDPR\nChunks\nSome hybrid solutions might require access to both on-premises and cloud resources.\nAn example of a use case would be an Identity Governance solution that uses a\nservice account on premises for access to AD DS and requires access to Azure AD.\n· · ·\n- **Hybrid Azure AD joined device** - Users might be able to use the device\nto sign-in to their on-premises domain. However, they can’t access Azure AD\nresources such as Microsoft 365. · · ·\nwithout a hybrid deployment that includes an on-premises AD DS environment.\nThe following table outlines common device ownership models and how they would\ntypically be joined to a domain: · · ·\nOur Domain\nKnowledge\nA cloud-only account can’t logon to a hybrid joined computer. You need to have an\non-premises AD account to logon to a hybrid joined computer.\nLLM Sorry, I cannot give a confident answer.\"\nLLM+BM25\nYes, a cloud-only user can log on to a hybrid joined computer. However, since\nyou mentioned that it’s not working in your lab environment, there might be some\nconfiguration issues or limitations that need to be addressed.\nEnsure that your hybrid environment is properly set up, and the devices are correctly\njoined to both on-premises Active Directory and Azure Active Directory. Addition-\nally, make sure that the user accounts are properly synced between the on-premises\nAD and Azure AD.\nIf you still face issues, it’s recommended to review your hybrid setup and trou-\nbleshoot any potential configuration problems.\nLLM+DPR\nYes, a cloud-only user can log on to a hybrid joined computer. However, if it’s\nnot working in your test environment, there might be some configuration issues or\nrequirements that need to be addressed.\n309\nTo enable cloud-only users to log on to hybrid joined devices, you need to ensure\nthe following:\n1. The devices must be running Windows 10 or later.\n2. The devices must be joined to both on-premises Active Directory Domain\nServices (AD DS) and Azure Active Directory (Azure AD).\n3. Azure AD Connect must be configured and running to synchronize your on-\npremises AD DS with Azure AD.\n4. Seamless Single Sign-On (SSO) should be enabled to allow users to access Azure\nAD resources without needing to re-enter their credentials.\nIf you have met these requirements and are still experiencing issues, you may need\nto troubleshoot your configuration or check for any errors in the synchronization\nprocess.\nLLM+EXP\nNo, a cloud-only user cannot logon to a hybrid joined computer. To logon to a\nhybrid joined computer, you need to have an on-premises AD account. If your\ncompany plans to remove on-premises AD user accounts in the near future, you\nmay need to consider other options such as using Azure AD Domain Services or\nimplementing a password hash synchronization solution. These options will allow\nyou to continue using cloud-only accounts while still being able to logon to hybrid\njoined devices.\nTable 17: Case details on the front page. GPT-4 is employed as the backbone LLM generator. The question and\ngrounded answer are presented, followed by summarized BM25 and DPR chunks, along with domain knowledge\nderived from our domain-specific LM. Due to space limitations, only the top chunks of BM25 and DPR are displayed.\nThen we present responses from four baselines: LLM, LLM+BM25, LLM+DPR, LLM+EXP. The question and\nanswer is found in: https://learn.microsoft.com/en-us/answers/questions/2096/\nQuestion\nGet metrics on Azure Storage Account Blob being used, not the entire data store.\nIs it possible via the Azure Portal to get metrics on specific BLOB containers’\ntransactions and the files they affected? I can only get metrics for the entire storage\naccount and multiple apps/services use the same account, I need to find out which\ncontainer is being used so I can narrow down which app consumes the most resources\non the storage account.\nGrounded\nAnswer\nWith Log Analytics, the measurement can be achieved by aggregating storage logs\non container/folder. You can refer to the below for detailed guidelines.\nYou need to calculate based on request or response sizes. Here’s an example for\nIngress/Egress:\n1 StorageBlobLogs\n2 | extend Ingress =\niff(isnull(RequestHeaderSize),0,RequestHeaderSize)\n+ iff(isnull(RequestBodySize),0,RequestBodySize)\n3 | extend Egress =\niff(isnull(ResponseHeaderSize),0,ResponseHeaderSize)\n+ iff(isnull(ResponseBodySize),0,ResponseBodySize)\n4 | project OperationName, Ingress, Egress,\nRequestHeaderSize, RequestBodySize,\nResponseHeaderSize, ResponseBodySize\nAzure Monitor Metrics: https://learn.microsoft.com/en-us/azure/\nstorage/common/storage-metrics-in-azure-monitor\n310\nIf the above suggestion doesn’t help you: The workaround is to aggregate the\nstatistics from analytics logs to capture changes plus create a capacity baseline by\nenumerating objects in each container. Will you please elaborate on what usage of\ncontainers you need? What’s the model of container layout versus the final users?\nHow many containers and blobs in each container can be projected? Kindly let us\nknow if the above helps or you need further assistance on this issue.\nBM25\nChunks\nTo understand how disallowing public access may affect client applications, we\nrecommend that you enable logging and metrics for that account and analyze\npatterns of anonymous requests over an interval of time. Use metrics · · ·\nList the account-level metric definition You can list the metric definition of your\nstorage account or the Blob storage service. Use the [ az monitor metrics list -\ndefinitions ] command. · · ·\n- [ Can I enable persistence on a previously created cache? ] - [ Can I enable AOF\nand RDB persistence at the same time? ] - [ How does persistence work with\ngeo-replication? ] - [ Which persistence model should I choose? ] - · · ·\nDPR\nChunks\nAzure Storage is a core Azure service, and forms an important part of the majority of\nsolutions that customers deploy to the Azure infrastructure. Azure Storage includes\ncapabilities · · ·\nTo log Azure Storage data with Azure Monitor and analyze it with Azure Log\nAnalytics, you must first create a diagnostic setting that indicates what types of\nrequests and for which storage services you want to log data. To create · · ·\nEnable blob inventory reports for your storage account. See [Enable Azure Storage\nblob inventory reports](blob-inventory-how-to.md). Use the following · · ·\nOur Domain\nKnowledge\nThere is no way to get container level metrics from the portal or the\nmetrics service. You would need to write some code to do this. If\nyou are using Azure Monitor( https://learn.microsoft.com/en-us/azure/\nazure-monitor/overview?WT.mc_id=AZ-MVP-5000120) you can create a diag-\nnostic setting to send the logs and metrics for Blob storage to an Azure Log An-\nalytics workspace. From there you can query the logs to get the information you\nneed.\nLLM Yes, it is possible to get metrics on specific Blob containers’ transactions and the\nfiles they affected in Azure Portal. You can use the Azure Storage Analytics feature\nto collect metrics and logs for your Blob containers.\nTo enable Storage Analytics, you need to go to your Storage account in the Azure\nPortal, select \"Storage Analytics\" from the left-hand menu, and then configure the\nsettings for metrics and logs. You can choose to enable metrics for specific Blob\ncontainers and set the retention period for the data. Once you have enabled Storage\nAnalytics, you can view the metrics and logs in the Azure Portal by going to your\nStorage account, selecting \"Metrics\" from the left-hand menu, and then selecting\nthe Blob container and time range you want to view. This will give you detailed\ninformation on the transactions and files affected by that container.\nLLM+BM25\nYes, it is possible to get metrics on specific Blob containers’ transactions and the\nfiles they affected via the Azure Portal. You can use Azure Metrics Explorer to\ntrack anonymous requests to a storage account and determine which containers are\nbeing accessed anonymously. To do this, you can enable logging and metrics for\nthat account and analyze patterns of anonymous requests over an interval of time.\nUse metrics to determine the number of anonymous requests to the storage account,\nand use logs to determine which containers are being accessed anonymously. Ad-\nditionally, you can list the metric definition of your storage account or the Blob\nstorage service using the az monitor metrics list-definitions command. This will\nallow you to see the metrics for specific Blob containers. I hope this helps!\n311\nLLM+DPR\nYes, it is possible to get metrics on specific Blob containers’ transactions and the\nfiles they affected. You can use Azure Storage Analytics to monitor and analyze the\nusage of your Blob containers.\nTo get metrics on a specific Blob container, you can follow these steps:\n1. Enable Storage Analytics for your Blob container. You can do this by navigating\nto your Blob container in the Azure portal, selecting \"Monitoring\" from the left-hand\nmenu, and then selecting \"Storage Analytics\" and enabling it.\n2. Once Storage Analytics is enabled, you can view metrics for your Blob container\nby selecting \"Metrics\" from the left-hand menu. From there, you can select the Blob\ncontainer you want to view metrics for and choose the specific metrics you want to\nsee, such as \"Transactions\" or \"Egress\".\nLLM+EXP\nUnfortunately, you cannot directly get container-level metrics from the Azure Portal\nor the metrics service. However, you can achieve this by using Azure Monitor and\nAzure Log Analytics workspace.\nTo get metrics on specific Blob containers’ transactions and the files they affected,\nfollow these steps:\n1. Set up Azure Monitor https://docs.microsoft.com/en-us/azure/\nazure-monitor/overview for your storage account.\n2. Create a diagnostic setting to send the logs and metrics for Blob storage to an\nAzure Log Analytics workspace.\n3. Query the logs in the Log Analytics workspace to get the information you need.\nBy querying the logs in the Log Analytics workspace, you can filter the data based\non container names and analyze the transactions and files affected. This will help\nyou identify which container is being used the most and narrow down the app\nconsuming the most resources on the storage account.\nTable 18: Case study on Azure Storage metrics. GPT-4 is employed as the backbone LLM generator. The\nquestion and grounded answer are presented, followed by summarized BM25 and DPR chunks, along with domain\nknowledge derived from our domain-specific LM. Due to space limitations, only the top chunks of BM25 and DPR\nare displayed. Then we present responses from four baselines: LLM, LLM+BM25, LLM+DPR, LLM+EXP. The\nquestion and answer is found in: https://learn.microsoft.com/en-us/answers/questions/172078/\n312"
}