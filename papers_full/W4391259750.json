{
    "title": "Potholes and traffic signs detection by classifier with vision transformers",
    "url": "https://openalex.org/W4391259750",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5014580670",
            "name": "Satish Kumar Satti",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5054410379",
            "name": "Goluguri N. V. Rajareddy",
            "affiliations": [
                "GITAM University"
            ]
        },
        {
            "id": "https://openalex.org/A5071356251",
            "name": "Kaushik Mishra",
            "affiliations": [
                "GITAM University"
            ]
        },
        {
            "id": "https://openalex.org/A5039341855",
            "name": "Amir H. Gandomi",
            "affiliations": [
                "University of Technology Sydney"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3156447956",
        "https://openalex.org/W3009899240",
        "https://openalex.org/W4206300761",
        "https://openalex.org/W3048911013",
        "https://openalex.org/W2947568086",
        "https://openalex.org/W2965435055",
        "https://openalex.org/W4200207011",
        "https://openalex.org/W2788638839",
        "https://openalex.org/W3006380048",
        "https://openalex.org/W3010799168",
        "https://openalex.org/W4210949094",
        "https://openalex.org/W3084946993",
        "https://openalex.org/W3198759116",
        "https://openalex.org/W3198643537",
        "https://openalex.org/W3195758065",
        "https://openalex.org/W4200035114",
        "https://openalex.org/W2472350142",
        "https://openalex.org/W2193145675",
        "https://openalex.org/W2002427601",
        "https://openalex.org/W2398751808",
        "https://openalex.org/W4383613937",
        "https://openalex.org/W3175913009",
        "https://openalex.org/W3147948705",
        "https://openalex.org/W4248196768",
        "https://openalex.org/W4214746678",
        "https://openalex.org/W3103942004",
        "https://openalex.org/W4282824342",
        "https://openalex.org/W639708223",
        "https://openalex.org/W3106250896"
    ],
    "abstract": null,
    "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2024) 14:2215  | https://doi.org/10.1038/s41598-024-52426-4\nwww.nature.com/scientificreports\nPotholes and traffic signs \ndetection by classifier with vision \ntransformers\nSatish Kumar Satti 1, Goluguri N. V. Rajareddy 2, Kaushik Mishra 2 & Amir H. Gandomi 3,4*\nDetecting potholes and traffic signs is crucial for driver assistance systems and autonomous vehicles, \nemphasizing real-time and accurate recognition. In India, approximately 2500 fatalities occur annually \ndue to accidents linked to hidden potholes and overlooked traffic signs. Existing methods often \noverlook water-filled and illuminated potholes, as well as those shaded by trees. Additionally, they \nneglect the perspective and illuminated (nighttime) traffic signs. To address these challenges, this \nstudy introduces a novel approach employing a cascade classifier along with a vision transformer. \nA cascade classifier identifies patterns associated with these elements, and Vision Transformers \nconducts detailed analysis and classification. The proposed approach undergoes training and \nevaluation on ICTS, GTSRDB, KAGGLE, and CCSAD datasets. Model performance is assessed using \nprecision, recall, and mean Average Precision (mAP) metrics. Compared to state-of-the-art techniques \nlike YOLOv3, YOLOv4, Faster RCNN, and SSD, the method achieves impressive recognition with a \nmAP of 97.14% for traffic sign detection and 98.27% for pothole detection.\nIn the present context, global transportation options encompass air travel, metros, buses, and various personal \nvehicles. Among these, road transportation stands out as a widespread and economical means of connecting \ndiverse locations. However, owing to diverse road conditions or lapses in driver attention, accidents are a daily \noccurrence. While drivers are expected to focus on the road, additional assistance can enhance their awareness \nand alert them to potential emerging hazards. This assistance has the potential to minimize human errors by \nactively monitoring the driving environment and providing timely warnings along with recommendations and \nalarms. This study focuses on the development of an Intelligent Transport System designed to alert drivers to \npotential degradation. Specifically, the research addresses the challenges associated with detecting potholes and \ntraffic signs in the conditions prevalent on Indian roads.\nThe statistics starkly illustrate the detrimental impact of potholes on road safety. In 2018, accidents stem-\nming from potholes led to the tragic loss of 15 lives. The gravity of the situation is further highlighted by the \nfigures for preceding years, with 9423 accidents and 3597 fatalities in 2017, 6424 accidents and 2324 lives lost \nin 2016, and 10,876 incidents resulting in 3416 deaths in  20151. Similarly, in 2014, accidents related to potholes \nclaimed the lives of 3039 individuals. According to the Ministry of India, Uttar Pradesh recorded the highest \nnumber of fatalities attributed to potholes in  20182, with 1043 cases, followed by Haryana with 222 instances \nand Maharashtra with 166 fatalities. The issue persisted, causing 4775 incidents in 2019 and 3564 accidents in \n2020. This worrisome trend highlights the pressing need for effective measures to address the impact of potholes \non road safety in India.\nThe efficacy of Intelligent Transport Systems, as well as autonomous and assisted driving, hinges heavily \non the precise identification of traffic signs and  potholes3. This identification empowers drivers with real-time \ninformation through automated traffic sign detection and pothole detection, enabling better control of their \nactions and elevating the safety and convenience of operating motor vehicles. Automated traffic sign and pothole \ndetection is a fundamental component for automated driving systems. Its potential advantages for the future are \nsubstantial. Nonetheless, challenges such as variations in illumination, adverse weather conditions contribute \nto the intricacies of real-world traffic scenarios, indicating that the domain of traffic sign and pothole detection \nstill harbors numerous unanswered questions. The crucial aspect of traffic sign and pothole detection lies in the \nability to recognize minor signs and tiny potholes within a complex environment, ensuring the resilience and \nOPEN\n1Department of Computer Science and Engineering, VFSTR Deemed to be University, Guntur 522213, \nIndia. 2Department of Computer Science and Engineering, GITAM Deemed to be University, \nVisakhapatnam 530045, India. 3Faculty of Engineering and Information Technology, University of Technology \nSydney, Ultimo, NSW 2007, Australia. 4University Research and Innovation Center (EKIK), Óbuda University, \n1034 Budapest, Hungary. *email: gandomi@uts.edu.au\n2\nVol:.(1234567890)Scientific Reports |         (2024) 14:2215  | https://doi.org/10.1038/s41598-024-52426-4\nwww.nature.com/scientificreports/\naccuracy of the detection system. Numerous studies have extensively explored methods for identifying traffic \n signs4. Leveraging the geometric shapes and vibrant colors of traffic signs, algorithms based on color and shape \nhave been proposed. These algorithms extract relevant information to produce features from the region of inter-\nest (ROI) containing the traffic signs.\nIn recent times, deep learning has gained popularity in the realm of traffic sign and pothole detection. \nCertain attention-based detection approaches incorporate an attention module to extract the ROI from the \ninput image and optimize features against complex  backgrounds5. The integration of these two techniques sig -\nnificantly improves the accuracy of recognizing minor traffic signs while reducing false alarms. However, the \ndeployment of deep neural networks on movable platforms poses challenges due to time-consuming processes \nand high computational requirements. Detecting traffic signs and potholes on moving devices becomes chal-\nlenging. Consequently, many approaches opt for lightweight network models developed through compression \ntechniques. This approach aims to reduce the computational load, enabling real-time traffic sign identification \non mobile platforms.\nImproving the robustness and accuracy of traffic sign recognition and pothole detection systems relies heav-\nily on the ability to effectively recognize and interpret minor traffic signs and tiny potholes within complicated \ndifferent environments. Extensive research has been devoted to the exploration of various techniques in this \n domain4. Given that traffic signs typically exhibit geometric shapes such as triangles, circles, and rectangles, \ncoupled with vibrant colors, several algorithms have been proposed that leverage color and shape-based informa-\ntion for traffic sign detection. These proposed algorithms extract pertinent features from the region of interest \n(ROI) encompassing the traffic sign. In recent years, deep learning has gained significant popularity within the \nrealm of traffic sign detection. Advanced network architectures have been devised to enhance detection accuracy, \nparticularly for small-sized traffic signs. To effectively recognize traffic signs in intricate environments, numerous \napproaches have incorporated image segmentation techniques.\nAdditionally, attention-based detection methods have emerged, employing attention modules to extract ROIs \nfrom input images and fine-tune features within complex  backgrounds 5. By leveraging these strategies, the \naccuracy of traffic signs and potholes recognition has been greatly improved while minimizing the occurrence \nof false alarms. However, one major challenge arises when attempting to deploy deep neural networks on mov-\ning platforms. The process of deploying these networks is often time-consuming and computationally intensive, \nmaking real-time traffic sign detection on movable devices a formidable task. Furthermore, the recognition of \nminor traffic signs and potholes in complex environments is a critical aspect of traffic sign detection systems. \nResearchers have dedicated substantial efforts to develop effective techniques that leverage deep learning, image \nsegmentation, attention mechanisms, and compression methods to enhance the accuracy and efficiency of traffic \nsign recognition, particularly for small-sized signs.\nThe current scenario lacks a comprehensive model capable of effectively alerting concerned authorities and \ndrivers about the condition of roads. In this work, we propose an innovative approach that utilizes image recog-\nnition and computer vision techniques for the identification of potholes and traffic signs. This endeavor holds \nimmense industrial potential, particularly in the domains of Driver Assistance Systems and Intelligent Autono-\nmous  Vehicles6. The first step of a typical pothole and traffic sign identification technique involves locating the \nprecise position of potholes and traffic sign regions. The second step is assigning suitable classifications to the \ndetected potholes and traffic signals. In our study, we have developed a gradient-boosting cascade classifier \nspecifically tailored to accurately locate potholes and traffic signs even on challenging road surfaces. Addition -\nally, we employ a vision transformer to effectively identify and assign labels to the detected potholes and traffic \nsigns. By combining these techniques, we aim to provide a robust and reliable system for comprehensive road \nanalysis and safety enhancement.\nBased on existing literature, conventional approaches to pothole detection face limitations in identifying \nwater-filled potholes and those either illuminated or concealed by tree shadows. Similarly, current traffic sign \nidentification algorithms lack the ability to recognize perspective-oriented traffic signs and exhibit reduced \naccuracy in detecting illuminated signs at night. This research aims to introduce a more efficient and distinctive \napproach, specifically tailored for challenging climatic and topographical conditions.\nThe main aim of this work is to develop a model that will detect potholes and traffic signs in challenging \nenvironmental conditions. The notable contributions of this research include:\n• Detection of water-filled potholes and potholes affected by illumination and obscured by tree shadows.\n• Recognition of perspective traffic signs and tiny traffic signs affected by illumination.\n• A novel model capable of detecting both potholes and traffic signs under diverse conditions.\nThe rest of the paper is organized as follows: Section II illustrates a review of existing literature relevant to \nthe research topic. Section III illustrates detailed information on how the research was conducted, including \nthe research design, participants, materials, and procedures. Section IV illustrates a presentation of the findings \nobtained from the research and interpretation and analysis of the results. Section V discusses a summary of the \nmain findings and their significance. Section VI illustrates a list of all the sources cited in the paper.\nLiterature survey\nPothole detection\nA novel model for pothole detection utilising CNN and LSTM was developed by Varona et al. 7, which reached \n93% accuracy. Dhiman et al. 8 proposed using deep learning and stereo-vision analysis to spot the potholes in \nthe road. A novel CNN method proposed by Aparna et al. 9 has achieved 97% accuracy in identifying potholes \nin road pavements. The pothole detection model was developed by Sawalakhe et al. 10 for use on a single-board \n3\nVol.:(0123456789)Scientific Reports |         (2024) 14:2215  | https://doi.org/10.1038/s41598-024-52426-4\nwww.nature.com/scientificreports/\nRaspberry Pi. This model acquires the image/audio, processes it using computer vision algorithms, and then \npinpoints the location of any potholes in the road. At last, the GPS coordinates of the pothole will be sent to \nthe relevant authorities so that they may take the necessary next steps. A location-aware convolutional neural \nnetwork (CNN) model was proposed by Chen et al. 11. It leaves out the global context in favour of localised \nanalysis of discriminative areas in road photographs. There are two steps: first, find the potholes, and then, sort \nthem into categories. The overall accuracy of this model is 95.2%. An Internet of Things (IoT) model dubbed \n\"DeepBus\" was proposed by Bansal et al.12 to locate road defects in India. Internet-of-Things sensors are used to \ntrack the locations of the craters in real time. Users and authorities everywhere may now see an interactive map \nshowing the precise locations of all known potholes. Users and administrators alike will get alerts so they may \ntake appropriate measures as soon as possible.\nTraffic sign detection\nThe YOLOv3 layer-based network pruning along with a patch-wise approach for recognising very small traf-\nfic signals was introduced by Rehman et al. 13. In addition, an anchor box selection algorithm was presented to \ndetermine the optimal anchor set. It decreases the overall miss rate and the proportion of false positives. It was \ntrained and evaluated using data derived from traffic signs in Germany and Sweden, where it scored best in terms \nof mean absolute precision. A traffic sign-detection model was proposed by Wang et al.14, and it uses a refinement \nclassifier and a lightweight super-clad detector. It boosts processing speed by using spatial statistics. The model \nonly has 6.49 million parameters, making it rather simple. It was tested and refined using the Tisunga—Tencent \n100k dataset, where it achieved 92.16% mAP with a per-frame processing time of 0.150 s.\nTo better recognise traffic signs, Wang et al.15 suggested a lightweight approach based on YOLOv4 tiny. An \nenhanced K-means clustering technique is employed to increase the recall rate and preciseness of the target \nposition. A large-scale feature map technique is presented to enhance the precision of large-scale tiny item \ndetection. The mAp and recall rates were both increased by 5.73 percentage points and 7.29 percentage points, \nrespectively, after being trained and evaluated on the TT 100k dataset. To detect and identify traffic signs, Cao \net al.16 proposed an improved sparse R CNN model. To distinguish even the tiniest of traffic signals, a unique \ndetection model is provided, and a multiscale fusion structure technique is used. All of the research relies on the \nTT100k dataset, which achieved 62.3 mAP .\nThe IFA-FPN method was developed for traffic sign recognition by Tang et al.17. The Tsinghua-Tencent 100k \ndataset (TT100k), the Swedish Traffic Sign Dataset (STSD), and the German Traffic Sign Detection Benchmark \n(GTSDB) are utilised for the evaluations. The experimental results demonstrate the efficacy of the proposed IFA-\nFPN in detecting traffic signs. When the recommended IFA-FPN is applied to the Cascade RCNN, it receives a \nmAP of 80.3% in GTSDB, which is 9.9% higher than FPN; a mAP of 65.2% in STSD, which is 3.5% higher than \nFPN; and a mAP of 93.6% in TT100k, which is 1.6% higher than FPN.\nSatti et al.18 presented a system for recognizing traffic signs on Indian highways. The traffic sign objects are \nfound using a cascade classifier, and the traffic signs are classified using CNN. It simplifies the model since CNN \njust takes the frames that include the traffic sign objects. The trials were conducted using the ICTS dataset, and \ngreater mAP values were obtained. A technique for identifying traffic signs was presented by Y ang et al.19. The col-\nour probability model and colour HOG are used for feature extraction and localization. CNN is in charge of traffic \nsign categorization. The experiments are carried out and assessed using datasets from the GTSDB and CTSD.\nMost existing methods exhibit low detection accuracy in low-light conditions, and they often struggle to \ndetect occluded and perspective traffic signs. Additionally, many models lack the capability to identify traffic \nsigns in low-light situations. Furthermore, there is a gap in addressing the detection of potholes covered by \ntree shadows and filled with water. Moreover, the size and severity of potholes are not quantified or adequately \naddressed in the current approaches.\nIn an effort to maintain focus on the road while driving, drivers often overlook traffic signs and potholes. Such \nlapses pose potential dangers to both the driver and surrounding individuals. This issue might be mitigated with \nan effective means of alerting the driver without requiring a shift in their attention. The majority of tasks related \nto traffic sign recognition and pothole detection have primarily been executed on foreign roads, where the road \nconditions significantly differ from those in India. This research proposal centers on the development of models \nspecifically tailored for detecting traffic signs and potholes in Indian road conditions. The objective is not only to \nenhance road safety but also to instil a sense of confidence in drivers navigating unfamiliar or challenging routes.\nMoreover, the application of AI with ML or DL techniques has revolutionized the world today. For instance, \napplications like predicting the air passenger traffic  flow30, health monitoring in urban traffic in the V ANET \n network31, tracking moving vehicles from the video footage for automatic traffic flow  analysis33, object detection \nin video surveillance  systems34,35, etc. Furthermore, In order to assist traffic flow analysis (TFA) and solutions \nthat need the forecast of many traffic variables, such as driving behaviour, journey time, speed, density, incident, \nand traffic flow, the  paper32 analyses the application of data fusion (DF) approaches in Intelligent Transportation \nSystems.\nProposed system\nOverview\nUpon delving into the extensive literature survey outlined in Section II, it becomes apparent that the realm of \nrecognizing potholes and traffic signs on Indian road surfaces has been largely unexplored. In light of this, the \nproposed system sets forth a groundbreaking approach to tackle this challenge and revolutionize the recognition \nprocess within the context of Indian road conditions. To begin with, a meticulously designed cascade classifier \ntakes center stage, diligently working to pinpoint the exact locations of potholes and traffic signs strewn across \nthe vast expanse of Indian roads. This initial phase serves as a crucial foundation for the subsequent steps. \n4\nVol:.(1234567890)Scientific Reports |         (2024) 14:2215  | https://doi.org/10.1038/s41598-024-52426-4\nwww.nature.com/scientificreports/\nSubsequently, leveraging the power of Vision Transformer, a cutting-edge technique is employed to unleash \nthe potential for precise prediction of both potholes and traffic signs. Figure 1 serves as a visual representation, \nshowcasing the architecture of this innovative system, where the synergistic integration of cascade classifiers and \nvision transformers propels the realm of pothole and traffic sign prediction.\nDataset\nTo implement this model, 5000 pothole images and 19,775 traffic signs of 40 classes are acquired at diverse \nroad conditions using the Samsung Galaxy C7 Pro mobile. The sample pothole and traffic sign images from the \nobtained dataset are shown in Figs.  2 and 3, respectively. Figure  4 shows the sample images of the challenged \ntraffic signs for detection. Table 1 showcases the comprehensive distribution of the pothole and traffic sign data-\nset, meticulously curated to facilitate the training and testing of our innovative model. On the flip side, Table  2 \nshows a detailed breakdown of the dataset allocation specifically for both training and testing purposes. To \nensure optimal image quality, a series of preprocessing techniques have been applied to the dataset. Notably, the \nimplementation of Gaussian filtering and  CLAHE20 techniques has been instrumental in enhancing the overall \nclarity and visual fidelity of the images, thus fortifying the foundation for accurate and reliable model training \nand evaluation.\nFigure 1.  Proposed pothole and traffic sign prediction architecture.\nFigure 2.  Pothole samples from dataset.\n5\nVol.:(0123456789)Scientific Reports |         (2024) 14:2215  | https://doi.org/10.1038/s41598-024-52426-4\nwww.nature.com/scientificreports/\nHaar feature extractions\nIn this section, we delve into the fascinating process of extracting Haar-like features to build a powerful cascade \nclassifier. By applying the Convolutional kernels depicted in Fig.  5 to the image, we unlock a realm of valuable \ninformation. Kernels 1 and 2 skillfully capture the essence of edge features, while kernels 3 and 4 go a step \nfurther, to capture both edge and diagonal features. This strategic combination of kernels equips our cascade \nclassifier with the ability to detect and distinguish these distinctive visual characteristics, ensuring reliable and \nprecise identification.\nFigure 3.  Collected traffic sign samples.\nFigure 4.  Challenging samples from the ICTS dataset.\nTable 1.  Distribution of collected potholes.\nEnvironmental condition Total images collected Samples used for training (80%) Samples used for validation (20%)\nDay Time 1700 1360 340\nRainy 1600 1280 320\nLow Light 1700 1360 340\nTotal 5000 4000 1000\n6\nVol:.(1234567890)Scientific Reports |         (2024) 14:2215  | https://doi.org/10.1038/s41598-024-52426-4\nwww.nature.com/scientificreports/\nObject detection using cascade classifier\nBoosting strategies, as a general approach, play a pivotal role in transforming weak learners from Section C into \nrobust learners. This technique empowers each newly constructed tree with an enhanced version of the original \ndataset. Initially, in the gradient boosting algorithm (gbm), a decision tree is trained, assigning equal weight to \neach observation. Following the evaluation of this primary tree, the weights of challenging-to-classify data points \nare increased, while the weights of easily classifiable observations are decreased. Consequently, the subsequent \nTable 2.  Distribution of collected traffic sign  symbols36.\nLabels # of images Training (80%) Testing (20%)\n200 m 508 406 81\n100–500 m 272 218 44\nBarrier ahead 288 230 46\nBroad wideness ahead 421 337 67\nCrossroad 523 418 84\nCattle 275 220 44\nCycle crossing 899 719 144\nDangerous dip 726 581 116\nFalling rocks 200 160 32\nFerry 369 295 59\nGap in median 788 630 126\nGuarded 200 m 274 219 44\nGuarded 50–100 m 272 218 44\nHump 666 533 107\nLeft-hand curve 495 396 79\nLeft hair pin bend 510 408 82\nLeft reverse bend 653 522 104\nLoose gravel 200 160 32\nMajor road ahead 775 620 124\nMen at work 413 330 66\nNarrow bridge 434 347 69\nNarrow road ahead 400 320 64\nPedestrian 2033 1626 325\nRight hair pin bend 739 591 118\nRight reverse bend 235 188 38\nRight-hand curve 912 730 146\nRound about 120 96 19\nSteep ascendant 912 730 146\nSteep descendant 420 336 67\nStaggered intersection 420 336 67\nSlippery road 382 306 61\nSide road right 382 306 61\nSide road left 1350 1080 216\nSchool ahead 288 230 46\nTraffic signal ahead 170 136 27\nUnguarded 200 m 382 306 61\nUnguarded 50–100 m 272 218 44\nY-intersection 272 218 44\nTotal 19,775 15,819 3163\nFigure 5.  Convolutional Kernels used to extract Haar features.\n7\nVol.:(0123456789)Scientific Reports |         (2024) 14:2215  | https://doi.org/10.1038/s41598-024-52426-4\nwww.nature.com/scientificreports/\ntree is constructed based on this weighted data, with the primary aim of improving the prediction accuracy of \nthe initial tree. The fusion of the primary and secondary trees gives birth to a novel model. Next, the classifica -\ntion error of the two-tree cascade model is measured, and a new tree (third) is developed to predict the modified \nresiduals. This iterative process is repeated for multiple epochs, wherein each successive tree aids in identifying \nobservations that were not well-categorized by the previous tree models. The essence of gradient boosting lies \nin training numerous models incrementally, conservatively, and chronologically. By utilizing gradients in the \nloss function, gbm effectively identifies the areas where improvement is needed. The loss function serves as a \nguiding measure to assess the accuracy of the model coefficients. The selection and design of the loss function \ndepend on the specific objectives and requirements of the problem at hand, dictating the conceptual perspective \nthrough which it is formulated and utilized. Figure 6 illustrates the process of training the cascade classifier.\nThe loss function is estimated based on the negative log probability that is converted to log.\nStep 1: Start the model with a constant\nwhere L indicates loss function and Y i and γ represents observed and expected value respectively. At this level, \nthe initial tree is to be constructed with a single leaf, after which trees with a larger depth are built. Usually, the \nmean Y i values are for regression and log values for classification.\nStep 2: for m = 1 to M {M indicates no of trees to construct}\nEquation (2) returns the negative gradient of each observation for all trees that form the expected values of the \nprevious classifier.\n(b) Add a regression tree to the γim values and generate terminal regions Rjm for j= 1, 2,... m .\nEquation (3) gets the cumulative predicted values of each terminal node of all tees with shrink loss function, as \nwell as the prediction of preceding learners.\nwhere η is the learning rate.\nStep 3: Get the output Fm (x).\nThus, gbm constructs the ultimate prediction by accumulating inputs from each tree.\nObject classification using vision transformers\nViTs, short for Vision Transformers, revolutionize the field of computer vision by leveraging the Transformer \nmodel, originally designed for natural language processing. By adopting this powerful architecture, ViTs have \ngarnered immense attention and have emerged as frontrunners in diverse image classification tasks, delivering \nstate-of-the-art  outcomes21. Figure 1 offers a comprehensive overview of Vision Transformers, illustrating their \n(1)F0(x) = arg min\n∑n\ni=1 L(Yi,γ)\n(2)(a) Compute γ im =−\n[ ∂ L(Yi,F(xi))\n∂ F(xi)\n]\nF(x)=F(m −1)(x)\nfor i= 1,... n\n(3)(c) For j= 1, 2,...m measure γ jm = arg min\n∑\nxi∈Rij\nL(Yi,Fm −1 (xi) + γ)\n(4)(d) Modernize Fm (x) = Fm −1 (x) + η\n∑Jm\nj=1 γjm I(x ∈R jm )\nFigure 6.  Training cascade classifier using gradient boosting method.\n8\nVol:.(1234567890)Scientific Reports |         (2024) 14:2215  | https://doi.org/10.1038/s41598-024-52426-4\nwww.nature.com/scientificreports/\nfunctionality. To process an input image, it is first partitioned into a grid of smaller regions known as patches. \nThese patches typically encompass a fixed number of pixels, such as 16 × 16 or 32 × 32. Subsequently, the patches \nare transformed into flattened structures and undergo linear projection, producing lower-dimensional feature \nvectors referred to as patch embeddings. These embeddings effectively encapsulate local information from dis-\ntinct areas of the image, enabling comprehensive analysis and understanding.\nThe embedded transformer receives a sequence of 1D token embedding as an input. In order to handle the \n2D images, the original image x is flattened into a series of reshaped 2D patches xP.\nHere, H and W indicate the height and width of the input image and C indicates the channel of the input image \n(either 1 or 3). (  P, P ) represents the height and width of each patch, N is the total obtained patches and it can \nbe computed by (7).\nThese patches are aid as the actual input for the transformer. Equation (8 ) is used to reshape the patches and \nproject them into D dimensions. The outcome of the projection is referred to as the patch embeddings.\nSimilar to the Transformer model in NLP , Vision Transformers incorporate positional information into the \ninput data. Positional embeddings represent the spatial relationship between different patches in the image. They \nencode the position and order of the patches and are added to the patch embeddings. The patch embeddings, \nalong with the positional embeddings, serve as input to a stack of Transformer encoder layers. Each encoder \nlayer consists of two sub-layers: the multi-head self-attention mechanism and the feed-forward neural network.\nThe self-attention mechanism allows the model to capture dependencies and relationships between different \npatches. It computes weighted sums of the patch embeddings, where the weights are determined by the similarity \nbetween patches. The attention mechanism attends to all patches simultaneously, enabling global context under-\nstanding. After self-attention, a feed-forward neural network is applied to each patch independently. This network \nconsists of fully connected layers, allowing non-linear transformations and feature extraction. The output of the \nTransformer encoder is a sequence of feature vectors, each representing a patch. To obtain a final classification, \na global average pooling is applied to aggregate the patch representations into a single vector. This vector is then \npassed through a fully connected layer with softmax activation to produce class probabilities for different labels.\nTraining a Vision Transformer involves optimizing the model parameters, including the patch embeddings \nand the weights of the Transformer encoder, using labeled data. This is typically done through techniques like \nstochastic gradient descent (SGD) and backpropagation, where the model’s predictions are compared to the \nground truth labels, and the gradients are computed to update the parameters. During inference, a trained Vision \nTransformer can take an input image, extract patch embeddings, pass them through the Transformer encoder, \nand produce class probabilities for image classification tasks.\nThe proposed algorithm\nThe following algorithm illustrates the procedure of the proposed method for traffic sign and pothole detection \nusing cascade classifier with the vision transformer.\n(5)x∈ RH×W ×C\n(6)xp ∈ RN × (P2·C)\n(7)N = HW\nP2\n(8)z0 =\n[\nXclass;X 1\npE;X 2\npE;...; X N\np E\n]\n+ Epos Where, E ∈ R(P2·C )×D Epos ∈ R(N +1 )× D\n9\nVol.:(0123456789)Scientific Reports |         (2024) 14:2215  | https://doi.org/10.1038/s41598-024-52426-4\nwww.nature.com/scientificreports/\n----------------------------------------------------------------------------------------------------------------------------------------------------------\nFor each layer, let Cf and d be the maximum false classification and minimum detection rate respectively. Ft is the final false \nclassification rate;\nLet P and N are positive and negative images set;\nInitialize D0=1 and F0=1 for the present layer; {D0: Detection rate, F0: False positive rate}\nSet x=0; { x indicates the number of layers}\nWhile Fx > Ft do :\nx=x+1;\nFx= Fx-1;\nSet nx=0;{ nx is used to  indicate nth weak classifier in xth layer.}\nWhile Fx > Cf * Fx-1 do :\nnx=nx + 1;\nTo train a strong classifier with nx weak classifiers on positive P and Negative N, an optimized \nadaptive boosting algorithm is used;\nCompute Dx and Fx of the cascade classifier;\nWhile Dx < d * Dx-1 do :\nAdjust the threshold of xth strong classifier;\nCompute Dx and Fx of the cascade classifier;\nEnd\nEnd\nInitialize N → Empty;\nUpdate N with false classification images.\nEnd\nReturns a Strong Cascade Classifierss\nBegin   // Vision Transformers\n//Initialize the Vision Transformer model\nInitialize_vision_transformer_model()\n//Load the image\nload_image()\n//Preprocess the image\npreprocess_image(image)\n// Use the Vision Transformer for feature extraction\nvision_transformer_model(preprocessed_image)\n//Initialize the object detection head\ninitialize_object_detection_head()\n//Forward pass through the object detection head\nobject_detection_head(features)\n//Post-process the detection results\npost_process_detection_results(detection_results)\n//Visualize or use the final detection results\nvisualize_results(final_detections)\nEnd\n------------------------------------------------------------------------------------------------------------------------------------------------------- \nAlgorithm 1.  Traffic sign and pothole detection using cascade classifier with vision transformer\nResults and discussions\nThis experiment is conducted on the Windows 10 platform, featuring 64 GB of RAM, an 8 GB NVIDIA RTX 4000 \nGPU, and a 3.60 GHz processor. The proposed methodology is implemented using Python programming. The \ncollected images of potholes and traffic signs are organized, as detailed in Tables  1 and 2, respectively. To train \nthe model, 80% of the dataset is employed, while the remaining 20% is reserved for testing. The Cascade classifier \nensemble, incorporating a sequence of nine gradient boosting techniques, is trained to detect pothole and traffic \nsign objects in road images, with bounding boxes delineating their locations. The parameters utilized for train-\ning the cascade classifier are outlined in Table 3. Subsequently, vision transformers are employed to predict the \nspecific categories of the identified pothole and traffic sign objects. The parameters used for training the vision \ntransformers are presented in Table 4. To appraise the efficacy of the proposed model, accuracy, recall, and Mean \nTable 3.  Cascade classifier parameters. \nImage size Gradient boosting levels Scale factor Minimum neighbours Max FAR Minimum hit rate\n64 × 64 9 2.0 4 0.27 0.997\n10\nVol:.(1234567890)Scientific Reports |         (2024) 14:2215  | https://doi.org/10.1038/s41598-024-52426-4\nwww.nature.com/scientificreports/\nAverage Precision (mAP) as performance/evaluation metrics are considered for the taken datasets. These metrics \nare then compared to those obtained from other cutting-edge techniques such as YOLOv3, YOLOv4, Faster \nRCNN, and SSD. This comparison is carried out to determine which system achieves the highest accuracy, recall, \nand mAP . This comparison aids in determining the most effective method for detecting potholes and traffic signs.\nFigures 7 and 8 display the confusion matrices for pothole and traffic sign predictions, respectively. A confu-\nsion matrix is a performance measurement tool used in machine learning and classification tasks to evaluate the \naccuracy of a model. It’s particularly useful when dealing with supervised learning algorithms where the output \nis categorical. The matrix itself is a table layout that allows visualization of the performance of an algorithm. It \ntypically has four sections: True Positive (TP), True Negative (TN), False Positive (FP) and False Negative (FN). It \nis evident from Fig. 7 that the proposed strategy accurately predicted the potholes on the road. Likewise, Fig.  8 \ndemonstrates the traffic sign prediction through the proposed model and notably, the TP rate is higher than the \nother three metrics. For both scenarios, the evaluation provides insights into the model’s performance. Using \nthese components, the confusion matrix provides a comprehensive view of how well a classification model per-\nforms by summarizing the model’s predictions against the actual outcomes. It helps in understanding where the \nmodel is making mistakes, such as misclassifying one class as another.\nTable 4.  Vision transformer parameters.\nHyper parameter Value\nLearning_Rate 0.001\nWeight_Decay 0.0001\nBatch_Size 256\nNum_Epochs 10\nImage_Size 72\nPatch_Size 6\nNum_Patches (image_size // patch_size) ** 2\nProjection_Dim 64\nNum_Heads 4\nTransformer_Layers_Size 8\nFigure 7.  Confusion matrix of pothole prediction.\n11\nVol.:(0123456789)Scientific Reports |         (2024) 14:2215  | https://doi.org/10.1038/s41598-024-52426-4\nwww.nature.com/scientificreports/\nFigure  9 presents the Mean Average Precision (mAP) graphs generated during the model training. The \nevaluation of the trained model includes assessment using metrics such as precision, recall, and mAP . For \ninstance, accuracy measures the overall correctness of predictions, while precision and recall focus on specific \naspects like correctly identifying positive cases (presence of a sign or pothole). These metrics help in assessing \nthe effectiveness of the models in identifying road signs or potholes, which is crucial for improving road safety \nand infrastructure maintenance.\nThe proposed model is compared with other state-of-the-art techniques, including  YOLOv322,  YOLOv423, \nFaster  RCNN24, and  SSD25, demonstrating superior accuracy. Figure 10 showcases the comparative results on the \n ICTS26-based traffic sign dataset, while Fig.  11 provides a comparative analysis of the proposed model against \nother existing models on the  GTSRB27-based traffic sign dataset. Additionally, Figs.  12 and 13 depict the com-\nparative analysis of the proposed method with other object detection models on the  KAGGLE28 and  CCSAD29 \ndatasets, respectively. It is apparent from these figures that the proposed approach outperforms existing strategies \nin terms of mAP , precision, and recall. The fusion of the cascade classifier with the vision transformer facilitates \nthe inherent capabilities of the vision transformer in capturing global context, learning fine details, and adapt -\nability to various conditions making them promising candidates for improving accuracy and robustness in traffic \nsign and pothole detection systems.\nFigure 8.  Confusion matrix of traffic sign prediction.\nFigure 9.  Precision, recall, and mean average precision (mAP) plots.\n12\nVol:.(1234567890)Scientific Reports |         (2024) 14:2215  | https://doi.org/10.1038/s41598-024-52426-4\nwww.nature.com/scientificreports/\nAs depicted in Table 5, it is evident that the proposed methodology surpassed other methods, achieving a \nprecision value of 97.90%, a recall of 95.69%, and a mean Average Precision of 97.14%. All methods highlighted \nin Table 5 underwent training and evaluation on the ICTS dataset. Moving on to Table 6, a comparative analysis \nof the proposed method on the GTSRDB benchmark dataset reveals that it achieved the highest detection accu-\nracy among other object detection models, with a mean Average Precision of 98.57%. Tables  7 and 8 provide a \ncomparative analysis of the proposed method with other state-of-the-art object detection models on the KAGGLE \nFigure 10.  Predicting potholes in different conditions on the CCSAD dataset.\n13\nVol.:(0123456789)Scientific Reports |         (2024) 14:2215  | https://doi.org/10.1038/s41598-024-52426-4\nwww.nature.com/scientificreports/\nand CCSAD pothole datasets. Across both pothole datasets, the proposed model demonstrates superior detection \nperformance, yielding a mean Average Precision of 97.27% and 97.17%, respectively.\nFigure 10 shows the detection results of potholes under illumination and tree shadow conditions. All the \npotholes are detected with higher prediction scores and tiny potholes are also detected with the proposed meth-\nodology. Figure 11 depicts the detection results of the potholes filled with water (wet potholes). The proposed \nmethod detects all the water-filled potholes with the higher prediction score and with less training time.\nFigure 12 illustrates the prediction outcomes for traffic signs under night time conditions. Notably, traffic \nsigns such as hump, pedestrian, 200 m ahead, crossroad, gap in median, narrow bridge, crossroad, men at work, \ny-intersection, gap in median, crossroad, hump, 200 m ahead, hump, pedestrian, and a gap in median exhibit \nhigher prediction accuracy under various illumination conditions. Moving to Fig. 13, the prediction results focus \non perspective traffic signs. Existing methods falter in detecting these signs, but the proposed model excels in \naccurately identifying them.\nThe proposed approach successfully identifies both potholes and traffic signs, even in challenging conditions. \nIn particular, water-filled potholes pose a detection challenge for Faster RCNN and SSD. Although YOLOv3 and \nYOLOv4 detect some of these water-filled potholes, they struggle with tiny or small ones. Potholes obscured by \ntree shadows are detected by Faster RCNN, SSD, YOLOv3, and YOLOv4, but with a high false detection rate. \nIn contrast, the proposed method achieves comprehensive detection of all potholes with high accuracy and \nsignificantly reduces false positives.\nRegarding images of perspective traffic signs, Faster RCNN, SSD, and YOLOv3 encounter challenges in \ndetection. While YOLOv4 successfully identifies a limited number of slightly angled perspective images, the \nproposed method outperforms by precisely recognizing all perspective images with superior detection accuracy.\nFigure  14 illustrates the comparative assessment of the proposed method concerning the training time \nrequired to train the model. The suggested model exhibits a shorter training duration in comparison to the \nother methods highlighted in Fig. 14.\nThis efficiency stems from the fact that the proposed model doesn’t rely on image-specific biases, thanks to \nits utilization of multi-head self-attention. The model dissects images into a sequence of positional embedding \npatches, processed by the transformer encoder, thereby capturing both regional and global features of the image. \nUltimately, when applied to datasets derived from ICTS, GTSRDB, KAGGLE, and CCSAD, the proposed model \ndemonstrates superior accuracy with a reduced training time. The achieved mean Average Precision (mAP) of \n97.14% for traffic sign detection and 98.27% for pothole detection surpasses benchmarks set by leading tech-\nniques such as YOLOv3, YOLOv4, Faster RCNN, and SSD, showcasing the effectiveness of the proposed method-\nology. Notably, our model showcases remarkable proficiency in accurately predicting potholes concealed in tree \nshadows, affected by varying illumination conditions, or filled with water, all achieved with a higher accuracy rate \nand reduced training time. Furthermore, our model exhibits exceptional competence in predicting traffic signs \nunder challenging conditions like illumination variations, perspective distortions, and blurriness. The system’s \nability to recognize water-filled potholes and illuminated traffic signs, as well as handling perspective distor -\ntions and tree shading, marks a significant stride in improving safety under diverse environmental conditions.\nFigure 11.  Predicting potholes filled with water.\n14\nVol:.(1234567890)Scientific Reports |         (2024) 14:2215  | https://doi.org/10.1038/s41598-024-52426-4\nwww.nature.com/scientificreports/\nConclusion\nIn this innovative study, we introduce a groundbreaking approach that transforms the landscape of predicting \npotholes and traffic signs on Indian roads. Our methodology initiates with the creation of a cascade classi-\nfier, adept at pinpointing the exact location of potholes and traffic sign objects, skillfully outlining bounding \nboxes around each identified entity. Taking a leap forward, we employ the state-of-the-art vision transformer to \nprecisely forecast the specific class of potholes and traffic signs, pushing the boundaries of detection accuracy. \nThorough training and evaluation of our model are conducted on prominent datasets, including ICTS, GTSRDB, \nKAGGLE, and CCSAD, utilizing quantitative metrics such as precision, recall, and mean Average Precision \n(mAP). Compared to state-of-the-art techniques like YOLOv3, YOLOv4, Faster RCNN, and SSD, the method \nachieves impressive recognition with a mAP of 97.14% for traffic sign detection and 98.27% for pothole detection.\nThe proposed model’s performance in predicting potholes filled with water, navigating through challenges \nlike obscured visibility due to tree shadows, or coping with changing lighting conditions represents a significant \nstride forward in road safety technology. Detecting water-filled potholes is particularly critical, as these can be \nhighly hazardous and challenging to identify, especially when combined with shadows or varying light condi-\ntions. The proposed model’s ability to discern these obscured or challenging instances substantially enhances \nroad safety. Moreover, the system’s remarkable proficiency in predicting traffic signs under adverse conditions \nFigure 12.  Predicting traffic signs at Illumination conditions.\n15\nVol.:(0123456789)Scientific Reports |         (2024) 14:2215  | https://doi.org/10.1038/s41598-024-52426-4\nwww.nature.com/scientificreports/\nFigure 13.  Predicting perspective traffic signs.\nTable 5.  Comparative analysis on ICTS based dataset.\nMethodology Precision (%) Recall (%) mAP@0.5 (%)\nYOLOv3 94.77 91.72 92.50\nYOLOv4 95.35 94.64 95.07\nFaster RCNN 93.73 92.46 93.35\nSSD 95.52 94.79 95.89\nProposed model 97.90 95.69 97.14\nTable 6.  Comparative analysis of the GTSRB dataset. \nMethodology Precision (%) Recall (%) mAP@0.5 (%)\nYOLOv3 93.47 93.26 95.57\nYOLOv4 94.14 93.74 96.21\nFaster RCNN 94.23 94.60 95.07\nSSD 95.19 95.97 97.55\nProposed model 98.09 97.64 98.57\n16\nVol:.(1234567890)Scientific Reports |         (2024) 14:2215  | https://doi.org/10.1038/s41598-024-52426-4\nwww.nature.com/scientificreports/\nlike fluctuating lighting, perspective distortions, and blurriness significantly elevates its reliability in real-world \nscenarios. These conditions are commonly encountered on roads where factors like changing weather, time of \nday, or camera perspectives can impact the quality of visual data. Additionally, the model’s adaptability to adjust \nto perspective distortions and identify traffic signs under tree shade enhances its versatility in handling diverse \nenvironmental scenarios. It has the potential to revolutionise automated systems’ ability to perceive and respond \neffectively to various road conditions, thereby reducing accidents and contributing substantially to the overall \nefficiency and safety of transportation infrastructure.\nWithstanding to aforementioned advantages, the combined cascade classifier and vision transformer system \nmay pose computational challenges, especially when deployed in real-time applications on resource-constrained \ndevices. Although the model is trained on diverse datasets, real-world variations may not be fully represented, \nnecessitating continuous updates and expansion of the training datasets. Implement adaptive learning mecha -\nnisms to enable the system to continually improve and adapt to evolving road conditions and emerging chal-\nlenges. Focus on optimizing the proposed approach for real-time implementation on edge devices, considering \nthe computational limitations of such platforms. The study lays a strong foundation for advancing pothole and \ntraffic sign detection, and ongoing research and development efforts can further refine and expand its applica-\ntions in enhancing road safety.\nTable 7.  Comparative analysis of the KAGGLE dataset.\nMethodology Precision (%) Recall (%) mAP@0.5 (%)\nYOLOv3 96.42 97.27 96.38\nYOLOv4 96.30 95.46 94.52\nFaster RCNN 95.58 96.10 96.22\nSSD 97.02 97.52 96.31\nProposed model 98.09 97.15 98.27\nTable 8.  Comparative analysis on the CCSAD dataset.\nMethodology Precision (%) Recall (%) mAP@0.5 (%)\nYOLOv3 95.77 94.06 96.50\nYOLOv4 97.35 96.27 94.07\nFaster RCNN 95.73 95.64 96.25\nSSD 95.52 95.57 95.19\nProposed model 98.23 95.69 97.17\n1.29\n2.09\n1.41\n2.18\n1.11\n0\n0.5\n1\n1.5\n2\n2.5\nYOLOv3Y OLOv4F aster \nRCNN\nSSDO ur \nModel\nTime(Hrs)\nMethods\nFigure 14.  Training time comparison.\n17\nVol.:(0123456789)Scientific Reports |         (2024) 14:2215  | https://doi.org/10.1038/s41598-024-52426-4\nwww.nature.com/scientificreports/\nData availability\nThe datasets used and/or analysed during the current study are available from the author (Satish Kumar Satti) \nupon reasonable request.\nReceived: 26 October 2023; Accepted: 18 January 2024\nReferences\n 1. Business Standard. [New Delhi, Dec 02 2019]. 2,015 pedestrians lost their lives due to potholes in 2018: Govt. https:// www. busin \ness- stand ard. com/ artic le/ curre nt- affai rs/2- 015- pedes trians- lost- their- lives- due- to- potho les- in- 2018- govt- 11912 02007 47_1. html.\n 2. The Times of India. [Dipak K Dash / TNN / Oct 12, 2019] States register steep decline in pothole-related deaths; experts skeptical. \nhttps:// times ofind ia. india times. com/ india/ states- regis ter- steep- decli ne- in- potho le- relat ed- deaths- exper ts- skept ical/ artic leshow/ \n71548 085. cms.\n 3. Patra, S., Middya, A. I. & Roy, S. PotSpot: Participatory sensing based monitoring system for pothole detection using deep learning. \nMultimedia Tools Appl. 80(16), 25171–25195 (2021).\n 4. Jin, Y . et al. Multi-feature fusion and enhancement single shot detector for traffic sign recognition. IEEE Access 8, 38931–38940. \nhttps:// doi. org/ 10. 1109/ ACCESS. 2020. 29758 28 (2020).\n 5. Li, X. et al. Traffic sign detection based on improved faster R-CNN for autonomous driving. J. Supercomput. 25, 1–21 (2022).\n 6. Satti, S. K. et al. A machine learning approach for detecting and tracking road boundary lanes. ICT Express 7(1), 99–103 (2021).\n 7. Varona, B., Monteserin, A. & Teyseyre, A. A deep learning approach to automatic road surface monitoring and pothole detection. \nPers. Ubiquit. Comput. https:// doi. org/ 10. 1007/ s00779- 019- 01234-z (2019).\n 8. Dhiman, A. & Klette, R. Pothole detection using computer vision and learning. IEEE Trans. Intell. Transp. Syst. 21(8), 3536–3550. \nhttps:// doi. org/ 10. 1109/ TITS. 2019. 29312 97 (2020).\n 9. Satti, S. K., Maddula, P . & Vishnumurthy Ravipati, N. V . Unified approach for detecting traffic signs and potholes on Indian roads. \nJ. King Saud Univ. Comput. Inf. Sci. https:// doi. org/ 10. 1016/j. jksuci. 2021. 12. 006 (2021).\n 10. Sawalakhe, H. & Prakash, R. Development of roads pothole detection system using image processing. In Intelligent embedded \nsystems. Lecture notes in electrical engineering Vol. 492 (eds Thalmann, D. et al.) (Springer, 2018). https:// doi. org/ 10. 1007/ 978- \n981- 10- 8575-8_ 20.\n 11. Chen, H., Y ao, M. & Gu, Q. Pothole detection using location-aware convolutional neural networks. Int. J. Mach. Learn. Cybern.  \n11, 899–911. https:// doi. org/ 10. 1007/ s13042- 020- 01078-7 (2020).\n 12. Bansal, K. et al. DeepBus: Machine learning based real time pothole detection system for smart transportation using IoT. Internet \nTechnol. Lett. 3(3), e156 (2020).\n 13. Rehman, Y ., Amanullah, H., Shirazi, M. A. & Kim, M. Y . Small traffic sign detection in big images: Searching needle in a hay. IEEE \nAccess 10, 18667–18680. https:// doi. org/ 10. 1109/ ACCESS. 2022. 31508 82 (2022).\n 14. Wang, Z., Wang, J., Li, Y . & Wang, S. Traffic sign recognition with lightweight two-stage model in complex scenes. IEEE Trans. \nIntell. Transp. Syst. 23(2), 1121–1131. https:// doi. org/ 10. 1109/ TITS. 2020. 30205 56 (2022).\n 15. Wang, L., Zhou, K., Chu, A., Wang, G. & Wang, L. An improved light-weight traffic sign recognition algorithm based on YOLOv4-\ntiny. IEEE Access 9, 124963–124971. https:// doi. org/ 10. 1109/ ACCESS. 2021. 31097 98 (2021).\n 16. Cao, J., Zhang, J. & Jin, X. A traffic-sign detection algorithm based on improved sparse R-CNN. IEEE Access  9, 122774–122788. \nhttps:// doi. org/ 10. 1109/ ACCESS. 2021. 31096 06 (2021).\n 17. Tang, Q., Cao, G. & Jo, K.-H. Integrated feature pyramid network with feature aggregation for traffic sign detection. IEEE Access \n9, 117784–117794. https:// doi. org/ 10. 1109/ ACCESS. 2021. 31063 50 (2021).\n 18. Satti, S. K. R-ICTS: Recognize the Indian cautionary traffic signs in real-time using an optimized adaptive boosting cascade clas -\nsifier and a convolutional neural network. Concurr. Comput. Pract. Exp. 34(10), e6796 (2022).\n 19. Y ang, Y ., Luo, H., Xu, H. & Wu, F . Towards real-time traffic sign detection and classification. IEEE Trans. Intell. Transp. Syst. 17(7), \n2022–2031. https:// doi. org/ 10. 1109/ TITS. 2015. 24824 61 (2016).\n 20. Jones, M. & Viola, P . Fast multi-view face detection. Mitsubishi Electric Research Lab TR-20003-96 3.14 (2003): 2.\n 21. Dosovitskiy, A., et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv: 2010.  \n11929 (2020).\n 22. Redmon, J. & Farhadi, A. Y olov3: An incremental improvement. arXiv preprint arXiv: 1804. 02767 (2018).\n 23. Bochkovskiy, A., Wang, C.-Y . & Mark Liao, H.-Y . Y olov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv: \n2004. 10934 (2020).\n 24. Ren, S., et al. Faster R-CNN: Towards real-time object detection with region proposal networks. arXiv preprint arXiv: 1506. 01497 \n(2015).\n 25. Liu, W ., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y . & Berg, A. C. SSD: Single shot multibox detector. In European \nConference on Computer Vision 21–37 (Springer, 2016).\n 26. Satti, S. K. & Suganya Devi, K. Indian cautionary traffic sign data-set. IEEE Dataport (2020).\n 27. Houben, S., Stallkamp, J., Salmen, J., Schlipsing, M. & Igel, C. Detection of traffic signs in real-world images: The German traffic \nsign detection benchmark. In Proceedings of the International Joint Conference on Neural Networks (2013). https:// doi. org/ 10. 1109/ \nIJCNN. 2013. 67068 07.\n 28. Nienaber, S., Booysen, M. J. & Kroon, R. S. Detecting potholes using simple image processing techniques and real-world footage \n(2015).\n 29. Guzmán, R., Hayet, J.-B. & Klette, R. Towards ubiquitous autonomous driving: The CCSAD dataset. In International Conference \non Computer Analysis of Images and Patterns (Springer, 2015).\n 30. Y assine, S. & Stanulov, A. A comparative analysis of machine learning algorithms for the purpose of predicting Norwegian air \npassenger traffic. Int. J. Math. Stat. Comput. Sci. 2, 28–43 (2024).\n 31. Singh, P . et al. W-GeoR: Weighted geographical routing for V ANET’s health monitoring applications in urban traffic networks. \nIEEE Access 10, 38850–38869 (2021).\n 32. Kashinath, S. A. et al. Review of data fusion methods for real-time and multi-sensor traffic flow analysis. IEEE Access 9, 51258–\n51276 (2021).\n 33. Mohd, N. A. et al. Vehicles counting from video stream for automatic traffic flow analysis systems. Int. J. 8(11), 25 (2020).\n 34. Alotaibi, M. F ., Omri, M., Abdel-Khalek, S., Khalil, E. & Mansour, R. F . Computational intelligence-based harmony search algorithm \nfor real-time object detection and tracking in video surveillance systems. Mathematics  10(5), 733 (2022).\n 35. Vijayalakshmi, B. et al. An attention-based deep learning model for traffic flow prediction using spatiotemporal features towards \nsustainable smart city. Int. J. Commun. Syst. 34(3), e4609 (2021).\n 36. Satti, S. K., Suganya Devi, K., Sekar, K., Dhar, P . & Srinivasan, P . ICTS: Indian cautionary traffic sign classification using deep learn-\ning. In 2022 IEEE International Conference on Distributed Computing and Electrical Circuits and Electronics (ICDCECE), Ballari, \nIndia (2022), 1–7. https:// doi. org/ 10. 1109/ ICDCE CE539 08. 2022. 97929 96.\n18\nVol:.(1234567890)Scientific Reports |         (2024) 14:2215  | https://doi.org/10.1038/s41598-024-52426-4\nwww.nature.com/scientificreports/\nAuthor contributions\nS.K.S.: Data Curation, Writing—Original draft preparation, Coding, Investigation, Experimentation. G.N.V .R.: \nMethodology, Software, Coding, Investigation, Experimentation. K.M.: Writing—Reviewing and Editing, Vali-\ndation. A.H.G.: Conceptualization, Supervision, Visualization, Validation, Writing—Reviewing and Editing.\nFunding\nOpen access funding provided by Óbuda University.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to A.H.G.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2024"
}