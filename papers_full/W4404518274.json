{
    "title": "Autonomous International Classification of Diseases Coding Using Pretrained Language Models and Advanced Prompt Learning Techniques: Evaluation of an Automated Analysis System Using Medical Text",
    "url": "https://openalex.org/W4404518274",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A1978783002",
            "name": "Yan Zhuang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2102348571",
            "name": "Junyan Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2615549568",
            "name": "Xiuxing Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2097201696",
            "name": "Chao Liu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2096181728",
            "name": "Yue Yu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1903192020",
            "name": "Wei Dong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2309971216",
            "name": "Kunlun He",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1507651605",
        "https://openalex.org/W1997057722",
        "https://openalex.org/W2998230683",
        "https://openalex.org/W4283687125",
        "https://openalex.org/W2659296870",
        "https://openalex.org/W3183592839",
        "https://openalex.org/W2996403675",
        "https://openalex.org/W3184458532",
        "https://openalex.org/W4377091639",
        "https://openalex.org/W4302617179",
        "https://openalex.org/W2971274815",
        "https://openalex.org/W4288088047",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4307217099",
        "https://openalex.org/W4282983782",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3131999495",
        "https://openalex.org/W4320065330",
        "https://openalex.org/W4386187806",
        "https://openalex.org/W4287902301",
        "https://openalex.org/W4385757404",
        "https://openalex.org/W3119438769",
        "https://openalex.org/W3205270560",
        "https://openalex.org/W4309444617",
        "https://openalex.org/W3166699508",
        "https://openalex.org/W4287278250",
        "https://openalex.org/W4288260502",
        "https://openalex.org/W3172718868",
        "https://openalex.org/W3126960149",
        "https://openalex.org/W3212368093",
        "https://openalex.org/W4287631906",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2965276171",
        "https://openalex.org/W4365511667",
        "https://openalex.org/W4393156091",
        "https://openalex.org/W4366557491"
    ],
    "abstract": "Background Machine learning models can reduce the burden on doctors by converting medical records into International Classification of Diseases (ICD) codes in real time, thereby enhancing the efficiency of diagnosis and treatment. However, it faces challenges such as small datasets, diverse writing styles, unstructured records, and the need for semimanual preprocessing. Existing approaches, such as naive Bayes, Word2Vec, and convolutional neural networks, have limitations in handling missing values and understanding the context of medical texts, leading to a high error rate. We developed a fully automated pipeline based on the Key–bidirectional encoder representations from transformers (BERT) approach and large-scale medical records for continued pretraining, which effectively converts long free text into standard ICD codes. By adjusting parameter settings, such as mixed templates and soft verbalizers, the model can adapt flexibly to different requirements, enabling task-specific prompt learning. Objective This study aims to propose a prompt learning real-time framework based on pretrained language models that can automatically label long free-text data with ICD-10 codes for cardiovascular diseases without the need for semiautomatic preprocessing. Methods We integrated 4 components into our framework: a medical pretrained BERT, a keyword filtration BERT in a functional order, a fine-tuning phase, and task-specific prompt learning utilizing mixed templates and soft verbalizers. This framework was validated on a multicenter medical dataset for the automated ICD coding of 13 common cardiovascular diseases (584,969 records). Its performance was compared against robustly optimized BERT pretraining approach, extreme language network, and various BERT-based fine-tuning pipelines. Additionally, we evaluated the framework’s performance under different prompt learning and fine-tuning settings. Furthermore, few-shot learning experiments were conducted to assess the feasibility and efficacy of our framework in scenarios involving small- to mid-sized datasets. Results Compared with traditional pretraining and fine-tuning pipelines, our approach achieved a higher micro–F1-score of 0.838 and a macro–area under the receiver operating characteristic curve (macro-AUC) of 0.958, which is 10% higher than other methods. Among different prompt learning setups, the combination of mixed templates and soft verbalizers yielded the best performance. Few-shot experiments showed that performance stabilized and the AUC peaked at 500 shots. Conclusions These findings underscore the effectiveness and superior performance of prompt learning and fine-tuning for subtasks within pretrained language models in medical practice. Our real-time ICD coding pipeline efficiently converts detailed medical free text into standardized labels, offering promising applications in clinical decision-making. It can assist doctors unfamiliar with the ICD coding system in organizing medical record information, thereby accelerating the medical process and enhancing the efficiency of diagnosis and treatment.",
    "full_text": null
}