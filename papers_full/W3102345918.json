{
  "title": "Joint Learning with Pre-trained Transformer on Named Entity Recognition and Relation Extraction Tasks for Clinical Analytics",
  "url": "https://openalex.org/W3102345918",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2099349455",
      "name": "Miao Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2162717862",
      "name": "Ganhui Lan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2000693772",
      "name": "Fang Du",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2751360390",
      "name": "Victor Lobanov",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2937845937",
    "https://openalex.org/W2964167098",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W2738031524",
    "https://openalex.org/W2963613359",
    "https://openalex.org/W3032007299",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2922551710",
    "https://openalex.org/W2992412259",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W1934019294",
    "https://openalex.org/W2977316918",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W2799125718",
    "https://openalex.org/W4302585900",
    "https://openalex.org/W2972483465",
    "https://openalex.org/W1623072288",
    "https://openalex.org/W2997712488",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2963997908",
    "https://openalex.org/W2954268854",
    "https://openalex.org/W2741956709"
  ],
  "abstract": "In drug development, protocols define how clinical trials are conducted, and are therefore of paramount importance. They contain key patient-, investigator-, medication-, and study-related information, often elaborated in different sections in the protocol texts. Granular-level parsing on large quantity of existing protocols can accelerate clinical trial design and provide actionable insights into trial optimization. Here, we report our progresses in using deep learning NLP algorithms to enable automated protocol analytics. In particular, we combined a pre-trained BERT transformer model with joint-learning strategies to simultaneously identify clinically relevant entities (i.e. Named Entity Recognition) and extract the syntactic relations between these entities (i.e. Relation Extraction) from the eligibility criteria section in protocol texts. When comparing to standalone NER and RE models, our joint-learning strategy can effectively improve the performance of RE task while retaining similarly high NER performance, likely due to the synergy of optimizing toward both tasks’ objectives via shared parameters. The derived NLP model provides an end-to-end solution to convert unstructured protocol texts into structured data source, which will be embedded into a comprehensive clinical analytics workflow for downstream trial design missions such like patient population extraction, patient enrollment rate estimation, and protocol amendment prediction.",
  "full_text": "Proceedings of the 3rd Clinical Natural Language Processing Workshop, pages 234–242\nNovember 19, 2020.c⃝2020 Association for Computational Linguistics\n234\nJoint Learning with Pre-trained Transformer on Named Entity\nRecognition and Relation Extraction Tasks for Clinical Analytics\nMiao Chen\nCovance\nmiao.chen\n@covance.com\nGanhui Lan\nJanssen\nganhuilan\n@gmail.com\nFang Du\nCovance\nfangdu64\n@gmail.com\nVictor Lobanov\nCovance\nvictor.lobanov\n@covance.com\nAbstract\nIn drug development, protocols deﬁne how\nclinical trials are conducted, and are therefore\nof paramount importance. They contain key\npatient-, investigator-, medication-, and study-\nrelated information, often elaborated in differ-\nent sections in the protocol texts. Granular-\nlevel parsing on large quantity of existing pro-\ntocols can accelerate clinical trial design and\nprovide actionable insights into trial optimiza-\ntion. Here, we report our progresses in us-\ning deep learning NLP algorithms to enable\nautomated protocol analytics. In particular,\nwe combined a pre-trained BERT transformer\nmodel with joint-learning strategies to simul-\ntaneously identify clinically relevant entities\n(i.e. Named Entity Recognition) and extract\nthe syntactic relations between these entities\n(i.e. Relation Extraction) from the eligibility\ncriteria section in protocol texts. When com-\nparing to standalone NER and RE models, our\njoint-learning strategy can effectively improve\nthe performance of RE task while retaining\nsimilarly high NER performance, likely due to\nthe synergy of optimizing toward both tasks’\nobjectives via shared parameters. The derived\nNLP model provides an end-to-end solution to\nconvert unstructured protocol texts into struc-\ntured data source, which will be embedded\ninto a comprehensive clinical analytics work-\nﬂow for downstream trial design missions such\nlike patient population extraction, patient en-\nrollment rate estimation, and protocol amend-\nment prediction.\n1 Introduction\nClinical trial protocols, often called “study proto-\ncols” or just “protocols”, are the foundational docu-\nments that specify the detailed plans of conducting\nclinical trials to validate the safety and/or efﬁcacy\nof drugs. They contain key information about the\ntargeted disease indications, the eligible patients,\nthe investigated medication, the visit schedules, and\nthe treatment endpoints etc. Across the entire life-\ncycle of clinical trials starting from study design &\nplanning to data analysis & publication, it is always\ncritical to comprehend this information accurately\nand unambiguously. However, since protocols are\nmainly unstructured or semi-structured texts (i.e.\nnatural languages), application of computer-aided\ninformation extraction is challenging and thus lim-\nited. Current protocol analytic practices are labour-\nand time-intensive, involving numerous manual re-\nsource checking and cross referencing activities.\nThe pressing needs of reducing the costs and boost-\ning the speed of drug development have created an\nindustry-wide demand in developing a more efﬁ-\ncient, effective, and scalable mechanism to process\ntext-based protocols.\nTo address the above demand, we present in this\npaper our efforts and progresses in developing a\ndeep learning Natural Language Processing (NLP)\napproach to extract clinically relevant information\nfrom protocols. In particular, we targeted two tasks:\nNamed Entity Recognition (NER) and Relation Ex-\ntraction (RE), and transferred the Bidirectional En-\ncoder Representations from Transformers model\n(BERT, a pre-trained transformer NLP model) via a\njoint-learning strategy to extract clinically relevant\nentities and their syntactic relationships simulta-\nneously by training on our in-house clinical trial\nprotocol corpus.\nIn alignment with the industry’s patient-centric\nbusiness emphasis, we focused this work on ex-\ntracting the patient eligibility information from the\n“Eligibility Criteria” section in the protocols, which\nunambiguously determines whether a patient could\nbe included in or excluded from the clinical trial.\nThis is particularly important because patient re-\ncruitment is an essential and currently rate-limiting\nstep in clinical trials. Accurate parsing of this part\nof protocols can facilitate quick identiﬁcation of\neligible patients as well as other clinical analytics\n235\nmissions.\nClinical trial protocols are a type of profes-\nsional documents with rigorous and highly domain-\nspeciﬁc terms associated via complex yet precise\nrelations. Like other professionally developed doc-\numents, protocols have to pass multiple quality-\ncontrol checkpoints, and thus require less pre-\nprocessing (e.g. text correcting/cleaning) than\nmany other types of documents such as social me-\ndia posts before submitting to NLP models. On the\nother hand, the domain-speciﬁc nature of protocols\nrequires extra attention when transferring models\ntrained from generic or other professional domains.\nTo elaborate, a protocol contains many clinical and\nmedical terms (e.g. medications and diseases etc.)\nthat are not commonly seen in other domains, but\nthose are exactly the entities that our model needs\nto recognize; furthermore, it is also challenging\nthat the entities may be connected in dramatically\ndifferent ways under different domain-speciﬁc con-\ntexts. For example, in the clinical domain, the word\n“trial” refers to “clinical trial” that is associated\nwith “patients”, “diseases”, and “medicines” etc.;\nwhereas in the legal or even generic domains, “trial”\ncommonly means “legal trial” that is frequently\nconnected to “jury”, “prosecutor” and “defendant”\netc. Therefore, the success of the transfer-learning\nlargely relies on maximizing domain-speciﬁc “gra-\ndients” for ﬁne-tuning the model parameters.\nThis presented work is continued from our recent\nstudy on clinical protocols, in which we developed\nstandalone BERT-based NLP models for NER and\nRE tasks for processing the “Eligibility Criteria”\nsection in protocols (Chen et al., 2020).\nBased on the observation that different clinically\nrelevant entities are not equally involved in all rela-\ntions, we hypothesized that by combining the NER\nand RE tasks in the same BERT network via a joint-\nlearning strategy, the textures of clinical trials may\nbecome more visible for training and thus improve\nthe performance of both tasks. As will be shown in\nthe later sections of this paper, our results validated\nthis hypothesis and showed that the joint-learning\nmodel can provide signiﬁcant improvement over\nstandalone models.\nThis improved model is being embedded into an\nautomated pipeline that aims to accelerate the cur-\nrent manual process of identifying similar clinical\ntrials from the historical protocols, and to stream-\nline the querying process of identifying potentially\neligible patients for clinical trials.\n2 Related Work\nNER and RE are two classic NLP tasks that have\nbeen studied separately for decades. In its ear-\nlier developments, NER, as a sequence labeling\ntask, has mainly employed probabilistic sequence\nlabeling techniques such as conditional random\nﬁelds (CRF), maximum entropy Markov models,\nand hidden Markov models (Lafferty et al., 2001;\nMcCallum et al., 2000; Bikel et al., 1998). More\nrecently, researchers have started using deep learn-\ning family of algorithms to capture the transitions\nbetween hidden states for NER tasks, including re-\ncurrent neural networks (RNN), bidirectional long\nshort-term memory (BiLSTM) together with CRF.\nLately, pre-trained transformer models, with BERT\nas a prominent example, have been developed and\nused to represent contextual embeddings of text\nand gained great success in NER tasks along with\nother NLP tasks (Devlin et al., 2018; Lee et al.,\n2019).\nWith regard to RE, it is usually treated as a\ntext classiﬁcation between the interested entity\npairs. Many classiﬁcation algorithms, such as sup-\nport vector machine, logistic regression, perceptron\netc., have been applied to this problem (Bach and\nBadaskar, 2007; Jurafsky, 2000). Similar to NER,\nthe latest developments in solving RE tasks have\nalso employed deep learning algorithms using neu-\nral network models to emulate entity relations with\ncomponents such as attention, biafﬁne, and bidirec-\ntional tree-structured LSTM-RNNs (Nguyen and\nVerspoor, 2019; Wang et al., 2019a; Miwa and\nBansal, 2016). Pre-trained models were also used\nto provide contextualized encoding information to\nthe neural network layers for the RE task (Lee et al.,\n2019; Wang et al., 2019b).\nAlthough they can be tackled independently,\nNER and RE tasks are in fact synergistically con-\nnected: if we knew two entities and their types in\na sentence, it would be easier to classify their re-\nlations; similarly, if we knew the relation between\ntwo phrases, then it would be less challenging to la-\nbel their entity types. This has naturally motivated\nefforts in joint or multi-task learning for NER and\nRE, hoping to achieve better performances in both\ntasks by simultaneously training the same neural\nnetwork towards combined objectives. Despite the\ndifferences in their details, the practices in NER\nand RE joint learning usually share a general high-\nlevel architecture: they sequentially stack together\nthe word and sequence embedding layers, the NER\n236\nprediction layer, the NER entity embedding layer,\nand the relation representation/handling layers. For\nword and sequence contextualized embedding lay-\ners, where many network variations be present, re-\nsearchers have investigated using BiLSTM, RNN,\nand BERT pre-trained transformers. (Bekoulis\net al., 2018b; Wang et al., 2019a; Giorgi et al.,\n2019; Huang et al., 2019b; Katiyar and Cardie,\n2017). These studies usually emphasized more on\nevaluating different joint models, leaving the com-\nparison between joint and standalone models to be\ninvestigated.\nPre-trained transformer models, e.g. BERT, XL-\nNet, and GPT, have achieved state-of-the-art perfor-\nmance across a great number of benchmark NLP\ntasks (Devlin et al., 2018; Yang et al., 2019; Rad-\nford et al., 2018). They provide the beneﬁts of rep-\nresenting bidirectional context and encoding text\nsequence by a series of attention layers. From the\ntransfer learning standpoint, various NLP tasks can\nbe treated as downstream tasks appended to the\npre-trained models, and the pre-trained parame-\nters (usually from large scale corpora in a generic\ndomain) together with the NLP task speciﬁc pa-\nrameters are ﬁne-tuned via continued training on\na relatively smaller and task-speciﬁc training data\nset. To enhance domain speciﬁcity, BERT has also\nbeen customized and retrained on speciﬁc domains\nsuch as the biomedical domain against relevant cor-\npus, examples including BioBERT, ClinicalBERT,\nand SciBERT (Lee et al., 2019; Alsentzer et al.,\n2019; Beltagy et al., 2019). Also, there has been\na surge in studies applying BERT in speciﬁc NLP\ncontexts for ﬁne-tuning tasks such as predicting\nhospital re-admission, extracting bacteria-biotope\nrelations, biomedical named entity normalization,\netc. (Huang et al., 2019a; Jettakul et al., 2019; Li\net al., 2019).\nWe have previously investigated the applications\nof ﬁne-tuning pre-trained BERT models on a pro-\ntocol corpus for NER and RE tasks separately.\nEncouraged by many successful studies on pre-\ntrained transformer and joint models, we continued\nto explore joint-learning strategies combined with\nBERT to co-train NER and RE tasks against our\nin-house clinical protocol corpus. We abstracted\na neural architecture from two popular joint mod-\nels and experimented with a number of variations\n(Bekoulis et al., 2018b; Giorgi et al., 2019). We be-\nlieve these continued efforts not only help selecting\nthe best-performing model for our applications, but\nalso provide a comprehensive understanding of var-\nious transfer learning strategies’ performance under\na real-world setup, shedding light on developing\nbusiness-oriented AI applications for the healthcare\nand clinical trial industry.\n3 Data Set\nData. Our data is comprised of the eligibility cri-\nteria sections from 470 Covance in-house drug de-\nvelopment study protocols (less than 2% of the\ntotal number of in-house protocols). The eligibility\ncriteria section in a protocol explicitly and unam-\nbiguously deﬁnes the rules to include or exclude a\npatient, thus directly determining the patient popu-\nlation available for the trial. The corpus contains\n30,183 criteria sentences in total. We randomly\nsplit the sentences into training and test sets with a\n2:1 ratio, resulting in 20,122 sentences for training\nand 10,061 for test.\nThe sentences were manually annotated by\nbiomedical experts. Clinically relevant entities and\nthe associated entity relations are labelled based\non our annotation guideline. We used the BIO tag\nformat to denote the beginning, inside, and out-\nside of the entities (Ramshaw and Marcus, 1999).\nWe focused on 15 types of entities and 7 types of\nsyntactic relations (as shown in Table 1 and Table\n2):\nTable 1: Train and test data counts for the NER task.\nEntity Train Test\nCondition 12,682 8,537\nObservation 7,309 5,218\nProcedure 3,406 2,234\nDevice 221 140\nDrug 7,793 5,858\nInvestigational product 329 224\nEvent 2,430 1,625\nRefractory condition 381 278\nDemographics 498 381\nMeasurement 4,540 3,344\nTemporal constraints 6,968 4,589\nQualiﬁer/modiﬁer 7,853 5,196\nAnatomic location 427 223\nNegation cue 921 615\nPermission cue 1,236 869\n4 Methodology\n4.1 Joint Model Overview\nAfter reviewing the previous NER and RE joint\nmodels, we established a general network archi-\ntecture that includes key components for the joint\nlearning while allowing experiment using varia-\n237\nTable 2: Train and test data counts for the RE task.\nRelation Train Test\nis negated 703 468\nis permitted 1,009 673\nmodiﬁed by 5,715 3,810\nhas value 3,326 2,218\nhas temporal constraint 6,169 4,112\nis located 215 143\nspeciﬁed by 3,729 2,486\ntotal count 20,866 13,910\ntions in local network designs. The main structure\nof the joint model is shown in Figure 1.\nWe used BERT pre-trained transformer as the\nembedding/encoding layer. The NER layer occurs\nafter the BERT layer and uses softmax for NER\nclassiﬁcation. More speciﬁcally, it takes the BERT\noutput vectors as its input and ﬁrst passes through\na fully connected layer and then the output layer\nwhere NER labels are classiﬁed using softmax func-\ntion (Goodfellow et al., 2016). The NER classiﬁca-\ntion loss function based on cross-entropy is:\nlossNER =\nn∑\ni=1\n−log( esi,li\n∑k\nj=1 esi,cj\n) (1)\nwhere n is the total number of NER tokens, li is\nthe actual NER label for the ith token, k is the\nnumber of NER label classes, cj denotes any of the\nNER label classes, si,li is the linear score for the\nith token belonging to its actual class li, and si,cj\nis the linear score for the ith token belonging to\nentity class cj.\nFollowing the NER layer, we appended an NER\nlabel embedding layer, which is concatenated with\nthe outputs from the previous BERT layer to serve\nas the input for the subsequent RE task. Because\nan entity could be paired with other entities before\nor after it in a sentence, it should be mapped differ-\nently in these 2 cases. In our model, the entity vec-\ntors are processed in the relation pair handling layer,\nby 1) mapping them using a fully connected layer\nto head vectors for representing entities as heads in\na pair, and 2) mapping them using a parallel fully\nconnected layer to tail vectors for representing tails\nin a pair. Then an entity pair, composed of a head\nand a tail vector, employs a classiﬁcation function,\nbeing either softmax or biafﬁne function, to pro-\nduce the RE classiﬁcation result, i.e., the relation\ntype between the two entities. More details about\nthe RE model variations can be found in section\n4.2.1.\nThe RE loss function is also cross-entropy based:\nlossRE =\nn∑\ni=1\n−log( esi,qi\n∑k\nj=1 esi,rj\n) (2)\nwhere n is the total number of relations, qi is the\nactual relation label for the ith entity pair, si,qi\nis the linear score for the ith relation belonging\nto its actual class qi, k is the number of relation\ntypes, and si,rj is the linear score for theith relation\nbelonging to relation class rj.\nThe overall joint model loss is derived by sum-\nming the NER and RE losses:\nlossjoint = lossNER + lossRE (3)\n4.2 Model Options\nBy keeping the NER layers unchanged in this gen-\neral network architecture, we further experimented\nwith different options for the RE sub-network and\nevaluated their effects on joint task performance.\n4.2.1 RE Sub-network Options\nFor the RE task, we explored methods of repre-\nsenting entities pairs and classifying their relations,\nwhich are rendered as the relation handling and\nthe classiﬁcation layers in Figure 1. We tested two\noptions, denoted as re m1 and re m2 respectively.\nModel option re m1 is based on (Bekoulis et al.,\n2018a,b), which passes entity vectors derived from\nthe NER layer through a fully connected layer for\nobtaining its head entity representation and simi-\nlarly through another fully connected layer for tail\nentity representation, and then adds vectors of a\npair of entities to serve as the relation vector for\nthe two paired entities (i.e. head and tail entities):\nhi,j = hi + hj (4)\nwhere hi and hj are vectors for head and tail enti-\nties and hi,j is the resulting vector from summing\nthe two.\nWe subsequently constructed a fully connected\nlayer to classify the relation vectors. Differing from\n(Bekoulis et al., 2018a,b), in which the RE classes\nwere assumed to be not mutually exclusive and\nthe RE classiﬁcation was treated as a multi-label\nclassiﬁcation task using a sigmoid function, rela-\ntions in our study are mutually exclusive from each\nother and henceforth we used a softmax function\nto classify the relations. Also note that (Bekoulis\net al., 2018a,b) used bidirectional LSTM for em-\nbedding/encoding and we replaced it with BERT\nas described in 4.1.\n238\nFigure 1: Neural architecture of the joint model for NER and RE tasks.\nThe other model option, re m2, is similar to\nthe practice in (Giorgi et al., 2019; Nguyen and\nVerspoor, 2019): we ﬁrst applied two parallel fully\nconnected layers to derive head and tail entity repre-\nsentation respectively, and then performed biafﬁne\nclassiﬁcation using the head and the tail vectors in\nan entity pair. The biafﬁne classiﬁcation function\nis:\nbiaffine (hi, hj) =hiT Uhj + W(hi||hj) +b\n(5)\nwhere hi and hj denotes the head and tail entity\nvectors respectively,U is a tensor of size ofm×l×\nm, W is a matrix of size ofl∗2m, with m being the\nhidden size of the head/tail vector and l being the\nnumber of RE labels, hi||hj denotes concatenating\nthe two vectors, and b is a bias vector of size of l.\nThe above biafﬁne function has a bilinear term\nhiT Uhj and a linear term W(hi||hj), along with\nthe bias term. We experimented with either in-\ncluding both the bilinear and linear terms ( bilin-\near+linear), or only including the bilinear term\n(bilinear only).\n4.2.2 RE Negative Sample Construction\nduring Training\nA common challenge in RE classiﬁcation tasks is\nthe overwhelming choices of negative samples. In\nprinciple, any entity pair without syntactic relations\nis a negative sample. To address this challenge, we\nevaluated two negative sample construction strate-\ngies. One strategy is to scan through all the possible\nentity pairs and mark the pairs without syntactic\nrelations as negative relation samples. Since this\noption relies on relation information from gold stan-\ndard data, we denote it as gs-based. The other strat-\negy, denoted as incremental, incrementally builds\nnegative relation samples based on NER predicted\nlabels, as in (Giorgi et al., 2019; Nguyen and Ver-\nspoor, 2019). More speciﬁcally, in the incremental\nstrategy, an entity pair is included as a negative\nsample if 1) two entities in this pair are correctly\npredicted by the NER layer and are without rela-\ntions, or 2) any of the entities in this pair is incor-\nrectly predicted by the NER. Hence, the former\nway of constructing negative samples is static as\nthe samples remain unchanged throughout the train-\ning, whereas the latter way is dynamic, as whether\nor not an entity pair is included as a negative sam-\nple depends on the NER prediction result during\ntraining sessions.\n4.2.3 Evaluation Options\nWe evaluated the joint learning model’s perfor-\nmance on NER and RE tasks, by reporting micro-\nlevel precision, recall, and f1-measure for both\ntasks. For RE, we evaluated on relations between\ngold standard entities without considering NER\npredicted entities (the gs-based option), and also\nevaluated on relations yielded from NER predicted\nentities (the end-to-end option). In other words, the\n239\ngs-based option evaluates RE performance when\nwe know which tokens are actual entities, and the\nend-to-end option evaluates the performance in the\nscenario when we do not have actual entity infor-\nmation, which is a more realistic scenario when\nevaluating a RE model’s performance in produc-\ntion systems.\n4.2.4 Standalone Models\nTo assess the effects of joint learning options, we\nbuilt NER and RE standalone models from the\ncorresponding sub-networks in the joint learning\narchitecture for NER and RE tasks separately and\nevaluate their performances.\nFor the NER standalone model, following the\nBERT layer, we added a fully connected layer\nwith softmax classiﬁcation. For the RE standalone\nmodel, instead of having an intermediate NER\nlayer, we appended two parallel fully connected\nlayer directly on the BERT output to derive the\nhead and tail entity representations, and then clas-\nsiﬁed entity pair relations using a softmax function.\nFor standalone model evaluations, we employed the\nsame precision/recall/f1-measures as in the joint\nmodels by evaluating against the gold standard (the\ngs-based option).\nIt is worth noting that in real-world practice, we\ndo not know which tokens are entities so we have\nto use NER prediction as entity input for RE eval-\nuation. Thus, we included a real-world inspired\nend-to-end metric for RE that evaluates the per-\nformance using NER standalone model prediction\nas inputs, which effectively takes into account the\npropagated NER prediction errors (the end-to-end\nevaluation option).\n4.2.5 Pre-trained Models\nFor pre-trained models, we experimented with\nBERT base, a smaller version of BERT that com-\nprises 110 million parameters, and BioBERT, a\nmodel derived from retraining the original BERT\nusing large-scale biomedical texts (Lee et al., 2019).\nWe chose to use the uncased version of BERT base\nin which all text is lower cased; and since BioBERT\nis cased only, we used the model with all original\ncases preserved in text.\n4.3 Hyperparameters\nWe used the same hyperparameter values across\nall the models as shown in Table 3. For BERT\nlayer hyperparameters we used the same values\nas in the original BERT model. The models were\nimplemented using the Tensorﬂow library.\nTable 3: Hyperparameter Values.\nHyperparameters Value\nNumber of training epochs 20\nLearning rate 2 × 10−5\nTraining batch size 32\nMaximum sequence length 128\nNER embedding size 16\n5 Results and Analysis\nOur results are summarized in Table 4 and we elab-\norate our ﬁnding below.\nre m1 vs. re m2 RE sub-network option. For\nthe NER task, the four re m1 models performed\nsimilarly to the eight re m2 models. The highest\nrecall, precision and f1-measure are achieved in\nre m2’s gs-based negative sampling option (model\n#12), which performs only marginally better than\nthe other re m1 and re m2 models. For the RE task,\nin the BERT scenario, the re m2 models greatly\noutperform the re m1 models in all three measures\n(P/R/F). For example, model #5, a re m2 model\nusing gs-based negative sampling, achieved end-\nto-end f1-measure of 58.14%, whereas its coun-\nterpart, model #1, in the re m1 model family, has\nf1-measure of 44.25%, a 13.89% drop from model\n#5. This result demonstrates that the biafﬁne classi-\nﬁcation, the entity pair representation and the classi-\nﬁcation option used in re m2, can lead to much bet-\nter RE performance than softmax classiﬁcation as\nused in re m1. However, for BioBERT pre-trained\nmodel, the result is not as decisive: re m2 does\nnot consistently outperform re m1. For example,\nmodel #15 (re m2) has better RE performance than\nmodel #11 (re m1) yet model #12 (re m2) exhibits\nlower RE performance than model #11 (re m1).\nBiafﬁne variations for the re m2 option .\nWithin the re m2 model, we evaluated the strate-\ngies to classify relations using both the bilinear\nand linear parts of the biafﬁne function ( bilinear\n+ linear) or using only the bilinear part ( bilinear\nonly). The results are exhibited as models #3 to\n#6 (BERT) and #12 to #15 (BioBERT) in Table\n4. The two strategies achieved similar results on\nthe NER task for both BERT and BioBERT cases.\nFor RE end-to-end performance, the bilinear only\nstrategy combined with BERT and gs-based nega-\ntive sampling for training (model #5) achieved the\nbest f1-measure and recall; and the bilinea + lin-\n240\near strategy together with BioBERT and gs-based\nnegative sampling got the highest precision (model\n#12). Overall, we observed that the biafﬁne classi-\nﬁcation options play a less signiﬁcant role in model\nperformance comparing to other modeling compo-\nnents such like the negative sampling strategies and\npre-trained model options.\ngs-based vs. incremental RE negative sam-\npling. We tested the two negative sampling strate-\ngies on both re m1 and re m2 models (models #1\nto #6 and #10 to #15 in Table 4). In the case of\nusing pre-trained BERT model, we observed that\ngs-based negative sampling outperforms the incre-\nmental option (models #1 vs. #2, #3 vs. #4, #5 vs.\n#6) with signiﬁcant margin. In particular, model #1\nexceeded #2 by 4.45% for end-to-end f1-measure,\nand models #3 and #5 exceeding #4 and #6 by\n4.91% and 5.47%, respectively. Interestingly, in\ncontrast, for the BioBERT case, the incremental\nstrategy is superior to the gs-based strategy by an\neven larger margin, e.g. with model #11 excedding\n#10 by 19.69%. Therefore the effect of RE negative\nsampling strategies is jointly determined with the\npre-trained model option, and can be signiﬁcant.\nJoint-learning vs. standalone model . Our re-\nsults show that joint-learning models generally im-\nprove RE performance over the standalone RE\nmodel but do not signiﬁcantly affect the NER task\n( 1% drop in f1-measure). because the incremen-\ntal strategy requires NER net, the standalone RE\ncan only be evaluated using the gs-based strategy,\nand we had to use gold standard entity information\nas RE input. The joint-learning models outper-\nform the standalone RE in most of the scenarios\nwhen measured with the gs-based evaluation op-\ntion. When conducting the end-to-end RE task,\nthe joint-learning models exhibit dramatic perfor-\nmance improvement over the standalone models,\ne.g. f1-measure of 58.14% for model #5 (joint) vs.\n48.15% for #9 (standalone) and 55.37% model #15\n(joint) vs. 26.41% for #18 (standalone). Despite of\nthe slightly weaker performance in NER task, the\ngreat gain in the end-to-end RE task demonstrates\nthat the joint-learning models a better solution in\nreal-world applications.\nBERT vs. BioBERT . Comparing between\nthe two pre-trained models, for the NER task,\nBioBERT yields better result (around 70%) than\nBERT (around 69%), possibly due to its addi-\ntional language model pre-trained from biomed-\nical corpus. BERT-based joint-learning models,\nwhen using re m2 negative sampling strategy, out-\nperforms the re m1 strategy, but this trend does\nnot hold in the BioBERT-based joint-learning mod-\nels. BioBERT standalone model performance on\nthe RE task is severely impacted by this conﬁg-\nuration (model #18). Although BioBERT-based\nmodel achieves reasonable performance in some\njoint-learning (e.g. model #13) strategies, it fails\nin others (e.g. model #10). These results indicate\nthat joint-learning with BERT is more robust with\nmore stable performance than BioBERT.\nIn summary, our results demonstrated that joint-\nlearning is a superior strategy, thanks to its steady\nand signiﬁcant performance gain in the end-to-end\nRE task. However, since no model can achieve the\nbest NER and RE performance simultaneously, it\nis still necessary to balance the two tasks’ perfor-\nmances when choosing the proper joint-learning\nmodel to prioritize production needs.\n6 Conclusion and Future Work\nIn this reported work, we employed joint-learning\nmodels to identify entities and relations in clinical\nprotocols by using pre-trained transformer NLP\ndeep learning models. To the best of our knowl-\nedge, this is the ﬁrst attempt to tackle the NER and\nRE tasks in a joint and pre-trained deep learning\nfashion on real-world protocols, which is inherently\na corpus of high complexity. Our contribution is\nthree fold: 1) we abstracted a neural network ar-\nchitecture from literature combining pre-trained\ntransformer model with joint learning for NER &\nRE tasks, 2) we experimented with different model\noptions based on the joint learning network archi-\ntecture, 3) we examined performance on a com-\nplex clinical corpus, which is a less studied but\nhighly impactful domain for such tasks. Our results\ndemonstrated that joint-learning models can greatly\nimprove RE performance over the standalone mod-\nels despite of a minor decrease in the NER per-\nformance. Among all the evaluated joint-learning\nstrategies, the biafﬁne RE model, gold-standard\nbased negative sampling, together with the BERT\npre-trained model, led to generally better perfor-\nmance than other strategies. These results provide\nevidence on the effectiveness of using joint and\ndeep learning on parsing clinical protocol text, and\nthus for future work, we will continue exploring\nmore sophisticated joint and multi-task learning\nnetwork architectures to further enhance the NER\nand RE parsing performance.\n241\nTable 4: NER & RE task performance from joint and standalone models (in percentage).\nNo.\nPre-\ntrained\nmodel\nMethod RE sub-network\noption\nRE negative\nsampling\nfor training\nNER Performance RE Performance\ngs-based end-to-end\nP R F P R F P R F\n1 bert Joint model re m1 gs-based 66.25 72.71 69.32 70.12 51.93 59.27 47.24 43.68 44.25\n2 bert Joint model re m1 incremental 66.73 73.07 69.74 69.29 34.86 43.67 52.44 34.85 39.80\n3 bert Joint model re m2,\nbilinear + linear gs-based 66.61 73.10 69.69 69.57 64.57 66.86 52.63 64.57 57.89\n4 bert Joint model re m2,\nbilinear + linear incremental 66.55 72.92 69.58 70.71 51.69 59.38 54.58 51.68 52.98\n5 bert Joint model re m2,\nbilinear only gs-based 66.46 73.12 69.62 70.47 64.64 67.29 52.97 64.63 58.14\n6 bert Joint model re m2,\nbilinear only incremental 66.72 72.79 69.61 66.72 72.79 69.61 53.56 51.93 52.67\n7 bert Standalone\nNER - - 67.79 73.19 70.37 - - - - - -\n8 bert Standalone\nRE linear gs-based - - - 64.37 48.85 54.89 - - -\n9 bert Standalone\nend to end linear - 67.79 73.19 70.37 - - - 48.52 48.85 48.15\n10 biobert Joint model re m1 gs-based 67.93 73.49 70.58 61.84 45.81 51.53 17.35 45.79 24.59\n11 biobert Joint model re m1 incremental 67.70 73.08 70.27 66.74 40.77 49.02 51.73 40.76 44.28\n12 biobert Joint model re m2,\nbilinear + linear gs-based 68.02 73.54 70.66 70.03 66.45 67.93 30.66 66.45 41.64\n13 biobert Joint model re m2,\nbilinear + linear incremental 67.71 73.33 70.38 70.44 55.21 61.50 53.48 55.21 54.22\n14 biobert Joint model re m2,\nbilinear only gs-based 67.98 73.53 70.63 69.65 66.41 67.82 23.93 66.40 34.73\n15 biobert Joint model re m2,\nbilinear only incremental 67.70 73.44 70.44 72.69 56.19 63.01 54.66 56.20 55.37\n16 biobert Standalone\nNER - - 68.97 73.50 71.15 - - - - - -\n17 biobert Standalone\nRE linear gs-based - - - 63.43 52.54 57.05 - - -\n18 biobert Standalone\nend to end linear - 68.97 73.50 71.15 - - - 18.03 52.53 26.41\nReferences\nEmily Alsentzer, John R Murphy, Willie Boag, Wei-\nHung Weng, Di Jin, Tristan Naumann, and Matthew\nMcDermott. 2019. Publicly available clinical bert\nembeddings. arXiv preprint arXiv:1904.03323.\nNguyen Bach and Sameer Badaskar. 2007. A review of\nrelation extraction. Literature review for Language\nand Statistics II, 2.\nGiannis Bekoulis, Johannes Deleu, Thomas Demeester,\nand Chris Develder. 2018a. Adversarial training\nfor multi-context joint entity and relation extraction.\narXiv preprint arXiv:1808.06876.\nGiannis Bekoulis, Johannes Deleu, Thomas Demeester,\nand Chris Develder. 2018b. Joint entity recogni-\ntion and relation extraction as a multi-head selection\nproblem. Expert Systems with Applications, 114:34–\n45.\nIz Beltagy, Arman Cohan, and Kyle Lo. 2019. Scibert:\nPretrained contextualized embeddings for scientiﬁc\ntext. arXiv preprint arXiv:1903.10676.\nDaniel M Bikel, Scott Miller, Richard Schwartz,\nand Ralph Weischedel. 1998. Nymble: a high-\nperformance learning name-ﬁnder. arXiv preprint\ncmp-lg/9803003.\nMiao Chen, Fang Du, Ganhui Lan, and Victor S\nLobanov. 2020. Using pre-trained transformer\ndeep learning models to identify named entities\nand syntactic relations for clinical protocol analysis.\nIn AAAI Spring Symposium: Combining Machine\nLearning with Knowledge Engineering (1).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJohn Giorgi, Xindi Wang, Nicola Sahar, Won Young\nShin, Gary D Bader, and Bo Wang. 2019. End-to-\nend named entity recognition and relation extraction\nusing pre-trained language models. arXiv preprint\narXiv:1912.13415.\nIan Goodfellow, Yoshua Bengio, and Aaron Courville.\n2016. 6.2. 2.3 softmax units for multinoulli output\ndistributions. In Deep Learning., pages 180–184.\nMIT Press.\nKexin Huang, Jaan Altosaar, and Rajesh Ranganath.\n2019a. Clinicalbert: Modeling clinical notes and\npredicting hospital readmission. arXiv preprint\narXiv:1904.05342.\nWeipeng Huang, Xingyi Cheng, Taifeng Wang, and\nWei Chu. 2019b. Bert-based multi-head selection\nfor joint entity-relation extraction. In CCF Interna-\ntional Conference on Natural Language Processing\nand Chinese Computing, pages 713–723. Springer.\n242\nAmarin Jettakul, Duangdao Wichadakul, and Peerapon\nVateekul. 2019. Relation extraction between bacte-\nria and biotopes from biomedical texts with attention\nmechanisms and domain-speciﬁc contextual repre-\nsentations. BMC bioinformatics, 20(1):627.\nDan Jurafsky. 2000. Speech & language processing.\nPearson Education India.\nArzoo Katiyar and Claire Cardie. 2017. Going out on\na limb: Joint extraction of entity mentions and re-\nlations without dependency trees. In Proceedings\nof the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 917–928.\nJohn Lafferty, Andrew McCallum, and Fernando CN\nPereira. 2001. Conditional random ﬁelds: Prob-\nabilistic models for segmenting and labeling se-\nquence data. ICML proceedings.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2019. Biobert: a pre-trained\nbiomedical language representation model for\nbiomedical text mining. Bioinformatics.\nFei Li, Yonghao Jin, Weisong Liu, Bhanu Pratap Singh\nRawat, Pengshan Cai, and Hong Yu. 2019. Fine-\ntuning bidirectional encoder representations from\ntransformers (bert)–based models on large-scale\nelectronic health record notes: An empirical study.\nJMIR medical informatics, 7(3):e14830.\nAndrew McCallum, Dayne Freitag, and Fernando CN\nPereira. 2000. Maximum entropy markov models\nfor information extraction and segmentation. In\nICML, volume 17, pages 591–598.\nMakoto Miwa and Mohit Bansal. 2016. End-to-end re-\nlation extraction using lstms on sequences and tree\nstructures. arXiv preprint arXiv:1601.00770.\nDat Quoc Nguyen and Karin Verspoor. 2019. End-to-\nend neural relation extraction using deep biafﬁne at-\ntention. In European Conference on Information Re-\ntrieval, pages 729–738. Springer.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nLance A Ramshaw and Mitchell P Marcus. 1999. Text\nchunking using transformation-based learning. In\nNatural language processing using very large cor-\npora, pages 157–176. Springer.\nHaoyu Wang, Ming Tan, Mo Yu, Shiyu Chang, Dakuo\nWang, Kun Xu, Xiaoxiao Guo, and Saloni Potdar.\n2019a. Extracting multiple-relations in one-pass\nwith pre-trained transformers. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 1371–1377, Florence,\nItaly. Association for Computational Linguistics.\nHaoyu Wang, Ming Tan, Mo Yu, Shiyu Chang, Dakuo\nWang, Kun Xu, Xiaoxiao Guo, and Saloni Pot-\ndar. 2019b. Extracting multiple-relations in one-\npass with pre-trained transformers. arXiv preprint\narXiv:1902.01030.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5753–5763.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.79868483543396
    },
    {
      "name": "Relationship extraction",
      "score": 0.6942028999328613
    },
    {
      "name": "Named-entity recognition",
      "score": 0.6667782068252563
    },
    {
      "name": "Analytics",
      "score": 0.6640045642852783
    },
    {
      "name": "Workflow",
      "score": 0.6455915570259094
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6164554357528687
    },
    {
      "name": "Machine learning",
      "score": 0.5832147002220154
    },
    {
      "name": "Protocol (science)",
      "score": 0.555338442325592
    },
    {
      "name": "Transformer",
      "score": 0.5489752888679504
    },
    {
      "name": "Parsing",
      "score": 0.5439467430114746
    },
    {
      "name": "Natural language processing",
      "score": 0.48224908113479614
    },
    {
      "name": "Dependency grammar",
      "score": 0.4815533459186554
    },
    {
      "name": "Information extraction",
      "score": 0.4602486193180084
    },
    {
      "name": "Task (project management)",
      "score": 0.3958832025527954
    },
    {
      "name": "Data mining",
      "score": 0.28678837418556213
    },
    {
      "name": "Database",
      "score": 0.12759336829185486
    },
    {
      "name": "Medicine",
      "score": 0.09451675415039062
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Alternative medicine",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I60638368",
      "name": "Covance (United States)",
      "country": "US"
    }
  ],
  "cited_by": 20
}