{
  "title": "Multi-Graph Transformer for Free-Hand Sketch Recognition",
  "url": "https://openalex.org/W2997736261",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2098197978",
      "name": "Xu Peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227475324",
      "name": "Joshi, Chaitanya K.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3083767140",
      "name": "Bresson, Xavier",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1815076433",
    "https://openalex.org/W2963561004",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2156387975",
    "https://openalex.org/W2964321699",
    "https://openalex.org/W2948798935",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2805516822",
    "https://openalex.org/W2603445054",
    "https://openalex.org/W1972420097",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2964311892",
    "https://openalex.org/W3034752215",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2911327180",
    "https://openalex.org/W2884466206",
    "https://openalex.org/W2949117887",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2999249053",
    "https://openalex.org/W2515723519",
    "https://openalex.org/W2411243951",
    "https://openalex.org/W3026792269",
    "https://openalex.org/W2962945761",
    "https://openalex.org/W2949877834",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2913618226",
    "https://openalex.org/W2906351695",
    "https://openalex.org/W2984864519",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2743832495",
    "https://openalex.org/W2776402438",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2952444318",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W4210257598",
    "https://openalex.org/W3036435328",
    "https://openalex.org/W2985199329",
    "https://openalex.org/W2991550749",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2962990451",
    "https://openalex.org/W2963307918",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2558460151",
    "https://openalex.org/W2950166981",
    "https://openalex.org/W2606712314",
    "https://openalex.org/W3101029400",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2964338167",
    "https://openalex.org/W2945895945",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2910152998",
    "https://openalex.org/W2624431344",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W3035421056",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W3020197498",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2466618734",
    "https://openalex.org/W2938500406",
    "https://openalex.org/W3016763241",
    "https://openalex.org/W2962767366"
  ],
  "abstract": "Learning meaningful representations of free-hand sketches remains a challenging task given the signal sparsity and the high-level abstraction of sketches. Existing techniques have focused on exploiting either the static nature of sketches with Convolutional Neural Networks (CNNs) or the temporal sequential property with Recurrent Neural Networks (RNNs). In this work, we propose a new representation of sketches as multiple sparsely connected graphs. We design a novel Graph Neural Network (GNN), the Multi-Graph Transformer (MGT), for learning representations of sketches from multiple graphs which simultaneously capture global and local geometric stroke structures, as well as temporal information. We report extensive numerical experiments on a sketch recognition task to demonstrate the performance of the proposed approach. Particularly, MGT applied on 414k sketches from Google QuickDraw: (i) achieves small recognition gap to the CNN-based performance upper bound (72.80% vs. 74.22%), and (ii) outperforms all RNN-based models by a significant margin. To the best of our knowledge, this is the first work proposing to represent sketches as graphs and apply GNNs for sketch recognition. Code and trained models are available at https://github.com/PengBoXiangShang/multigraph_transformer.",
  "full_text": "1\nMulti-Graph Transformer\nfor Free-Hand Sketch Recognition\nPeng Xu∗, Chaitanya K. Joshi, and Xavier Bresson\nAbstract—Learning meaningful representations of free-hand\nsketches remains a challenging task given the signal sparsity\nand the high-level abstraction of sketches. Existing techniques\nhave focused on exploiting either the static nature of sketches\nwith Convolutional Neural Networks (CNNs) or the temporal\nsequential property with Recurrent Neural Networks (RNNs).\nIn this work, we propose a new representation of sketches as\nmultiple sparsely connected graphs. We design a novel Graph\nNeural Network (GNN), the Multi-Graph Transformer (MGT),\nfor learning representations of sketches from multiple graphs,\nwhich simultaneously capture global and local geometric stroke\nstructures, as well as temporal information. We report extensive\nnumerical experiments on a sketch recognition task to demon-\nstrate the performance of the proposed approach. Particularly,\nMGT applied on 414k sketches from Google QuickDraw: (i)\nachieves small recognition gap to the CNN-based performance\nupper bound ( 72.80% vs. 74.22%) and infers faster than the\nCNN competitors, and (ii) outperforms all RNN-based models\nby a signiﬁcant margin. To the best of our knowledge, this is\nthe ﬁrst work 1 proposing to represent sketches as graphs and\napply GNNs for sketch recognition. Code and trained models are\navailable at https://github.com/PengBoXiangShang/multigraph\ntransformer.\nIndex Terms—transformer, multi-graph transformer, MGT,\ngraph neural network, GNN, sketch, free-hand sketch, hand-\ndrawn sketch, sketch recognition, sketch classiﬁcation, neural\nrepresentation of sketch, graph representation of sketch.\nI. I NTRODUCTION\nF\nREE-HAND sketches are drawings made without the use\nof any instruments. Sketches are different from traditional\nimages: they are formed of temporal sequences of strokes [1],\n[2], while images are static collections of pixels with dense\ncolor and texture patterns. Sketches capture high-level abstrac-\ntion of visual objects with very sparse information compared to\nregular images, which makes the modelling of sketches unique\nand challenging.\nThe modern prevalence of touchscreen devices has led to\na ﬂourishing of sketch-related applications in recent years,\nincluding sketch recognition [3], [4], sketch scene understand-\ning [5], sketch hashing [2], sketch-based image retrieval [6],\n[7], [8], [9], [10], [11], sketch-related generation [1], [12],\n[13], [3], sketch self-supervised learning [14], [15], etc.\nIf we assume sketches to be 2D static images, CNNs can be\ndirectly applied to sketches, such as “Sketch-a-Net” [16]. If\nwe now suppose that sketches are ordered sequences of point\nPeng Xu, Chaitanya K. Joshi, and Xavier Bresson are with School of Com-\nputer Science and Engineering, Nanyang Technological University, Singapore.\n∗ Corresponding to Peng Xu, email: peng.xu@ntu.edu.sg.\nXavier Bresson is supported in part by NRF Fellowship NRFF2017-10.\n1The preliminary version of this work can be found in arXiv https://arxiv.\norg/abs/1912.11258v1 and https://arxiv.org/abs/1912.11258v2\n(a) original sketch\n (b) 1-hop connected\n (c) 2-hop connected\nFig. 1. Sketches can be seen as sets of curves and strokes, which are\ndiscretized by graphs.\nreplot as picture\nCNN\ncoordinate-based \nmodel\npicture\ncoordinate\nFig. 2. In sketch-based human-computer interaction scenarios, it is time-\nconsuming to render and transfer pictures of sketches. Solely transferring\nstroke coordinates leads to real-time applications.\ncoordinates, then RNNs can be used to recursively capture the\ntemporal information, e.g., “SketchRNN” [1].\nIn this work, we introduce a new representation of sketches\nwith graphs. We assume that sketches are sets of curves and\nstrokes, which are discretized by a set of points representing\nthe graph nodes. This view offers high ﬂexibility to encode\ndifferent sketch geometric properties as we can decide dif-\nferent connectivity structures between the node points. We\nuse two types of graphs to represent sketches: intra-stroke\ngraphs and extra-stroke graphs. The ﬁrst graphs capture the\nlocal geometry of strokes, independently to each other, with\nfor example 1-hop or 2-hop connected graphs, see Figure 1.\nThe second graphs encode the global geometry and temporal\ninformation of strokes. Another advantage of using graphs is\nthe freedom to choose the node features. For sketches, spatial,\ntemporal and semantic information is available with the stroke\npoint coordinates, the ordering of points, and the pen state\ninformation, respectively. In summary, representing sketches\nwith graphs offers a universal representation that can make\nuse of global and local spatial sketch structures, as well as\ntemporal and semantic information.\nTo exploit these graph structures, we propose a new Trans-\nformer [17] architecture that can use multiple sparsely con-\nnected graphs. It is worth reporting that a direct application of\narXiv:1912.11258v3  [cs.CV]  25 Mar 2021\n2\nthe original Transformer model on the input spatio-temporal\nfeatures provides poor results. We argue that the issue comes\nfrom the graph structure in the original Transformer which\nis a fully connected graph. Although fully-connected word\ngraphs work impressively for Natural Language Processing,\nwhere the underlying word representations themselves contain\nrich information, such dense graph structures provide poor\ninnate priors/inductive bias [18] for 2D sketch tasks. Trans-\nformers require sketch-speciﬁc design coming from geometric\nstructures. This led us to naturally extend Transformers to\nmultiple arbitrary graph structures. Moreover, graphs provide\nmore robustness to handle noisy and style-changing sketches\nas they focus on the geometry of stokes and not on the speciﬁc\ndistribution of points.\nAnother advantage of using domain-speciﬁc graphs is to\nleverage the sparsity property of discretized sketches. Observe\nthat intra-stroke and extra-stroke graphs are highly sparse\nadjacency matrices. In practical sketch-based human-computer\ninteraction scenarios, it is time-consuming to directly transfer\nthe original sketch picture from user touch-screen devices\nto the back-end servers. To ensure real-time applications,\ntransferring the stroke coordinates as a character string would\nbe more beneﬁcial, see Figure 2.\nOur main contributions can be summarised as follows:\n1) We propose to model sketches as sparsely connected\ngraphs, which are ﬂexible to encode local and global\ngeometric sketch structures. To the best of our knowl-\nedge, it is the ﬁrst time that graphs are proposed for\nrepresenting sketches.\n2) We introduce a novel Transformer architecture that can\nhandle multiple arbitrary graphs. Using intra-stroke and\nextra-stroke graphs, the proposed Multi-Graph Trans-\nformer (MGT) learns both local and global patterns\nalong sub-components of sketches.\n3) Numerical experiments demonstrate the performances\nof our model. MGT signiﬁcantly outperforms RNN-\nbased models, and achieves small recognition gap to\nCNN-based architectures. Moreover, our MGT infers\nfaster than the strongest CNN baseline (see details in\nSection IV-B7). This is promising for real-time sketch-\nbased human-computer interaction systems. Note that\nfor sketch recognition, CNNs are the performance upper\nbound of coordinate-based models that involve trun-\ncating coordinate sequences, e.g., RNN or Transformer\nbased architectures.\n4) This Multi-Graph Transformer model is agnostic to\ngraph domains, and can be used beyond sketch appli-\ncations. We transfer our multi-graph transformer idea to\nconduct Relation Extraction on a NLP benchmark, i.e.,\nSemEval-2010 task 8, outperforming the state-of-the-art\nCNNs by a clear margin (prediction acc.: ours ( 89.45%)\nvs. CNN ( 86.60%)).\nFrom the perspective of representing sketches and the\npractical sketch-oriented applications, the main advantages\nof our proposed Multi-Graph Transformer against CNN are:\n1) Our model can be used for real-time sketch-based\nHCI (Human Computer Interaction) systems, because it\ndoes not need to render and transfer sketch pictures (see\nFigure 2).\n2) Our MGT is faster than the strongest CNN baseline, i.e.,\nInception V3. This will be demonstrated by experiments\nin Section IV-B7.\n3) As demonstrated in [19], coordinate-based models ( e.g.,\nour MGT, RNNs) are more suitable than CNNs for\nthe stroke-level tasks, e.g., perceptual grouping, stroke-\ngrained segmentation.\n4) Our MGT takes the stroke sequence as input, thus it\ncan handle the tasks that need to understand the logic\nand timing patterns of sketching process, e.g., stroke-by-\nstroke generation, stroke-level abstraction. Speciﬁcally,\nstroke-by-stroke sketch generation needs models learn\nthe temporal information, because the models need to\ndecide the order of stroke generation. However, as\ndiscussed in [20], CNNs intrinsically fail to work for\nthese tasks.\nThe rest of this paper is organized as follows: Section II\nbrieﬂy summarizes related work. Section III describes our\nproposed Multi-Graph Transformer. Experimental results and\ndiscussion are presented in Section IV, followed by a con-\nclusion Section V. Finally, we discuss our future work in\nSection VI.\nII. R ELATED WORK\nA. Neural Network Architectures for Sketches\nCNNs are a common choice for feature extraction from\nsketches. “Sketch-a-Net” [16] is a representative CNN-based\nmodel having a sketch-speciﬁc architecture. It was directly in-\nspired from AlexNet [21] with larger ﬁrst layer ﬁlters, no layer\nnormalization, larger pooling sizes, and high dropout. Song et\nal. [22] further improved Sketch-a-Net by adding spatial-\nsemantic attention layers. “SketchRNN” [1] is a seminal work\nto model temporal stroke sequences with RNNs, by taking\nthe key point coordinates of stroke as input. “SketchRNN”\nreminds the researchers that sketching is a dynamic process\nso that the temporal patterns of sketching strokes should also\nbe considered.\nRecently, some CNN-RNN hybrid architectures have been\nproposed for sketches, e.g., dual-branch networks [23], [2],\ncascaded networks [4], [24]. For more detailed summary and\ncomparison for sketch representing networks, please check a\ncomprehensive survey [20].\nEssentially, the aforementioned CNN or RNN -based net-\nwork architectures model sketch in Euclidean space. How\nto model sketch in non-Euclidean spaces is an interesting\nquestion. In this work, we propose a novel Graph Neural\nNetwork architecture for learning sketch representations from\nmultiple sparse graphs, combining both stroke geometry and\ntemporal order.\nB. Graph Neural Networks\nGraph Neural Networks (GNNs) [25], [26], [27], [28],\n[29], [30], [31], [32] aim to generalize neural networks to\nnon-Euclidean domains such as graphs and manifolds. GNNs\n3\niteratively build representations of graphs through recursive\nneighborhood aggregation (or message passing), where each\ngraph node gathers features from its neighbors to represent\nlocal graph structure. Recently, Graph Neural Networks have\nbeen widely applied for various domains, e.g., vision, lan-\nguage. However, sketch-oriented Graph Neural Networks are\nstill under-studied.\nC. Transformers\nThe Transformer architecture [17], originally proposed as a\npowerful and scalable alternative to RNNs, has been widely\nadopted in the Natural Language Processing community for\ntasks such as machine translation [33], [34], language mod-\nelling [35], [36], and question-answering [37], [38].\nTransformers for NLP can be regarded as GNNs which\nuse self-attention [39], [40] for neighborhood aggregation\non fully-connected word graphs [41]. However, GNNs and\nTransformers perform poorly when sketches are modelled as\nfully-connected graphs. This work advocates for the injection\nof inductive bias into Transformers through domain-speciﬁc\ngraph structures.\nIII. M ETHOD\nA. Notation\nWe assume that the training dataset D consists of N\nlabeled sketches: D = {(Xn, zn)}N\nn=1. Each sketch Xn has\na class label zn, and can be formulated as a S-step sequence\n[Cn, fn, p] ∈RS×4. Cn = {(xs\nn, ys\nn)}S\ns=1 ∈RS×2 is the coor-\ndinate sequence of the sketch points Xn. All sketch point co-\nordinates have been uniformly scaled to xs\nn, ys\nn ∈[0, 256]2. If\nthe true length of Cn is shorter than S then the vector [−1, −1]\nis used for padding. Flag bit vector fn ∈{f1, f2, f3}S×1 is\na ternary integer vector that denotes the pen state sequence\ncorresponding to each point of Xn. It is deﬁned as follows: f1\nif the point (xs\nn, ys\nn) is a starting or ongoing point of a stroke,\nf2 if the point is the ending point of a stroke, and f3 for a\npadding point. Vector p = [0, 1, 2, ··· , S−1]T is a positional\nencoding vector that represents the temporal position of the\npoints in each sketch Xn.\nGiven D, we aim to model Xn as multiple sparsely\nconnected graphs and learn a deep embedding space, where\nthe high-level semantic tasks can be conducted, e.g., sketch\nrecognition.\nB. Multi-Modal Input Layer\nGiven a sketch Xn, we model its S stroke points as S nodes\nof a graph. Each node has three features: (i) Cs\nn is the spatial\npositional information of the current stroke point s, (ii) fs\nn is\nthe pen state of the current stroke point. This information helps\nto identify the stroke points belonging to the same stroke, and\n(iii) ps is the temporal information of the current stroke point.\nAs sketching is a dynamic process, it is important to use the\ntemporal information.\nThe complete model architecture for our Multi-Graph Trans-\nformer is presented in Figure 3. Let us start by describing the\nLinear \nCoordinates Flag\tBits Pos.\tEnc. \nInput\tEmbedding:\t \nEmbedding\tLookup \n\u0000 × \u0000 × 2 \u0000 × \u0000 × 1 \u0000 × \u0000 × 1\n\u0000 × \u0000 × \u0000\n× \u0000\nGraph\t2 \nMHA \n\u0000 × \u0000 × \u0000\n+ \nGraph\t1 \nMHA \n\u0000 × \u0000 × \u0000\n+ \nGraph\tG \nMHA \n\u0000 × \u0000 × \u0000\n+ \n. . .\nLinear ,\tReLU \nConcatenate: \u0000 × \u0000 × \u0000 \u0000\n\u0000 × \u0000 × \u0000\nBatch\tNorm. \nLinear ,\tReLU \nDropout \nDropout \n\u0000 × \u0000 × \u0000\n+ \nBatch\tNorm. \nGraph\tEmbedding: \u0000 × \u0000 × \u0000\nMLP \nSum\tNodes:\t \u0000 × \u0000\nSoftmax:\t \u0000 × \u0000\n\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000\nMGMHA \nsub-layer \nFF\tsub-layer \nFig. 3. Multi-Graph Transformer architecture. Each MGT layer is composed\nof (i) a Multi-Graph Multi-Head Attention (MGMHA) sub-layer and (ii) a\nposition-wise fully connected Feed-Forward (FF) sub-layer. See details in text.\n“B” denotes batch size.\ninput layer. The ﬁnal vector at node s of the multi-modal input\nlayer is deﬁned as\n(hs\nn)(l=0) = C(E1(Cs\nn), E2(fs\nn), E2(ps)), (1)\nwhere E1(Cs\nn) is the embedding of Cs\nn with a linear layer of\nsize 2×ˆd, E2(fs\nn) and E2(ps) are the embeddings of the ﬂag bit\nfs\nn (3 discrete values) and the position encoding ps (S discrete\nvalues) from an embedding dictionary of size (S +3) ×ˆd, and\nC(·, ·) is the concatenation operator. The node vector (hs\nn)(l=0)\nhas dimension d = 3ˆd. The design of the input layer was\nselected after extensive ablation studies, which are described\nin subsequent sections.\n4\nAttention\tLayer \n\u0000\n\u0000\n\u0000\nGraph \n\u0000\n\u0000\n\u0000\nDropout \n\u0000\n\u0000\n\u0000\nDropout Dropout \n\u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 \u0000 ( ℎ , ℎ , ℎ , \u0000 )\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\nℎ\n× \u0000\nℎ \u0000 \u0000 \u0000 \u0000\nFig. 4. Multi-Head Attention Layer, consisting of several Graph Attention\nLayers in parallel.\nC. Multi-Graph Transformer\nThe initial node embedding (hs\nn)(l=0) is updated by\nstacking L Multi-Graph Transformer (MGT) layers (7). Let\nus describe all layers.\n1) Graph Attention Layer: Let A be a graph adjacency\nmatrix of size S ×S and Q ∈ RS×dq , K ∈ RS×dk , V ∈\nRS×dv be the query, key, and value matrices. We deﬁne a\ngraph attention layer as\nGraphAttention(Q, K, V, A) =A ⊙ softmax(QKT\n√dk\n)V,\n(2)\nwhere ⊙ is the Hadamard product. We simply weight the\n“Scaled Dot-Product Attention” [17] with the graph edge\nweights. We set dq = dk = dv = d\nI , where I is the number\nof attention heads.\n2) Multi-Head Attention Layer: We aggregate the graph\nattentions with multiple heads:\nMultiHead(Q, K, V, A) =C(head1,···, headI)WO, (3)\nwhere WO ∈RIdv×d and each attention head is computed\nwith the graph attention layer (2):\nheadi = GraphAttention(QWQ\ni , KWK\ni , VWV\ni , A), (4)\nwhere WQ\ni ∈Rd×dq , WK\ni ∈Rd×dk , and WV\ni ∈Rd×dv .\nWe add dropout [42] before the linear projections of Q, K\nand V. An illustration of the Multi-Head Attention Layer is\npresented in Figure 4.\n3) Multi-Graph Multi-Head Attention Layer : Given a set\nof adjacency graph matrices {Ag}G\ng=1, we can concatenate\nMulti-Head Attention Layers:\nMultiGraphMultiHeadAttention(Q, K, V, {Ag}G\ng=1) =\nReLU(C(ghead1, ··· , gheadG)W\n˜O),\n(5)\nwhere W˜O ∈RGd×d and each Multi-Head Attention Layer is\ncomputed with (3):\ngheadg = MultiHead(Q, K, V, Ag). (6)\n4) Multi-Graph Transformer Layer: The Multi-Graph\nTransformer (MGT) at layer l for node s is deﬁned as\n(hs\nn)(l) = MGT((hn)(l−1))\n= ˆhs\nn + FF(l)(ˆhs\nn),\n(7)\nwhere the intermediate feature representation ˆhs\nn is deﬁned as:\nˆhs\nn = (MGMHAs\nn)(l)((h1\nn)(l−1), ··· , (hS\nn)(l−1)). (8)\nThe MGT layer is thus composed of (i) a Multi-Graph Multi-\nHead Attention (MGMHA) sub-layer (5) and (ii) a position-\nwise fully connected Feed-Forward (FF) sub-layer. Each MHA\nsub-layer (6) and FF (7) has residual-connection [43] and batch\nnormalization [44]. See Figure 3 for an illustration.\nD. Sketch Embedding and Classiﬁcation Layer\nGiven a sketch Xn with tn key points, its continuous\nrepresentation hn is simply given by the sum over all its node\nfeatures from the last MGT layer:\nhn =\ntn∑\ns=1\n(hs\nn)(L). (9)\nFinally, we use a Multi-Layer Perceptron (MLP) to classify\nthe sketch representation hn, see Figure 3.\nE. Sketch-Speciﬁc Graphs\nIn this section, we discuss the graph structures we used\nin our Graph Transformer layers. We considered two types\nof graphs, which capture local and global geometric sketch\nstructures.\nThe ﬁrst class of graphs focus on representing the local\ngeometry of individual strokes. We choose K-hop graphs\nto describe the local geometry of strokes. The intra-stroke\nadjacency matrix is deﬁned as follows:\nAK-hop\nn,ij =\n{\n1 if j ∈NK-hop\ni and j ∈global(i),\n0 otherwise ,\n(10)\nwhere NK-hop\ni is the K-hop neighborhood of node i and\nglobal(i) is the stroke of node i.\nThe second class of graphs capture the global and temporal\nrelationships between the strokes composing the whole sketch.\nWe deﬁne the extra-stroke adjacency matrix as follows:\nAglobal\nn,ij =\n{\n1 if |i −j|= 1and global(i) ̸= global(j),\n0 otherwise .\n(11)\nThis graph will force the network to pay attention between\ntwo points belonging to two distinct strokes but consecutive\nin time, thus allowing the model to understand the relative\narrangement of strokes.\n5\nTABLE I\nSUMMARY STATISTICS FOR OUR SUBSET OF QUICK DRAW.\nSet # Samples # Truncated (ratio) # Key Points\nmax min mean std\nTraining 345,000 11788 (3.42%) 100 2 43.26 21.85\nValidation 34,500 1218 (3.53%) 100 2 43.24 21.89\nTest 34,500 1235 (3.58%) 100 2 43.20 21.93\nIV. E XPERIMENTS\nA. Experimental Setting\nIn this section, we detail our experimental settings.\n1) Dataset and Pre-Processing: Google QuickDraw [1] 2 3\nis the largest available sketch dataset containing 50 Million\nsketches as simpliﬁed stroke key points in temporal order,\nsampled using the Ramer–Douglas–Peucker algorithm after\nuniformly scaling image coordinates within 0 to 256. Unlike\nsmaller crowd-sourced sketch datasets, e.g., TU-Berlin [45],\nQuickDraw samples were collected via an international\nonline game where users have only 20 seconds to sketch\nobjects from 345 classes, such as cats, dogs, clocks, etc.\nThus, sketch classiﬁcation on QuickDraw not only involves\na diversity of drawing styles, but can also be highly abstract\nand noisy, making it a challenging and practical test-bed\nfor comparing the effectiveness of various neural network\narchitectures. Following recent practices [11], [2], we create\nrandom training, validation and test sets from the full dataset\nby sampling 1000, 100 and 100 sketches respectively from\neach of the 345 categories in QuickDraw. Although the\ntransformer architecture is able to handle sketches with any\nﬁnite number of key points, following [2], we truncate or pad\nall samples to a uniform length of 100 key points/steps to\nfacilitate efﬁcient training of RNN and GNN-based models,\nto save parameters and training time. We provide summary\nstatistics for our training, validation and test sets in Table I,\nand histograms visualizing the key points per sketch are\nshown in Figure 5.\n2) Evaluation Metrics: Our evaluation metric for sketch\nrecognition is “top K accuracy”, the proportion of samples\nwhose true class is in the top K model predictions, for values\nk = 1, 5, 10. (Note that acc.@k = 1.0 means 100%)\n3) Implementation Details: For fair comparison\nunder similar hardware conditions, all experiments were\nimplemented in PyTorch [46] 4 and run on one Nvidia\n1080Ti GPU. For Transformer models, we use the following\nhyperparameter values: S = 100, L = 4, ˆd = 128, G = 3\n(A1-hop, A2-hop, Aglobal), and I = 8 (per graph) for our\nBase model (and ˆd = 256 for our Large model). Our FF\nsub-layer is a d-dimensional linear layer ( d = 3ˆd) followed\nby ReLU [47] and dropout. The MLP Classiﬁer consists of\ntwo 4 ˆd-dimensional linear layers with ReLU and dropout,\n2https://quickdraw.withgoogle.com/data\n3https://github.com/googlecreativelab/quickdraw-dataset\n4https://pytorch.org/\n0 20 40 60 80 100\nKey Points\n0\n2000\n4000\n6000\n8000\n10000\n12000\n14000Sketch Amount\n(a) Training\n0 20 40 60 80 100\nKey Points\n0\n200\n400\n600\n800\n1000\n1200\n1400Sketch Amount (b) Validation\n0 20 40 60 80 100\nKey Points\n0\n200\n400\n600\n800\n1000\n1200\n1400Sketch Amount\n(c) Test\nFig. 5. Histograms of key points per sketch for our subset of QuickDraw.\nThe sharp spike at 100 key points is due to truncation.\nfollowed by a 345-dimensional linear projection representing\nlogits over the 345 categories in QuickDraw. We train all\nmodels by minimizing the softmax cross-entropy loss using\nthe Adam [48] optimizer for 100 epochs. We use an initial\nlearning rate of 5e −5 and multiply by a factor 0.7 every\n10 epochs. We use an early stopping strategy (with the\nhyper-parameter “patience” of 10 epochs) for selecting the\nﬁnal model, and the checkpoint with the highest validation\nperformance is chosen to report test performance.\n4) Baselines: (i) From the perspective of coordinate-based\nsketch recognition, RNN models are a simple-yet-effective\nbaseline. Following Xu et al . [2], we design several bi-\ndirectional LSTM [49] and GRU [50] models at increasing\nparameter budgets comparable with MGT. The ﬁnal RNN\nstates are concatenated and passed to the MLP classiﬁer\ndescribed previously. We use batch size 256, initial learning\nrate 1e−4 and multiply by 0.9 every 10 epochs. We train\nmodels with both our multi-modal input (Section III-B) as\nwell as the 4D input from [2].\n(ii) Although converting sketch coordinates to images adds\ntime overhead in practical settings and can be seen as auxilary\ninformation, we compare MGT to various state-of-the-art CNN\narchitectures. It is important to note that sketch sequences\nwere truncated/padded for training both MGT and RNNs,\nhence image-based CNNs stand as an upper bound in terms of\nperformance. For Inception V3 [51] and MobileNet V2 [52],\ninitial learning rate is 1e−3 and multiplied by 0.5 every 10\nepochs. For other CNN baselines, the initial learning rate and\ndecay are conﬁgured following their original papers. For each\nmodel, we use the maximum possible batch size. Following\nstandard practice in computer vision [43], [53], we employ\nearly stopping based on observing over-ﬁtting in the validation\nloss, and select the checkpoint with the highest validation\naccuracy for evaluation on the test set.\n(iii) To evaluate the effectiveness of the proposed Graph\n6\nTransformer layer, we compare it with popular GNN variants:\nthe Graph Convolutional Network [29] and the Graph Atten-\ntion Network [40] 5. All GNN models follow the same hyper-\nparameter setup as Transformers ( L = 4, ˆd = 256) and are\naugmented with residual connections and batch normalization\nfor fair comparison, following [54]. Optimal hyper-parameters\nand learning rate schedules are selected based on validation\nset performance.\nB. Results\nFor fair comparison with RNN and CNN baselines at\nvarious parameter budgets, we implement two conﬁgurations\nof MGT: Base (10M parameters) and Large (40M parameters).\nAdditionally, we perform several ablation studies to evaluate\nthe effectiveness of our multi-graph architecture and our\nsketch-speciﬁc input design. Our main results are presented\nin Table II.\n1) Comparison with RNN Baselines: We trained RNNs at\nvarious parameter budgets, and present result for the best\nperforming bi-directional LSTM and GRU models in Table\nII: (i) MGT outperforms both LSTM and GRU baselines by\na signiﬁcant margin (by 3% acc.@1 for Base, 5% for Large),\nindicating that both geometry and temporal order of strokes\nare important for sketch representation learning. (ii) Training\nlarger RNNs is harder to converge, leading to degrading\nperformance, e.g., GRUs outperform deeper LSTMs by 2%.\nThese results are not surprising: RNNs are notoriously\nhard to train at scale [58], while Transformer performance\nis known to improve with scale, even with billions of model\nparameters [59].\n2) Comparison with CNN Baselines: Table II also presents\nperformance of several state-of-the-art CNN architectures\nfor computer vision: (i) Inception V3 [51] and MobileNet\nV2 [52] are the best performing CNN architectures. Our\nMGT Base has competitive or better recognition accuracy\nthan all other baselines: AlexNet [21], VGG-11 [55], ResNet\nmodels [43], and DenseNet-201 [53]. (ii) MGT Large has\nsmall performance gap to Inception V3 and MobileNet V2\n(i.e., 72.80% acc.@1 vs. 74.22%, 72.80% acc.@1 vs. 73.10%)\nand outperforms all other CNN architectures by almost 2%.\n(iii) Somewhat counter-intuitively, shallow networks (Incep-\ntion V3, MobileNet V2) outperform deeper networks (ResNet-\n152, Densenet-201) by almost 2%. This result highlights that\nCNNs designed for images with dense colors and textures are\nun-suitable for sparse sketches.\nNote that MobileNet V2 is speciﬁcally designed for fast\ninference on mobile phones and is not directly comparable in\nterms of model parameters.\n3) Ablations for Multi-Graph Architecture: We design sev-\neral ablation studies to evaluate our sketch-speciﬁc multi-graph\narchitecture in Table III: (i) We evaluate Graph Transformers\ntrained on fully-connected graphs, i.e. vanilla Transformers\n5For GAT, we use the same scaled dot-product attention mechanism as GT\nfor efﬁciency.\n(GT #1), fully-connected graphs within strokes (GT #2), as\nwell as random graphs with 10%, 20% and 30% connectivity\n(GT #3, #4, and #5 respectively). We compare their perfor-\nmance with Graph Transformers trained on sketch-speciﬁc\ngraphs A1-hop (GT #6), A2-hop (GT #7), A3-hop (GT #8), and\nAglobal (GT #9). We ﬁnd that vanilla Transformers on fully-\nconnected (52.49% acc.@1) and random graphs (52.71%,\n53.52%, 53.22%) perform poorly compared to sketch-speciﬁc\ngraph structures determined by domain expertise, such as\nfully-connected stroke graphs (64.87%) and A1-hop (70.23%).\nThe superior performance of K-hop graphs suggests that\nTransformers beneﬁt from sparse graphs representing local\nsketch geometry. We also evaluate a combined sketch-speciﬁc\ngraph structure, i.e., A1-hop||A2-hop||Aglobal (GT #10), where\nthe graph connectivity is the logical union set of A1-hop,\nA2-hop, and Aglobal. However, this structure fails to gain\nperformance improvement over A1-hop, A2-hop, and Aglobal,\ndespite involving more domain knowledge.\n(ii) We experiment with various permutations of graphs\nfor multi-graph models (MGT #11-#17). We ﬁnd that using\na 3-graph architecture (MGT #17) combining local sketch\ngeometry ( A1-hop, A2-hop) and global temporal relationships\n(Aglobal) signiﬁcantly boosts performance over 2-graph and 1-\ngraph models (72.80% vs. 72.37% for 2-graph and 70.82%\nfor 1-graph). This result is interesting because using global\ngraphs independently (GT #9) leads to comparatively poor\nperformance (54.88%). Additionally, we found that using\ndiverse graphs (MGT #15, #17) is better than using the same\ngraph (MGT #14). Comparing MGT #14 and MGT #6 further\nshows that performance gains are due to the multi-graph\narchitecture as opposed to more model parameters.\n(iii) We also repeatedly input the adjacency matrix of\nGT #10 ( i.e., A1-hop||A2-hop||Aglobal) three times as the\nmultiple graph structures to train our MGT (see MGT #16\nin Table III). Compared with MGT #17, there is a clear\nperformance gap (71.26% vs. 72.80%). This further validates\nour idea of learning sketch representations through multiple\nseparate graphs.\n4) Comparison with GNN Baselines: In Table IV, we\npresent performance of our Graph Transformer model\ncompared to Graph Convolutional Networks (GCN) [29]\nand Graph Attention Networks (GAT) [40], two popular\nGNN variants: (i) We ﬁnd that all models perform similarly\non fully-connected graphs. Using 1-hop graphs results in\nsigniﬁcant gains for all models, with Transformer performing\nthe best. (ii) Interestingly, both GNNs on fully-connected\ngraphs are outperformed by a simple position-wise embedding\nmethod without any graph structure: each node undergoes\n4 feed-forward (FF) layers followed by summation and\nthe MLP classiﬁer. These results further highlights the\nimportance of sketch-speciﬁc graph structures for the success\nof Transformers. Our ﬁnal models use the Transformer layer,\nwhich implicitly includes the FF sub-layer (7).\n5) Ablations for Multi-Modal Input: In Table V, we\nexperiment with various permutations of our sketch-speciﬁc\nmulti-modal input design. We aggregate information from\n7\nTABLE II\nTEST SET PERFORMANCE OF MGT VS. THE STATE -OF-THE -ART RNN AND CNN ARCHITECTURES . THE 1st/2nd/3rd BEST RESULTS PER COLUMN ARE\nINDICATED IN RED /BLUE /MAGENTA .\nNetwork Conﬁgurations Recognition Accuracy Parameter\nAmountacc.@1 acc.@5 acc.@10\nBi-directional LSTM #1 4D Input, ˆd = 256, L= 4, DropoutLSTM = 0.5, DropoutMLP = 0.15 0.6665 0.8820 0.9189 5,553,241\nBi-directional LSTM #2 4D Input, ˆd = 256, L= 5, DropoutLSTM = 0.5, DropoutMLP = 0.15 0.6524 0.8697 0.9133 7,130,201\nBi-directional GRU 4D Input, ˆd = 256, L= 5, DropoutGRU = 0.5, DropoutMLP = 0.15 0.6768 0.8854 0.9234 5,419,097\nAlexNet [21]\nStandard architecture and conﬁgurations\n0.6808 0.8847 0.9203 58,417,305\nVGG-11 [55] 0.6743 0.8814 0.9191 130,179,801\nInception V3 [51] 0.7422 0.9189 0.9437 25,315,474\nResNet-18 [43] 0.7031 0.9030 0.9351 11,353,497\nResNet-34 [43] 0.7009 0.9010 0.9347 21,461,657\nResNet-152 [43] 0.6924 0.8973 0.9312 58,850,713\nDenseNet-201 [53] 0.7050 0.9013 0.9331 18,755,673\nMobileNet V2 [52] 0.7310 0.9161 0.9429 2,665,817\nSCNet [56] 0.7123 0.9026 0.9351 24,222,489\nResNet-102+BSConv-U [57] 0.7172 0.9037 0.9334 7,029,791\nVanillaTransformer [17] ˆd = 256, L= 4, I= 8, Dropout= 0.1, Fully-connected graph 0.5249 0.7802 0.8486 14,029,401\nMGT (Base) ˆd = 128, L= 4, I= 24, Dropout= 0.1, A1-hop,A2-hop,Aglobal graphs 0.7070 0.9030 0.9351 10,096,601\nMGT (Large) ˆd = 256, L= 4, I= 24, Dropout= 0.25, A1-hop,A2-hop,Aglobal graphs 0.7280 0.9106 0.9387 39,984,729\nTABLE III\nABLATION STUDY FOR MULTI -GRAPH ARCHITECTURE OF MGT. GT DENOTES SINGLE -GRAPH VARIANTS OF MGT. T HE 1st/2nd BEST RESULTS PER\nCOLUMN ARE INDICATED IN RED /BLUE . ||DENOTES THE LOGICAL UNION OPERATION .\nNetwork Conﬁgurations Recognition Accuracy Parameter\nAmountG Graph Structure Itotal ˆd L Dropout acc.@1 acc.@5 acc.@10\nGT #1 1 Fully-connected ( vanilla ) 8 256 4 0.10 0.5249 0.7802 0.8486 14,029,401\nGT #2 1 Intra-stroke Fully-connected 8 256 4 0.10 0.6487 0.8697 0.9151 14,029,401\nGT #3 1 Random (10%) 8 256 4 0.10 0.5271 0.7890 0.8589 14,029,401\nGT #4 1 Random (20%) 8 256 4 0.10 0.5352 0.7945 0.8617 14,029,401\nGT #5 1 Random (30%) 8 256 4 0.10 0.5322 0.7917 0.8588 14,029,401\nGT #6 1 A1-hop 8 256 4 0.10 0.7023 0.8974 0.9303 14,029,401\nGT #7 1 A2-hop 8 256 4 0.10 0.7082 0.8999 0.9336 14,029,401\nGT #8 1 A3-hop 8 256 4 0.10 0.7028 0.8991 0.9327 14,029,401\nGT #9 1 Aglobal 8 256 4 0.10 0.5488 0.8009 0.8659 14,029,401\nGT #10 1 A1-hop ||A2-hop ||Aglobal 8 256 4 0.10 0.7057 0.9021 0.9346 14,029,401\nMGT #11 2 A1-hop , A2-hop 16 256 4 0.25 0.7149 0.9049 0.9361 28,188,249\nMGT #12 2 A1-hop , Aglobal 16 256 4 0.25 0.7111 0.9041 0.9355 28,188,249\nMGT #13 2 A2-hop , Aglobal 16 256 4 0.25 0.7237 0.9102 0.9400 28,188,249\nMGT #14 3 A1-hop , A1-hop , A1-hop 24 256 4 0.25 0.7077 0.9020 0.9340 39,984,729\nMGT #15 3 A1-hop , A2-hop , A3-hop 24 256 4 0.25 0.7156 0.9066 0.9365 39,984,729\nMGT #16 3 A1-hop ||A2-hop ||Aglobal 24 256 4 0.25 0.7126 0.9051 0.9372 39,984,729\nMGT #17 3 A1-hop , A2-hop , Aglobal 24 256 4 0.25 0.7280 0.9106 0.9387 39,984,729\nTABLE IV\nTEST SET PERFORMANCE OF GRAPH TRANSFORMER VS . OTHER GNN VARIANTS . THE 1st/2nd BEST RESULTS PER COLUMN ARE INDICATED IN\nRED /BLUE .\nNetwork Graph Structure Recognition Accuracy Parameter\nAmountacc.@1 acc.@5 acc.@10\nGraph Convolutional Networks (GCN) [29] fully-connected 0.4098 0.7384 0.8213 6,948,441A1-hop 0.6800 0.8869 0.9224\nGraph Attention Networks (GAT) [40] fully-connected 0.4098 0.6960 0.7897 11,660,889A1-hop 0.6977 0.8952 0.9298\nGraph Transformer (GT) fully-connected 0.5242 0.7796 0.8465 14,029,401A1-hop 0.7057 0.8992 0.9311\nposition-wise feed-forward None 0.5296 0.7901 0.8576 4,586,073\n8\nTABLE V\nABLATION STUDY FOR MULTI -MODAL INPUT FOR MGT (L ARGE ). N OTATIONS : “+” AND “C(··· )” DENOTE “SUM ” AND “CONCATENATE ”,\nRESPECTIVELY . THE 1st/2nd BEST RESULTS PER COLUMN ARE INDICATED IN RED /BLUE .\nInput Permutation\nRecognition Accuracy\nacc.@1 acc.@5 acc.@10\ncoordinate 0.6512 0.8735 0.9162\ncoordinate + ﬂag bit 0.6568 0.8762 0.9176\ncoordinate + ﬂag bit + position encoding 0.6600 0.8766 0.9182\nC (coordinate, ﬂag bit) 0.7017 0.8996 0.9321\nC (coordinate, ﬂag bit, position encoding) 0.7280 0.9106 0.9387\n4D Input 0.6559 0.8758 0.9175\n4D Input + position encoding 0.6606 0.8781 0.9190\nC (4D Input, position encoding) 0.7117 0.9048 0.9366\nspatial (coordinates), semantic (ﬂag bits), and temporal\n(position encodings) modalities via summation (as in\nTransformers for NLP) or concatenation: (i) Effectively\nusing all modalities is important for performance ( e.g.,\n“C(coordinate, ﬂag bit, position encoding)” outperforms\n“coordinate” and “ C(coordinate, ﬂag bit)”: 72.80% acc.@1\nvs. 65.12%, 70.17%). (ii) Concatenation works better than\n4D input as well as summation ( e.g., “ C(coordinate, ﬂag\nbit, position encoding)” outperforms “ C(4D Input, position\nencoding)” and “coordinate + ﬂag bit + position encoding”:\n72.80% vs. 71.17%, 66.06%).\n6) Qualitative Results: In Figure 6 and Figure 7, we\nvisualize attention heads at each layer of MGT for various\ntest set samples. Each sub-ﬁgure contains attention heads for\neach of the three graphs ( A1-hop, A2-hop, Aglobal), and each of\nthe rows #1-#4 in each sub-ﬁgure correspond to layers #1-#4.\nDarker reds indicate higher attention values. All ﬁgures are\nbest viewed in color.\nAttention heads in the initial layers attend very strongly to\ncertain neighbors and very weakly to others, i.e., the model\nbuilds local patterns for sketch sub-components (strokes)\nthrough message passing along their contours. In penultimate\nlayers, the intensity of neighborhood attention is signiﬁcantly\nlower and evenly distributed, indicating that the model is\naggregating information from various strokes at each node.\nAdditionally, we believe Aglobal graphs are critical for\nmessage passing between strokes, enabling the model to\nunderstand their relative arrangement. For example, in\nFigure 6, both the head and feet of the bird are attached to\nthe bottom of its body. In Figure 7, the feet of the teddy bear\nare associated.\n7) Comparison for Time Cost: To demonstrate the speed\nadvantage of our model, we also compare our MGT with the\nstrongest CNN baselines, i.e., Inception V3, MobileNet V2.\nWe report the total time taken by models to perform inference\nover 34,500 sketches from the test set in Table VI. All models\nwere implemented in PyTorch and run on an Intel Xeon CPU\nE5-2690 v4 server using a single Nvidia 1080Ti GPU, with\nbatch size of 256 and 16 workers per run.\nIn Table VI, we observe: our MGT infers obviously faster\nTABLE VI\nEVALUATION TIMING COMPARISON FOR MULTI -GRAPH TRANSFORMER\nAND CNN S, AVERAGED OVER 3 RUNS . INFERENCE TIME IS THE TOTAL\nWALL CLOCK TIME FOR PERFORMING INFERENCE OVER 34,500 SKETCHES\nFROM THE TEST SET .\nNetwork Inference Time\nParameter\nAmount\nInception V3 2.682 ±0.154s 25,315,474\nMobileNet V2 1.423 ±0.070s 2,665,817\nMGT (Base) 0.849 ±0.044s 10,096,601\nMGT (Large) 0.843 ±0.048s 39,984,729\nTABLE VII\nPERFORMANCE COMPARISON ON THE RELATION EXTRACTION TASK ON\nSEMEVAL-2010 TASK 8. T HE 1st/2nd BEST RESULTS PER COLUMN ARE\nINDICATED IN RED /BLUE .\nNetwork Graph Structure Prediction\nAccuracy\ntextual CNN [60] - 0.8660\nTransformer [61] fully-connected 0.8900\nTransformer fully- and partially-connected 0.8945\nthan both Inception V3 and MobileNet V2.\nNote that, unlike CNNs and RNNs, GNNs and GTs for\nsparse graph data formats are not natively supported by\nPyTorch. We believe that our MGT can infer faster in our\nfuture work, if we further speed up MGT via using the\ntailor-made GNN libraries such as Deep Graph Library 6 and\nPyTorch Geometric 7.\n8) Evaluation on Relation Extraction: The main idea of\nour multi-graph transformer is injecting domain knowledge\ninto Transformers through domain-speciﬁc graphs. We also\ntry to evaluate this idea in other modalities beyond sketch,\ne.g., NLP tasks. Thus, we transfer our multi-graph transformer\nidea to conduct Relation Extraction (RE) on a RE benchmark\n(SemEval-2010 task 8 [62]), outperforming the state-of-the-art\nCNNs by a clear margin (as reported in Table VII, prediction\n6https://www.dgl.ai/\n7https://github.com/rusty1s/pytorch geometric\n9\n(a) A1-hop\n(b) A2-hop\n(c) Aglobal\nFig. 6. Attention heads at each layer of MGT for a test set sample labelled bird. Each layer has I = 8 attention heads per graph in total. Darker reds\nindicate higher attention values. Best viewed in color.\n10\n(a) A1-hop\n(b) A2-hop\n(c) Aglobal\nFig. 7. Attention heads at each layer of MGT for a test set sample labelled teddy. Each layer has I = 8 attention heads per graph in total. Darker reds\nindicate higher attention values. Best viewed in color.\n11\n0  1  …  1  1  1  1  1  …  1  1  …  1  0  0 \n… … … \nFig. 8. A partially-connected graph illustration for a long sentence. The\nentities and the other words are denoted by the solid and hollow squares,\nrespectively. The corresponding attention mask is also provided.\naccuracy: ours (89.45%) vs. CNN [60] (86.60%)). In particu-\nlar, when we use Bidirectional Encoder Representations from\nTransformers (BERT) [63] as Transformer-based encoder, each\nsentence will be encoded as a fully-connected graph. Given\na long sentence, if its two entities are overly far from each\nother, the RE model should give more attention on the words\nbetween and around the two entities. Therefore, we also use\nmultiple graph structures to inject this domain-knowledge into\nthe Transformer-based encoder: (i) Fully-connected: In the\nearly and middle stages of training, we allow the transformer\nto encode each sentence as a fully-connected graph, regardless\nof its length. (ii) Partially-connected: In the last few epochs, if\nin a given long sentence its two entities are far apart, we force\nthe transformer to encode the long sentence as a partially-\nconnected graph, which consists of the words between and\naround the two entities. We use a binary attention mask to\nencode this. See Figure 8 for an illustration.\nThis experimental phenomenon further encourages us to\nexplore the applications of using domain-speciﬁc graph struc-\ntures to inject the domain-knowledge into Transformers in\nother modalities and tasks.\nV. C ONCLUSION\nThis paper introduces a novel representation of free-hand\nsketches as multiple sparsely connected graphs. We design a\nMulti-Graph Transformer (MGT) for capturing both geometric\nstructure and temporal information from sketch graphs. The\nintrinsic traits of the MGT architecture include: (i) using\ngraphs as universal representations of sketch geometry, as well\nas temporal and semantic information, (ii) injecting domain\nknowledge into Transformers through sketch-speciﬁc graphs,\nand (iii) making full use of multiple intra-stroke and extra-\nstroke graphs.\nFor sketch community: We, for the ﬁrst time, propose\nto model sketches as sparsely connected graphs. For GNN\ncommunity: We design a novel graph transformer model\nthat considers prior/domain knowledge via multiple graph\nstructures. This multi-graph modeling idea works well for both\nsketch and other modalities.\nWe hope MGT can serve as a foundation for future work\nin sketch applications and network architectures, motivating\nthe community towards sketch representation learning using\ngraphs. Additionally, for the graph neural network (GNN)\ncommunity, we hope that MGT helps free-hand sketch become\na new test-bed for GNNs.\nVI. F UTURE WORK\nOur future work will focus on applying our MGT as a\ngeneral neural representation in various sketch tasks, e.g.,\nsketch generation. More code and results will be updated\ncontinuously on our project page 8.\nREFERENCES\n[1] D. Ha and D. Eck, “A neural representation of sketch drawings,” arXiv\npreprint arXiv:1704.03477, 2017.\n[2] P. Xu, Y . Huang, T. Yuan, K. Pang, Y .-Z. Song, T. Xiang, T. M.\nHospedales, Z. Ma, and J. Guo, “Sketchmate: Deep hashing for million-\nscale human sketch retrieval,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , 2018, pp. 8090–8098.\n[3] F. Liu, X. Deng, Y .-K. Lai, Y .-J. Liu, C. Ma, and H. Wang, “Sketchgan:\nJoint sketch completion and recognition with generative adversarial\nnetwork,” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2019, pp. 5830–5839.\n[4] R. K. Sarvadevabhatla, J. Kundu, and V . Babu R, “Enabling my robot\nto play pictionary: Recurrent neural networks for sketch recognition,” in\nProceedings of the 24th ACM international conference on Multimedia ,\n2016, pp. 247–251.\n[5] Y . Ye, Y . Lu, and H. Jiang, “Human’s scene sketch understanding,”\nin Proceedings of the 2016 ACM on International Conference on\nMultimedia Retrieval, 2016, pp. 355–358.\n[6] P. Sangkloy, N. Burnell, C. Ham, and J. Hays, “The sketchy database:\nlearning to retrieve badly drawn bunnies,” ACM Transactions on Graph-\nics, vol. 35, no. 4, pp. 1–12, 2016.\n[7] L. Liu, F. Shen, Y . Shen, X. Liu, and L. Shao, “Deep sketch hashing:\nFast free-hand sketch-based image retrieval,” inProceedings of the IEEE\nconference on computer vision and pattern recognition, 2017, pp. 2862–\n2871.\n[8] Y . Shen, L. Liu, F. Shen, and L. Shao, “Zero-shot sketch-image hashing,”\nin Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018, pp. 3598–3607.\n[9] J. Collomosse, T. Bui, and H. Jin, “Livesketch: Query perturbations\nfor guided sketch-based visual search,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition , 2019, pp.\n2879–2887.\n[10] A. Dutta and Z. Akata, “Semantically tied paired cycle consistency for\nzero-shot sketch-based image retrieval,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition , 2019, pp.\n5089–5098.\n[11] S. Dey, P. Riba, A. Dutta, J. Llados, and Y .-Z. Song, “Doodle to search:\nPractical zero-shot sketch-based image retrieval,” in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition , 2019,\npp. 2179–2188.\n[12] W. Chen and J. Hays, “Sketchygan: Towards diverse and realistic\nsketch to image synthesis,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , 2018, pp. 9416–9425.\n[13] Y . Lu, S. Wu, Y .-W. Tai, and C.-K. Tang, “Image generation from\nsketch constraint using contextual gan,” in Proceedings of the European\nConference on Computer Vision , 2018, pp. 205–220.\n[14] P. Xu, Z. Song, Q. Yin, Y . Song, and L. Wang, “Deep self-supervised\nrepresentation learning for free-hand sketch,” IEEE Transactions on\nCircuits and Systems for Video Technology , 2020.\n[15] H. Lin, Y . Fu, X. Xue, and Y .-G. Jiang, “Sketch-bert: Learning\nsketch bidirectional encoder representation from transformers by self-\nsupervised learning of sketch gestalt,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition , 2020, pp.\n6758–6767.\n[16] Q. Yu, Y . Yang, Y .-Z. Song, T. Xiang, and T. Hospedales, “Sketch-a-net\nthat beats humans,” arXiv preprint arXiv:1501.07873 , 2015.\n[17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\nneural information processing systems , vol. 30, pp. 5998–6008, 2017.\n[18] P. W. Battaglia, J. B. Hamrick, V . Bapst, A. Sanchez-Gonzalez, V . Zam-\nbaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner\net al., “Relational inductive biases, deep learning, and graph networks,”\narXiv preprint arXiv:1806.01261 , 2018.\n[19] K. Li, K. Pang, Y .-Z. Song, T. Xiang, T. M. Hospedales, and H. Zhang,\n“Toward deep universal sketch perceptual grouper,” IEEE Transactions\non Image Processing , vol. 28, no. 7, pp. 3219–3231, 2019.\n[20] P. Xu, “Deep learning for free-hand sketch: A survey,” arXiv preprint\narXiv:2001.02600, 2020.\n8https://github.com/PengBoXiangShang/multigraph transformer\n12\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” Communications of the ACM,\nvol. 60, no. 6, pp. 84–90, 2017.\n[22] J. Song, Q. Yu, Y .-Z. Song, T. Xiang, and T. M. Hospedales, “Deep\nspatial-semantic attention for ﬁne-grained sketch-based image retrieval,”\nin Proceedings of the IEEE International Conference on Computer\nVision, 2017, pp. 5551–5560.\n[23] Q. Jia, M. Yu, X. Fan, and H. Li, “Sequential dual deep learning\nwith shape and texture features for sketch recognition,” arXiv preprint\narXiv:1708.02716, 2017.\n[24] L. Li, C. Zou, Y . Zheng, Q. Su, H. Fu, and C. L. Tai, “Sketch-r2cnn: An\nrnn-rasterization-cnn architecture for vector sketch recognition,” IEEE\nTransactions on Visualization and Computer Graphics , 2020.\n[25] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y . Philip, “A\ncomprehensive survey on graph neural networks,” IEEE Transactions\non Neural Networks and Learning Systems , pp. 1–21, 2020.\n[26] J. Bruna, W. Zaremba, A. Szlam, and Y . Lecun, “Spectral networks and\nlocally connected networks on graphs,” in International Conference on\nLearning Representations, 2014.\n[27] M. Defferrard, X. Bresson, and P. Vandergheynst, “Convolutional neural\nnetworks on graphs with fast localized spectral ﬁltering,” Advances in\nneural information processing systems , vol. 29, pp. 3844–3852, 2016.\n[28] S. Sukhbaatar, R. Fergus et al. , “Learning multiagent communication\nwith backpropagation,” Advances in neural information processing sys-\ntems, vol. 29, pp. 2244–2252, 2016.\n[29] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with graph\nconvolutional networks,” in International Conference on Learning Rep-\nresentations, 2017.\n[30] W. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation\nlearning on large graphs,” in Advances in neural information processing\nsystems, 2017, pp. 1024–1034.\n[31] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M.\nBronstein, “Geometric deep learning on graphs and manifolds using\nmixture model cnns,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition , 2017, pp. 5115–5124.\n[32] U. S. Shanthamallu, J. J. Thiagarajan, H. Song, and A. Spanias,\n“Gramme: Semisupervised learning using multilayered graph attention\nmodels,” IEEE Transactions on Neural Networks and Learning Systems ,\nvol. 31, no. 10, pp. 3977–3988, 2020.\n[33] S. Edunov, M. Ott, M. Auli, and D. Grangier, “Understanding back-\ntranslation at scale,” arXiv preprint arXiv:1808.09381 , 2018.\n[34] Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. F. Wong, and L. S. Chao,\n“Learning deep transformer models for machine translation,” arXiv\npreprint arXiv:1906.01787, 2019.\n[35] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving\nlanguage understanding by generative pre-training,” OpenAI Blog, 2018.\n[36] Z. Dai, Z. Yang, Y . Yang, W. W. Cohen, J. Carbonell, Q. V . Le, and\nR. Salakhutdinov, “Transformer-xl: Attentive language models beyond\na ﬁxed-length context,” arXiv preprint arXiv:1901.02860 , 2019.\n[37] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[38] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. Salakhutdinov, and Q. V . Le,\n“Xlnet: Generalized autoregressive pretraining for language understand-\ning,” arXiv preprint arXiv:1906.08237 , 2019.\n[39] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine translation by\njointly learning to align and translate,” arXiv preprint arXiv:1409.0473,\n2014.\n[40] P. Veli ˇckovi´c, G. Cucurull, A. Casanova, A. Romero, P. Li `o, and\nY . Bengio, “Graph Attention Networks,” in International Conference\non Learning Representations , 2018.\n[41] Z. Ye, Q. Guo, Q. Gan, X. Qiu, and Z. Zhang, “Bp-transformer:\nModelling long-range context via binary partitioning,” arXiv preprint\narXiv:1911.04070, 2019.\n[42] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-\ndinov, “Dropout: A simple way to prevent neural networks from over-\nﬁtting,” The journal of machine learning research , vol. 15, no. 1, pp.\n1929–1958, 2014.\n[43] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition , 2016, pp. 770–778.\n[44] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,” arXiv preprint\narXiv:1502.03167, 2015.\n[45] M. Eitz, J. Hays, and M. Alexa, “How do humans sketch objects?” ACM\nTransactions on graphics, vol. 31, no. 4, pp. 1–10, 2012.\n[46] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al. , “Pytorch: An\nimperative style, high-performance deep learning library,” in Advances\nin neural information processing systems , 2019, pp. 8026–8037.\n[47] X. Glorot, A. Bordes, and Y . Bengio, “Deep sparse rectiﬁer neural\nnetworks,” in Proceedings of the fourteenth international conference on\nartiﬁcial intelligence and statistics , 2011, pp. 315–323.\n[48] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\narXiv preprint arXiv:1412.6980 , 2014.\n[49] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural\ncomputation, 1997.\n[50] K. Cho, B. Van Merri ¨enboer, D. Bahdanau, and Y . Bengio, “On the\nproperties of neural machine translation: Encoder-decoder approaches,”\narXiv preprint arXiv:1409.1259 , 2014.\n[51] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking\nthe inception architecture for computer vision,” in Proceedings of the\nIEEE conference on computer vision and pattern recognition , 2016, pp.\n2818–2826.\n[52] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen,\n“Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings\nof the IEEE conference on computer vision and pattern recognition ,\n2018, pp. 4510–4520.\n[53] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely\nconnected convolutional networks,” in Proceedings of the IEEE confer-\nence on computer vision and pattern recognition , 2017, pp. 4700–4708.\n[54] X. Bresson and T. Laurent, “An experimental study of neural networks\nfor variable graphs,” in International Conference on Learning Represen-\ntations Workshop, 2018.\n[55] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” arXiv preprint arXiv:1409.1556 , 2014.\n[56] J.-J. Liu, Q. Hou, M.-M. Cheng, C. Wang, and J. Feng, “Improving con-\nvolutional networks with self-calibrated convolutions,” in Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition ,\n2020, pp. 10 096–10 105.\n[57] D. Haase and M. Amthor, “Rethinking depthwise separable convolu-\ntions: How intra-kernel correlations lead to improved mobilenets,” in\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2020, pp. 14 600–14 609.\n[58] R. Pascanu, T. Mikolov, and Y . Bengio, “On the difﬁculty of training\nrecurrent neural networks,” in International conference on machine\nlearning, 2013, pp. 1310–1318.\n[59] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catan-\nzaro, “Megatron-lm: Training multi-billion parameter language models\nusing gpu model parallelism,” arXiv preprint arXiv:1909.08053 , 2019.\n[60] X. Guo, H. Zhang, H. Yang, L. Xu, and Z. Ye, “A single attention-based\ncombination of cnn and rnn for relation classiﬁcation,” IEEE Access ,\nvol. 7, pp. 12 467–12 475, 2019.\n[61] H. Wang, M. Tan, M. Yu, S. Chang, D. Wang, K. Xu, X. Guo, and\nS. Potdar, “Extracting multiple-relations in one-pass with pre-trained\ntransformers,” arXiv preprint arXiv:1902.01030 , 2019.\n[62] I. Hendrickx, S. N. Kim, Z. Kozareva, P. Nakov, D. O. S ´eaghdha,\nS. Pad ´o, M. Pennacchiotti, L. Romano, and S. Szpakowicz, “Semeval-\n2010 task 8: Multi-way classiﬁcation of semantic relations between pairs\nof nominals,” arXiv preprint arXiv:1911.10422 , 2019.\n[63] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8123669028282166
    },
    {
      "name": "Transformer",
      "score": 0.6425954103469849
    },
    {
      "name": "Sketch",
      "score": 0.6399986743927002
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5324704647064209
    },
    {
      "name": "Sketch recognition",
      "score": 0.5011141300201416
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49492520093917847
    },
    {
      "name": "Graph",
      "score": 0.45981699228286743
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.4516828954219818
    },
    {
      "name": "Recurrent neural network",
      "score": 0.42286813259124756
    },
    {
      "name": "Named-entity recognition",
      "score": 0.41998663544654846
    },
    {
      "name": "Feature learning",
      "score": 0.41907206177711487
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3905664086341858
    },
    {
      "name": "Machine learning",
      "score": 0.37424227595329285
    },
    {
      "name": "Artificial neural network",
      "score": 0.3332711458206177
    },
    {
      "name": "Task (project management)",
      "score": 0.3173198699951172
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3022899031639099
    },
    {
      "name": "Algorithm",
      "score": 0.14909875392913818
    },
    {
      "name": "Gesture recognition",
      "score": 0.08651620149612427
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Gesture",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}