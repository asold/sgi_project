{
  "title": "Review of: \"Limitations of and Lessons from the Learning of Large Language Models\"",
  "url": "https://openalex.org/W4390704595",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2158690399",
      "name": "Jean Lieber",
      "affiliations": [
        "Université de Lorraine"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4390704595"
  ],
  "abstract": "I must say that my confidence level is low, which makes some technical parts of the paper difficult to understand.See below for comments to make your paper readable to a wider audience (including me, for a deeper understanding)...The summary below is based on my understanding of the paper.If it contains deep errors, this reflects my difficulty of understanding it... SUMMARY =======LLMs are based on a sequence of tokens bound to a maximal size that is rather large (1000s) but finite.The author infers from that fact that the \"inference capabilities\" of learning based on LLMs are local, in some way.Now, it is argued from other studies that \"local inferences\" corresponds to restricting the logic involved: intuitionistic logic requires \"local\" inferences, while classical logic (including, e.g., the excluded middle) cannot be inferred by means of local inferences.From this, it is concluded that LLMs cannot handle classical logic, hence a limitation of these models, as they are currently defined and implemented.",
  "full_text": "Review of: \"Limitations of and Lessons from the Learning of\nLarge Language Models\"\nJean Lieber\n1\n1\n University of Lorraine\nPotential competing interests:\n No potential competing interests to declare.\nPRELIMINARY STATEMENTS\n======================\nI must say that my confidence level is low, which makes\nsome technical parts of the paper difficult to understand.\nSee below for comments to make your paper readable to a wider\naudience (including me, for a deeper understanding)...\nThe summary below is based on my understanding of the paper.\nIf it contains deep errors, this reflects my difficulty of\nunderstanding it...\nSUMMARY\n=======\nLLMs are based on a sequence of tokens bound to a maximal size\nthat is rather large (1000s) but finite. The author infers\nfrom that fact that the \"inference capabilities\" of learning\nbased on LLMs are local, in some way. Now, it is argued from\nother studies that \"local inferences\" corresponds to restricting\nthe logic involved: intuitionistic logic requires \"local\" inferences,\nwhile classical logic (including, e.g., the excluded middle) cannot\nbe inferred by means of local inferences. From this, it is concluded\nthat LLMs cannot handle classical logic, hence a limitation of\nthese models, as they are currently defined and implemented.\nGENERAL REMARKS, QUESTIONS, AND SUGGESTIONS\n==========================================\nQeios, CC-BY 4.0   ·   Review, \nJanuary 10, 2024\nQeios ID: LFGWOS   ·   https://doi.org/10.32388/LFGWOS\n1\n/\n3\nOne interest of this paper is that it can contribute to a debate\non the coverage of LLMs from a theoretical viewpoint.\nFrom my opinion, the paper needs some improvements to be more\nconvincing (to me, at least): the argument is presented very quickly\nand should be improved.\nA counter-argument is that the finite size of each element of\nthe dataset is something one cannot expect not to have, given\nthe fact that computers have finite-size memory.\n(By the way, it is known from the no free lunch theorem of\nmachine learning that every learning model fails to be the\nbest one for every training dataset and every test dataset.)\nBecause of this and because of the more particular remarks\nbelow (that are also important ones), I suggest that this\npaper should be reworked in depth to make it more\nunderstandable and more convincing.\nOTHER REMARKS (but not only details)\n=============\n(Page numbering according to the downloaded pdf file.)\nPage 1\nThe third paragraph of the introduction (\"The results of this\npaper...\") suggests that your work /might/ give insights on some\nproperties that a training database should have in order to\nwell train an LLM so as to achieve \"some logical behavior\".\nPage 2\nIn the section \"Large Language Models\", 2-3 lines before the end:\n\"this is the only piece of information needed in the current\nargumentation.\" The phrasing is a little bit odd to me, in particular,\nbecause of the use of the word \"thing\".\nAbout the two rules of lambda-calculus. The right arrow symbol is\nalready used above with a different meaning, and it is confusing.\nQeios, CC-BY 4.0   ·   Review, \nJanuary 10, 2024\nQeios ID: LFGWOS   ·   https://doi.org/10.32388/LFGWOS\n2\n/\n3\nLast paragraph before table 1:\n- There is something missing in the sentence \"It was found that proofs\n in intuitionistic logic and expressions (i.e., programs) in typed lambda\n calculus.\"\n- \"map to long programs\": remove \"lo\"\nPage 3\nThe formal part about the lambda-mu calculus is very hard for me, and I think\nthat the paper can be improved there, either by giving more details and\nmore rigor in definitions or by just giving the consequences of it that\nare needed, with the appropriate references (I am not familiar with\nthe lambda-mu calculus, and I had to search the web to reach a shallow\nunderstanding of what you have written).\nIn particular, the two rules you present are hard to understand:\n- As on page 2, some of the arrows should use another symbol: both\n rules use \"-->\" twice, and I imagine that they don't have the same \n meaning? Trying to match your rules with the ones I find on the web,\n some \"-->\" have to be understood as substitutions (that are defined\n with the symbol \":=\" for lambda calculus, on the previous page...).\n- [alpha]a is introduced, but you also use the notation alpha[something],\n that was not.\nPage 4\nThe argument should be developed: this is supposed to be the core of\nyour paper.\nPage 5\nReference Parigot, 92: \"Lambda-my\" should be substituted with \"Lambda-mu\".\n \nQeios, CC-BY 4.0   ·   Review, \nJanuary 10, 2024\nQeios ID: LFGWOS   ·   https://doi.org/10.32388/LFGWOS\n3\n/\n3",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.510190486907959
    },
    {
      "name": "Mathematics education",
      "score": 0.3835860788822174
    },
    {
      "name": "Linguistics",
      "score": 0.37404149770736694
    },
    {
      "name": "Natural language processing",
      "score": 0.3679043650627136
    },
    {
      "name": "Psychology",
      "score": 0.3416069746017456
    },
    {
      "name": "Artificial intelligence",
      "score": 0.32027918100357056
    },
    {
      "name": "Philosophy",
      "score": 0.08173829317092896
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I90183372",
      "name": "Université de Lorraine",
      "country": "FR"
    }
  ]
}