{
  "title": "MAPLE: A Framework for Active Preference Learning Guided by Large Language Models",
  "url": "https://openalex.org/W4409361378",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2973156046",
      "name": "Saaduddin Mahmud",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A5068693731",
      "name": "Mason Nakamura",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A348962114",
      "name": "Shlomo Zilberstein",
      "affiliations": [
        "University of Massachusetts Amherst"
      ]
    },
    {
      "id": "https://openalex.org/A2973156046",
      "name": "Saaduddin Mahmud",
      "affiliations": [
        "University of Massachusetts Amherst",
        "Amherst College"
      ]
    },
    {
      "id": "https://openalex.org/A5068693731",
      "name": "Mason Nakamura",
      "affiliations": [
        "University of Massachusetts Amherst",
        "Amherst College"
      ]
    },
    {
      "id": "https://openalex.org/A348962114",
      "name": "Shlomo Zilberstein",
      "affiliations": [
        "University of Massachusetts Amherst",
        "Amherst College"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1999874108",
    "https://openalex.org/W4393335679",
    "https://openalex.org/W2786426168",
    "https://openalex.org/W4220972421",
    "https://openalex.org/W3029228949",
    "https://openalex.org/W2763110165",
    "https://openalex.org/W2980192220",
    "https://openalex.org/W2939047085",
    "https://openalex.org/W3008082783",
    "https://openalex.org/W6844160198",
    "https://openalex.org/W6849156259",
    "https://openalex.org/W2763152536",
    "https://openalex.org/W6848909144",
    "https://openalex.org/W6735374517",
    "https://openalex.org/W3201919343",
    "https://openalex.org/W6847631053",
    "https://openalex.org/W3092156990",
    "https://openalex.org/W3037476194",
    "https://openalex.org/W1971913572",
    "https://openalex.org/W6891765477",
    "https://openalex.org/W2899455150",
    "https://openalex.org/W6855622313",
    "https://openalex.org/W4289422208",
    "https://openalex.org/W6948505540",
    "https://openalex.org/W4390093735",
    "https://openalex.org/W4378771755",
    "https://openalex.org/W1591675293",
    "https://openalex.org/W2798823231",
    "https://openalex.org/W2735318784",
    "https://openalex.org/W4387634951",
    "https://openalex.org/W6851767921",
    "https://openalex.org/W6741115023",
    "https://openalex.org/W6729780516",
    "https://openalex.org/W2986878711",
    "https://openalex.org/W6754981700",
    "https://openalex.org/W2098774185",
    "https://openalex.org/W6688515271"
  ],
  "abstract": "The advent of large language models (LLMs) has sparked significant interest in using natural language for preference learning. However, existing methods often suffer from high computational burdens, taxing human supervision, and lack of interpretability. To address these issues, we introduce MAPLE, a framework for large language model-guided Bayesian active preference learning. MAPLE leverages LLMs to model the distribution over preference functions, conditioning it on both natural language feedback and conventional preference learning feedback, such as pairwise trajectory rankings. MAPLE employs active learning to systematically reduce uncertainty in this distribution and incorporates a language-conditioned active query selection mechanism to identify informative and easy-to-answer queries, thus reducing the burden on humans. We evaluate MAPLE's sample efficiency and preference inference quality across two benchmarks, including a real-world vehicle route planning benchmark using OpenStreetMap data. Our results demonstrate that MAPLE accelerates the learning process and effectively improves humans' ability to answer queries.",
  "full_text": "MAPLE: A Framework for Active Preference Learning\nGuided by Large Language Models\nSaaduddin Mahmud, Mason Nakamura, Shlomo Zilberstein\nManning College of Information and Computer Sciences\nUniversity of Massachusetts Amherst\n{smahmud,mnakamura,shlomo}@umass.edu\nAbstract\nThe advent of large language models (LLMs) has sparked\nsignificant interest in using natural language for prefer-\nence learning. However, existing methods often suffer from\nhigh computational burdens, taxing human supervision, and\nlack of interpretability. To address these issues, we in-\ntroduce MAPLE, a framework for large language model-\nguided Bayesian active preference learning. MAPLE lever-\nages LLMs to model the distribution over preference func-\ntions, conditioning it on both natural language feedback and\nconventional preference learning feedback, such as pairwise\ntrajectory rankings. MAPLE employs active learning to sys-\ntematically reduce uncertainty in this distribution and incor-\nporates a language-conditioned active query selection mecha-\nnism to identify informative and easy-to-answer queries, thus\nreducing the burden on humans. We evaluate MAPLE’s sam-\nple efficiency and preference inference quality across two\nbenchmarks, including a real-world vehicle route planning\nbenchmark using OpenStreetMap data. Our results demon-\nstrate that MAPLE accelerates the learning process and ef-\nfectively improves humans’ ability to answer queries.\nIntroduction\nWith recent advances in artificial intelligence, autonomous\nagents are being increasingly deployed in real-world ap-\nplications to address complex tasks (Zilberstein 2015; Di-\netterich 2017). A prominent method for efficiently align-\ning these agents with human preferences is Active Learning\nfrom Demonstration (Active LfD) (Biyik 2022). Preference-\nbased Active LfD, a variant of LfD, aims to infer a pref-\nerence function from human-generated rankings over a set\nof observed behaviors using a Bayesian active learning ap-\nproach.\nRecent advancements in natural language processing have\ninspired many researchers to leverage language-based ab-\nstraction for learning human preferences (Soni et al. 2022;\nGuan, Sreedharan, and Kambhampati 2022). This approach\noffers a more flexible and interpretable way of learning pref-\nerences compared to conventional methods (Sadigh et al.\n2017; Brown, Goo, and Niekum 2019; Brown et al. 2019).\nMore recent work (Yu et al. 2023; Ma et al. 2023) has fo-\ncused on utilizing large language models (LLMs), such as\nCopyright © 2025, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nChatGPT (Achiam et al. 2023), with prompting-based ap-\nproaches to learn preferences from natural language instruc-\ntions. However, these methods often require significant com-\nputational resources and taxing human supervision, as they\nlack a systematic querying approach.\nTo address these challenges, we introduce a novel frame-\nwork: MAPLE (Model-guidedActive Preference Learning).\nMAPLE begins by utilizing a large language model (LLM)\nto interpret the natural language instructions from humans\nand to estimate an initial distribution over preference func-\ntions. Then, it applies an active learning approach to system-\natically reduce uncertainty on the correct preference func-\ntion. This is achieved through standard Bayesian posterior\nupdates, conditioned on both conventional preference learn-\ning feedback, such as pairwise trajectory rankings, and lin-\nguistic feedback such as clarification or explanations of the\ncause behind the preference. By integrating Bayesian infer-\nence with LLMs, MAPLE achieves higher preference in-\nference accuracy than a general-purpose pre-trained LLM\nwhile offering greater sample efficiency and flexibility than\npurely Bayesian methods.\nTo further ease human effort, MAPLE incorporates a\nlanguage-conditioned in-context (Dong et al. 2024) active\nquery selection mechanism that takes advantage of feedback\non the difficulty of previous queries to choose future queries\nthat are informative and easier to answer. MAPLE repre-\nsents preference functions as a linear combination of ab-\nstract language concepts, providing a modular structure that\nenables the framework to acquire new concepts over time\nand enhance sample efficiency for future instructions. More-\nover, this interpretable structure allows for human auditing\nof the learning process, facilitating human-guided valida-\ntion (Mahmud, Saisubramanian, and Zilberstein 2023) be-\nfore applying the preference function to optimize behavior.\nIn our experiments, we evaluate the efficacy of MAPLE\nin terms of sample efficiency during learning, as well as\nthe quality of the final preference function. We use an envi-\nronment based on the popular Minigrid (Chevalier-Boisvert\net al. 2023) and introduce a new realistic vehicle routing\nbenchmark based on OpenStreetMap (OpenStreetMap Con-\ntributors 2017) data, which includes text descriptions of the\nroad network of different cities in the USA. Our evalua-\ntion shows the effectiveness of MAPLE on preference infer-\nence while improving the human’s ability to answer queries.\nThe Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25)\n27518\nOur contributions are threefold:\n• We propose a Bayesian preference learning framework\nthat leverages task-independent pre-trained LLMs and\nnatural language explanations to reduce uncertainty over\npreference functions.\n• We provide a language-conditioned in-context active\nquery selection approach to reduce human burden.\n• We perform extensive evaluations, including the design\nof a realistic new benchmark that can be used for future\nresearch in this area.\nRelated Work\nLearning from demonstration Most Learning from\nDemonstration (LfD) algorithms learn a reward function us-\ning expert trajectories (Ng and Russell 2000; Abbeel and\nNg 2004; Ziebart et al. 2008). Some of these approaches uti-\nlize a Bayesian framework to learn the reward or preference\nfunction (Ramachandran and Amir 2007; Brown et al. 2020;\nMahmud, Saisubramanian, and Zilberstein 2023), and some\npair it with active learning to reduce the number of human\nqueries (Sadigh et al. 2017; Basu, Singhal, and Dragan 2018;\nBiyik 2022). However, these methods are unable to utilize\nnatural language abstraction during Learning from Demon-\nstrations, whereas our method can use both. In addition, we\nemploy language-conditioned active learning to reduce user\nburden, an approach not previously explored in this context.\nA related method in this area, REVEALE (Mahmud,\nSaisubramanian, and Zilberstein 2023), uses explanations\nto detect misalignment and infer a distribution over pref-\nerences. However, while REVEALE relies on feature\nattribution-based explanation methods and incorporates hu-\nman selection or random sampling for the learning and au-\nditing process, our approach utilizes natural language expla-\nnations and integrates active learning to enhance efficiency\nfor preference learning.\nNatural language in intention communication With the\nadvent of natural language processing, several works have\nfocused on directly communicating abstract concepts to\nagents (Tevet et al. 2022; Guo et al. 2022; Lin et al. 2022;\nWang et al. 2024; Sontakke et al. 2024; Tien et al. 2024;\nLou et al. 2024). The key difference is that this line of work\ndirectly conditions behavior in natural language, whereas\nwe learn a language-abstracted preference function. This ap-\nproach offers several advantages, including increased trans-\nparency, more fine-grained trade-offs between concepts, and\nenhanced transferability. The work most closely related to\nours is (Lin et al. 2022), which infers rewards from language\nbut restricts them to stepwise decision making.\nOther lines of work (Yu et al. 2023; Ma et al. 2023) aim to\nlearn reward functions directly by prompting LLMs. How-\never, these methods often struggle with identifying tempo-\nrally extended abstract behaviors. Furthermore, these ap-\nproaches cannot utilize conventional preference feedback,\nwhereas MAPLE can utilize both. In addition, they either\nlack a systematic way of acquiring human feedback or rely\non data-hungry evolutionary algorithms. In contrast, our ap-\nproach employs more efficient Bayesian active learning.\nAbstraction in reward learning Abstraction in reward\nlearning helps simplify complex objectives, improves gen-\neralization, and facilitates interaction between AI systems\nand humans. Several prior research efforts leverage abstract\nconcepts to learn reward functions (Lyu et al. 2019; Il-\nlanes et al. 2020; Guan et al. 2021; Bobu et al. 2021; Icarte\net al. 2022; Guan, Valmeekam, and Kambhampati 2022;\nSoni et al. 2022; Guan, Sreedharan, and Kambhampati 2022;\nSilver et al. 2022; Zhang et al. 2022; Bucker et al. 2023; Cui\net al. 2023). Two methods closely related to our work are\nPRESCA (Soni et al. 2022) and RBA (Guan, Valmeekam,\nand Kambhampati 2022). PRESCA learns state-based ab-\nstract concepts to be avoided, while RBA learns tempo-\nrally extended concepts with two variants: global (eliciting\npreference weights directly from humans) and local (tuning\nweights using binary search). Our approach also leverages\ntemporally extended concepts but learns preference func-\ntions from natural language feedback using active learning.\nUnlike RBA, which relies on direct preference weights from\nhumans or binary search, our method uses LLM-guided ac-\ntive learning for more expressive and informative preference\nelicitation while reducing human effort.\nSome works use offline behavior datasets or demonstra-\ntions to learn various skills (Lee and Popovi ´c 2010; Wang\net al. 2017; Zhou and Dragan 2018; Peng et al. 2018; Luo\net al. 2020; Chebotar et al. 2021; Peng et al. 2021), which\ncomplement our approach. While MAPLE may also utilize\nsuch datasets in pre-training, the main purpose of MAPLE\nis to encode human preferences using natural language.\nActive learning Previous works have explored different\nacquisition functions for active learning, typically focus-\ning on selecting queries that maximize certain uncertainty\nquantization metrics. These metrics include predictive en-\ntropy (Gal, Islam, and Ghahramani 2017), uncertainty vol-\nume reduction (Sadigh et al. 2017), uncertainty weighted\ndensity score (Wray and Zilberstein 2016), mutual informa-\ntion maximization (Biyik et al. 2019), and maximizing the\nvariation ratio (Gal, Islam, and Ghahramani 2017). Our ap-\nproach complements these methods by integrating language-\nconditioned query selection to reduce the user burden. Al-\nthough any of these methods can be paired with MAPLE,\nwe opt for the variation ratio because of its ease of calcula-\ntion and high effectiveness.\nLLMs and Probabilistic Reasoning Many previous and\ncontemporary research has examined the integration of\nprobabilistic reasoning with large language models (LLMs)\nfor active querying in classification and preference learning\ntasks (Rao and Daum´e III 2018; Yu et al. 2020; Piriyakulkij,\nKuleshov, and Ellis 2023; Grand et al. 2024; Andukuri et al.\n2024). Among these, the work of Piriyakulkij, Kuleshov,\nand Ellis (2023) is particularly relevant, as it focuses on ac-\ntively learning a binary preference function for single-step\ndecision-making problems by combining LLMs and prob-\nabilistic reasoning. However, there are two key differences\nfrom our approach: their method does not leverage linguis-\ntic explanations for preference learning (relying solely on\nyes/no feedback) and does not utilize in-context learning for\nquery selection to reduce user burden.\n27519\n“I prefer routes that are safe, have minimal \nturns, and small changes in elevation. I have no \npreference for drive duration.”\nScenerio 1: A   B. Explanation: “A looks safer and has \nless curves. I am indifferent about the drive duration”\nScenerio 2: “I’m unable to rank the trajectories as they \nare very similar in terms of safety.”\nMAPLE\nBA\nDo you prefer \nroute A or B?\nPrior Posterior\nAligned \nRoute \nPlanner\nFigure 1: Application of MAPLE to the Natural Language Vehicle Routing Task.\nBackground\nMarkov decision process (MDP) A Markov Decision\nProcess (MDP) M is represented by the tuple M =\n(S, A, T, S0, R, γ), where S is the set of states, A is the set\nof actions, T : S × A × S → [0, 1] is the transition func-\ntion, S0 is the initial state distribution, and γ ∈ [0, 1) is the\ndiscount factor. A history ht is a sequence of states up to\ntime t, (s0, . . . , st)1. The reward function R : H × A →\n[−Rmax, Rmax] maps histories and actions to rewards. For\nsome problems, a goal function G : H → [0, 1] is provided\nthat maps histories to goal achievements. In such problems,\nthe reward function is typically R : H × A → [−Rmax, 0]\nand R(ht ∪ sg, a) = 0 for G(ht ∪ sg) = 1. A policy\nπ : H × A → [0, 1] is a mapping from histories to a dis-\ntribution over actions. The policyπ induces a value function\nV π : S → R, which represents the expected cumulative\nreturn V π(s) that the agent can achieve from state s when\nfollowing policy π. An optimal policy π∗ maximizes the ex-\npected cumulative return V ∗(s) from any state s, particu-\nlarly from the initial state s0.\nBayesian Preference Learning A preference function ω\nmaps a trajectory τ to a real number representing how well\nthe trajectory aligns with a human’s objectives. The goal\nof preference learning is to infer this function from human\nfeedback, enabling it to be directly used as the reward func-\ntion in a Markov Decision Process (MDP) or to construct a\nreward function (discussed further in later sections).\nOne common approach is to learn this function from a\ndataset of pairwise preferences, represented as\nD = {(τ1\n1 ≻ τ2\n1 ), (τ1\n2 ≻ τ2\n2 ), . . . ,(τ1\nn ≻ τ2\nn)}.\nHere, τ1\ni and τ2\ni are two trajectories, and τ1\ni ≻ τ2\ni indicates\nthat τ1\ni is preferred over τ2\ni .\nUsing a Bayesian framework, as outlined in Ramachan-\ndran and Amir (2007), preference learning defines a proba-\nbility distribution over preference functions conditioned on\nthe dataset D via Bayes’ rule:\nP(ω | D) ∝ P(D |ω)P(ω).\nThe likelihood function P(D |ω) is modeled in various\nways. Following BREX (Brown et al. 2020), we use the\n1A history is can also be a sequence of observations.\nBradley–Terry model (Bradley and Terry 1952), which de-\nfines the likelihood as:\nP(D |ω) =\nY\n(τ1\ni ≻τ2\ni )∈D\neβω(τ1\ni )\neβω(τ1\ni ) + eβω(τ2\ni ) . (1)\nHere, β ∈ [0, ∞) is the inverse-temperature parameter.\nVariation Ratio Given a conditional probability distribu-\ntion P(· |X) with support {yi}k\ni=0, the variation ratio of an\ninput X is defined as follows:\nVariation Ratio(X) = 1 − arg max\nyi\nP(yi | X)\nProblem Formulation\nMAPLE We define a MAPLE problem instance as the tu-\nple (M|R, H, L, C,Ω, Dτ ), where:\n• M|R is an MDP with an undefined or partially defined 2\nreward function R.\n• H is the human interaction function that acts as the inter-\nface between the human and MAPLE. Humans provide\ntheir feedback, preferences, and explanations in response\nto natural language queries posed by MAPLE.\n• L is the LLM interaction function that generates natural\nlanguage LLM queries and returns structured output.\n• C is an expanding set of concepts in natural language\n{c1, c2, . . . , cn}. We also use C(·) to refer to a mapping\nmodel that takes a trajectory embeddingϕ(τ) and a natu-\nral language concept embedding ψ(ci) and maps them\nto a numeric value indicating the degree to which the\ntrajectory τ satisfies the concept ci. For non-Markovian\nconcepts, ϕ(·) may be a sequence model such as a\ntransformer. For Markovian concepts, we can define\nC(ϕ(τ), ψ(ci)) = P\ns ∈ τ C(ϕ(s), ψ(ci)), where ϕ(s)\nis the state embedding.\n• Ω is the space of all preference functions. In MAPLE,\npreference functions ω over a trajectory τ are modeled as\na linear combination of the concepts and their associated\nweights:\nω(τ) =\nX\nci ∈ C\nωci · C(ϕ(τ), ψ(ci)) (2)\n2A partially defined reward function may be a sparse reward\nfunction that encodes signals for goals but not path preferences.\n27520\n• Dτ is a dataset of unlabeled trajectories{τ1, τ2, . . . , τm}.\nThe objective of MAPLE is to model the repeated interac-\ntion between a human and an agent, where the human com-\nmunicates their task objective OH\nT in natural language, and\nthe agent is responsible for completing the task in alignment\nwith that objective. MAPLE accomplishes this by actively\nlearning a symbolic preference function ω using large lan-\nguage models (LLMs), enabling the agent to optimize its\nbehavior according to this function to ensure that its actions\nalign with human preferences.\nMotivating example Consider an intelligent route plan-\nning system that takes a source, a destination, and user pref-\nerences about the route in natural language, as illustrated in\nFigure 1. Datasets for several preference-defining concepts\nsuch as speed, safety, battery friendliness, smoothness, au-\ntopilot friendliness, and scenic views can be easily obtained\nand used to pre-train the concept mapping function C(·).\nThe goal of MAPLE is to take natural language instruc-\ntions from a human and map them to a preference function\nω interactively so that a search algorithm can optimize it\nto find the preferred route. MAPLE incorporates preference\nfeedback on top of natural language feedback to address is-\nsues such as hallucination (Huang et al. 2023) and calibra-\ntion required to directly apply a pre-trained LLM to unseen\ntasks. Additionally, MAPLE allows the human to skip diffi-\ncult queries and applies in-context learning to identify which\nquery to present, making the system more human-friendly.\nFurthermore, the preference function inference process in\nMAPLE is fully interpretable, enabling a human to audit the\nprocess thoroughly and provide the necessary feedback for\nimprovement. Finally, the interaction with the human is re-\npeated, allowing MAPLE to acquire and memorize new con-\ncepts and become more efficient for future tasks.\nDetailed Description of the Proposed Method\nA key innovation of MAPLE is the integration of con-\nventional feedback from the preference learning literature\nwith more expressive linguistic feedback, formally cap-\ntured within a Bayesian framework introduced in RE-\nVEALE (Mahmud, Saisubramanian, and Zilberstein 2023):\nP(ω | Fb, Fl) ∝ P(Fb | ω)P(Fl | ω)P(ω) (3)\nAbove, Fb represents the set of feedback observed in con-\nventional preference learning algorithms, specifically in the\ncontext of this paper pairwise trajectory ranking.3 Fl denotes\nthe set of linguistic feedback. We can rewrite the equation as:\nP(ω | Fb, Fl) ∝ P(Fb | ω)|\n{z }\nBradley-Terry Model\nP(ω | Fl)| {z }\nLLM\nP(Fl)| {z }\nUniform\n(4)\n∝ P(Fb | ω)| {z }\nBradley-Terry Model\nP(ω | Fl)| {z }\nLLM\n(5)\nHere, the likelihood of Fb given ω is defined using the\nBradley-Terry Model. The likelihood of ω given Fl is es-\ntimated using an LLM. Beyond incorporating linguistic\n3MAPLE can handle any conventional feedback for which\nP(Fb | ω) is defined.\nAlgorithm 1: MAPLE\nRequire: Human instruction OH\nT , Acquisition function Af ,\nNumber of LLM query K\n1: Fb, Fq ← ∅,∅\n2: Fl ← {OH\nT }\n3: ΩT ← {ωi}n\ni=0 ∼ P(ω | Fl)\n4: while condition not met do\n5: Q ← {(τi, τj) : τi, τj ∈ Dτ ∧ i ̸= j ∧ (τi, τj) ̸∈ Fb}\n6: q ← Query Selection(Af , Q, Fq, ΩT , L, K)\n7: (fb, fl, fq) ← H(q)\n8: Fb, Fl, Fq ← Fb ∪ {fb}, Fl ∪ {fl}, Fq ∪ {fq}\n9: ΩT ← {ωi}n\ni=0 ∼ P(ω | Fb, Fl)\n10: end while\n11: return ΩT\nfeedback via LLMs, MAPLE advances conventional active\nlearning methods. Conventional active learning typically fo-\ncuses on selecting queries that reduce the maximum uncer-\ntainty of the posterior but lacks a flexible mechanism to ac-\ncount for human capability in responding to certain types of\nqueries. MAPLE’s Oracle-guided active query selection en-\nhances any conventional acquisition function by leveraging\nlinguistic feedback to alleviate the human burden associated\nwith difficult queries. In the rest of this section, we provide\nmore details on MAPLE, particularly Algorithms 1 and 2.\nInitialization\nMAPLE starts by taking natural language instruction about\ntask preference OH\nT and initializes the pairwise preference\nfeedback set Fb, linguistic feedback Fl, and feedback on\nquery difficulty Fq (lines 1-2, Algorithm 1). After that, the\ninitial set of weights is sampled using the LLM from the\ndistribution P(ω | Fl) as Fb is still empty (line 3, Algo-\nrithm 1). To sample ω from P(ω | Fl), we explore two sam-\npling strategies described below.\nPreference weight sampling from LLM We directly\nprompt the LLM L to provide linear weights ω over the\nabstract concepts. Specifically, we provide L with a prompt\ncontaining the task description T , a list of known concepts\nC, human preference OH\nT , and examples of pairs of in-\nstruction weights, along with additional answer generation\ninstructions G. The LLM processes this prompt and returns\nan answer OL\nωi:\nOL\nωi ← L(prompt(T , C, DI, G,OH\nT ))\nWe can take advantage of the text generation temperature\nto collect a diverse set of samples. We define the set of all the\ngenerated weights as OL\nω. Then, P(ωj | Fl) can be modeled\nfor any arbitrary ωj as follows:\nP(ωj | Fl) = exp\n\u0000\n−βlEωi∈OL\nω\n[Distance(ωi, ωj)]\n\u0001\n(6)\nIn this case, Euclidean or Cosine distance can be applied.\nDistribution weight sampling using LLM The second\napproach we explore is distribution modeling using an LLM.\nHere, we use similar prompts as in the previous approach;\nhowever, we instruct the LLM to generate parameters for\n27521\nP(ω |Fl). For example, for the weight of each conceptωci ∈\nω, we prompt L to generate a range ωrange\nci = [ ωmin\nci , ωmax\nci ].\nThen we can define P(ω | Fl) as follows:\nP(ω | Fl) =\n(\n1, if ωci ∈ ωrange\nci , ∀ωci ∈ ω\n0, otherwise. (7)\nWe can similarly model this for other forms of distribu-\ntion, such as the Gaussian distribution. Once the initializa-\ntion process is complete, MAPLE iteratively reduces its un-\ncertainty using human feedback.\nLLM-Guided Active Preference Learning\nAfter initialization, MAPLE iteratively follows three steps:\n1) query selection, 2) human feedback collection, and 3)\npreference posterior update, discussed below.\nOracle-guided active query selection (OAQS) At the be-\nginning of each iteration, MAPLE selects a queryq (a pair of\ntrajectories) (lines 5-6, Algorithm 1) fromDτ that would re-\nduce uncertainty the most while mitigating query difficulty\nbased on human feedback. The query selection process is de-\nscribed in Algorithm 2, which starts by sorting all the queries\nbased on an acquisition function Af . In this paper, we use\nthe variation ratio for its flexibility and high efficacy. In par-\nticular, for trajectory ranking queries, the score for(τi, τj) is\ncalculated as:\nEω∼ΩT [1 − max(P(τi ≻ τj | ω), P(τj ≻ τi | ω))]\nNote that other acquisition functions can also be used. Once\nsorted, OAQS iterates over the topK queries and selects the\nfirst query that the oracle (in our case, an LLM) evaluates\nto be answerable by the human (lines 2-11). Finally, Algo-\nrithm 2 returns the least difficult query q among the top K\nqueries selected by Af . We now analyze the performance of\nOAQS based on the characterization of the oracle.\nDefinition 1 Let Q denote the set of all possible queries,\nand QA ⊆ Q represent the subset of queries answerable by\nH. The absolute query success rate (AQSR)is defined as\nthe probability that a randomly selected query q belongs to\nQA, i.e., P(q ∈ QA).\nDefinition 2 The query success rate (QSR)of a query se-\nlection strategy is defined as the probability that a query q,\nselected by the strategy, belongs to QA, i.e., P(q ∈ QA |\nstrategy).\nProposition 1 The QSR of a random query selection strat-\negy: P(q ∈ QA | random) = AQSR\nProposition 2 Assuming the independence of AQSR from\nacquisition function ranking, the QSR of a top-query selec-\ntion strategy, which always selects the highest-rated query\nby Af , P(q ∈ QA | top) = AQSR.\nProposition 3 The QSR of the OAQS strategy is given by\nAQSR · Y1 · 1 − [AQSR · (1 − Y0 − Y1) + Y0]K\n1 − [AQSR · (1 − Y0 − Y1) + Y0] ,\nwhere Y0 = P(L(Fq, q /∈ QA) = False)\nand Y1 = P(L(Fq, q∈ QA) = True).\nHere, we assume the independence of AQSR, Y0, and Y1\nfrom acquisition function ranking.\nAlgorithm 2: Oracle-Guided Query Selection\nRequire: Acquisition function Af , List of queries Q,\nQuery preference feedback Fq, Weights from current\nposterior ΩT , Oracle L, Number of Oracle queries K\n1: Qsort ← sort(Q|Af , ΩT )\n2: Qtop ← Qsort[0 : K]\n3: for q ∈ Qtop do\n4: sq ← L(prompt(Fq, q))\n5: if sq is True then\n6: return q\n7: end if\n8: end for\n9: return Qsort[0]\nCorollary 1 Based on Proposition 3, the OAQS will have a\nhigher QSR than the random query selection strategy and\nthe top-query selection strategy iff,Y0 +Y1 > 1 as K → ∞.\nDefinition 3 The optimal query success rate (OQSR)of a\nstrategy is defined as the probability that the strategy returns\nthe query q∗ with the highest value according to an acquisi-\ntion function Af , among all answerable queries, i.e.,\nP(q∗ = arg max\nq∈Q\nAf (q)I(q ∈ QA)),\nwhere q∗ is the query returned by the strategy.\nProposition 4 Under the assumption of Proposition 2, the\nOQSR of a random query selection strategyequals 1/|Q|.\nProposition 5 Under the assumption of Proposition 2, the\nOQSR of a top-query selection strategyequals the AQSR.\nProposition 6 Under the assumption of Proposition 3, the\nOQSR of the OAQS strategy is given by\nOQSR = AQSR · Y1 · 1 − [(1 − AQSR)Y0]K\n1 − (1 − AQSR)Y0\n.\nCorollary 2 Based on Proposition 6, the OAQS strategy\nwill have a higher OQSR than the top-query selection strat-\negy if (1 − AQSR)Y0 + Y1 > 1 as K → ∞, and\nhigher OQSR than the random query selection strategy if\nAQSR · Y1 > 1−(1−AQSR)Y0\n|Q| as K → ∞.\nHuman feedback collection MAPLE queries the human\nH using the query q returned by Algorithm 2 to collect feed-\nback. For each query q, MAPLE provides a pair of trajecto-\nries, and H returns an answer OH\nτ = (fb, fl, fq), where fb\nis binary feedback, fl is an optional natural language expla-\nnation associated with that feedback—possibly empty if the\nhuman does not provide an explanation—and fq is an op-\ntional natural language feedback about the difficulty of the\nquery. Each piece of feedback is then added to the corre-\nsponding feedback set (lines 7-8, Algorithm 1).\nLLM-guided posterior update Once feedback is added\nto the set, we update our current weight sample ΩT by sam-\npling P(ω | Fb, Fl) ∝ P(Fb | ω)P(ω | Fl) using MCMC\nsampling, where P(ω | Fb) is given by Equation 1, and\nP(ω | Fl) is given by Equations 6 and 7.\n27522\nSource: 740 N Pleasant St, Amherst, MA 01003\nDestination: 440 Russell St, Hadley, MA 01035\nHuman Instruction: I want scenic roads, driving friendly route for a new driver\nCalculate Route\nFigure 2: OpenStreetMap Routing\nStopping criteria While MAPLE can employ various\nstopping criteria for active query generation, in this paper,\nwe use a fixed budget approach, where MAPLE operates\nwithin a predefined maximum query limit.\nHandling unknown concepts It should be noted that hu-\nmans may provide instructions OH\nT that cannot be ade-\nquately captured by the concepts available in the concept\nmaps. Although this case is beyond the scope of this paper,\nseveral remedies exist in the literature to address this issue.\nLLMs can be prompted to add new concepts when generat-\ning weights. By leveraging the generalization capability of\nC(·), we can attempt to apply these new concepts directly.\nIf the new concept is significantly different from those inC,\nfew-shot learning techniques can be employed. In particu-\nlar, during interactions, if a new concept is important, we\ncan use nonparametric few-shot learning from human feed-\nback, such as nearest-neighbor search, to improve concept\nmapping (Tian et al. 2024). Finally, if a new concept arises\nrepeatedly, it can be added to the concept map by retrain-\ning C with data collected from multiple interactions through\nfew-shot learning, as considered in (Soni et al. 2022).\nPolicy Optimization\nThe method for utilizing the weights generated by MAPLE\nto optimize the policy varies based on the trajectory encod-\ning and the chosen policy solver algorithm. For example,\nfor Markovian preferences, the weights can be used directly\nwith an MDP solver. In non-Markovian settings, the weights\ncan be used to rank trajectories and directly align the pol-\nicy with algorithms such as DPO (Rafailov et al. 2024),\nor train a dense reward function (Guan, Valmeekam, and\nKambhampati 2022) using preference learning algorithms\nsuch as TREX (Brown et al. 2019), and then use that reward\nfunction with reinforcement learning algorithms.\nExperiments\nIn this section, we describe a comprehensive evaluation of\nMAPLE within the two environments detailed below. It is\nimportant to note that none of the models used in our ex-\nperiments were fine-tuned for the tasks. We ran the local\nFigure 3: HomeGrid\nlanguage model, specifically Mistral-7B-instruct-v0.3 (4-bit\nquantization), on a computer equipped with 64GB RAM and\nan Nvidia RTX 4090 24GB graphics card. For larger mod-\nels, we relied on public API infrastructure. Due to space\nconstraints, we present results using preference weight sam-\npling as it outperformed distribution weight sampling in\nboth benchmarks.\nOpenStreetMap Routing We use OpenStreetMap to gen-\nerate routing graphs for different states in the United States.\nThe environment (Fig. 2) includes a concept mapping func-\ntion capable of using ten different concepts: 1) Time, 2)\nSpeed, 3) Safety, 4) Scenic, 5) Battery Friendly, 6) Gas Sta-\ntion Nearby, 7) Charging Station Nearby, 8) Human Driv-\ning Friendly, 9) Battery ReGen Friendly, and 10) Autopilot\nFriendly. The goal is to find a route between a given source\nand destination that aligns with user preferences. To gen-\nerate Dτ , we used 200 pairs of random sources and desti-\nnations with weights randomly sampled from Ω. For mod-\neling human interaction, we utilized two different datasets,\neach containing 50 human interaction templates. Each inter-\naction includes a ground truth preference weight and a nat-\nural language description generated based on those weights.\nThe first dataset, called “Clear,” provides clear, knowledge-\nable instructions. The second dataset, called “Natural,” ob-\nfuscates the “Clear” dataset with more natural language typi-\ncal of everyday conversation and contextual information, for\nexample:\nClear: “I prefer routes that are safe and scenic, with a mod-\nerate focus on speed and low importance on time.”\nNatural: “I’m planning a weekend drive to enjoy the coun-\ntryside, so I’m not in a hurry. I want the route to be as safe\nas possible because I’ll be driving with my family. It would\nbe great if the drive is scenic too, so we could take in the\nbeautiful views along the way. Speed isn’t a top concern,\nand we’re really just out to enjoy the journey rather than\nworry about how long it takes to get there.”\nFor modeling fl, the human clarifies the type of car (gas,\nautonomous, or electric) with a probability of 0.2 per feed-\nback. For fq, the human cannot answer when the two con-\ncepts with the highest weights (based on the ground truth) in\nboth trajectories are closer than a predefined threshold.\n27523\n0 5 10 15 20 25 30\nNumber of Queries\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90T est Accuracy\nB-REX\nMAPLE (Clear)\nMAPLE (Natural)\nFigure 4: Test accuracy (OSM Routing)\n0 5 10 15 20 25 30\nNumber of Queries\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00T est Accuracy\nB-REX\nMAPLE (Clear)\nMAPLE (Natural)\nFigure 5: Test accuracy (HomeGrid)\nHomeGrid The HomGrid environment (Fig. 3) is a sim-\nplified Minigrid (Chevalier-Boisvert et al. 2023) setting de-\nsigned to simulate a robot performing household tasks (Lin\net al. 2023). It features a discrete, finite action space and a\npartially observable language-based state space for a 3 × 3\ngrid, detailing the objects and floors in each grid square,\nwithin a truncated 12 × 14 grid. The abstract concepts in-\nclude: 1) avoiding objects such as tables and chairs, 2) avoid-\ning walls, 3) avoiding placing objects such as bottles and\nplates on the floor, 4) avoiding placing objects on the stove,\nand 5) avoiding placing objects on the left chairs. A total of\n60 trajectories were manually generated to update the poste-\nrior distribution of the weights ω for each method. For mod-\neling fl, the human highlights the concept that was the most\ninfluential for their preference. The modeling of fq follows\na similar approach to that used in OSM Routing.\nExperimental Results\nWe use three key metrics for evaluation: 1) the cosine dis-\ntance between inferred preference weights (MAP of the dis-\n0 5 10 15 20 25 30\nNumber of Queries\n0.1\n0.2\n0.3\n0.4\n0.5Average Cosine Distance\nB-REX\nMAPLE (Clear)\nMAPLE (Natural)\nFigure 6: Cosine distance (OSM Routing)\n0 5 10 15 20 25 30\nNumber of Queries\n0.1\n0.2\n0.3\n0.4Average Cosine Distance\nB-REX\nMAPLE (Clear)\nMAPLE (Natural)\nFigure 7: Cosine distance (HomeGrid)\ntribution) and ground truth preference weights; 2) preference\nprediction accuracy, which evaluates the model’s ability to\ngeneralize and accurately predict human preferences from\nan unseen set of trajectories; and 3)The policy cost differ-\nence compares the true cost of two policies: one trained\nusing the ground truth preference function and the other\ntrained using the learned preference function, with both eval-\nuated against the ground truth preference function.\nImpact of linguistic feedback Figures 4, 6, and Table\n1, present the results of the OSM routing domain. In this\nexperiment, we did not apply OAQS; instead, we selected\nqueries randomly from the dataset to isolate the impact of\nlanguage. Several noteworthy insights emerge from the re-\nsults. First, observe that MAPLE, utilizing only linguistic\nfeedback, outperforms B-REX on both the natural and clear\ndatasets when provided with at least 10 instances of con-\nventional feedback, highlighting the effectiveness of com-\nbining complex language feedback with conventional feed-\nback. Additionally, as the feedback increases, the accuracy\nof B-REX begins to approach that of MAPLE. This sug-\n27524\nOSM Routing\nModel Test Accuracy Expected Cost ∆\nB-REX 0.79 ± 0.04 0.66 ± 0.10\nMistral-7B-Ins. 0.85 ± 0.02 0.34 ± 0.07\nGPT-4o 0.88 ± 0.01 0.22 ± 0.05\nGPT-4o-mini 0.87 ± 0.02 0.29 ± 0.07\nGemini-1.5-Pro 0.86 ± 0.01 0.26 ± 0.05\nMistral-Large-2 0.85 ± 0.02 0.27 ± 0.06\nTable 1: Test accuracy and cost for different models with\n5-feedback (OSM Routing, Natural)\ngests that MAPLE is particularly advantageous when feed-\nback is limited, such as in online settings where the agent\nmust quickly infer rewards. Examining the cosine distance\noffers further insight. Language alone appears to be almost\nsufficient to align the reward angle, as the cosine distance re-\nmains static despite the increasing number of queries. This\nsuggests that preference feedback is more effective in cali-\nbrating the magnitude of the preference vector than its direc-\ntion. In contrast, while B-REX achieves good accuracy with\nlarge amounts of feedback, it seems to exhibit significant\nmisalignment, which could suggest overfitting and potential\nfailure in out-of-distribution scenarios. Lastly, we evaluated\nexisting publicly available models and found that both GPT-\n4o and GPT-4o-mini outperformed other models. However,\nthe smaller local model (Mistral-7B Instruct) proved to be\ncompetitive, so we used it to generate all the results shown\nin Figures 4, 5, 6, and 7.\nFigures 5, 7, and Table 2 show the results of the Home-\nGrid experiments. In this environment, we observe that natu-\nral instructions do affect performance, but MAPLE still sig-\nnificantly outperforms B-REX in both datasets. Notably, the\nMistral-Large-2 models surpassed B-REX by a wide mar-\ngin, achieving nearly one-third of the cost difference. Sur-\nprisingly, GPT-4-mini performed poorly, with a cost differ-\nence that was worse than that of B-REX. This is due to the\n5 10 15 20 25 30\nNumber of Queries\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90T est Accuracy\nActive B-REX\nMAPLE w\\o OAQS\nMAPLE with OAQS\nFigure 8: Efficacy of OAQS (OSM Routing, Natural)\nHomeGrid\nModel Test Accuracy Expected Cost ∆\nB-REX 0.82 ± 0.04 1.40 ± 0.24\nMistral-7B-Ins. 0.92 ± 0.02 0.67 ± 0.13\nGPT-4o 0.87 ± 0.03 0.89 ± 0.20\nGPT-4o-mini 0.83 ± 0.04 3.90 ± 0.66\nGemini-1.5-Pro 0.87 ± 0.04 1.00 ± 0.18\nMistral-Large-2 0.85 ± 0.04 0.55 ± 0.14\nTable 2: Test accuracy and cost for different models with\n5-feedback (HomeGrid, Natural)\ninference of highly misaligned preference weights for cer-\ntain instructions. In this environment, we also see that most\nof the angle alignment was done using language feedback,\nand B-REX remains highly misaligned even after 30 rounds\nof feedback.\nImpact of OAQS The results of the OAQS using an LLM\nas an oracle are shown in Figures 8 and 9. In this experi-\nment, we use K=10. In the routing environment, the Active\nQuery Success Rate (AQSR) is approximately 0.64, while\nin the HomeGrid environment, it is 0.46. We first evalu-\nated the ability of various models for the selection of in-\ncontext queries (Table 3) using a dataset of 500 queries. The\nMistral-7B model, used in the previous experiment, failed\nto meet the condition Y0 + Y1 > 1 in both environments.\nThe Gemini-1.5-Pro model showed the best overall perfor-\nmance among the publicly available models and was used to\ngenerate Figures 8 and 9.\nFigures 8 and 9 compare the test accuracy of Active B-\nREX with MAPLE, with and without OAQS. In both envi-\nronments, MAPLE with OAQS achieved the highest perfor-\nmance, with a significant margin in the OSM routing envi-\nronment. We also calculated the Query Success Rate (QSR)\nfor all three algorithms: 0.43 for Active B-REX, 0.43 for\nMAPLE without OAQS, and 0.58 for MAPLE with OAQS\nin the routing domain. The QSR was lower than the AQSR\n5 10 15 20 25 30\nNumber of Queries\n0.75\n0.80\n0.85\n0.90T est Accuracy\nActive B-REX\nMAPLE w\\o OAQS\nMAPLE with OAQS\nFigure 9: Efficacy of OAQS (HomeGrid, Natural)\n27525\nOSM Routing HomeGrid\nModel Y0 Y1 Y0 Y1\nMistral-7B-Ins. 0.30 0.65 0.17 0.77\nGPT-4o 0.84 0.43 0.53 0.58\nGPT-4o-mini 0.56 0.48 0.36 0.75\nGemini-1.5-Pro 0.62 0.72 0.50 0.87\nMistral-Large-2 0.50 0.78 0.49 0.84\nTable 3: Performance of different models for in-context\nquery selection (See Propositions 3 and 6)\nfor the top-query selection strategy due to a violation of the\nindependence assumption, suggesting that the variation ratio\nis more likely to select more challenging queries. We refer\nto this experimental metric as the Effective Query Success\nRate (EQSR). Based on Proposition 3, the QSR for MAPLE\nwith OAQS should be 0.77, but it was observed to be lower\nfor the same reason. Replacing AQSR with EQSR in Propo-\nsition 3 gives us a value of 0.59, which closely matches the\nexperimental value. Therefore, we conclude that EQSR is\na more practical metric to estimate the success of a model\nbased on Y0 and Y1. This phenomenon is also observed in\nHomeGrid. Finally, in the HomeGrid environment, the over-\nall EQSR was low, around 0.2; therefore, even with OAQS,\nwe saw a modest increase of 1-3 feedback signals after 30\nqueries, which was not enough to create a large margin, and\ntherefore we see only a modest difference between MAPLE\nwith and without OAQS.\nConclusion\nWe introduce MAPLE, a novel framework for active pref-\nerence learning guided by large language models (LLMs).\nBy combining the capabilities of LLMs with language-\nconditioned query selection mechanisms, MAPLE system-\natically reduces uncertainty in inferred user preferences\nwhile simultaneously minimizing the burden on human\nfeedback. Through experiments in two domains, includ-\ning a real-world vehicle route planning task using Open-\nStreetMap data, we demonstrated the superior sample effi-\nciency and high-quality preference inference of the frame-\nwork. These results highlight the potential of integrating ac-\ntive learning with LLM-guided mechanisms in preference-\ndriven decision-making systems.\nOur study reveals key areas for further exploration. Al-\nthough MAPLE effectively identifies and prioritizes infor-\nmative queries, its performance can be constrained in set-\ntings where user feedback is ambiguous or noisy. In future\nwork, we plan to extend MAPLE to more complex environ-\nments and tasks, explore different types of linguistic feed-\nback, and conduct user studies to evaluate its usability and\neffectiveness. We are also interested in leveraging multi-\nmodal feedback (e.g., combining textual input with visual\nor haptic cues) to improve alignment in scenarios where tex-\ntual feedback alone is insufficient.\nAcknowledgments\nThis research was supported in part by the U.S. Army DEV-\nCOM Analysis Center (DAC) contract W911QX23D0009,\nand by NSF grants 2321786, 2326054, and 2416459.\nReferences\nAbbeel, P.; and Ng, A. Y . 2004. Apprenticeship learning via\ninverse reinforcement learning. In 21st International Con-\nference on Machine learning.\nAchiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.;\nAleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.;\nAnadkat, S.; et al. 2023. GPT-4 technical report. arXiv\npreprint arXiv:2303.08774.\nAndukuri, C.; Fr¨anken, J.-P.; Gerstenberg, T.; and Goodman,\nN. 2024. STaR-GATE: Teaching Language Models to Ask\nClarifying Questions. In 1st Conference on Language Mod-\neling.\nBasu, C.; Singhal, M.; and Dragan, A. D. 2018. Learn-\ning from richer human guidance: Augmenting comparison-\nbased learning with feature queries. In 13th International\nConference on Human-Robot Interaction.\nBiyik, E. 2022. Learning preferences for interactive auton-\nomy. Ph.D. thesis, Stanford University.\nBiyik, E.; Palan, M.; Landolfi, N. C.; Losey, D. P.; and\nSadigh, D. 2019. Asking easy questions: A user-friendly ap-\nproach to active reward learning. In 3rd Annual Conference\non Robot Learning.\nBobu, A.; Paxton, C.; Yang, W.; Sundaralingam, B.; Chao,\nY .-W.; Cakmak, M.; and Fox, D. 2021. Learning percep-\ntual concepts by bootstrapping from human queries. arXiv\npreprint arXiv:2111.05251.\nBradley, R. A.; and Terry, M. E. 1952. Rank Analysis of\nIncomplete Block Designs: I. The Method of Paired Com-\nparisons. Biometrika.\nBrown, D. S.; Goo, W.; and Niekum, S. 2019. Better-than-\ndemonstrator imitation learning via automatically-ranked\ndemonstrations. In 3rd Annual Conference on Robot Learn-\ning.\nBrown, D. S.; Goo, W.; Prabhat, N.; and Niekum, S. 2019.\nExtrapolating beyond suboptimal demonstrations via inverse\nreinforcement learning from observations. In 36th Interna-\ntional Conference on Machine Learning.\nBrown, D. S.; Niekum, S.; Coleman, R.; and Srinivasan, R.\n2020. Safe imitation learning via fast Bayesian reward in-\nference from preferences. In 37th International Conference\non Machine Learning.\nBucker, A.; Figueredo, L. F. C.; Haddadin, S.; Kapoor, A.;\nMa, S.; Vemprala, S.; and Bonatti, R. 2023. LATTE: LAn-\nguage Trajectory TransformEr. In IEEE International Con-\nference on Robotics and Automation.\nChebotar, Y .; Hausman, K.; Lu, Y .; Xiao, T.; Kalashnikov,\nD.; Varley, J.; Irpan, A.; Eysenbach, B.; Julian, R.; Finn,\nC.; et al. 2021. Actionable models: Unsupervised offline\nreinforcement learning of robotic skills. arXiv preprint\narXiv:2104.07749.\n27526\nChevalier-Boisvert, M.; Dai, B.; Towers, M.; Perez-Vicente,\nR.; Willems, L.; Lahlou, S.; Pal, S.; Castro, P. S.; and Terry,\nJ. 2023. Minigrid & Miniworld: Modular & customizable re-\ninforcement learning environments for goal-oriented tasks.\nIn Advances in Neural Information Processing Systems.\nCui, Y .; Karamcheti, S.; Palleti, R.; Shivakumar, N.; Liang,\nP.; and Sadigh, D. 2023. “No, to the Right” – Online lan-\nguage corrections for robotic manipulation via shared au-\ntonomy. arXiv preprint arXiv:2301.02555.\nDietterich, T. G. 2017. Steps toward robust artificial intelli-\ngence. AI Magazine.\nDong, Q.; Li, L.; Dai, D.; Zheng, C.; Ma, J.; Li, R.; Xia,\nH.; Xu, J.; Wu, Z.; Chang, B.; Sun, X.; Li, L.; and Sui, Z.\n2024. A Survey on In-context Learning. In Conference on\nEmpirical Methods in Natural Language Processing.\nGal, Y .; Islam, R.; and Ghahramani, Z. 2017. Deep Bayesian\nactive learning with image data. InInternational Conference\non Machine Learning.\nGrand, G.; Pepe, V .; Andreas, J.; and Tenenbaum, J. B. 2024.\nA Llama Sunk My Battleship! Asking Rational Questions\nwith LLMs via Bayesian Inference. In The First Workshop\non System-2 Reasoning at Scale, NeurIPS 2024.\nGuan, L.; Sreedharan, S.; and Kambhampati, S. 2022.\nLeveraging approximate symbolic models for reinforce-\nment learning via skill diversity. arXiv preprint\narXiv:2202.02886.\nGuan, L.; Valmeekam, K.; and Kambhampati, S. 2022. Rela-\ntive behavioral attributes: Filling the gap between symbolic\ngoal specification and reward learning from human prefer-\nences. arXiv preprint arXiv:2210.15906.\nGuan, L.; Verma, M.; Guo, S. S.; Zhang, R.; and Kambham-\npati, S. 2021. Widening the pipeline in human-guided rein-\nforcement learning with explanation and context-aware data\naugmentation. Advances in Neural Information Processing\nSystems.\nGuo, C.; Zou, S.; Zuo, X.; Wang, S.; Ji, W.; Li, X.; and\nCheng, L. 2022. Generating diverse and natural 3D human\nmotions from text. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition.\nHuang, L.; Yu, W.; Ma, W.; Zhong, W.; Feng, Z.; Wang, H.;\nChen, Q.; Peng, W.; Feng, X.; Qin, B.; et al. 2023. A survey\non hallucination in large language models: Principles, tax-\nonomy, challenges, and open questions. ACM Transactions\non Information Systems.\nIcarte, R. T.; Klassen, T. Q.; Valenzano, R.; and McIlraith,\nS. A. 2022. Reward machines: Exploiting reward function\nstructure in reinforcement learning. Journal of Artificial In-\ntelligence Research.\nIllanes, L.; Yan, X.; Icarte, R. T.; and McIlraith, S. A. 2020.\nSymbolic plans as high-level instructions for reinforcement\nlearning. In 30th International Conference on Automated\nPlanning and Scheduling.\nLee, S. J.; and Popovi ´c, Z. 2010. Learning behavior styles\nwith inverse reinforcement learning. ACM Transactions on\nGraphics.\nLin, J.; Du, Y .; Watkins, O.; Hafner, D.; Abbeel, P.; Klein,\nD.; and Dragan, A. 2023. Learning to model the world with\nlanguage. arXiv preprint arXiv:2308.01399.\nLin, J.; Fried, D.; Klein, D.; and Dragan, A. 2022. In-\nferring rewards from language in context. arXiv preprint\narXiv:2204.02515.\nLou, X.; Zhang, J.; Wang, Z.; Huang, K.; and Du, Y . 2024.\nSafe reinforcement learning with free-form natural language\nconstraints and pre-trained language models. arXiv preprint\narXiv:2401.07553.\nLuo, Y .-S.; Soeseno, J. H.; Chen, T. P.-C.; and Chen, W.-\nC. 2020. CARL: Controllable agent with reinforcement\nlearning for quadruped locomotion. ACM Transactions on\nGraphics.\nLyu, D.; Yang, F.; Liu, B.; and Gustafson, S. 2019. SDRL:\ninterpretable and data-efficient deep reinforcement learning\nleveraging symbolic planning. In 33rd AAAI Conference on\nArtificial Intelligence.\nMa, Y . J.; Liang, W.; Wang, G.; Huang, D.-A.; Bastani, O.;\nJayaraman, D.; Zhu, Y .; Fan, L.; and Anandkumar, A. 2023.\nEureka: Human-level reward design via coding large lan-\nguage models. arXiv preprint arXiv:2310.12931.\nMahmud, S.; Saisubramanian, S.; and Zilberstein, S. 2023.\nExplanation-guided reward alignment. In 32nd Interna-\ntional Joint Conference on Artificial Intelligence.\nNg, A. Y .; and Russell, S. J. 2000. Algorithms for inverse\nreinforcement learning. In 17th International Conference on\nMachine Learning.\nOpenStreetMap Contributors. 2017. Planet dump retrieved\nfrom planet.osm.org. https://www.openstreetmap.org.\nPeng, X. B.; Kanazawa, A.; Malik, J.; Abbeel, P.; and\nLevine, S. 2018. SFV: Reinforcement learning of physical\nskills from videos. ACM Transactions On Graphics.\nPeng, X. B.; Ma, Z.; Abbeel, P.; Levine, S.; and Kanazawa,\nA. 2021. AMP: Adversarial motion priors for stylized\nphysics-based character control. ACM Transactions On\nGraphics.\nPiriyakulkij, W. T.; Kuleshov, V .; and Ellis, K. 2023. Ac-\ntive preference inference using language models and proba-\nbilistic reasoning. Foundation Models for Decision Making\nWorkshop at NeurIPS 2023.\nRafailov, R.; Sharma, A.; Mitchell, E.; Manning, C. D.; Er-\nmon, S.; and Finn, C. 2024. Direct preference optimization:\nYour language model is secretly a reward model. Advances\nin Neural Information Processing Systems.\nRamachandran, D.; and Amir, E. 2007. Bayesian inverse\nreinforcement learning. In 20th International Joint Confer-\nence on Artifical intelligence.\nRao, S.; and Daum ´e III, H. 2018. Learning to Ask Good\nQuestions: Ranking Clarification Questions using Neural\nExpected Value of Perfect Information. In 56th Annual\nMeeting of the Association for Computational Linguistics\n(Volume 1: Long Papers).\nSadigh, D.; Dragan, A. D.; Sastry, S. S.; and Seshia, S. A.\n2017. Active preference-based learning of reward functions.\nIn Robotics: Science and Systems XIII.\n27527\nSilver, T.; Athalye, A.; Tenenbaum, J. B.; Lozano-Perez, T.;\nand Kaelbling, L. P. 2022. Learning neuro-symbolic skills\nfor bilevel planning. arXiv preprint arXiv:2206.10680.\nSoni, U.; Thakur, N.; Sreedharan, S.; Guan, L.; Verma,\nM.; Marquez, M.; and Kambhampati, S. 2022. Towards\ncustomizable reinforcement learning agents: Enabling pref-\nerence specification through online vocabulary expansion.\narXiv preprint arXiv:2210.15096.\nSontakke, S.; Zhang, J.; Arnold, S.; Pertsch, K.; Biyik, E.;\nSadigh, D.; Finn, C.; and Itti, L. 2024. RoboCLIP: One\ndemonstration is enough to learn robot policies. Advances\nin Neural Information Processing Systems.\nTevet, G.; Raab, S.; Gordon, B.; Shafir, Y .; Cohen-Or, D.;\nand Bermano, A. H. 2022. Human motion diffusion model.\narXiv preprint arXiv:2209.14916.\nTian, S.; Li, L.; Li, W.; Ran, H.; Ning, X.; and Tiwari, P.\n2024. A survey on few-shot class-incremental learning.\nNeural Networks.\nTien, J.; Yang, Z.; Jun, M.; Russell, S. J.; Dragan, A.; and\nBiyik, E. 2024. Optimizing robot behavior via compara-\ntive language feedback. In 3rd HRI Workshop on Human-\nInteractive Robot Learning.\nWang, Y .; Sun, Z.; Zhang, J.; Xian, Z.; Biyik, E.; Held, D.;\nand Erickson, Z. 2024. RL-VLM-F: Reinforcement learn-\ning from vision language foundation model feedback. arXiv\npreprint arXiv:2402.03681.\nWang, Z.; Merel, J. S.; Reed, S. E.; de Freitas, N.; Wayne,\nG.; and Heess, N. 2017. Robust imitation of diverse behav-\niors. Advances in Neural Information Processing Systems.\nWray, K. H.; and Zilberstein, S. 2016. A POMDP formu-\nlation of proactive learning. In 30th AAAI Conference on\nArtificial Intelligence, 3202–3208.\nYu, L.; Chen, H.; Wang, S. I.; Lei, T.; and Artzi, Y . 2020. In-\nteractive Classification by Asking Informative Questions. In\n58th Annual Meeting of the Association for Computational\nLinguistics.\nYu, W.; Gileadi, N.; Fu, C.; Kirmani, S.; Lee, K.-H.; Arenas,\nM. G.; Chiang, H.-T. L.; Erez, T.; Hasenclever, L.; Humplik,\nJ.; et al. 2023. Language to rewards for robotic skill synthe-\nsis. arXiv preprint arXiv:2306.08647.\nZhang, R.; Bansal, D.; Hao, Y .; Hiranaka, A.; Gao, J.; Wang,\nC.; Mart ´ın-Mart´ın, R.; Fei-Fei, L.; and Wu, J. 2022. A\ndual representation framework for robot learning with hu-\nman guidance. In 6th Annual Conference on Robot Learn-\ning.\nZhou, A.; and Dragan, A. D. 2018. Cost functions for robot\nmotion style. In 2018 IEEE/RSJ International Conference\non Intelligent Robots and Systems.\nZiebart, B. D.; Maas, A. L.; Bagnell, J. A.; and Dey, A. K.\n2008. Maximum entropy inverse reinforcement learning. In\nProceedings of the 23rd AAAI Conference on Artificial In-\ntelligence.\nZilberstein, S. 2015. Building strong semi-autonomous sys-\ntems. In 29th AAAI Conference on Artificial Intelligence.\n27528",
  "topic": "Maple",
  "concepts": [
    {
      "name": "Maple",
      "score": 0.8901654481887817
    },
    {
      "name": "Preference",
      "score": 0.6502870321273804
    },
    {
      "name": "Computer science",
      "score": 0.5323434472084045
    },
    {
      "name": "Psychology",
      "score": 0.32088178396224976
    },
    {
      "name": "Mathematics",
      "score": 0.11207708716392517
    },
    {
      "name": "Ecology",
      "score": 0.08880499005317688
    },
    {
      "name": "Biology",
      "score": 0.07111245393753052
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 1
}