{
  "title": "DeepNet: Scaling Transformers to 1,000 Layers",
  "url": "https://openalex.org/W4394698525",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2100802234",
      "name": "Hongyu Wang",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2099570760",
      "name": "Shuming Ma",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1974723233",
      "name": "Li Dong",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2612910427",
      "name": "Shaohan Huang",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2131422403",
      "name": "Dongdong Zhang",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2171151462",
      "name": "Furu Wei",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6756718674",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W6780805062",
    "https://openalex.org/W6810671578",
    "https://openalex.org/W6810296985",
    "https://openalex.org/W6788811087",
    "https://openalex.org/W6805239564",
    "https://openalex.org/W6768952226",
    "https://openalex.org/W6802297474",
    "https://openalex.org/W2970290486",
    "https://openalex.org/W6757468910",
    "https://openalex.org/W6780086851",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W3103334733",
    "https://openalex.org/W6774776664",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6747381837",
    "https://openalex.org/W6784447870",
    "https://openalex.org/W3174726724",
    "https://openalex.org/W6772381481",
    "https://openalex.org/W6631943919",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2971033911",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6850625674",
    "https://openalex.org/W3017454464",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W6810220367",
    "https://openalex.org/W6772383348",
    "https://openalex.org/W3175301726",
    "https://openalex.org/W3105425516",
    "https://openalex.org/W2257408573",
    "https://openalex.org/W2760656271",
    "https://openalex.org/W2903193068",
    "https://openalex.org/W2970279348",
    "https://openalex.org/W2964085268",
    "https://openalex.org/W6845401343",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W6854866820",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W6635469476",
    "https://openalex.org/W2692059227",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W6796761347",
    "https://openalex.org/W6845452229",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2507296351",
    "https://openalex.org/W3177096435",
    "https://openalex.org/W3037492894",
    "https://openalex.org/W6763468762",
    "https://openalex.org/W2884822772",
    "https://openalex.org/W6797610904",
    "https://openalex.org/W3016010032",
    "https://openalex.org/W4386071687",
    "https://openalex.org/W6767164110",
    "https://openalex.org/W6796242362",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W6752192525",
    "https://openalex.org/W3093871477",
    "https://openalex.org/W2979636403",
    "https://openalex.org/W4301914798",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3205328383",
    "https://openalex.org/W4200634402",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W4300963525",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W3010768098",
    "https://openalex.org/W4226155321",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3175746962"
  ],
  "abstract": "In this paper, we propose a simple yet effective method to stabilize extremely deep Transformers. Specifically, we introduce a new normalization function ( <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">DeepNorm</small> ) to modify the residual connection in Transformer, accompanying with theoretically derived initialization. In-depth theoretical analysis shows that model updates can be bounded in a stable way. The proposed method combines the best of two worlds, i.e., good performance of Post-LN and stable training of Pre-LN, making <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">DeepNorm</small> a preferred alternative. We successfully scale Transformers up to 1,000 layers (i.e., 2,500 attention and feed-forward network sublayers) without difficulty, which is one order of magnitude deeper than previous deep Transformers. Extensive experiments demonstrate that <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">DeepNet</small> has superior performance across various benchmarks, including machine translation, language modeling (i.e., BERT, GPT) and vision pre-training (i.e., BEiT). Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly outperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points, which indicates a promising scaling direction. Our code is available at <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://aka.ms/torchscale</uri> .",
  "full_text": "1\nDeepNet: Scaling Transformers to 1,000 Layers\nHongyu Wang∗, Shuming Ma∗, Li Dong, Shaohan Huang, Dongdong Zhang and Furu Wei\nAbstract—In this paper, we propose a simple yet effective method to stabilize extremely deep Transformers. Specifically, we introduce a\nnew normalization function (DEEP NORM ) to modify the residual connection in Transformer, accompanying with theoretically derived\ninitialization. In-depth theoretical analysis shows that model updates can be bounded in a stable way. The proposed method combines the\nbest of two worlds, i.e., good performance of Post-LN and stable training of Pre-LN, making DEEP NORM a preferred alternative. We\nsuccessfully scale Transformers up to 1,000 layers (i.e., 2,500 attention and feed-forward network sublayers) without difficulty, which is\none order of magnitude deeper than previous deep Transformers. Extensive experiments demonstrate that DEEP NET has superior\nperformance across various benchmarks, including machine translation, language modeling (i.e., BERT, GPT) and vision pre-training (i.e.,\nBEiT). Remarkably, on a multilingual benchmark with 7,482 translation directions, our 200-layer model with 3.2B parameters significantly\noutperforms the 48-layer state-of-the-art model with 12B parameters by 5 BLEU points, which indicates a promising scaling direction. Our\ncode is available at https://aka.ms/torchscale.\nIndex Terms—Transformers, Training Stability, Loss Landscape, Big Models, Optimization\n✦\n1 I NTRODUCTION\nRecent years have witnessed a trend towards large-\nscale Transformer [1] models. The capacity has substantially\nincreased from millions of parameters [2], [3] to billions [4],\n[5], [6], [7], [8], [9], [10], [11], and even trillions [12], [13].\nLarge-scale models yield state-of-the-art performance on a\nwide range of tasks, and show impressive abilities in few-\nshot and zero-shot learning. Despite an enormous number of\nparameters, their depths are limited by the training instability\nof Transformers.\nNguyen et al. [14] found that pre-norm residual con-\nnections (Pre-LN) improve the stability of Transformers\nbased on post-norm connections (Post-LN). However, the\ngradients of Pre-LN at bottom layers tend to be larger than\nat top layers [15], leading to a degradation in performance\ncompared with Post-LN. In order to alleviate the above\nissue, there have been efforts on improving the optimization\nof deep Transformer by means of better initialization [16],\n[17], [18], or better architecture [15], [19], [20], [21]. These\napproaches can stabilize a Transformer model with up to\nhundreds of layers. Yet, none of previous methods has been\nsuccessfully scaled to 1,000 layers.\nOur aim is to improve the training stability of Trans-\nformers and scale the model depth by orders of magnitude.\nTo this end, we study the cause of unstable optimization,\nfinding the exploding model update is responsible for\nthe instability. Motivated by the above observation, we\nintroduce a new normalization function ( DEEP NORM ) at\nresidual connections [22], which has theoretical justification\nof bounding the model update by a constant. We adopt the\n• Equal contribution. Work was done during Hongyu’s internship at\nMicrosoft Research.\n• Hongyu Wang is with the School of Computer and Control Engineering,\nUniversity of Chinese Academy of Sciences, Beijing, 100049. E-mail:\nwanghongyu22@mails.ucas.ac.cn\n• Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang and Furu Wei\nare with Microsoft Research. E-mail: {shumma, lidong1, shaohanh,\ndozhang, fuwei}@microsoft.com\nManuscript received February 20, 2023; revised mmm dd, 2023.\n(a) Post-LN Transformers\n (b) D EEP NET\nFig. 1. The loss surface of 36-layer vanilla Post-LN and DEEP NET at the\nearly stage of training.\nfilter normalization [23] to visualize the loss surface of vanilla\nPost-LN and DEEP NET on the IWSLT-14 De-En data set at the\nearly stage of training. Figure 1 shows that the loss surface\nof DEEP NET is much smoother compared with vanilla Post-\nLN. The proposed method is simple yet effective, with just\nlines of code change. The approach improves the stability\nof Transformers so that we are able to scale model depth\nto more than 1,000 layers. Moreover, experimental results\nshow that DEEP NORM combines the best of two worlds, i.e.,\ngood performance of Post-LN and stable training of Pre-\nLN. The proposed method can be a preferred alternative of\nTransformers, not only for extremely deep (such as > 1000\nlayers) models, but also for existing large models.\nExtensive experiments demonstrate that DEEP NET has\nsuperior performance across various benchmarks, including\nmachine translation, language modeling (i.e., BERT, GPT)\nand vision pre-training (i.e., BEiT). Notably, our 200-layer\nmodel with 3.2B parameters achieves 5 BLEU improvement\non a massively multilingual machine translation benchmark\ncompared to state-of-the-art model [24] with 48 layers and\n12B model size.\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3386927\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2\nThis paper includes the analysis for Pre-LN variants\nand the experiments for language modeling and vision pre-\ntraining of our ICML 2023 paper [25] which is an extension\nand application of our proposed framework for training\nstability of deep Transformers in this work.\n2 I NSTABILITY OF DEEP TRANSFORMER\nWe study the causes of the instability for deep Transformers.\nOur analysis begins with the observation: better initialization\nmethods stabilize the training of Transformer. This has also\nbeen verified by previous work [16], [18], [26]. Therefore, we\nstudy the training process of Post-LN with or without proper\ninitialization. With better initialization, we down-scale the\nweights of l-th layer by kl = N − l + 1, l∈ [1, N] after\nperforming Xavier initialization. For example, the output\nprojection Wl\no of FFN in l-th layer is initialized as:\nWl\no ∽ N\n\u0012\n0, 1\nk2\nl d′\n\u0013\n,\nwhere d′ is an average of input and output dimensions.\nWe name this model Post-LN-init. Notice that different from\nthe prior work [16], we narrow the scale of lower layers\ninstead of the higher layers. We believe that it helps to\nseparate the effect of the gradient scale from the model\nupdate. Besides, Post-LN-init has the same architecture as\nPost-LN, which eliminates the impact from the architecture.\nWe train 18L-18L Post-LN and 18L-18L Post-LN-init on\nthe IWSLT-14 De-En machine translation data set. Figure 2\nvisualizes their gradients and validation loss curves. As\nshown in Figure 2(c), Post-LN-init converged while Post-LN\ndid not. Post-LN-init has an even larger gradient norm in\nthe last several layers, although its weights have been scaled\ndown. Furthermore, we visualize the gradient norm of the\nlast decoder layer with varying model depth from 6L-6L to\n24L-24L. Figure 2 shows that the gradient norm of Post-LN-\ninit in the last layer is still much larger than that of Post-LN,\nregardless of model depth. It concludes that the exploding\ngradients in deep layers should not be the root cause of\ninstability of Post-LN, while the scale of model update tends\nto account for it.\nThen we demonstrate that the instability of Post-LN\ncomes from a chain of several issues, including gradient\nvanishing as well as too large model updates. As shown\nin Figure 3(a), we first visualize the norm of model update\n||∆F||2 at the early stage of training:\n||∆F||2 = ||F(x, θi) − F(x, θ0)||2, (1)\nwhere x and θi denotes input, and model parameters after\ni-th updates. Post-LN has an exploding update at the very\nbeginning of training, and then nearly no update shortly. It\nindicates that the model has been stuck in a spurious local\noptima. Both warm-up and better initialization help alleviate\nthis issue, enabling the model to update smoothly. When\nthe update explodes, the inputs to LN become large (see\nFigure 3(b) and Figure 3(c)). According to the theoretical\nanalysis from Xiong et al. [27], the magnitude of gradient\nthrough LN is inversely proportional to the magnitude of its\ninput:\n||∂LN (x)\n∂x ||2 = O(\n√\nd\n||x||2\n).\nFigure 3(b) and Figure 3(c) show that ||x||2 is significantly\nlarger than\n√\nd (d = 512) without warm-up or proper\ninitialization. This explains the gradient vanishing problem\noccurred in the training of Post-LN (see Figure 3(d)).\nAbove all, the instability starts from the large model\nupdate at the beginning of training. It renders the model\ntrapped in a bad local optima, which in turn increases the\nmagnitude of inputs to each LN. As training continues,\nthe gradient through LN becomes increasingly small, thus\nresulting in severe gradient vanishing. The vanishing gradi-\nents make it difficult to escape from the local optima, and\nfurther destabilize the optimization. On the contrary, Post-\nLN-init has relatively small updates, and the inputs to LN\nare stable. This relieves suffering from gradient vanishing,\nmaking optimization more stable.\n3 D EEP NET: EXTREMELY DEEP TRANSFORMERS\nIn this section, we introduce our extremely deep Trans-\nformers named DEEP NET. It can stabilize the optimization\nby mitigating the exploding model update problem. We\nfirst provide the estimation of the expected magnitude of\nDEEP NET’s model update. Then we provide the theoretical\nanalysis to show that its updates can be bounded by a\nconstant with our proposed D EEP NORM .\n3.1 Architecture\nDEEP NET is based on the Transformer architecture. Com-\npared to the vanilla Transformer, it uses our newDEEP NORM ,\ninstead of Post-LN, for each sub-layer. The formulation of\nDEEP NORM can be written as:\nxl+1 = LN(αxl + Gl(xl, θl)),\nwhere α is a constant, and Gl(xl, θl) is the function of the\nl-th Transformer sub-layer (i.e., attention or feed-forward\nnetwork) with parameters θl. Besides, DEEP NET scales the\nweights θl inside residual branches by β. Notably, both α\nand β are constants that only depend on the architecture,\nand we provide the derivation in Section 3.3.\n3.2 Expected Magnitude of Model Update\nAttention is an important part of Transformer. Without loss\nof generality, we study the 1-head case. Let Q, K, V∈ Rn×d\ndenote the query, key, value, respectively.WQ, WK, WV ∈\nRd×dk are the input projection matrices, and WO ∈ Rdk×d\nis the output projection matrix. Then, the attention module\ncan be formulated as:\nAttn(Q, K, V) = softmax(QWQ(KW K)T\n√dk\n)V WV WO\nWe study the magnitude of the attention module.\nLemma 1 proves that WQ and WK do not change the bound\nof attention output’s magnitude.\nLemma 1. Given X = (x1, x2, ...xn)T ∈ Rn×d, where xi is\ni.i.d, V ar[xi] = 1, Mean[xi] = 0 and qi ∈ R for all i ∈ [1, n],\nit satisfies that\nsoftmax(q1, q2, ..., qn)X Θ= xi,\nwhere Θ= stands for equal upper bound of expected magnitude.\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3386927\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n3\n13 14 15 16 17 18\nDecoder Layers\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5Gradient Norm\nPost-LN\nPost-LN-init\n6 12 18 24\nLayers\n0\n1\n2\n3Gradient Norm\nPost-LN\nPost-LN-init\n0 10 20 30\nEpochs\n3\n6\n9\n12\n15Validation Loss\nPost-LN\nPost-LN-init\nFig. 2. (a) Gradient norm in the top layers of 18L-18L models. (b) Gradient norm in the last layer of the models with depths varying from 6L-6L to\n24L-24L. (c) Validation loss curves of 18L-18L models.\n0 10 20 30\nIterations\n0\n10\n20\n30Model Update\nPost-LN + no warmup\nPost-LN + 4k warmup\nPost-LN-init + 4k warmup\n(a) Accumulated model update\n0 10 20 30\nIterations\n20\n25\n30\n35FFN Norm\nPost-LN + no warmup\nPost-LN + 4k warmup\nPost-LN-init + 4k warmup (b) Input from FFN to LN\n0 10 20 30\nIterations\n20\n30\n40\n50Self-Attn Norm\nPost-LN + no warmup\nPost-LN + 4k warmup\nPost-LN-init + 4k warmup (c) Input from attention to LN\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18\nDecoder Layers\n0.0\n0.3\n0.6\n0.9Gradient Norm\nPost-LN + no warmup\nPost-LN + 4k warmup\nPost-LN-init + 4k warmup\n(d) Gradient norm in all decoder layers\nFig. 3. Visualization of the model update, the average input of LNs, and the gradients for the 18L-18L models at the early stage of training.\nIn other words, the magnitude of attention output only de-\npends on the value and output projection: Attn(Q, K, V) Θ=\nV WV WO. In this work, we only consider the magnitude\nof model update, so it is sufficiently instructive to study\nthe case where the hidden dimension equals to 1. For\nsimplicity, we reduce the matrices WV , WO to the scalars\nv, w, which means Attn(Q, K, V) Θ= vwV . Similarly, we\nhave F F N(X) Θ= vwX, where v, wdenotes the parameters\nof the feed-forward network.\nWe define the model update as ||∆F||2 = ||F(x, θ∗) −\nF(x, θ)||2. Based on the analysis above, we have the fol-\nlowing theorem to characterize ||∆F||2’s magnitude of an\nN-layer DEEP NET with N attentions and FFNs.\nTheorem 2. Given an N-layer DEEP NET F(x, θ) (θ =\n{θ1, θ2, ..., θ2N }), where θ2l−1 and θ2l denote the parameters\nof self-attention and FFN in l-th layer, and each sub-layer is\nnormalized with DEEP NORM : xl+1 = LN(αxl + Gl(xl, θl)),\nthe expected model update ||∆F||2 satisfies:\n||∆F||2 = O(\n2NX\ni=1\nq\nv2\ni + w2\ni\nα ||θ∗\ni − θi||2)\nVanilla Post-LN can be regarded as a special case of\nDEEP NET, where α = 1 and vl = wl = 1 at Xavier\ninitialization [28]. Based on Theorem 2, we have ||∆F||2 =\nO(P2N\ni=1 ||θ∗\ni − θi||2) for vanilla Post-LN. It shows that the\nmodel tends to accumulate the update of each sub-layer,\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3386927\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n4\n6 26 46 66 86\nLayers\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5Model Update\nPost-LN\nDeepNet\nFig. 4. Model updates of vanilla Post-LN and DEEP NET at the early stage\nof training. The visualization is conducted on 64-128-2 tiny Transformers\nwith depth varying from 6L-6L to 100L-100L. It shows that DEEP NET has\nmuch smaller and more stable updates than Post-LN.\nwhich leads to exploding magnitude of model’s update and\ndestabilizes the optimization at the early stage. This explains\nour findings in Section 2.\nBesides, Theorem 2 also explains why warm-ups and\nsmaller initialization can stabilize the training of Post-LN.\nWarm-ups can reduce the magnitude of the model update\nby decreasing ||θ∗\ni − θi||2, while smaller initialization lowersq\nv2\ni + w2\ni .\nFurthermore, we study the magnitude of DEEP NET\nwith an N-layer encoder and an M-layer decoder. Let\nFed(x, y, θe, θd) denotes the model, where x, yis the input of\nencoder and decoder. θe follows the same definition as θ in\nTheorem 2. θd = {θd1, θd2, ..., θd,3M } stands for the parame-\nters of self-attentions, cross-attentions, and FFNs. We use{αe,\nGel} and {αd, Gdl} to distinguish the notations between the\nencoder and the decoder. The following theorem shows the\nexpected magnitude of the encoder-decoder’s model update\n||∆Fed||2 = ||Fed(x, y, θ∗\ne, θ∗\nd) − Fed(x, y, θe, θd)||2.\nTheorem 3. Given an encoder-decoder DEEP NET\nFed(x, y, θe, θd) with N encoder layers and M decoder\nlayers, where each encoder sub-layer is normalized as\nxl+1 = LN(αexl + Gel(xl, θel)), and the decoder sub-\nlayer is normalized as xl+1 = LN(αdxl + Gdl(xl, θdl)), the\nexpected model update ||∆Fed||2 satisfies:\n||∆Fed||2\n= O(\nMX\nj=1\nvd,3j−1wd,3j−1\nαd\n2NX\ni=1\nq\nv2\nei + w2\nei\nαe\n||θ∗\nei − θei||2\n+\n3MX\nj=1\nq\nv2\ndj + w2\ndj\nαd\n||θ∗\ndj − θdj||2) (2)\nThe vanilla encoder-decoder model satisfies that all of\n{αe, αd, vei, wei, vdi, wdi} equal to 1, so we have ||∆Fed|| =\nO(M P2N\ni=1 ||θ∗\nei − θei||2 + P3M\nj=1 ||θ∗\ndj − θdj||2). It indicates\nthe similar accumulative effect which leads to fast growth\nof the magnitude regarding the model depth (see Figure 4).\nFurthermore, the cross-attention propagates the magnitude\nfrom the encoder to the decoder, which explains why the\ndecoder is more unstable than the encoder [20].\n3.3 Derivation for DEEP NORM and the Initialization\nWe show that the expected model updates for DEEP NET can\nbe bounded by a constant with proper parameters α and β.\nOur analysis is based on SGD update, and we empirically\nverify it works well for Adam optimizer [29]. We provide the\nanalysis on the encoder-decoder architecture, which can be\nnaturally extended to encoder-only and decoder-only models\nin the same way. Analogous to Zhang et al. [17], we set our\ngoal for the model update as follows:\nGOAL: Fed(x, y, θe, θd) is updated by Θ(η) per SGD\nstep after initialization as η → 0. That is ||∆Fed||2 =\nΘ(η) where ∆Fed\n∆= Fed(x, y, θe − η ∂L\n∂θe\n, θd − η ∂L\n∂θd\n) −\nFed(x, y, θe, θd).\nFor SGD optimizer, the update of each decoder layer\n||θ∗\ndi − θdi||2 equals to η|| ∂L\n∂θdi\n||2. Xiong et al. [27] proved that\nPost-LN decreases the magnitude of backpropagating error\nsignal, so we have || ∂F\n∂θdj\n||2 ≤ ||∂F\n∂θd,3M\n||2. With || ∂F\n∂θd,3M\n||2\nΘ=\n||θd,3M||2\nαd\nand the assumption ||∂L\n∂F ||2 = O(1), the second\nterm of Equation 2 can be bounded as:\n3MX\nj=1\nq\nv2\ndj + w2\ndj\nαd\n||θ∗\ndj − θdj||2 (3)\n≤ η||∂L\n∂F ||2 · ||∂F\n∂θd,3M\n||2\n3MX\nj=1\nq\nv2\ndj + w2\ndj\nαd\nΘ= 3ηM v2\nd + w2\nd\nα2\nd\n(4)\nThere are multiple schemes to bound Equation 4 by Θ(η).\nIn order to balance the effect of residual connections and\nthe initialization, we set α2\nd = (3M)\n1\n2 , v2\nd + w2\nd = (3M)−1\n2\nand vd = wd = βd due to symmetry, that is αd = (3M)\n1\n4 ,\nβd = (12 M)−1\n4 . Similarly, we use ve = we = βe =\n0.87(N4M)− 1\n16 , αe = 0.81(N4M)\n1\n16 to bound the first term\nin Equation 2. Detailed derivation is shown in Appendix C.\nIn comparison with Post-LN, we visualize the model\nupdates for DEEP NET on IWSLT-14 De-En translation data\nset at the early training stage. Figure 4 shows that the model\nupdate of DEEP NET is nearly constant, while the model\nupdate of Post-LN is exploding. Following Hao et al. [30],\nwe further visualize the loss landscape and trajectory of the\noptimization of vanilla Post-LN and DEEP NET with varying\nmodel depth. Figure 5 presents that the loss landscape of\nvanilla Post-LN is less smooth with the increasing model\ndepth, while our model tends to have consistent smoothness\nacross different depth. Besides, vanilla Post-LN is easier to be\nstuck in a spurious local optima, which verifies our analysis\nin Section 2.\nIn summary, we apply our approach as follows:\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3386927\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n5\n4\n3\n2\n1\n0\n1\n2\n3\n4\n4\n3\n2\n1\n0\n1\n2\n3\n4\n10\n15\n20\n25\n30\nstart point\nend point\n10\n15\n20\n25\n30\nValidation Loss\n4\n3\n2\n1\n0\n1\n2\n3\n4\n4\n3\n2\n1\n0\n1\n2\n3\n4\n10\n15\n20\n25\n30\n35\nstart point\nend point\n10\n15\n20\n25\n30\n35\nValidation Loss\n4\n3\n2\n1\n0\n1\n2\n3\n4\n4\n3\n2\n1\n0\n1\n2\n3\n4\n10\n15\n20\n25\n30\nstart point\nend point\n10\n15\n20\n25\n30\nValidation Loss\n4\n3\n2\n1\n0\n1\n2\n3\n4\n4\n3\n2\n1\n0\n1\n2\n3\n4\n8\n12\n16\n20\n24\nstart point\nend point\n8\n10\n12\n14\n16\n18\n20\n22\nValidation Loss\n4\n3\n2\n1\n0\n1\n2\n3\n4\n4\n3\n2\n1\n0\n1\n2\n3\n4\n8\n12\n16\n20\n24\nstart point\nend point\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\nValidation Loss\n4\n3\n2\n1\n0\n1\n2\n3\n4\n4\n3\n2\n1\n0\n1\n2\n3\n4\n8\n12\n16\n20\n24\nstart point\nend point\n10\n12\n14\n16\n18\n20\n22\n24\nValidation Loss\n4\n 3\n 2\n 1\n 0 1 2 3 44\n3\n2\n1\n0\n1\n2\n3\n4\n10\n15\n20\n25\n30\nValidation Loss\n(a) 10L-10L\n4\n 3\n 2\n 1\n 0 1 2 3 44\n3\n2\n1\n0\n1\n2\n3\n4\n10\n15\n20\n25\n30\n35\nValidation Loss (b) 50L-50L\n4\n 3\n 2\n 1\n 0 1 2 3 44\n3\n2\n1\n0\n1\n2\n3\n4\n10\n15\n20\n25\n30\nValidation Loss (c) 100L-100L\n4\n 3\n 2\n 1\n 0 1 2 3 44\n3\n2\n1\n0\n1\n2\n3\n4\n8\n10\n12\n14\n16\n18\n20\n22\nValidation Loss (d) 10L-10L\n4\n 3\n 2\n 1\n 0 1 2 3 44\n3\n2\n1\n0\n1\n2\n3\n4\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\nValidation Loss (e) 50L-50L\n4\n 3\n 2\n 1\n 0 1 2 3 44\n3\n2\n1\n0\n1\n2\n3\n4\n10\n12\n14\n16\n18\n20\n22\n24\nValidation Loss (f) 100L-100L\nFig. 5. Loss landscape and trajectory of DEEP NET (a, b, c) and vanilla Post-LN (d, e, f) at the early stage of training. The visualization is conducted\non 64-128-2 tiny Transformers with varying depth.\nEncoder-decoder architecture\n1) Apply standard initialization (e.g., Xavier initial-\nization) for each encoder and decoder layer.\n2) For encoder layers, scale the weights of feed-\nforward networks as well as the value projection\nand the output projection of attention layers by\n0.87(N4M)− 1\n16 , and set the weight of residual\nconnections as 0.81(N4M)\n1\n16 .\n3) For decoder layers, scale the weights of feed-\nforward networks as well as the value projection\nand the output projection of attention layers by\n(12M)−1\n4 , and set the weight of residual connec-\ntions as (3M)\n1\n4 .\nThe derivation of encoder-only (such as BERT) and\ndecoder-only architectures can be conducted in the same\nway (see Appendix B). We summarize the steps as follows:\nEncoder-only (or decoder-only) architecture\n1) Apply standard initialization (e.g., Xavier initial-\nization) for each layer.\n2) For each layer, scale the weights of feed-forward\nnetworks as well as the value projection and the\noutput projection of attention layers by (8N)−1\n4\n(or (8M)−1\n4 ), and set the weight of residual\nconnections as (2N)\n1\n4 (or (2M)\n1\n4 ).\n4 D EEP NET FOR PRE-LN TRANSFORMER\nIn this section, we further extend our analysis framework\nto Pre-LN variants, which are widely adopted as the ar-\nchitecture for vision transformers [31] and large language\nmodels [5], [32]. We first introduce the architecture of DEEP -\nNET for Pre-LN. Then we estimate the expected magnitude\nof model update. Moreover, we show that the model update\nof Pre-LN-style DEEP NET grows logarithmically as the depth\nincreases, which can also be bounded independent of depth\nwith our proposed initialization.\n4.1 Architecture\nFor DEEP NET, we introduce the extra normalization inside\neach sublayer to ease the explosion of activation during\ntraining. Specially, for the multihead attentions, the layer\nnormalization modules are before the qkv projection and the\noutput projection, which can be formulated as:\nQ, K, V= WQLN(x), WKLN(x), WV LN(x) (5)\nMSA(x) = x + WOLN(Attention(Q, K, V)) (6)\nwhere WQ, WK, WV , and WO are the parameters of the\nmultihead self-attention. For the feed-forward network, we\nplace the normalizations before the input projection and the\noutput projection, which are written as:\nFC1(x) = W1LN(x) (7)\nFC2(x) = W2LN(x) (8)\nFFN(x) = FC2(ϕ(FC1(x))) (9)\nwhere W1 and W2 are parameters of the feed-forward\nlayers, and ϕ is the non-linear activation function.\n4.2 Expected Magnitude of Model Update\nBased on the framework before, we study the magnitude of\nDEEP NET for Pre-LN with an N-layer encoder under SGD\nupdate. With Lemma 1, the query and key projection do\nnot change the bound of expected magnitude of attention\nupdate. Similarly, we denote the parameters of the encoder\nθe as {vl, wl}L\nl=1, where wl and vl denote the scale of input\nand output projection of FFN, or value and output projection\nof attention module. We set the scale of shortcut α as 1\nto prevent the exponential accumulation of model update\nalong residual shortcuts. Above all, we have the following\ntheorem to characterize the expected model update of an\nN-layer Pre-LN-style DEEP NET. The proof is detailed in the\nAppendix A.4.\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3386927\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6\nTheorem 4. Given an N-layer DEEP NET F(x, θ), the l-th sub-\nlayer is formulated as xl = xl−1 + Wl,2LN(ϕ(Wl,1LN(xl−1))).\nUnder SGD update, ∆F satisfies:\n∆F = O(η(\nPL\nl=1(1 + v2\nl\nw2\nl\n)\nPL\nn=1 v2n\n+\nLX\nl=1\nLX\nk=2\n1 + v2\nl\nw2\nlPL\nn=1 v2n\nv2\nk\nPk−1\nn=1 v2n\n))\n(10)\nwhere η is learning rate, L equals to 2N.\nIf we apply standard initialization (e.g., Xavier initial-\nization) for each sublayer, the output can preserve the\nvariance of input. Therefore, vl and wl can be estimated\nas 1 at the beginning of training. With Theorem 4, we have\n||∆F|| = O(log L). It shows that the expected magnitude\nof model update for DEEP NET grows logarithmically as the\ndepth increases, which is much smaller than that of vanilla\nPost-LN. It indicates that DEEP NET is easier to be optimized\nand can be scaled up to extremely deep models.\n4.3 Derivation\nFurthermore, we demonstrate that the expected model\nupdate of DEEP NET can be further bounded with proper\ninitialization. The detailed derivation can be found in Ap-\npendix A.4. We adopt v = w = β = √log L to bound the\nmodel update independent of the depth. In summary, we\napply our initialization as follows:\n1) Apply standard initialization (e.g., Xavier initial-\nization) for each layer.\n2) For each layer, scale the weights of feed-forward\nnetworks as well as the value projection and the\noutput projection of attention layers by √log 2N\n(or √log 2M).\n5 N EURAL MACHINE TRANSLATION\nWe verify the effectiveness of DEEP NET on the popular ma-\nchine translation benchmarks, including IWSLT-14 German-\nEnglish (De-En), WMT-17 English-German (En-De), WMT-14\nEnglish-German (En-De) and WMT-14 English-French (En-\nFr) data set. We compare our method with multiple state-\nof-the-art deep Transformer models, including DLCL [19],\nNormFormer [15], ReZero [21], R-Fixup [17], T-Fixup [18],\nDS-init [16], and Admin [20]. We reproduce the baselines\nwith their open-source code, and set the hyper-parameters\nthe same for a fair comparison.\nWe use BLEU as the evaluation metric for all experiments.\nBesides, we adopt the in-built BLEU scripts of Fairseq to\nevaluate all models. Table 1 and Table 2 reports the results\nof the baselines and DEEP NET on WMT-17 En-De, WMT-14\nEn-De and WMT-14 En-Fr translation data set, respectively.\nAccording to their LNs, the baselines are grouped into three\ncategories: Pre-LN, Post-LN, and No-LN. All the compared\nmodels are base-size with different depths.\nCompared with the models with Post-LN, DEEP NET is\nmore stable, and can successfully scale to 100L-100L, reaching\nthe 28.9 BLEU on the test set. In contrast, the baselines\nwith Post-LN lead to unstable optimization when the depth\ngoes to 50L-50L. Besides, DEEP NET achieves comparable\nperformance with these baselines when the models are\nshallow.\nIn addition, we compare DEEP NET with the methods\nwithout LN. Both R-Fixup and T-Fixup introduce better\ninitialization methods, which stabilize the training of No-LN\nTransformer with up to 50-50 layers. Yet, their performance\nis not as good as those with Post-LN. Besides, half-precision\ncould destabilize the training of ReZero, leading to its diver-\ngence with 18-18 layers. This observation is also reported\nby Liu et al. [20]. Moreover, deeper models (50L-50L) do not\noutperform the shallow models (18L-18L). In comparison,\nDEEP NET achieves better translation accuracy than these\nmethods, and scaling to deeper models brings no harm to\nthe performance.\nCompared with the Post-LN baselines, the models with\nPre-LN are more stable. Both vanilla Pre-LN and DLCL\ncan be scaled to 100L-100L, and 50L-50L NormFormer is\nalso trained successfully. Nevertheless, Pre-LN leads to a\n0.5-1.0 BLEU drop compared with the converged Post-LN\nmodels. We presume this should be caused by the problem\nthat gradients of Pre-LN at earlier layers tend to be larger\nthan gradients at later layers [15]. We leave it as the future\nwork. In contrast, DEEP NET alleviates the problem by using\nPost-LN, and outperforms all the Pre-LN baselines.\nConvergence with varying depth.We vary the depths of\nthe models from 10L-10L to 100L-100L with an interval of 10\nlayers.All experiments are conducted with mixed precision\ntraining, except ReZero 1. Figure 6 shows the results on\nthe IWSLT-14 data set. We train the models for 8,000 steps\nbecause we find most divergence occurs at the beginning\nof optimization. Overall, DEEP NET is stable from shallow\nto deep. It converges fast, achieving over 30 BLEU in only\n8,000 steps while most of the baselines do not. Moreover, the\nperformance keeps improving as the model goes deeper.\nLarge learning rate, batch size, and hidden dimension.\nWe further scale DEEP NET to larger learning rate, batch size,\nand hidden dimension, respectively. For each experiment,\nwe only change one hyperparameter with the others fixed.\nFigure 7 reports the loss curves on the WMT-17 validation\nset. It shows that DEEP NET can be trained without difficulty\nin all the largest settings. The loss of DEEP NET with 1024\nhidden size increases after 10K steps because of overfitting.\nBesides, it indicates that DEEP NET can benefit from the larger\nsettings, resulting in faster convergence and lower validation\nloss.\nWe conduct experiments on the large-scale multilingual\nmachine translation, which is a good testbed for large models.\nWe first use OPUS-100 corpus [33] to evaluate our model.\nOPUS-100 is an English-centric multilingual corpus covering\n100 languages, which is randomly sampled from the OPUS\ncollection. We scale DEEP NET up to 1,000 layers. The model\nhas a 500-layer encoder, a 500-layer decoder, 512 hidden\nsize, 8 attention head, and 2,048 dimensions of feed-forward\nlayers. More details can be found in the Appendix.\nTable 3 summarizes the results of DEEP NET and the\nbaselines. It shows that increasing the depth can significantly\nimprove the translation quality of NMT: the baseline of 48\n1. According to our experiments, ReZero is unstable with half preci-\nsion, even when the model is shallow.\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3386927\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n7\nTABLE 1\nBLEU scores on the WMT -17 En-De test set for different models with varying depth.\nAL-BL refers to A-layer encoder and B-layer decoder.\nModels LN 6L-6L 18L-18L 50L-50L 100L-100L\nVanilla Post-LN [1] Post-LN 28.1 diverged\nDS-Init [16] Post-LN 27.9 diverged\nAdmin [20] Post-LN 27.9 28.8 diverged\nReZero [21] No-LN 26.9 diverged\nR-Fixup [17] No-LN 27.5 28.4 27.7 diverged\nT-Fixup [18] No-LN 27.5 28.4 27.9 diverged\nVanilla Pre-LN [1] Pre-LN 27.0 28.1 28.0 27.4\nDLCL [19] Pre-LN 27.4 28.2 diverged 27.5\nNormFormer [15] Pre-LN 27.0 28.3 27.8 diverged\nDEEP NET (ours) DEEP NORM 27.8 28.8 29.0 28.9\nTABLE 2\nBLEU scores on the WMT14 En-De and WMT14 En-Fr test set for different models with varying depth.\nAL-BL refers to A-layer encoder and B-layer decoder.\nModels LN WMT14 En-De WMT14 En-Fr\n6L-6L 18L-18L 6L-6L 18L-18L\nT-Fixup [18] No-LN 26.4 28.0 39.8 41.9\nVanilla Post-LN [1] Post-LN 27.4 diverged 39.6 diverged\nAdmin [20] Post-LN 27.4 28.4 39.4 42.4\nVanilla Pre-LN [1] Pre-LN 26.6 27.8 39.6 41.8\nNormFormer [15] Pre-LN 27.2 27.9 39.7 42.1\nDEEP NET (ours) DEEP NORM 27.3 28.7 39.9 42.4\n10 40 70 100\nLayers\n0\n10\n20\n30BLEU\nPost-LN\nDS-init\nAdmin\nDeepNet\n10 40 70 100\nLayers\n20\n25\n30\n35BLEU\nT-Fixup\nReZero (fp32)\nR-Fixup\nDeepNet\n10 40 70 100\nLayers\n20\n25\n30\n35BLEU\nPre-LN\nDLCL\nNormFormer\nDeepNet\nFig. 6. BLEU scores on the IWSLT -14 De-En test set for different deep models with varying depth from 10L-10L to 100L-100L.\nlayers achieves a gain of 3.2 points on average over the 12-\nlayer model. DEEP NET can successfully scale up the depth to\n1,000 layers, outperforming the baseline by an improvement\nof 4.4 BLEU. It is noted that DEEP NET is only trained for 4\nepochs, and the performance can be further improved given\nmore computation budgets.\n6 M ASSIVELY MULTILINGUAL NEURAL MACHINE\nTRANSLATION\nScaling law in terms of depth.We train DEEP NET of {12,\n20, 100, 200, 1000} layers on the OPUS-100 data set. For the\nevaluation, we report the case-sensitive detokenized BLEU\nusing sacreBLEU [34] for the results of OPUS-100. 2. Figure 8\nillustrates the scaling curve. Compared with bilingual NMT,\nmultilingual NMT benefits more from scaling the depth of the\nmodel because of its hunger in model capacity. We observe\nlogarithmic growth of the BLEU score for multilingual NMT,\nand the scaling law can be written as:\nL(d) = A log(d) + B\nwhere d is the depth, and A, Bare the constants regarding\nthe other hyper-parameters.\nComparsion given similar training FLOPs.Following\n[35], [36], the training FLOPs can be estimated as 6ND,\nwhere N and D denote the parameters of the model and\n2. BLEU+case.mixed+lang.{src}-{tgt}+numrefs.1+smooth.exp+tok.13a\n+version.1.4.14\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3386927\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n8\nTABLE 3\nAverage BLEU for DEEP NET and the baseline on the OPUS-100 test sets.\nModels # Layers # Params X→En En →X Avg\nBaseline [33]\n12 133M 27.5 21.4 24.5\n24 173M 29.5 22.9 26.2\n48 254M 31.4 24.0 27.7\nDEEP NET (ours) 200 863M 33.2 29.0 31.1\n1000 3.8B 33.9 30.2 32.1\nTABLE 4\nComparison for DEEP NET and the baseline given the similar training FLOPs on the OPUS-100 test sets.\nAll models are trained with the same batch size and 50K steps. AL-BL refers to A-layer encoder and B-layer decoder.\nModels # Layers # Params X→En En →X Avg\nPost-LN [1] 12L-12L 610M 33.1 28.9 31.0\nDEEP NET (ours) 90L-6L 480M 33.6 28.6 31.1\n48L-48L 33.7 29.5 31.6\n0 10K 20K 30K\nIterations\n3.8\n4.0\n4.2\n4.4\n4.6\n4.8Validation Loss\nlr = 5e-4\nlr = 1e-3\nlr = 1.5e-3\n0 10K 20K 30K\nIterations\n3.8\n4.0\n4.2\n4.4\n4.6\n4.8Validation Loss\nbsz = 64 x 4k\nbsz = 128 x 4k\nbsz = 256 x 4k\n0 10K 20K 30K\nIterations\n3.8\n4.0\n4.2\n4.4\n4.6\n4.8Validation Loss\n512-2048-8\n768-3072-12\n1024-4096-16\nFig. 7. WMT -17 En-De validation loss curves for 18L-18L DEEP NET with varying learning rate, batch size and hidden dimension.\n10 100 1000\nDepth\n25\n27\n29\n31\n33BLEU\nFig. 8. Average BLEU scores for DEEP NET with varying depth on the\nOPUS-100 En-X and X-En test sets.\nthe size of training data, respectively. Therefore, we train\nDEEP NET of a 48-layer encoder layers, a 48-layer decoder\nand 512 hidden dimension on the OPUS-100 data set, while\nthe baseline [1] has a 12-layer encoder, a 12-layer decoder and\n1024 hidden dimension. Given the similar training FLOPs,\nall models are trained with 50K steps and the same batch\nsize. The other hyperparameters are detailed in Appendix.\nTable 4 shows that the deep and narrow DEEP NET\noutperforms the shallow and wide baseline by a gain of\n0.6 BLEU on the test set of OPUS-100 data set, indicating that\ndeepening the model is a more promising direction given\nthe similar training FLOPs.\nComparison with the asymmetric encoder-decoder.We\npresent the comparison of the asymmetric and symmetric\nencoder-decoder architecture in Table 4. We train DEEP NET\nwith a 90-layer encoder and a 6-layer decoder on the OPUS-\n100 data set. As shown in Table 4, the symmetric architecture\n(48L-48L) outperforms the asymmetric architecture (90L-\n6L) by a gain of 0.5 BLEU on the test set. It shows that\na shallow decoder leads to the degradation of performance\non multilingual machine translation, especially for En → X\ntranslation directions.\nMore data and language directions. To explore the\nlimits of DEEP NET on multilingual NMT, we then scale\nup the training data by using CCMatrix [37]. We also expand\nthe data from CCAligned [38], OPUS [33], and Tatoeba 3 to\ncover all languages of Flores101 evaluation sets. The final\ndata consists of 102 languages, 1932 directions, and 12B\nsentence pairs. With the data, we train DEEP NET with a 100-\nlayer encoder, 100-layer decoder, 1,024 hidden dimension,\n16 heads, and 4,096 intermediate dimension of feed-forward\n3. https://tatoeba.org/en/\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3386927\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n9\nTABLE 5\nBLEU scores for DEEP NET and M2M-100 on various evaluation sets.\nModels # Layers # Params WMT OPUS TED Flores\nM2M-100 [24] 48 12B 31.9 18.4 18.7 13.6\nDEEP NET (ours) 200 3.2B 33.9 23.0 20.1 18.6\nTABLE 6\nResults on the GLUE development set.\nModels LR MNLI QNLI QQP SST CoLA MRPC STS Avg.\nTransformer [1] 5e-4 86.7/86.7 92.2 91.0 93.4 59.8 86.4 89.4 85.7\n1e-3 diverged\nDEEP NET (ours) 1e-3 86.6/86.6 92.8 90.6 93.7 60.4 90.2 90.1 86.4\nlayers. Following [35], [36], the training FLOPs can be\nestimated as 5.2 ZFLOPs, resulting in up to 18 days on\n128 TESLA V100-32GB GPUs. More details can be found in\nthe Appendix D.4.\nWe compare DEEP NET with the state-of-the-art mul-\ntilingual NMT model M2M-100 [24]. M2M-100 has a 24-\nlayer encoder, a 24-layer decoder, and 4,096 hidden size,\nresulting in up to 12B parameters. Compared with M2M-100,\nDEEP NET is deep and narrow with only 3.2B parameters. For\na fair comparison, we generate the model with beam size 5\nand length penalty 1.\nFollowing M2M-100 [24], we evaluate the models on\nseveral multilingual translation evaluation data sets, includ-\ning WMT [39], [40], [41], [42], OPUS [33], TED [43], and\nFlores [44]. The language pairs from the WMT data set are\nEnglish-centric. There are 10 languages including English,\nand most of them are high-resource. For the OPUS data\nset, we select the non-English directions from the test set,\nwhich has 30 evaluation pairs. The TED evaluation set has 28\nlanguages and 756 directions, and the data is from the spoken\nlanguage domain. The Flores data set has all translation\npairs between 102 languages. We use a subset covering\nthe languages supported by both M2M-100 and DEEP NET,\nresulting in 87 languages and 7,482 translation directions. We\nreport the results in Table 5. For a fair comparison, we use the\nsame evaluation methods as the baseline. For WMT, OPUS,\nand TED, we adopt the same test sets and evaluation scripts\nas in M2M-100 [24], and the results of M2M-100 are directly\nfrom the paper [24]. For the Flores-101 evaluation set, we\nreport the spBLEU4 of M2M-12B with the public checkpoint\nand script.5 Table 5 shows that DEEP NET has significantly\nbetter performance than M2M-100 on all evaluation data\nsets, indicating that deepening the model is a very promising\ndirection to improve the quality of NMT models.\n7 M ASKED LANGUAGE MODELING\nWe compare DEEP NET with Transformer [1] on masked\nlanguage modeling [2], [45]. For a fair comparison, we pre-\ntrain DEEP NET and the baselines on the English Wikipedia\n4. https://github.com/facebookresearch/flores\n5. https://github.com/pytorch/fairseq/tree/main/examples/\nm2m 100\nand the Bookcorpus [45] with 12 layers, 768 hidden dimen-\nsions, and 3072 FFN dimensions. More details regarding\nhyperparameters can be found in the Appendix.\nWe search the pre-training learning rate among {5e-4,\n1e-3}, and choose the largest one that can converge. We\nfine-tune the models on the GLUE [46] benchmarks. Table 6\ndemonstrates the results of DEEP NET and the baselines. It\nshows that our model has better performance and training\nstability than the strong baselines with a gain of average 0.7\npoints.\n8 C AUSAL LANGUAGE MODELING\nWe implement DEEP NET on causal language modeling,\nwhich is the pre-training task for recent large language\nmodels (e.g., GPT [5], [47], LLaMA [32], [48], etc). We start\nwith a model that has the same configuration as GPT-3\nMedium (350M), and further scale its depth from 24L to\n48L and 72L. All models are trained on an English-language\ncorpus, which is a subset of the data from [45] and the English\nportion of CC100 corpus. We adopt the GPT-2 tokenizer [4]\nto preprocess the data. The other training hyperparameters\nare detailed in the Table 22 of Appendix.\nWe compare DEEP NET with GPT-2 [4] and Norm-\nformer [15]. Normformer is a state-of-the-art architecture\nfor causal language modeling. For a fair comparison, we\nreproduce the results of our model and the baselines un-\nder the same setting. We evaluate their performance of\nin-context learning. Following the previous work [5], we\nchoose Winogrande [49], Winograd [50], Storycloze [51], and\nHellaswag [52] as the benchmark.\nTable 7 summarizes the results in the zero-shot setting. It\nshows that DEEP NET achieves significant improvements over\nboth GPT-2 and Normformer across different scales. Besides,\nDEEP NET tolerates a larger learning rate than the baselines,\nindicating that our model is more stable in optimization. This\nallows DEEP NET to further scale up without pain. Table 8 and\nTable 9 report the results in the few-shot setting. DEEP NET is\nalso better at few-shot learning than the baselines across four\ndata sets, proving the effectiveness of DEEP NET on causal\nlanguage modeling.\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3386927\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n10\nTABLE 7\nZero-shot results for DEEP NET and the baselines (WGe: Winogrande, WG: Winograd, SC: Storycloze, and HS: Hellaswag data set).\nModels # Layers LR WGe WG SC HS Avg.\nGPT-2 [4]\n24L\n5e-4 55.2 65.3 70.8 44.8 59.0\nGPT-2 [4] 1e-3 diverged\nNormformer [15] 5e-4 54.3 68.1 72.0 45.9 60.1\nNormformer [15] 1e-3 diverged\nDEEP NET (ours) 1e-3 54.3 71.9 72.4 46.9 61.4\nGPT-2 [4]\n48L\n5e-4 57.3 67.0 74.0 48.0 61.6\nNormformer [15] 5e-4 56.5 70.5 74.0 49.8 62.7\nDEEP NET (ours) 1.2e-3 57.0 73.3 74.7 51.2 64.1\nGPT-2 [4]\n72L\n5e-4 58.0 70.9 75.7 51.7 64.1\nNormformer [15] 5e-4 57.4 75.4 75.2 53.6 65.4\nDEEP NET (ours) 1.2e-3 57.9 73.7 76.6 55.1 65.8\nTABLE 8\nOne-shot results for DEEP NET and the baselines (WGe: Winogrande, WG: Winograd, SC: Storycloze, and HS: Hellaswag data set).\nModels # Layers LR WGe WG SC HS Avg.\nGPT-2 [4]\n24L\n5e-4 54.4 66.7 71.0 44.8 59.2\nGPT-2 [4] 1e-3 diverged\nNormformer [15] 5e-4 54.0 67.4 72.1 45.6 59.8\nNormformer [15] 1e-3 diverged\nDEEP NET (ours) 1e-3 54.1 70.2 72.8 47.3 61.1\nGPT-2 [4]\n48L\n5e-4 56.0 69.5 74.2 48.5 62.1\nNormformer [15] 5e-4 54.7 71.2 74.8 50.6 62.8\nDEEP NET (ours) 1.2e-3 56.8 71.6 74.9 51.5 63.7\nGPT-2 [4]\n72L\n5e-4 56.9 71.2 76.0 52.2 64.1\nNormformer [15] 5e-4 57.8 69.8 76.8 54.0 64.6\nDEEP NET (ours) 1.2e-3 59.8 74.0 77.9 55.5 66.8\nTABLE 9\nFour-shot results for DEEP NET and the baselines (WGe: Winogrande, WG: Winograd, SC: Storycloze, and HS: Hellaswag data set).\nModels # Layers LR WGe WG SC HS Avg.\nGPT-2 [4]\n24L\n5e-4 54.0 67.7 69.8 44.6 59.0\nGPT-2 [4] 1e-3 diverged\nNormformer [15] 5e-4 54.3 70.2 71.4 45.9 60.5\nNormformer [15] 1e-3 diverged\nDEEP NET (ours) 1e-3 57.6 74.7 72.8 47.5 63.2\nGPT-2 [4]\n48L\n5e-4 57.7 71.2 73.8 48.7 62.9\nNormformer [15] 5e-4 56.8 75.4 75.9 50.7 64.7\nDEEP NET (ours) 1.2e-3 57.9 71.9 76.4 51.9 64.5\nGPT-2 [4]\n72L\n5e-4 57.5 73.3 76.1 52.4 64.8\nNormformer [15] 5e-4 57.7 74.0 77.0 54.9 65.9\nDEEP NET (ours) 1.2e-3 58.3 74.0 79.0 55.7 66.8\n9 M ASKED IMAGE MODELING\nWe pretrain DEEP NET under masked image modeling frame-\nwork (BEiT; [53], [54]), and then fine-tune the model on\nvarious downstream vision tasks by appending lightweight\ntask layers. Specifically, we encourage DEEP NET to recon-\nstruct corresponding discrete visual tokens [54], based on the\ncorrupt input images.\nWe compare DEEP NET with the vanilla ViT [31]. All\nmodels are pretrained on the ImageNet-1k [55] with 300\nepochs schedule under the same settings for a fair compar-\nison. After that, we fine-tune them on ImageNet-1k for the\nimage classification and on ADE20k [56] for the semantic\nsegmentation. Further, we evaluate the robustness of all\nfine-tuned models on various ImageNet variants, namely\nImageNet-Adversarial [57], ImageNet-Rendition [58] and\nImageNet-Sketch [59]. We summarize the results of those\nvision tasks in Table 10. The hyperparameters are detailed in\nAppendix.\nTable 10 shows that DEEP NET surpasses the vanilla\nViT by 0.4% and 0.6% for ViT-Base and ViT-Large on the\nvalidation set of ImageNet, respectively. Moreover,DEEP NET\noutperforms the baseline by a significant margin across three\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3386927\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n11\nTABLE 10\nResults on vision tasks. We report top-1 accuracy on ImageNet and its variants, and mIoU metric on ADE20k for semantic segmentation.\nWe compare both ViT -Base (12L) and ViT -Large (24L).\nModels # Layers ADE20k ImageNet ImageNet ImageNet ImageNet Avg.Adversarial Rendition Sketch\nViT [31] 12L 51.4 84.5 45.9 55.6 42.2 57.1\nDEEP NET (ours) 52.2 84.9 48.9 57.7 43.9 58.9\nViT [31] 24L 54.2 86.2 60.1 63.2 48.5 64.5\nDEEP NET (ours) 54.6 86.8 65.4 67.5 52.0 67.9\n41.8\n42.0\n42.2\n42.4\n42.6BLEU\n* /4\n* /2\n*\n2 *\n4 *\nShortcut scale\n34.5\n35.0\n41.8\n42.0\n42.2\n42.4\n42.6BLEU\n* /4\n* /2\n*\n2 *\n4 *\nInitialization scale\n0\n1\nFig. 9. BLEU scores of DEEP NET on the test set of WMT -14 En-Fr data set for different scales of shortcut (Left) and initialization (Right). Note thatα∗\nand β∗ denote the parameters of DEEP NORM . All models are trained with an 18-layer encoder and 18-layer decoder.\nTABLE 11\nAblations for DeepNorm and its initialization on WMT -14 En-Fr test set.\nModels # Layers BLEU\nDeepNet\n36L\n42.4\n- DeepNorm diverged\n- Initializaton 42.2\nImageNet variants. By appending the UperNet [60] task\nlayer, we conduct semantic segmentation experiments on\nADE20k. For ViT-Base models, DEEP NET achieves a gain of\n0.8% mIoU compared with the vanilla ViT. For ViT-Large\nmodels, DEEP NET can boost the performance to 54.6%.\n10 A BLATION STUDY\nIn this section, we present the ablation study of DEEP NET on\nthe WMT-14 English-French (En-Fr) data set. All models are\ntrained with an 18-layer encoder, an 18-layer decoder and\n512 hidden dimensions for 100K steps. The hyperparameters\nare detailed in the Appendix. We report the BLEU scores of\nall models on the test set.\nFirst we ablate the effect of DEEP NORM and its ini-\ntialization. Table 11 shows that removing the initialization\nleads to the degradation of performance, 0.2 BLEU dropped\ncompared with DEEP NET. Besides, removing DEEP NORM\nresults in the divergence.\nMoreover, we ablate different values of shortcut scale\nα and initialization scale β of DEEP NORM . Let α∗ and β∗\ndenote the parameters for DEEP NORM . We set the scale of\nshortcut as α∗ and vary β from {0.25, 0.5, 1, 2, 4}β∗. Then\nwe set the scale of initialization as β∗ and vary α from\n{0.25, 0.5, 1, 2, 4}α∗. Figure 9 shows that small shortcut scale\nand large initialization scale results in the instability of the\ntraining, while large shortcut scale and small initialization\nscale tends to undermine the performance. Therefore, in this\nwork, we use α and β of similar magnitude to achieve the\nbalance between good performance and stable training.\n11 R ELATED WORK\nTransformers have achieved success across many fields, in-\ncluding machine translation [1], [61], language modelling [2],\n[4], [5], [45], speech recognition [62], vision pre-training [31],\n[53], [54] and vision-language pre-training [63]. Despite\ntheir great success, the Transformers suffer a lot from\nthe instability of their optimization, which increases the\ncost of the training for large-scale models. Most successful\nimplementations involve warmup stage, Adam optimizer\nand layer normalization.\nThere are a lot of efforts to understand the effect of these\ncomponents and improve the stability of Transformers. For\nPost-LN Transformers, Liu et al. [64] claimed that the neces-\nsity of warmup stage comes from reducing large variance of\nAdam optimizer at the early stage. They further proposed\nRAdam to rectify the variance of the adaptive learning rate.\nZhang et al. [16] showed that a depth-scaled initialization\ncan reduce the output variance of residual connections to\nease gradient vanishing through layer normalization. Liu et\nal. [20] argued that the gradient vanishing of decoder is eased\nby Adam, and heavy dependency on Post-LN’s residual\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3386927\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n12\nbranches amplifies small parameter perturbations, leads to\nsignificant disturbances in the model output. Wang et al. [19]\nadopted densely connected layers to train deep Transformer\nfor machine translation.\nExcept for Post-LN Transformers, Xionget al. [27], Wanget\nal. [19] and Nguyen et al. [14] empirically validated that Pre-\nLN Transformers are easier to be optimized, and the warmup\nstage can be safely removed. Xiong et al. [27] found that\nwarmup stage also helps quiet a lot for other optimizer (e.g.\nStochastic Gradient Descent). They further proved that for\nPost-LN Transformers, the gradients’ scale in deep layers is\nlarger. Thus they argued that explosive gradients in deep\nlayers of Post-LN require warmup stage to stabilize. Ding et\nal. [65] proposed the precision bottleneck relaxation and\nsandwich-LN to stabilize the training. Normformer [15]\nintroduced head-scaled attention mechanism and extra\nnormalization to improve the performance and speed up\ntraining for language modeling.\nAnother line of research aims to train LayerNorm-free\nTransformers. Bachlechner et al. [21] introduced ReZero,\nwhich removed layer normalization and set the weights\nof residual branches as zero. ReZero successfully trained\nvery deep Transformer and achieved faster training and\nbetter performance for language modeling. Zhang et al. [17]\nfirst showed that a deep residual network with CNN, MLP\nblocks can be successfully trained without normalization.\nThey proposed a weight initialization named Fixup to\nconstraint explosive variance of model’s update, and added\nextra layers to preserve model’s capability. Inspired by this\nwork, Huang et al. [18] further analysed the magnitude of\nattention module and proposed a weight initialization named\nT-Fixup for deep LayerNorm-free Transformer. With analysis\nframework of T-Fixup [18], Xu et al. [26] proposed a data-\ndependent initialization strategy for vanilla and relation-\naware Transformer on pre-trained encodings.\n12 C ONCLUSION\nWe improve the stability of Transformer and successfully\nscale it to 1,000 layers. This is achieved by our DEEP NET\nwith a novel normalization function called DEEP NORM .\nIt has theoretical justification to stabilize the optimization\nwith a constant upper bound for model updates. Extensive\nexperimental results verify the effectiveness of our methods\nacross various tasks, including machine translation, language\nmodeling (i.e., BERT, GPT) and vision pre-training (i.e., BEiT),\nwhich makes DEEP NET a promising option for scaling up\nany Transformer models. In the future, we will extend DEEP -\nNET to support more diverse tasks, e.g., protein structure\nprediction [66].\nREFERENCES\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin NeurIPS, 2017, pp. 5998–6008.\n[2] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training\nof deep bidirectional transformers for language understanding,” in\nNAACL-HLT, 2019, pp. 4171–4186.\n[3] A. Conneau, K. Khandelwal, N. Goyal, V . Chaudhary, G. Wenzek,\nF. Guzm ´an, E. Grave, M. Ott, L. Zettlemoyer, and V . Stoyanov,\n“Unsupervised cross-lingual representation learning at scale,” in\nACL 2020, D. Jurafsky, J. Chai, N. Schluter, and J. R. Tetreault, Eds.,\n2020, pp. 8440–8451.\n[4] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n“Language models are unsupervised multitask learners,” 2019.\n[5] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P . Dhariwal,\nA. Neelakantan, P . Shyam, G. Sastry, A. Askell, S. Agarwal,\nA. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,\nD. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,\nM. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,\nA. Radford, I. Sutskever, and D. Amodei, “Language models are\nfew-shot learners,” in NeurIPS, 2020.\n[6] Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. X. Chen,\nH. Lee, J. Ngiam, Q. V . Le, Y. Wu, and Z. Chen, “Gpipe: Efficient\ntraining of giant neural networks using pipeline parallelism,” in\nNeurIPS, 2019, pp. 103–112.\n[7] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY. Zhou, W. Li, and P . J. Liu, “Exploring the limits of transfer\nlearning with a unified text-to-text transformer,” Journal of Machine\nLearning Research, vol. 21, no. 140, pp. 1–67, 2020.\n[8] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun,\nN. Shazeer, and Z. Chen, “Gshard: Scaling giant models with\nconditional computation and automatic sharding,” in ICLR, 2021.\n[9] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, H. F. Song,\nJ. Aslanides, S. Henderson, R. Ring, S. Young, E. Rutherford, T. Hen-\nnigan, J. Menick, A. Cassirer, R. Powell, G. van den Driessche, L. A.\nHendricks, M. Rauh, P . Huang, A. Glaese, J. Welbl, S. Dathathri,\nS. Huang, J. Uesato, J. Mellor, I. Higgins, A. Creswell, N. McAleese,\nA. Wu, E. Elsen, S. M. Jayakumar, E. Buchatskaya, D. Budden,\nE. Sutherland, K. Simonyan, M. Paganini, L. Sifre, L. Martens,\nX. L. Li, A. Kuncoro, A. Nematzadeh, E. Gribovskaya, D. Donato,\nA. Lazaridou, A. Mensch, J. Lespiau, M. Tsimpoukelli, N. Grigorev,\nD. Fritz, T. Sottiaux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama,\nC. de Masson d’Autume, Y. Li, T. Terzi, V . Mikulik, I. Babuschkin,\nA. Clark, D. de Las Casas, A. Guy, C. Jones, J. Bradbury, M. Johnson,\nB. A. Hechtman, L. Weidinger, I. Gabriel, W. S. Isaac, E. Lockhart,\nS. Osindero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub, J. Stanway,\nL. Bennett, D. Hassabis, K. Kavukcuoglu, and G. Irving, “Scaling\nlanguage models: Methods, analysis & insights from training\ngopher,” CoRR, vol. abs/2112.11446, 2021.\n[10] X. V . Lin, T. Mihaylov, M. Artetxe, T. Wang, S. Chen, D. Simig,\nM. Ott, N. Goyal, S. Bhosale, J. Du, R. Pasunuru, S. Shleifer,\nP . S. Koura, V . Chaudhary, B. O’Horo, J. Wang, L. Zettlemoyer,\nZ. Kozareva, M. T. Diab, V . Stoyanov, and X. Li, “Few-shot learning\nwith multilingual language models,” CoRR, vol. abs/2112.10668,\n2021.\n[11] S. Smith, M. Patwary, B. Norick, P . LeGresley, S. Rajbhandari,\nJ. Casper, Z. Liu, S. Prabhumoye, G. Zerveas, V . Korthikanti,\nE. Zheng, R. Child, R. Y. Aminabadi, J. Bernauer, X. Song,\nM. Shoeybi, Y. He, M. Houston, S. Tiwary, and B. Catanzaro, “Using\ndeepspeed and megatron to train megatron-turing NLG 530b, A\nlarge-scale generative language model,” CoRR, vol. abs/2201.11990,\n2022.\n[12] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling\nto trillion parameter models with simple and efficient sparsity,”\nCoRR, vol. abs/2101.03961, 2021.\n[13] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun,\nY. Zhou, A. W. Yu, O. Firat, B. Zoph, L. Fedus, M. Bosma, Z. Zhou,\nT. Wang, Y. E. Wang, K. Webster, M. Pellat, K. Robinson, K. Meier-\nHellstern, T. Duke, L. Dixon, K. Zhang, Q. V . Le, Y. Wu, Z. Chen,\nand C. Cui, “Glam: Efficient scaling of language models with\nmixture-of-experts,” CoRR, vol. abs/2112.06905, 2021.\n[14] T. Q. Nguyen and J. Salazar, “Transformers without tears: Improv-\ning the normalization of self-attention,” CoRR, vol. abs/1910.05895,\n2019.\n[15] S. Shleifer, J. Weston, and M. Ott, “Normformer: Improved\ntransformer pretraining with extra normalization,” CoRR, vol.\nabs/2110.09456, 2021.\n[16] B. Zhang, I. Titov, and R. Sennrich, “Improving deep transformer\nwith depth-scaled initialization and merged attention,” in EMNLP-\nIJCNLP, 2019, pp. 898–909.\n[17] H. Zhang, Y. N. Dauphin, and T. Ma, “Fixup initialization: Residual\nlearning without normalization,” in ICLR, 2019.\n[18] X. S. Huang, F. P ´erez, J. Ba, and M. Volkovs, “Improving trans-\nformer optimization through better initialization,” in ICML, ser.\nProceedings of Machine Learning Research, vol. 119, 2020, pp.\n4475–4483.\n[19] Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. F. Wong, and L. S. Chao,\n“Learning deep transformer models for machine translation,” in\nACL, 2019, pp. 1810–1822.\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3386927\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n13\n[20] L. Liu, X. Liu, J. Gao, W. Chen, and J. Han, “Understanding\nthe difficulty of training transformers,” in Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing\n(EMNLP), 2020, pp. 5747–5763.\n[21] T. Bachlechner, B. P . Majumder, H. H. Mao, G. W. Cottrell, and J. J.\nMcAuley, “Rezero is all you need: Fast convergence at large depth,”\nCoRR, vol. abs/2003.04887, 2020.\n[22] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in 2016 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2016, pp. 770–778.\n[23] H. Li, Z. Xu, G. Taylor, C. Studer, and T. Goldstein, “Visualizing\nthe loss landscape of neural nets,” in NeurIPS, 2018, pp. 6391–6401.\n[24] A. Fan, S. Bhosale, H. Schwenk, Z. Ma, A. El-Kishky, S. Goyal,\nM. Baines, O. Celebi, G. Wenzek, V . Chaudhary, N. Goyal, T. Birch,\nV . Liptchinsky, S. Edunov, M. Auli, and A. Joulin, “Beyond english-\ncentric multilingual machine translation,” J. Mach. Learn. Res. ,\nvol. 22, pp. 107:1–107:48, 2021.\n[25] H. Wang, S. Ma, S. Huang, L. Dong, W. Wang, Z. Peng, Y. Wu,\nP . Bajaj, S. Singhal, A. Benhaim, B. Patra, Z. Liu, V . Chaudhary,\nX. Song, and F. Wei, “Magneto: A foundation transformer,” in\nInternational Conference on Machine Learning, ICML 2023 , A. Krause,\nE. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, Eds.\n[26] P . Xu, D. Kumar, W. Yang, W. Zi, K. Tang, C. Huang, J. C. K. Cheung,\nS. J. D. Prince, and Y. Cao, “Optimizing deeper transformers on\nsmall datasets,” in ACL/IJCNLP, 2021, pp. 2089–2102.\n[27] R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang,\nY. Lan, L. Wang, and T. Liu, “On layer normalization in the\ntransformer architecture,” in ICML, ser. Proceedings of Machine\nLearning Research, vol. 119, 2020, pp. 10 524–10 533.\n[28] X. Glorot and Y. Bengio, “Understanding the difficulty of training\ndeep feedforward neural networks,” in AISTATS 2010, Y. W. Teh\nand D. M. Titterington, Eds.\n[29] D. P . Kingma and J. Ba, “Adam: A method for stochastic optimiza-\ntion,” in ICLR 2015, 2015.\n[30] Y. Hao, L. Dong, F. Wei, and K. Xu, “Visualizing and understanding\nthe effectiveness of BERT,” in EMNLP-IJCNLP 2019, K. Inui, J. Jiang,\nV . Ng, and X. Wan, Eds. Association for Computational Linguistics,\n2019, pp. 4141–4150.\n[31] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:\nTransformers for image recognition at scale,” in International\nConference on Learning Representations, 2021.\n[32] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez,\nA. Joulin, E. Grave, and G. Lample, “LLaMA: open and efficient\nfoundation language models,” CoRR, vol. abs/2302.13971, 2023.\n[33] B. Zhang, P . Williams, I. Titov, and R. Sennrich, “Improving\nmassively multilingual neural machine translation and zero-shot\ntranslation,” in ACL 2020. Association for Computational Linguis-\ntics, 2020, pp. 1628–1639.\n[34] M. Post, “A call for clarity in reporting BLEU scores,” in Proceedings\nof the Third Conference on Machine Translation: Research Papers, WMT\n2018, October 31 - November 1, 2018 , O. Bojar, R. Chatterjee, C. Fe-\ndermann, M. Fishel, Y. Graham, B. Haddow, M. Huck, A. Jimeno-\nYepes, P . Koehn, C. Monz, M. Negri, A. N ´ev´eol, M. L. Neves,\nM. Post, L. Specia, M. Turchi, and K. Verspoor, Eds. Association\nfor Computational Linguistics, 2018, pp. 186–191.\n[35] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai,\nE. Rutherford, D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark,\nT. Hennigan, E. Noland, K. Millican, G. van den Driessche,\nB. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae,\nO. Vinyals, and L. Sifre, “Training compute-optimal large language\nmodels,” CoRR, vol. abs/2203.15556, 2022.\n[36] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,\nR. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws\nfor neural language models,” CoRR, vol. abs/2001.08361, 2020.\n[37] H. Schwenk, G. Wenzek, S. Edunov, E. Grave, A. Joulin, and A. Fan,\n“CCMatrix: Mining billions of high-quality parallel sentences on the\nweb,” in Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference\non Natural Language Processing, ACL/IJCNLP, C. Zong, F. Xia, W. Li,\nand R. Navigli, Eds., 2021, pp. 6490–6500.\n[38] A. El-Kishky, V . Chaudhary, F. Guzm´an, and P . Koehn, “CCAligned:\nA massive collection of cross-lingual web-document pairs,” in\nProceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP, B. Webber, T. Cohn, Y. He, and Y. Liu,\nEds., 2020, pp. 5960–5969.\n[39] O. Bojar, C. Buck, C. Federmann, B. Haddow, P . Koehn, J. Leveling,\nC. Monz, P . Pecina, M. Post, H. Saint-Amand, R. Soricut, L. Specia,\nand A. Tamchyna, “Findings of the 2014 workshop on statistical\nmachine translation,” in Proceedings of the Ninth Workshop on\nStatistical Machine Translation, WMT@ACL 2014. The Association\nfor Computer Linguistics, 2014, pp. 12–58.\n[40] O. Bojar, R. Chatterjee, C. Federmann, Y. Graham, B. Haddow,\nS. Huang, M. Huck, P . Koehn, Q. Liu, V . Logacheva, C. Monz,\nM. Negri, M. Post, R. Rubino, L. Specia, and M. Turchi, “Findings\nof the 2017 conference on machine translation (WMT17),” inProceed-\nings of the Second Conference on Machine Translation, WMT, O. Bojar,\nC. Buck, R. Chatterjee, C. Federmann, Y. Graham, B. Haddow,\nM. Huck, A. Jimeno-Yepes, P . Koehn, and J. Kreutzer, Eds., 2017,\npp. 169–214.\n[41] O. Bojar, C. Federmann, M. Fishel, Y. Graham, B. Haddow,\nP . Koehn, and C. Monz, “Findings of the 2018 conference on\nmachine translation (WMT18),” in Proceedings of the Third Conference\non Machine Translation: Shared Task Papers, WMT 2018 , O. Bojar,\nR. Chatterjee, C. Federmann, M. Fishel, Y. Graham, B. Haddow,\nM. Huck, A. Jimeno-Yepes, P . Koehn, C. Monz, M. Negri, A. N´ev´eol,\nM. L. Neves, M. Post, L. Specia, M. Turchi, and K. Verspoor, Eds.\n[42] L. Barrault, O. Bojar, M. R. Costa-juss `a, C. Federmann, M. Fishel,\nY. Graham, B. Haddow, M. Huck, P . Koehn, S. Malmasi, C. Monz,\nM. M ¨uller, S. Pal, M. Post, and M. Zampieri, “Findings of the\n2019 conference on machine translation (WMT19),” in Proceedings\nof the Fourth Conference on Machine Translation, WMT , O. Bojar,\nR. Chatterjee, C. Federmann, M. Fishel, Y. Graham, B. Haddow,\nM. Huck, A. Jimeno-Yepes, P . Koehn, A. Martins, C. Monz, M. Negri,\nA. N´ev´eol, M. L. Neves, M. Post, M. Turchi, and K. Verspoor, Eds.\nAssociation for Computational Linguistics, 2019, pp. 1–61.\n[43] Y. Qi, D. S. Sachan, M. Felix, S. Padmanabhan, and G. Neubig,\n“When and why are pre-trained word embeddings useful for neural\nmachine translation?” in Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT, M. A. Walker, H. Ji, and\nA. Stent, Eds. Association for Computational Linguistics, 2018,\npp. 529–535.\n[44] N. Goyal, C. Gao, V . Chaudhary, P . Chen, G. Wenzek, D. Ju,\nS. Krishnan, M. Ranzato, F. Guzm´an, and A. Fan, “The FLORES-101\nevaluation benchmark for low-resource and multilingual machine\ntranslation,” CoRR, vol. abs/2106.03193, 2021.\n[45] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly optimized\nBERT pretraining approach,” CoRR, vol. abs/1907.11692, 2019.\n[46] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,\n“GLUE: A multi-task benchmark and analysis platform for natural\nlanguage understanding,” in BlackboxNLP, 2018, pp. 353–355.\n[47] OpenAI, “GPT-4 technical report,” CoRR, vol. abs/2303.08774, 2023.\n[48] H. Touvron, L. Martin, K. Stone, P . Albert, A. Almahairi, Y. Babaei,\nN. Bashlykov, S. Batra, P . Bhargava, S. Bhosale, D. Bikel, L. Blecher,\nC. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu,\nand et al., “Llama 2: open foundation and fine-tuned chat models,”\nCoRR, vol. abs/2307.09288, 2023.\n[49] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi, “WinoGrande:\nAn adversarial winograd schema challenge at scale,” in AAAI, 2020,\npp. 8732–8740.\n[50] H. J. Levesque, E. Davis, and L. Morgenstern, “The winograd\nschema challenge,” in Principles of Knowledge Representation and\nReasoning, 2012.\n[51] N. Mostafazadeh, M. Roth, A. Louis, N. Chambers, and J. Allen,\n“Lsdsem 2017 shared task: The story cloze test,” in Proceedings of the\n2nd Workshop on Linking Models of Lexical, Sentential and Discourse-\nlevel Semantics, 2017, pp. 46–51.\n[52] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “Hel-\nlaSwag: Can a machine really finish your sentence?” in ACL, 2019,\npp. 4791–4800.\n[53] H. Bao, L. Dong, S. Piao, and F. Wei, “BEiT: BERT pre-training\nof image transformers,” in International Conference on Learning\nRepresentations, 2022.\n[54] Z. Peng, L. Dong, H. Bao, Q. Ye, and F. Wei, “BEiT v2: Masked\nimage modeling with vector-quantized visual tokenizers,” ArXiv,\nvol. abs/2208.06366, 2022.\n[55] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3386927\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n14\nL. Fei-Fei, “Imagenet large scale visual recognition challenge,” IJCV,\n2015.\n[56] B. Zhou, H. Zhao, X. Puig, T. Xiao, S. Fidler, A. Barriuso, and\nA. Torralba, “Semantic understanding of scenes through the\nADE20K dataset,” Int. J. Comput. Vis., vol. 127, no. 3, pp. 302–321,\n2019.\n[57] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song,\n“Natural adversarial examples,” in IEEE CVPR, 2021.\n[58] D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo,\nR. Desai, T. Zhu, S. Parajuli, M. Guo, D. Song, J. Steinhardt, and\nJ. Gilmer, “The many faces of robustness: A critical analysis of\nout-of-distribution generalization,” in IEEE ICCV, 2021.\n[59] H. Wang, S. Ge, Z. Lipton, and E. P . Xing, “Learning robust global\nrepresentations by penalizing local predictive power,” in Advances\nin Neural Information Processing Systems, 2019, pp. 10 506–10 518.\n[60] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun, “Unified perceptual\nparsing for scene understanding,” in ECCV, 2018.\n[61] S. Ma, L. Dong, S. Huang, D. Zhang, A. Muzio, S. Singhal, H. H.\nAwadalla, X. Song, and F. Wei, “DeltaLM: Encoder-decoder pre-\ntraining for language generation and translation by augmenting\npretrained multilingual encoders,” CoRR, vol. abs/2106.13736, 2021.\n[62] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo,\nand S. Kumar, “Transformer transducer: A streamable speech\nrecognition model with transformer encoders and rnn-t loss,” in\nICASSP 2020-2020 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP). IEEE, 2020, pp. 7829–7833.\n[63] W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal,\nO. Mohammed, S. Singhal, S. Som, and F. Wei, “Image as a foreign\nlanguage: BEiT pretraining for all vision and vision-language tasks,”\nArXiv, vol. abs/2208.10442, 2022.\n[64] L. Liu, H. Jiang, P . He, W. Chen, X. Liu, J. Gao, and J. Han, “On the\nvariance of the adaptive learning rate and beyond,” in ICLR, 2020.\n[65] M. Ding, Z. Yang, W. Hong, W. Zheng, C. Zhou, D. Yin, J. Lin,\nX. Zou, Z. Shao, H. Yang, and J. Tang, “Cogview: Mastering\ntext-to-image generation via transformers,” in Advances in Neural\nInformation Processing Systems 34: Annual Conference on Neural\nInformation Processing Systems 2021, NeurIPS 2021, December 6-14,\n2021, virtual, M. Ranzato, A. Beygelzimer, Y. N. Dauphin, P . Liang,\nand J. W. Vaughan, Eds., 2021, pp. 19 822–19 835.\n[66] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ron-\nneberger, K. Tunyasuvunakool, R. Bates, A. ˇZ´ıdek, A. Potapenko,\nA. Bridgland, C. Meyer, S. A. A. Kohl, A. J. Ballard, A. Cowie,\nB. Romera-Paredes, S. Nikolov, R. Jain, J. Adler, T. Back, S. Petersen,\nD. Reiman, E. Clancy, M. Zielinski, M. Steinegger, M. Pacholska,\nT. Berghammer, S. Bodenstein, D. Silver, O. Vinyals, A. W. Senior,\nK. Kavukcuoglu, P . Kohli, and D. Hassabis, “Highly accurate\nprotein structure prediction with AlphaFold,” Nature, vol. 596,\nno. 7873, pp. 583–589, 2021.\n[67] R. Karakida, S. Akaho, and S. Amari, “Universal statistics of fisher\ninformation in deep neural networks: Mean field approach,” in The\n22nd International Conference on Artificial Intelligence and Statistics,\nAISTATS 2019, K. Chaudhuri and M. Sugiyama, Eds.\nHongyu Wang received the B.E. degree from\nthe School of Computer Science and Technology,\nUniversity of Science and Technology of China,\nHefei, China, in 2022. He is currently working\ntoward the Ph. D. degree in School of Computer\nand Control Engineering, University of Chinese\nAcademy of Sciences, Beijing, China. His re-\nsearch interests include deep learning, natural\nlanguage processing and computer vision.\nShuming Mais a senior researcher at Microsoft\nResearch Asia. Before joining MSRA in 2019,\nhe received his Master’s and Bachelor’s degrees\nfrom Peking University, with a focus on natural\nlanguage processing. His research interests are\nlarge-scale language model pre-training and mul-\ntilingual NLP . He has published 30+ papers at\ntop-tier conferences (e.g. ICML, ACL, EMNLP ,\nNAACL).\nLi Dongis a Principal Researcher at Microsoft\nResearch Asia, working on multimodal learning,\nand human language technology. He received\nhis PhD in School of Informatics at University of\nEdinburgh in 2019.\nShaohan Huang received the B.S. and M.S.\ndegrees from Beihang University, Beijing, China\nin 2014 and 2017, respectively. He is currently a\nsenior researcher at Microsoft Research Asia,\nBeijing, China. His current research interests\ninclude deep learning, and natural language\nprocessing.\nDongdong Zhangis a principal researcher at Mi-\ncrosoft Research Asia. His research interests are\nneural machine translation, large-scale language\nmodel pre-training, multilingual generation, etc.\nFuru Wei received the B.E. and Ph.D. de-\ngree from the Department of Computer Science,\nWuhan University, Wuhan, China, in 2004 and\n2009 respectively. He is currently a Partner Re-\nsearch Manager at Microsoft Research Asia,\nBeijing, China, where he is leading the Natural\nLanguage Processing group and overseeing the\nteam’s research on Foundation Models (across\ntasks, languages and modalities) and AGI, NLP ,\nMT, Speech and Multimodal AI.\nThis article has been accepted for publication in IEEE Transactions on Pattern Analysis and Machine Intelligence. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2024.3386927\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6436646580696106
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5792323350906372
    },
    {
      "name": "Initialization",
      "score": 0.5786069631576538
    },
    {
      "name": "Transformer",
      "score": 0.5732437968254089
    },
    {
      "name": "Normalization (sociology)",
      "score": 0.5634434223175049
    },
    {
      "name": "Scaling",
      "score": 0.5169355273246765
    },
    {
      "name": "Residual",
      "score": 0.4184033274650574
    },
    {
      "name": "Algorithm",
      "score": 0.4171207547187805
    },
    {
      "name": "Approx",
      "score": 0.412531316280365
    },
    {
      "name": "Mathematics",
      "score": 0.18054473400115967
    },
    {
      "name": "Programming language",
      "score": 0.17344853281974792
    },
    {
      "name": "Electrical engineering",
      "score": 0.10920649766921997
    },
    {
      "name": "Voltage",
      "score": 0.08376586437225342
    },
    {
      "name": "Engineering",
      "score": 0.07400268316268921
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Anthropology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210113369",
      "name": "Microsoft Research Asia (China)",
      "country": "CN"
    }
  ]
}