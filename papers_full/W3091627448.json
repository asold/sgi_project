{
  "title": "MEGATRON-CNTRL: Controllable Story Generation with External Knowledge Using Large-Scale Language Models",
  "url": "https://openalex.org/W3091627448",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2098197978",
      "name": "Xu Peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221878990",
      "name": "Patwary, Mostofa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221882394",
      "name": "Shoeybi, Mohammad",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227412475",
      "name": "Puri, Raul",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743188307",
      "name": "Fung, Pascale",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202058456",
      "name": "Anandkumar, Anima",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2988481184",
      "name": "Catanzaro, Bryan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970574558",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2914204778",
    "https://openalex.org/W2997195635",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W2889009749",
    "https://openalex.org/W3014521650",
    "https://openalex.org/W2996739759",
    "https://openalex.org/W2951048068",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2133286915",
    "https://openalex.org/W3117704329",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2250770256",
    "https://openalex.org/W2963825865",
    "https://openalex.org/W2889002152",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W2914855263",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2966584111",
    "https://openalex.org/W2888213795",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2971008823",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W2962753250",
    "https://openalex.org/W2964669873",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W3034614576",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W2517226069",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3023786569"
  ],
  "abstract": "Existing pre-trained large language models have shown unparalleled generative capabilities. However, they are not controllable. In this paper, we propose MEGATRON-CNTRL, a novel framework that uses large-scale language models and adds control to text generation by incorporating an external knowledge base. Our framework consists of a keyword predictor, a knowledge retriever, a contextual knowledge ranker, and a conditional text generator. As we do not have access to ground-truth supervision for the knowledge ranker, we make use of weak supervision from sentence embedding. The empirical results show that our model generates more fluent, consistent, and coherent stories with less repetition and higher diversity compared to prior work on the ROC story dataset. We showcase the controllability of our model by replacing the keywords used to generate stories and re-running the generation process. Human evaluation results show that 77.5% of these stories are successfully controlled by the new keywords. Furthermore, by scaling our model from 124 million to 8.3 billion parameters we demonstrate that larger models improve both the quality of generation (from 74.5% to 93.0% for consistency) and controllability (from 77.5% to 91.5%).",
  "full_text": "MEGATRON -CNTRL : Controllable Story Generation with External\nKnowledge Using Large-Scale Language Models\nPeng Xu‡∗, Mostofa Patwary§, Mohammad Shoeybi§, Raul Puri§,\nPascale Fung‡, Anima Anandkumar§, Bryan Catanzaro§\n‡The Hong Kong University of Science and Technology,§NVIDIA\npeng.xu@connect.ust.hk, mpatwary@nvidia.com\nAbstract\nExisting pre-trained large language models\nhave shown unparalleled generative capabili-\nties. However, they are not controllable. In\nthis paper, we propose M EGATRON -CNTRL ,\na novel framework that uses large-scale lan-\nguage models and adds control to text gener-\nation by incorporating an external knowledge\nbase. Our framework consists of a keyword\npredictor, a knowledge retriever, a contextual\nknowledge ranker, and a conditional text gen-\nerator. As we do not have access to ground-\ntruth supervision for the knowledge ranker, we\nmake use of weak supervision from sentence\nembedding. The empirical results show that\nour model generates more ﬂuent, consistent,\nand coherent stories with less repetition and\nhigher diversity compared to prior work on the\nROC story dataset. We showcase the controlla-\nbility of our model by replacing the keywords\nused to generate stories and re-running the\ngeneration process. Human evaluation results\nshow that 77.5% of these stories are success-\nfully controlled by the new keywords. Further-\nmore, by scaling our model from 124 million\nto 8.3 billion parameters we demonstrate that\nlarger models improve both the quality of gen-\neration (from 74.5% to 93.0% for consistency)\nand controllability (from 77.5% to 91.5%).\n1 Introduction\nText generation has recently attracted signiﬁcant\nattention from the research community as large pre-\ntrained language models, such as GPT-2 (Radford\net al., 2018, 2019) demonstrated promising results\nfor generating long, grammatically correct, and ﬂu-\nent text. Finetuning these models has shown signif-\nicant improvements in downstream tasks, such as\npersona chat (Wolf et al., 2019). However, one non-\nnegligible drawback of these large models is the\n∗ This work was done during the internship of Peng Xu at\nNVIDIA. Corresponding authors: Peng Xu, Mostofa Patwary.\nContext [FEMALE] was on a long road trip .\nGuan et al. (2020)she had just driven into town . she had been\ndriving for hours . she was so tired she decided\nto take a nap . when she woke up , she still\nhad n’t slept well .\nMEGATRON-\nCNTRL-124M\nkeyword: driving\nshe was on her way to see a strange country.\nshe had never seen anything like that before.\nﬁnally she saw a small town that was so small.\nit was a great little town with a nice park !\nMEGATRON-\nCNTRL-8B\nkeyword: driving\nshe was driving in the middle of [PLACE]. all\nof a sudden the tire pressure light came on. she\ngot out to check her tire. it was ﬂat so she used\nthe roadside assistance.\nMEGATRON-\nCNTRL-8B-ANT\nkeyword: attract\nshe really wanted to see a few attractions. the\nﬁrst one she saw was a giant water park. it was\namazing. it ended up being a fun experience.\nTable 1: Stories generated by models of increas-\ning capacity and controllability. As the model size\ngrows, story quality becomes increasingly coherent,\nﬂuent, and logically consistent. The last row demon-\nstrates how M EGATRON -CNTRL -8B-ANT model con-\ntrols the story generation with a new keyword, “attract”.\nNote that [MALE] and [FEMALE] denote names and\n[PLACE] denotes locations.\nlack of knowledge which humans use to produce\nnatural text. For example, GPT-2 based models\nproduce degraded generations that are illogical and\nungrammatical for knowledge-driven generation\ntasks, such as story generation. Guan et al. (2020)\ntherefore introduced commonsense knowledge to\nthe pre-trained language model by further ﬁnetun-\ning on commonsense datasets. Although implicit\nencoding of knowledge is helpful for knowledge\nincorporation, there is still a lack of training mech-\nanism to teach the model when and what to incor-\nporate from external knowledge.\nIn addition, these large pre-trained language\nmodels are hard to control. Recently, plug-and-play\nlanguage models Dathathri et al. (2019) addressed\nwhole document controllability by adding a lin-\near classiﬁer on top of GPT-2 to predict whether\ngenerated text observes a particular style or prop-\nerty. Keskar et al. (2019) controlled a 1.2B pa-\nrameter language model generation via the use of\narXiv:2010.00840v1  [cs.CL]  2 Oct 2020\n.H\\ZRUGV\u0003\n3UHGLFWRU\n.QRZOHGJH\u0003%DVH\n,QSXW\u0003&RQWH[W\u001d\u0003\n%(57\n&RQWH[WXDO\u0003\n.QRZOHGJH\u0003\n5DQNHU\n*37\u0010\u0015\n.QRZOHGJH\u0003\n5HWULHYHU3UHGLFWHG\u0003.H\\ZRUGV\n&RQGLWLRQDO\u0003\n*HQHUDWRU\n*37\u0010\u0015\n,QSXW\u0003&RQWH[W\u001d\u0003 \u0003\n1HZ\u0003,QSXW\u0003&RQWH[W\u001d\u0003\n.QRZOHGJH\u00036HQWHQFHV\n7RS\u00035DQNHG\u0003.QRZOHGJH\u0003\n6HQWHQFHV\u0003\u0003\u0003\n,QSXW\u0003&RQWH[W\u001d\u0003\n*HQHUDWHG\u00036WRU\\\n6HQWHQFH\n([WHUQDO\u0003&RQWURO([WHUQDO\u0003.H\\ZRUGV\u0003\n\u000b2SWLRQDO\f\nRYHUZULWH\nRYHUZULWH\nFigure 1: Overview of our generation process. Based on an input context, we generate keywords for future context,\nuse the keywords to retrieve the relevant knowledge from an external knowledge-base, ﬁlter them based on their\nrelevance to the context, and use the top scored knowledge sentences to guide the generation.\ncontrol codes prepended to the model input. Boyd\net al. (2020) controlled the personality of a dialogue\nagent by conditioning it on prior conversations of a\ntarget actor. However, these controlling conditions\nare predeﬁned, limited in their capability, and are\nonly used once at the beginning to condition the\ngeneration of the rest of the document. They do\nnot provide control granularity at either a sentence\nor sub-document level.\nIn this work, we address these shortcomings and\ndevelop an efﬁcient controllable text generation\nframework that we apply to the story generation\ntask. In order to provide manual control to users\nthrough a set of interpretable keywords, we ﬁrst\ndevelop a keyword predictor model for the next\nsentence. These keywords are then used to retrieve\nknowledge sentences from an external knowledge\nbase. Not all the retrieved knowledge is relevant to\nthe story context and often it is noisy. To this end,\nwe introduce a novel contextual ranker that ranks\nknowledge sentences based on the relevance to the\ncontext. As we do not have access to ground-truth\nsupervision for this contextual knowledge ranker,\nwe make use of sentence embedding for weak su-\npervision. The top-ranked knowledge sentences\nfrom the knowledge ranker are then fed to the con-\nditional text generator to guide generation. By\ngiving the knowledge in addition to the context, we\nprovide rich information for the generator to attend\nto and help the model better understand the ratio-\nnale between sentences. Table 1 shows an example\nof controllable story generation with increasing\nmodel capacity.\nSummary of Contributions:\n• We propose a novel generation framework that al-\nlows dynamical incorporation of external knowl-\nedge into language model as well as control for\ntext generation.\n• Using both automatic metrics as well as human\nevaluations, we demonstrate that our model gen-\nerates more ﬂuent, consistent, and coherent sto-\nries with lower repetition rate and higher diversi-\nties compared to the previous state-of-the-art on\nROC story datasets (Mostafazadeh et al., 2016).\n• We showcase the controllability of our model\nby replacing the keywords used to generate sto-\nries. Human evaluation results show that up to\n91.5% of the generated stories are successfully\ncontrolled by the new keywords .\n• We scale our model from 124 million to 8.3 bil-\nlion parameters and demonstrate that both quali-\nties, as well as controllability of the generations,\nimprove as the model size increases.\n2 Framework\nIn our problem setup, we complete a story using\nthe ﬁrst sentence as input, similar to Guan et al.\n(2020). We augment the generation process with an\nexternal knowledge-base and develop a methodol-\nogy that can guide and control the story generation.\nOur approach consists of the following four steps\nconnected together as shown in Figure 1:\n1. Given the story context, a keyword predictor\nmodel ﬁrst predicts a set of keywords for the\nnext sentence yet to be generated.\n2. A knowledge retriever then takes the gen-\nerated keywords and queries an external\nknowledge-base where each knowledge triple\nis converted into natural language “knowledge\nsentences” using templates.\n3. A contextual knowledge ranker then ranks the\nexternal knowledge sentences based on their\nrelevance to the story context.\n4. Finally, a generator takes both the story con-\ntext as well as the top-ranked knowledge sen-\ntences as input and generates the next sentence\nin the story. The output sentence is appended\nto the story context and steps 1-4 are repeated.\nThis formulation naturally allows controllability by\nreplacing the keyword prediction process with man-\nual external keywords. This work uses dynamic\nplanning of the keywords and knowledge at each\ngeneration step. This allows the users to participate\nand control the generation on the go. As a result,\nthey don’t need to pre-specify the keywords explic-\nitly. We also note that it is challenging to statically\nplan all the knowledge needed for generation at\nthe beginning. This issue becomes severe for long\ngenerations. To formalize this method, we start by\nintroducing notation used throughout the paper and\nthen detail each aforementioned four steps in the\nfollowing subsections.\nNotation: A knowledge-base, G is de-\nﬁned as a set of knowledge triples t =\n(subject, relation, object). A knowledge sentence,\nr is deﬁned as r = T(t) by mapping t using prede-\nﬁned templates T. For example, (eiffel tower, At-\nLocation, paris) is transformed into eiffel tower is\nat paris. We should highlight that since our frame-\nwork transforms the triple knowledge database into\nnatural language sentences, any knowledge base in\nnatural language format can be readily incorporated\ninto our framework. We use superscripts to index\nstory sentences and deﬁne a story S of length l as\na sequence of individual story sentences si where\nS = {s1, s2, ··· , sl}. We use Ki = {ki\n1, ··· , ki\nq}\nto denote the keywords associated with story sen-\ntence si. A keyword ki\nq is made up of subword to-\nkens from our language model’s vocabulary. Note\nthat the number of keywords q per sentence varies\nand can be zero. We deﬁne Ri = {ri\n1, ··· , ri\nv}\nas the knowledge associated with si, where ri\nj de-\nnotes the j-th knowledge sentence associated si.\nThe number of knowledge sentences v varies per\nsentence and can be zero. Note that v ̸= q because\na keyword can have multiple knowledge triples\nassociated with it. Given this notation, we de-\nﬁne the story context Xi = {x1, ··· , xi}where\nxi = [Ri, si]. The goal of this work is to generate\nxi given Xi−1, that is to ﬁrst predict the knowledge\nRi contained in si and then predict si itself.\n2.1 Keyword Predictor Model\nTo provide manual control to users, we ﬁrst develop\na keyword predictor model. Given the current story\ncontext Xi−1, the model predicts a set of keywords\nKi for the next sentence yet to be generated. The\nprediction of keywords instead of directly predict-\ning knowledge triples not only allows us to control\nthe generation in an interpretable manner, but it\nalso helps to greatly reduce the search space for the\nknowledge triples. We formulate this keyword pre-\ndiction problem similar to a left-to-right language\nmodel where the goal is to predict the string of\nconcatenated keywords:\np(Ki|Xi−1) =\nq∏\nj=1\np(ki\nj|Xi−1, Ki\n<j), (1)\nwhere K<j denotes all the predicted keywords up\nto the jth keyword and p is the probability distri-\nbution. We use a GPT-2 (Radford et al., 2019)\ntransformer to model this probability distribution.\nWe optimize the keyword predictor with maximum\nlikelihood training and a next token prediction loss.\nFollowing Yao et al. (2019), we provide the la-\nbels for Ki by extracting keywords from a ground\ntruth training sentencesi using the RAKE algorithm\n(Rose et al., 2010) to train our keyword predictor.\nNote that our model allows generation of multi-\nple keywords and thus provides the ﬂexibility to\nchoose a subset of them as the control signal to ﬁt\nin the generation.\n2.2 Knowledge Retrieval\nIn this step, we use the generated keywords Ki in\nSection 2.1 and retrieve all the related knowledge\ntriples from our knowledge base G. This is sim-\nply done by converting all knowledge triples into\nknowledge sentences using predeﬁned templates\nand then matching keywords against the knowl-\nedge sentences. This results in the knowledge set\nˆRi = {ˆri\n1, ··· , ˆri\nz}with size z. Future work will\nfocus on replacing this simple retrieval with a learn-\nable module similar to Guu et al. (2020).\nAlgorithm 1Building Pseudo Label of Ri\nInput: Story sentence si and its preceding sentence\nsi−1, U SE encoder U, R AKE keywords extractor, and\nknowledge base G\nOutput: Pseudo Label of Ri\n1: Extract keywords Ki from si using RAKE\n2: Find ¯R = {T(t)|t ∈G and ∃ki\nj ∈Ki, s.t. ki\nj ∈t}\n3: Encode each ¯rj ∈¯R to Ur\nj using USE\n4: Encode [si−1, si] to Us\n5: Compute cosine similarityscore between each Ur\nj\nand Us\n6: return ¯rjs with the top N highest score\n2.3 Building Pseudo Label ofRi\nThe main challenge for controlling generation with\nknowledge is that we have no explicit access to the\nhidden, latent controlling knowledge humans use\nto supervise their story writing. That means Ri,\nthe knowledge associated with si is not available.\nWe, therefore, propose to use a weakly supervised\nsignal to build the pseudo labels of Ri from si.\nWe hypothesize that Ri should 1) overlap with si\nin terms of keywords and 2) have strong connec-\ntions to both the preceding sentence si−1 and si.\nWe include si−1 along with si because it is hard\nto retrieve appropriate knowledge using only si\ndue to the ambiguity of natural language. We also\ndid not include other previous context beyond si−1\nas additional context overwhelms the information\ncontained in si.\nFollowing our hypothesis, we ﬁrst extract key-\nwords Ki from si using RAKE (Rose et al., 2010)\nand then match Ki with all knowledge triples in\nG. Transforming the retrieved triples into knowl-\nedge sentences gives us our set of ¯Ri. We then take\nthe sentence si and si−1, concatenate them, and\nencode them using the Universal Sentence Encoder\n(USE) (Cer et al., 2018), a widely-used toolkit for\nsemantic similarity, Us = U([si−1, si]), where we\ndenote the encoder of USE as U. For each ¯ri\nj ∈¯Ri,\nwe then calculate the cosine similarity between Us\nand Ur\nj = U(¯rj) and sort ¯Ri based on this score.\nWe take the top N highest scores ¯ri\nj as a pseudo\nlabel of Ri. Algorithm 1 describes this process.\nDuring the training phase of each following model,\nwe use this pseudo label of Ri to represent Ri.\n2.4 Contextual Knowledge Ranker\nWhile knowledge retrieval with keywords reduces\nthe controlling knowledge candidate space from\nthe knowledge base G to the subset ˆRi, this set\nis still large and noisy since words are ambiguous\nand can have multiple senses. We, therefore, con-\ntextualize the knowledge sentences in ˆRi to obtain\nrelevant and useful ones under Xi−1. To do this,\nwe develop a contextual knowledge ranker. The\nmodel is trained with pseudo-labels extracted with\naccess to the future sentence si as described in Sec.\n2.3.\nWe use a BERT model to encode both the context\nXi−1 and each knowledge sentence ˆri\nj ∈ ˆRi. To\nadapt to the format of BERT, we append a [SEP]\ntoken to each Rj and sj inside the context Xi−1.\nA [CLS] token is then added to the beginning of\nXi−1. For segment ids, we mark the tokens from\nthe knowledge base as 0 and those from the story\nas 1. The representation of Xi−1 and ˆri\nj are then\nobtained after applying a linear layer on top of the\nembedding of the [CLS] token:\nVx = W1 BERTCLS(Xi−1),\nVj = W2 BERTCLS(ˆri\nj),\nwhere W1 and W2 are learnable weights. We then\ncalculate the relevance score C between Xi−1 and\nˆri\nj using the inner product between Vx and Vj as :\nCi\nj = C(Xi−1, ˆri\nj) = VxVj (2)\nWe take Ri (Sec. 2.3) as positive samples and\nˆRi\\Ri as negative samples to train our ranker.\nGiven a positive and a negative knowledge sen-\ntence rp and rn, we deﬁne the ranking loss as\nL = max{0, M−C(Xi−1, rp) +C(Xi−1, rn)} (3)\nwhere M is a margin and determined empirically.\nAlgorithms 2 describe the ranker training process.\nAt inference time, we simply calculate Cj for all\nˆri\nj ∈ˆRi, sort them based on Ci\nj score and pick the\ntop N most relevant knowledge sentences as Ri.\n2.5 Conditional Generator\nThe conditional generator is a language model that\nincorporates the controlling knowledge and gener-\nates the following sentences. It concatenates the\nstory context Xi−1 and controlling knowledge Ri\nas input and generates si. A GPT-2 transformer is\nused to model this conditional probability distribu-\ntion. We describe the concatenated input represen-\ntation in the Appendix A.5.\nAlgorithm 2Knowledge Ranker Training\nParameters: BERT model parameters Θ and ranker\nmodel parameters W1 and W2\nInput: A story Sl with l sentences and a knowledge\nbase G\n1: Initialize Θ using a pre-trained B ERT model and\nW1, W2 randomly.\n2: Dataset D = ∅\n3: Call Algorithm 1 to retrieve R1 from G using s1.\n4: for i ∈{2, . . . , l} do\n5: Call Algorithm 1 to retrieve Ri using si.\n6: Get ˆRi using knowledge retrieval (Section 2.2)\n7: for j ∈{1, . . . , N} do\n8: Sample rp from Ri and rn from ˆRi\\Ri\n9: D = D ∪(Xi−1, rp, rn)\n10: end for\n11: end for\n12: for (X, rp, rn) ∈D do\n13: Calculate loss L using Equation 3\n14: Optimize B ERT, W1, W2\n15: end for\n16: return BERT, W1, W2\n3 Experimental Setup\n3.1 Datasets\nWe use the ROC story dataset (Mostafazadeh\net al., 2016) for our experiments. It consists of\n98,161 stories, where each story contains ﬁve sen-\ntences. 88,344/4,908/4,909 stories are used for\ntrain/validation/test sets, respectively. Following\nGuan et al. (2020), for each sentence, delexicaliza-\ntion is performed by replacing all the names and en-\ntities in stories with special placeholders, [MALE],\n[FEMALE], and [NEUTRAL] for male, female and\nunknown names and entities, respectively. Given\nthe ﬁrst sentence of each story, our model’s task is\nto generate the rest of the story. For our external\nknowledge base, we use ConceptNet (Speer and\nHavasi, 2012), consists of 600k knowledge triples.\n3.2 Models\nWe used Megatron-LM (Shoeybi et al., 2019) for\npre-trained BERT and GPT-2 models to initialize\nour contextual knowledge ranker and generative\nmodels, respectively. For the model conﬁgurations,\nhidden size, number of layers, and attention heads,\nwe used the conﬁgurations of BERT and GPT-2 as\nin Megatron-LM. For generation with our GPT-2\nmodels, we used a top- k sampling scheme (Fan\net al., 2018) with k = 40 and a softmax tempera-\nture of 0.7. We detail the training hyperparameters\nand the input representations for GPT-2 and BERT\nin Appendix A.1 & A.2 . Both the keyword predic-\ntor and the conditional sentence generator follow\nthe same settings.\nTo train our contextual knowledge ranker, we set\nthe margin to 5.0. We set the number of knowledge\nsentences in Ri to 10. Therefore, for a given story\ncontext, the top 10 retrieved knowledge sentences\nfrom ConceptNet according to USE are chosen as\nthe positive samples. We further select 40 nega-\ntive samples to compute our margin loss. We then\nrandomly sample 50 (positive, negative) pairs for\neach story context to train our contextual knowl-\nedge ranker. In total, we used ∼15 million pairs\nfor training and ∼1 million pairs for validation.\nAfter training our ranker, we achieve a validation\naccuracy of 0.9.\n3.3 Controllability Experiment Setup\nTo test the controllability of our model, we perform\nexperiments where we change keywords to their\nantonyms. With antonyms, we expect maximal\nchange to the story generation. To do that, we ﬁrst\nuse MEGATRON -CNTRL -124 M to generate key-\nwords K and corresponding full story S. Then we\nidentify the ﬁrst keyword ki\na ∈Ki from K whose\nantonym is available at WordNet (Miller, 1995). If\nmultiple antonyms for ki\na are available we sample\none with a uniform random probability. Afterwards,\nwe provide the start of story {s1, s2, ··· , si−1},\nthe keywords shared with our original story\n{K1, K2, ··· , Ki−1}, and the antonym of ki\na to\neither MEGATRON -CNTRL -124 M or larger mod-\nels (e.g. MEGATRON -CNTRL -355 M). We then\nlet the model ﬁnish the generation. We refer\nto these generations as MEGATRON -CNTRL -ANT,\nfor example, we call the antonym generations from\nMEGATRON -CNTRL -355 M model as MEGATRON -\nCNTRL -355 M-ANT.\n3.4 Baselines\nWe compare our model with the following state-\nof-the-art story generation models. (1) Plan and\nwrite (Yao et al., 2019): The authors use an\nLSTM-based model to ﬁrst generate a sequence\nof keywords for planning the story. These key-\nwords are then used to condition the generation.\n(2) Knowledge enhanced GPT-2 (Guan et al.,\n2020): This work is currently the SOTA for ROC\nstory generation. It ﬁnetunes a pre-trained GPT-2\nmodel with knowledge triples from commonsense\ndatasets. Similar to our method, the knowledge\ntriples are converted to sentences with templates.\nA multitask learning framework is then developed\nto further ﬁnetune the story generation task and\nclassify corrupted stories from real ones. We do\nnot compare to Fan et al. (2019) because Guan et al.\n(2020) has already shown their model signiﬁcantly\noutperforms Fan et al. (2019) and in this work, we\ncompare to Guan et al. (2020). (3) GPT-2-124M:\nThis baseline ﬁnetunes a GPT-2 model with a next\ntoken prediction loss on the story.\n3.5 Evaluation\nWe conduct both automatic as well as human eval-\nuations to assess our generation.\n3.5.1 Automatic Evaluation\nWe use the following metrics to compare differ-\nent models: Repeat: measures the redundancy of\nthe generated story by reporting the percentage\nof the stories that contain at least one repeated 4-\ngram (Shao et al., 2019). Distinct: measures the\ndiversity of generated stories by reporting the ratio\nbetween distinct 4-grams to all generated 4-grams.\nPerplexity: In the inference phase, our models in-\nvolve two steps of generation: (i) generate set of\nknowledge sentences, Ri from story context Xi−1,\n(ii) generate story sentence, si from Xi−1 and Ri.\nTo report the perplexity of the conditional generator\nwe sample Ri sequentially before generating each\nstory sentence si and report the total perplexity of\nall sentences si for i ∈[2, l] where l is the number\nof sentences in the story.\n3.5.2 Human Evaluation on Quality\nWe conduct human evaluations on Amazon Me-\nchanical Turk1 (AMT) to analyze the quality of\nour generations on three aspects: Fluency, Co-\nherence, and Consistency. To evaluate ﬂuency,\nwe show the annotators a pair of generated sto-\nries from two models. We ask them to evaluate\neach sentence independently and choose the story\nwith better overall ﬂuency. Fluency of a story is\ndeﬁned as a measure of intra-sentence linguistic\nquality and grammatical correctness taken over all\nsentences of the story. For coherence, we provide\nthe same stories as in ﬂuency but ask to choose the\none with better inter-sentence causal and temporal\ndependencies. We let the annotators choose tie for\nboth ﬂuency and coherence.\nDifferent from the settings of ﬂuency and coher-\nence, we only show one generated story to anno-\ntators to evaluate consistency. They are required\nto choose whether the story is logically consistent,\nbased on whether the story self contradicts or not.\n1https://www.mturk.com/\nWe set up these three evaluations as independent\nAMT tasks to make sure the tasks do not inﬂu-\nence each other and introduce spurious correlations\nbetween labels. To reduce noise in our labeling pro-\ncess, we only accepted workers with an approval\nrating over 90% and have over 1k accepted jobs.\nWe further limited the location of the annotators to\nthe United States. For each example, we explicitly\nask them to spend at least 15 seconds to evaluate\ncoherency and 10 seconds to evaluate the other two\nproperties. In total, we randomly sample 200 sto-\nries and assign ﬁve annotators for each story. We\nadopted majority voting to make ﬁnal decisions\namong the ﬁve annotators.\n3.5.3 Human Evaluation on Controllability\nTo evaluate how controllable our model is, we con-\nduct another human evaluation just for controlla-\nbility. We show the annotators the start of a story,\noriginal keywords, and the corresponding genera-\ntion. We then show the antonyms of the keywords,\nalong with the corresponding generated story, and\nask the annotators if the new story has changed\ncompared to the original story in accordance with\nthe meaning of the keyword’s antonyms. The rest\nof the AMT settings for these experiments are the\nsame as our consistency experiments.\n4 Results\nIn this section, we ﬁrst perform automatic and hu-\nman evaluations with different model sizes and\ncompare our framework to the existing baselines.\nWe then evaluate the controllability of our model\nand ﬁnally show ablation study varying GPT-2 and\nBERT model sizes. The detailed conﬁguration of\nthe model sizes are shown in Table 2. We provide\nseveral generated stories in Appendix A.7 varying\nthe length of the given context. We use M-C NTRL\nto denote MEGATRON -CNTRL in the tables due to\nthe limited space.\nConditional Keyword Knowledge\nModel Name Generator Generator Ranker\n(GPT-2) (G PT-2) ( B ERT)\nM-CNTRL-124M 124M 124M 336M\nM-CNTRL-355M 355M 355M 336M\nM-CNTRL-774M 774M 774M 336M\nM-CNTRL-2B 2.5B 2.5B 336M\nM-CNTRL-8B 8.3B 2.5B 336M\nTable 2: Number of parameters of our models (M-\nCNTRL is the short form for MEGATRON -CNTRL ).\nSource A Coherence ↑ Fluency↑ Source B\nM-CNTRL-124M 78.5%- 13.0% 66.5%- 22.5% Yao et al. (2018)\nM-CNTRL-124M 46.0% - 39.0% 44.5% - 43.5% Guan et al. (2020)\nM-CNTRL-355M 56.0% - 30.5% 46.5% - 30.5% Guan et al. (2020)\nM-CNTRL-355M 52.0% - 31.5% 46.5% - 39.0% M-CNTRL-124M\nM-CNTRL-774M 44.5% - 41.5% 56.0% - 33.5% M-CNTRL-355M\nM-CNTRL-2B 50.5% - 30.5% 53.0% - 39.0% M-CNTRL-774M\nM-CNTRL-8B 46.0% - 39.5% 46.5% - 46.5% M-C NTRL-2B\nTable 3: Pairwise comparison between our models and baselines. Percentages in the format “A% - B%” indicate\nhow often annotators rank the samples from source A better than from source B for a given category, and vice\nversa. Percentage pairs do not sum to 100% as the annotators were allowed to choose “tie” as being of equal\nquality. M EGATRON -CNTRL -124 M achieves better results than all baselines. Scaling the models shows better\ncoherence and ﬂuency.\nName PPL ↓ Repeat↓ Distinct↑ Consistency↑\n(Human Eval)\nGPT-2-124M 6.98 27.2 74.1 69.5\nYao et al. (2018) NA 13.3 63.7 49.0\nGuan et al. (2020)7.04 22.1 77.1 67.0\nM-CNTRL-124M 9.37 20.0 80.1 74.5\nM-CNTRL-355M 8.02 19.9 81.6 75.5\nM-CNTRL-774M 6.58 21.3 81.6 80.5\nM-CNTRL-2B 6.31 21.2 82.6 89.0\nM-CNTRL-8B 6.21 21.2 82.8 93.0\nTable 4: Evaluation results for the previous state-of-\nthe-art models as well as our algorithm at different\nsizes. Perplexity, repeat, and distinct are evaluated auto-\nmatically whereas consistency is obtained using human\nevaluations. Our smallest model with 124M parame-\nters achieves better distinct and consistency score com-\npared to prior work. Increasing model size up to 8B\nimproves perplexity, distinct, and consistency scores.\nFor reference, the ground truth human writing gives 7.6\nscore for repeat and 88.9 for distinct.\n4.1 Automatic and Human Evaluations\nTable 4 shows that our smallest model,\nMEGATRON -CNTRL -124 M achieves better\ndistinct and consistency scores compared to\nprevious work. For repetition, our model is worse\nthan Yao et al. (2019) which was also observed in\nGuan et al. (2020). The reason could be their small\n8M model is better at learning short term statistics\n(e.g. 4-grams), while large models are better at\nlearning long term dependencies. Compared to\nother GPT-2 based models (i.e. GPT-2-124M and\nGuan et al. (2020)), MEGATRON -CNTRL -124 M\nachieves lower repeat and higher distinct scores,\nhence our model generates less repetitive stories.\nWe notice from Table 4 that our perplexity (PPL)\nscore is much higher than other GPT-2-based mod-\nels. Our hypothesis for why this occurs is that other\nGPT-2-based methods directly model and report\nP(si|s1, s2, ··· , si−1) while our conditional gen-\nerator models and reports P(si|Xi−1, Ri). When\ncomputing perplexity, [s1, s2, ··· , si−1] are given\nground truth tokens, but Ri and all R in Xi−1 must\nbe sampled from a distribution that is learned with\nweak supervision. This sampling introduces noise\nand non-determinism that results in higher perplex-\nity. This discrepancy is not an issue when analyzing\nautomatic evaluation metrics within our model fam-\nily. When scaling our model from 124M up to 8B\nparameters we see a consistent drop in PPL and\nincrease in distinct. This shows larger models can\ngenerate better stories with more diversity.\nHuman evaluation results are presented in last\ncolumn of Table 4 (consistency) and in Table\n3. Comparing MEGATRON -CNTRL -124 M to Yao\net al. (2019), we achieve much better coherence,\nﬂuency, and consistency scores, which shows the\nbeneﬁt of large pre-trained transformer models.\nComparing MEGATRON -CNTRL -124 M to Guan\net al. (2020) which uses a similar base model, we\nﬁnd that ﬂuency is similar, however we should note\nthat Guan et al. (2020) is not controllable and our\nmodel has signiﬁcantly better coherence (+7.0%)\nin Table 3 and consistency (+7.5%) in Table 4. We\nattribute this to the use of the retrieved knowledge,\nRi. By explicitly providing facts pertinent to the\nnext sentence, the conditional generative model\ncan focus on just generating text. By comparison,\na standard autoregressive GPT-2 model is tasked\nwith predicting both the topics and the text of the\nnext sentence.\nScaling this up, and comparing MEGATRON -\nCNTRL -355 M to Guan et al. (2020), our\nmodel signiﬁcantly outperforms in all aspects.\nFurthermore, a thorough comparison among\nMEGATRON -CNTRL -355 M, MEGATRON -CNTRL -\n774 M, MEGATRON -CNTRL -2B, MEGATRON -\nCNTRL -8B shows that scaling the model size fur-\nther almost always improves the quality of genera-\ntion in terms of ﬂuency, coherence and consistency.\nFor consistency, our best model at 8B parameters\nachieves a score of 93%.\n4.2 Controllability Evaluation\nWe evaluate the controllability by changing key-\nwords to their antonyms as detailed in Section\n3.3 & 3.5. Table 5 shows repeat and distinct\nfor MEGATRON -CNTRL -124 M as well as the con-\ntrolled versions at three different sizes. Altering\ncontrol with antonym keywords gives lower repeat\nand higher distinct scores than the original genera-\ntion. As the model size increases, the repeat stays\nalmost constant while distinct improves. These\nresults show that changing keywords manually re-\nsults in distinct and not repeated text.\nName Repeat ↓ Distinct↑\nM-CNTRL-124M 20.0 80.1\nM-CNTRL-124M-ANT 17.8 80.9\nM-CNTRL-355M-ANT 18.0 81.6\nM-CNTRL-8B-ANT 18.5 82.8\nTable 5: Comparing controllability of the models by\nchanging the keywords to their antonyms. Controlled\ngenerations show less repetition and higher diversity\ncompared to the original one.\nFurther supporting this hypothesis, evaluation of\ncontrollability in Table 6 shows that MEGATRON -\nCNTRL -124 M-ANT achieves a high controllabil-\nity score of 77.5%. This means that by changing\nthe keywords to their antonym, 77.5% of newly\ngenerated stories change their semantic content to\nfollow the new antonym keywords. We also show\nthat larger models are better able to leverage key-\nword control. Scaling the model size from 124M\nto 355M and 8B model further improves the con-\ntrollability score to 84.5% and 91.5%, respectively.\nWe again observe the quality (e.g. coherence) of\nour controlled generation improves as the model\nsize scales to 8B.\nName Controllability ↑\nM-CNTRL-124M-ANT 77.5%\nM-CNTRL-355M-ANT 84.5%\nM-CNTRL-8B-ANT 91.5%\nTable 6: Human evaluation for controllability by\nchanging keywords to their antonyms. Over 77% of\nour generation changes according to the keywords.\n4.3 Ablation Studies\nIn this section, we conduct the ablation study on\nthe planning strategy and external knowledge. The\nName Repeat ↓Distinct↑\nM-CNTRL-124M(D) 20.04 80.14\nM-CNTRL-124Mw/o knowledge (D) 23.59 79.39\nM-CNTRL-124M(S) 23.87 79.45\nM-CNTRL-124Mw/o knowledge (S) 23.98 79.61\nTable 7: Ablation studies of static (S) vs dynamic (D)\nplanning strategy, with and without knowledge.\nstudy of model size can be found in the Appendix\nA.3.\n4.3.1 Planning Strategy\nIn this section, we investigate the effects of plan-\nning strategy in our framework. Yao et al. (2019)\nshowed that static planning works better than dy-\nnamic planning for LSTM-based models. To intro-\nduce the static planning in our model, we predicted\nall the keywords and relevant knowledge sentences\nfrom the starting sentence and generated the en-\ntire stories. When we compare these generations\nwith the stories generated by dynamic planning, we\nsee in Table 7 (ﬁrst and third rows) that dynamic\nplanning outperforms the static planning strategy\nwith higher distinction (+0.7%) and lower repeti-\ntion (-3.8%) scores. This is due to direct guidance\nover each sentence provided by the retrieved knowl-\nedge from dynamic planning . In contrast, in static\nplanning, the retrieved knowledge sentences are all\npredicted together at the beginning using only the\nstarting sentence, which makes the supervision for\neach story sentence weaker and noisier.\n4.3.2 External Knowledge\nIn this section, we investigate the importance of\nretrieved knowledge. Table 7 (ﬁrst and second\nrows) shows that, when excluding the knowledge\nfrom our framework (i.e. MEGATRON -CNTRL -\n124 M w/o knowledge), distinction score decreases\nby 0.8% and repetition increases by 3.6%, high-\nlighting the importance of external knowledge in\nour approach. Unlike dynamic planning, we ob-\nserve that in static planning, the external knowledge\ndoes not play an important role in the quality of\nthe generations and using or not using the knowl-\nedge leads to similar qualities. This observation\nalso conﬁrms that knowledge needs to be planned\ndynamically.\n5 Future Work\nShort story sentences in ROC story dataset limits\nour exploration from several potential research di-\nrections. For example, how long the control signal\nwould propagate for longer generations? Investi-\ngating this issue using longer story datasets (e.g.\nWRITING PROMPTS (Fan et al., 2018)) is a subject\nfor future work. Other interesting direction may\ninclude incorporating the structure-level control-\nlability by adding it as either an extra input for\nthe conditional generator or a multitask learning\nsupervision for each sequence.\nWe also observed that in some cases during the\ngeneration, our model simply mentions the given\nword in the sentence, and talks about things that are\nnot strictly related to or restricted by the given word.\nFor example, the generated story of MEGATRON -\nCNTRL -8B in Table 15 only mentions the keyword\n“realize” instead of centering around it. This is\ncaused by the RAKE keywords extractor, which\ndoes not always extract the keywords that represent\nthe sentence well. One way to mitigate this issue is\nto leverage longer context information to identify\nbetter keywords which is subject of the future work.\n6 Related Work\nKnowledge Incorporation of knowledge into lan-\nguage models has shown promising results for\ndownstream tasks, such as factual correct genera-\ntion (Logan et al., 2019) , commonsense knowledge\ngraph construction (Bosselut et al., 2019), entity\ntyping (Zhang et al., 2019) and etc. More recently,\nseveral works have shown that inclusion of learned\nmechanisms for explicit or implicit knowledge can\nlead to the state-of-the-art results in Question An-\nswering (Guu et al., 2020; Karpukhin et al., 2020;\nLee et al., 2019; Lewis et al., 2020) and dialogue\nmodeling (Roller et al., 2020).\nStorytelling There are several different story-\ntelling tasks described throughout the literature.\nStorytelling can be classiﬁed into story completion\n(Chen et al., 2019), story ending generation (Guan\net al., 2019), story generation from prompts (Fan\net al., 2018) or titles (Yao et al., 2019), and story\ngeneration from a given sentence (Guan et al.,\n2020). Different approaches have been developed\nto model the structure of stories with storylines\n(Yao et al., 2019), skeletons (Xu et al., 2018),\nConditional Variational AutoEncoders (Wang and\nWan, 2019) and a coarse-to-ﬁne framework (Fan\net al., 2019). Other works focus on incorporat-\ning commonsense knowledge into story generation\nwith attention-based models (Guan et al., 2019;\nChen et al., 2019). Recently, pre-trained language\nmodels have been used to ﬁnetune on both story\ncompletion datasets and commonsense knowledge\nto further improve the quality of story completion\n(Guan et al., 2020). However, few works concern\nthe controllability of language model generation,\nespecially for the large pre-trained models that are\ncommon in today’s literature.\nControllable Generation Controllable text gen-\neration has a wide range of applications, including\ncontrolling through persona (Zhang et al., 2018;\nBoyd et al., 2020), politeness (Niu and Bansal,\n2018), etc. Wiseman et al. (2018) presented con-\ntrolling generations by learning latent, discrete tem-\nplates from data. Fu et al. (2019) discovered the\nimportance of pivot words that determines the sen-\ntence attributes and presented a lexical analysis\nframework. To control large pre-trained models,\nKeskar et al. (2019) demonstrated the ability to con-\ntrol text generation through a wide range of aspects,\nsuch as domains and links. Plug-and-play language\nmodels Dathathri et al. (2019) also address whole\ndocument controllability by adding a linear classi-\nﬁer on top of GPT-2 to predict whether generated\ntext observes a particular style or property. Prabhu-\nmoye et al. (2020) provides a good survey of ﬁve\nmodules for control. Differing from these works,\nwe control the generation through keywords backed\nby external knowledge.\n7 Conclusion\nIn this paper, we proposed a novel framework that\nadds control to text generation with external knowl-\nedge. Our model ﬁrst generates a set of keywords\nand a knowledge retriever then queries an external\nknowledge base for triples related to the keywords.\nBased on the relevance to the story context, a con-\ntextual knowledge ranker ranks the retrieved knowl-\nedge sentences and feeds the top ones to a condi-\ntional generator to generate the next story sentence.\nExperimental results on the ROC story dataset\nshowed that our model outperforms state-of-the-art\nmodels by generating less repetitive, more diverse\nand logically consistent stories. Human evalua-\ntion of the controllability of our model shows that\n91.5% of generated stories are successfully con-\ntrolled by changing keywords to their antonym. In\nline with current trends, we also demonstrate that\nusing larger pre-trained language models consis-\ntently improves both the quality of the generated\nstories and controllability.\nReferences\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. Comet: Commonsense transformers for au-\ntomatic knowledge graph construction. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 4762–4779.\nAlex Boyd, Raul Puri, Mohammad Shoeybi, Mostofa\nPatwary, and Bryan Catanzaro. 2020. Large scale\nmulti-actor generative dialog modeling. arXiv\npreprint arXiv:2005.06114.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\net al. 2018. Universal sentence encoder. arXiv\npreprint arXiv:1803.11175.\nJiaao Chen, Jianshu Chen, and Zhou Yu. 2019. In-\ncorporating structured commonsense knowledge in\nstory completion. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence , volume 33, pages\n6244–6251.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2019. Plug and play language mod-\nels: a simple approach to controlled text generation.\narXiv preprint arXiv:1912.02164.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-\nerarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889–898.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2019.\nStrategies for structuring story generation. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 2650–\n2660.\nYao Fu, Hao Zhou, Jiaze Chen, and Lei Li. 2019. Re-\nthinking text attribute transfer: A lexical analysis. In\nProceedings of the 12th International Conference on\nNatural Language Generation, pages 24–33.\nAaron Gokaslan and Vanya Cohen. 2019. Openweb-\ntext corpus.\nJian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and\nMinlie Huang. 2020. A knowledge-enhanced pre-\ntraining model for commonsense story generation.\narXiv preprint arXiv:2001.05139.\nJian Guan, Yansen Wang, and Minlie Huang. 2019.\nStory ending generation with incremental encoding\nand commonsense knowledge. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence , vol-\nume 33, pages 6473–6480.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. arXiv\npreprint arXiv:2002.08909.\nVladimir Karpukhin, Barlas O ˘guz, Sewon Min, Ledell\nWu, Sergey Edunov, Danqi Chen, and Wen-\ntau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. arXiv preprint\narXiv:2004.04906.\nNitish Shirish Keskar, Bryan McCann, Lav R. Varsh-\nney, Caiming Xiong, and Richard Socher. 2019.\nCtrl: A conditional transformer language model for\ncontrollable generation.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised\nopen domain question answering. arXiv preprint\narXiv:1906.00300.\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K ¨uttler, Mike Lewis, Wen-tau Yih, Tim\nRockt¨aschel, et al. 2020. Retrieval-augmented gen-\neration for knowledge-intensive nlp tasks. arXiv\npreprint arXiv:2005.11401.\nRobert Logan, Nelson F Liu, Matthew E Peters, Matt\nGardner, and Sameer Singh. 2019. Barack’s wife\nhillary: Using knowledge graphs for fact-aware lan-\nguage modeling. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5962–5971.\nGeorge A Miller. 1995. Wordnet: a lexical database for\nenglish. Communications of the ACM , 38(11):39–\n41.\nNasrin Mostafazadeh, Lucy Vanderwende, Wen-tau\nYih, Pushmeet Kohli, and James Allen. 2016. Story\ncloze evaluator: Vector space representation evalu-\nation by predicting what happens next. In Proceed-\nings of the 1st Workshop on Evaluating Vector-Space\nRepresentations for NLP, pages 24–29.\nTong Niu and Mohit Bansal. 2018. Polite dialogue gen-\neration without parallel data. Transactions of the As-\nsociation for Computational Linguistics, 6:373–389.\nShrimai Prabhumoye, Alan W Black, and Rus-\nlan Salakhutdinov. 2020. Exploring control-\nlable text generation techniques. arXiv preprint\narXiv:2005.01822.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nKurt Shuster, Eric M Smith, et al. 2020. Recipes\nfor building an open-domain chatbot. arXiv preprint\narXiv:2004.13637.\nStuart Rose, Dave Engel, Nick Cramer, and Wendy\nCowley. 2010. Automatic keyword extraction from\nindividual documents. Text mining: applications\nand theory, 1:1–20.\nZhihong Shao, Minlie Huang, Jiangtao Wen, Wenfei\nXu, et al. 2019. Long and diverse text generation\nwith planning-based hierarchical variational model.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3248–\n3259.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion\nparameter language models using gpu model paral-\nlelism. arXiv preprint arXiv:1909.08053.\nRobert Speer and Catherine Havasi. 2012. Represent-\ning general relational knowledge in conceptnet 5. In\nLREC, pages 3679–3686.\nTrieu H Trinh and Quoc V Le. 2018. A simple\nmethod for commonsense reasoning. arXiv preprint\narXiv:1806.02847.\nTianming Wang and Xiaojun Wan. 2019. T-cvae:\nTransformer-based conditioned variational autoen-\ncoder for story completion. In Proceedings of the\n28th International Joint Conference on Artiﬁcial In-\ntelligence, pages 5233–5239. AAAI Press.\nSam Wiseman, Stuart M Shieber, and Alexander M\nRush. 2018. Learning neural templates for text gen-\neration. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3174–3187.\nThomas Wolf, Victor Sanh, Julien Chaumond, and\nClement Delangue. 2019. Transfertransfo: A\ntransfer learning approach for neural network\nbased conversational agents. arXiv preprint\narXiv:1901.08149.\nJingjing Xu, Xuancheng Ren, Yi Zhang, Qi Zeng, Xi-\naoyan Cai, and Xu Sun. 2018. A skeleton-based\nmodel for promoting coherence among sentences in\nnarrative story generation. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 4306–4315.\nLili Yao, Nanyun Peng, Ralph Weischedel, Kevin\nKnight, Dongyan Zhao, and Rui Yan. 2019. Plan-\nand-write: Towards better automatic storytelling. In\nProceedings of the AAAI Conference on Artiﬁcial In-\ntelligence, volume 33, pages 7378–7385.\nLili Yao, Nanyun Peng, Ralph M. Weischedel, Kevin\nKnight, Dongyan Zhao, and Rui Yan. 2018. Plan-\nand-write: Towards better automatic storytelling.\nCoRR, abs/1811.05701.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. In Advances in Neural Information Process-\ning Systems, pages 9051–9062.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you\nhave pets too? In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 2204–\n2213.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. Ernie: Enhanced\nlanguage representation with informative entities. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 1441–\n1451.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. CoRR, abs/1506.06724.\nA Appendices\nA.1 G PT-2 Hyperparameters:\nWe used the BPE subword tokenizer from Radford\net al. (2019) to tokenize each sentence of the ROC\nstory dataset. The maximum sequence length is set\nto 1024 learned positional embeddings. An Adam\noptimizer (Kingma and Ba, 2014) with learning\nrate of 0.0001 is utilized. We added dropout to\nboth the embedding layer and multi-head attention\nlayers with 0.1 probability. Gradients were clipped\nto a global gradient norm of 5.0. We ﬁnetuned the\nGPT-2 models for 5 epochs and selected the best\none with the lowest perplexity on the validation set.\nA.2 B ERT Hyperparameters:\nWe set the maximum sequence length to\n512 learned positional embeddings. We\nused a WordPiece tokenizer with the\nbert-large-uncased vocabulary for to-\nkenization. The model was also optimized with\nan Adam optimizer with a learning rate of 0.0001,\nbut it used a weight decay of 0.01. Gradients\nare clipped to a global norm of 1.0. We also\nadded dropout to embedding layer and multi-head\nattention layers with 0.1 probability. For the\nselection of margin, we tried 0.1, 0.5, 1.0, and 5.0.\nThe choice of 5.0 gives the best result.\nA.3 Model Size\nIn addition to analyzing the effect of scale on our\nconditional generative model, we also performed\nan ablation study on the model size of our GPT-2-\nbased keyword predictor and BERT-based ranker\nmodels. The results in Table 8 show that increasing\nthe keyword model size from 774M to 2B reduces\nthe repetition while keeping the other scores sim-\nilar. Increasing the size of our contextual ranker\nfrom 336M to 1.2B reduces the repetition while\nalso decreasing the diversity. It is not clear which\none is better. We conjecture that as the positive sam-\nples, Ri, we used to train our contextual ranker are\nweakly supervised, and the fact that we used tem-\nplates to synthetically convert knowledge triples\nto knowledge sentences, scaling the model size\nmight be overﬁtting to noise. We, therefore, use\nthe smaller, more computationally efﬁcient model\nwith 336M parameters for ranker models in all our\nexperiments.\nName (a-b-c) PPL ↓ Repeat↓ Distinct↑\n2B-2B-336M6.31 21.2 82.6\n2B-2B-1.2B 6.35 19.7 81.2\n2B-774M-1.2B 6.33 20.4 81.4\nTable 8: Ablation studies varying keyword prediction\nmodel ( b) and ranking model ( c) keeping the condi-\ntional generator ﬁxed ( a). Increasing keyword predic-\ntion model reduces repetition. Larger ranking models\ndoes not give consistently better scores as it may overﬁt\nto noise due to the weakly supervised labels.\nA.4 Datasets Used for pre-trained Models\nThe pre-trained GPT-2 models were trained on a\n174GB corpora including: Wikipedia (Devlin et al.,\n2018), OpenWebText (Gokaslan and Cohen, 2019),\nRealNews (Zellers et al., 2019), and CC-Stories\n(Trinh and Le, 2018). For BERT models we in-\nclude BooksCorpus (Zhu et al., 2015) in the train-\ning dataset.\nA.5 Input Format\nFor the format of Rj, we add a “ SEP ” string to\nseparate different knowledge sentences rj\nk in Rj.\nWe add a “ EOK ” string to denote the end of\nknowledge sentences.\nFor the story context Xi−1 =\n{x1, x2, ··· , xi−1} where xj = [ Rj, sj], we\nadd a “OS” token to the end of each sj to\ndenote the end of sentence. At the end of the\nstory, a “ |endoftext|” token is appended. We\nthen concatenated Xi−1 with Ri as the input to\nconditional generator.\nA.6 Preprocessing\nWe follow the implementation of Guan et al. (2020),\nto replace “ .” with “. ”. More details have already\nbeen provided in Section 3.1.\nA.6.1 Computation Resources\nWe used up to 20 DGX-1V servers (a total of 160\nTesla V100 32GB GPUs) for our experiments. Our\ninfrastructure is highly optimized for large scale\ndeep learning applications. The servers are con-\nnected via NVSwitch with 300 GB/sec bandwidth\nbetween GPUs inside a server and 100 GB/sec of\ninterconnect bandwidth between servers using 8 In-\nﬁniBand adapters per server. As our story dataset is\nsmall, the training of GPT-2 models usually takes\nless than three hours. For the BERT model, it took\neight hours as we have ∼15 million samples to\ntrain.\nA.7 Generation Examples\nA.7.1 Single Sentence Context\nWe present ﬁve generation examples keeping the\ncontext ﬁxed to one sentence.\nContext: [MALE] and his friends are bored on a\nweekend day.\nGuan et al. (2019)\nthey decided to sneak into a movie . they got a bunch\nof popcorn and a movie . then , they made it to a\nmovie . now , they have a really good time .\nMEGATRON-CNTRL-124M keyword: go, decides\nit is going to be a big night. [MALE] and his friends\ndecide to go out. they get drunk and have a great\ntime. when they are done they feel happy.\nMEGATRON-CNTRL-8B keyword: go, decides\nthey decided to go to the video game store. [MALE]\npicks out a new video game. [MALE] and his friends\nhave fun playing the new video game. [MALE] is no\nlonger bored.\nMEGATRON-CNTRL-8B-ANT keyword: come\na new tv show begins to come on the local channel.\n[MALE] is excited to watch his favorite tv show.\nhe watches his show and they all laugh together.\n[MALE] and his friends have a fun time watching\ntheir favorite show.\nTable 9: Example 1\nContext: [MALE] had a heavy duty backpack.\nGuan et al. (2019)\nhe took it to the airport . he checked his pockets .\n[MALE] did not have a bag . [MALE] was sad .\nMEGATRON-CNTRL-124M keyword: took\nhe took it to the store one morning. he put it on the\nshelf at the store. a coworker looked at it. then he\ntold [MALE] he hated lugging around.\nMEGATRON-CNTRL-8B keyword: took\nhe took it to the army base camp. he packed it with\nitems like food , ammo , and tools. he took it to the\njungle every day. one day , he took it out and his\nbackpack exploded.\nMEGATRON-CNTRL-8B-ANT keyword: abstain\nhe abstained from buying another backpack.\n[MALE] used the old backpack for years. his old\nbackpack was a big burden. [MALE] bought a new\nbackpack with greater weight capacity.\nTable 10: Example 2\nContext: [NEUTRAL] ’s blind date insisted on going\nto the new sushi place.\nGuan et al. (2019)\nshe accepted without saying a word . later , he picked\nout a sushi roll . [NEUTRAL] was so happy with\nher new sushi rolls . she had to wait for him to come\nback .\nMEGATRON-CNTRL-124M keyword: sure, date\nshe decided to go to the restaurant. she ordered a\nblowﬁsh. they gave her a small grin. she had a great\ntime.\nMEGATRON-CNTRL-8B keyword: thought, rude\n[NEUTRAL] was nervous because he was worried\nshe would be rude. but at the table , she was sweet\nand friendly. and she was very nice , even though\nshe ate little sushi. [NEUTRAL] wondered if maybe\nblind dates were actually very wonderful !\nMEGATRON-CNTRL-8B-ANT keyword: uncertain\n[NEUTRAL] was uncertain. she was not a sushi\nperson but agreed to try it. [NEUTRAL] and her\nblind date ordered salmon sushi. the salmon sushi\nwas delicious.\nTable 11: Example 3\nContext: [MALE] hiked up the local hill.\nGuan et al. (2019)\nhe stopped and picked up his brush . he used that\nbrush to brush his teeth . afterwards , he was ready\nto ride back home . he went on his way home .\nMEGATRON-CNTRL-124M keyword: top\nhe got a great view from the top. his legs felt great.\nhe climbed up the mountain and felt much better. he\nwent back to the hill and reached the top.\nMEGATRON-CNTRL-8B keyword: top\nhe was at the top of the hill looking down. the weather\nlooked beautiful. the clouds suddenly passed. the\nsun began to shine over the land again.\nMEGATRON-CNTRL-8B-ANT keyword: bottom\nhe saw a huge boulder at the bottom. he could n’t\nbelieve its size. he had to climb to the top of the hill.\nhe was afraid he would fall if he stood on top.\nTable 12: Example 4\nContext: her favorite glasses were ruined.\nGuan et al. (2019)\nshe was really upset . she had no choice but to buy\nnew ones . she went to the mall to shop and found a\ngreat pair . she bought them and was so happy .\nMEGATRON-CNTRL-124M keyword: wore\nshe wore them every day. then she had a horrible\naccident. her sister had broken them. she had to buy\nnew ones.\nMEGATRON-CNTRL-8B keyword: wore\nshe wore them daily. she got them at the shoe store.\nshe found them and they were her favorite. after a\nwhile they got broken.\nMEGATRON-CNTRL-8B-ANT keyword: refresh\nshe was desperate to refresh them. she found a recipe\nonline. she poured bleach in them. she was glad to\nhave functional glasses.\nTable 13: Example 5\nA.7.2 Varying Number of Sentences in the\nContext\nWe present three generation examples varying the\nnumber of sentences in the context.\nContext: [MALE] and his friends are bored on a\nweekend day. it is going to be a big night.\nMEGATRON-CNTRL-124M keyword: go, decides\n[MALE] and his friends decide to go out. they get\ndrunk and have a great time. when they are done they\nfeel happy.\nMEGATRON-CNTRL-8B keyword: decides\n[MALE] decides to watch horror movies. his friends\njoin him. they watch horror movies all night.\nMEGATRON-CNTRL-8B-ANT keyword: stop\nthey stop by a local arcade after the baseball game.\n[MALE] turns on his favorite arcade game. [MALE]\n’s favorite game makes him feel happy.\nTable 14: The context contains two sentences.\nContext: [MALE] liked trading commodities. one\nday [MALE] was trading several products. [MALE]\nnoticed that he gained a lot of money from the new\ntrades.\nMEGATRON-CNTRL-124M keyword: lost, money\n[MALE] invested the money into stocks. [MALE]\nlost all of his trading proﬁts.\nMEGATRON-CNTRL-8B keyword: realize\n[MALE] realized that the new trades were his best\ntrading idea. [MALE] quit his old job and went full\ntime on his new trading idea.\nMEGATRON-CNTRL-8B-ANT keyword: keep\n[MALE] decided to keep doing this. now [MALE]\nhad a successful trading career.\nTable 15: The context contains three sentences.\nContext: we went to a dinner and show one night.\nthe ﬁrst guy we saw was really nice. after that we\nasked him out. he agreed.\nMEGATRON-CNTRL-124M keyword: got\nwe got into a bad argument.\nMEGATRON-CNTRL-8B keyword: NONE (no key-\nwords)\nwe had a really good time.\nMEGATRON-CNTRL-8B-ANT keyword: end\nwe ended up dating exclusively.\nTable 16: The context contains four sentences.\nA.8 Human Evaluation Interface for\nAnnotators\nBelow we provide the interfaces used for human\nevaluations (coherence, ﬂuency, logical consis-\ntency, and controllability).\n",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7182056307792664
    },
    {
      "name": "Language model",
      "score": 0.6259969472885132
    },
    {
      "name": "Controllability",
      "score": 0.5487297177314758
    },
    {
      "name": "Natural language processing",
      "score": 0.5392178297042847
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.5304626226425171
    },
    {
      "name": "Embedding",
      "score": 0.5231212973594666
    },
    {
      "name": "Generator (circuit theory)",
      "score": 0.48698338866233826
    },
    {
      "name": "Sentence",
      "score": 0.48144954442977905
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4787418246269226
    },
    {
      "name": "Scaling",
      "score": 0.474235475063324
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45642751455307007
    },
    {
      "name": "Repetition (rhetorical device)",
      "score": 0.4518628716468811
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.43900972604751587
    },
    {
      "name": "Knowledge base",
      "score": 0.4369587004184723
    },
    {
      "name": "Linguistics",
      "score": 0.1814914345741272
    },
    {
      "name": "Mathematics",
      "score": 0.10940679907798767
    },
    {
      "name": "Applied mathematics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Power (physics)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 12
}