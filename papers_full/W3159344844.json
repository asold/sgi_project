{
    "title": "Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers",
    "url": "https://openalex.org/W3159344844",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A3112618577",
            "name": "Hila Chefer",
            "affiliations": [
                "Tel Aviv University"
            ]
        },
        {
            "id": "https://openalex.org/A2778446839",
            "name": "Shir Gur",
            "affiliations": [
                "Tel Aviv University"
            ]
        },
        {
            "id": "https://openalex.org/A2059111593",
            "name": "Lior Wolf",
            "affiliations": [
                "Tel Aviv University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6788135285",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W6736518430",
        "https://openalex.org/W2962858109",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W6790978476",
        "https://openalex.org/W6734194636",
        "https://openalex.org/W6767259515",
        "https://openalex.org/W6739575509",
        "https://openalex.org/W6685133223",
        "https://openalex.org/W6791353385",
        "https://openalex.org/W3120875261",
        "https://openalex.org/W6769627184",
        "https://openalex.org/W1933349210",
        "https://openalex.org/W3035422918",
        "https://openalex.org/W6639102338",
        "https://openalex.org/W6737947904",
        "https://openalex.org/W6766904570",
        "https://openalex.org/W2195388612",
        "https://openalex.org/W2190008860",
        "https://openalex.org/W2133059825",
        "https://openalex.org/W6760904063",
        "https://openalex.org/W6784094891",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2988157455",
        "https://openalex.org/W2606462007",
        "https://openalex.org/W6757328097",
        "https://openalex.org/W3109440167",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3012430016",
        "https://openalex.org/W6769311223",
        "https://openalex.org/W6767279747",
        "https://openalex.org/W6775970589",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W1787224781",
        "https://openalex.org/W6754041176",
        "https://openalex.org/W6787329981",
        "https://openalex.org/W6738549535",
        "https://openalex.org/W6779879114",
        "https://openalex.org/W2295107390",
        "https://openalex.org/W6784333009",
        "https://openalex.org/W2776207810",
        "https://openalex.org/W6639204139",
        "https://openalex.org/W6746078580",
        "https://openalex.org/W6788620109",
        "https://openalex.org/W6785727093",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W3170470779",
        "https://openalex.org/W6786708909"
    ],
    "abstract": "Transformers are increasingly dominating multi-modal reasoning tasks, such as visual question answering, achieving state-of-the-art results thanks to their ability to contextualize information using the self-attention and co-attention mechanisms. These attention modules also play a role in other computer vision tasks including object detection and image segmentation. Unlike Transformers that only use self-attention, Transformers with co-attention require to consider multiple attention maps in parallel in order to highlight the information that is relevant to the prediction in the model's input. In this work, we propose the first method to explain prediction by any Transformer-based architecture, including bi-modal Transformers and Transformers with co-attentions. We provide generic solutions and apply these to the three most commonly used of these architectures: (i) pure self-attention, (ii) self-attention combined with co-attention, and (iii) encoder-decoder attention. We show that our method is superior to all existing methods which are adapted from single modality explainability.",
    "full_text": "Generic Attention-model Explainability for Interpreting\nBi-Modal and Encoder-Decoder Transformers\nHila Chefer1 Shir Gur1 Lior Wolf1,2\n1The School of Computer Science, Tel Aviv University\n2Facebook AI Research (FAIR)\nAbstract\nTransformers are increasingly dominating multi-modal\nreasoning tasks, such as visual question answering, achiev-\ning state-of-the-art results thanks to their ability to con-\ntextualize information using the self-attention and co-\nattention mechanisms. These attention modules also play\na role in other computer vision tasks including object\ndetection and image segmentation. Unlike Transform-\ners that only use self-attention, Transformers with co-\nattention require to consider multiple attention maps in\nparallel in order to highlight the information that is rel-\nevant to the prediction in the model’s input. In this\nwork, we propose the ﬁrst method to explain prediction\nby any Transformer-based architecture, including bi-modal\nTransformers and Transformers with co-attentions. We\nprovide generic solutions and apply these to the three\nmost commonly used of these architectures: (i) pure self-\nattention, (ii) self-attention combined with co-attention, and\n(iii) encoder-decoder attention. We show that our method\nis superior to all existing methods which are adapted\nfrom single modality explainability. Our code is avail-\nable at: https://github.com/hila-chefer/\nTransformer-MM-Explainability .\n1. Introduction\nMulti-modal Transformers may change the way that\ncomputer vision is practiced. While the state of the art com-\nputer vision models are often trained as task-speciﬁc mod-\nels that infer a ﬁxed number of labels, Radford et al. [28]\nhave demonstrated that by training an image-text model that\nemploys Transformers for encoding each modality, tens of\ndownstream tasks can be performed, without further train-\ning (“zero-shot”), at comparable accuracy to the state of\nthe art. Subsequently, Ramesh et al. [30] used a bi-modal\nTransformer to generate images that match a given descrip-\ntion in unseen domains with unprecedented performance.\nThese two contributions merge text and images differ-\nently. The ﬁrst encodes the text with a Transformer [40],\nthe image by either a ResNet [15] or a Transformer, and\nthen applies a symmetric contrastive loss. The second con-\ncatenates the quantized image representation to the text\ntokens and then employs a Transformer model. There\nare also many other methods of combining text and im-\nages [38, 21, 19, 18]. What is common to all of these is\nthat the mapping from the two inputs to the prediction con-\ntains interaction between the two modalities. These inter-\nactions often challenge the existing explainability methods\nthat are aimed at attention-based models, since, as far as we\ncan ascertain, all existing Transformer explainability meth-\nods (e.g., [5, 1]) heavily rely on self-attention, and do not\nprovide adaptations to any other form of attention, which is\ncommonly used in multi-modal Transformers.\nAnother class of Transformer models that is not re-\nstricted to self-attention is that of Transformer encoder-\ndecoders, i.e. generative models, in which the model typ-\nically receives an input from a single domain, and produces\noutput from a different one. These models are used in an\nemerging class of object detection [4, 49] and image seg-\nmentation [43, 27, 42] methods, and are also widely used\nfor various NLP tasks, such as machine translation [40, 17].\nIn these object detection methods, for example, embed-\ndings of the position-speciﬁc and class-speciﬁc queries are\ncrossed with the encoded image information.\nWe propose the ﬁrst explainability method that is ap-\nplicable to all Transformer architectures, and demonstrate\nits effectiveness on the three most commonly used Trans-\nformer architectures: (i) pure self-attention, (ii) self-\nattention combined with co-attention, and (iii) encoder-\ndecoder attention. We use an exemplar model from each\narchitecture, and prove our method’s superiority over exist-\ning Transformer explainability methods, adapted from their\nsingle modality origin. Our explainability prescription is\neasier to implement than existing methods, such as [5], and\ncan be readily applied to any attention-based architecture.\n1\narXiv:2103.15679v1  [cs.CV]  29 Mar 2021\n2. Related work\nExplainability in computer vision Interpreting com-\nputer vision algorithms usually entails the synthesis of a\nheatmap that depicts the computed relevancy at each im-\nage location. This can be class-dependent (for every possi-\nble label), or class-agnostic, in which case it depends only\non the input and the model. Unlike most methods below,\nour method is of the ﬁrst type. There are multiple families\nof explainability methods, including saliency-based meth-\nods [8, 34, 23, 48, 44, 47], methods that consider acti-\nvations [10] using the forward pass or the backprop [45],\nperturbation based methods [11, 12], and methods based\non Shapley-values [22, 6]. The latter enjoy clear theoret-\nical motivation. Theoretical justiﬁcation is also given to\nattribution-based methods, through the theory of the Deep\nTaylor Decomposition [24]. Such methods assign rele-\nvancy recursively from the top layer, backward, such that\nthe sum of relevancies remains ﬁxed. The LRP method [3],\nis one such prominent method. Since LRP and most vari-\nants [25, 33, 22] are class agnostic [16], class-speciﬁc ex-\ntensions were introduced [13, 16, 14].\nGradient-based methods directly consider the gradient of\nthe loss with respect to the input of each layer, as computed\nthrough backpropagation. Examples include class agnostic\nmethods [33, 37, 35, 36]. A related class-speciﬁc approach\nis the Grad-CAM method [32], which considers the input\nfeatures with the class-dependent gradient at the top layers.\nExplainability for Transformers Most attempts to ex-\nplain Transformers directly employ the attention maps.\nThis, however, neglects the intermediate attention scores,\nas well as the other components of the Transformers. As\nnoted by Chefer et al [5], the computation in each attention\nhead mixes queries, keys, and values and cannot be fully\ncaptured by considering only the inner products of queries\nand keys, which is what is referred to as attention.\nLRP was applied to capture the relative importance of\nthe attention heads within each Transformer block by V oita\net al. [41]. This method, however, does not propagate the\nrelevancy scores back to the input to produce a heatmap.\nAbnar et al. [1] propose a way to combine the attention\nscores across multiple layers. Two methods are suggested:\nattention rollout and attention ﬂow. The ﬁrst combines at-\ntention linearly along alternative paths in the pairwise at-\ntention graph. It is shown in [5] that this method fails to\ndistinguish between positive and negative contributions to\nthe decision, leading to an accumulation of relevancy scores\nacross the layers in cases for which these should be can-\ncelled out. The attention ﬂow method is formulated as a\nmax-ﬂow problem on the same pairwise attention graph.\nWhile it was shown in [1] to somewhat outperform roll-\nout in speciﬁc scenarios, this method is too slow to support\nlarge-scale evaluations.\nIn contrast to these methods, Chefer et al. [5] provide\na comprehensive treatment of the information propagation\nwithin all components of the Transformer model, which\nback propagates the information through all layers from the\ndecision back to the input. The solution is based on Layer-\nwise Relevance Propagation [3], with gradient integration\nfor the self-attention layers, and is shown to be very effec-\ntive for single modality Transformer encoders, such as [9].\nThis method, however, does not provide a solution for atten-\ntion modules other than self-attention, thus can not provide\nexplanations for all Transformer architectures.\nTransformers in computer vision Transformer technol-\nogy has become increasingly prevalent for bi-modal tasks,\nsuch as image captioning and text-based image retrieval.\nWe distinguish between networks that rely on self-attention,\nsuch as VisualBERT [18] and Oscar [19] and those that also\nemploy co-attention modules, such as LXMERT [38] and\nViLBERT [21]. Our method provides suitable visualization\nfor both types.\nOur method also provides the ﬁrst complete solution,\nas far as we can ascertain, for Transformer encoder-\ndecoders [40, 29, 17], which have been increasingly preva-\nlent in computer vision. In the DETR Transformer-based\ndetection method [4], the image is encoded by a Trans-\nformer encoder, and the obtained information is co-attended\ntogether with queries that are both positional and class-\nbased. Our method can be also applied to encoder-based\nvisual Transformers, such as those used for image recog-\nnition [7, 9, 39], and image segmentation with a CNN de-\ncoder [46]. However, in this case, existing Transformer ex-\nplainability methods can also be applied.\n3. Method\nOur method uses the model’s attention layers to produce\nrelevancy maps for each of the interactions between the in-\nput modalities in the network. In this work, we focus on im-\nage and text interactions, and attention modules for gener-\native models, i.e., encoder-decoder attention. However, our\nmethod is easily applicable to any Transformer-based archi-\ntecture, and can also be generalized to address more than\ntwo modalities. In the following, we discuss the method’s\npropagation rules under the assumption of two modalities,\ne.g. text and image for simplicity, followed by a detailed de-\nscription of how to apply our method to each of the model\ntypes used in this work.\nLet t, ibe the number of text and image input tokens\nrespectively. To simplify notation, we use the same sym-\nbols (t, i) to identify variables that are associated with the\ntwo domains. Multi-modal attention networks contain four\ntypes of interactions between the input tokens: Att and Aii\nare the self-attention interactions for the text and image to-\nkens, respectively. Ati, Ait are the multi-modal attention\ninteractions, where Ati represents the inﬂuence of the im-\nage tokens on each text token, and Ait represents the inﬂu-\n2\nMulti-Head\nAttention\nK QV\nAdd & Norm\nInput\nSelf-Attention\nMulti-Head\nAttention\nK QV\nAdd & Norm\nInputContext\nCo-Attention\n(a) (b)\nFigure 1: (a) Self-attention and (b) co-attention modules.\nence of the text tokens on each image token.\nIn accordance with the attention interactions described,\nwe construct a relevancy map per interaction, i.e. Rtt, Rii\nfor self-attention, and Rti, Rit for bi-modal attention.\nThe method calculates the relevancy maps by a forward\npass on the attention layers, with each layer contributing to\nthe aggregated relevance matrices using the update rules we\nwill describe in the following subsections.\nRelevancy initialization Before the attention operations,\neach token is self-contained. Thus, self-attention interac-\ntions are initialized with the identity matrix. For bi-modal\ninteractions, before the attention layers, each modality is\nseparate and does not contain context from the other modal-\nity, therefore, the relevancy maps are initialized to zeros.\nRii = Ii×i, Rtt = It×t (1)\nRit = 0i×t, Rti = 0t×i (2)\nRelevancy update rules As the attention layers contex-\ntualize the tokens, our method modiﬁes the relevancy maps\nthat are impacted by the mixture of token embeddings. Re-\ncall the attention mechanism presented in [40]:\nA = softmax(Q ·K⊤\n√dh\n) (3)\nO = A ·V (4)\nwhere (·) denotes matrix multiplication, O ∈Rh×s×dh is\nthe output of the attention module, Q ∈ Rh×s×dh is the\nqueries matrix, and K, V ∈Rh×q×dh are the keys and val-\nues matrices. h is the number of heads,dh is the embedding\ndimension, and s, q∈{i, t}indicate the domains and the\nnumber of tokens in each domain, i.e., the attention takes\nplace between s query tokens and q key tokens. Note that,\nas can be seen in Fig. 1, for self-attention layers, it holds\nthat s = q and Q, K, V are all projections of the input to\nthe attention unit, while in co-attention Q is a projection\nof the input, and K, V are projections of the context input\nfrom the other modality. A ∈Rh×s×q is the attention map,\nwhich intuitively deﬁnes connections between each pair of\ntokens from s, q. Since the attention module is followed by\na residual connection, as shown in Fig. 1, we accumulate\nthe relevancies by adding each layer’s contribution to the\naggregated relevancies, similar to [1] in which the identity\nmatrix is added to account for residual connections.\nOur method uses the attention map A of each attention\nlayer to update the relevancy maps. Since each such map\nis comprised of h heads, we follow [5] and use gradients to\naverage across heads. Note that V oita et al. [41] show that\nattention heads differ in importance and relevance, thus a\nsimple average across heads results in distorted relevancy\nmaps. The ﬁnal attention map ¯A ∈Rs×q of our method is\nthen deﬁned as follows:\n¯A = Eh((∇A ⊙A)+) (5)\nwhere ⊙is the Hadamard product,∇A := ∂yt\n∂A for yt which\nis the model’s output for the class we wish to visualizet, and\nEh is the mean across the heads dimension. Following [5]\nwe remove the negative contributions before averaging.\nFor self-attention layers that satisfy ¯A ∈Rs×s the up-\ndate rules for the affected aggregated relevancy scores are:\nRss = Rss + ¯A ·Rss (6)\nRsq = Rsq + ¯A ·Rsq (7)\nIn Eq. 6 we account for the fact that the tokens were already\ncontextualized in previous attention layers by applying ma-\ntrix multiplication with the aggregated self-attention matrix\nRss, as done in [1, 5]. For Eq. 7, notice that the previ-\nous bi-modal attention layers inserted context from q into\ns, therefore, when the self-attention mixes tokens from s, it\nalso mixes the context q in each token from s. The previous\nlayers’ mixture of context is embodied by Rsq. Thus, we\ncalculate the added context from the self-attention process.\nIn the case of ¯A ∈ Rs×q, where a bi-modal atten-\ntion is applied, the update rules of the relevancy accumu-\nlators include normalization for the self-attention matrices\nRxx, x∈{s, q}. Since we initialized Rxx = Ix×x, and\nEq. 6 accumulates the relevancy matrices at each layer, we\ncan consider an aggregated self-attention matrix Rxx as a\nmatrix comprised of two parts, the ﬁrst is the identity matrix\nfrom the initialization, and the second, ˆRxx = Rxx −Ix×x\nis the matrix created by the aggregation of self-attention\nacross the layers. Since Eq. 5 uses gradients to average\nacross heads, the values of ˆRxx are typically reduced. We\nwish to account equally both for the fact that each token\ninﬂuences itself and for the contextualization by the self-\nattention mechanism. Therefore, we normalize each row in\nˆRxx so that it sums to 1. Intuitively, rowi in ˆRxx disclosed\nthe self-attention value of each token w.r.t. the i-th token,\nand the identity matrix Ix×x sets that value for each token\n3\nw.r.t. itself as 1. Thus:\nˆSxx\nm,n =\n|x|∑\nk=1\nˆRxx\nm,k (8)\n¯Rxx = ˆRxx/ˆSxx + Ix×x , (9)\nwhere / stands for matrix division element by element. In\nthe above, we normalize each row in ˆRxx by dividing each\nelement in the row by the sum of the row. Next, we deﬁne\nthe following aggregation rules for bi-modal attention units:\nRsq = Rsq + (¯Rss)⊤·¯A ·¯Rqq (10)\nRss = Rss + ¯A ·Rqs (11)\nEq. 10 accounts for the fact that the tokens of each modality\nwere already contextualized in previous attention layers by\napplying matrix multiplication with the normalized aggre-\ngated self-attention matrices ¯Rss, ¯Rqq.\nFor Eq. 11, notice that the previous bi-modal attention\nlayers integrate the embeddings of the two modalities, thus\nwhen contextualizing s with q, q also contains information\nfrom s, embodied in Rqs.\nNote that the above rules are described w.r.t. input from\nmodality s ∈{i, t}, and context from modality q ∈{i, t}\ni.e. the rules are symmetrically applied to both modalities,\nimage and text.\n3.1. Obtaining classiﬁcation relevancies\nIn order to make the ﬁnal classiﬁcation, Transformer-\nbased models usually regard the [CLS] token, which is\na token that is added to the input tokens and constructs a\ngeneral representation of all the input tokens. To retrieve\nper-token relevancies for classiﬁcation tasks, one can con-\nsider the row corresponding to the [CLS] token in the\ncorresponding relevancy map. For instance, assuming the\n[CLS] token is the ﬁrst token in the text modality, to ex-\ntract relevancies per text token, one should consider the ﬁrst\nrow of Rtt, and to extract the image token relevancies, con-\nsider the ﬁrst row in Rti which describes the connections\nbetween the [CLS] token and each image token.\n3.2. Adaptation to attention types\nIn this work, we examine our method on three differ-\nent types of attention mechanisms used in Transformer-\nbased networks. The architectures and matching propaga-\ntion rules are visualized in Fig. 2. The ﬁrst architecture type\nis a multi-modal Transformer, where the two modalities are\nconcatenated and separated by the [SEP] token [18, 19],\nas demonstrated in Fig. 2(a). Such networks only use self-\nattention to contextualize the modalities, i.e. only Eq. 6.\nSince the model is based on pure self-attention, we produce\none relevancy map R(t+i,t+i) which deﬁnes connections\nbetween the modalities, as well as within each modality. In\norder to visualize the tokens related to the classiﬁcation, one\nshould consider the row of R(t+i,t+i) which corresponds to\nthe token used for classiﬁcation. This row Rcls\n(t+i) yields\na relevancy score per image token and per text token.\nThe second type is a multi-modal attention network that\nincorporates co-attention modules that contextualize each\nmodality with the other modality [38, 21], as can be seen\nin Fig. 2(b). Such networks require all propagation rules\ndescribed above, for each modality. To produce relevan-\ncies for the classiﬁcation, we simply follow the example in\nSec 3.1, since as Fig. 2(b) depicts, the [CLS] token in this\ncase is the ﬁrst token of the text modality.\nThe third and last type is a generative model where there\nis one input modality, and the output is from a different\ndomain [4, 49, 43, 27, 42, 40, 17], which is visualized in\nFig. 2(c). Such networks contain an encoder that utilizes\nself-attention on the input and a decoder. The decoder has\ntwo types of inputs. The ﬁrst is the encoded data, which\nremains unchanged, and the second are inputs from the\ndecoder’s domain. The decoder proceeds to utilize self-\nattention on the decoder domain’s tokens, followed by a\nco-attention layer contextualizing them with the encoder’s\noutput. To clarify, in this case, the relevance update rules\nare as follows: notate by e the encoder’s tokens, and by d\nthe decoder’s tokens. The relevancy matrices are:Ree, Rdd\nfor the self-attention interactions, and Rde for the bi-modal\ninteractions between the decoder’s tokens and the encoder’s\ntokens. Notice that since the encoder is not contextualized,\nwe do not have a relevancy matrixRed. The encoder’s self-\nattention calculation for Ree simply follows Eq. 6. For the\ndecoder’s self-attention, we apply Eq. 6, 7. For the bi-modal\nattention in the decoder, we follow Eq. 10 to account for\nself-attention in the encoder and the decoder. Notice that\nEq. 11 is irrelevant since we do not have a relevancy map\nfor Rqs = Red. In order to extract relevancies in this case,\nwe consider the relevancy map Rde. In this work, we use\nan object detection model as our exemplar encoder-decoder\narchitecture. For such models, each token from d is a query\nrepresenting an object in the input image. In order to pro-\nduce relevancy for each of the image regions w.r.t. an ob-\nject j that was detected, one should consider thej-th row of\nRde, which corresponds to the j-th detection. Rde\nj contains\na relevancy score per each encoder token, which is in this\ncase an image region.\n4. Baselines\nWe focus on methods that are both common in the ex-\nplainability literature, and applicable to the extensive tests\nwe report in this work. We present baselines of three\nclasses, following [5]: attention map baselines, gradient\nbaselines, and relevancy map baselines. Our attention map\nbaselines are raw attention and rollout. Raw attention re-\n4\nSelf\nAttention\n[SEP]\ntext\nimage\nxN\n6\nxNi xM\nxNt\nContext\nCo\nAttention\n10 11\nCo\nAttention\n10 11\nSelf\nAttention\n6 7\nSelf\nAttention\n6 7\nSelf\nAttention\n6\nSelf\nAttention\n6\nimage\ntext\n[CLS]\nxN\nxM\nimage queries\nContext\nimage-encoding\nSelf\nAttention\n6 Self\nAttention\nCo\nAttention\n6 7\n10\n(a) (b) (c)\nFigure 2: Illustration of the three architecture types presented in our work. The numbers in each attention module represent\nthe Eq. number of the rule applied by our method on the module’s forward pass. (a) VisualBERT: a pure self-attention\narchitecture. (b) LXMERT: self-attention with co-attention encoder architecture. (c) DETR: encoder-decoder architecture.\ngards only the last layer’s attention map as the relevancy\nmap, e.g. Rtt = Att, where Att is the last text self-\nattention map. The second is rollout, which follows [1]\nfor all the self-attention layers. Since the rollout baseline\nis based solely on self-attention, to distinguish from raw at-\ntention, we employ the following for Rsq, s, q∈{t, i}:\nRsq = (Rss)⊤·¯A ·Rqq (12)\nwhere Rss, Rqq are the self-attention relevancies computed\nby rollout, and ¯A ∈Rs×q is the last bi-attention map. For\nour gradient baselines, we use the Grad-CAM [32] adap-\ntation described in [5], i.e., we examine the last attention\nlayer, and perform Grad-CAM on the attention map’s heads.\nLastly, our relevancy map baselines include partial LRP, fol-\nlowing [41], which uses the LRP relevancy values of the last\nattention layer to average across the heads, and the Trans-\nformer attribution method described in [5]. The method in\n[5] employs Eq. 5 for all attention layers in order to average\nacross heads, in the following way:\n¯A = Eh((∇A ⊙RA)+) (13)\nwhere the only difference compared to Eq. 5, is that [5] uses\nthe LRP [3] relevancy values ofA, i.e. RA, instead of using\nthe raw attention maps as done in Eq. 5. Additionally, [5]\nuses Eq. 6 for all self-attention layers. For non self-attention\nlayers, our version of [5] takes the last attention map, and\naverages across heads using Eq. 13. Note that while apply-\ning our method only requires a few simple hooks for the\nattention modules, LRP requires a custom implementation\nof all network layers.\n5. Experiments\nOur experiments include three Transformer-based mod-\nels, each representing one of the three types of architectures\nwe refer to in this work. See Fig. 2 for illustrations of each\nof the architectures. In addition, to compare with previous\nwork [5, 1] in the same setting for which these methods\nwere conceived, we also consider ViT [9]. The relevancy\npropagation for each model follows Sec. 3.2.\nThe ﬁrst model we examine is VisualBERT [18], which\nrepresents a self-attention based architecture, and the sec-\nond model is LXMERT [38], which represents an architec-\nture combining self-attention and co-attention in a Trans-\nformer encoder for two modalities.\nFor both models, we perform positive and negative per-\nturbation tests on each modality separately to evaluate the\nquality of the relevancy matrices produced by the methods.\nWe use the visual question answering [2] task in testing the\nexplanations since this task requires the models to demon-\nstrate an understanding of both input modalities and the\nconnections between them.\nThe perturbation tests are performed as follows: ﬁrst, a\npre-trained network is used for extracting relevancy maps\nfor 10, 000 randomly picked samples from the validation set\nof the VQA dataset. Second, we gradually remove the to-\nkens of a given modality and measure the mean top-1 accu-\nracy of the network. In positive perturbation, tokens are re-\nmoved from the highest relevance to the lowest, while in the\nnegative version, from lowest to highest. In positive pertur-\nbation, one expects to see a steep decrease in performance,\nwhich indicates that the removed tokens are important to\nthe classiﬁcation score. In negative perturbation, a good ex-\nplanation would maintain the accuracy of the model while\nremoving tokens that are not related to the classiﬁcation. In\nboth cases, we measure the area-under-the-curve (AUC), to\nevaluate the decrease in the model’s accuracy.\nWe note that in all perturbation tests, the accuracy does\nnot reach 0%, even when removing 100% of the tokens of\neach modality. This is since the input from the other modal-\nity remains intact therefore the models can rely on a single\nmodality to provide a reasonable answer.\nNotice that the LXMERT [38] image perturbation test\nresults, which are depicted in Fig. 4(a,b), demonstrate a\n5\n0 25 50 75 100\n% of tokens removed\n40\n50\n60\n70Accuracy\nOurs (AUC: 63.24)\nTrans. att. (61.46)\nraw att. (61.34)\npartial LRP (60.90)\nGrad-CAM (60.08)\nrollout (58.64)\n0 25 50 75 100\n% of tokens removed\n40\n45\n50\n55\n60\n65\n70Accuracy\nOurs (AUC: 51.1)\nTrans. att. (52.75)\nraw att. (54.17)\npartial LRP (52.82)\nGrad-CAM (59.21)\nrollout (57.23)\n0 25 50 75 100\n% of tokens removed\n10\n20\n30\n40\n50\n60\n70Accuracy\nOurs (AUC: 48.70)\nTrans. att. (48.24)\nraw att. (38.32)\npartial LRP (45.15)\nGrad-CAM (37.99)\nrollout (32.05)\n0 25 50 75 100\n% of tokens removed\n10\n20\n30\n40\n50\n60\n70Accuracy\nOurs (AUC: 21.61)\nTrans. att. (21.68)\nraw att. (32.56)\npartial LRP (24.22)\nGrad-CAM (34.14)\nrollout (39.29)\n(a) (b) (c) (d)\nFigure 3: LXMERT perturbation test results. For negative perturbation, larger AUC is better; for positive perturbation,\nsmaller AUC is better. (a) negative perturbation on image tokens, (b) positive perturbation on image tokens, (c) negative\nperturbation on text tokens, and (d) positive perturbation on text tokens.\nis the animal eating ? did he catch the ball ? is the tub white ? did the man just catch\nthe frisbee ?\nOurs\nis the animal eating ? did he catch the ball ? is the tub white ? did the man just catch\nthe frisbee ?\nTrans. att. [5]\nFigure 4: A comparison between our method (top) and the method of [5] (bottom) for VQA with the LXMERT model.\nRelevancy for text is given as shades of red. Relevancy for images is given by multiplying each region by the relative\nrelevancy. The results for the text part are similar. For the images, our method provides much more focused results. Both\nobservations are aligned with the quantitative results. Answers (left to right): no, yes, yes, no.\n0 25 50 75 100\n% of tokens removed\n40\n45\n50\n55\n60\n65\n70Accuracy\nOurs (AUC: 68.40)\nTrans. att. (68.19)\nraw att. (67.68)\npartial LRP (67.42)\nGrad-CAM (65.89)\nrollout (67.44)\n0 25 50 75 100\n% of tokens removed\n40\n45\n50\n55\n60\n65\n70Accuracy\nOurs (AUC: 49.90)\nTrans. att. (49.39)\nraw att. (50.85)\npartial LRP (51.22)\nGrad-CAM (63)\nrollout (58.41)\n0 25 50 75 100\n% of tokens removed\n20\n30\n40\n50\n60\n70Accuracy\nOurs (AUC: 52.79)\nTrans. att. (49.68)\nraw att. (46.99)\npartial LRP (46.51)\nGrad-CAM (44.31)\nrollout (40.93)\n0 25 50 75 100\n% of tokens removed\n20\n30\n40\n50\n60\n70Accuracy\nOurs (AUC: 31.66)\nTrans. att. (33.91)\nraw att. (36.52)\npartial LRP (36.85)\nGrad-CAM (41.45)\nrollout (40.77)\n(a) (b) (c) (d)\nFigure 5: VisualBERT perturbation test results. For negative perturbation, larger AUC is better; positive perturbation, smaller\nAUC is better. (a) negative perturbation on image tokens, (b) positive perturbation on image tokens, (c) negative perturbation\non text tokens, and (d) positive perturbation on text tokens.\n6\nSupervised Weakly supervised segmentation\ndetection rollout [1] raw Grad-CAM [32] partial Trans. Oursattention LRP [41] attribution [5]\nAP 51.8 0.1 5.6 2.3 4.7 7.2 13.1 (+5.9)\nAPmedium 56.3 0.1 9.6 2.3 8.0 10.4 14.4 (+4.0)\nAPlarge 67.6 0.2 6.9 4.7 5.1 12.4 24.6 (+12.2)\nAR 67.4 0.4 11.7 5.5 10.4 13.4 19.3 (+5.9)\nARmedium 72.8 0.1 21.8 5.9 19.9 21.0 23.9 (+2.1)\nARlarge 85.1 0.9 10.8 10.7 8.0 19.4 33.2 (+13.8)\nTable 1: DETR [49]-based weakly supervised segmentation results on the MSCOCO [20] validation set, higher is better.\nAP=average precision, AR=average recall. The subscripts indicate benchmark subsets. The ﬁrst column is the DETR [49]\nbounding boxes detection scores obtained for each category, while the rest of the columns are for segmentation maps.\nDetection\nbounding\nbox\nOurs\nTrans. att. [5]\npartial LRP [41]\nGrad-CAM [32]\nraw att.\nrollout [1]\nFigure 6: Sample segmentation masks for DETR [49]. Each row represents a method. Detections (from left to right): remote,\nremote, cat, cat, person, skis, airplane, bus, oven (in the last two samples, the bounding box is almost the entire frame). Our\nmethod produces the most accurate results, and the segmentations are consistent with the detections produced by DETR.\nclear advantage to our method compared to other methods.\nFor negative perturbation, the AUC using our method is\nthe largest by a sizeable margin, and the accuracy is well-\npreserved even after removing more than 80% of the image\ntokens, and for positive perturbation, notice the very steep\ndecrease in accuracy, and the low AUC.\nAs can be seen in Fig. 2(b), the [CLS] token for\nLXMERT [38] is the ﬁrst token of the text modality, thus\nfollowing Sec. 3.2, Rti is the map used for extracting rel-\nevancies in the image perturbation case. Since Rti is a\nmulti-modal relevancy map, the image perturbation tests\nbest demonstrate the advantage of using our method over\nall existing methods, which fall short in evaluating relevan-\ncies from the co-attention modules.\nFor the LXMERT [38] text perturbation tests which are\ndepicted in Fig. 4(c,d), notice that by Sec. 3.2, we visual-\nize Rtt which is a self-attention map, where the dominat-\ning update rule is Eq. 6. This rule is identical to the rule\nemployed by the Transformer attribution [5] baseline, ex-\ncept for the head averaging in Eq. 13. Therefore, the main\ndifference between our proposed method and the method\ndescribed in [5] is the choice to use LRP [3] in the head av-\neraging process. This results in very similar results for both\nmethods. For completeness, we provide in the supplemen-\ntary results for our method when adding LRP, as is done in\nEq. 13. The rest of the methods fall far behind.\n7\nrollout raw att. GCAM LRP T. Attr Ours\nN Predicted 53.10 45.55 41.52 50.49 54.16 54.61\nTarget - - 42.02 50.49 55.04 55.67\nP Predicted 20.05 23.99 34.06 19.64 17.03 17.32\nTarget - - 33.56 19.64 16.04 16.72\nTable 2: ViT [9] positive (P) and negative (N) perturbation\nAUC results for the predicted and target classes, on the Im-\nageNet [31] validation set. For negative perturbation, larger\nAUC is better; positive perturbation, smaller AUC is better.\nGCAM=Grad-CAM; T. Attr = Transformer attribution [5].\nFig. 4 presents typical results for our method and for\nTransformer attribution [5]. The rest of the methods are not\ncompetitive and their matching samples are presented in the\nsupplementary. As can be seen, the text results are similar,\nas predicted by the quantitative results. Our image attention\nresults are much more focused on the relevant image parts\nthan those of the baseline method.\nNote that since VisualBERT [18] is based on pure self-\nattention, the difference between our method and the Trans-\nformer attribution [5] method stems from the choice of\nwhether or not to use LRP [3] for head averaging in Eq. 5,\nsimilarly to the LXMERT [38] text (but not image) pertur-\nbation tests. As can be seen in Fig. 5, our method outper-\nforms all methods and achieves very similar results to those\nof [5], and in some cases, such as the text perturbation test,\neven outperforms [5] by a sizeable margin. This demon-\nstrates that the use of LRP [3] is unnecessary, even for pure\nself-attention architectures.\nThe third model we experiment on is DETR [4], which\nis an encoder-decoder model, as seen in Fig. 2(c). We use\na pre-trained DETR model with the ImageNet pre-trained\nbackbone ResNet-50, which is trained for object detection\non the MSCOCO [20] dataset. Importantly, this model\nhas only been trained for object detection, i.e., producing\nbounding boxes and classiﬁcations for each object in the in-\nput image. To evaluate the different explainability methods,\nour test uses each of the methods on the 5, 000 samples of\nthe MSCOCO [20] validation set to produce segmentation\nmasks, i.e. we consider the output of each method to be a\nsegmentation mask. We ﬁrst ﬁlter the queries to include\nonly ones where the classiﬁcation probability is higher than\n50% and then employ Otsu’s thresholding method [26] to\nseparate the foreground and the background of the segmen-\ntation. See supplementary for the full details.\nOur generated segmentation masks visualize the bound-\ning boxes predicted by DETR, therefore it should be noted\nthat the produced masks are inherently dependent on the\nquality of the corresponding bounding boxes, i.e., when\nthe predicted bounding box is not sufﬁcient, naturally, the\nmask produced for it will be at least equally inaccurate.\nIn addition, since the explainability methods are not aimed\nat producing segmentation maps, they often do not output\ncontiguous masks, and the Otsu threshold may also create\n”holes” in the produced masks. For all the reasons above,\nwe decrease the minimal IoU used for MSCOCO evaluation\nfrom 0.5 to 0.2, which signiﬁcantly beneﬁts all the methods,\nand we present the results of the MSCOCO segmentation\nevaluation for the categories where the produced bounding\nboxes are good enough for the generation of segmentation\nmasks, e.g., we do not present results for small objects 1.\nAs can be seen in Tab. 1, our method outperforms all other\nmethods by a very large margin, which indicates that our\nnovel formulations are necessary for non self-attention ar-\nchitectures. Notice the correlation in Tab. 1 between the\nbounding box evaluation for DETR and our segmentation.\nSee Fig. 6 for visualizations of the masks.\nLastly, in order to compare our method with exist-\ning single-modality baselines, we present the positive and\nnegative perturbation tests on ViT-Base [9], as performed\nby [5]. As mentioned, since ViT-Base [9] is a single-\nmodality Transformer encoder, the only difference between\nour method and the Transformer attribution method of [5] is\nthe use of LRP [3] in Eq. 5, as shown in Eq. 13. Therefore,\nas can be seen in Tab. 2, the differences between our method\nand the method proposed in [5] are very mild, which is an-\nother indication that LRP [3] can be removed. Tab. 2 also\nshows improvement in performance when using the target\nclass instead of the predicted class for gradient propagation\nin Eq. 5, which, as stated in [5], indicates that our method\nis able to produce class-speciﬁc visualizations.\nAblation study We present in the supplementary three\nvariations of our method that demonstrate the effectiveness\nof our normalization (Eq. 8,9), the necessity of the aggre-\ngation in all our rules 6, 7, 10, 11, and the need for the\nself-attention updates to the bi-modal rule 10.\n6. Conclusions\nTransformers play an increasingly dominant role in com-\nputer vision, with image-text Transformers and Transform-\ners that perform tasks that have output domains that are\nmore complex than the labels provided by a classiﬁer, pre-\nsenting groundbreaking results. In order to debug such\nmodels, as well as to support downstream tasks, and the in-\ncreasing demand for model-interpretability, it is required to\nhave complete and accurate explainability methods. How-\never, the current explainability literature for Transformers\nis limited, overly focuses on pure attention maps, and lacks\nthe methodology for treating co-attention maps.\n1We choose this working point since using a stricter threshold leads to\nbaseline results that are slightly better than chance and our method outper-\nforms but provides a score that is only 2-3 times better than chance.\n8\nOur method carefully tracks the evolution and mixing of\nthe attention maps. It provides a generic prescription that\nis applicable to all attention models we are aware of. Em-\npirically, it outperforms the existing methods across Trans-\nformer architectures and evaluation metrics. In some cases,\nwhen self-attention is prominent, the recent method by\nChefer et al. [5] is the only method that can provide compa-\nrable results. However, in the majority of the experiments,\nour method leads over all methods by a very sizable margin.\nAcknowledgment\nThis project has received funding from the European Re-\nsearch Council (ERC) under the European Unions Horizon\n2020 research and innovation programme (grant ERC CoG\n725974). The contribution of the ﬁrst author is part of a\nMaster thesis research conducted at Tel Aviv University.\nReferences\n[1] Samira Abnar and Willem Zuidema. Quantifying atten-\ntion ﬂow in transformers. arXiv preprint arXiv:2005.00928,\n2020. 1, 2, 3, 5, 7\n[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi\nParikh. Vqa: Visual question answering. In Proceedings\nof the IEEE International Conference on Computer Vision\n(ICCV), December 2015. 5\n[3] Sebastian Bach, Alexander Binder, Gr ´egoire Montavon,\nFrederick Klauschen, Klaus-Robert M ¨uller, and Wojciech\nSamek. On pixel-wise explanations for non-linear classi-\nﬁer decisions by layer-wise relevance propagation.PloS one,\n10(7):e0130140, 2015. 2, 5, 7, 8\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. arXiv preprint\narXiv:2005.12872, 2020. 1, 2, 4, 8\n[5] Hila Chefer, Shir Gur, and Lior Wolf. Transformer inter-\npretability beyond attention visualization. arXiv preprint\narXiv:2012.09838, 2020. 1, 2, 3, 4, 5, 6, 7, 8, 9\n[6] Jianbo Chen, Le Song, Martin J. Wainwright, and Michael I.\nJordan. L-shapley and c-shapley: Efﬁcient model interpre-\ntation for structured data. In International Conference on\nLearning Representations, 2019. 2\n[7] Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Hee-\nwoo Jun, Prafulla Dhariwal, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In Proceedings of the\n37th International Conference on Machine Learning , vol-\nume 1, 2020. 2\n[8] Piotr Dabkowski and Yarin Gal. Real time image saliency\nfor black box classiﬁers. In Advances in Neural Information\nProcessing Systems, pages 6970–6979, 2017. 2\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 2, 5, 8\n[10] Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal\nVincent. Visualizing higher-layer features of a deep network.\nUniversity of Montreal, 1341(3):1, 2009. 2\n[11] Ruth Fong, Mandela Patrick, and Andrea Vedaldi. Un-\nderstanding deep networks via extremal perturbations and\nsmooth masks. In Proceedings of the IEEE International\nConference on Computer Vision, pages 2950–2958, 2019. 2\n[12] Ruth C Fong and Andrea Vedaldi. Interpretable explanations\nof black boxes by meaningful perturbation. In Proceedings\nof the IEEE International Conference on Computer Vision ,\npages 3429–3437, 2017. 2\n[13] Jindong Gu, Yinchong Yang, and V olker Tresp. Understand-\ning individual decisions of cnns via contrastive backpropaga-\ntion. In Asian Conference on Computer Vision, pages 119–\n134. Springer, 2018. 2\n[14] Shir Gur, Ameen Ali, and Lior Wolf. Visualization of su-\npervised and self-supervised neural networks via attribution\nguided factorization. In AAAI, 2021. 2\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770–778, 2016. 1\n[16] Brian Kenji Iwana, Ryohei Kuroki, and Seiichi Uchida.\nExplaining convolutional neural networks using softmax\ngradient layer-wise relevance propagation. arXiv preprint\narXiv:1908.04351, 2019. 2\n[17] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvinine-\njad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and\nLuke Zettlemoyer. Bart: Denoising sequence-to-sequence\npre-training for natural language generation, translation, and\ncomprehension. arXiv preprint arXiv:1910.13461, 2019. 1,\n2, 4\n[18] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,\nand Kai-Wei Chang. Visualbert: A simple and perfor-\nmant baseline for vision and language. arXiv preprint\narXiv:1908.03557, 2019. 1, 2, 4, 5, 8\n[19] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei\nHu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu\nWei, et al. Oscar: Object-semantics aligned pre-training for\nvision-language tasks. InEuropean Conference on Computer\nVision, pages 121–137. Springer, 2020. 1, 2, 4\n[20] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision , pages 740–755.\nSpringer, 2014. 7, 8\n[21] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vil-\nbert: Pretraining task-agnostic visiolinguistic representations\nfor vision-and-language tasks. In Advances in Neural Infor-\nmation Processing Systems, pages 13–23, 2019. 1, 2, 4\n[22] Scott M Lundberg and Su-In Lee. A uniﬁed approach to\ninterpreting model predictions. In Advances in Neural Infor-\nmation Processing Systems, pages 4765–4774, 2017. 2\n[23] Aravindh Mahendran and Andrea Vedaldi. Visualizing deep\nconvolutional neural networks using natural pre-images. In-\n9\nternational Journal of Computer Vision , 120(3):233–255,\n2016. 2\n[24] Gr ´egoire Montavon, Sebastian Lapuschkin, Alexander\nBinder, Wojciech Samek, and Klaus-Robert M ¨uller. Ex-\nplaining nonlinear classiﬁcation decisions with deep taylor\ndecomposition. Pattern Recognition, 65:211–222, 2017. 2\n[25] Woo-Jeoung Nam, Shir Gur, Jaesik Choi, Lior Wolf, and\nSeong-Whan Lee. Relative attributing propagation: Inter-\npreting the comparative contributions of individual units in\ndeep neural networks. arXiv preprint arXiv:1904.00605 ,\n2019. 2\n[26] Nobuyuki Otsu. A threshold selection method from gray-\nlevel histograms. IEEE transactions on systems, man, and\ncybernetics, 9(1):62–66, 1979. 8\n[27] Matthieu Paul, Martin Danelljan, Luc Van Gool, and Radu\nTimofte. Local memory attention for fast video semantic\nsegmentation. arXiv preprint arXiv:2101.01715, 2021. 1, 4\n[28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. arXiv preprint arXiv:2103.00020, 2021. 1\n[29] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. Exploring the limits of transfer learn-\ning with a uniﬁed text-to-text transformer. arXiv preprint\narXiv:1910.10683, 2019. 2\n[30] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya\nSutskever. Zero-shot text-to-image generation. arXiv\npreprint arXiv:2102.12092, 2021. 1\n[31] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al. Imagenet large\nscale visual recognition challenge. International journal of\ncomputer vision, 115(3):211–252, 2015. 8\n[32] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das,\nRamakrishna Vedantam, Devi Parikh, and Dhruv Batra.\nGrad-cam: Visual explanations from deep networks via\ngradient-based localization. In Proceedings of the IEEE in-\nternational conference on computer vision , pages 618–626,\n2017. 2, 5, 7\n[33] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje.\nLearning important features through propagating activation\ndifferences. In Proceedings of the 34th International Con-\nference on Machine Learning-Volume 70, pages 3145–3153.\nJMLR. org, 2017. 2\n[34] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.\nDeep inside convolutional networks: Visualising image\nclassiﬁcation models and saliency maps. arXiv preprint\narXiv:1312.6034, 2013. 2\n[35] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi ´egas,\nand Martin Wattenberg. Smoothgrad: removing noise by\nadding noise. arXiv preprint arXiv:1706.03825, 2017. 2\n[36] Suraj Srinivas and Franc ¸ois Fleuret. Full-gradient repre-\nsentation for neural network visualization. In Advances in\nNeural Information Processing Systems , pages 4126–4135,\n2019. 2\n[37] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic\nattribution for deep networks. In Proceedings of the 34th\nInternational Conference on Machine Learning-Volume 70 ,\npages 3319–3328. JMLR. org, 2017. 2\n[38] Hao Tan and Mohit Bansal. Lxmert: Learning cross-\nmodality encoder representations from transformers. arXiv\npreprint arXiv:1908.07490, 2019. 1, 2, 4, 5, 7, 8\n[39] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. arXiv preprint arXiv:2012.12877, 2020. 2\n[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998–6008, 2017. 1,\n2, 3, 4\n[41] Elena V oita, David Talbot, Fedor Moiseev, Rico Sennrich,\nand Ivan Titov. Analyzing multi-head self-attention: Spe-\ncialized heads do the heavy lifting, the rest can be pruned. In\nProceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 5797–5808, 2019. 2,\n3, 5, 7\n[42] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille,\nand Liang-Chieh Chen. Max-deeplab: End-to-end panop-\ntic segmentation with mask transformers. arXiv preprint\narXiv:2012.00759, 2020. 1, 4\n[43] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen,\nBaoshan Cheng, Hao Shen, and Huaxia Xia. End-to-\nend video instance segmentation with transformers. arXiv\npreprint arXiv:2011.14503, 2020. 1, 4\n[44] Matthew D Zeiler and Rob Fergus. Visualizing and under-\nstanding convolutional networks. InEuropean conference on\ncomputer vision, pages 818–833. Springer, 2014. 2\n[45] Jianming Zhang, Sarah Adel Bargal, Zhe Lin, Jonathan\nBrandt, Xiaohui Shen, and Stan Sclaroff. Top-down neu-\nral attention by excitation backprop. International Journal\nof Computer Vision, 126(10):1084–1102, 2018. 2\n[46] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip HS Torr, et al. Rethinking semantic segmen-\ntation from a sequence-to-sequence perspective with trans-\nformers. arXiv preprint arXiv:2012.15840, 2020. 2\n[47] Bolei Zhou, David Bau, Aude Oliva, and Antonio Torralba.\nInterpreting deep visual representations via network dissec-\ntion. IEEE transactions on pattern analysis and machine\nintelligence, 2018. 2\n[48] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,\nand Antonio Torralba. Learning deep features for discrimina-\ntive localization. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 2921–2929,\n2016. 2\n[49] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable {detr}: Deformable transform-\ners for end-to-end object detection. In International Confer-\nence on Learning Representations, 2021. 1, 4, 7\n10\nA. Code\nThe code contains Jupyter notebooks with the examples presented for LXMERT and DETR. Both notebooks also allow\nusing images from the internet. For LXMERT, we also support the option of asking a free form question.\nFigure 1: LXMERT examples from the Jupyter notebook. The notebook contains both the examples from the paper (top),\nand examples of uploaded images and free form questions (bottom).\n11\nFigure 2: DETR example from the Jupyter notebook. The notebook contains the examples from the paper.\nB. Extended LXMERT VQA visual results\nIn Fig. 3 we present extended results for Fig. 4 in the paper, i.e. we present the explanations extracted by each method for\ntypical samples from the VQA dataset using the LXMERT model for question answering.\nC. Preparing the DETR relevancy maps for the COCO segmentation evaluation code\nIn this section, we elaborate on the process of extracting segmentation masks from DETR’s object detection results. The\nextracted segmentation masks are then used for our DETR tests, as presented in Sec. 5 of the paper.\nDETR has been trained for object detection,i.e., producing a bounding box and a classiﬁcation for each object in the input\nimage. In order to evaluate the different explainability methods, we refer to theRqi relevancy map, where thej-th row deﬁnes\nthe relevance of each image feature to the j-th query, i.e. the j-th bounding box, as described in Sec. 3.2 of the paper. Our\ntest uses each of the explainability methods on the 5, 000 samples of the MSCOCO validation set to produce segmentation\nmasks, as described in Alg. 1. We ﬁrst ﬁlter the queries to include only ones where the classiﬁcation probability is higher\nthan 50% (Alg. 1, L. 3). Then, for each query j that is left, we use the relevancy matrix Rqi in row j as a heatmap of the\nimage features (Alg. 1, L. 6), noting the important pixels for thej-th predicted bounding box. Since most of our baselines, as\nwell as our method, produce non-negative relevancies, we use Otsu’s thresholding method to separate the foreground and the\nbackground of the segmentation mask (Alg. 1, L. 7). Then, the DETR segmentation evaluation code upsamples the masks\nto the target mask size, followed by a sigmoid operation, which only leaves the strictly positive values of the segmentation\nmap (Alg. 1, L. 8-9). Finally, the DETR segmentation evaluation code upsamples the generated map back to the size of the\noriginal image (Alg. 1, L. 10).\nAlgorithm 1 Obtain Segmentation Masks from Heatmaps\nInput : (i) input image (ii) logits ∈Rq×c obtained by the detection alg., whereq is the number of queries (bounding boxes),\nand c is the number of object classes, (iii) Rqi- relevancy matrix per query, from the explainability alg.\nOutput : masks ∈Rq×h×w where q is the number of queries, and h, ware the spatial dimensions of the input image.\nmasks[j] is the segmentation map corresponding to the j-th bounding box.\n1: q ←queries\n2: probabilities ←softmax(logits)\n3: keep ←j ∈q, where max(probabilities[j]) > 0.5\n4: masks ←[[0, ...,0], ...,[0, ...,0]]\n5: for j ∈keep:\n6: masks[j] ←Rqi[j]\n7: masks[j] ←Otsu(masks[j])\n8: masks[j] ←Upsample(masks[j], size=targetMaskSize, method=”bilinear”)\n9: masks[j] ←sigmoid(masks[j]) > 0.5\n10: masks[j] ←Upsample(masks[j], size=origImageSize, method=”nearest”)\n12\nis the animal eating ? did he catch the ball ? is the tub white ? did the man just catch\nthe frisbee ?\nOurs\nis the animal eating ? did he catch the ball ? is the tub white ? did the man just catch\nthe frisbee ?\nTrans. att.\nis the animal eating ? did he catch the ball ? is the tub white ? did the man just catch\nthe frisbee ?\npartial LRP\nis the animal eating ? did he catch the ball ? is the tub white ? did the man just catch\nthe frisbee ?\nGrad-CAM\nis the animal eating ? did he catch the ball ? is the tub white ? did the man just catch\nthe frisbee ?\nraw attn.\nis the animal eating ? did he catch the ball ? is the tub white ? did the man just catch\nthe frisbee ?\nrollout\nFigure 3: A comparison between our method (top) and the baselines for VQA with the LXMERT model. Relevancy for text\nis given as shades of red. Relevancy for images is given by multiplying each region by the relative relevancy. Notice that\nboth for the images and the text our method achieves favorable results. Answers (left to right): no, yes, yes, no.\n13\nD. Ablation Study\nOurs w/o norm. w/o aggregation Eq.10 w/o self-att.\nAP 13.1 11.7 0.1 11.5\nAPmedium 14.4 13.9 0.0 13.8\nAPlarge 24.6 20.9 0.2 20.5\nAR 19.3 18.0 0.5 17.8\nARmedium 23.9 23.9 0.0 23.8\nARlarge 33.2 29.2 1.0 28.6\nTable 3: Performance for different ablation variants of our method on the DETR experiments. Higher is better.\nOurs w/o norm. w/o aggregation Eq.10 w/o self-att.\nNeg. img 63.24 62.49 60.41 62.18\nPos. img 51.10 50.60 60.40 50.57\nNeg. text 48.70 48.64 41.72 48.64\nPos. text 21.61 21.59 41.72 21.59\nTable 4: Area-under-the-curve for different ablation variants of our method on the LXMERT experiments. For negative\nperturbation, larger AUC is better; for positive perturbation, smaller AUC is better.\nWe present three variations of our method. Firstly, we verify the effectiveness of our normalization for the self-attention\nrelevancies presented in Eq. 8,9. Since the normalization is applied to rule 10, we expect it to affect mostly bi-modal\nrelevancies, i.e. the image perturbation experiments for LXMERT, and the DETR tests. The second ablation we present\nstudies the necessity of the aggregation in all our rules 6,7,10,11, i.e. instead of adding the former relevancy matrix to the\nnewly constructed one, we only keep the new one,e.g. for rule 6 the update becomes: Rss = ¯A ·Rss. Lastly, we explore the\nneed for the self-attention updates to the bi-modal rule 10 by changing the update rule to:Rsq = Rsq + ¯A. All our ablations\nare done on the LXMER, DETR experiments since, as mentioned several times, VisualBERT is based on pure self-attention,\nwhich yields similar results to the Transformer attribution baseline.\nAs can be seen from Tab. 3, all the components included in our method are crucial to its success on DETR, and the ablations\ncause a sizeable decrease in performance. It should be noted that for the reasonable ablations of not using normalization and\nnot using self-attention in Eq.10, our ablations still outperform all other methods signiﬁcantly for the DETR experiment.\nFor the image perturbation test on LXMERT, presented in Tab. 4, we observe relatively mild differences between our\nmethod and the ablations of no normalization and no self-attention, this can be attributed to the fact that in contrast to DETR,\nLXMERT only uses 36 image regions that had gone through Non-maximum Suppression (NMS), therefore the added context\nfrom the self-attention to the multi-modal attention is not as crucial, since usually the top-1 image region is identical to that\nof the ablations, and is sufﬁcient to make the classiﬁcation.\nE. Using LRP with our method\nWe present the results for the LXMERT perturbation tests evaluated by the area-under-the-curve measure for our method\nwith LRP for completeness, i.e. with head averaging as presented in the Transformer attribution method and in Eq. 13 instead\nof the head averaging in Eq. 5. The results in Tab. 5 support and substantiate the conclusions presented in the paper: for\nthe image perturbation tests, whether or not LRP is used, our method’s contributions lead to a large gap in performance over\nall baseline methods. LRP itself leads to a small degradation in performance. For the text perturbation tests which are, as\nmentioned in the paper, self-attention based, our method is similar in performance to the Transformer Attribution method.\nHere, too, the choice of whether or not to use LRP is insigniﬁcant. Given the complexity of implementing LRP (see Sec. 4\nof the main text), we advocate to eliminate it.\nF. Perturbation experiments graphs\nIn Fig. 4, 5, we present enlarged graphs corresponding to our perturbation experiments for better clarity.\n14\nOurs Ours w/ LRP Transformer att. raw attn. partial LRP Grad-CAM rollout\nNeg. img 63.24 62.41 61.46 61.34 60.90 60.08 58.64\nPos. img 51.10 51.10 52.75 54.17 52.82 59.21 57.23\nNeg. text 48.70 48.25 48.24 38.32 45.15 37.99 32.05\nPos. text 21.61 21.68 21.68 32.56 24.22 34.14 39.29\nTable 5: Area-under-the-curve for all the baselines and our method with and without LRP on the LXMERT experiments. For\nnegative perturbation, larger AUC is better; for positive perturbation, smaller AUC is better.\n0 20 40 60 80 100\n% of tokens removed\n35\n40\n45\n50\n55\n60\n65\n70Accuracy\nOurs (AUC: 63.24)\nTrans. att. (61.46)\nraw att. (61.34)\npartial LRP (60.90)\nGrad-CAM (60.08)\nrollout (58.64)\n0 20 40 60 80 100\n% of tokens removed\n40\n45\n50\n55\n60\n65\n70Accuracy\nOurs (AUC: 51.1)\nTrans. att. (52.75)\nraw att. (54.17)\npartial LRP (52.82)\nGrad-CAM (59.21)\nrollout (57.23)\n(a) (b)\n0 20 40 60 80 100\n% of tokens removed\n10\n20\n30\n40\n50\n60\n70Accuracy\nOurs (AUC: 48.70)\nTrans. att. (48.24)\nraw att. (38.32)\npartial LRP (45.15)\nGrad-CAM (37.99)\nrollout (32.05)\n0 20 40 60 80 100\n% of tokens removed\n10\n20\n30\n40\n50\n60\n70Accuracy\nOurs (AUC: 21.61)\nTrans. att. (21.68)\nraw att. (32.56)\npartial LRP (24.22)\nGrad-CAM (34.14)\nrollout (39.29)\n(c) (d)\nFigure 4: LXMERT perturbation test results. For negative perturbation, larger AUC is better; for positive perturbation,\nsmaller AUC is better. (a) negative perturbation on image tokens, (b) positive perturbation on image tokens, (c) negative\nperturbation on text tokens, and (d) positive perturbation on text tokens.\n15\n0 20 40 60 80 100\n% of tokens removed\n40\n45\n50\n55\n60\n65\n70Accuracy\nOurs (AUC: 68.40)\nTrans. att. (68.19)\nraw att. (67.68)\npartial LRP (67.42)\nGrad-CAM (65.89)\nrollout (67.44)\n0 20 40 60 80 100\n% of tokens removed\n40\n45\n50\n55\n60\n65\n70Accuracy\nOurs (AUC: 49.90)\nTrans. att. (49.39)\nraw att. (50.85)\npartial LRP (51.22)\nGrad-CAM (63)\nrollout (58.41)\n(a) (b)\n0 20 40 60 80 100\n% of tokens removed\n20\n30\n40\n50\n60\n70Accuracy\nOurs (AUC: 52.79)\nTrans. att. (49.68)\nraw att. (46.99)\npartial LRP (46.51)\nGrad-CAM (44.31)\nrollout (40.93)\n0 20 40 60 80 100\n% of tokens removed\n20\n30\n40\n50\n60\n70Accuracy\nOurs (AUC: 31.66)\nTrans. att. (33.91)\nraw att. (36.52)\npartial LRP (36.85)\nGrad-CAM (41.45)\nrollout (40.77)\n(c) (d)\nFigure 5: VisualBERT perturbation test results. For negative perturbation, larger AUC is better; for positive perturbation,\nsmaller AUC is better. (a) negative perturbation on image tokens, (b) positive perturbation on image tokens, (c) negative\nperturbation on text tokens, and (d) positive perturbation on text tokens.\n16"
}