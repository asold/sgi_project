{
    "title": "Transformer-Based Source-Free Domain Adaptation",
    "url": "https://openalex.org/W3169909366",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2370281093",
            "name": "Yang Guang-lei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2101529693",
            "name": "Tang, Hao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2387851866",
            "name": "Zhong, Zhun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2007056435",
            "name": "Ding Mingli",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2101740539",
            "name": "Shao Ling",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2744565219",
            "name": "Sebe, Nicu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2743231409",
            "name": "Ricci Elisa",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2964109570",
        "https://openalex.org/W2963275094",
        "https://openalex.org/W3092878394",
        "https://openalex.org/W3110059533",
        "https://openalex.org/W2766897166",
        "https://openalex.org/W2946812986",
        "https://openalex.org/W2982204955",
        "https://openalex.org/W2593814746",
        "https://openalex.org/W3148140980",
        "https://openalex.org/W1722318740",
        "https://openalex.org/W2798593490",
        "https://openalex.org/W3188511781",
        "https://openalex.org/W2962835731",
        "https://openalex.org/W2487365028",
        "https://openalex.org/W3035256099",
        "https://openalex.org/W3034591020",
        "https://openalex.org/W2279034837",
        "https://openalex.org/W1903029394",
        "https://openalex.org/W2980096013",
        "https://openalex.org/W1731081199",
        "https://openalex.org/W2963542740",
        "https://openalex.org/W3035456997",
        "https://openalex.org/W2627183927",
        "https://openalex.org/W3034218934",
        "https://openalex.org/W2948429981",
        "https://openalex.org/W2605488490",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W3154062460",
        "https://openalex.org/W3127751679",
        "https://openalex.org/W2593768305",
        "https://openalex.org/W3034526587",
        "https://openalex.org/W2948959975",
        "https://openalex.org/W3137292936",
        "https://openalex.org/W2097482982",
        "https://openalex.org/W3035576098",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W2962687275",
        "https://openalex.org/W3095422700",
        "https://openalex.org/W2962986791",
        "https://openalex.org/W3122239467",
        "https://openalex.org/W3035235949",
        "https://openalex.org/W2964278684",
        "https://openalex.org/W3175146247",
        "https://openalex.org/W2970987681",
        "https://openalex.org/W3034738317",
        "https://openalex.org/W3173251459",
        "https://openalex.org/W2963532621",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W2159291411",
        "https://openalex.org/W1536680647",
        "https://openalex.org/W3034373371",
        "https://openalex.org/W22861983",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2779610669",
        "https://openalex.org/W3108516375",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2998459790",
        "https://openalex.org/W3147183491",
        "https://openalex.org/W1836465849",
        "https://openalex.org/W3159481202",
        "https://openalex.org/W2963094258",
        "https://openalex.org/W3034563784",
        "https://openalex.org/W2795155917",
        "https://openalex.org/W2739759330",
        "https://openalex.org/W3128723389",
        "https://openalex.org/W3128099838",
        "https://openalex.org/W3034324233",
        "https://openalex.org/W1934241014"
    ],
    "abstract": "In this paper, we study the task of source-free domain adaptation (SFDA), where the source data are not available during target adaptation. Previous works on SFDA mainly focus on aligning the cross-domain distributions. However, they ignore the generalization ability of the pretrained source model, which largely influences the initial target outputs that are vital to the target adaptation stage. To address this, we make the interesting observation that the model accuracy is highly correlated with whether or not attention is focused on the objects in an image. To this end, we propose a generic and effective framework based on Transformer, named TransDA, for learning a generalized model for SFDA. Specifically, we apply the Transformer as the attention module and inject it into a convolutional network. By doing so, the model is encouraged to turn attention towards the object regions, which can effectively improve the model's generalization ability on the target domains. Moreover, a novel self-supervised knowledge distillation approach is proposed to adapt the Transformer with target pseudo-labels, thus further encouraging the network to focus on the object regions. Experiments on three domain adaptation tasks, including closed-set, partial-set, and open-set adaption, demonstrate that TransDA can greatly improve the adaptation accuracy and produce state-of-the-art results. The source code and trained models are available at https://github.com/ygjwd12345/TransDA.",
    "full_text": "Transformer-Based Source-Free Domain Adaptation\nGuanglei Yang1 Hao Tang2 Zhun Zhong2 Mingli Ding1 Ling Shao3 Nicu Sebe2 Elisa Ricci14\n1Haerbin Institute of Technology 2University of Trento 3IIAI 4Fondazione Bruno Kessler\nAbstract\nIn this paper, we study the task of source-free domain adaptation (SFDA), where\nthe source data are not available during target adaptation. Previous works on SFDA\nmainly focus on aligning the cross-domain distributions. However, they ignore the\ngeneralization ability of the pretrained source model, which largely inﬂuences the\ninitial target outputs that are vital to the target adaptation stage. To address this,\nwe make the interesting observation that the model accuracy is highly correlated\nwith whether or not attention is focused on the objects in an image. To this end, we\npropose a generic and effective framework based on Transformer, named TransDA,\nfor learning a generalized model for SFDA. Speciﬁcally, we apply the Transformer\nas the attention module and inject it into a convolutional network. By doing\nso, the model is encouraged to turn attention towards the object regions, which\ncan effectively improve the model’s generalization ability on the target domains.\nMoreover, a novel self-supervised knowledge distillation approach is proposed\nto adapt the Transformer with target pseudo-labels, thus further encouraging the\nnetwork to focus on the object regions. Experiments on three domain adaptation\ntasks, including closed-set, partial-set, and open-set adaption, demonstrate that\nTransDA can greatly improve the adaptation accuracy and produce state-of-the-art\nresults. The source code and trained models are available at https://github.\ncom/ygjwd12345/TransDA.\n1 Introduction\nDeep learning has enabled several advances in various tasks in computer vision, such as image\nclassiﬁcation [12, 22], object detection [17, 51], semantic segmentation [38], and so on. However,\ndeep models suffer from signiﬁcant performance degradation when applied to an unseen target\ndomain, due to the well-documented domain shift problem [ 18]. To solve this problem, domain\nadaptation has been introduced, aiming to transfer knowledge from a fully labeled source domain to a\ntarget domain [39, 58, 69]. A common strategy in domain adaptation is to align the feature distribution\nbetween the source and target domains by minimizing the domain shift through various metrics, such\nas Correlation Distances [ 66, 57], Maximum Mean Discrepancy [ 40, 41, 29], Sliced Wasserstein\nDiscrepancy [32], and Enhanced Transport Distance [33]. Another popular paradigm leverages the\nidea of adversarial learning [20] to minimize cross-domain discrepancy [64, 34, 56, 54, 31].\nDespite the success of current domain adaptation methods, they work under the strict condition that\nthe source data are always available during training. However, such a condition has two drawbacks\nthat hinder the application of these methods. First, the source datasets, such as VisDA [ 47] and\nGTA V [52], are usually large and imply high saving and loading costs, restricting their use on certain\nplatforms, especially portable devices. Second, fully accessing the source data may violate data\nprivacy policies. To avoid this issue, companies or organizations prefer to provide the learned models\nrather than the data. Therefore, designing a domain adaptation method without requiring the source\ndatasets has great practical value. To this end, in this paper, we aim to address the recently introduced\nproblem of source-free domain adaptation [35] (SFDA), in which only the model pretrained on the\nsource and the unlabeled target dataset are provided for target adaptation.\nPreprint. Under review.\narXiv:2105.14138v1  [cs.CV]  28 May 2021\nFigure 1: Evaluation of attention for the source model on Ofﬁce-31 dataset. (a) Examples of Grad-\nCAM [16] visualization and prediction results in A→W. (b) The distribution and (c) the accuracy of\nthe source model on focused and non-focused samples in A→W and A→D.\nSo far, few works [34, 35, 1] has been proposed for SFDA, aiming to align the source and target\ndistributions by learning with underlying constraints, such as information maximization and label con-\nsistency. However, all of these perform adaptation with a model pretrained on the source data, while\nneglecting the source model’s generalization ability. In SFDA, the adaptation process largely relies on\nthe accuracy of the source model on the target domain. Without a source model that generalizes well,\nthe generated pseudo-labels may contain signiﬁcant noise, and learning with them will undoubtedly\nharm the model’s performance. In this paper, we attempt to improve the generalization ability of\nthe source model for SFDA. Different from existing out-of-domain generalization methods [49, 48],\nwhich aims to improve the model generalization by augmenting the diversity of the source samples,\nin this paper we introduce a new perspective for building a robust source model, motivated by the\nfollowing observation. In Figure 1, we directly apply the source model on the unseen target samples\n(A→W and A→D on Ofﬁce-31 [53]) and produce the heat maps by Grad-CAM [16]. We use Amazon\nMechanical Turk and ask annotators to label the samples with “focused / non-focused” according to\nwhether the heat map (red region) is localized on the object. Examples are shown in Figure 1(a). We\nobserve that the accuracy of the focused samples is much higher than that of the non-focused samples\n(see Figure 1(b, c)). This ﬁnding reveals that if a network can effectively focus on the objects in the\nimages, it will have a high prediction accuracy on these images.\nInspired by the above observation, we propose TransDA for SDFA by equipping a convolutional\nmodel with a Transformer [13] module, which can effectively encourage the network to focus on the\nobjects and thus improve the performance on target samples. Speciﬁcally, by injecting the Transformer\nafter the last convolutional layer of ResNet-50, we can leverage its long-range dependence to force\nthe model to pay more attention to the objects. Albeit simple, this modiﬁcation can signiﬁcantly\nimprove the generalization ability of the source model. In addition, during the target adaptation,\nwe propose a self-supervised knowledge distillation with generated pseudo-labels, further leading\nthe Transformer to learn to focus on the objects of target samples. We evaluate our TransDA on\nthree domain adaptation tasks, including closed-set [53], partial-set [3], and open-set [46] domain\nadaptation. Extensive results demonstrate that TransDA outperforms state-of-the-art methods on\nall tasks. For example, on the Ofﬁce-Home dataset, TransDA advances the best accuracies from\n71.8% to 79.3%, from 79.3% to 81.3%, and from 72.8% to 76.8% for the closed-set, partial-set and\nopen-set settings, respectively. To summarize, this work provides the following three contributions:\n• Through an in-depth empirical study, we reveal for the ﬁrst time the importance of the network\nattention for SFDA. This provides a new perspective for improving the generalization ability of\nthe source model.\n• We propose a Transformer-based network for SFDA, which can effectively lead the model to pay\nattention to the objects, and thus signiﬁcantly increases the model generalization ability. To our\nknowledge, we are the ﬁrst to propose Transformer for solving the domain adaptation task.\n• We introduce a novel self-supervised knowledge distillation approach to further help the Trans-\nformer to focus on target objects.\n2\n2 Related Work\nTraditional Domain Adaptation. Domain adaptation aims to improve the target learning by using\na labeled source domain that belongs to a different distribution. A number of methods have been\nproposed for unsupervised domain adaptation. One common solution is to guide the model to learn\na domain-invariant representation by minimizing the domain discrepancy [66, 57, 40, 41, 29]. For\nexample, ETD [33] employs an enhanced transport distance to reﬂect the discriminative information.\nIn a similar way, CAN [29] optimizes the network by considering the discrepancy between the intra-\nand the inter-class domains. More recently, several methods have focused on the feature discrimination\nability during domain adaptation. They introduce different normalization or penalization strategies to\nboost the feature discrimination ability on the target domain, such as batch normalization [6], batch\nspectral penalization [9], batch nuclear-norm maximization [10], and transferable normalization [62].\nAnother branch of methods exploit a generative adversarial network to address the domain confusion\n[15, 58]. BDG [64] and GVB-GD [11] use bi-directional generation to construct domain-invariant\nrepresentations. Despite the large success of the above methods, they typically require access to\nsource data when learning from the target domain. This may lead to personal information leakage\nand thus violate data privacy requirements.\nSource-Free Domain Adaptation. To alleviate the issue of data privacy, recent works [34, 35, 1]\nhave turned their attention to the problem of source-free domain adaptation, where only the pretrained\nsource model and the target samples are provided during the target adaptation stage. C3-GAN [34]\nintroduces a collaborative class conditional generative adversarial net to produce target-style training\nsamples. In addition, a clustering-based regularization is also used to obtain more discriminative\nfeatures in the target domain. Meanwhile, SHOT [35] freezes the classiﬁer module and learns the\ntarget-speciﬁc feature extractor with information maximization and self-supervised pseudo-labeling,\nwhich can align the target feature distribution with the source domain. DECISION [1] extends SHOT\nto a multi-source setting by learning different weights for each source model. Different from the\nabove methods, in this paper, we attempt to improve the generalization ability of the source model\nby injecting a Transformer module into the network. Our approach can be readily incorporated into\nmost existing frameworks to boost the adaptation accuracy.\nVision Transformers. The Transformer was ﬁrst proposed by [59] for machine translation and has\nbeen used to establish state-of-the-art results in many natural language processing tasks. Recently,\nthe Vision Transformer (ViT) [13] achieved state-of-the-art results on the image classiﬁcation task\nby directly applying Transformers with global self-attention on full-sized images. Since then,\nTransformer-based approaches have been shown to be efﬁcient in many computer vision tasks,\nincluding object detection [ 5, 72], image segmentation [ 8, 71], video inpainting [ 67], virtual try-\non [50], video classiﬁcation [ 45], pose estimation [ 26, 27], object re-identiﬁcation [ 23], image\nretrieval [14], and depth estimation [65]. Different from these approaches, in this paper, we adopt a\nTransformer-based network to tackle the source-free domain adaptation task. To this end, we propose\na generic yet straightforward TransDA framework for domain adaptation to encourage the model to\nfocus on the object regions, leading to improved domain generalization.\n3 Method\n3.1 Problem Deﬁnition\nIn traditional domain adaptation (DA), models are trained on an unlabeled target domainXt={xi\nt}M\ni=1\nand a source domain Xs={xj\ns}N\nj=1 along with corresponding labels Ys={yj\ns}N\nj=1, where yj\ns belongs\nto the set of K classes. The distributions of source and target data are denoted as xs∼pdata(xs)\nand xt∼pdata(xt), respectively, where pdata(xt)̸=pdata(xs). The goal is to learn a target network\nφt, using the labeled source data and unlabeled target data, that can accurately recognize the target\nsamples. In SFDA, (1) the labeled source data is only used to pretrain a source model φs; and (2) the\ntarget network φt is learned with the pretrained source model φs and the unlabeled target data Xt.\n3.2 Overview\nThe overall framework of the proposed method, which consists of (1) source training and (2) target\nadaptation, is shown in Figure 2. First, we train the source model φs with samples from the source\ndomain using the cross-entropy loss. The source model φs is composed of two modules: the feature\n3\nFigure 2: Overview of the proposed method. The model includes a feature extractor f and a classiﬁer\ng. We inject a Transformer module (structure is shown on the right) into f to obtain a representation\nwith improved generalization capability. First, we train the model with the labeled source data.\nThen, we create a teacher model (Tea) and a student (Stu), cloned from the source model, for target\nadaptation. The teacher model is used to produce pseudo-labels for computing the self-labeling loss\n(Lsl) and knowledge distillation loss (Lkd). We also calculate the information maximization loss\n(Lim) based on the outputs of the student model. Here, sg: stop-gradient, EMA: exponential moving\naverage, FC: fully connected layer, BN: batch normalization layer.\nextractor fs:X→Rd and the classiﬁer gs:Rd→RK, where dindicates the dimension of the output\nof the feature extractor and Krefers to the number of categories in the source data. Therefore, we\nhave φs=fs◦gs. Different from existing methods that use a convolutional neural network (CNN)\nas the feature extractor (e.g., ResNet [22]), we propose to inject a Transformer module [ 13] after\nthe last layer of the CNN, which can help the feature extractor pay more attention to the objects\nand thus produce a more robust representation. Second, we aim to learn a network φt in the target\ndomain, given the pretrained φs. Speciﬁcally, we maintain a teacher model φTea\nt and a student model\nφStu\nt , which are both initialized by the parameters of φs. Following [35], we ﬁx the classiﬁers and\nonly update the feature extractors. The updating strategies for the feature extractors are different.\nFor fStu\nt , we update it with the gradients produced by the target adaptation losses. fTea\nt is updated\nwith an exponential moving average of the parameters of fStu\nt . The teacher model φTea\nt is used to\nproduce a hard pseudo-label ˆyt and soft pseudo-label ¯yt for calculating the self-labeling loss and\nknowledge distillation loss, respectively. In addition, the information maximization loss is computed\nwith the outputs of φStu\nt . The above three losses are used to update fStu\nt , where the information\nmaximization and self-labeling losses are employed to align the cross-domain feature distributions\nand the knowledge distillation loss is designed to force the model to focus on objects.\n3.3 Model Pretraining on Source with Transformer\nLoss Function. In this stage, we aim to learn a source model with the labeled source data. Generally,\ngiven a network φs initialized on ImageNet [12], we train it with the label smoothing cross-entropy\nloss [44]:\nLce = −Exs∈Xs\nK∑\nk=1\nˆyk log σ(φs(xs)), (1)\nwhere ˆyk=(1 −α) yk+α/K. αis the smoothing factor and is empirically set to 0.1. σ(.) is the\nsoftmax operation.\nDiscussion. In SFDA, the target adaptation stage greatly relies on the target outputs obtained by\nthe source model. Hence, it is important to learn a robust source model, such as to counteract with\ndomain bias. As shown in Figure 1, the model can effectively classify the target samples if it can\n4\nlocalize the object regions. This observation is reasonable, because the model learns to capture the\ncommon object patterns instead of domain-speciﬁc information (e.g., background, color distribution)\nif it can always focus on the objects. Therefore, one solution for improving the generalization ability\nis forcing the model to focus on objects during training. Existing approaches typically select a\nCNN model as the feature extractor of φs. However, due to the intrinsic locality of the convolution\noperation (i.e., small receptive ﬁelds), the CNN model prefers to capture local information, which\nmay lead it to overﬁt on domain-speciﬁc information and thus fail to focus on objects, especially\nwhen encountering a large domain shift.\nEquipping the Model with a Transformer. To address the drawback of CNNs in SFDA, in this\npaper, we propose to inject a Transformer module after the convolutional network [13]. By doing so,\nwe can leverage the property of the Transformer to capture long-range dependencies and explicitly\nencourage the model pay more attention to the objects. This enables us to reduce the impact of\ndomain-speciﬁc information and produce more robust and transferable representations.\nSpeciﬁcally, we construct the feature extractor by injecting the Transformer after the last convolutional\nlayer of ResNet-50 [22]. Since the input of the Transformer should be a sequence, we ﬁrst reshape the\noutput of the ResNet-50 backbone F∈Rh×w×ˆd to ˆF∈Ru×ˆd, where h, w, and ˆdindicate the height,\nwidth, and dimension of F, respectively. uis the product of hand w. Then, ˆF is regarded as the\ninput sequence with patch embeddings for the Transformer.\nIn the ﬁrst layer of the Transformer, we map the dimension of the patch embeddings ˆF from ˆdto\n¯dwith a linear projection layer, producing Z0∈Ru×¯d. Then, Z0 is fed into LTransformer layers,\nwhich include multi-headed self-attention (MSA) and multi-layer perceptron (MLP) blocks.\nGiven the feature Zl−1 obtained from the l−1-th Transformer layer, MSA (Zl−1) is deﬁned as:\nMSA(Zl−1) = Zl−1+cont(AH1(LN(Zl−1)); AH2(LN(Zl−1)); ··· ; AHm(LN(Zl−1)))×W, (2)\nwhere AH represents a self-attention head [59], cont(·)) is the concentration operation, LN(·) is the\nlayer normalization [61], and mindicates the number of self-attention heads. W∈Rm·ˇd×¯d are the\nlearnable weight matrices, where ˇdis the output dimension of each AH.\nThe output of MSA is then transformed into an MLP block with a residual skip, formulated as:\nZl = MLP(LN(MSA(Zl−1))) + MSA(Zl−1). (3)\nThe structure of the Transformer layer is illustrated in the right part of Figure 2.\nGiven the output Zl∈Ru×¯d of the Transformer, we obtain the global feature by average pooling,\nwhich is fed into the following layers, including one FC layer, one BN layer, and the classiﬁer gs.\n3.4 Self-Training on Target with Transformer\nInformation Maximization. In the target adaptation stage, we are given a pretrained source model\nφs and the unlabeled target domain. We ﬁrst initialize the target model φt with the parameters of φs.\nFollowing [35], we ﬁx the classiﬁer gt to maintain the class distribution information of the source\ndomain, and update the feature extractor ft using the information maximization (IM) loss [2, 19, 25].\nThis enables us to reduce the feature distribution gap between the source and target domains. The IM\nloss consists of a conditional entropy term and a diversity term:\nLim = −Ext∈Xt\nK∑\nk=1\nσ(φt(xt)) logσ(φt(xt)) +\nK∑\nk=1\n¯pk log ¯pk, (4)\nwhere ¯p=Ext∈Xt [σ(φt(xt))] is the mean of the softmax outputs for the current batch.\nSelf-Labeling. Although the IM loss can make the predictions on the target domain more conﬁdent\nand globally diverse, it is inevitably affected by the noise generated by wrong label matching. To\naddress this issue, one solution is to utilize a self-labeling strategy to further constrain the model. In\nthis paper, we use self-supervised clustering [35, 1, 34] to generate pseudo-labels for target samples.\nSpeciﬁcally, we ﬁrst compute the centroid for each class in the target domain by weighted k-means\nclustering,\nµ(0)\nk =\n∑\nxt∈Xt σ(φt(xt))ft(xt)∑\nxt∈Xt σ(φt(xt)) . (5)\n5\nThen, the initial pseudo-labels are generated by the nearest centroid classiﬁer:\nˆyt = arg min\nk\n1 − ft(xt) ·µ(0)\nk\n||ft(xt)||2||µ(0)\nk ||2\n, (6)\nwhere ||∗|| 2 denotes the L2-norm. Finally, the class centroids and pseudo-labels are updated as\nfollows:\nµ(1)\nk =\n∑\nxt∈Xt ξ(ˆyt = k)ft(xt)∑\nxt∈Xt ξ(ˆyt) , ˆyt = arg min\nk\n1 − ft(xt) ·µ(1)\nk\n||ft(xt)||2||µ(1)\nk ||2\n, (7)\nwhere ξ(∗) is an indicator that produces 1 when the argument is true. Although the pseudo-labels\nand the centroids can be updated by Eq. (7) multiple times, we ﬁnd that one round of updating is\nsufﬁcient. Given the generated pseudo-labels, the loss function for self-labeling is calculated using\nthe cross-entropy loss, formulated by:\nLsl = −Ext∈Xt\nK∑\nk=1\nξ(ˆyt = k) logσ(φt(xt)). (8)\nSelf-Knowledge Distillation. Recall that we aim to encourage the network to focus on the objects in\norder to produce more robust feature representations. Although we inject a Transformer module into\nthe model to achieve this goal, we hope to further improve object attention ability by learning with the\ntarget samples. The above loss functions (Lim and Lsl) are designed to align the feature distribution\nof the domain, but do not explicitly consider the attention constraint. Therefore, they cannot further\nimprove object attention ability of the Transformer. DINO [ 7] showed that learning with a self-\nknowledge distillation strategy can lead the Transformer to capture more semantic information, i.e.,\npay more attention to objects. Inspired by this observation, we propose to adopt the self-knowledge\ndistillation strategy to force the model to turn more attention to objects in the target samples.\nSpeciﬁcally, we employ a teacher modelφTea\nt and a student model φStu\nt to implement self-knowledge\ndistillation. We use the teacher model φTea\nt to generate pseudo-labels and optimize the parameters of\nthe student model φStu\nt with training losses. Hence, Eq. (5), Eq. (6), and Eq. (7) are re-formulated by\nreplacing ft and gt with fTea\nt and gTea\nt , respectively. Similarly, Lim and Lsl are re-formulated by\nreplacing φt with φStu\nt .\nFor the self-knowledge distillation, we generate the soft pseudo-labels by\n¯yt = exp(δ(fTea\nt (xt),µ(1)\nk )/τ)\n∑K\nk=1 exp(δ(fTea\nt (xt),µ(1)\nk )/τ\n, (9)\nwhere δ(a,b) indicates the cosine distance between aand b. Then, our knowledge distillation loss is\nformulated as:\nLkd = −Ext∈Xt\nK∑\nk=1\n¯yt log φStu\nt (xt). (10)\nIn summary, the ﬁnal objective of self-training on the target domain is given by\nLtgt = Lim + αLsl + βLkd, (11)\nwhere αand βare the weights of self-labeling loss and self-knowledge distillation loss. We update\nφStu\nt with Ltgt. Meanwhile, we update φTea\nt with an exponential moving average of the parameters\nof φStu\nt . Note that the classiﬁers of φTea\nt and φStu\nt are both ﬁxed during training.\n4 Experiments\n4.1 Experimental Setup\nDatasets. We conduct experiments on three datasets, including Ofﬁce-31 [53], Ofﬁce-Home [60], and\nVisDA [47]. Ofﬁce-31 includes 4,652 images and 31 categories from three domains,i.e., Amazon (A),\nWebcam (W), and DSLR (D). Ofﬁce-Home consists of around 15,500 images from 65 categories. It is\n6\nTable 1: Accuracy (%) on Ofﬁce-31 for closed-set domain adaptation (ResNet-50).\nMethod A →D A →W D →W W →D D →A W →A Avg\nETD [33] 88.0 92.1 100.0 100.0 71.0 67.8 86.2\nBDG [64] 93.6 93.6 99.0 100.0 73.2 72.0 88.5\nCDAN+BSP [9] 93.0 93.3 98.7 100.0 73.6 72.6 88.5\nCDAN+BNM [10] 92.9 92.8 98.8 100.0 73.5 73.8 88.6\nCDAN+TransNorm [62] 94.0 95.7 98.7 100.0 73.4 74.2 89.3\nGVB-GD [11] 96.1 93.8 98.8 100.0 74.9 72.8 89.4\nGSDA [24] 94.8 95.7 99.1 100.0 73.5 74.9 89.7\nSHOT [35] 94.0 90.1 98.4 99.9 74.7 74.3 88.6\n3C-GAN [34] 92.7 93.7 98.5 99.8 75.3 77.8 89.6\nCAN [29] 95.0 94.5 99.1 99.8 78.0 77.0 90.6\nTransDA (Ours) 97.2 95.0 99.3 99.6 73.7 79.3 90.7\nTable 2: Accuracy (%) on Ofﬁce-Home for closed-set domain adaptation (ResNet-50).\nMethod Ar →Cl Ar→Pr Ar→Rw Cl→Ar Cl→Pr Cl→Rw Pr→Ar Pr→Cl Pr→Rw Rw→Ar Rw→Cl Rw→Pr Avg\nETD [33] 51.3 71.9 85.7 57.6 69.2 73.7 57.8 51.2 79.3 70.2 57.5 82.1 67.3BDG [64] 51.5 73.4 78.7 65.3 71.5 73.7 65.1 49.7 81.1 74.6 55.1 84.8 68.7CDAN+BNM [10] 56.2 73.7 79.0 63.1 73.6 74.0 62.4 54.8 80.7 72.4 58.9 83.5 69.4CDAN+TransNorm [62] 56.3 74.2 79.0 63.9 73.5 73.1 62.3 55.2 80.3 73.5 58.4 83.3 69.4GVB-GD [11] 57.0 74.7 79.8 64.6 74.1 74.6 65.2 55.1 81.0 74.6 59.7 84.3 70.4GSDA [24] 61.3 76.1 79.4 65.4 73.3 74.3 65.0 53.2 80.0 72.2 60.6 83.1 70.3RSDA+DANN [21] 53.2 77.7 81.3 66.4 74.0 76.5 67.9 53.0 82.0 75.8 57.8 85.4 70.9SHOT [35] 57.1 78.1 81.5 68.0 78.2 78.1 67.4 54.9 82.2 73.3 58.8 84.3 71.8TransDA (Ours) 67.5 83.3 85.9 74.0 83.8 84.4 77.0 68.0 87.0 80.5 69.9 90.0 79.3\ncomposed of four domains: Artistic images (Ar), Clip Art (Cl), Product images (Pr), and Real-World\nimages (Rw). VisDA contains 152K synthetic images (regarded as the source domain) and 55K real\nobject images (regarded as the target domain), which are divided into 12 shared classes.\nEvaluation Settings. We evaluate the proposed method on three DA settings, including closed-set\nDA, partial-set DA [4], and open-set DA [37]. Closed-set DA is a standard setting, which assumes\nthat the source and target domains share the same class set. Partial-set DA assumes that the target\ndomain belongs to a sub-class set of the source domain. In contrast, open-set DA assumes that the\ntarget domain includes unknown classes that are absent in the source domain. For closed-set DA, we\nevaluate our method on all three datasets. For partial-set and open-set DA, we evaluate our method on\nOfﬁce-Home. Following [35], for partial-set DA, we choose 25 classes for the target domain, while\nall 65 classes are use for the source domain. For open-set DA, we select 25 classes as the shared\nclasses while the other classes make up the unknown class in the target domain.\nImplementation Details. We use a ResNet-50 [ 22] pretrained on ImageNet [ 22] as the feature\nextractor backbone. Moreover, the Transformer [13] layers are injected after the backbone, followed\nby a bottleneck layer with batch normalization [ 28] and a task-speciﬁc classiﬁer layer. Different\nfrom [35, 1], we adopt a teacher-student structure for target adaptation. We use stochastic gradient\ndescent(SGD) with momentum 0.9 and weight decay 10−3 to update the network. The learning\nrates are set to 10−3 for the backbone and Transformer layers and set to 10−2 for the bottleneck and\nclassiﬁer layers. For source training, we train the model over 100, 50, and 10 epochs for Ofﬁce-31,\nOfﬁce-Home, and VisDA, respectively. For target adaptation, the number of epochs is set to 15 for\nall settings. We set αand β in Eq. (11) to 0.3 and 1, respectively, which yields consistently high\nperformance across all settings. The batch size is set to 64, and the size of the input image is reshaped\nto 224×224.\n4.2 Comparison with State-of-the-Art Methods\nWe ﬁrst compare the proposed TransDA with state-of-the-art methods under the closed-set, partial-set,\nand open-set DA. For closed-set DA, the compared methods include: ETD [33], BDG [64], BSP [9],\nBNM [10], TransNorm [ 62], GVB-GD [ 11], GSDA [ 24], CAN [ 29], SHOT [ 35], GSDA [ 24],\nRSDA [21], DANN [15], CDAN [42], MDD [70], and 3C-GAN [34]. For partial-set DA and open-set\nDA, we compare with ETN [ 4], SAFN [ 63], SHOT [35], TIM [ 30], OSBP [ 55], STA [ 37], and\nBA3US [36]. In these methods, only SHOT [ 35] and 3C-GAN [ 34] are designed for source-free\ndomain adaptation.\nResults on Closed-Set DA . We report the results on Ofﬁce-31, Ofﬁce-Home, and VisDA in Ta-\nble 1, Table 2, and Table 3, respectively. We can make the following three observations. (1) Our\nTransDA outperforms all compared methods on all datasets, yielding state-of-the-art accuracies for\nclosed-set DA. (2) When using the same backbone, our TransDA surpasses the source-free method\n(SHOT [35]) by a large margin. Speciﬁcally, when using ResNet-50 as the backbone, TransDA outper-\nforms SHOT [35] by 2.1%, 7.5%, and 8.0% on Ofﬁce-31, Ofﬁce-Home, and VisDA, respectively. This\n7\nTable 3: Accuracy (%) on VisDA for closed-set domain adaptation (ResNet-50). ∗indicates the\nmethods that use ResNet-101 as the backbone. †indicates reproduction using the ofﬁcial code.\nMethod airplane bicycle bus car horse knife motorcycle person plant skateboard train truck Avg\nDANN [15] - - - - - - - - - - - - 63.7\nCDAN [42] - - - - - - - - - - - - 70.0\nMDD [70] - - - - - - - - - - - - 74.6\nRSDA [21] - - - - - - - - - - - - 75.8\nGSDA [24] 93.1 67.8 83.1 83.4 94.7 93.4 93.4 79.5 93.0 88.8 83.4 36.7 81.5\nSHOT [35]† 94.5 85.7 77.3 52.2 91.6 15.7 82.6 80.3 87.8 88.0 85.1 58.8 75.0\nTransDA (Ours)97.2 91.1 81.0 57.5 95.3 93.3 82.7 67.2 92.0 91.8 92.5 54.7 83.0\nSWD [32]∗ 90.8 82.5 81.7 70.5 91.7 69.5 86.3 77.5 87.4 63.6 85.6 29.2 76.4\n3C-GAN [34]∗ 94.8 73.4 68.8 74.8 93.1 95.4 88.6 84.7 89.1 84.7 83.5 48.1 81.6\nSTAR [43]∗ 95.0 84.0 84.6 73.0 91.6 91.8 85.9 78.4 94.4 84.7 87.0 42.2 82.7\nSHOT [35]∗ 94.3 88.5 80.1 57.3 93.1 94.9 80.7 80.3 91.5 89.1 86.3 58.2 82.9\nTable 4: Accuracy (%) on Ofﬁce-Home for partial-set and open-set domain adaptation (ResNet-50).\nPartial-set DA Ar→Cl Ar→Pr Ar→Rw Cl→Ar Cl→Pr Cl→Rw P →Ar Pr →Cl Pr→Rw Rw→Ar Rw→Cl Rw→Pr Avg\nIW AN [68] 53.9 54.5 78.1 61.3 48.0 63.3 54.2 52.0 81.3 76.5 56.8 82.9 63.6SAN [3] 44.4 68.7 74.6 67.5 65.0 77.8 59.8 44.7 80.1 72.2 50.2 78.7 65.3ETN [4] 59.2 77.0 79.5 62.9 65.7 75.0 68.3 55.4 84.4 75.7 57.7 84.5 70.5SAFN [63] 58.9 76.3 81.4 70.4 73.0 77.8 72.4 55.3 80.4 75.8 60.4 79.9 71.8\nBA3US [36] 60.6 83.2 88.4 71.8 72.8 83.4 75.5 61.6 86.5 79.3 62.8 86.1 76.0SHOT [35] 64.8 85.2 92.7 76.3 77.6 88.8 79.7 64.3 89.5 80.6 66.4 85.8 79.3TransDA (Ours)73.0 79.5 90.9 72.0 83.4 86.0 81.1 71.0 86.9 87.8 74.9 89.2 81.3\nOpen-set DA Ar→Cl Ar→Pr Ar → Cl→Ar Cl→Pr Cl→Rw Pr→Ar Pr→Cl Pr→Rw Rw→Ar Rw→Cl Rw→Pr Avg\nATI-λ[46] 55.2 52.6 53.5 69.1 63.5 74.1 61.7 64.5 70.7 79.2 72.9 75.8 66.1OSBP [55] 56.7 51.5 49.2 67.5 65.5 74.0 62.5 64.8 69.3 80.6 74.7 71.5 65.7STA [37] 58.1 53.1 54.4 71.6 69.3 81.9 63.4 65.2 74.9 85.0 75.8 80.8 69.5TIM [30] 60.1 54.2 56.2 70.9 70.0 78.6 64.0 66.1 74.9 83.2 75.7 81.3 69.6SHOT [35] 64.5 80.4 84.7 63.1 75.4 81.2 65.3 59.3 83.3 69.6 64.6 82.3 72.8TransDA (Ours)71.9 79.1 84.3 71.4 77.1 82.0 68.2 67.5 83.1 76.0 73.0 87.6 76.8\nveriﬁes the effectiveness of the proposed TransDA for source-free DA. (3) On VisDA, TransDA with\nResNet-50 can produce competitive results compared to the methods that use ResNet-101 as the\nbackbone, further demonstrating the superiority of the proposed TransDA .\nResults on Partial-Set and Open-Set DA. To verify the versatility of TransDA , we evaluate it on\ntwo more challenging tasks,i.e., partial-set DA and open-set DA. Results on Ofﬁce-Home are reported\nin Table 4. The advantage of TransDA is similar to that for closed-set DA. That is, TransDA clearly\noutperforms the compared methods on both settings. Speciﬁcally, TransDA is 2.0% and 4.0%\nbetter than SHOT [35] on partial-set DA and open-set DA, respectively. This demonstrates that our\nTransformer-based structure is effective under various domain adaptation settings.\n4.3 Ablation Study\nAccuracy Comparison. In Table 5, we study the effectiveness of the proposed Transformer structure\nand self-knowledge distillation. We ﬁrst explain the components in Table 5: Source Onlyindicates\nthe pretrained source model; Baseline denotes further training the model on the target data with the\ninformation maximization and self-labeling losses; +Transformer means injecting the Transformer\nmodule into the model; +EMA refers to using the teacher-student structure; and +KD indicates\nusing the self-knowledge distillation loss. From Table 5, we can draw the following conclusions. (1)\nInjecting the Transformer into the network can consistently improve the results, regardless the model\nas learnt on the source data or the target data. Speciﬁcally, when using the Transformer, the accuracy\nof Baseline improves from 72.1% to 78.8% on Ofﬁce-Home. This demonstrates the effectiveness\nof the Transformer in domain adaptation. (2) Adding the knowledge distillation loss can further\nboost the performance, verifying the advantage of the self-knowledge distillation. (3) Applying\nthe teacher-student structure fails to produce clear improvements, indicating that the gains of +KD\nare mainly obtained by the knowledge distillation rather than by generating pseudo-labels with the\nteacher model.\nVisualization of Grad-CAM and Statistics Study for Focused and Non-Focused Samples . In\nFigure 3(a), we compare the Grad-CAM visualizations for different variants of our method. We\nobtain the following ﬁndings. (1) When adding the Transformer into the network, the red regions\non the objects increase, indicating that the network is encouraged to pay more attentions on the\nobjects. (2) When training the model with the knowledge distillation loss, the attention ability of the\nnetwork is further improved. In addition, we use Amazon Mechanical Turk to estimate the “focused /\nnon-focused” samples for different methods (Figure 3(b)). We can observe that the focused samples\n8\nFigure 3: Visualization of (a) Grad-CAM and (b) statistics studies for different methods. Results are\nevaluated on Ofﬁce-31 (A→C).\nTable 5: Ablation study on the Transformer, teacher-student structure (EMA) and self-knowledge\ndistillation (KD). Results are evaluated for the closed-set DA under Ofﬁce-31, Ofﬁce-Home, and\nVisDA.\nMethod Ofﬁce-31 Ofﬁce-Home VisDA\nSource Only 78.6 65.7 46.7\n+ Transformer 80.8 67.6 48.0\nBaseline 89.0 72.1 75.0\n+ Transformer 90.0 78.8 81.0\n+ Transformer + EMA 90.2 78.7 81.2\n+ Transformer + KD 90.7 79.3 83.0\nand accuracies increase when adding the Transformer and the knowledge distillation loss. The above\nobservations verify that 1) the proposed Transformer structure and knowledge distillation loss can\neffectively encourage the network to focus on objects, and 2) improving the attention ability of the\nnetwork can consistently improve the accuracy for domain adaptation.\nt-SNE Visualization. In Figure 4, we show the t-SNE of features for different methods. We ﬁnd\nthat adding the Transformer and the knowledge distillation can (1) lead the intra-class samples to\nbe more compact and (2) reduce the distances between source and target domains clusters. These\nﬁndings reveal that TransDA can encourage the model to be more robust to intra-class variations and\ncan decrease the cross-domain distribution gap.\n(a) Baseline\n (b) Baseline+Transformer\n (c) Baseline+Transformer+KD\nFigure 4: t-SNE visualization for different methods on Ofﬁce-31 (A→W). We use the outputs of the\nfeature extractor as the features. Red/black denote the source/target domains. Best viewed in color.\n5 Conclusion\nIn this paper, we propose a generic yet straightforward representation learning framework, named\nTransDA, for source-free domain adaptation (SFDA). Speciﬁcally, by employing a Transformer\nmodule and learning the model with the self-knowledge distillation loss, the network is encouraged to\npay more attention to the objects in an image. Experiments on closed-set, partial-set, and open-set DA\nconﬁrm the effectiveness of the proposed TransDA. Importantly, this work reveals that the attention\nability of a network is highly related to its adaptation accuracy. We hope these ﬁndings will provide a\nnew perspective for designing domain adaptation algorithms in the future.\n9\nReferences\n[1] Sk Miraj Ahmed, Dripta S Raychaudhuri, Sujoy Paul, Samet Oymak, and Amit K Roy-Chowdhury.\nUnsupervised multi-source domain adaptation without access to source data. arXiv, 2021. 2, 3, 5, 7\n[2] John S Bridle, Anthony JR Heading, and David JC MacKay. Unsupervised classiﬁers, mutual information\nand’phantom targets’. 1992. 5\n[3] Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Michael I Jordan. Partial transfer learning with\nselective adversarial networks. In Proc. CVPR, 2018. 2, 8\n[4] Zhangjie Cao, Kaichao You, Mingsheng Long, Jianmin Wang, and Qiang Yang. Learning to transfer\nexamples for partial domain adaptation. In Proc. CVPR, 2019. 7, 8\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In Proc. ECCV, 2020. 3\n[6] Fabio Maria Carlucci, Lorenzo Porzi, Barbara Caputo, Elisa Ricci, and Samuel Rota Bulo. Multidial:\nDomain alignment layers for (multisource) unsupervised domain adaptation. IEEE Trans. on PAMI, 2020.\n3\n[7] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin. Emerging properties in self-supervised vision transformers. arXiv, 2021. 6\n[8] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille, and\nYuyin Zhou. Transunet: Transformers make strong encoders for medical image segmentation. arXiv, 2021.\n3\n[9] Xinyang Chen, Sinan Wang, Mingsheng Long, and Jianmin Wang. Transferability vs. discriminability:\nBatch spectral penalization for adversarial domain adaptation. In Proc. ICML, 2019. 3, 7\n[10] Shuhao Cui, Shuhui Wang, Junbao Zhuo, Liang Li, Qingming Huang, and Qi Tian. Towards discriminability\nand diversity: Batch nuclear-norm maximization under label insufﬁcient situations. In Proc. CVPR, 2020.\n3, 7\n[11] Shuhao Cui, Shuhui Wang, Junbao Zhuo, Chi Su, Qingming Huang, and Qi Tian. Gradually vanishing\nbridge for adversarial domain adaptation. In Proc. CVPR, 2020. 3, 7\n[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In Proc. CVPR, 2009. 1, 4\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. Proc. ICLR, 2021. 2, 3, 4, 5, 7\n[14] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and Hervé Jégou. Training vision transformers for\nimage retrieval. arXiv, 2021. 3\n[15] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette,\nMario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. JMLR. 3, 7, 8\n[16] Jacob Gildenblat and contributors. Pytorch library for cam methods. https://github.com/jacobgil/\npytorch-grad-cam, 2021. 2\n[17] Ross Girshick. Fast r-cnn. In Proc. ICCV, 2015. 1\n[18] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classiﬁ-\ncation: A deep learning approach. In Proc. ICML, 2011. 1\n[19] Ryan Gomes, Andreas Krause, and Pietro Perona. Discriminative clustering by regularized information\nmaximization. 2010. 5\n[20] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial networks. Proc. NeurIPS, 2014. 1\n[21] Xiang Gu, Jian Sun, and Zongben Xu. Spherical space domain adaptation with robust pseudo-label loss.\nIn Proc. CVPR, 2020. 7, 8\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn Proc. CVPR, 2016. 1, 4, 5, 7\n10\n[23] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li, and Wei Jiang. Transreid: Transformer-based\nobject re-identiﬁcation. arXiv, 2021. 3\n[24] Lanqing Hu, Meina Kan, Shiguang Shan, and Xilin Chen. Unsupervised domain adaptation with hierarchi-\ncal gradient synchronization. In Proc. CVPR, 2020. 7, 8\n[25] Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama. Learning discrete\nrepresentations via information maximizing self-augmented training. In Proc. ICML, 2017. 5\n[26] Lin Huang, Jianchao Tan, Ji Liu, and Junsong Yuan. Hand-transformer: Non-autoregressive structured\nmodeling for 3d hand pose estimation. In Proc. ECCV, 2020. 3\n[27] Lin Huang, Jianchao Tan, Jingjing Meng, Ji Liu, and Junsong Yuan. Hot-net: Non-autoregressive\ntransformer for 3d hand-object pose estimation. In Proc. ACM MM, 2020. 3\n[28] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. In Proc. ICML. PMLR, 2015. 7\n[29] Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann. Contrastive adaptation network for\nunsupervised domain adaptation. In Proc. CVPR, 2019. 1, 3, 7\n[30] Jogendra Nath Kundu, Naveen Venkat, Ambareesh Revanur, R Venkatesh Babu, et al. Towards inheritable\nmodels for open-set domain adaptation. In Proc. CVPR, 2020. 7, 8\n[31] Vinod Kumar Kurmi, Shanu Kumar, and Vinay P Namboodiri. Attending to discriminative certainty for\ndomain adaptation. In Proc. CVPR, 2019. 1\n[32] Chen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, and Daniel Ulbricht. Sliced wasserstein discrepancy\nfor unsupervised domain adaptation. In Proc. CVPR, 2019. 1, 8\n[33] Mengxue Li, Yi-Ming Zhai, You-Wei Luo, Peng-Fei Ge, and Chuan-Xian Ren. Enhanced transport distance\nfor unsupervised domain adaptation. In Proc. CVPR, 2020. 1, 3, 7\n[34] Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si Wu. Model adaptation: Unsupervised domain\nadaptation without source data. In Proc. CVPR, 2020. 1, 2, 3, 5, 7, 8\n[35] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis\ntransfer for unsupervised domain adaptation. In Proc. ICML, 2020. 1, 2, 3, 4, 5, 7, 8\n[36] Jian Liang, Yunbo Wang, Dapeng Hu, Ran He, and Jiashi Feng. A balanced and uncertainty-aware\napproach for partial domain adaptation. Proc. ECCV, 2020. 7, 8\n[37] Hong Liu, Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Qiang Yang. Separate to adapt: Open set\ndomain adaptation via progressive separation. In Proc. CVPR, 2019. 7, 8\n[38] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmenta-\ntion. In Proc. CVPR, 2015. 1\n[39] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I Jordan. Learning transferable features with deep\nadaptation networks. Proc. ICML, 2015. 1\n[40] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Unsupervised domain adaptation with\nresidual transfer networks. In Proc. NeurIPS, 2016. 1, 3\n[41] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Deep transfer learning with joint\nadaptation networks. In Proc. ICML, 2017. 1, 3\n[42] Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain\nadaptation. Proc. NeurIPS, 2018. 7, 8\n[43] Zhihe Lu, Yongxin Yang, Xiatian Zhu, Cong Liu, Yi-Zhe Song, and Tao Xiang. Stochastic classiﬁers for\nunsupervised domain adaptation. In Proc. CVPR, 2020. 8\n[44] Rafael Müller, Simon Kornblith, and Geoffrey Hinton. When does label smoothing help? arXiv, 2019. 4\n[45] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan Asselmann. Video transformer network. arXiv, 2021.\n3\n[46] Pau Panareda Busto and Juergen Gall. Open set domain adaptation. In Proc. ICCV, 2017. 2, 8\n11\n[47] Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda: The\nvisual domain adaptation challenge. arXiv, 2017. 1, 6\n[48] Fengchun Qiao and Xi Peng. Uncertainty-guided model generalization to unseen domains. In Proc. CVPR,\n2021. 2\n[49] Fengchun Qiao, Long Zhao, and Xi Peng. Learning to learn single domain generalization. In Proc. CVPR,\n2020. 2\n[50] Bin Ren, Hao Tang, Fanyang Meng, Runwei Ding, Ling Shao, Philip HS Torr, and Nicu Sebe. Cloth\ninteractive transformer for virtual try-on. arXiv preprint arXiv:2104.05519, 2021. 3\n[51] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection\nwith region proposal networks. Proc. NeurIPS, 2015. 1\n[52] Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth from\ncomputer games. In Proc. ECCV. Springer, 2016. 1\n[53] Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new\ndomains. In Proc. ECCV, 2010. 2, 6\n[54] Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classiﬁer discrepancy\nfor unsupervised domain adaptation. In Proc. CVPR, 2018. 1\n[55] Kuniaki Saito, Shohei Yamamoto, Yoshitaka Ushiku, and Tatsuya Harada. Open set domain adaptation by\nbackpropagation. In Proc. ECCV, 2018. 7, 8\n[56] Swami Sankaranarayanan, Yogesh Balaji, Carlos D Castillo, and Rama Chellappa. Generate to adapt:\nAligning domains using generative adversarial networks. In Proc. CVPR, 2018. 1\n[57] Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frustratingly easy domain adaptation. In Proc.\nAAAI, 2016. 1, 3\n[58] Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation.\nIn Proc. CVPR, 2017. 1, 3\n[59] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. arXiv, 2017. 3, 5\n[60] Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing\nnetwork for unsupervised domain adaptation. In Proc. CVPR, 2017. 6\n[61] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, and Lidia S Chao. Learning\ndeep transformer models for machine translation. arXiv, 2019. 5\n[62] Ximei Wang, Ying Jin, Mingsheng Long, Jianmin Wang, and Michael I Jordan. Transferable normalization:\nTowards improving transferability of deep neural networks. In Proc. NeurIPS, 2019. 3, 7\n[63] Ruijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive feature\nnorm approach for unsupervised domain adaptation. In Proc. ICCV, 2019. 7, 8\n[64] Guanglei Yang, Haifeng Xia, Mingli Ding, and Zhengming Ding. Bi-directional generation for unsupervised\ndomain adaptation. In Proc. AAAI, 2020. 1, 3, 7\n[65] Guanglei Yang, Hao Tang, Mingli Ding, Nicu Sebe, and Elisa Ricci. Transformers solve the limited\nreceptive ﬁeld for monocular depth prediction. arXiv preprint arXiv:2103.12091, 2021. 3\n[66] Ting Yao, Yingwei Pan, Chong-Wah Ngo, Houqiang Li, and Tao Mei. Semi-supervised domain adaptation\nwith subspace learning for visual recognition. In Proc. CVPR, 2015. 1, 3\n[67] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning joint spatial-temporal transformations for\nvideo inpainting. In Proc. ECCV, 2020. 3\n[68] Jing Zhang, Zewei Ding, Wanqing Li, and Philip Ogunbona. Importance weighted adversarial nets for\npartial domain adaptation. In Proc. CVPR, 2018. 8\n[69] Yang Zhang, Philip David, and Boqing Gong. Curriculum domain adaptation for semantic segmentation of\nurban scenes. In Proc. ICCV, 2017. 1\n12\n[70] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for\ndomain adaptation. In Proc. ICML. PMLR, 2019. 7, 8\n[71] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng\nFeng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. In Proc. CVPR, 2021. 3\n[72] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable\ntransformers for end-to-end object detection. In Proc. ICLR, 2021. 3\n13"
}