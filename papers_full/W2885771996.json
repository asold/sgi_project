{
  "title": "Sampling Informative Training Data for RNN Language Models",
  "url": "https://openalex.org/W2885771996",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2907571604",
      "name": "Jared Fernandez",
      "affiliations": [
        "Northwestern University"
      ]
    },
    {
      "id": "https://openalex.org/A2098223845",
      "name": "Doug Downey",
      "affiliations": [
        "Northwestern University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2177410802",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2605203995",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W115367774",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W4294635920",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2117278770",
    "https://openalex.org/W2794302998",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W1842094663",
    "https://openalex.org/W2162287622",
    "https://openalex.org/W2567070169",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2963970792",
    "https://openalex.org/W2963748792",
    "https://openalex.org/W4300427683",
    "https://openalex.org/W2107878390",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2134800885",
    "https://openalex.org/W4300092207"
  ],
  "abstract": "We propose an unsupervised importance sampling approach to selecting training data for recurrent neural network (RNNs) language models. To increase the information content of the training set, our approach preferentially samples high perplexity sentences, as determined by an easily queryable n-gram language model. We experimentally evaluate the heldout perplexity of models trained with our various importance sampling distributions. We show that language models trained on data sampled using our proposed approach outperform models trained over randomly sampled subsets of both the Billion Word (Chelba et al., 2014 Wikitext-103 benchmark corpora (Merity et al., 2016).",
  "full_text": "Proceedings of ACL 2018, Student Research Workshop, pages 9–13\nMelbourne, Australia, July 15 - 20, 2018.c⃝2018 Association for Computational Linguistics\n9\nSampling Informative Training Data for RNN Language Models\nJared Fernandezand Doug Downey\nDepartment of Electrical Engineering and Computer Science\nNorthwestern University\nEvanston, IL 60208\njared.fern@u.northwestern.edu, ddowney@eecs.northwestern.edu\nAbstract\nWe propose an unsupervised importance\nsampling approach to selecting training\ndata for recurrent neural network (RNN)\nlanguage models. To increase the infor-\nmation content of the training set, our\napproach preferentially samples high per-\nplexity sentences, as determined by an eas-\nily queryable n-gram language model. We\nexperimentally evaluate the heldout per-\nplexity of models trained with our var-\nious importance sampling distributions.\nWe show that language models trained on\ndata sampled using our proposed approach\noutperform models trained over randomly\nsampled subsets of both the Billion Word\n(Chelba et al., 2014) and Wikitext-103\nbenchmark corpora (Merity et al., 2016).\n1 Introduction\nThe task of statistical language modeling seeks\nto learn a joint probability distribution over se-\nquences of natural language words. In recent\nwork, recurrent neural network (RNN) language\nmodels (Mikolov et al., 2010) have produced state-\nof-the-art perplexities in sentence-level language\nmodeling, far below those of traditional n-gram\nmodels (Melis et al., 2017). Models trained on\nlarge, diverse benchmark corpora such as the Bil-\nlion Word Corpus and Wikitext-103 have seen re-\nported perplexities as low as 23.7 and 37.2, respec-\ntively (Kuchaiev and Ginsburg, 2017; Dauphin\net al., 2017).\nHowever, building models on large corpora is\nlimited by prohibitive computational costs, as the\nnumber of training steps scales linearly with the\nnumber of tokens in the training corpus. Sentence-\nlevel language models for these large corpora can\nbe learned by training on a set of sentences sub-\nsampled from the original corpus. We seek to de-\ntermine whether it is possible to select a set of\ntraining sentences that is signiﬁcantly more infor-\nmative than a randomly drawn training set. We\nhypothesize that by training on higher informa-\ntion and more difﬁcult training sentences, RNN\nlanguage models can learn the language distribu-\ntion more accurately and produce lower perplexi-\nties than models trained on similar-sized randomly\nsampled training sets.\nWe propose an unsupervised importance sam-\npling technique for selecting training data for\nsentence-level RNN language models. We lever-\nage n-gram language models’ rapid training and\nquery time, which often requires just a single pass\nover the training data. We determine a prelimi-\nnary heuristic for each sentence’s importance and\ninformation content by calculating its average per-\nword perplexity. Our technique uses an ofﬂine n-\ngram model to score sentences and then samples\nhigher perplexity sentences with increased proba-\nbility. Selected sentences are then used for training\nwith corrective weights to remove the sampling\nbias. As entropy and perplexity have a monotonic\nrelationship, selecting sentences with higher aver-\nage n-gram perplexity also increases the average\nentropy and information content.\nWe experimentally evaluate the effectiveness of\nmultiple importance sampling distributions at se-\nlecting training data for RNN language models.\nWe compare the heldout perplexities of models\ntrained with randomly sampled and importance\nsampled training data on both the One Billion\nWord and Wikitext-103 corpora. We show that\nour importance sampling techniques yield lower\nperplexities than models trained on similarly sized\nrandom samples. By using ann-gram model to de-\ntermine the sampling distribution, we limit added\ncomputational costs of our importance sampling\napproach. We also ﬁnd that applying perplexity-\nbased importance sampling requires maintaining\na relatively high weight on low perplexity sen-\ntences. We hypothesize that this is because low\n10\nperplexity sentences frequently contain common\nsubsequences that are useful in modeling other\nsentences.\n2 Related Work\nStandard stochastic gradient descent (SGD) iter-\natively selects random examples from the train-\ning set to perform gradient updates. In con-\ntrast, weighted SGD has been proven to acceler-\nate the convergence rates of SGD by leveraging\nimportance sampling as a means of variance re-\nduction (Needell et al., 2014; Zhao and Zhang,\n2015). Weighted SGD selects examples from an\nimportance sampling distribution and then trains\non the selected examples with corrective weights.\nWeights of each training example i are set to be\n1\nPr(i) , where Pr(i) is the probability of selecting\nexample i. The weighting provides an unbiased\nestimator of overall loss by removing the bias of\nthe importance sampling distribution. In expecta-\ntion, each example’s contribution to the total loss\nfunction is the same as if the example had been\ndrawn uniformly at random.\nAlain et al. (2015) developed an importance\nsampling technique for training deep neural net-\nworks by sampling examples directly accord-\ning to their gradient norm. To avoid the high\ncomputational costs of gradient computations,\nKatharopoulos and Fleuret (2018) sample accord-\ning to losses as approximated by a lightweight\nRNN model trained along side their larger pri-\nmary RNN model. Both techniques observed in-\ncreased convergence rates and reduced errors in\nimage classiﬁcation tasks. In comparison, we\nuse a ﬁxed ofﬂine n-gram model to compute our\nsampling distribution, which can be trained and\nqueried much more efﬁciently than a neural net-\nwork model.\nIn natural language processing, subsampling of\nlarge corpora has been used to speed up training\nfor both language modeling and machine trans-\nlation. For domain speciﬁc language modeling,\nMoore and Lewis (2010) used an n-gram model\ntrained on in-domain data to score sentences and\nthen selected the sentences with low perplexities\nfor training. Both Cho et al. (2014) and Koehn\nand Haddow (2012) used similar perplexity-based\nsampling to select training data for domain spe-\nciﬁc machine translation systems. Importance\nsampling has also been used to increase rate of\nconvergence for a class of neural network lan-\nguage models which use a set of binary classiﬁers\nto determine sequence likelihood, rather than cal-\nculating the probabilities jointly (Xu et al., 2011).\nBecause these subsampling techniques are used\nto learn domain speciﬁc distributions different\nfrom the distribution of the original corpus, they\ntarget lower perplexity sentences and have no need\nfor corrective weighting. In contrast, we study\nhow training sets generated using weighted im-\nportance sampling can be selected to maximize\nknowledge of the entire corpus for the standard\nlanguage modeling task.\n3 Methodology\nFirst, we train an ofﬂine n-gram model over sen-\ntences randomly sampled from the training corpus.\nUsing the n-gram model, we score perplexities for\nthe remaining sentences in the training corpus.\nWe propose multiple importance sampling and\nlikelihood weighting schemes for selecting train-\ning sequences for an RNN language model. Our\nproposed sampling distributions (discussed in de-\ntail below) bias the training set to select higher\nperplexity sentences in order to increase the train-\ning set’s information content. We then train an\nRNN language model on the sampled sentences\nwith weights set to the reciprocal of the sentence’s\nselection probability.\n3.1 Z-Score Sampling (Zfull)\nThis sampling distribution naively selects sen-\ntences according to their z-score, as calculated in\nterms of their n-gram perplexities. The selection\nprobability of sequence sis set as:\nPKeep(s) =kpr\n(ppl(s) − µppl\nσppl\n+ 1\n)\n,\nwhere ppl(s) is the n-gram perplexity of sentence\ns, µppl is the average n-gram perplexity, σppl is\nthe standard deviation of n-gram perplexities, and\nkpr is a normalizing constant to ensure a proper\nprobability distribution.\nFor sentences with z-scores less than −1.00 or\nsequences where ppl(s) was in the 99th percentile\nof n-gram perplexities, sequences are assigned\nPkeep(s) = kpr. This ensured all sequences had\npositive selection probability and limited bias to-\nwards the selection of high perplexity sequences\nin the tail of the distribution. Upon examination,\nsequences with perplexities in the 99th percentile\nwere generally esoteric or nonsensical. Selection\n11\nof these high perplexity sentences provided min-\nimal accuracy gain in exchange for their boosted\nselection probability.\n3.2 Limited Z-Score Sampling (Zα)\nTraining on low perplexity sentences can be help-\nful in learning to model higher perplexity sen-\ntences which share common sub-sequences. How-\never, naive z-score sampling results in the selec-\ntion of few low perplexity sentences. Additionally,\nthe low perplexity sentences that are selected tend\nto dominate the training weight space due to their\nlow selection probability.\nTo smooth the distribution in the weight space,\nselection probability is only determined using z-\nscores for sentences where their perplexities are\ngreater than the mean. Thus, the selection proba-\nbility of sentence sis:\nPKeep(s) =\n{\nkpr\n(\nαppl(s)−µppl\nσppl\n+ 1\n)\n, if ppl(s) >µppl.\nkpr, else.\nwhere α is a constant parameter that determines\nthe weight of the z-score in calculating the se-\nquence’s selection probability.\n3.3 Squared Z-Score Sampling (Z2)\nTo investigate the effects of sampling from more\ncomplex distributions, we also evaluate an impor-\ntance sampling scheme where sentences are sam-\npled according to their squared z-score.\nPKeep(s) =\n\n\n\nkpr\n(\nα\n(ppl(s)−µppl\nσppl\n)2\n+ 1\n)\n, if ppl(s) >µppl.\nkpr, else.\n4 Experiments\nWe experimentally evaluate the effectiveness of\nthe Zfull and Z2 sampling methods, as well as the\nZα method for various values of parameter α.\n4.1 Dataset Details\nSentence-level models were trained and evaluated\non samples from Wikitext-103 and the One Bil-\nlion Word Benchmark corpus. To create a dataset\nof independent sentences, the Wikitext-103 corpus\nwas parsed to remove headers and to create indi-\nvidual sentences. The training and heldout sets\nwere combined, shufﬂed, and then split to cre-\nate new 250k token test and validation sets. The\nremaining sequences were set as a new training\nset of approximately 99 million tokens. In Bil-\nlion Word experiments, training sequences were\nsampled from a 500 million subset of the released\ntraining split. Billion Word models were evaluated\non 250k token test and validation sets randomly\nsampled from the released heldout split.\nModels were trained on 500 thousand, 1 mil-\nlion, and 2 million token training sets sampled\nfrom each training split. Rare words were replaced\nwith <unk> tokens, resulting in vocabularies of\n267K and 250K for the Wikitext and Billion Word\ncorpora, respectively.\n4.2 Model Details\nTo calculate the sampling distribution, an n-gram\nmodel was trained on a heldout set with the same\nnumber tokens used to train each RNN model\n(Hochreiter and Schmidhuber, 1997). For exam-\nple, the sampling distribution used to build a 1\nmillion token RNN training set was determined\nusing perplexities calculated by an n-gram model\nalso trained on 1 million tokens. N-gram mod-\nels were trained as 5-gram models with Kneser-\nNey discounting (Kneser and Ney, 1995) using\nSRILM (Stolcke, 2002). For efﬁcient calculation\nof sentence perplexities, we query our models us-\ning KenLM (Heaﬁeld, 2011).\nRNN models were built using a two-layer long\nshort-term memory (LSTM) neural network, with\n200-dimensional hidden and embedding layers.\nEach training set was trained on for 10 epochs us-\ning the Adam gradient optimizer (Kingma and Ba,\n2014) with a mini-batch size of 12.\n5 Results\nIn Tables 1 and 2, we summarize the performances\nof models trained on samples from Wikitext-103\nand the Billion Word Corpus, respectively. We re-\nport the Random and n-gram baseline perplexities\nfor RNN and n-gram language models trained on\nrandomly sampled data. We also report µngram\nand σngram for each training set, which are the\nmean and standard deviation in sentence perplex-\nity as evaluated by the ofﬂine n-gram model.\nIn all experiments, RNN language models\ntrained using our sampling approaches yield a de-\ncrease in model perplexity as compared to RNN\nmodels trained on similar sized randomly sam-\npled sets. As size of the training set increases,\nthe RNNs trained on importance sampling datasets\n12\nModel Tokens µngram σngram ppl\nn-gram 500k — — 492.3\nRandom 500k 449.0 346.4 749.1\nZ0.5 500k 497.1 398.8 643.9\nZ1.0 500k 544.1 440.1 645.2\nZ2.0 500k 615.7 481.3 593.2\nZ4.0 500k 729.0 523.6 571.4\nZ2 500k 576.5 499.7 720.0\nZfull 500k 627.1 451.9 663.7\nn-gram 1M — — 502.7\nRandom 1M 448.9 380.2 550.6\nZ0.5 1M 495.7 431.8 545.73\nZ1.0 1M 540.4 475.4 435.4\nZ2.0 1M 615.6 528.4 426.9\nZ4.0 1M 732.9 584.4 420.1\nZ2 1M 571.5 535.7 435.7\nZFull 1M 608.6 489.9 416.3\nn-gram 2M — — 502.6\nRandom 2M 430.45 392.1 341.3\nZ0.5 2M 471.8 445.2 292.7\nZ1.0 2M 514.6 493.9 289.8\nZ2.0 2M 582.8 544.6 346.9\nZ4.0 2M 684.6 604.7 294.6\nZ2 2M 518.4 522.9 287.9\nZFull 2M 568.4 506.5 312.5\nTable 1: Perplexities for Wikitext models. All pro-\nposed models outperform the random and n-gram\nbaselines as number of training tokens increases.\nalso yield signiﬁcantly lower perplexities than\nthe n-gram models trained on randomly sampled\ntraining sets. As expected, µngram and σngram in-\ncrease substantially for training sets generated us-\ning our proposed sampling methods.\nOverall, the Z4.0 sampling method produced\nthe most consistent reductions in average perplex-\nity of 102.9 and 54.2 compared to the Random\nand n-gram baselines, respectively. ZFull and Z2\nexhibit higher variance in their heldout perplex-\nity as compared to the Zα and baseline methods.\nWe expect that this is because these methods se-\nlect higher perplexity sequences with signiﬁcantly\nhigher probability than Zα methods. As a result,\nlow perplexity sentences, which may contain com-\nmon subsequences helpful in modeling other sen-\ntences, are ignored in training.\n6 Conclusions and Future Work\nWe introduce a weighted importance sampling\nscheme for selecting RNN language model train-\ning data from large corpora. We demonstrate that\nmodels trained with data generated using this ap-\nproach yield perplexity reductions of up to 24%\nwhen compared to models trained over randomly\nsampled training sets of similar size. This tech-\nnique leverages higher perplexity training sen-\nModel Tokens µngram σngram ppl\nn-gram 1M — — 432.5\nRandom 1M 433.2 515.4 484.0\nZ0.5 1M 476.8 410.9 436.6\nZ1.0 1M 543.8 529.0 421.5\nZ4.0 1M 726.4 517.3 427.3\nZfull 1M 635.19 458.69 495.75\nZ2 1M 639.2 593.7 435.3\nTable 2: Perplexities for Billion Word models. Zα\nand Z2 both outperform the random baseline and\nare comparable to the n-gram baseline.\ntences to learn more accurate language models,\nwhile limiting added computational cost of impor-\ntance calculations.\nIn future work, we will examine the perfor-\nmance of our proposed selection techniques in ad-\nditional parameter settings, with various values of\nα and thresholds in the limited z-score methods\nZα. We will evaluate the performance of sam-\npling distributions based on perplexities calculated\nusing small, lightweight RNN language models\nrather than n-gram language models. Addition-\nally, we will also be evaluating the performance of\nsampling distributions calculated based on a sen-\ntence’s subsequences and unique n-gram content.\nFurthermore, we plan on adapting this importance\nsampling approach to use online n-gram models\ntrained alongside the RNN language models for\ndetermining the importance sampling distribution.\nAcknowledgements\nThis work was supported in part by NSF Grant\nIIS-1351029. Support for travel provided in\npart by the ACL Student Travel Grant (NSF\nGrant IIS-1827830) and the Conference Travel\nGrant from Northwestern University’s Ofﬁce of\nthe Provost. We thank Dave Demeter, Thanapon\nNoraset, Yiben Yang, and Zheng Yuan for helpful\ncomments.\nReferences\nGuillaume Alain, Alex Lamb, Chinnadhurai Sankar,\nAaron Courville, and Yoshua Bengio. 2015. Vari-\nance reduction in sgd by distributed importance sam-\npling. arXiv preprint arXiv:1511.06481.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2014. One billion word benchmark for mea-\nsuring progress in statistical language modeling. In\nFifteenth Annual Conference of the International\nSpeech Communication Association.\n13\nKyunghyun Cho, Bart van Merrienboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using rnn encoder–decoder\nfor statistical machine translation. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 1724–\n1734.\nYann N Dauphin, Angela Fan, Michael Auli, and David\nGrangier. 2017. Language modeling with gated con-\nvolutional networks. In International Conference on\nMachine Learning, pages 933–941.\nKenneth Heaﬁeld. 2011. Kenlm: Faster and smaller\nlanguage model queries. In Proceedings of the Sixth\nWorkshop on Statistical Machine Translation, pages\n187–197. Association for Computational Linguis-\ntics.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation,\n9(8):1735–1780.\nAngelos Katharopoulos and Franc ¸ois Fleuret. 2018.\nNot all samples are created equal: Deep learn-\ning with importance sampling. arXiv preprint\narXiv:1803.00942.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for m-gram language modeling. In\nAcoustics, Speech, and Signal Processing, 1995.\nICASSP-95., 1995 International Conference on, vol-\nume 1, pages 181–184. IEEE.\nPhilipp Koehn and Barry Haddow. 2012. Towards ef-\nfective use of training data in statistical machine\ntranslation. In Proceedings of the Seventh Workshop\non Statistical Machine Translation, pages 317–321.\nAssociation for Computational Linguistics.\nOleksii Kuchaiev and Boris Ginsburg. 2017. Factor-\nization tricks for lstm networks. arXiv preprint\narXiv:1703.10722.\nG´abor Melis, Chris Dyer, and Phil Blunsom. 2017. On\nthe state of the art of evaluation in neural language\nmodels. arXiv preprint arXiv:1707.05589.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture\nmodels. arXiv preprint arXiv:1609.07843.\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recur-\nrent neural network based language model. In\nEleventh Annual Conference of the International\nSpeech Communication Association.\nRobert C Moore and William Lewis. 2010. Intelligent\nselection of language model training data. In Pro-\nceedings of the ACL 2010 conference short papers,\npages 220–224. Association for Computational Lin-\nguistics.\nDeanna Needell, Rachel Ward, and Nati Srebro. 2014.\nStochastic gradient descent, weighted sampling, and\nthe randomized kaczmarz algorithm. In Advances\nin Neural Information Processing Systems, pages\n1017–1025.\nAndreas Stolcke. 2002. Srilm-an extensible language\nmodeling toolkit. In Seventh International Confer-\nence on Spoken Language Processing.\nPuyang Xu, Asela Gunawardana, and Sanjeev Khudan-\npur. 2011. Efﬁcient subsampling for training com-\nplex language models. In Proceedings of the Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1128–1136. Association for Com-\nputational Linguistics.\nPeilin Zhao and Tong Zhang. 2015. Stochastic opti-\nmization with importance sampling for regularized\nloss minimization. In International Conference on\nMachine Learning, pages 1–9.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9903392791748047
    },
    {
      "name": "Language model",
      "score": 0.8298464417457581
    },
    {
      "name": "Computer science",
      "score": 0.8219295740127563
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6755963563919067
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6639915704727173
    },
    {
      "name": "Sampling (signal processing)",
      "score": 0.6620675325393677
    },
    {
      "name": "Recurrent neural network",
      "score": 0.6250141859054565
    },
    {
      "name": "Training set",
      "score": 0.5555885434150696
    },
    {
      "name": "Word (group theory)",
      "score": 0.49876952171325684
    },
    {
      "name": "Data modeling",
      "score": 0.47008776664733887
    },
    {
      "name": "Natural language processing",
      "score": 0.45755162835121155
    },
    {
      "name": "Machine learning",
      "score": 0.44805479049682617
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4374054968357086
    },
    {
      "name": "Data set",
      "score": 0.4220000207424164
    },
    {
      "name": "Artificial neural network",
      "score": 0.40377485752105713
    },
    {
      "name": "Database",
      "score": 0.07989072799682617
    },
    {
      "name": "Filter (signal processing)",
      "score": 0.07063046097755432
    },
    {
      "name": "Mathematics",
      "score": 0.06156134605407715
    },
    {
      "name": "Computer vision",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I111979921",
      "name": "Northwestern University",
      "country": "US"
    }
  ],
  "cited_by": 8
}