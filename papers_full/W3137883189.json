{
  "title": "Chord Conditioned Melody Generation With Transformer Based Decoders",
  "url": "https://openalex.org/W3137883189",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2955578869",
      "name": "Kyoyun Choi",
      "affiliations": [
        "Seoul National University",
        "ORCID"
      ]
    },
    {
      "id": "https://openalex.org/A2132471339",
      "name": "Jonggwon Park",
      "affiliations": [
        "Seoul National University",
        "ORCID"
      ]
    },
    {
      "id": "https://openalex.org/A2316787294",
      "name": "Wan Heo",
      "affiliations": [
        "ORCID",
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A2121931813",
      "name": "Sung-Wook Jeon",
      "affiliations": [
        "Seoul National University",
        "ORCID"
      ]
    },
    {
      "id": "https://openalex.org/A2117683655",
      "name": "Jong-Hun Park",
      "affiliations": [
        "Seoul National University",
        "ORCID"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2598521311",
    "https://openalex.org/W2992790584",
    "https://openalex.org/W6771230977",
    "https://openalex.org/W1531333757",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2898827701",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2910096452",
    "https://openalex.org/W6751598888",
    "https://openalex.org/W1965555277",
    "https://openalex.org/W6754559877",
    "https://openalex.org/W6734423058",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2974016341",
    "https://openalex.org/W3025780271",
    "https://openalex.org/W6755406732",
    "https://openalex.org/W6765005523",
    "https://openalex.org/W2897625918",
    "https://openalex.org/W6746781809",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W3000748319",
    "https://openalex.org/W2809621972",
    "https://openalex.org/W2996575285",
    "https://openalex.org/W2592737436",
    "https://openalex.org/W6763362620",
    "https://openalex.org/W6755182157",
    "https://openalex.org/W6746363961",
    "https://openalex.org/W6746846697",
    "https://openalex.org/W1590874898",
    "https://openalex.org/W6749351710",
    "https://openalex.org/W1992855217",
    "https://openalex.org/W6732135373",
    "https://openalex.org/W2782672947",
    "https://openalex.org/W2952480865",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2146950091",
    "https://openalex.org/W2964089573",
    "https://openalex.org/W6743002019",
    "https://openalex.org/W2963032576",
    "https://openalex.org/W2950547518",
    "https://openalex.org/W4288287237",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2948574791",
    "https://openalex.org/W3016512233",
    "https://openalex.org/W2991377373",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2952405182",
    "https://openalex.org/W2990093647",
    "https://openalex.org/W2591984255",
    "https://openalex.org/W2963656263",
    "https://openalex.org/W2772474126",
    "https://openalex.org/W2772281385",
    "https://openalex.org/W2953100410",
    "https://openalex.org/W2991178431",
    "https://openalex.org/W2773479825",
    "https://openalex.org/W2746068898",
    "https://openalex.org/W2963557407",
    "https://openalex.org/W2805697608",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2892104732",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2898148140",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2792210438",
    "https://openalex.org/W2963681776"
  ],
  "abstract": "For successful artificial music composition, chords and melody must be aligned well. Yet, chord conditioned melody generation remains a challenging task mainly due to its multimodality. While few studies have focused on this task, they face difficulties in generating dynamic rhythm patterns aligned appropriately with a given chord progression. In this paper, we propose a chord conditioned melody Transformer, a K-POP melody generation model, which separately produces rhythm and pitch conditioned on a chord progression. The model is trained in two phases. A rhythm decoder (RD) is trained first, and subsequently a pitch decoder is trained by utilizing the pre-trained RD. Experimental results show that reusing RD at the pitch decoding stage and training with pitch varied rhythm data improve the performance. It was also observed that the samples produced by the model well reflected the key characteristics of dataset in terms of both pitch and rhythm related features, including chord tone ratio and rhythm distribution. Qualitative analysis reveals the model&#x2019;s capability of generating various melodies in accordance with a given chord progression, as well as the presence of repetitions and variations within the generated melodies. With subjective human listening test, we come to a conclusion that the model was able to successfully produce new melodies that sound pleasant in terms of both rhythm and pitch (Source code available at <uri>https://github.com/ckycky3/CMT-pytorch</uri>).",
  "full_text": "Received February 19, 2021, accepted March 8, 2021, date of publication March 12, 2021, date of current version March 22, 2021.\nDigital Object Identifier 10.1 109/ACCESS.2021.3065831\nChord Conditioned Melody Generation With\nTransformer Based Decoders\nKYOYUN CHOI\n , JONGGWON PARK\n , WAN HEO\n , SUNGWOOK JEON\n ,\nAND JONGHUN PARK\nDepartment of Industrial Engineering, Seoul National University, Seoul 08826, Republic of Korea\nInstitute for Industrial Systems Innovation, Seoul National University, Seoul 08826, Republic of Korea\nCorresponding author: Jonghun Park (jonghun@snu.ac.kr)\nThis work was supported in part by the National Research Foundation of Korea (NRF) Grant through the Ministry of Science and\nICT (MSIT) under Grant NRF-2019R1F1A1053366, and in part by the MSIT and National IT Industry Promotion Agency (NIPA)’s HPC\nSupport Project.\nABSTRACT For successful artiﬁcial music composition, chords and melody must be aligned well. Yet, chord\nconditioned melody generation remains a challenging task mainly due to its multimodality. While few studies\nhave focused on this task, they face difﬁculties in generating dynamic rhythm patterns aligned appropriately\nwith a given chord progression. In this paper, we propose a chord conditioned melody Transformer, a K-POP\nmelody generation model, which separately produces rhythm and pitch conditioned on a chord progression.\nThe model is trained in two phases. A rhythm decoder (RD) is trained ﬁrst, and subsequently a pitch decoder\nis trained by utilizing the pre-trained RD. Experimental results show that reusing RD at the pitch decoding\nstage and training with pitch varied rhythm data improve the performance. It was also observed that the\nsamples produced by the model well reﬂected the key characteristics of dataset in terms of both pitch and\nrhythm related features, including chord tone ratio and rhythm distribution. Qualitative analysis reveals the\nmodel’s capability of generating various melodies in accordance with a given chord progression, as well as\nthe presence of repetitions and variations within the generated melodies. With subjective human listening test,\nwe come to a conclusion that the model was able to successfully produce new melodies that sound pleasant\nin terms of both rhythm and pitch (Source code available at https://github.com/ckycky3/CMT-pytorch).\nINDEX TERMSAttention mechanism, computer generated music, deep learning, melody generation, neural\nnetworks.\nI. INTRODUCTION\nWith the rapid development of machine learning techniques,\nresearch applying them to numerous music information\nretrieval tasks [1]–[4] can often be found. In particular,\nthe subject of music generation employing deep neural net-\nworks has been explored in diverse ways: creating various\nforms of music including piano music [5], [6], lead sheet [7],\nand multitrack MIDI [8]–[10], or modifying a given piece of\nmusic with style transfer [11] or reinforcement learning [12],\nto name a few.\nIn order for a piece of music to be pleasant to listen to,\nchords and melody must be in harmony. It is essential to\ncapture the relationship between chords and melody in vari-\nous music composition tasks. Yet, chord conditioned melody\ngeneration is challenging mainly due to its large search space\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Bin Liu\n.\nand the absence of standard quantitative measures for per-\nformance assessment. Most previous work had limitations in\nterms of expressiveness of chords or suffered from generating\ndynamic rhythm patterns adequately aligned with a given\nchord progression.\nSince Transformer [13] and its extensions showed impres-\nsive results in many sequence modelling tasks [14], [15],\nthere have been several studies that have adopted the concept\nof attention in the domain of music information retrieval\n[16]–[18]. This paper focuses on the task of generating mono-\nphonic melody for K-POP music, conditioned on a given\nchord progression. We introduce a novel chord conditioned\nmelody generation model, named CMT (Chord conditioned\nMelody Transformer), and a training method for the model so\nthat it can generate a melody with appropriate rhythms for a\ngiven chord progression.\nThe training procedure consists of two phases, and rhythm\nand pitch decoders are jointly trained at both phases. At the\nVOLUME 9, 2021 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 42071\nK. Choiet al.: Chord Conditioned Melody Generation With Transformer-Based Decoders\nﬁrst phase, training data are augmented through pitch shifts\nto make the rhythm decoder robust against chord variations.\nAt the second phase, the rhythm decoder is retained from the\nﬁrst phase and ﬁne tuned, while the pitch decoder is newly\ninitialized. In particular, when producing melodies with the\ntrained model, rhythm generation precedes the generation of\neach note’s pitch, as the pitch decoder utilizes the rhythm\ndecoder’s intermediate rhythm representation.\nExperimental results show that the proposed two-phase\ntraining approach as well as the data augmentation improve\nthe model’s performance. Quantitative analyses were con-\nducted against several metrics to experimentally show that\ngenerated melodies are in harmony with given chord pro-\ngressions and have characteristics similar to those of dataset.\nIt turns out that generated melodies adaptively follow chord\nprogressions, showing repetitions and variations. Further-\nmore, subjective listening test demonstrates that the proposed\nmodel produces better melodies than the alternative model\nconsidered.\nII. RELATED WORK\nA. CHORD CONDITIONED MUSIC GENERATION\nSince chord and melody are two of the most essential ele-\nments that make up modern pop music, many studies have\nbeen conducted to capture their relationships in music gen-\neration. While there have been various research results on\narranging a chord progression for melody [19]–[22], the task\nof chord conditioned melody generation has been relatively\nless investigated.\nBrunner et al. [23] presented JamBot that consists of\ntwo LSTMs (Long Short-Term Memory) [24] for generating\nchord progressions and chord conditioned polyphonic music,\nrespectively. Predicted chords and piano-rolls were extracted\nfrom MIDI data to respectively train chord and polyphonic\nLSTMs. The heuristic applied to extract chords assumed that\nthe three most played notes of a bar are the notes that make up\nthe triad chord of the bar. This heuristic has drawbacks in that\nthe inferred chords are inaccurate and limited to only triads.\nFurthermore, only the 50 most frequent chords were used\nfor training the chord LSTM, and the model was unable to\ncapture the relationship between different chords since chords\nwere symbolized as one-hot vectors.\nMIDINet [25] explored a GAN (Generative Adversarial\nNetwork) [26] framework to generate multitrack music in\nthree ways: from scratch, with chords, or with a priming\nmelody. With regard to the chord conditioned generation,\na CNN (Convolutional Neural Network) generator takes\nrandom noises with additional chord vectors to produce\npiano-roll matrices. Each chord of a bar is represented as\na chord vector with 13 dimensions: 12 dimensional one-hot\nvector for a root note and an additional dimension to specify a\nchord type, either major or minor. While such chord vectors\nmaintain the relationship between chords with respect to a\nroot note and a chord type, they can only express 24 triads.\nChord based rhythm and melody cross-generation\nmodel (CRMCG) introduced by Zhu et al. [27] aims to\nFIGURE 1. An example of chord, rhythm and pitch representations with\nthe first 2 bars of the song ‘‘I Have A Dream’’ from the movie ‘‘Mamma\nMia!’’ . O, H and R stand for onset, hold, and rest, respectively.\nbuild rhythms and melodies separately. It is composed of\na chord GRU (Gated Recurrent Unit) [28] and two auto-\nencoders, one for rhythm and the other for melody. Both\nrhythm and melody auto-encoders utilize GRUs for encoding\nand decoding. Chords given every 2 bars pass through the\nchord GRU. Together with the result of melody encoder,\noutputs of rhythm encoder and chord GRU are then fed into\nthe rhythm and melody decoders, respectively. The model’s\ndownside compared to this work is that the chord vectors\nare not fed into the rhythm decoder, not reﬂecting the fact\nthat the rhythms used for chords are closely related to that of\nmelodies.\nAnother attempt to disentangle rhythms and pitch classes\nwas made by Yang et al.[29]. Explicitly-constrained condi-\ntional variational auto-encoder (EC 2-V AE) extracts rhythm\nand pitch latent variables from chord and melody. A rhythm\ndecoder estimates distributions over rhythm tokens from a\nlatent rhythm variable, and a global decoder receives the dis-\ntributions to reconstruct melody. If the rhythm distributions\nhave been trained so that the probability of repeating preced-\ning rhythm patterns is high, generated melody would simply\nrepeat some rhythm pattern over and over again. Contrary\nto EC 2-V AE, the pitch decoder of CMT takes as input the\nrhythm tokens sampled from the distributions of the rhythm\ndecoder. In this way, other rhythm tokens with lower proba-\nbilities can also be sampled even if the rhythm distributions\nwere trained to repeat preceding rhythm patterns, leading to\nmore variations in rhythm.\nB. ATTENTION MECHANISM AND MUSIC TRANSFORMER\nThe purpose of attention mechanism in tasks such as machine\ntranslation is to compute how much weights should be multi-\nplied to each element of a source sequence when generating\na target sequence. The situation in which the source sequence\nis equal to the target sequence is called self-attention.\nTransformer [13] is a sequence model that relies solely on\nthe self-attention and it showed outstanding performance\nin machine translation. It has an encoder-decoder structure,\nutilizing the self-attention for both encoder and decoder. The\n42072 VOLUME 9, 2021\nK. Choiet al.: Chord Conditioned Melody Generation With Transformer-Based Decoders\nFIGURE 2. Structure of CMT (Chord conditioned Melody Transformer). (a) Flow diagram for the rhythm decoding and pitch decoding, which are\ndepicted as dotted lines and solid lines, respectively.⊕denotes concatenation andc, r, p stand for chord, rhythm, and pitch, respectively.\n(b) Detailed architecture of the rhythm and pitch decoders.N identical self-attention blocks are stacked.\ndecoder needs an upper triangular mask to prevent attending\nto succeeding elements, whereas no such masking is needed\nin the encoder since the whole source sequence is available.\nHowever, depending only on the self-attention without\nconvolution or recurrence results in loss of information about\nthe order in a sequence. To overcome this, Transformer injects\nabsolute positional encoding into the input. Shaw et al.[30]\nsuggested relative self-attention that models relative distance\nbetween elements more explicitly. Unfortunately, since rela-\ntive distance between every element pair needs to be com-\nputed, memory complexity grows quadratically along with\nthe sequence length.\nHuang et al. [16] came up with a memory efﬁcient rel-\native self-attention model so that it can be employed for\nmuch longer sequences. As an application, the authors pre-\nsented Music Transformer, a music generation model that can\ncapture long-term structures. It demonstrated state-of-the-art\nperformance in the task of piano music generation. Music\nTransformer adopts Transformer’s decoder, except the fact\nthat the attention to the encoder’s output is omitted since there\nis no encoder. We employ the same Transformer architecture\nin our rhythm and pitch decoders.\nIII. METHODS\nA. DATA REPRESENTATION\nRepresentations of chord and rhythm in this paper are identi-\ncal to those of [29]. Chords are symbolized by 12 dimensional\nbinary vectors. Each component of chord vector, c, corre-\nsponds to an activated pitch class of a chord. On the other\nhand, three dimensional one-hot rhythm vector r indicates\none of three types of rhythmic elements: onset of a note,\nholding state of an onset note, and rest.\nFor pitch representation, we employed a 50 dimensional\none-hot pitch vector, denoted as p. The ﬁrst 48 dimensions\nrespectively indicate the onset of MIDI pitch from 48 (C3)\nto 95 (B6), and two additional dimensions signify holding\nstate and rest, respectively. Any data out of the pitch range\nare shifted in octaves to ﬁt within the range.\nFigure 1 illustrates an example of the data representations\nwith the ﬁrst 2 bars of the song ‘‘I Have A Dream’’. For the\nsake of simplicity, unit time step in the example is assumed\nto be the length of the 8th note in Figure 1, which differs\nfrom our actual implementation of the minimum time length\nof the 16th note.\nB. MODEL ARCHITECTURE\nArchitecture of the model proposed in this work, CMT,\nis shown in Figure 2. CMT consists of three modules: chord\nencoder, rhythm decoder, and pitch decoder, denoted as Ec,\nDr, and Dp in the following deﬁnitions, respectively.\nLet T be the total number of time steps. The goal of CMT\nis to generate rhythm sequence r1:T ={r 1,..., rT }and pitch\nsequence p1:T ={p 1,..., pT }auto-regressively from a given\nchord sequence c1:T ={c 1,..., cT }, where subscript indi-\ncates the index of a sequence element. Embedding matrices\nof chord, rhythm, and pitch, denoted as Mc, Mr , and Mp,\nrespectively, are multiplied to their corresponding vectors\nbefore the vectors are fed into the modules.\n1) CHORD ENCODER\nThe chord encoder (CE) takes the form of a bidirectional\nLSTM (BLSTM) [31]. Replacing the self-attention encoder\nin the Transformer with a BLSTM encoder led to reduction\nin the ﬂuctuation of loss values. CE’s output ˜c1:T can be\nformulated as follows:\n˜c1:T =Ec(Mc ·c1:T ) (1)\n2) RHYTHM DECODER\nThe rhythm decoder (RD) consists of a stack of N\nself-attention blocks, as depicted in Figure 2(b). When decod-\ning the t-th rhythm token for 2 ≤t ≤T , only the preceding\nVOLUME 9, 2021 42073\nK. Choiet al.: Chord Conditioned Melody Generation With Transformer-Based Decoders\nrhythm sequence {r1,..., rt−1}is accessible and the remain-\ning T −(t −1) tokens must be masked. The masked rhythm\nsequence is then given as an input to RD, together with ˜c1:t−1,\nto yield ˆrt :\nˆrt =[Dr((Mr ·r1:t−1) ⊕˜c1:t−1)]t (2)\nwhere ⊕denotes concatenation and subscript ’1 : t −1’\nindicates a sequence of tokens or vectors of length T with\nT −(t −1) masks at the end. Note that since the output\nof a BLSTM contains hidden states from both forward and\nbackward directions, each non-masked vector in ˜c1:t−1 is pro-\nduced considering the whole chord sequence c1:T . An output\nlayer that consists of a fully-connected layer and a softmax\nlayer, is added to convert ˆrt into probability distributions over\nrhythm tokens, pr(ˆrt ).\n3) PITCH DECODER\nThe pitch decoder (PD) consists of another stack of N\nself-attention blocks. At the decoding step of the t-th pitch\ntoken, the whole sequences of both chord and rhythm, c1:T\nand r1:T respectively, are available. Rather than training\nanother separate rhythm encoder, we reuse the intermediate\nrhythm representation ˜r1:T from RD:\n˜r1:T =Dr((Mr ·r1:T ) ⊕˜c1:T ) (3)\nAfter obtaining ˜r1:t−1 from ˜r1:T , together with ˜c1:t−1 and\n˜r1:t−1, PD receives a masked sequence p1:t−1 of length T ,\nwhich consists of (t −1) preceding pitch tokens and T −(t−1)\nmasks. The output ˆpt can be formulated as follows:\nˆpt =[Dp((Mp ·p1:t−1) ⊕˜c1:t−1 ⊕˜r1:t−1)]t (4)\nˆpt is also converted into probability distributions over pitch\ntokens pr(ˆpt ) after passing through another fully-connected\nlayer, followed by a softmax layer.\nC. TWO-PHASE TRAINING\nThere are two loss terms for CMT, namely rhythm loss and\npitch loss. The training procedure for CMT is divided into two\nphases. The ﬁrst phase trains RD, and then PD is trained at\nthe second phase with the pre-trained RD. Since computing ˆpt\nrequires ˜r1:T , RD’s parameters are involved in minimization\nof the pitch loss too. Since back propagating only the pitch\nloss at the second phase might result in performance degra-\ndation of the pre-trained RD, the sum of rhythm loss and pitch\nloss is minimized at the second phase while RD and PD are\njointly trained as in [32], [33].\nD. PITCH VARIED RHYTHM DATA\nSince distribution of pitch classes depends on the key, differ-\nent keys in different training data may disrupt the training of\nPD. There are two ways to overcome this difﬁculty: shifting\nthe pitch of melodies as well as chords by semitones from −5\nto +6, resulting in 12 times more data, or shifting every song\ninto the same key. While being equivalent in the number of\noriginal songs the model is trained with, the latter is much\nmore efﬁcient in terms of time and computing resources.\nTherefore, all the training data have been shifted into one key,\nC for major and A for minor, respectively in this paper. PD\nis trained to generate a melody in C major or A minor, and\nmelody in any other key can be produced by shifting the result\nup or down by certain semitones.\nOn the other hand, as for rhythm, pitch varied dataset is\nmore valuable than the single key dataset when training RD.\nMelodies in 12 different keys obtained by shifting the pitch\nof one melody have same rhythm but differ only in pitch,\nand the same is true for the chords. RD, which receives\nonly the chords as input, can be trained with 12 times more\ninstances that has the same starting time and duration of\nchords, yielding the same ground truth rhythm labels but with\ndifferent chords. By training RD with the pitch varied dataset,\nwe anticipate that RD will not only be trained to capture the\ntiming of chords but also be robust to their pitch classes.\nIV. EXPERIMENTS\nA. DATA\n1) DATASET\nAll datasets for quantitative experiments in this work came\nfrom EWLD (Enhanced Wikifonia Leadsheet Dataset) [34].\nIt is a music leadsheet dataset of more than 5,000 scores. After\nﬁltering out inappropriate genres for singing melody such\nas traditional, piano, chorale, and scores that do not contain\nchord or melody, approximately 4,000 scores were divided\ninto training / validation / test sets by ratios of 8:1:1.\nFor qualitative evaluation, models were trained on a cus-\ntom K-POP score dataset. About 1,400 leadsheets of K-POP\nmelodies were acquired from a commercial sheetmusic web-\nsite and converted to MusicXML by use of commercial lead-\nsheet recognition software. The ratios of training, validation,\ntest split were 8:1:1, respectively.\n2) PREPROCESSING\nAll the songs were shifted to C major or A minor key. Songs\nthat contain key changes were split into multiple songs with\na single key each. Each of these splitted songs were consid-\nered as an independent song, except that they were grouped\ntogether to be included in the same set when dividing the data\ninto training / validation / test sets.\nEach song in the dataset was further divided into pieces of\nmusic with 8 bars, with a sliding window of 4 bars. The unit\nnote considered was 16th note. As a result, there were 128 unit\nnotes in each data instance. Moreover, data instances were\ndiscarded if the melody does not satisfy any of the following\nconditions: MIDI pitch range is limited to 4 octaves based on\nthe assumption that the range of human singing voice would\nnot exceed 4 octaves. For similar reasons, two consecutive\nnotes should not be more than one octave apart in pitch. The\npercentage of the rest should be less than 25%, in other words,\n32 time steps.\nB. TRAINING AND GENERATION\nNegative log-likelihood was employed as a loss function for\ntraining of RD. For PD, focal loss [35] was employed to\n42074 VOLUME 9, 2021\nK. Choiet al.: Chord Conditioned Melody Generation With Transformer-Based Decoders\nresolve the label imbalance problem since there were much\nmore tokens of holding state and rest than those of pitch onset.\nLet pr(rt ) and pr(pt ) respectively be the output probabili-\nties of RD and PD for the ground truth labels. The total loss\nL =Lr +Lp is to be minimized where rhythm loss Lr and\npitch loss Lp are respectively deﬁned as follows:\nLr =−\nT∑\nt=2\nlog (pr(rt ))\nLp =−\nT∑\nt=2\n(1 −pr(pt ))γ log (pr(pt )) (5)\nwith focusing parameter γ =2.\nOur model was implemented with PyTorch. The embed-\nding dimensions of chord, rhythm, and pitch were 128, 32,\nand 256, respectively. A single-layer BLSTM with the hidden\ndimension of 56 was employed as CE. For the rhythm and\npitch decoders, 8 (=N ) self-attention blocks of 16 heads were\nstacked with dropout probability of 0.2. To enable stacking\nidentical self-attention blocks and concatenating embedding\nvectors to modules’ outputs, the hidden dimensions of RD\nand PD were respectively set to be 144 and 512. Adam opti-\nmizer [36] was adopted as an optimizer. The initial learning\nrate was 10 −4, decaying with the factor 0.5 if the validation\nloss does not decrease for more than 4 epochs, until the\nminimum value of 10 −6.\nAt generation stage, rhythm and pitch sequences were\ndecoded auto-regressively, implying that the t-th element of a\nsequence was generated after the preceding (t −1) elements\nhad been generated. Whereas ground truth rhythm sequence\nwas given as input when training PD, rhythm sequence gen-\nerated by RD was fed instead during the melody generation\nby PD. To generate melodies with dynamic rhythms rather\nthan with simple repetitions of a single pattern, rhythm tokens\nwere sampled with probability pr(ˆrt ) at each time step t\ninstead of applying argmax. Pitch tokens were sampled with\ntop-5 sampling strategy for the same reason. At each time\nstep, a pitch token was sampled from one of the 5 most\nplausible tokens based on the rescaled top-5 probabilities.\nSince CMT is a variant of Transformer, it requires a seed\nto generate a melody. To start constructing melody of 8 bars,\nthe ﬁrst token was chosen heuristically depending on a given\nchord sequence as follows. If there was no chord at the ﬁrst\ntime step, the rest token was set as the ﬁrst token for both\nrhythm and pitch. Otherwise, the onset of the pitch corre-\nsponding to the starting chord’s root note was set as the ﬁrst\ntoken.\nC. EXPERIMENT SETTINGS\nFor CMT, ﬁve experiments with different settings were con-\nducted. The most basic setting was 1 phase (1P) training,\nwhile the training procedure of the other settings was com-\nposed of 2 phases (2P). At the ﬁrst phase of 2P settings,\nthe effects of pitch varied rhythm data and the loss terms\nwere examined. We use the following abbreviation scheme\nfor referring to the settings: PV for training with the pitch\nvaried data and SK for training with the single key data. With\nregard to the loss term, rhythm only (RO) settings and rhythm\nwith pitch (RP) settings were compared. As a result, the ﬁve\nexperiment settings considered were 1P, 2P-PV-RO, 2P-SK-\nRO, 2P-PV-RP, and 2P-SK-RP, respectively.\nDuring the ﬁrst phase, CMT was trained with the pitch\nvaried rhythm data for PV settings while the single key data\nwere used for the training of SK settings. To ensure that the\nmodel is trained with equivalent amount of data for both\nsettings, the number of maximum training epochs was 100 for\nPV settings and 1,200 for SK settings. Only the rhythm loss\nLr was minimized for RO settings, while the loss term for RP\nsettings was L =Lr +Lp.\nFor the second phase of all 2P settings, the model was\ntrained with the single key data for 100 epochs. At the begin-\nning of the second phase, RD’s parameters trained from the\nﬁrst phase were restored while the parameters of CE and PD\nwere initialized randomly. The learning rate of RD’s parame-\nters was ﬁxed to 10−6. In the 1P setting, the model was trained\nwith the single key data for the maximum of 1,300 epochs.\nAs an ablation study, two baseline models were compared\nwith CMT. First model is a vanilla Transformer. It consists\nof only CE and PD, both of which are stacks of self-attention\nblocks. Second model replaces the self-attention CE of the\nﬁrst baseline model with a BLSTM. For both baseline mod-\nels, there are no separate rhythm decoders, and PDs are\nresponsible for predicting pitch tokens directly.\nV. EVALUATION\nA. QUANTITATIVE EVALUATION\nTo assess whether or not melody generation models have been\ntrained well, generated melodies were quantitatively evalu-\nated in the following four ways: examining token accuracy\nand loss value, analyzing chord tone ratio, using MGEval\nframework [37], and investigating bar rhythms.\n1) VALIDATION ACCURACY AND LOSS\nBy referring to [38], [39], accuracy of predicting a token at\nthe next time step was employed as an evaluation metric for\nmodel selection. For the two-phase training of CMT, RD with\nthe highest validation rhythm accuracy, which mostly corre-\nsponds to the model with the smallest loss value, was chosen\nas the pre-trained model during the second phase.\nWhile low pitch accuracy doesn’t necessarily mean that\nthe generated melody is not in harmony, high validation pitch\naccuracy can be interpreted as the model’s ability to generate\nmelody that is harmonious with the chord progression. Two\ntypes of pitch accuracies were calculated. The ﬁrst type con-\nsiders all the 50 pitch tokens, including hold and rest. This is\nan adequate measure for comparing the two baseline models.\nHowever, predicting hold and rest tokens at the pitch decod-\ning step is trivial for CMT since PD takes the ground truth\nrhythm sequence as input. When comparing different CMT\nsettings, the accuracy obtained without those two tokens\nVOLUME 9, 2021 42075\nK. Choiet al.: Chord Conditioned Melody Generation With Transformer-Based Decoders\nTABLE 1. Validation accuracies and loss values of two baseline models and CMT in different experiment settings. The total lossL =Lr +Lp is the sum of\nrhythm lossLr and pitch lossLp.\nreveals the performance of PD more clearly. Accordingly,\nto compute this second type of accuracy over the 48 onset\npitch tokens, the number of correctly predicted pitch tokens\nwas counted only when the corresponding rhythm token was\nan onset.\nThe results of two baseline models and ﬁve settings for\nCMT settings are reported in Table 1. Higher pitch accuracies\nand lower pitch loss values of baseline 2 over baseline 1\nexplain the reason for choosing BLSTM as an encoder for\nCMT, instead of self-attention blocks. All the CMT settings\nyielded better results with respect to pitch, suggesting the\neffectiveness of a separate rhythm decoder.\nComparisons between PV and SK settings show that train-\ning with the pitch varied rhythm data at the ﬁrst phase\nresulted in improvements in terms of both rhythm and pitch.\n2P-PV-RO and 2P-PV-RP achieved higher accuracies than\ntheir counterparts, 2P-SK-RO and 2P-SK-RP, respectively.\nEven though the settings differ in training of the ﬁrst phase\nand only the RD’s parameters were retained at the sec-\nond phase, their pitch accuracies exhibit notable differences.\nAccordingly, it can be inferred that accurate RD helps the\ntraining of PD. The fact that the results of RP settings were\nbetter than those of RO settings also supports the claim. It can\nbe interpreted as the effect of sharing intermediate rhythm\nrepresentations from RD at pitch decoding step.\n2) CHORD TONE RATIO\nTo assess how well the generated melodies comply with\na given chord progression, we compared the chord tone\nratios for the test dataset and those for the results by CMT,\nEC2-V AE [29], and the two baseline models. While the goal\nof Yang et al.[29] was not melody generation, EC 2-V AE was\nconsidered as an alternative since it can produce melodies\nfrom scratch by sampling latent variables from a latent space,\nand in particular it took an approach most similar to ours in\nterms of chord conditioning and separation of rhythm and\npitch.\nCMT, EC 2-V AE, and the two baseline models were all\ntrained with EWLD to generate melodies of 8 bars. Con-\nditioned on the chord progressions of the 2,950 test data\ninstances, 2,950 samples were constructed from each model.\nUnder the intuition that a harmonic melody would have\nchordal notes especially on the ﬁrst strong beat, we also\ncomputed the chord tone ratio of the ﬁrst beat of each bar,\nalong with the overall ratio.\nTABLE 2. Mean values of chord tone ratios.\nThe average chord tone ratios computed from the test\ndataset, CMT with 2P-PV-RP setting, two baseline models\nand EC 2-V AE are summarized in Table 2. For the test data\nof EWLD dataset, 71.42% of notes were one of the notes\nthat make up the chord on their onset time on average. The\naverage chord tone ratios of two baseline models and EC 2-\nV AE were 50.43%, 68.82%, and 63.19%, respectively, which\nare clearly lower than that of the test dataset. Conversely,\nthe average ratio of CMT was 72.53%, similar to that of the\ndataset. This tendency appears to be more evident in the ratios\nfor the ﬁrst beats. From this result, it can be concluded that\nCMT was trained to make good use of the chord information\nand produce melody that is harmonious with the chord to a\nsimilar level to that of the dataset.\n3) MGEval FRAMEWORK\nMGEval (Music Generation Evaluation) is a framework\ndeveloped by Yang and Lerch [37] to evaluate generated\nmusic. It extracts nine metrics from a piece of music,\nﬁve of which are pitch-based features and four related to\nrhythm. Each feature name and its abbreviation are speciﬁed\nin Table 3.\nTABLE 3. Features extracted in MGEval framework [37].\n42076 VOLUME 9, 2021\nK. Choiet al.: Chord Conditioned Melody Generation With Transformer-Based Decoders\nTABLE 4. Results of applying the MGEval framework to the test dataset and the outputs by CMT and EC2-VAE, respectively. ForM∈{MCMT,MEC2 }and\nfeature f , KLD indicates the Kullback-Leibler divergence between the two PDFs estimated fromδf (Mtest,M) andδf (Mtest,Mtest), and OA stands for\nthe overlapping area.\nBased on MGEval, we measured how much the statis-\ntics of the generated music agree with those of the dataset\nthrough carrying out pairwise cross-validation. We applied\nthe evaluation framework to the EWLD’s test dataset as\nwell as the music pieces generated by models, described\nin Section V-A2. For all the features considered, the\nEuclidean distances between all the possible pairs of two\nmusic samples were computed.\nLet Mtest, MCMT, and MEC2 respectively denote the test\ndataset and the sets of music pieces produced by CMT and\nEC2-V AE, respectively. df\nij denotes the Euclidean distance\nbetween the pieces i and j with respect to feature f . For feature\nf , two distance sets δf (Mtest,MCMT) ={d f\nij|i ∈Mtest,j ∈\nMCMT, ∀i,j}and δf (Mtest,Mtest) = {df\nij|i ∈Mtest,j ∈\nMtest, ∀i,j s.t. i ̸=j}were converted into probability den-\nsity functions (PDFs), to obtain Kullback-Leibler divergence\n(KLD) [40] as well as overlapping area (OA) values between\nthe two PDFs. If CMT was trained to be able to capture\nthe characteristics of the dataset well in terms of feature f ,\nδf (Mtest,MCMT) and δf (Mtest,Mtest) would follow similar\ndistributions, resulting in small KLD and large OA. KLD\nand OA between δf (Mtest,MEC2 ) and δf (Mtest,Mtest) were\ncomputed in the same way.\nTable 4 shows means and standard deviations for the\nfeatures (except for histograms and matrices) of the test\ndataset, CMT, and EC 2-V AE, along with the results of\ndistribution comparison between the test dataset and each\nof the two models. The results show that CMT outper-\nformed EC 2-V AE in terms of both KLD and OA for\nall the features considered. Visualization results for the\nPDFs converted from three distance sets, δf (Mtest,Mtest),\nδf (Mtest,MCMT), and δf (Mtest,MEC2 ), can be found in the\nsupplementary material.\n4) BAR RHYTHM ANALYSIS\nFurthermore, distributions of bar rhythms were examined.\nIn our data representation, one bar consists of sixteen time\nsteps and there are three kinds of rhythm tokens. Therefore,\nthe number of possible rhythm patterns in a bar is 3 16. For\nevery instance of 8 bars in the test dataset and a piece of music\nproduced by CMT and EC 2-V AE, we counted the number\nof different bar rhythms and compared the Jensen-Shannon\ndivergence [41] between distributions. Only CMT and EC 2-\nV AE were considered and not the baseline models mentioned\nabove, since the baseline models do not predict rhythm tokens\nexplicitly.\nThere were 1,219 different bar rhythms in the test dataset.\nFor the generated samples by CMT and EC 2-V AE, the num-\nbers of different bar rhythms were 2,259 and 3,532, respec-\ntively. The total number of different bar rhythms in all three\nsets was 5,419.\nThe Jensen-Shannon divergence [41] between the test\ndataset and outputs by CMT was 8.14 ×10−2, which was\nsmaller than 2.17 ×10−1, the divergence between the test\ndataset and EC 2-V AE’s outputs. This indicates that the distri-\nbution of bar rhythms generated by CMT is more similar to\nthat of the dataset.\nFIGURE 3. Bar chart of rhythm patterns in the test dataset, CMT, and\nEC2-VAE. 10 most frequent rhythms from the test data are sorted on the\nx-axis by their frequencies and they-axis indicates the counts of each\nbar rhythm.\nFigure 3 depicts the results in more detail. Out of 5,419 pat-\nterns, 10 most frequent bar rhythms in the test dataset are\nillustrated in bar charts. The dataset and CMT show a similar\ntendency while EC 2-V AE does not. Speciﬁcally, for the 2nd\nmost frequent bar rhythm, the pattern occurred for almost\n2,000 times in the dataset and CMT’s generations, whereas\nthe occurrence was less than 500 for EC 2-V AE. On the other\nhand, for the 6th most frequent bar rhythm, the frequency was\nless than 500 in the dataset and CMT’s generation results but\nmore than 1,000 in generation results by EC 2-V AE.\nVOLUME 9, 2021 42077\nK. Choiet al.: Chord Conditioned Melody Generation With Transformer-Based Decoders\nFIGURE 4. Melodies generated in accordance with chords in major and minor keys. (a) and (b) are different melodies generated from a single chord\nprogression in a major key. (c) and (d) are generated for another chord progression in a minor key, conditioned on the rhythms of (a) and (b),\nrespectively.\nB. QUALITATIVE EVALUATION\n1) VARIETY AND CHORD ACCORDANCE\nFigure 4 demonstrates the capability of CMT to generate\nvarious melodies that are consonant with given chord pro-\ngressions. From the test instances of the custom score dataset\ndescribed in Section IV-A1, two chord progressions were\nextracted, one in a major key and the other in a minor key.\nFigures 4 (a) and (b) show the scores of two melodies\ngenerated by CMT for a single chord progression in a major\nkey. CMT was able to produce different outputs even with the\nsame input due to sampling strategy that substitutes argmax,\nas explained in Section IV-B. The results of feeding the\nrhythms of (a) and (b) into PD with a chord progression in\na minor key are presented in Figures 4 (c) and (d), respec-\ntively. It can be observed that pitch progressions are adjusted\nappropriately.\n2) REPETITIONS AND VARIATIONS\nFigure 5 depicts the samples of 8-bar music produced by\nCMT, conditioned on randomly sampled chord progressions\nfrom the custom score test set. Repeated rhythms within each\nsample are depicted as the boxes with solid lines, and their\nvariations as the boxes of the same color but with dashed\nlines.\nIn Figure 5 (a), the rhythm of the dotted 8th - 16th -\n8th - 8th notes is repeated, marked as the solid red box.\nDespite the same rhythm, the notes’ pitches differ slightly\nfrom each other, leading to avoidance of boredom caused by\nsimple repetition of the same melody. Rhythm itself varies\nfrom the original pattern, bordered by the dashed red boxes:\nconsolidation of the second (16th) and the third (8th) notes\ninto the dotted 8th note in the 4th bar, and fragmentation of\nthe ﬁrst dotted 8th note into the 16th and the 8th notes in the\n7th bar. Similar variations can also be observed in the green\nboxes.\nRepetitions that are longer than those of Figure 5 (a) can\nbe found in the orange boxes of Figure 5 (b). The 1st and 5th\nbars have the same rhythms. The 2nd and 6th bars differ from\nthem only at the 4th beat, changing the 8th note to the rest of\nthe same length. Purple boxes reveal that rhythmic repetition\nFIGURE 5. Examples of 8-bar music generated by CMT. Boxes with solid\nlines represent repeated rhythms and those with dashed lines of the\nsame color indicate their variations. Arrows emphasize similar pitch\ncontours.\nis not just limited to that of a ﬁxed length. Figure 5 (c) shows\nan example for the repetitions of pitch contours. Two types\nof pitch contour patterns are repeated in three bars, depicted\nas the blue and brown arrows, respectively. Note that similar\nrhythm patterns can be observed in all the bars except the 8th.\n3) HUMAN EVALUATION\nWe trained EC 2-V AE with the custom score dataset as an\nalternative model to compare with CMT. CMT and EC 2-\nV AE generated 15 pieces of 8-bar music each, which were\nconditioned on the chord progressions from randomly sam-\npled test data instances. With additional 15 instances from\nthe test dataset, total 45 pieces of music were the candidates\nof listening samples. 40 participants listened to three ran-\ndomly sampled pieces of music from each of the following\nsets: test dataset, results generated by CMT and EC 2-V AE,\nrespectively. Each participant was asked to rate each sample\n42078 VOLUME 9, 2021\nK. Choiet al.: Chord Conditioned Melody Generation With Transformer-Based Decoders\nin a 5-point Likert scale, from 1 (very low) to 5 (very high).\nFour criteria were chosen by referring to [25], [27], [29]:\n1) Rhythm: Whether the rhythm sounds good or not.\n2) Harmony: How harmonious the melody and chord pro-\ngression are.\n3) Creativity: How novel the melody is.\n4) Naturalness: How much the music sounds like those\ncomposed by human.\nThe results of human evaluation are visualized in Figure 6\nas violin plots. CMT’s rhythm, pitch, and naturalness scores\nwere higher than those of EC 2-V AE, suggesting that its results\nsound better in terms of rhythm and pitch, and are also more\nnatural and sound like human made music. As for creativity,\nCMT got higher scores than real melodies in the dataset. This\nimplies that CMT was not trained to simply copy the training\ndata, and also that the produced melodies are novel.\nFIGURE 6. Result of human evaluation comparing ground truth dataset\n(D), CMT (C) and EC2-VAE (E). The middle bars indicate the mean values.\nIn conclusion, CMT appears to be able to produce novel\nmelodies that are more natural and sound like human made\nthan those generated by EC 2-V AE.\nVI. CONCLUSION\nThis paper introduced a novel chord conditioned K-POP\nmelody generation model, named CMT, and proposed train-\ning methods to improve its performance. The model generates\nrhythm ﬁrst, and then pitch sequence corresponding to the\nrhythm. Comparisons between different experiment settings\nproved that dividing the training procedure into two phases\nand training with the pitch varied rhythm data were effec-\ntive in improving the accuracy of both rhythm and pitch.\nEvaluations with several quantitative metrics implied that the\ndistributions of the test dataset overlap more with those of\nthe proposed model’s results than those of the alternatives\nconsidered in terms of both pitch and rhythm based features,\nincluding chord tone ratios and bar rhythms. Examples of\ngenerated music demonstrated the model’s ability to adap-\ntively generate various melodies with repetitive and varying\npatterns for a given chord. The results of human listening test\ndemonstrated that the melodies generated by the proposed\nmodel were novel, harmonious, natural in rhythm, and sound\nlike music composed by human.\nYet, one of the drawbacks of the proposed work is that the\nlength of generated music is limited to 8 bars. For a generative\nmodel to produce real K-POP melodies, it should be capable\nof generating melodies with repetitions and variations over\nlonger time spans. Song generation with consideration of\nstructures such as verse and chorus is also necessary. CMT\nitself can be trained with longer songs, or can be utilized in\nother scenarios such as constructing a chorus from a verse.\nWe leave the task of generating longer melodies for future\nwork.\nREFERENCES\n[1] K. Markov and T. Matsui, ‘‘Music genre and emotion recognition using\nGaussian processes,’’ IEEE Access, vol. 2, pp. 688–697, 2014, doi: 10.\n1109/ACCESS.2014.2333095.\n[2] P.-H. Kuo, T.-H.-S. Li, Y.-F. Ho, and C.-J. Lin, ‘‘Development of an\nautomatic emotional music accompaniment system by fuzzy logic and\nadaptive partition evolutionary genetic algorithm,’’ IEEE Access, vol. 3,\npp. 815–824, 2015, doi: 10.1109/ACCESS.2015.2443985.\n[3] I. Goienetxea, I. Mendialdua, I. Rodriguez, and B. Sierra, ‘‘Statistics-\nbased music generation approach considering both rhythm and melody\ncoherence,’’IEEE Access, vol. 7, pp. 183365–183382, 2019, doi: 10.1109/\nACCESS.2019.2959696.\n[4] R. Yang, L. Feng, H. Wang, J. Yao, and S. Luo, ‘‘Parallel recurrent\nconvolutional neural networks-based music genre classiﬁcation method\nfor mobile devices,’’ IEEE Access, vol. 8, pp. 19629–19637, 2020, doi:\n10.1109/ACCESS.2020.2968170.\n[5] C. Hawthorne, A. Stasyuk, A. Roberts, I. Simon, C.-Z. A. Huang,\nS. Dieleman, E. Elsen, J. Engel, and D. Eck, ‘‘Enabling factorized piano\nmusic modeling and generation with the MAESTRO dataset,’’ 2018,\narXiv:1810.12247. [Online]. Available: http://arxiv.org/abs/1810.12247\n[6] R. Sabathe, E. Coutinho, and B. Schuller, ‘‘Deep recurrent music writer:\nMemory-enhanced variational autoencoder-based musical score compo-\nsition and an objective measure,’’ in Proc. Int. Joint Conf. Neural Netw.\n(IJCNN), May 2017, pp. 3467–3474.\n[7] F. Pachet, A. Papadopoulos, and P. Roy, ‘‘Sampling variations of sequences\nfor structured music generation,’’ in Proc. ISMIR, Suzhou, China, 2017,\npp. 167–173.\n[8] H. W. Dong, W. Y. Hsiao, L. C. Yang, and Y. H. Yang, ‘‘Musegan:\nMulti-track sequential generative adversarial networks for symbolic music\ngeneration and accompaniment,’’ in Proc. AAAI, New Orleans, LA, USA,\n2018, pp. 34–41.\n[9] A. Roberts, J. Engel, C. Raffel, C. Hawthorne, and D. Eck, ‘‘A hierarchical\nlatent vector model for learning long-term structure in music,’’ in Proc.\nICML, Stockholm, Sweden, 2018, pp. 4364–4373.\n[10] I. Simon, A. Roberts, C. Raffel, J. Engel, C. Hawthorne, and\nD. Eck, ‘‘Learning a latent space of multitrack measures,’’ 2018,\narXiv:1806.00195. [Online]. Available: http://arxiv.org/abs/1806.00195\n[11] G. Brunner, A. Konrad, Y. Wang, and R. Wattenhofer, ‘‘MIDI-V AE: Mod-\neling dynamics and instrumentation of music with applications to style\ntransfer,’’ in Proc. ISMIR, Paris, France, 2018, pp. 747–754.\n[12] N. Jaques, S. Gu, D. Bahdanau, J. M. Hernández-Lobato, R. E. Turner, and\nD. Eck, ‘‘Sequence tutor: Conservative ﬁne-tuning of sequence generation\nmodels with KL-control,’’ 2016, arXiv:1611.02796. [Online]. Available:\nhttp://arxiv.org/abs/1611.02796\n[13] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. NeurIPS,\nLong Beach, CA, USA, 2017, pp. 6000–6010.\n[14] Q. Guo, J. Huang, N. Xiong, and P. Wang, ‘‘MS-pointer network: Abstrac-\ntive text summary based on multi-head self-attention,’’ IEEE Access, vol. 7,\npp. 138603–138613, 2019, doi: 10.1109/ACCESS.2019.2941964.\n[15] S. Shang, J. Liu, and Y. Yang, ‘‘Multi-layer transformer aggregation\nencoder for answer generation,’’ IEEE Access, vol. 8, pp. 90410–90419,\n2020, doi: 10.1109/ACCESS.2020.2993875.\n[16] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, N. Shazeer, I. Simon,\nC. Hawthorne, A. M. Dai, M. D. Hoffman, M. Dinculescu, and D. Eck,\n‘‘Music transformer,’’ 2018,arXiv:1809.04281. [Online]. Available: http://\narxiv.org/abs/1809.04281\n[17] J. Park, K. Choi, S. Jeon, D. Kim, and J. Park, ‘‘A Bi-directional trans-\nformer for musical chord recognition,’’ in Proc. ISMIR, Delft, The Nether-\nlands, 2019, pp. 620–627.\n[18] Q. Lin, Y. Niu, Y. Zhu, H. Lu, K. Z. Mushonga, and Z. Niu, ‘‘Hetero-\ngeneous knowledge-based attentive neural networks for short-term music\nrecommendations,’’ IEEE Access, vol. 6, pp. 58990–59000, 2018, doi:\n10.1109/ACCESS.2018.2874959.\nVOLUME 9, 2021 42079\nK. Choiet al.: Chord Conditioned Melody Generation With Transformer-Based Decoders\n[19] H. Lim, S. Rhyu, and K. Lee, ‘‘Chord generation from symbolic\nmelody using BLSTM networks,’’ in Proc. ISMIR, Suzhou, China, 2017,\npp. 621–627.\n[20] H. Chu, R. Urtasun, and S. Fidler, ‘‘Song from PI: A musically plausible\nnetwork for pop music generation,’’ 2016, arXiv:1611.03477. [Online].\nAvailable: http://arxiv.org/abs/1611.03477\n[21] W. Yang, P. Sun, Y. Zhang, and Y. Zhang, ‘‘CLSTMS: A combina-\ntion of two LSTM models to generate chords accompaniment for sym-\nbolic melody,’’ in Proc. Int. Conf. High Perform. Big Data Intell. Syst.\n(HPBD&IS), May 2019, pp. 176–180.\n[22] B.-S. Lin and T.-C. Yeh, ‘‘Automatic chord arrangement with key detection\nfor monophonic music,’’ in Proc. Int. Conf. Soft Comput., Intell. Syst. Inf.\nTechnol. (ICSIIT), Sep. 2017, pp. 21–25.\n[23] G. Brunner, Y. Wang, R. Wattenhofer, and J. Wiesendanger, ‘‘JamBot:\nMusic theory aware chord based generation of polyphonic music with\nLSTMs,’’ in Proc. IEEE 29th Int. Conf. Tools With Artif. Intell. (ICTAI),\nNov. 2017, pp. 519–526.\n[24] S. Hochreiter and J. Schmidhuber, ‘‘Long short-term memory,’’ Neural\nComput., vol. 9, no. 8, pp. 1735–1780, Nov. 1997, doi: 10.1162/NECO.\n1997.9.8.1735.\n[25] L. C. Yang, S. Y. Chou, and Y. H. Yang, ‘‘MidiNet: A convolutional\ngenerative adversarial network for symbolic-domain music generation,’’\nin Proc. ISMIR, Suzhou, China, 2017, pp. 324–331.\n[26] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, ‘‘Generative adversarial nets,’’ in\nProc. Adv. Neural Inf. Process. Syst., 2014, pp. 2672–2680.\n[27] H. Zhu, Q. Liu, N. J. Yuan, C. Qin, J. Li, K. Zhang, G. Zhou, F. Wei,\nY. Xu, and E. Chen, ‘‘XiaoIce band: A melody and arrangement generation\nframework for pop music,’’ in Proc. 24th ACM SIGKDD Int. Conf. Knowl.\nDiscovery Data Mining, Jul. 2018, pp. 2837–2846.\n[28] K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio, ‘‘On the\nproperties of neural machine translation: Encoder–Decoder approaches,’’\nin Proc. SSST-8, 8th Workshop Syntax, Semantics Struct. Stat. Transl.,\n2014, pp. 103–111.\n[29] R. Yang, D. Wang, Z. Wang, T. Chen, J. Jiang, and G. Xia, ‘‘Deep music\nanalogy via latent representation disentanglement,’’ in Proc. ISMIR, Delft,\nThe Netherlands, 2019, pp. 596–603.\n[30] P. Shaw, J. Uszkoreit, and A. Vaswani, ‘‘Self-attention with relative\nposition representations,’’ in Proc. Conf. North Amer. Chapter Assoc.\nComput. Linguistics, Hum. Lang. Technol., (Short Papers), vol. 2, 2018,\npp. 464–468.\n[31] A. Graves and J. Schmidhuber, ‘‘Framewise phoneme classiﬁcation with\nbidirectional LSTM networks,’’ in Proc. IEEE Int. Joint Conf. Neural\nNetw., Jul. 2005, pp. 2047–2052, doi: 10.1109/IJCNN.2005.1556215.\n[32] T. P. Chen and L. Su, ‘‘Harmony transformer: Incorporating chord segmen-\ntation into harmony recognition,’’ in Proc. ISMIR, Delft, The Netherlands,\n2019, pp. 259–267.\n[33] B. Bi, C. Li, C. Wu, M. Yan, W. Wang, S. Huang, F. Huang, and L. Si,\n‘‘PALM: Pre-training an Autoencoding & Autoregressive language model\nfor context-conditioned generation,’’ 2020, arXiv:2004.07159. [Online].\nAvailable: http://arxiv.org/abs/2004.07159\n[34] F. Simonetta, F. Carnovalini, N. Orio, and A. Rodà, ‘‘Symbolic music\nsimilarity through a graph-based representation,’’ in Proc. Audio Mostly\nSound Immersion Emotion, Sep. 2018, p. 26.\n[35] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar, ‘‘Focal loss for dense\nobject detection,’’ inProc. IEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017,\npp. 2980–2988.\n[36] D. P. Kingma and J. Ba, ‘‘Adam: A method for stochastic optimiza-\ntion,’’ 2014, arXiv:1412.6980. [Online]. Available: http://arxiv.org/abs/\n1412.6980\n[37] L.-C. Yang and A. Lerch, ‘‘On the evaluation of generative models in\nmusic,’’Neural Comput. Appl., vol. 32, no. 9, pp. 4773–4784, May 2020,\ndoi: 10.1007/s00521-018-3849-7.\n[38] J. Wu, C. Hu, Y. Wang, X. Hu, and J. Zhu, ‘‘A hierarchical recurrent neural\nnetwork for symbolic melody generation,’’ IEEE Trans. Cybern., vol. 50,\nno. 6, pp. 2749–2757, Jun. 2020, doi: 10.1109/TCYB.2019.2953194.\n[39] F. Colombo, A. Seeholzer, and W. Gerstner, ‘‘Deep artiﬁcial composer:\nA creative neural network model for automated melody genera-\ntion,’’ in Proc. EvoMUSART, Amsterdam, The Netherlands, 2017,\npp. 81–96.\n[40] S. Kullback and R. A. Leibler, ‘‘On information and sufﬁciency,’’ Ann.\nMath. Statist., vol. 22, no. 1, pp. 79–86, 1951.\n[41] J. Lin, ‘‘Divergence measures based on the Shannon entropy,’’ IEEE Trans.\nInf. Theory, vol. 37, no. 1, pp. 145–151, Jan. 1991, doi: 10.1109/18.61115.\nKYOYUN CHOIreceived the B.S. degree in indus-\ntrial engineering from Seoul National University\n(SNU), South Korea, in 2016, where he is currently\npursuing the Ph.D. degree with the Information\nManagement Laboratory, Department of Industrial\nEngineering. His current research interests include\ngenerative artiﬁcial intelligence, deep learning\napplications, and symbolic music generation.\nJONGGWON PARK received the B.S. degree in\nindustrial engineering from Seoul National Uni-\nversity (SNU), South Korea, in 2018, where he is\ncurrently pursuing the Ph.D. degree with the Infor-\nmation Management Laboratory, Department of\nIndustrial Engineering. His current research inter-\nests include audio signal processing, sequence to\nsequence learning, and deep learning applications.\nWAN HEO received the B.S. degree in indus-\ntrial engineering from Seoul National University\n(SNU), South Korea, in 2017, where he is currently\npursuing the Ph.D. degree with the Information\nManagement Laboratory, Department of Industrial\nEngineering. His current research interests include\nsequence to sequence learning, deep learning gen-\neration, and audio signal processing.\nSUNGWOOK JEON received the B.S. degree in\nindustrial engineering from Seoul National Uni-\nversity (SNU), South Korea, in 2012, where he\nis currently pursuing the Ph.D. degree with the\nInformation Management Laboratory, Department\nof Industrial Engineering. His current research\ninterests include sequence generation models and\ndeep learning applications.\nJONGHUN PARK received the Ph.D. degree in\nindustrial and systems engineering from the Geor-\ngia Institute of Technology, Atlanta, in 2000, with\na focus on computer science. He is currently a\nProfessor with the Department of Industrial Engi-\nneering, Seoul National University (SNU), South\nKorea. Before joining SNU, he was an Assistant\nProfessor with the School of Information Sciences\nand Technology, Pennsylvania State University,\nUniversity Park, and the Department of Industrial\nEngineering, KAIST, Daejeon. His research interests include generative\nartiﬁcial intelligence and deep learning applications.\n42080 VOLUME 9, 2021",
  "concepts": [
    {
      "name": "Melody",
      "score": 0.9338566660881042
    },
    {
      "name": "Chord (peer-to-peer)",
      "score": 0.8304915428161621
    },
    {
      "name": "Rhythm",
      "score": 0.8042619228363037
    },
    {
      "name": "Speech recognition",
      "score": 0.7353375554084778
    },
    {
      "name": "Computer science",
      "score": 0.7204031944274902
    },
    {
      "name": "Transformer",
      "score": 0.5212893486022949
    },
    {
      "name": "Active listening",
      "score": 0.4615706205368042
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35201841592788696
    },
    {
      "name": "Acoustics",
      "score": 0.1897413730621338
    },
    {
      "name": "Communication",
      "score": 0.11624518036842346
    },
    {
      "name": "Engineering",
      "score": 0.08271515369415283
    },
    {
      "name": "Psychology",
      "score": 0.07180196046829224
    },
    {
      "name": "Musical",
      "score": 0.06928372383117676
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Distributed computing",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "topic": "Melody",
  "institutions": [
    {
      "id": "https://openalex.org/I139264467",
      "name": "Seoul National University",
      "country": "KR"
    }
  ],
  "cited_by": 30
}