{
  "title": "Framework for Deep Learning-Based Language Models Using Multi-Task Learning in Natural Language Understanding: A Systematic Literature Review and Future Directions",
  "url": "https://openalex.org/W4210894209",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4212686134",
      "name": "Rahul Manohar Samant",
      "affiliations": [
        "Symbiosis International University"
      ]
    },
    {
      "id": "https://openalex.org/A1965189781",
      "name": "Mrinal R. Bachute",
      "affiliations": [
        "Symbiosis International University"
      ]
    },
    {
      "id": "https://openalex.org/A2185630633",
      "name": "Shilpa Gite",
      "affiliations": [
        "Symbiosis International University"
      ]
    },
    {
      "id": "https://openalex.org/A2182624013",
      "name": "Ketan Kotecha",
      "affiliations": [
        "Symbiosis International University"
      ]
    },
    {
      "id": "https://openalex.org/A4212686134",
      "name": "Rahul Manohar Samant",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1965189781",
      "name": "Mrinal R. Bachute",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2185630633",
      "name": "Shilpa Gite",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2182624013",
      "name": "Ketan Kotecha",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2147152072",
    "https://openalex.org/W6680532216",
    "https://openalex.org/W6682691769",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6780805062",
    "https://openalex.org/W6773631536",
    "https://openalex.org/W6770147883",
    "https://openalex.org/W3142036593",
    "https://openalex.org/W2753709519",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2251869843",
    "https://openalex.org/W6636510571",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W6731031554",
    "https://openalex.org/W6682839988",
    "https://openalex.org/W2963355447",
    "https://openalex.org/W6639301165",
    "https://openalex.org/W2251189452",
    "https://openalex.org/W6713582272",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2120615054",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2739996966",
    "https://openalex.org/W2561300216",
    "https://openalex.org/W6732491067",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2912602450",
    "https://openalex.org/W2963477629",
    "https://openalex.org/W6688533166",
    "https://openalex.org/W2966661",
    "https://openalex.org/W6743446608",
    "https://openalex.org/W2885141472",
    "https://openalex.org/W2896342044",
    "https://openalex.org/W6679434410",
    "https://openalex.org/W2470673105",
    "https://openalex.org/W2562439797",
    "https://openalex.org/W2964189376",
    "https://openalex.org/W6715743342",
    "https://openalex.org/W6631501603",
    "https://openalex.org/W2907492528",
    "https://openalex.org/W6726873649",
    "https://openalex.org/W2962946486",
    "https://openalex.org/W6695662000",
    "https://openalex.org/W2462025561",
    "https://openalex.org/W2734389934",
    "https://openalex.org/W2250966211",
    "https://openalex.org/W6691524494",
    "https://openalex.org/W2977540023",
    "https://openalex.org/W2963769536",
    "https://openalex.org/W6626481562",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W6768021236",
    "https://openalex.org/W6768851824",
    "https://openalex.org/W6765807869",
    "https://openalex.org/W6761910064",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W6762122294",
    "https://openalex.org/W6752542827",
    "https://openalex.org/W3145127497",
    "https://openalex.org/W2779005544",
    "https://openalex.org/W2911279001",
    "https://openalex.org/W6771876938",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W6762392948",
    "https://openalex.org/W3139434402",
    "https://openalex.org/W6676310258",
    "https://openalex.org/W3014773921",
    "https://openalex.org/W2963344337",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W3113040795",
    "https://openalex.org/W3163832451",
    "https://openalex.org/W2961050448",
    "https://openalex.org/W3042612565",
    "https://openalex.org/W2786860129",
    "https://openalex.org/W3089436844",
    "https://openalex.org/W3016258141",
    "https://openalex.org/W6779848791",
    "https://openalex.org/W2937699986",
    "https://openalex.org/W3007095883",
    "https://openalex.org/W2943651431",
    "https://openalex.org/W3021931813",
    "https://openalex.org/W2125453582",
    "https://openalex.org/W6745591650",
    "https://openalex.org/W6754414506",
    "https://openalex.org/W3148049738",
    "https://openalex.org/W3083911419",
    "https://openalex.org/W2963852536",
    "https://openalex.org/W3143418328",
    "https://openalex.org/W3002772880",
    "https://openalex.org/W3018202776",
    "https://openalex.org/W6784581610",
    "https://openalex.org/W2963925965",
    "https://openalex.org/W2963972328",
    "https://openalex.org/W4239943352",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2252335727",
    "https://openalex.org/W3171009994",
    "https://openalex.org/W2284289336",
    "https://openalex.org/W2963083845",
    "https://openalex.org/W3088409176",
    "https://openalex.org/W2415204069",
    "https://openalex.org/W2407776548",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2964046515",
    "https://openalex.org/W3104395172",
    "https://openalex.org/W4293350112",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2563351168",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W4299280717",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3103843209",
    "https://openalex.org/W3005742798"
  ],
  "abstract": "Learning human languages is a difficult task for a computer. However, Deep Learning (DL) techniques have enhanced performance significantly for almost all-natural language processing (NLP) tasks. Unfortunately, these models cannot be generalized for all the NLP tasks with similar performance. NLU (Natural Language Understanding) is a subset of NLP including tasks, like machine translation, dialogue-based systems, natural language inference, text entailment, sentiment analysis, etc. The advancement in the field of NLU is the collective performance enhancement in all these tasks. Even though MTL (Multi-task Learning) was introduced before Deep Learning, it has gained significant attention in the past years. This paper aims to identify, investigate, and analyze various language models used in NLU and NLP to find directions for future research. The Systematic Literature Review (SLR) is prepared using the literature search guidelines proposed by Kitchenham and Charters on various language models between 2011 and 2021. This SLR points out that the unsupervised learning method-based language models show potential performance improvement. However, they face the challenge of designing the general-purpose framework for the language model, which will improve the performance of multi-task NLU and the generalized representation of knowledge. Combining these approaches may result in a more efficient and robust multi-task NLU. This SLR proposes building steps for a conceptual framework to achieve goals of enhancing the performance of language models in the field of NLU.",
  "full_text": "Received January 8, 2022, accepted February 2, 2022, date of publication February 7, 2022, date of current version February 16, 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.3149798\nFramework for Deep Learning-Based Language\nModels Using Multi-Task Learning in Natural\nLanguage Understanding: A Systematic Literature\nReview and Future Directions\nRAHUL MANOHAR SAMANT\n1, MRINAL R. BACHUTE\n2, SHILPA GITE\n 3,\nAND KETAN KOTECHA\n 3\n1Computer Science and Information Technology Department, Symbiosis Institute of Technology, Symbiosis International (Deemed University),\nPune 412115, India\n2Department of Electronics and Telecommunication Engineering, Symbiosis Institute of Technology, Symbiosis International (Deemed University),\nPune 412115, India\n3Symbiosis Centre of Applied AI (SCAAI), Symbiosis International (Deemed University), Pune 412115, India\nCorresponding author: Mrinal R. Bachute (mrinal.bachute@sitpune.edu.in)\nABSTRACT Learning human languages is a difﬁcult task for a computer. However, Deep Learning (DL)\ntechniques have enhanced performance signiﬁcantly for almost all-natural language processing (NLP) tasks.\nUnfortunately, these models cannot be generalized for all the NLP tasks with similar performance. NLU\n(Natural Language Understanding) is a subset of NLP including tasks, like machine translation, dialogue-\nbased systems, natural language inference, text entailment, sentiment analysis, etc. The advancement in\nthe ﬁeld of NLU is the collective performance enhancement in all these tasks. Even though MTL (Multi-\ntask Learning) was introduced before Deep Learning, it has gained signiﬁcant attention in the past years.\nThis paper aims to identify, investigate, and analyze various language models used in NLU and NLP to ﬁnd\ndirections for future research. The Systematic Literature Review (SLR) is prepared using the literature search\nguidelines proposed by Kitchenham and Charters on various language models between 2011 and 2021. This\nSLR points out that the unsupervised learning method-based language models show potential performance\nimprovement. However, they face the challenge of designing the general-purpose framework for the language\nmodel, which will improve the performance of multi-task NLU and the generalized representation of\nknowledge. Combining these approaches may result in a more efﬁcient and robust multi-task NLU. This\nSLR proposes building steps for a conceptual framework to achieve goals of enhancing the performance of\nlanguage models in the ﬁeld of NLU.\nINDEX TERMS Deep learning, knowledge representation, multi-task NLU, unsupervised learning.\nI. INTRODUCTION\nNLU is a relatively new research topic in which a computer\nanalyses and extracts information from natural language text\nbefore doing standard NLU tasks, viz. information retrieval,\nquestion-answering, language translation, text summariza-\ntion, news classiﬁcation, and so on. The recent trends in text\nmining caters to the ever-increasing need of extraction of\nhigh-quality information from structured as well unstructured\ntext. On the contrary, the recent trends in systematic language\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Hong-Mei Zhang\n.\nunderstanding (SLU) are in the direction of understanding\nactionable intents in the input text, along with grammatical\nstructural correctness of the input language. The application\ndomains for NLU are listed in Table 1. The popular business\napplication based on NLU is a chatbot. According to\nGartner’s AI customer service statistics [1], chatbots will be\nresponsible for 85% of customer service by 2020. According\nto Crunchbase’s AI stats [1], more than 10,000 developers\nnow build chatbots for Facebook Messenger.\nAccording to Juniper statistics [2], Chatbots are expected\nto cut business costs by $8 billion. These statistics predict\nthe emergence of AI-powered chatbots (conversational AI) in\n17078\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ VOLUME 10, 2022\nR. M. Samantet al.: Framework for DL-Based Language Models Using MTL in Natural Language Understanding\nTABLE 1. Application domains for NLU.\nmany business sectors like banking, education, tourism, legal,\nand government, where customer interaction and customer\nexperience can be designed using AI-based techniques.\nA. RELEVANCE AND SIGNIFICANCE\nBecause the general goals of AI are to make computers\nand intelligent devices listen, talk, and understand language;\nthink and solve problems; and create new things, research in\nthe ﬁeld of NLU is relevant to many aspects of AI. Most\nNLU’s tasks entail reading, interpreting, and categorizing\nmaterial. This requirement necessitates the creation of\nsystems capable of answering questions after reading a\nparagraph or document. It necessitates human-like language\ncomprehension abilities. Machine reading comprehension\ncan also be employed in virtual assistants so that after reading\ndocuments, these assistants can aid in answering customer\nservice concerns. It can also be utilized in the workplace\nto assist users in reading and processing emails or large-\nscale business papers, as well as summarizing pertinent infor-\nmation. In-home automation also voice-activated assistants\nhelp users communicate with various home appliances in\nmeaningful ways.\nB. EVOLUTION OF DEEP LEARNING MODELS USED IN\nNLU TASKS\nThe underlying major sub-task in all NLU tasks is text\nclassiﬁcation or categorization (TC). Text data belongs to\nheterogeneous sources like social media, electronic com-\nmunication, or interrogative data like QA from the client\ninteraction. Text is an excellent basis of the information, but\ninferring useful information can be complex and laborious\ndue to its unstructured style. Text categorization (TC) can\nbe achieved through manual or automatic labelling. Due\nto the availability of explosive data in text form in many\napplications, automated text categorization is becoming one\nof the most effective methods. There are two main categories\nof Automatic text classiﬁcation - rule-based and AI-based\nmethods. The ﬁrst category of Rule-based methods group\ntext into different classes using a set of predeﬁned rules\nand require a deep knowledge about related domains. The\nsecond type, AI-based approaches, learn to classify text\nbased on the training of data using pre-labelled examples.\nAn ML algorithm learns the relation between the text and its\nlabels. Most classical machine learning-based models follow\nthe staged procedure. The majority of NLU’s tasks entail\nreading, comprehending, and interpreting. Some features are\nmanually retrieved from any document in the ﬁrst phase,\nand those features are then ﬁtted to a classiﬁer to create\na prediction in the second step. A bag of words (BoW)\nis an example of a manually extracted feature, and Naive\nBayes (NB), Logistic Regression (LR), Support Vector\nMachines (SVM), and Random Forests (RF) are prominent\nclassiﬁcation techniques. This staged approach has sev-\neral limitations; for example, depending on the manually\nextracted features requires complex feature analysis to obtain\ndecent performance. Due to its considerable dependence\non domain local knowledge for features engineering, the\ngeneralization of this method for new tasks demands more\neffort. Further, these models cannot exploit the availability\nof colossal training data because of the predeﬁned features.\nNeural Network methodologies have been used to overcome\nthe restrictions by using manually extracted features. The\nmain focus of these approaches depends on a machine\nlearning algorithm that maps text into a low dimensional\ncontinuous feature vector, so manually extracted features are\nnot needed. Deerwester et al. [3], in 1989, proposed Latent\nSemantic Analysis (LSA). It is one of the earliest embedding\nmodels. LSA is a linear model with fewer than one million\nparameters that have been trained on the corpus of 200K\nwords. Some of the limitations of this model are the statistical\nproperties of an LSA space when used as a measure of\nsimilarity and the limited use of dimensional information in\nthe vector representation. Bengeo et al. [4] introduced the\nﬁrst natural language model in 2000. It is based on a feed-\nforward neural network and trained using 14 million words.\nThese premature embedding models, on the other hand,\nunderperform traditional models based on manually derived\nfeatures. This scenario drastically changed when much larger\nembedding models with much larger training data were\ndeveloped. In 2013, Google developed a series of word2vec\nmodels [5] trained using the corpus of 6 billion words and\nimmediately became state of the art for many NLU tasks.\nIn 2017, AI2 and the University of Washington collaborated\nto create a contextual embedding model based on a three-\nlayer bidirectional LSTM using 93 million parameters and\n1 billion words. Because it captures contextual information,\nthe Elmo [6] model performs substantially better than the\nword2vec approach. In 2018, OpenAI began developing\nembedding models using Google’s Transformer [7], a rev-\nolutionary neural network architecture. The transformer is\nentirely dependent on attention, which enhances the accuracy\nof large-scale model training on TPU signiﬁcantly. GPT [8],\nthe ﬁrst model designed with Transformers, is currently\ncommonly utilized for text generation jobs. Google also\ndeveloped BERT [9] based on the bidirectional transformer\nin 2018. BERT consists of 340M parameters and is trained\nusing a corpus of 3.3 billion words. The trend of using bigger\nmodels with more training data continues with the recent\nintroduction of OpenAI’s latest GPT-3 model [10]. It has\n170 billion parameters, and Google’s Gshard [11] contains\n600 billion parameters. Other popular models based on\ngenerative pre-trained transformer techniques include T-NLG\nfrom Microsoft with 17 billion parameters and Megatron\nVOLUME 10, 2022 17079\nR. M. Samantet al.: Framework for DL-Based Language Models Using MTL in Natural Language Understanding\nfrom NVIDIA with 1 trillion training parameters. [12]\nAlthough these massive-scale models perform admirably\non certain NLU tasks, other researchers contend they lack\nlanguage understanding and are unsuitable for many mission-\ncritical applications. [13], [14]. The evolution of deep\nlearning models in NLP and NLU is depicted in Figure 1\nII. PRIOR RESEARCH\nOne of the objectives of this SLR is to explore the existing\ndeep learning models in the area of NLU for multi-tasks.\nFigure 2 outlines various sections in this paper.\nThere is a scarcity of SLRs in the research topic of\nNLU. The review [15] is one of the recent and pertinent\nsurveys on deep learning models using transformers as an\nunderlying architecture. This review focus on knowledge\nencoding techniques for transformer-based models. It also\npoints out the challenges faced by these models as context\nand language dependence issues. The highlight of this study is\nestablishing BERT as the backend model in all such variants.\nAnother study [15] provides an extensive review of multi-\ntask learning. (MTL) The study contributes to the domain by\noffering practical approaches into settings of MTL that are\nintroduced for supervised learning, unsupervised learning,\nsemi-supervised, active learning, reinforcement learning,\nonline learning, and multi-view learning. It also suggests\nparallel and distributed MTL for improving the speed and\nperformance of those models. It also presented recent\ntheoretical analyses for MTL. The survey [16] summarized\nand examined the current state-of-the-art (SOTA) NLP\nmodels for standard NLP tasks for optimal performance\nand efﬁciency. The signiﬁcant contribution of this survey\nis to provide a detailed understanding and functioning of\nthe different architectures, a taxonomy of NLP, NLU, and\nNLG designs, and comparative evaluations. This survey [17]\nrightly pointed out that the self-attention mechanism and\ntransformer-based architectures exponentially improve the\nperformance of language models.\nThere are a few constraints of the previous research, which\nis enlisted as follows:\n1. Current surveys are task-speciﬁc and architecture-\ncentered.\n2. Current literature does not check the generalization of\nlanguage models to be suitable for all NLU tasks.\n3. Very few surveys discuss the knowledge representation\nmethods for multi-tasks.\n4. Few surveys examine the existing online tools to build a\ngeneral-purpose framework for multi-tasks NLU.\nThis SLR is comprehensive in terms of examining the\ncurrent trends and challenges related to building a general-\npurpose framework for multi-task NLU, and quality of\navailable benchmarking datasets in the public domain, and\nthe techniques used for creating such a framework.\nA. MOTIVATION\nThere is no prevailing SLR with the exhaustive examination\nof general-purpose language models for multi-task NLU\nTABLE 2. Research questions.\ncovering explicit beneﬁts, comparative analysis, taxonomies,\nand pit-falls. Table 2 lists research questions.\nB. GOALS FOR THIS RESEARCH\nNLU mainly comprises tasks like inference, text entailment,\nsentiment analysis and named entity recognition. The ﬁeld\nof NLU research aims to attain the task proﬁciency for\nthese tasks contained in standard benchmarking datasets like\nGLUE (General Language Understanding Evaluation) and\nsuperGLUE. (Super GLUE). The goal of this research is to\nﬁrst match and then surpass the established score of existing\nmodels in the literature. This SLR aims at recognizing\nand judgmentally examining the research papers and their\noutput concerning the framed research questions. The RQs\nof interest are listed in Table 2.\nC. CONTRIBUTION OF THIS STUDY\nThe contributions of this SLR:\n1. This study identiﬁed 93 primary studies on language\nmodels in NLU from 2011 to 2021.\n2. A detailed study of benchmarking datasets in the public\ndomain is made. A suitable benchmark for a general-\npurpose framework of language models for multi-task\nNLU is provided.\n3. A summary of available online tools for building a\ngeneral-purpose framework of language models for multi-\ntask NLU is presented.\n4. The research gaps were identiﬁed. These gaps lead to\nfuture directions in the research area of NLU.\n5. A conceptualizing framework for the general-purpose\nlanguage models with enhanced transformer encoding\nwith active learning for multi-tasking NLU is proposed to\nproduce this SLR.\nIII. METHODOLOGY FOR RESEARCH\nThe guiding principles introduced by Kitchenham and\nCharters [18] were followed for preparing this SLR.\nTable 5 depicts the techniques of PIOC (Population, Interven-\ntion, Outcome, Context) utilized for enclosing the research\nquestions. The procedural ﬂowchart for this process is shown\nin Figure 3.\n17080 VOLUME 10, 2022\nR. M. Samantet al.: Framework for DL-Based Language Models Using MTL in Natural Language Understanding\nFIGURE 1. Evolution of deep learning models in NLU.\nA. RESEARCH STUDIES SELECTION CRITERIA\nThe key phrases were selected to acquire the required search\nresults to inquire about the RQs of the domain. The search\nstring is shown below:\n(multi-task nlu’’ OR ‘‘multi-task nlu framework’’ OR’’\nnatural language understanding’’ AND ‘‘unsupervised\nlearning’’ OR ‘‘active learning’’ OR ‘‘deep learning’’ AND\n‘‘attention model’’)\nThe search results are displayed in Table 3. Even though\nthis domain has been studied since 2000, the focus is put on\nthe papers from 2011 to 2021 to depict current development\nin the ﬁeld.\nVOLUME 10, 2022 17081\nR. M. Samantet al.: Framework for DL-Based Language Models Using MTL in Natural Language Understanding\nFIGURE 2. Outline of the paper.\nTABLE 3. Literature databases search result.\nB. INCLUSION AND EXCLUSION CRITERIA\nResearch papers considered for this SLR must be relevant.\nThe studies ranged from enhancing the techniques, building\nthe frameworks for NLU, and catering to different application\ndomains. The language selected is English, and it may be\npeer-reviewed. The inclusion and exclusion criteria to select\nthe papers are as follows:\nC. STUDY SELECTION RESULTS\nThe ﬂowchart for choosing the pertinent papers for this SLR\nis shown in Figure 3. The search string was selected to\nearmark 781papers from the different databases mentioned\nin Table 4. After removing duplicates and applying inclusion\nand exclusion criteria remaining 106 research papers were\nTABLE 4. Inclusion and exclusion criteria.\nconsidered for this SLR. Using snowballing techniques to\ninclude signiﬁcant contributions, the total was increased\nto 115. Finally, after applying quality assessment criteria,\n102 studies were selected for preparing a systematic literature\nreview.\nD. CRITERIA OF QUALITY ASSESSMENT FOR STUDY\nSELECTION\nQuality assessment criteria ensure the relevance of research\npapers to riposte the RQs. The research studies were graded\nas 1 or 0 according to the criteria mentioned in Table 6.\n17082 VOLUME 10, 2022\nR. M. Samantet al.: Framework for DL-Based Language Models Using MTL in Natural Language Understanding\nTABLE 5. PIOC information (Population, Intervention, Outcome, Context).\nTABLE 6. Quality assessment criteria.\nThe score 4 of quality is considered for conducting this SLR.\nTable 9 lists the quality assessment criteria.\nE. DATA EXTRACTION\nA concise overview of the data extraction process to answer\nthe research questions from the studies is listed in Table 7.\nF. DATA SYNTHESIS\nTable 7 shows the data synthesis to address the Research\nQuestions elaborately.\nIV. BACKGROUND\nNatural language understanding involves building language\nmodels, training these models, and testing them for accuracy.\nVarious NLU tasks, such as question answering and NLI, can\nbe cast as a classiﬁcation problem. This section presents the\nTC tasks given in this study. Sentiment analysis is the method\nof extracting the polarity and lookout of customers’ views.\nThe problem can be expressed as a two-class or multiclass\nproblem. A news classiﬁcation system can aid consumers in\nopting into relevant news in the present by spotting incipient\ntopics or making appropriate news suggestions depending\non the reader’s preferences. Topic classiﬁcation is a job\nTABLE 7. Categorization of the chosen Studies to answer research\nquestions.\nthat involves determining the overall theme or title of a\ndocument, whether a movie review is about viewer rating\nor revenue grossed over the speciﬁed period. Question-and-\nAnswer (QA) Extractive and generative QA tasks are the\ntypes of QA tasks. Extractive Take extents in a document in\nSQUAD [19] as an example of a Text Classiﬁcation task for\na question and a collection of an appropriate answer. Each\ncandidate’s response is classiﬁed as correct or not correct\nby the algorithm. QA-NLI forecasts whether the meaning\nof one text can be predicted from the meaning of the other.\nA label belongs to entailment contradiction and is unbiased\nto a couple of Text units by an NLI system [20].\nQA Extractive and generative QA tasks are the two types\nof QA tasks. Extractive Tasks are spread across the document\nlength in SQUAD [23] as an example of a TC task given\na question and a collection of probable responses. Each\ncandidate’s response is classiﬁed as correct or not correct by\nthe algorithm. This study only discusses extractive QA, a text\ncreation assignment that generates answers on the ﬂy.\nNLI forecasts whether the meaning of one text may\nbe predicted from the meaning of the other. A text pair\ncomparison is a generalized kind of NLI called paraphrasing.\nThe problem of determining how likely one sentence is\nparaphrased from the other by comparing the semantic\nsimilarity of two sentences.\nNeural Machine Translation - The objective of neural\nmachine translation is to translate text by simulating the\ncapabilities of the human brain. The goal is to translate a\ngiven source language into a target language retaining its\nmeaning and intent. When translating, human brains ﬁrst\ncomprehend the sentence, then build a mental representation\nof the sentence, and ﬁnally convert this mental representation\ninto a sentence in another language.\nThrough two modular processes of encoding and decoding,\nneural machine translation imitates the human translation\nprocess. The encoder turns source language utterances into\nsemantic space vector representations. Depending on the\nsemantic vectors produced by the encoder, the decoder con-\nstructs semantically identical phrases in the target language.\nRNN machine translation is a crucial basic model, and there\nVOLUME 10, 2022 17083\nR. M. Samantet al.: Framework for DL-Based Language Models Using MTL in Natural Language Understanding\nFIGURE 3. Flowchart for selection of relevant papers.\nhave been numerous advancements in advanced network\ntopologies and unique model training methodologies.\nMachine reading comprehension is the task of reading\nand comprehending the text by a computer program. The\nendeavour necessitates the creation of systems that can\nrespond to queries after reading a document. This require-\nment has many uses, including allowing search engines to\noffer intelligent and correct responses to natural language\ninquiries by reading related text. Furthermore, reading\ncomprehension machines can be used in virtual assistants to\nanswer customer support questions after reading documents.\nIt can also be utilized in the workplace to assist users in\nreading and processing emails or business papers, as well\nas summarizing pertinent information. Utilizing large-scale,\nmanually annotated datasets has aided recent improvements\nin machine reading comprehension. This section looks at the\nnumerous deep learning models that have been presented\nfor text categorization problems. Based on their model\nstructures, these models are put into the following categories:\n1. Text is considered as a bag of words in feed-forward\nnetworks (section A) 2. RNN-based models are utilized to\nguess word dependencies by viewing the text as words in a\nspeciﬁc order. (Section B) 3. For text categorization, CNN-\nbased models are taught to identify text styles. (Section C)\n4. Capsule networks deal with the problem of information\nloss in CNS pooling operations and have been used for text\ncategorization (section D) 5. The attention mechanism is\nuseful in constructing Dell models because it is effective in\nidentifying correlated words in the text (section E) 6. Graph\nneural networks are designed to represent natural language’s\ninherent graph structure. (Section F) 7. Hybrid models\nsyndicate attention and texts to capture local and global\nfeatures (section G) 8. Transformers are mainly employed for\nfar more parallelization than RNNs, allowing for GPU-based\ntraining of huge language models (section H). These various\ntypes of models based on deep learning techniques is shown\nin Figure 4.\nA. FEED-FORWARD NETWORKS BASED MODELS\nSimple DL models for text representation include feed-\nforward networks. Despite this, they have a good level of\naccuracy on several TC benchmarks. Text is viewed as a\ncollection of words in these models. These models acquire\na vector representation for each word by word2vec [21]\n17084 VOLUME 10, 2022\nR. M. Samantet al.: Framework for DL-Based Language Models Using MTL in Natural Language Understanding\nFIGURE 4. Types of language models based on Deep Learning.\nor GloVe [22]. These are popular embeddings models.\nJoulin et al. [23] introduced another classiﬁer called fastText.\nIt is efﬁcient and straightforward. A collection of n-grams\nis used as a supplementary feature in fastText to know\nthe information about local word order. This technique\nproves efﬁcient by reporting comparable results to the\nmethods that utilize the order of words [24]. Le and\nMikolove [25] introduced doc2vec, a method for learning\nﬁxed-length feature depictions of variable-length text, using\nan unsupervised algorithm.\nB. RNN-BASED MODELS\nUsually, the text is treated as an order of words in RNN-\nbased models. The basic purpose of an RNN-based model for\ntext categorization is to capture word relationships between\nsentences and text structure. Plain RNN-based models, on the\nother hand, do not perform as well as standard feed-forward\nneural networks. The Long Short-Term Memory (LSTM)\nmodel is one of many RNN variations meant to acquire long-\nterm dependencies of words of sentences.\nA memory cell with input, output, and forget gate is\ninvented to remember the values over a stipulated time frame.\nLSTM models address the vanishing gradient and gradient\nexploding problems that plain RNNs suffer from using this\nmemory cell. Tai and his colleagues, [26] to learn rich\nsemantic representations, built a tree-LSTM model, which is\na generalization of LSTM structured network topologies. Due\nto the syntactic structures of natural language, it combines\nwords to form phrases, tree-LSTM is a more efﬁcient model\nfor NLP tasks than chain-LSTM. They demonstrate the\nperformance of tree-LSTMs on two tasks: sentiment analysis\nand forecasting semantic relation between two sentences.\nReference [27] Zhu et al. improved the performance of\nthe chain-structured LSTM to its predecessors by storing\nmany successor cells by a recursive process using memory\ncells.\nVOLUME 10, 2022 17085\nR. M. Samantet al.: Framework for DL-Based Language Models Using MTL in Natural Language Understanding\nBy capturing useful information with various timeframes,\nthe multi-time scale LSTM (MT-LSTM) neural network [28]\nis constructed to represent signiﬁcant texts like sentences and\ndocuments. The hidden layers of a typical LSTM model are\ndivided into numerous classes by MT-LSTM. At different\ntimes, each group is active and updated. MT- LSTM can\nsuccessfully ﬁt many documents in the model. MT- LSTM\nis reported to beat a collection of baselines, including LSTM\nand RNN-based text categorization models. RNNs are better\nat memorizing the local structure of word order, but they\nstruggle with long-range type dependencies. For sentiment\nanalysis, TopicRNN is said to outperform the RNN baseline.\nOther RNN-based models are also intriguing. Multi-task\nlearning is used by Liu et al. [29] to train RNNs to utilize\nannotated data for training from various linked tasks.\nC. MODELS BASED ON CNN\nCNNs are taught to identify patterns in space, while RNNs are\ntrained to detect patterns over time [30]. RNNs perform well\nin NLP tasks like RQA-POS tagging, which need an under-\nstanding of long-range semantics, but CNN performs well\nin situations where sensing local and location-independent\npatterns in the document is critical. These observed patterns\ncould be signiﬁcant sentences that convey a speciﬁc emotion.\nAs a result, CNN has become the popular choice for common\ntext categorization model designs. Kalchbrenner et al. [31]\nsuggested one original text categorization algorithm. Based\non CNN. The model is termed the Dynamic K- Max\nPooling Model depending on the speciﬁed pooling technique.\nDynamic CNN (DCNN) is a cable news network. The ﬁrst\nstage of DCNN produces sentence metrics. The second\nstage involves a convolutional structure that integrates wide\nconvolutional stages with dynamic convolutional settings.\nThe dynamic K-Max-pooling layers are utilized to con-\nstruct a map of features over the entire sentence that\ncaptures various degrees of relatedness between the terms.\nThe pooling parameter is selected at run-time based on\nthe size of the sentence and convolution hierarchy level.\nFor text categorization, Kim [32] suggests a considerably\nsimpler CNN-based model. In this research, four distinct\nways to learn word embeddings are compared:1. The model\nCNN-rand randomly initializes all word embeddings and\nthen modiﬁes them throughout training. 2. CNN-static,\nin which the pre-trained word2vec embeddings are utilized\nand remain static throughout model training, 3. In CNN-\nnon-static, the word2vec embeddings are adjusted throughout\ntraining duration for individual task tasks. 4. In CNN-\nmulti-channel, two sets of word embedding vectors, which\nare prepared using word2vec, and one model is modiﬁed\nthroughout training, and the other remains unchanged. These\nmodels based on CNNs outperform previous models in\nsentiment scrutiny and question classiﬁcation. Liu et al. [33]\npresented a novel CNN-based model that modiﬁes the\nKim- CNN architecture in two ways. A dynamic Max\npooling approach collects additional detailed information\nfrom various document parts. Second, a hidden bottleneck\nlayer is positioned in between the pooling and output layers\nto learn compact document representations to lessen the size\nof the model and improve accuracy. Rather than using low-\ndimension word vectors as input to CNNs, the researchers\nuse high-dimension text to know the embeddings of short text\nareas of categorization in [33].\nPrusa and Khoshgoftaar [34] offered a technique for\nencoding input text by using CNNs that signiﬁcantly\ndecreases the amount of memory used and the amount of\ntraining time necessary to acquire alphabet-level text data\nrepresentations. In this method, the model grows with the\nnumber of characters, allowing better information from the\ntext to be maintained in the augmented version.\nThere have been studies looking into how word embed-\ndings and CNN architectures affect the model’s performance.\nConneue et al. [35] introduced a VDCNN model which uses\ndeep architecture model for the task of text classiﬁcation,\nwhich is inspired by resNets [36]. It uses modest convolutions\nand pooling operations and works directly at the character\nlevel. The performance of VDCCN improves as the depth\nis increased, according to this study. Deque et al. [37]\nchanged the structure of VDCNN to match the limits of\nmobile platforms without sacriﬁcing performance. They\ncould reduce the model size by up to 20x with a 0.4 %\nto 1.3 % accuracy loss. Guo et al. [38] investigated the\neffects of word embeddings and recommended that weighted\nword embeddings be used in CNN model with multiple\nchannels. Zhang and Wallace [39] studied various types of\nword embedding techniques and mechanisms, for pooling\nconcluding that word2vec and GloVe perform better than\none-hot vectors. Max-pooling is the best among the existing\npooling approaches.\nD. CAPSULE NEURAL NETWORK-BASED MODELS\nCNN uses several layers of convolutions and pooling to\nclassify pictures or text. Pooling operations detect signif-\nicant features and minimize the computation complexity\nof convolution processes, but they miss spatial information\nand may misclassify items depending on their orientation\nor proportion. Hinton et al. [40] offered a new technique\ntermed capsule networks to overcome the challenges of\npooling (CapsNets). A capsule is a neuron collection whose\nactivity vector shows many characteristics of a block or\npartial block. The length of the vector denotes the likelihood\nthat the block exists, and the vector’s orientation represents\nthe block’s properties. Capsules direct every capsule from\nthe below layer to its best suitable parent capsule in the\nabove layer, by available information in the network up to\nthe last layer for categorization will stop directing. This\ntask can be achieved using various algorithms like dynamic\nrouting- by- agreement [41] or the Expectation-Maximization\nalgorithm [42]. These networks have recently been used to\nclassify text, with capsules customized by representing a line\nin the document or the entire document in the form of a\nvector. Kim et al. [43] developed a capsNet-based model\nwith a comparable architecture. The model is made up of\n17086 VOLUME 10, 2022\nR. M. Samantet al.: Framework for DL-Based Language Models Using MTL in Natural Language Understanding\nfour layers: 1. Documents are accepted by the input layer\nas a series of word embeddings, 2. Feature map is made\nby a convolutional layer and utilizes a gated-linear unit to\nretain certain information, 3. Local features are gathered\nby a convolutional capsule layer, 4. And ﬁnally, a text\ncapsule layer that predicts class labels. Objects can be built\nmore freely in text than in photographs, according to the\nauthors. In contrast to the positions, the semantics of a\ndocument can remain unchanged, although the sequence of\nsome lines of the document is changed. Ren and Lu [44]\npresented a new variation of capsNets that employs a\ncombined architectural style among capsules and a novel k-\nmeans clustering-based routing algorithm. First, all codeword\nvectors in codebooks are used to create word embeddings.\nThe lower-level capsules’ features are then consolidated in\nhigh-level capsules utilizing k-means routing.\nE. MODELS WITH MECHANISM OF ATTENTION\nThe way one pays attention to distinct sections of a\nphotograph or related words of a single sentence motivates\nattention. Attention is becoming a central concept and tool\nin developing DL models for NLP [45]. It can be thought of\nas a vector of signiﬁcant weights in a nutshell. To guess a\nword in a sentence, it is estimated how strongly it is related\nto the other words by using the attention vector, and by\nadding weighted values of the attention vector, the target\nvalue is predicted. This section examines some of the most\nwidespread attention models that help build a new frame of\nmind. Yang et al. [46] suggested a network based on the\nmechanism of hierarchical attention for text categorisation.\nThis model has two distinguishing features: one hierarchical\nstructure that mimics the hierarchical structure of the\ndocument and two levels of attention mechanisms applied\nboth at the word level and sentence levels, allowing it to\npay differential attention to more and less signiﬁcant parts\nwhile constructing the document representation. On six text\ncategorization tests, this model surpasses earlier methods\nby a substantial margin. The hierarchical attention approach\nis extended to cross-lingual sentiment classiﬁcation by\nZhou et al. [47]. The document is modelled using an LSTM\nnetwork in each language. Afterwards, the ﬁnal sentiment\nanalysis is performed using hierarchical attention in the\nsentence-level model. The attention models, designed at\nword-level, on the other hand, understand important words\nin each stage. Shen et al. [48] discovered a model which\nhas self-attention mechanism for NLP, with directional and\nmulti-dimensional attention between elements from input\nsequences. To learn sentence embedding, a high-weight\nneural net is utilized, which is exclusively dependent on the\ntype of attention and has not consist of CNNs or RNNs.\nLiu et al. [49] provided an LSTM model for NLI that includes\ninner attention. To encode a text, this model employs a\ntwo-stage procedure. First stage sentence representation is\ngenerated using average pooling across word-level Bi-LSTM.\nAfter that, an attention mechanism is used to swap average\npooling on the same phrase with superior representations.\nThis technique pushed up the sentence representation at the\nﬁrst stage and further utilized attention to process the text.\nF. MODELS BASED ON GRAPH NEURAL NETWORKS\nEven though ordinary texts have a serial order, they also\ncomprise inherent graph structures similar to parse trees that\nspeculate the relationships based on syntax and semantics\nof the sentences. TextRank [50] is an original graph-based\nNLP model developed. The authors propose that a natural\nlanguage text be represented as a graph G (V , E), with V\ndenoting a collection of nodes and E denoting a set of\nedges between the nodes. Nodes characterize various text\nunits that complete the sentences. Depending on the type\nof applications, edges can also be used to express multiple\nforms of relationships between nodes. Contemporary Graph\nNeural Network (GNN) is created by adapting DL methods.\nTextRank is one such method to graph data. Over the\nlast few years, DNN models based on CNNs, RNNs, and\nautoencoders have been adapted to handle the complexity\nof graph data [51]. GCNs [52] and their derivatives are\nvery prevalent among the numerous types of GNNs as they\nare operative and easy to mix with other networks, and\nthey have reached optimal results in various applications.\nOn graphs, GCNs are a more efﬁcient variation of CNNs.\nTo learn graph representations, GCNs pile layers of ﬁrst-\norder spectrum ﬁlters trailed by an activation function, which\nis nonlinear. TC is a common application of GNNs in NLP.\nTo infer document labels, GNNs use the interrelationships of\ndocuments or words [53]. For TC, Yao et al. [54] employed\na GCNN model which uses CNN with graph networks. They\nlearn a Text Graph Convolutional Network (Text GCN) for\nthe corpus after creating a single text graph. It is based\non words occurring together and word relations among\nthe document. Text GCN starts learning with a one-hot\nrepresentation of every word in the document and then further\nlearns embeddings for both documents and documents. This\nmethod is supervised by known document class annotations.\nIt’s expensive to train GNNs for a colossal text corpus. Efforts\nhave been made to reduce the cost of modelling by either\ndropping model complexity or adapting the model training\ntechniques.\nG. MODELS WITH HYBRID TECHNIQUES\nMany hybrid models have been built to detect global and local\ndocuments by combining LSTM and CNN architectures.\nA Convolutional LSTM (C-LSTM) network is proposed\nby Zhou et al. [55]. C-LSTM uses a CNN to excerpt an\narrangement of phrase (n-gram) representations, then input\nto an LSTM network to generate the sentence-level represen-\ntation. For document modelling, Zhang et al. [56] suggest\na Dependency Sensitive CNN (DSCNN). Chen et al. [57]\nused a CNN-RNN model to perform multi-label TC. A CNN\nis used by Tang et al. [58] to understand sentence repre-\nsentations that encode the inherent relationships between\nsentences. Xiao and Cho [59] considered a document in the\nspeciﬁc order of characters rather than words and suggested\nVOLUME 10, 2022 17087\nR. M. Samantet al.: Framework for DL-Based Language Models Using MTL in Natural Language Understanding\nencoding documents using alphabet-based convolution with\nrecurrent layers. When compared to word-level models, the\nproposed model achieved comparable results with many\nfewer parameters.\nFor learning word representations, Recurrent CNN [60]\nuses a recurrent structure for detecting long-range contextual\ndependence. To cut through the clutter, max-pooling is used\nto automatically select the most important terms for the\ntext categorization task. In machine reading comprehension,\nLiu et al. [61] proposed a resilient Stochastic Answer\nNetwork (SAN) for reasoning using a multi-step approach.\nSAN incorporates multiple types of neural networks, such\nas networks with memory. Bi-LSTM, CNN, attention, and\nTransforms are all examples of transforms. The represen-\ntations of the context for questions -answering system for\nthe passages are attained using the Bi-LSTM component.\nIts attention method generates a passage representation that\nis question-aware. The passage is then stored in a working\nmemory created by another LSTM. Finally, predictions are\ngenerated by an answer module that uses a Gated Recurrent\nUnit. Combining highway networks with RNNs and CNNs\nhas been the subject of several studies. Information travels\nlayer by layer in conventional multi-layer neural networks.\nWith increasing depth, gradient-based DNN training becomes\nmore complex. Highway networks [62] are designed to\nmake deep neural network training easier. They permit the\nunrestricted ﬂow of information across multiple levels over\ndata highways, akin to ResNet [36] quick connections.\nH. MODELS BASED ON TRANSFORMERS\nThe sequential processing of text is one of the computational\nobstacles that RNNs face. Even though RNNs are more\nsequential than CNNs, the computing cost of capturing\nassociations among words in a phrase climb with the\nlength of the sentence, much like CNNs. Transformers-based\nmodels [7] overcome this restriction by using self-attention\nto calculate an ‘‘attention score’’ for every word in a sentence\nof the document simultaneously, modelling the inﬂuence of\neach word on the others. Transformers, unlike CNNs and\nRNNs, allow for far more parallelization, allowing for the\nefﬁcient training of huge models on massive volumes of data\non GPUs.\nSince 2018, several huge-scale Transformer-based Pre-\ntrained LMs have emerged. Transformer-based models\nemploy far deeper network architectures (viz., 48-layer based\nTransformers [63]). The pre-training of these models is also\ndone on larger amounts of text to capture the context of the\ntext representations.\nPopular PLMs are classiﬁed by representation forms,\nmodel styles, pre-training tasks, and relevant tasks, according\nto the latest survey by Qiu et al. [64]. Autoregressive\nPLMs and autoencoding PLMs are the two types of PLMs.\nOpenGPT [8], a unidirectional model that forecasts a text\nverbatim from either left to right direction or vice-versa,\nwith every word prediction based on earlier predictions,\nbeing one of the early autoregressive PLMs. By adding\nlinear classiﬁers for relevant tasks and adjusting labels related\nto tasks, OpenGPT can be tailored to downstream tasks\nlike TC. BERT [9] is one of the baselines autoencoding\nPLMs. Contrasting OpenGPT, which forecasts words based\non past forecasts, BERT is normally trained by utilizing the\nmasked language modelling (MLM) task, which arbitrarily\nmasks some part in a text sequence and then improves\nthem independently using encoding vectors produced by\na Transformer, which processes text in both directions.\nRoBERTa [65] is a more robust version of BERT trained\nwith more data. ALBERT [66] reduces BERT’s memory\nuse while increasing its training pace. DistillBERT [67]\nuses knowledge distillation technique throughout pre-training\nto reduce BERT’s size by almost half while preserving\nits original capabilities and speeding up inference by a\nfactor of two. SpanBERT [68] is a BERT extension that\nimproves the representation and prediction of text spans.\nExternal knowledge bases are incorporated into ERNIE [69]\nto improve performance. XLNet [70] combines the concepts\nof autoregressive models such as OpenGPT and BERT.\nAs previously stated, OpenGPT learns text representation for\nnatural language creation using a left-to-right Transformer,\nwhereas BERT employs a transformer, which processes text\nin both directions, for natural language interpretation. Uniﬁed\nLanguage Model (UniLM) [71] is a model for understanding\nand creating natural language. UniLM has been pre-trained\non different types of language modelling tasks which are\nnot related to the direction of parsing. XLNet uses a\ntransformation operation throughout the pre-training phase to\nallow words from both the left and right sides of the context to\nbe included, making it a bi-directional autoregressive model.\nIn the Transformers model, the transformations are performed\nby utilizing a speciﬁc attention mask. To facilitate position-\naware word prediction, XLNet provides a two-stage self-\nattention schema. This schema is based on how distributions\nof words change dramatically, relevant to word location.\nAs previously stated, OpenGPT learns text representation\nfor natural language creation using a left-to-right Trans-\nformer, whereas BERT employs a transformer that can pro-\ncess text in both directions for natural language interpretation.\nThe Uniﬁed Language Model (UniLM) [71] is a model\nfor understanding and creating natural language. UniLM\nhas been pre-trained on different language modelling tasks\nwhich are not direction speciﬁc. A common Transformer\narchitecture fused with the speciﬁc self-attention masks\nare used to create the uniﬁed modelling. UniLM [71]\nhas reached new SOTA performance for natural language\ninterpretation and generating tasks, outperforming prior\nPLMs. The performance analysis of signiﬁcant language\nmodels, along with the techniques employed, is summarized\nin Table 8.\nV. RESULTS\nThis section provides the answers to the research questions.\nMajor issues and challenges are outlined in multitasked-based\ndeep learning language models.\n17088 VOLUME 10, 2022\nR. M. Samantet al.: Framework for DL-Based Language Models Using MTL in Natural Language Understanding\nTABLE 8. Performance analysis of significant language models.\nRQ1. What are the different algorithmic approaches\navailable in combining MTL to improve the accuracy of the\nproposed framework for NLU?\nThe major challenge in NLU research involves language\nrepresentation of various tasks for training. The previous\nmethod, rule-based NLU, relied on human-generated char-\nacteristics. As a result, it cannot recognize unexpected\nwords and requires a signiﬁcant amount of time for features\nengineering. NLU, based on deep learning, on the other\nhand, extract features and learns language representations\nautomatically. MTL models, which were created from the\nbase of single-task learning, which learns a generalized\nrepresentation of while preventing overﬁtting of a given\nindividual task, have gained a lot of attention in recent years.\n• MT-DNN [72]- This model and MTL models based\non the bidirectional transformer are two examples of\nMTL models. MT-DNN learns many tasks at the same\ntime. Instead of travelling through both common and\ntask-speciﬁc levels, this approach, on the other hand,\nuses a method of knowledge distillation that teaches\na smaller multi-task model using larger single-task\nmodels. MTL models, on the other hand, have several\ndifﬁculties when it comes to fulﬁlling the essential duty\nof maintaining overall accuracy. The accuracy of the\nmodel can be affected due to joint training with the equal\nweights.\nFurthermore, during MTL, the model is unable to master\nthe main job fully. These drawbacks are compensated\nby presenting the SIWLM (Sequential and Intensive\nWeighted Language Modeling) scheme, inspired by\nYang et al. [73]. SIWLM comprises two types of learn-\ning: sequential weighted learning (SWL) and intensive\nweighted learning (IWL). Core and supplementary tasks\nin SIWLM have an initial task weight, and all tasks\nare independently changed during training. The loss\nfunctions are multiplied by these altered weights. The\nideal weight for each job in the GLUE datasets can\nbe calculated. The MTDNN-SIWLM attains equivalent\nor better performance on all GLUE datasets when that\nweight is applied.\n• Task Interference- Another challenge is tackling task\ninterference in multi-task learning. Task interference\ncan be seen as a paradox of invariance vs sensitivity:\nessential information for one task may be futile infor-\nmation for the other, resulting in possibly conﬂicting\naims while training multi-task networks leading to poor\noverall performance.\n• Attention Strategies - The studies [76-77] propose\ntwo separate task attention strategies. To begin, task-\nspeciﬁc data-dependent modulation signals boost or\ndecrease neural activity. Second, task-speciﬁc Residual\nAdapter extracts task-speciﬁc information blended with\nthe symbols created by a common task structure. This\nmethod enables us to learn a common symbol system\nthat serves all activities while also collaborating with a\ntask-related processing to develop more complex task-\nrelated structures. Different task attention strategies may\nlead to minimum task interference.\n• Common features learning- [76] describes a method\nfor learning a low-dimensional representation used in\nvarious applications. The method uses a new regularizer\nto manage the number of learning features common\nto all tasks, which builds on the well-known 1-norm\nregularization problem. The authors show that this prob-\nlem is comparable to a convex optimization problem\nand propose an iterative solution. The approach has\na straightforward interpretation: it alternates between\nsupervised and unsupervised steps, in which the latter\nlearns representations that are common across tasks,\nand the former learns task-speciﬁc functions using these\nVOLUME 10, 2022 17089\nR. M. Samantet al.: Framework for DL-Based Language Models Using MTL in Natural Language Understanding\nrepresentations. There is scope to extend this approach\nand develop new iterative algorithms.\nWhile deep learning and deep reinforcement learn-\ning (RL) has shown promise in enabling computers\nto perform complicated tasks, existing approaches data\nneeds make it challenging to acquire a wide range\nof capabilities, especially when each work is learned\nindependently from scratch. To solve multi-task learning\ndifﬁculties, a natural strategy is to train a network\non multiple tasks simultaneously to uncover shared\nstructure across the tasks in a way that is more efﬁcient\nand effective than tackling tasks individually. [77] It may\nobtain the hypothesized beneﬁts of multi-task learning\nwithout the cost of ultimate performance if optimization\nissues are properly addressed for multi-task learning.\nAll the NLP tasks, typically QA, content summarization,\nNLI, formulate the candidate tasks are considered a bench-\nmark for the Natural Language Decathlon (decaNLP). [72].\nThis study proposes an approach to recast all jobs as a\nseries of questions to be answered in a certain context.\nA novel multi-tasks question answering network (MQAN) is\nproposed, which learns decaNLP tasks without ﬁne-tuning\nany network [72]. The knowledge representation other than\nthe QA pair needs to be evaluated for MTL- NLU.\nThe approach suggested in MT-DNN [72] appears to be\npractical due to its exhaustive nature. The accuracy of the\nmodel is sensitive to strategies adopted for effectively han-\ndling task interference. Higher amount of task interference\ncan adversely affect the performance of the model.\nRQ2 Which are the standard benchmarking datasets for\nMTL- NLU tasks evaluation?\nGLUE [78] and its successor SuperGLUE [79] are the\nstandard benchmarks for evaluating a model’s performance\non a set of tasks rather than a single task to keep a\nbroad picture of NLU performance. GLUE comprises of the\nfollowing ingredients:\n1. A benchmark dataset consists of nine text-related NLU\ntasks based on speciﬁed datasets and chosen to deal with\na wide variety of dataset parameters like size, type and\nhardness of tasks.\n2. A analytical dataset for evaluating and analyzing the\nframework accuracy in natural language concerning a\nwide range of linguistic phenomena and a public leader\nboard for tracking performance.\nIn recent years, new pre-training and transfer learning\nmodels and approaches have resulted in signiﬁcant\nperformance gains across various language comprehen-\nsion tasks. The GLUE benchmark, introduced in 2019,\nprovided a single-number metric summarising progress on\nvarious such tasks.\nHowever, performance on the benchmark has lately\napproached that of non-expert humans, indicating that\nthere is limited potential for further development.\nIn [79], SuperGLUE, a new benchmark inspired by\nGLUE that includes a new set of more difﬁcult language\ncomprehension problems, enhanced resources, and a\nTABLE 9. Detailed information about GLUE and superGLUE SOURCE\n[80,81].\nnew public leaderboard, is established as a de-facto\nbenchmarking standard for MTL-NLU. Table 9 enlists the\ndetails about these datasets.\nThe ﬁeld of benchmarking datasets for natural language\nunderstanding is rapidly evolving. There is deﬁnitely a\nfuture possibility to include more tasks related to NLU.\nRQ3. Which are the learning methods for improving the\nlearning performance of NLU in combining multiple tasks?\nText-based games use natural language to recreate worlds\nand interact with players. Recent research has used them as\na testbed for autonomous language-understanding bots, with\nthe premise that comprehending the meanings of words or\nsemantics is critical to how humans understand, reason, and\nact in these worlds. However, it’s unclear how much semantic\nunderstanding of the text is used by artiﬁcial agents.\n• Semantics-The studies [82-83] conducted in the context\nof ZORK-I, a text adventure game, revealed contradic-\ntory evidence to this basic principle and proposed an\nimprovement in the structure of the NLU unit. It probes\nthe extent of semantics in the reinforcement learning\nagents used in text adventure games. It also demonstrates\n17090 VOLUME 10, 2022\nR. M. Samantet al.: Framework for DL-Based Language Models Using MTL in Natural Language Understanding\nthe techniques to adjust the semantics plugged into the\nsystem.\n• In joint training in the study [82], the authors proposed\na deep RNN with partial relative dialogue memory by\nmutually training the NLU unit and System Action\nPrediction unit. This approach is different from the\nconventional one where the NLU unit and SAP unit\nare trained in a pipeline. The major drawback of noisy\nNLU badly affecting the performance of SAP unit is\nsuccessfully handled.\n• Gated-Attention-The study [83] proposed Gated-\nAttention Reader- an integrated model with a new\nattention mechanism based on multiple interaction. This\ntechnique permits the user to create a query-related rep-\nresentation of tokens for accurate response prediction.\nThis study improves on attention mechanism to improve\nthe accuracy of the results. The study [84] proposed\nUMLFiT, a technique to ﬁne-tune any language model.\nIt is a transfer learning method applicable to any task.\nThis method effectively outperforms the existing models\non various TC tasks, and the prediction errors are\nsigniﬁcantly reduced by 19 to 24 %. It also requires only\n100 labelled samples to match the training from scratch\non 100x more samples. In the study [85], a novel training\ntechnique is used to train CNN in news comprehension\nexperiments that use news articles and summarized\nbullet points in the form of QA pairs for training.\nThis method adds a new training method for QA\nsystems.\n• Frame semantics-The study [86] proposed a new\ntheory—Frame semantics for the English language.\nIt addresses the issue of frame-semantic analyzing\nby means of a statistical method which treats lexical\ntargets in their sentential context and predicts frame\nsemantic structures is handled. This model is based on\nlatent variables and semi-supervised learning to remove\ndisambiguates from frames.\n• Construction Grammar-In the study [87], Construc-\ntion Grammar (CG) is used along with AI to represent\nthe knowledge for a deep understanding of a text. The\nexperiments involve Winograd Schema (WS) – a major\ntest for AI. The results showed that the proposed CG\napproach has more potential for task resolution in the\ndeep understanding of natural languages.\n• Transformer-based representation-The study [88]\nsummarizes the latest transformer-based models related\nto NLP models. It provides a thorough explanation and\nworking of various designs, relative assessments, and\nforthcoming guidelines in NLP.\n• GROW Model-In the study [89], a method is proposed\nwith a pro-active attitude driving the dialogue to reach\ndifferent coaching goals for elderly users. It used the\ncommunication supporting technique based on GROW\nmodel.\n• FastText-The study [90] evaluates the word embeddings\ntechniques for the Nursing care domain. It proposes an\nautomatic labelling framework for dialogues between\npatients and nurses to record care activities. It also\npointed out fastText as a better word embeddings model\nby achieving 0.79 as an F1 score measure.\n• NLU Services -Study [91] describe the functioning of\nNLU services and their role in the general architecture\nof a chatbot. It presents the comparison of existing\nNLU services like DialogFlow (Google), Watson (IBM),\nand Alexa (Amazon). Study [92] focuses on the Italian\nlanguage as a non-English language for using the NLU\nengine.\n• RASA-NLU-In the study [93], a functional framework\nis proposed, and the principles of RASA NLU are\nintroduced. It combines neural nets (NN) and RASA\nNLU for implementing the systems based on entity\nextraction after recognizing intents. The ﬁndings state\nthat RASA NLU outperforms NN for a single word,\nbut NN has better integrity for segregated words\nclassiﬁcation. The study [94] highlights an approach\nto syndicate various types of semantics and language\nmodels while pre-training and ﬁne-tuning stage for the\nimprovement in the accuracy of prediction.\n• OOD Input- The study [82] deals with the problem\nof Out of Domain (OOD) input. OOD input may\nlead to system failure. A novel method to generate\nhigh-quality pseudo-ODD samples is proposed. The\ntraining is done using a generative adversarial network.\nAn auxiliary classiﬁer is used to regularize the gen-\nerated OOD samples. The results show improvement\nin OOD detection and efﬁcient utilization of unlabeled\ndata.\n• Semantic Vector - In the study [75], semantic vector\nlearning for NLU is explored. An NLU embeddings\nmodel is focused in the light of the understanding\nrelationship between unstructured text and correspond-\ning structured semantic knowledge. The contribution\nmethod is to create matching vector with relevant seman-\ntic frame. The applications of this model include visu-\nalizing distance-based semantic search and similarity-\nbased intent classiﬁcation and re-ranking.\n• Selective classiﬁer method-The study [95] tries to\nsolve the problem of domain-speciﬁc training. A new\ntechnique called a top-down particular multi-classiﬁer\nsystem ensemble model is proposed, and it offers a\nsigniﬁcant improvement over the word embeddings\nmethod. The study [96] demonstrates an effective user\ninterface design for NLU based stock analysis system.\nThe system is based on RNN with hyperparameter tun-\ning. The study [97] substantiates the theoretical analysis\nof NLU as a methodological problem. The study [98]\nunderlines the unsuitability of NLP approaches for\nNLU systems. The conventional methods based on\nstatistical-based supervised learning must be replaced\nby the holistic cognitive modelling approach. A new\nparadigm called Onto-Agent- based on human cognition\nis proposed.\nVOLUME 10, 2022 17091\nR. M. Samantet al.: Framework for DL-Based Language Models Using MTL in Natural Language Understanding\n• Learning without forgetting - In the study [99], the\nproblem of adding new tasks with existing training data\nis tackled. The proposed method – learning without\nforgetting with CNN is capable of learning new tasks\nwith new task data; while preserving the original\ncapabilities. This method is better than feature extraction\nand ﬁne-tuning adaptation in multi-task learning.\n• Garden Path sentences- The study [100] addresses the\nproblem of Garden Path (GP) sentences. It demonstrates\nthe effective use of Popescu’s model of NLU systems\nthat states that syntactic, semantic, and cognitive back-\nground knowledge is essential in training data.\n• Dynamic Integration of background knowledge-The\nstudy [101] introduces a new architecture for the\nactive integration of explicit background knowledge in\nthe NLU model. Experiments on Document Question\nAnswering (DQA) and recognizing textual entailment\n(RTC) demonstrate the effectiveness and ﬂexibility of\nthe proposal.\n• Representation conversion-The study [102] describes\nand evaluates various methods to the alteration of\nthe gold standard corpus data from Stanford-typed\nDependencies and Penn-style constituent trees to the\nmodern English Universal dependencies (UD 2.2). The\noutcomes show a reduction of 1.5% errors.\nAmong these approaches, the method of gated attention is\nprevalent in the current literature due to its efﬁciency.\nRQ4. Which techniques are effective for reducing the\nneed for huge annotated data samples?\n• VIRAAL- The study [105] proposes a new approach\n– Virtual Adversarial Active Learning (VIRAAL) to\nreduce annotation efforts in NLU systems. It uses\na semi-supervised model that regularizes the model\nthrough local distribution smoothness. The importance\nof Entropy-based active learning is underlined by\nquerying more informative samples without requiring\nadditional components. Results show that this method\nis robust on multi-task NLU training.\n• Sub-modularity-inspired data ranking function -The\nstudy [106] tries to address the problems for small\ndomains, which requires a huge amount of domain-\nrelated training data. The data selection technique is\nproposed in a low-data regime that enables training with\nfewer labelled data.\n• HUMOD- In the study [107], the unavailability of\na common metric to evaluate the replies against\nhuman judgment is handled. This study contributes by\ndeveloping a benchmark dataset with human annotation\nand diverse responses. HUMOD- a high-quality human-\nannotated movie dialogues dataset. It is created from the\nCornell movie dataset. Detailed analysis on the structure\nof dialogues and human perception score in comparison\nwith existing models is presented.\n• SSG Framework- The study [108] proposed a\ndual Learning technique for semi-supervised NLU.\nThis study introduces a dual-task, semantic to sentence\ngeneration (SSG) framework. It enables the NLU\nmodel to fully use labelled and unlabeled data. The\nframework achieves impressive results on publicly\navailable datasets like ATIS and SNP.\n• Auxiliary Tasks In the study [109], a novel approach\nof utilizing auxiliary tasks to provide additional super-\nvision of the main task to compensate for the data\npaucity is proposed. It addresses the issue of assigning\nand optimizing importance weights to auxiliary tasks. A\nweighted likelihood function of auxiliary task is formed\nas a surrogate before the main task leading to the reduced\nneed for additional training data.\nThe VIRAAL [72] method is more effective in reducing\nthe need for huge annotation samples for training.\nRQ5. Which are prevalent knowledge representation\ntechniques for MTL- NLU?\n• Word embeddings- Zhang and Wallace [39] studied the\neffects of several word-embedding methods and pooling\nmechanisms, concluding that word2vec and GloVe are\nbetter than one-hot vectors and that Max-polling is the\nbest among the existing pooling approaches.\n• ELMO - The Elmo [6] model performs substan-\ntially better than the word2vec approach because\nit captures contextual information in knowledge\nrepresentation.\n• EntityNLM-The study [110] proposes a knowledge\nrepresentation method for tracking and evolution of\nthe introduction of entities present in lengthy doc-\numents. The technique is called EntityNLM. It can\nmodel dynamic entities, dynamically update repre-\nsentations and contextually generate mentions of the\nentities. Different tasks like language modelling, con-\nference resolution, and entity prediction outperform the\nbaseline.\n• Model Pruning-The study [111] proposes a technique\nto compress bulky language models while preserving\ninformation for a explicit task. A speciﬁc layer is\nselected among various layers to prune the language\nmodel.\n• The model pruning [111] technique is more efﬁcient\nand widely used method in compacting knowledge\nrepresentation.\nVI. DISCUSSIONS\nThis study examined a signiﬁcant number of research\npublications (116 studies, to be speciﬁc) on various aspects\nfor building language models created using deep learning\napproaches in this SLR. Several critical questions regarding\nLanguage models were discussed in this review. The\nsummarized points are as follows -\n• FFNN (Feed-forward neural networks) see input text\nas a bag of words. The RNNs can understand word\nordering. The CNNs are strong in recognising patterns\nlike signiﬁcant words. The attention mechanisms are\ngreat at identifying associated words in the input text,\nSiamese Neural Nets s are utilised for text similarity\n17092 VOLUME 10, 2022\nR. M. Samantet al.: Framework for DL-Based Language Models Using MTL in Natural Language Understanding\ntasks and, GNNs performs better if natural language\nstructures are beneﬁcial to the intended task. But the\ntransformer-based models outperform all other models.\n• The architecture of the framework is determined by\nthe intended task, ready existence of the annotated\nsamples and the restrictions of application domain.\nThese requirements can only be fulﬁlled by hybrid\narchitecture language models.\n• Existing studies explore MTL-based language models,\nwhich use a supervised learning paradigm. Very few\nstudies combine other techniques with MTL to focus\non unsupervised learning. There is scope to com-\nbine transformer-based representations with MTL for\nimproving the efﬁciency and accuracy of the language\nframework model.\n• There is scope in MTL to efﬁciently handle the issue\nof out-of-domain task detection and task interference by\nusing the active learning technique. The availability of\nannotated samples is less in NLU tasks. There is scope\nfor developing approaches to reduce the need for more\nannotated examples using active learning techniques.\n• Pretrained Language models (PLM) improved perfor-\nmance for all text related tasks and autoencoding PLMs\nare more commonly used language models due to its\nefﬁciency.\n• The layers related to the particular task can be trained\nindependently or jointly with the PLM, depending on\nthe existence of domain related labels. Multi-task ﬁne-\ntuning is an excellent alternative for leveraging labelled\ndata from related domains.\n• The size of existing transformer-based models is humon-\ngous. There are many approaches available for the\ncompaction of transformer-based representation. The\noptimal strategy for text encoding for NLU is not\nestablished in the current literature.\n• Fusing commonsense knowledge into DL models has\nthe probability to boost model efﬁciency dramatically,\nsimilar to how humans use commonsense understanding\nto complete various jobs. A generic QA system, for\nexample, could answer queries regarding the real world.\nWhen there isn’t enough information to address a\nproblem, commonsense knowledge can aid. Machine\nlearning-based systems can reason about unknowns\nusing ‘‘default’’ conventions, similar to how people do,\nby using commonsense.\nWhile DL models have shown potential on challenging\nbenchmarks, the functioning of most of these models is not\ninterpretable. For instance, it is still unclear how few models\noutperform others on one dataset while underperforming\non others? What things exactly have deep learning models\ndiscovered? Although the attention mechanisms offer some\nperception into these concerns, a comprehensive analysis\nof these models’ underlying behaviour and dynamics is\nstill absent. Greater knowledge of these models’ theoretical\nelements can aid in the development of better models tailored\nto diverse text analysis settings.\nFIGURE 5. Proposed framework for multi-task NLU.\nA. PROPOSED FRAMEWORK\nThis study started the SLR to answer ﬁve Research Questions\nto lay the groundwork for future study in the ﬁeld of\nNLU. These RQs served as the cornerstone for the planned\nmulti-task NLU framework. Figure 5 depicts the framework\nproposed.\nAs depicted in Figure 5, the proposed framework is derived\nfrom BERT and has three bottom layers, which are common\nfor all the tasks, and the top-end layers show task-related\nrepresentation and output. The input is text converted as\nword embeddings. The transformer encoder then uses self-\nattention to get contextual information for each word and\ncreates a string of contextual embeddings in the next stage.\nThe exchanged semantic presentation is used to achieve\nrequired goals in multi-tasks. The active learning technique\nis further used to obtain more information from fewer but\nmore informative examples, reducing the demand for more\nsamples.\nThe training of the proposed framework includes stages\nlike pre-training and multi-task learning. The pre-training\nstage is similar to that of the BERT model. Masked language\nmodelling and next sentence prediction are used to learn the\nparameters of both the encoders. The model’s parameters are\nlearned using the stochastic gradient descent (SGD) method\nin the ﬁnal stage.\nThis framework is expected to perform well on the premise\nof combining multiple techniques.\nVII. LIMITATIONS OF THE STUDY\nThe present DL language models employed in text classiﬁ-\ncation tasks were examined and critically analyzed by this\nSLR. It offers comparisons and challenges for developing\nDL models for multi-task NLU. However, due to the scarcity\nof literature research and work in this ﬁeld, as well as the\nwide range of DL models, ﬁnding and selecting relevant\nliterature is a laborious, hard, and difﬁcult job. To meet the\nrequired inclusion and exclusion criteria, the keywords used\nto search for valuable publications and procedures may vary\nor change.\nVOLUME 10, 2022 17093\nR. M. Samantet al.: Framework for DL-Based Language Models Using MTL in Natural Language Understanding\nOne of the major restrictions of SLR for the domain is\nthat, even though a systematic review process is followed,\nit cannot guarantee that all pertinent works of the domain\nwere extracted. The most relevant electronic databases in\ncomputer science were included in the search databases.\nAnother limitation is the authors’ preconceived notions\nregarding the multi-task NLU procedure as a whole.\nThe suggested framework is in the design stages, and its\nempirical validation is beyond the scope of this SLR, but it\ndemonstrates the long-term research goals.\nVIII. FUTURE WORK AND CHALLENGES\nSeveral concerns will need to be addressed in future\ninvestigations.\n• Outlier tasks- To begin with, outlier tasks that are\nunconnected to other tasks are known to impede the\nperformance of all activities when they are learned\ntogether. There are a few strategies for reducing the\nharmful consequences of outlier tasks. However, there\nare no deﬁned approaches or theoretical assessments to\ninvestigate the detrimental consequences. This issue is a\ncritical issue that requires more research to make MTL\nsafe for human use.\n• Learning methods - Deep learning has emerged as\na prominent strategy in various ﬁelds, with various\nMTL deep learning models presented in the feature\nalteration, low-rank, task bunching, and task relatedness\nlearning methods. As previously said, the majority of\nthem simply have hidden common layers. This method\nis effective when all the tasks are linked, but it is prone\nto outlier tasks, signiﬁcantly degrade performance.\n• Security - The resilience of this multi-task DNN\nframework is to be checked against various types of\nattacks.\n• Extension- Finally, most studies to date have focused\non supervised learning tasks, with only a few focusing\non learning types like unsupervised, semi-supervised,\nactive, and reinforcement to those non-supervised\nlearning problems; it is logical to modify or extend\nvarious multi-task methodologies. It is believed that\nsuch adaption and extension will necessitate greater\neffort in developing relevant models.\nIX. CONCLUSION\nThe majority of the issues that MTL faces today are the same\nchallenges it has encountered for the past two decades. The\nSLR aims to investigate contemporary DL-based language\nmodels for multi-tasking NLU to ﬁnd areas where progress\ncan be made. While it is still usually accepted that task\nrelatedness leads to good bias, Caruana [112] demonstrated\nthat certain inductive biases can be harmful, and while there\nis no strong universal notion of measuring it. The underlying\ndifﬁculty of task interference, in which MTL is hampered\nby a plethora of complex and competing goals. Deeper and\nmore general strategies for task selection and assessment\nare still needed. As more study into the consequences of\nMTL is conducted, it is critical to continue to improve the\nunderstanding of task connection and selection.\nThe SLR examines and analyses DL-based language\nmodels for multi-tasking NLU research by:\n• Identifying the problems with existing language models\nfor multi-tasking NLU\n• Recognizing the necessity to combine supervised and\nunsupervised learning paradigms\n• Exploring the need to combine transformer-based rep-\nresentation with word embeddings presentation for the\ntext.\n• Identifying model compression strategies that can be\nused to lower the size of the dataset.\n• This area of study for various application areas,\nincluding conversational AI, chatbot-based systems for\neducation, legal, stock market, and customer service,\nto name a few, is getting investigated.\nThe ﬁndings suggest that the hybrid model, which\ncombines diverse strategies such as MTL and active learning,\nis given more consideration because of its effectiveness\nin managing text-related tasks for NLU. It is also noticed\nthat the merging of supervised and unsupervised paradigms\nreceives less attention. As a result, building a multi-task\nNLU combination model is a promising prospect. There’s\nstill scope to ﬁgure out how to make MTL models that are\nboth resilient and capable of enabling the next generation of\ngeneral AI.\nREFERENCES\n[1] Kommandotech. (2021). Astounding Artiﬁcial Intelligence\nStatistics for 2020 . Accessed: Oct. 22, 2021. [Online]. Available:\nhttps://kommandotech.com/statistics/artiﬁcial-intelligence-statistics/\n[2] Botcore. (2018). Chatbots: The Past, Present, and Future. Accessed:\nOct. 22, 2021. [Online]. Available: https://botcore.ai/blog/chatbots-the-\npast-present-and-future\n[3] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and\nR. Harshman, ‘‘Indexing by latent semantic analysis,’’ J. Amer. Soc. Inf.\nSci., vol. 41, no. 6, pp. 391–407, 1990.\n[4] Y . Bengio, R. Ducharme, P. Vincent, and C. Jauvin, ‘‘A neural proba-\nbilistic language model,’’ J. Mach. Learn. Res., vol. 3, pp. 1137–1155,\nNov. 1985.\n[5] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, ‘‘Dis-\ntributed representations of words and phrases and their compositionality,’’\nin Proc. Adv. Neural Inf. Process. Syst., 2013, pp. 3111–3119.\n[6] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee,\nand L. Zettlemoyer, ‘‘Deep contextualized word representations,’’ 2018,\narXiv:1802.05365.\n[7] A. Vaswani, N. Shazeer, and N. Parmar, ‘‘Attention is all you need,’’ in\nProc. Adv. Neural Inf. Process. Syst., 2017, pp. 5998–6008.\n[8] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever.\nImproving Language Understanding by Generative Pre-Training.\nAccessed: Oct. 22, 2021. [Online]. Available: https://s3-us-west-2.\namazonaws.com/openai-assets/researchcovers/languageunsupervised/\nlanguageunderstandingpaper.pdf\n[9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ 2018,\narXiv:1810.04805.\n[10] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan,\nand P. Dhariwal, ‘‘Language models are few-shot learners,’’ 2020,\narXiv:2005.14165.\n[11] D. Lepikhin, H. Lee, Y . Xu, D. Chen, O. Firat, Y . Huang, M. Krikun,\nN. Shazeer, and Z. Chen, ‘‘GShard: Scaling giant models with conditional\ncomputation and automatic sharding,’’ 2020, arXiv:2006.16668.\n17094 VOLUME 10, 2022\nR. M. Samantet al.: Framework for DL-Based Language Models Using MTL in Natural Language Understanding\n[12] Developer. (2021). Announcing Megatron or Training Trillion Param-\neter Models Rivavailability. Accessed: Oct. 22, 2021. [Online].\nAvailable: https://developer.nvidia.com/blog/announcing-megatron-for-\ntraining-trillion-parameter-models-riva-availability/\n[13] G. Marcus, ‘‘The next decade in AI: Four steps towards robust artiﬁcial\nintelligence,’’ 2020, arXiv:2002.06177.\n[14] Y . Nie, A. Williams, E. Dinan, M. Bansal, J. Weston, and D. Kiela,\n‘‘Adversarial NLI: A new benchmark for natural language understand-\ning,’’ 2019, arXiv:1910.14599.\n[15] E. S. Dos Reis, C. A. Da Costa, D. E. Da Silveira, and R. S. Bavaresco,\n‘‘Transformer aftermath review,’’ Commun. ACM, vol. 64, no. 4,\npp. 154–163, Apr. 2021.\n[16] Y . Zhang and Q. Yang, An Overview of Multi-Task Learning, vol. 5.\nLondon, U.K.: Oxford Univ. Press, 2018, pp. 30–43.\n[17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, and\nA. N. Gomez, ‘‘Attention is all you need,’’ in Proc. Adv. Neural Inf.\nProcess. Syst., Dec. 2017, pp. 5999–6009.\n[18] B. Kitchenham and S. Charters, ‘‘Guidelines for performing systematic\nliterature reviews in SE,’’Guidel. Perform. Syst. Lit. Rev., vol. 3, pp. 1–44,\nOct. 2007.\n[19] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, ‘‘SQuAD: 100,000+\nquestions for machine comprehension of text,’’ 2016, arXiv:1606.05250.\n[20] M. Marelli, L. Bentivogli, M. Baroni, R. Bernardi, S. Menini, and\nR. Zamparelli, ‘‘SemEval-2014 task 1: Evaluation of compositional\ndistributional semantic models on full sentences through semantic\nrelatedness and textual entailment,’’ in Proc. 8th Int. Workshop Semantic\nEval. (SemEval), 2014, pp. 1–8.\n[21] T. Mikolov, K. Chen, G. Corrado, and J. Dean, ‘‘Efﬁcient estimation of\nword representations in vector space,’’ 2013, arXiv:1301.3781.\n[22] J. Pennington, R. Socher, and C. Manning, ‘‘GloVE: Global vectors for\nword representation,’’ in Proc. Conf. Empirical Methods Natural Lang.\nProcess. (EMNLP), vol. 2014, pp. 1532–1543.\n[23] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. Jégou, and\nT. Mikolov, ‘‘FastText.Zip: Compressing text classiﬁcation models,’’\n2016, arXiv:1612.03651.\n[24] S. Wang and C. D. Manning, ‘‘Baselines and bigrams: Simple, good\nsentiment and topic classiﬁcation,’’ in Proc. 50th Annu. Meeting Assoc.\nComput. Linguistics, vol. 2. 2012, pp. 90–94.\n[25] Q. Le and T. Mikolov, ‘‘Distributed representations of sentences and\ndocuments,’’ in Proc. Int. Conf. Mach. Learn., 2014, pp. 1188–1196.\n[26] K. Sheng Tai, R. Socher, and C. D. Manning, ‘‘Improved semantic\nrepresentations from tree-structured long short-term memory networks,’’\n2015, arXiv:1503.00075.\n[27] X. Zhu, P. Sobihani, and H. Guo, ‘‘Long short-term memory over recur-\nsive structures,’’ in Proc. Int. Conf. Mach. Learn., 2015, pp. 1604–1612.\n[28] P. Liu, X. Qiu, X. Chen, S. Wu, and X.-J. Huang, ‘‘Multi-timescale\nlong short-term memory neural network for modelling sentences and\ndocuments,’’ in Proc. Conf. Empirical Methods Natural Lang. Process.,\n2015, pp. 2326–2335.\n[29] P. Liu, X. Qiu, and X. Huang, ‘‘Recurrent neural network for text\nclassiﬁcation with multi-task learning,’’ 2016, arXiv:1605.05101.\n[30] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner, ‘‘Gradient-based\nlearning applied to document recognition,’’ Proc. IEEE, vol. 86, no. 11,\npp. 2278–2324, Nov. 1998.\n[31] N. Kalchbrenner, E. Grefenstette, and P. Blunsom, ‘‘A convolutional\nneural network for modelling sentences,’’ in Proc. 52nd Annu. Meeting\nAssoc. Comput. Linguistics, 2014, pp. 1–11.\n[32] Y . Kim, ‘‘Convolutional neural networks for sentence classiﬁcation,’’ in\nProc. Conf. Empirical Methods Natural Lang. Process., 2014, pp. 1–62.\n[33] J. Liu, W.-C. Chang, Y . Wu, and Y . Yang, ‘‘Deep learning for extreme\nmulti-label text classiﬁcation,’’ in Proc. 40th Int. ACM SIGIR Conf. Res.\nDevelop. Inf. Retr., Aug. 2017, pp. 115–124.\n[34] J. D. Prusa and T. M. Khoshgoftaar, ‘‘Designing a better data\nrepresentation for deep neural networks and text classiﬁcation,’’ in Proc.\nIEEE 17th Int. Conf. Inf. Reuse Integr. (IRI), Jul. 2016, pp. 411–416.\n[35] A. Conneau, H. Schwenk, L. Barrault, and Y . Lecun, ‘‘Very deep\nconvolutional networks for text classiﬁcation,’’ 2016, arXiv:1606.01781.\n[36] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for\nimage recognition,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2016, pp. 770–778.\n[37] A. B. Duque, L. L. J. Santos, D. Macédo, and C. Zanchettin, ‘‘Squeezed\nvery deep convolutional neural networks for text classiﬁcation,’’ in\nArtiﬁcial Neural Networks and Machine Learning (Lecture Notes\nin Computer Science). Munich, Germany: European Neural Network\nSociety, 2019.\n[38] B. Guo, C. Zhang, J. Liu, and X. Ma, ‘‘Improving text classiﬁcation\nwith weighted word embeddings via a multi-channel TextCNN model,’’\nNeurocomputing, vol. 363, pp. 366–374, Oct. 2019.\n[39] Y . Zhang and B. Wallace, ‘‘A sensitivity analysis of (and practitioners’\nguide to) convolutional neural networks for sentence classiﬁcation,’’\n2015, arXiv:1510.03820.\n[40] G. E. Hinton, A. Krizhevsky, and S. D. Wang, ‘‘Transforming auto-\nencoders,’’ in Proc. Int. Conf. Artif. Neural Netw. Espoo, Finland:\nSpringer, 2011, pp. 44–51.\n[41] S. Sabour, N. Frosst, and G. E. Hinton, ‘‘Dynamic routing between\ncapsules,’’ in Proc. Adv. Neural Inf. Process. Syst., 2017, pp. 3856–3866.\n[42] S. Sabour, N. Frosst, and G. Hinton, ‘‘Matrix capsules with em routing,’’\nin Proc. 6th Int. Conf. Learn. Represent., 2018, pp. 1–15.\n[43] J. Kim, S. Jang, and S. Choi, ‘‘Text classiﬁcation using capsules,’’\nNeurocomputing, vol. 376, no. 1, pp. 214–221, Feb. 2018.\n[44] H. Ren and H. Lu, ‘‘Compositional coding capsule network with K-means\nrouting for text classiﬁcation,’’ 2018, arXiv:1810.09177.\n[45] D. Bahdanau, K. Cho, and Y . Bengio, ‘‘Neural machine translation by\njointly learning to align and translate,’’ 2014, arXiv:1409.0473.\n[46] Z. Yang, D. Yang, C. Dyer, X. He, A. J. Smola, and E. H. Hovy,\n‘‘Hierarchical attention networks for document classiﬁcation,’’ in Proc.\nNAACL, 2016, pp. 1480–1489.\n[47] X. Zhou, X. Wan, and J. Xiao, ‘‘Attention-based LSTM network for\ncross-lingual sentiment classiﬁcation,’’ in Proc. Conf. Empirical Methods\nNatural Lang. Process., 2016, pp. 247–256.\n[48] T. Shen, T. Zhou, G. Long, J. Jiang, S. Pan, and C. Zhang, ‘‘DiSAN:\nDirectional self-attention network for RNN/CNN-free language under-\nstanding,’’ in Proc. 32nd AAAI Conf. Artif. Intell., 2018, pp. 5446–5455.\n[49] Y . Liu, C. Sun, L. Lin, and X. Wang, ‘‘Learning natural language\ninference using bidirectional LSTM model and inner-attention,’’ 2016,\narXiv:1605.09090.\n[50] R. Mihalcea and P. Tarau, ‘‘TextRank: Bringing order into text,’’ in Proc.\nConf. Empirical Methods Natural Lang. Process., 2004, pp. 404–411.\n[51] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and P. S. Yu, ‘‘A comprehen-\nsive survey on graph neural networks,’’ 2019, arXiv:1901.00596.\n[52] T. N. Kipf and M. Welling, ‘‘Semi-supervised classiﬁcation with graph\nconvolutional networks,’’ 2016, arXiv:1609.02907.\n[53] G. Cucurull, A. Casanova, A. Romero, P. Liü, and Y . Bengio, ‘‘Graph\nattention networks,’’ 2017, arXiv:1710.10903.\n[54] L. Yao, C. Mao, and Y . Luo, ‘‘Graph convolutional networks for\ntext classiﬁcation,’’ in Proc. AAAI Conf. Artif. Intell., vol. 33, 2019,\npp. 7370–7377.\n[55] C. Zhou, C. Sun, Z. Liu, and F. C. M. Lau, ‘‘A C-LSTM neural network\nfor text classiﬁcation,’’ 2015, arXiv:1511.08630.\n[56] R. Zhang, H. Lee, and D. R. Radev, ‘‘Dependency sensitive convolutional\nneural networks for modeling sentences and documents,’’ in Proc. Conf.\nNorth Amer. Chapter Assoc. Comput. Linguistics, Hum. Lang. Technol.,\n2016, pp. 1–11.\n[57] G. Chen, D. Ye, E. Cambria, J. Chen, and Z. Xing, ‘‘Ensemble application\nof convolutional and recurrent neural networks for multi-label text\ncategorization,’’ in Proc. IJCNN, 2017, pp. 2377–2383.\n[58] D. Tang, B. Qin, and T. Liu, ‘‘Document modeling with gated recurrent\nneural network for sentiment classiﬁcation,’’ in Proc. Conf. Empirical\nMethods Natural Lang. Process., 2015, pp. 1422–1432.\n[59] Y . Xiao and K. Cho, ‘‘Efﬁcient character-level document clas-\nsiﬁcation by combining convolution and recurrent layers,’’ 2016,\narXiv:1602.00367.\n[60] R. Wang, Z. Li, J. Cao, T. Chen, and L. Wang, ‘‘Convolutional recurrent\nneural networks for text classiﬁcation,’’ in Proc. Int. Joint Conf. Neural\nNetw. (IJCNN), Jul. 2019, pp. 1–6.\n[61] X. Liu, Y . Shen, K. Duh, and J. Gao, ‘‘Stochastic answer networks for\nmachine reading comprehension,’’ 2017, arXiv:1712.03556.\n[62] R. Srivastava, K. Greff, and J. Schmidhuber, ‘‘Training very deep\nnetworks,’’ in Proc. Adv. Neural Inf. Process. Syst., 2015, pp. 1–9.\n[63] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever,\n‘‘Language models are unsupervised multi-task learners,’’ OpenAI Blog,\nvol. 1, no. 8, p. 9, 2019.\n[64] X. Qiu, T. Sun, Y . Xu, Y . Shao, N. Dai, and X. Huang, ‘‘Pre-trained models\nfor natural language processing: A survey,’’ 2020, arXiv:2003.08271.\n[65] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, ‘‘RoBERTa: A robustly optimized\nBERT pretraining approach,’’ 2019, arXiv:1907.11692.\n[66] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut,\n‘‘ALBERT: A lite BERT for self-supervised learning of language\nrepresentations,’’ 2019, arXiv:1909.11942.\nVOLUME 10, 2022 17095\nR. M. Samantet al.: Framework for DL-Based Language Models Using MTL in Natural Language Understanding\n[67] V . Sanh, L. Debut, J. Chaumond, and T. Wolf, ‘‘DistilBERT, a\ndistilled version of BERT: Smaller, faster, cheaper and lighter,’’ 2019,\narXiv:1910.01108.\n[68] M. Joshi, D. Chen, Y . Liu, D. S. Weld, L. Zettlemoyer, and O. Levy,\n‘‘SpanBERT: Improving pre-training by representing and predicting\nspans,’’ 2019, arXiv:1907.10529.\n[69] Y . Sun, S. Wang, Y . Li, S. Feng, X. Chen, H. Zhang, X. Tian,\nD. Zhu, H. Tian, and H. Wu, ‘‘ERNIE: Enhanced representation through\nknowledge integration,’’ 2019, arXiv:1904.09223.\n[70] Z. Yang, Z. Dai, Y . Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V . Le,\n‘‘XLNet: Generalized autoregressive pre-training for language under-\nstanding,’’ in Proc. Adv. Neural Inf. Process. Syst., 2019, pp. 5754–5764.\n[71] L. Dong, N. Yang, W. Wang, F. Wei, X. Liu, Y . Wang, J. Gao, M.\nZhou, and H.-W. Hon, ‘‘Uniﬁed language model pre-training for natural\nlanguage understanding and generation,’’ in Proc. Adv. Neural Inf.\nProcess. Syst., 2019, pp. 13042–13054.\n[72] B. McCann, N. S. Keskar, C. Xiong, and R. Socher, ‘‘The natural\nlanguage decathlon: Multitask learning as question answering,’’ 2018,\narXiv:1806.08730.\n[73] S. Son, S. Hwang, S. Bae, S. J. Park, and J.-H. Choi, ‘‘A sequential and\nintensive weighted language modeling scheme for multi-task learning-\nbased natural language understanding,’’ Appl. Sci., vol. 11, no. 7, p. 3095,\nMar. 2021.\n[74] M. Mcshane, ‘‘Natural language understanding (NLU, not NLP) in\ncognitive systems,’’ AI Mag., vol. 38, no. 4, pp. 43–56, 2017.\n[75] S. Jung, ‘‘Semantic vector learning for natural language understanding,’’\nComput. Speech Lang., vol. 56, pp. 130–145, Jul. 2019.\n[76] T. Yu, ‘‘Gradient surgery for multi-task learning,’’ in Proc. 34th Conf.\nNeural Inf. Process. Syst., Vancouver, BC, Canada, 2020, pp. 5824–5836.\n[77] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,\n‘‘GLUE: A multi-task benchmark and analysis platform for natural\nlanguage understanding,’’ 2018, arXiv:1804.07461.\n[78] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman,\n‘‘GLUE: A multi-task benchmark and analysis platform for natural\nlanguage understanding,’’ 2018, arXiv:1804.07461.\n[79] A. Wang, ‘‘SuperGLUE: A stickier benchmark for general-purpose\nlanguage understanding systems,’’ in Proc. 33rd Conf. Neural Inf.\nProcess. Syst., Vancouver, BC, Canada, 2019, pp. 1–15.\n[80] K. Narasimhan and M. Hausknecht, ‘‘Reading and acting while\nblindfolded: The need for semantics in text game agents,’’ 2021,\narXiv:2103.13552.\n[81] A. Zweig and D. Weinshall, ‘‘Hierarchical regularization cascade for\njoint learning,’’ in Proc. 30th Int. Conf. Mach. Learn., vol. 2, 2013,\npp. 1074–1082.\n[82] G. Chen and M. Huang, ‘‘Out-of-domain detection for natural language\nunderstanding in dialog systems,’’ IEEE/ACM Trans. Audio Speech Lang.\nProcess., vol. 28, pp. 1198–1209, 2020.\n[83] B. Dhingra, ‘‘Gated-attention readers for text comprehension,’’ in Proc.\n55th Annu. Meeting ACL, Vancouver, BC, Canada, 2017, pp. 1832–1846.\n[84] J. Howard, ‘‘Universal language model ﬁne-tuning for text classiﬁca-\ntion,’’ in Proc. 56th Annu. Meeting ACL, Melbourne, VIC, Australia,\n2018, pp. 328–339.\n[85] D. Chen, ‘‘A through examination of the CNN/daily mail reading\ncomprehension task,’’ in Proc. 54th Annu. Meeting ACL, Berlin,\nGermany, 2018, pp. 2358–2367.\n[86] W. Che, Y . Liu, Y . Wang, B. Zheng, and T. Liu, ‘‘Towards better UD\nparsing,’’ in Proc. CoNLL, 2018, pp. 55–64.\n[87] D. Kiselev, ‘‘An AI using construction grammar to understand text:\nParsing improvements,’’ Int. J. Cogn. Inform. Natural Intell., vol. 15,\nno. 2, pp. 47–61, Apr. 2021.\n[88] S. Singh and A. Mahmood, ‘‘The NLP cookbook: Modern recipes for\ntransformer based deep learning architectures,’’ IEEE Access, vol. 9,\npp. 68675–68702, 2021.\n[89] C. Montenegro, A. L. Zorrilla, J. M. Olaso, R. Santana, R. Justo,\nJ. A. Lozano, and M. I. Torres, ‘‘A dialogue-act taxonomy for a virtual\ncoach designed to improve the life of elderly,’’ Multimodal Technol.\nInteract., vol. 3, no. 3, p. 52, Jul. 2019.\n[90] T. Mairittha, N. Mairittha, and S. Inoue, ‘‘Automatic labelled dialogue\ngeneration for nursing record systems,’’ J. Pers. Med., vol. 10, no. 3,\npp. 1–24, Sep. 2020.\n[91] D. Braun, ‘‘Evaluating natural language understanding services for\nconversational question answering systems,’’ in Proc. SIGDIAL Conf.,\nSaarbrucken, Germany, 2017, pp. 174–185.\n[92] M. Zubani, L. Sigalini, I. Serina, and A. E. Gerevini, ‘‘Evaluating different\nnatural language understanding services in a real business case for the\nItalian language,’’Proc. Comput. Sci., vol. 176, pp. 995–1004, Oct. 2020.\n[93] A. Jiao, ‘‘An intelligent chatbot system based on entity extraction\nusing RASA NLU and neural network,’’ J. Phys. Conf. Ser.,\nvol. 1487, Mar. 2020, Art. no. 012014, doi: 10.1088/1742-\n6596/1487/1/0120142020.\n[94] T. Mayer, ‘‘Enriching language models with semantics,’’ in Proc. 24th\nEur. Conf. Artif. Intell., Santiago, Spain, Aug. 2020, pp. 1–3.\n[95] G. B. Jenset and B. McGillivray, ‘‘Enhancing domain-speciﬁc supervised\nnatural language intent classiﬁcation with a top-down selective ensemble\nmodel,’’ Mach. Learn. Knowl. Extraction, vol. 1, no. 2, pp. 630–640,\nApr. 2019.\n[96] P. Lauren, ‘‘A conversational user interface for stock analysis,’’ in Proc.\nInt. Conf. Big Data, 2019, pp. 5298–5305.\n[97] V . Shymko, ‘‘Natural language understanding: Methodological concep-\ntualization,’’Psycholinguistics, vol. 25, no. 1, pp. 431–443, Apr. 2019.\n[98] E. Tomal and K. D. Forbus, ‘‘EA NLU: Practical language understanding\nfor cognitive modelling,’’ in Proc. 22nd Int. Florida Artif. Intell. Res. Soc.\nConf., vol. 22, 2009, pp. 117–122.\n[99] Z. Li and D. Hoiem, ‘‘Learning without forgetting,’’ 2016,\narXiv:1606.09282.\n[100] J.-L. Du, ‘‘Predicting garden path sentences based on natural language\nunderstanding system,’’ Int. J. Adv. Comput. Sci. Appl., vol. 3, no. 11,\n2012, doi: 10.14569/IJACSA.2012.031101.\n[101] D. Weissenborn, T. Koàiský, and C. Dyer, ‘‘Dynamic integration of back-\nground knowledge in neural NLU systems,’’ 2017, arXiv:1706.02596.\n[102] S. Peng and A. Zeldes, ‘‘All roads lead to UD: Converting stanford\nand penn parses to english universal dependencies with multi-layer\nannotations,’’ in Proc. Joint Workshop Linguistic Annotation, Multiword\nExpressions, Construct. Workshop, 2018, pp. 167–177.\n[103] G. Senay, B. Y . Idrissi, and M. Haziza, ‘‘VirAAL: Virtual adversarial\nactive learning for NLU,’’ Tech. Rep., May 2020. [Online]. Available:\nhttps://arxiv.org/abs/2005.07287V2\n[104] B. Wu, B. Wei, J. Liu, K. Wu, and M. Wang, ‘‘Faceted text segmentation\nvia multitask learning,’’ IEEE Trans. Neural Netw. Learn. Syst. , vol. 32,\nno. 9, pp. 3846–3857, Sep. 2021, doi: 10.1109/TNNLS.2020.3015996.\n[105] M. Dimovski, C. Musat, V . Ilievski, A. Hossmann, and M. Baeriswyl,\n‘‘Submodularity-inspired data selection for goal-oriented chatbot training\nbased on sentence embeddings,’’ 2018, arXiv:1802.00757.\n[106] A. Tiwari, T. Saha, S. Saha, S. Sengupta, A. Maitra, R. Ramnani,\nand P. Bhattacharyya, ‘‘A dynamic goal adapted task-oriented dialogue\nagent,’’PLoS ONE, vol. 16, no. 4, Apr. 2021, Art. no. e0249030.\n[107] E. Merdivan, D. Singh, S. Hanke, J. Kropf, A. Holzinger, and M. Geist,\n‘‘Human annotated dialogues dataset for natural conversational agents,’’\nAppl. Sci., vol. 10, no. 3, p. 762, Jan. 2020.\n[108] S. Zhu, R. Cao, and K. Yu, ‘‘Dual learning for semi-supervised\nnatural language understanding,’’ IEEE/ACM Trans. Audio Speech Lang.\nProcess., vol. 28, pp. 1936–1947, 2020.\n[109] B. Shi, ‘‘Auxiliary task reweighting for minimum-data learning,’’ in Proc.\n34th Conf. Neural Inf. Process. Syst., Vancouver, BC, Canada, 2020,\npp. 7148–7160.\n[110] Y . Ji, C. Tan, S. Martschat, Y . Choi, and N. A. Smith, ‘‘Dynamic entity\nrepresentations in neural language models,’’ 2017, arXiv:1708.00781.\n[111] L. Liu, X. Ren, J. Shang, X. Gu, J. Peng, and J. Han, ‘‘Efﬁcient\ncontextualized representation: Language model pruning for sequence\nlabelling,’’ in Proc. Conf. Empirical Methods Natural Lang. Process.,\n2018, pp. 1215–1225.\n[112] R. Caruana, ‘‘Multitask learning,’’ Mach. Learn. J., vol. 28, no. 1,\npp. 41–75, 1997.\nRAHUL MANOHAR SAMANT received the\nmaster’s degree in information technology from\nMumbai University. He is currently pursuing\nthe Ph.D. degree with the Symbiosis Institute\nof Technology, Symbiosis International (Deemed\nUniversity), Pune. His research interests include\nconversational AI, deep learning, and language\nmodeling.\n17096 VOLUME 10, 2022\nR. M. Samantet al.: Framework for DL-Based Language Models Using MTL in Natural Language Understanding\nMRINAL R. BACHUTEreceived the M.E. degree\nin digital electronics and the Ph.D. degree in elec-\ntronics. She is currently an Associate Professor\nand an Industry Liaison Ofﬁcer at the Department\nof Electronics and Telecommunication Engineer-\ning, Symbiosis Institute of Technology, Pune Sym-\nbiosis International (Deemed University), Pune,\nMaharashtra, India. She has teaching experience of\n20 years. She has received research funding from\nthe University of Pune and AICTE QIP Grants.\nHer research interests include digital image processing, machine learning,\nartiﬁcial intelligence, and adaptive signal processing. She has delivered\ninvited talks and expert sessions at the various national and international\nlevels, including at Langara University, Vancouver, Canada, organized by\nIET Canada at ZE Power Engineering, Vancouver, and IET Trinidad and\nTobago. She has worked as a reviewer for conferences and reputed journals,\nlike Springer, Nature, and Elsevier.\nSHILPA GITE received the Ph.D. degree in\nusing deep learning for assistive driving in semi-\nautonomous vehicles from Symbiosis Interna-\ntional (Deemed University), Pune, India, in 2019.\nShe is currently working as an Associate Professor\nwith the Computer Science Department, Sym-\nbiosis Institute of Technology, Pune. She is also\nworking as an Associate Faculty at the Symbiosis\nCentre of Applied AI (SCAAI). She has around\n13 years of teaching experience. She has published\nmore than 60 research papers in international journals and 25 Scopus indexed\ninternational conferences. Her research interests include deep learning,\nmachine learning, medical imaging, and computer vision. She was a recipient\nof the Best Paper Award at the 11th IEMERA Conference held virtually at\nImperial College London, London, in October 2020.\nKETAN KOTECHA received the M.Tech. and\nPh.D. degrees from IIT Bombay.\nHe is currently the Head of the Symbiosis\nCentre for Applied AI (SCAAI), the Director of\nthe Symbiosis Institute of Technology, a CEO of\nthe Symbiosis Centre for Entrepreneurship and\nInnovation (SCEI), and the Dean of the Faculty\nof Engineering, Symbiosis International (Deemed\nUniversity). He has expertise and experience in\ncutting-edge research and AI and deep learning\nprojects for the last (more than) 25 years. He has published more\nthan 100 articles widely in several excellent peer-reviewed journals on\ntopics ranging from cutting-edge AI, education policies, teaching-learning\npractices, and AI for all. He was a recipient of the two SPARC projects worth\nINR 166 lakhs from MHRD Government of India in AI in collaboration\nwith Arizona State University, USA, and The University of Queensland,\nAustralia.\nDr. Kotecha was a recipient of numerous prestigious awards, like the\nErasmus+Faculty Mobility Grant to Poland, the DUO-India Professors\nFellowship for research in responsible AI in collaboration with Brunel\nUniversity, U.K., the LEAP Grant at Cambridge University, U.K., the\nUKIERI Grant with Aston University, U.K., and a Grant from the Royal\nAcademy of Engineering, U.K., under the Newton Bhabha Fund. He has\npublished three patents and delivered keynote speeches at various national\nand international forums, including at the Machine Intelligence Laboratory,\nUSA, IIT Bombay under the World Bank Project, the International Indian\nScience Festival organized by the Department of Science Technology,\nGovernment of India, and many more. He is an Academic Editor of the Peerj\nComputer Science journal and an Associate Editor of IEEE A CCESS journal.\nVOLUME 10, 2022 17097",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8475067615509033
    },
    {
      "name": "Natural language understanding",
      "score": 0.7463058233261108
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7088547945022583
    },
    {
      "name": "Natural language processing",
      "score": 0.6531936526298523
    },
    {
      "name": "Task (project management)",
      "score": 0.589806318283081
    },
    {
      "name": "Multi-task learning",
      "score": 0.5510462522506714
    },
    {
      "name": "Field (mathematics)",
      "score": 0.5260947942733765
    },
    {
      "name": "Sentiment analysis",
      "score": 0.5259578227996826
    },
    {
      "name": "Inference",
      "score": 0.505872905254364
    },
    {
      "name": "Natural language",
      "score": 0.46095114946365356
    },
    {
      "name": "Language model",
      "score": 0.4226093590259552
    },
    {
      "name": "Machine translation",
      "score": 0.42227989435195923
    },
    {
      "name": "Machine learning",
      "score": 0.3569202423095703
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I244572783",
      "name": "Symbiosis International University",
      "country": "IN"
    }
  ],
  "cited_by": 105
}