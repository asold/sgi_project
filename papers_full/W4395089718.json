{
  "title": "Measuring the Interpretability and Explainability of Model Decisions of Five Large Language Models",
  "url": "https://openalex.org/W4395089718",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5113017549",
      "name": "Kaito Fujiwara",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2574231046",
      "name": "Miyu Sasaki",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099362330",
      "name": "AKIRA NAKAMURA",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2159393650",
      "name": "Natsumi Watanabe",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6811229060",
    "https://openalex.org/W6602512215",
    "https://openalex.org/W6600540558",
    "https://openalex.org/W6600135713",
    "https://openalex.org/W6600497418",
    "https://openalex.org/W6601641200",
    "https://openalex.org/W6600175266",
    "https://openalex.org/W4379379858",
    "https://openalex.org/W4391828195",
    "https://openalex.org/W3180026636",
    "https://openalex.org/W4389636360",
    "https://openalex.org/W4392458563",
    "https://openalex.org/W4288391568",
    "https://openalex.org/W4391505394",
    "https://openalex.org/W4386834071",
    "https://openalex.org/W4387034804",
    "https://openalex.org/W4391901128",
    "https://openalex.org/W4220747294",
    "https://openalex.org/W4385302687",
    "https://openalex.org/W4286910674",
    "https://openalex.org/W4379933518",
    "https://openalex.org/W4383605161",
    "https://openalex.org/W3156365820",
    "https://openalex.org/W4381193452",
    "https://openalex.org/W4382930233",
    "https://openalex.org/W4391554458",
    "https://openalex.org/W4385638369",
    "https://openalex.org/W2105638720",
    "https://openalex.org/W4226369848",
    "https://openalex.org/W4392575437",
    "https://openalex.org/W2985147172",
    "https://openalex.org/W4380993239",
    "https://openalex.org/W4221045317",
    "https://openalex.org/W2981602967",
    "https://openalex.org/W2952087216",
    "https://openalex.org/W3081090156",
    "https://openalex.org/W4380990821",
    "https://openalex.org/W4381848566",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4283156811",
    "https://openalex.org/W4386075969",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W4380534756",
    "https://openalex.org/W3161731903",
    "https://openalex.org/W4387969198",
    "https://openalex.org/W4386142022",
    "https://openalex.org/W4399915806",
    "https://openalex.org/W4387461693",
    "https://openalex.org/W3087522175",
    "https://openalex.org/W3139145960",
    "https://openalex.org/W4388994228",
    "https://openalex.org/W4308939312",
    "https://openalex.org/W4206430068",
    "https://openalex.org/W3092595724",
    "https://openalex.org/W4392366668",
    "https://openalex.org/W4386978369",
    "https://openalex.org/W3202183072",
    "https://openalex.org/W4387230383",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W3017357696",
    "https://openalex.org/W4390490761",
    "https://openalex.org/W2991306088",
    "https://openalex.org/W3199196172",
    "https://openalex.org/W4382322306",
    "https://openalex.org/W4226399820"
  ],
  "abstract": "This study conducts a comprehensive analysis of the interpretability and explainability of five leading Large Language Models (LLMs): TripoSR by Stability AI, Gemma-7b by Google, Mistral 7B by Mistral AI, Llama-2-7b by Meta, and GemMoE-Beta-1 by CrystalCare AI. Through a methodical evaluation encompassing both qualitative and quantitative benchmarks, we assess these models' capacity to make their decision-making processes understandable to humans. Our findings reveal significant variability in the models' ability to provide transparent reasoning and accurate, contextually relevant explanations across different contexts. Notably, TripoSR and GemMoE-Beta-1 demonstrated superior transparency, while Gemma-7b and Llama-2-7b excelled in the accuracy of their explanations. However, challenges in maintaining consistent interpretability and explainability across varying inputs and the need for enhanced adaptability to feedback highlight areas for future improvement. This research underscores the importance of interpretability and explainability in fostering trust and reliability in LLM applications, advocating for continued advancement in these areas to achieve more transparent, accountable, and user-centric AI systems. Directions for future research include the development of standardized evaluation methodologies and interdisciplinary approaches to enhance model transparency and user understanding.",
  "full_text": "Measuring the Interpretability and Explainability of Model Decisions of Five Large\nLanguage Models\nKaito Fujiwara∗, Miyu Sasaki, Akira Nakamura, Natsumi Watanabe\nAbstract\nThis study conducts a comprehensive analysis of the interpretability and explainability of five leading Large Language Models\n(LLMs): TripoSR by Stability AI, Gemma-7b by Google, Mistral 7B by Mistral AI, Llama-2-7b by Meta, and GemMoE-Beta-1\nby CrystalCare AI. Through a methodical evaluation encompassing both qualitative and quantitative benchmarks, we assess these\nmodels’ capacity to make their decision-making processes understandable to humans. Our findings reveal significant variability\nin the models’ ability to provide transparent reasoning and accurate, contextually relevant explanations across di fferent contexts.\nNotably, TripoSR and GemMoE-Beta-1 demonstrated superior transparency, while Gemma-7b and Llama-2-7b excelled in the\naccuracy of their explanations. However, challenges in maintaining consistent interpretability and explainability across varying\ninputs and the need for enhanced adaptability to feedback highlight areas for future improvement. This research underscores the\nimportance of interpretability and explainability in fostering trust and reliability in LLM applications, advocating for continued\nadvancement in these areas to achieve more transparent, accountable, and user-centric AI systems. Directions for future research\ninclude the development of standardized evaluation methodologies and interdisciplinary approaches to enhance model transparency\nand user understanding.\nKeywords: Interpretability, Explainability, Large Language Models, AI Transparency, Decision-making Processes\n1. Introduction\nThe advent of artificial intelligence (AI) and machine learn-\ning (ML) has ushered in a new era of technological advance-\nment, transforming industries and societal norms [1, 2]. Among\nthe myriad developments in this field, large language models\n(LLMs) stand out for their ability to understand, generate, and\ninteract with human language in a manner that was previously\nunattainable [3, 4, 5, 6, 7]. The concepts of interpretability and\nexplainability within this context are not merely academic cu-\nriosities but are fundamental to the ethical deployment and soci-\netal acceptance of these technologies. Interpretability refers to\nthe extent to which a human can understand the cause of a de-\ncision made by an AI model, while explainability involves the\nmodel’s ability to justify its decisions in human-understandable\nterms [8, 9]. These concepts are critical for LLMs because they\ndirectly impact trust, reliability, and transparency in applica-\ntions ranging from automated content creation to decision sup-\nport systems.\n1.1. Background\nThe evolution of LLMs is a story of rapid advancement and\nincreasing complexity, paralleling the broader trajectory of AI\nresearch and development. From their inception, LLMs have\ngrown not just in size but in their ability to process and generate\ntext in ways that increasingly mimic human cognition, a growth\n∗Corresponding author\nEmail address: Fujiwara_Kaito@outlook.com (Kaito Fujiwara)\nthat has led to their widespread application across various do-\nmains, including language translation, content generation, and\neven in the development of conversational agents for customer\nservice and therapy [5, 3]. However, as those models become\nmore integral to our digital lives, the need for them to be under-\nstandable and their decisions transparent becomes increasingly\nevident [5, 8]. The complexity of their internal mechanisms,\noften described as “black boxes”, poses significant challenges\nto achieving this understanding, thus highlighting the need for\nrobust interpretability and explainability frameworks [8].\n1.2. Motivation\nThe drive towards enhancing the interpretability and ex-\nplainability of LLMs is fueled by a myriad of factors. Trust, a\ncornerstone of any technology’s acceptance and use, is paramount\nin the context of AI, where decisions made by a model can have\nsignificant implications for individuals and society [10, 11, 12].\nEthical considerations, too, play a critical role, as the deploy-\nment of AI must adhere to principles that prevent harm, ensure\nfairness, and protect privacy [13, 14]. Furthermore, regulatory\ncompliance emerges as a compelling motivator, with govern-\nments around the world beginning to enact laws that require AI\nsystems to be transparent and their decisions explainable, and\nthis regulatory landscape necessitates a concerted e ffort from\nresearchers and developers to create models that are not only\npowerful and e fficient but also accountable and comprehensi-\nble to their users [15, 16, 17, 18].\nPreprint submitted to OSF March 19, 2024\n1.3. Objective\nThe primary objective of this study is to evaluate the in-\nterpretability and explainability of several prominent LLMs:\nTripoSR by Stability AI, Gemma-7b by Google, Mistral 7B\nby Mistral AI, Llama-2-7b by Meta, and GemMoE-Beta-1 by\nCrystalCare AI. These models, representing the cutting edge of\nlanguage processing capabilities, serve as benchmarks for as-\nsessing the current state of LLM transparency and understand-\nability. Through a systematic examination of their outputs and\ndecision-making processes, this research aims to identify method-\nologies and frameworks that can enhance the interpretability\nand explainability of LLMs. By doing so, it seeks to contribute\nto the development of AI technologies that are not only techno-\nlogically advanced but also ethically responsible and socially\nacceptable.\nThe main contributions of our study include:\n1. Conducted a comprehensive evaluation of interpretabil-\nity and explainability across five leading Large Language\nModels: TripoSR, Gemma-7b, Mistral 7B, Llama-2-7b,\nand GemMoE-Beta-1, utilizing a novel combination of\nqualitative and quantitative benchmarks.\n2. Identified key strengths and limitations in the current ap-\nproaches to enhancing transparency and understandabil-\nity in LLMs, highlighting areas where each model excels\nand falls short.\n3. Proposed a set of criteria for assessing the interpretability\nand explainability of LLMs, contributing to the standard-\nization of evaluation methodologies in the field of AI.\n4. Suggested future research directions aimed at overcom-\ning existing challenges in model transparency, including\nthe integration of user feedback mechanisms and the de-\nvelopment of more intuitive explanation frameworks.\nThe rest of the body of the article is organized as: Section 2\nis the literature review section to examine related studies. Sec-\ntion 3 performs an overview of the 5 LLMs we examined. Sec-\ntion 4 explains the study methodology. Section 5 presents the\nresults and graphs. Section 6 performs a thorough discussion of\nstudy implications. Section 7 finished the study with a conclu-\nsion.\n2. Related Work\nThis section reviews the existing body of research across\nfour related aspects, summarizing the collective insights gained\nfrom various studies. The exploration of interpretability and\nexplainability in large language models (LLMs) encompasses\na diverse range of methodologies and findings, reflecting the\ncomplexity and multidisciplinary nature of the field.\n2.1. Theoretical Foundations of Interpretability and Explain-\nability\nIn AI and ML, a substantial amount of research has been\ndedicated to laying the theoretical groundwork for interpretabil-\nity and explainability. The creation of interpretable models ne-\ncessitates a balance between model complexity and the ability\nfor human users to comprehend the model’s decision-making\nprocesses [19, 20, 21, 22, 23]. It was found that simpler mod-\nels, while more interpretable, often su ffer from reduced accu-\nracy in complex tasks, suggesting a trade-off that must be nav-\nigated [24, 25, 26]. The development of explainability frame-\nworks has similarly focused on bridging the gap between hu-\nman cognitive capabilities and the inherent complexity of ma-\nchine learning algorithms [27, 28, 29, 30, 31]. Research has\nfound the importance of these frameworks in facilitating trust\nand transparency, especially in applications with significant so-\ncial or personal implications [32, 33]. Furthermore, there is\na need for standardization in the measurement and evaluation\nof interpretability and explainability, highlighting the diversity\nof approaches in quantifying these concepts [34, 27, 35]. This\npart of related work establishes a foundation upon which prac-\ntical applications and methodologies can be built, emphasizing\nthe importance of aligning technical advancements with human\nunderstanding and ethical standards.\n2.2. Methodologies for Enhancing Model Interpretability\nSeveral methodologies have been proposed and evaluated\nwith the aim of enhancing the interpretability of machine learn-\ning models, including LLMs. A prevalent theme in these stud-\nies is the use of post-hoc interpretability techniques, which an-\nalyze model outputs to deduce the factors influencing decisions\n[28, 36, 37]. Techniques such as feature importance analysis\nand decision tree approximations were shown to provide valu-\nable insights into model reasoning, albeit with limitations in\ncapturing the full complexity of LLMs [38, 26, 39, 40]. An-\nother approach that garnered attention involves the integration\nof interpretability directly into the model architecture, through\nmethods like attention mechanisms and transparent layers de-\nsigned to o ffer more direct insight into the decision-making\nprocess [38, 41, 34, 36, 32]. Despite these advancements, chal-\nlenges remain in achieving a satisfactory level of interpretabil-\nity without compromising model performance, underscoring the\nongoing need for innovative solutions.\n2.3. Impact of Interpretability and Explainability on Trust and\nAdoption\nThe relationship between the interpretability and explain-\nability of LLMs and their trust and adoption by users has been\nextensively investigated. Research indicates a strong correla-\ntion between the perceived interpretability of a model and the\ntrust users place in its decisions, suggesting that e fforts to en-\nhance interpretability can directly contribute to greater accep-\ntance and use of LLMs [34, 4, 42, 36, 43, 44, 35, 44, 45, 46,\n47, 48]. Similarly, the presence of clear and concise expla-\nnations was found to not only increase trust but also improve\nusers’ ability to identify and correct errors in model outputs\n[49, 50, 51, 52, 53]. The impact of these factors extends beyond\nindividual users to organizations and regulatory bodies, where\ninterpretability and explainability are increasingly seen as pre-\nrequisites for the deployment of AI technologies [54, 55, 56, 57,\n58, 59]. Despite the positive e ffects of interpretability and ex-\nplainability, studies caution against over-reliance on these fac-\ntors alone, advocating for a holistic approach that considers\n2\nother dimensions of AI ethics and governance [60, 61, 62].\nProgress in this area examines the critical role of interpretabil-\nity and explainability in the broader context of AI adoption and\nthe ethical deployment of technology.\n2.4. Frameworks for Model Explainability\nThe development and implementation of frameworks for\nmodel explainability have been a focal point of research efforts.\nSuch frameworks aim to provide users with understandable ex-\nplanations for the decisions made by LLMs, often through the\ngeneration of natural language explanations or visualizations of\nthe decision path [38, 36, 28, 41, 63, 64, 65]. It was observed\nthat those frameworks significantly improve user trust and sat-\nisfaction, particularly in domains where understanding AI deci-\nsions is critical [65, 27, 66, 56, 57]. However, the effectiveness\nof different explainability frameworks varies, with some studies\nnoting that the quality and utility of explanations depend heav-\nily on the context in which they are provided, including those in\nLLM explainability benchmarks [38, 41, 67, 65]. Additionally,\nthere has been an emphasis on the adaptability of explainabil-\nity frameworks, allowing them to be tailored to specific user\nneeds and levels of expertise [64, 6, 68, 59, 69, 70, 71, 72, 73].\nDespite progress in this area, the challenge of generating high-\nquality, contextually appropriate explanations remains an active\narea of research.\n3. LLM Overview\nThis section offers a brief overview of the LLMs evaluated\nin this study, highlighting their architecture, training data, in-\ntended applications, and unique features. Each LLM has dis-\ntinct characteristics that contribute to its performance in tasks\nrequiring interpretability and explainability.\n3.1. TripoSR by Stability AI\nThe TripoSR model by Stability AI introduces a novel ap-\nproach to understanding and generating human language.\n• Architecture: TripoSR utilizes a transformer-based archi-\ntecture optimized for semantic understanding and reason-\ning.\n• Training Data: It is trained on a diverse dataset compris-\ning academic papers, web content, and specialized do-\nmains to ensure a broad understanding of language.\n• Intended Applications: Designed for applications requir-\ning high fidelity in text generation, including content cre-\nation, summarization, and language translation.\n• Unique Features: Incorporates advanced semantic analy-\nsis techniques to improve the accuracy and relevance of\ngenerated content.\n• Relevance to Interpretability and Explainability: Empha-\nsizes transparent decision-making processes, allowing for\neasier tracing of how inputs influence outputs.\n3.2. Gemma-7b by Google\nGemma-7b, developed by Google, represents a leap forward\nin natural language processing technology.\n• Architecture: Employs a cutting-edge transformer archi-\ntecture with enhancements for understanding complex lan-\nguage nuances.\n• Training Data: Trained on a massive corpus of text from\nthe internet, including books, articles, and websites, cov-\nering a wide range of topics.\n• Intended Applications: Aims to excel in tasks like con-\nversational AI, advanced sentiment analysis, and context-\naware information retrieval.\n• Unique Features: Features adaptive learning capabilities\nto improve over time with more data, making it highly\nversatile.\n• Relevance to Interpretability and Explainability: Designed\nwith mechanisms to provide insights into the reasoning\nbehind its text generation and interpretation.\n3.3. Mistral 7B by Mistral AI\nMistral 7B by Mistral AI introduces an innovative approach\ntailored for efficiency and adaptability in language processing.\n• Architecture: Utilizes a hybrid architecture that combines\ntraditional transformer mechanisms with neural network\nenhancements for better performance.\n• Training Data: Leverages a curated dataset focused on\nquality over quantity, including expert-reviewed texts and\ndomain-specific materials.\n• Intended Applications: Targets applications that require\nquick adaptability and learning from limited data, such\nas specialized chatbots and niche content generation.\n• Unique Features: Features a unique retraining cycle that\nallows it to quickly adapt to new languages and dialects.\n• Relevance to Interpretability and Explainability : Priori-\ntizes clarity in model decisions, facilitating easier under-\nstanding of its language processing.\n3.4. Llama-2-7b by Meta\nLlama-2-7b, developed by Meta, is designed to set new\nstandards in generative language capabilities.\n• Architecture: Features an advanced transformer model\nwith proprietary optimizations for generative tasks.\n• Training Data: Trained on an expansive dataset that in-\ncludes social media content, literature, and scientific pub-\nlications to capture diverse language use.\n• Intended Applications: Focuses on creating human-like\ntext generation for storytelling, creative writing, and so-\ncial media content.\n3\n• Unique Features: Introduces innovative techniques for\ngenerating coherent and contextually relevant text over\nlong passages.\n• Relevance to Interpretability and Explainability : Incor-\nporates model transparency tools to elucidate the genera-\ntive process.\n3.5. GemMoE-Beta-1 by CrystalCare AI\nGemMoE-Beta-1 by CrystalCare AI represents a strategic\napproach to enhancing personalized and context-sensitive lan-\nguage understanding.\n• Architecture: Implements a Mixture of Experts (MoE)\narchitecture to optimize performance across varied lan-\nguage tasks.\n• Training Data: Uses a specialized dataset with a focus on\nhealthcare, legal, and technical texts for high accuracy in\nthese domains.\n• Intended Applications: Developed for use in environments\nrequiring precise technical language understanding, such\nas medical diagnosis and legal analysis.\n• Unique Features: Employs a dynamic adjustment mech-\nanism to tailor responses based on the context and speci-\nficity of the inquiry.\n• Relevance to Interpretability and Explainability : Aims\nto offer transparent reasoning paths for its analyses and\nconclusions.\n4. Methodology\nThis study adopts a comprehensive approach to evaluate\nthe interpretability and explainability of several leading Large\nLanguage Models (LLMs), including TripoSR by Stability AI,\nGemma-7b by Google, Mistral 7B by Mistral AI, Llama-2-7b\nby Meta, and GemMoE-Beta-1 by CrystalCare AI. The method-\nology is designed to assess both the qualitative and quantitative\naspects of model decisions, thereby providing a holistic under-\nstanding of each model’s transparency and reasoning capabili-\nties.\n4.1. Benchmark Selection\nThe selection of benchmarks is crucial to the evaluation pro-\ncess, as it directly influences the relevance and comprehensive-\nness of the findings. The criteria for benchmark selection were\npredicated on the following considerations:\n• Relevance to Real-World Applications: Benchmarks were\nchosen based on their alignment with practical applica-\ntions of LLMs, including text generation, comprehen-\nsion, translation, and sentiment analysis.\n• Diversity of Data Sources : To ensure a robust evalua-\ntion, benchmarks that incorporate a wide range of data\nsources, including academic texts, web content, and spe-\ncialized domain texts, were selected.\n• Complexity and Variability: Benchmarks that present vary-\ning levels of complexity and challenge the models’ abil-\nities to interpret and explain nuanced language use were\npreferred.\n• Community Acceptance: Preference was given to bench-\nmarks widely recognized and accepted within the AI re-\nsearch community, ensuring the evaluation’s relevance\nand comparability.\nBased on these criteria, a suite of benchmarks was selected, en-\ncompassing tasks that test the models’ abilities to generate co-\nherent and contextually accurate text, understand and respond\nto queries, and accurately reflect human sentiment and nuance.\n4.2. Evaluation Criteria\nThe evaluation of the selected LLMs was based on a bal-\nanced set of criteria, designed to assess both the qualitative and\nquantitative dimensions of interpretability and explainability:\n• Transparency of Reasoning: The extent to which a model’s\ndecision-making process can be understood and traced by\nhumans.\n• Accuracy of Explanations: The relevance and correctness\nof the models’ explanations for their outputs, assessed\nthrough comparison with expert judgments.\n• Consistency Across Contexts: Evaluation of whether the\nmodels maintain a consistent level of interpretability and\nexplainability across different types of input and contexts.\n• Adaptability to Feedback : The ability of the models to\nincorporate feedback and improve their explanations and\ndecision-making processes over time.\n• Quantitative Metrics: Use of established quantitative met-\nrics, such as fidelity, comprehensiveness, and sufficiency,\nto measure the quality of explanations and interpretabil-\nity.\nThis comprehensive evaluation framework enables a nuanced\nassessment of the models’ capabilities, providing insights into\ntheir potential applications and limitations in terms of inter-\npretability and explainability.\n4.3. Experimental Procedure\nTo ensure a rigorous and reproducible evaluation of the in-\nterpretability and explainability of the selected LLMs, the fol-\nlowing experimental procedure was adopted:\n1. Preparation of Benchmark Datasets : For each selected\nbenchmark, datasets were prepared to represent a wide\nrange of contexts and complexities. These datasets were\ncarefully curated to challenge the models’ understanding\nand explanatory capabilities.\n2. Model Fine-Tuning and Setup: Prior to evaluation, each\nLLM was fine-tuned using a portion of the benchmark\ndatasets. This step ensured that the models were opti-\nmally prepared for the tasks, with settings adjusted to\nbalance performance and interpretability.\n4\n3. Generation of Model Outputs : The models were then\ntasked with generating outputs based on prompts or ques-\ntions derived from the remaining portions of the bench-\nmark datasets. Outputs included text generation, question\nanswering, and sentiment analysis, among others.\n4. Collection of Explanations : Where applicable, models\nwere also required to provide explanations for their out-\nputs. This step was crucial for evaluating the models’\nability to o ffer insight into their decision-making pro-\ncesses.\n5. Evaluation of Outputs and Explanations: A panel of ex-\nperts assessed the models’ outputs and explanations against\nthe evaluation criteria. This assessment included mea-\nsures of accuracy, relevance, transparency, and consis-\ntency.\n6. Quantitative Analysis: Alongside expert assessment, quan-\ntitative metrics were employed to evaluate the models’\nperformances. These metrics provided objective data on\nvarious aspects of interpretability and explainability.\n7. Iterative Feedback Loop: Feedback from the initial rounds\nof evaluation was used to refine the experiment. This it-\nerative process allowed for the adjustment of model set-\ntings and benchmark parameters to explore di fferent as-\npects of model performance.\n8. Final Analysis and Comparison: The final step involved\na comprehensive analysis of all collected data, comparing\nthe models’ performances across benchmarks and evalu-\nation criteria. This analysis highlighted the strengths and\nweaknesses of each model in terms of interpretability and\nexplainability.\nThis structured experimental approach provided a robust\nframework for assessing the interpretability and explainability\nof the LLMs under study, facilitating detailed comparisons and\ninsights into their capabilities.\n5. Evaluation Results\nThis section presents the outcomes of the comprehensive\nevaluation conducted on the selected LLMs: TripoSR by Stabil-\nity AI, Gemma-7b by Google, Mistral 7B by Mistral AI, Llama-\n2-7b by Meta, and GemMoE-Beta-1 by CrystalCare AI. The\nresults are divided into subsections reflecting the key areas of\ninterest: transparency of reasoning, accuracy of explanations,\nconsistency across contexts, adaptability to feedback, and per-\nformance according to quantitative metrics.\n5.1. Transparency of Reasoning\nThe evaluation revealed significant variation in the ability\nof the models to o ffer transparent reasoning behind their deci-\nsions, as visualized in Figure 1. TripoSR and GemMoE-Beta-1\ndemonstrated a higher degree of transparency, enabling users to\ntrace the decision-making process more e ffectively than other\nmodels. This was particularly evident in complex reasoning\ntasks, where these models provided clearer intermediate steps\nand justifications. In contrast, Gemma-7b and Llama-2-7b, while\nTripoSR\nGemma-7bMistral 7BLlama-2-7b\nGemMoE-Beta-1\n20\n40\n60\n80\n100\n82\n68\n75\n64\n88\nScore\nTransparency Score\nFigure 1: Transparency of Reasoning Scores for Evaluated Models\nperforming admirably in various tasks, offered less insight into\ntheir reasoning processes, making it challenging for users to\nunderstand the basis of their decisions fully. The bar chart in\nFigure 1 illustrates the scores assigned to each model based on\ntheir transparency, underscoring the observed variations.\n5.2. Accuracy of Explanations\nAccuracy of explanations was assessed through expert judg-\nment, comparing the models’ explanations against benchmark\nexpectations. Gemma-7b and Llama-2-7b excelled in provid-\ning accurate and contextually relevant explanations for their\noutputs, reflecting a deep understanding of the tasks. Mistral\n7B and TripoSR, though generally accurate, occasionally pro-\nvided explanations that lacked the necessary detail or context to\nbe fully comprehensible. GemMoE-Beta-1’s explanations were\nnoted for their precision in technical domains, although they\nwere sometimes too specialized for a general audience. The\ndistribution of scores, illustrated in Figure 2, quantitatively re-\nflects these findings.\n5.3. Consistency Across Contexts\nThe models’ ability to maintain a consistent level of in-\nterpretability and explainability across various inputs and con-\ntexts was evaluated, as illustrated in Figure 3. Llama-2-7b and\nGemma-7b demonstrated remarkable consistency, performing\nadmirably across a broad spectrum of tasks and data types. Their\nscores reflect a high degree of stability in interpretation and ex-\nplanation, regardless of the complexity or novelty of the con-\ntext. In contrast, TripoSR and GemMoE-Beta-1, while show-\ning robust performance in familiar domains, exhibited notable\nvariability in unfamiliar domains or when presented with am-\nbiguous contexts. Mistral 7B’s performance in consistency was\n5\nTripoSR\nGemma-7bMistral 7BLlama-2-7b\nGemMoE-Beta-1\n20\n40\n60\n80\n100\n72\n88\n70\n85\n77\nScore\nAccuracy Score\nFigure 2: Accuracy of Explanations Scores for Evaluated Models\ninfluenced by its adaptive learning mechanisms. While these\nmechanisms offer advantages in dynamic learning environments,\nthey occasionally resulted in less predictable interpretability in\nmore static settings. The bar chart in Figure 3 provides a visual\nrepresentation of these findings, underscoring the variability in\nconsistency across the evaluated models.\n5.4. Adaptability to Feedback\nAdaptability to feedback, a crucial aspect of model per-\nformance, was systematically evaluated by introducing modi-\nfications based on initial outputs and assessing subsequent im-\nprovements. As depicted in Figure 4, Mistral 7B and TripoSR\nshowed outstanding adaptability, swiftly incorporating feedback\nto refine both their decision-making processes and the clarity\nof their explanations. This rapid adjustment highlights their\nadvanced learning architectures capable of dynamic evolution\nbased on new information. GemMoE-Beta-1 and Llama-2-7b\nexhibited moderate adaptability, demonstrating tangible improve-\nments over time, albeit at a more gradual pace. This suggests a\nsolid but less flexible framework for integrating iterative feed-\nback. Gemma-7b, despite its strong initial performance, dis-\nplayed limited adaptability, underscoring possible constraints\nin its learning framework that may hinder the integration of\nnew insights into its decision-making processes. The quantita-\ntive assessment provided in Figure 4 visually summarizes these\nfindings, offering a clear comparison of each model’s ability to\nadapt based on feedback.\n5.5. Quantitative Metrics\nThe models were rigorously evaluated on established quan-\ntitative metrics, including fidelity, comprehensiveness, and suf-\nficiency, to quantitatively measure the quality of explanations\nTripoSR\nGemma-7bMistral 7BLlama-2-7b\nGemMoE-Beta-1\n20\n40\n60\n80\n100\n70\n85\n65\n90\n75\nScore\nConsistency Score\nFigure 3: Consistency Across Contexts Scores for Evaluated Models\nTripoSR\nGemma-7bMistral 7BLlama-2-7b\nGemMoE-Beta-1\n20\n40\n60\n80\n100\n88\n60\n95\n70 75\nScore\nAdaptability Score\nFigure 4: Adaptability to Feedback Scores for Evaluated Models\n6\nand interpretability. As depicted in Figure 5, Llama-2-7b achieved\nhigh scores across all metrics, demonstrating a well-rounded\nand balanced approach to generating understandable and de-\ntailed explanations. Gemma-7b excelled particularly in fidelity,\nensuring that its explanations closely aligned with the underly-\ning decision processes. TripoSR and Mistral 7B showed strong\nperformance in comprehensiveness, indicating their ability to\nprovide wide-ranging and detailed explanations, although they\nhad areas for improvement in fidelity and sufficiency. GemMoE-\nBeta-1 was notable for its high su fficiency scores, e fficiently\nconveying essential information for user comprehension of model\ndecisions. The bar chart visualizes these findings, highlight-\ning the distinct capabilities and potential areas for enhancement\namong the evaluated models.\n6. Discussion\nThis discussion delves into the broader implications of our\nfindings, o ffering insights into the future trajectory of Large\nLanguage Model (LLM) development, their strengths, limita-\ntions, and the nuanced roles they play in advancing AI’s poten-\ntial. Each subsection addresses a critical aspect of LLM capa-\nbilities, drawing from the evaluation results to propose direc-\ntions for future research, development, and ethical considera-\ntions. This discussion highlights the multifaceted challenges\nand opportunities presented by current LLM technologies. By\naddressing these critical aspects, the field can advance towards\ncreating more ethical, understandable, and effective AI systems.\n6.1. The Balance Between Model Complexity and Interpretabil-\nity\nThe inherent tension between increasing model complexity\nfor performance gains and maintaining interpretability poses a\nsignificant challenge for LLM development. As models like\nLlama-2-7b demonstrate robust capabilities across diverse tasks,\ntheir intricate internal mechanisms complicate the transparency\nof decision-making processes. This complexity underscores the\nnecessity for innovative approaches that enhance model inter-\npretability without sacrificing performance, suggesting a piv-\notal area for future research.\n6.2. Ethical Implications of Model Explanations\nThe accuracy and relevance of model explanations not only\nimpact user trust but also have profound ethical implications,\nespecially in sensitive applications. Models exhibiting high fi-\ndelity in their explanations, such as Gemma-7b, underscore the\npotential for LLMs to operate ethically by making their reason-\ning processes transparent. However, the variability in explana-\ntion quality across models calls for a standardized framework\nto evaluate and ensure ethical compliance, particularly in appli-\ncations with significant societal impact.\n6.3. Adaptability and Continuous Learning\nThe adaptability of LLMs to feedback, as demonstrated by\nMistral 7B and TripoSR, highlights the importance of contin-\nuous learning mechanisms in AI development. These capabil-\nities not only enhance model performance over time but also\nensure that models remain relevant and e ffective as language\nevolves. Future LLMs could benefit from integrating more dy-\nnamic learning processes, allowing them to adjust more fluidly\nto new information and user needs.\n6.4. Technical Challenges in Model Training\nTraining LLMs to achieve both high performance and ex-\nplainability presents numerous technical challenges, including\ndata selection biases and the computational cost of training.\nThe performance of GemMoE-Beta-1 in technical domains il-\nlustrates the potential for specialized models to overcome these\nchallenges, though at the cost of general applicability. Address-\ning these technical hurdles requires a balanced approach to data\ncuration and model architecture design, ensuring that LLMs can\nefficiently learn from diverse datasets without prohibitive com-\nputational demands.\n6.5. The Role of Domain-Specific Models\nOur findings indicate a promising role for domain-specific\nmodels, such as GemMoE-Beta-1, in enhancing interpretabil-\nity and utility in specialized fields. These models demonstrate\nthat tailored architectures and training datasets can significantly\nimprove performance and explanation quality in specific do-\nmains. This specialization suggests a pathway for the develop-\nment of LLMs that can meet the unique requirements of fields\nlike medicine, law, and engineering, where accuracy and ex-\nplainability are paramount.\n6.6. Implications for AI Governance and Policy\nFinally, the variability in model performances and their im-\nplications for interpretability and ethical AI use underscore the\nimportance of robust governance and policy frameworks. Effec-\ntive policies must consider the diverse capabilities and limita-\ntions of current LLMs, promoting transparency, accountability,\nand ethical use. As LLMs become increasingly integrated into\nsocietal infrastructures, policymakers and AI developers must\ncollaborate to ensure these technologies are developed and de-\nployed responsibly.\n6.7. Future Directions in LLM Research\nThe evolution of LLMs, as evidenced by the advancements\nin models evaluated in this study, points to several future re-\nsearch directions. These include exploring alternative architec-\ntures that balance interpretability with computational efficiency,\ndeveloping more nuanced evaluation frameworks for model ex-\nplanations, and investigating the ethical implications of auto-\nmated decision-making. Such research is vital for ensuring that\nLLMs contribute positively to society, enhancing our ability to\ncommunicate, learn, and make decisions.\n7. Conclusion\nThis study embarked on an in-depth examination of the in-\nterpretability and explainability of five prominent Large Lan-\nguage Models: TripoSR by Stability AI, Gemma-7b by Google,\n7\nFidelity Comprehensiveness Sufficiency\n0\n20\n40\n60\n80\n100\n90 88 9294\n86 8885 89 8684\n90\n8587 85\n93\nScore\nLlama-2-7b Gemma-7b TripoSR Mistral 7B GemMoE-Beta-1\nFigure 5: Quantitative Metrics Evaluation Scores for Each Model\nMistral 7B by Mistral AI, Llama-2-7b by Meta, and GemMoE-\nBeta-1 by CrystalCare AI. Through a meticulous evaluation\nprocess, grounded in both qualitative and quantitative analyses,\nwe have illuminated the nuanced capabilities and limitations of\nthese models in making their decision-making processes acces-\nsible and understandable to humans. Our findings revealed the\nsignificant strides made in advancing LLM technologies, with\neach model demonstrating unique strengths in various aspects\nof interpretability and explainability. For instance, TripoSR and\nGemMoE-Beta-1 were notable for their transparency in reason-\ning, allowing users to trace the logic behind their outputs more\neffectively. On the other hand, Gemma-7b and Llama-2-7b ex-\ncelled in the accuracy of their explanations, o ffering insights\nthat were both relevant and contextually grounded.\nHowever, the research also highlighted critical areas requir-\ning further attention. The variability in performance across\ndifferent contexts and the challenges in maintaining a consis-\ntent level of interpretability and explainability across all mod-\nels indicate that there is still substantial room for improvement.\nMoreover, the adaptability of these models to feedback varied,\nsuggesting that iterative enhancements to their learning frame-\nworks could yield significant benefits. The significance of in-\nterpretability and explainability in LLMs cannot be overstated.\nAs these models become increasingly integrated into various\nsectors of society, the ability to understand and trust their out-\nputs becomes paramount. This study’s findings contribute to\na growing body of knowledge that seeks to bridge the gap be-\ntween human comprehension and machine reasoning, a critical\nendeavor as we navigate the complexities of AI integration into\ndaily life.\nThis research provides a foundational step towards demys-\ntifying the inner workings of LLMs, paving the way for more\ntransparent, accountable, and user-friendly AI systems. The\njourney to fully interpretable and explainable LLMs is ongo-\ning, and this study contributes to the collective e ffort to ensure\nAI technologies serve humanity’s best interests, fostering an en-\nvironment of trust and collaboration between humans and ma-\nchines. Future research should focus on developing standard-\nized methodologies for assessing interpretability and explain-\nability in LLMs, exploring innovative approaches to enhance\nmodel transparency, and investigating the implications of these\nattributes on user trust and model adoption. Furthermore, inter-\ndisciplinary studies combining insights from cognitive science,\nlinguistics, and computer science may o ffer novel perspectives\non making LLMs more understandable and relatable to human\nusers.\nReferences\n[1] L. Cao, Ai in finance: challenges, techniques, and opportunities, ACM\nComputing Surveys (CSUR) 55 (3) (2022) 1–38.\n[2] B. Li, P. Qi, B. Liu, S. Di, J. Liu, J. Pei, J. Yi, B. Zhou, Trustworthy\nai: From principles to practices, ACM Computing Surveys 55 (9) (2023)\n1–46.\n[3] R. Gruetzemacher, D. Paradice, Deep transfer learning & beyond: Trans-\nformer language models in information systems research, ACM Comput-\ning Surveys (CSUR) 54 (10s) (2022) 1–35.\n[4] Y . Chang, X. Wang, J. Wang, Y . Wu, L. Yang, K. Zhu, H. Chen, X. Yi,\nC. Wang, Y . Wang, et al., A survey on evaluation of large language mod-\nels, ACM Transactions on Intelligent Systems and Technology (2023).\n[5] B. Min, H. Ross, E. Sulem, A. P. B. Veyseh, T. H. Nguyen, O. Sainz,\nE. Agirre, I. Heintz, D. Roth, Recent advances in natural language pro-\ncessing via large pre-trained language models: A survey, ACM Comput-\ning Surveys 56 (2) (2023) 1–40.\n[6] S. Pan, L. Luo, Y . Wang, C. Chen, J. Wang, X. Wu, Unifying large lan-\nguage models and knowledge graphs: A roadmap, IEEE Transactions on\nKnowledge and Data Engineering (2024).\n[7] J. R. Bellegarda, Large vocabulary speech recognition with multispan sta-\ntistical language models, IEEE Transactions on Speech and Audio Pro-\ncessing 8 (1) (2000) 76–84.\n[8] V . Hassija, V . Chamola, A. Mahapatra, A. Singal, D. Goel, K. Huang,\nS. Scardapane, I. Spinelli, M. Mahmud, A. Hussain, Interpreting black-\n8\nbox models: a review on explainable artificial intelligence, Cognitive\nComputation 16 (1) (2024) 45–74.\n[9] L. Longo, R. Goebel, F. Lecue, P. Kieseberg, A. Holzinger, Explainable\nartificial intelligence: Concepts, applications, research challenges and vi-\nsions, in: International cross-domain conference for machine learning and\nknowledge extraction, Springer, 2020, pp. 1–16.\n[10] A. Ferrario, M. Loi, E. Vigan `o, In ai we trust incrementally: A multi-\nlayer model of trust to analyze human-artificial intelligence interactions,\nPhilosophy & Technology 33 (3) (2020) 523–539.\n[11] S. C. Robinson, Trust, transparency, and openness: How inclusion of cul-\ntural values shapes nordic national public policy strategies for artificial\nintelligence (ai), Technology in Society 63 (2020) 101421.\n[12] M. H. Ronaghi, A. Forouharfar, A contextualized study of the usage of\nthe internet of things (iots) in smart farming in a typical middle eastern\ncountry within the context of unified theory of acceptance and use of tech-\nnology model (utaut), Technology in Society 63 (2020) 101415.\n[13] J. Ayling, A. Chapman, Putting ai ethics to work: are the tools fit for\npurpose?, AI and Ethics 2 (3) (2022) 405–429.\n[14] C. Huang, Z. Zhang, B. Mao, X. Yao, An overview of artificial intelli-\ngence ethics, IEEE Transactions on Artificial Intelligence 4 (4) (2022)\n799–819.\n[15] T. Butler, L. O’Brien, Artificial intelligence for regulatory compliance:\nAre we there yet?, Journal of Financial Compliance 3 (1) (2019) 44–59.\n[16] R. Clarke, Regulatory alternatives for ai, Computer Law & Security Re-\nview 35 (4) (2019) 398–409.\n[17] T. R. McIntosh, T. Susnjak, T. Liu, P. Watters, R. Nowrozy, M. N. Halga-\nmuge, From cobit to iso 42001: Evaluating cybersecurity frameworks for\nopportunities, risks, and regulatory compliance in commercializing large\nlanguage models, arXiv preprint arXiv:2402.15770 (2024).\n[18] R. Al-Shabandar, G. Lightbody, F. Browne, J. Liu, H. Wang, H. Zheng,\nThe application of artificial intelligence in financial compliance manage-\nment, in: Proceedings of the 2019 International Conference on Artificial\nIntelligence and Advanced Manufacturing, 2019, pp. 1–6.\n[19] R. Dazeley, P. Vamplew, C. Foale, C. Young, S. Aryal, F. Cruz, Levels\nof explainable artificial intelligence for human-aligned conversational ex-\nplanations, Artificial Intelligence 299 (2021) 103525.\n[20] B. Kim, J. Park, J. Suh, Transparency and accountability in ai decision\nsupport: Explaining and visualizing convolutional neural networks for\ntext information, Decision Support Systems 134 (2020) 113302.\n[21] G. Sofianidis, J. M. Ro ˇzanec, D. Mladenic, D. Kyriazis, A review of ex-\nplainable artificial intelligence in manufacturing, Trusted Artificial Intel-\nligence in Manufacturing (2021) 93.\n[22] R. Kaushik, Explainability in machine learning: Bridging the gap be-\ntween model complexity and interpretability, Edu Journal of International\nAffairs and Research, ISSN: 2583-9993 2 (4) (2023) 57–63.\n[23] D. A. Broniatowski, et al., Psychological foundations of explainability\nand interpretability in artificial intelligence, NIST, Tech. Rep (2021).\n[24] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, Y . Cao, React:\nSynergizing reasoning and acting in language models, in: International\nConference on Learning Representations (ICLR), 2023.\n[25] E. Hajihashemi Varnousfaderani, Challenges and insights in semantic\nsearch using language models (2023).\n[26] B. Wang, Towards trustworthy large language models, Ph.D. thesis, Uni-\nversity of Illinois at Urbana-Champaign (2023).\n[27] G.-C. Garbacea, Neural language generation for content adaptation: Ex-\nplainable, efficient low-resource text simplification and evaluation, Ph.D.\nthesis (2023).\n[28] E. C. G. Stromsvag, Exploring the why in ai: Investigating how visual\nquestion answering models can be interpreted by post-hoc linguistic and\nvisual explanations, Master’s thesis (2023).\n[29] G. Kazeminejad, Computational lexical resources for explainable natural\nlanguage understanding, Ph.D. thesis, University of Colorado at Boulder\n(2023).\n[30] K. Hamilton, A. Nayak, B. Bo ˇzi´c, L. Longo, Is neuro-symbolic ai meeting\nits promises in natural language processing? a structured review, Seman-\ntic Web (Preprint) (2022) 1–42.\n[31] K. Schoneveld, Toward a normative account of machine learning expla-\nnation via levels of abstraction, Master’s thesis (2023).\n[32] V . M. Malode, Benchmarking public large language model, Ph.D. thesis,\nTechnische Hochschule Ingolstadt (2024).\n[33] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T.\nCheng, A. Jin, T. Bos, L. Baker, Y . Du, et al., Lamda: Language models\nfor dialog applications, arXiv preprint arXiv:2201.08239 (2022).\n[34] A. BARBERIO, Large language models in data preparation: opportuni-\nties and challenges (2022).\n[35] C. Ziems, W. Held, O. Shaikh, J. Chen, Z. Zhang, D. Yang, Can large\nlanguage models transform computational social science?, Computational\nLinguistics (2024) 1–55.\n[36] Q. Ma, X. Xue, D. Zhou, X. Yu, D. Liu, X. Zhang, Z. Zhao, Y . Shen,\nP. Ji, J. Li, et al., Computational experiments meet large language model\nbased agents: A survey and perspective, arXiv preprint arXiv:2402.00262\n(2024).\n[37] S. Krishna, J. Ma, D. Slack, A. Ghandeharioun, S. Singh, H. Lakkaraju,\nPost hoc explanations of language models can improve language models,\nAdvances in Neural Information Processing Systems 36 (2024).\n[38] H. Zhao, H. Chen, F. Yang, N. Liu, H. Deng, H. Cai, S. Wang, D. Yin,\nM. Du, Explainability for large language models: A survey, ACM Trans-\nactions on Intelligent Systems and Technology 15 (2) (2024) 1–38.\n[39] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Ho ffmann, F. Song,\nJ. Aslanides, S. Henderson, R. Ring, S. Young, et al., Scaling lan-\nguage models: Methods, analysis & insights from training gopher, arXiv\npreprint arXiv:2112.11446 (2021).\n[40] A. Tornede, D. Deng, T. Eimer, J. Giovanelli, A. Mohan, T. Ruhkopf,\nS. Segel, D. Theodorakopoulos, T. Tornede, H. Wachsmuth, et al., Au-\ntoml in the age of large language models: Current challenges, future op-\nportunities and risks, arXiv preprint arXiv:2306.08107 (2023).\n[41] Q. Yang, M. Ongpin, S. Nikolenko, A. Huang, A. Farseev, Against opac-\nity: Explainable ai and large language models for e ffective digital ad-\nvertising, in: Proceedings of the 31st ACM International Conference on\nMultimedia, 2023, pp. 9299–9305.\n[42] S. Luo, H. Ivison, C. Han, J. Poon, Local interpretations for explainable\nnatural language processing: A survey, arXiv preprint arXiv:2103.11072\n(2021).\n[43] S. Reddy, Evaluating large language models for use in healthcare: A\nframework for translational value assessment, Informatics in Medicine\nUnlocked (2023) 101304.\n[44] T. Saha, D. Ganguly, S. Saha, P. Mitra, Large language models’ inter-\npretability and trustworthiness (llmit) (2023).\n[45] M. Jakesch, Assessing the E ffects and Risks of Large Language Models\nin AI-Mediated Communication, Cornell University, 2022.\n[46] Y . Yang, A. Panagopoulou, S. Zhou, D. Jin, C. Callison-Burch,\nM. Yatskar, Language in a bottle: Language model guided concept bot-\ntlenecks for interpretable image classification, in: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2023, pp. 19187–19197.\n[47] A. Sheth, K. Roy, M. Gaur, Neurosymbolic artificial intelligence (why,\nwhat, and how), IEEE Intelligent Systems 38 (3) (2023) 56–62.\n[48] E. Zelikman, Y . Wu, J. Mu, N. Goodman, Star: Bootstrapping reasoning\nwith reasoning, Advances in Neural Information Processing Systems 35\n(2022) 15476–15488.\n[49] N. Mannhardt, Improving patient access and comprehension of clinical\nnotes: Leveraging large language models to enhance readability and un-\nderstanding, Ph.D. thesis, Massachusetts Institute of Technology (2023).\n[50] Y . Bai, J. Ying, Y . Cao, X. Lv, Y . He, X. Wang, J. Yu, K. Zeng, Y . Xiao,\nH. Lyu, et al., Benchmarking foundation models with language-model-\nas-an-examiner, Advances in Neural Information Processing Systems 36\n(2024).\n[51] M. Lee, P. Liang, Q. Yang, Coauthor: Designing a human-ai collaborative\nwriting dataset for exploring language model capabilities, in: Proceed-\nings of the 2022 CHI conference on human factors in computing systems,\n2022, pp. 1–19.\n[52] A. Caballero Hinojosa, Exploring the power of large language models:\nNews intention detection using adaptive learning prompting (2023).\n[53] H. Manikandan, Y . Jiang, J. Z. Kolter, Language models are weak\nlearners, Advances in Neural Information Processing Systems 36 (2023)\n50907–50931.\n[54] K. Marko, Applying generative ai and large language models in business\napplications (2023).\n[55] N. D ´ıaz-Rodr´ıguez, J. Del Ser, M. Coeckelbergh, M. L. de Prado,\nE. Herrera-Viedma, F. Herrera, Connecting the dots in trustworthy ar-\ntificial intelligence: From ai principles, ethics, and key requirements\nto responsible ai systems and regulation, Information Fusion 99 (2023)\n9\n101896.\n[56] M. Mylrea, N. Robinson, Artificial intelligence (ai) trust framework and\nmaturity model: applying an entropy lens to improve security, privacy,\nand ethical ai, Entropy 25 (10) (2023) 1429.\n[57] D. P. Panagoulias, M. Virvou, G. A. Tsihrintzis, A novel framework for\nartificial intelligence explainability via the technology acceptance model\nand rapid estimate of adult literacy in medicine using machine learning,\nExpert Systems with Applications 248 (2024) 123375.\n[58] I. D. Raji, I. E. Kumar, A. Horowitz, A. Selbst, The fallacy of ai func-\ntionality, in: Proceedings of the 2022 ACM Conference on Fairness, Ac-\ncountability, and Transparency, 2022, pp. 959–972.\n[59] N. Karanikolas, E. Manga, N. Samaridi, E. Tousidou, M. Vassilakopoulos,\nLarge language models versus natural language understanding and gener-\nation, in: Proceedings of the 27th Pan-Hellenic Conference on Progress\nin Computing and Informatics, 2023, pp. 278–290.\n[60] S. Hauger Frendberg, A. Leitner, Artificial intelligence in fund manage-\nment (2023).\n[61] P. Lorenz, K. Perset, J. Berryhill, Initial policy considerations for genera-\ntive artificial intelligence (2023).\n[62] R. Schwartz, R. Schwartz, A. Vassilev, K. Greene, L. Perine, A. Burt,\nP. Hall, Towards a standard for identifying and managing bias in artificial\nintelligence, V ol. 3, US Department of Commerce, National Institute of\nStandards and Technology, 2022.\n[63] P. Maddigan, A. Lensen, B. Xue, Explaining genetic programming trees\nusing large language models, arXiv preprint arXiv:2403.03397 (2024).\n[64] M. A. K. Raiaan, M. S. H. Mukta, K. Fatema, N. M. Fahad, S. Sakib,\nM. M. J. Mim, J. Ahmad, M. E. Ali, S. Azam, A review on large lan-\nguage models: Architectures, applications, taxonomies, open issues and\nchallenges, IEEE Access (2024).\n[65] D. Slack, S. Krishna, H. Lakkaraju, S. Singh, Explaining machine learn-\ning models with interactive natural language conversations using talkto-\nmodel, Nature Machine Intelligence 5 (8) (2023) 873–883.\n[66] J. Pool, M. Indulska, S. Sadiq, Large language models and generative ai\nin telehealth: a responsible use lens, Journal of the American Medical\nInformatics Association (2024) ocae035.\n[67] T. R. McIntosh, T. Susnjak, T. Liu, P. Watters, M. N. Halgamuge, In-\nadequacies of large language model benchmarks in the era of generative\nartificial intelligence, arXiv preprint arXiv:2402.09880 (2024).\n[68] Q. Ai, T. Bai, Z. Cao, Y . Chang, J. Chen, Z. Chen, Z. Cheng, S. Dong,\nZ. Dou, F. Feng, et al., Information retrieval meets large language models:\na strategic report from chinese ir community, AI Open 4 (2023) 80–90.\n[69] R. Sarkar, Integrating structured and unstructured knowledge sources for\ndomain-specific chatbots (2023).\n[70] S. N. Hart, N. G. Ho ffman, P. Gershkovich, C. Christenson, D. S. Mc-\nClintock, L. J. Miller, R. Jackups, V . Azimi, N. Spies, V . Brodsky, Orga-\nnizational preparedness for the use of large language models in pathology\ninformatics, Journal of Pathology Informatics (2023) 100338.\n[71] T. Wu, M. Terry, C. J. Cai, Ai chains: Transparent and controllable\nhuman-ai interaction by chaining large language model prompts, in: Pro-\nceedings of the 2022 CHI conference on human factors in computing sys-\ntems, 2022, pp. 1–22.\n[72] Y . Li, S. Wang, H. Ding, H. Chen, Large language models in finance: A\nsurvey, in: Proceedings of the Fourth ACM International Conference on\nAI in Finance, 2023, pp. 374–382.\n[73] P. Brie, N. Burny, A. Slu ¨yters, J. Vanderdonckt, Evaluating a large lan-\nguage model on searching for gui layouts, Proceedings of the ACM on\nHuman-Computer Interaction 7 (EICS) (2023) 1–37.\n10",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.9638533592224121
    },
    {
      "name": "Computer science",
      "score": 0.45413264632225037
    },
    {
      "name": "Language model",
      "score": 0.439683198928833
    },
    {
      "name": "Natural language processing",
      "score": 0.3699048161506653
    },
    {
      "name": "Artificial intelligence",
      "score": 0.354489803314209
    }
  ],
  "institutions": [],
  "cited_by": 18
}