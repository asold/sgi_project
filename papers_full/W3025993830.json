{
    "title": "IntelliCode compose: code generation using transformer",
    "url": "https://openalex.org/W3025993830",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2627672354",
            "name": "Alexey Svyatkovskiy",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A3024207340",
            "name": "Shao Kun Deng",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2175401833",
            "name": "Shengyu Fu",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A1981173961",
            "name": "Neel Sundaresan",
            "affiliations": [
                "Microsoft (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6713507509",
        "https://openalex.org/W2794601162",
        "https://openalex.org/W2360967250",
        "https://openalex.org/W2955270045",
        "https://openalex.org/W1970607969",
        "https://openalex.org/W3146720657",
        "https://openalex.org/W1994573369",
        "https://openalex.org/W2950368691",
        "https://openalex.org/W2052468877",
        "https://openalex.org/W3150567095",
        "https://openalex.org/W2957919018",
        "https://openalex.org/W2380058981",
        "https://openalex.org/W2947160092",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2973727699",
        "https://openalex.org/W2549416390",
        "https://openalex.org/W2973049837",
        "https://openalex.org/W2466175319",
        "https://openalex.org/W2886424491",
        "https://openalex.org/W2108325777",
        "https://openalex.org/W2754629507",
        "https://openalex.org/W6638523607",
        "https://openalex.org/W2924902521",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W3005855585",
        "https://openalex.org/W2266912522",
        "https://openalex.org/W2951861246",
        "https://openalex.org/W2787998955",
        "https://openalex.org/W3100026183",
        "https://openalex.org/W2142403498",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W2143861926",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2963159690",
        "https://openalex.org/W2962964385",
        "https://openalex.org/W2952355681",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2123044578",
        "https://openalex.org/W2971291256",
        "https://openalex.org/W1821462560"
    ],
    "abstract": "In software development through integrated development environments (IDEs), code completion is one of the most widely used features. Nevertheless, majority of integrated development environments only support completion of methods and APIs, or arguments. In this paper, we introduce IntelliCode Compose $-$ a general-purpose multilingual code completion tool which is capable of predicting sequences of code tokens of arbitrary types, generating up to entire lines of syntactically correct code. It leverages state-of-the-art generative transformer model trained on 1.2 billion lines of source code in Python, $C\\#$, JavaScript and TypeScript programming languages. IntelliCode Compose is deployed as a cloud-based web service. It makes use of client-side tree-based caching, efficient parallel implementation of the beam search decoder, and compute graph optimizations to meet edit-time completion suggestion requirements in the Visual Studio Code IDE and Azure Notebook. Our best model yields an average edit similarity of $86.7\\%$ and a perplexity of 1.82 for Python programming language.",
    "full_text": "IntelliCode Compose: Code Generation using Transformer\nAlexey Svyatkovskiyâˆ—\nMicrosoft\nRedmond, WA, USA\nalsvyatk@microsoft.com\nShao Kun Dengâˆ—\nMicrosoft\nRedmond, WA, USA\nshade@microsoft.com\nShengyu Fu\nMicrosoft\nRedmond, WA, USA\nshengyfu@microsoft.com\nNeel Sundaresan\nMicrosoft\nRedmond, WA, USA\nneels@microsoft.com\nABSTRACT\nIn software development through integrated development envi-\nronments (IDEs), code completion is one of the most widely used\nfeatures. Nevertheless, majority of integrated development environ-\nments only support completion of methods and APIs, or arguments.\nIn this paper, we introduce IntelliCode Compose â€“ a general-\npurpose multilingual code completion tool which is capable of\npredicting sequences of code tokens of arbitrary types, generating\nup to entire lines of syntactically correct code. It leverages state-\nof-the-art generative transformer model trained on 1.2 billion lines\nof source code in Python, C#, JavaScript and TypeScript program-\nming languages. IntelliCode Compose is deployed as a cloud-based\nweb service. It makes use of client-side tree-based caching, efficient\nparallel implementation of the beam search decoder, and compute\ngraph optimizations to meet edit-time completion suggestion re-\nquirements in the Visual Studio Code IDE and Azure Notebook.\nOur best model yields an average edit similarity of 86.7% and a\nperplexity of 1.82 for Python programming language.\nCCS CONCEPTS\nâ€¢ Software and its engineering â†’Integrated and visual devel-\nopment environments; â€¢ Computing methodologies â†’Neu-\nral networks .\nKEYWORDS\nCode completion, neural networks, naturalness of software\nACM Reference Format:\nAlexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan.\n2020. IntelliCode Compose: Code Generation using Transformer. InProceed-\nings of the 28th ACM Joint European Software Engineering Conference and\nSymposium on the Foundations of Software Engineering (ESEC/FSE â€™20), No-\nvember 8â€“13, 2020, Virtual Event, USA. ACM, New York, NY, USA, 11 pages.\nhttps://doi.org/10.1145/3368089.3417058\nâˆ—Equal contribution.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA\nÂ© 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-7043-1/20/11. . . $15.00\nhttps://doi.org/10.1145/3368089.3417058\n1 INTRODUCTION\nMachine learning has shown a great promise towards improving\nautomated software engineering across all stages. Some of the\nearly applications of machine learning of source code include code\nsearch [1, 2], bug detection and localization [3], program synthe-\nsis [4], code summarization [5] and code completion [6â€“10].\nThere are numerous code completion systems capable of effec-\ntively recommending method and API calls [ 6, 9â€“11], or finding\nthe correct argument [12â€“14]. Majority of argument completion\nsystems would, however, only work when the name of the method\nor API call is already typed in, thus leaving the task of completing\nthe method calls to software developers.\nIn this paper, we introduce IntelliCode Compose â€“ a general-\npurpose code completion framework, capable of generating code\nsequences of arbitrary token types, including local variables, meth-\nods or APIs, arguments, as well as punctuation, language keywords,\nand delimiters. IntelliCode Compose serves as a universal program-\nming language modeling tool, effectively generating syntactically\ncorrect code in multiple programming languages, capable of com-\npleting an entire line of code in a couple of key strokes, with a user\nexperience inspired by Gmail Smart Compose [15]. The proposed\nsystem is able to learn to infer types of programming language\nidentifiers and long-range code semantics without inputs extracted\nby means of a static analyzer explicitly passed to the model as\nfeatures.\nThe nature of the problem of code sequence completion makes\nstatistical language modeling approach a promising starting point.\nTo predict a whole line of source code tokens given an existing code\ncontext ğ¶ and vocabulary ğ‘‰, we train a neural model to generate\ntokens {ğ‘šğ‘¡}âŠ‚ ğ‘‰,ğ‘¡ = 0...ğ‘, conditioned on a sequence of preceding\ntokens {ğ‘ğ‘¡},ğ‘¡ = 0...ğ‘‡ of code snippet ğ¶.\nThe main contributions of the paper are as follows: (i) we intro-\nduce and pretrain a multi-layer generative transformer model for\ncode (GPT-C), which is a variant of the GPT-2 [ 16] trained from\nscratch on a large unsupervised multilingual source code dataset\n(cf. sections 3 and 4), comparing it to the monolingual counterparts,\nand a simple n-gram language modeling baseline, (ii) we propose\nand deploy a novel end-to-end code sequence completion system\ncalled IntelliCode Compose based on the GPT-C and an efficient\nclient-side caching system (cf. sections 7 and 8), (iii) we evaluate the\nquality of language model pretraining of GPT-C using perplexity,\nshowing that our best model achieves a perplexity of 1.82; we also\nshow that IntelliCode Compose achieves an average edit similarity\narXiv:2005.08025v2  [cs.CL]  29 Oct 2020\nESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan\nof 86.7% (cf. section 10), (iv) we introduce MultiGPT-C â€“ a multilin-\ngual version of our model, discuss and compare various approaches\nto multilingual modeling (cf section 9), (v) finally, we discuss and\ndocument practical challenges of training intermediate-sized neural\ntransformer models on high-performance computing clusters, and\ncloud-based model deployment (cf. section 12).\n2 MOTIVATING EXAMPLE\nFig. 1 shows an example method completion and an argument com-\npletion in C# programming language served by the Intellicode [17]\nextension in Visual Studio IDE1, as well as the whole-line of code\ncompletion generated by IntelliCode Compose, with the novel com-\npletion user experience. Previously existing code completion tools\nhave focused on specific token types or features, often failing to\nhave a holistic view of the surrounding context. For example, having\nselected a method to call on the s variable, there are still numerous\ncombinations of arguments to be passed to StartsWith, mak-\ning this task non-trivial. Correctly suggesting a whole-line of code\nrequires the model to infer types of the target token for method com-\npletion, and the correct local variables to be passed as arguments\nto the methods. Furthermore, additional structural and semantic\ninformation needs to be extracted from the context in order to make\naccurate statement-level suggestions.\n3 DATASET\nWe collect a large unsupervised source code dataset to train and\nevaluate the code sequence completion model. It comprises over 1.2\nbillion lines of source code in Python, C#, JavaScript and TypeScript\nprogramming languages, as summarized in Tab. 1. A total of over\n52000 top-starred (non-fork) projects in GitHub has been selected,\ncontaining libraries from a diverse set of domains, with over 4.7\nmillion source code files.\nWe split the dataset into development and test sets in the propor-\ntion 70-30 on the repository level. The development set is then split\nat random into training and validation sets in the proportion 80-20.\nThe final deployed model is retrained using the entire dataset.\n4 APPROACH\n4.1 Baseline Code Sequence Completion Model\nWe use statistical language modeling approach based on the n-\ngram model as a baseline in this work. The n-gram model is the\nprobabilistic Markov chain model for predicting text given the\ncontext consisting of n-1 preceding tokens.\nGiven context tokens {ğ‘ğ‘¡},ğ‘¡ = 0...ğ‘›âˆ’1, the model estimates the\nnext token probabilities based on the relative frequency counts:\nğ‘ƒ(ğ‘š|ğ‘0,ğ‘1,...ğ‘ğ‘›âˆ’1)= ğ‘(ğ‘0,ğ‘1,...ğ‘ğ‘›âˆ’1,ğ‘š)Ãğ‘(ğ‘0,ğ‘1,...ğ‘ğ‘›âˆ’1), (1)\nhere, numerator gives the count of allğ‘0,ğ‘1,...ğ‘ğ‘›âˆ’1,ğ‘š n-grams in the\ntraining corpus, while the denominator gives the cumulative n-gram\ncount that share common prefix ğ‘0,ğ‘1,...ğ‘ğ‘›âˆ’1. During training, the\nn-grams are extracted by rolling the window of size n sub-tokens,\nwith stride one (see more details on tokenization in section (5.1)).\n1https://visualstudio.microsoft.com/vs/\n4.2 Neural Code Sequence Completion Model\nTransformers [18â€“21] are a family of neural networks designed\nto process ordered sequential data. They have found numerous\napplications in the fields of natural language processing (NLP)\nand natural language understanding (NLU), including machine\ntranslation, question answering, and document summarization.\nSeveral transformer models such as GPT-2, BERT, XLNet, and\nRoBERTa [16, 20â€“22] have demonstrated the ability to learn effec-\ntively from unlabeled data to perform a wide variety of downstream\ntasks given supervised discriminative fine-tuning on each specific\ntask. In this work we build on the progress of transformers in NLP\nand NLU, applying it to an emerging field of source code under-\nstanding: a form of NLU with additional structural constraints and\ninsights from lexemes, abstract syntax tree (AST) or concrete syntax\ntree (CST), and dataflow graph.\nA transformer block will typically consist of a multi-head self-\nattention, followed by a two-layer multi-layer perceptron (MLP) [18],\noptionally containing residual connections and layer normaliza-\ntion [23]. Recent neural architecture searches of transformer models\nhave shown that using depth-wise separable convolutions along\nwith self-attention may speed up training without loss of accu-\nracy [24]. A typical transformer architecture for a sequence-to-\nsequence task will have an encoder (a stack of transformer blocks)\nand a decoder stack. Unlike the vanilla recurrent neural networks\n(RNNs) or their gated variants, including LSTM and GRU, transform-\ners do not require tokens in a sequence to be processed in a specific\norder, thus allowing more options for training parallelization [25].\nComposed of feed-forward layers, convolutions, and self-attention,\ntransformers are easy to quantize and serve in production.\nGPT-2 is an auto-regressive pre-trained model consisting of a\ndecoder-only transformer stack and one or more output layers,\noften referred to as â€œheadsâ€. GPT-2 for language modeling task has\na linear output layer with softmax output activation. IntelliCode\nCompose is built around a multi-layer generative pretrained trans-\nformer model for code (GPT-C), which is a variant of the GPT-2\ntrained from scratch on source code data, with weights of the out-\nput linear layer tied to the input embedding matrix, having specific\nhyperparameters as described in Tab. 4.\nIn order to predict a sequence of response tokens ğ‘€ = {ğ‘šğ‘¡},ğ‘¡ =\n0...ğ‘, conditioned on code snippet typed in by a software devel-\noper {ğ‘ğ‘¡},ğ‘¡ = 0...ğ‘‡, we need to estimate the following conditional\nprobability distribution:\nğ‘ƒ(ğ‘š0,ğ‘š1,...ğ‘šğ‘|ğ‘0,...ğ‘ğ‘‡)=\nğ‘Ã–\nğ‘–=1\nğ‘ƒ(ğ‘šğ‘–|ğ‘0,ğ‘1,...ğ‘ğ‘‡,ğ‘š0,...ğ‘šğ‘–âˆ’1). (2)\nWith the autoregressive approach, the objective is to maximize\nthe following log-likelihood:\nğ¿(ğ‘€)=\nâˆ‘ï¸\nğ‘–\nlog ğ‘ƒ(ğ‘šğ‘–|ğ‘0,ğ‘1,...ğ‘ğ‘‡,ğ‘šğ‘–âˆ’ğ‘˜,ğ‘šğ‘–âˆ’ğ‘˜+1,...ğ‘šğ‘–âˆ’1; Î˜) (3)\nwhere ğ‘˜ is the length of predicted code sequence, and the condi-\ntional probability ğ‘ƒ is modeled using a neural network with pa-\nrameters Î˜. These parameters are learned via stochastic gradient\ndescent optimization procedure.\nGPT-C applies a multi-headed self-attention operation over the\ninput context tokens followed by position-wise feed-forward layers\nIntelliCode Compose: Code Generation using Transformer ESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA\nFigure 1: Comparison of code completion scenarios. Top: method completion and argument completion served by Intellicode.\nBottom: whole-line of code completion served by the IntelliCode Compose.\nTable 1: Summary of the training dataset.\nProgramming language Number of files ( Ã—103) Number of lines ( Ã—106) Number of repositories\nC# 1172 201 4836\nPython 1200 240 18174\nJavaScript 1982 681 26553\nTypeScript 437 85 3255\nto produce an output distribution over target tokens:\nâ„0 = ğ‘Šğ‘’ Â·ğ¶+ğ‘Šğ‘, (4)\nâ„ğ‘™ = transformer_block(â„ğ‘™âˆ’1),âˆ€ğ‘™ = 1...ğ‘›, (5)\nğ‘ƒ(ğ‘šğ‘¡) = ğ‘¦ğ‘¡ = softmax(â„ğ‘› Â·ğ‘Šğ‘‡\nğ‘’ ),ğ‘¡ = 0...ğ‘, (6)\n(7)\nwhere ğ¶ = ğ‘âˆ’ğ‘˜,ğ‘âˆ’ğ‘˜+1,...ğ‘âˆ’1 is the context vector of tokens, ğ‘›is the\nnumber of layers, ğ‘Šğ‘’ âˆˆğ‘…|ğ‘‰|Ã—ğ‘‘ğ‘¥ is the tokens embedding matrix,\nand ğ‘Šğ‘ âˆˆğ‘…ğ‘ğ‘ğ‘¡ğ‘¥ Ã—ğ‘‘ğ‘¥ is the position embedding matrix, which en-\ncodes relative positions of tokens in a sequence. ğ‘ğ‘ğ‘¡ğ‘¥ is the length\nof the sequence attended to (context length), |ğ‘‰|is vocabulary size,\nand ğ‘‘ğ‘¥ is the embedding dimension.\nWe are reusing the input token embedding matrix as the out-\nput classification matrix [ 26], which allows to remove the large\nfully connected layer reducing the number of parameters by 25%.\nMore specifically, we introduce a projection matrix ğ´ = (ğ‘)ğ‘–ğ‘— âˆˆ\nğ‘…ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ Ã—ğ‘‘ğ‘¥ initialized according to a random uniform distribution.\nGiven an encoded code context and a hidden state at the last tempo-\nral step â„ğ‘›(ğ‘‡)âˆˆ ğ‘…ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ , we obtain the predicted token embedding\nvector by multiplying the two together asğ‘Šğ‘ğ‘Ÿğ‘’ğ‘‘\nğ‘’ = (wğ‘ğ‘Ÿğ‘’ğ‘‘)ğ‘— âˆˆğ‘…ğ‘‘ğ‘¥\nas:\nwğ‘ğ‘Ÿğ‘’ğ‘‘\nğ‘— =\nâˆ‘ï¸\nğ‘–\nâ„ğ‘›ğ‘–(ğ‘‡)ğ‘ğ‘–ğ‘—. (8)\nSubsequently, the logits are obtained as:\nğ‘¦ğ‘˜ =\nâˆ‘ï¸\nğ‘—\nwğ‘˜ğ‘—wğ‘ğ‘Ÿğ‘’ğ‘‘\nğ‘— +ğ‘ğ‘˜ (9)\nwhere ğ‘ğ‘˜,ğ‘˜ = 0...|ğ‘‰|âˆ’1 is the bias vector, andğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ is the number\nof hidden units per transformer block.\nDuring inference, beam-search decoding algorithm is applied\nto iteratively extract best token sequences according to a negative\nlog-likelihood optimization objective. This is explained in more\ndetail in section 7.\n5 PREPROCESSING\nIn what follows, we treat the source code data as a sequence of to-\nkens corresponding to the output of a lexical analyzer. Incidentally,\nthis can also be constructed through an in-order traversal of the\nterminal nodes of a concrete syntax tree (CST). In this work, we do\nnot leverage high-level structural representation such as abstract or\nconcrete syntax trees or control flow graphs, as it introduces addi-\ntional overhead and dependencies which slows down the inference\nand reduces coverage of the code completion system. Additionally,\nfor most programming languages, such representations can only be\ncorrectly retrieved on complete code snippets that are syntactically\ncorrect, which is often not available for a code completion system.\nOur approach is based on statistical language modeling of source\ncode, with several normalization rules extracted from concrete\nsyntax tree of a program. To overcome the issue of different styles\nand white space or tab conventions, we transform the code into\nsymbolic program tokens using custom tokenizers and regenerate\nthe code with a common style. During preprocessing, we parse\nprogram code in each file, extract information about token types\nand apply it to normalize the code, extract subtoken vocabulary and\nencode the sequences. This is done both for training and inference.\n5.1 Overcoming a Closed Vocabulary Problem\nA typical language model will attempt to generate a probability\ndistribution over all tokens in the vocabulary. This requires the\nESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan\nmodel to have access to encodings of all such tokens. In vanilla\nlanguage models this is achieved with a fixed vocabulary matrix,\nthus limiting model coverage to unseen tokens.\nThe issues of coverage can be addressed by using finer-level\nencodings for tokens. Instead of learning representations for each\ntoken, we learn representations for subtokens or combinations of\nUnicode characters. This both reduces the need to store an entire\nvocabulary and makes the model more robust to out-of-vocabulary\ntokens. This allows us to potentially generalize to previously unseen\nmethods, APIs, other language identifiers, or even training code\ncompletion models for multiple programming languages.\nWe experiment with two specific ways of tokenization:\n(1) Byte-Pair Encoding (BPE) tokenization â€“ unsupervised to-\nkenization, in which the most frequently occurring pair of\nUnicode characters is recursively replaced with a character\nthat does not occur in the vocabulary â€“ the approach adopted\nby various contextual language models in NLP.\n(2) Tokenization by splitting programming language identifiers\nusing casing conventions, such as camelCase, and\nPascalCase or snake_case â€“ the approach that has\nbeen shown to work for programming languages, though\nnot applicable to natural languages.\nWe use the sentencepiece2 tokenizer to extract subtoken level\nvocabulary, with special tokens for control flow and code struc-\nture representation. More specifically, we add control flow tokens\n<BOF> and <EOF> to mark the beginning and ending of a file in\norder to disambiguate similar identifier names in different files, and\n<EOL> to mark the ending of a line. Additionally, since Python\nuses white-spaces and indentation to demarcate code scope, we\nintroduce <INDENT> and <DEDENT> tokens to represent those\nscope delimiters. Fig. 2 illustrates the tokenization approaches.\n5.2 Exposing Sensitive Data through Code\nSuggestions\nProduction-level code completion systems based on statistical lan-\nguage modeling are commonly trained on vast amounts of source\ncode mined from GitHub or other version control systems. As large\namount of public data is ingested, it is unavoidable to encounter\ncases where people unintentionally leave sensitive information in\ntheir code, as part of string literals, code comments, or configura-\ntion files. Fig. 3 shows an example completion served by the Tab-\nNine [27] system exposing irrelevant and potentially sensitive data.\nTo tackle this problem, the training process needs to be shielded\nfrom inadvertently gaining access to secrets or personally identifi-\nable data. For this reason, we identify and normalize numeric literals,\nstring literals and comments, including docstrings, to<NUM_LIT>,\n<STR_LIT> and <COMMENT> special tokens3, respectively. How-\never, we have found that the most frequently used literals often\ncontain relevant information and can be used directly in the com-\npletions. For each language, a number of top most frequent numeric\nand string literals are preserved as <STR_LIT:lit> where lit\nis the original literal. For instance:\"__main__\", \"POST\", \"en\",\n2https://github.com/google/sentencepiece\n3For C#, in addition to <STR_LIT> and <NUM_LIT>, we also introduce\n<CHAR_LIT> for character literals. For JavaScript, we also introduce <RE_LIT>\nfor regular expression literals.\nTable 2: Number of literals kept for each type, and the per-\ncentile they represent for each training dataset.\nNumber kept Python C# JS,TS\nString 200 18 13 20\nNumber 50 63 58 70\nChar 20 - 42 -\nRegEx 50 - - 17\n\"default\". We did, however, leave identifier names to make code\nsuggestions context-dependent. The exact number of literals kept\nas well as the percentage they represent in the training data are\nshown in Table. 2.\n6 MODEL TRAINING\nOptimizing transformer neural networks is a computationally inten-\nsive problem which requires the engagement of high-performance\ncomputing (HPC) clusters in order to improve time to solution.\nSelection of well-performing hyperparameters requires searching a\nhigh-dimensional space. To evaluate a neural architecture or a set of\nhyperparameters entails running full model training and inference.\nWe scale up the training using synchronous data-parallel dis-\ntributed training algorithm with local gradient accumulation. The\nlearning rate controlling the magnitude of the weight update during\ngradient optimization is lowered upon completion of each epoch\naccording to the cosine decay. In a distributed regime, we increase\nthe learning rate during the first few epochs (â€œwarm-upâ€ period) to\nfacilitate reliable model convergence.\nThe offline training module of the IntelliCode Compose sys-\ntem is implemented as a Python library integrating PyTorch and\nHorovod [28] with Adasum algorithm for gradient summation 4.\nThe software stack makes use of CUDA 10, GPU accelerated deep\nlearning primitives from CuDNN 7, and PyTorch 1.2.0, NCCL col-\nlective communication library. We have trained our models on 5\nLambda V100 boxes, each having sixteen V100 GPUs with 32 GB\nHBM2 memory, eight 100 GB InfiniBand, and one 100 GB Ethernet\nconnection, managed with Kubernetes.\nWith the data-parallel implementation, pure computation time\nğ‘‡ğ‘ğ‘ğ‘¡ğ‘â„ per mini-batch step remains constant in the number of\nworker GPUs. The amount of data processed during one mini-batch\nstep increases linearly with the number of engaged workersğ‘. Syn-\nchronization between workers performed by means of a tree-like\nallreduce, would yield logarithmic complexity ğ‘‡ğ‘ ğ‘¦ğ‘›ğ‘ âˆlog ğ‘.\nThus, the number of mini-batches would decrease linearly with ğ‘,\ngiving a following scaling model:\nğ‘‡ğ‘’ğ‘ğ‘œğ‘â„ = 1\nğ‘ Â·(ğ‘‡ğ‘ğ‘ğ‘¡ğ‘â„ +ğ‘‡ğ‘ ğ‘¦ğ‘›ğ‘)= 1\nğ‘ Â·(ğ´+ğµÂ·ğ‘™ğ‘œğ‘”(ğ‘))= ğ‘‚(ğ‘™ğ‘œğ‘”(ğ‘)\nğ‘ )\n(10)\nOverall, the model architecture, tokenization, and training pro-\ncedure produce a large number of hyperparameters that must be\ntuned to maximize predictive performance. These hyperparameters\ninclude numerical values such as the learning rate and number of\ntransformer layers, dimension of embedding space, but also abstract\ncategorical variables such as the precise model architecture or the\n4https://github.com/horovod/horovod/pull/1484\nIntelliCode Compose: Code Generation using Transformer ESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA\nFigure 2: Illustration of tokenization approaches. From top to bottom: a raw code snippet consisting of a comment and an API\ncall with arguments, corresponding BPE split using sentencepiece, and corresponding split based on casing conventions.\nFigure 3: Example completion served by the TabNine system\nexposing a fragment of hash value.\nTable 3: Neural network training performance summary\nfor monolingual (Python) and multilingual (C#, Python,\nJavaScript and TypeScript) 24-layer GPT-C models on 80\nGPU workers.\nMonolingual Multilingual\nCumulative batch size 160 160\nTime per epoch 2.8 hours 19.7 hours\nSamples / second 163 Â±5 148 Â±5\nTokens / second 167000 Â±5000 152000 Â±5000\nsource code normalization algorithm. The number of trainable pa-\nrameters in the GPT-C transformer model scales near-linearly as a\nfunction of number of transformer blocks, and quadratically with\nthe number of hidden units per block as:ğ‘‘ğ‘¥Â·(|ğ‘‰|+ğ‘ğ‘ğ‘¡ğ‘¥)+ğ´Â·ğ‘›Â·ğ‘‘2\nğ‘šğ‘œğ‘‘ğ‘’ğ‘™.\nThe constant ğ´here is defined by the parameters of the MLP part\nof the transformer.\nThe best performing monolingual GPT-C models have 24 layers,\nscaled dot-product attention with 16 heads, and are trained with\nBPE vocabulary size of 50000, while the best multilingual version\nhas 26 transformer layers, 16 heads, and the vocabulary size of\n60000 subtokens. The rest of the model architecture parameters is\nsummarized in Tab. 4.\nWe train GPT-C using Adam stochastic optimization scheme with\nweight decay fix, base learning rate of6.25Ã—10âˆ’5, cumulative batch\nsize of 128, learning rate decay of 0.98 per epoch, and categorical\ncross-entropy loss. For multilingual model, each training mini-batch\nFigure 4: Top: time required to complete one pass over the\ndataset (one \"epoch\") during training versus the number\nof worker GPUs engaged. Experimental data are compared\nwith a semi-empirical theoretical scaling model and ideal\nscaling. Bottom: training loss as a function of epoch for\nmonolingual and multilingual models.\nhas to be composed of sentences coming from the same language,\nwhich is sampled at random from the set of all available languages.\n7 SEQUENCE DECODING\nEach inference call to the model yields a probability distribution\nvector over subtokens in the vocabulary. This can be conceptualized\nESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan\nTable 4: Well-performing values of model architecture hy-\nperparameters.\nHyperparameter Explanation Best value\nğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ Hidden units per layer 1024\nğ‘ğ‘ğ‘¡ğ‘¥ Code context length 1024\nğ‘‘ğ‘¥ Embedding dimension 1024\nğ‘â„ğ‘’ğ‘ğ‘‘ Attention heads 16\nDropout Dropout keep probability 0.9\nTable 5: Inference speed comparison for search scenarios\nwith different beam widths ğ‘˜ and sequence lengths ğ¿, with\ndifferent beam search setup.\nğ¿ k Sequential (ms) Parallel (ms) Cached (ms)\n10 1 250 250 220\n10 10 1700 1000 820\n25 15 7500 3000 2700\nas an ğ‘-ary tree of subtokens rooted in the last subtoken of the code\ncontext typed in by a developer. The depth of the tree is defined as\na length of the desired completion sequence. Each code sequence\nsuggestion is effectively a path on the tree, from the root node to a\nterminal node. The beam search algorithm is employed to explore\nand rank those paths, improving recommendation relevance of\ncode sequences. At every step, the results are aggregated and the\ntop k results are selected, where k is the beam width. Decoding\ncontinues for a preset number of subtokens or until a break token is\nreached. The set of break tokens includes the <EOL> (end-of-line)\ntoken as well as other language-specific tokens that often precede\nend-of-line under common code style patterns.\nA naive beam search implementation would iterate over the top\nk candidates at every step to produce the output vector. However,\nfor a sequence of length ğ¿, this would require ğ¿Ã—ğ‘˜ inference calls\nto the model, significantly increasing the inference time and degrad-\ning the overall real-time user experience. Instead, we aggregate top\nk candidates and perform batched inference calls at every decod-\ning step, which reduces the number of inference calls to ğ¿. Tab. 5\nprovides a comparison of the inference speeds for scenarios with\ndifferent beam widths and sequence lengths, quoting speed-ups\ngained through parallelization.\nGiven sequential nature of the beam search decoding, we cache\nthe attention keys and values of the transformer blocks as computed\nby the model for previous step (token), passing it as input to the\nmodel at the current step instead of recalculating from scratch. This\nfurther speeds up inference by 10%. The speed improvement with\nparallel and cached search is most apparent for large ğ¿.\n8 CLIENT-SIDE POST-PROCESSING\n8.1 Completion Caching\nDuring our user experience study, we have found that a response\ntime under 100 ms is necessary to avoid any feeling of delay or\nlag. To achieve this in a cloud-based model deployment setting, we\nintroduce caching on the client-side. Any time a developer types\na non-alphanumeric character, suggestions are queried from the\nserver. Those suggestions, each as a list of tokens along with their\nscores, are stored into a trie 5, and this trie is then placed into a\ncache. The cache key is the piece of code preceding the point where\nthe suggestion was queried. This approach allows us to prune the\ntree efficiently at a character-level as the user continues typing. To\nobtain the final completion suggestion, we simply traverse this tree\ngreedily by always branching to the node with the highest score.\nThrough experimentation, we have found that the model oc-\ncasionally returns multiple similar but equally valid suggestions.\nIn order to preserve accuracy, we terminate the completion-tree\ntraversal if none of the child nodes has a score that is equal to or\nlarger than the score of its parent multiplied by a ratio ğ‘…, defined\nas:\nğ‘… = ğ›¼\n1 +ğ‘’\nâˆ’ğ¿\nğœ…\n. (11)\nThis early-stopping allows us to break the suggestion at points\nwhere the model is equally confident in multiple valid suggestions.\nHere, ğ¿is the position of the root node of the trie,ğ›¼is the relaxation\nfactor, andğœ…is the curvature factor.ğ›¼is used to adjust the values of\nğ‘…for very small or very large values ofğ¿. A lower value ofğ›¼would\nrelax the policy producing longer completion suggestions, while\na value closer to 1.0 would tighten the policy producing shorter\nsuggestions. ğœ… controls the rate of increase of the ğ‘…. A smaller ğœ…\nwould give a steeper curve for smaller values ofğ¿, producing shorter\nsuggestions, while a larger value of ğœ… would yield a flatter curve\nresulting in longer completion suggestion. In our deployment, we\nselect ğ›¼ = 0.8 and ğœ… = 10 to gain a balance between suggestion\nlength and relevance. We have found this approach to yield the\nhighest increase in productivity, allowing software developers to\nretain a certain level of control over the suggestions. Fig. 5 shows\nan example code completion suggestion and the corresponding\ncompletion-tree.\n8.2 Suggestion Processing\nAs mentioned in section 5, we introduce several new tokens that\nare not present literally in the input code. As we decode the output\nsequences, the model would generate those new tokens at the ap-\npropriate locations. In order to incorporate those tokens fluently\ninto the completion sequences, we need to post-process them on\nthe client side into printable characters using the following rules:\n(1) <BOF> and <EOF> tokens are ignored, as they almost never\nseen in the suggestion sequence and do not provide any\nadditional information relevant to the completion sequence.\n(2) <EOL> serves as a break token for beam search decoder. We\ntruncate the completion at this token as it indicates the end\nof the line.\n(3) <STR_LIT> and <NUM_LIT> tokens become placeholders\nand are replaced by the default literals (empty string and\nnumber 0, respectively). Visual Studio Code provides the\nability to insert code snippets with placeholders in them,\nwhich the user can easily navigate through using TAB key.\nFor raw literal tokens such as <STR_LIT:__main__>, as\n5A trie is a tree-like data structure where each node is a sub-string and strings can be\ncomposed by traversing down a path from the root.\nIntelliCode Compose: Code Generation using Transformer ESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA\nFigure 5: Top: code snippet and the completion suggestion served by the model in Visual Studio Code. Bottom: completion-tree\nreconstructed by the client application, with the highlighted path shown in red (lower section of the tree is truncated to reduce\nvisual clutter).\nmentioned in section 5.2, we use the raw literal image as the\nplaceholder instead.\n9 MULTILINGUAL MODEL\nMultilingual approach allows low-resource programming languages\nto benefit from more popular languages in terms of modeling quality.\nMultilingual models also hold a promise of being easier to maintain\nand serve in production.\nTo prepare a multilingual version of the IntelliCode Compose,\nwe have extracted a shared sub-token vocabulary for Python, C#,\nJavaScript and TypeScript programming languages using the BPE\ntokenizer. We explored and compared the following four ways of\ntraining multilingual GPT-C models:\n(1) Language-agnostic baseline completely disregards the lan-\nguage type information, effectively attempting to train the\nmodel as monolingual. We have found this approach to un-\nderperform significantly as compared to the monolingual\nversions for each language.\n(2) Language-type embedding . Looking for a stronger baseline,\nwe introduced the language type embedding matrix ğ‘Šğ‘™ âˆˆ\nğ‘…ğ‘ğ‘™ğ‘ğ‘›ğ‘” Ã—ğ‘‘ğ‘¥ , combining it via addition with the token and\nposition embedding matrices of GPT-C model for each token\nin the sequences during the forward pass, given by Eq. 4,\naccording to:â„0 = ğ‘Šğ‘’Â·ğ¶+ğ‘Šğ‘+ğ‘Šğ‘™. ğ‘ğ‘™ğ‘ğ‘›ğ‘” denotes the number\nof programming languages in the training dataset.\n(3) Language specific control codes , is an approach introduced\nin [29, 30] that uses target prefixes to facilitate constrained\nlanguage modeling or multitask learning. In what follows,\nfor each programming language we insert a sequence of\ntokens in the beginning of each training sample accord-\ning to: \" lang âˆ—remaining token sequence\", where lang âˆˆ\n{Python,C#,JavaScript,TypeScript}. In expectation, control\ncodes would signal the neural network that a given sequence\nbelongs to a particular programming language. As shown\nin Tab. 6 this approach works rather well, yielding results\ncomparable with monolingual counterparts.\n(4) Finally, we add a programming language classification task\nduring model pretraining , an approach inspired by natural\nlanguage inference (NLI) RocStories and SWAG tasks [31, 32].\nAs such, the model is trained using two optimization objec-\ntives: language modeling and multiple choice classification to\nESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan\ndetect programming language. Givenğ‘ğ‘™ğ‘ğ‘›ğ‘” one-dimensional\nmonolingual arrays of subtokens.\nAs seen from Tab. 6, language specific control codes provide\nnecessary supervision to constrain language generation for each\nspecific language. The multitask approach of language modeling\ncombined with multiple choice classification provides a further im-\nprovement. As such, the double heads multilingual model, referred\nto as MultiGPT-C throughout the paper, is selected for multilingual\ndeployment candidate.\n10 EVALUATION\n10.1 Evaluation Metrics\nWe use perplexity to evaluate the quality of language model pre-\ntraining of GPT-C models, defined as:\nğ‘ƒğ‘ƒğ¿ = ğ‘’ğ‘¥ğ‘(âˆ’\nğ‘‡âˆ‘ï¸\nğ‘–\nğ‘ƒ(ğ‘¥ğ‘–)ğ‘™ğ‘œğ‘”ğ‘ƒ(ğ‘¥ğ‘–)),âˆ€ğ‘– âˆˆ0...ğ‘‡. (12)\nwhere ğ‘¥ğ‘– is the truth label and ğ‘ƒ(ğ‘¥ğ‘–)is the model output. A model\nwith lower perplexity assigns higher probabilities to the true tokens,\nand is expected to perform better.\nBesides perplexity, we consider two evaluation metrics to mea-\nsure offline performance of the code sequence completion sys-\ntem: the Recall-Oriented Understudy for Gisting Evaluation score\n(ROUGE) [33] and the Levenshtein similarity.\nThe output sequences served by the IntelliCode Compose may\nconsist of up to 20â€“30 characters, including identifier, literals, lan-\nguage keywords, punctuation marks, whitespaces, and delimiters.\nFrom our online telemetry, we have observed that the accepted sug-\ngestions are on average 25 characters long, with 25% of accepted\nsuggestions being longer than 45 characters. Accuracy could mea-\nsure correctness of the exact match, failing, however, to capture\nthe proximity when a completion suggestion partially matches the\ntarget sequence, which could still be a valid completion suggestion.\nFor instance, given model suggestion: tf.train.\nAdamOptimizer(learning_rate=<NUM_LIT>) and target\nsequence tf.train.GradientDescentOptimizer(), we\nsee that the model correctly captures the intent of software de-\nveloper to create an optimizer, suggesting the AdamOptimizer\nwith the learning rate parameter, while the target is the standard\ngradient descent optimizer, with default value of the learning rate\nparameter.\nThe ROUGE score is the metric commonly used to evaluate\nmachine translation models. Its ROUGE-L variant is based on the\nLongest Common Subsequence (LCS) [34] statistics. LCS takes into\naccount structure similarity and identifies longest co-occurring\nğ‘›-grams.\nThe Levenshtein distance measures how many single-character\nedits â€“ including insertion, substitution, or deletion â€“ does it take\nto transform one sequence of tokens to another. Quite often, even if\na suggested completion is only an approximate match, developers\nare willing to accept it, making appropriate edits afterwards. As\nsuch, the Levenshtein edit similarity is a critical evaluation metric.\nROUGE score and edit similarity capture string similarity of\ncompletion suggestions and the target code. However, not all kinds\nof imperfections in the code suggestions generated by the model\nare accepted equally by the end users. In particular, syntax errors\nare relatively easy-to-find bugs which are considered â€œsillyâ€ by soft-\nware developers. We utilize tree-sitter6 parser to estimate syntactic\ncorrectness of the completion suggestions generated by the tool,\nby parsing the file-level code context together with the suggestion\ngenerated by the model. Given the IntelliCode Compose line-of-\ncode completions may represent partial code statements, for this\nexperiment, we remove end-of-line token from the list of break\ntokens for beam search decoder, capturing the latest complete state-\nment decoded. We use the Black 7 formatter to compress multi-line\nstatements to single lines to minimize the amount of partial code\nstatements generated by the tool. We observe 93% of completion\nsuggestions in Python programming language are syntactically cor-\nrect. By visually inspecting several completion suggestions which\nhave failed to parse by tree-sitter, we conclude that roughly a half\nof those can be attributed to partially generated statements.\n10.2 Evaluation Results\nWe start by comparing our monolingual GPT-C model for Python\nprogramming language against the simple n-gram language models\nwith n = 3, 5, and 7. As seen in Tab. 7, performance of the simple\nn-gram language model is significantly lower, improving for inter-\nmediate n, then drops again for n = 7. Inspecting model suggestions\nvisually, we identified that the n-gram approach is highly sensi-\ntive to out-of-vocabulary tokens, especially for seven-gram, which\nrepresents the main limitation of this approach.\nNext, Tab. 8 provides a detailed summary of evaluation results\nfor a selected subset of well-performing models. As seen, the best\nmonolingual validation level performance in terms of edit similar-\nity and the ROUGE-L precision and recall is achieved for Python\nprogramming language. We explain it using the notion of â€œnatural-\nnessâ€ of source code introduced in [ 37]. Naturalness of code has\na strong connection with the fact that developers prefer to write\nand read code that is conventional, idiomatic, and familiar as it\nhelps understanding and maintaining software systems, leading to\ncode predictability. Python is also the most popular programming\nlanguage, according to PopularitY of Programming Language index 8,\nleading to more code adoption and reuse.\nRemarkably, multilingual model achieves a comparable perfor-\nmance in terms of edit similarity and ROUGE-L precision, but yield-\ning a a significantly lower ROUGE-L recall for C# programming\nlanguage. For JavaScript and TypeScript programming languages,\nhowever, all the metrics are improved with the multilingual model.\n10.3 Online Evaluation\nAs we roll out IntelliCode Compose internally for performance and\nuser experience evaluation, we have collected anonymous usage\ndata and telemetry. Since we are not aware of any existing code\ncompletion system that offers a similar experience, selecting the\ncorrect metric to evaluate online performance was particularly chal-\nlenging. The key online evaluation metrics that we are measuring\nare the surfacing rate (SR) and the click-through rate (CTR) over\na period of time. The SR is the total number of completions dis-\nplayed divided by the total number of times a completion could\n6https://tree-sitter.github.io/tree-sitter/\n7https://black.readthedocs.io/en/stable/\n8http://pypl.github.io/PYPL.html\nIntelliCode Compose: Code Generation using Transformer ESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA\nTable 6: Detailed evaluation results for various multilingual modeling approaches based on GPT-C. Model performance metrics\nare reported on multilingual test sample.\nModel PPL ROUGE-L Edit similarity (%) Model size\nPrecision Recall\nBaseline 2.15 0.25 0.24 56.3 374M\nLanguage Embedding 1.94 0.52 0.66 71.7 379M\nControl codes 1.73 0.64 0.75 81.5 374M\nMultiGPT-C (double heads) 1.65 0.66 0.76 82.1 374M\nTable 7: Python n-gram language model baseline perfor-\nmance comparison against the monolingual GPT-C model\npre-trained on Python programming language.\nModel ROUGE-L Edit similarity (%)\nPrecision Recall\n3-gram LM 0.16 0.28 37.8\n5-gram LM 0.40 0.45 59.7\n7-gram LM 0.34 0.34 39.2\nGPT-C 0.80 0.86 86.7\npotentially be shown, which is after every character typed into a\ncode document when the extension is active. The CTR is defined as\nthe fraction of accepted completions over the total number of com-\npletions displayed. Over 150 thousands requests, we have seen a SR\nof 9.2% and a CTR of 10%, which roughly translates to suggestions\nbeing shown every 11 characters and users committing\nThe SR is not only dependent on the accuracy of the model but\nalso on the typing speed of a user and their network reliability. The\nlow CTR can be partially attributed to the momentum in typing. As\nthe users type, it is unlikely they will stop after every keystroke to\nexamine suggestions. Similar to traditional code completion scenar-\nios, users tend to overshoot for a few characters before committing\na desired suggestion. Due to this momentum effect, the surfacing\nrate captured in our telemetry is systematically lower as compared\nto what a user may actually experience.\n11 KNOWLEDGE DISTILLATION\nKnowledge distillation [38] is the model compression technique\nin which a smaller model â€“ the student network â€“ is trained to\nreproduce the results of a larger model â€“ the teacher network. It\nhas been shown in literature [ 39, 40], that it is possible to reach\ncomparable performance on various tasks using distilled neural\nnetworks, resulting in models that are lighter and faster at inference\ntime.\nInspired by DistilBERT [40], we scale down our pretrained trans-\nformer models by reducing the number of transformer blocks, while\nkeeping the architecture of the transformer block and embedding\nlayers intact. The GPT-C model size scales near-linearly with the\nnumber of transformer blocks.\nWe experiment with student models having 8 and 12 transformer\nblocks, having our best 26 layer model serve as a teacher, initializ-\ning the student models with pretrained teacher weights and biases.\nTab. 9 summarizes the distillation results for JavaScript and Type-\nScript programming languages, comparing it to the monolingual\nteacher model trained on JavaScript and TypeScript. As seen, dis-\ntillation from 26 to 12 layers speeds up the inference by a factor\nof 2.7, incurring 6% edit similarity loss and 5% ROUGE-L precision\nloss. In a more extreme scenario, distilling a 26 layer model down\nto only 8 layers, we obtained a 4.5Ã—inference speed up at a cost of\n8% edit similarity and 9% ROUGE-L precision.\n12 MODEL DEPLOYMENT\nThe IntelliCode Compose service is designed as two-layer service:\nthe server-side model inference module and the client-side comple-\ntion provider module. The main reason for this setup is to minimize\nthe inference time for the best user experience. As we deploy the\nmodel on a cloud-based server, we have control over the hardware\nsetup and can guarantee resource availability.\nThe server-side module is deployed as a containerized web ap-\nplication to Azure Kubernetes Service 9 and listens on a HTTPS\nendpoint. It processes completion requests and returns the model\noutput. It is implemented in Python and executes model inference\nusing PyTorch and ONNX runtime10. We employ several graph-\nlevel model optimizations, including constant folding, and operator\nfusion for layer normalization and GELU sub-graphs.\nThe client-side completion provider is a Visual Studio Code\nextension implemented in TypeScript. The completion provider\nmonitors user inputs and is responsible for communicating with the\nweb service as well as post-processing model outputs as described\nin section 8.\n13 RELATED WORK\nThis work is related to a large set of literature in the area of NLP,\nand NLU, as well as deep learning and particularly transformers.\nWe refer the interested reader to the numerous publications about\ntransformer models [16, 20â€“22], and focus on code completion for\nthe remainder of this section.\nNumerous intelligent code completion systems for both statically\nand dynamically typed languages have been proposed [6, 11, 41, 42].\nBest Matching Neighbor (BMN) and statistical language models\nsuch as ğ‘›-grams, as well as RNN-based approaches leveraging se-\nquential nature of the source code have been particularly effective\nat creating such systems.\n9https://azure.microsoft.com/en-us/services/kubernetes-service/\n10https://github.com/microsoft/onnxruntime\nESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu, and Neel Sundaresan\nTable 8: Detailed evaluation results for our best-performing monolingual (GPT-C) and multilingual (MultiGPT-C) models,\nincluding the zero-shot performance of Python model on C# programming language, as well as the open source Hugging-\nFace [35] medium-sized GPT-2 checkpoint pretrained on WebText [36] dataset. Multilingual model performance metrics are\nreported separately for each of the programming languages from the training set.\nModel Test (Train) Languages PPL ROUGE-L Edit similarity (%) Model size\nPrecision Recall\nGPT-C C# (C#) 1.91 0.57 0.79 76.8 366M\nGPT-C Python (Python) 1.82 0.80 0.86 86.7 366M\nGPT-C JS,TS (JS,TS) 1.40 0.58 0.72 84.1 366M\nGPT-C, zero-shot C# (Python) â€“ 0.39 0.50 57.6 366M\nGPT-2, HuggingFace Python (WebText) â€“ 0.25 0.38 34.6 355M\nMultiGPT-C C# (C#,Python,JS,TS) 2.01 0.53 0.66 74.6 374M\nMultiGPT-C Python (C#,Python,JS,TS) 1.83 0.76 0.80 84.1 374M\nMultiGPT-C JS,TS (C#,Python,JS,TS) 1.36 0.68 0.82 87.6 374M\nTable 9: Performance of distilled monolingual models of various sizes that are trained and evaluated on JavaScript and Type-\nScript programming languages, as compared to the teacher model. The inference speed is calculated using the beam search\ndepth of ğ¿=25 and width of ğ‘˜=15.\nModel ROUGE-L Edit similarity (%) Model size Inference speed\nPrecision Recall\nDistilGPT-C, tiny 0.53 0.58 78.0 96M 600ms\nDistilGPT-C, small 0.55 0.65 79.3 124M 1000ms\nGPT-C, teacher 0.58 0.72 84.1 366M 2700ms\nAmong the models that have found practical applications in\nIDEs are that of [9] â€“ for method and API completion based on a\nneural language model and ASTs. The approach described in [10]\nreformulates code completion as a task of learning to rank the valid\ncompletion suggestions computed from static analyses in order to\nimprove computational speed and memory efficiency, effectively su-\nperseding [9]. The code completion system based on [9] is deployed\nas part of IntelliCode [17] extension in Visual Studio Code IDE,\nand [6] â€“ snippet matching based on frequency models and BMN â€“\nhas been deployed as part of Eclipse Code Recommenders [43, 44].\nClosest to our work is probably Tabnine [27], which uses GPT-\n2 to serve ranked lists of code sequence suggestions. However,\nthis tool does not attempt to complete longer sequences of 20â€“30\ncharacters long, up to a whole line of code, and we are not aware\nof any currently deployed tool that does so.\n14 CONCLUSIONS\nWe have introduced and deployed a general-purpose AI-powered\ncode completion system called IntelliCode Compose, capable of\ngenerating code sequences of arbitrary token types, including local\nvariables, methods or APIs, arguments, as well as language key-\nwords, and delimiters. IntelliCode Compose serves as a universal\nprogramming language modeling tool, effectively generating syn-\ntactically correct code in multiple programming languages, capable\nof completing a whole-line of code in a couple of key strokes.\nIntelliCode Compose is built around the GPT-C â€“ a multi-layer\ngenerative pretrained transformer model for code, which is a variant\nof the GPT-2 trained from scratch on source code data. Our best\nmodel yields an average edit similarity of 86.7% and perplexity of\n1.82 for Python programming language.\nWe have documented and overcome several practical challenges\nof training transformer neural networks on HPC clusters, model\ndeployment in the cloud, and client-side caching to meet the edit-\ntime code completion inference speed requirement of at most 100\nms per call. We have also thoroughly studied and documented the\nmultilingual modeling approaches on a dataset consisting of four\nprogramming languages.\nIn the future, we are planning to extend IntelliCode Compose\ncapabilities by focusing on completion personalization, and fine-\ntuning on custom user code. Besides code completion, we plan\nto apply large-scale unsupervised language model pretraining on\nsource code to tackle several other automated software engineering\ntasks, including automatic program repair, and code search.\nREFERENCES\n[1] Xiaodong Gu, Hongyu Zhang, Dongmei Zhang, and Sunghun Kim. Deep API\nlearning. CoRR, abs/1605.08535, 2016.\n[2] Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. Deep code search. In Proceed-\nings of the 40th International Conference on Software Engineering , ICSE â€™18, page\n933â€“944, New York, NY, USA, 2018. Association for Computing Machinery.\n[3] Song Wang, Taiyue Liu, and Lin Tan. Automatically learning semantic features for\ndefect prediction. In Proceedings of the 38th International Conference on Software\nEngineering, ICSE â€™16, page 297â€“308, New York, NY, USA, 2016. Association for\nComputing Machinery.\n[4] Richard Shin, Miltiadis Allamanis, Marc Brockschmidt, and Oleksandr Polo-\nzov. Program synthesis and semantic parsing with learned code idioms. CoRR,\nabs/1906.10816, 2019.\nIntelliCode Compose: Code Generation using Transformer ESEC/FSE â€™20, November 8â€“13, 2020, Virtual Event, USA\n[5] Uri Alon, Omer Levy, and Eran Yahav. code2seq: Generating sequences from\nstructured representations of code. CoRR, abs/1808.01400, 2018.\n[6] Marcel Bruch, Martin Monperrus, and Mira Mezini. Learning from examples\nto improve code completion systems. In Proceedings of the Joint Meeting of the\nEuropean Software Engineering Conference and the Symposium on the Foundations\nof Software Engineering (ESEC/FSE) , 2009.\n[7] Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu.\nOn the naturalness of software. In Proceedings of the International Conference on\nSoftware Engineering (ICSE) , 2012.\n[8] Tung Thanh Nguyen, Anh Tuan Nguyen, Hoan Anh Nguyen, and Tien N Nguyen.\nA statistical semantic language model for source code. In Proceedings of the Joint\nMeeting of the European Software Engineering Conference and the Symposium on\nthe Foundations of Software Engineering (ESEC/FSE) , 2013.\n[9] Alexey Svyatkovskiy, Ying Zhao, Shengyu Fu, and Neel Sundaresan. Pythia:\nAi-assisted code completion system. In Proceedings of the 25th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining , KDD â€™19, page\n2727â€“2735, New York, NY, USA, 2019. Association for Computing Machinery.\n[10] Alexey Svyatkovskiy, Sebastian Lee, Anna Hadjitofi, Maik Riechert, Juliana\nFranco, and Miltiadis Allamanis. Fast and memory-efficient neural code comple-\ntion, 2020.\n[11] Muhammad Asaduzzaman, Chanchal Roy, Kevin Schneider, and Daqing Hou.\nCscc: Simple, efficient, context sensitive code completion. Proceedings - 30th\nInternational Conference on Software Maintenance and Evolution, ICSME 2014 ,\npages 71â€“80, 12 2014.\n[12] Cheng Zhang, Juyuan Yang, Yi Zhang, Jing Fan, Xin Zhang, Jianjun Zhao, and\nPeizhao Ou. Automatic parameter recommendation for practical api usage. In\nProceedings - 34th International Conference on Software Engineering, ICSE 2012 ,\nProceedings - International Conference on Software Engineering, pages 826â€“836,\n7 2012.\n[13] Mattia Fazzini, Qi Xin, and Alessandro Orso. Automated api-usage update for\nandroid apps. In Proceedings of the 28th ACM SIGSOFT International Symposium\non Software Testing and Analysis , ISSTA 2019, page 204â€“215, New York, NY, USA,\n2019. Association for Computing Machinery.\n[14] Hui Liu, Qiurong Liu, Cristian-Alexandru Staicu, Michael Pradel, and Yue Luo.\nNomen est omen: Exploring and exploiting similarities between argument and\nparameter names. In Proceedings of the 38th International Conference on Software\nEngineering, ICSE â€™16, page 1063â€“1073, New York, NY, USA, 2016. Association\nfor Computing Machinery.\n[15] Mia Xu Chen, Benjamin N. Lee, Gagan Bansal, Yuan Cao, Shuyuan Zhang, Justin\nLu, Jackie Tsay, Yinan Wang, Andrew M. Dai, Zhifeng Chen, and et al. Gmail smart\ncompose: Real-time assisted writing. In Proceedings of the 25th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining , KDD â€™19, page\n2287â€“2295, New York, NY, USA, 2019. Association for Computing Machinery.\n[16] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. Language models are unsupervised multitask learners. 2019.\n[17] Microsoft Corporation. Ai-assisted development. https://marketplace.\nvisualstudio.com/items?itemName=VisualStudioExptTeam.VSIntelliCode. Vis-\nited Jan 2020.\n[18] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need,\n2017.\n[19] Alec Radford. Improving language understanding by generative pre-training.\n2018.\n[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-\ntraining of deep bidirectional transformers for language understanding. CoRR,\nabs/1810.04805, 2018.\n[21] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov,\nand Quoc V. Le. Xlnet: Generalized autoregressive pretraining for language\nunderstanding. CoRR, abs/1906.08237, 2019.\n[22] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly\noptimized BERT pretraining approach. CoRR, abs/1907.11692, 2019.\n[23] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization,\n2016.\n[24] David R. So, Chen Liang, and Quoc V. Le. The evolved transformer. CoRR,\nabs/1901.11117, 2019.\n[25] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,\nand Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language\nmodels using model parallelism, 2019.\n[26] Hakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and\nword classifiers: A loss framework for language modeling. CoRR, abs/1611.01462,\n2016.\n[27] Jacob Jackson. Autocompletion with deep learning. https://tabnine.com/blog/\ndeep/. Visited Sep 2019.\n[28] Alexander Sergeev and Mike Del Balso. Horovod: fast and easy distributed deep\nlearning in TensorFlow. arXiv preprint arXiv:1802.05799 , 2018.\n[29] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits\nof transfer learning with a unified text-to-text transformer. arXiv e-prints , 2019.\n[30] Nitish Shirish Keskar, Bryan McCann, Lav R. Varshney, Caiming Xiong, and\nRichard Socher. Ctrl: A conditional transformer language model for controllable\ngeneration, 2019.\n[31] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv\nBatra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. A corpus and cloze\nevaluation for deeper understanding of commonsense stories. In Proceedings of\nthe 2016 Conference of the North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies , pages 839â€“849, San Diego,\nCalifornia, June 2016. Association for Computational Linguistics.\n[32] Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. Swag: A large-scale\nadversarial dataset for grounded commonsense inference, 2018.\n[33] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In\nText Summarization Branches Out , pages 74â€“81, Barcelona, Spain, July 2004.\nAssociation for Computational Linguistics.\n[34] Chin-Yew Lin and Franz Josef Och. Automatic evaluation of machine translation\nquality using longest common subsequence and skip-bigram statistics. InProceed-\nings of the 42nd Annual Meeting of the Association for Computational Linguistics\n(ACL-04), pages 605â€“612, Barcelona, Spain, July 2004.\n[35] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,\nAnthony Moi, Pierric Cistac, Tim Rault, Râ€™emi Louf, Morgan Funtowicz, and Jamie\nBrew. Huggingfaceâ€™s transformers: State-of-the-art natural language processing.\nArXiv, abs/1910.03771, 2019.\n[36] Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.\ngithub.io/OpenWebTextCorpus, 2019.\n[37] Miltiadis Allamanis, Earl T. Barr, Premkumar T. Devanbu, and Charles A. Sutton.\nA survey of machine learning for big code and naturalness.CoRR, abs/1709.06182,\n2017.\n[38] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a\nneural network, 2015.\n[39] Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin.\nDistilling task-specific knowledge from bert into simple neural networks, 2019.\n[40] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a\ndistilled version of bert: smaller, faster, cheaper and lighter, 2019.\n[41] Veselin Raychev, Martin Vechev, and Eran Yahav. Code completion with statistical\nlanguage models. In ACM SIGPLAN Notices , volume 49, pages 419â€“428. ACM,\n2014.\n[42] Sebastian Proksch, Johannes Lerch, and Mira Mezini. Intelligent code comple-\ntion with bayesian networks. ACM Transactions on Software Engineering and\nMethodology (TOSEM) , 25(1):3, 2015.\n[43] Eclipse Foundation. Code Recommenders. www.eclipse.org/recommenders.\nVisited June 2017.\n[44] Eclipse Recommenders. Eclipse SnipMatch. http://www.eclipse.org/\nrecommenders/manual/#snipmatch, 2014. Visited Jun 2017."
}