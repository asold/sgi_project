{
    "title": "Multimodal Emotion Recognition With Transformer-Based Self Supervised Feature Fusion",
    "url": "https://openalex.org/W3088631780",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A4288622722",
            "name": "Siriwardhana, Shamane",
            "affiliations": [
                "University of Auckland"
            ]
        },
        {
            "id": "https://openalex.org/A4303663662",
            "name": "Kaluarachchi, Tharindu",
            "affiliations": [
                "University of Auckland"
            ]
        },
        {
            "id": "https://openalex.org/A2743197393",
            "name": "Billinghurst Mark",
            "affiliations": [
                "University of Auckland"
            ]
        },
        {
            "id": "https://openalex.org/A3178874258",
            "name": "Nanayakkara, Suranga",
            "affiliations": [
                "University of Auckland"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2954345346",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W6767305871",
        "https://openalex.org/W6767101625",
        "https://openalex.org/W1494198834",
        "https://openalex.org/W2842511635",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W2933138175",
        "https://openalex.org/W2769857323",
        "https://openalex.org/W343636949",
        "https://openalex.org/W2913939497",
        "https://openalex.org/W4230277160",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W2784665486",
        "https://openalex.org/W2074788634",
        "https://openalex.org/W3015489952",
        "https://openalex.org/W2775666429",
        "https://openalex.org/W2769112066",
        "https://openalex.org/W2963182768",
        "https://openalex.org/W6745709488",
        "https://openalex.org/W2914132458",
        "https://openalex.org/W3007708573",
        "https://openalex.org/W2964051877",
        "https://openalex.org/W6756244841",
        "https://openalex.org/W3014475539",
        "https://openalex.org/W6769481939",
        "https://openalex.org/W6758354414",
        "https://openalex.org/W6754988648",
        "https://openalex.org/W2973049979",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6766673545",
        "https://openalex.org/W6753277404",
        "https://openalex.org/W6755541679",
        "https://openalex.org/W2095176743",
        "https://openalex.org/W2253728219",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W6769352634",
        "https://openalex.org/W2971250720",
        "https://openalex.org/W6748726628",
        "https://openalex.org/W6719667659",
        "https://openalex.org/W6766904570",
        "https://openalex.org/W2963533390",
        "https://openalex.org/W6761937618",
        "https://openalex.org/W2985882473",
        "https://openalex.org/W2146334809",
        "https://openalex.org/W2966715458",
        "https://openalex.org/W2969958763",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4297808394",
        "https://openalex.org/W2887997593",
        "https://openalex.org/W3205498744",
        "https://openalex.org/W2943235166",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2339343773",
        "https://openalex.org/W3023371261",
        "https://openalex.org/W1595126664",
        "https://openalex.org/W3037572520",
        "https://openalex.org/W2963686995",
        "https://openalex.org/W2963103975",
        "https://openalex.org/W2883409523",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2963710346",
        "https://openalex.org/W2808631503",
        "https://openalex.org/W2980927909",
        "https://openalex.org/W2971671202",
        "https://openalex.org/W2465534249",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2898651222",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2769114491",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2963403868"
    ],
    "abstract": "Emotion Recognition is a challenging research area given its complex nature, and humans express emotional cues across various modalities such as language, facial expressions, and speech. Representation and fusion of features are the most crucial tasks in multimodal emotion recognition research. Self Supervised Learning (SSL) has become a prominent and influential research direction in representation learning, where researchers have access to pre-trained SSL models that represent different data modalities. For the first time in the literature, we represent three input modalities of text, audio (speech), and vision with features extracted from independently pre-trained SSL models in this paper. Given the high dimensional nature of SSL features, we introduce a novel Transformers and Attention-based fusion mechanism that can combine multimodal SSL features and achieve state-of-the-art results for the task of multimodal emotion recognition. We benchmark and evaluate our work to show that our model is robust and outperforms the state-of-the-art models on four datasets.",
    "full_text": "Received July 13, 2020, accepted September 18, 2020, date of publication September 25, 2020, date of current version October 7, 2020.\nDigital Object Identifier 10.1 109/ACCESS.2020.3026823\nMultimodal Emotion Recognition With\nTransformer-Based Self Supervised\nFeature Fusion\nSHAMANE SIRIWARDHANA\n1, THARINDU KALUARACHCHI\n 1, MARK BILLINGHURST2,\nAND SURANGA NANAYAKKARA1\n1Augmented Human Laboratory, Auckland Bioengineering Institute, The University of Auckland, Auckland 1010, New Zealand\n2Empathic Computing Laboratory, Auckland Bioengineering Institute, The University of Auckland, Auckland 1010, New Zealand\nCorresponding author: Shamane Siriwardhana (shamane@ahlab.org)\nThis work was supported by the Assistive Augmentation Research Grant through the Entrepreneurial Universities (EU) initiative of New\nZealand.\nABSTRACT Emotion Recognition is a challenging research area given its complex nature, and humans\nexpress emotional cues across various modalities such as language, facial expressions, and speech. Repre-\nsentation and fusion of features are the most crucial tasks in multimodal emotion recognition research. Self\nSupervised Learning (SSL) has become a prominent and inﬂuential research direction in representation\nlearning, where researchers have access to pre-trained SSL models that represent different data modalities.\nFor the ﬁrst time in the literature, we represent three input modalities of text, audio (speech), and vision with\nfeatures extracted from independently pre-trained SSL models in this paper. Given the high dimensional\nnature of SSL features, we introduce a novel Transformers and Attention-based fusion mechanism that can\ncombine multimodal SSL features and achieve state-of-the-art results for the task of multimodal emotion\nrecognition. We benchmark and evaluate our work to show that our model is robust and outperforms the\nstate-of-the-art models on four datasets.\nINDEX TERMS Multimodal emotion recognition, self-supervised learning, self-attention, transformer,\nBERT.\nI. INTRODUCTION\nMultimodal human Emotion Recognition and Sentiment\nAnalysis is an important aspect of many applications, such as\ncustomer service, health-care, and education [1]. Advances\nin Deep Learning [2] (DL) have improved the multimodal\nemotion recognition by a substantial margin [3], [4]. Two\nprimary research directions in multimodal emotion recog-\nnition are (1) how to represent raw data modalities, and\n(2) how to fuse such modalities before the prediction layer.\nA good representation of data should capture emotional\ncues that can generalize over different speakers, background\nconditions, and semantic contents. A good fusion mech-\nanism should be able to combine input modalities effec-\ntively. When it comes to representing data modalities, earlier\napproaches employed traditional emotion recognition fea-\ntures, such as Mel-frequency cepstral coefﬁcients (MFCC)\nfeatures [5], facial muscle movement features [6] and glove\nembeddings [7]. Rather than using low-level features, recent\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Kemal Polat\n.\nwork has also explored the applicability of transfer learning\ntechniques [8]–[10] to extract features from pre-trained DL\nmodels. Such work mainly focuses on extracting features\nrelated to facial expressions [11] and speech signals [12] from\nalready trained DL networks based on supervised learning\nmethods. Most of the prior work uses both low-level features\nand deep features (features extracted from pre-trained DL\nmodels) [13], [14], rather than representing all modalities\nwith deep features.\nIn contrast to previous work, we represent all input modal-\nities (audio, video, and text) with deep features extracted\nfrom pre-trained Self Supervised Learning (SSL) – a pow-\nerful representation learning technique – models [15]–[17].\nAlthough SSL features give powerful representations of the\ninput modalities, it is an extremely challenging task to fuse\nthem before the ﬁnal prediction due to the following reasons:\n1) High dimensionality of SSL embeddings\n2) Longer sequence lengths of SSL features\n3) Mismatch between sizes and sequence lengths of SSL\nfeatures across modalities that have extracted from dif-\nferent SSL models\n176274 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/VOLUME 8, 2020\nS. Siriwardhanaet al.: Multimodal Emotion Recognition With Transformer-Based Self Supervised Feature Fusion\nFIGURE 1. Overview of the Self Supervised Embedding Fusion Transformer (SSE-FT). The proposed fusion mechanism takes SSL embeddings from the\nthree modalities as the inputs. First, speech and video modalities are modified by prepending a uniqueCLS token, followed by a Self-Attention\nTransformer. TheseCLS tokens for each modality can aggregate the information in the entire sequence (Refer to Section III-B). Then, the information\nacross modalities is extracted with six Inter-Modality-Attention (IMA) based Transformer blocks. Each IMA Transformer takes theCLS token from one\nmodality and the entire embedding sequence of other modality as the inputs. For example, the notationSpeech → Text denotes that theCLS token\ncomes from the speech modality, while the other embedding sequence comes from the text modality. This can be recognized as attending from speech\nto text. Each IMA Transformer outputs aCLS token enriched with embedding-level information gained from the other modality (Refer to Section III-C).\nHadamard product is applied toCLS token pairs that belong to the same modality to extract essential features before the concatenation layer (Refer to\nSection III-D). Finally, the concatenated vectors go through a fully connected layer to produce the output probabilities.\nAlthough a simple concatenation seems like a viable\noption, additional trainable parameters required to fully\nconnect the high dimensional SSL embeddings make the\nnetwork prone to overﬁtting. Considering these prob-\nlems, we propose a reliable and effective SSL fea-\nture fusion mechanism based on the core concepts of\nSelf-Attention [18] and Transformers [18]–[20]. We used\nthree publicly available pre-trained SSL models of\nRoBERTa [19] to represent text, Wav2Vec [17] to rep-\nresent speech and FAb-net [16] to represent facial\nexpressions.\nWe introduce our novel fusion mechanism as Self Super-\nvised Embedding Fusion Transformer (SSE-FT). As depicted\nin Figure 1, our framework mainly consists of two Self\nAttention-based Transformers and six Inter-modality Atten-\ntion (IMA) based Transformers blocks (refer to Section III).\nFirst, two Self Attention-based Transformers modify the\nspeech and video SSL embeddings. This modiﬁcation step\nadds a special token named CLS to both speech and video\nsequences that can aggregate the information embedded in the\nentire sequence. We did not modify the text SSL embedding\nsequence since they get extracted from the Transformer based\nVOLUME 8, 2020 176275\nS. Siriwardhanaet al.: Multimodal Emotion Recognition With Transformer-Based Self Supervised Feature Fusion\nmodel where the embedding sequence already contains a CLS\ntoken. Then, all three SSL embedding sequences pass through\nsix IMA based Transformers that enrich each modality’s\nsequence representation with useful information from other\nmodality. In this step, we speciﬁcally use CLS tokens related\nto each modality. Finally, we introduce a Hadamard product\nbased computation to compute the most important feature in\neach modality. In summary, our main contributions are as\nfollows1:\n• Use of tri-modal SSL features extracted from three inde-\npendently pre-trained SSL architectures in multimodal\nemotion recognition.\n• Introduction of a novel Transformers based fusion mech-\nanism that fuses SSL features having arbitrary embed-\ndings, sizes, and sequence lengths.\n• Evaluation and comparison of robustness and general-\nizability of our model on four publicly available multi-\nmodal datasets.\n• Conduct a series of ablation studies to understand the\neffect of each major component in the architecture.\nII. BACKGROUND AND RELATED WORK\nIn this section, we cover background and closely related work\nto our research. First, we give a brief introduction to feature\nextraction mechanisms used in multimodal emotion recogni-\ntion. Then, we summarize the theory of SSL and explain the\nthree pre-trained SSL models used in this research. Finally,\nwe highlight closely related work on multimodal fusion.\nA. FEATURE EXTRACTION MECHANISMS\nIn multimodal emotion recognition settings, most of the\nprior work [11] use a mix of low-level and deep features.\nThis section gives an overview of different feature extraction\nmechanisms used in prior work.\n1) LOW-LEVEL FEATURE EXTRACTION MECHANISMS\nUsually, multimodal emotion recognition algorithms con-\nsist of a feature extraction mechanism and fusion meth-\nods [21]. Previous work have discussed several feature extrac-\ntion mechanisms for commonly used data modalities, such as\naudio, video, and text. MFCC [22] and COV AREP [5] can be\nidentiﬁed as typical speech feature extraction mechanisms.\nWord-to-Vector methods such as Skip-gram and Glove [7]\nare usual examples for text features. There are direct tools\nlike FACET [23] for facial feature extraction to understand\nemotions.\n2) DEEP FEATURE EXTRACTION MECHANISMS\nFeatures extracted from pre-trained DL models are known as\ndeep features. Usually, such DL models ﬁrst get trained with\none or more large supervised datasets. The previous work [9]\nhave used pre-trained facial recognition networks to extract\nfacial features. Previous work has also [24], [25] used the\n1Pytorch implementation available at https://github.com/shamanez/\nSelf-Supervised-Embedding-Fusion-Transformer\npre-trained speech to text models to extract speech features\nfor the sentiment analysis task. Such work highlights that\nthe deep features extracted from input modalities performed\nbetter compared to low-level features.\nB. MULTIMODAL FEATURES EXTRACTED FROM\nPRE-TRAINED-SSL ALGORITHMS\nFeatures extracted from pre-trained SSL models are known\nas Self Supervised Embeddings (SSE). SSL has become a\nprominent representation learning paradigm in both Natural\nLanguage Processing (NLP) and Computer Vision (CV) com-\nmunities [15], [20], [26]. SSL algorithms have two stages.\nThe ﬁrst stage is known as pre-training, while the second\nstage uses pre-trained SSL models to extract features for\ndownstream tasks. The pre-training stage utilizes a given\nset of pretext tasks and a large number of unlabelled data.\nSuch pretext tasks use regularities and connections in existing\ndata to design a supervisory signal. Tasks like determining\nimage rotation [27], [28], ﬁnding missing words in a sen-\ntence [19], [20] can be stated as examples for pretext tasks\nused in NLP and CV domains. The features generated from\npre-trained SSL algorithms have problem agnostic qualities\nsince they have not been trained with problem speciﬁc man-\nual labels [29].\nRecent literature describes how bigger SSL models consist\nof billions of parameters like GPT-2, GPT-3 [30]–[32] that\noutperform baseline models in different NLP tasks. However,\ntraining such models from scratch is a very computationally\nexpensive task. Therefore, we highlight the importance of\nexploiting the pre-trained versions of publicly available SSL\nmodels as feature extractors for multimodal raw data streams.\nIn our research, we use three publicly available pre-trained\nSSL models to extract features. Previous work [13], [14] have\nused SSL features extracted from BERT [20] to represent text\nmodality in multimodal emotion recognition. To the best of\nour knowledge, this is the ﬁrst time two or more pre-trained\nSSL models are used to extract features in multimodal emo-\ntion recognition.\nC. SUMMARY OF SSL MODELS USED IN MULTIMODAL\nFEATURE EXTRACTION\nWe use three pre-trained SSL models in this research. All the\nmodel checkpoints were taken from publicly available repos-\nitories. We did not ﬁne-tune any SSL model with multimodal\nemotion recognition datasets. Features for each data modality\nwere extracted from frozen SSL models.\n1) RoBERTa\nRoBERTa [19] is an extension of the BERT [20] model, which\nhas shown competitive results in GLUE language modelling\ntasks [33]. The main difference between RoBERTa and BERT\nis the training mechanism. RoBERTa does not use the next\nsentence prediction task. We use the pre-trained RoBERTa\nfrom the open-sourced fairseq toolkit [34]. The model con-\nsists of 355M parameters and has been pre-trained on large\nEnglish-language text datasets [35]. The network architecture\n176276 VOLUME 8, 2020\nS. Siriwardhanaet al.: Multimodal Emotion Recognition With Transformer-Based Self Supervised Feature Fusion\nis similar to BERT, which contains a 24-layer transformer\nencoder. We input tokenized raw text to the model and use\nthe output from the ﬁnal layer as the feature representation.\nRoBERTa can handle large tokenized sentences with a maxi-\nmum length of 512 words where each sentence is mapped to\nan embedding of 1024 ﬂoating points.\n2) Wav2Vec\nThe architecture of the Wav2vec [17] has been developed on\nlayers of temporal convolution, and the pretext task used for\nSelf Supervised Training leverages the concept of Contrastive\nPredictive Coding [36]. As authors in wav2Vec suggested,\nthe context representation C can be used as an embedding\nto represent the raw audio waveform. The authors set the size\nof the embedding to 512 and the maximum audio waveform\nlength to 9.5 seconds. The network consisted of 35M parame-\nters and was pre-trained on 960 hours of audio taken from the\nLibrispeech dataset [37]. The pre-trained model checkpoint\nwas downloaded from the Fairseq repository [34].\n3) FABNET\nWe used the pre-trained Fabnet [16] model to obtain embed-\ndings for each frame in the video that contained the speaker’s\nface. The pretext task of Fabnet is specially designed to\nencourage the network to learn the facial attributes that\nencodes the landmarks, pose, and emotions. Given only the\nembeddings correspond to the source and target frames,\nthe network is asked to map the source frame to the target\nframe by predicting a ﬂow ﬁeld between them, thus forcing\nthe network to understand the offset that should occur in the\nsource image pixels to obtain the target image. This proxy\ntask forces the network to distil the information required to\ncompute the ﬂow ﬁeld (e.g. the head pose and expression)\ninto the source and target embeddings. The source and target\nframes are taken from the same face-track of a person with\nthe same identity but with different expressions/poses. The\nnetwork is pre-trained on two large datasets from voxceleb\ndatasets [38]. The embedding dimension is 256. We use this\nnetwork to obtain the representation for each video frame.\nD. MULTIMODAL FEATURE FUSION MECHANISMS\nA wide range of previous work uses Convolutional Neural\nNetworks (CNN), and LSTM based DL models as fusion\nmechanisms [3], [39]. Recent work has explored the effec-\ntiveness of novel DL architectures like Transformers [40]\nand Graph Convolution Nets [41] as fusion methods. When\ncomparing sequential deep learning architectures similar to\nLSTM and RNN, recent work highlights both computa-\ntional efﬁciency and effectiveness of Transformer [18] based\nmethods. In contrast to our work, all these methods work\nwith low-level features. There is prior work using BERT-\nbased [20] SSL features for text, while other modalities are\nrepresented with low-level features. These works discuss\nfusion mechanisms based on RNN and Self Attention mech-\nanisms [13]. To the best of our knowledge, this is the ﬁrst\ntime that a fusion mechanism is proposed when representing\nall three modalities with SSL features. Since SSL features\nhave high dimensional embeddings, larger sequence sizes,\nand different sequence lengths and embedding dimensions in\nbetween modalities, we designed a Transformer based fusion\nmechanism that is both efﬁcient and more accurate than the\nprevious state-of-the-art.\nIII. METHODOLOGY\nIn this section, we present each component of our novel\nfusion mechanism which is Self Supervised Embedding\nFusion Transformer (SSE-FT). First, we describe the feature\nextraction process that uses pretrained SSL models. Next,\nwe explain the core concept behind the modiﬁcation of\nspeech and video SSL embeddings. After that, we introduce\nour cross model fusion method that builds upon the idea\nof Inter-Modality-Attention (IMA). Finally, we explain the\nHadarmard computation based feature selection.\nA. SELF SUPERVISED EMBEDDING EXTRACTION\nAs the ﬁrst step, we extracted features from raw data\nmodalities using three pre-trained SSL models described in\nSection II-C. As depicted in the Table 1, dimensions and max-\nimum training sequence lengths of SSL features vary across\neach modality. Both pre-trained models of RoBERTa [19] and\nWav2Vec [17] were accessed from Fairseq code-base [34]\nand used to extract text and speech SSL features. To download\nthe pre-trained Fabnet model and extract features for video\nmodality, we referred to their publication [16]. To extract\nfeatures from videos, we cropped faces from each video\nframe using Retina-Face [42] facial recognition model. Then,\nwe sent each frame that consist of a face through the\npre-trained Fabnet model to obtain features of the video\nmodality.\nTABLE 1. Statistics of the embedding extracted from pre-trained SSL\nmodels.\nB. MODIFICATION OF SSL EMBEDDING\nFigure 2 consists of two Transformer blocks that illustrates\nthe process of speech and video embedding sequence mod-\niﬁcation. Features extracted from SSL models have large\nembedding size and long sequence length. We wanted to\ndevelop a mechanism where a single embedding can rep-\nresent a long embedding sequence related to a modality.\nTo achieve this, we modify both Wav2Vec embeddings\n(A) and Fabnet embeddings (V) by prepending a trainable\nvector named CLS and apply Self Attention to each embed-\nding sequence as depicted in Equation 1. The Self Atten-\ntion mechanism in Equation 2 works similar to the original\nVOLUME 8, 2020 176277\nS. Siriwardhanaet al.: Multimodal Emotion Recognition With Transformer-Based Self Supervised Feature Fusion\nFIGURE 2. Modification of Wav2Vec and Fabnet SSL embeddings with Transformer\nblocks. First, each embedding sequence for speech and text modalities are modified\nby prepending unique tokens ofCLSspeech and CLSvideo. Then, each of the modified\nembedding sequences goes though two separate Self Attention-based Transformers\nblocks.\nTransformer mechanism [18]. In Equation 2, symbols Q, K,\nV , anddQ refers to Query, Key, Value, and dimensionality of\nQuery vector, respectively.\nVembeddings\n=Self Attention[prepend\n(\n[CLS]V , Fabnetseq\n)\n]\nAembeddings\n=Self Attention[prepend\n(\n[CLS]A, Wav2Vecseq\n)\n] (1)\nSelf Attention\n=softmax\n(\nQKT\n√dQ\n)\nV (2)\nIn our embedding sequence modiﬁcation phase, we drew\ninspiration from how BERT [20] or RoBERTa [19] models\nrepresent an entire sequence using the ﬁrst unique token\ncalled CLS (stands for classiﬁcation). Since the Self Atten-\ntion mechanism in BERT-based models is bidirectional (past\nand future), the CLS token, which is the ﬁrst token of the\nsequence, is encoded with all information to its right, which is\nthe future sequence. Therefore, the CLS token can be used as\na compressed representation to solve classiﬁcation problems\nlike sentiment analysis. In our model, we only prepended\nCLS tokens to Wav2Vec and FabNet embeddings sequences\nbecause they do not follow a similar architecture to BERT.\nSince RoBERTa is a BERT based model, we used the text\nembedding sequence as it is. Having access to three CLS\ntokens representing three modalities helped us efﬁciently\ncompute IMA and design a straight forward late fusion\nmechanism.\nC. INTER-MODALITY-ATTENTION (IMA) BASED\nFUSION LAYER\nFigure 3 which consists of six Transformer blocks, illustrates\nthe functionality of the IMA based fusion layer. The primary\npurpose of the IMA fusion layer is to share relevant informa-\ntion across modalities. The IMA fusion layer was designed to\nembed the representation of one modality with information\ngained from representations of other modalities. The IMA\nlayer works similarly to the Self Attention in Equation 2,\nexcept it creates the Query (Q) vector from the CLS token\nof one modality and Key (K) -Value (V) vectors from the\nembedding sequence of the other modality.\nThe IMA fusion layer’s inputs consist of three embedding\nsequences where the ﬁrst token of each embeddings sequence\nis the CLS token. Since the CLS token of each modality\naggregates the sequence’s information, the IMA attention is\ncomputed between the CLS token of one modality and the\nentire embedding sequence of the other modality. This way,\nthere are six IMA Transformer blocks, where each Trans-\nformer block’s Q vector is calculated from one modality’s\nCLS token, and K-V vectors are calculated from another\nmodality’s entire embedding sequence.\nD. APPLICATION OF HADARMARD PRODUCT PRIOR TO\nTHE PREDICTION LAYER\nAs the next step, we explored the possible ways of combining\nthem prior to the prediction layer. The concatenation of six\ntokens seems to be the obvious way to combine information.\nHowever, in our work, we further simplify the CLS tokens\n176278 VOLUME 8, 2020\nS. Siriwardhanaet al.: Multimodal Emotion Recognition With Transformer-Based Self Supervised Feature Fusion\nFIGURE 3. Extended view of IMA fusion layer. The fusion mechanism consists of six Inter-Modality-Attention (IMA) based Transformer blocks. Each of\nthe Transformer blocks enables aCLS token from one modality to attend to the entire embedding sequence of other modality and gather vital\ncross-modal information. The fusion process finally outputs sixCLS tokens that are enriched with inter-modality information.\nbefore concatenation. As illustrated in Figure 1, the six CLS\nembeddings can be grouped into three pairs considering their\ncore-modality (modality of the Q vector in the IMA com-\nputation). Finally, to extract the essential information that\nstems from one modality, we take the Hadamard product (⊙)\nbetween CLS tokens pairs of the same Core-Modality. Equa-\ntion 3 illustrates the computation of the Hadamard product\nbetween six CLS tokens that are computed by the IMA layer\n(Figure 3). Symbols v, a, and t are used to represent video,\nspeech (audio), and text modalities, respectively. vﬁnal, aﬁnal,\nand tﬁnal are the three resultant vectors after computing the\nHadamard product between IMA fusion layer outputs that\nbelong to the same core-modality. Intuitively, the Hadamard\nproduct is used to extract the mutual information among\nthe two CLS representations. Prior work [43] also has high-\nlighted the effectiveness of using the Hadamard computa-\ntion to enrich the information in BERT embeddings. Finally,\nafter the Hadamard computation, we concatenate the ﬁnal\nthree representations and send them through a prediction\nlayer. We compare the use of Hadamard computation based\nfusion with 6-vector concatenation in our ablation study (see\nSection VI-E) and empirically show that the proposed method\nworks better.\nvﬁnal =[CLS]video→speech ⊙[CLS]video→text\naﬁnal =[CLS]speech→video ⊙[CLS]speech→text\ntﬁnal =[CLS]text→video ⊙[CLS]text→speech\nFinalfusion =concatenation\n(\nvﬁnal, aﬁnal, tﬁnal\n)\n(3)\nVOLUME 8, 2020 176279\nS. Siriwardhanaet al.: Multimodal Emotion Recognition With Transformer-Based Self Supervised Feature Fusion\nTABLE 2. Hyperparameter tuning. Hyperparameters of SSE-FT that we use for the various tasks. The ‘‘# of Attention Blocks. ’’ and ‘‘# IMA Heads’’ are for\neach transformer. We performed a basic grid search for hyperparameters, such as the dropout rate, the number of transformer blocks, and attention\nheads.\nE. SUMMARY OF THE FUSION METHOD\nIn summary, our fusion method consists of three major parts.\nFirst, we modify speech and video embedding sequences\nby adding two trainable CLS embeddings and send them\nthrough two different Self Attention based Transformers.\nAfter the embedding modiﬁcation step, we send the embed-\ndings through six Transformer blocks that consist of IMA\nto capture cross-modality information. Finally, we computed\nthe Hadamard product between CLS tokens related to the\nsame modality to enrich the token with the most relevant\ninformation. We use only three modalities in our experiments,\nbut this method can be easily extended when using more than\nthree modalities of SSL features.\nIV. IMPLEMENTATION DETAILS\nThis section presents the details of the model implementation\nand the experimental setup. We implemented our model with\nFairseq2 [34] a sequential data processing framework which\nis built on Pytorch3 DL framework. The training was con-\nducted in distributed GPU settings using two 1080 NVIDIA\nTitan GPUs. Details of the ﬁnal hyperparameters are illus-\ntrated in the Table 2.\nV. EVALUATION DATASETS\nTo proceed with our experiments, we used four publicly\navailable datasets. All these datasets consist of speech, text,\nand video modalities. We compare our proposed method\nwith the state-of-the-art results published for each dataset,\nas described in the Results Section VI. It is important to note\nthe variations present in the evaluation metrics used by prior\nwork for different datasets. Thus, to have a fair evaluation of\nour model’s performance, we followed the same evaluation\nprocedure many prior works used [3], [4], [4], [40], which\nshowed the state-of-the-art results for each dataset. We used\nAccuracy, F1-score, Mean Average Error, and Correlation\nCoefﬁcient as the main evaluation metrics. A summary of the\nstatistics of all datasets used can be found in Table 3. Both\nIEMOCAP [44] and MELD [3] datasets were annotated with\ncommon categorical emotion classes such as Happy, Sad,\nAngry, Neutral, and Excitement. Both CMU-MOSI [45] and\nCMU-MOSEI [4] are annotated with sentiment scores that\nvaries between −3 to +3. In Section VI, we further explain\n2https://github.com/pytorch/fairseq\n3https://pytorch.org\nTABLE 3. Summary of the datasets statistics. The number of training,\nvalidation, and testing examples in different datasets that we used for\nconduct experiments.\nthe datasets and evaluation metrics used in the evaluation\nprocess.\nVI. RESULTS\nIn this section, we ﬁrst explain the experiments conducted to\nevaluate our model’s performance on four datasets and then\npresent the ablation studies conducted to understand the func-\ntionality of our proposed model. This work aims to design\nan effective fusion mechanism when representing all input\nfeature modalities with SSL features. Mainly, we focused on\ndesigning a fusion mechanism that can be easily extended\nwith SSL features of several modalities. We also wanted\nto highlight the effectiveness of Self Supervised features in\nthe task of multimodal emotion recognition. For compari-\nson and evaluation purposes, we mainly used MuLT [40] as\nthe closest multimodal fusion mechanism to our proposed\nmethod. Although MuLT [40] uses Transformer based fusion\nmechanism, their work has not focused on using SSL fea-\ntures, which allows us to highlight the effectiveness of SSL\nfeatures. Our Transformer based fusion mechanism consists\nof unique components such as embeddings modiﬁcation with\nCLS token, modality-speciﬁc CLS token-based IMA, and\nHadamard computation to extract information. These compo-\nnents were explicitly introduced to work with SSL features.\nAt the time of writing, we did not ﬁnd similar work that\nspeciﬁcally focuses on using high dimensional SSL features\nto represent all modalities.\nA. IEMOCAP EXPERIMENTS\nThe IEMOCAP [44] dataset contains conversation data\nof 10 male and female actors collected in 5 sessions, with\neach session consisting of 2 unique actors. The data is seg-\nmented by utterances, where each utterance is transcribed and\n176280 VOLUME 8, 2020\nS. Siriwardhanaet al.: Multimodal Emotion Recognition With Transformer-Based Self Supervised Feature Fusion\nTABLE 4. Results of multimodal emotion analysis on IEMOCAP with non-aligned multimodal sequences. We report Binary Accuracy (BA) ad F1 score for\neach emotion. Performances of other models are taken from the MULT [40].\nTABLE 5. Results of multimodal emotion analysis on CMU-MOSEI with non-aligned multimodal sequences. We report Seven Class accuracy, BA (binary\naccuracy) and F1 score (*for all the scores up to here, the higher the better), MAE (Mean-absolute Error, the lower the better), and Corr (Pearson\nCorrelation Coefficient, the higher the better). Performance’s of other models are taken from the MULT [40].\nannotated. Labels are chosen from emotion classes of Anger,\nHappy, Sad, Neutral, Excitement, Frustration, Fear, Surprise,\nand others. Since the dataset is not uniformly distributed\namong the classes, we followed prior work [4], [40], and only\nused the four most frequent labels, which are Happy, Sad,\nAnger, and Excitement.\nTo provide a fair evaluation, we followed prior work [4],\n[40] when designing the ﬁnal step of the output layer of our\nmodel (to compute the binary accuracy for each emotion).\nFor each utterance, the algorithm predicts the availability of\neach emotion. We split the dataset into training and testing\nby taking examples from the ﬁrst four sessions for training\nand the last session for testing. Therefore, the training and\ntesting datasets have data from 8 and 2 different actors respec-\ntively. This way of splitting also allows us to evaluate the\nalgorithm in speaker-independent settings, which is essential\nfor real-world scenarios. Table 4 shows the superior accuracy\nand the f1 score for each emotion compared to previous work.\nB. CMU-MOSEI EXPERIMENTS\nFor multimodal language analysis that consists of\n22000 examples with each having relevant audio (speech),\nvideo, and text input streams. The dataset is mainly used to\nanalyze sentiment, and the dataset is created by extracting\nvideos from YouTube, with three people annotating each\nexample to reduce the bias. Unlike annotations in other\ndatasets, which consist of discrete emotions, such as happy\nand sad, this data set is annotated by assigning a senti-\nment score to each example that varies from −3 to +3,\nwhere −3 corresponds to extremely negative sentiment and\n+3 means extremely positive. To evaluate our model with the\nCMU-MOSEI dataset, we followed the latest prior work [40],\nwhich uses seven class accuracy and binary accuracy to\nevaluate their models. Unlike a usual classiﬁcation task,\nhere, the model is trained on predicting the sentiment score\nby minimizing the mean absolute loss (L1 loss). Once the\nalgorithms are trained, the predicted score is rounded off to\nthe nearest integer in the integer set −3 to +3, which classi-\nﬁes the data into seven classes. The binary accuracy is then\ncalculated using zero as the threshold for the sentiment score\n(following the prior work [4], [40], label zero was removed\nwhen evaluating the binary accuracy). Similar to previous\nwork [4], [40], we used labels and dataset splits provided in\nthe CMU-SDK [46]. Table 5 shows the performance of our\nmodel compared to the state-of-the-art models. As the results\nsuggest, our model outperforms state-of-the art models on\neach evaluation metric by a fair margin.\nC. CMU-MOSI EXPERIMENTS\nCMU-MOSI [45] multimodal sentiment analysis dataset is\nsimilar to the CMU-MOSEI [4] dataset in all aspects, except\nfor the number of examples. It consists of 2200 examples\nof YouTube movie reviews. Similar to MOSEI, we used the\nlabels and dataset splits provided in CMU-SDK. Table 6\nshows the comparison of the performance of our model with\nrecently published work. Although the CMU-MOSI dataset\nconsists of fewer training examples than other datasets, our\nmodel can still outperform prior work by a fair margin.\nD. MELD EXPERIMENTS\nMELD [3] dataset has more than 12000 utterances from\nFriends television series. In contrast to other datasets, MELD\nis a conversational dataset that has some examples with sev-\neral actors in a single utterance. Each utterance is annotated,\nselecting from one of the seven emotion classes: Anger,\nDisgust, Sadness, Joy, Surprise, Fear, and Neutral. To give\na fair comparison, we provide the seven class accuracy of our\nmodel classiﬁed using a Softmax layer. Even though we used\nMuLT [40] as our closest benchmark for all other datasets,\nfor MELD [3] dataset, we could not ﬁnd the performance\nVOLUME 8, 2020 176281\nS. Siriwardhanaet al.: Multimodal Emotion Recognition With Transformer-Based Self Supervised Feature Fusion\nTABLE 6. Results of multimodal emotion analysis on CMU-MOSI with non-aligned multimodal sequences. We report Seven Class accuracy, Binary\nAccuracy (BA), F1 score, MAE (Mean-absolute Error), and Corr (Pearson Correlation Coefficient). Performances of other models are taken from the\nMULT [40].\nTABLE 7. Results of multimodal emotion analysis on MELD with\nnon-aligned multimodal sequences. We report Seven Class weighted\naccuracy and the F1 score. Performances of other models are taken from\nthe QIN [47].\nof MuLT [40] to compare due to the unavailability of the\nlow-level features. Table 7 shows the comparison and superi-\nority of our model with other recent evaluations on the MELD\ndataset for seven class emotion recognition accuracy.\nE. ABLATION STUDIES\nAs depicted in Table 8, we conducted a series of ablation\nstudies using the CMU-MOSEI [4] dataset to understand\nthe inﬂuence of different components in the proposed fusion\nmechanism. We selected CMU-MOSEI because it has the\nhighest number of training examples, compared to other\ndatasets. Mainly there are three types of ablation studies\ndescribed as follows:\n• Ablation study on speech, text, and video input\nmodalities.\n• Ablation study on the use of IMA layers (Pre-IMA layer)\n• Ablation study on the use of Hadamard product\n(Post-IMA layer)\n1) UNIMODAL INPUT\nAs illustrated in the ﬁrst part of the Table 8 (Unimodal\nTransformers), we checked the impact of each input modality\non the ﬁnal accuracy. In the unimodal experiments, the CLS\ntoken extracted after the Self Attention Transformer was\ntaken as the ﬁnal representation (only for speech and video\nmodalities). The unimodal results highlight the signiﬁcance\nof text features. Text modality gives 80.2% of binary senti-\nment accuracy and 47.7% for 7-class sentiment accuracy. The\nmodel with only speech modality shows 67.5% and 43.8%\nfor binary and seven-class accuracy. Finally, the model with\nonly video modality shows 66.3% and 43.6% for binary and\nseven-class accuracies.\nIt is essential to highlight that the model with only text\nmodality performs signiﬁcantly better than other modali-\nties. A possible reason could be the power of the RoBERTa\nembeddings compared to other SSL feature modalities.\nWe compare the results of the unimodal ablation study with\nrecorded MuLT [40] ablation study results that show a sim-\nilar trend. Since CMU-MOSEI dataset is collected from\nreal-world YouTube review videos examples, most of the\nsentiment contents can be understood with the text modality.\n2) DUAL-MODAL INPUTS\nNext, we conducted experiments to understand the model’s\nperformances with dual-modal inputs. In this experiment,\nwe used the two CLS tokens after the IMA fusion layer as\nthe ﬁnal representation. As illustrated in Table 8 (dual-modal\nsection), a model that takes text and speech as inputs gives the\nbest results of 54.1% of seven class sentiment accuracy and\n86% binary sentiment accuracy. The model that takes speech\nand video gives the lowest results of 44.18% for seven class\nsentiment accuracy and 68.2% for binary sentiment accuracy.\nDual modality results also highlight the high informative\nnature of text modality concerning CMU-MOSI dataset.\n3) PRE-IMA LAYER\nIn this study, we compared the model’s performance with-\nout the IMA block under the tri-modal setting. We mainly\nwanted to explore the cause of the improvement by the\nsix IMA fusion Transformer blocks. We used three CLS\ntokens extracted from three embedding sequences prior to the\nIMA layer. The CLS tokens for speech and video modality\nextracted after Self Attention blocks. The CLS token for text\nsequence was directly extracted from RoBERTa embeddings.\nFinally, we concatenated three vectors before sending them\nthrough the prediction layer. This setting achieves an accu-\nracy of 47.5% for seven class sentiment and 81.9% for binary\naccuracy. When compared with the best performing model,\nwhich gives 55.5% accuracy for seven class sentiments clas-\nsiﬁcation and 87.3% for binary sentiment classiﬁcation, thus\nhighlighting the effectiveness of the proposed IMA fusion\nTransformers.\n4) POST-IMA LAYER\nIn this ablation study, we explored the effectiveness of our\nproposed Hadarmad product based fusion mechanism that\ncomes after the IMA fusion. In our ﬁnal model, we extract\nCLS tokens from all six IMA Transformers. Then, we com-\nputed the Hadamard product between CLS tokens that belong\nto the same modality. In this experiment, we did not apply\n176282 VOLUME 8, 2020\nS. Siriwardhanaet al.: Multimodal Emotion Recognition With Transformer-Based Self Supervised Feature Fusion\nTABLE 8. Evaluation results of the ablation studies. SSE-FT ablation studies on CMU-MOSI dataset in terms of the significance of individual modalities\n(Unimodal), combinations of two modalities (Dual-modal) pre-IMA fusion mechanism and psot-IMA fusion mechanism. L, A, V denote language (text),\naudio (speech), and visual. The notation of (h) means higher the better and the notation of (l) means lower the better.\nthe Hadarmad product to the CLS tokens but concatenated all\nsix CLS tokens and sent it to the ﬁnal prediction layer. For\nthis experiment, the model achieves 53.3% for seven class\nsentiment accuracy and 84.6% for binary sentiment accuracy.\nThe comparison of results with our ﬁnal model highlights\nthe effectiveness of the Hadamard product-based information\nextraction. As the ﬁnal result suggests, the use of Hadamard\ncomputation improves both binary and seven-class sentiment\naccuracy by nearly 3% while reducing the number of trainable\nparameters since pure concatenation of all six vectors adds\nthree times more parameters to the ﬁnal prediction layer.\nVII. DISCUSSION\nTo the best of our knowledge, there is no prior work that uses\nSSL features to represent all three input modalities. Recent\nwork have only used SSL features to represent text modality\nwith pre-trained BERT features [48]. These work mainly use\ncommon DL architectures like CNN and LSTM. For the ﬁrst\ntime in the literature, we comprehensively explore the repre-\nsentation of all modalities with SSL features, overcoming the\nchallenge of the high dimensional nature of SSL features.\nSSL has become a popular research area and already shown\nimprovements in NLP and CV . Since the SSL paradigm can\nuse widely available unlabelled data, an increasing number\nof pre-trained SSL models for different data streams are\nbeing open-sourced to the research community. Usually, these\nmodels have different architectures, and they are trained\nindependently with different pretext tasks. Therefore, in this\nresearch, we highlight the importance of introducing effective\nand reliable fusion mechanisms that can be used to fuse\nmultimodal SSL features.\nOur proposed fusion mechanism mainly uses two\nSelf-Attention based Transformers and six IMA based Trans-\nformers. The fusion mechanism was mainly designed to work\nwith discrete sequences of SSL embeddings while carefully\nconsidering differences in SSL embeddings generated from\ndifferent pre-trained architectures. Due to these reasons,\nwe can easily extend this proposed mechanism with more\nor new SSL features extracted from different pre-trained\nmodels. The experiments conducted with four publicly avail-\nable datasets highlight the ability of SSL features to provide\nbetter results for the task of multimodal emotion recognition.\nThen, the results for ablation studies show the state-of-the-art\nperformance of our proposed fusion mechanism.\nVIII. CONCLUSION AND FUTURE WORK\nIn this work, we focused on using pre-trained SSL models\nas feature extractors to improve the task of emotion recogni-\ntion. To achieve our goal, we designed a transformer-based\nmultimodal fusion mechanism that has the ability to per-\nform well by understanding inter-modality connections.\nWe ﬁrst evaluated our model with strong baselines from four\nwell-established multimodal affective datasets and demon-\nstrated that our method can outperform previous state-of-the-\nart methods. Next, we conducted strong ablation studies to\nunderstand the important components in our fusion mecha-\nnism. It is important to have a stable and well-investigated\nfusion mechanism when using SSL features as inputs the\nfeatures generated from SSL techniques are generally high\ndimensional and can be considered as high-level features. The\nresults suggest that we can effectively use SSL features from\ndifferent pretrained models to solve the task of multimodal\nemotion recognition. The use of SSL algorithms allows us to\nleverage the potential within largely available unsupervised\ndata to tasks like emotion recognition. This method also\nenables us to use already available pre-trained SSL models\nthat are usually very expensive to train and takes considerable\namount of training time, without re-training or training from\nscratch.\nVOLUME 8, 2020 176283\nS. Siriwardhanaet al.: Multimodal Emotion Recognition With Transformer-Based Self Supervised Feature Fusion\nAlthough we only focused on speech, video, and text in\nthis work, we would like to explore ways to fuse SSL fea-\ntures from other modalities like electroencephalogram (EEG)\ndata [49] in future work. With pre-trained SSL models,\nwe used independently trained models for each modality;\nhowever, recent literature shows that certain SSL algorithms\ncan learn joint information between video and text for tasks\nlike video question answering [48]. Therefore, we aim to\nexplore such models to extract features and ways to design\nSSL models that can learn joint representations between\naudio (speech), video, and text in future research.\nREFERENCES\n[1] R. W. Picard, Affective Computing. Cambridge, MA, USA: MIT Press,\n2000.\n[2] I. Goodfellow, Y . Bengio, and A. Courville, Deep Learning. Cambridge,\nMA, USA: MIT Press, 2016.\n[3] S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and\nR. Mihalcea, ‘‘MELD: A multimodal multi-party dataset for emotion\nrecognition in conversations,’’ 2018, arXiv:1810.02508. [Online]. Avail-\nable: http://arxiv.org/abs/1810.02508\n[4] A. B. Zadeh, P. P. Liang, S. Poria, E. Cambria, and L.-P. Morency,\n‘‘Multimodal language analysis in the wild: CMU-MOSEI dataset and\ninterpretable dynamic fusion graph,’’ in Proc. 56th Annu. Meeting Assoc.\nComput. Linguistics, vol. 1, 2018, pp. 2236–2246.\n[5] G. Degottex, J. Kane, T. Drugman, T. Raitio, and S. Scherer,\n‘‘COV AREP—A collaborative voice analysis repository for speech tech-\nnologies,’’ in Proc. IEEE Int. Conf. Acoust., Speech Signal Process.\n(ICASSP), May 2014, pp. 960–964.\n[6] R. Ekman, What the Face Reveals: Basic and Applied Studies of Spon-\ntaneous Expression Using the Facial Action Coding System (FACS).\nNew York, NY , USA: Oxford Univ. Press, 1997.\n[7] J. Pennington, R. Socher, and C. Manning, ‘‘Glove: Global vectors for\nword representation,’’ in Proc. Conf. Empirical Methods Natural Lang.\nProcess. (EMNLP), 2014, pp. 1532–1543.\n[8] H.-W. Ng, V . D. Nguyen, V . V onikakis, and S. Winkler, ‘‘Deep learning\nfor emotion recognition on small datasets using transfer learning,’’ in Proc.\nACM Int. Conf. Multimodal Interact. (ICMI), 2015, pp. 443–449.\n[9] Z. Han, H. Zhao, and R. Wang, ‘‘Transfer learning for speech emotion\nrecognition,’’ in Proc. IEEE IEEE 5th Int. Conf. Big Data Secur. Cloud\n(BigDataSecurity) Int. Conf. High Perform. Smart Comput., (HPSC) IEEE\nInt. Conf. Intell. Data Secur. (IDS), May 2019, pp. 96–99.\n[10] M. Ezzeldin A. ElShaer, S. Wisdom, and T. Mishra, ‘‘Transfer learn-\ning from sound representations for anger detection in speech,’’ 2019,\narXiv:1902.02120. [Online]. Available: http://arxiv.org/abs/1902.02120\n[11] K. Feng and T. Chaspari, ‘‘A review of generalizable transfer learning\nin automatic emotion recognition,’’ Frontiers Comput. Sci., vol. 2, p. 9,\nFeb. 2020.\n[12] B. Nagarajan and V . R. M. Oruganti, ‘‘Deep net features for com-\nplex emotion recognition,’’ 2018, arXiv:1811.00003. [Online]. Available:\nhttp://arxiv.org/abs/1811.00003\n[13] N.-H. Ho, H.-J. Yang, S.-H. Kim, and G. Lee, ‘‘Multimodal approach of\nspeech emotion recognition using multi-level multi-head fusion attention-\nbased recurrent neural network,’’ IEEE Access, vol. 8, pp. 61672–61686,\n2020.\n[14] Z. Sun, P. Sarma, W. Sethares, and Y . Liang, ‘‘Learning relationships\nbetween text, audio, and video via deep canonical correlation for multi-\nmodal language analysis,’’ 2019, arXiv:1911.05544. [Online]. Available:\nhttp://arxiv.org/abs/1911.05544\n[15] L. Jing and Y . Tian, ‘‘Self-supervised visual feature learning with deep\nneural networks: A survey,’’ 2019, arXiv:1902.06162. [Online]. Available:\nhttp://arxiv.org/abs/1902.06162\n[16] O. Wiles, A. S. Koepke, and A. Zisserman, ‘‘Self-supervised learning\nof a facial attribute embedding from video,’’ 2018, arXiv:1808.06882.\n[Online]. Available: http://arxiv.org/abs/1808.06882\n[17] S. Schneider, A. Baevski, R. Collobert, and M. Auli, ‘‘Wav2vec: Unsu-\npervised pre-training for speech recognition,’’ 2019, arXiv:1904.05862.\n[Online]. Available: http://arxiv.org/abs/1904.05862\n[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., 2017, pp. 5998–6008.\n[19] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, ‘‘RoBERTa: A robustly optimized\nBERT pretraining approach,’’ 2019, arXiv:1907.11692. [Online]. Avail-\nable: http://arxiv.org/abs/1907.11692\n[20] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, ‘‘BERT: Pre-training\nof deep bidirectional transformers for language understanding,’’ 2018,\narXiv:1810.04805. [Online]. Available: http://arxiv.org/abs/1810.04805\n[21] M. El Ayadi, M. S. Kamel, and F. Karray, ‘‘Survey on speech emo-\ntion recognition: Features, classiﬁcation schemes, and databases,’’ Pattern\nRecognit., vol. 44, no. 3, pp. 572–587, Mar. 2011.\n[22] M. Swain, A. Routray, and P. Kabisatpathy, ‘‘Databases, features and clas-\nsiﬁers for speech emotion recognition: A review,’’ Int. J. Speech Technol.,\nvol. 21, no. 1, pp. 93–120, Mar. 2018.\n[23] S. Stöckli, M. Schulte-Mecklenbeck, S. Borer, and A. C. Samson, ‘‘Facial\nexpression analysis with affdex and facet: Avalidation study,’’ Behav. Res.\nMethods, vol. 50, no. 4, pp. 1446–1460, 2018.\n[24] Z. Lu, L. Cao, Y . Zhang, C.-C. Chiu, and J. Fan, ‘‘Speech sentiment\nanalysis via pre-trained features from end-to-end ASR models,’’ in Proc.\nICASSP-IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP),\nMay 2020, pp. 7149–7153.\n[25] N. Tits, K. El Haddad, and T. Dutoit, ‘‘ASR-based features for emo-\ntion recognition: A transfer learning approach,’’ 2018, arXiv:1805.09197.\n[Online]. Available: http://arxiv.org/abs/1805.09197\n[26] P. Sermanet, C. Lynch, Y . Chebotar, J. Hsu, E. Jang, S. Schaal, S. Levine,\nand G. Brain, ‘‘Time-contrastive networks: Self-supervised learning from\nvideo,’’ in Proc. IEEE Int. Conf. Robot. Autom. (ICRA), May 2018,\npp. 1134–1141.\n[27] C. Doersch, A. Gupta, and A. A. Efros, ‘‘Unsupervised visual representa-\ntion learning by context prediction,’’ in Proc. IEEE Int. Conf. Comput. Vis.\n(ICCV), Dec. 2015, pp. 1422–1430.\n[28] T. N. Mundhenk, D. Ho, and B. Y . Chen, ‘‘Improvements to context based\nself-supervised learning,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit., Jun. 2018, pp. 9339–9348.\n[29] A. Kolesnikov, X. Zhai, and L. Beyer, ‘‘Revisiting self-supervised visual\nrepresentation learning,’’ 2019, arXiv:1901.09005. [Online]. Available:\nhttp://arxiv.org/abs/1901.09005\n[30] I. Solaiman, M. Brundage, J. Clark, A. Askell, A. Herbert-V oss,\nJ. Wu, A. Radford, G. Krueger, J. Wook Kim, S. Kreps, M. McCain,\nA. Newhouse, J. Blazakis, K. McGufﬁe, and J. Wang, ‘‘Release strategies\nand the social impacts of language models,’’ 2019, arXiv:1908.09203.\n[Online]. Available: http://arxiv.org/abs/1908.09203\n[31] L. R. Varshney, N. S. Keskar, and R. Socher, ‘‘Pretrained AI models:\nPerformativity, mobility, and change,’’ 2019, arXiv:1909.03290. [Online].\nAvailable: http://arxiv.org/abs/1909.03290\n[32] T. B. Brown et al., ‘‘Language models are few-shot learners,’’ 2020,\narXiv:2005.14165. [Online]. Available: http://arxiv.org/abs/2005.14165\n[33] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and\nS. R. Bowman, ‘‘GLUE: A multi-task benchmark and analysis platform\nfor natural language understanding,’’ 2018, arXiv:1804.07461. [Online].\nAvailable: http://arxiv.org/abs/1804.07461\n[34] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and\nM. Auli, ‘‘Fairseq: A fast, extensible toolkit for sequence modeling,’’ 2019,\narXiv:1904.01038. [Online]. Available: http://arxiv.org/abs/1904.01038\n[35] Y . Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba,\nand S. Fidler, ‘‘Aligning books and movies: Towards story-like visual\nexplanations by watching movies and reading books,’’ in Proc. IEEE Int.\nConf. Comput. Vis. (ICCV), Dec. 2015, pp. 19–27.\n[36] A. van den Oord, Y . Li, and O. Vinyals, ‘‘Representation learning with con-\ntrastive predictive coding,’’ 2018, arXiv:1807.03748. [Online]. Available:\nhttp://arxiv.org/abs/1807.03748\n[37] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, ‘‘Librispeech:\nAn ASR corpus based on public domain audio books,’’ in Proc.\nIEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), Apr. 2015,\npp. 5206–5210.\n[38] J. Son Chung, A. Nagrani, and A. Zisserman, ‘‘V oxCeleb2: Deep\nspeaker recognition,’’ 2018, arXiv:1806.05622. [Online]. Available:\nhttp://arxiv.org/abs/1806.05622\n[39] J. K. P. Seng and K. L.-M. Ang, ‘‘Multimodal emotion and sentiment mod-\neling from unstructured big data: Challenges, architecture, & techniques,’’\nIEEE Access, vol. 7, pp. 90982–90998, 2019.\n176284 VOLUME 8, 2020\nS. Siriwardhanaet al.: Multimodal Emotion Recognition With Transformer-Based Self Supervised Feature Fusion\n[40] Y .-H. H. Tsai, S. Bai, P. P. Liang, J. Z. Kolter, L.-P. Morency,\nand R. Salakhutdinov, ‘‘Multimodal transformer for unaligned multi-\nmodal language sequences,’’ 2019, arXiv:1906.00295. [Online]. Available:\nhttp://arxiv.org/abs/1906.00295\n[41] D. Ghosal, N. Majumder, S. Poria, N. Chhaya, and A. Gelbukh, ‘‘Dia-\nlogueGCN: A graph convolutional neural network for emotion recog-\nnition in conversation,’’ 2019, arXiv:1908.11540. [Online]. Available:\nhttp://arxiv.org/abs/1908.11540\n[42] J. Deng, J. Guo, Y . Zhou, J. Yu, I. Kotsia, and S. Zafeiriou, ‘‘Retinaface:\nSingle-stage dense face localisation in the wild,’’ 2019, arXiv:1905.00641.\n[Online]. Available: https://arxiv.org/abs/1905.00641\n[43] O. Sido, ‘‘SQuAD: Integrating PCE and Non-PCE approaches,’’\nTech. Rep., 2019. [Online]. Available: https://www.semanticscholar.org/\npaper/SQuAD%3A-Integrating-PCE-and-Non-PCE-approaches-Sido/\n52992010675e411808c5eea61826609521904be7?p2df\n[44] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim,\nJ. N. Chang, S. Lee, and S. S. Narayanan, ‘‘IEMOCAP: Interactive emo-\ntional dyadic motion capture database,’’ Lang. Resour. Eval., vol. 42, no. 4,\np. 335, Dec. 2008.\n[45] A. Zadeh, R. Zellers, E. Pincus, and L.-P. Morency, ‘‘MOSI: Mul-\ntimodal corpus of sentiment intensity and subjectivity analysis in\nonline opinion videos,’’ 2016, arXiv:1606.06259. [Online]. Available:\nhttp://arxiv.org/abs/1606.06259\n[46] A. Zadeh, P. P. Liang, S. Poria, P. Vij, E. Cambria, and L.-P. Morency,\n‘‘Multi-attention recurrent network for human communication comprehen-\nsion,’’ in Proc. 32nd AAAI Conf. Artif. Intell., 2018, p. 5642.\n[47] Y . Zhang, Q. Li, D. Song, P. Zhang, and P. Wang, ‘‘Quantum-\ninspired interactive networks for conversational sentiment analysis,’’\nTech. Rep., 2019, pp. 5436–5442. [Online]. Available: https://doi.org/\n10.24963/ijcai.2019/755, doi: 10.24963/ijcai.2019/755.\n[48] J. Lu, D. Batra, D. Parikh, and S. Lee, ‘‘ViLBERT: Pretrain-\ning task-agnostic visiolinguistic representations for vision-and-language\ntasks,’’ 2019, arXiv:1908.02265. [Online]. Available: http://arxiv.org/\nabs/1908.02265\n[49] P. Sarkar and A. Etemad, ‘‘Self-supervised learning for ECG-based\nemotion recognition,’’ 2019, arXiv:1910.07497. [Online]. Available:\nhttp://arxiv.org/abs/1910.07497\n[50] B. Knyazev, R. Shvetsov, N. Efremova, and A. Kuharenko, ‘‘Convolutional\nneural networks pretrained on large face recognition datasets for emotion\nclassiﬁcation from video,’’ 2017, arXiv:1711.04598. [Online]. Available:\nhttp://arxiv.org/abs/1711.04598\nSHAMANE SIRIWARDHANAreceived the bach-\nelor’s degree in electrical and electronic engineer-\ning from the University of Peradeniya, Sri Lanka,\nin 2016, and the M.Eng. degree from The Univer-\nsity of Auckland, New Zealand, in 2019, where\nhe is currently pursuing the Ph.D. degree from the\nAuckland Bioengineering Institute, The Univer-\nsity of Auckland. During his masters, he worked\nin the ﬁeld of transfer reinforcement learning.\nHis research interests are related to multimodal\ndeep learning, emotion recognition, unsupervised learning, natural language\nunderstanding, and human–computer interaction.\nTHARINDU KALUARACHCHI received the\nbachelor’s degree in electronic and telecom-\nmunication engineering from the University of\nMoratuwa, Sri Lanka, in 2016. He is currently pur-\nsuing the Ph.D. degree with the Auckland Bioengi-\nneering Institute, The University of Auckland. His\nresearch interests are related to human-centered\nmachine learning, emotion recognition, unsuper-\nvised learning, and human–computer interaction.\nMARK BILLINGHURST received the Ph.D.\ndegree in electrical engineering from the Univer-\nsity of Washington under the supervision of Prof.\nT. Furness III and Prof. L. Shapiro. He is currently\nworking as a Professor and leading the Empa-\nthetic Computing Laboratory, Auckland Bioengi-\nneering Institute, The University of Auckland.\nHe has coauthored several books and publications\nin peer-reviewed books, journals, and conference\nproceedings leading to more than 24,000 citations.\nHe was awarded a Discover Magazine Award in 2001 for creating the Magic\nBook technology. In 2019, he was awarded with the VGTC Virtual Reality\nCareer Award.\nSURANGA NANAYAKKARAreceived the B.Eng.\nand Ph.D. degrees from the National University of\nSingapore, in 2005 and 2010, respectively. Later,\nhe was a Postdoctoral Researcher with the Pattie\nMaes’s Fluid Interfaces Group, MIT Media Lab.\nIn 2011, he founded the Augmented Human Lab to\nexplore ways of creating novel human–computer\ninterfaces as natural extensions of our body, mind,\nand behaviour. He is currently working as an Asso-\nciate Professor and leading the Augmented Human\nLaboratory, Auckland Bioengineering Institute, The University of Auckland.\nFor the totality and breadth of achievements, he has won many awards\nincluding young inventor under 35 (TR35 award) in the Asia Paciﬁc region\nby MIT TechReview, Outstanding Young Persons of Sri Lanka (TOYP), and\nINK Fellowship 2016.\nVOLUME 8, 2020 176285"
}