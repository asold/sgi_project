{
  "title": "Training Data Extraction From Pre-trained Language Models: A Survey",
  "url": "https://openalex.org/W4385734176",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2578927670",
      "name": "Shotaro Ishihara",
      "affiliations": [
        "Nikkei Business Publications (Japan)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4281483318",
    "https://openalex.org/W3115042282",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W4385894687",
    "https://openalex.org/W4385573947",
    "https://openalex.org/W4386566901",
    "https://openalex.org/W3166854338",
    "https://openalex.org/W4385574204",
    "https://openalex.org/W3013068160",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4287545908",
    "https://openalex.org/W4293469690",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W4226142937",
    "https://openalex.org/W1873763122",
    "https://openalex.org/W4385573004",
    "https://openalex.org/W3170764772",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W4285143763",
    "https://openalex.org/W2784621220",
    "https://openalex.org/W2080957047",
    "https://openalex.org/W4313547874",
    "https://openalex.org/W4287124560",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W2208157769",
    "https://openalex.org/W4294106961",
    "https://openalex.org/W4404752333",
    "https://openalex.org/W3095645723",
    "https://openalex.org/W3106051020",
    "https://openalex.org/W4385679821",
    "https://openalex.org/W4366974303",
    "https://openalex.org/W3211753216",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4287663285",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3186492090",
    "https://openalex.org/W4308672130",
    "https://openalex.org/W4318719586",
    "https://openalex.org/W3125912651",
    "https://openalex.org/W2612281133",
    "https://openalex.org/W4288057780",
    "https://openalex.org/W4320458011",
    "https://openalex.org/W4283810944",
    "https://openalex.org/W3154109599",
    "https://openalex.org/W4361866125",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W2566079294",
    "https://openalex.org/W2963919731",
    "https://openalex.org/W4226099034",
    "https://openalex.org/W4389519044",
    "https://openalex.org/W3177765786",
    "https://openalex.org/W4283805899",
    "https://openalex.org/W4311555033",
    "https://openalex.org/W4368304560",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3153896080",
    "https://openalex.org/W3096738375",
    "https://openalex.org/W4307937235",
    "https://openalex.org/W2952604841",
    "https://openalex.org/W4311996998",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3104578551",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W3170901302",
    "https://openalex.org/W2473418344",
    "https://openalex.org/W4283170666",
    "https://openalex.org/W4385573569",
    "https://openalex.org/W4385572819",
    "https://openalex.org/W1544505227",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W3207429447",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4285224048",
    "https://openalex.org/W4312220150",
    "https://openalex.org/W4281923881",
    "https://openalex.org/W3173298348",
    "https://openalex.org/W3130178918",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W4285225959",
    "https://openalex.org/W3029511006",
    "https://openalex.org/W4226137521",
    "https://openalex.org/W4367365458",
    "https://openalex.org/W3035367371",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4310822350",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W3165327186",
    "https://openalex.org/W2979792666",
    "https://openalex.org/W4297900086",
    "https://openalex.org/W3138815606",
    "https://openalex.org/W3103245149",
    "https://openalex.org/W4385572891",
    "https://openalex.org/W2795435272",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4385573876",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W2042492924",
    "https://openalex.org/W4287324286",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W3206066344",
    "https://openalex.org/W4307106501",
    "https://openalex.org/W4318351475",
    "https://openalex.org/W3138758728",
    "https://openalex.org/W2051267297",
    "https://openalex.org/W3085948490",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4321022177",
    "https://openalex.org/W4283070861",
    "https://openalex.org/W2535690855",
    "https://openalex.org/W3173769540",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W4402264526",
    "https://openalex.org/W3123459983",
    "https://openalex.org/W4283172211",
    "https://openalex.org/W3035616549",
    "https://openalex.org/W3213134179",
    "https://openalex.org/W2963456518",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W179875071",
    "https://openalex.org/W3213545440",
    "https://openalex.org/W2101832700"
  ],
  "abstract": "As the deployment of pre-trained language models (PLMs) expands, pressing security concerns have arisen regarding the potential for malicious extraction of training data, posing a threat to data privacy.This study is the first to provide a comprehensive survey of training data extraction from PLMs.Our review covers more than 100 key papers in fields such as natural language processing and security.First, preliminary knowledge is recapped and a taxonomy of various definitions of memorization is presented.The approaches for attack and defense are then systemized.Furthermore, the empirical findings of several quantitative studies are highlighted.Finally, future research directions based on this review are suggested.",
  "full_text": "Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023), pages 260–275\nJuly 14, 2023 ©2023 Association for Computational Linguistics\nTraining Data Extraction From Pre-trained Language Models: A Survey\nShotaro Ishihara\nNikkei Inc.\n1-3-7, Otemachi, Chiyoda-ku, Tokyo\nshotaro.ishihara@nex.nikkei.com\nAbstract\nAs the deployment of pre-trained language\nmodels (PLMs) expands, pressing security con-\ncerns have arisen regarding the potential for\nmalicious extraction of training data, posing a\nthreat to data privacy. This study is the first\nto provide a comprehensive survey of training\ndata extraction from PLMs. Our review cov-\ners more than 100 key papers in fields such as\nnatural language processing and security. First,\npreliminary knowledge is recapped and a tax-\nonomy of various definitions of memorization\nis presented. The approaches for attack and\ndefense are then systemized. Furthermore, the\nempirical findings of several quantitative stud-\nies are highlighted. Finally, future research\ndirections based on this review are suggested.\n1 Introduction\nPre-trained language models (PLMs) are widely\nused in natural language processing. Statistical\nmodels that assign probabilities to token sequences\nhave been studied, and large neural networks are\nincreasingly being used for pre-training with large\ndatasets. This scaling has led to fluent natural lan-\nguage generation and success in many other down-\nstream tasks (Devlin et al., 2019). In some cases,\nparameter updates are not required for downstream\ntasks (Radford et al., 2019; Brown et al., 2020).\nWith increasing applications of PLMs, secu-\nrity concerns have increased considerably (Ben-\nder et al., 2021; Bommasani et al., 2021; Wei-\ndinger et al., 2022). Studies have revealed the\nrisk of language models exhibiting unintentional\nmemorization of training data, and occasionally\noutputting memorized information (Carlini et al.,\n2019, 2021, 2023b; Lee et al., 2023). In particular,\nCarlini et al. (2021) identified that personal infor-\nmation can be extracted by generating numerous\nsentences from PLMs and performing membership\ninference (Shokri et al., 2017). These attacks on\nPLMs are referred to as training data extraction\nand are undesirable because of privacy, decreased\nutility, and reduced fairness concerns (Carlini et al.,\n2023b). However, with the evolution of PLMs, lim-\nited progress has been achieved in addressing these\nconcerns, and security technology is yet to mature.\nThis study is the first to provide a comprehen-\nsive survey of training data extraction from PLMs.\nStarting with the pioneering work, we reviewed\nmore than 100 previous and subsequent studies.\nSpecifically, we screened papers citing Carlini et al.\n(2021)1 based on the relationships, the number of\ncitations, and their acceptance. First, Section 2\npresents preliminary knowledge. We then discuss\nseveral topics with the following contributions:\n• A taxonomy of various definitions of mem-\norization (Section 3) was presented. Train-\ning data extraction has become close to the\nfamous security attack known as model inver-\nsion (Fredrikson et al., 2015).\n• We systematize the approaches toattack (Sec-\ntion 4) and defense (Section 5). Furthermore,\nwe highlight empirical findings (Section 6)\nfrom several quantitative evaluation studies.\n• Based on the review, we suggest future re-\nsearch directions (Section 7).\n2 Preliminaries about PLMs\nThis section describes the basics of modern PLMs.\nFirst, we explain the methodology used for training\nlanguage models and generating texts. Next, the\nstandard practical schema is introduced.\n2.1 Language Models\nLanguage models represent a probability distribu-\ntion over the sequences of tokens. Based on the\npre-training method, language modeling can be cat-\negorized into two types (Yang et al., 2023): autore-\ngressive language modeling, which predicts words\n1https://scholar.google.com/scholar?cites=\n12274731957504198296\n1\n260\nsequentially from left to right (Bengio et al., 2000;\nMikolov et al., 2010), and masked language mod-\neling, which hides some parts of a sentence and\nfills in the gaps (Devlin et al., 2019). The former is\nsometimes called causal language modeling (Tiru-\nmala et al., 2022).\nThis study is focused on autoregressive lan-\nguage models with transformer (Vaswani et al.,\n2017), following many recent studies on train-\ning data extraction. Note that some studies have\nfocused on masked language models such as\nBERT (Lehman et al., 2021; Mireshghallah et al.,\n2022a; He et al., 2022) and T5 (Carlini et al.,\n2023b). Most studies address pre-training rather\nthan fine-tuning (Mireshghallah et al., 2022b).\nAutoregressive language models take a series of\ntokens as input and output a probability distribution\nfor the next token. We show a schema of training\nand generation by following Carlini et al. (2021).\nTraining. The following statistical model was\nassumed for distribution:\nPr(x1,x2,...,x n),\nwhere x1,x2,...,x n is a sequence of tokens from\na vocabulary using the chain rule of probability:\nPr(x1,x2,...,x n) = Πn\ni=1Pr(xi |x1,...,x i−1).\nLet fθ(xi |x1,...,x i−1) denote the likelihood of\ntoken xi when evaluating neural network f with\nparameters θ. Language models are trained to op-\ntimize the probability of the data in a training set.\nFormally, training involves minimizing the loss\nfunction as follows:\nL(θ) = −log Πn\ni=1fθ(xi |x1,...,x i−1)\nfor each data in the training set. This setting can\nbe qualitatively regarded as memorizing the flow\nof sentences in each training data.\nGenerating. New tokens can be generated by\niterating the following process:\n1. Choose ˆxi+1 ∼fθ(xi+1|x1,...,x i).\n2. Feed ˆxi+1 back into the model to choose\nˆxi+2 ∼fθ(xi+2|x1,..., ˆxi+1).\nThis decoding process continues until conditions\nare satisfied. The simplest is greedy decoding, se-\nlecting the most probable tokens one by one. How-\never, studies have revealed that simply maximizing\nthe output probability generates text that is not nat-\nural to humans (Li et al., 2016; Holtzman et al.,\n2020). Therefore, several approaches have been\nproposed for sampling from a probability distribu-\ntion such as top-k sampling (Fan et al., 2018) and\ntop-p sampling (Appendix A).\n2.2 Pre-training and Fine-tuning\nPrior to BERT (Devlin et al., 2019), specific mod-\nels were trained for individual tasks. By contrast,\nin the PLMs approach, large neural networks with\nlarge datasets are pre-trained and fine-tuned for\nseveral downstream tasks. Radford et al. (2018)\nrevealed that autoregressive language modeling is\neffective for PLMs with transformers. This ex-\ntension, GPT-2 (Radford et al., 2019) and GPT-3\n(Brown et al., 2020), can be applied to various tasks\nwithout fine-tuning by providing a few examples\n(in-context learning). The scaling of large mod-\nels with large datasets has attracted considerable\nresearch attention (Appendix B).\nPLMs exhibit a significant advantage in using\ndatasets that match a specific domain. These mod-\nels can exhibit superior performance in domain-\nspecific tasks than larger models pre-trained on gen-\neral datasets. Studies, such as BioMegatron (Shin\net al., 2020), BioGPT (Luo et al., 2022), Galac-\ntica (Taylor et al., 2022), and BloombergGPT (Wu\net al., 2023), have been conducted. However,\nthe potential risk of training data extraction, espe-\ncially when using sensitive datasets in pre-training,\nshould be considered (Nakamura et al., 2020;\nLehman et al., 2021; Jagannatha et al., 2021; Sing-\nhal et al., 2022; Yang et al., 2022). There are\nalso ethical topics such as the human rights in the\ntexts (Li et al., 2018; Ginart et al., 2019; Garg et al.,\n2020; Henderson et al., 2022) and plagiarism re-\ngarding copyright (Lee et al., 2023). Examples\ninclude PLMs created from contracts (Chalkidis\net al., 2020; Zheng et al., 2021), clinical informa-\ntion (Kawazoe et al., 2021), music (Agostinelli\net al., 2023), and source code (Chen et al., 2021).\n3 Definitions of Memorization\nMemorization is the concept that PLMs store and\noutput information about the training data. There\nis a wide variety of research on memorization, with\ndiverse definitions and assumptions. We illustrate\na taxonomy of definitions in Figure 1.\n3.1 Eidetic memorization\nA mainstream method iseidetic memorization (Car-\nlini et al., 2021) and its variations (Thomas Mc-\nCoy et al., 2021; Carlini et al., 2023b; Kandpal\n261\nmemorization\nverbatim memorization\n approximate memorization\neidetic memorization\ncounterfactual memorization\ndiﬀerential privacy based\nFigure 1: Taxonomy of definitions of memorization.\net al., 2022; Tirumala et al., 2022). These defini-\ntions assume that PLMs output memorized data\nwhen appropriate prompts are provided. Carlini\net al. (2021) defined eidetic memorization as Defi-\nnition 3.1, and in a subsequent study (Carlini et al.,\n2023b), they adopted the definition in Definition\n3.2. They stated that eidetic memorization can be\nused in cases in which no prompt, whereas the sub-\nsequent definition is suitable for conditions with\nprompts. Some studies have adopted definitions\nsimilar to those in Definition 3.2. Examples in-\nclude Tirumala et al. (2022) with a per-token def-\ninition of exact memorization, and Kandpal et al.\n(2022) with a document-level definition of perfect\nmemorization.\nDefinition 3.1 (eidetic memorization). A string s\nis k-eidetic memorized by PLM fθ if a prompt p\nexists such that f(p) = sand sappears at most k\ntimes in the training set.\nDefinition 3.2 (a variation of eidetic memorization).\nA string sis k-memorized with ktokens of context\nfrom a PLM fθ if a (length-k) string pexists such\nthat the concatenation [p||s] is contained in the\ntraining set, and fθ produces s when prompted\nwith pby using greedy decoding.\n3.2 Differential privacy\nDifferential privacy (Dwork et al., 2006) is widely\nused in memorization, and definitions based on\ndifferential privacy have been devised (Jagielski\net al., 2020; Nasr et al., 2021). Differential privacy\nwas formulated based on the premise that removing\nany data from the training set should not consider-\nably change trained models. Although this method\nprotects the personal information of a single user,\nBrown et al. (2022) reported that the method can-\nnot capture the complexity of social and linguistic\ndata. Differential privacy is introduced as a defense\napproach in Section 5.2.\n3.3 Counterfactual memorization\nStudies have defined counterfactual memorization\nas the difference between a training data’s expected\nloss under a model that has and has not been trained\non that data (Feldman and Zhang, 2020; van den\nBurg and Williams, 2021). Zhang et al. (2021c)\ninvestigated this form of memorization in PLMs\nbased on the taxonomy of human memorization in\npsychology.\nThe definition of counterfactual memorization\nhas received limited attention in training data ex-\ntraction. Carlini et al. (2023b) noted that this defini-\ntion requires training thousands of models to mea-\nsure privacy. Thus, evaluating PLMs becomes diffi-\ncult because of their inference costs. Furthermore,\nKandpal et al. (2022) remarked that this definition\nis not considered a privacy attack scenario because\naccess to the training corpus is assumed. This phe-\nnomenon is related to the adversarial knowledge\npresented in Section 4.2.\n3.4 Approximate memorization\nAlthough the definitions of memorization thus far\nassume exact string matches, definitions have been\nproposed to relax this condition. Here, Ippolito\net al. (2022) refer to definitions based on exact\nstring matches as verbatim memorization. They re-\nvealed that verbatim memorization can be handled\nby simply adjusting the decoding method and pro-\nposed alternative definitions called approximate\nmemorization that consider string fuzziness, as\npresented in Definition 3.3. Some methods have\nbeen proposed to calculate similarity. Ippolito et al.\n(2022) set the condition that BLEU(s,g) (Papineni\net al., 2002) is greater than 0.75. The threshold\nvalue of 0.75 was selected by qualitatively inspect-\ning examples. Lee et al. (2022) defined that the\ntoken is memorized if it is part of a substring of 50\ntokens of a string in the training data.\nDefinition 3.3 (approximate memorization) . A\nstring s is k-approximately memorized by PLM\nfθ if a (length- k) string p exists such that (s,g)\nsatisfies certain conditions of similarity, andfθ pro-\nduces gwhen prompted with p.\n3.5 Revisiting model inversion\nReconstructing training data from a model presents\na well-known security concern called model inver-\n262\nPrompt\nTraining setPre-train\nPLMs\nGenerate Candidate\nMembership \ninference\nDefense: \nTraining\nDefense: \nPre-processing\nDefense: \nPost-processing\ndata deduplication\ndata sanitization\nregularization\ndiﬀerential privacy\nﬁlterling\nconﬁdence masking\nknowledge distillation\nFigure 2: The procedure of training data extraction attacks and possible defenses.\nsion attacks (Fredrikson et al., 2015). Carlini et al.\n(2021) explained that the main difference is that\ntraining data extraction does not allow fuzziness.\nHowever, this difference has decreased since the\nintroduction of relaxed definitions of memorization.\nKandpal et al. (2022) mentioned several previous\nstudies (Carlini et al., 2019, 2021; Inan et al., 2021)\nas model inversion.\n4 Training Data Extraction Attacks\nThis section systematizes the attack procedure.\nMost studies follow Carlini et al. (2021). They\nrevealed that hundreds of verbatim text sequences\ncan be extracted from the training data. Given\na PLM, the procedure consists of two steps, can-\ndidate generation, and membership inference, as\ndisplayed in Figure 2.\n4.1 Candidate generation\nThe first step is to generate numerous texts from\na given PLM. Texts can be generated from PLMs\nusing several decoding methods, as discussed in\nAppendix A. Here, Carlini et al. (2023b) reported\nthat the choice of the decoding strategy does not\nconsiderably affect their experimental results. In\ncontrast, Lee et al. (2023) observed that top-k and\ntop-p sampling tended to extract more training data.\nAnother perspective is the procedure for pro-\nviding prompts. Prompts are provided accord-\ning to two options, giving only a special token 2\n(sometimes called no prompt) or specific strings as\nprompts. Studies have constructed prompts by ex-\ntracting data from the dataset considered to be used\n2Carlini et al. (2021) used <|endoftext|>, as indicated at\nhttps://github.com/ftramer/LM_Memorization.\nin creating PLMs. Carlini et al. (2021) randomly\nsampled between 5 and 10 tokens from scraped\ndata. Carlini et al. (2023b) extracted a subset of\nthe Pile dataset (Gao et al., 2020) in prompting\nGPT-Neo model family (Black et al., 2022).\n4.2 Membership inference\nMembership inference aims to predict whether\nany particular example is used to train a machine\nlearning model (Shokri et al., 2017; Song and\nShmatikov, 2019; Hisamoto et al., 2020). This\nresult can lead directly to privacy violations. We\ndescribe membership inference on PLMs from the\nfollowing five perspectives in a survey paper (Hu\net al., 2022): target model, adversarial knowledge,\napproach, algorithm, and domain.\nTarget model. This study focuses on autoregres-\nsive language models as discussed in Section 2.1.\nAttacks on other models such as word embed-\ndings (Song and Raghunathan, 2020; Mahloujifar\net al., 2021; Meehan et al., 2022), natural language\nunderstanding (Parikh et al., 2022), text classifica-\ntion (Nasr et al., 2019; Zhang et al., 2022; Elmahdy\net al., 2022), and image diffusion models (Carlini\net al., 2023a) exist but are not covered.\nAdversarial knowledge. The second perspective\nis the knowledge that can be handled explicitly by\nattackers. We describe two aspects of adversarial\nknowledge, namely models and training sets. The\npatterns of adversarial knowledge in this study are\nsummarized in Appendix C.\nHu et al. (2022) presented the adversarial knowl-\nedge of models. The models are classified into\ntwo categories, namely white-box and black-box,\n263\naccording to accessibility (Nasr et al., 2019). Un-\nder the white-box setting, an attacker can obtain\nall information and use it for the attack. This in-\ncludes the training procedure and the architecture\nand trained parameters of the target model. How-\never, in the black-box setting, an attacker can only\nhave limited access to the target model. Hu et al.\n(2022) classified the black-box setting into three\nparts, namely full confidence scores, top-k con-\nfidence scores, and prediction labels only. They\ndiffer in the extent of access an attacker has to the\nPLMs output. The setting of full confidence scores\nassumes a situation in which the training process\nof the model is unknown, but all outputs for any\ngiven input are available. Therefore, an attacker\ncan obtain prediction labels with probabilities and\ncalculate the loss. The setting of top-k confidence\nscores indicates that an attacker can obtain several\ncandidates of the output. The scope of the attack\nis restricted because losses cannot be calculated.\nAnother setting provides only labels without pre-\ndiction values (Choquette-Choo et al., 2021; Zhu\net al., 2023). Many web services with PLMs, such\nas DeepL3 and ChatGPT4, only allow users to view\nlabels for the model output.\nFurthermore, we describe the adversarial knowl-\nedge of the training sets. In the white-box setting,\nthe training set is stated and publicly available. The\nmost harmful attacks are black box setups that do\nnot assume access to the training set. Such attacks\ninclude PLMs created by private datasets. In some\ncases, the data are partially publicly available. Such\ncases include the ones wherein only the beginning\nof the news article is available for free, certain edi-\ntions are accessible, and some articles have been\nmade private over time. Although the data itself are\nnot partially published, substrings can be inferred\nin the hidden private data using a priori knowledge\n(Henderson et al., 2018; Carlini et al., 2019). Ex-\namples are prompts like \"Bob’s phone number is\"\nand \"Alice’s password is\".\nWe must be aware of scenarios in which the\ndataset and PLMs are unwillingly leaked and be-\ncome public. Adversarial knowledge is immedi-\nately converted to the white-box level. For example,\neven if a web service with PLMs trained on a pri-\nvate dataset provides users with only a string, it is\ncrucial to discuss risks when both the dataset and\nthe PLMs are unintentionally made public.\n3https://www.deepl.com/translator\n4https://openai.com/blog/chatgpt/\nApproach. Hu et al. (2022) divided the mem-\nbership inference approaches into three categories,\nnamely classifier-based (Shokri et al., 2017; Song\nand Shmatikov, 2019), metric-based (Bentley et al.,\n2020; Choquette-Choo et al., 2021; Song and Mit-\ntal, 2021), and differential comparisons (Hui et al.,\n2021). For example, in shadow training (Shokri\net al., 2017; Song and Shmatikov, 2019), a primary\nclassifier-based method, additional training is as-\nsumed in the model (white-box settings). Some\nmetric-based methods can be applied to realistic\nblack-box settings.\nIn studies of training data extraction from PLMs,\nperplexity is often used for metrics of member-\nship inference (Carlini et al., 2019, 2021). Given\na sequence of tokens x1,...,x n, the perplexity is\ndefined as:\nP= exp\n(\n−1\nn\nn∑\ni=1\nlog fθ(xi|x1,...,x i−1)\n)\nAlgorithm. The fourth perspective is whether\nthe algorithm is centralized or federated. Federated\nlearning approaches have received considerable at-\ntention in privacy protection research (Melis et al.,\n2019; Nasr et al., 2019; Lee et al., 2021; Kairouz\net al., 2021). However, focusing on training data\nextraction, the mainstream approach is based on\ncentralized methods as of April 2023.\nDomain. Text datasets are rooted in various do-\nmains, as described in Section 2.2. Clinics are a cru-\ncial research field that involves handling of highly\nconfidential information. Lehman et al. (2021) re-\ncovered patient names and their associated condi-\ntions from PLMs using electronic clinical records.\nJagannatha et al. (2021) demonstrated that patients\nwith rare disease profiles may be highly vulnera-\nble to higher privacy leakages through experiments\nusing PLMs of clinical data. Many other domains\nrequire careful processing, such as contracts (Yin\nand Habernal, 2022) and source code5. A discus-\nsion of the right to be forgotten in the legal and\nnews industries has emerged (Li et al., 2018; Gi-\nnart et al., 2019; Garg et al., 2020; Henderson et al.,\n2022). Therefore, it should be ensured that PLMs\ndo not unintentionally become digital archives.\nPublicly available datasets do not necessarily\nindicate that they are completely independent of\nthe risk of training data extraction from PLMs.\nThe context in which the information is shared\n5https://github.blog/\n2021-06-30-github-copilot-research-recitation/\n264\nshould be known to respect privacy (Dourish, 2004;\nNissenbaum, 2009). Nissenbaum’s contextual in-\ntegrity (Nissenbaum, 2009) states that a change in\nany one of five characteristics (data subject, sender,\nrecipient, information type, and transmission prin-\nciple) may alter privacy expectations. Brown et al.\n(2022) emphasized the importance of PLMs only\nwith data explicitly intended for public use. The\nItalian Data Protection Authority issued a state-\nment6 on March 2023 in accordance with the Euro-\npean General Data Protection Regulation (GDPR)\nagainst OpenAI, the provider of ChatGPT, for their\ndata processing.\n5 Training Data Extraction Defenses\nThis section systematizes approaches to defense.\nWe can mitigate privacy risks before, during, and\nafter creating PLMs as displayed in Figure 2.\nThe classification was reconstructed using refer-\nences (Hu et al., 2022; Huang et al., 2022; Jagielski\net al., 2023). Extensive studies have been con-\nducted on the hazardous generation of PLMs (Ku-\nrita et al., 2020; Mei et al., 2022; Levy et al., 2022;\nOuyang et al., 2022; Carlini et al., 2023c). How-\never, this study focused on training data extraction.\n5.1 Pre-processing\nFirst, pre-processing the training set is considered.\nData sanitization. The simplest solution is to\nidentify and remove any text that conveys personal\ninformation (Ren et al., 2016; Continella et al.,\n2017; Vakili et al., 2022). However, as noted in\nSection 4.2, privacy depends on the context, and de-\ntermining privacy from the string alone is difficult.\nBrown et al. (2022) proposed that data sanitization\nis only useful for removing context-independent,\nwell-defined, static pieces of personal information\nfrom the training set.\nData deduplication. Studies have indicated that\ndata deduplication mitigates the memorization of\nPLMs (Allamanis, 2019; Kandpal et al., 2022; Lee\net al., 2022). This method is more efficient than\nmethods that train models and is expected to be\na practical solution. Empirical findings on data\ndeduplication are presented in Section 6.2.\n5.2 Training\nThe second method is a pre-training strategy.\n6https://www.garanteprivacy.it/home/docweb/-/\ndocweb-display/docweb/9870847\nDifferential privacy. Applying differential pri-\nvacy (Dwork et al., 2006) methods for providing\ndata privacy guarantees in machine learning models\nhas attracted considerable research attention. Dif-\nferential privacy is a data protection measure that\nis designed to ensure that providing data does not\nreveal much information about the user. However,\napplying these algorithms (e.g., DP-SGD (Abadi\net al., 2016) and DP-FedAvg (Ramaswamy et al.,\n2020)) to PLMs is challenging. Performance degra-\ndation and increased computation and memory us-\nage are the primary concerns.\nTo address this problem, a framework has been\nproposed for training models in two steps (Yu et al.,\n2021, 2022; Li et al., 2022; He et al., 2023)7. In the\nframework, large amounts of non-private datasets\nare used for pre-training to obtain general features;\nnext, additional training is applied with a sensi-\ntive dataset using a differential privacy algorithm.\nDowney et al. (2022) reported that the differential\nprivacy approach is effective in preventing memo-\nrization, despite its computational and model per-\nformance costs. Note that Tramèr et al. (2022)\nsummarized a critical view. They argued that pub-\nlicly accessible datasets are not free from privacy\nrisks because they contain information that is un-\nintentionally released to the public. Therefore, dis-\ncussing whether private information that we want\nto hide is contained in the public dataset is essen-\ntial. It is known that understanding the semantic\nguarantee of differential privacy is difficult when\nprivate data is involved (Cummings et al., 2021).\nAnother barrier to applying differential privacy\nto PLMs is the requirement of defining secret\nboundaries even though text data are not binary.\nStudies have considered various levels of granular-\nity, from individual tokens or words to sentences,\ndocuments, or even the entire user dataset (McMa-\nhan et al., 2018; Levy et al., 2021; Lukas et al.,\n2023).\nRegularization. Regularization is a well-known\napproach for suppressing overfitting in machine\nlearning models. The memorization of models is\ntypically associated with overfitting (Yeom et al.,\n2018; Zhang et al., 2021b). Therefore, regulariza-\ntion during training that reduces overfitting can be\nused as a measure of membership inference (Hu\net al., 2022). Mireshghallah et al. (2021) proposed\na regularization method regarding the memoriza-\n7A study has also appeared that applies these algorithms\nto in-context learning settings (Panda et al., 2023).\n265\ntion of PLMs and claimed usefulness compared\nwith differential privacy methods. Some studies\nhave constrained the representation of neural net-\nworks by the information bottleneck layer (Alemi\net al., 2017; Henderson and Fehr, 2023).\nPre-training large neural networks has distinctive\ntendencies compared with common machine learn-\ning. A single data in the training set is not used for\ntoo many epochs in pre-training and is sometimes\nused for less than one epoch. Furthermore, Carlini\net al. (2021) reported that a characteristic of PLM\nmemorization is the emergence of training data\nwith an abnormally lower loss than the average.\nTirumala et al. (2022) revealed that large language\nmodels can memorize most of their data before\noverfitting and tend not to forget much informa-\ntion through the training process. Biderman et al.\n(2023) have focused on the training process and\nattempted to predict the memorization of PLMs.\nKnowledge distillation. Another approach is\nknowledge distillation (Hinton et al., 2015), in\nwhich the output of a large teacher model is used\nto train a small student model. Shejwalkar and\nHoumansadr (2021) revealed that knowledge dis-\ntillation can be used to restrict an attacker’s direct\naccess to a private training set, which considerably\nreduces membership information leakage.\n5.3 Post-processing\nThe third step is to post-process the PLMs output.\nConfidence masking. Limiting the output of\nPLMs is a simple but effective defense mechanism.\nFor example, confidence masking can be used for\nadjusting adversarial knowledge, as presented in\nSection 4.2 and Appendix C.\nFiltering. Filtering the output of PLMs before\nproviding them to users is crucial. Identifying items\nto be filtered incurs a cost, and ensuring diversity\nremains challenging. Perez et al. (2022) proposed\na method to automatically identify test cases by ex-\ntracting potentially dangerous outputs by detailing\nprompts using various PLMs.\n6 Empirical Findings\nThis section presents empirical findings on training\ndata extraction from PLMs. Initial studies were\nlimited to qualitative evaluations, but subsequent\nstudies (Lee et al., 2022; Kandpal et al., 2022; Ip-\npolito et al., 2022; Tirumala et al., 2022; Downey\net al., 2022; Carlini et al., 2023b; Lee et al., 2023)\nhave focused on quantitative evaluations.\nIn particular, based on one of the first compre-\nhensive quantitative studies (Carlini et al., 2023b),\nwe report on the impact of the model size, the\nstring duplication in the training set, and the length\nof prompts. They used various sizes of GPT-Neo\nmodel family (Black et al., 2022), which are the\nautoregressive language models pre-trained by the\nPile dataset (Gao et al., 2020). Four model sizes,\nnamely 125 million, 1.3 billion (B), 2.7 B, and 6\nB parameters, were considered. The number of\nduplicate strings was determined by analyzing the\nPile dataset. A subset of 50,000 sentences from\nthe Pile dataset was used for evaluation, and the\ndistribution of duplicates was considered. The be-\nginning of each sentence was cut out at a certain\nnumber of tokens and considered as a prompt. The\namount of memorization was calculated as the frac-\ntion of generations that exactly reproduce the true\nstring for their prompt averaged over all prompts\nand sequence lengths.\n6.1 Larger models memorize more\nCarlini et al. (2023b) revealed that a near-perfect\nlog-linear relationship exists such that the larger\nthe model size is, the more strings are memorized.\nNumerically, a ten-fold increase in the model size\nincreased the amount of memorization by 19 ppt.\nFor comparison, they performed the same analysis\nwith the GPT-2 model family. The amount of mem-\norization was 40 % for 1.3 B GPT-neo compared\nwith 6 % for the GPT-2 of the same size. This phe-\nnomenon implied the effect of memorization of the\ntraining data, not just the model size.\nCarlini et al. (2023b) used the definition of ver-\nbatim memorization, and Ippolito et al. (2022) con-\nfirmed similar results with the definition of ap-\nproximate memorization. Although not sufficiently\nquantitative, initial studies (Carlini et al., 2019;\nZhang et al., 2021b) have provided preliminary ev-\nidence. Tirumala et al. (2022) and Lee et al. (2023)\nalso revealed that larger models memorize more.\n6.2 Duplicate strings are memorized\nCarlini et al. (2023b) reported that a clear log-linear\ntrend exists between the number of duplicates and\nthe amount of memorization. They measured the\namount of memorization for each bucket with du-\nplicate counts ranging from 2 to 900. Kandpal\net al. (2022) and Lee et al. (2022) also revealed that\nduplication in the training set of PLMs relates to\n266\nthe likelihood of memorizing strings and proposed\nthat deduplication mitigates training data extrac-\ntion. However, memorization can occur even with\nonly a few duplicates, and deduplication cannot\nprevent it completely. Chang et al. (2023) reported\nthat the degree of memorization of ChatGPT and\nGPT-4 (OpenAI, 2023) was related to the frequency\nof the passages that appeared on the web.\n6.3 Longer prompts extract more\nCarlini et al. (2023b) revealed that the amount\nof memorization increases with the length of the\nprompt. For example, the amount of memorization\nby the 6 B model was 33 % for 50 tokens, com-\npared with 65 % for 450 tokens. This experiment\nwas inspired by the findings of Carlini et al. (2019).\nThey suggested that setting the maximum prompt\nlength available to users considerably reduces the\nrisk of training data extraction.\n7 Conclusion & Future Directions\nWe have reviewed over 100 papers for the first com-\nprehensive survey on training data extraction from\nPLMs. The final section provides suggestions for\nfuture research directions. We hope that this study\nhighlights the importance of training data extrac-\ntion from PLMs and accelerates the discussion.\n7.1 Is memorization always evil?\nMost studies did not distinguish the degree of dan-\nger of memorized strings (Lee et al., 2020). Ideally,\nthe undesirable memorization of telephone num-\nbers and email addresses must be separated from\nthe acceptable memorization. Huang et al. (2022)\nwas among the first to differentiate between memo-\nrization and association in PLMs. They concluded\nthat the risk of specific personal information being\nleaked is low because PLMs cannot semantically\nassociate personal information with their owners.\nThe boundary between memorization and knowl-\nedge of PLMs remains ambiguous with the defini-\ntion of approximate memorization (Ippolito et al.,\n2022; Lee et al., 2022). Deduplication of training\nsets, which is considered useful in Sections 5 and\n6, leads to the elimination of helpful knowledge.\nTherefore, we must consider what memorization\nis (Haviv et al., 2022) and balance the security\nconcerns with the model performance, depending\non the final application. The definition of coun-\nterfactual memorization introduced in Section 3.3\nincorporated psychological findings that could be\nuseful despite its challenges.\n7.2 Toward broader research fields\nDiscussing the handling of the fuzziness of a string\nis important. Ippolito et al. (2022) stated that the\ncurrent definition of approximate memorization fo-\ncuses on English, and different considerations are\nrequired for other conditions such as non-English\nlanguages. In addition, they suggested two research\nareas that could help improve the definition: im-\nage generation memorization and plagiarism de-\ntection. Images are more difficult to generate than\ntext for matching exactly with the original. There-\nfore, fuzzy memorization has been investigated\nand measured. Fredrikson et al. (2015), which\nproposed the model inversion attack, used face\nrecognition in images as the subject of their ex-\nperiments. Studies have used metrics that consider\nimage similarity (Zhang et al., 2020; Haim et al.,\n2022; Balle et al., 2022). Furthermore, the trend to-\nward pre-training in both images and language (Lu\net al., 2019; Li et al., 2020) should be considered.\nThe limitations of the definition of verbatim tex-\ntual matching have been discussed in plagiarism\ndetection research (Roy et al., 2009; Potthast et al.,\n2010). Similarities are explored from multiple per-\nspectives, including word changes, shuffling, and\nparaphrasing.\n7.3 Evaluation schema\nRoom for ingenuity exists in the construction of\nevaluation sets. Establishing a schema for quanti-\ntative evaluation, which has received considerable\nattention, is critical. Studies mentioned in Sections\n4 and 6 have created evaluation sets by extracting\na subset of the training set. Sampling is essential\nbecause of inference time limitations. However, we\nmust be careful to see if there are other factors to\nconsider besides the distribution of the number of\nduplicates to avoid bias due to sampling.\nEvaluation metrics for the training data extrac-\ntion are open for discussion. Carlini et al. (2022)\npostulated that the ideal evaluation metric must be\nbased on realistic attack scenarios, whereas most\nstudies on membership inference measure the aver-\nage accuracy rate. They proposed that membership\ninference should be evaluated by the true positive\nrate with a low false positive rate. The Training\nData Extraction Challenge8 measures attack speed\nas well as recall and precision.\n8https://github.com/google-research/\nlm-extraction-benchmark\n267\nLimitations\nFirst, this study focused on PLMs in training data\nextraction, particularly autoregressive language\nmodels. Other target models, such as masked lan-\nguage models (described in Section 2.1) and word\nembeddings (noted in Section 4.2), require another\ndiscussion. Additionally, due to prioritization con-\nstraints, the discussion on other topics, including\nmodel inversion attacks and the federated learning\napproach, was limited. However, these areas are es-\ntablished and can be supplemented by other studies\n(Fredrikson et al., 2015; Zhang et al., 2021a).\nSecond, in practical applications of PLM, it is\nnecessary to audit not only security but also var-\nious other aspects such as performance degrada-\ntion (Mökander et al., 2023). There are a number\nof security concerns beyond training data extrac-\ntion (noted in Section 5). There are also papers\ndiscussing performance degradation of PLMs over\ntime (Ishihara et al., 2022).\nFinally, this comprehensive survey is based on\ninformation as of April 2023. Studies on training\ndata extraction from PLMs have primarily focused\non natural language processing and security. These\ndomains are undergoing rapid changes. Therefore,\nsome of the content may become obsolete in the\nnear future.\nEthics Statement\nThe privacy concerns regarding training data ex-\ntraction from PLMs were reviewed to help mature\ndiscussions in academia and industry. Of course,\nits purpose is not to promote these attacks.\nStudies on PLMs tend to focus on the English\nlanguage, which is the language used by the ma-\njority of people in the world, and the same is true\nfor training data extraction. Therefore, this study\nfocused on English. As indicated in Section 7.2,\nresearch on other languages is encouraged.\nAcknowledgements\nWe would like to thank Editage ( www.editage.\ncom) for English language editing.\nReferences\nMartin Abadi, Andy Chu, Ian Goodfellow, et al. 2016.\nDeep learning with differential privacy. In Proceed-\nings of the 2016 ACM SIGSAC Conference on Com-\nputer and Communications Security, CCS ’16, pages\n308–318, New York, NY , USA. Association for Com-\nputing Machinery.\nDavid H Ackley, Geoffrey E Hinton, and Terrence J Se-\njnowski. 1985. A learning algorithm for boltzmann\nmachines. Cognitive Science, 9(1):147–169.\nAndrea Agostinelli, Timo I Denk, Zalán Borsos, et al.\n2023. MusicLM: Generating music from text. arXiv\npreprint arXiv:2301.11325.\nAlexander A. Alemi, Ian Fischer, Joshua V . Dillon, et al.\n2017. Deep variational information bottleneck. In\nProceedings of the 5th International Conference on\nLearning Representations.\nMiltiadis Allamanis. 2019. The adverse effects of code\nduplication in machine learning models of code. In\nProceedings of the 2019 ACM SIGPLAN Interna-\ntional Symposium on New Ideas, New Paradigms,\nand Reflections on Programming and Software, On-\nward! 2019, pages 143–153, New York, NY , USA.\nAssociation for Computing Machinery.\nBalle, Cherubin, and Hayes. 2022. Reconstructing train-\ning data with informed adversaries. In 2022 IEEE\nSymposium on Security and Privacy (SP), volume 0,\npages 1138–1156.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, et al. 2021. On the dangers of stochastic\nparrots: Can language models be too big? In Pro-\nceedings of the 2021 ACM Conference on Fairness,\nAccountability, and Transparency, FAccT ’21, pages\n610–623, New York, NY , USA. Association for Com-\nputing Machinery.\nYoshua Bengio, Réjean Ducharme, and Pascal Vincent.\n2000. A neural probabilistic language model. In\nAdvances in Neural Information Processing Systems,\nvolume 13. MIT Press.\nJason W Bentley, Daniel Gibney, Gary Hoppenworth,\net al. 2020. Quantifying membership inference vul-\nnerability via generalization gap and other model\nmetrics. arXiv preprint arXiv:2009.05669.\nStella Biderman, Usvsn Sai Prashanth, Lintang\nSutawika, et al. 2023. Emergent and predictable\nmemorization in large language models. arXiv\npreprint arXiv:2304.11158.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace\nHe, Connor Leahy, Kyle McDonell, Jason Phang,\nMichael Pieler, Usvsn Sai Prashanth, Shivanshu Puro-\nhit, Laria Reynolds, Jonathan Tow, Ben Wang, and\nSamuel Weinbach. 2022. GPT-NeoX-20B: An open-\nsource autoregressive language model. In Proceed-\nings of BigScience Episode #5 – Workshop on Chal-\nlenges & Perspectives in Creating Large Language\nModels, pages 95–136, virtual+Dublin. Association\nfor Computational Linguistics.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, et al.\n2021. On the opportunities and risks of foundation\nmodels. arXiv preprint arXiv:2108.07258.\n268\nHannah Brown, Katherine Lee, Fatemehsadat\nMireshghallah, et al. 2022. What does it mean for\na language model to preserve privacy? In Pro-\nceedings of the 2022 ACM Conference on Fairness,\nAccountability, and Transparency, FAccT ’22, pages\n2280–2292, New York, NY , USA. Association for\nComputing Machinery.\nTom Brown, Benjamin Mann, Nick Ryder, et al. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nNicholas Carlini, Steve Chien, Milad Nasr, et al. 2022.\nMembership inference attacks from first principles.\nIn 2022 IEEE Symposium on Security and Privacy\n(SP), pages 1897–1914.\nNicholas Carlini, Jamie Hayes, Milad Nasr, et al. 2023a.\nExtracting training data from diffusion models. arXiv\npreprint arXiv:2301.13188.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\net al. 2023b. Quantifying memorization across neural\nlanguage models. In Proceedings of the 11th Inter-\nnational Conference on Learning Representations.\nNicholas Carlini, Matthew Jagielski, Christopher A\nChoquette-Choo, et al. 2023c. Poisoning Web-\nScale training datasets is practical. arXiv preprint\narXiv:2302.10149.\nNicholas Carlini, Chang Liu, Úlfar Erlingsson, et al.\n2019. The secret sharer: Evaluating and testing un-\nintended memorization in neural networks. In 28th\nUSENIX Security Symposium (USENIX Security 19),\npages 267–284.\nNicholas Carlini, Florian Tramèr, Eric Wallace, et al.\n2021. Extracting training data from large lan-\nguage models. In 30th USENIX Security Symposium\n(USENIX Security 21), pages 2633–2650.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. LEGAL-BERT: The muppets straight out of\nlaw school. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 2898–\n2904, Online. Association for Computational Lin-\nguistics.\nKent K Chang, Mackenzie Cramer, Sandeep Soni,\net al. 2023. Speak, memory: An archaeology of\nbooks known to ChatGPT/GPT-4. arXiv preprint\narXiv:2305.00118.\nMark Chen, Jerry Tworek, Heewoo Jun, et al. 2021.\nEvaluating large language models trained on code.\narXiv preprint arXiv:2107.03374.\nChristopher A. Choquette-Choo, Florian Tramer,\nNicholas Carlini, et al. 2021. Label-only membership\ninference attacks. In Proceedings of the 38th Inter-\nnational Conference on Machine Learning, volume\n139 of Proceedings of Machine Learning Research,\npages 1964–1974. PMLR.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\net al. 2022. PaLM: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311.\nAndrea Continella, Yanick Fratantonio, Martina Lindor-\nfer, et al. 2017. Obfuscation-resilient privacy leak\ndetection for mobile apps through differential anal-\nysis. In Proceedings 2017 Network and Distributed\nSystem Security Symposium , Reston, V A. Internet\nSociety.\nRachel Cummings, Gabriel Kaptchuk, and Elissa M\nRedmiles. 2021. “I need a better description”: An\ninvestigation into user expectations for differential\nprivacy. In Proceedings of the 2021 ACM SIGSAC\nConference on Computer and Communications Se-\ncurity, CCS ’21, pages 3037–3052, New York, NY ,\nUSA. Association for Computing Machinery.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nPaul Dourish. 2004. What we talk about when we talk\nabout context. Personal and Ubiquitous Computing,\n8(1):19–30.\nC M Downey, Wei Dai, Huseyin A Inan, et al.\n2022. Planting and mitigating memorized content\nin Predictive-Text language models. arXiv preprint\narXiv:2212.08619.\nCynthia Dwork, Frank McSherry, Kobbi Nissim, et al.\n2006. Calibrating noise to sensitivity in private data\nanalysis. In Theory of Cryptography, pages 265–284.\nSpringer Berlin Heidelberg.\nAdel Elmahdy, Huseyin A. Inan, and Robert Sim. 2022.\nPrivacy leakage in text classification a data extraction\napproach. In Proceedings of the Fourth Workshop\non Privacy in Natural Language Processing, pages\n13–20, Seattle, United States. Association for Com-\nputational Linguistics.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\nHierarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889–898, Melbourne, Australia. Association\nfor Computational Linguistics.\nVitaly Feldman and Chiyuan Zhang. 2020. What neural\nnetworks memorize and why: discovering the long\ntail via influence estimation. In Proceedings of the\n34th International Conference on Neural Information\nProcessing Systems, number Article 242 in NIPS’20,\npages 2881–2891, Red Hook, NY , USA. Curran As-\nsociates Inc.\n269\nMatt Fredrikson, Somesh Jha, and Thomas Ristenpart.\n2015. Model inversion attacks that exploit confi-\ndence information and basic countermeasures. In\nProceedings of the 2015 ACM SIGSAC Conference\non Computer and Communications Security , CCS\n’15, pages 1322–1333, New York, NY , USA. Associ-\nation for Computing Machinery.\nLeo Gao, Stella Biderman, Sid Black, et al. 2020. The\npile: An 800GB dataset of diverse text for language\nmodeling. arXiv preprint arXiv:2101.00027.\nSanjam Garg, Shafi Goldwasser, and Prashant Nalini\nVasudevan. 2020. Formalizing data deletion in the\ncontext of the right to be forgotten. In Advances in\nCryptology – EUROCRYPT 2020 , pages 373–402.\nSpringer International Publishing.\nAntonio A Ginart, Melody Y Guan, Gregory Valiant,\net al. 2019. Making AI forget you: data deletion in\nmachine learning. In Proceedings of the 33rd Interna-\ntional Conference on Neural Information Processing\nSystems, NIPS’19, pages 3518–3531, Red Hook, NY ,\nUSA. Curran Associates Inc.\nNiv Haim, Gal Vardi, Gilad Yehudai, et al. 2022. Recon-\nstructing training data from trained neural networks.\nIn Advances in Neural Information Processing Sys-\ntems.\nAdi Haviv, Ido Cohen, Jacob Gidron, et al. 2022. Un-\nderstanding transformer memorization recall through\nidioms. arXiv preprint arXiv:2210.03588.\nJiyan He, Xuechen Li, Da Yu, et al. 2023. Exploring\nthe limits of differentially private deep learning with\ngroup-wise clipping. In Proceedings of the 11th In-\nternational Conference on Learning Representations.\nXuanli He, Lingjuan Lyu, Chen Chen, and Qiongkai Xu.\n2022. Extracted BERT model leaks more informa-\ntion than you think! In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1530–1537, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nJames Henderson and Fabio James Fehr. 2023. A V AE\nfor transformers with nonparametric variational in-\nformation bottleneck. In Proceedings of the 11th\nInternational Conference on Learning Representa-\ntions.\nPeter Henderson, Mark S Krass, Lucia Zheng, et al.\n2022. Pile of law: Learning responsible data filter-\ning from the law and a 256GB open-source legal\ndataset. In 36th Conference on Neural Information\nProcessing Systems Datasets and Benchmarks Track.\nPeter Henderson, Koustuv Sinha, Nicolas Angelard-\nGontier, et al. 2018. Ethical challenges in Data-\nDriven dialogue systems. In Proceedings of the 2018\nAAAI/ACM Conference on AI, Ethics, and Society ,\nAIES ’18, pages 123–129, New York, NY , USA. As-\nsociation for Computing Machinery.\nTom Henighan, Jared Kaplan, Mor Katz, et al. 2020.\nScaling laws for autoregressive generative modeling.\narXiv preprint arXiv:2010.14701.\nGeoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. 2015.\nDistilling the knowledge in a neural network. In\nNIPS Deep Learning and Representation Learning\nWorkshop.\nSorami Hisamoto, Matt Post, and Kevin Duh. 2020.\nMembership inference attacks on sequence-to-\nsequence models: Is my data in your machine trans-\nlation system? Transactions of the Association for\nComputational Linguistics, 8:49–63.\nAri Holtzman, Jan Buys, Li Du, et al. 2020. The curious\ncase of neural text degeneration. In Proceedings of\nthe 8th International Conference on Learning Repre-\nsentations.\nHongsheng Hu, Zoran Salcic, Lichao Sun, et al. 2022.\nMembership inference attacks on machine learning:\nA survey. ACM Computing Surveys, 54(11s).\nJie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang.\n2022. Are large pre-trained language models leaking\nyour personal information? In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2022,\npages 2038–2047, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nBo Hui, Yuchen Yang, Haolin Yuan, et al. 2021. Prac-\ntical blind membership inference attack via differ-\nential comparisons. In 28th Annual Network and\nDistributed System Security Symposium, NDSS 2021,\nvirtually, February 21-25, 2021. The Internet Society.\nHuseyin A Inan, Osman Ramadan, Lukas Wutschitz,\net al. 2021. Training data leakage analysis in lan-\nguage models. In 3rd Privacy-Preserving Machine\nLearning Workshop.\nDaphne Ippolito, Florian Tramèr, Milad Nasr, et al.\n2022. Preventing verbatim memorization in language\nmodels gives a false sense of privacy. arXiv preprint\narXiv:2210.17546.\nShotaro Ishihara, Hiromu Takahashi, and Hono Shirai.\n2022. Semantic shift stability: Efficient way to detect\nperformance degradation of word embeddings and\npre-trained language models. In Proceedings of the\n2nd Conference of the Asia-Pacific Chapter of the As-\nsociation for Computational Linguistics and the 12th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 205–216,\nOnline only. Association for Computational Linguis-\ntics.\nAbhyuday Jagannatha, Bhanu Pratap Singh Rawat, and\nHong Yu. 2021. Membership inference attack suscep-\ntibility of clinical language models. arXiv preprint\narXiv:2104.08305.\nMatthew Jagielski, Om Thakkar, Florian Tramèr, et al.\n2023. Measuring forgetting of memorized training\nexamples. In Proceedings of the 11th International\nConference on Learning Representations.\n270\nMatthew Jagielski, Jonathan Ullman, and Alina Oprea.\n2020. Auditing differentially private machine learn-\ning: how private is private SGD? In Proceedings of\nthe 34th International Conference on Neural Infor-\nmation Processing Systems, number Article 1862 in\nNIPS’20, pages 22205–22216, Red Hook, NY , USA.\nCurran Associates Inc.\nPeter Kairouz, H Brendan McMahan, Brendan Avent,\net al. 2021. Advances and open problems in feder-\nated learning. Foundations and Trends® in Machine\nLearning, 14(1–2):1–210.\nNikhil Kandpal, Eric Wallace, and Colin Raffel. 2022.\nDeduplicating training data mitigates privacy risks in\nlanguage models. In Proceedings of the 39th Inter-\nnational Conference on Machine Learning, volume\n162 of Proceedings of Machine Learning Research,\npages 10697–10707. PMLR.\nJared Kaplan, Sam McCandlish, Tom Henighan, et al.\n2020. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361.\nYoshimasa Kawazoe, Daisaku Shibata, Emiko Shino-\nhara, et al. 2021. A clinical specific BERT developed\nusing a huge japanese clinical text corpus. PloS one,\n16(11):e0259763.\nKeita Kurita, Paul Michel, and Graham Neubig. 2020.\nWeight poisoning attacks on pretrained models. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 2793–\n2806, Online. Association for Computational Lin-\nguistics.\nHongkyu Lee, Jeehyeong Kim, Seyoung Ahn, et al.\n2021. Digestive neural networks: A novel defense\nstrategy against inference attacks in federated learn-\ning. Computers and Security, 109(C).\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, et al. 2020.\nBioBERT: a pre-trained biomedical language repre-\nsentation model for biomedical text mining. Bioin-\nformatics, 36(4):1234–1240.\nJooyoung Lee, Thai Le, Jinghui Chen, et al. 2023. Do\nlanguage models plagiarize? In Proceedings of\nthe ACM Web Conference 2023 , WWW ’23, page\n3637–3647, New York, NY , USA. Association for\nComputing Machinery.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. 2022. Deduplicating training\ndata makes language models better. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 8424–8445, Dublin, Ireland. Association for\nComputational Linguistics.\nEric Lehman, Sarthak Jain, Karl Pichotta, Yoav Gold-\nberg, and Byron Wallace. 2021. Does BERT pre-\ntrained on clinical notes reveal sensitive data? In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 946–959, Online. Association for Computa-\ntional Linguistics.\nDaniel Levy, Ziteng Sun, Kareem Amin, et al. 2021.\nLearning with user-level privacy. In Advances in\nNeural Information Processing Systems.\nSharon Levy, Emily Allaway, Melanie Subbiah, Lydia\nChilton, Desmond Patton, Kathleen McKeown, and\nWilliam Yang Wang. 2022. SafeText: A benchmark\nfor exploring physical safety in language models.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2407–2421, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nGen Li, Nan Duan, Yuejian Fang, Ming Gong, and\nDaxin Jiang. 2020. Unicoder-vl: A universal encoder\nfor vision and language by cross-modal pre-training.\nProceedings of the AAAI Conference on Artificial\nIntelligence, 34(07):11336–11344.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting ob-\njective function for neural conversation models. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 110–119, San Diego, California. Association\nfor Computational Linguistics.\nTiffany Li, Eduard Fosch Villaronga, and Peter Kiese-\nberg. 2018. Humans forget, machines remember:\nArtificial intelligence and the right to be forgotten.\nComputer Law & Security Review, 34(2):304.\nXuechen Li, Florian Tramer, Percy Liang, et al. 2022.\nLarge language models can be strong differentially\nprivate learners. In Proceedings of the 10th Interna-\ntional Conference on Learning Representations.\nJiasen Lu, Dhruv Batra, Devi Parikh, et al. 2019. Vil-\nbert: Pretraining task-agnostic visiolinguistic rep-\nresentations for vision-and-language tasks. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 32. Curran Associates, Inc.\nNils Lukas, Ahmed Salem, Robert Sim, et al. 2023. An-\nalyzing leakage of personally identifiable information\nin language models. In 2023 IEEE Symposium on\nSecurity and Privacy (SP).\nRenqian Luo, Liai Sun, Yingce Xia, et al. 2022.\nBioGPT: generative pre-trained transformer for\nbiomedical text generation and mining. Briefings\nin Bioinformatics, 23(6).\nSaeed Mahloujifar, Huseyin A Inan, Melissa Chase, et al.\n2021. Membership inference on word embedding\nand beyond. arXiv preprint arXiv:2106.11384.\nH. Brendan McMahan, Daniel Ramage, Kunal Talwar,\net al. 2018. Learning differentially private recurrent\nlanguage models. In Proceedings of the 6th Interna-\ntional Conference on Learning Representations.\n271\nCasey Meehan, Khalil Mrini, and Kamalika Chaudhuri.\n2022. Sentence-level privacy for document embed-\ndings. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 3367–3380, Dublin,\nIreland. Association for Computational Linguistics.\nAlex Mei, Anisha Kabir, Sharon Levy, Melanie Subbiah,\nEmily Allaway, John Judge, Desmond Patton, Bruce\nBimber, Kathleen McKeown, and William Yang\nWang. 2022. Mitigating covertly unsafe text within\nnatural language systems. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2022,\npages 2914–2926, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nLuca Melis, Congzheng Song, Emiliano De Cristofaro,\net al. 2019. Exploiting unintended feature leakage in\ncollaborative learning. In 2019 IEEE Symposium on\nSecurity and Privacy (SP), pages 691–706.\nTomas Mikolov, Martin Karafiát, Lukas Burget, et al.\n2010. Recurrent neural network based language\nmodel. In Interspeech, volume 2, pages 1045–1048.\nfit.vutbr.cz.\nFatemehsadat Mireshghallah, Kartik Goyal, Archit\nUniyal, Taylor Berg-Kirkpatrick, and Reza Shokri.\n2022a. Quantifying privacy risks of masked language\nmodels using membership inference attacks. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing , pages 8332–\n8347, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nFatemehsadat Mireshghallah, Huseyin Inan, Marcello\nHasegawa, Victor Rühle, Taylor Berg-Kirkpatrick,\nand Robert Sim. 2021. Privacy regularization: Joint\nprivacy-utility optimization in LanguageModels. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 3799–3807, Online. Association for Computa-\ntional Linguistics.\nFatemehsadat Mireshghallah, Archit Uniyal, Tianhao\nWang, David Evans, and Taylor Berg-Kirkpatrick.\n2022b. An empirical analysis of memorization in\nfine-tuned autoregressive language models. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1816–\n1826, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nJakob Mökander, Jonas Schuett, Hannah Rose Kirk,\nand Luciano Floridi. 2023. Auditing large language\nmodels: a three-layered approach. arXiv preprint\narXiv:2302.08500.\nYuta Nakamura, Shouhei Hanaoka, Yukihiro Nomura,\net al. 2020. KART: Parameterization of privacy\nleakage scenarios from pre-trained language mod-\nels. arXiv preprint arXiv:2101.00036.\nMilad Nasr, Reza Shokri, and Amir Houmansadr.\n2019. Comprehensive privacy analysis of deep\nlearning: Passive and active white-box inference at-\ntacks against centralized and federated learning. In\n2019 IEEE Symposium on Security and Privacy (SP),\npages 739–753.\nMilad Nasr, Shuang Song, Abhradeep Thakurta, et al.\n2021. Adversary instantiation: Lower bounds for\ndifferentially private machine learning. In2021 IEEE\nSymposium on Security and Privacy (SP), volume 0,\npages 866–882.\nHelen Nissenbaum. 2009. Privacy in Context. Stanford\nUniversity Press.\nOpenAI. 2023. GPT-4 technical report. arXiv preprint\narXiv:2303.08774.\nLong Ouyang, Jeff Wu, Xu Jiang, et al. 2022. Training\nlanguage models to follow instructions with human\nfeedback. In Advances in Neural Information Pro-\ncessing Systems.\nAshwinee Panda, Tong Wu, Jiachen T Wang, et al. 2023.\nDifferentially private In-Context learning. arXiv\npreprint arXiv:2305.01639.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nRahil Parikh, Christophe Dupuy, and Rahul Gupta. 2022.\nCanary extraction in natural language understanding\nmodels. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 552–560, Dublin,\nIreland. Association for Computational Linguistics.\nEthan Perez, Saffron Huang, Francis Song, et al. 2022.\nRed teaming language models with language models.\narXiv preprint arXiv:2202.03286.\nMartin Potthast, Benno Stein, Alberto Barrón-Cedeño,\nand Paolo Rosso. 2010. An evaluation framework for\nplagiarism detection. In Coling 2010: Posters, pages\n997–1005, Beijing, China. Coling 2010 Organizing\nCommittee.\nAlec Radford, Karthik Narasimhan, Tim Salimans, et al.\n2018. Improving language understanding by genera-\ntive pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, et al. 2019.\nLanguage models are unsupervised multitask learn-\ners. OpenAI blog, 1(8):9.\nSwaroop Ramaswamy, Om Thakkar, Rajiv Mathews,\net al. 2020. Training production language mod-\nels without memorizing user data. arXiv preprint\narXiv:2009.10031.\n272\nJingjing Ren, Ashwin Rao, Martina Lindorfer, et al.\n2016. ReCon: Revealing and controlling PII leaks\nin mobile network traffic. In Proceedings of the\n14th Annual International Conference on Mobile Sys-\ntems, Applications, and Services, MobiSys ’16, page\n361–374. Association for Computing Machinery.\nChanchal K Roy, James R Cordy, and Rainer Koschke.\n2009. Comparison and evaluation of code clone de-\ntection techniques and tools: A qualitative approach.\nScience of Computer Programming, 74(7):470–495.\nVirat Shejwalkar and Amir Houmansadr. 2021. Mem-\nbership privacy for machine learning models through\nknowledge transfer. Proceedings of the AAAI Con-\nference on Artificial Intelligence, 35(11):9549–9557.\nHoo-Chang Shin, Yang Zhang, Evelina Bakhturina,\nRaul Puri, Mostofa Patwary, Mohammad Shoeybi,\nand Raghav Mani. 2020. BioMegatron: Larger\nbiomedical domain language model. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4700–4706, Online. Association for Computational\nLinguistics.\nReza Shokri, Marco Stronati, Congzheng Song, et al.\n2017. Membership inference attacks against machine\nlearning models. In 2017 IEEE Symposium on Secu-\nrity and Privacy (SP), pages 3–18.\nKaran Singhal, Shekoofeh Azizi, Tao Tu, et al. 2022.\nLarge language models encode clinical knowledge.\narXiv preprint arXiv:2212.13138.\nShaden Smith, Mostofa Patwary, Brandon Norick,\net al. 2022. Using DeepSpeed and megatron\nto train Megatron-Turing NLG 530b, a Large-\nScale generative language model. arXiv preprint\narXiv:2201.11990.\nCongzheng Song and Ananth Raghunathan. 2020. In-\nformation leakage in embedding models. In Pro-\nceedings of the 2020 ACM SIGSAC Conference on\nComputer and Communications Security, CCS ’20,\npages 377–390, New York, NY , USA. Association\nfor Computing Machinery.\nCongzheng Song and Vitaly Shmatikov. 2019. Audit-\ning data provenance in Text-Generation models. In\nProceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery & Data Min-\ning, KDD ’19, pages 196–206, New York, NY , USA.\nAssociation for Computing Machinery.\nLiwei Song and Prateek Mittal. 2021. Systematic evalu-\nation of privacy risks of machine learning models. In\n30th USENIX Security Symposium (USENIX Security\n21), pages 2615–2632.\nYixuan Su, Tian Lan, Yan Wang, et al. 2022. A con-\ntrastive framework for neural text generation. In\nAdvances in Neural Information Processing Systems.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, et al.\n2022. Galactica: A large language model for science.\narXiv preprint arXiv:2211.09085.\nR Thomas McCoy, Paul Smolensky, Tal Linzen, et al.\n2021. How much do language models copy from\ntheir training data? evaluating linguistic novelty\nin text generation using RA VEN. arXiv preprint\narXiv:2111.09509.\nKushal Tirumala, Aram H. Markosyan, Luke Zettle-\nmoyer, et al. 2022. Memorization without overfitting:\nAnalyzing the training dynamics of large language\nmodels. In Advances in Neural Information Process-\ning Systems.\nFlorian Tramèr, Gautam Kamath, and Nicholas Car-\nlini. 2022. Considerations for differentially private\nlearning with Large-Scale public pretraining. arXiv\npreprint arXiv:2212.06470.\nThomas Vakili, Anastasios Lamproudis, Aron Henriks-\nson, and Hercules Dalianis. 2022. Downstream task\nperformance of BERT models pre-trained using auto-\nmatically de-identified clinical data. In Proceedings\nof the Thirteenth Language Resources and Evalua-\ntion Conference, pages 4245–4252, Marseille, France.\nEuropean Language Resources Association.\nGerrit van den Burg and Chris Williams. 2021. On\nmemorization in probabilistic deep generative mod-\nels. Advances in Neural Information Processing Sys-\ntems, 34:27916–27928.\nAshish Vaswani, Noam Shazeer, Niki Parmar, et al.\n2017. Attention is all you need. In Advances in\nNeural Information Processing Systems, volume 30.\nCurran Associates, Inc.\nPablo Villalobos, Jaime Sevilla, Lennart Heim, et al.\n2022. Will we run out of data? an analysis of the\nlimits of scaling datasets in machine learning. arXiv\npreprint arXiv:2211.04325.\nLaura Weidinger, Jonathan Uesato, Maribeth Rauh, et al.\n2022. Taxonomy of risks posed by language models.\nIn Proceedings of the 2022 ACM Conference on Fair-\nness, Accountability, and Transparency, FAccT ’22,\npages 214–229, New York, NY , USA. Association\nfor Computing Machinery.\nShijie Wu, Ozan Irsoy, Steven Lu, et al. 2023.\nBloombergGPT: A large language model for finance.\narXiv preprint arXiv:2303.17564.\nJingfeng Yang, Hongye Jin, Ruixiang Tang, et al.\n2023. Harnessing the power of LLMs in practice:\nA survey on ChatGPT and beyond. arXiv preprint\narXiv:2304.13712.\nXi Yang, Aokun Chen, Nima PourNejatian, et al. 2022.\nA large language model for electronic health records.\nNPJ digital medicine, 5(1):194.\nSamuel Yeom, Irene Giacomelli, Matt Fredrikson, et al.\n2018. Privacy risk in machine learning: Analyzing\nthe connection to overfitting. In 2018 IEEE 31st\nComputer Security Foundations Symposium (CSF),\npages 268–282.\n273\nYing Yin and Ivan Habernal. 2022. Privacy-preserving\nmodels for legal natural language processing. In Pro-\nceedings of the Natural Legal Language Processing\nWorkshop 2022, pages 172–183, Abu Dhabi, United\nArab Emirates (Hybrid). Association for Computa-\ntional Linguistics.\nDa Yu, Saurabh Naik, Arturs Backurs, et al. 2022. Dif-\nferentially private fine-tuning of language models. In\nProceedings of the 10th International Conference on\nLearning Representations.\nDa Yu, Huishuai Zhang, Wei Chen, et al. 2021. Large\nscale private learning via low-rank reparametrization.\nIn Proceedings of the 38th International Conference\non Machine Learning, volume 139 of Proceedings\nof Machine Learning Research, pages 12208–12218.\nPMLR.\nChen Zhang, Yu Xie, Hang Bai, et al. 2021a. A survey\non federated learning. Knowledge-Based Systems,\n216:106775.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, et al.\n2021b. Understanding deep learning (still) requires\nrethinking generalization. Communications of the\nACM, 64(3):107–115.\nChiyuan Zhang, Daphne Ippolito, Katherine Lee, et al.\n2021c. Counterfactual memorization in neural lan-\nguage models. arXiv preprint arXiv:2112.12938.\nRuisi Zhang, Seira Hidano, and Farinaz Koushanfar.\n2022. Text revealer: Private text reconstruction via\nmodel inversion attacks against transformers. arXiv\npreprint arXiv:2209.10505.\nYuheng Zhang, Ruoxi Jia, Hengzhi Pei, et al. 2020. The\nsecret revealer: Generative model-inversion attacks\nagainst deep neural networks. In 2020 IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition (CVPR). IEEE.\nLucia Zheng, Neel Guha, Brandon R Anderson, et al.\n2021. When does pretraining help? assessing self-\nsupervised learning for law and the CaseHOLD\ndataset of 53,000+ legal holdings. In Proceedings\nof the 18th International Conference on Artificial\nIntelligence and Law , ICAIL ’21, pages 159–168,\nNew York, NY , USA. Association for Computing\nMachinery.\nTianqing Zhu, Dayong Ye, Shuai Zhou, et al. 2023.\nLabel-only model inversion attacks: Attack with the\nleast information. IEEE Transactions on Information\nForensics and Security, 18:991–1005.\nA Type of Decoding\nTwo classes of methods, namely deterministic and\nstochastic, are used for decoding (Su et al., 2022).\nIn the deterministic method, the most probable\ntokens based on the probability distribution of\nthe model are used. Greedy methods and beam\nsearches are widely used. However, studies have re-\nvealed that simply maximizing the output probabil-\nity generates text that is not natural to humans (Li\net al., 2016; Holtzman et al., 2020). Therefore,\nseveral approaches have been proposed for sam-\npling from a probability distribution. Stochastic\nmethods include top-k sampling (Fan et al., 2018),\ntop-p sampling, and nucleus sampling (Holtzman\net al., 2020), in which samples are extracted from\nthe lexical subset. A method to adjust the proba-\nbility distribution using the temperature parameter\nwas used to increase the diversity of the generated\ntexts (Ackley et al., 1985).\nIn the candidate generation step in Section 4.1,\ntexts can be generated from PLMs using several\ndecoding methods. Some studies adopted a greedy\nmethod (Carlini et al., 2023b). Others used top-\nk sampling (Carlini et al., 2021; Lee et al., 2022)\nand tuned the temperature (Carlini et al., 2021) to\nincrease the diversity of the generated texts.\nB Scaling Law for Language Models\nBuilding PLMs requires large datasets. Studies\nhave proposed models with larger parameters pre-\ntrained with large datasets (Smith et al., 2022;\nChowdhery et al., 2022). Experimental results re-\nvealed the existence of a scaling law (Kaplan et al.,\n2020; Henighan et al., 2020). This study suggested\nthat the performance of language models using the\ntransformer improves as the model size, dataset\nsize, and amount of computation increase. Villalo-\nbos et al. (2022) cautioned that the data available\nfor pre-training language models may be exhausted\nin the near future.\nC Patterns of Adversarial Knowledge\nTable 1 presents the patterns of adversarial knowl-\nedge of the models and Table 2 details the pat-\nterns of adversarial knowledge of the training set.\nThese tables provide specific patterns. For example,\nwhite-box for models indicates PLMs published\non platforms such as Hugging Face 9 with train-\ning explanations, which can be downloaded. As\ndiscussed in Section 4.2, two main types, namely\nwhite and black boxes, exist. In black-box settings,\nseveral patterns depend on the situation. Table 1\nreveals the classification of the black-box proposed\nby Hu et al. (2022): full confidence scores, top-k\nconfidence scores, and prediction labels. In Table 2,\n9https://huggingface.co/models\n274\nAdversarial knowledge Model or the output Pattern\nwhite-box all Models are available with proper explanations.\nblack-box full confidence scores All outputs of models are available.\ntop-k confidence scores Top-k outputs of models are available.\nprediction label only Only prediction labels are available.\nTable 1: Adversarial knowledge of models and patterns.\nAdversarial knowledge Training set Pattern\nwhite-box all Dataset used for training is stated and publicly available.\nblack-box partial Dataset used for training is stated but not available.\nDataset used for training is stated and partially available.\nnothing Dataset used for training is not stated.\nTable 2: Adversarial knowledge of training sets and patterns.\nseveral possible patterns of adversarial knowledge\nare presented on training sets.\n275",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7815617322921753
    },
    {
      "name": "Data extraction",
      "score": 0.5720502734184265
    },
    {
      "name": "Software deployment",
      "score": 0.5697252154350281
    },
    {
      "name": "Memorization",
      "score": 0.5497952103614807
    },
    {
      "name": "Taxonomy (biology)",
      "score": 0.5053499341011047
    },
    {
      "name": "Training set",
      "score": 0.46838051080703735
    },
    {
      "name": "Natural language",
      "score": 0.44122129678726196
    },
    {
      "name": "Training (meteorology)",
      "score": 0.42818647623062134
    },
    {
      "name": "Natural language understanding",
      "score": 0.41300103068351746
    },
    {
      "name": "Data science",
      "score": 0.39041388034820557
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38855302333831787
    },
    {
      "name": "Natural language processing",
      "score": 0.33167803287506104
    },
    {
      "name": "Psychology",
      "score": 0.1106874942779541
    },
    {
      "name": "Software engineering",
      "score": 0.09469494223594666
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "MEDLINE",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Mathematics education",
      "score": 0.0
    },
    {
      "name": "Meteorology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210114208",
      "name": "Nikkei Business Publications (Japan)",
      "country": "JP"
    }
  ]
}