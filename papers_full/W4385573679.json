{
  "title": "Don’t Prompt, Search! Mining-based Zero-Shot Learning with Language Models",
  "url": "https://openalex.org/W4385573679",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3150863713",
      "name": "Mozes van de Kar",
      "affiliations": [
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A2947173880",
      "name": "Mengzhou Xia",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2095803999",
      "name": "Danqi Chen",
      "affiliations": [
        "Princeton University"
      ]
    },
    {
      "id": "https://openalex.org/A2176075802",
      "name": "Mikel Artetxe",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3099403624",
    "https://openalex.org/W3164972323",
    "https://openalex.org/W2124634352",
    "https://openalex.org/W2163455955",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4221160826",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4225373076",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W3106109117",
    "https://openalex.org/W3197876970",
    "https://openalex.org/W4226176833",
    "https://openalex.org/W4221149883",
    "https://openalex.org/W197270748",
    "https://openalex.org/W4206636317",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W4221167546",
    "https://openalex.org/W4281784180",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4281481998",
    "https://openalex.org/W4221151371",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W4205857304",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4253067820"
  ],
  "abstract": "Masked language models like BERT can perform text classification in a zero-shot fashion by reformulating downstream tasks as text infilling. However, this approach is highly sensitive to the template used to prompt the model, yet practitioners are blind when designing them in strict zero-shot settings. In this paper, we propose an alternative mining-based approach for zero-shot learning. Instead of prompting language models, we use regular expressions to mine labeled examples from unlabeled corpora, which can optionally be filtered through prompting, and used to finetune a pretrained model. Our method is more flexible and interpretable than prompting, and outperforms it on a wide range of tasks when using comparable templates. Our results suggest that the success of prompting can partly be explained by the model being exposed to similar examples during pretraining, which can be directly retrieved through regular expressions.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7508–7520\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nDon’t Prompt, Search!\nMining-based Zero-Shot Learning with Language Models\nMozes van de Kar1 Mengzhou Xia2 Danqi Chen2 Mikel Artetxe3\n1University of Amsterdam 2Princeton University 3Meta AI\nmozesvandekar@gmail.com {mengzhou,danqic}@cs.princeton.edu\nartetxe@meta.com\nAbstract\nMasked language models like BERT can per-\nform text classification in a zero-shot fashion by\nreformulating downstream tasks as text infill-\ning. However, this approach is highly sensitive\nto the template used to prompt the model, yet\npractitioners are blind when designing them\nin strict zero-shot settings. In this paper, we\npropose an alternative mining-based approach\nfor zero-shot learning. Instead of prompting\nlanguage models, we use regular expressions\nto mine labeled examples1 from unlabeled cor-\npora, which can optionally be filtered through\nprompting, and used to finetune a pretrained\nmodel. Our method is more flexible and inter-\npretable than prompting, and outperforms it on\na wide range of tasks when using comparable\ntemplates. Our results suggest that the success\nof prompting can partly be explained by the\nmodel being exposed to similar examples dur-\ning pretraining, which can be directly retrieved\nthrough regular expressions.\n1 Introduction\nRecent work has obtained strong zero-shot results\nby prompting language models (Brown et al., 2020;\nChowdhery et al., 2022). As formalized by Schick\nand Schütze (2021a), the core idea is to reformulate\ntext classification as language modeling using a\npattern and a verbalizer. Given the input space X,\nthe output space C and the space of possible strings\nV ∗, the pattern t : X → V ∗maps each input into a\nstring with a masked span, whereas the verbalizer\nv : C → V ∗ maps each label into a string. A\nlanguage model can then be used for zero-shot\nclassification by picking the most likely completion\nfor the masked textarg maxc∈C p(v(c) | t(x)).2 In\n1We use ‘labeled examples’ throughout the paper to denote\nthe examples that match regex-based patterns of different\nlabels. They are weakly-supervised and can be noisy.\n2We focus on masked language models, and allow multi-\ntoken verbalizers through autoregressive decoding (see §3).\nLeft-to-right language models also fit the framework by plac-\ning the mask at the end or scoring the full populated prompt.\nIt is [mask]. \nIt was too scary. \n2\nFilter  with zero-shot prompting \ndiscard\n-\nMLM\nLabel mismatch?\nTask Descr ipt ion\n3\nFinetune  a  pretrained model on the mined dataset\npattern:       (is|was) {VERBALIZER}. {INPUT}\nverbalizer:  good  / bad\n… it. The film  is good. That is …The actor played well. \n1\nMining : match pattern and extract  {INPUT}\n+ -\nIt was not funny at all.\nThe actor played well. \n+ -\nIt was not funny at all.\nThe actor played well. \nTex t Cor pus\nFigure 1: Proposed method. 1) We mine labeled ex-\namples from a text corpus with regex-based patterns.\n2) Optionally, we filter examples for which zero-shot\nprompting predicts a different label. 3) We finetune a\npretrained language model with a classification head.\nfew-shot settings, better results can be obtained by\nprepending a few labeled examples (Brown et al.,\n2020), or using them in some form of fine-tuning\n(Schick and Schütze, 2021a; Gao et al., 2021).\nHowever, prompting is known to be sensitive to\nthe choice of the pattern and the verbalizer, yet prac-\ntitioners are blind when designing them in true zero-\nshot settings (Jiang et al., 2020; Perez et al., 2021).\nConnected to that, subtle phenomena like the sur-\nface form competition (Holtzman et al., 2021) have\na large impact on performance. Recent work has\ntried to mitigate these issues through calibration\n(Zhao et al., 2021), prompt combination (Schick\nand Schütze, 2021a; Lester et al., 2021; Zhou et al.,\n2022) or automatic prompt generation (Shin et al.,\n2020; Gao et al., 2021). At the same time, there\nis still not a principled understanding of how lan-\nguage models become few-shot learners, with re-\ncent work analyzing the role of the pretraining data\n(Chan et al., 2022) or the input-output mapping of\n7508\nTask Prompting pattern Mining pattern\nSentiment {INPUT}. It was {VERBALIZER} . (is|was) {VERBALIZER}*. {INPUT}\nTopic class. {INPUT}. It is about {VERBALIZER} . {VERBALIZER}*. {INPUT}\nNLI {INPUT:HYP} {VERBALIZER}, {INPUT:PREM} {INPUT:HYP} {VERBALIZER}, {INPUT:PREM}\nTable 1: Patterns. {VERBALIZER} is replaced with the verbalizers in Table 2. For mining, *. captures everything\nup to a sentence boundary, and {INPUT}, {INPUT:HYP} and {INPUT:PREM} capture a single sentence.\nTask Lbl Verbalizers\nSent. Pos. goodgoodgoodgoodgoodgoodgoodgoodgoodgoodgoodgoodgoodgoodgoodgoodgood, great, awesome, incredible\nNeg. badbadbadbadbadbadbadbadbadbadbadbadbadbadbadbadbad, awful, terrible, horrible\nNLI\nEnt. YesYesYesYesYesYesYesYesYesYesYesYesYesYesYesYesYes, Therefore, Thus, Accordingly,\nHence, For this reason\nCon. NoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNo, However, But, On the contrary,\nIn contrast\nNeu. MaybeMaybeMaybeMaybeMaybeMaybeMaybeMaybeMaybeMaybeMaybeMaybeMaybeMaybeMaybeMaybeMaybe, Also, Furthermore, Secondly,\nAdditionally, Moreover,In addition\nTable 2: Verbalizers for sentiment classification and\nNLI. See Table 9 for verbalizers used in topic classifica-\ntion. When using a single verbalizer, we choose the one\nunderlined. Multi-token verbalizers are in italic. Lbl:\nlabel, Ent./Con./Neu: entailment, contradiction, neutral.\nin-context demonstrations (Min et al., 2022).\nIn this paper, we propose an alternative approach\nto zero-shot learning that is more flexible and inter-\npretable than prompting, while obtaining stronger\nresults in our experiments. Similar to prompting,\nour method requires a pretrained language model,\npattern, and verbalizer, in addition to an unlabeled\ncorpus (e.g., the one used for pretraining). As il-\nlustrated in Figure 1, our approach works by using\nthe pattern and verbalizer to mine labeled examples\nfrom the corpus through regular expressions, and\nleveraging them as supervision to finetune the pre-\ntrained language model. This allows to naturally\ncombine multiple patterns and verbalizers for each\ntask, while providing a signal to interactively de-\nsign them by looking at the mined examples. In\naddition, we show that better results are obtained\nby filtering the mined examples through prompting.\nExperiments in sentiment analysis, topic\nclassification and natural language inference (NLI)\nconfirm the effectiveness of our approach, which\noutperforms prompting by a large margin when\nusing the exact same verbalizers and comparable\npatterns. Our results offer a new perspective on\nhow language models can perform downstream\ntasks in a zero-shot fashion, showing that similar\nexamples often exist in the pretraining corpus,\nwhich can be directly retrieved through simple\nextraction patterns.\n2 Proposed Method\nAs shown in Figure 1, our method has three steps:\nMine. We first use the pattern and a set of verbal-\nizers to extract labeled examples from the corpus.\nTo that end, we define patterns that are filled with\nverbalizers and expanded into regular expressions.\nFor instance, the pattern and verbalizer in Figure 1\nwould extract every sentence following “is good. ”\nor “was good. ”as an example of the positive class,\nand every sentence following “is bad. ” or “was\nbad. ”as an example of the negative class. In prac-\ntice, the patterns that we define are comparable to\nthe ones used for prompting, and the verbalizers\nare exactly the same (see Tables 1 and 2). Ap-\npendix A gives more details on how we expand\npatterns into regular expressions. While prior work\nin prompting typically uses a single verbalizer per\nclass, our approach allows to naturally combine\nexamples mined through multiple verbalizers in a\nsingle dataset. So as to mitigate class imbalance\nand keep the mined dataset to a reasonable size, we\nmine a maximum of 40k examples per class after\nbalancing across the different verbalizers.\nFilter. As an optional second step, we explore\nautomatically removing noisy examples from the\nmined data. To that end, we classify the mined\nexamples using zero-shot prompting, and remove\nexamples for which the predicted and the mined\nlabel do not match. This filtering step is reliant on\nthe performance of prompting, and we only remove\n10% of the mismatching examples for which zero-\nshot prompting is the most confident.\nFinetune. Finally, we use the mined dataset to\nfinetune a pretrained language model in the stan-\ndard supervised fashion (Devlin et al., 2019), learn-\ning a new classification head.\n3 Experimental Settings\nTasks. We evaluate on three types of tasks: bi-\nnary sentiment analysis on Amazon (Zhang et al.,\n7509\nSentiment analysis Topic class. NLI avg\namz imd mr sst ylp agn dbp yah mnl qnl rte snl\nFull-shot Fine-tuning 97.1 95.7 88.8 94.4 95.0 95.1 99.3 76.8 78.5 92.6 67.6 90.5 89.3\nZero-shot\nPrompting 81.5 78.4 71.1 77.4 81.9 34.0 36.4 28.2 47.1 50.8 52.3 39.6 56.6\nw/ multi verb. 83.5 81.8 78.3 81.9 83.1 54.6 51.1 34.1 46.5 58.2 61.4 44.1 63.2\nProposed method 92.0 86.7 80.5 85.6 92.0 79.2 80.4 56.1 50.4 53.2 62.6 46.0 72.0\nTable 3: Main results (accuracy). All systems are based on RoBERTa-base, and all zero-shot systems use\ncomparable patterns (see Table 1). We report average accuracy across 3 runs for all systems except prompting. w/\nmulti verb.: prompting with different sets of verbalizers (Table 9) and averaging the probabilities.\n2015), IMDb (Maas et al., 2011), MR (Pang and\nLee, 2005), SST-2 (Socher et al., 2013) and Yelp\n(Zhang et al., 2015), topic classification on AG\nNews (Zhang et al., 2015), DBPedia (Zhang et al.,\n2015) and Yahoo Topics3 (Zhang et al., 2015), and\nNLI on MNLI (Williams et al., 2018), QNLI (Ra-\njpurkar et al., 2016), RTE (Dagan et al., 2005;\nBar-Haim et al., 2006; Giampiccolo et al., 2007;\nBentivogli et al., 2009) and SNLI (Bowman et al.,\n2015). We report accuracy on the test set when\navailable, falling back to the validation set for SST-\n2, MNLI, RTE and QNLI. For all systems involv-\ning fine-tuning, we report the average across 3 runs\nwith different random seeds. We ran all develop-\nment experiments on SST-2 and AG News without\nany exhaustive hyperparameter exploration, and\nevaluate the rest of the tasks blindly.\nApproaches. We compare the following meth-\nods in our experiments, using RoBERTa-base (Liu\net al., 2019) as the pretrained model in all cases:\n• Full-shot fine-tuning: We finetune RoBERTa\non the original training set adding a new clas-\nsification head. We train for 3 epochs with a\nbatch size of 32. All the other hyperparameters\nfollow Liu et al. (2019). Refer to Appendix B\nfor more details.\n• Zero-shot prompting: Standard prompting, de-\nscribed in §1. Multi-token verbalizer proba-\nbilities are calculated autoregressively, picking\nthe most likely token at each step (Schick and\nSchütze, 2021c). We report results using both\na single verbalizer per class, as it is common\nin prior work, as well as multiple verbalizers\nper class, which is more comparable to our ap-\nproach. For the latter, we combine the probabil-\n3The Yahoo Answers dataset was downloaded by, and\naccess was limited to, the University of Amsterdam, where all\nexperiments were carried out.\nPrompting Mining\ngood/ bad 78.1 72.0\ngreat/ awful 82.3 82.1\nawesome/ terrible 82.3 83.9\nincredible/ horrible 83.1 87.3\ncombined 81.7 85.4\nTable 4: Average sentiment accuracy using different\nverbalizers. We report mining results without filtering.\nMore detailed results are provided in Table 12.\nities of each verbalizer by averaging.4\n• Zero-shot mining: Our proposed method, de-\nscribed in §2. For the mining step, we use the\nfirst 100 shards from the C4 corpus (Raffel et al.,\n2020), which cover 9.8% of the data. For the\nfiltering step, we use single-verbalizer prompt-\ning to filter 10% of the mislabeled examples.\nFor the fine-tuning step, we use the same set-\ntings as in the full-shot setup, except that we\ntrain for 5,000 steps with a dropout probability\nof 0.4.5 To mitigate class imbalance, we form\nbatches by first sampling the class for each in-\nstance from the uniform distribution, and then\npicking a random example from the mined data\nbelonging to that class.\nPatterns and verbalizers. We use comparable\npatterns for prompting and mining with the exact\nsame verbalizers, which we report in Table 1 and 2.\nThese were designed without any experiment, sim-\nulating a zero-shot setting. We design our patterns\n4We also tried summing or taking the maximum, which\nobtained similar results as shown in Appendix C.\n5During development, we found that high dropout and\nearly stopping help mitigating model overfitting caused by\nthe misalignment between the mined and the true distribution.\nHowever, evaluation on all tasks shows mixed results. We\nstick to the original setup with high dropout to be faithful to\nthe rigorous zero-shot scenario, and report additional results\nwith standard dropout in Appendix C.\n7510\nPattern Verbalizer avg\nPrompting\n{VERBALIZER} stars: {INPUT} 5 / 1 51.0\n{INPUT} I {VERBALIZER} it. love / hate 73.1\n{INPUT} It is {VERBALIZER}. good / bad 78.1\nMining\n{VERBALIZER} star*. {INPUT} 5 / 1 72.1\nI {VERBALIZER}*. {INPUT} love / hate 85.5\n(is|was) {VERBALIZER}*. {INPUT} good / bad 72.0\nTable 5: Average sentiment accuracy using different\npatterns and verbalizers. We report mining results\nwithout filtering (more details are provided in Table 13).\nData Filter Sent. Topic NLI\nSupervised - 94.2 90.4 82.3\nMined\nnone 85.4 71.5 52.0\nprompting 87.4 71.9 53.0\nsupervised† 91.4 85.5 63.8\nTable 6: Filtering results (average accuracy). †: uses\nmined data for training and another supervised classifier\nas the filter. This is not a zero-shot setting and serves as\nan upper limit for the results using a perfect filter. More\ndetailed results are provided in Table 11.\nto capture sentences following a verbalizer, rather\nthan sentences containing the verbalizer, as the re-\nsulting dataset would otherwise be trivial (solvable\nby detecting the presence of certain words).\n4 Results and Analysis\nWe next discuss our main findings and report addi-\ntional results in Appendix C.\nMain results. We report our main results in Ta-\nble 3. Our method outperforms prompting by 8.8\npoints on average, and the improvements are con-\nsistent across all tasks.\nEffect of patterns and verbalizers. Table 4 re-\nports sentiment results using different verbaliz-\ners. Consistent with prior work, we find that both\nprompting and mining are highly sensitive to the\nchoice of the verbalizer, yet combining them all\nroughly matches the results of the best performing\none. As shown in Table 5, using different patterns\nhas an even larger impact. Interestingly, patterns\nand verbalizers that do well with one approach do\nnot necessarily do well with the other.\nEffect of filtering. Table 6 reports additional re-\nsults using the full-shot systems for filtering, or not\n# Lbl Mined example\n1 Pos. Do you have an idea of how broad your vocal\nrange was?\n2 Pos. Once home, we began priming.\n3 Neg. People in Wall Street and other financial services\nfirms should have paid more attention to the data.\n4 Neg. So I bought this unit, which said it had the same\ntechnical features as the other brand, such as\nnumber of channels etc, and this one performed\namazing!!\nTable 7: Mined examples for sentiment analysis. See\nmore examples in Table 17 and mined NLI examples in\nTable 18.\nusing any filtering at all. We find that prompting-\nbased filtering brings modest but consistent im-\nprovements across all types of tasks. We compare\nthis to filtering out all examples with mismatching\nlabels with the full-shot model, which results in\nmuch larger gains and approaches the performance\nof the fully supervised system for sentiment and\ntopic classification tasks. This can be seen as an\nupper-limit of what could be reached with perfect\nfiltering, which leaves ample room to improve our\napproach focusing on the filtering step alone.\nQualitative analysis. We manually assessed 20\nmined examples for sentiment analysis and report\nsome representative instances in Table 7. We find\nthat the mined data covers many domains like fi-\nnance and technology. Most examples are correct\n(#1, #3), but there are also instances with wrong\nlabels (#4). In addition, we find that 40% of an-\nalyzed examples show weak or neutral sentiment\n(#2). The impact of such irrelevant examples is\nunclear and worth of future study.\n5 Related work\nRecent work in zero-shot learning has explored\na similar generate-filter-finetune approach, but\nusing large language models instead of mining to\ngenerate training data (Schick and Schütze, 2021b;\nLiu et al., 2022; Meng et al., 2022; Ye et al., 2022).\nMining-based approaches have a long tradition\nin information extraction (Riloff, 1996; Riloff and\nJones, 1999). However, to the best of our knowl-\nedge, we are the first to apply them for zero-shot\nlearning as an alternative to prompting. Instead of\nmining examples for the target task, Bansal et al.\n(2020) define task-agnostic pretraining objectives\non unlabeled corpora. Closer to our work, Meng\net al. (2020) mask label-indicative words in an\n7511\nunlabeled corpus, and train a model to predict\ntheir corresponding label. Concurrent to our work,\nHan and Tsvetkov (2022) try locating a subset\nof the pretraining data that supports prompting\nin specific tasks. Finally, Razeghi et al. (2022)\nshow a strong correlation between performance on\nspecific instances and the frequency of terms from\nthose instances in the pretraining data.\n6 Conclusions\nIn this work, we have shown that mining-based\nzero-shot learning outperforms prompting. More-\nover, our approach shows headroom for further im-\nprovement by exploring filtering techniques. The\nflexibility of our approach enables additional direc-\ntions like domain filtering, bootstrapping, and inter-\nactive pattern/verbalizer design, where practition-\ners would inspect a few mined examples and refine\ntheir patterns until they are satisfied. In addition,\nour methods can serve as a partial explanation for\nwhy prompting works, showing that task-relevant\nexamples are often present in the pretraining corpus\nin an explicit form, to the extent that they can be\ndirectly mined through simple regular expressions.\nNevertheless, we believe that there can be other fac-\ntors involved, as evidenced by the best patterns and\nverbalizers being different for mining and prompt-\ning, and we believe that delving deeper into the\nrelation between pretraining data and prompting\nperformance is an interesting future direction.\nLimitations\nDeveloping zero-shot methods in a rigorous man-\nner is challenging: the strict zero-shot scenario\ndoes not allow using annotated data except for the\nfinal evaluation, yet it is difficult to make devel-\nopment decisions without any signal. We decided\nto use AG News and SST-2 during development\nwithout any exhaustive hyperparameter exploration,\nand evaluate blindly in the rest of the tasks. At the\nsame time, we designed all patterns and verbalizers\nwithout any experiment, based solely on our own\nintuition. We believe that the comparison between\nprompting and mining is fair as we used compa-\nrable patterns with the exact same verbalizers and\npretrained model. However, it is possible that our\npatterns, verbalizers and/or hyperparameters are\nsuboptimal, and better results could be obtained\nwith either prompting or mining using other con-\nfigurations.\nAn important limitation of our approach is that it\ncan be difficult to design extraction patterns for cer-\ntain tasks like multiple choice questions. However,\nprompting is known to suffer from a similar limi-\ntation, with certain tasks like WiC being difficult\nto formulate as language modeling and obtaining\nrandom chance performance (Brown et al., 2020).\nDifferent from prompting, our approach requires\nan intermediate step after pretraining to mine data\nand finetune the model, which takes 2-7 hours using\na single Nvidia Titan RTX GPU and 4 Intel Xeon\nCPUs. However, inference cost is similar or even\nfaster than prompting, as our approach does not\nincur on any overhead for multi-token and multi-\nverbalizer setups.\nAcknowledgements\nWe thank Ves Stoyanov, Jingfei Du, Timo Schick\nand Sewon Min for their feedback. Mozes van\nde Kar received a travel grant from ELLIS and\nQualcomm to attend the conference.\nReferences\nTrapit Bansal, Rishikesh Jha, Tsendsuren Munkhdalai,\nand Andrew McCallum. 2020. Self-supervised meta-\nlearning for few-shot natural language classification\ntasks. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 522–534, Online. Association for\nComputational Linguistics.\nRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,\nDanilo Giampiccolo, Bernardo Magnini, and Idan\nSzpektor. 2006. The second pascal recognising tex-\ntual entailment challenge. In Proceedings of the sec-\nond PASCAL challenges workshop on recognising\ntextual entailment, volume 6. Venice.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The fifth pascal recognizing\ntextual entailment challenge. In TAC.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\n7512\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nStephanie C. Y . Chan, Adam Santoro, Andrew K.\nLampinen, Jane X. Wang, Aaditya Singh, Pierre H.\nRichemond, Jay McClelland, and Felix Hill. 2022.\nData distributional properties drive emergent in-\ncontext learning in transformers.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment chal-\nlenge. In Machine Learning Challenges Workshop,\npages 177–190. Springer.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan, and\nBill Dolan. 2007. The third PASCAL recognizing\ntextual entailment challenge. In Proceedings of the\nACL-PASCAL Workshop on Textual Entailment and\nParaphrasing, pages 1–9, Prague. Association for\nComputational Linguistics.\nXiaochuang Han and Yulia Tsvetkov. 2022. Orca: In-\nterpreting prompted language models via locating\nsupporting data evidence in the ocean of pretraining\ndata. arXiv preprint arXiv:2205.12600.\nAri Holtzman, Peter West, Vered Shwartz, Yejin Choi,\nand Luke Zettlemoyer. 2021. Surface form com-\npetition: Why the highest probability answer isn’t\nalways right. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7038–7051, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nQuentin Lhoest, Albert Villanova del Moral, Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\nLewis Tunstall, Joe Davison, Mario Šaško, Gun-\njan Chhablani, Bhavitvya Malik, Simon Brandeis,\nTeven Le Scao, Victor Sanh, Canwen Xu, Nicolas\nPatry, Angelina McMillan-Major, Philipp Schmid,\nSylvain Gugger, Clément Delangue, Théo Matus-\nsière, Lysandre Debut, Stas Bekman, Pierric Cis-\ntac, Thibault Goehringer, Victor Mustar, François\nLagunas, Alexander Rush, and Thomas Wolf. 2021.\nDatasets: A community library for natural language\nprocessing. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning: System Demonstrations, pages 175–184, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nAlisa Liu, Swabha Swayamdipta, Noah A. Smith, and\nYejin Choi. 2022. Wanli: Worker and ai collaboration\nfor natural language inference dataset creation.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 142–150, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\n7513\nYu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han.\n2022. Generating training data with language models:\nTowards zero-shot language understanding.\nYu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong,\nHeng Ji, Chao Zhang, and Jiawei Han. 2020. Text\nclassification using label names only: A language\nmodel self-training approach. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 9006–9017,\nOnline. Association for Computational Linguistics.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work?\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploit-\ning class relationships for sentiment categorization\nwith respect to rating scales. In Proceedings of the\n43rd Annual Meeting of the Association for Compu-\ntational Linguistics (ACL’05), pages 115–124, Ann\nArbor, Michigan. Association for Computational Lin-\nguistics.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue few-shot learning with language models. In\nAdvances in Neural Information Processing Systems.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nYasaman Razeghi, Robert L. Logan, Matt Gardner, and\nSameer Singh. 2022. Impact of pretraining term\nfrequencies on few-shot reasoning.\nEllen Riloff. 1996. Automatically generating extraction\npatterns from untagged text. In Proceedings of the\nThirteenth National Conference on Artificial Intelli-\ngence - Volume 2, AAAI’96, page 1044–1049. AAAI\nPress.\nEllen Riloff and Rosie Jones. 1999. Learning dictio-\nnaries for information extraction by multi-level boot-\nstrapping. In Proceedings of the Sixteenth National\nConference on Artificial Intelligence and the Eleventh\nInnovative Applications of Artificial Intelligence Con-\nference Innovative Applications of Artificial Intelli-\ngence, AAAI ’99/IAAI ’99, page 474–479, USA.\nAmerican Association for Artificial Intelligence.\nTimo Schick and Hinrich Schütze. 2021a. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255–269, Online. Association for Computa-\ntional Linguistics.\nTimo Schick and Hinrich Schütze. 2021b. Generating\ndatasets with pretrained language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 6943–\n6951, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nTimo Schick and Hinrich Schütze. 2021c. It’s not just\nsize that matters: Small language models are also few-\nshot learners. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2339–2352, Online. Association\nfor Computational Linguistics.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric\nWallace, and Sameer Singh. 2020. AutoPrompt: Elic-\niting Knowledge from Language Models with Auto-\nmatically Generated Prompts. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4222–4235,\nOnline. Association for Computational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nJiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu, Jiangtao\nFeng, Zhiyong Wu, Tao Yu, and Lingpeng Kong.\n2022. Zerogen: Efficient zero-shot learning via\ndataset generation.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In Advances in Neural Information Pro-\ncessing Systems, volume 28. Curran Associates, Inc.\nTony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models.\nChunting Zhou, Junxian He, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2022. Prompt con-\nsistency for zero-shot task generalization.\n7514\nA Pattern expansion for mining\nFor each class, examples are mined by filling in\nthe pattern with the verbalizer and extracting sen-\ntences that match the filled-in pattern. The process\nof expanding the patterns into regular expressions\nis as follows. First, we replace {VERBALIZER} with\na capturing group containing all verbalizers sepa-\nrated by the alternation operator |. For example,\nthe verbalizer good, great, awesome is expanded\ninto (good|great|awesome). Finally, we replace\nthe keywords described in Table 8 with the cor-\nresponding regular expressions. The result is a\nregular expression containing capturing groups for\nextracting sentences in a case-insensitive fashion.\nNote that we use a simplistic sentence definition\nin order to keep the regex manageable. Since we\nassume that a period always ends a sentence, this\nmistakenly interprets abbreviations as multiple sen-\ntences (e.g., “U.S.A. ”contains 3 sentences). To\naddress this, we filter out mined sentences shorter\nthan 4 characters.\nB Additional experimental details\nPatterns and verbalizers. For each category of\ntasks we use the same mining pattern, as shown\nin Table 1. The complete list of verbalizers for\neach task is given in Table 9. Tasks with the same\nclasses share the same verbalizers. This means that\nall sentiment and NLI tasks have the same verbal-\nizers. Each topic classification task, however, has a\nunique set of verbalizers. Note that while SNLI and\nMNLI (3-way NLI) have the same verbalizers as\nRTE and QNLI (2-way NLI), the mined datasets do\ndiffer since 2-way NLI does not include a neutral\nclass.\nHyperparameters. Table 10 shows the hyperpa-\nrameters used for finetuning the RoBERTa-base\nmodel. All the other hyperparameters and classi-\nfication head architecture follow Liu et al. (2019).\nWe have two fine-tuning configurations, one for\nfine-tuning in the full-shot setting and one for zero-\nshot fine-tuning on the mined dataset. These con-\nfigurations differ only in the maximum number of\nsteps, dropout rate and batch sampler.\nDatasets. We use Huggingface (Lhoest et al.,\n2021) for loading all evaluation datasets without\nany additional processing, except for MR which\nis detokenized using Moses scripts. We evaluate\non the test set, falling back to the validation set for\nSST-2, MNLI, RTE and QNLI.\nKeyword Regex\n{VERBALIZER} Replaced with the verbalizer\n* regex: [^.!?]*?\nGreedily matches non-sentence-\nending characters\n{INPUT} regex: ([^.!?]+[.!?]+)\nMatches a single sentence, extracted\nwith the key “INPUT”\nTable 8: Keywords that compile into regular expres-\nsions. These keywords are used in the mining patterns\nand verbalizers.\nC Additional results\nComplete results for full-shot, prompting and min-\ning are combined in Table 11. Results showing the\neffect of pattern and verbalizer choice on binary\nsentiment classification are presented in Table 12\nand Table 13, respectively.\nAs explained in the main text, development ex-\nperiments were only conducted on AGNews and\nSST-2. On these tasks, we found that high regular-\nization partially mitigates overfitting caused by the\nmisalignment between the mined dataset and real\ndataset. However, this high regularization shows\nmixed results for non-development tasks. For full\ntransparency, we compare these performance differ-\nences in Table 14, but, in the main text, we stick to\nthe original setup with high dropout to be faithful\nto the rigorous zero-shot scenario.\nFor multi-verbalizer prompting, we combine the\nprobabilities of each verbalizer with an aggregation\nfunction. Results for using the average, the max\nand the sum are shown in Table 15.\nIn Table 16 we show the agreement between\nthe mined labels and the labels according to the\nfiltering method, which in our experiments is ei-\nther a full-shot finetuned model or single-verbalizer\nprompting.\nTable 17 and Table 18 show a random sample\nof examples from the mined training dataset for\nrespectively binary sentiment analysis and NLI. In\nthe main text, Table 7 shows a representative se-\nlection of examples for sentiment analysis. These\nexamples were manually picked from the random\nsample in Table 17.\n7515\nTask Class Verbalizers\nSentiment Positive goodgoodgoodgoodgoodgoodgoodgoodgoodgoodgoodgoodgoodgoodgoodgoodgood, great, awesome, incredible\nNegative badbadbadbadbadbadbadbadbadbadbadbadbadbadbadbadbad, awful, terrible, horrible\nAGNews\nWorld worldworldworldworldworldworldworldworldworldworldworldworldworldworldworldworldworld, foreign, global, Asia, Europe, China\nSports sportssportssportssportssportssportssportssportssportssportssportssportssportssportssportssportssports, football, basketball, tennis, soccer, baseball\nBusiness businessbusinessbusinessbusinessbusinessbusinessbusinessbusinessbusinessbusinessbusinessbusinessbusinessbusinessbusinessbusinessbusiness, stock, financial, profit, economy, finance\nSci/Tech technologytechnologytechnologytechnologytechnologytechnologytechnologytechnologytechnologytechnologytechnologytechnologytechnologytechnologytechnologytechnologytechnology, science, research, chemical, iPhone, smartphone\nDBPedia\nCompany companycompanycompanycompanycompanycompanycompanycompanycompanycompanycompanycompanycompanycompanycompanycompanycompany, business, manufacturer, operates in\nEducational institution schoolschoolschoolschoolschoolschoolschoolschoolschoolschoolschoolschoolschoolschoolschoolschoolschool, college, education, university\nArtist artistartistartistartistartistartistartistartistartistartistartistartistartistartistartistartistartist, writer, song, composer\nAthlete sportssportssportssportssportssportssportssportssportssportssportssportssportssportssportssportssports, runner, basketball, football\nOffice holder politicspoliticspoliticspoliticspoliticspoliticspoliticspoliticspoliticspoliticspoliticspoliticspoliticspoliticspoliticspoliticspolitics, president, Senate, politician\nMean of transportation busbusbusbusbusbusbusbusbusbusbusbusbusbusbusbusbus, bike, car, train, ship, plane, aircraft\nBuilding buildingbuildingbuildingbuildingbuildingbuildingbuildingbuildingbuildingbuildingbuildingbuildingbuildingbuildingbuildingbuildingbuilding, office, house, monument\nNatural place riverriverriverriverriverriverriverriverriverriverriverriverriverriverriverriverriver, forest hill, nature\nVillage towntowntowntowntowntowntowntowntowntowntowntowntowntowntowntowntown, village, small population, small town\nAnimal animalanimalanimalanimalanimalanimalanimalanimalanimalanimalanimalanimalanimalanimalanimalanimalanimal, species, horse, dog, pet, habitat\nPlant plantplantplantplantplantplantplantplantplantplantplantplantplantplantplantplantplant, leaf, flower, herb\nAlbum albumalbumalbumalbumalbumalbumalbumalbumalbumalbumalbumalbumalbumalbumalbumalbumalbum,recording, record company\nFilm filmfilmfilmfilmfilmfilmfilmfilmfilmfilmfilmfilmfilmfilmfilmfilmfilm, movie, actor, actress\nWritten work writtenwrittenwrittenwrittenwrittenwrittenwrittenwrittenwrittenwrittenwrittenwrittenwrittenwrittenwrittenwrittenwritten, book, novel, poem\nYahoo\nSociety & Culture culturecultureculturecultureculturecultureculturecultureculturecultureculturecultureculturecultureculturecultureculture, holiday, society\nScience & Mathematics sciencesciencesciencesciencesciencesciencesciencesciencesciencesciencesciencesciencesciencesciencesciencesciencescience, technology, math, research\nHealth healthhealthhealthhealthhealthhealthhealthhealthhealthhealthhealthhealthhealthhealthhealthhealthhealth, body, exercise, stress relieve\nEducation & Reference schoolschoolschoolschoolschoolschoolschoolschoolschoolschoolschoolschoolschoolschoolschoolschoolschool, college, education, university\nComputers & Internet computercomputercomputercomputercomputercomputercomputercomputercomputercomputercomputercomputercomputercomputercomputercomputercomputer, internet, keyboard, software\nSports sportssportssportssportssportssportssportssportssportssportssportssportssportssportssportssportssports, football, basketball, game\nBusiness & Finance businessbusinessbusinessbusinessbusinessbusinessbusinessbusinessbusinessbusinessbusinessbusinessbusinessbusinessbusinessbusinessbusiness, stock, financial, profit\nEntertainment & Music filmfilmfilmfilmfilmfilmfilmfilmfilmfilmfilmfilmfilmfilmfilmfilmfilm, movie, actor, writer\nFamily & Relationships lovelovelovelovelovelovelovelovelovelovelovelovelovelovelovelovelove, family, father, mother\nPolitics & Government politicspoliticspoliticspoliticspoliticspoliticspoliticspoliticspoliticspoliticspoliticspoliticspoliticspoliticspoliticspoliticspolitics, president, Senate, politician\nNLI\nEntailment YesYesYesYesYesYesYesYesYesYesYesYesYesYesYesYesYes, Therefore, Thus, Accordingly, Hence,For this reason\nContradiction NoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNoNo, However, But, On the contrary, In contrast\nNeutral MaybeMaybeMaybeMaybeMaybeMaybeMaybeMaybeMaybeMaybeMaybeMaybeMaybeMaybeMaybeMaybeMaybe, Also, Furthermore, Secondly, Additionally, Moreover, In addition\nTable 9: Verbalizers. When using a single verbalizer we choose the one underlined. In the multi-verbalizer setting\nwe use all listed verbalizers. Sentiment includes Amazon, IMDB, MR, SST-2 and Yelp; NLI includes MNLI, QNLI,\nRTE and SNLI. Multi-token verbalizers are italic.\nParameter full-shot zero-shot\nModel RoBERTa-base (123M) RoBERTa-base (123M)\nModel selection last last\nBatch size 32 32\nOptimizer adam adam\nLearning rate 1.00e-05 1.00e-05\nLR schedule 6% warmup with linear\ndecay\n6% warmup with linear\ndecay\nAdam epsilon 1.00e-08 1.00e-08\nAdam β1 0.9 0.9\nAdam β1 0.999 0.999\nWeight decay 0 0\nClassifier dropout 0 0\nAttention dropout 0.1 0.1\nHidden dropout 0.1 0.4\nMax steps - 5000\nMax epochs 3 -\nBatch sampler - inverse class frequency\nweighted sampling\nTable 10: Hyperparameters for full-shot finetuning and zero-shot finetuning with the mined dataset.\n7516\nFull-shot Prompting Mining\nfine- single- multi- single- multi- + filter\nmaj tuning verb verb verb verb full-shot zero-shot\nSentiment Analysis\nAmazon 50.0 97.1 ±0.0 81.5 83.5 71.7 ±2.2 90.6±1.7 94.4±0.2 92.0±0.6\nIMDB 50.0 95.7 ±0.0 78.4 81.8 78.2 ±0.5 83.0±4.1 91.6±0.4 86.7±1.3\nMR 50.0 88.8 ±0.1 71.1 78.3 70.5 ±1.0 77.7±2.4 86.3±0.7 80.5±1.0\nSST-2 50.9 94.4 ±0.2 77.4 77.4 77.3 ±1.4 83.8±2.4 89.5±0.7 85.6±1.1\nYelp 50.0 95.0 ±0.1 81.9 83.1 62.1 ±2.3 92.0±2.2 95.2±0.6 92.0±1.5\navg 50.2 94.2 78.1 81.7 72.0 85.4 91.4 87.4\nTopic Classification\nAGNews 25.0 95.1 ±0.1 34.0 54.6 71.0 ±0.7 78.4±0.6 89.5±0.2 79.2±0.6\nDBPedia 7.1 99.3 ±0.0 36.4 51.1 63.7 ±1.0 79.8±0.0 97.1±0.3 80.4±0.4\nYahoo 10.0 76.8 ±0.1 28.2 34.1 51.7 ±0.5 56.3±2.5 69.8±0.1 56.1±2.1\navg 14.0 90.4 32.9 46.6 62.1 71.5 85.5 71.9\nNLI\nMNLI 35.3 78.5 0.0 47.1 46.5 48.2 ±0.5 49.2±0.5 65.5±0.3 50.4±0.4\nQNLI 50.5 92.6 ±0.1 50.8 58.2 51.6 ±0.3 52.9±1.2 70.2±1.1 53.2±0.6\nRTE 52.7 67.6 ±1.4 52.3 61.4 55.0 ±0.2 61.1±2.1 57.8±1.3 62.6±0.9\nSNLI 34.3 90.5 ±0.1 39.6 44.1 38.0 ±0.7 44.6±0.9 61.9±2.8 46.0±1.1\navg 41.6 82.3 47.5 52.5 48.2 52.0 63.8 53.0\nmacro avg 38.6 89.3 56.6 63.2 61.6 70.8 80.7 72.1\nTable 11: Complete results for each task and system. When applicable, results show fine-tuning average\nperformance and standard deviation over 3 seeds. Full-shot shows the fine-tuning results with the hyperparameters\ndescribed in Table 10. Prompting shows the single and multi-verbalizer baseline results. Mining results show single\nand multi-verbalizer performance without filtering, in addition to multi-verbalizer performance with full-shot and\nzero-shot filtering. Maj: majority baseline\nAmazon IMDB MR SST-2 Yelp avg\nPrompting\ngood / bad 81.5 78.4 71.1 77.3 81.9 78.1\ngreat / awful 82.9 82.7 80.8 82.6 82.6 82.3\nawesome / terrible 84.5 82.0 78.3 82.8 84.0 82.3\nincredible / horrible 86.6 83.5 78.0 80.0 87.2 83.1\ncombined 83.5 81.8 78.3 81.9 83.1 81.7\nMining\ngood / bad 71.7±2.2 78.2±0.5 70.5±1.0 77.3±1.4 62.1±2.3 72.0\ngreat / awful 88.4±1.7 75.7±4.0 73.0±2.5 79.8±2.1 93.3±0.5 82.1\nawesome / terrible 91.4±0.6 81.3±1.4 73.7±1.6 78.8±1.3 94.3±0.7 83.9\nincredible / horrible 90.5±0.2 88.1±0.2 80.5±1.0 83.5±0.6 94.0±0.4 87.3\ncombined 90.6 ±1.7 83.0±4.1 77.7±2.4 83.8±2.4 92.0±2.2 85.4\nTable 12: Verbalizer comparison for sentiment tasks. For mining, we report average performance and standard\ndeviation over 3 seeds without filtering.\nPattern Verbalizer Amazon IMDB MR SST-2 Yelp avg\nPrompting\n{VERBALIZER} stars: {INPUT} 5 / 1 50.4 50.0 50.0 50.9 53.7 51.0\n{INPUT} I {VERBALIZER} it. love / hate 77.6 73.3 64.6 69.8 80.3 73.1\n{INPUT} It is {VERBALIZER}. good / bad 81.5 78.4 71.1 77.4 81.9 78.1\nMining\n{VERBALIZER} star*. {INPUT} 5 / 1 68.7±0.4 75.4±2.1 70.8±1.7 78.3±2.4 67.5±5.3 72.1\ni {VERBALIZER}*. {INPUT} love / hate 88.9±0.3 84.0±1.2 79.2±0.8 84.0±0.8 91.1±0.7 85.5\n(is|was) {VERBALIZER}*. {INPUT} good / bad 71.7±2.2 78.2±0.5 70.5±1.0 77.3±1.4 62.1±2.3 72.0\nTable 13: Template comparison. Performance for three different templates on sentiment tasks comparing prompting\nand mining without filtering. Additionally, we show standard deviations over three seeds for the mining approach.\nThe verbalizer column shows the verbalizer for the positive and the negative class, respectively.\n7517\nDefault dropout High dropout\nfull-shot zero-shot full-shot zero-shot\nSentiment Analysis\nAmazon 95.8 ±0.0 91.0±0.4 94.4±0.2 92.0±0.6\nIMDB 94.4 ±0.2 80.1±4.0 91.6±0.4 86.7±1.3\nMR 88.0 ±0.2 76.5±1.1 86.3±0.7 80.5±1.0\nSST-2 92.0 ±0.5 80.2±1.6 89.5±0.7 85.6±1.1\nYelp 96.6 ±0.1 94.4±0.6 95.2±0.6 92.0±1.5\navg 93.3 84.4 91.4 87.4\nTopic Classification\nAGNews 90.8 ±0.2 77.2±1.0 89.5±0.2 79.2±0.6\nDBPedia 98.8 ±0.0 83.9±0.4 97.1±0.3 80.4±0.3\nYahoo 72.9 ±0.1 56.6±2.4 69.8±0.1 56.1±2.1\navg 87.5 72.5 85.5 71.9\nNLI\nMNLI 77.7 ±1.0 52.9±0.8 65.5±0.3 50.4±0.4\nQNLI 77.0 ±1.6 57.8±0.9 70.2±1.1 53.2±0.6\nRTE 72.3 ±1.1 61.4±2.9 57.8±1.3 62.6±0.9\nSNLI 80.2 ±0.2 49.4±1.0 61.9±2.8 46.0±1.1\navg 76.8 55.4 ±0.4 63.8 53.0\nmacro avg 86.4 71.8 80.7 72.1\nTable 14: Performance with high dropout and default dropout. These results use our proposed mining method +\nfiltering and compares 2 settings of the hidden layer dropout: the default setting of 0.1 and the high regularization\nsetting of 0.4, the value that was found most effective during development experiments on AGNews and SST-2.\nAveraging Max Sum\nSentiment Analysis\nAmazon 83.5 82.7 83.5\nIMDB 81.8 81.2 82.1\nMR 78.3 77.4 78.3\nSST-2 81.9 81.3 81.9\nYelp 83.1 82.3 83.1\nTopic Classification\nAGNews 54.6 53.4 54.6\nDBPedia 51.1 49.1 51.4\nYahoo 34.1 34.1 34.1\nNLI\nMNLI 46.5 46.4 47.0\nQNLI 58.2 58.4 57.4\nRTE 61.4 61.7 60.6\nSNLI 44.1 42.8 43.7\nTable 15: Results for different probability aggregation functions for multi-verbalizer prompting.\n7518\nFull-shot Prompting\nSentiment Analysis\nAmazon 68.7 63.6\nIMDB 64.5 63.6\nMR 66.1 63.6\nSST-2 66.7 63.6\nYelp 69.1 63.6\nTopic Classification\nAGNews 52.3 31.8\nDBPedia 25.7 13.4\nYahoo 36.4 22.2\nNLI\nMNLI 39.2 42.0\nQNLI 52.7 62.2\nRTE 50.3 62.2\nSNLI 40.8 42.0\nTable 16: Label agreement. Percentage of examples for which the mined label is equal to the label predicted by a\nfull-shot model or by single-verbalizer prompting.\n# Mined Label Mined Example\n1 Negative So I bought this unit, which said it had the same technical features as the other brand, such as\nnumber of channels etc, and this one performed amazing!!\n2 Positive The founders of Clickfunnels have focused not only on creating a great internet site for you yet also\noffering you enough expertise and details to act as an informed company person / entrepreneur.\n3 Positive Once home, we began priming.\n4 Negative While you can ’ t view the content on the second page without either logging in or signing up since\nthere ’ s no ‘ x ’ button, there ’ s a trick involving 3D Touch that can help.\n5 Positive That i savored them a long way a great deal more compared to My spouse and i believed i would\ncertainly.\n6 Positive Do you have an idea of how broad your vocal range was?\n7 Positive Also recently the lovely Megan Washington modelled for our latest transeasonal collection we just\nshot last week.\n8 Positive What are 3 things that make you happy?\n9 Positive Be devoted to one another in love.\n10 Negative An unexpected situation arose with my father requiring help for three to four months.\n11 Positive Simply AWESOME!\n12 Negative I don ’ t disagree with that, however the voices have never been as loud or as many as now about\nthis topic of “anti TPS ”, that is progress, that is a movement, that is what we need to inspire EA to\nfinally listen and do something about it.\n13 Positive Adverse drug reactions are based on evaluation of data from pre-marketing phase 2-3 studies and\nupdated based on pooled data from 18 placebo-controlled pre- and post-marketing studies, including\napproximately 5,000 patients treated with varenicline.\n14 Negative I have the books from last year and have spoke to the college to get this years as they may be\nchanging to another type.\n15 Negative I was down-to-my-core terrified.\n16 Negative He didn ’ t win, and our support of him became rather limited when we determined he was not\nwinning.\n17 Positive It was in a four-star hotel in the Boca Raton Resort Bungalows.\n18 Positive I wish my preschool was this nice.\n19 Negative People in Wall Street and other financial services firms should have paid more attention to the data.\n20 Positive I guess I am quite transparent.\nTable 17: Random sample of mined examples for sentiment analysis.\n7519\n# Mined Label Mined Premise Mined Hypothesis\n1 Neutral When seniors and their caregivers develop and\nenjoy deep friendships, it can bolster feelings of\nwell-being, happiness and security.\nit can help to make activities like exercising and\nhealthy eating more enjoyable.\n2 Contradiction Customer service skills training for staff should in-\nclude basic customer service skills of active listen-\ning, how to handle difficult situations, telephone\netiquette, and interpersonal communication tips.\nmost of all, it should focus on how to build a\nrelationship with a student.\n3 Entailment The easy way to remember the arrhythmias most\ncommonly associated with SSS — is to think of\nwhat one might expect if the SA node became\n“sick ”.\nthere is sinus bradycardia and arrhythmia — si-\nnus pauses (which may be longlasting, ultimately\nleading to sinus arrest) — and SA nodal block.\n4 Contradiction Primarily due to President Obama ’ s historic an-\nnouncement, more Americans are visiting Cuba,\nmaking a bad situation worse.\nfor those of you with the ability to book your\nCuban Rent A Car in advance, all the above offi-\ncial websites still offer availability at the writing\nof this article.\n5 Neutral In 1989, Tufts University School of Medicine cre-\nated the Minority High School Tutorial PLUS Pro-\ngram to provide local minority / disadvantaged\nstudents with access to medical student tutors.\nin 1989, Tufts University School of Medicine\nreceived a grant from the National Institutes of\nHealth (NIH) to start the Minority High School\nResearch Apprenticeship Program.\n6 Contradiction In fact, regardless of the cost escalations on those\nother projects, legislators are doing their job by\nshowing such prudence here.\nwhen all is said and done, the legislature should\napprove the project aimed at repairing and upgrad-\ning seats and improving lighting and drainage at\nthe facility.\n7 Entailment Also, this is the root of consciousness, because\nconsciousness, awareness needs an opposite, a\ncounterpart, a border, to awake at.\nconsciousness is a form of pain, originally, defi-\nnitely.\n8 Contradiction Does Amaryl cause hair loss? the use of Amaryl does not cause hair loss.\n9 Entailment “Golay has got no serious issues. he is resorting to such statements for cheap popu-\nlarity, ” remarked Bhim Dahal, the spokesperson\nof ruling Sikkim Democratic Front (SDF).\n10 Neutral The point is, it has to go. I’ll be removing a lot of the buttons in favor of\ntextual links, and will probably replace them with\na single button promoting Firefox.\n11 Contradiction Most of the times precisely originating from a\nsincere analysis of the weaknesses, the tension\nfield between opposites and the assembling of\ncross-functional teams, through the clash of di-\nverse approaches and views, the influence of “ca-\nreer changers ” from other fields, and openness to-\nwards the new, the unorthodox, the unpredictable.\nwithout diminishing the importance of diversity,\ntogetherness is what produces the most over-\nwhelming feeling of success.\n12 Entailment Beef and poultry safety tips are essential to follow. an effective and healthy way to lose weight is to\nget regular exercise and harmful toxins, as soon\nas you have to eat properly.\n13 Neutral One vehicle that is widely chosen is a motorbike\nto carry out daily activities.\nthe matic motor is very easy to operate.\n14 Entailment Storage companies are often located near major\ntravel routes to make it easier for customers to\naccess the facility.\nyou may see signs for local rental companies in\nyour area at the side of the motorway, or major\nroutes near your location.\n15 Entailment Connecting with people about a negative experi-\nence often equates to a positive outcome.\nI ’ ve decided to list all the ‘ abnormal ’ things I\ndo but wouldn ’ t usually talk about.\n16 Entailment A smile does go a long way! March 13, coming soon.\n17 Contradiction Accordingly, it is a cultural taboo to affirm, “I am\nLove, ” which is our Authentic Self, the Immanent\nDivine Essence that we all share.\nthe Rishis who wrote the Upanishads realized that\nBrahman and Atman — as the Absolute and Self,\nrespectively — are One, declaring Tat tvam asi ‘\nYou are That.\n18 Contradiction In these countries, ceramic proppants are used\nmainly in wells with higher closure pressures and\nother challenging environments.\nceramic proppants are the leading proppant type\nin the Chinese and Russian markets.\n19 Contradiction Given her recent prognosis and the fact that she\nDOES drive us mental with her meltdown and\nseemingly erratic behaviour sometimes, I really\nneed to count to 10 and not lose my rag with her\nmore than I do.\nwith her reaction to breakfast this morning and\nsubsequent meltdown, I don’t think I could have\nchosen a more difficult resolution...\n20 Entailment The Best Khao Soi in Chiang Mai, Thailand, is in\na Mall!\nI said it . . . the best Khao Soi in Chiang Mai,\nThailand, is in Central Airport Plaza Mall.\nTable 18: Random sample of mined examples for NLI.\n7520",
  "topic": "Zero (linguistics)",
  "concepts": [
    {
      "name": "Zero (linguistics)",
      "score": 0.7687854170799255
    },
    {
      "name": "Computer science",
      "score": 0.7446539402008057
    },
    {
      "name": "Shot (pellet)",
      "score": 0.6583042144775391
    },
    {
      "name": "Language model",
      "score": 0.569048285484314
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5481834411621094
    },
    {
      "name": "Natural language processing",
      "score": 0.5356603860855103
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.49481624364852905
    },
    {
      "name": "Template",
      "score": 0.4848860204219818
    },
    {
      "name": "Speech recognition",
      "score": 0.3274821937084198
    },
    {
      "name": "Linguistics",
      "score": 0.09252330660820007
    },
    {
      "name": "Programming language",
      "score": 0.08434072136878967
    },
    {
      "name": "Engineering",
      "score": 0.06555831432342529
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Aerospace engineering",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I887064364",
      "name": "University of Amsterdam",
      "country": "NL"
    },
    {
      "id": "https://openalex.org/I20089843",
      "name": "Princeton University",
      "country": "US"
    }
  ]
}