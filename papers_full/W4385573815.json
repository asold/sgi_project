{
  "title": "CDGP: Automatic Cloze Distractor Generation based on Pre-trained Language Model",
  "url": "https://openalex.org/W4385573815",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2282946026",
      "name": "Shang-Hsuan Chiang",
      "affiliations": [
        "National Chung Hsing University"
      ]
    },
    {
      "id": "https://openalex.org/A5111019906",
      "name": "Ssu-Cheng Wang",
      "affiliations": [
        "National Chung Hsing University"
      ]
    },
    {
      "id": "https://openalex.org/A4227228426",
      "name": "Yao-Chung Fan",
      "affiliations": [
        "National Chung Hsing University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2889048825",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3017596848",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W2140695848",
    "https://openalex.org/W19376526",
    "https://openalex.org/W2805070499",
    "https://openalex.org/W2251056936",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2963123047"
  ],
  "abstract": "Manually designing cloze test consumes enormous time and efforts. The major challenge lies in wrong option (distractor) selection. Having carefully-design distractors improves the effectiveness of learner ability assessment. As a result, the idea of automatically generating cloze distractor is motivated. In this paper, we investigate cloze distractor generation by exploring the employment of pre-trained language models (PLMs) as an alternative for candidate distractor generation. Experiments show that the PLM-enhanced model brings a substantial performance improvement. Our best performing model advances the state-of-the-art result from 14.94 to 34.17 (NDCG@10 score). Our code and dataset is available at https://github.com/AndyChiangSH/CDGP.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5835–5840\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nCDGP: Automatic Cloze Distractor Generation based on Pre-trained\nLanguage Model\nShang-Hsuan Chiang and Ssu-Cheng Wang and Yao-Chung Fan\nDepartment of Computer Science and Engineering,\nNational Chung Hsing University, Taichung, Taiwan\nAbstract\nManually designing cloze test consumes enor-\nmous time and efforts. The major challenge\nlies in wrong option (distractor) selection. Hav-\ning carefully-design distractors improves the\neffectiveness of learner ability assessment. As\na result, the idea of automatically generating\ncloze distractor is motivated. In this paper,\nwe investigate cloze distractor generation by\nexploring the employment of pre-trained lan-\nguage models (PLMs) as an alternative for can-\ndidate distractor generation. Experiments show\nthat the PLM-enhanced model brings a sub-\nstantial performance improvement. Our best\nperforming model advances the state-of-the-\nart result from 14.94 to 34.17 (NDCG@10\nscore). Our code and dataset is available at\nhttps://github.com/AndyChiangSH/CDGP.\n1 Introduction\nA cloze test is an assessment consisting of a por-\ntion of language with certain words removed (cloze\ntext), where the participant is asked to select the\nmissing language item from a given set of options.\nSpecifically, a cloze question (as illustrated in Fig-\nure 1) is composed by a sentence with a word re-\nmoved (a blank space) and list of options (one an-\nswer and three wrong options).\nThe cloze test with carefully-design distractors\ncan improve the effectiveness of learner ability as-\nsessment. However, manually designing cloze test\nconsumes enormous time and efforts. The major\nchallenge lies in wrong option (distractor) selection.\nAs a result, automatic cloze distractor generation\nis proposed (Ren and Q. Zhu, 2021; Kumar et al.,\n2015; Narendra et al., 2013).\nIn this paper, we extend the candidate-ranking\nframework reported in (Ren and Q. Zhu, 2021)\nby exploring the employment of PLMs as an al-\nternative for candidate distractor generation. In\nthis paper, we propose a cloze distractor generation\nframework called CDGP (Automatic Cloze Distrac-\ntor Generation based on PLMs) which incorporates\nFigure 1: A Cloze Test Example: the challenge to cloze\ntest preparation lies in wrong option selection. A good\nwrong option selection improve the effectiveness of\nlearner ability assessment.\na serial of training and ranking strategies to boost\nthe performance of distractor generation based on\nPLMs.\nThe contribution of this work is as follows.\n• We show that PLM-based methods brings sig-\nnificant performance improvement over the\nknowledge-driven methods (Ren and Q. Zhu,\n2021) (generating candidates from Probase\n(Wu et al., 2012) or Wordnet (Miller, 1995))\n• We conduct evaluation using two benchmark-\ning datasets. The experiment results indicates\nthat our CDGP significantly outperforms the\nstate-of-the-art result (Ren and Q. Zhu, 2021).\nWe advance NDCG@10 score from 19.31 to\n34.17 (improving up to 177%).\n2 Related Work\nThe methods on cloze distractor generation can\nbe sorted into the following two categories. The\nfirst category (Correia et al., 2010; Lee and Sen-\neff, 2007) is to prepare cloze distractors based on\nlinguistic heuristic rules. The problem with these\nmethods is that the results are far from practically\nsatisfactory. The second category (Kumar et al.,\n2015; Narendra et al., 2013) is to construct candi-\ndate distractors from domain-specific vocabulary\nor taxonomies and employ classifiers for selecting\nfinal distractors. The results by the methods of this\ncategory are still less than satisfactory due to the\n5835\nFigure 2: CDGP Framework\ndomain generalization and the generation quality.\nTo improve the quality, (Ren and Q. Zhu, 2021)\nproposes to use knowledge bases (Wordnet (Miller,\n1995) and Probase (Wu et al., 2012)) to analyze the\nword semantic and hypernym-hyponym relations\nfor generating candidate distractors. In this paper,\nwe explore the employment of PLMs as a alterna-\ntive for the knowledge bases in (Ren and Q. Zhu,\n2021) and also explore various linguistic features\nfor candidate selection.\n3 Methodology\n3.1 CDGP Framework\nWe extend the framework proposed by (Ren and\nQ. Zhu, 2021) by exploring the employment of\npre-trained language models as an alternative for\ncandidate distractor generation. Specifically, as\nillustrated in Figure 2, the framework consists of\ntwo stages: (1) Candidate Set Generator (CSG)\nand (2) Distractor Selector (DS). In this paper, we\nrevisit the framework by considering (1) PLMs at\nCSG and (2) various features at DS.\n3.2 Candidate Set Generator (CSG)\nThe input to CSG is a question stem and the corre-\nsponding answer. The output is a distractors candi-\ndate set of size k.\nIn this study, we use PLM to generate candidates.\nLet M() be PLM model. For a given training in-\nstance (S,A,D ), where Sis a cloze stem, Ais the\nanswer, and Dis a distractor. We explore the fol-\nlowing two training setting for generating distractor\ncandidates.\n1. Naive Fine-Tune:\nM(S⊗[Mask]) →D\nThe input is a given stem S with the cloze\nblank filled in [Mask] (denoted by S⊗[Mask]).\nThe idea is to fine-tune the PLMs to predictD.\nThe training objective is to find a parameter\nset θminimizing the following loss function\n−log(p(D|S; θ))\n2. Answer-Relating Fine-Tune: The input is\nfurther concatenated with cloze answer A.\nThe idea is to guide the model to refer Ato\ngenerate D. Specifically,\nM(S⊗[Mask][Sep]A) →D\nThe training objective is to find a parameter\nset θminimizing the following loss function\n−log(p(D|S,A; θ))\n3.3 Distractor Selector (DS)\nThe input to DS is a question stem S, an answer A,\nand a candidate set {Di} from CSG. We investigate\nthe following features for ranking candidates.\n• Confidence Score s0: the confidence score of\nDi given by the PLM at CSG. Specifically,\ns0 = p(Di|S,A; θ)\n• Word Embedding Similarity s1: the word em-\nbedding score between Aand Dgiven by the\ncosine similarity between ⃗Aand ⃗D. Specifi-\ncally,\ns1 = 1−cos( ⃗A, ⃗Di)\n• Contextual-Sentence Embedding Similarity\ns2: the sentence-level cosine similarity be-\ntween the stem with the blank filled in A(de-\nnoted by ⃗S⊗A) and the stem with the blank\nfilled in D(denoted by ⃗S⊗Di ).\ns2 = 1−cos(⃗S⊗A,⃗S⊗Di )\n• POS match score s3: the POS (part-of-speech)\nmatching indicator. s3 = 1, if Aand Di has\nthe same POS tag. Otherwise s3 = 0.\n5836\nDataset CLOTH-M CLOTH-H CLOTH (Total)traindev test traindev test train dev test#passages2341355 355 3172450 478 5513805 813#questions22056327331985479477948318768501106711516V ocab. size 15096 32212 37235Avg. #sentence16.26 18.92 17.79Avg. #words 242.88 365.1 313.16\nTable 1: The statistics of the training, developing and\ntesting datasets of CLOTH-M (middle school),\nCLOTH-H (high school). (Xie et al., 2017)\nDataset Short-term Long-term\nGM STR MP LTR O\nCLOTH 0.265 0.503 0.044 0.180 0.007\nCLOTH-M 0.330 0.413 0.068 0.174 0.014\nCLOTH-H 0.240 0.539 0.035 0.183 0.004\nTable 2: The question type statistics of 3000 sampled\nquestions where GM, STR, MP, LTR and O denotes\ngrammar, short-term-reasoning, matching paraphrasing,\nlong-term-reasoning and others respectively. (Xie et al.,\n2017)\nThe final score of a distractor Di is then com-\nputed by a weighted sum over the individual score\nwith MinMax normalization.\nscore(Di) =\n3∑\ni=0\nwi ·MinMax-Norm(si)\nDistractors with Top-3 scores are selected as the\nfinal resultant distractors.\n4 Performance Evaluation\n4.1 Dataset\nTo validate the performance of our methodology,\nwe use the following two datasets.\n• CLOTH datatset (Xie et al., 2017) The\ndataset comes from English cloze exercises.\nThe datasets consists of a passage with cloze\nstems, answers and distractors. The data statis-\ntics are summarized in Table 1 and Table 2.\n• DGen dataset (Ren and Q. Zhu, 2021) The\nDGen dataset released by (Ren and Q. Zhu,\n2021), which is a reorganized dataset from\nSciQ (Welbl et al., 2017) and MCQL (Liang\net al., 2018). We compare our methods with\nthe SOTA method (Ren and Q. Zhu, 2021)\nbased on this dataset. The data statistics are\nlisted in Table 3 and Table 4.\n4.2 Implementation Details\nWe select bert-base-uncased (Devlin et al., 2018)\nas the default PLM. We use Adam optimizer with\nData Split # of questions\ntotal 2880\ntrain 2321\nvalid 300\ntest 259\nTable 3: DGen statistics. (Ren and Q. Zhu, 2021)\nDomain TotalScienceV ocab.Commen\nSense Trivia\n# of questions2880 758 956 706 460\n#of distractors3.13 3.00 3.99 3.48 2.99\nTable 4: DGen statistics in different domains. (Ren and\nQ. Zhu, 2021)\nan initial learning rate setting to 0.0001. We set\nthe PLM maximal input length to 64. The default\nbatch size is set to 64. All models are trained with\nNVIDIA® Tesla T4.\nFor computing word embedding similarity in DS,\nwe use the fasttext model (Bojanowski et al., 2016)\nas the default embedding model. The fasttext is\ntrained with the cbow setting. The minimal and\nmaximal n-gram parameter are set to 3 and 6. The\nvector dimension is set to 100. The initial learn-\ning rate is 0.05. In addition, the size of distractor\ncandidate set kis set to 10 as a default value.\n4.3 Evaluation Metric\nAutomatic Score We use the same setting of\n(Ren and Q. Zhu, 2021); the models are com-\npared by the following automatic scores: Precision\n(P@1), F1 score (F1@3, F1@10), Mean Recipro-\ncal Rank (MRR@10), and Normalized Discounted\nCumulative Gain (NDCG@10).\n4.4 Evaluation Results\n4.4.1 Results on DGen\nIn this set of experiment, our goal is to compare our\nmethod with the SOTA method (Ren and Q. Zhu,\n2021). Table 5 shows the comparison results. In\naddition to the BERT model, we also report our\nCDGP variants based on (SciBERT, RoBERTa,\nand BART). From Table 5, it can be seen that the\nNDCG@10 of CDGP with SciBERT was improved\nfrom 19.31 to 34.17, surpassing the existing SOTA\nmethod by 77%.\nAn interesting finding here is that in this set of\nexperiment, we see CDGP using SciBERT show\nthe best results. We think this confirms the do-\nmain matchesness between DGen dataset. Note\nSciBERT which is pre-trained based on science\n5837\nModels P@1 F1@3 MRR@10 NDCG@10\nDGen (Wordnet CSG) 9.31 7.71 14.34 14.94\nDGen (Probase CSG) 10.85 9.19 17.51 19.31\nDGen (w/o CSG) 5.01 5.59 9.28 11.6\nCDGP (BERT) 10.81 7.72 18.15 24.47\nCDGP (SciBERT) 13.13 12.23 25.12 34.17\nCDGP (RoBERTa) 13.13 9.65 19.34 24.52\nCDGP (BART) 8.49 8.24 16.01 22.66\nTable 5: Comparison Results: Comparing CDGP with the DGen (Ren and Q. Zhu, 2021)\nModels P@1 F1@3 F1@10 MRR@10 NDCG\n@10\nNaive 12.60 10.00 12.45 22.70 30.32\nAnswer\nRelating18.50 13.80 15.37 29.96 37.82\nTable 6: The Results of Naive and Answer-Relating\nFine-Tuning Comparison\nliterature and DGen is a dataset related to scientific\ndomains.\n4.4.2 Results on CLOTH dataset\nIn this experiment, we evaluate the performance of\nour models on CLOTH dataset and conduct abla-\ntion studies for our CDGP model.\nComparing Fine-Tuning Strategy In this set of\nexperiment, we compare the performance of naive\nfine-tuning and answer-relating fine-tuning. The\nresults are presented in Table 6.\nFrom the above results, it can be observed that\nthe overall score of answer-relating fine-tuning is\nhigher than that of naive fine-tuning. Therefore,\nwe select answer-relating fine-tuning as a default\nfine-tuning strategy.\nComparing Pre-trained Language Models In\nthis set of experiment, we experiment with using\ndifferent pre-trained language models. The follow-\ning are the pre-trained language models used in the\nexperiments. (1) BERT (Devlin et al., 2018), (2)\nSciBERT (Beltagy et al., 2019), (3) RoBERTa (Liu\net al., 2019), (4) BART (Lewis et al., 2019).\nTable 7 shows the comparison result. Through\nthis experiment, we see that the BERT model has\nthe most outstanding performance, so we use the\nBERT model for subsequent experiments.\nComparing DS Factors There are four scor-\ning factors in DS, namely s0 (confidence score),\ns1 (word embedding similarity), s2 (contextual\nsentence similarity) and s3 (part-of-speech match\nModels P@1 F1@3 F1@10 MRR@10 NDCG\n@10\nBERT 18.50 13.80 15.37 29.96 37.82\nSciBERT 8.10 9.13 12.22 19.53 28.76\nRoBERTa10.50 9.83 10.25 20.42 28.17\nBART 14.20 11.07 11.37 24.29 31.74\nTable 7: Results on Comparing the Employment of\nDifferent Pre-trained Language Models (fine-tuned with\nCLOTH dataset)\nw0 w1 w2 w3 P@1 F1@3 MRR@10NDCG\n@10\n0.25 0.25 0.25 0.25 18.50 13.80 29.96 37.82\n0.4 0.2 0.2 0.2 19.40 15.33 31.11 39.12\n0.6 0.15 0.15 0.1 19.30 15.50 31.26 39.49\n0.8 0.05 0.05 0.1 18.90 15.43 30.88 39.56\nTable 8: Distractor Selector Features Weighting Com-\nparison\nscore). In this experiment, we adjust the weighting\nof each scoring index of DS (from w0 to w3), and\ncompare the difference of using different weight\nratios. Table 8 shows the experiment results.\nFrom the results in Table 8, we see that if the\nweights of s1 and s2 is adjusted lower, a better\ndistractor generation performance is observed, but\nif they are set too low, the performance starts to\ndegrade.\nAfter the experiments, we see that the DS\nweights setting to (0.6, 0.15, 0.15, 0.1) show the\nbest performance. We use this weighting setting as\ndefault values for other experiments.\nComparing w/o CDGP Components Through\nthe above experiment studies, we obtain the besting\nparameter settings for CDGP. In order to prove the\neffectiveness of the CDGP design, in this set of\nexperiments, we compare the use or not of each\ncomponent in the framework. Table 9 presents the\nexperimental results.\nFrom the results, we can see that the\nwhole CDGP framework (CSG+DS with\n5838\nMethods P@1 F1@3 F1@10 MRR@10NDCG\n@10\nCSG+DS 19.30 15.50 15.37 31.26 39.49\nCSG 18.50 14.90 15.37 30.57 38.73\nDS 4.00 6.43 5.05 12.02 19.12\nNone 4.10 6.03 5.05 11.81 18.65\nTable 9: Ablation study on CDGP components\n(w0,w1,w2,w3) = (0.6,0.15,0.15,0.1)) shows\nthe best performing results compared with the\noptions using only one or none of the components.\nFurthermore, we see that using only CSG improves\nthe performance (107.7%, in terms of NDCG@10,\ncompared with none scheme (which uses BERT’s\nMLM capability to have distractor candicate\nwithout any fine-tuning), while using only DS\nbrings slightly performance improvement (2.5%).\nSuch results indicate that the major performance\nimprovement comes from the CSG employment.\n4.4.3 Result on Human Evaluation\nWe also recruit 40 human evaluators from our cam-\npus. The evaluation process is as follows. First,\nthe evaluator takes a cloze exam (a passage with\n10 cloze multiple choice questions). The passages\nare randomly selected from the CLOTH dataset.\nFor a selected passage, we keep five original ques-\ntions and replace the rest five questions with the\ngeneration results by our model. Our goal is to\nobserve the answering correct rate over the man-\nually designed distractors and the automatically\ndesigned distractors. Furthermore, we also ask the\nevaluators to exam the quality of the generated dis-\ntractors. Specifically, after the exam, we ask (1) the\nevaluators to guess which questions are generated\nby CDGP and (2) rank the distractor difficulty by\nLikert scale ranging from 1-5.\nAnswering Correct Rate We find that the cor-\nrect rate of the human cloze questions is 50.5%,\nwhile the correct rate of CDGP questions is 66%.\nThe correct rate of CDGP questions is slightly\nhigher than that of human questions, which shows\nthat the difficulty of CDGP distractors is slight eas-\nier than that of human questions. Improving and\ncontrolling the difficulty of automatically gener-\nated distractors will be an interesting future work\ndirection.\nDistinguishing Human-design or CDGP Ques-\ntion In the test of judging whether a question is\na CDGP question, the correct rate of the evalua-\ntors’ guess is 53%, which nearly to a random guess,\nFigure 3: The testers’ feedback on the difficulty of\nthe questions generated by CDGP (1: easiest, 5: most\ndifficult)\nshowing that the evaluator cannot effectively dis-\ntinguish between human and CDGP questions.\nExamining Difficulty of Generated Distractors\nFrom the tester feedback, as shown in Figure 3,\nthe testers’ ratings of difficulty are normally distri-\nbution, indicating that the difficulty level of the\nquestions is moderate. It can be seen that the\nperformance of CDGP questions is close to that\nof manual-design questions, which confirms that\nCDGP can assist in the cloze distractor preparation.\n5 Conclusion\nOur study indicates that PLM-based candidate\ndistractor generator is a better alternative for\nknowledge-based component. The experiment re-\nsults show that our model significantly surpassed\nthe SOTA method, demonstrating the effectiveness\nof PLM-based distractor generation on Cloze Test.\nAlso, the result shows that using domain-specific\nPLM will further boost the generation quality.\n6 Limitations\nThe major limitation for this study is that the cur-\nrent evaluation on the test dataset cannot truly re-\nflect the distractor generation quality. A mismatch\nwith the ground truth distractors do not imply the\ngenerated distractor is not a feasible one. Also,\nwe have no way to control the difficulty and the\ncorrectness of distractor generation.\nAcknowledgement\nThis work is supported by NSTC 110-2634-F-005-\n006-project Smart Sustainable New Agriculture Re-\nsearch Center (SMARTer), NSTC Taiwan Project\nunder grant 109-2221-E-005-058-MY3, and Delta\nResearch Center, Delta Electronics, Inc. We thank\nto National Center for High-performance Comput-\ning (NCHC) of National Applied Research Labora-\ntories (NARLabs) in Taiwan for providing compu-\ntational and storage resources.\n5839\nReferences\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scibert:\nA pretrained language model for scientific text. In\nEMNLP. Association for Computational Linguistics.\nPiotr Bojanowski, Edouard Grave, Armand Joulin,\nand Tomas Mikolov. 2016. Enriching word vec-\ntors with subword information. arXiv preprint\narXiv:1607.04606.\nRui Pedro dos Santos Correia, Jorge Baptista, Nuno\nMamede, Isabel Trancoso, and Maxine Eskenazi.\n2010. Automatic generation of cloze question dis-\ntractors. In Second language studies: acquisition,\nlearning, education and technology.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nGirish Kumar, Rafael E Banchs, and Luis Fernando\nD’Haro. 2015. Revup: Automatic gap-fill question\ngeneration from educational texts. In Proceedings\nof the Tenth Workshop on Innovative Use of NLP for\nBuilding Educational Applications, pages 154–161.\nJohn Lee and Stephanie Seneff. 2007. Automatic gen-\neration of cloze items for prepositions. In Eighth\nAnnual Conference of the International Speech Com-\nmunication Association. Citeseer.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2019.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. CoRR, abs/1910.13461.\nChen Liang, Xiao Yang, Neisarg Dave, Drew Wham,\nBart Pursel, and C Lee Giles. 2018. Distractor gen-\neration for multiple choice questions using learning\nto rank. In Proceedings of the thirteenth workshop\non innovative use of NLP for building educational\napplications, pages 284–290.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nGeorge A Miller. 1995. Wordnet: a lexical database for\nenglish. Communications of the ACM, 38(11):39–41.\nAnnamaneni Narendra, Manish Agarwal, and Rak-\nshit Shah. 2013. Automatic cloze-questions genera-\ntion. In Proceedings of the International Conference\nRecent Advances in Natural Language Processing\nRANLP 2013, pages 511–515.\nSiyu Ren and Kenny Q. Zhu. 2021. Knowledge-driven\ndistractor generation for cloze-style multiple choice\nquestions. Proceedings of the AAAI Conference on\nArtificial Intelligence, 35(5):4339–4347.\nJohannes Welbl, Nelson F Liu, and Matt Gardner. 2017.\nCrowdsourcing multiple choice science questions.\narXiv preprint arXiv:1707.06209.\nWentao Wu, Hongsong Li, Haixun Wang, and Kenny Q\nZhu. 2012. Probase: A probabilistic taxonomy for\ntext understanding. In Proceedings of the 2012 ACM\nSIGMOD International Conference on Management\nof Data, pages 481–492.\nQizhe Xie, Guokun Lai, Zihang Dai, and Eduard Hovy.\n2017. Large-scale cloze test dataset created by teach-\ners. arXiv preprint arXiv:1711.03225.\n5840",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7656612396240234
    },
    {
      "name": "Natural language processing",
      "score": 0.5370160341262817
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5139017105102539
    },
    {
      "name": "Language model",
      "score": 0.4743415415287018
    },
    {
      "name": "Speech recognition",
      "score": 0.39482104778289795
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I162838928",
      "name": "National Chung Hsing University",
      "country": "TW"
    }
  ]
}