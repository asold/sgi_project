{
  "title": "KnowSemLM: A Knowledge Infused Semantic Language Model",
  "url": "https://openalex.org/W2985074250",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2108342033",
      "name": "Haoruo Peng",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2098141378",
      "name": "Qiang, Ning",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2122007671",
      "name": "Dan Roth",
      "affiliations": [
        "University of Pennsylvania",
        "University of Illinois Urbana-Champaign"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2142269169",
    "https://openalex.org/W2739505524",
    "https://openalex.org/W2963925965",
    "https://openalex.org/W2257051837",
    "https://openalex.org/W2165516035",
    "https://openalex.org/W2566266660",
    "https://openalex.org/W2963217826",
    "https://openalex.org/W2760579680",
    "https://openalex.org/W2250506749",
    "https://openalex.org/W2963097991",
    "https://openalex.org/W2952230511",
    "https://openalex.org/W2145374219",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W2129272839",
    "https://openalex.org/W2806070455",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W1825628421",
    "https://openalex.org/W2250738489",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W4239966930",
    "https://openalex.org/W2250189634",
    "https://openalex.org/W2741200275",
    "https://openalex.org/W2097732278",
    "https://openalex.org/W2252139350",
    "https://openalex.org/W1702669762",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2563017805",
    "https://openalex.org/W2165962657",
    "https://openalex.org/W3099987284",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2476140796",
    "https://openalex.org/W2151295812",
    "https://openalex.org/W2963655793",
    "https://openalex.org/W2571747590",
    "https://openalex.org/W2963290255",
    "https://openalex.org/W2252052418",
    "https://openalex.org/W2803267010",
    "https://openalex.org/W2157003655",
    "https://openalex.org/W2048968173",
    "https://openalex.org/W2250544821",
    "https://openalex.org/W2250836735",
    "https://openalex.org/W2692059227",
    "https://openalex.org/W2578412240",
    "https://openalex.org/W2963838094",
    "https://openalex.org/W2963498218",
    "https://openalex.org/W2185115617",
    "https://openalex.org/W2158899491",
    "https://openalex.org/W2466175319",
    "https://openalex.org/W2169943035",
    "https://openalex.org/W1991018417",
    "https://openalex.org/W2475245295",
    "https://openalex.org/W2593847145",
    "https://openalex.org/W2127194753",
    "https://openalex.org/W2758362814",
    "https://openalex.org/W2949875129",
    "https://openalex.org/W2963247627",
    "https://openalex.org/W2964028591",
    "https://openalex.org/W2098844768",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2252215150",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W1523160384",
    "https://openalex.org/W4293547730",
    "https://openalex.org/W2117130368",
    "https://openalex.org/W2962841164",
    "https://openalex.org/W2000900121",
    "https://openalex.org/W2251344056",
    "https://openalex.org/W2211728022",
    "https://openalex.org/W2739992537",
    "https://openalex.org/W2571726121",
    "https://openalex.org/W2103095311",
    "https://openalex.org/W2152345753",
    "https://openalex.org/W2562564313",
    "https://openalex.org/W2758815496",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2335559472",
    "https://openalex.org/W2296266385",
    "https://openalex.org/W2091812280",
    "https://openalex.org/W2129615653"
  ],
  "abstract": "Story understanding requires developing expectations of what events come next in text. Prior knowledge – both statistical and declarative – is essential in guiding such expectations. While existing semantic language models (SemLM) capture event co-occurrence information by modeling event sequences as semantic frames, entities, and other semantic units, this paper aims at augmenting them with causal knowledge (i.e., one event is likely to lead to another). Such knowledge is modeled at the frame and entity level, and can be obtained either statistically from text or stated declaratively. The proposed method, KnowSemLM, infuses this knowledge into a semantic LM by joint training and inference, and is shown to be effective on both the event cloze test and story/referent prediction tasks.",
  "full_text": "Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 550–562\nHong Kong, China, November 3-4, 2019.c⃝2019 Association for Computational Linguistics\n550\nKnowSemLM : A Knowledge Infused Semantic Language Model\nHaoruo Peng1, Qiang Ning1, Dan Roth1,2\nDepartment of Computer Science\n1University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA\n2University of Pennsylvania, Philadelphia, PA 19104, USA\n{hpeng7,qning2}@illinois.edu, danroth@seas.upenn.edu\nAbstract\nStory understanding requires developing ex-\npectations of what events come next in text.\nPrior knowledge – both statistical and declar-\native – is essential in guiding such expecta-\ntions. While existing semantic language mod-\nels (SemLM) capture event co-occurrence in-\nformation by modeling event sequences as se-\nmantic frames, entities, and other semantic\nunits, this paper aims at augmenting them with\ncausal knowledge (i.e., one event is likely to\nlead to another). Such knowledge is mod-\neled at the frame and entity level, and can\nbe obtained either statistically from text or\nstated declaratively. The proposed method,\nKnowSemLM1, infuses this knowledge into a\nsemantic LM by joint training and inference,\nand is shown to be effective on both the event\ncloze test and story/referent prediction tasks.\n1 Introduction\nNatural language understanding requires a coher-\nent understanding of a series of events or actions in\na story. In story comprehension, we need to under-\nstand not only what events have appeared in text,\nbut also what is likely to happennext. While event\nextraction has been well studied (Ji and Grishman,\n2008; Huang and Riloff, 2012; Li et al., 2013;\nPeng et al., 2016; Nguyen et al., 2016; Nguyen\nand Grishman, 2016), the task of predicting fu-\nture events (Radinsky et al., 2012; Radinsky and\nHorvitz, 2013) has received less attention.\nOne perspective is to utilize the co-occurrence\ninformation between past and future events\nlearned from a large corpus, which has been stud-\nied in script learning works (Chambers and Ju-\nrafsky, 2008; Pichotta and Mooney, 2014, 2016a;\nPeng and Roth, 2016; Peng et al., 2017). However,\nonly considering co-occurrence information is not\n1Related resources refer to https://cogcomp.\nseas.upenn.edu/page/publication_view/886.\nsufﬁcient for modeling event sequences in natural\nlanguage. Human decisions on the likelihood of a\nspeciﬁc event depend on both local context – what\nhas happened earlier in text – and global context\n– knowledge gained from human experience. This\npaper leverages both the local and global context\ninformation to model event sequences, and shows\nthat it can lead to more accurate predictions of fu-\nture events. For example, the following text snip-\npet describes a scenario of someone taking a ﬂight:\n... I checked in at the counter, took my luggage\nto the security area, got cleared ten minutes in ad-\nvance, and waited for my plane ...\nThis example consists of a series of events, i.e.,\n“check in (a ﬂight)”, “be cleared (at the secu-\nrity)”, “wait for (the plane)”, etc., which humans\nwho have traveled by plane are very familiar with.\nHowever, this event sequence appears infrequently\nin text.2 Consequently, only relying on event co-\noccurrence in text is not sufﬁcient – there is also a\nneed to model some “common sense” information.\nThe local and global contexts in this ex-\nample are illustrated in Figure 1. The ex-\nisting event sequence is “(sub)check in[ﬂight]”,\n“(sub)clear[security]” and “(sub)wait for[plane]”\n(denoted by blue dots), where “sub” means\nsubject. Language models (LM) for statisti-\ncal co-occurrences of events can capture this lo-\ncal context and generate a distribution over all\npossible events, e.g., “(sub)purchase[food]” and\n“(sub)go to[work]”, as in the blue circle.\nMore importantly, global context is the knowl-\nedge of event causality learned from human ex-\nperience in the form of “cause-effect” event pairs\n(i.e., one event leads to another). One such\npair is represented as “(sub)wait for[plane] ⇒\n2The events “check in” and “be cleared” only co-occur\ntwice in a same document in the 20-year New York Times\ncorpus (1987-2007); we count with frame and entity level\nabstractions (see Section 2.1 for details).\n551\nFigure 1: Local and global context information when\nmodeling event sequences. The blue dots are events\nthat are already described in text. The blue circle indi-\ncates local context, i.e., event sequences inferred from\na large corpus via semantic LMs; the red circle repre-\nsents global context, i.e., events learned from human\nexperience via knowledge of event causality (which\nmay overlap with local context). For event represen-\ntations, we abstract over the surface forms of semantic\nframes and entities, where “sub” represents the shared\ncommon subject. The proposed KnowSemLM lever-\nages both information to better predict future events.\n(sub)get on[plane]”, which means that one has to\nwait for a plane before getting on it (red dashed\narrow in Figure 1). Global context, as a result,\nhelps generate a distribution over a focused set\nof expected events, as in the red circle. Note\nthat the causality links have directions, and one\nevent might lead to multiple possible events, e.g.,\none has to wait for the plane before it takes off\n“(sub)wait for[plane] ⇒[plane]take off”. Such\nconnections can be viewed as temporal relations.\nHere, we consider causality to include tempo-\nral orderings of events which align with common\nsense. More discussions are provided in Sec. 6.\nThus, we propose KnowSemLM, a knowledge\ninfused semantic language model. It combines\nknowledge from external sources (in the form of\nevent causality) with the basic semantic LM (Peng\net al., 2017) trained on a given text corpus. Our\nmodel is a generative model of events, where\neach event is either generated based on a piece of\nknowledge or generated from the semantic LM.\nWhen predicting future events at inference time,\nwe generate two distributions over events: one\nfrom the given knowledge, and the other from the\nsemantic LM. We also learn a binary variable that\nselects the distribution from which we take the\nnext event. In this way, the proposedKnowSemLM\nhas the ability to generate event sequences based\non both local and global context, and better imitate\nthe story generation process.\nThis knowledge infused semantic LM operates\non abstractions over the surface form – semantic\nframes and entities. We associate each semantic\nunit (frames and entities) with an embedding and\nconstruct a joint embedding space for each event.\nWe train KnowSemLM on a large corpus and use\nthe same embedding setting for events involved in\nthe knowledge. The event causality knowledge is\nmined either statistically from the training corpus\nor declaratively for constrained domains (both in\nthe form of event pairs). In the statistical way, we\nutilize a set of discourse connectives to identify\n“cause-effect” event pairs and ﬁlter them based on\ntheir counts; if provided with event templates for\nspeciﬁc domains, we also manually write down\nsuch pairs based on human experience. In both\nways, we further enrich the knowledge base by\nconsidering transitivity among event pairs.\nWe evaluate KnowSemLM on two tasks – event\ncloze test and story/referent predictions. In both\ncases, we model text as a sequence of events\nand apply trained KnowSemLM to calculate con-\nditional probabilities of future events given text\nand knowledge. We show that KnowSemLM can\noutperform competitive results from models with\nno such knowledge. In addition, we demonstrate\nthe language modeling ability of KnowSemLM\nthrough quantitative and qualitative analysis.\nThe main contributions can be summarized\nas follows: 1) formulation of knowledge used\nin story generation as event causality; 2) pro-\nposal of KnowSemLM to integrate such event\ncausality knowledge into semantic language mod-\nels; 3) demonstration of the effectiveness of\nKnowSemLM via multiple benchmark tests.\nThe rest of the paper is organized as follows.\nWe deﬁne how we model events and event causal-\nity knowledge in Sec. 2, followed by the descrip-\ntion of the knowledge infused KnowSemLM (Sec.\n3). The training procedure of KnowSemLM is de-\ntailed in Sec. 4, followed by our experimental re-\nsults and analysis (Sec. 5) and related work (Sec.\n6). We conclude in Sec. 7.\n2 Event and Knowledge Modeling\nTo better understand the proposed KnowSemLM,\nhere we ﬁrst introduce the event representation and\n552\nevent causality model used in this paper.\n2.1 Event Representation\nTo preserve the full semantic meaning of events,\nwe need to consider multiple semantic aspects:\nsemantic frames, entities, and sentiments. We\nadopt the event representation proposed in Peng\net al. (2017), which is built upon abstractions of\nthree basic semantic units: (disambiguated) se-\nmantic frames, subjects & objects in such seman-\ntic frames, and sentiments of the frame text.\nIn a nutshell, the event representation is a com-\nbination of the above three semantic elements.\n... Steven Avery committed murder. He was ar-\nrested, charged and tried ...\nFor example, the event representations of the\nabove text would be (four separate events):\nPER[new]-commit.01-ARG[new](NEG)\nARG[new]-arrest.01-PER[old](NEU)\nARG[new]-charge.05-PER[old](NEU)\nARG[new]-try.01-PER[old](NEG)\nHere, “commit.01”, “arrest.01” and so on rep-\nresent disambiguated predicates (“01” and “05”\nrefer to the disambiguated senses in VerbNet).\nThe arguments (subject and object) of a predi-\ncate are denoted with NER types (“PER, LOC,\nORG, MISC”) or “ARG” if unknown, along with a\n“[new/old]” label indicating if it is the ﬁrst appear-\nance in the sequence. Additionally, the sentiment\nof a frame is represented as positive (POS), neural\n(NEU), or negative (NEG).\nWe formally deﬁne such an explicit and ab-\nstracted event as e. Computationally, the vector\nrepresentation of an event evec is built in a joint\nsemantic space:\nevec = Wf rf + Were + Wsrs.\nDuring language model training, we learn frame\nembeddings Wf (rf ,re,rs are one-hot vectors for\neach unique frame, entity and sentiment abstrac-\ntion, respectively) as well as the transforming ma-\ntrices We and Ws.\n2.2 Knowledge: Causality between Events\nWe model the knowledge gained from human ex-\nperience as pre-determined relationship between\nevents. Since we are modeling event sequences,\nthe knowledge of one event leads to another is very\nimportant, hence event causality. We formally de-\nﬁne a piece of event knowledge as\nex ⇒ey,\nmeaning that the outcome event ey is a possible re-\nsult of the causal event ex. Note that event causal-\nity here is directional, and one event may lead to\nmultiple different outcomes. We group all event\nknowledge pairs with the same causal event, thus\nevent ex can lead to a set of events:\nex ⇒{ey1 ,ey2 ,ey3 ,··· ,eym}.\nWe store all such event causality structures in a\nknowledge base KBEC.\n3 Knowledge Infused SemLM\nWith a proper modeling of events and event\ncausality above, this section explains the pro-\nposed KnowSemLM, a method to inject causal-\nity knowledge into a semantic LM. Speciﬁcally,\nKnowSemLM is based on FES-RNNLM ( Frame-\nEntity-Sentiment infused Recurrent Neural Net\nLanguage Model) proposed in Peng et al. (2017).\nWe brieﬂy review FES-RNNLM and describe how\nKnowSemLM adds knowledge on top of it.\n3.1 FES-RNNLM\nTo model semantic sequences and train the joint\nevent representations in Sec. 2.1, we build neural\nlanguage models over such sequences. Peng et al.\n(2017) uses Log-Bilinear Language model (Mnih\nand Hinton, 2007), but since we require the use\nof event causality knowledge to be based on past\nevents, we choose to implement an RNN language\nmodel (RNNLM) where the generation of future\nevents is only dependent on past events.\nFor ease of explanation, we denote a seman-\ntic sequence of joint event representations as\n[e1,e2,··· ,et], with et being the tth event in the\nsequence. Thus, we model the conditional proba-\nbility of an event et given its context as\nplm(et|e1,··· ,et−1)\n= softmax(Wsht + bs)\n= exp(evec\nt (Wsht + bs))∑\ne∈Vexp(evec(Wsht + bs)).\nNote that the softmax operation is carried out over\nthe event vocabulary V, i.e., all possible events in\nthe language model. Moreover, the hidden layer\nht in RNN is computed as: ht = φ(evec\nt Wi +\nht−1Wh + bh), where φis the activation function.\nFor language model training, we learn parameters\nWs, bs, Wi, Wh, and bh, and maximize the se-\nquence probability ∏k\nt=1 plm(et|e1,e2,··· ,et−1).\n553\nFigure 2: Overview of the computational workﬂow\nfor the proposed KnowSemLM. There are two key\ncomponents: 1) a knowledge selection model, which\nactivates the use of knowledge based on probabilisti-\ncally matching causal event and produce a distribution\nover outcome events via attention; 2) a sequence gen-\neration model, which takes input from both the knowl-\nedge selection model and the base semantic language\nmodel (FES-RNNLM) to generate future events via a\ncopying mechanism. Note that the single dots indicate\nexplicit event representations while three consecutive\ndots stand for event vectors.\n3.2 KnowSemLM\nIn Figure 2, we show the computational work-\nﬂow of the proposed KnowSemLM. There are two\nkey components: 1) a knowledge selection model,\nwhich activates the use of knowledge based on\nprobabilistically matching causal events and pro-\nduces a distribution over outcome events; 2) a se-\nquence generation model, which takes input from\nboth the knowledge selection model and the base\nsemantic language model (FES-RNNLM) to gen-\nerate future events via a copying mechanism.3\nKnowledge Selection Model\nFor an event in the sequence et, we ﬁrst match it\nwith possible causal events {ex}in the knowledge\nbase KBEC based on the bi-focal attention of pre-\nvious events. Thus, from the knowledge base, we\nget a list of outcome events Vy ≜ {ey1 ,ey2 ,···}.\nComputationally, we model the conditional\nprobability of matching with causal event ex and\noutcome event ey from knowledge base given the\ncontext of e1,e2,··· ,et as\npkn(ex ⇒ey|e1,e2,··· ,et)\n= exp(evec\nx Waht) exp(evec\ny Wbht)∑\ne∈Vx,e′∈Vy exp(evecWaht) exp(e′vecWbht).\n3The proposed computational framework ofKnowSemLM\nis similar to DynoNet proposed in He et al. (2017). Compared\nto DynoNet, the knowledge base utilized here operates on\nevent level representations rather than on tokens.\nHere, we use the bi-focal attention mecha-\nnism (Nema et al., 2018) via attention parameters\nWa,Wb, and apply it on the hidden layerht, which\nembeds information from all previous events in the\nsequence. Therefore, we produce a distribution\nover the set of possible outcome events Vy.\nSequence Generation Model\nThe base semantic LM produces a distribution\nover events from the language model vocabulary,\nwhich represents local context, while the knowl-\nedge selection model generates a set of outcome\nevents with a probability distribution, which rep-\nresents global context of event causality knowl-\nedge. The sequence generation model then com-\nbines the local and global context for generat-\ning future events. Therefore, we model the con-\nditional probability of event et+1 given context\np(et+1|Context) = p(et+1|e1,e2,··· ,et,KBEC).\nThis overall distribution is computed via a copy-\ning mechanism (Jia and Liang, 2016), i.e., we ei-\nther generate the next event (ei) from the language\nmodel vocabulary ( V) or copy from the outcome\nevent set (ey) based on the following probabilities:\n{\np(et+1 = ei ∈V|Context) = (1 −λ)plm(ei)\np(et+1 = ey ∈Vy|Context) = λpkn(ey).\nHere, λ is a learned scaling parameter to choose\nbetween events from LM vocabularyVand events\nfrom event causality knowledge base KBEC.\n4 Construction of KnowSemLM\n4.1 Dataset and Preprocessing\nDataset: We use the New York Times (NYT) Cor-\npus4 (from year 1987 to 2007) as the training cor-\npus. It contains over 1.8M documents in total.\nPreprocessing: We preprocess all training doc-\numents with Semantic Role Labeling and Part-\nof-Speech tagging. We also implement the ex-\nplicit discourse connective identiﬁcation module\nof a shallow discourse parser (Song et al., 2015).\nAdditionally, we utilize within-document entity\nco-reference (Peng et al., 2015a) to produce co-\nreference chains and get the anaphoricity informa-\ntion. To obtain all annotations, we use the Illinois\nNLP tools (Khashabi et al., 2018). 5 Further, we\nobtain event representations from text with frame,\nentity and sentiment level abstractions by follow-\ning procedures described in Peng et al. (2017).\n4https://catalog.ldc.upenn.edu/\nLDC2008T19\n5http://cogcomp.org/page/software/\n554\n4.2 Knowledge Mining\nStatistical Way: Part of the human knowledge\ncan be mined from text itself. Since discourse con-\nnectives are important for relating different text\nspans, we carefully select discourse connectives\nwhich can indicate a “cause-effect” situation. For\nexample, “The police arrested Jack because he\nkilled someone.” In this sentence, readers can gain\nthe knowledge of “the person who kills shall be\narrested”, which can be represented as “PER[*]-\nkill.01-*[*](*) ⇒*[*]-arrest.01-PER[old](*)” ac-\ncording to the abstractions speciﬁed in Sec. 2.\nIn practice, we choose 22 “cause-effect” con-\nnectives/phrases (such as “because”, “due to”, “in\norder to”). We then extract all event pairs con-\nnected by such connectives from the NYT train-\ning data, and abstract over their surface forms to\nget the event level representations. Finally, we ﬁl-\nter cases where the direction of the event causal-\nity pairs is unclear from a statistical standpoint.\nSpeciﬁcally, we calculate the ratio of counts of\none direction over another, i.e. θ = #(ex⇒ey)\n#(ey⇒ex) .\nIf θ >2, then we store ex ⇒ey as knowledge;\nwhile θ <0.5, we only keep ey ⇒ex. In the case\nof 0.5 <θ< 2, we ﬁlter both event causality pairs\nsince we are unsure of the knowledge statistically.\nAfter the above ﬁltering procedures, we auto-\nmatically get 8,293 different pairs of event pairs\n(without human efforts). According to Sec. 2, we\nmerge them if they have the same causal event, i.e.\nex ⇒ey and ex ⇒ez becomes ex ⇒{ey,ex}.\nThus, we get a total of 2,037 causal events (trees);\nand on average, each causal event has 4 possible\noutcome events. Furthermore, those event pairs\nof knowledge deﬁned in this work are transitive,\ne.g., if e1 ⇒e2 and e2 ⇒e3, then we can have\ne1 ⇒e3. Considering this transitivity, we iterate\nover all pairs twice, and derive more event causal-\nity pairs, achieving a total number of 9,022.6\nDeclarative Way: Besides mining knowledge au-\ntomatically from text corpus, we also take full\nadvantage of human input in some practical sit-\nuations. For the InScript Corpus (Modi et al.,\n2017), it speciﬁes 10 everyday scenarios, e.g.,\n“Bath”, “Flight”, “Haircut”. In each scenario,\nthe corpus also provides event templates and the\ncorresponding event template annotations for the\ntext. Examples of such generated event causal-\n6We do not further carry out the transitivity expansion\nprocess, since empirically the noise it introduces outweighs\nthe beneﬁts it brings (see Sec. 5.4 for details).\nMethod Accuracy\nGranroth-Wilding and Clark (2016) 49.57%\nWang et al. (2017) 55.12%\nKnowSemLM w/o knowledge 39.23%\nKnowSemLM w/o transit. & ﬁne-tuning 43.56%\nKnowSemLM w/o ﬁne-tuning 45.28%\nKnowSemLM 56.27%\nTable 1: Accuracy results for the event cloze task.\nKnowSemLM outperforms previously reported results\nand we show the ablation study results for model with-\nout the use of knowledge (w/o knowledge), without the\nuse of knowledge transitivity as described in Sec 4.2\n(w/o transit.) and without ﬁne-tuning on the dev data\n(w/o ﬁne-tuning), resepctively.\nity knowledge can be referred back to Sec. 1,\ne.g., “(sub)wait for[plane] ⇒(sub)get on[plane]”.\nIn total, we manually generate 875 event causal-\nity pairs and group them with 121 causal events.\nHere, since during the manual generation process,\nwe try to cover all event causality knowledge that\nmakes sense; we do not further apply the transitive\nproperty and expand.\n4.3 Model Training\nBased on the formulation in Sec. 3, we apply\nthe overall sequence probability as the objective:∏k\nt=1 p(et|e1,e2,··· ,et−1,KBEC).where kis the\nsequence length. For the sequence generation\nmodel, we implement the Long Short-Term Mem-\nory (LSTM) network with a layer of 64 hidden\nunits while the dimension of the input event vec-\ntor representation is 200. Because we carry out\nthe same event-level abstractions as in Peng et al.\n(2017), the event vocabulary is the same, with the\nsize of ∼4M different events.7\n5 Experiments\nWe show thatKnowSemLM can achieve better per-\nformance for the event cloze test and story/referent\nprediction tasks compared to models without the\nuse of knowledge. We also evaluate the language\nmodeling ability of KnowSemLM through quanti-\ntative and qualitative analysis.\n5.1 Application for Event Cloze Test\nTask Description and Setting: We utilize the\nMCNC task and dataset proposed in Granroth-\nWilding and Clark (2016) as the benchmark evalu-\nation. For each test instance, the goal is to recover\nthe event (deﬁned as predicate with associated en-\ntities) from an event chain given multiple choices.\n7Please see Table 2 in Peng et al. (2017) for details.\n555\nSince the event deﬁnition in this task is compat-\nible with our representation deﬁned in Sec 2.1 8,\nwe can directly convert event chains into our se-\nmantic event sequences. In this application task,\nwe train KnowSemLM on the NYT portion of the\nGigaword9 corpus, and also ﬁne-tune on the de-\nvelopment set speciﬁed in this task10.\nApplication of KnowSemLM: For each test case\n(i.e., an event chain inside a document), we ﬁrst\nconstruct the event level representation as de-\nscribed in Sec. 2 for each event in the chain. We\nthen apply KnowSemLM to obtain the overall se-\nquence probability by replacing the missing event\nwith each candidate choice. The ﬁnal decision is\nmade by choosing the event with the highest prob-\nability. Note that the event causality knowledge\nhere for both training and testing is generated au-\ntomatically from NYT corpus speciﬁed in Sec. 4.2\n(the Statistical Way). To efﬁciently calculate the\nsequence probability, we limit the context window\nsize surrounding the missing event to be 10.\nResults: The accuracy results are shown in Ta-\nble 1. We compare KnowSemLM with previous\nreported results on this event cloze test (Granroth-\nWilding and Clark, 2016; Wang et al., 2017).\nKnowSemLM outperforms both baselines and we\nfurther carry out the ablation study to measure the\nimpact of knowledge, transitivity of knowledge,\nand ﬁne-tuning. We can see that it is important\nfor the semantic LM to consider knowledge and\nalso learn the process of applying such knowledge\nin event sequences, i.e., the ﬁne-tuning step.\n5.2 Application for Story Prediction\nTask Description and Setting:We use the bench-\nmark ROCStories dataset (Mostafazadeh et al.,\n2017), and follow the test setting in Peng et al.\n(2017). For each instance, we are given a\nfour-sentence story and the system needs to pre-\ndict the correct ﬁfth sentence from two choices;\nwith the incorrect ending being semantically un-\nreasonable, or un-related. Instead of treating the\ntask as a supervised binary classiﬁcation prob-\nlem with a development set to tune, we evaluate\nKnowSemLM in an unsupervised fashion where\n8Our event representation is abstracted on a higher level.\nThus, we process the original NYT documents, where event\nchains come from, for abstraction purposes; and then match\nit to the event chains in the test data.\n9https://catalog.ldc.upenn.edu/\nLDC2011T07\n10https://mark.granroth-wilding.co.uk/\npapers/what_happens_next/\nBaselines Accuracy\nSeq2Seq 58.0%\nMostafazadeh et al. (2016) 58.5%\nSeq2Seq with attention 59.1%\nModel w/o Knowledge S. M.V .\nFES-LM (Peng et al., 2017) 62.3% 61.6%\nKnowledge Model S. M.V .\nKnowSemLM 66.5% 63.1%\nTable 2: Accuracy results for story cloze test in the\nunsupervised setting. “S.” represents the inference\nmethod with the single most informative feature while\n“M.V .” means majority voting.\nwe directly evaluate on the test set. In such a\nway, we can directly compare with the FES-LM\nmodel proposed in Peng et al. (2017), which is\nbase model of KnowSemLM without the use of\nknowledge. Similar to the training of FES-LM,\nwe ﬁne tune KnowSemLM on the in-domain short\nstory training data, with the model trained on NYT\ncorpus as initialization.11\nApplication of KnowSemLM: For each test story,\nwe generate a set of conditional probability fea-\ntures from KnowSemLM. We ﬁrst construct the\nevent level representation as described in Sec. 2.\nWe then utilize the conditional probability of the\nﬁfth sentence given previous context sentences\nand the knowledge base KB EC as features. Here\nKBEC is generated automatically from NYT cor-\npus speciﬁed in Sec. 4.2 without human efforts.\nWe get multiple features depending on how long\nwe go back in the context in terms of events. In\npractice, we get at most 12 events as context since\none sentence can contain multiple events. Thus,\nfor each story, we generate at most 12 pairs of\nconditional probability features from two given\nchoices. Every pair of such features can yield a\ndecision on which ending is more probable. Here,\nwe test two different inference methods: a single\nmost informative feature (where we go with the\ndecision made by the pair of features which have\nthe highest ratio) or majority voting based on the\ndecision made jointly by all feature pairs.\nResults: The accuracy results are shown in Ta-\nble 2. We compare KnowSemLM with Seq2Seq\nbaselines (Sutskever et al., 2014) and Seq2Seq\nwith attention mechanism (Bahdanau et al.,\n2014). We also include the DSSM system\n11We iterate over the NYT corpus until it converges on the\nperplexity metric for the development set, and then the model\nis further trained on ROC-Stories training set for 5 epochs.\n556\nMethod Accuracy\nBase (Modi et al., 2017) 62.65%\nEntityNLM (Ji et al., 2017) 74.23%\nBase∗ 60.58%\nBase∗w/ FES-RNNLM 63.79%\nBase∗w/ KnowSemLM 76.15%\nTable 3: Accuracy results for the referent predic-\ntion task on InScript Corpus. We re-implement the\nbase model (Modi et al., 2017) as “Base ∗”, and ap-\nply KnowSemLM to add additional features. “Base ∗\nw/ FES-RNNLM” is the ablation study where no event\ncausality knowledge is used. Even though “Base ∗”\nmodel performs not as good as the original base\nmodel, we achieve the best performance with added\nKnowSemLM features.\nfrom Mostafazadeh et al. (2016) as the original\nreported result. KnowSemLM outperforms both\nbaselines and the base model without the use of\nknowledge, i.e., FES-LM. The best performance\nachieved by KnowSemLM uses single most infor-\nmative feature, with the feature being the condi-\ntional probability depending on only the nearest\npreceding event and event causality knowledge).\n5.3 Application for Referent Prediction\nTask Description and Setting: For referent pre-\ndiction task, we follow the setting in Modi et al.\n(2017), where the system predicts the referent of\nan entity (or a new entity) given the preceding\ntext. The task is evaluated on the InScript Cor-\npus, which contains a group of documents where\nevents are manually annotated according to pre-\ndeﬁned event templates. Each document contains\none entity which needs to be resolved. The In-\nScript Corpus can be divided into 10 situations and\nis split into standard training, development, and\ntesting sets. We ﬁne-tune KnowSemLM on the In-\nScript Corpus training set, with the model trained\non NYT corpus as initialization.\nApplication of KnowSemLM: For each test case\n(i.e., an entity inside a document), each can-\ndidate choice will be represented as a differ-\nent event representation. Note that the event\nrepresentation here comes from the event tem-\nplates deﬁned in the InScript Corpus. In the\nmeantime, we can extract the event sequence\nfrom the preceding context. Thus, we can ap-\nply KnowSemLM to compute the conditional prob-\nability of the candidate event et+1 given the\nevent sequence and the event causality knowl-\nPerplexity\nFES-RNNLM 121.8\nKnowSemLM w/o transitivity 120.7\nKnowSemLM 120.4\nNarrative Cloze Test (Recall@30)\nFES-RNNLM 47.9\nKnowSemLM w/o transitivity 49.3\nKnowSemLM 49.6\nTable 4: Results for perplexity and narrative cloze\ntest. Both studies are conducted on the NYT hold-\nout data. “FES-RNNLM” represents the semantic LM\nwithout the use of knowledge. The numbers show that\nKnowSemLM has lower perplexity and higher recall on\nnarrative cloze test, which demonstrates the contribu-\ntion of the infused knowledge.\nMatch/Event Activation/Event λ\nNYT 0.13 0.03 0.36\nInScript 0.82 0.28 0.46\nTable 5: Statistics for the use of event causality\nknowledge in KnowSemLM. We gather the statistics\nfor both NYT and InScript Corpus. “Match/Event” rep-\nresents average number of times a causal event match is\nfound in the event causality knowledge base per event;\nwhile “Activation/Event” stands for the average num-\nber of times we actually generate event predictions\nfrom the outcome events of the knowledge base. In ad-\ndition, we believe the ratio of “Activation/Event” over\n“Match/Event” co-relates with the scaling parameterλ.\nedge: pk(et+1|et−k,et−k+1,··· ,et,KBEC).Here,\nknowledge in KB EC is generated manually from\nevent templates speciﬁed in Sec. 4.2. Moreover,\nindex kdecides how far back we consider the pre-\nceding event sequence. We then add this set of\nconditional probabilities as additional features in a\nbase model (re-implementation of the linear model\nproposed in Modi et al. (2017), namely “Base ∗”)\nto train a classiﬁer to predict the right referent.\nResults: The accuracy results are shown in Ta-\nble 3. We compare with the original base model\nas well as the EntityNLM proposed in Ji et al.\n(2017) as baselines. Our re-implemented base\nmodel (“Re-base”) does not perform as good as\nthe original model. However, with the help of ad-\nditional features from FES-RNNLM, we outper-\nform the base model. More importantly, with addi-\ntional features from KnowSemLM, we achieve the\nbest performance and beat the EntityNLM system.\nThis demonstrates the importance of the manually\nadded event causality knowledge, and the ability\nof KnowSemLM to successfully capture it.\n557\n5.4 Analysis of KnowSemLM\nFirst, to evaluate the language modeling ability of\nKnowSemLM, we report perplexity and narrative\ncloze test results. We employ the same experimen-\ntal setting as detailed in Peng and Roth (2016) on\nthe NYT hold-out data. Results are shown in Ta-\nble 4. Here, “FES-RNNLM” serves as the seman-\ntic LM without the use of knowledge for the abla-\ntion study. The numbers shows that KnowSemLM\nhas lower perplexity and higher recall on narrative\ncloze test; which demonstrates the contribution of\nthe infused event causality knowledge. The results\nw.r.t. the transitivity evaluation shows that the ex-\npansion through knowledge transitivity improves\nthe model quality.\nWe also gather the statistics to analyze the us-\nage of event causality knowledge inKnowSemLM.\nWe compute two key values: 1) average num-\nber of times a causal event match is found in the\nevent causality knowledge base per event (so that\nwe can potentially use the outcome events to pre-\ndict), i.e. “Match/Event”; 2) average number of\ntimes we actually generate event predictions from\nthe outcome events of the knowledge base (result\nof the ﬁnal probability distribution), i.e. “Activa-\ntion/Event”. We get the statistics on both NYT\nand InScript Corpus, and associate the numbers\nwith the scaling parameter λ in Table 5. The\nfrequency of event matches and event activations\nfrom knowledge are both much lower in NYT than\nin InScript. Moreover, we can compute the chance\nof an outcome event being used as the prediction\nwhen it participates in the probability distribution.\nOn NYT, it is 0.03/0.13 = 23%; while on In-\nScript, it is 0.28/0.82 = 34%. We believe such\nchance co-relates with the scaling parameter λ.\nFor qualitative analysis, we provide a compar-\native example between KnowSemLM and FES-\nRNNLM in practice. The system is fed into the\nfollowing input:\n... Jane wanted to buy a new car. She had to\nborrow some money from her father. ...\nSo, on an event level, we abstract the text as\n“PER[new]-want.01-buy.01-ARG[new](NEU),\nPER[old]-have.04-borrow.01-ARG[new](NEU)”.\nFor FES-RNNLM, the system predicts the next\nevent as “PER[old]-sell.01-ARG[new](NEU)”\nsince in training data, there are many co-\noccurrences between the “borrow” event and\n“sell” event (coming from ﬁnancial news articles\nin NYT). In contrast, for KnowSemLM, since\nwe have the knowledge “PER[*]-borrow.01-\nARG[*](*) ⇒PER[old]-return.01-ARG[old](*)”,\nmeaning that something borrowed by someone is\nlikely to be returned, the predicted event would\nbe “PER[old]-return.01-ARG[old](NEU)”. This\nis closer to the real text semantically: ... She\npromised to return the money once she got a job ...\nSuch an example shows that KnowSemLM works\nin situations where 1) the required knowledge\nis stored in the event causality knowledge base,\nand 2) the training data contains scenarios where\nrequired knowledge is put into use.\n6 Related Work\nOur work is built upon the previous works for se-\nmantic language models (Peng and Roth, 2016;\nPeng et al., 2017; Chaturvedi et al., 2017). This\nline of work is in general inspired by script learn-\ning. Early works (Schank and Abelson, 1977;\nMooney and DeJong, 1985) tried to learn scripts\nvia construction of knowledge bases from text.\nMore recently, researchers focused on utilizing\nstatistical models to extract high-quality scripts\nfrom large amounts of data (Chambers and Ju-\nrafsky, 2008; Bejan, 2008; Jans et al., 2012; Pi-\nchotta and Mooney, 2014; Granroth-Wilding and\nClark, 2016; Rudinger et al., 2015; Pichotta and\nMooney, 2016a,b). Other works aimed at learn-\ning a collection of structured events (Chambers,\n2013; Cheung et al., 2013; Balasubramanian et al.,\n2013; Bamman and Smith, 2014; Nguyen et al.,\n2015; Inoue et al., 2016). In particular, Ferraro\nand Durme (2016) presented a uniﬁed probabilis-\ntic model of syntactic and semantic frames while\nalso demonstrating improved coherence. Several\nworks have employed neural embeddings (Modi\nand Titov, 2014a,b; Frermann et al., 2014; Titov\nand Khoddam, 2015). Some prior works have\nused scripts-related ideas to help improve NLP\ntasks (Irwin et al., 2011; Rahman and Ng, 2011;\nPeng et al., 2015b).\nSeveral recent works focus on narrative/story\ntelling (Rishes et al., 2013), as well as studying\nevent structures (Brown et al., 2017). Most re-\ncently, Mostafazadeh et al. (2016, 2017) proposed\nstory cloze test as a standard way to test a system’s\nability to model semantics. They released ROC-\nStories dataset, and organized a shared task for\nLSDSem’17; which yields many interesting works\non this task. Cai et al. (2017) developed a model\nthat uses hierarchical recurrent networks with at-\n558\ntention to encode sentences and produced a strong\nbaseline. Lee and Goldwasser (2019) considered\nthe problem of learning relation aware event em-\nbeddings for commonsense inference, which can\naccount for different relations between events, be-\nyond simple event similarity. We differ from them\nbecause the basic semantic unit we model is event\nlevel abstractions instead of word tokens.\nThe deﬁnition of event causality knowledge\nin this work includes temporal ordering relation-\nships. Much progress has been made in iden-\ntifying and modeling such relations. In early\nworks (Mani et al., 2006; Chambers et al., 2007;\nBethard et al., 2007; Verhagen and Pustejovsky,\n2008), the problem was formulated as a clas-\nsiﬁcation problem for determining the pair-wise\nevent temporal relations; while recent works (Do\net al., 2012; Mirza and Tonelli, 2016; Ning et al.,\n2017, 2018) took advantage of utilizing structural\nconstraints such as transitive properties of tem-\nporal relationships via ILP to achieve better re-\nsults. Comparatively, the concept of event causal-\nity knowledge here is broader and more ﬂexible.\nAny event causality relation gained from human\nexperience could be represented and utilized in\nKnowSemLM; as shown in Sec. 4.2 that such\nknowledge can be both mined from corpus and\nwritten down declaratively.\nSince we formulate the semantic sequence mod-\neling problem as a language modeling issue, we\nalso review recent neural language modeling liter-\nature. Bengio et al. (2003) introduced a model that\nlearns word vector representations as part of a sim-\nple neural network architecture for language mod-\neling. Collobert and Weston (2008) decoupled the\nword vector training from the downstream train-\ning objectives, which paved the way for Collobert\net al. (2011) to use the full context of a word for\nlearning the word representations. The skip-gram\nand continuous bag-of-words (CBOW) models of\nMikolov et al. (2013) propose a simple single-\nlayer architecture based on the inner product be-\ntween two word vectors. Mnih and Kavukcuoglu\n(2013) also proposed closely-related vector log-\nbilinear models, vLBL and ivLBL, and Levy and\nGoldberg (2014) proposed explicit word embed-\ndings based on a PPMI metric. Additionally, re-\nsearcher have been attempting to infuse knowl-\nedge into the language modeling process (Ahn\net al., 2016; Yang et al., 2016; Ji et al., 2017; He\net al., 2017; Clark et al., 2018).\nMost recently, pre-trained language models\nsuch as BERT (Devlin et al., 2019), GPT (Radford\net al., 2018), and XLNET (Yang et al., 2019) have\nachieved much success for language modeling and\ngeneration tasks. Our proposed knowledge in-\nfused semantic language model can not be directly\napplied upon such word-level pre-trained language\nmodels. However, as future works, we are inter-\nested in exploring the possibility of pre-training\na semantic language model with frame and entity\nabstractions on a large corpus with event causality\nknowledge, and ﬁne-tune it on application tasks.\n7 Conclusion\nThis paper proposes KnowSemLM, a knowledge\ninfused semantic LM. It utilizes both local con-\ntext (i.e., what has been described in text) and\nglobal context (i.e., causality knowledge about\nevents) to predict future events. We show that\nsuch event causality knowledge can be obtained\nstatistically from a corpus or declaratively in\nspeciﬁc scenarios. Similar to previous works,\nKnowSemLM takes advantage of event-level ab-\nstractions to achieve generalization. Evaluations\ndemonstrate that the knowledge awareness of the\nproposed KnowSemLM helps improve results on\ntasks such as the event cloze test and story/referent\nprediction.\nAcknowledgments\nWe thank the anonymous reviewers for their in-\nsightful comments. This work was supported by\nthe IBM-ILLINOIS Center for Cognitive Comput-\ning Systems Research (C3SR) - a research col-\nlaboration as part of the IBM AI Horizons Net-\nwork, as well as by contracts HR0011-15-C-0113\nand HR0011-18-2-0052 with the US Defense Ad-\nvanced Research Projects Agency (DARPA). Ap-\nproved for Public Release, Distribution Unlimited.\nThe views expressed are those of the authors and\ndo not reﬂect the ofﬁcial policy or position of the\nDepartment of Defense or the U.S. Government.\n559\nReferences\nSungjin Ahn, Heeyoul Choi, Tanel P ¨arnamaa, and\nYoshua Bengio. 2016. A neural knowledge lan-\nguage model. arXiv preprint arXiv:1608.00318.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. In Proceedings of\nthe International Conference on Learning Represen-\ntations (ICLR).\nNiranjan Balasubramanian, Stephen Soderland,\nMausam, and Oren Etzioni. 2013. Generating\ncoherent event schemas at scale. In Proceedings of\nthe Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nDavid Bamman and Noah A. Smith. 2014. Unsuper-\nvised discovery of biographical structure from text.\nTransactions of the Association for Computational\nLinguistics (TACL).\nCosmin Adrian Bejan. 2008. Unsupervised discovery\nof event scenarios from texts. In Proceedings of the\n21st International Florida Artiﬁcial Intelligence Re-\nsearch Society Conference.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of Machine Learning Re-\nsearch (JMLR).\nSteven Bethard, James H Martin, and Sara Klingen-\nstein. 2007. Timelines from text: Identiﬁcation of\nsyntactic temporal relations. In Proceedings of the\nInternational Conference on Semantic Computing\n(ICSC).\nSusan Brown, Claire Bonial, Leo Obrst, and Martha\nPalmer. 2017. The rich event ontology. In Proceed-\nings of the Events and Stories in the News Workshop.\nZheng Cai, Lifu Tu, and Kevin Gimpel. 2017. Pay at-\ntention to the ending: Strong neural baselines for the\nROC story cloze task. In Proceedings of the Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL).\nNathanael Chambers. 2013. Event schema induction\nwith a probabilistic entity-driven model. In Pro-\nceedings of the Conference on Empirical Methods\nin Natural Language Processing (EMNLP).\nNathanael Chambers and Daniel Jurafsky. 2008. Un-\nsupervised Learning of Narrative Event Chains. In\nProceedings of the Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL).\nNathanael Chambers, Shan Wang, and Dan Juraf-\nsky. 2007. Classifying temporal relations between\nevents. In Proceedings of the Annual Meeting of the\nACL on Interactive Poster and Demonstration Ses-\nsions.\nSnigdha Chaturvedi, Haoruo Peng, and Dan Roth.\n2017. Story comprehension for predicting what hap-\npens next. In Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nJackie Chi Kit Cheung, Hoifung Poon, and Lucy Van-\nderwende. 2013. Probabilistic Frame Induction. In\nProceedings of the Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics (NAACL).\nElizabeth Clark, Yangfeng Ji, and Noah A Smith. 2018.\nNeural text generation in stories using entity repre-\nsentations as context. In Proceedings of the Confer-\nence of the North American Chapter of the Associa-\ntion for Computational Linguistics (NAACL).\nRonan Collobert and Jason Weston. 2008. A uniﬁed\narchitecture for natural language processing: deep\nneural networks with multitask learning. In Pro-\nceedings of the International Conference on Ma-\nchine Learning (ICML).\nRonan Collobert, Jason Weston, L´eon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel P. Kuksa.\n2011. Natural language processing (almost) from\nscratch. Journal of Machine Learning Research\n(JMLR).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language un-\nderstanding. In Proceedings of the Conference of\nthe North American Chapter of the Association for\nComputational Linguistics (NAACL).\nQuang Do, Wei Lu, and Dan Roth. 2012. Joint infer-\nence for event timeline construction. In Proceedings\nof the Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nFrancis Ferraro and Benjamin Van Durme. 2016. A\nuniﬁed bayesian model of scripts, frames and lan-\nguage. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence (AAAI).\nLea Frermann, Ivan Titov, and Manfred Pinkal. 2014.\nA hierarchical bayesian model for unsupervised in-\nduction of script knowledge. In Proceedings of the\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics (EACL).\nMark Granroth-Wilding and Stephen Clark. 2016.\nWhat Happens Next? Event Prediction Using a\nCompositional Neural Network Model. In Proceed-\nings of the AAAI Conference on Artiﬁcial Intelli-\ngence (AAAI).\nHe He, Anusha Balakrishnan, Mihail Eric, and Percy\nLiang. 2017. Learning symmetric collaborative dia-\nlogue agents with dynamic knowledge graph embed-\ndings. In Proceedings of the Annual Meeting of the\nAssociation for Computational Linguistics (ACL).\n560\nRuihong Huang and Ellen Riloff. 2012. Modeling tex-\ntual cohesion for event extraction. In Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence\n(AAAI).\nNaoya Inoue, Yuichiroh Matsubayashi, Masayuki Ono,\nNaoaki Okazaki, and Kentaro Inui. 2016. Model-\ning context-sensitive selectional preference with dis-\ntributed representations. In Proceedings of the Inter-\nnational Conference on Computational Linguistics\n(COLING).\nJoseph Irwin, Mamoru Komachi, and Yuji Mat-\nsumoto. 2011. Narrative schema as world knowl-\nedge for coreference resolution. In Proceedings of\nthe SIGNLL Conference on Computational Natural\nLanguage Learning (CoNLL).\nBram Jans, Steven Bethard, Ivan Vulic, and Marie-\nFrancine Moens. 2012. Skip n-grams and rank-\ning functions for predicting script events. In Pro-\nceedings of the Conference of the European Chap-\nter of the Association for Computational Linguistics\n(EACL).\nHeng Ji and Ralph Grishman. 2008. Reﬁning event ex-\ntraction through cross-document inference. In Pro-\nceedings of the Annual Meeting of the Association\nfor Computational Linguistics (ACL).\nYangfeng Ji, Chenhao Tan, Sebastian Martschat, Yejin\nChoi, and Noah A Smith. 2017. Dynamic entity rep-\nresentations in neural language models. In Proceed-\nings of the Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP).\nRobin Jia and Percy Liang. 2016. Data recombina-\ntion for neural semantic parsing. In Proceedings of\nthe Annual Meeting of the Association for Computa-\ntional Linguistics (ACL).\nDaniel Khashabi, Mark Sammons, Ben Zhou, Tom\nRedman, Christos Christodoulopoulos, Vivek Sriku-\nmar, Nicholas Rizzolo, Lev Ratinov, Guanheng Luo,\nQuang Do, Chen-Tse Tsai, Subhro Roy, Stephen\nMayhew, Zhili Feng, John Wieting, Xiaodong Yu,\nYangqiu Song, Shashank Gupta, Shyam Upadhyay,\nNaveen Arivazhagan, Qiang Ning, Shaoshi Ling,\nand Dan Roth. 2018. CogCompNLP: Your Swiss\nArmy Knife for NLP. In Proceedings of the\nLanguage Resources and Evaluation Conference\n(LREC).\nI-Ta Lee and Dan Goldwasser. 2019. Multi-relational\nscript learning for discourse relations. In Proceed-\nings of the Annual Meeting of the Association for\nComputational Linguistics (ACL).\nOmer Levy and Yoav Goldberg. 2014. Linguistic reg-\nularities in sparse and explicit word representations.\nIn Proceedings of the SIGNLL Conference on Com-\nputational Natural Language Learning (CoNLL).\nQi Li, Heng Ji, and Liang Huang. 2013. Joint event\nextraction via structured prediction with global fea-\ntures. In Proceedings of the Annual Meeting of the\nAssociation for Computational Linguistics (ACL).\nInderjeet Mani, Marc Verhagen, Ben Wellner,\nChong Min Lee, and James Pustejovsky. 2006. Ma-\nchine learning of temporal relations. In Proceedings\nof the 21st International Conference on Computa-\ntional Linguistics and the 44th annual meeting of\nthe Association for Computational Linguistics.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efﬁcient estimation of word represen-\ntations in vector space. In Proceedings of Workshop\nat ICLR.\nParamita Mirza and Sara Tonelli. 2016. CATENA:\nCausal and temporal relation extraction from nat-\nural language texts. In Proceedings of the Inter-\nnational Conference on Computational Linguistics\n(COLING).\nAndriy Mnih and Geoffrey Hinton. 2007. Three new\ngraphical models for statistical language modelling.\nIn Proceedings of the International Conference on\nMachine Learning (ICML).\nAndriy Mnih and Koray Kavukcuoglu. 2013. Learning\nword embeddings efﬁciently with noise-contrastive\nestimation. In Proceedings of the Conference on\nNeural Information Processing Systems (NeurIPS).\nAshutosh Modi and Ivan Titov. 2014a. Inducing neu-\nral models of script knowledge. In Proceedings of\nthe SIGNLL Conference on Computational Natural\nLanguage Learning (CoNLL).\nAshutosh Modi and Ivan Titov. 2014b. Learning se-\nmantic script knowledge with event embeddings. In\nICLR Workshop.\nAshutosh Modi, Ivan Titov, Vera Demberg, Asad Say-\need, and Manfred Pinkal. 2017. Modeling seman-\ntic expectation: Using script knowledge for refer-\nent prediction. Transactions of the Association for\nComputational Linguistics (TACL).\nRaymond J. Mooney and Gerald DeJong. 1985. Learn-\ning schemata for natural language processing. In\nProceedings of the International Joint Conference\non Artiﬁcial Intelligence (IJCAI).\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James F. Allen. 2016. A cor-\npus and cloze evaluation for deeper understanding of\ncommonsense stories. In Proceedings of the Confer-\nence of the North American Chapter of the Associa-\ntion for Computational Linguistics (NAACL).\nNasrin Mostafazadeh, Michael Roth, Annie Louis,\nNathanael Chambers, and James Allen. 2017. LS-\nDSem 2017 Shared Task: The Story Cloze Test. In\nProceedings of the 2nd Workshop on Linking Models\nof Lexical, Sentential and Discourse-level Seman-\ntics.\nPreksha Nema, Shreyas Shetty, Parag Jain, Anirban\nLaha, Karthik Sankaranarayanan, and Mitesh M.\n561\nKhapra. 2018. Generating descriptions from struc-\ntured data using a bifocal attention mechanism and\ngated orthogonalization. In Proceedings of the Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics (NAACL).\nKiem-Hieu Nguyen, Xavier Tannier, Olivier Ferret,\nand Romaric Besanc ¸on. 2015. Generative event\nschema induction with entity disambiguation. In\nProceedings of the Annual Meeting of the Associ-\nation for Computational Linguistics (ACL).\nThien Huu Nguyen, Kyunghyun Cho, and Ralph Gr-\nishman. 2016. Joint event extraction via recurrent\nneural networks. In Proceedings of the Conference\nof the North American Chapter of the Association\nfor Computational Linguistics (NAACL).\nThien Huu Nguyen and Ralph Grishman. 2016. Mod-\neling skip-grams for event detection with convolu-\ntional neural networks. In Proceedings of the Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP).\nQiang Ning, Zhili Feng, and Dan Roth. 2017. A struc-\ntured learning approach to temporal relation extrac-\ntion. In Proceedings of the Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP).\nQiang Ning, Hao Wu, Haoruo Peng, and Dan Roth.\n2018. Improving Temporal Relation Extraction with\na Globally Acquired Statistical Resource. In Pro-\nceedings of the Conference of the North American\nChapter of the Association for Computational Lin-\nguistics (NAACL).\nHaoruo Peng, Kai-Wei Chang, and Dan Roth. 2015a.\nA joint framework for coreference resolution and\nmention head detection. In Proceedings of the\nSIGNLL Conference on Computational Natural\nLanguage Learning (CoNLL).\nHaoruo Peng, Snigdha Chaturvedi, and Dan Roth.\n2017. A joint model for semantic sequences:\nFrames, entities, sentiments. In Proceedings of\nthe SIGNLL Conference on Computational Natural\nLanguage Learning (CoNLL).\nHaoruo Peng, Daniel Khashabi, and Dan Roth. 2015b.\nSolving hard coreference problems. In Proceed-\nings of the Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(NAACL).\nHaoruo Peng and Dan Roth. 2016. Two discourse\ndriven language models for semantics. In Proceed-\nings of the Annual Meeting of the Association for\nComputational Linguistics (ACL).\nHaoruo Peng, Yangqiu Song, and Dan Roth. 2016.\nEvent detection and co-reference with minimal su-\npervision. In Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nKarl Pichotta and Raymond J. Mooney. 2014. Sta-\ntistical script learning with multi-argument events.\nIn Proceedings of the Conference of the European\nChapter of the Association for Computational Lin-\nguistics (EACL).\nKarl Pichotta and Raymond J. Mooney. 2016a. Learn-\ning statistical scripts with LSTM recurrent neural\nnetworks. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence (AAAI).\nKarl Pichotta and Raymond J Mooney. 2016b. Using\nsentence-level lstm language models for script infer-\nence. In Proceedings of the Annual Meeting of the\nAssociation for Computational Linguistics (ACL).\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving language\nunderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf.\nKira Radinsky, Sagie Davidovich, and Shaul\nMarkovitch. 2012. Learning causality for news\nevents prediction. In Proceedings of the Interna-\ntional World Wide Web Conferences (WWW).\nKira Radinsky and Eric Horvitz. 2013. Mining the web\nto predict future events. In Proceedings of the ACM\nInternational Conference on Web Search and Data\nMining (WSDM).\nAltaf Rahman and Vincent Ng. 2011. Coreference res-\nolution with world knowledge. In Proceedings of\nthe Annual Meeting of the Association for Computa-\ntional Linguistics (ACL).\nElena Rishes, Stephanie M Lukin, David K Elson, and\nMarilyn A Walker. 2013. Generating different story\ntellings from semantic representations of narrative.\nIn International Conference on Interactive Digital\nStorytelling, pages 192–204. Springer.\nRachel Rudinger, Pushpendre Rastogi, Francis Ferraro,\nand Benjamin Van Durme. 2015. Script induction\nas language modeling. In Proceedings of the Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP).\nRoger C. Schank and Robert P. Abelson. 1977.Scripts,\nplans, goals, and understanding: An inquiry into\nhuman knowledge structures . Oxford, England:\nLawrence Erlbaum.\nYangqiu Song, Haoruo Peng, Parisa Kordjamshidi,\nMark Sammons, and Dan Roth. 2015. Improving a\npipeline architecture for shallow discourse parsing.\nIn Proceedings of the SIGNLL Conference on Com-\nputational Natural Language Learning (CoNLL).\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Proceedings of the Conference on Neural\nInformation Processing Systems (NeurIPS).\n562\nIvan Titov and Ehsan Khoddam. 2015. Unsupervised\ninduction of semantic roles within a reconstruction-\nerror minimization framework. In Proceedings\nof the Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(NAACL).\nMarc Verhagen and James Pustejovsky. 2008. Tem-\nporal processing with the TARSQI toolkit. In Pro-\nceedings of the International Conference on Compu-\ntational Linguistics (COLING).\nZhongqing Wang, Yue Zhang, and Ching-Yun Chang.\n2017. Integrating order information and event re-\nlation for script event prediction. In Proceedings\nof the Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. XLNet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nZichao Yang, Phil Blunsom, Chris Dyer, and Wang\nLing. 2016. Reference-aware language models. In\nProceedings of the Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8110988140106201
    },
    {
      "name": "Natural language processing",
      "score": 0.6791205406188965
    },
    {
      "name": "Event (particle physics)",
      "score": 0.6461029052734375
    },
    {
      "name": "Referent",
      "score": 0.6386585235595703
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5847605466842651
    },
    {
      "name": "Frame (networking)",
      "score": 0.5548287630081177
    },
    {
      "name": "Inference",
      "score": 0.47423359751701355
    },
    {
      "name": "Semantic memory",
      "score": 0.44793829321861267
    },
    {
      "name": "Semantic role labeling",
      "score": 0.4438972771167755
    },
    {
      "name": "Test (biology)",
      "score": 0.438951700925827
    },
    {
      "name": "Semantic computing",
      "score": 0.43182143568992615
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.4187106192111969
    },
    {
      "name": "Semantic Web",
      "score": 0.20385608077049255
    },
    {
      "name": "Linguistics",
      "score": 0.18786928057670593
    },
    {
      "name": "Psychology",
      "score": 0.12398296594619751
    },
    {
      "name": "Programming language",
      "score": 0.11188149452209473
    },
    {
      "name": "Cognition",
      "score": 0.08149543404579163
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Sentence",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I79576946",
      "name": "University of Pennsylvania",
      "country": "US"
    }
  ],
  "cited_by": 16
}