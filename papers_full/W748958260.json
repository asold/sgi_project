{
  "title": "Texts in, meaning out: neural language models in semantic similarity task for Russian",
  "url": "https://openalex.org/W748958260",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221376487",
      "name": "Kutuzov, Andrey",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Andreev, Igor",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2251803266",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W193898724",
    "https://openalex.org/W2120101509",
    "https://openalex.org/W52972111",
    "https://openalex.org/W2251676919",
    "https://openalex.org/W1662133657",
    "https://openalex.org/W2251771443"
  ],
  "abstract": "Distributed vector representations for natural language vocabulary get a lot of attention in contemporary computational linguistics. This paper summarizes the experience of applying neural network language models to the task of calculating semantic similarity for Russian. The experiments were performed in the course of Russian Semantic Similarity Evaluation track, where our models took from the 2nd to the 5th position, depending on the task. We introduce the tools and corpora used, comment on the nature of the shared task and describe the achieved results. It was found out that Continuous Skip-gram and Continuous Bag-of-words models, previously successfully applied to English material, can be used for semantic modeling of Russian as well. Moreover, we show that texts in Russian National Corpus (RNC) provide an excellent training material for such models, outperforming other, much larger corpora. It is especially true for semantic relatedness tasks (although stacking models trained on larger corpora on top of RNC models improves performance even more). High-quality semantic vectors learned in such a way can be used in a variety of linguistic tasks and promise an exciting field for further study.",
  "full_text": " \nНа входе тексты, На выходе смысл: \nНейроННые языковые модели для \nзадач семаНтической близости \n(На материале русского языка)\nКутузов А. (akutuzov@hse.ru)\nНИУ Высшая Школа Экономики  \nи Mail.ru Group, Москва, Россия\nАндреев И. (i.andreev@corp.mail.ru)\nMail.ru Group, Москва, Россия\nКлючевые слова:  искусственные нейронные сети, машинное обуче -\nние, семантическая близость, дистрибутивная семантика, векторные \nрепрезентации лексики, word2vec\nTexTs in, Meaning ouT: neural \nlanguage Models in seManTic \nsiMilariTy Tasks for russian\nKutuzov A. (akutuzov@hse.ru)\nNational Research University Higher School of Economics and \nMail.ru Group, Moscow, Russia\nAndreev I. (i.andreev@corp.mail.ru)\nMail.ru Group, Moscow, Russia\nDistributed vector representations for natural language vocabulary get a lot \nof attention in contemporary computational linguistics. This paper sum -\nmarizes the experience of applying neural network language models to the \ntask of calculating semantic similarity for Russian. The experiments were \nperformed in the course of Russian Semantic Similarity Evaluation track, \nwhere our models took from 2nd to 5th position, depending on the task.  \n We introduce the tools and corpora used, comment on the nature of the \nevaluation track and describe the achieved results. It was found out that \nContinuous Skip-gram and Continuous Bag-of-words models, previously \nsuccessfully applied to English material, can be used for semantic modeling \nof Russian as well. Moreover, we show that texts in Russian National Corpus \n(RNC) provide an excellent training material for such models, outperform -\ning other, much larger corpora. It is especially true for semantic relatedness \ntasks (although stacking models trained on larger corpora on top of RNC \nmodels improves performance even more).  \n High-quality semantic vectors learned in such a way can be used in a va-\nriety of linguistic tasks and promise an exciting field for further study.\nKeywords: neural embeddings, machine learning, semantic similarity, dis -\ntributional semantics, vector word representations, word2vec\nKutuzov A., Andreev I.\n \n1. Introduction\nThis paper describes authors' experience with participating in Russian Semantic \nSimilarity Evaluation (RUSSE) track. Our system was trained using neural network \nlanguage models; the process is explained below, together with the workflow for \nevaluation. We also comment on the nature of the RUSSE tasks and discuss features \nof neural models for Russian.\nSince Ferdinand de Saussure, it is known that linguistic sign (including word) \nis arbitrary. It means that there is no direct connection between its form and concept \n(meaning). Consequently, printed orthographic words per se do not contain sense. \nWhat is important for the task discussed here, is that if given only disjoint word forms, \na computer (an artificial intelligence) can't hope to grasp the concepts behind them \nand decide whether they are semantically similar or not.\nAt the same time, detecting degree of semantic similarity between lexical units \nis an important task in computational linguistics. The reason is threefold. First, \nit is a means in itself: often, applications demand calculating the “semantic distance” \nbetween words, for example, in finding synonyms or near-synonyms for search query \nexpansion or other needs [Turney and Pantel 2010]. Second, once we know which \nwords are similar and to what extent, we can “draw a semantic map” of the language \nin question and use this knowledge in a multitude of tasks, from machine transla -\ntion [Mikolov et al. 2013b] to natural language generation [Dinu and Baroni 2014]. \nFinally, measuring performance in semantic similarity task is a convenient way to es -\ntimate soundness of a semantic model in general.\nConsequently, various methods of overcoming linguistic arbitrariness and calcu -\nlating semantic similarity for natural language texts were invented and evaluated for \nmany widespread languages. However, computational linguistics community lacks \nexperience in computing semantic similarity for Russian texts. Thus, the task of ap -\nplying state-of-the-art methods to this material promised to be interesting, and kept \nits promise.\nThe paper is structured as follows. In the Section 2 we give a brief outline of RUSSE \nevaluation track. The Section 3 describes the models we used to compute semantic simi-\nlarity and the corpora to train these models on. In the Section 4, results are evaluated \nand influence of various model settings discussed. The Section 5 lists the main results \nof our research. In the Section 6, we conclude and propose directions for future work.\n2. Task Description\nRUSSE1 is the first attempt at semantic similarity evaluation contest for Russian \nlanguage. It consists of four tracks: two for the relatedness task and two for the asso -\nciation task. Participants were presented with a list of word pairs and had to fill in the \ndegree of semantic similarity between each pair, in the range [0;1].\n1 http:/ /russe.nlpub.ru; the authors of the present paper are under the number 9 in the partici-\npants’ list.\nTexts in, Meaning out: Neural Language Models in Semantic Similarity Tasks for Russian\n \nIn the semantic relatedness task, participants were to detect word pairs in syn -\nonymic, hyponymic or hypernymic relations and to separate them from unrelated \npairs. First track test set in this task included word pairs with human-annotated simi-\nlarities between them. Systems' performance was measured with Spearman's rank \ncorrelation between these human scores and the system scores. The second track aim \nwas to distinguish between semantically related pairs from RuThes Lite thesaurus \n[Лукашевич 2011] and random pairings. Average precision was used as evaluation \nmetrics for this track and for the tracks in the second task.\nIn the association task, participants had to detect whether the words or multi-\nword expressions are associated (topically related) to each other. First track in this \ntask mixed random pairings and associations taken from the Russian Associative The -\nsaurus\n2. The second track test set included associations from Sociation.org database 3.\nAn ideal system should have always assigned 0 to unrelated pairs and positive \nvalues to related or associated ones, thus achieving average precision of 1.0. In the \ncase of the first semantic relatedness track an ideal system was to rank the pairs iden -\ntically to the human judgment, to achieve Spearman’s rho of 1.0.\nIn the end, participants were rated with four scores: hj (Spearman’s rho for the \nfirst relatedness track), rt  (average precision for the second relatedness track), ae (av-\nerage precision for the first association track) and ae2  (average precision for the sec -\nond association track). The contest itself is described in detail in [Panchenko et al. \n2015]. We participated in all tracks, using different models.\nIn general, the choice of test data and evaluation metrics seems to be sound. \nHowever, we would like to comment on two issues.\n1.  Test sets for the rt and ae2 tasks include many related word pairs which share \nlong character strings (e.g., “благоразумие; благоразумность”). This allows \nreaching unexpectedly good performance without building any complicated \nmodels, using only character-level analysis. We were able to achieve average \nprecision of 0.79 for rt  task and 0.72 for ae2  task with the following algo -\nrithm: if two words share strings more than 3 characters in length, choose \nthe longest of such strings; its length divided by 10 is the semantic similarity \nbetween words; if no such strings are found, assume similarity is zero.  \nIt seems trivial that in Russian, words which share stems are virtually al-\nways semantically similar in this or that way. Thus, the contest would benefit \nif the ratio of such pairs became lower, so that the participants had to design \nsystems that strive to understand meaning, not to compare strings of char -\nacters. Certainly, this issue is conditioned by the usage of RuThes and Socia -\ntion databases, which by design contain lots of related words with common \nstems. It is difficult to design a dataset of semantically related lexical units \nfor Russian which would not be haunted by this problem. However, this \nis the challenge for organizers of the future evaluations. Other RUSSE tracks \ndo not suffer from this flaw.\n2 http:/ /tesaurus.ru/dict/dict.php\n3 http:/ /sociation.org\nKutuzov A., Andreev I.\n \n2.  The test set for the ae task was Russian Associative Thesaurus. It was collected \nbetween 1988 and 1997; many entries can already be considered a bit ar -\nchaic (“колхоз; путь ильича”, “президент; ельцин”, etc). Perhaps, this is the \nreason for often observed disagreement in systems' performance measured \nin ae and in ae2 . These datasets differ chronologically, and it greatly influ -\nences association sets. Note striking difference in comparison to semantic re -\nlatedness task: synonymic, hyponymic and hypernymic relations are stable for \ndozens or even hundreds of years, while associations can dramatically change \nin ten years, depending on social processes. At the same time, such glitches \ncover only small part of the entries, and this is only a minor remark.\nIn the next chapter we describe our approach to computing semantic similarity \nfor Russian.\n3. Neural Networks Meet Corpora\nThe methods of automatically measuring semantic similarity fall into two large \ngroups: knowledge-based and distributional ones [Harispe et al. 2013]. The former \ndepend on building (manually or semi-automatically) a comprehensive ontology for \na given language, which functions as a conceptual network. Once such a network \nis complete, one can employ various measures to calculate distance between concepts \nin this network: in general, the shorter is the path, the higher is the similarity.\nWe employed other, distributional approach, motivated by the notion that mean-\ning is defined by usage and semantics can be derived from the contexts a given word \ntakes [Lenci 2008]. Thus, these algorithms are inherently statistical and data-driven, \nnot ruled by a curated conceptual system, as is the case for knowledge-based ones.\nIf lexical meaning is generally the sum of word usages, then the most obvious \nway to capture it is to take into account all contexts a word participates in, given \na large enough corpus. In distributional semantics, words are usually represented \nas vectors in semantic space [Turney and Pantel 2010]. In other words, each lexical \nunit is a vector of its “neighborhood” to all other words in the lexicon, after applying \nvarious distances and weighting coefficients. The matrix of n rows and n columns \n(where n is the size of the lexicon) with “neighborhood degrees” in the cells is then \na distributional model of the language. One can compare vectors for different words \n(e.g., calculating their cosine similarity) and find how “far” they are from each other. \nThis distance turns out to be the semantic similarity we sought, expressed continu -\nously from 0 (totally unrelated words) to 1 (absolute synonyms).\nSuch an approach theoretically scales well (one has to simply add more texts \nto the corpus to get new words and contexts) and does not demand laborious and sub-\njective process of building an ontology. Meaning is extracted directly from linguistic \nevidence: the researcher only has to polish weighting algorithms. Also, fixed-length \nvector representations instead of orthographic words constitute excellent input to ma-\nchine learning systems, independent of their particular aim.\nThe fly in the ointment is that traditional distributional semantic models (DSMs) \nare very computationally expensive. The reason is the dimensionality of their vectors, \nTexts in, Meaning out: Neural Language Models in Semantic Similarity Tasks for Russian\n \ngenerally equal to the size of the lexicon. As a result, a model has to operate on sparse \nbut very large matrices. For example, if a corpus includes one million distinct word \ntypes (not a maximum value, as we show below), we will have to compute dot products \nof 1M-dimensional vectors each time we need to find how similar two words are. Vec -\ntors' dimensionality can be reduced to reasonable values using tricks like singular value \ndecomposition or principal components, but this often degrades performance or quality.\nAs a kind of remedy to this, artificial neural networks can learn distributed \nvector representations or “neural embeddings” of comparatively small size (usually \nhundreds of components) [Bengio 2003]. Neural models are directly trained on large \ncorpora to produce vectors which maximize similarity between contextual neighbors \nfound in the data, while minimizing similarity for unseen contexts. Vectors are initial-\nized randomly, but in the course of the training the model converges and semantically \nsimilar words obtain similar vector representations. However, these models were slow \nto train because of non-linear hidden layer.\nRecently, Continuous Bag-of-Words (CBOW) and Continuous Skip-gram  neu-\nral network language models without hidden layer, implemented in the Word2Vec tool \n[Mikolov et al. 2013a], seriously changed the field; using smart combination of already \nknown techniques, they learn high quality embeddings in a very short time. These algo-\nrithms clearly outperform traditional DSMs in various semantic tasks [Baroni et al. 2014].\nFor this competition, we tested both CBOW and skip-gram models. Evaluation \nresults (for a wide range of settings) are given in Section 4.\nIn order to train neural language models one needs not only algorithms, but also \ncorpora. We used 3 text collections:\n1.  News: a corpus of contemporary Russian news-wire texts collected by a com-\nmercial news aggregator. Corpus volume is about 1.8 billion tokens, more \nthan 19 million word types. It was crawled from 1500 news portals, and \nnews pieces themselves are dated from 1 September of 2013 to 30 June \nof 2014 (more than 9 million documents total).\n2.  Web: a corpus of texts found on Russian web pages. It originates from \na search index for one of the major search engines in the Russian market, \nthus is supposed to be quite representative. This source repository itself con -\ntains billions of documents, but to train the model we randomly selected \nabout 9 million pieces (no attention was paid to their source or any other \nproperties). Thus, hopefully the corpus contains all major types of texts \nfound in the Internet, in nearly all possible genres and styles.  \nBoilerplate and templates were filtered out to leave only main textual con -\ntent of these pages, with the help of boilerpipe  library [Kohlschütter et al. \n2010]. After removing non-Cyrillic sentences, the resulting web corpus con -\ntained approximately 940 million tokens.\n3.  Ruscorpora: Russian National Corpus consists of texts which supposedly rep-\nresent the Russian language as a whole. It has been developed for more than \n10 years by a large group of top-ranking linguists, who select texts and segments \nfor inclusion into the corpus. It was extensively described in the literature (see \n[Плунгян 2005], [Савчук 2005]). The size of the main part of RNC is 230 mil-\nlion word tokens, but we worked with the dump containing 174 million tokens.\nKutuzov A., Andreev I.\n \nAll the corpora were lemmatized with MyStem [Segalovich 2003]. We used ver -\nsion 3.0 of the software, with disambiguation turned on. Stop-words were removed, \nas well as single-word sentences (they are useless for constructing context vectors). \nBecause we removed stop-words ourselves, word2vec sub-sampling feature was not \nused. After this pre-processing, News  corpus contained 1,300 million tokens, Web  \ncorpus 620 million tokens, and Ruscorpora  107 million tokens.\nThese corpora represent three different “stimuli” to neural network training \nalgorithm. Ruscorpora is a balanced academic corpus of decent but comparatively \nsmall size, Web  is large, but noisy and unbalanced. Finally, News  is even larger than \nWeb, but cleaner and biased towards one particular genre. These differences caused \ndifferent results in semantic similarity tasks for models trained on the corpora in ques-\ntion (although all corpora proved to be good training sets).\nWe note that Ruscorpora , notwithstanding its size, certainly won this race, re -\nceiving scores essentially higher than the models trained on other two collections. \nThe details are given in the next section.\n4. Evaluation\nThere can be two reasons for a model to perform worse in comparison to the \ngold standard in this evaluation contest: either the model outputs incorrect similarity \nvalues (cosine distances in our case), or one or both words in the presented pair are \nunknown to the model. The former can be treated only by re-training the model with \ndifferent settings or different training set, while the latter can be partially remedied \nby a couple of tricks, both of which we used.\nThe first trick exploits the issue described in the Section 2: many semantically \nsimilar words in Russian have common stems. We “computed” similarity using the \nlongest common string algorithm in case of unknown words, as a kind of “emer -\ngency treatment”. For Ruscorpora models it consistently increased average precision \nin rt track by 0.02 ... 0.05.\nAnother trick is building model assemblies, allowing to “fall back” to another \nmodel in case when unknown words are met. In our case, we knew that Ruscorpora  \nmodel is the best, but only for the words it knows. The Web  model is slightly worse, \nbut knows a lot more distinct words (millions instead of hundreds of thousands). Thus, \nwe query Web model for the word pairs unknown to Ruscorpora. Similarity measures \nrange strictly from 0 to 1 and are generally compatible across models. Only if the words \nare unknown even to the Web model, we fall back further to the longest common string \ntrick. In our experience, such assemblies seriously improved overall performance.\nMost important training parameters for our task are algorithm, vector size, win -\ndow size and frequency threshold. The algorithm can be either CBOW or skip-gram, \nwith the latter being considerably slower. Also, skip-gram performance was consistently \nworse for all corpora except news . This seems to be specific for Russian, as previous \nresearch for English corpora stated that skip-gram is generally better [Mikolov 2013a].\nVector size is the number of dimensions in vector representations; increasing vec-\ntor size generally increases both performance and training time. Window is context \nTexts in, Meaning out: Neural Language Models in Semantic Similarity Tasks for Russian\n \nwidth: how many words to the right and to the left will be considered. Larger window \nsize increases training time and also leads to the model being more “topical” opposed \nto “functional” [Levy and Goldberg 2014]. It means that the model assigns similar \nvectors to topically associated words, not only to direct semantic relatives (synonyms, \netc). This is quite natural, as the model trains on neighbors more distant from the ana-\nlyzed lexical units. Unsurprisingly, models trained on large windows perform better \nin association tasks, while those trained on micro-windows of size 1 or 2 (only imme -\ndiate neighbors) excel at catching direct semantic or functional relations.\nFinally, frequency threshold or minimal count is a minimum frequency a word \nmust possess in order to be considered by the model. All the lexical units with lower \nfrequency are ignored during training and are not assigned vector representations. \nIt is useful in order to get rid of low-frequency noise and train only on sufficiently \npresented evidence. Moreover, the less distinct words the model possess, the faster \nis training; the downside is, of course, absence of some words in the model lexicon.\nIn our experience, typical training speed on an Intel Xeon E5620 2.4GHz ma -\nchine (14 cores) was 116,386 words per second for CBOW algorithm. Web corpus \nmodel training with vector size 500, minimal count 100, window 10 and 5 iterations \n(epochs) took approximately 7 hours; the model saw 3 168 819 885 words in total. \nThis timing is consistent with [Mikolov et al. 2013a].\nThe Table 1 presents our best-performing models, as submitted to RUSSE contest.\nTable 1. Our best results submitted to the evaluation\nTrack hj rt ae ae2\nRank (among \n18 participants)\n2 5 5 4\nTraining \nsettings\nCBOW on Rus-\ncorpora with \ncontext window \n5, minimal \ncount 5 + CBOW \non Web with \ncontext window \n10, minimal \ncount 2\nCBOW on Rus-\ncorpora with \ncontext window \n5, minimal \ncount 5 + \nCBOW on Web \nwith context \nwindow 10, \nminimal count 2\nSkip-gram \non News \nwith context \nwindow 10, \nminimal \ncount 10\nCBOW \non Web \nwith \ncontext \nwindow 5, \nminimal \ncount 2\nScore 0.7187 0.8839 0.8995 0.9662\nNote that minimal count values (defining how much of low-frequency long tail \nis cut off) are different for different corpora. The optimal setting possibly depends \non the vocabulary distribution in a particular text collection, and on how closely it fol-\nlows Zipfian law. We leave this for further research.\nIt is clear that Ruscorpora beats both Web and News corpus in the task of distin-\nguishing semantically related words. This is impressive considering its size: it seems \nthat balance and clever selection of texts for corpus do really make sense and allow \nthe model to learn very high quality vectors. However, when we turn to the task \nKutuzov A., Andreev I.\n \nof detecting associations, sheer volume and diversity of News  and Web become para-\nmount, and they outperform Ruscorpora  models. It is interesting that News  model \nis better with predicting associations from Russian Associative Thesaurus. Probably, \nthis reflects more “official” spirit of this resource in comparison with more colloquial \nnature of Sociaton.org database in the ae2  track, better modeled with Web  texts.\nThe plots below show how performance in different RUSSE tracks depends \non training settings. Two parameters did not change: training mode (CBOW for Rus-\ncorpora and Web  and skip-gram for News ) and minimal count (5 for Ruscorpora , \n2 for Web and 10 for News ); they reproduce the values in the Table 1. Only selected \nplots are shown here; see the link to the others in the Section 5.\nThe plots prove that while increasing vector size generally leads to quality in -\ncrease, after a certain threshold this growth can sometimes stop or even revert\n4. This \nis the case for Ruscorpora (Fig. 1), but not for Web (Fig. 2) or News. We hypothesize \nthat the reason is the size of these two corpora: the volume of data allows filling vec -\ntor components with meaningful relationships, while with Ruscorpora  the model \ncan't learn so many relationships because of data insufficiency; as a result, vectors are \nfilled with noise. This is again consistent with the notion that vector size increase must \nbe accompanied by data growth, expressed in [Mikolov et al. 2013].\nfig. 1. Ruscorpora model performance in rt track depending on vector size\n4 Vector sizes start with 52, because training time is optimal when dimensionality is a multiple of 4.\nTexts in, Meaning out: Neural Language Models in Semantic Similarity Tasks for Russian\n \nfig. 2. Web model performance in rt track depending on vector size\nAs for the window size dynamics, we observe clear direct correlation between \nwindow size and ae2 performance and inverse correlation for rt  performance (Fig. 3). \nAs already stated, a shorter window favors strict functional and semantic relations, \nwhile a larger window (10 words and more) allows catching more vague topical rela -\ntions. Interestingly, Ruscorpora  models are better at ae task with short windows, un-\nlike ae2 (Fig. 4); perhaps, associations from ae  dictionary are more syntagmatic and \ntend to occur close to each other, while Sociation pairs are topical par excellence . This \nfurther proves deep difference between these two associative tasks.\n5. Discussion\nThe first result of our research is that neural embedding models are shown \nto be directly applicable to Russian semantic similarity tasks. Rich morphology does \nnot pose an obstacle for learning meaningful vector representations, with prepro -\ncessing limited to lemmatizing (training on unlemmatized text decreases perfor -\nmance, unlike English tasks where one often doesn't need to even stem the corpus). \nThe result is very persuasive. We believe it is worth to try augmenting many NLP \ntools for Russian with neural embeddings to make existing instruments more seman -\ntically aware.\nKutuzov A., Andreev I.\n \nfig. 3. News model performance in rt track depending on window size\nfig. 4. Ruscorpora model performance in ae track depending on window size\nTexts in, Meaning out: Neural Language Models in Semantic Similarity Tasks for Russian\n \nAnother, more unexpected outcome of our participation in RUSSE was that Rus -\nsian National Corpus (RNC) turned out to be an excellent training set for neural net -\nwork language models. When at start, we were sure that the amount of data plays \ndominant role and that the national corpus will eventually lose, because of being sub -\nstantially smaller. However, it was quite the opposite: in the majority of comparisons \n(especially for semantic relatedness task) models trained on RNC outperformed their \ncompetitors, often even with vectors of lower dimensionality.\nThe only explanation is that RNC is really representative of the Russian language, \nthus providing balanced linguistic evidence for all major vocabulary tiers. Addition -\nally, it seems to contain little or no noise and junk fragments, which sometimes occur \nin other corpora. To sum it up, we certainly recommend training neural language \nmodels on RNC, if this resource is available.\nThe resulting models for each of the three corpora, trained with optimal settings, \ncan be downloaded at http:/ /ling.go.mail.ru/misc/dialogue_2015.html; the full set \nof performance plots for different training settings is also there.\n6. Future Work\nWe have only scratched the surface of exploiting neural embeddings to deal with \nRussian language material. The next step should be to perform a comprehensive study \nof errors typical for each model in their semantic similarity or other decisions. This \ncan shed light on the real nature of differences between models and help in studying \nhuman errors.\nAnother very interesting field of research is corpora comparison through the out -\nput of neural language models trained on them [Kutuzov and Kuzmenko 2015]. Here \nwe, in a way, arrive to an almost omnipotent “mind” able to rapidly evaluate huge \ncorpora, taking into consideration what meanings words in their vocabularies take \nand how they are different from each other.\nOf course, this is not an exhaustive outlook of computational linguistics research \ndirections related to neural lexical vectors. Their foundational nature allows to em -\nploy them everywhere meaning is important; we anticipate a serious growth in se -\nmantic tools' quality.\nLast but not least, we plan to implement a full-fledged web service for testing and \nquerying distributed semantic models for Russian, particularly neural ones. A proto -\ntype to try with is already available online at http:/ /ling.go.mail.ru/dsm.\nAcknowledgments\nThe authors thank the anonymous reviewers for their helpful comments. Sup -\nport from the Basic Research Program of the National Research University Higher \nSchool of Economics is also acknowledged.\nKutuzov A., Andreev I.\n \nReferences\n1. Baroni M., Dinu G., Kruszewski, G. (2014), Don’t count, predict! A systematic \ncomparison of context-counting vs. context-predicting semantic vectors, Pro -\nceedings of the 52nd Annual Meeting of the Association for Computational Lin -\nguistics, Vol. 1, pp. 238–247.\n2. Bengio Y., Ducharme R., Vincent P., Janvin C. (2003), A neural probabilistic lan -\nguage model, The Journal of Machine Learning Research, 3, pp. 1137–1155.\n3. Dinu G., Baroni M. (2014), How to make words with vectors: Phrase generation \nin distributional semantics, Proceedings of the 52nd Annual Meeting of the As -\nsociation for Computational Linguistics, Vol. 1, pp. 624–633.\n4. Harispe S., Ranwez S., Janaqi S., Montmain J. (2013), Semantic Measures for the \nComparison of Units of Language, Concepts or Instances from Text and Knowl-\nedge Base Analysis, arXiv preprint, available at http:/ /arxiv.org/abs/1310.1285\n5. Kohlschütter C., Fankhauser P., Nejdl W. (2010), Boilerplate detection using shal-\nlow text features, Proceedings of the third ACM international conference on Web \nsearch and data mining, pp. 441–450.\n6. Kutuzov A., Kuzmenko E. (2015), Comparing Neural Lexical Models of a Clas -\nsic National Corpus and a Web Corpus: The Case for Russian, A. Gelbukh (Ed.): \nCICLing 2015, Part I, Springer LNCS 9041, pp. 47–58.\n7. Lenci A. (2008), Distributional semantics in linguistic and cognitive research, \nItalian journal of linguistics, 20(1), pp. 1–31.\n8. Levy O., Goldberg Y. (2014), Dependency-based word embeddings, Proceedings \nof the 52nd Annual Meeting of the Association for Computational Linguistics, \nVol. 2, pp. 302–308.\n9. Loukachevitch N. V. (2011), Thesauri in information retrieval tasks [Тезаурусы \nв задачах информационного поиска], Moscow State University Press.\n10. Mikolov T., Chen K., Corrado G., Dean J. (2013a). Efficient estimation of word \nrepresentations in vector space, arXiv preprint, available at http:/ /arxiv.org/\nabs/1301.3781\n11. Mikolov T., Le Q. V., Sutskever I. (2013b). Exploiting similarities among languages \nfor machine translation, arXiv preprint, available at http:/ /arxiv.org/abs/1309.4168\n12. Panchenko A., Loukachevitch N. V., Ustalov D., Paperno D., Meyer C. M., Konstan -\ntinova N. (2015), RUSSE: The First Workshop on Russian Semantic Similarity, \nProceedings of the Dialogue 2015 conference, Moscow, Russia, pp. xx-yy.\n13. Plungian V. A. (2005), Why we make Russian National Corpus? [Зачем мы де -\nлаем Национальный корпус русского языка?], Otechestvennye Zapiski, 2\n14. Savchuk S. O. (2005), Meta-text annotation in Russian National Corpus: founda -\ntions and main functions [Метатекстовая разметка в Национальном корпусе \nрусского языка: базовые принципы и основные функции], Russian National \nCorpus, pp. 62–88.\n15. Segalovich I. (2003), A Fast Morphological Algorithm with Unknown Word Guess-\ning Induced by a Dictionary for a Web Search Engine, MLMTA, pp. 273–280.\n16. Turney P. D., Pantel, P. (2010), From frequency to meaning: Vector space models \nof semantics, Journal of artificial intelligence research, 37(1), pp. 141–188.",
  "topic": "Meaning (existential)",
  "concepts": [
    {
      "name": "Meaning (existential)",
      "score": 0.7215039134025574
    },
    {
      "name": "Semantic similarity",
      "score": 0.6773453950881958
    },
    {
      "name": "Natural language processing",
      "score": 0.63579922914505
    },
    {
      "name": "Task (project management)",
      "score": 0.6318645477294922
    },
    {
      "name": "Computer science",
      "score": 0.5953294038772583
    },
    {
      "name": "Linguistics",
      "score": 0.5870414972305298
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.5770375728607178
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5554034113883972
    },
    {
      "name": "Russian language",
      "score": 0.47063350677490234
    },
    {
      "name": "Psychology",
      "score": 0.32836371660232544
    },
    {
      "name": "Philosophy",
      "score": 0.11960357427597046
    },
    {
      "name": "Engineering",
      "score": 0.051082730293273926
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I118501908",
      "name": "National Research University Higher School of Economics",
      "country": "RU"
    }
  ]
}