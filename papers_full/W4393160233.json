{
  "title": "Can Large Language Models Serve as Rational Players in Game Theory? A Systematic Analysis",
  "url": "https://openalex.org/W4393160233",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3160859509",
      "name": "Caoyun Fan",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A4214366645",
      "name": "Jindou Chen",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2107094063",
      "name": "Yaohui Jin",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2109788442",
      "name": "Hao He",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A3160859509",
      "name": "Caoyun Fan",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A4214366645",
      "name": "Jindou Chen",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2107094063",
      "name": "Yaohui Jin",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2109788442",
      "name": "Hao He",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6853108898",
    "https://openalex.org/W4296154596",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W169877102",
    "https://openalex.org/W6851225003",
    "https://openalex.org/W2795242643",
    "https://openalex.org/W6852595710",
    "https://openalex.org/W6682523407",
    "https://openalex.org/W6679136022",
    "https://openalex.org/W3123190515",
    "https://openalex.org/W6852556678",
    "https://openalex.org/W6823392219",
    "https://openalex.org/W4389712918",
    "https://openalex.org/W6619727252",
    "https://openalex.org/W2739316507",
    "https://openalex.org/W6660445052",
    "https://openalex.org/W6634004741",
    "https://openalex.org/W4313913486",
    "https://openalex.org/W6649915010",
    "https://openalex.org/W6640339052",
    "https://openalex.org/W6790899821",
    "https://openalex.org/W2729639734",
    "https://openalex.org/W6750806242",
    "https://openalex.org/W6676468133",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4302445826",
    "https://openalex.org/W7001894244",
    "https://openalex.org/W6843270802",
    "https://openalex.org/W6729099016",
    "https://openalex.org/W2012626098",
    "https://openalex.org/W6697546665",
    "https://openalex.org/W6621213390",
    "https://openalex.org/W4294576339",
    "https://openalex.org/W2331890423",
    "https://openalex.org/W2302439321",
    "https://openalex.org/W4378718176",
    "https://openalex.org/W4307475457",
    "https://openalex.org/W1517381134",
    "https://openalex.org/W4376455614",
    "https://openalex.org/W1924498641",
    "https://openalex.org/W4389611357",
    "https://openalex.org/W623306076",
    "https://openalex.org/W4363624465",
    "https://openalex.org/W4321455981",
    "https://openalex.org/W3193020011",
    "https://openalex.org/W2128769827",
    "https://openalex.org/W3128342390",
    "https://openalex.org/W2039876175",
    "https://openalex.org/W3126056200",
    "https://openalex.org/W1568846557",
    "https://openalex.org/W1998996598",
    "https://openalex.org/W1963327833",
    "https://openalex.org/W2767135981",
    "https://openalex.org/W645071124",
    "https://openalex.org/W4298023569",
    "https://openalex.org/W2593164128",
    "https://openalex.org/W4361855702",
    "https://openalex.org/W4318480772"
  ],
  "abstract": "Game theory, as an analytical tool, is frequently utilized to analyze human behavior in social science research. With the high alignment between the behavior of Large Language Models (LLMs) and humans, a promising research direction is to employ LLMs as substitutes for humans in game experiments, enabling social science research. However, despite numerous empirical researches on the combination of LLMs and game theory, the capability boundaries of LLMs in game theory remain unclear. In this research, we endeavor to systematically analyze LLMs in the context of game theory. Specifically, rationality, as the fundamental principle of game theory, serves as the metric for evaluating players' behavior --- building a clear desire, refining belief about uncertainty, and taking optimal actions. Accordingly, we select three classical games (dictator game, Rock-Paper-Scissors, and ring-network game) to analyze to what extent LLMs can achieve rationality in these three aspects. The experimental results indicate that even the current state-of-the-art LLM (GPT-4) exhibits substantial disparities compared to humans in game theory. For instance, LLMs struggle to build desires based on uncommon preferences, fail to refine belief from many simple patterns, and may overlook or modify refined belief when taking actions. Therefore, we consider that introducing LLMs into game experiments in the field of social science should be approached with greater caution.",
  "full_text": "Can Large Language Models Serve as Rational Players in Game Theory?\nA Systematic Analysis\nCaoyun Fan*, Jindou Chen*, Yaohui Jin‚Ä†, Hao He‚Ä†\nMoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University\n{fcy3649, goldenbean, jinyh, hehao}@sjtu.edu.cn\nAbstract\nGame theory, as an analytical tool, is frequently utilized to\nanalyze human behavior in social science research. With the\nhigh alignment between the behavior of Large Language\nModels (LLMs) and humans, a promising research direc-\ntion is to employ LLMs as substitutes for humans in game\nexperiments, enabling social science research. However, de-\nspite numerous empirical researches on the combination of\nLLMs and game theory, the capability boundaries of LLMs in\ngame theory remain unclear. In this research, we endeavor to\nsystematically analyze LLMs in the context of game theory.\nSpecifically, rationality, as the fundamental principle of game\ntheory, serves as the metric for evaluating players‚Äô behavior\n‚Äî building a clear desire, refining belief about uncertainty,\nand taking optimal actions. Accordingly, we select three clas-\nsical games (dictator game, Rock-Paper-Scissors, and ring-\nnetwork game) to analyze to what extent LLMs can achieve\nrationality in these three aspects. The experimental results\nindicate that even the current state-of-the-art LLM (GPT-4)\nexhibits substantial disparities compared to humans in game\ntheory. For instance, LLMs struggle to build desires based on\nuncommon preferences, fail to refine belief from many sim-\nple patterns, and may overlook or modify refined belief when\ntaking actions. Therefore, we consider that introducing LLMs\ninto game experiments in the field of social science should be\napproached with greater caution.\nIntroduction\nGame theory (Roughgarden 2010; Dufwenberg 2011) is a\nmathematical theory for evaluating human behavior. Due to\nits highly abstract representation of real-life situations (Os-\nborne and Rubinstein 1995), it becomes a standard analytical\ntool (Charness and Rabin 2002; Cachon and Netessine 2006)\nin the field of social science (e.g., economics, psychology,\nsociology, etc.). With the rapid development of Large Lan-\nguage Models (LLMs) (Ouyang et al. 2022; OpenAI 2023),\na significant advancement is the high alignment between the\nbehavior of LLMs and humans (Bai et al. 2022; Ouyang\net al. 2022; Fan et al. 2024). As a result, many researchers\nconsider LLMs as human-like research subjects (Dillion\net al. 2023) and analyze LLMs‚Äô professional competence in\n*These authors contributed equally.\n‚Ä†Corresponding author.\nCopyright ¬© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nRational player\nDesire\nPercept\nActionBelief\nPreference\nRule\nHistorical records\nùë°!ùë°!\"#ùë°!\"$\n‚Ä¶‚Ä¶\nOpponent\nConsequence\nWIN LOSEor\nFigure 1: Overview of a player‚Äôs behavior in game theory.\nsocial science through game experiments (Chen et al. 2023;\nAkata et al. 2023; Johnson and Obradovich 2023). How-\never, despite the strong motivation for the combination of\nLLMs and game theory (Horton 2023; Guo 2023), the pre-\nliminary researches mainly treat LLMs and game theory em-\npirically as analytical tools in social science (Aher, Arriaga,\nand Kalai 2022; Park et al. 2022; Akata et al. 2023; Bybee\n2023), without systematically analyzing LLMs in the con-\ntext of game theory. As a result, many fundamental aspects\nof LLMs in game theory remain unclear. For example, what\nresearch subjects cannot LLMs play? What types of games\nare LLMs not good at playing? What kind of game processes\nare LLMs more suitable for? And so on.\nWe consider it necessary to systematically analyze LLMs\nin the context of game theory, because such analysis can\nclarify the capability boundaries of LLMs and provide fur-\nther guidance for the widespread use of LLMs in social sci-\nence research. Essentially, the role of game theory is to eval-\nuate the behavior of the research subjects (players) (Rough-\ngarden 2010), as shown in Fig. 1, a player needs to take an\naction a ‚àà Abased on preference P and perceived game in-\nformation I (e.g., game rules and historical records) in order\nto win the game. And rationality, as the fundamental princi-\nple of game theory, is the metric for evaluating players‚Äô be-\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n17960\nhavior (Roughgarden 2010; Dufwenberg 2011). A rational\nplayer is considered to possess three characteristics (Zagare\n1984; Osborne and Rubinstein 1995) as:\n‚Ä¢ build a clear desire for the game.\n‚Ä¢ refine belief about uncertainty in the game.\n‚Ä¢ take optimal actions based on desire and belief.\nSpecifically, desire D(¬∑) represents a player‚Äôs (concrete)\nopinion of each consequence within a game, determined by\na player‚Äôs (abstract) preference P. Belief ‚Ñ¶I is refined from\nthe game information I, and represents a player‚Äôs subjective\njudgment of uncertainty (e.g., opponent‚Äôs action). Taking the\noptimal action a ‚àà Arequires a player to reason by combin-\ning desire D(¬∑) and belief ‚Ñ¶I in the game process. More\ndetails can be found in Section .\nIn this research, we consider the three characteristics of a\nrational player as a reasonable perspective for systematically\nanalyzing LLMs in the context of game theory. Accordingly,\nwe select three classical games (dictator game, Rock-Paper-\nScissors, and ring-network game) for these three charac-\nteristics, respectively. With the dictator game, we find that\nLLMs have the basic ability to build a clear desire. However,\nwhen assigned uncommon preferences, LLMs often suffer\nfrom decreased mathematical ability and inability to under-\nstand preferences. With Rock-Paper-Scissors, we observe\nthat LLMs cannot refine belief from many simple patterns,\nwhich makes us pessimistic about LLMs playing games that\nrequire refining complex beliefs. Nonetheless, GPT-4 ex-\nhibits astonishingly human-like performance in certain pat-\nterns, able to become increasingly confident of refined belief\nas the game information increases. With the ring-network\ngame, we consider that LLMs cannot autonomously follow\nthe player‚Äôs behavior in Fig. 1. Explicitly decomposing the\nbehavior in the game process can improve the ability of\nLLMs to take optimal actions, but the phenomenon of over-\nlooking / modifying refined belief remains unavoidable.\nIn summary, our research systematically explores the ca-\npability boundaries of LLMs in the context of game theory\nfrom three perspectives, and we consider that our research\ncan pave the way for the smooth introduction of LLMs in\nthe field of social science.\nRelated Work\nLLMs in Social Science\nA significant advantage of LLMs is the high alignment with\nhuman behavior (Bai et al. 2022; Ouyang et al. 2022). There-\nfore, from the perspective of cost and efficiency, many social\nscience researches began to employ LLMs to replace hu-\nmans as research subjects (Aher, Arriaga, and Kalai 2022;\nArgyle et al. 2023; Bybee 2023; Park et al. 2022). For ex-\nample, in order to explore fairness and framing effects in\nsociology, LLMs were introduced into the classic game ex-\nperiments (Horton 2023), which demonstrated the potential\nof LLMs to deal with social issues; in the research of con-\nsumer behavior (Brand, Israeli, and Ngwe 2023), the behav-\nior of LLMs was consistent with economic theory in many\nrespects (i.e. downward-sloping demand curves, diminish-\ning marginal utility of income, and state dependence); in fi-\nnance research (Chen et al. 2023), LLMs‚Äô decisions in bud-\ngetary allocation scenarios received higher rationality scores\ncompared to humans; and in psychology experiments (Dil-\nlion et al. 2023), the behavior of LLMs was highly consistent\nwith the mainstream values of society.\nWhile these researches demonstrate the rationality of\nLLMs replacing human research subjects in certain social\nscience domains, there is still a lack of systematic analysis\nof the capability boundaries of LLMs in social science.\nGame Theory\nGame theory, as a mathematical theory, provides a frame-\nwork for analyzing and predicting the behavior of rational\nplayers under conditions of uncertainty (Roughgarden 2010;\nDufwenberg 2011). Game theory was originally developed\nin economics (Ichiishi 2014), and a wide range of economic\nbehaviors, such as market competition, auction mechanism,\nand pricing strategies, were modeled as game experiments\n(Samuelson 2016). With the rapid cross-fertilization of sci-\nentific theories (Shubik 1982), game theory was also applied\nto politics, sociology, psychology, and other fields of social\nscience (Larson 2021; Dillion et al. 2023).\nThe research on the performance of LLMs in game the-\nory has the following advantages: strong operability, the ex-\nperimental design of game theory is often relatively simple;\nstrong analyzability, game theory has comprehensive theo-\nretical support for the experimental results; strong general-\nization, game theory is a high-level abstraction of many phe-\nnomena in the field of social science.\nPreliminaries of Game Theory\nThe core of game theory (Roughgarden 2010) is to guide\nplayers to take optimal actions under conditions of uncer-\ntainty1. Generally, a game is modeled in five parts:\n‚Ä¢ Game information I, e.g., game rules, historical records.\n‚Ä¢ A set A of actions from that players can take.\n‚Ä¢ A set C of possible consequences of action.\n‚Ä¢ A consequence function g : A ‚Üí Cthat associates a\nconsequence with each actions.\n‚Ä¢ A desire function Dc : C ‚ÜíR, which is determined by\nthe player‚Äôs preference P. For any c1, c2 ‚àà C, the player\nprefers c1 if and only if Dc(c1) > Dc(c2).\nTo eliminate uncertainty in the game process, almost\nall game researches employ the belief theory (Morgen-\nstern 1945; Lindley and Savage 1955). That is, a rational\nplayer will estimate a (subjective) probability distribution\nfor any uncertainty based on I, and this is referred to as\nthe player‚Äôs belief (Osborne and Rubinstein 1995). Specif-\nically, the player is assumed to have a belief ‚Ñ¶I, a be-\nlief‚Äôs probability distributionp(‚Ñ¶I), a consequence function\ng : A √ó‚Ñ¶I ‚Üí C. Then, the player attempts to find the op-\ntimal strategy œÄ‚àó(a|I) by maximizing the expected desire\nwith the consideration of ‚Ñ¶I as:\nœÄ‚àó(a|I) = argmax\na‚ààA\nEœâ‚àºp(‚Ñ¶I)[D(a, œâ)], (1)\n1We assume that uncertainty arises only from the opponent‚Äôs\naction. All games in this research satisfy this assumption.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n17961\nwhere D(¬∑) is a simplification of Dc ‚ó¶ g(¬∑).\nIn fact, Eq. 1 explicitly expresses three characteristics of a\nrational player: having a clear desire corresponds to building\nthe desire function D(¬∑); refining belief about uncertainty\ncorresponds to sampling in the belief‚Äôs probability distri-\nbution œâ ‚àº p(‚Ñ¶I); taking optimal actions corresponds to\nchoosing the action that maximizes desire argmax\na‚ààA\nD(a).\nLLMs in Game Theory\nIn this section, we endeavor to conduct a systematic analy-\nsis of LLMs in the context of game theory. Specifically, we\nevaluate to what extent LLMs can achieve three character-\nistics of a rational player through three classic games (dic-\ntator game, Rock-Paper-Scissors, and ring-network game).\nThe LLMs we analyze are openAI‚Äôstext-davinci-003\n(GPT-3), gpt-3.5-turbo (GPT-3.5), gpt-4 (GPT-4),\nthe current state-of-the-art LLMs. All prompts used in the\nthree games, as well as some examples of LLMs perfor-\nmance, can be found in Appendix.\nCan LLMs Build A Clear Desire?\nThe premise of game theory is that each player has an ab-\nstract preference P for the consequence set C. A rational\nplayer should build a concrete desire function D(¬∑) based\non preference P to measure the desire for each consequence\nc ‚àà C. In sociological research (Burns et al. 2021), game\nexperiments are frequently designed to explore the phe-\nnomenon where players with different preferences (cooper-\native or competitive) may have entirely different desires for\nthe same consequence (win-win).\nFor humans, preference and desire seem to be coexistent,\nwhile for LLMs, preference is assigned through a textual\nprompt. Therefore, we need to analyze whether LLMs can\nbuild reasonable desires from textual prompts.\nGame: Dictator Game The dictator game (Charness and\nRabin 2002) is a classic game experiment in sociology\n(Guala and Mittone 2010), which is used to analyze players‚Äô\npersonal preferences P. In this game, there are two players:\nthe dictator and the recipient. Given two allocation options,\nthe dictator needs to take action, choosing one of two alloca-\ntion options, while the recipient must accept the allocation\noption chosen by the dictator. Here, the dictator‚Äôs choice is\nconsidered to reflect the personal preference (Camerer and\nThaler 1995; Leder and Sch ¬®utz 2018). For example, given\ntwo allocation options as:\n‚Ä¢ Option X: The dictator gets $300, the recipient gets $300.\n‚Ä¢ Option Y: The dictator gets $500, the recipient gets $100.\nA dictator who prefers equality is more likely to choose Op-\ntion X, while a dictator who prefers self-interest is more\nlikely to choose Option Y .\nWe choose the dictator game to analyze LLMs‚Äô desire for\ntwo reasons. First, the desires of this game are diverse. Un-\nlike most games with a fixed preference (e.g., to maximize\none‚Äôs own interest), this game allows players to have di-\nverse preferences, which results in diverse desire functions\nand different choices. Second, since the recipient‚Äôs action is\nLLM Pref. Option\nEQ CI SI AL\nGPT-3\nEQ - 1.0 1.0 1.0\nCI 0.4 - 0.3 0.5\nSI 1.0 1.0 - 1.0\nAL 0.0 0.0 0.1 -\nGPT-3.5\nEQ - 1.0 1.0 1.0\nCI 1.0 - 0.9 1.0\nSI 1.0 1.0 - 1.0\nAL 1.0 0.6 0.8 -\nGPT-4\nEQ - 1.0 1.0 1.0\nCI 1.0 - 1.0 0.9\nSI 1.0 1.0 - 1.0\nAL 1.0 1.0 1.0 -\nTable 1: Accuracy of LLMs in the dictator game, where Pref.\nis an abbreviation for Preference.\nknown (to accept), there is no uncertainty in this game, i.e.,\nthe belief ‚Ñ¶I is fixed to œâI. This makes LLMs immune to\npotential interference from the biased belief. Therefore, the\noptimal strategy of the dictator game is expressed as:\nœÄ‚àó(a|I) = argmax\na‚àà{X,Y }\n{D(X, œâI), D(Y, œâI)}, (2)\nwhere X and Y refer to the dictator choosing option X and\noption Y , respectively. Thus, by providing multiple alloca-\ntion options, we can analyze whether the desires built by\nLLMs match the corresponding preferences.\nSetup Following (Grech and Nax 2018), we set four pref-\nerences for LLMs, to analyze different desires as:\n‚Ä¢ Equality (EQ): You have a stronger preference for fair-\nness between players and hate inequality.\n‚Ä¢ Common-Interest (CI): You have a stronger preference\nfor common interest and maximize the joint income.\n‚Ä¢ Self-Interest (SI): You have a stronger preference for\nyour own interest and maximize your own income.\n‚Ä¢ Altruism (AL): You have a stronger preference for an-\nother player‚Äôs interest and maximize another player‚Äôs in-\ncome.\nCompared to the original setting (Charness and Rabin 2002),\nwe adjust the allocation options corresponding to each pref-\nerence to be closer and introduce an additional preference\nAL, thereby increasing the challenge of the game. Specifi-\ncally, we set up allocation options for EQ, CI, SI, and AL\nas follows: ($300, $300), ($400, $300), ($100, $500), and\n($500, $100), respectively. In each option, the first number\nrepresents the dictator‚Äôs income, and the second number rep-\nresents the recipient‚Äôs income. It is worth noting that in game\ntheory, SI and EQ are the most common preferences, fol-\nlowed by CI, while AL hardly ever occurs.\nIn our experiments, we assign LLMs a specific preference\n(e.g., EQ) through a textual prompt, and then verify whether\nLLMs can make preference-consistent choices under differ-\nent combinations of allocation options (i.e., EQ-CI, EQ-SI,\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n17962\nConfusion of \nnumbers\n($300 < $500)\nConfusion of\npreferences\n(AL or CI)\nGPT-4: ‚Ä¶ option X gives the other player 500\ndollars , option Y gives him 300 dollars ‚Ä¶ to\nmaximize the other player‚Äôsincome ‚Ä¶\nGPT-3.5: ‚Ä¶ option Y leads to a higher total income\nof 700 dollars . ‚Ä¶ in order to maximize another\nplayer‚Äôsincome, my final option would be Y .\nGPT-3: By choosing option Y, you will be giving\nanother player an income of 300 dollars which is\nhigher than the 500 dollars if you chose option X.\nOption X (AL)\n($100, $500)\nOption Y (CI)\n($400, $300)vs\nAL\nAL\nAL\nFigure 2: A case of the dictator game. All LLMs are assigned\nthe preference AL, and the allocation options are AL-CI.\nand EQ-AL). Therefore, for each preference, each LLM is\nrequired to play three different dictator games. Each exper-\niment is repeated 10 times and we report the accuracy. The\ntemperature of LLMs is set to 0.7.\nAnalysis The experimental results are displayed in Table\n1. When assigned common preferences (EQ and SI), all\nthree LLMs made preference-consistent choices in all ex-\nperiments, demonstrating the basic ability of LLMs to build\nclear desires from textual prompts. However, LLMs per-\nformed poorly when given uncommon preferences (CI and\nAL). Specifically, for the preference of CI, both GPT-3.5 and\nGPT-4 had sporadic errors, and the accuracy of GPT-3 was\nless than half; for the preference of AL, GPT-3.5 also made\na large number of errors, while GPT-3 almost completely\nmisunderstood AL (making the reference-consistent choice\nonly once). The experimental results reveal significant dif-\nferences in the ability of LLMs to build desires when as-\nsigned common / uncommon preferences.\nTo further analyze the ability of LLMs to build a desire,\nwe conducted a case study on the preference AL as illus-\ntrated in Fig. 2. GPT-3‚Äôs error stemmed from a lack of math-\nematical ability (confusion of numbers), which never oc-\ncurred when GPT-3 is assigned a common preference. This\nseems to imply that the mathematical ability of LLMs as-\nsigned different preferences would be significantly different.\nGPT-3.5 incorrectly assumed that a higher joint income im-\nplied the maximization of the recipient‚Äôs income (confusion\nof preferences), which can be attributed to the deviation of\nthe built desire of GPT-3.5. GPT-4 performed well in this\ncase, both analysis and choice were consistent with humans.\nInsight: LLMs have the basic ability to build clear de-\nsires based on textual prompts, but struggle to build de-\nsires from uncommon preferences.\nWe consider that providing more explicit and specific ex-\nplanations of preferences may be helpful to LLMs when\ngame experiments involve uncommon preferences.\nCan LLMs Refine Belief?\nIn game theory, a rational player needs to refine belief ‚Ñ¶I\nabout uncertainty (e.g., opponent‚Äôs action) from the game\ninformation I. Essentially, refining belief is a process of\nStrategy Name Description\nat\no = C constant remain\nconstant\nat\no = f(a<t\no ) loop-2 loop\nbetween two actions\nloop-3 loop among three actions\nat\no = f(a<t\nm ) copy\ncopy opponent‚Äôs previous action\ncounter counter opponent‚Äôs previous action\nat\no ‚àº p(P) sample sample\nin preference probability\nTable 2: Summary of the opponent‚Äôs strategy in R-S-P.\nsynthesizing surface-level information into deeper insights.\nBecause of the emphasis on decision-making in high uncer-\ntainty (Wellman 2017), game experiments in politics often\nexamine players‚Äô ability to refine belief.\nUnfortunately, even for humans, refining belief can be a\nchallenge. Therefore, it is meaningful to determine which\ntypes of beliefs LLMs can or cannot refine.\nGame: Rock-Paper-Scissors Rock-Paper-Scissors (R-P-\nS) is a simultaneous, zero-sum game for two players. The\nrules of R-P-S are simple: rock beats scissors, scissors beat\npaper, paper beats rock; and if both players take the same\naction, the game is a tie.\nR-P-S is an ideal game to analyze LLMs‚Äô ability to refine\nbelief. On the one hand, analyzing statistical patterns of non-\nrandom opponents‚Äô historical records can bring significant\nadvantages in R-P-S (Fisher 2008). On the other hand, for\nLLMs, R-P-S‚Äôs preference (to win) is clear and the rules are\nsimple: given the opponent‚Äôs action, LLMs can always take\nthe correct action based on the rules. Therefore, we consider\nthat LLMs‚Äô performance in R-P-S can reflect LLMs‚Äô ability\nto refine belief in game theory.\nSpecifically, in round i, the player‚Äôs (my) action is noted\nas ai\nm and the opponent‚Äôs action is noted as ai\no. After play-\ning t ‚àí 1 consecutive rounds with the same opponent, the\nhistorical records {a<t\no , a<t\nm } can be considered as the game\ninformation I for refining belief ‚Ñ¶I in round t. So, the opti-\nmal strategy in round t can be expressed as:\nœÄ‚àó(at\nm|I) = œÄ‚àó(at|a<t\no , a<t\nm )\n= argmax\nat\nm‚ààA\nEat\no‚àºp(‚Ñ¶{s<to ,a<t\nm })[D(at\no, at\nm)]. (3)\nSince LLMs can grasp the preferences and rules of R-P-\nS, the difficulty of Eq. 3 lies in refining belief, i.e., at\no ‚àº\np(‚Ñ¶{s<t\no ,a<t\nm }).\nSetup In international R-P-S programming competitions\n(Billings 2000), a non-random opponent‚Äôs action (in round\nt) is determined by the historical records{a<t\no , a<t\nm } and the\nopponent‚Äôs preference P as:\nat\no ‚àº p(A|a<t\no , a<t\nm , P). (4)\nEssentially, refining belief refers to making p(‚Ñ¶) approach\np(A|a<t\no , a<t\nm , P). For a fine-grained analysis of the ability\nof LLMs to refine belief, we set up 4 simple opponent‚Äôs pat-\nterns based on Eq. 4, as shown in Table 2. at\no = C is the\nbasic pattern, evaluating the most basic refinement ability of\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n17963\nFigure 3: Average payoff of LLMs for each round in R-S-P.\n‚Ä¶ can give me the\nmost points regardless\nof the other player‚Äôs\nchoice ... I will choose\nR as a counter ‚Ä¶\n‚Ä¶ the other player has\nchosen P, R, S, P, R, S ...\nIt doesn‚Äôt seem to follow\na specific pattern ‚Ä¶ I\nshould choose P‚Ä¶\n‚Ä¶ It appears that their\nchoices seem to be\nrandom ... rather than\ntrying to guess the\nother ‚Ä¶ I choose R.\nround\n(a) Analysis of GPT-3.5\n‚Ä¶ the other player\nmay follow a rotating\npatter ‚Ä¶ this is purely\nspeculative ‚Ä¶ then I\nshould choose S ‚Ä¶\n‚Ä¶ rotating their choices\nin the pattern P, R, S, ...\nno guarantee that the\nother player follow ‚Ä¶ S\nis a reasonable choice.\n‚Ä¶ consistently rotated\nin the pattern as P, R,\nS ‚Ä¶ Based on the\nobserved pattern , the\nnext action is S.\nround\n(b) Analysis of GPT-4\nFigure 4: Analysis of LLMs on loop-3. The symbols under\nthe round axis indicate the opponent‚Äôs action for each round.\nLLMs. In this pattern, we conduct three experiments with\nthe opponent‚Äôs actions remaining constant as R, S, and P, re-\nspectively. at\no = f(a<t\no ) is determined by a<t\no . Under the\nMarkov assumption (Puterman 1994), this pattern behaves\nas a loop. We conduct three loop-2 experiments (R-P, P-\nS, S-R) and one loop-3 experiment (R-P-S) in this pattern.\nat\no = f(a<t\nm ) is determined by a<t\nm . Under the Markov as-\nsumption, we conduct two experiments in this pattern: copy\n/ counter the player‚Äôs previous actionat‚àí1\nm . at\no ‚àº p(P) is de-\ntermined by the preferenceP. To implement this pattern, we\nset a preference probability distribution of(0.70, 0.15, 0.15)\nand conduct three experiments where the opponent has a\npreference for R, S, and P respectively, to take action by\nsampling in the distribution probability.\nTo quantify the results of R-P-S, we set the payoff for a\nwin as 2, for a tie as 1, and for a loss as 0. In each exper-\niment, LLMs need to play 10 consecutive rounds of R-P-S\nagainst an opponent with a specific pattern, and the histori-\ncal records are updated in time. Each experiment is repeated\n10 times, and the temperature of LLMs is set to 0.7.\nAnalysis The average payoffs of each LLM are shown in\nFig. 3. Specifically, in the basic pattern (constant), GPT-3\nperformed close to random guessing, suggesting that GPT-3\nlacked the basic ability to refine belief. In contrast, GPT-\n3.5‚Äôs average payoff was significantly higher than random\nguessing and continued to rise; GPT-4 consistently took cor-\nrect actions after approximately 3 rounds. In at\no = f(a<t\no )\npattern (loop-2, loop-3), GPT-3 and GPT-3.5 appeared to be\ncapable of capturing some cyclical features, but they were\nunable to take correct actions. However, the performance of\nGPT-4 was exciting, with the update of historical records,\nthe payoff was clearly rising. This led us to believe that GPT-\n4 can refine belief from this pattern. In at\no = f(a<t\nm ) pattern\n(copy, counter), the situation was not ideal, GPT-4 seemed\nto have a slight advantage, but the overall performance of\nLLMs was not good enough. In at\no ‚àº p(P) pattern (sam-\nple), the performance of all LLMs was similar to random\nguessing. Overall, LLMs are unable to refine belief well in\nmost patterns, whereas for humans, the patterns involved in\nour experiments are quite easy to refine.\nFor a more detailed analysis, we compared the analysis\nof GPT-3.5 and GPT-4 on loop-3, as shown in Fig. 4. The\nanalysis of GPT-3.5 demonstrated a lack of ability to refine\nbelief. Even though GPT-3.5 expressed that the opponent‚Äôs\nactions were P-R-S loops, it still believed that the opponent\ndid not follow a specific pattern. The analysis of GPT-4, in\ncontrast, was amazing, not only can GPT-4 summarize the\nopponent‚Äôs pattern, but the tone gradually changed from un-\ncertain to confident as the historical records were updated.\nInsight: Currently, the ability of LLMs to refine belief is\nstill immature and cannot refine belief from many specific\npatterns (even if simple).\nTherefore, we strongly recommend the cautious introduc-\ntion of LLMs in game experiments that require refining\ncomplex belief. Nevertheless, the performance of GPT-4\nin at\no = f(a<t\no ) pattern makes us look forward to more\npowerful LLMs in the future.\nCan LLMs Take Optimal Actions?\nTaking optimal actions is the ultimate goal of a rational\nplayer in game theory, which requires the player to rea-\nson with known information (desire D(¬∑) and belief ‚Ñ¶I).\nEconomics‚Äô obsession with optimal actions naturally makes\ngame experiments in economics focus on analyzing players‚Äô\nactions (Kirzner 1962; O‚Äôsullivan, Sheffrin, and Swan 2007).\nHowever, for LLMs, there are various forms of combining\ndesire and belief to take optimal actions, and it is unclear\nwhich form LLMs are more suitable in the game process.\nHere, we mainly explore the effect of the form of belief on\nLLMs taking optimal actions.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n17964\n#\nImplicit Belief ‚Üí Take Action Explicit Belief ‚Üí Take Action Given Belief ‚Üí Take Action\nGPT-3 GPT-3.5 GPT-4 GPT-3 GPT-3.5 GPT-4 GPT-3 GPT-3.5 GPT-4\nam am am ao am ao am ao am am am am\n(a) 0.20 0.50 0.10 0.65 0.15 0.95 0.60 1.00 0.75 0.75 0.85 1.00\n(b) 0.40 0.40 0.00 0.60 0.30 1.00 0.65 1.00 0.60 0.40 0.95 1.00\n(c) 0.10 0.10 0.00 0.75 0.00 0.95 0.25 0.95 0.65 0.15 0.90 1.00\n(d) 0.05 0.10 0.00 0.30 0.00 0.95 0.35 1.00 0.75 0.10 0.80 1.00\nTable 3: Performance of LLMs in different settings in the ring-network game.ao represents the accuracy of refining belief (the\nopponent‚Äôs action), and am represents the accuracy of taking the optimal action.\n0\n10\n5\n0\n5\n5\n10\n15\nOpponent\nPlayer\nX\nY\nU V\n(a) Payoff bimatrix\nV\n0 5\n5 10\nOpponent\nX\nY\n10 0\n5 15\nU V\nPlayer\nU\nX\nY\nùê∑! ùëå, ùëà > ùê∑!(ùëã, ùëà)\nùê∑! ùëå, ùëâ > ùê∑!(ùëã, ùëâ)\nùê∑\" ùëâ|ùëå > ùê∑\"(ùëà|ùëå)\nùëé! = ùëå\nùëé\" = ùëâ (b) Ideal game process\nFigure 5: Overview of ring-network game, where red num-\nbers / blue numbers represent the player‚Äôs and opponent‚Äôs\npayoffs, and Dm(¬∑) and Do(¬∑) represent the player‚Äôs and op-\nponent‚Äôs desire functions.\nGame: Ring-Network Game The ring-network game is\na game experiment that evaluates the rationality of taking\nactions in economics (Kneeland 2015). In this research, we\nsimplify it to a kind of 2√ó 2 game (two players with two dis-\ncrete actions). This game involves two players, the opponent\nand the player, whose preferences are to maximize their own\npayoff. In the game process, the opponent and the player\nneed to take an action ao ‚àà {X, Y} and am ‚àà {U, V},\nrespectively. The payoff bimatrix M consists of the oppo-\nnent‚Äôs matrix Mo and the player‚Äôs matrix Mm, as shown in\nFig. 5(a), which specifies the payoffs of both sides for each\ncombination of actions.\nThe characteristic of the ring-network game is that play-\ners‚Äô optimal action is determined sequentially by the other\nplayers‚Äô optimal actions (Kneeland 2015). The ideal game\nprocess is shown in Fig. 5(b), for the opponent, the payoff\nof Y is always higher than X regardless of the player‚Äôs ac-\ntions. Therefore, the opponent‚Äôs optimal action is always Y .\nFor the player, the opponent‚Äôs optimal action can be ana-\nlyzed according to the opponent‚Äôs payoff matrix Mo, so the\nplayer should be able to refine belief ‚Ñ¶: ao = Y . Then, the\nplayer can take the optimal action (am = V ) based on belief\nand the player‚Äôs payoff matrix Mm. According to the above\nanalysis, the game information I is the payoff bimatrix M,\nand the player‚Äôs optimal strategy can be expressed as:\nœÄ‚àó(am|I) = argmax\nam‚àà{U,V }\n[p(ao|M) ¬∑ Dm(am|ao, M)], (5)\nwhere refining belief corresponds top(ao|M) and taking the\noptimal action corresponds to Dm(am|ao, M). What we fo-\nV\n10 0\n5 15\nPlayer\nU\nX\nY\nV\n8 7\n7 8\nPlayer\nU\nX\nY\nV\n10 0\n5 6\nPlayer\nU\nX\nY\nV\n40 0\n5 15\nPlayer\nU\nX\nY\n(a) (b) (c) (d)\nFigure 6: Setup of the player‚Äôs payoff matrix.\ncus on is what form of bridging these two parts is more suit-\nable for LLMs to take optimal action.\nSetup Specifically, we set up three forms of combining be-\nlief based on Eq. 5 to analyze the performance of LLMs tak-\ning optimal actions in the ring-network game as:\n‚Ä¢ Implicit Belief ‚Üí Take Action: We prompt LLMs in the\ndialogue to take the optimal action based on the payoff\nbimatrix directly, i.e., LLM(am|M). In this form, LLMs\nneed to autonomously transform this process into Eq. 5.\n‚Ä¢ Explicit Belief ‚Üí Take Action: First, we prompt LLMs\nin the dialogue to refine belief (analyze the opponent‚Äôs\naction) based on the payoff bimatrix, i.e., LLM(ao|M).\nThen, we continue the dialogue by prompting LLMs to\ntake the optimal action based on the payoff bimatrix and\nthe refined belief, i.e.,LLM(am|ao, M). In this form, Eq.\n5 is explicitly decoupled into two parts.\n‚Ä¢ Given Belief ‚Üí Take Action:The opponent‚Äôs optimal ac-\ntion is explicitly provided to LLMs in the dialogue, and\nwe prompt LLMs to take the optimal action based on\nthe opponent‚Äôs optimal action and payoff bimatrix, i.e.,\nLLM(am|ao, M). In this form, LLMs only need to im-\nplement the second part of Eq. 5.\nBy analyzing the performance of LLMs in these three forms,\nwe expect to obtain some caveats to help LLMs take optimal\nactions in game theory.\nIn our experiments, in order to control the difficulty of re-\nfining belief, we keep the opponent‚Äôs payoff matrix constant,\nwhich means the player‚Äôs belief ‚Ñ¶: ao = Y should remain\nconstant. We set up different player‚Äôs payoff matrices, as\nshown in Fig. 6, to adjust the difficulty of taking the optimal\naction: (a) is the original setup; (b) reduces the difference in\npayoffs while keeping the expected payoffs toam ‚àà {U, V}\nconstant; (c) increases the expected payoff for the incorrect\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n17965\nGPT-3.5: So, the rational choice\nfor another player to maximize his\nown points would be Option Y.\nBelief\nùëù(ùëé!|ùëÄ)\nGPT-3.5: Option U gives me the\nchance to win 40 points ‚Ä¶ the\nrational choice is Option U.\nAction\nùëù(ùëé\"|ùëé!, ùëÄ)\n(a) Belief is overlooked: p(am|ao, M) ‚Üí p(am|M).\nGPT-4: In summary, considering\nonly their own point gain, the other\nplayer would choose Option Y.\nBelief\nùëù(ùëé!|ùëÄ)\nGPT-4: ‚Ä¶ the other player might\nswitch to Option X, the best choice\nis Option U ‚Ä¶\nAction\nùëù(ùëé\"|ùëé!, ùëÄ)\n(b) Belief is modified: p(am|ao, M) ‚Üí p(am|ÀÜao, M).\nFigure 7: Two cases of LLMs‚Äô inability to take optimal ac-\ntions based on refined belief.\naction am = U; and (d) decreases the expected payoff for\nthe correct action am = V .\nIn practice, we find that LLMs are biased towards action\nnames, e.g. GPT-3 prefers U to V . In order to eliminate the\ninfluence of the bias of LLMs to take the optimal action, we\nswap the payoffs of U and V in the player‚Äôs payoff matrix\nin Fig. 6, to form a swapped payoff matrix, and we repeat\nthe game 10 times each under the original and swapped pay-\noff matrices and report the accuracy of the LLMs taking the\noptimal action. The temperature of LLMs is set to 0.7.\nAnalysis The performance of LLMs is shown in Table 3.\nSince GPT-3 performs poorly in all three forms, we mainly\nanalyze the performance of GPT-3.5 and GPT-4.\nIt is well known that human players‚Äô belief in game the-\nory is implicit, so the form closest to humans taking optimal\nactions would be Implicit Belief ‚Üí Take Action. However,\nall LLMs performed poorly in this form, and GPT-4 was al-\nmost completely unable to even take the optimal action. This\nreflected the capability gap between LLMs and humans, that\nwas, LLMs cannot autonomously follow human behavior in\nthe game process. In contrast, in the form of Explicit Belief\n‚Üí Take Action, by decomposing human behavior explic-\nitly, the accuracy of LLMs to take the optimal action was\nsignificantly improved. This showed that LLMs were more\nsuitable to take optimal actions in the explicit game pro-\ncess. This phenomenon was not unique to game theory, and\nmany researches pointed out that explicitly decoupling hu-\nman thoughts (think step-by-step) can significantly improve\nthe performance of LLMs (Wei et al. 2022).\nHowever, we were surprised that in the form of Explicit\nBelief ‚Üí Take Action, LLMs were able to accurately refine\nbelief (the accuracy for ao is above 0.95), but were unable\nto make the optimal action based on the refined belief well\nin subsequent dialogues, with the accuracy of GPT-4 being\nabout 0.70 for am, and the accuracy of GPT-3.5 being even\nlower. As a comparison, we observed that when in the form\nof Given Belief ‚Üí Take Action, GPT-4 was able to consis-\ntently take the optimal action, and GPT-3.5‚Äôs accuracy also\nexceeded 0.80. Intuitively, LLMs are more suitable for tak-\ning optimal actions combining given belief rather than re-\nfined belief, even though the content of the two beliefs is\nthe same. In order to explore the reasons, we conducted a\ndetailed study on the error cases of GPT-3.5 and GPT-4 in\nthe form of Explicit Belief ‚Üí Take Action, and we summa-\nrized the two situations for LLMs‚Äô inability to take optimal\nactions based on refined belief as:\n‚Ä¢ Belief is overlooked: LLMs are confused by the game in-\nformation and thus overlook the refined belief to take the\noptimal action in the subsequent dialogue.\n‚Ä¢ Belief is modified: LLMs lack confidence in the refined\nbelief and thus modify the refined belief to take the opti-\nmal action in the subsequent dialogue.\nThe error cases are shown in Fig. 7. In the first situation, as\nshown in Fig. 7(a), LLMs were confused by the expected\npayoff (Dm(U) > Dm(V )), and thus incorrectly equated\np(am|ao, M) with p(am|M). This occurred mainly on GPT-\n3.5. Observing the performance of GPT-3.5 in the form of\nExplicit Belief ‚Üí Take Action, the accuracy of taking the\noptimal action was around 0.60 when the expected payoffs\nwere the same (a and b), while the accuracy dropped to\naround 0.30 when the expected payoffs were different (c and\nd). In the second case, as shown in Fig. 7(b), LLMs modified\nthe refined correct belief when taking the action due to lack\nof confidence, i.e., changing p(am|ao, M) to p(am|ÀÜao, M).\nWe found that modification of refined belief occurred more\nfrequently on GPT-4.\nInsight: We consider that LLMs do not have the ability to\nautonomously follow human behavior in the game process\n(in Fig. 1). As a result, it is necessary to explicitly decou-\nple human behavior for LLMs in game theory.\nHowever, even in the explicit game process, LLMs still ap-\npear to overlook / modify the refined belief. One possible\nsolution is to transform the refined belief into the given\nbelief in the dialogue.\nConclusion\nThe rapid development of LLMs leads us to believe that\nLLMs will eventually be integrated in all aspects of the\nhuman world, and therefore it is urgent to systematically\nanalyze the capability boundaries of LLMs in various do-\nmains. In this research, we endeavor to systematically ana-\nlyze LLMs in an important field of social science ‚Äî game\ntheory. Our experiments evaluate to what extent LLMs can\nserve as rational players from three aspects and find some\nweaknesses of LLMs in game theory.\nAs an early attempt to analyze LLMs in the context of\ngame theory, our research has some limitations. For exam-\nple, the difficulty of the game we selected is relatively low,\nnot close enough to the real game scenarios; our perspective\nof analyzing the ability of LLMs is not rich enough, only\nconsidering the principle of rationality; our process of an-\nalyzing the game experiments is relatively rough and lacks\nmore comparative and ablative experiments; and so on. In\nconclusion, the research on LLMs in the context of game\ntheory is still in a very preliminary stage, and a lot of ex-\nploratory researches are required.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n17966\nAcknowledgments\nThis work was supported by the Shanghai Municipal Sci-\nence and Technology Major Project (2021SHZDZX0102),\nand the Fundamental Research Funds for the Central Uni-\nversities.\nReferences\nAher, G.; Arriaga, R.; and Kalai, A. T. 2022. Using Large\nLanguage Models to Simulate Multiple Humans. Arxiv.\nAkata, E.; Schulz, L.; Coda-Forno, J.; Oh, S. J.; Bethge, M.;\nand Schulz, E. 2023. Playing repeated games with Large\nLanguage Models. Arxiv.\nArgyle, L. P.; Busby, E. C.; Fulda, N.; Gubler, J. R.; Rytting,\nC.; and Wingate, D. 2023. Out of one, many: Using language\nmodels to simulate human samples. Political Analysis.\nBai, Y .; Jones, A.; Ndousse, K.; Askell, A.; Chen, A.; Das-\nSarma, N.; Drain, D.; Fort, S.; Ganguli, D.; Henighan, T.;\nJoseph, N.; Kadavath, S.; Kernion, J.; Conerly, T.; Showk,\nS. E.; Elhage, N.; Hatfield-Dodds, Z.; Hernandez, D.; Hume,\nT.; Johnston, S.; Kravec, S.; Lovitt, L.; Nanda, N.; Olsson,\nC.; Amodei, D.; Brown, T. B.; Clark, J.; McCandlish, S.;\nOlah, C.; Mann, B.; and Kaplan, J. 2022. Training a Helpful\nand Harmless Assistant with Reinforcement Learning from\nHuman Feedback. Arxiv.\nBillings, D. 2000. The first international RoShamBo pro-\ngramming competition. ICGA Journal.\nBrand, J.; Israeli, A.; and Ngwe, D. 2023. Using GPT for\nMarket Research. SSRN Electronic Journal.\nBurns, T. R.; Roszkowska, E.; Corte, U.; and Machado, N.\n2021. Sociological game theory: agency, social structures\nand interaction processes. Sociologia, Problemas e Pr¬¥aticas.\nBybee, L. 2023. Surveying Generative AI‚Äôs Economic Ex-\npectations. Arxiv.\nCachon, G. P.; and Netessine, S. 2006. Game theory in sup-\nply chain analysis. Models, methods, and applications for\ninnovative decision making.\nCamerer, C.; and Thaler, R. H. 1995. Anomalies: Ultima-\ntums, Dictators and Manners. JEP.\nCharness, G.; and Rabin, M. 2002. Understanding social\npreferences with simple tests. QJE.\nChen, Y .; Liu, T. X.; Shan, Y .; and Zhong, S. 2023. The\nEmergence of Economic Rationality of GPT. Arxiv.\nDillion, D.; Tandon, N.; Gu, Y .; and Gray, K. 2023. Can\nAI language models replace human participants? Trends in\nCognitive Sciences.\nDufwenberg, M. A. 2011. Game theory. Wiley interdisci-\nplinary reviews. Cognitive science.\nFan, C.; Tian, J.; Li, Y .; He, H.; and Jin, Y . 2024. Compa-\nrable Demonstrations are Important in In-Context Learning:\nA Novel Perspective on Demonstration Selection. ICASSP.\nFisher, L. 2008. Rock, paper, scissors : game theory in ev-\neryday life. New York: Basic Books.\nGrech, P. D.; and Nax, H. H. 2018. Rational Altruism?\nOn Preference Estimation and Dictator Game Experiments.\nGames & Political Behavior eJournal.\nGuala, F.; and Mittone, L. 2010. Paradigmatic Experiments:\nthe Dictator Game. Journal of Socio-economics.\nGuo, F. 2023. GPT Agents in Game Theory Experiments.\nArxiv.\nHorton, J. J. 2023. Large language models as simulated eco-\nnomic agents: What can we learn from homo silicus? Tech-\nnical report, National Bureau of Economic Research.\nIchiishi, T. 2014. Game theory for economic analysis. Else-\nvier.\nJohnson, T.; and Obradovich, N. 2023. Evidence of behav-\nior consistent with self-interest and altruism in an artificially\nintelligent agent. Arxiv.\nKirzner, I. M. 1962. Rational action and economic theory.\nJournal of Political Economy.\nKneeland, T. 2015. Identifying higher-order rationality.\nEconometrica.\nLarson, J. M. 2021. Networks of conflict and cooperation.\nAnnual Review of Political Science.\nLeder, J.; and Sch¬®utz, A. 2018. Encyclopedia of Personality\nand Individual Differences. Springer.\nLindley, D. V .; and Savage, L. J. 1955. The Foundations of\nStatistics. The Mathematical Gazette.\nMorgenstern, O. 1945. Theory of Games and Economic Be-\nhavior. Journal of the American Statistical Association.\nOpenAI. 2023. GPT-4 Technical Report. Arxiv.\nOsborne, M. J.; and Rubinstein, A. 1995. A Course in Game\nTheory. Wiley.\nO‚Äôsullivan, A.; Sheffrin, S. M.; and Swan, K. 2007. Eco-\nnomics: Principles in action. Prentice-Hall Inc.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright,\nC. L.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray,\nA.; Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens,\nM.; Askell, A.; Welinder, P.; Christiano, P. F.; Leike, J.; and\nLowe, R. 2022. Training language models to follow instruc-\ntions with human feedback. In NeurIPS.\nPark, J. S.; Popowski, L.; Cai, C.; Morris, M. R.; Liang, P.;\nand Bernstein, M. S. 2022. Social simulacra: Creating pop-\nulated prototypes for social computing systems. In UIST.\nPuterman, M. L. 1994. Markov Decision Processes: Discrete\nStochastic Dynamic Programming. In Wiley.\nRoughgarden, T. 2010. Algorithmic game theory. ACM.\nSamuelson, L. 2016. Game theory in economics and beyond.\nJournal of Economic Perspectives.\nShubik, M. 1982. Game theory in the social sciences: Con-\ncepts and solutions. In The Journal of Finance.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.;\nXia, F.; Chi, E. H.; Le, Q. V .; and Zhou, D. 2022. Chain-\nof-Thought Prompting Elicits Reasoning in Large Language\nModels. In NeurIPS.\nWellman, L. A. 2017. Mitigating political uncertainty. Re-\nview of Accounting Studies.\nZagare, F. C. 1984. Game Theory: Concepts and Applica-\ntions. Sage Publications.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n17967",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.48716995120048523
    },
    {
      "name": "Game theory",
      "score": 0.45192548632621765
    },
    {
      "name": "Cognitive science",
      "score": 0.3803129494190216
    },
    {
      "name": "Mathematical economics",
      "score": 0.3302513659000397
    },
    {
      "name": "Psychology",
      "score": 0.31440916657447815
    },
    {
      "name": "Economics",
      "score": 0.1610303819179535
    }
  ]
}