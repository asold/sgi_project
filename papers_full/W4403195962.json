{
    "title": "Knowledge Editing for Large Language Models: A Survey",
    "url": "https://openalex.org/W4403195962",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2109424293",
            "name": "Song Wang",
            "affiliations": [
                "University of Virginia"
            ]
        },
        {
            "id": "https://openalex.org/A2234128883",
            "name": "Yaochen Zhu",
            "affiliations": [
                "University of Virginia"
            ]
        },
        {
            "id": "https://openalex.org/A2125426276",
            "name": "Haochen Liu",
            "affiliations": [
                "University of Virginia"
            ]
        },
        {
            "id": "https://openalex.org/A5097767807",
            "name": "Zaiyi Zheng",
            "affiliations": [
                "University of Virginia"
            ]
        },
        {
            "id": "https://openalex.org/A2095964268",
            "name": "Chen Chen",
            "affiliations": [
                "University of Virginia"
            ]
        },
        {
            "id": "https://openalex.org/A2137915745",
            "name": "Jundong Li",
            "affiliations": [
                "University of Virginia"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3184144760",
        "https://openalex.org/W3177323791",
        "https://openalex.org/W2075210427",
        "https://openalex.org/W1933349210",
        "https://openalex.org/W4353015365",
        "https://openalex.org/W3106976604",
        "https://openalex.org/W4285291532",
        "https://openalex.org/W4391136507",
        "https://openalex.org/W3104215796",
        "https://openalex.org/W4389523760",
        "https://openalex.org/W4393147193",
        "https://openalex.org/W4385571232",
        "https://openalex.org/W4206965408",
        "https://openalex.org/W4394743141",
        "https://openalex.org/W3152884768",
        "https://openalex.org/W4387426919",
        "https://openalex.org/W2909212904",
        "https://openalex.org/W4206118214",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2945260553",
        "https://openalex.org/W4385574113",
        "https://openalex.org/W3127227595",
        "https://openalex.org/W3037979089",
        "https://openalex.org/W4385231746",
        "https://openalex.org/W4402670278",
        "https://openalex.org/W2593116425",
        "https://openalex.org/W4385572928",
        "https://openalex.org/W4205460703",
        "https://openalex.org/W3096831136",
        "https://openalex.org/W2560730294",
        "https://openalex.org/W4389519122",
        "https://openalex.org/W4404780750",
        "https://openalex.org/W4386566752",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2950626540",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W4385570984",
        "https://openalex.org/W4402670752",
        "https://openalex.org/W3092053846",
        "https://openalex.org/W4402671939",
        "https://openalex.org/W3044438666",
        "https://openalex.org/W4220967417",
        "https://openalex.org/W4366390744",
        "https://openalex.org/W4323655724",
        "https://openalex.org/W2129217160",
        "https://openalex.org/W2912924812",
        "https://openalex.org/W2962881743",
        "https://openalex.org/W4385570326",
        "https://openalex.org/W4393147125",
        "https://openalex.org/W4392297945",
        "https://openalex.org/W2963870853",
        "https://openalex.org/W4385567201",
        "https://openalex.org/W4389520749",
        "https://openalex.org/W4382246105",
        "https://openalex.org/W4385571124",
        "https://openalex.org/W4385574041",
        "https://openalex.org/W4229026446",
        "https://openalex.org/W4385569933",
        "https://openalex.org/W4200313831",
        "https://openalex.org/W4385894687",
        "https://openalex.org/W3169283738",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W4389523672",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W1985610876",
        "https://openalex.org/W4285113702",
        "https://openalex.org/W3155584966",
        "https://openalex.org/W3170180819",
        "https://openalex.org/W4385573164",
        "https://openalex.org/W3131645232",
        "https://openalex.org/W3098267758",
        "https://openalex.org/W3089472875",
        "https://openalex.org/W4384561707",
        "https://openalex.org/W2963961878",
        "https://openalex.org/W2080133951",
        "https://openalex.org/W3175604467",
        "https://openalex.org/W4385572634",
        "https://openalex.org/W4385572883",
        "https://openalex.org/W4210827551",
        "https://openalex.org/W4389524330",
        "https://openalex.org/W4389520370",
        "https://openalex.org/W4393152647",
        "https://openalex.org/W3176828726",
        "https://openalex.org/W3211777899",
        "https://openalex.org/W4404783928",
        "https://openalex.org/W4389518797",
        "https://openalex.org/W4389519586",
        "https://openalex.org/W4366588626",
        "https://openalex.org/W6770432743",
        "https://openalex.org/W3154934604",
        "https://openalex.org/W3041133507",
        "https://openalex.org/W4288329833",
        "https://openalex.org/W3202712981"
    ],
    "abstract": "Large Language Models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently, Knowledge-based Model Editing (KME), also known as Knowledge Editing or Model Editing , has attracted increasing attention, which aims at precisely modifying the LLMs to incorporate specific knowledge, without negatively influencing other irrelevant knowledge. In this survey, we aim at providing a comprehensive and in-depth overview of recent advances in the field of KME. We first introduce a general formulation of KME to encompass different KME strategies. Afterward, we provide an innovative taxonomy of KME techniques based on how the new knowledge is introduced into pre-trained LLMs, and investigate existing KME strategies while analyzing key insights, advantages, and limitations of methods from each category. Moreover, representative metrics, datasets, and applications of KME are introduced accordingly. Finally, we provide an in-depth analysis regarding the practicality and remaining challenges of KME and suggest promising research directions for further advancement in this field.",
    "full_text": null
}