{
  "title": "A Transformer-Based Model With Self-Distillation for Multimodal Emotion Recognition in Conversations",
  "url": "https://openalex.org/W4367281761",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1997697239",
      "name": "Ma Hui",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A598734319",
      "name": "Wang Jian",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2366246759",
      "name": "Lin Hongfei",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1910347209",
      "name": "Zhang Bo",
      "affiliations": [
        "Dalian University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2351212978",
      "name": "Zhang Yi-jia",
      "affiliations": [
        "Dalian Maritime University"
      ]
    },
    {
      "id": "https://openalex.org/A1937328933",
      "name": "Xu, Bo",
      "affiliations": [
        "Dalian University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2184126968",
    "https://openalex.org/W2982605440",
    "https://openalex.org/W2998563994",
    "https://openalex.org/W2805662932",
    "https://openalex.org/W2891359673",
    "https://openalex.org/W2964300796",
    "https://openalex.org/W3116016199",
    "https://openalex.org/W3040953174",
    "https://openalex.org/W3211488063",
    "https://openalex.org/W3216650611",
    "https://openalex.org/W2985882473",
    "https://openalex.org/W3173396651",
    "https://openalex.org/W3207810231",
    "https://openalex.org/W3203741465",
    "https://openalex.org/W4221154966",
    "https://openalex.org/W2146334809",
    "https://openalex.org/W1987030537",
    "https://openalex.org/W2963686995",
    "https://openalex.org/W2102953093",
    "https://openalex.org/W2158943324",
    "https://openalex.org/W2996849360",
    "https://openalex.org/W4303649263",
    "https://openalex.org/W4385573848",
    "https://openalex.org/W2886193235",
    "https://openalex.org/W2079725295",
    "https://openalex.org/W2583643061",
    "https://openalex.org/W2546919788",
    "https://openalex.org/W2963128932",
    "https://openalex.org/W2964346351",
    "https://openalex.org/W2964010806",
    "https://openalex.org/W2787581402",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W3034266838",
    "https://openalex.org/W3128412859",
    "https://openalex.org/W3206201541",
    "https://openalex.org/W6638523607",
    "https://openalex.org/W6637551013",
    "https://openalex.org/W2739879705",
    "https://openalex.org/W2887783173",
    "https://openalex.org/W3034342078",
    "https://openalex.org/W2620998106",
    "https://openalex.org/W6772695483",
    "https://openalex.org/W2987861506",
    "https://openalex.org/W2981441441",
    "https://openalex.org/W2886300652",
    "https://openalex.org/W3094378983",
    "https://openalex.org/W3137028092",
    "https://openalex.org/W3096318498",
    "https://openalex.org/W3174340663",
    "https://openalex.org/W3205421042",
    "https://openalex.org/W3034756453",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3098556456",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W2090777335",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W6805026025",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W1690739335",
    "https://openalex.org/W2939362715",
    "https://openalex.org/W4309427394",
    "https://openalex.org/W3217114103",
    "https://openalex.org/W2997224071",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2187089797"
  ],
  "abstract": "Emotion recognition in conversations (ERC), the task of recognizing the\\nemotion of each utterance in a conversation, is crucial for building empathetic\\nmachines. Existing studies focus mainly on capturing context- and\\nspeaker-sensitive dependencies on the textual modality but ignore the\\nsignificance of multimodal information. Different from emotion recognition in\\ntextual conversations, capturing intra- and inter-modal interactions between\\nutterances, learning weights between different modalities, and enhancing modal\\nrepresentations play important roles in multimodal ERC. In this paper, we\\npropose a transformer-based model with self-distillation (SDT) for the task.\\nThe transformer-based model captures intra- and inter-modal interactions by\\nutilizing intra- and inter-modal transformers, and learns weights between\\nmodalities dynamically by designing a hierarchical gated fusion strategy.\\nFurthermore, to learn more expressive modal representations, we treat soft\\nlabels of the proposed model as extra training supervision. Specifically, we\\nintroduce self-distillation to transfer knowledge of hard and soft labels from\\nthe proposed model to each modality. Experiments on IEMOCAP and MELD datasets\\ndemonstrate that SDT outperforms previous state-of-the-art baselines.\\n",
  "full_text": "arXiv:2310.20494v1  [cs.AI]  31 Oct 2023\n1\nA Transformer-based Model with Self-distillation\nfor Multimodal Emotion Recognition in\nConversations\nHui Ma, Jian W ang, Hongfei Lin, Bo Zhang, Yijia Zhang, and Bo X u\nAbstract—Emotion recognition in conversations (ERC), the\ntask of recognizing the emotion of each utterance in a con-\nversation, is crucial for building empathetic machines. Ex ist-\ning studies focus mainly on capturing context- and speaker-\nsensitive dependencies on the textual modality but ignore t he\nsigniﬁcance of multimodal information. Different from emo tion\nrecognition in textual conversations, capturing intra- an d inter-\nmodal interactions between utterances, learning weights b etween\ndifferent modalities, and enhancing modal representation s play\nimportant roles in multimodal ERC. In this paper , we propose a\ntransformer-based model with self-distillation (SDT) 1 for the task.\nThe transformer-based model captures intra- and inter-mod al\ninteractions by utilizing intra- and inter-modal transfor mers, and\nlearns weights between modalities dynamically by designin g a\nhierarchical gated fusion strategy . Furthermore, to learn more\nexpressive modal representations, we treat soft labels of t he\nproposed model as extra training supervision. Speciﬁcally , we\nintroduce self-distillation to transfer knowledge of hard and soft\nlabels from the proposed model to each modality . Experiment s\non IEMOCAP and MELD datasets demonstrate that SDT out-\nperforms previous state-of-the-art baselines.\nIndex T erms—Multimodal emotion recognition in conversa-\ntions, intra- and inter-modal interactions, multimodal fu sion,\nmodal representation.\nI. I N T RO D U CT IO N\nE\nMOTION recognition in conversations (ERC) aims to\nautomatically recognize the emotion of each utterance\nin a conversation. The task has recently become an impor-\ntant research topic due to its wide applications in opinion\nmining [1], health care [2], and building empathic dialogue\nsystems [3], etc. Unlike traditional emotion recognition ( ER)\non context-free sentences, modeling context- and speaker-\nsensitive dependencies lie at the heart of ERC.\nExisting mainstream works on ERC can generally be cat-\negorized into sequence- and graph-based methods. Sequence -\nbased methods [4]–[11] use recurrent neural networks or\ntransformers to model long-distance contextual informati on\nin a conversation. In contrast, graph-based methods [12]–[ 15]\ndesign graph structures for conversations and then use grap h\nneural networks to capture multiple dependencies. Althoug h\nHui Ma, Jian W ang (Corresponding author), Hongfei Lin, Bo Zh ang and Bo\nXu are with the School of Computer Science and T echnology , Da lian Univer-\nsity of T echnology , Dalian 116024, China (e-mail:huima@ma il.dlut.edu.cn;\nwangjian@dlut.edu.cn; hﬂin@dlut.edu.cn; zhangbo1998@m ail.dlut.edu.cn;\nxubo@dlut.edu.cn).\nY ijia Zhang is with the School of Information Science and\nT echnology , Dalian Maritime University , Dalian 116024, Ch ina (e-\nmail:zhangyijia@dlmu.edu.cn)\n1 The code is available at https://github.com/butterﬂiesss /SDT .\nWendy, we had a deal!  \nYeah, you promised! \nWendy! Wendy! Wendy! \n[\nanger ] Who was that? [ neutral ]\nWendy bailed. I have no \nwaitress. [ \nsadness ] Oh... that's too bad. \nBye bye. [ \nsadness ] \nTwelve dollars an hour. \n[\nneutral ] Mon. I wish I could, \nbut I've made plans to \nwalk around. [ \nneutral ]\n1\n2\n4\n5\n3\n6\nMonica Rachel \nAudio Visual Text \nTurn \nText \nYou know, Rachel, \nwhen you ran out of \nyour wedding, I was \nthere for you. [ \nanger ] \n7\nAudio Visual \nFig. 1. A multimodal conversation example from the Friends T V series.\nthese methods show promising performance, most of them fo-\ncus primarily on textual conversations without leveraging other\nmodalities (i.e., acoustic and visual modalities). Accord ing to\nMehrabian [16], people express emotions in a variety of ways ,\nincluding verbal, vocal, and facial expressions. Therefor e, mul-\ntimodal information is more useful for understanding emoti ons\nthan unimodal information.\nUnlike emotion recognition in textual conversations, we\nargure that three key characteristics are essential for mul -\ntimodal ERC: intra- and inter-modal interactions between\nconversation utterances, different contributions of moda lities,\nand efﬁcient modal representations. An example is shown in\nFig. 1. (1) T o understand the importance of intra- and inter-\nmodal interactions, let us focus on the single and multiple\nmodalities, respectively. Recognizing “anger” emotion of the\n7th utterance spoken by Monica is difﬁcult using only “Y ou\nknow , Rachel, when you ran out of your wedding, I was\nthere for you. ”, but it becomes easy when looking back to\nthe textual expression of the 6th utterance because Rachel has\nmade plans to walk around. Additionally, we believe there\nare two types of inter-modal interactions: interactions be tween\nthe same and different utterances. First, as stated above, i t\nis hard to identify the emotion of the 7th utterance using\nits textual expression; however, it also would be easy when\nfused with its visual and acoustic expressions since they bu rst\ninstantaneously. Second, we know that the textual expressi on\nof the 5th utterance shows “neutral” emotion, and hence it\ncould be possible to identify “neutral” emotion of the 6th\nutterance by interacting this utterance’s visual expressi on and\nthe 5th utterance’s textual expression. (2) T o understand the\n2\nimportance of contributions of different modalities, let u s focus\non the 3rd utterance. The textual and acoustic expressions play\nmore critical roles in recognizing “sadness” emotion than t he\nvisual expression because a smiling face usually means “joy ”\nemotion. (3) T o understand the importance of efﬁcient modal\nrepresentations, let us focus on the textual expression of t he\n1st utterance, which contains multiple exclamation points. If\nthe learned representation does not include the meaning of “ !”,\nit is challenging to identify “anger” emotion.\nTherefore, it is valuable to capture intra- and inter-modal\ninteractions between utterances, dynamically learn weigh ts\nbetween modalities, and enhance modal representations for\nmultimodal ERC. However, existing studies of the task have\nsome limitations in achieving these characteristics. On th e\none hand, most methods have drawbacks in modeling intra-\nand inter-modal interactions. For example, CMN [4], ICON\n[5], and DialogueRNN [6] concatenate unimodal features at\nthe input level, and thus cannot capture intra-modal intera c-\ntions explicitly. While DialogueTRM [10] designs hierarch ical\ntransformer and multi-grained interaction fusion modules to\nexplore intra- and inter-modal emotional behaviors, it ig-\nnores inter-modal interactions between different utteran ces.\nMMGCN [13] and MM-DFN [17] are graph-based fusion\nmethods that require manually constructed graph structure s to\nrepresent conversations. On the other hand, existing metho ds\nrely on the designed model to learn modal representations, b ut\nno work focuses on further improving modal representations\nusing model-agnostic techniques for ERC.\nIn this work, a transformer-based model with self-\ndistillation (SDT) is proposed to take into account the thre e\naforementioned characteristics. First, we introduce intr a- and\ninter-modal transformers in a modality encoder to capture\nintra- and inter-modal interactions, and take positional a nd\nspeaker embeddings as additional inputs of these transform -\ners to capture contextual and speaker information. Next, a\nhierarchical gated fusion strategy is proposed to dynamica lly\nfuse information from multiple modalities. Then, we predic t\nemotion labels of conversation utterances based on fused mu l-\ntimodal representations in an emotion classiﬁer. W e call th e\nabove three components a transformer-based model. Finally , to\nlearn more effective modal representations, we introduce s elf-\ndistillation into the proposed transformer-based model, w hich\ntransfers knowledge of hard and soft labels from the model to\neach modality. W e treat the proposed model as the teacher and\ndesign three students according to three existing modaliti es.\nThese students are trained by distilling knowledge from the\nteacher to learn better modal representations.\nIn summary, our contributions are as follows:\n• W e propose a transformer-based model for multimodal\nERC that contains a modality encoder for capturing\nintra- and inter-modal interactions between conversation\nutterances and a hierarchical gated fusion strategy for\nadaptively learning weights between modalities.\n• T o learn more effective modal representations, we devise\nself-distillation that transfers knowledge of hard and sof t\nlabels from the proposed model to each modality.\n• Experiments on two benchmark datasets show the superi-\nority of our proposed model. In addition, several studies\nare conducted to investigate the impact of positional and\nspeaker embeddings, intra- and inter-modal transformers,\nself-distillation loss functions, and hierarchical gated\nfusion strategy.\nThe rest of this paper is organized as follows: Section II\ndiscusses the related work; Section III formalizes the task\ndeﬁnition and describes the proposed model; Section IV give s\nthe experimental settings; Section V presents the experime ntal\nresults and discussion; Finally, Section VI concludes the p aper\nand provides directions for further work.\nII. RE L AT E D WO RK\nA. Emotion Recognition in Conversations\nERC has attracted widespread interest among researchers\nwith the increase in available conversation datasets, such as\nIMEOCAP [18], A VEC [19], and MELD [20], etc. Early stud-\nies primarily used lexicon-based methods [21], [22]. Recen t\nworks have generally resorted to deep neural networks and\nfocused on modeling context- and speaker-sensitive depen-\ndencies. W e divide the existing methods into two categories :\nspeaker-ignorant and speaker-dependent methods, accordi ng to\nwhether they utilize speaker information.\nSpeaker-ignorant methods do not distinguish speakers and\nfocus only on capturing contextual information in a convers a-\ntion. HiGRU [7] contains two gated recurrent units (GRUs) to\nmodel contextual relationships between words and utteranc es,\nrespectively. AGHMN [23] uses a hierarchical memory net-\nwork to enhance utterance representations and introduce an\nattention GRU to model contextual information. MVN [11]\nutilizes a multi-view network to model word- and utterance-\nlevel dependencies in a conversation. In contrast, speaker -\ndependent methods model both context- and speaker-sensiti ve\ndependencies. DialogueRNN [6] leverages three distinct GR Us\nto update speaker, context, and emotional states in a conver -\nsation, respectively. DialogueGCN [12] uses a graph convol u-\ntional network to model speaker and conversation sequentia l\ninformation. HiTrans [8] consists of two hierarchical tran s-\nformers to capture global contextual information and explo its\nan auxiliary task to model speaker-sensitive dependencies .\nHowever, most of them are proposed for the textual modal-\nity, ignoring the effectiveness of other modalities. Due to the\npromising performance in the multimodal community, some\napproaches tend to address multimodal ERC. DialogueTRM\n[10] explores intra- and inter-modal emotional behaviors u sing\nhierarchical transformer and multi-grained interaction f usion\nmodules, respectively. MMGCN [13] constructs a fully con-\nnected graph to model multimodal and long-distance con-\ntextual information, and speaker embeddings are added for\nencoding speaker information. MM-DFN [17] designs a graph-\nbased dynamic fusion module to reduce redundancy and\nenhance complementarity between modalities. MMTr [24]\npreserves the integrity of main modal representations and\nenhances weak modal representations by using multi-head\nattention. UniMSE [25] performs modality fusion at syntact ic\nand semantic levels and introduces inter-modality contras tive\nlearning to differentiate fusion representations among sa mples.\n3\nThis paper focuses on exploring intra- and inter-modal inte rac-\ntions between utterances, learning weights between modali ties,\nand enhancing modal representations for multimodal ERC.\nB. Multimodal Language Analysis\nMultimodal language analysis is a rapidly growing ﬁeld\nand includes various tasks [26], such as multimodal emotion\nrecognition, sentiment analysis, and personality traits r ecogni-\ntion. The key in this area is to fuse multimodal information.\nEarly studies on multimodal fusion mainly included early\nfusion and late fusion. Early fusion [27], [28] integrates\nfeatures of different modalities at the input level. Late fu sion\n[29], [30] constructs distinct models for each modality and\nthen ensembles their outputs by majority voting or weighted\naveraging, etc. Unfortunately, as stated in [31], these two kinds\nof fusion methods cannot effectively capture intra- and int er-\nmodal interactions.\nSubsequently, model fusion has become popular and various\nmodels have been proposed. TFN [32] models unimodal,\nbimodal, and trimodal interactions explicitly by computin g\nCartesian product. LMF [31] utilizes low-rank weight tenso rs\nfor multimodal fusion, which reduces the complexity of TFN.\nMFN [33] learns cross-modal interactions with an attention\nmechanism and stores information over time by a multi-view\ngated memory. MulT [34] utilizes cross-modal transformers\nto model long-range dependencies across modalities. Rahma n\net al. [35] ﬁne-tuned large pre-trained transformer models for\nmultimodal language by designing a multimodal adaptation\ngate (MAG). Self-MM [36] uses a unimodal label generation\nstrategy to acquire independent unimodal supervision and t hen\nlearns multimodal and unimodal tasks jointly. Y uan et al. [3 7]\nadopted transformer encoders to model intra- and inter-mod al\ninteractions between modality sequences. In order to captu re\nintra- and inter-modal interactions between conversation utter-\nances and meanwhile learn weights between modalities, we\npresent a transformer-based model.\nC. Knowledge Distillation\nKnowledge distillation (KD) aims at transferring knowledg e\nfrom a large teacher network to a small student network. The\nknowledge mainly includes soft labels of the last output lay er\n(i.e., output-based knowledge) [38], features of intermed iate\nlayers (i.e., feature-based knowledge) [39], and relation ships\nbetween different layers (i.e., relation-based knowledge ) [40].\nDepending on the learning schemes, existing methods on KD\nare categorized into three classes: ofﬂine distillation [4 1], [42],\nonline distillation [43], [44], and self-distillation [45 ], [46].\nIn ofﬂine distillation, the teacher network is ﬁrst trained and\nthen the pre-trained teacher distills its knowledge to guid e the\nstudent training. In online distillation, the teacher and s tudent\nnetworks are updated simultaneously, and hence its trainin g\nprocess is only one-phase. Self-distillation is a special c ase of\nonline distillation that teaches a single network using its own\nknowledge.\nRecently, KD has been used for multimodal emotion recog-\nnition. For example, Albanie et al. [47] transferred visual\nknowledge into a speech emotion recognition model using\nunlabelled video data. W ang et al. [48] proposed K-injectio n\nsubnetworks to distill linguistic and acoustic knowledge r ep-\nresenting group emotions and transfer implicit knowledge i nto\nthe audiovisual model for group emotion recognition. Schon -\neveld et al. [49] applied KD to further improve performance f or\nfacial expression recognition. Most existing models belon g to\nofﬂine distillation, which requires training a teacher net work.\nIn contrast, self-distillation needs no extra network exce pt for\nthe network itself. While self-distillation has been succe ssfully\napplied in computer vision and natural language processing\n[50]–[52], it focuses on unimodal tasks.\nIn this work, we adopt the idea of self-distillation to enhan ce\nmodal representations for multimodal ERC. Moreover, outpu t-\nbased knowledge is used only due to the following reasons: (1 )\nSoft labels can be used as training supervision which contai n\ndark knowledge [38] and can provide effective regularizati on\nfor the model [53]. (2) Intuitively, the features of differe nt\nmodalities vary widely, and hence matching fused multimoda l\nfeatures with unimodal features is inappropriate 2. (3) Our\nteacher and student networks lying in the same model have\ndifferent architectures that results in an inability to inj ect rela-\ntionships between different layers of the teacher network i nto\nthe student network [41]. Therefore, we adopt output-based\nknowledge rather than feature- and relation-based knowled ge.\nIII. M E T H O D O L O G Y\nA. T ask Deﬁnition\nA conversation is composed of N consecutive utterances\n{u1, u 2, · · ·, u N } and M speakers {s1, s 2, · · ·, s M }. Each\nutterance ui is spoken by a speaker sφ (ui), where φ is the\nmapping between an utterance and its corresponding speaker ’s\nindex. Moreover, ui involves textual ( t), acoustic ( a), and\nvisual ( v) modalities, and their feature representations are de-\nnoted as ut\ni ∈ Rdt , ua\ni ∈ Rda , and uv\ni ∈ Rdv , respectively. W e\nrepresent textual, acoustic, and visual modality sequence s of\nall utterances in the conversation as Ut = [ut\n1; ut\n2; · · ·; ut\nN ] ∈\nRN×dt , Ua = [ ua\n1; ua\n2 ; · · ·; ua\nN ] ∈ RN×da , and Uv =\n[uv\n1; uv\n2; · · ·; uv\nN ] ∈ RN×dv , respectively. The ERC task aims\nto predict the emotion label of each utterance ui from pre-\ndeﬁned emotion categories.\nB. Overview\nFig. 2 gives an overview of our proposed SDT . After\nextracting utterance-level unimodal features, the transf ormer-\nbased model consists of three modules: a modality encoder\nmodule for capturing intra- and inter-modal interactions b e-\ntween different utterances, a hierarchical gated fusion mo dule\nfor adaptively learning weights between modalities, and an\nemotion classiﬁer module for predicting emotion labels. Fu r-\nthermore, we introduce self-distillation and devise two ki nds of\nlosses to transfer knowledge from our proposed model within\neach modality to learn better modal representations.\n2 W e tried to add feature-based knowledge, but the performanc e drops\nsigniﬁcantly .\n4\nS\nIntra-modal \nTransformer \nInter-modal \nTransformer \n( ) a t ®\n( ) v t ®\nText \nAudio \nVisual \nS\nS\nConv1D Conv1D Conv1D \ntU\naU\nvU\nPositional Embeddings \nSpeaker Embeddings Forward Flow \nSupervision from Soft Labels \nSupervision from Hard Labels \nIntra-modal \nTransformer \n( ) a a ®\nInter-modal \nTransformer \n( ) t a ®\n( ) v a ®\nIntra-modal \nTransformer \n( ) v v ®\nInter-modal \nTransformer \n( ) t v ®\n( ) a v ®\nS\nModality Encoder \nElement-wise Addition \nSoftmax \nClassifier \nC\ns\ns\ns\nC\ns\ns\ns\nC\ns\ns\ns\n Classifier  Classifier  Classifier v a t\nY\nµY\nµaY µtYµvY\nCE L\nKL L\nTask L\nElement-wise Product \nSigmoid Function \n( ) t t ® t¢H\na¢H\nv¢H\ns\nConcatenation C\n¢H\nHierarchial Gated Fusion \nSelf-distillation \nEmotion Classifier \nDenseNet \nopenSMILE \nRoBERTa \n Feature Extraction \nFig. 2. The overall architecture of SDT . After extracting ut terance-level unimodal features, it consists of four key co mponents: Modality Encoder, Hierarchical\nGated Fusion, Emotion Classiﬁer, and Self-distillation.\nC. Modality Encoder\nThe modality encoder obtains modality-enhanced modality\nsequence representations that can learn intra- and inter-m odal\ninteractions between conversation utterances.\nT emporal Convolution: T o ensure that three unimodal\nsequence representations lie in the same space, we feed them\ninto a 1D convolutional layer:\nU′\nm = Conv1D (Um, k m) ∈ RN×d, m ∈ { t, a, v }, (1)\nwhere km is the size of convolutional kernel for m modality,\nN is the number of utterances in the conversation, and d is\nthe common dimension.\nPositional Embeddings: T o utilize positional and sequen-\ntial information of the utterance sequence, we introduce po si-\ntional embeddings [54] to augment the convolved sequence:\nPE(pos, 2i) = sin\n( pos\n100002i/d\n)\n,\nPE(pos, 2i+1) = cos\n( pos\n100002i/d\n)\n,\n(2)\nwhere pos is the utterance index and i is the dimension index.\nSpeaker Embeddings: T o capture speaker information of\nthe utterance sequence, we also design speaker embeddings t o\naugment the convolved sequence. Speaker sj in conversations\nis mapped into a vector:\nsj = Vso (sj ) ∈ Rd, j = 1, 2, · · ·, M, (3)\nwhere M is the total number of speakers, Vs ∈ Rd×M is a\ntrainable speaker embedding matrix, and o (sj ) ∈ RM is a\none-hot vector of speaker sj , i.e., 1 in the jth position and 0\notherwise.\nHence, speaker embeddings corresponding to the conversa-\ntion can be represented as SE =\n[\nsφ (u1); sφ (u2); · · ·; sφ (uN )\n]\n.\nOverall, we augment positional and speaker embeddings to\nthe convolved sequence:\nHm = U′\nm + PE + SE. (4)\nHere, Hm is the low-level positional- and speaker-aware\nutterance sequence representation for m modality.\nIntra- and Inter-modal T ransformers: W e introduce\nintra- and inter-modal transformers to model intra- and int er-\nmodal interactions for the utterance sequence, respective ly.\nThese transformers adopt the transformer encoder [54], whi ch\ncontains three inputs, queries Q ∈ RTq ×dk , keys K ∈ RTk×dk ,\nand values V ∈ RTk ×dv . W e denote the transformer encoder\nas Transformer (Q, K, V).\nFor the intra-modal transformer, we take Hm as queries,\nkeys, and values:\nHm→m = Transformer (Hm, Hm, Hm) ∈ RN×d, (5)\nwhere m ∈ { t, a, v }. The intra-modal transformer enhances\nm-modality sequence representation by itself and thus can ca p-\nture intra-modal interactions between the utterance seque nce.\nFor the inter-modal transformer, we take Hm as queries,\nand Hn as keys and values:\nHn→m = Transformer (Hm, Hn, Hn) ∈ RN×d, (6)\nwhere m ∈ { t, a, v } and n ∈ { t, a, v } − { m}. The inter-\nmodal transformer enables m modality to get information from\n5\nn modality and hence can capture inter-modal interactions\nbetween the utterance sequence.\nIn summary, n-enhanced m-modality sequence representa-\ntion, Hn→m, is obtained from the modality encoder module,\nwhere n, m ∈ { t, a, v }.\nD. Hierarchical Gated Fusion\nW e design a hierarchical gated fusion module containing\nunimodal- and multimodal-level gated fusions to adaptivel y\nobtain enhanced single-modality sequence representation and\ndynamically learn weights between these enhanced modality\nrepresentations, respectively.\nUnimodal-level Gated Fusion: W e ﬁrst use a gated mech-\nanism to ﬁlter out irrelevant information in Hn→m:\ngn→m = σ (Wn→m ·Hn→m) , (7)\nH′\nn→m = Hn→m ⊗ gn→m, (8)\nwhere Wn→m ∈ Rd×d is a weight matrix, σ is the sigmoid\nfunction, ⊗ is the element-wise product, and gn→m denotes\nthe gate.\nThen, we concatenate H′\nm→m, H′\nn1→m, and H′\nn2→m, fol-\nlowed by a fully connected (FC) layer to obtain enhanced\nm-modality sequence representation:\nH′\nm =Wm ·\n[\nH′\nm→m; H′\nn1→m; H′\nn2→m\n]\n+bm ∈ RN×d, (9)\nwhere m ∈ { t, a, v }, n1 and n2 represent other two modalities,\nWm ∈ R3d×d and bm ∈ Rd are trainable parameters.\nW e set H′\nm = [h′\nm1; h′\nm2; · · ·; h′\nmN ], where h′\nmi is en-\nhanced m-modality representation for the utterance ui.\nMultimodal-level Gated Fusion: W e also design a gated\nmechanism using the softmax function to dynamically learn\nweights between enhanced modalities for each utterance.\nSpeciﬁcally, the ﬁnal multimodal representation of the ut-\nterance ui is calculated by:\n[gti; gai; gvi] = softmax ( [W·h′\nti; W·h′\nai; W·h′\nvi] ), (10)\nh′\ni =\n∑\nm∈{t,a,v }\nh′\nmi ⊗ gmi, (11)\nwhere W ∈ Rd×d is a weight matrix, gti, gai, and gvi are\nlearned weights of t, a, v modalities for the utterance ui,\nrespectively.\nThus, multimodal sequence representation of conversation\nutterances is obtained and denoted as H′ = [h′\n1; h′\n2; · · ·; h′\nN ].\nE. Emotion Classiﬁer\nT o calculate probabilities over C emotion categories, H′ is\nfed into a classiﬁer with an FC and softmax layer:\nE = We ·H′ + be ∈ RN×C , (12)\nˆY = softmax (E) , (13)\nwhere We ∈ Rd×C and be ∈ RC are trainable parameters. W e\nset ˆY = [ˆy1; ˆy2; · · ·; ˆyN ], where ˆyi is the emotion probability\nvector for the utterance ui. Finally, we choose argmax (ˆyi) as\nthe predicted emotion label for ui.\nT ask Loss: W e utilize the cross-entropy loss for estimating\nthe quality of emotion predictions during training:\nLT ask = − 1\nN\nN∑\ni=1\nC∑\nj=1\nyi,j log (ˆyi,j ), (14)\nwhere N represents the number of utterances in the conver-\nsation, and C represents the number of emotion classes. yi\nand ˆyi denote the ground-truth one-hot vector and probability\nvector for the emotion of ui, respectively.\nF . Self-distillation\nSoft labels containing informative dark knowledge can be\nused as training supervision; hence, we devise self-distil lation\nto transfer knowledge of hard and soft labels to each modalit y,\nand guide the model in learning more expressive modal\nrepresentations.\nW e treat our proposed transformer-based model as the\nteacher and design three students according to existing mod al-\nities. Speciﬁcally, a classiﬁer consisting of an FC and soft max\nlayer only used during training, is set after each unimodal-\nlevel gated fusion. During training, textual, acoustic, an d visual\nmodality encoders with their corresponding unimodal-leve l\ngated fusions and classiﬁers are trained as three students ( i.e.,\nstudent t, student a, student v) via distilling from the teacher.\nThe output of student m is its predicted emotion probabil-\nities:\nEm = W′\nm ·ReLU (H′\nm) +b′\nm ∈ RN×C , (15)\nˆYm = softmax (Em) ,\nˆYτ\nm = softmax (Em/τ ) , (16)\nwhere m ∈ { t, a, v }, W′\nm ∈ Rd×C and b′\nm ∈ RC are train-\nable parameters. τ is the temperature to soften ˆYm (written\nas ˆYτ\nm after softened) and a higher τ produces a softer distri-\nbution over classes [38]. W e set ˆYm = [ˆym1; ˆym2; · · ·; ˆymN ]\nand ˆYτ\nm = [ˆyτ\nm1; ˆyτ\nm2; · · ·; ˆyτ\nmN ].\nDuring training, we introduce two kinds of losses to train\nthe student m to learn better enhanced m-modality sequence\nrepresentation, where m ∈ { t, a, v }.\nCross Entropy Loss: W e minimize the cross entropy loss\nbetween the predicted probability of the student m and the\nground-truth:\nLm\nCE = − 1\nN\nN∑\ni=1\nC∑\nj=1\nyi,j log (ˆymi,j ), (17)\nwhere ˆymi is the emotion probability vector of the student\nm for ui. In this way, knowledge from hard labels is directly\nintroduced to the student to learn better modal representat ions.\nKL Divergence Loss: T o make the output probability of\nthe student m approximate the output of the teacher (i.e., soft\nlabels), the Kullback-Leibler (KL) divergence loss betwee n\nthem is minimized:\nLm\nKL = 1\nN\nN∑\ni=1\nC∑\nj=1\nˆyτ\nmi,j log\n(\nˆyτ\nmi,j\nˆyτ\ni,j\n)\n, (18)\n6\nT ABLE I\nSTAT I S T I C S O F T H E T WO DATA S E T S .\nDataset #Conversations #Utterances #Classes\nTrain+V al T est Train+V al T est\nIEMOCAP 120 31 5810 1623 6\nMELD 1153 280 11098 2610 7\nwhere ˆyτ\nmi and ˆyτ\ni are soften probability distributions of the\nstudent m and the teacher, respectively. In this way, knowledge\nfrom soft labels is transferred to the student to learn bette r\nmodal representations.\nWith both hard and soft labels, the overall loss can be\nexpressed as:\nL = γ1LT ask + γ2LCE + γ3LKL , (19)\nLCE =\n∑\nm∈{t,a,v }\nLm\nCE , (20)\nLKL =\n∑\nm∈{t,a,v }\nLm\nKL , (21)\nwhere γ1, γ2, and γ3 are hyper-parameters that control the\nweights of the three kinds of losses. In experiments, we set\nγ1 = γ2 = γ3 = 1.\nIV . E X P E RIM E N TA L SE T T IN G S\nA. Datasets and Evaluations\nW e use IEMOCAP [18] and MELD [20] datasets to evaluate\nthe proposed model. The statistics of the two datasets are li sted\nin T able I.\nIEMOCAP: The dataset consists of two-way conversations\nof ten speakers, containing 153 conversations and 7, 433\nutterances. The dataset is divided into ﬁve sessions, where the\nﬁrst four sessions are used for training, while the last one i s\nfor testing. Each utterance is labeled with one of six emotio ns:\nhappy, sad, neutral, angry, excited, and frustrated.\nMELD: This is a multi-speaker conversation dataset col-\nlected from the Friends TV series, containing 1, 433 conver-\nsations and 13, 708 utterances. Each utterance is labeled with\none of seven emotions: neutral, surprise, fear, sadness, jo y,\ndisgust, and anger.\nEvaluation Metrics: Following previous works [6], [12],\nwe report the overall accuracy and weighted average F1-scor e\nto measure overall performance, and also present the accura cy\nand F1-score on each emotion class.\nB. F eature Extraction\nW e extract utterance-level unimodal features as follows.\nT extual Modality: Following [55], we employ RoBER T a\nLarge model [56] to extract textual features. Roberta, a\npre-trained model using a multi-layer transformer encoder\narchitecture, builds on BER T which can efﬁciently learn\ntextual representations. W e ﬁne-tune RoBER T a for emotion\nrecognition from conversation transcripts and then take [\nCLS ]\ntokens’ embeddings at the last layer as textual features. Th e\ndimensionality of textual feature representation is 1024.\nAcoustic Modality: Following [13], we use openSMILE\n[57] for acoustic feature extraction. openSMILE, a ﬂexible\nfeature extraction toolkit for signal processing, provide s a\nscriptable console application to conﬁgure modular featur e\nextraction components. After using openSMILE toolkit, an F C\nlayer reduces the dimensionality of acoustic feature repre sen-\ntation to 1582 for IEMOCAP and 300 for MELD.\nVisual Modality: Following [13], we use DenseNet [58]\npre-trained on Facial Expression Recognition Plus dataset\nfor visual feature extraction. DenseNet, an effective CNN\narchitecture, consists of multiple dense blocks, each of wh ich\ncontains multiple layers. The output of DenseNet is set to 342;\nthat is, the dimensionality of visual feature representati on is\n342.\nC. Baselines\nW e compare SDT with the following baseline models.\nCMN [4]: It uses two GRUs and memory networks to\nmodel contextual information for both speakers, but it is on ly\navailable for dyadic conversations.\nICON [5]: It is an extension of CMN that captures inter-\nspeaker emotional inﬂuences using another GRU. Similar to\nCMN, the model is applied to dyadic conversations.\nDialogueRNN [6]: It adopts three distinct GRUs to track\nthe speaker, context, and emotional states in conversation s,\nrespectively.\nThe above models concatenate textual, acoustic, and visual\nfeatures to obtain multimodal utterance representations.\nMMGCN [13]: It constructs a conversation graph based on\nall three modalities and designs a multimodal fused graph co n-\nvolutional network to model contextual dependencies acros s\nmultiple modalities.\nDialogueTRM [10]: It uses a hierarchical transformer\nto manage the differentiated context preference within eac h\nmodality and designs a multi-grained interactive fusion fo r\nlearning different contributions across modalities for an utter-\nance.\nMM-DFN [17]: It designs a graph-based dynamic fusion\nmodule to fuse multimodal context features, and this module\ncould reduce redundancy and enhance complementarity be-\ntween modalities.\nMMT r [24]: It uses distinct bidirectional long short-term\nmemory networks (Bi-LSTMs) to learn contextual representa -\ntions at the speaker’s self-context level and contextual co ntext\nlevel, and designs a cross-modal fusion module to enhance\nweak modal representations.\nUniMSE [25]: It uses T5 to fuse acoustic and visual\nmodal features with multi-level textual features, and perf orms\ninter-modality contrastive learning to obtain discrimina tive\nmultimodal representations.\nFor a fair comparison, we re-run all baselines, except\nMMTr and UniMSE, whose source codes are not released 3.\n3 W e carefully implemented DialogueTRM to explore its perfor mance using\nour extracted features, since its source code is not availab le; MMTr uses\nbasically same feature extractors as us, and therefore we di d not implement\nit; UniMSE uses T5 to learn contextual information on textua l sequences and\nembeds multimodal fusion layers into T5, and hence our extra cted features\ncannot be used for UniMSE and we also did not implement it.\n7\nT ABLE II\nRE S U LT S O N T H E IE M O CA P DATA S E T; “*”: BA S E L I N E S A R E R E -I M P L E M E N T E D U S I N G O U R E X T R AC T E D F E AT U R E S ; B O L D F O N T D E N OT E S T H E B E S T\nP E R F O R M A N C E .\nModels\nIEMOCAP\nhappy sad neutral angry excited frustrated\nACC w-F1ACC F1 ACC F1 ACC F1 ACC F1 ACC F1 ACC F1\nCMN 24.31 30.30 56.33 62.02 52.34 52.41 61.76 60.17 56.19 60.76 72.44 61.27 56.87 56.33\nICON 25.00 31.30 67.35 73.17 55.99 58.50 69.41 66.29 70.90 67.09 7 1.92 65.08 62.85 62.25\nDialogueRNN 25.00 34.95 82.86 84.58 54.43 57.66 61.76 64.42 90.97 76.30 62.20 59.55 65.43 64.29\nMMGCN 32.64 39.66 72.65 76.89 65.10 62.81 73.53 71.43 77.93 75.40 65.09 63.43 66.61 66.25\nDialogueTRM 61.11 57.89 84.90 81.25 69.27 68.56 76.47 65.99 76.25 76.13 5 0.39 58.09 68.52 68.20\nMM-DFN 44.44 44.44 77.55 80.00 71.35 66.99 75.88 70.88 74.25 76.42 5 8.27 61.67 67.84 67.85\nMMTr - - - - - - - - - - - - 72.27 71.91\nUniMSE - - - - - - - - - - - - 70.56 70.66\nDialogueRNN* 57.64 57.64 77.96 80.25 75.52 70.56 68.24 64.99 73.91 75.95 5 9.06 62.41 69.38 69.37\nMMGCN* 50.00 56.25 78.78 81.43 71.35 67.57 68.24 66.29 75.92 76.82 6 5.09 64.92 69.62 69.61\nDialogueTRM* 72.22 62.84 85.71 83.33 69.27 68.12 79.41 66.67 67.22 75.00 57.22 63.28 69.87 69,93\nMM-DFN* 57.64 52.87 84.49 86.07 76.04 71.66 70.59 65.04 73.24 75.26 55.91 62.19 69.87 69.91\nSDT (Ours) 72.71 66.19 79.51 81.84 76.33 74.62 71.88 69.73 76.79 80.17 67.14 68.68 73.95 74.08\nw/o self-distillation 71.53 58.52 79.59 79.43 69.27 70.65 69.41 67.05 69.23 77.09 6 7.98 68.07 70.73 71.10\nIn addition, we re-implement DialogueRNN, MMGCN, Dia-\nglogueTRM, and MM-DFN with our extracted features,\nnamely DialogueRNN*, MMGCN*, DiaglogueTRM*, and\nMM-DFN*. W e use the same data splits to implement all\nmodels.\nD. Implementation Details\nW e implement the proposed model using Pytorch 4 and use\nAdam [59] as optimizer with an initial learning rate of 1. 0e− 4\nfor IEMOCAP and 5. 0e − 6 for MELD. The batch size is\n16 for IEMOCAP and 8 for MELD, and the temperature\nτ for the two datasets are set to 1 and 8, respectively. For\nthe 1D convolutional layers, the number of input channels\nare set to 1024, 1582, and 342 for textual, acoustic, and\nvisual modalities, respectively (i.e., their correspondi ng feature\ndimensions) on IEMOCAP . On MELD, these parameters are\nset to 1024, 300, and 342, respectively. In addition, the number\nof output channels and kernel size are set to 1024 and 1\nrespectively for all three modalities on the two datasets. F or\nthe transformer encoder, the hidden size, number of attenti on\nheads, feed-forward size, and number of layers are set to 1024,\n8, 1024, and 1, respectively. T o prevent overﬁtting, we set the\nL2 weight decay to 1. 0e − 5 and employ dropout with a rate\nof 0. 5. All results are averages of 10 runs.\nV . R E S U LT S A N D DIS CU S S IO N\nA. Overall Results\nT able II and T able III present the performance of baselines\nand SDT on IEMOCAP and MELD datasets, respectively. On\nIEMOCAP dataset, SDT performs better than all baselines and\noutperforms MMTr by 1. 68% and 2. 17% in terms of overall\naccuracy and weighted F1-score, respectively. In addition ,\nSDT achieves a signiﬁcant improvement on most emotion\nclasses in terms of F1-score. On MELD dataset, SDT achieves\nthe best performance compared to all baselines in terms\nof overall accuracy and weighted F1-score, and outperforms\n4 https://pytorch.org/\nUniMSE by 2. 46% and 1. 09%, respectively. Similar to IEMO-\nCAP , SDT performs superior on most emotion classes in terms\nof F1-score.\nOverall, the above results indicate the effectiveness of\nSDT . Furthermore, we have several similar ﬁndings on the\ntwo datasets: (1) DialogueTRM has a superior performance\ncompared to DialogueRNN, MMGCN, and MM-DFN that\nuse T extCNN [60] to extract textual features. This is becaus e\ntextual modality plays a more important role for ERC [13],\nand DialogueTRM extracts textual features using BER T [61],\nwhich is more powerful than T extCNN. (2) The baselines\ngain further improvement and achieve comparable results\nwhen using our extracted utterance features. The results sh ow\nthat our feature extractor is more effective and sequence-\nand graph-based baselines can achieve similar performance\nusing our extracted features. (3) Even without self-distil lation,\nour proposed model is still comparable to strong baselines,\ndemonstrating the power of the proposed transformer-based\nmodel.\nB. Ablation Study\nW e carry out ablation experiments on IEMOCAP and\nMELD. T able IV reports the results under different ablation\nsettings.\nAblation on T ransformer-based Model: Positional em-\nbeddings, speaker embeddings, intra-modal transformers, and\ninter-modal transformers are four crucial components of ou r\nproposed transformer-based model. W e remove only one com-\nponent at a time to evaluate the effectiveness of the compo-\nnent. From T able IV, we conclude that: (1) All components\nare useful because removing one of them leads to perfor-\nmance degradation. (2) Positional and speaker embeddings\nhave considerable effects on the two datasets, which means\ncapturing sequential and speaker information are valuable .\n(3) Inter-modal transformers are more important than intra -\nmodal transformers on the two datasets. This indicates that\ninter-modal interactions between conversation utterance s could\nprovide more helpful information.\n8\nT ABLE III\nRE S U LT S O N T H E M E L D DATA S E T.\n3\nModels\nMELD\nneutral surprise fear sadness joy disgust anger\nACC w-F1ACC F1 ACC F1 ACC F1 ACC F1 ACC F1 ACC F1 ACC F1\nDialogueRNN 82.17 76.56 46.62 47.64 0.00 0.00 21.15 24.65 49.50 51.49 0.0 0 0.00 48.41 46.01 60.27 57.95\nMMGCN 84.32 76.96 47.33 49.63 2.00 3.64 14.90 20.39 56.97 53.76 1.4 7 2.82 42.61 45.23 61.34 58.41\nDialogueTRM 83.20 79.41 56.94 55.27 12.00 17.39 27.88 36.48 60.45 60.30 1 6.18 20.18 51.01 49.79 65.10 63.80\nMM-DFN 79.06 75.80 53.02 50.42 0.00 0.00 17.79 23.72 59.20 55.48 0.0 0 0.00 50.43 48.27 60.96 58.72\nMMTr - - - - - - - - - - - - - - 64.64 64.41\nUniMSE - - - - - - - - - - - - - - 65.09 65.51\nDialogueRNN* 85.11 79.60 54.09 56.72 10.00 12.66 29.81 38.63 62.94 63.81 22.06 2 7.27 53.62 53.24 66.70 65.31\nMMGCN* 81.53 79.20 58.36 57.75 8.00 13.79 31.73 39.40 69.90 63.43 20.59 24.56 52.17 53.49 66.40 65.21\nDialogueTRM* 83.44 79.54 54.45 57.09 24.00 27.91 33.17 40.95 60.45 62.79 22.06 28.04 58.26 53.96 66.70 65.76\nMM-DFN* 83.52 79.65 63.35 58.17 32.00 26.67 26.44 35.71 63.68 64.89 19.12 24.76 49.28 52.15 66.55 65.48\nSDT (Ours) 83.22 80.19 61.28 59.07 13.80 17.88 34.90 43.69 63.24 64.29 22.65 28.78 56.93 54.33 67.55 66.60\nw/o self-distillation 82.01 80.00 57.65 57.96 20.00 23.81 32.21 41.61 65.17 64.22 25.00 27.42 57.97 54.05 66.97 66.26\nT ABLE IV\nRE S U LT S O F A B L AT I O N S T U D I E S O N T H E T WO DATA S E T S .\nIEMOCAP MELD\nACC w-F1 ACC w-F1\nSDT 73.95 74.08 67.55 66.60\nTransformer-based model\nw/o positional embeddings 72.27 72.39 66.86 66.20\nw/o speaker embeddings 71.84 72.03 67.13 66.18\nw/o intra-modal transformers 73.38 73.36 67.13 66.21\nw/o inter-modal transformers 72.09 72.26 66.97 65.55\nSelf-distillation\nw/o LCE 73.07 73.32 67.39 66.37\nw/o LKL 72.95 73.03 67.09 66.33\nModality\nT ext 66.42 66.58 66.82 65.52\nAudio 59.77 59.34 48.12 40.81\nV isual 41.47 42.71 48.05 32.01\nT ext + Audio 72.52 72.75 67.05 66.24\nT ext + V isual 69.01 69.07 67.20 66.18\nAudio + V isual 62.05 62.26 47.24 40.21\nAblation on Self-distillation Loss Functions: There are\ntwo kinds of losses ( i.e., LCE and LKL ) for self-distillation.\nT o verify the importance of these losses, we remove one loss a t\na time. T able IV shows that LCE and LKL are complementary\nand our model performs best when all losses are included. The\nresult demonstrates that transferring knowledge of both ha rd\nand soft labels from the proposed transformer-based model t o\neach modality can further boost the model performance.\nEffect of Different Modalities: T o show the effect of\ndifferent modalities, we remove one or two modalities at a\ntime. From T able IV, we observe that: (1) For unimodal\nresults, the textual modality has far better performance th an the\nother two modalities, indicating that the textual feature p lays\na leading role in ERC. This ﬁnding is consistent with previou s\nworks [10], [13], [17]. (2) Any bimodal results are better\nthan its own unimodal results. Moreover, fusing the textual\nmodality and acoustic or visual modality performs superior\nto the fusion of the acoustic and visual modalities due to the\nimportance of textual features. (3) Using all three modalit ies\ngives the best performance. The result can validate that emo -\ntion is affected by verbal, vocal, and visual expressions, a nd\nintegrating multimodal information is essential for ERC.\n/s54/s57/s46/s53/s54\n/s54/s55/s46/s49/s48\n/s55/s49/s46/s57/s55\n/s54/s53/s46/s54/s56\n/s55/s51/s46/s49/s54\n/s54/s55/s46/s49/s50\n/s55/s51/s46/s57/s53\n/s54/s55/s46/s53/s53\n/s73/s69/s77/s79/s67/s65/s80 /s77/s69/s76/s68\n/s54/s52/s46/s48/s48\n/s54/s53/s46/s48/s48\n/s54/s54/s46/s48/s48\n/s54/s55/s46/s48/s48\n/s54/s57/s46/s48/s48\n/s55/s50/s46/s48/s48\n/s55/s53/s46/s48/s48\n/s65/s67/s67/s32/s40/s37/s41\n/s68/s97/s116/s97/s115/s101/s116\n/s32/s85/s110/s105/s45/s67/s97/s116/s45/s84/s114/s97/s110/s115/s102/s111/s114/s109/s101/s114\n/s32/s65/s100/s100\n/s32/s67/s111/s110/s99/s97/s116/s101/s110/s97/s116/s105/s111/s110\n/s32/s79/s117/s114/s115\n(a) Overall accuracy\n/s54/s57/s46/s54/s52\n/s54/s53/s46/s57/s55\n/s55/s50/s46/s50/s48\n/s54/s53/s46/s51/s50\n/s55/s51/s46/s51/s48\n/s54/s54/s46/s49/s54\n/s55/s52/s46/s48/s56\n/s54/s54/s46/s54/s48\n/s73/s69/s77/s79/s67/s65/s80 /s77/s69/s76/s68\n/s54/s52/s46/s48/s48\n/s54/s53/s46/s48/s48\n/s54/s54/s46/s48/s48\n/s54/s57/s46/s48/s48\n/s55/s50/s46/s48/s48\n/s55/s53/s46/s48/s48\n/s119/s45/s70/s49/s32/s40/s37/s41\n/s68/s97/s116/s97/s115/s101/s116\n/s32/s85/s110/s105/s45/s67/s97/s116/s45/s84/s114/s97/s110/s115/s102/s111/s114/s109/s101/s114\n/s32/s65/s100/s100\n/s32/s67/s111/s110/s99/s97/s116/s101/s110/s97/s116/s105/s111/s110\n/s32/s79/s117/s114/s115\n(b) Weighted F1-score\nFig. 3. Performance of different fusion methods on the two da tasets. Bold\nfont means that the improvement to all baselines is statisti cally signiﬁcant\n(t-test with p <0.05).\nEffect of Different Fusion Strategies: T o investigate the\neffect of our proposed hierarchical gated fusion module, we\ncompare it with two typical information fusion strategies: (1)\nAdd: representations are fused via element-wise addition. (2)\nConcatenation: representations are directly concatenate d and\nfollowed by an FC layer. Add treats all representations equa lly,\nwhile Concatenation could implicitly choose the important\ninformation due to the FC layer. For a fair comparison, we\nreplace the hierarchical gated fusion module of our model wi th\nhierarchical add and concatenation operations to implemen t\nthe Add and Concatenation fusion strategies, respectively . In\naddition, we also compare SDT with a general transformer-\nbased fusion method (i.e., unimodal features are concatena ted\nand then fed into a transformer encoder) that we call Uni-Cat -\nTransformer.\nAs shown in Fig. 3, compared with other fusion strategies,\nour proposed hierarchical gated fusion strategy signiﬁcan tly\noutperforms them. The result indicates that directly fusin g\nrepresentations with Add and Concatenation is sub-optimal .\nOur proposed hierarchical gated fusion module ﬁrst ﬁlters\nout irrelevant information at the unimodal level and then\ndynamically learns weights between different modalities a t the\nmultimodal level, which can more effectively fuse multimod al\nrepresentations.\nIn addition, our model achieves a signiﬁcant performance\nimprovement over Uni-Cat-Transformer that demonstrates t he\neffectiveness of the proposed SDT in multimodal fusion.\n9\n/s48 /s53/s48 /s49/s48/s48 /s49/s53/s48 /s50/s48/s48\n/s48/s46/s48/s48\n/s48/s46/s53/s48\n/s49/s46/s48/s48\n/s49/s46/s53/s48\n/s50/s46/s48/s48\n/s76/s111/s115/s115\n/s69/s112/s111/s99/s104/s115\n/s32\n/s32\n/s32\n/s32\n(a) LCE & LT ask\n/s48 /s53/s48 /s49/s48/s48 /s49/s53/s48 /s50/s48/s48\n/s48/s46/s49/s48\n/s48/s46/s50/s48\n/s48/s46/s51/s48\n/s48/s46/s52/s48\n/s48/s46/s48/s48\n/s76/s111/s115/s115\n/s69/s112/s111/s99/s104/s115\n/s32\n/s32\n/s32\n(b) LKL\nFig. 4. Trends of all losses during training on the IEMOCAP da taset.\nInterestingly, Uni-Cat-Transformer has poorer performan ce\nthan Add and Concatenation on IEMOCAP; however, it shows\nan acceptable performance on MELD. This may be because\ninteractions between modalities are not as complex on MELD\nas on IEMOCAP , and hence modeling modal interactions\nby multiple transformer encoders could generate some noise\non MELD that makes Ui-Cat-Transformer gain comparable\nperformance with Add and Concatenation. In contrast, SDT\nhas superior performance than all baselines as it contains a\nhierarchical gated fusion module to ﬁlter out noise informa -\ntion, which further illustrates the usefulness of our propo sed\nhierarchical gated fusion strategy.\nC. T rends of Losses\nDuring training, we illustrate the trends of all types of los ses\non IEMOCAP dataset to better understand how these losses\nwork, and Fig. 4 displays the results.\nFrom Fig. 4(a), we ﬁnd that LT ask, Lt\nCE , La\nCE , and Lv\nCE\nkeep descending in the whole training process. From Fig. 4(b ),\nwe can see Lt\nKL and La\nKL also have decreasing trends except\nfor ﬂuctuations at the beginning, and Lv\nKL goes down during\nearly training except for the ﬂuctuation and then goes up and\nachieves stability. Therefore, all of the losses can conver ge.\nThese show that all students can learn knowledge from hard\nand soft labels to improve the model performance. Besides, w e\nﬁnd that losses of student v (i.e., Lv\nCE and Lv\nKL ) are larger\nthan the other two students. This may due to a unsuitable\nlearning rate for the student v. Hence, we would like to\nadaptively modify learning rates between different modali ties\nto effectively optimize the proposed model in the future.\nD. Multimodal Representation V isualization\nW e extract multimodal representations for each utterance o n\nIEMOCAP from our proposed transformer-based model with-\nout and with self-distillation. Besides, pre-extracted un imodal\nrepresentations are concatenated to produce original mult i-\nmodal representations. Then, these multimodal representa tions\nare projected into two dimensions via the t-SNE algorithm\n[62].\nFig. 5 illustrates the visualization results with differen t emo-\ntion categories. Compared with original multimodal repres en-\ntations, representations learned by the proposed transfor mer-\nbased model become more clustered even without self-\ndistillation. However without self-distillation, multim odal rep-\nresentations of similar emotions (i.e., “happy” and “excit ed”,\n“angry” and “frustrated”) are difﬁcult to separate; furthe rmore,\nrepresentations of “neutral” emotion are intermingled wit h\nother emotions. By comparing Fig. 5(b) and Fig. 5(c), we\nobserve that our model with self-distillation yields a bett er\nseparation and representations of different emotions are l ess\nmixed together. Therefore, introducing self-distillatio n training\ncould learn more effective multimodal representations.\nOn the other hand, we also show the visualization results\nwith different genders of speakers in Fig. 6. Fig. 6(b) and\nFig. 6(c) form two large clusters respectively correspondi ng\nto the gender of the speaker. This interesting ﬁnding indica tes\nthat with or without self-distillation, our model can disti nguish\nthe gender of the speaker, which may also be helpful for ERC.\nE. Case Study\nT o demonstrate the efﬁcacy of SDT , we present a case\nstudy. Fig. 7 shows a conversation that comes from MELD.\nSDT identiﬁes the emotions of all utterances successfully,\nwhile DialogueRNN* and MMGCN* predict the 3rd utterance\nas “surprise” incorrectly, probably because a question mar k\n“?” generally expresses “surprise”. This could indicate th e\nmore powerful multimodal fusion capability of our proposed\nmodel. On the other hand, using only the textual modality, ou r\nmodel recognizes the 4th utterance as “neutral” wrongly. T o\nexplore the reason behind it, we visualize multi-head atten tion\nweights of SDT (only text) and SDT for the 4th utterance,\nrespectively. For SDT , we ﬁnd that the weights of textual\nfeatures are obviously larger than acoustic and visual feat ures\nfor the 4th utterance by outputting their weights. Therefore,\nwe visualize only attention weights of the transformers tha t\nform enhanced textual modality representation in Fig. 8, an d\nother visualization results can be found in the appendix.\nAs can be seen from Fig. 8(a), the 4th utterance depends\nheavily on the 3rd and 5th utterances when using only the\ntextual modality. The 3rd utterance, which expresses “neutral”\nemotion, may be more important due to a larger number of\ndarkest attention heads; hence the 4th utterance is identiﬁed\nas the same emotion as the 3rd utterance, i.e., “neutral”.\nThe utterance can be correctly recognized as “disgust” by\nSDT for the following reasons: (1) According to Fig. 8(b),\nthe text of the 4th utterance is inﬂuenced the most by the\ntext of the 5th utterance whose emotion is “disgust”. (2)\nFrom Fig. 8(c) and Fig. 8(d), we observe that the acoustic\nand visual expressions of the 2nd, 4th and 5th utterances\nare more valuable for the 4th utterance’s textual expression,\nand the 4th and 5th utterances express “disgust” emotion.\nOverall, the results show that interactions between modali ties\nare helpful in identifying emotion from different perspect ives,\nand therefore it is necessary to use multimodal information .\nIn addition, comparing Fig. 8(a) and Fig. 8(b), SDT learns\nbetter to correlate the 4th utterance with the 5th utterance. The\nﬁnding illustrates that introducing self-distillation ca n learn\nmore appropriate attention weights.\n10\n(a) Origin representations (b) Without self-distillation (c) With self-distillation\nFig. 5. t-SNE visualization of the multimodal representati ons with different emotion categories on the IEMOCAP datase t.\n(a) Origin representations (b) Without self-distillation (c) With self-distillation\nFig. 6. t-SNE visualization of the multimodal representati ons with different genders of speakers on the IEMOCAP datase t.\nTurn Speaker Visual Audio Text Dialogue \nRNN* \nMM \nGCN* \nSDT \n(only text) SDT Ground \nTruth \n1 Joey Oh my god, \nyou’re back! surprise surprise surprise surprise surprise \n2 Phoebe \nOhh, let me see \nit! Let me see \nyour hand! \nsurprise surprise surprise surprise surprise \n3 Monica \nWhy do you \nwant to see my \nhand? \nsurprise surprise neutral neutral neutral \n4 Phoebe \nI wanna see \nwhat’s in your \nhand. I wanna \nsee the trash. \ndisgust disgust \nneutral disgust disgust \n5 Phoebe \nEww! Oh, it’s all \ndirty. You should \nthrow this out. \ndisgust disgust disgust disgust disgust \nFig. 7. An example of emotion recognition results in a conver sation from the\nMELD dataset.\nF . Error Analysis\nAlthough the proposed SDT achieves strong performance,\nit still fails to detect some emotions. W e analyze confusion\nmatrices of the test set on the two datasets. From Fig. 9, we\nsee that: (1) SDT misclassiﬁes similar emotions, like “happ y”\nand “excited”, “angry” and “frustrated” on IEMOCAP , and\n“surprise” and “anger” on MELD. (2) SDT also tends to\nmisclassify other emotions as “neutral” on MELD due to that\n“neutral” is the majority class. (3) It is difﬁcult to correc tly\ndetect “fear” and “disgust” emotions on MELD because the\ntwo emotions are minority classes. Thus, it is challenging\nto recognize similar emotions and emotions with unbalanced\ndata.\nBesides, we also investigate SDT performance on emotional\nshift (i.e., two consecutive utterances spoken by the same\nspeaker have different emotions). As shown in T able V,\nwe observe that SDT performs poorer on utterances with\n\u0018\n\u0019\n3H`LY! \u0017\n\u0018\n\u0019\n\u001a\n\u001b\n\u001c\n\u0018\n\u0019\n\u001a\n\u001b\n\u001c\n(a) Intra-modal transformer\n(t → t) of SDT (only text)\n\u0018\n\u0019\n3H`LY! \u0017\n\u0018\n\u0019\n\u001a\n\u001b\n\u001c\n\u0018\n\u0019\n\u001a\n\u001b\n\u001c\n(b) Intra-modal transformer\n(t → t) of SDT\n\u0018\n\u0019\n3H`LY! \u0017\n\u0018\n\u0019\n\u001a\n\u001b\n\u001c\n\u0018\n\u0019\n\u001a\n\u001b\n\u001c\n(c) Inter-modal transformer\n(a → t) of SDT\n\u0018\n\u0019\n3H`LY! \u0017\n\u0018\n\u0019\n\u001a\n\u001b\n\u001c\n\u0018\n\u0019\n\u001a\n\u001b\n\u001c\n(d) Inter-modal transformer\n(v → t) of SDT\nFig. 8. Multi-head attention visualization for the 4th utterance in Fig. 7.\nThere are 8 attention heads and different colors represent d ifferent heads. The\ndarker the color, the more important for the 4th utterance.\nT ABLE V\nT E S T AC C U R AC Y O F S D T O N U T T E R A N C E S W I T H A N D W I T H O U T\nE M OT I O NA L S H I F T\nDataset Emotional Shift w/o Emotional Shift\n#Utterances ACC #Utterances ACC\nIEMOCAP 410 54.88 1151 80.71\nMELD 1003 61.62 861 73.05\n11\n(a) IEMOCAP\n (b) MELD\nFig. 9. The confusion matrices of the test set on the two datas ets. The rows\nand columns represent true and predicted labels, respectiv ely .\nemotional shift than that without it 5 , which is consistent with\nprevious works. The emotional shift in conversations is a\ncomplex phenomenon caused by multiple latent variables, e. g.,\nthe speaker’s personality and intent; however, SDT and most\nexisting models do not consider these factors, which may res ult\nin poor performance. Further improvement on the case needs\nto be explored.\nVI. C O N CL U S IO N\nIn this paper, we propose SDT , a transformer-based model\nwith self-distillation for multimodal ERC. W e use intra- an d\ninter-modal transformers to model intra- and inter-modal i nter-\nactions between conversation utterances. T o dynamically l earn\nweights between different modalities, we design a hierarch ical\ngated fusion strategy. Positional and speaker embeddings a re\nalso leveraged as additional inputs to capture contextual a nd\nspeaker information. In addition, we devise self-distilla tion\nduring training to transfer knowledge of hard and soft label s\nwithin the model to learn better modal representations, whi ch\ncould further improve performance. W e conduct experiments\non two benchmark datasets and the results demonstrate the\neffectiveness and superiority of SDT .\nThrough error analysis, we ﬁnd that distinguishing simi-\nlar emotions, detecting emotions with unbalanced data, and\nemotional shift are key challenges for ERC that are worth\nfurther exploration in future work. Furthermore, transfor mer-\nbased fusion methods cause high computational costs as the\nself-attention mechanism of transformer has a complexity o f\nO\n(\nN2)\nwith respect to sequence length N. T o alleviate the\nissue, Ding et al. [63] proposed sparse fusion for multimoda l\ntransformers. Similarly, we plan to design a novel multimod al\nfusion method for transformers to reduce computational cos ts\nin the future.\nACK N OW L E D G M E N T S\nThis work is partially supported by the Natural Science\nFoundation of China (No. 62006034), the Natural Science\n5 In this paper, without emotional shift means two consecutiv e utterances\nspoken by the same speaker have same emotions.\n\u0018\n\u0019\n3H`LY! \u0017\n\u0018\n\u0019\n\u001a\n\u001b\n\u001c\n\u0018\n\u0019\n\u001a\n\u001b\n\u001c\n(a) Intra-modal transformer\n(a → a) of SDT\n\u0018\n\u0019\n3H`LY! \u0017\n\u0018\n\u0019\n\u001a\n\u001b\n\u001c\n\u0018\n\u0019\n\u001a\n\u001b\n\u001c\n(b) Inter-modal transformer\n(t → a) of SDT\n\u0018\n\u0019\n3H`LY! \u0017\n\u0018\n\u0019\n\u001a\n\u001b\n\u001c\n\u0018\n\u0019\n\u001a\n\u001b\n\u001c\n(c) Inter-modal transformer\n(v → a) of SDT\n\u0018\n\u0019\n3H`LY! \u0017\n\u0018\n\u0019\n\u001a\n\u001b\n\u001c\n\u0018\n\u0019\n\u001a\n\u001b\n\u001c\n(d) Intra-modal transformer\n(v → v) of SDT\n\u0018\n\u0019\n3H`LY! \u0017\n\u0018\n\u0019\n\u001a\n\u001b\n\u001c\n\u0018\n\u0019\n\u001a\n\u001b\n\u001c\n(e) Inter-modal transformer\n(t → v) of SDT\n\u0018\n\u0019\n3H`LY! \u0017\n\u0018\n\u0019\n\u001a\n\u001b\n\u001c\n\u0018\n\u0019\n\u001a\n\u001b\n\u001c\n(f) Inter-modal transformer\n(a → v) of SDT\nFig. 10. Multi-head attention visualization for the 4th utterance in Fig. 7.\nFoundation of Liaoning Province (No. 2021-BS-067), and the\nFundamental Research Funds for the Central Universities (N o.\nDUT21RC(3)015).\nAP P E N D IX\nAT T E N T IO N VIS UA L IZ AT IO N\nMulti-head attention weights of the transformers in our SDT\nthat form enhanced acoustic and visual modality representa -\ntions are visualized in Fig. 10.\nRE F E RE N CE S\n[1] A. Kumar, P . Dogra, and V . Dabas, “Emotion analysis of twi tter\nusing opinion mining, ” in 2015 Eighth International Conference on\nContemporary Computing , 2015, pp. 285–290.\n[2] F . A. Pujol, H. Mora, and A. Mart´ ınez, “Emotion recognit ion to improve\ne-healthcare systems in smart cities, ” in Research & Innovation F orum\n2019, A. V isvizi and M. D. Lytras, Eds., 2019, pp. 245–254.\n[3] L. Zhou, J. Gao, D. Li, and H.-Y . Shum, “The design and impl ementation\nof xiaoice, an empathetic social chatbot, ” Computational Linguistics ,\nvol. 46, no. 1, pp. 53–93, 2020.\n[4] D. Hazarika, S. Poria, A. Zadeh, E. Cambria, L.-P . Morenc y , and R. Zim-\nmermann, “Conversational memory network for emotion recog nition\nin dyadic dialogue videos, ” in Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Computa tional\nLinguistics: Human Language T echnologies, V olume 1 (Long P apers),\n2018, pp. 2122–2132.\n[5] D. Hazarika, S. Poria, R. Mihalcea, E. Cambria, and R. Zim mermann,\n“ICON: Interactive conversational memory network for mult imodal\nemotion detection, ” in Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing , 2018, pp. 2594–2604.\n12\n[6] N. Majumder, S. Poria, D. Hazarika, R. Mihalcea, A. Gelbu kh, and\nE. Cambria, “Dialoguernn: An attentive rnn for emotion dete ction in\nconversations, ” in Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, vol. 33, no. 01, 2019, pp. 6818–6825.\n[7] W . Jiao, H. Y ang, I. King, and M. R. Lyu, “HiGRU: Hierarchi cal gated\nrecurrent units for utterance-level emotion recognition, ” in Proceedings\nof the 2019 Conference of the North American Chapter of the As so-\nciation for Computational Linguistics: Human Language T ec hnologies,\nV olume 1 (Long and Short P apers) , 2019, pp. 397–406.\n[8] J. Li, D. Ji, F . Li, M. Zhang, and Y . Liu, “HiTrans: A transf ormer-\nbased context- and speaker-sensitive model for emotion det ection in\nconversations, ” in Proceedings of the 28th International Conference on\nComputational Linguistics , 2020, pp. 4190–4200.\n[9] H. Ma, J. W ang, L. Qian, and H. Lin, “Han-regru: hierarchi cal attention\nnetwork with residual gated recurrent unit for emotion reco gnition in\nconversation, ” Neural Computing and Applications , vol. 33, no. 7, pp.\n2685–2703, 2021.\n[10] Y . Mao, G. Liu, X. W ang, W . Gao, and X. Li, “DialogueTRM: E xploring\nmulti-modal emotional dynamics in a conversation, ” in Findings of the\nAssociation for Computational Linguistics: EMNLP 2021 , 2021, pp.\n2694–2704.\n[11] H. Ma, J. W ang, H. Lin, X. Pan, Y . Zhang, and Z. Y ang, “ A mul ti-view\nnetwork for real-time emotion recognition in conversation s, ” Knowledge-\nBased Systems , vol. 236, p. 107751, 2022.\n[12] D. Ghosal, N. Majumder, S. Poria, N. Chhaya, and A. Gelbu kh,\n“DialogueGCN: A graph convolutional neural network for emo tion\nrecognition in conversation, ” in Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Proces sing, 2019,\npp. 154–164.\n[13] J. Hu, Y . Liu, J. Zhao, and Q. Jin, “MMGCN: Multimodal fus ion via\ndeep graph convolution network for emotion recognition in c onversa-\ntion, ” in Proceedings of the 59th Annual Meeting of the Association fo r\nComputational Linguistics and the 11th International Join t Conference\non Natural Language Processing (V olume 1: Long P apers) , 2021, pp.\n5666–5675.\n[14] W . Nie, R. Chang, M. Ren, Y . Su, and A. Liu, “I-gcn: Increm ental\ngraph convolution network for conversation emotion detect ion, ” IEEE\nTransactions on Multimedia , pp. 1–1, 2021.\n[15] M. Ren, X. Huang, W . Li, D. Song, and W . Nie, “Lr-gcn: Late nt\nrelation-aware graph convolutional network for conversat ional emotion\nrecognition, ” IEEE Transactions on Multimedia , pp. 1–1, 2021.\n[16] A. Mehrabian et al. , Silent messages . W adsworth Belmont, CA, 1971.\n[17] D. Hu, X. Hou, L. W ei, L. Jiang, and Y . Mo, “Mm-dfn: Multim odal\ndynamic fusion network for emotion recognition in conversa tions, ” in\n2022 IEEE International Conference on Acoustics, Speech an d Signal\nProcessing, 2022.\n[18] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S . Kim, J. N.\nChang, S. Lee, and S. S. Narayanan, “Iemocap: Interactive em otional\ndyadic motion capture database, ” Language Resources and Evaluation ,\nvol. 42, no. 4, pp. 335–359, 2008.\n[19] B. Schuller, M. V alstar, R. Cowie, and M. Pantic, “ A vec 2 012: The\ncontinuous audio/visual emotion challenge - an introducti on, ” in Pro-\nceedings of the 14th ACM International Conference on Multim odal\nInteraction, 2012, p. 361–362.\n[20] S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and R. Mihal-\ncea, “MELD: A multimodal multi-party dataset for emotion re cognition\nin conversations, ” in Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , 2019, pp. 527–536.\n[21] C. M. Lee and S. Narayanan, “T oward detecting emotions i n spoken\ndialogs, ” IEEE Transactions on Speech and Audio Processing , vol. 13,\nno. 2, pp. 293–303, 2005.\n[22] L. Devillers and L. V idrascu, “Real-life emotions dete ction with lexical\nand paralinguistic cues on human-human call center dialogs , ” in Ninth\nInternational Conference on Spoken Language Processing , 2006.\n[23] W . Jiao, M. Lyu, and I. King, “Real-time emotion recogni tion via\nattention gated hierarchical memory network, ” in Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence , vol. 34, no. 05, 2020, pp.\n8002–8009.\n[24] S. Zou, X. Huang, X. Shen, and H. Liu, “Improving multimo dal fusion\nwith main modal transformer for emotion recognition in conv ersation, ”\nKnowledge-Based Systems , vol. 258, p. 109978, 2022.\n[25] G. Hu, T .-E. Lin, Y . Zhao, G. Lu, Y . Wu, and Y . Li, “Unimse: T owards\nuniﬁed multimodal sentiment analysis and emotion recognit ion, ” in\nProceedings of the 2022 Conference on Empirical Methods in N atural\nLanguage Processing , 2022.\n[26] P . P . Liang, Z. Liu, A. Bagher Zadeh, and L.-P . Morency , “ Multimodal\nlanguage analysis with recurrent multistage fusion, ” in Proceedings\nof the 2018 Conference on Empirical Methods in Natural Langu age\nProcessing, 2018, pp. 150–161.\n[27] M. W ¨ ollmer, F . W eninger, T . Knaup, B. Schuller, C. Sun, K. Sagae,\nand L.-P . Morency , “Y outube movie reviews: Sentiment analy sis in an\naudio-visual context, ” IEEE Intelligent Systems , vol. 28, no. 3, pp. 46–\n53, 2013.\n[28] S. Poria, I. Chaturvedi, E. Cambria, and A. Hussain, “Co nvolutional\nmkl based multimodal emotion recognition and sentiment ana lysis, ” in\n2016 IEEE 16th International Conference on Data Mining , 2016, pp.\n439–448.\n[29] B. Nojavanasghari, D. Gopinath, J. Koushik, T . Baltruˇ saitis, and L.-P .\nMorency , “Deep multimodal fusion for persuasiveness predi ction, ” in\nProceedings of the 18th ACM International Conference on Mul timodal\nInteraction, 2016, p. 284–288.\n[30] O. Kampman, E. J. Barezi, D. Bertero, and P . Fung, “Inves tigating audio,\nvideo, and text fusion methods for end-to-end automatic per sonality pre-\ndiction, ” in Proceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (V olume 2: Short P apers) , 2018, pp. 606–\n611.\n[31] Z. Liu, Y . Shen, V . B. Lakshminarasimhan, P . P . Liang, A. Bagher Zadeh,\nand L.-P . Morency , “Efﬁcient low-rank multimodal fusion wi th modality-\nspeciﬁc factors, ” in Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (V olume 1: Long P apers),\n2018, pp. 2247–2256.\n[32] A. Zadeh, M. Chen, S. Poria, E. Cambria, and L.-P . Morenc y , “T ensor\nfusion network for multimodal sentiment analysis, ” in Proceedings\nof the 2017 Conference on Empirical Methods in Natural Langu age\nProcessing, 2017, pp. 1103–1114.\n[33] A. Zadeh, P . P . Liang, N. Mazumder, S. Poria, E. Cambria, and L.-P .\nMorency , “Memory fusion network for multi-view sequential learning, ”\nin Proceedings of the AAAI Conference on Artiﬁcial Intelligen ce, vol. 32,\nno. 1, 2018.\n[34] Y .-H. H. Tsai, S. Bai, P . P . Liang, J. Z. Kolter, L.-P . Mor ency , and\nR. Salakhutdinov , “Multimodal transformer for unaligned m ultimodal\nlanguage sequences, ” in Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , 2019, pp. 6558–6569.\n[35] W . Rahman, M. K. Hasan, S. Lee, A. Bagher Zadeh, C. Mao, L. -P .\nMorency , and E. Hoque, “Integrating multimodal informatio n in large\npretrained transformers, ” in Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics , 2020, pp. 2359–2369.\n[36] W . Y u, H. Xu, Z. Y uan, and J. Wu, “Learning modality-spec iﬁc\nrepresentations with self-supervised multi-task learnin g for multimodal\nsentiment analysis, ” in Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, vol. 35, no. 12, 2021, pp. 10 790–10 797.\n[37] Z. Y uan, W . Li, H. Xu, and W . Y u, “Transformer-based feat ure re-\nconstruction network for robust multimodal sentiment anal ysis, ” in\nProceedings of the 29th ACM International Conference on Mul timedia,\n2021, p. 4400–4407.\n[38] G. Hinton, O. V inyals, and J. Dean, “Distilling the know ledge in a neural\nnetwork, ” arXiv preprint arXiv:1503.02531 , 2015.\n[39] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y . Ben-\ngio, “Fitnets: Hints for thin deep nets, ” in International Conference on\nLearning Representations , 2015.\n[40] J. Y im, D. Joo, J. Bae, and J. Kim, “ A gift from knowledge d istillation:\nFast optimization, network minimization and transfer lear ning, ” in In\nProceedings of CVPR , 2017.\n[41] N. Passalis and A. T efas, “Learning deep representatio ns with probabilis-\ntic knowledge transfer, ” in Proceedings of the European Conference on\nComputer V ision , 2018, pp. 268–284.\n[42] T . Li, J. Li, Z. Liu, and C. Zhang, “Few sample knowledge d istillation\nfor efﬁcient network compression, ” in Proceedings of the IEEE/CVF\nConference on Computer V ision and P attern Recognition , 2020, pp.\n14 639–14 647.\n[43] Y . Zhang, T . Xiang, T . M. Hospedales, and H. Lu, “Deep mut ual\nlearning, ” in Proceedings of the IEEE conference on computer vision\nand pattern recognition , 2018, pp. 4320–4328.\n[44] I. Chung, S. Park, J. Kim, and N. Kwak, “Feature-map-lev el online\nadversarial knowledge distillation, ” in International Conference on Ma-\nchine Learning , 2020, pp. 2006–2015.\n[45] L. Zhang, J. Song, A. Gao, J. Chen, C. Bao, and K. Ma, “Be yo ur\nown teacher: Improve the performance of convolutional neur al networks\nvia self distillation, ” in Proceedings of the IEEE/CVF International\nConference on Computer V ision , 2019.\n[46] Y . Hou, Z. Ma, C. Liu, and C. C. Loy , “Learning lightweigh t lane\ndetection cnns by self attention distillation, ” in Proceedings of the\n13\nIEEE/CVF international conference on computer vision , 2019, pp. 1013–\n1021.\n[47] S. Albanie, A. Nagrani, A. V edaldi, and A. Zisserman, “E motion recog-\nnition in speech using cross-modal transfer in the wild, ” in Proceedings\nof the 26th ACM international conference on Multimedia , 2018, pp.\n292–301.\n[48] Y . W ang, J. Wu, P . Heracleous, S. W ada, R. Kimura, and S. K urihara,\n“Implicit knowledge injectable cross attention audiovisu al model for\ngroup emotion recognition, ” in Proceedings of the 2020 International\nConference on Multimodal Interaction , 2020, pp. 827–834.\n[49] L. Schoneveld, A. Othmani, and H. Abdelkawy , “Leveragi ng recent\nadvances in deep learning for audio-visual emotion recogni tion, ” P attern\nRecognition Letters , vol. 146, pp. 1–7, 2021.\n[50] T . Moriya, T . Ochiai, S. Karita, H. Sato, T . T anaka, T . As hihara,\nR. Masumura, Y . Shinohara, and M. Delcroix, “Self-distilla tion for\nimproving ctc-transformer-based asr systems. ” in INTERSPEECH, 2020,\npp. 546–550.\n[51] T . Zhou, P . Cao, Y . Chen, K. Liu, J. Zhao, K. Niu, W . Chong, and S. Liu,\n“ Automatic icd coding via interactive shared representati on networks\nwith self-distillation mechanism, ” in Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguistics a nd the 11th\nInternational Joint Conference on Natural Language Proces sing (V olume\n1: Long P apers) , 2021, pp. 5948–5957.\n[52] X. Luo, Q. Liang, D. Liu, and Y . Qu, “Boosting lightweigh t single image\nsuper-resolution via joint-distillation, ” in Proceedings of the 29th ACM\nInternational Conference on Multimedia , 2021, p. 1535–1543.\n[53] L. Y uan, F . E. T ay , G. Li, T . W ang, and J. Feng, “Revisitin g knowledge\ndistillation via label smoothing regularization, ” in Proceedings of the\nIEEE/CVF Conference on Computer V ision and P attern Recogni tion,\n2020, pp. 3903–3911.\n[54] A. V aswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jone s, A. N.\nGomez, L. u. Kaiser, and I. Polosukhin, “ Attention is all you need, ”\nin Proceedings of NIPS , vol. 30, 2017.\n[55] D. Ghosal, N. Majumder, A. Gelbukh, R. Mihalcea, and S. P oria,\n“COSMIC: COmmonSense knowledge for eMotion identiﬁcation in\nconversations, ” in Findings of the Association for Computational Lin-\nguistics: EMNLP 2020 , 2020, pp. 2470–2481.\n[56] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy , M. Lewis,\nL. Zettlemoyer, and V . Stoyanov , “Roberta: A robustly optim ized bert\npretraining approach, ” arXiv preprint arXiv:1907.11692 , 2019.\n[57] F . Eyben, F . W eninger, F . Gross, and B. Schuller, “Recen t developments\nin opensmile, the munich open-source multimedia feature ex tractor, ” in\nProceedings of the 21st ACM international conference on Mul timedia,\n2013, pp. 835–838.\n[58] G. Huang, Z. Liu, L. V an Der Maaten, and K. Q. W einberger, “Densely\nconnected convolutional networks, ” in Proceedings of the IEEE confer-\nence on computer vision and pattern recognition , 2017, pp. 4700–4708.\n[59] D. P . Kingma and J. Ba, “ Adam: A method for stochastic opt imization, ”\nin Proceedings of ICLR , 2015.\n[60] Y . Kim, “Convolutional neural networks for sentence cl assiﬁcation, ” in\nProceedings of the 2014 Conference on Empirical Methods in N atural\nLanguage Processing , 2014, pp. 1746–1751.\n[61] J. Devlin, M.-W . Chang, K. Lee, and K. T outanova, “BER T: Pre-\ntraining of deep bidirectional transformers for language u nderstanding, ”\nin Proceedings of the 2019 Conference of the North American Cha pter\nof the Association for Computational Linguistics: Human La nguage\nT echnologies, V olume 1 (Long and Short P apers) , 2019, pp. 4171–4186.\n[62] L. V an der Maaten and G. Hinton, “V isualizing data using t-sne. ” Journal\nof machine learning research , vol. 9, no. 11, 2008.\n[63] Y . Ding, A. Rich, M. W ang, N. Stier, P . Sen, M. Turk, and\nT . H ¨ ollerer, “Sparse fusion for multimodal transformers, ” arXiv preprint\narXiv:2111.11992, 2021.\nHui Ma received the M.S. degree from Dalian\nUniversity of T echnology , China, in 2019. She is\ncurrently working toward the Ph.D. degree at School\nof Computer Science and T echnology , Dalian Uni-\nversity of T echnology . Her research interests include\nnatural language processing, dialogue system, and\nsentiment analysis.\nJian W ang received the Ph.D. degree from Dalian\nUniversity of T echnology , China, in 2014. She is\ncurrently a professor at School of Computer Science\nand T echnology , Dalian University of T echnology .\nHer research interests include natural language pro-\ncessing, text mining, and information retrieval.\nHongfei Lin received the Ph.D. degree from North-\neastern University , China, in 2000. He is currently\na professor at School of Computer Science and\nT echnology , Dalian University of T echnology . His\nresearch interests include natural language process-\ning, text mining, and sentimental analysis.\nBo Zhang received the B.S. degree from Tiangong\nUniversity , China, in 2019. He is currently working\ntoward the Ph.D. degree at School of Computer Sci-\nence and T echnology , Dalian University of T echnol-\nogy . His research interests include natural language\nprocessing, dialogue system, and text generation.\nYijia Zhang received the Ph.D. degree from the\nDalian University of T echnology , China, in 2014.\nHe is currently a professor at School of Information\nScience and T echnology , Dalian Maritime Univer-\nsity . His research interests include natural language\nprocessing, bioinformatics, and text mining.\nBo Xu received the Ph.D. degree from the Dalian\nUniversity of T echnology , China, in 2018. He is\ncurrently an associate professor at School of Com-\nputer Science and T echnology , Dalian University of\nT echnology . His research interests include informa-\ntion retrieval, dialogue system, and natural language\nprocessing.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7897180914878845
    },
    {
      "name": "Transformer",
      "score": 0.6909758448600769
    },
    {
      "name": "Emotion recognition",
      "score": 0.6259252429008484
    },
    {
      "name": "Speech recognition",
      "score": 0.5557248592376709
    },
    {
      "name": "Distillation",
      "score": 0.4920894503593445
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4358363449573517
    },
    {
      "name": "Hidden Markov model",
      "score": 0.42686235904693604
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.34697550535202026
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3324839472770691
    },
    {
      "name": "Voltage",
      "score": 0.09288862347602844
    },
    {
      "name": "Engineering",
      "score": 0.0821598470211029
    },
    {
      "name": "Electrical engineering",
      "score": 0.08010983467102051
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    }
  ]
}