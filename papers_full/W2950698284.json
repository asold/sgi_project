{
  "title": "Bayesian Grammar Induction for Language Modeling",
  "url": "https://openalex.org/W2950698284",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Chen, Stanley F.",
      "affiliations": [
        "Harvard University Press"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1972099155",
    "https://openalex.org/W1638203394",
    "https://openalex.org/W1982944197",
    "https://openalex.org/W2049633694",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W1978470410",
    "https://openalex.org/W26104448",
    "https://openalex.org/W103303497",
    "https://openalex.org/W2077574412",
    "https://openalex.org/W2047706513",
    "https://openalex.org/W2079145130",
    "https://openalex.org/W1597533204",
    "https://openalex.org/W2163929346",
    "https://openalex.org/W1966812932",
    "https://openalex.org/W2135625884",
    "https://openalex.org/W2061079066"
  ],
  "abstract": "We describe a corpus-based induction algorithm for probabilistic context-free grammars. The algorithm employs a greedy heuristic search within a Bayesian framework, and a post-pass using the Inside-Outside algorithm. We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks. In two of the tasks, the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques. The third task involves naturally-occurring data, and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm.",
  "full_text": "arXiv:cmp-lg/9504034v1  1 May 1995\nBayesian Grammar Induction for Language Modeling\nStanley F. Chen\nAiken Computation Laboratory\nDivision of Applied Sciences\nHarvard University\nCambridge, MA 02138\nsfc@das.harvard.edu\nAbstract\nWe describe a corpus-based induction algo-\nrithm for probabilistic context-free gram-\nmars. The algorithm employs a greedy\nheuristic search within a Bayesian frame-\nwork, and a post-pass using the Inside-\nOutside algorithm. We compare the per-\nformance of our algorithm to n-gram mod-\nels and the Inside-Outside algorithm in\nthree language modeling tasks. In two of\nthe tasks, the training data is generated by\na probabilistic context-free grammar and in\nboth tasks our algorithm outperforms the\nother techniques. The third task involves\nnaturally-occurring data, and in this task\nour algorithm does not perform as well as\nn-gram models but vastly outperforms the\nInside-Outside algorithm.\n1 Introduction\nIn applications such as speech recognition, handwrit-\ning recognition, and spelling correction, performance\nis limited by the quality of the language model uti-\nlized (Bahl et al., 1978; Baker, 1975; Kernighan et\nal., 1990; Srihari and Baltus, 1992). However, static\nlanguage modeling performance has remained ba-\nsically unchanged since the advent of n-gram lan-\nguage models forty years ago (Shannon, 1951). Yet,\nn-gram language models can only capture depen-\ndencies within an n-word window, where currently\nthe largest practical n for natural language is three,\nand many dependencies in natural language occur\nbeyond a three-word window. In addition, n-gram\nmodels are extremely large, thus making them diﬃ-\ncult to implement eﬃciently in memory-constrained\napplications.\nAn appealing alternative is grammar-based lan-\nguage models. Language models expressed as a\nprobabilistic grammar tend to be more compact\nthan n-gram language models, and have the abil-\nity to model long-distance dependencies (Lari and\nYoung, 1990; Resnik, 1992; Schabes, 1992). How-\never, to date there has been little success in con-\nstructing grammar-based language models competi-\ntive with n-gram models in problems of any magni-\ntude.\nIn this paper, we describe a corpus-based induc-\ntion algorithm for probabilistic context-free gram-\nmars that outperforms n-gram models and the\nInside-Outside algorithm (Baker, 1979) in medium-\nsized domains. This result marks the ﬁrst time\na grammar-based language model has surpassed n-\ngram modeling in a task of at least moderate size.\nThe algorithm employs a greedy heuristic search\nwithin a Bayesian framework, and a post-pass us-\ning the Inside-Outside algorithm.\n2 Grammar Induction as Search\nGrammar induction can be framed as a search prob-\nlem, and has been framed as such almost without ex-\nception in past research (Angluin and Smith, 1983).\nThe search space is taken to be some class of gram-\nmars; for example, in our work we search within the\nspace of probabilistic context-free grammars. The\nobjective function is taken to be some measure de-\npendent on the training data; one generally wants to\nﬁnd a grammar that in some sense accurately models\nthe training data.\nMost work in language modeling, including n-\ngram models and the Inside-Outside algorithm, falls\nunder the maximum-likelihood paradigm, where one\ntakes the objective function to be the likelihood of\nthe training data given the grammar. However, the\noptimal grammar under this objective function is\none which generates only strings in the training data\nand no other strings. Such grammars are poor lan-\nguage models, as they overﬁt the training data and\ndo not model the language at large. In n-gram mod-\nels and the Inside-Outside algorithm, this issue is\nS → SX (1 − ǫ)\nS → X (ǫ)\nX → A (p(A)) ∀ A ∈ N − { S, X }\nAa → a (1) ∀ a ∈ T\nN = the set of all nonterminal symbols\nT = the set of all terminal symbols\nProbabilities for each rule are in parentheses.\nTable 1: Initial hypothesis grammar\nevaded by bounding the size and form of the gram-\nmars considered, so that the “optimal” grammar\ncannot be expressed. However, in our work we do\nnot wish to limit the size of the grammars consid-\nered.\nThe basic shortcoming of the maximum-likelihood\nobjective function is that it does not encompass the\ncompelling intuition behind Occam’s Razor, that\nsimpler (or smaller) grammars are preferable over\ncomplex (or larger) grammars. A factor in the ob-\njective function that favors smaller grammars over\nlarge can prevent the objective function from pre-\nferring grammars that overﬁt the training data.\nSolomonoﬀ (1964) presents a Bayesian grammar in-\nduction framework that includes such a factor in a\nmotivated manner.\nThe goal of grammar induction is taken to be ﬁnd-\ning the grammar with the largest a posteriori prob-\nability given the training data, that is, ﬁnding the\ngrammar G′ where\nG′ = arg max\nG\np(G|O)\nand where we denote the training data as O, for ob-\nservations. As it is unclear how to estimate p(G|O)\ndirectly, we apply Bayes’ Rule and get\nG′ = arg max\nG\np(O|G)p(G)\np(O) = arg max\nG\np(O|G)p(G)\nHence, we can frame the search for G′ as a search\nwith the objective function p(O|G)p(G), the likeli-\nhood of the training data multiplied by the prior\nprobability of the grammar.\nWe satisfy the goal of favoring smaller grammars\nby choosing a prior that assigns higher probabilities\nto such grammars. In particular, Solomonoﬀ pro-\nposes the use of the universal a priori probability\n(Solomonoﬀ, 1960), which is closely related to the\nminimum description length principle later proposed\nby (Rissanen, 1978). In the case of grammatical lan-\nguage modeling, this corresponds to taking\np(G) = 2 −l(G)\nwhere l(G) is the length of the description of the\ngrammar in bits. The universal a priori probabil-\nity has many elegant properties, the most salient\nof which is that it dominates all other enumerable\nprobability distributions multiplicatively. 1\n3 Search Algorithm\nAs described above, we take grammar induction to\nbe the search for the grammar G′ that optimizes the\nobjective function p(O|G)p(G). While this frame-\nwork does not restrict us to a particular grammar\nformalism, in our work we consider only probabilis-\ntic context-free grammars.\nWe assume a simple greedy search strategy. We\nmaintain a single hypothesis grammar which is ini-\ntialized to a small, trivial grammar. We then try to\nﬁnd a modiﬁcation to the hypothesis grammar, such\nas the addition of a grammar rule, that results in a\ngrammar with a higher score on the objective func-\ntion. When we ﬁnd a superior grammar, we make\nthis the new hypothesis grammar. We repeat this\nprocess until we can no longer ﬁnd a modiﬁcation\nthat improves the current hypothesis grammar.\nFor our initial grammar, we choose a grammar\nthat can generate any string, to assure that the\ngrammar can cover the training data. The initial\ngrammar is listed in Table 1. The sentential symbol\nS expands to a sequence of X’s, where X expands\nto every other nonterminal symbol in the grammar.\nInitially, the set of nonterminal symbols consists of\na diﬀerent nonterminal symbol expanding to each\nterminal symbol.\nNotice that this grammar models a sentence as\na sequence of independently generated nonterminal\nsymbols. We maintain this property throughout the\nsearch process, that is, for every symbol A′ that we\nadd to the grammar, we also add a rule X → A′.\nThis assures that the sentential symbol can expand\n1 A very thorough discussion of the universal a priori\nprobability is given by Li and Vit´ anyi (1993).\nS\n✟✟✟\n❍❍❍\nS\n✟✟ ❍❍\nS\nX\nAMary\nMary\nX\nAtalks\ntalks\nX\nAslowly\nslowly\nS\n✟✟✟\n❍❍❍\nS\n✟✟ ❍❍\nS\nX\nABob\nBob\nX\nAtalks\ntalks\nX\nAslowly\nslowly\nFigure 1: Initial Viterbi Parse\nS\n✟✟✟\n❍❍❍\nS\nX\nAMary\nMary\nX\nB\n✟✟ ❍❍\nAtalks\ntalks\nAslowly\nslowly\nS\n✟✟✟\n❍❍❍\nS\nX\nABob\nBob\nX\nB\n✟✟ ❍❍\nAtalks\ntalks\nAslowly\nslowly\nFigure 2: Predicted Viterbi Parse\nto every symbol; otherwise, adding a symbol will not\naﬀect the probabilities that the grammar assigns to\nstrings.\nWe use the term move set to describe the set of\nmodiﬁcations we consider to the current hypothesis\ngrammar to hopefully produce a superior grammar.\nOur move set includes the following moves:\nMove 1: Create a rule of the form A → BC\nMove 2: Create a rule of the form A → B|C\nFor any context-free grammar, it is possible to ex-\npress a weakly equivalent grammar using only rules\nof these forms. As mentioned before, with each new\nsymbol A we also create a rule X → A.\n3.1 Evaluating the Objective Function\nConsider the task of calculating the objective func-\ntion p(O|G)p(G) for some grammar G. Calculating\np(G) = 2 −l(G) is inexpensive 2; however, calculating\np(O|G) requires a parsing of the entire training data.\nWe cannot aﬀord to parse the training data for each\ngrammar considered; indeed, to ever be practical for\n2Due to space limitations, we do not specify our\nmethod for encoding grammars, i.e., how we calculate\nl(G) for a given G. However, this will be described in\nthe author’s forthcoming Ph.D. dissertation.\ndata sets of millions of words, it seems likely that we\ncan only aﬀord to parse the data once.\nTo achieve this goal, we employ several approx-\nimations. First, notice that we do not ever need\nto calculate the actual value of the objective func-\ntion; we need only to be able to distinguish when\na move applied to the current hypothesis grammar\nproduces a grammar that has a higher score on the\nobjective function, that is, we need only to be able\nto calculate the diﬀerence in the objective function\nresulting from a move. This can be done eﬃciently\nif we can quickly approximate how the probability\nof the training data changes when a move is applied.\nTo make this possible, we approximate the proba-\nbility of the training data p(O|G) by the probability\nof the single most probable parse, or Viterbi parse,\nof the training data. Furthermore, instead of recal-\nculating the Viterbi parse of the training data from\nscratch when a move is applied, we use heuristics to\npredict how a move will change the Viterbi parse.\nFor example, consider the case where the training\ndata consists of the two sentences\nO = {Bob talks slowly , Mary talks slowly }\nIn Figure 1, we display the Viterbi parse of this data\nunder the initial hypothesis grammar used in our\nalgorithm.\nNow, let us consider the move of adding the rule\nB → Atalks Aslowly\nto the initial grammar (as well as the concomitant\nrule X → B). A reasonable heuristic for predict-\ning how the Viterbi parse will change is to replace\nadjacent X’s that expand to Atalks and Aslowly re-\nspectively with a single X that expands to B, as\ndisplayed in Figure 2. This is the actual heuristic\nwe use for moves of the form A → BC, and we have\nanalogous heuristics for each move in our move set.\nBy predicting the diﬀerences in the Viterbi parse re-\nsulting from a move, we can quickly estimate the\nchange in the probability of the training data.\nNotice that our predicted Viterbi parse can stray\na great deal from the actual Viterbi parse, as errors\ncan accumulate as move after move is applied. To\nminimize these eﬀects, we process the training data\nincrementally. Using our initial hypothesis gram-\nmar, we parse the ﬁrst sentence of the training data\nand search for the optimal grammar over just that\none sentence using the described search framework.\nWe use the resulting grammar to parse the second\nsentence, and then search for the optimal grammar\nover the ﬁrst two sentences using the last grammar\nas the starting point. We repeat this process, pars-\ning the next sentence using the best grammar found\non the previous sentences and then searching for the\nbest grammar taking into account this new sentence,\nuntil the entire training corpus is covered.\nDelaying the parsing of a sentence until all of the\nprevious sentences are processed should yield more\naccurate Viterbi parses during the search process\nthan if we simply parse the whole corpus with the\ninitial hypothesis grammar. In addition, we still\nachieve the goal of parsing each sentence but once.\n3.2 Parameter Training\nIn this section, we describe how the parameters of\nour grammar, the probabilities associated with each\ngrammar rule, are set. Ideally, in evaluating the ob-\njective function for a particular grammar we should\nuse its optimal parameter settings given the training\ndata, as this is the full score that the given grammar\ncan achieve. However, searching for optimal param-\neter values is extremely expensive computationally.\nInstead, we grossly approximate the optimal values\nby deterministically setting parameters based on the\nViterbi parse of the training data parsed so far. We\nrely on the post-pass, described later, to reﬁne pa-\nrameter values.\nReferring to the rules in Table 1, the parameter ǫ is\nset to an arbitrary small constant. The values of the\nparameters p(A) are set to the (smoothed) frequency\nof the X → A reduction in the Viterbi parse of the\ndata seen so far. The remaining symbols are set to\nexpand uniformly among their possible expansions.\n3.3 Constraining Moves\nConsider the move of creating a rule of the form\nA → BC. This corresponds to k3 diﬀerent speciﬁc\nrules that might be created, where k is the current\nnumber of symbols in the grammar. As it is too\ncomputationally expensive to consider each of these\nrules at every point in the search, we use heuristics\nto constrain which moves are appraised.\nFor the left-hand side of a rule, we always cre-\nate a new symbol. This heuristic selects the opti-\nmal choice the vast majority of the time; however,\nunder this constraint the moves described earlier in\nthis section cannot yield arbitrary context-free lan-\nguages. To partially address this, we add the move\nMove 3: Create a rule of the form A → AB|B\nWith this iteration move, we can construct gram-\nmars that generate arbitrary regular languages. As\nyet, we have not implemented moves that enable\nthe construction of arbitrary context-free grammars;\nthis belongs to future work.\nTo constrain the symbols we consider on the right-\nhand side of a new rule, we use what we call trig-\ngers.3 A trigger is a phenomenon in the Viterbi\nparse of a sentence that is indicative that a particu-\nlar move might lead to a better grammar. For exam-\nple, in Figure 1 the fact that the symbols Atalks and\nAslowly occur adjacently is indicative that it could\nbe proﬁtable to create a rule B → AtalksAslowly . We\nhave developed a set of triggers for each move in our\nmove set, and only consider a speciﬁc move if it is\ntriggered in the sentence currently being parsed in\nthe incremental processing.\n3.4 Post-Pass\nA conspicuous shortcoming in our search framework\nis that the grammars in our search space are fairly\nunexpressive. Firstly, recall that our grammars\nmodel a sentence as a sequence of independently gen-\nerated symbols; however, in language there is a large\ndependence between adjacent constituents. Further-\nmore, the only free parameters in our search are the\nparameters p(A); all other symbols (except S) are\nﬁxed to expand uniformly. These choices were nec-\nessary to make the search tractable.\nTo address this issue, we use an Inside-Outside al-\ngorithm post-pass. Our methodology is derived from\n3This is not to be confused with the use of the term\ntriggers in dynamic language modeling.\nthat described by Lari and Young (1990). We cre-\nate n new nonterminal symbols {X1, . . . , X n}, and\ncreate all rules of the form:\nXi → Xj Xk i, j, k ∈ { 1, . . . , n }\nXi → A i ∈ { 1, . . . , n },\nA ∈ Nold − { S, X }\nNold denotes the set of nonterminal symbols ac-\nquired in the initial grammar induction phase, and\nX1 is taken to be the new sentential symbol. These\nnew rules replace the ﬁrst three rules listed in Table\n1. The parameters of these rules are initialized ran-\ndomly. Using this grammar as the starting point,\nwe run the Inside-Outside algorithm on the training\ndata until convergence.\nIn other words, instead of using the naive S →\nSX |X rule to attach symbols together in parsing\ndata, we now use the Xi rules and depend on the\nInside-Outside algorithm to train these randomly\ninitialized rules intelligently. This post-pass allows\nus to express dependencies between adjacent sym-\nbols. In addition, it allows us to train parameters\nthat were ﬁxed during the initial grammar induc-\ntion phase.\n4 Previous Work\nAs mentioned, this work employs the Bayesian gram-\nmar induction framework described by Solomonoﬀ\n(1960; 1964). However, Solomonoﬀ does not specify\na concrete search algorithm and only makes sugges-\ntions as to its nature.\nSimilar research includes work by Cook et al.\n(1976) and Stolcke and Omohundro (1994). This\nwork also employs a heuristic search within a\nBayesian framework. However, a diﬀerent prior\nprobability on grammars is used, and the algorithms\nare only eﬃcient enough to be applied to small data\nsets.\nThe grammar induction algorithms most suc-\ncessful in language modeling include the Inside-\nOutside algorithm (Lari and Young, 1990; Lari\nand Young, 1991; Pereira and Schabes, 1992), a\nspecial case of the Expectation-Maximization al-\ngorithm (Dempster et al., 1977), and work by\nMcCandless and Glass (1993). In the latter work,\nMcCandless uses a heuristic search procedure simi-\nlar to ours, but a very diﬀerent search criteria. To\nour knowledge, neither algorithm has surpassed the\nperformance of n-gram models in a language model-\ning task of substantial scale.\n5 Results\nTo evaluate our algorithm, we compare the perfor-\nmance of our algorithm to that of n-gram models\nand the Inside-Outside algorithm.\nFor n-gram models, we tried n = 1 , . . . , 10 for each\ndomain. For smoothing a particular n-gram model,\nwe took a linear combination of all lower order n-\ngram models. In particular, we follow standard prac-\ntice (Jelinek and Mercer, 1980; Bahl et al., 1983;\nBrown et al., 1992) and take the smoothed i-gram\nprobability to be a linear combination of the i-gram\nfrequency in the training data and the smoothed\n(i − 1)-gram probability, that is,\np(w0|W = wi−1 · · ·w−1) =\nλi,c(W )\nc(W w0)\nc(W ) +\n(1 − λi,c(W ))p(w0|wi−2 · · ·w−1)\nwhere c(W ) denotes the count of the word sequence\nW in the training data. The smoothing parameters\nλi,c are trained through the Forward-Backward al-\ngorithm (Baum and Eagon, 1967) on held-out data.\nParameters λi,c are tied together for similar c to pre-\nvent data sparsity.\nFor the Inside-Outside algorithm, we follow the\nmethodology described by Lari and Young. For a\ngiven n, we create a probabilistic context-free gram-\nmar consisting of all Chomsky normal form rules\nover the n nonterminal symbols {X1, . . . X n} and the\ngiven terminal symbols, that is, all rules\nXi → Xj Xk i, j, k ∈ { 1, . . . , n }\nXi → a i ∈ { 1, . . . , n }, a ∈ T\nwhere T denotes the set of terminal symbols in the\ndomain. All parameters are initialized randomly.\nFrom this starting point, the Inside-Outside algo-\nrithm is run until convergence.\nFor smoothing, we combine the expansion distri-\nbution of each symbol with a uniform distribution,\nthat is, we take the smoothed parameter ps(A → α)\nto be\nps(A → α) = (1 − λ)pu(A → α) + λ 1\nn3 + n|T |\nwhere pu(A → α) denotes the unsmoothed parame-\nter. The value n3 + n|T | is the number of diﬀerent\nways a symbol expands under the Lari and Young\nmethodology. The parameter λ is trained through\nthe Inside-Outside algorithm on held-out data. This\nsmoothing is also performed on the Inside-Outside\npost-pass of our algorithm. For each domain, we\ntried n = 3 , . . . , 10.\nBecause of the computational demands of our\nalgorithm, it is currently impractical to apply it\nto large vocabulary or large training set problems.\nbest entropy entr. relative\nn (bits/word) to n-gram\nideal grammar 2.30 − 6.5%\nour algorithm 7 2.37 − 3.7%\nn-gram model 4 2.46\nInside-Outside 9 2.60 +5.7%\nTable 2: English-like artiﬁcial grammar\nbest entropy entr. relative\nn (bits/word) to n-gram\nideal grammar 4.13 − 10.4%\nour algorithm 9 4.44 − 3.7%\nn-gram model 4 4.61\nInside-Outside 9 4.64 +0.7%\nTable 3: Wall Street Journal-like artiﬁcial grammar\nHowever, we present the results of our algorithm in\nthree medium-sized domains. In each case, we use\n4500 sentences for training, with 500 of these sen-\ntences held out for smoothing. We test on 500 sen-\ntences, and measure performance by the entropy of\nthe test data.\nIn the ﬁrst two domains, we created the train-\ning and test data artiﬁcially so as to have an ideal\ngrammar in hand to benchmark results. In particu-\nlar, we used a probabilistic grammar to generate the\ndata. In the ﬁrst domain, we created this grammar\nby hand; the grammar was a small English-like prob-\nabilistic context-free grammar consisting of roughly\n10 nonterminal symbols, 20 terminal symbols, and\n30 rules. In the second domain, we derived the gram-\nmar from manually parsed text. From a million\nwords of parsed Wall Street Journal data from the\nPenn treebank, we extracted the 20 most frequently\noccurring symbols, and the 10 most frequently oc-\ncurring rules expanding each of these symbols. For\neach symbol that occurs on the right-hand side of\na rule but which was not one of the most frequent\n20 symbols, we create a rule that expands that sym-\nbol to a unique terminal symbol. After removing\nunreachable rules, this yields a grammar of roughly\n30 nonterminals, 120 terminals, and 160 rules. Pa-\nrameters are set to reﬂect the frequency of the cor-\nresponding rule in the parsed corpus.\nFor the third domain, we took English text and\nreduced the size of the vocabulary by mapping each\nword to its part-of-speech tag. We used tagged Wall\nStreet Journal text from the Penn treebank, which\nhas a tag set size of about ﬁfty.\nIn Tables 2–4, we summarize our results. The\nideal grammar denotes the grammar used to gener-\nate the training and test data. For each algorithm,\nwe list the best performance achieved over all n tried,\nand the best n column states which value realized\nthis performance.\nWe achieve a moderate but signiﬁcant improve-\nment in performance over n-gram models and the\nInside-Outside algorithm in the ﬁrst two domains,\nwhile in the part-of-speech domain we are outper-\nformed by n-gram models but we vastly outperform\nthe Inside-Outside algorithm.\nIn Table 5, we display a sample of the number\nof parameters and execution time (on a Decstation\n5000/33) associated with each algorithm. We choose\nn to yield approximately equivalent performance for\neach algorithm. The ﬁrst pass row refers to the main\ngrammar induction phase of our algorithm, and the\npost-pass row refers to the Inside-Outside post-pass.\nNotice that our algorithm produces a signiﬁcantly\nmore compact model than the n-gram model, while\nrunning signiﬁcantly faster than the Inside-Outside\nalgorithm even though we use an Inside-Outside\npost-pass. Part of this discrepancy is due to the fact\nthat we require a smaller number of new nonterminal\nsymbols to achieve equivalent performance, but we\nhave also found that our post-pass converges more\nquickly even given the same number of nonterminal\nsymbols.\n6 Discussion\nOur algorithm consistently outperformed the Inside-\nOutside algorithm in these experiments. While we\npartially attribute this diﬀerence to using a Bayesian\ninstead of maximum-likelihood objective function,\nbest entropy entr. relative\nn (bits/word) to n-gram\nn-gram model 6 3.01\nour algorithm 7 3.15 +4.7%\nInside-Outside 7 3.93 +30.6%\nTable 4: English sentence part-of-speech sequences\nWSJ n entropy no. time\nartif. (bits/word) params (sec)\nn-gram 3 4.61 15000 50\nIO 9 4.64 2000 30000\nﬁrst pass 800 1000\npost-pass 5 4.60 4000 5000\nTable 5: Parameters and Training Time\nwe believe that part of this diﬀerence results from a\nmore eﬀective search strategy. In particular, though\nboth algorithms employ a greedy hill-climbing strat-\negy, our algorithm gains an advantage by being able\nto add new rules to the grammar.\nIn the Inside-Outside algorithm, the gradient de-\nscent search discovers the “nearest” local minimum\nin the search landscape to the initial grammar. If\nthere are k rules in the grammar and thus k pa-\nrameters, then the search takes place in a ﬁxed k-\ndimensional space Rk. In our algorithm, it is possi-\nble to expand the hypothesis grammar, thus increas-\ning the dimensionality of the parameter space that\nis being searched. An apparent local minimum in\nthe space Rk may no longer be a local minimum in\nthe space Rk+1; the extra dimension may provide a\npathway for further improvement of the hypothesis\ngrammar. Hence, our algorithm should be less prone\nto suboptimal local minima than the Inside-Outside\nalgorithm.\nOutperforming n-gram models in the ﬁrst two do-\nmains demonstrates that our algorithm is able to\ntake advantage of the grammatical structure present\nin data. However, the superiority of n-gram models\nin the part-of-speech domain indicates that to be\ncompetitive in modeling naturally-occurring data, it\nis necessary to model collocational information ac-\ncurately. We need to modify our algorithm to more\naggressively model n-gram information.\n7 Conclusion\nThis research represents a step forward in the quest\nfor developing grammar-based language models for\nnatural language. We induce models that, while be-\ning substantially more compact, outperform n-gram\nlanguage models in medium-sized domains. The al-\ngorithm runs essentially in time and space linear in\nthe size of the training data, so larger domains are\nwithin our reach.\nHowever, we feel the largest contribution of this\nwork does not lie in the actual algorithm speciﬁed,\nbut rather in its indication of the potential of the in-\nduction framework described by Solomonoﬀ in 1964.\nWe have implemented only a subset of the moves\nthat we have developed, and inspection of our re-\nsults gives reason to believe that these additional\nmoves may signiﬁcantly improve the performance of\nour algorithm.\nSolomonoﬀ’s induction framework is not re-\nstricted to probabilistic context-free grammars. Af-\nter completing the implementation of our move\nset, we plan to explore the modeling of context-\nsensitive phenomena. This work demonstrates that\nSolomonoﬀ’s elegant framework deserves much fur-\nther consideration.\nAcknowledgements\nWe are indebted to Stuart Shieber for his suggestions\nand guidance, as well as his invaluable comments on\nearlier drafts of this paper. This material is based\non work supported by the National Science Founda-\ntion under Grant Number IRI-9350192 to Stuart M.\nShieber.\nReferences\n[Angluin and Smith1983] D. Angluin and\nC.H. Smith. 1983. Inductive inference: theory\nand methods. ACM Computing Surveys , 15:237–\n269.\n[Bahl et al.1978] L.R. Bahl, J.K. Baker, P.S. Cohen,\nF. Jelinek, B.L. Lewis, and R.L. Mercer. 1978.\nRecognition of a continuously read natural corpus.\nIn Proceedings of the IEEE International Confer-\nence on Acoustics, Speech and Signal Processing ,\npages 422–424, Tulsa, Oklahoma, April.\n[Bahl et al.1983] Lalit R. Bahl, Frederick Jelinek,\nand Robert L. Mercer. 1983. A maximum likeli-\nhood approach to continuous speech recognition.\nIEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence , PAMI-5(2):179–190, March.\n[Baker1975] J.K. Baker. 1975. The DRAGON\nsystem – an overview. IEEE Transactions on\nAcoustics, Speech and Signal Processing , 23:24–\n29, February.\n[Baker1979] J.K. Baker. 1979. Trainable gram-\nmars for speech recognition. In Proceedings of\nthe Spring Conference of the Acoustical Society of\nAmerica, pages 547–550, Boston, MA, June.\n[Baum and Eagon1967] L.E. Baum and J.A. Eagon.\n1967. An inequality with application to statistical\nestimation for probabilistic functions of Markov\nprocesses and to a model for ecology. Bulletin\nof the American Mathematicians Society , 73:360–\n363.\n[Brown et al.1992] Peter F. Brown, Vincent J. Del-\nlaPietra, Peter V. deSouza, Jennifer C. Lai, and\nRobert L. Mercer. 1992. Class-based n-gram\nmodels of natural language. Computational Lin-\nguistics, 18(4):467–479, December.\n[Dempster et al.1977] A.P. Dempster, N.M. Laird,\nand D.B. Rubin. 1977. Maximum likelihood from\nincomplete data via the EM algorithm. Journal\nof the Royal Statistical Society , 39(B):1–38.\n[Jelinek and Mercer1980] Frederick Jelinek\nand Robert L. Mercer. 1980. Interpolated esti-\nmation of Markov source parameters from sparse\ndata. In Proceedings of the Workshop on Pattern\nRecognition in Practice, Amsterdam, The Nether-\nlands: North-Holland, May.\n[Kernighan et al.1990] M.D.\nKernighan, K.W. Church, and W.A. Gale. 1990.\nA spelling correction program based on a noisy\nchannel model. In Proceedings of the Thirteenth\nInternational Conference on Computational Lin-\nguistics, pages 205–210.\n[Lari and Young1990] K. Lari and S.J. Young. 1990.\nThe estimation of stochastic context-free gram-\nmars using the inside-outside algorithm. Com-\nputer Speech and Language , 4:35–56.\n[Lari and Young1991] K. Lari and S.J. Young. 1991.\nApplications of stochastic context-free grammars\nusing the inside-outside algorithm. Computer\nSpeech and Language , 5:237–257.\n[Li and Vit´ anyi1993] Ming Li and Paul Vit´ anyi.\n1993. An Introduction to Kolmogorov Complex-\nity and its Applications . Springer-Verlag.\n[McCandless and Glass1993] Michael K. McCandless\nand James R. Glass. 1993. Empirical acquisition\nof word and phrase classes in the ATIS domain. In\nThird European Conference on Speech Communi-\ncation and Technology , Berlin, Germany, Septem-\nber.\n[Pereira and Schabes1992] Fernando Pereira and\nYves Schabes. 1992. Inside-outside reestimation\nfrom partially bracket corpora. In Proceedings of\nthe 30th Annual Meeting of the ACL , pages 128–\n135, Newark, Delaware.\n[Resnik1992] P. Resnik. 1992. Probabilistic tree-\nadjoining grammar as a framework for statistical\nnatural language processing. In Proceedings of the\n14th International Conference on Computational\nLinguistics.\n[Rissanen1978] J. Rissanen. 1978. Modeling by the\nshortest data description. Automatica, 14:465–\n471.\n[Schabes1992] Y. Schabes. 1992. Stochastic lexical-\nized tree-adjoining grammars. In Proceedings of\nthe 14th International Conference on Computa-\ntional Linguistics .\n[Shannon1951] C.E. Shannon. 1951. Prediction and\nentropy of printed English. Bell Systems Techni-\ncal Journal , 30:50–64, January.\n[Solomonoﬀ1960] R.J. Solomonoﬀ. 1960. A prelimi-\nnary report on a general theory of inductive infer-\nence. Technical Report ZTB-138, Zator Company,\nCambridge, MA, November.\n[Solomonoﬀ1964] R.J. Solomonoﬀ. 1964. A formal\ntheory of inductive inference. Information and\nControl, 7:1–22, 224–254, March, June.\n[Srihari and Baltus1992] Rohini Srihari and Char-\nlotte Baltus. 1992. Combining statistical and\nsyntactic methods in recognizing handwritten sen-\ntences. In AAAI Symposium: Probabilistic Ap-\nproaches to Natural Language , pages 121–127.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6560320854187012
    },
    {
      "name": "Grammar",
      "score": 0.6045110821723938
    },
    {
      "name": "Natural language processing",
      "score": 0.5432092547416687
    },
    {
      "name": "Grammar induction",
      "score": 0.45935961604118347
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44503533840179443
    },
    {
      "name": "Linguistics",
      "score": 0.3887025713920593
    },
    {
      "name": "Programming language",
      "score": 0.37110215425491333
    },
    {
      "name": "Rule-based machine translation",
      "score": 0.13026434183120728
    },
    {
      "name": "Philosophy",
      "score": 0.0927518904209137
    }
  ]
}