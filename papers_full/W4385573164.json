{
  "title": "Fine-tuned Language Models are Continual Learners",
  "url": "https://openalex.org/W4385573164",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2932737571",
      "name": "Thomas Scialom",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2890328207",
      "name": "Tuhin Chakrabarty",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2041095597",
      "name": "Smaranda Muresan",
      "affiliations": [
        "Columbia University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2891012317",
    "https://openalex.org/W2554863749",
    "https://openalex.org/W3100152912",
    "https://openalex.org/W4226464635",
    "https://openalex.org/W3117055973",
    "https://openalex.org/W4225484930",
    "https://openalex.org/W3212849024",
    "https://openalex.org/W2534253848",
    "https://openalex.org/W4301365964",
    "https://openalex.org/W3196731672",
    "https://openalex.org/W3034639488",
    "https://openalex.org/W4288336773",
    "https://openalex.org/W4285293425",
    "https://openalex.org/W3035008906",
    "https://openalex.org/W4384434135",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2788388592",
    "https://openalex.org/W3112170794",
    "https://openalex.org/W3167056186",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W2060277733",
    "https://openalex.org/W2951936329",
    "https://openalex.org/W2995409942",
    "https://openalex.org/W4283332761",
    "https://openalex.org/W4205340316",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2950681488",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2963559848",
    "https://openalex.org/W4287800011",
    "https://openalex.org/W2964189064",
    "https://openalex.org/W3100749323",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4287890137",
    "https://openalex.org/W3047636089",
    "https://openalex.org/W2951583236",
    "https://openalex.org/W4225620948",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4398570551",
    "https://openalex.org/W3172053684",
    "https://openalex.org/W3205068155"
  ],
  "abstract": "Recent work on large language models relies on the intuition that most natural language processing tasks can be described via natural language instructions and that models trained on these instructions show strong zero-shot performance on several standard datasets. However, these models even though impressive still perform poorly on a wide range of tasks outside of their respective training and evaluation sets.To address this limitation, we argue that a model should be able to keep extending its knowledge and abilities, without forgetting previous skills. In spite of the limited success of Continual Learning, we show that Fine-tuned Language Models can be continual learners.We empirically investigate the reason for this success and conclude that Continual Learning emerges from self-supervision pre-training. Our resulting model Continual-T0 (CT0) is able to learn 8 new diverse language generation tasks, while still maintaining good performance on previous tasks, spanning in total of 70 datasets. Finally, we show that CT0 is able to combine instructions in ways it was never trained for, demonstrating some level of instruction compositionality.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6107–6122\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nFine-tuned Language Models are Continual Learners\nThomas Scialom1∗ Tuhin Chakrabarty2∗ Smaranda Muresan 2\n1Meta AI\n2Department of Computer Science, Columbia University\ntscialom@fb.com, tuhin.chakr@cs.columbia.edu, smara@cs.columbia.edu\nAbstract\nRecent work on large language models relies\non the intuition that most natural language pro-\ncessing tasks can be described via natural lan-\nguage instructions and that models trained on\nthese instructions show strong zero-shot perfor-\nmance on several standard datasets. However,\nthese models even though impressive still per-\nform poorly on a wide range of tasks outside of\ntheir respective training and evaluation sets. To\naddress this limitation, we argue that a model\nshould be able to keep extending its knowledge\nand abilities, without forgetting previous skills.\nIn spite of the limited success of Continual\nLearning we show that Fine-tuned Language\nModels can be continual learners. We empir-\nically investigate the reason for this success\nand conclude that Continual Learning emerges\nfrom self-supervision pre-training. Our result-\ning model Continual-T0 (CT0) is able to learn\n8 new diverse language generation tasks, while\nstill maintaining good performance on previous\ntasks, spanning in total 70 datasets. Finally, we\nshow that CT0 is able to combine instructions\nin ways it was never trained for, demonstrating\nsome level of instruction compositionality.1\n1 Introduction\nRecent work has shown that large language mod-\nels have the ability to perform zero-shot and few-\nshot learning reasonably well (Brown et al., 2020;\nRae et al., 2021; Smith et al., 2022). A particu-\nlarly successful line of work relies on the intuition\nthat most natural language processing tasks can be\ndescribed via natural language instructions. For\nexample, a summarization task can be reformat-\nted as a response to a natural language input as\nshown in Table 1. Sanh et al. (2022) and Wei et al.\n(2022) have released T0 and FLAN respectively\nand shown that fine-tuning models on a massive\n∗Both Authors Contributed Equally\n1Our code is publicly available at https://github.com/\nThomasScialom/T0_continual_learning.\nThe picture appeared on the wall of a\nPoundland store on Whymark Avenue [...] How\nwould you rephrase that in a few words?\nGraffiti artist Banksy is believed to be\nbehind [....]\nTable 1: An instance from T0 training set (Sanh et al.,\n2022) where a summarization task is reformatted as a\nnatural language response to a natural language input.\nmixture of NLP datasets expressed via such natural\nlanguage instructions (i.e., instruction tuning), im-\nproves the zero-shot performance of large language\nmodels. FLAN is extremely large in size (137B)\nand is not publicly available limiting its further use\nand reproducibility. Conversely T0 (Sanh et al.,\n2022) is publicly available and orders of magnitude\nsmaller and hence we resort to working with T0.\nHowever impressive, these models are still lim-\nited to simple instructions and mainly Natural Lan-\nguage Understanding (NLU) tasks. These models\nperform poorly on a wide range of tasks that differ\nfrom their respective evaluation sets. To improve\ntheir ability on new and diverse tasks, one needs\nto fine-tune these models again. However, one key\nproblem associated with fine-tuning is catastrophic\nforgetting (French, 1999). So, how can we extend\nthese models knowledge and abilities, without suf-\nfering from catastrophic forgetting?\nIn this paper, we study Continual Learning of\nlarge language models fine-tuned on natural lan-\nguage instructions and investigate their ability to\nadapt to diverse tasks, while avoiding catastrophic\nforgetting on the older tasks. For this purpose,\nwe propose Continual-T0 (CT0), a T0 model that\nuses Continual Learning with rehearsal (Shin et al.,\n2017), i.e. using a memory buffer containing a\nsmall portion of previous data replayed during train-\ning (Section 3). We start from T0, a model trained\njointly on 50 datasets, resulting in a good zero-shot\nperformance on 12 completely different datasets.\nWe are then able to teach progressively 8 new di-\nverse tasks, while maintaining almost 100% of the\n6107\ninitial performance on all the previous datasets.\nThis result is obtained by using only 1% of data for\nmemory buffer. Notably, we also maintain the per-\nformance for the T0 zero-shot evaluation datasets,\neven though no rehearsal for those was done, the\nfirst of its kind setup for CL (Section 4).\nOur final model, Continual-T0 (CT0) in addition\nto performing as well as T0 on all the different\nT0 datasets, can also understand instructions about\nthe newly introduced tasks focused on language\ngeneration problems such as writing a haiku, gen-\nerating empathetic responses in a dialogue, sim-\nplifying text, generating a headline with decoding\nconstraints, generating natural language explana-\ntions for Natural Language Inferece (NLI) tasks,\ngenerating a Tweet on a given topic in the style\nof a given author, or question answering for new\ndomain/concepts such as COVID-19.\nWe also conduct an extensive analysis and show\nthat our newly learned instructions can be com-\nposed in ways never seen during training, leading\nto better generalization (Section 4.3). Given the sur-\nprising performance of a simple continual learning\nstrategy, we empirically investigate the reason for\nthis success. Why transformer models like T0 are\ncontinual learners? Is it because of their multi-task\nnature or the instruction tuning paradigm? Or does\nthe large scale parameterization of language mod-\nels contribute to this success? Our experimental\nanalysis show that the easy adaptability and con-\ntinual learning capabilities actually emerge from\npre-training and not the above, including scale\n(Table 5, Section 5.1).\n2 Related Work\nContinual Learning Current models are limited\nin continuously learning without forgetting any pre-\nviously acquired knowledge and abilities. Research\nin this direction has investigated various strategies\nsuch as External Memory, Constraints and Model\nPlasticity (Parisi et al., 2019). External Memory\nmethods often simply use rehearsal with a replay\nduring training (Rebuffi et al., 2017). de Mas-\nson D’Autume et al. (2019) also proposed local\nfine-tuning at inference time, leveraging examples\nsimilar to the considered input.\nThrough the lens of NLP tasks, Biesialska et al.\n(2020) look at the problem of CL and discuss major\nchallenges involved. Jin et al. (2021) show CL al-\ngorithms are effective for knowledge preservation.\nTheir study also infer that continual pretraining im-\nproves temporal generalization. (Douillard et al.,\n2021) proposed a a dynamic expansion of special\ntokens with a transformer architecture. Mi et al.\n(2020) and Madotto et al. (2021) perform CL for\ntask oriented dialog systems by using replay based\nstrategy. Cao et al. (2021) propose a new CL frame-\nwork for NMT models, while Ke et al. (2021) pro-\nposes a novel capsule network based model called\nB-CL (Bert based CL) for sentiment classification\ntasks. Jin et al. (2020) show how existing CL algo-\nrithms fail at learning compositional phrases. Lin\net al. (2022) propose a benchmark and highlight key\nchallenges for continual model refinement in Out-\nof-Distribution data streams. More recently, Sun\net al. (2019) propose a lifelong learning method\nLAMOL that is capable of continually learning\nnew tasks by replaying pseudo-samples of previous\ntasks that require no extra memory or model capac-\nity. To the best of our knowledge, LAMOL corre-\nsponds to the state-of-the-art for CL in NLP. Most\nsimilar to our work is that of Yin et al. (2022) who\nalso study continual learning from task instructions\nbased on the NATURAL-INSTRUCTION bench-\nmark (Mishra et al., 2022).Finally instead of lim-\niting to vision-only and language-only tasks Srini-\nvasan et al. (2022) study the challenge of learning\nmultimodal tasks in a CL setting, and systemati-\ncally evaluate how upstream continual learning can\nrapidly generalize to new multimodal and unimodal\ntasks\nMost of the aforementioned works fall into the 2\nscenarios differentiated by Lomonaco and Maltoni\n(2017): 1) learning new data of known classes (on-\nline learning), and 2) learning new classes (class-\nincremental learning). Thus, the study are often\nlimited to a narrow domain, or a specific task. In\nour work, we propose to address Continual Learn-\ning more broadly: learning a diverse set of new\ntasks different from the ones used for training. For\nthis, we leverage the idea of instruction tuning (Wei\net al., 2022; Sanh et al., 2022), that enables us to\nframe any NLP task as a response to a natural lan-\nguage input and use rehearsal as a mechanism to\navoid catastrophic forgetting (Shin et al., 2017).\n3 Continual Learning for Fine-tuned\nLanguage Models\n3.1 Continual Learning via Rehearsal (CLR)\nOur objective is to maintain the model’s existing\nlearned skills, while progressively learning more\ntasks. To prevent the model from catastrophic for-\n6108\ngetting, we rely on an external memory module,\nstoring a subset of previous training data (Shin\net al., 2017). We define the tasks to be learned as\na task sequence T = (T1, T2, , TN ) of N tasks. Di\nis the corresponding dataset for task Ti. Formally,\nthe training data augmented with rehearsal Dr\ni is\ndefined as:\nDr\ni = Di\n⋃i−1∑\nj=1\n(rDj) (1)\nwhere r is the rehearsal hyper-parameter that con-\ntrols the percentage of examples sampled from pre-\nvious tasks T1, ...Ti−1. We note that r = 0corre-\nsponds to no memory, and r = 1is equivalent to a\nmulti-task setup using all the previous examples.\n3.2 Continual-T0 (CT0)\nFor all our experiments, we instantiate our model\nwith the T0 model (Sanh et al., 2022). T0 is a T5\nmodel (Raffel et al., 2020) fine-tuned in a multitask\nsetting on 50 datasets, where the natural language\ninstructions corresponding to individual tasks are\nused as the input. The set of these 50 tasks corre-\nsponds therefore to T1 in 1. This massive instruc-\ntion tuning allows the model to perform well in\na zero-shot setup, by leveraging the information\npresents only in the instructions. Our initial model\nis T0_3B, the T0 version with (only) 3 Billions pa-\nrameters for all our experiments. We used the same\nhyper-parameters as the ones reported in Sanh et al.\n(2022)2. The only new hyper-parameter introduced\nin our paper is the rehearsal proportion r. We ex-\nplore r ∈ [0, 0.25%, 1%] as reported in our first set\nof results (see Section 3).\nFor each of T0 training tasks,we consider\n100,000 examples for training, such that 1% re-\nhearsal corresponds to 1,000 examples that will be\nused as the memory buffer for rehearsal. Thus, for\ndatasets with fewer training examples, we upsam-\nple them and conversely for largest datasets like\nGigaword or Simplification, we limit to 100,000 ex-\namples. Note that here, while we used rehearsal\nfor the training data of T0 training tasks, we\nnever used any data from T0 zeroshot tasks, so\nit remains completely zero-shot. It is important\nto highlight that rehearsal is the standard for CL,\nand a zero-shot set up with no rehearsal has never\nbeen explored yet to the best of our knowledge.\n2See more details at https://huggingface.co/\nbigscience/T0pp\n3.3 Tasks\nWe briefly describe all the tasks T used to progres-\nsively train and evaluate our model (a more com-\nplete description is also given in Appendix 7.2).\nT0 Tasks. As detailed in Section 1, we instantiate\nour model with T0 weights. T0 is trained in a multi-\ntask setting on a collection of 50 datasets spanning\nfrom QA, Classification to Summarization. We re-\nfer to this set of 50 datasets as T0 train (T0tr). To\nevaluate the true zero-shot performance for T0, the\nauthors evaluated it on a set of 12 datasets corre-\nsponding to 4 tasks different from T0 train: Nat-\nural Language Inference, Co-reference resolution,\nWord sense disambiguation and Sentence comple-\ntion. We refer to this set as T0 zero-shot (T0zs).\nNew Tasks. To extend T0 capabilities and bench-\nmark its performance in our continual learning\nsetup,we introduce 8 new tasks focused on lan-\nguage generation, unlike the existing T0 evaluation\ntasks and majority of the T0 training tasks (except\nsummarization). These tasks include: 1) Text Sim-\nplification (Simpl) with the goal of paraphrasing a\ngiven text using simple language, where we train\nour model on WikiAuto Jiang et al. (2020) and\nevaluate it on the WikiAuto and ASSET datasets\n(Alva-Manchego et al., 2020); 2) Headline Gen-\neration with Constraint (HGen), where given a\nnews article D and an input keyword X, the goal\nis to generate a headline that contains the keyword\nat the beginning, at the end or anywhere (see Ta-\nble 2 for a sample instruction to generate a head-\nline containing the keyword at the beginning). To\ncreate the training data, we simply leverage the\ngold-reference to select the keyword X, such that\nour model is trained with consistent and plausi-\nble instructions; 3) Haiku Generation (Haiku) ,\nwhere the task is to generate a Haiku — a type of\nshort form poetry originally from Japan — given\na topic (see Table 2 for a sample instruction). We\ntrain on pairs (Haiku, title) from Reddit and gen-\nerate Haikus for novel topics at inference time; 4)\nCovid QA (CQA) (Möller et al., 2020), a Question\nanswering task focusing on COVID-19. Because\nT0 has been extensively trained on a QA dataset,\nCovidQA in its original format simply requires do-\nmain transfer. To make the task more challenging,\nwe propose to provide only the question as an input,\nnow framing the task as “learn the answer by heart”\nin an encyclopedia style task. This way the task\nframing can be seen as a new strategy to incorpo-\n6109\nText\nSmpfl(Simp)\nInstruction Make this text simpler: \"A Georgian inscription around the drum attests his name.\"\nOutput A Georgian writing on the drum is his name.\nHeadline\nGeneration\n(HGen)\nInstruction\nMake a title for this article that begins with \"protesters\": police arrested five\nanti-nuclear protesters thursday after they sought to disrupt loading of a french\nantarctic research and supply vessel , a spokesman for the protesters said .\nOutput protesters target french research ship\nHaiku\nGen (Haiku)\nInstruction Generate a haiku about ’Seagulls crying high’\nOutput Seagulls crying high / the air smelling of sea salt / Or is it my tears?\nCovid QA\n(CQA)\nInstruction In the context of the COVID pandemic,who is at greater risk of dying\nfrom COVID19?\nOutput patients with underlying medical conditions and the elderly\nInquisitive\nQuestion\nGen(InqQG)\nInstruction\nGiven the following text, write the possible curious question it answers: \"Positrons\ndo not travel backwards in time. Positron-electron annihilation ....... So, we know\nthey collide frequently enough to light up the galaxy in that part of the spectrum\nOutput How often do electrons and positrons collide in nature?\nEmpathetic\nDialog\nGeneration\n(EmDg)\nInstruction\nThe associated emotion is \"disappointed\" and the input prompt is \"Had to cancel our\nfamily vacation coming up next month. My husband’s work said he couldn’t go\nafter they already approved the time off.\". Now what would be your response, given\nthe following dialogue context:=== - I had to cancel our family vacation coming\nup next month.\nOutput I am really sorry to hear that. I hope everything is alright.\nExplanation\nGeneration\n(Exp)\nInstruction\nExplain why the two following sentences are contradicting each other: \"Sentence 1:\nA statue at a museum that no seems to be looking at.\"; Sentence 2: \"Tons of\npeople are gathered around the statue.\"\nOutput If tons of people are gathered around the statue, it is not possible that no one seems to\nbe looking at it.\nTwitter\nStylometry\n(TwSt)\nInstruction Write a tweet about #WelcomeToNewYork, in the style of taylorswift13\nOutput GUYS. #WelcomeToNewYork will be up on iTunes any minute now.\nThis is not a drill!! GO GO GO\nTable 2: Example Instructions with their respective ground-truth for 8 new tasks learned continually from T0.\nrating knowledge and preventing the model from\nconcept drift.\n5) Inquisitive Question Generation (InqQG)\nwhere we train our model on the ELI5 dataset (Fan\net al., 2019) to generate questions that typically\nrequire long form answers; 6) Empathetic Dia-\nlogue Generation (EmDg) , where we generate\na response to a conversational context grounded\nin emotional situations using the Empathetic Dia-\nlogue data Rashkin et al. (2019); 7) Explanation\nGeneration (Exp) where we train a model on the\neSNLI (Camburu et al., 2018) benchmark to gener-\nate natural language explanations given a premise,\nhypothesis and a label (entail, contradict, neutral);\n8) Twitter Stylometry (TwSt), where we generate\na relevant tweet given a hashtag and the tweet’s au-\nthor by fine-tuning on the data consisting of tweets\nfrom the top 20 most followed users in Twitter re-\nleased by Tareaf (2017). We illustrate the 8 new\ntasks with their instructions in Table 2. A complete\ndetailed description for all the 8 tasks with train,\nvalidation splits is available in the Appendix 7.3.1.\n3.4 Automatic Metrics\nWe report the accuracy for T0 zero-shot tasks, and\nstandard metrics for NLG like BLEU(Papineni\net al., 2002) and SARI(Xu et al., 2016) for Sim-\nplification, ROUGE (Lin, 2004) for Headline Gen-\neration, or BERTScore (Zhang et al., 2020) 3for\nopen-domain NLG tasks as it has been found to\ncorrelate well with human judgements.\nWe also designed customized metrics for some\nof the tasks. 4 For instance, to evaluate Twitter\nStylometry where the task is to generate a tweet in\nthe style of the author, we trained a Ridge Classifier\nto predict the author given the evaluated tweet. For\nHaiku generation, we know that in general, a Haiku\ncontains only 17 syllables, broken up into three\nlines. We therefore create a metric to reflect the\ntask structure that integrates i) the differences in\nsyllables and number of lines between the gold and\ngenerated haiku, ii) the BLEU score between gold\n3We use the BERTScore version based off deberta-mnli\n4All those metrics implementations are available in the\npublicly released code.\n6110\nand predicted, and iii) the presence of the topic in\nthe generated haiku. We report all the details for\nthe metrics in the Appendix 7.4.\n4 Results\n0 40 80 120 134\nSteps\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\nRelative Gain\nSimplification (Simp)\nSimp(0.0%)\nSimp(0.25%)\nSimp(1.0%)\nT0zs(0.0%)\nT0zs(0.25%)\nT0zs(1.0%)\n0 40 80 120 134\nSteps\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\nRelative Gain\nHaiku\nHaiku(0.0%)\nHaiku(0.25%)\nHaiku(1.0%)\nT0zs(0.0%)\nT0zs(0.25%)\nT0zs(1.0%)\nFigure 1: Rehearsal ablation with 0.0, 0.25 and 1.0% of\ntraining data showing target task performance along\nwith T0 zero-shot performance(T0zs) with Relative\nGain in Y axis vs Number of training steps in X axis.\nThe results are normalised in % such that -1 corresponds\nto 100% decrease and +1 means +100% increase w.r.t.\nthe initial performance.\n4.1 Learning a New Task at a time\nFirst, we test CLR independently on three tasks\n(Headline Generation with Constraint, Simplifica-\ntion, and Haiku Generation), by varying the re-\nhearsal hyper-parameter between 0%, 0.25% and\n1%, respectively. We report the results in terms of\nRelative Gain in Figure 1.\nWe observe that for the three tasks, the rehearsal\nvalue does not affect the task result: all the blue\ncurves are consistent. Conversely, the rehearsal\nvalue has a dramatic impact on the T0 zero-shot\nresults (green curves). As already discussed previ-\nously, at 0% rehearsal,the model catastrophically\nforgets the T0 zero-shot tasks. Conversely, with\nonly 0.25% rehearsal we observe an almost perfect\nstability. Finally, with 1% rehearsal (solid line), T0\nzero-shot results are stationary, indicating that our\nmodel is able to maintain its performance on those\ntasks, while learning a new task.\n4.2 Learning a Sequence of New Tasks\nAs observed from our previous experiments using\nContinual Learning via rehearsal we can learn a\nnew task at any time without catastrophic forget-\nting, with just a very little rehearsal percentage. As\na next step, we propose to measure whether lan-\nguage models can progressively learn more tasks\nwithout catastrophic forgetting. This is an impor-\ntant direction as it would allow the models to con-\ntinually increase their knowledge and capabilities\nwithout forgetting the knowledge already acquired.\nTo test this hypothesis, we start from T0 check-\npoint, a model trained on 50 datasets. We progres-\nsively train it on a sequence of 8 new NLG tasks\n(see Section 7.3.1 and Table 2 for description of\nthose tasks) using Continual Learning via rehearsal\n(r = 1%). We call our final model CT0.\nTo measure the actual success for CL on a se-\nquence of N tasks, we introduce the notion of Up-\nper Bound (UB). UB corresponds to the maximum\nperformance achieved by the model, when fine-\ntuned only on a specific task, Tn. Arguably, the\nmodel succeeds in CL, if it maintains a perfor-\nmance close to UB, while learning new tasks. The\nnormalised results, i.e .,Relative Gain for a given\ntask Tn, correspond to the actual scores s divided\nby their task Tn UB, sTn /UBTn . Hence, 1 corre-\nsponds to performing similar to the UB for any task.\nThe model is expected to start bellow 1 before step\nn since it has not been trained yet on Tn, while\nfor the latest steps t with t > n, results below 1\nindicate task forgetting.\nIn Figure 2, we display the progressive sequen-\ntial learning on the 8 new tasks. We learn a new\ntask, starting from T0, and add to our rehearsal\n6111\n0\n40\n80\n120\n134\n174\n214\n254\n268\n308\n348\n388\n406\n446\n486\n526\n545\n585\n625\n665\n685\n725\n765\n805\n826\n866\n906\n946\n968\n1008\n1048\n1088\n1111\nSteps\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Relative Gain\nT0zs\nASSET\nSimp\nHGen\nHaiku\nCQA\nInqQG\nEmDg\nExp\nTwSt\nFigure 2: Progressive Relative Gain results for CT0 (11B) during the sequential learning(Y axis) vs Number of\nTraining steps(X axis). The curves for tasks T0, ...T7 are displayed respectively at step 0, ..., isuch that only the\nfirst task, Simplification (green and orange) is present at step 0, then HGen (red) etc.\nbuffer 1% of the data of the learned task. We ob-\nserve an improvement progressively for each task,\nthat is our model keeps learning new tasks. At the\nsame time, the performance is preserved for the\nother tasks, (i.e., the Relative gain remains around\n1) indicating the success of our CLR method in a\nsequential learning setup through more than 1000\ngradient steps over 8 different tasks.\nIn Table 3, we report the results on all the 8 new\ntasks as well as T0tr and T0zs (see Section 3.3),\ncorresponding respectively to the evaluation sets\nof the 50 training datasets used in T0, and the 12\ndatasets kept apart for the zero-shot evaluation. In\nthe first bloc of Table 3, we observe the starting\nperformance of our two initial checkpoints, T0_3B\nand T0pp(11B). The second bloc corresponds to\ntheir respective Upper Bounds. We report the re-\nsults for our models after training them progres-\nsively on the 8 new tasks, as well as the baseline\nLAMOL (see Section 2; for fair comparison we\nadapted LAMOL initialising it with T03B, addi-\ntional details can be found in Appendix 7). The\nCT03B and CT0pp results in Table 3 are reported\nafter the model was fine-tuned on the latest task in\nthe sequence (intermediary steps are given in Table\n7 in Appendix).\nOur two CT0 models obtain final results very\nclose to their UB, maintaining 99.8% for T0pp and\n98.0% for T0_3B. This clearly indicates the effi-\nciency of the CLR method. Notably, no task suffers\na decrease in performance more than 2% for T0pp.\nTable 3 shows how the CT0 model remembers and\nretains knowledge from tasks trained at very early\nstages of the Continual Learning process. More-\nover, CT0 still performs well on the zero-shot set\nof tasks (T0zs) despite no rehearsal for those.\nIt should also be noted that the T0pp model fails\nto generalize for most NLG tasks, as opposed to\nour CT0 model. For instance Table 6 in Appendix\nshows it can generate a haiku that has a perfect syl-\nlable count of 17 given an unseen topic of ‘moun-\ntain winds haunt’. It can also generate reasonable\nnatural language explanations that often comply\nwith our commonsense. Moreover, CT0 obtains a\nnew state-of-the-art on the ASSET evaluation set,\nimproving over MUSS (Martin et al., 2020): 85.9\nBLEU4 Vs 72.98 and 46.6 SARI Vs 44.15, and\ndespite not using all the training data available.\nIn contrast to Continual Learning with rehearsal,\nLAMOL clearly diverges from its UB (T03B) indi-\ncating catastrophic forgetting. While LAMOL was\nknown to perform well mostly on NLU tasks, we\nhypothesise that the generative nature for our tasks\nis not suited for the method. Finally, Continual\nLearning with rehearsal approach is task order\ninvariant as demonstrated by revfinal results: rev-\nfinal corresponds to CT03B trained on the 8 tasks\nwithin in the reverse order 5. We give more details\n5We report task order invariance results only using 3B and\n6112\nabout the order choice in the Appendix.\n4.3 Zero-shot Instruction Compositionality\nOur CT0 model has learned effectively to process\ndifferent instructions in specific contexts: word\nlevel constraint in the context of headline genera-\ntion, or an emotional tone in the context of dialogue.\nDoes CT0 understand these instructions in different\ncontexts? To answer this question, and to explore\nwhether CT0 can learn instruction compositionality\nwe conduct several experiments.\nZero-Shot Constraint. In Table 4 we explore\nhow our model succeeds in understanding con-\nstraint instructions beyond the one it was exposed\nduring training. Our model was trained on Head-\nline Generation with Constraint (HGen) instruc-\ntions with only one match, such as Make a title for\nthis article containing “X”. To test generalization,\nwe prompt our CT0 model with unseen instructions\nwith 2 and 3 matches, such as Make a title for this\narticle containing “X” and “Y\" , or Make a title\nfor this article containing “X” and “Y\" and “Z\" .\nWe also compose instructions from constraint and\nTwitter Stylometry resulting in instructions such as\nWrite a tweet about X, in the style of Y, containing Z.\nCT0 respects the Contain constraint 77% for n = 1.\nThe score naturally drops when n > 1, however\nthe satisfiablity is still 50% of the time for n = 2\nand 40% for n = 3. As expected, the ROUGE-\n1 score also improves: NoCons: 30.2, #Cons=1:\n38.9, #Cons=2: 43.9 and #Cons=3: 47.4. When\nwe compose HGen and TwSt, CT0 also performs\nsignificantly better compared to CT 0NoCons (46.4\nvs. 10.7).\nZero-Shot Emotional Haiku. We explore\nwhether combining an emotion with the Haiku\ninstructions would help control the haiku gener-\nation. Note that during training, only the task\nof Empathetic Dialogue has been exposed to\nemotion. Our results, reported in Figure 3, indicate\nthat CT0 is able to combine an emotion with the\nHaiku instructions in a zero-shot setting. For\ninstance, given the following new instruction\nGenerate a haiku about “held my hand”. The\nassociated emotion is “faithful”. , our model\noutput is “ He held my hand through thick and\nthin, Through sickness and health, through life and\ndeath”. A qualitative analysis also shows that CT0\nunderstands subtle nuances; for instance given as\nnot 11B due to computing restrictions\nsad\nterrified angry lonely trustingcontent faithful grateful\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8% Positive\nFigure 3: Emotion Generalization: Percentage of Haiku\nclassified as positive, when adding emotion specific\nconstraints to the Haiku instruction. We used an open\nsource binary sentiment analysis classifier.6\ninput Generate a haiku about “Seagulls crying\nhigh”. The associated emotion is “nostalgic”. our\nmodel output is “Seagulls crying high, A familiar\nscene, from a childhood Now ”.\nThese are promising results regarding CT0’s abil-\nity to comprehend new instructions, including in-\nstruction composition. While contemporaneous\nwork by Nayak et al. (2022) propose a novel form\nof soft prompting for compositional zero-shot learn-\ning we show that a continually fine-tuned language\nmodel is able to perform the same.\n5 Discussion\n5.1 Why could LLMs be lifelong learners?\nGiven our current experimental protocol, one can\ndraw different hypotheses: is CL a consequence\nemerging from the massive multi-task pre-training\nin T0? Or from the instruction tuning paradigm\nof T0? Or from the scaling law as studied by Ra-\nmasesh et al. (2021)? To answer this research ques-\ntion, we applied the same CL setup starting from\n1) T5-small, 2) T5-3B, and 3) a T5-3B architec-\nture randomly initialised. Our results in Table 5\nshow that CT5 with 3B parameters performs sim-\nilar to CT 03B on the 8 tasks. While CT 5-small\nobtains as expected a lower average performance,\nit still mostly maintains great results w.r.t. its Upper\nBound, indicating that CL does not emerge from\nscale. Conversely, when initialised randomly the\nmodel is not even able to obtain a good UB. These\nresults draw a clear conclusions:CL emerges from\nthe intensive pre-training stage. This confirms\ncontemporaneous findings by Cossu et al. (2022)\n6113\nT0tr T0zs ASSET Simp HGen Haiku CQA InqQG EmDg Exp TwSt\nR1 Acc B4/SARI B4/SARI R1/Cons Hcust BS 1Tok/BS BS BS Clf/BS\nT0_3B 49.8 48.2 70.1/41.0 12.8/41.1 33.6/32.2 34.2 47.6 2.1/58.7 48.6 32.7 54.4/38.0\nT0pp 54.2 65.6 56.5/37.7 11.7/40.1 34.9/35.9 31.6 46.0 2.4/59.8 49.7 37.2 66.4/45.1\nUB_3B 49.8 48.2 79.9/45.2 13.8/44.6 39.7/81.0 62.6 90.0 5.3/63.3 55.7 71.8 74.8/56.5\nUB_pp 54.2 65.6 85.3/46.1 15.0/44.8 41.9/86.9 63.9 90.0 4.9/65.7 56.6 73.5 74.4/57.9\nLamol 32.6 33.6 37.3/12.6 8.4/21.4 22.9/33.5 25.8 46.6 1.8/47.9 45.1 27.6 50.1/35.2\nCT03B 47.9 46.6 78.0/44.5 14.6/43.7 37.3/77.5 60.4 86.8 5.2/61.9 55.3 72.4 74.8/56.5\nCT0pp 53.7 64.4 85.9/46.6 14.6/44.7 40.7/85.5 65.8 89.8 4.8/65.2 56.2 73.0 74.4/57.9\nrevfinal 48.1 48.8 83.3/45.4 14.6/43.9 39.0/81.6 61.2 88.6 4.4/61.9 55.0 72.4 73.2/57.3\nTable 3: Results for the starting checkpoints T0_3B and T0pp(11B), their upper bounds scores and our final models\nas well as LAMOL. Bolded result means there less than 2% forgetting. T0tr and T0zs denote respectively for T0\ntrain-tasks and T0 zero-shot-tasks and are the average accuracy obtained on all their evaluation datasets. B4, R1, BS\ndenote BLEU4, ROUGE-1 and BERTScore. Note that we detail the intermediate steps results in the Appendix.\nHGen TwSt\n# Cons 1 2 3 1\nCT0 77.0 56.4 39.5 46.4\nCT 0NoCons 33.6 15.4 8.1 10.7\nTable 4: Table showing Constraint generalisation i.e\n% of instructions completely respected, when provid-\ning constraints for unseen prompts. CT 0NoCons corre-\nsponds to providing the same input without constrain.\nand Mehta et al. (2021) in other setups and even\nmodalities. We report the detailed results for those\nexperiments in the Appendix.\n5.2 Toward Concept Drift\nIn the original CovidQA the task consists of an-\nswering a question present in a given paragraph. In\nthis setup, one can arguably succeed into answer-\ning questions about COVID by transferring the task\nknowledge, even without particular domain knowl-\nedge about COVID. In our paper, we intentionally\nchose to not provide the context for CQA but only\nthe question. This alternative setup corresponds\nto learning by heart the answer to a question. Our\nresults in Table 3 show that while we framed CQA\nas a new task to learn, our proposed setup also\nopens new way to tackle concept drift, by directly\nincorporating knowledge into a model.\n5.3 Data Efficiency\nOur method based on rehearsal learning is simple\nyet efficient. While the complexity in term of data\nstorage and training is not constant (O(1)), with\nonly 1% of the previous training data we are able\nto retain model abilities. This result is still data and\ncomputationally efficient, compared to the standard\napproach of retraining the model from scratch on\nall tasks. In cases where the number of tasks to\nlearn would grow by several order of magnitude,\nmore sophisticated methods could be explored. We\nleave this for future research.\n6 Conclusion\nWe explored for the first time Continual Learning\nfor instruction-based models. Our results indicate\nthat fine-tuned Language Models are efficient con-\ntinual learners: 1% rehearsal is enough to maintain\na high performance on previously learned tasks,\nwhile learning new ones. Additionally, we show\nthat our model CT0 is able to comprehend new\ninstructions obtained via instruction composition.\nThe current technique to learn multiple tasks is to\ntrain a model from scratch. We hope this work\npaves the way toward a new paradigm where mod-\nels do not have to be retrained all over again. We\nbelieve our experimental findings will contribute\nto the effectiveness of large language models, en-\nabling them to progressively adapt to new concepts\nand acquire more and more abilities. As an analogy\nwith Software Development, this could be seen as\nlearning new features. New checkpoints are like\nnew versions of a model. In this context, Continual\nLearning will help toward the Call to Build Models\nLike We Build Open-Source Software.7\n7https://tinyurl.com/3b7b2nrc\n6114\nT0tr T0zs ASSET Simp HGen Haiku CQA InqQG EmDg Exp TwSt\nR1 Acc B4/SARI B4/SARI R1/Cons Hcust BS 1Tok/BS BS BS Clf/BS\nUB_rand N/A N/A 0.5/24.3 0.0/29.6 1.5/0.1 9.6 25.2 1.2/25.4 36.3 33.1 24.7\nUB_T5small N/A N/A 87.8/45.9 15.6/43.2 35.3/67.8 53.4 54.1 3.4/57.0 51.3 33.8 52.4/54.6\nUB_T53b N/A N/A 87.0/45.6 15.4/43.7 33.0/89.4 63.0 89.9 2.92/61.5 55.3 71.6 75.6/55.4\nUB_T0 49.8 48.2 79.9/45.2 13.8/44.6 39.7/81.0 62.6 90.0 5.3/63.3 55.7 71.8 74.8/56.5\nCTrand N/A N/A 0.0/22.9 0.0/28.5 0.2/0.0 9.6 25.2 1.2/27.9 28.1 30.7 24.7\nCT5small N/A N/A 85.5/45.8 15.0/42.8 34.6/64.8 51.8 49.5 3.3/56.0 51.2 32.3 52.4/54.6\nCT53B N/A N/A 84.6/45.8 14.8/44.0 38.3/88.3 62.3 85.8 4.64/62.1 55.5 73.1 75.6/55.4\nCT03B 47.9 46.6 78.0/44.5 14.6/43.7 37.3/77.5 60.4 86.8 5.2/61.9 55.3 72.4 74.8/56.5\nTable 5: Results including T5-small and T5-3B, T0_3B, and a 3B Transformer randomly initialised. We can observe\nthat 1) only CTrand largely degrades w.r.t. its UB, UB_rand; 2) even T5_small is able to mostly maintain its\nperformance indicating that scale is not what matter the most.\nInstr\nMake a title for this article, finishing with\n\"escalates\": the sri lankan government\nannounced the closure of government\nschools with immediate effect as a military\ncampaign against tamil separatists escalated\nin the north of the country .\nCT0 sri lanka closes schools as war with\ntamils escalates\nT0pp sri lanka closes schools as tamil\nrebels advance\nInstr Write a haiku about ‘mountain winds\nhaunt’\nCT0 mountain winds haunt, the hollow of the\nstones, voices echo there.\nT0pp a lone tree in the mountains is haunted by\nthe wind\nInstr\nExplain why the two following sentences\ndo not entail each other: \"Sentence 1: A\nwoman with a green headscarf, blue shirt\nand a very big grin.\"; Sentence 2:\"The\nwoman has been shot.\"\nCT0 A woman cannot be smiling if she has\nbeen shot.\nT0pp No\nTable 6: Outputs for HGen, Haiku and Exp from T0pp\nand our continually learned final model CT0.\nAcknowledgements\nWe would like to thank the anonymous reviewers\nfor their helpful comments. Tuhin is funded by\nColumbia Center of Artifical Intelligence & Tech-\nnology (CAIT) and the Amazon Science Ph.D. Fel-\nlowship).\nLimitations\nAs discussed in 5.3, CL with rehearsal still requires\nto use a buffer of data previously seen which limits\nseveral scenarios where those data would not be\navailable anymore. While we have done our best\nto select numerous and diverse tasks in this paper,\nit still represents a limited set. Would our results\nstill hold given hundred or thousand tasks? In other\nmodalities? It should also be noted that our study\nis limited to English-only datasets, as we started\nfrom T0 which is not multilingual in nature. Addi-\ntionally while results using automatic metrics give\na fair idea of task performance and measuring CL\nabilities, we would like to conduct a human evalua-\ntion in near future although its expensive give the\nsize of test data and the number of tasks\nEthics Statement\nAlthough we use language models trained on data\ncollected from the Web, which have been shown to\nhave issues with gender bias and abusive language,\nthe inductive bias of our models should limit in-\nadvertent negative impacts. Unlike model variants\nsuch as GPT, T5 is a conditional language model,\nwhich provides more control of the generated out-\nput. We have verified carefully that our training\nor evaluation data does not contain any toxic text\nand it underwent manual inspection by the authors\nand experts. We also believe our work in contin-\nual learning is a step towards data efficiency and\nconservation of computing resources, as one saves\ntraining time by only using 1% rehearsal\nReferences\nFernando Alva-Manchego, Louis Martin, Antoine Bor-\ndes, Carolina Scarton, Benoît Sagot, and Lucia Spe-\ncia. 2020. ASSET: A dataset for tuning and evalua-\ntion of sentence simplification models with multiple\nrewriting transformations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\n6115\nLinguistics, pages 4668–4679, Online. Association\nfor Computational Linguistics.\nMagdalena Biesialska, Katarzyna Biesialska, and\nMarta R. Costa-jussà. 2020. Continual lifelong learn-\ning in natural language processing: A survey. In\nProceedings of the 28th International Conference\non Computational Linguistics , pages 6523–6541,\nBarcelona, Spain (Online). International Committee\non Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nOana-Maria Camburu, Tim Rocktäschel, Thomas\nLukasiewicz, and Phil Blunsom. 2018. e-snli: Natu-\nral language inference with natural language explana-\ntions. In Advances in Neural Information Processing\nSystems, volume 31. Curran Associates, Inc.\nYue Cao, Hao-Ran Wei, Boxing Chen, and Xiaojun\nWan. 2021. Continual learning for neural machine\ntranslation. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 3964–3974, Online. Association\nfor Computational Linguistics.\nAndrea Cossu, Tinne Tuytelaars, Antonio Carta, Lu-\ncia Passaro, Vincenzo Lomonaco, and Davide Bac-\nciu. 2022. Continual pre-training mitigates for-\ngetting in language and vision. arXiv preprint\narXiv:2205.09357.\nCyprien de Masson D’Autume, Sebastian Ruder, Ling-\npeng Kong, and Dani Yogatama. 2019. Episodic\nmemory in lifelong language learning. Advances in\nNeural Information Processing Systems, 32.\nArthur Douillard, Alexandre Ramé, Guillaume Coua-\niron, and Matthieu Cord. 2021. Dytox: Transformers\nfor continual learning with dynamic token expansion.\narXiv preprint arXiv:2111.11326.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\nier, Jason Weston, and Michael Auli. 2019. ELI5:\nLong form question answering. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 3558–3567, Florence,\nItaly. Association for Computational Linguistics.\nRobert M French. 1999. Catastrophic forgetting in con-\nnectionist networks. Trends in cognitive sciences ,\n3(4):128–135.\nChao Jiang, Mounica Maddela, Wuwei Lan, Yang\nZhong, and Wei Xu. 2020. Neural CRF model for\nsentence alignment in text simplification. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, ACL 2020, Online,\nJuly 5-10, 2020, pages 7943–7960. Association for\nComputational Linguistics.\nXisen Jin, Junyi Du, Arka Sadhu, Ram Nevatia, and\nXiang Ren. 2020. Visually grounded continual learn-\ning of compositional phrases. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 2018–2029,\nOnline. Association for Computational Linguistics.\nXisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao,\nShang-Wen Li, Xiaokai Wei, Andrew Arnold, and\nXiang Ren. 2021. Lifelong pretraining: Continu-\nally adapting language models to emerging corpora.\narXiv preprint arXiv:2110.08534.\nZixuan Ke, Hu Xu, and Bing Liu. 2021. Adapting BERT\nfor continual learning of a sequence of aspect senti-\nment classification tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 4746–4755, Online.\nAssociation for Computational Linguistics.\nBill Yuchen Lin, Sida Wang, Xi Lin, Robin Jia, Lin\nXiao, Xiang Ren, and Scott Yih. 2022. On continual\nmodel refinement in out-of-distribution data streams.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3128–3139, Dublin, Ireland.\nAssociation for Computational Linguistics.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nVincenzo Lomonaco and Davide Maltoni. 2017.\nCore50: a new dataset and benchmark for contin-\nuous object recognition. In Conference on Robot\nLearning, pages 17–26. PMLR.\nAndrea Madotto, Zhaojiang Lin, Zhenpeng Zhou, Se-\nungwhan Moon, Paul Crook, Bing Liu, Zhou Yu, Eu-\nnjoon Cho, Pascale Fung, and Zhiguang Wang. 2021.\nContinual learning in task-oriented dialogue systems.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n7452–7467, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nLouis Martin, Angela Fan, Éric de la Clergerie, Antoine\nBordes, and Benoît Sagot. 2020. Muss: multilin-\ngual unsupervised sentence simplification by mining\nparaphrases. arXiv preprint arXiv:2005.00352.\nSanket Vaibhav Mehta, Darshan Patil, Sarath Chandar,\nand Emma Strubell. 2021. An empirical investigation\nof the role of pre-training in lifelong learning. arXiv\npreprint arXiv:2112.09153.\nFei Mi, Liangwei Chen, Mengjie Zhao, Minlie Huang,\nand Boi Faltings. 2020. Continual learning for natu-\nral language generation in task-oriented dialog sys-\ntems. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 3461–3474,\nOnline. Association for Computational Linguistics.\n6116\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2022. Cross-task generaliza-\ntion via natural language crowdsourcing instructions.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3470–3487, Dublin, Ireland.\nAssociation for Computational Linguistics.\nTimo Möller, Anthony Reina, Raghavan Jayakumar,\nand Malte Pietsch. 2020. COVID-QA: A question\nanswering dataset for COVID-19. In Proceedings of\nthe 1st Workshop on NLP for COVID-19 at ACL 2020,\nOnline. Association for Computational Linguistics.\nNihal V Nayak, Peilin Yu, and Stephen H Bach. 2022.\nLearning to compose soft prompts for compositional\nzero-shot learning. arXiv preprint arXiv:2204.03574.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: A method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting on Association for Computa-\ntional Linguistics, ACL ’02, pages 311–318, Philadel-\nphia, Pennsylvania. ACL.\nGerman I Parisi, Ronald Kemker, Jose L Part, Christo-\npher Kanan, and Stefan Wermter. 2019. Continual\nlifelong learning with neural networks: A review.\nNeural Networks, 113:54–71.\nFabian Pedregosa, Gaël Varoquaux, Alexandre Gram-\nfort, Vincent Michel, Bertrand Thirion, Olivier Grisel,\nMathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-\ncent Dubourg, et al. 2011. Scikit-learn: Machine\nlearning in python. the Journal of machine Learning\nresearch, 12:2825–2830.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research, 21:1–\n67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nVinay Venkatesh Ramasesh, Aitor Lewkowycz, and\nEthan Dyer. 2021. Effect of scale on catastrophic\nforgetting in neural networks. In International Con-\nference on Learning Representations.\nHannah Rashkin, Eric Michael Smith, Margaret Li, and\nY-Lan Boureau. 2019. Towards empathetic open-\ndomain conversation models: A new benchmark and\ndataset. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5370–5381, Florence, Italy. Association for\nComputational Linguistics.\nSylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg\nSperl, and Christoph H Lampert. 2017. icarl: In-\ncremental classifier and representation learning. In\nProceedings of the IEEE conference on Computer\nVision and Pattern Recognition, pages 2001–2010.\nArij Riabi, Thomas Scialom, Rachel Keraron, Benoît\nSagot, Djamé Seddah, and Jacopo Staiano. 2021.\nSynthetic data augmentation for zero-shot cross-\nlingual question answering. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 7016–7030.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\net al. 2022. Multitask prompted training enables zero-\nshot task generalization. In International Conference\non Learning Representations.\nThomas Scialom and Jacopo Staiano. 2020. Ask to\nlearn: A study on curiosity-driven question genera-\ntion. In Proceedings of the 28th International Con-\nference on Computational Linguistics, pages 2224–\n2235.\nHanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon\nKim. 2017. Continual learning with deep generative\nreplay. Advances in neural information processing\nsystems, 30.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, et al. 2022. Using deep-\nspeed and megatron to train megatron-turing nlg\n530b, a large-scale generative language model. arXiv\npreprint arXiv:2201.11990.\nTejas Srinivasan, Ting-Yun Chang, Leticia Leonor Pinto\nAlva, Georgios Chochlakis, Mohammad Rostami,\nand Jesse Thomason. 2022. Climb: A continual\nlearning benchmark for vision-and-language tasks.\narXiv preprint arXiv:2206.09059.\nFan-Keng Sun, Cheng-Hao Ho, and Hung-Yi Lee. 2019.\nLamol: Language modeling for lifelong language\nlearning. In International Conference on Learning\nRepresentations.\nBin Tareaf. 2017. R.: Tweets dataset-top 20 most fol-\nlowed users in twitter social platform. Harvard Data-\nverse, 2.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2022. Finetuned lan-\nguage models are zero-shot learners. In International\nConference on Learning Representations.\nWei Xu, Courtney Napoles, Ellie Pavlick, Quanze Chen,\nand Chris Callison-Burch. 2016. Optimizing Sta-\ntistical Machine Translation for Text Simplification.\n6117\nTransactions of the Association for Computational\nLinguistics, 4:401–415.\nWenpeng Yin, Jia Li, and Caiming Xiong. 2022. Con-\nTinTin: Continual learning from task instructions.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3062–3072, Dublin, Ireland.\nAssociation for Computational Linguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\n6118\n7 Appendix\n7.1 Tasks Order\nThe task order has been selected 1) randomly\namong the three first tasks Text Simplifiction, Head-\nline Generation with Constraint and Haiku Gen-\neration, and 2) in light of the actual success, we\nprogressively kept adding new tasks. This setup\ncorresponds to a realistic usage of our proposed\nmethod, where future tasks were thus unknown\neven for us. To assess a potential impact of the\norder, we also conduct an alternative experiment\nwith our 3B model, where the order is reversed. We\ndid not experimented further different orders due\nto the high computation required.\n7.2 Tasks\nIn this section, we describe all the tasks T used\nto progressively train and evaluate our model. For\nall the new tasks (i.e., not the T0 tasks), we also\ndesigned instructions, as illustrated in Table 2.\n7.3 Automatic Metrics\n7.3.1 New Tasks\nAll of our newly introduced tasks are language gen-\neration tasks in contrast to the T0 evaluation tasks\nand majority of the T0 training tasks (all except\nsummarization).\nText Simplification (Simpl) Jiang et al. (2020)\nprovided WikiAuto, a set of 400,000 aligned sen-\ntences from English Wikipedia and Simple English\nWikipedia as a resource to train sentence simpli-\nfication systems. The test set contains 4,000 ex-\namples. In addition, we also evaluate our models\non a second Text Simplification dataset, ASSET\n(Alva-Manchego et al., 2020). This is a dataset ded-\nicated for the evaluation of sentence simplification\nin English, providing 2,000 multiple references per\nexample, unlike previous simplification datasets.\nTable 2 shows our designed instructions for this\ntask.\nHeadline Generation with Constraint (HGen).\nWhile writing a title for a news article, it can be\nvery useful to add additional constraints, such as\nthe presence of certain words. However, traditional\ndecoding strategies like the BeamSearch often fail\nto achieve this goal as discussed in 4. Gigaword is\none of T0 training dataset. Our new task consists\nof generating a title given a news article with addi-\ntional constraints. Towards this goal, for a given\ndocument D and an input keyword X we design the\nfollowing three instructions: [Make a title for this\narticle, starting with/ ending with/ that contains\n“X” : D where X is a word we want to be present\nin the output text at the beginning/end/anywhere,\nand D the source document, as illustrated in Table\n2. To create the training data, we simply leverage\nthe gold-reference to select the word X, such that\nour model is trained with consistent and plausible\ninstructions. Gigaword contains millions of train-\ning examples. The original test set is composed of\n1,951 examples, so we convert it to 3 sets of 1,951\nexamples for our Start/End/Contain instructions,\nrespectively.\nHaiku Generation (Haiku). For the task of\nhaiku generation, we crawl 8 10,718 haikus with\nat least 1 up-vote from the Subreddit haiku, 9 and\nsplit it in 9,742 and 974 example for the train and\ntest sets, respectively. Table 2 shows an example in-\nstruction for Haiku Generation about a given topic.\nCovid QA (CQA) Möller et al. (2020) created\nCOVID-QA, a Question Answering dataset con-\nsisting of 2,019 question/answer pairs annotated\nby volunteer biomedical experts on scientific arti-\ncles related to COVID-19. We consider this dataset\nsince to the best of our knowledge, T0 has never\nbeen exposed to any COVID-19 related data. In its\noriginal version, the dataset is framed as SQuAD\n(Rajpurkar et al., 2016), with triplets (context, ques-\ntion, answer), where the context contains the an-\nswer. Because T0 has been extensively trained on\nQA dataset, CovidQA in its original format simply\nrequires domain transfer. To make the task more\nchallenging, we propose to provide only the ques-\ntion as an input, now framing the task as “learn\nthe answer by heart” in an encyclopedia style task.\nThis way the task framing can be seen as a new\nstrategy to incorporating knowledge and prevent-\ning the model from concept drift.\nInquisitive Question Generation (InqQG) To\nfoster long form question answering Fan et al.\n(2019) created the ELI5 dataset that comprises\n270,000 English-language threads from the Reddit\nforum of the same name, 10 where an online com-\nmunity provides answers to several open ended\ninquisitive questions. Table 2 shows an example\ninstruction in order to generate inquisitive ques-\n8The crawling part was done by Tuhin Chakrabarty and at\nColumbia.\n9https://www.reddit.com/r/haiku/\n10https://www.reddit.com/r/ExplainLikeImfive/\n6119\ntions. As opposed to standard Question Generation\nbased on SQuAD, ELI5 enables open-ended ques-\ntions, closer to human-style questions (Scialom and\nStaiano, 2020). We filtered out the Reddit threads\nto keep only well formed questions,11 resulting in\n61,710 and 1,681 examples for the training and test\nset, respectively.\nEmpathetic Dialogue Generation (EmDg)\nRashkin et al. (2019) proposed a benchmark for\nempathetic dialogue generation by creating a\ndataset of conversations grounded in emotional\nsituations. Each example in the dataset contains an\ninput emotion, situation in which dialogue appears\nand the entire conversation. We display in Table\n2 the corresponding instruction. At the example\nlevel, our training and test datasets contain 58,770\nand 8,396 examples, respectively.\nExplanation Generation (Exp). The Stanford\nNatural Language Inference dataset consists of a\nclassification task, where given a Premise(P) and\nan Hypothesis(H), the model has to chose between\n3 options: entailed, contradiction or not related.\nCamburu et al. (2018) extend this NLI dataset by\nannotating the explanations of the label in natural\nlanguage. In our paper, we consider as input the\nPremise(P), the Hypothesis(H), and the label, and\ntrain our model to generate the explanation. The\ndataset is composed of 100,000 and 9,824 train and\ntest examples, respectively.\nTwitter Stylometry (TwSt) Tareaf (2017) ex-\ntracted tweets from the top 20 most followed users\nin Twitter social platform, including singers such as\nKaty Perry or Selena Gomez, as well as the official\naccount of Barack Obama when he was president of\nthe USA. The style for tweets largely differs from\none account to an another, e.g. @BarackObama:\n“It’s time to #ActOnClimate” vs. @KimKardashian:\n“makes me want to go back blonde but i’m scared\nit will ruin my hair :-( ”. We define the Stylome-\ntry task as generating a relevant tweet given i) a\nhashtag, and ii) the tweet’s author. We thus se-\nlected only tweets containing hashtags (#) from the\noriginal dataset, resulting in a total of 13,041 and\n250 examples for train and test sets, respectively.\nWe display at the bottom of Table 2 an example\ninstruction for this task.\n11I.e, starting in “W” or “H” and finishing with a ques-\ntion mark. See the code for the exact implementation, class\nELI5promptFormat in data_handler.py.\n7.4 Automatic Metrics\nT0 zero-shot evaluation set (see Section 3.3) only\ncontains tasks framed as classification. For T0\nevaluation, Sanh et al. (2022) compute the loglike-\nlihood of each of the target options, and the option\nwith the highest log-likelihood is selected as the\nprediction. This strategy holds when restricting\nthe evaluation to classification tasks. However, in\nthe context of an open-ended model able to per-\nform NLG tasks, a user is interested in the actual\noutput of the model rather than probabilities. We\ntherefore report the accuracy of the prediction com-\npared to the ground-truth answer for all those tasks.\nThis measure is more conservative, as it requires\nan exact match.\nIn the context of Continual Learning, we also\nsuspect that using only a comparison of the log-\nlikelihood of respective classes would not reflect\nthe actual model’s memory, since the decoders are\nknown to suffer from catastrophic forgetting more\nthan the encoders (Riabi et al., 2021).\nStandard NLG Metrics. For the standard tasks,\nwe rely on widely used metrics: ROUGE (Lin,\n2004) for Summarization; BLEU (Papineni et al.,\n2002) and SARI (Xu et al., 2016) for Simplifica-\ntion. In this paper, we also include open-domain\nNLG tasks, such as Dialogue or Explanation gen-\neration. The space of possible correct outputs is\ntoo large in this case to rely on n-gram based met-\nrics like BLEU or ROUGE. For this reason, we\nreport BERTScore (Zhang et al., 2020) to measure\nthe similarity between a prediction and its gold-\nreference in those tasks.12\nWhen possible, we also designed customized\nmetrics that are better suited for the task.13\nCustomized NLG Metrics.\n• Constraint: For our prompts with constraint,\nsuch as “Write a text that starts/contains/ends\nwith [some word]”, we also report the accuracy\nof respecting the constraint. Concretely, an out-\nput is correct only if it contains the [word] at the\nright location: the beginning for start, the end for\nend; any location for contain.\n• First Word Distribution (1Tok).In ELI5, the ques-\ntions are supposed to be inquisitive, not factual\nlike in SQuAD. Therefore, the distribution of the\n12We used BERTScore based ondeberta-mnli that is shown\nto have high correlation with human judgements.\n13All those metrics implementations are available in the\npublicly released code.\n6120\nfirst words is very informative. For instance, the\npercentage of questions starting with “why/how”\nis more important than “what”. We therefore rely\non the Jensen Shannon Divergence between the\nfirst words distributions of the ground truth ex-\namples and our predictions. We report its inverse,\nso the higher the better.\n• Author Classification (Clf) In Twitter Stylome-\ntry, the author is part of the input, so the gener-\nated tweet is aligned with the author’s style. To\nmeasure this condition, we train a classifier on\nthe dataset, with the tweets as inputs, and the\ncorresponding author names as target categories.\nWe trained a Ridge Classifier using scikit-learn\n(Pedregosa et al., 2011), and obtained 0.81% ac-\ncuracy. This high accuracy allows this Clf metric\nto be informative enough.\n• Hcust Haiku is a type of short form poetry orig-\ninally from Japan as illustrated in the Table 2.\nIn general, it contains only 17 syllables, broken\nup into three lines. We calculate two differences\nbetween the prediction and the ground-truth: i)\nfor the number of lines, and ii) for the number\nof syllables. Hcust corresponds to the average of\nthese two differences, BLEU and the Constraint\nsatisfiability (i.e., if the generated haiku contains\nthe topic phrase X that was present in the instruc-\ntion).\n7.5 Evaluation for T0 Train Set\nBecause there are 50 datasets with thousands of ex-\namples in the test sets per task, evaluating on each\nexamples would be computationally intensive. For\nthis reason we restricted this set to 1000 examples\nrandomly sampled from all the examples in the test\nsets. Because the set contains both NLG and NLU\ntasks, using the accuracy is not enough. For sim-\nplicity we used therefore ROUGE-1 which allows\nis consistent with accuracy for NLU tasks but also\nallows to take into account NLG evaluation,\n7.6 Additional Results\nIn the main paper, Table 5 we reported the addi-\ntional results when starting from T5 and a random\ntransformer. These results are discussed in the first\nsection of our Discussion.\nIn Table 7 we report the progressive results, and\nnot just the initial checkpoint, the Upper Bound\nand the final model.\n7.7 Implementation Details\nFor all our experiments woth T0_3B and T0pp,\nwe instantiate our model with the T0 model (Sanh\net al., 2022) using the official implementation. 14\nFor fine-tuning T0_3B, we used the same hyper-\nparameters as the ones reported in Sanh et al.\n(2022): all the details from the batch-size to the\nlearning rate are provided in details here. 15\nThe only new hyper-parameter introduced in our\npaper is the rehearsal proportion r. We explored\nr ∈ [0, 0.25%, 1%] as reported in our first set of\nresults.\nFor each task, we consider 100,000 examples\nfor training, such that 1% rehearsal corresponds to\n1,000 examples from the memory buffer. Thus, for\ndatasets with fewer training examples, we upsam-\nple them and conversely for largest datasets like\nGigaword or Simplification, we limit to 100,000\nexamples.\nWhen we scaled our best setup to the 11B param-\neters version of T0, T0pp, we observed instability\nin validation performance. Thus, we changed the\nlearning rate from 1e-3 to 1e-4 as well as the opti-\nmizer to AdamW instead of Adafactor for all our\n11B experiments. All the other hyper-parameters\nremain similar to the 3B model.\nFor the T5 ablations, we again used the Hug-\nging Face implementations 16 and applied the same\nhyper-parameters as above.\nAt inference time, we use greedy decoding, i.e.\na Beam Search with K = 1.\n14https://huggingface.co/bigscience/T0pp\n15https://huggingface.co/bigscience/T0pp\n16https://huggingface.co/t5-3b\n6121\nT0zs ASSET Simp HGen Haiku CQA InqQG EmDg Exp TwSt\nAcc B4/SARI B4/SARI R1/Cons Hcust BS 1Tok/BS BS BS Clf/BS\nT0_3B 48.2 70.1/41.0 12.8/41.1 33.6/32.2 34.2 47.6 2.1/58.7 48.6 32.7 54.4/38.0\nT0pp (11B) 65.6 56.5/37.7 11.7/40.1 34.9/35.9 31.6 46.0 2.4/59.8 49.7 37.2 66.4/45.1\n+Simp 3B 48.9 79.9/45.2 13.8/44.6 30.3/31.0 30.9 43.9 2.0/56.1 40.2 34.9 50.8/42.5\n+Simp 11B 66.7 85.3/46.1 15.0/44.8 34.9/36.1 33.0 47.2 2.1/59.0 48.1 39.2 68.8/47.6\n+HGen 3B 46.9 81.4/44.9 14.1/43.9 39.7/81.0 33.7 44.2 2.5/55.9 45.9 55.2 19.6/37.3\n+HGen 11B 65.5 84.5/46.1 15.3/44.8 41.9/86.9 35.9 46.6 2.9/59.7 48.9 36.4 69.6/48.1\n+Haiku 3B 48.8 81.6/45.0 14.6/43.9 39.0/78.2 62.6 43.0 2.3/54.9 47.2 39.0 65.6/44.5\n+Haiku 11B 64.6 83.5/46.1 14.9/45.1 41.1/83.0 63.9 46.0 2.9/59.9 48.9 37.5 66.4/46.2\n+CQA 3B 48.5 79.7/44.4 14.0/43.8 37.6/75.4 62.2 90.0 2.0/54.4 42.5 38.7 66.4/45.3\n+CQA 11B 64.6 84.3/46.1 14.5/44.9 40.9/83.7 63.6 90.0 2.9/59.2 48.5 42.7 67.2/47.3\n+InqQG 3B 47.4 65.2/41.2 14.6/43.8 37.9/77.7 60.4 89.6 5.3/63.3 46.8 34.2 59.2/45.4\n+InqQG 11B 65.5 85.5/46.3 14.9/44.8 40.6/81.7 64.5 89.9 4.9/65.7 49.2 47.7 61.2/45.9\n+EmDg 3B 48.6 73.9/43.8 15.0/43.7 38.0/77.7 62.9 88.6 4.7/62.7 55.7 35.2 53.6/42.7\n+EmDg 11B 66.4 85.3/46.3 15.1/44.7 40.9/84.1 65.0 89.9 5.3/65.5 56.6 37.0 61.6/45.8\n+Exp 3B 47.4 74.6/44.0 14.2/43.5 37.9/80.9 60.9 86.5 4.9/62.3 55.2 71.8 54.8/43.4\n+Exp 11B 65.0 85.6/46.5 14.9/44.7 40.7/84.6 64.5 89.8 4.8/65.5 56.5 73.5 63.6/46.3\n+TwSt 3B 46.6 78.0/44.5 14.6/43.7 37.3/77.5 60.4 86.8 5.2/61.9 55.3 72.4 74.8/56.5\n+TwSt 11B 64.4 85.9/46.6 14.6/44.7 40.7/85.5 65.8 89.8 4.8/65.2 56.2 73.0 74.4/57.9\nrev_final 48.8 83.3/45.4 14.6/43.9 39.0/81.6 61.2 88.6 4.4/61.9 55.0 72.4 73.2/57.3\nTable 7: Progressive results for T0 3B and 11B results for continual training set up with best 3B results underlined\n& best 11B results bolded. T0zs denotes T0 zero-shot and is the average accuracy obtained on 12 eval datasets. B4,\nR1, BS denote BLEU-4, ROUGE-1 and BERTScore.\n6122",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.842188835144043
    },
    {
      "name": "Forgetting",
      "score": 0.6457687616348267
    },
    {
      "name": "Language model",
      "score": 0.580436110496521
    },
    {
      "name": "Natural language",
      "score": 0.5607260465621948
    },
    {
      "name": "Principle of compositionality",
      "score": 0.5525821447372437
    },
    {
      "name": "Natural language understanding",
      "score": 0.5473626852035522
    },
    {
      "name": "Artificial intelligence",
      "score": 0.530765950679779
    },
    {
      "name": "Intuition",
      "score": 0.45290476083755493
    },
    {
      "name": "Natural language processing",
      "score": 0.39229294657707214
    },
    {
      "name": "Cognitive psychology",
      "score": 0.18888789415359497
    },
    {
      "name": "Cognitive science",
      "score": 0.14821654558181763
    },
    {
      "name": "Psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I78577930",
      "name": "Columbia University",
      "country": "US"
    }
  ],
  "cited_by": 39
}