{
  "title": "Point Cloud Learning with Transformer",
  "url": "https://openalex.org/W4287197011",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1976790177",
      "name": "Qi Zhong",
      "affiliations": [
        "Southwest University"
      ]
    },
    {
      "id": "https://openalex.org/A4212758541",
      "name": "Xian-Feng Han",
      "affiliations": [
        "Southwest University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1676314349",
    "https://openalex.org/W2553307952",
    "https://openalex.org/W6600648412",
    "https://openalex.org/W6600339963",
    "https://openalex.org/W6600609735",
    "https://openalex.org/W4252907012",
    "https://openalex.org/W4234552385",
    "https://openalex.org/W6600595061",
    "https://openalex.org/W6635231071",
    "https://openalex.org/W3163925244",
    "https://openalex.org/W3157424380",
    "https://openalex.org/W3034317823",
    "https://openalex.org/W2963719584",
    "https://openalex.org/W2948107928",
    "https://openalex.org/W3039448353",
    "https://openalex.org/W3118806719",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W4394652702",
    "https://openalex.org/W2805479641",
    "https://openalex.org/W3035429765",
    "https://openalex.org/W3014902535",
    "https://openalex.org/W2963053547",
    "https://openalex.org/W3035398346",
    "https://openalex.org/W3155390614",
    "https://openalex.org/W2796426482",
    "https://openalex.org/W3106728613",
    "https://openalex.org/W3034584726",
    "https://openalex.org/W2798777114",
    "https://openalex.org/W3034482224",
    "https://openalex.org/W3171433839",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2953823434",
    "https://openalex.org/W2798270772",
    "https://openalex.org/W2963830382",
    "https://openalex.org/W2960986959",
    "https://openalex.org/W2953399169",
    "https://openalex.org/W3034664537",
    "https://openalex.org/W2998719065",
    "https://openalex.org/W2963231572",
    "https://openalex.org/W2211722331",
    "https://openalex.org/W2810240468",
    "https://openalex.org/W3012494314",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W2556802233",
    "https://openalex.org/W2981199548",
    "https://openalex.org/W2963158438",
    "https://openalex.org/W2990613095",
    "https://openalex.org/W3130482387",
    "https://openalex.org/W2998658430",
    "https://openalex.org/W1644641054",
    "https://openalex.org/W2963121255",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2979750740",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2799162093",
    "https://openalex.org/W3034324855",
    "https://openalex.org/W2964253930",
    "https://openalex.org/W3035272603",
    "https://openalex.org/W2606202972",
    "https://openalex.org/W2963123724",
    "https://openalex.org/W2952689920",
    "https://openalex.org/W1920022804",
    "https://openalex.org/W2962928871",
    "https://openalex.org/W2997333789",
    "https://openalex.org/W2986382673",
    "https://openalex.org/W3035739565"
  ],
  "abstract": "Abstract Remarkable performance from Transformer networks in Natural Language Processing promote the development of these models in dealing with computer vision tasks such as image recognition and segmentation. In this paper, we introduce a novel framework, called Multi-level Multi-scale Point Transformer (MLMSPT) that works directly on the irregular point clouds for representation learning. Specifically, a point pyramid transformer is investigated to model features with diverse resolutions or scales we defined, followed by a multi-level transformer module to aggregate contextual information from different levels of each scale and enhance their interactions. While a multi-scale transformer module is designed to capture the dependencies among representations across different scales. Extensive evaluation on public benchmark datasets demonstrate the effectiveness and the competitive performance of our methods on 3D shape classification, segmentation tasks.",
  "full_text": "Point Cloud Learning with Transformer\nQi Zhong \nSouthwest University\nXian-Feng Han  (  xianfenghan@swu.edu.cn )\nSouthwest University\nResearch Article\nKeywords: Point Cloud, Transformer, Multi-Level Multi-Scale, Classi\u0000cation, Segmentation\nPosted Date: October 27th, 2022\nDOI: https://doi.org/10.21203/rs.3.rs-2200447/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nSpringer Nature 2021 L ATEX template\nPoint Cloud Learning with Transformer\nQi Zhong 1† and Xian-Feng Han 1*†\n1*College of Computer and Information Science, Southwest\nUniversity , Tiansheng Road, Chongqing, 400715, China.\n*Corresponding author(s). E-mail(s): xianfenghan@swu.edu.cn;\nContributing authors: cuihuazq@email.swu.edu.cn;\n† These authors contributed equally to this work.\nAbstract\nRemarkable performance from Transformer networks in Natural Language\nProcessing promote the development of these models in dealing with com-\nputer vision tasks such as image recognition and segmentation. In this paper,\nwe introduce a novel framework, called Multi-level Multi-scale Point Trans-\nformer (MLMSPT) that works directly on the irregular point clouds for\nrepresentation learning. Speciﬁcally , a point pyramid transformer is investi-\ngated to model features with diverse resolutions or scales we deﬁned, fol-\nlowed by a multi-level transformer module to aggregate contextual infor-\nmation from different levels of each scale and enhance their interactions.\nWhile a multi-scale transformer module is designed to capture the depen-\ndencies among representations across different scales. Extensive evaluation on\npublic benchmark datasets demonstrate the effectiveness and the competitive\nperformance of our methods on 3D shape classiﬁcation, segmentation tasks.\nKeywords: Point Cloud, Transformer, Multi-Level Multi-Scale, Classiﬁcation,\nSegmentation\n1 Introduction\nRecently , point cloud analysis has been drawing more and more attention, since\npoint cloud, becoming a preferred representation for tasks of classiﬁcation and seg-\nmentation, can provide much richer geometric as well as photometric information\nin comparison with 2D images. Speciﬁcally , the outstanding success of deep learn-\ning strategies further make the task of 3D point cloud analysis achieve remarkable\n1\nSpringer Nature 2021 L ATEX template\n2 P oint Cloud Learning with Transformer\nadvancements in a diverse range of applications, such as autonomous driving [ 1][2],\nrobotics [ 3][4], virtual/augmented reality [ 5][6]. However, effective and efﬁcient fea-\nture learning from point clouds is still a challenge problem due to the irregular,\nunordered and sparse nature of point clouds.\nT o tackle such crucial challenges, many state-of-the-arts works focus on trans-\nforming the unstructured point cloud into either voxel grids [\n7] or multi-view images\n[8]. Although impressive progress has been made, these types of regular repre-\nsentation inevitably give rise to loss of underlying geometric information during\ntransformation, as well as high computation cost and memory consumption. The\nappearance of point-wise methods, such as PointNet [\n9], has revolutionized point\ncloud learning. These approaches directly process the raw point clouds by adopting\nshared Multi-Layer Perceptrons (MLP) [\n10] or deﬁning convolutional kernels [ 11]\nor constructing graph [ 12].However, most existing approaches may not be effective\nenough to learn context-dependent representation for point clouds.\nIn this work, we propose a novel point-based transformer architecture, named\nMulti-level multi-scale transformer (MLMST) following the tremendous achieve-\nment by transformer models in the ﬁelds of Natural Language Processing (NLP)\nand 2D Computer V ision. Actually , transformer provide an idea strategy to model\nrelationships between points, since it is permutation invariant and highly expres-\nsive as convolution. As shown in Figure\n1, our MLMST mainly consists of three\ncarefully-designed modules: (1) a point pyramid transformer (PPT), capturing con-\ntext information from different resolution or receptive ﬁelds. (2) a multi-level\ntransformer (ML T), learning the cross-level feature interaction to further aggregate\ngeometric and semantic information and (3) a multi-scale transformer (MST), model-\ning the context interaction across different scales to improve the expressive capability .\nBased on these three core modules, we can report that our MLMST is able to better\ncapture the long-range contextual dependencies from different levels and scales in an\nend-to-end manner.\nW e evaluate the effectiveness and representation capability of our network on\nseveral public benchmark datasets (e.g., ModelNet [\n13], ShapNet [ 14]) for 3D point\ncloud classiﬁcation and segmentation tasks. Extensive results show that MLMST can\nachieve highly striking performance comparable to state-of-the-art methods.\nIn summary , the main contributions of our work are as follows:\n• W e design a novel point pyramid transformer, a multi-level transformer and a\nmulti-scale transformer to capture the cross-level and cross-scale context-aware\nfeature interaction to improve the discriminative power of learned representation.\n• Based on these three modules, we construct an end-to-end network architecture,\nnamed Multi-level Multi-scale Point Transformer (MLMSPT), taking the unstruc-\ntured point clouds as input for highly effective geometric and semantic feature\nlearning.\n• Extensive experiments on challenging benchmarks demonstrate that our MLM-\nSPT model presents state-of-the-art performance on the tasks of 3D object\nclassiﬁcation, as well as segmentation based on point clouds.\nSpringer Nature 2021 L ATEX template\nP oint Cloud Learning with Transformer 3\n!\"#\n!\"#\n!\"#\n!\"#\n!\"#\n!\"#\n$\"%#&'\"()$\"%#&'\"()$\"%#&'\"()\n!$# %& &\n%&\n%&\n'!(\n&)*++,-,.*/,01\n%2*/342 567288,19 :0,1/ :;4*6,8 #4*1+-04624 !3)/,<)2=2) #4*1+-04624 !3)/,<+.*)2 #4*1+-04624 :0,1/ &)038 7*+28 #*+>+\n$29621/*/,01\n*!!+\n*!!+\n*!!+\n+\n+,-\n+,.\n%& %&\n/0-. 1/- -12\n34.\n/-4\nFig. 1 The overall architecture of Multi-level Multi-scale Point Transformer model for point cloud analy-\nsis. The network mainly contains three key components: a Point Pyramid Transformer encoding pointwise\nfeatures with three different scales or resolutions; a Multi-level Transformer and a Multi-scale Transformer\nmodeling cross-level and cross-scale context dependencies representation. pFFN represents point Feature\nForward Network.\n2 Related W ork\n2.1 Deep learning on Point Clouds\nRecently , increasing attention has been paid to design deep neural networks for 3D\npoint cloud learning, which achieve the state-of-the-art performance in the applica-\ntions of 3D object classiﬁcation [\n15], part segmentation [ 16] as well as semantic\nsegmentation [ 17]. In this section, we provide a brief review of these present\napproaches that speciﬁcally can be categorized into four groups:\nV oxel based Methods [18] attempt to voxelize the unstructured point clouds\ninto regular volumetric grid structure, so that the standard 3D convolutional neu-\nral networks can be directly applied similarity as the image to learn descriptors.\nFor example, V oxNet [\n7] is a milestone towards real 3D learning. However, these\napproaches have difﬁculty in capturing high resolution or ﬁne-grained features due to\nthe sparsity , loss of geometric information during rasterization, as well as expensive\nmemory and computational consumption [\n3]. Different efforts later have been made\nto alleviate this problem. OctNet [ 19] represents the point clouds using a hybrid grid-\noctree structure which is applicable to high resolution inputs of size 256 ×256 ×256.\nAnd Kd-tree [ 20] is another structure that can also be utilized to provide improved\ngrid resolution.\nMulti-view based Methods [8] project the raw 3D point clouds into a collection\nof 2D images rendered from different viewpoints, followed by image-wise feature\nextraction with well-designed 2D convolutional neural networks. Then fusing these\nfeatures forms the ﬁnal output representation for various analyses. Although remark-\nable performance is achieved, this kind of approaches suffers from information loss\nduring the projection process [\n21] and becomes time-consuming when dealing with\nsparse point clouds. On the other hand, it is difﬁcult to determine the appropriate\nnumber of views for modeling the underlying geometric structure [\n22].\nPoint Cloud based Methods directly manipulate unstructured and unordered\npoint clouds, and take the 3D coordinates or/and RGB or/and normal as initial input.\nSpringer Nature 2021 L ATEX template\n4 P oint Cloud Learning with Transformer\nAs the pioneering work, the emergence of PointNet [ 9] can be considered as a\nmilestone in the domain of learning point clouds, which guides the development of\npointwise MLP methods. This series of works usually utilize shared MLP to process\neach point individually to perform feature extraction. However, their performance\nis limited since they fail to capture local spatial relationships in the data [\n21][23].\nRecent approaches begin to concentrate on deﬁning effective convolution kernels\nfor points. KPConv [\n11] deﬁnes the point convolution using any number of kernel\npoints with ﬁlter weights on each point, which gives more ﬂeibgility and is invari-\nant to point order. FPConv [\n24] proposes a surface-style convolution for point cloud\nanalysis by learning local ﬂattening, which can be treated as a complementary to the\nvolumetric-style convolution.\nGraph based Methods lead to a new trend of irregular data processing, which\nrepresent the point cloud as graph to model the local geometric information among\npoints [\n25]. ECC [ 26] and DGCNN [ 27] propose different edge-dependent convo-\nlution operations to aggregate neighboring features spatially . GAC [ 28] deﬁne the\nﬁlter kernel using the learned attentional weights assigned to neighboring points,\nwhich is heleful for semantic segmentation.3D-GCN [\n16] introduce a well-designed\ndeformable 3D kernel to guarantee scale invariance, and a 3D graph max-pooling\noperation to summarize cross-scale features. SPH3D-GCN [\n12] proposes a separa-\nble spherical convolutional kernel for graph neural networks. This network achieves\nhighly competitive performance on the standard benchmarks.\n2.2 T ransformer in Computer Vision\nThe Transformer networks can be perceived as a signiﬁcant breakthrough in Natural\nLanguage Processing (NLP), whose success is mainly attributed to the self-attention\nmechanisms which can model long-range information and dependencies in the input\ndata [\n29]. Recently , many architectures begin to take transformer and self-attention\ninto consideration for computer vision task. Image GPT [ 30] is the ﬁrst work to inves-\ntigate the Transformer for learning image representation in an unsupervised fashion.\nV iT [\n31] applies the original Transformer to image patches instead of pixels for\nimage classiﬁcation task, which can achieve the state-of-the-art performance with\nless computational resources consumption. DETR [\n32] performs object detection\nfrom the set prediction point of view using a transformer encoder-decoder architec-\nture. The advantage of DETR is that it doesnot require the hand-designed modules\n(e.g., non-maximal suppression ) usually used in the previous frameworks.\nInspired by the fundamental mechanism of Transformer in NLP and Computer\nV ision, we aim to model the cross-level and cross-scale feature dependencies to\nobtain ﬁne-grained performance for point cloud based tasks with our well-designed\ntransformer modules.\n3 Point Cloud Representation with T ransformer\nAs illustrated in Figure\n1, given an input point cloud of N points P ∈ RN×C , where\nC represents the dimension of pointwise properties. Here we aim at modeling cross-\nlevel cross-scale interaction to discrimnatively boost the expressive capability of\nSpringer Nature 2021 L ATEX template\nP oint Cloud Learning with Transformer 5\n!!\"\n#$%!!\"!\n&$%!!\"!\n'()*+,(\n!\n\"!!!\n!!! -./01+2\n3$%!!\"\n!\n400(506.5 7+,\n400(506.5 8(+09'(\n!!\"\n:90,90$ !!\"\n!\n !\"#$%&'!()#%*)%+\"#%,- .)/0/-#12%3/'4(0\nFig. 2 Architecture of point self attention mechanism used in Point Pyramid Transformer.\nlearned representations. Farthest point sampling (FPS) is initially performed to obtain\nthree point clouds with different resolutions, followed by potential feature learning\nwith feature embedding module. Then, we extract hierarchically multi-scale repre-\nsentations via our Point Pyramid Transformer (PPT). For each path of PPT module,\nMulti-level Transformer (ML T) consumes the concatenation of features from dif-\nferent levels to capture point cross-level representation correlation or interaction.\nFinally , through a Multi-scale Transformer (MST), we relate the point feature across\ndifferent resolutions to learn a discriminative representation.\n3.1 Point Pyramid T ransformer\nEmpirically , we can argue that different resolutions actually corresponding to differ-\nent scales or having different size of receptive ﬁelds during feature extraction using\nthe same operator. Therefore, in order to model a hierarchical semantic or contex-\ntual information with different scales for point cloud, we introduce a point pyramid\ntransformer module. The farthest point sampling (FPS) operations are progressively\nperformed on input point cloud P to get three point clouds P1 , P2 , P3 with different\nresolutions N1 = N, N2 = N/2 and N3 = N/4, respectively , followed by generating\nan initial pointwise feature map pyramid P = {F 0\ni ∈ RNi ×D ,i = 1,2,3} via feature\nembedding block using pointwise feedforward network. .\nThe Point Pyramid Transformer (PPT) module takes P as input, where each\nbranch independently maps corresponding scale feature maps into latent represen-\ntation F i\nPPT ∈ RNi ×D\n′′\n,i = 1,2,3. Here, since self-attention mechanism, as the core\nof Transformer models, is permutation invariant and can model long-range context\ndependencies, the essential building block of PPT , therefore, will based on point\nself-attention mechanism (PSA) as shown in Figure\n2. Formally , we formulate the\nattention operator as follows:\nF l\ni = A (F l\ni ) =σ (Ψ (F l\ni )Φ (F l\ni )T /\n√\nD)∆ (F l\ni ) +F l\ni (1)\nwhere i = 1,2,3 denotes the ith scale or resolution, l = 0,1,2,3,4 represents the lth\nlayer. Ψ (•), Φ (•) and ∆ (•) are linear point transformation used in our paper.\nΨ (F l\ni ) =F l\ni W q ∈ RNi ×D\n′\n(2)\nSpringer Nature 2021 L ATEX template\n6 P oint Cloud Learning with Transformer\nΦ (F l\ni ) =F l\ni W k ∈ RNi ×D\n′\n(3)\n∆ (F l\ni ) =F l\ni W v ∈ RNi ×D (4)\nW q ,W k ∈ RD×D\n′\n,W v ∈ RD×D deﬁne the learnable weight parameters. σ adopts the\nsoftmax function to normalize the weights.\nSubsequently , from top to bottom in our PPT , each path is constructed by stacking\nfour sequential PSA modules to obtain pointwise feature representation from certain\nresolution of point cloud. Finally , to fully investigate cross-level information, feature\nmaps in different levels together with the initial input are concatenated to generate\nthe output of PPT module F i\np pt.\nF i\np pt = concat(F 0\ni ,F 1\ni ,F 2\ni ,F 3\ni ,F 4\ni ),i = 1,2,3 (5)\nActually , we can modify the number of resolution or scale branches, the stacked\nPSA modules or levels, and the size of input feature maps according to speciﬁc\napplications.\n3.2 Multi-level T ransformer\nTheoretically , aggregating information contained in different levels can boost the\nexpressive power of pointwise features [\n33]. In order to take fully advantage of\nmulti-level context and model the long-range dependencies or interaction across\nthese levels, we introduce Multi-level Transformer (ML T) module based on multi-\nhead self-attention mechanism, which consists of three independently parallel ML T\ncorresponding to each scale or resolution path.\nOur ML T operator consumes F i\nPPT to encode much richer relationships amongst\npoints, which is formally deﬁned as:\nF i\nMLT = M A(F i\nPPT ) =concat(A 1 ,A 2 ,...,A M ) +F i\nPPT (6)\nwhere\nA m (F i\nPPT ) =\nσ (Qi\nm (Ki\nm )T /\n√\nD\n′′\n/M)V i\nm (7)\nQi\nm = F i\nPPT W i\nQm (8)\nKi\nm = F i\nPPT W i\nKm (9)\nV i\nm = F i\nPPT W i\nVm (10)\nHere, M is the number of heads. m indicates the mth head. W i\nQm ∈ RD\n′′\n×dq , W i\nKm ∈\nRD\n′′\n×dk , W i\nVm ∈ RD\n′′\n×dv are learnable weight matrices. W e set dq = dk = dv = D\n′′\n/M.\nLastly , these three individual ML T map the level-concatenated features with three\nresolutions from PPT into three independent shape-semantic aware representation\n{F i\nMLT ,i = 1,2,3}.\nSpringer Nature 2021 L ATEX template\nP oint Cloud Learning with Transformer 7\n3.3 Multi-scale T ransformer\nGenerally , point features from different scales or resolutions corresponding to dif-\nferent contextual or semantic information [\n34]. T o enhance interaction among low-,\nmid- and high-resolution, we also adopt multi-head self-attention mechanism to con-\nstruct our multi-scale Transformer (MST) module and generates relations across\ndifferent scales.\nThe obtained cross-level interacted feature maps F i\nMLT from ML T are fed into\nour MST model. W e ﬁrst sample the maps of these three different scales directly\nup to the same size as the original input to our network via interpolation operation\nused in PointNet++ [\n10]. Then we concatenate them together to encode multi-scale\ninformation.\nF i\nu p = U p(F i\nMLT ),i = 1,2,3 (11)\nFcat = concat(F 1\nu p,F 2\nu p,F 3\nu p) (12)\nSubsequently , the multi-scale transformer operation is performed on the Fcat .\nWith the similar multi-head self-attention strategy as Equ.(6), the formulation of\nMST module is deﬁned as:\nFMST = M A(Fcat ) (13)\nBy integrating Multi-scale Transformer module, we can further boost the abil-\nity of our network to learn a discriminative representation for each point with\nsemantically and geometrically enhanced information.\n4 Experiments\nIn order to evaluate the performance of our proposed Multi-level Multi-scale Trans-\nformer model, we conduct extensive experiments for the problems of 3D point\ncloud classiﬁcation, segmentation and provide comparison with the state-of-the-art\napproaches. Here, we adopt Pytorch framework to implement our Transformer archi-\ntecture on NVIDIA Titan R TX with 24G memory . And Adam optimizer and step LR\nlearning decay scheduler are used to train all the models. The following expands on\nthe discussion of experiments and results.\n4.1 Point Cloud Classiﬁcation\n4.1.1 Datasets\nThe classiﬁcation task is performed on ModelNet40 benchmark datasets. Mod-\nelNet40 consists of 12,311 CAD models from 40 categories, in which 9,843\ninstances are selected for training and 2,468 shapes are utilized for testing. Follow-\ning PointNet[\n9], 1,024 points are uniformly sampled from each object model. During\ntraining, we perform operations, including random point dropout, random scaling in\n[0.6, 1.55] and random shifting in [-0.2, 0.2] on input point clouds for data augmenta-\ntion. W e train the classiﬁcation network for 250 epochs using an initial learning rate\n0.01, which is dynamically adjusted using cosine annealing strategy . The batch size\nis set to 32.\nSpringer Nature 2021 L ATEX template\n8 P oint Cloud Learning with Transformer\nT able 1 3D Object Classiﬁcation results on ModelNet40 dataset.\nMethod Representation Input Size ModelNet40\n3DShapeNets [ 13] V oxelization 30 3 77.32%\nOctNet [ 19] V olumetric 128 3 86.5%\nMVCNN [ 8] Multi-view 12 × 2242 90.1%\nGVCNN [ 35] Multi-view 8 × 93.1%\nDeepNet [ 36] Points 5000 × 3 90.0%\nECC [ 26] Points 1000 ×3 83.2%\nDGCNN [ 27] Points 1024 × 3 92.2%\nKd-Net [ 20] Points 2 15 × 3 88.5%\nKPConv [ 11] Point 7000 × 3 92.9%\nPointNet [ 9] Points 1024 × 3 89.2%\nPointNet++ [ 10] Points 1024 × 3 90.7%\n3DmFV -Net [37] Points 2048 × 3 91.4%\nFoldingNet [ 38] Points 2048 × 3 88.4%\nKC-Net [ 39] Points 1024 × 3 91.0%\nPointCNN [ 40] Points 1024 × 3 92.5%\nPCNN [ 41] Points 1024 × 3 92.3%\nRGCNN [ 25] Points 1024 ×3 90.5%\nShapeContextNet [ 42] Points 1024 × 3 90.0%\nSpec-GCN [ 43] Points 2048 × 3 92.1%\nSRN [ 44] Points 1024 × 3 91.5%\nPoint2Node [ 45] Points 1000 × 3 93.0%\nPoint2Sequence [ 46] Points 1024 × 3 92.6%\nPointConv [ 47] Points 1024 × 3 92.5%\nΨ -CNN [ 48] Points - 92.0%\nFPConv [ 24] - - 92.5%\nPoint Transformer [ 49] Points 1024 × 3 89.6%\nSPH3D-GCN [ 12] Points 1000 × 3 92.1%\nDR-Net [ 50] Points 1024 × 3 93.1%\nPoint Transformer [ 51] Points 1024 × 3 92.8%\nPT [ 49] Points 1024 × 3 92.9%\nPCT [ 52] Points 1024 × 3 93.2%\nPRA-Net [ 53] Points 1024 × 3 93.2%\nP AConv [54] Points 1024 × 3 93.2%\nOurs Points 1024 × 3 93.2%\nPointNet++ [ 10] Points+normals 5000 × 6 91.9%\nSpiderCNN [ 55] Points+normals 1024 × 6 92.4%\nSFCNN [ 56] Points+normals 1024 × 6 92.3%\nPointW eb [57] Points+normals 1024 × 6 92.3%\nELM [ 15] Points+normals 2048 × 6 93.2%\nFPConv [ 24] Points+normals - 92.5%\n4.1.2 Performance Comparison\nT able\n1 quantitatively reports the experimental comparisons with several state-of-\nthe-art methods. Our MLMST directly operates on the raw xyz coordinates of only\n1,024 points to yield these results. Speciﬁcally , on ModelNet40, our model achieves\nthe best performance with a superior accuracy 93 .2%, outperforming the voxel grid-\ninput, multiple views-input and almost all state-of-the-art point-input methods. These\ncompetitive results convincingly demonstrate the effectiveness of our MLMST .\nSpringer Nature 2021 L ATEX template\nP oint Cloud Learning with Transformer 9\n\u000fƭ \u0013%\t \u001a\nŸ\u0019 \u0016ĠQ˶\n\u0013\u0014\n˶[\u0003\n\u0001\u0004\n˶\nˎ\u0019ɮƁ\u0001`\u0002\n\u0003\n\u0010\n\r\n\u0010 \u0004\u0004\u0003 \u0003\u0004\n8aǖ'\u0004ǲ+\n\f\u0004\n˶\n\u0011\n/\n0\n˶\n\u0003\n\u0003\n˶\nǂ\"\u0001\u001f˶\nŗ\n˶ \u0004\u0004\n˶\n\u0017bƐa\u0018\u0002 \n\u0010\n\u0003\n5\u0001\n\u0001\u0012Ɲ\n%( \u001b\n@~\n\t\n\f h\n˶\nq\n[\nʑ1\n\t\n˶\n\u000e\u001b\n/\n>\n\t\n\u001bĽ\u000b\n,\n/\nP }\u0003 k\u001anl \u0017\n #Zv\u0004 \u0012\n/\n(\n\u0011\n\t\n˶\n\u0001\u0018\n˶\n\u00074\u000fS\u0012\n@ A\n\u0012\u000f\n\n\u001f[\\ a ]w\u0010\n9\n[\\\nęˮ\n˶ \u0004\u0004\n˶ ,\n\u0006\nq\n\u0001\u0004\n˶\n\n\u0004\n˶\nȓ ǳŘ\n\u0019\u001b \u001a\u001d+\n$\n,\n#\n2/0\n)Ú \u0007 čįǦ\u0011\n\\[^`\u001b ^\n\nF\nĶå37 .J\n[B\n[ _\n\n\n˯Ƴ\n\u0010\nș\u000f\n\u001c\u0001\nQ\n\u0001\n˶\n\u0005\u000f\u000e\f\nȬ\n\u0001ˏ\n\u0001\nȃ\u0010\u0001Ź \u0007 \u0010 Í˶Ã\u000eİ\u0001ȚȱɭǴǃ ɠ\u0001ǫȩɜ ɒ ɥ\nȵI\u001eJ\u0017\u0002æʬ\u001cț^\nQ Ě\rù\u0002¢\u000fɉǻ \r\u0005\b\u0003\u0013ă ƂE\r\u000eÄJ~Ŋ\u0007\u000b\bźO˶ \u0002£\u0007ç\u0002ěSĜ˶ úǢ= ČɁ\b\n\u0006\nʜ \u0004 Â\u0002¤\u0002ŧ\u0002Ì«\u0007 \u000b\n\"\nƑ\u0002Á˶ƒ˶Ī ȼ\b\u001aȜƺ\b\t \u0001 \u0007\n\u0010\u0005 \f \u0005 \u0006\u0012\u0011 \nȝ 7\u0013ɂ2ļ\nǰ \u0002û\u0001\u0010 \u0014Q!Q#Q\u0010&1ƻ\nˡ\u0002\u0005\u0013\u000e\b\t\u0012\u0003\u000e\r\n\n\r\u000b8\u0005ŋĝA\u000f\u001f ˶\u001fpȗ\u0017 \u000b\u000b8 L ¬\u0007 ˶ \r\u000bÇ50Ƀ Ɉţ \u001c \u0001 \f\u0004\r\n\u0017 \rÅ \t\bʱ4\u0001ʲ5Ȳ \n \r ȁ\u0002\u0002ǟ \u0018\"ɯ\u0001Ǌ\nɀ\u0002ƞ\u001fɞ!\u0002\u0005 ˴\u000b\bè\nȾǏ\u0001\u0011ˍʿɴɪ\u0002ü \u0010˶\u0010 Ɋ\u0002 é\u0001\u0006 \u0001\nɖê Ũ&\n\u0014\nɷ\n\u0002\rəÆ˶#0H \u0003 E \f \u0017 ˶\u0010˶\u0010 ʳ \u00013ǽ\u0002\u0002\u001a &8\u0001Ŕ \u0002  Ɵ\u0002?ũ\u0002ý˶ʾʝƽ \n \f ˀ \u000e \f \u0001ǐ˓ \u0002\u0010Ơ5ȳ\u0010\u0002\u001e(\nƓ K˔ʴ\n\u0013\u0001\n\u0011\u0003\n\b\n\u0006\n\t\n\u0001 CI/\n\u0014\u000eF ˶ ªÀ˶\u001e \u001f\u0002\u000e\u0017\u0004\u0005ĸ\u0005 þ˵˶ƴʒ É?\u0007\u0005È˕ȋĻ\bl\u0003\b \r ʵ 6[ \r\n\u001d8\u0011Ƶ\u0002 Î\b˶ Ū˶Ɣ˶\u0018\u0006\u000f\u00197 \u0003 \u0014 ! \u001a\u0005\u0015\u0010 \u0004\u0003\u0005\nˢeǾ ǵȞ\u0001ȟƃ\nɗɋƶ š\nȠǬ\nɤ\nř ƕ\nC J\u001eÓ Ċ\nǍ\u0005[iŌÛ\u001c ɹ\n:\u0014 C\n\n\u0011\u0003\f\nɑɣɰǀT\rz \n\u0002 \u0003 \nʈˣȉ\nȸǋ\n\n\u0002 \u0001 \n2(1J\n\u0016\n9\nB3&E<\u0003 \u001f\n\n˶\n\f\n˶\n$\n-/=Ĺō\nƬM\u0004¥Ŏ ǷƄ\u0006\nʞ9 \u0001\n*-\n1(r \n(&.\nȿÜ\"\n\u0001 \n,\u0004@ \u0018\u0005\u0006@\"\u0007\b \t@ \u001a@ -\n>@Y˶Y˶ Ɩū­˶¦\u000f ʟŬ˶Z˶ƗƘ˶\r\u0002 \u0003\u000e\u000fëĖʶìʷ\u0010ȡ\u0011˖ssŻŭRɺ\u0012Ȣtʸ\u0013 ˁʄ )ƩŮRǿ®ÏíůGŰ¯Z§ű˶ \u0002\u0007\b\t\n\u000b\fQËŲGî˰\u0013 tȣ˂ƙų˶ Ŵ˶ ¨°±˶I\u0011ǑÊƨ)²Đ )˶\u0001\u0005ï˶ ð³´µ¶·˶\u000b.\f@ /\u0019@?\r@5@\nƧ #%\n4\n;GA\n,?' :7\n8\nI.@H\nDF*\n6\nÕ >)+5\nɓ\nɸɛ\u0001ȯ\n\u000e5\u000b\b\t\f S:ʐTġ\n˶\n\u0015$\r\r\nQ\nƅ $\u001a\u001a!ÐV\n˶\u000e\u000e\u0016ʽ\n˶\n\u0005\b˶\nWĔ \u0007\nơ\u0019\n\u0006\nɻ\n\u000fkƢǙ˗ jǌ \f\u0013\u0003\u001b\u000b ɼˤǕ\u0003\nʹ\n˶ʓǎ\n\t\nǼɌı\b7 \u0017\u0002ñ\nƮ|\u0002\n\u0013\u0014\f\n˶\n\u0007 2˶\n\u0011u\n˶\nʠ\n˶ v\u0001˶\nż Ś\u001e e ė\nK\n˃˥ʡ\nɽ\u0004 ɫ<\u000e\n\u0006\n˶\nƫB\u001d\n\b\n: $\n@\n\u0003\n\u0003\n\u0014\u0001˶\nK\nK\nQ\n\u0011\n˶\n x) \u0012ʔme \u0013D\n\u0004\n\u0003,\n\t\n\u0003\n˶\n\b ĺHÝ)ľ \u0007\nʭ ť\nĞ\n˶\n\t\n\n\t\n\u0003\n˶\n\u0005\b\u000b\u000e\n˶\n\u0003\u0006\n\u0010\n\u000b\n\u0010\n¸ÿ\u0006\n˱ ˦3\n\u0006\n\u0012 ʕ#\n˶\n\u0005\n\u0006\n˶\n6\n˶\n'''\u0005\u0005˶\n\b :Ñ\u000e\n\n\n\f\n˶\n\u0011\n˶\nȷ\u001c\u0003\u0003\n˶\n{\n`\n[\n\u0005\u0005˶4\n˶\n\u0011\n\u0011\n˶\n¹\n\u0006\n˶\u0005\u0005˶7\nĢ\n˶\n\u0006\n\u0019˶\n\u0005l\u0012\u0016\u0016˶\nȤ \u0011\u0012ǒː\nʮ \n\u000e\n\u0006\n\u000b\u000b\bĿ\u000bm\n\u0006\n\u000e\nÖ\n\u0006\n˶\nn\u0001˧\n˶XƯ\b\n\u0006\n˶\n\u000f\u0010\u000f\nQ\n\u0017\nQ\n\u0010\u000e\n@ \u001c\n\u0003 }\n˶ \u001f \u0004\n\u000f\u0001˶\nC\n\u0003\u0003\n\u0001˶\n\r\u000b\r º\n\u0006\n\u000e\u0007\n˶\nw\u0001\u000f˶\nǈo\u0001 \u0012Ɔnȅ >\u0006\n\u0019% \"\u0019Ž\n\t\u0014 \u0014- \nư \u0014\n\u0017Ƈ\n\u0006\nĎǄ\n\u0001ī\n\u0002$ ˄ȍʢƈś\u0001\n˶\nˑǸ\nȖÞ MēƷ\u0003#ȇ;\u0002\u0002\n\u0007 ŇĬ \u0007 ŏ\nĄi N\u0004 \u0001ɾ\n\t\n\u0003\n˶\n˨ \u0016\u0002ą\"\u000e\n\u001d˅\n˶\u0005ƚ\n˶ ģ Ő< Ę\n˶ \u0001\u0004\n˶\n0\u0011\n@ <\n@\u000f 1\n@\n+\n\n\u0001\na[s \u000bLM\nQ\n |bH^c\n/\n[y P4\u0005\u0005Ĩ\n˶\n\u0004 \u0001\u0003 ˩c \u0014\u0003\nŀ O\n˶\n\u0005\u0005˶\nċ\u000b˶\nĀ\u0007 \r˶\r\u0005˶ ˘Ŝɏ \u0014\nŝ\u0002ǣ\u0001˙\u0016Ȼx\u0001\u000f\u0003\n˶\n6 8\u0019\n\f\n˶\nĤŞ\u0001˶\n\u000b\u000b˶\nǤ}\n\t\n\n˶\nNJ\nQ\n\nK8,\n\nL\u0005 \fƱȎȥ\u0003\u0004\u0003\n˶\n%/M7yz\n˶\n\n\t\n˶\ncz\u0018&'(\u0002\n@\n\b \"\n˶\nf \\\u0002wv6\n˶\n˞ ˟\n˶\n\fǇ\n\t\t\n˶\n\u0010= )\n@\u0001ʣdƣʯ \u0015\u0018\n˶\n*\u0001;\n\u0019 G 0ʖ=\u0010@\n˶\n\t\n˶\n\u0015\n˶\nŁ\nȏȑ \u0014\u0018\n\t\n˶\n\u0015 \u0004\n˶\n\u0007 .Ť +\u000f \u0005\n\u0013\u0018\n˶\n;i\n\nǓ\nN*\u0015\u0014\n ʉU\nā\n˶\nɧ ʊɳɿd ˆˇ\n\u0003\n\t\n˶\n\u001e\u001f\n@2\n0\n@!\n@3\n\u0010\n@\n\u0017*\n\u0002\u001e D\n\u0006\n\u0007\u000eşÔƉ \u0007\n3 E<\n mn\n\n+)c! j\u0016 \u0017 % \u0015W O\u0006 J\u0004O5\u0014o\n \u0007\n\u0012\n! \u0015ghF.ɍǗ\nȔ\n\u0003\n\t\n\u0001ő \u0004\u0014 ʤ1 ǜɵ\u0001ˈ\u000e\u001c \u001a˚ˉŵfʋǧƊ\u0007 +ǔʺǚɎ\u0012ʀ\n\b\nǠƪʥ\n˛\n\u000fʁ\u001bƾ\u0015 \u0004\n\t\n\u0004\n˶ ȪʌŶL\u0015\f\n˶ łʻǪ\nɕ\n4 \\d ǭȹ\nȆ \u0001Wɇßȕ\nƋ\nò\u0002\n\\©àǯȈ\u0018\u000f˶\n\f\n\f\u0012 ǩȐǛ \u0001˒ʗ \u0004ʇˊ\u0012ǡƛ \u0007˶ * \u0006\nó\u0001\u001d \n˶\n\u0006\n\u0007\n˶\nĲ;ʦ\n˶\noɱɲ\u001bʂ\nƸ ǥƌ\nɝŕŷ\nĒDáUŠ\fU\nƼ\nƹR\u001a\n~ T\n?X V \u0017\n# \u0011\u001b˜Ȯɨ˲\u0002\\ \u0016\u0002\u001aƍ žã \u0007 ˪\u0015\u0018\f\u0004\u000f˶\nƤ\u0003\u0003\n\u0003\n˶\nǅ\u0001\u0012h\u0012\u0004 \u0001\u0014Ȧ& \u0012\u0019\n \n˶\n. \n\u0018 \u001d\n[\n\n\n]p Ʉ\u0002\u0016\n\u0005\b\u000b »\n˶\n\u0005\u001c\n/\u0013\n/\u001d \u001e\n\u001f\n\u001d\u001f\n/|\u0018/V\n˶\nǮƎ\u000b Òĳâ\t\n\u000f\n\t\n\u0013˶\n$\nŃ\n¼\n\n\n\u0006\u0006\n*Nķ\n\u0010˶ ʰr\n˶\n0\n\t\n\u001b\u0004˝\u0015\n˶\n\u0007 j\u001c\u0004Ʌʼ\u001cʘ\u000eĩ\n\u0005˶\n'%\u0002\u0011 \u0011\u00126$ \u0017ń\u0006\nʧȊ ˳ȶʍ\n\t\nȧ- ɟr˫\nǶɘȰĕ %\n\u0016(˶\u0003\u0013\u0001˶ \u001c\u0003\u0018 -\n/\n6ȺǱ\nȽŖ\u0002ǉ\f]\u000f\f\n˶A\u0013\n˶\nď^½k \u0007 ˬ Ȍ ƥ\u0002 ğˠ\u001e\u0019\u000f \u0015\u0014\nȨɢǝ\n\b\n\u0006\n˶2\u0001\u001d\n@ 04\n@ ØŒ\u000b \r˶\u0005\u0005\r\n˶\n\u0003 Ʀ.> ǆ{\u0004\n˶\n TȘ_ŅŢ ʨɩ\n\u001aʃ\nɔɦ\u00013 ȫB ǹ\u0002ôʎ ĥ\u0004ʅʆ \u0001˭\u0001\u0004\u0005\u0006\n\u000f\n\r \u0007\n\u0007\n\u0006\n\u0006\u0006\n¾\n˶\u0005˶\nň\n¡\n\u0005\n\u001d\u0004ŉ\bʙĦ\bõ\u0004 \u0001g\n!Ɯ\n˶\n'\n\u001d\u000e\u001b\u000fĂ w\u0015\n˶\n \u0007 #ˋ ɐȄȒ\u0019\u0003u\u0004\n˶\n\u0012 \u001a\n\n\n\u0002öĭ\u0007 ˌȭ\n- ǘ\n\u0016\n\u0005ʩ1PY\n\nt2`\n@\u0014 \u0016\u0001 ȀqǨ \u0017 Xä\n\u001d\nʪ\u0003\n˶\nNJ\nQ\n[\f\n9\u0004 ĈĴ\n˶\u000e¿\n˶\nœ\u000b ÷\u0004 ʚɆ,PƏſǺ\u0014 \u0001\u0004\n˶\nbʏ\ng\nǁȂȴ\u001e Ć\n!ć`Į\u0002øx\u0015\u0004\n˶\nQ\n-y\n˶\n\u0007\u0013\n/\n#Ǟ ƀf\n +\u0012 \u0014\u0015\n@\u0013 \u0013\n@\nu\n=9*\u0007 ×\u0005\b\nĵƿɬɶɡ{\" \u0015\n&\n\u000bđ(\u000b ņÙ\u000b\u000eP\nħĉŦ\u0001\u0001˶ ɚ\u0004\n\u0005\b\n˶\nʛ\n˶\nFig. 3 V isualization of part segmentation on ShapeNet Part.\nT able 2 Part segmentation results on ShapeNet part dataset. The mean IoU across all the shape instances\nand IoU for each category are reported.\nMethod mIoU aero bag cap car chair ep guitar knife lamp laptop motor mug pistol rocket skate table\nShapeNet [ 14] 81.4 81.0 78.4 77.7 75.7 87.6 61.9 92.0 85.4 82.5 95.7 70.6 91.9 85.9 53.1 69.8 75.3\nPointNet [ 9] 83.7 83.4 78.7 82.5 74.9 89.6 73.0 91.5 85.9 80.8 95.3 65.2 93.0 81.2 57.9 72.8 80.6\nPointNet++ [ 10] 85.1 82.4 79.0 87.7 77.3 90.8 71.8 91.0 85.9 83.7 95.3 71.6 94.1 81.3 58.7 76.4 82.6\nKD-Net [ 20] 82.3 80.1 74.6 74.3 70.3 88.6 73.5 90.2 87.2 71.0 94.9 57.4 86.7 78.1 51.8 69.9 80.3\nSO-Net [ 58] 84.9 82.8 77.8 88.0 77.3 90.6 73.5 90.7 83.9 82.8 94.8 69.1 94.2 80.9 53.1 72.9 83.0\nRGCNN [ 25] 84.3 80.2 82.8 92.6 75.3 89.2 73.7 91.3 88.4 83.3 96.0 63.9 95.7 60.9 44.6 72.9 80.4\nDGCNN [ 27] 85.2 84.0 83.4 86.7 77.8 90.6 74.7 91.2 87.5 82.8 95.7 66.3 94.9 81.1 63.5 74.5 82.6\nSRN [ 44] 85.3 82.4 79.8 88.1 77.9 90.7 69.6 90.9 86.3 84.0 95.4 72.2 94.9 81.3 62.1 75.9 83.2\nSFCNN [ 56] 85.4 83.0 83.4 87.0 80.2 90.1 75.9 91.1 86.2 84.2 96.7 69.5 94.8 82.5 59.9 75.1 82.9\nPointConv [ 47] 85.7 - - - - - - - - - - - - - - -\n3D-GCN [ 16] 85.1 83.1 84.0 86.6 77.5 90.3 74.1 90.9 86.4 83.8 95.6 66.8 94.8 81.3 59.6 75.7 82.6\nELM [ 15] 85.2 84.0 80.4 88.0 80.2 90.7 77.5 91.2 86.4 82.6 95.5 70.0 93.9 84.1 55.6 75.6 82.1\nW eak Sup. [ 59] 85.0 83.1 82.6 80.8 77.7 90.4 77.3 90.9 87.6 82.9 95.8 64.7 93.9 79.8 61.9 74.9 82.9\nGBN [ 60] 85.9 84.5 82.2 86.8 78.9 91.1 74.5 91.4 89.0 84.5 95.9 69.6 94.2 83.4 57.8 75.5 83.5\nPoint Transformer [ 51] 85.3 - - - - - - - - - - - - - - -\nPCT [ 52] 85.8 84.5 83.5 85.9 78.7 90.9 75.1 92.1 87.0 85.0 95.9 69.6 94.5 82.2 61.4 76.0 83.0\nOurs 86.0 83.6 84.7 86.3 79.8 91.1 71.2 90.2 88.6 84.9 95.9 72.8 94.8 83.4 56.2 76.7 82.6\n4.2 Part Segmentation\nPart segmentation is a challenging task aiming to assign a part label to each point in\na given 3D point cloud object.\n4.2.1 Datasets\nW e evaluate the part segmentation task on the broadly used ShapeNet part dataset\n[\n14], which covers 16,881 shapes of 3D point cloud objects from 16 different cate-\ngories. The object in each category are labeled with less than 6 parts, amounting to\n50 parts in total, where each point is associated with a part label. These models are\nsplit into 14,007 examples for training and 2,874 models for testing. Here, we sample\n2,048 points from the dataset. During training, we adopt the same data augmentation\nstrategy as that for classiﬁcation. W e train our segmentation model 200 epochs with\na mini batch size of 16.\n4.2.2 Evaluation metric\nThe Intersection-over-Union (IoU) on points is considered as the metric for quanti-\ntatively evaluating the segmentation results of our model and comparison with other\nexisting methods. Following the previous works [\n9], we deﬁne the IoU of each cate-\ngory as the average of IoUs for all the shapes belonging to each category . In addition,\nSpringer Nature 2021 L ATEX template\n10 P oint Cloud Learning with Transformer\nT able 3 Ablation analysis on our Multi-level Multi-scale Transformer architecture.\nMethod Accuracy\nBaseline 92.2\nBaseline + PPT 92.4%\nBaseline + PPT + ML T 93.0%\nBaseline + PPT + MST 92.8%\nMLMST 93.2%\nthe overall mean IoU (mIoU) is ﬁnally calculated by taking average of IoUs across\nall the shape instances.\n4.2.3 Results\nT able\n2 summarizes the performance comparison between our MLMST with several\nbaselines. From the quantitative results, it can be clearly seen that our Transformer\nmodel reaches a much better part segmentation performance with the instance mIoU\nof 86.0%, outperforming the state-of-the-art approaches. The visualization of part\nsegmentation on the ShapeNet part is given in Figure\n3. These results show the\nrobustness of our MLMST to diverse shapes.\n4.3 Ablation Study\nIn this section, we perform extensive ablation study to investigate the effectiveness\nof each individual components of our MLMST architecture using ModelNet40 for\nevaluation. Speciﬁcally , we adopt the single-resolution MLP as our baseline. T able\n3 summarises the classiﬁcation accuracy of different design choices. From these\nresults, we can claim that the integration of PPT , ML T and MST modules achieve\nsigniﬁcant performance improvement over baseline. This further demonstrates that\nfeature interactions across different levels and scales are beneﬁcial to discriminative\npoint cloud representation learning.\n4.4 Conclusion\nIn this paper, we introduced, Mult-level Multi-scale Point Transformer, an end-to-end\narchitecture relying on self-attention mechanism for point cloud analysis, which inte-\ngrates three fundamental building modules, a point pyramid transformer, a multi-level\ntransformer and a multi-scale transformer, to enrich contextual interaction across\ndifferent levels and scales. Extensive experiments conducted on challenging bench-\nmarks have demonstrated our MLMSPT achieves the state-of-the-arts performance\non 3D object classiﬁcation and segmentation. W e believe that Transformer can play\nan important role in learning point cloud representation, therefore, further investiga-\ntion of its development and application to various point cloud based tasks should be\nexplored in future.\nACKNOWLEDGMENT\nThis research was supported by the National Natural Science Foundation of China\n(No. 62002299), and the Natural Science Foundation of Chongqing of China (No.\nSpringer Nature 2021 L ATEX template\nP oint Cloud Learning with Transformer 11\ncstc2020jcyj-msxmX0126), and the Fundamental Research Funds for the Central\nUniversities (No. SWU120005)\nReferences\n[1] Guo, Y ., W ang, H., Hu, Q., Liu, H., Liu, L., Bennamoun, M.: Deep learning for\n3d point clouds: A survey . IEEE transactions on pattern analysis and machine\nintelligence (2020)\n[2] Hu, Q., Y ang, B., Xie, L., Rosa, S., Guo, Y ., W ang, Z., Trigoni, N., Markham,\nA.: Randla-net: Efﬁcient semantic segmentation of large-scale point clouds.\nIn: Proceedings of the IEEE/CVF Conference on Computer V ision and Pattern\nRecognition, pp. 11108–11117 (2020)\n[3] Nezhadarya, E., T aghavi, E., Razani, R., Liu, B., Luo, J.: Adaptive hierarchical\ndown-sampling for point cloud classiﬁcation. In: Proceedings of the IEEE/CVF\nConference on Computer V ision and Pattern Recognition, pp. 12956–12964\n(2020)\n[4] W ang, Y ., Solomon, J.M.: Deep closest point: Learning representations for point\ncloud registration. In: Proceedings of the IEEE/CVF International Conference\non Computer V ision, pp. 3523–3532 (2019)\n[5] Gojcic, Z., Zhou, C., W egner, J.D., Guibas, L.J., Birdal, T .: Learning multiview\n3d point cloud registration. In: Proceedings of the IEEE/CVF Conference on\nComputer V ision and Pattern Recognition, pp. 1759–1769 (2020)\n[6] Jiang, H., Y an, F ., Cai, J., Zheng, J., Xiao, J.: End-to-end 3d point cloud instance\nsegmentation without detection. In: Proceedings of the IEEE/CVF Conference\non Computer V ision and Pattern Recognition, pp. 12796–12805 (2020)\n[7] Maturana, D., Scherer, S.: V oxnet: A 3d convolutional neural network for\nreal-time object recognition. In: 2015 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS), pp. 922–928 (2015). IEEE\n[8] Su, H., Maji, S., Kalogerakis, E., Learned-Miller, E.: Multi-view convolu-\ntional neural networks for 3d shape recognition. In: Proceedings of the IEEE\nInternational Conference on Computer V ision, pp. 945–953 (2015)\n[9] Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: Deep learning on point sets for\n3d classiﬁcation and segmentation. In: Proceedings of the IEEE Conference on\nComputer V ision and Pattern Recognition, pp. 652–660 (2017)\n[10] Qi, C.R., Yi, L., Su, H., Guibas, L.J.: Pointnet++: Deep hierarchical feature\nlearning on point sets in a metric space. In: Advances in Neural Information\nProcessing Systems, pp. 5099–5108 (2017)\nSpringer Nature 2021 L ATEX template\n12 P oint Cloud Learning with Transformer\n[11] Thomas, H., Qi, C.R., Deschaud, J.-E., Marcotegui, B., Goulette, F ., Guibas,\nL.J.: Kpconv: Flexible and deformable convolution for point clouds. In: Pro-\nceedings of the IEEE/CVF International Conference on Computer V ision, pp.\n6411–6420 (2019)\n[12] Lei, H., Akhtar, N., Mian, A.: Spherical kernel for efﬁcient graph convolu-\ntion on 3d point clouds. IEEE transactions on pattern analysis and machine\nintelligence (2020)\n[13] Wu, Z., Song, S., Khosla, A., Y u, F ., Zhang, L., T ang, X., Xiao, J.: 3d\nshapenets: A deep representation for volumetric shapes. In: Proceedings of the\nIEEE Conference on Computer V ision and Pattern Recognition, pp. 1912–1920\n(2015)\n[14] Yi, L., Kim, V .G., Ceylan, D., Shen, I., Y an, M., Su, H., Lu, C., Huang, Q., Shef-\nfer, A., Guibas, L., et al.: A scalable active framework for region annotation in\n3d shape collections. ACM Transactions on Graphics (TOG) 35(6), 210 (2016)\n[15] Fujiwara, K., Hashimoto, T .: Neural implicit embedding for point cloud anal-\nysis. In: Proceedings of the IEEE/CVF Conference on Computer V ision and\nPattern Recognition, pp. 11734–11743 (2020)\n[16] Lin, Z.-H., Huang, S.-Y ., W ang, Y .-C.F .: Convolution in the cloud: Learning\ndeformable kernels in 3d graph convolution networks for point cloud analysis.\nIn: Proceedings of the IEEE/CVF Conference on Computer V ision and Pattern\nRecognition, pp. 1800–1809 (2020)\n[17] Zhang, J., Zhu, C., Zheng, L., Xu, K.: Fusion-aware point convolution for online\nsemantic 3d scene segmentation. In: Proceedings of the IEEE/CVF Conference\non Computer V ision and Pattern Recognition, pp. 4534–4543 (2020)\n[18] Le, T ., Duan, Y .: Pointgrid: A deep network for 3d shape understanding. In: Pro-\nceedings of the IEEE Conference on Computer V ision and Pattern Recognition,\npp. 9204–9214 (2018)\n[19] Riegler, G., Osman Ulusoy , A., Geiger, A.: Octnet: Learning deep 3d rep-\nresentations at high resolutions. In: Proceedings of the IEEE Conference on\nComputer V ision and Pattern Recognition, pp. 3577–3586 (2017)\n[20] Klokov , R., Lempitsky , V .: Escape from cells: Deep kd-networks for the recog-\nnition of 3d point cloud models. In: Proceedings of the IEEE International\nConference on Computer V ision, pp. 863–872 (2017)\n[21] Y ou, Y ., Lou, Y ., Liu, Q., T ai, Y .-W ., Ma, L., Lu, C., W ang, W .: Pointwise\nrotation-invariant network with adaptive sampling and 3d spherical voxel con-\nvolution. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence,\nvol. 34, pp. 12717–12724 (2020)\nSpringer Nature 2021 L ATEX template\nP oint Cloud Learning with Transformer 13\n[22] Zhang, C., Song, Y ., Y ao, L., Cai, W .: Shape-oriented convolution neural net-\nwork for point cloud analysis. In: Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, vol. 34, pp. 12773–12780 (2020)\n[23] Y ang, J., Lee, C., Ahn, P ., Lee, H., Yi, E., Kim, J.: Pbp-net: Point projection\nand back-projection network for 3d point cloud segmentation. arXiv preprint\narXiv:2011.00988 (2020)\n[24] Lin, Y ., Y an, Z., Huang, H., Du, D., Liu, L., Cui, S., Han, X.: Fpconv: Learn-\ning local ﬂattening for point convolution. In: Proceedings of the IEEE/CVF\nConference on Computer V ision and Pattern Recognition, pp. 4293–4302\n(2020)\n[25] T e, G., Hu, W ., Zheng, A., Guo, Z.: Rgcnn: Regularized graph cnn for point\ncloud segmentation. In: 2018 ACM Multimedia Conference on Multimedia\nConference, pp. 746–754 (2018). ACM\n[26] Simonovsky , M., Komodakis, N.: Dynamic edge-conditioned ﬁlters in convo-\nlutional neural networks on graphs. In: Proceedings of the IEEE Conference on\nComputer V ision and Pattern Recognition, pp. 3693–3702 (2017)\n[27] W ang, Y ., Sun, Y ., Liu, Z., Sarma, S.E., Bronstein, M.M., Solomon,\nJ.M.: Dynamic graph cnn for learning on point clouds. arXiv preprint\narXiv:1801.07829 (2018)\n[28] W ang, L., Huang, Y ., Hou, Y ., Zhang, S., Shan, J.: Graph attention convolution\nfor point cloud semantic segmentation. In: Proceedings of the IEEE Conference\non Computer V ision and Pattern Recognition, pp. 10296–10305 (2019)\n[29] Khan, S., Naseer, M., Hayat, M., Zamir, S.W ., Khan, F .S., Shah, M.: Transform-\ners in vision: A survey . arXiv preprint arXiv:2101.01169 (2021)\n[30] Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., Sutskever, I.:\nGenerative pretraining from pixels. In: International Conference on Machine\nLearning, pp. 1691–1703 (2020). PMLR\n[31] Dosovitskiy , A., Beyer, L., Kolesnikov , A., W eissenborn, D., Zhai, X.,\nUnterthiner, T ., Dehghani, M., Minderer, M., Heigold, G., Gelly , S., et al.: An\nimage is worth 16x16 words: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929 (2020)\n[32] Carion, N., Massa, F ., Synnaeve, G., Usunier, N., Kirillov , A., Zagoruyko, S.:\nEnd-to-end object detection with transformers. In: European Conference on\nComputer V ision, pp. 213–229 (2020). Springer\n[33] Zhang, D., Zhang, H., T ang, J., W ang, M., Hua, X., Sun, Q.: Feature pyra-\nmid transformer. In: European Conference on Computer V ision, pp. 323–339\nSpringer Nature 2021 L ATEX template\n14 P oint Cloud Learning with Transformer\n(2020). Springer\n[34] Huang, Z., Y u, Y ., Xu, J., Ni, F ., Le, X.: Pf-net: Point fractal network for 3d point\ncloud completion. In: Proceedings of the IEEE/CVF Conference on Computer\nV ision and Pattern Recognition, pp. 7662–7670 (2020)\n[35] Feng, Y ., Zhang, Z., Zhao, X., Ji, R., Gao, Y .: Gvcnn: Group-view convolu-\ntional neural networks for 3d shape recognition. In: Proceedings of the IEEE\nConference on Computer V ision and Pattern Recognition, pp. 264–272 (2018)\n[36] Ravanbakhsh, S., Schneider, J., Poczos, B.: Deep learning with sets and point\nclouds. arXiv preprint arXiv:1611.04500 (2016)\n[37] Ben-Shabat, Y ., Lindenbaum, M., Fischer, A.: 3dmfv: Three-dimensional point\ncloud classiﬁcation in real-time using convolutional neural networks. IEEE\nRobotics and Automation Letters 3(4), 3145–3152 (2018)\n[38] Y ang, Y ., Feng, C., Shen, Y ., Tian, D.: Foldingnet: Point cloud auto-encoder via\ndeep grid deformation. In: Proceedings of the IEEE Conference on Computer\nV ision and Pattern Recognition, pp. 206–215 (2018)\n[39] Shen, Y ., Feng, C., Y ang, Y ., Tian, D.: Mining point cloud local structures by\nkernel correlation and graph pooling. In: Proceedings of the IEEE Conference\non Computer V ision and Pattern Recognition, pp. 4548–4557 (2018)\n[40] Li, Y ., Bu, R., Sun, M., Wu, W ., Di, X., Chen, B.: Pointcnn: Convolution on\nx-transformed points. In: Advances in Neural Information Processing Systems,\npp. 820–830 (2018)\n[41] Atzmon, M., Maron, H., Lipman, Y .: Point convolutional neural networks by\nextension operators. arXiv preprint arXiv:1803.10091 (2018)\n[42] Xie, S., Liu, S., Chen, Z., Tu, Z.: Attentional shapecontextnet for point cloud\nrecognition. In: Proceedings of the IEEE Conference on Computer V ision and\nPattern Recognition, pp. 4606–4615 (2018)\n[43] W ang, C., Samari, B., Siddiqi, K.: Local spectral graph convolution for point\nset feature learning. In: Proceedings of the European Conference on Computer\nV ision (ECCV), pp. 52–66 (2018)\n[44] Duan, Y ., Zheng, Y ., Lu, J., Zhou, J., Tian, Q.: Structural relational reasoning of\npoint clouds. In: Proceedings of the IEEE Conference on Computer V ision and\nPattern Recognition, pp. 949–958 (2019)\n[45] Han, W ., W en, C., W ang, C., Li, X., Li, Q.: Point2node: Correlation learning of\ndynamic-node for point cloud feature modeling. In: Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, vol. 34, pp. 10925–10932 (2020)\nSpringer Nature 2021 L ATEX template\nP oint Cloud Learning with Transformer 15\n[46] Liu, X., Han, Z., Liu, Y .-S., Zwicker, M.: Point2sequence: Learning the shape\nrepresentation of 3d point clouds with an attention-based sequence to sequence\nnetwork. In: AAAI (2019)\n[47] Wu, W ., Qi, Z., Fuxin, L.: Pointconv: Deep convolutional networks on 3d point\nclouds. In: Proceedings of the IEEE/CVF Conference on Computer V ision and\nPattern Recognition, pp. 9621–9630 (2019)\n[48] Lei, H., Akhtar, N., Mian, A.: Octree guided cnn with spherical kernels for 3d\npoint clouds. In: Proceedings of the IEEE Conference on Computer V ision and\nPattern Recognition (2019)\n[49] Zhao, H., Jiang, L., Jia, J., T orr, P ., Koltun, V .: Point transformer. arXiv preprint\narXiv:2012.09164 (2020)\n[50] Qiu, S., Anwar, S., Barnes, N.: Dense-resolution network for point cloud classi-\nﬁcation and segmentation. In: Proceedings of the IEEE/CVF Winter Conference\non Applications of Computer V ision, pp. 3813–3822 (2021)\n[51] Engel, N., Belagiannis, V ., Dietmayer, K.: Point transformer. arXiv preprint\narXiv:2011.00931 (2020)\n[52] Guo, M.-H., Cai, J.-X., Liu, Z.-N., Mu, T .-J., Martin, R.R., Hu, S.-M.: Pct: Point\ncloud transformer. arXiv preprint arXiv:2012.09688 (2020)\n[53] Cheng, S., Chen, X., He, X., Liu, Z., Bai, X.: Pra-net: Point relation-aware\nnetwork for 3d point cloud analysis. IEEE Transactions on Image Processing\n30, 4436–4448 (2021)\n[54] Xu, M., Ding, R., Zhao, H., Qi, X.: Paconv: Position adaptive convolution with\ndynamic kernel assembling on point clouds. In: Proceedings of the IEEE/CVF\nConference on Computer V ision and Pattern Recognition, pp. 3173–3182\n(2021)\n[55] Xu, Y ., Fan, T ., Xu, M., Zeng, L., Qiao, Y .: Spidercnn: Deep learning on point\nsets with parameterized convolutional ﬁlters. In: Proceedings of the European\nConference on Computer V ision (ECCV), pp. 87–102 (2018)\n[56] Rao, Y ., Lu, J., Zhou, J.: Spherical fractal convolutional neural networks for\npoint cloud recognition. In: Proceedings of the IEEE Conference on Computer\nV ision and Pattern Recognition, pp. 452–460 (2019)\n[57] Zhao, H., Jiang, L., Fu, C.-W ., Jia, J.: Pointweb: Enhancing local neighbor-\nhood features for point cloud processing. In: Proceedings of the IEEE/CVF\nConference on Computer V ision and Pattern Recognition, pp. 5565–5573\n(2019)\nSpringer Nature 2021 L ATEX template\n16 P oint Cloud Learning with Transformer\n[58] Li, J., Chen, B.M., Hee Lee, G.: So-net: Self-organizing network for point cloud\nanalysis. In: Proceedings of the IEEE Conference on Computer V ision and\nPattern Recognition, pp. 9397–9406 (2018)\n[59] Xu, X., Lee, G.H.: W eakly supervised semantic point cloud segmentation:\nT owards 10x fewer labels. In: Proceedings of the IEEE/CVF Conference on\nComputer V ision and Pattern Recognition, pp. 13706–13715 (2020)\n[60] Qiu, S., Anwar, S., Barnes, N.: Geometric back-projection network for point\ncloud classiﬁcation. IEEE Transactions on Multimedia, 1–1 (2021).\nhttps://doi.\norg/10.1109/TMM.2021.3074240",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7450985312461853
    },
    {
      "name": "Computer science",
      "score": 0.6915295124053955
    },
    {
      "name": "Point cloud",
      "score": 0.6677183508872986
    },
    {
      "name": "Segmentation",
      "score": 0.6460784673690796
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4836953282356262
    },
    {
      "name": "Feature learning",
      "score": 0.4411352276802063
    },
    {
      "name": "Machine learning",
      "score": 0.3565225601196289
    },
    {
      "name": "Engineering",
      "score": 0.16988104581832886
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I142108993",
      "name": "Southwest University",
      "country": "CN"
    }
  ],
  "cited_by": 29
}