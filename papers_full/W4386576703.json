{
  "title": "Does Transliteration Help Multilingual Language Modeling?",
  "url": "https://openalex.org/W4386576703",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5070392506",
      "name": "Ibraheem Muhammad Moosa",
      "affiliations": [
        "Pennsylvania State University"
      ]
    },
    {
      "id": "https://openalex.org/A5043723724",
      "name": "Mahmud Elahi Akhter",
      "affiliations": [
        "North South University"
      ]
    },
    {
      "id": "https://openalex.org/A5058671564",
      "name": "Ashfia Binte Habib",
      "affiliations": [
        "North South University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2942810103",
    "https://openalex.org/W2964114970",
    "https://openalex.org/W3028807187",
    "https://openalex.org/W3183430109",
    "https://openalex.org/W2107026277",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3169244955",
    "https://openalex.org/W4294103325",
    "https://openalex.org/W3099919888",
    "https://openalex.org/W2985620815",
    "https://openalex.org/W3199212312",
    "https://openalex.org/W3173172013",
    "https://openalex.org/W2325864482",
    "https://openalex.org/W2899641520",
    "https://openalex.org/W3003257820",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W2970316683",
    "https://openalex.org/W4252684946",
    "https://openalex.org/W2889511806",
    "https://openalex.org/W3037312903",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W4287633642",
    "https://openalex.org/W3197797121",
    "https://openalex.org/W3156326119",
    "https://openalex.org/W3169369929",
    "https://openalex.org/W3035547806",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W1974292291",
    "https://openalex.org/W3100198908",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2790325757",
    "https://openalex.org/W2560939934",
    "https://openalex.org/W4297730150",
    "https://openalex.org/W4230579319",
    "https://openalex.org/W3037582736",
    "https://openalex.org/W3197876970",
    "https://openalex.org/W3173954987",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W2995230342",
    "https://openalex.org/W3034199299",
    "https://openalex.org/W2739967986",
    "https://openalex.org/W3035032094",
    "https://openalex.org/W1972978214",
    "https://openalex.org/W3037373636",
    "https://openalex.org/W2964144280"
  ],
  "abstract": "Script diversity presents a challenge to Multilingual Language Models (MLLM) by reducing lexical overlap among closely related languages. Therefore, transliterating closely related languages that use different writing scripts to a common script may improve the downstream task performance of MLLMs. We empirically measure the effect of transliteration on MLLMs in this context. We specifically focus on the Indic languages, which have the highest script diversity in the world, and we evaluate our models on the IndicGLUE benchmark. We perform the Mann-Whitney U test to rigorously verify whether the effect of transliteration is significant or not. We find that transliteration benefits the low-resource languages without negatively affecting the comparatively high-resource languages. We also measure the cross-lingual representation similarity of the models using centered kernel alignment on parallel sentences from the FLORES-101 dataset. We find that for parallel sentences across different languages, the transliteration-based model learns sentence representations that are more similar.",
  "full_text": "Findings of the Association for Computational Linguistics: EACL 2023, pages 670–685\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nDoes Transliteration Help Multilingual Language Modeling?\nIbraheem Muhammad Moosa\nPennsylvania State University\nibraheem.moosa@psu.edu\nMahmud Elahi Akhter and Ashfia Binte Habib\nDept. of Electrical and Computer Engineering,\nNorth South University, Dhaka, Bangladesh\n{mahmud.akhter01,ashfia.habib}@northsouth.edu\nAbstract\nScript diversity presents a challenge to Multi-\nlingual Language Models (MLLM) by reduc-\ning lexical overlap among closely related lan-\nguages. Therefore, transliterating closely re-\nlated languages that use different writing scripts\nto a common script may improve the down-\nstream task performance of MLLMs. We em-\npirically measure the effect of transliteration on\nMLLMs in this context. We specifically focus\non the Indic languages, which have the highest\nscript diversity in the world, and we evaluate\nour models on the IndicGLUE benchmark. We\nperform the Mann-Whitney U test to rigorously\nverify whether the effect of transliteration is\nsignificant or not. We find that transliteration\nbenefits the low-resource languages without\nnegatively affecting the comparatively high-\nresource languages. We also measure the cross-\nlingual representation similarity of the models\nusing centered kernel alignment on parallel sen-\ntences from the FLORES-101 dataset. We find\nthat for parallel sentences across different lan-\nguages, the transliteration-based model learns\nsentence representations that are more similar.\n1 Introduction\nIn the last few years, we have seen impressive ad-\nvances in many NLP tasks. These advances have\nbeen primarily led by the availability of large rep-\nresentative corpora and improvement in the archi-\ntecture of large language models. While improving\nmodel architectures, training methods, regulariza-\ntion techniques, etc., can help advance the state of\nNLP in general, the unavailability of large, diverse\ncorpora is the bottleneck for most languages (Joshi\net al., 2020). Thus to inclusively advance the state\nof NLP across languages, it is crucial to develop\ntechniques for training MLLMs that can extract the\nmost out of existing multilingual corpora. Here, we\nfocus on the issue of diverse writing scripts used by\nclosely related languages that may prevent MLLMs\nfrom learning good cross-lingual representations.\nPrevious papers (Pfeiffer et al., 2021) have noted\nthat low-resource languages that use unique scripts\ntend to have very few tokens representing them at\nthe tokenizer. As a result, these languages tend\nto have more UNKnown tokens, and the words in\nthese languages tend to be more split up by sub-\nword tokenizers. Often we can easily transliterate\nfrom one script to another using rule-based sys-\ntems. For example, there are established standards\nthat can be used to transliterate Greek (ISO 843),\nCyrillic (ISO 9), Indic scripts (ISO 15919), and\nThai (ISO 11940) to the Latin script.\nIn this paper, we focus on the Indic languages,\nwhich have the highest script diversity in the world.\nMany South Asian and Southeast Asian languages\nare intimately connected linguistically, historically,\nphonologically (Littell et al., 2017) and phyloge-\nnetically. However, due to different scripts, it is\ndifficult for MLLMs to fully exploit this shared\ninformation. Among the Indic languages we con-\nsidered in this study we encounter eleven different\nscripts. These are shown in Table 1. Nevertheless,\nthese scripts have shared ancestry from the ancient\nBrahmic script (Hockett et al., 1997; Coningham\net al., 1996) and have similar structures that we can\neasily use to transliterate them to a common script.\nAlso, many of these languages heavily borrow from\nSanskrit, and due to its influence, many words are\nshared among these languages. Therefore, due to\ntheir relatedness and highly diverse script barrier,\nthe Indic languages presents a unique opportunity\nto analyze the effects of transliteration on MLLMs.\nWe empirically measure the effect of translitera-\ntion on the downstream performance of MLLMs.\nWe pretrain ALBERT (Base, 11M Parameters)\n(Lan et al., 2020) and RemBERT (Base, 192M Pa-\nrameters) (Chung et al., 2020) models from scratch\n670\non Indic languages. We pretrain two variants of\neach model, one with the original writing scripts\nand the other after transliterating to a common\nwriting script. Henceforth, we will refer to the\ntransliterated script model as uni-script model and\nthe other as a multi-script model. We evaluate the\nmodels on downstream tasks from the IndicGLUE\nbenchmark dataset (Kakwani et al., 2020). In order\nto rigorously compare the two models, we finetune\nusing nine random seeds on all downstream tasks.\nThen we perform the Mann-Whitney U test (MWU)\nbetween the uni-script and multi-script models. Us-\ning the MWU test, we conclude that transliter-\nation significantly benefits the low-resource lan-\nguages without negatively affecting the compara-\ntively high-resource languages.\nWe also measure the Cross-Lingual Representa-\ntion Similarity (CLRS) to understand why the uni-\nscript model performs better than the multi-script\nmodel. To measure the CLRS, we use the centered\nkernel alignment (CKA) (Kornblith et al., 2019)\nsimilarity score. We measure the CKA similarity\nscore between the hidden representations of the\nmodels on the parallel sentences of the Indic lan-\nguages from the FLORES-101 dataset (Goyal et al.,\n2022). We find that, compared to the multi-script\nmodels, the uni-script models achieve a higher\nCKA score, and it is more stable throughout the\nhidden layers of the models. Based on this, we\nconclude that the uni-script models learn better\ncross-lingual representation than the multi-script\nmodels. In summary, our contributions are primar-\nily three-fold:\n1. We find that transliteration significantly ben-\nefits the low-resource languages without\nnegatively affecting the comparatively high-\nresource languages.\n2. We establish this finding through rigorous ex-\nperiments and show the statistical significance\nalong with the effect size of transliteration\nusing the Mann-Whitney U test.\n3. Using CKA on the FLORES-101 dataset, we\nshow that transliteration helps MLLMs learn\nbetter cross-lingual representation.\nOur code is available at Github 1 and our model\n1https://github.com/ibraheem-moosa/\nXLM-Indic\nweights can be downloaded from HF Hub 2 3 4 5 .\n2 Motivation and Background\n2.1 Motivation\nIn their study, Joshi et al. (2020) showed the re-\nsource disparity between low-resource and high-\nresource languages, and Ruder (2020) discussed\nthe necessity of working with low-resource lan-\nguages. A large body of work suggests that\nlanguage-relatedness can help MLLMs achieve\nbetter performance on low-resource languages by\nleveraging related high-resource languages. For\ninstance, Pires et al. (2019) found that lexical over-\nlap improved mBERT’s multilingual representation\ncapability even though it learned to capture multi-\nlingual representations with zero lexical overlaps.\nDabre et al. (2017) showed that transfer learning\nin the same or linguistically similar language fam-\nily gives the best performance for NMT. Lauscher\net al. (2020) found that language relatedness is\ncrucial for POS-tagging and dependency parsing\ntasks. Although, corpus size is much more impor-\ntant for NLI and Question Answering tasks. Wu\nand Dredze (2020) showed that bilingual BERT\noutperformed monolingual BERT on low-resource\nlanguages when the languages were linguistically\nclosely related. Nevertheless, mBERT outper-\nformed bilingual BERT on low-resource languages.\n2.2 Script Barrier in Multilingual Language\nModels\nOne of the major challenges in leveraging trans-\nfer between high-resource and low-resource lan-\nguages is overcoming the script barrier. Script bar-\nrier exists when multiple closely related languages\nuse different scripts. Anastasopoulos and Neu-\nbig (2019) found that for morphological inflection,\nscript barrier between closely related languages\nimpedes cross-lingual learning, and language re-\nlatedness improved cross-lingual transfer. Translit-\neration and phoneme-based techniques have been\nproposed to solve this issue. For example, Muriki-\nnati et al. (2020) expanded upon Anastasopoulos\nand Neubig (2019) and showed that both transliter-\n2https://huggingface.co/ibraheemmoosa/\nxlmindic-base-uniscript\n3https://huggingface.co/ibraheemmoosa/\nxlmindic-base-multiscript\n4https://huggingface.co/ibraheemmoosa/\nxlmindic-rembert-uniscript\n5https://huggingface.co/ibraheemmoosa/\nxlmindic-rembert-multiscript\n671\nation and grapheme to phoneme (g2p) conversion\nremoves script barrier and improves cross-lingual\nmorphological inflection and Rijhwani et al. (2019)\nshowed that pivoting low-resource languages to\ntheir closely related high-resource languages re-\nsults in better zero shot entity linking capacity\nand used phoneme-based pivoting to overcome\nthe script barrier. Bharadwaj et al. (2016) showed\nthat phoneme representation outperformed ortho-\ngraphic representations for NER. Chaudhary et al.\n(2018) also used phoneme representation to resolve\nscript barriers and adapt word embeddings to low-\nresource languages.\n2.3 Transliteration in Language Modeling\nDifferent works have applied transliteration in dif-\nferent aspect for language models. For instance,\nGoyal et al. (2020) and Song et al. (2020) both uti-\nlized transliteration and showed that language re-\nlatedness was required for improving performance\non NMT. Amrhein and Sennrich (2020) studied\nhow transliteration improved NMT and came to\nthe conclusion that transliteration offered signifi-\ncant improvement for low-resource languages with\ndifferent scripts.\nKhemchandani et al. (2021) showed on Indo-\nAryan languages that language relatedness could be\nexploited through transliteration along with bilin-\ngual lexicon-based pseudo-translation and aligned\nloss to incorporate low-resource languages into\npretrained mBERT. Muller et al. (2021) showed\nthat for unseen languages, the script barrier hin-\ndered transfer between low-resource and high-\nresource languages for MLLMs and transliteration\nremoved this barrier. They showed that translit-\nerating Uyghur, Buryat, Erzya, Sorani, Meadow\nMari, and Mingrelian to Latin script and finetun-\ning mBERT on the respective corpus with masked\nlanguage modeling objective improved their down-\nstream POS performance significantly. In contrast,\nK et al. (2020) and Artetxe et al. (2020) proposes\nthat mBERT can learn cross-lingual representa-\ntions without any lexical overlap, a shared vocabu-\nlary, or joint training. However, these works focus\non zero-shot cross-lingual transfer learning only.\nFrom the literature, it can be seen that many in\nthe community believe transliteration to be a po-\ntential solution for script barriers. However, most\nof the work shows the benefits of transliteration\nfor NMT. Nevertheless, there is no solid empirical\nanalysis of the effects of transliteration for MLLMs\napart from Dhamecha et al. (2021); Muller et al.\n(2021). Hence, the motivation behind this paper is\nto provide a solid empirical analysis of the effect of\ntransliteration for MLLMs with statistical analysis\nand determine whether or not it helps models learn\nbetter cross-lingual representation.\nIt should also be noted that, even though our\nidea seems to be similar to Muller et al. (2021)\nand Dhamecha et al. (2021), there are major differ-\nences. For instance, Muller et al. (2021) adapted\nexisting pretrained model to very low-resource lan-\nguages. Whereas, we focus on training the models\nwith transliteration from scratch. We also train our\nmodels on 20 languages and evaluate on more than\n50 tasks. Unlike Dhamecha et al. (2021), we also\ninclude Dravidian Languages in our analysis. Fur-\nthermore, we focused on the issue of script barrier\nwhile Dhamecha et al. (2021) focused on multilin-\ngual fine-tuning. Whereas, we adopt multilingual\nfine-tuning on all our models. Thus the improve-\nment we see comes only from circumventing the\nscript barrier. Moreover, we have provided statis-\ntical testing to show the significance of translitera-\ntion instead of just showing better metrics. We also\nperformed cross-lingual representation similarity\nanalysis to show the benefits of transliteration.\n2.4 Cross Lingual Similarity Learning in\nLanguage Modeling\nSeveral techniques have recently been used to study\nthe hidden representations of multilingual language\nmodels. Kudugunta et al. (2019) study CLRS of\nNMT models using SVCCA (Raghu et al., 2017).\nSingh et al. (2019) used PWCCA (Morcos et al.,\n2018) to study the CLRS of mBERT and found\nthat it drastically fell with depth. (Conneau et al.,\n2020) have used CKA to study the CLRS of bilin-\ngual BERT models. They found that similarity is\nhighest in the first few layers and drops moder-\nately with depth. Müller et al. (2021) used CKA\nto study CLRS of mBERT before and after finetun-\ning on downstream tasks. They found in all cases\nthat CLRS increases steadily in the first five layers,\nthen it decreases in the later layers. From this, they\nconcluded that mBERT learns multilingual align-\nment in the early layers and preserves it throughout\nfinetuning. Del and Fishel (2021) applied various\nsimilarity measures to understand CLRS of vari-\nous multilingual masked language models. Their\nresults also show that CLRS increases in the first\nhalf of the models, while in the later layers, this\n672\nsimilarity steadily falls.\n3 Experiment and Results\n3.1 Mann–Whitney U test\nWe perform Mann–Whitney U test (MWU) (Mann\nand Whitney, 1947; Wilcoxon, 1945) to determine\nif the performance differences between the multi-\nscript and the uni-script models are significant. In\nshort, it tells us the effect of transliteration on\nmodel performance. MWU is a non-parametric\nhypothesis test between two groups/populations.\nMWU is chosen because it has weak assumptions.\nThe only assumptions of MWU are that the samples\nof the two groups are independent of each other,\nand the samples are ordinal. Under the MWU, our\nnull hypothesis or h0 is that the performances of\nthe uni-script (group 1) and the multi-script (group\n2) models are similar, and the alternative hypoth-\nesis or ha is that the performances (groups) are\ndifferent. We set our confidence interval αat 0.05\nand reject the h0 for the p-values < α. We also\nreport three test statistics as the p-value only gives\nstatistical significance, which can be misleading at\ntimes (Sullivan and Feinn, 2012).\nThe test statistics are three different effect sizes\nthat convey three different information. These test\nstatistics are absolute effect size (δ), common lan-\nguage effect size (ρ), and standardized effect size\n(r). The absolute effect size δis the difference be-\ntween the mean of the models’ performance metric,\nwhich is given as,\nδ= µuni-script-µmulti-script\nfor any given task and language. When the h0 is\nrejected for any given task, a positive δindicates\nthe uni-script model is better, and a negative δindi-\ncates the multi-script model is better. The details\nand results of common language effect size ( ρ),\nand standardized effect size ( r) are presented in\nappendix D.\n3.2 Dataset\nThe ALBERT models were pretrained on a sub-\nset of the OSCAR corpus containing Indo-Aryan\nlanguages. We use the unshuffled deduplicated ver-\nsion of OSCAR corpus (Ortiz Su’arez et al., 2019)\navailable via Huggingface datasets library (Lhoest\net al., 2021). We pretrain on Panjabi, Hindi, Ben-\ngali, Oriya, Assamese, Gujarati, Marathi, Sinhala,\nNepali, Sanskrit, Goan Konkani, Maithili, Bihari,\nand Bishnupriya portion of the OSCAR corpus.\nThe RemBERT models were trained on a signifi-\ncantly larger pretraining corpus with additional lan-\nguages. We pretrained the RemBERT models on a\ncombination of Wikipedia (Foundation), mC4 (Raf-\nfel et al., 2019), OSCAR2109 (Abadji et al., 2021)\nand OSCAR corpus. These datasets are also avail-\nable via the Huggingface datasets library. In addi-\ntion to the languages in the ALBERT pretraining\ncorpus, we consider English, four Dravidian lan-\nguages Kannada, Telugu, Malayalam, and Tamil,\nand an Indo-Aryan language Dhivehi. We evalu-\nLang. Sub-family Script Size(GB)\nen Germanic Latin 131\nhi Central Indo-Aryan Devanagari 43\nmr Southern Indo-Aryan Devanagari 35\nbn Eastern Indo-Aryan Bengali 28\nta South Dravidian Tamil 22\nml South Dravidian Malayalam 10\nte South-Central Dravidian Telugu 7\nkn South Dravidian Kannada 6\nsi Insular Indo-Aryan Sinhala 5\nne Northern Indo-Aryan Devanagari 4\ngu Western Indo-Aryan Gujarati 3.5\npa Northwestern Indo-Aryan Gurmukhi 2\nor Eastern Indo-Aryan Oriya 0.5\nsa Sanskrit Devanagari 0.2\nas Eastern Indo-Aryan Bengali 0.1\ndv Insular Indo-Aryan Thaana 0.1\nbpy Eastern Indo-Aryan Bengali < 0.1\ngom Southern Indo-Aryan Devanagari < 0.1\nbh Eastern Indo-Aryan Devanagari < 0.1\nmai Eastern Indo-Aryan Devanagari < 0.1\nTable 1: Languages in our pretraining corpus and their\nwriting scripts and the pretraining corpus sizes used for\nthe RemBERT model\nate the models on four downstream tasks from In-\ndicGLUE (Kakwani et al., 2020), which are News\nArticle Classification, WSTP, CSQA, and NER.\nWe use the balanced Wikiann dataset from Rahimi\net al. (2019) for NER. In addition, we evaluate the\nmodels on other publicly available datasets that\nare part of the IndicGLUE benchmark. These are\nBBC Hindi News Classification, Soham Bengali\nNews Classification, INLTK Headlines Classifi-\ncation, IITP Movie, and Product Review Senti-\nment Analysis (Akhtar et al., 2016), MIDAS Dis-\ncourse Mode Classification (Dhanwal et al., 2020)\nand ACTSA Sentiment Classification (Mukku and\nMamidi, 2017) datasets.\n3.3 Transliteration Method\nWe transliterate Indic language texts to Latin script\nusing the ISO 15919 transliteration scheme. We\ntested with two publicly available implementations\nof this scheme, Aksharamukha (Rajan, 2015) and\nPyICU (PyICU). We found the quality of translit-\n673\neration of the Aksharamukha library to be better.\nThus we use this library for transliterating the in-\nputs to the ALBERT uni-script model. However,\nthe Aksharamukha implementation is very slow\ncompared to the PyICU implementation. As we\nsignificantly expanded our pretraining corpus for\nthe RemBERT model, we switched to PyICU for\nthe RemBERT uni-script model.\n3.4 Downstream Finetuning\nWe finetune the models on each downstream task\nindependently. The specific hyperparameters used\nfor each task are reported in the appendix B. On\nall tasks, we finetune with nine random seeds and\nreport the average and standard deviation of the\nmetrics. In Table 2 and Table 4, we report the per-\nformances on IndicGLUE benchmark tasks and in\nTable 3 on other publicly available datasets. Here,\nwe discuss the results on each of the the models on\neach of the tasks. Furthermore, in appendix D, we\nshow the test statistics for all the datasets.\nWikipedia Section Title Prediction: For both\nRemBERT and ALBERT models, the uni-script\nmodel performed better on all languages except\nMalayalam (ml). We noticed that a letter of Malay-\nalam script is not properly transliterated by the\nPyICU library. This introduced some artifacts in\nthe form of unnecessary splitting of words by the\nsubword tokenizer.\nNews Category Classification: It is interesting\nthat on this task the uni-script models performed\nbetter for Panjabi (pa) and Oriya (or) languages.\nIt is clear from Table 1 that these two languages\nare low-resource compared to Bengali (bn) and\nMarathi (mr). On Bengali and Marathi we see\nslight performance degradation which is not statis-\ntically significant. This shows the validity of our\nfirst finding.\nNamed Entity Recognition: On this task we\nsee that the uni-script model performs much bet-\nter for Assamese (as), Oriya(or), Panjabi (pa) and\nGujarati (gu). These languages are low-resource\nand here again the uni-script model shines. The\nlarge performance improvement on this task can\nbe explained by the fact that Named Entities usu-\nally have the same spelling after transliteration for\nIndian languages. Thus the uni-script model has\nbetter chances for learning various named-entities\nduring pre-training.\nArticle Genre, Sentiment & Discourse Mode\nClassification: We evaluate the models on various\nother sequence classification datasets that are part\nof the IndicGLUE benchmark. Here again the uni-\nscript model usually performs better than the multi-\nscript model. However for two tasks in Malayalam\n(ml) and Tamil (ta) we see better performance for\nthe multi-script model. We already mentioned that\nthere is some tokenization issue for Malayalam\nwhich can explain the results for Malayalam. The\nresults for Tamil suggests that it may be a good\nidea to try both uni-script and multi-script model if\nthey are available to see which performs best on a\nparticular task. However this is the only instance of\na task where we see the multi-script model perform\nbetter.\n3.5 Zero Shot Capability Testing\nWe use the CSQA task to test the zero-shot capabil-\nity of the models as we can use the models without\nfinetuning. This task is designed to test whether\nlanguage models can be used as knowledge bases\n(Petroni et al., 2019). In Table 4 we report the re-\nsults. We note that the RemBERT models perform\nmuch better than the ALBERT models on this task.\nThis is expected as the ALBERT models’ memo-\nrization capability is hampered by weight sharing.\nThe ALBERT uni-script model is better on all\nlanguages compared to the ALBERT multi-script\nmodel. This shows the potential of a uni-script\nmodel in a restricted low parameter situation. For\nthe RemBERT models, the results are mixed. How-\never, on average the uni-script model performs bet-\nter than the multi-script model. The worst results\nare for Malayalam (ml) which as we mentiond be-\nfore has some tokenization issues.\n4 Cross-lingual Representation Similarity\nIn this section, we analyze why the uni-script model\nperforms better than the multi-script model from\nthe perspective of Cross-Lingual Representation\nSimilarity. Following (Müller et al., 2021), (Con-\nneau et al., 2020) and (Del and Fishel, 2021) we ap-\nply CKA to measure CLRS. We use the CKA imple-\nmentation from the Ecco library (Alammar, 2021).\nWe use parallel sentences on thirteen languages\nfrom the FLORES-101 (Goyal et al., 2022) dataset.\nFor the ALBERT models, which are trained on\nonly the Indo-Aryan languages, we only consider\nPanjabi, Hindi, Bengali, Oriya, Assamese, Gujarati,\nMarathi, and Nepali sentences. For the RemBERT\nmodels, we additionally consider Kannada, Telugu,\nMalayalam, Tamil, and English sentences.\n674\nModel pa hi bn or as gu mr kn te ml ta avg\nWikipedia Section Title PredictionRemBERTMS 68.42±0.92 70.90±0.39 72.58±0.45 69.92±0.90 68.37±1.37 72.93±0.58 73.23±0.61 71.67±0.41 92.98±0.19 69.03±0.57 69.77±0.45 73.00RemBERTUS 71.01±0.22 72.45±0.29 73.65±0.21 75.37±0.69 72.50±0.91 76.35±0.29 74.58±0.72 74.21±0.29 93.66±0.09 69.33±0.35 70.63±0.22 74.89δ 2.59 1.55 1.07 5.45 4.13 3.42 1.34 2.54 0.68 0.31 0.86 1.89p−value 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0035 0.0004 0.0004 0.2505 0.0006 -\nALBERTMS 74.33±0.83 78.18±0.33 81.18±0.28 74.35±1.2 76.70±0.83 76.37±0.53 79.10±0.84 - - - - 77.17ALBERTUS 77.55±0.61 82.24±0.18 84.38±0.29 81.47±0.99 81.74±0.82 82.39±0.27 82.74±0.52 - - - - 81.78δ 3.22 4.06 3.20 7.12 5.04 6.02 3.64 - - - - 4.61p−value 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 - - - - -\nNews Category ClassificationRemBERTMS 95.67±0.38 - 97.90±0.17 96.59±0.18 - 98.22±0.58 99.16±0.16 97.23±0.10 99.03±0.12 91.25±0.43 97.33±0.18 96.93RemBERTUS 96.92±0.29 - 97.78±0.12 97.55±0.14 - 99.02±0.14 99.14±0.21 97.10±0.12 99.03±0.66 92.08±0.40 97.49±0.20 97.34δ 1.24 - -0.11 0.95 - 0.80 -0.03 -0.13 0.00 0.83 0.16 0.41p−value 0.0003 - 0.0981 0.0004 - 0.0040 0.7783 0.0995 0.7548 0.0014 0.0814 -\nALBERTMS 96.83±0.19 - 98.14±0.14 98.09±0.16 - 98.80±0.43 99.58±0.25 - - - - 98.30ALBERTUS 97.90±0.17 - 97.99±0.22 98.77±0.12 - 99.40±0.54 99.47±0.21 - - - - 98.70δ 1.07 - -0.15 0.68 - 0.60 -0.18 - - - - 0.40p−value 0.0003 - 0.181 0.0004 - 0.03084 0.1683 - - - - -\nNamed Entity Recognition (F1-Score)RemBERTMS 69.47±1.72 90.95±0.33 95.51±0.18 87.92±1.26 79±0.22 69±0.94 90.72±0.17 72.65±1.81 81.82±1.81 89.17±0.25 90.07±0.33 83.40RemBERTUS 81.91±1.93 91.73±0.39 96.19±0.21 88.92±2.88 83.50±2.75 80.25±1.42 90.75±0.35 78.98±1.50 84.97±0.45 89.26±0.46 90.18±0.27 86.97δ 12.44 0.78 0.68 1.00 4.28 10.31 0.02 6.33 3.15 0.01 0.12 3.56p−value 0.00004 0.0005 0.00001 0.1615 0.0019 0.00004 0.6665 0.00004 0.00004 0.7304 0.2973 -\nALBERTMS 76.69±1.5 91.80±0.42 96.39±0.19 84.18±1.8 75.45±1.8 69.10±2.9 88.72±0.40 - - - - 83.19ALBERTUS 85.42±1.9 92.93±0.21 97.31±0.22 93.54±0.58 89.06±2.2 80.16±0.15 90.56±0.44 - - - - 89.85δ 8.73 1.13 0.92 9.36 13.61 11.06 1.84 - - - - 6.66p−value 0.0004066 0.0004066 0.0003983 0.0004038 0.000401 0.0004066 0.0004095 - - - - -\norange indicates the multi-script and uni-script models are equal and blue indicates the uni-script model is better\nTable 2: Results on Classification Tasks from IndicGLUE Benchmark\nLanguage Dataset RemBERTMS RemBERTUS δ p −valueALBERTMS ALBERTUS δ p −value\nArticle Genre Classificationhi BBC News 76.80±0.84 77.78±0.92 0.98 0.0466 77.28±1.51 79.14±0.60 1.86 0.0088bn Soham News Article Classification92.86±0.10 93.69±0.20 0.83 0.0004 93.22±0.49 93.89±0.48 0.67 0.0090gu INLTK Headlines 90.27±0.47 91.60±0.28 1.33 0.0004 90.41±0.69 90.73±0.75 0.32 0.6249mr INLTK Headlines 91.24±0.50 92.27±0.39 1.03 0.0008 92.21±0.23 92.04±0.47 -0.17 0.3503ml INLTK Headlines 94.11±0.49 93.33±0.22 -0.78 0.003 - - - -ta INLTK Headlines 95.59±0.70 94.93±0.30 -0.65 0.013 - - - -\nSentiment Analysishi IITP Product Reviews 72.17±1.98 72.85±0.63 0.68 0.9646 76.33±0.84 77.18±0.77 0.85 0.04099hi IITP Movie Reviews 58.66±1.09 62.65±2.74 3.99 0.0023 65.91±2.2 66.34±0.16 0.15 0.8941te ACTSA 61.18±1.38 60.53±0.85 -0.66 0.1981 - - - -\nDiscourse Mode Classificationhi MIDAS Discourse 78.07±0.83 79.46±0.67 1.39 0.0415 78.39±0.33 78.54±0.91 0.15 0.7561\norange indicates the multi-script and uni-script models are equal, cyan indicates multi-script is better than uni-script models and blue indicates vice versa\nTable 3: Accuracy on Public Datasets\nModel pa hi bn or as gu mr ta te ml kn avg\nCloze-style QA (Zero Shot)\nRemBERTMS 33.93 39.06 38.93 37.32 37.66 84.21 46.15 37.02 34.42 38.45 40.75 42.53\nRemBERTUS 33.92 40.10 39.62 38.28 39.26 85.37 45.92 36.68 34.36 37.16 44.29 43.17\nδ -0.01 1.04 0.69 0.96 1.6 1.16 -0.23 -0.34 -0.06 -1.29 3.54 0.64\nALBERTMS 31.04 36.72 35.19 34.63 33.92 59.86 36.14 - - - - 38.21\nALBERTUS 32.77 38.52 36.38 36.00 37.36 70.22 39.53 - - - - 41.54\nδ 1.73 1.8 1.19 1.37 3.44 10.36 3.39 - - - - 3.33\ncyan indicates multi-script is better than uni-script models and blue indicates vice versa\nTable 4: Test accuracy on CSQA\nFirst, we calculate the sentence embeddings of\nthese parallel sentences from the models. Sentence\nembedding is calculated by averaging the hidden\nstate representations of the tokens. Then, we calcu-\nlate the CKA similarity score between the sentence\nembeddings for each language pair. For each lan-\nguage, we average its CKA similarity scores. In\nFigure 1 we plot this average CKA similarity for\neach layer of the models.\nWe see that CLRS score drops significantly at the\nlast layer for all models. However, the uni-script\nmodels retain high CLRS score until the eleventh\nlayer, whereas the multi-script models have low\nCLRS score from the ninth layer. Overall the CLRS\nscore of the uni-script models are more stable. This\nindicates that the uni-script models have learned\n675\n(a) ALBERTMS\n (b) ALBERTUS\n(c) RemBERTMS\n (d) RemBERTUS\nFigure 1: CKA Similarity Score for the multi-script and uni-script models\nbetter cross-lingual representations.\n5 Tokenizer Quality Analysis\nIn terms of performance, we expect the transliter-\nation model to exploit better tokenization across\nthe languages. Following (Ács, 2019) and (Rust\net al., 2021), we measure the subword fertility (av-\nerage number of tokens per word) and the ratio\nof words unbroken by the tokenizer. From fig-\nure 2, we can see that transliteration reduces the\nsplitting of words. This indicates that many words\nthat were represented by different tokens in the\nmulti-script model are represented by a single to-\nken in the transliteration model. On average, the\nALBERT uni-script tokenizer has a lower subword\nfertility score of 1.55 compared to the multi-script\ntokenizer’s 1.825. The uni-scirpt tokenizer also has\na lower proportion of continued word score of 0.36\nwhile the multi-script tokenizer has a score of 0.45.\n6 Conclusion and Future Work\nIn this paper, we show that transliterating closely re-\nlated languages to a common script improves mul-\ntilingual language model performance and leads to\nbetter cross-lingual representations. We conducted\nrigorous statistical analysis to quantify the signif-\nicance and effect size of transliteration on down-\nstream task performance. We found that translit-\neration especially improves performance on com-\nparatively low-resource languages and did not hurt\nthe performance on high-resource languages. This\nfindings are in agreement with (Dhamecha et al.,\n2021; Muller et al., 2021). Our results indicate that\nin other scenarios where closely related languages\nuse different scripts, transliteration can be used to\nimprove the performance of language models. For\nexample, Slavic and Turkic languages present sim-\nilar scenarios. We would like to extend our study\nto models at different scales and more languages\nin the future. Also, another interesting future di-\nrection would be to just use the transliteration for\npretraining signal but give the model the ability to\ndeal with the original scripts.\nLimitations\nA limitation of our work is that it introduces a\ntransliteration step into the model pipeline. Thus\nwe need a stable implementation of the translitera-\ntion scheme. Thus the model can become tied to a\nspecific version of the transliteration library. Also\n676\n(a) Subword Fertility.\n (b) Unbroken Ratio.\nFigure 2: Subword fertility (lower is better) and unbroken ratio (higher is better)\nthe transliteration scheme is not perfect as we saw\nfor Malayalam, it introduced some artifacts. Finally\ngiven our limited computational budget, we could\nnot run experiments with a lot of models at differ-\nent scales. Thus the impact of transliteration over\ndifferent model scales has not been explored. Even\nthough our work has these limitations, it clearly\nshows transliteration as an important tool for train-\ning better multilingual models.\nEthics Statement\nIn their study, Joshi et al. (2020) showed the re-\nsource disparity between low-resource and high-\nresource languages, and (Ruder, 2020) also high-\nlighted the necessity of working with low-resource\nlanguages. However, creating representative and\ninclusive corpora is a difficult task and an ongoing\nprocess and is not always possible for many low-\nresource languages. Thus to inclusively advance\nthe state of NLP across languages, it is crucial to\ndevelop techniques for training MLLMs that can ex-\ntract the most out of existing multilingual corpora.\nHence, we believe our analysis might help MLLMS\nwith low-resource languages in real-world appli-\ncations. However, there is one ethical issue that\nwe want to state explicitly. Even though we pre-\ntrain on a comparatively large multilingual corpus,\nthe model may exhibit harmful gender, ethnic and\npolitical bias. If the model is fine-tuned on a task\nwhere these issues are important, it is necessary\nto take special consideration when relying on the\nmodel’s decisions.\nReferences\nJulien Abadji, Pedro Javier Ortiz Suárez, Laurent Ro-\nmary, and Benoît Sagot. 2021. Ungoliant: An op-\ntimized pipeline for the generation of a very large-\nscale multilingual web corpus. In CMLC-9, Proceed-\nings of the Workshop on Challenges in the Manage-\nment of Large Corpora (CMLC-9) 2021. Limerick,\n12 July 2021 (Online-Event), pages 1 – 9, Mannheim.\nLeibniz-Institut für Deutsche Sprache.\nJudit Ács. 2019. Exploring BERT’s Vocabulary. Blog\nPost.\nMd. Shad Akhtar, Asif Ekbal, and Pushpak Bhat-\ntacharyya. 2016. Aspect based sentiment analysis:\nCategory detection and sentiment classification for\nhindi. In proceedings of the 17th International Con-\nference on Intelligent Text Processing and Computa-\ntional Linguistics (CICLING 2016), April 3-9, 2016,\nKonya, Turkey, pages 246–257. Association for Com-\nputational Linguistics.\nJ Alammar. 2021. Ecco: An open source library for the\nexplainability of transformer language models. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing: System Demonstrations, pages 249–257,\nOnline. Association for Computational Linguistics.\nChantal Amrhein and Rico Sennrich. 2020. On Roman-\nization for model transfer between scripts in neural\nmachine translation. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n2461–2469, Online. Association for Computational\nLinguistics.\nAntonios Anastasopoulos and Graham Neubig. 2019.\nPushing the limits of low-resource morphological in-\nflection. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n984–996, Hong Kong, China. Association for Com-\nputational Linguistics.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 4623–4637, Online. Association\nfor Computational Linguistics.\n677\nAkash Bharadwaj, David Mortensen, Chris Dyer, and\nJaime Carbonell. 2016. Phonologically aware neural\nmodel for named entity recognition in low resource\ntransfer settings. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1462–1472, Austin, Texas. Associ-\nation for Computational Linguistics.\nAditi Chaudhary, Chunting Zhou, Lori Levin, Graham\nNeubig, David R. Mortensen, and Jaime Carbonell.\n2018. Adapting word embeddings to new languages\nwith morphological and phonological subword repre-\nsentations. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3285–3295, Brussels, Belgium. Association\nfor Computational Linguistics.\nHyung Won Chung, Thibault Févry, Henry Tsai, Melvin\nJohnson, and Sebastian Ruder. 2020. Rethinking\nembedding coupling in pre-trained language models.\nCoRR, abs/2010.12821.\nR.A.E. Coningham, F.R. Allchin, C.M. Batt, and\nD. Lucy. 1996. Passage to india? anuradhapura and\nthe early use of the brahmi script. Cambridge Ar-\nchaeological Journal, 6(1):73–97.\nAlexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Emerging cross-\nlingual structure in pretrained language models. In\nProceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2020,\nOnline, July 5-10, 2020, pages 6022–6034. Associa-\ntion for Computational Linguistics.\nRaj Dabre, Tetsuji Nakagawa, and Hideto Kazawa. 2017.\nAn empirical study of language relatedness for trans-\nfer learning in neural machine translation. In Pro-\nceedings of the 31st Pacific Asia Conference on Lan-\nguage, Information and Computation , pages 282–\n286. The National University (Phillippines).\nMaksym Del and Mark Fishel. 2021. Establishing in-\nterlingua in multilingual language models. CoRR,\nabs/2109.01207.\nTejas Dhamecha, Rudra Murthy, Samarth Bharad-\nwaj, Karthik Sankaranarayanan, and Pushpak Bhat-\ntacharyya. 2021. Role of Language Relatedness in\nMultilingual Fine-tuning of Language Models: A\nCase Study in Indo-Aryan Languages. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 8584–8595,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nSwapnil Dhanwal, Hritwik Dutta, Hitesh Nankani, Nilay\nShrivastava, Yaman Kumar, Junyi Jessy Li, Debanjan\nMahata, Rakesh Gosangi, Haimin Zhang, Rajiv Ratn\nShah, and Amanda Stent. 2020. An annotated dataset\nof discourse modes in Hindi stories. In Proceedings\nof the Twelfth Language Resources and Evaluation\nConference, pages 1191–1196, Marseille, France. Eu-\nropean Language Resources Association.\nWikimedia Foundation. Wikimedia downloads.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-\nJen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-\nishnan, Marc’Aurelio Ranzato, Francisco Guzmán,\nand Angela Fan. 2022. The Flores-101 evaluation\nbenchmark for low-resource and multilingual ma-\nchine translation. Transactions of the Association for\nComputational Linguistics, 10:522–538.\nVikrant Goyal, Sourav Kumar, and Dipti Misra Sharma.\n2020. Efficient neural machine translation for low-\nresource languages via exploiting related languages.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics: Student\nResearch Workshop, pages 162–168, Online. Associ-\nation for Computational Linguistics.\nCharles F. Hockett, Peter T. Daniels, and William Bright.\n1997. The world 's writing systems. Language,\n73(2):379.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6282–6293, Online. Association for Computational\nLinguistics.\nKarthikeyan K, Zihan Wang, Stephen Mayhew, and\nDan Roth. 2020. Cross-lingual ability of multilin-\ngual BERT: an empirical study. In 8th International\nConference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\nview.net.\nDivyanshu Kakwani, Anoop Kunchukuttan, Satish\nGolla, Gokul N.C., Avik Bhattacharyya, Mitesh M.\nKhapra, and Pratyush Kumar. 2020. IndicNLPSuite:\nMonolingual corpora, evaluation benchmarks and\npre-trained multilingual language models for Indian\nlanguages. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 4948–\n4961, Online. Association for Computational Lin-\nguistics.\nYash Khemchandani, Sarvesh Mehtani, Vaidehi Patil,\nAbhijeet Awasthi, Partha Talukdar, and Sunita\nSarawagi. 2021. Exploiting language relatedness\nfor low web-resource language model adaptation: An\nIndic languages study. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 1312–1323, Online. Association\nfor Computational Linguistics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nSimon Kornblith, Mohammad Norouzi, Honglak Lee,\nand Geoffrey E. Hinton. 2019. Similarity of\nneural network representations revisited. CoRR,\nabs/1905.00414.\n678\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2018: System Demonstrations, Brussels, Belgium,\nOctober 31 - November 4, 2018, pages 66–71. Asso-\nciation for Computational Linguistics.\nSneha Kudugunta, Ankur Bapna, Isaac Caswell, and\nOrhan Firat. 2019. Investigating multilingual NMT\nrepresentations at scale. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1565–1575, Hong Kong,\nChina. Association for Computational Linguistics.\nAnoop Kunchukuttan. 2020. The Indic-\nNLP Library. https://github.com/\nanoopkunchukuttan/indic_nlp_\nlibrary/blob/master/docs/indicnlp.\npdf.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli ´c, and\nGoran Glavaš. 2020. From zero to hero: On the\nlimitations of zero-shot language transfer with mul-\ntilingual Transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 4483–4499, On-\nline. Association for Computational Linguistics.\nQuentin Lhoest, Albert Villanova del Moral, Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\nLewis Tunstall, Joe Davison, Mario Šaško, Gun-\njan Chhablani, Bhavitvya Malik, Simon Brandeis,\nTeven Le Scao, Victor Sanh, Canwen Xu, Nicolas\nPatry, Angelina McMillan-Major, Philipp Schmid,\nSylvain Gugger, Clément Delangue, Théo Matus-\nsière, Lysandre Debut, Stas Bekman, Pierric Cis-\ntac, Thibault Goehringer, Victor Mustar, François\nLagunas, Alexander Rush, and Thomas Wolf. 2021.\nDatasets: A community library for natural language\nprocessing. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning: System Demonstrations, pages 175–184, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nPatrick Littell, David R. Mortensen, Ke Lin, Katherine\nKairis, Carlisle Turner, and Lori Levin. 2017. URIEL\nand lang2vec: Representing languages as typological,\ngeographical, and phylogenetic vectors. In Proceed-\nings of the 15th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nVolume 2, Short Papers, pages 8–14, Valencia, Spain.\nAssociation for Computational Linguistics.\nDaniel Lüdecke. 2020. sjstats: Statistical Functions for\nRegression Models (Version 0.18.0).\nH. B. Mann and D. R. Whitney. 1947. On a Test of\nWhether one of Two Random Variables is Stochas-\ntically Larger than the Other. The Annals of Mathe-\nmatical Statistics, 18(1):50 – 60.\nAri S. Morcos, Maithra Raghu, and Samy Bengio. 2018.\nInsights on representational similarity in neural net-\nworks with canonical correlation. In Advances in\nNeural Information Processing Systems 31: Annual\nConference on Neural Information Processing Sys-\ntems 2018, NeurIPS 2018, December 3-8, 2018, Mon-\ntréal, Canada, pages 5732–5741.\nMarius Mosbach, Maksym Andriushchenko, and Diet-\nrich Klakow. 2021. On the stability of fine-tuning\nBERT: misconceptions, explanations, and strong\nbaselines. In 9th International Conference on Learn-\ning Representations, ICLR 2021, Virtual Event, Aus-\ntria, May 3-7, 2021. OpenReview.net.\nSandeep Sricharan Mukku and Radhika Mamidi. 2017.\nACTSA: Annotated corpus for Telugu sentiment anal-\nysis. In Proceedings of the First Workshop on Build-\ning Linguistically Generalizable NLP Systems, pages\n54–58, Copenhagen, Denmark. Association for Com-\nputational Linguistics.\nBenjamin Muller, Antonios Anastasopoulos, Benoît\nSagot, and Djamé Seddah. 2021. When being un-\nseen from mBERT is just the beginning: Handling\nnew languages with multilingual language models.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 448–462, Online. Association for Computa-\ntional Linguistics.\nBenjamin Müller, Yanai Elazar, Benoît Sagot, and\nDjamé Seddah. 2021. First align, then predict: Un-\nderstanding the cross-lingual ability of multilingual\nBERT. In Proceedings of the 16th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Main Volume, EACL 2021, Online,\nApril 19 - 23, 2021, pages 2214–2231. Association\nfor Computational Linguistics.\nNikitha Murikinati, Antonios Anastasopoulos, and Gra-\nham Neubig. 2020. Transliteration for cross-lingual\nmorphological inflection. In Proceedings of the\n17th SIGMORPHON Workshop on Computational\nResearch in Phonetics, Phonology, and Morphology,\npages 189–197, Online. Association for Computa-\ntional Linguistics.\nPedro Javier Ortiz Su’arez, Benoit Sagot, and Laurent\nRomary. 2019. Asynchronous pipelines for process-\ning huge corpora on medium to low resource in-\nfrastructures. In Proceedings of the Workshop on\nChallenges in the Management of Large Corpora\n679\n(CMLC-7) 2019, Proceedings of the Workshop on\nChallenges in the Management of Large Corpora\n(CMLC-7) 2019. Cardiff, 22nd July 2019, pages\n9 – 16, Mannheim. Leibniz-Institut f\"ur Deutsche\nSprache.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nJonas Pfeiffer, Ivan Vulic, Iryna Gurevych, and Sebas-\ntian Ruder. 2021. Unks everywhere: Adapting multi-\nlingual language models to new scripts. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2021, Vir-\ntual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021, pages 10186–10203. Association\nfor Computational Linguistics.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4996–5001, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nPyICU. Pyicu transliteration tool. https://pypi.\norg/project/PyICU/. Accessed: 2022-03-15.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv e-prints.\nMaithra Raghu, Justin Gilmer, Jason Yosinski, and\nJascha Sohl-Dickstein. 2017. SVCCA: singular vec-\ntor canonical correlation analysis for deep learning\ndynamics and interpretability. In Advances in Neural\nInformation Processing Systems 30: Annual Confer-\nence on Neural Information Processing Systems 2017,\nDecember 4-9, 2017, Long Beach, CA, USA, pages\n6076–6085.\nAfshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-\nsively multilingual transfer for NER. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 151–164, Florence,\nItaly. Association for Computational Linguistics.\nVinodh Rajan. 2015. Aksharamukha transliteration tool.\nhttps://github.com/virtualvinodh/\naksharamukha-python. Accessed: 2021-10-\n04.\nShruti Rijhwani, Jiateng Xie, Graham Neubig, and\nJaime G. Carbonell. 2019. Zero-shot neural transfer\nfor cross-lingual entity linking. In The Thirty-Third\nAAAI Conference on Artificial Intelligence, AAAI\n2019, The Thirty-First Innovative Applications of\nArtificial Intelligence Conference, IAAI 2019, The\nNinth AAAI Symposium on Educational Advances in\nArtificial Intelligence, EAAI 2019, Honolulu, Hawaii,\nUSA, January 27 - February 1, 2019 , pages 6924–\n6931. AAAI Press.\nSebastian Ruder. 2020. Why You Should Do\nNLP Beyond English. http://ruder.io/\nnlp-beyond-english.\nPhillip Rust, Jonas Pfeiffer, Ivan Vuli´c, Sebastian Ruder,\nand Iryna Gurevych. 2021. How good is your tok-\nenizer? on the monolingual performance of multilin-\ngual language models. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 3118–3135, Online. Association\nfor Computational Linguistics.\nJasdeep Singh, Bryan McCann, Richard Socher, and\nCaiming Xiong. 2019. BERT is not an interlin-\ngua and the bias of tokenization. In Proceedings\nof the 2nd Workshop on Deep Learning Approaches\nfor Low-Resource NLP , DeepLo@EMNLP-IJCNLP\n2019, Hong Kong, China, November 3, 2019, pages\n47–55. Association for Computational Linguistics.\nHaiyue Song, Raj Dabre, Zhuoyuan Mao, Fei Cheng,\nSadao Kurohashi, and Eiichiro Sumita. 2020. Pre-\ntraining via leveraging assisting languages for neural\nmachine translation. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics: Student Research Workshop, pages 279–\n285, Online. Association for Computational Linguis-\ntics.\nG. M. Sullivan and R. Feinn. 2012. Using Effect Size-or\nWhy the P Value Is Not Enough. J Grad Med Educ,\n4(3):279–282.\nPauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt\nHaberland, Tyler Reddy, David Cournapeau, Ev-\ngeni Burovski, Pearu Peterson, Warren Weckesser,\nJonathan Bright, Stéfan J. van der Walt, Matthew\nBrett, Joshua Wilson, K. Jarrod Millman, Nikolay\nMayorov, Andrew R. J. Nelson, Eric Jones, Robert\nKern, Eric Larson, C J Carey, ˙Ilhan Polat, Yu Feng,\nEric W. Moore, Jake VanderPlas, Denis Laxalde,\nJosef Perktold, Robert Cimrman, Ian Henriksen, E. A.\nQuintero, Charles R. Harris, Anne M. Archibald, An-\ntônio H. Ribeiro, Fabian Pedregosa, Paul van Mul-\nbregt, and SciPy 1.0 Contributors. 2020. SciPy 1.0:\nFundamental Algorithms for Scientific Computing in\nPython. Nature Methods, 17:261–272.\nFrank Wilcoxon. 1945. Individual comparisons by rank-\ning methods. Biometrics Bulletin, 1(6):80.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\n680\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nShijie Wu and Mark Dredze. 2020. Are all languages\ncreated equal in multilingual BERT? In Proceedings\nof the 5th Workshop on Representation Learning for\nNLP, pages 120–130, Online. Association for Com-\nputational Linguistics.\nA Cloze Style QA Evaluation Method\nSince a word can be tokenized to multiple tokens\nby the subword tokenizer, correctly evaluating the\nmodel on this task requires special care. Specif-\nically, we have to use the same number of mask\ntokens as the number of subword tokens that a word\ngets split into. Then we calculate the probability\nfor the word by multiplying the probability of the\nsubword tokens predicted by the masked language\nmodel.\nB Pretraining Details\nCorpus Preparation: Since the OSCAR corpus\ncontains raw text from the Web, we apply a few\nfiltering and normalization. First, we discard en-\ntries where the dominant script does not match\nthe language tag provided by the OSCAR corpus.\nThen we use the IndicNLP normalizer (Kunchukut-\ntan, 2020) to normalize the raw text. For the uni-\nscript model, we then transliterate all the text to\nISO-15919 format using the Aksharamukha (Rajan,\n2015) library.\nFor the RemBERT models we do not perform\nany of the filtering mentioned above since our pre-\ntraining corpus is comparatively very large. In this\ncase, we use the PyICU library (PyICU) for translit-\nerating to ISO-15919 format.\nTokenizer Training: For the ALBERT mod-\nels, we train two SentencePiece tokenizers (Kudo\nand Richardson, 2018) on the transliterated and the\nnon-transliterated corpus with a vocabulary size of\n50,000. For the RemBERT models we train Uni-\ngram tokenizers from the Tokenizers library (Wolf\net al., 2020) with a vocabulary size of 65,536.\nALBERT Model Training: We first pretrained\nan ALBERT base model from scratch on the non-\ntransliterated corpus as our baseline. Afterward,\nwe pretrained another ALBERT base from scratch\non the transliterated corpus. We chose the base\nmodel due to computing constraints. We trained\nthe models on a single TPUv3 VM. Both models\nwere trained using the same hyperparameters. We\nfollowed the hyperparameters used in (Lan et al.,\n2020) except for batch size and learning rate. The\npretraining objective is also the same as (Lan et al.,\n2020).We used a batch size of 256, which is the\nhighest that fits into TPU memory, whereas the\nALBERT paper used a batch size of 4096. As our\nbatch size is 1/16th of the ALBERT paper, we use\na learning rate of 1e-3/8, which is approximately\n1/16th of the learning rate used in the ALBERT\npaper (1.76e-2). Additionally, we use the Adam\noptimizer (Kingma and Ba, 2015) instead of the\nLAMB optimizer. The rest of the hyperparameters\nwere the same as the ALBERT paper. Specifically,\nwe use a sequence length of 512 with absolute po-\nsitional encoding, weight decay of 1e-2, warmup\nsteps of 5000, max gradient norm of 1.0, and Adam\nepsilon of 1e-6. The models were trained for 1M\nsteps. Each model took about 7.5 days to train. We\nuse the ALBERT implementation from the Hug-\ngingface Transformers Library (Wolf et al., 2020).\nRemBERT Model Training: We pretrained an\nRemBERT base models similar to the ALBERT\nmodels. We trained the models on a single TPUv3\nVM. Both models were trained using the same\nhyperparameters. We followed the hyperparam-\neters used in (Chung et al., 2020) except for batch\nsize and learning rate. The pretraining objective\nis also the same as (Chung et al., 2020). We used\na batch size of 256, which is the highest that fits\ninto TPU memory, whereas the RemBERT paper\nused a batch size of 2048. As our batch size is 1/8th\nof the RemBERT paper, we use a learning rate\nof 2e-4/8, which is 1/8th of the learning rate used\nin the RemBERT paper. Similar to the ALBERT\nmodel, we use the Adam optimizer (Kingma and\nBa, 2015). The rest of the hyperparameters were\nthe same as the RemBERT paper. Specifically, we\nuse a sequence length of 512 with absolute posi-\ntional encoding, weight decay of 1e-2, warmup\nsteps of 15000, max gradient norm of 1.0, and\nAdam epsilon of 1e-6. The models were trained\nfor 1M steps. Each model took about 7.5 days to\ntrain. We use the RemBERT implementation from\nthe Huggingface Transformers Library (Wolf et al.,\n2020).\nC Downstream Hyperparameters\nHyperparameters for downstream tasks are pre-\nsented in Table 5 and Table 6.\n681\nTask TPU Batch Size Learning Rate Weight Decay Dropout Epochs Warmup Ratio\nNews Category Classification False 16 2e-5 0.01 0.1 20 0.10\nWikipedia Section-Title Prediction True 256 2e-5 0.01 0.1 3 0.10\nNamed Entity Recognition True 512 2e-5 0.01 0.1 20 0.10\nBBC Hindi News Classification False 16 2e-5 0.01 0.1 20 0.10\nSoham Bengali News Classification False 16 2e-5 0.01 0.1 8 0.10\nINLTK Headlines Classification False 256 2e-5 0.01 0.1 20 0.10\nIITP Movie Review False 64 5e-5 0.01 0.25 20 0.10\nIITP Product Review False 16 5e-5 0.01 0.5 20 0.10\nMIDAS Discourse Mode False 32 2e-5 0.01 0.5 20 0.10\nTable 5: Hyperparameters for ALBERT models\nTask TPU Batch Size Learning Rate Weight Decay Dropout Steps Label Smoothing\nNews Category Classification False 16 1e-5 0.1 0.1 2500 0.0\nWikipedia Section-Title Prediction True 256 8e-6 0.1 0.1 12500 0.0\nNamed Entity Recognition False 16 5e-5 0.1 0.1 10000 0.0\nBBC Hindi News Classification False 16 1e-5 0.01 0.1 2500 0.0\nSoham Bengali News Classification False 16 1e-5 0.1 0.1 2500 0.1\nINLTK Headlines Classification False 16 1e-5 0.1 0.1 5000 0.0\nIITP Movie Review False 16 1e-5 0.1 0.1 5000 0.0\nIITP Product Review False 16 1e-5 0.1 0.1 5000 0.0\nACTSA Sentiment Classification False 16 1e-5 0.1 0.1 5000 0.0\nMIDAS Discourse Mode False 16 8e-6 0.1 0.1 2500 0.1\nTable 6: Hyperparameters for RemBERT models\nFor the ALBERT models batch size was chosen\nto be the maximum that fits in memory. This was\ndone so that each batch contains approximately the\nsame number of tokens. Otherwise the hyperparam-\neters were chosen following the recommendations\nof (Mosbach et al., 2021). On the highly skewed\nIITP Movie Review, IITP Product Review and MI-\nDAS Discourse we found that this default setting\nresulted in worse performance compared to the in-\ndependent baselines. So we finetuned the learning\nrate and classifier dropout on the validation set of\nthese tasks.\nFor the RemBERT models learning rate, weight\ndecay, dropout, steps and label smoothing were\nchosen based on grid search with a few values.\nD Test Statistics Results\nρgives us the probability of one group being better\nthan the other group. That is the probability that a\nrandom performance sample of the the uni-script\nmodel is greater than a random performance sam-\nple of the multi-script model. The last test statistic\nis r which indicates the magnitude of difference\nbetween the performance values of the uni-script\nmodel (group 1) and the multi-script model (group\n2). rshows us how realistically significant the per-\nformance differences are between models even if\nthe performance difference is statistically signif-\nicant. It gives us a value between 0 to 1 and its\nranges are: small effect ( 0 ≤ r≤ 0.3) , medium\neffect ( 0.3 < r≤ 0.5) and large effect (0.5 <\nr). We performed MWU on all downstream tasks\nexcept CSQA. On CSQA, we only report the δ.\nThe MWU is performed using the SciPy library\n(Virtanen et al., 2020), and the results are further\nvalidated using R (Lüdecke, 2020). These statistic\nare reported in Table 7 for the IndicGLUE classi-\nfication tasks and in Table 8 for the public dataset\nclassification tasks.\nE Cross-lingual Similarity of ALBERT\nModels on All Language Pairs\n682\nModel pa hi bn or as gu mr kn te ml ta\nWikipedia Section Title Prediction\nRemBERTρ 1 1 1 1 1 1 0.91 1 1 0.67 0.99\nRemBERTr 0.83 0.83 0.83 0.84 0.83 0.84 0.69 0.83 0.83 0.27 0.81\nALBERTρ 1 1 1 1 1 1 1 - - - -\nALBERTr 0.83 0.83 0.83 0.83 0.83 0.83 0.83 - - - -\nNews Category Classification\nRemBERTρ 1 - 0.27 1 - 0.87 0.46 0.27 0.45 0.94 0.75\nRemBERTr 0.85 - 0.39 0.84 - 0.68 0.07 0.39 0.07 0.75 0.41\nALBERTρ 1 - 0.31 1 - 0.80 0.31 - - - -\nALBERTr 0.86 - 0.32 0.83 - 0.51 0.32 - - - -\nNamed Entity Recognition\nRemBERTρ 1.00 0.95 0.99 0.70 0.91 1.00 0.57 1.00 1.00 0.56 0.65\nRemBERTr 0.83 0.75 0.81 0.33 0.69 0.83 0.10 0.83 0.83 0.08 0.25\nALBERTρ 1 1 1 1 1 1 1 - - - -\nALBERTr 0.83 0.83 0.83 0.83 0.83 0.83 0.83 - - - -\nTable 7: Test Statistics on Classification Tasks from IndicGLUE Benchmark\nLanguage Dataset RemBERTρ RemBERTr ALBERTρ ALBERTr\nArticle Genre Classification\nhi BBC News 0.78 0.47 0.87 0.62\nbn Soham News Article Classification 1 0.84 0.87 0.62\ngu INLTK Headlines 1 0.84 0.57 0.12\nmr INLTK Headlines 0.98 0.79 0.36 0.22\nml INLTK Headlines 0.08 0.70 - -\nta INLTK Headlines 0.15 0.59 - -\nSentiment Analysis\nhi IITP Product Reviews 0.51 0.01 0.79 0.48\nhi IITP Movie Reviews 0.93 0.72 0.52 0.03\nte ACTSA 0.31 0.30 - -\nDiscourse Mode Classification\nhi MIDAS Discourse 0.79 0.48 0.45 0.07\nTable 8: Test Statistics on Public Datasets\n683\n(a) multi-script PA-X\n (b) uni-script PA-X\n(c) multi-script HI-X\n (d) uni-script HI-X\n(e) multi-script BN-X\n (f) uni-script BN-X\n(g) multi-script OR-X\n (h) uni-script OR-X\nFigure 3: CKA of multi-script and uni-script on all language pairs for pa, hi,bn and or\n684\n(a) multi-script AS-X\n (b) uni-script AS-X\n(c) multi-script GU-X\n (d) uni-script GU-X\n(e) multi-script MR-X\n (f) uni-script MR-X\n(g) multi-script NE-X\n (h) uni-script NE-X\nFigure 4: CKA of multi-script and uni-script on all language pairs for AS, GU, MR and NE\n685",
  "topic": "Transliteration",
  "concepts": [
    {
      "name": "Transliteration",
      "score": 0.8960540294647217
    },
    {
      "name": "Computer science",
      "score": 0.8383521437644958
    },
    {
      "name": "Natural language processing",
      "score": 0.7655830979347229
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7040685415267944
    },
    {
      "name": "Scripting language",
      "score": 0.6778508424758911
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5194565653800964
    },
    {
      "name": "Focus (optics)",
      "score": 0.50751793384552
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4783698618412018
    },
    {
      "name": "Sentence",
      "score": 0.4487707018852234
    },
    {
      "name": "Task (project management)",
      "score": 0.43918025493621826
    },
    {
      "name": "Programming language",
      "score": 0.11583980917930603
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130769515",
      "name": "Pennsylvania State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I157386601",
      "name": "North South University",
      "country": "BD"
    }
  ]
}