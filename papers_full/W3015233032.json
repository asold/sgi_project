{
    "title": "Poor Man's BERT: Smaller and Faster Transformer Models.",
    "url": "https://openalex.org/W3015233032",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2256769809",
            "name": "Hassan Sajjad",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2576593349",
            "name": "Fahim Dalvi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2123720402",
            "name": "Nadir Durrani",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1989325080",
            "name": "Preslav Nakov",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2970565456",
        "https://openalex.org/W3177265267",
        "https://openalex.org/W2963400886",
        "https://openalex.org/W2975059944",
        "https://openalex.org/W3034709122",
        "https://openalex.org/W2924902521",
        "https://openalex.org/W2136922672",
        "https://openalex.org/W2787560479",
        "https://openalex.org/W3023585950",
        "https://openalex.org/W2964204621",
        "https://openalex.org/W2980965328",
        "https://openalex.org/W2970454332",
        "https://openalex.org/W3105966348",
        "https://openalex.org/W131533222",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W2971702703",
        "https://openalex.org/W2970820321",
        "https://openalex.org/W2953369973",
        "https://openalex.org/W3034292689",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2998183051",
        "https://openalex.org/W3015982254",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W3014568172",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W2975429091",
        "https://openalex.org/W3104033643",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2605717780",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3017722917",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2975381464",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2964303116",
        "https://openalex.org/W3033737024",
        "https://openalex.org/W2130158090",
        "https://openalex.org/W2997710335",
        "https://openalex.org/W2119479037",
        "https://openalex.org/W3025792726",
        "https://openalex.org/W3203309275"
    ],
    "abstract": "The ongoing neural revolution in Natural Language Processing has recently been dominated by large-scale pre-trained Transformer models, where size does matter: it has been shown that the number of parameters in such a model is typically positively correlated with its performance. Naturally, this situation has unleashed a race for ever larger models, many of which, including the large versions of popular models such as BERT, XLNet, and RoBERTa, are now out of reach for researchers and practitioners without large-memory GPUs/TPUs. To address this issue, we explore a number of memory-light model reduction strategies that do not require model pre-training from scratch. The experimental results show that we are able to prune BERT, RoBERTa and XLNet models by up to 40%, while maintaining up to 98% of their original performance. We also show that our pruned models are on par with DistilBERT in terms of both model size and performance. Finally, our pruning strategies enable interesting comparative analysis between BERT and XLNet.",
    "full_text": null
}