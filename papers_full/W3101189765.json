{
  "title": "Single and Multi-Agent Deep Reinforcement Learning for AI-Enabled Wireless Networks: A Tutorial",
  "url": "https://openalex.org/W3101189765",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2913441846",
      "name": "Amal Feriani",
      "affiliations": [
        "University of Manitoba"
      ]
    },
    {
      "id": "https://openalex.org/A1909747623",
      "name": "Ekram Hossain",
      "affiliations": [
        "University of Manitoba"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6742490908",
    "https://openalex.org/W6741002519",
    "https://openalex.org/W6683204974",
    "https://openalex.org/W3037207668",
    "https://openalex.org/W2945082786",
    "https://openalex.org/W3011182939",
    "https://openalex.org/W6638018090",
    "https://openalex.org/W6692846177",
    "https://openalex.org/W6627932998",
    "https://openalex.org/W2119717200",
    "https://openalex.org/W2747213132",
    "https://openalex.org/W6749557584",
    "https://openalex.org/W6640044251",
    "https://openalex.org/W6762058563",
    "https://openalex.org/W6683084483",
    "https://openalex.org/W2981708654",
    "https://openalex.org/W3037900478",
    "https://openalex.org/W6632428414",
    "https://openalex.org/W2919603045",
    "https://openalex.org/W1542941925",
    "https://openalex.org/W6771261610",
    "https://openalex.org/W4254547512",
    "https://openalex.org/W2979897999",
    "https://openalex.org/W2902098903",
    "https://openalex.org/W6762029327",
    "https://openalex.org/W6751972096",
    "https://openalex.org/W6764419582",
    "https://openalex.org/W6784003830",
    "https://openalex.org/W6771753067",
    "https://openalex.org/W6752164047",
    "https://openalex.org/W6732811253",
    "https://openalex.org/W1646707810",
    "https://openalex.org/W3010243247",
    "https://openalex.org/W6713134421",
    "https://openalex.org/W3080475059",
    "https://openalex.org/W6631137000",
    "https://openalex.org/W6766978945",
    "https://openalex.org/W2057913812",
    "https://openalex.org/W6747473740",
    "https://openalex.org/W2898035736",
    "https://openalex.org/W6684205842",
    "https://openalex.org/W2963079995",
    "https://openalex.org/W6748839928",
    "https://openalex.org/W6684921986",
    "https://openalex.org/W6637967152",
    "https://openalex.org/W32403112",
    "https://openalex.org/W6685444567",
    "https://openalex.org/W6687681856",
    "https://openalex.org/W3006762012",
    "https://openalex.org/W6782150861",
    "https://openalex.org/W6776743714",
    "https://openalex.org/W4245108548",
    "https://openalex.org/W1491843047",
    "https://openalex.org/W6767300576",
    "https://openalex.org/W6680657880",
    "https://openalex.org/W6784723405",
    "https://openalex.org/W6748315105",
    "https://openalex.org/W6764053384",
    "https://openalex.org/W2941205169",
    "https://openalex.org/W2953708620",
    "https://openalex.org/W6762427411",
    "https://openalex.org/W6751494529",
    "https://openalex.org/W6735677848",
    "https://openalex.org/W2580909119",
    "https://openalex.org/W6762819967",
    "https://openalex.org/W6767941141",
    "https://openalex.org/W6765603843",
    "https://openalex.org/W6776634759",
    "https://openalex.org/W6777944346",
    "https://openalex.org/W6776367181",
    "https://openalex.org/W6767486501",
    "https://openalex.org/W3082925502",
    "https://openalex.org/W6781998821",
    "https://openalex.org/W3045086480",
    "https://openalex.org/W2952867909",
    "https://openalex.org/W2981096252",
    "https://openalex.org/W3044812257",
    "https://openalex.org/W6749304979",
    "https://openalex.org/W3013326326",
    "https://openalex.org/W6752380930",
    "https://openalex.org/W3089192431",
    "https://openalex.org/W6738796088",
    "https://openalex.org/W6756975441",
    "https://openalex.org/W6760290758",
    "https://openalex.org/W3041540203",
    "https://openalex.org/W6770332567",
    "https://openalex.org/W3041552970",
    "https://openalex.org/W6760707525",
    "https://openalex.org/W3081088023",
    "https://openalex.org/W2975128548",
    "https://openalex.org/W2901196053",
    "https://openalex.org/W2976344064",
    "https://openalex.org/W2808139065",
    "https://openalex.org/W2768629321",
    "https://openalex.org/W2913326990",
    "https://openalex.org/W6761324081",
    "https://openalex.org/W6767327128",
    "https://openalex.org/W3023161348",
    "https://openalex.org/W3017275226",
    "https://openalex.org/W3011538535",
    "https://openalex.org/W2903523957",
    "https://openalex.org/W6777536727",
    "https://openalex.org/W2993809815",
    "https://openalex.org/W2963000651",
    "https://openalex.org/W6718140377",
    "https://openalex.org/W2998709521",
    "https://openalex.org/W2963961639",
    "https://openalex.org/W6758373747",
    "https://openalex.org/W3023368031",
    "https://openalex.org/W6764132201",
    "https://openalex.org/W2740377041",
    "https://openalex.org/W2900395175",
    "https://openalex.org/W2958874452",
    "https://openalex.org/W2908259400",
    "https://openalex.org/W6773222559",
    "https://openalex.org/W3080441795",
    "https://openalex.org/W3092058816",
    "https://openalex.org/W6751139674",
    "https://openalex.org/W3088337262",
    "https://openalex.org/W3037040187",
    "https://openalex.org/W3038939462",
    "https://openalex.org/W6771274123",
    "https://openalex.org/W1641379095",
    "https://openalex.org/W2981038142",
    "https://openalex.org/W2908261578",
    "https://openalex.org/W2963457007",
    "https://openalex.org/W1521003796",
    "https://openalex.org/W2140135625",
    "https://openalex.org/W2970202659",
    "https://openalex.org/W1908253641",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W2964043796",
    "https://openalex.org/W2742739149",
    "https://openalex.org/W2991046523",
    "https://openalex.org/W2781726626",
    "https://openalex.org/W3005600496",
    "https://openalex.org/W3135089279",
    "https://openalex.org/W2971587637",
    "https://openalex.org/W2103561211",
    "https://openalex.org/W2943020286",
    "https://openalex.org/W3030487185",
    "https://openalex.org/W3025747022",
    "https://openalex.org/W2575731723",
    "https://openalex.org/W2921864399",
    "https://openalex.org/W2165150801",
    "https://openalex.org/W2787938642",
    "https://openalex.org/W2807741983",
    "https://openalex.org/W3018736630",
    "https://openalex.org/W3001618744",
    "https://openalex.org/W2121863487",
    "https://openalex.org/W3034906924",
    "https://openalex.org/W3022898652",
    "https://openalex.org/W2992223242",
    "https://openalex.org/W3037684076",
    "https://openalex.org/W3023564296",
    "https://openalex.org/W2952603690",
    "https://openalex.org/W1191599655",
    "https://openalex.org/W2173248099",
    "https://openalex.org/W2963226019",
    "https://openalex.org/W3093287223",
    "https://openalex.org/W2970927156",
    "https://openalex.org/W2950222626",
    "https://openalex.org/W1771410628",
    "https://openalex.org/W2945007422",
    "https://openalex.org/W3098680154",
    "https://openalex.org/W2155027007",
    "https://openalex.org/W2905026759",
    "https://openalex.org/W2970356364",
    "https://openalex.org/W2963241167",
    "https://openalex.org/W3017890190",
    "https://openalex.org/W2945924974",
    "https://openalex.org/W2793398421",
    "https://openalex.org/W3021175792",
    "https://openalex.org/W3048977193",
    "https://openalex.org/W2173564293",
    "https://openalex.org/W2402144811",
    "https://openalex.org/W2048226872",
    "https://openalex.org/W3089325617",
    "https://openalex.org/W1757796397",
    "https://openalex.org/W3044650002",
    "https://openalex.org/W2950934634",
    "https://openalex.org/W2970514967",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2951915386",
    "https://openalex.org/W2806985155",
    "https://openalex.org/W2602963933",
    "https://openalex.org/W2962737466",
    "https://openalex.org/W2970277495",
    "https://openalex.org/W2964410826",
    "https://openalex.org/W2922221969",
    "https://openalex.org/W3038006656",
    "https://openalex.org/W2963960193",
    "https://openalex.org/W2158282517",
    "https://openalex.org/W2963407617",
    "https://openalex.org/W2956068307",
    "https://openalex.org/W2201581102"
  ],
  "abstract": "Deep Reinforcement Learning (DRL) has recently witnessed significant advances that have led to multiple successes in solving sequential decision-making problems in various domains, particularly in wireless communications. The future sixth-generation (6G) networks are expected to provide scalable, low-latency, ultra-reliable services empowered by the application of data-driven Artificial Intelligence (AI). The key enabling technologies of future 6G networks, such as intelligent meta-surfaces, aerial networks, and AI at the edge, involve more than one agent which motivates the importance of multi-agent learning techniques. Furthermore, cooperation is central to establishing self-organizing, self-sustaining, and decentralized networks. In this context, this tutorial focuses on the role of DRL with an emphasis on deep Multi-Agent Reinforcement Learning (MARL) for AI-enabled 6G networks. The first part of this paper will present a clear overview of the mathematical frameworks for single-agent RL and MARL. The main idea of this work is to motivate the application of RL beyond the model-free perspective which was extensively adopted in recent years. Thus, we provide a selective description of RL algorithms such as Model-Based RL (MBRL) and cooperative MARL and we highlight their potential applications in 6G wireless networks. Finally, we overview the state-of-the-art of MARL in fields such as Mobile Edge Computing (MEC), Unmanned Aerial Vehicles (UAV) networks, and cell-free massive MIMO, and identify promising future research directions. We expect this tutorial to stimulate more research endeavors to build scalable and decentralized systems based on MARL.",
  "full_text": "1\nSingle and Multi-Agent Deep Reinforcement\nLearning for AI-Enabled Wireless Networks: A\nTutorial\nAmal Feriani and Ekram Hossain, Fellow, IEEE\nAbstract—Deep Reinforcement Learning (DRL) has recently\nwitnessed signiﬁcant advances that have led to multiple suc-\ncesses in solving sequential decision-making problems in various\ndomains, particularly in wireless communications. The future\nsixth-generation (6G) networks are expected to provide scalable,\nlow-latency, ultra-reliable services empowered by the application\nof data-driven Artiﬁcial Intelligence (AI). The key enabling\ntechnologies of future 6G networks, such as intelligent meta-\nsurfaces, aerial networks, and AI at the edge, involve more than\none agent which motivates the importance of multi-agent learning\ntechniques. Furthermore, cooperation is central to establishing\nself-organizing, self-sustaining, and decentralized networks. In\nthis context, this tutorial focuses on the role of DRL with an\nemphasis on deep Multi-Agent Reinforcement Learning (MARL)\nfor AI-enabled 6G networks. The ﬁrst part of this paper will\npresent a clear overview of the mathematical frameworks for\nsingle-agent RL and MARL. The main idea of this work is to\nmotivate the application of RL beyond the model-free perspective\nwhich was extensively adopted in recent years. Thus, we provide\na selective description of RL algorithms such as Model-Based RL\n(MBRL) and cooperative MARL and we highlight their potential\napplications in 6G wireless networks. Finally, we overview the\nstate-of-the-art of MARL in ﬁelds such as Mobile Edge Com-\nputing (MEC), Unmanned Aerial Vehicles (UA V) networks, and\ncell-free massive MIMO, and identify promising future research\ndirections. We expect this tutorial to stimulate more research\nendeavors to build scalable and decentralized systems based on\nMARL.\nKeywords:- 6G networks, Deep Reinforcement Learn-\ning (DRL), Multi-Agent Reinforcement Learning (MARL),\nModel-Based Reinforcement Learning (MBRL), decentralized\nnetworks\nI. I NTRODUCTION\nThe evolution of wireless networks such as the ﬁfth-\ngeneration (5G) and beyond 5G (also referred to as 6G)\nnetworks is driven by a huge increase of connected devices, the\ngrowing demand for high data rate applications, and the con-\nvergence between communications and computing. Intelligent\nsignal processing along with AI-driven super-intelligent radio\naccess and network control will be among the key technologies\nin future wireless networks to achieve scalability, context-\nawareness, and energy efﬁciency along with massive capacity\nand connectivity. 5G wireless networks are expected to pro-\nvide Ultra-Reliable, Low Latency Communications (URLLC),\nThe authors are with the Department of Electrical and Computer En-\ngineering, University of Manitoba, Winnipeg, MB, Canada (e-mails: feri-\nania@myumanitoba.ca, ekram.hossain@umanitoba.ca). The work was sup-\nported by a Discovery Grant from the Natural Sciences and Engineering\nResearch Council of Canada (NSERC).\nEnhanced Mobile Broadband (eMBB), and massive Machine-\nType Communications (mMTC) to enable the deployment\nof Internet-of-Things (IoT) services such as e-Health, smart\ncities, extended reality, etc. Meanwhile, several research ini-\ntiatives [1], [2], [3] proposed road-maps or visions for 6G\nnetworks to overcome the limitations of 5G technologies.\nAll these works envision that 6G networks will rely on au-\ntonomous systems providing scalable, reliable, and secure ser-\nvices. For instance, the transition from 5G to 6G will introduce\nnew key technologies such as TeraHertz (THz) and optical\nwireless communications (e.g. visible light communications),\nintelligent metasurface-aided wireless communications, aerial\nnetworks, and Multi-access Mobile Edge Computing (MEC).\nThe recent success of AI techniques, namely Machine\nLearning (ML) and Deep Learning (DL), has spurred the\nadoption of a learning perspective to solve wireless control\nand management problems. For instance, Deep Neural Net-\nworks (DNN) are universal approximators able to estimate\nany function thus they can approximate optimal solutions\nfor complex tasks. DNNs can be used in three different ML\nsettings: supervised ML, unsupervised ML, and Reinforcement\nLearning (RL). Although supervised ML methods showed\npromising results in several wireless problems such as channel\nestimation and IRS joint beamforming and shift optimization,\nthey require the availability of a large amount of a priori\nlabeled training and testing data which is difﬁcult to obtain for\nreal-life scenarios. This motivates RL techniques that rely on\n“trial and error” to solve sequential decision-making problems.\nIn RL, a learning agent interacts with an environment by\nchoosing an action and observing the system’s next state and\nan immediate reward. Naturally, the agent seeks to ﬁnd optimal\nactions to maximize its rewards. To do so, two approaches\ncan be adopted: model-free or model-based. Model-based RL\n(MBRL) assumes that the agent knows the system dynamics\nthat is how the system transits from one state to another\none and how rewards are generated. This approach is not\nalways feasible, especially in complex systems where the\nagent has restricted to no knowledge about its environment.\nThis motivates model-free techniques where an agent learns\noptimal strategies without any knowledge about the system.\nAs a generalization of the single-agent RL setting, multi-agent\nRL seeks to solve decision-making problems involving more\nthan one agent.\nIn RL, we differentiate between two key steps: training\nand inference. During the training phase, the agent interacts\nwith the environment to collect experience. The environment\narXiv:2011.03615v1  [cs.LG]  6 Nov 2020\n2\nis often a simulator mimicking the real-world system since it\nis expensive to directly interact with the real system. These\ncollected experiences constitute the training dataset of the\nRL agent and will be used to learn the optimal decision-\nmaking rule. In Deep RL (DRL), DNNs are used to ap-\nproximate the agent’s optimal strategy or policy and/or its\noptimal utility function (see Figure 1). In this case, given\nthe system’s current state, the DNN learns to predict either a\ndistribution over actions or the action expected reward Q(s,a).\nTherefore, the agent chooses its next action as the one that has\neither the highest probability or the highest expected reward.\nAfter receiving the reward from the environment, the DNN\nparameters are updated accordingly. The generalization power\nof DNNs enables solving high-dimensional problems with\ncontinuous or combinatorial state spaces. In the context of\nwireless communications, DRL is advantageous compared to\nthe traditional optimization methods thanks to their real-time\ninference. However, the training phase of DNNs requires a\nconsiderable amount of computation power which necessi-\ntates the use of GPUs and high-performance CPU clusters.\nThe most popular DL frameworks are Tensorﬂow [4] and\npytorch [5]. Once the training is complete, the agent can\nmake decisions in real-time which is a considerable advantage\ncompared to traditional optimization problems. To accelerate\nthe inference of DNNs further, different libraries implement\nsophisticated compression techniques such as quantization to\nfasten the execution of DNNs on mobile or edge devices.\nFig. 1: Representation of a DRL framework.\nA. Scope of this Tutorial\nIn this work, we emphasize the role of DRL in future 6G\nnetworks. In particular, our objective is to discuss several\nDRL learning frameworks to advance the current state-of-\nthe-art and accommodate the requirements of 6G networks.\nFirst, we overview single-agent RL methods and shed light\non MBRL techniques. Although MBRL has received less\ninterest, it can show considerable advantage compared to\nmodel-free algorithms. MBRL consists in learning a model\nrepresenting the environment dynamics and utilize the learned\nmodel to compute an optimal policy. The main advantage\nof having a model of the environment is the ability to plan\nahead which makes these methods more sample-efﬁcient. In\naddition, MBRL is more robust to changes in the environment\ndynamics or rewards and has better exploratory behaviors.\nRecent progress in MBRL, especially for robotics, has shown\nthat MBRL can be more efﬁcient than MFRL. However,\nfor most applications, it is challenging to learn an accurate\nmodel of the world. For this reason, model-free algorithms\nare preferred. However, in the second part of the tutorial, we\nargue that single-agent RL is not sufﬁcient to model scalable\nand self-organizing systems often containing a considerable\nnumber of interconnected agents. This claim is justiﬁed since\nsingle-agent RL algorithms learn a decision-making rule for\none entity without considering the existence of other entities\nthat can impact its behavior. Thus, we will study extensions\nof both model-free and model-based single-agent approaches\nto multi-agent decision-making.\nMARL is the generalization of single-agent RL that enables\na set of agents to learn optimal policies using interactions\nwith the environment and each other. Thus, MARL does not\nignore the presence of the other agents during the learning\nprocess which makes it a harder problem. Essentially, several\nchallenges arise in the multi-agent case such as i) non-\nstationarity of the system due to the agents simultaneously\nchanging their behaviors; ii) scalability issue since the joint\naction space grows exponentially with the number of agents;\niii) partial observability problem arising often in real-world\napplications where agents have access only to partial informa-\ntion of system; iv) privacy and security are also core challenges\nof the deployment of MARL systems in real-world scenarios.\nMore details on these issues will be presented in Section V-A.\nMARL is generally formulated as a Markov Game (MG) or\nalso called a Stochastic Game (SG). MG generalizes Markov\nDecision Processes (MDPs) used to model single-agent RL\nproblems and repeated games in game theory literature. In\nrepeated games, the same players repeatedly play a given game\ncalled stage game. Thus, repeated games consider a stateless\nstatic environment, and the agents’ utilities are only impacted\nby the interactions between agents. This is a crucial limitation\nof normal-form game theory frameworks to model multi-agent\nproblems. MG remedies this shortcoming by considering a\ndynamic environment impacting the agents’ rewards. MGs\ncan be classiﬁed into three families fully cooperative , fully\ncompetitive or mixed. Fully cooperative scenarios assume that\nthe agents have the same utility or reward function whereas\nfully competitive settings involve agents with opposite goals\noften known as zero-sum games. The mixed setting covers the\ngeneral case where no restriction on the rewards is considered.\nThis is also referred to as general-sum games. In this paper,\nwe focus on fully cooperative MARL and consider MGs as the\nmathematical formalism to model such problems. However,\nMGs handle only problems with full observability or perfect\ninformation. Other extensions to model partial observability\nwill be discussed as well.\nBecause fully cooperative agents share the same reward\nfunction, they are obliged to choose optimal joint actions.\nIn this context, coordination between agents is crucial in\nselecting optimal joint strategies. To illustrate the importance\nof coordination, we consider the example from [6]. Let us\nexamine a scenario with two agents at a given state of the\nenvironment where they can choose between two actions a1\n3\nand a2. We assume that the joint actions (a1,a1) and (a2,a2)\nare both optimal joint actions (i.e. yield maximum reward).\nWithout coordination, the ﬁrst agent can choose a1 and the\nsecond agent can pick a2 which result in non-optimal joint\naction (a1,a2). Thus, although both agents individually choose\ndecisions induced by one of the optimal joint actions, the\nresultant joint action is not optimal. This is why coordination\nis important to make sure agents choose the same joint\naction proﬁle. The most straightforward way to establish\ncollaboration is to assume the existence of a central unit to\ncollect information from the agents, ﬁnd the optimal joint\naction, and communicate it back to the agents. In practice,\nhowever, this assumption is either costly or infeasible. Ideally,\nthe agents learn to adapt their own policies with limited to no\ncommunication while ensuring coordination. This motivates\nthe decentralized approaches in solving MARL problems.\nDifferent approaches can be adopted to solve fully co-\noperative tasks depending on the information used by the\nagents for learning. Independent Learners (IL) is a fully\ndecentralized scheme where agents have access to their local\ninformation only and independently optimize their policies to\nmaximize their returns. However, this approach rules out the\ncooperation between agents and ignores the non-stationarity\nissue. Classic MARL approaches adopted Nash Equilibrium\n(NE) as a solution concept. Nevertheless, as outlined by [7],\nsolving multi-agent scenarios using NE is problematic for\nseveral reasons. First, ﬁnding Nash equilibria is challenging\nand proved to be PPAD-complete for two-player general-sum\ngames [8]. Furthermore, in the case of multiple equilibria,\nit is questionable how the agents will converge toward the\nsame equilibrium. This encourages other solution concepts\nto solve cooperative multi-agent problems. In this tutorial,\nwe will examine DRL techniques for learning optimal joint\npolicies. We distinguish between three sub-ﬁelds of deep\nMARL: learned cooperation , emergent communication , and\nnetworked agents. Learned cooperation seeks to learn cooper-\native behaviors without any communication means. If commu-\nnication is permitted, the agents can either learn an efﬁcient\ncommunication protocol as in the emergent communication\nﬁeld or assume the existence of a communication structure like\nin the networked agents framework. The latter is advantageous\nsince it considers heterogeneous agents with different goals\ncooperating to maximize the team average return. On the\ncontrary, the other learning methods assume homogeneous\nagents with the same reward function.\nThe scope of this tutorial is summarized in Figure 2. The\ncore motivation of this work is to provide a tutorial on\nthe tools enabling the design of multi-agent algorithms in\na decentralized fashion. Decentralized MARL is an active\narea of research. The expertise of the wireless communication\ncommunity can boost the progress of this ﬁeld via design-\ning efﬁcient means of communication between agents. Other\nmulti-agent learning strategies such as federated learning and\ndistributed RL are not covered in this tutorial . Federated\nLearning aims to learn a common global model in a distributed\nfashion while maintaining the privacy of the learning agents.\nThis is different from cooperative MARL where the agents\nseek to learn their own policies without a central coordinator.\nFig. 2: Scope of the tutorial.\nIn addition, distributed RL aims to accelerate the training\nof DRL algorithms using sophisticated distributed computing\ntechniques. From now on, we will use “distributed” and\n“decentralized” interchangeably; unless otherwise stated, they\nboth refer to decentralized agents without a central controller.\nB. Existing Surveys and Tutorials\nDRL has become the cornerstone of a plethora of algorithms\nfor intelligently solving complex tasks in multiple areas such\nas MEC, aerial networks, etc. Several efforts have focused on\nsummarizing these contributions. For example, [9] provides a\ncomprehensive overview of single-agent DRL algorithms for\ncommunications and networking problems such as dynamic\nnetwork access, data rate control, wireless caching, data of-\nﬂoading, network security, and connectivity preservation. [10]\naddresses the recent DRL contributions in MEC, software-\ndeﬁned networking, and network virtualization in 5G. Other\nsurveys focus on speciﬁc applications of DRL such as au-\ntonomous Internet-of-Things [11], resource management for\n5G heterogeneous networks [12], and mobile edge caching\n[13]. Most of these works are restricted to model-free\nsingle-agent DRL methods. In addition to model-free DRL,\nour work focuses on other less explored RL techniques\nsuch as MBRL for single-agent settings and cooperative\nMARL for multiple agent scenarios.\nMore recently, the role of DRL in 6G networks has been\na topic of increasing interest. A white paper [14] on ML\nfor 6G networks overviews the key ML enabling techniques\nsuch as RL and federated learning and discusses the poten-\ntial applications in different network layers in addition to\nhighlighting several open research directions. In [15], the\nauthors reviewed the applications of ML, particularly DRL,\nin vehicular networks with a discussion on the role of AI in\nthe future 6G vehicular networks. Furthermore, [16] considers\nthe role of DL and DRL for URLLC communications in\nfuture 6G networks. Similarly, these contributions mostly\nfocus on single-agent DRL for future 6G networks with limited\nexposure to decentralized learning.\nIn this paper, we argue that single-agent RL methods are\nnot enough to meet the requirements for 6G networks in\nterms of reliability, latency, and efﬁciency. In fact, single-\nagent models of wireless problems eliminate any possibility\n4\nof cooperation or coordination in the network since the agent\nconsiders all the other agents as a part of the environment.\nFor this reason, we highlight the importance of MARL,\nparticularly cooperative MARL, in the development of scalable\nand decentralized systems for 6G networks. In this context,\n[17] showcases the potential applications of MARL to build\ndecentralized and scalable solutions for vehicle-to-everything\nproblems. In addition, the authors in [18] provide an overview\nof the evolution of cooperative MARL with an emphasis on\ndistributed optimization. Our work does not only consider\ncooperative MARL but also MBRL as enabling techniques\nfor future 6G networks and we focus on delivering a more\napplied perspective of MARL to solve wireless communication\nproblems. Table II summarizes the existing surveys on DRL\nand 6G and highlights the key differences compared to our\nwork.\nC. Contributions and Organization of this Paper\nThe main contributions of this paper can be summarized as\nfollows:\n• We provide a comprehensive tutorial on single-agent DRL\nframeworks. Model-free RL (MFRL) is based on learning\nwhereas MBRL is based on planning. To the best of our\nknowledge, this is the ﬁrst initiative to present MBRL\nfundamentals and potentials in future 6G networks. Re-\ncent developments in the MBRL literature render these\nmethods appealing for their sample efﬁciency (which is\nmeasured in terms of the minimum number of samples\nrequired to achieve a near-optimal policy) and their\nadaptation capabilities to changes in the environment;\n• We present different MARL frameworks and summarize\nrelevant MARL algorithms for 6G networks. In this work,\nwe focus on the following: emergent communication\nwhere agents can learn to establish communication pro-\ntocols to share information; learning cooperation details\ndifferent algorithms to learn collaborative behaviors in a\ndecentralized manner; networked agents to enable coop-\neration between heterogeneous agents with limited shared\ninformation;\n• We also review the literature on applications of MARL\nin several enabling technologies for 6G networks such\nas MEC and control of aerial (e.g. drone-based) net-\nworks, beamforming in cell-free massive Multiple-\nInput Multiple-Output (MIMO) communications, spec-\ntrum management in Heterogeneous Networks (HetNets)\nand in THz communications, and distributed deployment\nand control of intelligent reﬂecting surface (IRS)-aided\nwireless systems;\n• We present open research directions and challenges re-\nlated to deployment of efﬁcient, scalable, and decentral-\nized algorithms based on RL.\nThe rest of the paper is organized as follows. In Section II,\nwe introduce the mathematical background for both single-\nagent RL and MARL. Standard algorithms for single-agent RL\nare reviewed in Section III. In Section IV, we introduce MBRL\nand detail potential applications for 6G systems. Section V ﬁrst\nsummarizes the different challenges of MARL and afterward\nTABLE I: Summary of notations and symbols\nS, A, O State, action, and observation spaces\nA,O Joint action and observation spaces\nR, P Reward and transition functions respectively\nH Episode horizon or length of a trajectory\nD Replay buffer\nγ Discount factor\nπ∗ Agent’s optimal policy\nb(s) Belief state of a state s∈S\nπθ Parametrized policy with parameters θ\nπ Joint policy of multiple agents\nQπ,V π V/Q-function under the policy π\nQφ Parameterized Q-function with parameters φ\nˆQ,ˆV, ˆπ Approximate V/Q-function and policy\n¯Q Target Q-network\nJ Inﬁnite-horizon discounted return\nAdv Advantage function\ndwells on the cooperative MARL algorithms according to the\ntype of cooperation they address. Section VI is dedicated to\nrecent contributions of the mentioned algorithms in several\nwireless communication problems, followed by a conclusion\nand future research directions outlined in Section VII. A\nsummary of key notations and symbols is given in Table I.\nII. B ACKGROUND\nThe objective of this section is to present the mathematical\nbackground and preliminaries for both single agent and multi-\nagent RL.\nA. Single-Agent Reinforcement Learning\n1) Markov Decision Process\nIn RL, a learning agent interacts with an environment to\nsolve a sequential decision-making problem. Fully observ-\nable environments are modeled as MDPs deﬁned as a tuple\n(S,A,P,R,γ ). S and Adeﬁne the state and the action spaces\nrespectively; P := S×A↦→[0,1] denoted the probability of\ntransiting from a state sto a state s′after executing an action\na; R : S ×A×S ↦→R is the reward function that deﬁnes\nthe agent’s immediate reward for executing an action a at a\nstate s and resulting in the transition to s′; and γ ∈[0,1] is\na discount factor that trades-off the immediate and upcoming\nrewards. The full observability assumption of MDPs enables\nthe agent to access the exact state of the system sat every time\nstep t. Given the state s, the agent will decide to take an action\na transiting the system to a new state s′ sampled from the\nprobability distribution P(.|s,a). The agent will be rewarded\nwith an immediate compensation R(s,a,s ′). Thus, the agent’s\nexpected return is expressed as E\n[∑∞\nt=0 γtR(s,a,s ′)|a ∼\nπ(.|s),s0\n]\n. This is referred to as inﬁnite-horizon discounted\nreturn. Another popular formulation is undiscounted ﬁnite-\nhorizon return E\n[∑H\nt=0 R(s,a,s ′)|a ∼π(.|s),s0\n]\nwhere the\nreturn is compute over a ﬁnite horizon H. This is common\nin episodic tasks (i.e. tasks that have an end). Note that the\nﬁnite-horizon setting can be viewed as inﬁnite-horizon case by\n5\nTABLE II: Summary of existing surveys on DRL and MARL for 5G and beyond wireless networks\nPaper Summary RL techniques Scope\nDRL MBRL MARL\n[9] A comprehensive overview of single-agent DRL\nalgorithms for communications and networking\nproblems\n\u0013 \u0017 \u0017 Wireless Networks\n[10] Review of the recent DRL contributions in MEC,\nSoftware Deﬁned Network (SDN) and network\nvirtualization in 5G\n\u0013 \u0017 \u0017 5G networks\n[11] Survey on DRL applications and challenges in\nAutonomous IoT\n\u0013 \u0017 \u0017 Autonomous IoT\n[12] Summary of DRL applications in resource man-\nagement in 5G HetNets\n\u0013 \u0017 \u0017 5G HetNets\n[13] Survey on DRL for mobile edge caching \u0013 \u0017 \u0017 Mobile Edge Caching\n[14] Overview of different ML techniques and present\npotential applications in different network layers\n\u0013 \u0017 \u0017 6G Wireless Networks\n[15] Review of ML and DRL applications in 6G vehic-\nular networks\n\u0013 \u0017 \u0017 6G Vehicular networks\n[16] A comprehensive tutorial on URLLC communica-\ntions with a focus on the role of DL and DRL\nfor achieving URLLC communiations in future 6G\nnetworks\n\u0013 \u0017 \u0017 URLLC communications\n[17] A survey on applications of MARL in addressing\nvehicular networks related issues\n\u0013 \u0017 \u0013 5G Vehicular Networks\n[18] An overview of the evolution of cooperative\nMARL with an emphasis on distributed optimiza-\ntion\n\u0013 \u0017 \u0013 Wireless Communication\nOur work A complete and comprehensive overview of\nDRL methods to build decentralized solutions for\nB5G/6G networks\n\u0013 \u0013 \u0013 B5G/6G Wireless Networks\nTABLE III: Summary of abbreviations\nAI Artiﬁcial Intelligence MDP Markov Decision Process\nA2C Advantage Actor-Critic A3C Asynchronous Advantage Actor-Critic\nB The Bellman Operator MC Monte Carlo\nCSI Channel State Information MEC Mobile Edge Computing\nCTDE Centralized Training, Decentralized Execu-\ntion\nMFRL Model-Free RL\nDec-POMDP Decentralized Partially Observable Markov\nProcesses\nMG Markov Games\nDQN Deep Q-Networks mMTC Massive Machine-Type Communications\nDRL Deep Reinforcement Learning MADDPG Multi-Agent Deep Deterministic Policy Gra-\ndient\nDP Dynamic Programming NN Neural Networks\nDDPG Deep Deterministic Policy Gradient PG Policy Gradients\nDPG Deterministic Policy Gradient POMDP Partially Observable Markov Decision Pro-\ncess\nD2D Device-to-Device POSG Partially Observable Stochastic Game\neMBB Enhanced Mobile Broadband SG Stochastic Games\nHetNets Heterogeneous Networks TD Temporal Difference\nIL Independent Learners UA V Unmanned Aerial Vehicles\nIRS Intelligent Reﬂecting URLLC Ultra-Reliable, Low Latency Communications\nMAL Multi-Agent Learning V2V Vehicle-to-Vehicle\nMARL Multi-Agent Reinforcement Learning MMDP Multi-agent Markov Decision Process\nMBRL Model-Based RL XRL eXplainable RL\n6\naugmenting the state space with an absorbing state transiting\ncontinuously to itself with zero rewards.\nThe agent aims to ﬁnd an optimal policy π∗, a mapping from\nthe environment states to actions, that maximizes the expected\nreturn. A policy or a strategy describes the agent’s behaviour at\nevery time step t. A deterministic policy returns actions to be\ntaken in each perceived state. On the other hand, a stochastic\npolicy outputs a distribution over actions. Under a given policy\nπ, we can deﬁne a value function or a Q-function which\nmeasures the expected accumulated rewards staring from any\ngiven state st or any pair (st,at) and following the policy π,\nas shown below:\nVπ(s) = E\n[\n∑∞\nt=0 γtR(st,at,st+1)|at ∼π(.|st),s0 = s\n]\nQπ(s,a) =E\n[∑∞\nt=0γtR(st,at,st+1)|at∼π(.|st),s0 =s,a0 =a\n]\n.\nUsing Dynamic Programming (DP) methods such as Value\nIteration or Policy Iteration [19] to solve an MDP mandates\nthat the dynamics of the environment ( P and R) are known\nwhich is often not possible. This motivates the model-free\nRL approaches that ﬁnd the optimal policy without knowing\nthe world’s dynamics. MBRL methods learn a model of the\nenvironment by estimating the transition function and/or the\nreward function and use the approximate model to learn or\nimprove a policy. Model-free RL algorithms are discussed\nin detail in Section III and MBRL is further investigated in\nSection IV.\nExample: Several wireless problems have been formulated\nas MDPs. As an example, [20] presented downlink power\nallocation problem in a multi-cell environment as an MDP.\nThe agent is an ensemble of K base stations (or a controller\nfor K base stations). The state space consists of the users’\nchannel quality and their localization with respect to a given\nbase station. The agent selects the power levels for K base\nstations to maximize the entire network throughput.\n2) Partially Observable Markov Decision Process\nIn the previous section, it was assumed that the agent has\naccess to the full state information. However, this assumption\nis violated in most real-world applications. For example, IoT\ndevices collect information about their environments using\nsensors. The sensor measurements are noisy and limited, hence\nthe agent will only have partial information about the world.\nSeveral problems such as perceptual aliasing prevent the agent\nfrom knowing the full state information using the sensors’\nobservations. In this context, Partially Observable Markov\nDecision Processes (POMDP) generalize the MDP framework\nto take into account the uncertainty about the state information.\nPOMDP is described by a 7-tuple (S,A,P,R,γ,O,Z ) where\nthe ﬁrst ﬁve elements are the same as deﬁned in §II-A1;\nO is the observations space and Z : S ×A×O ↦→[0,1]\ndenotes the probability distribution over observations given\na state s ∈S and an action a ∈A. To solve a POMDP, we\ndistinguish two main approaches. The ﬁrst one is history-based\nmethods where the agent maintains an observation history\nHt = {o1,...,o t−1}or an action-observation history Ht =\n{(o0,a0),..., (ot−1,at−1)}. The history is used to learn a\npolicy π(.|Ht) or a Q-function Q(Ht,at). As an analogy with\nMDPs, the agent state becomes the history Ht. As a result,\nthis method has a large state space which can be alleviated by\nusing a truncated version of the history (i.e. k-order Markov\nmodel). However, limited histories have also caveats since long\nhistories need more computational power and short histories\nsuffer from possible information loss. It is not straightforward\nhow the value of k is chosen. Another way to avoid the\nincreasing dimension of the history with time is by deﬁning the\nnotion of belief state bt(s) = p(s|Ht),∀s∈S as a distribution\nover states. Thus, the history Ht is indirectly used to estimate\nthe probability of being at a state s. Therefore, a Q-function\nQ(b(s),a) or a policy π(.|b(s)) (see Table I) can be learned\nusing the belief states instead of the history. If the POMDP\nis known, the belief states are updated using Bayes’ rule.\nOtherwise, a Bayesian approach can be considered. Another\napproach to solve a POMDP is Predictive State Representation\n[21]. The main idea consists in predicting what will happen in\nthe future instead of relying on past actions and observations.\nExample: POMDP formulation was applied to solve differ-\nent wireless problems characterized by partial access to the\nenvironment state. For example, [22] proposed a POMDP\nrepresentation of dynamic task ofﬂoading in IoT fog systems.\nThe authors assume that the IoT devices have imperfect\nChannel State Information (CSI). In this scenario, the agent is\nthe IoT device, and based on the estimated CSI and the queue\nstate, it decides the tasks to be executed locally or ofﬂoaded.\nPOMDPs are widely used in wireless sensor networks.\nB. Multi-Agent Reinforcement Learning\nMARL tackles sequential decision-making problems involv-\ning a set of agents. Hence, the system dynamics is inﬂuenced\nby the joint action of all the agents. More intuitively, the\nreward received by an agent is no longer a function of its own\nactions but a function of all the agents’ actions. Therefore,\nto maximize the long-term reward, an agent should take into\nconsideration the policies of the other agents. In what follows,\nwe will present mathematical backgrounds for MARL. Please\nrefer to Section VI for examples on how to formulate wireless\ncommunication problems using the discussed mathematical\nframeworks below.\n1) Markov/Stochastic Games\nMGs or SGs [23] extend the MDP formalism to the multi-\nagent setting to take into account the relation between agents\n[24]. Let N >1 be the number of agents, S is the state space\nand Ai denotes the action space of the i’s agent. The joint\naction space of all agents is given by A:= A1 ×···× AN.\nFrom now on, we will use bold and underlined characters to\ndifferentiate between joint and individual functions.\nAt a state s, each agent i selects an action ai and the joint\naction a = [ai]i∈N will be executed in the environment. The\ntransition from the state s to the new state s′ is governed by\nthe transition probability function P : S ×A×S ↦→[0,1].\nEach agent i will receive an immediate reward ri deﬁned by\nthe reward function Ri : S×A×S ↦→R. Therefore, the MG is\n7\nformally deﬁned by the tuple (N,S, (Ai)i∈N,P, (Ri)i∈N,γ)\nwhere γis a discount factor. Note that the transition and reward\nfunctions in MG are dependent on the joint action space A.\nEach agent i seeks to ﬁnd the optimal policy π∗\ni : S ↦→Ai\nthat maximizes its long-term return. The joint policy π of\nall agents is deﬁned as π(a|s) = ∏\ni∈N πi(ai|s). Hence, the\nvalue-function of an agent i is deﬁned as follows:\nVπ\ni (s) = Eπ\n[\n∑∞\nt=0 γtRi(st,at,st+1)|at ∼π(st),s0 = s\n]\n.\nConsequently, the optimal policy of the agent i is a function\nof its opponents’ policies π−i.\nThe complexity of MARL systems arises from this prop-\nerty because the other agents’ policies are non-stationary\nand change during learning. See Section V-A for a detailed\ndiscussion of MARL challenges. As mentioned before, we\ndistinguish three solution concepts for MGs: fully-cooperative,\nfully competitive, and mixed. In fully cooperative settings, all\nthe agents have the same reward function Ri = R and hence\nthe same value or state-action function. Fully-cooperative MGs\nare also referred to as Multi-agent MDP (MMDP). This simpli-\nﬁes the problem since standard single-agent RL algorithms can\nbe applied if all the agents are coordinated using a central unit.\nOn the other hand, fully competitive MGs ( ∑\niRi = 0) and\ngeneral-sum MGs (∑\niRi∈R) are addressed by searching for\na NE. We will focus on the subsequent sections on extensions\nof MG for cooperative problems.\nExample: MGs are the most straightforward generalization of\nsingle-agent wireless problems to the multi-agent scenarios.\nAs an example, the problem of ﬁeld coverage by a team of\nUA Vs is modeled as MG in [25].\n(a) MDP\n (b) POMDP\n(c) Markov Games\n (d) Dec-POMDP\nFig. 3: Mathematical frameworks described in Section II.\n2) Dec-POMDP\nThe intertwinement of agents in the multi-agent setting adds\nmore complexity to ﬁnding optimal policies for each agent. In\nfact, each agent needs full information about the other agents’\nactions to maximize its long-term rewards. Consequently, the\nuncertainty about the other opponents in addition to the state\nuncertainty call for an extension of the MG framework to\nmodel cooperative agents under partial observable environ-\nments. In this context, Decentralized Partially Observable\nMarkov Decision Process (Dec-POMDP) [26] is the adopted\nmathematical framework to study the cooperative sequential\ndecision-making problems under uncertainty. This is a direct\ngeneralization of POMDPs to the multi-agent settings. A Dec-\nPOMDP is described as (N,S, (Ai)i∈N,P,R,γ, (Oi)i∈N,Z)\nwhere the six ﬁrst elements are same as deﬁned in §II-B1; R\nis a global reward function shared by all the agents; Oi is the\nobservation space of the i’s agent with O:= O1 ×···× ON\nis the joint observation space and Z : S ×A×O ↦→[0,1]\nis the observation function which provides the probability\nP(o|a,s′) of the agents observing o = [ o1 ×···× oN]\nafter executing a joint action a ∈ A and transiting to a\nnew state s′ ∈S. A Dec-POMDP is a speciﬁc case of the\nPartially Observable Stochastic Games (POSG) [27] deﬁned\nas a tuple (N,S, (Ai)i∈N,P, (Ri)i∈N,γ, (Oi)i∈N,Z) where\nall the elements are the same as in Dec-POMDP expect\nthe reward function Ri which becomes individual for each\nagent. POSG enables the modeling of self-interest agents\nwhereas Dec-POMDP exclusively models cooperative agents\nin partial observable environments. At a state s, each agent\ni receives its own observation oi without knowing the other\nagents’ observations. Thus, each agent i chooses an action\nai yielding a joint action to be executed in the environment.\nBased on a common immediate reward, each agent strives\nto ﬁnd a local policy πi : Oi ↦→ Ai that maximizes its\nteam long-term reward. Thus, the joint policy is given by\nπ= [π1,...,π N]. The policy πi is called local because each\nagent acts according to its own local observations without\ncommunicating or sharing information with the other agents.\nExample: Multi-agent task ofﬂoading [28] and multi-agent\ncooperative edge caching [29] are wireless problems which\ncan be modeled as Dec-POMDP problems.\n3) Networked Markov Games\nCooperative MGs or Dec-POMDPs are only suitable for\nhomogeneous cooperative agents since they share the same\nreward signal R1 = ··· = RN = R. However, most of real-\nworld applications involve heterogeneous agents with distinct\npreferences and goals. In addition, sharing the same reward\nfunction requires global information from all the agents to\nestimate a global value or a state-action functions which\ncomplicates the decentralization of such models. To over-\ncome these shortcomings, Networked MG generalizes the MG\nframework to model cooperative agents with different reward\nfunctions by leveraging shared information through a commu-\nnication network (see Figure 6.b). Formally, Networked MGs\nare described as a tuple (N,S, (Ai)i∈N,P, (Ri)i∈N,(Gt)t≥0)\nwhere the ﬁrst ﬁve elements are same as deﬁned in §II-B1\nand Gt = (N,Et) is a time-varying communication network\nlinking N nodes with a set of edges Et at time t. An\nedge (i,j) ∈ Et,∀i,j means that both agents i and j can\ncommunicate and share information mutually at time t. Hence-\n8\nforth, agents know their local and neighboring information\nand seek to learn an optimal joint policy by maximizing the\nteam-average reward ¯R(s,a,s ′) = 1\nN−1\n∑\ni∈N Ri(s,a,s ′) for\nany (s,a,s ′) ∈S ×A×S. To summarize, the advantages\nof Networked MGs compared to classical MGs are: (i) the\npossibility to model heterogeneous agents with different re-\nward functions; (ii) the reduction of the coordination cost\nby considering neighbor-to-neighbor communication which\nfacilitates the design of decentralized MARL algorithms; (iii)\nthe privacy preserving property since agents are not mandated\nto share their reward functions.\nExample: Networked MDP can be applied in multiple wireless\nscenarios where agents are linked with a communication\ngraph. For example, base stations in cell-free networks can\ncollaborate to compute optimal beamforming while minimiz-\ning interference [30]. The communication graph will enable\nthe base stations to share information with their neighbors.\nThus, better collaboration is possible.\nIII. S INGLE AGENT MODEL -FREE RL A LGORITHMS\nA. Preliminaries\nWe start by deﬁning useful notions and concepts for the\nunderstanding of the algorithms discussed below.\nMFRL methods can be categorized into two classes depend-\ning on the agent’s learned objective. In value-based methods,\nan approximate value function ˆV is learned and the agent’s\npolicy π is obtained by acting greedily with respect to ˆV.\nThus, state-values are essential for action selection. Policy\nevaluation methods seek to learn an estimate of the value\nfunction ˆV = Vπ for a given policy π. Alternatively, policy-\nbased methods aim to directly learn a parameterized policy\nwithout resorting to a value function. A well-known variant\nof policy-based methods learns an approximation of the value\nfunction but the action selection is still independent of the\nvalue estimates. These are the actor-critic methods where\napproximations to both the policy and the value function are\nlearned. The actor refers to the policy and the critic is the\napproximate value function. Henceforth, we will denote by θ\nthe policy’s parameters and πθ is the policy parametrized by\nθ. In actor-critic methods, Vπθ\nφ denotes the approximate value\nfunction under the policy πθ where φis a learnable parameter\nvector.\nWe distinguish between two main learning principles: (i)\nMonte Carlo (MC) and (ii) DP methods. The former methods\nutilize experience to approximate value functions and policies.\nIn contrast, DP methods are known for solving the Bellman\nOptimality equations. More details will be provided in the\nfollowing sections. Temporal Difference (TD) is a famous\ncombination of these two learning frameworks. Therefore,\nan important question arises when MC and TD methods are\nadopted: how actions are selected and samples are generated\nfor learning? This leads to the two key approaches for learning\nfrom experience, namely, off-policy and on-policy methods.\nRecall that the agent interacts with its environment by exe-\ncuting actions and after improve or evaluate its policy using\nthe collected data. Therefore, we can distinguish between\ntwo distinct processes: the policy used for data collection\nand the policy being improved or evaluated. The former is\ncalled behavior policy and the latter is referred to as target\npolicy or control policy. In off-policy methods, the behavior\npolicy is different from the target policy. However, in on-\npolicy methods, the behavior and the target policies are the\nsame, meaning that the policy used to collect the data samples\nis the same as the one being evaluated or improved. Thus,\nthe notation Vπθ\nφ means that the value function V is learned\nusing samples from the policy πθ. If πθ is the same as the\npolicy the agent is learning, this is an on-policy algorithm.\nThe advantage of an off-policy setting is the possibility to\nuse a more exploratory behavior policy to continue to visit all\nthe possible actions. This is why off-policy methods encourage\nexploration. Exploration-exploitation trade-off is a well-known\nchallenge in RL: The agent can exploit the knowledge from its\npast experiences to choose actions with the highest expected\nrewards but it also needs to explore other actions to improve\nits action selection.\nAnother key distinction between RL learning frameworks is\nthe use of Bootstrapping. The general idea of bootstrapping\nis estimated values of states are updated based on estimates\nof the values of the next states. DP and TD methods use\nbootstrapping whereas MC algorithms rely on actual complete\nepisodic returns.\nFurthermore, another dimension to consider while designing\nan RL algorithm is how to represent the approximate value\nfunction. In tabular setting, a table of state-values is main-\ntained and updated for every visited state s. Function approx-\nimators have enabled the recent revolution in RL thanks to\nits generalization power with high-dimensional state data. For\nexample, DNNs are famous non-linear function approximators\nused to compute value functions or policies.\nAs mentioned above, the source of the training data is\ncrucial for learning. On one hand, batch/ofﬂine RL considers\nthe agent is provided with a dataset of interactions and learns\na policy using the given dataset without interacting with the\nenvironment. On the other hand, the agent can collect data by\nquerying the real environment or a simulator. This is referred\nto as online RL.\nFigure 4 summarizes the different categorization of RL\nmethods. In this tutorial, we will focus on both online\npolicy-based and value-based methods with DNNs as function\napproximators. Table IV provides a comparative overview\nof the discussed algorithms below. This review of MFRL\nmethods is not exhaustive since several resources with in-depth\ndescriptions are already available (i.e. in [19], [9]).\nB. Policy-Based Algorithms\nPolicy-based methods directly search for the optimal policy\nby maximizing the agent’s expected long-term reward J as in\n(1). The policy is parameterized by a function approximator\nπθ(a|s), typically a DNN with learnable weights θ. The Policy\nGradient (PG) methods, introduced in [31], learn the optimal\nparameters θ∗by performing gradient ascent on the objective\nJ. Using the PG theorem [31], the policy gradients are\nexpressed as in (2) and estimated using samples or trajectories\ncollected under the current policy. This is why PG methods\n9\nFig. 4: Categorization of different RL settings. The classes colored in blue are covered in Section II.\nare on-policy methods. For each gradient update, the agent\nneeds to interact with the environment and collect trajectories.\nSamples collected at iteration k cannot be reused for the next\npolicy update. This sample inefﬁciency represents one of the\nmajor drawbacks of PG methods.\nJ(θ) = Eπθ\n[ ∞∑\nt=0\nγtR(st,at)\n]\n. (1)\n∇θJ(θ) = Eπθ\n[ T∑\nt=0\n∇log πθ(at|st)Qπθ(st,at)\n]\n(2)\nIn (2), Qπθ is not known and needs to be estimated. Several\napproaches are possible. The well-known REINFORCE algo-\nrithm [32] uses the rewards-to-go deﬁned as ∑T\nk=tR(sk,ak).\nThe major caveat of the REINFORCE algorithm is that it is\nwell-deﬁned for episodic problems only since the rewards-to-\ngo are computed at the end of an episode. Furthermore, REIN-\nFORCE algorithm suffers from high variance. In (2), action\nlikelihoods are multiplied by their expected return thus PG\nalgorithm shifts the action distribution such that good actions\nare more likely than bad ones. Consequently, small variations\nin the returns can lead to a completely different policy. This\nmotivates actor-critic methods where an approximation of Qπθ\nis learned. Note that it also possible to estimate the value\nfunction Vπθ or the advantage function Advπθ = Qπθ −Vπθ.\nLearning a critic reduces the variance of gradient estimates\nsince different samples are used whereas in the rewards-to-go\nonly one sample trajectory is considered. However, actor-critic\nmethods introduce bias since the Qπθ estimate can be biased\nas well. In this context, [33] proposed Generalized Advantage\nEstimation based on the idea of n-step returns to reduce the\nbias. In what follows, we will examine the most common\npolicy gradient algorithms.\nAsynchronous Advantage Actor-Critic (A3C) [34] proposes\na parallel implementation of the actor-critic algorithm. In\nthe original version of A3C, a global NN outputs the ac-\ntion probabilities and an estimate of the agent’s advantage\nfunction. Thus, the actor and the critic share the network\nlayers. Several workers are instantiated with local copies of\nthe global network parameters and the environment. These\nworkers are created as CPU threads in the same machine. In\nparallel, each worker interacts with its local environment and\ncollects experiences to estimate the gradients with respect to\nthe network parameters. Afterward, the worker propagates its\ngradients and updates the parameters of the global network.\nTherefore, the global model is constantly updated by the\nworkers. This learning scheme enables the collection of more\ndiverse experiences since each worker interacts with their local\ncopy of the environment independently. The drawback of the\nasynchronous training scheme is that some workers will be\nusing old versions of the global network. In this context,\nAdvantage Actor-Critic (A2C) adopts a synchronous and de-\nterministic implementation where all the workers’ gradients\nare aggregated and averaged to update the global network.\nAs mentioned before, PG algorithms suffer from sample\ninefﬁciency since only one gradient update is performed per a\nbatch of collected data. This motivates the goal to use the data\nmore efﬁciently. Besides, it is hard to pick the learning rate\nsince it affects the training performance and can dramatically\nalter the visitation distribution. Intuitively, a high learning rate\ncan result in a bad policy update which means that the next\nbatch of data is collected using a bad policy. Recovering from\na bad policy update is not guaranteed. This motivate Trust\nRegion Policy Optimization (TRPO) [35] where the original\noptimization problem in (1) is solved under the constraint of\nensuring the new updated policy is close to the old one. To\ndo so, the constraint is deﬁned in terms of Kullback–Leibler\ndivergence (DKL) which measures the difference between two\nprobability distributions. More formally, let θk be the policy\nparameters at iteration k. We would like to ﬁnd the new\nparameters θk+1 such that\nθk+1 = arg max\nθ\nL(θ)\n= arg max\nθ\nE(s,a)∼πθk\n[πθ(a|s)\nπθk(a|s)Avdπθk(s,a)\n]\ns.t. DKL(θ||θk) ≤δ,\n10\nwhere δ is the trust region radius. Let F be the Fisher-\ninformation matrix. With a ﬁrst-order approximation of the\nobjective ( L(θ) ≈ ∇θLT(θ)(θ −θk)) and a second-order\nTaylor expansion of the constraint ( DKL(θ||θk) ≈ 1\n2 (θ −\nθk)TF(θ −θk)), the update rule is given by θk+1 = θk +√\n2δ\n∇θLT(θ)F−1∇θL(θ) F−1∇θL(θ). The term F−1∇θL(θ) is\ncalled the natural gradients . Consequently, evaluating the\nnatural gradients necessitates inverting the matrix F which is\nexpensive. To overcome this issue, TRPO implements the con-\njugate gradient algorithm to solve the system Fx = ∇θL(θ)\nwhich involves evaluating Fx instead. Finally, the matrix-\nvector product Fx is computed as ∇θ(∇θDKL(θ||θk)Tx)\nwhich is easy to evaluate using any auto-differentiation library\nlike Tensorﬂow. In a similar vein, Proximal Policy Opti-\nmization (PPO) [36] algorithm solves the same optimization\nproblem as TRPO but proposes a simpler implementation by\nintroducing a new loss function:\nLPPO(θ) = min\nθ\n(πθ(a|s)\nπθk(a|s)Avdπθk(s,a),\nclip( πθ(a|s)\nπθk(a|s),1 −δ,1 + δ)Avdπθk(s,a)\n)\n,\nwhere “clip” is a function used to keep the values of the ratio\nπθ(a|s)\nπθk(a|s) between 1 −δ and 1 + δ to penalize the new policy\nif gets far from the old policy.\nExample: The advantage of policy-based algorithms is they\nare applicable to discrete and continuous action spaces. For\nexample, [37] applies the TRPO algorithm to ﬁnd optimal\nrouting strategies in a network.\nC. Value-Based Algorithms\nAs explained above, value-based algorithms focus on esti-\nmating the agent’s value function. Thus, the policy is computed\nimplicitly or greedily with respect to the approximate value\nfunction.\nThe MC method approximates the value of a state s by\naveraging the rewards obtained after visiting the state s until\nthe end of an episode. Consequently, MC methods are deﬁned\nonly for episodic tasks. In DP, the optimal value V∗ and\nstate-action Q∗function are computed by solving the Bellman\nOptimality equations (3-4) and thus the optimal policy is\nobtained greedily with respect to the Q-values (5):\nV∗(s) = max\na\n[\nR(s,a) + γEs′V∗(s′)\n]\n(3)\nQ∗(s) = R(s,a) + γEs′\n[\nmax\na′\nQ∗(s′,a′)\n]\n(4)\nπ∗(s) = arg max\na\nQ∗(s,a). (5)\nLet B : R|S×A| ↦→ R|S×A| denote the Bellman\noptimality operator such that [BQ](s,a) = R(s,a) +\nγ∑\ns′P(s′|s,a) maxa′Q∗(s′,a′). Therefore, equation (4) can\nbe written in more compact way Q∗= BQ∗. As a result, Q∗\nis the called the ﬁxed point of the Bellman optimality operator\nand the methods solving for this ﬁxed point can be called ﬁxed\npoint methods. The value iteration algorithm approximates\nQ∗ by iteratively applying the Bellman optimality operator\nˆQk = BˆQk−1. This algorithm is guaranteed to converge\nto Q∗ since B is a contraction and Q∗ always exists and\nis unique. Besides, value iteration relies on bootstrapping to\nestimate the value of next states. However, to evaluate the\nBellman operator, the transition function is needed. This is the\nmajor drawback of DP methods that assume the environment\ndynamics are known. To overcome this issue, TD methods\ncombine the main ideas of MC and DP. They use experience\nas in MC and bootstrapping as in DP. The update rule of TD\nalgorithm is as follows:\nˆQ(s,a) = (1−α) ˆQ(s,a) +α[R(s,a) +γmaxa′ ˆQ(s′,a′)], (6)\nwhere ˆV and ˆQ are the approximate value and state-action\nfunctions and α is a learning rate.\nTD methods can be on-policy or off-policy. Let ˆπthe policy\nderived from ˆQ(i.e. ϵ-greedy). For on-policy TD, the samples\nused to estimate ˆQ are generated using the current policy ˆπ\ncontinuously updated greedily with respect to ˆQ. SARSA is a\nwell-known on-policy TD algorithm where the agent collects\nexperiences in the form {(s,a,r,s ′,a′)}. Since the action in\nthe next state is known, the max operator in the RHS of the\nTD update (6) is removed.\nQ-learning algorithm [38] revolutionized the RL world\nallowing the development of an off-policy TD algorithm. Any\npolicy ˜π ̸= ˆπ can be used to generate experiences. When ˆQ\nis represented with a function approximator with parameters\nφ, Q-learning algorithm minimizes the Bellman error (7) and\nupdates the Q-function parameters as in (9). The Bellman error\nis not a contraction anymore thus the convergence guarantees\ndiscussed earlier are not valid anymore. Equation (8) deﬁnes\nthe targets. Note the update rule in (9) does not consider the\nfull gradient of the Bellman error since it ignores the gradients\nof the targets with respect to the parameters φ. This is why, this\nlearning algorithm is also called semi-gradient method [19].\nφ∗= arg min\nφ\n1\n2\n∑\n(s,a,r,s′)\n||ˆQφ(s,a) −y||2. (7)\ny= R(s,a) + γmax\na′\nˆQφ(s′,a′). (8)\nφ= φ−α\n∑\n(s,a,r,s′)\n∇φQφ(s,a)\n(\nQφ(s,a) −y\n)\n. (9)\nDeep versions of the Q-learning methods such as Deep Q-\nNetworks (DQN) [39] have been developed. In particular,\nthe Q-function is parameterized using a DNN with weights\nφ. DQN and its variants are the most popular online Q-\nlearning algorithms that have shown impressive results in\nseveral communication applications. To stabilize the learning\nusing DNN, DQN introduces techniques such as an experience\nreplay buffer D = {(s,a,r,s ′)}to avoid correlated samples\nand get a better gradient estimation and a target network ¯Q,\nwhose parameters φ\n′\nare periodically updated with the most\nrecent φ, making the targets (8) stationary and do not depend\non the learned parameters. The target network is updated\nperiodically as follows:\nyDQN = R(s,a) + γmax\na′\n¯Qφ′(s′,a′). (10)\nyDDQN = r(s,a) + γˆQ2(s′,arg max\na′\nˆQ1(s′,a′)). (11)\n11\nAlthough Q-learning algorithms are sample efﬁcient, they lack\nconvergence guarantees for non-linear function approximators\nand also suffer from the maximization bias which results in\nan unstable learning process. The max operator in equation\n(8) makes the algorithm overestimate the true Q-values and\nis also problematic for continuous action spaces. To over-\ncome the maximization bias, double learning technique uses\ntwo networks ˆQ1\nφ1 and ˆQ2\nφ2 with different parameters and\ndecouples the action selection from the action evaluation as\nshown in (11). Other variants of DQN have been suggested to\nguarantee a better convergence. Prioritized experience replay\nis proposed in [40] to ensure the sampling of rare and task-\nrelated experiences is more frequent than the redundant ones.\nDueling Network [41] suggests to separate the Q-function\nestimator into two separate networks: ˆV network to estimate\nthe state values and ˆAdv network to approximate the state-\ndependent action advantages.\nExample: DQN and its variants have become the go-to RL\nalgorithm to solve wireless problems with discrete action\nspaces. If the action space is continuous, researchers will\npropose a careful discretization scheme to ﬁt into the Q-\nlearning framework. As an example, [42] solves the dynamic\nmultichannel access problem in wireless networks using DQN.\nTable II lists different references that used DQN in multi-agent\nscenarios.\nD. Deterministic Policy Gradient (DPG) Algorithms\nThe max operator in (8) limits the Q-learning algorithms to\ndiscrete action space (see Table IV). In fact, when the action\nspace is discrete, it is tractable to compute the maximum\nof the Q-values. However, when the action space becomes\ncontinuous, ﬁnding the maximum involves costly optimization\nproblem. In this context, DPG algorithms [43] can be consid-\nered as an extension of Q-learning to continuous action spaces\nby replacing the maxa′Qφ(s′,a′) in (8) by Qφ(s′,µθ(s′))\nwhere µθ(s′) = arg maxa′Qφ(s′,a′). Thus, DPG algorithms\nconcurrently learn a Q-function Qφ and a policy µθ. The\nQ-function is learned by minimizing the Bellman error as\nexplained in the previous section. As for the policy, the\nobjective is to learn a deterministic policy that outputs the\naction corresponding to maxaQφ(s,a). The policy is called\ndeterministic because it gives the exact action to take at each\nstate s. Hence, the learning process consists in performing\ngradient ascent with respect to θto solve the objective in (12):\nJ(θ) = Es∼D\n[\nQφ(s,µθ(s))\n]\n. (12)\n∇θJ(θ) = Es∼D\n[\n∇θµθ(s)∇aQφ(s,a)|a=µθ(s)\n]\n. (13)\nNote that in the gradients formula above, the term\n∇aQφ(s,a) requires a continuous action space. Therefore, one\ndrawback of the DPG methods is that they cannot be applied\nto discrete action spaces. Observe as well that in (12), the\nstate is sampled from a replay buffer D which means that\nDPG algorithms are off-policy.\nDeep Deterministic Policy Gradient (DDPG) [44] has been\nwidely used to solve wireless communication problems. It is\nan extension of the DPG method where the policy µθ and\nthe critic Qφ are both DNNs. Recently, two variants of the\nDDPG algorithm have been proposed: Twin Delayed DDPG\n(TD3) [45] addresses the overestimation problem discussed in\n§ III-C whereas in Soft Actor Critic (SAC) [46] the agent\nmaximizes not only its expected return but also the policy\nentropy to improve robustness and stability.\nExample: DDPG has been widely used in continuous wireless\nproblems. In [47], DDPG is applied in energy harvesting\nwireless communications. Similarly, Table II lists several\nreferences using DDPG agents to solve multi-agent wireless\nproblems such as computation ofﬂoading, edge caching, UA V\nmanagement, etc.\nE. Theoretical Analysis and Challenges of RL\nAfter reviewing the key MFRL algorithms, we will discuss\nselective theoretical problems in both policy-based and value-\nbased methods. First, we will study the instability problems\nin value-based methods with function approximators. The\nstability means that the error ˆQ−Q∗ gets smaller when the\nnumber of iterations increases. As mentioned in the previous\nsection, the stability of the tabular Q-learning algorithm is\nguaranteed thanks to the contraction property of the Bellman\noperator. However, the contraction property is not satisﬁed\nwhen the Bellman error is minimized. Next, we focus on\nthe convergence rate and sample complexity of policy-based\nmethods. Sample complexity is deﬁned as the minimal number\nof samples or transitions to estimate Q∗ and achieve a near-\noptimal policy, and the convergence rate determines how fast\nthe learned policy converges to the optimal solution. Finally,\nwe examine the interpretability issue of DRL methods which\nis a crucial challenge towards the application of DRL in real-\nworld problems.\na) Instability of off-policy TD learning with function\napproximators\nIn tabular RL, value-based RL methods are guaranteed to\nconverge to the optimal value function which is the ﬁxed\npoint of the Bellman optimality equation. This fundamental\nresult is justiﬁed by the contraction property of the Bellman\noperator. Hence, successive applications of the Bellman op-\nerator converge to the unique ﬁxed point. However, these\nmethods combined with function approximators suffer from\ninstability and divergence. This is commonly referred to as\nthe deadly triad : function approximation, bootstrapping, and\noff-policy training [48]. These three elements are important\nto consider in any RL method. Function approximation is\ncrucial to handle large state spaces. Bootstrapping is known\nto learn faster than MC methods [19] thus this data efﬁciency\nis an advantage from using bootstrapping. Finally, off-policy\nlearning enables exploration since the behavior policy is often\nmore exploratory than the target policy. Therefore, trading off\none of these techniques means losing either generalization\npower, data efﬁciency, or exploration. This is why several\nresearch efforts are dedicated to ﬁnding stable and convergent\nalgorithms for off-policy learning with (nonlinear) function\napproximators and bootstrapping.\n12\nTABLE IV: Action and state spaces for model-free RL algorithms\nAction State On policy Off policy\nDiscrete Continuous Discrete Continuous\nPolicy-Based\nREINFORCE [32] \u0013 \u0013 \u0013 \u0013 \u0013\nA2C-A3C [34] \u0013 \u0013 \u0013 \u0013 \u0013\nPPO [36] \u0013 \u0013 \u0013 \u0013 \u0013\nTRPO [35] \u0013 \u0013 \u0013 \u0013 \u0013\nValue-Based\nDQN/DDQN [39] \u0013 \u0017 \u0013 \u0013 \u0013\nDueling DQN [41] \u0013 \u0017 \u0013 \u0013 \u0013\nDeterministic PG\nDDPG [44] \u0017 \u0013 \u0013 \u0013 \u0013\nTD3 [45] \u0017 \u0013 \u0013 \u0013 \u0013\nSAC [46] \u0017 \u0013 \u0013 \u0013 \u0013\nFixed-point methods rely on reducing the Bellman error to\nlearn an approximation of the optimal state-value or state-\naction functions. As a reminder, here is the expression of the\nobjective function governed by the Bellman error:\nL( ˆQ) = Es,a\n[(\nR(s,a) +γEs′[maxa′ ˆQ(s′,a′)] −ˆQ(s,a)\n)2]\n.\nTo compute an unbiased estimate of the loss, two samples of\nthe next state s′ are needed because of the inner expectation.\nThis is a well-known problem called the double sampling\nissue [49]. The implication of this issue is a state s must be\nvisited twice to collect two independent samples of s′ which\nis impractical. To overcome this problem, different approaches\nhave been adopted. The ﬁrst approach reformulates the Bell-\nman error as a saddle-point optimization problem where a\nnew concave function ν : S ×A ↦→ R is introduced such\nthat the loss becomes L( ˆQ,ν) = 2 Es,a,s′\n[\nν(s,a)\n(ˆQ(s,a) −\nR(s,a) −γmaxa′ ˆQ(s′,a′)\n)]\n−Es,a,s′\n[\nν(s,a)2]\n[50]. This is\ncalled a saddle-point problem since the loss will be minimized\nwith respect to the Q-function parameters and maximized\nwith respect to ν parameters. In this context, a recent work\n[51] proposed a convergent algorithm with nonlinear function\napproximators (e.g. NN) and off-policy data.\nAnother approach to tackle this issue is to replace the Q-\nfunction in the inner expectation term with another target\nfunction. The choice of the target function can be an old\nversion of the Q-function as in the famous DQN algorithm\nor the minimum of two target Q-functions as in TD3 and\nSAC. This gives a theoretical explanation of the role of target\nnetworks in stabilizing Q-learning with DNN.\nb) Convergence of PG with neural function approxi-\nmators\nIn this part, we will summarize the recent convergence\nresults of PG algorithms with NN as function approximators.\nDue to the non-convexity of the expected cumulative re-\nwards in (1) with respect to the policy and its parameters,\nthe analysis of the global optimality of stationary points is a\nhard problem. Besides, the policy gradients in (2) are obtained\nusing sampling and in practice, an approximate Q-function\nis learned to estimate the expected return. In ﬁnite-horizon\ntasks, estimating the Q-function using MC rollouts results\nin an unbiased approximation but a biased Q-function for\ndiscounted inﬁnite-horizon.\nTo tackle the unbiasedness issue, [31] introduces the com-\npatible function approximation theorem requiring the ap-\nproximate Q-function Q φ should satisfy two conditions:\n(i) compatibility condition with the policy πθ given by\n∇φQφ(s,a) = ∇θlog πθ(a|s), (ii) Q φ is learned to minimize\nEπθ[1/2(Qπθ(s,a) −Qφ(s,a))2]. If these two conditions are\nveriﬁed, the estimates of the policy gradients are unbiased.\nAnother approach presented in [52] is called random-horizon\nPG and proposed to use rollouts with random geometric time\nhorizons to unbiasedly estimate the Q-function for the inﬁnite-\nhorizon setting.\nArmed with the advances in non-convex optimization, sev-\neral research efforts ( [53], [54], [55], [52]) propose variants\nof the REINFORCE algorithm with a rate of convergence\nto ﬁrst- or second-order stationary points. [56] studies the\nnon-asymptotic global convergence rate of PPO and TRPO\nalgorithms parametrized with neural networks. These methods\nconverge to the optimal policies at the rate of O(\n√\n1/T),\nwhere T is the number of iterations. In addition, the work\nin [57] establishes the global optimality and convergence rate\nof (natural) actor-critic methods where both the actor and\nthe critic are represented by a NN. A rate of O(\n√\n1/T)\nis also proved and the authors emphasize the importance\nof the “compatibility” condition to achieve convergence and\nglobal optimality. In [58], better bounds on sample complexity\nof (natural) actor-critic are provided. In fact, the authors\ndemonstrate that the overall sample complexity for the mini-\nbatch actor-critic algorithm to reach an ϵ-accurate stationary\npoint is of O(ϵ−2 log(1/ϵ)) and that the natural actor-critic\nmethod requires O(ϵ−2 log(1/ϵ)) samples to attain an ϵ-\naccurate globally optimal point. In the same vein, the works\nin [59] and [60] establish the convergence rates and sample\ncomplexity bounds for the two-time scale scheme of (natural)\nactor-critic methods where the actor and the critic are updated\nsimultaneously with different learning rates. Furthermore, [61]\nstudies the convergence of PG in the context of constrained RL\nwhere the objective and the constraints are both non-convex\n13\nfunctions.\nc) On eXplainable RL (XRL)\nXRL is crucial to fully trust RL algorithms to be deployed in\nreal-world scenarios. Recently, a lot of efforts have focused on\ndesigning explainability algorithms for AI. However, most of\nthese works are tailored to supervised learning. XRL is more\nchallenging due to its unsupervised nature and the dependence\non complex DNN to achieve good performance. We will refer\nto the model or decisions to be explained as explanandum and\nthe generated explanations as explanans.\nAccording to [62], the term interpretability is deﬁned as\nthe ability to not only explain the model’s decisions but also\nto present these explanations in an understandable way to\noffer the possibility for non-expert users to predict the model’s\nbehavior. A common taxonomy classiﬁes interpretability mod-\nels along two main dimensions: the scope or level of the\nexplanans ( global vs local) and the time when the explana-\ntions are generated ( intrinsic vs post-hoc). Global approaches\nexplain the behavior of the whole model, whereas local ones\nprovide explanations to local predictions. The intrinsic or\ntransparent category encompasses models constructed to be\nself-explanatory by reducing their complexity. However, post-\nhoc interpretability algorithms seek to provide explanations\nof an ML model after training. Interpretability algorithms\ncan also be either model-speciﬁc if they are restricted to a\nprecise explanandum or model-agnostic if they are applica-\nble for any model/task. Intrinsic interpretability is hard to\nachieve in RL since they rely on complex DNN, thus post-\nhoc models are preferred. In addition, several XRL methods\nfocus on speciﬁc tasks/architectures, which limit their appli-\ncability to different settings. Consequently, the design of a\nmodel-agnostic approach, independent of the RL environment,\nRL learning method, and tasks, is still an ongoing research\ndirection. Furthermore, the current XRL algorithms provide\nexplanations targeted to a knowledgeable audience. Designing\nmore understandable approaches for a broader audience is also\na pressing issue to overcome in XRL. We refer the reader to\n[62], [63], and the references therein for a detailed overview\nof explainability methods in RL.\nIV. S INGLE AGENT MODEL -BASED RL A LGORITHMS\nA. Introduction to MBRL and Planning\nPlanning refers to computing an optimal policy π∗assuming\nthat the MDP M is known. Classical DP methods such as\nPolicy Iteration and Value Iteration are planning algorithms.\nIn MBRL, the agent computes an approximate model of the\nenvironment denoted by ˆM and performs planning in ˆM to\nﬁnd the optimal actions. In contrast, model-free methods, rep-\nresented previously, rely onlearning to obtain optimal policies.\nTherefore, MBRL has two important components for decision-\nmaking: the approximate model and the planning algorithm.\nFirst, we will present the concepts for learning world models\nand afterward, the planning methods are discussed. Figure (5a)\nprovides a holistic illustration of MBRL framework.\nThe learned models are representations of knowledge about\nthe real environment. In simpler words, the model can be an\napproximation of the transition function ˆP(s′|s,a) and/or the\nreward function ˆR(s,a). Consequently, for each state-action\npair (s,a), the model predicts the next state and the immediate\nreward. The model can be distributional if it outputs the dis-\ntribution over all possible next states or sampled in case only\none possible next state is produced according to the computed\nprobabilities. Furthermore, the model can be either global or\nlocal. Global models learn a representation of the whole state\nspace whereas a local model is valid only in regions of the\nstate space. Local models are more task-related and are easier\nto learn compared to global ones that require samples from the\nwhole state space. Practically, different settings are considered\nfor learning a model. In some applications, the agent may\nnot have access to the real state of the environment. This is\nthe case in POMDPs where the agent only receives partial\nobservations of the environment.\nAnother consideration is the dimension of the states. It\nmight be impossible to learn a model over high-dimensional\nstate spaces. Hence, state representation is an important feature\nof the learned dynamics. For state-based models, the transition\nfunction ˆPθ(s′|s,a) is parameterized by a DNN to learn\ndomain-agnostic dynamics. The agent collects a dataset of\ninteractions D = {(s,a,s ′)}t and the network is trained\nusing Maximum Likelihood. If the agent has access only\nto observations, it is possible to learn an observation tran-\nsition model to directly estimate the transition probabilities\nbetween observations. In the case of high-dimensional states\nor observations, the common approach is to infer a latent\nspace from the states/observations using techniques such as\nVariational Autoencoders (V AE) and learn a transition model\nis the obtained latent space. In contrast to these parametric\nmethods, several non-parametric approaches can be adopted,\nsuch as episodic memory [64], transition graphs [65], and\nGaussian Processes [66].\nArmed with our knowledge of methods to learn a model\nof the environment, we will investigate how the obtained dy-\nnamics are utilized for control. It is straightforward to observe\nthat simulated experience can be collected using the learned\nmodel to augment the available real-world samples. Planning is\ntherefore performed by applying the known RL methods such\nas Q-learning to estimate value functions using the simulated\nand real experience. The estimated value functions are thereby\nutilized to optimize or improve a policy. In this context, Dyna-\nQ algorithm [67] is a famous implementation of this approach\nof planning. It intertwines the learning and planning of the Q-\nfunction. The planning is performed by selecting uniformly\nrandom K initial state-action pairs (s,a), simulating the\nenvironment to obtain the next states and rewards, and update\nthe Q-values of the sampled pairs. Selecting the starting state-\naction pairs in a uniformly random manner is not always efﬁ-\ncient since all the pairs are treated equally. Different methods\nare proposed to address this issue. Prioritized sweeping [68]\nassigns priorities to state-action pairs according to the changes\nin their estimated values. In trajectory sampling methods, the\nsimulation is performed by collecting trajectories by following\nthe current policy to update the Q-values. All of these methods\nfall into the realm of background planning where the learned\nmodel is involved indirectly in the computation of the optimal\npolicies.\n14\nAnother approach of planning relies on the learned model to\nselect actions at the current state s. This is called decision-time\nplanning [19]. In this category, neither a policy nor a value\nfunction is required to act in the environment. In fact, action\nselection is formulated as an optimization problem (in (14))\nwhere the agent chooses a sequence of actions or a plan that\nmaximizes the expected rewards over a trajectory of length H:\na1,....,a H = arg max\na1,....,aH\nE\n[ H∑\nt=1\nR(st,at)|a1,....,a H\n]\n. (14)\nThere are two approaches to solve this planning problem\naccording to the size of the action space. For discrete action\nspaces, decision-time planning encompasses heuristic search,\nMC rollouts, and Monte Carlo Tree Search . Heuristic search\nconsists in considering a tree of possible continuations from\na state s and the values of the next actions are estimated\nrecursively by backing up the values from the leaf nodes.\nThen, the action with the highest value is selected. MC rollout\nplanning uses the approximated model to generate simulated\ntrajectories starting from each possible next action to estimate\nits value (see Figure 5a.2.a). At the end of this process, the\naction with the highest value is chosen in the current state.\nThe MCTS algorithm keeps track of the expected return of\nstate-action pairs encountered during MC rollouts to direct\nthe search toward more rewarding pairs. At the current state,\nMCTS expands a tree by executing an action according to\nthe estimated values. In the next state, MCTS evaluates the\nvalue of the obtained state by simulating MC rollouts and\npropagates it backward to the parent nodes. The same process\nis repeated in the next state. Two important observations can be\nmade regarding these approaches: (i) the estimated state-action\nvalues are completely discarded after the action is selected.\nNone of these algorithms stores Q-functions. And (ii) all these\napproaches are gradient-free.\nThe continuous action setting is more involved because\nit is complicated to perform a tree search. Alternatively,\ntrajectory optimization methods consider one possible action\nsequence a= {a1,...,a H}sampled from a ﬁxed distribution.\nThis sequence is then executed and the trajectory return\nJ(a) = ∑H\nt=0 R(st,at) is computed. Notice that the return\nis a function of the action sequence. If the learned model is\ndifferentiable, it is possible to compute the gradients of J(a)\nwith respect to the actions and update the action sequence\naccordingly (i.e. a = a+ ∇aJ(a)). This planning algorithm,\nknown as the random shooting method, exhibits several caveats\nsuch as sensitivity to the initial action selection and poor\nconvergence guarantees. This motivated a lot of varieties of\nplanning methods for continuous action spaces. For example,\nCross Entropy Method (CEM) is a famous planning approach\nto escape local optima which shooting methods suffer from.\nCompared to the shooting method, the main idea of CEM is\nto consider a normal distribution with parametrized mean and\ncovariance. The trajectories are sampled around the mean and\nthe average reward is computed for each sample to evaluate its\nﬁtness. Afterward, the mean and the covariance are updated\nusing the best samples. CEM is a simple and gradient-free\napproach and can exhibit fast convergence. For a detailed\ncomparison and benchmark of MBRL methods, we refer the\ninterested reader to [69].\nIn Table 5b, a comparison between MFRL and MBRL\nmethods, inspired from [70], is provided. Although model-\nfree methods can exhibit better asymptotic reward performance\nand are more computationally efﬁcient for deployment, model-\nbased algorithms are more data efﬁcient, robust to changes\nin the environment dynamics and rewards, and support richer\nforms of exploration. MBRL is preferred to model-free RL in\nmulti-task settings. In fact, the same learned dynamics can be\nused to perform multiple tasks without further training.\nHowever, several practical considerations should be consid-\nered for MBRL. Since the model is learned via interactions\nwith the environment, it is prone to the following problems:\n(1) insufﬁcient experience, (2) function approximation error,\n(3) small model error propagation, (4) error exploitation by\nthe planner, and (5) less reliability for longer model roll-\nouts [71]. To avoid these problems, it is recommended to\ncontinuously re-plan to avoid error accumulation. Limited\ndata can also cause model uncertainty which can be reduced\nby observing more data. Thus, it is better to estimate the\nuncertainty in the model predictions to know when to trust\nthe model to generate reliable plans/actions. One approach\nto estimate model uncertainty is Bayesian where methods\nsuch as Gaussian Processes [66] or Bayesian NN (i.e. [72])\nare applied. The work in [73] proposed to use ensemble\nmethods (bootstrapping) for uncertainty estimation where an\nensemble of models is learned and the ﬁnal predictions are the\ncombination across the models’ predictions.\nB. Applications of MBRL\nMBRL approaches received less interest from the wire-\nless communication community compared to their model-free\ncounterparts. However, we argue that MBRL is important\nto build practical systems. For instance, since it is hard to\ncollect data from a real wireless network, models are trained\nusing a simulator. The most important barrier to deploying\nsuch models in real-world settings is the reality gap where\nthe learned policy in a simulator does not perform as good\nin the real world. This is a serious issue in the context of\nbuilding RL-based algorithms for 6G networks. This line of\nwork is called sim2real which is an active area of research\nin robotics. In this context, the learned models are means to\nbridge the gap between the simulation and the real world.\nFurthermore, MBRL is advantageous because a learned model\nfor a source task can be re-used to learn a new task faster.\nCoupled with meta-learning techniques, MBRL is applied to\ngeneralize to new environments and changes in the agent’s\nworld. As an example, aerial networks or drone networks, a\nkey enabler of 6G systems, can beneﬁt from MBRL for a wide\nvariety of applications such as hovering and maneuvering tasks\n[74]. Another potential application of MBRL is related to task\nofﬂoading in MEC. A model can be learned to predict the\nload levels in edge servers. This will help the edge users to\nmake more efﬁcient ofﬂoading decisions, especially for delay-\nsensitive tasks. The main challenge in applying MBRL in\nmulti-agent problems is the non-stationarity issue. One key\n15\n(a) General Description of MBRL.\nMF MB\nAsymptotic rewards + +/−\nComputation at deployment + +/−\nData Efﬁciency − +\nAdaptation to changing rewards − +\nAdaptation to changing dynamics − +\nExploration − +\n(b) Model-free RL vs. MBRL.\nFig. 5: (a) Real experience from interactions with the environment is collected to learn a model. The model can be utilized\nin two fashions: (1) Background planning where simulated experience, generated by the model, is used to optimize a policy\nby applying any model-free learning methods on the simulated data. This planning approach is compatible with discrete and\ncontinuous action spaces; (2) Decision-time planning family seeks to ﬁnd the best sequence of actions for the current state\nusing the learned model. Depending on the size of the action space, different methods can be applied: For discrete action\nspace, Monte Carlo rollouts (2.a) or Monte Carlo Tree Search (2.c) are well-known approaches. Decision-Time planning for\ncontinuous action spaces relies on trajectory optimization (2.b) where a random action path is chosen and updated to maximize\nthe trajectory reward. (b) Comparative analysis of MBRL and model-free RL. The cases marked in yellow indicate that it\ndepends on the used MBRL methods.\napplication of MBRL in multi-agent problems is opponent\nmodeling. It consists in learning models to represent the\nbehaviors of other agents. In multi-agent systems, opponent\nmodeling is useful not only to promote cooperation and coor-\ndination but also to account for the behavior of the opponents\nto compensate for the partial observability (See Section V-A).\nIn addition, modeling other agents enables the decentralization\nof the problem because the agent can use the learned models\nto infer the strategies and/or rewards.\nThe work in [75] proposes a model-based algorithm for\ncooperative MARL. It is calledCooperative Prioritized Sweep-\ning. This paper extends the prioritized sweeping algorithm\nmentioned above to the multi-agent setting. The environment\nis modeled as factored MMDP and represented by a Dy-\nnamic Bayesian Network in which state-action interactions\nare represented as a coordination graph. This assumption\nallows the factorization of the transition function, reward\nfunction as well as the Q-values. Thus, these functions can\nbe learned in an efﬁcient and sparse manner. One drawback\nof this method is it assumes that the structure of the factored\nMDP is available beforehand which is impractical in some\napplications with high mobility. The authors in [76] consider\ntwo-agent competitive and cooperative settings in continuous\ncontrol problems. The problem addressed in [76] is how\nto learn separable models for each agent while capturing\nthe interactions between them. The approach is based on\nmulti-step generative models . Instead of learning a model\nusing one-step samples (s,a,s ′), multi-step generative models\nutilize past trajectory segments Tp with length H, Tp =\n{(st−H−1,at−H−1),..., (st,at))}to learn a distribution over\nthe future segments Tf = {(st+1,at+1),..., (st+H,at+H))}.\nHence, an encoder will learn a distribution over a latent\nvariable Z conditioned on future trajectory segments Q(Z|Tf)\nand a decoder reconstructs Tf such that ˆTf = D(Tp,Z). In\nthe 2-agent setting, the joint probability P(Tfx,Tfy) where\nTfx and Tfy are the future segments for player x and y\nrespectively. The key idea is to learn two disentangled latent\nspaces Zx and Zy. To do so, the algorithm proposed in the\npaper uses variational lower bound on the mutual information\n[77].\nV. C OOPERATIVE MULTI -AGENT REINFORCEMENT\nLEARNING\nA. Challenges and Implementation Schemes\nThis section is dedicated to discuss the challenges that\narise in multi-agent problems. Several research endeavours\n16\nproposed algorithms to address these issues which led to\ndifferent training schemes for cooperative agents. We start by\nsummarizing MARL challenges that we consider as funda-\nmental in developing systems for wireless communications.\nNon-stationarity: As mentioned before, in multi-agent en-\nvironments, players update their policies concurrently. As a\nconsequence, the agents’ rewards and state transitions depend\nnot only on their actions but also on the actions taken by their\nopponents. Hence, the Markov property, stating that the reward\nand transition functions depend only on the previous state and\nthe agent’s action, is violated and the convergence guarantees\nof single agent RL are no longer valid [78]. Due to the non-\nstationarity, the learning agent needs to consider the behaviour\nof other participants to maximize its return. One way to\novercome this issue is by using a central coordinator collecting\ninformation about agents’ observations and actions. In this\ncase, standard single-agent RL methods can be applied. Since\nthe centralized approach is not favorable, the non-stationarity\nchallenge need to be considered in designing decentralized\nMARL algorithms.\nScalability: To overcome the non-stationarity problem, it is\ncommon to include information about the joint action space\nin the learning procedure. This will give rise to a scalabil-\nity issue since the joint action space grows exponentially\nwith the number of agents. Therefore, the use of DNNs as\nfunction approximators becomes more pressing which adds\ncomplexity to the theoretical analysis of deep MARL. For\nsystems involving multiple agents (which are usually the cases\nwith wireless networks with many users), scalability becomes\ncrucial. Several research endeavors aimed to overcome this\nissue. One example is to learn factorized value function with\nrespect to the actions (see Section V-B2).\nPartial Observability : In real-world scenarios, the agents\nseldom have access to the true state of the system. They\nusually receive partial observations from the environment. Par-\ntial observability coupled with non-stationarity makes MARL\nmore challenging. In fact, as stated before, the non-stationarity\nissue mandates that the individual agents become aware of the\nother agents’ policies. With only partial information available,\nthe individual learners will struggle to overcome the non-\nstationarity of the system and account for the joint behavior.\nPrivacy and Security : Since coordination may involve\ninformation sharing between agents, privacy and security\nconcerns will arise. Shared private information (i.e. rewards)\nwith other agents is subject to attacks and vulnerabilities. This\nwill hinder the applicability of MARL algorithms in real-\nworld settings. This is why fully decentralized algorithms are\npreferred so that all the agents keep their information private.\nEnormous efforts have been made for addressing the privacy\nand security issues in supervised learning. However, in MARL,\nthis challenge is not extensively studied. Recently, the work\nin [79] has showed that attackers can infer information about\nthe training environment from a policy in single-agent RL.\nTo promote coordination while considering the challenges\ndiscussed above, different training schemes can be adopted.\n• Fully decentralized : A simple extension of single-agent\nRL to multi-agent scenarios will be IL where each agent\noptimizes its policy independently of the other participants.\nThus, the non-stationarity problem is ignored and no coor-\ndination or cooperation is considered. This technique suffers\nfrom convergence problems [80]. However, it may show\nsatisfying results in practice. In fact, recent works (see Table\nV) adopted the IL to solve several resource allocation and\ncontrol problems in wireless communication networks;\n• Fully centralized : This approach assume the existence\nof a centralized unit that can gather information such as\nactions, rewards, and observations from all the agents. This\ntraining scheme alleviates the partial observability and non-\nstationarity problems but it is impracticable for large scale\nand real-time systems;\n• Centralized training and decentralized execution : CTDE\nassumes the existence of a centralized controller that collects\nadditional information about the agents during training but\nthe learned policies are decentralized and executed using\nthe agent’s local information only. CTDE is considered in\nseveral MARL algorithms since it presents a simple solution\nto the partial observability and non-stationarity problems\nwhile allowing the decentralization of agents’ policies.\nB. Algorithms and Paradigms\nCoordination solutions considered in this tutorial are cate-\ngorized in two families: those based on communication and\nthose based on learning. Emergent communication studies the\nlearning of a communication protocol to exchange information\nbetween agents. Networked agents assume the existence of\na communication structure and learn cooperative behaviors\nthrough information exchange between neighbors. The second\nclass aims to learn cooperative behaviors without informa-\ntion sharing. The methods that we will present are not an\nexhaustive list of deep MARL since we concentrate on the key\nconcepts that can be applied for 6G technologies. We refer the\nreaders to [81], [82], and [83] for a extensive review of deep\nMARL literature.\n1) Emergent Communication\nThis an active research ﬁeld where cooperative agents are\nallowed to communicate, for example explicitly via sending\ndirect messages or implicitly by maintaining a shared memory.\nDeep communication problems are modeled as Dec-POMDPs\nwhere agents share a communication channel in a partially\nobservable environment and aim to maximize their joint utility.\nIn addition to optimizing their policies, the agents learn com-\nmunication protocols to collaborate better . Direct messages\ncan be learned concurrently with the Q-function. An NN is\ntrained to output, in addition to the Q-values, a message\nto communicate to the other participants in the next round.\nThis method involves exchanging information between all the\nagents which is expensive. Alternatively, memory-driven algo-\nrithms propose to use a shared memory as a communication\nchannel. All the agents access the shared memory before\ntaking an action and then write a response. The advantage\nof this method is that the agent does not communicate with\nthe rest of the agents directly which may eventually reduce\nthe communication cost. Besides, the agent policy depends on\n17\n(a) CTDE\n (b) Networked Agents\n (c) Fully Decentralized\nFig. 6: Three representative learning frameworks in MARL. Speciﬁcally, in (a), the centralized training and decentralized\nexecution scheme is characterized by a central unit collecting information from the agents (i.e observations, joint actions)\nto train the agents’ local policies. During execution, the agents only need their local information to act in the system. This\nscheme has a considerable communication cost which will increase with the number of users. In both (b) and (c), we present\ntwo decentralized learning structures where no central unit is needed. Figure (b) illustrates the decentralized over Networked\nAgents scheme where agents are able to communicate with their neighbors over a possibly time-varying network. This allows\nthe local information exchange. In (c), the fully decentralized or independent learners scheme is represented. In this learning\nframework, no explicit information exchange takes place between the agents.\nits private local observations and the collective memory and\nnot on messages from all the agents.\nIntegrating these methods into 6G systems requires learning\ncost-efﬁcient communication due to limited resources such\nas bandwidth. Lately, more research endeavors have focused\non learning efﬁcient communication protocols under limited-\nbandwidth constraints. Precisely, methods such as pruning,\nattention, and gating mechanisms are applied to reduce the\nnumber of messages communicated to the agents at each\ncontrol round (i.e. [84], [85], [86]). In addition to learning\ncost-efﬁcient communication protocols, this ﬁeld has many\nother open questions. For example, the robustness of the\nlearned policies to communication errors or delays caused\nby noisy channels, congestion, interference, etc needs to be\ninvestigated. [87] discusses the challenges and difﬁculties\nof learning communication in multi-agent environments. We\nargue that this technique can be useful in designing intelli-\ngent 6G systems. For example, the performance of MEC or\naerial communications systems can be boosted by integrating\ncommunication between agents. We strongly believe that this\nﬁeld can beneﬁt from the expertise of the wireless commu-\nnication community to develop more efﬁcient communication\nprotocols while taking into consideration the restrictions of the\ncommunication medians [86].\n2) Cooperation\nIn this section, we will overview coordination learning\nmethods without any explicit communication. As mentioned\nbefore, training independent and hence fully-decentralized\nagents suffer from convergence guarantees because of the non-\nstationarity problem. This issue is approached with different\nmethodologies in the deep MARL literature. The ﬁrst one\nconsists in generalizing the single-agent RL algorithms to the\nmulti-agent setting. In particular, most of the single-agent RL\nalgorithms such as DQN rely on experience replay buffers\nwhere state transitions are stored. In the multi-agent setting,\nthe data stored in replay memories are obsolete because\nagents update their policies in parallel. Several approaches\nwere proposed to address this problem and therefore enable\nthe use of replay buffers to train independent learners in\nmulti-agent environments [81]. Another line of work focused\non training cooperative agents using the CTDE framework.\nFor policy gradients, centralized critic(s) are learned using\nall agents’ policies to avoid non-stationarity and variance\nproblems and actors choose actions using local information\nonly. This method is applied, for example, in [88] to extend the\nDDPG algorithm to Multi-Agent Deep Deterministic Policy\nGradient (MADDPG) algorithm which can be used for systems\nwith heterogeneous and homogeneous agents. For Q-learning\nbased methods, the approach is to learn a centralized but\nfactorized global Q-function. For example, in [89], the team\nQ-function was decomposed as the sum of individual Q-\nfunctions whereas, in [90], the authors propose to use a mixing\nnetwork to combine the agents’ local Q-functions in a non-\nlinear way. Although these methods show promising results,\nthey can face several challenges such as representational ca-\n18\npacity [91] and inefﬁcient exploration [92]. As an alternative,\n[93] proposes to learn a single globally shared network that\noutputs different policies for homogeneous agents. All the\npresented methods above have a straightforward application\nin wireless communications since wireless networks are multi-\nagent systems by deﬁnition and coordination is crucial in such\nsystems. In fact, MADDPG has been applied in MEC and\naerial networks (i.e. UA V networks). See Section VI for more\ndetails.\n3) Decentralized MARL Over Networked Agents\nCooperative agents, modeled using a cooperative MG, usu-\nally assume that the agents share the same reward function,\nthus the homogeneity of the agents. This is not the case in most\nwireless communication problems where agents have different\npreferences or reward functions. For example, MEC networks\nencompass several types of IoT devices. Hence, it is important\nto account for the heterogeneity of the agents in the design\nof decentralized cooperative algorithms for 6G systems. In\nthis context, the objective is to form a team of heterogeneous\nagents (i.e. with different reward functions) collaborating to\nmaximize the team-average reward ¯R = 1\nN\n∑\ni∈N Ri. As\nexplained in Section II, networked agents cooperate and make\ndecisions using their local observations including shared in-\nformation by the neighbors over a communication network.\nThe existence of the communication network enables the\ncollaboration between agents without the intervention of a\ncentral unit. Let πi\nθi be the agent’s policy parametrized as a\nDNN. The joint policy is given by πθ = ∏\ni∈N πi\nθi(ai|s) and\nthe global Q-function is Qθ under the joint policy πθ. To ﬁnd\noptimal policies, the policy gradients, for each agent, can be\nexpressed as the product of the global Q-function Qθ and the\nlocal score function ∇θi log πi\nθi(ai|s). However, Qθ is hard\nto estimate knowing that the agents can only use their local\ninformation. Consequently, each agent learns a local copy Qθi\nof Qθ. In [94], an actor-critic algorithm is proposed where a\nconsensus-based approach is adopted to update the critics Qθi\nas the weighted average of the local and adjacent updates.\nWe refer the interested reader to [95] for other extensions and\nalgorithms for this framework with a theoretical analysis.\nVI. A PPLICATIONS\nA. MARL for MEC Systems\nMulti-access edge computing (MEC) is one of the enabling\ntechnologies for 5G and beyond 5G networks. We are witness-\ning a proliferation of smart devices running computationally\nexpensive tasks such as gaming, Virtual/Augmented Reality.\nTherefore, designing efﬁcient algorithms for MEC systems is\na crucial step in providing low-latency and high-reliability ser-\nvices. DRL has been extensively applied to solve several prob-\nlems in MEC networks including task/computation ofﬂoading\n(i.e. [96], [97]), edge caching [98], network slicing, resource\nallocation, etc [99]. Recently, more interest is accorded to\nMARL in MEC networks to account for the distributed nature\nof these networks.\nTask ofﬂoading has been studied in several works from a\nmulti-agent perspective with a focus on decentralized execu-\ntion. First, we examine the works proposing fully decentralized\nalgorithms based on the IL framework. In [100], each mobile\nuser is represented by a DDPG/DQN agent aiming to inde-\npendently learn an optimal task ofﬂoading policy to minimize\nits power consumption and task buffering delays. Therefore,\nthis paper provides a fully decentralized algorithm where users\ndecide using their local observations (task buffer length, user\nSINR, CSI), the allocated power levels for local execution, and\ntask ofﬂoading. Similarly, the approach in [101] is based on\nindependent Q-learning where each edge user selects the trans-\nmit power, the radio access technology, and the sub-channel.\nThe problems considered in the previous papers are formulated\nas MGs where the global state is the concatenation of the\nusers’ local observations and the agents act simultaneously on\nthe system to receive independent reward signals. Thus, this\nformalization enables the consideration of heterogeneous users\nwith different reward functions. Furthermore, [102] formalizes\nthe task ofﬂoading as a non-cooperative problem where each\nagent aims to minimize the task drop rate and execution delay.\nEach mobile user is represented as an A2C agent. An energy-\naware algorithm is presented in [103] where independent DQN\nagents are deployed in every edge server and the servers decide\nwhich user(s) should ofﬂoad their computations. However, the\nindependent nature of these works rules out any coordination\nbetween the learning agents which may hinder the convergence\nof these methods in practice (see Section V-A).\nCooperation is considered in [28] where the authors used\nthe MADDPG algorithm to jointly optimize the multi-channel\naccess and task ofﬂoading of MEC networks in industry\n4.0. The joint optimization problem is modeled as a Dec-\nPOMDP since the agents cannot observe the status of all the\nchannels and is solved using the CTDE paradigm. The use of\nMADDPG enables the coordination between agents without\nany explicit communication since the critic is learned using\nthe information from all the agents but the actors are executed\nin a decentralized matter. Experimental results showcased the\nimpact of cooperation in reducing the computation delay and\nenhancing the channel utilization compared to the IL case.\nFor edge caching , [29] and [104] propose MADDPG-like\nalgorithms to solve the cooperative multi-agent edge caching\nproblem. Both of these works model the cooperative edge\ncaching as Dec-POMDP and differ in the deﬁnition of the\nstate space and reward functions. In [29], the edge servers\nreceive the same reward as the average transmission delay\nreduction, whereas in [104], the weighted sum of the local\nand the neighbors’ hit rates is considered as a reward signal\nto encourage cooperation between adjacent servers. Simulation\nresults showed that the cooperative edge caching outperforms\ntraditional caching mechanisms such as Least Recently Used\n(LRU), Least Frequently Used (LFU), and First In First Out\n(FIFO).\nTo summarize, to offer massive URLLC services, the scal-\nability of MEC systems is crucial. We expect to see more\nresearch efforts leveraging the deep MARL techniques to study\nand analyze the reliability-latency-scalability trade-off of fu-\nture 6G systems. For example, applying the networked agents\nscheme to the above-mentioned problems is one direction to\nexplore in future works.\n19\nB. MARL for UAV-Assisted Wireless Communications\nThe application of deep MAR in UA V networks is getting\nmore attention recently. In general, these applications involve\nsolving cooperative tasks by a team of UA Vs without the inter-\nvention of a central unit. Hence, in UA V network management,\ndecentralized MARL algorithms are preferable in terms of\ncommunication cost and energy efﬁciency. The decentralized\nover networked agents scheme is suitable for this application\nif we assume that the UA Vs have sufﬁcient communication\ncapabilities to share information with the neighbors in its\nsensing and coverage area. However, due to the mobility of\nUA Vs, maintaining communication links with its neighbors\nto coordinate represents a considerable handicap for this\nparadigm.\nIn [105], the authors study the cooperation in links discovery\nand selection problem. Each UA V agent u perceives the local\navailable channels and decides to establish a link with another\nUA Vv over a shared channel. Due to different factors such\nas UA V mobility, wireless channel quality, and perception\ncapabilities, each UA Vuhas a different local set of perceived\nchannels Cu such that Cu ∩Cv ̸= ∅. A link is established\nbetween two agents uand vif they propagate messages on the\nsame channel simultaneously. Given the local information (i.e.\nstate) about whether the previous message was successfully\ndelivered, each UA V’s action is a pair (v,cu) denoting by\nv the other UA V and cu the propagation channel. Each agent\nreceives a reward ru deﬁned as the number of successfully sent\nmessages over time-varying channels. The algorithm proposed\nin [105] is based on independent Q-learning with two main\nmodiﬁcations: fractional slicing to deal with high dimensional\nand continuous action spaces and mutual sampling to share\ninformation (state-action pairs and Q-function parameters)\nbetween agents to alleviate the non-stationarity issue in the\nfully decentralized scheme. Thus, a central coordinating unit\nis necessary. The problem of ﬁeld coverage by a team of UA Vs\nis addressed in [25]. The authors formulated the problem as a\nMG where each agent state is deﬁned as its position in a 3D\ngrid. The UA Vs cooperate to maximize the full coverage in\nan unknown ﬁeld. The UA Vs are assumed to be homogeneous\nand have the same action and state spaces. The proposed\nalgorithm is based on Q-learning where a global Q-function is\ndecomposed using the approximation techniques Fixed Sparse\nRepresentation (FSR) and Radial Basis Function (RBF). This\ndecomposition technique does not allow the full decentraliza-\ntion of the algorithm since the basis functions depend on the\njoint state and action spaces. Thus, the UA Vs need to share\ninformation which has an important communication cost.\nAnother application of MARL in UA V networks is spectrum\nsharing which is analyzed in [106]. The UA V team is divided\ninto two clusters: the relaying UA Vs which provide relaying\nservices for the terrestrial primary users to spectrum access\nfor the other cluster that groups sensing UA Vs transmitting\ndata packets to a fusion center. Each UA V’s action is either\nto join the relaying or sensing clusters. The authors proposed\na distributed tabular Q-learning algorithm where each UA V\nlearns a local Q function using their local states without any\ncoordination with the other UA Vs. In a more recent work\n[107], the joint optimization of multi-UA V target assignment\nand path planning is solved using MARL. A team of UA Vs\npositioned in a 2D environment aims to serve T targets while\nminimizing the ﬂight distance. Each UA V covers only one\ntarget without collision with threat areas and other UA Vs.\nTo enforce the collision-free constraint, a collision penalty is\nadded to the reward thus rendering the problem a mixed RL\nsetting with both cooperation and competition. Consequently,\nthe MADDPG algorithm is adopted to solve the joint opti-\nmization. Furthermore, [108] formulates resource allocation\nin a downlink communication network as a SG and solved\nit using independent Q-learning. The work in [109] applies\nMARL for ﬂeet control, particularly, aerial surveillance and\nbase defense in a fully centralized fashion.\nC. MARL for Beamforming Optimization in Cell-Free MIMO\nand THz Networks\nDRL has been extensively applied for uplink/downlink\nbeamforming optimization. Particularly, several works focused\non beamforming computation in cell-free networks. In the\nfully centralized version of the cell-free architecture, all the\naccess points are connected and coordinated through a central\nprocessing unit to serve users in their coverage area. Although\nthe application of single-agent DRL for cell-free networks\nhas empirically shown optimal performance, the computational\ncomplexity and the communication cost increase drastically\nwith the number of users and access points. As a remedy\nto this issue, hybrid methods based on dynamic clustering\nand network partitioning are proposed. The core idea of these\nmethods is to cluster users and/or access points to reduce the\ncomputational and communication costs as well as to enhance\nthe coverage by reducing interference. As an example, in\n[30], a DDQN algorithm is implemented to perform dynamic\nclustering and a DDPG agent is dedicated to beamforming\noptimization. This joint clustering and beamforming opti-\nmization is formulated as an MDP and a central unit is\nused for training and execution. In [110], dynamic downlink\nbeamforming coordination is studied in a multi-cell MISO\nnetwork. The authors proposed a distributed algorithm where\nthe base stations are allowed to share information via a limited\nexchange protocol. Each base station is presented as a DQN\nagent trying to maximize its achievable rate while minimizing\nthe interference with the neighboring agents. The use of\nDQN networks required the discretization of the action space\nwhich is continuous by deﬁnition. The same framework can\nbe applied with PG or DDPG methods to handle continuous\naction space.\nFurthermore, THz communication channels are character-\nized by high attenuation and path loss which require trans-\nmitting highly directive beams to minimize the signal power\npropagating in directions other than the transmission direction.\nIn this context, directional beamforming and beam selection\nare possible solutions to enhance the communication range and\nreduce interference. Intelligent beamforming in THz MIMO\nsystems is another promising application of MARL for future\n6G networks.\n20\nD. Spectrum Management\nIn [111] and [112], the spectrum allocation in Device-to-\nDevice (D2D) enabled 5G HetNets is considered. The D2D\ntransmitters aim to select the available spectrum resources with\nminimal interference to ensure the minimum performance re-\nquirements for the cellular users. The authors in [111] consider\na non-cooperative scenario where the agents independently\nmaximize their throughput. Based on the selected resource\nblock in the previous time step, the D2D users choose the\nresource block for uplink transmission and receive a positive\nreward equivalent to the capacity of the D2D user if the\ncellular users’ constraints are satisﬁed, otherwise, a penalty\nis imposed. The problem is solved using a tabular Q-learning\nalgorithm where each D2D agent learns a local Q-function\nwith local state information. This work does not scale for\nhigh-dimensional state spaces since it is based on a tabular\napproach. This problem is addressed in [112] where actor-\ncritic based algorithms are proposed for spectrum sharing. Two\napproaches are used to promote cooperation between D2D\nusers. The ﬁrst is called multi-agent actor-critic where each\nagent learns a Q-function in a centralized manner using the\ninformation from all the other agents. The learned policies\nare executed in a decentralized fashion since the actor relies\non local information only. The second approach proposes to\nuse information from neighboring agents only to train the\ncritic instead of the information from all the agents to reduce\nthe computational complexity for large-scale networks. The\naction selection and the reward function are similar to the ones\ndeﬁned in the previous work. In this work, the state space is\nricher and contains information about (i) the instant channel\ninformation of the D2D corresponding link, (ii) the channel\ninformation of the cellular link (e.g. from the BS to the D2D\ntransmitter), (iii) the previous interference to the link, and (iv)\nthe resource block selected by the D2D link in the previous\ntime slot.\nAnother application of MARL is resource allocation\nin cellular-based vehicular communication networks. The\nVehicle-to-Vehicle (V2V) transmitters jointly select the com-\nmunication channel and the power level for transmission. The\nwork in [113] proposes a fully decentralized algorithm based\non DQN to maximize the Vehicle-to-Infrastructure capacity\nunder V2V latency constraints. Although the solution is decen-\ntralized in the sense that each agent trains a Q-network locally,\nthe state contains information about the other participants. The\nauthors include a penalty in the reward function to account\nfor the latency constraints in V2V communications. For more\nreferences on MARL in vehicular communications, we refer\nthe reader to [17].\nInterference mitigation will be a pressing issue in THz\ncommunications. Exploiting the THz bands is one key enabler\nof 6G systems for higher data rates. In [114], a multi-arm ban-\ndit based algorithm is proposed for intermittent interference\nmitigation from directional links in two-tier HetNets. However,\nthe proposed solution is valid for a single target receiver.\nAnother work in [115] proposes a two-layered distributed D2D\nmodel, where MARL is applied to maximize user coverage in\ndense-indoor environments with limited resources (i.e. a single\nTHz access point, limited bandwidth, and limited antenna\ngain). Devices in the ﬁrst layer can directly access the network\nresources and act as relays for the second layer devices. The\nobjective is to ﬁnd the optimal D2D links between the two\nlayers. The devices from the ﬁrst layer are modeled as Q-\nlearning agents and decide, by using local information, the\nD2D links to establish with the second layer devices. The\nagents receive two types of rewards: a private one for serving\na device in the second layer and a public reward regarding\nthe system throughput. To promote coordination, the agents\nreceive information about the number of their neighbors and\ntheir states. [116] studies a two-tier network with virtual-\nized small cells powered by energy harvesters and equipped\nwith rechargeable batteries. These cells can decide to ofﬂoad\nbaseband processes to a grid-connected edge server. MARL\nis applied to minimize grid energy consumption and trafﬁc\ndrop rate. The agents collaborate via exchanging information\nregarding their battery state. [117] inspects the problem of\nuser association in dynamic mmWave networks where users\nare represented as DQN agents independently optimizing their\npolicies using their local information.\nE. Intelligent Reﬂecting Surfaces (IRS)-Aided Wireless Com-\nmunications\nIntelligent Reﬂecting Surfaces (IRS)-aided wireless commu-\nnications have attracted increasing interest due to the coverage\nand spectral efﬁciency gains they provide. Multiple research\nworks proposed DRL-based algorithms for joint beamforming\nand phase shift computation. These contributions study sys-\ntems with a single IRS which is far from the real-world case.\nMore recent research endeavors seek to remedy this shortcom-\ning. For example, in [118], the authors consider a communica-\ntion system with multiple IRS cooperating together under the\ncoordination of an IRS controller. The joint beamforming and\nphase shift optimization problem is decoupled and solved in\nan alternating manner using fractional programming. Another\nline of work aims to provide secure and anti-jamming wireless\ncommunications by adjusting the IRS elements. This problem\nwas also approached using single-agent RL (i.e. in [119]). For\ndistributed deployment of multiple IRS, secure beamforming is\nsolved in [120] using alternating optimization scheme based on\nsuccessive convex approximation and manifold optimization\ntechniques. To the date of the writing of this tutorial, we are\nunable to ﬁnd a decentralized algorithm for a multi-IRS system\nbased on MARL techniques. This is why we believe that this\nis a promising direction to propose distributed deployment\nschemes for multi-IRS systems.\nVII. C ONCLUSION AND FUTURE RESEARCH DIRECTIONS\nWe have presented an overview of model-free, model-based\nsingle-agent RL, and cooperative MARL frameworks and al-\ngorithms. We have provided the mathematical backgrounds of\nthe key frameworks for both single-agent and multi-agent RL.\nAfterward, we have developed an understanding of the state-\nof-the-art algorithms in the studied ﬁelds. We have discussed\nthe use of model-based methods for solving wireless commu-\nnication problems. Focusing on cooperative MARL, we have\n21\nTABLE V: These papers applied MARL techniques for wireless communication problems. Learning type is the MARL training\ntechnique.\nWork Summary Learning Algorithm Cooperation?\n[100] Proposes a decentralized computation ofﬂoading algorithm\nfor multi-user MEC networks to allocate power levels for\nlocal execution and task ofﬂoading\nIL DDPG/DQN \u0017\n[101] Formulates computation ofﬂoading in multi-user MEC sys-\ntems as MARL problem to allocate transmit power, radio\naccess technology and subchannel\nIL Q-learning \u0017\n[102] Studies the task ofﬂoading problem in a non-cooperative\nmanner where agents learn independent policies to mini-\nmize the task drop rate and the execution delay\nIL A2C \u0017\n[103] An energy-aware multi-agent algorithm is proposed where\neach edge server decide which users should ofﬂoad their\ncomputations from a pool of edge devices\nIL DQN \u0017\n[28] Considers joint multi-channel access and task ofﬂoading in\ncooperative MEC networks\nCTDE MADDPG \u0013\n[29] Solves cooperative edge caching in MEC systems using\nMARL with shared reward signal\nCTDE MADDPG-like \u0013\n[104] Proposes different formulation (i.e. state space, reward\nfunction) of the MARL system for cooperative edge\ncaching\nCTDE MADDPG-like \u0013\n[105] Introduces fractional slicing and mutual sampling to learn\ncooperative links discovery and selection using independent\nQ-learning algorithm\nIL DQN \u0013\n[25] Studies a team of UA Vs providing the full coverage of an\nunknown ﬁeld while minimizing the overlapping UA Vs’\nﬁeld of views\nCentralized Q-learning \u0013\n[106] Considers spectrum sharing in UA V network and propose\na distributed algorithm where each UA V decides whether\nto join a relaying or sensing clusters\nIL Q-learning \u0017\n[107] The joint optimization of task-assignment and path plan-\nning in a multi-UA V network is studied\nCTDE MADDPG \u0013\n[108] Solves the resource allocation problem in a multi-UA V\ndownlink communication network using MARL\nIL Q-learning \u0017\n[109] Develops a multi-UA V ﬂeet control systems particularly for\naerial surveillance and base defense\nCentralized PG \u0013\n[110] Considers dynamic downlink beanmforming coordination\nwhere base stations cooperate to maximize their capac-\nity while mitigating inter-cell interference via a limited-\ninformation sharing protocol\nNetworked Agents DQN \u0013\n[111] Considers non-cooperative spectrum allocation in D2D\nHetNets with stochastic geometry\nIL Q-learning \u0017\n[112] Formulates a cooperative spectrum sharing problem in D2D\nunderlay communications as a decentralized multi-agent\nsystem\nCTDE MADDPG-like \u0013\n[113] Proposes a decentralized algorithm for joint sub-band and\npower level allocation in V2V enabled cellular networks\nIL DQN \u0017\n[115] A 2-layered multi-agent D2D model in THz communi-\ncations is proposed to maximize user coverage in dense-\nindoor environments\nNetworked Agents Q-learning \u0013\n[116] Applies MARL to minimize grid energy consumption and\ntrafﬁc drop rate in two-tier network with virtualized small\ncells\nNetworked Agents Q-learning \u0013\n[117] Solves user association problem in dynamic mmWave\nnetworks\nIL DQN \u0017\n22\noutlined different methods to establish coordination between\nagents. To showcase how these methods can be applied for\nwireless communication systems, we have reviewed the re-\ncent research contributions to adopt a multi-agent perspective\nin solving communication and networking problems. These\nproblems involve AI-enable MEC systems, intelligent control\nand management of UA V networks, distributed beamforming\nfor cell-free MIMO systems, cooperative spectrum sharing,\nTHz communications, and IRS deployment. We have chosen\nto focus on cooperative MARL applications because several\nsurveys on single-agent RL exist in the literature. Our objective\nhas been to highlight the potential of these DRL methods,\nspeciﬁcally MARL, in building self-organizing, reliable, and\nscalable systems for future wireless generations. In what\nfollows, we discuss research directions to enrich and bridge\nthe gap between both ﬁelds.\n• Network topologies : One of the shortcomings of the\nMG-based methods of MARL problems is the assumed\nhomogeneity of the studied systems. However, this is\nseldom the case in real-world scenarios like MEC-IoT\nor sensing networks. For this reason, we have motivated\nthe networked MARL paradigm where agents with dif-\nferent reward functions can cooperate. Accounting for the\nheterogeneity of the wireless communications systems is\nmandatory for practical algorithm design. Mobility is also\na challenge in wireless communication problems. Devel-\noping MARL algorithms with mobility considerations is\nan interesting research direction.\n• Constrained/safe RL : RL is based on maximizing the\nreward feedback. The reward function is designed by\nhuman experts to guide the agent policy search but reward\ndesign is often challenging and can lead to unintended\nbehavior. Wireless communication problems are often\nformulated as optimization problems under constraints.\nTo account for those constraints, most of the recent works\nadopt a reward shaping strategy where penalties are added\nto the reward function for violating the deﬁned con-\nstraints. In addition, reward shaping does not ensure that\nthe exploration during the training is constraint-satisfying.\nThis motivates the constrained RL framework. It enables\nthe development of more reliable algorithms ensuring\nthat the learned policies satisfy reasonable service quality\nor/and respect system constraints.\n• Theoretical guarantees : Despite the abundance of the\nexperimental works around RL methods, their conver-\ngence properties are still an active research area. Several\nendeavors, reviewed above, proposed convergence guar-\nantees for the policy gradient algorithms under speciﬁc\nassumptions such as unbiased gradient estimates. More\nurging theoretical questions need to be addressed such as\nthe convergence speed to a globally optimal solution, their\nrobustness due to approximation errors, their behaviors\nwhen limited sample data is available, etc.\n• Privacy: One of the challenges of the commercialization\nof DRL-based solutions is privacy. These concerns are\nrooted in the data required to train RL agents such as\nactions and rewards. Consequently, information about\nthe environment dynamics, the reward functions can be\ninferred by malicious agents [79]. Privacy-preserving al-\ngorithms are attracting more attention and interest. Differ-\nential privacy was investigated in the context of Federated\nLearning as well as DRL [121]. Privacy is not sufﬁciently\nexplored in the context of wireless communication.\n• Security and robustness : DNNs are known to be vul-\nnerable to adversarial attacks and several recent works\ndemonstrated the vulnerability of DRL to such attacks\nas well. To completely trust DRL-based methods in\nreal-world critical applications, an understanding of the\nvulnerabilities of these methods and addressing them is\na central concern in the deployment of AI-empowered\nsystems [122], [123]. In addition to adversarial attacks,\nthe robustness of the learned policies to differences in\nsimulation and real-world settings need to be addressed\nand studied (i.e. [124]).\nREFERENCES\n[1] W. Saad, M. Bennis, and M. Chen, “A vision of 6G wireless systems:\nApplications, trends, technologies, and open research problems,” IEEE\nnetwork, vol. 34, no. 3, pp. 134–142, 2019.\n[2] I. F. Akyildiz, A. Kak, and S. Nie, “ 6G and beyond: The future of\nwireless communications systems,” IEEE Access, vol. 8, pp. 133995–\n134030, 2020.\n[3] L. Bariah, L. Mohjazi, S. Muhaidat, P. C. Sofotasios, G. K. Kurt,\nH. Yanikomeroglu, and O. A. Dobre, “A prospective look: Key enabling\ntechnologies, applications and open research topics in 6G networks,”\narXiv preprint arXiv:2004.06049 , 2020.\n[4] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, G. Irving, M. Isard, et al. , “Tensorﬂow: A system\nfor large-scale machine learning,” in 12th {USENIX} symposium on\noperating systems design and implementation ( {OSDI} 16), pp. 265–\n283, 2016.\n[5] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. , “Pytorch: An\nimperative style, high-performance deep learning library,” in Advances\nin neural information processing systems , pp. 8026–8037, 2019.\n[6] C. Boutilier, “Planning, learning and coordination in multiagent deci-\nsion processes,” in Proceedings of the 6th conference on Theoretical\naspects of rationality and knowledge , pp. 195–210, 1996.\n[7] Y . Shoham, R. Powers, and T. Grenager, “Multi-agent reinforcement\nlearning: a critical survey,” Web manuscript, vol. 2, 2003.\n[8] X. Chen, X. Deng, and S.-H. Teng, “Settling the complexity of\ncomputing two-player nash equilibria,” Journal of the ACM (JACM) ,\nvol. 56, no. 3, pp. 1–57, 2009.\n[9] N. C. Luong, D. T. Hoang, S. Gong, D. Niyato, P. Wang, Y .-C.\nLiang, and D. I. Kim, “Applications of deep reinforcement learning\nin communications and networking: A survey,” IEEE Communications\nSurveys & Tutorials, vol. 21, no. 4, pp. 3133–3174, 2019.\n[10] Y . Qian, J. Wu, R. Wang, F. Zhu, and W. Zhang, “Survey on rein-\nforcement learning applications in communication networks,” Journal\nof Communications and Information Networks, vol. 4, no. 2, pp. 30–39,\n2019.\n[11] L. Lei, Y . Tan, K. Zheng, S. Liu, K. Zhang, and X. Shen, “Deep\nreinforcement learning for autonomous internet of things: Model, ap-\nplications and challenges,” IEEE Communications Surveys & Tutorials,\n2020.\n[12] Y . L. Lee and D. Qin, “A survey on applications of deep reinforcement\nlearning in resource management for 5G Heterogeneous Networks,”\nin 2019 Asia-Paciﬁc Signal and Information Processing Association\nAnnual Summit and Conference (APSIPA ASC) , pp. 1856–1862, IEEE,\n2019.\n[13] H. Zhu, Y . Cao, W. Wang, T. Jiang, and S. Jin, “Deep reinforcement\nlearning for mobile edge caching: Review, new features, and open\nissues,” IEEE Network, vol. 32, no. 6, pp. 50–57, 2018.\n[14] S. Ali, W. Saad, N. Rajatheva, K. Chang, D. Steinbach, B. Sliwa,\nC. Wietfeld, K. Mei, H. Shiri, H.-J. Zepernick, et al. , “ 6G white\npaper on machine learning in wireless communication networks,” arXiv\npreprint arXiv:2004.13875, 2020.\n23\n[15] F. Tang, Y . Kawamoto, N. Kato, and J. Liu, “Future intelligent and\nsecure vehicular network toward 6G: Machine-learning approaches,”\nProceedings of the IEEE , vol. 108, no. 2, pp. 292–307, 2019.\n[16] C. She, C. Sun, Z. Gu, Y . Li, C. Yang, H. V . Poor, and B. Vucetic,\n“A tutorial of ultra-reliable and low-latency communications in 6G:\nIntegrating theoretical knowledge into deep learning,” arXiv preprint\narXiv:2009.06010, 2020.\n[17] I. Althamary, C.-W. Huang, and P. Lin, “A survey on multi-agent\nreinforcement learning methods for vehicular networks,” in 2019 15th\nInternational Wireless Communications & Mobile Computing Confer-\nence (IWCMC), pp. 1154–1159, IEEE, 2019.\n[18] D. Lee, N. He, P. Kamalaruban, and V . Cevher, “Optimization for\nreinforcement learning: From a single agent to cooperative agents,”\nIEEE Signal Processing Magazine , vol. 37, no. 3, pp. 123–135, 2020.\n[19] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, 2018.\n[20] K. I. Ahmed and E. Hossain, “A deep Q-learning method for\ndownlink power allocation in multi-cell networks,” arXiv preprint\narXiv:1904.13032, 2019.\n[21] M. L. Littman and R. S. Sutton, “Predictive representations of state,”\nin Advances in neural information processing systems , pp. 1555–1561,\n2002.\n[22] R. Xie, Q. Tang, C. Liang, F. R. Yu, and T. Huang, “Dynamic\ncomputation ofﬂoading in IoT fog systems with imperfect channel state\ninformation: A POMDP approach,” IEEE Internet of Things Journal ,\n2020.\n[23] L. S. Shapley, “Stochastic games,” Proceedings of the national academy\nof sciences, vol. 39, no. 10, pp. 1095–1100, 1953.\n[24] M. L. Littman, “Markov games as a framework for multi-agent rein-\nforcement learning,” in Machine learning proceedings 1994 , pp. 157–\n163, Elsevier, 1994.\n[25] H. X. Pham, H. M. La, D. Feil-Seifer, and A. Neﬁan, “Cooperative and\ndistributed reinforcement learning of drones for ﬁeld coverage,” arXiv\npreprint arXiv:1803.07250, 2018.\n[26] F. A. Oliehoek, C. Amato, et al., A concise introduction to decentralized\nPOMDPs, vol. 1. Springer, 2016.\n[27] E. A. Hansen, D. S. Bernstein, and S. Zilberstein, “Dynamic pro-\ngramming for partially observable stochastic games,” in AAAI, vol. 4,\npp. 709–715, 2004.\n[28] Z. Cao, P. Zhou, R. Li, S. Huang, and D. Wu, “Multi-agent deep rein-\nforcement learning for joint multi-channel access and task ofﬂoading\nof mobile edge computing in industry 4.0,” IEEE Internet of Things\nJournal, 2020.\n[29] C. Zhong, M. C. Gursoy, and S. Velipasalar, “Deep multi-agent\nreinforcement learning based cooperative edge caching in wireless\nnetworks,” in ICC 2019-2019 IEEE International Conference on Com-\nmunications (ICC), pp. 1–6, IEEE, 2019.\n[30] Y . Al-Eryani, M. Akrout, and E. Hossain, “Multiple access in cell-\nfree networks: Outage performance, dynamic clustering, and deep\nreinforcement learning-based design,” IEEE Journal on Selected Areas\nin Communications, 2020.\n[31] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y . Mansour, “Policy\ngradient methods for reinforcement learning with function approxima-\ntion,” in Advances in neural information processing systems, pp. 1057–\n1063, 2000.\n[32] R. J. Williams, “Simple statistical gradient-following algorithms for\nconnectionist reinforcement learning,” Machine learning, vol. 8, no. 3-\n4, pp. 229–256, 1992.\n[33] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel, “High-\ndimensional continuous control using generalized advantage estima-\ntion,” arXiv preprint arXiv:1506.02438 , 2015.\n[34] V . Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley,\nD. Silver, and K. Kavukcuoglu, “Asynchronous methods for deep rein-\nforcement learning,” in International conference on machine learning ,\npp. 1928–1937, 2016.\n[35] J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust\nregion policy optimization,” in International conference on machine\nlearning, pp. 1889–1897, 2015.\n[36] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Prox-\nimal policy optimization algorithms,”arXiv preprint arXiv:1707.06347,\n2017.\n[37] A. Valadarsky, M. Schapira, D. Shahaf, and A. Tamar, “A machine\nlearning approach to routing,” arXiv preprint arXiv:1708.03074, 2017.\n[38] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning , vol. 8,\nno. 3-4, pp. 279–292, 1992.\n[39] V . Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou,\nD. Wierstra, and M. Riedmiller, “Playing atari with deep reinforcement\nlearning,” arXiv preprint arXiv:1312.5602 , 2013.\n[40] T. Schaul, J. Quan, I. Antonoglou, and D. Silver, “Prioritized experience\nreplay,” arXiv preprint arXiv:1511.05952 , 2015.\n[41] Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas,\n“Dueling network architectures for deep reinforcement learning,” in\nInternational conference on machine learning , pp. 1995–2003, 2016.\n[42] S. Wang, H. Liu, P. H. Gomes, and B. Krishnamachari, “Deep\nreinforcement learning for dynamic multichannel access in wireless\nnetworks,” IEEE Transactions on Cognitive Communications and Net-\nworking, vol. 4, no. 2, pp. 257–265, 2018.\n[43] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Ried-\nmiller, “Deterministic policy gradient algorithms,” in Proceedings of\nthe 31st International Conference on International Conference on\nMachine Learning, pp. 387–395, 2014.\n[44] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y . Tassa,\nD. Silver, and D. Wierstra, “Continuous control with deep reinforce-\nment learning,” arXiv preprint arXiv:1509.02971 , 2015.\n[45] S. Fujimoto, H. Van Hoof, and D. Meger, “Addressing func-\ntion approximation error in actor-critic methods,” arXiv preprint\narXiv:1802.09477, 2018.\n[46] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-\npolicy maximum entropy deep reinforcement learning with a stochastic\nactor,” arXiv preprint arXiv:1801.01290 , 2018.\n[47] C. Qiu, Y . Hu, Y . Chen, and B. Zeng, “Deep deterministic policy\ngradient (ddpg)-based energy harvesting wireless communications,”\nIEEE Internet of Things Journal , vol. 6, no. 5, pp. 8577–8588, 2019.\n[48] H. Van Hasselt, Y . Doron, F. Strub, M. Hessel, N. Sonnerat, and\nJ. Modayil, “Deep reinforcement learning and the deadly triad,” arXiv\npreprint arXiv:1812.02648, 2018.\n[49] L. Baird, “Residual algorithms: Reinforcement learning with function\napproximation,” in Machine Learning Proceedings 1995 , pp. 30–37,\nElsevier, 1995.\n[50] B. Dai, N. He, Y . Pan, B. Boots, and L. Song, “Learning from\nconditional distributions via dual embeddings,” in Artiﬁcial Intelligence\nand Statistics, pp. 1458–1467, 2017.\n[51] B. Dai, A. Shaw, L. Li, L. Xiao, N. He, Z. Liu, J. Chen, and L. Song,\n“SBEED: Convergent reinforcement learning with nonlinear function\napproximation,” in International Conference on Machine Learning ,\npp. 1125–1134, PMLR, 2018.\n[52] K. Zhang, A. Koppel, H. Zhu, and T. Bas ¸ar, “Global convergence of\npolicy gradient methods to (almost) locally optimal policies,” arXiv\npreprint arXiv:1906.08383, 2019.\n[53] M. Papini, D. Binaghi, G. Canonaco, M. Pirotta, and M. Restelli,\n“Stochastic variance-reduced policy gradient,” arXiv preprint\narXiv:1806.05618, 2018.\n[54] Z. Shen, A. Ribeiro, H. Hassani, H. Qian, and C. Mi, “Hessian aided\npolicy gradient,” in International Conference on Machine Learning ,\npp. 5729–5738, 2019.\n[55] P. Xu, F. Gao, and Q. Gu, “An improved convergence analysis of\nstochastic variance-reduced policy gradient,” inUncertainty in Artiﬁcial\nIntelligence, pp. 541–551, PMLR, 2020.\n[56] B. Liu, Q. Cai, Z. Yang, and Z. Wang, “Neural proximal/trust region\npolicy optimization attains globally optimal policy,” arXiv preprint\narXiv:1906.10306, 2019.\n[57] L. Wang, Q. Cai, Z. Yang, and Z. Wang, “Neural policy gradient\nmethods: Global optimality and rates of convergence,” arXiv preprint\narXiv:1909.01150, 2019.\n[58] T. Xu, Z. Wang, and Y . Liang, “Improving sample complexity bounds\nfor actor-critic algorithms,” arXiv preprint arXiv:2004.12956 , 2020.\n[59] Y . Wu, W. Zhang, P. Xu, and Q. Gu, “A ﬁnite time analysis of two time-\nscale actor critic methods,” arXiv preprint arXiv:2005.01350 , 2020.\n[60] T. Xu, Z. Wang, and Y . Liang, “Non-asymptotic convergence analysis\nof two time-scale (natural) actor-critic algorithms,” arXiv preprint\narXiv:2005.03557, 2020.\n[61] M. Yu, Z. Yang, M. Kolar, and Z. Wang, “Convergent policy op-\ntimization for safe reinforcement learning,” in Advances in Neural\nInformation Processing Systems , pp. 3127–3139, 2019.\n[62] E. Puiutta and E. Veith, “Explainable reinforcement learning: A sur-\nvey,” arXiv preprint arXiv:2005.06247 , 2020.\n[63] A. Heuillet, F. Couthouis, and N. D. Rodr ´ıguez, “Explainability in deep\nreinforcement learning,” arXiv preprint arXiv:2008.06693 , 2020.\n[64] H. P. van Hasselt, M. Hessel, and J. Aslanides, “When to use parametric\nmodels in reinforcement learning?,” in Advances in Neural Information\nProcessing Systems, pp. 14322–14333, 2019.\n24\n[65] A. Zhang, S. Sukhbaatar, A. Lerer, A. Szlam, and R. Fergus, “Compos-\nable planning with attributes,” in International Conference on Machine\nLearning, pp. 5842–5851, 2018.\n[66] M. Deisenroth and C. E. Rasmussen, “PILCO: A model-based and\ndata-efﬁcient approach to policy search,” in Proceedings of the 28th\nInternational Conference on machine learning (ICML-11) , pp. 465–\n472, 2011.\n[67] R. S. Sutton, “Integrated architectures for learning, planning, and\nreacting based on approximating dynamic programming,” in Machine\nlearning proceedings 1990 , pp. 216–224, Elsevier, 1990.\n[68] A. W. Moore and C. G. Atkeson, “Prioritized sweeping: Reinforcement\nlearning with less data and less time,” Machine learning, vol. 13, no. 1,\npp. 103–130, 1993.\n[69] E. Langlois, S. Zhang, G. Zhang, P. Abbeel, and J. Ba,\n“Benchmarking model-based reinforcement learning,” arXiv preprint\narXiv:1907.02057, 2019.\n[70] I. Mordatch and J. Hamrick, “Tutorial on model-based methods in\nreinforcement learning,” ICML, 2020.\n[71] M. Janner, J. Fu, M. Zhang, and S. Levine, “When to trust your model:\nModel-based policy optimization,” in Advances in Neural Information\nProcessing Systems, pp. 12519–12530, 2019.\n[72] Y . Gal, R. McAllister, and C. E. Rasmussen, “Improving PILCO with\nbayesian neural network dynamics models,” in Data-Efﬁcient Machine\nLearning workshop, ICML , vol. 4, p. 34, 2016.\n[73] K. Chua, R. Calandra, R. McAllister, and S. Levine, “Deep rein-\nforcement learning in a handful of trials using probabilistic dynamics\nmodels,” in Advances in Neural Information Processing Systems ,\npp. 4754–4765, 2018.\n[74] A. S. Polydoros and L. Nalpantidis, “Survey of model-based rein-\nforcement learning: Applications on robotics,” Journal of Intelligent\n& Robotic Systems , vol. 86, no. 2, pp. 153–173, 2017.\n[75] E. Bargiacchi, T. Verstraeten, D. M. Roijers, and A. Now ´e, “Model-\nbased multi-agent reinforcement learning with cooperative prioritized\nsweeping,” arXiv preprint arXiv:2001.07527 , 2020.\n[76] O. Krupnik, I. Mordatch, and A. Tamar, “Multi-agent reinforcement\nlearning with multi-step generative models,” in Conference on Robot\nLearning, pp. 776–790, 2020.\n[77] X. Chen, Y . Duan, R. Houthooft, J. Schulman, I. Sutskever, and\nP. Abbeel, “Infogan: Interpretable representation learning by informa-\ntion maximizing generative adversarial nets,” in Advances in neural\ninformation processing systems , pp. 2172–2180, 2016.\n[78] P. Hernandez-Leal, M. Kaisers, T. Baarslag, and E. M. de Cote, “A\nsurvey of learning in multiagent environments: Dealing with non-\nstationarity,” arXiv preprint arXiv:1707.09183 , 2017.\n[79] X. Pan, W. Wang, X. Zhang, B. Li, J. Yi, and D. Song, “How you\nact tells a lot: Privacy-leakage attack on deep reinforcement learning,”\narXiv preprint arXiv:1904.11082 , 2019.\n[80] M. Tan, “Multi-agent reinforcement learning: Independent vs. cooper-\native agents,” in Proceedings of the tenth international conference on\nmachine learning, pp. 330–337, 1993.\n[81] P. Hernandez-Leal, B. Kartal, and M. E. Taylor, “A survey and critique\nof multiagent deep reinforcement learning,” Autonomous Agents and\nMulti-Agent Systems, vol. 33, no. 6, pp. 750–797, 2019.\n[82] T. T. Nguyen, N. D. Nguyen, and S. Nahavandi, “Deep reinforcement\nlearning for multiagent systems: A review of challenges, solutions, and\napplications,” IEEE transactions on cybernetics , 2020.\n[83] K. Zhang, Z. Yang, and T. Bas ¸ar, “Multi-agent reinforcement learning:\nA selective overview of theories and algorithms,” arXiv preprint\narXiv:1911.10635, 2019.\n[84] J. Jiang and Z. Lu, “Learning attentional communication for multi-\nagent cooperation,” in Advances in neural information processing\nsystems, pp. 7254–7264, 2018.\n[85] H. Mao, Z. Gong, Z. Zhang, Z. Xiao, and Y . Ni, “Learning multi-agent\ncommunication under limited-bandwidth restriction for internet packet\nrouting,” arXiv preprint arXiv:1903.05561 , 2019.\n[86] R. Wang, X. He, R. Yu, W. Qiu, B. An, and Z. Rabinovich, “Learn-\ning efﬁcient multi-agent communication: An information bottleneck\napproach,” arXiv preprint arXiv:1911.06992 , 2019.\n[87] R. Lowe, J. Foerster, Y .-L. Boureau, J. Pineau, and Y . Dauphin, “On\nthe pitfalls of measuring emergent communication,” arXiv preprint\narXiv:1903.05168, 2019.\n[88] R. Lowe, Y . I. Wu, A. Tamar, J. Harb, O. P. Abbeel, and I. Mor-\ndatch, “Multi-agent actor-critic for mixed cooperative-competitive en-\nvironments,” in Advances in neural information processing systems ,\npp. 6379–6390, 2017.\n[89] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V . F. Zambaldi,\nM. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, et al. ,\n“Value-decomposition networks for cooperative multi-agent learning\nbased on team reward.,” in AAMAS, pp. 2085–2087, 2018.\n[90] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster, and\nS. Whiteson, “Qmix: Monotonic value function factorisation for deep\nmulti-agent reinforcement learning,” arXiv preprint arXiv:1803.11485,\n2018.\n[91] J. Castellini, F. A. Oliehoek, R. Savani, and S. Whiteson, “The\nrepresentational capacity of action-value networks for multi-agent\nreinforcement learning,” arXiv preprint arXiv:1902.07497 , 2019.\n[92] A. Mahajan, T. Rashid, M. Samvelyan, and S. Whiteson, “Maven:\nMulti-agent variational exploration,” inAdvances in Neural Information\nProcessing Systems, pp. 7613–7624, 2019.\n[93] J. K. Gupta, M. Egorov, and M. Kochenderfer, “Cooperative multi-\nagent control using deep reinforcement learning,” in International\nConference on Autonomous Agents and Multiagent Systems , pp. 66–\n83, Springer, 2017.\n[94] K. Zhang, Z. Yang, and T. Basar, “Networked multi-agent reinforce-\nment learning in continuous spaces,” in 2018 IEEE Conference on\nDecision and Control (CDC) , pp. 2771–2776, IEEE, 2018.\n[95] K. Zhang, Z. Yang, and T. Bas ¸ar, “Decentralized multi-agent reinforce-\nment learning with networked agents: Recent advances,” arXiv preprint\narXiv:1912.03821, 2019.\n[96] T. K. Rodrigues, K. Suto, H. Nishiyama, J. Liu, and N. Kato, “Machine\nlearning meets computation and communication control in evolving\nedge and cloud: Challenges and future perspective,” IEEE Communi-\ncations Surveys & Tutorials , vol. 22, no. 1, pp. 38–67, 2019.\n[97] A. Shakarami, M. Ghobaei-Arani, and A. Shahidinejad, “A survey on\nthe computation ofﬂoading approaches in mobile edge computing: A\nmachine learning-based perspective,” Computer Networks , p. 107496,\n2020.\n[98] M. Sheraz, M. Ahmed, X. Hou, Y . Li, D. Jin, and Z. Han, “Arti-\nﬁcial intelligence for wireless caching: Schemes, performance, and\nchallenges,” IEEE Communications Surveys & Tutorials , 2020.\n[99] M. McClellan, C. Cervell ´o-Pastor, and S. Sallent, “Deep learning at\nthe mobile edge: Opportunities for 5g networks,” Applied Sciences ,\nvol. 10, no. 14, p. 4735, 2020.\n[100] Z. Chen and X. Wang, “Decentralized computation ofﬂoading for\nmulti-user mobile edge computing: A deep reinforcement learning\napproach,” arXiv preprint arXiv:1812.07394 , 2018.\n[101] X. Liu, J. Yu, Z. Feng, and Y . Gao, “Multi-agent reinforcement learning\nfor resource allocation in IoT networks with edge computing,” China\nCommunications, vol. 17, no. 9, pp. 220–236, 2020.\n[102] J. Heydari, V . Ganapathy, and M. Shah, “Dynamic task ofﬂoading in\nmulti-agent mobile edge computing networks,” in 2019 IEEE Global\nCommunications Conference (GLOBECOM) , pp. 1–6, IEEE, 2019.\n[103] N. Naderializadeh and M. Hashemi, “Energy-aware multi-server mo-\nbile edge computing: A deep reinforcement learning approach,” in\n2019 53rd Asilomar Conference on Signals, Systems, and Computers ,\npp. 383–387, IEEE, 2019.\n[104] Y . Zhang, B. Feng, W. Quan, A. Tian, K. Sood, Y . Lin, and H. Zhang,\n“Cooperative edge caching: A multi-agent deep learning based ap-\nproach,” IEEE Access, vol. 8, pp. 133212–133224, 2020.\n[105] B. Yang and M. Liu, “Keeping in touch with collaborative UA Vs: A\ndeep reinforcement learning approach.,” in IJCAI, pp. 562–568, 2018.\n[106] A. Shamsoshoara, M. Khaledi, F. Afghah, A. Razi, and J. Ashdown,\n“Distributed cooperative spectrum sharing in UA V networks using\nmulti-agent reinforcement learning,” in 2019 16th IEEE Annual Con-\nsumer Communications & Networking Conference (CCNC) , pp. 1–6,\nIEEE, 2019.\n[107] H. Qie, D. Shi, T. Shen, X. Xu, Y . Li, and L. Wang, “Joint optimization\nof multi-UA V target assignment and path planning based on multi-agent\nreinforcement learning,” IEEE Access , vol. 7, pp. 146264–146272,\n2019.\n[108] J. Cui, Y . Liu, and A. Nallanathan, “The application of multi-agent\nreinforcement learning in UA V networks,” in2019 IEEE International\nConference on Communications Workshops (ICC Workshops), pp. 1–6,\nIEEE, 2019.\n[109] J. To ˇziˇcka, B. Szulyovszky, G. de Chambrier, V . Sarwal, U. Wani,\nand M. Gribulis, “Application of deep reinforcement learning to UA V\nﬂeet control,” in Proceedings of SAI Intelligent Systems Conference ,\npp. 1169–1177, Springer, 2018.\n[110] J. Ge, Y .-C. Liang, J. Joung, and S. Sun, “Deep reinforcement learning\nfor distributed dynamic miso downlink-beamforming coordination,”\nIEEE Transactions on Communications , 2020.\n25\n[111] K. Zia, N. Javed, M. N. Sial, S. Ahmed, A. A. Pirzada, and F. Pervez,\n“A distributed multi-agent RL-based autonomous spectrum allocation\nscheme in D2D enabled multi-tier hetnets,” IEEE Access , vol. 7,\npp. 6733–6745, 2019.\n[112] Z. Li and C. Guo, “Multi-agent deep reinforcement learning based\nspectrum allocation for D2D underlay communications,” IEEE Trans-\nactions on Vehicular Technology, vol. 69, no. 2, pp. 1828–1840, 2019.\n[113] H. Ye, G. Y . Li, and B.-H. F. Juang, “Deep reinforcement learning based\nresource allocation for V2V communications,” IEEE Transactions on\nVehicular Technology, vol. 68, no. 4, pp. 3163–3173, 2019.\n[114] R. Barazideh, O. Semiari, S. Niknam, and B. Natarajan, “Rein-\nforcement learning for mitigating intermittent interference in terahertz\ncommunication networks,” arXiv preprint arXiv:2003.04832 , 2020.\n[115] R. Singh and D. Sicker, “Ultra-dense low data rate (UDLD) commu-\nnication in the THz,” arXiv preprint arXiv:2009.10674 , 2020.\n[116] D. A. Temesgene, M. Miozzo, D. Gunduz, and P. Dini, “Distributed\ndeep reinforcement learning for functional split control in energy\nharvesting virtualized small cells,” IEEE Transactions on Sustainable\nComputing, 2020.\n[117] M. Sana, A. De Domenico, W. Yu, Y . Lostanlen, and E. C. Strinati,\n“Multi-agent reinforcement learning for adaptive user association in\ndynamic mmWave networks,” IEEE Transactions on Wireless Commu-\nnications, vol. 19, no. 10, pp. 6520–6534, 2020.\n[118] J. He, K. Yu, and Y . Shi, “Coordinated passive beamforming for\ndistributed intelligent reﬂecting surfaces network,” arXiv preprint\narXiv:2002.05915, 2020.\n[119] H. Yang, Z. Xiong, J. Zhao, D. Niyato, Q. Wu, H. V . Poor, and\nM. Tornatore, “Intelligent reﬂecting surface assisted anti-jamming com-\nmunications: A fast reinforcement learning approach,” arXiv preprint\narXiv:2004.12539, 2020.\n[120] Y . Xiu, J. Zhao, C. Yuen, Z. Zhang, and G. Gui, “Secure beamforming\nfor distributed intelligent reﬂecting surfaces aided mmwave systems,”\narXiv preprint arXiv:2006.14851 , 2020.\n[121] B. Wang and N. Hegde, “Privacy-preserving Q-learning with functional\nnoise in continuous spaces,” in Advances in Neural Information Pro-\ncessing Systems, pp. 11327–11337, 2019.\n[122] Y .-C. Lin, Z.-W. Hong, Y .-H. Liao, M.-L. Shih, M.-Y . Liu, and M. Sun,\n“Tactics of adversarial attack on deep reinforcement learning agents,”\narXiv preprint arXiv:1703.06748 , 2017.\n[123] A. Gleave, M. Dennis, C. Wild, N. Kant, S. Levine, and S. Russell,\n“Adversarial policies: Attacking deep reinforcement learning,” arXiv\npreprint arXiv:1905.10615, 2019.\n[124] L. Pinto, J. Davidson, R. Sukthankar, and A. Gupta, “Robust adversarial\nreinforcement learning,” arXiv preprint arXiv:1703.02702 , 2017.",
  "topic": "Reinforcement learning",
  "concepts": [
    {
      "name": "Reinforcement learning",
      "score": 0.8882638216018677
    },
    {
      "name": "Computer science",
      "score": 0.7355179190635681
    },
    {
      "name": "Scalability",
      "score": 0.6679777503013611
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5522333979606628
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4877975285053253
    },
    {
      "name": "Wireless network",
      "score": 0.48239007592201233
    },
    {
      "name": "Distributed computing",
      "score": 0.4074234366416931
    },
    {
      "name": "Wireless",
      "score": 0.40549516677856445
    },
    {
      "name": "Telecommunications",
      "score": 0.20619449019432068
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I46247651",
      "name": "University of Manitoba",
      "country": "CA"
    }
  ],
  "cited_by": 12
}