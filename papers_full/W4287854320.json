{
  "title": "KroneckerBERT: Significant Compression of Pre-trained Language Models Through Kronecker Decomposition and Knowledge Distillation",
  "url": "https://openalex.org/W4287854320",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287858665",
      "name": "Marzieh Tahaei",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2981480042",
      "name": "Ella Charlaix",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A4213788918",
      "name": "Vahid Nia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A734473377",
      "name": "Ali Ghodsi",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2258670661",
      "name": "Mehdi Rezagholizadeh",
      "affiliations": [
        "University of Waterloo"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3008374555",
    "https://openalex.org/W3015609966",
    "https://openalex.org/W3170233084",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W1515333551",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W4287901204",
    "https://openalex.org/W4297813615",
    "https://openalex.org/W1724438581",
    "https://openalex.org/W2769137120",
    "https://openalex.org/W4288379066",
    "https://openalex.org/W1530239534",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3098873988",
    "https://openalex.org/W3115511229",
    "https://openalex.org/W3099413717",
    "https://openalex.org/W3126074026",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2970418186",
    "https://openalex.org/W2948954216",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W3034199299",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W131533222",
    "https://openalex.org/W3006631416",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W3116594510",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2919207648",
    "https://openalex.org/W2754084392",
    "https://openalex.org/W3034408878",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3101248447",
    "https://openalex.org/W2923014074"
  ],
  "abstract": "Marzieh Tahaei, Ella Charlaix, Vahid Nia, Ali Ghodsi, Mehdi Rezagholizadeh. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2116 - 2127\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nKroneckerBERT: Significant Compression of Pre-trained Language Models\nThrough Kronecker Decomposition and Knowledge Distillation\nMarzieh S. Tahaei\nHuawei Noah’s Ark Lab\nmarzieh.tahaei@huawei.com\nElla Charlaix\nHuawei Noah’s Ark Lab\ncharlaixe@gmail.com\nVahid Partovi Nia\nHuawei Noah’s Ark Lab\nvahid.partovinia@huawei.com\nAli Ghodsi\nDepartment of Statistics\nActuarial Science, University of Waterloo\nali.ghodsi@uwaterloo.com\nMehdi Rezagholizadeh\nHuawei Noah’s Ark Lab\nMehdi.rezagholizadeh@huawei.com\nAbstract\nThe development of over-parameterized pre-\ntrained language models has made a significant\ncontribution toward the success of natural lan-\nguage processing. While over-parameterization\nof these models is the key to their generaliza-\ntion power, it makes them unsuitable for de-\nployment on low-capacity devices. We push\nthe limits of state-of-the-art Transformer-based\npre-trained language model compression us-\ning Kronecker decomposition. We present our\nKroneckerBERT, a compressed version of the\nBERTBASE model obtained by compressing the\nembedding layer and the linear mappings in the\nmulti-head attention, and the feed-forward net-\nwork modules in the Transformer layers. Our\nKroneckerBERT is trained via a very efficient\ntwo-stage knowledge distillation scheme us-\ning far fewer data samples than state-of-the-art\nmodels like MobileBERT and TinyBERT. We\nevaluate the performance of KroneckerBERT\non well-known NLP benchmarks. We show\nthat our KroneckerBERT with compression fac-\ntors of 7.7×and 21×outperforms state-of-the-\nart compression methods on the GLUE and\nSQuAD benchmarks. In particular, using only\n13% of the teacher model parameters, it retain\nmore than 99% of the accuracy on the majority\nof GLUE tasks.\n1 Introduction\nIn recent years, the emergence of Pre-trained Lan-\nguage Models(PLMs) has led to a significant break-\nthrough in Natural Language Processing (NLP).\nThe introduction of Transformers and unsupervised\npre-training on enormous unlabeled data are the\ntwo main factors that contribute to this success.\nTransformer-based models (Devlin et al., 2018;\nRadford et al., 2019; Yang et al., 2019; Shoeybi\net al., 2019) are powerful yet highly over-\nparameterized. The enormous size of these models\ndoes not meet the constraints imposed by edge de-\nvices on memory, latency, and energy consumption.\nTherefore there has been a growing interest in de-\nveloping new methodologies and frameworks for\nthe compression of these large PLMs. Similar to\nother deep learning models, the main directions for\nthe compression of these models include low-bit\nquantization (Gong et al., 2014; Prato et al., 2019),\nnetwork pruning (Han et al., 2015), matrix decom-\nposition (Yu et al., 2017; Lioutas et al., 2020) and\nKnowledge distillation (KD) (Hinton et al., 2015).\nThese methods are either used in isolation or in\ncombination to improve compression-performance\ntrade-off.\nRecent works have been relatively successful\nin compressing Transformer-based PLMs to a cer-\ntain degree (Sanh et al., 2019; Sun et al., 2019;\nJiao et al., 2019; Sun et al., 2020; Xu et al., 2020;\nWang et al., 2020; Kim et al., 2021); however, mod-\nerate and extreme compression of these models\n(compression factors >5 and 10 resepctively) is\nstill quite challenging. In particular, several works\n(Mao et al., 2020; Zhao et al., 2019a, 2021) that\nhave tried to go beyond the compression factor of\n10, have done so at the expense of a significant drop\nin performance.\nFollowing the classical assumption that matri-\nces often follow a low-rank structure, low-rank de-\ncomposition methods have been used for compres-\nsion of weight matrices in deep learning models\n(Yu et al., 2017; Swaminathan et al., 2020; Winata\net al., 2019) and especially Transformer-based mod-\nels (Noach and Goldberg, 2020; Mao et al., 2020).\nHowever, low-rank decomposition methods only\nexploit redundancies of the weight matrix in the\nhorizontal and vertical dimensions and thus limit\nthe flexibility of the compressed model. Kronecker\ndecomposition on the other hand exploits redun-\n2116\nFigure 1: An example of Kronecker product of two 2 by\n2 matrices.\ndancies in predefined patches and hence allows\nfor more flexibility in their representation. Recent\nworks prove Kronecker product to be more effec-\ntive in retaining accuracy after compression than\nSVD (Thakker et al., 2019).\nThis work proposes a novel framework that\nuses Kronecker decomposition for compression\nof Transformer-based PLMs and provides a very\npromising compression-performance trade-off for\nmedium and high compression levels, with 13%\nand 5% of the original model parameters respec-\ntively. We use Kronecker decomposition for the\ncompression of both Transformer layers and the\nembedding layer. For Transformer layers, the com-\npression is achieved by representing every weight\nmatrix both in the multi-head attention (MHA) and\nthe feed-forward neural network (FFN) as a Kro-\nnecker product of two smaller matrices. We also\npropose a Kronecker decomposition for compres-\nsion of the embedding layer. Previous works have\ntried different techniques to reduce the enormous\nmemory consumption of this layer (Khrulkov et al.,\n2019; Li et al., 2018). Our Kronecker decomposi-\ntion method can substantially reduce the amount of\nrequired memory while maintaining low computa-\ntion.\nUsing Kronecker decomposition for large com-\npression factors leads to a reduction in the model\nexpressiveness. This is due to the nature of the\nKronecker product and the fact that elements in\nthis representation are tied together. To address\nthis issue, we propose to distill knowledge from the\nintermediate layers of the original uncompressed\nnetwork to the Kronecker network during training.\nTraining of the state-of-the art BERT compres-\nsion models (Zhao et al., 2019a,b; Sun et al., 2020,\n2019) involve an extensive training which requires\nvast computational resources. For example in (Sun\net al., 2020), first a specially designed teacher, i.e\nIB-BERTLARGE is trained from scratch on the en-\ntire English wikipedia and Book Corpus. The stu-\ndent is then pretrained on the same corpus via KD\nwhile undergoing an additional progressive KD\nphase. Another example is TinyBERT(Jiao et al.,\n2019) which requires pretraining on the entire En-\nglish Wikipedia and also uses extensive data aug-\nmentation (20×) for fine-tuning on the downstream\ntasks. We show that our Kronecker BERT can out\nperform state-of-the-art with significantly less train-\ning requirements. More precisely, our Kronecker-\nBERT model undergoes a very light pretraining on\nonly 10% of the English Wikipedia for 3 epochs\nfollowed by finetuning on the original downstream\ndata.\nNote that, while our evaluations in this work\nare limited to BERT, this proposed compression\nmethod can be directly used to compress other\nTransformer-based NLP models. The main con-\ntributions of this paper are as follows:\n• Compression of the embedding layer using\nthe Kronecker decomposition with very low\ncomputational overhead.\n• Deploying the Kronecker decomposition for\nthe compression of Transformer modules.\n• Efficient training the compressed model via\nan intermediate-layer KD that uses only 10%\nof English Wikipedia in the pretraining stage.\n• Evaluating the proposed framework for com-\npression of BERTBASE model on well-known\nNLP benchmarks\n2 Related Work\nIn this section, we first go through some of the\nmost related works for BERT compression in the\nliterature and then review the few works that have\nused Kronecker decomposition for compression of\nCNNs and RNNs.\n2.1 Pre-trained Language Model\nCompression\nIn recent years, many model compression methods\nhave been proposed to reduce the size of PLMs\nwhile maintaining their performance on different\ntasks. KD, which was first introduced by (Buciluˇa\net al., 2006) and then later generalized by (Hin-\nton et al., 2015), is a popular compression method\nwhere a small student network is trained to mimic\nthe behavior of a larger teacher network. Recently,\nusing KD for the compression of PLMs has gained\na growing interest in the NLP community. BERT-\nPKD (Sun et al., 2019), uses KD to transfer knowl-\n2117\nedge from the teacher’s intermediate layers to the\nstudent in the fine-tuning stage. TinyBERT (Jiao\net al., 2019) uses a two-step distillation method\napplied both at the pre-training and at the fine-\ntuning stage. MobileBERT (Sun et al., 2020) also\nuses an intermediate-layer knowledge distillation\nmethodology, but the teacher and the student are de-\nsigned by incorporating inverted-bottleneck struc-\nture. Authors in (Zhao et al., 2019a) use a mixed-\nvocabulary training method to train models with\na smaller vocabulary. They combine this method\nwith intermediate layer KD through shared projec-\ntion matrices. In (Mao et al., 2020), the authors\npresent LadaBERT, a lightweight model compres-\nsion pipeline combining SVD-based matrix factor-\nization with weight pruning while using KD for\ntraining to achieve a high compression factor.\n2.2 Kronecker Decomposition\nKronecker products have previously been utilized\nfor the compression of CNNs and small RNNs.\nZhou and Wu 2015 was the first work that uti-\nlized Kronecker decomposition for NN compres-\nsion. They used a summation of multiple Kro-\nnecker products to replace weight matrices in the\nfully connected and convolution layers in simple\nCNN architectures like AlexNet. Thakker et al.,\n2020 used Kronecker product for the compression\nof very small language models for deployment on\nIoT devices. To reduce the amount of performance\ndrop after compression, they propose a hybrid ap-\nproach where the weight matrix is decomposed\ninto an upper part and lower part. The upper part\nremains un-factorized, and only the lower part is\nfactorized using the Kronecker product. More re-\ncently, Thakker et al. 2020 tried to extend the pre-\nvious work to non-IoT applications. Inspired by\nrobust PCA, they add a sparse matrix to Kronecker\nproduct factorization and propose an algorithm for\nlearning these two matrices together.\nTo the best of our knowledge, this work is the\nfirst attempt to compress Transformer-based lan-\nguage models using Kronecker decomposition. Un-\nlike prior arts, we use a simple Kronecker product\nof two matrices for the representation of linear lay-\ners and uses KD framework to improve the perfor-\nmance.\n3 Methodology\nIn this section, we first introduce the background\nof Kronecker decomposition and then explain our\nLook-up table Look-up table\nsee see\n=\nConventional Proposed  \n(Kronecker Embedding)\nFigure 2: Illustration of our proposed method for the\ncompression of the embedding layer. Left: conven-\ntional embedding stored in a lookup table. Right: Our\nproposed compression method where the original em-\nbedding matrix is represented as a Kronecker product\nof a matrix and a row vector. The matrix is stored in a\nlookup table to minimize computation overhead.\ncompression method in detail.\n3.1 Kronecker Product\nKronecker product is an operation that is applied\non two matrices resulting in a block matrix. Let\nA be a matrix ∈I Rm1×n1 , and let B be a matrix\n∈I Rm2×n2 , then the Kronecker product of A and\nB denoted by ⊗is a block matrix, where each block\n(i,j) is obtained by multiplying the element Ai,j\nby matrix B. Therefore, the resulting matrix A⊗B\nis ∈I Rm×n where m = m1m2 and n = n1n2.\nFigure 1 illustrates the Kronecker product between\ntwo small matrices. See (Graham, 2018) for more\ndetailed information on Kronecker products. Re-\nplacing matrix product with Kronecker product re-\nplaces the projection of the original linear space\nby a more constrained linear space in in which the\nprojection angle is defined by the core tensors, see\nFigure 5 in the appendix.\n3.2 Kronecker Decomposition\nGiven a shape for A and B, i.e. (m1,n1,m2,n2),\nany matrix W ∈ I Rm×n, can be approximated\nas a summation of Kronecker product of matrices\nAr ∈I Rm1×n1 and Br ∈I Rm2×n2 :\nW≈\nI∑\ni=1\nAi ⊗Bi (1)\nwe can obtain exact representation ofW by setting\nthe number of Kronecker summations I equal to\nmin(m1n1,m2n2). However, in order to achieve\n2118\ncompression, a much smaller value of I is often\nused. In fact prior arts show promising results\nusing a single Kronecker product (Thakker et al.,\n2019, 2020). When decomposing a matrix W ∈\nI Rm×n, as A⊗B, there are different choices for the\nshapes of A and B. The dimensions of A i.e m1\nand n1 can be any factor of mand nrespectively,\nthe dimensions of B will subsequently be equal to\nm2 = m/m1 and n2 = n/n1.\n3.2.1 The Nearest Kronecker Product\nThe nearest Kronecker problem is defined as find-\ning matrices A and B that their Kronecker product\nbest approximate a given W (for a given shape of\nA and B):\nmin\nA,B\n∥W −A ⊗B∥F . (2)\n(Van Loan and Pitsianis, 1993) show that this\nproblem can be solved using rank-1 SVD approxi-\nmation of rearranged W:\nmin\nA,B\nRn1,m1 (W) −V(A)V(B)⊤\n\nF\n. (3)\nHere, Vis an operation that transforms a matrix to\na vector (vectorizes) by stacking its columns and\nRm2,n2 is a rearrangement operation that extracts\npatches of size m2 ×n2, vectorizes the resulting\npatches and finally concatenates them together to\nform a matrix of size m2n2 ×m1n1. The rear-\nrangement operation turns the Kronecker product\ninto a matrix of rank one while retaining the Frobe-\nnius norm making the minimizations in Eq.3 and\nEq.2 equivalant. Hence, the rank-one SVD solu-\ntion U(:,1)σV(:,1)T can be used to obtain the\noptimum A and B as:\nA = V−1\nm1,n1\n(√σU(:,1)\n)\n(4)\nB = V−1\nm2,n2\n(√σV(:,1)\n)\n(5)\nHere, V−1\nm1,n1 (x) is an operation that transforms a\nvector x to a matrix of size m1 ×n1 by dividing\nthe vector to columns of sizem1 and concatenating\nthe resulting columns together. Similarly, rank- r\nSVD decomposition can be used to approximate\nsummation of Kronecker products. We use this\nmethod for the initialization of Kronecker layers\nfrom the non-compressed model.\n3.2.2 Relation to SVD\nBy choosing n1 = 1and m2 = 1, A becomes a\ncolumn vector of size ∈I Rm×1 and B becomes a\nrow vector of size I R1×n, then the Kronecker de-\ncomposition becomes equivalent to rank-1 SVD\ndecomposition. Therefore rank-1 SVD is a spe-\ncial case of Kronecker product decomposition and\nrank-rSVD is a special case of Kronecker product\nsummation decomposition. This indicates that with\nKronecker product one can achieve more flexibility\nthan low rank decomposition.\n3.2.3 Memory and Computation Reduction\nWhen representing W as A ⊗B, the number of\nelements is reduced from mn to m1n1 + m2n2.\nMoreover, using the Kronecker product to repre-\nsent linear layers can reduce the required compu-\ntation. In fact, a linear projection of any vector x\ncan be performed efficiently without explicit recon-\nstruction of A ⊗B using the following popular\nproperty of Kronecker product:\n(A ⊗B)X = V(BV−1\nn2,n1 (X)A⊤) (6)\nwhere A⊤is A transpose. The consequence of per-\nforming multiplication in this way is that it reduces\nthe number of FLOPs from (2m1m2 −1)n1n2 to:\nmin\n(\n(2n2 −1)m2n1 + (2n1 −1)m2m1,\n(2n1 −1)n2m1 + (2n2 −1)m2m1\n)\n(7)\n3.3 Kronecker Embedding Layer\nThe embedding layer in large language models is a\nvery large lookup table X ∈I Rv×d, where vis the\nsize of the dictionary and dis the embedding di-\nmension. In order to compress X using Kronecker\ndecomposition, the first step is to define the shape\nof Kronecker factors AE and BE. We define AE\nto be a matrix of size v×d\nn and BE to be a row\nvector of size n. There are two reasons for defin-\ning BE as a row vector. 1) it allows disentangled\nembedding of each word since every word has a\nunique row in AE. 2) the embedding of each word\ncan be obtained efficiently inO(d). More precisely,\nthe embedding for the i’th word in the dictionary\ncan be obtained by the Kronecker product between\nAE\ni and BE:\nXi = AE\ni ⊗BE (8)\nwhereAE is stored as a lookup table. Note that\nsince AE\ni is of size1×d\nn and BE is of size1×n, the\ncomputation complexity of this operation is O(d).\nFigure 2 shows an illustration of the Kronecker\nembedding layer.\n2119\nEmbedding\nMulti-head attention\nAdd and Norm\nFeedforward Network\nAdd and Norm\nClassifier\nEmbedding\nMulti-head attention\nAdd and Norm\nFeedforward Network\nAdd and Norm\nClassifier\nFFN output\nAttention matrices\nEmbedding output\n \nTeacher Student \nEnglish\nWikipedia\nTeacher\nStudentStudent\nKroenckerBERT\nTeacher\nKroenckerBERT\nInference on Edge\nTask dataset\n10% sample  \nEnglish Wikipedia\nTwo stage KD training\nFigure 3: Illustration of the proposed framework. Left: A diagram of the teacher BERT model and the student\nKroneckerBERT. Right: The two-stage KD methodology used to train KroneckerBERT.\n3.4 Kronecker Transformer\nThe Transformer layer is composed of two main\ncomponents: MHA and FFN. We use Kronecker de-\ncomposition to compress both. In the Transformer\nblock, the self-attention mechanism is done by pro-\njecting the input into the Key, Query, and Value\nembeddings and obtaining the attention matrices\nthrough the following:\nO = QK⊤\n√dk\n(9)\nAttention(Q,K,V) =softmax(O)V\nwhere Q, K, and V are obtained by multiplying\nthe input by WQ, WK, WV respectively. In a\nMHA module, there is a separate WQl, WKl, and\nWVl matrix per attention head to allow for a richer\nrepresentation of the data. In the implementation\nusually, matrices from all heads are stacked to-\ngether resulting in 3 matrices W′k, W′Q and W′V .\nInstead of decomposing the matrices of each head\nseparately, we use Kronecker decomposition after\nconcatenation:\nW′K = Ak ⊗BK (10)\nW′Q = AQ ⊗BQ\nW′V = AV ⊗BV\nBy choosing m2 to be smaller than the output di-\nmension of each attention head, matrix B in the\nKronecker decomposition is shared among all at-\ntention heads resulting in more compression. The\nresult of applying Eq.9 is then fed to a linear map-\nping (WO) to produce the MHA output. We use\nKronecker decomposition for compressing this lin-\near mapping as well the two weight matrices in the\nsubsequent FFN block:\nWO = AO ⊗BO (11)\nW1 = A1 ⊗B1 (12)\nW2 = A2 ⊗B2 (13)\n3.5 Knowledge Distillation\nIn the following section, we describe how KD is\nused to improve the training of the KroneckerBERT\nmodel.\n3.5.1 Intermediate KD\nLet Sbe the student, and T be the teacher, then\nfor a batch of data (X,y), we define fS\nl (X)\nandfT\nl (X) as the output of the lth layer for the\nstudent network and the teacher network respec-\ntively. The teacher here is the BERTBASE and the\nstudent is its corresponding KroneckerBERT that is\nobtained by replacing the embedding layer and the\nlinear mappings in MHA and FFN modules with\nKronecker factors(see Sections 3.3 and 3.4 for de-\ntails). Note that like other decomposition methods,\nwhen we use Kronecker factorization to compress\nthe model, the number of layers and the dimen-\nsions of the input and output of each layer remain\nintact. Therefore, when performing intermediate\nlayer KD, we can directly obtain the difference in\nthe output of a specific layer in the teacher and\nstudent networks without the need for projection.\nIn the proposed framework, the intermediate KD\nfrom the teacher to student occurs at the embedding\n2120\nModel Compression\nFactor\nFLOPS WK, WQ,\nWV , WO\nW1, W2\nT WE Number of sums\nn, m d\nBERT BASE 1× 21.7B 768, 768 768, 3072 768 1\nn1 , m1 n\nKroneckerBERT 8 7.8 × 5.2B 384, 384 2, 8 8 1\nKroneckerBERT 21 21× 1.4B 48, 384 2, 16 16 1\nKroneckerBERT 5 5.5 × 9.4B 384, 384 384, 384 12 4\nTable 1: Configuration of the Kronecker layers for the three KroneckerBERT models used in this paper.nand m\nare the input and output dimensions of the weight matrices (W ∈I Rm×n). m1,n1 indicates the shape of the first\nKronecker factor (A ∈I Rm1×n1 ). For embedding layer we only need to set the size of the row vector BE ∈I R1×n.\nlayer output, attention matrices and FFN outputs:\nLEmbedding(X) = MSE\n(\nES,ET)\nLAttention(X) =\n∑\nl\nMSE\n(\nOS\nl ,OT\nl\n)\nLFFN(X) =\n∑\nl\nMSE\n(\nHS\nl ,HT\nl\n)\nwhere ESand ET are the output of the embedding\nlayer from the student and the teacher respectively.\nOS\nl and OT\nl are the attention matrices (Eq.9), HS\nl\nand HT\nl are the outputs of the FFN, of layer lin\nthe student and the teacher respectively.\nOur final loss is as follows:\nL(x,y) =\n∑\n(x,y)\nLEmbedding(x) + (14)\nLAttention(x) +LFFN(x) +\nLLogit(x) +LStudent(x,y),\nwhere LStudent(x) is the supervised loss of the stu-\ndent, e.g. the cross entropy loss when fine-tuning\nfor sequence classification tasks.\n3.5.2 KD at pre-training\nInspired by prior works we use KD at the pre-\ntraining stage to capture the general domain knowl-\nedge from the teacher. For the pre-training distil-\nlation, the pretrained BERTBASE model is used as\nthe teacher. Intermediate layer KD is then used to\ntrain the KroneckeBERT network in the general do-\nmain. KD at pre-training improves the initialization\nof the Kronecker model for the task-specific KD\nstage. The loss at the pre-training stage involves\nthe intermediate KD loss as in Eq. 14 as well as\nthe masked language modeling and next sentence\nprediction. Unlike other methods, we perform pre-\ntraining distillation only on a small portion of the\ndataset (10% of the English Wikipedia) for a few\nepochs (3 epochs) which makes our training far\nmore efficient. See Table 10 in the Appendix for\na comparison of training requirements by various\nmethods.\n3.6 Model Settings\nThe first step of the proposed framework is to de-\nsign the Kronecker layers by defining the shape of\nA and B. Once the shape of one of them is set,\nthe shape of the other one can be obtained accord-\ningly. Therefore we only searched among different\nchoices for m1 and n1 which are limited to the\nfactors of the original weight matrix (mand nre-\nspectively). We used the same configuration for\nall the matrices in the MHA. Also For the FFN,\nwe chose the configuration for one layer, and for\nthe other layer, the dimensions are swapped. For\nthe embedding layer, since BE is a row vector, we\nonly need to choose n. The shapes of the Kro-\nnecker factors were chosen to obtain the desired\ncompression factor and FLOPS reduction accord-\ning to Eq.7. To investigate the effect of summation\nwe also selected one configuration with summation\nof 4 Kronecker products. Similarly, after fixing the\nnumber of summation we chose the configuration\nthat provided the desired compression and latency\nreduction. Table 1 summarises the configuration of\nKronecker factorization for the three compression\nfactors used in this work.\n3.7 Implementation details\nFor KD at the pre-training stage, the Kronecker-\nBERT model was initialized using the teacher (pre-\ntrained BERTBASE model). This means that for\nlayers that were not compressed like the last layer,\nthe values are copied from the teacher to the student.\nFor initialization of the compressed layers in the\npre-training stage, the nearest Kronecker solution\nexplained in section 3.2.1 is used to approximate\nKronecker factors (A and B) from the pre-trained\nBERTBASE model. In the pre-training stage, 10%\nof the English Wikipedia was used for 3 epochs.\n2121\nModel Params MNLI-(m/mm) SST-2 MRPC CoLA QQP QNLI RTE STS-B Avg\nBERTBASE 109.5M 83.9/83.4 93.4 87.9 52.8 71.1 90.9 67 85.2 79.5\nBERT4-PKD 7.6B 79.9/79.3 89.4 82.6 24.8 70.2 85.1 62.3 79.8 72.6\nMobileBERTTINY 15.1M 81.5/81.6 91.7 87.9 46.7 68.9 89.5 65.1 80.1 77.0\nTinyBERT 14.5M 82.5/81.8 92.6 86.4 44.1 71.3 87.7 66.6 80.4 77.0\nKroneckerBERT8 14.3M 83.0/82.7 91.9 88.5 39.8 71.5 90.2 67.2 84.5 77.7\nTable 2: Results on the test set of GLUE official benchmark. The results for BERT, BERT4-PKD and TinyBERT are\ntaken from (Jiao et al., 2019). For all other baselines, the results are taken from their associated papers. Note that\nour KroneckerBERT only performs pre-training KD on 10% of the Wikipedia. Also MobileBERT distils knowledge\nfrom a specially designed teacher that is trained from scratch and TinyBERT uses an extensive data augmentation in\nthe fine-tuning stage.\nModel Params MNLI-(m/mm) SST-2 MRPC CoLA QQP QNLI RTE STS-B\nBERTBASE 108.5M 83.9/83.4 93.4 87.9 52.8 71.1 90.9 67 85.2\nSharedProject 5.6M 76.4/75.2 84.7 84.9 - - - - -\nLadaBERT4 11M 75.8/76.1 84.0 - - 67.4 75.1 - -\nKroneckerBERT 21 5.2M 81.3/80.1 88.4 87.1 28.3 70.5 86.1 64.7 81.3\nTable 3: Results on the test set of the GLUE official benchmark for extreme compression factors. The results of\nthe baselines are taken from their associated papers. LadaBERT and SharedProject refer to (Mao et al., 2020) and\n(Zhao et al., 2019a) respectively.\nThe batch size in pre-training was set to 64 and\nthe learning rate was set to e-3. After pre-training,\nthe obtained Kronecker model is used to initial-\nize the Kronecker layers in the student model for\ntask-specific fine-tuning. The Prediction layer is\ninitialized from the fine-tuned BERTBASE teacher.\nFor fine-tuning on each task, we optimize the hyper-\nparameters based on the performance of the model\non the dev set. See appendix for more details on\nthe results and the selected hyperparameters.\n4 Experiments\nIn this section, we compare our KroneckerBERT\nwith the sate-of-the-art compression methods ap-\nplied to BERT on GLUE and SQuAD. We also\nperform an ablation study to investigate the effect\nof pretraining and KD.\n4.1 Baselines\nAs for baselines we select two main categories of\ncompression methods, those with compression fac-\ntor <10 and those with compression factor >10.\nIn the first category, we have BERT PKD (Sun\net al., 2019) with a low compression factor, and\nmodels with similar compression factor as our\nKroneckerBERT8: MobileBERT (Sun et al., 2020)\nand TinyBERT (Jiao et al., 2019). We also com-\npare our results to the dynaBERT model (Hou et al.,\n2020). For the second category, we compare our\nresults with SharedProject (Zhao et al., 2019a) and\nLadaBERT (Mao et al., 2020) with compression\nfactors in the rage of 10-20x.\n4.2 Results on the GLUE Benchmark\nWe evaluated the proposed framework on the Gen-\neral Language Understanding Evaluation (GLUE)\n(Wang et al., 2018) benchmark which consists\nof 9 natural language understanding tasks. We\nsubmitted the predictions of our proposed mod-\nels on the test data sets for different tasks\nto the official GLUE benchmark ( https://\ngluebenchmark.com/). Table 2 summarizes\nthe results on GLUE test set for compression fac-\ntors less than 10. We can see that KroneckerBERT8\noutperforms other baselines in the majority of tasks\nas well as on average. Moreover, the average per-\nformance of KroneckerBERT 8 excluding CoLA\nis 82.4 which is only 0.5% less than that of the\nteacher.\nTable 3 shows the results for extreme compres-\nsion on the GLUE test set. As indicated in the\ntable, the baselines for the higher compression fac-\ntors only provided results on a limited set of GLUE\ntasks. We can see that for higher compression fac-\ntors, KroneckerBERT21 outperforms the baselines\non all available results.\nIn table 4 we compare the performance of Kro-\nneckerBERT with the dynaBERT model (Hou et al.,\n2020). We compare the results on dev set since the\nresults on test set were not provided in their pa-\nper. We see that KroneckerBERT can outperform\ndynaBERT with fewer number of parameters on all\nGLUE tasks.\n2122\nModel Params MNLI-(m/mm) SST-2 MRPC CoLA QQP QNLI RTE STS-B Avg\nKroneckerBERT5 20M 82.8/83.5 91.2 87.2 49.5 91.1 89.4 68.9 88.1 81.3\nKroneckerBERT8 14.3M 82.8/83.5 91.1 87.5 43.6 90.9 90.7 69.66 88.0 80.9\nDynaBERT 27M 83/83.6 91.6 83.1 48.5 91.0 90.0 67.9 88.2 80.8\nTable 4: Results on the dev set of the GLUE. The results for DynaBERT are taken from (Hou et al., 2020)\nCMP SQuAD1.1 SQuAD2.0\nModel Factor EM F1 EM F1\nBERTBASE 1× 80.5 88 74.5 77.7\nBERT4-PKD 2.1 × 70.1 79.5 60.8 64.6\nTinyBERT 7.5 × 72.7 82.1 68.2 71.8\nKroneckerBERT8 7.8× 78.1 86.3 70.4 73.8\nKroneckerBERT21 21× 70.7 80.5 66.9 69.3\nTable 5: Results of the baselines and KroneckerBERT\non question SQuAD dev dataset. The results of the\nbaselines are taken from (Jiao et al., 2019).\nPre-training Fine-tuning MNLI-m SST-2 MRPC\n(393k) (67k) (3.7k)\nw KD w KD 82.8 91.0 87.5\nNone w KD 80.7 86.6 70.8\nw KD w/o KD 80.0 88.8 86.5\nTable 6: Ablation study of the effect pretraining and KD\nin the fine-tuning stage. The results show the perfor-\nmance of the KroneckerBERT8 on GLUE dev. w and\nw/o denote with and without, respectively.\n4.3 Results on SQuAD\nIn this section, we evaluate the performance of the\nproposed model on SQuAD datasets. SQuAD1.1\n(Rajpurkar et al., 2016) is a large-scale reading\ncomprehension which contains questions that have\nanswers in given context. SQuAD2.0 (Kudo and\nRichardson, 2018) also contains unanswerable\nquestions. Table 5 summarises the performance\non dev set. For both SQuAD1.1 and SQuAD2.0,\nKroneckerBERT8 with fewer number of parame-\nters can significantly outperform both TinyBERT\nand BERT4-PKD baselines. We have also listed\nthe performance of KroneckerBERT21. The results\nof baselines with higher compression factors on\nSQuAD were not available.\n4.4 Ablation Study\nIn this section, we investigate the effect of pre-\ntraining and KD in reducing the gap between the\noriginal BERTBASE model and the compressed Kro-\nneckerBERT. Table 6 summarises the results for\nKroneckerBERT8. Our proposed method uses KD\nin both the pre-training and the fine-tuning stages.\nFor this ablation study, pre-training is only per-\nformed via KD with the pre-trained BERTBASE as\n \n \n \n \n \n \nText\nFigure 4: T-SNE visualization of the output of the\nmiddle Transformer layer of the fine-tuned models\non SST-2 dev. Left: Fine-tuned BERT BASE, mid-\ndle: KroneckerBERT 8 fine-tuned without KD, right:\nKroneckerBERT8 when trained using KD in two stages.\nThe colours indicate the positive and negative classes.\nthe teacher. We perform experiments on 3 tasks\nfrom the GLUE benchmark with different sizes of\ntraining data, namely MNLI-m, SST-2, and MRPC.\nFor all tasks, the highest performance is obtained\nwhen the two-stage KD is used (first row). Note\nthat our light pretraining plays an important row\nin improving the performance as shown in the first\nand the second row (with and without pretraining\nrespectively). As the size of the task dataset de-\ncreases the effect of pretraining becomes more sig-\nnificant. Also, removing KD from the fine-tuning\nstage (task-agnostic compression) leads to an accu-\nracy drop on all task. However, the drop is not as\npronounced as removing the pretraining stage. It\nseems that KD in the fine-tuning stage has a larger\nimpact on tasks with larger datasets.\nWe also used t-SNE to visualize the output of\nthe FFN of the middle layer (layer 6) of the fine-\ntuned KroneckerBERT8 with and without KD in\ncomparison with the fine-tuned teacher, on SST-\n2 dev. Figure 4 shows the results. See how KD\nhelps the features of the middle layer to be more\nseparable with respect to the task compared to the\nno KD case.\n5 Conclusion\nWe introduced a novel method for compressing\nTransformer-based language models that uses Kro-\nnecker decomposition for the compression of the\nembedding layer and the linear mappings within\nthe Transformer blocks. The proposed framework\n2123\nwas used to compress the BERTBASE model. We\nused a very light two-stage KD method to train the\ncompressed model. We show that the proposed\nframework can significantly reduce the size and the\nnumber of computations while outperforming state-\nof-the-art. The proposed method can be directly ap-\nplied for compression of other Transformer-based\nlanguage models. The combination of the proposed\nmethod with other compression techniques such\nlayer truncation, pruning and quantization can be\nan interesting direction for future work.\nAcknowledgements\nAuthors would like to thank Mahdi Zolnouri, Seyed\nAlireza Ghaffari and Eyyüb Sari for informative\ndiscussions throughout this project. We also would\nlike to thank Aref Jaffari for preparing the out of\ndomain datasets.\nReferences\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The fifth pascal recognizing\ntextual entailment challenge. In TAC.\nCristian Bucilu ˇa, Rich Caruana, and Alexandru\nNiculescu-Mizil. 2006. Model compression.\nIn Proceedings of the 12th ACM SIGKDD\ninternational conference on Knowledge discovery\nand data mining, pages 535–541.\nZihan Chen, Hongbo Zhang, Xiaoji Zhang, and Leqi\nZhao. 2018. Quora question pairs. University of\nWaterloo.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nYunchao Gong, Liu Liu, Ming Yang, and Lubomir\nBourdev. 2014. Compressing deep convolutional\nnetworks using vector quantization. arXiv preprint\narXiv:1412.6115.\nAlexander Graham. 2018. Kronecker products and\nmatrix calculus with applications. Courier Dover\nPublications.\nSong Han, Huizi Mao, and William J Dally. 2015. Deep\ncompression: Compressing deep neural networks\nwith pruning, trained quantization and huffman cod-\ning. arXiv preprint arXiv:1510.00149.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam\nDziedzic, Rishabh Krishnan, and Dawn Song. 2020.\nPretrained transformers improve out-of-distribution\nrobustness. arXiv preprint arXiv:2004.06100.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nLu Hou, Lifeng Shang, Xin Jiang, and Qun Liu. 2020.\nDynabert: Dynamic bert with adaptive width and\ndepth. arXiv preprint arXiv:2004.04037.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. 2019.\nTinybert: Distilling bert for natural language under-\nstanding. arXiv preprint arXiv:1909.10351.\nValentin Khrulkov, Oleksii Hrinchuk, Leyla Mir-\nvakhabova, and Ivan Oseledets. 2019. Tensorized\nembedding layers for efficient model compression.\narXiv preprint arXiv:1901.10787.\nSehoon Kim, Amir Gholami, Zhewei Yao, Michael W\nMahoney, and Kurt Keutzer. 2021. I-bert:\nInteger-only bert quantization. arXiv preprint\narXiv:2101.01321.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing.\narXiv preprint arXiv:1808.06226.\nZhongliang Li, Raymond Kulhanek, Shaojun Wang,\nYunxin Zhao, and Shuang Wu. 2018. Slim embed-\nding layers for recurrent neural language models. In\nProceedings of the AAAI Conference on Artificial\nIntelligence, volume 32.\nVasileios Lioutas, Ahmad Rashid, Krtin Kumar, Md Ak-\nmal Haidar, and Mehdi Rezagholizadeh. 2020. Im-\nproving word embedding factorization for compres-\nsion using distilled nonlinear neural decomposition.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: Findings,\npages 2774–2784.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th Annual Meeting of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 142–150, Portland,\nOregon, USA. Association for Computational Lin-\nguistics.\nYihuan Mao, Yujing Wang, Chufan Wu, Chen Zhang,\nYang Wang, Yaming Yang, Quanlu Zhang, Yunhai\nTong, and Jing Bai. 2020. Ladabert: Lightweight\nadaptation of bert through hybrid model compression.\narXiv preprint arXiv:2004.04124.\nMarius Mosbach, Maksym Andriushchenko, and Diet-\nrich Klakow. 2020. On the stability of fine-tuning\nbert: Misconceptions, explanations, and strong base-\nlines. arXiv preprint arXiv:2006.04884.\n2124\nMatan Ben Noach and Yoav Goldberg. 2020. Com-\npressing pre-trained language models by ma-\ntrix decomposition. In Proceedings of the 1st\nConference of the Asia-Pacific Chapter of the\nAssociation for Computational Linguistics and the\n10th International Joint Conference on Natural\nLanguage Processing, pages 884–889.\nGabriele Prato, Ella Charlaix, and Mehdi Reza-\ngholizadeh. 2019. Fully quantized trans-\nformer for machine translation. arXiv preprint\narXiv:1910.10485.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable questions\nfor squad.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ Questions\nfor Machine Comprehension of Text. arXiv e-prints,\npage arXiv:1606.05250.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion\nparameter language models using model parallelism.\narXiv preprint arXiv:1909.08053.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empirical\nmethods in natural language processing, pages 1631–\n1642.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for bert model com-\npression. arXiv preprint arXiv:1908.09355.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. Mobilebert: a\ncompact task-agnostic bert for resource-limited de-\nvices. arXiv preprint arXiv:2004.02984.\nSridhar Swaminathan, Deepak Garg, Rajkumar Kan-\nnan, and Frederic Andres. 2020. Sparse low rank\nfactorization for deep neural network compression.\nNeurocomputing, 398:185–196.\nUrmish Thakker, Jesse Beu, Dibakar Gope, Chu Zhou,\nIgor Fedorov, Ganesh Dasika, and Matthew Mat-\ntina. 2019. Compressing rnns for iot devices by\n15-38x using kronecker products. arXiv preprint\narXiv:1906.02876.\nUrmish Thakker, Paul Whatamough, Matthew Mattina,\nand Jesse Beu. 2020. Compressing language mod-\nels using doped kronecker products. arXiv preprint\narXiv:2001.08896.\nCharles F Van Loan and Nikos Pitsianis. 1993. Approx-\nimation with kronecker products. In Linear algebra\nfor large scale and real-time applications, pages 293–\n314. Springer.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao,\nNan Yang, and Ming Zhou. 2020. Minilm: Deep\nself-attention distillation for task-agnostic compres-\nsion of pre-trained transformers. arXiv preprint\narXiv:2002.10957.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2017. A broad-coverage challenge corpus for\nsentence understanding through inference. arXiv\npreprint arXiv:1704.05426.\nGenta Indra Winata, Andrea Madotto, Jamin Shin,\nElham J Barezi, and Pascale Fung. 2019. On\nthe effectiveness of low-rank matrix factoriza-\ntion for lstm model compression. arXiv preprint\narXiv:1908.09982.\nCanwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei,\nand Ming Zhou. 2020. Bert-of-theseus: Compressing\nbert by progressive module replacing.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237.\nXiyu Yu, Tongliang Liu, Xinchao Wang, and Dacheng\nTao. 2017. On compressing deep models by low rank\nand sparse decomposition. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, pages 7370–7379.\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Wein-\nberger, and Yoav Artzi. 2020. Revisiting few-sample\nbert fine-tuning. arXiv preprint arXiv:2006.05987.\nYuan Zhang, Jason Baldridge, and Luheng He. 2019.\nPaws: Paraphrase adversaries from word scrambling.\narXiv preprint arXiv:1904.01130.\n2125\nSanqiang Zhao, Raghav Gupta, Yang Song, and Denny\nZhou. 2019a. Extreme language model compression\nwith optimal subwords and shared projections. arXiv\npreprint arXiv:1909.11687.\nSanqiang Zhao, Raghav Gupta, Yang Song, and\nDenny Zhou. 2019b. Extremely small bert mod-\nels from mixed-vocabulary training. arXiv preprint\narXiv:1909.11687.\nSanqiang Zhao, Raghav Gupta, Yang Song, and Denny\nZhou. 2021. Extremely small bert models from\nmixed-vocabulary training. In Proceedings of the\n16th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Main\nV olume, pages 2753–2759.\nShuchang Zhou and Jia-Nan Wu. 2015. Compression of\nfully-connected layer in neural network by kronecker\nproduct. arXiv preprint arXiv:1507.05775.\nA Appendix\nA.1 Training Details\nIn this section, we include more details of our exper-\nimental settings presented in Section 4 of the paper.\nFor optimization, we used BERTadam and searched\nlearning rate in range {5e−5,e −4,5e−4,}. For\npretraining the learning rate was set to 1e-13 and\nthe number of epochs was set to 3. The batch size\nin all experiments were set to 32. For the GLUE\nbenchmark, We searched epochs in range {5-15}\nfor all tasks except CoLA. For CoLA we searched\nepochs in range {15-30}. This is because similar\nto other studies (Mosbach et al., 2020; Zhang et al.,\n2020) we noticed that running CoLA for more\nepochs is necessary to reduce its sensitivity to ran-\ndom seed. The sequence length at the pre-training\nstage is set to 512 and at the fine-tuning stage is\nset to 128 for GLUE benchmark. For SQuAD1.1\nand SQuAD2 the sequence length is set 384 and\nthe batch size was set to 64 and the epochs were\nvaried in the range {1-14}. Also, the learning rate\nwas set to e-4 and\nTable 7 shows the result of the best-performing\nmodels on dev set for KroneckerBERT21. Tables 8\nshows the learning rate for the best-performing\nmodels. The training was performed on V100\nGPU and the average latency for training of\nKroneckerBERT21 for a batch size of 64 was 32ms.\nAll the values are the results of single runs.\nA.2 Out of domain robustness\nIt is shown that pre-trained Transformer-based lan-\nguage models are robust to out-of-domain (OOD)\nsamples (Hendrycks et al., 2020). In this sec-\ntion, we investigate how the proposed compression\nWx\nA ⊗B\n(A ⊗ B)x\nFigure 5: Geometrical interpretation of projecting a\nmatrix product onto a Kronecker product. The angle of\nprojection is defined by the the size of A and B.\nmethod affects the OOD robustness of BERT by\nevaluating the fined-tuned models on MRPC and\nSST-2 on PAWS (Zhang et al., 2019) and IMDb\n(Maas et al., 2011) respectively. We compare OOD\nrobustness with the teacher, BERTBASE and Tiny-\nBERT. TinyBERT fine-tuned checkpoints are ob-\ntained from their repository. Table 9 lists the results.\nKroencekrBERT8 outperforms TinyBERT on two\nof the three OOD experiments. We can see the\nfine-tuned KroneckerBERT8 models on MRPC is\nrobust to OOD since there is a small increase in\nperformance compared to BERTBASE. On IMDb\nour KroenckerBERT8 has a small drop in accuracy\n(1.5% compared to 9.5% for TinyBert) after com-\npression.\nA.3 Training efficiency\nTable 10 shows the training requirements of differ-\nent compression methods in terms of their training\ndata. Some models require pretraining a designed\nteacher from scratch before pretraining the student.\nKroneckerBERT however only pretrain on 10% of\nWikipedia for 3 epochs. For fine-tuning in contrast\nto TinyBERT our KroneckerBERT model is trained\nfor on the original data. The number of fine-tuning\nepochs for the majority of the GLUE tasks is less\nthan 15.\nA.4 Geometrical interpretation of Kronecker\nproduct projection\nFigure 5 shows a geometrical interpretation of Kro-\nnecker product projection versus the original lin-\near projection. It shows how Kronecker product\nconstraints the space of possible projections. The\nflexibility of this space is a function of the shape of\nthe core matrices A and B.\n2126\nModel MNLI-(m/mm) SST-2 MRPC CoLA QQP QNLI RTE STS-B Average\nKroenckerBERT21 80.6/80.6 88.9 86.2 34 90 87.2 66.4 86.4 77.8\nTable 7: The results of the best-performing models on GLUE dev.\nHyperparamter MNLI-m MNLI-mm SST-2 MRPC CoLA QQP QNLI RTE STS-B\nKroneckerBERT8 e-4 e-4 e-4 e-4 5e-5 e-4 e-4 1e-5 e-4\nKroneckerBERT21 5e-4 5e-4 5e-4 5e-4 5e-4 e-4 5e-4 5e-5 5e-4\nTable 8: The hyper-parameters for the KroneckerBERT models\nModel MRPC →PA WS SST-2 →IMDb RTE →HANS\nBERTBASE 61.3 88.0 50.7\nTinyBERT 61.3 78.5 51.2\nKroneckerBERT8 61.4 86.5 50.4\nTable 9: The results of out of distribution experiment. Fined-tuned models on MRPC and SST-2 are evaluated on\nthe dev sets of PAWS and IMDb respectively.\nModel Pretraining a specific teacher Pretraining student Fine-tuning student\nMobileBert IB-BERTLARGE on EW+BC EW+BC task data\nShared project BERTLARGE with mixed vocabulary on EW+BC EW+BC task data\nTinyBert None EW task data + Data Augmentation(20x)\nKroneckerBERT None 10% EW task data\nTable 10: Sample efficiency during training in various methods. EW and BC denotes English Wikipedia and Book\ncorpus with 2.5B and 800M words respectively.\nA.5 Datasets\nWe evaluate the proposed framework on the Gen-\neral Language Understanding Evaluation (GLUE)\n(Wang et al., 2018) benchmark ( https://\ngluebenchmark.com/). This benchmark con-\nsist of the following tasks in English language:\nStanford Sentiment Treebank (SST-2)(Socher et al.,\n2013) and CoLA (Warstadt et al., 2019) for Senti-\nment Classification. , on Microsoft Research Para-\nphrase Corpus (MRPC) (Dolan and Brockett, 2005)\nQuora Question Pairs (QQP) (Chen et al., 2018) for\nParaphrase Similarity Matching, Multi-Genre Nat-\nural Language Inference (MNLI) (Williams et al.,\n2017), and Recognizing Textual Entailment (RTE)\n(Bentivogli et al., 2009) for Natural Language in-\nference.\nWe also evaluate the performance of the model\non SQuAD datasets ( https://rajpurkar.\ngithub.io/SQuAD-explore). The datasets\nare distributed under the CC BY-SA 4.0 license.\nSQuAD1.1 (Rajpurkar et al., 2016) is a large-scale\nEnglish reading comprehension that contains 87K\nquestion that have answers in the training set(10k\nin the dev set). SQuAD2.0 (Rajpurkar et al., 2018)\ncombines the questions in SQuAD1.1 with over\n50,000 unanswerable questions (130k samples in\nthe training and 11k in the dev set).\n2127",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6326816082000732
    },
    {
      "name": "Decomposition",
      "score": 0.548624575138092
    },
    {
      "name": "Association (psychology)",
      "score": 0.44903022050857544
    },
    {
      "name": "Natural language processing",
      "score": 0.43685218691825867
    },
    {
      "name": "Compression (physics)",
      "score": 0.4221712350845337
    },
    {
      "name": "Kronecker delta",
      "score": 0.42201024293899536
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38928329944610596
    },
    {
      "name": "Philosophy",
      "score": 0.1116391122341156
    },
    {
      "name": "Chemistry",
      "score": 0.08345398306846619
    },
    {
      "name": "Epistemology",
      "score": 0.06853446364402771
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Composite material",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210159102",
      "name": "Huawei Technologies (Sweden)",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I151746483",
      "name": "University of Waterloo",
      "country": "CA"
    }
  ]
}