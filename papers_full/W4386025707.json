{
    "title": "Dysarthric Speech Transformer: A Sequence-to-Sequence Dysarthric Speech Recognition System",
    "url": "https://openalex.org/W4386025707",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2314066031",
            "name": "Seyed Reza Shahamiri",
            "affiliations": [
                "University of Auckland"
            ]
        },
        {
            "id": "https://openalex.org/A5092807467",
            "name": "Vanshika Lal",
            "affiliations": [
                "University of Auckland"
            ]
        },
        {
            "id": "https://openalex.org/A2597430764",
            "name": "Dhvani Shah",
            "affiliations": [
                "University of Auckland"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2892009249",
        "https://openalex.org/W1998929500",
        "https://openalex.org/W3197150384",
        "https://openalex.org/W6638749077",
        "https://openalex.org/W3174329270",
        "https://openalex.org/W3207244775",
        "https://openalex.org/W2972818416",
        "https://openalex.org/W4213390597",
        "https://openalex.org/W4220692048",
        "https://openalex.org/W2140360678",
        "https://openalex.org/W2333243418",
        "https://openalex.org/W180052447",
        "https://openalex.org/W2018363392",
        "https://openalex.org/W2412801388",
        "https://openalex.org/W2888807255",
        "https://openalex.org/W3183046363",
        "https://openalex.org/W2465292261",
        "https://openalex.org/W2581716499",
        "https://openalex.org/W2250686550",
        "https://openalex.org/W6677093080",
        "https://openalex.org/W2401277329",
        "https://openalex.org/W2794162441",
        "https://openalex.org/W2962826786",
        "https://openalex.org/W6749669830",
        "https://openalex.org/W2933389261",
        "https://openalex.org/W3120067823",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3157063407",
        "https://openalex.org/W4297747548",
        "https://openalex.org/W2115692477",
        "https://openalex.org/W1828163288",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3007328579"
    ],
    "abstract": "Automatic Speech Recognition (ASR) technologies can be life-changing for individuals who suffer from dysarthria, a speech impairment that affects articulatory muscles and results in incomprehensive speech. Nevertheless, the performance of the current dysarthric ASR systems is unsatisfactory, especially for speakers with severe dysarthria who most benefit from this technology. While transformer and neural attention-base sequences-to-sequence ASR systems achieved state-of-the-art results in converting healthy speech to text, their applications as a Dysarthric ASR remain unexplored due to the complexities of dysarthric speech and the lack of extensive training data. In this study, we addressed this gap and proposed our Dysarthric Speech Transformer that uses a customized deep transformer architecture. To deal with the data scarcity problem, we designed a two-phase transfer learning pipeline to leverage healthy speech, investigated neural freezing configurations, and utilized audio data augmentation. Overall, we trained 45 speaker-adaptive dysarthric ASR in our investigations. Results indicate the effectiveness of the transfer learning pipeline and data augmentation, and emphasize the significance of deeper transformer architectures. The proposed ASR outperformed the state-of-the-art and delivered better accuracies for 73% of the dysarthric subjects whose speech samples were employed in this study, in which up to 23% of improvements were achieved.",
    "full_text": "1 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \nDysarthric Speech Transformer: A Sequence-to-\nSequence Dysarthric Speech Recognition System \n \nSeyed Reza Shahamiri, Senior Member, IEEE, Vanshika Lal, and Dhvani Shah \n \n  \nAbstract— Automatic Speech Recognition (ASR) technologies \ncan be life-changing for individuals who suffer from dysarthria, a \nspeech impairment that affects articulatory muscles and results \nin incomprehensive speech. Nevertheless, the performance of the \ncurrent dysarthric ASR systems is unsatisfactory, especially for \nspeakers with severe dysarthria who most benefit from this \ntechnology. While transformer and neural attention -base \nsequences-to-sequence ASR systems achieved state-of -the-art \nresults in converting healthy  speech to text, their applications as \na Dysarthric ASR remain unexplored due to the complexities of \ndysarthric speech and the lack of extensive training data. In this \nstudy, we addressed this gap and proposed our Dysarthric \nSpeech Transformer that uses a customized deep transformer \narchitecture. To deal with the data scarcity problem, we designed \na two-phase transfer learning pipeline to leverage healthy speech, \ninvestigated neural freezing configurations, and utilized audio \ndata augmentation. Overall, we trained 45 speaker -adaptive \ndysarthric ASR in our investigations. Results indicate the \neffectiveness of the transfer learning pipeline and data \naugmentation, and emphasize the significance of deeper \ntransformer architectures. The proposed ASR outperformed the \nstate-of-the-art and delivered better accuracies for 73% of the \ndysarthric subjects whose speech samples were employed in this \nstudy, in which up to 23% of improvements were achieved. \n \nIndex Terms—Dysarthria, Dysarthric Speech Recognition, Deep \nLearning, Transformers  \nI. INTRODUCTION \nYSARTHRIA occurs when the muscles responsible \nfor articulation become weak or difficult to control. \nThe impairment typically results in speech that is \ndifficult to understand, and it is commonly caused by nervous \nsystem anomalies and illnesses that produce facial and \narticulatory muscle paralysis. Depending on the severity and \nunderlying cause, the signs and symptoms of dysarthria may \n \nThis paragraph of the first footnote will contain the date on which you \nsubmitted your paper for review, which is populated by IEEE. It is IEEE style \nto display  support information, including sponsor and financial support \nacknowledgment, here and not in an acknowledgment section at the end of the \narticle. For example, “This work was supported in part by the U.S. \nDepartment of Commerce under Grant 123456.” The n ame of the \ncorresponding author appears after the financial information, e.g.  \n(Corresponding author: Seyed Reza Shahamiri).  \nAll authors are with the Dep artment of Electrical, Computer, and Software \nEngineering, Faculty of Engineering, The University of Auckland, New \nZealand. \nSeyed Reza Shahamiri (e-mail: admin@rezanet.com).  \nVanshika Lal (e-mail: vlal080@aucklanduni.ac.nz). \nDhvani Shah (e-mail: dsha439@aucklanduni.ac.nz). \nColor versions of one or more of the figures in this article are available \nonline at http://ieeexplore.ieee.org \nbe slurred or slow speech, inability to whisper, ra pid speech, \nor monotone speech [1]. As the impairment progresses, speech \nproduced by dysarthric individuals becomes unintelligible due \nto the intensive muscle paralysis making phone production \ndifferent from normal (aka healthy) speakers [2]. Hence, \ndysarthric individuals struggle to communicate with others.  \nPeople with dysarthria and other speech disorders can \nbenefit from Automatic Speech Recognition systems (ASR) \nsince ASR can enable computers to hear them and talk on their \nbehalf. The technology ca n be life -changing for people \nsuffering from severe dysarthria as computers can talk on their \nbehalf and enable them to interact with digital devices [3]. \nASR is the technique by which a computer recognizes spoken \nlanguage or utterances. Recent advancements in ASR \ntechnologies have resulted in the widespread usage of ASR \nsystems in various devices, including smartphones and smart \nhome devices, to provide an automated assistant system that \ncan accurately transcribe spoken words . However, even the \nbest-performing ASR systems are ineffective for speakers with \nthe speech impairment, especially those who could benefit \nfrom the technology most, such as severe dysarthria sufferers. \nRowe et al. [4] indicated that the deficiency might be due to \nthe difficulties o f getting sufficiently diversified impaired \nspeech training samples. From a clinical standpoint, they \nuncovered how having variety might affect ASR performance, \nwhich might aid in developing a generalized system that \ndifficult-to-recognize speakers can use. \nThus, automated recognition of dysarthric speech remains a \nchallenge because of the features associated with the \nimpairment. The irregularities of dysarthric speech negatively \nimpact phone generation and articulation, resulting in high \ncomplexities in au tomatically processing and recognizing \ndysarthric speech. For example, the significant variations of \ndysarthric speech require modern ASR technologies to process \na large amount of the impaired speech from many speakers to \ncapture inter -speaker variability.  Nonetheless, the availability \nof public dysarthric speech samples is very limited since it is \ndifficult to capture a large amount of speech from such \nindividuals due to muscle fatigue caused by the impairment \n[5]. As such, the scarcity of dysarthric spe ech samples is one \nof the major challenges preventing the successful development \nof dysarthric speech recognition systems. \nAmongst the most successful ASR methods are \nTransformers and neural self -attention mechanisms [6]  that \nhave resulted in state -of-the-art ASR achievements. \nTransformers have proven highly effective for sequence -to-\nsequence transduction tasks because neural attention \nmodulates token representations using the interpretations of \nD \nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3307020\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \ncorrelated tokens in the sequence and increasing the learnin g \neffectiveness of long -range dependencies. Nevertheless, end -\nto-end transformer self-attention-based ASR systems are yet to \nbe properly investigated to recognize dysarthric speech since \nsuch architectures often require a significant amount of \ntraining dat a that are not publicly available for dysarthric \nspeech. \nTo address this gap, we propose our Dysarthric Speech \nTransformer (DST), a speaker -adaptive, end-to-end dysarthric \nASR that utilizes state -of-the-art transformer architectures \ncomposed of multiple encoder and decoder modules. To \naddress the scarcity of dysarthric data and be able to train such \ndeep architectures, we designed a training pipeline to apply \ntwo phases of transfer learning by utilizing two speech \ncorpora. Two steps of neural freezing and parameter \nadjustments enabled the DST to  learn and map healthy speech \nsignals to character sequences before fine- tuning to recognize \ndysarthric speech. The proposed pipeline also applies audio \ndata augmentation techniques to enhance the limited \navailability of dysarthric speech samples. The DST has been \nverified via speech samples collected from multiple dysarthric \nspeakers with different speech intelligibility levels and \ncompared in detail with state -of-the-art dysarthric speech \nrecognition systems evaluate d using the same speakers ’ \nutterances. \nThe contributions of this study can be summarized as \nfollows: \n1. The proposition of a seq- to-seq transformer ASR tailored \nfor dysarthric speech, \n2. Transformer architectural selection and configuration for \ndysarthric speech recognition,  \n3. Investigating the effects of depths over performance, \n4. Tackling the dysarthric data scarcity issue by designing a \ntwo-phase transfer learning and neural freezing, and \ninvestigating the best transfer learning architectural \nconfigurations, \n5. Studying the effects of audio data augmentation on the \nproposed Dysarthric Speech Transformer ’s performance, \nand \n6. Detailed per -speaker performance comparison with the \nstate-of-the-art. \nThe rest of this paper is organized as follows. The next \nsection briefly explains transformers in the speech recognition \ncontext. The research methodology is explained in the \nfollowing section, in which the proposed DST, the transfer \nlearning pipeline, datasets, and data augmentation are \ndescribed. The third  section provides f urther information on \nthe experimental setup, followed by the results and discussion \nsections. This section  also presents the comparative and \nbenchmarking study. The paper concludes with the conclusion \nsection and recommendations for future studies. \nII.\n D. SEQUENCE-TO-SEQUENCE ASR SYSTEMS AND \nTRANSFORMERS \nSeq-to-seq ASR is often referred to as the speech \nrecognition approach that uses deep learning- based encoder -\ndecoder models that map sequences of speech frames to \nsequences of characters. Both encoder and  decoder modules \nare trained together with the same loss function, in contrast to \ntraditional ASR models, where the acoustic, language, and \npronunciation models were  usually trained separately, each \nwith a separate loss function. Seq -to-seq models disregar d the \nframe-independence conjecture made by Hidden Markov \nModels and Connectionist Temporal Classification. This \nmeans their language model is implicit, and they can optimize \nerror rates more efficiently, resulting in better overall ASR \nperformance. The encoder module in seq-to-seq ASR converts \nthe speech frames presented either via traditional acoustic \nfeatures extraction methods or visually as spectrograms (aka \nvoicegrams) to hidden representations. The decoder then \nconverts these representations to a character sequence, \ncharacter by character.  \nInitial seq-to-seq ASR systems were commonly built using \nRecurrent Neural Networks ( RNNs). An example is Bahdanau \net al. [7], who employed a deep bi-directional RNN to encode \nthe speech signal into a suitable fea ture representation, and an \nattention-based Recurrent Sequence Generator RNN to  decode \nthis representation into a sequence of characters. \nNonetheless, using RNNs as the primary algorithm imposes \nlimitations that prevent seq -to-seq ASR systems from \nunlocking their full potential [8]. While RNNs work well for \nshort statements and prompt, their ability to learn larger \ncontexts are limited as they have a shorter window to \nreference from. Additionally, RNNs' sequential nature makes \nthem slow to train. To overc ome these limitations, \nTransformers and self- attention were proposed by Vaswani et \nal. [6], [9], in which the attention network intuitively learns to \npay attention to important features and ignores the rest, \nmaking the features context -aware. With self -attention, the \nnetwork can generate representations for characters based on \nother characters surro unding it, modulating token \nrepresentations. Attention acts as an interface between the \nencoder and decoder to provide the decoder with information \nfrom the decoder's hidden states. With multi -head attention, \nthe attention operation can be done multiple times for each \nattention layer. Transformers are deep neural networks that \nleverage the self-attention concept, commonly used in modern \nseq-to-seq tasks [10] . A study  compared transformers and \nRNNs in text- to-speech context  and observed a larger \nminibatch resulted in better validation L1 loss  [12] for \ntransformers with faster training but reported a negative \ninfluence on the L1 loss for RNNs [11]. \nTransformer and attention- based ASR was first introduced \nby Ding et al. [13], where a 2 -D attention mecha nism was \nproposed and evaluated on the Wall Street Journal normal \nspeech corpus. This study culminated with significantly lower \ntraining costs and achieved an excellent Word Error Rate \n(WER) demonstrating the Speech Transformer's efficiency \nand efficacy. S ince their introduction, transformers have been \nutilized in various ASR systems. \nWith respect to seq-to-seq dysarthric ASR, Google's Project \nEuphonia researchers employed a Recurrent Neural Network \nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3307020\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n3 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \nTransducers (RNN -T) [14] architecture composed of an \nencoder with eight Long Short -Term Memory (LSTM) layers \nand a language model with two LSTM layers. This ASR was \nevaluated on the Euphonia corpus [15] , which includes 294 \ndysarthria participants among other speech -impaired subjects. \nWhile the authors did not  report dysarthric-specific results, \nthey indicated that dysarthric speech was particularly difficult \nto model and hence was classified among high WER subsets. \nDespite the success of transformer normal speech ASR \nsystems, their application for dysarthric s peech is yet to be \nproperly investigated. Since training transformer ASRs \nrequires large datasets containing many labeled speech \nsamples, the training pipeline and architecture design process \nneed to carefully utilize the limited dysarthric speech data \navailable, exploring all available venues to transfer knowledge \nacross datasets and augment the available data. The following \nsection explains our Dysarthric Speech Transformer and \ntraining pipeline to fill this gap and reports how we overcame \nthe challenges mentioned before. \nIII.\n METHODOLOGY \nFigure 1 portrays the overall methodology and the training \npipeline. We began conducting experiments with healthy \nspeech to design the initial transformer architecture. We \nexperimented with various attention -based transformer \nencoder and decoder architectures to  identify the optimal base \nmodel structures. From these trials, we chose the two \ntransformer architectures presented in section III.B as the base \nmodels since they demonstrated superior performance \ncompared to the others . These base models were first train ed \non healthy connected speech in the first phase of transfer \nlearning. Next, we investigated the best neural freezing  [16] \narchitectures that best preserved the knowledge acquired from \nhealthy speech to fine -tune the models as normal -speech \ncontrol mode ls. Once the control models were trained, we \ninitiated the second phase of neural freezing and investigated \nthe best configurations that retained the previous base and \ncontrol knowledge but unlocked enough synaptic weights to \nlearn dysarthric speaker -specific acoustic features. We also \napplied audio data augmentation to reinforce the limited \ndysarthric speech available, then trained the control models for \neach dysarthric participant independently. In particular, we \nrefer to the initial models, trained on co nnected, healthy \nspeech, as 'base models'. Subsequently, the base models \ntrained on control subjects supplied by the dysarthric corpus \nare termed 'control models', while the control models trained \non dysarthric subjects are designated 'dysarthric models'. \nOur experiments were concluded by evaluating both \ntransformer architectures per dysarthric participant, including \nhow the model performed with and without the augmented \ndata, and comparing both transformer architectures with each \nother and the state -of-the-art. The rest of this section explains \nthis process in detail. \nA. Materials and Participants  \nThere are very few speech corpora available publicly that \ninclude dysarthric speech samples. Among them are Nemours \n[17], TORGO [18], and UA -Speech [19]. Google also has the \nEuphonia dataset [15] reported to have dysarthric samples, but \nthis dataset was not publicly available at the time this study \nwas conducted, and our request to access it was unsuccessful. \nAmong the remaining datasets, UA -Speech is the largest, has \nmore dysarthric participants, and has been the most widely \nused corpus in the literature for dysarthric speech recognition \nresearch [31]. As such, it was used in this study. \nUA-Speech was developed by the University of Illinois \nresearchers and  features speech samples obtained from 19 \ndysarthric individuals with speech intelligibility levels ranging \nfrom 2% to 95%. The intelligibility levels of dysarthric \nspeakers range from very low (0- 25%) intelligibility to low \n(25-50%), mild (50 -75%), and hi gh (75- 100%) intelligibility. \nSpeech intelligibility can be defined as to what extent speech \nis comprehensible by a typical listener and is one of the \nmechanisms to define the severity of dysarthria [20] . The \ncorpus overall provides speech samples collec ted from 28 \nspeakers, including 15 dysarthric speakers and 13 healthy \ncontrol speakers – the speech samples of the other four \ndysarthric participants are not publicly available  (speakers \nM02, M03, F01, M06).  \nUA-Speech data is subdivided into two subsets per speaker \nof common and uncommon words, where the uncommon \nwords are different per speaker. The common word samples \nare divided into B1, B2, and B3 blocks and are the same for all \nspeakers. Each block provides utterances recorded in different \nsessions.  \nHere, we used the 155 common words to build and evaluate \nthe dysarthric models and trained a separate model per \ndysarthric speaker. We used B1 and B2 utterances for training, \nbut B3 samples were withheld and only used for testing to \nensure they were unfores een by the models. Each speaker \nmodel was trained using audio samples from blocks B1 and \nB2 and then tested using audio samples from block B3. The \nvocabulary comprised ten numerals, 19 computer instructions, \n26 radio alphabets, and one hundred common words.  \nAdditionally, to enable the two- phase transfer learning and \nneural freezing, we employed normal speech samples from \neleven UA-Speech control subjects and the LJ Speech Dataset \n[21]. LJ Speech is a connected, normal speech corpus that \ncomprises around 24 hours of labeled audio data. It includes \n13,100 short audio recordings of a single speaker reading \npassages from seven non -fiction books. The total number of \nwords is 225,715, with 13,821 distinct words. Each audio file \nis a single -channel 16-bit PCM WA V with a sampling rate of \n22 KHz. However, LJ Speech utterances were resampled to \n16KHz to make them consistent with UA -Speech samples. \nTable 1 provides the participants' information. \nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3307020\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n1 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \n \nFig. 1. Methodology Overview \nTABLE 1. PARTICIPANTS \nNumber Participants Sex Age \nSpeech \nIntelligibility \n(%) \nIntelligibility \nLevel \n1 M04 Male >18 2 \nVery Low 2 F03 Female 51 6 \n3 M12 Male 19 7 \n4 M01 Male >18 17 \n5 M07 Male 58 28 \nLow 6 F02 Female 30 29 \n7 M16 Male 40 43 \n8 M05 Male 21 58 \nMild 9 M11 Male 48 62 \n10 F04 Female 18 62 \n11 M09 Male 18 86 \nHigh \n12 M14 Male 44 90 \n13 M10 Male 21 93 \n14 M08 Male 28 95 \n15 F05 Female 22 95 \n16-27 \nUA-Speech \nControl \nSpeakers \nFour \nfemale \nand \nseven \nmale  \nNot \nProvided \nNot \nApplicable \nHealthy \nSpeech \n28 LJ Speech \nSpeaker Male Not \nProvided \nNot \nApplicable \nHealthy \nSpeech \nB. The Proposed Dysarthric Speech Transformer \nArchitectures \nIn this study, we experimented with two transformer \narchitectures selected during our initial architecture design \nexperiments. Inspired by [6], [13], the architecture of the first \ntransformer model is presented in Fig.2, comprising four \nencoder modules and one decoder. Dysarthric speech samples \nwere prov ided to the model via voicegrams (aka \nspectrograms) with 200ms frames sliding 80ms while Fast \nFourier Transform of size 256 was applied. Before the \nvoicegrams were provided to the first encoder, down -sampling \nvia three convolutional layers was applied to utilize \nvoicegrams' structural locality. Each convolution layer applied \n64 filters of 1×11 dimension with two strides. \nThe speech features were then provided to the encoder \ncomponents to provide hidden representations that the decoder \nwould use. The encoder employed a multi -head attention layer \nwith two heads followed by a dropout layer. Before the output \nwas given to the feed -forward network, a residual connection \nwas used to re -insert the initial encoder input into the data \nstream, and a normalization layer was applied. Instead of \nbatch normalization, the transformers used layer normalization \nthat normalized each input vo icegram independently of others \ndue to the sequential nature of speech data. Finally , the feed -\nforward network processed the data , applied dropout and \nanother layer normalization, and  then passed the encoded \noutput to the next encoder module. The transform er was \ncomposed of four encoders stacked on top of each other. \nHowever, only the first encoder received the initial speech \nfeatures, and the following encoders received the encoded \noutput of the previous encoder. \nThe decoder was composed of three primary c omponents. \nThe initial attention and the feed -forward blocks were similar \nto the encoder architecture, but a middle attention component \nwas added that also received the hidden representations, the \noutput of the last encoder. The encoder received the text \ncorresponding to the given dysarthric speech sample, \nvectorized and character position information inserted. \nNonetheless, the input text was masked to include only the \nfirst N characters, but the last character was offset to force the \nmodel to learn to pred ict the N+1 character. The decoder used \nattention layers to recognize which tokens in the hidden \nrepresentations supplied by the encoders most likely \ncorrespond to the text token it was attempting to estimate. \nFinally, the decoder output was given to a dense output layer \nwith softmax activation to produce the next character \nprobabilities. The most probable character was found by \napplying an argmax function to the softmax result. \nDuring the inception phase, the initial input to the de coder \nwas an empty text with \"[start]\" token. Then, after the first \ncharacter was predicted, it was added to the initial text and fed \nback to the decoder to predict the next character. This process \ncontinued until the predicted character was token \"[end]\".  \nFor the second transfo rmer, a deeper encoder architecture \ndepicted in Figure 3  was designed. Compared to the previous \nencoder, the second transformer's encoder was modified  to 1) \nadd a second attention block and 2) replace the feed- forward \nnetwork block with two Depthwise Separable Convolution \nblocks to add more depth yet promote faster training. \nAdditionally, while the decoder architecture remained the \nsame as in Figure 2, we increased the number of encoder \nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3307020\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n5 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \nblocks to five and decoders to three. The rest of the \nconfigurations remained unchanged. \nIn order to identify the best -performing architectures and \nhyperparameters, LJ Speech samples were divided into 80% \ntraining and 20%  validation, and Bayesian Optimization \ntuning with the Gaussian process [22]  was used. Amongst the \nhyperparameters that were trailed and adjusted during this \nprocess were the number of hidden layers, number of attention \nheads, number of feed- forward blocks, number of encoder and \ndecoder layers, etc. The presented architectures achieved the \nbest initial performances on LJ Speech validation samples . All \nactivations functions were set to ReLU (instead of the output \nlayer), the optimizer was Adam, and categor ical cross-entropy \nwas the loss function. The training data was given to each \nnetwork in batches of 64 samples. \nC. Transfer Learning \nA two-phased transfer learning pipeline  via neural freezing  \nwas designed to leverage healthy and dysarthric speech \nsamples. To build the base models, both transformer \narchitectures were initially trained on the LJ Speech speaker \nwith a 99:1 training -to-validation ratio. Then, we investigated \nwhich layers to freeze and continued  training with UA-Speech \ncontrol samples before dysarthric models were trained. While \nbuilding the control models, we omitted one of the UA-Speech \ncontrol speakers to evaluate the transfer learning performance. \nThis approach was used to maximize the advant ages of the \navailable normal speech data to overcome the scarcity of \ndysarthric data.\n  \nNeural freezing locks the weights assigned to the frozen \nneurons and forces the learning algorithm to converge on the \nnew training data by only adjusting the unfrozen synaptic \nweights. Here, we applied neural freezing in two steps: once \nafter the LJ Speech models were trained and once before the \ndysarthric models were trained. In each step, we investigated \nfreezing which layers delivered the best performance.  The \ndetails of this investigation and which model components \nwere frozen during each neural freezing phase are provided in \nsection IV. \nD. Data Augmentation \nData augmentation is a technique commonly used in \nmachine learning tasks that applies random oscillations and \nperturbations to augment the training data without affecting \nthe class labels. The primary goal of data augmentation is to \nimprove model generalizability, especially when data is \nscarce. Adding additional data exposes the model  to more data \nvariations, which  often leads to improving the training \nefficacy. \nWe utilized audio data augmentation to create extra \nvoicegrams based on the available dysarthric speech data, \nincreasing the number of training samples . The new \nvoicegrams were created by shifting, noise inj ection, and \nspeed and pitch changing, similar to [23] . Pitch and speed \nmodifications were done by a factor of 10, white noise was \naltered by a random factor ranging from 0 to the length of the \naudio, and the shift was divided by a factor of 10. Each B1 and \nB2 UA -Speech dysarthric utterance was modified via the \nabove augmentation techniques. The augmented samples were \nthen added to the original dysarthric training data and used to \ntrain the dysarthric models. \n \nFig. 2. The first transformer architecture \nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3307020\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n6 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \n \n \nFig. 3. The modified encoder used in the second transformer \nE. Dysarthric Models Training and Evaluation \nOnce both transfer learning phases were applied and control \nmodels were ready, two dysarthric models were trained for \neach dysarthric speaker from Table 1 , one model per \ntransformer architecture presented before. During training, we \napplied both B1 and B2 original and augmented utterances. \nHowever, different neural freezing configurations stated in the \nnext section were studied per speaker to find the best match. \nSince UA -Speech only provides isolated words and not \nconnected speech, the performance of the models was \nmeasured using Word Recognition Accuracy (WRA) instead \nof WER. WRA is the most widely used metric in the literature \nto measure and benchmark  UA-Speech, and defined as the \npercentage of the number of words the model could correctly \nidentify to the number of words attempted. \nIV.\n EXPERIMENTS \nAll experiments were conducted on our specialized deep \nlearning workstation with an AMD Ryzen Threadripper  \n3990X 64- Core CPU, 256Gb RAM, and four NVIDIA RTX \n6000 GPUs. \nOnce both initial transformer architectures were finalized, \nthey were trained for 200 epochs on the LJ Speech speaker's \ndata as base models  to prepare for the first phase of transfer \nlearning. N ext, for the first transformer model based on the \narchitecture shown in Figure 2, the last two encoders were \nfrozen, and then the model was trained on UA-Speech control \nspeakers for another 100 epochs , yielding the first control \nmodel. For Transformer 2 (based on the modified architecture \nand encoder shown in Figure 3),  the best results were obtained \nwhen all three decoders were frozen , resulting in the s econd \ncontrol model . The training of the control models was done \nvia a speaker -independent paradigm in which all speech \nsamples from speaker CM06 were withheld during tr aining \nand only used for testing the models, while utterances of the \nremaining UA-Speech control speakers were used to train the \ncontrol models. The first transformer architecture delivered a \nWRA of 89% for the testing speaker, while the second \narchitecture delivered a 92% WRA. For testing purposes, we \nalso trained the second transformer only using the UA -Speech \ncontrol data bypassing the first transfer learning phase. This \nmodel delivered the best WRA of 87% for CM06, indicating \nthe effectiveness of the first transfer learning phase since a 5% \nWRA improvement was obtained. \nAfter the control models were trained, they were saved and \nemployed as the pre -trained networks for the second transfer \nlearning phase and the training of dysarthric models. We \ntrained an d evaluated 30 speaker -adaptive dysarthric models, \ntwo models per dysarthric speaker based on both transformer \narchitectures. All B1 and B2 UA -Speech dysarthric blocks \nplus the augmented training utterances were used to train the \ndysarthric models, and B3 speech samples were used for \ntesting. Additionally, to measure the effectiveness of the audio \ndata augmentation process explained before, we trained \nanother 15 dysarthric models based on Transformer 1  control \nmodel by only using the original dysarthric dat a, excluding the \naugmented utterances. \nIn the second phase of transfer learning, another set of \nneural freezing was applied before the dysarthric models were \ntrained, as stated before. Here, s ince the dysarthric speakers \nbelonged to different severity clas ses, and to enable the \nmodels to better adapt to the variations of dysarthric speech \nper severity class, Phase 2 neural freezing was done \ndifferently, as shown in Table 2. \nTABLE 2. TRANSFER LEARNING PHASE 2 NEURAL FREEZING \nCONFIGURATIONS \nIntelligibility \nLevel \nDysarthric \nModels \nTransformer 1 \n(no data \naugmentation) \nTransformer 1 \n(with data \naugmentation) \nTransformer 2 \n(with data \naugmentation) \nVery Low \nM04, F03, \nM12, M12, \nM01 \nDecoder Decoder Last decoder \nLow M07, F02, \nM16 \nLast encoder's \nfeed-forward \ncomponent \nLast encoder's \nfeed-forward \ncomponent \nLast decoder \nMild M05, M11, \nF04 \nDecoder's feed-\nforward \ncomponent \nDecoder \nLast decoder's \nfeed-forward \ncomponent \nHigh \nM09, M14, \nM10, M08, \nF05 \nDecoder's feed-\nforward \ncomponent \nSecond \ndecoder's dense \nlayer \nLast decoder's \nfeed-forward \ncomponent \nV. RESULTS AND DISCUSSION \nTable 3  shows the testing results with all 45 dysarthric \nmodels with both transformer architectures. The inclusion of \naugmented audio data improved the  first transformer's \naccuracy for 12 out of 15 speakers. The most significant \nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3307020\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n7 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \nincrease in WRA was for speaker F03, where an improvement \nof 17% was achieved, while M08 showed the highest WRA \ndecrease of 4% when augmented data was used during \ntraining. The a verage improvements for each intelligibility \nlevel with augmented data were 9%, 12%, and 3% for very \nlow, low, and mild intelligibility subjects, but no improvement \nwas gained with high intelligibility subjects. The augmented \ndysarthric voicegrams delivere d an absolute average WRA \nimprovement of 5% across all speakers, which shows data \naugmentation was overall effective. Data augmentation was \nparticularly effective for very low and low intelligibility \nsubjects, where significant improvements of up to 12% we re \nachieved. The speakers where no improvement was seen via \ndata augmentation were M09, M08, F05, and F04, among \nwhich the first three speakers' utterances were highly \ncomprehensible and almost  unrecognizable from normal \nspeech. Hence, data augmentation was ineffective and even \ndecreased performance for speakers whose speech was close \nto healthy.  \nTABLE 3. DYSARTHRIC MODELS TESTING RESULTS \nIntelligibility \nLevel \nDysarthric \nModels \nTransformer \n1 (no data \naugmentation) \nWRA \nTransformer \n1 (with data \naugmentation) \nWRA \nTransformer \n2 (with data \naugmentation) \nWRA \nVery Low \nM04 7% 9% 7% \nF03 34% 51% 51% \nM12 24% 34% 53% \nM01 31% 38% 51% \nVery Low Intelligibility \nAverage WRA (%) 24% 33% 41% \nLow \nM07 56% 72% 76% \nF02 52% 67% 74% \nM16 51% 56% 70% \nLow Intelligibility \nAverage WRA (%) 53% 65% 73% \nMild \nM05 55% 62% 66% \nM11 45% 54% 58% \nF04 63% 56% 72% \nMild Intelligibility \nAverage WRA (%) 54% 57% 65% \nHigh \nM09 79% 79% 84% \nM14 76% 79% 85% \nM10 89% 90% 91% \nM08 83% 79% 88% \nF05 89% 88% 93% \nHigh Intelligibility \nAverage WRA (%) 83% 83% 88% \nAbsolute Average WRA \n(%) 53% 60% 67% \nBetween the two transformer architectures trained with \noriginal and augmented data, Transformer 2 delivered better \nWRAs for 87% of the dysarthric participants. The biggest \nimprovement was for severe dysarthric subject M12, with a \n7% intelligibility level, for which Transformer 2 provided \n19% better WRA, followed by mild intelligibility subject F04, \nwith a 16% WRA increase. Nonetheless, the second \ntransformer architecture did not improve  WRA for the very \nlow intelligibility subject F03 and obtained a 2% lower WRA \nfor subject M04. Having a speech intelligibility score of 2%, \nM04 exhibited the lowest level of intelligibility among all \ndysarthric speakers. Although it  is challenging to provide a \nconcrete explanation for the subpar performance of both \nTransformers on this particular speaker, we hypothesize that \nthe extremely limited intelligibility of M04  speech, combined \nwith the speaker’s  distinct speech characteristics, led to \nsubstantial divergence  from the speech features found in the \nhealthy and control speech data used for pre -training the \nmodels. Consequently, the models struggled to establish a \nstrong and reliable mapping of M04’s speech signals to the \ndesignated vocabulary. \nFigure 4 portrays Transformer 2 improvements over \nTransformer 1, and Table 4  summarizes average \nimprovements across intelligibility levels. With respect to the \nintelligibility levels, Transformer 2 obtained better results \nacross all levels with an 8% average improvement across the \nspectrum instead of mild dysarthric subjects with high \nintelligibility, where on average , 5% better WRA was \nobtained. Overall, Transformer 2, which employed the deeper \nencoder architecture utilizing depthwise separable \nconvolutions over feed -forward layers and an increased \nnumber of encoders a nd decoders, yielded an overall 7% \nbetter WRA over the traditional transformer architecture. The \nsecond transformer's better performance indicates th at the \nincreasing depth and the modifications made to the \narchitecture were effective and enabled the model  to capture \nthe complexity of dysarthric speech better . Although this \nincreased depth might seemingly lead to slower training and \nmodel inception, we observed no significant disparities in \ntraining or inception times between the two transformer \narchitectures. This lack of distinction can be attributed to the \nfact that Transformer 2's depthwise convolution replaced the \nfeed-forward blocks in Transformer 1, resulting in improved \nefficiency in the encoders of Transformer 2. \nTABLE 4. INTELLIGIBILITY LEVEL IMPROVEMENTS ACHIEVED BY \nTRANSFORMER 2 OVER TRANSFORMER 1 \nIntelligibility Level Average WRA Improvements \nVery Low Intelligibility 8% \nLow Intelligibility 8% \nMild Intelligibility 8% \nHigh Intelligibility 5% \nOverall 7% \n \nFig. 4. Transformer 2 WRA improvements over Transformer 1 (with data \naugmentation) \n-2%\n0%\n1%\n4%\n4%\n4%\n5%\n5%\n6%\n7%\n9%\n13%\n14%\n16%\n19%\nM04\nF03\nM10\nM11\nM07\nM05\nM09\nF05\nM14\nF02\nM08\nM01\nM16\nF04\nM12\nVery Low Intelligibility\nMild Intelligibility\nLow Intelligibility\nHigh Intelligibility\nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3307020\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n8 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \nOverall, the proposed Dysarthric Speech Transformer \ndelivered a mean WRA of 68%, considering the best per -\nspeaker results obtained from all three sets of experimental \nresults shown in Table 3  with standard deviation σ=21%. \nFigure 5 portrays the statistical analysis of the proposed DST \nby plotting the best WRA achieved per speaker concerning \nmean and ±1σ and ±2σ above and below the mean. As can be \nseen, 80% of the WRAs obtained are within ±1σ  of the mean, \nand the remaining WRAs are very close to mean+σ and below \n+2σ, except M04 , which was an outlier. The WRA \ndistribution shown in Figure 5  indicates a normal distribution, \nwhich further establishes confidence in the results obtained \nfrom the proposed DST. \n \nFig. 5. Dysarthric Speech Transformer Statistical Analysis (WRA%) \nA. Performance Comparative Study and Benchmarking \nTable 5 provides a comparative study between the proposed \nDST and state-of-the-art WRA results reported in the literature \nfor the UA -Speech corpus. A fair comparison should pay \nspecial attention to the number of dysarthric participants in the \nstudy, the intelligibility class of the subjects,  and the \nvocabulary size. Th ese criteria are important because \nincreasing the vocabulary size adds to the complexity of ASR, \nand including more dysarthric subjects indicates better \ngeneralizability of the results and higher sta tistical \nsignificance. \nThe highest WRA reported on UA -Speech is [24] . \nHowever, in this study, all dysarthric utterances were mixed,  \nthen divided with a 75:25 train/test ratio. Given UA -Speech \npresents each utterance eight times, once for each different \nmicrophone array setup, this strategy of defining train and test \nspeech utterances should be avoided. This strategy likely \nemploys identical utterances for  training and testing, as all \nmicrophone samples were derived from the same recording \nsource. This mea ns the results presented did not indicate  the \nmodel’s generalizability but memorability and training \nperformance. Additionally, the results reported in [24]  were \nachieved over a small vocabulary of 29 words.  \nLikewise, while the authors of study [25]  indicated that \neach participant’s speech samples were divided into three \ncategories, they did not indicate whether the data was divided \nbased on UA -Speech block categorization (B1, B2, and B3 \nutterances) or microphone data; hence their results may have \nthe same limitation as [24]. Besides, not all UA -Speech \ndysarthric speakers were employed in their study. The next top \nWRA was reported in [26] , but the vocabulary size was only \n25 words, and the speech samples of only seven dysarthric \nparticipants were used in this study. \n \nTABLE 5. UA-SPEECH WRA PERFORMANCE COMPARISON \nReference ASR \nParadigm \n# UA-\nSpeech \nParticipants \nBest \nWRA \nReported \nComments \n[27] Speaker \nDependent 7 \nMaximum \nAverage \nWRA \n30.8% \nPLP + MAP \nAdaptation and \nHMM \n[27] Speaker \nAdaptive 7 \nMaximum \nAverage \nWRA \n36.8% \nPLP + MAP \nAdaptation and \nHMM \n[26] Speaker \nDependent 7 \nAbsolute \nAverage \nWRA \n81% \nMFCCs+MLPs \nwith MVML \narchitecture \n[26] Speaker \nIndependent All \nAbsolute \nAverage \nWRA \n75% \nMFCCs+MLPs \nwith MVML \narchitecture \n[24] Speaker \nDependent All \nAbsolute \nAverage \nWRA \n88% \nMFCCs+LL-\nSVM \n[25] Speaker \nDependent 9 \nAbsolute \nAverage \nWRA \n85% \nGNE+RNN \n[28] Speaker \nDependent All \nAbsolute \nAverage \nWRA \n59% \nThe model was \na hybrid MAP-\nMLLR-HMM \nwith MFCC \n[29] Speaker \nAdaptive All \nAbsolute \nAverage \nWRA \n54.16% \nPLP features \n+HMMs \n[5] Speaker \nAdaptive All \nAbsolute \nAverage \nWRA \n61% with \nno \nsynthetic \ndata used \nThe model was \nbased on \nSpatial \nConvolutional \nNetwork to \nrecognize \nword shapes \npresented as \nvoicegrams \nThe \nProposed \nDST \nSpeaker \nAdaptive All \nAbsolute \nAverage \nWRA \n68% \nTransformer \nand self-\nattention \nGNE: Glottal to Noise Excitation, PLP: Perceptual Linear Prediction, MAP: Maximum A \nPosteriori, HMM: Hidden Markov Model, MFCC: Mel -Frequency Cepstral Coefficients, \nMLP: Multilayer Perceptron ANN, MVML: Multi-View Multi Learner, SVM: Support \nVector Machine, RNN: Recurrent Neural Network  \nThus, the average WRA reported in [5]  is currently the \nhighest reported in the literature for UA- Speech that not only \nobtained from all 15 UA -Speech participants but also applied \nthe same train/test split strategy we considered in our study in \nwhich dysarthric B1 and B2 samples were used for training \nand B3 for testing. Similarly, the vocabulary us ed in [5] is \nidentical to ours. As such, we selected the dysarthric ASR \nreported in [5]  to benchmark our proposed DST since it \nenables us to perform a direct, fair comparison. Accordingly, \nper-speaker comparisons between the best WRAs achieved by \nthe p roposed Dysarthric Speech Transformer and the \nbenchmark system are depicted in  Figure 6. Notably, study [5] \nreported two sets of results, one when their model was trained \nwith the original and visually augmented data and another set \nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3307020\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n9 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \nwhen synthetically gene rated dysarthria speech samples were \nalso added. Since we did not use synthetic data, our \ncomparison is based on the results reported for the benchmark \nmodel trained with the original and augmented dysarthric data, \nexcluding experiments with synthetic data. \nThe proposed DST achieved better performances for 11 out \nof 15 dysarthric speakers. We can see significant \nimprovements for speakers M01, F03, and F04, with 23%, \n19%, and 18% better WRAs, respectively. DST's \nimprovements are more substantial for severe dys arthria, in \nwhich the DST obtained, on average, 13% better results than \nthe benchmark, followed by an average of 9% better WRAs \nfor low and mild intelligibility speakers. On the other hand, \nthere was no noticeable change for high intelligibility speakers \nwhere both the proposed DST and the benchmark ASR \nperformed similarly. Overall, the DST improved WRA over \n[5] by an average of 7%. Nonetheless, the benchmark \ndysarthric ASR delivered slightly better WRA for M04. \nVI.\n CONCLUSION \nIn this study, we experiment ed with transformer and \nattention-based architectures and proposed two seq -to-seq \narchitectures to develop a dysarthric ASR. We experimented \nand measured how increasing the depth and number of \ntransformer encoders and decoders could lead to better \nperformances, and how using depthwise separable convolution \ninstead of fully-connected encoder components could improve \nword recognition accuracies. To tackle the scarcity of \ndysarthric speech data to train deep transformers, we \ndeveloped a two- phase transfer lear ning pipeline and \ninvestigated the best neural freezing configurations that best \nretain the knowledge acquired from healthy speakers. We have \nalso studied how applying audio data augmentation could help \nfurther address the data scarcity issues. Overall, we trained \nand evaluated 45 dysarthric models based on two transformer \narchitectures we designed. Our proposed Dysarthric Speech \nTransformer outperformed the state -of-the-art and delivered \nup to 23% better accuracies. Specifically, the DST was  more \ncapable of recognizing severe dysarthria with very low speech \nintelligibility compared to the benchmark ASR.  \nWe recommend the following avenues for future \nresearchers to investigate and further improve the DST:  \n• We used B1 and B2 UA -Speech samples during our \nexperiments for training and B3 for testing. However, this \nstrategy did not use uncommon words provided by the \ndataset. We recommend future researchers include all \navailable UA -Speech data during training but only \nwithhold one of the B blocks for testing w ith unforeseen \nutterances. Including more dysarthric training samples \nmay result in further DST performance improvements. \nWe did not apply this strategy to maintain consistency \nwith the previous notable studies and enable fair \nbenchmarking. \n• Alternatively, common words could be used for training \nand uncommon words for testing, and vice versa, as this \nstrategy still keeps training data from leaking into the \ntesting set, providing objective measures of \ngeneralizability. \n• Since the models were initially trained on a large corpus \nwith an open- set vocabulary, we can consider DST an \nopen-vocabulary ASR. However, because UA -Speech is a \nclosed-set vocabulary, our evaluation was done as a fixed-\nvocabulary ASR, which is common in dysarthric ASR \nresearch on UA-Speech. For example, all studies reported \nin Table 5  were evaluated as fix -vocabulary ASRs. \nNevertheless, given the open- vocabulary nature of the \nbase model, cross -database evaluation could explore the \nopenness of the proposed ASR.  \n• Likewise, even though our evaluations were based on \nisolated words, the proposed DST can recognize \nconnected speech since it maps speech utterances to a \nsequence of characters. Other less widely used  dysarthric \ndatasets, such as TORGO, contain connected dysarthric \nspeech. Hence, this can be another avenue for future \nstudies to explore how well the DST performs as a \nconnected speech dysarthric ASR. This strategy could be \nspecifically of interest to mild and high intelligibility \ndysarthric speakers who are more capable of speaking \ncontinuously in contrast to severe dysarthric patients. \n9%\n51%\n53%\n51%\n76%\n74%\n70%\n66%\n58%\n72%\n84%\n85%\n91%\n88%\n93%\n11%\n32%\n41%\n28%\n69%\n67%\n56%\n65%\n50%\n54%\n85%\n86%\n90%\n87%\n94%\nM04 F03 M12 M01 M07 F02 M16 M05 M11 F04 M09 M14 M10 M08 F05\nDysarthric Speech Transformer The Benchmark\nFig. 6. The proposed Dysarthric Speech Transformer Word Recognition Accuracies vs [5] \nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3307020\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n10 \n> REPLACE THIS LINE WITH YOUR MANUSCRIPT ID NUMBER (DOUBLE -CLICK HERE TO EDIT) < \n \n• The use of synthetically generated dysarthric speech has \nbeen investigated and shown effective in helping rectify \nthe data scarcity issue [30] . This approach could produce \nunlimited synthetic speech for a given dysarthric speaker, \nwhich could further augment dysarthric speech samples. \nWe recommend future studies investigating whether this \ncan lead to open -vocabulary ASR or performance \nimprovements. \nREFERENCES \n[1] T. Tamura, Y. Tanaka, Y. Watanabe, and K. Sato, “Relationships \nbetween maximum tongue pressure and second formant transition in \nspeakers with different types of dysarthria,” PLoS One , vol. 17, no. \n3 March, 2022, doi: 10.1371/journal.pone.0264995. \n[2] C. Tanchip et al., “Validating Automatic Diadochokinesis Analysis \nMethods Across Dysarthria Severity and Syllable Task in \nAmyotrophic Lateral Sclerosis,” Journal of Speech, Language, and \nHearing Research, vol. 65, no. 3, 2022, doi: 10.1044/2021_JSLHR -\n21-00503. \n[3] B. F. Zaidi, S. A. Selouani, M. Boudraa, and M. Sidi Yakoub, \n“Deep neural network architectures for dysarthric speech analysis \nand recognition,” Neural Comput Appl , pp. 1 –20, Jan. 2021, doi: \n10.1007/s00521-020-05672-2. \n[4] S. H. Lee, M. Kim, H. G. Seo, B. M. Oh, G. Lee, and J. H. Leigh, \n“Assessment of dysarthria using one -word speech recognition with \nhidden Markov models,” J Korean Med Sci , vol. 34, no. 13, 2019, \ndoi: 10.3346/jkms.2019.34.e108. \n[5] S. R. Shahamiri, “Speech Vision: An End -to-End Deep Learning -\nBased Dysarthric Automatic Speech Recognition System,” IEEE \nTransactions on Neural Systems and Rehabilitation Engineering , \nvol. 29, pp. 852–861, 2021, doi: 10.1109/TNSRE.2021.3076778. \n[6] A. Vasw ani et al. , “Attention Is All You Need,” 31st Annual \nConference on Neural Information Processing Systems (NIPS \n2017), June 2017, doi: 10.48550/arxiv.1706.03762. \n[7] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio, \n“End-to-end attention -based large vocabulary speech recognition,” \nin IEEE International Conference on Acoustics, Speech and Signal \nProcessing (ICASSP), 2016. doi: 10.1109/ICASSP.2016.7472618. \n[8] S. S. Tirumala and S. R. Shahamiri, “A deep autoencoder approach \nfor speaker identific ation,” ICSPS 2017:  Proceedings of the 9th \nInternational Conference on Signal Processing Systems , 2017. doi: \n10.1145/3163080.3163097. \n[9] A. Vaswani et al. , “Tensor2Tensor for Neural Machine \nTranslation”, arXiv:1803.07416. \n[10] F. Chollet, Deep Learning wi th Python, Second Edition. Manning, \n2021. Accessed: Jan. 03, 2023. [Online]. Available: \nhttps://www.manning.com/books/deep-learning-with-python-\nsecond-edition \n[11] S. Karita et al., “A Comparative Study on Transformer vs RNN in \nSpeech Applications,” 2019 IEEE Automatic Speech Recognition \nand Understanding Workshop, ASRU 2019 - Proceedings, pp. 449–\n456, Sep. 2019, doi: 10.1109/ASRU46091.2019.9003750. \n[12] S. R. Shahamiri, W. M. N. W. Kadir, and S. Ibrahim, “A single-\nnetwork ANN-based oracle to verify logical software modules,” in \nICSTE 2010 - 2010 2nd International Conference on Software \nTechnology and Engineering, Proceedings , 2010. doi: \n10.1109/ICSTE.2010.5608808. \n[13] L. Dong, S. Xu, and B. Xu, “Speech-Transformer: A No-Recurrence \nSequence-to-Sequence Model for Speech Recognition; Speech -\nTransformer: A No -Recurrence Sequence -to-Sequence Model for \nSpeech Recognition,” 2018, doi: 10.1109/ICASSP.2018.8462506. \n[14] A. Graves, “Sequence Transduction with Recurrent Neural \nNetworks,” Nov. 2012, doi: 10.48550/arxiv.1211.3711. \n[15] R. L. MacDonald et al., “Disordered speech data collection: Lessons \nlearned at 1 million utterances from Project Euphonia,” in \nProceedings of the Annual Conference of the International Speech \nCommunication Association, INTERSPEECH , 2021. doi: \n10.21437/Interspeech.2021-697. \n[16] Z. D. Champiri, S. S. B. Salim, and S. R. Shahamiri, “The Role of \nContext for Recommendations in Digital Libraries,” In ternational \nJournal of Social Science and Humanity , vol. 5, no. 11, 2015, doi: \n10.7763/ijssh.2015.v5.585. \n[17] X. Menendez-Pidal, J. B. Polikoff, S. M. Peters, J. E. Leonzio, and \nH. T. Bunnell, “Nemours database of dysarthric speech,” \nInternational Conference on Spoken Language Processing, ICSLP, \nProceedings, vol. 3, pp. 1962–1965, 1996. \n[18] F. Rudzicz, A. K. Namasivayam, and T. Wolff, “The TORGO \ndatabase of acoustic and articulatory speech from speakers with \ndysarthria,” Lang Resour Eval, vol. 46, no. 4, pp. 523–541, 2012. \n[19] H. Kim et al. , “Dysarthric speech database for universal access \nresearch,” in INTERSPEECH 2008 -  9th Annual Conference of the \nInternational Speech Communication Association , Brisbane, QLD, \nAustralia, 2008, pp. 1741–1744. \n[20] M. C. Coppens -Hofman, H. Terband, A. F. M. Snik, and B. A. M. \nMaassen, “Speech Characteristics and Intelligibility in Adults with \nMild and Moderate Intellectual Disabilities,” Folia Phoniatrica et \nLogopaedica, vol. 68, no. 4, 2017, doi: 10.1159/000450548. \n[21] “The LJ Speech Dataset.” https://keithito.com/LJ -Speech-Dataset/ \n(accessed Oct. 07, 2022). \n[22] Tom O’Malley, Elie Bursztein, James Long, and Francis Chollet, \n“KerasTuner.” 2019. \n[23] B. Vachhani, C. Bhat, and S. K. Kopparapu , “Data Augmentation \nUsing Healthy Speech for Dysarthric Speech Recognition,” in \nProceedings of the Annual Conference of the International Speech \nCommunication Association, INTERSPEECH , ISCA: ISCA, Sep. \n2018, pp. 471–475. doi: 10.21437/Interspeech.2018-1751. \n[24] N. Rajeswari and S. Chandrakala, “Generative Model -Driven \nFeature Learning for dysarthric speech recognition,” Biocybern \nBiomed Eng , vol. 36, no. 4, pp. 553 –561, Jan. 2016, doi: \n10.1016/J.BBE.2016.05.003. \n[25] S. Selva Nidhyananthan, R. Shantha Sel va kumari, and V. \nShenbagalakshmi, “Assessment of dysarthric speech using Elman \nback propagation network (recurrent network) for speech \nrecognition,” Int J Speech Technol , vol. 19, no. 3, 2016, doi: \n10.1007/s10772-016-9349-1. \n[26] S. R. Shahamiri, “Neural network-based multi-view enhanced multi-\nlearner active learning: theory and experiments ,” Journal of \nExperimental & Theoretical Artificial Intelligence , vol. 34 , no. 6 , \npp. 989-1009, 2014, doi: 10.1080/0952813X.2021.1948921. \n[27] H. V. Sharma and M. Hasegawa -Johnson, “State -Transition \nInterpolation and MAP Adaptation for HMM -based Dysarthric \nSpeech Recognition,” in NAACL HLT 2010 Workshop on Speech \nand Language Processing for Assistive Technologies , Los Angeles, \nUSA: ACM, 2010. \n[28] S. Sehgal and S. Cunningham, “Model adaptation and adaptive \ntraining for the recognition of dysarthric speech,” in Proceedings of \nSLPAT 2015: 6th Workshop on Speech and Language Processing \nfor Assistive Technologies , Stroudsburg, PA, USA: Association for \nComputational Linguistics , 2015, pp. 65 –71. doi: \n10.18653/v1/W15-5112. \n[29] H. Christensen, S. P. Cunningham, C. Fox, P. D. Green, and T. \nHain, “A comparative study of adaptive, automatic recognition of \ndisordered speech,” in Proc. INTERSPEECH 2012 -  13th Annual \nConference of the International Speech Communication Associatio , \nPortland, OR, USA, 2012, pp. 1776–1779. \n[30] A. Hu, D. Phadnis, and S. R. Shahamiri, “Generating synthetic \ndysarthric speech to overcome dysarthria acoustic data scarcity,” J \nAmbient Intell Humaniz Comput , 202 1, doi: 10.1007/s12652 -021-\n03542-w. \n[31] S. Liu et al. , “Recent Progress in the CUHK Dysarthric Speech \nRecognition System,” IEEE/ACM Trans Audio Speech Lang \nProcess, vol. 29, 2021, doi: 10.1109/TASLP.2021.3091805. \n \n  \nThis article has been accepted for publication in IEEE Transactions on Neural Systems and Rehabilitation Engineering. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TNSRE.2023.3307020\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/"
}