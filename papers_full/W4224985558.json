{
  "title": "Myers-Briggs personality classification from social media text using pre-trained language models",
  "url": "https://openalex.org/W4224985558",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2899221548",
      "name": "Vitor dos Santos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A207621152",
      "name": "Ivandré Paraboni",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1985449126",
    "https://openalex.org/W2084413241",
    "https://openalex.org/W2593282256",
    "https://openalex.org/W3014726746",
    "https://openalex.org/W3004040504",
    "https://openalex.org/W2982567551",
    "https://openalex.org/W2141599568",
    "https://openalex.org/W2774157693",
    "https://openalex.org/W4299980331",
    "https://openalex.org/W62115097",
    "https://openalex.org/W4231760770",
    "https://openalex.org/W2964236337",
    "https://openalex.org/W2888219827",
    "https://openalex.org/W2960748659",
    "https://openalex.org/W3095676573",
    "https://openalex.org/W3090884027",
    "https://openalex.org/W3010832370",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3203185649",
    "https://openalex.org/W4255399422",
    "https://openalex.org/W1995208979",
    "https://openalex.org/W3008038783",
    "https://openalex.org/W3029351153",
    "https://openalex.org/W3029916314",
    "https://openalex.org/W3035976911",
    "https://openalex.org/W2534808068",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W4231510805",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2963563735",
    "https://openalex.org/W3113304723",
    "https://openalex.org/W2953320089",
    "https://openalex.org/W2807032982",
    "https://openalex.org/W2250238316",
    "https://openalex.org/W2741852978",
    "https://openalex.org/W2990614085",
    "https://openalex.org/W2803478636",
    "https://openalex.org/W3096028991",
    "https://openalex.org/W3014084332",
    "https://openalex.org/W3184979277",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4393675269"
  ],
  "abstract": "In Natural Language Processing, the use of pre-trained language models has been shown to obtain state-of-the-art results in many downstream tasks such as sentiment analysis, author identification and others. In this work, we address the use of these methods for personality classification from text. Focusing on the Myers-Briggs (MBTI) personality model, we describe a series of experiments in which the well-known Bidirectional Encoder Representations from Transformers (BERT) model is fine-tuned to perform MBTI classification. Our main findings suggest that the current approach significantly outperforms well-known text classification models based on bag-of-words and static word embeddings alike across multiple evaluation scenarios, and generally outperforms previous work in the field.",
  "full_text": "Journal of Universal Computer Science, vol. 28, no. 4 (2022), 378-395\nsubmitted: 1/7/2021, accepted: 21/2/2022, appeared: 28/4/2022 CC BY-ND 4.0\nMyers-Briggs personality classification from\nsocial media text using pre-trained language models\nVitor Garcia dos Santos\n(UniversityofSãoPaulo,Brazil\nhttps://orcid.org/0000-0003-4856-4043,vitorgds95@gmail.com)\nIvandré Paraboni\n(UniversityofSãoPaulo,Brazil\nhttps://orcid.org/0000-0002-7270-1477,ivandre@usp.br)\nAbstract: In Natural Language Processing, the use of pre-trained language models has been\nshown to obtain state-of-the-art results in many downstream tasks such as sentiment analysis,\nauthoridentificationandothers.Inthiswork,weaddresstheuseofthesemethodsforpersonality\nclassificationfromtext.FocusingontheMyers-Briggs(MBTI)personalitymodel,wedescribe\na series of experiments in which the well-known Bidirectional Encoder Representations from\nTransformers (BERT) model is fine-tuned to perform MBTI classification. Our main findings\nsuggestthatthecurrentapproachsignificantlyoutperformswell-knowntextclassificationmodels\nbasedonbag-of-wordsandstaticwordembeddingsalikeacrossmultipleevaluationscenarios,and\ngenerallyoutperformspreviousworkinthefield.\nKeywords: Naturallanguageprocessing,textclassification,Myers-Briggs,MBTI,personality,\nauthorprofiling\nCategories: I.2.7\nDOI: 10.3897/jucs.70941\n1 Introduction\nHumanpersonality-asetofrelativelystablebehaviourpatternsofanindividual[Allport\nandAllport,1921]-hasbeenthefocusofstudiesinmultipledisciplines,anditiswell-\nknowntocomputersciencethroughpersonalitymodelssuchastheBigFive[Goldberg,\n1990]and,perhapstoalesserextent,theMyers-BriggsTypeIndicator(MBTI)[Myers,\n1962].Modelsofthiskindassociatewordchoicesmadebyanindividual(e.g.,acustomer,\nasocialmediauseretc.)topre-definedpersonalitycategories(e.g.,extrovertsversus\nintroverts), allowing us to assess their personality traits for a wide range of practical\napplicationsinbothnaturallanguageinterpretation[dosSantosandParaboni,2019,dos\nSantoset al.,2020]andgeneration[Teixeiraet al.,2014].\nPersonalityassessmentmayhoweverrequiretheuseofpersonalityinventories(e.g.,\n[Johnet al.,1991])withtheaidofspecialists,whichmaybecomecostlyinlargescale.\nAsanalternativetothis,studiesinNaturalLanguageProcessing(NLP)andrelatedfields\nhaveaddressedtherelationbetweenlanguageuseandpersonalitytodevelopmethods\nforautomaticallydetectingthepersonalitytraitsofindividualsbasedontextsamplesthat\ntheyhavewritten(e.g.,onsocialmediaetc.)[PlankandHovy,2015,Liuet al.,2017,dos\nSantoset al.,2017,Wuet al.,2020].\nPersonalitydetectionfromtextmaybeseenasaninstanceofauthorprofiling,that\nis,thetaskofinferringanauthor’sdemographicsbasedontextsamplesthattheyhave\ndos Santos V.G., Paraboni I.: Myers-Briggs personality classification ... 379\nauthored [Rangel et al., 2020, Polignano et al., 2020, Price and Hodge, 2020, López-\nSantillet.al.,2020,SilvaandParaboni,2018a,dosSantoset al.,2019,DelmondesNeto\nandParaboni,2021].Asinotherprofilingtasks(e.g.,author’sgenderoragedetection),\nstudiesofthiskindareusuallyimplementedwiththeaidofsupervisedmachinelearning\nbased on text corpora labelled with personality information. The issue of personality\nclassificationfromtextwithaspecificfocusontheMBTIpersonalitymodelisthesubject\nofthepresentstudy.\nExistingworkinMBTIclassificationfromtextgenerallyfollowsmuchofthesame\nmethods seen elsewhere in NLP, which usually comprise the use of a bag-of-words\nmodelor,morerecently,staticwordembeddingssuchasthoseprovidedbyWord2vec\n[Mikolovet al., 2013] andsimilarapproaches. We notice, however,that more recent\ntextrepresentationmodels-inparticular,context-sensitiveembeddingssuchasthose\nprovidedbyBidirectionalEncoderRepresentationsfromTransformers(BERT)[Devlin\net al.,2019]-arestillrelativelyuncommoninMBTIclassification,eventhoughthese\nmodelshavebeenshowntoobtainstate-of-the-artresultsinawiderangeofNLPappli-\ncationsfromsentimentanalysis[Hoanget al.,2019]toauthoridentification[Barlasand\nStamatatos,2020],andmanyothers.\nBased on these observations, the present work addresses the use of pre-trained\nBERTlanguagemodelsforMBTIpersonalityclassificationfromtextwritteninmultiple\nlanguages.Indoingso,ourobjectiveistoshowthatbyfine-tuningBERTtothepresent\ntaskwemaysignificantlyoutperformtheuseofothertextrepresentationmodelsacross\ntheseevaluationscenarios,withtwomaincontributionstothefield:\n1 BERT-basedmodelsforMBTIpersonalityclassificationfromtextinmultiple\nlanguages.\n2 Robust,cross-validationresultsshowntobeconsistentlysuperiortothoseob-\ntainedbybag-of-wordsandstaticwordembeddingsalike,andtopreviouswork\ninthefield.\nThe reminder of this paper is structured as follows. Section 2 reviews recent ap-\nproachestoMBTIpersonalityclassificationfromtext.Section3introducesanumberof\ncomputationalmodelsforthetask-includingtheuseofpre-trainedlanguagemodelsand\nbaselinealternatives-andthedatasetstobetakenasabasisforourexperiments.Section\n4presentsresultsobtainedbythesemodels,andSection5summarisesourfindingsand\ndescribesopportunitiesforfuturework.\n2 Background\nInwhatfollowswereviewexistingworkinMBTIpersonalityclassification,andbriefly\ndiscussopportunitiesforusingpre-trainedlanguagemodelsinthistask.\n2.1 Related work\nTable1summarisesanumberofrecentstudiesinMBTIpersonalityclassificationfrom\ntextbyreportingthetargetlanguage(Ar=Arabic,En=English,De=German,Du=Dutch,\nIt=Italian,Fr=French,Pt=Portuguese,Sp=Spanish,In=Indonesian),domain(T=Twit-\nter,R=Reddit,F=Facebook,O=onlineforums,E=essays,V=vlogs),machinelearning\n380 dos Santos V.G., Paraboni I.: Myers-Briggs personality classification ...\nmethod(lr=logisticregression,svm=supportvectormachine,nb=NaiveBayes,rf=Ran-\ndomForest,ens=ensemble,seq=BERTsequencelearner,svd=singularvaluedecomposi-\ntion,xg=XGBoost),andlearningfeatures(w=word,c=character,pos=part-of-speech,\nu=userattributes,n=networkattributes,p=psycholinguisticfeaturesfromLIWC[Pen-\nnebaker et al., 2001] and MRC [Coltheart, 1981], t=LDA topics [Blei et al., 2003],\nw2v=Word2vec [Mikolov et al., 2013] and BERT [Devlin et al., 2019] word embed-\ndings,s=textstatistics).Furtherdetailsarediscussedindividuallyasfollows.\nStudy Language Domain Method Features\nPlank&Hovy En T lr w,u,n\nbenVerhoevenetal. De,Du,It,Fr,Pt,Sp T svm w,c\nLukitoetal. In T nb w,s,pos\nAlsadhan&Skillicorn Ar,De,Du,En,It,Fr,Pt,Sp T,O,E,F svd w\nGjurković&Šnajder En R lr,mlp,svm c,w,p,t,n\nKeh&Cheng En O seq BERT\nKatiyaretal. En T,O nb w\nWuetal. En R lr BERT\nDas&Prajapati En O ens w,w2v\nAbidinetal. En O rf s\nKhanetal. En O xg w\nAmirhosseini&Kazemian En O xg w\nTable 1: Related work\nThe work in [Plank and Hovy, 2015] is among the first of its kind to address the\nissueofMBTIpersonalityclassificationinanopen-vocabularyapproach,thatis,without\nresortingtopersonalitylexiconsorsimilarresources.Theworkaddressespersonality\nclassificationintheTwitterdomainbyusinglogisticregressionoverwordn-grams,user\n(e.g.,user’sgender)andnetwork(e.g.,numberofsocialmediafollowersetc.)features.\nResultsareshowntooutperformamajorityclassbaseline.\nThe work in [ben Verhoeven et al., 2016] introduces the TwiSty corpus, a large\nmultilingualTwitterdatasetlabelledwithMBTIinformationinsixlanguages(German,\nDutch,French,Italian,Portuguese,andSpanish.)Thecorpusconveys34milliontweets\nwrittenbyover18thousandusers.Asignificantportionofthedata(about59%)concerns\nSpanish texts, which makes the other languages much less represented (e.g., 2.2% in\nGerman,and2.6%inItalian.)Sincesomepersonalitytraitsarenaturallyrarerthanothers,\nthecorpusisalsoheavilyimbalancedacrossMBTIclasses.Toillustratetheuseofthe\ncorpusdata,resultsfromalinearSVMclassifierandmajorityclassarepresented.\nThestudyin[Lukitoet al.,2016]addressesMBTIclassificationfromTwitterdata\nintheIndonesianlanguagebycomparinganumberofmodelsbasedonNaiveBayes\nclassification,TF-IDFandpart-of-speechcounts.Amongthese,standardNaiveBayes\ntextclassificationisfoundtobetheoverallbeststrategy.\nTheworkin[AlsadhanandSkillicorn,2017]presentsacomprehensiveinvestigation\nofbothBigFiveandMBTIpersonalityclassificationinmultiplecorporaandlanguages.\nThemethodusessinglevaluedecomposition(SVD)todiscriminateextremepersonality\ntraits(e.g.,introvertversusextrovert).FormostlanguagesavailablefromtheTwiSty\ncorpus,resultsarefoundtooutperformthosein[benVerhoevenet al.,2016].\nTheworkin[GjurkovićandŠnajder,2018]introducestheMBTI9Kcorpus,alarge\ndos Santos V.G., Paraboni I.: Myers-Briggs personality classification ... 381\ncollectionofRedditpostslabelledwithMBTIinformation.Thecorpusconveys354.996\npostswrittenby9,872usersintheEnglishlanguage,anditisalsoheavilyimbalanced\nacross MBTI classes. The use of the data is illustrated by a number of experiments\ninvolvinglogisticregression,SVMandmulti-layerperceptron(MLP)classifiersusinga\nrangeofalternativetextfeatures(e.g.,wordandcharactern-grams,psycholinguistics-\nmotivatedfeaturesetc.)ResultsshowthatMLPclassifiersusingtheentirefeatureset\ngenerallyobtainsbestresults.\nTheworkin[KehandCheng,2019]isamongthefirsttousepre-trainedlanguage\nmodelsforMBTIpersonalityclassificationfromtext,andalsoforpersonality-dependent\nlanguage generation. To this end, a pre-trained BERT [Devlin et al., 2019] model is\nfine-tuned to classify texts taken from a purpose-built dataset of online discussions\naboutpersonality.TheauthorssuggestthattheBERTmodelpresentsaccuracyabove\n70%inthetask,andpointoutthatthisisconsiderablysuperiortotheresultsobserved\nin other domains such as the MBTI9k Reddit corpus [Gjurković and Šnajder, 2018].\nHowever, the analysis does not present any baseline results obtained from the same\ncorpus,soitremainsunclearwhetherthemodelisindeedsuperiortoexistingwork,or\nwhether personality classification from personality-related texts (e.g., in which users\npresumablydiscusstheirpersonalitytraits,personalitytestresultsetc.)maybesimply\nmorestraightforwardthanperformingthesametaskbasedonmoregeneralsocialmedia\ntext.\nTheworkin[Katiyaret al.,2020]investigatesapracticalapplicationofMBTIclassi-\nficationbyfocusingonsocialmediadata(e.g.,blogs,Twitter,andStackOverflow)asa\nmeanstorecruitprojectteams.Tothisend,amodelbasedonNaiveBayesclassification\nand TF-IDF counts is evaluated using a set of 40 Twitter and Stack Overflow users,\nwhoseresultssuggestthatitmaybepossibletoinferbothpersonalitytraitsandtechnical\nskillsfromtexttofacilitaterecruitment.\nTheworkin[Wuet al.,2020]addressestheissueofauthor-dependentwordembed-\ndingsforauthorprofilingclassificationbyintroducingamodelcalled Author2Vec.The\npossibleuseofthisformalismisillustratedbydiscussingtwodownstreamapplications,\nnamely,depressiondetectionandMBTIpersonalityclassificationfromtext.Thelatter\nmakesuseofalogisticregressionclassifierbuiltfromasubsetoftheMBTI9kcorpus\n[Gjurković and Šnajder, 2018] conveying about half of the original corpus data. The\nmodelisfoundtooutperformanumberofalternativeregressionmodelsbasedonstatic\nWord2vecembeddings[Mikolovet al.,2013],TF-IDFcounts,andLDAtopicmodelling\n[Bleiet al.,2003].\nFinally,anumberofrecentstudiesinMBTIpersonalityclassificationhavemadeuse\nofasocialmediacorpusavailablefromKaggle 1 whosedetailsregardingdomainanddata\ncollectionmethodsremainscarce.Theseincludetheworkin[DasandPrajapati,2020],\nwhichcomparesboosting,bagging,andstackingensemblemethodsusingconcatenated\nTF-IDF counts and word embeddings; the work in [Abidin et al., 2020], which uses\nrandomforestandfeaturesbasedontextstatistics(e.g.,sentencelength,punctuation\netc.); and the studies in [Khan et al., 2020] and [Amirhosseini and Kazemian, 2020],\nbothofwhichusingXGBoostensemblelearning[ChenandGuestrin,2016]overword\ncounts.\n1 https://www.kaggle.com/datasnaek/mbti-type\n382 dos Santos V.G., Paraboni I.: Myers-Briggs personality classification ...\n2.2 Summary\nExistingworkinMBTIpersonalityclassificationfromtextbasedonpre-trainedlanguage\nmodelssuchasBERTremainfew.Thetwomainexceptionsarethestudyin[Kehand\nCheng,2019],whichdoesnotprovidesufficientlycompleteevaluationdetailsforfurther\nanalysis,andtheworkin[Wuet al.,2020],whichismainlyfocusedonanovelauthor-\norientedwordembeddingformalism,andwhichusesonlyasmallsubsetoftheMBTI9k\ncorpusin[GjurkovićandŠnajder,2018]asaworkingexampleofhowmodelsofthis\nkindmaybebuilt.Thismotivatesamorecomprehensiveinvestigationofthepresent\ntaskalongtheselines,andtakingintoaccountlanguagesotherthanEnglish.\n3 Materials and methods\nWe envisaged a series of experiments in supervised machine learning from text to\ncomparestandardtextclassificationmodels-basedonbag-of-wordsandstaticword\nembeddingsalike-withthosebuiltbyfine-tuningapre-trainedBERTlanguagemodel\ntoperformMBTIclassification.Indoingso,wewouldliketoshowthattheBERT-based\napproachoutperformsthealternativesbyalargemargin.\nOurexperimentsfollowa3-stepssupervisedmachinelearningpipelinethatrelieson\ntrainingdata(i.e.,textdocuments)labelledwithMBTIpersonalityinformationto(1)\nbuildatextclassifiermodel,(2)usethemodeltopredicttheclassofpreviouslyunseen\ntestdata,andtoproduce(3)thecorrespondingoutputlabels.Thisprocedureisillustrated\ninFigure1.\nFigure 1: Experiment pipeline.\nAs a means to reduce the risk of overfitting, the experiments will be carried out\nin a 10-fold cross-validation setting, that is, each individual text classifier is built 10\ntimeswhilevaryingtheslicesofdatatakenastrainandtestsetsand,attheendofthe\nevaluation,wereportmeanresultsoverthe10execution.\nThemodelsunderdiscussionaretobeevaluateusingRedditandTwittertextdatain\nmultiplelanguages.Inallcases,MBTIpersonalitydetectionwillbemodelledasasetof\nfourindependentbinaryclassificationtasks 2 correspondingtothefourMBTIpersonality\n2 Forinstance,theEIclasswillbeassignedthezerovaluewhentheEtraitisprevalent,orthe\nonevalueotherwise.\ndos Santos V.G., Paraboni I.: Myers-Briggs personality classification ... 383\ntypeindicators[Myers,1962]:\n1. EI:Extraversion(E)versusIntroversion(I);\n2. NS:Intuition(N)versusSensing(S);\n3. TF:Thinking(T)versusFeeling(F);\n4. PJ:Perceiving(P)versusJudging(J).\nThefollowingsectionsdescribethemodelsdevelopedforMBTIpersonalityclas-\nsification, the corpora to be taken as train/test data, and further details regarding the\npre-processingandtrainingprocedures.\n3.1 Models\nMBTIpersonalityclassificationfromtextwillbeassessedbycomparingthreealterna-\ntives3:BERTpre-trainedlanguagemodels,longshort-termmemorynetworks(LSTM)\nusingstaticwordembeddings,andalogisticregressionbag-of-wordsbaseline.These\narediscussedinturnasfollows.\nThe main focus of the present work is the use of BERT [Devlin et al., 2019] pre-\ntrainedlanguagemodels,whicharepresentlyfine-tunedfortheMBTIpersonalityclassi-\nficationtask.Tothisend,wecomputecontext-dependent DistilBert[Sanhet al.,2019]\nembeddings, and then feed 32-token input sequences to a network conveying a 512-\nneurondenselayer.Thisisfollowedbya50%dropoutlayer,andbya2-neurondense\nlayerthatproducesthebinaryclassificationresult.\nAs an alternative to the use of pre-trained language models, we also consider the\nuse of a sequence classifier based on a static word embeddings representation using\nLSTMs.Methodsofthiskindhavebeenshowntoobtainencouragingresultsinawide\nrangeofNLPtasks,fromstanceandsentimentanalysis[Zhangandv Wang,2018,dos\nSantosandParaboni,2019,Pavanet al.,2020]toauthorprofiling[SilvaandParaboni,\n2018b,Ashrafet al.,2020,Escobar-Grisaleset al.,2021]andothers.Morespecifically,\nwecomputeWord2vec[Mikolovet al.,2013]skip-gramwordembeddingsfromeach\ncorpus4 usingastandard300-dimensionsizeand8-wordwindow.Thisrepresentationis\nfedintoafixedLSTMarchitecture(i.e.,withnofurtherfine-tuningduetocomputational\nefficiencyissues)comprisinga15-neuronattentionlayer,twoLSTMlayerscontaining\n15neuronseach,anda20%dropoutlayer.Thisisfollowedbya64-neurondenselayer,\nanda2-neuronsoftmaxoutputlayer.\nFinally, we also consider a standard bag-of-words text classifier based on logis-\ntic regression over a word n-grams text representation. This approach, hereby called\nReg.word,makesuseofTF-IDFn-gramcounts.Eachoftheseinputrepresentationswas\nsubject to univariate feature selection, and thek optimal features were searched in a\ndevelopmentdatasetwithinthe30000-1000rangeat-1000intervalsusingF1asascore\nfunction.OtherlogisticregressionparameterswerekeptconstantbyusingL2penalty,\nlbfgsoptimisation,balancedclassweights,and 10−4 tolerance.\n3 Codeavailablefromhttps://github.com/vitorsantos95/mbti-classifier\n4 Forthispurpose,thecorpusdataistakensimplyasacollectionofwordstrings,thatis,without\naccesstoany(MBTI)classinformation.\n384 dos Santos V.G., Paraboni I.: Myers-Briggs personality classification ...\n3.2 Data\nThemodelsdescribedintheprevioussectionaretobeevaluatedinmultiplelanguages,\nnamely,English,German,Italian,Dutch,French,Portuguese,andSpanish.Inthecaseof\nEnglish,weusetheMBTI9kcorpusofRedditposts[GjurkovićandŠnajder,2018],and\nfortheotherlanguagesweusetheTwiStycorpusintheTwitterdomain[benVerhoeven\net al.,2016].BothMBTI9kandTwiStytextsarelabelledwiththefourMBTIpersonality\nindicators,whoseclassdistributionissummarisedinTable2.Wenoticehoweverthat\nbothdatasetsareslightlysmallerthanthoseoriginallyreportedin[GjurkovićandŠnajder,\n2018]and[benVerhoevenet al.,2016]sincesomeofthedataarenolongeravailable\nonline,orwereremovedduetonoise.ThisissueismoreprevalentintheTwitterdomain\ningeneral,butithasalsobeenraisedinthecontextofthepresentRedditdatasetin[Wu\net al.,2020].\nLang. E I N S T F P J\nEn 1,423 5,053 5,625 851 4,168 2,308 3,759 2,717\nDe 92,452 180,252 227,409 45,295 113,414 159,290 170,232 102,472\nIt 26,445 59,048 69,161 16,332 44,157 41,336 36,012 49,481\nDu 70,904 33,589 76,987 27,506 35,791 68,702 66,447 38,046\nFr 249,742 481,480 566,473 164,749 297,702 433,520 451,187 280,035\nPt 27,920 32,387 44,919 15,388 27,160 33,147 32,536 27,771\nSp 243,840 277,788 334,483 187,145 199,573 322,055 302,092 219,536\nTable 2: Corpus class distribution across English (En), German (De), Italian (it), Dutch\n(Du), French (Fr), Portuguese (Pt), and Spanish (Sp) subsets.\n3.3 Procedure\nThedatafromeachcorpuswassubjecttoa30/70development/testsplit.Development\nsets were taken as an input to compute hyper-parameters for each model, and then\ndiscarded. Validation proper was performed using 10-fold cross validation over the\npreviouslyunseentestsets.\nAlltextsweresubjecttotheremovalofspecialcharacters(inparticular,emoticons).\nInapilotexperiment,wealsofoundoutthatstopwordsdidnotgenerallyimproveresults\nforthetaskathandand,accordingly,thesewereremovedusingNLTK[Bird,2006]for\nthesakeofefficiency.Otherthanthat,allinputtextswereleftunchanged.\nFrom the true positives (TP), true negatives (TN), false positives (FP) and false\nnegatives(FN)obtainedbyeachmodel,wecomputedprecision,recall,F1andaccuracy\nscoresasfollows[Powers,2011].\nPrecision= TP\nTP +FP\nRecall= TP\nTP +FN\nF1= 2∗TP\n2∗TP +FP +FN\ndos Santos V.G., Paraboni I.: Myers-Briggs personality classification ... 385\nAccuracy= TP +TN\nTP +TN +FP +FN\n4 Results\nTable3summarisesresultsobtainedbythemodelsdiscussedintheprevioussections,\nand also from a majority class baseline. All results were obtained by performing 10-\nfoldcross-validation.Forbrevity,inwhatfollowsweonlypresentthemeanF1scores\nobtained by each model. For the full results (i.e., precision, recall, F1 and accuracy\nscores)wereporttoTable7attheendofthisarticle.\nTask Model En De Sp Fr It Du Pt\nEI\nMajority 0.43 0.40 0.35 0.40 0.41 0.40 0.35\nReg.char 0.51 0.60 0.58 0.59 0.65 0.61 0.65\nReg.word 0.54 0.60 0.58 0.59 0.63 0.62 0.64\nLSTM 0.83 0.73 0.71 0.72 0.80 0.82 0.80\nBERT 0.94 0.90 0.86 0.89 0.95 0.88 0.93\nNS\nMajority 0.46 0.45 0.39 0.44 0.45 0.42 0.43\nReg.char 0.51 0.58 0.58 0.56 0.63 0.60 0.62\nReg.word 0.54 0.57 0.58 0.56 0.64 0.63 0.61\nLSTM 0.82 0.75 0.68 0.74 0.790.82 0.79\nBERT 0.91 0.90 0.83 0.87 0.89 0.73 0.75\nTF\nMajority 0.39 0.37 0.38 0.37 0.34 0.40 0.35\nReg.char 0.62 0.58 0.57 0.55 0.59 0.61 0.58\nReg.word 0.65 0.58 0.57 0.56 0.59 0.61 0.58\nLSTM 0.82 0.73 0.69 0.72 0.78 0.81 0.81\nBERT 0.89 0.91 0.89 0.88 0.93 0.95 0.96\nPJ\nMajority 0.36 0.38 0.37 0.38 0.37 0.39 0.35\nReg.char 0.57 0.57 0.58 0.56 0.62 0.59 0.57\nReg.word 0.60 0.58 0.57 0.56 0.61 0.60 0.58\nLSTM 0.82 0.72 0.69 0.70 0.78 0.80 0.79\nBERT 0.91 0.86 0.83 0.74 0.93 0.91 0.94\nTable 3: 10-fold cross-validation mean F1 results for English (En), German (De),\nSpanish (Sp), French (Fr), Italian (It), Dutch (Du), and Portuguese (Pt) data. Best F1\nresults for each class and language are highlighted.\nResultsfromTable3showthatBERTgenerallyoutperformsthealternativesinall\nbuttwocases-theNStaskinDutch(Du)andPortuguese(Pt)-inwhichcasetheLSTM\nmodelobtainedoverallbestresults.AccordingtoaMcNemartest[McNemar,1947],all\ndifferencesbetweenBERTandLSTMarestatisticallysignificantat p <0.001 except\nfortheNStaskinItalian(It),inwhichcasethedifferenceissignificantat p <0.005\nonly. This outcome offers support to our main research question, that is, the use of\npre-trainedlanguagemodelsforMBTIpersonalityclassificationoutperformsstandard\ntextclassificationmethodsbasedonbag-of-wordsandstaticwordembeddingsalike.\n386 dos Santos V.G., Paraboni I.: Myers-Briggs personality classification ...\nAsdiscussedinSection3,thepresentexperimentscouldnotuseexactlythesame\ndatasetsasinpreviousworkduetotheremovalofsocialmediatextsovertimeand,asa\nresult,adirectcomparisonisnotentirelypossible.Bearingthislimitationinmind,Table\n4presents-purelyforillustrationpurposes-meanF1resultsreportedin[Gjurkovićand\nŠnajder,2018]and[Wuet al.,2020]fortheEnglishMBTI9kcorpusalongsidethose\nobtainedbyourpresentBERTmodel.Similarly,Table5presentsweightedF1results\nreportedinboth[benVerhoevenet al.,2016]and[AlsadhanandSkillicorn,2017]for\ntheTwiStycorpusalongsidepresentBERT.\nTask Gjurković&Šnajder Wuet.al. Current(BERT)\nEI 0.83 0.69 0.94\nNS 0.79 0.77 0.91\nTF 0.64 0.68 0.89\nPJ 0.74 0.61 0.91\nTable 4: MBTI9k mean F1 results from previous work [Gjurković and Šnajder, 2018]\nand [Wu et al., 2020], and from the present BERT models.\ndos Santos V.G., Paraboni I.: Myers-Briggs personality classification ... 387\nLang. Task Verhoeven Alsadhan Current(BERT)\nDe\nEI 0.72 0.76 0.77\nNS 0.74 0.78 0.93\nTF 0.59 0.78 0.87\nPJ 0.62 0.80 0.92\nSp\nEI 0.61 0.72 0.84\nNS 0.62 0.73 0.91\nTF 0.60 0.72 0.79\nPJ 0.56 0.69 0.88\nFr\nEI 0.66 0.86 0.78\nNS 0.79 0.96 0.92\nTF 0.58 0.74 0.81\nPJ 0.57 0.84 0.86\nIt\nEI 0.78 0.90 0.88\nNS 0.79 0.67 0.95\nTF 0.52 0.83 0.93\nPJ 0.47 0.79 0.91\nDu\nEI 0.63 0.85 0.94\nNS 0.70 0.94 0.97\nTF 0.60 0.82 0.91\nPJ 0.58 0.87 0.94\nPt\nEI 0.67 0.85 0.92\nNS 0.73 0.94 0.93\nTF 0.62 0.80 0.94\nPJ 0.57 0.88 0.95\nTable 5: TwiSty weighted F1 results from previous work [ben Verhoeven et al., 2016]\nand [Alsadhan and Skillicorn, 2017], and from the present BERT models for the\nGerman (De), Spanish (Sp), French (Fr), Italian (It), Dutch (Du), and Portuguese (Pt)\nlanguages.\nAsameanstoillustratethemostrelevantwordfeaturesforeachtask,weperformed\neli5predictionexplanation 5 tocomputethetermsmorestronglycorrelatedwitheach\nclass,usingasanexampletheword-based Reg.word classifierandtheEnglish MBTI9k\ndataset.This,despitebeingoutperformedbyourmainBERTmodels,ismoresuitableto\ninterpretation.\nSelected features are illustrated in Table 6, in which word weights represent the\nchange(decrease/increase)oftheevaluationscorewhenthespecificfeatureisshuffled,\nkeeping in mind that MBTI classes are not independent and, due to class imbalance,\nwordsthatwouldintuitivelybemoreassociatedwithaparticularMBTItypemayhave\nbeenselectedbyassociationwithanother,concomitanttype(e.g.,ifuserslabelledas\nextravertsalsohappentobemostlyofthethinkingtypeetc.)\n5 https://eli5.readthedocs.io/en/latest/\n388 dos Santos V.G., Paraboni I.: Myers-Briggs personality classification ...\nWeight EI Weight NS Weight TF Weight PJ\n+0.113 job +0.051 yourself +0.035 thank +0.051 comcast\n+0.097 may +0.047 after +0.033 baby +0.036 25b2\n+0.090 never +0.047 job +0.030 amazing +0.034 now\n+0.086 them +0.046 trans +0.027 still +0.033 nice\n+0.084 free +0.046 up +0.025 two +0.033 story\n+0.081 im +0.046 etc +0.025 pregnancy +0.032 awesome\n+0.080 aren +0.045 than +0.025 actually +0.032 always\n+0.076 10 +0.044 all +0.024 team +0.029 others\n+0.073 couple +0.043 end +0.024 from +0.029 isn\n+0.073 wiki +0.041 know +0.023 these +0.029 game\n... ... ... ... ... ... ... ...\n-0.058 comcast -0.030 honestly -0.018 how -0.022 let\n-0.058 temple -0.030 without -0.018 fucking -0.022 well\n-0.059 death -0.030 run -0.018 ve -0.022 nothing\n-0.061 est -0.030 help -0.018 lol -0.023 soylent\n-0.062 city -0.031 amazing -0.018 point -0.023 edit\n-0.062 vs -0.031 running -0.019 something -0.023 still\n-0.062 week -0.031 totally -0.019 mind -0.023 sex\n-0.062 usually -0.032 thing -0.019 op -0.023 completely\n-0.062 own -0.032 won -0.020 lt -0.023 lmao\n-0.062 album -0.032 which -0.020 female -0.024 clinton\nTable 6: Top-10 positive and negative word weights for each classification task using\nthe Reg.word logistic regression classifier.\nFinally,thefollowingtextexamplesillustratecorrectlyclassifiedinstancesforeach\nMBTItype,inwhichthemostrelevant(word)featuresarehighlighted.\nIn Figure 2 Extraversion correlates positively with ’team’, and negatively with\n’technology’,whereasIntroversioncorrelatespositivelywith‘game’andnegativelywith\n‘dancing’.\nFigure 2: Extraversion (top) and introversion (bottom) features.\nInFigure3,Intuitioncorrelatespositivelywith‘people’andnegativelywith‘because’\n(whichsuggestsreasoning),whereasSensingstronglycorrelateswith‘because’andmore\nconcreteconcepts(e.g.,cars,school,kidsetc.)\ndos Santos V.G., Paraboni I.: Myers-Briggs personality classification ... 389\nFigure 3: Intuition (top) and Sensing (bottom) features.\nIn Figure 4, Thinking correlates positively with concrete concepts (e.g., model,\nengineer)andnegativelywithsentiment-chargedwords(e.g.,‘like’,‘love’.)Bycontrast,\nFeelingcorrelatespositivelywithsentiment,andnegativelywith‘would’(whichmight\nsuggestreasoning.)\nFigure 4: Thinking (top) and Feeling (bottom) features.\nFinally,inFigure5,Perceivingcorrelateswithacertainpreferencetotakeininfor-\nmation(e.g.,‘inform’),andJudgingcorrelatespositivelywithdecision-makingterms\n(e.g.,‘work’).\nFigure 5: Perceiving (top) and Judging (bottom) features.\nTheseexamples,althoughsuboptimalforthereasonsdiscussedabove,inourview\nsuggest a reasonable consistency with the MBTI guidelines. Examples in which the\nlogisticregressionclassifierdoesnotmaketherightdecision,bycontrast,include,the\nselectionof’party’asaprominentfeatureforExtraversionevenwhenthetermisused\ninitspolitical(andnotleisure)sense.InthecaseofourBERTmodel,however,errors\nofthiskindarearguablylesslikelytooccurgiventhemodel’scontextsensitivity.\n5 Final remarks\nThisworkhasaddressedtheissueofMBTIpersonalityclassificationfromtextwiththe\naid of pre-trained BERT language models. The present approach has been compared\n390 dos Santos V.G., Paraboni I.: Myers-Briggs personality classification ...\nagainstalternativesbasedonbag-of-wordsandstaticwordembeddingsrepresentations,\nanditsresultswerefoundtobeconsistentlysuperiorinanumberofevaluationscenar-\nios involving multiple target languages in the Reddit and Twitter domains, and by a\nsignificantmargin.\nDespitethepositiveinitialresults,however,thecurrentsetofexperimentsisonlya\nfirststeptowardsmoregeneral,domain-independentMBTIpersonalityclassification\nfromtext.Oneobviouslimitationofthepresentapproachis,forinstance,thefocuson\nonlytwoMBTIlanguageresources(namely,theMBTI9k[GjurkovićandŠnajder,2018]\nandTwiSty[benVerhoevenet al.,2016]corpora.)Althoughcoveringsevenlanguages\nintwolinguisticdomains,moreworkneedsbedonetoinvestigatethepresenttaskin\nothertextgenres.\nFurthermore,wenoticethatthepresentdiscussionhasbeenlimitedtotheissueof\nfine-tuning BERT models for the MBTI classification task for which training data is\nreadilyavailable.OutsidethepresentTwitterandRedditdomains,however,textcorpora\nlabelled with MBTI information may be scarce, and it may be necessary to resort do\ndomainadaptationmethods.Thesemayinclude,forinstance,theuseofBERT-based\nadversarialadaptationwithdistillation(AAD)methodproposedin[RyuandLee,2020]\nfor cross-domain sentiment analysis, among others. A study along these lines in the\ncontextofthepresentpersonalityclassificationtaskisalsoleftasfuturework.\nFinally,wenoticethatinrecentyearstherehasbeenasurgeintransformer-based\nlanguage models, including ELMo [Peters et al., 2017], XLNet [Yang et al., 2019],\nRoBERTa [Liu et al., 2019], GPT-3 [Brown et al., 2020], and many others. In most\ncases,thesemodelsareyettobeappliedtothepresentMBTIclassificationtask.\nAcknowledgements\nThisworkhasreceivedsupportfromtheUniversityofSãoPaulo.\ndos Santos V.G., Paraboni I.: Myers-Briggs personality classification ... 391\nEnglish German Spanish French Italian Dutch Portuguese\nTask ModelAcc P R F1 Acc P R F1 Acc P R F1 Acc P R F1 Acc P R F1 Acc P R F1 Acc P R F1\nEI\nMajority0.77 0.77 0.77 0.430.66 0.66 0.66 0.400.53 0.53 0.53 0.350.65 0.65 0.65 0.400.69 0.69 0.69 0.410.67 0.67 0.67 0.400.53 0.53 0.53 0.35\nReg.char0.56 0.55 0.26 0.510.63 0.49 0.46 0.600.59 0.57 0.56 0.580.63 0.46 0.46 0.590.70 0.42 0.52 0.650.68 0.80 0.75 0.610.65 0.58 0.63 0.65\nReg.word0.54 0.58 0.26 0.540.63 0.52 0.46 0.600.58 0.61 0.54 0.580.64 0.37 0.46 0.590.71 0.47 0.55 0.630.68 0.82 0.73 0.620.66 0.57 0.65 0.64\nLSTM 0.99 0.98 0.98 0.830.80 0.56 0.79 0.730.72 0.66 0.72 0.710.96 0.48 0.80 0.720.88 0.72 0.87 0.800.88 0.96 0.88 0.820.88 0.86 0.87 0.80\nBERT 0.87 0.67 0.690.940.86 0.69 0.860.900.85 0.82 0.850.860.86 0.72 0.840.890.93 0.84 0.930.950.92 0.96 0.930.880.92 0.91 0.920.93\nNS\nMajority0.86 0.86 0.86 0.460.83 0.83 0.83 0.450.64 0.64 0.64 0.390.77 0.77 0.77 0.440.80 0.80 0.80 0.450.73 0.73 0.73 0.420.74 0.74 0.74 0.43\nReg.char0.63 0.64 0.90 0.510.75 0.84 0.86 0.580.61 0.66 0.71 0.580.74 0.89 0.79 0.560.81 0.93 0.85 0.630.71 0.80 0.80 0.600.68 0.73 0.81 0.62\nReg.word0.66 0.70 0.89 0.540.78 0.89 0.85 0.570.61 0.69 0.69 0.580.73 0.87 0.80 0.560.76 0.84 0.86 0.640.70 0.81 0.79 0.630.68 0.72 0.82 0.61\nLSTM 0.99 0.99 0.99 0.820.85 0.90 0.85 0.750.76 0.93 0.75 0.680.85 0.97 0.85 0.740.92 0.96 0.93 0.790.90 0.95 0.910.820.91 0.96 0.920.79\nBERT 0.95 0.97 0.950.910.92 0.94 0.930.900.88 0.93 0.890.830.90 0.93 0.900.870.93 0.98 0.920.890.96 0.96 0.98 0.730.89 0.97 0.89 0.75\nTF\nMajority0.64 0.64 0.64 0.390.58 0.66 0.66 0.370.61 0.53 0.53 0.380.59 0.59 0.59 0.370.51 0.51 0.51 0.340.65 0.65 0.65 0.400.54 0.54 0.54 0.35\nReg.char0.64 0.66 0.75 0.620.59 0.61 0.50 0.580.59 0.49 0.47 0.570.59 0.41 0.50 0.550.60 0.73 0.59 0.590.65 0.49 0.49 0.610.64 0.68 0.58 0.58\nReg.word0.65 0.65 0.76 0.650.58 0.58 0.50 0.580.60 0.45 0.47 0.570.58 0.38 0.48 0.560.61 0.71 0.60 0.590.66 0.48 0.50 0.610.64 0.68 0.59 0.58\nLSTM 0.99 0.99 0.99 0.820.77 0.63 0.77 0.730.75 0.47 0.80 0.690.79 0.51 0.97 0.720.84 0.85 0.84 0.780.87 0.76 0.85 0.810.88 0.86 0.88 0.81\nBERT 0.93 0.95 0.940.890.89 0.86 0.880.910.85 0.75 0.830.890.85 0.78 0.850.880.93 0.94 0.930.930.94 0.89 0.920.950.95 0.94 0.940.96\nPJ\nMajority0.58 0.58 0.58 0.360.62 0.62 0.62 0.380.57 0.57 0.57 0.370.61 0.61 0.61 0.380.57 0.57 0.57 0.370.63 0.63 0.63 0.390.53 0.53 0.53 0.35\nReg.char0.57 0.54 0.65 0.570.60 0.65 0.69 0.570.59 0.66 0.64 0.580.61 0.76 0.66 0.560.62 0.57 0.55 0.620.65 0.80 0.69 0.590.58 0.64 0.61 0.57\nReg.word0.59 0.61 0.66 0.600.61 0.73 0.67 0.580.59 0.65 0.64 0.570.60 0.74 0.65 0.560.63 0.49 0.58 0.610.64 0.80 0.69 0.600.60 0.66 0.62 0.58\nLSTM 0.99 0.99 0.99 0.820.77 0.92 0.76 0.730.73 0.90 0.71 0.690.76 0.93 0.74 0.720.84 0.78 0.84 0.780.86 0.93 0.86 0.810.87 0.90 0.87 0.81\nBERT 0.91 0.92 0.930.910.90 0.93 0.900.860.86 0.90 0.860.830.82 0.91 0.820.740.93 0.91 0.920.930.93 0.95 0.940.910.94 0.95 0.940.94\nTable 7: 10-fold cross-validation mean accuracy (Acc), precision (P), recall (R) and F1 results. Best F1 results for each class and\nlanguage are highlighted.\n392 dos Santos V.G., Paraboni I.: Myers-Briggs personality classification ...\nReferences\n[Abidinet al.,2020] Abidin,N.H. Z.,Remli,M. A.,Ali,N. M.,Phon,D.N. E.,Yusoff,N.,Adli,\nH. K.,and ,A.H. B.(2020). “Improvingintelligentpersonalitypredictionusingmyers-briggs\ntypeindicatorandrandomforestclassifier,” InternationalJournalofAdvancedComputerScience\nandApplications,11(11).\n[AllportandAllport,1921] Allport,F. H.andAllport,G. W.(1921). “Personalitytraits:Their\nclassificationandmeasurement,” JournalofAbnormalAndSocialPsychology,16:6–40.\n[AlsadhanandSkillicorn,2017] Alsadhan,N.andSkillicorn,D.(2017). “Estimatingpersonality\nfromsocialmediaposts,” in2017IEEEInternationalConferenceonDataMiningWorkshops\n(ICDMW),pages350–356.\n[AmirhosseiniandKazemian,2020] Amirhosseini,M. H.andKazemian,H.(2020). “Machine\nlearning approach to personality type prediction based on the Myers-Briggs type indicator,”\nMultimodalTechnologiesandInteraction,4(1).\n[Ashrafet al.,2020] Ashraf, M. A., Nawab, R. M. A., and Nie, F. (2020). “A study of deep\nlearningmethodsforsame-genreandcross-genreauthorprofiling,” JournalofIntelligent&Fuzzy\nSystems,39:2353–2363.\n[BarlasandStamatatos,2020] Barlas,G.andStamatatos,E.(2020). “Cross-domainauthorship\nattributionusing pre-trainedlanguagemodels,” inMaglogiannis, I.,Iliadis, L.,andPimenidis,\nE.,editors, ArtificialIntelligenceApplicationsandInnovations,pages255–266,Cham.Springer\nInternationalPublishing.\n[benVerhoevenet al.,2016] benVerhoeven,Daelemans,W.,andPlank,B.(2016). “TwiSty:A\nmultilingualtwitterstylometrycorpusforgenderandpersonalityprofiling,” inProceedingsof\ntheTenthInternationalConferenceonLanguageResourcesandEvaluation(LREC’16),pages\n1632–1637,Portorož,Slovenia.EuropeanLanguageResourcesAssociation(ELRA).\n[Bird,2006] Bird, S. (2006). “NLTK: the natural language toolkit,” in Proceedings of the\nCOLING/ACL2006InteractivePresentationSessions,pages69–72.\n[Bleiet al.,2003] Blei,D. M.,Ng,A. Y.,andJordan,M. I.(2003). “LatentDirichletAllocation,”\nJournalofMachineLearningResearch,3(4-5):993–1022.\n[Brownet al.,2020] Brown,T. B.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.,Dhariwal,P.,\nNeelakantan,A.,Shyam,P.,Sastry,G.,Askell,A.,Agarwal,S.,Herbert-Voss,A.,Krueger,G.,\nHenighan,T.,Child,R.,Ramesh,A.,Ziegler,D. M.,Wu,J.,Winter,C.,Hesse,C.,Chen,M.,Sigler,\nE.,Litwin,M.,Gray,S.,Chess,B.,Clark,J.,Berner,C.,McCandlish,S.,Radford,A.,Sutskever,\nI.,andAmodei,D.(2020). “Languagemodelsarefew-shotlearners,” CoRR,abs/2005.14165.\n[ChenandGuestrin,2016] Chen,T.andGuestrin,C.(2016). “XGBoost:AScalableTreeBoost-\ningSystem,” in22ndACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandData\nMining,KDD’16,pages785–794,NewYork,NY,USA.AssociationforComputingMachinery.\n[Coltheart,1981] Coltheart,M.(1981). “TheMRCpsycholinguisticdatabase,” TheQuarterly\nJournalofExperimentalPsychologySectionA:HumanExperimentalPsychology,33(4):497–505.\n[DasandPrajapati,2020] Das,K.andPrajapati,H.(2020). “Personalityidentificationbasedon\nMBTIdimensionsusingnaturallanguageprocessing,” InternationalJournalofCreativeresearch\nThoughts,8(6):1653–1657.\n[DelmondesNetoandParaboni,2021] Delmondes Neto, J. P. and Paraboni, I. (2021). “Multi-\nsourceBERTstackensembleforcross-domainauthorprofiling,” ExpertSystems,e12869.\ndos Santos V.G., Paraboni I.: Myers-Briggs personality classification ... 393\n[Devlinet al.,2019] Devlin, J., Chang, M., Lee, K., and Toutanova, K. (2019). “BERT: pre-\ntrainingofdeepbidirectionaltransformersforlanguageunderstanding,” inBurstein,J.,Doran,C.,\nandSolorio,T.,editors, Proceedingsofthe2019ConferenceoftheNorthAmericanChapterofthe\nAssociationforComputationalLinguistics:HumanLanguageTechnologies,NAACL-HLT2019,\nMinneapolis,MN,USA,June2-7,2019,Volume1(LongandShortPapers),pages4171–4186.\nAssociationforComputationalLinguistics.\n[dosSantoset al.,2017] dosSantos,V. G.,Paraboni,I.,andSilva,B.B. C.(2017). “Bigfive\npersonalityrecognitionfrommultipletextgenres,” inText,SpeechandDialogue(TSD-2017)\nLectureNotesinArtificialIntelligencevol.10415,pages29–37,Prague,CzechRepublic.Springer-\nVerlag.\n[dosSantoset al.,2020] dos Santos, W. R., Funabashi, A. M. M., and Paraboni, I. (2020).\n“Searching Brazilian Twitter for signs of mental health issues,” in 12th International Confer-\nenceonLanguageResourcesandEvaluation(LREC-2020),pages6113–6119,Marseille,France.\nELRA.\n[dosSantosandParaboni,2019] dosSantos,W. R.andParaboni,I.(2019). “MoralStanceRecog-\nnitionandPolarityClassificationfromTwitterandElicitedText,” inRecentsAdvancesinNatural\nLanguageProcessing(RANLP-2019),pages1069–1075,Varna,Bulgaria.INCOMALtd.\n[dosSantoset al.,2019] dosSantos,W. R.,Ramos,R.M. S.,andParaboni,I.(2019). “Compu-\ntationalpersonalityrecognitionfromfacebooktext:psycholinguisticfeatures,wordsandfacets,”\nNewReviewofHypermediaandMultimedia,25(4):268–287.\n[Escobar-Grisaleset al.,2021] Escobar-Grisales, D., Vásquez-Correa, J. C., and Orozco-\nArroyave, J. R. (2021). “Gender recognition in informal and formal language scenarios via\ntransferlearning,” CoRR,abs/2107.02759.\n[GjurkovićandŠnajder,2018] Gjurković,M.andŠnajder,J.(2018). “Reddit:Agoldminefor\npersonalityprediction,” inSecondWorkshoponComputationalModelingofPeople’sOpinions,\nPersonality, and Emotions in Social Media, pages 87–97, New Orleans, USA. Association for\nComputationalLinguistics.\n[Goldberg,1990] Goldberg,L. R.(1990). “Analternativedescriptionofpersonality:TheBig-\nFivefactorstructure,” JournalofPersonalityandSocialPsychology,59:1216–1229.\n[Hoanget al.,2019] Hoang,M.,Bihorac,O. A.,andRouces,J.(2019). “Aspect-basedsentiment\nanalysisusingBERT,” in22ndNordicConferenceonComputationalLinguistics,pages187–196,\nTurku,Finland.LinköpingUniversityElectronicPress.\n[Johnet al.,1991] John,O. P.,Donahue,E.,andKentle,R.(1991). “TheBigFiveinventory-\nversions4aand54,” Technicalreport,Inst.PersonalitySocialResearch,UniversityofCalifornia,\nBerkeley,CA,USA.\n[Katiyaret al.,2020] Katiyar,S.,Kumar,S.,andWalia,H.(2020). “Personalitypredictionfrom\nstackoverflowbyusingnaivebayestheoremindatamining,” InternationalJournalofInnovative\nTechnologyandExploringEngineering(IJITEE),9.\n[KehandCheng,2019] Keh,S. S.andCheng,I.(2019). “Myers-Briggspersonalityclassifica-\ntion and personality-specific language generation using pre-trained language models,” CoRR,\nabs/1907.06333.\n[Khanet al.,2020] Khan,A. S.,Ahmad,H.,Asghar,M. Z.,Saddozai,F. K.,Arif,A.,andKhalid,\nH. A. (2020). “Personality classification from online text using machine learning approach,”\nInternationalJournalofAdvancedComputerScienceandApplications,11(3):460–476.\n[Liuet al.,2017] Liu,F.,Perez,J.,andNowson,S.(2017). “Alanguage-independentandcom-\npositional model for personality trait recognition from short texts,” in 15th Conference of the\nEuropeanChapteroftheAssociationforComputationalLinguistics,pages754–764,Valencia,\nSpain.AssociationforComputationalLinguistics.\n394 dos Santos V.G., Paraboni I.: Myers-Briggs personality classification ...\n[Liuet al.,2019] Liu,Y.,Ott,M.,Goyal,N.,Du,J.,Joshi,M.,Chen,D.,Levy,O.,Lewis,M.,\nZettlemoyer,L.,andStoyanov,V.(2019). “RoBERTa:ARobustlyOptimizedBERTPretraining\nApproach,” CoRR,abs/1907.11692.\n[López-Santillet.al.,2020] López-Santillet.al.,R.(2020). “Richerdocumentembeddingsfor\nauthorprofilingtasksbasedonaheuristicsearch,” InformationProcessing&Management,57(4).\n[Lukitoet al.,2016] Lukito, L. C., Erwin, A., Purnama, J., and Danoekoesoemo, W. (2016).\n“Socialmediauserpersonalityclassificationusingcomputationallinguistic,” in8thInternational\nConferenceonInformationTechnologyandElectricalEngineering(ICITEE),pages1–6.\n[McNemar,1947] McNemar,Q.(1947). “Noteonthesamplingerrorofthedifferencebetween\ncorrelatedproportionsorpercentages,” Psychometrika,12(2):153–157.\n[Mikolovet al.,2013] Mikolov,T.,Wen-tau,S.,andZweig,G.(2013). “Linguisticregularitiesin\ncontinuousspacewordrepresentations,” inProc.ofNAACL-HLT-2013,pages746–751,Atlanta,\nUSA.AssociationforComputationalLinguistics.\n[Myers,1962] Myers,I. B.(1962). “TheMyers-Briggstypeindicator,” ConsultingPsychologists\nPress.\n[Pavanet al.,2020] Pavan,M. C.,dosSantos,W. R.,andParaboni,I.(2020). “TwitterMoral\nStanceClassificationusingLongShort-TermMemoryNetworks,” in9thBrazilianConferenceon\nIntelligentSystems(BRACIS).LNAI12319,pages636–647.Springer.\n[Pennebakeret al.,2001] Pennebaker,J. W.,Francis,M. E.,andBooth,R. J.(2001). “Inquiry\nandWordCount:LIWC,” LawrenceErlbaum,Mahwah,NJ.\n[Peterset al.,2017] Peters,M. E.,Ammar,W.,Bhagavatula,C.,andPower,R.(2017). “Semi-\nsupervisedsequencetaggingwithbidirectionallanguagemodels,” inProc.ofACL-2017,pages\n1756–1765,Vancouver,Canada.AssociationforComputationalLinguistics.\n[PlankandHovy,2015] Plank,B.andHovy,D.(2015). “PersonalitytraitsonTwitter—or—How\ntoget1,500personalitytestsinaweek,” in6thWorkshoponComputationalApproachestoSubjec-\ntivity,SentimentandSocialMediaAnalysis,pages92–98,Lisbon.AssociationforComputational\nLinguistics.\n[Polignanoet al.,2020] Polignano,M.,de Gemmis,M.,andSemeraro,G.(2020). “Contextual-\nizedBERTsentenceembeddingsforauthorprofiling:Thecostofperformances,” inComputational\nScienceandItsApplications(ICCSA)-2020,LNCS12252,pages135–149,Cham.Springer.\n[Powers,2011] Powers,D.M. W.(2011). “Evaluation:FromPrecision,RecallandF-Measure\ntoROC,Informedness,Markedness&Correlation,” JournalofMachineLearningTechnologies,\n2(1):37–63.\n[PriceandHodge,2020] Price,S.andHodge,A.(2020). “Celebrityprofilingusingtwitterfol-\nlowerfeeds,” inWorkingNotesofCLEF2020-ConferenceandLabsoftheEvaluationForum,\nThessaloniki,Greece.CLEFandCEUR-WS.org.\n[Rangelet al.,2020] Rangel,F.,Rosso,P.,Zaghouani,W.,andCharfi,A.(2020). “Fine-grained\nanalysisoflanguagevarietiesanddemographics,” NaturalLanguageEngineering,page1-21.\n[RyuandLee,2020] Ryu,M.andLee,K.(2020). “KnowledgedistillationforBERTunsuper-\nviseddomainadaptation,” CoRR,abs/2010.11478.\n[Sanhet al.,2019] Sanh,V.,Debut,L.,Chaumond,J.,andWolf,T.(2019). “Distilbert,adistilled\nversionofbert:smaller,faster,cheaperandlighter,” arXivpreprintarXiv:1910.01108.\n[SilvaandParaboni,2018a] Silva,B.B. C.andParaboni,I.(2018a). “Learningpersonalitytraits\nfromFacebooktext,” IEEELatinAmericaTransactions,16(4):1256–1262.\n[SilvaandParaboni,2018b] Silva,B.B. C.andParaboni,I.(2018b). “Personalityrecognition\nfrom Facebook text,” in 13th International Conference on the Computational Processing of\nPortuguese(PROPOR-2018)LNCSvol.11122,pages107–114,Canela.Springer-Verlag.\ndos Santos V.G., Paraboni I.: Myers-Briggs personality classification ... 395\n[Teixeiraet al.,2014] Teixeira,C.V. M.,Paraboni,I.,da Silva,A.S. R.,andYamasaki,A. K.\n(2014). “Generatingrelationaldescriptionsinvolvingmutualdisambiguation,” inComputational\nLinguisticsandIntelligentTextProcessing(CICLing-2014),LectureNotesinComputerScience\n8403,pages492–502,Kathmandu,Nepal.Springer.\n[Wuet al.,2020] Wu, X., Lin, W., Wang, Z., and Rastorgueva, E. (2020). “Author2vec: A\nframeworkforgeneratinguserembedding,” arXivpreprintarXiv:2003.11627.\n[Yanget al.,2019] Yang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov, R., and Le,\nQ. V.(2019). “XLNet:GeneralizedAutoregressivePretrainingforLanguageUnderstanding,” in\n33rdConferenceonNeuralInformationProcessingSystems(NeurIPS2019),volume 32,pages\n5753–5763,Vancouver,Canada.CurranAssociates,Inc.\n[Zhangandv Wang,2018] Zhang,L.andv Wang,a.B. L.(2018). “Deeplearningforsentiment\nanalysis:Asurvey,” WIREsDataMiningandKnowledgeDiscovery,8(4):e1253.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8663153648376465
    },
    {
      "name": "Natural language processing",
      "score": 0.6317149996757507
    },
    {
      "name": "Transformer",
      "score": 0.5780037045478821
    },
    {
      "name": "Personality",
      "score": 0.5437662601470947
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5324248671531677
    },
    {
      "name": "Encoder",
      "score": 0.5123025178909302
    },
    {
      "name": "Identification (biology)",
      "score": 0.4951569139957428
    },
    {
      "name": "Field (mathematics)",
      "score": 0.48531574010849
    },
    {
      "name": "Language model",
      "score": 0.47501787543296814
    },
    {
      "name": "Word (group theory)",
      "score": 0.41978174448013306
    },
    {
      "name": "Linguistics",
      "score": 0.21748989820480347
    },
    {
      "name": "Psychology",
      "score": 0.09233534336090088
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I17974374",
      "name": "Universidade de São Paulo",
      "country": "BR"
    }
  ],
  "cited_by": 10
}