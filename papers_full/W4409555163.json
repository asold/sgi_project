{
  "title": "Generating textual explanations for scheduling systems leveraging the reasoning capabilities of large language models",
  "url": "https://openalex.org/W4409555163",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A3150736024",
      "name": "Cheyenne Powell",
      "affiliations": [
        "University of Strathclyde"
      ]
    },
    {
      "id": "https://openalex.org/A2162273432",
      "name": "Annalisa Riccardi",
      "affiliations": [
        "University of Strathclyde"
      ]
    },
    {
      "id": "https://openalex.org/A3150736024",
      "name": "Cheyenne Powell",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2162273432",
      "name": "Annalisa Riccardi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4387682038",
    "https://openalex.org/W4366262984",
    "https://openalex.org/W3217484302",
    "https://openalex.org/W3163565519",
    "https://openalex.org/W2981731882",
    "https://openalex.org/W4295995965",
    "https://openalex.org/W1787224781",
    "https://openalex.org/W4381713196",
    "https://openalex.org/W4327747413",
    "https://openalex.org/W4403137345",
    "https://openalex.org/W2077745719",
    "https://openalex.org/W4249805289",
    "https://openalex.org/W3040802234",
    "https://openalex.org/W4327657251",
    "https://openalex.org/W3177471157",
    "https://openalex.org/W4324302616",
    "https://openalex.org/W4410324978",
    "https://openalex.org/W2486648655",
    "https://openalex.org/W1838487097",
    "https://openalex.org/W4296634084",
    "https://openalex.org/W4392011713",
    "https://openalex.org/W4281386995",
    "https://openalex.org/W4322619929",
    "https://openalex.org/W4298181573",
    "https://openalex.org/W6948473772",
    "https://openalex.org/W4400324908",
    "https://openalex.org/W4323022381",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W2076785727",
    "https://openalex.org/W4399049946",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W3193042250",
    "https://openalex.org/W4320476369",
    "https://openalex.org/W4226139013",
    "https://openalex.org/W4380365974",
    "https://openalex.org/W3191803444",
    "https://openalex.org/W2973136764",
    "https://openalex.org/W2565851282",
    "https://openalex.org/W3009778389",
    "https://openalex.org/W4317940206",
    "https://openalex.org/W4312255565",
    "https://openalex.org/W3176986811",
    "https://openalex.org/W4399570858",
    "https://openalex.org/W6993078705",
    "https://openalex.org/W4360847209",
    "https://openalex.org/W3082925502",
    "https://openalex.org/W3046162336",
    "https://openalex.org/W4400484653",
    "https://openalex.org/W3211639647",
    "https://openalex.org/W4396792450",
    "https://openalex.org/W4308760226",
    "https://openalex.org/W4379919185",
    "https://openalex.org/W4290771878",
    "https://openalex.org/W4289666065",
    "https://openalex.org/W4389165057",
    "https://openalex.org/W4205534337",
    "https://openalex.org/W4392366650",
    "https://openalex.org/W2156391157",
    "https://openalex.org/W4353072268",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W3003107055",
    "https://openalex.org/W4405916586",
    "https://openalex.org/W4288263260",
    "https://openalex.org/W4392866835",
    "https://openalex.org/W4367595583",
    "https://openalex.org/W4210714644",
    "https://openalex.org/W4392240262",
    "https://openalex.org/W4405124153",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W2976264609",
    "https://openalex.org/W4390011017",
    "https://openalex.org/W4377865168",
    "https://openalex.org/W3013908988",
    "https://openalex.org/W3103342535",
    "https://openalex.org/W4311642023"
  ],
  "abstract": "Abstract Scheduling systems are critical for planning projects, resources, and activities across many industries to achieve goals efficiently. As scheduling requirements grow in complexity, the use of Artificial Intelligence (AI) solutions has received more attention. However, providing comprehensible explanations of these decision-making processes remains a challenge and blocker to adoption. The emergent field of eXplainable Artificial Intelligence (XAI) aims to address this by establishing human-centric interpretation of influencing factors for machine decisions. The leading field of autonomous interpretation in Natural Language Processing (NLP) is Large Language Model (LLM)s, for their generalist knowledge and reasoning capabilities. To explore LLMs’ potential to generate explanations for scheduling queries, we selected a benchmark set of Job Shop scheduling problems. A novel framework that integrates the selected language models, GPT-4 and Large Language Model Meta AI (LLaMA), into scheduling systems is introduced, facilitating human-like explanations to queries from different categories through few-shot learning. The explanations were analysed for accuracy, consistency, completeness, conciseness, and language across different scheduling problem sizes and complexities. The approach achieved an overall accuracy of 59% with GPT-4 and 35% with LLaMA, with minimal impact from the varied schedule sizes observed, proving the approach can handle different datasets and is performance scalable. Several responses demonstrated high comprehension of complex queries; however, response quality fluctuated due to the few-shot learning approach. This study establishes a baseline for measuring generalist LLM capabilities in handling explanations for autonomous scheduling systems, with promising results for an LLM providing XAI interactions to explain scheduling decisions.",
  "full_text": "Journal of Intelligent Information Systems (2025) 63:1287–1337\nhttps://doi.org/10.1007/s10844-025-00940-w\nRESEARCH\nGenerating textual explanations for scheduling systems\nleveraging the reasoning capabilities of large language\nmodels\nCheyenne Powell 1 · Annalisa Riccardi 1\nReceived: 21 November 2024 / Revised: 7 April 2025 / Accepted: 8 April 2025 /\nPublished online: 17 April 2025\n© The Author(s) 2025\nAbstract\nScheduling systems are critical for planning projects, resources, and activities across many\nindustries to achieve goals efﬁciently. As scheduling requirements grow in complexity, the\nuse of Artiﬁcial Intelligence (AI) solutions has received more attention. However, providing\ncomprehensible explanations of these decision-making processes remains a challenge and\nblocker to adoption. The emergent ﬁeld of eXplainable Artiﬁcial Intelligence (XAI) aims to\naddress this by establishing human-centric interpretation of inﬂuencing factors for machine\ndecisions. The leading ﬁeld of autonomous interpretation in Natural Language Processing\n(NLP) is Large Language Model (LLM)s, for their generalist knowledge and reasoning\ncapabilities. To explore LLMs’ potential to generate explanations for scheduling queries, we\nselected a benchmark set of Job Shop scheduling problems. A novel framework that inte-\ngrates the selected language models, GPT-4 and Large Language Model Meta AI (LLaMA),\ninto scheduling systems is introduced, facilitating human-like explanations to queries from\ndifferent categories through few-shot learning. The explanations were analysed for accuracy,\nconsistency, completeness, conciseness, and language across different scheduling problem\nsizes and complexities. The approach achieved an overall accuracy of 59% with GPT-4 and\n35% with LLaMA, with minimal impact from the varied schedule sizes observed, proving\nthe approach can handle different datasets and is performance scalable. Several responses\ndemonstrated high comprehension of complex queries; however, response quality ﬂuctuated\ndue to the few-shot learning approach. This study establishes a baseline for measuring gen-\neralist LLM capabilities in handling explanations for autonomous scheduling systems, with\npromising results for an LLM providing XAI interactions to explain scheduling decisions.\nKeywords Large language models · Natural language processing · Question answering\nsystems · Scheduling · Explainable artiﬁcial intelligence\nB Cheyenne Powell\ncheyenne.powell@strath.ac.uk\nAnnalisa Riccardi\nannalisa.riccardi@strath.ac.uk\n1 Mechanical and Aerospace Engineering, University of Strathclyde, 16 Richmond St.,\nGlasgow G1 1XQ, Scotland, UK\n123\nJournal of Intelligent Information Systems (2025) 63:1287–1337\n1 Introduction\nScheduling systems play a critical role across many industries, including transportation,\nhealthcare, and manufacturing (Atsmony & Mosheiov, 2022; Yao et al., 2020; Fikar & Hirsch,\n2017; Zhou et al., 2020). The efﬁcient allocation of resources and optimal sequencing of tasks\nis essential for achieving productivity, reducing costs, and improving overall operational per-\nformance (Zhou et al., 2020;A m e re ta l . ,2022; Moons et al., 2017). As the use of automated\nsystems has become increasingly common, this has led to greater use of AI Decision Making\nSystems (DMS)s to manage and improve scheduling capabilities for various purposes, such\nas construction planning, cloud computing maintenance, and medical treatments (Amer et al.,\n2021; Rjoub et al., 2021; Squires et al., 2022). DMS, in scheduling, refers to computational\nsystems that assist in making intelligent decisions and generating optimized schedules for\na given problem. These systems incorporate various algorithms, techniques, and models to\nanalyze scheduling constraints, objectives, and resources to generate optimal schedules.\nA 30 year study on automated scheduling within the construction industry found that by\n2021 most, if not all, schedules were still created through fully manual processes in real world\napplications. Examining over 1500 articles the studies highlighted issues on the accessibility\nof relevant data which impacts a model ability to learn and the limited capacity human oper-\nators have to validate existing approaches with real-world projects. The study recommended\nthe formalisation of methods and data, further extensive testing, and to integrate automated\nsystems to maximise potential beneﬁt (Amer et al., 2021).\nIn the space industry, a study into AI methodologies for the future of Low Earth Orbit\n(LEO) satellite arrays revealed the emergent capabilities for signal detection, network opti-\nmisation, and the potential for completely automated and robust systems (Al Homssi et al.,\n2024).\nAdditional research, targeting the optimisation (Goh et al., 2022) and functional advance-\nment (Herrmann & Schaub, 2023) of satellite scheduling, identiﬁed the beneﬁts in\nperformance and scalability of utilising automated systems but also highlighted the dedi-\ncated training required to achieve performance reduced effectiveness in ﬂexible or generalised\napplications and the integration of new data and concepts.\nThere has also been research reviewing the challenges in human operators managing more\nadvanced AI systems in time-sensitive missions as a result of the size of data being generated\nand assessed by the systems (Thangavel et al., 2023). Further concerns are present with\ntrust and transparency in autonomous system decisions and the factors that inﬂuence these\ndecisions, as the potential effect on mission outcomes can depend on the operator’s ability\nto validate results as accurate or whether they require modiﬁcation (Picard et al., 2021).\nMoreover, a study into the fairness, or perception thereof, of automated decision-making\nsystem for nurse shift and duty scheduling was conducted. The study found that staff were\nmuch more accepting of decisions when involved in the planning and advised that this be\nfactored into the operation of the model. There were issues highlighted, however, for potential\nbias applied by the model around need-based factors, such as family commitments, and these\nwould need to be captured and trained out, advising that great care must be taken when\nworking with automated systems that impact a person’s wellbeing (Uhde et al., 2020).\nThese challenges in managing and understanding AI systems and their decisions have\nled to the emergence of the Explainability measure of a system, which intends to provide\nexplanations on how and why decisions were made (Yang et al., 2024a). The means of\nestablishing ways for an autonomous system to provide explanations on the reasoning behind\nits decisions and outputs is a new ﬁeld of AI, known as XAI (Saeed & Omlin, 2023). Though\n123\n1288\nJournal of Intelligent Information Systems (2025) 63:1287–1337\na dedicated process, XAI is designed to capture and present information behind decisions\nand reasoning in formats such as natural language, example-based and graphical diagrams\n(Lai et al., 2021, 2023), for example to identify the most important factors contributing to\nhealthcare professional burnout by highlighting feature importance (Pillai et al., 2024). This\nprocess can be completed through a number of methods, categorised into data, model or\npost-hoc explainability approaches, including explanatory data analysis, joint prediction and\nexplanation, attribution methods, and knowledge extraction methods, to name only a few\n(Ali et al., 2023; Arrieta et al., 2020). Language use and quality are also inherently critical\nto how XAI performs and, therefore, is deeply connected with the ﬁeld of NLP.\nLanguage modelling is a fundamental task in NLP that aims at capturing the statistical\npatterns and structures within a given language. This approach, in its most common form,\ninvolves training a model to predict the next word or sequence of words in a sentence, using\nthe surrounding context as a guide. By learning these patterns, Language Models (LMs)\ncan generate coherent and contextually relevant text, complete sentences, and even perform\nvarious language-related tasks (Brown et al., 2020).\nLLMs, such as the Generative Pre-trained Transformer (GPT) series, have shown excep-\ntional proﬁciency in understanding and generating human language. LLMs, were trained on\nvast amounts of text data and employed deep learning techniques, including self-attention\nmechanisms and the transformer architecture, to learn rich linguistic patterns and contextual\ndependencies (V aswani et al., 2017; Brown et al., 2020). In recent years, the potential of\nthese models has been demonstrated for various NLP tasks and varied technical domains,\nincluding text generation, summarization, and question-answering (Shuster et al., 2022; Scao\net al., 2022; Glaese et al., 2022; Thoppilan et al., 2022;P o w e l le ta l . ,2023).\nThe application of LLMs offers a tremendous opportunity to enhance scheduling systems\nby leveraging their advanced reasoning capabilities and generating textual explanations to\njustify the systems’ outputs to smooth the decision-making processes, addressing the chal-\nlenges in bias and data utilisation, and signiﬁcantly improving operational efﬁciency (Zheng\net al., 2023; Bastola et al., 2023; Schroder, 2023).\nLLMs can be broadly categorised as either generalist, such as OpenAI’s ChatGPT (Wu\net al., 2023), or domain-speciﬁc, which are purpose-built or trained for a speciﬁc area of\nfunction (Jeong, 2024). Generalist LLMs are trained on large amounts of data on almost\nevery publicly available topic and have the ability to comprehend a wide variety of queries.\nWhereas domain-speciﬁc LLMs, which can be developed on top of a generalist platform\n(Jeong, 2024), are designed to answer targetted questions on a single or small number of\ntopics, with the intent of deepening the comprehension of the system; this is at the expense of\nﬂexibility, however, as mentioned earlier. Creating domain-speciﬁc capabilities from gener-\nalist LLMs requires the implementation of pre-training and ﬁne-tuning data practices, which\nprovides additional context to build domain knowledge in a speciﬁc area, such as business\nprocesses, ﬁnance management, and recommender systems aiming to improve performance\nand accuracy (Bernardi et al., 2024;W a n ge ta l . , 2024;Y a n ge ta l . , 2024b).\nThis paper aims to explore the integration of a generalist LLM into benchmark schedules,\nfocusing on their reasoning abilities and the generation of informative textual explanations\nto questions based on differently sized schedules in tabular form. To the author’s knowledge,\nthere is minimal available data in this area; however, two suitable studies have been identiﬁed.\nThe ﬁrst study assessed the performance of GPT models in answering questions on materials\nscience. Introducing the concept of Material Science Question Answering (MaScQA), the\nstudy compared the performance of the models when using a zero-shot or a chain-of-thought\nprompting method (Zaki et al., 2024). The second study focused on assessing the use of\n123\n1289\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nlanguage achieved by GPT models in answering domain-speciﬁc questions on Microsoft\nproducts and technical IT problems, using reference answers curated by cloud-computing\nspecialists. In this study, a small, domain-speciﬁc Language Model (LM) was pre-trained on\nthe question dataset, which the LLM then was given access to when answering the question\n(Yang et al., 2023).\nLikewise, LLaMA has also been extensively studied, often in combination with ChatGPT\nor GPT-4. For instance a recent study evaluated the clinical decision-making capabilities of\nLLaMA and found that the accuracy did not meet the human expert standard and could present\na considerable risk to patients in real-world applications, even when trained on real clinical\ncase data (Hager et al., 2024). Another study followed a similar process for assessing the\nperformance of LLaMA in responding to case law enquries. Conversely to the performance\non clinical decision-making, the post-tuned model demonstrated marked improvement in\naccuracy and F1-score (Satterﬁeld et al., 2024). This highlights the challenge for generalist\nLLMs in responding to domain-speciﬁc topics, even when pre-training is performed, as\nresults can be inconsistent.\nWhile the combination of both DMS and NLP is yet to be widely applied to scheduling\ncapabilities, it has shown potential in research on staff assignment, where an NLP prediction\nmodel was created to autonomously assign staff tasks using unstructured data and construc-\ntion scheduling (Mo et al., 2020). Elsewhere, NLP was utilised to analyze and validate the\nlogic of manually created schedules based on trained data (Amer et al., 2022) and the use of a\nGPT model to support the creation of construction schedules based on prompts (Prieto et al.,\n2023). These systems leverage NLP techniques by extracting and analyzing relevant infor-\nmation from textual data, enabling an automated understanding of scheduling requirements,\nconstraints, and objectives expressed in natural language.\nSeveral challenges need to be addressed to effectively leverage LLMs capabilities for\nscheduling systems. These include ensuring the robustness and reliability of the models’\nreasoning capabilities (Kasneci et al., 2023). There are also limitations in certain case studies\nusing a GPT model, as previously mentioned, for instance, lacking domain-speciﬁc knowl-\nedge to develop construction project schedules due to generalised training and no specialised\napplication existing (Prieto et al., 2023). Additionally, the ethical considerations necessary,\nsuch as fairness and data/training bias, were highlighted when using algorithmic DMS as the\nimpact can often be beyond the organisation itself (Marabelli et al., 2021). Furthermore, the\ninterpretability of LLMs, which stems from the lack of transparency in systems operations,\nis an area of active research that requires attention to ensure that the generated explanations\nare meaningful and trustworthy (Singh et al., 2023b).\nThe contributions of this paper are summarised as follows:\n– We introduce a novel approach that utilizes LLMs to generate textual explanations for\noptimally derived solutions to scheduling problems. This approach leverages the LLMs’\nability to comprehend tabular data and perform reasoning tasks.\n– We propose a comprehensive framework for evaluating the methodology by creating\na benchmark dataset that categorizes queries into three types: Swap, Increase,a n d\nDecrease. The generated explanations are assessed based on metrics such as correct-\nness, similarity, completeness, conciseness, and language quality, demonstrating a high\nlevel of comprehension in the results.\n– A benchmark dataset of job-shop scheduling problems was selected to test the pro-\nposed methodology. Performance was evaluated across different conﬁgurations, varying\nin the number and combination of jobs and machines, to validate the effectiveness of the\napproach.\n123\n1290\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nFollowing this introduction, this paper covers a background and literature review\n(Section 2) exploring existing research in the relevant areas discussing relatable concepts.\nThe methodology (Section 3) outlines the approach taken for generating queries (also referred\nto as questions) in each class, as well as the methodology used to engineer the prompt and\ngenerate a textual response. The results (Section 4) analyse the ﬁndings and discuss observed\npatterns and performance. Finally, the conclusion (Section 5) summarises the paper’s ﬁndings\nand suggests future opportunities to deepen research in this area. Additionally, Appendix A\ncontains tables used in completing assessments and summarising the scores from the LLMs.\n2 Background and literature review\n2.1 LLMs and XAI\nLLMs are evolving in tandem with efforts to incorporate XAI techniques, aiming to\nimprove the interpretability and transparency of these models. By integrating XAI meth-\nods, researchers strive to provide insights into the decision-making processes of autonomous\nsystems and enable users to understand and trust their outputs (Datta & Dickerson, 2023;\nNarteni et al., 2022).\nThere are a multitude of different approaches in development for XAI, as shown in recent\nstudies (Arrieta et al., 2020; Ali et al., 2023), each designed to address particular details within\nan AI system. Most XAI techniques identiﬁed do not utilise an LLM and instead directly\ninterface or integrate into the AI model to extract the explanatory information. For instance,\nExploratory Data Analysis (EDA) tools aim to extract signiﬁcant features of a domain-\nspeciﬁc model, where feature engineering is in place. Alternatively, a Teaching Explanations\nfor Decisions (TED) framework could be applied, to supplement training data with user-\nbased reasoning on a particular decision, which can then be combined with the output from\nthe model; or, for post-hoc approaches, a a Local Rule-Based Explanation (LORE) can\nbe implemented that extracts a decision tree from the model to infer the explanation by\nestablishing the rules for what causes the decision to be made along with the conditions for a\nreversal of the decision. These techniques either return quantitive-based explanations or are\nbuilt to be model-speciﬁc and do not offer easily understandable, human-language responses.\nAdditionally, the explanations generated are single execution without the means of feedback\nor interaction, which gives inherent beneﬁts of using an LLM for XAI for a model-agnostic\nand language-based approach. The most prominent disadvantage of implementing an LLM,\nespecially a publicly accessible model, is the lack of insight into the training of the model\nand where bias or fairness concerns may not be known or corrected prior to implementation.\nOne of the critical aspects to maximise the potential of LLMs is to optimize prompting,\nwith a leading technique being chain-of-thought prompting. The technique involves a series\nof structured textual reasoning steps that result in the ﬁnal output (Wei et al., 2022). The intent\nof this process is to reﬁne the performance from an LLM and generate better-reasoned results,\nwith the capacity for the LLM to synthesize its own chain-of-thought prompting following\nan initial guiding prompt (Shao et al., 2023). However, research into the social bias of LLMs\nhas also shown that chain-of-thought generated explanations can appear well-reasoned but\nactually contain misleading information, which inhibits the establishment of transparency\nand trust (Turpin et al., 2023).\nTo try and overcome the issues of fact hallucination and error propagation in chain-of-\nthought prompting, research was conducted to apply a more action focused approached\n123\n1291\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nto answering queries. The approach, termed ReACT, creates a combination of reasoning\ntraces and required actions to establish and adapt an executable plan to achieve the outcome,\nincluding the utilisation of external sources, aligning to the principles of reason to actand act\nto reason (Yao et al., 2023). Once this methodology is implemented, a generalist LLM, such\nas GPT-4, is able to complete highly technical and domain-speciﬁc activities, like root cause\nanalysis of Information Technology (IT) incidents, demonstrating the potential for real-world\napplications (Roy et al., 2024). The capability of the ReACT framework has facilitated new\ninvestigations on how LLM reasoning can be improved further, such as the introduction of\nPreAct, which leverages environmental and situational predictions into the process to yield\ngreater results in diverse environments (Fu et al., 2024).\nTechniques like Layer-wise Relevance Propagation (LRP) provide explanations from the\npredictions of neural networks by assigning relevance scores to the input features. It aims to\nunderstand the importance of each input feature in contributing to the ﬁnal prediction made\nby the model (Montavon et al., 2019). LRP works by attributing the model’s predictions back\nto individual input features, providing insights into which parts of the input are most and\nleast relevant for making a particular decision (Bach et al., 2015).\nResearchers are exploring methods to build interpretable models by leveraging the\nknowledge learned from LLMs. Augmented Interpretable Models (Aug-imodel), a recently\nintroduced technique that uses a LM to build an interpretable model but doesn’t rely on\nthe LLM during inference, ensuring transparency and efﬁciency gains in terms of speed\nand memory (Singh et al., 2023a). It addresses limitations in existing transparent models\nby incorporating world knowledge from modern LLMs, such as feature correlations. The\nmethod includes two approaches: Aug-GAM, which enhances a generalised additive model\nwith LLM-based decoupled embeddings, and Aug-Tree, which improves a decision tree by\ngenerating enhanced features using an LLM.\nPrototype networks for transformer language models, referred to as Prototypical-\nTransformer Explanation (Proto-Trex), have the aim of providing explanations for the\nnetwork’s decisions (Friedrich et al., 2021). The study demonstrated that these prototype\nnetworks performed on par with non-interpretable baselines for classiﬁcation tasks across\nvarious architectures and datasets. To enhance prototypical explanations, they presented a\nnovel interactive prototype learning setting named iProto-Trex, which took into account user\nfeedback certainty.\nThe paper “Attention is not Explanation” discussed how attention mechanisms, commonly\nused in transformer-based models, do not serve as adequate explanations for model predic-\ntions. Their results suggested that relying solely on attention weights to interpret model\nbehaviour may not provide meaningful insights into how the model arrives at its decisions\n(Jain & Wallace, 2019). Meanwhile, a paper by response, “Attention is not not Explanation”,\npresents a counterargument to this claim, where they proposed four alternative tests to assess\nwhen and whether attention can be used as an explanation. These tests included a simple\nuniform-weights baseline, variance calibration based on multiple random seed runs, a diag-\nnostic framework using frozen weights from pre-trained models, and an end-to-end adversar-\nial attention training protocol. The authors aim to gain meaningful interpretations of attention\nmechanisms in Recurrent Neural Network (RNN) models (Wiegreffe & Pinter, 2019).\nXAI in recommender systems, research was conducted with the aim to provide users\ninsights into product recommendations (Kim et al., 2023). Their work emphasized the need\nfor a uniﬁed explanation method centred around the human perspective. They later inves-\ntigated user-centred explainability components, such as scope (global/local) and format\n(text/visualization), using a conjoint survey. Results showed a preference for local expla-\nnations and visualizations over global ones, while lengthy textual interfaces were disliked.\n123\n1292\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nThese examples represent the ongoing efforts to make LLMs more explainable and\ninterpretable. By incorporating XAI techniques, researchers are working towards ensuring\ntransparency and building trust in LLMs.\nIn selecting the best approach for this study, the ten strategies outlined in a recent study\nwere considered (some of which are Explainability for Trustworthy LLMs and Human\nAlignment, LLM Enhancement via Explainable Prompting, and Generating User-Friendly\nExplanation for XAI) (Wu et al., 2024). As the goal of this study is to establish a bench-\nmark approach for generating textual explanations, the user-friendly explanation approach\nwas selected as the most appropriate. The authors encourage alternative approaches in future\nexperiments.\n2.2 Scheduling and XAI\nThere are valuable applications of XAI in the domain of scheduling offering transparent and\ncomprehensible insights into the decision-making process behind scheduling tasks, however,\nresearch and development in this area are still emerging. Through the application of XAI\ntechniques, users will gain a detailed understanding of the underlying logic of the scheduling\nmodel and the key factors that inﬂuence the generation of schedules (Ben Abdallah et al.,\n2023; ˇCyras et al., 2021; Gashi et al., 2023).\nIn the context of Machine Learning (ML), taking a speciﬁc classiﬁer and point in the feature\nspace, applying a rule-based explanation algorithm creates a rule that holds to the features\nof the classiﬁer, covering the given point and enabling classiﬁcation. These explanations are\nrobust in the context of the surrounding area in the feature space (Mullins, 2023). Considering\nthis concept for scheduling, human-readable rules are extracted from the scheduling model\nto provide understandable decision guidelines. Users can gain insights into how certain\nscheduling decisions are made based on these rules.\nIntegrating XAI with the scheduling model to incorporate user inputs and feedback may\nprovide a clear understanding of how they inﬂuence the ﬁnal schedule. By considering users’\nrequirements, the scheduling algorithm prioritizes tasks or resources in alignment with indi-\nvidual choices, allowing users to comprehend the rationale behind the prioritization of speciﬁc\nelements in the schedule (Chakraborti et al., 2020).\nIn a recent study, a robust analysis of consumer preferences for AI interfaces was under-\ntaken using a discrete-choice model grounded in random utility theory. Speciﬁcally, the\nresearchers opted for a mixed logit model to effectively account for variations in consumer\npreferences and accommodate the inherent heterogeneity among users. This approach enabled\na comprehensive evaluation of users’ choices, facilitating a deeper understanding of the fac-\ntors inﬂuencing their preferences for AI interfaces (Kim et al., 2023). The potential of this\nresearch could facilitate the development of interactive interfaces that allow users to explore\ndifferent scheduling scenarios and understand the impact of their inputs with the help of\ngraphical aids.\nA comprehensive survey discusses practical applications of Reinforcement Learning (RL)\nmethods to achieve fair solutions with high accuracy. The survey reviews the theory of\nfair reinforcement learning, including single-agent RL, multi-agent RL, long-term fairness\nvia RL, and ofﬂine learning. Additionally, the authors highlight key issues to explore for\nadvancing fair-RL, such as correcting societal biases, evaluating the feasibility of group\nfairness or individual fairness, and enhancing explainability in RL known as Explainable\nReinforcement Learning (XRL) (Gajane et al., 2022). XRL is aimed at providing clear and\n123\n1293\nJournal of Intelligent Information Systems (2025) 63:1287–1337\ntransparent insights into the decision-making process of learning agents, in particular for\nsystems performing sequential decision-making (Puiutta & V eith, 2020).\nThe study observes the potential of fair XRL for scheduling, by incorporating fairness con-\nsiderations into scheduling algorithms, users can gain transparent insights into how resources,\nincluding time, are allocated, leading to equitable distribution and mitigating biases (Puiutta\n& V eith,2020). Further research on fair-RL and XRL techniques for scheduling is considered\nnecessary for building trustworthy and inclusive scheduling systems that cater to diverse user\nneeds.\nAnother approach is that of CF explanations for XAI. These are considered to be ﬁve\ndeﬁcits related to psychological and computational evaluations in CF XAI. These deﬁcits\ninclude neglecting users, grounding of plausibility with psychology, considering sparsity\nbased on feature differences, evaluating coverage for plausible explanations, and performing\ncomparative testing (Keane et al., 2021). To apply CF explanations to scheduling, further\nresearch can explore XAI techniques that prioritize user-centric explanations, generate plau-\nsible and interpretable scheduling decisions, address resource allocation, ensure a compre-\nhensive evaluation of explanations for coverage and trustworthiness, and conduct comparative\ntesting to identify the most effective CF XAI methods for scheduling applications.\nBy integrating XAI into scheduling users can leverage various XAI techniques and tools,\nsuch as classiﬁcation methods for job scheduling problems, customizable rules, textual\ndescriptions, pseudo-code, decision trees, and ﬂowcharts. Additionally, job sequencing and\nscheduling problems, frequently formulated as mathematical programming models, can be\noptimized using AI technologies, with a particular focus on the application of Genetic Algo-\nrithm (GA) for ﬁnding optimal solutions in the scheduling process. This integration enhances\nthe transparency and interpretability of scheduling decisions, allowing users to better under-\nstand the reasoning behind decisions, leading to more informed and beneﬁcial scheduling\noutcomes (Chen, 2023).\n2.3 LLMs and XAI for scheduling\nLimited research exists regarding the use of LLMs and XAI in the context of scheduling,\nresulting in minimal scope for meaningful comparison of the proposed techniques in this\npaper. This section outlines the potential of using both LLMs and XAI to enhance transparency\nand interpretability in the decision-making process of scheduling. A recent study, building\non the understanding that scheduling data is often in a tabular structure (Francis, 2015), was\nconducted to determine the potential of LLMs in their ability to understand tabulated data.\nThe research explored using GPT-3 providing several challenges to the model (Sui et al.,\n2024). This produced varying outputs based on the choice of inputs, including table formats,\nprompts, partition masks, and role prompting. The paper proposes self-augmentation for\neffective structural prompting, leveraging LLMs’ internal knowledge for tasks like critical\nvalue/range identiﬁcation. As illustrated in Fig. 1, the LLM can extract signiﬁcant values\nfrom the table using self-augmented prompting, which aids in generating improved answers\nfor downstream tasks.\nTherefore, in the context of scheduling, this demonstrates that LLMs can process vol-\numes of tabular and textual data, including scheduling rules, constraints, and requirements\nto assist in automating the scheduling process. These models can also interpret and extract\nrelevant information from unstructured text, facilitating better decision-making and proﬁ-\ncient scheduling. Another study analyzed tabulated data from a scheduler based on ﬁxed and\nstructured queries. These queries were targeted at speciﬁc scheduled tasks and assessed the\n123\n1294\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nFig. 1 Illustration of self-augmented prompting (Sui et al., 2024)\nfeasibility of replacing them with alternative tasks without impacting the schedule, known as\na Single Exchange Property (SEP) concept. The generated prompts were fed to the LM along\nwith the initial query to aid with generating an appropriate response (Powell et al., 2023). A\nsummary of the process is shown in Fig. 2.\nThis outlines the current known capability for the use of LLMs with XAI for solving and\nexplaining queries on scheduling data and problems, signalling the emergent nature of this\ncombination of techniques.\n2.4 Scheduling benchmark set\nWhen considering the type of scheduling problem to adapt to the approach in this study, the\ntask-based nature of schedules aligned best with Job Shop Scheduling, which is a speciﬁc\nclass of scheduling problems which has been extensively research over many years (Xiong\net al., 2022). The approach in this paper is independent of the scheduling problem and can\nbe extended to any other problem type in future research, such as bin packing or employee\nscheduling.\nAs such, the history of job shop scheduling was examined, and publicly available bench-\nmark problems were identiﬁed from the substantial research by E. Taillard on job shop\nScheduling, in which 260 benchmark scheduling problems of varying sizes and optimality\nwere deﬁned. These benchmark schedules have been utilised in research for over 30 years\nwith over 3,000 citations, building a well-established baseline that can be applied to any\ntask-based, job-shop-aligned scheduling problem. Because of this, schedules of different\nsizes were selected from the original research, where the schedule data was available, to\ndetermine how the proposed methodology scales with increased schedule size and complex-\nity, which will form the basis of an XAI experiment to demonstrate the capabilities in a\nneutral, non-domain-speciﬁc setting. Please refer to Taillard ( 1993) for the algorithm used\nto create the different problem instances.\nFig. 2 NLP combined with argumentation to analyse a schedule (Powell et al., 2023)\n123\n1295\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nThe framework problems outlined by E. Taillard provided the means of generating solver\nresults for a set of benchmark schedules, where the number of jobs ( n) and the number of\nmachines (m) can be altered to control the size of the schedule. Within the schedule, each job\nhas an uninterruptable duration (or processing time), randomly determined between 1 and\n99 (Taillard, 1993; Jain & Meeran, 1999), that must be completed for the job to ﬁnish and\nmachines can only process one job at a time.\nThe collective performance of research for solving E. Taillard’s benchmark has been\nsummarised from dozens of different research studies to document the lower and upper\nbounds of solutions (Shylo & Shams, 2018); where the lower bound represents the optimal\nsolution, and the upper bound represents the current best feasible solution, with the goal of\noptimisation matching the bounds together through exhaustive solving (Brucker & Knust,\n2006). From the presented information, at the time of writing, of the 80 Job-Shop Scheduling\nbenchmark problems 21 remain with non-optimal upper bounds (Shylo & Shams, 2018).\nThe authors of this paper considered the solutions located within (Taillard, 1997)w e r e\nderived by Brinkkötter and Brucker ( 2001) for seven different schedules entailing 15 jobs by\n15 machines with makespans of 1218, labelled as TA03; 1175 as TA04; 1224 as TA05; 1238\nas TA06; 1227 as TA07; 1217 as TA08; and 1274 as TA09. Additionally, two schedules by\nHenning Dr. rer. nat. ( 2002) for 20 jobs and 15 machines with a makespan of 1342, labelled\nas TA13; 20 jobs on 20 machines with a makespan of 1647 as TA26; and lastly, 30 jobs\nand 20 machines with a makespan of 1956 as TA48 (Shylo, 2002) as shown in Table 1.\nThis paper used these schedules as benchmarks with the proposed methodology to generate\nquestions/answers and explanations.\nFigure 3 represents a Gantt chart of the schedule derived by Henning Dr. rer. nat. ( 2002)\nof a Job Shop scheduling problem TA13 across a time horizon. Each machine has 20 jobs\nassigned to it, with no overlapping of jobs across any machines.\n3 Methodology\nAn LLM based method was established to answer queries for three different categories of\nqueries to explore its potential by analysing different types of tabular job-shop scheduled data,\nadhering to the predeﬁned constraints of the schedule. The types of scheduled data analysed\nTable 1 Schedules used with\ntheir respective makespans\n(Brinkkötter & Brucker, 2001;\nHenning Dr. rer. nat., 2002;\nShylo, 2002; Taillard, 1997)\nSchedule Label Schedule Type jobs j\nby machines m\nMakespan\nTA03 15 j x1 5 m 1218\nTA04 15 j x1 5 m 1175\nTA05 15 j x1 5 m 1224\nTA06 15 j x1 5 m 1238\nTA07 15 j x1 5 m 1227\nTA08 15 j x1 5 m 1217\nTA09 15 j x1 5 m 1274\nTA13 20 j x1 5 m 1342\nTA26 20 j x2 0 m 1647\nTA48 30 j x2 0 m 1956\n123\n1296\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nFig. 3 A representation of schedule TA13 derived by Henning Dr. rer. nat. ( 2002) proposed by Taillard ( 1997)\nwere seven different 15 jx15m schedules and one schedule of the following combinations\n20 jx15m,2 0 jx20m, and 30 jx20m where j stand for jobs and m represent machines. The\napproach outlined in this paper could also be applied to any scheduling problem and is not\ndependent on a job shop scheduling format. The job shop scheduling problem was selected\ndue to the extensive studies conducted on the format, with challenges in solving some of the\nmost complex instances, which ﬁts the differing levels of complexity in problems utilised in\nthis study.\nThis study introduces the use of OpenAI’s GPT-4 and Meta’s LLaMA-3.1 to determine\nthe feasibility of task alterations of ten benchmark schedules, where the unique queries are\nshown in Table 2 and prompts provided are shown in Table 3. GPT-4 was selected due\nto observed high performance in domain-speciﬁc areas, such as medicine (as discussed in\nSections 1 and 2), whereas LLaMA-3.1 was selected because of its capability to be used on\nlocal machines, broadening the opportunity for this benchmark study to be replicated and\nbuilt upon (Ersoy & Er¸ sahin, 2024). Figure 4 provides an overview of the approach taken in\nproviding data to the LLMs and how the responses on the scheduled data were assessed. This\napproach utilises the schedules and queries that were created from each category described\nin Section 3.1 for the respective schedule and combined to prompt the LLMs as explained\nin Section 3.2. The generalist LLMs, as opposed to a specialised or pre-trained model, were\nchosen to explore the capabilities of these emergent tools in domain-speciﬁc and techni-\ncal problems. The LLM’s responses were assessed in ﬁve different ways as explained in\nSection 3.3 for each of the query categories where comparisons of the relations were dis-\ncussed.\n123\n1297\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nTable 2 Table displaying the unique queries for each of the query categories\nQuery Category Queries\nSwap 1. Could the start time of job a be exchanged with the start time of job b on machine c?\n2. Is it possible for the end time of job a to be exchanged with the end time of job b on\nmachine c?\n3. Is the exchange of job a and job b on machine c feasible?\n4. Can job a be exchanged between machines c and d?\n5. Can the processing times of job a on machine c, be exchanged with the processing\ntimes of job a on machine d?\n6. I’m considering swapping the start time of job a on machine c with the start time of\njob a on machine d. Is this possible?\n7. Suppose I swapped the end time of job a on machine c with the start time of job a on\nmachine d, is this possible?\nIncrease 1. Can the duration of job a on machine c be increased by z minutes?\n2. Can machine c overall running time be increased by z minutes without impacting the\noverall scheduled run time?\n3. Is it possible for the start time of job a on machine c to be increased by z minutes?\n4. If I increased the end time of job a by z minutes on machine c, would that be feasible?\nDecrease 1. Is it possible for the duration of job a on machine c to be reduced by z minutes?\n2. Can machine c overall running time be reduced by z minutes without impacting the\noverall scheduled run time?\n3. I need to know if the start time of job a on machine c can be reduced by z minutes.\n4. Would reducing the end time of job a on machine c by z minutes, be possible?\nTable 3 Table displaying the prompts used to answer each of the query categories\nPrompt used across all categories for answering queries\nThis is a schedule for a job shop problem.\nEach row labelled J# represents the job across each machine except the ﬁrst row, and each column except\nthe ﬁrst represents a machine number.\nThe scheduling of jobs and machines is not sequential and can be in any order; however, a machine can\nonly run one job at a time, and the same job cannot run at the same time on different machines.\nJobs are never to be repeated on the same machine, and there are no sequencing or dependency rules for\njobs on each machine; for example, job 5 can occur before job 4.\nThe schedule data provided below is not in order of the schedule and must be restructured to be sequential.\nWhen answering questions on the schedule, please consider all the data available and the potential knock-\non impact or conﬂict with other machines, reviewing all possible or necessary adjustments to fully answer\nthe query. There are also no deadlines for jobs or the schedule.\nEvery Answer MUST start with a yes or no followed by the explanation.\nThree examples of answering questions are below:\n1. Could the start time of job 9 be exchanged with the start time of job 15 on machine 2?\nanswer: Yes, the start time of job 9 can be exchanged with job 15 on machine 2, as there are no overlaps\nof the same jobs on the other machines.\n2. Can machine 11 overall running time be increased by 15 minutes?\n123\n1298\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nTable 3 continued\nPrompt used across all categories for answering queries\nanswer: No, machine 11 overall run time cannot be increased by 15 minutes as there would be an overlap\nin other jobs.\n3. I need to know if the start time of machine 15 can be reduced by 13 minutes.\nanswer: Yes, the start time of machine 15 can be reduced by 13 minutes, as there is availability within\nthe time requested.\n3.1 Query creation\nThree query categories were created, each containing variations of a set number of unique\nquestions, each of which is shown in Table 2. These are:\n– Swap - Inquiries around exchanging the processing times, start times, and same jobs\nacross machines. This category has seven unique benchmark questions.\n– Increase - Entails any queries involving an extension of job start, ﬁnish, or processing\nduration time on any job on a machine or machine run time. This category has four unique\nbenchmark questions.\n– Decrease - Similar to increase, entails the reduction of job start, ﬁnish, or processing\nduration processing time of a job or overall machine run time. Also has four unique\nbenchmark questions.\nThe swap category offered a greater range of possible queries over the other categories,\nwhich included exchanging the same jobs across different machines, while the schedule\nmaintained that all jobs were scheduled to run on each machine, as well as the option to swap\nany two jobs on the same machine. Additionally, the exchange of start times or processing\ntimes was asked.\nFig. 4 Overview of LLM analysis on scheduled data\n123\n1299\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nMeanwhile, the Increase and Decrease categories were provided with four unique ques-\ntions, with each containing a variation of machine and job numbers, supplied to the LLMs\nfor answering.\nAll unique queries were repeated n times with varying job and machine numbers used\nduring the assessments to measure the consistency of responses by the LLMs. The job and\nmachine numbers, including the times and durations, were randomly generated with con-\nstraints ensuring the machine and job numbers were within range of the type of schedule the\nquery was asked against.\n3.2 Answering benchmark schedule queries\nTwo tables for each schedule were created in a text ﬁle and used as part of the prompt to\nthe model. The ﬁrst table contained the processing times for each job on each machine with\nj rows by m columns. The second table, however, contained the start times for each job on\neach machine with j rows by m columns.\nPrompts, shown in Table 3, were provided with the scheduled data and combined with\nthe queries created from Section 3.1 to assist in the generation of the answers to the queries\ncreated. The overall process employs an example-based few-shot approach with the supplied\nschedule, using the api connector to both GPT-4 and LLaMA-3.1. While the prompt only\nincludes one example question and answer pair for each query category, this approach is\nconsidered few-shot learning (instead of one-shot learning) as the LLMs are unaware of the\nseparate query categories and will consider all examples when formulating the answers. The\ndecision was made not to include any additional prompt optimisation techniques, such as\nchain-of-thought (Shao et al., 2023) or ReACT (Yao et al., 2023), as the experiment aims to\nestablish baseline performance with generalist LLMs for this novel investigation.\nThe results, including all query variants, were analysed for their performance in correct-\nness, cosine similarity, completeness, response length, and use of language to assess the\nquality of the LLM response to answering the queries. The metrics used are outlined in the\nfollowing Sub-Sections.\n3.3 Performance measure\nThe evaluation of the performance of the proposed methodology is based on the analysis of\nthe accuracy, consistency and readability of the responses to the varying query categories and\nunique queries answered by the LLMs. Additionally, the potential similarities and patterns\nbetween each assessment metric and query category, in how the LLMs were able to generate\na response to queries from the tabular schedules of different sizes, was analysed.\n3.3.1 Correctness\nIn determining the correctness of the answers generated by the LLMs, the yes/no responses\nwere assessed, producing a binary value to signify whether the answer was deemed correct\nor not. This was done algorithmically to independently check the feasibility of the schedule\nalterations based on the questions, then compared with the response from the LLM. If the\nalterations queried in the original query were feasible and correlated to the yes response, a\nscore of 1 was given; likewise, if the alterations queried were not feasible and the response\nwas no,as c o r eo f 1 was also given to show the response as correct; otherwise, if any other\n123\n1300\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nresult, a score of 0 was recorded. However, where answers contain two components for\ncorrectness, for example, when exchanging processing times between two jobs (within the\nSwap category), either by altering the end time or start time, provided the algorithm produces\na result for the two outcomes, where at least one coincides with the output from the LLMs,\nas c o r eo f 1 was given to represent the response as correct. As all queries asked should\nreturn a yes/no response no other conditions were required to assess correctness in the LLMs\nresponses.\n3.3.2 Cosine similarity\nIn the context of LLMs, cosine similarity is the measure of similarity between two textual\nstatements. This was calculated by computing, following the method presented by Face\n(2024), the similarities between the answers created by the LLMs across each repeated unique\nquestion, excluding itself. This means n answers were generated for each query within each\ncategory, and each of these answers (from Table 2) was computed for the cosine similarity\nagainst the others generated for that question. The returned values were averaged across all\nresponses to the query and recorded.\nThe cosine similarity score ranges from -1 to 1,w h e r e -1 represents no similarity what-\nsoever, and 1 would be an identical response. Assessing the cosine similarity allows for a\nmeasure of consistency in language, tone, and response structure, which enables familiarity\nwith users in real-world applications.\n3.3.3 Response completeness\nThe response completeness was calculated to assess the LLMs capabilities in identifying\nand referencing the key components of the question, which include the job number(s) and\nmachine(s) speciﬁed. Additionally, the similarity of the response to the query asked was also\npart of the calculation to evaluate how much common language and terminology was used\nin the response.\nIn calculating the completeness of the response from the LLMs, two steps were followed:\n1. Check if the job and machine numbers from the query were mentioned within the\nresponse. These values range from 0 to 1, with 0 being no mention of the jobs or machines\nwithin the explanatory responses and 1 representing 100% of the noted jobs and machines\nmentioned.\n2. Calculate the cosine similarity between the query and answers generated to assess the\nuse of common words and terminology.\nThe resultant values are averaged to generate the total response completeness, which\nwill attain a value between 0 and 1; where a score of 0 means the response excludes all\nrelevant information provided in the question, and a score of 1 perfectly evidences the relevant\ninformation from the query and the greater comprehension the LLM exhibits.\n3.3.4 Word count\nThe word count of each response was also measured to analyse the difference in length of\nthe responses to assess if there is a correlation with other assessments and query categories.\nThe response tokens were set to a constant limit as detailed in Section 4 to minimise the\nﬂuctuation in response length and better represent real-world implementations.\n123\n1301\nJournal of Intelligent Information Systems (2025) 63:1287–1337\n3.3.5 BERTScore\nA sample benchmark of 30 queries and answers (including both yes and no responses), shown\nin Table 9 in Appendix A.1, was created for each category, by the authors of this paper. Each\nsample was mapped to the list of answers generated by the LLMs to return the BERTScore\n(F1), which assessed the quality of language used in each response in relation to the samples\nprovided. The BERTScore (F1) is the average of two-component values:\n1. Precision - measures the accuracy of words within the response; and\n2. Recall - measures the quality of phrases used within the response.\nAll three scores were calculated and presented; however, the results focused on analysing the\nBERTScore (F1) values. The calculation method follows the instructions provided in Face\n(2024) and is scored between 0 and 1,w h e r e0 has no resemblance to reference material and\n1 is identical to a statement in the reference material.\n3.3.6 Comparative performance analysis\nOnce all the assessments were calculated for all queries across all schedules and each query\ncategory, the results were analysed to compare the performance observed between each\ncategory. Graphs were plotted for each assessment metric to visualise the results and aid in\nassessing performance differences. The comparative performance was discussed, detailing\nrelevant insights and reasoning gained from the experiment.\n4 Results and discussion\nThe results section is presented in two parts. The ﬁrst part summarises the results for each\nquery category individually, and the second part discusses the results across all categories and\nscheduling problem sizes. There were a total of ﬁfteen unique queries across all the categories:\nseven queries for Swap and four queries for both Increase and Decrease. Each query contained\nvariations of job numbers, machine numbers, and different time intervals suitable for the\nrespective schedules, following which the subsequent responses were assessed, and the results\nwere averaged for each query and discussed in each category.\nSection 4.1 contains the assessed results for the Swap, Increase, Decrease query cate-\ngories, where all ten schedule formats were analysed, namely seven schedules of 15 jx15m\n(represented as schedules 1 - 7), one schedule 20 jx15m, one schedule 20 jx20m, and ﬁnally,\nschedule 30 jx20m. The results are presented in Tables 10 through 21 in the Appendices ( A.2,\nA.3 and A.4), the data from which was also used to plot all ﬁgures shown in Section 4.2.\nSection 4.2 contains a comparison overview between the categories with their respective\nFigures, where each schedule is represented as 15_15_1 to 15_15_7 for all schedules of\n15 jx15m, 20_15_1 for schedule 20 jx15m, 20_20_1 for schedule 20 jx20m, and 30_20_1 for\nschedule 30 jx20m. Additionally, the overall performance is compared with results observed\nin other studies to determine the success of the experiments.\nThe analysis conducted within each section provided valuable insight into the application\nand performance of the LLMs approach for distinct scheduling problems. It is important to\nnote the values in bold text shown in Tables 5 through 21 excluding Table 9,r e p r e s e n tt h e\nhighest average scores for each performance measure for the respective schedules.\nUsing GPT-4 and LLaMA-3.1 required hyperparameters to allow the exploration of the\nvariations of answers. Upon ﬁnding the most suitable settings shown in Table 4, it remained\nconsistent throughout testing to enable a fair assessment across each response.\n123\n1302\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nTable 4 Model conﬁguration to\ngenerate answers Model conﬁguration\ntemperature 1\nmax tokens 100\ntop_p 1\nfrequency penalty 0\npresence penalty 0\n4.1 Individual query categories\n4.1.1 Swap query category\nFor the Swap query category, Tables 10 and 12 represent the average correctness scores\nranging between 0.57 and 0.80 across all schedule sizes for GPT-4, with over 74% of queries\nachieving an average correctness score of 0.60 or higher. However, Tables 11,a n d 13 show\nthe LLaMA responses achieved averages between 0.00 and 0.29, as 40% of queries returned\nan average of 0.00. The GPT-4 performance is presented in Fig. 5a, where the scores were\nFig. 5 Average Correctness across the three query categories for all schedules for GPT-4\n123\n1303\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nrelatively high across all schedule variants. While LLaMA’s results shown in Fig. 6a visualise\nthe low scores for all schedules.\nThe average cosine similarity scores for GPT-4 ranged between 0.78 and 0.82, demon-\nstrating consistent similarities between answers without being identical, which was closely\nmatched by LLaMA with scores between 0.77 and 0.83. A density plot was created show-\ning where GPT-4 and LLaMA had only slight variations for cosine similarity, shown in\nFigs. 7aa n d 8a, where all 15 jx15m schedules represented with solid lines were compared\nwith 20 jx15m,2 0 jx20m, and 30 jx20m as broken lines.\nWith the completeness assessment, the average scores measured between 0.93 and 0.955\nfor GPT-4, and between 0.94 and 0.96 for LLaMA, representing a high degree of recall from\nthe elements provided within the query by both LLMs. Looking at Fig. 9a for GPT-4, each\nschedule was plotted against their average scores taken from each question, with schedule\n5o f1 5 jx15m, showing the lowest reading 0 .8153 taken from query 7 shown in Table 10.\nSchedule 20 jx15m, however, shows the second lowest reading of 0.8531, also taken from\nquery 7 in Table 12. The plot for LLaMA, in Fig. 10a, reveals less variance, with all schedules\nclosely aligned in average score distribution.\nThe average word count was calculated as between 59 and 73 words per response from\nGPT-4, and between 54 and 66 from LLaMA, which suggests a high degree of consistency in\nresponses from both LLMs. However, when looking closely at the individual query responses\nFig. 6 Average Correctness across the three query categories for all schedules for LLaMA-3.1\n123\n1304\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nFig. 7 Average Cosine Similarity across the three query categories for GPT-4\nfrom both LLMs, there were noticeable variances in the length as shown in Figs. 11aa n d 12a,\nwhich infers that particular wording of a query can greatly inﬂuence the length of the response.\nFor BERTscore (F1), the average scores for GPT-4 ranged between 0.61 and 0.66, with\nLLaMA achieving between 0.59 and 0.64, which shows the quality of the responses provided\nby both LLMs had a high degree of consistency with the human sample responses, with\nminimal ﬂuctuation shown in the scores, while also not too closely aligning with the reference\nmaterial. The consistency of these scores was very similar across both LLMs and is shown\nin Figs. 13aa n d 14a, representing a density violin plot of these values.\n4.1.2 Increase query category\nThe results for GPT-4 from the Increase query category are shown in Tables 14,a n d 16,\nwhere the average correctness scores were between 0.4 and 0.75, and over 62% of queries\nscoring an average of 0.6 or higher. The responses from LLaMA shown in Tables 15,a n d 17,\nachieved an average correctness between 0.25 and 0.45, with 12% of answers scoring 0 and\n13% scoring 0.6 or higher across all variations. The ﬂuctuations in these scores were reﬂected\nin Figs. 5ba n d 6b for GPT-4 and LLaMA respectively. Schedule 4 of 15 jx15m stands out\nfor having the lowest correctness score from GPT-4, along with schedule 7 from 15 jx15m,\n123\n1305\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nFig. 8 Average Cosine Similarity across the three query categories for LLaMA-3.1\nwhich has a very broad distribution due to both fully incorrect and fully correct answered\nquestions, whereas the scores from LLaMA were consistently distributed.\nThe average cosine similarity scores for GPT-4 ranged between 0.80 and 0.855, presenting\na high degree of consistency in the LLM responses, with LLaMA achieving very similar\nresults, scoring between 0.79 and 0.84. The density plots are shown in Figs. 7ba n d 8b( G P T -\n4 and LLaMA), identiﬁed schedule 5 of 15 jx15m from GPT-4 as the highest density with\na value around 0.83 when compared with the other schedules, while schedules 20 jx15m,\n20 jx20m, and 30 jx20m represented a lower overall density range. The distribution of cosine\nscores from LLaMA was similar for all schedules, with the exception of schedules 1 of\n15 jx15m and 30 jx20m with scores ranging from 0.72 to 0.90 and 0.72 to 0.84 respectively,\nexceeding the average range.\nThe average completeness was scored between 0.95 and 0.965 for both GPT-4 and LLaMA,\ndemonstrating the LLMs were both able to identify relevant information from the queries\nin almost every case. The very narrow range in scores can be seen in Figs. 9ba n d 10b,\nemphasizing how consistently the LLMs referenced the correct job and machine numbers.\nConsidering the average word count, which ranged between 51 and 66 for GPT-4 and\nbetween 53 and 71 for LLaMA, it can be observed the length of responses was fairly concise,\nwith neither of the LLMs used the full token limit on average. It can be noted, from GPT-4,\nthat queries 1 and 3 from 15 jx15m schedules 5, 6, and 3, respectively, had signiﬁcantly lower\n123\n1306\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nFig. 9 Average Response Completeness across three query categories for GPT-4\nthan average word counts, visible in Fig. 11b (also shown in Table 14). However, the nature\nof the queries within the category means, at times, the LLM can answer sufﬁciently well\nwith very few words. This pattern was not shared by the responses from LLaMA, which had\na more even distribution of length in response as shown in Fig. 12b.\nFor the average BERTscore (F1) this category, for GPT-4, achieved scores between 0.64\nand 0.70, which represents that the LLM consistently used language aligned to human ref-\nerence material, as shown by the minimal ﬂuctuation in Fig. 13b. LLaMA scored between\n0.63 and 0.665, demonstrating very similar performance, as shown in Fig. 14b.\n4.1.3 Decrease query category\nIn the Decrease query category results, Tables 18,a n d 20 for GPT-4 showed the correctness\nscore ranged between 0.3 and 0.8, outlining the variance in correctness scores and where\n42% of answers were above a score of 0.6. While Tables 19,a n d 21 represent the responses\nfrom LLaMA which scored between 0.45 and 0.75, with 40% of answers scoring above\n0.6. For GPT-4 it was observed the larger schedules performed below the average score\nranges of the 15 jx15m schedules, as shown in Fig. 5c, with overall average correctness\nscores ranging from 0.30 to 0.40. This may be the result of the comprehension necessary\nto successfully answer these query types in addition to assessing larger and more complex\n123\n1307\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nFig. 10 Average response completeness across the three category queries for LLaMA-3.1\ndatasets; the results ﬂuctuated where further experiments could be conducted to validate the\npattern. This was not the case with LLaMA, where the larger schedules achieved the same\nlevels of performance as the smaller variants, as shown in Fig. 6c, which demonstrated the\ndifference in comprehension that different LLMs can have.\nFor the cosine similarity assessment, the scores ranged between 0.78 and 0.83 for GPT-4\nand between 0.79 and 0.84 for LLaMA, maintained a consistent measure of similarity across\nall queries and schedule sizes from both LLMs. Given the average scores for each query of\nschedule 4 of 15 jx15m from GPT-4 contain the lowest values in this category, the scores\nfrom all other schedules, however, were very closely aligned as presented in the density\ngraph in Fig. 7c. The distribution of similarity scores from LLaMA was more varied, with\nschedules either aligning to the bottom or the top of the score range, seen in Fig. 8c, albeit\nwith relatively small differences.\nWith the average completeness scores that ranged between 0.90 and 0.965 for GPT-4 and\nbetween 0.94 and 0.965 for LLaMA, a greater variance was observed from GPT-4 in this\ncategory. However, it can be seen in Fig. 9c how closely aligned completeness scores are for\nschedules 1 and 2 of 15 jx15m, and schedule 20 jx15m, while the others had much larger\nscore ranges. Conversely, outside of schedule 5 of 15 jx15m, the completeness scores for\nLLaMA were evenly and closely distributed, as shown in Fig. 10c, highlighting the ability\nfor the LLM to return relevant information, even if the assessment may be incorrect.\n123\n1308\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nFig. 11 Average Word Count across the three query categories for GPT-4\nFor the word count, GPT-4 returned responses that ranged between 48 and 66 words on\naverage, which was the largest range of all the query categories. LLaMA returned responses\nbetween 62 and 70, towards matching the ranges from other query categories, emphasizing\nthe consistency in responses from this LLM. Considering Fig. 11c for GPT-4, there was a\nclear pattern of query 1 having signiﬁcantly fewer words (11 words less per response) on\naverage when compared with the overall schedule averages. This appears to be due to the\nstraightforward nature of the question: Is it possible for the duration of job X on machine Y\nto be reduced by Z minutes?, which the LLM is able to answer very concisely. Whereas with\nLLaMA in Fig. 12c showed no distinguishing pattern or irregular response.\nFinally, for the BERTscore (F1) assessment, the scores from GPT-4 in this category ranged\nbetween 0.67 and 0.72, which was observed to be the highest range of scores for all the query\ncategories. For LLaMA, the F1 scores ranged from 0.61 to 0.65, which aligned with the\nprevious scores from the other query categories. The consistency from both LLMs can be seen\nin Figs. 13ca n d14c where there was a close similarity of distribution across all schedules, with\nthe exception of schedules 4 and 6 of 15 jx15m for LLaMA which were densely distributed\nin the scores. The uniformity seen in the results also showed that increasing the data size\nand complexity does not have an adverse impact on the LLMs interpretation and style of\nresponses.\n123\n1309\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nFig. 12 Average Word Count across the three query categories for LLaMA-3.1\n4.2 Cross category comparison and performance discussion\nTo gain further insight from the results, the assessment metrics from each query category\nacross both LLM responses were also compared against each other to identify any correlations\nor signiﬁcant differences in performance. The results were also collated for each schedule\nvariant, in Tables 5 and 6, to assess whether scheduling size and complexity have any inﬂuence\non the assessment scores.\n4.2.1 Average correctness\nThe overall average correctness for the Swap category from GPT-4 was 0.67, while the\nIncrease and Decrease categories scored 0.60 and 0.49, respectively, presented in Table 7.\nFor LLaMA the overall average of correctness was 0.17, 0.33, and 0.57 for the Swap, Increase,\nand Decrease categories respectively, shown in Table 8. It is also worth noting that in Fig. 5a,\nb, and c, from the GPT-4 results, the deviation range in correctness averages increased from\nthe Swap category to the Increase category and then again to the Decrease category, with the\nsame pattern observed in the LLaMA results, seen in Figs. 6a, b, and c.\nGPT-4 performed well and consistently with correctness scores for Swap, underpinning\nthis LLMs ability to interpret the queries in this category. The Increase category was less\n123\n1310\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nFig. 13 Average BertScore F1 across the three query categories for GPT-4\nconsistent and, as a result, returned a drop in the overall average correctness, although some\nof the schedules matched the performance seen within the Swap category. With the Decrease\ncategory, there was consistently lower performance across all schedules, with the clear excep-\ntion of schedule 7 of 15 jx15m, which alone matched the level of performance of Swap.\nThe reduction in average correctness for Increase and Decrease query categories, from\nGPT-4, was most likely due to two things: initially, by requiring calculations to modify\nthe time by z minutes, and secondly, the openness of query 2 leaving room for different\ninterpretations for a generalist LLM. This brings the requirement on the LLM to understand\nthe queries and utilise deeper comprehension in analysing the schedule data to determine the\nfeasibility of the change. Additionally, with the Swap category, the queries were more direct\nand closed and may be resolved easily without calculations required of the schedule data,\nand therefore, a deep comprehension may not be required.\nInterestingly, the average correctness scores from LLaMA presented the reverse pattern,\nwith the Swap category returning the lowest scores, with improvements seen in the Increase\nand improved further in the Decrease category; the responses from LLaMA in the Decrease\ncategory outperformed GPT-4, the only area where this model performed better. This under-\npins the importance of assessing different LLMs, even without pre-training certain models\ncan perform better under certain conditions. LLaMAs capability with the Decrease query\n123\n1311\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nFig. 14 Average BertScore F1 across the three query categories for LLaMA-3.1\nTable 5 Average results across all categories for each schedule (GPT-4)\nCorrectness Cosine Similarity Response Completeness Word Count Bert Score f1\n15jx15m_1 0.62 0.8266 0.9593 54.4 0.6788\n15jx15m_2 0.62 0.8241 0.9545 64.9 0.6561\n15jx15m_3 0.63 0.8274 0.9436 64.7 0.6581\n15jx15m_4 0.57 0.8124 0.9439 58.8 0.6667\n15jx15m_5 0.61 0.8168 0.9321 60.7 0.6663\n15jx15m_6 0.61 0.8182 0.9485 56.1 0.6692\n15jx15m_7 0.62 0.8187 0.9443 58.7 0.6686\n20jx15m_1 0.52 0.8148 0.9522 57.7 0.6753\n20jx20m_1 0.55 0.8113 0.9487 60.2 0.6723\n30jx20m_1 0.51 0.7981 0.9462 60.6 0.6777\n123\n1312\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nTable 6 Average results across all categories for each schedule (LLaMA-3.1)\nCorrectness Cosine Similarity Response Completeness Word Count Bert Score f1\n15jx15m_1 0.41 0.8058 0.9511 62.8 0.6283\n15jx15m_2 0.38 0.8136 0.9536 63.1 0.6269\n15jx15m_3 0.28 0.8281 0.9575 61.0 0.6313\n15jx15m_4 0.33 0.8155 0.9576 58.6 0.6395\n15jx15m_5 0.45 0.8021 0.9491 59.0 0.6239\n15jx15m_6 0.39 0.7969 0.9540 61.2 0.6223\n15jx15m_7 0.35 0.8131 0.9584 59.7 0.6417\n20jx15m_1 0.28 0.8149 0.9531 66.6 0.6337\n20jx20m_1 0.35 0.8160 0.9584 61.2 0.6289\n30jx20m_1 0.33 0.7971 0.9576 59.5 0.6345\nTable 7 Average results for all categories across all schedules (GPT-4)\nCorrectness Cosine Similarity Response Completeness Word Count Bert Score f1\nSwap 0.67 0.8049 0.9479 63.9 0.6350\nIncrease 0.60 0.8287 0.9589 57.6 0.6798\nDecrease 0.49 0.8170 0.9353 57.6 0.6919\nOverall Total 0.59 0.8169 0.9473 59.7 0.6689\nTable 8 Average results for all categories across all schedules (LLaMA-3.1)\nCorrectness Cosine Similarity Response Completeness Word Count Bert Score f1\nSwap 0.17 0.8027 0.9489 59.8 0.6116\nIncrease 0.33 0.8159 0.9583 59.2 0.6504\nDecrease 0.57 0.8124 0.9580 64.8 0.6313\nOverall Total 0.35 0.8103 0.9551 61.3 0.6311\n123\n1313\nJournal of Intelligent Information Systems (2025) 63:1287–1337\ncategory should be explored further in future studies to better understand why these queries\nare better comprehended than others.\nFurthermore, in line with the reduction in overall correctness averages from GPT-4, and\nincreases seen with LLaMA, the increased complexity of the queries also introduced more\nvariability in the correctness scores, which signiﬁes that both LLMs are more inconsistent\nin comprehending the necessary data and details. This raises an important question: does the\nLLM understand the rules required for a job shop schedule? Pre-training or chain of thought\nprompting techniques may be able to boost the performance of generalist LLMs in these\ndomain-speciﬁc experiments and possibly narrow the gap in overall correctness.\nIt is worth noting, in Figs. 5da n d 6d when considering the collected average scores by\neach schedule, the density distribution is near-identical across all schedules, underlining that\nthe schedule size and complexity has little to no inﬂuence on a generalist LLM’s capabilities\nin accurately assessing scheduling data and queries.\n4.2.2 Average cosine similarity\nReviewing the overall average scores of cosine similarity Figs. 7a, b, c, 8a, b, and c show that\nthe density of scores for each query category was closely aligned for both LLMs, with only one\nindividual stand-out schedule. Schedule 5 of 15 jx15m for GPT-4, within the Increase query\ncategory, had a particularly narrow density, resulting in the clear separation from the other\nschedules, although this outcome was coincidental, was not matched by LLaMA, and does\nnot offer any insight into the operations of the LLM or the performance of other schedules\nor query categories.\nThe observed close alignment across the schedules and query categories, visualised in\nFigs. 7da n d 8d, demonstrates that both LLMs used very similar language in responses,\nregardless of the type of query or the size of the dataset.\n4.2.3 Average response completeness\nConsidering the average scores for completeness, as shown in Fig. 9a, b, and c for GPT-4, the\nscores achieved in each of the query categories were closely aligned on average, with total\naverages of 0.9479, 0.9589, and 0.9353 for Swap, Increase, and Decrease respectively, shown\nin Table 7. For LLaMA the total averages were 0.9489, 0.9583, and 0.9580 for Swap, Increase,\nand Decrease respectively, shown in Table 8, with the distribution plotted in Fig. 10a, b, and\nc.\nThe responses in the Swap and Increase query categories were the most consistent for\nGPT-4, with minimal variance and exceptional results, while the distribution of the LLaMA\nscores was very even across all schedules and query categories, collectively shown in Fig. 10d.\nThe responses returned in the Decrease query category, for GPT-4, had the most variance and\noutlying responses, which impacted the overall average, being the lowest of all the average\nscores, however, the completeness scores overall were consistent throughout the experiment\nfor all categories and schedule sizes, as shown in Fig. 9d for GPT-4, with a near-uniform\ndensity (excluding the observed outlier results). This proves that both LLMs were able to\ninterpret and return valid, relevant information, regardless of query type or data complexity,\neven when referencing domain-speciﬁc concepts.\n123\n1314\nJournal of Intelligent Information Systems (2025) 63:1287–1337\n4.2.4 Average word count\nLooking into the average word count scores for GPT-4, depicted in Fig. 11a, b and c, the\naverage word count for responses in the Swap query category was 64, with Increase and\nDecrease both returning 58 words on average; rounded up from results in Table 7.T h e\nresponses from LLaMA averaged 60, 59, and 65 for the Swap, Increase, and Decrease query\ncategories, respectively (after rounding), as shown in Table 8, with the distributions plotted\nin Fig. 12a, b, and c.\nIn addition to writing responses of similar size from both LLMs, there is also a shared\npattern of occasional outlying short responses, as seen through the schedules and query\ncategories. While assessing the word count by the schedule size presents a marginally larger\nvariance in length, as shown in Tables 5 and 6 and Figs. 11da n d 12d, these results provide\npredictability to the operation of both LLMs as users can expect to receive responses of similar\nlength, regardless of the style of query asked or the size of the data within the schedule.\n4.2.5 Average BertScore F1\nAssessing the average BertScore density plots in Figs. 13a, b, c, 14a, b, and c, there is a high\nlevel of consistency with the average BertScores across all schedules from both LLMs. In the\nSwap category, the GPT-4 responses achieved an average BertScore of 0.6350, with LLaMA\nachieving 0.6116, and the categories of Increase and Decrease achieved average scores of\n0.6798 and 0.6919 for GPT-4, and 0.6504 and 0.6313 from LLaMA respectively, detailed in\nTables 7 and 8.\nWith the best overall average BertScore being achieved by GPT-4 in the Decrease query\ncategory, it is evident that these responses best aligned with the human sample references,\nalthough there is no signiﬁcant difference in the performance across the query categories,\nwith LLaMA only scoring 5% lower overall. This is also the observation in Figs. 13da n d 14d,\nwhere there is a highly similar spread of scores across all schedule variants.\nGiven the generalist LLMs used in this experiment, these results are encouraging, as\nthe queries and data were specialist and domain-speciﬁc. A larger sample size of reference\nanswers would likely improve the observed BertScores, and this should be considered for\nany future experiments of this type.\n4.2.6 Performance discussion\nAs mentioned in Section 1, there is limited published research in this area of study at the\ntime of writing, which means there is no available data for direct comparison of performance\nresults. The two identiﬁed isolated examples with sufﬁcient similarities offer some insight\ninto how the experimental results of this paper compare to existing research.\nFor correctness measures (referred to as Accuracy in the referenced study), in the study\nassessing MaScQA, the GPT-4 model achieved accuracy scores of 60.15 for the zero-shot\napproach and 62.0 for the chain-of-thought method, which very closely aligns with the GPT-4\nresults generated through the experiment in this paper (overall average of 59) (Zaki et al.,\n2024). The narrow margin of difference is encouraging as the results generated for this paper\ndid not follow any extensive prompt optimisation techniques and, therefore, opens the oppor-\ntunity for further investigation. It should be expected that the LLaMA results would improve\nwith prompt optimisation or pre-training as well, as the overall results were considerably\nlower than GPT-4.\n123\n1315\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nAnother study focused on assessing the BertScore achieved by GPT models in answering\ndomain-speciﬁc queries on Microsoft products and technical IT problems. The results of the\nstudy returned an overall BertScore of 56.91 from the GPT-4 model, which is signiﬁcantly\nlower than both sets of results achieved in this paper (overall average of 0.6689 for GPT-4,\nand 0.6311 for LLaMA) (Yang et al., 2023). The performance demonstrated in this paper\nshows what can be achieved without dedicated pre-training and additive information to an\nLLM, underpinning the inherent capabilities of generalist LLMs and the approach introduced\nin this paper. Furthermore, the authors acknowledge that unintended bias may be introduced\nto a small, self-written set of reference material and that future studies should consider using\npublicly maintained reference material or one generated from a broader range of authors.\nNo suitable comparative research could be found for the completeness and cosine simi-\nlarity scores, and the limited value that could be derived from comparing the word count of\nresponses was recognised.\nThe results are encouraging for setting a solid performance basis from which more in-\ndepth or targetted research can build on. The model devised in this study can help form\nthe framework for enabling human-machine interactions or feedback to automated systems\nthrough an LLM (or integrated LM), which can add the introduction of AI solutions in task-\nbased scheduling industries, such as manufacturing, logistics, construction, and shift workers.\nIt is important to note that the involvement of human operators or workers is critical to the\nadoption and success of automated systems (as highlighted in Uhde et al. ( 2020)), which will\nfacilitate the correction or mitigation of ethical concerns in task assignment and scheduling,\navoiding such issues as worker overload or gender-bias.\n5 Conclusion\nThis paper focused on exploring the capabilities of generalist LLMs in answering queries,\nwith explanations, on a benchmark schedule to determine the potential for enabling trust in\nautomated systems for the future. Existing research exposed the limited number of studies\ninvestigating the use of generalist LLMs to advance the understanding of automated schedul-\ning systems and establish a means of XAI.\nBenchmark schedules were selected to create a baseline dataset of varying sizes and\ncomplexity derived from the Job Shop concept of scheduling to set out the novel experiment.\nQuery categories were deﬁned to challenge the LLMs with different temporal and logical\nconsiderations for swapping or modifying elements of the provided schedule datasets. A\nsingle, common prompt was designed to trigger the question-answer with a single example\nquery provided for each query category as a few-shot learning approach for the LLMs. The\nmethod of analysing the answer responses to the varied sizes of benchmark schedules was\nintroduced along with several assessment criteria calculating the number of correct responses,\nas well as the use of language within each response.\nThe results showed the GPT-4 was correct more often than not, with more inaccurate\nresponses from LLaMA, and the language used throughout the experiment was largely\nconcise, complete, consistent, and aligned to human interpretation. While there were clear\nﬂuctuations in the assessment of some of the LLMs responses, the performance of the LLMs\nwas not inﬂuenced by the size or complexity of the schedule datasets, highlighting the poten-\ntial for this approach to be introduced to real-world applications, such as construction planning\nor manufacturing scheduling, and much larger schedules. The performance is also encour-\naging as the generalist LLMs from this experiment were not pre-trained or supplemented\n123\n1316\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nwith specialist knowledge, presenting the opportunity for further enhancement of the success\nachieved in this paper.\nFuture and further studies should consider introducing prompt optimisation techniques\nto explore the potential of increasing overall correctness scores. Additionally, as in several\ncases, there were identiﬁable query and response pairs that performed exceptionally, either\nnegatively or positively, from the others in the same query category, which could be captured\nand introduced as a feedback loop to improve overall performance. Altering the model hyper-\nparameter settings could also impact the performance, as well as comparing the performance\nof additional alternative generalist LLM models. Furthermore, in the event of a domain-\nspeciﬁc, scheduling management focused LLM for development, the approach established\nin this paper should be investigated for performance differences and improvements. Finally,\ntesting this approach on a real-world application and data, integrated with an automated\nscheduling system, could directly prove the capability of AI for scheduling while providing\nin-built explainability and feedback loop to enable greater trust in wider adoption.\nAppendix A: Results for all the categories and query types\nA.1 Benchmark “yes” and “no” answers for each query category\nTable 9 Benchmark answers for each query category\nQuery\nCategory\nYES NO\nSwap 1. Yes, an exchange of the start times of job 7\nwith job 12 on machine 5 can take place. There\nwill not be any overlap or conﬂict with other\njobs and rescheduling would not be required.\n1. No, the exchange of start times for job 12\nand job 3 on machine 9 cannot be done as this\nwould cause a conﬂict with other jobs within\nthe schedule. To make this exchange possible\nit would be required to reschedule all activi-\nties.\n2. Yes, it is possible to exchange the end times\nof job 7 with job 12 on machine 5. There will\nnot be any overlap or conﬂict with other jobs\nand rescheduling would not be required.\n2. No, it is not possible to exchange the end\ntimes of job 6 with job 13 on machine 1 as\nthere would be a conﬂict with other jobs in\nthe schedule.\n3. Yes, it can be considered feasible to\nexchange jobs 9 and 14 on machine 12, as\nthere are no conﬂicts or overlaps that would\nprevent this from occurring.\n3. No, there is no feasible option in the cur-\nrent schedule to exchange jobs 11 and 2 on\nmachine 3. If the exchange took place there\nwould be overlaps with other jobs and would\nrequire a complete reschedule to ﬁnd a feasi-\nble solution.\n4. Yes, it appears possible to exchange job\n7 between machines 8 and 11. This will not\ncause any overlaps or scheduling conﬂicts\nwith other jobs or machines.\n4. No, it does not appear possible to exchange\njob 9 between machines 1 and 5, as doing\nso would cause a conﬂict with other jobs\nin the schedule. Jobs cannot overlap when\nbeing processed on machines and therefore\nthis exchange cannot be completed.\n123\n1317\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nTable 9 continued\nQuery\nCategory\nYES NO\n5. Yes, there is no reason the exchange of\nprocessing times of Job 6 on machine 12 with\nJob 9 on machine 9 cannot be completed, as\nthere are no identiﬁed conﬂicts preventing this\naction.\n5. No, this exchange of processing times\nbetween job 11 on machine 3 with job 2 on\nmachine 13 is not possible as this would lead\nto an overlap with other jobs in the sched-\nule and would require rescheduling in order\nto make this possible.\n6. Yes, it would be allowable and possible\nto swap the start times of jobs 8 and 4 on\nmachines 2 and 12 respectively. There are no\noverlaps with other jobs that would prevent\nthis from being possible.\n6. No, the start times of job 5 on machine 7\nand job 14 on machine 2 cannot be swapped as\nthis will cause overlap and conﬂict with other\njobs within the schedule. The schedule would\nneed to be completely modiﬁed to allow this\nto happen.\n7. Yes, swapping the end times of job 9 on\nmachine 11 with job 4 on machine 7 can be\ndone, as there are no issues with other jobs\nthat could stop this from happening.\n7. No, the swapping of the end times of job\n6 on machine 13 with job 15 on machine\n15 is not achievable due to the conﬂicts and\noverlaps this would trigger with other jobs\nwithin the schedule. A full reschedule would\nbe required to make this possible.\nIncrease 1. Yes, it would be possible to increase the\nduration of job 8 on machine 4 by 10 minutes\nas there is sufﬁcient slack in the schedule to\nallow this without issue.\n1. No, it wouldn’t be possible to increase the\nduration of job 3 on machine 14 by 11 min-\nutes as this would cause an overlap with the\njobs starting later on this machine and would\ntherefore require a complete reschedule.\n2. Yes, the overall running time of machine\n7 can be increased by 12 minutes without\nimpacting the overall scheduled completion\ntime, as its increased ﬁnishing time does not\nexceed the scheduled completion time.\n2. No, the overall schedule run time will be\nimpacted by increasing the running time of\nmachine 13, as this will exceed the current\nschedule completion time and therefore the\nincrease is not possible.\n3. Yes, it is possible for the start time of job\n9 on machine 14 to be increased by 20 min-\nutes as this increase does not affect the start or\ncompletion of other jobs within the schedule.\n3. No, its impossible to increase the start of\njob 11 on machine 1 as the consequence of\nthis would trigger conﬂicts and overlaps with\nother jobs within the schedule and therefore a\ncomplete reschedule would be required.\n4. Yes, it is feasible to increase the end time of\njob 5 on machine 6 by 12 minutes as there is\nadequate capacity for the schedule to tolerate\nthis without requiring a complete reschedule.\n4. No, increasing the end time of job 2 on\nmachine 3 is not feasible as this would con-\nﬂict with the start time of other jobs within the\nschedule and would therefore require a com-\nplete reschedule to satisfy this requirement.\nDecrease 1. Yes, there is the possibility to decrease the\nduration of job 8 on machine 5 by 13 minutes\nas this will not cause any conﬂict with other\njobs or breach scheduling rules.\n1. No, the duration of job 10 on machine 15\ncannot be decreased by 16 minutes as this will\ncause the job to breach scheduling rules or\nconﬂict with other jobs within the schedule.\n2. Yes, the overall run time of machine 7 can\nbe reduced by 17 minutes without impacting\nthe overall schedule, as this change keeps the\nmaximum schedule run time the same.\n2. No, this is not possible as the overall\nschedule run time is impacted by reducing the\noverall running time of machine 9 by 16 min-\nutes and therefore cannot be achieved without\na complete reschedule.\n123\n1318\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nTable 9 continued\nQuery\nCategory\nYES NO\n3. Yes, the start time of job 2 on machine 14\ncan be reduced by 18 minutes, as this does not\ncause any overlap with existing jobs nor break\nany of the scheduling rules.\n3. No, the start time of job 18 on machine 3\ncannot be reduced by 16 minutes as this would\ncause an overlap with an existing job or breach\nthe scheduling rules.\n4. Yes, it would be possible to reduce the end\ntime of job 17 on machine 12 by 14 minutes\nas this will not have an impact on any other\njobs or the operation of the schedule overall.\n4. No, the end time of job 15 on machine 6\ncannot be reduced by 14 minutes due to this\nbreaching the scheduling rules deﬁned for the\nproblem.\nA.2 Results for the Swap category and all questions\nTable 10 Average results for the Swap category 15jx15m all schedules for GPT-4\n# Correctness Cosine\nSimilarity\nResponse\nCompleteness\nWord\nCount\nBert\nScore f1\n15jx15m schedule 1\n1 0.6 0.7827 0.9600 59.2 0.6085\n2 0.8 0.8169 0.9482 68.2 0.6726\n3 0.4 0.7818 0.9305 50.0 0.6600\n4 0.6 0.7926 0.9425 62.4 0.6111\n5 0.8 0.8300 0.9631 52.8 0.6711\n60 . 8 0.8308 0.9682 65.2 0.6288\n7 1.0 0.8295 0.9670 67.2 0.6066\nAvg 0.71 0.8092 0.9542 60.7 0.6370\n15jx15m schedule 2\n1 0.6 0.8074 0.9593 57.0 0.6801\n2 0.6 0.8289 0.9501 71.0 0.6188\n3 0.8 0.7610 0.9228 64.8 0.5861\n4 0.4 0.8086 0.9320 63.2 0.6324\n5 0.2 0.7847 0.9585 58.6 0.6514\n60 . 6 0.8751 0.9733 68.4 0.6310\n7 1.0 0.8231 0.9584 74.2 0.6281\nAvg 0.60 0.8127 0.9506 65.3 0.6326\n15jx15m schedule 3\n1 0.4 0.8380 0.9614 54.6 0.6604\n2 1.0 0.8000 0.9441 72.2 0.5953\n3 0.8 0.7774 0.9149 77.4 0.5721\n4 0.8 0.7428 0.9237 78.6 0.6062\n5 0.4 0.8425 0.9563 69.8 0.6149\n6 1.0 0.8235 0.9740 70.6 0.6139\n123\n1319\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nTable 10 continued\n# Correctness Cosine\nSimilarity\nResponse\nCompleteness\nWord\nCount\nBert\nScore f1\n70 . 8 0.8595 0.9299 81.2 0.6133\nAvg 0.74 0.8120 0.9435 72.1 0.6109\n15jx15m schedule 4\n1 0.8 0.8279 0.9610 63.2 0.5962\n2 1.0 0.8278 0.9527 72.0 0.6478\n3 0.8 0.8010 0.9154 63.0 0.6214\n4 1.0 0.7924 0.9376 62.4 0.6104\n50 . 4 0.8420 0.9698 45.2 0.6712\n6 0.6 0.8130 0.9313 60.8 0.6301\n7 1.0 0.8404 0.9659 77.6 0.6062\nAvg 0.80 0.8206 0.9477 63.5 0.6262\n15jx15m schedule 5\n1 0.6 0.8040 0.9537 59.4 0.6417\n2 1.0 0.8228 0.9550 72.0 0.6721\n3 0.8 0.7990 0.9207 74.4 0.5621\n4 0.6 0.7460 0.9430 54.6 0.6607\n5 0.4 0.8080 0.9620 60.2 0.6464\n6 0.4 0.8194 0.9722 59.0 0.6653\n7 1.0 0.8218 0.8153 71.0 0.6285\nAvg 0.69 0.8030 0.9317 64.4 0.6395\n15jx15m schedule 6\n1 0.6 0.8096 0.9640 58.4 0.6369\n2 0.8 0.8097 0.9567 65.2 0.6548\n30 . 8 0.8258 0.9264 74.4 0.5651\n4 0.4 0.7981 0.9490 48.6 0.6583\n5 0.6 0.7703 0.9670 53.2 0.6544\n6 1.0 0.8201 0.9627 63.4 0.6163\n7 1.0 0.8056 0.9527 80.8 0.5848\nAvg 0.74 0.8056 0.9541 63.4 0.6244\n15jx15m schedule 7\n10 . 6 0.8443 0.9579 50.4 0.6473\n2 0.8 0.7771 0.9389 70.2 0.6620\n3 0.4 0.7765 0.9259 57.4 0.6572\n4 0.2 0.8326 0.9128 49.2 0.6802\n5 0.6 0.7626 0.9681 49.0 0.6490\n6 0.4 0.8123 0.9734 61.6 0.6452\n7 1.0 0.8249 0.9644 79.2 0.6205\nAvg 0.57 0.8043 0.9488 59.6 0.6516\n123\n1320\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nTable 11 Average results for the Swap category 15jx15m all schedules for LLaMA-3.1\n# Correctness Cosine\nSimilarity\nResponse\nCompleteness\nWord\nCount\nBert\nScore f1\n15jx15m schedule 1\n1 0.2 0.7999 0.9604 50.2 0.6727\n2 0.0 0.7719 0.9025 80.2 0.6057\n3 0.2 0.7981 0.9381 64.4 0.5872\n4 0.2 0.8087 0.9161 64.4 0.5493\n5 0.4 0.8572 0.9736 39.2 0.6751\n6 0.2 0.7776 0.9566 62.0 0.6212\n7 0.0 0.8216 0.9579 71.4 0.5371\nAvg 0.17 0.8050 0.9436 61.7 0.6069\n15jx15m schedule 2\n1 0.0 0.8190 0.9600 63.0 0.6165\n20 . 2 0.8443 0.9625 70.4 0.5843\n3 0.2 0.7926 0.9310 62.0 0.6005\n4 0.0 0.8175 0.9029 61.0 0.5850\n5 0.4 0.7821 0.9580 69.8 0.5681\n6 0.4 0.8310 0.9596 54.8 0.6156\n7 0.2 0.7990 0.9649 74.4 0.5932\nAvg 0.20 0.8122 0.9484 65.1 0.5947\n15jx15m schedule 3\n1 0.0 0.8092 0.9586 65.4 0.6057\n2 0.0 0.8291 0.9544 80.4 0.5822\n3 0.0 0.7926 0.9310 53.4 0.6358\n4 0.0 0.7647 0.9179 60.2 0.6203\n5 0.4 0.8590 0.9676 54.8 0.6163\n6 0.2 0.7628 0.9266 48.2 0.6700\n7 0.0 0.8122 0.9518 57.4 0.6232\nAvg 0.09 0.8042 0.9440 60.0 0.6219\n15jx15m schedule 4\n1 0.2 0.8171 0.9662 34.8 0.6843\n2 0.0 0.8478 0.9633 67.0 0.6127\n3 0.0 0.8126 0.9540 43.6 0.6845\n4 0.0 0.8108 0.9242 66.6 0.6048\n5 0.6 0.8297 0.9609 57.2 0.5912\n60 . 2 0.8623 0.9724 55.8 0.6183\n7 0.0 0.7999 0.9680 61.6 0.6175\nAvg 0.14 0.8257 0.9584 55.2 0.6305\n123\n1321\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nTable 11 continued\n# Correctness Cosine\nSimilarity\nResponse\nCompleteness\nWord\nCount\nBert\nScore f1\n15jx15m schedule 5\n1 0.2 0.8139 0.9591 56.4 0.6510\n2 0.4 0.8423 0.9638 65.6 0.5982\n3 0.0 0.8257 0.9516 52.8 0.6046\n4 0.2 0.7264 0.9221 53.4 0.5971\n5 0.6 0.8275 0.9715 44.6 0.6643\n6 0.6 0.7147 0.8734 60.8 0.6019\n70 . 0 0.8500 0.9639 73.2 0.5640\nAvg 0.29 0.8001 0.9436 58.1 0.6116\n15jx15m schedule 6\n1 0.0 0.7909 0.9608 43.0 0.6479\n2 0.0 0.7829 0.9480 72.4 0.5712\n3 0.0 0.7548 0.9339 59.4 0.5795\n4 0.0 0.7276 0.9054 62.4 0.5422\n5 0.4 0.7924 0.9799 34.4 0.6718\n60 . 2 0.8326 0.9727 45.0 0.6624\n7 0.2 0.7408 0.9533 62.8 0.5574\nAvg 0.11 0.7746 0.9506 54.2 0.6046\n15jx15m schedule 7\n10 . 2 0.8445 0.9694 41.0 0.6662\n2 0.6 0.8294 0.9620 74.4 0.5703\n3 0.2 0.7561 0.9252 65.6 0.6183\n4 0.4 0.7994 0.9097 55.8 0.6185\n5 0.4 0.7722 0.9702 56.2 0.6282\n6 0.2 0.8050 0.9673 52.2 0.6334\n7 0.0 0.7955 0.9631 59.4 0.6279\nAvg 0.29 0.8003 0.9524 57.8 0.6233\n123\n1322\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nTable 12 Average results for the Swap category for 20jx15m, 20jx20m and 30jx20m for GPT-4\n# Correctness Cosine\nSimilarity\nResponse\nCompleteness\nWord\nCount\nBert\nScore f1\n20jx15m\n10 . 4 0.8527 0.9698 37.0 0.6957\n2 1.0 0.8176 0.9456 74.0 0.6510\n3 0.6 0.7662 0.9302 55.2 0.6266\n4 0.4 0.7486 0.9450 65.4 0.5990\n5 0.0 0.8104 0.9704 53.8 0.6514\n6 0.6 0.8167 0.9686 67.2 0.6509\n7 1.0 0.8386 0.8531 66.6 0.6294\nAvg 0.57 0.8073 0.9404 59.9 0.6434\n20jx20m\n1 0.4 0.8017 0.9612 59.2 0.6497\n2 0.6 0.7709 0.9551 59.2 0.6377\n3 0.6 0.7613 0.9371 59.8 0.6100\n4 0.6 0.7302 0.9379 57.8 0.6534\n5 0.6 0.8242 0.9609 69.0 0.6234\n6 1.0 0.8394 0.9710 74.8 0.6114\n7 1.0 0.7932 0.9594 81.6 0.6303\nAvg 0.69 0.7887 0.9547 65.9 0.6308\n30jx20m\n1 0.8 0.7872 0.9512 68.0 0.6153\n20 . 8 0.8102 0.9454 63.8 0.7231\n3 0.6 0.7535 0.9399 58.0 0.6353\n4 0.6 0.7417 0.9393 62.0 0.6713\n5 0.4 0.7949 0.9714 57.4 0.6499\n6 0.2 0.8040 0.9683 57.8 0.6633\n7 1.0 0.8054 0.9580 82.4 0.6176\nAvg 0.63 0.7853 0.9533 64.2 0.6537\n123\n1323\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nTable 13 Average results for the Swap category for 20jx15m, 20jx20m and 30jx20m for LLaMA-3.1\n# Correctness Cosine\nSimilarity\nResponse\nCompleteness\nWord\nCount\nBert\nScore f1\n20jx15m\n1 0.0 0.8284 0.9547 52.8 0.6430\n2 0.0 0.8516 0.9537 73.0 0.5414\n3 0.0 0.7884 0.9257 67.8 0.5702\n4 0.0 0.7794 0.8911 64.6 0.5971\n5 0.0 0.8088 0.9725 47.2 0.6576\n6 0.0 0.8390 0.9723 57.2 0.6443\n7 0.0 0.7979 0.9237 71.0 0.5509\nAvg 0.00 0.8133 0.9419 61.9 0.6006\n20jx20m\n1 0.2 0.7636 0.9592 63.4 0.6438\n2 0.6 0.8165 0.9608 68.4 0.5780\n3 0.2 0.8061 0.9327 64.6 0.5445\n4 0.2 0.7802 0.9082 69.0 0.6168\n5 0.4 0.8233 0.9666 50.8 0.6376\n60 . 0 0.8256 0.9720 57.0 0.6181\n7 0.2 0.7885 0.9618 68.4 0.6138\nAvg 0.26 0.8006 0.9516 63.1 0.6075\n30jx20m\n1 0.2 0.7734 0.9602 57.4 0.6357\n2 0.2 0.8279 0.9627 60.0 0.6327\n3 0.0 0.7565 0.9396 60.0 0.5856\n4 0.0 0.7503 0.9191 76.4 0.5795\n5 0.2 0.8232 0.9702 43.6 0.6538\n6 0.2 0.7981 0.9630 63.8 0.6080\n7 0.2 0.8057 0.9647 66.0 0.6048\nAvg 0.14 0.7907 0.9542 61.0 0.6143\n123\n1324\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nA.3 Results for the increase category and all questions\nTable 14 Average results for the Increase category 15jx15m all schedules for GPT-4\n# Correctness Cosine\nSimilarity\nResponse\nCompleteness\nWord\nCount\nBert\nScore f1\n15jx15m schedule 1\n1 1.0 0.8489 0.9707 49.0 0.7076\n2 0.4 0.8146 0.9567 56.0 0.6582\n30 . 2 0.8915 0.9615 51.0 0.6796\n4 0.6 0.8567 0.9575 59.4 0.6817\nAvg 0.55 0.8529 0.9616 53.9 0.6818\n15jx15m schedule 2\n1 0.8 0.8313 0.9512 62.0 0.6666\n2 0.4 0.7833 0.9534 60.4 0.6540\n3 1.0 0.8580 0.9610 73.8 0.6241\n4 0.6 0.8547 0.9514 67.0 0.6303\nAvg 0.70 0.8319 0.9543 65.8 0.6437\n15jx15m schedule 3\n1 0.6 0.8183 0.9601 49.8 0.7426\n20 . 6 0.8682 0.9651 67.0 0.6700\n3 0.8 0.8507 0.9741 35.4 0.7202\n4 0.6 0.8385 0.9521 74.6 0.6200\nAvg 0.65 0.8439 0.9628 56.7 0.6882\n15jx15m schedule 4\n1 0.4 0.8181 0.9537 59.8 0.6636\n2 0.4 0.8040 0.9548 62.8 0.6742\n30 . 2 0.8556 0.9674 47.2 0.7361\n4 0.6 0.8451 0.9588 63.0 0.6586\nAvg 0.40 0.8307 0.9587 58.2 0.6831\n15jx15m schedule 5\n1 1.0 0.8338 0.9748 38.4 0.7758\n2 0.4 0.8021 0.9655 73.8 0.6407\n30 . 8 0.8353 0.9585 57.6 0.6498\n4 0.8 0.8302 0.9452 67.2 0.6446\nAvg 0.75 0.8254 0.9610 59.3 0.6777\n15jx15m schedule 6\n1 0.8 0.8504 0.9614 35.0 0.7752\n2 0.4 0.7640 0.9543 51.6 0.6689\n3 0.6 0.8340 0.9647 49.8 0.6777\n4 0.8 0.8765 0.9606 67.6 0.6371\nAvg 0.65 0.8312 0.9602 51.0 0.6897\n123\n1325\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nTable 14 continued\n# Correctness Cosine\nSimilarity\nResponse\nCompleteness\nWord\nCount\nBert\nScore f1\n15jx15m schedule 7\n1 0.6 0.8456 0.9524 54.4 0.6935\n2 0.0 0.7803 0.9575 59.0 0.6637\n30 . 4 0.8714 0.9553 57.4 0.6687\n4 1.0 0.8356 0.9494 72.4 0.6206\nAvg 0.50 0.8332 0.9536 60.8 0.6616\nTable 15 Average results for the Increase category 15jx15m all schedules for LLaMA-3.1\n# Correctness Cosine\nSimilarity\nResponse\nCompleteness\nWord\nCount\nBert\nScore f1\n15jx15m schedule 1\n1 0.0 0.8179 0.9620 61.6 0.6366\n2 0.6 0.7216 0.9290 76.4 0.6021\n30 . 4 0.8970 0.9678 62.4 0.6286\n4 0.8 0.7895 0.9454 55.4 0.6526\nAvg 0.45 0.8065 0.9511 64.0 0.6300\n15jx15m schedule 2\n1 0.6 0.7938 0.9503 58.4 0.6382\n2 0.6 0.7759 0.9567 53.4 0.6813\n3 0.2 0.8460 0.9674 51.2 0.6988\n40 . 0 0.8620 0.9514 56.6 0.6029\nAvg 0.35 0.8194 0.9564 54.9 0.6553\n15jx15m schedule 3\n1 0.2 0.8297 0.9725 60.0 0.6595\n20 . 2 0.8563 0.9697 56.4 0.6415\n3 0.0 0.8477 0.9703 50.2 0.7090\n4 0.6 0.8154 0.9453 57.4 0.6288\nAvg 0.25 0.8373 0.9645 56.0 0.6597\n15jx15m schedule 4\n1 0.4 0.8526 0.9693 45.4 0.7102\n2 0.6 0.7926 0.9495 72.6 0.6489\n30 . 0 0.8557 0.9633 65.2 0.6459\n4 0.4 0.7970 0.9557 50.4 0.6290\nAvg 0.35 0.8245 0.9595 58.4 0.6585\n123\n1326\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nTable 15 continued\n# Correctness Cosine\nSimilarity\nResponse\nCompleteness\nWord\nCount\nBert\nScore f1\n15jx15m schedule 5\n1 0.2 0.8181 0.9710 47.8 0.6713\n2 0.4 0.7739 0.9602 67.8 0.5981\n3 0.6 0.8357 0.9608 52.4 0.6810\n4 0.4 0.8011 0.9507 55.6 0.6627\nAvg 0.40 0.8072 0.9607 55.9 0.6533\n15jx15m schedule 6\n1 0.2 0.8252 0.9592 59.8 0.6123\n2 0.8 0.7900 0.9538 63.0 0.6204\n3 0.2 0.7968 0.9555 61.8 0.6708\n40 . 0 0.8317 0.9471 64.2 0.6010\nAvg 0.3 0.8109 0.9539 62.2 0.6261\n15jx15m schedule 7\n1 0.4 0.8324 0.9661 48.4 0.6787\n2 0.8 0.7985 0.9612 69.0 0.6134\n30 . 0 0.8502 0.9594 62.4 0.6849\n4 0.0 0.8189 0.9542 53.0 0.6519\nAvg 0.30 0.8250 0.9602 58.2 0.6572\nTable 16 Average results for the Increase category for 20jx15m, 20jx20m and 30jx20m for GPT-4\n# Correctness Cosine\nSimilarity\nResponse\nCompleteness\nWord\nCount\nBert\nScore f1\n20jx15m\n1 0.6 0.7880 0.9626 56.4 0.7113\n2 0.6 0.8071 0.9586 64.4 0.6539\n3 0.8 0.8367 0.9616 56.2 0.6778\n4 0.4 0.8243 0.9414 73.4 0.6769\nAvg 0.6 0.8140 0.9560 62.6 0.6800\n20jx20m\n1 0.8 0.8073 0.9635 55.2 0.6986\n2 0.2 0.7911 0.9674 57.4 0.6779\n30 . 6 0.8698 0.9657 45.2 0.7148\n4 1.0 0.7970 0.9482 62.2 0.6983\nAvg 0.65 0.8163 0.9612 55.0 0.6974\n30jx20m\n1 1.0 0.8214 0.9657 48.4 0.7555\n123\n1327\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nTable 16 continued\n# Correctness Cosine\nSimilarity\nResponse\nCompleteness\nWord\nCount\nBert\nScore f1\n2 0.4 0.7704 0.9646 44.8 0.6813\n30 . 4 0.8382 0.9586 58.6 0.6550\n4 0.4 0.7989 0.9470 58.4 0.6863\nAvg 0.55 0.8072 0.9589 52.6 0.6945\nTable 17 Average results for the Increase category for 20jx15m, 20jx20m and 30jx20m for LLaMA-3.1\n# Correctness Cosine\nSimilarity\nResponse\nCompleteness\nWord\nCount\nBert\nScore f1\n20jx15m\n1 0.4 0.8000 0.9671 64.0 0.6345\n2 0.4 0.8156 0.9638 73.0 0.6772\n3 0.2 0.8323 0.9585 69.6 0.6739\n40 . 0 0.8407 0.9461 75.8 0.6258\nAvg 0.25 0.8221 0.9589 70.6 0.6529\n20jx20m\n1 0.2 0.8048 0.9629 62.0 0.6392\n2 0.8 0.7653 0.9607 73.8 0.6043\n30 . 2 0.8610 0.9725 44.2 0.6903\n4 0.0 0.7972 0.9416 50.2 0.6649\nAvg 0.30 0.8071 0.9594 57.6 0.6497\n30jx20m\n1 0.2 0.8333 0.9742 40.4 0.7343\n2 0.4 0.8006 0.9674 55.6 0.6579\n3 0.4 0.8349 0.9621 56.8 0.6492\n4 0.4 0.7257 0.9309 62.6 0.6047\nAvg 0.35 0.7986 0.9586 53.9 0.6615\n123\n1328\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nA.4 Results for the decrease category and all questions\nTable 18 Average results for the Decrease category 15jx15m all schedules for GPT-4\n# Correctness Cosine\nSimilarity\nResponse\nCompleteness\nWord\nCount\nBert\nScore f1\n15jx15m schedule 1\n1 0.6 0.8120 0.9608 43.2 0.7437\n2 0.2 0.7668 0.9651 52.6 0.6770\n3 1.0 0.8386 0.9478 49.2 0.7222\n40 . 6 0.8529 0.9740 49.4 0.7282\nAvg 0.60 0.8176 0.9619 48.6 0.7178\n15jx15m schedule 2\n1 0.4 0.8028 0.9589 51.0 0.7308\n2 0.2 0.8205 0.9670 77.4 0.6338\n3 1.0 0.8606 0.9436 60.6 0.6870\n4 0.6 0.8278 0.9646 65.6 0.7163\nAvg 0.55 0.8279 0.9585 63.7 0.6920\n15jx15m schedule 3\n1 0.2 0.7991 0.8327 54.0 0.6915\n2 0.4 0.8044 0.9605 78.4 0.6347\n3 1.0 0.8238 0.9414 64.2 0.6809\n40 . 4 0.8783 0.9639 64.8 0.6934\nAvg 0.50 0.8264 0.9246 65.4 0.6751\n15jx15m schedule 4\n1 0.4 0.7861 0.8371 51.0 0.7013\n2 0.2 0.7592 0.9646 50.6 0.6960\n3 1.0 0.8137 0.9405 50.0 0.7150\n4 0.4 0.7842 0.9589 66.8 0.6506\nAvg 0.50 0.7858 0.9252 54.6 0.6907\n15jx15m schedule 5\n1 0.2 0.7691 0.7342 39.6 0.7102\n2 0.0 0.8259 0.9653 80.2 0.6178\n3 1.0 0.8686 0.9507 52.0 0.7196\n4 0.4 0.8246 0.9649 62.2 0.6786\nAvg 0.40 0.8220 0.9038 58.5 0.6815\n15jx15m schedule 6\n1 0.4 0.8011 0.8409 47.8 0.7155\n2 0.2 0.7955 0.9687 61.8 0.6525\n3 0.8 0.8275 0.9422 59.4 0.6866\n40 . 4 0.8477 0.9730 47.0 0.7193\nAvg 0.45 0.8179 0.9312 54.0 0.6935\n123\n1329\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nTable 18 continued\n# Correctness Cosine\nSimilarity\nResponse\nCompleteness\nWord\nCount\nBert\nScore f1\n15jx15m schedule 7\n1 0.6 0.7770 0.8405 41.6 0.7400\n2 0.8 0.8250 0.9652 56.0 0.6669\n3 1.0 0.8497 0.9493 60.2 0.6874\n4 0.8 0.8230 0.9664 65.2 0.6759\nAvg 0.80 0.8187 0.9303 55.8 0.6926\nTable 19 Average results for the Decrease category 15jx15m all schedules for LLaMA-3.1\n# Correctness Cosine\nSimilarity\nResponse\nCompleteness\nWord\nCount\nBert\nScore f1\n15jx15m schedule 1\n1 0.8 0.8207 0.9662 56.6 0.6471\n2 0.6 0.7796 0.9593 66.4 0.6713\n3 0.2 0.8118 0.9530 62.2 0.6564\n4 0.8 0.8113 0.9564 66.4 0.6169\nAvg 0.60 0.8058 0.9587 62.9 0.6479\n15jx15m schedule 2\n1 0.8 0.7917 0.9536 70.0 0.6548\n2 0.4 0.8054 0.9645 75.0 0.5981\n30 . 4 0.8346 0.9423 66.4 0.6118\n4 0.8 0.8051 0.9633 65.8 0.6581\nAvg 0.60 0.8092 0.9559 69.3 0.6307\n15jx15m schedule 3\n1 0.4 0.8161 0.9727 60.2 0.6394\n2 0.8 0.8386 0.9661 67.4 0.5927\n30 . 0 0.8591 0.9546 66.8 0.6422\n4 0.8 0.8578 0.9626 73.2 0.5753\nAvg 0.50 0.8429 0.9640 66.9 0.6124\n15jx15m schedule 4\n1 0.8 0.7986 0.9617 53.2 0.6292\n2 0.6 0.7832 0.9564 71.4 0.6361\n30 . 0 0.8508 0.9574 60.2 0.6345\n4 0.6 0.7527 0.9444 63.4 0.6184\nAvg 0.50 0.7963 0.9550 62.1 0.6296\n123\n1330\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nTable 19 continued\n# Correctness Cosine\nSimilarity\nResponse\nCompleteness\nWord\nCount\nBert\nScore f1\n15jx15m schedule 5\n1 1.0 0.7593 0.8927 64.0 0.6410\n2 0.8 0.7845 0.9646 62.0 0.5492\n30 . 0 0.8543 0.9576 50.0 0.6703\n4 0.8 0.7984 0.9574 75.8 0.5672\nAvg 0.65 0.7991 0.9431 63.0 0.6069\n15jx15m schedule 6\n1 1.0 0.8174 0.9572 61.0 0.6308\n2 1.0 0.7867 0.9596 69.6 0.6398\n3 0.2 0.8172 0.9567 60.2 0.6400\n4 0.8 0.7993 0.9566 77.8 0.6340\nAvg 0.75 0.8051 0.9575 67.2 0.6362\n15jx15m schedule 7\n1 0.6 0.8062 0.9702 47.6 0.6710\n2 0.2 0.8160 0.9623 74.6 0.6413\n30 . 2 0.8494 0.9631 53.2 0.6584\n4 0.8 0.7846 0.9551 76.6 0.6078\nAvg 0.45 0.8141 0.9627 63.0 0.6446\nTable 20 Average results for the Decrease category for 20jx15m, 20jx20m and 30jx20m for GPT 4\n# Correctness Cosine\nSimilarity\nResponse\nCompleteness\nWord\nCount\nBert\nScore f1\n20jx15m\n1 0.2 0.8341 0.9545 40.6 0.7033\n2 0.0 0.7824 0.9688 49.4 0.6852\n3 0.8 0.8564 0.9528 46.8 0.7478\n4 0.6 0.8196 0.9643 66.2 0.6734\nAvg 0.40 0.8231 0.9601 50.8 0.7024\n20jx20m\n1 0.2 0.8052 0.8492 38.8 0.7107\n2 0.0 0.8004 0.9621 73.6 0.6494\n3 0.6 0.8349 0.9422 72.8 0.6897\n40 . 4 0.8753 0.9681 54.0 0.7052\nAvg 0.30 0.8290 0.9304 59.8 0.6887\n30jx20m\n1 0.4 0.7849 0.8473 62.4 0.7044\n2 0.2 0.7770 0.9589 70.0 0.6610\n123\n1331\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nTable 20 continued\n# Correctness Cosine\nSimilarity\nResponse\nCompleteness\nWord\nCount\nBert\nScore f1\n3 0.6 0.8135 0.9426 56.2 0.7138\n40 . 2 0.8318 0.9656 71.2 0.6607\nAvg 0.35 0.8018 0.9263 65.0 0.6850\nTable 21 Average results for the Decrease category for 20jx15m, 20jx20m and 30jx20m for LLaMA-3.1\n# Correctness Cosine\nSimilarity\nResponse\nCompleteness\nWord\nCount\nBert\nScore f1\n20jx15m\n1 0.6 0.8184 0.9659 66.2 0.6528\n2 1.0 0.7590 0.9522 72.0 0.6426\n30 . 0 0.8420 0.9519 68.8 0.6652\n4 0.8 0.8173 0.9641 62.6 0.6303\nAvg 0.60 0.8092 0.9585 67.4 0.6477\n20jx20m\n1 0.8 0.8354 0.9676 54.4 0.6253\n2 0.6 0.8180 0.9660 70.6 0.6453\n30 . 0 0.8657 0.9614 58.2 0.6437\n4 0.6 0.8424 0.9621 69.0 0.6032\nAvg 0.50 0.8404 0.9643 63.1 0.6294\n30jx20m\n1 0.8 0.8242 0.9648 66.8 0.6291\n2 0.4 0.7836 0.9637 68.2 0.6073\n3 0.0 0.8141 0.9505 67.0 0.6195\n4 0.8 0.7857 0.9608 52.0 0.6544\nAvg 0.50 0.8019 0.9600 63.5 0.6276\nAcknowledgements This study was half-funded by ESA under the OSIP Co-Sponsored PhD activity: “Robust\nand Explainable Mission Planning and Scheduling (REMPS)” No. 4000132894/20/NL/MH/hm. The authors\nwould also like to acknowledge the support of ESA through the Visiting Researcher program. Finally, they\nwish to warmly thank Dimitris Kardaris (ESA) for all the insightful discussion about mission operations\nconstraints and opportunities for explainability.\nAuthor Contributions C.P . made substantial contributions to the conception or design of the work; the acqui-\nsition, analysis, and interpretation of data; wrote and reviewed the manuscript and prepared all ﬁgures and\ntables. A.R. assisted in the conceptualization of the work and reviewed the manuscript.\nData Availability No datasets were generated or analysed during the current study.\nDeclarations\nCompeting Interests The authors declare no competing interests.\n123\n1332\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,\nand indicate if changes were made. The images or other third party material in this article are included in the\narticle’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is\nnot included in the article’s Creative Commons licence and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\nTo view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .\nReferences\nAl Homssi, B., Dakic, K., Wang, K., et al. (2024). Artiﬁcial intelligence techniques for next-generation massive\nsatellite networks. IEEE Communications Magazine, 62(4), 66–72. https://doi.org/10.1109/MCOM.004.\n2300277\nAli, S., Abuhmed, T., El-Sappagh, S., et al. (2023). Explainable artiﬁcial intelligence (xai): What we know\nand what is left to attain trustworthy artiﬁcial intelligence. Information Fusion, 99(101), 805. https://doi.\norg/10.1016/j.inffus.2023.101805\nAmer, F., Hockenmaier, J., & Golparvar-Fard, M. (2022). Learning and critiquing pairwise activity relation-\nships for schedule quality control via deep learning-based natural language processing. Automation in\nConstruction, 134(104), 036. https://doi.org/10.1016/j.autcon.2021.104036\nAmer, F., Koh, H. Y ., & Golparvar-Fard, M. (2021). Automated methods and systems for construction planning\nand scheduling: Critical review of three decades of research. Journal of Construction Engineering and\nManagement, 147(7), 03121002. https://doi.org/10.1061/(asce)co.1943-7862.0002093\nArrieta, A. B., Díaz-Rodríguez, N., Del Ser, J., et al. (2020). Explainable artiﬁcial intelligence (xai): Concepts,\ntaxonomies, opportunities and challenges toward responsible ai. Information Fusion, 58, 82–115. https://\ndoi.org/10.1016/j.inffus.2019.12.012\nAtsmony, M., & Mosheiov, G. (2022). Scheduling to maximize the weighted number of on-time jobs on\nparallel machines with bounded job-rejection. Journal of Scheduling, 26 (2), 193–207. https://doi.org/\n10.1007/s10951-022-00745-7\nBach, S., Binder, A., Montavon, G., et al. (2015). On pixel-wise explanations for non-linear classiﬁer decisions\nby layer-wise relevance propagation. PLoS ONE, 10(7), e0130140. https://doi.org/10.1371/journal.pone.\n0130140\nBastola, A., Wang, H., Hembree, J., et al. (2023). Llm-based smart reply (lsr): Enhancing collaborative\nperformance with chatgpt-mediated smart reply system.https://doi.org/10.48550/arXiv.2306.11980\nBen Abdallah, E., Grati, R., & Boukadi, K. (2023). Towards an explainable irrigation scheduling approach by\npredicting soil moisture and evapotranspiration via multi-target regression. Journal of Ambient Intelli-\ngence and Smart Environments, 1–22,. https://doi.org/10.3233/AIS-220477\nBernardi, M. L., Casciani, A., Cimitile, M., et al. (2024). Conversing with business process-aware large\nlanguage models: the bpllm framework. Journal of Intelligent Information Systems, 62(6), 1607–1629.\nhttps://doi.org/10.1007/s10844-024-00898-1\nBrinkkötter, W., & Brucker, P . (2001). Solving open benchmark instances for the job-shop problem by parallel\nhead-tail adjustments. Journal of Scheduling, 4(1), 53–64. https://doi.org/10.1002/1099-1425(200101/\n02)4:1<53::AID-JOS59>3.0.CO;2-Y\nBrown, T., Mann, B., Ryder, N., et al. (2020). Language models are few-shot learners. In H. Larochelle,\nM. Ranzato, Hadsell R., et al. (Eds.) Advances in neural information processing systems (V ol. 33, pp.\n1877–1901). Curran Associates, In https://doi.org/10.5555/3495724.3495883\nBrucker, P ., & Knust, S. (2006). Complex scheduling. Springer. https://doi.org/10.1007/3-540-29546-1\nChakraborti T, Sreedharan S, & Kambhampati S (2020) The emerging landscape of explainable automated\nplanning & decision making. In C. Bessiere (Ed.) Proceedings of the Twenty-Ninth International Joint\nConference on Artiﬁcial Intelligence, IJCAI-20, international joint conferences on artiﬁcial intelligence\norganization (pp. 4803–4811). https://doi.org/10.24963/ijcai.2020/669 , survey track\nChen, T. C. T. (2023). Explainable Artiﬁcial Intelligence (XAI) in manufacturing (pp. 1–11). Cham: Springer\nInternational Publishing. https://doi.org/10.1007/978-3-031-27961-4_1\nˇCyras, K., Rago, A., Albini, E., et al. (2021). Argumentative xai: a survey. In Z.H. Zhou (Ed.) Proceedings\nof the Thirtieth International Joint Conference on Artiﬁcial Intelligence, IJCAI-21, international joint\nconferences on artiﬁcial intelligence organization (pp. 4392–4399) https://doi.org/10.24963/ijcai.2021/\n600, survey Track\n123\n1333\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nDatta, T., Dickerson, J.P . (2023). Who’s thinking? a push for human-centered evaluation of llms using the xai\nplaybook.https://doi.org/10.48550/arXiv.2303.06223\nErsoy, P ., & Er¸sahin, M. (2024). Optimal llm execution strategies for llama 3.1 language models across diverse\nhardware conﬁgurations: a comprehensive guide. Computational Intelligence and Machine Learning,5.\nhttps://www.cimachinelearning.com/llm-execution-strategies.php\nFace, H. (2024). all-MiniLM-L6-v2 sentence transformer.https://huggingface.co/sentence-transformers/all-\nMiniLM-L6-v2\nFace, H. (2024). Metric: bert_score. https://huggingface.co/spaces/evaluate-metric/bertscore\nFikar, C., & Hirsch, P . (2017). Home health care routing and scheduling: A review. Computers & Operations\nResearch, 77, 86–95. https://doi.org/10.1016/j.cor.2016.07.019\nFrancis, A. (2015). Graphical modelling classiﬁcation for construction project scheduling. Procedia Engi-\nneering, 123 , 162–168. https://doi.org/10.1016/j.proeng.2015.10.073, selected papers from Creative\nConstruction Conference 2015\nFriedrich, F., Schramowski, P ., Tauchmann, C., et al. (2021). Interactively providing explanations for trans-\nformer language models. In: HHAI. https://doi.org/10.3233/faia220218\nFu, D., Huang, J., Lu, S., et al. (2024). PreAct: Prediction enhances agent’s planning ability.https://doi.org/\n10.48550/arXiv.2402.11534\nGajane, P ., Saxena, A., Tavakol, M., et al (2022). Survey on fair reinforcement learning: Theory and prac-\ntice.https://doi.org/10.48550/arXiv.2205.10032\nGashi, M., Mutlu, B., & Thalmann, S. (2023). Impact of interdependencies: Multi-component system perspec-\ntive toward predictive maintenance based on machine learning and xai. Applied Sciences, 13(5), 3088.\nhttps://doi.org/10.3390/app13053088\nGlaese, A., McAleese, N., Tre ˛bacz, M., et al. (2022). Improving alignment of dialogue agents via targeted\nhuman judgements.https://doi.org/10.48550/arXiv.2209.14375\nGoh, E., V enkataram, H.S., Balaji, B., et al. (2022). SatNet: A benchmark for satellite scheduling\noptimization. In: AAAI-22 workshop on Machine Learning for Operations Research (ML4OR) .\nPasadena, CA: Jet Propulsion Laboratory, National Aeronautics and Space Administration, 2022.\nhttps://doi.org/2014/56106\nHager, P ., Jungmann, F., Holland, R., et al. (2024). Evaluation and mitigation of the limitations of large\nlanguage models in clinical decision-making. Nature Medicine, 30 (9), 2613–2622. https://doi.org/10.\n1038/s41591-024-03097-1\nHenning Dr rer nat A. (2002). Practical job shop scheduling issues . PhD thesis, Friedrich Schiller Univer-\nsity, Jena. https://www.db-thueringen.de/receive/dbt_mods_00000873, dissertation, Friedrich Schiller\nUniversityät Jena, 2003\nHerrmann, A., & Schaub, H. (2023). Reinforcement learning for the agile earth-observing satellite scheduling\nproblem. IEEE Transactions on Aerospace and Electronic Systems, 59 (5), 5235–5247. https://doi.org/\n10.1109/TAES.2023.3251307\nJain, S., & Wallace, B.C. (2019). Attention is not explanation. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language Tech-\nnologies V ol. 1 (Long and Short Papers) (pp. 3543–3556). Minneapolis, Minnesota: Association for\nComputational Linguistics. https://doi.org/10.18653/v1/N19-1357\nJain, A. S., & Meeran, S. (1999). Deterministic job-shop scheduling: Past, present and future. European\nJournal of Operational Research, 113(2), 390–434. https://doi.org/10.1016/S0377-2217(98)00113-1\nJeong C (2024) Domain-specialized llm: Financial ﬁne-tuning and utilization method using mistral 7b. Journal\nof Intelligence and Information Systems 30(1):93–120, https://doi.org/10.13088/jiis.2024.30.1.093\nKasneci, E., Sessler, K., Küchemann, S., et al. (2023). Chatgpt for good? on opportunities and challenges of\nlarge language models for education. Learning and Individual Differences, 103 (102), 274. https://doi.\norg/10.1016/j.lindif.2023.102274\nKeane, M.T., Kenny, E.M., Delaney, E., et al. (2021). If only we had better counterfactual explanations: Five\nkey deﬁcits to rectify in the evaluation of counterfactual xai techniques. In Z.H., Zhou (Eds.), Proceedings\nof the Thirtieth International Joint Conference on Artiﬁcial Intelligence, IJCAI-21, international joint\nconferences on artiﬁcial intelligence organization(pp. 4466–4474). https://doi.org/10.24963/ijcai.2021/\n609, survey Track\nKim, D., Song, Y ., Kim, S., et al. (2023). How should the results of artiﬁcial intelligence be explained to users?\n- research on consumer preferences in user-centered explainable artiﬁcial intelligence. Technological\nForecasting and Social Change, 188(122), 343. https://doi.org/10.1016/j.techfore.2023.122343\nLai, V ., Chen, C., Liao, Q.V ., et al. (2021). Towards a science of human-ai decision making: A survey of\nempirical studies. CoRR. https://doi.org/10.48550/arXiv.2112.11471\nLai, V ., Chen, C., Smith-Renner, A., et al. (2023). Towards a science of human-ai decision making: An overview\nof design space in empirical human-subject studies. In Proceedings of the 2023 ACM Conference on\n123\n1334\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nFairness, Accountability, and Transparency, Association for Computing Machinery, New York, NY, USA,\nFAccT ’23(pp. 1369–1385). https://doi.org/10.1145/3593013.3594087\nMarabelli, M., Newell, S., & Handunge, V . (2021). The lifecycle of algorithmic decision-making systems:\nOrganizational choices and ethical challenges. The Journal of Strategic Information Systems, 30 (3),\n101683. https://doi.org/10.1016/j.jsis.2021.101683\nMontavon, G., Binder, A., Lapuschkin, S., et al. (2019). Layer-Wise Relevance Propagation: An overview(pp.\n193–209). Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-28954-6_10\nMoons, S., Ramaekers, K., Caris, A., et al. (2017). Integrating production scheduling and vehicle routing\ndecisions at the operational decision level: A review and discussion. Computers & Industrial Engineering,\n104, 224–245. https://doi.org/10.1016/j.cie.2016.12.010\nMo, Y ., Zhao, D., Du, J., et al. (2020). Automated staff assignment for building maintenance using natural\nlanguage processing. Automation in Construction, 113(103), 150. https://doi.org/10.1016/j.autcon.2020.\n103150\nMullins, B. (2023). The shape of explanations: A topological account of rule-based explanations in machine\nlearning. https://doi.org/10.48550/arXiv.2301.09042\nNarteni, S., Orani, V ., Ferrari, E., et al. (2022). A new xai-based evaluation of generative adversarial networks\nfor imu data augmentation. In 2022 IEEE international conference on e-health networking, application\n& services (HealthCom) (pp 167–172). https://doi.org/10.1109/HealthCom54947.2022.9982780\nPicard, G., Caron, C., Farges, J.L., et al. (2021). Autonomous agents and multiagent systems challenges in earth\nobservation satellite constellations. In Proceedings of the 20th international conference on autonomous\nagents and multiagent systems, International Foundation for Autonomous Agents and Multiagent Systems,\nRichland, SC, AAMAS ’21(pp. 39—44). https://doi.org/10.5555/3463952.3463961\nPillai, M., Liu, C. C., Kwong, E., et al. (2024). Using an explainable machine learning approach to prioritize\nfactors contributing to healthcare professionals’ burnout. Journal of Intelligent Information Systems,\n62(4), 1113–1124. https://doi.org/10.1007/s10844-024-00862-z\nPowell, C., Berquand, A., Riccardi, A. (2023). Natural language processing for explainable satellite scheduling.\nIn SPACEOPS 2023, ARE, p #349. https://strathprints.strath.ac.uk/85129/\nPrieto, S. A., Mengiste, E. T., & García de Soto, B. (2023). Investigating the use of chatgpt for the scheduling\nof construction projects. Buildings,13(4). https://doi.org/10.3390/buildings13040857\nPuiutta, E., & V eith, E. M. (2020). Explainable reinforcement learning: A survey. International cross-domain\nconference for machine learning and knowledge extraction (pp. 77–95). Springer. https://doi.org/10.\n1007/978-3-030-57321-8_5\nRjoub, G., Bentahar, J., Abdel Wahab, O., et al. (2021). Deep and reinforcement learning for automated\ntask scheduling in large-scale cloud computing systems. Concurrency and Computation: Practice and\nExperience, 33(23), e5919. https://doi.org/10.1002/cpe.5919\nRoy, D., Zhang, X., Bhave, R., et al. (2024). Exploring llm-based agents for root cause analysis. In Companion\nproceedings of the 32nd ACM international conference on the foundations of software engineering,\nassociation for computing machinery, New York, NY, USA, FSE 2024(pp. 208—219). https://doi.org/10.\n1145/3663529.3663841\nSaeed, W., & Omlin, C. (2023). Explainable ai (xai): A systematic meta-survey of current challenges and\nfuture opportunities. Knowledge-Based Systems, 263(110), 273. https://doi.org/10.1016/j.knosys.2023.\n110273\nSatterﬁeld, N., Holbrooka, P ., & Wilcoxa, T. (2024). Fine-tuning llama with case law data to improve legal\ndomain performance. OSF. https://doi.org/10.31219/osf.io/e6mjs\nScao, T.L., Fan, A., Akiki, C., et al. (2022). Bloom: A 176b-parameter open-access multilingual language\nmodel. https://doi.org/10.48550/ARXIV .2211.05100\nSchroder, M. (2023). Autoscrum: Automating project planning using large language models. https://doi.org/\n10.48550/arXiv.2306.03197\nShao, Z., Gong, Y ., Shen, Y ., et al. (2023). Synthetic prompting: Generating chain-of-thought demonstrations\nfor large language models. In A. Krause, E. Brunskill, K. Cho, et al .(Eds.), Proceedings of the 40th\ninternational conference on machine learning, PMLR, Proceedings of Machine Learning Research(V ol.\n202, pp. 30706–30775). https://doi.org/10.5555/3618408.3619681\nShuster, K., Xu, J., Komeili, M., et al. (2022). Blenderbot 3: A deployed conversational agent that continually\nlearns to responsibly engage.https://doi.org/10.48550/arXiv.2208.03188\nShylo, O, (2002), Best known lower and upper bounds: Job shop scheduling problem : Taillard’s\ninstances.https://optimizizer.com/TA.php\n123\n1335\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nShylo, O.V ., & Shams, H. (2018). Boosting binary optimization via binary classiﬁcation: A case study of job\nshop scheduling. https://doi.org/10.48550/arXiv.1808.10813\nSingh, C., Askari, A., Caruana, R., et al. (2023). Augmenting interpretable models with large language models\nduring training. Nature Communications. https://doi.org/10.1038/s41467-023-43713-1\nSingh, C., Askari, A., Caruana, R., et al. (2023). Augmenting interpretable models with large language models\nduring training. Nature Communications. https://doi.org/10.1038/s41467-023-43713-1\nSquires, M., Tao, X., Elangovan, S., et al. (2022). A novel genetic algorithm based system for the scheduling\nof medical treatments. Expert Systems with Applications, 195(116), 464. https://doi.org/10.1016/j.eswa.\n2021.116464\nSui, Y ., Zhou, M., Zhou, M., et al. (2024). Table meets llm: Can large language models understand structured\ntable data? a benchmark and empirical study. In Proceedings of the 17th ACM international conference\non web search and data mining, association for computing machinery, New Y ork, NY , USA, WSDM ’24\n(pp. 645—654). https://doi.org/10.1145/3616855.3635752\nTaillard, E. (1997). Best lower and upper bounds known, from or-lib. http://mistic.heig-vd.ch/taillard/\nproblemes.dir/ordonnancement.dir/jobshop.dir/best_lb_up.txt\nTaillard, E. (1993). Benchmarks for basic scheduling problems. European Journal of Operational Research,\n64(2), 278–285. https://doi.org/10.1016/0377-2217(93)90182-M , project Management anf Scheduling\nThangavel, K., Spiller, D., Sabatini, R., et al. (2023). Trusted autonomous operations of distributed satellite\nsystems using optical sensors. Sensors,23(6). https://doi.org/10.3390/s23063344\nThoppilan, R., De Freitas, D., Hall, J., et al. (2022). Lamda: Language models for dialog applications. https://\ndoi.org/10.48550/arXiv.2201.08239\nTurpin, M., Michael, J., Perez, E., et al. (2023). Language models don’t always say what they think: unfaithful\nexplanations in chain-of-thought prompting. In Proceedings of the 37th international conference on\nneural information processing systems, Curran Associates Inc., Red Hook, NY, USA, NIPS ’23. https://\ndoi.org/10.5555/3666122.3669397\nUhde, A., Schlicker, N., Wallach, D.P ., et al. (2020). Fairness and decision-making in collaborative shift\nscheduling systems. In Proceedings of the 2020 CHI conference on human factors in computing systems\n(pp. 1–13). https://doi.org/10.1145/3313831.3376656\nV aswani, A., Shazeer, N., Parmar, N., et al. (2017). Attention is all you need. In Proceedings of the 31st\ninternational conference on neural information processing systems, Curran Associates Inc., Red Hook,\nNY, USA, NIPS’17(pp. 6000—6010). https://doi.org/10.5555/3295222.3295349\nWang, Y ., Shi, X., & Zhao, X. (2024). Mllm4rec: multimodal information enhancing llm for sequential rec-\nommendation. Journal of Intelligent Information Systems, 1–17 ,. https://doi.org/10.1007/s10844-024-\n00915-3\nWei, J., Wang, X., Schuurmans, D., et al. (2022). Chain-of-thought prompting elicits reasoning in large language\nmodels. In S. Koyejo, S. Mohamed, A. Agarwal, et al. (Eds.), Advances in neural information processing\nsystems (V ol. 35, pp. 24824–24837). Curran Associates, Inc. https://doi.org/10.5555/3600270.3602070\nWiegreffe, S., & Pinter, Y . (2019). Attention is not not explanation. In conference on empirical methods in\nnatural language processing. https://doi.org/10.48550/arXiv.1908.04626\nWu, X., Zhao, H., Zhu, Y ., et al. (2024). Usable xai: 10 strategies towards exploiting explainability in the llm\nera. https://doi.org/10.48550/arXiv.2403.08946\nWu, T., He, S., Liu, J., et al. (2023). A brief overview of chatgpt: The history, status quo and potential future\ndevelopment. IEEE/CAA Journal of Automatica Sinica, 10(5), 1122–1136. https://doi.org/10.1109/JAS.\n2023.123618\nXiong, H., Shi, S., Ren, D., et al. (2022). A survey of job shop scheduling problem: The types and models.\nComputers & Operations Research, 142(105), 731. https://doi.org/10.1016/j.cor.2022.105731\nYang, J., Jin, H., Tang, R., et al. (2024). Harnessing the power of llms in practice: A survey on chatgpt and\nbeyond. ACM Transactions on Knowledge Discovery from Data,18(6). https://doi.org/10.1145/3649506\nYang, X., Zhu, J., & De Meo, P . (2024). A quantum-like zero-shot approach for sentiment analysis in ﬁnance.\nJournal of Intelligent Information Systems, 1–17,. https://doi.org/10.1007/s10844-024-00912-6\nYang, X., Zhu, J., & De Meo, P . (2024). A quantum-like zero-shot approach for sentiment analysis in ﬁnance.\nJournal of Intelligent Information Systems, 1–17,. https://doi.org/10.1007/s10844-024-00912-6\nYao, S., Zhao, J., Y u, D., et al. (2023). React: Synergizing reasoning and acting in language models. In The\neleventh international conference on learning representations. https://openreview.net/forum?id=WE_\nvluYUL-X\nYao, E., Liu, T., Lu, T., et al. (2020). Optimization of electric vehicle scheduling with multiple vehicle types in\npublic transport. Sustainable Cities and Society, 52(101), 862. https://doi.org/10.1016/j.scs.2019.101862\nZaki, M., Jayadeva, Mausam, et al. (2024). Mascqa: investigating materials science knowledge of large lan-\nguage models. Digital Discovery, 3, 313–327. https://doi.org/10.1039/D3DD00188A\n123\n1336\nJournal of Intelligent Information Systems (2025) 63:1287–1337\nZheng, Z., Ren, X., Xue, F., et al. (2023). Response length perception and sequence scheduling: An llm-\nempowered llm inference pipeline. https://doi.org/10.48550/arXiv.2305.13144\nZhou, L., Zhang, L., & Fang, Y . (2020). Logistics service scheduling with manufacturing provider selection in\ncloud manufacturing. Robotics and Computer-Integrated Manufacturing, 65(101), 914. https://doi.org/\n10.1016/j.rcim.2019.101914\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and\ninstitutional afﬁliations.\n123\n1337",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8466430902481079
    },
    {
      "name": "Scheduling (production processes)",
      "score": 0.5446985363960266
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5270058512687683
    },
    {
      "name": "Machine learning",
      "score": 0.4329664707183838
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": []
}