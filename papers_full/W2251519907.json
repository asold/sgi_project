{
  "title": "Trans-dimensional Random Fields for Language Modeling",
  "url": "https://openalex.org/W2251519907",
  "year": 2015,
  "authors": [
    {
      "id": "https://openalex.org/A1995576685",
      "name": "Bin Wang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2100893253",
      "name": "Zhijian Ou",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2109175891",
      "name": "Zhiqiang TAN",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1973575289",
    "https://openalex.org/W1970689298",
    "https://openalex.org/W2113651538",
    "https://openalex.org/W2145677862",
    "https://openalex.org/W4297750044",
    "https://openalex.org/W2080018251",
    "https://openalex.org/W2097732278",
    "https://openalex.org/W2050971845",
    "https://openalex.org/W2043285245",
    "https://openalex.org/W2158148237",
    "https://openalex.org/W2289760663",
    "https://openalex.org/W2167717037",
    "https://openalex.org/W1984635093",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2160842254",
    "https://openalex.org/W2147010501",
    "https://openalex.org/W2963941964",
    "https://openalex.org/W1707676469",
    "https://openalex.org/W1501203061",
    "https://openalex.org/W1568229137",
    "https://openalex.org/W2114858359",
    "https://openalex.org/W2168304982",
    "https://openalex.org/W2106706098",
    "https://openalex.org/W2111305191",
    "https://openalex.org/W2100714283",
    "https://openalex.org/W1749358154",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W1511986666"
  ],
  "abstract": "Bin Wang, Zhijian Ou, Zhiqiang Tan. Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2015.",
  "full_text": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natural Language Processing, pages 785–794,\nBeijing, China, July 26-31, 2015.c⃝2015 Association for Computational Linguistics\nTrans-dimensional Random Fields for Language Modeling\nBin Wang1, Zhijian Ou1, Zhiqiang Tan2\n1Department of Electronic Engineering, Tsinghua University, Beijing 100084, China\n2Department of Statistics, Rutgers University, Piscataway, NJ 08854, USA\nwangbin12@mails.tsinghua.edu.cn, ozj@tsinghua.edu.cn,\nztan@stat.rutgers.edu\nAbstract\nLanguage modeling (LM) involves\ndetermining the joint probability of\nwords in a sentence. The conditional\napproach is dominant, representing the\njoint probability in terms of conditionals.\nExamples include n-gram LMs and neural\nnetwork LMs. An alternative approach,\ncalled the random ﬁeld (RF) approach, is\nused in whole-sentence maximum entropy\n(WSME) LMs. Although the RF approach\nhas potential beneﬁts, the empirical\nresults of previous WSME models are\nnot satisfactory. In this paper, we revisit\nthe RF approach for language modeling,\nwith a number of innovations. We\npropose a trans-dimensional RF (TDRF)\nmodel and develop a training algorithm\nusing joint stochastic approximation and\ntrans-dimensional mixture sampling. We\nperform speech recognition experiments\non Wall Street Journal data, and ﬁnd that\nour TDRF models lead to performances as\ngood as the recurrent neural network LMs\nbut are computationally more efﬁcient in\ncomputing sentence probability.\n1 Introduction\nLanguage modeling is crucial for a variety\nof computational linguistic applications, such\nas speech recognition, machine translation,\nhandwriting recognition, information retrieval and\nso on. It involves determining the joint probability\np(x) of a sentence x, which can be denoted as\na pair x = ( l,xl), where l is the length and\nxl = (x1,...,x l) is a sequence of lwords.\nCurrently, the dominant approach is conditional\nmodeling, which decomposes the joint probability\nof xl into a product of conditional probabilities 1\n1And the joint probability of x is modeled as p(x) =\nby using the chain rule,\np(x1,...,x l) =\nl∏\ni=1\np(xi|x1,...,x i−1). (1)\nTo avoid degenerate representation of the con-\nditionals, the history of xi, denoted as hi =\n(x1,··· ,xi−1), is reduced to equivalence classes\nthrough a mapping φ(hi) with the assumption\np(xi|hi) ≈p(xi|φ(hi)). (2)\nLanguage modeling in this conditional\napproach consists of ﬁnding suitable mappings\nφ(hi) and effective methods to estimate\np(xi|φ(hi)). A classic example is the traditional\nn-gram LMs with φ(hi) = ( xi−n+1,...,x i−1).\nVarious smoothing techniques are used for\nparameter estimation (Chen and Goodman, 1999).\nRecently, neural network LMs, which have begun\nto surpass the traditional n-gram LMs, also follow\nthe conditional modeling approach, with φ(hi)\ndetermined by a neural network (NN), which can\nbe either a feedforward NN (Schwenk, 2007) or a\nrecurrent NN (Mikolov et al., 2011).\nRemarkably, an alternative approach is used in\nwhole-sentence maximum entropy (WSME) lan-\nguage modeling (Rosenfeld et al., 2001). Speciﬁ-\ncally, a WSME model has the form:\np(x; λ) = 1\nZ exp{λTf(x)} (3)\nHere f(x) is a vector of features, which can be\narbitrary computable functions of x, λis the cor-\nresponding parameter vector, and Z is the global\nnormalization constant. Although WSME mod-\nels have the potential beneﬁts of being able to\nnaturally express sentence-level phenomena and\nintegrate features from a variety of knowledge\np(xl)p(⟨EOS⟩|xl), where ⟨EOS⟩is a special token placed\nat the end of every sentence. Thus the distribution of the\nsentence length is implicitly modeled.\n785\nsources, their performance results ever reported\nare not satisfactory (Rosenfeld et al., 2001; Amaya\nand Bened´ı, 2001; Ruokolainen et al., 2010).\nThe WSME model deﬁned in (3) is basically a\nMarkov random ﬁeld (MRF). A substantial chal-\nlenge in ﬁtting MRFs is that evaluating the gradi-\nent of the log likelihood requires high-dimensional\nintegration and hence is difﬁcult even for mod-\nerately sized models (Younes, 1989), let alone\nthe language model (3). The sampling methods\npreviously tried for approximating the gradient are\nthe Gibbs sampling, the Independence Metropolis-\nHasting sampling and the importance sampling\n(Rosenfeld et al., 2001). Simple applications of\nthese methods are hardly able to work efﬁcient-\nly for the complex, high-dimensional distribution\nsuch as (3), and hence the WSME models are in\nfact poorly ﬁtted to the data. This is one of the\nreasons for the unsatisfactory results of previous\nWSME models.\nIn this paper, we propose a new language\nmodel, called the trans-dimensional random\nﬁeld (TDRF) model, by explicitly taking\naccount of the empirical distributions of lengths.\nThis formulation subsequently enables us to\ndevelop a powerful Markov chain Monte Carlo\n(MCMC) technique, called trans-dimensional\nmixture sampling and then propose an effective\ntraining algorithm in the framework of stochastic\napproximation (SA) (Benveniste et al., 1990;\nChen, 2002). The SA algorithm involves jointly\nupdating the model parameters and normalization\nconstants, in conjunction with trans-dimensional\nMCMC sampling. Section 2 and 3 present the\nmodel deﬁnition and estimation respectively.\nFurthermore, we make several additional in-\nnovations, as detailed in Section 4, to enable\nsuccessful training of TDRF models. First, the\ndiagonal elements of hessian matrix are estimat-\ned during SA iterations to rescale the gradient,\nwhich signiﬁcantly improves the convergence of\nthe SA algorithm. Second, word classing is in-\ntroduced to accelerate the sampling operation and\nalso improve the smoothing behavior of the mod-\nels through sharing statistical strength between\nsimilar words. Finally, multiple CPUs are used to\nparallelize the training of our RF models.\nIn Section 5, speech recognition experiments\nare conducted to evaluate our TDRF LMs, com-\npared with the traditional 4-gram LMs and the re-\ncurrent neural network LMs (RNNLMs) (Mikolov\net al., 2011) which have emerged as a new state-\nof-art of language modeling. We explore the use\nof a variety of features based on word and class\ninformation in TDRF LMs. In terms of word error\nrates (WERs) for speech recognition, our TDRF\nLMs alone can outperform the KN-smoothing 4-\ngram LM with 9.1% relative reduction, and per-\nform comparably to the RNNLM with a slight\n0.5% relative reduction. To our knowledge, this\nresult represents the ﬁrst strong empirical evidence\nsupporting the power of using the whole-sentence\nlanguage modeling approach. Our open-source\nTDRF toolkit is released publicly 2.\n2 Model Deﬁnition\nThroughout, we denote 3 by xl = ( x1,...,x l) a\nsentence (i.e., word sequence) of length lranging\nfrom 1 to m. Each element of xl corresponds to\na single word. For l = 1 ,...,m , we assume\nthat sentences of length l are distributed from an\nexponential family model:\npl(xl; λ) = 1\nZl(λ)eλTf(xl), (4)\nwhere f(xl) = ( f1(xl),f2(xl),...,f d(xl))T is\nthe feature vector and λ = ( λ1,λ2,...,λ d)T is\nthe corresponding parameter vector, and Zl(λ) is\nthe normalization constant:\nZl(λ) =\n∑\nxl\neλTf(xl) (5)\nMoreover, we assume that length l is associated\nwith probability πl for l = 1 ,...,m . Therefore,\nthe pair (l,xl) is jointly distributed as\np(l,xl; λ) = πlpl(xl; λ). (6)\nWe provide several comments on the above\nmodel deﬁnition. First, by making explicit the\nrole of lengths in model deﬁnition, it is clear that\nthe model in (6) is a mixture of random ﬁelds\non sentences of different lengths (namely on sub-\nspaces of different dimensions), and hence will be\ncalled a trans-dimensional random ﬁeld (TDRF).\nDifferent from the WSME model (3), a crucial\naspect of the TDRF model (6) is that the mixture\nweights πl can be set to the empirical length\nprobabilities in the training data. The WSME\n2http://oa.ee.tsinghua.edu.cn/\n˜ouzhijian/software.htm\n3We add sup or subscript l, e.g. in xl,pl(), to make clear\nthat the variables and distributions depend on length l.\n786\nmodel (3) is essentially also a mixture of RFs, but\nthe mixture weights implied are proportional to the\nnormalizing constants Zl(λ):\np(l,xl; λ) = Zl(λ)\nZ(λ)\n1\nZl(λ)eλTf(xl), (7)\nwhere Z(λ) = ∑m\nl=1 Zl(λ).\nA motivation for proposing (6) is that it is\nvery difﬁcult to sample from (3), namely (7),\nas a mixture distribution with unknown weights\nwhich typically differ from each other by orders of\nmagnitudes, e.g. 1040 or more in our experiments.\nSetting mixture weights to the known, empirical\nlength probabilities enables us to develop a very\neffective learning algorithm, as introduced in Sec-\ntion 3. Basically, the empirical weights serve as a\ncontrol device to improve sampling from multiple\ndistributions (Liang et al., 2007; Tan, 2015) .\nSecond, it can be shown that if we incorporate\nthe length features 4 in the vector of features f(x)\nin (3), then the distribution p(x; λ) in (3) under\nthe maximum entropy (ME) principle will take the\nform of (6) and the probabilities (π1,...,π m) in\n(6) implied by the parameters for the length fea-\ntures are exactly the empirical length probabilities.\nThird, a feature fi(xl), 1 ≤i ≤d, can be any\ncomputable function of the sentence xl, such as\nn-grams. In our current experiments, the features\nfi(xl) and their corresponding parameters λi are\ndeﬁned to be position-independent and length-\nindependent. For example, fi(xl) = ∑\nkfi(xl,k),\nwhere fi(xl,k) is a binary function ofxl evaluated\nat position k. As a result, the feature fi(xl) takes\nvalues in the non-negative integers.\n3 Model Estimation\nWe develop a stochastic approximation algorith-\nm using Markov chain Monte Carlo to estimate\nthe parameters λand the normalization constants\nZ1(λ),...,Z m(λ) (Benveniste et al., 1990; Chen,\n2002). The core algorithms newly designed in\nthis paper are the joint SA for simultaneously\nestimating parameters and normalizing constants\n(Section 3.2) and trans-dimensional mixture sam-\npling (Section 3.3) which is used as Step I of the\njoint SA. The most relevant previous works that\nwe borrowed from are (Gu and Zhu, 2001) on SA\nfor ﬁtting a single RF, (Tan, 2015) on sampling and\n4The length feature corresponding to length lis a binary\nfeature that takes one if the sentence x is of length l, and\notherwise takes zero.\nestimating normalizing constants from multiple\nRFs of the same dimension, and (Green, 1995) on\ntrans-dimensional MCMC.\n3.1 Maximum likelihood estimation\nSuppose that the training dataset consists of nl\nsentences of length l for l = 1 ,...,m . First,\nthe maximum likelihood estimate of the length\nprobability πl is easily shown to be nl/n, where\nn = ∑m\nl=1 nl. By abuse of notation, we set\nπl = nl/n hereafter. Next, the log-likelihood of\nλgiven the empirical length probabilities is\nL(λ) = 1\nn\nm∑\nl=1\n∑\nxl∈Dl\nlog pl(xl; λ), (8)\nwhere Dl is the collection of sentences of length l\nin the training set. By setting to 0 the derivative of\n(8) with respect to λ, we obtain that the maximum\nlikelihood estimate of λ is determined by the\nfollowing equation:\n∂L(λ)\n∂λ = ˜p[f] −pλ[f] = 0, (9)\nwhere ˜p[f] is the expectation of the feature vector\nf with respect to the empirical distribution:\n˜p[f] = 1\nn\nm∑\nl=1\n∑\nxl∈Dl\nf(xl), (10)\nand pλ[f] is the expectation of f with respect to\nthe joint distribution (6) with πl = nl/n:\npλ[f] =\nm∑\nl=1\nnl\nnpλ,l[f], (11)\nand pλ,l[f] = ∑\nxl f(xl)pl(xl; λ). Eq.(9) has\nthe form of equating empirical expectations ˜p[f]\nwith theoretical expectations pλ[f], as similarly\nfound in maximum likelihood estimation of single\nrandom ﬁeld models.\n3.2 Joint stochastic approximation\nTraining random ﬁeld models is challenging due\nto numerical intractability of the normalizing con-\nstants Zl(λ) and expectations pλ,l[f]. We propose\na novel SA algorithm for estimating the parame-\nters λ by (9) and, simultaneously, estimating the\nlog ratios of normalization constants:\nζ∗\nl (λ) = log Zl(λ)\nZ1(λ), l = 1,...,m (12)\n787\nAlgorithm 1 Joint stochastic approximation\nInput: training set\n1: set initial values λ(0) = (0,..., 0)T and\nζ(0) = ζ∗(λ(0)) −ζ∗\n1 (λ(0))\n2: for t= 1,2,...,t max do\n3: set B(t) = ∅\n4: set (L(t,0),X(t,0)) = (L(t−1,K),X(t−1,K))\nStep I: MCMC sampling\n5: for k= 1 →Kdo\n6: sampling (See Algorithm 3)\n(L(t,k),X(t,k)) = SAMPLE (L(t,k−1),X(t,k−1))\n7: set B(t) = B(t) ∪{(L(t,k),X(t,k))}\n8: end for\nStep II: SA updating\n9: Compute λ(t) based on (14)\n10: Compute ζ(t) based on (15) and (16)\n11: end for\nwhere Z1(λ) is chosen as the reference value and\ncan be calculated exactly. The algorithm can be\nobtained by combining the standard SA algorithm\nfor training single random ﬁelds (Gu and Zhu,\n2001) and a trans-dimensional extension of the\nself-adjusted mixture sampling algorithm (Tan,\n2015).\nSpeciﬁcally, consider the following joint distri-\nbution of the pair (l,xl):\np(l,xl; λ,ζ) ∝ πl\neζl\neλTf(xl), (13)\nwhere πl is set to nl/n for l = 1 ,...,m , but\nζ = (ζ1,...,ζ m)T with ζ1 = 0 are hypothesized\nvalues of the truth ζ∗(λ) = ( ζ∗\n1 (λ),...,ζ ∗\nm(λ))T\nwith ζ∗\n1 (λ) = 0 . The distribution p(l,xl; λ,ζ)\nreduces to p(l,xl; λ) in (6) if ζ were identical\nto ζ∗(λ). In general, p(l,xl; λ,ζ) differs from\np(l,xl; λ) in that the marginal probability of\nlength lis not necessarily πl.\nThe joint SA algorithm, whose pseudo-code is\nshown in Algorithm 1, consists of two steps at\neach time tas follows.\nStep I: MCMC sampling. Generate a sample\nset B(t) with p(l,xl; λ(t−1),ζ(t−1)) as the station-\nary distribution (see Section 3.3).\nStep II: SA updating. Compute\nλ(t) = λ(t−1) + γλ\n{\n˜p[f] −\n∑\n(l,xl)∈B(t) f(xl)\nK\n}\n(14)\nwhere γλ is a learning rate of λ; compute\nζ(t−1\n2 ) = ζ(t) + γζ\n{δ1(B(t))\nπ1\n,..., δm(B(t))\nπm\n}\n(15)\nζ(t) = ζ(t−1\n2 ) −ζ\n(t−1\n2 )\n1 (16)\nwhere γζ is a learning rate of ζ, and δl(B(t)) is the\nrelative frequency of length lappearing in B(t):\nδl(B(t)) =\n∑\n(j,xj)∈B(t) 1(j = l)\nK . (17)\nThe rationale in (15) is to adjust ζ based on\nhow the relative frequencies of lengths δl(B(t))\nare compared with the desired length probabili-\nties πl. Intuitively, if the relative frequency of\nsome length l in the sample set B(t) is greater\n(or respectively smaller) than the desired length\nprobability πl, then the hypothesized value ζ(t−1)\nl\nis an underestimate (or overestimate) ofζ∗\nl (λ(t−1))\nand hence should be increased (or decreased).\nFollowing Gu & Zhu (2001) and Tan (2015), we\nset the learning rates in two stages:\nγλ =\n{\nt−βλ if t≤t0\n1\nt−t0+t\nβλ\n0\nif t>t 0 (18)\nγζ =\n{\n(0.1t)−βζ if t≤t0\n1\n0.1(t−t0)+(0.1t0)βζ if t>t 0 (19)\nwhere 0.5 <βλ,βζ <1. In the ﬁrst stage (t≤t0),\na slow-decaying rate of t−β is used to introduce\nlarge adjustments. This forces the estimates λ(t)\nand ζ(t) to fall reasonably fast into the true values.\nIn the second stage ( t > t0), a fast-decaying\nrate of t−1 is used. The iteration number t is\nmultiplied by 0.1 in (19), to make the the learning\nrate of ζ decay more slowly than λ. Commonly,\nt0 is selected to ensure there is no more signiﬁcant\nadjustment observed in the ﬁrst stage.\n3.3 Trans-dimensional mixture sampling\nWe describe a trans-dimensional mixture sam-\npling algorithm to simulate from the joint distri-\nbution p(l,xl; λ,ζ), which is used with (λ,ζ) =\n(λ(t−1),ζ(t−1)) at time tfor MCMC sampling in\nthe joint SA algorithm. The name “mixture sam-\npling” reﬂects the fact that p(l,xl; λ,ζ) represents\na labeled mixture, because l is a label indicating\nthat xl is associated with the distribution pl(xl; ζ).\nWith ﬁxed (λ,ζ), this sampling algorithm can\nbe seen as formally equivalent to reversible jump\nMCMC (Green, 1995), which was originally pro-\nposed for Bayes model determination.\nThe trans-dimensional mixture sampling algo-\nrithm consists of two steps at each time t: local\njump between lengths and Markov move of sen-\ntences for a given length. In the following, we de-\nnote by L(t−1) and X(t−1) the length and sequence\n788\nbefore sampling, but use the short notation (λ,ζ)\nfor (λ(t−1),ζ(t−1)).\nStep I: Local jump. The Metropolis-Hastings\nmethod is used in this step to sample the length.\nAssuming L(t−1) = k, ﬁrst we draw a new length\nj ∼ Γ(k,·). The jump distribution Γ(k,l) is\ndeﬁned to be uniform at the neighborhood of k:\nΓ(k,l) =\n\n\n\n1\n3, if k∈[2,m −1],l ∈[k−1,k + 1]\n1\n2, if k= 1,l ∈[1,2] or k= m,l ∈[m−1,m]\n0, otherwise\n(20)\nwhere mis the maximum length. Eq.(20) restricts\nthe difference between jand kto be no more than\none. If j = k, we retain the sequence and perform\nthe next step directly, i.e. set L(t) = kand X(t) =\nX(t−1). If j = k+ 1 or j = k−1, the two cases\nare processed differently.\nIf j = k + 1 , we ﬁrst draw an element\n(i.e., word) Y from a proposal distribution:\nY ∼ gk+1(y|X(t−1)). Then we set\nL(t) = j(= k+ 1) and X(t) = {X(t−1),Y }with\nprobability\nmin\n{\n1,Γ(j,k)\nΓ(k,j)\np(j,{X(t−1),Y }; λ,ζ)\np(k,X(t−1); λ,ζ)gk+1(Y|X(t−1))\n}\n(21)\nwhere {X(t−1),Y }denotes a sequence of length\nk+ 1 whose ﬁrst k elements are X(t−1) and the\nlast element is Y.\nIf j = k−1, we set L(t) = j(= k−1) and\nX(t) = X(t−1)\n1:j with probability\nmin\n{\n1,Γ(j,k)\nΓ(k,j)\np(j,X(t−1)\n1:j ; λ,ζ)gk(X(t−1)\nk |X(t−1)\n1:j )\np(k,X(t−1); λ,ζ)\n}\n(22)\nwhere X(t−1)\n1:j is the ﬁrst jelements of X(t−1) and\nX(t−1)\nk is the kth element of X(t−1).\nIn (21) and (22), gk+1(y|xk) can be ﬂexibly\nspeciﬁed as a proper density function in y. In our\napplication, we ﬁnd the following choice works\nreasonably well:\ngk+1(y|xk) = p(k+ 1,{xk,y}; λ,ζ)∑\nwp(k+ 1,{xk,w}; λ,ζ). (23)\nStep II: Markov move. After the step of local\njump, we obtain\nX(t) =\n\n\n\nX(t−1) if L(t) = k\n{X(t−1),Y } if L(t) = k+ 1\nX(t−1)\n1:k−1 if L(t) = k−1\n(24)\nThen we perform Gibbs sampling on X(t), from\nthe ﬁrst element to the last element (Algorithm 2)\nAlgorithm 2 Markov Move\n1: for i= 1 →L(t) do\n2: draw W ∼p(L(t),{X(t)\n1:i−1,w,X (t)\ni+1:L(t) }; λ,ζ)\n3: set X(t)\ni = W\n4: end for\n4 Algorithm Optimization and\nAcceleration\nThe joint SA algorithm may still suffer from\nslow convergence, especially when λ is high-\ndimensional. We introduce several techniques for\nimproving the convergence of the algorithm and\nreducing computational cost.\n4.1 Improving SA recursion\nWe propose two techniques to effectively improve\nthe convergence of SA recursion.\nThe ﬁrst technique is to incorporate Hessian\ninformation, similarly as in related works on s-\ntochastic approximation (Gu and Zhu, 2001) and\nstochastic gradient descent algorithms (Byrd et al.,\n2014). But we only use the diagonal elements of\nthe Hessian matrix to re-scale the gradient, due to\nhigh-dimensionality of λ.\nTaking the second derivatives ofL(λ) yields\nHi = −d2L(λ)\ndλ2\ni\n= p[f2\ni ] −\nm∑\nl=1\nπl(pl[fi])2 (25)\nwhere Hi denotes the ith diagonal element of\nHessian matrix. At time t, before updating the\nparameter λ(Step II in Section 3.2), we compute\nH\n(t−1\n2 )\ni = 1\nK\n∑\n(l,xl)∈B(t)\nfi(xl)2 −\nm∑\nl=1\nπl(¯pl[fi])2,\n(26)\nH(t)\ni = H(t−1)\ni + γH(H\n(t−1\n2 )\ni −H(t−1)\ni ), (27)\nwhere ¯pl[fi] = |B(t)\nl |−1 ∑\n(l,xl)∈B(t)\nl\nfi(xl), and\nB(t)\nl is the subset, of size |B(t)\nl |, containing all\nsentences of length lin B(t).\nThe second technique is to introduce the “mini-\nbatch” on the training set. At each iteration, a\nsubset D(t) of K sentences are randomly selected\nfrom the training set. Then the gradient is approx-\nimated with the overall empirical expectation ˜p[f]\nbeing replaced by the empirical expectation over\nthe subset D(t). This technique is reminiscent of\nstochastic gradient descent using a random sub-\nsample of training data to achieve fast convergence\n789\n0 20 40 60 80 100120\n140\n160\n180\n200\nt/10\n− log−likelihood\n \n \nwithout hessian\nwith hessian\n(a)\n0 500 1000 1500 200050\n100\n150\n200\nt/10\nnegative log−likelihood\n \n \nHessian+mini−batch\nHessian (b)\nFigure 1: Examples of convergence curves on\ntraining set after introducing hessian and training\nset mini-batching.\nof optimization algorithms (Bousquet and Bottou,\n2008).\nBy combining the two techniques, we revise the\nupdating equation (14) of λto\nλ(t)\ni = λ(t−1)\ni + γλ\nmax(H(t)\ni ,h)\n×\n{∑\n(l,xl)∈D(t) fi(xl)\nK −\n∑\n(l,xl)∈B(t) fi(xl)\nK\n} (28)\nwhere 0 < h <1 is a threshold to avoid H(t)\ni\nbeing too small or even zero. Moreover, a constant\ntc is added to the denominator of (18), to avoid too\nlarge adjustment of λ, i.e.\nγλ =\n{ 1\ntc+tβλ if t≤t0,\n1\ntc+t−t0+t\nβλ\n0\nif t>t 0. (29)\nFig.1(a) shows the result after introducing hessian\nestimation, and Fig.1(b) shows the effect of train-\ning set mini-batching.\n4.2 Sampling acceleration\nFor MCMC sampling in Section 3.3, the Gibbs\nsampling operation of drawing X(t)\ni (Step 2 in Al-\ngorithms 2) involves calculating the probabilities\nof all the possible elements in position i. This\nis computationally costly, because the vocabulary\nsize |V|is usually 10 thousands or more in prac-\ntice. As a result, the Gibbs sampling operation\npresents a bottleneck limiting the efﬁciency of\nsampling algorithms.\nWe propose a novel method of using class in-\nformation to effectively reduce the computational\ncost of Gibbs sampling. Suppose that each word\nin vocabulary V is assigned to a single class.\nIf the total class number is |C|, then there are,\non average, |V|/|C| words in each class. With\nthe class information, we can ﬁrst draw the class\nof X(t)\ni , denoted by c(t)\ni , and then draw a word\nAlgorithm 3 Class-based MCMC sampling\n1: function SAMPLE ((L(t−1),X(t−1)))\n2: set k= L(t−1)\n3: init (L(t),X(t)) = (k,X(t−1))\nStep I: Local jump\n4: generate j ∼Γ(k,·) (Eq.(20))\n5: if j = k+ 1 then\n6: generate C ∼Qk+1(c)\n7: generate Y ∼˘gk+1(y|X(t−1),C) (Eq.31)\n8: set L(t) = j and X(t) = {X(t−1),Y }with\nprobability (Eq.21) and (Eq.32)\n9: end if\n10: if j = k−1 then\n11: set L(t) = jand X(t) = X(t−1)\n1:k−1 with probabil-\nity Eq.(22) and (Eq.32)\n12: end if\nStep II: Markov move\n13: for i= 1 →L(t) do\n14: draw C ∼Qi(c)\n15: set c(t)\ni = Cwith probability (Eq.30)\n16: draw W ∈Vc(t)\ni\n17: set X(t)\ni = W\n18: end for\n19: return (L(t),X(t))\n20: end function\nbelonging to class c(t)\ni . The computational cost is\nreduced from |V|to |C|+ |V|/|C|on average.\nThe idea of using class information to accel-\nerate training has been proposed in various con-\ntexts of language modeling, such as maximum\nentropy models (Goodman, 2001b) and RNN LMs\n(Mikolov et al., 2011). However, the realization of\nthis idea is different for training our models.\nThe pseudo-code of the new sampling method is\nshown in Algorithm 3. Denote by Vc the subset of\nVcontaining all the words belonging to class c. In\nthe Markov move step (Step 13 to 18 in Algorithm\n3), at each position i, we ﬁrst generate a class C\nfrom a proposal distributionQi(c) and then accept\nCas the new c(t)\ni with probability\nmin\n{\n1,Qi(c(t)\ni )\nQi(C)\npi(C)\npi(c(t)\ni )\n}\n(30)\nwhere\npi(c) =\n∑\nw∈Vc\np(L(t),{X(t)\n1:i−1,w,X (t)\ni+1:L(t) }; λ,ζ).\nThe probabilities Qi(c) and pi(c) depend on\n{X(t)\n1:i−1,X(t)\ni+1:L(t) }, but this is suppressed in the\nnotation. Then we normalize the probabilities of\nwords belonging to class c(t)\ni and draw a word as\nthe new X(t)\ni from the class c(t)\ni .\nSimilarly, in the local jump step with k =\nL(t−1), if the proposal j = k + 1 (Step 5 to 9\n790\nin Algorithm 3), we ﬁrst generate C ∼Qk+1(c)\nand then generate Y from class Cby\n˘gk+1(y|xk,C) = p(k+ 1,{xk,y}; λ,ζ)∑\nw∈VC\np(k+ 1,{xk,w}; λ,ζ) (31)\nwith xk = X(t−1). Then we set L(t) = j and\nX(t) = {X(t−1),Y }with probability as deﬁned\nin (21), by setting\ngk+1(y|xk) = Qk+1(C)˘gk+1(y|xk,C). (32)\nIf the proposal j = k −1, similarly we use\nacceptance probability (22) with (32).\nIn our application, we construct Qi(c) dynami-\ncally as follows. Write xl for {X(t−1),Y }in Step\n8 or for X(t) in Step 11 of Algorithm 3. First,\nwe construct a reduced modelpc\nl(xl), by including\nonly the features that depend on xl\ni through its\nclass and retaining the corresponding parameters\nin pl(xl; λ,ζ). Then we deﬁne the distribution\nQi(c) = pc\nl({xl\n1:i−1,c,x l\ni+1:l}),\nwhich can be directly calculated without knowing\nthe value of xl\ni.\n4.3 Parallelization of sampling\nThe sampling operation can be easily parallelized\nin SA Algorithm 1. At each time t, both the\nparameters λ and log normalization constants ζ\nare ﬁxed at λ(t−1) and ζ(t−1). Instead of simu-\nlating one Markov Chain, we simulate J Markov\nChains on J CPU cores separately. As a result, to\ngenerate a sample set B(t) of size K, only K/J\nsampling steps need to be performed on each CPU\ncore. By parallelization, the sampling operation is\ncompleted J times faster than before.\n5 Experiments\n5.1 PTB perplexity results\nIn this section, we evaluate the performance of\nLMs by perplexity (PPL). We use the Wall Street\nJournal (WSJ) portion of Penn Treebank (PTB).\nSections 0-20 are used as the training data (about\n930K words), sections 21-22 as the development\ndata (74K) and section 23-24 as the test data\n(82K). The vocabulary is limited to 10K words,\nwith one special token ⟨UNK⟩denoting words\nnot in the vocabulary. This setting is the same as\nthat used in other studies (Mikolov et al., 2011).\nThe baseline is a 4-gram LM with modiﬁed\nKneser-Ney smoothing (Chen and Goodman,\nType Features\nw (w−3w−2w−1w0)(w−2w−1w0)(w−1w0)(w0)\nc (c−3c−2c−1c0)(c−2c−1c0)(c−1c0)(c0)\nws (w−3w0)(w−3w−2w0)(w−3w−1w0)(w−2w0)\ncs (c−3c0)(c−3c−2c0)(c−3c−1c0)(c−2c0)\nwsh (w−4w0) (w−5w0)\ncsh (c−4c0) (c−5c0)\ncpw (c−3c−2c−1w0) (c−2c−1w0)(c−1w0)\nTable 1: Feature deﬁnition in TDRF LMs\n1999), denoted by KN4. We use the RNNLM\ntoolkit5 to train a RNNLM (Mikolov et al., 2011).\nThe number of hidden units is 250 and other\nconﬁgurations are set by default6.\nWord classing has been shown to be useful in\nconditional ME models (Chen, 2009). For our\nTDRF models, we consider a variety of features\nas shown in Table 1, mainly based on word and\nclass information. Each word is deterministically\nassigned to a single class, by running the automat-\nic clustering algorithm proposed in (Martin et al.,\n1998) on the training data.\nIn Table 1, wi,ci,i = 0,−1,..., −5 denote the\nword and its class at different position offset i,\ne.g. w0,c0 denotes the current word and its class.\nWe ﬁrst introduce the classic word/class n-gram\nfeatures (denoted by “w”/“c”) and the word/class\nskipping n-gram features (denoted by “ws”/“cs”)\n(Goodman, 2001a). Second, to demonstrate that\nlong-span features can be naturally integrated in\nTDRFs, we introduce higher-order features “w-\nsh”/“csh”, by considering two words/classes sep-\narated with longer distance. Third, as an example\nof supporting heterogenous features that combine\ndifferent information, the crossing features “cp-\nw” (meaning class-predict-word) are introduced.\nNote that for all the feature types in Table 1, only\nthe features observed in the training data are used.\nThe joint SA (Algorithm 1) is used to train the\nTDRF models, with all the acceleration methods\ndescribed in Section 4 applied. The minibatch\nsize K = 300 . The learning rates γλ and γζ\nare conﬁgured as (29) and (19) respectively with\nβλ = βζ = 0.6 and tc = 3000. For t0, it is ﬁrst\ninitialized to be104. During iterations, we monitor\nthe smoothed log-likelihood (moving average of\n1000 iterations) on the PTB development data.\n5http://rnnlm.org/\n6Minibatch size=10, learning rate=0.1, BPTT steps=5. 17\nsweeps are performed before stopping, which takes about 25\nhours. No word classing is used, since classing in RNNLMs\nreduces computation but at cost of accuracy. RNNLMs were\nexperimented with varying numbers of hidden units (100-\n500). The best result from using 250 hidden units is reported.\n791\nmodels PPL (±std. dev.)\nKN4 142.72\nRNN 128.81\nTDRF w+c 130.69±1.64\nTable 2: The PPLs on the PTB test data. The class\nnumber is 200.\nWe set t0 to the current iteration number once the\nrising percentage of the smoothed log-likelihoods\nwithin 100 iterations is below 20%, and then\ncontinue 5000 further iterations before stopping.\nThe conﬁguration of hessian estimation (Section\n4.1) is γH = γλ and h= 10−4. L2 regularization\nwith constant 10−5 is used to avoid over-ﬁtting. 8\nCPU cores are used to parallelize the algorithm, as\ndescribed in Section 4.3, and the training of each\nTDRF model takes less than 20 hours.\nThe perplexity results on the PTB test data are\ngiven in Table 2. As the normalization constants\nof TDRF models are estimated stochastically, we\nreport the Monte Carlo mean and standard devi-\nation from the last 1000 iterations for each PPL.\nThe TDRF model using the basic “w+c” features\nperforms close to the RNNLM in perplexity. To be\ncompact, results with more features are presented\nin the following WSJ experiment.\n5.2 WSJ speech recognition results\nIn this section, we continue to use the LMs ob-\ntained above (using PTB training and develop-\nment data), and evaluate their performance mea-\nsured by WERs in speech recognition, by re-\nscoring 1000-best lists from WSJ’92 test data (330\nsentences). The oracle WER of the 1000-best lists\nis 3.4%, which are generated from using the Kaldi\ntoolkit7 with a DNN-based acoustic model.\nTDRF LMs using a variety of features and\ndifferent number of classes are tested. The results\nare shown in Table 3. Different types of features,\nlike the skipping features, the higher-order fea-\ntures and the crossing features can all be easily\nsupported in TDRF LMs, and the performance\nis improved to varying degrees. Particularly, the\nTDRF using the “w+c+ws+cs+cpw” features with\nclass number 200 performs comparable to the\nRNNLM in both perplexity and WER. Numerical-\nly, the relative reduction is 9.1% compared with\nthe KN4 LMs, and 0.5% compared with the RNN\nLM.\n7http://kaldi.sourceforge.net/\nmodel WER PPL (±std. dev.) #feat\nKN4 8.71 295.41 1.6M\nRNN 7.96 256.15 5.1M\nWSMEs (200c)\nw+c+ws+cs 8.87 ≈2.8 ×1012 5.2M\nw+c+ws+cs+cpw 8.82 ≈6.7 ×1012 6.4M\nTDRFs (100c)\nw+c 8.56 268.25±3.52 2.2M\nw+c+ws+cs 8.16 265.81±4.30 4.5M\nw+c+ws+cs+cpw 8.05 265.63±7.93 5.6M\nw+c+ws+cs+wsh+csh 8.03 276.90±5.00 5.2M\nTDRFs (200c)\nw+c 8.46 257.78±3.13 2.5M\nw+c+ws+cs 8.05 257.80±4.29 5.2M\nw+c+ws+cs+cpw 7.92 264.86±8.55 6.4M\nw+c+ws+cs+wsh+csh 7.94 266.42±7.48 5.9M\nTDRFs (500c)\nw+c 8.72 261.02±2.94 2.8M\nw+c+ws+cs 8.29 266.34±6.13 5.9M\nTable 3: The WERs and PPLs on the WSJ’92 test\ndata. “#feat” denotes the feature number. Differ-\nent TDRF models with class number 100/200/500\nare reported (denoted by “100c”/“200c”/“500c”)\n5.3 Comparison and discussion\nTDRF vs WSME . For comparison, Table 3 also\npresents the results from our implementation of\nthe WSME model (3), using the same features as\nin Table 1. This WSME model is the same as in\n(Rosenfeld, 1997), but different from (Rosenfeld\net al., 2001), which uses the traditional n-gram\nLM as the priori distribution p0.\nFor the WSME model (3), we can still use a\nSA training algorithm, similar to that developed in\nSection 3.2, to estimate the parameters λ. But in\nthis case, there is no need to introduce ζl, because\nthe normalizing constants Zl(λ) are canceled out\nas seen from (7). Speciﬁcally, the learning rate γλ\nand the L2 regularization are conﬁgured the same\nas in TDRF training. A ﬁxed number of iterations\nwith t0 = 5000 is performed. The total iteration\nnumber is 10000, which is similar to the iteration\nnumber used in TDRF training.\nIn order to calculate perplexity, we need to\nestimate the global normalizing constant Z(λ) =∑m\nl=1 Zl(λ) for the WSME model. Similarly\nas in (Tan, 2015), we apply the SA algorithm\nin Section 3.2 to estimate the log normalizing\nconstants ζ, while ﬁxing the parameters λ to be\nthose already estimated from the WSME model\nand using uniform probabilities πl ≡m−1.\nThe resulting PPLs of these WSME models are\nextremely poor. The average test log-likelihoods\nper sentence for these two WSME models are\n792\n−494 and −509 respectively. However, the W-\nERs from using the trained WSME models in\nhypothesis re-ranking are not as poor as would be\nexpected from their PPLs. This appears to indicate\nthat the estimated WSME parameters are not so\nbad for relative ranking. Moreover, when the\nestimated λ and ζ are substituted into our TDRF\nmodel (6) with the empirical length probabilities\nπl, the “corrected” average test log-likelihoods\nper sentence for these two sets of parameters are\nimproved to be −152 and −119 respectively. The\naverage test log-likelihoods are both −96 for the\ntwo corresponding TDRF models in Table 3. This\nis some evidence for the model deﬁciency of the\nWSME distribution as deﬁned in (3), and intro-\nducing the empirical length probabilities gives a\nmore reasonable model assumption.\nTDRF vs conditional ME. After training, TDRF\nmodels are computationally more efﬁcient in com-\nputing sentence probability, simply summing up\nweights for the activated features in the sentence.\nThe conditional ME models (Khudanpur and Wu,\n2000; Roark et al., 2004) suffer from the expen-\nsive computation of local normalization factors.\nThis computational bottleneck hinders their use\nin practice (Goodman, 2001b; Rosenfeld et al.,\n2001). Partly for this reason, although building\nconditional ME models with sophisticated features\nas in Table 1 is theoretically possible, such work\nhas not been pursued so far.\nTDRF vs RNN . The RNN models suffer from\nthe expensive softmax computation in the output\nlayer 8. Empirically in our experiments, the aver-\nage time costs for re-ranking of the 1000-best list\nfor a sentence are 0.16 sec vs 40 sec, based on\nTDRF and RNN respectively (no GPU used).\n6 Related Work\nWhile there has been extensive research on con-\nditional LMs, there has been little work on the\nwhole-sentence LMs, mainly in (Rosenfeld et al.,\n2001; Amaya and Bened ´ı, 2001; Ruokolainen et\nal., 2010). Although the whole-sentence approach\nhas potential beneﬁts, the empirical results of pre-\nvious WSME models are not satisfactory, almost\nthe same as traditional n-gram models. After\nincorporating lexical and syntactic information,\na mere relative improvement of 1% and 0.4%\n8This deﬁciency could be partly alleviated with\nsome speed-up methods, e.g. using word clustering\n(Mikolov, 2012) or noise contrastive estimation (Mnih and\nKavukcuoglu, 2013).\nrespectively in perplexity and in WER is reported\nfor the resulting WSEM (Rosenfeld et al., 2001).\nSubsequent studies of using WSEMs with gram-\nmatical features, as in (Amaya and Bened ´ı, 2001)\nand (Ruokolainen et al., 2010), report perplexity\nimprovement above 10% but no WER improve-\nment when using WSEMs alone.\nMost RF modeling has been restricted to ﬁxed-\ndimensional spaces 9. Despite recent progress,\nﬁtting RFs of moderate or large dimensions re-\nmains to be challenging (Koller and Friedman,\n2009; Mizrahi et al., 2013). In particular, the\nwork of (Pietra et al., 1997) is inspiring to us,\nbut the improved iterative scaling (IIS) method\nfor parameter estimation and the Gibbs sampler\nare not suitable for even moderately sized models.\nOur TDRF model, together with the joint SA al-\ngorithm and trans-dimensional mixture sampling,\nare brand new and lead to encouraging results for\nlanguage modeling.\n7 Conclusion\nIn summary, we have made the following contri-\nbutions, which enable us to successfully train T-\nDRF models and obtain encouraging performance\nimprovement.\n•The new TDRF model and the joint SA train-\ning algorithm, which simultaneously updates\nthe model parameters and normalizing con-\nstants while using trans-dimensional mixture\nsampling.\n•Several additional innovations including ac-\ncelerating SA iterations by using Hessian\ninformation, introducing word classing to ac-\ncelerate the sampling operation and improve\nthe smoothing behavior of the models, and\nparallelization of sampling.\nIn this work, we mainly explore the use of fea-\ntures based on word and class information. Future\nwork with other knowledge sources and larger-\nscale experiments is needed to fully exploit the\nadvantage of TDRFs to integrate richer features.\n8 Acknowledgments\nThis work is supported by Toshiba Corporation,\nNational Natural Science Foundation of China\n(NSFC) via grant 61473168, and Tsinghua Ini-\ntiative. We thank the anonymous reviewers for\nhelpful comments on this paper.\n9Using local ﬁxed-dimensional RFs in sequential models\nwas once explored, e.g. temporal restricted Boltzmann\nmachine (TRBM) (Sutskever and Hinton, 2007).\n793\nReferences\nFredy Amaya and Jos ´e Miguel Bened ´ı. 2001. Im-\nprovement of a whole sentence maximum entropy\nlanguage model using grammatical features. In\nAssociation for Computational Linguistics (ACL).\nAlbert Benveniste, Michel M ´etivier, and Pierre\nPriouret. 1990. Adaptive algorithms and stochastic\napproximations. New York: Springer.\nOlivier Bousquet and Leon Bottou. 2008. The\ntradeoffs of large scale learning. In NIPS, pages\n161–168.\nRichard H Byrd, SL Hansen, Jorge Nocedal, and\nYoram Singer. 2014. A stochastic quasi-newton\nmethod for large-scale optimization. arXiv preprint\narXiv:1401.7020.\nStanley F. Chen and Joshua Goodman. 1999. An em-\npirical study of smoothing techniques for language\nmodeling. Computer Speech & Language, 13:359–\n394.\nHanfu Chen. 2002. Stochastic approximation and its\napplications. Springer Science & Business Media.\nStanley F. Chen. 2009. Shrinking exponential lan-\nguage models. In Proc. of Human Language Tech-\nnologies: The 2009 Annual Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics.\nJoshua Goodman. 2001a. A bit of progress in language\nmodeling. Computer Speech & Language, 15:403–\n434.\nJoshua Goodman. 2001b. Classes for fast maximum\nentropy training. In Proc. of International Confer-\nence on Acoustics, Speech, and Signal Processing\n(ICASSP).\nPeter J. Green. 1995. Reversible jump markov\nchain monte carlo computation and bayesian model\ndetermination. Biometrika, 82:711–732.\nMing Gao Gu and Hong-Tu Zhu. 2001. Maxi-\nmum likelihood estimation for spatial models by\nmarkov chain monte carlo stochastic approximation.\nJournal of the Royal Statistical Society: Series B\n(Statistical Methodology), 63:339–355.\nSanjeev Khudanpur and Jun Wu. 2000. Maximum en-\ntropy techniques for exploiting syntactic, semantic\nand collocational dependencies in language model-\ning. Computer Speech & Language, 14:355–372.\nDaphne Koller and Nir Friedman. 2009. Probabilistic\ngraphical models: principles and techniques . MIT\npress.\nFaming Liang, Chuanhai Liu, and Raymond J Carroll.\n2007. Stochastic approximation in monte carlo\ncomputation. Journal of the American Statistical\nAssociation, 102(477):305–320.\nSven Martin, J¨org Liermann, and Hermann Ney. 1998.\nAlgorithms for bigram and trigram word clustering.\nSpeech Communication, 24:19–37.\nTomas Mikolov, Stefan Kombrink, Lukas Burget,\nJan H Cernocky, and Sanjeev Khudanpur. 2011.\nExtensions of recurrent neural network language\nmodel. In Proc. of International Conference on\nAcoustics, Speech and Signal Processing (ICASSP).\nTom´aˇs Mikolov. 2012. Statistical language models\nbased on neural networks. Ph.D. thesis, Brno\nUniversity of Technology.\nYariv Dror Mizrahi, Misha Denil, and Nando de Fre-\nitas. 2013. Linear and parallel learning of markov\nrandom ﬁelds. arXiv preprint arXiv:1308.6342.\nAndriy Mnih and Koray Kavukcuoglu. 2013. Learning\nword embeddings efﬁciently with noise-contrastive\nestimation. In Neural Information Processing Sys-\ntems (NIPS).\nStephen Della Pietra, Vincent Della Pietra, and John\nLafferty. 1997. Inducing features of random ﬁelds.\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence, 19:380–393.\nBrian Roark, Murat Saraclar, Michael Collins, and\nMark Johnson. 2004. Discriminative language\nmodeling with conditional random ﬁelds and the\nperceptron algorithm. In Proceedings of the 42nd\nAnnual Meeting on Association for Computational\nLinguistics (ACL), page 47.\nRonald Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.\n2001. Whole-sentence exponential language mod-\nels: a vehicle for linguistic-statistical integration.\nComputer Speech & Language, 15:55–73.\nRonald Rosenfeld. 1997. A whole sentence maximum\nentropy language model. In Proc. of Automatic\nSpeech Recognition and Understanding (ASRU).\nTeemu Ruokolainen, Tanel Alum ¨ae, and Marcus Do-\nbrinkat. 2010. Using dependency grammar features\nin whole sentence maximum entropy language mod-\nel for speech recognition. In Baltic HLT.\nHolger Schwenk. 2007. Continuous space language\nmodels. Computer Speech & Language , 21:492–\n518.\nIlya Sutskever and Geoffrey E Hinton. 2007. Learn-\ning multilevel distributed representations for high-\ndimensional sequences. In International Confer-\nence on Artiﬁcial Intelligence and Statistics (AIS-\nTATS).\nZhiqiang Tan. 2015. Optimally adjusted mixture sam-\npling and locally weighted histogram. In Technical\nReport, Department of Statistics, Rutgers University.\nLaurent Younes. 1989. Parametric inference for\nimperfectly observed gibbsian ﬁelds. Probability\ntheory and related ﬁelds, 82:625–645.\n794",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6380632519721985
    },
    {
      "name": "Joint (building)",
      "score": 0.5789597034454346
    },
    {
      "name": "Bin",
      "score": 0.5162265300750732
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.5051413178443909
    },
    {
      "name": "Computational linguistics",
      "score": 0.4994678497314453
    },
    {
      "name": "Natural language processing",
      "score": 0.42206746339797974
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36086201667785645
    },
    {
      "name": "Programming language",
      "score": 0.22603347897529602
    },
    {
      "name": "Engineering",
      "score": 0.20599707961082458
    },
    {
      "name": "Physics",
      "score": 0.07220214605331421
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I102322142",
      "name": "Rutgers, The State University of New Jersey",
      "country": "US"
    }
  ]
}