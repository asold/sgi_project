{
  "title": "BEHRT: Transformer for Electronic Health Records",
  "url": "https://openalex.org/W3017637887",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2898879642",
      "name": "Yikuan Li",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2751608690",
      "name": "Shishir Rao",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2227210337",
      "name": "Jose Roberto Ayala Solares",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2186042737",
      "name": "Abdelaali Hassaine",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2127490300",
      "name": "Rema Ramakrishnan",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2237919805",
      "name": "Dexter Canoy",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2116882681",
      "name": "Yajie Zhu",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2589677415",
      "name": "Kazem Rahimi",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2399110041",
      "name": "Gholamreza Salimi Khorshidi",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2898879642",
      "name": "Yikuan Li",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2751608690",
      "name": "Shishir Rao",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2227210337",
      "name": "Jose Roberto Ayala Solares",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2186042737",
      "name": "Abdelaali Hassaine",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2127490300",
      "name": "Rema Ramakrishnan",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2237919805",
      "name": "Dexter Canoy",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2116882681",
      "name": "Yajie Zhu",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2589677415",
      "name": "Kazem Rahimi",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2399110041",
      "name": "Gholamreza Salimi Khorshidi",
      "affiliations": [
        "University of Oxford"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2946185430",
    "https://openalex.org/W2752747624",
    "https://openalex.org/W2908201961",
    "https://openalex.org/W2905810301",
    "https://openalex.org/W6757493485",
    "https://openalex.org/W2625625371",
    "https://openalex.org/W2901424910",
    "https://openalex.org/W3000238064",
    "https://openalex.org/W1990652932",
    "https://openalex.org/W2027106132",
    "https://openalex.org/W2404901863",
    "https://openalex.org/W1974076946",
    "https://openalex.org/W2481271618",
    "https://openalex.org/W2255847468",
    "https://openalex.org/W2517259736",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W2119852447",
    "https://openalex.org/W2040492681",
    "https://openalex.org/W2265755165",
    "https://openalex.org/W1486250383",
    "https://openalex.org/W2944890781",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2898155085",
    "https://openalex.org/W6636364444",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2158698691",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W2956530858",
    "https://openalex.org/W6600651459",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2950035161",
    "https://openalex.org/W3105248300",
    "https://openalex.org/W2949190276",
    "https://openalex.org/W2937845937",
    "https://openalex.org/W3104523752",
    "https://openalex.org/W46659105",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2963271116",
    "https://openalex.org/W2950182411",
    "https://openalex.org/W2461746922",
    "https://openalex.org/W2346093960"
  ],
  "abstract": "Abstract Today, despite decades of developments in medicine and the growing interest in precision healthcare, vast majority of diagnoses happen once patients begin to show noticeable signs of illness. Early indication and detection of diseases, however, can provide patients and carers with the chance of early intervention, better disease management, and efficient allocation of healthcare resources. The latest developments in machine learning (including deep learning) provides a great opportunity to address this unmet need. In this study, we introduce BEHRT: A deep neural sequence transduction model for electronic health records (EHR), capable of simultaneously predicting the likelihood of 301 conditions in one’s future visits. When trained and evaluated on the data from nearly 1.6 million individuals, BEHRT shows a striking improvement of 8.0–13.2% (in terms of average precision scores for different tasks), over the existing state-of-the-art deep EHR models. In addition to its scalability and superior accuracy, BEHRT enables personalised interpretation of its predictions; its flexible architecture enables it to incorporate multiple heterogeneous concepts (e.g., diagnosis, medication, measurements, and more) to further improve the accuracy of its predictions; its (pre-)training results in disease and patient representations can be useful for future studies (i.e., transfer learning).",
  "full_text": "1Scientific  RepoRtS  |         (2020) 10:7155  | https://doi.org/10.1038/s41598-020-62922-y\nwww.nature.com/scientificreports\nBeHRt: transformer for electronic \nHealth Records\nYikuan Li1,2, Shishir Rao  1,2 ✉, José Roberto Ayala Solares1, Abdelaali Hassaine1, \nRema Ramakrishnan1, Dexter canoy  1, Yajie Zhu  1, Kazem Rahimi1 &  \nGholamreza Salimi-Khorshidi  1\nToday, despite decades of developments in medicine and the growing interest in precision healthcare, \nvast majority of diagnoses happen once patients begin to show noticeable signs of illness. Early \nindication and detection of diseases, however, can provide patients and carers with the chance of early \nintervention, better disease management, and efficient allocation of healthcare resources. The latest \ndevelopments in machine learning (including deep learning) provides a great opportunity to address \nthis unmet need. In this study, we introduce BEHRT: A deep neural sequence transduction model for \nelectronic health records (EHR), capable of simultaneously predicting the likelihood of 301 conditions \nin one’s future visits. When trained and evaluated on the data from nearly 1.6 million individuals, \nBEHRT shows a striking improvement of 8.0–13.2% (in terms of average precision scores for different \ntasks), over the existing state-of-the-art deep EHR models. In addition to its scalability and superior \naccuracy, BEHRT enables personalised interpretation of its predictions; its flexible architecture enables \nit to incorporate multiple heterogeneous concepts (e.g., diagnosis, medication, measurements, and \nmore) to further improve the accuracy of its predictions; its (pre-)training results in disease and patient \nrepresentations can be useful for future studies (i.e., transfer learning).\nThe field of precision healthcare aims to improve the provision of care through precise and personalised pre-\ndiction, prevention, and intervention. In recent years, advances in deep learning (DL) - a subfield of machine \nlearning (ML) - has led to great progress towards personalised predictions in cardiovascular medicine, radiology, \nneurology, dermatology, ophthalmology, and pathology. For instance, Ardila et al.\n1 introduced a DL model that \ncan predict the risk of lung cancer from a patient’s tomography images with 94.4% accuracy; Poplin et al.2 showed \nthat DL can predict a range of cardiovascular risk factors from just a retinal fundus photograph, and more exam-\nples can be found in other works\n3,4. A key contributing factor to this success, in addition to the developments in \nDL algorithms, was the massive influx of large multimodal biomedical data, including but not limited to, elec-\ntronic health records (EHR)\n5.\nThe adoption of EHR systems has greatly increased in recent years; hospitals that have adopted EHR systems \nnow exceeds 84% and 94% in the US and UK, respectively 6,7. As a result, EHR systems of a national (and/or a \nlarge) medical organisation now are likely to capture data from millions of individuals over many years. Each \nindividual’s EHR can link data from many sources (e.g., primary and hospital care) and hence contain “concepts” \nsuch as diagnoses, interventions, lab tests, clinical narratives, and more. Each instance of a concept can mean a \nsingle or multiple data points. Just a single hospitalisation, for instance, can generate thousands of data points for \nan individual, whereas a diagnosis can be a single data point (i.e., a disease code). This makes large-scale EHR a \nuniquely rich source of insight and an unrivalled data for training data-hungry ML models.\nIn traditional research on EHR data (including the ones using ML), individuals are represented by models as \na vector of attributes, or “features”\n8. This approach relies on experts’ ability to define the appropriate features and \ndesign the model’s structure (i.e., answering questions such as “what are the key features for this prediction?” or, \n“which features should have interactions with one another?”). Recent developments in deep learning, however, \nprovided us with models that can learn useful representations (e.g., of individuals or concepts) from raw or \nminimally-processed data, with minimal need for expert guidance\n9. This happens through a sequence of layers, \neach employing a large number of simple linear and nonlinear transformations to map their corresponding inputs \nto a representation; this progress across layers results in a final representation in which the data points form dis-\ntinguishable patterns.\n1Deep Medicine, Oxford Martin School, University of Oxford, Oxford, United Kingdom. 2These authors contributed \nequally: Yikuan Li and Shishir Rao. ✉e-mail: shishir.rao@stcatz.ox.ac.uk\nopen\n2Scientific  RepoRtS  |         (2020) 10:7155  | https://doi.org/10.1038/s41598-020-62922-y\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nSuch properties of DL models and their success in a wide range of applications led to their growing popularity \nin EHR research. One of the earliest works in applying deep learning to EHR, Liang et al .10 showed that deep \nneural networks can outperform support vector machines (SVM) and decision trees paired with manual feature \nengineering, over a number of prediction tasks on a number of different datasets. Tran et al.11 proposed the use \nof restricted Boltzmann machines (RBM) for learning a distributed representation of EHR, which was shown to \noutperform the manual feature extraction, when predicting the risk of suicide from individuals’ EHR. In a similar \napproach, Miotto et al.\n12 employed a stack of denoising autoencoders (SDA) instead of RBM, and showed that \nit outperforms many popular feature extraction and feature transformation approaches (e.g., PCA, ICA 13 and \nGaussian mixture models) for providing classifiers with useful patient representations to predict the onset of a \nnumber of diseases from EHR.\nThese early works on the application of DL to EHR did not take into account the subtleties of EHR data \n(e.g., the irregularity of the inter-visit intervals, and the temporal order or events). In an attempt to address this, \nNguyen et al.\n14 introduced a convolutional neural network (CNN) model called Deepr (Deep record) for predict-\ning the probability of readmission; they treated one’s medical history as a sequence of concepts (e.g., diagnosis \nand medication) and inserted a special word between each pair of consecutive visits to denote the time difference \nbetween them. In another similar attempt, Choi et al.\n15 introduced a shallow recurrent neural network (RNN) \nmodel to predict the diagnoses and medications that are likely to occur in the subsequent visit. Both these works \nemployed some embedding techniques to map non-numeric medical concepts to an algebraic space in which the \nsequence models can operate.\nOne of the improvements that was next introduced to the DL models of EHR aimed to enable them to capture \nthe long-term dependencies among events (e.g., key diagnoses such as diabetes can stay a risk factor over a per-\nson’s life, even decades after their first occurrence; certain surgeries may prohibit certain future interventions). \nPham et al.\n16 introduced a Long Short-Term Memory (LSTM) architecture with attention mechanism, called \nDeepCare, which outperformed standard ML techniques, plain LSTM, and plain RNN in tasks such as prediction \nof the onset of diabetes. In a similar development, Choi et al.\n17 proposed a model based on reverse-time attention \nmechanism to consolidate past influential visits using an end-to-end RNN model named RETAIN for the pre-\ndiction of heart failure. RETAIN outperformed most of the models at the time of its publication and provided a \ndecent baseline for the EHR learning research.\nGiven the success of deep sequence models and attention mechanisms in the past DL research for EHR, we \naim to build on some of the latest developments in deep learning and natural language processing (NLP)– more \nspecifically, Transformer architecture\n18 – while taking into account various EHR-specific challenges, and provide \nimproved accuracy for the prediction of future diagnoses. We named our model BEHRT (i.e., BERT for EHR), \ndue to architectural similarities that it has with (and our original inspirations that came from) BERT\n18, one of the \nmost powerful Transformer-based architectures in NLP .\nMethods\nData. In this study, we used Clinical Practice Research Datalink (CPRD)19: it contains longitudinal primary \ncare data from a network of 674 GP (general practitioner) practices in the UK, which is linked to secondary care \n(i.e., hospital episode statistics or HES) and other health and administrative databases (e.g., Office for National \nStatistics’ Death Registration). Around 1 in 10 GP practices (and nearly 7% of the population) in the UK contrib-\nute data to CPRD; it covers 35 million patients, among whom nearly 10 million are currently registered patients\n19. \nCPRD is broadly representative of the population by age, sex, and ethnicity. It has been extensively validated and \nis considered as the most comprehensive longitudinal primary care database\n20, with several large-scale epidemi-\nological reports19,21,22 adding to its credibility.\nHES, on the other hand, contains data on hospitalisations, outpatient visits, accident and emergency for all \nadmissions to National Health Service (NHS) hospitals in England23. Approximately 75% of the CPRD GP prac-\ntices in England (58% of all UK CPRD GP practices) participate in patient-level record linkage with HES, which \nis performed by the Health and Social Care Information Centre\n24. In this study, we only considered the data from \nGP practices that consented to (and hence have) record linkage with HES. The importance of primary care at the \ncentre of the national health system in the UK, the additional linkages, and all the aforementioned properties, \nmake CPRD one of the most suitable EHR datasets in the world for data-driven clinical/medical discovery and \nmachine learning.\nPre-processing of CPRD. We start with 8 million patients; in our first filtering, we only included patients \nthat are eligible for linkage to HES and meet CPRD’s quality standards (i.e., using the flags and time windows that \nCPRD provides to indicate the quality of one’s EHR). Furthermore, to only keep the patients that have enough \nhistory to be useful for prediction, we only kept individuals who have at least 5 visits in their EHR. At the end of \nthis process, we are left with \n=.P 16  million patients to train and evaluate BEHRT on. More details on our inclu-\nsion/exclusion steps, and the number of patients after each one of them, can be seen in Fig. 1.\nIn CPRD, diseases are classified using Read code25 and 10th revision of the International Statistical Classification \nof Diseases and Related Health Problems (ICD-10)26, for primary and hospital care, respectively. In ICD-10, one can \ndefine diseases at the desired level of granularity that is appropriate for the analysis of interest, by simply choosing \nthe level of hierarchy one wants to operate at; for instance, operating at ICD-10 chapter level will lead to 22 health \ncategories, while operation at ICD-10 sub-chapter level will lead to over 1,900. After that, we mapped both ICD-10 \ncodes (at level 4) and Read codes to Caliber codes\n27, which is an expert checked mapping dictionary from University \nCollege London. Eventually, this resulted in a total of =G 301 codes for diagnoses. We denote the list of all these \ndiseases as = =D d{} ii\nG\n1, where di denotes the ith disease code.\nIn our final data, for each patient ∈…p P{1,2 ,, } the medical history consists of a sequence of visits to GP \nand hospitals; each visit can contain concepts such as diagnoses, medications, measurements and more. In this \n3Scientific  RepoRtS  |         (2020) 10:7155  | https://doi.org/10.1038/s41598-020-62922-y\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nstudy, however, we are only considering the diagnoses; we denote each patient’s EHR as V vvv v{ ,,, ,}p ppp p\nn12 3 p=… , \nwhere np denotes the number of visits in patient p’s EHR, and v p\nj  contains the diagnoses in the jth visit, which can \nbe viewed as a list of mp\nj diagnoses (i.e., =…ddv {, ,}p\nj\nm1 p\nj ). In order to prepare the data for BEHRT, we order the \nvisits (hence diseases) temporally, and introduce a term to denote the start of medical history (i.e., CLS), and the \nspace between visits (i.e., SEP), which results in a new sequence, =…VC LS SEPS EP SEPvv v{, ,, ,, ,, }pp p p\nn12 p , that \nfrom now on will be how we see/denote each patient’s EHR as. This process is illustrated in Fig. 2.\nBEHRT: A Transformer-based Model for EHR. In this study, we aim to use a given patient’s past EHR to \npredict his/her future diagnoses (if any), as a multi-label classification problem (i.e., simultaneously predicting \na probability for each and every disease); this will result in a single predictive model that scales across a range of \ndiseases (as opposed to needing to train one predictive model per disease). Modelling EHR sequences requires \ndealing with four key challenges\n16: (C.1) complex and nonlinear interactions among past, present and future \nconcepts; (C.2) long-term dependencies among concepts (e.g., diseases occurring early in the history of a patient \neffecting events far later in future); (C.3) difficulties of representing multiple heterogeneous concepts of variable \nsizes and forms to the model; and (C.4) the irregular intervals between consecutive visits.\nFigure 1. Linkage and filtering of CPRD data. This flow lists all the key steps of our data cleaning and linkage \nprocedure. At each step, the number of patients that are included is shown. As you can see, we started with \nnearly 8 million patients and our final data (used for training and evaluation of our models) consists of 1.6 \nmillion patients, each meeting our inclusion criteria.\nFigure 2. Preparation of CPRD data for BEHRT. An example patient’s EHR sequence can be seen in the figure, \nwhich consists of 8 visits. In each visit, the record can consist of concepts such as diagnoses, medications and \nmeasurements; all these values are artificial and for illustration purposes. For this study, we are only interested \nin age and diagnoses. Therefore, as shown in at the bottom of the figure, we have taken only the diagnosis \nand age subset of the record to form the necessary sequences. This resulting sequence is how we represent \nevery patient’s EHR in our modelling process. Note that the visits shown in purple boxes are not going to be \nrepresented to the model, due to them lacking diagnoses.\n4Scientific  RepoRtS  |         (2020) 10:7155  | https://doi.org/10.1038/s41598-020-62922-y\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nSimilarities between sequences in EHR and natural language lead to successful transferability of techniques. \nFor instance, techniques such as BoW12, Skip-gram15, RNN17, and attention16,28 (a la their NLP usage) are used for \nlearning complex EHR representations. Also, Velupillai et al.29 also suggested to use NLP modelling for knowl-\nedge extraction, mental health treatment and large-scale clinical research from unstructured EHR. In addition, \nHuang et al.30 also proposed a NLP model (ClinicalBERT) to predict the 30-day hospital re-admission. In this \nstudy, we get our inspiration from the great success of Transformers31, and more specifically, a Transformer-based \narchitecture known as BERT18. We refer readers to the original papers18,31 for an exhaustive background descrip-\ntion for both Transformer and BERT.\nBy depicting diagnoses as words, each visit as a sentence, and a patient’s entire medical history as a document, \nwe facilitate the use of multi-head self-attention, positional encoding, and masked language model (MLM), for \nEHR – under a new architecture we call BEHRT. Figure 3b illustrates BEHRT’s architecture, which is designed \nto pre-train deep bidirectional representations of medical concepts by jointly conditioning on both left and right \ncontexts in all layers. The pre-trained representations can be simply employed for a wide range of downstream \ntasks, e.g., prediction of the next diseases, and disease phenomapping. Such bidirectional contextual awareness of \nBEHRT’s representations is a big advantage when dealing with EHR data, where due to variabilities in individuals’ \nhealth as well as practice of care, or simply due to random events, the order at which certain diseases happen can \nbe reversed, or the time interval between two diagnoses can be shorter or longer than actually recorded.\nBEHRT has many structural advantages over many of the previous methods for modelling EHR data. Firstly, \nwe use feed-forward neural networks to model the temporal evolution of EHR data through utilising various \nforms of sequential concepts in the data (e.g., age, order of visits), instead of using traditional state-of-the-art \nRNN and CNN that were explored in the past\n14,17. Recurrent neural networks are known to be notoriously hard \nto train due to their exploding and vanishing gradient problems32; these issues hamper these networks’ ability to \nlearn (particularly, when dealing with long sequential data). On the other hand, convolutional neural networks \nonly capture limited amount of information with convolutional kernels in the lower layers, and need to expand \ntheir receptive field through increasing the number of layers in a hierarchical architecture. BEHRT’s feed-forward \nstructure alleviates the exploding and vanishing gradient problems and captures information by simultaneously \nconsidering the full sequence -- a more efficient training through learning the data in parallel rather than in \nsequence (unlike the RNN).\nThe embedding layer in BEHRT, as shown in Fig. 3, learns the evolution of one’s EHR through a combination \nof four embeddings: disease, “position” , age, and “visit segment” (or “segment” , for short). This combination ena-\nbles BEHRT to define a representation that can capture one’s EHR in as much detail as possible. Disease codes are \nof course important in informing the model about the past diagnoses in one’s EHR. That is, there are many com-\nmon disease trajectories and morbidity patterns\n33 that knowing one’s past diseases can improve the accuracy of \nthe prediction for their future diagnoses. Positional encodings determine the relative position of concepts in EHR \nsequence, and enable the network to capture the positional interactions among diseases. In this paper, we used \nFigure 3. BEHRT architecture. Using the artificial data shown in Fig. 2, section (a) shows how BEHRT sees \none’s EHR. In addition to diagnosis and age, BEHRT also employs an encoding for an event’s positions (shown \nas POSITION) and an encoding for visit (shown as SEGMENT with A and B), which alternates between visits. \nThe summation of all these embeddings results in a final embedding shown at the bottom of (a), which will be \nthe latent contextual representation of one’s EHR at a given visit’s diagnosis. Section (b) on the other hand shows \nBEHRT’s Transformer-based architecture. It is first pre-trained by the MLM task, to learn the network \nparameters (including the disease embeddings) that can predict the masked disease tokens. When training the \nmodel in downstream tasks (i.e., T1 to T3 - detailed explanation can be found in section: Disease Prediction), \nthe model finetunes the weights pre-trained in the MLM task and learns the weights for the classification layer \n(i.e., mapping \nT1 to the pooling layer and finally to the subsequent diseases classifier).\n5Scientific  RepoRtS  |         (2020) 10:7155  | https://doi.org/10.1038/s41598-020-62922-y\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\na pre-determined encoding (as proposed by Vaswani et al.31), to avoid weak learning of positional embedding \ncaused by imbalanced distribution of medical sequence length in EHR. Given the feed-forward architecture of \nour network, positional encodings plays a key role in filling the gap resulting from the lack of a recurrent structure \nthat was historically the most common/successful approach for learning from sequences.\nAge and visit segment are two embeddings that are unique to BEHRT to further empower it in dealing with \nthe challenges we mentioned earlier. Age is known to be a key risk factor for most diseases (i.e., as we age, the \nrisk of many diseases increase). By embedding age and linking it to each visit/diagnosis, not only do we provide \nthe model with a sense of time (i.e., the time between events), but also we inform it of age as a universal epidemi-\nological notion of when things happened (that is comparable across patients). Visit segment can be either A or \nB, which are two symbols to represent two trainable vectors in the segment embeddings; it changes alternatively \nbetween visits and it is used for providing BEHRT with extra information to indicate the separation between visits \n(i.e., visit segments for two adjacent visits of a patient will always be different).\nNote that, there is no prescribed order for the multiple diagnoses within a visit. That is, for a given visit, the \nposition, age, and segment embedding will be identical; this makes BEHRT order-invariant for intra-visit con-\ncepts (i.e., invariant to the ordering of diagnoses within a visit). Thus, attention mechanism purely investigates \nintra-visit relationships among diseases.\nThrough a unique combination of the four aforementioned embeddings, we provide the model with disease \nsequences, a precise sense of timing of events, and data about the delivery of care. In other words, the model \nhas the ability to learn from the medical history and its corresponding age and visit patterns. All these, when \ncombined, can paint a picture of one’s health that traditionally we might have sought to capture through many \nfeatures manually extracted from EHR. Of course, we do not advocate for not using the full richness of the EHR \nand indeed see that as a follow up to this work. However, we aim to show how the complexity of our architecture \nwhen paired with this subset of EHR can still provide an accurate characterisation of one’s future health trajectory. \nBEHRT’s flexible architecture enables the use of additional concepts, e.g., by simply adding additional embed-\ndings to the existing four.\nPre-training BEHRT using masked language model (MLM). In EHR data, just like language, it is intu-\nitively reasonable to believe that a deep bidirectional model is more powerful than either a left-to-right model or \nthe shallow concatenation of a left-to-right and a right-to-left model. Therefore, we pre-trained BEHRT using the \nsame approach as the original BERT paper\n18, using MLM. We initialized disease, age, and segment embeddings \nrandomly, and the positional encoding, as discussed previously, stems from a pre-determined encoding of posi-\ntion.When training the network and specifically, the embeddings for the MLM task, we left 86.5% of the disease \nwords unchanged; 12% of the words were replaced with [mask]; and the remaining 1.5% of words, were replaced \nwith randomly-chosen disease words.\nUnder this setting, BEHRT does not know which of the disease words are masked, so it stores a contextual \nrepresentation of all of the disease words. Additionally, the small prevalence of change (i.e., only for 13.5% of all \ndisease words) will not hamper the model’s ability to understand the EHR data. Lastly, the replacement of the \ndisease words acts as injected noise into the model; it will distract the model from learning the true left and right \ncontext, and instead forces the model to fight through the noise and continue learning the overall disease trajec-\ntories. The pre-training MLM task was evaluated using precision score\n34, which calculates the ratio of true posi-\ntive over the number of predicted positive samples (precision calculated at a threshold of 0.5). The average is \ncalculated over all labels and over all patients. We see in Fig. 3b that the MLM classifier maps the tokens …T TN1  \nto the masked words.\nDisease Prediction. In order to provide a comprehensive evaluation of BEHRT, we assess its learning in \nthree predictive tasks: prediction of diseases in the next visit (T1), prediction of diseases in the next 6 months \n(T2), and prediction of diseases in the next 12 months (T3). In order to train our model and assess its predictions’ \naccuracy across these tasks, we first randomly allocated the patients into two groups: train and test (containing \n80% and 20% of the patients, respectively). To define the training examples (i.e., input-output pairs) for T1, we \nrandomly choose an index \nj ( <<jn3 p) for each patient and form =…xv v{, ,}pp p\nj1  and = +y wp j 1, as input and \noutput, respectively, where +wj 1 is a multi-hot vector of length G, with 1’s, indexed for diseases that exist in +v p\nj 1. \nNote that each patient contributes only one input-output pair to the training and evaluation process.\nFor both T2 and T3, the formation of input and label are slightly modified. First, patients that do not have 6 or \n12 months (for T2 and T3, respectively) worth of EHR (with or without a visit) after vp\n4 will not be included in the \nanalyses. Second, j is chosen randomly from (3, n*), where n* denotes the highest index after which there is 6 or \n12 months (for T2 and T3, respectively) worth of EHR (with or without a visit). Lastly, =y wp m6  and =y wp m12  \nare multi-hot vectors of length G, with 1 for concepts/diseases that exist in the next 6 and 12 months, respectively. \nAs a result of this final filtering of patients, we had 699 K, 391 K, and 342 K patients for T1, T2, and T3, respec-\ntively. A more comprehensive statistics for the population is shown in Supplementary Table S1. To clarify the task \ndesign, the BEHRT model is considered as “forced by design” to predict subsequent diseases in the patients’ med-\nical history, and only patients who have at least one diagnosis in the coming visit/6 months/12 months are \nincluded.\nWe feed these input medical histories into BEHRT for feature extraction. Next, as shown in Fig. 3b, the net-\nwork pools the information into a representation of the patient and passes it along to a single feed-forward classi-\nfier layer for output, subsequent visit prediction. Using this procedure, we train and test it three separate times for \neach of the three aforementioned tasks (T1–T3). We denote the model’s prediction for patient \np in the aforemen-\ntioned tasks as ⁎y p, where the ith entry is the model’s prediction of that person having di. The evaluation metrics \nwe used to compare y ’s and y*’s, are area under the receiver operating characteristic curve (AUROC) 35 and \n6Scientific  RepoRtS  |         (2020) 10:7155  | https://doi.org/10.1038/s41598-020-62922-y\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\naverage precision score (APS)36; the latter is a weighted mean of precision and recall achieved at different thresh-\nolds. We calculated the APS and AUROC for each patient first, and then averaged the resulting APS and AUROC \nscores across all patients35,36.\nTo further investigate BEHRT’s predictive performance, we carried out three experiments: (1) We investigated \nif BEHRT can implicitly learn gender and utilise this latent understanding in subsequent visit prediction; (2) we \ncarried out an ablation study by selectively deactivating age, segment, and/or position embeddings and seeing \ntheir effects on APS and AUROC; and (3) we assessed the model’s performance on the prediction of \nnew instances \nof diseases (i.e., predicting the labels/diagnoses, which had not appeared in one’s EHR history, at the time of pre-\ndiction). The number of diseases considered is the same as that in T1, T2, and T3.\nResources and Implementation.  In this project, we have Python for coding our models and analyses \npipelines. We relied on NVIDIA Titan Xp Graphical Processing Units (GPU) for pre-training, training, and test-\ning. BertAdam\n18 is used as optimiser for both MLM and disease prediction tasks.\nResults\nDisease Embedding. We used Bayesian Optimisation37 to find the optimal hyperparameters for the MLM \npre-training. The main hyperparameters here are the number of layers, the number of attention heads, hidden \nsize, and “intermediate size” – see the original BERT paper for the details. This process resulted in an optimal \narchitecture with 6 layers, 12 attention heads, intermediate layer size of 512, and hidden size of 288; For reprodu-\nceability purposes, we trained the MLM task for 100 epochs and the model’s performance was 0.6597 in precision \nscore. Further details can be found in Supplementary Table S2.\nGiven the importance of the disease embedding – resulting from training the MLM – and the effect that it can \nhave on the overall prediction results, we first show the performance of our pre-training process, which mapped \neach one of the \nG diseases to a 288-dimensional vector. Note that, for evaluating an embedding technique – even \nin NLP where the literature has a longer history and hence is more mature – there is not a single gold standard \nmetric38. In this study, we devised three approaches: visual investigation (i.e., in comparison with medical knowl-\nedge), medical validation by clinical professional, and evaluation in a prediction task. For the former, we used \nt-SNE\n39 to reduce the dimensionality of the disease vectors to 2 – results are shown in Fig. 4. Based on the result-\ning patterns, we can see that diseases that are known to co-occur and/or belong to the same clinical groups, are \ngrouped together. Note that, while most neighborhoods make sense and are aligned with medical knowledge, \nthere are diseases clusters that (due to extreme dimensionality reduction, for instance) might seem \ncounter-intuitive.\nA reassuring pattern that can be seen in Fig.  4, for instance, is the natural stratification of gender-specific \ndiseases. For instance, diseases that are unique to women (e.g., endometriosis, dysmenorrhea, menorrhagia, …) \nare quite distant from those that are unique to men (e.g., erectile dysfunction, primary malignancy of prostate, \n…). Such patterns seem to suggest that our disease embedding built an understanding of the context in which \ndiagnoses happen, and hence infer factors such as gender that it is not explicitly fed.\nFurthermore, the colour in Fig. 4 represent the original Caliber disease chapters (see the legends in the main \nsubplot). As can be seen, natural clusters are formed that in most cases consist of disease of the same chapter (i.e., \nthe same colour). Some of these clusters, however, are correlated but not identical to these chapters; for instance, \nmany eye and adnexa diseases are amongst nervous system diseases and many nervous system disease are also \namong many musculoskeletal diseases. Overall, this map can be seen as diseases’ correspondence to each other \nbased on 1.6 million people’s EHR.\nLastly, for each disease that occurred in at least 1% of the population (i.e., 87 diseases), we found the ten closest \ndiseases (using cosine similarity of their embeddings). Comparing these top-10 neighbourhoods against those \nprovided by a clinical researcher, we found a 0.757 overlap (i.e., nearly 76% of our 87 top-10 neighbourhoods were \nseen as clinically valid by a clinical expert). A full table of results for the 87 diseases is offered in Supplementary \nTable S3. The clinical researcher notes that while many of the most similar associations had clear overlap in symp-\ntomatology, some were graded to be poor disease associations. Thus, the researcher concludes that BEHRT has \na strong ability to understand the latent characteristics of the disease, without them being explicitly given to it.\nAttention and Interpretability. Another interesting property of BEHRT is its self-attention mechanism; \nthis gives it the ability to find the relationships among events which goes beyond temporal/sequence adjacency. \nThis self-attention mechanism is able to unearth deeper and more complex relationships between a diagnosis in \none visit and other diagnoses. We analyse the attention-based patterns for patients using the approach introduced \nby Vig\n40. These results for two example patients are shown in Fig. 5. Note that, since BEHRT is bidrectional, the \nself-attention mechanism captures non-directional relationships among diseases (i.e., their correspondence with \neach other, rather than one causing the other).\nFor patient A in Fig.  5, for example, the self-attention mechanism has shown strong connections between \nrheumatoid arthritis and enthesopathies and synovial disorders (far in the future of the patient). This is a great \nexample of where attention can go beyond recent events and find long-range dependencies among diseases. Note \nthat, as described earlier and illustrated in Fig. 3 , the sequence we model is a combination of four embedding \n(disease, age, segment, and position) that go through layers of transformations to form a latent space abstraction. \nWhile in Fig. 5 we labelled the cells with disease names, a more precise labelling will be diseases in their context \n(e.g., at a given age and visit).\nDisease Prediction. BEHRT after the MLM pre-training, can be considered a universal EHR feature extrac-\ntor that only with a small additional training can be employed for a range of downstream tasks. In this work, the \ndownstream task of choice is the multi-disease prediction problem that we described earlier. The results from the \n7Scientific  RepoRtS  |         (2020) 10:7155  | https://doi.org/10.1038/s41598-020-62922-y\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nevaluation of the model’s performance is shown in Table 1. This table demonstrates BEHRT’s superior predictive \npower compared to two of the most successful approaches in the literature (i.e., Deepr 12 and RETAIN17). To \ndemonstrate a fair comparison of models, we carefully analyse model architecture for Deepr and RETAIN. We \nnote Deepr models time between visits using special “words” and utilises demographic information (gender) for \nprediction. Regarding RETAIN, in addition to including gender to bolster predictive power, we implement an \narchitecture endorsed by the author\n41 that would augment the original architecture’s performance: we encode \ntime for the visits, as well as bidirectionality for the RNN framework. We used Bayesian Optimisation to find \nthe optimal hyperparameters for RETAIN and Deepr before the evaluation. More details on the hyperparameter \nsearch and optimisation can be found in Supplementary Tables S4 and S5. The three supervised subsequent pre-\ndiction task models were trained for 15–20 epochs.\nBesides comparing the APS, which provides an average view across all patients and all thresholds, we also \nassessed the model’s performance for predicting for each disease. To do so for a given disease \ndi, we only consid-\nered the ith entry in yp and ⁎y p vectors and calculated AUROC and APS scores, as well as their respective occur-\nrence ratios (percent of patients that have a given disease in their EHR) for comparison. The results for T2 (or, \nnext 6-months prediction task) is shown in Fig. 6. For visual convenience and in our analysis, we did not include \nrare diseases with prevalence of less than 1% in our data.\nThe result shows that BEHRT is able to make predictions with relatively high precision and recall for dis-\neases such as epilepsy (0.016), primary malignancy prostate (0.011), polymyalgia rheumatica (0.013), hypo \nor hyperthyroidism (0.047), and depression (0.0768). A numerical summary of this analysis can be found in \nSupplementary Table S6. Furthermore, a comparison of the general APS/AUROC trends across the three models \ncan be found in Supplementary Fig. S1.\nTo investigate the model’s performance further, we carried out an analysis to assess the model’s ability to learn \ngender-related concepts, without being given gender as an input variable; this “gender analysis” was done on \nthe results of the T2 task. Looking at the gender-specific diseases (e.g., endometriosis, female genital prolapse, \nFigure 4. Visual investigation of the disease embedding. In this image we see a graph of disease embeddings \nprojected in two dimensions where distance represents closeness of contextual association. The colors represent \nthe Caliber chapter. Most associations are accepted by medical experts and maintain the gender-based divisions \nin illnesses, among other things. We zoom in and profile four clusters in this plot – shown in subfigures (A–D).\n8Scientific  RepoRtS  |         (2020) 10:7155  | https://doi.org/10.1038/s41598-020-62922-y\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nand menorrhagia and polymenorrhoea that are unique to women, and hyperplasia of prostate, male infertil-\nity and erectile dysfunction that are unique to men), Supplementary Table S7 shows that, except for a few pre-\ndictions, BEHRT has correctly avoided matching male diseases to female patients and vice versa. One of the \nseemingly gender errors in our results, is “male infertility” , which has been assigned to female patients; after \nfurther investigations, we discovered that “male infertility” is recorded for both male (365) and female (1,734) \npatients; we also saw records of “female infertility” for both male (192) and female (2,306) patients. This was \ndue to gender-agnostic Read codes (K26y300, K26y400, K5B1100, K5By000, and others) mapped to both “male \ninfertility” and “female infertility” in the Caliber system. Disregarding “male infertility” and “female infertility” , \nwe see BEHRT naturally identifies gender in feature extraction and can make robust gender-specific conclusions \nfor subsequent disease prediction.\nAs mentioned earlier, BEHRT can accommodate variety of input concepts that exist in EHR. Therefore, selec-\ntion of the appropriate concepts can be an important aspect of designing the BEHRT architecture. Therefore, we \ncarried out an ablation study, where we selectively deactivated parts of the input space and assessed the mod-\nel’s performance for each resulting scenario; see Supplementary Table S8. While the results are not changing \nFigure 5. The analysis of BEHRT’s self-attention. This figure shows the EHR history of patients (A and B), \neach presented as two identical columns (shown chronologically, going downwards) for the convenience of \nassociation analysis. The left side of the column represents the disease of interest and the right column indicates \nthe corresponding associations to the highlighted disease on the left. The intensity of the blue on the right \ncolumn represents the strength of the attention score – the stronger the intensity, the stronger the association \nand hence the stronger the attention score. The attention scores are specifically retrieved from the attention \ncomponent of the last layer of BEHRT network.\nModel Name Next Visit (APS|AUROC) Next 6 M (APS|AUROC) Next 12 M (APS|AUROC)\nBEHRT 0.462|0.954 0.525|0.958 0.506|0.955\nDeepr 0.360|0.942 0.393|0.943 0.393|0.943\nRETAIN 0.382|0.921 0.417|0.927 0.413|0.928\nTable 1. Model performances in the prediction tasks.\n9Scientific  RepoRtS  |         (2020) 10:7155  | https://doi.org/10.1038/s41598-020-62922-y\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nsignificantly in terms of AUROC, we see a much bigger range in APS. Overall, the results illustrate that posi-\ntion and age are important features in modelling the EHR sequences; they both improved the baseline (i.e., \ndisease-only) model’s performance significantly. In contrast, adding segment to the baseline model resulted in a \nvery small (almost negligible) improvement.\nLastly, for a subset of diseases in the label that are occurring for the first time (first incidence), we calculate \npredictive performance of the three models in Table  2. BEHRT shows superior predictive performance in all \nthree tasks with respect to RETAIN and Deepr. The ranking of the three models in terms of performance (both \nAUROC/APS) is identical to the ranking shown in Table 1.\nDiscussion\nIn this paper, we introduced a novel deep neural network model for EHR called BEHRT; an interpretable per -\nsonalised risk model, which scales across a range of diseases and incorporates a wide range of EHR modalities/\nconcepts in its modular architecture. BEHRT can be pre-trained on a large dataset and then with small fine \ntuning will result in a striking performance in a wide range of downstream tasks. We demonstrated this property \nof the model by training and testing it on CPRD - one of the largest linked primary care EHR systems – for pre-\ndicting the next mostly likely diseases in one’s future visits. Based on our results, BEHRT outperformed the best \ndeep EHR models in the literature by more than ~8% (absolute improvement) in predicting for a range of more \nthan 300 diseases.\nBEHRT offers a flexible architecture that is capable of capturing more modalities of EHR data than we have \nalready demonstrated in this paper. We demonstrated this flexibility of BEHRT’s modular architecture by design-\ning a model that employed 4 key concepts from the EHR: diseases, age, segment, and position. Through this mix, \nthe model will not only have the ability to learn about the past diseases and their relationships with future diagno-\nses (i.e., learning disease trajectories), but also gain insights about the underlying generating process of EHR; we \ncan refer to this as the practice of care. In other words, the model will learn distributed/complex representations \nthat are capable of capturing concepts such as “this patient had diseases A and B at a young age, and suddenly, the \nfrequency of visits increased, and a new diagnosis C appeared; all these, plus the patient’s age, will increase the \nchance of disease D happening next” . In future works, one can add more to these four concepts and bring medi-\ncation, tests, and interventions to the model with minimum architectural changes – by only adding new rows to \nthe architecture depicted in Fig. 3a.\nFigure 6. Disease-wise precision analysis. Each circle in these graphs represents a disease, and its colour and \nsize denote the Caliber chapter and prevalence, respectively. Also, in these plots, we show APS and AUROC \non the x- and y-axis, respectively. Therefore, the further right and higher a disease, the better BEHRT’s job at \npredicting its occurrence in the next 6 months. Subplot (A) illustrated the full results, and subplots (B and C) \nillustrate the best and worst sections of the plot, in terms of BEHRT’s performance.\nModel Name Next Visit (APS|AUROC) Next 6 M (APS|AUROC) Next 12 M (APS|AUROC)\nBEHRT 0.216|0.904 0.228|0.907 0.226|0.905\nDeepr 0.095|0.800 0.104|0.814 0.098|0.805\nRETAIN 0.108|0.836 0.115|0.845 0.109|0.836\nTable 2. Model performances in the prediction tasks - First Incidence of Diseases.\n10Scientific  RepoRtS  |         (2020) 10:7155  | https://doi.org/10.1038/s41598-020-62922-y\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nOur primary objective in this study was to provide the field with an accurate model for the prediction of next/\nfuture diseases. While doing this, BEHRT provides multiple byproducts that each can be useful on their own and/\nor as a key component of future works. For instance, the disease embeddings resulting from BEHRT can provide \ngreat insights into how various diseases are related to each other; it goes beyond simple disease co-occurrence \nand rather learns the closeness of diseases based on their trajectories in a big population of patients. Furthermore, \nsuch pre-trained disease embeddings can be used by future researchers as a reliable disease vectors, ready for \nnumeric/algebraic operations. This is very similar to fields such as NLP , where sharing word vectors for other \nresearchers is a common practice. Additionally, we have shown that the disease correspondences that result from \nBEHRT’s attention mechanism can be useful for illustrating the disease trajectories for multi-morbid patients; \nnot only it shows how diseases co-occur, but also it shows the influence of certain diseases in one’s past on their \nfuture risk of other diseases. These correspondences are not strictly temporal but rather contextual. Through our \nanalyses of the supervised prediction tasks performance (T1, T2, and T3), we first see that BEHRT has the ability \nto make robust, gender-specific predictions without inclusion of gender. We also note that the ablation study has \nshowed us important embeddings for inclusion; while position was important for this task, we note that perhaps \nin diagnosing age-related diseases (e.g. Alzheimer’s disease, arthritis, etc), the age embedding might be more vital \nfor predictive power. Thirdly, we conclude that even for incident disease prediction, BEHRT performs better than \nRETAIN and Deepr. As a future work, we aim to provide these attention-visualisation tools to medical researchers \nto help them better understand the contextual meaning of a diagnosis in the midst of other diagnoses of patients. \nThrough this tool, medical researchers can even craft medical history timelines based on certain diseases or pat-\nterns and in a way, query our BEHRT model and visualiser to perhaps uncover novel disease contexts.\nDespite exploring some of the properties of BEHRT, there are a range of future works that one can base on \nBEHRT’s architecture and properties. For instance, BEHRT’s ability to summarise a patient’s health journey into a \nsimple vector enables its use in a wide range of machine learning and exploratory analysis frameworks. That is, in \nmost such analyses, one requires to measure the similarities between two medical concepts and/or two patients; \nthis has been shown to be achievable, regardless of the variabilities that are inherent in EHR data, going from one \npatient to the other. Another direction of future research, can be the use of BEHRT for other downstream tasks \nsuch as single-disease prediction tasks, or non-diagnosis tasks such as prediction of hospital readmission and/or \nmortality – all these are extremely important clinical events. In such analyses, BEHRT’s interpretability can also \nbe a useful feature to understand various pathways that will lead to a disease/event or result from a disease/event. \nTo improve BEHRT’s accuracy, also, one can rely on some of the well-known frameworks such as ensembles of \nBEHRTs that have been shown to be effective for other learners.\nData availability\nThe data that support the findings of this study are available from Clinical Practice Research Datalink (CPRD). \nThe link: https://www.cprd.com/Data explains more in depth about the nature and accessibility of the data. \nFurthermore, regarding accessibility, https://www.cprd.com/primary-care explains: “ Access to data from CPRD \nis subject to a full licence agreement containing detailed terms and conditions of use. Patient level datasets can be \nextracted for researchers against specific study specifications, following protocol approval from the Independent \nScientific Advisory Committee (ISAC). ” Thus, restrictions apply to the availability of these data, which were used \nunder license for the current study, and so are not publicly available.\nReceived: 2 October 2019; Accepted: 11 March 2020;\nPublished: xx xx xxxx\nReferences\n 1. Ardila, D. et al. End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography. \nNat. medicine 25, 954 (2019).\n 2. Poplin, R. et al. Prediction of cardiovascular risk factors from retinal fundus photographs via deep learning. Nat. Biomed. Eng. 2, 158 \n(2018).\n 3. Topol, E. J. High-performance medicine: the convergence of human and artificial intelligence. Nat. medicine 25, 44–56 (2019).\n 4. Esteva, A. et al. A guide to deep learning in healthcare. Nat. medicine 25, 24–29 (2019).\n 5. Shickel, B., Tighe, P . J., Bihorac, A. & Rashidi, P . Deep EHR: A Survey of Recent Advances in Deep Learning Techniques for \nElectronic Health Record (EHR) Analysis. IEEE journal biomedical health informatics 22, 1589–1604 (2017).\n 6. Electronic Public Health Reporting. ONC Annu. Meet., Available at: https://www.healthit.gov/sites/default/files/2018-12/\nElectronicPublicHealthReporting.pdf (2018).\n 7. Parasrampuria, S. & Henry, J. Hospitals’ Use of Electronic Health Records Data, 2015–2017. ONC Data Brief (2019).\n 8. Rahimian, F . et al. Predicting the risk of emergency admission with machine learning: Development and validation using linked \nelectronic health records. PLoS medicine 15, e1002695 (2018).\n 9. Solares, J. R. A. et al . Deep learning for electronic health records: A comparative review of multiple deep neural architectures. J. \nBiomed. Informatics 101, 103337, https://doi.org/10.1016/j.jbi.2019.103337 (2020).\n 10. Liang, Z., Zhang, G., Huang, J. X. & Hu, Q. V . Deep learning for healthcare decision making with EMRs. In 2014 IEEE International \nConference on Bioinformatics and Biomedicine (BIBM), 556–559 (IEEE, 2014).\n 11. Tran, T., Nguyen, T. D., Phung, D. & Venkatesh, S. Learning vector representation of medical objects via EMR-driven nonnegative \nrestricted Boltzmann machines (eNRBM). J. Biomed. Informatics, https://doi.org/10.1016/j.jbi.2015.01.012 (2015).\n 12. Miotto, R., Li, L., Kidd, B. A. & Dudley, J. T. Deep Patient: An Unsupervised Representation to Predict the Future of Patients from \nthe Electronic Health Records. Sci. reports 26094 (2016).\n 13. Cao, L., Chua, K. S., Chong, W ., Lee, H. & Gu, Q. A comparison of PCA, KPCA and ICA for dimensionality reduction in support \nvector machine. Neurocomputing 55, 321–336 (2003).\n 14. Nguyen, P ., Tran, T., Wickramasinghe, N. & Venkatesh, S. Deepr: A Convolutional Net for Medical Records. IEEE journal biomedical \nhealth informatics 21, 22–30 (2016).\n 15. Choi, E., Bahadori, M. T., Schuetz, A., Stewart, W . F . & Sun, J. Doctor AI: Predicting Clinical Events via Recurrent Neural Networks. \nIn Machine Learning for Healthcare Conference, 301–318 (2016).\n\n11Scientific  RepoRtS  |         (2020) 10:7155  | https://doi.org/10.1038/s41598-020-62922-y\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\n 16. Pham, T., Tran, T., Phung, D. & Venkatesh, S. DeepCare: A Deep Dynamic Memory Model for Predictive Medicine. In Pacific-Asia \nConference on Knowledge Discovery and Data Mining, 30–41 (Springer, 2016).\n 17. Choi, E. et al. RETAIN: An Interpretable Predictive Model for Healthcare using Reverse Time Attention Mechanism. In Advances \nin Neural Information Processing Systems, 3504–3512 (2016).\n 18. Devlin, J., Chang, M.-W ., Lee, K. & Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language \nUnderstanding. arXiv preprint arXiv:1810.04805 (2018).\n 19. Herrett, E. et al. Data Resource Profile: Clinical Practice Research Datalink (CPRD). Int. journal epidemiology 44, 827–836 (2015).\n 20. Walley, T. & Mantgani, A. The UK General Practice Research Database. The Lancet 350, 1097–1099, https://doi.org/10.1016/S0140-\n6736(97)04248-7 (1997).\n 21. Emdin, C. A. et al. Usual blood pressure, peripheral arterial disease, and vascular risk: cohort study of 4.2 million adults. Bmj 351, \nh4865 (2015).\n 22. Emdin, C. A. et al . Usual blood pressure, atrial fibrillation and vascular risk: evidence from 4.3 million adults. Int. journal \nepidemiology 46, 162–172 (2016).\n 23. Lee, F ., Patel, H. & Emberton, M. The ‘Top 10’ Urological Procedures: A Study of Hospital Episodes Statistics 1998–99. BJU \ninternational 90, 1–6 (2002).\n 24. Mohseni, H., Kiran, A., Khorshidi, R. & Rahimi, K. Influenza vaccination and risk of hospitalization in patients with heart failure: a \nself-controlled case series study. Eur. heart journal 38, 326–333 (2017).\n 25. NHS. Read Codes, Available at: https://digital.nhs.uk/services/terminology-and-classifications/read-codes (2019).\n 26. WHO. ICD-10 online versions, Available at: https://icd.who.int/browse10/2016/e (2019).\n 27. Kuan, V . et al. Articles A chronological map of 308 physical and mental health conditions from 4 million individuals in the English \nNational Health Service. The Lancet Digit. Heal. 1, e63–e77, https://doi.org/10.1016/S2589-7500(19)30012-3 (2019).\n 28. Cho, K. et al. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. arXiv preprint \narXiv:1406.1078 (2014).\n 29. Velupillai, S. et al. Using clinical Natural Language Processing for health outcomes research: Overview and actionable suggestions \nfor future advances. J. biomedical informatics 88, 11–19 (2018).\n 30. Huang, K., Altosaar, J. & Ranganath, R. ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission. arXiv preprint \narXiv:1904.05342 (2019).\n 31. Vaswani, A. et al. Attention Is All Y ou Need. In Advances in neural information processing systems, 5998–6008 (2017).\n 32. Pascanu, R., Mikolov, T. & Bengio, Y . On the difficulty of training Recurrent Neural Networks. arxiv (2012). 1211.5063.\n 33. MacMahon, S. et al. The Academy of Medical Sciences. Multimorbidity: a priority for global health research. The Acad. Med. Sci. \n1–127 (2018).\n 34. Powers, D. M. W . Evaluation: from Precision, Recall and F-measure to ROC, Informedness, Markedness and Correlation. arxiv  \n(2011).\n 35. Fawcett, T. An introduction to ROC analysis. Pattern Recognit. Lett., https://doi.org/10.1016/j.patrec.2005.10.010 (2006).\n 36. Zhu, M. Recall, precision and average precision. Dep. Stat. Actuar. Sci. Univ. Waterloo, Waterloo 2, 30 (2004).\n 37. Snoek, J., Larochelle, H. & Adams, R. P . Practical Bayesian Optimization of Machine Learning Algorithms. In Advances in neural \ninformation processing systems, 2951–2959 (2012).\n 38. Wang, B., Wang, A., Chen, F ., Wang, Y . & Kuo, C.-C. J. Evaluating Word Embedding Models: Methods and Experimental Results. \narXiv preprint arXiv:1901.09785 (2019).\n 39. Maaten, L. V . D. & Hinton, G. Visualizing Data using t-SNE. J. machine learning research 9, 2579–2605 (2008).\n 40. Vig, J. Visualizing Attention in Transformer-Based Language Representation Models. arXiv preprint arXiv:1904.02679 (2019).\n 41. Choi, E. “retain issue #3” , Available at: https://github.com/mp2893/retain/issues/3 (2016).\nAcknowledgements\nThis research was supported by grants from the Oxford Martin School (OMS), University of Oxford, National \nInstitute for Health Research (NIHR) Oxford Biomedical Research Centre, British Heart Foundation (BHF), \nand UKRI’s Global Challenge Research Fund (GCRF). The views expressed are those of the authors and not \nnecessarily those of the OMS, the BHF , the GRCF , the NIHR or the Department of Health and Social Care. This \nwork uses data provided by patients and collected by the NHS as part of their care and support and would not \nhave been possible without access to this data. The NIHR recognises and values the role of patient data, securely \naccessed and stored, both in underpinning and leading to improvements in research and care. We would also like \nto thank Wayne Dorrington for creating the illustrations in Figures 1–3.\nAuthor contributions\nY .L. and S.R. jointly conceived the experiment. S.R. and Y .L. jointly contributed to data cleaning and pipelining, \nBEHRT pre-training, and BEHRT next visit/6 months/12 months modelling. A.H. contributed to code mapping. \nS.R. and Y .L. jointly contributed to embedding analyses. Y .L. contributed to Precision analyses. S.R. contributed to \nattention mechanism/visualisation analyses. J.R.A.S. and S.R. contributed to embeddings visualisation. R.R. and \nK.R. contributed to medical validation of cosine similar embeddings. All authors: Y .L., S.R., J.R.A.S., A.H., R.R., \nD.C., Y .Z., K.R. and G.S.K. reviewed the manuscript.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary information is available for this paper at https://doi.org/10.1038/s41598-020-62922-y.\nCorrespondence and requests for materials should be addressed to S.R.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\n12Scientific  RepoRtS  |         (2020) 10:7155  | https://doi.org/10.1038/s41598-020-62922-y\nwww.nature.com/scientificreportswww.nature.com/scientificreports/\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Cre-\native Commons license, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons license and your intended use is not per-\nmitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the \ncopyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.\n \n© The Author(s) 2020",
  "topic": "Health records",
  "concepts": [
    {
      "name": "Health records",
      "score": 0.7409719228744507
    },
    {
      "name": "Medical diagnosis",
      "score": 0.7063779830932617
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6754552721977234
    },
    {
      "name": "Computer science",
      "score": 0.654128909111023
    },
    {
      "name": "Machine learning",
      "score": 0.6407699584960938
    },
    {
      "name": "Deep learning",
      "score": 0.6383565664291382
    },
    {
      "name": "Scalability",
      "score": 0.6353152394294739
    },
    {
      "name": "Electronic health record",
      "score": 0.5809657573699951
    },
    {
      "name": "Health care",
      "score": 0.5658950805664062
    },
    {
      "name": "Transfer of learning",
      "score": 0.5043984651565552
    },
    {
      "name": "Intervention (counseling)",
      "score": 0.43054407835006714
    },
    {
      "name": "Data science",
      "score": 0.4273068606853485
    },
    {
      "name": "Precision medicine",
      "score": 0.4252467155456543
    },
    {
      "name": "Medicine",
      "score": 0.32219600677490234
    },
    {
      "name": "Nursing",
      "score": 0.11129200458526611
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I40120149",
      "name": "University of Oxford",
      "country": "GB"
    }
  ]
}