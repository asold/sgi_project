{
  "title": "An Overview on Language Models: Recent Developments and Outlook",
  "url": "https://openalex.org/W4324108774",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2393119835",
      "name": "Wei, Chengwei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2144092192",
      "name": "Wang, Yun-Cheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1816970236",
      "name": "Wang Bin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744497908",
      "name": "Kuo, C. -C. Jay",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3153427360",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W3198659451",
    "https://openalex.org/W2963078909",
    "https://openalex.org/W2962717182",
    "https://openalex.org/W3176198948",
    "https://openalex.org/W2952902402",
    "https://openalex.org/W4300772098",
    "https://openalex.org/W2962935015",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W150057594",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W4238633816",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2888779557",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W1915251500",
    "https://openalex.org/W2013556410",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W4200594764",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W3106061119",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4226498390",
    "https://openalex.org/W2407039696",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2041167939",
    "https://openalex.org/W1989705153",
    "https://openalex.org/W4226124627",
    "https://openalex.org/W3021534166",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4309478822",
    "https://openalex.org/W2996851481",
    "https://openalex.org/W1970689298",
    "https://openalex.org/W2963928591",
    "https://openalex.org/W2097333193",
    "https://openalex.org/W4385893847",
    "https://openalex.org/W1575384945",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W4285798540",
    "https://openalex.org/W1597533204",
    "https://openalex.org/W3035736465",
    "https://openalex.org/W4385573687",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2437096199",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2090750282",
    "https://openalex.org/W2441154163",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W3171434230",
    "https://openalex.org/W2756487349",
    "https://openalex.org/W2963088785",
    "https://openalex.org/W2155033295",
    "https://openalex.org/W2682503061",
    "https://openalex.org/W2811030843",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W2577366047",
    "https://openalex.org/W3174150157",
    "https://openalex.org/W2134587001",
    "https://openalex.org/W3214536449",
    "https://openalex.org/W3100532709",
    "https://openalex.org/W4297411672",
    "https://openalex.org/W3021533447",
    "https://openalex.org/W2077444666",
    "https://openalex.org/W1979625042",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W46679369",
    "https://openalex.org/W3046357466",
    "https://openalex.org/W2951080837",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2161188302",
    "https://openalex.org/W1990005915",
    "https://openalex.org/W3153675281",
    "https://openalex.org/W4389520756",
    "https://openalex.org/W4385571824",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W3092185277",
    "https://openalex.org/W3183153947",
    "https://openalex.org/W196214544",
    "https://openalex.org/W3114326827",
    "https://openalex.org/W2049901611",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2996047202",
    "https://openalex.org/W4287890137",
    "https://openalex.org/W4285428908",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W3083494020",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W3004117589",
    "https://openalex.org/W3101891351",
    "https://openalex.org/W4387245356",
    "https://openalex.org/W2591264712",
    "https://openalex.org/W1984635093",
    "https://openalex.org/W1966812932",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W1994919150",
    "https://openalex.org/W2557436004",
    "https://openalex.org/W2116625254",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W3172335055",
    "https://openalex.org/W3034287667",
    "https://openalex.org/W2114123841",
    "https://openalex.org/W3184144760",
    "https://openalex.org/W2069143585",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2399344342",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2060277733",
    "https://openalex.org/W1749358154",
    "https://openalex.org/W2963141266",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2059800182",
    "https://openalex.org/W3103188966",
    "https://openalex.org/W3035643691",
    "https://openalex.org/W2964222268",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2251433671",
    "https://openalex.org/W2939111082",
    "https://openalex.org/W3208860256",
    "https://openalex.org/W3187255235",
    "https://openalex.org/W2963582782",
    "https://openalex.org/W2908230750",
    "https://openalex.org/W3036120435",
    "https://openalex.org/W4283329669",
    "https://openalex.org/W2808778018",
    "https://openalex.org/W2108540317",
    "https://openalex.org/W2963240019",
    "https://openalex.org/W2913129712",
    "https://openalex.org/W2953039212",
    "https://openalex.org/W2990391581",
    "https://openalex.org/W2159697950",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W4385572691",
    "https://openalex.org/W2091981305",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W4225412853",
    "https://openalex.org/W3213460052",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2124008567",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W4293182797",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4300466035",
    "https://openalex.org/W2032942114",
    "https://openalex.org/W2096175520",
    "https://openalex.org/W2996601440",
    "https://openalex.org/W3212893438",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W2995969307",
    "https://openalex.org/W3034340181",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3016339201",
    "https://openalex.org/W3202794254",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W4385572967",
    "https://openalex.org/W3042631625",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W4303182341",
    "https://openalex.org/W2160451571",
    "https://openalex.org/W4302176825",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W4288796004",
    "https://openalex.org/W2788277448",
    "https://openalex.org/W2001792610",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3172794097",
    "https://openalex.org/W3174702398",
    "https://openalex.org/W1601046647",
    "https://openalex.org/W2963362078",
    "https://openalex.org/W3166143997",
    "https://openalex.org/W1561264958",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W4287332702",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4308244910",
    "https://openalex.org/W3049346316",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W2293185259",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3101140821",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4287327373",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W2963022149",
    "https://openalex.org/W3104216863",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4307003748"
  ],
  "abstract": "Language modeling studies the probability distributions over strings of texts. It is one of the most fundamental tasks in natural language processing (NLP). It has been widely used in text generation, speech recognition, machine translation, etc. Conventional language models (CLMs) aim to predict the probability of linguistic sequences in a causal manner, while pre-trained language models (PLMs) cover broader concepts and can be used in both causal sequential modeling and fine-tuning for downstream applications. PLMs have their own training paradigms (usually self-supervised) and serve as foundation models in modern NLP systems. This overview paper provides an introduction to both CLMs and PLMs from five aspects, i.e., linguistic units, architectures, training methods, evaluation methods, and applications. Furthermore, we discuss the relationship between CLMs and PLMs and shed light on the future directions of language modeling in the pre-trained era.",
  "full_text": "AN OVERVIEW OF LANGUAGE MODELS :\nRECENT DEVELOPMENTS AND OUTLOOK\nChengwei Wei1, Yun-Cheng Wang1, Bin Wang2, and C.-C. Jay Kuo1\n1University of Southern California, Los Angeles, California, USA\n2National University of Singapore, Singapore\nchengwei@usc.edu\nABSTRACT\nLanguage modeling studies the probability distributions over strings of texts. It is one of the most\nfundamental tasks in natural language processing (NLP). It has been widely used in text generation,\nspeech recognition, machine translation, etc. Conventional language models (CLMs) aim to predict\nthe probability of linguistic sequences in a causal manner, while pre-trained language models (PLMs)\ncover broader concepts and can be used in both causal sequential modeling and fine-tuning for\ndownstream applications. PLMs have their own training paradigms (usually self-supervised) and\nserve as foundation models in modern NLP systems. This overview paper provides an introduction\nto both CLMs and PLMs from five aspects, i.e., linguistic units, architectures, training methods,\nevaluation methods, and applications. Furthermore, we discuss the relationship between CLMs and\nPLMs and shed light on the future directions of language modeling in the pre-trained era.\nKeywords Language model, Natural language processing, Pre-trained language model, Conventional language model.\n1 Introduction\nLanguage modeling studies the probability distributions over a sequence of linguistic units, such as words. It is one of\nthe most fundamental tasks and long-standing research topics in natural language processing (NLP). The developed\nlanguage models (LMs) find applications in many computational linguistic problems such as text generation, machine\ntranslation, speech recognition, natural language generation, question-and-answer systems, etc.\nThere are two major approaches to language modeling: 1) the statistical approach based on a relatively small corpus\nset, and 2) the data-driven approach based on a significantly larger corpus set. Conventional language models (CLMs)\npredict the probability of linguistic sequences in a causal manner. They can be learned by both language modeling\napproaches. The data-driven approach has become mainstream nowadays. It exploits a large number of corpora to train\nneural-network models, leading to pre-trained language models (PLMs). PLMs are then fine-tuned with task-specific\ndatasets and objectives for downstream applications. In this overview paper, we define CLMs as language models\nthat predict the probability of linguistic sequences in a causal manner. In contrast, PLMs refer to language models\npre-trained on a broad range of linguistic tasks and objectives. It is important to note that the two concepts are not\nexclusive. One LM can fall into both categories. For example, GPT models [1] can predict the probability of linguistic\nsequences in a causal manner. They are also pre-trained with various downstream tasks. We provide an overview of\nCLMs and PLMs and study them from five perspectives: 1) linguistic units, 2) architectures, 3) training methods, 4)\nevaluation methods, and 5) applications. In the end, we point out several future research directions.\nThe goal of CLMs is to model the probability distributions over sequences of linguistic units:\nP(u1, u2, ··· , ut), (1)\nwhere ui can be either a character, a word, a phrase, or other linguistic units. CLMs attempt to predict the next linguistic\nunit in a text sequence given its preceding contexts:\nP(ut|u<t) (2)\narXiv:2303.05759v2  [cs.CL]  3 Jul 2023\nOverview of Language Models\nCLMs are also called auto-regressive language models since the units are predicted in a causal way. Estimating the\nprobability of a text sequence as shown in Eq. (1) directly encounters the data sparsity problem. CLMs often estimate\nthe joint probability of the text sequence by decomposing a text sequence into smaller units. For example, CLMs\nleverage the chain rule and the conditional probability to estimate the joint probability in the form of\nP(u1, u2, ··· , ut) = P(u1)P(u2|u1)P(u3|u1, u2) ··· P(ut|u1, ...ut−1). (3)\nBefore the pre-training era, CLMs are often trained from scratch with a training corpus and, then, predict the probability\nof text sequences with respective applications. Representative models include N-grams LMs [2, 3, 4], exponential LMs\n[5, 6, 7] and earlier neural LMs [8, 9]. CLMs give a high probability to natural text sequences occurring frequently in\nthe real world. As a result, they play a fundamental role in text generation, speech recognition [10, 11, 12], and machine\ntranslation [13, 14, 15] until the emergence of PLMs. Nowadays, high-performance PLMs serve as the backbone of\nmany NLP systems. They are not limited to the causal predictive functionality of CLMs and provide more different\ntypes of LMs.\nThe differences between CLMs before the pre-training era and PLMs can be summarized below.\n• Training Methodology. With the development of deep learning, PLMs with neural network structures are\npre-trained by collections of massive unlabeled corpora to learn generic knowledge which is then transferred\nto downstream tasks by task-specific fine-tuning.\n• Causality Constraint. PLMs do not necessarily follow CLMs in predicting linguistic units as shown in Eq. (2).\nFor example, bidirectional LMs [16, 17] use both preceding and succeeding contexts to predict the missing\nlinguistic units via probability estimation:\nP(ut|u<t, u>t). (4)\nBidirectional LMs do not follow the causality constraint and the chain rule in Eq. (3), to access the probability\nof a text sequence, which makes it inherently different from CLMs.\n• Token Representation. Apart from the differences in the training paradigm and probability modeling, PLMs\nadopt a different representation for basic units called tokens. PLMs represent tokens by embedding them in a\nhigh-dimensional continuous space such as word embeddings [18, 19] and sentence embeddings [20, 21, 22].\nThe new representations offer a flexible and powerful tool that enables PLMs to handle a wide range of tasks.\nThis overview paper serves two objectives. On one hand, instead of only focusing on recently developed PLMs\n[23, 24, 25], we aim to provide a comprehensive overview of the basic concepts of LMs, the transition from CLMs to\nPLMs, LM’s recent developments and applications to beginners in the field. On the other hand, we would like to shed\nlight on future research directions and offer our outlook to experienced engineers and researchers in the NLP field. For\nexample, we cover large LMs (LLMs) in the survey as there are growing interests in LLMs due to the new services\nprovided by ChatGPT. Furthermore, we include efficient LMs as an emerging topic since there are increasing concerns\nabout large model sizes and high training costs of LLMs.\nThe rest of the paper is organized as below. We introduce several types of LMs that go beyond CLMs in Sec. 2, and\nprovide an overview of common ways to decompose text sequences into smaller linguistic units in Sec. 3. Sec. 4\nintroduces different model architectures. We discuss the training procedures of LMs in Sec. 5. Common evaluation\nmethods including, both intrinsic and extrinsic ones, are introduced in Sec. 6. The application of LMs to text generation\nis discussed in Sec. 7. We comment on the redundancy problem of LMs and analyze techniques for efficient LMs in\nSec. 8. Promising future research directions are pointed out in Sec. 9. Concluding remarks are given in Sec. 10\n2 Types of Language Models\nCLMs commonly refer to auto-regressive models that predict the next linguistic units given the preceding context as\nshown in Eq. (2). LMs can access the probability of a text sequence using the chain rule. The goal of CLMs is to decode\nthe probability of text sequences in a causal manner. In this section, we introduce more LMs that go beyond CLMs.\n2.1 Structural LM\nInstead of predicting linguistic units in a sequential or reversed sequential order, structural LMs [26, 27, 28, 29, 30]\npredict linguistic units based on pre-defined linguistic structures such as dependency or constituent parse trees. Structural\nLMs utilize the linguistic structure to bring linguistically relevant context closer to the linguistic unit to be predicted.\nFor example, given a parse tree structure, a structural LM can define the ancestor context A(ut) of ut as the sequence\n2\nOverview of Language Models\nFigure 1: The example of a dependency parse tree example [29].\nFigure 2: The use of different permutations in a natural sentence.\nfrom the root node to the parent of ut. For example, the ancestor sequence of word ‘strong’ is {‘binoculars’, ‘saw’,\nROOT} in Fig. 1. Then, the structural LM uses the ancestor context in the tree to predict the next linguistic unit as\nP(ut|A(ut)), (5)\nwhere A(ut) is the ancestor context of linguistic unit ut. Similar to CLMs, structural LMs are designed to model the\nprobability of text sequences. Differently, structural LMs decode the sequence probability in the order of their synthetic\nstructures. It has been successfully applied to sentence completion [28, 29] and speech recognition [26, 27].\n2.2 Bidirectional LM\nInstead of using the causal contexts to make predictions, bidirectional LMs utilize contexts from both directions as\nshown in Eq. (4). The masked LM is one representative bidirectional LM. It masks out linguistic units in a text\nsequence and, then, encodes their preceding and succeeding contexts to predict the masked linguistic units. Formally,\nthe prediction can be defined as the estimation of the following conditional probability\nP(um|¯S), (6)\nwhere um is the masked linguistic unit and ¯S is the corrupted text sequence by replacing a certain number of linguistic\nunits with [MASK] symbols. The goal of bidirectional LMs is to learn the inner dependency between linguistic units\nin an unsupervised manner. The trained model can inherit semantics meanings from large-scale unlabeled corpora.\nDifferent from CLMs that aim to model the generation probability of text sequences, pre-trained bidirectional LMs\nare used as the backbone that transfers the learned knowledge through further fine-tuning in various downstream\napplications.\n2.3 Permutation LM\nCLMs and masked LMs have their own advantages and disadvantages. A masked LM needs to create artificial tokens\nsuch as [mask], which never occur in downstream tasks while CLMs only condition on preceding context. The\npermutation LM [31] is a recently proposed LM that takes advantage of CLMs and masked LMs. Given an input\nsequence of linguistic units, permutation LMs randomize the order of input linguistic units and construct different\npermutations of the input sequence. Fig. 2 shows an example of different permutations given an input text sequence.\nLet Z be the set of all possible permutations. Permutation LMs predict the next linguistic unit, ut, in one permutation,\nZ, of the sequence based on\nP(ut|uZ\n<t), Z∈ Z. (7)\n3\nOverview of Language Models\n3 Linguistic Units\nTo estimate the probability of text sequences, LMs partition text sequences into small linguistic units such as characters,\nwords, phrases, or sentences. This process is called tokenization. The resulting linguistic units are called tokens.\nDifferent languages and models may have different appropriate tokenization methods. Here, we focus on English and\nuse it as an example. In this section, we examine typical tokenization methods used in language modeling according to\nunit sizes.\n3.1 Characters\nLMs can model text sequences probability based on characters [32, 33, 34, 35, 36]. As compared with other linguistics\nunits, using characters has a much smaller vocabulary size, leading to a smaller discrete space and model size. On\nthe other hand, it is challenging to predict the next character. Usually, it requires a long historical context. This\nmakes the performance of character-level LMs poorer than that of word-level LMs. In addition, the input and output\nlengths have to be longer to model the character distribution accurately. This results in higher computational costs,\nespecially for auto-regressive decoding. Several LM methods use the combination of words and characters to alleviate\nthe issue [37, 38, 39].\n3.2 Words and Subwords\nThe most natural tokenization for English is to decompose a text sequence into words by white spaces. Many LMs apply\nword tokenization. However, there are several issues of naive word tokenization. The first one is the Out-Of-V ocabulary\n(OOV) problem. Because an LM has a pre-defined vocabulary size that cannot be arbitrarily large. Less frequent words\nand words with character-level errors may not be stored in the pre-defined vocabulary. Thus, they cannot be retrieved\nfrom the dictionary. Although one can extend the vocabulary size to alleviate this problem, it will increase the model\nsize and still cannot handle all possible words.\nLMs beyond the word level still have the OOV problem while a single character is not semantically meaningful by\nthemselves. Recently, researchers are in favor of decomposing words into subwords if they do not appear in the\ndictionary. This offers a flexible and effective solution to the OOV problem [40, 41]. Several subword segmentation\nalgorithms are developed to boost the performance of LMs. They strike a balance between the good performance of\nword-level models and the flexibility of character-level models. Two subword segmentation approaches, statistics-based\nand linguistics-based, are presented below.\n3.2.1 Statistics-based Subword Tokenizers\nThe statistics-based subword tokenizers generate subword vocabulary purely based on the corpus. The associated\nmethods are derived from a compression point of view. They work by replacing the commonly appeared character\nsequences with a new symbol (word) that does not exist in the current vocabulary. Then, fewer bytes are needed for\ninformation transmission.\nByte Pair Encoding (BPE). BPE [42] is a simple data compression technique that replaces the most common pair\nof bytes in a sequence by a single unused byte recursively. It was adopted by [ 41] to solve the word segmentation\nproblem. That is, frequent characters or character sequences are merged to generate subwords. BPE is also used by\nseveral advanced PLMs such as GPT-2 [43] and RoBERTa [17] with the following algorithm, called the BPE merge\noperation.\n1. Prepare a training corpus and define the size of the subword vocabulary.\n2. Split all words into characters.\n3. Generate a new subword by merging a pair of characters or subwords with the highest frequency.\n4. Repeat step 3 until the desired vocabulary size is reached.\nAn illustration of the BPE merge operation conducted on a small dictionary is given in Fig. 3.\nWordPiece. [44] WordPiece is another data-driven subword algorithm. The difference between WordPiece and BPE is\nthat WordPiece merges the pair of A and B if they have the highest score P(AB)/P(A)P(B) (rather than the highest\nfrequency P(AB)) at each iterative step. For example, WordPiece merges the pair of “u” and “g” in Fig. 3 only if they\nhave the highest value, P(′ug′)/P(′u′)P(′g′), as compared with other pairs. WordPiece is used as the tokenization\nmethod in BERT [16], DistilBERT [45], and Electra [46].\n4\nOverview of Language Models\nFigure 3: Illustration of the BPE merge operation conducted on the dictionary {“hug\", “pug\", “pun\", “bun\"}. The\nvocabulary is initialized with all characters. Then, a new subword is created by merging the most frequent pair.\nThere are other statistics-based subword tokenizers such as Unigram [47]. SentencePiece 1, Huggingface tokenizers 2,\nand OpenNMT 3 are popular tokenizers. Their implementation contains the statistics-based subword tokenization.\nDifferent subword tokenizers and their performance comparison are studied in [48].\n3.2.2 Linguistics-based Subword Tokenizers\nLinguistics-based subword tokenizers exploit the linguistic knowledge and decompose words into smaller grammatical\nunits, such as morphemes or syllables. Such subword tokenizers are widely used in machine translation and speech\nrecognition among different languages [49, 50, 51, 52, 53, 54, 55]. For example, in machine translation, words formed\nby compounding, affixation, or inflection, can be conveniently translated by translating the morphemes, respectively.\nHowever, linguistics-based subword tokenizers are not as popular as statistics-based ones due to the complexity and the\nrule-based nature of language decomposition.\n3.3 Phrases\nThe semantic meaning of a single word can be ambiguous because of various contexts and set collocations. Since\nthe linguistic dictionary does not go beyond the word-level, the inter-word dependency is ignored. Phrase-level LMs\nreplace common and cohesive word sequences by phrases [56, 57, 58, 59]. Phrase-level LMs are suitable for some\napplications. For example, it is observed in [58] that short words with fewer syllables in automatic speech recognition\n(ASR) are more frequently misrecognized than longer ones. Since phrases provide longer phone sequences than their\nconstituents, they are more robust to recognition errors for ASR.\n3.4 Sentences\nAuto-regressive LMs with smaller linguistic units (e.g., characters, words, subwords, and phrases) rely on conditional\nprobabilities to estimate the probability of text sequences as given in Eq. (3). Sentence-level LMs [60, 61, 62, 63, 64]\navoid the use of the chain rule. They generate sentence features and, then, model the sentence probability directly.\nThis is because modeling the sentence probability directly is more convenient than that in Eq. (3) in encoding the\nsentence-level information. It is also easier to encode the inter-sentence information such as the effects of preceding\nutterances in a dialog flow.\n4 Architecture of Language Models\nIn this section, we conduct a survey on several common architectures to model the probability distributions of text\nsequences. They are N-gram, maximum entropy, and neural network models. While there are other LM architectures,\nlike Gaussian mixture LMs [65] and Hidden Markov LMs [66], we focus on the above-mentioned architectures due to\ntheir popularity in the research community. Furthermore, LMs can operate at various levels of linguistic units. For\n1https://github.com/google/sentencepiece\n2https://github.com/huggingface/tokenizers\n3https://github.com/OpenNMT/Tokenizer\n5\nOverview of Language Models\ngenerality and consistency with most recent literature, we use the term ‘token’ to refer to all linguistic units leveraged\nby different LMs for the rest of this paper.\n4.1 N-gram Models\nAn N-gram consists of N consecutive tokens from a text sequence. N-gram LMs [2, 3, 4] assume that the probability of\na token depends only on its preceding N-1 tokens and it is independent of other contexts. This is known as the Markov\nassumption. Thus, instead of using all historical contexts, N-gram LMs only use the previous N-1 tokens to predict the\ncurrent one; namely,\nP(ut|u<t) = P(ut|ut−N+1:t−1). (8)\nN-gram LMs calculate the conditional probability by counting the occurrence time of N-grams given a training corpus\nas\nP(ut|ut−N+1:t−1) = C(ut−N+1:t)\nC(ut−N+1:t−1). (9)\nN-gram LMs simplify the token probability calculation based on previous N-1 tokens, but they encounter two sparsity\nissues. First, if an N-gram, (ut−N+1:t), never occurs in the training corpus, the probability for the next tokens being ut\nis zero. Second, if the (N-1)-gram, (ut−N+1:t−1), in the denominator never occurs, we cannot calculate the probability\nof any tokens. These sparsity issues can be alleviated by smoothing techniques. A simple smoothing method [67, 68],\ncalled additive smoothing, is to add a small value to the count for every N-gram so as to avoid zero in the numerator\nand the denominator in Eq. (9). However, this simple smoothing is still deficient because it assigns the same probability\nfor N-grams that never occur in the training corpus.\nThere are more advanced smoothing techniques such as back-off and interpolation [ 69, 70, 71, 72, 73] that achieve\nbetter probability estimation. In back-off, lower-order N-grams are used for probability estimation if higher-order\nN-grams do not occur. For example, if C(ut−3:t−1) = 0, we back off to compute P(ut|ut−2:t−1). In interpolation,\ndifferent N-grams are considered for conditional probability computation. Mathematically, the N-gram probability is\nestimated by\nP(ut|ut−N+1:t−1) = λN P(ut|ut−N+1:t−1) + λN−1P(ut|ut−N:t−1)\n+ λN−2P(ut|ut−N−1:t−1) + ... + λ1P(ut), (10)\nwhere λi is the weight for each n-gram and PN\ni=1 λi = 1.\n4.2 Maximum Entropy Models\nMaximum Entropy models (also called the exponential models) [ 5, 6, 7] estimate the probability of text sequences\nusing feature functions in the form of\nP(u|h) = exp(aT f(u, u<t))P\nu′ exp(aT f(u′, u′\n<t)), (11)\nwhere f(u, u<t) is the feature function that generates the feature of token u and its historical context u<t,P\nw′ exp(aT f(u′, u′\n<t)) is a normalization factor, and a is a parameter vector derived by the Generalized Iterative\nScaling algorithm [74]. The features are usually generated from the N-grams.\n4.3 Feed-forward Neural Network (FNN) Models\nThe discrete nature of the N-gram model is its performance bottleneck even with advanced smoothing techniques.\nNeural LMs embrace the continuous embedding space (distributed representation) to overcome the data sparsity problem.\nFeed-forward Neural Network (FNN) LMs [8, 75, 76, 77] is one of the earlier neural network models.\nAn FNN LM takes historical contexts as the input, and outputs the probability distribution of tokens. As shown in Fig. 4,\neach token in the preceding context is represented as a vector through a projection layer (i.e., an embedding matrix).\nThese vectors of tokens are sent to the hidden layer with H hidden units followed by non-linear activation. Then, a\nsoftmax function is used to obtain the posterior probabilities for token candidates, P(ut = i|ut−N−1:t−1), which are\nthe probabilities of tokens given a specific history predicted by the language model.\nAn FNN LM uses a fixed window to collect fixed-length contexts. It is essentially a neural version of N-gram LMs. The\nFNN LM have several advantages over the N-gram LM by projecting tokens into continuous space. First, it can handle\nunseen N-grams by representing each token as an N-gram with a dense vector space. Second, it is storage-efficient\nsince it does not need to count and store the transition probability of conventional N-gram models.\n6\nOverview of Language Models\nFigure 4: The structure of FFN LMs, where ut−N+1, ..., ut−1 denotes the preceding contexts of ut in a fixed-window,\nand P, H, and O are the dimensions of the projection, the hidden layer, and the output layer, respectively.\n4.4 Recurrent Neural Network (RNN) Models\nIt is clearly insufficient to use the historical context in a fixed-length to predict the next token. In contrast to the\nlimited historical context used in the N-gram, maximum entropy and FNN LMs, Recurrent Neural Network (RNN)\nLMs [9, 78, 79, 80, 81] can exploit arbitrarily long histories to predict the next token.\nThe structure of a vanilla RNN LM is shown in Fig. 5. A token u(i) in position i is first converted into a one-hot\nrepresentation ˆu(i). Then, the recurrent hidden state, h(i + 1), is computed using the previous hidden state, h(i), and\nthe one-hot representation, ˆu(i), of token u(i) as\nh(i + 1) = f(W ˆu(i) + Uh(i)), (12)\nwhere f(·) is a non-linear activation function, W is the weight matrix of the connections from the input layer to the\nhidden layer, and U is the connection between the previous and current hidden layers, respectively. By iteratively\ncomputing the hidden states, RNN LMs can encode the historical context of varying length. Finally, the output layer\ngives the conditional probability of tokens y(t) = g(V h(t)), where V is the weight matrix connecting the hidden layer\nand the output layer and g(·) is the softmax activation function.\nIn theory, RNN LMs do not need the Markov assumption. They can use all preceding history to predict the next\ntoken. However, the inherent gradient vanishing problem of RNN hampers the learning of the model [82]. Since the\ngradient may become very small over a long distance, model weights are actually updated by the nearby context in\npractice. Generally, RNN LMs cannot learn the dependency between the current token and its far-away historical\ncontext. Although an attention mechanism can be introduced to RNNs to alleviate this problem [83, 84]. The inherent\nsequential nature of RNNs makes them less powerful than transformer-based LMs with a self-attention mechanism.\n4.5 Transformers\nThe transformer architecture [85] can capture long-term dependencies and important sequence components by exploiting\na self-attention mechanism. Unlike the recurrent structure of RNNs, a transformer is easy to parallelize in both training\nand inference. Its structure is shown in Fig. 6. It consists of an encoder and a decoder. Before being sent to the encoder,\nthe input textual sequence is first converted to an embedding through an embedding layer plus positional embedding.\n7\nOverview of Language Models\nFigure 5: The structure of RNN LMs.\nMulti-head attention, which is an ensemble of multiple self-attention mechanisms, enables the transformer to capture\nmore robust and diverse attention between tokens. The other parts in the transformer encoder include feed-forward\nlayers, residual connections, and normalization layers. The difference between the transformer encoder and decoder is\nthat the transformer decoder has an additional masked multi-head attention layer. The masking ensures the decoder can\nonly access preceding tokens of the current one, which makes the decoder auto-regressive.\nBased on different purposes, transformers have encoder-only, decoder-only, and encoder-decoder three variants as\nshown in Table 1 and Fig. 7. Encoder-only models can access all positions given an input and utilize bi-directional\ncontexts to predict tokens. They are suitable for tasks requiring understanding full sentences, such as text classification.\nTransformer decoder-only models can only use previous tokens to predict the current token (namely, auto-regressive\nmodels). They are good at text generation tasks such as story generation. Transformer encoder-decoder models can\naccess all tokens in the encoding phase, and tokens before the current token in the decoding phase. They are suitable for\nsequence-to-sequence tasks such as translation and summarization.\nEncoder-only models\n(Bidirectional)\nBERT [16]\nRoBERTa [17]\nELECTRA [46]\nDecoder-only models\n(Unidirectional)\nPaLM [86]\nGPT-1, 2 and 3 [1, 43, 87]\nTransformer XL [88]\nEncoder-Decoder models\n(Sequence to sequence)\nBART [89]\nT5 [90]\nTable 1: Transformer-based PLMs.\n5 Pre-trained Language Models\nPre-trained language models (PLMs) are dominating in the NLP field nowadays. With the development of deep\nlearning, the training and usage of PLMs have changed a lot as compared with conventional statistical LMs. Before\nbeing applied to real-world tasks, PLMs are first pre-trained on massive collections of corpora so that they learn\nuniversal representations that carry both syntactic and semantic knowledge. After pre-training, PLMs are fine-tuned for\ndownstream tasks so that the acquired knowledge can be transferred to different tasks. In the following, we first explain\nthe pre-training objectives in Sec. 5.1 and then talk about how to adapt PLMs to various tasks of interest through\nfine-tuning in Sec. 5.2. It is also worthwhile to point out several good survey papers on PLMs, e.g., [23, 24, 25].\n5.1 Pre-training\nThe most commonly used pre-training task is “missing token prediction”. There are other pre-training tasks for different\npurposes, e.g., next-sentence prediction, which allows an LM to learn sentence relationships.\n8\nOverview of Language Models\nFigure 6: The structure of a transformer [85].\nToken Prediction. Auto-regressive language LMs [1, 43, 87] are trained to predict the next token using previous tokens.\nWhile bidirectional LMs [16, 91, 17] mask a subset of tokens in a sample and learn to predict such masked tokens using\nthe rest of the context. For the latter, the most popular objective is the masked language model (MLM) objective as\nproposed in BERT [16]. The MLM objective is the cross-entropy loss in predicting masked tokens. It randomly masks\nout 15% of the input tokens and then predicts the masked tokens. The number of masked tokens is set to 15% based on\nexperimental verification. If the masking rate is too small, the model only learns from a limited number of masked\ntokens. On the other hand, if it is too large, there is not enough context to do reasonable predictions and models cannot\nlearn well.\nOther Pre-training Tasks. There are other pre-training tasks to make LMs learn better linguistic knowledge such\nas sentence relationships. For example, next sentence prediction is used as the pre-training task in BERT [16]. Next\nsentence prediction is formalized as a binary prediction task that decides whether two sentences are two consecutive\nsentences or not. In this way, a PLM can be used in downstream tasks that require the understanding of the relationship\nbetween two sentences, such as Question Answering (QA) and Natural Language Inference (NLI). Other pre-training\nobjectives are adopted by BART [89]. They include token deletion, text infilling, sentence permutation, and document\nrotation to corrupt the original sequence for reconstruction. Shuffled tokens are used in T5 [90] to increase the robustness\nof the learned representation.\n9\nOverview of Language Models\nFigure 7: Illustration of different transformer models, where BERT is the encoder-only model, GPT is the decoder-only\nmodel, and BART is the encoder-decoder model.\n5.2 Fine-Tuning, Adapter Tuning and Prompt Tuning\nPLMs learn non-task-specific language knowledge in the pre-training stage. Fine-tuning performs task-specific\nadaptations of the model so that they can be applied to different downstream tasks. The model parameters are updated\nin the fine-tuning stage. One approach is to design task-specific heads based on different label spaces and losses in\ndifferent downstream tasks, then update the entire model and task-specific heads. For instance, GPT [ 1] and BERT\n[16] added an extra linear output layer as task-specific heads in their original papers, and fine-tuned the entire set of\nparameters in the PLMs and the heads for various downstream tasks, such as natural language inference, question\nanswering, semantic similarity, and text classification. To make the fine-tuning mechanism more parameter efficient,\none can choose to only update certain layers of an LM and the task-specific heads.\nAdapter tuning [92, 93, 94] is proposed to make fine-tuning even more parameter efficient compared with updating the\nlast layers of a PLM only. It injects additional compact layers, calls adapters, into the original PLMs. Then, the new\nadapter layers are updated, while the parameters of the original PLMs are frozen during adapter tuning. In this way, the\nparameters of the original PLMs can be shared by different downstream tasks.\nPLMs are pre-trained by one or several pre-training objectives and, then, applied to different downstream tasks. The\ngap between pre-training tasks and downstream task-specific fine-tuning can be substantial. Prompt-tuning [25] is used\nto discover the potential of PLMs by mimicking the pre-training objectives in the fine-tuning or inference stage. As\nPLMs get more powerful, they can handle various downstream tasks by seeing a few examples without any gradient\nupdates or fine-tuning. This is achieved by prompt-based fine-tuning (or prompt-tuning in short).\nThe prompt can be divided into discrete prompts (also called hard prompts) and continuous prompts (also called soft\nprompts). A discrete prompt is a natural text template that could be manually designed by humans [ 87, 95, 96] or\nautomatic methods [97, 98, 99]. On the contrary, continuous prompts [100, 101, 102, 103] are continuous vectors in\nthe embedding space that do not correspond to real text. It sacrifices interpretability but relaxes the discrete prompt\nconstraint in that prompts should be real texts.\n10\nOverview of Language Models\nFigure 8: An illustration of (a) LM pre-training, (b) standard fine-tuning, and (c) discrete prompt-based fine-tuning (or\nprompt-tuning) [97].\nFig. 8 shows an example of the pre-training task, fine-tuning and discrete prompt-tuning of MLMs. In the pre-training,\nMLMs are trained to predict masked tokens. Assuming that the downstream task is the sentiment analysis of the movie\nreview. In standard fine-tuning, we train a new head on the top of a PLM and predict the sentiment labels. The original\ninput appended with a designed prompt, say, ‘It was’, is sent to the PLM. The PLM has to assign probabilities to\ndesigned answers, which can be ‘great’ or ‘terrible’. If the probability of ‘great’ is higher, then the label of the input\nwill be positive and vice versa. In this way, prompt-tuning converts a distinct downstream task to the token prediction\ntask to narrow the gap between the pre-training and fine-tuning stages.\n6 Model Evaluation\nThere are two LM evaluation types: intrinsic evaluation and extrinsic evaluation. The intrinsic evaluation examines the\ninternal properties of an LM while the extrinsic evaluation studies its performance in downstream tasks.\n6.1 Intrinsic Evaluation\nAuto-regressive LM.LMs estimate the probability of text sequences. A good LM assigns higher probabilities to natural\ntext sequences and lower ones to unreal or random text sequences. The perplexity is a common evaluation metric for\nthis purpose. Given a testing text sequence, the perplexity, denoted by P P L, is defined as the inverse probability of the\nsequence normalized by the number of tokens. Mathematically, we have\nP P L(S) = N\ns\n1\n(P(u1u2...uN ), (13)\nwhere S = u1u2...uN is a testing text sequence. The perplexity can be rewritten in form of\nP P L(S) =\nN\nvuut\nNY\ni=1\n1\nP(ui|u1...ui−1). (14)\nA good LM should maximize the text set probability. It is equivalent to minimizing the perplexity. The lower the\nperplexity, the better the LM.\nBidirectional Language Model. To calculate the inverse probability in Eq. (13), the auto-regressive LMs can use\na sequence of conditional probabilities. However, this approach does not work for bidirectional LMs (or masked\nLMs). Several intrinsic evaluation metrics have been proposed for bidirectional LMs. The pseudo-log-likelihood score\n(PLL) [104] is defined as\nP LL(S) =\n|S|X\ni=1\nlog P(ui|S\\i), (15)\n11\nOverview of Language Models\nwhere log P(ui|S\\i) is the conditional probability of token ui in sentence S with all remaining tokens. Instead of\nmaximizing the joint probability of the entire text sequence, a good bidirectional LM should maximize the probability\nof each token in the text sequence given other tokens. Based on PLLs, the pseudo-Perplexity (PPPL) for corpora C is\ndefined as [105]\nP P P L(C) = exp(− 1\nN\nX\nS∈C\nP LL(S)). (16)\nBoth PLL and PPPL provide effective means to measure the naturalness of sentences for a bidirectional LM. For\nexample, it was shown in [105] that PLL and PPPL correlate well with the performance of an LM on downstream tasks,\nsuch as automatic speech recognition and machine translation.\n6.2 Extrinsic Evaluation\nAny downstream task of LMs can be used for extrinsic evaluation. There are several common downstream tasks selected\nas extrinsic evaluation benchmarks. Two popular ones are GLUE (General Language Understanding Evaluation) [106]\nand SuperGLUE [107]. GLU is an evaluation benchmark for natural language understanding. It contains single-sentence\ntasks, similarity and paraphrase tasks, and inference tasks. SuperGLUE is an enhanced version of GLUE. It includes a\nnew set of more challenging language understanding tasks, more diverse task formats, improved resources, and a public\nleaderboard.\n6.3 Relation between Intrinsic and Extrinsic Evaluations\nIf an LM achieves a lower perplexity, does that mean it can also perform well on downstream tasks? In other words, is\nthere any correlation between pre-training tasks (based on token prediction) and the downstream tasks? There are many\nempirical studies on this question but few theoretical studies.\nEmpirical Studies. Researchers design experiments to understand what kind of knowledge is learned by an LM from\nthe pre-training tasks. Examples include [ 108, 109, 110, 111, 112, 113]. They use part-of-speech tagging, constituent\nlabeling, and dependency labeling to measure the degree of syntactic knowledge learning, and named entity labeling,\nsemantic role labeling, and semantic proto-role for testing semantic knowledge. Empirical studies show that pre-training\ntasks help LMs learn the linguistic knowledge such as the grammar [111] and the semantic role [113]. However, these\nexperimental results can only be used as evidence supporting that the token prediction tasks benefit downstream tasks.\nThey cannot explain the underlying mechanism.\nTheoretical Studies. Some researchers attempt to build the connection between LM’s perplexities and its performance\non downstream tasks mathematically. The text classification tasks were studied in [114]. They first hypothesized and\nverified that text classification tasks can be reformulated as sentence completion tasks. Since the LM pre-training task\nis essentially a sentence completion task, it does help the text classification downstream task. Then, they quantified\nthe connection mathematically and showed that the features from LMs that achieve ϵ-optimal in log-perplexity can\nlinearly solve text classification tasks with O(√ϵ) error. An underlying generative model was utilized in [115] to show\nthe relationship between the pre-training tasks and the downstream tasks. Current theoretical studies are limited in the\nsense that only a specific downstream task (say, the text classification task) is considered and the proof holds under\ncertain conditions.\n6.4 Beyond Single Metric for LM Evaluation\nExcept for the evaluation of LM’s performance on standard evaluation test sets, the LM performance on other aspects is\nalso important in real-world applications, such as efficiency [116, 117, 118, 119], bias [120, 121, 122, 123], robustness\n[124, 125, 126, 127, 128, 129, 130], explainability [131], and logical consistency [132]. In this section, we discuss\nevaluations on efficiency, bias, and robustness to provide a holistic review of evaluation aspects.\nEfficiency of LMs can be evaluated in several aspects, such as inference time, computational complexity, energy\nconsumption, model size, and training data size. Some work [ 116, 119, 117, 133] calculated the computational\ncomplexity, approximate financial, and environmental costs of training PLMs. They also suggested practical steps to\nreduce expenses in NLP research and applications. Discussion on the model size of recently developed PLMs was given\nin [118]. In Sec. 8 of this paper, we also discussed several methods to achieve efficient LMs. Table 2 shows the number\nof parameters, training data, cost, and time of recently developed LMs.\nBias in NLP refers to systematic prejudices of models resulting from erroneous assumptions, such as racism, sexism,\nand ableism. Bias is reflected in PLMs since they are trained on a large volume of real word data. Several studies have\nexamined bias in PLMs. The Sentence Encoder Association Test (SEAT) was proposed in [ 120] to investigate bias\n12\nOverview of Language Models\nin BERT [16]. A dataset was created in [ 122] to measure bias against gender, profession, race, and religion across\nmultiple PLMs, including BERT [16], RoBERTa [17], XLNet [31] and GPT-2 [43]. It was demonstrated in [123] that\nGPT-3 [87] consistently exhibits a significant anti-Muslim bias in various tasks. The work in [121] surveyed 146 papers\non bias in NLP and made recommendations for analyzing bias in NLP systems.\nRobustness of LMs refers to their capacity to perform effectively and consistently when confronted with input variations\n(e.g., typos and misspellings) that should not affect the system’s output. In other words, a robust LM should not be\neasily fooled by adversarial text. Recent studies[126, 127, 128] created a set of character or word level perturbations to\nsimulate various types of noise that LMs may encounter in real-world scenarios. They examined robustness of recently\ndeveloped PLMs, including BERT, RoBERTa and XLNets. The results suggest that input perturbations, even minor\nalterations, can harm the performance of these LMs. In addition, Robustness Gym [129], WildNLP [124], and TextFlint\n[130] are tools designed for robustness evaluation.\n7 Language Models in Text Generation\nOne of the most important applications of LMs is text generation, which aims to generate sequences of words based\non the input data. There are many text generation tasks because of different purposes and inputs. For example,\nthe automatic speech recognition (ASR) task demands that the input be a speech sequence while the output be the\ncorresponding text sequence. The machine translation task generates the translated text sequence based on the input\ntext sequence and the target language. Story Generation is a topic-to-text generation task. In this section, we introduce\ncommon techniques used in text generation and then explain how LMs can be applied in each of the representative\ntasks.\n7.1 Decoding Methods\nDecoding decides the next output linguistic unit to generate text. A good decoding method should generate coherent\ncontinuation given a context. As LMs get more sophisticated, decoding methods have played an increasingly important\nrole. As shown in Fig. 9, deficient decoding methods lead to bad generated texts even with a powerful LM. There are\ntwo main decoding methods for text generation.\nFigure 9: Comparison of texts generated by the powerful GPT-2 large language model (LLM) using Beam search (left)\nand pure sampling decoding (right). Beam search yields degenerate repetition (in blue) while pure sampling results in\nincoherent gibberish (in red) [134].\nMaximization-based decoding. This is the most commonly used decoding objective. Assuming that the model assigns\na higher probability to a higher quality text which is closer to the ground truth written by humans, the maximization-\nbased decoding strategy searches for tokens with the highest probability as the generated text. Greedy search [135, 136]\nchooses the token with the highest probability as the next token in a greedy manner. Beam search [ 137, 138, 139]\nkeeps a certain number of most likely tokens at each time step and selects the generated token sequences with the\noverall highest probability eventually. It avoids missing reasonable tokens that do not have the highest probability.\nTrainable decoding algorithms have been proposed recently. Trainable greedy decoding [140] is a neural-based solution\nthat works as part of a neural machine translation decoder. It utilizes reinforcement learning to find a translation that\nmaximizes a decoding objective.\nSampling-based decoding. It chooses the next token from a set of sampled tokens. Because maximization-based\ndecoding depends highly on the underlying model probabilities and suffers from producing degenerate repetition,\nsampling-based decoding increases the diversity of generated texts by random sampling. However, the simple pure\n13\nOverview of Language Models\nsampling may choose a token with low probability (from an unreliable tail distribution) as the next generated token.\nAs a result, the generated text could be unrelated to the prefix, leading to incoherent gibberish. Top-k sampling\n[141] and Nucleus sampling [134] have recently been proposed to address this problem. Both Top-k sampling and\nNucleus sampling sample from truncated LM distributions (i.e., sampling from the most probable tokens). Diverse\nBeam search [137] is a trainable sampling-based (stochastic) decoding algorithm based on the Beam search. It uses\nreinforcement learning to determine the beam diversity parameters for different inputs or tasks.\n7.2 Dialogue Systems\nA dialogue system aims at simulating human responses when conversing with human users. Recent dialogue systems\nsuch as ChatGPT 4 and LaMDA [ 142] have attracted a lot of attention in the generative AI field because of their\nsuperior performance as interactive chatbot systems. Dialogue systems can be categorized into task-oriented systems\nand open-domain systems. The former is designed for specific tasks such as customer service for online shopping.\nThe latter is also known as chatbots [ 143]. Most modern dialogue systems are fine-tuned versions of generative LMs.\nTaking ChatGPT as an example, ChatGPT is built based on a generative LM, GPT-3 [87] with 175 billion parameters.\nIt is further fine-tuned by supervised learning and reinforcement learning on labeled data.\nLMs play an important role in dialogue systems, especially in their natural language understanding (NLU) and natural\nlanguage generation (NLG) components [144, 145]. NLU is responsible for understanding and recognizing users’ intent.\nNowadays, for encoder-decoder PLMs’, the encoders provide informative representations for NLU, while the associated\ndecoders are responsible for generating an appropriate response. The latter involves constructing the response text,\nselecting appropriate words, and determining the correct phrasing and tone. The effectiveness of representations of\nPLMs was examined in [146] for dialogue tasks. The evaluation PLM targets included BERT [ 16] and GPT-2 [43].\nThe few-shot capability of PLMs in dialogue tasks such as NLU and NLG was evaluated in [147]. Overall, LMs in\ndialogue systems play a key role in understanding users’ input and generating appropriate and natural responses.\n7.3 Automatic Speech Recognition\nAutomatic speech recognition (ASR) is a speech-to-text generation task that aims to transform raw audio input into\nthe corresponding text sequence. The LM plays an essential role in an ASR system. First, it helps solve acoustically\nambiguous utterances. Second, it can lower the computational cost by constraining the search space in a set of words of\nhigher probability. Conventional ASR systems contain two independent models, an acoustic model and a language\nmodel, which are related by\nP(word|sound) ∝ P(sound|word)P(word). (17)\nThe acoustic model is conditioned on phones P(sound|word) while the LM gives the word distribution denoted by\nP(word). LMs help search the word hypotheses during recognition. Different types of LMs have been explored in\nASR, such as N-gram [148, 149], FFNN [150], RNN [151, 152] and Transformer [153]\nWith the development of deep learning techniques, end-to-end (E2E) ASR systems have emerged as the dominant\napproach in this field nowadays. E2E ASR systems do not train the acoustic model and the language model independently\nbut use a single-network architecture. For example, the Listen, Attend, and Spell (LAS) model [154] contains an encoder,\na decoder, and an attention network, which are trained jointly to predict the output text sequence. The LM component\nin the E2E ASR system is implicitly learned from the transcribed speech data. To address the challenge of limited\ntranscribed speech data for LM’s training, one solution is to integrate external language models trained on extensive text\ncorpora using LM integration [155, 156]. Shallow fusion [157, 158, 159] considers log-linear interpolation between the\nscores from an E2E ASR model and an external LM at the decoding stage. Deep fusion [157] integrates an external LM\nand the E2E ASR model by fusing their hidden states. Unlike shallow fusion and deep fusion, where the E2E ASR\nmodel and the external LM are separately trained, cold fusion [160] and component fusion [161] train the E2E ASR\nmodel and the external LM jointly.\n7.4 Machine Translation\nMachine translation is a text-to-text generation task where the text in the source language is translated into that of the\ntarget language. LMs adopted by machine translation are conditioned on the source sentence and the previous partial\ntranslation. The E2E machine translation models become prevailing nowadays. The language model is implicitly learned\nthrough E2E training. Recently, transformer-based models achieve great success in machine translation [ 85, 162].\nSimilar to ASR advancements, an external LM trained by extensive monolingual corpora can be incorporated into\nan E2E machine translation model through LM integration techniques [157]. Furthermore, many PLMs have shown\n4https://openai.com/blog/chatgpt/\n14\nOverview of Language Models\ntheir few-shot or zero-shot ability on machine translation [87, 86] although they have never been explicitly trained on\ntranslation parallel data between the source and the target languages.\n7.5 Detection of Generated texts\nAs the performance of LMs gets closer to or even outperforms humans, the misuse of LMs, such as fake news and fake\nproduct reviews generation, has become a serious problem. The ability to detect machine-generated texts is important.\nThere are two types of detection problems: 1) human written vs. machine generated, and 2) inveracious vs. veracious.\nMost datasets, e.g., [163, 164, 165], are collected for the first type. Problems of the second type are much harder than\nthose of the first type [ 166] since one needs to connect generated text to the fact, which requires a high-level knowledge\nreasoning capability.\nTwo common approaches to detecting machine-generated text are reviewed below. One is to exploit the probability\ndistribution of LMs [167, 168]. If the probability distribution of a text sequence is closer to that of human-written texts\nas compared with known machine-generated texts, the text sequence is classified as human-written. The other is to train\nclassifiers with supervised learning [169, 164]. It converts the distribution to a supervised binary classification task. For\nmore details on the detection of machine-generated texts, readers are referred to two survey papers [170, 171].\n8 Efficient Models\nAs recent PLMs get more powerful, their model size, training cost, and demand for training data increase tremendously.\nThey need high computational resources and energy consumption, limiting their real-world applications. Table 2 shows\nthe model size, training data, cost, and time of recently developed LMs. This issue is a concern to many people and the\nconstruction of efficient LMs has received attention.\nModel Year Number of Parameters Training data Training cost Training time\nBERT-Large 2018 340M 3.3B words $7,0005 64 TPU chips\n4 days\nXLNet-Lagre 2019 340M 32.9B tokens $245,0005 512 TPU v3 chips\n5.5 days\nGPT-2 2019 1.5B 8 million web pages $12,902–$43,008 [116] 32 TPU v3 chip\n168 hours\nMegatron-LM 2019 8.3B 174 GB of text data 512 GPUs\n2 days per epoch\nT5 2019 11B 745GB of text data Over $1.3 million [172]\nTuring-NLG 2020 17B\nGPT-3 2020 175B 570GB of text data Over $4.6 million6 1024 A100 GPUs\n34 days [173]\nMegatron-Turing\nNLG 2022 530B 270B tokens 2K A100 GPUs\n3 months7\nTable 2: Table of the number of parameters, training data, cost, and time of several large LMs, where blank cells indicate\nthat the data are not available. The sources are cited if the data are not obtained from the original work\n8.1 Data Usage\nPre-training Data Size. A critical question for PLM training is how much data is needed. The effect of the pre-training\ndata size on the RoBERTa model was studied in [174]. The learning curves of four model performance measures as\na function of the pre-training dataset size are shown in Fig. 10. When the data size ranges between 100M and 1B\nwords, three learning curves gradually level off and it implies that LMs encode most syntactic and semantic features.\nHowever, a much larger quantity of data is needed for LMs to acquire enough common-sense knowledge and other\nskills to achieve better performance on downstream NLU tasks.\nEfficient Pre-Training. Several methods have been proposed to use the pre-training data more efficiently. In the\npre-training of masked LMs, a certain percentage of tokens are masked and need to be inferred by context. This\napproach incurs a substantial amount of computational cost because the network only learns from a certain percentage\n5 https://syncedreview.com/2019/06/27/the-staggering-cost-of-training-sota-ai-models/\n6https://lambdalabs.com/blog/demystifying-gpt-3\n7https://www.deepspeed.ai/\n15\nOverview of Language Models\nFigure 10: The performance curves as functions of the pre-training dataset size, where the classifier probing measures\nthe quality of the syntactic and semantic features, the minimum description length probing quantifies the accessibility of\nthese features, the BLiMP curve measures the model’s knowledge of various syntactic phenomena, and the superGLUE\nmeasures the capability of handling NLU tasks [174].\nof tokens which are masked. To enhance training efficiency, the work in [46] uses “replaced token detection\" (rather\nthan “masked token prediction\") as the pre-training task. As shown in Fig. 11, a generator is trained to perform the\nmasked LM and predicts the masked tokens. Then, the main model works as a discriminator, called ELECTRA, which\nlearns to decide the original or replaced tokens. In this way, pre-training tasks are conducted on all tokens instead of a\nsmall subset of masked tokens. Learning from all input positions causes ELECTRA to train much faster than BERT\nwhich adopts masked token prediction. Besides, ELECTRA achieves higher accuracy on downstream tasks when it is\nfully trained. Later, a new pre-training task using an energy-based model, which is closely related to ELECTRA, is\nproposed in [175].\nFigure 11: The structure of ELECTRA (Efficiently Learning an Encoder that Classifier Token Replacements Accu-\nrately) [46]\nBridging Pre-training and Downstream Tasks. A typical pre-training task is token prediction, which often has a large\ngap with downstream tasks. To mitigate the gap between pre-training and downstream tasks, prompt tuning has been\nstudied in [43, 97, 95, 176]. As illustrated in Fig. 8, the head is trained to predict the masked tokens in masked LMs.\nFor the downstream sentiment analysis task, the head is trained to predict the positive or the negative label in traditional\nfine-tuning. A template (e.g., ‘It was’) and its expected text responses (e.g., ‘great’ and ‘terrible’) are used in prompt\ntuning. In this way, pre-training and prompt tuning share the same “token prediction\" objective.\n8.2 Model Size\nBesides improving training efficiency, efficient LMs focus on the design of models of smaller sizes. Many methods are\ninvestigated to reduce the model size so that the model can be implemented on mobile or edge devices with limited\n16\nOverview of Language Models\ncomputing resources. Model compression is a widely studied topic. Compression methods first train a large LM and then\ncompress it into a target size. Examples include model pruning [177, 178, 179], knowledge distillation [180, 45, 181],\nlow rank matrix approximation [81, 182, 183], and parameter sharing [184, 91, 185, 186].\n8.3 Inference latency\nInference efficiency is important to an LM, particularly in real-time applications. A model of a smaller size generally\nhas faster inference speed under the same setting. Knowledge distillation, pruning, and low rank matrix approximation\ncan be employed to achieve faster inference time while reducing the model size. For instance, DistilBERT [45], which\nis a distilled version of BERT, has demonstrated a 60% improvement in the inference speed compared to the original\nmodel. More than 2x speed-up in inference is achieved in [177] by pruning PLMs.\nFast inference speed can also be achieved by fast decoding methods. Non-autoregressive generation (NAG) models\n[187, 188, 189] predict each token simultaneously. They have a faster inference speed than autoregressive models due\nto parallel computation. On the other hand, the performance of NAG models is generally worse than autoregressive\nmodels since they do not consider the forward or backward dependency between tokens in the output text.\n9 Future Research Directions\nIn this section, we describe several promising future research directions in language modeling.\n9.1 Integration of LMs and KGs\nKnowledge Graph (KG) provides a structured representation of human knowledge [ 190, 191]. It has been widely\nused in many NLP applications, such as question answering [ 192] and text summarization [ 193], because of its\ncapability to represent relationships between entities. There is a growing interest in evaluating the knowledge learned in\nPLMs [ 194, 195], where the relationship between different semantic units is captured in the embedding space and the\nself-attention layers. Several ideas are proposed in [196, 197, 198, 199, 200, 201, 202] to leverage KGs for LM training.\nAs a result, the knowledge learned in the models can be greatly improved. Thus, it is worth careful investigation of\nintegrating KGs with LMs and understanding how they interact with each other.\nIt appears that KG can serve as an information database to be queried by LMs. LMs are powerful in natural language\nunderstanding and generation while KGs can organize and store the knowledge information extracted from the training\ncorpus. In other words, we may decompose knowledge sources into semantic and syntactic two components, which can\nbe handled by KGs and LMs, respectively.\nSpecifically, most reasoning is handled by KGs so that predictions are fact-based and explainable. On the other hand,\nLM serves as an interface to understand and interpret the language input and improve fluency, comprehensiveness,\nconciseness, etc., of the language output. Similar concepts were proposed in [203, 204]. In the training phase, a KG is\nconstructed based on the information extracted from the training corpus, and an LM can be trained simultaneously. In\nthe inference phase, an LM can serve as an interface between humans and the knowledge database represented in the\nform of KGs. There are advantages to assigning semantic and syntactic processing tasks to KGs and LMs, respectively.\nFor example, the decoupling facilitates incremental learning, allows a smaller model size, and improves interpretability.\nThey will be further elaborated on below.\n9.2 Incremental Learning\nIncremental learning aims to incorporate new information without re-training existing models entirely. The problem of\ncatastrophic forgetting associated with neural network models was pointed out in [205]. That is, the information that\nhas already been learned by a model can be gradually forgotten when training with new information. This problem\nis particularly critical to large LMs since new information keeps arriving. A solution to catastrophic forgetting was\nproposed in [206]. It attempts to remember prior important tasks by slowing down learning on weights that are more\nrelevant to them. However, it is difficult to define important tasks in LMs. In addition, re-training a large LM with both\nold and new data is too expensive. Lifelong learning of LMs [207, 208, 209] is another solution to accommodate new\ndata to update the knowledge in LMs. It worths further exploration.\nThe importance of developing a satisfactory solution to incremental learning for LMs cannot be over-emphasized.\nIncremental learning is challenging for neural networks. Yet, it is easy for KGs to add new data to (or remove old data\nfrom) an existing database by adding or removing factual triples [210]. Clearly, the current information in the KGs will\n17\nOverview of Language Models\nnot be overwritten by newly collected data. The information in the database is updated incrementally. To this end, the\nintegration of KGs and LMs provides an excellent solution that meets the need for incremental learning.\n9.3 Lightweight Models\nAs mentioned in Section 8, PLMs get more powerful at the expense of huge computational resources and energy\nconsumption. The cost issue has to be faced seriously in the development of large LMs (LLMs). Besides, LLMs are\nunfriendly to our environment due to their high carbon footprint. Green Learning (GL) targets learning solutions with\nlow carbon footprint. The design of lightweight models of smaller sizes and lower computational complexity without\nsacrificing performance has received more attention in recent years [211, 212, 213, 214]. The design of green LMs is\nan important topic worth serious investigation.\nCurrent PLMs are data-driven models that use neural architectures to learn generic language knowledge from a large\namount of data. Efforts have been made in the development of lightweight LMs. Model compression is one of the\npopular approaches to obtaining a small LM. Examples include knowledge distillation or pruning [215]. However, this\nmethodology appears to be a detour since it trains large models and then shrinks their sizes by compression. Instead, we\nmay incorporate the linguistic information and the domain knowledge to offer a more direct way to reduce the model\nsize and the amount of training data.\n9.4 Universal versus Domain-Specific Models\nA universal LM is developed to handle tasks in the general domain. For example, ChatGPT is a universal dialogue\nLM pre-trained on multilingual and general domain corpora. It can converse on open-domain topics in multiple\nlanguages. In contrast, domain-specific LMs [ 216, 217, 218, 219] are designed to deal with domain-specific tasks, e.g.,\nbiomedicine, economics, musicology, etc.\nA universal LM demands a huge model size, a large number of training examples, and a tremendous amount of\ncomputational resources. Based on the scaling law of neural language models [220], the inference performance scales\nas a power-law with the model size, the dataset size, and the amount of computing used for training. So far, the largest\nPLM contains 540-billion parameters [86]. Despite the superior performance and the flexibility to adapt to multiple\ntasks, we may wonder whether a huge universal LM is cost-effective.\nFor domain-specific LMs, the amount of training data in need is significantly lower. It was believed that the general\ndomain PLMs benefit the training of domain-specific LMs. However, it is reported in [217] that domain-specific LMs,\nwhich were pre-trained from scratch on in-domain data, can provide a solid foundation for biomedical NLP. In other\nwords, training a domain-specific LM may not need a huge amount of general corpora and labeled data. Domain-specific\nLMs to be deployed on task-specific scenarios with less training and inference efforts expect to receive more attention\nin the future.\n9.5 Interpretable Models\nAlthough deep-learning-based LMs are dominating the NLP field, they are inherently black-box methods without\nmathematical transparency. Its interpretability is of concern. Efforts have been made to explain the black-box LMs. As\nmentioned in 6.3, empirical studies are conducted to understand what PLMs have learned through experimental design.\nHowever, the progress in this direction may offer insights but not a satisfactory and clean answer. Providing theoretical\nexplanations or establishing explainable LMs is still a challenging and open issue. A direction to interpretability is to\ndesign an interpretable learning model from scratch. For example, we may incorporate KGs with LMs. KG is known to\nbe capable of improving the interpretability and transparency of the system in many reasoning tasks such as information\nretrieval [221] and recommendation systems [222]. For example, reasoning paths and data sources can be provided\nwith predictions when KGs are incorporated for reasoning. It is challenging for LMs to do so. It is critical to develop an\ninterpretable LM to avoid its hallucination in natural language generation [223].\n9.6 Machine Generated Text Detection\nThe most common application of LMs is text generation. As generative LM’s performance gets closer to or even\noutperforms humans, these LMs can be used for malicious purposes such as academic dishonesty, spamming, targeted\nbot attacks, and fake news/reviews generation. How to determine whether a text is generated by LMs or written by\nhumans is a big challenge nowadays. A high-performance machine-generated text classifier can only serve as a reference\nin real-world applications, since it has false positives (i.e., human-written texts classified as machine-generated) and\nfalse negatives (i.e., machine-generated texts classified as human-written). In addition, people may be even more\n18\nOverview of Language Models\ninterested in detecting veracious and unveracious texts. They care more about whether the text is true or not. Detecting\ndisinformation could be more difficult than detecting machine/human-generated text without assessing the factuality.\nAdditionally, the factuality may change as time goes by. It is critical to our society in developing effective tools to\nidentify malicious usages of generative LMs.\n10 Conclusion\nA comprehensive overview of CLMs and their successors, PLMs, was presented in this paper and a wide range of topics\nwas covered. First, different levels of linguistic units were introduced and how linguistic unit prediction is used to train\nlanguage models was examined. Second, tokenization methods adopted by language models were discussed. Third,\nlanguage model architectures and the training paradigm of PLMs were reviewed. Fourth, we studied the evaluation and\napplications of language models. Especially, several applications in the context of text generation were detailed. Finally,\nseveral future research directions were pointed out. The need for explainable, reliable, domain-specific, and lightweight\nlanguage models was emphasized.\nReferences\n[1] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by\ngenerative pre-training. 2018.\n[2] Peter F Brown, Vincent J Della Pietra, Peter V Desouza, Jennifer C Lai, and Robert L Mercer. Class-based\nn-gram models of natural language. Computational linguistics, 18(4):467–480, 1992.\n[3] Marcello Federico. Bayesian estimation methods for n-gram language model adaptation. In Proceeding of Fourth\nInternational Conference on Spoken Language Processing. ICSLP’96, volume 1, pages 240–243. IEEE, 1996.\n[4] Thomas R Niesler and Philip C Woodland. A variable-length category-based n-gram language model. In 1996\nIEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings, volume 1,\npages 164–167. IEEE, 1996.\n[5] Stephen A Della Pietra, Vincent J Della Pietra, Robert L Mercer, and Salim Roukos. Adaptive language modeling\nusing minimum discriminant estimation. In Speech and Natural Language: Proceedings of a Workshop Held at\nHarriman, New York, February 23-26, 1992, 1992.\n[6] Adam Berger, Stephen A Della Pietra, and Vincent J Della Pietra. A maximum entropy approach to natural\nlanguage processing. Computational linguistics, 22(1):39–71, 1996.\n[7] Roni Rosenfeld. A maximum entropy approach to adaptive statistical language modeling. 1996.\n[8] Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. A neural probabilistic language model. Advances in\nneural information processing systems, 13, 2000.\n[9] Tomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. Recurrent neural network\nbased language model. In Interspeech, volume 2, pages 1045–1048. Makuhari, 2010.\n[10] Frederick Jelinek, Lalit Bahl, and Robert Mercer. Design of a linguistic statistical decoder for the recognition of\ncontinuous speech. IEEE Transactions on Information Theory, 21(3):250–256, 1975.\n[11] Frederick Jelinek. Continuous speech recognition by statistical methods.Proceedings of the IEEE, 64(4):532–556,\n1976.\n[12] Lalit R Bahl, Frederick Jelinek, and Robert L Mercer. A maximum likelihood approach to continuous speech\nrecognition. IEEE transactions on pattern analysis and machine intelligence, (2):179–190, 1983.\n[13] Peter F Brown, John Cocke, Stephen A Della Pietra, Vincent J Della Pietra, Frederick Jelinek, John Lafferty,\nRobert L Mercer, and Paul S Roossin. A statistical approach to machine translation. Computational linguistics,\n16(2):79–85, 1990.\n[14] Franz Josef Och, Nicola Ueffing, and Hermann Ney. An efficient a* search algorithm for statistical machine\ntranslation. In Proceedings of the ACL 2001 Workshop on Data-Driven Methods in Machine Translation, 2001.\n[15] Kenji Yamada and Kevin Knight. A decoder for syntax-based statistical mt. In Proceedings of the 40th Annual\nmeeting of the Association for Computational Linguistics, pages 303–310, 2002.\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n19\nOverview of Language Models\n[17] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\n[18] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 2227–2237, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.\n[19] Chengwei Wei, Bin Wang, and C-C Jay Kuo. Synwmd: Syntax-aware word mover’s distance for sentence\nsimilarity evaluation. Pattern Recognition Letters, 170:48–55, 2023.\n[20] Bin Wang and C.-C. Jay Kuo. SBERT-WK: A sentence embedding method by dissecting bert-based word models.\nIEEE/ACM Transactions on Audio, Speech, and Language Processing, 28:2146–2157, 2020.\n[21] Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings. In\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894–6910,\n2021.\n[22] Bin Wang and Haizhou Li. Relational sentence embedding for flexible semantic matching. arXiv preprint\narXiv:2212.08802, 2022.\n[23] Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang. Pre-trained models for\nnatural language processing: A survey. Science China Technological Sciences, 63(10):1872–1897, 2020.\n[24] Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Yuan Yao, Ao Zhang,\nLiang Zhang, et al. Pre-trained models: Past, present and future. AI Open, 2:225–250, 2021.\n[25] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt,\nand predict: A systematic survey of prompting methods in natural language processing.ACM Computing Surveys,\n55(9):1–35, 2023.\n[26] Ciprian Chelba and Frederick Jelinek. Exploiting syntactic structure for language modeling. In Proceedings of\nthe 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on\nComputational Linguistics-Volume 1, pages 225–231, 1998.\n[27] Ciprian Chelba and Frederick Jelinek. Structured language modeling. Computer Speech & Language, 14(4):283–\n332, 2000.\n[28] Joseph Gubbins and Andreas Vlachos. Dependency language models for sentence completion. In Proceedings of\nthe 2013 Conference on Empirical Methods in Natural Language Processing, pages 1405–1410, 2013.\n[29] Piotr Mirowski and Andreas Vlachos. Dependency recurrent neural language models for sentence completion.\nIn Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language Processing (Volume 2: Short Papers) , pages 511–517,\n2015.\n[30] Chengwei Wei, Bin Wang, and C.-C. Jay Kuo. Task-specific dependency-based word embedding methods.\nPattern Recognition Letters, 2022.\n[31] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. Xlnet:\nGeneralized autoregressive pretraining for language understanding. Advances in neural information processing\nsystems, 32, 2019.\n[32] Ilya Sutskever, James Martens, and Geoffrey E Hinton. Generating text with recurrent neural networks. In ICML,\n2011.\n[33] Kyuyeon Hwang and Wonyong Sung. Character-level language modeling with hierarchical recurrent neural\nnetworks. In 2017 ieee international conference on acoustics, speech and signal processing (icassp) , pages\n5720–5724. IEEE, 2017.\n[34] Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. Character-aware neural language models. In\nThirtieth AAAI conference on artificial intelligence, 2016.\n[35] Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level language modeling\nwith deeper self-attention. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages\n3159–3166, 2019.\n[36] Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and\nColin Raffel. Byt5: Towards a token-free future with pre-trained byte-to-byte models. Transactions of the\nAssociation for Computational Linguistics, 10:291–306, 2022.\n20\nOverview of Language Models\n[37] Moonyoung Kang, Tim Ng, and Long Nguyen. Mandarin word-character hybrid-input neural network language\nmodel. In Twelfth Annual Conference of the International Speech Communication Association, 2011.\n[38] Yasumasa Miyamoto and Kyunghyun Cho. Gated word-character recurrent language model. In 2016 Conference\non Empirical Methods in Natural Language Processing, EMNLP 2016 , pages 1992–1997. Association for\nComputational Linguistics (ACL), 2016.\n[39] Lyan Verwimp, Joris Pelemans, Patrick Wambacq, et al. Character-word lstm language models. In Proceedings\nof the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1,\nLong Papers, pages 417–427, 2017.\n[40] Tomáš Mikolov, Ilya Sutskever, Anoop Deoras, Hai-Son Le, Stefan Kombrink, and Jan Cernocky. Subword\nlanguage modeling with neural networks. preprint (http://www. fit. vutbr. cz/imikolov/rnnlm/char. pdf), 8(67),\n2012.\n[41] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword\nunits. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), pages 1715–1725, 2016.\n[42] Philip Gage. A new algorithm for data compression. C Users Journal, 12(2):23–38, 1994.\n[43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[44] Mike Schuster and Kaisuke Nakajima. Japanese and korean voice search. In 2012 IEEE international conference\non acoustics, speech and signal processing (ICASSP), pages 5149–5152. IEEE, 2012.\n[45] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller,\nfaster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\n[46] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. Electra: Pre-training text encoders\nas discriminators rather than generators. arXiv preprint arXiv:2003.10555, 2020.\n[47] Taku Kudo. Subword regularization: Improving neural network translation models with multiple subword\ncandidates. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pages 66–75, 2018.\n[48] Kaj Bostrom and Greg Durrett. Byte pair encoding is suboptimal for language model pretraining. In Findings of\nthe Association for Computational Linguistics: EMNLP 2020, pages 4617–4624, 2020.\n[49] Daniel Kiecza, Tanja Schultz, and Alex Waibel. Data-driven determination of appropriate dictionary units for\nkorean lvcsr. In Proceedings of ICASSP, pages 323–327, 1999.\n[50] Mathias Creutz and Krista Lagus. Unsupervised morpheme segmentation and morphology induction from text\ncorpora using Morfessor 1.0. Helsinki University of Technology Helsinki, 2005.\n[51] Mathias Creutz, Teemu Hirsimäki, Mikko Kurimo, Antti Puurula, Janne Pylkkönen, Vesa Siivola, Matti Var-\njokallio, Ebru Arisoy, Murat Saraçlar, and Andreas Stolcke. Morph-based speech recognition and modeling of\nout-of-vocabulary words across languages. ACM Transactions on Speech and Language Processing (TSLP),\n5(1):1–29, 2007.\n[52] Tomaž Rotovnik, Mirjam Sepesy Mauˇcec, and Zdravko Kaˇciˇc. Large vocabulary continuous speech recognition\nof an inflected language using stems and endings. Speech communication, 49(6):437–452, 2007.\n[53] Ruhi Sarikaya, Mohamed Afify, and Yuqing Gao. Joint morphological-lexical language modeling (jmllm)\nfor arabic. In 2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP’07,\nvolume 4, pages IV–181. IEEE, 2007.\n[54] Ha¸ sim Sak, Murat Saraclar, and Tunga Güngör. Morphology-based and sub-word language modeling for turkish\nspeech recognition. In 2010 IEEE International Conference on Acoustics, Speech and Signal Processing, pages\n5402–5405. IEEE, 2010.\n[55] Mijit Ablimit, Graham Neubig, Masato Mimura, Shinsuke Mori, Tatsuya Kawahara, and Askar Hamdulla.\nUyghur morpheme-based language models and asr. In IEEE 10th INTERNATIONAL CONFERENCE ON\nSIGNAL PROCESSING PROCEEDINGS, pages 581–584. IEEE, 2010.\n[56] Bernhard Suhm and Alex Waibel. Towards better language models for spontaneous speech. 1994.\n[57] Klaus Ries, Finn Dag Buo, and Alex Waibel. Class phrase models for language modeling. In Proceeding of\nFourth International Conference on Spoken Language Processing. ICSLP’96, volume 1, pages 398–401. IEEE,\n1996.\n21\nOverview of Language Models\n[58] George Saon and Mukund Padmanabhan. Data-driven approach to designing compound words for continuous\nspeech recognition. IEEE transactions on Speech and audio processing, 9(4):327–332, 2001.\n[59] Michael Levit, Sarangarajan Parthasarathy, Shuangyu Chang, Andreas Stolcke, and Benoit Dumoulin. Word-\nphrase-entity language models: Getting more mileage out of n-grams. In Fifteenth Annual Conference of the\nInternational Speech Communication Association, 2014.\n[60] Ronald Rosenfeld. A whole sentence maximum entropy language model. In 1997 IEEE workshop on automatic\nspeech recognition and understanding proceedings, pages 230–237. IEEE, 1997.\n[61] Stanley F Chen and Ronald Rosenfeld. Efficient sampling and feature selection in whole sentence maximum\nentropy language models. In 1999 IEEE International Conference on Acoustics, Speech, and Signal Processing.\nProceedings. ICASSP99 (Cat. No. 99CH36258), volume 1, pages 549–552. IEEE, 1999.\n[62] Ronald Rosenfeld, Stanley F Chen, and Xiaojin Zhu. Whole-sentence exponential language models: a vehicle\nfor linguistic-statistical integration. Computer Speech & Language, 15(1):55–73, 2001.\n[63] Daphne Ippolito, David Grangier, Douglas Eck, and Chris Callison-Burch. Toward better storylines with\nsentence-level language models. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pages 7472–7478, 2020.\n[64] Haejun Lee, Drew A Hudson, Kangwook Lee, and Christopher D Manning. Slm: Learning a discourse language\nrepresentation with sentence unshuffling. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 1551–1562, 2020.\n[65] Mohamed Afify, Olivier Siohan, and Ruhi Sarikaya. Gaussian mixture language models for speech recognition.\nIn 2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP’07 , volume 4,\npages IV–29. IEEE, 2007.\n[66] Thomas Kuhn, Heinrich Niemann, and Ernst Günter Schukat-Talamazzini. Ergodic hidden markov models and\npolygrams for language modeling. In Proceedings of ICASSP’94. IEEE International Conference on Acoustics,\nSpeech and Signal Processing, volume 1, pages I–357. IEEE, 1994.\n[67] George James Lidstone. Note on the general case of the bayes-laplace formula for inductive or a posteriori\nprobabilities. Transactions of the Faculty of Actuaries, 8(182-192):13, 1920.\n[68] William Ernest Johnson. Probability: The deductive and inductive problems. Mind, 41(164):409–423, 1932.\n[69] Frederick Jelinek. Interpolated estimation of markov source parameters from sparse data. In Proc. Workshop on\nPattern Recognition in Practice, 1980, 1980.\n[70] Slava Katz. Estimation of probabilities from sparse data for the language model component of a speech recognizer.\nIEEE transactions on acoustics, speech, and signal processing, 35(3):400–401, 1987.\n[71] Kenneth W Church and William A Gale. A comparison of the enhanced good-turing and deleted estimation\nmethods for estimating probabilities of english bigrams. Computer Speech & Language, 5(1):19–54, 1991.\n[72] Reinhard Kneser and Hermann Ney. Improved backing-off for m-gram language modeling. In1995 international\nconference on acoustics, speech, and signal processing, volume 1, pages 181–184. IEEE, 1995.\n[73] Stanley F Chen and Joshua Goodman. An empirical study of smoothing techniques for language modeling.\nComputer Speech & Language, 13(4):359–394, 1999.\n[74] John N Darroch and Douglas Ratcliff. Generalized iterative scaling for log-linear models. The annals of\nmathematical statistics, pages 1470–1480, 1972.\n[75] Holger Schwenk and Jean-Luc Gauvain. Training neural network language models on very large corpora. In\nProceedings of human language technology conference and conference on empirical methods in natural language\nprocessing, pages 201–208, 2005.\n[76] Holger Schwenk. Continuous space language models. Computer Speech & Language, 21(3):492–518, 2007.\n[77] Ebru Arisoy, Tara N Sainath, Brian Kingsbury, and Bhuvana Ramabhadran. Deep neural network language\nmodels. In Proceedings of the NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model?\nOn the Future of Language Modeling for HLT, pages 20–28, 2012.\n[78] Tomáš Mikolov, Stefan Kombrink, Lukáš Burget, JanˇCernock`y, and Sanjeev Khudanpur. Extensions of recurrent\nneural network language model. In 2011 IEEE international conference on acoustics, speech and signal\nprocessing (ICASSP), pages 5528–5531. IEEE, 2011.\n[79] Stefan Kombrink, Tomas Mikolov, Martin Karafiát, and Lukás Burget. Recurrent neural network based language\nmodeling in meeting recognition. In Interspeech, volume 11, pages 2877–2880, 2011.\n22\nOverview of Language Models\n[80] Martin Sundermeyer, Ralf Schlüter, and Hermann Ney. Lstm neural networks for language modeling. In\nThirteenth annual conference of the international speech communication association, 2012.\n[81] Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. Breaking the softmax bottleneck: A\nhigh-rank rnn language model. arXiv preprint arXiv:1711.03953, 2017.\n[82] Sepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and problem solutions.\nInternational Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(02):107–116, 1998.\n[83] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align\nand translate. arXiv preprint arXiv:1409.0473, 2014.\n[84] Hongli Deng, Lei Zhang, and Lituan Wang. Global context-dependent recurrent neural network language model\nwith sparse feature learning. Neural Computing and Applications, 31(2):999–1011, 2019.\n[85] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n[86] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with\npathways. arXiv preprint arXiv:2204.02311, 2022.\n[87] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877–1901, 2020.\n[88] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-xl:\nAttentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages 2978–2988, 2019.\n[89] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin\nStoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 7871–7880, 2020.\n[90] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, Peter J Liu, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach.\nLearn. Res., 21(140):1–67, 2020.\n[91] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert:\nA lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.\n[92] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo,\nMona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International Conference on\nMachine Learning, pages 2790–2799. PMLR, 2019.\n[93] Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion:\nNon-destructive task composition for transfer learning. In Proceedings of the 16th Conference of the European\nChapter of the Association for Computational Linguistics: Main Volume, pages 487–503, 2021.\n[94] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora:\nLow-rank adaptation of large language models. In International Conference on Learning Representations.\n[95] Timo Schick and Hinrich Schütze. Exploiting cloze-questions for few-shot text classification and natural language\ninference. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational\nLinguistics: Main Volume, pages 255–269, 2021.\n[96] Timo Schick and Hinrich Schütze. Few-shot text generation with natural language instructions. In Proceedings\nof the 2021 Conference on Empirical Methods in Natural Language Processing, pages 390–402, 2021.\n[97] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners.\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3816–3830,\n2021.\n[98] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba.\nLarge language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022.\n[99] Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization\nwith\" gradient descent\" and beam search. arXiv preprint arXiv:2305.03495, 2023.\n23\nOverview of Language Models\n[100] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings\nof the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Volume 1: Long Papers), pages 4582–4597, 2021.\n[101] Zexuan Zhong, Dan Friedman, and Danqi Chen. Factual probing is [mask]: Learning vs. learning to recall.\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages 5017–5033, 2021.\n[102] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In\nProceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3045–3059,\n2021.\n[103] Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. In\nProceedings of the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages 5203–5212, 2021.\n[104] Alex Wang and Kyunghyun Cho. Bert has a mouth, and it must speak: Bert as a markov random field language\nmodel. In Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation,\npages 30–36, 2019.\n[105] Julian Salazar, Davis Liang, Toan Q Nguyen, and Katrin Kirchhoff. Masked language model scoring. In\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2699–2712,\n2020.\n[106] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Glue: A multi-task\nbenchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP\nWorkshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, 2018.\n[107] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems.\nAdvances in neural information processing systems, 32, 2019.\n[108] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin\nVan Durme, Samuel R Bowman, Dipanjan Das, et al. What do you learn from context? probing for sentence\nstructure in contextualized word representations. In 7th International Conference on Learning Representations,\nICLR 2019, 2019.\n[109] Mario Giulianelli, Jack Harding, Florian Mohnert, Dieuwke Hupkes, and Willem Zuidema. Under the hood:\nUsing diagnostic classifiers to investigate and improve how language models track agreement information. In\nProceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,\npages 240–248, 2018.\n[110] Ian Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical nlp pipeline. In Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics, pages 4593–4601, 2019.\n[111] Taeuk Kim, Jihun Choi, Daniel Edmiston, and Sang-goo Lee. Are pre-trained language models aware of phrases?\nsimple but strong baselines for grammar induction. arXiv preprint arXiv:2002.00737, 2020.\n[112] John Hewitt and Christopher D Manning. A structural probe for finding syntax in word representations. In\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4129–4138, 2019.\n[113] Anna Rogers, Olga Kovaleva, and Anna Rumshisky. A primer in bertology: What we know about how bert\nworks. Transactions of the Association for Computational Linguistics, 8:842–866, 2020.\n[114] Nikunj Saunshi, Sadhika Malladi, and Sanjeev Arora. A mathematical exploration of why language models help\nsolve downstream tasks. arXiv preprint arXiv:2010.03648, 2020.\n[115] Colin Wei, Sang Michael Xie, and Tengyu Ma. Why do pretrained language models help in downstream tasks?\nan analysis of head and prompt tuning. Advances in Neural Information Processing Systems, 34:16158–16170,\n2021.\n[116] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning\nin nlp. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages\n3645–3650, 2019.\n[117] Neil C Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel F Manso. The computational limits of deep\nlearning. arXiv preprint arXiv:2007.05558, 2020.\n[118] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of\nstochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference on fairness,\naccountability, and transparency, pages 610–623, 2021.\n24\nOverview of Language Models\n[119] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang,\nFiona Aga, Jinshi Huang, Charles Bai, et al. Sustainable ai: Environmental implications, challenges and\nopportunities. Proceedings of Machine Learning and Systems, 4:795–813, 2022.\n[120] Chandler May, Alex Wang, Shikha Bordia, Samuel R Bowman, and Rachel Rudinger. On measuring social\nbiases in sentence encoders. In Proceedings of NAACL-HLT, pages 622–628, 2019.\n[121] Su Lin Blodgett, Solon Barocas, Hal Daumé III, and Hanna Wallach. Language (technology) is power: A\ncritical survey of “bias” in nlp. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pages 5454–5476, 2020.\n[122] Moin Nadeem, Anna Bethke, and Siva Reddy. Stereoset: Measuring stereotypical bias in pretrained language\nmodels. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5356–5371,\n2021.\n[123] Abubakar Abid, Maheen Farooqi, and James Zou. Persistent anti-muslim bias in large language models. In\nProceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pages 298–306, 2021.\n[124] Barbara Rychalska, Dominika Basaj, Alicja Gosiewska, and Przemysław Biecek. Models in the wild: On\ncorruption robustness of neural nlp systems. In Neural Information Processing: 26th International Conference,\nICONIP 2019, Sydney, NSW, Australia, December 12–15, 2019, Proceedings, Part III 26 , pages 235–247.\nSpringer, 2019.\n[125] Marwan Omar, Soohyeon Choi, DaeHun Nyang, and David Mohaisen. Robust natural language processing:\nRecent advances, challenges, and future directions. IEEE Access, 2022.\n[126] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? a strong baseline for natural\nlanguage attack on text classification and entailment. In Proceedings of the AAAI conference on artificial\nintelligence, volume 34, pages 8018–8025, 2020.\n[127] Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, and Maosong Sun. Word-level\ntextual adversarial attacking as combinatorial optimization. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages 6066–6080, 2020.\n[128] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new\nbenchmark for natural language understanding. In Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 4885–4901, 2020.\n[129] Karan Goel, Nazneen Fatema Rajani, Jesse Vig, Zachary Taschdjian, Mohit Bansal, and Christopher Ré.\nRobustness gym: Unifying the nlp evaluation landscape. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language Technologies:\nDemonstrations, pages 42–55, 2021.\n[130] Xiao Wang, Qin Liu, Tao Gui, Qi Zhang, Yicheng Zou, Xin Zhou, Jiacheng Ye, Yongxin Zhang, Rui Zheng,\nZexiong Pang, et al. Textflint: Unified multilingual robustness evaluation toolkit for natural language processing.\nIn Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing: System Demonstrations, pages 347–355, 2021.\n[131] Julia El Zini and Mariette Awad. On the explainability of natural language processing deep models. ACM\nComputing Surveys, 55(5):1–31, 2022.\n[132] Marco Tulio Ribeiro, Carlos Guestrin, and Sameer Singh. Are red roses red? evaluating consistency of question-\nanswering models. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,\npages 6174–6184, 2019.\n[133] Yun-Cheng Wang, Jintang Xue, Chengwei Wei, and C-C Jay Kuo. An overview on generative ai at scale with\nedge-cloud computing. 2023.\n[134] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration.\narXiv preprint arXiv:1904.09751, 2019.\n[135] Tiancheng Zhao, Ran Zhao, and Maxine Eskenazi. Learning discourse-level diversity for neural dialog models\nusing conditional variational autoencoders. In Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 654–664, 2017.\n[136] Zhen Xu, Bingquan Liu, Baoxun Wang, Cheng-Jie Sun, Xiaolong Wang, Zhuoran Wang, and Chao Qi. Neural\nresponse generation via gan with an approximate embedding layer. In Proceedings of the 2017 conference on\nempirical methods in natural language processing, pages 617–626, 2017.\n25\nOverview of Language Models\n[137] Jiwei Li, Will Monroe, and Dan Jurafsky. A simple, fast diverse decoding algorithm for neural generation. arXiv\npreprint arXiv:1611.08562, 2016.\n[138] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David Crandall, and\nDhruv Batra. Diverse beam search for improved description of complex scenes. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 32, 2018.\n[139] Ilia Kulikov, Alexander Miller, Kyunghyun Cho, and Jason Weston. Importance of search and evaluation\nstrategies in neural dialogue modeling. In Proceedings of the 12th International Conference on Natural\nLanguage Generation, pages 76–87, 2019.\n[140] Jiatao Gu, Kyunghyun Cho, and Victor OK Li. Trainable greedy decoding for neural machine translation. In\nProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1968–1978,\n2017.\n[141] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 889–898,\n2018.\n[142] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia\nJin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint\narXiv:2201.08239, 2022.\n[143] Jinjie Ni, Tom Young, Vlad Pandelea, Fuzhao Xue, and Erik Cambria. Recent advances in deep learning based\ndialogue systems: A systematic survey. Artificial intelligence review, pages 1–101, 2022.\n[144] Bin Wang, Chen Zhang, Chengwei Wei, and Haizhou Li. A focused study on sequence length for dialogue\nsummarization. arXiv preprint arXiv:2209.11910, 2022.\n[145] Bin Wang, Chen Zhang, Yan Zhang, Yiming Chen, and Haizhou Li. Analyzing and evaluating faithfulness in\ndialogue summarization. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing, pages 4897–4908, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational\nLinguistics.\n[146] Chien-Sheng Wu and Caiming Xiong. Probing task-oriented dialogue representation from language models. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages\n5036–5051, Online, November 2020. Association for Computational Linguistics.\n[147] Andrea Madotto, Zihan Liu, Zhaojiang Lin, and Pascale Fung. Language models as few-shot learner for\ntask-oriented dialogue systems. arXiv preprint arXiv:2008.06239, 2020.\n[148] Fred Jelinek, B Merialdo, S Roukos, M Strauss, et al. Self-organized language modeling for speech recognition.\nIn Readings in speech recognition. Citeseer, 1990.\n[149] Manhung Siu and Mari Ostendorf. Variable n-grams and extensions for conversational speech language modeling.\nIEEE Transactions on Speech and Audio Processing, 8(1):63–75, 2000.\n[150] Ebru Arısoy, Stanley F Chen, Bhuvana Ramabhadran, and Abhinav Sethy. Converting neural network language\nmodels into back-off language models for efficient decoding in automatic speech recognition. IEEE/ACM\nTransactions on Audio, Speech, and Language Processing, 22(1):184–192, 2013.\n[151] Ebru Arisoy, Abhinav Sethy, Bhuvana Ramabhadran, and Stanley Chen. Bidirectional recurrent neural network\nlanguage models for automatic speech recognition. In 2015 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages 5421–5425. IEEE, 2015.\n[152] Zhiheng Huang, Geoffrey Zweig, and Benoit Dumoulin. Cache based recurrent neural network language model\ninference for first pass speech recognition. In 2014 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pages 6354–6358. IEEE, 2014.\n[153] Joonbo Shin, Yoonhyung Lee, and Kyomin Jung. Effective sentence scoring method using bert for speech\nrecognition. In Asian Conference on Machine Learning, pages 1081–1093. PMLR, 2019.\n[154] William Chan, Navdeep Jaitly, Quoc Le, and Oriol Vinyals. Listen, attend and spell: A neural network for large\nvocabulary conversational speech recognition. In 2016 IEEE international conference on acoustics, speech and\nsignal processing (ICASSP), pages 4960–4964. IEEE, 2016.\n[155] Anjuli Kannan, Yonghui Wu, Patrick Nguyen, Tara N Sainath, Zhijeng Chen, and Rohit Prabhavalkar. An analysis\nof incorporating an external language model into a sequence-to-sequence model. In 2018 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), pages 1–5828. IEEE, 2018.\n26\nOverview of Language Models\n[156] Shubham Toshniwal, Anjuli Kannan, Chung-Cheng Chiu, Yonghui Wu, Tara N Sainath, and Karen Livescu. A\ncomparison of techniques for language model integration in encoder-decoder speech recognition. In 2018 IEEE\nspoken language technology workshop (SLT), pages 369–375. IEEE, 2018.\n[157] Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, Loic Barrault, Huei-Chi Lin, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. On using monolingual corpora in neural machine translation. arXiv preprint\narXiv:1503.03535, 2015.\n[158] Jan Chorowski and Navdeep Jaitly. Towards better decoding and language model integration in sequence to\nsequence models. Proc. Interspeech 2017, pages 523–527, 2017.\n[159] Erik McDermott, Hasim Sak, and Ehsan Variani. A density ratio approach to language model fusion in end-to-\nend automatic speech recognition. In 2019 IEEE Automatic Speech Recognition and Understanding Workshop\n(ASRU), pages 434–441. IEEE, 2019.\n[160] Anuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and Adam Coates. Cold fusion: Training seq2seq models\ntogether with language models. Proc. Interspeech 2018, pages 387–391, 2018.\n[161] Changhao Shan, Chao Weng, Guangsen Wang, Dan Su, Min Luo, Dong Yu, and Lei Xie. Component fusion:\nLearning replaceable language model component for end-to-end speech recognition system. In ICASSP 2019-\n2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5361–5635.\nIEEE, 2019.\n[162] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, and Lidia S Chao. Learning deep\ntransformer models for machine translation. In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 1810–1822, 2019.\n[163] Max Weiss. Deepfake bot submissions to federal public comment websites cannot be distinguished from human\nsubmissions. Technology Science, 2019.\n[164] Adaku Uchendu, Thai Le, Kai Shu, and Dongwon Lee. Authorship attribution for neural text generation. In\nConf. on Empirical Methods in Natural Language Processing (EMNLP), 2020.\n[165] Tiziano Fagni, Fabrizio Falchi, Margherita Gambini, Antonio Martella, and Maurizio Tesconi. Tweepfake: About\ndetecting deepfake tweets. Plos one, 16(5):e0251415, 2021.\n[166] James Thorne and Andreas Vlachos. Automated fact checking: Task formulations, methods and future directions.\nIn Proceedings of the 27th International Conference on Computational Linguistics, pages 3346–3359, 2018.\n[167] Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck. Automatic detection of generated\ntext is easiest when humans are fooled. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1808–1822, 2020.\n[168] Sebastian Gehrmann, Hendrik Strobelt, and Alexander M Rush. Gltr: Statistical detection and visualization of\ngenerated text. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics:\nSystem Demonstrations, pages 111–116, 2019.\n[169] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi.\nDefending against neural fake news. Advances in neural information processing systems, 32, 2019.\n[170] Ganesh Jawahar, Muhammad Abdul-Mageed, and VS Laks Lakshmanan. Automatic detection of machine\ngenerated text: A critical survey. In Proceedings of the 28th International Conference on Computational\nLinguistics, pages 2296–2309, 2020.\n[171] Harald Stiff and Fredrik Johansson. Detecting computer-generated disinformation. International Journal of Data\nScience and Analytics, 13(4):363–383, 2022.\n[172] Or Sharir, Barak Peleg, and Yoav Shoham. The cost of training nlp models: A concise overview. arXiv preprint\narXiv:2004.08900, 2020.\n[173] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti,\nDmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. Efficient large-scale language\nmodel training on gpu clusters using megatron-lm. In Proceedings of the International Conference for High\nPerformance Computing, Networking, Storage and Analysis, pages 1–15, 2021.\n[174] Yian Zhang, Alex Warstadt, Xiaocheng Li, and Samuel Bowman. When do you need billions of words of\npretraining data? In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages\n1112–1125, 2021.\n27\nOverview of Language Models\n[175] Kevin Clark, Minh-Thang Luong, Quoc Le, and Christopher D Manning. Pre-training transformers as energy-\nbased cloze models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 285–294, 2020.\n[176] Taylor Shin, Yasaman Razeghi, Robert L Logan IV , Eric Wallace, and Sameer Singh. Autoprompt: Eliciting\nknowledge from language models with automatically generated prompts. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Processing (EMNLP), pages 4222–4235, 2020.\n[177] Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning of large language models. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6151–6162, 2020.\n[178] Elena V oita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention:\nSpecialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages 5797–5808, 2019.\n[179] Demi Guo, Alexander M Rush, and Yoon Kim. Parameter-efficient transfer learning with diff pruning. In Pro-\nceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4884–4896, 2021.\n[180] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert:\nDistilling bert for natural language understanding. In Findings of the Association for Computational Linguistics:\nEMNLP 2020, pages 4163–4174, 2020.\n[181] Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better: On the\nimportance of pre-training compact models. arXiv preprint arXiv:1908.08962, 2019.\n[182] Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, and Dawei Song. A tensorized\ntransformer for language modeling. Advances in neural information processing systems, 32, 2019.\n[183] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\n[184] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers.\narXiv preprint arXiv:1807.03819, 2018.\n[185] Raj Dabre and Atsushi Fujita. Recurrent stacking of layers for compact neural machine translation models. In\nProceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6292–6299, 2019.\n[186] Sascha Rothe, Shashi Narayan, and Aliaksei Severyn. Leveraging pre-trained checkpoints for sequence generation\ntasks. Transactions of the Association for Computational Linguistics, 8:264–280, 2020.\n[187] J Gu, J Bradbury, C Xiong, VOK Li, and R Socher. Non-autoregressive neural machine translation. In\nInternational Conference on Learning Representations (ICLR), 2018.\n[188] Yixuan Su, Deng Cai, Yan Wang, David Vandyke, Simon Baker, Piji Li, and Nigel Collier. Non-autoregressive\ntext generation with pre-trained language models. In Proceedings of the 16th Conference of the European\nChapter of the Association for Computational Linguistics: Main Volume, pages 234–243, 2021.\n[189] Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. Elmer: A non-autoregressive pre-trained\nlanguage model for efficient and effective text generation. arXiv preprint arXiv:2210.13304, 2022.\n[190] Yun-Cheng Wang, Xiou Ge, Bin Wang, and C-C Jay Kuo. Kgboost: A classification-based knowledge base\ncompletion method with negative sampling. Pattern Recognition Letters, 157:104–111, 2022.\n[191] Xiou Ge, Yun-Cheng Wang, Bin Wang, and C-C Jay Kuo. Compounde: Knowledge graph embedding with\ntranslation, rotation and scaling compound operations. arXiv preprint arXiv:2207.05324, 2022.\n[192] Xiao Huang, Jingyuan Zhang, Dingcheng Li, and Ping Li. Knowledge graph embedding based question\nanswering. In Proceedings of the twelfth ACM international conference on web search and data mining, pages\n105–113, 2019.\n[193] Luyang Huang, Lingfei Wu, and Lu Wang. Knowledge graph-augmented abstractive summarization with\nsemantic-driven cloze reward. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, pages 5094–5107, 2020.\n[194] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander\nMiller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th International Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2463–2473, 2019.\n[195] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H. Miller, and\nSebastian Riedel. How context affects language models’ factual predictions. In Automated Knowledge Base\nConstruction, 2020.\n28\nOverview of Language Models\n[196] Oshin Agarwal, Heming Ge, Siamak Shakeri, and Rami Al-Rfou. Knowledge graph based synthetic corpus\ngeneration for knowledge-enhanced language model pre-training. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language Technologies,\npages 3554–3565, Online, June 2021. Association for Computational Linguistics.\n[197] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. ERNIE: Enhanced lan-\nguage representation with informative entities. In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 1441–1451, Florence, Italy, July 2019. Association for Computational\nLinguistics.\n[198] Lei He, Suncong Zheng, Tao Yang, and Feng Zhang. KLMo: Knowledge graph enhanced pretrained language\nmodel with fine-grained relationships. In Findings of the Association for Computational Linguistics: EMNLP\n2021, pages 4536–4542, Punta Cana, Dominican Republic, November 2021. Association for Computational\nLinguistics.\n[199] Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D Manning, Percy S Liang,\nand Jure Leskovec. Deep bidirectional language-knowledge graph pretraining. Advances in Neural Information\nProcessing Systems, 35:37309–37323, 2022.\n[200] Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang, Christopher D Manning, and\nJure Leskovec. GreaseLM: Graph REASoning enhanced language models. In International Conference on\nLearning Representations, 2022.\n[201] Wenhan Xiong, Jingfei Du, William Yang Wang, and Veselin Stoyanov. Pretrained encyclopedia: Weakly\nsupervised knowledge-pretrained language model. arXiv preprint arXiv:1912.09637, 2019.\n[202] Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. KEPLER:\nA unified model for knowledge embedding and pre-trained language representation. Transactions of the\nAssociation for Computational Linguistics, 9:176–194, 2021.\n[203] Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. QA-GNN: Reasoning\nwith language models and knowledge graphs for question answering. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language Technologies,\npages 535–546, Online, June 2021. Association for Computational Linguistics.\n[204] Ziniu Hu, Yichong Xu, Wenhao Yu, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Kai-Wei Chang, and Yizhou\nSun. Empowering language models with knowledge graph reasoning for open-domain question answering. In\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9562–9581,\n2022.\n[205] Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3(4):128–135,\n1999.\n[206] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran\nMilan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming catastrophic forgetting in\nneural networks. Proceedings of the national academy of sciences, 114(13):3521–3526, 2017.\n[207] Kelvin Luu, Daniel Khashabi, Suchin Gururangan, Karishma Mandyam, and Noah A. Smith. Time waits for no\none! analysis and challenges of temporal misalignment. In Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies , pages\n5944–5958, Seattle, United States, July 2022. Association for Computational Linguistics.\n[208] Angeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai\nGimenez, Cyprien de Masson d’Autume, Tomas Kocisky, Sebastian Ruder, et al. Mind the gap: Assessing\ntemporal generalization in neural language models. Advances in Neural Information Processing Systems ,\n34:29348–29363, 2021.\n[209] Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen Li, Xiaokai Wei, Andrew Arnold, and Xiang\nRen. Lifelong pretraining: Continually adapting language models to emerging corpora. In Proceedings of\nBigScience Episode #5 – Workshop on Challenges & Perspectives in Creating Large Language Models, pages\n1–16, virtual+Dublin, May 2022. Association for Computational Linguistics.\n[210] Bin Wang, Guangtao Wang, Jing Huang, Jiaxuan You, Jure Leskovec, and C-C Jay Kuo. Inductive learning\non commonsense knowledge graph completion. In 2021 International Joint Conference on Neural Networks\n(IJCNN), pages 1–8. IEEE, 2021.\n[211] Roy Schwartz, Jesse Dodge, Noah A Smith, and Oren Etzioni. Green ai. Communications of the ACM ,\n63(12):54–63, 2020.\n29\nOverview of Language Models\n[212] Jingjing Xu, Wangchunshu Zhou, Zhiyi Fu, Hao Zhou, and Lei Li. A survey on green deep learning. arXiv\npreprint arXiv:2111.05193, 2021.\n[213] C.-C. Jay Kuo and Azad M Madni. Green learning: Introduction, examples and outlook. Journal of Visual\nCommunication and Image Representation, page 103685, 2022.\n[214] Yun-Cheng Wang, Xiou Ge, Bin Wang, and C-C Jay Kuo. GreenKGC: A lightweight knowledge graph\ncompletion method. arXiv preprint arXiv:2208.09137, 2022.\n[215] Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joey Gonzalez. Train big,\nthen compress: Rethinking model size for efficient training and inference of transformers. In International\nConference on Machine Learning, pages 5958–5968. PMLR, 2020.\n[216] Denghui Zhang, Zixuan Yuan, Yanchi Liu, Fuzhen Zhuang, Haifeng Chen, and Hui Xiong. E-bert: A phrase and\nproduct knowledge enhanced language model for e-commerce. arXiv preprint arXiv:2009.02835, 2020.\n[217] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. Domain-specific language model pretraining for biomedical natural language processing.\nACM Transactions on Computing for Healthcare (HEALTH), 3(1):1–23, 2021.\n[218] Kesong Liu, Jianhui Jiang, and Feifei Lyu. A domain knowledge enhanced pre-trained language model for\nvertical search: Case study on medicinal products. In Proceedings of the 29th International Conference on\nComputational Linguistics, pages 1014–1023, 2022.\n[219] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. Biogpt: generative\npre-trained transformer for biomedical text generation and mining. Briefings in Bioinformatics, 23(6), 2022.\n[220] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint\narXiv:2001.08361, 2020.\n[221] Laura Dietz, Chenyan Xiong, Jeff Dalton, and Edgar Meij. The second workshop on knowledge graphs and\nsemantics for text retrieval, analysis, and understanding (kg4ir). InThe 41st International ACM SIGIR Conference\non Research & Development in Information Retrieval, pages 1423–1426, 2018.\n[222] Yuhao Yang, Chao Huang, Lianghao Xia, and Chenliang Li. Knowledge graph contrastive learning for recom-\nmendation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in\nInformation Retrieval, pages 1434–1443, 2022.\n[223] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto,\nand Pascale Fung. Survey of hallucination in natural language generation.ACM Computing Surveys, 55(12):1–38,\n2023.\n30",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7465888857841492
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6309294700622559
    },
    {
      "name": "Language model",
      "score": 0.6232078075408936
    },
    {
      "name": "Machine translation",
      "score": 0.5642989873886108
    },
    {
      "name": "Natural language processing",
      "score": 0.5458226799964905
    },
    {
      "name": "Language understanding",
      "score": 0.4694706201553345
    },
    {
      "name": "Cover (algebra)",
      "score": 0.4494492709636688
    },
    {
      "name": "Engineering",
      "score": 0.0875137448310852
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    }
  ],
  "cited_by": 19
}