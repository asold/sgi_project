{
  "title": "AutoJSA: A Knowledge-Enhanced Large Language Model Framework for Improving Job Safety Analysis",
  "url": "https://openalex.org/W4411783251",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5110702060",
      "name": "Shuo Xu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5101680543",
      "name": "Jinsong Zhao",
      "affiliations": [
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2906271394",
    "https://openalex.org/W4323926504",
    "https://openalex.org/W4408373949",
    "https://openalex.org/W1988942495",
    "https://openalex.org/W3212632933",
    "https://openalex.org/W2555758468",
    "https://openalex.org/W2017820014",
    "https://openalex.org/W2806122638",
    "https://openalex.org/W4365147601",
    "https://openalex.org/W2056918233",
    "https://openalex.org/W4402351636",
    "https://openalex.org/W4366990278"
  ],
  "abstract": "Job Safety Analysis (JSA) is critical for proactively identifying workplace hazards, assessing their potential consequences, and implementing effective control measures. However, traditional JSA methods can be inefficient and prone to errors, particularly in complex industrial environments. This paper introduces AutoJSA, a knowledge-enhanced framework that leverages large language models (LLMs) to automate and optimize the JSA process. We collected 73 high-quality JSA reports from a chemical engineering company and divided the JSA workflow into three key tasks: hazard identification, consequence identification, and control measure generation. Two approaches ÔøΩ fine-tuning and retrieval-augmented generation (RAG) ÔøΩ were employed on a base LLM (GLM-4-9B-Chat) to adapt it for these domain-specific tasks. Experimental results demonstrate that both fine-tuning and RAG significantly improve task performance relative to the unmodified model, with fine-tuning generally providing larger gains. While RAG offers the advantages of dynamic knowledge retrieval and simpler implementation, it yields more modest improvements compared to fine-tuning. Moreover, a direct combination of fine-tuning and RAG did not show additional benefits under the current approach. Overall, AutoJSA effectively addresses the limitations of traditional JSA by increasing the accuracy and efficiency of safety analysis, laying the groundwork for fully automated JSA report generation. The findings underscore the potential of advanced artificial intelligence and natural language processing techniques to enhance workplace safety management in complex and rapidly evolving industrial settings.",
  "full_text": " \n  Research Article - Peer Reviewed Conference Proceeding \nESCAPE 35 - European Symposium on Computer Aided Process Engineering \nGhent, Belgium. 6-9 July 2025 \n Jan F.M. Van Impe, Gr√©goire L√©onard, Satyajeet S. Bhonsale, \nMonika E. Pola≈Ñska, Filip Logist (Eds.) \nhttps://doi.org/10.69997/sct.186169  Syst Control Trans 4:1300-1305 (2025) 1300 \nAutoJSA: A Knowledge-Enhanced Large Language Model \nFramework for Improving Job Safety Analysis \nShuo Xua, Jinsong Zhaoa* \na Tsinghua University, Department of Chemical Engineering, Beijing, China \n* Corresponding Author: jinsongzhao@tsinghua.edu.cn.  \nABSTRACT \nJob Safety Analysis (JSA) is critical for proactively identifying workplace hazards, assessing their \npotential consequences, and implementing effective control measures. However, traditional JSA \nmethods can be inefficient and prone to errors, particularly in complex industrial environments. \nThis paper introduces AutoJSA, a knowledge-enhanced framework that leverages large language \nmodels (LLMs) to automate and optimize the JSA process. We collected 73 high -quality JSA re-\nports from a chemical engineering company and d ivided the JSA workflow into three key tasks: \nhazard identification, consequence identification, and control measure generation. Two ap-\nproaches ‚Äî fine-tuning and retrieval-augmented generation (RAG) ‚Äî were employed on a base \nLLM (GLM-4-9B-Chat) to adapt it for these domain -specific tasks. Experimental results demon-\nstrate that both fine -tuning and RAG significantly improve task performance relative to the un-\nmodified model, with fine-tuning generally providing larger gains. While RAG offers the advantages \nof dynamic knowledge retrieval and simpler implementation, it yields more modest improvements \ncompared to fine -tuning. Moreover, a direct combination of fine -tuning and RAG did n ot show \nadditional benefits under the current approach. Overall, AutoJSA effectively addresses the limita-\ntions of traditional JSA by increasing the accuracy and efficiency of safety analysis, laying the \ngroundwork for fully automated JSA report generation.  The findings underscore the potential of \nadvanced artificial intelligence and natural language processing techniques to enhance workplace \nsafety management in complex and rapidly evolving industrial settings. \nKeywords: Artificial Intelligence, Job Safety Analysis, Large Language Model \n1. INTRODUCTION \nIn the chemical process industry, tasks such as cal-\nibrating temperature control systems, replacing seals on \nreactors, and conducting maintenance on pumps and \ncompressors are essential for maintaining safe and effi-\ncient operations. However, these activities  can expose \nworkers to hazards like electrical shock, high -tempera-\nture burns, or poisoning. Job Safety Analysis (JSA) is cru-\ncial for mitigating these risks by systematically examining \neach task, identifying potential hazards, and implement-\ning effective control measures [1]. \nHeinrich is considered the first scholar to introduce \nthe term Job Safety Analysis [ 2]. Since then, JSA has \nbeen widely applied in industries such as process engi-\nneering, construction, and healthcare [ 3], with various \nguidelines available for conducting JSA [ 4,5]. A \ncomprehensive JSA typically includes four key compo-\nnents: Job Description, Hazard Identification, Conse-\nquence Identification, and Control Measures Generation. \nJSA improves workplace safety by identifying and ad-\ndressing potential risks before wor k begins, helping to \nreduce accidents and injuries [ 6,7], and identifying po-\ntential human errors to ensure quality performance [8]. \nHowever, traditional JSA faces several challenges:  \nit may fail to identify all potential risks, especially those \narising from unforeseen circumstances or complex sce-\nnarios [9]. Additionally, it is time -consuming, particularly \nin dynamic environments [10], and there can be signifi-\ncant variations in hazard identification quality, as differ-\nent individuals may have different expertise levels [11]. \nTo address these challenges, researchers have ex-\nplored AI methods to automate JSA . For example, one \nstudy integrated JSA with a modified Petri net for non -\n \nXu et al. / LAPSE:2025.0360 Syst Control Trans 4:1300-1305 (2025) 1301  \nroutine operations, which helped identify sequential \nanomalies. However, its static and semi-quantitative na-\nture limits dynamic risk simulation and the ability to ac-\ncount for complex interactions [12]. Another study devel-\noped a knowledge graph for automati c JSA using \nGRAKN.AI, which provided solid risk assessments, but it \nrelied on pre -existing J SA documents and lacked real -\ntime data or natural language support [13]. Similarly, a \nthird proposed an ontology- based framework combined \nwith Building Information Modeling for automated J SA. \nHowever, it overlooks dynamic factors, such as site lay-\nout changes, and requires manual rule adjustments [14]. \nWhile these approaches are sophisticated, they often \nsuffer from limited accuracy and adaptability due to their \nreliance on expert rules, structured data, and static mod-\nels, which can miss subtle or emerging risks and restrict \ncross-domain applicability.  \nLarge language models (LLMs), like ChatGPT and \nGPT-4 [15], present a solution by handling unstructured \ntext, detecting new risks, and adapting across domains‚Äî\nmaking them ideal for automating hazard identification \nand safety reporting. Although some studies  have ex-\nplored the application of LLMs for similar tasks [16- 18], \nsystematic evaluation of LLMs for the entire JSA process \nremains limited. \nThis paper introduces AutoJSA, a knowledge -en-\nhanced LLM framework using fine -tuning and retrieval -\naugmented generation (RAG) techniques to improve JSA \naccuracy, efficiency, and adaptability in complex indus-\ntrial environments. \n2. METHODOLOGY \n2.1 Framework Overview of AutoJSA  \n \nFigure 1. Framework overview of AutoJSA. \nFigure 1 illustrates the framework of this study, \nwhich aims to generate JSA reports using LLMs. The da-\ntaset consists of high- quality JSA reports for training, \nvalidation, and testing. \nThe JSA report generation is divided into three sub-\ntasks: hazard identification, consequence identification, \nand control measure generation. Each task has corre-\nsponding JSA reports split into training, validation, and \ntest sets. \nFine-tuning and RAG are applied to each sub -task. \nFine-tuning optimizes the base model for task -specific \ndata, while RAG enhances information retrieval and \noutput generation using a structured knowledge base. \nBoth methods are evaluated through a predefined frame-\nwork to assess performance under different parameters. \nThe best-performing parameters, selected from val-\nidation results, are tested on the test set to compare the \nfine-tuned model, RAG-enhanced model, and base model. \nThe evaluation will determine the effectiveness of fine -\ntuning and RAG in improving JSA report generation. \n2.2 Data Description \nThe data used in this study consists of 73 high-qual-\nity JSA reports obtained from a chemical engineering \ncompany. These reports cover various jobs and provide \na detailed safety analysis for each step of the job. Each \nJSA report is composed of two main sect ions: JobInfo \nand JSA. A detailed description of the data structure can \nbe found in Figure 2.  \n \nFigure 2. Detailed description of JSA report. \nThese reports are divided into three parts: 58 for \ntraining, 8 for validation, and 7 for testing. \n2.3 Task Breakdown of JSA \nAutoJSA subdivides the JSA process into three key \ntasks: Hazard Identification, Consequence identification, \nand Control Measure Generation. This division reflects \nthe actual workflow followed during safety analysis, \nwhere each task builds upon the previous one to com-\nprehensively assess and mitigate risks associated with a \njob. \nTask 1 ‚Äî Hazard Identification: This task identifies \npotential hazards for each job step. The inputs include \nthe job name, job content, job step name, and step de-\ntails. The output is a list of hazards associated with the \nspecific step. \nTask 2 ‚Äî Consequence Identification: This task as-\nsesses the potential consequences of each identified \nhazard. The inputs include the same information as in \nHazard Identification, plus the specific hazard. The out-\nput is a list of possible consequences. \nTask 3 ‚Äî Control Measure Generation: This task \ngenerates control measures to mitigate the identified \nhazards. The inputs include the job name, job content, job \nstep details, hazard, and consequence. The output is a \nset of control measures. \nTable 1 shows the number of samples in the training \n\n \nXu et al. / LAPSE:2025.0360 Syst Control Trans 4:1300-1305 (2025) 1302  \nset, validation set, and test set for each task. \nTable 1: Distribution of s amples across t raining, valida-\ntion, and test sets for each task. \nTask Sample Size \nTrain Validation Test \nTask ÓÅ≤ ÓÅ≥ÓÅµÓÅ≤ ÓÅ¥ÓÅπ ÓÅ¥ÓÅ± \nTask ÓÅ≥ ÓÅ∏ÓÅ∂ÓÅ∫ ÓÅ≤ÓÅ≤ÓÅ∏ ÓÅ∫ÓÅ≥ \nTask ÓÅ¥ ÓÅ∏ÓÅ∂ÓÅ∫ ÓÅ≤ÓÅ≤ÓÅ∏ ÓÅ∫ÓÅ≥ \n2.4 Application of LLM \nLLMs are advanced AI systems trained on extensive \ntextual data to generate human -like text and perform \ntasks like text generation, translation, and question an-\nswering. In this study, LLMs automate and improve the \nJSA process, aiding in hazard identification, conse-\nquence identification, and control measure generation. \nFine-tuning and RAG techniques are used to optimize the \nmodel for better accuracy and relevance in the JSA do-\nmain. \nConsidering the task's complexity and resource re-\nquirements, we use GLM -4-9B-Chat [19] as the base \nmodel. GLM-4-9B-Chat is a large language model with \napproximately 9 billion parameters, developed as part of \nthe GLM-4 series by Zhipu AI. By fine-tuning and applying \nRAG, we adapt the model for JSA report generation, im-\nproving its ability to identify hazards, assess conse-\nquences, and suggest control measures. \n2.4.1 Fine-tuning \n \nFigure 3: A sample from the dataset used in Task 1. The \ndata used in this study is in Chinese. For the convenience \nof reading, all related content here and in subsequent \nsections has been translated into English. \nFine-tuning involves adapting a pre -trained LLM to \na specific task or domain by continuing its training on a \nsmaller, domain-specific dataset [20]. While pre -trained \nmodels like LLMs are trained on large, diverse datasets \nto capture general linguistic patterns, fine -tuning helps \nthe model specialize in a particular domain‚Äîin this case, \nJSA. This approach improves the model‚Äôs ability to iden-\ntify hazards, assess consequences, and generate control \nmeasures, ensuring that the outputs are relevant and ac-\ncurate for the specific task. \nFine-tuning works by initializing the pre -trained \nmodel with knowledge from its original training, followed \nby further training on task -specific data. In addition to \ntask-specific inputs (as described in Section 2.3), prompt \nengineering [21] is also employed to guide the model dur-\ning training. Figure 3 illustrates an example dataset used \nfor Task 1, and similar datasets were created for Tasks 2 \nand 3.\n The corresponding datasets can be found in the \nsupplementary materials.  Fine-tuning is performed by \nadjusting parameters based on the loss values from the \ntraining and validation sets, and the model with the low-\nest validation loss is selected for testing. \n2.4.2 RAG \nRAG enhances LLMs by incorporating external re-\ntrieval mechanisms to provide domain -specific, up -to-\ndate information [22]. Unlike traditional models that rely \nsolely on pre-trained knowledge, RAG retrieves relevant \nexternal data to improve accuracy. In the RAG framework, \nthe model retrieves documents based on the input query, \nenhancing generation for more contextually approp riate \nresponses, which is crucial for tasks like JSA. \n \nFigure 4. RAG framework uesd in this study. \nFor this study, we augmented the model‚Äôs inputs \nwith content retrieved via knowledge retrieval, as shown \nin Figures 3 and 4. For Task 1, a knowledge database was \ncreated by extracting 278 job steps and hazard lists. Us-\ning the BERT -base-Chinese model [ 23], we vectorized \nthe job steps and indexed them. The model retrieves the \nk most similar job steps based on cosine similarity and \nincorporates the corresponding hazard lists into the input \nfor the LLM. Similar processes were applied to Tasks 2 \nand 3, with variations in the search fields and content re-\ntrieved, and the RAG database used in this study is \n\n \nXu et al. / LAPSE:2025.0360 Syst Control Trans 4:1300-1305 (2025) 1303  \navailable in the supplementary materials. \nWe evaluated different values of k to determine the \noptimal performance for each task, selecting the value \nthat performed best on the validation set. \n2.4.3 Evaluation Criteria \nModel performance was assessed by measuring se-\nmantic similarity between model outputs and ground \ntruth. Our similarity evaluation employs a BERT -based \nsemantic encoding scheme: texts are first encoded using \nthe BERT-base-Chinese model (768-dimensional hidden \nstates), where the [CLS] token's hidden state serves as \nsentence embeddings. After L2 -normalization, we com-\npute cosine similarity through sim(\nùëéùëé,ùëèùëè)=ùëéùëé‚ä§ùëèùëè to quantify \nsemantic similarity. \n3. RESULTS AND DISCUSSION \n3.1 Fine-tuning \nWe adjusted the fine -tuning parameters based on \nthe model‚Äôs loss performance on the validation set. After \nmultiple rounds of experimentation, we identified a set of \nreasonable parameters that resulted in a lower validation \nloss. The specific parameters use d can be found in the \nsupplementary materials. Figure 5 illustrates the training \nand validation loss curves for the three tasks under these \nparameters during model training. \n \nFigure 5. Training and validation loss for three tasks. \nFrom the data trends, under the parameter settings \nused in this study, the training and validation losses for \nall three tasks showed a decreasing trend as the number \nof training epochs increased. This indicates that fine-tun-\ning was effective for all three tasks. However, after a cer-\ntain number of epochs, the validation loss for each task \nstarted to increase, suggesting overfitting. Based on \nthese experimental results, we selected the models cor-\nresponding to the epochs with the lowest validation loss \n‚Äî 5, 4, and 6 epochs for Tasks 1, 2, and 3, respectively \n‚Äî to be tested on the test set. \n3.2 RAG \nIn the construction of the RAG framework, we opti-\nmized the value of k for each task based on the validation \nset scores. Figure 6 shows the model's scores on the val-\nidation set for the three tasks with different values of k , \nalong with the time consumption per output generation. \n \nFigure 6. Model scores and time consumption vs. k. \nDuring the RAG framework construction, the mod-\nel's performance did not exhibit a significant trend with \nvarying k values. However, when looking at all three tasks, \nsmaller values of k  already yielded good results. This \nsuggests that the LLM places more importance on the \nmost similar inputs, and it is more effective at leveraging \nthe most relevant knowledge to solve the problem. As the \nvalue of k increases, more knowledge did not result in \nimproved model performance. This could be due to the \ninclusion of mor e low -similarity knowledge, which may \nhave interfered with the model's reasoning. Additionally, \nas k increases, the input text becomes longer, leading to \nlonger inference times. Based on these observations, we \nselected k=2, k=1, and k=1 for Tasks 1, 2, and 3, respec-\ntively, for testing on the test set.  \n3.3 Test Results \n \nFigure 7. Different model scores on the test set.  \nFigure 7 shows the performance scores of the orig-\ninal model, the fine-tuned model, and the RAG-enhanced \nmodel on the test set for the three tasks. It also includes \nthe results of the fine -tuned model when further aug-\nmented with RAG. \nThe test results show that both the fine -tuned and \nRAG-enhanced models outperform the original model. \nHowever, RAG provides modest improvements, while \nfine-tuning offers more substantial gains. Fine-tuning re-\nquires more computational resources and a comple x \n\n \nXu et al. / LAPSE:2025.0360 Syst Control Trans 4:1300-1305 (2025) 1304  \nimplementation, while RAG is simpler and allows dynamic \nupdates to the knowledge base, offering flexibility. \nWhen we combined the fine-tuned model with RAG, \nno performance improvement was observed over fine -\ntuning alone. This suggests that a more advanced inte-\ngration approach, such as using a separate model to \ncombine the outputs of both, may be needed to fully lev-\nerage the strengths of both techniques for optimizing \nJSA report generation. \n3.4 Case Study \nTable 2 shows the results of the original, fine-tuned, \nand RAG-enhanced models for hazard identification, as \ndescribed in Figure 3. The original model provides a \nbroad list of potential hazards in wastewater plant piping \nwelding, covering a wide range of risks such as gas leaks, \nfire hazards, and equipment malfunctions. While compre-\nhensive, this list can be overwhelming and less releva nt \nto the task, requiring users to filter key risks. \nThe fine-tuned model provides a more focused out-\nput, listing four primary hazards: hot work operations, \ntemporary electrical risks, inadequate equipment prepa-\nration, and insufficient protective equipment . This ap-\nproach improves clarity and helps users quickly identify \ncritical hazards, although it may overlook secondary risks. \nThe RAG- enhanced model incorporates external data-\nbases, identifying five specific hazards, including non -\ncompliance with safety permits and improper equipment \nplacement. This model of fers a detailed and practical \nidentification of risks, but its effectiveness depends on \nthe quality of the external data used. \nIn summary, the original model offers broad cover-\nage but may lead to information overload. The fine-tuned \nmodel provides precise and efficient results, ideal for \nquick hazard identification. The RAG -enhanced model \nensures comprehensive hazard identification with exter-\nnal knowledge. A better approach may involve \nstrategically combining fine-tuning and RAG to enhance \naccuracy and detail. \n4. CONCLUSION \nThe results demonstrate that the AutoJSA frame-\nwork enhances the accuracy and efficiency of Job Safety \nAnalysis by leveraging fine -tuning and RAG with LLMs. \nBoth methods outperform the base model in hazard iden-\ntification, consequence assessment, and control meas-\nure generation, with fine-tuning generally providing more \nsignificant improvements. The fine -tuned model pro-\nduces more concise outputs, while the RAG -enhanced \nmodel generates more comprehensive results. \nHowever, the study has limitations.  The current \nframework does not explicitly incorporate traditional risk \nassessment factors such as hazard severity and proba-\nbility of occurrence . The dataset of 73  Chinese JSA re-\nports, while sufficient for proof of concept, may not fully \nrepresent the diversity of tasks in broader industrial set-\ntings or different linguistic contexts. Fine-tuning, though \neffective, involves high computational cost, and combin-\ning it with RAG did not yield additional benefits, suggest-\ning potential inefficiencies in their integration. Addition-\nally, the BERT-based similarity metric used may not cap-\nture all qualitative aspects of JSA effectiveness. \nFuture research directions could address current \nlimitations by integrating risk assessment metrics into the \nframework, expanding the dataset to encompass diverse \nindustrial scenarios across different linguistic and cultural \ncontexts, exploring integration strategies for RAG and \nfine-tuning to enhance complementary benefits, and de-\nveloping hybrid evaluation metrics that combine BERT -\nbased similarity with qualitative safety performance indi-\ncators. \nAutoJSA demonstrates AI's transformative potential \nin workplace safety management by enhancing hazard \nModel Output Score \nOriginal model \nHot work causing fire or explosionÓÇì electrical equipment causing electric \nshockÓÇì lack of warning signs leading to unauthorized personnel entering haz-\nardous areasÓÇì insufficient preparation of protective equipment leading to in-\njuriesÓÇî \nÓÅ±ÓÇîÓÅ∏ÓÅ≤ÓÅ¥ÓÅ∂ \nFine-tuned \nmodel \nHot work riskÓÇì temporary electricity riskÓÇì inadequate equipment preparationÓÇì \ninsufficient protective equipmentÓÇî ÓÅ±ÓÇîÓÅπÓÅµÓÅ∏ÓÅ∏ \nRAG-enhanced \nmodel \nImproper handling of hot work and temporary electrical work permitsÓÇì incor-\nrect placement of warning signsÓÇì inadequate equipment preparationÓÇì insuffi-\ncient preparation of gas detectors and protective equipmentÓÇì insufficient per-\nsonnel trainingÓÇî \nÓÅ±ÓÇîÓÅ∏ÓÅ∏ÓÅ≥ÓÅ∂ \nExpected \nFire or explosion caused by hot workÓÇì electric shock due to electrical equip-\nment usageÓÇì failure to set up warning signs leading to unauthorized entry into \nhazardous areasÓÇì inadequate preparation of protective equipment resulting \nin injuriesÓÇî \n- \nTable 2: Comparison of output results. \n \nXu et al. / LAPSE:2025.0360 Syst Control Trans 4:1300-1305 (2025) 1305  \nidentification, consequence Identification, and control \nmeasure generation. The framework paves the way to-\nward fully automating JSA reports and ensuring safer op-\nerational practices in complex industries. \nDIGITAL SUPPLEMENTARY MATERIAL \nThe datasets, fine-tuning parameters, and all exper-\nimental results in this study can be accessed via the fol-\nlowing link:  \nhttps://cloud.tsinghua.edu.cn/d/f80c1ca00d694f7ea7f9/ \nACKNOWLEDGEMENTS \nThis work was supported by the National Key R&D \nProgram of China under Grant 2024YFC3013600. \nREFERENCES \n1. Analysis JH, Lebowitz J. Essentials of job hazard \nanalysis. Chem Eng Prog 114:52-6 (2018). \n2. Albrechtsen E, Solberg I, Svensli E. The application \nand benefits of job safety analysis. Saf Sci \n113:425-37 (2019). \n3. Ghasemi F, Doosti-Irani A, Aghaei H. Applications, \nshortcomings, and new advances of job safety \nanalysis (JSA): findings from a systematic review. \nSaf Health Work 14(2):153-62 (2023). \n4. Swartz G. Job hazard analysis: A guide to \nidentifying risks in the workplace. Government \nInstitutes (2001). \n5. Geronsin R. Job hazard assessment: a \ncomprehensive approach. Prof Saf 46(12):23 \n(2001). \n6. Yoon IK, Seo JM, Jang N, Oh SK, Shin D, Yoon ES. \nA practical framework for mandatory job safety \nanalysis embedded in the permit-to-work system \nand application to the gas industry. J Chem Eng \nJpn 44(12):976-88 (2011). \n7. Palega M. Application of the job safety analysis \n(JSA) method to assess occupational risk at the \nworkplace of the laser cutter operator. Manag Prod \nEng Rev 12(3) (2021). \n8. Roughton J, Crutchfield N. Job Hazard Analysis: A \nGuide for Voluntary Compliance and Beyond. \nButterworth-Heinemann (2015). \n9. Zheng W, Shuai J, Shan K. The energy source-\nbased job safety analysis and application in the \nproject. Saf Sci 93:9-15 (2017). \n10. Chi NW, Lin KY, Hsieh SH. Using ontology-based \ntext classification to assist job hazard analysis. Adv \nEng Informatics 28(4):381-94 (2014). \n11. Namian M. Factors Affecting Construction Hazard \nRecognition and Safety Risk Perception. North \nCarolina State University (2017). \n12. Li W, Cao Q, He M, Sun Y. Industrial non-routine \noperation process risk assessment using job safety \nanalysis (JSA) and a revised Petri net. Process Saf \nEnviron Prot 117:533-8 (2018). \n13. Pandithawatta S, Ahn S, Rameezdeen R, Chow CW, \nGorjian N, Kim TW. Development of a knowledge \ngraph for automatic job hazard analysis: The \nschema. Sensors 23(8):3893 (2023). \n14. Zhang S, Boukamp F, Teizer J. Ontology-based \nsemantic modeling of construction safety \nknowledge: towards automated safety planning for \njob hazard analysis (JHA). Autom Constr 52:29-41 \n(2015). \n15. Achiam J, Adler S, Agarwal S, Ahmad L, Akkaya I, \nAleman FL, ... McGrew B. GPT-4 technical report. \narXiv preprint arXiv:2303.08774 (2023). \n16. Bernardi ML, Cimitile M, Pecori R. Automatic job \nsafety report generation using RAG-based LLMs. \nIn: 2024 International Joint Conference on Neural \nNetworks (IJCNN). Ed: IEEE (2024) p. 1-8. \n17. Uddin SJ, Albert A, Ovid A, Alsharef A. Leveraging \nChatGPT to aid construction hazard recognition \nand support safety education and training. \nSustainability 15(9):7121 (2023). \n18. Kvale DK. Deep Learning in Construction Safety: \nQuality Assessment, Hazard Identification, and \nPreventive Measure Proposals in Job Safety \nAnalysis. NTNU (2023). \n19. GLM T, Zeng A, Xu B, Wang B, Zhang C, Yin D, ... \nWang Z. ChatGLM: a family of large language \nmodels from GLM-130B to GLM-4 all tools. arXiv \npreprint arXiv:2406.12793 (2024). \n20. Dodge J, Ilharco G, Schwartz R, Farhadi A, \nHajishirzi H, Smith N. Fine-tuning pretrained \nlanguage models: weight initializations, data \norders, and early stopping. arXiv preprint \narXiv:2002.06305 (2020). \n21. White J, Fu Q, Hays S, Sandborn M, Olea C, Gilbert \nH, ... Schmidt DC. A prompt pattern catalog to \nenhance prompt engineering with ChatGPT. arXiv \npreprint arXiv:2302.11382 (2023). \n22. Lewis P, Perez E, Piktus A, Petroni F, Karpukhin V, \nGoyal N, ... Kiela D. Retrieval-augmented \ngeneration for knowledge-intensive NLP tasks. Adv \nNeural Inf Process Syst 33:9459-74 (2020). \n23. Devlin J. BERT: pre-training of deep bidirectional \ntransformers for language understanding. arXiv \npreprint arXiv:1810.04805 (2018). \n¬© 2025 by the authors. Licensed to PSEcommunity.org and PSE \nPress. This is an open access article under the creative com-\nmons CC-BY-SA licensing terms. Credit must be given to creator \nand adaptations must be shared under the same terms. See \nhttps://creativecommons.org/licenses/by-sa/4.0/  \n \n",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6978440284729004
    },
    {
      "name": "Knowledge management",
      "score": 0.4633157253265381
    }
  ]
}