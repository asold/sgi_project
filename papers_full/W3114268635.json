{
  "title": "Reservoir Transformers",
  "url": "https://openalex.org/W3114268635",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2111184121",
      "name": "Sheng Shen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2893542027",
      "name": "Alexei Baevski",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": null,
      "name": "Ari Morcos",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2003317548",
      "name": "Kurt Keutzer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2139710560",
      "name": "Michael Auli",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A302325417",
      "name": "Douwe Kiela",
      "affiliations": [
        "Meta (Israel)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2111072639",
    "https://openalex.org/W3035812575",
    "https://openalex.org/W2086789740",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2981757109",
    "https://openalex.org/W2017827237",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2103179919",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2963334011",
    "https://openalex.org/W4287762561",
    "https://openalex.org/W2952809536",
    "https://openalex.org/W2913190747",
    "https://openalex.org/W2964013315",
    "https://openalex.org/W2155653793",
    "https://openalex.org/W2592867566",
    "https://openalex.org/W2611445505",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W2127342049",
    "https://openalex.org/W2276387674",
    "https://openalex.org/W1625958017",
    "https://openalex.org/W2752159661",
    "https://openalex.org/W3104235057",
    "https://openalex.org/W2125930537",
    "https://openalex.org/W2257408573",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2963140169",
    "https://openalex.org/W2805003733",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3008311476",
    "https://openalex.org/W2150354929",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W3034234149",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2970277060",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2970645034",
    "https://openalex.org/W3008851394",
    "https://openalex.org/W2971713705",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W3010768098",
    "https://openalex.org/W3195303735",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W4297823153",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W3033943443",
    "https://openalex.org/W2962698540",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W2123395972",
    "https://openalex.org/W2979476256",
    "https://openalex.org/W3035691519",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2626445779",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2886067286",
    "https://openalex.org/W4302023899",
    "https://openalex.org/W3123615524",
    "https://openalex.org/W4297797035",
    "https://openalex.org/W3007700590",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2966695781",
    "https://openalex.org/W2899790086",
    "https://openalex.org/W2979473749",
    "https://openalex.org/W1996640396",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2054879295",
    "https://openalex.org/W2963703360",
    "https://openalex.org/W1964175594",
    "https://openalex.org/W2964190861",
    "https://openalex.org/W188912188",
    "https://openalex.org/W2982316857",
    "https://openalex.org/W3007634665",
    "https://openalex.org/W2962939986",
    "https://openalex.org/W2963232038",
    "https://openalex.org/W4287667694",
    "https://openalex.org/W3041866211",
    "https://openalex.org/W2025919276",
    "https://openalex.org/W2119717200",
    "https://openalex.org/W3007451631",
    "https://openalex.org/W2565565355",
    "https://openalex.org/W2914924671",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2155910151",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2979874021",
    "https://openalex.org/W3127806443",
    "https://openalex.org/W2171865010",
    "https://openalex.org/W3044169851",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W3021293129",
    "https://openalex.org/W2949804919",
    "https://openalex.org/W2144902422",
    "https://openalex.org/W2161278885",
    "https://openalex.org/W2012903341",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W1509923979",
    "https://openalex.org/W2586160710",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2887258823"
  ],
  "abstract": "Sheng Shen, Alexei Baevski, Ari Morcos, Kurt Keutzer, Michael Auli, Douwe Kiela. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 4294–4309\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4294\nReservoir Transformers\nSheng Shen†, Alexei Baevski‡, Ari S. Morcos‡, Kurt Keutzer†,\nMichael Auli‡, Douwe Kiela‡\n†UC Berkeley; ‡Facebook AI Research\nsheng.s@berkeley.edu, dkiela@fb.com\nAbstract\nWe demonstrate that transformers obtain im-\npressive performance even when some of the\nlayers are randomly initialized and never up-\ndated. Inspired by old and well-established\nideas in machine learning, we explore a variety\nof non-linear “reservoir” layers interspersed\nwith regular transformer layers, and show im-\nprovements in wall-clock compute time until\nconvergence, as well as overall performance,\non various machine translation and (masked)\nlanguage modelling tasks.\n1 Introduction\nTransformers (Vaswani et al., 2017) have dom-\ninated natural language processing (NLP) in re-\ncent years, from large scale machine transla-\ntion (Ott et al., 2018) to pre-trained (masked)\nlanguage modeling (Devlin et al., 2018; Rad-\nford et al., 2018), and are becoming more pop-\nular in other ﬁelds as well, from reinforcement\nlearning (Vinyals et al., 2019) to speech recog-\nnition (Baevski et al., 2019) and computer vi-\nsion (Carion et al., 2020). Their success is enabled\nin part by ever increasing computational demands,\nwhich has naturally led to an increased interest\nin improving their efﬁciency. Scalability gains in\ntransformers could facilitate bigger, deeper net-\nworks with longer contexts (Kitaev et al., 2020;\nWang et al., 2020; Beltagy et al., 2020; Kaplan\net al., 2020; Tay et al., 2020b). Conversely,\nimproved efﬁciency could reduce environmental\ncosts (Strubell et al., 2019) and hopefully help de-\nmocratize the technology.\nIn this work, we explore a simple question: if\nsome layers of the transformer are kept frozen—\ni.e., never updated after random initialization—\ncan we match the performance of fully learned\ntransformers, while being more efﬁcient? Surpris-\ningly, the answer is resoundingly yes; and what\nis more, we ﬁnd that freezing layers may actually\nimprove performance.\nBeyond desirable efﬁciency gains, random lay-\ners are interesting for several additional reasons.\nFixed randomly initialized networks (Gallicchio\nand Scardapane, 2020) converge to Gaussian pro-\ncesses in the limit of inﬁnite width (Daniely et al.,\n2016), have intriguing interpretations in metric\nlearning (Rosenfeld and Tsotsos, 2019; Giryes\net al., 2016), and have been shown to provide\nexcellent “priors” either for subsequent learn-\ning (Ulyanov et al., 2018) or pruning (Frankle and\nCarbin, 2018). Fixed layers allow for efﬁcient\nlow-cost hardware implementations (Schrauwen\net al., 2007) and can be characterized using only a\nrandom number generator and its seed. This could\nfacilitate distributed training and enables highly\nefﬁcient deployment to edge devices, since it only\nrequires transmission of a single number. The\nstrong performance of networks with ﬁxed lay-\ners also sheds new light on the inner workings\nof BERT (Devlin et al., 2018), and layer-wise in-\nterpretations of such models (Rogers et al., 2020;\nTenney et al., 2019). It appears that “not all layers\nare created equal” (Zhang et al., 2019) is true to\nsuch an extent that some layers can simply remain\nrandom and ﬁxed.\nRandom projections have a long history in\nmachine learning. By Cover’s theorem (Cover,\n1965), any high-dimensional non-linear transfor-\nmation is more likely to be linearly separable\nthan its lower-or-equal-dimensional input space.\nBy Johnson-Lindenstrauss (Johnson and Linden-\nstrauss, 1984), random projections distort Eu-\nclidean distances very little under mild assump-\ntions, which is useful e.g. for dimensionality re-\nduction and random indexing (Sahlgren, 2005).\nFixed random layers in neural networks pre-date\ndeep learning by far (Gamba et al., 1961; Baum,\n1988). Indeed, random kernel methods have long\n4295\nbeen inﬂuential in machine learning (Rahimi and\nRecht, 2008, 2009).\nOne way to think of such layers is as “reser-\nvoirs” (Lukoˇseviˇcius and Jaeger, 2009), where a\nhighly non-linear high-dimensional black box rep-\nresentation is provided to a lightweight “readout”\nnetwork, as in echo state networks (Jaeger, 2003)\nand liquid state machines (Maass et al., 2002). The\nbeneﬁt of such an approach is that the reservoir has\nﬁxed parameters and is computationally efﬁcient,\nas it can be pre-computed and does not (necessar-\nily) require backpropagation.\nIn NLP, Wieting and Kiela (2019) showed that\nrandom sentence encoders present a strong base-\nline for text classiﬁcation, with subsequent work\nshowing applications in a variety of tasks from\nsummarization to machine translation (Enguehard\net al., 2019; Garg et al., 2020; Pilault et al., 2020).\nTo our knowledge, this work is the ﬁrst to exam-\nine this phenomenon in transformers, and the ﬁrst\nto recursively alternate reservoirs with subsequent\ntransformer layers acting as readout functions. We\nintroduce “reservoir transformers”, wherein ﬁxed\nrandom reservoir layers are interspersed with reg-\nular updateable transformer layers. The goal of\nthis work is to put our understanding of trans-\nformer models on a more solid footing by provid-\ning empirical evidence of their capabilities even\nwhen some of their parameters are ﬁxed. Our con-\ntributions are as follows:\n• We introduce a area under the convergence\ncurve metric for measuring performance-\nefﬁciency trade-offs, and show that replacing\nregular transformer layers with reservoir lay-\ners leads to improvements.\n• We show that the addition of reservoir layers\nleads to improved test set generalization on a\nvariety of tasks in a variety of settings.\n• We show that pre-trained masked lan-\nguage modelling architectures like BERT and\nRoBERTa (Liu et al., 2019) can beneﬁt from\nhaving some of their layers frozen, both dur-\ning pre-training as well as when ﬁne-tuning\non downstream tasks.\n• We experiment with different types of reser-\nvoir layers, including convolutional and re-\ncurrent neural network-based ones.\n• We show empirical evidence that the back-\nward pass can be skipped in its entirety by\napproximating upstream gradients using an\napproach we call backskipping, which can\nreduce the training compute further without\nsacriﬁcing performance.\n2 Approach\nThis paper is based on a very simple idea. Neural\nnetworks are trained via backpropagation, which\ninvolves consecutive steps of matrix addition and\nmultiplication, i.e.,\nθt+1 ←θt −η∂J\n∂θt\n; ∂J\n∂θt\n= ∂J\n∂Ln\n∂Ln\n∂Ln−1\n··· ∂L0\n∂x\nfor some objective J, parameterization θ and\nlearning rate η, with the gradient computed via\nthe chain rule, where Li is the i-th layer of the\nneural network and x is the input. Let L =\nTransformer(X) be a single layer in a Transformer\nnetwork (Vaswani et al., 2017), i.e.,\nH = MultiHeadSelfAttn(LayerNorm(X)) +X\nL= FFN(LayerNorm(H)) +H\nNow, during every “backward pass”, we com-\npute the Jacobian for parameters θL at layer L,\nwhich are used to update the parameters of L, θL\nt ,\nas well as to compute the next layer’s Jacobian,\nthus back-propagating the gradients. In this work\nhowever, for some of the layers, we still backprop-\nagate through them to compute gradients for ear-\nlier layers, but we never apply the parameter up-\ndate. As a result, these layers stay ﬁxed at their\ninitialization, saving computational resources.\n2.1 Background\nNaturally, never updating some of the parameters\nis computationally more efﬁcient, as some matrix\naddition operations can be skipped in the back-\nward pass, but why is this not detrimental to the\nperformance of the network?\nIn the early days of neural networks, the bot-\ntom layers were often kept ﬁxed as “associa-\ntors” (Block, 1962), or what (Minsky and Papert,\n2017) called the Gamba perceptron (Gamba et al.,\n1961; Borsellino and Gamba, 1961). Fixed ran-\ndom networks (Baum, 1988; Schmidt et al., 1992;\nPao et al., 1994) have been explored from many\nangles, including as “random kitchen sink” kernel\nmachines (Rahimi and Recht, 2008, 2009), “ex-\ntreme learning machines” (Huang et al., 2006) and\n4296\nreservoir computing (Jaeger, 2003; Maass et al.,\n2002; Luko ˇseviˇcius and Jaeger, 2009). In reser-\nvoir computing, input data are represented through\nﬁxed random high-dimensional non-linear rep-\nresentations, called “reservoirs”, which are fol-\nlowed by a regular (often but not necessarily lin-\near) “readout” network to make the ﬁnal classiﬁ-\ncation decision.\nThe theoretical justiﬁcation for these ap-\nproaches lies in two well-known results in ma-\nchine learning: Cover’s theorem (Cover, 1965)\non the separability of patterns states that high-\ndimensional non-linear transformations are more\nlikely to be linearly separable; and the Johnson-\nLindenstrauss lemma (Johnson and Lindenstrauss,\n1984) shows that (most) random projections dis-\ntort Euclidean distances very little.\nPractically, random layers can be seen as a\ncheap way to increase network depth. There are\ninteresting advantages to this approach. Fixed lay-\ners are known to have particularly low-cost hard-\nware requirements and can be easily implemented\non high-bandwidth FPGAs with low power con-\nsumption (Hadaeghi et al., 2017; Tanaka et al.,\n2019), or on optical devices (Hicke et al.,\n2013). This might yield interesting possibilities\nfor training in a distributed fashion across mul-\ntiple devices, as well as for neuromorphic hard-\nware (Neftci et al., 2017). This approach also fa-\ncilitates lower-latency deployment of neural net-\nworks to edge devices, since weights can be shared\nsimply by sending the seed number, assuming the\nrandom number generator is known on both ends.\n2.2 Reservoir Transformers\nThis work explores inserting random non-linear\ntransformations, or what we call reservoir layers,\ninto transformer networks. Speciﬁcally, we exper-\niment with a variety of reservoir layers:\n• Transformer Reservoir: The standard trans-\nformer layer as described above, but with all\nparameters ﬁxed after initialization, includ-\ning the self-attention module.\n• FFN Reservoir: A transformer-style ﬁxed\nfeed-forward layer without any self-attention,\ni.e., FFN(LayerNorm (Previous layer)) +\nPrevious layer.\n• BiGRU Reservoir: A ﬁxed bidirectional\nGated Recurrent Unit (Cho et al., 2014) layer,\nwhich is closer in spirit to previous work on\nreservoir computing, most of which builds on\nrecurrent neural network architectures.\n• CNN Reservoir: A ﬁxed Convolutional Neu-\nral Network (LeCun et al., 1998) layer,\nspeciﬁcally light dynamical convolution lay-\ners (Wu et al., 2019), which are known to be\ncompetitive with transformers in sequence-\nto-sequence tasks.\nWe ﬁnd that all these approaches work well, to\na certain extent. For clarity, we focus primarily on\nthe ﬁrst two reservoir layers, but include a broader\ncomparison in Appendix A.\nIn each case, contrary to traditional reservoir\ncomputing, our reservoir layers are interspersed\nthroughout a regular transformer network, or what\nwe call a reservoir transformer. Since random pro-\njections are not learned and might introduce noise,\nsubsequent normal transformer “readout” layers\nmight be able to beneﬁt from additional depth\nwhile allowing us to recover from any adverse ef-\nfects of randomness. For example, previous work\nhas shown that ResNets, with all of their parame-\nters ﬁxed except for the scale and shift parameters\nof batch normalization, can still achieve high per-\nformance, simply by scaling and shifting random\nfeatures (Frankle et al., 2020). Adding some form\nof noise to the parameters is also known to help\nconvergence and generalization (Jim et al., 1995,\n1996; Gulcehre et al., 2016; Noh et al., 2017).\n3 Evaluation\nWe evaluate the proposed approach on a variety of\nwell-known tasks in natural language processing,\nnamely: machine translation, language modelling\nand masked language model pre-training.\nWe set out to do this work with the main\nobjective of examining any potential efﬁciency\ngains, i.e. the relationship between compute\ntime and task performance. This is closely re-\nlated to efforts in Green AI, which are concerned\nwith the trade-offs between compute, data, and\nperformance (Schwartz et al., 2019). We pro-\npose to measure this trade-off via the area under\nthe convergence curve (AUCC): similarly to how\nthe area under the receiver operating characteris-\ntic (Bradley, 1997, AUC-ROC) measures a clas-\nsiﬁer’s performance independent of the classiﬁca-\ntion threshold, AUCC measures a model’s perfor-\nmance independent of the speciﬁc compute bud-\n4297\nget. Speciﬁcally, AUCC is computed as follows:\n∫ ˆT\nt=0\n∑\nx,y∈D\ngt(f(x),y) (1)\nwhere f is the network and g is the evalua-\ntion metric, measured until convergence time ˆT,\nwhich is the maximum convergence time of all\nmodels included in the comparison. Note that time\nhere is wall-clock time, not iterations. By con-\nvergence, we mean that validation performance\nhas stopped improving, and hence the convergence\ncurve whose area we measure plots the desired\nmetric over time. Runs are averaged over multiple\nseeds and reported with standard deviation. We\nnormalize raw AUCC scores by their maximum to\nensure a more interpretable [0 −1] range.\nOne potential downside of this approach is that\nthe AUCC metric could lead to higher scores for\na model that converges quickly but to ultimately\nworse performance, if measured in a small win-\ndow. This can be solved by making sure that ˆT is\nset sufﬁciently high. We include the raw valida-\ntion curves in the appendix to demonstrate that the\nchosen window sizes are sufﬁcient and the results\nare not a inﬂuenced by this limitation. In addition,\nwe report the number of trainable parameters and\nthe wall-clock training time until maximum per-\nformance (plus 95% and 99% convergence results\nin the appendix). Finally, we show test set gener-\nalization in each experiment. Overall, this gives us\na wide set of axes along which to examine models.\n3.1 Experimental Settings\nWe evaluate on IWSLT de-en (Cettolo et al., 2015)\nand WMT en-de (Bojar et al., 2014) for ma-\nchine translation; enwiki8 (LLC, 2009) for lan-\nguage modelling; and experiment with RoBERTa\n(Liu et al., 2019) in our pretraining experiments.\nFor IWSLT, we follow the pre-processing steps\nin Edunov et al. (2018). The train/val/test split\nis 129k/10k/6.8k sentences. For WMT, we fol-\nlow pre-process as in Ott et al. (2018), with\n4.5M/16.5k/3k sentences in train/val/test. For en-\nwiki8, we follow the pre-processing steps in Dai\net al. (2019). The train/val/test split is 1M/54k/56k\nsentences. For RoBERTa pretraining, we follow\nthe pre-processing steps in Liu et al. (2019).\nWe use 8 V olta V100 GPUs for WMT and en-\nwik8, 32 V100 GPUs for RoBERTa and a sin-\ngle V100 for IWSLT. The hyperparameters for\nIWSLT14 and WMT16 were set to the best-\nperforming values from Ott et al. (2018) and Kasai\net al. (2020) respectively. The enwik8 experiment\nsettings followed Bachlechner et al. (2020) and the\nRoBERTa experiments followed Liu et al. (2019).\nAll the experiments in this paper were run with\n3 random seeds and the mean and standard devia-\ntion are reported. For the relatively small IWSLT,\nthe ˆT value in the AUCC metric was set to4 hours.\nFor the larger WMT, we set it to 20 hours. For\nenwiki8, it was 30 hours; and for the RoBERTa\npre-training experiments, it was set to 60 hours.\nThe projection weights in random layers were\ninitialized using orthogonal initialization (Saxe\net al., 2013), since random orthogonal projec-\ntions should ideally be maximally information-\npreserving, and which was found to work well em-\npirically for initializing ﬁxed random representa-\ntions in previous work (Wieting and Kiela, 2019).\nBiases and layer norm parameters were initialized\nusing their respective PyTorch defaults (based on\nXavier init; Glorot and Bengio, 2010).\nWe intersperse reservoir layers in alternating\nfashion starting from the middle. Speciﬁcally, we\nalternate one reservoir layer with one transformer\nlayer, and place the alternating block in the mid-\ndle. For example: a 7-layer encoder LLLLLLL\nin which we replace three layers with reser-\nvoirs becomes LRLRLRL, and with two becomes\nLLRLRLL. See Appendix C for a study com-\nparing this strategy to alternative approaches (e.g.,\nfreezing in the bottom, middle or top).\n4 Experiments\nIn what follows, we ﬁrst show our main result, on\na variety of tasks: reservoir transformers mostly\nhave better AUCC metrics; less training time per\nepoch; less convergence time until the best valida-\ntion performance is achieved; and even improved\ntest set generalization metrics. As a strong base-\nline method, we compare to LayerDrop (Fan et al.,\n2019). LayerDrop can also be seen as a method\nthat dynamically bypasses parts of the computa-\ntion during Transformer training in an attempt to\nimprove efﬁciency, and making it a strong compar-\nison to examine our methods. Then, we examine\nwhether we can minimize the expectation over the\ngradients of upstream layers in the network such\nthat we do notat all have to pass gradients through\nthe reservoir layers, skipping their backward pass.\n4298\n2 4 6 8 10 12\n# Updatable Encoder Layers\n0.96\n0.97\n0.98\n0.99\n1.00valid BLEU AUCC\nTransformer\nT Reservoir\nFFN Reservoir\n10 15 20 25 30\n# Updatable Encoder Layers\n0.94\n0.95\n0.96\n0.97\n0.98\n0.99\n1.00valid BLEU AUCC\nTransformer\nT Reservoir\nFFN Reservoir\n30 40 50 60 70\n# Updatable Decoder Layers\n0.6\n0.7\n0.8\n0.9\n1.0valid bpc AUCC\nTransformer\nT Reservoir\nFFN Reservoir\n2 4 6 8 10 12\n# Updatable Encoder Layers\n32.5\n33.0\n33.5\n34.0test BLEU \nTransformer\nT Reservoir\nFFN Reservoir\n10 15 20 25 30\n# Updatable Encoder Layers\n26.25\n26.50\n26.75\n27.00\n27.25\n27.50\n27.75\n28.00test BLEU \nTransformer\nT Reservoir\nFFN Reservoir\n30 40 50 60 70\n# Updatable Decoder Layers\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4test bpc \nTransformer\nT Reservoir\nFFN Reservoir\nFigure 1: Validation (top) and test (bottom) results for IWSLT (left), WMT (middle) and enwiki8 language mod-\nelling (right). IWSLT and WMT are BLEU (high is good); enwiki8 is BPC (low is good). Comparison of regular\ntransformer (blue) and reservoir transformer with FFN (green) or Transformer reservoir (orange) layers added.\n4.1 Machine Translation\nMachine translation (MT) is one of the core\ntasks of NLP. We demonstrate on two well-known\nMT datasets, IWSLT’14 German-English and\nWMT’16 English-German, that reservoir trans-\nformers obtain a better AUCC. For the raw vali-\ndation plots over time that were used to calculate\nthe AUCC, please refer to Appendix F.\nFollowing Kasai et al. (2020), the architecture\nof the network is an N-layer reservoir transformer\nencoder, followed by a regular shallow one- or\ntwo-layer decoder. This design choice has been\nshown to lead to very good speed and efﬁciency\ntrade-offs, and serves as a good baseline for our\nexperiments. Moreover, shallow decoders make it\neasier to decide where to place reservoir layers (in\nthe encoder) and makes it more straightforward to\nidentify where performance gains come from.\nFigure 1 shows the results for IWSLT (left) and\nWMT (middle). On the y-axis we show valida-\ntion AUCC for the BLEU metric; on the x-axis\nwe show the number of updatable layers in the en-\ncoder. The performance of a regular transformer\nencoder with 6 layers and a reservoir transformer\nencoder with 6 layers plus N additional reservoir\nlayers are plotted for the same x-axis value to\nshow the total number of updated layers. Plots\nfor the total number of layers (updatable plus not-\nupdatable, so essentially shifted versions of the\nplots) are shown in Appendix E.\nWMT is much larger and requires a much\ndeeper encoder, as illustrated by the fact that a\ncertain minimum depth is required for reservoir\ntransformers to achieve a comparable validation\nAUCC. At test time, reservoir transformers outper-\nform regular transformers for almost all encoder\ndepths. The FFN Reservoir seems to work best\nin both cases, which is surprising because it does\nnot have any self-attention component at all. This\nﬁnding shows that self-attention, or the mecha-\nnism to summarize context information, should be\nlearned if present. Once the context features have\nbeen gathered, a random projection via a ﬁxed\nFFN module appears to be beneﬁcial.\nTable 1 and 2 show the time it took to achieve\nthe maximum validation BLEU score and how that\nrelates to the regular transformer, demonstrating\nthat reservoir transformers consistently converge\nfaster in terms of wall-clock time. We save up\nto 22% convergence wall-clock time using reser-\nvoir transformers as much with the same number\nof updateable layers. We save as much as 27%\ntime until convergence a 24 layer model on WMT,\nas shown in Table 2. One other noticeable point\nis that we can see that the T Reservoir achieves\nsimilar performance to LayerDrop on IWSLT and\nWMT in terms of wall-clock per epoch and wall-\nclock time to the best performance. However, on\nboth tasks, FFN Reservoir performs much better\nthan LayerDrop in terms of efﬁciency per epoch\n4299\nModel # Layers Frozen Max BLEU Train time Ratio # Params Train Time each\nuntil max(in hours) Trainable(Total) epoch(in seconds)\nTransformer\n6 0 34.52 ±0.07 2.548 ±0.06 1 26.8M 122.73 ±1.16\n8 0 34.59 ±0.11 2.557 ±0.05 1 31.1M 142.28 ±1.87\n10 0 34.56 ±0.05 3.173 ±0.04 1 35.3M 161.66 ±1.54\n12 0 34.29 ±0.12 3.521 ±0.09 1 39.5M 172.45 ±1.98\nT Reservoir\n6 2 34.37 ±0.12 2.422 ±0.03 0.95 22.6M (26.8M) 120.59 ±1.32\n8 2 34.80 ±0.07 2.450 ±0.06 0.96 26.8M (31.1M) 134.49 ±1.76\n10 2 34.70 ±0.03 2.831 ±0.05 0.89 31.1M (35.3M) 144.42 ±1.98\n12 2 34.78 ±0.04 3.476 ±0.04 0.98 35.3M (39.5M) 159.43 ±1.67\nFFN Reservoir\n6 2 34.43 ±0.15 2.120 ±0.04 0.83 22.6M (25.8M) 107.71 ±1.73\n8 2 34.56 ±0.16 2.203 ±0.06 0.86 26.8M (29.1M) 120.07 ±1.65\n10 2 34.66 ±0.02 2.493 ±0.05 0.79 31.1M (33.3M) 130.11 ±1.43\n12 2 34.76 ±0.03 3.241 ±0.04 0.92 35.3M (37.5M) 156.32 ±1.87\nLayerDrop\n6 2 34.59 ±0.15 2.364 ±0.08 0.92 22.6M (26.8M) 119.30 ±1.36\n8 2 34.58 ±0.16 2.554 ±0.05 0.99 26.8M (31.1M) 138.62 ±1.44\n10 2 34.57 ±0.07 3.404 ±0.06 1.07 31.1M (35.3M) 140.88 ±1.62\n12 2 33.65 ±0.24 3.251 ±0.04 0.92 35.3M (39.5M) 160.85 ±1.49\nTable 1: Wall-clock time (averaged over multiple runs) saved for IWSLT for different model types and encoder\ndepths. Max BLEU is for validation. Number of layers is for encoder, decoder depth is kept ﬁxed at 2. The ratio\nis computed compared to the corresponding number of layers in the regular transformer case.\nModel # Layers Frozen Max BLEU Train time Ratio # Params Train Time each\nuntil max(in hours) Trainable(Total) epoch(in hours)\nTransformer\n12 0 24.46 ±0.04 15.15 ±0.15 1 75.6M 0.505 ±0.005\n16 0 24.52 ±0.03 16.05 ±0.18 1 88.2M 0.643 ±0.006\n24 0 24.69 ±0.05 17.61 ±0.85 1 113.4M 0.877 ±0.029\n32 0 24.83 ±0.04 18.42 ±0.28 1 138.6M 1.036 ±0.010\nT Reservoir\n12 4 24.26 ±0.08 14.11 ±0.21 0.93 72.4M (75.6M) 0.472 ±0.007\n16 4 24.50 ±0.05 15.25 ±0.28 0.95 75.6M (88.2M) 0.596 ±0.009\n24 4 25.11 ±0.07 15.89 ±0.74 0.90 100.8M (113.4M) 0.776 ±0.024\n32 4 24.66 ±0.04 16.38 ±0.24 0.88 126.0M (138.6M) 0.998 ±0.009\nFFN Reservoir\n12 4 24.42 ±0.05 14.01 ±0.09 0.92 72.4M (71.4M) 0.441 ±0.003\n16 4 24.65 ±0.07 14.53 ±0.17 0.91 75.6M (83.9M) 0.524 ±0.006\n24 4 24.93 ±0.04 12.62 ±1.53 0.71 100.8M (109.2M) 0.743 ±0.018\n32 4 24.98 ±0.03 13.96 ±0.19 0.73 126.0M (134.4M) 0.964 ±0.007\nLayerDrop\n12 4 24.27 ±0.03 14.61 ±0.14 0.96 72.4M (75.6M) 0.489 ±0.006\n16 4 24.15 ±0.06 15.55 ±0.54 0.97 75.6M (88.2M) 0.597 ±0.017\n24 4 24.37 ±0.05 16.25 ±0.36 0.92 100.8M (113.4M) 0.823 ±0.013\n32 4 23.84 ±0.03 15.27 ±0.38 0.83 126.0M (138.6M) 1.028 ±0.012\nTable 2: Wall-clock time (averaged over multiple runs) saved for WMT for different model types and encoder\ndepths. Decoder depth is kept ﬁxed at 1.\nand achieves better/similar performance in less\ntime in each case. As a point of reference, a half\nhour gain on IWSLT would translate to a gain of\nseveral days in the training of bigger transformer\nmodels like GPT-3 (Brown et al., 2020).\nWe observe that reservoir transformers consis-\ntently perform better than, or are competitive to,\nregular transformers, both in terms of validation\nBLEU AUCC as well as test time BLEU, for all\nexamined encoder depths.\n4.2 Language Modelling\nTo examine whether the same ﬁndings hold for\nother tasks, we evaluate on the enwiki8 (LLC,\n2009) language modelling task. We examine the\nBPC (bits per character) rate for a variety of net-\nwork depths (since the task is language modelling,\nthese layers are in the decoder). The results show\nthat except for the 64-layer regular transformer,\nwhich appears to be particularly optimal for this\ntask, we obtain consistently better BPC for all\ndepths. We observe similar trends during test time.\n4.3 Masked Language Model Pretraining\nWe train RoBERTa (Liu et al., 2019) models from\nscratch at a variety of depths, both in the normal\nand reservoir setting. We ﬁnd that these networks\nshow minor differences in their best perplexity\n4300\n4 6 8 10 12 14 16\n# Updatable Decoder Layers\n91\n92\n93\n94\n95\n96valid accuracy \nTransformer\nT Reservoir\nFFN Reservoir\nTransformer (frozen finetuned)\n4 6 8 10 12 14 16\n# Updatable Decoder Layers\n78\n80\n82\n84\n86valid accuracy \nTransformer\nT Reservoir\nFFN Reservoir\nTransformer (frozen finetuned)\nFigure 2: Downstream RoBERTa performance on SST-2 (left) and MultiNLI-matched (right).\nModel Max BLEU AUCC Train time\nTransformer 34.59 ±0.11 114.57±0.08 142.28±1.87\nT Reservoir 34.80 ±0.07 115.26±0.26 134.49±1.70\nBackskip Reservoir 34.75±0.05 115.99±0.23 119.54±1.78\nTable 3: Validation max BLEU, AUCC at 4h and wall-\nclock time per epoch (averaged over multiple runs, in\nseconds) on IWSLT comparing backskipping with reg-\nular and reservoir transformers.\nand similar AUCC perplexity (see Appendix D).\nWe then examine the performance of these models\nwhen ﬁne-tuned on downstream tasks, speciﬁcally\nthe well known SST-2 (Socher et al., 2013) and\nMultiNLI-matched (Williams et al., 2017) tasks.\nWhen ﬁne-tuning the reservoir models, we keep\nthe reservoir layers ﬁxed (also ﬁne-tuning them\ndid not work very well, see Appendix D).\nFigure 2 shows the results of ﬁne-tuning. We\nobserve that the reservoir transformer outperforms\nnormal RoBERTa at all depths in both tasks. At\nlower depth, the improvements are substantial. As\na sanity check, we also experiment with freez-\ning some of the layers in a regular pre-trained\nRoBERTa model during ﬁne-tuning only (Trans-\nformer “frozen ﬁnetuned” in the Figure) and show\nthat this helps a little but is still outperformed by\nthe reservoir transformer.\nThese ﬁndings suggest that we can train a\nRoBERTa model without updating all of the lay-\ners, achieving similar perplexity at a similar com-\nputational cost, but with better downstream per-\nformance. This strategy could prove to be beneﬁ-\ncial in a wide variety of pre-training scenarios.\nWe follow Jawahar et al. (2019) and inves-\ntigate what the frozen layers in the Reservoir\nTransformer have actually “learned” (while being\nfrozen) as measured by probing tasks, reported in\nTable 4. The set of tasks comprises one surface\ntask, three syntactic tasks, and ﬁve semantic tasks.\nFrom the table, we can see that generally prob-\ning performance is quite similar between Trans-\nformer and the T Reservoir model. We also no-\nticed that the representations collected after the\nreservoir layer (3, 5, 7, 9) in the T Reservoir ac-\ntually have signiﬁcantly better performance over\nthe regular Transformer representations across all\nthe probing tasks. Related to our ﬁndings, V oita\nand Titov (2020) show that the wholly-randomly-\ninitialized model representations can still have rea-\nsonable probing accuracy if they are contextual-\nized, though the accuracy is strictly worse than\na trained one. These ﬁndings raise interesting\nrepercussions for the study of “BERTology”, as\nit clearly shows that even completely random and\nfrozen layers can represent linguistic phenomena.\n4.4 Backskipping\nWith the reservoir transformers as described\nabove, we obtain better efﬁciency by skipping\nthe “gradient application” matrix addition step in\nsome of the layers (i.e., updating the weights).\nOne step further would be to investigate skip-\nping the entire backward pass for reservoirs al-\ntogether, which would save us from having to do\nthe much more expensive matrix multiplication for\nthese layers that is required for the propagation of\ngradients through a regular layer.\nWe report on preliminary experiments where in\nthe backward pass we replace the gradients for\nthe layer Li going into the reservoir Li+1 with a\nnoisy estimate (Jaderberg et al., 2017; Czarnecki\net al., 2017). Promisingly, Oktay et al. (2020) re-\ncently asked “why spend resources on exact gradi-\nents when we’re going to use stochastic optimiza-\ntion?” and show that we can do randomized auto-\ndifferentiation quite successfully.\n4301\nModel Layer SentLen TreeDepth TopConst BShift Tense SubjNum ObjNum SOMO CoordInv\n(Surface) (Syntactic) (Syntactic) (Syntactic) (Semantic) (Semantic) (Semantic) (Semantic) (Semantic)\nTransformer\n1 84.56 ±0.54 32.30±0.41 54.40±0.33 49.99±0.01 80.98±0.32 76.26±0.09 50.01±0.19 76.38±0.61 54.33±0.47\n2 87.22 ±0.07 33.63±0.57 58.38±0.20 50.12±0.17 82.84±0.68 78.65±0.19 51.47±0.53 78.00±1.12 54.66±0.55\n3 84.25 ±0.16 32.60±0.17 54.41±0.10 50.02±0.01 81.72±0.59 77.00±0.13 51.32±0.64 76.57±1.13 54.13±0.51\n4 87.37 ±0.20 32.59±0.29 50.06±0.21 69.76±0.26 81.63±1.17 76.47±0.09 52.41±1.49 76.15±0.84 52.62±1.34\n5 84.61 ±0.24 31.14±0.48 44.76±0.38 74.82±0.11 80.16±0.19 73.66±0.16 52.95±1.77 72.90±0.21 51.26±1.14\n6 82.56 ±0.25 30.31±0.40 39.30±0.40 78.80±0.38 81.88±0.47 75.30±0.07 56.21±1.26 74.37±0.16 51.44±1.04\n7 70.85 ±0.13 26.65±0.72 40.70±0.13 78.98±0.32 85.11±0.31 72.03±0.46 58.15±0.46 68.71±0.91 55.39±0.27\n8 66.23 ±1.33 23.46±0.44 25.19±1.02 77.42±0.27 80.35±0.45 67.55±0.99 54.94±2.04 63.69±2.32 50.58±0.83\n9 71.17 ±0.29 31.21±0.31 58.42±0.29 85.55±0.44 86.77±0.19 80.30±0.08 64.36±1.20 81.68±0.45 66.90±0.49\n10 73.19 ±0.50 27.74±0.53 41.01±0.22 83.56±0.96 86.13±0.35 83.04±0.04 62.01±0.59 79.73±0.21 62.60±1.04\n11 71.37 ±0.42 30.22±0.28 48.58±0.35 84.40±0.44 87.28±0.59 82.34±0.15 61.10±0.14 80.00±0.40 64.44±0.38\n12 71.66 ±0.12 33.43±0.18 64.38±0.20 87.38±0.02 88.41±0.09 84.46±0.25 63.01±0.05 81.80±0.27 65.72±0.16\nT Reservoir\n1 87.75 ±0.10 31.60±0.21 50.38±0.23 50.00±0.00 80.40±0.18 76.47±0.20 50.53±0.14 73.48±0.15 53.55±0.70\n2 81.28 ±0.23 34.20±0.41 61.41±0.42 60.64±0.65 81.50±0.77 76.33±0.08 50.73±0.34 74.28±0.67 56.82±0.10\n3 89.28 ±0.09 36.42±0.11 67.36±0.45 75.64±0.52 85.42±0.18 80.53±0.02 52.50±1.80 78.47±1.81 57.16±0.27\n4 74.31 ±0.32 32.42±0.83 55.19±0.33 73.41±0.00 79.56±0.00 75.15±0.08 53.68±0.66 75.02±0.19 56.89±0.08\n5 88.03 ±0.22 38.34±0.64 68.65±0.29 82.25±0.12 86.80±0.02 82.27±0.33 57.95±0.24 80.82±0.91 58.05±0.10\n6 74.55 ±0.37 33.13±0.29 52.70±0.81 79.21±0.13 85.70±0.36 77.43±0.03 57.26±0.19 75.38±0.66 51.95±1.30\n7 85.82 ±0.37 37.63±0.13 70.43±0.05 84.12±0.35 86.88±0.07 82.86±0.30 61.17±0.21 80.79±0.17 61.83±0.95\n8 71.69 ±0.71 30.32±0.01 48.44±0.30 79.12±0.12 84.75±0.09 79.23±0.11 59.53±0.16 76.80±0.41 57.34±0.14\n9 85.86 ±0.12 37.89±0.03 69.53±0.37 85.55±0.12 87.98±0.22 84.13±0.01 63.06±0.01 82.55±0.31 66.07±0.05\n10 69.22 ±0.23 25.58±0.35 29.20±0.58 78.57±0.09 85.02±0.03 75.68±0.16 57.55±1.57 74.70±0.02 55.02±0.64\n11 65.70 ±0.05 30.57±0.03 47.56±0.02 81.20±0.00 86.78±0.02 83.73±0.05 60.38±0.17 80.59±0.15 62.50±0.11\n12 70.61 ±0.18 34.45±0.20 64.19±0.10 84.53±0.03 87.48±0.16 84.86±0.14 62.75±0.14 82.08±0.03 64.73±0.06\nTable 4: RoBERTa Probing Results. The line in bold text are the the frozen layers in the T Reservoir. Mean\naccuracy with standard deviation, gathered over 3 random seeds.\nHere, rather than minimizing the actual gradi-\nents ∂Li\n∂θLi , we minimize their expectation and train\nvia continuous-action REINFORCE (Williams,\n1992). That is, Li becomes a policy πa: s →µ\nwhere we sample actions a ∼N (µ,1). We train\nto minimize the gradient prediction loss via MSE,\ni.e., 1\nn\n∑n\ni=0(Ri−Vi(a))2, and the REINFORCE\nloss Ea[log(a) (R−V(a))], where the value net-\nwork V acts as the baseline. R is deﬁned as the\nmean of the gradients of the top layer Li+2, with\nthe sign ﬂipped. Thus, simply put, we train to min-\nimize the expectation of the true gradients at the\nlayer directly following the reservoir. We employ\nan annealing scheme where we ﬁrst train the value\nnetwork and propagate the true gradients during\nwarmup. Afterwards, we anneal the probability\nof backskipping instead of doing a true backward\npass (multiplying the probability by 0.99 every it-\neration until we only backskip). We experimented\nwith setting Rto the negation of the total loss but\nfound the mean upstream gradient reward to work\nbetter. We call this approach backskipping.\nAs shown in Table 3, the backskip reservoir ap-\nproach leads to a higher maximum BLEU score\nthan the regular transformer, with a much higher\nAUCC and much lower training time. The en-\ncoder depth is 8 with 2 frozen. Appendix G shows\nthe raw validation BLEU curves over time. We\nobserve that this approach helps especially during\nthe earlier stages of training. This ﬁnding opens\nup intriguing possibilities for having parts of neu-\nral networks be completely frozen both in the for-\nward as well as in the backward pass, while still\ncontributing to the overall model computation.\nThe computational cost is heavily reduced given\nthat we completely bypass the expensive back-\npropagation computation in the reservoir layers.\nBackskipping is shown to be a promising approach\nto further reduce computational costs, and would\nbe even more efﬁcient from a hardware perspec-\ntive since the circuitry for such layers (which do\nnot need to propagate gradients) can be hardwired.\n5 Related Work\nRecent work has shown that modern NLP mod-\nels are able to function with different numbers\nof layers for different examples (Elbayad et al.,\n2019; Fan et al., 2019; He et al., 2021); that differ-\nent layers specialize for different purposes (Zhang\net al., 2019); that layers can be compressed (Li\net al., 2020; Zhu et al., 2019; Shen et al., 2020;\nSun et al., 2020); and, that layers can be re-\nordered (Press et al., 2019). There is a growing\nbody of work in efﬁcient self-attention networks\n(Tay et al., 2020b), such as linear attention (Wang\net al., 2020), on how to process long context infor-\nmation (Beltagy et al., 2020; Ainslie et al., 2020)\nand on approximations to make transformers more\nscalable (Kitaev et al., 2020; Katharopoulos et al.,\n2020). BigBIRD (Zaheer et al., 2020) provides\nrandom keys as additional inputs to its attention\nmechanism. Locality sensitive hashing (LSH) as\nemployed e.g. in Reformer (Kitaev et al., 2020)\nutilizes a ﬁxed random projection. Random Fea-\nture Attention (Peng et al., 2021) uses random fea-\n4302\nture methods to approximate the softmax function.\nPerformer (Choromanski et al., 2020) computes\nthe transformer’s multi-head attention weights as\na ﬁxed orthogonal random projection. Closely re-\nlated to this work, Tay et al. (2020a) showed that\nrandomized alignment matrices in their “Synthe-\nsizer” architecture are sufﬁcient for many NLP\ntasks. While these works focus on random atten-\ntion, we show that entire layers can be random\nand ﬁxed. We also show that entire layers can be\nreplaced by ﬁxed random projections that do not\nhave any attention whatsoever.\nBeyond transformers, random features have\nbeen extensively explored. Examples of this in-\nclude FreezeOut (Brock et al., 2017), deep reser-\nvoir computing networks (Scardapane and Wang,\n2017; Gallicchio and Micheli, 2017), as well as\napplications in domains as varied as text classiﬁ-\ncation (Conneau et al., 2017; Zhang and Bowman,\n2018; Wieting and Kiela, 2019) or music classiﬁ-\ncation (Pons and Serra, 2019). It is well known\nthat randomly initialized networks can display im-\npressive performance on their own (Ulyanov et al.,\n2018; Rosenfeld and Tsotsos, 2019; Ramanujan\net al., 2020; V oita and Titov, 2020), which under-\nlies, for example, the recently popularized lottery\nticket hypothesis (Frankle and Carbin, 2018; Zhou\net al., 2019). We know that learning deep over-\nparameterized networks appears to help in gen-\neral (Li and Liang, 2018; Du et al., 2019). Our\nmethod constitutes a way to add both depth and\nparameters to transformer networks without much\ncomputational cost.\n6 Conclusion\nThis work demonstrated that state-of-the-art trans-\nformer architectures can be trained without up-\ndating all of the layers. This complements a\nlong history in machine learning of harnessing\nthe power of random features. We use the “area\nunder the convergence curve” (AUCC) metric\nto demonstrate that on a variety of tasks, and\nin a variety of settings, “reservoir transformers”\nachieve better performance-efﬁciency trade-offs.\nWe show that such reservoir transformers show\nbetter convergence rates and test-set generaliza-\ntion. We demonstrated that the backward pass can\nbe skipped altogether, opening up exciting vanues\nfor future research. Future work includes further\ninvestigating hybrid networks and backskipping\nstrategies, as well as utilizing pruning.\nAcknowledgements\nWe thank Eric Wallace, Zhewei Yao, Kevin Lin,\nZhiqing Sun, Zhuohan Li, Angela Fan, Shaojie\nBai, and anonymous reviewers for their comments\nand suggestions. SS and KK were supported by\ngrants from Samsung, Facebook, and the Berke-\nley Deep Drive Consortium.\nReferences\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: Encoding long and structured inputs\nin transformers. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP).\nThomas Bachlechner, Bodhisattwa Prasad Majumder,\nHuanru Henry Mao, Garrison W Cottrell, and Ju-\nlian McAuley. 2020. Rezero is all you need:\nFast convergence at large depth. arXiv preprint\narXiv:2003.04887.\nAlexei Baevski, Steffen Schneider, and Michael Auli.\n2019. vq-wav2vec: Self-supervised learning of\ndiscrete speech representations. arXiv preprint\narXiv:1910.05453.\nEric B Baum. 1988. On the capabilities of multilayer\nperceptrons. Journal of complexity, 4(3):193–215.\nIz Beltagy, Matthew E Peters, and Arman Cohan.\n2020. Longformer: The long-document trans-\nformer. arXiv preprint arXiv:2004.05150.\nHans-Dieter Block. 1962. The perceptron: A model\nfor brain functioning. i. Reviews of Modern Physics,\n34(1):123.\nOndˇrej Bojar, Christian Buck, Christian Federmann,\nBarry Haddow, Philipp Koehn, Johannes Leveling,\nChristof Monz, Pavel Pecina, Matt Post, Herve\nSaint-Amand, Radu Soricut, Lucia Specia, and Aleˇs\nTamchyna. 2014. Findings of the 2014 workshop\non statistical machine translation. In Proceedings of\nthe Ninth Workshop on Statistical Machine Trans-\nlation, Baltimore, Maryland, USA. Association for\nComputational Linguistics.\nA Borsellino and A Gamba. 1961. An outline of a\nmathematical theory of papa. Il Nuovo Cimento\n(1955-1965), 20(2):221–231.\nAndrew P Bradley. 1997. The use of the area under\nthe roc curve in the evaluation of machine learning\nalgorithms. Pattern recognition, 30(7):1145–1159.\nAndrew Brock, Theodore Lim, James M Ritchie, and\nNick Weston. 2017. Freezeout: Accelerate train-\ning by progressively freezing layers. arXiv preprint\narXiv:1706.04983.\n4303\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve,\nNicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. 2020. End-to-end object detection with\ntransformers. arXiv preprint arXiv:2005.12872.\nM. Cettolo, J. Niehues, S. St ¨uker, L. Bentivogli, and\nMarcello Federico. 2015. Report on the 11 th iwslt\nevaluation campaign , iwslt 2014. In Proceedings of\nIWSLT.\nKyunghyun Cho, Bart Van Merri ¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using rnn encoder-decoder\nfor statistical machine translation. arXiv preprint\narXiv:1406.1078.\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Jared Davis, Tamas Sarlos,\nDavid Belanger, Lucy Colwell, and Adrian Weller.\n2020. Masked language modeling for proteins via\nlinearly scalable long-context transformers. arXiv\npreprint arXiv:2006.03555.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Loic\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. arXiv preprint\narXiv:1705.02364.\nThomas M Cover. 1965. Geometrical and statistical\nproperties of systems of linear inequalities with ap-\nplications in pattern recognition. IEEE transactions\non electronic computers, (3):326–334.\nWojciech Marian Czarnecki, Grzegorz ´Swirszcz, Max\nJaderberg, Simon Osindero, Oriol Vinyals, and Ko-\nray Kavukcuoglu. 2017. Understanding synthetic\ngradients and decoupled neural interfaces. arXiv\npreprint arXiv:1703.00522.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics , Florence, Italy. Association for\nComputational Linguistics.\nAmit Daniely, Roy Frostig, and Yoram Singer. 2016.\nToward deeper understanding of neural networks:\nThe power of initialization and a dual view on ex-\npressivity. In Advances In Neural Information Pro-\ncessing Systems, pages 2253–2261.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nSimon Du, Jason Lee, Haochuan Li, Liwei Wang, and\nXiyu Zhai. 2019. Gradient descent ﬁnds global min-\nima of deep neural networks. In International Con-\nference on Machine Learning, pages 1675–1685.\nSergey Edunov, Myle Ott, Michael Auli, David Grang-\nier, and Marc’Aurelio Ranzato. 2018. Classical\nstructured prediction losses for sequence to se-\nquence learning. In Proceedings of the 2018 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nMaha Elbayad, Jiatao Gu, Edouard Grave, and Michael\nAuli. 2019. Depth-adaptive transformer. arXiv\npreprint arXiv:1910.10073.\nJoseph Enguehard, Dan Busbridge, Vitalii Zhelezniak,\nand Nils Hammerla. 2019. Neural language priors.\narXiv preprint arXiv:1910.03492.\nAngela Fan, Edouard Grave, and Armand Joulin. 2019.\nReducing transformer depth on demand with struc-\ntured dropout. arXiv preprint arXiv:1909.11556.\nJonathan Frankle and Michael Carbin. 2018. The lot-\ntery ticket hypothesis: Finding sparse, trainable neu-\nral networks. arXiv preprint arXiv:1803.03635.\nJonathan Frankle, David J Schwab, and Ari S Mor-\ncos. 2020. Training batchnorm and only batchnorm:\nOn the expressive power of random features in cnns.\narXiv preprint arXiv:2003.00152.\nClaudio Gallicchio and Alessio Micheli. 2017. Echo\nstate property of deep reservoir computing networks.\nCognitive Computation, 9(3):337–350.\nClaudio Gallicchio and Simone Scardapane. 2020.\nDeep randomized neural networks. In Recent Trends\nin Learning From Data, pages 43–68. Springer.\nA. Gamba, L. Gamberini, G. Palmieri, and R. Sanna.\n1961. Further experiments with papa. Il Nuovo Ci-\nmento (1955-1965), 20(2):112–115.\nAnkush Garg, Yuan Cao, and Qi Ge. 2020. Echo\nstate neural machine translation. arXiv preprint\narXiv:2002.11847.\nRaja Giryes, Guillermo Sapiro, and Alex M Bronstein.\n2016. Deep neural networks with random gaussian\nweights: A universal classiﬁcation strategy? IEEE\nTransactions on Signal Processing , 64(13):3444–\n3457.\nXavier Glorot and Yoshua Bengio. 2010. Understand-\ning the difﬁculty of training deep feedforward neu-\nral networks. In Proceedings of the thirteenth in-\nternational conference on artiﬁcial intelligence and\nstatistics, pages 249–256.\n4304\nCaglar Gulcehre, Marcin Moczulski, Misha Denil, and\nYoshua Bengio. 2016. Noisy activation functions.\nIn International conference on machine learning ,\npages 3059–3068.\nFatemeh Hadaeghi, Xu He, and Herbert Jaeger. 2017.\nUnconventional Information Processing Systems,\nNovel Hardware: A Tour D’Horizon.\nChaoyang He, Shen Li, Mahdi Soltanolkotabi, and\nSalman Avestimehr. 2021. Pipetransformer: Auto-\nmated elastic pipelining for distributed training of\ntransformers. In ICML.\nKonstantin Hicke, Miguel Escalona-Moran, Daniel\nBrunner, Miguel Soriano, Ingo Fischer, and Claudio\nMirasso. 2013. Information processing using tran-\nsient dynamics of semiconductor lasers subject to\ndelayed feedback. Selected Topics in Quantum Elec-\ntronics, IEEE Journal of, 19:1501610–1501610.\nGuang-Bin Huang, Qin-Yu Zhu, and Chee-Kheong\nSiew. 2006. Extreme learning machine: theory and\napplications. Neurocomputing, 70(1-3):489–501.\nMax Jaderberg, Wojciech Marian Czarnecki, Simon\nOsindero, Oriol Vinyals, Alex Graves, David Sil-\nver, and Koray Kavukcuoglu. 2017. Decoupled neu-\nral interfaces using synthetic gradients. In Inter-\nnational Conference on Machine Learning , pages\n1627–1635. PMLR.\nHerbert Jaeger. 2003. Adaptive nonlinear system iden-\ntiﬁcation with echo state networks. In Advances in\nneural information processing systems.\nGanesh Jawahar, Beno ˆıt Sagot, and Djam ´e Seddah.\n2019. What does BERT learn about the structure\nof language? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics.\nKam Jim, Bill G Horne, and C Lee Giles. 1995. Effects\nof noise on convergence and generalization in recur-\nrent networks. In Advances in neural information\nprocessing systems, pages 649–656.\nKam-Chuen Jim, C Lee Giles, and Bill G Horne. 1996.\nAn analysis of noise in recurrent neural networks:\nconvergence and generalization. IEEE Transactions\non neural networks, 7(6):1424–1438.\nWilliam B Johnson and Joram Lindenstrauss. 1984.\nExtensions of lipschitz mappings into a hilbert\nspace. Contemporary mathematics, 26(189-206):1.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n2020. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361.\nJungo Kasai, Nikolaos Pappas, Hao Peng, James\nCross, and Noah A Smith. 2020. Deep encoder,\nshallow decoder: Reevaluating the speed-quality\ntradeoff in machine translation. arXiv preprint\narXiv:2006.10369.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and Franc ¸ois Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear at-\ntention. arXiv preprint arXiv:2006.16236.\nYoon Kim. 2014. Convolutional neural net-\nworks for sentence classiﬁcation. arXiv preprint\narXiv:1408.5882.\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. arXiv\npreprint arXiv:2001.04451.\nYann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick\nHaffner. 1998. Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE ,\n86(11):2278–2324.\nYuanzhi Li and Yingyu Liang. 2018. Learning overpa-\nrameterized neural networks via stochastic gradient\ndescent on structured data. In Advances in Neural\nInformation Processing Systems, pages 8157–8166.\nZhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin,\nKurt Keutzer, Dan Klein, and Joseph E Gonzalez.\n2020. Train large, then compress: Rethinking model\nsize for efﬁcient training and inference of transform-\ners. arXiv preprint arXiv:2002.11794.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nMultiMedia LLC. 2009. Large text compression\nbenchmark.\nMantas Lukoˇseviˇcius and Herbert Jaeger. 2009. Reser-\nvoir computing approaches to recurrent neural net-\nwork training. Computer Science Review, 3(3).\nWolfgang Maass, Thomas Natschl ¨ager, and Henry\nMarkram. 2002. Real-time computing without sta-\nble states: A new framework for neural computa-\ntion based on perturbations. Neural computation ,\n14(11):2531–2560.\nMarvin Minsky and Seymour A Papert. 2017. Percep-\ntrons: An introduction to computational geometry .\nMIT press.\nEmre O Neftci, Charles Augustine, Somnath Paul,\nand Georgios Detorakis. 2017. Event-driven ran-\ndom back-propagation: Enabling neuromorphic\ndeep learning machines. Frontiers in neuroscience,\n11:324.\nHyeonwoo Noh, Tackgeun You, Jonghwan Mun, and\nBohyung Han. 2017. Regularizing deep neural net-\nworks by noise: Its interpretation and optimization.\nIn Advances in Neural Information Processing Sys-\ntems, pages 5109–5118.\n4305\nDeniz Oktay, Nick McGreivy, Joshua Aduol, Alex\nBeatson, and Ryan P Adams. 2020. Random-\nized automatic differentiation. arXiv preprint\narXiv:2007.10412.\nMyle Ott, Sergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling neural machine trans-\nlation. arXiv preprint arXiv:1806.00187.\nYoh-Han Pao, Gwang-Hoon Park, and Dejan J Sobajic.\n1994. Learning and generalization characteristics of\nthe random vector functional-link net. Neurocom-\nputing, 6(2):163–180.\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy\nSchwartz, Noah Smith, and Lingpeng Kong. 2021.\nRandom feature attention. In International Confer-\nence on Learning Representations.\nJonathan Pilault, Jaehong Park, and Christopher Pal.\n2020. On the impressive performance of randomly\nweighted encoders in summarization tasks. arXiv\npreprint arXiv:2002.09084.\nJordi Pons and Xavier Serra. 2019. Randomly\nweighted cnns for (music) audio classiﬁcation.\nIn ICASSP 2019-2019 IEEE international confer-\nence on acoustics, speech and signal processing\n(ICASSP), pages 336–340. IEEE.\nOﬁr Press, Noah A Smith, and Omer Levy. 2019. Im-\nproving transformer models by reordering their sub-\nlayers. arXiv preprint arXiv:1911.03864.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2018. Language\nmodels are unsupervised multitask learners.\nAli Rahimi and Benjamin Recht. 2008. Random fea-\ntures for large-scale kernel machines. In Advances\nin neural information processing systems , pages\n1177–1184.\nAli Rahimi and Benjamin Recht. 2009. Weighted sums\nof random kitchen sinks: Replacing minimization\nwith randomization in learning. In Advances in\nneural information processing systems, pages 1313–\n1320.\nVivek Ramanujan, Mitchell Wortsman, Aniruddha\nKembhavi, Ali Farhadi, and Mohammad Rastegari.\n2020. What’s hidden in a randomly weighted neural\nnetwork? In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition ,\npages 11893–11902.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in bertology: What we know about\nhow bert works. arXiv preprint arXiv:2002.12327.\nAmir Rosenfeld and John K Tsotsos. 2019. Intriguing\nproperties of randomly weighted networks: Gener-\nalizing while learning next to nothing. In 2019 16th\nConference on Computer and Robot Vision (CRV) ,\npages 9–16. IEEE.\nMagnus Sahlgren. 2005. An introduction to random\nindexing. In Methods and applications of semantic\nindexing workshop at the 7th international confer-\nence on terminology and knowledge engineering.\nAndrew M Saxe, James L McClelland, and Surya Gan-\nguli. 2013. Exact solutions to the nonlinear dynam-\nics of learning in deep linear neural networks. arXiv\npreprint arXiv:1312.6120.\nSimone Scardapane and Dianhui Wang. 2017. Ran-\ndomness in neural networks: an overview. Wiley\nInterdisciplinary Reviews: Data Mining and Knowl-\nedge Discovery, 7(2):e1200.\nWouter F Schmidt, Martin A Kraaijveld, and\nRobert PW Duin. 1992. Feedforward neural net-\nworks with random weights. In Proceedings of the\n11th International Conference on Pattern Recogni-\ntion, 1992. Vol. II. Conference B: Pattern Recogni-\ntion Methodology and Systems, pages 1–4.\nBenjamin Schrauwen, Michiel D’Haene, David Ver-\nstraeten, and Jan Campenhout. 2007. Compact hard-\nware for real-time speech recognition using a liquid\nstate machine. pages 1097 – 1102.\nRoy Schwartz, Jesse Dodge, Noah A Smith, and\nOren Etzioni. 2019. Green ai. arXiv preprint\narXiv:1907.10597.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W Mahoney, and Kurt\nKeutzer. 2020. Q-bert: Hessian based ultra low\nprecision quantization of bert. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence , vol-\nume 34, pages 8815–8821.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing ,\npages 1631–1642.\nEmma Strubell, Ananya Ganesh, and Andrew Mc-\nCallum. 2019. Energy and policy considera-\ntions for deep learning in nlp. arXiv preprint\narXiv:1906.02243.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. Mobilebert: a\ncompact task-agnostic bert for resource-limited de-\nvices. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics ,\npages 2158–2170.\nGouhei Tanaka, Toshiyuki Yamane, Jean Benoit\nH´eroux, Ryosho Nakane, Naoki Kanazawa, Seiji\nTakeda, Hidetoshi Numata, Daiju Nakano, and\nAkira Hirose. 2019. Recent advances in physical\nreservoir computing: A review. Neural Networks,\n115:100 – 123.\n4306\nYi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan,\nZhe Zhao, and Che Zheng. 2020a. Synthesizer: Re-\nthinking self-attention in transformer models. arXiv\npreprint arXiv:2005.00743.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020b. Efﬁcient transformers: A survey.\narXiv preprint arXiv:2009.06732.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBert rediscovers the classical nlp pipeline. arXiv\npreprint arXiv:1905.05950.\nDmitry Ulyanov, Andrea Vedaldi, and Victor Lempit-\nsky. 2018. Deep image prior. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, pages 9446–9454.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nOriol Vinyals, Igor Babuschkin, Wojciech M. Czar-\nnecki, Micha ¨el Mathieu, Andrew Dudzik, Juny-\noung Chung, David H. Choi, Richard Powell, Timo\nEwalds, Petko Georgiev, Junhyuk Oh, Dan Horgan,\nManuel Kroiss, Ivo Danihelka, Aja Huang, Lau-\nrent Sifre, Trevor Cai, John P. Agapiou, Max Jader-\nberg, Alexander S. Vezhnevets, R ´emi Leblond, To-\nbias Pohlen, Valentin Dalibard, David Budden, Yury\nSulsky, James Molloy, Tom L. Paine, Caglar Gul-\ncehre, Ziyu Wang, Tobias Pfaff, Yuhuai Wu, Ro-\nman Ring, Dani Yogatama, Dario W ¨unsch, Katrina\nMcKinney, Oliver Smith, Tom Schaul, Timothy Lil-\nlicrap, Koray Kavukcuoglu, Demis Hassabis, Chris\nApps, and David Silver. 2019. Grandmaster level in\nStarCraft II using multi-agent reinforcement learn-\ning. Nature, 575(7782):350–354.\nElena V oita and Ivan Titov. 2020. Information-\ntheoretic probing with minimum description length.\nIn Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP), pages 183–196.\nSinong Wang, Belinda Li, Madian Khabsa, Han\nFang, and Hao Ma. 2020. Linformer: Self-\nattention with linear complexity. arXiv preprint\narXiv:2006.04768.\nJohn Wieting and Douwe Kiela. 2019. No training\nrequired: Exploring random encoders for sentence\nclassiﬁcation. arXiv preprint arXiv:1901.10444.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2017. A broad-coverage challenge corpus for\nsentence understanding through inference. arXiv\npreprint arXiv:1704.05426.\nRonald J Williams. 1992. Simple statistical gradient-\nfollowing algorithms for connectionist reinforce-\nment learning. Machine learning, 8(3-4):229–256.\nFelix Wu, Angela Fan, Alexei Baevski, Yann N\nDauphin, and Michael Auli. 2019. Pay less attention\nwith lightweight and dynamic convolutions. arXiv\npreprint arXiv:1901.10430.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\net al. 2020. Big bird: Transformers for longer se-\nquences. arXiv preprint arXiv:2007.14062.\nChiyuan Zhang, Samy Bengio, and Yoram Singer.\n2019. Are all layers created equal? arXiv preprint\narXiv:1902.01996.\nKelly Zhang and Samuel Bowman. 2018. Language\nmodeling teaches you more than translation does:\nLessons learned through auxiliary syntactic task\nanalysis. In Proceedings of the 2018 EMNLP\nWorkshop BlackboxNLP: Analyzing and Interpret-\ning Neural Networks for NLP.\nHattie Zhou, Janice Lan, Rosanne Liu, and Jason\nYosinski. 2019. Deconstructing lottery tickets: Ze-\nros, signs, and the supermask. In Advances in Neu-\nral Information Processing Systems , pages 3597–\n3607.\nWei Zhu, Xiaofeng Zhou, Keqiang Wang, Xun Luo,\nXiepeng Li, Yuan Ni, and Guotong Xie. 2019.\nPANLP at MEDIQA 2019: Pre-trained language\nmodels, transfer learning and knowledge distillation.\nIn Proceedings of the 18th BioNLP Workshop and\nShared Task, pages 380–388, Florence, Italy. Asso-\nciation for Computational Linguistics.\nA Hybrid Networks and\nNon-Transformer Reservoirs\nWe investigate whether reservoir layers need to\nbe transformer-based (or transformers-without-\nattention, i.e., FFN). We examine two different\nalternatives: bidirectional Gated Recurrent Units\n(Cho et al., 2014) and Convolutional Neural Net-\nworks (LeCun et al., 1998; Kim, 2014), specif-\nically light dynamical convolutions (Wu et al.,\n2019). Figure 3 shows the results for these hy-\nbrids: depending on the setting, they may obtain\na better AUCC than the regular transformer, but\nthis is less consistent than with the other reservoir\nlayers, most likely because these layers have dif-\nferent computational properties. It’s possible that\nthese hybrids simply require further tuning, as we\nfound e.g. up-projecting to help for BiGRUs, but\nstudying this is outside of the scope of the current\nwork.\n4307\nModel # Layers Frozen Max BLEU Train time Ratio # Params Train Time each\nuntil max(in hours) Trainable(Total) epoch(in seconds)\nTransformer\n6 0 34.97 ±0.05 1.984 ±0.02 1 39.5M 177.84 ±2.98\n8 0 34.99 ±0.08 2.161 ±0.03 1 43.7M 206.59 ±3.47\n10 0 34.98 ±0.04 2.345 ±0.02 1 47.9M 236.72 ±3.52\n12 0 34.78 ±0.11 2.535 ±0.05 1 52.0M 265.90 ±4.97\nT Reservoir\n6 2 34.73 ±0.11 1.838 ±0.01 0.92 35.3M (39.5M) 166.11 ±2.21\n8 2 35.07 ±0.05 1.912 ±0.03 0.88 39.5M (43.7M) 190.08 ±3.73\n10 2 35.02 ±0.01 1.970 ±0.04 0.84 43.7M (47.9M) 204.42 ±2.89\n12 2 35.06 ±0.02 2.429 ±0.02 0.95 47.8M (52.0M) 236.41 ±4.35\nFFN Reservoir\n6 2 34.85 ±0.10 1.729 ±0.03 0.87 35.3M (37.4M) 161.72 ±2.32\n8 2 34.99 ±0.11 1.751 ±0.02 0.81 39.5M (41.6M) 180.21 ±2.68\n10 2 34.92 ±0.03 1.907 ±0.02 0.81 43.7M (45.8M) 191.40 ±2.49\n12 2 35.16 ±0.04 2.395 ±0.01 0.94 47.8M (49.9M) 216.08 ±2.57\nLayerDrop\n6 2 34.51 ±0.12 1.908 ±0.04 0.96 35.3M (39.5M) 169.62 ±3.16\n8 2 34.77 ±0.11 2.023 ±0.02 0.94 39.5M (43.7M) 186.71 ±2.17\n10 2 34.06 ±0.05 1.912 ±0.02 0.97 43.7M (47.9M) 205.52 ±3.31\n12 2 34.08 ±0.13 2.524 ±0.01 0.99 47.8M (52.0M) 222.45 ±2.21\nTable 5: Wall-clock time (averaged over multiple runs) for IWSLT for different model types and encoder depths.\nMax BLEU is for validation. Number of layers is for encoder, decoder depth is kept ﬁxed at 6. Ratio is computed\ncompared to comparable number of layers in the normal case.\n2 4 6 8 10 12\n# Updatable Encoder Layers\n0.96\n0.97\n0.98\n0.99\n1.00valid BLEU AUCC\nTransformer\nT Reservoir\nFFN Reservoir\nGRU Reservoir\nConv Reservoir\n2 4 6 8 10 12\n# Updatable Encoder Layers\n32.0\n32.5\n33.0\n33.5\n34.0test BLEU \nTransformer\nT Reservoir\nFFN Reservoir\nGRU Reservoir\nConv Reservoir\nFigure 3: IWSLT comparison of different hybrid archi-\ntectures with different reservoir layers.\nB Deep Decoders\nWe show that the same results hold for a 6-layer\ndecoder on IWSLT (although less pronounced for\nAUCC, probably because the decoder is computa-\ntionally heavier). See Figure 4 and Table 5.\n2 4 6 8 10 12\n# Updatable Encoder Layers\n0.96\n0.97\n0.98\n0.99\n1.00valid BLEU AUCC\nTransformer\nT Reservoir\nFFN Reservoir\n2 4 6 8 10 12\n# Updatable Encoder Layers\n33.2\n33.4\n33.6\n33.8\n34.0\n34.2\n34.4\n34.6test BLEU \nTransformer\nT Reservoir\nFFN Reservoir\nFigure 4: IWSLT validation AUCC and test BLEU with\n6-layer decoder.\nC Freezing Strategy\nWe explored different strategies for the placement\nof reservoir layers and found the “alternating”\nstrategy reported in the main body of the paper to\nwork best. Generally, we found repetitive applica-\n2 4 6 8 10\n# Updatable Encoder Layers\n0.92\n0.93\n0.94\n0.95\n0.96\n0.97\n0.98\n0.99\n1.00valid BLEU AUCC\nTransformer\nAlter T Reservoir\nMid T Reservoir\nTop T Reservoir\nBottom T Reservoir\nFigure 5: IWSLT with 2-layer decoder using different\nfreezing strategies.\ntion of reservoirs to yield diminishing returns, as\nmight be expected. See Figure 5.\nD RoBERTa Results\nHere we present the additional results for\nRoBERTa , i.e., convergence plots and AUCCs for\nvarious depth settings, in Figure 7. As stated in the\nmain paper, the differences in terms of AUCC and\nconvergence between RoBERTa models with and\nwithout reservoir layers are limited. Moreover,\nwe plot downstream task performance for SST-2\nand MNLI compared to the pretraining wall-clock\ntime in Figure 6. It can be seen that the FFN Reser-\nvoir can achieve up to 25% and 10% pretraining\ntime savings while matching the best performance\n4308\nModel IWSLT-Dec2 IWSLT-Dec6 WMT-Dec1# Layers Train time Max BLEU # Layers Train time Max BLEU # Layers Train time Max BLEUuntil 95% max(in hours) (95%) until 95% max(in hours) (95%) until 95% max(in hours) (95%)\nTransformer\n6 0.647 ±0.03 32.89 ±0.04 6 0.642 ±0.02 33.36 ±0.03 12 3.788 ±0.053 23.36 ±0.068 0.711 ±0.05 33.04 ±0.03 8 0.765 ±0.03 33.41 ±0.08 16 3.820 ±0.072 23.41 ±0.0510 0.808 ±0.02 33.96 ±0.08 10 0.898 ±0.04 33.32 ±0.07 24 5.262 ±0.607 23.50 ±0.0312 1.037 ±0.03 33.07 ±0.09 12 1.037 ±0.03 33.07 ±0.11 32 6.212 ±0.232 23.81 ±0.04\nT Reservoir\n6 0.569 ±0.02 32.78 ±0.03 6 0.599 ±0.01 33.09 ±0.05 12 3.563 ±0.061 23.21 ±0.048 0.619 ±0.04 33.12 ±0.05 8 0.726 ±0.02 33.38 ±0.09 16 3.603 ±0.056 23.80 ±0.0610 0.729 ±0.04 33.13 ±0.07 10 0.738 ±0.03 33.37 ±0.04 24 4.923 ±0.771 23.75 ±0.0212 0.982 ±0.02 33.03 ±0.11 12 0.958 ±0.01 33.46 ±0.09 32 5.780 ±0.214 23.71 ±0.03\nFFN Reservoir\n6 0.521 ±0.05 32.85 ±0.02 6 0.594 ±0.03 33.13 ±0.04 12 3.417 ±0.046 23.22 ±0.078 0.533 ±0.03 33.84 ±0.04 8 0.651 ±0.04 33.36 ±0.06 16 3.527 ±0.063 23.54 ±0.0510 0.614 ±0.01 33.05 ±0.08 10 0.627 ±0.05 33.26 ±0.03 24 4.197 ±0.697 23.74 ±0.0612 0.811 ±0.02 33.26 ±0.10 12 0.780 ±0.02 33.46 ±0.08 32 4.984 ±0.321 23.82 ±0.02\nLayerDrop\n6 0.837 ±0.08 32.87 ±0.05 6 0.706 ±0.01 33.08 ±0.03 12 3.912 ±0.068 23.33 ±0.088 0.934 ±0.07 33.12 ±0.03 8 0.753 ±0.04 33.14 ±0.05 16 3.581 ±0.076 23.17 ±0.0410 0.901 ±0.06 33.18 ±0.02 10 0.691 ±0.03 32.39 ±0.05 24 4.875 ±0.728 23.43 ±0.0712 0.914 ±0.01 32.33 ±0.06 12 0.803 ±0.02 32.94 ±0.10 32 5.980 ±0.219 22.97 ±0.08\nTable 6: Wall-clock time (averaged over multiple runs) for IWSLT/WMT for different model types and encoder\ndepths. 95% Max BLEU is for validation.\nModel IWSLT-Dec2 IWSLT-Dec6 WMT-Dec1# Layers Train time Max BLEU # Layers Train time Max BLEU # Layers Train time Max BLEUuntil 99% max(in hours) (99%) until 99% max(in hours) (99%) until 99% max(in hours) (99%)\nTransformer\n6 1.454 ±0.06 34.24 ±0.05 6 1.297 ±0.03 34.69 ±0.05 12 9.961 ±0.053 24.27 ±0.048 1.475 ±0.09 34.32 ±0.09 8 1.390 ±0.02 34.75 ±0.09 16 12.623 ±0.072 24.35 ±0.0610 1.526 ±0.04 34.25 ±0.04 10 1.622 ±0.05 34.64 ±0.03 24 13.412 ±0.837 24.49 ±0.0712 2.259 ±0.07 34.24 ±0.11 12 1.748 ±0.01 34.66 ±0.08 32 15.117 ±0.232 24.56 ±0.02\nT Reservoir\n6 1.257 ±0.04 34.05 ±0.09 6 1.291 ±0.03 34.51 ±0.10 12 8.314 ±0.062 24.15 ±0.068 1.472 ±0.06 34.47 ±0.05 8 1.339 ±0.03 34.80 ±0.04 16 9.221 ±0.073 24.41 ±0.0510 1.530 ±0.03 34.36 ±0.02 10 1.419 ±0.04 34.72 ±0.03 24 10.413 ±0.580 24.56 ±0.0312 2.043 ±0.05 34.53 ±0.07 12 1.642 ±0.02 34.87 ±0.02 32 11.465 ±0.227 24.49 ±0.01\nFFN Reservoir\n6 1.138 ±0.03 34.10 ±0.13 6 1.169 ±0.02 34.71 ±0.09 12 7.407 ±0.087 24.33 ±0.088 1.101 ±0.07 34.32 ±0.11 8 1.201 ±0.03 34.79 ±0.08 16 9.336 ±0.036 24.42 ±0.0510 1.281 ±0.01 34.36 ±0.03 10 1.276 ±0.03 34.63 ±0.03 24 9.978 ±0.546 24.91 ±0.0712 1.785 ±0.03 34.42 ±0.06 12 1.440 ±0.01 34.87 ±0.02 32 10.524 ±0.341 24.96 ±0.01\nLayerDrop\n6 1.363 ±0.05 34.58 ±0.14 6 1.253 ±0.01 34.42 ±0.10 12 8.372 ±0.059 24.17 ±0.048 1.468 ±0.03 34.50 ±0.12 8 1.244 ±0.04 34.44 ±0.09 16 9.741 ±0.043 23.93 ±0.0810 1.678 ±0.04 34.52 ±0.07 10 1.343 ±0.04 33.83 ±0.06 24 10.145 ±0.628 24.07 ±0.0912 2.071 ±0.02 33.45 ±0.23 12 1.423 ±0.02 33.97 ±0.12 32 10.168 ±0.329 23.81 ±0.03\nTable 7: Wall-clock time (averaged over multiple runs) saved for IWSLT/WMT for different model types and\nencoder depths. 99% Max BLEU is for validation.\nof vanilla transformers for MNLI-m and SST2, re-\nspectively.\n10 20 30 40 50 60\nPretraining Wall-clock Time\n78\n79\n80\n81\n82\n83\n84\n85Accuray on MNLI-m\nTransformer\nT Reservoir\nFFN Reservoir\n10 20 30 40 50 60\nPretraining Wall-clock Time\n91\n92\n93\n94\n95Accuray on SST2\nTransformer\nT Reservoir\nFFN Reservoir\nFigure 6: RoBERTa Reservoir Results, Pre-training\nversus downstream task plot for 12 layer RoBERTa.\nMNLI-m (left). SST-2 (right).\nE Reservoir Results for Total Layers\nHere we present the shifted Reservoir Results for\nIWSLT14, WMT16, Enwik8 and RoBERTa ﬁne-\ntuning in Figure 8, 9, 10, 11, respectively. We\nshow the same results also hold when it comes to\nreplace normal transformer blocks with Reservoir\nblocks at least for MT.\n0 12 24 36 48 60\n Training Hours (h)  \n4\n6\n8\n10\n12\n14\n16\n18\n20Validation PPL\nTransformer\nT Reservoir\nFFN Reservoir\n4 6 8 10 12 14 16\n# Updatable Decoder Layers\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\n1.000Valid PPL AUCC\nTransformer\nT Reservoir\nFFN Reservoir\nFigure 7: RoBERTa Reservoir Results, Training plot\nfor 12 layer RoBERTa (left). AUCC result (right).\nF Validation Plots\nHere we present the validation plots for train-\ning a 8-layer encoder, 2-layer decoder model\nfor IWSLT14, a 24-layer encoder, 1-layer de-\ncoder model for WMT16, a 48-layer decoder\nmodel for enwik8 and a 12-layer decoder model\nfor RoBERTa for detailed steps to calculate the\nAUCC. It can be clearly observed that given the\nconﬁgurations from Section 3.1, all the models\nhave converged. So when we compute the area un-\nder the convergence curve, this depicts the training\nefﬁciency of the model (basically time x perfor-\nmance) until convergence. Speciﬁcally, we set T\n4309\n2 4 6 8 10 12\n# Total Encoder Layers\n0.96\n0.97\n0.98\n0.99\n1.00valid BLEU AUCC\nTransformer\nT Reservoir\nFFN Reservoir\n2 4 6 8 10 12\n# Total Encoder Layers\n32.5\n33.0\n33.5\n34.0test BLEU \nTransformer\nT Reservoir\nFFN Reservoir\nFigure 8: Validation BLEU AUCC and test BLEU for\nIWSLT (high is good). Comparison of regular trans-\nformer and reservoir transformer with FFN or Trans-\nformer reservoir layers added.\n12.5 15.0 17.5 20.0 22.5 25.0 27.5 30.0 32.5\n# Total Encoder Layers\n0.94\n0.95\n0.96\n0.97\n0.98\n0.99\n1.00valid BLEU AUCC\nTransformer\nT Reservoir\nFFN Reservoir\n12.5 15.0 17.5 20.0 22.5 25.0 27.5 30.0 32.5\n# Total Encoder Layers\n26.25\n26.50\n26.75\n27.00\n27.25\n27.50\n27.75\n28.00test BLEU \nTransformer\nT Reservoir\nFFN Reservoir\nFigure 9: Validation BLEU AUCC and test BLEU for\nWMT (high is good). Comparison of regular trans-\nformer and reservoir transformer with FFN or Trans-\nformer reservoir layers added.\nsufﬁciently high for computing the AUCC, which\nis 4h for IWSLT, 20h for WMT, 30h for enwik8\nand 60h for RoBERTa pretraning. From the train-\ning plot in the appendix, we can see that each\nmodel has converged at that point. The Reser-\nvoir model in Figure 12 has 2 layers frozen for\nIWSLT14, 8 layers frozen for enwik8, and 4 lay-\ners frozen for WMT16 and RoBERTa.\nG Backskipping\nFigure 13 shows the BLUE curves for IWSLT\ncomparing regular vs reservoir vs backskipped\ntransformers, with the latter performing surpris-\ningly well.\n30 40 50 60 70\n# Total Decoder Layers\n0.6\n0.7\n0.8\n0.9\n1.0valid bpc AUCC\nTransformer\nT Reservoir\nFFN Reservoir\n30 40 50 60 70\n# Total Decoder Layers\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4test bpc \nTransformer\nT Reservoir\nFFN Reservoir\nFigure 10: Validation BPC AUCC and test BPC on the\nenwik8 language modelling task (low is good). Com-\nparison of regular and reservoir transformers for vary-\ning depths.\n4 6 8 10 12 14 16 18 20\n# Total Decoder Layers\n91\n92\n93\n94\n95\n96valid accuracy \nTransformer\nT Reservoir\nFFN Reservoir\nTransformer (frozen finetuned)\n4 6 8 10 12 14 16 18 20\n# Total Decoder Layers\n78\n80\n82\n84\n86valid accuracy \nTransformer\nT Reservoir\nFFN Reservoir\nTransformer (frozen finetuned)\nFigure 11: Downstream RoBERTa performance on\nSST-2 (left) and MultiNLI-matched (right).\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5\n Training Hours (h)  \n10\n15\n20\n25\n30\n35Validation BLEU\nTransformer\nT Reservoir\nFFN Reservoir\n0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0\n Training Hours (h)  \n10\n12\n14\n16\n18\n20\n22\n24\n26Validation BLEU\nTransformer\nT Reservoir\nFFN Reservoir\n0 5 10 15 20 25 30 35 40\n Training Hours (h)  \n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0Validation BPC\nValidation curve for training on enwik8\nTransformer\nT Reservoir\nFFN Reservoir\n0 12 24 36 48 60\n Training Hours (h)  \n4\n6\n8\n10\n12\n14\n16\n18\n20Validation PPL\nTransformer\nT Reservoir\nFFN Reservoir\nFigure 12: IWSLT with 2-layer decoder validation plot\n(upper left). WMT with 24-layer decoder validation\nplot (upper right). Enwik8 with 48-layer decoder val-\nidation plot (lower left). RoBERTa with 12-layer de-\ncoder validation plot (lower right).\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5\n Training Hours (h)  \n10\n15\n20\n25\n30\n35Validation BLEU\nValidation curve for training on IWSLT14\nTransformer\nT Reservoir\nBackskipped Reservoir\nFigure 13: IWSLT comparison of the regular, reser-\nvoir and backskipped transformer architectures (en-\ncoder has 8 layers with 2 frozen, if any).",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5108230113983154
    },
    {
      "name": "Computer science",
      "score": 0.4695596992969513
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.4218023121356964
    },
    {
      "name": "Library science",
      "score": 0.3823677897453308
    },
    {
      "name": "Engineering",
      "score": 0.30837592482566833
    },
    {
      "name": "Electrical engineering",
      "score": 0.20224526524543762
    },
    {
      "name": "Physics",
      "score": 0.1255294382572174
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I134446601",
      "name": "Berkeley College",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    }
  ],
  "cited_by": 18
}