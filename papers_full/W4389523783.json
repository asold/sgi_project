{
    "title": "Cabbage Sweeter than Cake? Analysing the Potential of Large Language Models for Learning Conceptual Spaces",
    "url": "https://openalex.org/W4389523783",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2250249616",
            "name": "Usashi Chatterjee",
            "affiliations": [
                "Cardiff University"
            ]
        },
        {
            "id": "https://openalex.org/A2899443481",
            "name": "Amit Gajbhiye",
            "affiliations": [
                "Cardiff University"
            ]
        },
        {
            "id": "https://openalex.org/A2189566537",
            "name": "Steven Schockaert",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2771134746",
        "https://openalex.org/W2078894097",
        "https://openalex.org/W4288265479",
        "https://openalex.org/W3034723486",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W810401179",
        "https://openalex.org/W4297411851",
        "https://openalex.org/W3022006665",
        "https://openalex.org/W4280496127",
        "https://openalex.org/W1796264214",
        "https://openalex.org/W4246139359",
        "https://openalex.org/W4206808166",
        "https://openalex.org/W2945764361",
        "https://openalex.org/W4226095990",
        "https://openalex.org/W4303647983",
        "https://openalex.org/W3199748991",
        "https://openalex.org/W4378508569",
        "https://openalex.org/W4317797582",
        "https://openalex.org/W2110981949",
        "https://openalex.org/W1893804550",
        "https://openalex.org/W3211686893",
        "https://openalex.org/W2075395879",
        "https://openalex.org/W2050482109"
    ],
    "abstract": "The theory of Conceptual Spaces is an influential cognitive-linguistic framework for representing the meaning of concepts. Conceptual spaces are constructed from a set of quality dimensions, which essentially correspond to primitive perceptual features (e.g. hue or size). These quality dimensions are usually learned from human judgements, which means that applications of conceptual spaces tend to be limited to narrow domains (e.g. modelling colour or taste). Encouraged by recent findings about the ability of Large Language Models (LLMs) to learn perceptually grounded representations, we explore the potential of such models for learning conceptual spaces. Our experiments show that LLMs can indeed be used for learning meaningful representations to some extent. However, we also find that fine-tuned models of the BERT family are able to match or even outperform the largest GPT-3 model, despite being 2 to 3 orders of magnitude smaller.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 11836–11842\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nCabbage Sweeter than Cake? Analysing the Potential of Large Language\nModels for Learning Conceptual Spaces\nUsashi Chatterjee, Amit Gajbhiye, Steven Schockaert\nCardiffNLP, Cardiff University, UK\n{chatterjeeu,gajbhiyea,schockaerts1}@cardiff.ac.uk\nAbstract\nThe theory of Conceptual Spaces is an influ-\nential cognitive-linguistic framework for repre-\nsenting the meaning of concepts. Conceptual\nspaces are constructed from a set of quality\ndimensions, which essentially correspond to\nprimitive perceptual features (e.g. hue or size).\nThese quality dimensions are usually learned\nfrom human judgements, which means that ap-\nplications of conceptual spaces tend to be lim-\nited to narrow domains (e.g. modelling colour\nor taste). Encouraged by recent findings about\nthe ability of Large Language Models (LLMs)\nto learn perceptually grounded representations,\nwe explore the potential of such models for\nlearning conceptual spaces. Our experiments\nshow that LLMs can indeed be used for learn-\ning meaningful representations to some extent.\nHowever, we also find that fine-tuned models\nof the BERT family are able to match or even\noutperform the largest GPT-3 model, despite\nbeing 2 to 3 orders of magnitude smaller.1\n1 Introduction\nConceptual spaces (Gärdenfors, 2000) represent\nconcepts in terms of cognitively meaningful fea-\ntures, called quality dimensions. For example, a\nconceptual space of colour is composed of three\nquality dimensions, representing hue, saturation\nand intensity. Conceptual spaces provide an elegant\nframework for explaining various cognitive and\nlinguistic phenomena (Gärdenfors, 2014). Within\nArtificial Intelligence (AI), the role of conceptual\nspaces is essentially to act as an intermediate rep-\nresentation layer, in between neural and symbolic\nrepresentations (Gärdenfors, 2004). As such, con-\nceptual spaces could play a central role in the devel-\nopment of explainable AI systems. Unfortunately,\nsuch representations are difficult to learn from data.\n1Our datasets and evaluation scripts are available\nat https://github.com/ExperimentsLLM/EMNLP2023_\nPotentialOfLLM_LearningConceptualSpace.\nMost applications of conceptual spaces are thus lim-\nited to narrow domains, where meaningful repre-\nsentations can be learned from ratings provided by\nhuman participants (Paradis, 2015; Zwarts, 2015;\nChella, 2015).\nIn this paper, we explore whether Large Lan-\nguage Models (LLMs) could be used for learn-\ning conceptual spaces. This research question is\nclosely related to the ongoing debate about the ex-\ntent to which Language Models (LMs) can learn\nperceptually grounded representations (Bender and\nKoller, 2020; Abdou et al., 2021; Patel and Pavlick,\n2022; Søgaard, 2023). Recent work seems to sug-\ngest this might indeed be possible, at least for the\ncolour domain. For instance, Abdou et al. (2021)\nfound that LMs are able to learn representations of\ncolour terms which are isomorphic to perceptual\ncolour spaces. When it comes to predicting the\ntypical colour of objectes, Paik et al. (2021) found\nthat the predictions of LMs are heavily skewed\nby surface co-occurrence statistics, which are un-\nreliable for colours due to reporting bias (Gor-\ndon and Durme, 2013), i.e. the fact that obvious\ncolours are rarely mentioned in text. However, Liu\net al. (2022a) found the effects of reporting bias\nto largely disappear in recent LLMs. These find-\nings suggest that it may now be possible to distill\nmeaningful conceptual space representations from\nLLMs, as long as sufficiently large models are used.\nHowever, existing analyses are limited in two ways:\n• Several works have explored the colour do-\nmain, and visual domains more generally (Li\net al., 2023), but little is known about the abil-\nities of LLMs in other perceptual domains.\n• Most analyses focus on classifying concepts,\ne.g. predicting colour terms or the materials\nfrom which objects are made, rather than on\nevaluating the underlying quality dimensions.\nWe address the first limitation by including an eval-\nuation in the taste domain. To address the second\n11836\nlimitation, rather than considering discrete labels\n(e.g. sweet), we use LLMs to rank concepts accord-\ning to the degree to which they have a particular\nfeature (e.g. sweetness).\n2 Datasets\nThe primary focus of our experiments is on the\ntaste domain, which has not yet been considered\nin this context, despite having a number of impor-\ntant advantages. For instance, the relevant quality\ndimensions are well-established and have a linear\nstructure (unlike hue in the colour domain). This\ndomain also seems particularly challenging, as the\ntypical terms which are used to describe taste only\napply to extreme cases. For instance, we can assert\nthat grapefruit is bitter and bananas are sweet, but\nit is less clear how a language model would learn\nwhether chicken is sweeter than cheese. As ground\ntruth, we rely on the ratings that were collected\nby Martin et al. (2014). They rated a total of 590\nfood items along six dimensions: sweet, salty, sour,\nbitter, umami and fat. The ratings were obtained by\na panel of twelve assessors who were experienced\nin sensory profiling. They scored the food items\nduring an eight month measurement phase, after\nhaving received 55 hours of training in a labora-\ntory. We manually rephrased some of the names of\nthe items in this dataset, to make them more natu-\nral. For instance, cherry (fresh fruit) was changed\nto cherry and hake (grilled with lemon juice) was\nchanged to grilled hake with lemon juice.\nWe complement our analysis in the taste domain\nwith experiments on three basic physical domains:\nmass, size and height. These were found to be\nparticularly challenging by Li et al. (2023), with\nLLMs often failing to outperform random guessing.\nAs the ground truth for mass, we use the house-\nhold dataset from Standley et al. (2017), which\nspecifies the mass of 56 household objects. The\noriginal dataset includes images of each object.\nWe removed 7 items which were not meaningful\nwithout the image, namely big elephant, small ele-\nphant, Ivan’s phone, Ollie the monkey, Marshy the\nelephant, boy doll and Dali Clock, resulting in a\ndataset of 49 objects. We treat this problem as a\nranking problem. Li et al. (2023) also created a\nbinary classification version of this dataset, which\ninvolves judging pairwise comparisons (e.g. is a\nred lego brick heavier than a hammer?). For size\nand height, we use the datasets created by Liu et al.\n(2022b). These size and height datasets each con-\nsist of 500 pairwise judgements (e.g. an ant is larger\nthan a bird). Note that unlike for the other datasets,\nno complete ranking is provided.\n3 Methods\nWe experiment with a number of different models.\nRanking with GPT-3 We use GPT-3 models\nof four different sizes2: ada, babbage, curie and\ndavinci. To rank items according to a given dimen-\nsion, we use a prompt that contains the name of\nthat dimension as the final word, e.g. for sweetness\nwe could use “It is known that [food item] tastes\nsweet”. We then use the probability of this final\nword, conditioned on the rest of the prompt, to rank\nthe item: the higher the probability of sweet, the\nmore we assume the item to be sweet.\nPairwise Comparisons with GPT-3To predict\npairwise judgements, we consider two approaches.\nFirst, we again use conditional probabilities. For\ninstance, to predict whether an ant is larger than\na bird, we would get the conditional probability of\nlarge in the sentences an ant is large and a bird is\nlarge. If the conditional probability we get from\nthe first sentence is lower than the probability from\nthe second sentence, we would predict that the\nclaim that an ant is larger than a bird is false. Sec-\nond, we use a prompt that asserts the statement\nto be true (e.g. “An ant is larger than a bird”) and\na prompt that asserts the opposite (e.g. “A bird is\nlarger than an ant”). We compute the perplexity\nof both statements and predict the version with the\nlowest perplexity to be the correct one.\nRanking with ChatGPT and GPT-4ChatGPT\nand GPT-4 are more difficult to use than GPT-\n3 because the OpenAI API does not allow us to\ncompute conditional probabilities for these mod-\nels. Instead, to use these conversational models,\nwe directly ask them to rank a set of items, using a\nprompt such as: Rank the following items accord-\ning to their size, from the largest to the smallest ,\nfollowed by a list of items to be ranked.\nBaseline: DeBERTa We consider two baselines.\nFirst, we use a DeBERTa-v3-large model (He et al.,\n2021), which we fine-tuned to predict the com-\nmonsense properties of concepts. To this end, we\n2The exact model sizes have not been made public,\nbut were estimated to be 350M parameters for ada, 1.3B\nparameters for babbage, 6.7B parameters for curie and\n175B parameters for davinci: https://blog.eleuther.ai/\ngpt3-model-sizes/.\n11837\nused the extended McRae dataset (McRae et al.,\n2005) introduced by Forbes et al. (2019) and the\naugmented version of CSLB3 introduced by Misra\net al. (2022). Together, these two datasets contain\n19,410 positive and 31,901 negative examples of\n(concept,property) pairs. We fine-tune the model\non these examples using the following prompt: can\nconcept be described as property? <MASK> .\nFor instance, for the example (banana,yellow), the\ncorresponding prompt would be: can banana be\ndescribed as yellow? <MASK> . The probability\nthat the concept has the property is then predicted\nusing a linear classifier that takes the final-layer\nembedding of the <MASK> token as input. We use\nthe resulting model for the different evaluations,\nwithout any further fine-tuning.\nBaseline: Bi-encoder As the second baseline,\nwe use two variants of the bi-encoder model from\nGajbhiye et al. (2022). First, we use the original\nBERT-large model from Gajbhiye et al. (2022) that\nwas trained on data from Microsoft Concept Graph\n(Ji et al., 2019) and GenericsKB (Bhakthavatsalam\net al., 2020). However, as these training sets are not\nspecifically focused on commonsense knowledge,\nwe used ChatGPT to construct a dataset of 109K\n(concept,property) pairs, since no existing dataset\nof sufficient size and quantity was available. The\nkey to obtain high-quality examples was to ask the\nmodel to suggest properties that are shared by sev-\neral concepts, and to vary the examples that were\nprovided as part of a few-shot prompting strategy.\nMore details on how we collected this dataset us-\ning ChatGPT are provided in Appendix A. We then\ntrained the BERT-large bi-encoder on this dataset.\n4 Experiments\nTaste Domain Table 1 summarises the main re-\nsults on the taste dataset. For this experiment, each\nmodel was used to produce a ranking of the 590\nfood items, which was then compared with the\nground truth in terms of Spearman’s rank corre-\nlation (ρ). For the LMs, we experimented with\ntwo prompts. Prompt 1 is of the form “[food item]\ntastes [sweet]”, e.g. “apple tastes sweet”. Prompt 2\nis of the form “it is known that [food item] tastes\n[sweet]”. For the DeBERTa model and the bi-\nencoders, we verbalised the sweetness property\nas “tastes sweet”, and similar for the others. The\nresults in Table 1 show a strong correlation, for\n3https://cslb.psychol.cam.ac.uk/propnorms\nSweet\nSalty\nSour\nBitter\nUmami\nFatty\nPROMPT 1 Ada 17.5 8.5 12.2 16.4 22.5 10.7\nBabbage 19.5 51.1 20.2 22.0 22.6 16.0\nCurie 36.0 46.3 32.8 23.2 22.6 31.7\nDavinci 55.0 63.2 33.3 27.2 57.0 52.0\nPROMPT 2 Ada 23.1 11.9 8.5 16.8 -6.4 9.9\nBabbage 27.9 55.7 19.3 23.9 29.7 34.0\nCurie 35.4 47.9 30.3 22.7 25.4 37.5\nDavinci 50.2 54.4 34.6 28.3 49.8 42.1\nDeBERTa 69.1 67.0 43.9 24.7 34.4 64.0\nBi-encMSCG+GKB 27.4 -4.4 15.4 14.4 -11.8 12.6\nBi-encChatGPT 60.3 47.1 40.4 9.0 40.7 40.2\nTable 1: Ranking using conditional probability (Spear-\nman ρ%). Prompt1: “[food item] tastes [property]”.\nPrompt 2: “it is known that [food item] tastes [prop-\nerty]”.\nItem Gold Davinci\nCracker with Nutella spread 5 324\nChocolate with nut 15 201\nSweet pancake with maple syrup 34 279\nFruit cake 41 265\nSweet cookies with chocolate 60 233\nCracker with jam 67 268\nCooked bell pepper 252 8\nRedcurrant 282 36\nRadish 431 97\nMascarpone cheese 477 38\nCooked green cabbage 512 49\nSaint-agur Cheese 584 61\nTable 2: Qualitative analysis of the predictions by\ndavinci for sweetness, using prompt 1. The table shows\nthe rank positions of several food items, when ranking\nthe items from the sweetest (rank 1) to the least sweet\n(rank 590), according to the ground truth and the predic-\ntions obtained with the Davinci model.\nthe GPT-3 models, between model size and perfor-\nmance, with the best results achieved by davinci.\nSimilar as was observed for the colour domain\nby Liu et al. (2022a), there seems to be a quali-\ntative change in performance between LLMs such\nas davinci and smaller models. While there are\nsome differences between the two prompts, similar\npatterns are observed for both choices. ChatGPT\nand GPT-4 were not able to provide a ranking of\nthe 590 items, and are thus not considered for this\nexperiment. We also tried ChatGPT on a subset of\n50 items, but could not achieve results which were\nconsistently better than random shuffling.\nSurprisingly, we find that the DeBERTa model\noutperforms davinci in most cases, and often by\na substantial margin. This is despite the fact that\n11838\nMass Mass Height Size\nρ Acc Acc Acc\nCOND . PROB . Ada 23.0 47.8 68.7 59.1\nBabbage 48.9 80.9 67.9 76.4\nCurie 30.6 65.1 77.6 86.4\nDavinci 36.2 76.8 76.4 80.4\nPERPLEXITY\nAda - 49.0 49.7 36.5\nBabbage - 55.0 59.1 66.7\nCurie - 56.6 43.3 45.5\nDavinci - 70.8 54.7 51.1\nRANK\nChatGPT† 28.6 68.3 89.9 84.3\nGPT-4† 58.6 84.9 99.1 99.1\nDeBERTa -8.9 42.8 86.6 93.9\nBi-encMSCG+GKB 31.1 69.2 69.7 71.9\nBi-encChatGPT 11.8 67.6 77.2 60.3\nTable 3: Results for physical properties, viewed as a\nranking problem (mass) and as a pairwise judgment\nproblem (mass, height and size). Prompt: “In terms of\n[mass/height/size], it is known that a typical [concept]\nis [heavy/tall/large]”. Results with † required manual\npost-processing of predictions.\nthe model is 2 to 3 orders of magnitude smaller.\nFurthermore, we can see a large performance gap\nbetween the two variants of the bi-encoder model,\nwith the model trained on ChatGPT examples out-\nperforming curie, and even davinci in two cases.\nTable 2 presents some examples of the predic-\ntions that were made by davinci for sweetness (us-\ning prompt 1), comparing the ranks according to\nthe ground truth (gold) with the ranks according to\nthe davinci predictions. The table focuses on some\nof the most egregious mistakes. As can be seen,\ndavinci fails to identify the sweetness of common\nfoods such as chocolate, fruit cake and jam. Con-\nversely, the model significantly overestimates the\nsweetness of different cheeses and vegetables.\nPhysical Properties Table 3 summarises the re-\nsults for the physical properties. For mass, we con-\nsider both the problem of ranking all objects, evalu-\nated using Spearman ρ%, and the problem of evalu-\nating pairwise judgments, evaluated using accuracy.\nHeight and size can only be evaluated in terms of\npairwise judgments. To obtain conditional proba-\nbilities from the GPT-3 models, we used a prompt\nof the form “In terms of [mass/height/size], it is\nknown that a typical [concept] is [heavy/tall/large]”.\nWe also tried a few variants, which performed\nworse. To compute perplexity scores, for evalu-\nating pairwise judgements, we used a prompt of\nthe form “[concept 1] is heavier/taller/larger than\n[concept 2]”. For the baselines, we obtained scores\nfor the properties heavy, tall and large.\nThe correlation between model size and per-\nformance is far from obvious here, except that\nada clearly underperforms the three larger mod-\nels. However, among the GPT-3 models,babbage\nactually achieves the best results in several cases.\nThe results based on conditional probabilities are\nconsistently better than those based on perplexity.\nChatGPT and GPT-4 were difficult to use with the\nranking prompt, as some items were missing, some\nwere duplicated, and many items were paraphrased\nin the ranking. The results in Table 3 were obtained\nafter manually correcting these issues. With this\ncaveat in mind, it is nonetheless clear that GPT-4\nperforms exceptionally well in this experiment. In\naccordance with our findings in the taste domain,\nDeBERTa performs very well on the height and\nsize properties, outperforming all GPT-3 models\nby a clear margin. For mass, however, DeBERTa\nfailed completely, even achieving a negative cor-\nrelation. The bi-encoder models perform well on\nheight and size, although generally underperform-\ning the largest GPT-3 models. For mass, the bi-\nencoder trained on ChatGPT examples performs\npoorly, while the model trained on Microsoft Con-\ncept Graph and GenericsKB was more robust. It is\nnotable that the results in Table 3 are considerably\nhigher than those obtained by Li et al. (2023) using\nOPT (Zhang et al., 2022). For mass, for instance,\neven the largest OPT model (175B) was not able to\ndo better than random guessing.\nIn Table 3, the pairwise judgments about mass\nwere assessed by predicting the probability of the\nword heavy (for the GPT-3 models) or by predict-\ning the probability that the property heavy was\nsatisfied (for the baselines). Another possibility is\nto use the word/property light instead, or to com-\nbine the two probabilities. Let us write pheavy\nto denote the probability obtained for heavy (i.e.\nthe conditional probability of the word, as pre-\ndicted by the language model, or the probability\nthat the property is satisfied, as predicted by a base-\nline model), and similar for plight. Then we can\nalso predict the relative mass of items based on\nthe value pheavy · (1 − plight) or based on the\nvalue pheavy/plight. These different possibilities\nare evaluated in Table 4. As can be seen, there is\nno variant that consistently outperforms the others.\nAnalysis of Training Data OverlapFor the base-\nlines, we may wonder to what extent their knowl-\nedge comes from the pre-trained language model,\n11839\nada babbage curie davinci DeBERTa Bi-enc MSCG+GKB Bi-encChatGPT\npheavy 46.6 81.7 64.7 76.8 67.7 69.2 42.9\n1 − plight 49.8 51.3 41.5 53.4 43.4 61.9 53.4\npheavy · (1 − plight) 46.6 81.7 64.7 76.8 47.8 69.2 48.1\npheavy/plight 61.4 68.3 52.1 65.9 65.2 75.7 46.5\nTable 4: Analysis of alternative strategies for predicting pairwise judgements about mass (accuracy).\nBitter Sour Mass Height Size\nρ ρ ρ Acc Acc\nFull training 24.7 43.9 -8.9 86.6 93.9\nFiltered training 24.8 35.0 30.7 82.0 90.8\nTable 5: Comparison of the DeBERTa model in two\nsettings: the full training setting, where the McRae and\nCSLB datasets are used for fine-tuning, and a filtered\nsetting, where relevant properties are omitted.\nand to what extent it has been injected during the\nfine-tuning step. For this analysis, we focus in\nparticular on the DeBERTa model, which was fine-\ntuned on the McRae and CSLB datasets. These\ndatasets indeed cover a number of physical prop-\nerties, as well as some properties from the taste\ndomain. Table 5 summarises how the performance\nof the DeBERTa model is affected when removing\nthe most relevant properties from the McRae and\nCSLB training sets, which we refer to as filtered\ntraining in the table. For instance, for the property\nbitter, in the filtered setting we omit all training\nexamples involving the properties “bitter\" and “can\nbe bitter in taste”; for sour we remove the prop-\nerties “sour” and “can be sour in taste”; for mass\nwe remove the properties “heavy\", “light\", “light\nweight\" and “can be lightweight\"; for height we\nremove the properties “short\", “can be short\", “tall\"\nand “can be tall\"; and for size we remove the prop-\nerties “large\" and “small\". Note that the McRae\nand CSLB datasets do not cover any properties that\nare related to sweetness, saltiness, umami and fat-\ntiness. The results in Table 5 show that filtering\nthe training data indeed has an effect on results,\nalthough the performance of the model overall re-\nmains strong. Interestingly, in the case of mass, the\nfiltered setting leads to clearly improved results.\n5 Conclusions\nWe proposed the use of a dataset from the taste\ndomain for evaluating the ability of LLMs to learn\nperceptually grounded representations. We found\nthat LLMs can indeed make meaningful predic-\ntions about taste, but also showed that a fine-tuned\nDeBERTa model, and in some cases even a fine-\ntuned BERT-large bi-encoder, can outperform GPT-\n3. The performance of these smaller models cru-\ncially depends on the quality of the available train-\ning data. For this reason, we explored the idea of\ncollecting training data from ChatGPT, using a new\nprompting strategy. We complemented our exper-\niments in the taste domain with an evaluation of\nphysical properties, where we achieved consider-\nably better results than those reported in the litera-\nture (Li et al., 2023). Whereas previous work was\nessentially aimed at understanding the limitations\nof language models, our focus was more practical,\nasking the question: can high-quality conceptual\nspace representations be distilled from LLMs? Our\nexperiments suggest that the answer is essentially\npositive, but that new approaches may be needed\nto optimally take advantage of the knowledge that\ncan be extracted from such models.\nLimitations\nIt is difficult to draw definitive conclusions about\nthe extent to which cognitively meaningful rep-\nresentations can be obtained by querying LLMs.\nAmong others, previous work has found that per-\nformance may dramatically differ depending on the\nprompt which is used; see e.g. (Liu et al., 2022a).\nWe have attempted to make reasonable choices\nwhen deciding on the considered prompts, through\ninitial experiments with a few variations, but clearly\nthis is not a guarantee that our prompts are close\nto being optimal. However, this also reinforces\nthe conclusion that LLMs are difficult to use di-\nrectly for learning conceptual spaces. While we be-\nlieve that taste represents an interesting and under-\nexplored domain, it remains to be verified to what\nextent LLMs are able to capture perceptual features\nin other domains.\nAcknowledgments This work was supported by\nEPSRC grant EP/V025961/1.\n11840\nReferences\nMostafa Abdou, Artur Kulmizev, Daniel Hershcovich,\nStella Frank, Ellie Pavlick, and Anders Søgaard.\n2021. Can language models encode perceptual struc-\nture without grounding? a case study in color. In\nProceedings of the 25th Conference on Computa-\ntional Natural Language Learning, pages 109–132,\nOnline. Association for Computational Linguistics.\nEmily M. Bender and Alexander Koller. 2020. Climbing\ntowards NLU: On meaning, form, and understanding\nin the age of data. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5185–5198, Online. Association for\nComputational Linguistics.\nSumithra Bhakthavatsalam, Chloe Anastasiades, and\nPeter Clark. 2020. Genericskb: A knowledge base of\ngeneric statements. CoRR, abs/2005.00660.\nAntonio Chella. 2015. A Cognitive Architecture for Mu-\nsic Perception Exploiting Conceptual Spaces, pages\n187–203. Springer International Publishing, Cham.\nMaxwell Forbes, Ari Holtzman, and Yejin Choi. 2019.\nDo neural language representations learn physical\ncommonsense? In Proceedings of the 41th Annual\nMeeting of the Cognitive Science Society, CogSci\n2019: Creativity + Cognition + Computation, Mon-\ntreal, Canada, July 24-27, 2019, pages 1753–1759.\ncognitivesciencesociety.org.\nAmit Gajbhiye, Luis Espinosa-Anke, and Steven\nSchockaert. 2022. Modelling commonsense proper-\nties using pre-trained bi-encoders. In Proceedings of\nthe 29th International Conference on Computational\nLinguistics, pages 3971–3983, Gyeongju, Republic\nof Korea. International Committee on Computational\nLinguistics.\nPeter Gärdenfors. 2000. Conceptual spaces - the geom-\netry of thought. MIT Press.\nJonathan Gordon and Benjamin Van Durme. 2013. Re-\nporting bias and knowledge acquisition. In Proceed-\nings of the 2013 workshop on Automated knowledge\nbase construction, AKBC@CIKM 13, San Francisco,\nCalifornia, USA, October 27-28, 2013, pages 25–30.\nACM.\nPeter Gärdenfors. 2004. Conceptual spaces as a frame-\nwork for knowledge representation. Mind and Mat-\nter, 2(2):9–27.\nPeter Gärdenfors. 2014. The Geometry of Meaning:\nSemantics Based on Conceptual Spaces . The MIT\nPress.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning. CoRR, abs/2111.09543.\nLei Ji, Yujing Wang, Botian Shi, Dawei Zhang,\nZhongyuan Wang, and Jun Yan. 2019. Microsoft\nconcept graph: Mining semantic concepts for short\ntext understanding. Data Intell., 1(3):238–270.\nLei Li, Jingjing Xu, Qingxiu Dong, Ce Zheng, Qi Liu,\nLingpeng Kong, and Xu Sun. 2023. Can language\nmodels understand physical concepts? CoRR,\nabs/2305.14057.\nFangyu Liu, Julian Eisenschlos, Jeremy Cole, and Nigel\nCollier. 2022a. Do ever larger octopi still amplify\nreporting biases? evidence from judgments of typical\ncolour. In Proceedings of the 2nd Conference of the\nAsia-Pacific Chapter of the Association for Compu-\ntational Linguistics and the 12th International Joint\nConference on Natural Language Processing (Vol-\nume 2: Short Papers), pages 210–220, Online only.\nAssociation for Computational Linguistics.\nXiao Liu, Da Yin, Yansong Feng, and Dongyan Zhao.\n2022b. Things not written in text: Exploring spatial\ncommonsense from visual signals. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 2365–2376, Dublin, Ireland. Association for\nComputational Linguistics.\nChristophe Martin, Michel Visalli, Christine Lange, Pas-\ncal Schlich, and Sylvie Issanchou. 2014. Creation of\na food taste database using an in-home “taste” profile\nmethod. Food Quality and Preference, 36:70–80.\nKen McRae, George S Cree, Mark S Seidenberg, and\nChris McNorgan. 2005. Semantic feature production\nnorms for a large set of living and nonliving things.\nBehavior research methods, 37(4):547–559.\nKanishka Misra, Julia Taylor Rayz, and Allyson Et-\ntinger. 2022. A property induction framework for\nneural language models. CoRR, abs/2205.06910.\nCory Paik, Stéphane Aroca-Ouellette, Alessandro Ron-\ncone, and Katharina Kann. 2021. The World of an\nOctopus: How Reporting Bias Influences a Language\nModel’s Perception of Color. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 823–835, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nCarita Paradis. 2015. Conceptual Spaces at Work in\nSensory Cognition: Domains, Dimensions and Dis-\ntances, pages 33–55. Springer International Publish-\ning, Cham.\nRoma Patel and Ellie Pavlick. 2022. Mapping language\nmodels to grounded conceptual spaces. In Interna-\ntional Conference on Learning Representations.\nAnders Søgaard. 2023. Grounding the vector space of\nan octopus: Word meaning from raw text. Minds\nMach., 33(1):33–54.\nTrevor Standley, Ozan Sener, Dawn Chen, and Silvio\nSavarese. 2017. image2mass: Estimating the mass\nof an object from its image. In 1st Annual Confer-\nence on Robot Learning, CoRL 2017, Mountain View,\n11841\nCalifornia, USA, November 13-15, 2017, Proceed-\nings, volume 78 of Proceedings of Machine Learning\nResearch, pages 324–333. PMLR.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona T. Diab, Xian Li, Xi Victoria Lin,\nTodor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shus-\nter, Daniel Simig, Punit Singh Koura, Anjali Srid-\nhar, Tianlu Wang, and Luke Zettlemoyer. 2022.\nOPT: open pre-trained transformer language mod-\nels. CoRR, abs/2205.01068.\nJoost Zwarts. 2015. Conceptual Spaces, Features, and\nWord Meanings: The Case of Dutch Shirts , pages\n57–78. Springer International Publishing, Cham.\nA Collecting Concept-Property Pairs\nusing ChatGPT\nTo obtain training data for the bi-encoder model\nusing ChatGPT, we used the following prompt:\nI am interested in knowing which prop-\nerties are satisfied by different concepts.\nI am specifically interested in properties,\nsuch as being green, being round, be-\ning located in the kitchen or being used\nfor preparing food, rather than in hyper-\nnyms. For instance, some examples of\nwhat I’m looking for are: 1. Sunflower,\ndaffodil, banana are yellow 2. Guitar,\nbanjo, mandolin are played by strum-\nming or plucking strings 3. Pillow, blan-\nket, comforter are soft and provide com-\nfort 4. Car, scooter, train have wheels\n5. Tree, log, paper are made of wood 6.\nStudy, bathroom, kitchen are located in\nhouse. Please provide me with a list of\n50 such examples.\nWe repeated this request with the same prompt\naround 10 times. After this, we changed the ex-\namples that are given (shown in bold above). This\nprocess was repeated until we had 287K concept-\nproperty pairs. After removing duplicates, a total\nof 109K such pairs remained. We found it was nec-\nessary to regularly change the examples provided\nto ensure the examples were sufficiently diverse\nand to avoid having too many duplicates. These ex-\namples were constructed manually, to ensure their\naccuracy and diversity. Asking for more than 50\nexamples with one prompt became sub-optimal, as\nthe model tends to focus on a narrow set of similar\nproperties when the list becomes too long.\nTo verify the quality of the generated dataset, we\nmanually inspected 500 of the generated concept-\nproperty pairs. In this sample, we identified 6 er-\nrors, which suggests that this dataset is of sufficient\nquality for our intended purpose. Compared to re-\nsources such as ConceptNet, the main limitation\nof the ChatGPT generated dataset is that it appears\nto be less diverse, in terms of the concepts and\nproperties which are covered. We leave a detailed\nanalysis of this dataset for future work.\nB Issues with ChatGPT and GPT-4\nFor the experiments in Table 3, we used Chat-\nGPT and GPT-4 to rank 49, 25 and 26 unique\nobjects according to their mass, height and size\nrespectively. The prompt used was as follows:\n“Rank the following objects based on their typical\n[mass/height/size], from [heaviest to the lightest/\ntallest to the shortest/ largest to the smallest]\", fol-\nlowed by a list of the items. We could not directly\nevaluate the responses of ChatGPT and GPT-4 be-\ncause of the following issues:\n• Missing objects: For instance, GPT-4 ranked\n48 out of 49 objects and ChatGPT ranked 46\nout of 49 objects respectively, according to\ntheir mass.\n• Paraphrasing: While ranking, both GPT-4 and\nChatGPT changed some of the the names of\nthe objects. For instance, “wooden train track\"\nwas renamed as \"wooden train track piece\",\n“gardening shears\" was renamed as \"garden\nshears\".\n• Duplicates: GPT-4 and ChatGPT both occa-\nsionally introduced duplicates in the list.\nTo address these issues, we removed the dupli-\ncates and appended the missing items at the end of\nthe ranking. We manually corrected the modified\nnames.\n11842"
}