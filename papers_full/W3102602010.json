{
    "title": "Multitask Learning of Negation and Speculation using Transformers",
    "url": "https://openalex.org/W3102602010",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2227930789",
            "name": "Aditya Khandelwal",
            "affiliations": [
                "Savitribai Phule Pune University"
            ]
        },
        {
            "id": "https://openalex.org/A3000401080",
            "name": "Benita Kathleen Britto",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W782030620",
        "https://openalex.org/W3031105340",
        "https://openalex.org/W1588558467",
        "https://openalex.org/W3000132966",
        "https://openalex.org/W133539977",
        "https://openalex.org/W2566847560",
        "https://openalex.org/W3103291281",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W1961993270",
        "https://openalex.org/W2968189201",
        "https://openalex.org/W2169028566",
        "https://openalex.org/W2032021697",
        "https://openalex.org/W2159230276",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2250777749",
        "https://openalex.org/W2171660026",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2043335066",
        "https://openalex.org/W1910766207",
        "https://openalex.org/W2905530341",
        "https://openalex.org/W2975195127",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2149147504",
        "https://openalex.org/W2086817924"
    ],
    "abstract": "Detecting negation and speculation in language has been a task of considerable interest to the biomedical community, as it is a key component of Information Extraction systems from Biomedical documents. Prior work has individually addressed Negation Detection and Speculation Detection, and both have been addressed in the same way, using 2 stage pipelined approach: Cue Detection followed by Scope Resolution. In this paper, we propose Multitask learning approaches over 2 sets of tasks: Negation Cue Detection & Speculation Cue Detection, and Negation Scope Resolution & Speculation Scope Resolution. We utilise transformer-based architectures like BERT, XLNet and RoBERTa as our core model architecture, and finetune these using the Multitask learning approaches. We show that this Multitask Learning approach outperforms the single task learning approach, and report new state-of-the-art results on Negation and Speculation Scope Resolution on the BioScope Corpus and the SFU Review Corpus.",
    "full_text": "Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis, pages 79–87\nNovember 20, 2020.c⃝2020 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17\n79\nMultitask Learning of Negation and Speculation using Transformers\nAditya Khandelwal\nCollege of Engineering Pune\nPune, India\nkhandelwalar16.comp\n@coep.ac.in\nBenita Kathleen Britto\nVeermata Jijabai Technological Institute\nMumbai, India\nbcbritto b16\n@it.vjti.ac.in\nAbstract\nDetecting negation and speculation in lan-\nguage has been a task of considerable in-\nterest to the biomedical community, as it is\na key component of Information Extraction\nsystems from Biomedical documents. Prior\nwork has individually addressed Negation De-\ntection and Speculation Detection, and both\nhave been addressed in the same way, using\na 2 stage pipelined approach: Cue Detection\nfollowed by Scope Resolution. In this pa-\nper, we propose Multitask learning approaches\nover 2 sets of tasks: Negation Cue Detection\n& Speculation Cue Detection, and Negation\nScope Resolution & Speculation Scope Res-\nolution. We utilise transformer-based archi-\ntectures like BERT, XLNet and RoBERTa as\nour core model architecture, and ﬁnetune these\nusing the Multitask learning approaches. We\nshow that this Multitask Learning approach\noutperforms the single task learning approach,\nand report new state-of-the-art results on Nega-\ntion and Speculation Scope Resolution on the\nBioScope Corpus and the SFU Review Cor-\npus.\n1 Introduction\nDetection of linguistic phenomena like Negation\nand Speculation are key components of Biomedical\nInformation Retrieval systems, as they signiﬁcantly\nalter the meaning of a sentence. While detecting\nthese are also useful in Sentiment Analysis systems,\nand systems used to determine the veracity of infor-\nmation, their primary use is in biomedical systems.\nThus, these tasks have attracted signiﬁcant inter-\nest from researchers over the years, and due to the\nsimilarity between these tasks, similar approaches\nhave been used to address them, and parallel cor-\npora containing annotations for both Negation and\nSpeculation have also been created, including:\n• BioScope Corpus (Szarvas et al., 2008)\n• SFU Review Corpus (Konstantinova et al.)\nPrior research has converged to using a 2 stage\napproach for both Negation Detection and Specu-\nlation Detection: Cue Detection and Scope Reso-\nlution, and solved each task independently. These\nsubtasks and their relevance to the Biomedical do-\nmain can be better understood using the following\nexample:\n(1) We found that T cells were [not] present,\n[perhaps] indicating an immuno deﬁciency.\nCue Detection involves ﬁnding the word(s) that\nexpress the linguistic phenomena being detected.\nIn the example given above, not is the negation\ncue, as it expresses the negation in the sentence.\nSimilarly, perhaps is the speculation cue.\nScope Resolution involves ﬁnding the word(s)\nthat were affected by the cue word of the linguis-\ntic phenomena being considered. In the example\nabove, the underlined words outline the scope for\neach cue. Speciﬁcally, for the negation cue not, the\nword present was negatively affected by it. Sim-\nilarly, for the speculation cue perhaps, the words\nindicating an immuno deﬁciency were affected by\nit, indicating that these words have an associated\nuncertainty.\nThe approaches addressing these tasks have var-\nied signiﬁcantly over the years, with recent work\nfocusing on using transformer-based architectures\nto perform transfer learning, and have given the\nbest results to date on Scope Resolution. On Cue\nDetection, they yield the best performance among\nneural models, but due to the small dataset sizes,\nthe best performance is still given by rule-based\nheuristic approaches.\nDespite the similarity among the subtasks, prior\nsystems have looked at these tasks independently.\nWe believe that a system can improve performance\non both tasks by learning from both simultaneously,\ndue to the similarity, which is what Multitask Learn-\ning is about.\nMultitask Learning involves jointly training the\nsame architecture to perform multiple tasks. It\n80\nrelies on the concept of using shared knowledge\nbetween both tasks, which is what the model is\nforced to learn to perform well at all tasks, even-\ntually leading to better performance on all tasks.\nFor neural models, this is especially useful, as the\nlower layers can share the same input representa-\ntion, thus getting more data to learn better lower\nlevel features from the input, and a task speciﬁc\nlayer ﬁnal layer can learn the task speciﬁc features.\nIn this paper, inspired by the success of trans-\nformers, we propose a method to perform Multi-\ntask Learning of negation and speculation using\ntransformer-based architectures. We explore a few\ndesign choices, and analyse the impact of these de-\nsign choices. We show that our approach provides\nsigniﬁcant beneﬁts over the normal independently\ntrained version. We also make all our code publicly\navailable1 . This paper is structured as follows: Sec-\ntion 2 contains a brief Literature Review, Section\n3 describes the Methodology in detail, Section 4\ntalks about the Experimentation Details, Section 5\ncontains the Results and their Analysis, and Section\n6 contains the Conclusion and Future Scope.\n2 Literature Review\nOver the years, methods addressing these subtasks\nhave ranged from simple whitelists based on fre-\nquency (rule-based), to traditional Machine Learn-\ning algorithms like SVMs, neural models like BiL-\nSTMS and transformer based models.\nKhandelwal and Sawant (2020) provide an exten-\nsive literature review of the methods for Negation\nCue Detection and Scope Resolution. For Spec-\nulation Cue Detection, most methods used were\nsimilar to the methods used for Negation Cue De-\ntection. Below, we summarise a few papers that\naddressed Speculation Cue Detection and Scope\nResolution.\n2.1 Traditional Machine Learning Methods\n¨Ozg¨ur and Radev (2009) used a Support Vector\nMachine (SVM) with a linear kernel to detect spec-\nulation cues. Once the speculation cues were iden-\ntiﬁed, the parts-of-speech tags and the syntactic\nstructure were used for scope resolution.\nMorante and Daelemans (2009) used the\nIGTREE classiﬁer with the help of gain ratio\n(TiMBL implementation) to classify the cues. For\nscope resolution, three classiﬁers were used to clas-\nsify if a token was the ﬁrst token in the scope se-\n1adityak6798.github.io\nquence (F-scope), the last (L-scope), or neither.\nThese three classiﬁers were Memory-based learn-\ning (as implemented in TiMBL), SVM and Condi-\ntional Random Field (CRF). A fourth classiﬁer, the\nmetalearner, used the output of the three classiﬁers\nto predict the scope classes.\nVelldal et al. (2010) used a Maximum Entropy\nClassiﬁcation approach to ﬁnd the speculation cue.\nFor scope resolution, they used a rule-based ap-\nproach. The rules operated on the dependency\nstructure of the parser (MaltParser and XLE).\nKilicoglu and Bergler (2008) used linguistic\nknowledge to detect speculation cues. This was\nachieved by using a semi-automatic lexical acqui-\nsition strategy as well as by using a dictionary\nof weighted speculation cues. In a follow-up pa-\nper (Kilicoglu and Bergler, 2010), they improved\non their previous work with the help of vague-\nness quantiﬁers and syntactic dependency relations.\nCues were detected with rules that operate on lexi-\ncal information and syntactic information obtained\nfrom the Stanford Lexical Parser. The scopes of the\ncues were detected with the help of the Stanford\nLexical Parser and dependency-based heuristics.\nVelldal (2011) compiled a list of words that were\nobserved to be cues in the training data, under the\nassumption that speculation cues can be treated\nas a closed class. He then checked occurrences\nof these words in the test data via a large-margin\nSVM classiﬁer to determine whether they were a\ncue or not.\nA CRF based approach was used in (Tang et al.,\n2010) to identify the hedge cues and their scopes in\nsentences. A CRF and large margin-based model\nwere trained simultaneously. Their outputs were\nprovided to another CRF model to get the cues\nof sentences. These cues were passed to another\nCRF model followed by post processing, to detect\nscopes of the given cues.\nMorante et al. (2010) described a memory based\napproach (IGTree as implemented in TiMBL) for\ncue detection. For scope resolution, a memory-\nbased approach was used with the help of syntactic\ndependencies of a sentence.\nRead et al. (2011) described a methodology to\nresolve the scope of a sentence using an SVM based\nconstituent ranker. The scope of a cue was assumed\nto be a constituent. Three broad classes of rules\nwere used to extract features from the parse trees.\nThe parse trees containing the cue were fed to the\nSVM-based ranker to output a ranked order of parse\n81\ntrees which was then declared to be the scope of\nthe cue.\nVelldal et al. (2012) used an SVM classiﬁer\nbased on manually deﬁned rules for speculation\ncue detection. For scope resolution, they experi-\nmented with three architectures: a rule-based sys-\ntem that used Data Driven Dependency Parsing to\ngenerate dependency structures, an SVM Ranker\nfor selecting subtrees in the constituent structures\nobtained via a Grammar-Driven Phrase Structure\nParser and hybrid of both the above systems.\nThe approach of (Moncecchi et al., 2012) was\nbased on CRF and usage of domain knowledge.\nThe task of cue detection was solved by using the\nsequential classiﬁer of CRF. The scope of these\ncues was resolved by using the CRF at the initial\nstage with a window size of two. Later, domain\nknowledge was used to incorporate rules in the\nsystem, which showed an improvement in perfor-\nmance of the system.\nCruz et al. (2016) used a classiﬁer-based ap-\nproach for speculation cue detection and scope\nresolution. The features for the classiﬁer were man-\nually deﬁned. For cue detection, an SVM-based\nclassiﬁer was used to predict the BIO tags. The\nscope was also identiﬁed using an SVM-based clas-\nsiﬁer to predict the in-scope and out-of-scope tags\nwhen the cues and tokens were provided as input\nto the classiﬁer. A Radial Basis Function (RBF)\nKernel was used with Cost Sensitive Learning to\nhandle imbalanced classes.\n2.2 Deep Learning Methods\nQian et al. (2016) used a CNN based approach to\nre-solve the scope of a speculation cue. The CNN\nframework took as input position and path features.\nFei et al. (2020) used a Recursive Neural Net-\nwork (RecurNN) followed by a CRF to detect the\nscope in a sentence which is named as the Recur-\nCRF model. The dependency tree based RecurNN\nlearnt a high-level representation of words in the\ngiven content. The output of the RecurNN was\ngiven to the CRF to fully under-stand the contex-\ntual information required to predict the scope of a\ngiven cue.\nRecently, Britto and Khandelwal (2020) ex-\ntended the approach by Khandelwal and Sawant\n(2020), who used BERT (Devlin et al., 2018) to\naddress Negation Cue Detection and Scope Res-\nolution. They experimented with using various\ntransformer-based architectures (BERT, XLNet\n(Yang et al., 2019) and RoBERTa (Liu et al., 2019)),\nand jointly training on multiple datasets to ad-\ndress speculation cue detection and scope resolu-\ntion. This approach gave the best results to date on\nNegation and Speculation Scope Resolution.\n2.3 Multitask Learning using Negation Scope\nResolution\nWe also review a couple of papers which have used\nMultitask Learning with Negation Scope Resolu-\ntion as one of the many tasks to jointly train the\nmodel. It is important to note that these paradigms\nwere explored to improve performance in the aux-\niliary tasks the model was trained, which were al-\nmost always harder than Negation Scope Resolu-\ntion.\nBhatia et al. (2018) perform joint entity extrac-\ntion and negation detection for biomedical articles.\nInitially, they used a hierarchical encoder-decoder\nmodel used for Named Entity Recognition (NER),\nand adapted it for the Multitask setting by sharing\nthe encoder, but using separate decoders for both\nthe tasks. To overcome the overparameterization\nduring low-resource settings, they propose usage\nof a conditional softmax shared decoder, where\ninstead of using 2 different decoder architectures,\nthey shared the decoder as well, and only had sepa-\nrate classiﬁcation heads. They also feed the output\nof the NER head as an additional input to the nega-\ntion head, which helps improve the performance.\nThey use BiLSTMs for both the encoder and de-\ncoder.\nBarnes et al. (2019) explore another joint task\nthat has been explored often, namely Sentiment\nAnalysis systems that are jointly trained with Nega-\ntion Detection systems. They mention that since\nSentiment Analysis is a harder task than negation\ndetection, and negation data is used as a task in the\npipeline for sentiment analysis, they perform selec-\ntive sharing of LSTM layers, and use negation as\nan auxillary task on which the sentiment analysis\nsystem is trained. Speciﬁcally, they use a separate\nCRF tagger for negation detection on the outputs\nof an intermediate layer for the sentiment analy-\nsis system, whose ﬁnal layer is used for sentiment\nclassiﬁcation. They use a BiLSTM-based network.\n3 Methodology\nSimilar to (Khandelwal and Sawant, 2020) and\n(Britto and Khandelwal, 2020), we use the trans-\nformer model (BERT/XLNet/RoBERTa) with a\n82\nclassiﬁcation head as our base model. To jointly\ntrain the model, we propose the following additions\nto the model.\n3.1 Cue Detection\nFor Cue Detection, we use 2 separate classiﬁcation\nheads for Negation Cue Detection and Speculation\nCue Detection respectively. The architecture can\nbe visualized as in Figure 1. We feed an input\nFigure 1: Multitask Cue Detection (Model Overview)\nsentence to the model, and use the output corre-\nsponding to the task we are looking to perform.\nThis architecture halves the number of parameters\nand inference time if we want to perform negation\nand speculation detection simultaneously.\nTo train this model, we only train on those sen-\ntences that have both negation and speculation cue\nlabels. Since we train on the BioScope Corpus, and\nthe SFU Review Corpus, all training samples have\nlabels for both negation and speculation. A single\ninput sentence is fed, and the model is trained on\nthe losses computed for both heads, negation and\nspeculation.\n3.2 Scope Resolution\nFor Scope Resolution, we use the same classiﬁca-\ntion head for both Negation and Speculation Scope\nResolution, and use preprocessing techniques to\nimplicitly tell the model which task to perform.\nFigure 2: Multitask Scope Resolution (Model\nOverview)\nFor Scope Resolution, we need to represent the\ncue words in the input sentence for which we want\nto ﬁnd the scope. This could be done via the Aug-\nment Preprocessing method used by (Britto and\nKhandelwal, 2020). This involves appending a spe-\ncial token before the cue word in the input sentence\nwhich represents the type of cue word. The types\nof cue words considered are:\n• Single Word Cue: tok[0]\n• Part of a Multiword Cue: tok[1]\n• Afﬁx (Sufﬁx / Preﬁx): tok[2]\nConsider the following example:\nInput Sentence: It seems that the treatment\nis not successful.\nNegation Cues: not\nPreprocessed Sentence: It seems that the treat-\nment is tok[0] not successful.\nTo jointly train the same model to make predic-\ntions, we have to also tell the model which task we\nexpect it to perform. To do this, we propose the\nfollowing methods which are slight modiﬁcations\nof the Augment preprocessing method:\n• Global: We represent the task by appending\nthe name of the task to be performed at\nthe end of the input sentence followed by\na [SEP] token. The cue words for both\nnegation and speculation are represented\nby the same set of special tokens. Speciﬁcally,\nInput Sentence: It seems that the treat-\nment is not successful.\nNegation Cues: not\nSpeculation Cues: seems\nInput Sentence for Negation: It seems that\nthe treatment is tok[0] not successful [SEP]\nNegation.\nInput Sentence for Speculation: It tok[0]\nseems that the treatment is not successful\n[SEP] Speculation.\nThus, the type of cue for both negation and\nspeculation is the same (single word cue),\nhence we use the same token (tok[0]) to aug-\nment the input sentence. The task is repre-\nsented by appending the task name to the end\nof the sentence.\n• Local: Here, we use the following tokens to\nrepresent the different types of negation and\nspeculation cues.\n– Negation-Single Word Cue: tok[0]\n83\n– Negation-Part of a Multiword Cue:\ntok[1]\n– Negation-Afﬁx (Sufﬁx / Preﬁx): tok[2]\n– Speculation-Single Word Cue: tok[4]\n– Speculation-Part of a Multiword Cue:\ntok[5]\n– Speculation-Afﬁx (Sufﬁx / Preﬁx):\ntok[6]\nSpeciﬁcally,\nInput Sentence: It seems that the treat-\nment is not successful.\nNegation Cues: not\nSpeculation Cues: seems\nInput Sentence for Negation: It seems that\nthe treatment is tok[0] not successful.\nInput Sentence for Speculation: It tok[4]\nseems that the treatment is not successful.\nHere, the tokens used to represent different\ntypes of negation cues are different than the\ntokens used to represent the different types\nof speculation cues, thus implicitly telling the\nmodel which scope it has to ﬁnd.\n4 Experimentation Details\nWe perform experimentation on the following\ndatasets:\n• BioScope Corpus:\n– BioScope Abstracts (BA) SubCorpora\n– BioScope Full Papers (BF) SubCorpora\n• SFU Review Corpus (SFU)\nWe believe that by training on multiple datasets,\nthe overﬁtting of the models can reduce, as the\ndatasets are fairly small in size (200-2000 samples),\ndespite the different domains of the datasets (Bio-\nScope Corpora is from the Biomedical Domain,\nand SFU Review Corpus contains general online\nreview text). Hence, we also experiment with train-\ning the models on multiple datasets, and testing on\nthe individual datasets.\nWe use a 70-15-15 train-dev-test split. The re-\nsults are reported as an average of 5 runs for train-\ning on a single dataset and an average of 3 runs for\ntraining on a combination of multiple datasets. We\nreport the Macro F1 Average (Token-level) score\nfor both Cue Detection and Scope Resolution.\nWe perform an early stopping (with a patience\nof 6) on the validation F1 Score. Since we jointly\ntrain 2 tasks, we experiment with these 2 ways to\nperform early stopping:\n• Separate: Here, we use 2 early stopping coun-\nters: One for Negation and one for Specula-\ntion. Speciﬁcally, we have separate validation\nsets for Negation and Speculation, and for\neach validation set, we run a different Early\nStopping Counter. Thus, the ﬁnal models for\nNegation and Speculation differ, although they\nare trained jointly.\n• Combined: Here, there is only one Early Stop-\nping used. Training is stopped when the aver-\nage of the validation F1 scores on the Nega-\ntion Validation set and the Speculation Valida-\ntion set do not improve for 6 epochs. Here, we\nhave the same ﬁnal model for both Negation\nand Speculation.\nWe train the models using GPUs available via\nGoogle Colaboratory. The code is publicly avail-\nable.\n5 Results and Analysis\nTo perform a better comparison of independently\ntrained models on multiple datasets, we train\nBERT, XLNet and RoBERTa on BF+BA, BF+SFU,\nBA+SFU and BF+BA+SFU for Negation Cue De-\ntection and Negation Scope Resolution. We train\nthe model as per the paper by Britto and Khandel-\nwal (2020), and average the results of 3 runs. The\nresults are shown in Tables 1 and 2. An analysis of\nthe results shown below is done in Section 5.4.\nTable 1: Negation Cue Detection (Trained on Multiple\nDatasets)\n5.1 Cue Detection\nThe results for Negation and Speculation Cue De-\ntection are shown in Table 3 (trained using the Com-\n84\nTable 2: Negation Scope Resolution (Trained on Multi-\nple Datasets)\nTable 3: Results for Cue Detection (Combined Early\nStopping)\nbined Early Stopping method), and Table 4 (trained\nusing the Separate Early Stopping method). We\ncompare the Combined and Early Stopping meth-\nods in Section 5.4.\nTable 4: Results for Cue Detection (Separate Early\nStopping)\nA comparison of the best models trained jointly\non Negation and Speculation compared with the\nindependently trained model variants and the state-\nof-the-art results is shown in Table 5.\nTable 5: Comparison of Cue Detection Results with\nState-of-the-Art Results\n5.2 Negation Scope Resolution\nThe results for Negation Scope Resolution are\nshown in Table 6 (trained using the Combined Early\nStopping method) and Table 7 (trained using the\nSeparate Early Stopping method). We compare the\nCombined and Early Stopping methods in Section\n5.4.\nA comparison of the best models trained jointly\non Negation and Speculation compared with the\nstate-of-the-art results for Negation Scope Reso-\nlution is shown in Table 8. Our joint training\napproach outperforms the existing state-of-the-art\nmodels (independently trained transformer based\narchitectures) on all datasets that we experiment\nwith.\n5.3 Speculation Scope Resolution\nThe results for Speculation Scope Resolution are\nshown in Table 9 (trained using the Combined Early\nStopping method) and Table 10 (trained using the\nSeparate Early Stopping method). We compare the\nCombined and Early Stopping methods in Section\n5.4.\n85\nTable 6: Results for Negation Scope Resolution (Com-\nbined Early Stopping)\nTable 7: Results for Negation Scope Resolution (Sepa-\nrate Early Stopping)\nTable 8: Comparison of Negation Scope Resolution Re-\nsults with State-of-the-Art Results\nTable 9: Results for Speculation Scope Resolution\n(Combined Early Stopping)\nTable 10: Results for Speculation Scope Resolution\n(Separate Early Stopping)\nA comparison of the best models trained jointly\non Negation and Speculation compared with the\nstate-of-the-art results for Speculation Scope Res-\nolution is shown in Table 11. Our joint training\napproach outperforms the existing state-of-the-art\nresults on BioScope Abstracts and SFU Review\nCorpus.\n5.4 Analysis\nOur proposed joint training scheme clearly yields\nsubstantial improvements over the independent\ntask-speciﬁc training approach, as we outperform\nthe independently trained models consistently, and\nreport new state-of-the-art results, as is illustrated\nin Tables 5, 8 and 11.\n86\nTable 11: Comparison of Speculation Scope Resolution\nResults with State-of-the-Art Results\n• XLNet consistently outperforms RoBERTa\nand BERT on the BioScope Corpora. For the\nSFU Review Corpus, we see a mixed bag of\nresults, but BERT and XLNet tend to outper-\nform RoBERTa. The impact of the similar-\nity between the pretraining corpora and the\ndataset for which the model is ﬁnetuned could\naccount for these observations.\nTable 12: Difference between Local and Global Prepro-\ncessing methods for Scope Resolution\n• For Scope Resolution, the Global preprocess-\ning method tends to outperform the Local pre-\nprocessing method. This trend is visible in Ta-\nble 12, which contains the difference between\nresults using the local preprocessing method\nand the global preprocessing method (i.e. Lo-\ncal - Global), averaged across all train-test\ndataset combinations, shown for each model-\ntask combination. The majority differences (8\nout of 12, or 66%) are negative, showing that\nglobal preprocessing method outperforms the\nlocal preprocessing method.\n• The Combined Early Stopping training\nmethod outperform the Separate Early Stop-\nping training method. This trend is visible in\nTable 13, which contains the difference be-\ntween the combined early stopping method\nTable 13: Difference between Combined and Separate\nEarly Stopping training methodologies\nand the separate early stopping method, (i.e.\nCombined - Separate), averaged across all\ntrain-test dataset combinations, shown for\neach model-task combination. The majority\ndifferences are positive, showing that Com-\nbined outperforms Separate. We reason that\nthe combined early stopping method avoids\noverﬁtting to the validation set, due to more\nexamples being considered in the validation\nset.\n6 Conclusion\nIn this paper, we explored the realm of Multi-\ntask training to jointly train the same model to\nperform both negation and speculation detection.\nWe experimented with transformer-based architec-\ntures (BERT, XLNet and RoBERTa), and proposed\nschemes to jointly train the cue detection model\nfor both negation and speculation, and the scope\nresolution model for both negation and specula-\ntion. Our approach yielded improvements over the\nindependently trained versions of the same architec-\ntures, and we reported new state-of-the-art results\nfor both negation and speculation scope resolution\non the BioScope Corpus and the SFU Review Cor-\npus. We also evaluated the different design choices\nthat were involved, and observed that the Com-\nbined Early Stopping variant gave the best overall\nperformance.\nThe future scope of this work would be to look\nat using this scheme to jointly train a model for\nmore such tasks, like NER and Sentiment Analysis,\nalong with Negation and Speculation Detection.\nReferences\nJeremy Barnes, Erik Velldal, and Lilja Øvrelid. 2019.\nImproving sentiment analysis with multi-task learn-\ning of negation. CoRR, abs/1906.07610.\nParminder Bhatia, Busra Celikkaya, and Mohammed\nKhalilia. 2018. End-to-end joint entity extraction\nand negation detection for clinical text. CoRR,\nabs/1812.05270.\n87\nBenita Kathleen Britto and Aditya Khandelwal. 2020.\nResolving the scope of speculation and negation us-\ning transformer-based architectures.\nNoa P Cruz, Maite Taboada, and Ruslan Mitkov. 2016.\nA machine-learning approach to negation and spec-\nulation detection for sentiment analysis. Journal of\nthe Association for Information Science and Technol-\nogy, 67(9):2118–2136.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nHao Fei, Yafeng Ren, and Donghong Ji. 2020. Nega-\ntion and speculation scope detection using recursive\nneural conditional random ﬁelds. Neurocomputing,\n374:22–29.\nAditya Khandelwal and Suraj Sawant. 2020. Neg-\nBERT: A transfer learning approach for negation\ndetection and scope resolution. In Proceedings of\nThe 12th Language Resources and Evaluation Con-\nference, pages 5739–5748, Marseille, France. Euro-\npean Language Resources Association.\nHalil Kilicoglu and Sabine Bergler. 2008. Recognizing\nspeculative language in biomedical research articles:\nA linguistically motivated perspective. BMC bioin-\nformatics, 9 Suppl 11:S10.\nHalil Kilicoglu and Sabine Bergler. 2010. A high-\nprecision approach to detecting hedges and their\nscopes. pages 70–77.\nNatalia Konstantinova, Sheila C. M. De Sousa, Noa P.\nCruz, Manuel J. Ma ˜na, and Ruslan Mitkov. A re-\nview corpus annotated for negation, speculation and\ntheir scope.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nGuillermo Moncecchi, Jean-Luc Minel, and Dina Won-\nsever. 2012. Improving speculative language de-\ntection using linguistic knowledge. In Proceedings\nof the Workshop on Extra-Propositional Aspects of\nMeaning in Computational Linguistics , pages 37–\n46.\nRoser Morante, Vincent Asch, and Walter Daelemans.\n2010. Memory-based resolution of in-sentence\nscopes of hedge cues. pages 40–47.\nRoser Morante and Walter Daelemans. 2009. Learn-\ning the scope of hedge cues in biomedical texts. In\nProceedings of the Workshop on Current Trends in\nBiomedical Natural Language Processing, BioNLP\n’09, page 28–36, USA. Association for Computa-\ntional Linguistics.\nArzucan ¨Ozg¨ur and Dragomir R. Radev. 2009. Detect-\ning speculations and their scopes in scientiﬁc text.\nIn Proceedings of the 2009 Conference on Empirical\nMethods in Natural Language Processing: Volume 3\n- Volume 3, EMNLP ’09, page 1398–1407, USA. As-\nsociation for Computational Linguistics.\nZhong Qian, Peifeng Li, Qiaoming Zhu, Guodong\nZhou, Zhunchen Luo, and Wei Luo. 2016. Specula-\ntion and negation scope detection via convolutional\nneural networks. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 815–825.\nJonathon Read, Erik Velldal, Stephan Oepen, and Lilja\nØvrelid. 2011. Resolving speculation and negation\nscope in biomedical articles with a syntactic con-\nstituent ranker.\nGy¨orgy Szarvas, Veronika Vincze, Rich´ard Farkas, and\nJ´anos Csirik. 2008. The bioscope corpus: Anno-\ntation for negation, uncertainty and their scope in\nbiomedical texts. In Proceedings of the Workshop\non Current Trends in Biomedical Natural Language\nProcessing, BioNLP ’08, page 38–45, USA. Associ-\nation for Computational Linguistics.\nBuzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan,\nand Shixi Fan. 2010. A cascade method for detect-\ning hedges and their scope in natural language text.\npages 13–17.\nErik Velldal. 2011. Predicting speculation: A sim-\nple disambiguation approach to hedge detection in\nbiomedical literature. Journal of biomedical seman-\ntics, 2 Suppl 5:S7.\nErik Velldal, Lilja Øvrelid, and Stephan Oepen. 2010.\nResolving speculation: Maxent cue classiﬁcation\nand dependency-based scope rules. pages 48–55.\nErik Velldal, Lilja Øvrelid, Jonathon Read, and\nStephan Oepen. 2012. Speculation and negation:\nRules, rankers, and the role of syntax. Computa-\ntional Linguistics, 38:369–410.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5753–5763."
}