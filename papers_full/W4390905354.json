{
  "title": "LF-Transformer: Latent Factorizer Transformer for Tabular Learning",
  "url": "https://openalex.org/W4390905354",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5093724023",
      "name": "Kwangtek Na",
      "affiliations": [
        "Inha University"
      ]
    },
    {
      "id": "https://openalex.org/A5101759745",
      "name": "Ju-Hong Lee",
      "affiliations": [
        "Inha University",
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A5034219746",
      "name": "Eunchan Kim",
      "affiliations": [
        "Seoul National University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6784614252",
    "https://openalex.org/W4297478855",
    "https://openalex.org/W4312815172",
    "https://openalex.org/W4322762071",
    "https://openalex.org/W3174086521",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W6745609711",
    "https://openalex.org/W3216660278",
    "https://openalex.org/W6750729320",
    "https://openalex.org/W6797867632",
    "https://openalex.org/W2054141820",
    "https://openalex.org/W1605688901",
    "https://openalex.org/W2319270064",
    "https://openalex.org/W6767303091",
    "https://openalex.org/W3126865201",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6796927292",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W2023320048",
    "https://openalex.org/W6633301734",
    "https://openalex.org/W2899743487",
    "https://openalex.org/W2125621954",
    "https://openalex.org/W2145862222",
    "https://openalex.org/W2114515438",
    "https://openalex.org/W6633499030",
    "https://openalex.org/W2015627422",
    "https://openalex.org/W6747597888",
    "https://openalex.org/W6631787827",
    "https://openalex.org/W3035422918",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W2132862423",
    "https://openalex.org/W6772853553",
    "https://openalex.org/W6791955017",
    "https://openalex.org/W3158303960",
    "https://openalex.org/W3093579165",
    "https://openalex.org/W1530210183",
    "https://openalex.org/W2998108143",
    "https://openalex.org/W4288108355",
    "https://openalex.org/W3170720134",
    "https://openalex.org/W3094502228"
  ],
  "abstract": "The field of deep learning for tabular datasets has made significant strides in recent times. Previously, gradient boosting and decision tree algorithms had been the go-to options for processing such datasets due to their superior performance. However, deep learning has now reached a level of development where it can compete with these algorithms on equal footing. Accordingly, we propose latent factorizer transformer (LF-Transformer). Our proposing method, LF-Transformer, involves applying the transformer architecture to columns and rows of a given dataset to identify the attention latent factor matrix. This matrix is then used for prediction. The process is akin to matrix factorization, which involves breaking down the original matrix into a latent matrix and then reconstructing it again. Our experimental results indicate that the LF-Transformer approach outperforms general feature embedding methods, providing superior feature presentation. Additionally, the approach has demonstrated a relative superiority in regression and classification across various datasets that we have tested. In conclusion, the LF-Transformer presents a promising direction for deep learning approaches in tabular datasets. Its ability to identify latent factors and provide superior performance in regression and classification makes it a compelling alternative to traditional algorithms.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1109/ACCESS.2017.DOI\nLF-Transformer: Latent Factorizer\nTransformer for Tabular Learning\nKWANGTEK NA1, JU-HONG LEE1, AND EUNCHAN KIM.2,3\n1Department of Electrical and Computer Engineering, Inha University, Incheon 22212, Republic of Korea\n2Department of Intelligence and Information, Seoul National University, Seoul 08826, Republic of Korea\n3College of Business Administration, Seoul National University, Seoul 08826, Republic of Korea\nCorresponding author: Ju-Hong Lee (e-mail: juhong@inha.ac.kr).\nABSTRACT The field of deep learning for tabular datasets has made significant strides in recent times.\nPreviously, gradient boosting and decision tree algorithms had been the go-to options for processing such\ndatasets due to their superior performance. However, deep learning has now reached a level of development\nwhere it can compete with these algorithms on equal footing. Accordingly, we propose latent factorizer\ntransformer (LF-Transformer). Our proposing method, LF-Transformer, involves applying the transformer\narchitecture to columns and rows of a given dataset to identify the attention latent factor matrix. This matrix\nis then used for prediction. This process is akin to matrix factorization, which involves breaking down the\noriginal matrix into a latent matrix and then reconstructing it again. Our experimental results indicate\nthat the LF-Transformer approach outperforms general feature embedding methods, providing superior\nfeature presentation. Additionally, the approach has demonstrated a relative superiority in regression and\nclassification across several datasets that we have tested. In conclusion, the LF-Transformer presents a\npromising direction for deep learning approaches in tabular datasets. Its ability to identify latent factors\nand provide superior performance in regression and classification makes it a compelling alternative to\ntraditional algorithms such as gradient boosting and decision trees.\nINDEX TERMS Tabular Dataset, Tabular Learning, Self-Attention, Row-wise Attention, Column-wise\nAttention, Matrix Factorization\nI. INTRODUCTION\nT\nHE prevailing opinion is that there is no field that deep\nlearning cannot penetrate. Deep neural network models,\nincluding ensemble models and their derivatives, have been\nevaluated as efficient and effective in various domains\nincluding text, audio, and images [1]–[5]. Furthermore, the\ntabular dataset has been evaluated as an unconquered castle\n[6]. Certainly, deep learning cannot be used to solve all\nthe problems. According to some claims, most tabular data\nmodels, which have recently received attention from the\nresearch community, have shown performances inferior to\nthat of extreme gradient boosting (XGBoost) [7] and light\ngradient boosting machine (LightGBM) [8] with significantly\nless tuning [9]. It has even been argued that deep learning is\nnot an absolute necessity for tabular data [9]. Several other\nstudies [7], [10], [11] have supported the claims of Shwartz-\nZiv and his colleague. These studies have supported the use\nand consideration of tree-based ensemble algorithms such as\nXGBoost and LightGBM for cases involving real-life tabular\ndatasets. It was recently suggested that TabNet [6] is superior\nto the gradient boosted decision tree (GBDT) algorithms for\ntabular datasets. However, at least as a result of implementing\nand experimenting with TabNet, the performance presented\nin the original study [6] was not properly reproduced, and\nit can be seen through the experimental results in [12].\nIn this competitive state-of-the-art algorithm development\non tabular datasets, we discuss the process of improve-\nment and development of a new model called the latent\nfactorizer transformer (LF-Transformer) based on the feature\ntokenizer transformer (FT-Transformer) [12] and matrix\nfactorization [13] in this study. The codes and datasets\nrelated to our proposed model (LF-Transformer) and re-\nsearch are publicly available on our GitHub project (URL:\nhttps://github.com/kwangtekNa/LF-Transformer/ accessed on\n5 January 2024.).\nThe LF-Transformer we propose, which combines the\nmatrix factorization idea with the transformer idea, has the\nfollowing contributions.\nContribution 1 . Column and row transformer enables\ncapturing not only the attention relationship between features\nVOLUME 4, 2016 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354972\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n(columns) but also the attention relationship between data\n(rows).\nContribution 2. Our idea of reconstructing the data matrix\nby embedding columns and rows into the latent space by\napplying the matrix factorization concept has shown good\nresults in feature representation.\nContribution 3. We designed a new architecture called\nLF-Transformer and had a relative superiority in regression\nand classification in many of the datasets we tested.\nII. RELATED WORKS\nTraditional machine learning methods such as gradient-\nboosted decision trees (GBDTs) and tree-based ensemble\nmodels [14], [15] are still widely used for tabular data\nmodeling [9]. However, several attempts have been made to\nuse deep learning with tabular data. These attempts such as\nTabNet [6], oblivious decision ensembles (NODE) [16], and\ndisjunctive normal form-based method (Net-DNF) [17] have\nshown better performances than that of GBDTs. However,\nsince the aforementioned studies did not employ standard\nbenchmarks and used different datasets, significant model\nand parameter tuning will be required when using these\nmodels while ensuring that the various problems associated\nwith deep neural networks (such as lack of locality, missing\ninput data, data type problems, etc.) are avoided. Thus, it is\nevident that extensive work must be done for with regard\nto fine tuning when using these models. In this regard, we\npresent a methodology combining the idea of FT-Transformer\nand matrix factorization. We then present a review as well as\na brief explanation of our study motivation. Firstly, we briefly\noutline the key ideas of tabular models that are relevant to\nthis study.\nA. TABULAR DATA LEARNING\nNODE: The NODE model [16] primarily ensures that the\nerror gradient is effectively backpropagated. Differentiable\noblivious decision trees are set up to perform this task,\nand similar to traditional tree models, their performance on\ndata segmentation and training for a selection function is\ndecided based on a threshold. Only one function is used at\neach level, resulting in a balanced tree for differentiation.\nThe final model is presented in the form of a differentiable\nensemble.\nTabNet: TabNet [6] is a representative and popular model\nthat combines deep learning with attentive mechanism for\ntabular dataset presented by Google AI. This model has been\nproven to perform well on multiple datasets. TabNet involves\na sequential decision step wherein an encoder encodes each\ncorresponding feature using sparse learning and selects a\nrelevant feature for each row using an attention mask. In\nencoding, a small set of features is forcibly selected, and the\nsparsemax layer is used. A typical advantage is that there\nis no need to select all or none of the features during the\nlearning process, and instead of using a threshold value for\neach feature, the learnable mask can deliver a softer decision.\nThis problem can also be mitigated through the use of a\nfeature selection method.\nFT-Transformer: The FT-Transformer has been proposed\nby [12]. The authors apply the transformer architecture [18]\nto perform regression and classification on a tabular dataset.\nThe FT-Transformer embeds both categorical and numerical\nfeatures in a d-dimensional vector space. In addition, the\nfinal representation of classification token (CLS token) is\nused for final prediction by inserting the CLS token into the\nembedding vector [12].\nSAINT: As far as we know, the self-attention and\nintersample attention transformer (SAINT) [19] is the first\nmodel to apply row attention to tabular datasets, which is\nreferred to as inter-sample attention in the paper. SAINT\nperforms self-attention between columns and then applies\ninter-sample attention sequentially to the output of self-\nattention. As per the authors, the application of inter-sample\nattention results in the following effect: In instances where\na particular feature is absent or contains noise in a given\nrow, SAINT is empowered to utilize intersample attention\nfor borrowing the relevant features from other data samples\nthat are similar in the batch.\nIII. MOTIVATION\nMatrix factorization [13] is a method of approximating\nthe original data matrix by mapping each column and\nrow of the data matrix to the latent feature space, and by\nmultiplying them again. It is mainly used in recommendation\nsystems or information retrieval. When considering a movie\nrecommendation system, each row is assigned to a user and\neach column is assigned to a movie title. At this moment,\nmatrix factorization decomposes this matrix which consists\nof users (rows) × movie titles (columns), and maps each to a\nlatent feature space. Its performance varies depending on the\nnumber of dimensions of the latent feature space determined\nby the researcher. Latent vectors’ each dimension of users\nand movie titles is mapped to the latent feature space, and\ncan be analyzed through latent semantic analysis (LSA). In\nthis kind of movie recommender system, each dimension\nof the latent space can be interpreted as the degree of sci-fi\ngenre propensity, the degree of romance propensity, and etc.\nThe matrix factorization can grasp the characteristics of data\nthat cannot be confirmed in the original data, and also has\nan effect of removing noise. We applied this idea of matrix\nfactorization to transformer. The purpose of the column\ntransformer is to generate column attention latent features\nusing the attention information between columns, and the\nrow transformer uses the attention information between rows\nto create row attention latent features and multiplies them\nagain to identify information that cannot be confirmed in\nthe original data.\nIV. PROPOSED MODEL\nAs depicted in Figure 1, the structure of our proposed\nmodel applies the concepts of FT-Transformer and matrix\nfactorization. A transformer is applied to each column and\n2 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354972\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nFIGURE 1: Architecture of LF-Transformer.\nNote: First, the original input data is embedded and inserted into a d-dimensional space, and a latent factor is inserted\nto learn column and row information. Afterwards, it goes through the column transformer and row transformer\nrespectively to obtain the column latent matrix with column information learned and row latent matrix with row\ninformation learned. These two matrices are multiplied again to produce the final embedded matrix, and the CLS\ntoken is inserted into it to obtain the final result.\nrow, resulting in the generation of column attention latent\nfactor matrix and row attention latent factor matrix. These\nmatrices are multiplied together to create the final embedding\nmatrix. The embedding matrix created at this time is a\nmatrix created after mapping column information and row\ninformation to latent space, and removes noise, which has\nthe effect of clarifying. A detailed analysis of this is covered\nin the analysis section. Then, after inserting the CLS token\nto be used for prediction into the latent embedding matrix,\nthe final output is obtained through the CLS query-wise\ntransformer.\nA. LATENT FACTORIZER\nThe latent factorizer module embeds the input feature in\nthe d-dimensional space and adds a latent factor so that the\nfinal representation vector of the input feature appears in\nthe Latent space. If f is the function embedding the input\nfeatures x which is one data point in the batch. it can be\nexpressed as follows.\nTj = bj + fj(xj), f j : Xj → Rd (1)\nHere, xj represents j-th, that is the input feature, bj represents\nthe bias, Tj represents the d-dimensional embedding vector\nof xj. We refer to the study (Gorishniy et al., 2021), the\nnumerical feature, f(num) can be expressed as an element-\nwise product of the weight vector, W(num)\nj ∈ Rd, and the\ncategorical feature, f(cat) and W(cat)\nj are selected by each cor-\nresponding vector from the look-up table W(cat)\nj ∈ Rsj×d.\nT(num)\nj = b(num)\nj + x(num)\nj W(num)\nj ∈ Rd (2)\nT(cat)\nj = b(cat)\nj + eT\nj W(cat)\nj ∈ Rd (3)\nT = stack[T(num)\n1 , . . . , T(num)\nk(num) , T(cat)\n1 , . . . , T(cat)\nk(cat) ]\nT ∈ Rk×d (4)\nAfterwards, in order to generate the latent factor matrix,\nwe insert the latent factor F into the embedding vector T.\nT = stack[F, T(num)\n1 , . . . , T(num)\nk(num) , T(cat)\n1 , . . . , T(cat)\nk(cat) ]\nT ∈ R(k+p)×d, F ∈ Rp×d\n(5)\nIn F ∈ Rp×d, the dimension, p is the dimension of the\nlatent factor which is hyper parameter being embedded in\nthe latent space.\nB. COLUMN AND ROW TRANSFORMER\nThe transformer layer we utilize is based on the pre-\nnormalization (PreNorm) transformer of the study (Gorishniy\net al., 2021). In (a) of Figure 2, it consists of a total of the\nL number of transformer layers and operates like a general\ntransformer up to the L−1th layer, but only the query latent\nfactor is input as the query input in the last layer, Lth Layer.\nTherefore, the final output is a latent factor matrix that has\nlearned from the column information or row information,\nand the column is formed as the B × p × d dimension, the\nrow is composed of p × M × d dimension. Here, B is the\nbatch size, M is k(num) + k(cat) the number of features,\nd is the dimension of the feature embedding, and p is the\ndimension of the latent factor. Column transformer plays a\nrole in identifying which features play an important role by\nutilizing the attention technique between features. As shown\nin (b) of Figure 2, the self-attention used in the previous\nstudy [12] is identical to the self-attention of the column\ntransformer in our proposed algorithm. Row transformer’s\nself-attention utilizes attention between each instance within\nbatch data. The row transformer’s self-attention allows it to\nlearn by borrowing information from other instances in the\nbatch if a specific instance is noisy or blur. More detailed\ncontent is covered in the analysis section. The input of\ncolumn transformer is the output of the column factorizer,\nwhich is B ×(M + p) ×d. The input of the row transformer\nis the output of row factorizer, that is (B + p) × M × d .\nVOLUME 4, 2016 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354972\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n(a) Process for Generating Latent Factors\n (b) Column and Row Transformer\nFIGURE 2: Column and Row Transformer.\nNote: As shown in (a), it consists of a total of L number of layers for transformer, and only query factorizer enters\nthe query in the last Lth layer. For the column and row transformer, the PreNorm transformer [12], [20] is used as\nin (b)\nC. CLS TOKENIZER AND CLS QUERY-WISE\nTRANSFORMER\nCLS Tokenizer works the same as column latent factorizer.\nHowever, a CLS token is inserted, not a latent factorizer,\nand the dimension of the token is always one-dimensional.\nCLS query-wise transformer works the same as Lth layer of\ncolumn transformer. However, the CLS Token for prediction,\nnot the latent factorizer, is inserted into the latent embedding\nmatrix. Therefore, the transformer’s self-attention is applied\nto the CLS query token, storing information for the final\nprediction.\nV. EXPERIMENTS\nA. DATASETS AND COMPUTING RESOURCES\nWe used a diverse set of eleven public datasets which is\nthe same dataset of the previous study (Gorishniy et al.,\n2021). However, considering the training time, we reduced\nthe size of large data through random sampling. All datasets\nare consisting of train set, validation set, test set and all\nalgorithms use the same datasets. Table 1 shows the source\nof datasets and Table 2 describes the details of the datasets.\nIn order to conduct the experiment, we employed particular\nhardware specifications. These specifications consisted of an\nNVIDIA Tesla V100 16GB GPU, an Intel Xeon 2.30GHz\nTABLE 1: Sources of Datasets.\nCA California Housing (real estate data) [21]\nAD Adult (income estimation) [22]\nHE Helena (anonymized dataset) [23] )\nJA Jannis (anonymized dataset) [23]\nHI Higgs (simulated physical particles) [24]\nAL ALOI (image dataset) [25]\nEP Epsilon (simulated physics experiments) [26]\nYE Year (audio features) [27]\nCO Covertype (forest characteristics) [28]\nY A Yahoo (search queries) [29]\nMI Microsoft (search queries) [30]\nCPU, and 64GB of RAM. The software utilized for the\nexperiment was Python version 3.8.8, along with PyTorch\nversion 1.7.1 and CUDA version 11.1.\nB. DATA PREPROCESSING\nWe preprocessed the same way of the previous study [12].\nBasically, the quantile transformation was applied, and\nstandard normalization was applied to the Helena dataset and\nthe ALOI dataset. For Epsilon dataset, As discussed in the\nprior study [12], we concluded that preprocessing adversely\naffects performance, so raw features were utilized without\n4 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354972\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 2: Properties of Datasets.\nCA AD HE JA HI AL EP YE CO Y A MI\nTrain 13000 26000 40000 55000 64000 12000 64000 100000 70000 23000 120000\nValidation 3000 6500 10000 13000 15000 3500 16000 25000 23000 5000 40000\nTest 4000 16000 15000 15000 19000 4000 20000 25000 23000 7000 40000\nNumerical 8 6 27 54 288 128 2000 90 54 699 136\nCategorical 0 6 0 0 0 0 0 0 0 0 0\nMetric RMSE ACC ACC ACC ACC ACC ACC RMSE ACC RMSE RMSE\nClass - 2 100 4 2 1000 2 - 7 - -\nTABLE 3: Hyperparameter of LF-Transformer\nCA AD HE JA HI AL EP YE CO Y A MI\nActivation reglu reglu reglu reglu reglu reglu reglu reglu reglu reglu reglu\nAttention dropout 0.451886 0.290285 0.021247 0.419491 0.402993 0.04885 0.15 0.395004 0.223646 0.10893 0.236703\nHidden factor 2.342426 2.037608 2.007966 1.061031 1.695314 1.333333 1.333333 1.333333 1.333333 1.333333 1.333333\nEmbedding dim 272 352 512 288 216 472 96 384 408 72 104\nHidden layer dropout 0.146239 0.160582 0.182515 0.463288 0.079468 0.218969 0 0.294022 0.19588 0.257506 0.087825\nInitialization kaiming kaiming kaiming kaiming kaiming kaiming kaiming kaiming kaiming kaiming kaiming\nNum of heads 8 8 8 8 8 8 8 8 8 8 8\nNum of repetitions in a transformer. 3 3 2 4 3 3 1 1 2 2 2\nPrenormalization FALSE TRUE TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\nResidual dropout 0 0 0 0 0.025131 0 0.15 0 0 0 0\nLatent factor dim 1 1 1 2 1 2 1 1 1 2 1\nBatch size 250 500 500 500 500 500 500 1000 500 125 1000\nLearning rate 9.23E-05 2.67E-05 1.42E-05 0.000109 0.000342 4.45E-05 0.0001 6.89E-05 3E-05 0.000188 4.69E-05\nOptimizer admaw admaw admaw admaw admaw admaw admaw admaw admaw admaw admaw\nPatience 16 16 16 16 16 16 16 16 16 16 16\nWeight decay 2.24E-06 1.88E-05 5.16E-05 2.9E-05 2.74E-05 2.38E-05 0.000025 5.05E-05 1.04E-05 9.15E-05 0.001\nTABLE 4: Results of Singular Model.\nCA AD HE JA HI AL EP YE CO Y A MI A VG RANKING\nTabNet 0.4989 0.8514 0.3727 0.7244 0.7208 0.8030 0.8856 9.1803 0.9076 0.8162 0.7478 8.64\nSNN 0.5032 0.8557 0.3720 0.7192 0.7199 0.8389 0.8932 9.2048 0.9099 0.8022 0.7512 8.36\nMLP 0.5043 0.8566 0.3811 0.7206 0.7248 0.8378 0.8874 9.0809 0.9102 0.7861 0.7468 5.91\nResNet 0.4855 0.8551 0.3928 0.7281 0.7272 0.8678 0.8960 9.0674 0.9182 0.7910 0.7401 3.73\nAutoInt 0.4801 0.8571 0.3668 0.7255 0.7224 0.8255 0.8882 9.1072 0.8819 0.8005 0.7500 6.82\nDCN-V2 0.4844 0.8563 0.3826 0.7157 0.7212 0.8445 0.8921 9.1433 0.9128 0.7906 0.7489 6.18\nGrowNet 0.4871 0.8560 - - 0.7238 - 0.8739 9.1289 - 0.7931 0.7507 7.56\nNODE 0.4651 0.8570 0.3483 0.7259 0.7248 0.8113 0.8959 8.9976 0.9130 0.7812 0.7452 4.36\nSAINT 0.4880 0.8524 0.3811 0.7151 0.7105 0.7760 - 9.6830 0.92417 0.79842 0.7870 8.80\nFT-Transformer 0.4599 0.8575 0.3883 0.7333 0.7293 0.8732 0.8953 9.0562 0.9320 0.7882 0.7425 2.18\nLF-Transformer 0.4588 0.8581 0.3865 0.7323 0.7257 0.8690 0.8962 9.0423 0.9325 0.7882 0.7423 2.00\npreprocessing.\nC. HYPERPARAMETER TUNING\nWe utilized Bayesian optimization for hyperparameter tuning.\nIn Table 3, it can be observed that an early stopping was\napplied with a patience of 16, and the total number of epochs\nwas not separately indicated. The Hidden factor is used to\ncalculate the output dimension of a fully connected layer\ninside a transformer. For example, when the input dimension\nof the fully connected layer is n, the output dimension would\nbe int (n × Hidden factor).\nD. RESULTS\nAs can be seen in Table 4, FT-Transformer and NODE\nperformed well on several datasets. However, our proposed\nmodel shows the best performance in most of the other\ndatasets. As discussed in the case of NODE in the previous\nstudy [12], a direct comparison is difficult because it has a\ncomplex structure in the form of an ensemble rather than\na single model. Table 5 shows the ensemble results of\nresnet, FT-Transformer, and our proposed algorithm. The\nensemble algorithms utilized the average results from each\nof the 15 individually trained models. As shown in the\ntable, the LF-Transformer demonstrates superior performance\ncompared to both individual models and a single model. It\nwas confirmed that the ensemble of NODE showed lower\nperformance than the FT-Transformer in the prior study\n[12], so it was not tested separately. Table 6 shows the\nperformance comparison of the LF-Transformer ensemble\nmodel with GBDT algorithms.\nVI. ANALYSIS\nThe mnist dataset was used to analyze the effects of column\ntransformer, row transformer, and latent factorizer. The mnist\ndataset is a handwriting image dataset, and although it is\nfar from the tabular dataset, it was chosen because it is very\nVOLUME 4, 2016 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354972\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nTABLE 5: Results of Ensemble Model.\nCA AD HE JA HI AL EP YE CO Y A MI A VG RANKING\nResNet 0.4773 0.8566 0.3964 0.7343 0.7319 0.8750 0.8970 9.0238 0.9223 0.7854 0.7365 2.45\nFT-Transformer 0.4497 0.8587 0.3947 0.7392 0.7325 0.8883 0.8964 9.0048 0.9388 0.7834 0.7408 2.18\nLF-Transformer 0.4466 0.8592 0.3960 0.7396 0.7331 0.8871 0.8968 8.9976 0.9391 0.7832 0.7405 1.36\nTABLE 6: Comparison of GBDT and LF-Transformer Ensemble.\nCA AD HE JA HI AL EP YE CO Y A MI\ncatboost 0.3902 0.8715 0.3843 0.7231 0.7272 0.8470 0.8769 9.1062 0.9224 0.7802 0.7389\nXGBoost 0.4323 0.8721 0.3719 0.7231 0.7251 0.7948 0.8762 9.1529 0.9255 0.7787 0.7380\nLF-Transformer 0.4466 0.8592 0.3960 0.7396 0.7331 0.8871 0.8968 8.9976 0.9391 0.7832 0.7405\nTABLE 7: Effects of Latent Factor Dimensions on Real Datasets.\nFactor Dim 1 2 10 15 25 30 35 45\nALOI (ACC) 0.8678 0.8690 0.8665 0.8649 0.8650 0.8645 0.8649 0.8640\nYahoo (RMSE) 0.7889 0.7882 0.7903 0.7893 0.7902 0.7889 0.7890 0.7898\npopular and easy to visualize the results.\nA. FEATURE ATTENTION MAP\nWe confirmed the attention map through the method proposed\nby [31] and [32]. In order to see the attention map, the\nattention score, softmax((QKT )/\n√\nd) value has to be\nchecked. The attention score of the row transformer is a three\ndimensional (3D) tensor, which is 784 × (B + p) × (B + p).\nAfter selecting factor p from the second component of tensor,\nit was visualized by removing factor p from the 784×(B+p).\nThe attention score of the column transformer is a 3D tensor,\nB × (784 +p) × (784 +p). To secure the attention map, if\nfactor p is chosen from (784 +p), the second component\nof tensor, it becomes a two dimensional (2D) matrix of\nB×(784+ p), and we exclude the factor p with the selection\nofvf B ×784 for visualization. Looking at the attention map\nof the Row Transformer in (a) of Figure 3, the features with\nnumbers show a strong attention, and the area around the\nnumber features shows very low attention. Parts that are far\nfrom the number features appear relatively bright and show\nsome degree of attention, but they are relatively very low\ncompared to the attention applied to the number features.\nB. BATCH ATTENTION MAP\nTo check the row transformer’s batch attention map, 20\nfeatures were randomly sampled in the dimension of mnist\ndata, 28 × 28 = 748in (b) of Figure 3. Although it is not\nknown exactly what part each feature is responsible for in\nthe image, features 15, 75, 362, 479, and 533 are identified\nas features that do not have numerical features, so it can be\nseen that attention is not captured at all. On the other hand,\nit can be seen that features such as 626 and 544 are helping\nto borrow information by paying attention to other samples\non the batch.\n(a) Column and Row Transformer Feature Attention Map\n(b) Row Transformer Batch Attention Map\nFIGURE 3: Transformer Attention Map.\nNote: In (a), the row and column transformer feature\nattention map is shown. The first line is the original\ninput, the second is the row attention feature map, and\nthe last is the column attention feature map. In (b), the\nrow transformer batch attention map from the top left\nto the right, 15 th, 70th, 75th, 124th, 151th, 174th, 203th,\n209th, 329 th, 334 th, 348 th, 263 th, 479 th, 487 th, 544 th,\n544th, 626th, 680th, 710th, and 713 th features are shown.\nC. EFFECT OF LATENT FACTOR DIMENSIONALITY\nIn Figure 4, when the latent factor is one-dimensional,\nit is confirmed that each number is mixed. However, as\nthe number of dimension of the latent factor increases,\nit is noticeable that the class of each number is clearly\ndistinguished. It is obvious that the latent factor is a hyper\nparameter, and the optimal value must be obtained through\nhyper parameter optimization.\nTable 7 confirms that the performance varies according\n6 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354972\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n(a) Factor Dim 1\n (b) Factor Dim 5\n (c) Factor Dim 10\n (d) Factor Dim 15\n (e) Factor Dim 20\n(f) Factor Dim 25\n (g) Factor Dim 30\n (h) Factor Dim 40\n (i) Factor Dim 50\n (j) Factor Dim 60\nFIGURE 4: T-sne Results of Latent Matrix.\nNote: The t-sne result of latent embedding matrix according to the dimension of latent factor for mnist data. Latent\nfactor in dimensions 1, 5, 10, 15, 20, 25, 30, 40, 50, and 60 from the top left to the right. It can be seen that the\nfeature representation improves as the latent dimension increases.\n(a) Column latent factorizer only (b) Our model\nFIGURE 5: Comparison of T-sne Results.\nNote: The t-sne result of embedding matrix with 40-\ndimnnsional latent factor. a) applies only the column\nlatent factorizer. (b) is our proposed model, and both\nColumn and Row latent factorizers have been applied..\nIt can be seen that the final embedding matrix performs\nbetter in feature representation.\nto the latent factor dimension for the ALOI dataset and the\nyahoo dataset. The ALOI dataset’s accuracy is better when\nthe score is closer to 1 and the Yahoo dataset’s root mean\nsquare error (RMSE) is better when the score is closer to 0.\nBoth datasets show better performance when the dimension\nof latent factors is 2.\nD. DIFFERENCE BETWEEN ORIGINAL AND LATENT\nEMBEDDING\nIn (a) of Figure 5, the t-sne result of the embedding matrix\napplied with the column latent factorizer looks okay in terms\nof separation. The distinction between 4 (brown) and 9 (blue)\nin the bottom left is not clear. However, as shown in (b) of\nFigure 5, the t-sne result of the embedding matrix to which\nthe Latent Factor 40 dimension is applied shows a relatively\nclear distinction between 4 and 9.\nThrough this, the effect can be confirmed like matrix fac-\ntorization, which restores the original matrix by embedding\nfeatures in the latent space. The performance of clustering\nis evaluated based on two important criteria: how tightly\nthe points are packed within each cluster, and how far apart\nthe clusters are from each other. It can be observed that our\nmodel outperforms in both of these metrics.\nVII. CONCLUSION\nA. LIMITATIONS AND FUTURE WORK\nIn addition to the common limitations of tabular data-based\ndeep learning ensemble algorithms, this study also intends\nto suggest additional research directions by identifying the\nfollowing limitations.\nFirst, our proposed model, LF-Transformer, does not fully\nprovide explainable insights. As discussed in the Analysis\n(section V), it is possible to identify which features and\ndata influenced the results through the attention map, but it\nis difficult to see that it provides all possible explanations.\nTherefore, additional research on the application of additional\nexplainable techniques is needed. Second, although the\nVOLUME 4, 2016 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354972\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\nsuperiority of LF-Transformer was demonstrated through\nvarious experiments in terms of performance, it was con-\nfirmed that our proposed model may not be optimal in\nseveral experiments. Besides the tabular data used in the\nexperiment, LF-Transformer may not be a globally optical\nsolution, so adjustments such as additional parameter tuning\nare required to properly utilize it. In addition, like other most\nof the tabular data-based algorithm improvement studies,\ncontinuous efforts are required to develop a globally optimal\nsolution through unending research and experiments. Third,\nthe LF-Transformer requires a significant amount of memory,\nbecause it creates separate data tensors for columns and rows\nin the model. This means that the model’s complexity is high\nand may not be optimal in terms of time and cost efficiency.\nTherefore, future research should consider metrics that factor\nin time and cost efficiency. Last but not least, since LF-\nTransformer applies a row-wise transformer, the batch size\nmust always be the same. This means that the entire dataset\nmust be divided by the batch size, and the dataset that is\nnot divisible by the batch size must be partially deleted\nor enlarged to match the size. For example, if the number\nof rows in an entire dataset is 4,120 and a batch size is\n200, only 120 rows are entered in the last batch of the last\niteration, so in this case, the total number of rows should be\nreduced by 4,000 or increased up to 4,200. Future research\nshould aim to solve this problem and create an environment\nin which the model can run well.\nB. CONCLUDING REMARK\nIn this study, as we were Influenced and inspired by previous\nstudies on FT-Transformer and matrix factorization, we pro-\nposed a model, so-called LF-Transformer, of creating the final\nlatent embedding matrix by generating an attention latent\nfactor matrix with a transformer. It was confirmed that the\ndata representation of our proposed method is rather superior.\nFurther, it was found that when this latent embedding matrix\nis used for classification or regression problems based on\ntabular datasets, it shows relatively superior performance\ncompared to other current state-algorithms. We hope that\nour study results and our model would serve as basis and\ngo-to option for further developments on tabular data-based\ndeep learning.\nReferences\n[1] T. Brown, B. Mann, N. Ryder, et al. , “Language\nmodels are few-shot learners,” Advances in neural\ninformation processing systems , vol. 33, pp. 1877–\n1901, 2020.\n[2] Y . Zhang, J. Qin, D. S. Park,et al., “Pushing the limits\nof semi-supervised learning for automatic speech\nrecognition,” arXiv preprint arXiv:2010.10504 , 2020.\n[3] B. Choi, Y . Lee, Y . Kyung, and E. Kim, “Albert with\nknowledge graph encoder utilizing semantic similarity\nfor commonsense question answering.,” Intelligent\nAutomation & Soft Computing , vol. 36, no. 1, 2023.\n[4] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and\nR. Girdhar, “Masked-attention mask transformer for\nuniversal image segmentation,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2022, pp. 1290–1299.\n[5] E. Kim, Y . Lee, J. Choi, B. Yoo, K. J. Chae, and C. H.\nLee, “Machine learning-based prediction of relative\nregional air volume change from healthy human lung\ncts.,” KSII Transactions on Internet & Information\nSystems, vol. 17, no. 2, 2023.\n[6] S. Ö. Arik and T. Pfister, “Tabnet: Attentive inter-\npretable tabular learning,” in Proceedings of the AAAI\nConference on Artificial Intelligence , vol. 35, 2021,\npp. 6679–6687.\n[7] T. Chen and C. Guestrin, “Xgboost: A scalable\ntree boosting system,” in Proceedings of the 22nd\nacm sigkdd international conference on knowledge\ndiscovery and data mining , 2016, pp. 785–794.\n[8] G. Ke, Q. Meng, T. Finley, et al., “Lightgbm: A highly\nefficient gradient boosting decision tree,” Advances in\nneural information processing systems , vol. 30, 2017.\n[9] R. Shwartz-Ziv and A. Armon, “Tabular data: Deep\nlearning is not all you need,” Information Fusion ,\nvol. 81, pp. 84–90, 2022.\n[10] L. Prokhorenkova, G. Gusev, A. V orobev, A. V . Doro-\ngush, and A. Gulin, “Catboost: Unbiased boosting with\ncategorical features,” Advances in neural information\nprocessing systems, vol. 31, 2018.\n[11] J. H. Friedman, “Greedy function approximation:\nA gradient boosting machine,” Annals of statistics ,\npp. 1189–1232, 2001.\n[12] Y . Gorishniy, I. Rubachev, V . Khrulkov, and A.\nBabenko, “Revisiting deep learning models for tabular\ndata,” Advances in Neural Information Processing\nSystems, vol. 34, pp. 18 932–18 943, 2021.\n[13] Y . Koren, R. Bell, and C. V olinsky, “Matrix factoriza-\ntion techniques for recommender systems,” Computer,\nvol. 42, no. 8, pp. 30–37, 2009.\n[14] T. G. Dietterich, “An experimental comparison of\nthree methods for constructing ensembles of decision\ntrees: Bagging, boosting, and randomization,” Machine\nlearning, vol. 40, no. 2, pp. 139–157, 2000.\n[15] M. Zi˛ eba, S. K. Tomczak, and J. M. Tomczak, “Ensem-\nble boosted trees with synthetic features generation in\napplication to bankruptcy prediction,” Expert systems\nwith applications, vol. 58, pp. 93–101, 2016.\n[16] S. Popov, S. Morozov, and A. Babenko, “Neural\noblivious decision ensembles for deep learning on\ntabular data,” arXiv preprint arXiv:1909.06312 , 2019.\n[17] L. Katzir, G. Elidan, and R. El-Yaniv, “Net-dnf: Effec-\ntive deep modeling of tabular data,” in International\nConference on Learning Representations , 2021.\n[18] A. Vaswani, N. Shazeer, N. Parmar, et al., “Attention\nis all you need,” Advances in neural information\nprocessing systems, vol. 30, 2017.\n8 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354972\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nAuthor et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n[19] G. Somepalli, M. Goldblum, A. Schwarzschild, C. B.\nBruss, and T. Goldstein, “Saint: Improved neural net-\nworks for tabular data via row attention and contrastive\npre-training,” arXiv preprint arXiv:2106.01342 , 2021.\n[20] Q. Wang, B. Li, T. Xiao, et al. , “Learning deep\ntransformer models for machine translation,” in Pro-\nceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , 2019, pp. 1810–1822.\n[21] R. K. Pace and R. Barry, “Sparse spatial autoregres-\nsions,” Statistics & Probability Letters , vol. 33, no. 3,\npp. 291–297, 1997.\n[22] R. Kohavi et al., “Scaling up the accuracy of naive-\nbayes classifiers: A decision-tree hybrid.,” in Kdd,\nvol. 96, 1996, pp. 202–207.\n[23] I. Guyon, L. Sun-Hosoya, M. Boullé, et al., “Analysis\nof the automl challenge series,” Automated Machine\nLearning, p. 177, 2019.\n[24] P. Baldi, P. Sadowski, and D. Whiteson, “Searching\nfor exotic particles in high-energy physics with deep\nlearning,” Nature communications, vol. 5, no. 1, pp. 1–\n9, 2014.\n[25] J.-M. Geusebroek, G. J. Burghouts, and A. W. Smeul-\nders, “The amsterdam library of object images,” Inter-\nnational Journal of Computer Vision , vol. 61, no. 1,\npp. 103–112, 2005.\n[26] G.-X. Yuan, C.-H. Ho, and C. -J. Lin, “An improved\nglmnet for l1-regularized logistic regression,” in Pro-\nceedings of the 17th ACM SIGKDD international\nconference on Knowledge discovery and data mining ,\n2011, pp. 33–41.\n[27] T. Bertin-Mahieux, D. P. Ellis, B. Whitman, and P.\nLamere, “The million song dataset,” 2011.\n[28] J. A. Blackard and D. J. Dean, “Comparative accu-\nracies of artificial neural networks and discriminant\nanalysis in predicting forest cover types from car-\ntographic variables,” Computers and electronics in\nagriculture, vol. 24, no. 3, pp. 131–151, 1999.\n[29] O. Chapelle and Y . Chang, “Yahoo! learning to rank\nchallenge overview,” in Proceedings of the learning\nto rank challenge , PMLR, 2011, pp. 1–24.\n[30] T. Qin and T. -Y . Liu, “Introducing letor 4.0 datasets,”\narXiv preprint arXiv:1306.2597 , 2013.\n[31] S. Abnar and W. Zuidema, “Quantifying attention flow\nin transformers,” arXiv preprint arXiv:2005.00928 ,\n2020.\n[32] A. Dosovitskiy, L. Beyer, A. Kolesnikov, et al., “An\nimage is worth 16x16 words: Transformers for image\nrecognition at scale,” arXiv preprint arXiv:2010.11929,\n2020.\n[33] J. Vanschoren, J. N. Van Rijn, B. Bischl, and L. Torgo,\n“Openml: Networked science in machine learning,”\nACM SIGKDD Explorations Newsletter, vol. 15, no. 2,\npp. 49–60, 2014.\n[34] J. Ho, N. Kalchbrenner, D. Weissenborn, and T.\nSalimans, “Axial attention in multidimensional trans-\nformers,” arXiv preprint arXiv:1912.12180 , 2019.\n[35] R. M. Rao, J. Liu, R. Verkuil,et al., “Msa transformer,”\nin International Conference on Machine Learning ,\nPMLR, 2021, pp. 8844–8856.\n[36] H. Iida, D. Thai, V . Manjunatha, and M. Iyyer,\n“Tabbie: Pretrained representations of tabular data,”\narXiv preprint arXiv:2105.02584 , 2021.\nKWANGTEK NA received the B.Sc. degree in\ncivil engineering and the M.S. degree in computer\nscience and engineering from Inha University,\nSouth Korea, in 2013 and 2017, respectively, where\nhe is currently pursuing the Ph.D. degree at the\nDepartment of Electrical and Computer Engineer-\ning. He is currently researching machine learning\nwith the Hanwha Group. His research interests\ninclude statistical machine learning, reinforcement\nlearning, and recommender systems.\nJU-HONG LEE received the B.S. and M.S. de-\ngrees in Computer Engineering from Seoul Na-\ntional University, Seoul, Korea, in 1983 and 1985,\nrespectively. He received the Ph.D. degree in Com-\nputer Science from KAIST, Daejeon, Korea, in\n2001. He is currently a Professor in Computer\nScience and Engineering with Inha University,\nIncheon, Korea. His research interests include\nMachine Learning, Data Mining, Financial Engi-\nneering. He is currently the CEO of Qhedge Co.,\nLtd. and is developing the Credit Scoring System and Hedge investment\nalgorithms for Portfolio Management, Index Tracking, and Arbitrage Trading\netc.\nEUNCHAN KIM received the B.A. degree in\neconomics from the University of Minnesota, Twin\nCities in 2012, and the M.S. degree in management\n(concentration: information systems) and the Ph.D.\ndegree in engineering (concentration: intelligence\nand information systems) from Seoul National\nUniversity in 2017 and 2023, respectively. He is\ncurrently working as a senior researcher at Hanwha\nGroup and a lecturer at the College of Business\nAdministration, Seoul National University. He also\nserves as a visiting scholar at both Seoul National University Hospital\nand Jeonbuk National University Hospital. His research interests include\ninformation systems, artificial intelligence (AI), applications of AI, and\nengineering management.\nVOLUME 4, 2016 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3354972\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6760863065719604
    },
    {
      "name": "Transformer",
      "score": 0.6669424176216125
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5784283876419067
    },
    {
      "name": "Gradient boosting",
      "score": 0.5140217542648315
    },
    {
      "name": "Boosting (machine learning)",
      "score": 0.4965417981147766
    },
    {
      "name": "Machine learning",
      "score": 0.49309560656547546
    },
    {
      "name": "Embedding",
      "score": 0.4453220069408417
    },
    {
      "name": "Ensemble learning",
      "score": 0.4304003119468689
    },
    {
      "name": "Decision tree",
      "score": 0.4284370541572571
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3876049816608429
    },
    {
      "name": "Random forest",
      "score": 0.334929883480072
    },
    {
      "name": "Engineering",
      "score": 0.1174900233745575
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I191879574",
      "name": "Inha University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I139264467",
      "name": "Seoul National University",
      "country": "KR"
    }
  ],
  "cited_by": 5
}