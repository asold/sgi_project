{
  "title": "Data Selection for Fine-tuning Large Language Models Using Transferred Shapley Values",
  "url": "https://openalex.org/W4385570911",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2596456142",
      "name": "Stephanie Schoch",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A2978053564",
      "name": "Ritwick Mishra",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A2153992776",
      "name": "Yangfeng Ji",
      "affiliations": [
        "University of Virginia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2970773487",
    "https://openalex.org/W3210375408",
    "https://openalex.org/W3102783139",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2949574275",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W3011279327",
    "https://openalex.org/W4233620411",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2921189944",
    "https://openalex.org/W3170680884",
    "https://openalex.org/W4309130897",
    "https://openalex.org/W4324002723",
    "https://openalex.org/W2597603852",
    "https://openalex.org/W3103649165",
    "https://openalex.org/W3187308167",
    "https://openalex.org/W2923014074"
  ],
  "abstract": "Although Shapley values have been shown to be highly effective for identifying harmful training instances, dataset size and model complexity constraints limit the ability to apply Shapley-based data valuation to fine-tuning large pre-trained language models. To address this, we propose TS-DShapley, an algorithm that reduces computational cost of Shapley-based data valuation through: 1) an efficient sampling-based method that aggregates Shapley values computed from subsets for valuation of the entire training set, and 2) a value transfer method that leverages value information extracted from a simple classifier trained using representations from the target language model. Our experiments applying TS-DShapley to select data for fine-tuning BERT-based language models on benchmark natural language understanding (NLU) datasets show that TS-DShapley outperforms existing data selection methods. Further, TS-DShapley can filter fine-tuning data to increase language model performance compared to training with the full fine-tuning dataset.",
  "full_text": "Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics - Student Research Workshop, pages 266–275\nJuly 10-12, 2023 ©2023 Association for Computational Linguistics\nData Selection for Fine-tuning Large Language Models\nUsing Transferred Shapley Values\nStephanie Schoch Ritwick Mishra Yangfeng Ji\nDepartment of Computer Science\nUniversity of Virginia\nCharlottesville, V A 22904\n{sns2gr,mbc7bu,yangfeng}@virginia.edu\nAbstract\nAlthough Shapley values have been shown to\nbe highly effective for identifying harmful train-\ning instances, dataset size and model complex-\nity constraints limit the ability to apply Shapley-\nbased data valuation to fine-tuning large pre-\ntrained language models. To address this, we\npropose TS-DS HAPLEY , an algorithm that\nreduces computational cost of Shapley-based\ndata valuation through: 1) an efficient sampling-\nbased method that aggregates Shapley values\ncomputed from subsets for valuation of the en-\ntire training set, and 2) a value transfer method\nthat leverages value information extracted from\na simple classifier trained using representations\nfrom the target language model. Our experi-\nments applying TS-DS HAPLEY to select data\nfor fine-tuning BERT-based language models\non benchmark natural language understand-\ning (NLU) datasets show that TS-DS HAPLEY\noutperforms existing data selection methods.\nFurther, TS-DS HAPLEY can filter fine-tuning\ndata to increase language model performance\ncompared to training with the full fine-tuning\ndataset.\n1 Introduction\nLarge language models (LMs) have achieved state-\nof-the-art performance on many natural language\nprocessing (NLP) tasks (Radford et al., 2019;\nBrown et al., 2020; Sanh et al., 2022). To adapt\nthese models to new datasets and tasks, the standard\napproach is to fine-tune a pre-trained LM on a tar-\ngeted downstream task. This allows the pre-trained\ngeneral linguistic knowledge to be leveraged while\nfine-tuning to learn the task-specific information.\nHowever, during fine-tuning, pre-trained LMs are\nprone to significant performance degradation in\nthe presence of noisy data (Srivastava et al., 2020).\nThis effect may be further amplified when noisy or\notherwise harmful instances are highly influential\nto the model parameters (Koh and Liang, 2017).\nAs a result, it is important to identify harmful in-\nFigure 1: An overview of TS-DS HAPLEY : 1) Pro-\ncess the data using the target LM; 2) Compute sampling\nchains using a subset of the training set and aggregate\nthe resulting Shapley values; and 3) Transfer the esti-\nmated data value information for use with the target LM\nby estimating the optimal low value data removal index.\nstances in the fine-tuning data that may obfuscate\nthe task information and degrade performance.\nTo automatically identify harmful data, prior\nworks have used training dynamics (Swayamdipta\net al., 2020) and estimation of marginal contribu-\ntions via leave-one-out retraining (Cook, 1977) or\ninfluence functions (Koh and Liang, 2017). Shap-\nley values, which satisfy certain desirable fairness\nguarantees, have also recently been adopted from\ncooperative game theory to measure datum con-\ntributions, where a data point’s Shapley value is\nthe average marginal contribution to every possible\ndata subset (Ghorbani and Zou, 2019).\nIn practice, Shapley-based data values are ap-\nproximated using various techniques (Ghorbani\nand Zou, 2019; Jia et al., 2019b, 2021; Kwon and\nZou, 2022; Schoch et al., 2022), as exact Shapley\nvalue computation over a dataset would require ex-\nhaustively retraining the model for every datum on\n266\nevery possible subset (i.e. exponential complexity\nwith respect to the number of data points ). How-\never, many of the existing approximation methods\nstill exhibit a computational bottleneck when con-\nsidering datasets and models at scale (e.g. datasets\nlarger than 5K instances). This, in turn, directly lim-\nits the application of Shapley-based data valuation\nto state-of-the-art LMs and many NLP datasets.\nTo address the challenges posed by 1)\nthe model constraint (the model retraining re-\nquirement) and 2) the dataset constraint (the\ntime-complexity/dataset size relation), we pro-\npose Transferred Sampling Data Shapley ( TS-\nDSHAPLEY ), an algorithm that utilizes two novel\ncomponents that directly address each constraint.\nSpecifically, to address the model constraint, we\npropose to compute Shapley-based data values us-\ning a simple, linear model that is trained on the\nlearned representation from the target LM. Addi-\ntionally, to address the dataset constraint, we pro-\npose a sampling-based method that computes Shap-\nley values on data subsets and aggregates them for\nvaluation of the entire training set.\nOur contributions are as follows: 1) we propose a\nsampling-based data Shapley computation method\nand demonstrate its efficacy empirically using as\nlittle as 2% of the original training data; 2) we pro-\npose the use of a simple linear classifier with a tar-\nget model’s pre-trained representation and demon-\nstrate empirically the performance gains achieved\nover alternate pre-trained embeddings; and 3) we\nshow the efficacy of Shapley-based data valuation\nand selection methods on benchmark NLU tasks\nusing fine-tuned large LMs.1\n2 Related Work\nWhile Shapley values are often applied in a post\nhoc manner following model training (Ghorbani\nand Zou, 2019; Kwon and Zou, 2022; Jia et al.,\n2019a,b, 2021; Schoch et al., 2022), the demon-\nstrated efficacy makes it a natural extension to ap-\nply such methods for data selection prior to train-\ning. To this end, Shapley values have been used\nfor evaluating data for transfer learning (Parvez\nand Chang, 2021) and in active learning (Ghorbani\net al., 2021).\nFurther, although Shapley-based data values\nhave primarily been considered model-specific, in\npractice, a subset of training instances that may\n1Code is available at https://github.com/\nstephanieschoch/ts-dshapley\nharm performance may be mislabeled (Koh and\nLiang, 2017; Swayamdipta et al., 2020; Ghorbani\nand Zou, 2019) or exhibit spelling mistakes or\ngrammatical errors (Sun et al., 2020; Srivastava\net al., 2020), which should be intrinsic to the\ndataset. Prior works have demonstrated the trans-\nferability of Shapley-based data values across var-\nious classifier architectures (Schoch et al., 2022)\nand have demonstrated the efficacy of surrogate\nKNN classifiers using pre-trained embeddings (Jia\net al., 2021). Notably, our work differs in that we\nutilize the pre-trained embeddings extracted from\nthe target LM and avoid the k-nearest neighbor as-\nsumption that training data far from a test datum do\nnot contribute to its prediction (Jia et al., 2019a).\n3 Method\nLet D = {(xi,yi)}n\ni=1 denote a training set\ncontaining n training instances. For each\ntraining instance i, the Shapley value ϕi is\ndefined as the average marginal contribution\nof i to every possible subset S ⊆ D that con-\ntains this instance (Ghorbani and Zou, 2019):\nϕi = ∑\nS⊆D;i∈S\n1\n( n−1\n|S\\{i}|){vA(S) −vA(S\\{i})}\nwhere vA(S) is a value function, typically defined\nas the development accuracy of model Atrained\non S. The challenge of calculating ϕi is two-fold:\nthe exponential complexity of all possible subsets\nS ⊆Dand the computational cost of training A\non each S and S\\{i}. While Shapley-based data\nvalues are approximated in practice, most existing\napproximation methods are not efficient enough\nfor large scale learning problems.\n3.1 TS-DS HAPLEY\nLet Atgt be the target classifier (i.e. large LM)\nthat we want to fine-tune on a subset of D. To\nreduce computational cost, we propose to (1) use\na linear classifier Asrc as the proxy of Atgt for\ndata valuation; (2) use multi-chain Monte Carlo\nsampling to compute Shapley values on different\nsubsets of D. For faithful data valuation, we further\npropose to train Asrc on the data representations\nextracted from Atgt.\nRepresentation Extraction. We extract the rep-\nresentations from the penultimate layer of the pre-\ntrained LM Atgt as the inputs for training Asrc.\nNote that training Asrc in this way is equivalent to\nfixing the LM and only fine-tuning the last classifi-\ncation layer. To further remove the redundancy in\n267\nthe representations and reduce computational cost,\nwe follow prior work by performing PCA on the\ncollection of representations and selecting the first\n32 principal components (Ghorbani and Zou, 2019;\nKwon and Zou, 2022; Schoch et al., 2022).\nSampling Data Shapley. Instead of directly esti-\nmating Shapley-based data values via Monte Carlo\nsampling on the whole training set, our approach\nperforms Monte Carlo sampling on subsets of the\ndata, which we refer to as sampling chains. Within\na single sampling chain c, we sample a subset of\ntraining instances St, estimate their contributions,\nand repeat T times. The contribution of each in-\nstance in St is calculated by removing one instance\nat a time in a random order. For example, the con-\ntribution of the first randomly removed instance i\nis cSt(i) =vAsrc (St) −vAsrc (St\\{i}), the contri-\nbution of the second randomly removed instance\nk is cSt(k) = vAsrc (St\\{i}) −vAsrc (St\\{i,k}),\nand so on. On the other hand, if an instance iis not\nin St, cSt(i) = 0.\nAfter T times, the Shapley value of instance i\nis approximated as ϕi ≈1\nT\n∑\nSt cSt(i). To balance\nthe computational efficiency and approximation,\nwe empirically define a range of the size |St|∈\n[s\n2 ,s], with subset size s as the sampling upper\nbound.\nComputation can be further sped up with mul-\ntiple Monte Carlo sampling chains S(c)\nt ,c ∈\n{1,...,J }. The corresponding value approxima-\ntion is defined as ϕi = 1\nJ\n∑\nc\n1\nT\n∑\nS(c)\nt\ncS(c)\nt\n(i).As\neach chain can be computed independently, the\nefficiency can be boosted with parallel comput-\ning. This novel idea of multi-chain sampling serves\nas the core of TS-DS HAPLEY and significantly\nspeeds up computation, in practice working with a\nsimple model Asrc.\nData Selection withTS-DS HAPLEY Values. To\nidentify harmful data points, we use the data re-\nmoval strategy of Ghorbani and Zou (2019) on\nAsrc and transfer the selection outcome to the tar-\nget model Atgt. Specifically, we gradually remove\ntraining instances from the lowest estimated contri-\nbution value to the highest estimated contribution\nvalue. Following each removal, we retrain Asrc\nand evaluate predictive performance on the held-\nout development data. As a result, this removal pro-\ncedure will identify a optimal subsetSopt that gives\nthe best predictive performance on Asrc. With the\nassumption of data value transferability (Schoch\net al., 2022), we expect that Atgt trained on Sopt\nwill give no worse, and likely better performance,\nthan Atgt trained on D. While this data removal\nstrategy is proposed in prior work (Ghorbani and\nZou, 2019), the data selection use case is novel in\nNLP.\n4 Experiments\n4.1 Experiment Setup\nPre-trained Large Language Models.We uti-\nlize two transformer-based large LMs for which\ntraditional Shapley-based data value computation\nwould be intractable: RoBERTa-base (Liu et al.,\n2019, 125M parameters) and DistilBERT (Sanh\net al., 2019, 66M parameters).\nDatasets. We select one GLUE benchmark\n(Wang et al., 2019) dataset from each task cate-\ngory: SST-2 (Socher et al., 2013), QQP (Iyer et al.,\n2017), and RTE (Dagan et al., 2006), representing\nSingle-Sentence Tasks, Similarity and Paraphrase\nTasks, and Inference Tasks, respectively. Addi-\ntional dataset details are reported in Appendix A.\nNotably, we select datasets of varied sizes to reflect\ndiverse sampling subset to training set size ratios.\nData Selection Baselines. We compare against\nperformance when training on the full data subset\nas well as three selection baselines: leave-one-out\n(LOO) (Cook, 1977), KNN-shapley (KNN) (Jia\net al., 2019a, 2021), and random sampling. For\nLOO, we use the same classifier architecture as\nwith TS-DS HAPLEY to compute value estimates.\nFor both LOO and KNN, we reduce the dataset us-\ning the data removal procedure defined in section 3.\nFinally, for random sampling, we remove a random\nsample of data points equal to the number of points\nremoved via TS-DS HAPLEY .\n4.2 Data Selection Experiment\nTo test the efficacy of using TS-DS HAPLEY to se-\nlect data for fine-tuning large LMs, we compute\ndata values using each method and perform the data\nremoval procedure described in section 3. Specif-\nically, we remove the lowest value data points\npreceding the data removal step that achieved the\nhighest development accuracy using Asrc. For TS-\nDSHAPLEY , we vary the subset size and number\nof chains based on dataset size, using subset size\n= 6.7k(10%),7.28k(2%),374(15%) and number\nof chains = 25,10,25 for SST-2, QQP, and RTE,\n268\nMethod Category Method RoBERTa DistilBERT\nSST-2 QQP RTE SST-2 QQP RTE\nFull Training Set\nLiu et al. (2019) 0.948 0.919 0.787 – – –\nSanh et al. (2019) – – – 0.913 0.885 0.599\nFull Dataset 0.950 0.917 0.788 0.908 0.905 0.618\nData Selection Baselines\nLeave-One-Out 0.947 – 0.784 0.912 – 0.614\nKNN Shapley 0.946 0.916 0.781 0.911 0.905 0.622\nRandom 0.947 0.917 0.684 0.911 0.905 0.589\nOur Method TS-DS HAPLEY 0.953 0.919 0.801 0.915 0.907 0.652\nTable 1: Predictive accuracy when selecting data using each valuation method. Results reflect the mean of five\ntrials. We do not report LOO as a baseline for QQP due to computational intractability.\nrespectively. Additional training and hyperparame-\nter details, including details of a limited hyperpa-\nrameter sweep, can be found in Appendix A.\nResults Results are shown in Table 1. TS-\nDSHAPLEY consistently outperforms baseline se-\nlection methods as well as performance using\nthe full fine-tuning dataset. Notably, data se-\nlection using TS-DS HAPLEY resulted in perfor-\nmance improvements of up to 1.3% and 3.4% for\nRoBERTa and DistilBERT, respectively, over the\npredictive performance when training using the\nfull fine-tuning dataset. These results indicate TS-\nDSHAPLEY successfully identifies data points that\nharm model performance. As an additional anal-\nysis, for the RTE dataset we show the location\nof harmful points identified by TS-DS HAPLEY\non a data map (Swayamdipta et al., 2020) in Ap-\npendix B.\n4.3 Sampling Hyperparameter Analysis\nTS-DS HAPLEY exhibited good performance for\ndata selection across various subset sizes and\nnumbers of chains. For example, on QQP TS-\nDSHAPLEY outperformed the full dataset and base-\nline methods when using a subset of just 2% of\nthe training set. To better understand the impact\nof different parameter values, we utilize a parame-\nter value grid on the RTE dataset and re-compute\nTS-DS HAPLEY . Specifically, using the best hyper-\nparameters from subsection 4.2 (see Appendix A),\nwe evaluate performance of RoBERTa and Distil-\nBERT using a parameter sweep of subset size as\na percentage of the total training set size, subset\nsize ∈ {1,2,5,10,15}%, and number of chains\n∈{2,5,10,15}and report the Pearson’s correla-\ntion between each parameter and performance.\nResults. All correlations are reported in Ap-\npendix B and summarized here. When subset\nModel Embeddings SST-2 QQP RTE\nRoBERTa\nRoBERTa 0.953 0.919 0.801\nDistilBERT 0.951 0.906 0.762\nGloVe 0.948 0.908 0.767\nDistilBERT\nDistilBERT 0.915 0.907 0.652\nRoBERTa 0.906 0.903 0.623\nGloVe 0.909 0.903 0.632\nTable 2: Predictive accuracy using TS-DS HAPLEY\nwith different word embeddings.\nsize > 2%, both models demonstrate a high pos-\nitive correlation between number of chains and\nperformance. For example, when using 15% of the\ntraining data, RoBERTa on RTE had a correlation\nof 0.94. Across the different number of chains,\nhowever, there was no consistent pattern of corre-\nlation between subset size and performance. This\nindicates that increasing number of chains (which\ncan be computed in-parallel) may be of more bene-\nfit compared to increasing sampling subset size.\n4.4 Effect of Different Embeddings\nTo test the efficacy of computing TS-DS HAPLEY\nusing the extracted representations from the target\nLM, we perform an experiment where we use the re-\nmoval indices computed with 1) the representation\nfrom a different language model (e.g. removing\nindices for fine-tuning RoBERTa using the optimal\nremoval index identified using DistilBERT data\nrepresentations), and 2) GloVe pre-trained word\nembeddings (Pennington et al., 2014), as a third-\nparty representation repository.\nResults. As shown in Table 2, while alternate\nembeddings can still lead to improvements over the\nfull data, using the representation from the target\nLM is beneficial and consistently outperforms other\nembeddings. The results suggest that low value\ndata is likely a combination of (i) inherently noisy\n269\ndata (e.g. mislabeled instances) and (ii) instances\nthat are harmful to specific models due to different\nmodel architectures and pre-training strategies.\n5 Conclusion\nIn this work, we propose TS-DS HAPLEY to ad-\ndress the model and dataset constraints that cur-\nrently contribute to a computational bottleneck\nwhen computing Shapley-based data value esti-\nmates.\nLimitations\nWhile we demonstrate the efficacy of TS-\nDSHAPLEY empirically, the current work is limited\nin terms of theoretical analysis. For example, while\nwe have good empirical performance with a linear\nSVM, additional analysis could determine if there\nare optimal ways to select an alternative simple\nmodel architecture for the source classifier depend-\ning on the target classifier or dataset. Additionally,\nwhile we found a strong correlation between num-\nber of sampling chains and performance when the\nsubset size was >2% of the training data size, the\nlower subset size threshold to observe this corre-\nlation may be dataset dependent, which additional\nanalysis could address.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nR Dennis Cook. 1977. Detection of influential obser-\nvation in linear regression. Technometrics, 19(1):15–\n18.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The PASCAL recognising textual entailment\nchallenge. In Machine learning challenges. evaluat-\ning predictive uncertainty, visual object classification,\nand recognising tectual entailment, pages 177–190.\nSpringer.\nAmirata Ghorbani and James Zou. 2019. Data shap-\nley: Equitable valuation of data for machine learning.\nIn International Conference on Machine Learning,\npages 2242–2251. PMLR.\nAmirata Ghorbani, James Zou, and Andre Esteva. 2021.\nData shapley valuation for efficient batch active learn-\ning. arXiv preprint arXiv:2104.08312.\nShankar Iyer, Nikhil Dandekar, and Kornel Csernai.\n2017. First quora dataset release: Question pairs.\nRuoxi Jia, David Dao, Boxin Wang, Frances Ann\nHubis, Nezihe Merve Gurel, Bo Li, Ce Zhang,\nCostas Spanos, and Dawn Song. 2019a. Efficient\ntask-specific data valuation for nearest neighbor al-\ngorithms. Proceedings of the VLDB Endowment ,\n12(11):1610–1623.\nRuoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis,\nNick Hynes, Nezihe Merve Gürel, Bo Li, Ce Zhang,\nDawn Song, and Costas J Spanos. 2019b. Towards\nefficient data valuation based on the shapley value.\nIn The 22nd International Conference on Artificial\nIntelligence and Statistics, pages 1167–1176. PMLR.\nRuoxi Jia, Fan Wu, Xuehui Sun, Jiacen Xu, David Dao,\nBhavya Kailkhura, Ce Zhang, Bo Li, and Dawn Song.\n2021. Scalability vs. utility: Do we have to sacrifice\none for the other in data importance quantification?\nIn Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 8239–\n8247.\nPang Wei Koh and Percy Liang. 2017. Understanding\nblack-box predictions via influence functions. In\nInternational conference on machine learning, pages\n1885–1894. PMLR.\nYongchan Kwon and James Zou. 2022. Beta shapley: a\nunified and noise-reduced data valuation framework\nfor machine learning. Proceedings of the 25th In-\nternational Conference on Artificial Intelligence and\nStatistics (AISTATS) 2022.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nMd Rizwan Parvez and Kai-Wei Chang. 2021. Evalu-\nating the values of sources in transfer learning. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5084–5116.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 1532–1543, Doha, Qatar.\nAssociation for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\n270\net al. 2022. Multitask prompted training enables zero-\nshot task generalization. In The Tenth International\nConference on Learning Representations.\nStephanie Schoch, Haifeng Xu, and Yangfeng Ji. 2022.\nCs-shapley: Class-wise shapley values for data valu-\nation in classification. In Advances in Neural Infor-\nmation Processing Systems.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empiri-\ncal methods in natural language processing, pages\n1631–1642.\nAnkit Srivastava, Piyush Makhija, and Anuj Gupta.\n2020. Noisy text data: Achilles’ heel of bert. In\nProceedings of the Sixth Workshop on Noisy User-\ngenerated Text (W-NUT 2020), pages 16–21.\nLichao Sun, Kazuma Hashimoto, Wenpeng Yin, Akari\nAsai, Jia Li, Philip Yu, and Caiming Xiong. 2020.\nAdv-bert: Bert is not robust on misspellings! gen-\nerating nature adversarial samples on bert. arXiv\npreprint arXiv:2003.04985.\nSwabha Swayamdipta, Roy Schwartz, Nicholas Lourie,\nYizhong Wang, Hannaneh Hajishirzi, Noah A Smith,\nand Yejin Choi. 2020. Dataset cartography: Mapping\nand diagnosing datasets with training dynamics. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9275–9293.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In the Pro-\nceedings of ICLR.\n271\nA Additional Experiment Details\nIn this section, we include additional experiment\nsetup details.\nA.1 Datasets\nDataset statistics are provided in Table 3, with fur-\nther description provided below.\nSST-2: Stanford Sentiment Treebank (Socher\net al., 2013) is a collection of English movie re-\nviews with human annotations of their sentiment.\nThe model is tasked with predicting a review’s sen-\ntiment as positive or negative.\nQQP: Quora Question Pairs (Iyer et al., 2017)\nis a collection of English question pairs from the\nwebsite Quora where the task is to determine if a\npair of questions are similar in meaning.\nRTE: Recognizing Textual Entailment (Dagan\net al., 2006) combines several English datasets\nfrom annual textual entailment challenges, where\nthe task is to predict if thetext entails the hypothesis\nor not.\nA.2 Hyperparameters\nFor each experiment, we consider a limited hyper-\nparameter sweep for each model, selection method,\nand task, with batch size ∈{16,32}and learning\nrate ∈{10−5,3 ×10−5}. The rest of the hyper-\nparameters are kept consistent across experiment\nconditions. We report the mean development set\naccuracy from five random initializations for which\nwe fine-tune for 10 epochs and select the model\ncheckpoint with the highest development set accu-\nracy. Results from each hyperparameter sweep are\nreported in Table 4 and Table 5.\nB Additional Results\nB.1 Additional Data Selection Analysis\nWhile we compare directly with baseline selec-\ntion methods that directly measure estimated data\ncontribution, we perform an additional analy-\nsis by comparing the indices removed with TS-\nDSHAPLEY with the mapped training dynamics us-\ning data maps (Swayamdipta et al., 2020). Specif-\nically, we first plot the data map for RoBERTa\ntrained on RTE using the same hyperparameters as\nin subsection 4.2. Then, we plot the same data map\nshowing only the data points that were identified by\nTS-DS HAPLEY to be harmful, i.e. removed from\nthe fine-tuning training data. These are shown in\nFigure 2 and Figure 3, respectively.\nWe observe that a handful of instances in the\nhard-to-learn region (identified by Swayamdipta\net al. (2020) to contain some mislabeled exam-\nples) were removed, as well as a small number\nof instances in the ambiguous region. Interest-\ningly though, we observe that 1) most of the data\npoints in RTE belonged to the easy-to-learn region,\nand 2) a cluster of easy-to-learn points were re-\nmoved. Swayamdipta et al. (2020) found that too\nmany easy-to-learn instances could decrease both\nin-distribution and out-of-distribution performance\nand noted that determining how to select an optimal\nbalance of easy-to-learn and ambiguous examples,\nparticularly in low data settings, was an open prob-\nlem. As TS-DS HAPLEY achieved a performance\ngain over the full dataset performance, these results\nsuggest that TS-DS HAPLEY may be effective to\npotentially determine an optimal balance and ad-\ndress this problem. We leave further analysis of\nthis to future work.\nB.2 Sampling Hyperparameter Analysis.\nPearson’s correlation coefficients for the sampling\nparameter analysis in section 4 are reported in Ta-\nble 6 and Table 7, where each result represents\nthe mean of five sampling and chain computation\ntrials.\n272\nDataset GLUE Task Category Task Metric Data Split\nTrain Dev\nSST-2 Single Sentence Tasks Sentiment Acc. 67k 1.8k\nQQP Similarity and Paraphrase Tasks Paraphrase Acc./F1 364k 40.4k\nRTE Inference Tasks NLI Acc. 2.5k 277\nTable 3: Statistics for each dataset. We use the train and development data splits as GLUE tasks have held out test\nset labels.\nModel Method SST-2 QQP RTE\nBS LR BS LR BS LR\nRoBERTa\nFull Dataset 16 10−5 32 3 × 10−5 16 3 × 10−5\nLeave-One-Out 32 10−5 – – 16 3 × 10−5\nKNN Shapley 16 10−5 32 3 × 10−5 16 3 × 10−5\nRandom 32 3 × 10−5 32 3 × 10−5 16 3 × 10−5\nTS-DS HAPLEY 32 10−5 32 3 × 10−5 16 3 × 10−5\nDistilBERT\nFull Dataset 16 10−5 32 3 × 10−5 32 3 × 10−5\nLeave-One-Out 32 10−5 – – 16 10−5\nKNN Shapley 16 10−5 32 3 × 10−5 16 10−5\nRandom 32 3 × 10−5 16 3 × 10−5 16 3 × 10−5\nTS-DS HAPLEY 16 3 × 10−5 16 10−5 16 3 × 10−5\nTable 4: Batch size (BS) and learning rate (LR) for the data selection experiment based on the hyperparameter\nsweep defined in section 4.\nModel Embeddings SST-2 QQP RTE\nBS LR BS LR BS LR\nRoBERTa\nRoBERTa 32 10−5 32 3 × 10−5 16 3 × 10−5\nDistilBERT 16 10−5 32 10−5 16 3 × 10−5\nGloVe 16 3 × 10−5 32 3 × 10−5 32 3 × 10−5\nDistilBERT\nDistilBERT 16 10−5 16 10−5 16 3 × 10−5\nRoBERTa 32 10−5 32 10−5 32 10−5\nGloVe 32 10−5 32 3 × 10−5 32 3 × 10−5\nTable 5: Batch size (BS) and learning rate (LR) for the embeddings switch experiment based on the hyperparameter\nsweep defined in section 4.\nModel Subset Size (%, #)\n1 (25) 2 (50) 5 (125) 10 (249) 15 (374)\nRoBERTa 0.119 0.013 0.892 0.929 0.942\nDistilBERT 0.240 0.104 0.613 0.776 0.714\nTable 6: Correlations between number of chains and performance for each subset size on the RTE dataset.\nModel Number of Sampling Chains\n2 5 10 15 20 25\nRoBERTa -0.463 0.127 -0.474 0.013 0.472 0.763\nDistilBERT 0.027 -0.034 0.530 0.447 0.737 0.692\nTable 7: Correlations between subset size and performance for each number of sampling chains on the RTE dataset.\n273\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000013/uni00000011/uni00000014/uni00000018/uni00000013/uni00000011/uni00000015/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000016/uni00000013\n/uni00000059/uni00000044/uni00000055/uni0000004c/uni00000044/uni00000045/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a\n/uni00000013/uni00000011/uni0000001b\n/uni00000013/uni00000011/uni0000001c/uni00000046/uni00000052/uni00000051/uni00000049/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048\n/uni00000044/uni00000050/uni00000045/uni0000004c/uni0000004a/uni00000058/uni00000052/uni00000058/uni00000056\n/uni00000048/uni00000044/uni00000056/uni0000005c/uni00000010/uni00000057/uni00000052/uni00000010/uni0000004f/uni00000048/uni00000044/uni00000055/uni00000051\n/uni0000004b/uni00000044/uni00000055/uni00000047/uni00000010/uni00000057/uni00000052/uni00000010/uni0000004f/uni00000048/uni00000044/uni00000055/uni00000051\n/uni00000035/uni00000037/uni00000028/uni00000010/uni00000035/uni00000052/uni00000025/uni00000028/uni00000035/uni00000037/uni00000044/uni00000003/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000030/uni00000044/uni00000053\n/uni00000046/uni00000052/uni00000055/uni00000055/uni00000048/uni00000046/uni00000057/uni00000011\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a\n/uni00000013/uni00000011/uni0000001b\n/uni00000013/uni00000011/uni0000001c\n/uni00000014/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018\n/uni00000046/uni00000052/uni00000051/uni00000049/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048\n/uni00000013\n/uni00000015/uni00000013/uni00000013\n/uni00000017/uni00000013/uni00000013\n/uni00000019/uni00000013/uni00000013\n/uni0000001b/uni00000013/uni00000013/uni00000047/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000057/uni0000005c\n/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016\n/uni00000059/uni00000044/uni00000055/uni0000004c/uni00000044/uni00000045/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c\n/uni00000013\n/uni00000015/uni00000013/uni00000013\n/uni00000017/uni00000013/uni00000013\n/uni00000019/uni00000013/uni00000013/uni00000047/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000057/uni0000005c\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013\n/uni00000046/uni00000052/uni00000055/uni00000055/uni00000048/uni00000046/uni00000057/uni00000051/uni00000048/uni00000056/uni00000056\n/uni00000013\n/uni00000015/uni00000013/uni00000013\n/uni00000017/uni00000013/uni00000013\n/uni00000019/uni00000013/uni00000013\n/uni0000001b/uni00000013/uni00000013/uni00000047/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000057/uni0000005c\nFigure 2: Data map for RoBERTa trained on the RTE dataset.\n274\n/uni00000013/uni00000011/uni00000014/uni00000015/uni00000018/uni00000013/uni00000011/uni00000014/uni00000018/uni00000013/uni00000013/uni00000011/uni00000014/uni0000001a/uni00000018/uni00000013/uni00000011/uni00000015/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000015/uni00000018/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000013/uni00000011/uni00000015/uni0000001a/uni00000018/uni00000013/uni00000011/uni00000016/uni00000013/uni00000013\n/uni00000059/uni00000044/uni00000055/uni0000004c/uni00000044/uni00000045/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a\n/uni00000013/uni00000011/uni0000001b\n/uni00000013/uni00000011/uni0000001c/uni00000046/uni00000052/uni00000051/uni00000049/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048\n/uni00000044/uni00000050/uni00000045/uni0000004c/uni0000004a/uni00000058/uni00000052/uni00000058/uni00000056\n/uni00000048/uni00000044/uni00000056/uni0000005c/uni00000010/uni00000057/uni00000052/uni00000010/uni0000004f/uni00000048/uni00000044/uni00000055/uni00000051\n/uni0000004b/uni00000044/uni00000055/uni00000047/uni00000010/uni00000057/uni00000052/uni00000010/uni0000004f/uni00000048/uni00000044/uni00000055/uni00000051\n/uni00000035/uni00000037/uni00000028/uni00000010/uni00000035/uni00000052/uni00000025/uni00000028/uni00000035/uni00000037/uni00000044/uni00000003/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000030/uni00000044/uni00000053\n/uni00000046/uni00000052/uni00000055/uni00000055/uni00000048/uni00000046/uni00000057/uni00000011\n/uni00000013/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000014\n/uni00000013/uni00000011/uni00000015\n/uni00000013/uni00000011/uni00000016\n/uni00000013/uni00000011/uni00000017\n/uni00000013/uni00000011/uni00000018\n/uni00000013/uni00000011/uni00000019\n/uni00000013/uni00000011/uni0000001a\n/uni00000013/uni00000011/uni0000001b\n/uni00000013/uni00000011/uni0000001c\n/uni00000014/uni00000011/uni00000013\n/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000018/uni00000013/uni00000013/uni00000011/uni0000001a/uni00000018\n/uni00000046/uni00000052/uni00000051/uni00000049/uni0000004c/uni00000047/uni00000048/uni00000051/uni00000046/uni00000048\n/uni00000013\n/uni00000015/uni00000013\n/uni00000017/uni00000013\n/uni00000019/uni00000013/uni00000047/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000057/uni0000005c\n/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016\n/uni00000059/uni00000044/uni00000055/uni0000004c/uni00000044/uni00000045/uni0000004c/uni0000004f/uni0000004c/uni00000057/uni0000005c\n/uni00000013\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013/uni00000047/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000057/uni0000005c\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000011/uni00000014/uni00000013/uni00000011/uni00000015/uni00000013/uni00000011/uni00000016/uni00000013/uni00000011/uni00000017/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000014/uni00000011/uni00000013\n/uni00000046/uni00000052/uni00000055/uni00000055/uni00000048/uni00000046/uni00000057/uni00000051/uni00000048/uni00000056/uni00000056\n/uni00000013\n/uni00000015/uni00000013\n/uni00000017/uni00000013\n/uni00000019/uni00000013/uni00000047/uni00000048/uni00000051/uni00000056/uni0000004c/uni00000057/uni0000005c\nFigure 3: Data map showing location of training instances that were removed by TS-DS HAPLEY for RoBERTa on\nRTE.\n275",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7853579521179199
    },
    {
      "name": "Shapley value",
      "score": 0.7715242505073547
    },
    {
      "name": "Classifier (UML)",
      "score": 0.5650702118873596
    },
    {
      "name": "Language model",
      "score": 0.5524097681045532
    },
    {
      "name": "Artificial intelligence",
      "score": 0.508995532989502
    },
    {
      "name": "Machine learning",
      "score": 0.47690320014953613
    },
    {
      "name": "Training set",
      "score": 0.4614841341972351
    },
    {
      "name": "Valuation (finance)",
      "score": 0.45175376534461975
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4166452884674072
    },
    {
      "name": "Data mining",
      "score": 0.38943952322006226
    },
    {
      "name": "Mathematics",
      "score": 0.11421144008636475
    },
    {
      "name": "Game theory",
      "score": 0.08532056212425232
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Mathematical economics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Finance",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ]
}