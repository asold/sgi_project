{
  "title": "A denoising-classification neural network for power transformer protection",
  "url": "https://openalex.org/W4312190772",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2323671635",
      "name": "Zongbo Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2150869293",
      "name": "Zaibin Jiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2163313857",
      "name": "Anyang He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1988074067",
      "name": "Nuo Xu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3129965327",
    "https://openalex.org/W2963849010",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2039375006",
    "https://openalex.org/W1983877450",
    "https://openalex.org/W2158246475",
    "https://openalex.org/W2059332919",
    "https://openalex.org/W2029659091",
    "https://openalex.org/W2050528450",
    "https://openalex.org/W2028166829",
    "https://openalex.org/W2104612563",
    "https://openalex.org/W2051855209",
    "https://openalex.org/W2067361882",
    "https://openalex.org/W1983823148",
    "https://openalex.org/W2018444483",
    "https://openalex.org/W2343421684",
    "https://openalex.org/W2963410812",
    "https://openalex.org/W2993533229",
    "https://openalex.org/W2102259958",
    "https://openalex.org/W2080975658",
    "https://openalex.org/W4233825779",
    "https://openalex.org/W2734666712",
    "https://openalex.org/W2007937350",
    "https://openalex.org/W2752928915",
    "https://openalex.org/W1544457184",
    "https://openalex.org/W2999516673",
    "https://openalex.org/W2803319818",
    "https://openalex.org/W3110525843",
    "https://openalex.org/W1745334888",
    "https://openalex.org/W2293078015",
    "https://openalex.org/W4237100288"
  ],
  "abstract": "Abstract Artificial intelligence (AI) can potentially improve the reliability of transformer protection by fusing multiple features. However, owing to the data scarcity of inrush current and internal fault, the existing methods face the problem of poor generalizability. In this paper, a denoising-classification neural network (DCNN) is proposed, one which integrates a convolutional auto-encoder (CAE) and a convolutional neural network (CNN), and is used to develop a reliable transformer protection scheme by identifying the exciting voltage-differential current curve (VICur). In the DCNN, CAE shares its encoder part with the CNN, where the CNN combines the encoder and a classifier. Based on the interaction of the CAE reconstruction process and the CNN classification process, the CAE regards the saturated features of the VICur as noise and removes them accurately. Consequently, it guides CNN to focus on the unsaturated features of the VICur. The unsaturated part of the VICur approximates an ellipse, and this significantly differentiates between a healthy and faulty transformer. Therefore, the unsaturated features extracted by the CNN help to decrease the data ergodicity requirement of AI and improve the generalizability. Finally, a CNN which is trained well by the DCNN is used to develop a protection scheme. PSCAD simulations and dynamic model experiments verify its superior performance.",
  "full_text": "Li et al. \nProtection and Control of Modern Power Systems            (2022) 7:52  \nhttps://doi.org/10.1186/s41601-022-00273-8\nORIGINAL RESEARCH\n© The Author(s) 2023. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nOpen Access\nProtection and Control of\nModern Power Systems\nA denoising-classification neural network \nfor power transformer protection\nZongbo Li1*  , Zaibin Jiao2, Anyang He2 and Nuo Xu1 \nAbstract \nArtificial intelligence (AI) can potentially improve the reliability of transformer protection by fusing multiple features. \nHowever, owing to the data scarcity of inrush current and internal fault, the existing methods face the problem of \npoor generalizability. In this paper, a denoising-classification neural network (DCNN) is proposed, one which inte-\ngrates a convolutional auto-encoder (CAE) and a convolutional neural network (CNN), and is used to develop a reli-\nable transformer protection scheme by identifying the exciting voltage-differential current curve (VICur). In the DCNN, \nCAE shares its encoder part with the CNN, where the CNN combines the encoder and a classifier. Based on the inter-\naction of the CAE reconstruction process and the CNN classification process, the CAE regards the saturated features \nof the VICur as noise and removes them accurately. Consequently, it guides CNN to focus on the unsaturated features \nof the VICur. The unsaturated part of the VICur approximates an ellipse, and this significantly differentiates between \na healthy and faulty transformer. Therefore, the unsaturated features extracted by the CNN help to decrease the data \nergodicity requirement of AI and improve the generalizability. Finally, a CNN which is trained well by the DCNN is used \nto develop a protection scheme. PSCAD simulations and dynamic model experiments verify its superior performance.\nKeywords: Transformer protection, Exciting voltage-differential current curve, Convolutional auto-encoder, \nConvolutional neural network, Denoising-classification neural network\n1 Introduction\nA power transformer is a critical element in a power \nsystem. The core issue of transformer protection is the \ndiscrimination between inrush current and an internal \nfault. Because of the advantages of simplicity and rapid \nresponse, differential protection configured with the sec -\nond harmonic restraint [1] has been widely used in power \nsystems. However, it can no longer meet the reliability \nrequirement of the increasingly complex power system \nbecause the second harmonic is inconsistent with inrush \ncurrent and an internal fault [1]. With the rapid develop -\nment of artificial intelligence (AI) [2, 3], many AI-based \nprotection schemes have emerged which fuse multiple \nfeatures. Recent work is summarized below.\n(1) The first category directly uses differential current \nas input to a machine learning (ML) algorithm to \nidentify the operating state. The adopted ML algo -\nrithms include artificial neural (ANN) [4–7], radial \nbasis function neural networks [8, 9], evolving \nneural nets [10], probabilistic neural network [11, \n12], hidden Markov model (HMM) [13], decision \ntree (DT) [14, 15], random forest (RF) [16], etc. In \nrecent years, deep neural networks have gained a \nlot of attention for developing transformer protec -\ntion. Examples are such as the accelerated convo -\nlutional neural network (CNN) presented in [17], \nand the new structure CLGNN in [18] combining a \nCNN and a light-gated recurrent unit.\n(2) The second category extracts the data features from \ndifferential current first, and then uses them as \ninput to an ML algorithm to identify the operating \nstate. In [19–24], various wavelet features of differ -\nential current are extracted by wavelet analysis and \n*Correspondence:  lizb@neepu.edu.cn\n1 School of Electrical Engineering, Northeast Electric Power University, Jilin, \nJilin, China\nFull list of author information is available at the end of the article\nPage 2 of 14Li et al. Protection and Control of Modern Power Systems            (2022) 7:52 \nused as input to the ML algorithms to build trans -\nformer protection, such as support vector machine \n(SVM) [19, 20], ANN [21], DT [22], Gaussian mixed \nmodel [23], k-nearest neighbors algorithm [24]. \nSimilarly, reference [25] uses the amplitude features \nof the primary and secondary currents as input to a \nfinite impulse response artificial neural network to \nbuild transformer protection.\nGenerally, AI demands the training samples to cover \nalmost all the scenarios in a power system. However, the \ninrush current and internal fault of the on-site trans -\nformers are small probability events whose recorded \nsamples are scarce. Therefore, the on-site transformers in \na real power system cannot meet the ergodicity require -\nment of the training samples. As a result, it is difficult for \nAI-based protection schemes to perform satisfactorily \nin a real power system. In AI application, to improve the \nclassifier’s performance, many first use a convolutional \nauto-encoder (CAE) [26] to extract the main features of \ninput data before training the classifier. However, in fact, \nthe features extracted by the CAE are not always help -\nful because they also face the problem of generalizability \nwhen the training samples are scarce.\nIn summary, to improve the generalizability of AI-\nbased protection schemes, it is critical to decrease the \ndata ergodicity requirement of AI. In this paper, a novel \ndeep neural network called a denoising-classification \nneural network (DCNN) is proposed and used to develop \nan AI-based transformer protection scheme by iden -\ntifying the exciting voltage- differential current curve \n(VICur) [27–29]. Typical VICurs are shown in Fig.  1, \nincluding normal operation, healthy transformer ener -\ngization (inrush current), internal fault, and faulty trans -\nformer energization (superposition of inrush and fault \ncurrents).\nFrom Fig.  1, the VICurs of internal fault and normal \noperation approximate ellipses with different features. \nWhen the transformer is energized, iron core saturation \ncauses the ellipse to distort irregularly. Thus, the VICurs \nof both healthy and faulty transformer energization have \nboth an unsaturated and a saturated part, where the \nunsaturated parts exhibit the same features as normal \noperation and internal fault. Clearly, the unsaturated fea -\ntures of a VICur differ significantly between a healthy and \na faulty transformer. If the adopted ML algorithm can \nfocus on the unsaturated part of the VICur and avoid the \ninfluence of the saturated part, the extracted features can \nbe used as the basis for identifying the operating states of \nthe transformer reliably, and are useful for decreasing the \nergodicity requirement of the training samples.\nThe proposed DCNN is a new deep structure integrat -\ning a CAE and a CNN. The CAE extracts the unsaturated \nfeatures of the VICur by reconstructing it as the unsatu -\nrated part while regarding the saturated part as noise for \nremoval. It shares its encoder with the CNN, and thus \nthe CNN combines the shared encoder and a classifier \nand realizes the data classification. During the training \nprocess, the DCNN achieves the interaction of the CAE \nreconstruction and the CNN classification through the \nshared encoder. Therefore, the CAE effectively guides \nthe CNN to focus on the unsaturated part of the VICur. \nFinally, by paying attention to the unsaturated part of \nthe VICur, the CNN develops a strong generalizability \nand is used to build an AI-based transformer protec -\ntion scheme. To certain extent, the developed protec -\ntion scheme in this paper can avoid the influences of the \nsaturated features of inrush current and decrease the \nergodicity requirement of the training samples. PSCAD \nsimulations and dynamic model experiments verify the \nsuperior performance of the proposed transformer pro -\ntection scheme through comparisons with existing work.\n2  Proposed denoising‑classification neural \nnetwork\nThe comprehensive features of the VICur can be exhib -\nited by its image. A CNN has a great ability to mine and \nclassify the depth features of the VICur image, and the \nCAE can guide its encoder part to extract the unsatu -\nrated features and remove the saturated features through \nthe reconstruction process of the input. We propose a \nDCNN structure to realize the interaction of the CNN \nand CAE. Through the guidance of the CAE, the CNN \ndevelops the ability to focus on the unsaturated features \na. normal operation       b. healthy transformer energization\nc. internal fault         d. faulty transformer energization\nFig. 1 VICurs under various operating states (experiments)\nPage 3 of 14\nLi et al. Protection and Control of Modern Power Systems            (2022) 7:52 \n \nof the VICur image. The extracted comprehensive fea -\ntures complement each other to reliably identify the \noperating states. Finally, the CNN trained by the DCNN \nis used to build the protection scheme.\n2.1  Input of DCNN\nThe input to the DCNN is a VICur image. Its acquisition \nprocess involves calculating and normalizing the exciting \nvoltage and differential current, and converting the dis -\ncrete data to a grayscale image. First, the exciting voltage \nand differential current are calculated. The exciting volt -\nage U is approximately equal to the primary voltage and \nthe differential current I is the sum of the primary and \nsecondary currents, as:\nwhere uk and ik  are the kth instantaneous values, and n is \nthe sample number.\nThe exciting voltage and differential current are then \nnormalized. This aims to limit the VICur into a fixed \nrange without changing the graphic features. Specifically, \nthe exciting voltage and differential current are normal -\nized by using the same maximum and minimum values, \nas:\nwhere u′\nk and i′\nk  are the normalized values, qmin and qmax \nare the minimum and maximum values of the vector Q \nshown as:\nFinally, the normalized VICur in ( 2) is converted into a \ngrayscale image, which covers the classification informa -\ntion and its size is m × m.\n(1)\nU =[ u1,... ,uk,... ,un ]T\nI =[ i1,... ,ik,... ,in ]T\n(2)\nu′\nk = 2 × (uk − qmin )/(qmax − qmin )\ni′\nk = 2 × (ik − qmin )/(qmax − qmin )\n(3)Q = [i1 ,... ,ik,... ,in,u1 ,... ,uk,... ,un]T\n2.2  Overview of the proposed DCNN\nThe proposed DCNN integrates a CAE block and \na CNN block which share the encoder, as shown in \nFig.  2. During the training process of the DCNN, the \nCAE block is trained with the objective of minimizing \nthe reconstruction loss, while the CNN block is trained \nwith the objective of minimizing the classification loss. \nThey are then used to build the transformer protection \nscheme. The details are provided below.\n2.2.1  CAE block of DCNN\nDuring the feature extraction of the VICur image, the \nCAE block regards the saturated features as noise for \nremoval and extracts the unsaturated features. Its \nstructure is shown in Fig.  3.\nThe CAE block consists of an encoder and a decoder. \nX′\nU-I  and ˜X′\nU-I  are its input and target output, respec -\ntively. The target output is the unsaturated part of the \ninput image. Unit k of the encoder involves calculations \nof convolution, batch normalization, and activation, \nand the result is written as oek.\nThe output of the encoder can be represented by ( 4), \nwhere the symbol “*” indicates the convolution calcula -\ntion, and r is the encoder’s unit number.\nFig. 2 DCNN structure illustrations\nFig. 3 CAE block illustrations\nPage 4 of 14Li et al. Protection and Control of Modern Power Systems            (2022) 7:52 \nLikewise, unit k of the decoder involves calculations of \ndeconvolution [30, 31], batch normalization, and activa -\ntion, and the result is represented as odk . The output of \nthe decoder can be represented by (5), where the symbol \n“**” indicates the deconvolution calculation, and s is the \ndecoder’s unit number.\nBased on (4) and (5), the reconstruction loss of the \nCAE block is defined by the mean square error between \nthe actual output and the target output, as:\nwhere d indicates the dth training sample, and D is the \nnumber of training samples. ˜x′\nduv and x′′\nduv refer to the \npixel values of the target output and actual output in the \nuth row and vth column, respectively. By minimizing the \nreconstruction loss in (6), the CAE block reconstructs the \nVICur image as the unsaturated part, and consequently, \nthe encoder extracts the unsaturated features. Mean -\nwhile, the CNN block of the DCNN is guided by this \nreconstruction process to develop the ability of focusing \non the unsaturated part.\n2.2.2  CNN block of DCNN\nThe CNN block realizes the data classification to iden -\ntify the operating states of the power transformer. Spe -\ncifically, it deals with a task of binary classification of the \nhealthy transformer including normal operation/external \nfault and healthy transformer energization, and faulty \ntransformer including internal fault and faulty trans -\nformer energization. Its structure is shown in Fig. 4.\nThe CNN block consists of the encoder of the CAE \nblock and a classifier. Unit k of the classifier also involves \nthe calculations of convolution, batch normalization, and \nactivation. The result is designated as ock , and therefore \nthe classifier’s output can be written as:\nwhere the function S(x) is the softmax function, and the \noutput is a 2-dimension vector, whose elements indi -\ncate the probability that the dth input image belongs to \nhealthy transformer or faulty transformer, respectively. r \nis the classifier’s unit number.\nBased on (7), the classification loss of the CNN block is \ndefined by the cross-entropy loss, as:\n(4)hen\n(\nX ′\nU-I\n)\n= oe1 ∗···∗ oek ∗···∗ oer\n(5)\n˜X ′\nU-I hde\n(\nhen\n(\nX ′\nU-I\n))\n= od1 ∗∗···∗∗ odk ∗∗···∗∗ ods\n(6)LCAE = 1\nDm 2\nD∑\nd=1\nm∑\nu=1\nm∑\nv=1\n(\nx′′\nduv − ˜x′\nduv\n)2\n(7)\nS\n(\nG\n(\nhen\n(\nX ′\nU-I\n)))\n= S(oc1 ∗···∗ ock ∗···∗ ocr)\n= [Pd0 ; Pd1]\nwhere M h and M f refer to the training samples of healthy \nand faulty transformer, respectively. With the guidance of \nthe CAE block and the objective of minimizing the classi-\nfication loss in (8), the CNN block develops the ability to \nfocus on the VICur image’s unsaturated part.\n2.2.3  Loss function of the proposed DCNN\nThe loss function of the DCNN is the weighted sum of \nthe reconstruction loss and the classification loss, given \nas:\nwhere α and β are the weights of the reconstruction loss \nand the classification loss, respectively.\nBased on the loss function (9 ), the CAE block and the \nCNN block interact through the shared encoder. From \nthe reconstruction process of DCNN, the CAE block \nguides the CNN block to focus on the unsaturated \npart of the VICur image. Conversely, according to the \nclassification results, the CNN block tests the unsatu -\nrated features extracted by the CAE block. Thus, the \nencoder parameters are determined by the CAE block \nand the CNN block together. The features extracted by \nthe encoder are suitable for both the ideal reconstruc -\ntion process and a satisfactory classification process. \nTherefore they are the optimal features for identifying \nthe operating states of the power transformer. Finally, \nthe CNN block trained by the DCNN has an improved \ngeneralization ability and is used to build a reliable pro -\ntection scheme.\n(8)LCNN =−\n∑\nd∈M h\nlog(Pd0) −\n∑\nd∈M f\nlog(Pd1)\n(9)\nLDCNN = α\nDm 2\nD�\nd=1\nm�\nu=1\nm�\nv=1\n�\nx′′\nduv − ˜x′\nduv\n�2\n− β\n\n �\nd∈M h\nlog(Pd0) +\n�\nd∈M f\nlog(Pd1)\n\n\nFig. 4 CNN block illustrations\nPage 5 of 14\nLi et al. Protection and Control of Modern Power Systems            (2022) 7:52 \n \n3  Proposed power transformer protection scheme\nFigure  5 shows the procedure of the transformer pro -\ntection scheme, including the following steps.\n(1) Calculate the differential current and the exciting \nvoltage according to (1).\n(2) Identify whether a disturbance occurs through the \nstart-up criterion. The fault components of the \nexciting voltage and differential current are used to \nconstruct the start-up criterion, as:\n where k is the kth sampling data, h is the sample \nnumber in one cycle, KU and KI are thresholds.\n(3) Obtain the VICur image. Suppose the start-up \ncriterion (10) is met at the sth sampling data, the \nobtained VICur can be represented as (11), where n \nis the sample number in the adopted data window. \nThen, the VICur in (11) is normalized through the \nmethod in (2), and is then converted into a gray -\nscale image.\n(10)\n�uk =\n⏐⏐|uk − uk−h |−|uk−h − uk−2h |\n⏐⏐ > K U\nor�ik =\n⏐⏐|ik − ik−h |−|ik−h − ik−2h |\n⏐⏐ > K I\n(4) Identify the operating states of the power trans -\nformer. The VICur image of each phase is used as \ninput to the CNN block to determine the operat -\ning states of each phase. When at least one phase \nis identified as “faulty transformer, ” the differential \nrelay sends a tripping signal.\n4  Case study\nThe training samples are collected from PSCAD simula -\ntion systems. To improve and verify the generalizability \nof the proposed protection scheme, the validation sam -\nples are obtained in the simulation system whose param -\neters are different from that of the training samples. Test \nsamples are collected in the dynamic model experiments. \nFigure 6 illustrates the equivalent model of the PSCAD \nsimulations and dynamic model experiments.\n4.1  Sample collection\nAll training samples are obtained from the step-down \ntransformer in the simulation system. The simulation \nconditions have been given full consideration, provided \nby Table  5 in the “ Appendix” .  In Table 5,  NO, EF, HTE, \nIF, and FTE  refer to  normal operation, external fault, \nhealthy transformer energization, internal fault, and \nfaulty transformer energization, respectively.  For exam -\nple, the energization time is one of the decisive factors for \nthe saturation occurrence and duration time; the faulty \nturn is an essential factor that determines the differen -\ntial current, the transformer loss, and the terminal volt -\nage. In addition, the magnetization curves, another factor \ndeciding the saturation features, are provided by Table  6 \nin the “ Appendix” . The sample numbers are 564, 478, \n760, and 760 for normal operation/external fault, healthy \ntransformer energization, internal fault, and faulty trans -\nformer energization, respectively.\nThe validation samples are obtained from a three-wind-\ning transformer whose operational conditions are also \nshown in Table 5, and its magnetization curve is provided \nin Table 6. The sample numbers are 225, 80, 160, and 160 \n(11)\nX s =\n[\n(is+1,us+1),... ,\n(\nis+k ,us+k\n)\n,... ,(is+n ,us+n )\n]T\nFig. 5 Logic diagram of transformer protection scheme\n Fig. 6 Model of simulations and experiments\nPage 6 of 14Li et al. Protection and Control of Modern Power Systems            (2022) 7:52 \nfor normal operation/external fault, healthy transformer \nenergization, internal fault, and faulty transformer ener -\ngization, respectively.\nThe test samples are obtained from an experimen -\ntal transformer consisting of three single transform -\ners. Table  7 in the “ Appendix” provides the transformer \nparameters and the experimental scenarios, e.g., the \ninternal faults are conducted on the primary or secondary \nside by connecting the contact terminals; the occurrence \ntimes of external fault, internal fault, and transformer \nenergization are set randomly; the minimum turn ratio \nof internal fault is 2.3%, etc. In Table  7, NO, EF, HTE, IF, \nand FTE refer to normal operation, external fault, healthy \ntransformer energization, internal fault, and faulty trans -\nformer energization,  respectively. The sample numbers \nare 48, 58, 63, and 54 for normal operation/external fault, \nhealthy transformer energization, internal fault, and \nfaulty transformer energization, respectively.\nBefore training the DCNN and building the protection \nscheme, the raw samples collected in the simulations and \nthe experiments are processed according to the methods \nin Sect.  2.1. Because the saturation duration of the iron \ncore is no larger than 10  ms when the residual flux is \nnot considered, a data window of 12–15 ms can contain \nsufficient unsaturated features. Herein, a data window \nof 13 ms is adopted, and the size of the VICur image is \n50 × 50. The target output of the CAE block is the image \nof the unsaturated part, and the saturated parts of the \ninput images are deleted manually.\n4.2  Selection of the optimal DCNN\nConsidering the reconstruction and classification losses, \nand the classification accuracy of VICur images compre -\nhensively, we select a relatively optimal structure shown \nin Fig.  7. The meanings of the characters, w, c, s, and p, \nhave also been given in Fig.  7. “Conv2d” , “ConvTrans-\npose2d” , “ReLU, ” and “BatchNorm” refer to 2-dimensional \nFig. 7 Structure of a relatively optimal DCNN\nTable 1 Effects of weights on the performance of the DCNN\nWeight Training Validation Test\nACC (%) RecLoss ClaLoss ACC (%) RecLoss ClaLoss ACC (%)\n0.2/1.0 99.84 0.4279 0.7798 99.52 0.2655 9.7790 93.27\n0.4/1.0 99.88 0.3967 1.7331 99.84 0.2299 2.8638 96.41\n0.6/1.0 99.61 9.7129 2.5993 99.20 4.4010 6.6541 93.27\n0.8/1.0 96.45 9.7129 11.0904 97.60 4.4010 5.4481 92.83\n1.0/1.0 99.45 9.7129 3.6392 99.36 4.4010 7.4553 95.96\n1.1/1.0 99.84 0.3074 1.7329 99.68 0.1914 1.7233 95.07\n1.2/1.0 99.84 9.7129 1.4730 99.36 4.4010 3.8111 93.27\n1.3/1.0 100.00 0.3236 0.6932 99.68 0.2155 13.9073 92.83\n1.4/1.0 99.80 9.7129 1.9064 99.52 4.4010 0.6932 95.07\n1.5/1.0 99.96 0.2226 0.2600 99.68 0.1702 5.0990 96.86\n1.6/1.0 99.41 0.2357 2.7451 99.36 0.1744 23.6253 93.27\n1.7/1.0 99.92 0.9992 0.9991 99.84 0.1803 0.6478 96.41\n1.8/1.0 99.80 9.7129 2.8593 99.20 4.4010 12.0977 93.72\n1.9/1.0 99.92 9.7129 1.0397 99.52 4.4010 6.9627 95.07\n2.0/1.0 99.61 9.7129 1.2131 99.84 4.4010 4.5306 93.27\nPage 7 of 14\nLi et al. Protection and Control of Modern Power Systems            (2022) 7:52 \n \nconvolution, 2-dimensional deconvolution, ReLU func -\ntion, and batch normalization, respectively.\nTable  1 summarizes the reconstruction and classifi -\ncation performance of the DCNN with different loss \nweights. In Table 1, ACC, RecLoss, ClaLoss refer to accu-\nracy, reconstruction loss,  and  classification loss, respec -\ntively. The weight β of the classification loss is set to 1.0, \nand the weight α of the reconstruction loss ranges from 0 \nto 2.0. As can be seen from Table  1, the DCNNs with the \nweights below perform the best (the following results are \ngiven in order of the reconstruction loss, classification \nloss, and classification accuracy):\n(1) α = 0.4, β = 1.0. The results of training samples \nare 0.3967, 1.7331, and 99.88%, respectively. The \nresults of validation samples are 0.2299, 2.8638, and \n99.84%, respectively.\n(2) α = 1.5, β = 1.0. The results of training samples \nare 0.2226, 0.2600, and 99.96%, respectively. The \nresults of validation samples are 0.1702, 5.0990, and \n99.68%, respectively.\n(3) α = 1.7, β = 1.0. The results of training samples \nare 0.9992, 0.9991, and 99.92%, respectively. The \nresults of validation samples are 0.1803, 0.6478, and \n99.84%, respectively.\nThe CNN blocks of the DCNNs above are then used as \nthe alternatives to classify the test samples. As expected, \nthey show strong generalization ability, with accuracies of \n96.38%, 96.84%, and 96.59%, respectively. Therefore, any \none of them is promising for building a reliable protec -\ntion scheme. In the following section, the DCNN with \nloss weights of 1.5 and 1.0 is taken as an example to detail \nthe training process and the advantages of the proposed \nprotection scheme.\n4.3  Training and test process of the selected DCNN\nFigure  8 details the training process of the selected \nDCNN with weights of 1.5 and 1.0. According to Fig.  8a, \nthe reconstruction and classification losses decrease \ngradually as the iteration time increases. As shown in \nFig. 8b, when the iteration progresses to the 100th time, \nthe total losses of the training and validation samples \ndrop to below 0.1 and 20, respectively.\nFigure  9 shows the reconstruction results of several \nVICur images in the validation samples, indicating that \nthe CAE block of the DCNN extracts the unsaturated \nfeatures effectively. The classification accuracies of the \ntraining and validation samples increase to 99.96% and \n99.68%, respectively. It indicates that the CNN block of \nthe DCNN has the desired classification performance. \nThe test samples are used to test the generalizability of \nthe CNN.\nSince they are affected by the complicated transient \nenvironment of the experiments, the test samples have \ndifferent features from the training samples. Because \nof these different features, the CAE block of the DCNN \nfails to completely remove the saturated parts of the test \nsamples, as can be seen from the reconstruction results \nof the partial VICur images in Fig.  10. However, the \nreconstructed images still contain sufficient unsaturated \nfeatures because the DCNN has the ability to focus on \nthe unsaturated part of the VICur image. In particular, \nfor normal operation/external fault, healthy transformer \nenergization, and internal fault, the CAE block of the \nDCNN has satisfactory reconstruction performance. \nHowever, since the differential current of faulty trans -\nformer energization is the superposition of fault current \nand inrush current, which have certain similarities, there \nare no distinct dividing points between the unsaturated \nand saturated parts. As a result, it is inevitable that the \nFig. 8 Training and validation results\nPage 8 of 14Li et al. Protection and Control of Modern Power Systems            (2022) 7:52 \nCAE block performs worse to the faulty transformer \nenergization. However, the faulty and saturated features \nare still significantly different from the unsaturated fea -\ntures of the healthy transformer. Therefore, the recon -\nstruction results of the faulty transformer energization \ndo not affect the correct identification of the CNN block.\nEquation  (12) details the classification results, where \nACC is the classification accuracy of the test samples, \nTP and TN are the respective scenario numbers that the \nhealthy and faulty transformers are identified correctly. \nFP and FN are the scenario numbers of faulty and healthy \ntransformers that are wrongly identified, respectively.\nIn (12), all the scenarios identified wrongly are related \nto the healthy transformer energization due to longer \nsaturation duration resulting from remanence. With no \nconsideration of remanence, the data window of 13  ms \nadopted in the proposed protection scheme contains \nsufficient unsaturated features for the operating state \nidentification. However, it is difficult for the dynamic \nmodel experiments to fully eliminate the effects of rema -\nnence when collecting the test samples. Affected by the \n(12)\nACC = TP + TN\nTP + TN + FP + FN\n= 99+ 117\n99+ 117+ 0 + 7 × 100% = 96.86%\nFig. 9 Reconstruction results of validation samples\nFig. 10 Reconstruction results of test samples\nFig. 11 An example of healthy transformer identified wrongly\nPage 9 of 14\nLi et al. Protection and Control of Modern Power Systems            (2022) 7:52 \n \nremanence, the unsaturated features may be insufficient \nfor some test samples, as in an example shown in Fig. 11.\nIn Fig.  11a, after the transformer is energized, the iron \ncore is saturated between 0.6786 and 0.6908 s in the first \ncycle. Therefore, the duration time of differential current \nsaturation is 12.2  ms. Consequently, the VICur image \nin Fig.  11b only contains unsaturated features of 0.8 ms. \nInevitably, it is wrongly identified as a “faulty trans -\nformer” by the CNN block.\nThe training samples fully consider various simulation \nconditions but the simulated scenarios are far from cov -\nering all possible scenarios. In particular, the validation \nand test samples have different operational conditions \nfrom the training samples. However, from the reconstruc-\ntion and classification results, the proposed DCNN can \neffectively extract the unsaturated features and reliably \nidentify the operating states of the power transformers. \nIt effectively proves that the DCNN helps the CNN block \ndecrease the ergodicity requirements. Therefore, the CNN \nblock is promising for building an AI-based transformer \nprotection scheme with a strong generalizability.\n4.4  Comparisons with common neural network\nComparisons with common neural networks are made \nto verify the improved generalizability of the proposed \nDCNN, including:\n(1) CNN. The VICur images are directly used as input \nof a CNN to identify the operating states of the \npower transformer.\n(2) CAE and classifier. The CAE extracts the features by \nreconstructing the VICur image as the unsaturated \npart. They are then used as input of a classifier to \nidentify the operating states of the power transformer.\nThese two neural networks adopt the same structure, same \ninitial values, and same samples as the DCNN in Sect. 4.3.\n4.4.1  Comparison with CNN\nFigure 12 shows the training process of the CNN. As can \nbe seen, the classification accuracies of the training and \nvalidation samples increase gradually as the classification \nloss decreases. When iterating to the 70th step, the clas -\nsification loss and accuracy become stable. Finally, the \nclassification accuracies of the training and validation sam-\nples reach 99.84% and 99.52%, respectively. It seems that \nthe CNN is trained well and has good performance on \nthe simulation samples. The test samples are also used to \ntest the generalizability of the CNN. From the test results, \nthe classification accuracy of the samples is only 92.83%, \nwhich indicates the CNN’s poor generalizability. The CNN \nblock of the DCNN develops the ability of focusing on the \nunsaturated part by the guidance of the CAE block. There-\nfore it has better generalizability, and can classify the train-\ning, validation, and test samples reliably with classification \naccuracies of 99.96%, 99.68%, and 96.86%, respectively.\n4.4.2  Comparison with CAE and classifier\nFigure  13 shows the training processes of the CAE and \nthe classifier. The final reconstruction loss shown in \nFig. 13a is lower than 0.2, so it seems that the CAE has \nextracted sufficient main features of the VICur images. \nThe output of the CAE encoder is then used as input to \ntrain the classifier. During the training process shown \nin Fig.  13b, its parameters are updated, and the encoder \nparameters are fine-tuned to decrease the classification \nloss. With the decrease of classification loss, the classifi -\ncation accuracies of the training and validation samples \nincrease gradually and finally reach 100% and 99.68%, \nrespectively. Hence, the classifier based on the features \nextraction of the CAE has good classification perfor -\nmance on both the training and validation samples.\nFurther, the generalizability of the CAE and the classifier \nis tested on the test samples. Compared with the CNN in \nSect. 4.4.1, the classifier has better generalizability with a \nclassification accuracy of 95.07% due to the feature extrac-\ntion process of the CAE. However, because the training pro-\ncesses of the encoder and the classifier are independent, the \ntraining process of the encoder doesn’t consider the effects \nof the updated parameters on the classification performance \nof the classifier. Therefore, the features extracted by the \nencoder are not the optimal ones for the sample classifica-\ntion of the classifier. Comparatively, the proposed DCNN \nrealizes the interaction of the CAE block’s reconstruction \nprocess and the CNN block’s classification process during \nthe process of the DCNN. Therefore, the features extracted \nFig. 12 Training and validation results of CNN\nPage 10 of 14Li et al. Protection and Control of Modern Power Systems            (2022) 7:52 \nby the encoder of the DCNN are optimal for the sample \nclassification of the CNN block. From the results, the CNN \nblock of the DCNN has the best generalization ability with a \nclassification accuracy of 96.86% for the test samples.\n4.4.3  Comparisons with different data size\nThe size of the training samples is adjusted further to \ncompare the classification performance of the three neu -\nral networks. The new training samples are randomly \nselected from the original training samples in the propor-\ntions of 15%, 30%, 45%, 60%, 75%, and 90%. The valida -\ntion and test samples, the neural networks, and the initial \nparameters are unchanged. Table 2 compares the results, \nafter the training and test processes, of the classification \naccuracies and the determined weights of the DCNN. \nFrom Table 2, it can be seen that as the size of the train -\ning samples increases, the classification performance of \nthe three neural networks improves noticeably. They per-\nform similarly and have good generalizability to the vali -\ndation samples, but perform quite differently to the test \nsamples. Comparatively, the CNN block of the DCNN \nwith the suitable weights has the best generalizability \nunder any size of training samples. For instance, when \nthe size of the training samples is 75%, the CNN block \nof the DCNN with the weights of 1.6 and 1.0 performs \nthe best with a classification accuracy of 95.07% for the \ntest samples, whereas the classification accuracies of the \nother two neural networks are only 91.93% and 93.72%. \nThe results in Table  2 demonstrate that the CNN block \ndetermined by the proposed DCNN has significantly \nimproved generalizability.\n4.5  Comparisons with other methods\nThis section compares this proposed protection scheme \nwith other schemes as briefly described below. We high -\nlight its advantages.\n(1) Traditional second harmonic restraint (Scheme 1). \nThe threshold of this harmonic restraint method is \n15%.\n(2) ANN-based protection scheme (Scheme 2). The dif-\nferential current in one cycle is used as input to the \nANN to identify the operating states of the power \ntransformer. It adopts a structure of double hidden \nFig. 13 Training and validation results\nTable 2 Comparisons under different training samples size\nData \nvolume (%)\nCNN (%) Classifier based on CAE (%) CNN block of DCNN (%)\nTraining Validation Test Training Validation Test Training Validation Test Weights (α/β)\n15 95.04 90.72 84.75 97.93 92.48 87.44 99.49 92.96 88.78 1.2/1.0\n30 98.83 96.00 87.89 100 98.24 90.58 100 98.88 91.93 1.3/1.0\n45 99.38 97.92 90.13 99.84 97.28 90.58 99.80 98.08 92.38 1.2/1.0\n60 99.92 99.68 91.93 99.92 99.36 92.38 99.92 98.24 94.17 1.4/1.0\n75 99.34 98.88 91.93 99.84 99.04 93.72 99.96 99.84 95.07 1.6/1.0\n90 99.96 99.36 92.83 99.73 98.88 93.72 99.69 99.36 95.52 1.4/1.0\n100 99.84 99.52 92.83 100 99.68 95.07 99.96 99.68 96.86 1.5/1.0\nPage 11 of 14\nLi et al. Protection and Control of Modern Power Systems            (2022) 7:52 \n \nlayer, where the neuron numbers of the first and the \nsecond hidden layers are 550 and 10, respectively.\n(3) RF-based protection scheme (Scheme  3). The dif -\nferential current in one cycle is used as input of RF \nto identify the operating states of each phase. The \ndecision tree number of the selected RF is 50, where \nthe maximum depth is 20. The node splitting of \neach decision tree is based on the Gini index.\n(4) Wavelet transform and SVM based protection \nscheme (Scheme  4) [20]. The detailed compo -\nnents (d2-d4) of the differential current, which are \nextracted through db4 mother over level d4, are \ndivided into four equal sections. Then the average \nenergy of the three phases in each section is com -\nputed. Finally, the average energy is used as input \nto an SVM to identify the operating states of the \npower transformer.\n(5) Geometric features of the VICur and SVM-based \nprotection scheme (Scheme 5) [27]. The inclination \nangle, the major axis and the ellipticity of the VICur \nare calculated by the methods provided in [27]. The \nthree features are combined as a feature vector used \nas input to the SVM to identify the operating states. \nThe parameters, c and g, are 18.3792 and 0.3789, \nrespectively.\nThe above five schemes adopt the same training samples \nas the proposed protection scheme, as shown in Table  5. \nThe classification accuracy of the test scenarios in Table 7 \nis used to compare the classification performance. As these \nschemes require the training and test scenarios to adopt \nthe same sampling rate, the sampling rate is thus adjusted \nto 10 kHz in this comparison. Table 3 summarizes the clas-\nsification results of Schemes 1–5 after the training and test \nprocesses, and the proposed protection scheme.\nFrom Table 3, Scheme  1 performs better than schemes \n2–5 with an accuracy of 94.21% for the identification of \ninternal faults and healthy transformer energization. How-\never, it has the highest rejection rate of 11.11% when an \ninternal fault occurs because of the harmonics which can-\nnot be ignored. In addition, Scheme 1 is obviously unsuit-\nable for identifying faulty transformer energization and \na normal operational/external fault. Schemes 2–4 have \nsatisfactory performance for faulty transformer energiza -\ntion, normal operational/ external faults, and internal \nfaults. However, they perform worse with a higher mal -\nfunction rate when a healthy transformer is energized, and \nspecifically, the identification accuracies of healthy trans-\nformer energization are only 77.59%, 75.86%, and 81.03%. \nExcept for the scenarios of normal operational/external \nfault, Scheme  5 performs the best for the identification \nof healthy transformer energization, internal faults, and \nfaulty transformer energization, compared with Schemes \n1–4. Comparatively, the proposed protection scheme in \nthis paper shows the best classification performance for all \nscenarios. When an internal fault occurs or a healthy trans-\nformer is energized, its rejection rate and malfunction rate \nare the lowest with classification accuracies of 100% and \n87.93%, respectively. It can also identify faulty transformer \nenergization and normal operational/external fault reliably. \nIn summary, the proposed protection scheme is superior, \nindicating that the proposed DCNN improves the perfor-\nmance of the AI-based transformer scheme.\n4.6  Run time test\nTo test the runtime of the proposed protection scheme, \nthis section uses the Python language and deploys it on \nthe developer kit of  NVIDIA® Jetson AGX Xavier ™. The \ntest platform is shown in Fig.  14 and the parameters of \nthe developer kit are provided in Table 4.\nThe runtime test by the platform only involves the \ncomputation time t of the protection procedure, includ -\ning data window length tdw , data processing time tdp , and \ncomputation time tct of the CNN block. From the test \nresults, the computation time of the proposed protection \nscheme is 25.92 ms, as:\nTable 3 Comparison results of classification accuracy (%)\nScenarios Scheme 1 Scheme 2 Scheme 3 Scheme 4 Scheme 5 Proposed \nscheme\nInternal fault 88.89 94.21 98.41 88.43 96.83 86.78 98.41 90.08 98.41 93.39 100 99.17\nHealthy transformer energization 100 77.59 75.86 81.03 87.93 87.93\nFaulty transformer energization 42.59 90.74 90.74 96.30 96.30 100\nNormal operation/external fault 18.75 100 100 100 95.83 100\nAll the scenarios 65.47 91.48 90.58 93.72 94.62 96.86\nFig. 14 Platform for runtime test\nPage 12 of 14Li et al. Protection and Control of Modern Power Systems            (2022) 7:52 \nAlthough the runtime reaches 25.92  ms, it still meets \nthe requirements of relay protection. Considering the \nclassification accuracy of the test samples, the proposed \nDCNN based transformer protection scheme has a cer -\ntain practicability.\n5  Conclusion\nThe power transformer plays an essential role in a power \nsystem. Therefore, its protection scheme is a critical issue. \nThe traditional protection scheme is differential protection \nconfigured with a second harmonic restraint. It has been \nwidely used in power systems. However, as the power sys-\ntem becomes increasingly complex, this differential pro -\ntection can no longer meet requirements. On the other \nhand, previously proposed AI-based protection schemes \nare not being accepted by the power system because of the \nergodicity requirement of the training samples.\nIn this paper, a new deep structure called DCNN is \nproposed to decrease the ergodicity requirement of the \ntraining samples, and a reliable transformer protection \nscheme is developed by using the DCNN to identify the \nVICur image. The DCNN is an integration of a CAE \nblock and a CNN block, where the CAE block shares its \nencoder with the CNN block. The DCNN uses the CAE \nblock to reconstruct the VICur image as the unsaturated \npart and uses the CNN block to classify the training sam -\nples. Because of the interaction process in the encoder, \nthe CAE block guides the CNN block to focus on the \nunsaturated part of the VICur image. Because the unsat -\nurated part of the VICur approximates an ellipse and dif -\nfers distinctly between a healthy and faulty transformer, \nthe ergodicity requirement of the training samples is \n(13)\nt = tdw + tdw + tdw\n= (13 + 4.14+ 8.78)ms\n= 25.92ms\ndecreased significantly. Therefore, the CNN block trained \nby the DCNN is helpful for building an AI-based trans -\nformer protection scheme with a strong generalization \nability. PSCAD simulations and dynamic experiments \nshow that the proposed protection scheme is a promising \nprospect for power systems.\nAppendix\nSee Tables 5, 6 and 7.\nTable 4 Parameters of developer kit\nGraphics processing \nunit (GPU)\n512-core Volta GPU with Tensor Cores\nCentral processing unit \n(CPU)\n8-core Advanced Reduced Instruction Set \nComputer Machine v8.2 64-bit CPU, 8 MB \nL2 + 4 MB L3\nMemory 16 GB 256-Bit Low Power Double Data Rate \nFourth Generation × 137 GB/s\nStorage 32 GB embedded multimedia card  5.1\nDeep learning accelera-\ntor\n(2×) NVDLA Engines*\nVision accelerator 7-way Very Long Instruction Word Vision \nProcessor\nEncoder/decoder (2×) 4Kp60 I High Efficiency Video Coding/(2x) \n4Kp60 12-Bit Support\nSize 105 mm × 105 mm\nDeployment Module (Jetson AGX Xavier)\nTable 5 Training and validation samples in PSCAD\nTraining Validation\n1st one 2nd one\nParameters\nConnection Y/Δ-11 Δ/Y/Y-11\nRatio 230/11 kV 230/35 kV 35/220/500 kV\nLeakage resistance 0.1 p.u\nCopper loss 0.005 p.u\nIron loss 5%, 7%, 10% 6%\nSampling frequency 20 kHz 2, 10 kHz\nScenarios\nNO Start-up time: 1.001 s, 1.002 s, …, 1.020 s\nEF Occurrence time: 1.001 s, 1.002 s, …, 1.020 s; \nType: single phase-to-ground, phase-to-\nphase, two phase-to-ground faults\nHTE Energization time: 1.001 s, 1.002 s, …, 1.020 s\nIF Occurrence time: 1.001 s, 1.002 s, …, 1.020 s;\nFault turns: 1.5%, 2.0%, 2.5%, 3.0%, and 5.0%\nFTE Energization time: 1.001 s, 1.002 s, …, 1.020 s;\nFault turns: 1.5%, 2.0%, 2.5%, 3.0%, and 5.0%\nNumbers\nNO/EF 564 225\nHTE 478 80\nIF 760 160\nFTE 760 160\nTable 6 Magnetization curves of training samples (p.u.)\nTraining: 1st one Training: 2nd one Validation\nVoltage Current Voltage Current Voltage Current\n0 0 0 0 0 0\n0.1 9.2280e−05 1.0 0.0025 0.10 0.0000533\n0.5 4.4895e−04 1.1 0.005 0.50 0.000259\n1.0 1.04754e−03 1.2 0.02 1.00 0.000605\n1.1 7.18316e−03 1.25 0.04 1.10 0.00415\n1.2 9.95666e−02 1.28 0.1 1.20 0.0575\n1.4 0.6285765 1.32 0.2 1.40 0.3629\n1.6 1.352529 1.36 0.3 1.60 0.78088\n1.8 2.179491 1.535 1 1.80 1.25834\n1.9 2.6178888 3.7 10 1.90 1.51145\nPage 13 of 14\nLi et al. Protection and Control of Modern Power Systems            (2022) 7:52 \n \nAbbreviations\nAI: Artificial intelligence; CAE: Convolutional auto-encoder; CNN: Convo-\nlutional neural network; DCNN: Denoising-classification neural network; \nVICur: Exciting voltage-differential current curve; ML: Machine learning; ANN: \nArtificial neural network; HMM: Hidden Markov model; DT: Decision tree; RF: \nRandom forest; SVM: Support vector machine; NO: Normal operation; EF: \nExternal fault; HTE: Healthy transformer energization; IF: Internal fault; FTE: \nFaulty transformer energization.\nAcknowledgements\nNot applicable.\nAuthor contributions\nAll authors contributed to the research, read and approved the manuscript. \nZJ proposed the initial concept of VICur, analyzed the feasible of VICur in the \nAI-based transformer protection scheme, and gave technique guidance in the \nwhole research process. ZL modeled the simulation system, conducted the \nexperiments, proposed this new neural network in this paper, and achieved \nits successful application in the transformer protection, and wrote the manu-\nscript. AH checked the training and test results, built the platform to test the \nruntime of the proposed protection scheme. NX completed the comparison \nwork, wrote the response letter and corrected the manuscript format.\nFunding\nThis work was supported by the National Natural Science Foundation of China \n(Grant No.: 20210333).\nAvailability of data and materials\nPlease contact author for data and material request.\nDeclarations\nCompeting interests\nThe authors declare that they have no known competing financial interests \nor personal relationships that could have appeared to influence the work \nreported in this paper.\nAuthor details\n1 School of Electrical Engineering, Northeast Electric Power University, Jilin, \nJilin, China. 2 School of Electrical Engineering, Xi’an Jiaotong University, Xi’an, \nShaanxi, China. \nReceived: 21 August 2022   Accepted: 6 December 2022\nReferences\n 1. Medeiros, R. P ., Costa, F. B., Silva, K. M., Muro, J. D. J. C., Júnior, J. R. L., & \nPopov, M. (2022). A clarke-wavelet-based time-domain power trans-\nformer differential protection. IEEE Transactions on Power Delivery, 37(1), \n317–328.\n 2. Haenlein, M., & Kaplan, A. (2019). A brief history of artificial intelligence: \nOn the past, present, and future of artificial intelligence. California \nManagement Review, 61(4), 5–14.\n 3. LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, \n521(7553), 436–444.\n 4. Perez, L. G., Flechsig, A. J., Meador, J. L., & Obradovic, Z. (1994). Training \nan artificial neural network to discriminate between magnetizing \ninrush and internal faults. IEEE Transactions on Power Delivery, 9(1), \n434–441.\n 5. Balaga, H., Gupta, N., & Vishwakarma, D. N. (2015). GA trained parallel \nhidden layered ANN based differential protection of three phase \npower transformer. International Journal of Electrical Power & Energy \nSystems, 67, 286–297.\n 6. Segatto, E. C., & Coury, D. V. (2006). A differential relay for power \ntransformers using intelligent tools. IEEE Transactions on Power Systems, \n21(3), 1154–1162.\n 7. Geethanjali, M., Slochanal, S. M. R., & Bhavani, R. (2008). PSO trained \nANN-based differential protection scheme for power transformers. \nNeurocomputing, 71(4–6), 904–918.\n 8. Moravej, Z., Vishwakarma, D. N., & Singh, S. P . (2003). Application of \nradial basis function neural network for differential relaying of a power \ntransformer. Computers & Electrical Engineering, 29(3), 421–434.\n 9. Tripathy, M., Maheshwari, R. P ., & Verma, H. K. (2008). Radial basis proba-\nbilistic neural network for differential protection of power transformer. \nIET Generation Transmission & Distribution, 2(1), 43–52.\n 10. Moravej, Z. (2005). Evolving neural nets for protection and condition \nmonitoring of power transformer. Electric Power Components and \nSystems, 33(11), 1229–1236.\n 11. Tripathy, M., Maheshwari, R. P ., & Verma, H. K. (2010). Power transformer \ndifferential protection based on optimal probabilistic neural network. \nIEEE Transactions on Power Delivery, 25(1), 102–112.\n 12. Tripathy, M., Maheshwari, R. P ., & Verma, H. K. (2007). Probabilistic \nneural-network-based protection of power transformer. IET Electric \nPower Applications, 1(5), 793–798.\n 13. Ma, X. X., & Shi, J. (2000). A new method for discrimination between \nfault and magnetizing inrush current using HMM. Electric Power Systems \nResearch, 56(1), 43–49.\n 14. Samantaray, S. R., & Dash, P . K. (2011). Decision tree based discrimina-\ntion between inrush currents and internal faults in power trans-\nformer. International Journal of Electrical Power & Energy Systems, 33(4), \n1043–1048.\n 15. Ozgonenel, O., & Karagol, S. (2014). Power transformer protection \nbased on decision tree approach. IET Electric Power Applications, 8(7), \n251–256.\n 16. Shah, A. M., & Bhalja, B. R. (2016). Fault discrimination scheme for power \ntransformer using random forest technique. IET Generation Transmission & \nDistribution, 10(6), 1431–1439.\n 17. Afrasiabi, S., Afrasiabi, M., Parang, B., & Mohammadi, M. (2020). Integration \nof accelerated deep neural network into power transformer differential \nprotection. IEEE Transactions on Industrial Informatics, 16(2), 865–876.\n 18. Afrasiabi, S., Afrasiabi, M., Parang, B., & Mohammadi, M. (2020). Design-\ning a composite deep learning based differential protection scheme of \npower transformers. Applied Soft Computing, 87, 105975.\n 19. Shah, A. M., & Bhalja, B. R. (2013). Discrimination between internal \nfaults and other disturbances in transformer using the support vector \nTable 7 Test samples in experiments\nParameters\nConnection Y/Δ-11 Ratio 220/220 V\nNo-load current,\nLoss\n1.17%,\n0.7%\nShort-circuit voltage,\nLoss\n11.2%,\n1.0%\nRated capacity 2 kVA Sampling frequency 10 kHz\nInsulation level E Product type BMD-10\nScenarios\nNO Load: 100%, 75%, 50%, 25% of rated load;\nStart-up time: random time\nEF Load: 100% of rated load; Occurrence time: ran-\ndom time; Type: single phase-to-ground, phase-\nto-phase; Location: primary, secondary sides\nHTE Energization time: random time\nIF Load: 100% of rated load; Occurrence time: \nrandom\nFault turns: primary side, 2.3%, 4.5%; secondary \nside, 4.5%\nFTE Occurrence time: random time; Fault turns: \nprimary side, 2.3%, 4.5%, 9.1%; secondary side, \n4.5%, 9.1%\nNumbers\nNO/EF 48 HTE 58\nIF 63 FTE 54\nPage 14 of 14Li et al. Protection and Control of Modern Power Systems            (2022) 7:52 \nmachine-based protection scheme. IEEE Transactions on Power Delivery, \n28(3), 1508–1515.\n 20. Jazebi, S., Vahidi, B., & Jannati, M. (2011). A novel application of wavelet \nbased SVM to transient phenomena identification of power transformers. \nEnergy Conversion and Management, 52(2), 1354–1363.\n 21. Mao, P . L., & Aggarwal, R. K. (2001). A novel approach to the classification \nof the transient phenomena in power transformers using combined \nwavelet transform and neural network. IEEE Transactions on Power Delivery, \n16(4), 654–660.\n 22. Bagheri, S., Moravej, Z., & Gharehpetian, G. B. (2018). Classification and \ndiscrimination among winding mechanical defects, internal and external \nelectrical faults, and inrush current of transformer. IEEE Transactions on \nIndustrial Informatics, 14(2), 484–493.\n 23. Jazebi, S., Vahidi, B., Hosseinian, S. H., & Faiz, J. (2009). Magnetizing inrush \ncurrent identification using wavelet based Gaussian mixture models. \nSimulation Modelling Practice and Theory, 17(6), 991–1010.\n 24. Thote, P . B., Daigavane, M. B., Daigavane, P . M., & Gawande, S. P . (2017). An \nintelligent hybrid approach using KNN-GA to enhance the performance \nof digital protection transformer scheme. Canadian Journal of Electrical \nand Computer Engineering-Revue Canadienne de Genie Electrique et Infor-\nmatique, 40(3), 151–161.\n 25. Orille, A. L., Khalil, N., & Valencia, J. A. V. (1999). A transformer differential \nprotection based on finite impulse response artificial neural network. \nComputers & Industrial Engineering, 37(1–2), 399–402.\n 26. Yu, J. B., & Zhou, X. K. (2020). One-dimensional residual convolutional \nautoencoder based characteristic learning for gearbox fault diagnosis. \nIEEE Transactions on Industrial Informatics, 16(10), 6347–6358.\n 27. Jiao, Z. B., & Li, Z. B. (2018). Novel magnetization hysteresis-based power-\ntransformer protection algorithm. IEEE Transactions on Power Delivery, \n33(5), 2562–2570.\n 28. Li, Z. B., Jiao, Z. B., & He, A. Y. (2020). Knowledge-based artificial neural \nnetwork for power transformer protection. IET Generation Transmission & \nDistribution, 14(24), 5782–5791.\n 29. Li, Z. B., Jiao, Z. B., & He, A. Y. (2021). Knowledge-based convolutional neu-\nral networks for transformer protection. CSEE Journal of Power and Energy \nSystems, 7(2), 270–278.\n 30. Noh, H., Hong, S., & Han, B. (2015) Learning deconvolution network for \nsemantic segmentation. In 2015 IEEE International Conference on Computer \nVision.\n 31. Zeiler, M. D., Krishnan, D., Taylor, G. W., & Fergus, R. (2010) Deconvolutional \nnetworks. In 2010 IEEE Computer Society Conference on Computer Vision \nand Pattern Recognition.",
  "topic": "Convolutional neural network",
  "concepts": [
    {
      "name": "Convolutional neural network",
      "score": 0.7279597520828247
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6681997776031494
    },
    {
      "name": "Transformer",
      "score": 0.5328961610794067
    },
    {
      "name": "Noise reduction",
      "score": 0.498516321182251
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.496051162481308
    },
    {
      "name": "Artificial neural network",
      "score": 0.49580034613609314
    },
    {
      "name": "Computer science",
      "score": 0.4804903566837311
    },
    {
      "name": "Deep learning",
      "score": 0.45368799567222595
    },
    {
      "name": "Classifier (UML)",
      "score": 0.44323253631591797
    },
    {
      "name": "Engineering",
      "score": 0.41312819719314575
    },
    {
      "name": "Machine learning",
      "score": 0.3672308325767517
    },
    {
      "name": "Voltage",
      "score": 0.18986529111862183
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I179060312",
      "name": "Northeast Electric Power University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I87445476",
      "name": "Xi'an Jiaotong University",
      "country": "CN"
    }
  ]
}