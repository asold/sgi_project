{
    "title": "Evaluating Large Language Models in Process Mining: Capabilities, Benchmarks, and Evaluation Strategies",
    "url": "https://openalex.org/W4392736374",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2114425813",
            "name": "Alessandro Berti",
            "affiliations": [
                "Fraunhofer Institute for Applied Information Technology",
                "RWTH Aachen University"
            ]
        },
        {
            "id": "https://openalex.org/A4319908999",
            "name": "Humam Kourani",
            "affiliations": [
                "RWTH Aachen University",
                "Fraunhofer Institute for Applied Information Technology"
            ]
        },
        {
            "id": "https://openalex.org/A4379822417",
            "name": "Hannes Häfke",
            "affiliations": [
                "Fraunhofer Institute for Applied Information Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2146769890",
            "name": "Chiao-Yun Li",
            "affiliations": [
                "Fraunhofer Institute for Applied Information Technology",
                "RWTH Aachen University"
            ]
        },
        {
            "id": "https://openalex.org/A1968738271",
            "name": "Daniel Schuster",
            "affiliations": [
                "RWTH Aachen University",
                "Fraunhofer Institute for Applied Information Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2114425813",
            "name": "Alessandro Berti",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4319908999",
            "name": "Humam Kourani",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4379822417",
            "name": "Hannes Häfke",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2146769890",
            "name": "Chiao-Yun Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1968738271",
            "name": "Daniel Schuster",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4243932450",
        "https://openalex.org/W4319793302",
        "https://openalex.org/W4385261902",
        "https://openalex.org/W4383473044",
        "https://openalex.org/W4383605161",
        "https://openalex.org/W4387075000",
        "https://openalex.org/W4390694561",
        "https://openalex.org/W4380355783",
        "https://openalex.org/W6794686226",
        "https://openalex.org/W4384919587",
        "https://openalex.org/W4389520779",
        "https://openalex.org/W4390573130",
        "https://openalex.org/W4386552569",
        "https://openalex.org/W2997566538",
        "https://openalex.org/W4363671827",
        "https://openalex.org/W4384268538",
        "https://openalex.org/W4388747974",
        "https://openalex.org/W4380993239",
        "https://openalex.org/W4386435778",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W2979582415",
        "https://openalex.org/W4226053975",
        "https://openalex.org/W4389524379",
        "https://openalex.org/W4389911350",
        "https://openalex.org/W4385292963",
        "https://openalex.org/W4387210548",
        "https://openalex.org/W4360980513",
        "https://openalex.org/W4389151826",
        "https://openalex.org/W4381586841",
        "https://openalex.org/W4383176435",
        "https://openalex.org/W4386184788",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4386555953",
        "https://openalex.org/W4385645323",
        "https://openalex.org/W4380353763",
        "https://openalex.org/W4365601026",
        "https://openalex.org/W4308244910"
    ],
    "abstract": null,
    "full_text": "Evaluating Large Language Models in Process\nMining: Capabilities, Benchmarks, and\nEvaluation Strategies\nAlessandro Berti1,2 , Humam Kourani1,2 , Hannes H¨ afke1 , Chiao-Yun\nLi1,2 , Daniel Schuster1,2\n1 Fraunhofer FIT, Sankt Augustin, Germany\n2 Process and Data Science Chair, RWTH Aachen University, Aachen, Germany\n{alessandro.berti,humam.kourani, hannes.haefke, chiao-yun.li,\ndaniel.schuster}@fit.fraunhofer.de\nAbstract. Using Large Language Models (LLMs) for Process Mining\n(PM) tasks is becoming increasingly essential, and initial approaches\nyield promising results. However, little attention has been given to de-\nveloping strategies for evaluating and benchmarking the utility of incor-\nporating LLMs into PM tasks. This paper reviews the current implemen-\ntations of LLMs in PM and reflects on three different questions. 1) What\nis the minimal set of capabilities required for PM on LLMs? 2) Which\nbenchmark strategies help choose optimal LLMs for PM? 3) How do we\nevaluate the output of LLMs on specific PM tasks? The answer to these\nquestions is fundamental to the development of comprehensive process\nmining benchmarks on LLMs covering different tasks and implementa-\ntion paradigms.\nKeywords: Large Language Models (LLMs)· Output Evaluation· Bench-\nmarking Strategies.\n1 Introduction\nProcess mining (PM) is a data science field focusing on deriving insights about\nbusiness process executions from event data recorded by information systems [1].\nSeveral types of PM exist, including process discovery (learning process models\nfrom event data), conformance checking (comparing event data with process\nmodels), and process enhancement (adding frequency/performance metrics to\nprocess models). Although many automated methods exist for PM, human an-\nalysts usually handle process analysis due to the need for domain knowledge.\nRecently, LLMs have emerged as conversational interfaces trained on exten-\nsive data [28], achieving near-human performance in various general tasks [38].\nTheir potential in PM lies in embedded domain knowledge useful for generat-\ning database queries and insights [21], logical and temporal reasoning capabilities\n[2,16], inference abilities over structured data [12]. Prior research has asserted\nthe usage of LLMs for PM tasks [3,4]. However, a comprehensive discussion on\narXiv:2403.06749v3  [cs.DB]  5 Apr 2024\n2 A. Berti et al.\nFig. 1: Outline of the contributions of this paper.\nnecessary capabilities for PM, LLMs’ suitability evaluation for process analytics,\nand assessment of LLMs’ outputs in the PM context is lacking.\nThe three main contributions of this paper are summarized in Fig. 1. First,\nbuilding upon prior work [3,4] proposing textual abstractions of process mining\nartifacts and an experimental evaluation of LLMs’ responses, the essential ca-\npabilities that LLMs must have for PM tasks are derived in Section 3.1. The\naforementioned capabilities allow us to narrow down the field of LLMs to those\nthat meet these requirements. Next, evaluation benchmarks for selecting suit-\nable LLMs are introduced in Section 3.2, incorporating both process-mining-\nspecific and general criteria such as reasoning, visual understanding, factuality,\nand trustworthiness. Finally, we suggest automatic, human, and self-assessment\nmethods for evaluating LLMs’ outputs on specific tasks in Section 3.3, aiming\nto establish a comprehensive PM benchmark and enhance confidence in LLMs’\nusage, addressing potential issues like hallucination.\nThis paper provides an orientation to process mining researchers investigating\nthe usage of LLMs, i.e., this paper aims to facilitate PM-on-LLMs research.\n2 Background\nLLMs enhance PM with superior capabilities, handling complex tasks through\ndata understanding and natural language processing. This section covers PM\ntasks with LLMs (Section 2.1) and the adopted implementation paradigms (Sec-\ntion 2.2) along with the provision of additional domain knowledge.\n2.1 Process Mining Tasks for LLMs\nThis subsection explores a range of PM tasks in which LLMs have already been\nadopted for process mining research. LLMs facilitate the automation of gener-\nating textual descriptions from process data, handling inputs such as event logs\nEvaluating Large Language Models in Process Mining 3\nor formal process models [4]. They also generate process models from textual\ndescriptions, with studies showing LLMs creating BPMN models and declara-\ntive constraints from text [7]. In the realm of anomaly detection, LLMs play a\ncrucial role in identifying process data anomalies, including unusual activities\nand performance bottlenecks, offering context-aware detection that adapts to\nnew patterns through prompt engineering. This improves versatility over tradi-\ntional methods [3,4]. For root cause analysis , LLMs analyze event logs to sug-\ngest causes of anomalies or inefficiencies, linking delays to specific conditions\nor events. This goes beyond predefined logic, employing language processing for\ncontext-aware analysis [3,4] In ensuring fairness, LLMs identify and mitigate\nbias in processes, suggesting adjustments. They analyze processes like recruit-\nment to detect disparities in rejection rates or delays by gender or nationality,\naiding in fair decision-making [22,4]. LLMs can also interpret and explain vi-\nsual data, including complex visualizations, by describing event flows in dotted\ncharts and identifying specific patterns, such as batch processing. For process\nimprovement, after PM tasks identify and analyze problems, LLMs can suggest\nactions and propose new process constraints [22,4].\n2.2 Implementation Paradigms of Process Mining on LLMs\nTo effectively employ LLMs for PM tasks, specific implementation paradigms\nare required [3,4]. This section outlines key approaches for implementing LLMs\nin PM tasks. We distinguish three main strategies:\n– Direct provision of insights : A prompt is generated that merges data ab-\nstractions with a query about the task. Also, interactive dialogue between\nthe LLM and the user is possible for step-by-step analysis. The user starts\nwith a query and refines or adjusts it based on the LLM’s feedback, con-\ntinuing until achieving the desired detail or accuracy, such as pinpointing\nprocess inefficiencies. For instance, to have LLMs identifying unusual behav-\nior in an event log, we combine a textual abstraction of the log (such as the\ndirectly-follows graph or list of process variants) with a question like “Can\nyou analyze the log to detect any unusual behavior?”\n– Code generation: LLMs can be used to create structured queries, like SQL,\nfor advanced PM tasks [11]. Rather than directly asking LLMs for answers,\nusers command LLMs to craft database queries from natural language. These\nqueries are then executed on the databases holding PM information. It is\napplicable to PM tasks that can be converted into database queries, such as\nfiltering event logs or computing the average duration of process steps. Also,\nLLMs can be used to generate executable programs that use existing PM\nlibraries to infer insights over the event data [9].\n– Automated hypotheses generation: Combining the previous strategies by us-\ning textual data abstraction to prompt LLMs for autonomous hypotheses\ngeneration [3,4]. The hypotheses are accompanied by SQL queries for verifi-\ncation against event data. Results confirm or refute these hypotheses, with\npotential for LLM-suggested refinements of hypotheses.\n4 A. Berti et al.\nLLMs may require additional knowledge about processes and databases to\nimplement PM tasks, for example, in anomaly detection and crafting accurate\ndatabase queries. Some strategies are used to equip LLMs with this additional\ndomain knowledge [14], including fine-tuning and prompt engineering.\n3 Evaluating LLMs in Process Mining\nThis section introduces criteria for selecting LLMs that are suitable for PM\ntasks. Moreover, we introduce criteria for evaluating their outputs. First, in\nSection 3.1, we discuss the fundamental capabilities needed for PM (long context\nwindow, acceptance of visual prompts, coding, factuality). Then, we introduce in\nSection 3.2 general-purpose and process-mining-specific benchmarks to measure\nthe different LLMs on process-mining-related tasks. To foster the development\nof process-mining-specific benchmarks and to be able to evaluate a given output,\nwe propose in Section 3.3 different methods to evaluate the output of an LLM.\n3.1 LLMs Capabilities Needed for Process Mining\nIn this section, we discuss four important capabilities of LLMs for PM tasks:\n– Long Context Window: Event logs in PM often include a vast amount of cases\nand events, challenging the context window limit of LLMs, which restricts\nthe token count in a prompt [13]. Moreover, also the textual specification\nof process models requires a significant amount of information. The context\nwindow limit can be severe in many currently popular LLMs. 3 Even simple\nabstractions like the ones introduced in [3] (directly-follows graph, list of\nprocess variants) may exceed this limitation. The context window, which is\nset during model training, must be large enough for the data size. Recent\nefforts aim to extend this limit, though quality may decline [13,20].\n– Accepting Visual Prompts : Visualizations in PM, such as the dotted chart\nand the performance spectrum [15], summarizing process behavior, empower\nanalysts to spot interesting patterns not seen in tables. Interpreting visual\nprompts is key for semi-automated PM. Large Visual Models (LVMs) use ar-\nchitectures similar to language models trained on annotated image datasets\n[31]. They perform tasks like object detection and image synthesis, recogniz-\ning patterns, textures, shapes, colors, and spatial relations. 4\n– Coding (Text-to-SQL) Capabilities: With the context window limit prevent-\ning full event log inclusion in prompts, generating scripts and database\nqueries is crucial for analyzing event data. As discussed in Section 2.2,\ntext-to-SQL assists in filtering and analyzing event data. Key requirements\nfor text-to-SQL in PM include understanding database schemas, perform-\ning complex joins, using database-specific operators (e.g., for calculating\ndate differences), and translating PM concepts into queries. Overall, modern\nLLMs offer excellent coding capabilities [3].\n3 https://community.openai.com/t/are-the-full-8k-gpt-4-tokens-available-on-chatgpt/237999\n4 GPT-4 and Google Bard/Gemini are popular models supporting both visual and\ntextual prompts.\nEvaluating Large Language Models in Process Mining 5\n– Factuality: LLM hallucination involves generating incorrect or fabricated in-\nformation [24]. Factuality measures an LLM’s ability to cross-check its out-\nputs against real facts or data, crucial for PM tasks like anomaly detection\nand root cause analysis. This may involve leveraging external databases [19],\nknowledge bases, or internet search [32] for validation. For instance, verify-\ning the sequence Cancel Order” followed by Deliver Order” against public\ndata in anomaly detection. LLMs with web browsing can access up-to-date\ninformation, enhancing factuality.5\n3.2 Relevant LLMs Benchmarks\nAfter identifying the required capabilities for LLMs in PM, benchmarking strate-\ngies are essential to measure the quality of the textual outputs returned by the\nLLMs satisfying such capabilities.\nConsidering the wide array of available benchmarks for assessing LLMs be-\nhavior, we focus on identifying those most relevant to PM capabilities. In [5], a\ncomprehensive collection of benchmarks is introduced. This section aims to se-\nlect and utilize some of these benchmarks to evaluate various aspects of LLMs’\nperformance in PM contexts.\n– Traditional benchmarks: Textual prompts are crucial for LLMs evaluation in\nPM. Benchmarks like AGIEval assess models via standardized exams [37],\nand MT-Bench focuses on conversational and instructional capabilities [36].\nAnother benchmark evaluates LLMs on prompts of long size [6].\n– Domain knowledge benchmarks : Domain knowledge is essential for LLMs\nin PM to identify anomalies using metrics and context. Benchmarks like\nXIEZHI assess knowledge across different fields (economics, science, engi-\nneering) [8], while ARB evaluates expertise in areas like mathematics and\nnatural sciences [26].\n– Visual benchmarks: Understanding PM visualizations, such as dotted charts,\nis essential (c.f. Section 3.1). LLMs must accurately process queries on these\nvisualizations. MMBench tests models on image tasks [17], and MM-Vet\nassesses recognition, OCR, among others [35]. Yet, they may not fully meet\nPM visualization analysis needs, particularly in evaluating line orientations\nand point size/color.\n– Benchmarks for Text-to-SQL : In PM, generating SQL from natural lan-\nguage is key for tasks like event log filtering. Benchmarks such as SPIDER\nand SPIDER-realistic test LLMs on text-to-SQL conversion [23]. The APPS\nbenchmark evaluates broader code generation abilities [10].\n– Fairness benchmarks: they evaluate LLM fairness in PM by analyzing group\ntreatment and bias detection. DecodingTrust measures LLM trustworthiness,\ncovering toxicity, bias, robustness, privacy, ethics, and fairness [30].\n– Benchmarking the generation of hypotheses : LLMs’ ability to generate hy-\npotheses from event data is vital to implement semi-autonomous PM agents.\n5 https://cointelegraph.com/news/chat-gpt-ai-openai-browse-internet-no-longer-limited-info-2021\n6 A. Berti et al.\nTable 1: Implementation paradigms and benchmarks for LLMs in the context of\ndifferent PM tasks.\nTask Paradigms Benchmarks Classes\nDirect Provision\nCode Generation\nHypotheses Generation\nTraditional\nDomain Knowledge\nVisual Prompts\nText-to-SQL\nFairness\nHypotheses Generation\nProcess DescriptionX X X\nProcess ModelingX X X X X X X\nAnomaly DetectionX X X X X X\nRoot Cause AnalysisX X X X X X\nEnsuring FairnessX X X X X X X\nExpl. and InterpretingX X XVisualizations\nProcess ImprovementX X X X X X X X\nWhile specific benchmarks for hypothesis generation are lacking, related\nstudies like [29] and [34] evaluate LLMs using scientific papers.\nIn Table 1, we link process mining (PM) tasks to implementation paradigms\nand benchmarks. We discuss these tasks:\n– Process description requires understanding technical terms relevant to the\ndomain, crucial for accurately describing processes.\n– Process modeling involves generating models from text, using SQL for declar-\native and BPMN XML for procedural models. LLMs should offer various\nmodel hypotheses.\n– Anomaly detection and root cause analysis need domain knowledge to ana-\nlyze process sequences or identify event attribute combinations causing is-\nsues.\n– Fairness involves detecting biases by analyzing event attributes and values,\nnecessitating hypothesis generation by LLMs.\n– Explaining and interpreting visualizations requires extracting features from\nimages and texts, offering contextual insights, like interpreting performance\nspectrum visualization [15].\n– Process improvement entails suggesting text proposals or new constraints to\nenhance current models, leveraging code generation capabilities and under-\nstanding process limitations.\nWhile general-purpose benchmarks are already developed and are easily ac-\ncessible, they are not entirely suited for the task of PM-on-LLMs. In particu-\nlar, visual capabilities (explaining and interpreting PM visualizations) and au-\ntonomous hypotheses generation require more PM-specific benchmarks. How-\never, little research exists on PM-specific benchmarks [3,4].\n3.3 How to Evaluate LLMs Outputs\nThis section outlines criteria for assessing the quality of outputs generated by\nLLMs in PM tasks, serving two primary objectives. The first objective is to as-\nEvaluating Large Language Models in Process Mining 7\nsist users in identifying and addressing hallucinations and inaccuracies in LLMs’\noutputs. The second aim is to establish criteria for developing an extensive bench-\nmark specifically tailored to PM applications of LLMs. The strategies follow:\n– Automatic evaluation is particularly suited for text-to-SQL tasks. In this\ncontext, the formal accuracy and conciseness (indicated by the length of the\nproduced query) of the SQL queries generated can be efficiently assessed.\nAdditionally, the creation of declarative constraints, designed to enhance\nprocess execution, can also be evaluated in terms of their formal correctness.\n– Human evaluation is essential for LLM tasks like direct querying and hy-\npothesis generation. For direct querying tasks such as anomaly detection\nand root cause analysis, important criteria are recall (the model’s ability\nto identify expected insights) and precision (the correctness of insights).\nThese criteria also apply to hypothesis generation. Additionally, evaluating\nthe feedback cycle’s effectiveness in validating original hypotheses is crucial\nfor these tasks.\n– Self-evaluation in LLMs tackles hallucinations, as noted by [24]. Techniques\ninclude chain-of-thought, where LLMs detail their reasoning, enhancing ex-\nplanations [33]. Confidence scores let LLMs assess their insights’ reliability,\ndiscarding uncertain outputs for quality [27]. Ensembling, or using results\nfrom multiple LLM sessions, increases accuracy via majority voting or confi-\ndence checks [18]. Self-reflection, an LLM reviewing its or another’s output,\ndetects errors [25]. In anomaly detection, using confidence scores to exclude\ndoubtful anomalies and ensembling to confirm detections across sessions im-\nproves reliability.\n4 Conclusion\nThis paper examines LLM applications in PM, offering three main contributions:\nidentification of necessary LLM capabilities for PM, review of benchmarks from\nliterature, and strategies for evaluating LLM outputs in PM tasks. These strate-\ngies aim to build confidence in LLM use and establish benchmarks to assess LLM\neffectiveness across PM implementations.\nOur discussion centers on current generative AI capabilities within PM, an-\nticipating advancements like deriving event logs from videos. Despite future en-\nhancements, the criteria discussed here should remain pertinent. Benchmarking\nfor PM tasks on large language models (LLMs) will evolve, including both general\nand PM-specific benchmarks, yet the foundational aspects and methodologies are\nexpected to stay consistent.\nReferences\n1. van der Aalst, W.M.P.: Process Mining - Data Science in Action, Second Edition.\nSpringer (2016)\n8 A. Berti et al.\n2. Bang, Y., Cahyawijaya, S., et al., N.L.: A Multitask, Multilingual, Multimodal\nEvaluation of ChatGPT on Reasoning, Hallucination, and Interactivity (2023).\nhttps://doi.org/10.48550/arXiv.2302.04023\n3. Berti, A., Qafari, M.S.: Leveraging Large Language Models (LLMs) for Process\nMining (Technical Report) (2023). https://doi.org/10.48550/arXiv.2307.12701\n4. Berti, A., Schuster, D., van der Aalst, W.M.P.: Abstractions, Scenarios, and\nPrompt Definitions for Process Mining with LLMs: A Case Study (2023).\nhttps://doi.org/10.48550/arXiv.2307.02194\n5. Chang, Y., Wang, X., et al., J.W.: A Survey on Evaluation of Large Language\nModels (2023). https://doi.org/10.48550/arXiv.2307.03109\n6. Dong, Z., Tang, T., et al., J.L.: BAMBOO: A Comprehensive Benchmark for\nEvaluating Long Text Modeling Capacities of Large Language Models (2023).\nhttps://doi.org/10.48550/arXiv.2309.13345\n7. Grohs, M., Abb, L., et al., N.E.: Large Language Models Can Accomplish Business\nProcess Management Tasks. In: BPM 2023 International Workshops. Lecture Notes\nin Business Information Processing, vol. 492, pp. 453–465. Springer (2023)\n8. Gu, Z., Zhu, X., et al., H.Y.: Xiezhi: An Ever-Updating Benchmark for Holistic\nDomain Knowledge Evaluation (2023). https://doi.org/10.48550/arXiv.2306.05783\n9. H¨ arer, F.: Conceptual model interpreter for Large Language Models. In: ER 2023.\nCEUR Workshop Proceedings, vol. 3618. CEUR-WS.org (2023)\n10. Hendrycks, D., Basart, S., et al., S.K.: Measuring Coding Challenge Competence\nWith APPS. In: NeurIPS Datasets and Benchmarks 2021 (2021)\n11. Jessen, U., Sroka, M., Fahland, D.: Chit-Chat or Deep Talk: Prompt Engineering\nfor Process Mining (2023). https://doi.org/10.48550/arXiv.2307.09909\n12. Jiang, J., Zhou, K., et al., Z.D.: StructGPT: A General Framework for Large\nLanguage Model to Reason over Structured Data. In: EMNLP 2023. pp. 9237–\n9251. Association for Computational Linguistics (2023)\n13. Jin, H., Han, X., et al., J.Y.: LLM Maybe LongLM: Self-Extend LLM Context\nWindow Without Tuning (2024). https://doi.org/10.48550/arXiv.2401.01325\n14. Kampik, T., Warmuth, C., et al., A.R.: Large Process Models: Busi-\nness Process Management in the Age of Generative AI (2023).\nhttps://doi.org/10.48550/arXiv.2309.00900\n15. Klijn, E.L., Fahland, D.: Performance Mining for Batch Processing Using the Per-\nformance Spectrum. In: BPM 2019 International Workshops. Lecture Notes in\nBusiness Information Processing, vol. 362, pp. 172–185. Springer (2019)\n16. Liu, H., Ning, R., et al., Z.T.: Evaluating the Logical Reasoning Ability of Chat-\nGPT and GPT-4 (2023). https://doi.org/10.48550/arXiv.2304.03439\n17. Liu, Y., Duan, H., et al., Y.Z.: MMBench: Is Your Multi-modal Model an All-\naround Player? (2023). https://doi.org/10.48550/arXiv.2307.06281\n18. Lu, K., Yuan, H., et al., R.L.: Routing to the Expert: Effi-\ncient Reward-guided Ensemble of Large Language Models (2023).\nhttps://doi.org/10.48550/arXiv.2311.08692\n19. Pan, S., Luo, L., et al., Y.W.: Unifying Large Language Models and Knowledge\nGraphs: A Roadmap (2023). https://doi.org/10.48550/arXiv.2306.08302\n20. Peng, B., Quesnelle, J., et al., H.F.: YaRN: Efficient Context Window Extension\nof Large Language Models (2023). https://doi.org/10.48550/arXiv.2309.00071\n21. Petroni, F., Rockt¨ aschel, T., et al., S.R.: Language Models as Knowledge Bases? In:\nEMNLP-IJCNLP 2019. pp. 2463–2473. Association for Computational Linguistics\n(2019)\n22. Qafari, M.S., van der Aalst, W.M.P.: Fairness-Aware Process Mining. In: CoopIS\n2019. Lecture Notes in Computer Science, vol. 11877, pp. 182–192. Springer (2019)\nEvaluating Large Language Models in Process Mining 9\n23. Rajkumar, N., Li, R., Bahdanau, D.: Evaluating the Text-to-SQL Capabilities of\nLarge Language Models (2022). https://doi.org/10.48550/arXiv.2204.00498\n24. Rawte, V., Chakraborty, S., et al., A.P.: The Troubling Emergence of Halluci-\nnation in Large Language Models - An Extensive Definition, Quantification, and\nPrescriptive Remediations. In: EMNLP 2023. pp. 2541–2573. Association for Com-\nputational Linguistics (2023)\n25. Ren, J., Zhao, Y., et al., T.V.: Self-Evaluation Improves Selective Generation in\nLarge Language Models (2023)\n26. Sawada, T., Paleka, D., et al., A.H.: ARB: Advanced Reasoning Benchmark for\nLarge Language Models (2023). https://doi.org/10.48550/arXiv.2307.13692\n27. Singh, A.K., Devkota, S., et al., B.L.: The Confidence-Competence\nGap in Large Language Models: A Cognitive Study (2023).\nhttps://doi.org/10.48550/arXiv.2309.16145\n28. Teubner, T., Flath, C.M., et al., C.W.: Welcome to the Era of ChatGPT et al.\nBus. Inf. Syst. Eng. 65(2), 95–101 (2023)\n29. Tong, S., Mao, K., Huang, Z., Zhao, Y., Peng, K.: Automating Psychological Hy-\npothesis Generation with AI: Large Language Models Meet Causal Graph (2023)\n30. Wang, B., Chen, W., et al., H.P.: DecodingTrust: A Compre-\nhensive Assessment of Trustworthiness in GPT Models (2023).\nhttps://doi.org/10.48550/arXiv.2306.11698\n31. Wang, J., Liu, Z., et al., L.Z.: Review of Large Vision Models and Visual Prompt\nEngineering (2023). https://doi.org/10.48550/arXiv.2307.00855\n32. Wang, L., Ma, C., et al., X.F.: A Survey on Large Language Model based Au-\ntonomous Agents (2023). https://doi.org/10.48550/arXiv.2308.11432\n33. Wei, J., Wang, X., et al., D.S.: Chain-of-Thought Prompting Elicits Reasoning in\nLarge Language Models. In: NeurIPS 2022 (2022)\n34. Yang, Z., Du, X., et al., J.L.: Large Language Models for Au-\ntomated Open-domain Scientific Hypotheses Discovery (2023).\nhttps://doi.org/10.48550/arXiv.2309.02726\n35. Yu, W., Yang, Z., et al., L.L.: MM-Vet: Evaluating Large Multimodal Models for\nIntegrated Capabilities (2023). https://doi.org/10.48550/arXiv.2308.02490\n36. Zheng, L., Chiang, W., et al., Y.S.: Judging LLM-as-a-judge with MT-Bench and\nChatbot Arena (2023). https://doi.org/10.48550/arXiv.2306.05685\n37. Zhong, W., Cui, R., et al., Y.G.: AGIEval: A Human-Centric Benchmark for Eval-\nuating Foundation Models (2023). https://doi.org/10.48550/arXiv.2304.06364\n38. Zhou, Y., Muresanu, A.I., et al., Z.H.: Large Language Models are Human-Level\nPrompt Engineers. In: ICLR 2023. OpenReview.net (2023)"
}