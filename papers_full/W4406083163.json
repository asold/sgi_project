{
  "title": "The architecture of language: Understanding the mechanics behind LLMs",
  "url": "https://openalex.org/W4406083163",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5093393081",
      "name": "Andrea Filippo Ferraris",
      "affiliations": [
        "University of Bologna",
        "Vrije Universiteit Brussel"
      ]
    },
    {
      "id": "https://openalex.org/A3169780131",
      "name": "Davide Audrito",
      "affiliations": [
        "University of Bologna"
      ]
    },
    {
      "id": "https://openalex.org/A2153458393",
      "name": "Luigi Di Caro",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A238079813",
      "name": "Cristina Poncib√≤",
      "affiliations": [
        "University of Turin"
      ]
    },
    {
      "id": "https://openalex.org/A5093393081",
      "name": "Andrea Filippo Ferraris",
      "affiliations": [
        "Vrije Universiteit Brussel",
        "University of Bologna"
      ]
    },
    {
      "id": "https://openalex.org/A3169780131",
      "name": "Davide Audrito",
      "affiliations": [
        "University of Bologna"
      ]
    },
    {
      "id": "https://openalex.org/A2153458393",
      "name": "Luigi Di Caro",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A238079813",
      "name": "Cristina Poncib√≤",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6682631176",
    "https://openalex.org/W6781031682",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W4392402185",
    "https://openalex.org/W4210647952",
    "https://openalex.org/W3209632425",
    "https://openalex.org/W4402692877",
    "https://openalex.org/W4295857769",
    "https://openalex.org/W2070150502",
    "https://openalex.org/W6839169259",
    "https://openalex.org/W6777615688",
    "https://openalex.org/W3097529099",
    "https://openalex.org/W3103368673",
    "https://openalex.org/W2036705578",
    "https://openalex.org/W1995341919",
    "https://openalex.org/W4390189088",
    "https://openalex.org/W4382202657",
    "https://openalex.org/W4391319559",
    "https://openalex.org/W2069143585",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W4404658388",
    "https://openalex.org/W4322766882",
    "https://openalex.org/W2040870580",
    "https://openalex.org/W4385571011",
    "https://openalex.org/W4294631187",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W6851175542",
    "https://openalex.org/W2147800946",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4283733745",
    "https://openalex.org/W2766736793",
    "https://openalex.org/W4353015365"
  ],
  "abstract": "Abstract Large language models (LLMs) have significantly advanced artificial intelligence (AI) and natural language processing (NLP) by excelling in tasks like text generation, machine translation, question answering and sentiment analysis, often rivaling human performance. This paper reviews LLMs‚Äô foundations, advancements and applications, beginning with the transformative transformer architecture, which improved on earlier models like recurrent neural networks and convolutional neural networks through self-attention mechanisms that capture long-range dependencies and contextual relationships. Key innovations such as masked language modeling and causal language modeling underpin leading models like Bidirectional encoder representations from transformers (BERT) and the Generative Pre-trained Transformer (GPT) series. The paper highlights scaling laws, model size increases and advanced training techniques that have driven LLMs‚Äô growth. It also explores methodologies to enhance their precision and adaptability, including parameter-efficient fine-tuning and prompt engineering. Challenges like high computational demands, biases and hallucinations are addressed, with solutions such as retrieval-augmented generation to improve factual accuracy. By discussing LLMs‚Äô strengths, limitations and transformative potential, this paper provides researchers, practitioners and students with a comprehensive understanding. It underscores the importance of ongoing research to improve efficiency, manage ethical concerns and shape the future of AI and language technologies.",
  "full_text": "Cambridge Forum on AI: Law and Governance (2025), 1, e11, 1‚Äì19\ndoi:10.1017/cfl.2024.16\nPOSITION PAPER\nThe architecture of language: Understanding the\nmechanics behind LLMs\nAndrea Filippo Ferraris1,2\n , Davide Audrito3, Luigi Di Caro4 and Cristina Poncib√≤5\n1Department of Law, University of Bologna, Bologna, Italy;2DIKE and the Law Faculty, Vrije Universiteit, Brussel, Belgium;\n3Computer Science Department, University of Torino, and Legal Studies Department, University of Bologna, Bologna, Italy;\n4Computer Science Department, University of Torino, Torino, Italy and5Department of Law, University of Turin, Torino,\nItaly\nCorresponding author: Cristina Poncib√≤; Email:cristina.poncibo@unito.it\n(Received 1 December 2024; accepted 11 December 2024)\nAbstract\nLarge language models (LLMs) have significantly advanced artificial intelligence (AI) and natural language\nprocessing (NLP) by excelling in tasks like text generation, machine translation, question answering and\nsentiment analysis, often rivaling human performance. This paper reviews LLMs‚Äô foundations, advance-\nments and applications, beginning with the transformative transformer architecture, which improved on\nearlier models like recurrent neural networks and convolutional neural networks through self-attention\nmechanisms that capture long-range dependencies and contextual relationships. Key innovations such\nas masked language modeling and causal language modeling underpin leading models like Bidirectional\nencoder representations from transformers (BERT) and the Generative Pre-trained Transformer (GPT)\nseries. The paper highlights scaling laws, model size increases and advanced training techniques that have\ndriven LLMs‚Äô growth. It also explores methodologies to enhance their precision and adaptability, includ-\ning parameter-efficient fine-tuning and prompt engineering. Challenges like high computational demands,\nbiases and hallucinations are addressed, with solutions such as retrieval-augmented generation to improve\nfactual accuracy. By discussing LLMs‚Äô strengths, limitations and transformative potential, this paper pro-\nvides researchers, practitioners and students with a comprehensive understanding. It underscores the\nimportance of ongoing research to improve efficiency, manage ethical concerns and shape the future of\nAI and language technologies.\nKeywords: large language models (LLMs); artificial intelligence (AI); natural language processing (NLP)\n1. Introduction\nLarge language models (LLMs) have revolutionized artificial intelligence (AI) and natural language\nprocessing (NLP) by achieving unprecedented proficiency in understanding and generating human\nlanguage. Built upon the transformative transformer architecture, LLMs excel in tasks such as text\ngeneration, machine translation, question answering and sentiment analysis, often matching or\nsurpassing human performance (Naveed et al.,2024).\nThis paper provides a brief overview of LLMs, touching upon their theoretical foundations,\ntechnical advancements and practical applications. We begin by introducing the transformer archi-\ntecture, which addressed the limitations of earlier models like recurrent neural networks (RNNs) and\nconvolutional neural networks (CNNs) through self-attention mechanisms that capture long-range\n¬© The Author(s), 2025. Published by Cambridge University Press. This is an Open Access article, distributed under the terms of the Creative\nCommons Attribution licence (http://creativecommons.org/licenses/by/4.0), which permits unrestricted re-use, distribution and\nreproduction, provided the original article is properly cited.\nhttps://doi.org/10.1017/cfl.2024.16 Published online by Cambridge University Press\n2 Andrea Filippo Ferraris et al.\ndependencies and contextual relationships. We explore how scaling laws, increased model sizes\nand advanced training techniques have propelled LLMs to new heights. Key innovations such as\nmasked language modeling (MLM) and causal language modeling (CLM) underpin models like\nBERT (Devlin et al.,2019) and the GPT series. The paper also examines practical methodologies\nthat enhance the adaptability and precision of LLMs, including fine-tuning strategies like parameter-\nefficient fine-tuning (PEFT) and techniques such as prompt engineering. We address challenges\nassociated with LLMs, such as computational demands, biases and hallucinations ‚Äì where mod-\nels generate plausible but incorrect information ‚Äì and present solutions like retrieval-augmented\ngeneration (RAG) to improve factual accuracy.\nBy outlining both the capabilities and limitations of LLMs, this paper aims to provide a founda-\ntional understanding for legal researchers, practitioners and students. We emphasize the transforma-\ntive potential of these models in shaping the future of AI and language technologies, underscoring\nthe importance of ongoing research to enhance efficiency and address ethical considerations.\n2. Understanding the context: NLP and neural networks\nNLP is a multidisciplinary field that combines linguistics, computer science and machine learning\nto enable machines to interpret and generate human language. Early NLP systems relied heavily on\nrule-based methods, which required extensive domain knowledge and were limited in scalability.\nThese were soon replaced by statistical approaches and simple neural networks like the perceptron\n(Rosenblatt, 1958), which could learn basic patterns from data. These methods were soon replaced\nby statistical approaches and simple neural networks (McCulloch & Pitts,1943) like theperceptron,\nwhich could learn basic patterns from data.\n2.1. Neural networks: Foundations and challenges\nNeural networks, inspired by the structure of the human brain, consist of layers of interconnected\nnodes or ‚Äúneurons.‚Äù These neurons process input data by applyingweights1 (scaling factors) and\nbiases2 (offsets) before passing the result through an activation function. Early models like the\nperceptron were capable of handling simple classification tasks by adjusting these parameters to\nminimize errors, as quantified by a loss function 3 (Terven, Cordova-Esparza, Ramirez-Pedraza,\nChavez-Urbiola & Romero-Gonzalez,2024) ‚Äì a measure of the difference between predicted and\nactual outputs. However, these models were limited to linear decision boundaries and struggled with\nmore complex, nonlinear problems.\nThe introduction of the backpropagation algorithm (Werbos, 1974) marked a significant\nadvancement, allowing neural networks to adjust weights and biases more effectively usinggra-\ndient descent . This method calculates gradients of the loss function to iteratively update the net-\nwork‚Äôs parameters. Despite this breakthrough, deeper networks encountered thevanishing gradient\n1A weight in a neural network controls the importance of an input in influencing the output of a neuron. It adjusts the\ninput‚Äôs value before summing it with others, effectively determining how much the input will impact the neuron‚Äôs decision.\nHigher weights give more significance to certain inputs, guiding the network‚Äôs behavior.\n2Bias acts as an offset added to the weighted sum of inputs, allowing the activation threshold to shift. This adjustment helps\nthe model better fit the data by enabling more flexible decision boundaries, ensuring the neuron can activate even when inputs\nare zero or minimal. It functions like the intercept in a linear equation, improving the network‚Äôs ability to generalize across\ndiverse input patterns.\n3The loss function refers to a measure of how far off a neural network‚Äôs predictions are from the actual values. When Paul\nWerbos introduced the backpropagation algorithm, it allowed networks to adjust their internal parameters, such as weights\nand biases, by calculating the error through this loss function. The goal during training is to minimize the loss, which indicates\nthat the model‚Äôs predictions are becoming more accurate. The backpropagation algorithm updates the network‚Äôs parameters\nto reduce this loss iteratively, leading to improved model performance over time.\nhttps://doi.org/10.1017/cfl.2024.16 Published online by Cambridge University Press\nCambridge Forum on AI: Law and Governance 3\nproblem4 (Hochreiter,1998), where gradients diminished as they propagated backward, slowing or\nhalting the learning process in earlier layers.\n2.2. Hardware and architectural advances\nThe resurgence of neural networks in the 21st century was driven by advancements in hardware,\nparticularly graphics processing units (GPUs), which enabled efficient parallel computation. These\nimprovements made it feasible to train deeper networks on large datasets, resulting in breakthroughs\nintaskslikecomputervisionandspeechrecognition.However,neuralnetworksstillfacedlimitations\nin handling sequential data and long-range dependencies, crucial for many NLP tasks.\n2.3. From RNNs and CNNs to transformers\nToaddressthesechallenges,moreadvancedarchitecturesweredeveloped. RNNs (Rumelhart,Hinton\n& Williams,1986) introduced feedback loops to retain information across time steps, making them\nsuitable for sequential data. Similarly,CNNs (LeCun et al.,1989; Lecun, Bottou, Bengio, & Haffner,\n1998), designed for grid-like data such as images, provided local pattern detection. While these\narchitectures offered improvements, they still struggled with scalability and efficiently capturing\nlong-range dependencies in NLP tasks.\nThe introduction of transformers revolutionized NLP by addressing these challenges, offering\nsuperior handling of context and enabling parallel processing of large datasets. This innovation laid\nthe groundwork for the development of LLMs, which can capture intricate language patterns and\nperform complex tasks with remarkable accuracy and fluency.\n3. Transformers based architecture: a new paradigm\nIntroducedintheseminal2017paper Attention is All Y ou Need(Vaswanietal., 2017),thetransformer\narchitecture fundamentally shifted the way models process and understand sequential data by elimi-\nnating the need for recurrent and CNNs traditionally used in language models. Instead, transformers\nrely on a mechanism called self-attention, which allows them to consider the entire input sequence\nsimultaneously rather than processing it step-by-step.\n4. Self-attention mechanism\nSelf-attention is the key innovation of the transformer architecture. Unlike recurrent networks,\nwhich process data in order, or convolutional networks, which focus on local patterns, transformers\nenable each word or token in the input to weigh the relevance of every other word in the sequence. It\nenables the model to weigh the relevance of each word (or token) in the input sequence with respect\nto each other word, capturing long-range dependencies and contextual relationships more effectively.\nMathematically, self-attention operates as follows:\n1. Input representation:\nGiven an input sequence of tokens:\nX = [x1,x2, ‚Ä¶ ,xn]\n4The vanishing gradient problem arises during the training of neural networks, mainly when using backpropagation to\nupdate weights. It occurs when the gradients of the loss function with respect to the weights become extremely small as they\nare propagated backward through many layers of the network. This results in weights in the early layers of the network not\nbeing updated effectively, which slows down or stops the learning process. For further information see Hochreiter, Sepp. ‚ÄúThe\nvanishing gradient problem during learning recurrent neural nets and problem solutions.‚Äù\nhttps://doi.org/10.1017/cfl.2024.16 Published online by Cambridge University Press\n4 Andrea Filippo Ferraris et al.\nEach token is embedded into a continuous vector space to obtain embeddings:\nE = [e1,e2, ‚Ä¶ ,en]\n2. Linear projections:\nForeachembedding ei,wecomputethree vectors:a query qi,a key ki anda value vi,usinglearned\nweight matricesWùõó, Wk and Wv:\nqi = ei √ó Wùúô\nki = ei √ó Wk\nvi = ei √ó Wv\n3. Scaled dot-product attention:\nThe attention score between tokeni and tokenj is calculated using the scaled dot-product of their\nqueries and keys:\nAttentionScorei,j =\nqi ‚ãÖ kj\n‚àödk\n‚Ä¢ qi ‚ãÖ kt\nv denotes the dot product ofqi and the transpose ofkv.\n‚Ä¢ dk is the dimensionality of the key vectors.\n‚Ä¢ The division by‚àö dk scales the dot products to prevent large values that could result in small\ngradients during training.\nThe attention weights are obtained by applying the softmax function (Figure 1) to the attention\nscores:\nùõº(ij) = softmax(AttentionScore(ij))\nThe output for each tokeni is a weighted sum of the value vectorsvv of all tokens:\nzi =\nn\n‚àë\nj=1\nùõºij ‚ãÖ vj\nFigure 1.Illustratesthe softmax activation function asusedinlargelanguagemodels(LLMs).Eachrawoutputis\nexponentiatedandthennormalizedbydividingbythesumofexponentiatedoutputs,ensuringtheresultingprobabilities\nrangefrom0to1.\nhttps://doi.org/10.1017/cfl.2024.16 Published online by Cambridge University Press\nCambridge Forum on AI: Law and Governance 5\n4. Context vector computation:\nCollectively, in matrix form:\nQ = E√ó Wùúì\nK = E√ó Wk\nV = E√ó Wv\nAttention(Q, K, V) = softmax( QKT\n‚àödk\n) ‚ãÖ V\n‚óã Q, K, andV are matrices of queries, keys, and values for all tokens.\n‚óã Kt is the transpose ofK.\n‚óã The multiplicationQ √ó Kt computes the attention scores for all pairs of tokens simultaneously.\nThis mechanism allows the model to focus on relevant parts of the input while generating output.\nFor example, in the sentence ‚ÄúThe cat sat on the mat because it was soft,‚Äù the model can accurately\ncapture that ‚Äúit‚Äù refers to ‚Äúthe mat‚Äù by assigning higher attention weights between these tokens.\n4.1. Positional encoding\nOne challenge in processing sequences simultaneously is maintaining the sense of order in the data,\nas transformers do not process inputs sequentially like RNNs. To address this,positional encod-\ning (Chen et al.,2021) is introduced. As shown inFigure 2, positional encoding adds information\nabout the position of each token in the sequence,5 ensuring the model can differentiate between\nwords appearing at different positions and preserve the natural order of language (Kazemnejad,\nPadhi, Ramamurthy, Das & Reddy,2023). These encodings are incorporated into the model‚Äôs input\nembeddings,6 allowing transformers to maintain both position and context without the need for\nrecurrence.\nThe positional encodingPE is added to the input embeddings to inject positional information:\nEinput = E + PE\nThe positional encoding is defined using sine and cosine functions of varying frequencies:\nFor positionpos and dimensioni:\n‚Ä¢ For even dimensions (2i):\nPE(pos, 2i) = sin ( pos\n10000\n2i\ndmodel\n)\n5Tokenization refers to the process of splitting text into smaller units calledtokens that the model can process. In the con-\ntext of transformers, tokenization breaks down sentences into tokens, often words or subwords. Each token corresponds to a\nspecific index in a vocabulary list, allowing the model to work with the text numerically. Tokenization is a crucial first step\nin transforming natural language into a format that a model can understand, making it an integral part of how transformers\nhandle input sequences and extract meaning.\n6An embedding is a numerical representation of tokens (words or subwords) that encodes their meanings in the form of\nvectors ‚Äì arrays of numbers. These vectors place words in a multi-dimensional space where similar words are positioned closer\ntogether, helping models like transformers understand relationships and context between tokens.\nhttps://doi.org/10.1017/cfl.2024.16 Published online by Cambridge University Press\n6 Andrea Filippo Ferraris et al.\nFigure 2.Theimageillustrateshowpositionalencodingworksintransformers.Wordembeddings(blueboxes)arecreated\nfrominputslike‚ÄúThe‚Äùand‚Äúquick,‚Äùwhilepositionalinformation(pinkboxes)trackswordorder.Thesearecombinedand\nthenpassedtothetransformermodel(greenbox),enablingittounderstandwordorderinsequenceprocessing.\n‚Ä¢ For odd dimensions (2i+ 1):\nPE(pos, 2i + 1) = cos ( pos\n10000\n2i\ndmodel\n)\nWhere:\n‚Ä¢ pos is the position index of the token in the sequence.\n‚Ä¢ i is the dimension index.\n‚Ä¢ d_model is the dimensionality of the embeddings.\nThis formulation allows the model to learn positional relationships because the positional encod-\nings provide unique vectors for each position, and the sinusoidal functions enable the model to\ngeneralize to sequences longer than those seen during training.\n4.2. Multi-head attention\nWhile self-attention allows the model to consider relationships between tokens, multi-head attention\n(represented inFigure 3) extends this capability by enabling the model to focus on different positions\nand representation subspaces (Cordonnier, Loukas & Jaggi,2021).\n1. Multiple attention heads:\nInstead of computing attention once, the model usesh different attention heads, each with its own\nset of learned projections:\nhttps://doi.org/10.1017/cfl.2024.16 Published online by Cambridge University Press\nCambridge Forum on AI: Law and Governance 7\nFigure3. Thisimagerepresentsthecorecomponentsofthetransformerarchitecture,focusingonthemulti-headattention\nmechanism.Theleftsideshowsthestackedlayersofmulti-headattentionandfeedforwardlayers,whichareappliedboth\ntotheinputandoutputsequences.Positionalencodingisaddedtoaccountforwordorderinthesequence.Ontheright,a\nzoom-inrevealshowscaleddot-productattentionworksbycombiningquery,key,andvaluematrices,normalizedthrough\nasoftmaxfunction,tocalculateattentionscores.Thisenablestransformerstoefficientlycapturerelationshipsbetween\nwordsregardlessoftheirposition.(Vaswanietal., 2017).\nFor headi:\nQi = Einput √ó Wùúô\ni\nKi = Einput √ó Wk\ni\nVi = Einput √ó Wv\ni\nheadi = Attention(Qi, Ki, Vi)\n2. Concatenation and output projection:\nThe outputs from all heads are concatenated and projected to form the final output:\nMultiHead (Q, K, V) = Concat (head1 ‚Ä¶ headh) √ó WO\nwhere WO is the output projection matrix.\nBy having multiple heads, the model can capture diverse aspects of the input, such as syntax and\nsemantics, and learn different types of relationships.\nhttps://doi.org/10.1017/cfl.2024.16 Published online by Cambridge University Press\n8 Andrea Filippo Ferraris et al.\n4.3. Feed-forward networks and residual connections\nAfter the multi-head attention layer, each position undergoes a fully connectedposition-wise feed-\nforward network (FFN):\nFFN(x) = max(0, √ó xW1 + b1) √ó W2 + b2\nWhere:\n‚Ä¢ W‚ÇÅ and W‚ÇÇ are weight matrices.\n‚Ä¢ b‚ÇÅ and b‚ÇÇ are bias vectors.\n‚Ä¢ max(0, x) denotes the rectified linear unit activation function.\nThe FFN is applied independently to each position, allowing the model to transform the attended\nrepresentations into a higher-level abstraction.\nTo facilitate training and improve gradient flow, the transformer architecture employsresidual\nconnections and layer normalization:\n1. Residual connections:\nThe input to each sublayer is added to its output:\nResidual(x) = x + Sublayer(x)\n2. Layer normalization:\nThe residual output is normalized to stabilize the training:\nOutput = LayerNorm(Residual(x))\nThese techniques help prevent vanishing or exploding gradients and allow for deeper networks by\nensuring that the signal remains strong as it moves through the layers.\n5. Overall transformer architecture\nThe transformer architecture consists of two main components: the encoder and the decoder. This\ndesign is particularly effective for sequence-to-sequence tasks like machine translation, where an\ninput sequence in one language is transformed into an output sequence in another.\n5.1. Encoder‚Äìdecoder structure\nThe encoder-decoder structure, illustrated inFigure 4, is a fundamental mechanism in sequence-to-\nsequence models designed for tasks such as translation, summarization, and text generation.\nFigure 4.Diagramofanencoder‚Äìdecodertransformermodeldemonstratingsequence-to-sequencetranslation.\nhttps://doi.org/10.1017/cfl.2024.16 Published online by Cambridge University Press\nCambridge Forum on AI: Law and Governance 9\nThe encoder processes the input sequence to generate a contextual representation. It begins by\nconverting input tokens into continuous vectors using an embedding layer and adds positional\nencodings to retain the order of tokens. The encoder is composed of multiple identical layers, each\ncontainingamulti-headself-attentionmechanismandaposition-wiseFFN,bothfollowedbyresidual\nconnections and layer normalization to enhance training stability and gradient flow.\nThe self-attention mechanism allows each token to attend to all other tokens in the sequence,\ncapturing dependencies regardless of distance. The FFN further transforms these representations,\nintroducing nonlinearity and enabling the model to learn complex patterns.\nThe decoder generates the output sequence by predicting one token at a time, using both the\nencoder‚Äôs output and its own previously generated tokens. Like the encoder, it starts with an embed-\nding layer and positional encodings. Each decoder layer includes a masked multi-head self-attention\nmechanism(topreventaccesstofuturetokens),amulti-headattentionmechanismovertheencoder‚Äôs\noutput (allowing focus on relevant parts of the input) and a position-wise FFN, each followed by\nresidual connections and layer normalization.\nDuring theencoding phase, the encoder processes the entire input sequence simultaneously, pro-\nducing encoded representations for each position. In thedecoding phase , the decoder generates\nthe output sequence step by step. At each step, it considers its own past outputs through masked\nself-attention and attends to the encoder‚Äôs output via encoder‚Äìdecoder attention, enabling it to\nincorporate information from the input sequence relevant to generating the next token.\n5.1.1. Decoder-onlytransformers\nIn some applications, only the decoder part of the transformer is used.Decoder-only transformers,\nsuch as the GPT series, are specialized for tasks involving sequence generation based on prior con-\ntext, like language modeling and text generation. These models consist solely of decoder layers with\nmasked multi-head self-attention to ensure that predictions depend only on preceding tokens. They\nare trained to predict the next token in a sequence, making them ideal for tasks like autocomplete\nand text continuation. A visual representation of this structure, showing an attention word heatmap\nof a decoder-only architecture, is illustrated inFigure 5.\n5.1.2. Encoder-onlytransformers\nConversely,encoder-only transformers consist solely of the encoder stack and are designed for lan-\nguage understanding tasks. Models like BERT utilize this architecture. They employ bidirectional\nself-attention mechanisms, allowing tokens to attend to both past and future positions, thereby\ncapturing context from the entire sequence. These models are trained using MLM, where some\ninput tokens are masked, and the model learns to predict them based on surrounding context. This\napproach is effective for tasks such as sentiment analysis, named entity recognition, and question\nanswering.\n5.2. Open perspective\nDespitetheirstrengths,transformersarenotwithoutchallenges.Theself-attentionmechanism,while\npowerful, requires significant computational resources, particularly in terms of memory. This is\nbecause self-attention involves comparing every element in the input sequence with every other\nelement, which scales quadratically with the input length. For very large datasets or long input\nsequences, this can become prohibitively expensive. However, recent research has been focused on\naddressing these limitations by developing more efficient variants of transformers, such as sparse\ntransformers and reformers, which aim to reduce the computational load without sacrificing perfor-\nmance. Additionally, quantized models7 (Egashira, Vero, Staab, He & Vechev,2024) further enhance\n7A quantized model is a neural network where the precision of the model‚Äôs parameters (weights and activations) is reduced,\ntypically from floating-point (‚Äúfp,‚Äù typically 32-bit or 16-bit) to lower precision, such as 8-bit or even smaller, like 4-bit. The\nhttps://doi.org/10.1017/cfl.2024.16 Published online by Cambridge University Press\n10 Andrea Filippo Ferraris et al.\nFigure 5.Attentionheatmapfromadecoder-onlymodel,illustratinghoweachtokeninthesequenceattendsonlytoitself\nandprevioustokens.Thetriangularstructureresultsfrommaskedself-attention,ensuringthemodelgeneratestext\nautoregressivelybyrelyingsolelyonpastcontext.\nefficiency by reducing the precision of model weights (e.g., from 32-bit to 4-bit), allowing significant\nreductions in memory usage and enabling models to run on smaller hardware without significant\nperformance.\n6. LLMs: Scaling transformers to new heights\nBuilding upon the transformative capabilities of the transformer architecture,LLMs represent a sig-\nnificant advancement in AI by scaling the core innovations of transformers to unprecedented levels\n(Naveed et al., 2024). LLMs leverage self-attention mechanisms and extensive training to capture\nintricate patterns in text, enabling them to perform a wide array of language tasks with remarkable\nproficiency.\n6.1. Scaling laws and model sizes\nA critical aspect of LLMs is their scale ‚Äì in terms of model size, training data quantity and computa-\ntional resources ‚Äì which significantly impacts their performance. Research by AI labs and research\ngoal of quantization is to reduce the memory footprint and computational requirements of the model, enabling faster infer-\nence and allowing deployment on resource-constrained hardware like mobile devices or edge systems, without significantly\ndegrading performance. Quantization is especially important for large models, such as LLMs, to optimize them for efficiency\nwhile maintaining accuracy.\nhttps://doi.org/10.1017/cfl.2024.16 Published online by Cambridge University Press\nCambridge Forum on AI: Law and Governance 11\ncenters has establishedscaling laws that describe how increasing these factors lead to predictable\nimprovements in model capabilities:\n‚Ä¢ Model size (parameters): LLMs like GPT-3 and GPT-4 contain hundreds of billions of param-\neters. Increasing the number of parameters allows the model to capture more complex patterns\nand nuances in language.\n‚Ä¢ Data quantity: Training on larger datasets exposes the model to a broader range of language\nuses, contexts and knowledge. This diversity enhances the model‚Äôs ability to generalize across\ndifferent tasks.\n‚Ä¢ Compute resources: Training large models on vast datasets requires substantial computational\npower. Advances in hardware (such as GPUs and TPUs) and distributed training techniques\nhave enabled the training of LLMs at this scale.\nScalinglawssuggestthatasweproportionallyscaleupmodelsize,dataandcomputeresources,the\nmodel‚Äôs performance continues to improve, often following a power‚Äìlaw relationship. This has moti-\nvated the development of ever-larger models to push the boundaries of language understanding and\ngeneration. However, it must be noted that as of today, this approach brings with it several challenges\nandconsiderations‚Äìbotheconomicalandethical‚Äìsuchastheincreasingneedforexpensivecompu-\ntational resources, environmental impact due to the large-scale consumption of electricity. This has\nled researchers to look into different directions, such as using smaller models (Lu Z. et al,2024) in\ncombination with use of highly curated and specialized training sets as an alternative to ever growing\nmodels (Liu et al.,2024).\n6.1.1. Domain-specificsmalllanguagemodels\nWhile scaling has driven remarkable achievements in general-purpose language models, recent\nresearch has demonstrated the promise of smaller, specialized models trained on domain-specific\ndata. These models, typically ranging from hundreds of millions to a few billion parameters, leverage\ntargeted training data to achieve performance comparable to larger models within their special-\nized domains (Hsieh et al.,2023; Javaheripi et al.,2023). The efficiency of these models stems from\ntheir concentrated focus on domain-specific patterns, terminology and task requirements, effectively\nreducing the computational overhead associated with maintaining broad language understanding,\nthus having a significantly reduced environmental impact with parameter counts several orders of\nmagnitude smaller (Schick & Sch√ºtze,2020). This approach has proven particularly valuable in fields\nsuch as biomedicine (Gu et al.,2021) and legal document analysis8 where domain expertise and pre-\ncision are necessary. The success of these specialized models suggests that strategic data curation and\ndomain-focused architecture optimization may offer a complementary path to the scaling paradigm\n(Zhang, Zeng, Wang & Lu,2024).\n6.2. Training techniques\nLLMs are typically trained in two stages:pretraining (Schneider, Meske & Kuss,2024; Wang, Li, Wu,\nHovy & Sun,2023; Zhou et al.,2023) andfine-tuning (Parthasarathy, Zafar, Khan & Shahid,2024).\nDuringpretraining,themodellearnsgenerallanguagerepresentationsfromvastamountsoftextdata\nwithoutexplicitsupervision(Ding,Qin&Yangetal., 2023).Twoprimaryobjectivesguidethisphase:\n1. Causal language modeling (CLM ): Utilized by models like the GPT series, CLM trains the\nmodel to predict the next word in a sequence given all previous words. This unidirectional\napproach is suitable for generation tasks, where the model maximizes the likelihood of the\nnext word based on the preceding context.\n8See an example of a practical application:https://www.personal.ai/pi-ai/legal-ai-the-small-language-model-advantage.\nhttps://doi.org/10.1017/cfl.2024.16 Published online by Cambridge University Press\n12 Andrea Filippo Ferraris et al.\n2. Masked language modeling (MLM ): Employed by models such as BERT, MLM involves\npredicting missing words in a sequence where some tokens are randomly masked. This bidi-\nrectional approach allows the model to learn from both left and right contexts, minimizing the\nprediction error for the masked tokens (Merchant, Rahimtoroghi, Pavlick & Tenney,2020).\nAfterpre-training,LLMsundergo fine-tuning ontask-specificdatasetstoadaptthemtoparticular\napplications. Fine-tuning can besupervised, using labeled data for tasks like question answering,\nsentiment analysis, or named entity recognition. In cases where labeled data is scarce,unsupervised\nfine-tuning leverages unsupervised objectives to adapt the model to new domains.\nUnsupervised learning plays a crucial role in the initial training phase, enabling the model to\nlearn general language patterns from unlabeled data. Supervised learning becomes important during\nfine-tuning, where the model is taught to perform specific tasks based on labeled datasets.\n6.2.1. Parameter-efficientfine-tuning(PEFT)\nPEFT(Han,Gao,Liu,Zhang&Zhang, 2024;Xu,Xie,Qin,Tao&Wang, 2023)methodshaveemerged\nto address the computational challenges associated with fine-tuning massive models with billions\nof weights (Fu et al.,2023). Instead of updating all weights, techniques likeLow-Rank Adaptation\n(LoRA) allow only a small subset of weights to be fine-tuned.LoRA (Hu et al.,2021) introduces\nlow-rank matrices to specific layers, adapting the model without altering its full architecture, which\nsignificantly reduces memory and computational demands.\nQuantized LoRA (QLoRA) combines this approach with quantization, storing model weights\nin lower-precision formats like 4-bit, further reducing resource needs while maintaining accuracy.\nQuantization-aware LoRA (QA-LoRA) (Xu et al., 2023) goes a step further by applying quanti-\nzation selectively to critical weight matrices, balancing efficiency and performance in constrained\nenvironments.Thesetechniquesenablethefine-tuningoflargemodelsonsmallerhardware,reducing\ncomputational overhead without sacrificing precision.\n6.3. Parameter tuning\nWhile fine-tuning optimizes model architecture for specific applications, parameter tuning offers\nflexible adjustments to model outputs based on input prompts. This helps tailor responses for char-\nacteristics like creativity, precision or length, enhancing task-specific performance without altering\nthe model‚Äôs structure (Liao, Li, Shang & Ma,2022).\n6.3.1. KeyparametersinLLMtuning\n‚Ä¢ T emperature: This controls the randomness of the model‚Äôs output by adjusting the probability\ndistribution of predicted words. Lower temperatures make responses more deterministic, while\nhigher temperatures increase variability, fostering creativity in responses like poetry.\n‚Ä¢ Seed: The seed ensures reproducibility by fixing the random number generator‚Äôs starting point,\nmaking it possible to produce the same outputs for identical inputs across multiple trials ‚Äì\ncrucial for testing and debugging.\n‚Ä¢ T op-k sampling:Thistechniquerestrictsthenextwordpredictiontothekmostprobablewords,\nreducing the risk of the model choosing unlikely or incoherent words. Smaller values of k make\nthe output more focused, enhancing accuracy.\n‚Ä¢ T op-p (nucleus sampling): A more dynamic approach than Top-k, nucleus sampling selects\nwords whose combined probability exceeds a certain threshold (p), ensuring a balance between\ndiversity and coherence.\n‚Ä¢ Max T okens: This parameter limits the number of tokens the model generates in response,\nuseful for managing the length of outputs, such as in summarization tasks where brevity is\nneeded.\nhttps://doi.org/10.1017/cfl.2024.16 Published online by Cambridge University Press\nCambridge Forum on AI: Law and Governance 13\n‚Ä¢ Frequency and Presence Penalty: Both parameters manage word repetition. Frequency penal-\nties reduce redundancy by discouraging the model from repeating words, while presence\npenalties further limit the reuse of words that have already appeared in the text.\n‚Ä¢ Stop sequences: These are specific tokens that signal the model to halt text generation, partic-\nularly useful for structured tasks or dialogues that need concise responses.\n‚Ä¢ Logit bias : Logit bias allows for direct control over the probability distribution, steering the\nmodel toward or away from certain words ‚Äì vital for ensuring the use of domain-specific\nterminology or avoiding irrelevant language.\nBy adjusting these parameters, users can ensure LLMs meet specific needs, whether optimizing\nfor creativity, precision or domain-specific vocabulary. This layer of control complements fine-tuning\nandprovidesapowerfultoolsetfortaskadaptation,enablingmoreeffectiveutilizationofLLMsacross\ndiverse applications.\n6.4. Prompt engineering\nPrompt engineering is a technique used to optimize the inputs provided to LLMs, ensuring they gen-\nerate more accurate, relevant and useful outputs (Chen, Zhang, Langren√© & Zhu,2023). A ‚Äúprompt‚Äù\nin this context refers to the text or instructions given to the model, guiding its response. Unlike meth-\nods that alter the model‚Äôs architecture or underlying weights, prompt engineering focuses solely on\nrefining the input to influence the output without changing the model itself.\n6.4.1. Keyconceptsinpromptengineering\n‚Ä¢ Clarity and specificity : Well-crafted prompts are clear and specific, reducing ambiguity and\nleading to more accurate responses.\n‚Ä¢ Contextual information: Providing the right amount of background or context can signifi-\ncantly improve the relevance and coherence of the model‚Äôs outputs.\n‚Ä¢ Task demonstration (Few-shot learning): By including examples of the desired task (few-shot\nlearning), the model can generalize better and provide higher-quality responses.\n6.4.2. Techniquesinpromptengineering\n1. Zero-shot prompting: The model is expected to generate a response without any examples,\nrelying purely on pre-trained knowledge.\n2. One-shot prompting: A single example of the task is provided in the prompt, helping the\nmodel better understand the format and expected output while still minimizing the number of\nexamples.\n3. Few-shot prompting: A few examples of the task are included in the prompt, which helps the\nmodel understand the format and expected output.\n4. Chain-of-thought (CoT) prompting: CoT guides the model through a step-by-step reasoning\nprocess,whichisparticularlyeffectivefortasksthatrequirelogicalprogressionorcomplexrea-\nsoning. CoT methodologies often have subcategories, such asTabular CoT, which is tailored\nfor handling tasks that involve structured data or tables by applying step-by-step reasoning\nwithin tabular formats.\n5. Instruction tuning: Clear and direct instructions help the model perform specific tasks, such\nas summarizing or generating lists.\n6. Self-consistency: This technique involves generating multiple responses for the same prompt\nand selecting the most consistent one, improving reliability, especially in reasoning tasks.\nhttps://doi.org/10.1017/cfl.2024.16 Published online by Cambridge University Press\n14 Andrea Filippo Ferraris et al.\n6.5. Evaluating LLMs performance\nEvaluating LLMs is essential to ensuring their accuracy, reliability and fairness across various appli-\ncations, from healthcare to law. Performance evaluation can be divided into two main categories:\nhuman assessments and automated methods.\nHuman evaluation involves domain experts or users reviewing model outputs for factors like\nfluency, coherence, relevance and factual accuracy (Feng et al.,2024). This approach is especially crit-\nical in fields such as legal (Chalkidis, Fergadiotis, Malakasiotis, Aletras & Androutsopoulos,2020),\nfinancial (Wu et al.,2023) and medical applications (Wang & Zhang,2024), where nuanced and\ncontext-specific knowledge is required. However, human evaluation is labor-intensive and difficult\nto scale for large volumes of model iterations or outputs.\nAutomated evaluation methods provide scalable and objective metrics. They measure aspects\nsuch as fluency, accuracy and relevance of the text output. Common methods include the following:\n‚Ä¢ Bilingual Evaluation Understudy (BLEU) (Papineni, Roukos, Ward & Zhu,2002), Recall-\nOriented Understudy for Gisting Evaluation (ROUGE)(Lin,2004)and Metric for Evaluation\nof Translation with Explicit ORdering (METEOR) (Lavie & Denkowski,2009) scores: These\nassess the quality of text generation (e.g., translation, summarization) by comparing model\noutputs to reference texts based on content overlap and lexical similarity.\n‚Ä¢ Perplexity (Colla, Delsanto, Agosto, Vitiello & Radicioni,2022) andF1 scores (Zhang, Wang &\nZhao,2015):Perplexitymeasureshowwellalanguagemodelpredictssequencesoftext,focusing\nonfluency.F1scores,combiningprecisionandrecall,areusedforclassificationtaskstoevaluate\nhow well the model categorizes or identifies information.\n‚Ä¢ Adversarial robustness testing(Zimmermann, Brendel, Tramer & Carlini,2022): This method\ntests how LLMs perform under challenging or adversarial inputs, ensuring that models can\nhandle unexpected or tricky queries without producing incorrect or biased responses.\n‚Ä¢ Fairness and bias testing (Rodolfa, Saleiro & Ghani,2020): These frameworks measure the\nethicalperformanceofmodelsbyidentifyingandmitigatinganygender,racialorculturalbiases\nin generated content, ensuring the model outputs are fair and nondiscriminatory.\nThese evaluation techniques help optimize LLMs for performance while ensuring they meet\nethical and reliability standards across various applications.\n7. Context windows, hallucinations and other challenges in LLMs\nLLMs excel in tasks involving language comprehension and generation, but they are not without\nlimitations. Two of the most prominent challenges are the management of context windows and the\nissue of hallucinations, among other inherent difficulties in LLMs.\nLLMs operate withinfixed context windows (Dsouza, Glaze, Shin & Sala,2024), typically from a\nfew thousands to a few hundred thousand tokens. This limitation constrains the amount of text the\nmodel can consider at once. In scenarios requiring long-form analysis, like legal reviews or complex\nconversations, earlier parts of the input might be discarded, leading to a loss of continuity and poten-\ntiallyimpactingthequalityoftheresponse.WhilenewermodelssuchasGPT-4haveextendedcontext\nwindows, the inherent limitation remains, posing challenges for tasks that demand deep contextual\nunderstanding.\nHallucinations (Azamfirei, Kudchadkar & Fackler,2023) occur when LLMs generate text that is\nplausiblebutincorrectorentirelyfabricated(Ye,Liu,Zhang,Hua&Jia, 2023).BecauseLLMsgenerate\npredictions based on statistical patterns learned from training data, they might confidently present\nfalse information. This is particularly dangerous in critical fields such as healthcare, finance and law,\nwhere factual accuracy is essential. Models can invent statistics, references or claims, complicating\nhttps://doi.org/10.1017/cfl.2024.16 Published online by Cambridge University Press\nCambridge Forum on AI: Law and Governance 15\nFigure 6.ThisimagedemonstratesgenderbiasinLLMs.Themodelassumesafemalenurseandamaledoctorinits\ntranslations,reflectingcommonstereotypesembeddedintrainingdata.\nthe task of trustworthiness. Mitigation strategies include refining training datasets, incorporating\nreal-time knowledge bases and enhancing human oversight during model fine-tuning.\nLLMs also struggle with bias amplification, as they reflect the biases present in their training\ndata, which can perpetuate harmful stereotypes (illustrated inFigure 6, where an example of gender\nbias learned by the model is depicted). Additionally, LLMs remainopaque in their decision-making\nprocesses, making it difficult to interpret how outputs are generated. Finally,energy consumption is\na growing concern (Samsi et al.,2023), as training large models demands substantial computational\nresources, raising ethical and environmental considerations.\n8. Improving LLM accuracy: Retrieval-augmented generation (RAG)\nLLMs have shown remarkable capabilities in natural language understanding and generation.\nHowever, as previously presented, they still face limitations, particularly around the accuracy and\ntimeliness of the information they generate. These models are trained on vast datasets but may lack\nup-to-date or domain-specific knowledge, leading to hallucinations, outdated responses, or factually\nincorrect outputs. This is where RAG steps in to enhance the accuracy and factual reliability of LLMs.\n8.1. Retrieval-augmented generation (RAG)\nRAG is an advanced approach that integrates information retrieval systems with LLMs to enhance\ntheir accuracy and relevance (Lewis et al.,2020; Li, Su, Cai, Wang & Liu,2022). RAG models com-\nbine the generative capabilities of LLMs with external knowledge sources, such as databases or\ndocument collections, enabling the model to pull real-time information rather than relying solely\non pretrained data. This mechanism addresses the limitations of LLMs, such as hallucinations and\noutdated knowledge, by grounding generated responses in retrieved, factual data. A flowchart illus-\ntratingthearchitectureofaRAGmodel,detailingtheinteractionbetweentheretrievalandgeneration\ncomponents, is shown inFigure 7.\nRAG operates in two phases: retrieval and generation. In the retrieval phase, the model searches\na vast external knowledge base to gather relevant information based on the input query. In the\ngeneration phase, the retrieved data is then used to condition the response, enabling the LLM to\nhttps://doi.org/10.1017/cfl.2024.16 Published online by Cambridge University Press\n16 Andrea Filippo Ferraris et al.\nFigure7. Thediagramshowcasestheworkflowofretrieval-augmentedgeneration(RAG).Itstartswithapromptandquery,\nwhichareusedtosearchexternalknowledgesourcesforrelevantinformation.Theretrievedinformationenhancesthe\ncontext,whichisthenpassedalongwiththeoriginalquerytothelargelanguagemodel(LLM).Thisimprovedcontexthelps\ntheLLMgenerateamoreaccurateandrelevanttextresponse.\nprovide more accurate and contextually grounded outputs. By integrating retrieval with generation,\nRAG mitigates the issue of hallucinations, significantly reducing instances of fabricated or inaccurate\ncontent.\nThe ability to retrieve up-to-date information makes RAG particularly effective for dynamic fields\nsuch as news reporting, medical diagnostics and legal document analysis, where real-time accuracy\nis paramount.\n8.2. Evaluation of RAG efficacy and metrics\nEvaluating the efficacy of RAG models requires both traditional and novel metrics tailored to the\nretrieval-enhanced framework. Key metrics include the following:\n‚Ä¢ Retrieval accuracy: Ensures that the external knowledge source effectively supplements the\nLLM, reducing hallucinations and improving factuality. Tools such as Retrieval Augmented\nGeneration Assessment (RAGAs) (Es, James, Espinosa-Anke & Schockaert, 2023) (RAG\nAutomatic Scoring) are emerging to automate this process by evaluating both the retrieval\nquality and the final generated output.\n‚Ä¢ Factuality and groundedness: A critical metric for RAG models is ensuring that generated\nresponses are factually grounded in the retrieved documents. Evaluation frameworks like Luna\n(Saidov, Bakalova, Taktasheva, Mikhailov & Artemova,2024) assess how well the generated text\naligns with retrieved facts, helping to reduce inaccuracies and inconsistencies.\n9. Conclusions\nLLMs have revolutionized NLP by harnessing transformer architectures to achieve unprece-\ndented proficiency in language understanding and generation. They have transformed industries\nhttps://doi.org/10.1017/cfl.2024.16 Published online by Cambridge University Press\nCambridge Forum on AI: Law and Governance 17\nsuch as healthcare, law and customer service by enabling applications that require high fluency\nand precision. Despite these advancements, LLMs face ongoing challenges, including compu-\ntational resource demands, context window limitations and issues related to bias and factual\naccuracy.\nInnovations like PEFT, quantization techniques and RAG are actively addressing these challenges,\nenhancing the efficiency, scalability and reliability of LLMs. As these models continue to grow in\nscale and capability, they hold the promise of extending beyond language tasks to impact fields like\ncomputer vision and enable multimodal AI applications.\nWith a continued focus on improving efficiency and addressing ethical considerations, LLMs\nare poised to play a pivotal role in shaping the future of technology and AI, driving forward the\ncapabilities of AI systems across a wide array of domains.\nFunding statement. This research was supported by the Fondazione CRT, under the 2023 Call, aimed at advancing inter-\ndisciplinary studies in the intersection of law, LLMs and policy. The funding body played no role in the design, execution or\npublication of this work.\nCompeting interests. The authors declare that they have no competing interests, financial or otherwise, that could have\ninfluenced the content or conclusions of this research.\nReferences\nAzamfirei, R., Kudchadkar, S. R., & Fackler, J.(2023). Large language models and the perils of their hallucinations.J Critical\nCare, 27(1), 120.\nChalkidis, I., Fergadiotis, M., Malakasiotis, P ., Aletras, N., & Androutsopoulos, I. (2020). LEGAL-BERT: The Muppets\nstraight out of law school. arXiv.https://arxiv.org/abs/2010.02559.\nChen, B., Zhang, Z., Langren√©, N., & Zhu, S.(2023).Unleashingthepotentialofpromptengineeringinlargelanguagemodels:\nA comprehensive review. arXiv preprint arXiv:2310.14735.\nChen, P .-C., T sai, H., Bhojanapalli, S., Chung, H. W ., Chang, Y .-W ., & Ferng, C.-S.(2021). A simple and effective positional\nencoding for transformers. arXiv.https://arxiv.org/abs/2104.08698.\nColla, D., Delsanto, M., Agosto, M., Vitiello, B., & Radicioni, D. P . (2022). Semantic coherence markers: The contribution\nof perplexity metrics.Artificial Intelligence in Medicine, 134, 102393.\nCordonnier, J.-B., Loukas, A., & Jaggi, M. (2021). Multi-head attention: Collaborate instead of concatenate. arXiv.https://\narxiv.org/abs/2006.16362.\nDevlin, J., Chang, M.-W ., Lee, K., & T outanova, K.(2019).BERT:Pre-trainingofdeepbidirectionaltransformersforlanguage\nunderstanding. arXiv.https://arxiv.org/abs/1810.0480.\nDing, N., Qin, Y ., Y ang, G., Wei, F ., Y ang, Z., Su, Y ., Hu, S., Chen, Y ., Chan, C.-M., Chen, W ., Yi, J., Zhao, W ., W ang, X.,\nLiu, Z., Zheng, H.-T., Chen, J., Liu, Y ., Tang, J., Li, J., & Sun, M. (2023). Parameter-efficient fine-tuning of large-scale\npre-trained language models.Nature Machine Intelligence, 5(3), 220‚Äì235. doi:10.1038/s42256-023-00626-4\nDsouza, A., Glaze, C., Shin, C., & Sala, F . (2024). Evaluating language model context windows: A ‚ÄúWorking Memory‚Äù test\nand inference-time correction. arXiv preprint arXiv:2407.03651.\nEgashira, K., Vero, M., Staab, R., He, J., & Vechev, M.(2024). Exploiting LLM quantization. arXiv.https://arxiv.org/abs/2405.\n18137.\nEs, S., James, J., Espinosa-Anke, L., & Schockaert, S. (2023). RAGAS: Automated evaluation of retrieval augmented\ngeneration. arXiv preprint arXiv:2309.15217.\nFeng, K., Ding, K., Ma, K., W ang, Z., Zhang, Q., & Chen, H. (2024). Sample-efficient human evaluation of large language\nmodels via maximum discrepancy competition. arXiv.https://arxiv.org/abs/2404.08008.\nFu, Z., Y ang, H., So, A. M. C., Lam, W ., Bing, L., & Collier, N. (2023, June). On the effectiveness of parameter-efficient\nfine-tuning. InProceedings of the AAAI conference on artificial intelligence (Vol. 37, No. 11, 12799‚Äì12807).\nGu, Y ., Tinn, R., Cheng, H., Lucas, M., Usuyama, N., Liu, X., ‚Ä¶ Poon, H. (2021). Domain-specific language model pre-\ntraining for biomedical natural language processing.ACM Transactions on Computing for Healthcare (HEALTH) , 3(1),\n1‚Äì23.\nHan, Z., Gao, C., Liu, J., Zhang, J., & Zhang, S. Q.(2024). Parameter-efficient fine-tuning for large models: A comprehensive\nsurvey. arXiv.https://arxiv.org/abs/2403.14608\nHochreiter, S. (1998). The vanishing gradient problem during learning recurrent neural nets and problem solutions.\nInternational Journal of Uncertainty, Fuzziness and Knowledge-Based Systems , 6(2), 107‚Äì116. https://doi.org/10.1142/\nS0218488598000094\nhttps://doi.org/10.1017/cfl.2024.16 Published online by Cambridge University Press\n18 Andrea Filippo Ferraris et al.\nHsieh, C. Y ., Li, C. L., Y eh, C. K., Nakhost, H., Fujii, Y ., Ratner, A., ‚Ä¶ Pfister, T. (2023). Distilling step-by-step!\nOutperforming larger language models with less training data and smaller model sizes. arXiv preprint arXiv:2305.02301.\nHu, E. J., Shen, Y ., W allis, P ., Allen-Zhu, Z., Li, Y ., W ang, S.,‚Ä¶ Chen, W .(2021). Lora: Low-rank adaptation of large language\nmodels. arXiv preprint arXiv:2106.09685.\nJavaheripi, M., Bubeck, S., Abdin, M., Aneja, J., Bubeck, S., Mendes, C. C. T., ‚Ä¶ Gopi, S. (2023). Phi-2: The surprising\npower of small language models.Microsoft Research Blog, 1, 3.\nKazemnejad, A., Padhi, I., Ramamurthy, K. N., Das, P ., & Reddy, S. (2023). The impact of positional encoding on length\ngeneralization in transformers. arXiv.https://arxiv.org/abs/2305.19466.\nLavie, A., & Denkowski, M. J. (2009). The METEOR metric for automatic evaluation of machine translation. Machine\nTranslation, 23(2-3), 105‚Äì115.\nLeCun Y ., Boser B., Denker J. S., Henderson D., Howard R. E., Hubbard W ., & Jackel L. D.(1989). Backpropagation applied\nto handwritten zip code recognition.Neural Computation, 1(4), 541‚Äì551. doi:https://doi.org/10.1162/neco.1989.1.4.541\nLeCun, Y ., Bottou, L., Bengio, Y ., & Haffner, P .(1998).Gradient-basedlearningappliedtodocumentrecognition. Proceedings\nof the IEEE, 86(11), 2278‚Äì2324.https://doi.org/10.1109/5.726791\nLewis, P ., Perez, E., Piktus, A., Petroni, F ., Karpukhin, V ., Goyal, N.,‚Ä¶ Kiela, D. (2020). Retrieval-augmented generation\nfor knowledge-intensive NLP tasks.Advances in Neural Information Processing Systems, 33, 9459‚Äì9474.\nLi, H., Su, Y ., Cai, D., W ang, Y ., & Liu, L. (2022). A survey on retrieval-augmented text generation. arXiv preprint\narXiv:2202.01110\nLiao, L., Li, H., Shang, W ., & Ma, L.(2022). An empirical study of the impact of hyperparameter tuning and model optimiza-\ntion on the performance properties of deep neural networks.ACM Transactions on Software Engineering and Methodology\n(TOSEM), 31(3), 1‚Äì40.\nLin, C.-Y . (2004). ROUGE: A package for automatic evaluation of summaries. InT ext Summarization Branches Out (pp.\n74‚Äì81). Association for Computational Linguistics.https://aclanthology.org/W04-1013\nLiu, Y ., Cao, J., Liu, C., Ding, K., & Jin, L.(2024). Datasets for large language models: A comprehensive survey. arXiv.https://\narxiv.org/abs/2402.18041\nLu, Z., Li, X., Cai, D., Yi, R., Liu, F ., Zhang, X., Lane, N. D., & Xu, M.(2024). Small language models: Survey, measurements,\nand insights. arXiv.https://arxiv.org/abs/2409.15790\nLu, Z., Li, X., Cai, D., Yi, R., Liu, F ., Zhang, X.,‚Ä¶ Xu, M. (2024).Smalllanguagemodels:Survey,measurements,andinsights.\narXiv.https://arxiv.org/abs/2409.15790.\nMcCulloch, W . S., & Pitts, W .(1943).Alogicalcalculusoftheideasimmanentinnervousactivity. The Bulletin of Mathematical\nBiophysics, 5(4), 115‚Äì133. doi:10.1007/BF02478259\nMerchant, A., Rahimtoroghi, E., Pavlick, E., & T enney, I. (2020). What happens to BERT embeddings during fine-tuning?\narXiv preprint arXiv:2004.14448.\nNaveed, H., Khan, A. U., Qiu, S., Saqib, M., Anwar, S., Usman, M., ‚Ä¶ Mian, A. (2024). A comprehensive overview of large\nlanguage models. arXiv.https://arxiv.org/abs/2307.06435.\nPapineni, K., Roukos, S., W ard, T., & Zhu, W . J. (2002, July). BLEU: A method for automatic evaluation of machine\ntranslation. InProceedings of the 40th annual meeting of the Association for Computational Linguistics(pp. 311‚Äì318).\nParthasarathy, V . B., Zafar, A., Khan, A., & Shahid, A.(2024). The ultimate guide to fine-tuning LLMs from basics to break-\nthroughs: An exhaustive review of technologies, research, best practices, applied research challenges and opportunities.\narXiv.https://arxiv.org/abs/2408.13296.\nRodolfa, K. T., Saleiro, P ., & Ghani, R. (2020). Bias and fairness (in Machine Learning). InBig Data and Social Science: A\nPractical Guide to Methods and T ools(pp. 281‚Äì312). Chapman and Hall/CRC.\nRosenblatt, F . (1958). The perceptron: A probabilistic model for information storage and organization in the brain.\nPsychological Review, 65(6), 386‚Äì408. doi:10.1037/h0042519\nRumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986). Learning internal representations by error propagation.\nSaidov, M., Bakalova, A., Taktasheva, E., Mikhailov, V ., & Artemova, E. (2024). LUNA: A framework for language\nunderstanding and naturalness assessment. arXiv preprint arXiv:2401.04522.\nSamsi S., Zhao D., McDonald J., Li B., Michaleas A., Jones M., Bergeron W ., Kepner J., Tiwari D., & Gadepally V .(2023).\nFrom words to watts: Benchmarking the energy costs of large language model inference. In2023 IEEE High Performance\nExtreme Computing Conference (HPEC), 1‚Äì9.10.1109/HPEC58863.2023.10363447.\nSchick, T., & Sch√ºtze, H.(2020).It‚Äôsnotjustsizethatmatters:Smalllanguagemodelsarealsofew-shotlearners.arXivpreprint\narXiv:2009.07118.\nSchneider, J., Meske, C., & Kuss, P .(2024).Foundationmodels. Business and Information Systems Engineering,66(2),221‚Äì231.\ndoi:10.1007/s12599-024-00851-0\nT erven, J., Cordova-Esparza, D. M., Ramirez-Pedraza, A., Chavez-Urbiola, E. A., & Romero-Gonzalez, J. A. (2024). Loss\nfunctions and metrics in deep learning. arXiv.https://arxiv.org/abs/2307.02694.\nV aswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ‚Ä¶ Polosukhin, I. (2017). Attention is all you\nneed. arXiv.https://arxiv.org/abs/1706.03762.\nhttps://doi.org/10.1017/cfl.2024.16 Published online by Cambridge University Press\nCambridge Forum on AI: Law and Governance 19\nW ang, D., & Zhang, S.(2024).Largelanguagemodelsinmedicalandhealthcarefields:Applications,advances,andchallenges.\nArtificial Intelligence Review, 57(299). doi:10.1007/s10462-024-10921-0\nW ang, H., Li, J., Wu, H., Hovy, E., & Sun, Y . (2023). Pre-trained language models and their applications.Engineering, 25,\n51‚Äì65. doi:10.1016/j.eng.2022.04.024\nWerbos P .(1974).BeyondRegressionNewToolsforPredictionandAnalysisintheBehavioralSciences,Ph.D.Thesis,Harvard\nUniversity, Cambridge.\nWu, S., Irsoy, O., Lu, S., Dabravolski, V ., Dredze, M., Gehrmann, S., & Mann, G.(2023). BloombergGPT: A large language\nmodel for finance.https://arxiv.org/abs/2303.17564(Retreived 10 October 2024).\nXu, L., Xie, H., Qin, S.-Z. J., Tao, X., & W ang, F . L.(2023). Parameter-efficient fine-tuning methods for pretrained language\nmodels: A critical review and assessment. arXiv.10.48550/arXiv.2312.12148\nY e, H., Liu, T., Zhang, A., Hua, W ., & Jia, W .(2023). Cognitive mirage: A review of hallucinations in large language models.\narXiv preprint arXiv:2309.06794.\nZhang, D., W ang, J., & Zhao, X.(2015, September). Estimating the uncertainty of average F1 scores. InProceedings of the 2015\nInternational conference on the theory of information retrieval(pp. 317‚Äì320).\nZhang, P ., Zeng, G., W ang, T., & Lu, W . (2024). Tinyllama: An open-source small language model. arXiv preprint\narXiv:2401.02385.\nZhou, C., Li, Q., Li, C., Y u, J., Liu, Y ., W ang, G.,‚Ä¶ Sun, L. (2023). A comprehensive survey on pretrained foundation models:\nA history from BERT to ChatGPT. arXiv.https://arxiv.org/abs/2302.09419.\nZimmermann, R. S., Brendel, W ., Tramer, F ., & Carlini, N. (2022). Increasing confidence in adversarial robustness\nevaluations. Advances in Neural Information Processing Systems, 35, 13174‚Äì13189.\nAndrea Filippo Ferraris, PhD Fellow LAST-JD, ALMA AI, University of Bologna and PhD Fellow in Law at DIKE and Law\nfaculty, Vrije Universiteit Brussel, Brussels. Email:andrea.ferraris3@unibo.itand andrea.filippo.ferraris@vub.be\nDavide Audrito,PhDFellowatLAST-JD,ComputerScienceDepartment,UniversityofTorino,andLegalStudiesDepartment,\nUniversity of Bologna. Email:d.audrito@unito.itand davide.audrito2@unibo.it\nLuigi Di Caro , Associate Professor of Computer Science, Computer Science Department, University of Torino. Email:\nluigi.dicaro@unito.it\nCristina Poncib√≤ , Full Professor of Comparative Law, Department of Law, University of Turin. Email: cristina.pon-\ncibo@unito.it\nCite this article: Ferraris A.F, Audrito D, Caro L.D and Poncib√≤ C. (2025). The architecture of language: Understanding the\nmechanics behind LLMs.Cambridge Forum on AI: Law and Governance 1, e11, 1‚Äì19.https://doi.org/10.1017/cfl.2024.16\nhttps://doi.org/10.1017/cfl.2024.16 Published online by Cambridge University Press",
  "topic": "Architecture",
  "concepts": [
    {
      "name": "Architecture",
      "score": 0.6058017611503601
    },
    {
      "name": "Linguistics",
      "score": 0.49981212615966797
    },
    {
      "name": "History",
      "score": 0.33927327394485474
    },
    {
      "name": "Philosophy",
      "score": 0.18724673986434937
    },
    {
      "name": "Archaeology",
      "score": 0.06294000148773193
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I13469542",
      "name": "Vrije Universiteit Brussel",
      "country": "BE"
    },
    {
      "id": "https://openalex.org/I9360294",
      "name": "University of Bologna",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I55143463",
      "name": "University of Turin",
      "country": "IT"
    }
  ]
}