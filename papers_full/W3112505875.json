{
  "title": "Applying Artificial Intelligence Methods for the Estimation of Disease Incidence: The Utility of Language Models",
  "url": "https://openalex.org/W3112505875",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5018493716",
      "name": "Yuanzhao Zhang",
      "affiliations": [
        "Babylon Health"
      ]
    },
    {
      "id": "https://openalex.org/A5065948462",
      "name": "Robert Walecki",
      "affiliations": [
        "Babylon Health"
      ]
    },
    {
      "id": "https://openalex.org/A5062667641",
      "name": "Joanne R. Winter",
      "affiliations": [
        "Babylon Health"
      ]
    },
    {
      "id": "https://openalex.org/A5091078461",
      "name": "Felix Bragman",
      "affiliations": [
        "Babylon Health"
      ]
    },
    {
      "id": "https://openalex.org/A5053161455",
      "name": "Sara Lourenço",
      "affiliations": [
        "Babylon Health"
      ]
    },
    {
      "id": "https://openalex.org/A5071513905",
      "name": "Christopher D. Hart",
      "affiliations": [
        "Babylon Health"
      ]
    },
    {
      "id": "https://openalex.org/A5053333696",
      "name": "Adam Baker",
      "affiliations": [
        "Babylon Health"
      ]
    },
    {
      "id": "https://openalex.org/A5053316074",
      "name": "Yura Perov",
      "affiliations": [
        "Babylon Health"
      ]
    },
    {
      "id": "https://openalex.org/A5088787234",
      "name": "Saurabh Johri",
      "affiliations": [
        "Babylon Health"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2751792491",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W6676373471",
    "https://openalex.org/W6679844565",
    "https://openalex.org/W2252215182",
    "https://openalex.org/W2057536936",
    "https://openalex.org/W2169767637",
    "https://openalex.org/W2964006392",
    "https://openalex.org/W6692707676",
    "https://openalex.org/W2514071032",
    "https://openalex.org/W6726028665",
    "https://openalex.org/W6685812147",
    "https://openalex.org/W6680930200",
    "https://openalex.org/W6687413744",
    "https://openalex.org/W2768114048",
    "https://openalex.org/W2769783090",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2912654919",
    "https://openalex.org/W6682691769",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W4239510810",
    "https://openalex.org/W2131953535",
    "https://openalex.org/W2564257742",
    "https://openalex.org/W2888967035",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W2794557536",
    "https://openalex.org/W2119821739",
    "https://openalex.org/W2191167248",
    "https://openalex.org/W2259469853",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2604272474",
    "https://openalex.org/W2963078493",
    "https://openalex.org/W2618735189",
    "https://openalex.org/W2937845937"
  ],
  "abstract": "Background: AI-driven digital health tools often rely on estimates of disease incidence or prevalence, but obtaining these estimates is costly and time-consuming. We explored the use of machine learning models that leverage contextual information about diseases from unstructured text, to estimate disease incidence. Methods: We used a class of machine learning models, called language models, to extract contextual information relating to disease incidence. We evaluated three different language models: BioBERT, Global Vectors for Word Representation (GloVe), and the Universal Sentence Encoder (USE), as well as an approach which uses all jointly. The output of these models is a mathematical representation of the underlying data, known as “embeddings.” We used these to train neural network models to predict disease incidence. The neural networks were trained and validated using data from the Global Burden of Disease study, and tested using independent data sourced from the epidemiological literature. Findings: A variety of language models can be used to encode contextual information of diseases. We found that, on average, BioBERT embeddings were the best for disease names across multiple tasks. In particular, BioBERT was the best performing model when predicting specific disease-country pairs, whilst a fusion model combining BioBERT, GloVe, and USE performed best on average when predicting disease incidence in unseen countries. We also found that GloVe embeddings performed better than BioBERT embeddings when applied to country names. However, we also noticed that the models were limited in view of predicting previously unseen diseases. Further limitations were also observed with substantial variations across age groups and notably lower performance for diseases that are highly dependent on location and climate. Interpretation: We demonstrate that context-aware machine learning models can be used for estimating disease incidence. This method is quicker to implement than traditional epidemiological approaches. We therefore suggest it complements existing modeling efforts, where data is required more rapidly or at larger scale. This may particularly benefit AI-driven digital health products where the data will undergo further processing and a validated approximation of the disease incidence is adequate.",
  "full_text": "METHODS\npublished: 15 December 2020\ndoi: 10.3389/fdgth.2020.569261\nFrontiers in Digital Health | www.frontiersin.org 1 December 2020 | Volume 2 | Article 569261\nEdited by:\nAngus Roberts,\nKing’s College London,\nUnited Kingdom\nReviewed by:\nYun Wang,\nFacebook, United States\nHao Wang,\nShenzhen University General Hospital,\nChina\n*Correspondence:\nRobert Walecki\nrobert.walecki@babylonhealth.com\nSpecialty section:\nThis article was submitted to\nHealth Informatics,\na section of the journal\nFrontiers in Digital Health\nReceived: 03 June 2020\nAccepted: 13 October 2020\nPublished: 15 December 2020\nCitation:\nZhang Y, Walecki R, Winter JR,\nBragman FJS, Lourenco S, Hart C,\nBaker A, Perov Y and Johri S (2020)\nApplying Artiﬁcial Intelligence Methods\nfor the Estimation of Disease\nIncidence: The Utility of Language\nModels. Front. Digit. Health 2:569261.\ndoi: 10.3389/fdgth.2020.569261\nApplying Artiﬁcial Intelligence\nMethods for the Estimation of\nDisease Incidence: The Utility of\nLanguage Models\nYuanzhao Zhang, Robert Walecki*, Joanne R. Winter, Felix J. S. Bragman, Sara Lourenco,\nChristopher Hart, Adam Baker, Yura Perov and Saurabh Johri\nBabylon Health, London, United Kingdom\nBackground: AI-driven digital health tools often rely on estimates of di sease incidence\nor prevalence, but obtaining these estimates is costly and t ime-consuming. We explored\nthe use of machine learning models that leverage contextual information about diseases\nfrom unstructured text, to estimate disease incidence.\nMethods: We used a class of machine learning models, called language m odels,\nto extract contextual information relating to disease inci dence. We evaluated three\ndifferent language models: BioBERT, Global Vectors for Wor d Representation (GloVe),\nand the Universal Sentence Encoder (USE), as well as an appro ach which uses all\njointly. The output of these models is a mathematical repres entation of the underlying\ndata, known as “embeddings.” We used these to train neural ne twork models to predict\ndisease incidence. The neural networks were trained and val idated using data from the\nGlobal Burden of Disease study, and tested using independen t data sourced from the\nepidemiological literature.\nFindings: A variety of language models can be used to encode contextual information\nof diseases. We found that, on average, BioBERT embeddings w ere the best for disease\nnames across multiple tasks. In particular, BioBERT was the best performing model when\npredicting speciﬁc disease-country pairs, whilst a fusion model combining BioBERT,\nGloVe, and USE performed best on average when predicting dis ease incidence in\nunseen countries. We also found that GloVe embeddings perfo rmed better than BioBERT\nembeddings when applied to country names. However, we also n oticed that the models\nwere limited in view of predicting previously unseen diseas es. Further limitations were also\nobserved with substantial variations across age groups and notably lower performance\nfor diseases that are highly dependent on location and clima te.\nInterpretation: We demonstrate that context-aware machine learning models can\nbe used for estimating disease incidence. This method is qui cker to implement than\ntraditional epidemiological approaches. We therefore sug gest it complements existing\nmodeling efforts, where data is required more rapidly or at l arger scale. This may\nparticularly beneﬁt AI-driven digital health products whe re the data will undergo further\nprocessing and a validated approximation of the disease inc idence is adequate.\nKeywords: natural language processing, disease incidence , health statistic data, deep learning, machine learning\nZhang et al. Applying Artiﬁcial Intelligence Methods\n1. INTRODUCTION\nAccurate, comprehensive estimations of global health stati stics\nare crucially important for informing health priorities and\nhealth policy decisions at global, national, and local scales (\n1).\nHowever, obtaining accurate and informative estimates of d isease\nincidence and prevalence requires a substantial amount of ti me,\nmoney and expertise to design rigorous data collection proces ses,\ngather data, and build infrastructure for data collection. This\nis particularly challenging in developing countries where he alth\nsystems have less capacity. More recently, comprehensive data\non the incidence of diﬀerent diseases in diﬀerent settings hav e\nbecome an important component of AI-driven digital health\nproducts addressing global healthcare needs, which is diﬃcu lt to\nachieve if data availability is limited.\nWhilst collecting high quality data remains an important\npublic health priority, sometimes rapid decision making is\nneeded. Emerging diseases and medical advances for instanc e\nnew drugs or vaccines, are two examples whereby public health\npriorities shift rapidly and policy makers cannot wait for the d ata\nfor thoroughly evidence-based decisions. Policy decision s are\nthen made using the best available knowledge, such as data fr om\nsimilar settings, data with known biases, or local expert opin ion.\nMachine learning models have also used this information to\nimpute estimates of disease incidence or prevalence much\nmore quickly.\nA “class” of machine learning techniques called deep neural\nnetworks (more broadly, deep learning algorithms) have rec ently\nseen a tremendous rise in their adoption across various ﬁelds (\n2).\nHowever, they require large “training” datasets to achieve high\npredictive performance, limiting their predictive ability wh en\nsuch data is not readily available. Large amounts of informa tion\nabout diseases exist online in free text and other unstructu red\nformats, which has led to an increasing interest in the use\nof methods from Natural Language Processing (NLP), called\nlanguage models, for healthcare.\nLanguage models are widely used, for example in predictive\ntext (\n3) and language translation ( 4). They estimate a probability\ndistribution over a set of words (semantics and syntax), to\ncompute the likelihood of some text occurring, given an\ninput sequence. In order for language models to process\nand understand natural language, free-text words (or whole\nsentences) are converted into numeric values; referred to a s\nword embeddings (or dense representations), which encode\ncontextual information and meaning. The quality of these\nembeddings will be dependent on the underlying mechanics of\nthe transformation and on the original text, which aﬀects the\nutility of the embeddings for downstream tasks. This makes w ord\nembeddings especially useful for healthcare, since pre-trai ned\nembedding models obtained from publicly available biomedic al\ntext and data can be exploited for a variety of tasks.\nThe eﬀectiveness of language models has been demonstrated\non general language problems, such as question answering\n(\n5) and sentiment analysis ( 6). In medical contexts, deep\nlearning approaches have been used for for diagnosis ( 7), disease\nclustering (8) and temporal modeling of electronic health records\n(9–13). Word embeddings have been explored for automated\ndisease cohort selection ( 14), predicting hospital readmission\nfrom clinical notes ( 15) and for automated radiology report\nannotation ( 16). However, their use has never been explored in\nan epidemiological application for estimating disease inciden ce.\nIn this study, we evaluated the utility of using diﬀerent\npre-learned language models [GloVe ( 17), BioBERT ( 18), and\nthe Universal Sentence Encoder [USE] ( 19)] to train disease\nincidence predictive networks. These language models comput e a\nvectorized representation of free text inputs. When transform ing\ndisease and country names to embeddings, these will capture t he\nassociated meaning and context, which (combined with age) c an\nbe exploited as a rich feature set for training a neural networ k for\ndisease incidence estimation. We compared the performance of\ndiﬀerent word embeddings in three diﬀerent scenarios:\n1. Where we have data on the incidence of the disease in other\ncountries and data on the incidence of other diseases in the\nsame country, and are simply missing data for a speciﬁc\ndisease-country pairing.\n2. Where we have data on the incidence of the disease in other\ncountries, but no data for the country of interest.\n3. Where we have data on the incidence of other diseases in that\nsame country, but no data on the disease of interest.\n2. METHODS\n2.1. Ethics Declarations\nThe analyses shown in this paper used publicly available,\naggregated data and therefore ethical approval was not requir ed.\n2.2. Data Sources\n2.2.1. Global Burden of Disease Study\nThe Global Burden of Disease (GBD) study (\n20), conducted\nby the Institute for Health Metrics and Evaluation (IHME),\naims to systematically and scientiﬁcally quantify health l osses\nglobally. The GBD dataset captures data from 195 countries\nglobally, and combines these data to produce accurate age- an d\nsex-speciﬁc estimates of the incidence, prevalence, and rate s of\ndisability and mortality that are caused by over 350 diseases and\ninjuries. Data are used from many sources, including survey s,\nadministrative data (including vital registration data, ce nsus data,\nepidemiological, and/or demographic surveillance data), ho spital\ndata, insurance claims data, disease registries, and other re lated\nsources. As well as data published in the scientiﬁc literatur e,\nunpublished data are sourced directly from collaborating\nresearchers. This dataset was used primarily to develop and\nvalidate the methods used in this paper using cross-validation .\n2.2.2. Additional Sources Using Published\nEpidemiological Data\nIn order to investigate the ability of the deep learning\nmodels to generalize, additional data stemming from published\nscientiﬁc literature and national reports was used as an\nindependent test set. These data were sourced to inform\ndisease incidence estimates for Babylon Health’s AI symptom\nchecker, and are typically data from national statistics, dise ase\nsurveillance/registries, and large-scale or population-lev el cohort\nand cross-sectional studies. It includes studies from 25 co untries\nfor 232 diseases.\nFrontiers in Digital Health | www.frontiersin.org 2 December 2020 | Volume 2 | Article 569261\nZhang et al. Applying Artiﬁcial Intelligence Methods\n2.3. Word Embeddings\nThere are many methods for learning word embeddings from\ntext. Words are generally represented as binary, one-hot\nencodings which map each word in a vocabulary to a unique\nindex in a vector. These word encodings can then be used as\ninputs to a machine learning model, such as a neural network,\nto learn the context of words. The information encoded in the se\nembeddings is tied to the context that was used to train the\nneural network. Word embeddings can discover hidden semant ic\nrelationships between words and can compute complex similarity\nmeasures. If these embeddings were obtained from training\non diﬀerent data sources, the context encoded would likely\ndiﬀer. Consequently, better performance in downstream tasks\nwill be linked to the information content encoded in these de nse\nrepresentations of words and its relationship with the task i tself.\nIn this paper, we evaluated diﬀerent types of word\nrepresentations, obtained by diﬀerent modeling strategies,\non the downstream task of predicting disease incidence. This\nwas performed by using the embeddings as inputs to a neural\nnetwork for estimating disease incidence.\n2.3.1. Global Vectors for Word Representation\nThe Global Vectors for Word Representation (GloVe) model\nis built on the word2vec method (\n21), which initially converts\nwords to numeric values. The GloVe model then learns its\nembeddings from a co-occurrence matrix of words, where each\npotential combination of words is represented as an entry in th e\nmatrix as the number of times the two words occur together\nwithin a pre-speciﬁed context window. This window moves\nacross the entire corpus. In this work, we used the pre-trained\nGloVe model trained on common crawl data (\n17) from raw web\npage data. Some Publicly available information about diseas es\nand demographics of diﬀerent countries are present in such data\nand therefore, it is expected that such embeddings will facilit ate a\nprediction in our model.\n2.3.2. BioBERT\nBidirectional Encoder Representations from Transformers\n(BERT) (\n22) is a contextualized word representation model\nwhich learns the context for a given word from the words that\nprecede and follow it in a body of text (\n22). We used BioBERT,\nwhich is a model initialized with the general BERT model but\npre-trained on large-scale biomedical corpora, such as PubMed\nabstracts and PMC full-text articles. This enables the mode l to\nlearn the biomedical context of words.\n2.3.3. Universal Sentence Encoder\nThe Universal Sentence Encoder (USE) is a language model\nwhich encodes context-aware representations of English\nsentences as ﬁxed-dimension embeddings.\n2.3.4. Feature Fusion\nIn addition to testing each of the language models individuall y,\nwe performed feature fusion to combine the three word\nembeddings into a single vector by concatenation. The neura l\nnetwork was then trained on the combined representation.\n2.3.5. Sentence Embeddings\nWe treat the names of diseases and countries as sentences\nbecause they may consist of multiple words. The models USE\nand BioBERT are capable of producing embeddings for sentences\nand for words. GloVe and other word2vec models are limited to\nproduce only embeddings for single words. In order to extract\nsentence embeddings with those models, we use the bag of words\napproach and computed the min, max and average values of all\nword embeddings within a bag of words.\n2.3.6. Evaluating the Contextual Information of Word\nEmbeddings\nIt is important to evaluate the context that each embedding\ntype captured, prior to using them for training disease incidenc e\nestimation models. For the embeddings to be meaningful, the\nword representations for either countries or diseases need to\nencapsulate relationships amongst each other. For instance,\ncountry embeddings for France and Spain should display\nsimilarities between each other that cover both geographica l and\nsocioeconomic metrics.\nWe performed two classiﬁcation experiments to evaluate\nthe context captured by the diﬀerent embeddings. In these\nexperiments, the input features were word embeddings obtaine d\nfrom either disease or country names whilst the classiﬁcation\nlabels were either GBD disease groups or country clusters (\n23).\nThe resulting classiﬁcation accuracy can serve as a metric t o\ncapture the contextual power of each embedding method when\napplied to either diseases or countries.\nThe ﬁrst experiment aimed at evaluating whether disease\nembeddings capture context and similarities between disease s.\nIn this experiment, we learned to predict the 17 high-level\nGBD disease groups (section A.6) from disease embeddings.\nThe second experiment was focused on embeddings computed\nfrom countries and whether they can capture both geographical\nand economic dimensions. This can be evaluated by considerin g\nthe classiﬁcation of 21 country clusters, such as “High-Inco me\nAsia Paciﬁc” and “Western Europe” from country embeddings\n(section A.7).\nLinear Support Vector Machines (\n24) were trained for each\nexperiment across a candidate set of hyperparameters. Models\nwere trained and evaluated using 3-fold cross-validation. T he\ncross-validation experiments were repeated ten times to miti gate\nany potential bias in the training and validation split. The be st\nperforming model for each embedding across both experiments\nwere then used to assess the accuracy.\n2.4. Training a Neural Network to Estimate\nDisease Incidence\nThe process for training a neural network to predict disease\nincidence rates is illustrated in Figure 1. These input features to\nthe neural network consist of embeddings of disease, country ,\nand age group. The neural network outputs a prediction for the\nincidence of a speciﬁed disease. Prior to training, the value s for\ndisease incidence are pre-processed with a log transformatio n.\nAn inverse log transformation must therefore be applied to the\nneural network output to obtain the disease incidence rate.\nFrontiers in Digital Health | www.frontiersin.org 3 December 2020 | Volume 2 | Article 569261\nZhang et al. Applying Artiﬁcial Intelligence Methods\nFIGURE 1 | An illustration of machine learning pipeline we used for est imation of disease incidence. si represents the sentence embeddings of the disease of intere st\n(e.g., HIV), ci represents the embedding of the country of interest (e.g., U K), ai represents the age group of interest (e.g., 30–34 years), an d labeli represents the\nground-truth value (from the GBD study).\n2.4.1. Applications of Disease Incidence Prediction\nWe investigated the three following incidence\nestimation applications:\n• Application 1—speciﬁc disease-country pairs . This task\nsimulates a scenario where we need to predict incidence rates\nfor a speciﬁc diseases in a selected set of countries. This is\nimportant if data points are missing or are diﬃcult to collect\nin the target country. For this application, we have data of the\ntarget disease in other countries, and data of other disease s in\nall countries yet data for a speciﬁc target disease-country pa ir\nis missing.\n• Application 2—previously unseen countries . There may\nbe cases where there is no high-quality data available in\ncountries with poor healthcare and data infrastructure. For\nthese situations, it may be desirable to predict incidence ra tes\nof all diseases. For this application, we simulate the case whe re\nwe have no data for any disease in the target country but\ncomprehensive incidence data for all others.\n• Application 3—previously unseen diseases . This represents\na situation where we have a key disease for which incidence\ndata is diﬃcult to obtain. This application consequently deal s\nwith the prediction of disease incidence rates for a given\ndisease. In this case, incidence data is available for other\ndiseases, but there is no data about the new, “unseen” diseas e\nin any country.\n2.4.2. Data Inputs\n2.4.2.1. Disease embeddings\nWe generated disease embeddings using each of the methods\ndescribed in section 2.3.\n2.4.2.2. Country embeddings\nWe used the GloVe model to create representations of countrie s.\nWe observed in our experiments that this model performed best\nfor that purpose (see sections 2.3.6 and 3.2, Table 1).\n2.4.2.3. Age embeddings\nThe 20 age groups of 5-years periods (0–4, 5–9, . . . , 95+) were\nrepresented as binary one-hot vectors. Representing age groups\nin this way means that they are treated as separate categories , so\nthat non-linear associations between incidence and age can e asily\nbe modeled.\n2.4.3. Experimental Set-Up\nWe have used two independent and non-overlapping data sets for\nmodel development and evaluation.\nThe ﬁrst data set is the GBD data (section 2.2.1) which was\nused for model development and hyper-parameter tuning. This\ndataset consists of 199 diseases that are annotated with inc idence\nvalues across 195 countries and 20 age groups. We removed\na subset of data points with zero incidence values from the\noriginal GBD study (132,903/626,580 data points, 21%). Zero\nincidence can happen either because data is not available or th e\nactual incidence value is zero for some speciﬁc data entries. Since\nthe distribution of disease incidence values was highly ske wed,\nwe log-transformed the data to base 10. The predictions from\nthe model were inverse log-transformed to derive estimates of\ndisease incidence.\nWe selected the hyperparameters of the model using 10-fold\ncross-validation on the GBD data. This avoids over-optimist ic\nestimates of the model’s performance, which can arise if the\nmodel is trained and tested on the same data. In each fold, we\nFrontiers in Digital Health | www.frontiersin.org 4 December 2020 | Volume 2 | Article 569261\nZhang et al. Applying Artiﬁcial Intelligence Methods\nTABLE 1 | Classiﬁcation results for GBD disease groups using disease e mbeddings and country clusters with country embeddings.\nGBD disease groups Country clusters\nModel GloVe BioBERT USE GloVe BioBERT USE\nAccuracy 0.77 (0.03) 0.77 (0.02) 0.66 (0.02) 0.73 (0.02) 0.17 (0.02) 0.62 (0.03)\nMean (Standard deviation) of cross-validation results are reported. The n umbers in bold highlight the best performing models.\ntrain with 90% of the GBD data, and predict on the remaining\n10%. We use the following split for each application (see\nsection 2.4):\nFor Application 1 , each fold contains randomly selected\ncountry-disease pairs, where it is possible that data from the\nsame disease or country can occur in the training and validat ion\nset but not both. This model is optimized for predicting diseas e\nincidence for country-disease combinations the model has n ot\nseen before, for example HIV in Singapore. In this example, the\ntraining data may contain disease incidence estimates for o ther\ndiseases in Singapore, and for HIV in other countries. The mod el\nis therefore able to learn from these combinations of samples a nd\nthen to predict the incidence for a diﬀerent disease-country pa ir.\nFor Application 2 , we ensured cross-validation was\nindependent of the country. Within each fold of the data,\nthe model was trained on data from 90% of countries, and\nvalidated on data from the remaining 10% of countries.\nFor Application 3 , we ensured that cross-validation was\nindependent of disease, but not country. Within each data fol d,\nthe model was trained on data from 90% of diseases, and\nvalidated on data using the remaining 10% of diseases.\nWe further used the published epidemiological data as\nthe independent test set. The best performing model found\nand trained on the GBD data was then evaluated on this\nsecondary dataset.\n2.4.4. Implementation Details\nWe performed a hyperparameter search on the neural networks\nfor each of the three applications. The ﬁnal neural network\narchitecture was selected based on the overall performance a cross\nall applications using 10-fold cross validation. The resulti ng\nneural network has ﬁve hidden layers that are stacked in form of\na funnel with 256, 128, 64, 16, and 4 neurons, respectively. Eac h\nlayer of the neural network consisted of a fully connected lay er,\nfollowed by Batch Normalization (\n25) and a Rectiﬁed Linear\nUnit (ReLU). We applied the root mean squared error as a loss\nfunction on the predicted outputs. Lastly, we used the Adam\noptimizer (\n26) with an initial learning rate of 3 × 10− 4 and\nstandard values for the exponential decay of moment estimate s.\n2.4.5. Evaluation of Model Performance\n2.4.5.1. Baseline comparisons\nFor the dataset sources described in section 2.4.3 (GBD\ndata and published epidemiological data), we investigated th e\nperformance of neural networks in predicting disease inciden ce\nacross applications deﬁned in section 2.4.1. Additionally, w e\ncompared the performance of the neural networks against three\nseparate baselines:\n1. The global average incidence (Global) for the disease\nof interest. This is a naïve baseline that all models\nshould outperform.\n2. A ridge regression (RidgeReg) model trained on language\nembeddings denoted as RidgeReg. This allows us to gauge\nthe performance gain obtained from more complex models,\nsuch as neural networks. Note that this baseline uses BioBERT\nembeddings for diseases and GloVe embeddings for countries\nas inputs.\n3. Neural network models where the input features are one-\nhot encoded vectors (OneHot) for countries and diseases,\nrespectively. We considered a model with only disease labels\n(OneHotd), only country labels (OneHot c), and one that\nuses both disease and country one-hot vectors (OneHot d,c).\nThis allows us to assess the gain obtained by using\nlanguage embeddings.\n2.4.5.2. Metrics for evaluating performance\nWe used the mean absolute error (MAE) in log10 space to\nevaluate the performance of the disease incidence estimation .\nFor example, a prediction with MAE of 0.2 is either 1.58 times\nlarger or lower than the “ground truth” value. The factor of\n1.58 is computed by inverse transformation (10 0.2 = 1.58). To\nmeasure the similarity of relative rankings of the estimate s (in\nour case, between our predictions and disease incidence value s\nin the GBD study), we calculated the inter-group concordanc e ρ c\nranking whose values are bounded between 0 (worst) and 1 (bes t)\n(detailed deﬁnition can be found in section A.1 in the Appendix).\n3. RESULTS\nWe ﬁrst evaluated the contextual information of the country\nand disease embeddings to justify their use in all following\nexperiments (section 3.1). We then evaluated the performance\nof each language embedding on the three possible applications\n(section 3.2) and report results for both the GBD (section\n2.2.1) cross-validation results and the independent test se t\n(section 2.2.2). We also conducted additional experiments ( see\nAppendix) including: (1) estimating the accuracy of the BioBERT\nfeature model on the GBD data across diﬀerent age groups\n(section A.2), (2) estimating the accuracy of the BioBERT fe ature\nmodel on the GBD data across various types of diseases (section\nA.3) and (3) demonstrating an illustrative scenario where w e\npredicted UK disease incidence for previously unseen diseases\n(section A.4).\n3.1. Contextual Information of Disease and\nCountry Embeddings\nResults for the classiﬁcation experiments for GBD disease grou ps\nusing disease embeddings and country clusters using country\nFrontiers in Digital Health | www.frontiersin.org 5 December 2020 | Volume 2 | Article 569261\nZhang et al. Applying Artiﬁcial Intelligence Methods\nTABLE 2 | Application 2 model performance with different input featu res for previously unseen diseases on the GBD and the test dat a.\nData\nModel Global RidgeReg OneHot d OneHotc OneHotd,c BioBERT GloVe USE Fusion\nTraining and Validation (from the GBD study)\nMAE N/A 1.03 N/A 0.805 N/A 0.781 0.807 0.765 0.736\nρ c N/A 0.726 N/A 0.790 N/A 0.796 0.775 0.826 0.806\nTest (from the epidemiological literature)\nMAE N/A 1.02 N/A N/A N/A 0.933 0.989 1.01 2.43\nρ c N/A 0.982 N/A N/A N/A 0.967 0.971 0.962 0.931\nThe numbers in bold highlight the best performing models.\nTABLE 3 | Application 1 model performance with different input featu res for speciﬁc disease-country pairs on the GBD and the test d ata.\nData\nModel Global RidgeReg OneHot d OneHotc OneHotd,c BioBERT GloVe USE Fusion\nTraining and Validation (from the GBD study)\nMAE 0.207 0.559 0.166 0.155 0.152 0.157 0.157 0.168 0.157\nρ c 0.952 0.867 0.987 0.988 0.990 0.990 0.988 0.985 0.988\nTest (from the epidemiological literature)\nMAE N/A 1.13 N/A N/A N/A 0.835 0.910 1.06 3.78\nρ c N/A 0.97 N/A N/A N/A 0.977 0.970 0.960 0.938\nThe numbers in bold highlight the best performing models.\nTABLE 4 | Application 3 model performance with different input featu res for previously unseen countries on the GBD and the test da ta.\nData\nModel Global RidgeReg OneHot d OneHotc OneHotd,c BioBERT GloVe USE Fusion\nTraining and Validation (from the GBD study)\nMAE 0.212 0.562 0.204 N/A N/A 0.197 0.198 0.209 0.196\nρ c 0.965 0.866 0.953 N/A N/A 0.954 0.955 0.953 0.955\nTest (from the epidemiological literature)\nMAE N/A 1.12 N/A N/A N/A 0.881 0.921 1.07 0.937\nρ c N/A 0.976 N/A N/A N/A 0.972 0.977 0.970 0.978\nThe numbers in bold highlight the best performing models.\nembeddings are shown in Table 1. We report the mean and\nstandard-deviation from 10-repeated 3-fold cross validati on\nexperiments. The results indicate that GloVe and use country\nembeddings capture meaningful relationships between countr ies\nwhilst BioBERT country embeddings are ineﬀective as they\nwere trained on large-scale biomedical corpora. In contrast , all\nlanguage models are able to eﬀectively capture disease semant ics\nwith BioBERT and GloVe performing best and equitably.\n3.2. Effect of Language Embedding on\nDisease Incidence Estimation\nResults for the performance across various embeddings are\nreported for the GBD data and independent test data in\nTables 2–4. Models that exploited BioBERT embeddings on\naverage saw the best performance with consistently lower MAE\nand high concordance scores. However, models that employ\nGloVe and USE embeddings can also perform well, such as in\nthe previously unseen countries application ( Table 4) where the\nFusion model performed best on average.\nWhilst most embedding methods produced accurate\nincidence estimates in the GBD dataset, it is apparent that\nBioBERT, followed by GloVe embeddings, produced the best\nresults generally in the independent test set when compared to\nUSE. For instance, BioBERT and GloVe had an MAE of 0.157\nand 0.157 with concordance of 0.990 and 0.988, respectively\ncompared to an MAE of 0.168 and a concordance of 0.985\nfor USE in the speciﬁc disease-country pairs application\n(Table 3). This illustrates that these embeddings contain\ninformative, contextual information. This is validated in the\none-hot model, which used one-hot encoded representations\nand suﬀered in performance as seen in the previously unseen\ndiseases ( Table 3) and previously unseen countries ( Table 4).\nNote that models with one-hot features are only applicable\nwhen the target disease or country is present in the training\nand test data. For this reason, they do not apply to the test\nFrontiers in Digital Health | www.frontiersin.org 6 December 2020 | Volume 2 | Article 569261\nZhang et al. Applying Artiﬁcial Intelligence Methods\nset because the diseases in the training and test data do\nnot overlap.\nWe evaluated the utility of BioBERT embeddings by\ncomparing the performance of the neural network method\n(BioBERT) with a ridge regression that used BioBERT feature s\n(RidgeReg). The neural network method saw consistently better\nresults compared to the ridge regression across all applicatio ns\nfor the GBD dataset.\nThe performance of most neural network models was\nconsistently high in the speciﬁc disease-country pairs applica tion\n(Table 3) and previously unseen countries ( Table 4). However,\nthere was a marked decrease in the validation metrics within the\npreviously unseen diseases application ( Table 3). For instance,\nthe MAE of BioBERT rose from 0.157 ( Table 2) and 0.197\n(Table 4) to 0.781 whilst the concordance of GloVe for instance\ndropped from 0.955 ( Table 3) and 0.988 ( Table 4) to 0.775.\n3.3. Performance Across Different\nMagnitudes of Incidence\nWe examined the performance of the network trained with\nfeature fusion across diseases with diﬀerent magnitudes of\nincidence rate. We compared the distribution of errors with\nthe baseline model that predicted incidence rates using a glo bal\naverage estimate ( Figure 2). Note that in these plots the Y-axis\nshows the MAE in log space and X-axis shows the predicted\nexponent of the incidence value. The incidence is normalized\nfor a population of 100,000 people. The true incidence can be\ncomputed as follows: I = 10x/ 100,000.\nFor the previously unseen countries application ( Figure 2),\nwe observed a decrease in the error magnitude at higher\nincidence rates across the baseline and the trained network . This\nillustrates that both predictive models saw higher accuracy for\ncommon diseases whilst exhibiting a reduction in performanc e\nfor rare diseases. However, this eﬀect is more pronounced in th e\nbaseline model.\nIn the previously unseen diseases application using neural\nnetworks with feature fusion, we analyzed the error distrib ution\nin the GBD validation set and the independent test set with\ndata originating from peer-reviewed literature ( Figure 3). For\nthis application, the residual distribution is much wider and\nthe accuracy is lower for both the GBD validation set and test\nset. Importantly, we did not validate the global average base line\nmethod in this application since a global average prediction ca n\nnot be made if we deal with new diseases.\n4. DISCUSSION\nIn this study, we tested the ability of diﬀerent language mode ls\nto encode contextual information, and used the correspondin g\nembeddings as inputs to a neural network which was used to\npredict disease incidence. We found that on average, models\nusing BioBERT embeddings performed best across all metrics.\nWe observed high performance when predicting for previously\nunseen countries and speciﬁc disease-country pairs, which wa s\nconsistent across age groups (for more details, see section A .2).\nPerformance for previously unseen diseases was lower, varie d\nsubstantially with age, and performance was notably lower fo r\nFIGURE 2 | Residual plots for (A) naive baseline and (B) NN predictions with\nfeature fusion for previously unseen countries on the GBD da taset. Red lines\nindicate the standard deviation. This is also the logarithm of the error. It is\npositive (negative) if the ML model overestimates (underes timates) the true\nvalue. The plots show that the errors for diseases with a high er incidence rate\nare lower.\ndiseases which are highly dependent on location and climate.\nOverall, predictions were more accurate for common diseases\nthan rare diseases (see section A.3).\nBioBERT was on average, the best-performing language\nmodel for creating disease embeddings across all three\napplications: predicting disease incidence for previously\nunseen diseases, previously unseen countries, and speciﬁc\ndisease-country pairs. The word embeddings for BioBERT\nare trained with text from medical journals and other clinica l\nliterature; this model should therefore have the most relev ant\ncontext for interpreting words, which should be reﬂected in\nbetter disease incidence estimates from the neural network\nusing these embeddings. Interestingly for previously unseen\ndiseases and speciﬁc disease-country pairs, using feature fus ion\nto combine information from the three language models\nresulted in substantially higher MAE than using BioBERT\nor other language models individually when we tested our\nmodels on external data from published epidemiological\nliterature. This suggests that using BioBERT alone results\nin suﬃcient contextual information, and further feature\nFrontiers in Digital Health | www.frontiersin.org 7 December 2020 | Volume 2 | Article 569261\nZhang et al. Applying Artiﬁcial Intelligence Methods\nFIGURE 3 | Residual plots for previously unseen diseases on data from (A)\nGBD and (B) peer-reviewed journals. Red lines indicate the standard de viation.\naugmentation from other sources only adds redundant or\ncorrelated data.\nWhen comparing our predictions for the GBD data, we\nobserved that performance for previously unseen diseases was\nsigniﬁcantly lower than for previously unseen countries and\nspeciﬁc disease-country pairs. The purely data-driven neural\nnetwork is able to predict disease incidence better for previo usly\nunseen countries and speciﬁc disease-country pairs because i t\nalready has data for the incidence of the disease it is trying\nto predict, and can draw suﬃcient context from the country\nembeddings to make a prediction for a new country. However, it\nis diﬃcult to fully encapsulate how a previously unseen disease\nis similar to other diseases within a word embedding, and so\nthe model’s predictive ability is more limited for previously\nunseen diseases. This reﬂects our general state of knowledg e;\nwe can make good inferences for disease incidence in countri es\nwhere data is lacking, based on our knowledge of the country’ s\nsocioeconomic situation, location, and healthcare provisi on,\nbut struggle to predict the incidence of an unknown disease,\nregardless of how much data we have on other diseases in the\nsame country. This is because the incidence of a disease is\nnot only inﬂuenced by country-level factors but also by many\nbiological, immunological, and sociodemographic factors.\nWe also observed discrepancies in the MAE of the predictions\nacross the GBD study and the published epidemiological study\n(independent test set) yet generally equitable performance i n\nthe concordance index for disease-country pairs ( Table 2) and\npreviously unseen countries ( Table 4) applications. Whilst this\nmay suggest that the trained models have trouble generalizin g,\nthis is in fact a symptom of the inability of machine learning\nmodels to adapt to data that diﬀers from the training data\nstatistics. Both datasets are independent and contain certa in non-\noverlapping sets of examples. As the test set contains new unseen\nexamples, we face a scenario where incidence values need to be\npredicted for out of distribution examples. Additionally, it is\na valid to assume that both datasets are drawn from diﬀerent\ndistributions as the mechanisms for generating the data diﬀe r.\nThis is a case of distribution shift, which can negatively aﬀe ct\nperformance (\n27). In eﬀect, both out of distribution examples\nand distribution shift are likely to negatively impact metric s,\nsuch as the MAE, which measures the average magnitude of\nthe error. Despite this disparity, we observed equitable leve ls\nof performance when considering the inter-group concordanc e\nindex (except for the application for the previously unseen\ndiseases). This demonstrates that the ranking of the predict ions\nis maintained despite the systematic errors introduced by th e\nnew dataset; further supporting the utility of language model s\nand neural networks in the challenging problem of predicting\ndisease incidence.\nDeep learning methods for predicting disease incidence,\nwhich use contextual embeddings learnt from unstructured\ninformation, have the potential to give better estimates of d isease\nincidence than are currently available for settings where h igh\nquality data is lacking. In resource poor settings, where heal thcare\ninfrastructure is weak and expressed through the lack of doct ors,\nnurses and hospitals, it is unlikely that there is access to re liable\ndata that facilitates estimating disease incidence (\n28). In these\ncircumstances, the deployment of automated methods, such as\nthe those presented in this paper, show the potential to beneﬁt\nsuch populations (\n29).\nStudies, such as the GBD, which rigorously model disease\nstatistics using information from multiple data sources, are\nlimited by the time lag of data becoming available, and in the ir\nability to incorporate new conditions due to the substantial eﬀort\ninvolved in reviewing data and building new models. Whilst\nthe method we propose in this paper may be less rigorous,\nit is substantially quicker to implement for new diseases and\ncan be easily updated to incorporate up-to-date contextual\ninformation for existing diseases. We therefore suggest it a s a\nuseful complement to existing modeling eﬀorts, where data is\nrequired more rapidly or at larger scale than traditional met hods\nallow for. This could be particularly useful for use in AI-drive n\ndigital health products, where the data will undergo further\nprocessing and a clinician-validated approximation of the dise ase\nincidence is adequate.\n5. CONCLUSION\nIn this work, we developed a machine learning method\nbased on deep learning and transfer learning. We used\nembeddings, which had been trained by their creators using\nFrontiers in Digital Health | www.frontiersin.org 8 December 2020 | Volume 2 | Article 569261\nZhang et al. Applying Artiﬁcial Intelligence Methods\nlarge amounts of unstructured and freely accessible text dat a, to\ntrain the target neural network for incidence estimation us ing\nepidemiological data from the GBD study.. We investigated th ree\npopular diﬀerent language model architectures and text corpora\n(BioBERT, USE, GloVE) in addition to numerous baselines. We\nhave shown that the BioBERT language model performs well at\nencoding contextual information relating to disease incid ence.\nThe resulting embeddings can be used as inputs to a neural\nnetwork to successfully predict disease incidence for previo usly\nunseen countries and speciﬁc disease-country pairs, but is mo re\nlimited in predicting for previously unseen diseases. Whilst\nthis method is not a replacement for robust epidemiological\nmodeling, we suggest that it could be a useful alternative wh en\nfaced with situations where approximate estimates are needed\nrapidly on a large-scale.\nDATA AVAILABILITY STATEMENT\nThe data analyzed in this study is subject to the following\nlicenses/restrictions: we used data from the Global Burden of\nDisease study to develop and test the methods used in this paper ,\nwhich is available under license from the Institute for Heal th\nMetrics and Evaluation. The additional data used for validat ion is\navailable from national statistics, disease surveillance/ registries,\nand scientiﬁc research papers. Requests to access the GBD\ndatasets should be directed to http://www.healthdata.org/ gbd.\nAUTHOR’S NOTE\nThis manuscript was developed as part of research initiative at\nBabylon Health.\nAUTHOR CONTRIBUTIONS\nYZ: data analysis, ﬁgures, and writing. RW: data interpretatio n,\nliterature search, and writing. JW: data collection, data\ninterpretation, and writing. AB and CH: study design\nand writing. SL: study design and data collection. FB:\ndata analysis and writing. SJ and YP: study design\nand data interpretation. YZ and RW: study design. All\nauthors contributed to the article and approved the\nsubmitted version.\nSUPPLEMENTARY MATERIAL\nThe Supplementary Material for this article can be found\nonline at: https://www.frontiersin.org/articles/10.338 9/fdgth.\n2020.569261/full#supplementary-material\nREFERENCES\n1. Kyu HH, Abate D, Abate KH, Abay SM, Abbafati C, Abbasi N,\net al. Global, regional, and national disability-adjusted life-ye ars (DALYs)\nfor 359 diseases and injuries and healthy life expectancy (HALE) for\n195 countries and territories, 1990–2017: a systematic analysis f or the\nGlobal Burden of Disease Study 2017. Lancet. (2018) 392:1859–922.\ndoi: 10.1016/S0140-6736(18)32335-3\n2. Le Cun Y, Bengio Y, Hinton G. Deep learning. Nature. (2015) 521:436–44.\ndoi: 10.1038/nature14539\n3. Dai Z, Yang Z, Yang Y, Carbonell JG, Le QV , Salakhutdinov R. Tra nsformer-\nXL: attentive language models beyond a ﬁxed-length context. CoRR. (2019)\nabs/1901.02860. Available online at: http://arxiv.org/abs/1 901.02860\n4. Brants T, Popat AC, Xu P , Och FJ, Dean J. Large language models\nin machine translation. In: Proceedings of the 2007 Joint Conference on\nEmpirical Methods in Natural Language Processing and Computational Natural\nLanguage Learning (EMNLP-CoNLL).Prague: Association for Computational\nLinguistics (2007). p. 858–67.\n5. Kumar A, Irsoy O, Ondruska P , Iyyer M, Bradbury J, Gulrajani I, et a l. Ask\nme anything: dynamic memory networks for natural language processing.\nIn: International Conference on Machine Learning.New York, NY (2016). p.\n1378–87.\n6. Dos Santos C, Gatti M. Deep convolutional neural networks for sentiment\nanalysis of short texts. In: Proceedings of COLING 2014, The 25th International\nConference on Computational Linguistics: Technical PapersExperimental Set-\nup. Dublin (2014). p. 69–78.\n7. Liu S, Liu S, Cai W, Pujol S, Kikinis R, Feng D. Early diagnosis of\nAlzheimer’s disease with deep learning. In: 2014 IEEE 11th International\nSymposium on Biomedical Imaging (ISBI).Beijing: IEEE (2014). p. 1015–8.\ndoi: 10.1109/ISBI.2014.6868045\n8. Doshi-Velez F, Ge Y, Kohane I. Comorbidity clusters in autism spectrum\ndisorders: an electronic health record time-series analysis. Pediatrics. (2014)\n133:e54. doi: 10.1542/peds.2013-0819\n9. Choi E, Schuetz A, Stewart WF, Sun J. Medical concept representa tion\nlearning from electronic health records and its application on heart failu re\nprediction. arXiv. (2016) 160203686.\n10. Choi Y, Chiu CYI, Sontag D. Learning low-dimensional represent ations of\nmedical concepts. AMIA Summits Transl Sc Proc.(2016) 2016:41.\n11. Choi E, Bahadori MT, Schuetz A, Stewart WF, Sun J. Doctor\nAI: Predicting clinical events via recurrent neural networks. In:\nMachine Learning for Healthcare Conference. Boston, MA (2016).\np. 301–18.\n12. Lipton ZC, Kale DC, Elkan C, Wetzel R. Learning to diagnose\nwith LSTM recurrent neural networks. arXiv. (2015) 151\n103677.\n13. Che Z, Purushotham S, Khemani R, Liu Y. Distilling knowledge fro m\ndeep networks with applications to healthcare domain. arXiv. (2015) 151\n203542.\n14. Glicksberg B, Miotto R, Johnson K, Shameer K, Li L, Chen R, et al.\nAutomated disease cohort selection using word embeddings from\nElectronic Health Records. Pac Symp Biocomput. (2018) 23:145–56.\ndoi: 10.1142/9789813235533_0014\n15. Huang K, Altosaar J, Ranganath R. ClinicalBERT: Modeling Clinical Notes and\nPredicting Hospital Readmission.(2019).\n16. Banarjee I, Chen M, Lungren M, Rubin D. Radiology report annota tion\nusing intelligent word embeddings: applied to multi-institutional c hest\nCT cohort. J Biomed Inform. (2018) 77:11–20. doi: 10.1016/j.jbi.2017.\n11.012\n17. Pennington J, Socher R, Manning CD. Glove: Global vectors f or word\nrepresentation. In: Proceedings of the 2014 Conference on Empirical Methods\nin Natural Language Processing (EMNLP). Doha (2014). p. 1532–43.\ndoi: 10.3115/v1/D14-1162\n18. Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, et al. Biobert: pre-traine d\nbiomedical language representation model for biomedical text minin g. arXiv.\n(2019) 190108746. doi: 10.1093/bioinformatics/btz682\n19. Cer D, Yang Y, Yi Kong S, Hua N, Limtiaco N, John RS, et al. Universal\nSentence Encoder.(2018).\n20. James SL, Abate D, Abate KH, Abay SM, Abbafati C, Abbasi N, et al. Global,\nregional, and national incidence, prevalence, and years lived wit h disability\nfor 354 diseases and injuries for 195 countries and territories, 1 990–2017:\na systematic analysis for the Global Burden of Disease Study 2017 . Lancet.\n(2018) 392:1789–858. doi: 10.1016/S0140-6736(18)32279- 7\nFrontiers in Digital Health | www.frontiersin.org 9 December 2020 | Volume 2 | Article 569261\nZhang et al. Applying Artiﬁcial Intelligence Methods\n21. Mikolov T, Sutskever I, Chen K, Corrado G, Dean J. Distributed\nrepresentations of words and phrases and their compositionality. In: Advances\nin Neural Information Processing Systems. (Lake Tahoe, NV: Harrahs and\nHarveys) (2013). p. 5021.\n22. Devlin J, Chang MW, Lee K, Toutanova K. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv. (2018)\n181004805.\n23. Global Burden of Disease Collaborative Network. Global Burden\nof Disease Study 2017 (GBD 2017) Cause, REI, and Location\nHierarchies. Seattle, WA: Institute for Health Metrics and Evaluation\n(IHME) (2017).\n24. Cortes C, Vapnik V. Support-vector networks. Mach Learn.(1995) 20:273–97.\ndoi: 10.1007/BF00994018\n25. Ioﬀe S, Szegedy C. Batch Normalization: Accelerating Deep Network Training\nby Reducing Internal Covariate Shift.(2015).\n26. Kingma DP , Ba J. Adam: A Method for Stochastic Optimization.(2014).\n27. Ben-David S, Blitzer J, Crammer K, Pereira F. Analysis of representa tions for\ndomain adaptation. In: Schölkopf B, Platt JC, Hoﬀman T, editors. Advances in\nNeural Information Processing Systems 19.Vancouver, BC: MIT Press (2007).\np. 137–44.\n28. Nambiar B, Hargreaves DS, Morroni C, Heys M, Crowe S, Pagel C, et a l.\nImproving health-care quality in resource-poor settings. Bull World Health\nOrgan. (2017) 95:76–8. doi: 10.2471/BLT.16.170803\n29. Wahl B, Cossy-Gantner A, Germann S, Schwalbe NR. Artiﬁcial in telligence\n(AI) and global health: how can AI contribute to health in resource -poor\nsettings? BMJ Global Health.(2018) 3:798. doi: 10.1136/bmjgh-2018-000798\nConﬂict of Interest: All authors were employed by Babylon Health.\nCopyright © 2020 Zhang, Walecki, Winter, Bragman, Lourenco, Hart, Baker, Perov\nand Johri. This is an open-access article distributed underthe terms of the Creative\nCommons Attribution License (CC BY). The use, distributionor reproduction in\nother forums is permitted, provided the original author(s) and the copyright owner(s)\nare credited and that the original publication in this journal is cited, in accordance\nwith accepted academic practice. No use, distribution or reproduction is permitted\nwhich does not comply with these terms.\nFrontiers in Digital Health | www.frontiersin.org 10 December 2020 | Volume 2 | Article 569261",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.71770179271698
    },
    {
      "name": "Machine learning",
      "score": 0.6850842833518982
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6618444919586182
    },
    {
      "name": "Representation (politics)",
      "score": 0.5328391790390015
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5232335925102234
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.523186981678009
    },
    {
      "name": "Sentence",
      "score": 0.5135005116462708
    },
    {
      "name": "Language model",
      "score": 0.4791133403778076
    },
    {
      "name": "ENCODE",
      "score": 0.4646766781806946
    },
    {
      "name": "Artificial neural network",
      "score": 0.4436882436275482
    },
    {
      "name": "Natural language processing",
      "score": 0.4418376386165619
    },
    {
      "name": "Disease",
      "score": 0.42286720871925354
    },
    {
      "name": "Medicine",
      "score": 0.16354233026504517
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210134914",
      "name": "Babylon Health",
      "country": "GB"
    }
  ]
}