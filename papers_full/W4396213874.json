{
  "title": "Synoptic Reporting by Summarizing Cancer Pathology Reports using Large Language Models",
  "url": "https://openalex.org/W4396213874",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2886184781",
      "name": "Sivaraman Rajaganapathy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2784921537",
      "name": "Shaika Chowdhury",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5095926126",
      "name": "Vincent Buchner",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098223376",
      "name": "Zhe He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2132814769",
      "name": "Xiaoqian Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096165556",
      "name": "Ping Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A222328280",
      "name": "James R. Cerhan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2561837190",
      "name": "Nansu Zong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2791081619",
    "https://openalex.org/W4213108646",
    "https://openalex.org/W2569382952",
    "https://openalex.org/W2406701925",
    "https://openalex.org/W2945899651",
    "https://openalex.org/W3095828973",
    "https://openalex.org/W4213196899",
    "https://openalex.org/W2114014706",
    "https://openalex.org/W2170408491",
    "https://openalex.org/W2561650518",
    "https://openalex.org/W2765488845",
    "https://openalex.org/W3006614947",
    "https://openalex.org/W4308522603",
    "https://openalex.org/W3178327418",
    "https://openalex.org/W4224311357",
    "https://openalex.org/W3127145819",
    "https://openalex.org/W3164718925",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4389421203",
    "https://openalex.org/W6838461927",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4392359953",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W2470288096",
    "https://openalex.org/W4231696577",
    "https://openalex.org/W2040319037",
    "https://openalex.org/W2071120326",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4392740458",
    "https://openalex.org/W4392271030",
    "https://openalex.org/W4225948283",
    "https://openalex.org/W4386655789",
    "https://openalex.org/W4378473969",
    "https://openalex.org/W1991750682"
  ],
  "abstract": "Abstract Background Synoptic reporting, the documenting of clinical information in a structured manner, is known to improve patient care by reducing errors, increasing readability, interoperability, and report completeness. Despite its advantages, manually synthesizing synoptic reports from narrative reports is expensive and error prone when the number of structured fields are many. While the recent revolutionary developments in Large Language Models (LLMs) have significantly advanced natural language processing, their potential for innovations in medicine is yet to be fully evaluated. Objectives In this study, we explore the strengths and challenges of utilizing the state-of-the-art language models in the automatic synthesis of synoptic reports. Materials and Methods We use a corpus of 7,774 cancer related, narrative pathology reports, which have annotated reference synoptic reports from Mayo Clinic EHR. Using these annotations as a reference, we reconfigure the state-of-the-art large language models, such as LLAMA-2, to generate the synoptic reports. Our annotated reference synoptic reports contain 22 unique data elements. To evaluate the accuracy of the reports generated by the LLMs, we use several metrics including the BERT F1 Score and verify our results by manual validation. Results We show that using fine-tuned LLAMA-2 models, we can obtain BERT Score F1 of 0.86 or higher across all data elements and BERT F1 scores of 0.94 or higher on over 50% (11 of 22) of the questions. The BERT F1 scores translate to average accuracies of 76% and as high as 81% for short clinical reports. Conclusions We demonstrate successful automatic synoptic report generation by fine-tuning large language models.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5872622728347778
    },
    {
      "name": "Cancer",
      "score": 0.5239834189414978
    },
    {
      "name": "Natural language processing",
      "score": 0.44708919525146484
    },
    {
      "name": "Information retrieval",
      "score": 0.40816596150398254
    },
    {
      "name": "Data science",
      "score": 0.33710020780563354
    },
    {
      "name": "Pathology",
      "score": 0.3346376419067383
    },
    {
      "name": "Medicine",
      "score": 0.2487625777721405
    },
    {
      "name": "Internal medicine",
      "score": 0.06002318859100342
    }
  ],
  "institutions": [],
  "cited_by": 3
}