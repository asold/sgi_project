{
  "title": "CORECODE: A Common Sense Annotated Dialogue Dataset with Benchmark Tasks for Chinese Large Language Models",
  "url": "https://openalex.org/W4393147036",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2003480995",
      "name": "Dan Shi",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A5047526176",
      "name": "Chaobin You",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A2115630446",
      "name": "Jiantao Huang",
      "affiliations": [
        "Zhejiang Lab"
      ]
    },
    {
      "id": "https://openalex.org/A2127495539",
      "name": "Taihao Li",
      "affiliations": [
        "Zhejiang Lab"
      ]
    },
    {
      "id": "https://openalex.org/A2139156831",
      "name": "Deyi Xiong",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A2003480995",
      "name": "Dan Shi",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A5047526176",
      "name": "Chaobin You",
      "affiliations": [
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A2115630446",
      "name": "Jiantao Huang",
      "affiliations": [
        "Zhejiang Lab"
      ]
    },
    {
      "id": "https://openalex.org/A2127495539",
      "name": "Taihao Li",
      "affiliations": [
        "Zhejiang Lab"
      ]
    },
    {
      "id": "https://openalex.org/A2139156831",
      "name": "Deyi Xiong",
      "affiliations": [
        "Tianjin University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4221143735",
    "https://openalex.org/W2989588035",
    "https://openalex.org/W2891732163",
    "https://openalex.org/W2137024897",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W3168838873",
    "https://openalex.org/W4226025568",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W2981876780",
    "https://openalex.org/W6635469476",
    "https://openalex.org/W3021524072",
    "https://openalex.org/W2016089260",
    "https://openalex.org/W3207553988",
    "https://openalex.org/W2972851234",
    "https://openalex.org/W6642011314",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W4308244210",
    "https://openalex.org/W6796821673",
    "https://openalex.org/W3092860793",
    "https://openalex.org/W6755519508",
    "https://openalex.org/W2936262349",
    "https://openalex.org/W2898695519",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3023160663",
    "https://openalex.org/W3134155563",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3207166518",
    "https://openalex.org/W3186326110",
    "https://openalex.org/W2991223644",
    "https://openalex.org/W4221143296",
    "https://openalex.org/W3006892896",
    "https://openalex.org/W3102999298",
    "https://openalex.org/W2998374885",
    "https://openalex.org/W2970062726",
    "https://openalex.org/W3036362489",
    "https://openalex.org/W2964006684",
    "https://openalex.org/W4285300946",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W3185743606",
    "https://openalex.org/W3098199128",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W3101850416",
    "https://openalex.org/W4283815582",
    "https://openalex.org/W3174464510",
    "https://openalex.org/W4361806395",
    "https://openalex.org/W4285292976",
    "https://openalex.org/W2971236147",
    "https://openalex.org/W3176495666",
    "https://openalex.org/W4286903249",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4294214983",
    "https://openalex.org/W4388184238",
    "https://openalex.org/W4366327277",
    "https://openalex.org/W2963101081",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2996848635"
  ],
  "abstract": "As an indispensable ingredient of intelligence, commonsense reasoning is crucial for large language models (LLMs) in real-world scenarios. In this paper, we propose CORECODE, a dataset that contains abundant commonsense knowledge manually annotated on dyadic dialogues, to evaluate the commonsense reasoning and commonsense conflict detection capabilities of Chinese LLMs. We categorize commonsense knowledge in everyday conversations into three dimensions: entity, event, and social interaction. For easy and consistent annotation, we standardize the form of commonsense knowledge annotation in open-domain dialogues as \"domain: slot = value\". A total of 9 domains and 37 slots are defined to capture diverse commonsense knowledge. With these pre-defined domains and slots, we collect 76,787 commonsense knowledge annotations from 19,700 dialogues through crowdsourcing. To evaluate and enhance the commonsense reasoning capability for LLMs on the curated dataset, we establish a series of dialogue-level reasoning and detection tasks, including commonsense knowledge filling, commonsense knowledge generation, commonsense conflict phrase detection, domain identification, slot identification, and event causal inference. A wide variety of existing open-source Chinese LLMs are evaluated with these tasks on our dataset. Experimental results demonstrate that these models are not competent to predict CORECODE's plentiful reasoning content, and even ChatGPT could only achieve 0.275 and 0.084 accuracy on the domain identification and slot identification tasks under the zero-shot setting. We release the data and codes of CORECODE at https://github.com/danshi777/CORECODE to promote commonsense reasoning evaluation and study of LLMs in the context of daily conversations.",
  "full_text": "CORECODE: A Common Sense Annotated Dialogue Dataset with Benchmark\nTasks for Chinese Large Language Models\nDan Shi1, Chaobin You1, Jiantao Huang2, Taihao Li2, Deyi Xiong1*\n1College of Intelligence and Computing, Tianjin University, Tianjin, China\n2Zhejiang Lab, Hangzhou, China\n{shidan, chaobinyou, dyxiong}@tju.edu.cn, {jthuang, lith}@zhejianglab.com\nAbstract\nAs an indispensable ingredient of intelligence, common-\nsense reasoning is crucial for large language models (LLMs)\nin real-world scenarios. In this paper, we propose CORE-\nCODE, a dataset that contains abundant commonsense\nknowledge manually annotated on dyadic dialogues, to eval-\nuate the commonsense reasoning and commonsense conflict\ndetection capabilities of Chinese LLMs. We categorize com-\nmonsense knowledge in everyday conversations into three\ndimensions: entity, event, and social interaction. For easy\nand consistent annotation, we standardize the form of com-\nmonsense knowledge annotation in open-domain dialogues\nas “domain: slot = value”. A total of 9 domains and 37\nslots are defined to capture diverse commonsense knowl-\nedge. With these pre-defined domains and slots, we collect\n76,787 commonsense knowledge annotations from 19,700\ndialogues through crowdsourcing. To evaluate and enhance\nthe commonsense reasoning capability for LLMs on the cu-\nrated dataset, we establish a series of dialogue-level reason-\ning and detection tasks, including commonsense knowledge\nfilling, commonsense knowledge generation, commonsense\nconflict phrase detection, domain identification, slot identi-\nfication, and event causal inference. A wide variety of ex-\nisting open-source Chinese LLMs are evaluated with these\ntasks on our dataset. Experimental results demonstrate that\nthese models are not competent to predict CORECODE’s\nplentiful reasoning content, and even ChatGPT could only\nachieve 0.275 and 0.084 accuracy on the domain identifi-\ncation and slot identification tasks under the zero-shot set-\nting. We release the data and codes of CORECODE at\nhttps://github.com/danshi777/CORECODE to promote com-\nmonsense reasoning evaluation and study of LLMs in the\ncontext of daily conversations.\nIntroduction\nCommonsense reasoning is a crucial component of intelli-\ngence(Liu and Singh 2004; Cambria et al. 2011), which in-\nvolves the ability to make logical deductions, infer implicit\ninformation and apply background knowledge to solve prob-\nlems as well as understand the world. In recent years, explor-\ning and improving the ability of NLP models for the acqui-\nsition and application of commonsense knowledge has been\n* Corresponding author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nattracting growing interest, leading to extensive research in\nthis field (Lv et al. 2020; Wang et al. 2020; Liu et al. 2022).\nIt is widely acknowledged that LLMs, trained on a huge\namount of data, are able to obtain broad knowledge covering\na wide range of domains (Rae et al. 2021; Hoffmann et al.\n2022; Touvron et al. 2023; Du et al. 2022a; Guo et al. 2023),\nincluding commonsense knowledge (West et al. 2022; Bian\net al. 2023; Bang et al. 2023). However, commonsense rea-\nsoning is still regarded as a major challenge for LLMs (Zhou\net al. 2020; Bhargava and Ng 2022). Studies disclose that\nLLMs fall short in performing adequate commonsense rea-\nsoning (Wei et al. 2022). For example, ChatGPT 1 does not\nprecisely know what the needed commonsense knowledge\nfor answering a specific question is (e.g., questions in social\nand temporal domains) (Bian et al. 2023).\nTo mitigate this issue, we propose CORECODE\n(Commonsense Reasoning and Conflict Detection in di-\nalogues), a dataset that contains abundant commonsense\nknowledge manually annotated on Chinese dyadic dia-\nlogues, to assess how much commonsense knowledge the\nLLMs have gained and how well they can be improved in\ncommonsense reasoning and conflict detection with the an-\nnotated knowledge in CORECODE.\nSpecifically, we focus on annotating fine-grained com-\nmonsense knowledge in multi-turn dyadic dialogues. The\nknowledge annotated in a dialogue is context-sensitive and\ngrounded exclusively in that particular dialogue. Inspired by\nthe annotation convention used in task-oriented dialogue,\nin which dialogue states are denoted in the form of “do-\nmain: slot = value”, e.g. “hotel: price range = moderate”\n(Budzianowski et al. 2018; Zhu et al. 2020; Quan et al.\n2020), we standardize the representation of commonsense\nknowledge in open-domain dialogues also in the form of\n“domain: slot = value”. We categorize commonsense knowl-\nedge into three dimensions, namely entity, event, and so-\ncial interaction, and then construct an ontology over these\ndimensions, which defines all possible domains for each\ndimension and all possible slots for each domain. Thanks\nto the guidance of this ontology, crowdsourcing annotators\nare able to conveniently annotate fine-grained commonsense\nknowledge in a consistent way.\nOver the curated dataset, we develop six benchmark tasks:\n1https://openai.com/blog/chatgpt\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18952\ncommonsense knowledge filling, commonsense knowledge\ngeneration, commonsense conflict phrase detection, domain\nidentification, slot identification and event causal inference.\nThese tasks, organized in different forms (e.g., multiple-\nchoice, span extraction, and text generation), facilitate the\nevaluation and enhancement of commonsense reasoning in\nLLMs.\nWe conduct numerous experiments on CORECODE, at-\ntempting to explore two main research questions: (1) Can\nLLMs master and apply commonsense knowledge well\nenough to achieve good performance on these tasks? (2)\nHow much further improvements can be obtained by LLMs\nif they are fine-tuned on CORECODE? Extensive experi-\nments demonstrate that our benchmark tasks are challenging\nfor existing Chinese LLMs, as all evaluated LLMs perform\npoorly on most tasks. We also show that although the perfor-\nmance of LLMs improves after being fine-tuned on CORE-\nCODE, they fail to obtain robust commonsense reasoning\nability. When perturbations are introduced, the fine-tuning\nperformance has significantly dropped.\nRelated Work\nA variety of datasets and benchmarks focusing on different\naspects of commonsense knowledge over textual inputs have\nbeen proposed, including science common sense datasets\nARC (Clark et al. 2018) and QASC (Khot et al. 2020),\ntemporal common sense dataset MC-TACO (Zhou et al.\n2019), numerical common sense dataset NumerSense (Lin\net al. 2020), physical common sense dataset PIQA (Bisk\net al. 2020), social common sense dataset Social IQA (Sap\net al. 2019b) and general common sense datasets Common-\nsenseQA (Talmor et al. 2019), OpenBookQA (Mihaylov\net al. 2018), and WSC (Levesque, Davis, and Morgenstern\n2012). These datasets only examine the model’s knowledge\nand ability in a certain commonsense aspect in the form of\nmultiple-choice questions.\nMeanwhile, there have also been many studies devoted\nto annotating commonsense knowledge within dialogues.\nATOMIC (Sap et al. 2019a; Hwang et al. 2021) is one such\ndataset that consists of a large set of inference types. How-\never, ATOMIC is context-insensitive, as its commonsense\nreasoning operates on phrases taken out of context, disre-\ngarding whether an event is performed by the same indi-\nvidual. TIMEDIAL (Qin et al. 2021) focuses on the time\nreasoning ability of language models in dialogues, while\nCICERO (Ghosal et al. 2022) provides cause, subsequent\nevents, prerequisites, motivations, and emotional reactions\nfor utterances in dialogues, focusing on these five event-\nrelated reasoning types. Both datasets cover only a spe-\ncific aspect of commonsense knowledge. CIDER (Ghosal\net al. 2021) extracts knowledge in dialogues into knowledge\ntriplets, which covers fewer commonsense knowledge types\nthan us. For example, subsequent event, subsequent emo-\ntional reaction, frequency are beyond the scope of CIDER.\nTo the best of our knowledge, CORECODE is the first\nlarge-scale Chinese dialogue-oriented commonsense knowl-\nedge annotation dataset involving comprehensive common-\nsense knowledge in three dimensions: entity, event, and so-\ncial interaction, covering a large number of perspectives\nsuch as attributes, time, space, and causality. Yet another fea-\nture that must be emphasized is that within CORECODE,\nwe manually provide phrases corresponding to the phrases\nin an original dialogue, which are against common sense in\nthat context. This aims to probe the model’s capacity to de-\ntect and locate such phrases that are inconsistent with the\ncontext in terms of commonsense reasoning.\nDataset Creation\nThe raw data of CORECODE is derived from NaturalConv\n(Wang et al. 2021) and DuLeMon (Xu et al. 2022) datasets,\nboth of which contain multi-turn dialogues between two\npeople. Dialogues in NaturalConv involve a variety of top-\nics (including but not limited to sports, entertainment, and\ntechnology). We first take an automatic screening method to\nidentify dialogues that are rich in commonsense knowledge,\nfollowing Zhou et al. (2021).\nSpecifically, we first identify candidate concepts (nouns,\nverbs, adjectives) in each dialogue turn using part-of-speech\ntagging. We then query the ConceptNet using the identified\nconcepts in each utterance to obtain a list of one-hop com-\nmonsense triples in the form of(e1, r, e2). Next, we examine\nif the entity e2 from the triple appears in the concept set of\nthe succeeding utterance. If there is a match, it indicates a\npotential commonsense link between the two utterances.\nUnlike Zhou et al. (2021) who retain dialogues with only\none commonsense triple match, we employ a stricter crite-\nrion by retaining dialogues where more than three common-\nsense triple matches are detected. This ensures that the kept\ndialogues possess a substantial amount of commonsense rea-\nsoning. The statistics of the screening results on Natural-\nConv and DuLeMon are shown in our arXiv version2.\nMoreover, to differentiate between the two sides of the\nconversation, we employ the notation “A: ” or “B: ” preced-\ning each utterance to denote the respective speaker.\nData Annotation\nOver the selected dialogues, we perform commonsense\nknowledge annotation. To guarantee the consistency of an-\nnotations across multiple crowd-sourced workers, we adopt\na standardized annotation procedure.\nWe categorize commonsense knowledge in everyday con-\nversations into three dimensions: entity, event, and social\ninteraction. Crowd-sourced workers first need to identify\nspecific instances under these three dimensions from dia-\nlogues. Then, with the assistance of linguists, we divide each\nof these three dimensions into multiple domains to which\ntheir commonsense knowledge belongs, and define differ-\nent slots for each domain, forming a two-level hierarchi-\ncal taxonomy. Such design is guided by three fundamen-\ntal principles: coverage, exclusivity, and easiness. The cov-\nerage rule ensures that the commonsense knowledge sys-\ntem encompasses nearly all conceivable types of common-\nsense knowledge in dialogues. Exclusivity mandates that\neach commonsense knowledge type remains distinct, devoid\nof any overlap with other types. Lastly, the easiness prin-\nciple indicates that the commonsense knowledge system is\n2https://arxiv.org/abs/2312.12853\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18953\nstraightforward for annotators to employ. With this conven-\ntion, crowd-sourced workers are instructed to annotate the\nidentified instances with commonsense knowledge in the\nform of “domain: slot = value”. In addition to such annota-\ntions, they are also required to provide phrases that, in terms\nof common sense, conflict with the original textual context.\nBelow, we describe each step in detail.\nEntity, Event, Social Interaction Recognition. The first\nstep of the annotation process is to identify specific instances\nof entity, event, and social interaction that exist in dialogues,\naccording to the following definitions.\n• Entities refer to objectively existing and distinguishable\nphysical objects in the real world, either representing\na general category of people or things, such as “cats”,\n“movies”, or referring to specific individuals or objects,\nsuch as “Yao Ming”, “Wolf Warrior”, etc.\n• Events are typically text spans in the form of “subject\n+ predicate” or “subject + predicate + object”. They are\nfine-grained semantic units that describe the state of en-\ntities and their actions (Zhou et al. 2022). For example,\n“He looks very excited ” describes the state of the sub-\nject, and “He broke his toy” illustrates an action where\nthe subject interacts with the object.\n• Social interactions refer to the set of rules and guide-\nlines that constrain people’s behavior when interacting\nwith others. They encompass a collection of social norms\nand customs that people are expected to adhere to (Bian\net al. 2023). For instance, “It is customary to knock on\nthe door before entering someone else’s room”.\nAnnotation of Involved Commonsense Knowledge. Un-\nder each of the three dimensions, we define domains and\nslots. For entities, we divide the relevant commonsense\nknowledge into three corresponding domains: attribute,\ncomparison, and space. These domains capture specific\nproperties of the object itself, relationships between the ob-\nject and other objects, and relationships between the ob-\nject and the spatial environment in which it is located, re-\nspectively. Under each domain, there are further divisions\ninto different slots. For example, under the attribute domain,\nthere are slots “Is”, “Is A”, “Has”, “Is Made Of”, and so\non. For events, relevant commonsense knowledge includes\nthe prerequisite, cause, and consequence of an event, as well\nas the temporal and spatial factors associated with the event.\nFor social interactions, we focus on the social norms that hu-\nmans follow. Instead of subdividing into multiple domains,\nwe divide seven slots under the social norms domain. There\nare 9 domains and 37 slots included in the three dimensions\nin total. The full inventory of all domains and slots can be\nfound in our arXiv version.\nThe second step of the annotation process is to label each\nentity, event or social interaction instance with its common-\nsense knowledge in the form of “domain: slot = value”. The\nannotated “value” need not necessarily be an exact span ex-\ntracted from the original dialogue, but can be a grammat-\nically correct and semantically fluent clause summarized\nfrom the dialogue, ensuring that the event and its “domain:\nslot = value” in isolation is informationally complete and\nlogically consistent. It has been emphasized to annotators\nthat for the “event cause” slot in the “cause” domain and the\n“subsequent event” slot in the “consequence” domain, an-\nnotations should take the form of an event, i.e., either in the\nstructure of “subject + predicate” or “subject + predicate +\nobject”. In addition, the annotators need to indicate which\nphrases or clauses in the original dialogue led to the identi-\nfication of this commonsense knowledge, so as to provide a\nbasis for the next step.\nRewriting of Commonsense Conflict Phrases. Finally,\nfor each set of phrases from the original dialogue indicated\nin the previous step, annotators are required to choose one\nphrase and provide it with the following two commonsense\nconflict phrases:\n(1) Commonsense Conflict Phrase 1: This phrase should\nbe obtained by conforming to the minimal modification prin-\nciple, i.e., modifying only one or two words in the original\nphrase. There should be a commonsense conflict or error af-\nter using this phrase to replace the original phrase in the di-\nalogue.\n(2) Commonsense Conflict Phrase 2: This phrase should\nbe created by modifying as many words as possible in the\noriginal phrase in compliance with the maximum modifica-\ntion principle. When constructing this phrase, annotators can\ninclude words that appear in the dialogue to maintain con-\nsistency with the dialogue’s context. However, it is crucial to\nensure as much as possible that the meaning of this phrase\ndiffers from the Commonsense Conflict Phrase 1.\nThe purpose of this annotation step is to explore whether\nLLMs are able to detect the location of phrases that con-\nflict with the dialogue context in terms of common sense.\nTherefore, annotators must ensure that after replacing the\noriginal phrase in the dialogue with the annotated conflict\nphrase, there should be only a commonsense error while the\ndialogue maintains grammatically correct and fluent.\nTo comprehensively evaluate the commonsense reasoning\nability of LLMs, we propose two distinct annotated subsets\nwith varying difficulty levels. During the annotation proce-\ndure on 9.7K dialogues, we represent the subject and object\nof events using the speaker indicators “A” or “B” from the\ndialogue and group these annotated instances as an EASY\nset. A HARD set is annotated on another 10K dialogues,\nwhere “x” is uniformly employed to denote the subject of\nall events, while “y” is used to represent the predicate of all\nevents, regardless of the dialogue participant to whom the\nevent pertains. Significant challenges in reasoning through\nevents are provided in the HARD set, as LLMs are required\nto first deduce and locate the event initiator before reason-\ning.\nAnnotation Quality Control\nIn order to standardize the annotation form and control\nthe quality of common sense annotations, we design and\ndevelop a knowledge acquisition platform where crowd-\nsourced workers need to properly click on the appropriate\nbuttons and fill in the corresponding values given the dia-\nlogue history.\nWe adopt a very strict quality control protocol to ensure\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18954\nthe quality of annotations. First, we train two reviewers with\n200 dialogues. The annotation consistency of the two re-\nviewers is high, with an average Cohen’s Kappa (McHugh\n2012) of 80.7% across the annotation tasks. We only hire\nannotators who have relevant experience in text annotation,\ne.g., those who have participated in annotation tasks such as\nChinese multi-turn dialogue writing and correction, entity\nextraction or syntactic structure annotation in Chinese texts.\nSecond, 200 candidate workers participate in a pre-\nannotation stage. They adhere to the prescribed rules to an-\nnotate dialogues. The two reviewers will review annotations\nof these participants to distinguish whether the annotations\nmeet the requirements. The process has an elimination rate\nof roughly 80%, with 43 labelers passing this stage.\nThird, we proceed to the training phase. We divided the\nparticipants into groups of 5 people each. We train 1-2 qual-\nity inspectors within each group, who in turn are responsible\nfor the instruction of the annotators. During this progression,\nquality inspectors evaluate the rule comprehension and error\ncorrection capabilities of the annotators. Those who do not\nmeet the criteria are subjected to further training or elimi-\nnated from the process.\nAt last, 6 quality inspectors with an average Cohen’s\nKappa of 59.4%, as well as 15 annotators, proceed to the\nformal annotation stage. We take iterative verification and\nrevision during this stage. Any data deemed unsatisfactory\nwill be returned for revision until they are qualified.\nOverall Statistics\nThe overall statistics of the annotated dataset are shown in\nTable 1. After annotating on 19.7K dialogues, we obtained\n76,787 annotations, each comprising the original dialogue,\nan entity/event/social interaction instance, a commonsense\nknowledge represented by a domain-slot-value triplet, the\ninvolved phrase from the original dialogue, and two com-\nmonsense knowledge conflict phrases. The average number\nof turns and tokens per dialogue is 19.40 and 501.58, in-\ndicating that the annotated dialogues are lengthy and infor-\nmative. The social interaction dimension’s knowledge pri-\nmarily serves to constrain behavior but is seldom mentioned\nin dialogues, resulting in limited annotated commonsense\nknowledge for this dimension. The annotations for entity,\nevent, and social interaction dimensions constitute 58.42%,\n41.54%, and 0.03% of the overall annotations, respectively.\nBenchmark Tasks\nWe use our dataset as a testbed and define 6 tasks in different\nforms, attempting to evaluate dialogue-level commonsense\nreasoning capabilities of Chinese LLMs. For each task, we\nprovide both its definition and associated prompt that is con-\nstructed to allow LLMs to complete the task in the continu-\nation to the prompt.\nCommonsense Knowledge Filling\nTask definition. This task is to fill desirable commonsense\nknowledge into a masked dialogue where a commonsense\nphrase is replaced with [MASK]. In order to automatically\nassess the performance of the task, we formulate the task in\nthe form of multiple-choice questions.\nHARD EASY\nTotal\n# dialogues 10,000\n9,700 19,700\nMax. turns per dialogue 26 26 26\nMin. turns per dialogue 14 16 15\nAvg. turns per dialogue 18.69 20.10 19.40\nMax. # tokens per dialogue 1,002 953 977.5\nMin. # tokens per dialogue 194 231 212.5\nAvg. # tokens per dialogue 464.18 538.98 501.58\nAvg. # tokens per turn 24.83 26.81 25.82\n# annotated\ninstances 37,777 39,010 76,787\n# annotated entities 21,320 23,541 44,861\n# annotated events 16,439 15,461 31,900\n# annotated social interactions 18 8 26\n# domain-slot-v\nalue triplets 37,777 39,010 76,787\n# commonsense conflict phrases 75,554 78,020 153,574\nTable 1: Overall statistics of the CORECODE dataset.\nPrompt. The input prompt to LLMs for this task con-\nsists of the question, masked dialogue, answer choices, and\nsuffix: question \\n masked dialogue \\n (a) phrase1 (b)\nphrase2 (c) phrase3 \\n “answer: the correct option is”. The\nthree phrases are the corresponding masked commonsense\nphrase and two manually composed commonsense conflict\nphrases. See our arXiv version for examples of all tasks.\nCommonsense Knowledge Generation\nTask definition. We frame this task as a generative task\nthat takes the annotated commonsense knowledge values as\nground truth and asks LLMs to generate the values accord-\ning to the dialogue context.\nPrompt. The input prompt is formatted as: dialogue \\n\nquestion \\n “answer:”, where the question is formed by the\nentity/event/social interaction and its annotated slot through\na predefined template and some explanatory text.\nCommonsense Conflict Phrase Detection\nTask definition. We define this task as a span extraction\ntask. We replace the corresponding phrases in the original\ndialogue with the annotated commonsense conflict phrases,\nand ask LLMs to extract the commonsense conflict phrases.\nPrompt. The prompt format is: dialogue with replaced\ncommonsense conflict phrase \\n question \\n “answer:”.\nDomain Identification\nTask definition. This task is also defined as a multiple-\nchoice-question task. Take the entity dimension as an exam-\nple, LLMs are required to select the domain to which the\nrelationship between an entity and its annotated value be-\nlongs, based on the given dialogue context. Since the social\ninteraction dimension includes a single domain, this task is\nperformed on the entity and event dimensions.\nPrompt. The prompt is formatted like: question \\n entity\nor event \\n annotated value \\n dialogue \\n (a) domain1 (b)\ndomain2 ··· (x) domainn \\n “answer: the correct domain\nis”.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18955\nSlot Identification\nTask definition. This task is similar to the Domain Iden-\ntification task but involves selecting from more fine-grained\nslot options and is performed across all three dimensions.\nPrompt. The input prompt is in the form of: question \\n\nentity/event/social interaction \\n annotated value \\n dia-\nlogue \\n (a) slot1 (b) slot2 ··· (x) slotn \\n “answer: the\ncorrect option is”.\nEvent Causal Inference\nCausal inference is one of the crucial reasoning abilities of\nhuman intelligence, which involves establishing the correct\ncause-and-consequence relationships between events. These\nrelationships are captured in the “cause: event cause” slot\nand the “consequence: subsequent event” slot of our taxon-\nomy. We specially design three generative event causal in-\nference tasks that utilize the annotated knowledge involved\nin these two slots.\n• Subtask 1: Event Cause Inference. Given the dialogue\nand event, LLMs are required to generate the cause of the\nevent.\n• Subtask 2: Subsequent Event Inference. Given the di-\nalogue and event, the consequence of the event is gener-\nated by LLMs.\n• Subtask 3: Clipped Subsequent Event Inference.\nGiven the event and the truncated dialogue where the\ncontext succeeding the event is discarded, we require\nLLMs to generate the consequence of the event.\nExperiments\nEvaluated LLMs\nWe evaluated a diverse list of Chinese LLMs that cover\na variety of training processes and scales 3: (1) LLMs\nonly being pre-trained on large-scale training corpora, in-\ncluding GLM-10B (Du et al. 2022b) and BLOOM-7.1B\n(Scao et al. 2022), (2) LLMs being both pre-trained and\ninstruction-tuned, including ChatGLM-6B 4, ChatGLM2-\n6B5, MOSS-SFT-16B 6, Baichuan-7B 7, BLOOMZ-1.7B,\nBLOOMZ-7.1B, BLOOMZ-7.1B-MT (Muennighoff et al.\n2022), and BELLE-7B, which is the SFT version based on\nBLOOMZ-7.1B-MT. We used two variants of BELLE fined-\ntuned on 200K and 2M instructions separately, i.e., BELLE-\n7B-0.2M8 and BELLE-7B-2M 9. We also evaluated two\nvariants Chinese-Alpaca-Plus-7B and Chinese-Alpaca-Plus-\n13B of Chinese-Alpaca-Plus (Cui, Yang, and Yao 2023). We\nexperimented on the recommended hyperparameter settings\n3All the experiments in the main paper were conducted on the\nHARD set. Experimental results on the EASY set are available in\nour arXiv version.\n4https://github.com/THUDM/ChatGLM-6B\n5https://github.com/THUDM/ChatGLM2-6B\n6https://huggingface.co/fnlp/moss-moon-003-sft\n7https://github.com/baichuan-inc/baichuan-7B\n8https://huggingface.co/BelleGroup/BELLE-7B-0.2M\n9https://huggingface.co/BelleGroup/BELLE-7B-2M\nof all LLMs. We also evaluated ChatGPT (i.e., GPT-3.5-\nturbo) from OpenAI as a reference.\nFurthermore, to explore the impact of in-context learn-\ning (ICL) on model performance, we also carried out exper-\niments on ChatGLM-6B under the few-shot settings, includ-\ning 1-shot, 3-shot and 5-shot settings.\nEvaluation Metrics\nFor the commonsense knowledge filling, domain identifica-\ntion and slot identification tasks (we refer these three tasks\nto the selection tasks), we used the accuracy of selecting the\ncorrect answer as the evaluation metric. During inference,\nwe have found that even if we explicitly state in the prompt\nthat models should output only the answer option indicator\n(i.e. a, b, c, etc.), not all models follow this instruction. There\nis no uniformity in the form of answers generated by each\nmodel. Moreover, sometimes models output answers with\nrationales attached. In order to avoid the underestimation of\nthe model performance due to the varying output formats,\nwe adopted a series of filtering measures to find the correct\nanswer in the output as much as possible. For example, in\nthe case where the ground-truth is “(a) premise”, the gener-\nated answers “a”, “A”, “(a)”, “(A)”, “a)”, “A)”, “(a)premise”,\n“(a) premise”, “premise” are all counted as correct.\nFor the span extraction task, i.e., the commonsense con-\nflict phrase detection task, we used F1 and EM scores calcu-\nlated by comparing model outputs to ground-truth answers.\nFor the two generation tasks, namely the commonsense\nknowledge generation task and the event causal inference\ntask, we evaluated LLMs with F1 and EM scores together\nwith reference based metrics: BLEU, METEOR, ROUGE\nand CIDEr.\nPerformance of LLMs without Fine-tuning\nWe report the performance of the selection tasks in Table 3,\nand the performance of the generation and span extraction\ntasks in Table 2. We can see that CoRECODE is a very chal-\nlenging benchmark for all evaluated LLMs.\nFrom Table 3, we observe that models which are\ninstruction-tuned with SFT significantly outperform models\nbeing only pre-trained. The best-performing models across\nthe three tasks are ChatGLM2-6B, BLOOMZ-7.1B, and\nChinese-Alpaca-Plus-13B, respectively. Notably, on the slot\nidentification task, Chinese-Alpaca-Plus-13B achieves an\noutstanding and unparalleled score.\nOn the commonsense knowledge generation task,\nBLOOMZ family achieves very high scores, as shown in\nTable 2. After checking the outputs of each model, we have\nfound that models like ChatGLM and BELLE usually gen-\nerate leading sentences or explanatory reasons in their re-\nsponses, despite our prompt explicitly instructing them not\nto do so. In contrast, BLOOMZ-1.7B and BLOOMz-7.1B\ntypically generate relatively short phrases as answers, which\nis consistent with the form of our annotations. They hence\nachieve higher scores than other evaluated LLMs.\nTo exclude the effect of answer form and answer length\non the performance, we handed over the outputs of evalu-\nated LLMs to ChatGPT for scoring, the average results of\nwhich are also reported in Table 2. We described the task\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18956\nModel\nCommonsense Knowledge Generation CCPD\nF1 EM BLEU1 BLEU2 METEOR ROUGE-L CIDEr ChatGPT Score F1 EM\nGLM-10B 0.023 0.000 0.000 0.000 0.032 0.000 0.001 3.190 0.011 0.000\nBLOOM-7.1B 0.071 0.000 0.017 0.000 0.115 0.004 0.017 3.455 0.024 0.000\nChatGLM2-6B 0.160 0.004 0.001 0.000 0.145 0.001 0.002 3.940 0.029 0.001\nBELLE-7B-0.2M 0.090 0.019 0.015 0.000 0.105 0.010 0.041 3.265 0.024 0.000\nBELLE-7B-2M 0.111 0.008 0.004 0.000 0.140 0.003 0.010 3.555 0.007 0.000\nBLOOMZ-1.7B 0.388 0.234 0.234 0.000 0.164 0.234 0.585 4.060 0.004 0.000\nBLOOMZ-7.1B 0.438 0.284 0.282 0.000 0.199 0.283 0.707 3.980 0.041 0.003\nBLOOMZ-7.1B-MT 0.435 0.300 0.300 0.000 0.184 0.300 0.750 4.030 0.047 0.010\nMOSS-SFT-16B 0.199 0.071 0.066 0.000 0.147 0.049 0.174 3.780 0.038 0.001\nBaichuan-7B 0.071 0.000 0.000 0.000 0.072 0.000 0.001 3.445 0.002 0.000\nChinese-Alpaca-Plus-7B 0.129 0.015 0.014 0.000 0.099 0.015 0.039 3.375 0.021 0.000\nChinese-Alpaca-Plus-13B 0.133 0.021 0.018 0.000 0.104 0.020 0.051 3.490 0.021 0.000\nChatGLM-6B 0.147 0.000 0.000 0.000 0.166 0.000 0.000 3.745 0.044 0.001\nChatGLM-6B 1-shot 0.202 0.061 0.035 0.000 0.154 0.048 0.120 3.770 0.038 0.002\nChatGLM-6B 3-shot 0.274 0.115 0.091 0.000 0.175 0.111 0.277 3.885 0.060 0.006\nChatGLM-6B 5-shot 0.215 0.097 0.095 0.000 0.147 0.096 0.240 3.685 0.052 0.007\nChatGPT 0.296 0.071 0.044 0.000 0.258 0.045 0.111 - 0.104 0.021\nTable 2: Overall performance of evaluated LLMs on the commonsense knowledge generation and commonsense conflict phrase\ndetection task. CCPD: Commonsense Conflict Phrase Detection.\nModel CKF DI SI\nGLM-10B 0.157 0.060 0.051\nBLOOM-7.1B 0.329 0.108 0.039\nChatGLM-6B 0.788 0.246 0.113\nChatGLM2-6B 0.818 0.286 0.153\nBELLE-7B-0.2M 0.392 0.208 0.212\nBELLE-7B-2M 0.599 0.169 0.109\nBLOOMZ-1.7B 0.709 0.248 0.044\nBLOOMZ-7.1B 0.758 0.444 0.165\nBLOOMZ-7.1B-MT 0.695 0.341 0.168\nMOSS-SFT-16B 0.445 0.353 0.110\nBaichuan-7B 0.416 0.071 0.055\nChinese-Alpaca-Plus-7B 0.584 0.385 0.060\nChinese-Alpaca-Plus-13B 0.510 0.126 0.449\nChatGPT 0.896 0.275 0.084\nTable 3: Overall performance of evaluated LLMs on the\nthree selection tasks. CKF: Commonsense Knowledge Fill-\ning. DI: Domain Identification. SI: Slot Identification.\nto ChatGPT and asked it to score the answers according to\nour pre-defined scoring criteria (see in our arXiv version).\nThe average scores obtained by these models vary from 3 to\n5. According to our criteria, this suggests that the answers\ngenerated by LLMs are more likely to be “answers that fit\nthe context of the dialogue but are not a specific answer to\nthe question” or “answers that are semantically inconsistent\nwith the ground-truth answer but are also correct”.\nTable 2 also indicates improved model performance under\nthe few-shot settings. However, the performance under the\n5-shot setting is worse than that under the 3-shot setting.\nThis might be due to the long length of our dialogues (as\nshown in Table 1, the average number of tokens per dialogue\nis 501). The excessive length of model inputs under the 5-\nshot setting might lead to a decline in performance.\nFigure 1: Performance of fine-tuned LLMs on the com-\nmonsense knowledge generation and commonsense conflict\nphrase detection task. The horizontal lines show the perfor-\nmance of LLMs without fine-tuning.\nPerformance of LLMs Being Fine-tuned\nWe further evaluated LLMs after they were fine-tuned on\nCORECODE. Specifically, we fine-tuned BLOOM-7.1B\nand ChatGLM-6B on 2K, 4K, 6K, 8K, and 10K examples\nrespectively in the LoRA (Hu et al. 2022) manner, and tested\nthese fine-tuned models on another 2K data.\nResults on the commonsense knowledge generation and\ncommonsense conflict phrase detection tasks are shown in\nFigure 1. Fine-tuning on different sizes of data results in\nlarge performance gains for both models. On the common-\nsense conflict phrase detection task, the F1 score rises as\nthe size of training data increases. In contrast, on the com-\nmonsense knowledge generation task, the performance rises\nfirst and then falls as the number of training instances in-\ncreases, indicating that approximately 4K training instances\nare sufficient for this task. Training with the same amount of\ntraining data for the same epochs on both tasks brings more\nperformance gains for BLOOM-7.1B than for ChatGLM-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18957\nFigure 2: Results of fine-tuned LLMs on the three selection tasks. The horizontal lines show the performance of LLMs without\nfine-tuning.\nFigure 3: Results of fine-tuned LLMs on the perturbed test sets of the three selection tasks, by option re-indicating (Opt. Rei.),\noption shuffling (Opt. Shu.) and both.\n6B. The reason could be that it is easier for BLOOM-7.1B\nwithout SFT to acquire such knowledge than ChatGLM-6B\nwith SFT. For the three selection tasks, as shown in Figure\n2, there is a positive correlation between model performance\nand training data size on most tasks. Both models obtain a\nsubstantial improvement after fine-tuning.\nRobustness Analysis\nAlthough fine-tuning on CORECODE significantly im-\nproves LLMs in commonsense reasoning, is the common-\nsense reasoning ability that LLMs obtained through fine-\ntuning robust? To investigate this question, we conducted\nthree robustness tests on the three selection tasks: (1) option\nre-indicating, (2) option shuffling, and (3) both. For (1) op-\ntion re-indicating, we change the option indicators from a,\nb, c to 1, 2, 3 in the process of forming the prompt. For (2)\noption shuffling, we shuffle the candidate options and then\nre-form the input prompt. For (3) both, we implement both\noption re-indicating and option shuffling.\nResults in Figure 3 indicate decreased accuracy for both\nmodels. Generally, the two LLMs are especially sensitive to\noption re-indicating, demonstrating larger drops. However,\nthey are more robust to option shuffling, maintaining rela-\ntively higher accuracy. The largest performance degradation\noccurs when both perturbations are executed.\nPerturbation causes a dramatic drop to BLOOM-7.1B. In\nfine-tuning LLMs on CORECODE, we use option indica-\ntors, e.g., “b”, as labels to be learned/predicted. ChatGLM-\n6B with SFT is better capable of understanding and fol-\nlowing instructions than BLOOM-7.1B. It can align indica-\ntors to the corresponding answer options during training and\ncombine them with task instructions to master the involved\ncommonsense reasoning ability. BLOOM-7.1B, however,\nprefers to learn to answer by memorizing the corresponding\ninput-label mappings and struggles to answer correctly after\nre-indicating and shuffling options. For instance, on the slot\nidentification task, our training data has a large number of\nexamples with the label “b”. BLOOM-7.1B seems to learn\nsuch a shortcut incorrectly (i.e., mapping questions to la-\nbel “b”). After shuffling answer options (the correct answer\nindicators are now mostly not “b”), the model still outputs\nplenty of “b”, resulting in very low accuracies.\nConclusion\nIn this paper, we have presented CORECODE, a large-scale\ncommonsense knowledge annotated dialogue dataset with\nover 76K annotations, and defined 6 benchmark tasks in the\nform of selection, extraction and generation, to assess the\ncapability of LLMs in learning and applying commonsense\nknowledge. A diverse list of Chinese LLMs have been eval-\nuated, which achieve poor performance on all tasks, demon-\nstrating the difficulty and utility of the proposed dataset. We\nhave further revealed the robustness issue of LLM common-\nsense knowledge acquisition via fine-tuning. We hope this\nwork could be used to track and facilitate future advances in\ncontext-sensitive LLM commonsense reasoning.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18958\nAcknowledgments\nThe present research was supported by Zhejiang Lab (No.\n2022KH0AB01). We would like to thank the anonymous re-\nviewers for their insightful comments.\nReferences\nBang, Y .; Cahyawijaya, S.; Lee, N.; Dai, W.; Su, D.; Wilie,\nB.; Lovenia, H.; Ji, Z.; Yu, T.; Chung, W.; et al. 2023. A\nmultitask, multilingual, multimodal evaluation of chatgpt on\nreasoning, hallucination, and interactivity. arXiv preprint\narXiv:2302.04023.\nBhargava, P.; and Ng, V . 2022. Commonsense knowledge\nreasoning and generation with pre-trained language models:\nA survey. In Proceedings of the AAAI Conference on Artifi-\ncial Intelligence, volume 36, 12317–12325.\nBian, N.; Han, X.; Sun, L.; Lin, H.; Lu, Y .; and He, B.\n2023. Chatgpt is a knowledgeable but inexperienced solver:\nAn investigation of commonsense problem in large language\nmodels. arXiv preprint arXiv:2303.16421.\nBisk, Y .; Zellers, R.; Gao, J.; and Choi, Y . 2020. Piqa: Rea-\nsoning about physical commonsense in natural language.\nIn Proceedings of the AAAI conference on artificial intel-\nligence, volume 34, 7432–7439. ISBN 2374-3468. Issue:\n05.\nBudzianowski, P.; Wen, T.-H.; Tseng, B.-H.; Casanueva, I.;\nUltes, S.; Ramadan, O.; and Ga ˇsi´c, M. 2018. MultiWOZ\n- A Large-Scale Multi-Domain Wizard-of-Oz Dataset for\nTask-Oriented Dialogue Modelling. In Proceedings of the\n2018 Conference on Empirical Methods in Natural Lan-\nguage Processing, 5016–5026. Association for Computa-\ntional Linguistics. EMNLP 2018.\nCambria, E.; Song, Y .; Wang, H.; and Hussain, A. 2011.\nIsanette: A Common and Common Sense Knowledge Base\nfor Opinion Mining. 2011 IEEE 11th International Confer-\nence on Data Mining Workshops, 315–322.\nClark, P.; Cowhey, I.; Etzioni, O.; Khot, T.; Sabharwal, A.;\nSchoenick, C.; and Tafjord, O. 2018. Think you have solved\nquestion answering? try arc, the ai2 reasoning challenge.\nCui, Y .; Yang, Z.; and Yao, X. 2023. Efficient and effective\ntext encoding for chinese llama and alpaca. arXiv preprint\narXiv:2304.08177.\nDu, N.; Huang, Y .; Dai, A. M.; Tong, S.; Lepikhin, D.; Xu,\nY .; Krikun, M.; Zhou, Y .; Yu, A. W.; and Firat, O. 2022a.\nGlam: Efficient scaling of language models with mixture-of-\nexperts. In International Conference on Machine Learning,\n5547–5569. PMLR. ISBN 2640-3498.\nDu, Z.; Qian, Y .; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and\nTang, J. 2022b. GLM: General Language Model Pretraining\nwith Autoregressive Blank Infilling. In Proceedings of the\n60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 320–335. Dublin, Ire-\nland: Association for Computational Linguistics.\nGhosal, D.; Hong, P.; Shen, S.; Majumder, N.; Mihalcea, R.;\nand Poria, S. 2021. CIDER: Commonsense Inference for\nDialogue Explanation and Reasoning. In Proceedings of the\n22nd Annual Meeting of the Special Interest Group on Dis-\ncourse and Dialogue, 301–313. Singapore and Online: As-\nsociation for Computational Linguistics.\nGhosal, D.; Shen, S.; Majumder, N.; Mihalcea, R.; and Po-\nria, S. 2022. CICERO: A Dataset for Contextualized Com-\nmonsense Inference in Dialogues. InProceedings of the 60th\nAnnual Meeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), 5010–5028. Dublin, Ire-\nland: Association for Computational Linguistics.\nGuo, Z.; Jin, R.; Liu, C.; Huang, Y .; Shi, D.; Yu, L.; Liu, Y .;\nLi, J.; Xiong, B.; Xiong, D.; et al. 2023. Evaluating large\nlanguage models: A comprehensive survey. arXiv preprint\narXiv:2310.19736.\nHoffmann, J.; Borgeaud, S.; Mensch, A.; Buchatskaya, E.;\nCai, T.; Rutherford, E.; de Las Casas, D.; Hendricks, L. A.;\nWelbl, J.; and Clark, A. 2022. An empirical analysis of\ncompute-optimal large language model training. 35: 30016–\n30030.\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,\nS.; Wang, L.; and Chen, W. 2022. LoRA: Low-Rank Adapta-\ntion of Large Language Models. In The Tenth International\nConference on Learning Representations, ICLR 2022, Vir-\ntual Event, April 25-29, 2022. OpenReview.net.\nHwang, J. D.; Bhagavatula, C.; Le Bras, R.; Da, J.; Sak-\naguchi, K.; Bosselut, A.; and Choi, Y . 2021. (Comet-)\natomic 2020: on symbolic and neural commonsense knowl-\nedge graphs. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 35, 6384–6392. ISBN 2374-\n3468. Issue: 7.\nKhot, T.; Clark, P.; Guerquin, M.; Jansen, P.; and Sabhar-\nwal, A. 2020. Qasc: A dataset for question answering via\nsentence composition. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 34, 8082–8090.\nISBN 2374-3468. Issue: 05.\nLevesque, H.; Davis, E.; and Morgenstern, L. 2012. The\nwinograd schema challenge. In Thirteenth international\nconference on the principles of knowledge representation\nand reasoning.\nLin, B. Y .; Lee, S.; Khanna, R.; and Ren, X. 2020. Birds have\nfour legs?! NumerSense: Probing Numerical Commonsense\nKnowledge of Pre-Trained Language Models. In Proceed-\nings of the 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), 6862–6868. Online:\nAssociation for Computational Linguistics.\nLiu, H.; and Singh, P. 2004. ConceptNet—a practical com-\nmonsense reasoning tool-kit. BT technology journal, 22(4):\n211–226.\nLiu, J.; Liu, A.; Lu, X.; Welleck, S.; West, P.; Le Bras, R.;\nChoi, Y .; and Hajishirzi, H. 2022. Generated Knowledge\nPrompting for Commonsense Reasoning. In Proceedings\nof the 60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), 3154–3169.\nAssociation for Computational Linguistics.\nLv, S.; Guo, D.; Xu, J.; Tang, D.; Duan, N.; Gong, M.; Shou,\nL.; Jiang, D.; Cao, G.; and Hu, S. 2020. Graph-Based Rea-\nsoning over Heterogeneous External Knowledge for Com-\nmonsense Question Answering. 34(5): 8449–8456.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18959\nMcHugh, M. L. 2012. Interrater reliability: the kappa statis-\ntic. Biochemia medica, 22(3): 276–282.\nMihaylov, T.; Clark, P.; Khot, T.; and Sabharwal, A. 2018.\nCan a Suit of Armor Conduct Electricity? A New Dataset\nfor Open Book Question Answering. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natural Lan-\nguage Processing, 2381–2391. Brussels, Belgium: Associa-\ntion for Computational Linguistics.\nMuennighoff, N.; Wang, T.; Sutawika, L.; Roberts, A.; Bi-\nderman, S.; Scao, T. L.; Bari, M. S.; Shen, S.; Yong, Z. X.;\nSchoelkopf, H.; Tang, X.; Radev, D.; Aji, A. F.; Almubarak,\nK.; Albanie, S.; Alyafeai, Z.; Webson, A.; Raff, E.; and Raf-\nfel, C. 2022. Crosslingual Generalization through Multitask\nFinetuning. CoRR, abs/2211.01786.\nQin, L.; Gupta, A.; Upadhyay, S.; He, L.; Choi, Y .; and\nFaruqui, M. 2021. TIMEDIAL: Temporal Commonsense\nReasoning in Dialog. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), 7066–7076.\nOnline: Association for Computational Linguistics.\nQuan, J.; Zhang, S.; Cao, Q.; Li, Z.; and Xiong, D. 2020.\nRiSAWOZ: A Large-Scale Multi-Domain Wizard-of-Oz\nDataset with Rich Semantic Annotations for Task-Oriented\nDialogue Modeling. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Process-\ning (EMNLP), 930–940. Association for Computational Lin-\nguistics.\nRae, J. W.; Borgeaud, S.; Cai, T.; Millican, K.; Hoffmann, J.;\nSong, F.; Aslanides, J.; Henderson, S.; Ring, R.; and Young,\nS. 2021. Scaling language models: Methods, analysis & in-\nsights from training gopher.\nSap, M.; Le Bras, R.; Allaway, E.; Bhagavatula, C.; Lourie,\nN.; Rashkin, H.; Roof, B.; Smith, N. A.; and Choi, Y . 2019a.\nAtomic: An atlas of machine commonsense for if-then rea-\nsoning. In Proceedings of the AAAI conference on artificial\nintelligence, volume 33, 3027–3035. ISBN 2374-3468. Is-\nsue: 01.\nSap, M.; Rashkin, H.; Chen, D.; Le Bras, R.; and Choi, Y .\n2019b. Social IQa: Commonsense Reasoning about Social\nInteractions. In Proceedings of the 2019 Conference on Em-\npirical Methods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), 4463–4473. Hong Kong, China:\nAssociation for Computational Linguistics.\nScao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ili´c, S.; Hesslow,\nD.; Castagn ´e, R.; Luccioni, A. S.; Yvon, F.; and Gall ´e, M.\n2022. Bloom: A 176b-parameter open-access multilingual\nlanguage model. arXiv preprint arXiv:2211.05100.\nTalmor, A.; Herzig, J.; Lourie, N.; and Berant, J. 2019. Com-\nmonsenseQA: A Question Answering Challenge Targeting\nCommonsense Knowledge. InProceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4149–4158. Min-\nneapolis, Minnesota: Association for Computational Lin-\nguistics.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi`ere, B.; Goyal, N.; Hambro, E.; and\nAzhar, F. 2023. Llama: Open and efficient foundation lan-\nguage models.\nWang, P.; Peng, N.; Ilievski, F.; Szekely, P.; and Ren, X.\n2020. Connecting the Dots: A Knowledgeable Path Gen-\nerator for Commonsense Question Answering. In Findings\nof the Association for Computational Linguistics: EMNLP\n2020, 4129–4140. Association for Computational Linguis-\ntics.\nWang, X.; Li, C.; Zhao, J.; and Yu, D. 2021. Naturalconv:\nA chinese dialogue dataset towards multi-turn topic-driven\nconversation. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 35, 14006–14014.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Xia, F.; Chi,\nE.; Le, Q. V .; and Zhou, D. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. 35: 24824–\n24837.\nWest, P.; Bhagavatula, C.; Hessel, J.; Hwang, J.; Jiang, L.;\nLe Bras, R.; Lu, X.; Welleck, S.; and Choi, Y . 2022. Sym-\nbolic Knowledge Distillation: from General Language Mod-\nels to Commonsense Models. In Proceedings of the 2022\nConference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language Tech-\nnologies, 4602–4625. Seattle, United States: Association for\nComputational Linguistics.\nXu, X.; Gou, Z.; Wu, W.; Niu, Z.-Y .; Wu, H.; Wang, H.;\nand Wang, S. 2022. Long time no see! open-domain con-\nversation with long-term persona memory. arXiv preprint\narXiv:2203.05797.\nZhou, B.; Khashabi, D.; Ning, Q.; and Roth, D. 2019. “Go-\ning on a vacation” takes longer than “Going for a walk”:\nA Study of Temporal Commonsense Understanding. In\nProceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 3363–3369. Hong Kong, China: Asso-\nciation for Computational Linguistics.\nZhou, P.; Gopalakrishnan, K.; Hedayatnia, B.; Kim, S.; Pu-\njara, J.; Ren, X.; Liu, Y .; and Hakkani-T ¨ur, D. Z. 2021.\nCommonsense-Focused Dialogues for Response Genera-\ntion: An Empirical Study. In SIGDIAL Conferences.\nZhou, X.; Zhang, Y .; Cui, L.; and Huang, D. 2020. Evalu-\nating commonsense in pre-trained language models. In Pro-\nceedings of the AAAI conference on artificial intelligence ,\nvolume 34, 9733–9740.\nZhou, Y .; Shen, T.; Geng, X.; Long, G.; and Jiang, D. 2022.\nClarET: Pre-training a Correlation-Aware Context-To-Event\nTransformer for Event-Centric Generation and Classifica-\ntion. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long\nPapers), 2559–2575. Dublin, Ireland: Association for Com-\nputational Linguistics.\nZhu, Q.; Huang, K.; Zhang, Z.; Zhu, X.; and Huang, M.\n2020. CrossWOZ: A Large-Scale Chinese Cross-Domain\nTask-Oriented Dialogue Dataset. Transactions of the Asso-\nciation for Computational Linguistics, 8: 281–295.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n18960",
  "topic": "Benchmark (surveying)",
  "concepts": [
    {
      "name": "Benchmark (surveying)",
      "score": 0.7840255498886108
    },
    {
      "name": "Computer science",
      "score": 0.723501443862915
    },
    {
      "name": "Natural language processing",
      "score": 0.6214601397514343
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5390206575393677
    },
    {
      "name": "Common sense",
      "score": 0.4396505653858185
    },
    {
      "name": "Geography",
      "score": 0.051483213901519775
    },
    {
      "name": "Cartography",
      "score": 0.05039787292480469
    },
    {
      "name": "Philosophy",
      "score": 0.042343974113464355
    },
    {
      "name": "Epistemology",
      "score": 0.041933298110961914
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I162868743",
      "name": "Tianjin University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210123185",
      "name": "Zhejiang Lab",
      "country": "CN"
    }
  ]
}