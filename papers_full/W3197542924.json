{
    "title": "CDLM: Cross-Document Language Modeling",
    "url": "https://openalex.org/W3197542924",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A4202202809",
            "name": "Caciularu, Avi",
            "affiliations": [
                "Bar-Ilan University"
            ]
        },
        {
            "id": "https://openalex.org/A4200857849",
            "name": "Cohan, Arman",
            "affiliations": [
                "Allen Institute for Artificial Intelligence",
                "University of Washington"
            ]
        },
        {
            "id": "https://openalex.org/A4200857852",
            "name": "Beltagy, Iz",
            "affiliations": [
                "Allen Institute for Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A4222134924",
            "name": "Peters, Matthew E.",
            "affiliations": [
                "Allen Institute for Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A4282352164",
            "name": "Cattan, Arie",
            "affiliations": [
                "Bar-Ilan University"
            ]
        },
        {
            "id": "https://openalex.org/A4227139904",
            "name": "Dagan, Ido",
            "affiliations": [
                "Bar-Ilan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3203147340",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2970726176",
        "https://openalex.org/W1974336599",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2962691502",
        "https://openalex.org/W2911997761",
        "https://openalex.org/W3174821968",
        "https://openalex.org/W2995638926",
        "https://openalex.org/W2741515891",
        "https://openalex.org/W3019932981",
        "https://openalex.org/W2798935874",
        "https://openalex.org/W2889787757",
        "https://openalex.org/W2963204221",
        "https://openalex.org/W3214661170",
        "https://openalex.org/W3118485687",
        "https://openalex.org/W3034715004",
        "https://openalex.org/W3153553531",
        "https://openalex.org/W2787905871",
        "https://openalex.org/W3033529678",
        "https://openalex.org/W2178628967",
        "https://openalex.org/W3098824823",
        "https://openalex.org/W3093830302",
        "https://openalex.org/W3003186568",
        "https://openalex.org/W2507397568",
        "https://openalex.org/W3015468748",
        "https://openalex.org/W2925618549",
        "https://openalex.org/W3121592593",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W2251552857",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2098345921",
        "https://openalex.org/W3015453090",
        "https://openalex.org/W3105721709",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W3099700870",
        "https://openalex.org/W2470673105",
        "https://openalex.org/W2970550868",
        "https://openalex.org/W3045733172",
        "https://openalex.org/W2964222246",
        "https://openalex.org/W2869686875",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W3117131443",
        "https://openalex.org/W3034671305"
    ],
    "abstract": "We introduce a new pretraining approach geared for multi-document language modeling, incorporating two key ideas into the masked language modeling self-supervised objective. First, instead of considering documents in isolation, we pretrain over sets of multiple related documents, encouraging the model to learn cross-document relationships. Second, we improve over recent long-range transformers by introducing dynamic global attention that has access to the entire input to predict masked tokens. We release CDLM (Cross-Document Language Model), a new general language model for multi-document setting that can be easily applied to downstream tasks. Our extensive analysis shows that both ideas are essential for the success of CDLM, and work in synergy to set new state-of-the-art results for several multi-text tasks. Code and models are available at https://github.com/aviclu/CDLM.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2648–2662\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n2648\nCDLM: Cross-Document Language Modeling\nAvi Caciularu1∗ Arman Cohan2,3 Iz Beltagy2\nMatthew E. Peters2 Arie Cattan1 Ido Dagan1\n1Computer Science Department, Bar-Ilan University, Ramat-Gan, Israel\n2Allen Institute for Artiﬁcial Intelligence, Seattle, W A\n3Paul G. Allen School of Computer Science & Engineering, University of Washington\navi.c33@gmail.com, {armanc,beltagy,matthewp}@allenai.org\narie.cattan@gmail.com, dagan@cs.biu.ac.il\nAbstract\nWe introduce a new pretraining approach\ngeared for multi-document language modeling,\nincorporating two key ideas into the masked\nlanguage modeling self-supervised objective.\nFirst, instead of considering documents in iso-\nlation, we pretrain over sets of multiple related\ndocuments, encouraging the model to learn\ncross-document relationships. Second, we im-\nprove over recent long-range transformers by\nintroducing dynamic global attention that has\naccess to the entire input to predict masked\ntokens. We release CDLM (Cross-Document\nLanguage Model), a new general language\nmodel for multi-document setting that can be\neasily applied to downstream tasks. Our ex-\ntensive analysis shows that both ideas are es-\nsential for the success of CDLM, and work in\nsynergy to set new state-of-the-art results for\nseveral multi-text tasks.1\n1 Introduction\nThe majority of NLP research addresses a single\ntext, typically at the sentence or document level.\nYet, there are important applications which are con-\ncerned with aggregated information spread across\nmultiple texts, such as cross-document coreference\nresolution (Cybulska and V ossen, 2014), classify-\ning relations between document pairs (Zhou et al.,\n2020) and multi-hop question answering (Yang\net al., 2018).\nExisting language models (LMs) (Devlin et al.,\n2019a; Liu et al., 2019; Raffel et al., 2020), which\nare pretrained with variants of the masked language\nmodeling (MLM) self-supervised objective, are\nknown to provide powerful representations for in-\nternal text structure (Clark et al., 2019; Rogers\net al., 2020a), which were shown to be beneﬁcial\n∗ Work partly done as an intern at AI2.\n1Code and models are available at https://github.\ncom/aviclu/CDLM\nFigure 1: An example from Multi-News (Fabbri et al.,\n2019). Circled words represent matching events and\nthe same color represents mention alignments.\nalso for various multi-document tasks (Yang et al.,\n2020; Zhou et al., 2020).\nIn this paper, we point out that beyond model-\ning internal text structure, multi-document tasks\nrequire also modeling cross-text relationships, par-\nticularly aligning or linking matching information\nelements across documents. For example, in Fig. 1,\none would expect a competent model to correctly\ncapture that the two event mentions suing and al-\nleges, from Documents 1 and 2, should be matched.\nAccordingly, capturing such cross-text relation-\nships, in addition to representing internal text struc-\nture, can prove useful for downstream multi-text\ntasks, as we demonstrate empirically later.\nFollowing this intuition, we propose a new sim-\nple cross-document pretraining procedure, which\nis applied over sets of related documents, in which\ninformative cross-text relationships are abundant\n(e.g. like those in Fig. 1). Under this setting, the\nmodel is encouraged to learn to consider and repre-\nsent such relationships, since they provide useful\nsignals when optimizing for the language modeling\nobjective. For example, we may expect that it will\nbe easier for a model to unmask the word alleges\nin Document 2 if it would manage to effectively\n“peek” at Document 2, by matching the masked\nposition and its context with the corresponding in-\nformation in the other document.\nNaturally, considering cross-document context\nin pretraining, as well as in ﬁnetuning, requires\n2649\na model that can process a fairly large amount of\ntext. To that end, we leverage recent advances in\ndeveloping efﬁcient long-range transformers (Belt-\nagy et al., 2020; Zaheer et al., 2020), which utilize\na global attention mode to build representations\nbased on the entire input. Overcoming certain re-\nstrictions in prior utilization of global attention (see\nSection 2.1), we introduce a dynamic attention pat-\ntern during pretraining, over all masked tokens, and\nlater utilize it selectively in ﬁnetuning.\nCombining pretraining over related documents\nalong with our global attention pattern yields a\nnovel pretraining approach, that is geared to learn\nand implicitly encode informative cross-document\nrelationships. As our experiments demonstrate,\nthe resulting model, termed Cross-Document Lan-\nguage Model (CDLM), can be generically applied\nto downstream multi-document tasks, eliminating\nthe need for task-speciﬁc architectures. We show\nempirically that our model improves consistently\nover previous approaches in several tasks, includ-\ning cross-document coreference resolution, multi-\nhop question answering, and document matching\ntasks. Moreover, we provide controlled experi-\nments to ablate the two contributions of pretraining\nover related documents as well as new dynamic\nglobal attention. Finally, we provide additional\nanalyses that shed light on the advantageous behav-\nior of our CDLM. Our contributions are summa-\nrized below:\n• A new pretraining approach for multi-\ndocument tasks utilizing: (1) sets of related\ndocuments instead of single documents; (2) a\nnew dynamic global attention pattern.\n• The resulting model advances the state-of-the-\nart for several multi-document tasks.\n2 Method\n2.1 Background: the Longformer Model\nRecently, long-range LMs (e.g., Longformer (Belt-\nagy et al., 2020), BigBird (Zaheer et al., 2020))\nhave been proposed to extend the capabilities of ear-\nlier transformers (Vaswani et al., 2017) to process\nlong sequences, using a sparse self-attention archi-\ntecture. These models showed improved perfor-\nmance on both long-document and multi-document\ntasks (Tay et al., 2021). In the case of multiple doc-\numents, instead of encoding documents separately,\nthese models allow concatenating them into a long\nsequence of tokens and encoding them jointly. We\nbase our model on Longformer, which sparsiﬁes\nthe full self-attention matrix in transformers by\nusing a combination of a localized sliding win-\ndow (called local attention), as well as a global\nattention pattern on a few speciﬁc input locations.\nSeparate weights are used for global and local at-\ntention. During pretraining, Longformer assigns\nlocal attention to all tokens in a window around\neach token and optimizes the Masked Language\nModeling (MLM) objective. Before task-speciﬁc\nﬁnetuning, the attention mode is predetermined for\neach input token, assigning global attention to a\nfew targeted tokens, such as special tokens, that are\ntargeted to encode global information. Thus, in the\nLongformer model, global attention weights are not\npretrained. Instead, they are initialized to the local\nattention values, before ﬁnetuning on each down-\nstream task. We conjecture that the global attention\nmechanism can be useful for learning meaning-\nful representations for modeling cross-document\n(CD) relationships. Accordingly, we propose aug-\nmenting the pretraining phase to exploit the global\nattention mode, rather than using it only for task-\nspeciﬁc ﬁnetuning, as described below.\n2.2 Cross-Document Language Modeling\nWe propose a new pretraining approach consisting\nof two key ideas: (1) pretraining over sets ofrelated\ndocuments that contain overlapping information\n(2) pretraining with a dynamic global attention pat-\ntern over masked tokens, for referencing the entire\ncross-text context.\nPretraining Over Related Documents Docu-\nments that describe the same topic, e.g., different\nnews articles discussing the same story, usually\ncontain overlapping information. Accordingly, var-\nious CD tasks may leverage from an LM infrastruc-\nture that encodes information regarding alignment\nand mapping across multiple texts. For example,\nfor the case of CD coreference resolution, con-\nsider the underlined predicate examples in Figure 1.\nOne would expect a model to correctly align the\nmentions denoted by suing and alleges, effectively\nrecognizing their cross-document relation.\nOur approach to cross-document language mod-\neling is based on pretraining the model on sets (clus-\nters) of documents, all describing the same topic.\nSuch document clusters are readily available in a\nvariety of existing CD benchmarks, such as multi-\ndocument summarization (e.g., Multi-News (Fab-\nbri et al., 2019)) and CD coreference resolution\n(e.g., ECB+ (Cybulska and V ossen, 2014)). Pre-\n2650\nLongformer Encoder\nDoc1 Doc2\n<CD-mask></doc-s>......HarryShearer... suing...<doc-s> </doc-s><doc-s>\nalleges\nis HarryShearer\nFigure 2: CDLM pretraining: The input consists of con-\ncatenated documents, separated by special document\nseparator tokens. The masked (unmasked) token col-\nored in yellow (blue) represents global (local) attention.\nThe goal is to predict the masked token alleges, based\non the global context, i.e, the entire set of documents.\ntraining the model over a set of related documents\nencourages the model to learn cross-text mapping\nand alignment capabilities, which can be leveraged\nfor improved unmasking, as exempliﬁed in Sec. 1.\nIndeed, we show that this strategy directs the model\nto utilize information across documents and helps\nin multiple downstream CD tasks.\nPretraining With Global Attention To support\ncontextualizing information across multiple docu-\nments, we need to use efﬁcient transformer models\nthat scale linearly with input length. Thus, we base\nour cross-document language model (CDLM) on\nthe Longformer model (Beltagy et al., 2020), how-\never, our setup is general and can be applied to\nother similar efﬁcient Transformers. As described\nin Sec. 2.1, Longformer sparsiﬁes the expensive\nattention operation for long inputs using a com-\nbination of local and global attention modes. As\ninput to the model, we simply concatenate related\ndocuments using new special document separator\ntokens, ⟨doc-s⟩and ⟨/doc-s⟩, for marking doc-\nument boundaries. We apply a similar masking\nprocedure as in BERT: For each training example,\nwe randomly choose a sample of tokens (15%) to\nbe masked;2 however, our pretraining strategy tries\nto predict each masked token while considering\nthe full document set, by assigning them global at-\ntention, utilizing the global attention weights (see\nSection 2.1). This allows the Longformer to contex-\ntualize information both across documents as well\nas over long-range dependencies within-document.\nThe non-masked tokens use local attention, by uti-\nlizing the local attention weights, as usual.\nAn illustration of the CD pretraining procedure\nis depicted in Fig. 2, where the masked token as-\nsociated with alleges (colored in yellow) globally\nattends to the whole sequence, and the rest of the\nnon-masked tokens (colored in blue) attend to their\nlocal context. With regard to the example in Fig. 1,\n2For details of masking see BERT (Devlin et al., 2019b).\nthis masking approach aims to implicitly compel\nthe model to learn to correctly predict the word al-\nleges by looking at the second document, optimally\nat the phrase suing, and thus capture the alignment\nbetween these two events and their contexts.\n2.3 CDLM Implementation\nIn this section, we provide the experimental details\nused for pretraining our CDLM model.\nCorpus data We use the preprocessed Multi-\nNews dataset (Fabbri et al., 2019) as the source\nof related documents for pretraining. This dataset\ncontains 44,972 training document clusters, origi-\nnally intended for multi-document summarization.\nThe number of source documents (that describe\nthe same topic) per cluster varies from 2 to 10, as\ndetailed in Appendix A.1. We consider each clus-\nter of at least 3 documents for our cross-document\npretraining procedure. We compiled our training\ncorpus by concatenating related documents that\nwere sampled randomly from each cluster, until\nreaching the Longformer’s input sequence length\nlimit of 4,096 tokens per sample. Note that this\npretraining dataset is relatively small compared to\nconventional datasets used for pretraining. How-\never, using it results in the powerful CDLM model.\nTraining and hyperparameters We pretrain the\nmodel according to our pretraining strategy, de-\nscribed in Section 2.2. We employ the Longformer-\nbase model (Beltagy et al., 2020) using the Hug-\ngingFace implementation (Wolf et al., 2020) and\ncontinue its pretraining, over our training data, for\nan additional 25k steps.3 The new document sepa-\nrator tokens are added to the model vocabulary and\nrandomly initialized before pretraining. We use\nthe same setting and hyperparameters as in Beltagy\net al. (2020), and as elaborated in Appendix B.\n3 Evaluations and Results\nThis section presents experiments conducted to\nevaluate our CDLM , as well as the the ablations\nand baselines we used. For the intrinsic evaluation\nwe measured the perplexity of the models. For\nextrinsic evaluations we considered event and en-\ntity cross-document coreference resolution, paper\ncitation recommendation, document plagiarism de-\ntection, and multihop question answering. We also\n3The training process for the base model takes 8 days on 8\nRTX8000 GPUs. Training large models requires roughly 3x\ncompute; therefore we do not focus on large models here and\nleave that for future work.\n2651\nconducted an attention analysis, showing that our\nCDLM indeed captured cross-document and long-\nrange relations during pretraining.4\nBaseline LMs Recall that CDLM employs mul-\ntiple related documents during pretraining, and as-\nsigns global attention to masked tokens. To system-\natically study the importance of these two compo-\nnents, we consider the following LM baselines:\n– Longformer: the underlying Longformer\nmodel, without additional pretraining.\n– Local CDLM : pretrained using the same corpus\nof CDLM with the Longformer’s attention pattern\n(local attention only). This baseline is intended to\nseparate the effect of simply continuing pretraining\nLongformer on our new pre-training data.\n– Rand CDLM : Longformer with the additional\nCDLM pretraining, while using random, unrelated\ndocuments from various clusters. This baseline\nmodel allows assessing whether pretraining using\nrelated documents is beneﬁcial.\n– Preﬁx CDLM : pretrained similarly as CDLM\nbut uses global attention for the ﬁrst tokens in the\ninput sequence, rather than the masked ones. This\nresembles the attention pattern of BIGBIRD (Za-\nheer et al., 2020), adopted for our cross-document\nsetup. We use this ablation for examining this al-\nternative global attention pattern, from prior work.\nThe data and pretraining hyperparameters used\nfor the ablations above are the same as the ones\nused for our CDLM pretraining, except for the\nunderlying Longformer, which is not further pre-\ntrained, and the Rand CDLM , that is fed with dif-\nferent document clusters (drawn from the same\ncorpus). During all the experiments, the global at-\ntention weights used by the underlying Longformer\nand by Local CDLM are initialized to the values\nof their pretrained local attention weights. All the\nmodels above further ﬁnetune their global atten-\ntion weights, depending on the downstream task.\nWhen ﬁnetuning CDLM and the above models on\ndownstream tasks involving multiple documents,\nwe truncate the longer inputs to the Longformer’s\n4,096 token limit.\n3.1 Cross-Document Perplexity\nFirst, we conduct a cross-document (CD) perplex-\nity experiment, in a task-independent manner, to as-\n4Since the underlying Longformer model is encoder-only,\nwe evaluate on tasks that can be modeled using the encoder-\nonly setting. We leave extensions to address seq2seq tasks\nlike generation to future work.\nModel Validation Test\nLongformer 3.89 3.94\nLocal CDLM 3.78 3.84\nRand CDLM 3.68 3.81\nPreﬁx CDLM 3.20 3.41\nCDLM 3.23 3.39\nTable 1: Cross-document perplexity evaluation on the\nvalidation and tests set of Multi-News. Lower is better.\nsess the contribution of the pretraining process. We\nused the Multi-News validation and test sets, each\nof them containing 5,622 document clusters, to con-\nstruct the evaluation corpora. Then we followed\nthe same protocol from the pretraining phase - 15%\nof the input tokens are randomly masked, where\nthe challenge is to predict the masked token given\nall documents in the input sequence. We matched\nthe pretraining phase of each one of the ablation\nmodels: In CDLM and Rand CDLM , we assigned\nglobal attention for the masked tokens, and for Pre-\nﬁx CDLM the global attention is assigned to the\n15% ﬁrst input tokens. Both Longformer and Local\nCDLM used local attention only. Perplexity is then\nmeasured by computing exponentiation of the loss.\nThe results are depicted in Table 1. The ad-\nvantage of CDLM over Rand CDLM , which was\npretrained equivalently over an equivalent amount\nof (unrelated) CD data, conﬁrms that CD pretrain-\ning, over related documents, indeed helps for CD\nmasked token prediction across such documents.\nPreﬁx CDLM introduces similar results since it\nwas pretrained using a global attention pattern\nand the same corpora used by CDLM . The Lo-\ncal CDLM is expected to have difﬁculty to predict\ntokens across documents since it was pretrained\nwithout using global attention. Finally, the under-\nlying Longformer model, which is reported as a\nreference point, is inferior to all the ablations since\nit was pretrained in a single document setting and\nwithout global attention or further pretraining on\nthis domain. Unlike the two local-attentive models,\nCDLM is encouraged to look at the full sequence\nwhen predicting a masked token. Therefore, as\nin the pretraining phase, it exploits related infor-\nmation in other documents, and not just the local\ncontext of the masked token, henceCDLM , as well\nas Preﬁx CDLM , result with a substantial perfor-\nmance gain.\n3.2 Cross-Document Coreference Resolution\nCross-document (CD) coreference resolution deals\nwith identifying and clustering together textual\n2652\nmentions across multiple documents that refer to\nthe same concept (see Fig. 1). The considered men-\ntions can be either entity mentions, usually noun\nphrases, or event mentions, typically verbs or nom-\ninalizations that appear in the text.\nBenchmark. We evaluated our CDLM by utiliz-\ning it over the ECB+ corpus (Cybulska and V ossen,\n2014), the most commonly used dataset for CD\ncoreference. ECB+ consists of within- and cross-\ndocument coreference annotations for entities and\nevents (statistics are given in Appendix A.2). Fol-\nlowing previous work, for comparison, we conduct\nour experiments on gold event and entity mentions.\nWe follow the standard coreference resolution\nevaluation metrics: MUC (Vilain et al., 1995),\nB3 (Bagga and Baldwin, 1998), CEAFe (Luo,\n2005), their average CoNLL F1, and the more re-\ncent LEA metric (Moosavi and Strube, 2016).\nAlgorithm. Recent approaches for CD corefer-\nence resolution train a pairwise scorer to learn the\nprobability that two mentions are co-referring. At\ninference time, an agglomerative clustering based\non the pairwise scores is applied, to form the coref-\nerence clusters. We made several modiﬁcations\nto the pairwise scorer. The current state-of-the-art\nmodels (Zeng et al., 2020; Yu et al., 2020) train the\npairwise scorer by including only the local contexts\n(containing sentences) of the candidate mentions.\nThey concatenate the two input sentences and feed\nthem into a transformer-based LM. Then, part of\nthe resulting tokens representations are aggregated\ninto a single feature vector which is passed into an\nadditional MLP-based scorer to produce the coref-\nerence probability estimate. To accommodate our\nproposed CDLM model, we modify this model-\ning by including the entire documents containing\nthe two candidate mentions, instead of just their\ncontaining sentences, and assigning the global at-\ntention mode to the mentions’ tokens and to the\n[CLS] token. The full method and hyperparame-\nters are elaborated in Appendix C.1.\nBaselines. We consider state-of-the-art baselines\nthat reported results over the ECB+ benchmark.\nThe following baselines were used for both event\nand entity coreference resolution:\n– Barhom et al. (2019) is a model trained jointly\nfor solving event and entity coreference as a single\ntask. It utilizes semantic role information between\nthe candidate mentions.\n– Cattan et al. (2020) is a model trained in an end-\nto-end manner (jointly learning mention detection\nand coreference following Lee et al. (2017)), em-\nploying the RoBERTa-large model to encode each\ndocument separately and to train a pair-wise scorer\natop.\n– Allaway et al. (2021) is a BERT-based model\ncombining sequential prediction with incremental\nclustering.\nThe following baselines were used for event\ncoreference resolution. They all integrate exter-\nnal linguistic information as additional features.\n– Meged et al. (2020) is an extension of Barhom\net al. (2019), leveraging external knowledge ac-\nquired from a paraphrase resource (Shwartz et al.,\n2017).\n– Zeng et al. (2020) is an end-to-end model, encod-\ning the concatenated two sentences containing the\ntwo mentions by the BERT-large model. Similarly\nto our algorithm, they feed a MLP-based pairwise\nscorer with the concatenation of the [CLS] repre-\nsentation and an attentive function of the candidate\nmentions representations.\n– Yu et al. (2020) is an end-to-end model similar to\nZeng et al. (2020), but uses rather RoBERTa-large\nand does not consider the [CLS] contextualized\ntoken representation for the pairwise classiﬁcation.\nResults. The results on event and entity CD\ncoreference resolution are depicted in Table 2.\nOur CDLM outperforms all methods, including\nthe recent sentence based models on event coref-\nerence. All the results are statistically signiﬁ-\ncant using bootstrap and permutation tests with\np< 0.001 (Dror et al., 2018). CDLM largely sur-\npasses state-of-the-art results on entity coreference,\neven though these models utilize external informa-\ntion and use large pretrained models, unlike our\nbase model. In Table 3, we provide the ablation\nstudy results. Using our model with sentences only,\ni.e., considering only the sentences where the can-\ndidate mentions appear (as the prior baselines did),\nexhibits lower performance, resembling the best\nperforming baselines. Some crucial information\nabout mentions can appear in a variety of locations\nin the document, and is not concentrated in one sen-\ntence. This characterizes long documents, where\npieces of information are often spread out. Overall,\nthe ablation study shows the advantage of using\nour pretraining method, over related documents\nand using a scattered global attention pattern, com-\n2653\nMUC B3 CEAFe LEA CoNLL\nR P F1 R P F1 R P F1 R P F1 F1\nEvent\nBarhom et al. (2019) 78.1 84.0 80.9 76.8 86.1 81.2 79.6 73.3 76.3 64.6 72.3 68.3 79.5\nMeged et al. (2020) 78.8 84.7 81.6 75.9 85.9 80.6 81.1 74.8 77.8 64.7 73.4 68.8 80.0\nCattan et al. (2020) 85.1 81.9 83.5 82.1 82.7 82.4 75.2 78.9 77.0 68.8 72.0 70.4 81.0\nZeng et al. (2020) 85.6 89.3 87.5 77.6 89.7 83.2 84.5 80.1 82.3 - - - 84.3\nYu et al. (2020) 88.1 85.1 86.6 86.1 84.7 85.4 79.6 83.1 81.3 - - - 84.4\nAllaway et al. (2021) 81.7 82.8 82.2 80.8 81.5 81.1 79.8 78.4 79.1 - - - 80.8\nCDLM 87.1 89.2 88.1 84.9 87.9 86.4 83.3 81.2 82.2 76.7 77.2 76.9 85.6\nEntity\nBarhom et al. (2019) 81.0 80.8 80.9 66.8 75.5 70.9 62.5 62.8 62.7 53.5 63.8 58.2 71.5\nCattan et al. (2020) 85.7 81.7 83.6 70.7 74.8 72.7 59.3 67.4 63.1 56.8 65.8 61.0 73.1\nAllaway et al. (2021) 83.9 84.7 84.3 74.5 70.5 72.4 70.0 68.1 69.2 - - - 75.3\nCDLM 88.1 91.8 89.9 82.5 81.7 82.1 81.2 72.9 76.8 76.4 73.0 74.7 82.9\nTable 2: Results on event and entity cross-document coreference resolution on ECB+ test set.\nF1 ∆\nfull document CDLM 85.6\n− sentences only CDLM 84.2 -1.4\n− Longformer 84.6 -1.0\n− Local CDLM 84.7 -0.9\n− Rand CDLM 84.1 -1.5\n− Preﬁx CDLM 85.1 -0.5\nTable 3: Ablation results (CoNLL F1) on our model on\nthe test set of ECB+ event coreference.\npared to the other examined settings. Recently, our\nCDLM-based coreference model was utilized to\ngenerate event clusters within an effective faceted-\nsummarization system for multi-document explo-\nration (Hirsch et al., 2021).\n3.3 Document matching\nWe evaluate our CDLM over document matching\ntasks, aiming to assess how well our model can cap-\nture interactions across multiple documents. We\nuse the recent multi-document classiﬁcation bench-\nmark by Zhou et al. (2020) which includes two\ntasks of citation recommendation and plagiarism\ndetection. The goal of both tasks is categorizing\nwhether a particular relationship holds between\ntwo input documents. Citation recommendation\ndeals with detecting whether one reference docu-\nment should cite the other one, while the plagia-\nrism detection task infers whether one document\nplagiarizes the other one. To compare with recent\nstate-of-the-art models, we utilized the setup and\ndata selection from Zhou et al. (2020), which pro-\nvides three datasets for citation recommendation\nand one for plagiarism detection.\nBenchmarks. For citation recommendation, the\ndatasets include the ACL Anthology Network Cor-\npus (AAN; Radev et al., 2013), the Semantic\nScholar Open Corpus (OC; Bhagavatula et al.,\n2018), and the Semantic Scholar Open Research\nCorpus (S2ORC; Lo et al., 2020). For plagiarism\ndetection, the dataset is the Plagiarism Detection\nChallenge (PAN; Potthast et al., 2013).\nAAN is composed of computational linguistics\npapers which were published on the ACL Anthol-\nogy from 2001 to 2014, OC is composed of com-\nputer science and neuroscience papers, S2ORC is\ncomposed of open access papers across broad do-\nmains of science, and PAN is composed of web\ndocuments that contain several kinds of plagiarism\nphenomena. For further dataset prepossessing de-\ntails and statistics, see Appendix A.3.\nAlgorithm. For our models, we added the\n[CLS] token at the beginning of the input se-\nquence, assigned it global attention, and concate-\nnated the pair of texts, according to the ﬁnetuning\nsetup discussed in Section 2.2. The hyperparame-\nters are further detailed in Appendix C.2.\nBaselines. We consider the reported results of\nthe following recent baselines:\n– HAN (Yang et al., 2016) proposed the Hierar-\nchical Attention Networks (HANs). These models\nemploy a bottom-up approach in which a docu-\nment is represented as an aggregation of smaller\ncomponents i.e., sentences, and words. They set\ncompetitive performance in different tasks involv-\ning long document encoding (Sun et al., 2018).\n– SMASH (Jiang et al., 2019) is an attentive hi-\nerarchical recurrent neural network (RNN) model,\nused for tasks related to long documents.\n– SMITH (Yang et al., 2020) is a BERT-based\nhierarchical model, similar HANs.\n– CDA (Zhou et al., 2020) is a cross-document\nattentive mechanism (CDA) built on top of HANs,\nbased on BERT or GRU models (see Section 4).\n2654\nModel AAN OC S2orc PAN\nSMASH (2019) 5 80.8 - - -\nSMITH (2020) 5 85.4 - - -\nBERT-HAN (2020) 65.0 86.3 90.8 87.4\nGRU-HAN+CDA (2020) 75.1 89.9 91.6 78.2\nBERT-HAN+CDA (2020) 82.1 87.8 92.1 86.2\nLongformer 85.4 93.4 95.8 80.4\nLocal CDLM 83.8 92.1 94.5 80.9\nRand CDLM 85.7 93.5 94.6 79.4\nPreﬁx CDLM 87.3 94.8 94.7 81.7\nCDLM 88.8 95.3 96.5 82.9\nTable 4: F1 scores over the document matching bench-\nmarks’ test sets.\nBoth SMASH and SMITH reported results only\nover the AAN benchmark. In addition, they used a\nslightly different version of the AAN dataset,5 and\nincluded the full documents, unlike the dataset that\n(Zhou et al., 2020) used, which we utilized as well,\nthat considers only the documents’ abstracts.\nResults. The results on the citation recommenda-\ntion and plagiarism detection tasks are depicted in\nTable 4. We observe that even thoughSMASH and\nSMITH reported results using the full documents\nfor the AAN task, our model outperforms them,\nusing the partial version of the dataset, as in Zhou\net al. (2020). Moreover, unlike our model, CDA\nis task-speciﬁc since it trains new cross-document\nweights for each task, yet it is still inferior to our\nmodel, evaluating on the three citation recommen-\ndation benchmarks. On the plagiarism detection\nbenchmark, interestingly, our models does not per-\nform better. Moreover, CDA impairs the perfor-\nmance of BERT-HAN , implying that dataset does\nnot require detailed cross-document attention at all.\nIn our experiments, ﬁnetuning BERT-HAN+CDA\nover the PAN dataset yielded poor results:F1 score\nof 79.6, substantially lower compared to our mod-\nels. The relatively small size of PAN may explain\nsuch degradations.\n3.4 Multihop Question answering\nIn the task of multihop question answering, a model\nis queried to extract answer spans and evidence sen-\ntences, given a question and multiple paragraphs\nfrom various related and non-related documents.\nThis task includes challenging questions, that an-\nswering them requires ﬁnding and reasoning over\n5Following the most recent work of Zhou et al. (2020),\nwe evaluate our model on their version of the dataset. We\nalso quote the results of SMASH and SMITH methods, even\nthough they used a somewhat different version of this dataset,\nhence their results are not fully comparable to the results of\nour model and those of CDA.\nModel Ans Sup Joint\nTransformer-XH (2020) 66.2 72.1 52.9\nGraph Recurrent Retriever (2020) 73.3 76.1 61.4\nRoBERTa-lf (2020) 73.5 83.4 63.5\nBIGBIRD (2020) 75.5 87.1 67.8\nLongformer 74.5 83.9 64.5\nLocal CDLM 74.1 84.0 64.2\nRand CDLM 72.7 84.8 63.7\nPreﬁx CDLM 74.8 84.7 65.2\nCDLM 74.7 86.3 66.3\nTable 5: HotpotQA-distractor results ( F1) for the dev\nset. We use the “base” model size results from prior\nwork for direct comparison. Ans: answer span, Sup:\nSupporting facts.\nmultiple supporting documents.\nBenchmark. We used the HotpotQA-distractor\ndataset (Yang et al., 2018). Each example in the\ndataset is comprised of a question and 10 differ-\nent paragraphs from different documents, extracted\nfrom Wikipedia; two gold paragraphs include the\nrelevant information for properly answering the\nquestion, mixed and shufﬂed with eight distractor\nparagraphs (for the full dataset statistics, see Yang\net al. (2018)). There are two goals for this task: ex-\ntraction of the correct answer span, and detecting\nthe supporting facts, i.e., evidence sentences.\nAlgorithm. We employ the exact same setup\nfrom (Beltagy et al., 2020): We concatenate all the\n10 paragraphs into one large sequence, separated\nby document separator tokens, and using special\nsentence tokens to separate sentences. The model\nis trained jointly in a multi-task manner, where\nclassiﬁcation heads specialize on each sub-task, in-\ncluding relevant paragraphs prediction, evidence\nsentences identiﬁcation, extracting answer spans\nand inferring the question types (yes, no, or span).\nFor details and hyperparameters, see Appendix C.3\nand Beltagy et al. (2020, Appendix D).\nResults. The results are depicted in Table 5,\nwhere we included also the results for Transformer-\nXH (Zhao et al., 2020), a transformer-based model\nthat constructs global contextualized representa-\ntions, Graph Recurrent Retriever (Asai et al.,\n2020), a recent strong graph-based passage re-\ntrieval method, RoBERTa (Liu et al., 2019), which\nwas modiﬁed by Beltagy et al. (2020) to operate on\nlong sequences (dubbed RoBERTa-lf), and BIG-\nBIRD (Zaheer et al., 2020), a long-range trans-\nformer model which was pretrained on a massive\namount of text. CDLM outperforms all the ablated\nmodels as well as the comparably sized models\n2655\nfrom prior work (except for BIGBIRD ), especially\nin the supporting evidence detection sub-task. We\nnote that the BIGBIRD model was pretrained on\nmuch larger data, using more compute resources\ncompared both to the Longformer model and to our\nmodels. We suspect that with more compute and\ndata, it is possible to close the gap between CDLM\nand BIGBIRD performance. We leave for future\nwork evaluating a larger version of the CDLM\nmodel against large, state-of-the-art models.\n3.5 Attention Analysis\nIt was recently shown that during the pretraining\nphase, LMs learn to encode various types of linguis-\ntic information, that can be identiﬁed via their atten-\ntion patterns (Wiegreffe and Pinter, 2019; Rogers\net al., 2020b). In Clark et al. (2019), the atten-\ntion weights of BERT were proved as informative\nfor probing the degree to which a particular token\nis “important”, as well as its linguistic roles. For\nexample, they showed that the averaged attention\nweights from the last layer of BERT are beneﬁcial\nfeatures for dependency parsing.\nWe posit that our pretraining scheme, which\ncombines global attention and a multi-document\ncontext, captures alignment and mapping informa-\ntion across documents. Hence, we hypothesize\nthat the global attention mechanism favors cross-\ndocument (CD), long-range relations. To gain more\ninsight, our goal is to investigate if our proposed\npretraining method leads to relatively higher global\nattention weights between co-referring mentions\ncompared to non-co-referring ones, even without\nany ﬁnetuning over CD coreference resolution.\nBenchmark. We randomly sampled 1,000 posi-\ntive and 1,000 negative coreference-pair examples\nfrom the ECB+ CD coreference resolution bench-\nmark, for both events and entities. Each example\nconsists of two concatenated documents and two\ncoreference candidate mentions (see Section 3.2).\nAnalysis Method. For each example, which con-\ntains two mention spans, we randomly pick one to\nbe considered as the source span, while the second\none is the target span. We denote the set of the\ntokens in the source and target spans as S and T,\nrespectively. Our goal is to quantify the degree of\nalignment between S and T, using the attention\npattern of the model. We ﬁrst assign global atten-\ntion to the tokens in the source span (in S). Next,\nwe pass the full input through the model, compute\nthe normalized attention weights for all the tokens\nDoc 1: President Obama will name Dr. Regina Benjamin as\nU.S. Surgeon General in a Rose Garden announcement late\nthis morning. Benjamin, an Alabama family physician, [...]\nDoc 2 : [...] Obama nominates new surgeon general:\nMacArthur “genius grant ”fellow Regina Benjamin. [...]\nFigure 3: An example from ECB+ corpus. The un-\nderlined phrases represent a positive, co-referring event\nmention pair. The blue (green) colored mention is con-\nsidered as the source (target) span.\nin the input with respect to S, by aggregating the\nscores extracted from the last layer of the model.\nThe score for an input token i /∈S, is given by\ns(i|S) ∝exp\n\n\nn∑\nk=1\n∑\nj∈S\n(\nαk\ni,j + αk\nj,i\n)\n\n,\nwhere αk\ni,j is the global attention weight from token\nito token jproduced by head k, and nis the total\nnumber of attention heads (the score is computed\nusing only the last layer of the model). Note that we\ninclude both directions of attention. The target span\nscore is then given by s(T|S) = 1\n|T|\n∑\nj∈T s(j|S).\nFinally, we calculate the percentile rank (PR) of\ns(T|S), compared to the rest of the token scores\nwithin the containing document of T, namely,\n{s(i|S)|i /∈T}.\nFor positive coreference examples, plausible re-\nsults are expected to be associated with high at-\ntention weights between the source and the target\nspans, resulting with a high value of s(T|S), and\nthus, yielding a higher PR. For negative examples,\nthe target span is not expected to be promoted with\nrespect to the rest of the tokens in the document.\nResults. First, we apply the procedure above over\none selected example, depicted in Figure 3. We\nconsider the two CD co-referring event mentions:\nname and nominates as the source and target spans,\nrespectively. The target span received a PR of 69%\nwhen evaluating the underlying Longformer. No-\ntably, it received a high PR of 90% when using our\nCDLM , demonstrating the advantage of our novel\npretraining method. Next, we turn to a systematic\nexperiment, elucidating the relative advantage of\npretraining with global attention across related doc-\numents. In Table 6, we depict the mean PR (MPR)\ncomputed over all the sampled examples, for all\nour pretrained models. We observe that none of\nthe models fail6 on the set of negatives, since the\nnegative examples contain reasonable event or en-\ntity mentions, rather than random, non informative\n6Typically, PR of∼50% corresponds to random ranking.\n2656\nPos. MPR (%) Neg. MPR (%)\nevents entities events entities\nLocal\nLongformer 61.9 59.7 54.8 50.5\nLocal CDLM 62.2 60.8 54.6 52.6\nGlobal\nRand CDLM 70.6 69.1 56.6 53.2\nPreﬁx CDLM 70.7 69.4 58.5 56.5\nCDLM 72.1 70.3 58.0 55.7\nTable 6: Cross-document coreference resolution align-\nment MPR scores of the target span, with respect to the\ntokens in the same document.\nspans. For the positive examples, the gap of up\nto 10% of MPR between the “Local” and “Global”\nmodels shows the advantage of adopting global\nattention during the pretraining phase. This indi-\ncates that the global attention mechanism implicitly\nhelps to encode alignment information.\n4 Related Work\nRecently, long-context language models (Beltagy\net al., 2020; Zaheer et al., 2020) introduced the idea\nof processing multi-document tasks using a single\nlong-context sequence encoder. However, pretrain-\ning objectives in these models consider only single\ndocuments. Here, we showed that additional gains\ncan be obtained by MLM pretraining using multi-\nple related documents as well as a new dynamic\nglobal attention pattern.\nProcessing and aggregating information from\nmultiple documents has been also explored in the\ncontext of document retieval, aiming to extract in-\nformation from a large set of documents (Guu et al.,\n2020; Lewis et al., 2020a,b; Karpukhin et al., 2020).\nThese works focus on retrieving relevant informa-\ntion from often a large collection of documents,\nby utilizing short-context LMs, and then generate\ninformation of interest. CDLM instead provides an\napproach for improving the encoding and contex-\ntualizing information across multiple documents.\nAs opposed to the mentioned works, our model\nutilizes long-context LM and can include broader\ncontexts of more than a single document.\nThe use of cross-document attention has been\nrecently explored by the Cross-Document Atten-\ntion (CDA) (Zhou et al., 2020). CDA speciﬁ-\ncally encodes two documents, using hierarchical\nattention networks, with the addition of cross at-\ntention between documents, and makes similar-\nity decision between them. Similarly, the recent\nDCS model (Ginzburg et al., 2021) suggested a\ncross-document ﬁnetuning scheme for unsuper-\nvised document-pair matching method (process-\ning only two documents at once). Our CDLM, by\ncontrast, is a general pretrained language model\nthat can be applied to a variety of multi-document\ndownstream tasks, without restrictions on the num-\nber of input documents, as long as they ﬁt the input\nlength of the Longformer.\nFinally, our pretraining scheme is conceptually\nrelated to cross-encoder models that leverage simul-\ntaneously multiple related information sources. For\nexample, the Translation Language Model (TLM)\n(Conneau and Lample, 2019) encodes together sen-\ntences and their translation, while certain cross-\nmodality encoders pretrain over images and texts\nin tandem (e.g., ViLBERT (Lu et al., 2019)).\n5 Conclusion\nWe presented a novel pretraining strategy and tech-\nnique for cross-document language modeling, pro-\nviding better encoding for cross-document (CD)\ndownstream tasks. Our contributions include the\nidea of leveraging clusters of related documents\nfor pretraining, via cross-document masking, along\nwith a new long-range attention pattern, together\ndriving the model to learn to encode CD relation-\nships. This was achieved by extending the global at-\ntention mechanism of the Longformer model to ap-\nply already in pretraining, creating encodings that\nattend to long-range information across and within\ndocuments. Our experiments assess that our cross-\ndocument language model yields new state-of-the-\nart results over several CD benchmarks, while, in\nfact, employing substantially smaller models. Our\nanalysis showed that CDLM implicitly learns to\nrecover long-distance CD relations via the atten-\ntion mechanism. We propose future research to\nextend this framework to train larger models, and\nto develop cross-document sequence-to-sequence\nmodels, which would support CD tasks that involve\na generation phase.\nAcknowledgments\nWe thank Doug Downey and Luke Zettlemoyer for\nfruitful discussions and helpful feedback, and Yoav\nGoldberg for helping us connect with collaborators\non this project. The work described herein was\nsupported in part by grants from Intel Labs, the\nIsrael Science Foundation grant 1951/17, the Israeli\nMinistry of Science and Technology, and the NSF\nGrant OIA-2033558.\n2657\nReferences\nEmily Allaway, Shuai Wang, and Miguel Ballesteros.\n2021. Sequential cross-document coreference reso-\nlution. arXiv preprint arXiv:2104.08413.\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2020. Learn-\ning to retrieve reasoning paths over wikipedia graph\nfor question answering. In International Conference\non Learning Representations (ICLR).\nAmit Bagga and Breck Baldwin. 1998. Algorithms\nfor scoring coreference chains. In The ﬁrst interna-\ntional conference on language resources and evalua-\ntion workshop on linguistics coreference, volume 1,\npages 563–566. Citeseer.\nShany Barhom, Vered Shwartz, Alon Eirew, Michael\nBugert, Nils Reimers, and Ido Dagan. 2019. Re-\nvisiting joint modeling of cross-document entity and\nevent coreference resolution. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4179–4189, Florence,\nItaly. Association for Computational Linguistics.\nIz Beltagy, Matthew E Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150.\nChandra Bhagavatula, Sergey Feldman, Russell Power,\nand Waleed Ammar. 2018. Content-based citation\nrecommendation. In Proceedings of the 2018 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n238–251, New Orleans, Louisiana. Association for\nComputational Linguistics.\nArie Cattan, Alon Eirew, Gabriel Stanovsky, Mandar\nJoshi, and Ido Dagan. 2020. Streamlining cross-\ndocument coreference resolution: Evaluation and\nmodeling. ArXiv, abs/2009.11032.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286, Florence, Italy. Association\nfor Computational Linguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. Advances in\nNeural Information Processing Systems (NIPS).\nAgata Cybulska and Piek V ossen. 2014. Using a\nsledgehammer to crack a nut? lexical diversity\nand event coreference resolution. In Proceedings\nof the Ninth International Conference on Language\nResources and Evaluation (LREC’14), pages 4545–\n4552, Reykjavik, Iceland. European Language Re-\nsources Association (ELRA).\nAgata Cybulska and Piek V ossen. 2015. “Bag of\nEvents” approach to event coreference resolution.\nsupervised classiﬁcation of event templates. IJCLA,\npage 11.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019a. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019b. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nRotem Dror, Gili Baumer, Segev Shlomov, and Roi Re-\nichart. 2018. The hitchhiker’s guide to testing statis-\ntical signiﬁcance in natural language processing. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1383–1392, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nAlexander Fabbri, Irene Li, Tianwei She, Suyi Li, and\nDragomir Radev. 2019. Multi-news: A large-scale\nmulti-document summarization dataset and abstrac-\ntive hierarchical model. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 1074–1084, Florence, Italy.\nAssociation for Computational Linguistics.\nDvir Ginzburg, Itzik Malkiel, Oren Barkan, Avi Caciu-\nlaru, and Noam Koenigstein. 2021. Self-supervised\ndocument similarity ranking via contextualized lan-\nguage models and hierarchical inference. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021 , pages 3088–3098, Online.\nAssociation for Computational Linguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-\nsupat, and Mingwei Chang. 2020. Retrieval aug-\nmented language model pre-training. In Proceed-\nings of the International Conference on Machine\nLearning (ICML).\nEran Hirsch, Alon Eirew, Ori Shapira, Avi Caciu-\nlaru, Arie Cattan, Ori Ernst, Ramakanth Pasunuru,\nHadar Ronen, Mohit Bansal, and Ido Dagan. 2021.\nifacetsum: Coreference-based interactive faceted\nsummarization for multi-document exploration. In\nProceedings of the Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP): Sys-\ntem Demonstrations.\nJyun-Yu Jiang, Mingyang Zhang, Cheng Li, Michael\nBendersky, Nadav Golbandi, and Marc Najork.\n2019. Semantic text matching for long-form docu-\nments. In The World Wide Web Conference (WWW).\n2658\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 6769–\n6781, Online. Association for Computational Lin-\nguistics.\nKian Kenyon-Dean, Jackie Chi Kit Cheung, and Doina\nPrecup. 2018. Resolving event coreference with\nsupervised representation learning and clustering-\noriented regularization. In Proceedings of the\nSeventh Joint Conference on Lexical and Com-\nputational Semantics , pages 1–10, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nKenton Lee, Luheng He, Mike Lewis, and Luke Zettle-\nmoyer. 2017. End-to-end neural coreference reso-\nlution. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 188–197, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nMike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Ar-\nmen Aghajanyan, Sida Wang, and Luke Zettlemoyer.\n2020a. Pre-training via paraphrasing. Advances in\nNeural Information Processing Systems (NeurIPS).\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2020b. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Advances in\nNeural Information Processing Systems (NeurIPS).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-\nney, and Daniel Weld. 2020. S2ORC: The semantic\nscholar open research corpus. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4969–4983, Online. As-\nsociation for Computational Linguistics.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic visi-\nolinguistic representations for vision-and-language\ntasks. In Advances in Neural Information Process-\ning Systems (NIPS).\nXiaoqiang Luo. 2005. On coreference resolution per-\nformance metrics. In Proceedings of the confer-\nence on Human Language Technology and Empiri-\ncal Methods in Natural Language Processing, pages\n25–32. Association for Computational Linguistics.\nYehudit Meged, Avi Caciularu, Vered Shwartz, and Ido\nDagan. 2020. Paraphrasing vs coreferring: Two\nsides of the same coin. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020 ,\npages 4897–4907, Online. Association for Computa-\ntional Linguistics.\nNaﬁse Sadat Moosavi and Michael Strube. 2016.\nWhich coreference evaluation metric do you trust?\na proposal for a link-based entity aware metric. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 632–642, Berlin, Germany. As-\nsociation for Computational Linguistics.\nMartin Potthast, Matthias Hagen, Tim Gollub, Martin\nTippmann, Johannes Kiesel, Paolo Rosso, Efstathios\nStamatatos, and Benno Stein. 2013. Overview of\nthe 5th international competition on plagiarism de-\ntection. In Conference on Multilingual and Multi-\nmodal Information Access Evaluation (CLEF).\nDragomir R Radev, Pradeep Muthukrishnan, Vahed\nQazvinian, and Amjad Abu-Jbara. 2013. The ACL\nanthology network corpus. Language Resources\nand Evaluation, 47(4):919–944.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020a. A primer in BERTology: What we know\nabout how BERT works. Transactions of the Associ-\nation for Computational Linguistics, 8:842–866.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020b. A primer in BERTology: What we know\nabout how BERT works. Transactions of the Associ-\nation for Computational Linguistics (TACL).\nVered Shwartz, Gabriel Stanovsky, and Ido Dagan.\n2017. Acquiring predicate paraphrases from news\ntweets. In Proceedings of the 6th Joint Conference\non Lexical and Computational Semantics (*SEM\n2017), pages 155–160, Vancouver, Canada. Associa-\ntion for Computational Linguistics.\nQingying Sun, Zhongqing Wang, Qiaoming Zhu, and\nGuodong Zhou. 2018. Stance detection with hierar-\nchical attention network. In Proceedings of the 27th\nInternational Conference on Computational Linguis-\ntics, pages 2399–2409, Santa Fe, New Mexico, USA.\nAssociation for Computational Linguistics.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang\nShen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu\nYang, Sebastian Ruder, and Donald Metzler. 2021.\nLong range arena: A benchmark for efﬁcient trans-\nformers. In International Conference on Learning\nRepresentations (ICLR).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\n2659\nyou need. In Advances in neural information pro-\ncessing systems (NIPS).\nMarc Vilain, John Burger, John Aberdeen, Dennis Con-\nnolly, and Lynette Hirschman. 1995. A model-\ntheoretic coreference scoring scheme. In Proceed-\nings of the 6th conference on Message understand-\ning, pages 45–52. Association for Computational\nLinguistics.\nSarah Wiegreffe and Yuval Pinter. 2019. Attention is\nnot not explanation. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 11–20, Hong Kong, China. Associ-\nation for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nLiu Yang, Mingyang Zhang, Cheng Li, Michael Ben-\ndersky, and Marc Najork. 2020. Beyond 512 tokens:\nSiamese multi-depth transformer-based hierarchical\nencoder for long-form document matching. In Pro-\nceedings of the ACM International Conference on\nInformation & Knowledge Management (CIKM).\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 2369–2380, Brussels, Belgium. Association\nfor Computational Linguistics.\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,\nAlex Smola, and Eduard Hovy. 2016. Hierarchical\nattention networks for document classiﬁcation. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 1480–1489, San Diego, California. Associa-\ntion for Computational Linguistics.\nXiaodong Yu, Wenpeng Yin, and Dan Roth. 2020.\nPaired representation learning for event and entity\ncoreference. arXiv preprint arXiv:2010.12808.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big Bird: Transformers for\nlonger sequences. Advances in Neural Information\nProcessing Systems (NeurIPS).\nYutao Zeng, Xiaolong Jin, Saiping Guan, Jiafeng Guo,\nand Xueqi Cheng. 2020. Event coreference reso-\nlution with their paraphrases and argument-aware\nembeddings. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics ,\npages 3084–3094, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nChen Zhao, Chenyan Xiong, Corby Rosset, Xia\nSong, Paul Bennett, and Saurabh Tiwary. 2020.\nTransformer-XH: Multi-evidence reasoning with ex-\ntra hop attention. In International Conference on\nLearning Representations (ICLR).\nXuhui Zhou, Nikolaos Pappas, and Noah A. Smith.\n2020. Multilevel text alignment with cross-\ndocument attention. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 5012–5025, On-\nline. Association for Computational Linguistics.\n2660\nA Dataset Statistics and Details\nIn this section, we provide details regrading the\npretraining corpus and benchmarks we used during\nour experiments.\nA.1 Multi-News Corpus\nWe used the preprocessed, not truncated version of\nMulti-News, which totals 322MB of uncompressed\ntext.7 Each one of the preprocessed documents\ncontains up to 500 tokens. The average and 90 th\npercentile of input length is 2.5k and 3.8K tokens,\nrespectively. In Table 7 we list the number of re-\nlated documents per cluster. This follows the origi-\nnal dataset construction suggested in Fabbri et al.\n(2019).\n# of docs in cluster Frequency\n3 12,707\n4 5,022\n5 1,873\n6 763\n7 382\n8 209\n9 89\n10 33\nTotal 21,078\nTable 7: MultiNews training set statistics.\nA.2 ECB+ Dataset\nIn Table 8, we list the statistics about training, de-\nvelopment, and test splits regarding the topics, doc-\numents, mentions and coreference clusters. We\nfollow the data split used by previous works (Cybul-\nska and V ossen, 2015; Kenyon-Dean et al., 2018;\nBarhom et al., 2019): For training, we consider the\ntopics: 1, 3, 4, 6-11, 13- 17, 19-20, 22, 24-33; For\nValidation, we consider the topics: 2, 5, 12, 18, 21,\n23, 34, 35; For test, we consider the topics: 36-45.\nTrain Validation Test\nTopics 25 8 10\nDocs 594 196 206\nMentions 3808/4758 1245/1476 1780/2055\nClusters 411/472 129/125 182/196\nTable 8: ECB+ dataset statistics. The slash numbers for\nMentions and Clusters represent event/entity statistics.\n7We used the dataset available in\nhttps://drive.google.com/open?id=\n1qZ3zJBv0zrUy4HVWxnx33IsrHGimXLPy.\nA.3 Paper Citation Recommendation &\nPlagiarism Detection Datasets\nIn Table 9, we list the statistics about training, de-\nvelopment, and test splits for each benchmark sep-\naratly, and in Table 10, we list the document and\nexample counts for each benchmark. The statistics\nare taken from Zhou et al. (2020).\nDataset Train Validation Test\nAAN 106,592 13,324 13,324\nOC 240,000 30,000 30,000\nS2ORC 152,000 19000 19000\nPAN 17,968 2,908 2,906\nTable 9: Document-to-Document benchmarks statis-\ntics: Details regrading the training, validation, and test\nsplits.\nDataset # of doc pairs # of docs\nAAN 132K 13K\nOC 300K 567K\nS2ORC 190K 270K\nPAN 34K 23K\nTable 10: Document-to-Document benchmarks statis-\ntics: The reported numbers are the count of document\npairs and the count of unique documents.\nThe preprocessing of the datasets performed\nby Zhou et al. (2020) includes the following steps:\nFor AAN, only pairs of documents that include ab-\nstracts are considered, and only their abstracts are\nused. For OC, only one citation per paper is con-\nsidered, and the dataset was downsampled signiﬁ-\ncantly. For S2ORC, formed pairs of citing sections\nand the corresponding abstract in the cited paper\nare included, and the dataset was downsampled sig-\nniﬁcantly. For PAN, pairs of relevant segments out\nof the entire document were extracted.\nFor all the datasets, negative pairs were sampled\nrandomly. Then, a standard preprocessing that in-\ncludes ﬁltering out characters that are not digits,\nletters, punctuation, or white space in the texts was\nperformed.\nB CDLM Pretraining Hyperparameters\nIn this section, we detail the hyperparameters set-\nting of the models we pretrained, includingCDLM\nPreﬁx CDLM , Rand CDLM , and Local CDLM :\nThe input sequences are of the length of 4,096,\neffective batch size of 64 (using gradient accumula-\ntion and batch size of 8), a maximum learning rate\n2661\nCD-LM\n</doc-s>...<doc-s>[CLS] <m> </m>\nsum \n... </doc-s>...<doc-s> <m> </m>...\nsum \nFigure 4: CD-coreference resolution pairwise mention representation, using the new setup, for our CDLM models.\nmi\nt, mj\nt and st are the cross-document contextualized representation vectors for mentionsiand j, and of the[CLS]\ntoken, respectively. mi\nt ◦mj\nt is the element-wise product between mi\nt and mj\nt . mt(i,j) is the ﬁnal produced\npairwise-mention representation. The tokens colored in yellow represent global attention, and tokens colored in\nblue represent local attention.\nof 3e-5, and a linear warmup of 500 steps, followed\nby a power 3 polynomial decay. For speeding up\nthe training and reducing memory consumption, we\nused the mixed-precision (16-bits) training mode.\nThe pretraining took 8 days, using eight 48GB\nRTX8000 GPUs. The rest of the hyperparame-\nters are the same as for RoBERTa (Liu et al., 2019).\nNote that training CDLM using the large version\nof the Longformer model might require 2-3 times\nmore memory and time.\nC Finetuning on Downstream Tasks\nIn this section, we elaborate further implementation\ndetails regarding the downstream tasks that we ex-\nperimented, including the hyperparameter choices\nand the algorithms used.\nC.1 Cross-Document Coreference Resolution\nThe setup for our cross-document coreference res-\nolution pairwise scoring is illustrated in Figure 4.\nWe concatenate the relevant documents using the\nspecial document separator tokens, then encode\nthem using our CDLM along with the [CLS] to-\nken at the beginning of this sequence, as suggested\nin Section 2.2. For within-document coreference\ncandidate examples, we use just the single contain-\ning document with one set of document separa-\ntors, for the single input document. Inspired by Yu\net al. (2020), we use candidate mention marking:\nwe wrap the mentions with special tokens ⟨m⟩and\n⟨/m⟩in order to direct the model to speciﬁcally\npay attention to the candidates representations. Ad-\nditionally, we assign global-attention to [CLS],\n⟨m⟩, ⟨/m⟩, and the mention tokens themselves, ac-\ncording to the ﬁnetuning strategy proposed in Sec-\ntion 2.2. Our ﬁnal pairwise-mention representation\nis formed like in Zeng et al. (2020) and Yu et al.\n(2020): We concatenate the cross-document contex-\ntualized representation vectors for the tth sample:\nmt(i,j) =\n[\nst,mi\nt,mj\nt ,mi\nt ◦mj\nt\n]\n,\nwhere [·] denotes the concatenation operator, st is\nthe cross-document contextualized representation\nvector of the [CLS] token, and each of mi\nt and\nmj\nt is the sum of candidate tokens of the corre-\nsponding mentions (iand j). Then, we train the\npairwise scorer according to the suggested ﬁnetun-\ning scheme. At test time, similar to most recent\nworks, we apply agglomerative clustering to merge\nthe most similar cluster pairs.\nRegarding the training data collection and hyper-\nparameter setting, we adopt the same protocol as\nsuggested in Cattan et al. (2020):8 Our training set\nis composed of positive instances which consist of\nall the pairs of mentions that belong to the same\n8We used the implementation taken from https://\ngithub.com/ariecattan/cross_encoder\n2662\ncoreference cluster, while the negative examples\nare randomly sampled.\nThe resulting feature vector is passed through\na MLP pairwise scorer that is composed of one\nhidden layer of the size of 1024, followed by the\nTanh activation. We ﬁnetune our models for 10\nepochs, with an effective batch size of 128. We\nused eight 32GB V100-SMX2 GPUs for ﬁnetuning\nour models. The ﬁnetuning process took ∼28 and\n∼45 hours per epoch, for event coreference and\nentity coreference, respectively.\nC.2 Multi-Document Classiﬁcation\nWe tune our models for 8 epochs, using a batch size\nof 32, and used the same hyperparameter setting\nfrom Zhou et al. (2020, Section 5.2). 9 We used\neight 32GB V100-SMX2 GPUs for ﬁnetuning our\nmodels. The ﬁnetuning process took ∼2,∼5,∼3,\nand ∼0.5 hours per epoch, for AAN, OC, S2ORC,\nand for PAN, respectively. We used the mixed-\nprecision training mode, to reduce time and mem-\nory consumption.\nC.3 Multihop Question Answering\nFor preparing the data for training and evalu-\nation, we follow our ﬁnetuning scheme: for\neach example, we concatenate the question\nand all the 10 paragraphs in one long con-\ntext. We particularly use the following input\nformat with special tokens and our document\nseparators: “ [CLS] [q] question [/q]\n⟨doc-s⟩⟨t⟩ title1 ⟨/t⟩⟨s⟩sent1,1 ⟨/s⟩\n⟨s⟩ sent1,2 ⟨/s⟩ ⟨/doc-s⟩ ... ⟨t⟩\n⟨doc-s⟩ title2 ⟨/t⟩ sent2,1 ⟨/s⟩ ⟨s⟩\nsent2,2 ⟨/s⟩⟨s⟩...” where [q], [/q], ⟨t⟩,\n⟨/t⟩, ⟨s⟩, ⟨/s⟩, [p] are special tokens represent-\ning, question start and end, paragraph title start\nand end, and sentence start and end, respectively.\nThe new special tokens were added to the models\nvocabulary and randomly initialized before task\nﬁnetuning. We use global attention to question\ntokens, paragraph title start tokens as well as sen-\ntence tokens. The model’s structure is taken from\nBeltagy et al. (2020).\nSimilar to Beltagy et al. (2020), we ﬁnetune our\nmodels for 5 epochs, using a batch size of 32, learn-\ning rate of 1e-4, 100 warmup steps. Finetuning on\nour models took ∼6 hours per epoch, using four\n48GB RTX8000 GPUs for ﬁnetuning our models.\n9we used the script https://github.com/\nXuhuiZhou/CDA/blob/master/BERT-HAN/run_\nex_sent.sh"
}