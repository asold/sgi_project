{
  "title": "An Object Detection Algorithm for Rotary-Wing UAV Based on AWin Transformer",
  "url": "https://openalex.org/W4210487018",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2098767721",
      "name": "Yunlong Fan",
      "affiliations": [
        "PLA Information Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A1995857402",
      "name": "Ou Li",
      "affiliations": [
        "PLA Information Engineering University"
      ]
    },
    {
      "id": "https://openalex.org/A2123439527",
      "name": "Guangyi Liu",
      "affiliations": [
        "PLA Information Engineering University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6794345597",
    "https://openalex.org/W2982770724",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W6792695861",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6791943378",
    "https://openalex.org/W2765322173",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W2964345399",
    "https://openalex.org/W2991277185",
    "https://openalex.org/W2808032407",
    "https://openalex.org/W3089146878",
    "https://openalex.org/W2901871277",
    "https://openalex.org/W3042788887",
    "https://openalex.org/W6762082143",
    "https://openalex.org/W6799166919",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W6792155083",
    "https://openalex.org/W3136561353",
    "https://openalex.org/W6784094891",
    "https://openalex.org/W2976792973",
    "https://openalex.org/W3110391149",
    "https://openalex.org/W6797360341",
    "https://openalex.org/W2990467641",
    "https://openalex.org/W2100495367",
    "https://openalex.org/W3200501610",
    "https://openalex.org/W3044875250",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2986358680",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W3175630421",
    "https://openalex.org/W3034971973",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W2964444661",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W6774670964",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W4313007769",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2944165510",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3187418919",
    "https://openalex.org/W3009561768",
    "https://openalex.org/W3157528469",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3139049060",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "The increasing use of rotary-wing UAVs poses security risks, which makes image detection of rotary-wing UAVs a critical issue. This paper proposes an object detection algorithm for rotary-wing UAVs based on a transformer network. A self-attention mechanism is used to utilize the local contextual information to extract the features of the rotary-wing UAV more effectively, which improves the accuracy of object detection. Meanwhile, a new self-attention mechanism is designed, in which the query vector and the key vector of the surrounding annular area are calculated separately and then concatenated by different heads of attention. Experimental results show that, compared with existing algorithms, the proposed algorithm improves the mean average precision by 1.7&#x0025; on the proposed rotary-wing UAV dataset.",
  "full_text": "Received December 6, 2021, accepted January 20, 2022, date of publication January 27, 2022, date of current version February 4, 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.3147264\nAn Object Detection Algorithm for Rotary-Wing\nUAV Based on AWin Transformer\nYUNLONG FAN\n , OU LI\n , AND GUANGYI LIU\nSchool of Information Engineering, PLA Strategic Support Force Information Engineering University, Zhengzhou 450001, China\nCorresponding author: Yunlong Fan (ﬂynn92510@163.com)\nABSTRACT The increasing use of rotary-wing UA Vs poses security risks, which makes image detection\nof rotary-wing UA Vs a critical issue. This paper proposes an object detection algorithm for rotary-wing\nUA Vs based on a transformer network. A self-attention mechanism is used to utilize the local contextual\ninformation to extract the features of the rotary-wing UA V more effectively, which improves the accuracy of\nobject detection. Meanwhile, a new self-attention mechanism is designed, in which the query vector and the\nkey vector of the surrounding annular area are calculated separately and then concatenated by different heads\nof attention. Experimental results show that, compared with existing algorithms, the proposed algorithm\nimproves the mean average precision by 1.7% on the proposed rotary-wing UA V dataset.\nINDEX TERMS Object detection, unmanned aerial vehicle, annular window, transformer.\nI. INTRODUCTION\nWith the development of science and technology, the appli-\ncation of rotary-wing drones is increasing. While bringing\nconvenience, it also causes security risks such as privacy\nleakage and intrusion of key facilities [1]–[4]. Owing to\nthe diverse types of rotary-wing UA Vs, the complex and\nchangeable environment and limited memory resources of\nedge computing devices, UA V detection is still a challenging\ntask [1]–[7]. Therefore, in terms of the security requirements\nof actual applications and other aspects, the research on the\ndetection of UA Vs is constantly deepening [1]–[4]. Espe-\ncially, the application of object detection technology in the\nﬁelds of intelligent security, automatic driving, smart homes,\nand robot vision is further developed. The image object detec-\ntion technology based on deep learning [8], [9] can achieve\nreal-time accuracy.\nThe existing studies on UA V detection consider the infor-\nmation sources including photoelectric, thermal, and acoustic\nsensors, as well as radars and radios. The optical image\nsensors are commonly used, and UA V detection technologies\nbased on optical images have been studied [10]–[16]. Most of\nthese studies propose solutions based on convolutional neural\nnetworks (CNNs). However, the CNNs used are not sufﬁcient\nfor extracting object features and using context information.\nIt is necessary to improve the detection accuracy according\nto the characteristics of the rotary-wing UA V and conduct\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Jenny Mahoney.\nin-depth research on the UA V detection algorithm based on\ndeep neural networks.\nCurrently, the main methods of using contextual fea-\ntures include methods based on convolutional neural net-\nworks [17] and methods based on transformer networks [18].\nSome studies have focused on convolutional neural net-\nwork algorithms, including FPN [19], Cascade R-CNN [20],\nand several deformed networks of FPN [21]–[23]. These\nstudies use the relevance of different regions to assign dif-\nferent weights to the object regions, thereby strengthening\nthe contextual connection of object features. Other studies\ninvestigated object detection algorithms based on transformer\nnetworks, including Transformer [24], ViT [25], DETR [26],\nDeformable DETR [27], Swin Transformer [28] and CSWin\nTransformer [29], and so on. Through the transformer net-\nwork structure, the multi-head self-attention mechanism and\nits variants are used to make full use of the contextual infor-\nmation of the entire image, thereby improving the detection\naccuracy.\nThe limitations in the research on published rotary-wing\nUA V image object detection algorithms are described below.\nSome works [10], [11] utilized deep CNN networks such\nas VGG16 and YOLO v3 with transfer learning to detect\nUA Vs. The disadvantage is that the network structure is\nsimple, resulting in insufﬁcient feature extraction ability of\ndrones. The study in [12] utilized super-resolution techniques\nto enlarge the image, which improved recall instead of preci-\nsion. Some works [13]–[15] combined a CNN network with\nfeatures such as the Generic Fourier Descriptor and Haar\nVOLUME 10, 2022 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 13139\nY. Fanet al.: Object Detection Algorithm for Rotary-Wing UAV Based on AWin Transformer\nCascade features, but the expressive ability of artiﬁcially\ndesigned features is not strong enough. The work in [16]\nproposed a TIB-Net to better detect small-size drones, but\nthe ability to learn the context information is not sufﬁciently\nstrong.\nIn the research on published rotary-wing UA V image object\ndetection algorithms, most of them used the CNN network\narchitecture as the backbone network for UA V detection.\nCompared with the detection algorithms based on the trans-\nformer structure, the long-dependent learning ability of the\nworks is weak, and the learning ability of context connection\nis weak as well. Therefore, it is easy to cause missed alarms\nand false alarms in scenes where the rotary-wing UA V is\nmotion-blurred, densely distributed, and easily confused with\nthe background.\nCompared with CNN, transformer has unique advantages\nbecause it can learn the global information of the image in\nthe initial stage. In addition, transformer uses the attention\nmechanism to capture contextual information, so it has a\nstronger learning ability for long-term dependence and con-\ntextual connections. However, these advantages are at the cost\nof increasing the calculation amount, which is reﬂected in\nViT [25] and DETR [26]. To achieve a detection accuracy\nsimilar to that of the CNN-based object detection algorithm,\nmore calculations and more training time consumption are\nrequired. The algorithms proposed later such as Deformable\nDETR [27], Swin Transformer [28], and CSWin Trans-\nformer [29] put forward different solutions to the problem\nof a large amount of calculation. They reduced the number\nof parameters of the network while ensuring that the accu-\nracy was not reduced. The algorithms of Swin Transformer\nand CSWin Transformer [29] are based on the hierarchical\nstructure designed in CNN. The transformer structure is used\nas the backbone of the object detection network, and the\nadvantages of CNN and Transformer are combined. However,\nDeformable DETR [27] lacks the design of a hierarchical\nstructure. The sliding window attention mechanism in Swin\nTransformer [28] applies the same attention operation across\nmultiple heads. The cross-shaped sliding window attention\nmechanism in CSWin Transformer [29] focuses on the image\nin the long strip area. It is not designed for the characteristics\nof rotary-wing UA Vs, leading to computational redundancy.\nThe analysis of the existing methods reveals that the front\nand back scenes are easily confused during UA V detection,\nresulting in false alarms and missing alarms. This is because\nthe extraction of contextual information is not sufﬁcient in the\ncurrent studies, and there is no more effective self-attention\nmechanism designed for rotary-wing UA Vs.\nTo solve the problem of missed and false alarms in the\ndetection of rotary-wing UA Vs and to improve the accu-\nracy of object detection, this paper proposes a UA V object\ndetection algorithm based on an annular window transformer\nnetwork, which has a good learning ability for context con-\nnection. Meanwhile, a new self-attention mechanism with\nthe annular window is designed, which takes advantage\nof the characteristics of strong symmetry and concentrated\nimage area of rotary-wing UA Vs. This method can effectively\nimprove the learning ability of the characteristics and con-\ntext connections of rotary-wing UA Vs, and can reduce the\ndetection of false alarms and missed alarms, which is of great\nsigniﬁcance.\nThe main contributions of this paper are:\n1) A UA V object detection algorithm based on the trans-\nformer network was proposed. By combining the contex-\ntual connection of deep self-attention transformers and the\nhierarchical and progressive detection characteristics of con-\nvolutional networks, this algorithm achieves high-precision\nrotary-wing UA V object detection.\n2) A new self-attention mechanism with the annular win-\ndow was proposed. The query vector and the key vector of\nthe surrounding annular area are calculated separately, and\nthen they are concatenated by different heads of attention. The\nproposed method can make better use of the strong correlation\narea around the object and reduce the amount of calculation\nof the attention mechanism.\nThe remainder of this paper is organized as follows.\nIn Section 2, related works are reviewed. In Section 3, the pro-\nposed network architecture and system model are introduced.\nIn Section 4, a self-attention mechanism based on the annular\nwindow is designed, and the computational complexity of the\nalgorithm is analyzed. The experimental results are analyzed\nand discussed in Section 5. Finally, the work is summarized\nin Section 6.\nII. RELATED WORK\nA. DETECTION ALGORITHMS BASED ON\nCONVOLUTIONAL NEURAL NETWORK\nRen S. et al.[30] proposed an end-to-end Faster R-CNN net-\nwork and designed the RPN (Region Proposal Network) mod-\nule. Compared with Fast R-CNN [43], the detection accuracy\nis improved. Though the detection speed is increased by\n10 times, it is still not real-time enough. He K. et al.[31] pro-\nposed Mask R-CNN to solve the problem of object instance\nsegmentation. Mask R-CNN adopts the RoI align method,\nwhich mainly solves the problem that the features in the\nRoI pooling in Faster R-CNN are not aligned with the RoI.\nKaiming He et al. [32] proposed Focal Loss to solve the\nextremely uneven ratio of positive and negative samples in\na single-stage detector and reduce the weight of the loss of\nsamples classiﬁed as background scenes. Besides, a dense\nmulti-object detector, RetinaNet, was designed to evaluate\nthe effect of this loss, which improves the detection accuracy\nof the single-stage detector. Lin T. et al.[19] proposed FPN\n(feature pyramid networks) that uses the inherent pyramid\nhierarchy of CNN to construct a feature pyramid at an addi-\ntional cost. This method solves the problem of multi-scale\nchanges and reduces the amount of calculation better than the\nmethod of image pyramids. Tian Z. et al.[33] proposed a non-\nanchor frame-based detector FCOS, which is a pixel-by-pixel\nobject detection algorithm based on a fully convolutional\nneural network. FCOS can provide an anchor-free and no-\nproposal solution. Z. Cai et al. [20] proposed a multi-stage\n13140 VOLUME 10, 2022\nY. Fanet al.: Object Detection Algorithm for Rotary-Wing UAV Based on AWin Transformer\nobject detection algorithm Cascade R-CNN, which cascades\nmultiple RCNN networks and continuously optimizes the\ndetection results.\nThe above algorithms have contributed to the devel-\nopment of object detection algorithms, but the extrac-\ntion of contextual information of UA V objects is not\nsufﬁcient.\nB. DETECTION ALGORITHMS BASED ON TRANSFORMER\nNETWORK\nAshish Vaswani et al.[24] proposed the Transformer struc-\nture that integrates the multi-head self-attention mechanism\nin the encoder and decoder. This structure achieved good\nresults in the ﬁeld of natural language processing. Com-\npared with the traditional CNN network, the Transformer\nstructure has a stronger ability to learn contextual features.\nX. Chen et al.[25] proposed ViT (vision Transformer) and\nused Transformer to solve image classiﬁcation problems.\nThe image is divided into ﬁxed-size packages, and then the\npackage embedding vector is processed by a linear transfor-\nmation. After the package embedding vector of the image\nis input into Transformer, the encoder is used to extract\nfeatures. N. Carion et al.[26] proposed DETR and applied\nTransformer to the object detection problem. DETR uses\nCNN and Transformer as the main body to establish a hybrid\nalgorithm. First, the CNN algorithm is used to extract the\nfeatures. Then, the position is decoded and input into the stan-\ndard Transformer decoder. Finally, according to the output of\nthe Transformer decoder, the object type and detection frame\nare estimated. Zhu et al.[27] proposed Deformable DETR to\nreduce a large amount of calculation and memory usage of\nDETR during training. It uses the feature map after attention\ncalculation for training and improves the extraction method of\nthe key vector and the generation method of the contribution\nmap. The training speed of Deformable DETR is 10 times\nfaster than that of DETR. Liu et al.[28] proposed Swin Trans-\nformer, which can be used as a general backbone network for\ncomputer vision. Also, they proposed a Transformer method\nof hierarchical representation which uses a sliding window\nto perform self-attention calculations. This hierarchical struc-\nture has the ﬂexibility of modeling at different scales, and\nit has linear computational complexity relative to the single-\npixel size. Dong et al.[29] proposed a new visual Transformer\nstructure based on the CSWin Transformer. They improved\nthe attention mechanism and proposed a cross-shaped win-\ndow to calculate self-attention. Besides, local enhanced\nposition coding was investigated. This structure achieves\nbetter performance than Swin Transformer and has fewer\nparameters.\nAs for the current studies on the transformer network,\nthe self-attention mechanism is designed for general\nobjects and does not make full use of the symmetry\ncharacteristics of rotary-wing UA Vs. The speciﬁc self-\nattention mechanism needs to be investigated for rotary-wing\nUA Vs.\nC. OBJECT DETECTION ALGORITHMS FOR UAV OPTICAL\nIMAGES BASED ON DEEP LEARNING\nMuhammad Saqib et al.[10] compared convolutional neural\nnetworks such as ZF and VGG through experiments. The\nexperimental results showed that the convolutional neural\nnetwork is effective for drone detection and can effectively\ndistinguish between drones and birds. Eren Unlu et al.[11]\nproposed an autonomous drone detection system, which uses\na static wide-angle lens and a reversible low-angle lens.\nAlso, they adopted a combined multi-frame deep learning test\nmethod. Besides, the initial detection of small air intruders on\nthe main image plane and the detection on the zoomed image\nplane were performed simultaneously, which minimizes the\ncost of resource-exhaustion detection algorithms. Vasileios\nMagoulianitis et al.[12] proposed to employ super-resolution\ntechnology in the detection process to improve the recall\nrate. The image is magniﬁed 2 times by a super-resolution\ndepth model before it is input into the drone detector. The\nmodel is trained in an end-to-end manner to take full advan-\ntage of the joint optimization effect. Experimental results\nproved that the recall rate of the detector is improved. Eren\nUnlu et al. [13] used two-dimensional, rotation-invariant,\nand translation-invariant general Fourier descriptor features\nto classify the object as a drone or a bird through a neu-\nral network. To train this system, a large dataset consisting\nof birds and drones was collected from open source. This\nmethod achieves a classiﬁcation accuracy of up to 85.3%.\nMACIEJ et al.[14] proposed a new UA V image dataset and a\nsemi-automatic labeling method for the dataset. Meanwhile,\nthey designed a high-performance detection model based on\na deep neural network. Dongkyu Lee et al. [15] introduced\na comprehensive drone detection system based on machine\nlearning. The system infers the location and model of the\ndrone image based on the camera image and machine classiﬁ-\ncation. Han Sun et al.[16] proposed a UA V detection network\nwith a small iterative backbone. The integration of the spatial\nattention module into the network backbone to emphasize the\ninformation of small objects can better locate small drones\nand further improve detection performance.\nCurrent research on UA V object detection algorithms for\nimage data sources is insufﬁcient. In addition, the detection\nnetwork design is relatively simple, and the context infor-\nmation of UA V objects is not fully utilized. In this paper,\ncombining the characteristics of the rotary-wing UA V in\nthe image, a more effective feature extraction method and\ndetection algorithm are proposed.\nIII. METHOD\nIn this section, the overall architecture design of the proposed\nnetwork is introduced ﬁrst. Then, a transformer network mod-\nule based on the annular window is proposed.\nA. NETWORK ARCHITECTURE\nThe overall architecture of the network is based on\nthe hierarchical representation of the CSWin Transformer\nVOLUME 10, 2022 13141\nY. Fanet al.: Object Detection Algorithm for Rotary-Wing UAV Based on AWin Transformer\nFIGURE 1. Overall network architecture.\nnetwork [29]. It retains the hierarchical stage design and\ngradually reduces the number of image tokens in each\nstage through convolution operations. In this way, the out-\nput of each stage has the same feature map resolution as a\ntypical convolutional neural network. This design has two\nadvantages. One advantage is that only partial correlation\ninformation is processed for the image, thereby reducing the\ncalculation amount of the attention mechanism. The other\nadvantage is that hierarchical detection and recognition of\nobjects of different sizes is performed, thereby improving the\ndetection accuracy of the entire image.\nA transformer network based on the annular window\n(AWin Transformer) is designed to improve the multi-head\nself-attention mechanism based on the sliding window. Also,\na multi-head self-attention mechanism based on the annular\nwindow is adopted to calculate the attention weights. This\nmechanism enables the network to better extract the charac-\nteristics of rotary-wing UA Vs, which will be described in the\nnext section. The transformer network consists of four stages,\nand a different number of AWin Transformer modules are\nused in each stage to better adapt to the characteristics of each\nstage.\nThe overall framework of the transformer network based\non the annular window is shown in Fig. 1. The input is an\nimage with a size of H ×W ×3, and the transformer network\nis input after convolutional embedding transformation [34].\nThe convolutional embedding transformation passes through\na convolutional layer with a convolution kernel of 7 ×7 and\na stride of 5. Therefore, the number of input tokens of the\ntransformer network is H/4×W /4, and the channel dimension\nis C. The transformer network consists of 4 stages, and each\nstage is composed of Ni transformer network modules based\non annular windows. A convolution kernel of 3 ×3 and a\nstride of 2 are used between two adjacent stages to reduce the\nnumber of picture tokens in the image, thus increasing the\nresolution of the image and doubling the channel dimension.\nIn Fig. 1, the number at the top of each stage indicates the\nnumber of tokens and the channel dimension of the output\nfeature map at this stage. Taking the ﬁrst stage as an exam-\nple, the number of picture tokens at this stage is H/4×W /4,\nand the channel dimension is C. Then, after each stage, the\nnumber of tokens is reduced to 1/4 of the previous stage, and\nthe channel dimension is increased to 2 times of the previous\nstage.\nB. TRANSFORMER NETWORK MODULE BASED ON THE\nANNULAR WINDOW\nIn this paper, a transformer network module based on the\nannular window was designed, and its structure is shown in\nFig. 2.\nFIGURE 2. The transformer network structure based on the annular\nwindow.\nIn the transformer network module based on the annular\nwindow, the standard multi-head self-attention (MSA) mod-\nule is replaced with the multi-head self-attention based on\nthe annular window (AWin SA) module, and the other layers\nremain unchanged. As shown in Fig. 2, the AWin Transformer\nmodule consists of a multi-head self-attention module based\non the annular window (AWin SA) and a multi-layer per-\nceptron (MLP) with a GELU (Gaussian Error Linear Units)\nnonlinear layer. Layer normalization (LN, Layer Norm) is\nperformed before the AWin SA module, and the output is\nconnected to the initial input using the residual connection\nmethod. The MLP module operates in the same way.\nIn the AWin Transformer module, the function of the\nGELU layer is to perform nonlinear activation and random\nregularization changes on the input of the neural network.\nThe function of the LN layer is to normalize the input of all\nneurons in a certain layer of the deep network.\nIV. DESIGN OF SELF-ATTENTION MECHANISM BASED\nON ANNULAR WINDOW\nIn this section, the self-attention mechanism based on the\nannular window is described. The image of the rotary-wing\nUA V has the characteristics of approximate symmetry and\nconcentration in the object area. In other words, a single\nUA V instance generally occupies a local area within a certain\nannulus in the image rather than the global or long strip\narea of the image. To this end, this paper proposes a self-\nattention mechanism based on annular windows, which can\nperform better than the mobile sliding window design in Swin\nTransformer and the cross-shaped sliding window design in\nCSWin Transformer for rotary-wing UA V detection.\nA. SELF-ATTENTION MECHANISM BASED ON THE\nANNULAR WINDOW\nThe standard multi-head self-attention mechanism has strong\ncontext correlation capabilities, but the complexity of the\nalgorithm is squared with the size of the feature map. There-\nfore, this paper adopts the idea of self-attention calculation\nin the local window and designs a self-attention mechanism\n13142 VOLUME 10, 2022\nY. Fanet al.: Object Detection Algorithm for Rotary-Wing UAV Based on AWin Transformer\nbased on the annular window. A schematic of this mechanism\nis shown in Fig. 3.\nFIGURE 3. Schematic diagram of self-attention mechanism based on the\nannular window.\nDenote the query vector of the object to be detected as Q,\nwhich corresponds to the red dotted area in Fig. 3; denotes the\nkey vector as K, which corresponds to the blue annular area in\nthe ﬁgure; denotes the input feature map as X, X ∈RH×W ×C .\nH, W ∈ R+, C ∈ N+, where H and W are the height\nand width of the input feature image, respectively, and C is\nthe dimension of the input feature image. Assuming that the\nwidth of the annular area is rw and the number of annular\nareas is d, then the interval between the annular area and\nthe query vector is (d −1) ·rw, which is the distance from\nthe annular area to the red dot in the graph. Fig. 3 shows the\nchanges in the annular area when rw =1 and d =3. Based\non this, it can be calculated that the number of tokens in each\nannular area is:\nNtoken =\n{\n8(d −1)(rw)2, d ≥2\n(rw)2, d =1 (1)\nAs shown in Fig. 3, the two rows show the value area of\nthe key used in the attention calculation of the annular sliding\nwindow in the AWin Transformer for different query points.\nThe input image is segmented in the form of a set of annular\nsliding windows. When the position of the query point is\ndetermined, according to the preset parameters, the width of\nthe ring window rw and the serial number d of the annular\nwindow. The input image is divided to generate d annular\nwindow regions that do not overlap with each other. In Fig. 3,\nthe red dot represents the position of the query point, and the\nannular area composed of blue squares represents the annular\nsliding window area corresponding to the key value.\nAs shown in Fig. 3, the ﬁrst row shows the change in the\nannular sliding window area with the sequence number d\nwhen the query point is located in the central area. The second\nline shows the change in the annular sliding window area with\nthe sequence number d when the query point is located near\nthe upper-left corner of the image.\nBy adjusting the width rw of the annular area and the\nnumber d of the annular area, a balance can be achieved\nbetween the computational complexity of the model and the\nlearning ability of the model. As the number of stages in\nthe network increases, the maximum width of the annular\narea (2d −1) ·rw increases to associate with more areas.\nIn particular, if the query vector is at the edge of the feature\nmap, its annular window may exceed the range of the feature\nmap. The excess part can be ﬁlled with tokens with zero\nelements to ensure the uniformity of the calculation form with\nno additional calculations.\nThe input feature image X can be divided into N\nannular regions that do not overlap each other, i.e.,\nX = [X1, X2, . . . ,XN ], where the superscript N =\nmax(W /rw, H/rw). Assuming that the query, key, and value\nvector projection matrix of the k-th head of the multi-\nattention mechanism have the dimensions of dk , the self-\nattention weight of the annular area of the k-th head is deﬁned\nas:\nheadk =Attention(XiW Q\nk , XiW K\nk , XiW V\nk ), i =1, . . . ,N\n(2)\nwhere W Q\nk ∈ RC×dk , W K\nk ∈ RC×dk , and W V\nk ∈ RC×dk\nrepresent the projection matrix of the query, key, and value\nvectors of the k-th head, respectively. Setting dk to C/K, the\nself-attention can be calculated as:\nAttention(Q, K, V ) =SoftMax(QKT\n√dk\n)V (3)\nThe attention weights of different annular areas in the K\nheads of the AWin SA module are calculated, and then calcu-\nlation results are concatenated as the output of the attention\nweights of the entire module. The speciﬁc formula is as\nfollows:\nAWinAttention(X) =Concat(head1, head2, . . . ,headk )W O\n(4)\nIn Eq. 4, W O ∈ RC×C is a commonly used projection\nmatrix. It is used to project the self-attention result to the\ndimension of the object output, and its value is generally set\nto C by default.\nThe calculation of the attention weights of the K heads of\nthe AWin SA module is performed in parallel. The attention\nweights of all tokens in the entire feature map can be calcu-\nlated after concatenation.\nThe output of the attention weight AWinAttention(X) is\ncalculated through the AWin SA module. The transformer\nmodule based on the annular sliding window (as shown in\nFig. 2) can be expressed as:\nˆXl =AWinAttention(LN(Xl−1)) +Xl−1 (5)\nXl =MLP(LN( ˆXl )) +ˆXl (6)\nwhere LN and MLP represent the output after the LN layer\nand the MLP layer, respectively; ˆXl represents the output after\nthe AWin SA module in the l-th AWin Transformer module;\nXl represents the output of the l-th AWin Transformer mod-\nule, where the superscript l ={1, . . . ,Ni}, i ={1, 2, 3, 4}\nrepresents the value of l in each stage ranging from 1 to Ni\nin the four stages of the network. In particular, X0 represents\nVOLUME 10, 2022 13143\nY. Fanet al.: Object Detection Algorithm for Rotary-Wing UAV Based on AWin Transformer\nthe input of the ﬁrst AWin Transformer module in each stage,\ni.e., the output of the previous convolutional layer.\nThe above calculation formula is derived under the premise\nof ignoring the position code. However, since there is no posi-\ntional relationship in the self-attention calculation, impor-\ntant positional information of the image may be ignored.\nTo address this issue, different position coding mechanisms\nare used in the existing visual Transformer.\nSpeciﬁcally, APE [35] and CPE [36] add location infor-\nmation to the input tokens before they are input to the\nTransformer module, while RPE [37] and LePE [29] merge\nlocation information into each converter block. In this paper,\nthe LePE position code with the best performance is used.\nAssuming that the edge between the value elements vi and vj\nis a vector eV\nij ∈E, the self-attention calculation formula is:\nAttention(Q, K, V ) =SoftMax(QKT\n√\nD )V +EV (7)\nB. COMPUTATIONAL COMPLEXITY ANALYSIS AND\nNETWORK PARAMETER SETTINGS\nThe computational complexity of the proposed method is\nanalyzed. First, the computational complexity of the multi-\nhead self-attention mechanism module based on the annular\nwindow is expressed as follows:\n(AWinAttention)=4HWC2 +HWC ×[(2d −1) ·rw]2\n(8)\nThe complexity of the standard Transformer’s multi-\nattention mechanism is expressed as:\n(MSA) =4HWC2 +2(HW )2C (9)\nFIGURE 4. Comparing the computational complexity of the self-attention\nmechanism based on the annular window.\nA comparison of the calculation complexity of the two\nmechanisms is shown in Fig. 4. It can be seen that the annular\narea (effective calculation part) is a part of the feature map,\nso the area of the annular area must be less than or equal to\nthe area of the feature map, that is\n[(2d −1) ·rw]2 ≤HW < 2HW (10)\nTherefore, the computational complexity of the multi-head\nself-attention mechanism module based on the annular win-\ndow is less than that of the multi-attention mechanism of the\nstandard Transformer, i.e.,\n(AWinAttention)< (MSA) (11)\nThe above analysis indicates that our proposed AWin SA\nmodule has a lower computational complexity than the stan-\ndard MSA module. The speciﬁc computational complexity is\nrelated to the total area of the annular area [(2d −1) ·rw]2,\nthat is, the parameter settings of the width of the annular area\nrw and the number of annular areas d. Deﬁning dmax as the\nmaximum number of annular areas, and the side length of the\nlargest annular area can then be deﬁned as (2d max −1) ·rw.\nAs the number of stages in the transformer network increases\nwith the resolution of the feature map, the value of rw max can\nbe increased appropriately to meet the receptive ﬁeld of the\nobject.\nTABLE 1. Specific parameter settings in AWin transformer.\nTABLE 2. Comparison of specific network parameter settings.\nNext, the parameter setting of the AWin Transformer net-\nwork is investigated. For deep neural networks, more network\nparameters usually lead to stronger learning ability and ﬁtting\nability, as well as better detection performance. Therefore, the\nperformance of the algorithm should be compared under a\nsimilar number of parameters to avoid the inﬂuence of param-\neters on the network performance and observe the inﬂuence\nof the different network structures on the performance of the\nalgorithm.\nTo compare the performance of the existing transformer-\nbased object detection algorithm with the same level of\nparameters, the AWin Transformer network is designed to\nhave a similar number of parameters to the Swin-S network\nproposed in Swin Transformer. The speciﬁc parameter set-\ntings of the AWin Transformer network are listed in Table 1.\nThe AWin Transformer network is compared with other\ntransformer backbone networks under a similar number of\nparameters on the ImageNet-1K dataset. Using 224 2 images\nas input, the calculated FLOPs of the two networks are listed\nin Table 2.\n13144 VOLUME 10, 2022\nY. Fanet al.: Object Detection Algorithm for Rotary-Wing UAV Based on AWin Transformer\nV. EXPERIMENTS\nIn this paper, the MS COCO 2017 dataset [40] is chosen as\nthe benchmark. The trainval35k training set and the minival\nvalidation set are used for algorithm training and testing,\nrespectively. The algorithm is implemented with the Python\nlanguage and runs on a Windows platform equipped with an\nRTX 3090 GPU for training. The AdamW optimizer [41] is\nused for the training. The initial learning rate is set to 10 −4.\nThe weight decay is set to 0.05, the batch size is set to 8, and\nthe training is performed in 36 epochs.\nOn the COCO dataset, the accuracy of the proposed algo-\nrithm and typical object detection algorithms is veriﬁed.\nSubsequently, the effectiveness of the proposed algorithm\nis veriﬁed on the multi-rotor UA V dataset presented in this\npaper. Finally, an ablation experiment is conducted to test the\nvalidity of each part of the calculation.\nIn existing drone object detection algorithms, most algo-\nrithms are designed based on the CNN network architecture.\nBecause these works do not give all the code in python, the\ntypical CNN algorithms are used as alternatives for compari-\nson. In Table 3, we list three representative algorithms, Faster\nR-CNN, RetinaNet and FCOS algorithms based on a CNN\nnetwork structure, and four algorithms based on transformers,\nsuch as DETR, Deformable DETR, Swin Transformer and\nCSWin Transformer algorithms. The algorithms are com-\npared with the detection performance of our proposed algo-\nrithm on the COCO dataset.\nTABLE 3. Comparison of accuracy with typical object detection\nalgorithms on the COCO dataset.\nTypical object detection algorithms are used for perfor-\nmance comparison, including Faster R-CNN [30], Reti-\nnaNet [32], FCOS [33], DETR [26], Deformable DETR [27],\nand Cascade Mask R-CNN [20]. Among them, Faster\nR-CNN [30], RetinaNet [32], FCOS [33], DETR [26], and\nDeformable DETR [27] algorithms use ResNet-101 net-\nwork [42] as the backbone; the Cascade Mask R-CNN [20]\nuses Swin-S network [28] and CSWin-S network [29] as the\nbackbone.\nThe performances of the object detection algorithms on the\nCOCO dataset are listed in Table 3. As shown in Table 3,\ncompared with the Faster R-CNN algorithm, the mean aver-\nage precision (mAP) of our proposed algorithm is improved\nby 15.8%. This is because the proposed algorithm uses\na transformer structure. Compared with the CNN network\nstructure, this structure has a stronger ability to learn the\ncontext features of the image, and can better extract the target\nfeatures and distinguish the target from the non-target. The\noccurrence of missed alarms and false alarms is reduced,\nthereby improving the detection accuracy.\nAt the same time, we also compared the performance of\nsome current excellent object detection algorithms based on\nthe transformer structure. As shown in Table 3, compared\nwith the DETR and Deformable DETR algorithms, our algo-\nrithm achieved 10.3% and 6.5% mAP improvements, respec-\ntively. As for the Cascade Mask R-CNN detection algorithm,\nthe use of AWin as the backbone contributes to 3.4% and\n1.5% better mAP than Swin-S and CSWin-S respectively.\nThis is because the proposed algorithm takes advantage of\nthe strong symmetry and the concentrated image area of the\nrotary-wing UA V. In addition, we design an AWin Trans-\nformer structure, which has a better detection effect on the\nrotary-wing UA V, so the detection accuracy is improved.\nMeanwhile, this paper proposes a Rotor-Drone dataset for\nimage detection of UA Vs. The dataset consists of 10,000\nimages of rotary-wing drones. The dataset is randomly\ndivided into a training set, veriﬁcation set, and test set at a\nratio of 7:1:2. These images are obtained from the Internet\nand extracted from the videos. All examples of rotary-wing\nUA Vs in the picture are labeled manually.\nIn Table 4, we tested the proposed algorithm with Faster\nR-CNN, RetinaNet, FCOS, Swin Transformer, and CSWin\nTransformer algorithms on the Rotor-Drone dataset. The met-\nrics of the average precision of different IoU thresholds and\nthe FPS are tested on the test dataset of the Rotor-Drone\ndataset. It can be seen that our proposed algorithm obtains\na higher mAP than the comparative algorithms. Compared\nwith Faster R-CNN and RetinaNet, the proposed algorithm\nimproves the mAP by 11.1% and 9.0%, respectively. Com-\npared with Cascade Mask R-CNN that uses Swin-S and\nCSWin-S as the backbone network, the proposed algorithm\nimproves the mAP by 3.0% and 1.7%, respectively. This is\nconsistent with the detection results of the COCO dataset.\nThe experimental results on the Rotor-Drone dataset further\nveriﬁed the effectiveness of our algorithm.\nAs shown in Table 3 and Table 4, compared with the\nCNN-based object detection algorithms, the proposed algo-\nrithm has a lower FPS, but the detection accuracy is obviously\nhigher. Meanwhile, compared with the detection algorithms\nthat used transformers as backbone networks, the inference\ntime is closed.\nAs shown in Fig. 5, the change in the loss during the\ntraining process of our proposed algorithm based on the AWin\nVOLUME 10, 2022 13145\nY. Fanet al.: Object Detection Algorithm for Rotary-Wing UAV Based on AWin Transformer\nTABLE 4. Comparison of accuracy with typical object detection\nalgorithms on the Rotor-Drone dataset.\nFIGURE 5. The curve of the loss of the AWin Transformer algorithm with\nthe number of training iterations.\nTransformer is represented. The number of training epochs is\nset to 36. As shown in the ﬁgure, as the number of training\nepochs of the network increases, the loss of the network\ngradually decreases, which means that the detection ability\nof the network is also on the rise. In other words, the grater\nthe number of epochs of training, the stronger the detection\nability of the network.\nAs shown in Fig. 6, the curve results of AP 0.5 of the\nsix detection algorithms with the number of training epochs\nare shown. Among them, the number of training epochs of\nFaster R-CNN, RetinaNet and FCOS algorithms is set to 12,\nand the number of training epochs of Swin Transformer,\nCSWin Transformer and our proposed AWin Transformer\nalgorithm is set to 36. It can be seen that as the number of\ntraining epochs increases, the detection accuracy of various\nalgorithms gradually increases and tends to be stable. Among\nthe three detection algorithms based on CNN, FCOS (green\nsolid line shown in the ﬁgure) has a better detection per-\nformance. Comprehensively considering the comparison of\nthe six algorithms, our proposed algorithm (brown solid line\nshown in the ﬁgure) achieves the best detection accuracy.\nFIGURE 6. Comparison on the curve of the AP0.5 with the number of\ntraining epochs tested on the validation set of Rotor-Drone dataset.\nFIGURE 7. Comparison on the PR (precision-recall) curve on the test set\nof Rotor-Drone dataset.\nThe PR curve results of our algorithm and the comparative\nalgorithms on the Rotor-Drone test set are shown in Fig. 7.\nThe ﬁgure shows the curve of precision varying with the\nrecall of each algorithm when IoU is 0.5. It can be seen\nfrom the ﬁgure that the value range of precision and recall is\nbetween 0 and 1. As the recall increases, precision decreases.\nThe area enclosed by the PR curve and X-Y axis represents\nthe average precision. The larger the area, the higher the\naverage precision of the algorithm. It is obvious from the\nﬁgure that our algorithm (brown solid line) has the largest\nenclosed area between the axes and the PR curve; that is, the\nproposed algorithm achieves the best detection accuracy.\nTo verify the effectiveness of the components of the pro-\nposed algorithm, an ablation experiment is performed on\nthe Rotor-Drone dataset using Mask R-CNN as the default\ndetection algorithm. The proposed self-attention mechanism\nbased on the annular window is compared with the existing\nself-attention mechanisms, including shifted window self-\nattention [28], spatially separable self-attention [39], and\ncross-shaped window self-attention mechanism [29]. To bet-\nter compare the performance of the self-attention mechanism\n13146 VOLUME 10, 2022\nY. Fanet al.: Object Detection Algorithm for Rotary-Wing UAV Based on AWin Transformer\nand exclude other inﬂuencing factors as much as possi-\nble, the same non-overlapping image element embedding\nmethod [38] and RPE position coding [37] are used for all\nthe algorithms.\nTABLE 5. Ablation study on self-attention mechanism selection.\nIt can be seen from Table 5 that our proposed attention\nmechanism based on the annular window achieves better\nmAP than the other three self-attention mechanisms. Com-\npared with the self-attention mechanism based on the cross-\nshaped window, our proposed attention mechanism improves\nthe mAP by 1.6%, indicating that it can better extract the\ncontextual feature information of the rotary-wing UA V for\nUA V object detection.\nTABLE 6. Ablation study the components of the proposed algorithm.\nNext, ablation studies are performed on the components of\nthe proposed algorithm. The experiments are performed on\nthe Rotor-Drone dataset, with Mask R-CNN as the default\ndetection algorithm. In our proposed detection algorithm\nbased on the AWin Transformer, the number of AWin Trans-\nformer modules Ni in each stage, the annular area rw in the\nAWin self-attention mechanism, and the maximum number\nof annular areas dmax are all assigned different values in\ndifferent stages. Thus, the algorithm can adapt to the object\nreceptive ﬁeld as the number of stages increases and detect\nthe object better. Table 6 lists the results of the ablation exper-\niments on the three parameters, where one of the parameters\nis set to a ﬁxed value. It can be seen that when the number\nof AWin Transformer modules Ni in each stage is ﬁxed, the\nmAP of the algorithm decreases the most. This is because\nthe depth of the network is not large enough, which reduces\nthe learning ability of the network and leads to the failure of\nobject detection. When the annular area width rw is ﬁxed,\nthe mAP of the algorithm decreases. This is because as the\nstage passes, the annular area cannot completely cover the\nreceptive ﬁeld of the object, which leads to a decrease in\nthe detection accuracy. Similarly, a similar result is obtained\nby ﬁxing the maximum number of annular areas dmax, but\nthe average detection accuracy is slightly higher than that of\nﬁxing rw. This is because ﬁxing dmax makes fewer changes\nagainst the ﬁxed rw compared with the original design. The\nabove results show that the design of the components in our\nalgorithm is effective.\nMoreover, there are many typical scenes in which rotary-\nwing UA Vs are difﬁcult to detect, such as dense and large\nnumbers of UA Vs, blurred objects, low resolution, and high\nsimilarity between foreground and background. To demon-\nstrate the advantages of our proposed algorithm compared\nwith existing algorithms, some visual object detection results\nof rotary-wing UA Vs are presented.\nExisting image object detection algorithms for rotary-wing\nUA Vs are mainly based on the CNN framework. There-\nfore, we compare the detection effect ﬁgures between the\nFOCS-based UA V detection algorithm and the rotary-wing\nUA V detection algorithm based on the AWin Transformer\nproposed by us.\nFIGURE 8. Comparison of detection results of overlapping object instance\ndetection boxes.\nAs shown in Fig. 8, the detection results of the overlapping\nobject instance detection boxes are compared. It can be seen\nthat the FCOS has a poor detection effect (shown in the upper\npart in Fig. 8), and multiple drones are mistakenly detected\nas one drone. Some drones are not detected. The proposed\nalgorithm can effectively detect every UA V instance (shown\nin the lower part in Fig. 8), and the detection accuracy is\nhigher.\nAs shown in Fig. 9, the detection results for the instances\noverlapping and fuzzy owing to out-of-focus are represented.\nIt can be seen that the FCOS algorithm (shown in the upper\npart in Fig. 9) only detected the ﬁrst UA V, and missed the\nVOLUME 10, 2022 13147\nY. Fanet al.: Object Detection Algorithm for Rotary-Wing UAV Based on AWin Transformer\nFIGURE 9. Comparison of detection results of overlapping and fuzzy\nobjects.\nsubsequent UA Vs. Our proposed algorithm (shown in the\nlower part in Fig. 9) can detect every UA V instance.\nFIGURE 10. Comparison of detection results of confusing foreground and\nbackground.\nAs shown in Fig. 10, the detection results of the confusion\nforeground and background are represented. It can be seen\nthat, in the upper part in Fig. 10, the UA V in the middle of\nthe ﬁgure is similar to the color of the building in the back-\nground, and the FCOS algorithm make a missing detection.\nMeanwhile, our proposed algorithm (shown in the lower part\nin Fig. 10) can accurately detect UA Vs.\nFIGURE 11. Comparison of detection results with small and fuzzy objects.\nAs shown in Fig. 11, the detection results for small and\nfuzzy objects are presented. It can be seen that it is difﬁcult\nfor the FCOS algorithm (shown in the upper part in Fig. 11)\nto detect all the UA V instances, and the detection accuracy\nis low. Meanwhile, the algorithm we proposed (shown in the\nlower part in Fig. 11) can detect all UA V instances with higher\ndetection accuracy, but there is a problem of misdetection at\nthe same time.\nIt can be seen that in scenes where the detection of rotary-\nwing UA Vs is difﬁcult, such as dense and large numbers\nof targets, blurred targets, and high similarity between the\ntargets and the background, our algorithm can better detect\ndrone targets and reduce false alarms and missed alarms for\ndrones. This is because the proposed algorithm can better\nestablish a contextual connection between the drone target\nand the entire picture, so the ability to learn the characteristics\nof the drone is stronger, resulting in a better detection effect.\nVI. CONCLUSION\nThis paper proposes a UA V object detection algorithm\nbased on the annular window transformer network. The self-\nattention mechanism is used to combine the local contextual\ninformation to extract the features of the rotary-wing UA V\nmore effectively, thereby improving the accuracy of object\n13148 VOLUME 10, 2022\nY. Fanet al.: Object Detection Algorithm for Rotary-Wing UAV Based on AWin Transformer\ndetection. Meanwhile, a new self-attention mechanism is pro-\nposed. The query vector and the key vector of the surrounding\nannular area are calculated separately, and the calculation\nresults are concatenated. Experimental results show that com-\npared with the existing detection algorithm, the proposed\nalgorithm increases the mean average precision by 1.7% on\nthe proposed rotary-wing UA V dataset. However, there are\nstill many improvements in our work. On the one hand, the\nreal-time performance of the algorithm needs to be further\nimproved. On the other hand, although we collected the var-\nious application scenarios of rotary-wing UA Vs as much as\npossible in the dataset, the data are also not rich enough to\naddress the detection challenges in some special scenarios,\nsuch as densely distributed and blocked rotary-wing UA V\ngroups. Future work will involve collecting and producing\ncorresponding datasets for the speciﬁc challenges of detec-\ntion difﬁculties, in order to solve the detection difﬁculties of\nrotary-wing UA Vs more practically in actual challenges.\nREFERENCES\n[1] S. Samaras, E. Diamantidou, D. Ataloglou, N. Sakellariou, A. Vafeiadis,\nV. Magoulianitis, A. Lalas, A. Dimou, D. Zarpalas, K. Votis, P. Daras,\nand D. Tzovaras, ‘‘Deep learning on multi sensor data for counter UA V\napplications—A systematic review,’’ Sensors, vol. 19, no. 22, p. 4837,\nNov. 2019.\n[2] A. Coluccia, G. Parisi, and A. Fascista, ‘‘Detection and classiﬁcation of\nmultirotor drones in radar sensor networks: A review,’’ Sensors, vol. 20,\nno. 15, p. 4172, Jul. 2020.\n[3] B. Taha and A. Shoufan, ‘‘Machine learning-based drone detection\nand classiﬁcation: State-of-the-art in research,’’ IEEE Access, vol. 7,\npp. 138669–138682, 2019, doi: 10.1109/ACCESS.2019.2942944.\n[4] S. Park, H. T. Kim, S. Lee, H. Joo, and H. Kim, ‘‘Survey on anti-drone\nsystems: Components, designs, and challenges,’’ IEEE Access, vol. 9,\npp. 42635–42659, 2021, doi: 10.1109/ACCESS.2021.3065926.\n[5] F. Jiang, K. Wang, L. Dong, C. Pan, W. Xu, and K. Yang, ‘‘Deep-learning-\nbased joint resource scheduling algorithms for hybrid MEC networks,’’\nIEEE Internet Things J., vol. 7, no. 7, pp. 6252–6265, Jul. 2020, doi:\n10.1109/JIOT.2019.2954503.\n[6] F. Jiang, K. Wang, L. Dong, C. Pan, W. Xu, and K. Yang, ‘‘AI driven\nheterogeneous MEC system with UA V assistance for dynamic environ-\nment: Challenges and solutions,’’ IEEE Netw., vol. 35, no. 1, pp. 400–408,\nDec. 2020, doi: 10.1109/MNET.011.2000440.\n[7] F. Jiang, L. Dong, K. Wang, K. Yang, and C. Pan, ‘‘Distributed\nresource scheduling for large-scale MEC systems: A multi-agent ensemble\ndeep reinforcement learning with imitation acceleration,’’ IEEE Internet\nThings J., early access, Sep. 20, 2021, doi: 10.1109/JIOT.2021.3113872.\n[8] G. E. Hinton and R. R. Salakhutdinov, ‘‘Reducing the dimensionality of\ndata with neural networks,’’ Science, vol. 313, no. 5786, pp. 504–507,\n2006.\n[9] Y. LeCun, Y. Bengio, and G. Hinton, ‘‘Deep learning,’’ Nature, vol. 521,\nno. 7553, pp. 436–444, May 2015.\n[10] M. Saqib, S. Daud Khan, N. Sharma, and M. Blumenstein, ‘‘A study on\ndetecting drones using deep convolutional neural networks,’’ in Proc. 14th\nIEEE Int. Conf. Adv. Video Signal Based Surveill. (AVSS), Aug. 2017,\npp. 1–5, doi: 10.1109/A VSS.2017.8078541.\n[11] E. Unlu, E. Zenou, N. Riviere, and P.-E. Dupouy, ‘‘Deep learning-based\nstrategies for the detection and tracking of drones using several cameras,’’\nIPSJ Trans. Comput. Vis. Appl., vol. 11, no. 1, pp. 1–13, Dec. 2019.\n[12] V. Magoulianitis, D. Ataloglou, A. Dimou, D. Zarpalas, and P. Daras,\n‘‘Does deep super-resolution enhance UA V detection?’’ inProc. 16th IEEE\nInt. Conf. Adv. Video Signal Based Surveill. (AVSS), Sep. 2019, pp. 1–6,\ndoi: 10.1109/A VSS.2019.8909865.\n[13] E. Unlu, E. Zenou, and N. Riviere, ‘‘Using shape descriptors for UA V\ndetection,’’Electron. Imag., vol. 9, pp. 1–5, Jan. 2018.\n[14] M. L. Pawelczyk and M. Wojtyra, ‘‘Real world object detection dataset\nfor quadcopter unmanned aerial vehicle detection,’’ IEEE Access, vol. 8,\npp. 174394–174409, 2020, doi: 10.1109/ACCESS.2020.3026192.\n[15] D. Lee, W. Gyu La, and H. Kim, ‘‘Drone detection and identiﬁ-\ncation system using artiﬁcial intelligence,’’ in Proc. Int. Conf. Inf.\nCommun. Technol. Converg. (ICTC), Oct. 2018, pp. 1131–1133, doi:\n10.1109/ICTC.2018.8539442.\n[16] H. Sun, J. Yang, J. Shen, D. Liang, L. Ning-Zhong, and H. Zhou, ‘‘TIB-Net:\nDrone detection network with tiny iterative backbone,’’ IEEE Access,\nvol. 8, pp. 130697–130707, 2020, doi: 10.1109/ACCESS.2020.3009518.\n[17] Z. Zou, Z. Shi, Y. Guo, and J. Ye, ‘‘Object detection in 20 years: A survey,’’\n2019, arXiv:1905.05055.\n[18] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao,\nC. Xu, Y. Xu, Z. Yang, Y. Zhang, and D. Tao, ‘‘A survey on vision\nTransformer,’’ 2020, arXiv:2012.12556.\n[19] T.-Y. Lin, P. Dollar, R. Girshick, K. He, B. Hariharan, and S. Belongie,\n‘‘Feature pyramid networks for object detection,’’ in Proc. IEEE Conf.\nComput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 936–944, doi:\n10.1109/CVPR.2017.106.\n[20] Z. Cai and N. Vasconcelos, ‘‘Cascade R-CNN: Delving into high quality\nobject detection,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\nJun. 2018, pp. 6154–6162, doi: 10.1109/CVPR.2018.00644.\n[21] M. Tan, R. Pang, and Q. V. Le, ‘‘EfﬁcientDet: Scalable and\nefﬁcient object detection,’’ in Proc. IEEE/CVF Conf. Comput.\nVis. Pattern Recognit. (CVPR), Jun. 2020, pp. 10778–10787, doi:\n10.1109/CVPR42600.2020.01079.\n[22] S. Qiao, L.-C. Chen, and A. Yuille, ‘‘DetectoRS: Detecting objects with\nrecursive feature pyramid and switchable atrous convolution,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2021,\npp. 10208–10219, doi: 10.1109/CVPR46437.2021.01008.\n[23] G. Ghiasi, T.-Y. Lin, and Q. V. Le, ‘‘NAS-FPN: Learning scalable fea-\nture pyramid architecture for object detection,’’ in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 7029–7038, doi:\n10.1109/CVPR.2019.00720.\n[24] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. 31st Int.\nConf. Neural Inf. Process. Syst., 2017, pp. 6000–6010.\n[25] X. Chen, H. Fan, R. Girshick, and K. He, ‘‘Improved baselines with\nmomentum contrastive learning,’’ 2020, arXiv:2003.04297.\n[26] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, ‘‘End-to-end object detection with Transformers,’’ in Proc.\nECCV, Aug. 2020, pp. 213–229.\n[27] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, ‘‘Deformable DETR:\nDeformable Transformers for end-to-end object detection,’’ in Proc. ICLR,\n2021, pp. 1–16.\n[28] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, ‘‘Swin\nTransformer: Hierarchical vision Transformer using shifted windows,’’ in\nProc. ICCV, 2021, pp. 1–14.\n[29] X. Dong, J. Bao, D. Chen, W. Zhang, N. Yu, L. Yuan, D. Chen, and B. Guo,\n‘‘CSWin Transformer: A general vision Transformer backbone with cross-\nshaped Windows,’’ 2021, arXiv:2107.00652.\n[30] S. Ren, K. He, R. Girshick, and J. Sun, ‘‘Faster R-CNN: Towards real-\ntime object detection with region proposal networks,’’ IEEE Trans. Pat-\ntern Anal. Mach. Intell., vol. 39, no. 6, pp. 1137–1149, Jun. 2017, doi:\n10.1109/TPAMI.2016.2577031.\n[31] K. He, G. Gkioxari, P. Dollar, and R. Girshick, ‘‘Mask R-CNN,’’ in Proc.\nIEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017, pp. 2980–2988, doi:\n10.1109/ICCV.2017.322.\n[32] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar, ‘‘Focal loss\nfor dense object detection,’’ IEEE Trans. Pattern Anal. Mach. Intell.,\nvol. 42, no. 2, pp. 318–327, Feb. 2020, doi: 10.1109/TPAMI.2018.\n2858826.\n[33] Z. Tian, C. Shen, H. Chen, and T. He, ‘‘FCOS: Fully convolutional one-\nstage object detection,’’ inProc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV),\nOct. 2019, pp. 9626–9635, doi: 10.1109/ICCV.2019.00972.\n[34] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and\nL. Zhang, ‘‘CvT: Introducing convolutions to vision Transformers,’’ 2021,\narXiv:2103.15808.\n[35] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ 2017,\narXiv:1706.03762.\n[36] X. Chu, Z. Tian, B. Zhang, X. Wang, X. Wei, H. Xia, and C. Shen,\n‘‘Conditional positional encodings for vision Transformers,’’ 2021,\narXiv:2102.10882.\n[37] P. Shaw, J. Uszkoreit, and A. Vaswani, ‘‘Self-attention with relative posi-\ntion representations,’’ 2018, arXiv:1803.02155.\nVOLUME 10, 2022 13149\nY. Fanet al.: Object Detection Algorithm for Rotary-Wing UAV Based on AWin Transformer\n[38] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, ‘‘An image is worth 16×16 words: Trans-\nformers for image recognition at scale,’’ 2020, arXiv:2010.11929.\n[39] X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and C. Shen,\n‘‘Twins: Revisiting the design of spatial attention in vision Transformers,’’\n2021, arXiv:2104.13840.\n[40] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,\nand C. L. Zitnick, ‘‘Microsoft COCO: Common objects in context,’’ in\nProc. ECCV (Lecture Notes in Computer Science), vol. 8693. Cham,\nSwitzerland: Springer, 2014, doi: 10.1007/978-3-319-10602-1_48.\n[41] I. Loshchilov and F. Hutter, ‘‘Decoupled weight decay regularization,’’ in\nProc. Int. Conf. Learn. Represent., 2019, pp. 1–19.\n[42] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for image\nrecognition,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\nJun. 2016, pp. 770–778, doi: 10.1109/CVPR.2016.90.\n[43] R. Girshick, ‘‘Fast R-CNN,’’ in Proc. IEEE Int. Conf. Comput. Vis. (ICCV),\nDec. 2015, pp. 1440–1448, doi: 10.1109/ICCV.2015.169.\nYUNLONG FAN is currently pursuing the Ph.D.\ndegree in information and communication engi-\nneering with the Department of Information and\nCommunication Engineering, PLA Strategic Sup-\nport Force Information Engineering University,\nZhengzhou, China. His research interests include\ncomputer vision, image object detection, and small\nobject detection.\nOU LI received the Ph.D. degree from the\nNational Digital Switching System Engineering\nand Technological Research and Development\nCenter (NDSC), Zhengzhou, China, in 2001. He is\ncurrently a Professor at the NDSC. His research\ninterests include wireless communication technol-\nogy, wireless sensor networks, cognitive radio net-\nworks, MIMO, and spectrum sensing.\nGUANGYI LIU received the B.S. degree in com-\nmunication engineering from Information Engi-\nneering University, Zhengzhou, China, in 2005,\nand the M.S. and Ph.D. degrees in communication\nengineering from the National Digital Switching\nSystem Engineering and Technological Research\nand Development Center, Zhengzhou, in 2009 and\n2015, respectively. His research interests include\nwireless communications and signal analysis.\n13150 VOLUME 10, 2022",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7547061443328857
    },
    {
      "name": "Wing",
      "score": 0.6187539100646973
    },
    {
      "name": "Artificial intelligence",
      "score": 0.579703688621521
    },
    {
      "name": "Computer vision",
      "score": 0.5467537641525269
    },
    {
      "name": "Object detection",
      "score": 0.5461902618408203
    },
    {
      "name": "Transformer",
      "score": 0.45942452549934387
    },
    {
      "name": "Object (grammar)",
      "score": 0.44225913286209106
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.25871336460113525
    },
    {
      "name": "Engineering",
      "score": 0.15116456151008606
    },
    {
      "name": "Aerospace engineering",
      "score": 0.07418307662010193
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "cited_by": 8
}