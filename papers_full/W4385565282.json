{
  "title": "AliBERT: A Pre-trained Language Model for French Biomedical Text",
  "url": "https://openalex.org/W4385565282",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5042645079",
      "name": "Aman Berhe",
      "affiliations": [
        null,
        "Inserm",
        "Institut de Recherche pour le Développement",
        "Nutrition et obésité : approches systémiques",
        "Sorbonne Université",
        "Unité de Modélisation Mathématique et Informatique des Systèmes Complexes"
      ]
    },
    {
      "id": "https://openalex.org/A5004942336",
      "name": "Guillaume Draznieks",
      "affiliations": [
        null,
        "Inserm",
        "Institut de Recherche pour le Développement",
        "Nutrition et obésité : approches systémiques",
        "Sorbonne Université",
        "Unité de Modélisation Mathématique et Informatique des Systèmes Complexes"
      ]
    },
    {
      "id": "https://openalex.org/A5044128867",
      "name": "Vincent Martenot",
      "affiliations": [
        null,
        "Inserm",
        "Institut de Recherche pour le Développement",
        "Nutrition et obésité : approches systémiques",
        "Sorbonne Université",
        "Unité de Modélisation Mathématique et Informatique des Systèmes Complexes"
      ]
    },
    {
      "id": "https://openalex.org/A5024109428",
      "name": "Valentin Masdeu",
      "affiliations": [
        null,
        "Inserm",
        "Institut de Recherche pour le Développement",
        "Nutrition et obésité : approches systémiques",
        "Sorbonne Université",
        "Unité de Modélisation Mathématique et Informatique des Systèmes Complexes"
      ]
    },
    {
      "id": "https://openalex.org/A5083498034",
      "name": "Lucas Davy",
      "affiliations": [
        null,
        "Inserm",
        "Institut de Recherche pour le Développement",
        "Nutrition et obésité : approches systémiques",
        "Sorbonne Université",
        "Unité de Modélisation Mathématique et Informatique des Systèmes Complexes"
      ]
    },
    {
      "id": "https://openalex.org/A5088630634",
      "name": "Jean‐Daniel Zucker",
      "affiliations": [
        null,
        "Inserm",
        "Institut de Recherche pour le Développement",
        "Nutrition et obésité : approches systémiques",
        "Sorbonne Université",
        "Unité de Modélisation Mathématique et Informatique des Systèmes Complexes"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3161430317",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W3198722157",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W3028237516",
    "https://openalex.org/W3114551148",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3101058639",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4288099476",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W4220967417",
    "https://openalex.org/W2141599568",
    "https://openalex.org/W4385681388",
    "https://openalex.org/W3105220303",
    "https://openalex.org/W2995435108",
    "https://openalex.org/W3192478068",
    "https://openalex.org/W3214298066",
    "https://openalex.org/W3045332379",
    "https://openalex.org/W3157788795",
    "https://openalex.org/W3166508187",
    "https://openalex.org/W3199051761",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2970078680",
    "https://openalex.org/W2990847667",
    "https://openalex.org/W4287760320",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W3215431778",
    "https://openalex.org/W3034999214"
  ],
  "abstract": "Over the past few years, domain specific pretrained language models have been investigated and have shown remarkable achievements in different downstream tasks, especially in biomedical domain. These achievements stem on the well known BERT architecture which uses an attention based self-supervision for context learning of textual documents. However, these domain specific biomedical pretrained language models mainly use English corpora. Therefore, non-English, domain-specific pretrained models remain quite rare, both of these requirements being hard to achieve. In this work, we proposed AliBERT, a biomedical pretrained language model for French and investigated different learning strategies. AliBERT is trained using regularized Unigram based tokenizer trained for this purpose. AliBERT has achieved state of the art F1 and accuracy scores in different down-stream biomedical tasks. Our pretrained model manages to outperform some French non domain-specific models such as CamemBERT and FlauBERT on diverse down-stream tasks, with less pretraining and training time and with much smaller corpora.",
  "full_text": "The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks, pages 223–236\nJuly 13, 2023 ©2023 Association for Computational Linguistics\nAliBERT: A Pre-trained Language Model for French Biomedical Text\nAman Berhe∗\nQuinten\nIRD, Sorbonne University, UMMISCO\n91, bvd Hopital, F-75013, Paris, France\namanzaid.berhe@ird.fr\nGuillaume Draznieks∗\nQuinten\n8 rue Vernier, 75017 Paris\ngdraznieks@student.ethz.ch\nVincent Martenot\nQuinten\n8 rue Vernier, 75017 Paris\nv.martenot@quinten-france.com\nValentin Masdeu\nQuinten\n8 rue Vernier, 75017 Paris\nv.masdeu@quinten-france.com\nLucas Davy\nQuinten\n8 rue Vernier, 75017 Paris\nl.davy@quinten-france.com\nJean-Daniel Zucker\nIRD, Sorbonne University, UMMISCO,\nINSERM, Sorbonne University, NUTRIOMICS,\n91, bvd Hopital, F-75013, Paris, France\njean-daniel.zucker@ird.fr\nAbstract\nOver the past few years, domain-specific pre-\ntrained language models have been investi-\ngated and have shown remarkable achieve-\nments in different downstream tasks, espe-\ncially in biomedical domain. These achieve-\nments stem on the well-known BERT archi-\ntecture which uses an attention-based self-\nsupervision for context learning of textual\ndocuments. However, these domain-specific\nbiomedical pre-trained language models mainly\nuse English corpora. Therefore, non-English,\ndomain-specific pre-trained models remain\nquite rare, both of these requirements being\nhard to achieve. In this work, we proposed\nAliBERT, a biomedical pre-trained language\nmodel for French and investigated different\nlearning strategies. AliBERT is trained using\nregularized Unigram based tokenizer trained\nfor this purpose. AliBERT has achieved state-\nof-the-art F1 and accuracy scores in different\ndown-stream biomedical tasks. Our pre-trained\nmodel manages to outperform some French\nnon domain-specific models such as Camem-\nBERT and FlauBERT on diverse down-stream\ntasks, with less pre-training and training time\nand with much smaller corpora.\n1 Introduction\nRecent contextual language models have achieved\ntremendous results in almost all domains using\ntextual information. Transformers (Vaswani et al.,\n2017) based pre-trained language models (T-PLM)\nhave contributed and continue to contribute to the\n∗The first two authors have equal contribution.\nsuccess of natural language processing (NLP) in\nmultiple domains of expertise. Furthermore, very\nlarge transformer based models which require hun-\ndreds of billions of parameters have shown extra-\nordinary achievements and became more accessi-\nble.\nThe biomedical field is one of the most important\ndomain and its associated textual corpora is one\nof the fast-growing sources of information in sev-\neral languages. Hence, researchers have leveraged\nPLMs to represent biomedical knowledge from\ndifferent sources, following their success in the\ngeneral domain. There are plenty of Biomedical\nPre-trained Language Models (B-PLMs) that have\nachieved interesting results and that help decision\nmaking in the biomedical field, such as BioBERT\n(Lee et al., 2020), PubMedBERT (Gu et al., 2022),\nBioELECTRA (raj Kanakarajan et al., 2021), etc.\nPLMs are trained using different training mech-\nanisms. The most common are masked language\nmodeling (MLM) (Devlin et al., 2019), replaced\ntoken detection (RTD) (Clark et al., 2020) or next\nsentence prediction (NSP) (Devlin et al., 2019).\nTraining a biomedical language model using dif-\nferent strategies does benefit the different down-\nstream tasks. Furthermore, B-PLMs apply vari-\nous pre-training methods since they borrow some\ncharacteristics from already existing PLMs. Com-\nmonly used pre-training methods includes contin-\nual pre-training (CPT), mixed domain pre-training\nand domain-specific pre-training (DSPT). In this\nwork, DSPT was used for training our proposed\nmodel from scratch using domain-specific French\n223\ncorpora. Furthermore, B-PLMs use tokens as in-\nput. Tokenization is indeed the basic step of lan-\nguage model training since it is the tokens that\nare directly used as discrete input for model pre-\ntraining. There are different ways to tokenize a\ntext input. The most common tokenization tech-\nniques are Byte Pair Encoding (BPE) (such as; Sen-\ntencePiece, WordPiece, etc.) and Unigram sub-\nword based tokenization. Consideration and imple-\nmentation of different tokenization techniques are\nequally important to achieve better performance of\nB-PLMs, especially when the model is language-\nspecific. Language-specific PLMs can use common\ntokenization techniques like BPE, but they can also\ntailor the tokenization process and train a tokenizer\nthat can fit a specific language and domain under\nconsideration. In a similar way the biomedical text\ndiffers from general domain texts, so the use of cus-\ntom tokenization allows for better representation\nof most biomedical vocabulary (words).\nBiomedical language models in languages other\nthan English, i.e. PLMs that are both domain and\nlanguage-specific, are quite rare. In the field of non-\nEnglish language-specific models, there are a few\nthat focus on French language, such as Camem-\nBERT (Martin et al., 2020) and FlauBERT (Le\net al., 2020). French is a very rich language and\nFrench-based PLMs (Martin et al., 2020; Le et al.,\n2020) have shown the importance of such language-\nspecific model for different purposes. However,\nFrench biomedical textual information have not\nbeen implemented using transformers based PLM.\nYet, there are a few French language word embed-\nding in biomedical domains. (Dynomant et al.,\n2019) compared different word embedding tech-\nniques (word2vec (Mikolov et al., 2013), GloVe\n(Pennington et al., 2014)) for French health-related\ndocuments. Given the disadvantages of embed-\nding words for their representation it is neces-\nsary to build B-PLMs for better representation.\nIn this work, we propose AliBERT (named af-\nter Jean-Louis-Marc AliBERT the French pioneer\nof dermatology), a BERT-based language-specific\nand domain-specific Biomedical language model.\nAliBERT uses a masked language model (MLM)\npre-training mechanism which randomly masks\nsome of the tokens from the input biomedical text\nand predicts the masked tokens based on the con-\ntext of the input. Thereby learning the context\nof each word according to the biomedical text in-\nput. A Unigram based tokenizer with a novel regu-\nlarization algorithm has been trained for AliBERT\npre-training. In addition to the MLM, we have\nalso trained an ELECTRA-based (Clark et al.,\n2020) model calledAliBERT-ELECTRA. AliBERT-\nELECTRA is trained using the replaced token de-\ntection mechanism using the same vocabularies\nand tokenization steps as AliBERT. In addition, the\nLAMB optimizer is studied to analyze its computa-\ntional speed gain during model pre-training. Here\nare the main contributions of our work:\n• A French biomedical language model, a\nlanguage-specific and domain-specific PLM,\nwhich can be used to represent French biomed-\nical text for different downstream tasks.\n• A normalization of a Unigram sub-word tok-\nenization of French biomedical textual input\nwhich improves our vocabulary and overall\nperformance of the models trained.\n• AliBERT outperforms other French PLMs in\ndifferent downstream tasks. It is a foundation\nmodel that achieved state-of-the-art results on\nFrench biomedical text. Models are available\non HuggingFace hub1 and datasets are avail-\nable to the public 2.\nThis paper is organized in the following manner:\nfirst the related work is discussed in section 2, dif-\nferent language-specific and domain-specific PLMs\nand their pre-training objectives and strategies are\ndiscussed. Second, section 3 presents our B-PLM\nAliBERT with details on architecture, tokenization\nand optimization. Then, section 4 discusses the\nfine-tuning and evaluation of our models in down-\nstream tasks. Next, section 5 explain the experi-\nments and results on the down-stream tasks. Then,\nsection 6 discusses the results found and the draw-\nbacks we encountered in detail. Finally, section 7\nconcludes the findings of this paper and points\nout our future directions concerning the domain-\nspecific and language-specific PLMs.\n2 Related Work\nIn recent years, the number of language models\nbased on Transformers (Vaswani et al., 2017) has\ngrown rapidly and their performance has been re-\nmarkable in many areas. The pioneers of Trans-\nformers based PLMs (T-PLMs) are BERT (Devlin\n1Quinten-datalab/AliBERT\n2https://gitlab.par.quinten.io/qlab/alibert\n224\net al., 2019) and GPT (Radford et al., 2018) which\nare a stack of encoders and decoders of transform-\ners, respectively. Consequently, the T-PLMs can be\nmainly divided as transformer encoder based mod-\nels such as ALBERT (Lan et al., 2020), RoBERTa\n(Zhuang et al., 2021), ELECTRA (Clark et al.,\n2020), and transformer decoder based model such\nas BART (Lewis et al., 2019), PEGASUS (Zhang\net al., 2019), and T5 (Raffel et al., 2020). De-\nvlin et al. (2019) played an important role for the\nincrease of T-PLMs and fine-tuning many down-\nsteam tasks. They also paved the way for other\nlanguages (other than English), such as (Martin\net al., 2020; Le et al., 2020; Delobelle et al., 2020;\nCañete et al., 2020), to develop language-specific\n(monolingual) language models.\nThere are very few French language models\n(Martin et al., 2020; Le et al., 2020; Copara et al.,\n2020; Douka et al., 2021; Cattan et al., 2021).\nCamemBERT (Martin et al., 2020) and FlauBERT\n(Le et al., 2020) are trained on general knowl-\nedge French corpora. CamemBERT used OSCAR\ndataset which is composed of 130 Gigabytes (GB)\nof raw French text with 32.7 Billion tokens whereas\nFlauBERT utilized 71 GB of raw text with 12.7 Bil-\nlion of token. BERTweetFR (Guo et al., 2021)\nis another French PLM trained on French tweets.\nBERTweetFR is a general domain which is ini-\ntialized using CamemBERT utilizing the largest\nFrench tweet corpora which is composed of 16 GB\nof 226 Million tweets. They took tweets with an\naverage length of 30 tokens. Kamal Eddine et al.\n(2021) developed a BART based French language\nmodel called BARThez which is a generative lan-\nguage model based on BART3 (Lewis et al., 2019).\nBARThez used 66 GB (110 GB after tokenization)\nraw text for pre-training. Cattan et al. (2021) inves-\ntigated the usability of transformer based models\nfor French question answering task and provided\na model known as FrALBERT which is based on\nthe compact language models (parameter efficient\nBERT) called ALBERT (Lan et al., 2020). FrAL-\nBERT is a compact language model pre-trained on\nthe French version of the Wikipedia encyclopedia\nas of 04/05/2021. Their dataset is composed of 4\nGB of text and 17 million sentences. There are\ntwo French domain-specific PLMs. The first one is\nJuriBERT (Douka et al., 2021), it is a French legal\nlanguage model (language and domain-specific)\n3BART: De-noising Sequence-to-Sequence pre-training\nfor Natural Language Generation, Translation, and Compre-\nhension\nwhich is trained on 6.3 GB of raw legal text4. The\nsecond is CamemBioBERT (Copara et al., 2020), it\nis a fine-tuned CamemBERT (Martin et al., 2020)\nusing biomedical text from a French language chal-\nlenge known as DEFT (\"Défi Fouille de Textes\")5.\nDura et al. (2022) introduced their ongoing work on\na clinical French language model, known as EDS\n(Entrepôt des Données de Santé), that uses 21 mil-\nlion French clinical reports from electronic health\nrecords (EHR) from several hospitals in the Paris\narea. Dura et al. (2022) claimed that their prelim-\ninary results achieved better results than Camem-\nBERT (Martin et al., 2020). They have trained EDS\nfrom scratch (EDS-from-scratch) and continuous\ntraining over CamemBERT (EDS-fine-tuned).\nRegarding domain-specific-language models,\nLee et al. (2020) built the first BERT based lan-\nguage model in English in the biomedical domain,\nknown as BioBERT. BioBERT (Lee et al., 2020) is\nbuilt on top of the BERT (Devlin et al., 2019) model\nusing abstracts of biomedical articles. Following\nthe publication of BioBERT, biomedical language\nmodels have gained considerable momentum. A\nsurvey (Kalyan et al., 2021) studied many publicly\navailable language models in the biomedical do-\nmain and provided a survey of systematic literature\nreview, known as AMMU. AMMU includes 121\narticles of biomedical language models.\nAMMU investigated the core B-PLMs concepts,\nsuch as pre-training methods, pre-training tasks,\nfine-tuning methods and embeddings. Furthermore,\nKalyan et al. (2021) disclosed different types of cor-\npora along with the language models that used the\ncorpus. The main corpora included were electronic\nhealth record (EHR), radiology reports, social me-\ndia texts and scientific literature. They have listed\nout the most common learning objectives such as\nMasked Language Modeling (MLM), Replaced\nToken Detection (RTD), Next Sentence Predic-\ntion (NSP), Sentence Order Prediction (SOP) and\nSpan Boundary Objective (SBO). There are few\nnon-English transformer-based biomedical PLMs\n(Schneider et al., 2020; Bressem et al., 2020; López-\nGarcía et al., 2021; Vakili et al., 2022). Most of\nthe models are pre-trained using the continual pre-\ntraining (CPT) approach which means they used al-\nready pre-trained language-specific general knowl-\nedge PTM. We invite readers to refer to the AMMU\n4Number of token used in JuriBERT (Douka et al., 2021)\nnot mentioned in the paper\n5DEFT is a scientific evaluation campaign on Francophone\ntext mining.\n225\nsurvey for details (Kalyan et al., 2021).\nTo the best of our knowledge, there is not yet a\nFrench biomedical transformer based PLM trained\nfrom scratch. However, as mentioned above we are\naware of an ongoing work on a PLM for French\nclinical reports using proprietary EHR (Dura et al.,\n2022) From the literature we can clearly see that\nthere is a gap in pre-trained language models for\nFrench biomedical text mining. Hence, our primary\ngoal is to address this gap and enhance the tokeniza-\ntion of French biomedical texts. Instead of relying\nsolely on general tokenization methods, we have\nstandardized the tokenization process specifically\nfor French biomedical texts.\n3 Methods\nThis sections focuses on how the proposed pre-\ntrained language model, AliBERT, was built. It\ndescribes the pre-training strategy and architecture,\npre-training corpora, tokenization and optimization\nof our models.\n3.1 Pre-training strategies\nThere are different kinds of pre-training strategies\nto train a transformers based models (Kalyan et al.,\n2021). Pre-training from scratch (PTS) is the strat-\negy used for training AliBERT and its variants.\nThey are trained from scratch using biomedical\ncorpora to better represent the biomedical context\nof words. Training our models from scratch helps\nto represent vocabulary that only exists in biomedi-\ncal text, which will be discussed in subsection 3.3.\nThe models developed are based on the trans-\nformers (Vaswani et al., 2017) architecture and\nRoBERTa (Zhuang et al., 2021) a variant of the\nBERT (Devlin et al., 2019) model is used as\nmasked language model (MLM), transformers and\nBERT architecture will not be discussed here be-\ncause they have been discussed extensively in many\nresearch works (Devlin et al., 2019; Martin et al.,\n2020). Therefore, AliBERT is trained in the course\nof self-supervised learning by masking 15% of the\nwords from the input text (sequence of words). All\nnecessary steps and configurations are discussed in\nthe following subsections.\n3.2 Pre-training data\nThe pre-training corpus was gathered from differ-\nent sub-corpora of French biomedical textual doc-\numents. The sources used are a database of drug\nleaflets (\"Base de données publique des médica-\nments\"), a French equivalent of Physician’s Desk\nReference i.e. RCP6, biomedical articles from Sci-\nenceDirect7, Thesis manuscripts in French and ar-\nticles from Cochrane database8. It can be inferred\nfrom the names of the corpora that they cover vari-\nous topics in the biomedical domain and that they\nhave different writing styles. Table 1 summarises\nthe different corpora collected and used for pre-\ntraining AliBERT models.\nName Quantity Size\nDrug database 23K 550Mb\nRCP 35K 2200Mb\nArticles 500K 4300Mb\nThesis 300K 300Mb\nCochrane 7.6K 27Mb\nTable 1: Corpora used to pre-train AliBERT\nThe corpora were collected from different\nsources. Scientific articles are collected from Sci-\nenceDirect using an API provided on subscrip-\ntion and where French articles in biomedical do-\nmain were selected. The summaries of thesis\nmanuscripts are collected from \"Système universi-\ntaire de documentation (SuDoc)\" which is a catalog\nof universities documentation system. Short texts\nand some complete sentences were collected from\nthe public drug database which lists the character-\nistics of tens of thousands of drugs. Furthermore,\na similar drug database known as \"Résumé des\nCaractéristiques du Produit (RCP)\" is also used to\nrepresent a description of medications that are in-\ntended to be utilized by biomedicine professionals.\nPages of biomedical articles from Cochrane are\nalso collected. Hence, our corpus for pre-training\nis composed altogether of around 7 gigabytes (GB)\ntextual documents.\nWhen compared with the corpora of already ex-\nisting French T-PLMs, our corpus is big enough\nto represent a biomedical text. Table 2 compares\nthe different corpora used for pre-training French\nlanguage models.\n6The \"Résumé des Caractéristiques du Produit\" (RCP)\ndatabase aims at providing more accurate information than the\ninstructions note for use of medicines.\n7ScienceDirect is a website which provides access to a\nlarge bibliographic database of scientific and medical publica-\ntions of the Dutch publisher Elsevier.\n8Cochrane is a British international charitable organisa-\ntion formed to organise medical research findings to facilitate\nevidence-based choices about health interventions involving\nhealth professionals, patients and policy makers.\n226\nModel Domain Size Source\nCamemBERT (Martin et al., 2020) general 138 GB OSCAR\nFlauBERT (Le et al., 2020) general 71 GB WMT19, OPUS, Wikmedia\nBERTweetFR (Guo et al., 2021) general 16 GB French tweets\nJuriBERT (Douka et al., 2021) legal 6.3 GB LégalFrance & Court of Causation\nFrAlbert (Cattan et al., 2021) general 4.0 GB Wikipedia\nAliBERT biomedical 7.0 GB ScienceDirect, SuDoc, Drug databases\nand Cochrane\nTable 2: Comparison of the AliBERT corpus with that of the existing French PLMs.\n3.3 Tokenization\nIn the context of Pre-trained Language Models\n(PLMs), tokenization refers to the process of divid-\ning the input text into subwords or words known\nas tokens that will serve as the input to the model.\nMost BERT based PLMs use sub-word tokeniza-\ntion scheme such as Byte Pair Encoding (BPE),\nWordPiece and SentencePiece. However, the tok-\nenization process can be adapted or trained to meet\na specific purpose and/or to represent a vocabulary\nin a specific domain. We chose to train our own to-\nkenizer to ensure that its vocabulary encompasses\nthe necessary biomedical terms.\nA normalization step prior to tokenization, par-\nticularly adapted to French, was used to enhance\nour vocabulary. In this step we added a space\nafter a selected list of punctuation mark. It nor-\nmalises the representation of the text, and facil-\nitates both the tokenization and learning by the\nneural network. Hence, this step leads to a sig-\nnificant reduction of duplicates, such as, (\"MOT\",\n\"_MOT\"),(\"_siècle\",\"_siècles\") which were intro-\nduced due to punctuation marks like \"(\", \" :\", \"-\",\netc. in the text.\nWe have trained different tokenizers, such as Un-\nigram, WordPiece with different parameters (vocab-\nulary size, regularization). Unlike BPE, Unigram\nstarts with a large vocabulary and removes tokens\nuntil it reaches the desired vocabulary size. During\ntraining, at every step, Unigram computes a loss\nover the corpus given the current vocabulary. Then,\nfor each symbol it calculates how much the overall\nloss would increase if the symbol was removed,\nand looks for the symbols that would decrease it\nthe most. Appendix A discusses the steps taken\nduring tokenization with an example and compares\nUnigram tokenizers trained from scratch and the\ntokenizer from CamemBERT (Martin et al., 2020).\n3.3.1 Training configurations\nWhen training a large language model, it is neces-\nsary to take into account different configurations\nneeded to build a well-performing model. There-\nfore, model architecture, training strategy, opti-\nmization and computation are key parameters to\nconsider.\nModel architecture and training: We have\nmainly developed two architectures of our French\nB-PLM namely AliBERT: a BERT (Lan et al.,\n2020) based and AliBERT-ELECTRA an ELEC-\nTRA (Clark et al., 2020) based, models. BERT\nand ELECTRA differ only in their learning strat-\negy. The former uses masked language modeling\n(MLM) and the later uses replaced token detection\n(RTD). AliBERTbase has the same architecture as\nBERTbasewhich has a length (L) of 12, height (H)\nof 512 and a self-attention head (A) of 12.\nFor MLM, a sequence of words is given as in-\nput and 15% of the words are hidden. The input\ngoes through the tokenization stage and the words\nare tokenized. The tokens are padded or truncated\nto have a maximum length of 512 tokens. Hence,\nspecial tokens \"[CLS]\",\"[PAD]\" are added if the\nsequence length is less than 512 tokens. Then the\nembeddings of the tokens are passed to the trans-\nformer layers to learn the context of the input and\nthe relationship of the tokens. Finally, the output of\nthe transformers is passed to a feed-forward neural\nnetwork to compute the probability distribution of\nthe token to predict the masked tokens/words. For\nmore detail on this training method see the original\nwork of BERT (Devlin et al., 2019).\nAnother strategy different from MLM is RTD,\nin RTD the objective is to predict which tokens\nhave been replaced and which have not. A very\nsimple pre-trained model is used as a generator to\npredict a masked word from the input text. Then,\nthe predicted words are used to replace the masked\n227\ninputs and the unmasked sentence is used as input\ntext in the discriminator model. Eventually, the\ndiscriminator model is used to identify the original\nwords of the original input text. For more details\nof the architecture, we invite our readers to refer to\nthe original work of ELECTRA (Ozyurt, 2020).\nOptimization: AliBERT was originally trained\nusing the ADAM 9 optimizer for faster and bet-\nter training as used in BERT. Meanwhile, a recent\nwork by You et al. (2020) introduced an optimizer\nknown as LAMB that reduces the training time\nof BERT from 3 days (4320 minutes) to 76 min-\nutes. Therefore, AliBERT was also trained using\nLAMB optimizer for the purpose of comparing it\nwith ADAM optimizer which is the default for our\npre-trained models.\nThe models trained using LAMB optimizer\ntrained much faster than their counter part (using\nADAM). However the performance of the models\ntrained with LAMB was not as good as the models\ntrained with ADAM. The loss of the model quickly\nreduces when LAMB optimizer is used during train-\ning. Figure 3 in Appendix B shows the comparison\nof time taken to train using LAMB and ADAM at-\nomizers on our models. Moreover,AliBERT trained\nwith ADAM optimizer achieved better results in\nNER downstream task. Table 6 compares two\nAliBERT models trained with ADAM ( AliBERT)\nand LAMB (AliBERT-LAMB) optimizers.\n4 Fine-tuning and Model Evaluation\nIn order to evaluate the level of understanding of\nFrench biomedical tasks by AliBERT, we have fine-\ntuned AliBERT on standard pre-trained language\nmodel evaluation tasks such as biomedical named\nentity recognition (NER), biomedical text classifi-\ncation, etc. Below, we discussed how the tasks are\ntrained.\n4.1 Biomedical named entity recognition\n(NER)\nFor the NER task we have used HuggingFace10 to-\nken classification pipeline using our AliBERT mod-\nels. The first dataset used is \"CAS dataset\", from\nthe work of Grouin et al. (2019), which is used in\ndifferent challenges of French biomedical text chal-\nlenge known as \"DEFT (Défis Fouille de Texte)\".\n9Adam is an algorithm for first-order gradient-based opti-\nmization of stochastic objective functions, based on adaptive\nestimates of lower-order moments.\n10HuggingFace: the AI community building the future.\nhttps://huggingface.co/\nIt is composed of clinical French texts which fo-\ncuses on specific specialties of medical domains\nsuch as cardiology, urology, oncology, obstetrics,\npneumatic, etc. The annotation in this dataset in-\nclude plenty of biomedical entities where some of\nthem do not have adequate annotation. Hence, we\nhave kept only the five most-annotated types, i.e,\nanatomy, pathology, symptom, substance and value.\nAppendix D describes the annotated dataset used in\nNER task for fine-tuning and evaluation purposes.\nMeanwhile, QUAERO (Névéol et al., 2014)\ndatasets is used for more experiment and fine-\ntuning. QUAERO datasets is composed of ten an-\nnotated entity categories corresponding to UMLS\n(Unified Medical Language System) semantic\ngroups. The annotation was performed using au-\ntomatic pre-annotations and validated by trained\nhuman annotators. We have selected five entities\nthat are most related to biomedical concepts, from\nthe QUAERO-MEDLINE datasets which consist of\narticle titles from the MEDLINE11 database. The\nfive entities are selected according to their defini-\ntion and their relatedness with biomedical domain.\nThe entities and the dataset are discussed in ap-\npendix Appendix D.\n4.2 Biomedical text classification\nFor the biomedical text classification, we have used\na private dataset which is composed of 410,000\nexamples and 789 classes. Hence, it is an extreme\nclassification problem. Classes that have more than\n1000 examples have been selected. A sequence\nclassification model from hugging face was used\nto fine-tune a downstream classification model.\n5 Experiments and Results\nAliBERTbase was trained on 48 GPUs Nvidia A100\n(12 nodes each with 4 GPUs) for 20 hours with\n512 input tokens and a batch size of 960 (20 batch\nsize for each GPU). We have used a vocabulary of\n40K sub-word units which are built using Unigram\ntokenization algorithm.\nOur models have been evaluated using the above-\nmentioned fine-tuning models and on the masked\ntoken prediction. The results found using our mod-\nels have been compared to the CamemBERT (Mar-\ntin et al., 2020) French PLM which is the state-of-\nthe-art in French language. Unfortunately, we were\nnot able to compare our models with biomedical\n11http://www.ncbi.nlm.nih.gov/pubmed/\n228\nPLMs due to the lack of French PLM in biomedical\ndomain.\nThe downstream tasks on which our models have\nbeen evaluated are Biomedical NER, classification\nand Masking Language Modeling (MLM). The\ndownstream datasets are not included in the train-\ning dataset of our models. However, there might\nbe an overlap with the QUAERO-MEDLINE arti-\ncle titles. The results obtained on these tasks are\ndetailed below.\nBiomedical Named Entity Recognition (NER)\nA token classification model was fine-tuned from\nthe pre-trained models mainly in 5 biomedical en-\ntity types, these are symptoms, anatomy, sub-\nstance, value and pathology. Our models have\noutperformed CamemBERT in most of the enti-\nties and in their macro average of precision (P),\nrecall(R) and F1 score (F1).\nThe results found in Table 3 are trained upon\na batch size of 80, learning rate (lr) of 2e-5 and\nweight decay of 0.01 and the dataset used for each\nof the entities is discussed on Appendix D. Table 3\nillustrates that AliBERT and AliBERT-ELECTRA\noutperformed CamemBERT considering the preci-\nsion of the models to detect the entities. Camem-\nBERT achieved higher F1 score than our models’\nfor the \"Pathology\" entity. This is due to the fact\nthat the pathology entities in the dataset are very\nlong text that includes many words that exist in\nthe general French language words (CamemBERT\nvocabularies). For example, \"tumeur qui est\nd’allure maligne et qui envahissait la\nface postérieure et la corne vésicale\ndroite\" is annotated as a single pathology en-\ntity. However, our models exhibited a notewor-\nthy improvement in F1 score for the other entities\nwhen compared to CamemBERT. Furthermore, our\nmodel outperformed CamemBERT for disorder (in-\ncluding pathology) on the QUAERO dataset.\nTable 4 shows the results of NER task on the\nQUAERO dataset and it compares the results with\nCamemBERT. Our model outperformed the two\nmodels on identifying different kinds of entities\n(Disorder, Anatomy, Device, Disorder and Proce-\ndure) in QUAERO dataset with around 15% macro\naverage f1 score improvement. We selected the\nentities that are closely related to biomedical con-\ncepts. In, Table 4 CamemBERT was not able to\nidentify any medical device whereas AliBERT and\nAliBERT-ELECTRA detected the devices with f1\nscore of 42%. Hence, we can say that the B-PTMs\ncan identify to the specific terms used in the do-\nmain.\nMasking language modeling and classification\nWe have also compared the ability of the models\nto predict masked tokens and biomedical text clas-\nsification. In the same way our proposed models\nhave outperformed CamemBERT. For this exper-\niment of unmasking evaluation a subset of 3000\ntext of clean texts (1000 articles of ScienceDirect,\n1000 articles from Cochrane, 1000 thesis abstracts\nfrom SuDuc) was used. For the biomedical text\nclassification, we selected classes with more than\n1000 examples, resulting in 50 classes, from our\nprivate data. Table 5 illustrates the performance of\ndifferent models for the prediction of the masked\nword and classification, in top 1, 3 and 5 Accuracy\n(Acc, 3-Acc and 5-Acc respectively).\nAliBERT has outperformed CamemBERT on pre-\ndicting a masked word prediction (see Figure B for\nexamples). It can be seen in Table 5 AliBERT has\nan increase of 23% in accuracy when compared\nwith CamemBERT. In the same way for text classi-\nfication our models achieved better top 1 accuracy.\nHence, it clearly shows that in-domain pre-trained\nlanguage models are really important while dealing\nwith a domain-specific texts and hence domain-\nspecific downstream tasks.\n6 Discussion\nOur pre-trained language models trained on in-\ndomain (biomedical) textual documents tend to out-\nperform models that are trained on general domain\ntextual documents which is also seen on the litera-\nture review of pre-trained language models for En-\nglish language such as BioBERT (Lee et al., 2020),\nPubMedBERT (Gu et al., 2022), etc. Training\nPLMs using the masked language model (MLM)\nobjective shows somewhat better results, but the\ndifference is not significant compared to the re-\nplaced token prediction (MLM) objective. More-\nover, choosing the right optimizer like LAMB has\nan effect on the training speed of the pre-trained\nmodels but not on the performance of the models.\nDuring the training of our models different types of\ntokenizers, such as, Unigram, WordPiece, Senten-\ncePiece, BPE, etc. are trained and compared with\neach other. Unigram tokenizer along with our nor-\nmalization (see section 3) step tend to outperform\nother tokenizers. Unigram was also trained into two\nways, cased and uncased respectively. Lower cas-\ning the input text achieved better results than letting\n229\nModels’ performances on CAS dataset\nCamemBERT AliBERT AliBERT-ELECTRA\nEntities P R F1 P R F1 P R F1\nSubstance 0.96 0.87 0.91 0.96 0.91 0.93 0.95 0.91 0.93\nSymptom 0.89 0.91 0.90 0.96 0.98 0.97 0.94 0.98 0.96\nAnatomy 0.94 0.91 0.88 0.97 0.97 0.98 0.96 0.97 0.96\nValue 0.88 0.46 0.60 0.98 0.99 0.98 0.93 0.93 0.93\nPathology 0.79 0.70 0.74 0.81 0.39 0.52 0.85 0.57 0.68\nMacro Avg 0.89 0.79 0.81 0.94 0.85 0.88 0.92 0.87 0.89\nTable 3: French Biomedical named entity recognition (NER) results. Performance in bold is the best achieved for\nthe entity in question and the metrics in question\nModels’ performances on QUAERO MEDLINE dataset\nCamemBERT AliBERT AliBERT-ELECTRA\nEntity P R F1 P R F1 P R F1\nAnatomy 0.649 0.641 0.645 0.795 0.811 0.803 0.799 0.801 0.800\nChemical 0.844 0.847 0.846 0.878 0.893 0.885 0.898 0.818 0.856\nDevice 0.000 0.000 0.000 0.506 0.356 0.418 0.549 0.338 0.419\nDisorder 0.772 0.818 0.794 0.857 0.843 0.850 0.883 0.809 0.845\nProcedure 0.880 0.894 0.887 0.969 0.967 0.968 0.944 0.976 0.960\nMacro Avg 0.655 0.656 0.655 0.807 0.783 0.793 0.818 0.755 0.782\nTable 4: Biomedical named entity recognition (NER) results on the QUAERO MEDLINE dataset.Performance in\nbold is the best achieved for the entity in question and the measure in question\nMLM Classification\nModel Acc 3-Acc 5-Acc Acc 3-Acc 5-Acc\nCamemBERT 0.49 0.57 0.62 0.66 0.72 0.99\nAliBERT 0.72 0.83 0.87 0.68 0.73 0.99\nAliBERT-ELECTRA 0.71 0.83 0.87 0.68 0.73 0.99\nTable 5: Results predicting the masked tokens (MLM) and biomedical classification\nupper cases as it is. Biomedical text tend to have\nlots of words that are written in capital letters. But\nwe have noted that they are not enough to be used\nfor training our models as upper cases. Biomedical\nnamed entity recognition (B-NER) and biomedical\ntext classification (private data, hence results not\nreported) were used to fine-tune our models to a\nspecific task. Our models tend to generalize faster\nthan the French counterpart general PLMs. For\nAliBERT or AliBERT-ELECTRA fewer examples\nof B-NER text inputs were required to start learn-\ning and generalize quickly and accurately. On the\nother hand, Camembert took more time to general-\nize and with less precision for biomedical entities.\nThis is understandable as it was not trained using\ndomain texts. In the same manner, this behaviour\nwas reflected during biomedical text classification\ntask. This can also be seen as a comparison to\nthe vocabularies used by CamemBERT and our\nmodels. Our tokenizer’s (Unigram) vocabulary and\nCamemBERT tokenizer’s (SentencePiece) have a\nhuge difference in content and size. The Unigram\ntokenizers used to train our models have a vocabu-\nlary size of 40008 while CamemBERT has a size\nof 32005. CamemBERT’s vocabulary does not\ninclude most biomedical words. In fact, the two\ntokenizers have about 10,000 tokens in common\nin their vocabularies. Although the performance\nof our models is already very good, more and var-\nied corpora could improve the models’ capabilities.\nFor example, medical notes, often found in elec-\ntronic health records (\"EHRs\"), can help represent\nthe knowledge and experience of practitioners.\nIn addition, to improve the models, continu-\n230\nous training on a general purpose pre-trained lan-\nguage, such as CamemBERT, could be imple-\nmented. Since our tokenizers were a bit different\nand our goal is to study a purely biomedical PLM,\nwe have not investigated it yet.\n7 Conclusion\nThis paper proposes a French biomedical pre-\ntrained language model that was trained on sev-\neral corpora of French biomedical textual materi-\nals. Two variants of the model are proposed using\ntwo different pre-training strategies. AliBERT is a\npre-trained model based on BERT (Devlin et al.,\n2019) which used the pre-training strategy of mask-\ning language models (MLM). AliBERT-ELECTRA\nis based on ELECTRA (Clark et al., 2020) and\nused a replaced token prediction (RTP) learning\nstrategy. Furthermore, a tokenization adaptation\nstrategy was introduced as a building block for\npre-training the two proposed models. A LAMB\noptimizer has also been tested to speed-up the learn-\ning of AliBERT. The proposed pre-training models\nhave been tested on different downstream tasks\nand achieved state-of-the-art results on different\ntasks. Biomedical entity recognition (NER) and\nbiomedical text classification downstream tasks are\nfine-tuned using different biomedical textual docu-\nments. Hence, AliBERT is expected to be used by\ndifferent organization and practitioners that work\nwith biomedical text for better understanding and\nto help make informed decisions regarding biomed-\nical situations.\nLimitations\nAlthough our models performed well in all down-\nstream tasks, the models also have some limitations.\nOne of the limitations is the lack of varied biomed-\nical corpus. Hence, we plan to work on integrat-\ning clinical documents e.g. EHR data, specifically\nphysician notes, to make the model more robust to\nvarious kind of biomedical documents. The models\ncan also be enlarged by using continual learning\nstrategy from well-known French pre-trained lan-\nguage models. CamemBERT (Martin et al., 2020)\ncan be used as a base model and the training can\nbe continued using our biomedical corpus, like\nBioBERT (Lee et al., 2020) and others did. More-\nover, our models used 512 sequence of tokens and\nmore longer sequence lengths can be used as seen\nin the long language models like BigBird (Zaheer\net al., 2020).\nWe are currently working on a new version of\nAliBERT with more data and a greater diversity of\ncorpora that include text from EHR and medical\nnotes in our corpora. Finally, we also plan to train\nAliBERT to generate biomedical texts for different\npurposes.\nA reasonable amount of computational resources\nwas used to conduct this study, since approximately\n20,160 hours of GPU computation were used to cre-\nate the three pre-trained models presented above.\nThe total environmental cost according to Green\nAlgorithm (Lannelongue et al., 2021)12 is equiva-\nlent to 1.45 MWh or 71.11 kg CO2e. This com-\nputational cost and environmental impact should\nbe taken into consideration when training such a\nmodel.\nEthics Statement\nAliBERT, a BERT-based biomedical language\nmodel for the French language, has the potential\nto improve healthcare and research in French lan-\nguage. However, it is essential to address ethical\nconsiderations such as biases, privacy, misinforma-\ntion, access and control, as well as accountability\nand transparency. We have implemented measures\nto mitigate biases, protect privacy, prevent mali-\ncious use and optimize efficiency as much as possi-\nble.\nTo responsibly develop and deployAliBERT, col-\nlaboration between developers, researchers, poli-\ncymakers, and healthcare professionals are crucial.\nBy working together, stakeholders can ensure that\nAliBERT benefits a wide range of users and upholds\nethical standards, ultimately maximizing its poten-\ntial to improve healthcare and research in French\nbiomedical domain.\nAcknowledgements\nThe authors acknowledge a support from the\nFrench Institute for Sustainable Science (IRD)\nwithin the framework of the France Relance plan\nto support research and development (R&D) em-\nployment and collaborative research. This work\nwas partly performed using HPC resources from a\nGENCI-IDRIS grant.\nReferences\nKeno K Bressem, Lisa C Adams, Robert A Gaudin,\nDaniel Tröltzsch, Bernd Hamm, Marcus R Makowski,\n12http://calculator.green-algorithms.org/\n231\nChan-Yong Schüle, Janis L Vahldiek, and Stefan M\nNiehues. 2020. Highly accurate classification of\nchest radiographic reports using a deep learning nat-\nural language model pre-trained on 3.8 million text\nreports. Bioinformatics, pages 5255–5261.\nOralie Cattan, Christophe Servan, and Sophie Rosset.\n2021. On the usability of transformers-based models\nfor a french question-answering task. In Proceedings\nof the International Conference on Recent Advances\nin Natural Language Processing (RANLP) , pages\n244–255.\nJosé Cañete, Gabriel Chaperon, Rodrigo Fuentes, Jou-\nHui Ho, Hojin Kang, and Jorge Pérez. 2020. Span-\nish pre-trained bert model and evaluation data. In\nPML4DC@ICLR.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors. In 8th International Conference on Learning\nRepresentations (ICLR), pages 1–18.\nJenny Copara, Julien Knafou, Nona Naderi, Claudia\nMoro, Patrick Ruch, and Douglas Teodoro. 2020.\nContextualized french language models for biomed-\nical named entity recognition. In 6e conférence\nconjointe journées d’études sur la parole (jep, 33e\nédition), traitement automatique des langues na-\nturelles (taln, 27e édition), rencontre des étudiants\nchercheurs en informatique pour le traitement au-\ntomatique des langues (récital, 22e édition). atelier\ndéfi fouille de textes, pages 36–48.\nPieter Delobelle, Thomas Winters, and Bettina Berendt.\n2020. Robbert: A dutch roberta-based language\nmodel. Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics (ACL):\nStudent Research Workshop, pages 203–209.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 conference of the\nnorth American chapter of the Association for Com-\nputational Linguistics (ACL): Human Language Tech-\nnologies, pages 4171–4186.\nStella Douka, Hadi Abdine, Michalis Vazirgiannis, Ra-\njaa El Hamdani, and David Restrepo Amariles. 2021.\nJuribert: A masked-language model adaptation for\nfrench legal text. In Proceedings of the Natural Legal\nLanguage Processing Workshop 2021, pages 95–101.\nBasile Dura, Charline Jean, Xavier Tannier, Alice Cal-\nliger, Romain Bey, Antoine Neuraz, and Rémi Fli-\ncoteaux. 2022. Learning structures of the french\nclinical language: Development and validation of\nword embedding models using 21 million clinical\nreports from electronic health records. pages 1–10.\nEmeric Dynomant, Romain Lelong, Badisse Dahamna,\nClément Massonnaud, Gaétan Kerdelhué, Julien\nGrosjean, Stéphane Canu, and Stefan J Darmoni.\n2019. Word embedding for the french natural lan-\nguage in health care: Comparative study. JMIR Med-\nical Informatics, page e12310.\nCyril Grouin, Natalia Grabar, Vincent Claveau, and\nThierry Hamon. 2019. Clinical case reports for NLP.\nIn Proceedings of the 18th BioNLP Workshop and\nShared Task, pages 273–282.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto\nUsuyama, Xiaodong Liu, Tristan Naumann, Jianfeng\nGao, and Hoifung Poon. 2022. Domain-specific lan-\nguage model pretraining for biomedical natural lan-\nguage processing. ACM transactions on computing\nfor healthcare, pages 1–23.\nYanzhu Guo, Virgile Rennard, Christos Xypolopoulos,\nand Michalis Vazirgiannis. 2021. Bertweetfr: Do-\nmain adaptation of pre-trained language models for\nfrench tweets. In Proceedings of the seventh work-\nshop on Noisy User-generated Text, pages 445–450.\nKatikapalli Subramanyam Kalyan, Ajit Rajasekharan,\nand Sivanesan Sangeetha. 2021. Ammu: A survey\nof transformer-based biomedical pretrained language\nmodels. Journal of Biomedical Informatics , page\n103982.\nMoussa Kamal Eddine, Antoine Tixier, and Michalis\nVazirgiannis. 2021. Barthez: A skilled pretrained\nfrench sequence-to-sequence model. In Proceedings\nof the 2021 conference on empirical methods in natu-\nral language processing, pages 9369–9390.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. 8th International Con-\nference on Learning Representations (ICLR), pages\n1–17.\nLoïc Lannelongue, Jason Grealey, and Michael Inouye.\n2021. Green algorithms: Quantifying the carbon\nfootprint of computation. Advanced Science, page\n2100707.\nHang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Max-\nimin Coavoux, Benjamin Lecouteux, Alexandre Al-\nlauzen, Benoit Crabbé, Laurent Besacier, and Didier\nSchwab. 2020. Flaubert: Unsupervised language\nmodel pre-training for french. In Proceedings of the\ntwelfth Language Resources and Evaluation Confer-\nence (LREC), pages 2479–2490.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: A pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, pages 1234–1240.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nand Veselin Stoyanov andLuke Zettlemoyer. 2019.\nBart: Denoising sequence-to-sequence pre-training\n232\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics\n(ACL), pages 7871–7880.\nGuillermo López-García, José M. Jerez, Nuria Ribelles,\nEmilio Alba, and Francisco J. Veredas. 2021. Trans-\nformers for clinical coding in spanish. IEEE access,\npages 72387–72397.\nLouis Martin, Benjamin Muller, Pedro Javier Ortiz\nSuárez, Yoann Dupont, Laurent Romary, Éric Ville-\nmonte de la Clergerie, Djamé Seddah, and Benoît\nSagot. 2020. Camembert: A tasty french language\nmodel. In Proceedings of the 58th annual meeting of\nthe Association for Computational Linguistics (ACL).\nTomáš Mikolov, Wen-tau Yih, and Geoffrey Zweig.\n2013. Linguistic regularities in continuous space\nword representations. In Proceedings of the 2013\nconference of the north american chapter of the Asso-\nciation for Computational Linguistics (ACL): Human\nLanguage Technologies, pages 746–751.\nAurélie Névéol, Cyril Grouin, Jeremy Leixa, Sophie\nRosset, and Pierre Zweigenbaum. 2014. The quaero\nfrench medical corpus: A ressource for medical entity\nrecognition and normalization. pages 24–30.\nIbrahim Burak Ozyurt. 2020. On the effectiveness of\nsmall, discriminatively pre-trained language repre-\nsentation models for biomedical text mining. In Pro-\nceedings of the first workshop on Scholarly Document\nProcessing, pages 104–112.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language processing\n(emnlp), pages 1532–1543.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. pages 1–12.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Journal of machine learning research, pages\n1–67.\nKamal raj Kanakarajan, Bhuvana Kundumani, and\nMalaikannan Sankarasubbu. 2021. Bioelectra: Pre-\ntrained biomedical text encoder using discriminators.\nIn Proceedings of the 20th workshop on Biomedical\nLanguage Processing, pages 143–154.\nElisa Terumi Rubel Schneider, João Vitor Andrioli\nde Souza, Julien Knafou, Lucas Emanuel Silva e\nOliveira, Jenny Copara, Yohan Bonescki Gumiel,\nLucas Ferro Antunes de Oliveira, Emerson Cabr-\nera Paraiso, Douglas Teodoro, and Cláudia Maria\nCabral Moro Barra. 2020. Biobertpt-a portuguese\nneural language model for clinical named entity\nrecognition. In Proceedings of the 3rd Clinical Natu-\nral Language Processing Workshop, pages 65–72.\nThomas Vakili, Anastasios Lamproudis, Aron Henriks-\nson, and Hercules Dalianis. 2022. Downstream task\nperformance of BERT models pre-trained using auto-\nmatically de-identified clinical data. In Proceedings\nof the Thirteenth Language Resources and Evalua-\ntion Conference, pages 4245–4252, Marseille, France.\nEuropean Language Resources Association.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in Neural Information process-\ning Systems (NIPS).\nYang You, Jing Li, Sashank J. Reddi, Jonathan Hseu,\nSanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song,\nJames Demmel, Kurt Keutzer, and Cho-Jui Hsieh.\n2020. Large batch optimization for deep learning:\nTraining bert in 76 minutes. In 8th International Con-\nference on Learning Representations (ICLR), pages\n1–38.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Advances in neural information\nprocessing systems, pages 17283–17297.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter J. Liu. 2019. Pegasus: Pre-training with extracted\ngap-sentences for abstractive summarization. Pro-\nceedings of the 37th International Conference on\nMachine Learning (ICML), pages 11328–11339.\nLiu Zhuang, Lin Wayne, Shi Ya, and Zhao Jun. 2021.\nA robustly optimized bert pre-training approach with\npost-training. In Proceedings of the 20th chinese na-\ntional conference on computational linguistics, pages\n1218–1227.\nAppendix\nA Tokenizers comparison and\nnormalization\nFigure 1 depicts the steps taken during tokenization\nwith an example and compares Unigram tokeniz-\ners trained from scratch and the tokenizer from\nCamemBERT(Martin et al., 2020).\nDifferent tokenizers are trained from scratch and\nare compared with one another according to their\nperformance. Figure 2 shows the performance of\nUnigram, BPE and WordPiece tokenization algo-\nrithms. Unigram tokenization have higher propor-\ntion of words and learns faster than other tokeniz-\ners. It has also achieved the best results in training\nAliBERT and fine-tuning tasks. In Figure 2, to-\nkenizers with a legend \"_L_\" describes that the\ntext is lower cased and \"_NoNo_\" shows that the\nnormalization step is ignored during training the\ntokenizer.\n233\nFigure 1: Normalization and tokenization example. During normalization step the input text is normalized by adding\na space after the punctuation (shown by the orange vertical lines) and removing a space before it (shown by the\nred vertical lines) and then used to train the tokenizer (Unigram). The Unigram tokenizers are trained from scratch\nwhile developing AliBERT, Unigram uses text input as it is (does not change the cases), Unigram_L lower cased the\ninput text and Unigram_N_N is the not-normalized version of Unigram and CamemBERT is the tokenizer used by\nCamemBERT (Martin et al., 2020), a French PLM.\nFigure 2: Proportion of individual words with less than x words\nB Optimization:\nThe models trained using LAMB optimizer trained\nmuch faster than their counter part (using ADAM).\nHowever the performance of the models trained\nwith LAMB was not as good as the models trained\nwith ADAM. Figure 3 shows the comparison of\ntime taken to train using LAMB and ADAM at-\nomizers on our models. The loss of the model\nquickly reduces when LAMB optimizer is used\nduring training.\nTable 6 compares two same models with differ-\nent optimizers. AliBERT uses ADAM optimizer\nand AliBERT-LAMB uses LAMB optimizer for\npre-training. The two models are compared on\nNER task on the CAS dataset. AliBERT outper-\nformed AliBERT-LAMP in terms of precision (p),\nrecall (r) and f1 score (F1) for all the entity types\nexcept \"Pathology\".\nC MLM examples\nFigure 4 presents few biomedical text examples for\nthe prediction of masked words. Predicted words\ncolored in green are the correct predictions. Blue\ncolors shows the prediction is correct in the top 2\npredictions, purple color depicts that the correct\nprediction is the top 3 and the red colors show the\ncorrect word has not been predicted. As can be\nseen, Figure 4 AliBERT and AliBERT-ELECTRA\noutperformed the two French PLMs. This confirms\nthat the need for training domain-specific language\nmodels, specifically B-PLMs.\nD NER finetunning dataset\nThe two publicly available name entity recognition\n(NER) datasets used for fintunning and evaluating\nour models are CAS and QUAERO NER datasets\nwhich are described in Table 7 and Table 8 respec-\ntively. We have selected the biomedical entities\n234\nFigure 3: Training time comparison between models using the ADAM and LAMB optimizer. The latter allows for\nfaster training but does not lead to better performance.\nModels’ performances on CAS dataset\nAliBERT AliBERT-LAMB\nEntities P R F1 P R F1\nSubstance 0.96 0.91 0.90 0.95 0.87 0.88\nSymptom 0.96 0.98 0.97 0.95 0.97 0.96\nAnatomy 0.97 0.97 0.98 0.97 0.95 0.96\nValue 0.98 0.99 0.98 0.92 0.81 0.86\nPathology 0.81 0.39 0.52 0.87 0.52 0.65\nTable 6: French Biomedical named entity recognition (NER) ADAM and LAMP optimizer comparison. Performance\nin bold is the best achieved for the entity in question and the metrics in question\nfrom the whole datasets.\n235\nFigure 4: MLM prediction examples and comparison between different Language Model for French Text. For each\nsentence where a word has been masked, the list of the first five most probable words according to the model are\ngiven. The colors show the position of the correct prediction, i.e. green is 1st, blue is 2nd, purple is 3rd and red\nindicates the correct word is not within the list.\nAnnotation Occurrences Description\nSubstance 2,009 Refers to the pharmacological substances used by the patient\n(drugs, commercial names and generics)\nSymptom 5,240 Entities that are used to make a diagnosis that reveals the pathology\nof the patient.\nAnatomy 4,780 Refers to all anatomical parts (arms, cells, cytoplasm, etc.)\nValue 1,743 Refers to values and units, grades, etc. corresponding to examina-\ntion results, or descriptions of Symptoms\nPathology 764 Concerns diseases and all that is pathological (adenocarcinoma,\ncarcinoma, fistula, etc.)\nTable 7: Number of annotations in CAS (NER) dataset used for evaluation\nAnnotation Occurrences Description\nAnatomy 1,464 A UMLS concept that refers to a particular part of the body\nChemical 1,028 Refers to chemicals and drugs inside and outside of the body, i.e.\nprotein, enzyme, clinical drug, etc.\nDevice 126 Includes all devices that are used in the biomedical domain i.e,\nmedical, drug delivery and medical devices\nDisorder 2,825 Refers to any abnormality or disease of the body. E.g, disease,\nsymptom, etc.\nProcedure 1,631 Refers to procedures and activities practices in the biomedical\ndomain.\nTable 8: Number of annotations in QUAERO-MEDLINE NER dataset used for evaluation\n236",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8547046184539795
    },
    {
      "name": "Language model",
      "score": 0.7415854930877686
    },
    {
      "name": "Natural language processing",
      "score": 0.6743592619895935
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.6731740236282349
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6503275632858276
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6382999420166016
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I154526488",
      "name": "Inserm",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210166444",
      "name": "Institut de Recherche pour le Développement",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210096450",
      "name": "Nutrition et obésité : approches systémiques",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I39804081",
      "name": "Sorbonne Université",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210153851",
      "name": "Unité de Modélisation Mathématique et Informatique des Systèmes Complexes",
      "country": "FR"
    }
  ],
  "cited_by": 4
}