{
  "title": "Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis",
  "url": "https://openalex.org/W4389524026",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2101011862",
      "name": "Hongyi Zheng",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2635299887",
      "name": "Abulhair Saparov",
      "affiliations": [
        "New York University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3159959439",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W2964048171",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W2963969878",
    "https://openalex.org/W2962800603",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4385570291",
    "https://openalex.org/W4306294746",
    "https://openalex.org/W4376654357",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W2076253536",
    "https://openalex.org/W4319049323",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4302305823",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W4378510422",
    "https://openalex.org/W4221143046"
  ],
  "abstract": "Recent advances in prompt engineering enable large language models (LLMs) to solve multi-hop logical reasoning problems with impressive accuracy. However, there is little existing work investigating the robustness of LLMs with few-shot prompting techniques. Therefore, we introduce a systematic approach to test the robustness of LLMs in multi-hop reasoning tasks via domain-agnostic perturbations. We include perturbations at multiple levels of abstractions (e.g. lexical perturbations such as typos, and semantic perturbations such as the inclusion of intermediate reasoning steps in the questions) to conduct behavioral analysis on the LLMs. Throughout our experiments, we find that models are more sensitive to certain perturbations such as replacing words with their synonyms. We also demonstrate that increasing the proportion of perturbed exemplars in the prompts improves the robustness of few-shot prompting methods.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 4560–4568\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nNoisy Exemplars Make Large Language Models More Robust:\nA Domain-Agnostic Behavioral Analysis\nHongyi Zheng\nNew York University\nhz2212@nyu.edu\nAbulhair Saparov\nNew York University\nas17582@nyu.edu\nAbstract\nRecent advances in prompt engineering enable\nlarge language models (LLMs) to solve multi-\nhop logical reasoning problems with impres-\nsive accuracy. However, there is little existing\nwork investigating the robustness of LLMs with\nfew-shot prompting techniques. Therefore, we\nintroduce a systematic approach to test the ro-\nbustness of LLMs in multi-hop reasoning tasks\nvia domain-agnostic perturbations. We include\nperturbations at multiple levels of abstractions\n(e.g. lexical perturbations such as typos, and\nsemantic perturbations such as the inclusion of\nintermediate reasoning steps in the questions)\nto conduct behavioral analysis on the LLMs.\nThroughout our experiments, we find that mod-\nels are more sensitive to certain perturbations\nsuch as replacing words with their synonyms.\nWe also demonstrate that increasing the pro-\nportion of perturbed exemplars in the prompts\nimproves the robustness of few-shot prompting\nmethods.\n1 Introduction\nLarge language models (LLMs) achieve human-\nlike performance on many natural language pro-\ncessing tasks after few-shot learning due to increas-\ning scale (Kaplan et al., 2020). However, they often\nstruggle in conducting multi-hop reasoning tasks\nafter standard prompting (Rae et al., 2021). Re-\ncently, multiple prompt engineering methods such\nas chain-of-thought prompting (Wei et al., 2023),\nzero-shot prompting (Kojima et al., 2023) and least-\nto-most-prompting (Zhou et al., 2023) have led to\nsignificant empirical improvements in these tasks.\nDespite these signs of progress, there is an impor-\ntant drawback in recent studies: the datasets used\nin these experiments are often idealized, noise-free,\nand rather distinct from examples that LLMs en-\ncounter in real applications, which put the general-\nizability of these prompting methods into question\nsince applications of LLMs in practice are often\nnoisy, containing errors, redundant or irrelevant\nsentences, utilizing out-of-distribution vocabulary,\netc. For instance, Cheng et al. (2018) show that\nFigure 1: A simple repetition may result in LLM\nproducing incorrect solution.\nvery small amount of common lexical perturbations\nsuch as word replacement and deletion could result\nin drastic change in machine translation results.\nThere are few existing studies investigating the\nrobustness of these prompting schemes through be-\nhavioral experiments on perturbed examples. Thus\nin our research, we create a selection of domain-\nagnostic tests to investigate the robustness of state-\nof-the-art prompting methods. Our two main goals\nare: (1) to compare and contrast the performance\nof prompting methods with respect to various per-\nturbations, and (2) to explore empirical approaches\nthat may improve their robustness.1\n2 Related Work\n2.1 Prompt Engineering\nContemporary prompt engineering methods that\naim to improve LLMs’ reasoning performance stem\nfrom chain-of-thought prompting (COT) proposed\nby Wei et al. (2023), which draws inspiration from\nthe earlier work of Ling et al. (2017) with the key\nidea of augmenting standard few-shot prompting\nwith a chain-of-thought (i.e. a description of the\nreasoning steps that lead to the answer). COT im-\nproves LLM’s performance in a wide range of rea-\nsoning tasks. Nevertheless, recent research shows\nthat it may lead to inconsistent reasoning steps\n1Our code is open source and available at https://gith\nub.com/Hiroki39/Noisy-Exemplars-Make-Large-Langu\nage-Models-More-Robust\n4560\nFigure 2: Examples for each type of perturbation test. A more detailed example is shown in Table 1.\nand thus worse performance under certain circum-\nstances (Ye and Durrett, 2022a), which highlights\nthe necessity to conduct further behavioral analy-\nses to identify such circumstances and find ways to\nmitigate this issue.\nCOT inspired a few subsequent prompting tech-\nniques. Zero-shot prompting (0COT) proposed by\nKojima et al. (2023) requires significantly less hu-\nman engineering to generate prompts compared\nwith the original approach. Least-to-most prompt-\ning (LTM) proposed by Zhou et al. (2023) decom-\nposes complex reasoning tasks into easier subprob-\nlems to improve the model performance. Selection-\nInference prompting (Creswell et al., 2023), LAM-\nBADA (Kazemi et al., 2023), and tree-of-thought\nprompting (Yao et al., 2023) further break down the\nproblem so that the LLM is queried for each step of\nthe reasoning. Lastly, self-consistency prompting\n(Wang et al., 2023) uses sampling and aggrega-\ntion techniques to diversify reasoning paths and\nincrease the chance of deriving correct answers.\nThese methods have been shown to be effective\nin increasing model accuracy under noise-free en-\nvironments. Our work, on the other hand, focuses\non investigating the robustness of these methods\nwhen a variety of perturbations are present.\n2.2 Behavioral Testing\nThe concept of behavioral testing (also known as\nblack-box testing) is first proposed by Beizer and\nWiley (1996) as an effective approach to probing\nlarge software or computer systems. Ribeiro et al.\n(2020) brought many of these insights to the test-\ning of NLP models and proposed CHECK LIST , a\ncomprehensive domain-agnostic methodology that\nembraces the benefits of challenge sets such as\nsystematic control of test examples (Belinkov and\nGlass, 2019) while avoiding their drawbacks such\nas the fact that challenge sets are artificial and do\nnot resemble real data.\nRecent behavioral analysis of LLMs has revealed\ntheir deficiencies when handling domain-specific\nperturbations. LLMs are shown to be sensitive\nto domain-specific perturbations in reading com-\nprehension (Jia and Liang, 2017), text classifi-\ncation (Gan and Mori, 2023), as well as logical\nreasoning tasks (Ye and Durrett, 2022b). Some\ntests investigate LLM consistency via replacing\nreal concept nouns with fictional ones (Saparov\nand He, 2023) and adding semantically related yet\nlogically irrelevant distractor sentences (Saparov\net al., 2023). While those studies mainly fo-\ncus on domain-specific perturbations, or on syn-\nthetic settings, our work evaluates the prompt\nengineering methods’ robustness against domain-\nagnostic perturbations applied to more realistic\ndata, across more levels of abstraction, which are\nmore widespread in the practical use of LLMs.\n3 Method\nWe aim to conduct domain-agnostic analysis\nwith tests that span multiple levels of abstraction,\nare easy to automate, while still closely resembling\nexamples in real applications. Thus, we use the\nfollowing four perturbation tests:\nTypo. We test whether the model’s output is sen-\nsitive to typing errors in its input by introducing\ntypos. To be robust to this perturbation, LLMs\ncannot rely on copying problem sentences with re-\npeated words to produce reasoning chains. Typos\nare introduced by randomly swapping one charac-\nter with its adjacent character within a token with\nprobability 0.1 given that the token has more than\none character and is not numeric.\nSynonym. We test whether models could recognize\nsemantically similar tokens that refer to the same\nobject of interest by replacing some words with\nsynonyms. To be robust to this perturbation, LLMs\nshould not exploit lexical spurious correlations. Op-\nerationally, every noun and verb token is replaced\nby one of its synonyms within its WordNet (Miller,\n1992) synsets with probability 0.2.2\nRepetition. We also test whether models are ro-\nbust to relevant but redundant information by dupli-\ncating a sentence in the input. To be robust to this\nperturbation, the model must ignore the redundant\nsentence, or utilize it to complete the CoT. Opera-\ntionally, we randomly choose a sentence within the\nproblem text other than the question sentence (i.e.\n2higher than Typo probability to ensure the number of\nperturbed tokens is comparable with Typo test\n4561\nFigure 3: Perturbations on the test question vs accuracy under various combinations of datasets and prompting\napproaches. 95% confidence intervals are shown.\nthe last sentence), and insert a copy of it before the\nlast sentence. This minimizes the risk of breaking\ncoreference links (Jia and Liang, 2017).\nShortcut. We test whether model behavior is\naffected if an intermediate result is given in the\nquestion description. The LLM may take advan-\ntage of this extra information to expedite reasoning.\nConversely, the LLM could also be confused, as it\nwould serve as redundant information if the LLM\nfirst ignored this directly given intermediate result\nbut later derived it. Operationally, we extract the\nfirst reasoning step for the problem (first hop for\nCOT and 0COT; first sub-problem and its corre-\nsponding answer for LTM) and insert it before the\nquestion sentence.\n4 Experimental Setup\n4.1 Model and Dataset\nThe GSM8K (Cobbe et al., 2021) and Strate-\ngyQA (Geva et al., 2021) datasets are used for all\nexperiments. The relatively simple problem state-\nments in these datasets facilitate the creation of\nadversarial examples for testing. The two datasets\nprovide the intermediate reasoning steps required\nby the few-shot prompting methods. For the choice\nof LLMs, GPT-3.5-Turbo (Brown et al., 2020),\nLLaMA2 7B and LLaMA2 13B models are used in\nour experiments. All models are open source with\nweights available for public use.\n4.2 Prompting Methods\nWe experiment withCOT, 0COT, and LTM. Our\nprompt design closely follows Zhou et al. (2023),\nKojima et al. (2023), and Shi et al. (2023). For few-\nshot prompting techniques (i.e. COT and LTM),\neach input prompt includes either one or eight ex-\nemplar problems as well as their corresponding\nsolution, followed by the test problem. The exem-\nplars are created from the problems within GSM8K\neither with or without perturbations to allow us bet-\nter understand how perturbing the exemplars would\naffect the performance of the LLMs in answering\nthe test problem. For 0COT, the test question is di-\nrectly presented without exemplars but instead fol-\nlowed by \"A: Let’s think step by step\". To facilitate\nanswer extraction, a high-level instruction, \" End\nyour response with ’The answer is <answer>’\"\nis prepended to every prompt. We do not apply\nLTM prompting for StrategyQA as it does not pro-\nvide high-quality decompositions for each question,\nwhich are required by LTM prompting.\n5 Results\nWe carry out two experiments. In the first ex-\nperiment, we evaluate the effect of perturbations\nof the test question on LLM performance under\ndifferent prompting methods. In the second ex-\nperiment, we perturb exemplar questions and vary\nthe proportion of perturbed exemplars for few-shot\nprompting techniques to investigate whether an in-\ncreasing share of perturbed exemplars would lead\nto better robustness against the perturbations in the\ntest question.\n5.1 Do Perturbations in the Test Question\nAffect Prompting Performance?\nIn this experiment, we investigate the effect of\nperturbations on prompting methods with original\nunperturbed exemplars. For COT and LTM trials,\none unperturbed question and its associated reason-\ning process are provided as the exemplar, followed\nby the perturbed test question, whereas no exem-\nplar is provided for 0COT trials. The results are\nillustrated in Figure 3. We draw a few conclusions\nfrom the results:\n4562\nFigure 4: Number of perturbed in-context exemplars vs accuracy for GSM8K trials. Note that the total number of\nin-context exemplars is always 8. Dashed red lines indicate the accuracy of 0COT in subsection 5.1. 95%\nconfidence intervals are shown.\nIn general, prompting methods are most sus-\nceptible toSynonym replacement. When conduct-\ning the Synonym tests, the accuracy of the LLM\nfalls below the baseline performance across trials.\nFor instance, for GPT-3.5-Turbo on the GSM8K\ndataset, Synonym perturbation lowers the accuracy\nby 0.097 ∼ 0.122 (green bars in the top-left sub-\nplot of Figure 3) as compared to the baseline (blue\nbars), which are larger decreases as compared\nto the Typo (0.034 ∼ 0.044, orange bars) and\nRepetition tests (0.046 ∼ 0.062, red bars). This\nmight result from low-frequency phrases and sen-\ntences created by the Synonym perturbation, as ev-\nery token is substituted with its synonym regardless\nof the adjacent tokens. Many substitutions would\nlikely create grammatically correct yet infrequent\nsentences and phrases (e.g. “She eats three repasts\nper day”) which might lead to worse performance.\n5.2 Do Perturbations in the Exemplars Affect\nFew-shot Prompting Performance?\nFollowing the previous experiment, a question\nof interest naturally arises: would perturbing the\nexemplars during the few-shot learning process im-\nproves the robustness of few-shot prompting meth-\nods? To answer this question, we conduct another\nexperiment for two few-shot prompting methods\nCOT and LTM. In each trial, 8 exemplars are pre-\nsented to the LLM, with a proportion of them ( 0,\n1, 2, 4, or 8 out of 8) being perturbed. Then, the\nperturbed test question is presented to the LLM.\nThe results are shown in Figure 4 and Figure 5. We\nfind the following key observations:\nIncreasing the proportion of perturbed ex-\nemplars improves few-shot prompting perfor-\nmance, except for theTypo perturbation. From\nthe results, the accuracy for both few-shot prompt-\ning methods trends upwards when the number\nof perturbed exemplars presented to the LLM in-\ncreases aside from LLaMA2-13B on the shortcut\nperturbation, demonstrating the evidence that the\nLLM is able to adapt to the perturbations through\nthe few-shot learning process. Take GPT-3.5-Turbo\non GSM8K dataset as an example, if excluding tri-\nals with the Typo perturbation, increasing the num-\nber of perturbed exemplars from 0 to 8 (lightest\nbars versus darkest bars in the third row of Fig-\nure 4) results in an average of 0.035 increase in\naccuracy, with the minimum increase of 0.011 and\nthe maximum of 0.114. The performance of the\nLLM in many trials surpasses the benchmark set by\nthe 0COT method in subsection 5.1 with the max-\n4563\nFigure 5: Number of perturbed in-context exemplars vs accuracy for StrategyQA trials. Note that we do not apply\nLTM on StrategyQA and all bars represent COT results. Dashed red lines indicate the accuracy of 0C OT in\nsubsection 5.1. 95% confidence intervals are shown.\nimum advantage of 0.048 achieved by COT after\nfew-shot learning with all eight exemplars with the\nSynonym perturbation. Typo trials turn out to be\nthe exception, where increasing the number of per-\nturbed exemplars does not lead to an improvement\nin accuracy. We speculate that this results from the\nfact that typos are much more common in the pre-\ntraining dataset as compared to other perturbations,\nwhich offsets the benefits of few-shot learning on\nperturbed exemplars.\nThis experiment suggests that perturbing in-\ncontext exemplars may serve as a more efficient\nalternative to augmenting pretraining with pertur-\nbations. It is much cheaper and easier to introduce\nperturbed exemplars at inference time rather than\nin pretraining.\n6 Discussion and Future Work\nIn conclusion, through two experiments, we con-\nducted an investigation of the robustness of state-of-\nthe-art prompting schemes via a series of domain-\nagnostic perturbation tests. Our first experiment\nrevealed the robustness of 0COT prompting versus\nfew-shot prompting methods when the few-shot\nexemplars are unperturbed. Our second experiment\ndemonstrated that perturbing few-shot exemplars\nled to notable improvements in robustness to per-\nturbations in the test question, which is valuable\nin real applications. Suppose a user knows in ad-\nvance that the questions of interest will be subject\nto some certain kind of perturbation (e.g. mathe-\nmatics problems scraped from the online forums\nwhich are subject to typos and uneliminated HTML\ntags, or questions asked by beginner-level English\nspeakers that may contain grammatical errors). The\nuser can then prepend exemplars perturbed in a sim-\nilar fashion to improve the robustness of the LLM\non their test examples. This can easily be applied in\nreal-world use cases as all perturbations discussed\nabove are easily automated.\nThere are a number of directions for future work\navailable: the first is to explore the effect of more\ncomprehensive perturbations, such as presenting\nthe LLM with a problem statement containing mul-\ntiple possible lines of reasoning that each lead to the\ncorrect conclusion, or adding some semantically\nrelated but logically irrelevant extra information to\nthe problem statement (Shi et al., 2023). Another\ndirection is to explore the influence of several other\nvariables on the robustness of the prompting meth-\nods, such as the model size, the number of hops\nrequired to answer the problem, whether the prob-\nlem is counterfactual, etc. Lastly, we observe that\nin some cases, though the LLM is able to produce\nthe correct answer under perturbation, the model\nis more likely to produce sentences with errors as\nshown in Table 1 (e.g. “Janet lay 16 eggs per day”).\nFurther studying the relationship between pertur-\nbations in the model’s input and the errors in the\noutput would be illuminating.\nLimitations\nWe acknowledge several limitations of our work.\nFirstly, due to the constraints in computational\npower, available time, and budget, we only ex-\nperiment with a relatively small set of LLMs and\ndatasets, and our results could benefit from ex-\nperiments on an increased number of models and\ndatasets with more diverse tasks. Secondly, some\nperturbations might introduce unintended side ef-\nfects. For instance, in the Repetition test, adding\nrepeated information might not always be idempo-\ntent, as repeating sentences like A man walked into\nthe barmight alter the correct answer to the prob-\nlem. These nuances and subtleties might require\nmore detailed manual inspection.\nReproducibility Statement\nIn our experiments, a fixed random seed (42) is\napplied over all trials to ensure the reproducibil-\nity of perturbed questions and exemplars, and the\n4564\nsampling temperature is set to 0 so that the model\nbehavior is as deterministic as possible. The exper-\niment output files as well as the analysis codes are\navailable in our GitHub repository (www.github.c\nom/Hiroki39/Noisy-Exemplars-Make-Large\n-Language-Models-More-Robust ).\nReferences\nB. Beizer and J. Wiley. 1996. Black box testing: Tech-\nniques for functional testing of software and systems.\nIEEE Software, 13(5):98–.\nYonatan Belinkov and James Glass. 2019. Analysis\nmethods in neural language processing: A survey.\nTransactions of the Association for Computational\nLinguistics, 7:49–72.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. CoRR,\nabs/2005.14165.\nYong Cheng, Zhaopeng Tu, Fandong Meng, Junjie Zhai,\nand Yang Liu. 2018. Towards robust neural machine\ntranslation. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1756–1766, Mel-\nbourne, Australia. Association for Computational\nLinguistics.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems.\nAntonia Creswell, Murray Shanahan, and Irina Higgins.\n2023. Selection-inference: Exploiting large language\nmodels for interpretable logical reasoning. In The\nEleventh International Conference on Learning Rep-\nresentations.\nChengguang Gan and Tatsunori Mori. 2023. Sensitivity\nand robustness of large language models to prompt\ntemplate in japanese text classification tasks.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics, 9:346–\n361.\nRobin Jia and Percy Liang. 2017. Adversarial exam-\nples for evaluating reading comprehension systems.\nIn Proceedings of the 2017 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2021–2031, Copenhagen, Denmark. Association for\nComputational Linguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models.\nMehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu,\nand Deepak Ramachandran. 2023. Lambada: Back-\nward chaining for automated reasoning in natural\nlanguage.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2023. Large lan-\nguage models are zero-shot reasoners.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word\nproblems. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 158–167, Vancouver,\nCanada. Association for Computational Linguistics.\nGeorge A. Miller. 1992. WordNet: A lexical database\nfor English. In Speech and Natural Language: Pro-\nceedings of a Workshop Held at Harriman, New York,\nFebruary 23-26, 1992.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, H. Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, Antonia\nCreswell, Nat McAleese, Amy Wu, Erich Elsen, Sid-\ndhant M. Jayakumar, Elena Buchatskaya, David Bud-\nden, Esme Sutherland, Karen Simonyan, Michela Pa-\nganini, Laurent Sifre, Lena Martens, Xiang Lorraine\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou,\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\npoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-\ntiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,\nDaniel Toyama, Cyprien de Masson d’Autume, Yujia\nLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\nAidan Clark, Diego de Las Casas, Aurelia Guy,\nChris Jones, James Bradbury, Matthew J. Johnson,\nBlake A. Hechtman, Laura Weidinger, Iason Gabriel,\nWilliam S. Isaac, Edward Lockhart, Simon Osindero,\nLaura Rimell, Chris Dyer, Oriol Vinyals, Kareem\nAyoub, Jeff Stanway, Lorrayne Bennett, Demis Hass-\nabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021.\nScaling language models: Methods, analysis & in-\nsights from training gopher. CoRR, abs/2112.11446.\n4565\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,\nand Sameer Singh. 2020. Beyond accuracy: Be-\nhavioral testing of NLP models with CheckList. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4902–\n4912, Online. Association for Computational Lin-\nguistics.\nAbulhair Saparov and He He. 2023. Language models\nare greedy reasoners: A systematic formal analysis\nof chain-of-thought.\nAbulhair Saparov, Richard Yuanzhe Pang, Vishakh Pad-\nmakumar, Nitish Joshi, Seyed Mehran Kazemi, Na-\njoung Kim, and He He. 2023. Testing the general\ndeductive reasoning capacity of large language mod-\nels using ood examples.\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan\nScales, David Dohan, Ed Chi, Nathanael Schärli, and\nDenny Zhou. 2023. Large language models can be\neasily distracted by irrelevant context.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,\nEd H. Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023. Self-consistency improves\nchain of thought reasoning in language models. In\nInternational Conference on Learning Representa-\ntions.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L. Griffiths, Yuan Cao, and Karthik\nNarasimhan. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models.\nXi Ye and Greg Durrett. 2022a. The unreliability of ex-\nplanations in few-shot prompting for textual reason-\ning. In Advances in Neural Information Processing\nSystems.\nXi Ye and Greg Durrett. 2022b. The unreliability of\nexplanations in few-shot prompting for textual rea-\nsoning.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nClaire Cui, Olivier Bousquet, Quoc Le, and Ed Chi.\n2023. Least-to-most prompting enables complex rea-\nsoning in large language models.\n4566\nAppendices\nA Test Question and Model Output Example\nInput Question Model Output\nCOT 0COT LTM\nOriginal\nJanet’s ducks lay 16 eggs per\nday. She eats three for break-\nfast every morning and bakes\nmuffins for her friends every\nday with four. She sells the re-\nmainder at the farmers’ market\ndaily for $2 per fresh duck egg.\nHow much in dollars does she\nmake every day at the farmers’\nmarket?\nJanet uses 3+4=7 eggs ev-\nery day for breakfast and\nmuffins. So she has 16-\n7=9 eggs left to sell at\nthe farmers’ market. She\nmakes 9*2=$18 every day\nat the farmers’ market.\nThe answer is 18.\nJanet has 16 eggs per day.\nShe eats 3 for breakfast\nand uses 4 for muffins,\nwhich leaves her with 9\neggs. She sells these 9\neggs at $2 per egg, which\nmeans she makes $18 per\nday at the farmers’ mar-\nket. The answer is $18.\nHow many eggs does Janet use\nfor breakfast and muffins every\nday? Janet uses 3+4=7 eggs ev-\nery day. How many eggs does\nshe have left to sell? So she has\n16-7=9 eggs left to sell every day.\nHow much money does she make\nselling the eggs? Selling each\negg for $2, she makes 9*2=$18\nevery day at the farmers’ market.\nThe answer is 18.\nTypo\nJanet’s ducks aly 16 eggs per\nday. She eats three for break-\nfast every omrning and bakes\nmuffnis for her friends every\nday with four. She sells the re-\nmainder at the farmers’ market\ndaily ofr $2 per fresh duck egg.\nHow much in dollars does she\nmake every day at the farmers’\nmarket?\nJanet uses 3 eggs for\nbreakfast and 4 for\nmuffins, so she uses\n3+4=7 eggs per day.\nThat means she has\n16-7=9 eggs left to sell\nat the farmers’ market.\nSelling each egg for\n$2, she makes 9*2=$18\nevery day at the farmers’\nmarket. The answer is\n18.\nJante has 16 eggs per day,\nshe eats 3 for breakfast\nand uses 4 for muffins,\nwhich leaves her with 9\neggs. She sells these 9\neggs at $2 per egg, which\nmeans she makes $18 ev-\nery day at the farmers’\nmarket. The answer is\n$18.\nHow many eggs does Janet use\nfor breakfast every day? Janet\nuses 3 eggs for breakfast every\nday. How many eggs does Janet\nuse for muffins every day? Janet\nuses 4 eggs for muffins every day.\nHow many eggs does Janet have\nleft to sell? Janet has 16 - 3\n- 4 = 9 eggs left to sell. How\nmuch money does Janet make ev-\nery day at the farmers’ market?\nJanet makes 9 x $2 = $18 every\nday at the farmers’ market. The\nanswer is $18.\nSynonym\nJanet’s ducks lay 16 eggs per\nday. She eats three for break-\nfast every morning and bakes\nmuffins for her friends every\nday with four. She sells the\nresidue at the farmers’ market\ndaily for $2 per fresh duck egg.\nHow much in dollars does she\nmake every sidereal day at the\nfarmers’ market?\nJanet lays 16 eggs per day,\nand she eats 3 + 4 = 7\neggs per day. So, she has\n16 - 7 = 9 eggs left to\nsell at the farmers’ mar-\nket. She makes 9 x $2 =\n$18 every day at the farm-\ners’ market. The answer\nis 18.\nJanet has 16 duck eggs\nper day. She eats 3\nfor breakfast and bakes\nmuffins with 4, which\nleaves her with 9 eggs.\nShe sells the remaining 9\neggs at the farmers’ mar-\nket for $2 each, which\nmeans she makes $18 per\nday. The answer is $18.\nHow many orchids does Janet\nhave left after breakfast and bak-\ning muffins? Janet has 16 - 3 -\n4 = 9 orchids left. How much\nmoney does she make from sell-\ning these orchids? She makes 9\nx $2 = $18 from selling the or-\nchids. How much money does\nJanet make every day at the farm-\ners’ market? So Janet makes $18\nevery day at the farmers’ market.\nThe answer is 18.\nRepetition\nJanet’s ducks lay 16 eggs per\nday. She eats three for break-\nfast every morning and bakes\nmuffins for her friends every\nday with four. She sells the\nremainder at the farmers’ mar-\nket daily for $2 per fresh duck\negg. Janet’s ducks lay 16 eggs\nper day. How much in dollars\ndoes she make every day at the\nfarmers’ market?\nJanet uses 3+4=7 eggs ev-\nery day for breakfast and\nmuffins. So she has 16-\n7=9 eggs left to sell at\nthe farmers’ market. She\nmakes 9*2=$18 every day\nat the farmers’ market.\nThe answer is 18.\nJanet has 16 eggs per day.\nShe eats 3 for breakfast\nand uses 4 for muffins,\nwhich leaves her with 9\neggs. She sells these 9\neggs at $2 per egg, which\nmeans she makes $18 ev-\nery day at the farmers’\nmarket. The answer is\n$18.\nHow many eggs does Janet use\nfor breakfast and muffins every\nday? Janet uses 3+4=7 eggs ev-\nery day. How many eggs does\nshe have left to sell? So she has\n16-7=9 eggs left to sell every day.\nHow much money does she make\nselling the eggs? Selling each\negg for $2, she makes 9*2=$18\nevery day at the farmers’ market.\nThe answer is 18.\n4567\nShortcut\nJanet’s ducks lay 16 eggs per\nday. She eats three for break-\nfast every morning and bakes\nmuffins for her friends every\nday with four. She sells the re-\nmainder at the farmers’ market\ndaily for $2 per fresh duck egg.\nJanet sells 16 - 3 - 4 = 9 duck\neggs a day. How much in dol-\nlars does she make every day\nat the farmers’ market?\nJanet sells 9 duck eggs\na day, so she makes 9 x\n2 =18 every day at the\nfarmers’ market. The an-\nswer is $18.\nJanet sells 9 duck eggs a\nday, so she makes 9 x $2\n= $18 every day at the\nfarmers’ market. The an-\nswer is $18.\nHow many eggs does Janet sell?\nJanet sells 9 duck eggs a day.\nHow much in dollars does she\nmake every day at the farmers’\nmarket? She makes 9 x 2 =18\nevery day at the farmers’ market.\nThe answer is 18.\nTable 1: An example of different perturbations and corresponding model outputs. The first column illustrates the\noriginal and perturbed example test question and the next three columns show the model outputs with COT, 0COT,\nand LTM respectively.\n4568",
  "topic": "Robustness (evolution)",
  "concepts": [
    {
      "name": "Robustness (evolution)",
      "score": 0.7760473489761353
    },
    {
      "name": "Computer science",
      "score": 0.6740607619285583
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4469095766544342
    },
    {
      "name": "Natural language processing",
      "score": 0.3565889000892639
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    }
  ],
  "cited_by": 2
}