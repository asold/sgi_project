{
  "title": "Radio transformer networks: Attention models for learning to synchronize in wireless systems",
  "url": "https://openalex.org/W2346584795",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A4227131048",
      "name": "Timothy J O'Shea",
      "affiliations": [
        "Virginia Tech"
      ]
    },
    {
      "id": "https://openalex.org/A2545530297",
      "name": "Latha Pemula",
      "affiliations": [
        "Virginia Tech"
      ]
    },
    {
      "id": "https://openalex.org/A2098683697",
      "name": "Dhruv Batra",
      "affiliations": [
        "Virginia Tech"
      ]
    },
    {
      "id": "https://openalex.org/A2158851322",
      "name": "T. Charles Clancy",
      "affiliations": [
        "Virginia Tech"
      ]
    },
    {
      "id": "https://openalex.org/A4227131048",
      "name": "Timothy J O'Shea",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2545530297",
      "name": "Latha Pemula",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098683697",
      "name": "Dhruv Batra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2158851322",
      "name": "T. Charles Clancy",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6682137061",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W6674330103",
    "https://openalex.org/W6618372016",
    "https://openalex.org/W2152175008",
    "https://openalex.org/W2272847350",
    "https://openalex.org/W2149256602",
    "https://openalex.org/W603908379",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2951759938",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2951527505"
  ],
  "abstract": "We introduce learned attention models into the radio machine learning domain for the task of modulation recognition by leveraging spatial transformer networks and introducing new radio domain appropriate transformations. This attention model allows the network to learn a localization network capable of synchronizing and normalizing a radio signal blindly with zero knowledge of the signals structure based on optimization of the network for classification accuracy, sparse representation, and regularization. Using this architecture we are able to outperform our prior results in accuracy vs signal to noise ratio against an identical system without attention, however we believe such an attention model has implication far beyond the task of modulation recognition.",
  "full_text": "Radio Transformer Networks: Attention Models for\nLearning to Synchronize in Wireless Systems\nTimothy J. O’Shea\nVirginia Tech\nArlington, V A\noshea@vt.edu\nLatha Pemula\nVirginia Tech\nBlacksburg, V A\nlpemula@vt.edu\nDhruv Batra\nVirginia Tech\nBlacksburg, V A\ndbatra@vt.edu\nT. Charles Clancy\nVirginia Tech\nArlington, V A\ntcc@vt.edu\nAbstract—We introduce learned attention models into the ra-\ndio machine learning domain for the task of modulation recogni-\ntion by leveraging spatial transformer networks and introducing\nnew radio domain appropriate transformations. This attention\nmodel allows the network to learn a localization network capable\nof synchronizing and normalizing a radio signal blindly with zero\nknowledge of the signal’s structure based on optimization of the\nnetwork for classiﬁcation accuracy, sparse representation, and\nregularization. Using this architecture we are able to outperform\nour prior results in accuracy vs signal to noise ratio against an\nidentical system without attention, however we believe such an\nattention model has implication far beyond the task of modulation\nrecognition.\nKeywords— Radio Transformer Networks, Radio communica-\ntions, Software Radio, Cognitive Radio, Deep Learning, Con-\nvolutional Autoencoders, Neural Networks, Machine Learning,\nAttention Models, Spatial Transformer Networks, Synchronization,\nRadioML, Signal Processing\nI. I NTRODUCTION\nCognitive radio and signal processing in general has long\nrelied on a relatively well deﬁned set of expert systems and\nexpert knowledge to operate. Unfortunately in the realization\nof cognitive radio, this has greatly limited the ability of systems\nto generalize and perform real learning and adaptation to\nnew and unknown signals and tasks. By approaching signal\nrecognition, synchronization, and reasoning from a ground-up\nfeature learning angle, we seek to be able to build cognitive\nradio systems which truly generalize and adapt without running\ninto barriers of expert knowledge such as many of the current\nday solutions which address more narrowly scoped problems.\nIn our prior work, we looked at the application of deep\nconvolutional neural networks to the task of modulation recog-\nnition [9] through blind feature learning on time domain radio\nsignals. We were able to achieve excellent classiﬁcation per-\nformance at both low and high SNR by learning time domain\nfeatures directly from a dataset with harsh channel impairments\n(oscillator drift, clock drift, fading, noise). However we had\nno notion of attention in this work and instead forced the\ndiscriminative network to learn features invariant to each of\nthese channel effects. In communications receivers (and many\niterative expert modulation classiﬁcation algorithms), we typi-\ncally perform synchronization on the signal before performing\nadditional signal processing steps. This synchronization can\nbe thought of as a form of attention which estimates a time,\nfrequency, phase, and sample timing offset in order to create\na normalized version of the signal.\nAttention models have recently been gaining widespread\nadoption in the computer vision community for a number\nof important reasons. They introduce a learned model for\nattention capable of removing numerous variances and para-\nmetric search spaces in the input data and focuses on the\ntask of extracting a canonical form attention patch with these\nvariations removed to make downstream tasks easier and of\nlower complexity. These were ﬁrst introduced as recurrent\nnetworks [4] which were quite expensive, but have made\nsigniﬁcant progress since then.\nFigure 1. Generalized Transformer Network Architecture\nSpatial transformer networks (STNs) [8] were recently\nintroduced to provide an end to end feed-forward model of\nattention which can be trained directly from loss on each\ntraining example and compactly evaluated on new samples.\nThey consist of a trained Localization network which performs\nparameter regression, a ﬁxed parametric transform operation,\nand a trained discriminative classiﬁer to select a class estimate.\nIn the image domain, where these have so far been applied, a\n2D Afﬁne transform is used to extract an attention patch which\nis shifted, scaled, and rotated in 2-space from the original\nimage according to a 2x3 parameter vector θ, generalized in\nﬁgure 1.\nIn this work, we propose a Radio Transformer Network\n(RTN), which leverages the generalization of the STN ar-\nchitecture, but introduces radio-domain speciﬁc parametric\ntransforms. This attention model can be used to learn directly\nhow to synchronize in wireless systems, and enables our\nmodulation recognition system to outperform the attention-\nless version of itself by assisting in the normalization of the\narXiv:1605.00716v1  [cs.LG]  3 May 2016\nreceived signal prior to classiﬁcation. By constructing this\nnormalized received signal with an attention model we greatly\nsimplify the task of the discriminative network by relaxing\nthe requirements on various variations of the received signal\nit must recognize, reducing the complexity and increasing the\nperformance necessary in a discriminative network.\nThis is an important result in modulation recognition but\nalso more widely in radio communications and signal pro-\ncessing, as it demonstrates that we can learn to synchronize\nrather than relying on expert systems and estimators derived\nthrough a costly analytic process. We believe attention models\nwill play an important and wide-spread role in forthcoming\nmachine learning based signal processing systems.\nII. L EARNING TO CLASSIFY SIGNALS\nFigure 2. Original ConvNet Architecture without Attention\nIn prior work [9] we compare supervised learning using\na deep convolutional network with no expert features, to a\nhandful of widely used machine learning techniques on expert\nsignal amplitude, phase, and envelope moments. We used the\narchitecture shown in ﬁgure 2 using a convolutional frontend,\na dense backend, and a softmax with categorical cross-entropy\ntraining using Adam/SGD against a synthetic dataset.\nIn ﬁgure 3 we summarize the results of that experiment\nwithout an attention model, demonstrating a signiﬁcant im-\nprovement over the moment based features and conventional\nclassiﬁers.\nThis is an exciting result as it demonstrates that feature\nlearning on raw RF data does work, and in this case it is\nworking better than conventional widely used expert features!\nIII. U SING ATTENTION TO SYNCHRONIZE\nTo effectively synchronize to a wireless signal, we must\ndevelop a transform which, with the right parameters is able to\ncorrect for channel induced variation. Within the scope of this\npaper, we consider channel variation due to time offset, time\ndilation, frequency offset, and phase offset. These are effects\nwhich exist in any real system containing transmitters and\nreceivers whose oscillators and clocks are not locked together.\nFor now we do not address the problem of fading, but we\nFigure 3. Original ConvNet Performance vs Expert Statistics\nbelieve the correction of fading through equalization has the\npotential to also be addressed as an attention model, either\njointly or subsequently to the transformations addressed here.\nA. Timing and Symbol-Rate Recovery\nTiming and symbol-rate recovery are relatively straightfor-\nward processes involving the re-sampling of the input signal at\nthe correct starting offset and sampling increment. This is very\nmuch akin to the extraction of visual pixels at the correct offset\nof a 1D Afﬁne transformation, and so we treat it as such by\ndirectly leveraging the Afﬁne transformation used in the image\ndomain. We represent our data as a 2D image, with a two rows\ncontaining I/Q and an N columns containing samples in time.\nA full 2D Afﬁne transformation allows for translation,\nrotation, and scaling in 2-dimensions given by a 2x6 element\nparameter vector. To restrict this to 1-Dimensional translation\nand scaling in the time dimension, we can simply introduce\nthe following mask 1 and then readily use 2D Afﬁne transform\nimplementations from the image domain.\n[\nθ0 0 θ2\n0 θ1 0\n]\n(1)\nB. Phase and Frequency Offset Recovery\nPhase and frequency offset recovery is a task which doesn’t\nhave an immediate analogue in the vision domain. However\nthis transform in signal processing is relatively straightforward.\nWe simply mix our signal with a complex sinusoid with the\nproper initial phase and frequency as deﬁned by two new\nunknown parameters.\nyn = xn ∗ exp(nθ3 + θ4) (2)\nWe directly implement this transform as a new layer in\nKeras (on top of Theano and Tensorﬂow), and cascade it\nbefore the Afﬁne transform in the Transformer module of our\nnetwork.\nC. Parameter Estimation\nThe task of synchronization now becomes the task of\nparameter estimation of θi values passed into our transformer\nmodule. We experimentally try a number of different neural\nnetwork architectures for performing this parameter regression\ntask, and ultimately introduce two new domain appropriate\nlayers into Keras to help assist in their estimation.\n1) Complex Convolution 1D Layer: Complex neural net-\nworks are not widely used and still faced some theoretical\nissues especially in automated differentiation, so we represent\nour signal as a two row 2D matrix with the real component in\nthe ﬁrst row and imaginary component in the second row. In\ntheory real valued convolutions in a neural network can learn\nthe relationship between these components to some extent, but\nby introducing a complex convolution operation, we simplify\nthe learning task and ensure that we learn a ﬁlter with the\nproperties we are used to working with.\nFor a complex valued input vector X of size 2xN, we deﬁne\na weight vector W of M complex ﬁlters each 2xK in length.\nWe may then compute the output for each of the k output\nvalues as.\n[\nconv(X0,:,Wk,0,:) − conv(X1,:,Wk,1,:)\nconv(X0,:,Wk,1,:) +conv(X1,:,Wk,0,:)\n]\n(3)\nThis allows us to leverage existing, highly optimized real\nconvolution operations and obtain a differentiable operation\nwhich can be trained with back-propagation.\n2) Complex to Power and Phase: Creating a differentiable\nCartesian to Polar operation which makes it easier for the\nnetwork to operate directly on input phase and magnitude is\nslightly more involved. We compute magnitude squared simply\nas mn = pow(x0,n,2) +pow(x1,n,2), but for phase compu-\ntation we use a simpliﬁed and differentiable approximation of\natan2 without conditionals implemented in Keras on top of\nTheano and Tensorﬂow.\nz = Xq/K.clip(K.abs(Xi),1e-3,1e6)\nz = z*K.sign(xi)\nzd1 = 1+0.28*K.pow(z,2)\nz1 = z/zd1 + PI*K.sign(Xq)\nzd2 = K.pow(z,2) + 0.28\nz2 = z/zd2 + (K.sign(Xq)-1)*0.5*PI\nzc = K.abs(z) - 1\natan2 = (K.sign(zc)-1)*(-0.5)*z1 +\n(K.sign(zc)+1)*0.5*z2\n3) Network Architecture: We evaluate dozens of localiza-\ntion network architectures with slight variation between dense\nconnections, convolutional layers, and complex-convolutional\nlayers, using various activation functions, and achieve our best\nperformance using the composite network shown in ﬁgure\n4. This uses both the complex convolutional layer and the\ncomplex to polar layers within the localization network and\nan identical discriminative network to the one we used without\nattention in front of it for comparison.\n4) Training Details: We train this network using Keras [7]\non top of Theano [2] and TensorFlow [6] using an NVidia\nGeforce Titan X inside a Digits Devbox. We use dropout [5]\nof 0.5 between each layer for regularization, and the Adam [3]\nmethod of stochastic gradient descent to ﬁt network parameters\non our training set.\nWe train with a batch size of 1024 and an initial learning\nrate of 0.001. We train for roughly 350 epochs, reducing our\nlearning rate by half each validation loss stops decreasing.\nTraining takes about 3 hours on a Titan X GPU, but feed-\nforward evaluation or signal classiﬁcation takes less than 10ms.\nIV. D ATA-SET AND METHODOLOGY\nFor our classiﬁer performance evaluation, in this work and\nin prior work, we leverage the RadioML.com 2016.04C open\nsource dataset and perform at 60/40 split between train and test\nsets. This consists of 11 modulations (8 digital and 3 analog) at\nvarying SNR levels, with random walk simulations on center\nfrequency, sample clock rate, sample clock offset, and initial\nphase, as well as limited multi-path fading. We believe it is\ncritically important to test with real channel effects early to\nensure realistic assumptions early in our models. This dataset\nis labeled with SNR and Modulation-Type, when performing\nsupervised training we use only the modulation-type labels\nof the training set, and evaluate the classiﬁcation accuracy\nperformance at each SNR label step for the test set.\nTraining and validation loss along with learning rate are\nshown throughout the trainin in ﬁgure 5.\nFigure 5. Training Loss and Learning Rate\nV. C LASSIFICATION PERFORMANCE\nEvaluating the performance of the RTN on the test set, we\nobtain slightly increased performance over the model without\nattention. Similar accuracies are obtained at slightly lower SNR\nvalues ( 1dB) and high SNR performance is slightly improved\nand more stable as shown in ﬁgure 6.\nWe suspect the complexity of the discriminative network\ncould now be reduced due to lower complexity of the normal-\nized signal but we do not investigate this work further here for\nfair comparison of the same discriminative network.\nFigure 4. Radio Transformer Network Architecture\nFigure 6. Radio Transformer Network Performance\nPerformance of the convolutional neural network without\nattention is also improved from our prior work by increasing\ndropout and better learning rate policy to match that used in\nour RTN training. This is reﬂected in ﬁgure 6.\nVI. A TTENTION LEARNING PERFORMANCE\nWe have shown that classiﬁcation performance is improved\nwhile using a radio localization network to extract normalized\npatches on the dataset. However, we can not only look at\nclassiﬁcation performance, it is also interesting to look at the\nradio sample data before and after transformation to observe\nany normalization that has occurred. It is difﬁcult to visualize\nexactly what has occured here, looking at time domain data\ndoes not yield clean, obvious performance improvement on the\ndata. Upon attempting to plot an eye-diagram of the QPSK-\nclass signals, it is clear that synchronizaiton is at this point\nstill horribly noisy and partial. However, if we plot the the\nconstellation density for 50 test examples over a range of 20\nFigure 7. Performance of the RTN at High SNR (18dB)\nFigure 8. Density Plots of Pre- and Post-Transformed Input Constellations\ntime samples each, shown in ﬁgure 8, we can start to see a bit\nmore density forming around the constellation points vs what\nwe started with, which is a good sign to start with.\nClearly much work needs to be done to improve and\nquantify synchronization performance, in-fact we have no real\nreason to expect perfect synchronization from a classiﬁcation\ntask, just enough normalization to make things easier on the\ndiscriminative network. We will continue to investigate this\narea, and additional tasks other than modulation recognition\nwhich may improve the synchronization properties for demod-\nulation beyond what has been achieved here.\nVII. C ONCLUSIONS\nBy developing a feed forward model for radio attention, we\nhave demonstrated that we can effectively learn to synchronize\nusing deep convolutional neural networks with domain speciﬁc\ntransforms and layer conﬁgurations. Normalizing out time,\ntime-dilation, frequency and phase offsets using learned esti-\nmators does effectively improve our modulation classiﬁcation\nperformance and requires no expert knowledge about the\nsignals of interest to train.\nWhile the training complexity of such a network is high,\nthe feed-forward execution of it is actually quite compact\nand viable for real-world use and deployment. Platforms such\nas the TX1 with highly parallel, low clock rate GPGPU\narchitectures further enable the low-SWaP deployment of these\nalgorithms for which they are exceptionally well suited.\nACKNOWLEDGMENTS\nThe authors would like to thank the Bradley Department\nof Electrical and Computer Engineering at the Virginia Poly-\ntechnic Institute and State University,the Machine Learning &\nPerception Group, the Hume Center, and DARPA all for their\ngenerous support in this work.\nThis research was developed with funding from the De-\nfense Advanced Research Projects Agency’s (DARPA) MTO\nOfﬁce under grant HR0011-16-1-0002. The views, opinions,\nand/or ﬁndings expressed are those of the author and should\nnot be interpreted as representing the ofﬁcial views or policies\nof the Department of Defense or the U.S. Government.\nREFERENCES\n[1] C. Clancy, J. Hecker, E. Stuntebeck, and T. O’Shea,\n“Applications of machine learning to cognitive radio\nnetworks”, Wireless Communications, IEEE, vol. 14, no.\n4, pp. 47–52, 2007.\n[2] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R.\nPascanu, G. Desjardins, J. Turian, D. Warde-Farley, and\nY . Bengio, “Theano: a CPU and GPU math expres-\nsion compiler”, in Proceedings of the Python for Sci-\nentiﬁc Computing Conference (SciPy) , Oral Presentation,\nAustin, TX, Jun. 2010.\n[3] D. Kingma and J. Ba, “Adam: a method for stochastic\noptimization”, arXiv preprint arXiv:1412.6980 , 2014.\n[4] V . Mnih, N. Heess, A. Graves, et al., “Recurrent models\nof visual attention”, in Advances in Neural Information\nProcessing Systems, 2014, pp. 2204–2212.\n[5] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, “Dropout: a simple way to prevent neu-\nral networks from overﬁtting”, The Journal of Machine\nLearning Research, vol. 15, no. 1, pp. 1929–1958, 2014.\n[6] M. Abadi, A. Agarwal, et al., TensorFlow: large-scale\nmachine learning on heterogeneous systems , Software\navailable from tensorﬂow.org, 2015. [Online]. Available:\nhttp://tensorﬂow.org/.\n[7] F. Chollet, Keras, https://github.com/fchollet/keras, 2015.\n[8] M. Jaderberg, K. Simonyan, A. Zisserman, et al., “Spatial\ntransformer networks”, in Advances in Neural Informa-\ntion Processing Systems , 2015, pp. 2008–2016.\n[9] T. J. O’Shea and J. Corgan, “Convolutional radio modula-\ntion recognition networks”, CoRR, vol. abs/1602.04105,\n2016. [Online]. Available: http : / / arxiv. org / abs / 1602 .\n04105.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7357823848724365
    },
    {
      "name": "Transformer",
      "score": 0.602389395236969
    },
    {
      "name": "Synchronizing",
      "score": 0.6018698811531067
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.4948241114616394
    },
    {
      "name": "Artificial intelligence",
      "score": 0.486640065908432
    },
    {
      "name": "Wireless",
      "score": 0.4454204738140106
    },
    {
      "name": "Machine learning",
      "score": 0.4344823956489563
    },
    {
      "name": "Deep learning",
      "score": 0.4319510757923126
    },
    {
      "name": "Speech recognition",
      "score": 0.3568897843360901
    },
    {
      "name": "Telecommunications",
      "score": 0.14386317133903503
    },
    {
      "name": "Engineering",
      "score": 0.09934389591217041
    },
    {
      "name": "Transmission (telecommunications)",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I859038795",
      "name": "Virginia Tech",
      "country": "US"
    }
  ],
  "cited_by": 13
}