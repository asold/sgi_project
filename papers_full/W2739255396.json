{
  "title": "On the State of the Art of Evaluation in Neural Language Models",
  "url": "https://openalex.org/W2739255396",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Melis, G\\'abor",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221974744",
      "name": "Dyer, Chris",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2900299007",
      "name": "Blunsom, Phil",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287957930",
      "name": "Melis, Gábor",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1499864241",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W2732547613",
    "https://openalex.org/W2514713644",
    "https://openalex.org/W2042204882",
    "https://openalex.org/W2963266340",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2757047188",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W1771459135",
    "https://openalex.org/W2473934411",
    "https://openalex.org/W2754517384",
    "https://openalex.org/W2616619952",
    "https://openalex.org/W2950059857",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W1810943226"
  ],
  "abstract": "Ongoing innovations in recurrent neural network architectures have provided a steady influx of apparently state-of-the-art results on language modelling benchmarks. However, these have been evaluated using differing code bases and limited computational resources, which represent uncontrolled sources of experimental variation. We reevaluate several popular architectures and regularisation methods with large-scale automatic black-box hyperparameter tuning and arrive at the somewhat surprising conclusion that standard LSTM architectures, when properly regularised, outperform more recent models. We establish a new state of the art on the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the Hutter Prize dataset.",
  "full_text": "Under review as a conference paper at ICLR 2018\nON THE STATE OF THE ART OF EVALUATION IN\nNEURAL LANGUAGE MODELS\nG´abor Melis†, Chris Dyer†, Phil Blunsom†‡\n{melisgl,cdyer,pblunsom}@google.com\n†DeepMind\n‡University of Oxford\nABSTRACT\nOngoing innovations in recurrent neural network architectures have provided a\nsteady inﬂux of apparently state-of-the-art results on language modelling bench-\nmarks. However, these have been evaluated using differing codebases and limited\ncomputational resources, which represent uncontrolled sources of experimental\nvariation. We reevaluate several popular architectures and regularisation meth-\nods with large-scale automatic black-box hyperparameter tuning and arrive at the\nsomewhat surprising conclusion that standard LSTM architectures, when properly\nregularised, outperform more recent models. We establish a new state of the art\non the Penn Treebank and Wikitext-2 corpora, as well as strong baselines on the\nHutter Prize dataset.\n1 I NTRODUCTION\nThe scientiﬁc process by which the deep learning research community operates is guided by em-\npirical studies that evaluate the relative quality of models. Complicating matters, the measured\nperformance of a model depends not only on its architecture (and data), but it can strongly depend\non hyperparameter values that affect learning, regularisation, and capacity. This hyperparameter\ndependence is an often inadequately controlled source of variation in experiments, which creates a\nrisk that empirically unsound claims will be reported.\nIn this paper, we use a black-box hyperparameter optimisation technique to control for hyperpa-\nrameter effects while comparing the relative performance of language modelling architectures based\non LSTMs, Recurrent Highway Networks (Zilly et al., 2016) and NAS (Zoph & Le, 2016). We\nspecify ﬂexible, parameterised model families with the ability to adjust embedding and recurrent\ncell sizes for a given parameter budget and with ﬁne grain control over regularisation and learning\nhyperparameters.\nOnce hyperparameters have been properly controlled for, we ﬁnd that LSTMs outperform the more\nrecent models, contra the published claims. Our result is therefore a demonstration that replication\nfailures can happen due to poorly controlled hyperparameter variation, and this paper joins other\nrecent papers in warning of the under-acknowledged existence of replication failure in deep learn-\ning (Henderson et al., 2017; Reimers & Gurevych, 2017). However, we do show that careful controls\nare possible, albeit at considerable computational cost.\nSeveral remarks can be made in light of these results. First, as (conditional) language models serve\nas the central building block of many tasks, including machine translation, there is little reason to\nexpect that the problem of unreliable evaluation is unique to the tasks discussed here. However, in\nmachine translation, carefully controlling for hyperparameter effects would be substantially more\nexpensive because standard datasets are much larger. Second, the research community should strive\nfor more consensus about appropriate experimental methodology that balances costs of careful ex-\nperimentation with the risks associated with false claims. Finally, more attention should be paid\nto hyperparameter sensitivity. Models that introduce many new hyperparameters or which perform\nwell only in narrow ranges of hyperparameter settings should be identiﬁed as such as part of standard\npublication practice.\n1\narXiv:1707.05589v2  [cs.CL]  20 Nov 2017\nUnder review as a conference paper at ICLR 2018\nT h e _ ac\nh e _ c aT t\nt\n(a) two-layer LSTM/NAS with skip connections\nT h e _ ac\nh e _ c aT t\nt (b) RHN with two processing steps per input\nFigure 1: Recurrent networks with optional down-projection, per-step and per-sequence dropout (dashed and\nsolid lines).\n2 M ODELS\nOur focus is on three recurrent architectures:\n• The Long Short-Term Memory (Hochreiter & Schmidhuber, 1997) serves as a well known\nand frequently used baseline.\n• The recently proposed Recurrent Highway Network (Zilly et al., 2016) is chosen because\nit has demonstrated state-of-the-art performance on a number of datasets.\n• Finally, we also include NAS (Zoph & Le, 2016), because of its impressive performance\nand because its architecture was the result of an automated reinforcement learning based\noptimisation process.\nOur aim is strictly to do better model comparisons for these architectures and we thus refrain from\nincluding techniques that are known to push perplexities even lower, but which are believed to be\nlargely orthogonal to the question of the relative merits of these recurrent cells. In parallel work with\na remarkable overlap with ours, Merity et al. (2017) demonstrate the utility of adding a Neural Cache\n(Grave et al., 2016). Building on their work, Krause et al. (2017) show that Dynamic Evaluation\n(Graves, 2013) contributes similarly to the ﬁnal perplexity.\nAs pictured in Fig. 1a, our models with LSTM or NAS cells have all the standard components:\nan input embedding lookup table, recurrent cells stacked as layers with additive skip connections\ncombining outputs of all layers to ease optimisation. There is an optional down-projection whose\npresence is governed by a hyperparameter from this combined output to a smaller space which\nreduces the number of output embedding parameters. Unless otherwise noted, input and output\nembeddings are shared, see (Inan et al., 2016) and (Press & Wolf, 2016).\nDropout is applied to feedforward connections denoted by dashed arrows in the ﬁgure. From the\nbottom up: to embedded inputs (input dropout), to connections between layers (intra-layer dropout),\nto the combined and the down-projected outputs (output dropout). All these dropouts have random\nmasks drawn independently per time step, in contrast to the dropout on recurrent states where the\nsame mask is used for all time steps in the sequence.\nRHN based models are typically conceived of as a single horizontal “highway” to emphasise how\nthe recurrent state is processed through time. In Fig. 1b, we choose to draw their schema in a way\nthat makes the differences from LSTMs immediately apparent. In a nutshell, the RHN state is passed\nfrom the topmost layer to the lowest layer of the next time step. In contrast, each LSTM layer has\nits own recurrent connection and state.\nThe same dropout variants are applied to all three model types, with the exception of intra-layer\ndropout which does not apply to RHNs since only the recurrent state is passed between the layers.\n2\nUnder review as a conference paper at ICLR 2018\nFor the recurrent states, all architectures use either variational dropout (Gal & Ghahramani, 2016,\nstate dropout)1 or recurrent dropout (Semeniuta et al., 2016), unless explicitly noted otherwise.\n3 E XPERIMENTAL SETUP\n3.1 D ATASETS\nWe compare models on three datasets. The smallest of them is the Penn Treebank corpus by Marcus\net al. (1993) with preprocessing from Mikolov et al. (2010). We also include another word level\ncorpus: Wikitext-2 by Merity et al. (2016). It is about twice the size of Penn Treebank with a larger\nvocabulary and much lighter preprocessing. The third corpus is Enwik8 from the Hutter Prize dataset\n(Hutter, 2012). Following common practice, we use the ﬁrst 90 million characters for training, and\nthe remaining 10 million evenly split between validation and test.\n4 T RAINING DETAILS\nWhen training word level models we follow common practice and use a batch size of 64, truncated\nbackpropagation with 35 time steps, and we feed the ﬁnal states from the previous batch as the initial\nstate of the subsequent one. At the beginning of training and test time, the model starts with a zero\nstate. To bias the model towards being able to easily start from such a state at test time, during\ntraining, with probability 0.01 a constant zero state is provided as the initial state.\nOptimisation is performed by Adam (Kingma & Ba, 2014) with β1 = 0 but otherwise default\nparameters ( β2 = 0.999, ϵ = 10−9). Setting β1 so turns off the exponential moving average\nfor the estimates of the means of the gradients and brings Adam very close to RMSProp without\nmomentum, but due to Adam’s bias correction, larger learning rates can be used.\nBatch size is set to 64. The learning rate is multiplied by 0.1 whenever validation performance does\nnot improve ever during 30 consecutive checkpoints. These checkpoints are performed after every\n100 and 200 optimization steps for Penn Treebank and Wikitext-2, respectively.\nFor character level models (i.e. Enwik8), the differences are: truncated backpropagation is per-\nformed with 50 time steps. Adam’s parameters are β2 = 0.99, ϵ = 10−5. Batch size is 128.\nCheckpoints are only every 400 optimisation steps and embeddings are not shared.\n5 E VALUATION\nFor evaluation, the checkpoint with the best validation perplexity found by the tuner is loaded and\nthe model is applied to the test set with a batch size of 1. For the word based datasets, using\nthe training batch size makes results worse by 0.3 PPL while Enwik8 is practically unaffected due\nto its evaluation and training sets being much larger. Preliminary experiments indicate that MC\naveraging would bring a small improvement of about 0.4 in perplexity and 0.005 in bits per character,\nsimilar to the results of Gal & Ghahramani (2016), while being a 1000 times more expensive which\nis prohibitive on larger datasets. Therefore, throughout we use the mean-ﬁeld approximation for\ndropout at test time.\n5.1 H YPERPARAMETER TUNING\nHyperparameters are optimised by Google Vizier (Golovin et al., 2017), a black-box hyperparameter\ntuner based on batched GP bandits using the expected improvement acquisition function (Desautels\net al., 2014). Tuners of this nature are generally more efﬁcient than grid search when the number\nof hyperparameters is small. To keep the problem tractable, we restrict the set of hyperparameters\nto learning rate, input embedding ratio, input dropout, state dropout, output dropout, weight decay.\nFor deep LSTMs, there is an extra hyperparameter to tune:intra-layer dropout. Even with this small\nset, thousands of evaluations are required to reach convergence.\n1Of the two parameterisations, we used the one in which there is further sharing of masks between gates\nrather than independent noise for the gates.\n3\nUnder review as a conference paper at ICLR 2018\nModel Size Depth Valid Test\nMedium LSTM, Zaremba et al. (2014) 10M 2 86.2 82.7\nLarge LSTM, Zaremba et al. (2014) 24M 2 82.2 78.4\nVD LSTM, Press & Wolf (2016) 51M 2 75.8 73.2\nVD LSTM, Inan et al. (2016) 9M 2 77.1 73.9\nVD LSTM, Inan et al. (2016) 28M 2 72.5 69.0\nVD RHN, Zilly et al. (2016) 24M 10 67.9 65.4\nNAS, Zoph & Le (2016) 25M - - 64.0\nNAS, Zoph & Le (2016) 54M - - 62.4\nAWD-LSTM, Merity et al. (2017) † 24M 3 60.0 57.3\nLSTM\n10M\n1 61.8 59 .6\nLSTM 2 63.0 60 .8\nLSTM 4 62.4 60 .1\nRHN 5 66.0 63 .5\nNAS 1 65.6 62 .7\nLSTM\n24M\n1 61.4 59 .5\nLSTM 2 62.1 59 .6\nLSTM 4 60.9 58 .3\nRHN 5 64.8 62 .2\nNAS 1 62.1 59 .7\nTable 1: Validation and test set perplexities on Penn Treebank for models with different numbers of parameters\nand depths. All results except those from Zaremba are with shared input and output embeddings. VD stands\nfor Variational Dropout from Gal & Ghahramani (2016). †: parallel work.\nParameter budget. Motivated by recent results from Collins et al. (2016), we compare models\non the basis of the total number of trainable parameters as opposed to the number of hidden units.\nThe tuner is given control over the presence and size of the down-projection, and thus over the\ntradeoff between the number of embedding vs. recurrent cell parameters. Consequently, the cells’\nhidden size and the embedding size is determined by the actual parameter budget, depth and the\ninput embedding ratiohyperparameter.\nFor Enwik8 there are relatively few parameters in the embeddings since the vocabulary size is only\n205. Here we choose not to share embeddings and to omit the down-projection unconditionally.\n6 R ESULTS\n6.1 P ENN TREEBANK\nWe tested LSTMs of various depths and an RHN of depth 5 with parameter budgets of 10 and 24\nmillion matching the sizes of the Medium and Large LSTMs by (Zaremba et al., 2014). The results\nare summarised in Table 1.\nNotably, in our experiments even the RHN with only 10M parameters has better perplexity than the\n24M one in the original publication. Our 24M version improves on that further. However, a shallow\nLSTM-based model with only 10M parameters enjoys a very comfortable margin over that, with\ndeeper models following near the estimated noise range. At 24M, all depths obtain very similar\nresults, reaching 58.3 at depth 4. Unsurprisingly, NAS whose architecture was chosen based on its\nperformance on this dataset does almost equally well, even better than in Zoph & Le (2016).\n6.2 W IKITEXT -2\nWikitext-2 is not much larger than Penn Treebank, so it is not surprising that even models tuned for\nPenn Treebank perform reasonably on this dataset, and this is in fact how results in previous works\nwere produced. For a fairer comparison, we also tune hyperparameters on the same dataset. In Table\n2, we report numbers for both approaches. All our results are well below the previous state of the\nare for models without dynamic evaluation or caching. That said, our best result, 65.9 compares\n4\nUnder review as a conference paper at ICLR 2018\nModel Size Depth Valid Test\nVD LSTM, Merity et al. (2016) 20M 2 101.7 96.3\nVD+Zoneout LSTM, Merity et al. (2016) 20M 2 108.7 100.9\nVD LSTM, Inan et al. (2016) 22M 2 91.5 87.7\nAWD-LSTM, Merity et al. (2017) † 33M 3 68.6 65.8\nLSTM (tuned for PTB)\n10M\n1 88.4 83 .2\nLSTM 1 72.7 69 .1\nLSTM 2 73.8 70 .7\nLSTM 4 78.3 74 .3\nRHN 5 83.5 79 .5\nNAS 1 79.6 75 .9\nLSTM (tuned for PTB)\n24M\n1 79.8 76 .3\nLSTM 1 69.3 65 .9\nLSTM 2 69.1 65 .9\nLSTM 4 70.5 67 .6\nRHN 5 78.1 75 .6\nNAS 1 73.0 69 .8\nTable 2: Validation and test set perplexities on Wikitext-2. All results are with shared input and output embed-\ndings. †: parallel work.\nfavourably even to the Neural Cache (Grave et al., 2016) whose innovations are fairly orthogonal to\nthe base model.\nShallow LSTMs do especially well here. Deeper models have gradually degrading perplexity, with\nRHNs lagging all of them by a signiﬁcant margin. NAS is not quite up there with the LSTM\nsuggesting its architecture might have overﬁtted to Penn Treebank, but data for deeper variants\nwould be necessary to draw this conclusion.\n6.3 E NWIK 8\nIn contrast to the previous datasets, our numbers on this task (reported in BPC, following convetion)\nare slightly off the state of the art. This is most likely due to optimisation being limited to 14 epochs\nwhich is about a tenth of what the model of Zilly et al. (2016) was trained for. Nevertheless, we\nmatch their smaller RHN with our models which are very close to each other. NAS lags the other\nmodels by a surprising margin at this task.\n7 A NALYSIS\nOn two of the three datasets, we improved previous results substantially by careful model speciﬁ-\ncation and hyperparameter optimisation, but the improvement for RHNs is much smaller compared\nto that for LSTMs. While it cannot be ruled out that our particular setup somehow favours LSTMs,\nwe believe it is more likely that this effect arises due to the original RHN experimental condition\nhaving been tuned more extensively (this is nearly unavoidable during model development).\nNaturally, NAS beneﬁtted only to a limited degree from our tuning, since the numbers of Zoph & Le\n(2016) were already produced by employing similar regularisation methods and a grid search. The\nsmall edge can be attributed to the suboptimality of grid search (see Section 7.3).\nIn summary, the three recurrent cell architectures are closely matched on all three datasets, with\nminuscule differences on Enwik8 where regularisation matters the least. These results support the\nclaims of Collins et al. (2016), that capacities of various cells are very similar and their apparent\ndifferences result from trainability and regularisation. While comparing three similar architectures\ncannot prove this point, the inclusion of NAS certainly gives it more credence. This way we have\ntwo of the best human designed and one machine optimised cell that was the top performer among\nthousands of candidates.\n5\nUnder review as a conference paper at ICLR 2018\nModel Size Depth Valid Test\nStacked LSTM, Graves (2013) 21M 7 - 1.67\nGrid LSTM, Kalchbrenner et al. (2015) 17M 6 - 1.47\nMI-LSTM, Wu et al. (2016) 17M 1 - 1.44\nLN HM-LSTM, Chung et al. (2016) 35M 3 - 1.32\nByteNet, Kalchbrenner et al. (2016) - 25 - 1.31\nVD RHN, Zilly et al. (2016) 23M 5 - 1.31\nVD RHN, Zilly et al. (2016) 21M 10 - 1.30\nVD RHN, Zilly et al. (2016) 46M 10 - 1.27\nLSTM\n27M\n4 1.29 1 .31\nRHN 5 1.30 1 .31\nNAS 4 1.38 1 .40\nLSTM 46M 4 1.28 1 .30\nRHN 5 1.29 1 .30\nNAS 4 1.32 1 .33\nTable 3: Validation and test set BPCs on Enwik8 from the Hutter Prize dataset.\n7.1 T HE EFFECT OF INDIVIDUAL FEATURES\nDown-projection was found to be very beneﬁcial by the tuner for some depth/budget combinations.\nOn Penn Treebank, it improved results by about 2–5 perplexity points at depths 1 and 2 at 10M, and\ndepth 1 at 24M, possibly by equipping the recurrent cells with more capacity. The very same models\nbeneﬁted from down-projection on Wikitext-2, but even more so with gaps of about 10–18 points\nwhich is readily explained by the larger vocabulary size.\nWe further measured the contribution of other features of the models in a series of experiments. See\nTable 4. To limit the number of resource used, in these experiments only individual features were\nevaluated (not their combinations) on Penn Treebank at the best depth for each architecture (LSTM\nor RHN) and parameter budget (10M or 24M) as determined above.\nFirst, we untied input and output embeddings which made perplexities worse by about 6 points\nacross the board which is consistent with the results of Inan et al. (2016).\nSecond, without variational dropout the RHN models suffer quite a bit since there remains no\ndropout at all in between the layers. The deep LSTM also sees a similar loss of perplexity as having\nintra-layer dropout does not in itself provide enough regularisation.\nThird, we were also interested in how recurrent dropout (Semeniuta et al., 2016) would perform in\nlieu of variational dropout. Dropout masks were shared between time steps in both methods, and\nour results indicate no consistent advantage to either of them.\n7.2 M ODEL SELECTION\nWith a large number of hyperparameter combinations evaluated, the question of how much the tuner\noverﬁts arises. There are multiple sources of noise in play,\n(a) non-deterministic ordering of ﬂoating-point operations in optimised linear algebra routines,\n(b) different initialisation seeds,\n(c) the validation and test sets being ﬁnite samples from a inﬁnite population.\nTo assess the severity of these issues, we conducted the following experiment: models with the best\nhyperparameter settings for Penn Treebank and Wikitext-2 were retrained from scratch with various\ninitialisation seeds and the validation and test scores were recorded. If during tuning, a model just\ngot a lucky run due to a combination of (a) and (b), then retraining with the same hyperparameters\nbut with different seeds would fail to reproduce the same good results.\nThere are a few notable things about the results. First, in our environment (Tensorﬂow with a single\nGPU) even with the same seed as the one used by the tuner, the effect of (a) is almost as large\nas that of (a) and (b) combined. Second, the variance induced by (a) and (b) together is roughly\nequivalent to an absolute difference of 0.4 in perplexity on Penn Treebank and 0.5 on Wikitext-2.\n6\nUnder review as a conference paper at ICLR 2018\nSize 10M Size 24M\nModel Depth Valid Test Depth Valid Test\nLSTM 1 61.8 59 .6 4 60.9 58 .3\n- Shared Embeddings 1 67.6 65 .2 4 65.6 63 .2\n- Variational Dropout 1 62.9 61 .2 4 66.3 64 .5\n+ Recurrent Dropout 1 62.8 60 .6 4 65.2 62 .9\n+ Untied gates 1 61.4 58 .9 4 64.0 61 .3\n+ Tied gates 1 61.7 59 .6 4 60.4 58 .0\nRHN 5 66.0 63 .5 5 64.8 62 .2\n- Shared Embeddings 5 72.3 69 .5 5 67.4 64 .6\n- Variational Dropout 5 74.4 71 .7 5 74.7 71 .7\n+ Recurrent Dropout 5 65.5 63 .0 5 63.4 61 .0\nTable 4: Validation and test set perplexities on Penn Treebank for variants of our best LSTM and RHN models\nof two sizes.\nThird, the validation perplexities of the best checkpoints are about one standard deviation lower than\nthe sample mean of the reruns, so the tuner could ﬁt the noise only to a limited degree.\nBecause we treat our corpora as a single sequence, test set contents are not i.i.d., and we cannot apply\ntechniques such as the bootstrap to assess (c). Instead, we looked at the gap between validation and\ntest scores as a proxy and observed that it is very stable, contributing variance of 0.12–0.3 perplexity\nto the ﬁnal results on Penn Treebank and Wikitext-2, respectively.\nWe have not explicitly dealt with the unknown uncertainty remaining in the Gaussian Process that\nmay affect model comparisons, apart from running it until apparent convergence. All in all, our\nﬁndings suggest that a gap in perplexity of 1.0 is a statistically robust difference between models\ntrained in this way on these datasets. The distribution of results was approximately normal with\nroughly the same variance for all models, so we still report numbers in a tabular form instead of\nplotting the distribution of results, for example in a violin plot (Hintze & Nelson, 1998).\n7.3 S ENSITIVITY\nTo further verify that the best hyperparameter setting found by the tuner is not a ﬂuke, we plotted the\nvalidation loss against the hyperparameter settings. Fig. 2 shows one such typical plot, for a 4-layer\nLSTM. We manually restricted the ranges around the best hyperparameter values to around 15–25%\nof the entire tuneable range, and observed that the vast majority of settings in that neighbourhood\nproduced perplexities within 3.0 of the best value. Widening the ranges further leads to quickly\ndeteriorating results.\nSatisﬁed that the hyperparameter surface is well behaved, we considered whether the same results\ncould have possibly been achieved with a simple grid search. Omitting input embedding ratiobe-\ncause the tuner found having a down-projection suboptimal almost non-conditionally for this model,\nthere remain six hyperparameters to tune. If there were 5 possible values on the grid for each hyper-\nparameter (with one value in every 20% interval), then we would need 65, nearly 8000 trials to get\nwithin 3.0 of the best perplexity achieved by the tuner in about 1500 trials.\n7.4 T YING LSTM GATES\nNormally, LSTMs have two independent gates controlling the retention of cell state and the admis-\nsion of updates (Eq. 1). A minor variant which reduces the number of parameters at the loss of some\nﬂexibility is to tie the input and forget gates as in Eq. 2. A possible middle ground that keeps the\nnumber of parameters the same but ensures that values of the cell state cremain in [−1,1] is to cap\n7\nUnder review as a conference paper at ICLR 2018\nFigure 2: Average per-word negative log-likelihoods of hyperparameter combinations in the neighbourhood of\nthe best solution for a 4-layer LSTM with 24M weights on the Penn Treebank dataset.\nthe input gate as in Eq. 3.\nct = ft ⊙ct−1 + it ⊙jt (1)\nct = ft ⊙ct−1 + (1−ft) ⊙jt (2)\nct = ft ⊙ct−1 + min(1−ft,it) ⊙jt (3)\nWhere the equations are based on the formulation of Sak et al. (2014). All LSTM models in this pa-\nper use the third variant, except those titled “Untied gates” and “Tied gates” in Table 4 corresponding\nto Eq. 1 and 2, respectively.\nThe results show that LSTMs are insensitive to these changes and the results vary only slightly even\nthough more hidden units are allocated to the tied version to ﬁll its parameter budget. Finally, the\nnumbers suggest that deep LSTMs beneﬁt from bounded cell states.\n8 C ONCLUSION\nDuring the transitional period when deep neural language models began to supplant their shallower\npredecessors, effect sizes tended to be large, and robust conclusions about the value of the mod-\nelling innovations could be made, even in the presence of poorly controlled “hyperparameter noise.”\nHowever, now that the neural revolution is in full swing, researchers must often compare competing\ndeep architectures. In this regime, effect sizes tend to be much smaller, and more methodological\ncare is required to produce reliable results. Furthermore, with so much work carried out in parallel\nby a growing research community, the costs of faulty conclusions are increased.\nAlthough we can draw attention to this problem, this paper does not offer a practical methodologi-\ncal solution beyond establishing reliable baselines that can be the benchmarks for subsequent work.\nStill, we demonstrate how, with a huge amount of computation, noise levels of various origins can be\ncarefully estimated and models meaningfully compared. This apparent tradeoff between the amount\nof computation and the reliability of results seems to lie at the heart of the matter. Solutions to the\nmethodological challenges must therefore make model evaluation cheaper by, for instance, reducing\nthe number of hyperparameters and the sensitivity of models to them, employing better hyperpa-\nrameter optimisation strategies, or by deﬁning “leagues” with predeﬁned computational budgets for\na single model representing different points on the tradeoff curve.\nREFERENCES\nJunyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural net-\nworks. CoRR, abs/1609.01704, 2016. URL http://arxiv.org/abs/1609.01704.\nJasmine Collins, Jascha Sohl-Dickstein, and David Sussillo. Capacity and trainability in recurrent\nneural networks. arXiv preprint arXiv:1611.09913, 2016.\n8\nUnder review as a conference paper at ICLR 2018\nThomas Desautels, Andreas Krause, and Joel W. Burdick. Parallelizing exploration-exploitation\ntradeoffs in Gaussian process bandit optimization. Journal of Machine Learning Research, 15:\n4053–4103, 2014. URL http://jmlr.org/papers/v15/desautels14a.html.\nYarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent\nneural networks. In Advances in Neural Information Processing Systems, pp. 1019–1027, 2016.\nDaniel Golovin, Benjamin Solnik, Subhodeep Moitra, Greg Kochanski, John Karro, and D Scul-\nley. Google vizier: A service for black-box optimization. In Proceedings of the 23rd ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1487–1495.\nACM, 2017.\nEdouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a\ncontinuous cache. CoRR, abs/1612.04426, 2016. URL http://arxiv.org/abs/1612.\n04426.\nAlex Graves. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013.\nURL http://arxiv.org/abs/1308.0850.\nPeter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.\nDeep reinforcement learning that matters. arXiv preprint arXiv:1709.06560, 2017.\nJerry L Hintze and Ray D Nelson. Violin plots: a box plot-density trace synergism. The American\nStatistician, 52(2):181–184, 1998.\nSepp Hochreiter and J ¨urgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9\n(8):1735–1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL\nhttp://dx.doi.org/10.1162/neco.1997.9.8.1735.\nMarcus Hutter. The human knowledge compression contest. 2012.\nHakan Inan, Khashayar Khosravi, and Richard Socher. Tying word vectors and word classiﬁers: A\nloss framework for language modeling. CoRR, abs/1611.01462, 2016. URL http://arxiv.\norg/abs/1611.01462.\nNal Kalchbrenner, Ivo Danihelka, and Alex Graves. Grid long short-term memory. CoRR,\nabs/1507.01526, 2015. URL http://arxiv.org/abs/1507.01526.\nNal Kalchbrenner, Lasse Espeholt, Karen Simonyan, A¨aron van den Oord, Alex Graves, and Koray\nKavukcuoglu. Neural machine translation in linear time. CoRR, abs/1610.10099, 2016. URL\nhttp://arxiv.org/abs/1610.10099.\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nBen Krause, Emmanuel Kahembwe, Iain Murray, and Steve Renals. Dynamic evaluation of neural\nsequence models. arXiv preprint arXiv:1709.07432, 2017.\nMitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The Penn treebank. Computational linguistics, 19(2):313–330, 1993.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. CoRR, abs/1609.07843, 2016. URL http://arxiv.org/abs/1609.07843.\nStephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and optimizing LSTM\nlanguage models. CoRR, abs/1708.02182, 2017. URL http://arxiv.org/abs/1708.\n02182.\nTomas Mikolov, Martin Karaﬁ´at, Lukas Burget, Jan Cernock `y, and Sanjeev Khudanpur. Recurrent\nneural network based language model. In Interspeech, volume 2, pp. 3, 2010.\nOﬁr Press and Lior Wolf. Using the output embedding to improve language models. CoRR,\nabs/1608.05859, 2016. URL http://arxiv.org/abs/1608.05859.\n9\nUnder review as a conference paper at ICLR 2018\nNils Reimers and Iryna Gurevych. Reporting score distributions makes a difference: Performance\nstudy of lstm-networks for sequence tagging. CoRR, abs/1707.09861, 2017. URL http://\narxiv.org/abs/1707.09861.\nHasim Sak, Andrew W. Senior, and Franc ¸oise Beaufays. Long short-term memory based recur-\nrent neural network architectures for large vocabulary speech recognition. CoRR, abs/1402.1128,\n2014. URL http://arxiv.org/abs/1402.1128.\nStanislau Semeniuta, Aliaksei Severyn, and Erhardt Barth. Recurrent dropout without memory loss.\nCoRR, abs/1603.05118, 2016. URL http://arxiv.org/abs/1603.05118.\nYuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, and Ruslan Salakhutdinov. On mul-\ntiplicative integration with recurrent neural networks. CoRR, abs/1606.06630, 2016. URL\nhttp://arxiv.org/abs/1606.06630.\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.\nCoRR, abs/1409.2329, 2014. URL http://arxiv.org/abs/1409.2329.\nJulian G. Zilly, Rupesh Kumar Srivastava, Jan Koutn ´ık, and J ¨urgen Schmidhuber. Recurrent\nhighway networks. CoRR, abs/1607.03474, 2016. URL http://arxiv.org/abs/1607.\n03474.\nBarret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint\narXiv:1611.01578, 2016.\n10",
  "topic": "Treebank",
  "concepts": [
    {
      "name": "Treebank",
      "score": 0.9735903739929199
    },
    {
      "name": "Computer science",
      "score": 0.756769597530365
    },
    {
      "name": "Hyperparameter",
      "score": 0.693752646446228
    },
    {
      "name": "Language model",
      "score": 0.6449989080429077
    },
    {
      "name": "Code (set theory)",
      "score": 0.6168661713600159
    },
    {
      "name": "Black box",
      "score": 0.5796434879302979
    },
    {
      "name": "Deep neural networks",
      "score": 0.5402843952178955
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5211707949638367
    },
    {
      "name": "Scale (ratio)",
      "score": 0.519236147403717
    },
    {
      "name": "Variation (astronomy)",
      "score": 0.5121316909790039
    },
    {
      "name": "State (computer science)",
      "score": 0.5006778240203857
    },
    {
      "name": "Artificial neural network",
      "score": 0.47914189100265503
    },
    {
      "name": "Natural language processing",
      "score": 0.46934330463409424
    },
    {
      "name": "Machine learning",
      "score": 0.43599390983581543
    },
    {
      "name": "Programming language",
      "score": 0.23597866296768188
    },
    {
      "name": "Parsing",
      "score": 0.12985765933990479
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Astrophysics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}