{
  "title": "Do language models have coherent mental models of everyday things?",
  "url": "https://openalex.org/W4385570912",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5029619813",
      "name": "Yuling Gu",
      "affiliations": [
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5087135401",
      "name": "Bhavana Dalvi Mishra",
      "affiliations": [
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A5049837138",
      "name": "Peter Clark",
      "affiliations": [
        "Allen Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1619326861",
    "https://openalex.org/W4230262515",
    "https://openalex.org/W2017531347",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W3214326397",
    "https://openalex.org/W4287084089",
    "https://openalex.org/W4288244092",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W2480989967",
    "https://openalex.org/W4385574160",
    "https://openalex.org/W2938667732",
    "https://openalex.org/W4385572747",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W2613049552",
    "https://openalex.org/W2809729522",
    "https://openalex.org/W2795843265",
    "https://openalex.org/W3043771371",
    "https://openalex.org/W4310923406",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W176362184",
    "https://openalex.org/W2087382039",
    "https://openalex.org/W4226218072",
    "https://openalex.org/W4300978535",
    "https://openalex.org/W3196886373",
    "https://openalex.org/W2032152873",
    "https://openalex.org/W4385567134",
    "https://openalex.org/W1625371922"
  ],
  "abstract": "When people think of everyday things like an egg, they typically have a mental image associated with it. This allows them to correctly judge, for example, that “the yolk surrounds the shell” is a false statement. Do language models similarly have a coherent picture of such everyday things? To investigate this, we propose a benchmark dataset consisting of 100 everyday things, their parts, and the relationships between these parts, expressed as 11,720 “X relation Y?” true/false questions. Using these questions as probes, we observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these everyday things, but do not have fully coherent “parts mental models” (54-59% accurate, 19-43% conditional constraint violation). We propose an extension where we add a constraint satisfaction layer on top of the LM’s raw predictions to apply commonsense constraints. As well as removing inconsistencies, we find that this also significantly improves accuracy (by 16-20%), suggesting how the incoherence of the LM’s pictures of everyday things can be significantly reduced.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 1892–1913\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nDo language models have coherent mental models of everyday things?\nYuling Guand Bhavana Dalvi Mishraand Peter Clark\nAllen Institute for AI, Seattle, W A\n{yulingg,bhavanad,peterc}@allenai.org\nAbstract\nWhen people think of everyday things like an\negg, they typically have a mental image asso-\nciated with it. This allows them to correctly\njudge, for example, that “the yolk surrounds the\nshell” is a false statement. Do language models\nsimilarly have a coherent picture of such ev-\neryday things? To investigate this, we propose\na benchmark dataset consisting of 100 every-\nday things, their parts, and the relationships\nbetween these parts, expressed as 11,720 “X\nrelation Y?” true/false questions. Using these\nquestions as probes, we observe that state-of-\nthe-art pre-trained language models (LMs) like\nGPT-3 and Macaw have fragments of knowl-\nedge about these everyday things, but do not\nhave fully coherent “parts mental models” (54-\n59% accurate, 19-43% conditional constraint\nviolation). We propose an extension where we\nadd a constraint satisfaction layer on top of\nthe LM’s raw predictions to apply common-\nsense constraints. As well as removing incon-\nsistencies, we find that this also significantly\nimproves accuracy (by 16-20%), suggesting\nhow the incoherence of the LM’s pictures of\neveryday things can be significantly reduced.1\n1 Introduction\nPsychologists and cognitive scientists hypothesize\nthat humans develop mental models of the world,\nnamely internal, conceptual representations of the\nenvironment which we base our decisions and ac-\ntions on (Ha and Schmidhuber, 2018; Jonassen\nand Henning, 1996). Hespos and Spelke (2004)\nobserved that 5-month-old human infants exhibit\nunderstanding of mechanical properties of objects\nin terms of arrangements and motions of surfaces,\nwell before they can understand language. Draw-\ning loosely on this idea, but without making any\nclaims about how LMs reason internally (Shanahan,\n1We make our data and code publicly available athttps:\n//github.com/allenai/everyday-things.\nFigure 1: While humans appear to have coherent men-\ntal pictures of everyday things (e.g., an egg, A), our\nquestion-asking probes suggest that LMs do not (e.g.,\none LM answered that the egg white both surrounds and\nis surrounded by the shell, B). This model incoherence\ncan be reduced by applying commonsense constraints\n(e.g., surrounds is asymmetric), resulting in a more co-\nherent parts model (C).\n2022; Andreas, 2022), we investigate if pre-trained\nlanguage models show evidence of coherent inter-\nnal representations of everyday things, analogous\nto human mental models, via probing. We focus\non mental models in the context of ordinary ob-\njects that we encounter in our everyday lives. Such\ncommonsense knowledge helps us understand how\nthese everyday things work and how to interact\nwith them. For example, when someone tries to\nmake a fried egg, they know that it has a shell and\n1892\nthat it can be cracked open to reveal the egg white\nand yolk inside. However, if a system does not\nhave a coherent picture of such everyday things,\nthinking that the egg yolk surrounds the shell, then\nit might have to resort to ridiculous approaches\nsuch as trying to scrape the egg yolk off the shell\ninto the pan.\nWe explore a first version of this, in which we\nconsider only knowledge about an object’s parts\nand their relationships. We refer to this knowl-\nedge as a parts mental model. We first create a\nbenchmark dataset of 100 everyday things, by ask-\ning human annotators to draw a graph representing\ntheir parts mental model (e.g., Figure 2) depicting\nthe parts of an everyday thing, spatial relationships,\nconnections between its parts and functional depen-\ndencies (if any). Then we probe two representative\nstate-of-the-art LMs with questions about these\neveryday things. We find that the LMs’ parts men-\ntal models are generally of poor quality. Further,\nmodel predictions can violate basic consistency\nconstraints e.g. transitivity. To alleviate this, we\napply constraint reasoning to derive more accurate\nand consistent mental models of everyday things,\ncorrecting some of the LMs’ original inconsisten-\ncies. This is illustrated in Figure 1.\nOur contributions are:\n1. We present a benchmark dataset of parts men-\ntal models consisting of 100 everyday things,\n2.2K parts and 11.7K relationships.\n2. We show that SOTA LMs like GPT-3 and\nMacaw are poor at answering relationship\nqueries between parts of everyday things. The\nparts mental models derived using their pre-\ndictions are only 54-59% accurate, and sig-\nnificantly inconsistent (19-43% conditional\nviolation τ).\n3. We propose a neuro-symbolic method that ap-\nplies constraint reasoning on top of raw LM\npredictions as a way of obtaining more consis-\ntent (0% conditional violation τ) and more ac-\ncurate mental models (16-20% improvement).\nThis suggests a broader cognitive architecture\n(LM + reasoner) for future systems, to better\nconstruct mental models than the LM alone.\n2 Related work\nMental models: The idea of mental models\n(Johnson-Laird, 1983) is not new. Many years\nago, Craik (1943) proposed that thinking itself is\nthe manipulation of internal representations of the\nworld. Craik (1943) described mental models as\na ‘small-scale model’ of external reality and of its\nown possible actions within someone’s head. Such\na mental model is useful in many ways, including\nallowing one to try out various alternatives, make\nconclusions, react to future situations, learn from\npast events, and in general, improve competency.\nYears later, when Johnson-Laird (2006) outlined\nthe mental processes that underlie human reason-\ning, he based his discussion on the fundamental\nassumption that human beings can construct inter-\nnal representations of spatial layouts, and specified\nmental models to be iconic. In his words, a mental\nmodel’s “parts and the relations among them cor-\nrespond to the parts of the layout and the relations\namong them.” While coherent internal representa-\ntions of spatial layouts are crucial for human rea-\nsoning, their role, coherence, and even existence\nin LMs have not been systematically explored. In\nthis work, we try to bridge this gap by proposing\na benchmark dataset and methodology to compare\nhuman internal representations of spatial layouts of\neveryday things with those of LMs.\nPrior datasets: Prior works on reasoning about\nobject/body parts include Li et al. (2019b) which\nfocused on human body parts and human inter-\naction with other objects. The PTR benchmark\n(Hong et al., 2021) is a QA dataset about objects\nand their parts, combining 5 everyday things: chair,\ntable, bed, refrigerator, and cart, to create ques-\ntions across 70K different scenes. Ji et al. (2022)\nused tangram puzzles to analyze shape naming, part\nnaming and segmentation divergence across partic-\nipants when they see a certain shape. Contributing\nto this existing body of datasets, the dataset we\nintroduce serves as a resource for researchers to\nstudy canonical parts mental models for a wide va-\nriety of everyday things, focusing on relationships\nbetween parts of objects, which is fundamental to\nhow humans think and interact with these things.\nLarge language models:Despite recent advances\nin LMs, studies suggest that they still struggle at\nreasoning with real-world entities and concepts.\nBisk et al. (2020) found that when LMs answer\nquestions involving physical commonsense rea-\nsoning, their performance at that time was near\nchance level for questions involving spatial rela-\ntions like “top” and “bottom.” Sahu et al. (2022)\ndemonstrated the lack of conceptual consistency in\nLMs by correlating models’ answers on common-\nsense reasoning questions (CSQA dataset) and their\n1893\nTree\n Flashlight\nFigure 2: Our everyday things dataset, ParRoT, covers different entities, both natural (e.g. tree) and man-made (e.g.\nflashlight). Above are two examples of such everyday things. In each case, we show a (a) diagram of the entity, and\n(b) parts graph of the everyday thing drawn by crowdworkers. The parts graphs illustrate how our dataset contains a\nvariety of relations between parts.\nanswers on associated conceptual questions from\nConceptNet knowledge base. To improve existing\nsystems, progress has been made such as by im-\nposing constraints with neuro-symbolic approaches\n(Nye et al., 2021; Mitchell et al., 2022) and incor-\nporating both textual and visual information (Dan\net al., 2020). Inspired by recent progress, we pro-\npose a constraint reasoning method that applies\nhard commonsense constraints (e.g., if ‘A above B’\nis True then ‘A below B’ cannot be True) on top of\nraw LM predictions to produce more accurate and\nconsistent mental models of everyday things.\n3 Parts mental models and Task\nWe define “parts mental model” for everyday things\nin this section. Then in the rest of the paper, we\ndescribe how we collect a dataset for them, measure\nLMs’ coherence on them, and finally apply external\nreasoning to improve the accuracy and consistency\nof LMs’ parts mental model.\nHere, we use parts mental model to mean a parts-\nfocused subset of a complete mental model of an\nentity. We represent a parts mental model as a\ndirected graph where parts of the everyday thing\nform the nodes of this graph and these nodes are\nconnected with edges indicating how these parts\nare related to each other. Based on prior works\nsuch as Renz (2002) and Gunning et al. (2010), we\nselected 11 spatial orientation relations to focus\non. In addition, we augmented these with relations\ndescribing connectivity and functional dependency.\nIn total, we consider 14 relationships (across these\n3 categories) between parts, listed in Table 2.\nNote that the notion of a single “parts mental\nmodel” for an everyday thing is somewhat uncon-\nstrained (e.g., which parts to pick? what version\nof the entity are we talking about?). To make this\ntask more well-defined, we also provide a prede-\nfined list of parts as a guide (details in Section 4.1),\nand the task for annotators or a model is to specify\nrelationships between them as they see appropriate,\nusing our ontology of relationships. This is im-\nportant so that we can do meaningful comparisons\nbetween language models and humans’ notion of\nparts mental models of everyday things.\nFigure 2 shows two examples of parts mental\nmodels in our dataset, where edges encode rela-\ntionships between parts. E.g., in a tree, “trunk is\nabove the roots”; in a flashlight, “bulb requires\nthe batteries,” etc. Inspired by previous literature,\nwe envision that such parts mental models would\nplay a key role when one carries out daily activities\ninvolving these everyday things.\nTask\nHere we define our task: “Construct a parts men-\ntal model for everyday things” with the following\ninput/output specifications:\n• Input: Everyday thing, Parts list, Relation vo-\ncabulary (14 relations).\n• Output: List of tuples (x, r, y) where relation\nrholds between parts xand y.\nIn Section 4 we describe how we acquire a bench-\nmark dataset by asking human annotators to carry\nout this task. Once we have collected gold-standard\nparts mental models for everyday things based on\nthe human annotations, we prompt LMs for their\n2A requires B denotes A cannot perform its primary func-\ntion without B.\n1894\nGiven as seed\n(unique)\nAnnotated\nmental models\nAvg. annotated\nper mental model\nAnnotated + enriched (*)\n(Total)\nTotal avg. per\nmental model\n(Total /\n# mental models)\n# everyday things 100 100 - 100 -\n# mental models - 300 - 300 -\n# parts 716 2191 7.30 2191 7.30\n# relations (p1, rln, p2) 8 2752 9.17 11720 39.07\n# spatial relations 6 1858 6.19 9956 33.19\n# connectivity relation(s) 1 818 2.73 1612 5.37\n# functional relation(s) 1 76 0.25 152 0.51\nTable 1: Statistics of ParRoT, our Everyday Things Dataset. *Enriched refers to implied relations, see Section 4.3\nType Relations\nSpatial\norientation\npart of, has part, inside, contains,\nin front of, behind, above, below,\nsurrounds, surrounded by, next to∗\nConnectivity directly connected to∗\nFunctional\ndependency requires2, required by\nTable 2: Relationships encoded in “parts mental models”\nof everyday things. Among these relations, ‘next to’\nand ‘directly connected to’ relations are bi-directional,\nwhereas the other 12 relations are uni-directional.\nparts mental models and evaluate how well they do\non this task. Our proposed method to measure this\nis described in Section 5. In particular, we are inter-\nested in (1) how accurate are LM-generated parts\nmental models when compared to gold-standard\nmodels in our dataset and (2) ignoring accuracy,\nhow consistent are these generated parts mental\nmodels with respect to basic commonsense con-\nstraints? I.e., Do they at least conform to the 4\ntypes of commonsense constraints laid out in Sec-\ntion 5.2 e.g., ‘above’ and ‘below’ are inverse rela-\ntions, so if the LM predicts that in a tree, (trunk is\nabovethe roots) then it should also predict (roots\nare belowthe trunk).\n4 Everyday Things Dataset: ParRoT\n(Parts and Relations of Things)\nWe created a dataset of common entities that one\nwould encounter in their daily life. For each every-\nday thing, our dataset (ParRoT) contains a “parts\nmental model” in the form of a graph, which de-\npicts parts of the entity and relational information\nabout the parts. Such a graph encodes a parts-\nfocused mental model of that everyday thing, po-\ntentially useful for reasoning about how the entity\nworks and how to interact with it.\n4.1 Everyday entities\nWe first compiled a list of entities from children’s\nbooks, vocabulary lists (Grades 1-8), and online\nweb search.3 For the unique entities in this list, the\nauthors manually filtered out those entities that are\nnot common in everyday setting or have too few\n(i.e. only 1 or 2 parts) or too many parts (composite\nscenes). Specifically, we kept 100 entities that are\ncommon everyday things that a child would be\nfamiliar with, with a mix of natural and man-made\nthings. This annotation task involves answering the\nfollowing question for each item in the list: “Do\nyou imagine this is something that most people\nwould have seen in their everyday lives?”\nWe recognize there could be many variants of a\nsingle everyday entity e.g. different types of coffee\nmakers. To narrow down the possibilities, the au-\nthors picked a diagram for each everyday thing via\nweb search and carefully annotated a parts list for\neach of them to guide the level of granularity we\nare looking for. In some cases, the entity name was\nqualified to disambiguate further e.g. “digital clini-\ncal thermometer” instead of just “thermometer.”\n4.2 Mental model annotations\nWe ask crowdworkers to draw sketches of every-\nday things covering spatial relations, connectivity,\nand functional dependencies between parts (Table\n2). To encourage the format of the mental model\ngraphs to be more standardized across annotators,\nwe ask that the nodes (in circles) mainly contain\nlabels from the “Parts list” provided. However, to\ncollect mental models that are most natural to the\nworkers, they were also told that they can ignore\nparts in the “Parts list” if they seem unimportant,\nor add extra parts that seem important. We also\nspecified for edges to be labeled with the relations\n3Appendix A provides more details on the source of the\nlist of everyday things.\n1895\nshown in Table 2.4\nGiven the name of an everyday thing, list of parts,\nand example diagram, 3 crowdworkers were re-\ncruited to sketch mental models for each everyday\nthing.5 Figure 2 shows examples of such sketches.\nAccording to Norman (2013), mapping that takes\nadvantage of spatial analogies leads to immediate\nunderstanding and is more natural. Sketching out\nsuch a graph allows workers more flexibility in\ntaking advantage of spatial analogies between the\nactual entity and the sketch (see flashlight exam-\nple in Figure 2). Therefore, we hypothesize that\ndrawing a graph would be easier or more natural\nfor crowdworkers than typing a list of relations.6\n4.3 Statistics\nParRoT consists of 100 everyday things ranging\nfrom devices like coffee maker, space heater to\nnatural entities like tree and butterfly with number\nof parts (provided as a seed list to crowdworkers)\nranging from 3-14. We collected 3 mental mod-\nels per everyday thing. We take the parts mental\nmodels annotated by crowdworkers to be correct\nbut not complete. I.e., they may include only those\nrelations that they think are salient for the every-\nday thing, and also omit the ones that can be easily\ninferred from what they have annotated e.g., when\n(trunk is abovethe roots) is annotated, (roots are\nbelowthe trunk) can be omitted (Figure 2, tree ex-\nample). For each everyday thing’s mental model\nannotation, with the relation tuples annotated, we\nautomatically add relations that are implied via\nenrichment based on 4 types of constraints (sym-\nmetric, asymmetric, inverse, and transitive). The\ninferred relations include both relations that are la-\nbeled True (e.g. A above B being True implies that\nB below A is True) and relations that are labeled\nFalse (e.g. A above B being True implies B above\nA is False). This gives a total of 11.7K gold rela-\ntion tuples (6894 with “True” as gold labels and\n4826 with “False” as gold labels). Table 1 provides\nadditional dataset statistics. Appendix C discusses\nthe unanimity and diversity of mental models for\nthese everyday things.\n4For ease of annotation, they do not need to repeat anno-\ntations that mean the same thing. e.g. if they annotated ( x,\nabove, y), they do not need to annotate ( y, below, x) again.\nWe automatically generate these in our data post-processing.\n5More details can be found in Appendix B.\n6Later these sketches are transcribed into (x, r, y) tuples.\n5 Measuring and Improving Parts Mental\nModels\nOur proposed approach, ParRoT-Con,7 comprises\ntwo main components.8 The first component “Prob-\ning a Pre-trained Language Model” sends an ex-\nhaustive list of relation queries to a LM querying\nfor every relation between each pair of parts (e.g.\nall relationships between egg white, yolk, shell,\nshell membrane and air cell). This gives us a\nlarge set of candidate relation tuples along with\nthe model’s confidence in each of them. Incorrect\nrelation predictions can result in inconsistencies in\nthe mental model. E.g, “egg white both surrounds\nand is surrounded by the egg shell.” The second\ncomponent “constraint reasoning” then applies a\nconstraint satisfaction layer on top of these raw\npredictions to choose a subset of these relation tu-\nples that are maximally probable and minimally\nconflicting with each other. Note that ParRoT-Con\nis a zero-shot approach, where both probing LMs\nand constraint reasoning steps do not require any\ntask-specific fine-tuning or re-training.\n5.1 Probing a Pre-trained Language Model\nWe use the following pre-trained language models\nfor our study: GPT-3 (Brown et al., 2020) and\nMacaw9 (Tafjord and Clark, 2021). We probe them\nusing True/False questions of type: “Judge whether\nthis statement is true or false: In an <everyday\nthing>, <part1 relation part2>.” For each query q,\nwe record an answer a∈{True,False }, and the\nmodel’s beliefs about the likelihood of the relation\nbeing “True” as\np(True|q)\np(True|q) +p(False|q).\n5.2 Constraint Reasoning\nWe observed a significant amount of inconsistency\nin raw predictions from these LMs by considering\nthe following constraints:\n• Symmetric relations:This constraint ensures\nsymmetric relations like “directly connected\nto” and “next to” hold both ways.\ni.e. xrln y↔yrln x\n7First obtain the output of “stochastic parrots,” (Bender\net al., 2021) then apply constraints to reason on top of the\noutput.\n8See Appendix D Figure 8 for an illustration.\n9A SOTA T5-11B based question-answering system that\noutperforms GPT-3 on some QA tasks.\n1896\n• Asymmetric relations: For asymmetric re-\nlations like part of, has part, inside, contains,\nin front of, behind, above, below, surrounds,\nsurrounded by, requires, required by, this con-\nstraint makes sure that both “xrln y” and “y\nrln x” cannot be true at the same time.\ni.e. ¬(xrln y) ∨¬(yrln x)\n• Inverse relations: For a set of inverse re-\nlations e.g. above vs below, this constraint\nmakes sure that (xabove y) and (ybelow x)\nhave the same truth value.\ni.e. xrln y↔yinverse(rln) x\n• Transitive relations:For relations like inside,\ncontains, in front of, behind, above, below,\nsurrounds, surrounded by, this constraint will\nimpose transitivity.\ni.e. xrln y∧yrln z→xrln z\nIn this step, we try to resolve inconsistencies\nin LMs’ raw predictions by solving a MaxSAT\nconstraint satisfaction problem where each (x, re-\nlation, y) tuple is represented as a variable with\nconfidence value from the LM used as its weight\n(soft clause). We introduce 4 types of hard con-\nstraints (listed above) between these variables as\nhard clauses and any constraint violation results in\nan extremely high penalty. Given a WCNF formula\nwith these, a weighted MaxSAT solver tries to find\nan optimal assignment of truth values to relation\ntuples that maximizes the sum of weights of satis-\nfied soft clauses and satisfies all the formula’s hard\nclauses. We use the RC2 MaxSAT solver (Ignatiev\net al., 2018b) in PySAT (Ignatiev et al., 2018a).\n6 Results and Analysis\n6.1 Evaluation Metrics\nWe evaluate the parts mental models produced by\nthe two LMs in terms of accuracy and consistency:\nAccuracy: We compute the True/False accuracy\nof parts mental models based on the 11.7K gold\nrelation tuples present in ParRoT.\nConsistency: Following Kassner et al. (2021);\nMitchell et al. (2022), we adapt the Conditional\nViolation (τ) (Li et al., 2019a) metric to measure\ninconsistency across the 4 types of constraints de-\nfined in Section 5.2. For constraints L(x) →R(x)\nimposed on samples x∈D, where Dis the dataset,\nwe calculate conditional violation as:\nτ =\n∑\nx∈D\n[\n⋁\n(L,R)\n¬(L(x) →R(x))\n]\n∑\nx∈D\n[\n⋁\n(L,R)\nL(x)\n] .\n6.2 Results\nQ1: How consistent are LMs when they answer\nquestions about everyday things?\nWe measure the consistency of parts mental mod-\nels constructed by LMs based on 4 types of con-\nstraints described in Section 5.2. This measurement\nis purely based on LMs’ predictions and is inde-\npendent of relations in the gold mental models ac-\nquired for the everyday things. Table 3 shows that\nLMs contradict themselves (19-43% conditional\nviolation) when we ask them multiple questions\nabout parts of the same everyday thing to probe\nfor their parts mental model. E.g., in Appendix D,\nthe LM believes that in an egg, “yolk surrounds\nthe shell” and “shell surrounds the yolk” are both\nTrue. Table 3 also breaks down the LMs’ inconsis-\ntency across 4 types of constraints. We observe that\nGPT-3 struggles with maintaining consistency for\nsymmetric and inverse relations, whereas Macaw-\n11B finds it most challenging to satisfy constraints\nfor asymmetric relations.\nQ2: Do language models have accurate mental\nmodels of everyday things?\nNext, we investigate how accurate are these parts\nmental models when compared to gold mental mod-\nels in our ParRoT dataset. Table 4 shows that such\nqueries pertaining to parts of everyday things are\nchallenging for even SOTA models, with an av-\nerage accuracy of 54-59%. This is barely better\nthan the majority class baseline at 59% and random\nchance at 50%.\nThe LMs’ low performance shows that ParRoT\nis a challenging dataset, which is expected given\nthe fact that this dataset queries for commonsense\nknowledge about everyday things (e.g. spatial rela-\ntionship between parts of a device) that are often\nomitted in text, and hence less likely seen during\npre-training. Further, by construction, our queries\nminimally differ e.g. for relations between parts of\na tree, the edit distance between a statement with\ntrue relation “the leaves are above the roots” and\nfalse relation “the leaves are below the roots” is just\n1 word. This makes our task even more challenging\n1897\n%Conditional Violation (lower is better)\n%True\ntuples\nSymmetric\nrelations\nAsymmetric\nrelations\nInverse\nrelations\nTransitive\nrelations\nAvg.\n(macro)\nAvg.\n(micro)\nGPT-3\n(text-davinci\n-003)\n12.64 66.37\n(1,987/2,994)\n23.01\n(4,699/20,422)\n71.14\n(13,869/19,495)\n32.18\n(6,550/20,354) 48.17 42.84\n(27,105/63,265)\nMacaw-11B 57.77 29.98\n(3,089/10,305)\n64.97\n(42,170/64,910)\n33.63\n(21,642/64,361)\n10.08\n(44,121/437,746) 34.66 19.23\n(111,022/577,322)\nTable 3: Parts mental models constructed by LMs are significantly inconsistent with respect to their own predictions,\nviolating basic commonsense constraints. In brackets, we indicate (# violations) / (# constraints fired).\n# params Base\nLM (%)\nParRoT-Con\n(%)\nImprove\n(%)\nGPT-3 (text-\ndavinci-003) 175B 53.83 70.26 16.42\nMacaw-11B 11B 59.45 79.28 19.84\nTable 4: Comparing the accuracy of parts mental models\nbefore and after constraint reasoning on ParRoT dataset.\nas the models need to understand the semantics of\nrelational phrases to give the correct answer.\nQ3: Does ParRoT-Con, our proposed constraint\nreasoning approach, help create more accurate\nmental models?\nOur proposed approach, ParRoT-Con, utilizes the\ninherent inconsistency in LMs’ raw predictions to\nself-correct their own parts mental models. It finds\nan optimal assignment of truth values to relation\ntuples that accounts for both the model’s origi-\nnal beliefs (about the likelihood of each relation\nstatement being True or False), and the 4 types of\ncommonsense constraints imposed. By imposing\nthe commonsense constraints as hard constraints,\nour proposed method produces perfectly consis-\ntent mental models for all LMs with respect to the\nimposed constraints i.e. % conditional violation\nbecomes 0 for all columns in Table 3. Using these\nbasic commonsense constraints, ParRoT-Con im-\nproves parts mental model accuracy significantly\nby 16-20% on ParRoT (Table 4).\n6.3 Further analysis\nMost effective range We analyze what is the\nquality range of mental models that ParRoT-Con\nis most effective on. We quantify the quality of\nparts mental models by defining accuracy@ s, a\nmetric that says a mental model is correct if the\nproportion of correct relations is at least s%. We\nthen plot the percentage of mental models (out of\n300) that are correct vs accuracy@sfor different\nvalues of s, where s ∈ {50,60,70,80,90,100}.\nFigure 3 shows that ParRoT-Con not only effec-\ntively increases the percentage of mental models\nthat are approximately correct (s= 50, 60) but also\nthe percentage of mental models that are (almost)\ntotally correct (s= 90, 100). The improvements\nwith constraint reasoning are even more prominent\nwhen it comes to increasing the percentage of men-\ntal models that are at least 60-80% accurate. This\nis likely attributed to the improvement in mental\nmodels that have enough signals from LMs’ raw\npredictions and also enough margin to improve.\nAccuracy of parts mental models per relation\nFigure 4 shows that the base LMs are more accurate\nin predictions for queries containing relationships\nlike ‘part of’ which is more likely to be stated in\ntext than spatial relations like ‘above’, ‘below’, and\n‘behind’ which are lower-level physical details of-\nten not mentioned in text. Different models also\ndiffer in which relationships they perform better\non: e.g. GPT-3 performs poorly on bi-directional\nrelations like ‘connects’ and ‘next to’, with accu-\nracy way below chance level, while Macaw-11B\nachieves around 70% accuracy for queries involv-\ning these relations.\nSuccess and failure across models per everyday\nthing LMs show both similarities and differ-\nences in what everyday things they have better men-\ntal models of. For each model, Figure 5 shows the\ntop 20 everyday things that the models performed\nbest on in terms of base LM accuracy. Both GPT-3\nand Macaw-11B perform well on the following ev-\neryday things: sandwich, kayak, dog, kite, bird, rat,\ncat, pencil sharpener, tree, cable car, and butterfly.\nIt is interesting to see that both models perform\nwell on several natural living things like animals\n(e.g. dog, bird, rat, cat), insect (e.g. butterfly),\nand plant (e.g. tree). Figure 6 shows the top 20\neveryday things that the models performed worst\non in terms of base LM accuracy. We observe that\n1898\n(a) GPT-3\n(b) Macaw-11B\nFigure 3: Percentage of correct mental models vs\naccuracy@s shows that for both GPT-3 and Macaw-\n11B, there is a higher percentage of correct mental mod-\nels after constraint reasoning (orange) as compared to\nraw LM predictions (blue), no matter the threshold for\nconsidering a mental model to be correct is lower or\nhigher. For improvements from constraint reasoning\n(black), we observe the highest increase in percentage\nof mental models that are at least 60-80% accurate.\nentities like typewriter, bed, air conditional, and\ncomputer are challenging for both models to form\naccurate mental models of. Although the models\nshare some similarities in what everyday things\nthey have better/worse mental models of, they also\nshow differences, especially for man-made devices:\ne.g. GPT-3 does well but Macaw-11B performs\npoorly on forming an accurate parts mental model\nof piano; Macaw-11B does well, but GPT-3 per-\nforms poorly on devices like doorbell, digital clini-\ncal thermometer, and binoculars.\n7 Conclusion\nDo language models have coherent mental models\nof everyday things? To systematically study this\nquestion, we present a benchmark dataset, ParRoT,\nconsisting of 300 human-constructed mental mod-\nels for 100 everyday objects, including over 2K\n(a) GPT-3\n(b) Macaw-11B\nFigure 4: Accuracy of base LM and improvement\nachieved through constraint reasoning on different rela-\ntions in ParRoT dataset.\nparts and 11.7K relationships between these parts.\nOur experiments reveal that even SOTA LMs gen-\nerally have poor mental models (inaccurate and vio-\nlating basic commonsense constraints) of everyday\nthings, thus providing insight into their apparent\nknowledge and behavior not previously explored.\nWe apply constraint reasoning on top of base LM\npredictions to construct more coherent mental mod-\nels. Our method, ParRoT-Con, improves both accu-\nracy (up to 20% improvement) and consistency (up\nto 43% improvement) of such parts mental models.\nThis suggests a broader cognitive architecture (LM\n+ reasoner) for future systems, to construct more\ncoherent mental models than using the LM alone.\n1899\n(a) GPT-3\n(b) Macaw-11B\nFigure 5: 20 everyday things that each model achieved\nbest performanceon, based on models’ raw predictions\n(i.e. Base LM). In almost all cases, constraint reasoning\nboosts the accuracy of the parts mental models produced\nby the base LM, pushing it even closer to 100%.\n(a) GPT-3\n(b) Macaw-11B\nFigure 6: 20 everyday things that each model achieved\nworst performanceon, based on models’ raw predic-\ntions (i.e. Base LM). In many of these cases, the accu-\nracy of the parts mental models produced by the base\nLM is at around or below chance level and constraint\nreasoning boosts accuracy to beyond 50%.\n1900\nLimitations\nCommon everyday things change over the years.\nWhile we try to choose ones that are in chil-\ndren’s vocabulary, over decades, devices evolve\nand humans change in which things they interact\nwith more frequently, affecting which relationships\nwould be more prominent in an average person’s\nmental model. So the parts mental models in such a\ndataset may not stay constant over time (e.g. some\nentities may be less familiar and certain relations\nmay be less salient to annotators of the future). It\nwould be interesting to use our ParRoT dataset as a\npoint of comparison when studying mental models\nof everyday things in the future to reveal interesting\ninsights on how humans’ mental models of every-\nday things evolve over time.\nOther important future directions include to ex-\nplore how more coherent mental models can help\nin complex reasoning tasks about everyday things,\ncombine these parts mental models with mental\nmodels along other dimensions e.g. Gu et al.\n(2022a,b), as well as using our dataset of common-\nsense queries about everyday things as a source\nof follow-up questions for existing QA tasks e.g.,\nPIQA (Bisk et al., 2020) and CSQA (Talmor et al.,\n2019).\nThis paper only focuses on relationships (spa-\ntial orientation, connectivity, and functional depen-\ndency) between parts of everyday things. However,\nour approach ParRoT-Con is easily extensible to\nother applications such as:\n• spatial relations in other domains e.g. for geo-\ngraphical distances, we can similarly impose\nconstraints on inverse relations likecloser and\nfurther\n• temporal relations e.g. on a timeline, if event\nA occurred before event B, then event B can-\nnot have occurred before event A (before is\nasymmetric)\nWe leave the demonstration of the generalizability\nof our approach to future works.\nEthics Statement\nAll annotators that participated in the data collec-\ntion process have been anonymized. The only per-\nsonal information we collect is the worker IDs from\nAmazon Mechanical Turk, which we will not re-\nlease. No personally identifiable information is\ncontained in our dataset or otherwise released. We\ntook great care to pay fair wages, and were respon-\nsive to feedback and questions throughout the data\ncollection process. This study involves the use of\nlarge-scale language models. We only use them\nto generate True/False answers to questions about\nparts of everyday things, therefore we do not fore-\nsee any substantial ethical issues with their use for\nresearch presented in this submission.\nAcknowledgements\nWe thank the anonymous ACL reviewers, as well\nas Ernest Davis, Chris Callison-Burch and mem-\nbers of the Aristo team at AI2 for their valuable\nfeedback on an earlier draft.\nReferences\nJacob Andreas. 2022. Language models as agent mod-\nels. In Findings of the Association for Computational\nLinguistics: EMNLP 2022 , pages 5769–5779, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language mod-\nels be too big? In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, FAccT ’21, page 610–623, New York, NY ,\nUSA. Association for Computing Machinery.\nYonatan Bisk, Rowan Zellers, Ronan Le bras, Jianfeng\nGao, and Yejin Choi. 2020. Piqa: Reasoning about\nphysical commonsense in natural language. Proceed-\nings of the AAAI Conference on Artificial Intelligence,\n34(05):7432–7439.\nWonder House Books. 2018a. My First 100 Things that\nmove. Wonder House Books.\nWonder House Books. 2018b. My First Library : Boxset\nof 10 Board Books for Kids. Wonder House Books.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nKenneth James Williams Craik. 1943. The nature of ex-\nplanation, volume 445. Cambridge University Press.\n1901\nSoham Dan, Hangfeng He, and Dan Roth. 2020. Under-\nstanding spatial relations through multiple modalities.\nIn Proceedings of the Twelfth Language Resources\nand Evaluation Conference, pages 2368–2372, Mar-\nseille, France. European Language Resources Asso-\nciation.\nValorie Fisher. 2019. Now You Know How It Works .\nScholastic.\nSteve Graham, Karen R. Harris, and Connie Loy-\nnachan. The Basic Spelling V ocabulary List.\nhttps://www.readingrockets.org/article/\nbasic-spelling-vocabulary-list . Accessed:\n2022-09-23.\nYuling Gu, Bhavana Dalvi, and Peter Clark. 2022a.\nDREAM: Improving situational QA by first elab-\norating the situation. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 1115–1127, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nYuling Gu, Yao Fu, Valentina Pyatkin, Ian Magnus-\nson, Bhavana Dalvi Mishra, and Peter Clark. 2022b.\nJust-DREAM-about-it: Figurative language under-\nstanding with DREAM-FLUTE. In Proceedings of\nthe 3rd Workshop on Figurative Language Process-\ning (FLP), pages 84–93, Abu Dhabi, United Arab\nEmirates (Hybrid). Association for Computational\nLinguistics.\nDavid Gunning, Vinay K Chaudhri, Peter E Clark, Ken\nBarker, Shaw-Yi Chaw, Mark Greaves, Benjamin\nGrosof, Alice Leung, David D McDonald, Sunil\nMishra, et al. 2010. Project halo update—progress\ntoward digital aristotle. AI Magazine, 31(3):33–58.\nDavid R Ha and Jürgen Schmidhuber. 2018. World\nmodels. arXiv preprint, abs/1803.10122.\nGraeme S. Halford. 1993. Children’s Understanding:\nThe Development of Mental Models. Lawrence Erl-\nbaum Associates, Inc.\nS. J. Hespos and E. S Spelke. 2004. Conceptual precur-\nsors to language. In Nature. Nature.\nYining Hong, Li Yi, Josh Tenenbaum, Antonio Tor-\nralba, and Chuang Gan. 2021. Ptr: A benchmark\nfor part-based conceptual, relational, and physical\nreasoning. In Advances in Neural Information Pro-\ncessing Systems , volume 34, pages 17427–17440.\nCurran Associates, Inc.\nAlexey Ignatiev, Antonio Morgado, and Joao Marques-\nSilva. 2018a. PySAT: A Python toolkit for prototyp-\ning with SAT oracles. In SAT, pages 428–437.\nAlexey Ignatiev, Antonio Morgado, and Joao Marques-\nSilva. 2018b. Rc2: a python-based maxsat solver.\nMaxSAT Evaluation, 2018:22.\nAnya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr,\nWai Keen V ong, Robert Hawkins, and Yoav Artzi.\n2022. Abstract visual reasoning with tangram shapes.\nIn Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pages 582–\n601, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nP. Johnson-Laird. 1983. Mental Models : Towards a\nCognitive Science of Language, Inference and Con-\nsciousness. Harvard University Press.\nP. Johnson-Laird. 2006. How we reason. Oxford Uni-\nversity Press.\nDavid H. Jonassen and Philip Henning. 1996. Mental\nmodels: Knowledge in the head and knowledge in the\nworld. Educational Technology archive, 39:37–42.\nNora Kassner, Oyvind Tafjord, Hinrich Schütze, and\nPeter Clark. 2021. BeliefBank: Adding memory to a\npre-trained language model for a systematic notion\nof belief. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 8849–8861, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nTao Li, Vivek Gupta, Maitrey Mehta, and Vivek\nSrikumar. 2019a. A logic-driven framework for\nconsistency of neural models. arXiv preprint\narXiv:1909.00126.\nYong-Lu Li, Liang Xu, Xinpeng Liu, Xijie Huang,\nYue Xu, Mingyang Chen, Ze Ma, Shiyi Wang,\nHao-Shu Fang, and Cewu Lu. 2019b. Hake: Hu-\nman activity knowledge engine. arXiv preprint\narXiv:1904.06539.\nGeorge A. Miller. 1994. WordNet: A lexical database\nfor English. In Human Language Technology: Pro-\nceedings of a Workshop held at Plainsboro, New\nJersey, March 8-11, 1994.\nEric Mitchell, Joseph J. Noh, Siyan Li, William S. Arm-\nstrong, Ananth Agarwal, Patrick Liu, Chelsea Finn,\nand Christopher D. Manning. 2022. Enhancing self-\nconsistency and performance of pretrained language\nmodels with nli. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP). Association for Computational\nLinguistics.\nDonald A. Norman. 2013. The Design of Everyday\nThings: Revised and Expanded Edition . Basic\nBooks.\nMaxwell Nye, Michael Tessler, Josh Tenenbaum, and\nBrenden M Lake. 2021. Improving coherence and\nconsistency in neural sequence models with dual-\nsystem, neuro-symbolic reasoning. In Advances in\nNeural Information Processing Systems, volume 34,\npages 25192–25204. Curran Associates, Inc.\nJochen Renz, editor. 2002. The Region Connection\nCalculus, pages 41–50. Springer Berlin Heidelberg,\nBerlin, Heidelberg.\n1902\nPritish Sahu, Michael Cogswell, Yunye Gong, and\nAjay Divakaran. 2022. Unpacking large language\nmodels with conceptual consistency. arXiv preprint\narXiv:2209.15093.\nMurray Shanahan. 2022. Talking about large language\nmodels. arXiv preprint, abs/2212.03551.\nOyvind Tafjord and Peter Clark. 2021. General-purpose\nquestion-answering with Macaw. arXiv preprint\narXiv:2109.02593.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149–4158, Minneapolis, Minnesota. Association for\nComputational Linguistics.\n1903\nA Source of everyday things\nWe compiled a list of 100 everyday things from:\n1. Children’s books\n(a) My First Library series (Books, 2018b)\n(b) Now you know how it works (Fisher, 2019)\n(c) My first 100 things that move (Books, 2018a)\n2. V ocabulary lists\n(a) Grade 1-5 vocabulary list (Graham et al.)\n(b) Select from all the nouns from an 8th-grade vocabulary list that were also under either “artifact”\nor “device” in WordNet (Miller, 1994)\n3. Online web search\nB Details on mental model annotation task\nMechanical Turk task instructions:\n1904\nOur participants were recruited on the Amazon Mechanical Turk platform. The workers met minimum\nqualification in AMT: 95% approval rate. They were from US locations and rated at Amazon’s Masters\nLevel. Workers were paid at a rate of ≈$15/hr.\nC Unanimity and diversity in parts mental models\nPeople vary greatly in how they construct mental models, but the underlying reasoning is often structurally\nsimilar i.e. in accordance with commonsense constraints (Halford, 1993; Jonassen and Henning, 1996).\nIn our ParRoT dataset, similarly, contradictions amongst crowdworkers (e.g., for guitar, one worker\nannotated that the neck is part of the fingerboard, while another annotated that the fingerboard is part\nof the neck) are extremely rare. There are only 80 instances out of 11720 in total in our entire dataset\n(0.68%) – less than 1%.\nWe also looked at relations overlapped across workers in our dataset to analyze if workers pay attention\nto similar or different aspects of everyday things. To do so, we gathered a set of (p1, rln, p2) relations\nthat are common across all 3 annotators for each everyday thing. These relationships are ones that\nachieved full agreement across all the 3 assigned annotators for that everyday thing in terms of the\nspatial/connectivity/functional relationship annotated and the parts involved. Together, we refer to this\nset as the ParRoT++ dataset. Table 5 summarizes the number of such high-agreement relationships for\neach everyday thing. Everyday things with few or no high-agreement relationships (refer Figure 7 for\nan example) imply higher diversity among annotators in terms of which spatial/connectivity/functional\nrelationship and what parts they decided to include in their annotations. There are a total of 508 overlapped\nrelations in ParRoT++, out of the 11720 in ParRoT, suggesting that attention is often paid to different\naspects of everyday things.\nIn Table 6, we present accuracy on ParRoT++, revealing similar results for relationships that achieved\nfull agreement across all assigned annotators. Using basic commonsense constraints, ParRoT-Con\nimproves parts mental model accuracy significantly by 16-22% on ParRoT++. These trends are similar\nto that obtained for ParRoT, illustrating that the results hold across all gold-standard parts relations,\nregardless of whether they are more unanimous or diverse across annotators.\n1905\n# full-agreem.\nrelations Everyday thing(s)\n36 coffee maker, fish\n28 rabbit\n18 deer\n16 egg, electric stove, tree\n14 ink pen\n12 laptop, sandwich, rice cooker, airplane, table\n10 fire extinguisher, bird\n8 elevator, flashlight, stroller, dishwasher, kayak, ship, teapot, telescope,\ncorn, hot air balloon, microwave\n6 wheelchair, barbeque grill, kite, microphone, computer, duck, helicopter\n4\npillow, truck, washing machine, door, hair dryer, rocket, screw, toaster,\nbutterfly, chair, knife, photo frame, shoe, baby bottle, bed, bird cage,\ncar, chainsaw, electric tea kettle, humidifier, piano\n2 binoculars, digital camera, zipper, apple, digital clinical thermometer, earphone, flower,\nwindmill, backpack, dog, doorbell, lightbulb, bat, cat, umbrella, stethoscope, tent\n0\nair conditioner, bicycle, blender, boat, glider, guitar, house, pencil sharpener,\ntable fan, dryer, pencil, suitcase, telephone, microscope, refrigerator, space\nheater, typewriter, violin, wall clock, window, bookcase, bus, cable car, calculator,\nsaucepan, train, cow, rat, table lamp\nTable 5: Number of relationships that achieved full agreement across all the 3 assigned annotators for each everyday\nthing. Higher number of such relations indicates more unanimous parts mental model annotations, whereas lower\nnumber reflects more diversity.\n# params Base\nLM (%)\nParRoT-Con\n(%)\nImprove\n(%)\nGPT-3 (text-\ndavinci-003) 175B 55.51 71.13 15.62\nMacaw-11B 11B 60.04 82.41 22.38\nTable 6: Comparing the accuracy of parts mental models before and after constraint reasoning on ParRoT++ dataset.\nFigure 7: Example parts mental model annotations from ParRoT: (a) we provide the crowdworkers a diagram of\ncow retrieved from the Web. (b), (c), (d) are parts mental model sketches by 3 different crowdworkers. Note that all\n3 models are accurate but there is some divergence in terms of (1) part names: e.g., ‘head’ vs ‘forehead’ and (2)\nwhich relation tuples they consider salient. Similar forms of diversity have been reported in Ji et al. (2022), for\ninstance, as part naming divergence and segmentation divergence.\n1906\nD Pictorial illustration of ParRoT-Con\nOur proposed approach, ParRoT-Con, is illustrated in Figure 8 with an example everyday entity “egg”.\nFigure 8: When asked about relationships between parts of an everyday thing, LMs can produce inconsistent\nrelations. E.g., GPT-3 believes that in an egg, “yolk surrounds the shell” and “shell surrounds the yolk” are both\nTrue. Our proposed neuro-symbolic method, ParRoT-Con, applies constraint reasoning over raw LM predictions to\nproduce more accurate and consistent mental models of everyday things.\nE Accuracy on different everyday things\nTable 7 gives example prompts and GPT-3’s responses (includes both correct and incorrect) for entity\n“tree”. Top 20 and bottom 20 everyday things that each model achieved best and worst performance on are\nshown in Figures 5 and 6 respectively. Further, Figure 11 demonstrates everyday things with 21st to 80th\nranking in terms of the base LM accuracy.\nModel Prompt Model’s Answer\nGPT-3 Judge whether this statement is true or false:\nIn a tree, twig is directly connected to the branches. True (correct)\nGPT-3 Judge whether this statement is true or false:\nIn a tree, trunk is above the roots. False (incorrect)\nGPT-3 Judge whether this statement is true or false:\nIn a tree, roots are surrounded by the trunk. True (incorrect)\nGPT-3 Judge whether this statement is true or false:\nIn a tree, trunk is below the roots. False (correct)\nTable 7: Example prompts and GPT-3’s responses for an everyday entity “tree”.\nF Use of models for inference\nFor all experiments in this paper we used existing models/toolkits without any re-training or fine-tuning.\nWe used GPT-3 text-davinci-003 and Macaw (T5-11B based) as representative LMs for our experiments.\nTo probe GPT-3 text-davinci-003, we used their web API which took around 30 to 60 msec per relation\ntuple (one T/F question). To probe Macaw, we used two 48GB GPUs and it takes around 10.4 msec\nper relation tuple. We also run a MaxSAT solver for each everyday entity’s parts mental model. To\nsolve a constraint satisfaction problem per parts mental model takes a few msec up to around 3 minutes\ndepending on the WCNF formula involved.\nG On the use of our dataset and code\nWe have made all data and code used in this paper publicly available. Our dataset and code are released\nfor research purposes only.\n1907\nH FAQs\nQ: Does ChatGPT do better?\nFrom informal tests, we find that ChatGPT is not devoid of mistakes either. We provide some\nexamples to illustrate how the lack of coherent mental models of everyday things may also appear\nfor other models of the GPT-3.5 family, like ChatGPT in Figure 9. Others have also found ChatGPT\nresponses that convey ridiculous interactions with everyday things e.g. it generates that “When you\nfry an egg, the white and the yolk are both held together by the eggshell.” (See Figure 10)\nQ: GPT-3 and ChatGPT models are often updated, when were the models accessed for your\nexperiments?\nIn our experiments with GPT-3, we used the text-davinci-003 model and queried the API on Decem-\nber 16, 2022 (during the period of time between 12 PM to 3.30 PM PST). ChatGPT as in Figure 9 was\naccessed on December 17, 2022 (at around 9.30 PM PST). It would be interesting for researchers to in-\nvestigate if future versions of the systems can construct better parts mental models of everyday things.\nQ: How do you ensure high-quality mental models are acquired via crowdsourcing?\nWe enforced a set of manual and automated checks during data acquisition which includes collecting\nmental model sketches and transcribing them into relation tuples.\nManual checks: We randomly sampled 15 mental model sketches and made sure that the tran-\nscription of relation tuples was accurate i.e. all the relations tuples in mental model sketches drawn\nby crowdworkers were precisely added to our dataset. We also checked the quality and format of\nsketches (‘.png’ files) which will be released with our dataset.\nAutomated checks:After enriching with implied relations, we also programatically checked that all\nindividual mental models (total of 11.7K relations) in ParRoT are fully consistent (based on the 4\ncommonsense constraints described in Section 5.2).\nQ: Do similar trends apply to smaller models?\nExperiments on Macaw-3B, Macaw-large, UnifiedQA-large pointed towards the same trends. We\nalso make our code and data fully accessible at https://github.com/allenai/everyday-things\nfor interested researchers to experiment with other models of interest to them.\nQ: Can ParRoT-Con be applied to other languages?\nWhile our dataset is in English, relationships between parts of everyday things could indeed be\nauthored for/ translated into other languages. We made our code and data publicly available, so\nothers could use the infrastructure to apply the technique to other languages.\n1908\nChatGPT\nFigure 9: Like GPT-3 (text-davinci-003), ChatGPT also seems to have incoherent mental pictures of everyday things.\n1909\nChatGPT\nFigure 10: ChatGPT provides ridiculous responses regarding daily life activities such as frying an egg, illustrating\npoor mental models of everyday things and interactions with them. (Example by @bio_bootloader, posted on\nTwitter https://twitter.com/bio_bootloader/status/1599131249553330176/photo/1 at 11:59 AM Dec 3,\n2022.)\n1910\n(a) GPT-3\n (b) Macaw-11B\nFigure 11: Performance on other everyday things. Accuracy of base LM and improvement achieved through\nconstraint reasoning on different everyday things in our dataset.\n1911\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nYes, we discussed the limitations of our work in the \"Limitations\" section.\n□\u0013 A2. Did you discuss any potential risks of your work?\nYes, we discussed the potential risks of our work in the \"Ethics Statement\" section.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nYes, the abstract at the start and section 1 introduction.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nNo.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 4 provides details on the dataset we created. Section 5 discusses how we use existing language\nmodels.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nYes, we cited the models used in Section 5.1. We explained who helped with the creation of the dataset\n(Section 4 and Appendix B on crowdworkers and instructions given to them).\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nAppendix G.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nAppendix G. \"Our dataset and code are released for research purposes only. \"\n□\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\n\"Ethics Statement\" section discusses that we removed any personally identiﬁable information.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nWe provide details on domain of our data (Section 4), crowdworker demographics (Appendix B on\ncrowdworkers)\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 4.3\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1912\nC □\u0013 Did you run computational experiments?\nSection 5\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nResults table in Section 6. Appendix F .\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 5 discusses the experimental setup in detail. But no hyperparameter search is needed for our\npurposes.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4 dataset statistics and Section 6 results.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nSection 4 and Appendix B on crowdworkers and instructions given to them.\n□\u0013 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nAppendix B on crowdworkers and instructions given to them.\n□\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nAppendix B on crowdworkers.\n□\u0013 D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nAppendix B. We explained why are we collecting this data and how the data would be used.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□\u0013 D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nAppendix B on crowdworkers.\n1913",
  "topic": "Everyday life",
  "concepts": [
    {
      "name": "Everyday life",
      "score": 0.6837790012359619
    },
    {
      "name": "Constraint (computer-aided design)",
      "score": 0.6285247802734375
    },
    {
      "name": "Computer science",
      "score": 0.6215155720710754
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5598353147506714
    },
    {
      "name": "Mental state",
      "score": 0.4554843306541443
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44081810116767883
    },
    {
      "name": "Extension (predicate logic)",
      "score": 0.42409634590148926
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.41933608055114746
    },
    {
      "name": "Natural language processing",
      "score": 0.3363766372203827
    },
    {
      "name": "Psychology",
      "score": 0.2512332499027252
    },
    {
      "name": "Cognitive psychology",
      "score": 0.24930498003959656
    },
    {
      "name": "Mathematics",
      "score": 0.18709543347358704
    },
    {
      "name": "Epistemology",
      "score": 0.1671580970287323
    },
    {
      "name": "Programming language",
      "score": 0.11750337481498718
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210140341",
      "name": "Allen Institute",
      "country": "US"
    }
  ]
}