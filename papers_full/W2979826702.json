{
  "title": "Transformers: State-of-the-Art Natural Language Processing",
  "url": "https://openalex.org/W2979826702",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A1873885448",
      "name": "Thomas Wolf",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2978660204",
      "name": "Lysandre Debut",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2901920096",
      "name": "Victor Sanh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2794942406",
      "name": "Julien Chaumond",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2795365971",
      "name": "Clément Delangue",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2980009562",
      "name": "Anthony Moi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2979782832",
      "name": "Pierric Cistac",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2979533015",
      "name": "Tim Rault",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1207224943",
      "name": "Rémi Louf",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2924642807",
      "name": "Morgan Funtowicz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2797161518",
      "name": "Joe Davison",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2924981553",
      "name": "Sam Shleifer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2954968672",
      "name": "Patrick von Platen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100847467",
      "name": "Clara Ma",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A148301988",
      "name": "Yacine Jernite",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1885034522",
      "name": "Julien Plu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2970865618",
      "name": "Canwen Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3099917620",
      "name": "Teven Le Scao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2222789889",
      "name": "Sylvain Gugger",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3099575957",
      "name": "Mariama Drame",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3105155681",
      "name": "Quentin Lhoest",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2294834069",
      "name": "Alexander Rush",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2953320089",
    "https://openalex.org/W3100279624",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2972119347",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W4297747548",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2963691697",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2979949198",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2951490152",
    "https://openalex.org/W2804032941",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3037109418",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2752194699",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4287760320",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3037191812",
    "https://openalex.org/W4302343710",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W2964343359",
    "https://openalex.org/W2964110616"
  ],
  "abstract": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander Rush. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2020.",
  "full_text": "Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 38–45\nNovember 16-20, 2020.c⃝2020 Association for Computational Linguistics\n38\nTransformers: State-of-the-Art Natural Language Processing\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,\nAnthony Moi, Pierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, Joe Davison,\nSam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Alexander M. Rush\nHugging Face, Brooklyn, USA / {first-name}@huggingface.co\nAbstract\nRecent progress in natural language process-\ning has been driven by advances in both model\narchitecture and model pretraining. Trans-\nformer architectures have facilitated building\nhigher-capacity models and pretraining has\nmade it possible to effectively utilize this ca-\npacity for a wide variety of tasks. Trans-\nformers is an open-source library with the\ngoal of opening up these advances to the\nwider machine learning community. The li-\nbrary consists of carefully engineered state-\nof-the art Transformer architectures under a\nuniﬁed API. Backing this library is a cu-\nrated collection of pretrained models made\nby and available for the community. Trans-\nformers is designed to be extensible by re-\nsearchers, simple for practitioners, and fast\nand robust in industrial deployments. The li-\nbrary is available at https://github.com/\nhuggingface/transformers.\n1 Introduction\nThe Transformer (Vaswani et al., 2017) has rapidly\nbecome the dominant architecture for natural lan-\nguage processing, surpassing alternative neural\nmodels such as convolutional and recurrent neural\nnetworks in performance for tasks in both natural\nlanguage understanding and natural language gen-\neration. The architecture scales with training data\nand model size, facilitates efﬁcient parallel training,\nand captures long-range sequence features.\nModel pretraining (McCann et al., 2017; Howard\nand Ruder, 2018; Peters et al., 2018; Devlin et al.,\n2018) allows models to be trained on generic cor-\npora and subsequently be easily adapted to speciﬁc\ntasks with strong performance. The Transformer\narchitecture is particularly conducive to pretrain-\ning on large text corpora, leading to major gains in\naccuracy on downstream tasks including text classi-\nﬁcation (Yang et al., 2019), language understanding\n(Liu et al., 2019b; Wang et al., 2018, 2019), ma-\nchine translation (Lample and Conneau, 2019a),\ncoreference resolution (Joshi et al., 2019), com-\nmonsense inference (Bosselut et al., 2019), and\nsummarization (Lewis et al., 2019) among others.\nThis advance leads to a wide range of practical\nchallenges that must be addressed in order for these\nmodels to be widely utilized. The ubiquitous use of\nthe Transformer calls for systems to train, analyze,\nscale, and augment the model on a variety of plat-\nforms. The architecture is used as a building block\nto design increasingly sophisticated extensions and\nprecise experiments. The pervasive adoption of pre-\ntraining methods has led to the need to distribute,\nﬁne-tune, deploy, and compress the core pretrained\nmodels used by the community.\nTransformers is a library dedicated to supporting\nTransformer-based architectures and facilitating the\ndistribution of pretrained models. At the core of\nthe libary is an implementation of the Transformer\nwhich is designed for both research and production.\nThe philosophy is to support industrial-strength im-\nplementations of popular model variants that are\neasy to read, extend, and deploy. On this founda-\ntion, the library supports the distribution and usage\nof a wide-variety of pretrained models in a cen-\ntralized model hub. This hub supports users to\ncompare different models with the same minimal\nAPI and to experiment with shared models on a\nvariety of different tasks.\nTransformers is an ongoing effort maintained by\nthe team of engineers and researchers at Hugging\nFace with support from a vibrant community of\nover 400 external contributors. The library is re-\nleased under the Apache 2.0 license and is available\non GitHub1. Detailed documentation and tutorials\nare available on Hugging Face’s website2.\n1https://github.com/huggingface/\ntransformers\n2https://huggingface.co/transformers/\n39\nFigure 1: Average daily unique downloads of the most downloaded pretrained models, Oct. 2019 to May 2020.\n2 Related Work\nThe NLP and ML communities have a strong cul-\nture of building open-source research tools. The\nstructure of Transformers is inspired by the pi-\noneering tensor2tensor library (Vaswani et al.,\n2018) and the original source code for BERT (De-\nvlin et al., 2018), both from Google Research.\nThe concept of providing easy caching for pre-\ntrained models stemmed from AllenNLP (Gard-\nner et al., 2018). The library is also closely re-\nlated to neural translation and language modeling\nsystems, such as Fairseq (Ott et al., 2019), Open-\nNMT (Klein et al., 2017), Texar (Hu et al., 2018),\nMegatron-LM (Shoeybi et al., 2019), and Mar-\nian NMT (Junczys-Dowmunt et al., 2018). Build-\ning on these elements, Transformers adds extra\nuser-facing features to allow for easy downloading,\ncaching, and ﬁne-tuning of the models as well as\nseamless transition to production. Transformers\nmaintains some compatibility with these libraries,\nmost directly including a tool for performing infer-\nence using models from Marian NMT and Google’s\nBERT.\nThere is a long history of easy-to-use, user-\nfacing libraries for general-purpose NLP. Two core\nlibraries are NLTK (Loper and Bird, 2002) and\nStanford CoreNLP (Manning et al., 2014), which\ncollect a variety of different approaches to NLP in\na single package. More recently, general-purpose,\nopen-source libraries have focused primarily on\nmachine learning for a variety of NLP tasks, these\ninclude Spacy (Honnibal and Montani, 2017), Al-\nlenNLP (Gardner et al., 2018), ﬂair (Akbik et al.,\n2019), and Stanza (Qi et al., 2020). Transform-\ners provides similar functionality as these libraries.\nAdditionally, each of these libraries now uses the\nTransformers library and model hub as a low-level\nframework.\nSince Transformersprovides a hub for NLP mod-\nels, it is also related to popular model hubs includ-\ning Torch Hub and TensorFlow Hub which collect\nframework-speciﬁc model parameters for easy use.\nUnlike these hubs, Transformers is domain-speciﬁc\nwhich allows the system to provide automatic sup-\nport for model analysis, usage, deployment, bench-\nmarking, and easy replicability.\n3 Library Design\nTransformers is designed to mirror the standard\nNLP machine learning model pipeline: process\ndata, apply a model, and make predictions. Al-\nthough the library includes tools facilitating train-\ning and development, in this technical report we\nfocus on the core modeling speciﬁcations. For\ncomplete details about the features of the library\nrefer to the documentation available on https:\n//huggingface.co/transformers/.\nEvery model in the library is fully deﬁned by\nthree building blocks shown in the diagram in Fig-\nure 2: (a) a tokenizer, which converts raw text to\nsparse index encodings, (b) a transformer, which\ntransforms sparse indices to contextual embed-\ndings, and (c) a head, which uses contextual em-\nbeddings to make a task-speciﬁc prediction. Most\nuser needs can be addressed with these three com-\nponents.\nTransformers Central to the library are carefully\ntested implementations of Transformer architecture\nvariants which are widely used in NLP. The full list\nof currently implemented architectures is shown in\nFigure 2 (Left). While each of these architectures\n40\nHeads\nName Input Output Tasks Ex. Datasets\nLanguage Modeling x1:n−1 xn ∈V Generation WikiText-103\nSequence Classiﬁcation x1:N y ∈C Classiﬁcation,\nSentiment Analysis\nGLUE, SST,\nMNLI\nQuestion Answering x1:M ,xM:N y span [1 :N] QA, Reading\nComprehension\nSQuAD,\nNatural Questions\nToken Classiﬁcation x1:N y1:N ∈CN NER, Tagging OntoNotes, WNUT\nMultiple Choice x1:N , X y ∈X Text Selection SW AG, ARC\nMasked LM x1:N\\n xn ∈V Pretraining Wikitext, C4\nConditional Generation x1:N y1:M ∈VM Translation,\nSummarization\nWMT, IWSLT,\nCNN/DM, XSum\nTransformers\nMasked [x1:N\\n ⇒xn]\nBERT (Devlin et al., 2018)\nRoBERTa (Liu et al., 2019a)\nAutoregressive [x1:n−1 ⇒xn]\nGPT / GPT-2 (Radford et al., 2019)\nTrans-XL (Dai et al., 2019)\nXLNet (Yang et al., 2019)\nSeq-to-Seq [∼x1:N ⇒x1:N ]\nBART (Lewis et al., 2019)\nT5 (Raffel et al., 2019)\nMarianMT (J.-Dowmunt et al., 2018)\nSpecialty: Multimodal\nMMBT (Kiela et al., 2019)\nSpecialty: Long-Distance\nReformer (Kitaev et al., 2020)\nLongformer (Beltagy et al., 2020)\nSpecialty: Efﬁcient\nALBERT (Lan et al., 2019)\nElectra (Clark et al., 2020)\nDistilBERT (Sanh et al., 2019)\nSpecialty: Multilingual\nXLM/RoBERTa (Lample and Conneau, 2019b)\nTransformer\nTokenizer\nHead Head\nTokenizers\nName Ex. Uses\nCharacter-Level BPE NMT, GPT\nByte-Level BPE GPT-2\nWordPiece BERT\nSentencePiece XLNet\nUnigram LM\nCharacter Reformer\nCustom Bio-Chem\nFigure 2: The Transformers library. (Diagram-Right) Each model is made up of a Tokenizer, Transformer, and\nHead. The model is pretrained with a ﬁxed head and can then be further ﬁne-tuned with alternate heads for different\ntasks. (Bottom) Each model uses a speciﬁc Tokenizer either implemented in Python or in Rust. These often differ\nin small details, but need to be in sync with pretraining. (Left) Transformer architectures specialized for different\ntasks, e.g. understanding versus generation, or for speciﬁc use-cases, e.g. speed, image+text. (Top) heads allow a\nTransformer to be used for different tasks. Here we assume the input token sequence is x1:N from a vocabulary V,\nand y represents different possible outputs, possibly from a class set C. Example datasets represent a small subset\nof example code distributed with the library.\n41\nshares the same multi-headed attention core, there\nare signiﬁcant differences between them including\npositional representations, masking, padding, and\nthe use of sequence-to-sequence design. Addition-\nally, various models are built to target different\napplications of NLP such as understanding, gener-\nation, and conditional generation, plus specialized\nuse cases such as fast inference or multi-lingual\napplications.\nPractically, all models follow the same hierarchy\nof abstraction: a base class implements the model’s\ncomputation graph from an encoding (projection\non the embedding matrix) through the series of self-\nattention layers to the ﬁnal encoder hidden states.\nThe base class is speciﬁc to each model and closely\nfollows the model’s original implementation which\ngives users the ﬂexibility to easily dissect the inner\nworkings of each individual architecture. In most\ncases, each model is implemented in a single ﬁle\nto enable ease of extensibility.\nWherever possible, different architectures fol-\nlow the same API allowing users to switch easily\nbetween different models. A set of Auto classes\nprovides a uniﬁed API that enables very fast switch-\ning between models and even between frameworks.\nThese classes automatically instantiate with the\nconﬁguration speciﬁed by the user-speciﬁed pre-\ntrained model.\nTokenizers A critical NLP-speciﬁc aspect of the\nlibrary is the implementations of the tokenizers nec-\nessary to use each model. Tokenizer classes (each\ninheriting from a common base class) can either be\ninstantiated from a corresponding pretrained model\nor can be conﬁgured manually. These classes store\nthe vocabulary token-to-index map for their corre-\nsponding model and handle the encoding and de-\ncoding of input sequences according to a model’s\nspeciﬁc tokenization process. The tokenizers im-\nplemented are shown in Figure 2 (Right). Users\ncan easily modify tokenizer with interfaces to add\nadditional token mappings, special tokens (such as\nclassiﬁcation or separation tokens), or otherwise\nresize the vocabulary.\nTokenizers can also implement additional useful\nfeatures for the users. These range from token type\nindices in the case of sequence classiﬁcation to\nmaximum length sequence truncating taking into\naccount the added model-speciﬁc special tokens\n(most pretrained Transformer models have a maxi-\nmum sequence length).\nFor training on very large datasets, Python-based\ntokenization is often undesirably slow. In the\nmost recent release, Transformers switched its im-\nplementation to use a highly-optimized tokeniza-\ntion library by default. This low-level library,\navailable at https://github.com/huggingface/\ntokenizers, is written in Rust to speed up the\ntokenization procedure both during training and\ndeployment.\nHeads Each Transformer can be paired with\none out of several ready-implemented heads\nwith outputs amenable to common types of\ntasks. These heads are implemented as ad-\nditional wrapper classes on top of the base\nclass, adding a speciﬁc output layer, and op-\ntional loss function, on top of the Transformer’s\ncontextual embeddings. The full set of im-\nplemented heads are shown in Figure 2 (Top).\nThese classes follow a similar naming pattern:\nXXXForSequenceClassification where\nXXX is the name of the model and can be used\nfor adaptation (ﬁne-tuning) or pretraining. Some\nheads, such as conditional generation, support extra\nfunctionality like sampling and beam search.\nFor pretrained models, we release the heads used\nto pretrain the model itself. For instance, for BERT\nwe release the language modeling and next sen-\ntence prediction heads which allows easy for adap-\ntation using the pretraining objectives. We also\nmake it easy for users to utilize the same core Trans-\nformer parameters with a variety of other heads for\nﬁnetuning. While each head can be used generally,\nthe library also includes a collection of examples\nthat show each head on real problems. These ex-\namples demonstrate how a pretrained model can be\nadapted with a given head to achieve state-of-the-\nart results on a large variety of NLP tasks.\n4 Community Model Hub\nTransformers aims to facilitate easy use and dis-\ntribution of pretrained models. Inherently this is\na community process; a single pretraining run fa-\ncilitates ﬁne-tuning on many speciﬁc tasks. The\nModel Hub makes it simple for any end-user to ac-\ncess a model for use with their own data. This hub\nnow contains 2,097 user models, both pretrained\nand ﬁne-tuned, from across the community. Fig-\nure 1 shows the increase and distribution of popular\ntransformers over time. While core models like\nBERT and GPT-2 continue to be popular, other spe-\ncialized models including DistilBERT (Sanh et al.,\n2019), which was developed for the library, are\n42\nFigure 3: Transformers Model Hub. (Left) Example of a model page and model card for SciBERT (Beltagy\net al., 2019), a pretrained model targeting extraction from scientiﬁc literature submitted by a community contrib-\nutor. (Right) Example of an automatic inference widget for the pretrained BART (Lewis et al., 2019) model for\nsummarization. Users can enter arbitrary text and a full version of the model is deployed on the ﬂy to produce a\nsummary.\nnow widely downloaded by the community.\nThe user interface of the Model Hub is designed\nto be simple and open to the community. To upload\na model, any user can sign up for an account and\nuse a command-line interface to produce an archive\nconsisting a tokenizer, transformer, and head. This\nbundle may be a model trained through the library\nor converted from a checkpoint of other popular\ntraining tools. These models are then stored and\ngiven a canonical name which a user can use to\ndownload, cache, and run the model either for ﬁne-\ntuning or inference in two lines of code. To load\nFlauBERT (Le et al., 2020), a BERT model pre-\ntrained on a French training corpus, the command\nis:\n1 tknzr = AutoTokenizer.from_pretrained(\n2 \"flaubert/flaubert_base_uncased\")\n3 model = AutoModel.from_pretrained(\n4 \"flaubert/flaubert_base_uncased\")\nWhen a model is uploaded to the Model Hub, it\nis automatically given a landing page describing its\ncore properties, architecture, and use cases. Addi-\ntional model-speciﬁc metadata can be provided via\na model card (Mitchell et al., 2018) that describes\nproperties of its training, a citation to the work,\ndatasets used during pretraining, and any caveats\nabout known biases in the model and its predictions.\nAn example model card is shown in Figure 3 (Left).\nSince the Model Hub is speciﬁc to transformer-\nbased models, we can target use cases that would\nbe difﬁcult for more general model collections. For\nexample, because each uploaded model includes\nmetadata concerning its structure, the model page\ncan include live inference that allows users to ex-\nperiment with output of models on a real data. Fig-\nure 3 (Right) shows an example of the model page\nwith live inference. Additionally, model pages in-\nclude links to other model-speciﬁc tools like bench-\nmarking and visualizations. For example, model\npages can link to exBERT (Hoover et al., 2019), a\nTransformer visualization library.\nCommunity Case Studies The Model Hub high-\nlights how Transformers is used by a variety of\ndifferent community stakeholders. We summarize\nthree speciﬁc observed use-cases in practice. We\nhighlight speciﬁc systems developed by users with\ndifferent goals following the architect, trainer, and\nend-user distinction of Strobelt et al. (2017):\nCase 1: Model ArchitectsAllenAI, a major NLP\nresearch lab, developed a new pretrained model for\nimproved extraction from biomedical texts called\nSciBERT (Beltagy et al., 2019). They were able\nto train the model utilizing data from PubMed to\nproduce a masked language model with state-of-\nthe-art results on targeted text. They then used the\nModel Hub to distribute the model and promote\nit as part of their CORD - COVID-19 challenge,\nmaking it trivial for the community to use.\nCase 2: Task TrainersResearchers at NYU were\n43\ninterested in developing a test bed for the per-\nformance of Transformers on a variety of differ-\nent semantic recognition tasks. Their framework\nJiant (Pruksachatkun et al., 2020) allows them to\nexperiment with different ways of pretraining mod-\nels and comparing their outputs. They used the\nTransformers API as a generic front-end and per-\nformed ﬁne-tuning on a variety of different models,\nleading to research on the structure of BERT (Ten-\nney et al., 2019).\nCase 3: Application UsersPlot.ly, a company fo-\ncused on user dashboards and analytics, was in-\nterested in deploying a model for automatic doc-\nument summarization. They wanted an approach\nthat scaled well and was simple to deploy, but had\nno need to train or ﬁne-tune the model. They were\nable to search the Model Hub and ﬁnd DistilBART,\na pretrained and ﬁne-tuned summarization model\ndesigned for accurate, fast inference. They were\nable to run and deploy the model directly from the\nhub with no required research or ML expertise.\n5 Deployment\nAn increasingly important goal of Transformers is\nto make it easy to efﬁciently deploy model to pro-\nduction. Different users have different production\nneeds, and deployment often requires solving sig-\nniﬁcantly different challenges than training. The\nlibrary thereforce allows for several different strate-\ngies for production deployment.\nOne core propery of the libary is that models\nare available both in PyTorch and TensorFlow, and\nthere is interoperability between both frameworks.\nA model trained in one of frameworks can be saved\nthrough standard serialization and be reloaded from\nthe saved ﬁles in the other framework seamlessly.\nThis makes it particularly easy to switch from one\nframework to the other one along the model life-\ntime (training, serving, etc.).\nEach framework has deployment recommenda-\ntions. For example, in PyTorch, models are compat-\nible with TorchScript, an intermediate representa-\ntion of a PyTorch model that can then be run either\nin Python in a more efﬁcient way, or in a high-\nperformance environment such as C++. Fine-tuned\nmodels can thus be exported to production-friendly\nenvironment, and run through TorchServing. Ten-\nsorFlow includes several serving options within its\necosystem, and these can be used directly.\nTransformers can also export models to interme-\ndiate neural network formats for further compila-\nFigure 4: Experiments with Transformers inference in\ncollaboration with ONNX.\ntion. It supports converting models to the Open\nNeural Network Exchange format (ONNX) for de-\nployment. Not only does this allow the model to\nbe run in a standardized interoperable format, but\nalso leads to signiﬁcant speed-ups. Figure 4 shows\nexperiments run in collaboration with the ONNX\nteam to optimize BERT, RoBERTa, and GPT-2\nfrom the Transformers library. Using this interme-\ndiate format, ONNX was able to achieve nearly a\n4x speedup on this model. The team is also ex-\nperimenting with other promising intermediate for-\nmats such as JAX/XLA (Bradbury et al., 2018) and\nTVM (Chen et al., 2018).\nFinally, as Transformers become more widely\nused in all NLP applications, it is increasingly im-\nportant to deploy to edge devices such as phones\nor home electronics. Models can use adapters to\nconvert models to CoreML weights that are suit-\nable to be embedded inside a iOS application, to\nenable on-the-edge machine learning. Code is also\nmade available3. Similar methods can be used for\nAndroid devices.\n6 Conclusion\nAs Transformer and pretraining play larger roles in\nNLP, it is important for these models to be acces-\nsible to researchers and end-users. Transformers\nis an open-source library and community designed\nto facilitate users to access large-scale pretrained\nmodels, to build and experiment on top of them,\nand to deploy them in downstream tasks with state-\nof-the-art performance. Transformers has gained\nsigniﬁcant organic traction since its release and is\nset up to continue to provide core infrastructure\nwhile helping to facilitate access to new models.\n3https://github.com/huggingface/\nswift-coreml-transformers\n44\nReferences\na. PyTorch Hub. https://pytorch.org/hub/. Ac-\ncessed: 2020-6-29.\nb. TensorFlow hub. https://www.tensorflow.\norg/hub. Accessed: 2020-6-29.\nAlan Akbik, Tanja Bergmann, Duncan Blythe, Kashif\nRasul, Stefan Schweter, and Roland V ollgraf. 2019.\nFlair: An easy-to-use framework for state-of-the-art\nnlp. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics (Demonstrations), pages 54–\n59. aclweb.org.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientiﬁc text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3615–\n3620.\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The Long-Document transformer.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli C ¸ elikyilmaz, and Yejin Choi.\n2019. Comet: Commonsense transformers for auto-\nmatic knowledge graph construction. In ACL.\nJames Bradbury, Roy Frostig, Peter Hawkins,\nMatthew James Johnson, Chris Leary, Dou-\ngal Maclaurin, and Skye Wanderman-Milne.\n2018. JAX: composable transformations of\nPython+NumPy programs.\nTianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin\nZheng, Eddie Yan, Haichen Shen, Meghan Cowan,\nLeyuan Wang, Yuwei Hu, Luis Ceze, et al. 2018.\n{TVM}: An automated end-to-end optimizing com-\npiler for deep learning. In 13th {USENIX}Sympo-\nsium on Operating Systems Design and Implementa-\ntion ({OSDI}18), pages 578–594.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na Fixed-Length context.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding.\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson Liu, Matthew Pe-\nters, Michael Schmitz, and Luke Zettlemoyer. 2018.\nAllenNLP: A deep semantic natural language pro-\ncessing platform.\nMatthew Honnibal and Ines Montani. 2017. spacy 2:\nNatural language understanding with bloom embed-\ndings, convolutional neural networks and incremen-\ntal parsing. To appear, 7(1).\nBenjamin Hoover, Hendrik Strobelt, and Sebastian\nGehrmann. 2019. exBERT: A visual analysis tool to\nexplore learned representations in transformers mod-\nels.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nACL.\nZhiting Hu, Haoran Shi, Bowen Tan, Wentao Wang,\nZichao Yang, Tiancheng Zhao, Junxian He, Lianhui\nQin, Di Wang, Xuezhe Ma, Zhengzhong Liu, Xiao-\ndan Liang, Wangrong Zhu, Devendra Singh Sachan,\nand Eric P Xing. 2018. Texar: A modularized, ver-\nsatile, and extensible toolkit for text generation.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2019. Spanbert:\nImproving pre-training by representing and predict-\ning spans. arXiv preprint arXiv:1907.10529.\nMarcin Junczys-Dowmunt, Roman Grundkiewicz,\nTomasz Dwojak, Hieu Hoang, Kenneth Heaﬁeld,\nTom Neckermann, Frank Seide, Ulrich Germann, Al-\nham Fikri Aji, Nikolay Bogoychev, Andr´e F T Mar-\ntins, and Alexandra Birch. 2018. Marian: Fast neu-\nral machine translation in c++.\nDouwe Kiela, Suvrat Bhooshan, Hamed Firooz, and\nDavide Testuggine. 2019. Supervised multimodal\nbitransformers for classifying images and text.\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer.\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean Senel-\nlart, and Alexander Rush. 2017. OpenNMT: Open-\nsource toolkit for neural machine translation. In\nProceedings of ACL 2017, System Demonstrations,\npages 67–72, Vancouver, Canada. Association for\nComputational Linguistics.\nGuillaume Lample and Alexis Conneau. 2019a. Cross-\nlingual language model pretraining. In NeurIPS.\nGuillaume Lample and Alexis Conneau. 2019b. Cross-\nlingual language model pretraining.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. ALBERT: A lite BERT for self-supervised\nlearning of language representations.\nHang Le, Lo¨ıc Vial, Jibril Frej, Vincent Segonne, Max-\nimin Coavoux, Benjamin Lecouteux, Alexandre Al-\nlauzen, Benoˆıt Crabb´e, Laurent Besacier, and Didier\nSchwab. 2020. Flaubert: Unsupervised language\nmodel pre-training for french. In Proceedings of\nThe 12th Language Resources and Evaluation Con-\nference, pages 2479–2490, Marseille, France. Euro-\npean Language Resources Association.\n45\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Ves Stoyanov, and Luke Zettlemoyer.\n2019. BART: Denoising Sequence-to-Sequence pre-\ntraining for natural language generation, translation,\nand comprehension.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019a.\nRoBERTa: A robustly optimized BERT pretraining\napproach.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke S. Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv, abs/1907.11692.\nEdward Loper and Steven Bird. 2002. NLTK: The nat-\nural language toolkit.\nChristopher D Manning, Mihai Surdeanu, John Bauer,\nJenny Rose Finkel, Steven Bethard, and David Mc-\nClosky. 2014. The stanford CoreNLP natural lan-\nguage processing toolkit. In Proceedings of 52nd\nannual meeting of the association for computational\nlinguistics: system demonstrations , pages 55–60.\naclweb.org.\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In I Guyon, U V Luxburg,\nS Bengio, H Wallach, R Fergus, S Vishwanathan,\nand R Garnett, editors, Advances in Neural Informa-\ntion Processing Systems 30, pages 6294–6305. Cur-\nran Associates, Inc.\nMargaret Mitchell, Simone Wu, Andrew Zaldivar,\nParker Barnes, Lucy Vasserman, Ben Hutchinson,\nElena Spitzer, Inioluwa Deborah Raji, and Timnit\nGebru. 2018. Model cards for model reporting.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations.\nYada Pruksachatkun, Phil Yeres, Haokun Liu, Jason\nPhang, Phu Mon Htut, Alex Wang, Ian Tenney, and\nSamuel R Bowman. 2020. jiant: A software toolkit\nfor research on general-purpose text understanding\nmodels. arXiv preprint arXiv:2003.02249.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton,\nand Christopher D Manning. 2020. Stanza: A\npython natural language processing toolkit for many\nhuman languages.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed Text-to-Text trans-\nformer.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled version\nof BERT: smaller, faster, cheaper and lighter.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion\nparameter language models using gpu model paral-\nlelism. arXiv preprint arXiv:1909.08053.\nHendrik Strobelt, Sebastian Gehrmann, Hanspeter Pﬁs-\nter, and Alexander M Rush. 2017. Lstmvis: A tool\nfor visual analysis of hidden state dynamics in recur-\nrent neural networks. IEEE transactions on visual-\nization and computer graphics, 24(1):667–676.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBert rediscovers the classical nlp pipeline. In ACL.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Fran-\ncois Chollet, Aidan N. Gomez, Stephan Gouws,\nLlion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki\nParmar, Ryan Sepassi, Noam Shazeer, and Jakob\nUszkoreit. 2018. Tensor2tensor for neural machine\ntranslation. CoRR, abs/1803.07416.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł Ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I Guyon, U V Luxburg, S Bengio,\nH Wallach, R Fergus, S Vishwanathan, and R Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2019. Superglue: A\nstickier benchmark for general-purpose language un-\nderstanding systems. ArXiv, abs/1905.00537.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. In ICLR.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. ArXiv, abs/1906.08237.",
  "topic": "Art history",
  "concepts": [
    {
      "name": "Art history",
      "score": 0.5141780376434326
    },
    {
      "name": "Art",
      "score": 0.490401029586792
    },
    {
      "name": "Transformer",
      "score": 0.48080888390541077
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.4562976360321045
    },
    {
      "name": "Computer science",
      "score": 0.42094406485557556
    },
    {
      "name": "History",
      "score": 0.26137232780456543
    },
    {
      "name": "Engineering",
      "score": 0.12461817264556885
    },
    {
      "name": "Archaeology",
      "score": 0.1245415210723877
    },
    {
      "name": "Electrical engineering",
      "score": 0.11023387312889099
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4387154989",
      "name": "Hugging Face",
      "country": null
    }
  ],
  "cited_by": 7303
}