{
    "title": "Do Language Models Understand Measurements?",
    "url": "https://openalex.org/W4385573798",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2102182784",
            "name": "Sung-Jin Park",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2104717169",
            "name": "Seung-Woo Ryu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2183278384",
            "name": "Edward Choi",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2998696444",
        "https://openalex.org/W2986266667",
        "https://openalex.org/W3044438666",
        "https://openalex.org/W3174583470",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3155039781",
        "https://openalex.org/W2948445958",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W3100879603",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2950645060",
        "https://openalex.org/W3198593990",
        "https://openalex.org/W3111372685",
        "https://openalex.org/W2962817854",
        "https://openalex.org/W3201339301",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W3035428952",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W2963921564",
        "https://openalex.org/W3037063616",
        "https://openalex.org/W3102999298",
        "https://openalex.org/W3176456866",
        "https://openalex.org/W3100778284"
    ],
    "abstract": "Recent success of pre-trained language models (PLMs) has stimulated interest in their ability to understand and work with numbers. Yet, the numerical reasoning over measurements has not been formally studied despite their importance. In this study, we show that PLMs lack the capability required for reasoning over measurements. Furthermore, we find that a language model trained on a measurement-rich corpus shows better performance on understanding measurements. We propose a simple embedding strategy to better distinguish between numbers and units, which leads to a significant improvement in the probing tasks.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1782–1792\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nDo Language Models Understand Measurements?\nSungjin Park Seungwoo Ryu Edward Choi\nKAIST\n{zxznm, swryu, edwardchoi}@kaist.ac.kr\nAbstract\nRecent success of pre-trained language models\n(PLMs) has stimulated interest in their ability to\nunderstand and work with numbers. Yet, the nu-\nmerical reasoning over measurements has not\nbeen formally studied despite their importance.\nIn this study, we show that PLMs lack the ca-\npability required for reasoning over measure-\nments. Furthermore, we find that a language\nmodel trained on a measurement-rich corpus\nshows better performance on understanding\nmeasurements. We propose a simple embed-\nding strategy to better distinguish between num-\nbers and units, which leads to a significant im-\nprovement in the probing tasks.\n1 Introduction\nThe success of pre-trained language models\n(PLMs) has led to more research on their ability to\nunderstand commonsense. In this context, numeri-\ncal reasoning over text (NRoT) is a NLP model’s\nability to interpret and work with numbers in ei-\nther digit or word form (Spithourakis and Riedel,\n2018). Recent studies on NRoT test PLMs to an-\nswer questions on numeracy (Wallace et al., 2019),\nscalar magnitude comparison (Zhang et al., 2020),\nnumerical facts (Lin et al., 2020), and math word\nproblems (Wu et al., 2021).\nDespite these efforts, existing works lack an anal-\nysis of the forms in which numbers appear. In par-\nticular, we focus on the case where numbers appear\nas a measurement in the context. In most scien-\ntific articles, measurements are an integral part of\nthe context for capturing its appropriate meaning.\nFor example, the two sentences \"40g of Aspirin\nis lethal\" and \"40mg of Aspirin is lethal\" contain\nthe same words except for the unit of measure-\nment (UoM), but the second sentence is incorrect\nbecause of the UoM.\nIn this work, we examine the measuring skill of\nPLMs: the ability to understand the system of mea-\nsurement and perform numerical reasoning over\nmeasurements. We design three measuring skill\ntests (MSTs) and study how many measuring skills\ncan be acquired. Specifically, UNIT CONVERSION ,\nREFERENCE RANGE DETECTION , and MEASURE -\nMENT COMPARISON require understanding of the\nsystem of measurement, the normal range of the\nbiomedical entity, and the ability to combine knowl-\nedge about the system of measurement and NRoT,\nrespectively. Table 1 shows an example of each of\nthe measuring skill tests.\nMST results showed that the models struggled\nto find the largest (or smallest) value on the list of\nmeasurements and convert the measurement to an-\nother unit, while they performed well on other tests.\nCompared to other PLMs, BioBERT (Lee et al.,\n2020) showed superior performance on UNIT CON -\nVERSION and REFERENCE RANGE DETECTION ,\nwhich implies that pre-training with measurement-\nrich text helps the model understand the system of\nmeasurement. Finally, we speculate that the lack\nof skills to distinguish numbers, units, and other\nwords in the context makes the models fail in some\nMSTs. To mitigate this, we introduce scale embed-\nding, which provides the model with the informa-\ntion regarding the position and scale of the numbers\nin the input text. We show that scale embedding\nsignificantly improves the MST performance of all\nPLMs.\n2 Measuring Skill Test\nIn this section, we describe three MSTs to carefully\nstudy the ability of PLMs to understand the system\nof measurement and perform numerical reasoning\nover the measurements.\n2.1 Unit Conversion\nThis task requires the model to decide whether\nthe two measurements represent the same quantity.\nFor example, the model might correctly predict\n[MASK] in a sentence, such as \"3.5g and 3500mg\nare [MASK] value\" to be filled with same if it under-\n1782\nTask Example Answer Candidates\nCOMPARISON 1.59mg is[MASK]than 3.8g larger, smaller\nARGMIN/MAX [MASK]value among 0.5mg, 3.4g, 2.8mg is 0.5mg largest, smallest, middle\nSORTING sort 0.53mg, 32.54g, 2.8mg in[MASK]order is 0.53mg, 32.54g, 2.8mg increasing, decreasing, random\nUNITCONVERSION 3.5g and 3500mg are[MASK]value same , different\nREFERENCERANGEDETECTION85mg/dL of Glucose is[MASK] normal, abnormal\nTable 1: Examples of measuring skill tests (MSTs). We underline the correct answer for each example.\nTask Template\nCOMPARISON [M]is[MASK]than[M]ARGMIN/MAX [MASK]value among[LoM]is[M]SORTING sort[LoM]in[MASK]order is[LoM]UNITCONVERSION [M]and[M]are[MASK]valueREFERENCERANGEDETECTION[M]of[ENT]is[MASK]\nTable 2: Templates which we used for data generation.\n[M],[LoM], and [ENT] are the placeholder for the mea-\nsurement, the list of measurements, and the biomedical\nentity, respectively.\nstands the conversion of units correctly. In general,\nit is a convention to combine the unit (e.g., liter,\nmeter) and its prefix (e.g., kilo, milli) to represent\nthe numerical value of the measurement within a\nrange [10−3, 103). Therefore, various unit prefixes\ncan appear in a single passage, even if the units\nare the same. To handle this, UNIT CONVERSION\nis essential for complex reasoning over measure-\nments. To succeed in UNIT CONVERSION , we ex-\npect the model to handle the unit and numerical\nvalue jointly, based on an understanding of the sys-\ntem of measurement.\n2.2 Reference Range Detection\nGiven a biomedical entity and measurement, this\ntask requires a model to predict whether the mea-\nsurement falls within the reference range. Knowl-\nedge of the biomedical entity plays a crucial role\nin understanding measurements, since the unit is\ndetermined by the biomedical entity. For example,\nwe measure the hemoglobin level in g/dL. In addi-\ntion to understanding UoMs, PLMs must rely on\ndomain knowledge embedded in their parameters\nto solve this task, as context alone does not provide\nsufficient clues as to what the reference range is for\nthe given biomedical entity.\n2.3 Measurement Comparison\nGiven two measurements (or a series of n mea-\nsurements), the task is to predict the correct re-\nlationship between them. We created the syn-\nthetic dataset following other well-known NRoT\ntasks. Here, we consider three numerical reason-\ning tasks: COMPARISON (Talmor et al., 2020),\nARGMIN/MAX (Wallace et al., 2019), and SORT-\nING (Pal and Baral, 2021), all requiring the model\nto compare numbers. Note that each measurement\nin this task can have a different unit prefix. For\nexample, the sample \"1.59mg is [MASK] than 3.8g\"\ncontaining two different units \"mg\" and \"g\" appears\nin the COMPARISON dataset. This task assesses the\nmodel’s ability to combine an understanding of\nmeasurements and numerical reasoning skills.\n3 Experiments\nProbing Setup We formulated MSTs as a Cloze\ntest (Talmor et al., 2020) to fully utilize the\nknowledge captured by masked language modeling\n(MLM). Specifically, a PLM received the masked\ninputs given in Table 1, and the MLM head output\nthe probability distribution of the answer candi-\ndates for [MASK]. Among the answer candidates,\nwe chose the one with the highest probability as\nthe final prediction.\nWe probed four transformer-based PLMs.\nBERT (Devlin et al., 2019) and ALBERT (Lan\net al., 2020) were trained on Wikipedia articles and\nBook Corpus. BioBERT (Lee et al., 2020) was\ntrained on biomedical articles from PubMed ab-\nstracts, and BlueBERT (Peng et al., 2020) used\nboth clinical (MIMIC-III (Johnson et al., 2016))\nand biomedical (PubMed abstracts) corpus for pre-\ntraining. We also tested a randomly initialized\ntransformer encoder (i.e. Scratch) to evaluate the\ndifficulty of our MSTs. For each model, we did not\nupdate the parameters during training, except for\nthe MLM head in the last transformer layer. In all\ntasks, the models were trained with three random\nseeds and we report the mean classification accu-\nracy for all the probing tasks. Appendix A provides\nfurther details on training and evaluation.\nData Preparation We manually crafted templates\nin Table 2 that contained at most two slots for mea-\nsurements and [MASK] token for an answer. We\ninstantiated [M] and [LoM] by sampling the mea-\nsurement and the list of measurements, respectively.\nFor measurement sampling, we independently sam-\npled a number and a unit and then combined them.\nSpecifically, we sampled units from the predefined\n1783\nTask MEASUREMENT COMPARISON\nUNIT REF\nCOMP ARG SORT\nModel Notation in ex in ex in ex in ex in ex\nALBERT Sci 81.2 77.3 60.4 58.0 78.2 76.5 48.6 49.9 71.9 59.9\nDeci 81.8 72.1 57.1 50.5 82.5 74.3 61.5 56.2 71.1 61.0\nBERT Sci 73.3 72.4 55.1 52.2 45.6 45.0 52.7 51.2 73.5 64.3\nDeci 81.4 77.0 60.9 54.3 54.9 54.5 61.9 59.2 77.2 67.5\nBioBERT Sci 82.7 82.3 55.0 54.4 68.2 69.1 58.7 57.3 81.3 63.7\nDeci 90.1 88.0 59.0 57.6 77.3 73.0 73.0 70.5 87.0 64.2\nBlueBERT Sci 77.3 76.3 46.9 46.9 63.6 64.3 53.0 51.3 73.6 65.4\nDeci 74.6 73.2 57.0 55.5 73.0 68.0 59.2 57.1 77.1 69.0\nScratch Sci 50.9 50.8 40.2 37.1 33.3 33.8 52.5 50.7 66.3 60.8\nDeci 57.7 51.3 44.3 43.0 33.3 33.7 56.8 53.9 62.6 65.0\nTable 3: Test-set results on MSTs. We report the classification accuracy on interpolation (in) and extrapolation (ex)\ntest dataset. COMP , ARG , SORT , UNIT , and REF are abbreviations of COMPARISON , ARGMIN /MAX , SORTING , UNIT\nCONVERSION , and REFERENCE RANGE DETECTION , respectively. Sci and Deci stand for scientific and decimal\nnotations, respectively.\nset in Table 7 which consists of SI units and some\nunits in MIMIC-III.\nThe numbers in the training dataset were sam-\npled from [10−2, 102). For evaluation, we con-\nstructed two evaluation datasets: 1) Interpolation\nsampled numbers from the same range as the train-\ning dataset; 2) Extrapolation sampled numbers\nfrom [10−3, 103). Note that we did not consider\nthe numbers outside the range[10−3, 103), because\nmany of the unit prefixes are in the power of thou-\nsands. Zhang et al. (2020) reported that represent-\ning numbers in scientific notation made it easier\nfor the language model to capture the scale of num-\nbers. Following this observation, we tested two\ndifferent number notations: decimal and scientific.\nFor example, 32.6 can be represented as 32.6 and\n3.26E+01 in decimal and scientific notation, respec-\ntively. We randomly varied the number of digits\nafter the decimal point between zero and three, and\nthe significant digits were maintained after convert-\ning the number notation.\nFor REFERENCE RANGE DETECTION , we col-\nlected biomedical entities from six tables in\nMIMIC-III (INPUT, OUTPUT, LAB, PRESCRIP-\nTION, PROCEDURE, and CHART) and chose the\nsubset.\nWe report the number of samples and the distri-\nbution of labels for each MST in Table 8.\n4 Results and Analysis\nMeasuring Skills of PLMs Table 3 shows the re-\nsults of MSTs stated in Section 2.\nPLMs performed reasonably well on COMPARI -\nSON , SORTING , and REFERENCE RANGE DETEC -\nTION , but struggled considerably on ARGMIN /MAX\nand UNIT CONVERSION tasks. This shows that\nsome measuring skills are difficult to learn from\nan LM objective. Similar to previous NRoT stud-\nies (Wallace et al., 2019; Pal and Baral, 2021),\nPLMs often failed to successfully extrapolate to\nvalues outside the training range. Further, in most\ncases, MST results got worse when we represented\nnumbers in scientific notation.\nWe observed that BioBERT outperformed other\nPLMs in UNIT CONVERSION , REFERENCE RANGE\nDETECTION , and COMPARISON , and showed com-\nparable performance in the rest of the MSTs. Com-\npared to BioBERT, BlueBERT was pre-trained on\na larger volume of biomedical text, but showed\nworse performance. This shows that pre-training on\nmeasurement-rich corpora assists the model in ac-\nquiring measuring skills, but further training on the\nnoisy clinical text could harm it when performing\nreasoning over measurements. We also found that\nALBERT outperformed its competitors in SORT-\nING even though it performed the same or worse on\nother tasks. This may be because ALBERT benefits\nfrom its sentence order prediction (SOP) objective,\nwhich predicts the ordering of two consecutive seg-\nments of text.\nEffect of using Different PromptsOne can expect\nthat the choice of prompt has an impact on the re-\nsults, and recent studies (Jiang et al., 2020; Petroni\net al., 2019) support this. To see whether the results\n1784\nTask MEASUREMENTCOMPARISON UNIT REFCOMP ARG SORT\nPrompt Set Modelin(∆) ex(∆) in(∆) ex(∆) in(∆) ex(∆) in(∆) ex(∆) in(∆) ex(∆)\nLABEL\nALBERT 78.7 (-3.1) 70.8 (-1.3) 40.9 (-16.2) 33.1 (-17.4) 73.6 (-8.9) 67.2 (-7.1) 55.5 (-6.0) 56.0 (-0.2) 51.1 (-20.0) 36.0 (-25.0)BERT 73.1 (-8.3) 70.8 (-6.2) 54.0 (-6.9) 50.7 (-3.6) 54.0 (-0.9) 54.3 (-0.2) 56.6 (-5.3) 55.0 (-4.2) 40.3 (-36.9) 13.7 (-53.8)BioBERT 82.8 (-7.3) 80.2 (-7.8) 56.7 (-2.3) 55.7 (-1.9) 66.4 (-10.9) 62.6 (-10.4) 61.9 (-11.1) 60.4 (-10.1) 69.1 (-17.9) 59.6 (-4.6)BlueBERT 75.0 (0.4) 69.7 (-3.5) 56.9 (-0.1) 55.3 (-0.2) 70.1 (-2.9) 66.6 (-1.4) 56.4 (-2.8) 54.9 (-2.2) 76.4 (-0.7) 70.6 (1.6)\nCONTEXT\nALBERT 67.8 (-14.0) 61.8 (-10.3) 49.1 (-8.0) 43.5 (-7.0) 72.3 (-10.2) 68.2 (-6.1) 50.4 (-11.1) 50.5 (-5.7) 65.8 (-5.3) 56.9 (-4.1)BERT 70.2 (-11.2) 67.9 (-9.1) 52.4 (-8.5) 47.1 (-7.2) 51.8 (-3.1) 50.6 (-3.9) 56.1 (-5.8) 55.2 (-4.0) 66.4 (-10.8) 63.2 (-4.3)BioBERT 80.7 (-9.4) 78.4 (-9.6) 58.1 (-0.9) 55.9 (-1.7) 73.6 (-3.7) 69.7 (-3.3) 60.4 (-12.6) 59.3 (-11.2) 75.2 (-11.8) 64.8 (0.6)BlueBERT 71.5 (-3.1) 65.6 (-7.6) 51.9 (-5.1) 48.5 (-7.0) 60.6 (-12.4) 56.4 (-11.6) 50.7 (-8.5) 50.7 (-6.4) 67.4 (-9.7) 64.9 (-4.1)\nUOM\nALBERT 87.9 (6.1) 80.2 (8.1) 71.0 (13.9) 58.8 (8.3) 90.5 (8.0) 85.5 (11.2) 64.2 (2.7) 56.9 (0.7) N/A N/ABERT 90.0 (8.6) 87.2 (10.2) 69.4 (8.5) 68.2 (13.9) 67.1 (12.2) 63.0 (8.5) 67.2 (5.3) 64.8 (5.6) N/A N/ABioBERT 96.5 (6.4) 94.8 (6.8) 72.1 (13.1) 69.3 (11.7) 84.0 (6.7) 79.6 (6.6) 82.5 (9.5) 77.9 (7.4) N/A N/ABlueBERT 88.3 (13.7) 83.8 (10.6) 66.5 (9.5) 63.8 (8.3) 76.5 (3.5) 71.7 (3.7) 66.6 (7.4) 62.4 (5.3) N/A N/A\nTable 4: Test-set results on different sets of prompts. We report the classification accuracy and the performance\ndifference (∆). We obtain ∆ by subtracting the results in Table 3 from this table.\nTask MEASUREMENTCOMPARISON REFCOMP ARG SORT\nModel in(∆) ex(∆) in(∆) ex(∆) in(∆) ex(∆) in(∆) ex(∆)\nALBERT 87.5 (5.7) 85.3 (13.2) 71.8 (14.7) 68.2 (17.7) 87.3 (4.8) 85.3 (11.0) 65.6 (-5.5) 53.8 (-7.2)\nBERT 88.3 (6.9) 86.8 (9.8) 59.3 (-1.6) 60.6 (6.3) 77.5 (22.6) 77.5 (23.0) 69.6 (-7.6) 64.2 (-3.3)\nBioBERT 94.7 (4.6) 93.5 (5.5) 70.7 (11.7) 68.3 (10.7) 83.6 (6.3) 82.9 (9.9) 77.6 (-9.4) 65.7 (1.5)\nBlueBERT 88.7 (14.1) 86.1 (12.9) 64.8 (7.8) 60.9 (5.4) 75.2 (2.2) 74.2 (6.2) 70.3 (-6.8) 66.0 (-3.0)\nTable 5: Test-set results on rule-based conversion experiments. We report the classification accuracy and the\nperformance difference (∆).\nin Table 3 are maintained as the prompt differs, we\ntrained and evaluated PLMs on three distinct sets\nof prompts: CONTEXT , UOM, and LABEL . Specifi-\ncally, CONTEXT , UOM, and LABEL examine how\nconsistent MST results are against various linguis-\ntic expressions of prompts, the set of unique UoMs\nin the dataset, and the choice of answer candidates,\nrespectively. Note that we considered answer can-\ndidates as part of the prompt, since the prompt\ndetermines the set of correct answers.\nFor CONTEXT , we manually created four addi-\ntional templates that have the same meaning as the\noriginal template in Table 2. For UOM , we used\nonly a subset of units g, l, m, and s, which appear\nfrequently in the general text. For LABEL , we in-\ncluded synonyms of the label as answer candidates.\nFor example, \"less\", \"smaller\", and \"lower\" are the\nanswers for the prompt \"1.59mg is [MASK] than\n3.8mg.\". More details of the experiments are in the\nAppendix B.\nThe results with the decimal notation are shown\nin Table 4. We can see that the results vary with\nthe choice of prompt, indicating that PLMs are\nindeed sensitive to it. However, we found that\nMST performance maintains a similar tendency in\nevery experiment: BioBERT works well on COM -\nPARSION , UNIT CONVERSION , and REFERENCE\nRANGE DETECTION , and ALBERT works well on\nSORTING .\nRule-based Conversion of Measurements Mea-\nsurements exhibit a certain pattern, regardless of\nthe domain, because of a global standard: the In-\nternational System of Units (SI). Thus, we can\nmanually detect and convert all units in the text\nwithout difficulty. Then, it is natural to wonder if\nconverting all units based on rules is easier than\nmaking the language model understand the sys-\ntem of measurement. To answer this question, we\ntested the rule-based conversion that detects mea-\nsurements with the regular expression and converts\nthem into a prefix-free form. For example, the sen-\ntence \"2.5mg is [MASK] than 3.8g\" is converted\nto \"0.0025g is [MASK] than 3.8g\" after the rule-\nbased conversion. We examined the rule-based\nconversion on MEASUREMENT COMPARISON and\nREFERENCE RANGE DETECTION .\nThe results with the decimal notation are shown\nin Table 5. The rule-based conversion increased\nMEASUREMENT COMPARISON performance be-\ncause the convertedMEASUREMENT COMPARISON\ndoes not require an understanding of unit conver-\nsion to solve the problem. However, it can be seen\nthat almost all models became worse on REFER -\nENCE RANGE DETECTION . This shows that the\nknowledge about the reference range is highly cor-\nrelated with the specific UoM. Thus, the rule-based\nconversion is a suboptimal choice if we want to\nutilize the domain knowledge embedded in PLMs.\n1785\nFigure 1: Our scale embedding.\nTask MEASUREMENTCOMPARISON UNIT REFCOMP ARG SORT\nModel in(∆) ex(∆) in(∆) ex(∆) in(∆) ex(∆) in(∆) ex(∆) in(∆) ex(∆)\nALBERT 92.9 (11.1) 78.9 (6.8) 73.6 (16.5) 63.5 (13.0) 92.9 (10.4) 86.8 (12.5) 75.7 (14.2) 68.0 (11.8) 83.5 (12.4) 63.9 (2.9)\nBERT 95.9 (14.5) 89.0 (12.0) 79.8 (18.9) 75.6 (21.3) 91.7 (36.8) 90.9 (36.4) 87.9 (26.0) 80.2 (21.0) 95.3 (18.1) 62.6 (-4.9)\nBioBERT 98.4 (8.3) 93.2 (5.2) 85.9 (26.9) 83.0 (25.4) 94.0 (16.7) 93.0 (20.0) 90.1 (17.1) 85.7 (15.2) 98.4 (11.4) 61.9 (-2.3)\nBlueBERT 97.4 (22.8) 88.1 (14.9) 75.9 (18.9) 67.7 (12.2) 91.8 (18.8) 90.3 (22.3) 80.0 (20.8) 76.2 (19.1) 94.3 (17.2) 66.1 (-2.9)\nScratch 70.2 (12.5) 60.3 (9.0) 45.5 (1.2) 44.1 (1.1) 33.2 (-0.1) 33.8 (0.1) 60.3 (3.5) 56.1 (2.2) 69.0 (6.4) 66.3 (1.3)\nTable 6: Effect of scale embedding on MSTs. We report the classification accuracy and performance improvement\n(∆) after applying scale embedding.\nScale Embedding and its Effect In Section 4, we\nobserved that none of the PLMs showed a perfect\nunderstanding of each MST. We suspect that such\na gap originates in the deficiency of PLM’s ability\nto extract numerical values from measurements\nand compare their magnitudes. To this end, we\npropose scale embedding, an additional embedding\nthat provides the model with the information of the\nposition and scale of numbers in the input text.\nAs described in Figure 1, we incrementally as-\nsigned the index to each token from the end to\nthe beginning of a sentence. If we encounter a\ntoken that is not included in the numerical value,\nthen we reset the index to zero and keep assigning\nthe index zero to tokens until another numerical\nvalue appears. We distinguished between numeri-\ncal and nonnumerical subwords using the regular\nexpression. Note that we trained only the scale\nembedding and MLM head while freezing other\npre-trained weights of the language model. This\nallows us to adapt the model to any numerical rea-\nsoning tasks simply by plugging a different scale\nembedding into them.\nTable 6 shows the MST results after the scale\nembedding is applied to all models, where we can\nsee significantly improved test results, even for\nARGMIN /MAX and UNIT CONVERSION .1 Note\nthat the scale embedding is minimally effective for\nScratch, except for COMPARISON . This shows that\nsolving our MSTs requires more than just simple\nembeddings, and a PLM that understands context\nis an essential element.\n1The full set of experimental results are shown in Table 13\nin the Appendix.\n5 Related Works\nOver the years, numerical reasoning has been an\nactive research area. Some works investigate the\nnumeracy of static word embeddings (Naik et al.,\n2019), contextualized language embeddings (Wal-\nlace et al., 2019), and multilingual words (John-\nson et al., 2020). Wallace et al. (2019) shows\nthat ELMo, BERT, and GloVe embeddings are\ncapable of capturing numeracy, but only within\nthe range of numbers seen during training. Gen-\nBERT (Geva et al., 2020), NumGPT (Jin et al.,\n2021), and NT5 (Yang et al., 2021) focus on incor-\nporating arithmetic skills into pre-trained models.\nAnother task that deals with numerical quantities is\nmeasurement estimation. VerbPhysics (Forbes and\nChoi, 2017) proposes the dataset to compare the\nrelative scales between the physical attributes of\nvarious objects. DoQ (Elazar et al., 2019) provides\nan empirical distribution over possible values of\nquantitative attributes. Zhang et al. (2020) tests that\nNLP models contain information about the scalar\nmagnitudes of physical objects. Although previous\nstudies probed numerical reasoning over numeral\nand physical attributes, no attempt has been made\nto investigate reasoning over measurements.\n6 Conclusion\nTo the best of our knowledge, our study is the first\nto investigate reasoning over measurements. Our\nanalysis shows that PLMs lack the capability re-\nquired for reasoning over measurements. We pro-\nposed a scale embedding approach that provides in-\nformation on the position and scale of numbers, and\nit significantly increases the MST performance.\n1786\nAcknowledgements\nResearch supported with Cloud TPUs from\nGoogle’s TPU Research Cloud (TRC). This work\nwas supported by Institute of Information &\nCommunications Technology Planning & Evalu-\nation (IITP) grant (No.2019-0-00075), the Korea\nHealth Industry Development Institute (KHIDI)\ngrant (No.HI21C1138), and the Korea Medi-\ncal Device Development Fund grant (Project\nNo.: 1711138160, KMDF_PR_20200901_0097),\nfunded by the Korea government (MSIT, MOTIE,\nMOHW, MFDS).\nLimitations\nOur scale embedding can make mistakes when\nthe unit itself contains numbers (e.g. mg/100ml).\nTherefore, scale embedding should not be applied\nto UoM containing numbers through exception han-\ndling.\nOur work will be largely affected by the created\nprompts. If the prompt is not obvious for PLMs to\nunderstand, although they have such reasoning abil-\nity, they may not give the correct answer. To miti-\ngate this problem, we conducted experiments with\ndifferent sets of prompts in Section 4 and showed\nthat the results maintain their tendency across the\nprompts. Despite these efforts, it is still unclear\nwhat the optimal choice of the prompt is. We re-\nmain this problem as a future work.\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nYanai Elazar, Abhijit Mahabal, Deepak Ramachandran,\nTania Bedrax-Weiss, and Dan Roth. 2019. How large\nare lions? inducing distributions over quantitative\nattributes. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3973–3983, Florence, Italy. Association for\nComputational Linguistics.\nMaxwell Forbes and Yejin Choi. 2017. Verb physics:\nRelative physical knowledge of actions and objects.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 266–276, Vancouver, Canada.\nAssociation for Computational Linguistics.\nMor Geva, Ankit Gupta, and Jonathan Berant. 2020.\nInjecting numerical reasoning skills into language\nmodels. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 946–958, Online. Association for Computa-\ntional Linguistics.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How Can We Know What Language\nModels Know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nZhihua Jin, Xin Jiang, Xingbo Wang, Qun Liu, Yong\nWang, Xiaozhe Ren, and Huamin Qu. 2021. Numgpt:\nImproving numeracy ability of generative pre-trained\nmodels. arXiv preprint arXiv:2109.03137.\nAlistair E W Johnson, Tom J Pollard, Lu Shen, Li-\nWei H Lehman, Mengling Feng, Mohammad Ghas-\nsemi, Benjamin Moody, Peter Szolovits, Leo An-\nthony Celi, and Roger G Mark. 2016. Mimic-iii,\na freely accessible critical care database. Scientific\ndata, 3(1):1–9.\nDevin Johnson, Denise Mak, Andrew Barker, and Lexi\nLoessberg-Zahl. 2020. Probing for multilingual nu-\nmerical understanding in transformer-based language\nmodels. In Proceedings of the Third BlackboxNLP\nWorkshop on Analyzing and Interpreting Neural Net-\nworks for NLP, pages 184–192, Online. Association\nfor Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learning\nof language representations. In International Confer-\nence on Learning Representations.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon\nKim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.\n2020. Biobert: a pre-trained biomedical language\nrepresentation model for biomedical text mining.\nBioinformatics, 36(4):1234–1240.\nBill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xiang\nRen. 2020. Birds have four legs?! NumerSense:\nProbing Numerical Commonsense Knowledge of Pre-\nTrained Language Models. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6862–6868,\nOnline. Association for Computational Linguistics.\nAakanksha Naik, Abhilasha Ravichander, Carolyn Rose,\nand Eduard Hovy. 2019. Exploring numeracy in\nword embeddings. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 3374–3380, Florence, Italy. Asso-\nciation for Computational Linguistics.\nKuntal Kumar Pal and Chitta Baral. 2021. Investigating\nnumeracy learning ability of a text-to-text transfer\nmodel. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021, pages 3095–3101,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\n1787\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learning\nlibrary. In H. Wallach, H. Larochelle, A. Beygelz-\nimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems\n32, pages 8024–8035. Curran Associates, Inc.\nYifan Peng, Qingyu Chen, and Zhiyong Lu. 2020. An\nempirical study of multi-task learning on BERT for\nbiomedical text mining. In Proceedings of the 19th\nSIGBioMed Workshop on Biomedical Language Pro-\ncessing, pages 205–214, Online. Association for\nComputational Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nGeorgios Spithourakis and Sebastian Riedel. 2018. Nu-\nmeracy for language models: Evaluating and improv-\ning their ability to predict numbers. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 2104–2115, Melbourne, Australia. Association\nfor Computational Linguistics.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020. oLMpics-on what language\nmodel pre-training captures. Transactions of the As-\nsociation for Computational Linguistics, 8:743–758.\nEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh,\nand Matt Gardner. 2019. Do NLP models know num-\nbers? probing numeracy in embeddings. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 5307–5315, Hong\nKong, China. Association for Computational Linguis-\ntics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nQinzhuo Wu, Qi Zhang, Zhongyu Wei, and Xuanjing\nHuang. 2021. Math word problem solving with ex-\nplicit numerical values. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 5859–5869, Online. Association\nfor Computational Linguistics.\nPeng Jian Yang, Ying Ting Chen, Yuechan Chen, and\nDaniel Cer. 2021. Nt5?! training T5 to perform nu-\nmerical reasoning. arXiv preprint arXiv:2104.07307.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text genera-\ntion. In Advances in Neural Information Processing\nSystems, volume 34, pages 27263–27277. Curran As-\nsociates, Inc.\nXikun Zhang, Deepak Ramachandran, Ian Tenney,\nYanai Elazar, and Dan Roth. 2020. Do language\nembeddings capture scales? In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2020,\npages 4889–4896, Online. Association for Computa-\ntional Linguistics.\n1788\nUoM\nw/o Prefix w/ Prefix\nm m, cm, mm, µm, nm\nA A, mA, µA, nA\nK K, mK, µK\nM M, mM, µM, nM\nEq/l Eq/l, mEq/l, µEq/l, mEq/ml, mEq/µl\ng/l g/l, mg/l, µg/l, mg/dl, g/dl, µg/dl, ng/dl, g/ml, mg/ml\nIU/l IU/l, IU/ml, mIU/ml, µIU/ml, mIU/l, µIU/l, IU/µl, mIU/µl\nU/l U/l, U/ml, U/µl\nl/min l/min, dl/min, ml/min, µl/min\n#/l #/dl, #/ml, #/µl\nk/l k/dl, k/ml, k/µl\nl l, dl, ml, µl, nl, pl, fl\ng g, mg, µg, ng, pg, fg\ns s, ms, µs, ns\nm/hr m/hr, cm/hr, mm/hr, µm/hr\nl/hr l/hr, dl/hr, ml/hr, µl/hr\nTable 7: List of units used for data generation.\nA Further Details of Measuring Skill Test\nA.1 Data Statistics\nTable 8 shows the statistics of MSTs we used for\nexperiments.\nA.2 Training and Evaluation\nThe BERT configuration of all models is the same\nas the base model (L=12, H=768, A=12, Total Pa-\nrameters=110M) in (Devlin et al., 2019). Maxi-\nmum sequence length is 512.\nWe trained the model with batch size 256 for 30\nepochs. We used the Adam optimizer for training.\nThe learning rate started from 5e-5 and linearly de-\ncayed towards 1e-8. We early stopped the training\nwhen the validation accuracy did not increase for 2\nepochs. The batch size for evaluation is 128, and\nother settings are the same as training. We found\nthe optimal hyperparameters using the grid search,\nwhere we evaluated the learning rate [1e-5, 2e-5,\n5e-5, 1e-4], batch size [16,32,64,128].\nB More Details of Prompt Sets\nThe results with both decimal and scientific nota-\ntion are shown in Table 9.\nB.1 LABEL\nInspired by Yuan et al. (2021), we included syn-\nonyms as an answer to make the prompt diverse.\nWe used the website https://www.wordhippo.\ncom/ to search for synonyms. Among the search\nresults, we chose two words that match the context.\nWe report the list of synonyms in Table 10.\nB.2 CONTEXT\nIf the context differs from what PLM saw dur-\ning pre-training, then PLMs will struggle to solve\nMSTs even if they understand the measuring\nskills. To mitigate this, we prepared four addi-\ntional prompts with the same meaning. Additional\nprompts are listed in Table 11.\nB.3 UOM\nIn the general domain, some UoMs listed in Ta-\nble 7 rarely appear in the context. For example,\ninternational units per liter (IU/l) is frequently used\nin pharmacology, but not in other scientific articles.\nTherefore, we can wonder if some rare biomedical\nunits disrupt the understanding of general domain\nPLMs (e.g., BERT and ALBERT). To answer this\nquestion, we replaced all UoMs in the dataset with\nthe commonly used UoMs: g, l, m, and s.\nC Additional Results on Rule-based\nConversion\nTable 12 describes the complete set of MST results\nafter applying rule-based conversion.\nD Additional Results on Scale Embedding\nTable 13 describes the MST results of scale embed-\nding with decimal and scientific notation.\nE Experimental Environment\nWe trained the models with Google TPU v2-8 and\nv3-8. We used PyTorch 1.10.0 (Paszke et al., 2019)\nand Huggingface Transformers (Wolf et al., 2020)\n4.3.3 for experiments.\n1789\nTask Split Number Range# SamplesLabel Distribution\nCOMPARISON\ntrain interpolation299,394 smaller: 0.5, larger: 0.5\nvalid interpolation29,986 smaller: 0.498, larger: 0.502\nextrapolation30,000 smaller: 0.495, larger: 0.505\ntest interpolation29,988 smaller: 0.501, larger: 0.499\nextrapolation30,000 smaller: 0.501, larger: 0.499\nARGMIN/MAX\ntrain interpolation300,000 smallest:0.333, middle: 0.334, largest: 0.333\nvalid interpolation30,000 smallest:0.333, middle: 0.334, largest: 0.333\nextrapolation30,000 smallest:0.333, middle: 0.334, largest: 0.333\ntest interpolation30,000 smallest:0.332, middle: 0.335, largest: 0.333\nextrapolation30,000 smallest:0.335, middle: 0.332, largest: 0.333\nSORTING\ntrain interpolation300,000 decreasing: 0.333, random: 0.332, increasing: 0.335\nvalid extrapolation30,000 decreasing: 0.333, random: 0.332, increasing: 0.335\nextrapolation30,000 decreasing: 0.337, random: 0.332, increasing: 0.331\ntest interpolation30,000 decreasing: 0.337, random: 0.332, increasing: 0.331\nextrapolation30,000 decreasing: 0.328, random: 0.339, increasing: 0.333\nUNIT CONVERSION\ntrain interpolation259,588 same: 0.489, different: 0.511\nvalid interpolation23,931 same: 0.489, different: 0.511\nextrapolation28,814 same: 0.5, different: 0.5\ntest interpolation23,538 same: 0.483, different: 0.517\nextrapolation28,696 same: 0.498, different: 0.502\nREFERENCE RANGE DETECTION\ntrain interpolation201,061 normal: 0.575, abnormal: 0.425\nvalid interpolation17,111 normal: 0.593, abnormal: 0.407\nextrapolation21,212 normal: 0.618, abnormal: 0.382\ntest interpolation16,948 normal: 0.586, abnormal: 0.414\nextrapolation18,429 normal: 0.659, abnormal: 0.341\nTable 8: Statistics of MSTs used for experiments.\nTask MEASUREMENTCOMPARISON UNIT REFCOMP ARG SORT\nPrompt Set Model Notationin(∆) ex(∆) in(∆) ex(∆) in(∆) ex(∆) in(∆) ex(∆) in(∆) ex(∆)\nLABEL\nALBERTSci 77.1 (-4.1) 73.8 (-3.5) 40.5 (-19.9) 34.2 (-23.8) 71.3 (-6.9) 65.6 (-10.9) 54.5 (5.9) 53.0 (3.1) 51.4 (-20.5) 34.6 (-25.3)Deci 78.7 (-3.1) 70.8 (-1.3) 40.9 (-16.2) 33.1 (-17.4) 73.6 (-8.9) 67.2 (-7.1) 55.5 (-6.0) 56.0 (-0.2) 51.1 (-20.0) 36.0 (-25.0)\nBERT Sci 68.9 (-4.4) 68.7 (-3.7) 42.9 (-12.2) 42.9 (-9.3) 47.3 (1.7) 46.4 (1.4) 53.4 (0.7) 52.2 (1.0) 47.4 (-26.1) 23.0 (-41.3)Deci 73.1 (-8.3) 70.8 (-6.2) 54.0 (-6.9) 50.7 (-3.6) 54.0 (-0.9) 54.3 (-0.2) 56.6 (-5.3) 55.0 (-4.2) 40.3 (-36.9) 13.7 (-53.8)\nBioBERTSci 74.0 (-8.7) 73.3 (-9.0) 50.9 (-4.1) 50.8 (-3.6) 61.8 (-6.4) 61.7 (-7.4) 55.8 (-2.9) 54.1 (-3.2) 59.3 (-22.0) 54.3 (-9.4)Deci 82.8 (-7.3) 80.2 (-7.8) 56.7 (-2.3) 55.7 (-1.9) 66.4 (-10.9) 62.6 (-10.4) 61.9 (-11.1) 60.4 (-10.1) 69.1 (-17.9) 59.6 (-4.6)\nBlueBERTSci 77.0 (-0.3) 76.3 (0.0) 44.7 (-2.2) 44.3 (-2.6) 64.6 (1.0) 65.1 (0.8) 52.9 (-0.1) 51.3 (0.0) 70.0 (-3.6) 63.8 (-1.6)Deci 75.0 (0.4) 69.7 (-3.5) 56.9 (-0.1) 55.3 (-0.2) 70.1 (-2.9) 66.6 (-1.4) 56.4 (-2.8) 54.9 (-2.2) 76.4 (-0.7) 70.6 (1.6)\nCONTEXT\nALBERTSci 68.4 (-12.8) 65.4 (-11.9) 49.3 (-11.1) 48.2 (-9.8) 71.4 (-6.8) 69.2 (-7.3) 49.7 (1.1) 50.0 (0.1) 65.7 (-6.2) 57.2 (-2.7)Deci 67.8 (-14.0) 61.8 (-10.3) 49.1 (-8.0) 43.5 (-7.0) 72.3 (-10.2) 68.2 (-6.1) 50.4 (-11.1) 50.5 (-5.7) 65.8 (-5.3) 56.9 (-4.1)\nBERT Sci 65.4 (-7.9) 64.5 (-7.9) 46.8 (-8.3) 44.6 (-7.6) 44.6 (-1.0) 43.9 (-1.1) 52.7 (0.0) 52.5 (1.3) 65.4 (-8.1) 58.9 (-5.4)Deci 70.2 (-11.2) 67.9 (-9.1) 52.4 (-8.5) 47.1 (-7.2) 51.8 (-3.1) 50.6 (-3.9) 56.1 (-5.8) 55.2 (-4.0) 66.4 (-10.8) 63.2 (-4.3)\nBioBERTSci 74.6 (-8.1) 74.2 (-8.1) 53.3 (-1.7) 52.9 (-1.5) 68.4 (0.2) 67.9 (-1.2) 55.0 (-3.7) 54.7 (-2.6) 70.8 (-10.5) 61.2 (-2.5)Deci 80.7 (-9.4) 78.4 (-9.6) 58.1 (-0.9) 55.9 (-1.7) 73.6 (-3.7) 69.7 (-3.3) 60.4 (-12.6) 59.3 (-11.2) 75.2 (-11.8) 64.8 (0.6)\nBlueBERTSci 68.4 (-8.9) 67.1 (-9.2) 40.4 (-6.5) 40.6 (-6.3) 56.8 (-6.8) 56.5 (-7.8) 50.3 (-2.7) 50.2 (-1.1) 66.5 (-7.1) 60.3 (-5.1)Deci 71.5 (-3.1) 65.6 (-7.6) 51.9 (-5.1) 48.5 (-7.0) 60.6 (-12.4) 56.4 (-11.6) 50.7 (-8.5) 50.7 (-6.4) 67.4 (-9.7) 64.9 (-4.1)\nUOM\nALBERTSci 92.8 (11.6) 87.2 (9.9) 73.0 (12.6) 68.7 (10.7) 89.0 (10.8) 84.6 (8.1) 60.5 (11.9) 56.1 (6.2) N/A N/ADeci 87.9 (6.1) 80.2 (8.1) 71.0 (13.9) 58.8 (8.3) 90.5 (8.0) 85.5 (11.2) 64.2 (2.7) 56.9 (0.7) N/A N/A\nBERT Sci 81.5 (8.2) 78.9 (6.5) 60.6 (5.5) 61.8 (9.6) 50.7 (5.1) 49.0 (4.0) 56.5 (3.8) 53.2 (2.0) N/A N/ADeci 90.0 (8.6) 87.2 (10.2) 69.4 (8.5) 68.2 (13.9) 67.1 (12.2) 63.0 (8.5) 67.2 (5.3) 64.8 (5.6) N/A N/A\nBioBERTSci 91.3 (8.6) 90.3 (8.0) 53.6 (-1.4) 52.2 (-2.2) 72.7 (4.5) 70.5 (1.4) 66.1 (7.4) 63.2 (5.9) N/A N/ADeci 96.5 (6.4) 94.8 (6.8) 72.1 (13.1) 69.3 (11.7) 84.0 (6.7) 79.6 (6.6) 82.5 (9.5) 77.9 (7.4) N/A N/A\nBlueBERTSci 85.8 (8.5) 83.0 (6.7) 57.9 (11.0) 57.3 (10.4) 68.6 (5.0) 69.4 (5.1) 57.2 (4.2) 53.5 (2.2) N/A N/ADeci 88.3 (13.7) 83.8 (10.6) 66.5 (9.5) 63.8 (8.3) 76.5 (3.5) 71.7 (3.7) 66.6 (7.4) 62.4 (5.3) N/A N/A\nTable 9: Test-set results on different sets of prompts. We report the classification accuracy and the performance\ndifference (∆). We obtain ∆ by subtracting the results in Table 3 from this table.\n1790\nTask Answer Candidates Synonyms\nCOMPARISON\nlarger higher, bigger\nsmaller lower, less\nARGMIN /MAX\nlargest biggest, maximum\nmiddle medium, intermediate\nsmallest lowest, minimum\nSORTING\nincreasing growing, ascending\nrandom unclear, confusing\ndecreasing reducing, descending\nUNIT CONVERSION\nsame equal, identical\ndifferent distinct, unlike\nREFERENCE RANGE DETECTION\nnormal regular, safe\nabnormal irregular, lethal\nTable 10: Synonyms of the answer candidates we used for LABEL .\nTask Template\nCOMPARISON\n[M] is [MASK] than [M]\ncompared to[M], [M] is [MASK] value\nthe measurement of control group ([M]) is [MASK] than [M]\ncomparison:[M], [M], result: [MASK]\n[M] [MASK] [M]\nARGMIN /MAX\nThe [MASK] value among[LoM] is [M]\n[M] is the [MASK] value of[LoM]\nAmong the list of measurements[LoM], the [MASK] value is[M]\nargmin,argmax:[LoM], [M], result: [MASK]\n[MASK] [LoM] , [M]\nSORTING\nsort [LoM] in [MASK] order is[LoM]\narranging[LoM] in [MASK] order is[LoM]\n[LoM] is obtained by sorting[LoM] in [MASK] order\nsort: [LoM], [LoM], result: [MASK]\n[LoM] [MASK] [LoM]\nUNIT CONVERSION\n[M] and [M] are the[MASK] value\nconvert[M] to [MASK] value, then the result is[M]\ncompare[M] to [M], the two are the[MASK] value\nmeasurement comparison:[M], [M], result: [MASK]\n[M] , [M] [MASK]\nREFERENCE RANGE DETECTION\n[M] of [ENT] is [MASK]\n[M] of [ENT] falls into[MASK] range\nThe physician decides[M] of [ENT] as [MASK]\nreference range:[ENT], [M], result: [MASK]\n[ENT] [M] [MASK]\nTable 11: Templates for CONTEXT . [M] is the measurement and [LoM] is the list of measurements.\n1791\nTask MEASUREMENTCOMPARISON REFCOMP ARG SORT\nModel Notation in(∆) ex(∆) in(∆) ex(∆) in(∆) ex(∆) in(∆) ex(∆)\nALBERT Sci 73.8 (-7.4) 73.0 (-4.3) 54.7 (-5.7) 51.0 (-7.0) 80.4 (2.2) 77.7 (1.2) 71.8 (-0.1) 62.3 (2.4)\nDeci 87.5 (5.7) 85.3 (13.2) 71.8 (14.7) 68.2 (17.7) 87.3 (4.8) 85.3 (11.0) 65.6 (-5.5) 53.8 (-7.2)\nBERT Sci 69.2 (-4.1) 68.6 (-3.8) 53.7 (-1.4) 52.4 (0.2) 53.4 (7.8) 53.4 (8.4) 73.2 (-0.3) 63.5 (-0.8)\nDeci 88.3 (6.9) 86.8 (9.8) 59.3 (-1.6) 60.6 (6.3) 77.5 (22.6) 77.5 (23.0) 69.6 (-7.6) 64.2 (-3.3)\nBioBERT Sci 80.6 (-2.1) 80.1 (-2.2) 50.4 (-4.6) 47.0 (-7.4) 69.0 (0.8) 68.1 (-1.0) 79.0 (-2.3) 64.3 (0.6)\nDeci 94.7 (4.6) 93.5 (5.5) 70.7 (11.7) 68.3 (10.7) 83.6 (6.3) 82.9 (9.9) 77.6 (-9.4) 65.7 (1.5)\nBlueBERT Sci 68.3 (-9.0) 65.8 (-10.5) 40.2 (-6.7) 40.1 (-6.8) 66.9 (3.3) 67.0 (2.7) 74.0 (0.4) 65.0 (-0.4)\nDeci 88.7 (14.1) 86.1 (12.9) 64.8 (7.8) 60.9 (5.4) 75.2 (2.2) 74.2 (6.2) 70.3 (-6.8) 66.0 (-3.0)\nScratch Sci 58.2 (7.3) 54.6 (3.8) 43.0 (2.8) 41.0 (3.9) 33.3 (0.0) 33.7 (-0.1) 64.7 (-1.6) 62.8 (2.0)\nDeci 78.8 (21.1) 73.9 (22.6) 43.5 (-0.8) 42.9 (-0.1) 33.2 (-0.1) 33.9 (0.2) 63.6 (1.0) 64.3 (-0.7)\nTable 12: Test-set results on rule-based conversion experiments. We report the classification accuracy and the\nperformance difference.\nTask MEASUREMENTCOMPARISON UNIT REFCOMP ARG SORT\nModel Notation in(∆) ex(∆) in(∆) ex(∆) in(∆) ex(∆) in(∆) ex(∆) in(∆) ex(∆)\nALBERT Sci 93.4 (12.2) 86.3 (9.0) 73.2 (12.8) 66.0 (8.0) 92.7 (14.5) 90.1 (13.6) 74.8 (26.2) 61.6 (11.7) 87.0 (15.1) 63.4 (3.5)Deci 92.9 (11.1) 78.9 (6.8) 73.6 (16.5) 63.5 (13.0) 92.9 (10.4) 86.8 (12.5) 75.7 (14.2) 68.0 (11.8) 83.5 (12.4) 63.9 (2.9)\nBERT Sci 96.4 (23.1) 95.0 (22.6) 80.9 (25.8) 80.5 (28.3) 89.8 (44.2) 89.5 (44.5) 79.9 (27.2) 67.5 (16.3) 92.4 (18.9) 61.9 (-2.4)Deci 95.9 (14.5) 89.0 (12.0) 79.8 (18.9) 75.6 (21.3) 91.7 (36.8) 90.9 (36.4) 87.9 (26.0) 80.2 (21.0) 95.3 (18.1) 62.6 (-4.9)\nBioBERTSci 98.3 (15.6) 96.3 (14.0) 81.3 (26.3) 80.7 (26.3) 94.0 (25.8) 93.6 (24.5) 89.3 (30.6) 66.7 (9.4) 96.0 (14.7) 64.7 (1.0)Deci 98.4 (8.3) 93.2 (5.2) 85.9 (26.9) 83.0 (25.4) 94.0 (16.7) 93.0 (20.0) 90.1 (17.1) 85.7 (15.2) 98.4 (11.4) 61.9 (-2.3)\nBlueBERTSci 96.0 (18.7) 93.1 (16.8) 76.0 (29.1) 74.9 (28.0) 86.2 (22.6) 85.8 (21.5) 77.4 (24.4) 63.3 (12.0) 91.1 (17.5) 66.7 (1.3)Deci 97.4 (22.8) 88.1 (14.9) 75.9 (18.9) 67.7 (12.2) 91.8 (18.8) 90.3 (22.3) 80.0 (20.8) 76.2 (19.1) 94.3 (17.2) 66.1 (-2.9)\nScratch Sci 59.5 (8.6) 57.4 (6.6) 41.4 (1.2) 39.9 (2.8) 33.4 (0.1) 33.8 (0.0) 52.5 (0.0) 50.6 (-0.1) 80.0 (13.7) 61.8 (1.0)Deci 70.2 (12.5) 60.3 (9.0) 45.5 (1.2) 44.1 (1.1) 33.2 (-0.1) 33.8 (0.1) 60.3 (3.5) 56.1 (2.2) 69.0 (6.4) 66.3 (1.3)\nTable 13: Effect of scale embedding on MSTs. We report the classification accuracy and performance improvement\n(∆) after applying scale embedding.\n1792"
}