{
  "title": "DP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass",
  "url": "https://openalex.org/W4386755323",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2501483526",
      "name": "Minxin Du",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2235717150",
      "name": "Xiang Yue",
      "affiliations": [
        "The Ohio State University"
      ]
    },
    {
      "id": "https://openalex.org/A2309300153",
      "name": "Sherman S. M. Chow",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2237530992",
      "name": "Tianhao Wang",
      "affiliations": [
        "University of Virginia"
      ]
    },
    {
      "id": "https://openalex.org/A2101505231",
      "name": "Chenyu Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105996224",
      "name": "Huan Sun",
      "affiliations": [
        "The Ohio State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2473418344",
    "https://openalex.org/W3188505388",
    "https://openalex.org/W2767079719",
    "https://openalex.org/W4290960278",
    "https://openalex.org/W4367047191",
    "https://openalex.org/W6657138077",
    "https://openalex.org/W2888161220",
    "https://openalex.org/W2950321888",
    "https://openalex.org/W2998378988",
    "https://openalex.org/W3003815046",
    "https://openalex.org/W2897830718",
    "https://openalex.org/W6759579507",
    "https://openalex.org/W3139053233",
    "https://openalex.org/W6779331008",
    "https://openalex.org/W1560153690",
    "https://openalex.org/W2963952467",
    "https://openalex.org/W1446333884",
    "https://openalex.org/W2251655261",
    "https://openalex.org/W4285143763",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W2594311007",
    "https://openalex.org/W3046764764",
    "https://openalex.org/W3193647133",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2535690855",
    "https://openalex.org/W3096738375",
    "https://openalex.org/W6638832512",
    "https://openalex.org/W2963515066",
    "https://openalex.org/W3182470338",
    "https://openalex.org/W2795435272",
    "https://openalex.org/W4200633373",
    "https://openalex.org/W4287122891",
    "https://openalex.org/W2887818006",
    "https://openalex.org/W2950943617",
    "https://openalex.org/W4221146452",
    "https://openalex.org/W3212471751",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W3165327186",
    "https://openalex.org/W2781521040",
    "https://openalex.org/W3207429447",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W1976420017",
    "https://openalex.org/W2784621220",
    "https://openalex.org/W2401502000",
    "https://openalex.org/W3027379683",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2946930197",
    "https://openalex.org/W2013823004",
    "https://openalex.org/W3170764772",
    "https://openalex.org/W4297174827",
    "https://openalex.org/W3158160082",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3013068160",
    "https://openalex.org/W3206066344",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W3011594683",
    "https://openalex.org/W2112380340",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2963796896",
    "https://openalex.org/W4205228770",
    "https://openalex.org/W3134922363",
    "https://openalex.org/W3046518446",
    "https://openalex.org/W4287124863",
    "https://openalex.org/W4207079044",
    "https://openalex.org/W4385679725",
    "https://openalex.org/W4287123801",
    "https://openalex.org/W3098049952",
    "https://openalex.org/W4285100324",
    "https://openalex.org/W2798768357",
    "https://openalex.org/W3173528555",
    "https://openalex.org/W4302011600",
    "https://openalex.org/W1873763122",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963965291",
    "https://openalex.org/W4286961857",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4225700955",
    "https://openalex.org/W3033357972",
    "https://openalex.org/W2962796461",
    "https://openalex.org/W1856966722",
    "https://openalex.org/W2250539671"
  ],
  "abstract": "Differentially private stochastic gradient descent (DP-SGD) adds noise to gradients in back-propagation, safeguarding training data from privacy leakage, particularly membership inference. It fails to cover (inference-time) threats like embedding inversion and sensitive attribute inference. It is also costly in storage and computation when used to fine-tune large pre-trained language models (LMs). We propose DP-Forward, which directly perturbs embedding matrices in the forward pass of LMs. It satisfies stringent local DP requirements for training and inference data. To instantiate it using the smallest matrix-valued noise, we devise an analytic matrix Gaussian~mechanism (aMGM) by drawing possibly non-i.i.d. noise from a matrix Gaussian distribution. We then investigate perturbing outputs from different hidden (sub-)layers of LMs with aMGM noises. Its utility on three typical tasks almost hits the non-private baseline and outperforms DP-SGD by up to 7.7pp at a moderate privacy level. It saves 3$\\times$ time and memory costs compared to DP-SGD with the latest high-speed library. It also reduces the average success rates of embedding inversion and sensitive attribute inference by up to 88pp and 41pp, respectively, whereas DP-SGD fails.",
  "full_text": "DP-Forward: Fine-tuning and Inference on Language Models with\nDifferential Privacy in Forward Pass\nMinxin Du\nThe Chinese University of Hong Kong\ndm018@ie.cuhk.edu.hk\nXiang Yueâˆ—\nThe Ohio State University\nyue.149@osu.edu\nSherman S. M. Chowâ€ \nThe Chinese University of Hong Kong\nsmchow@ie.cuhk.edu.hk\nTianhao Wang\nUniversity of Virginia\ntianhao@virginia.edu\nChenyu Huang\nIndependent\nhcyray@gmail.com\nHuan Sun\nThe Ohio State University\nsun.397@osu.edu\nABSTRACT\nDifferentially private stochastic gradient descent (DP-SGD) adds\nnoise to gradients in back-propagation, safeguarding training data\nfrom privacy leakage, particularly membership inference. It fails to\ncover (inference-time) threats like embedding inversion and sensi-\ntive attribute inference. It is also costly in storage and computation\nwhen used to fine-tune large pre-trained language models (LMs).\nWe propose DP-Forward, which directly perturbs embedding\nmatrices in the forward pass of LMs. It satisfies stringent local DP\nrequirements for training and inference data. To instantiate it us-\ning the smallest matrix-valued noise, we devise an analytic matrix\nGaussian mechanism (aMGM) by drawing possibly non-i.i.d. noise\nfrom a matrix Gaussian distribution. We then investigate perturb-\ning outputs from different hidden (sub-)layers of LMs with aMGM\nnoises. Its utility on three typical tasks almost hits the non-private\nbaseline and outperforms DP-SGD by up to7.7pp at a moderate pri-\nvacy level. It saves3Ã—time and memory costs compared to DP-SGD\nwith the latest high-speed library. It also reduces the average suc-\ncess rates of embedding inversion and sensitive attribute inference\nby up to 88pp and 41pp, respectively, whereas DP-SGD fails.\nCCS CONCEPTS\nâ€¢ Security and privacy â†’Data anonymization and sanitiza-\ntion; Privacy-preserving protocols; Privacy protections.\nKEYWORDS\nLocal Differential Privacy, Natural Language Processing, Pre-trained\nLanguage Models, Privacy-preserving Fine-tuning and Inference of\nLMs, Embedding Matrices, Analytic Matrix Gaussian Mechanism\nACM Reference Format:\nMinxin Du, Xiang Yue, Sherman S. M. Chow, Tianhao Wang, Chenyu Huang,\nand Huan Sun. 2023. DP-Forward: Fine-tuning and Inference on Language\nâˆ—The first two authors contributed equally to this work.\nâ€ Corresponding author is from Dept. of Information Engineering, CUHK, Hong Kong.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nCCS â€™23, November 26â€“30, 2023, Copenhagen, Denmark.\nÂ© 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0050-7/23/11. . . $15.00\nhttps://doi.org/10.1145/3576915.3616592\nModels with Differential Privacy in Forward Pass. In Proceedings of the 2023\nACM SIGSAC Conference on Computer and Communications Security (CCS\nâ€™23), November 26â€“30, 2023, Copenhagen, Denmark. ACM, New York, NY,\nUSA, 18 pages. https://doi.org/10.1145/3576915.3616592\n1 INTRODUCTION\nThe deep learning architecture of transformer [ 68] is now gain-\ning popularity in computer vision and has been widely utilized in\nnatural language processing (NLP). Transformer-based language\nmodels (LMs), such as BERT [20] and GPT [59, 60], have remarkably\nachieved state-of-the-art performance in almost every NLP task.\nThey are first pre-trained on massive (public) self-labeled corpora\nand then fine-tuned for various tasks using much smaller, poten-\ntially private corpora. It avoids training from scratch and the possi-\nble shortage of task-specific corpora while earning versatility.\nTraining data contributing to the improved utility of fine-tuned\nLMs can be sensitive. LMs can (unintentionally) memorize them [12]\nand become vulnerable to membership inference attacks (MIAs) [63]\nthat identify whether an example is in the training set. Worse still,\nverbatim training text (e.g., SSNs) can be extracted via only black-\nbox access to GPT-2 [ 13]. It is also possible to recover personal\nhealth information (e.g., patient-condition pairs) from BERT trained\nover a clinical corpus [42] based on the extraction attack [13].\nDifferential privacy (DP) [22] has emerged as thede facto privacy\nstandard for protecting individual privacy. To thwart MIAs on indi-\nvidualsâ€™ training data, DP stochastic gradient descent (DP-SGD) [1]\ncan be used. It clips the gradients of each example in a batch and\nadds random Gaussian noise to the aggregated gradient. It is more\ngeneral than earlier attempts [17, 18] that focus on convex prob-\nlems and has been implemented in modern ML frameworks, such as\nPyTorch and TensorFlow. One can apply it to fine-tune LM-based\nNLP pipelines while ensuring example-level privacy, assuming each\nindividual contributes an example, typically a sequence-label pair.\nUnfortunately, DP-SGD often uses atrusted party to curate usersâ€™\nsensitive training data. Although it can be done distributively [9, 50]\nvia secure aggregation [15] with extra costs and trust assumptions,\nit offers central DP (CDP) at its core. 1 Instantiating per-example\ngradients as large as entire pipelines (e.g., >110M parameters for\nBERT-Base) is obliviously costly. Moreover, maintaining the utility\nof pipelines trained by the noisy aggregated one is tricky due to\n1Distributed DP-SGD adds local noise too small to achieve LDP. But it is protected by\nsecret sharing. When all shares are aggregated, they cancel out each other, assuming\nan honest majority. It thus faces a â€œsynchronizationâ€ issue begging for identification\nand recovery mechanisms with computation and communication overheads [9].\narXiv:2309.06746v2  [cs.CR]  19 Sep 2023\nCCS â€™23, November 26â€“30, 2023, Copenhagen, Denmark. Du, et al.\nthe dimensional â€œcurse. â€ A recent study [79, Table 4] shows that the\naverage accuracy in fine-tuning LMs for four NLP tasks at moderate\nprivacy is 65.7% (vs. 91.8% without DP). Finally, the inference-time\nembeddings are not perturbed by the noise added during training,\nleaving inference queries vulnerable to various recovery attacks [56,\n64], ranging from sensitive attributes (e.g., authorship) to raw text.\n1.1 Natural Privacy via Perturbing Embeddings\nWe propose DP-Forward, a radically different approach that per-\nturbs forward-pass signals: Users can locally inject noise into the\nembeddings of (labeled) sequences before sharing them for training,\nin contrast to perturbing gradients in back-propagation (possibly\nby an untrusted party). It is meant for provablelocal DP (LDP) guar-\nantees, thus protecting against stronger adversaries than DP-SGD.\nOur approach also naturally fits the federated learning (FL) set-\nting that does not gather usersâ€™ data but with substantial differences\nâ€“ FL typically shares noiseless local model updates. Note that any\nsubsequent computation (e.g., gradient computation) on noisy em-\nbeddings incurs no extra privacy loss due to the free post-processing\nof LDP. One might force DP-SGD to offer LDP by adding â€œenoughâ€\nnoise to the orders-of-magnitude larger per-example gradient from\na user, but it may yield unusable models at a similar privacy level.\nDP-Forward also extends its applicability to inference via adding\nnoise to usersâ€™ test-sequence embeddings, ensuring LDP as in\ntraining. As a â€œsideâ€ benefit, it can effectively mitigate emerging\nembedding-based privacy risks [56, 64] beyond MIAs.\nIt is evident that the design goals of DP-Forward naturally align\nin tandem with our overarching objectives: LDP (vs. CDP), more\ndirect protection of raw data (vs. gradients) against new threats [56,\n64], and can be as efficient as regular non-private training (allowing\nbatch processing of noisy embeddings). The foundation support-\ning these desiderata, unfortunately, was unavailable. A dedicated\nmechanism to perturb the forward-pass signals is indispensable.\nSpecifically, we need to derive noises for embeddings of train-\ning/inference text sequences obtained through the forward pass\nof LM-based pipelines as a real- and matrix-valued function. One\nmight adopt the classical Gaussian mechanism (GM) [ 23] to add\ni.i.d. noise drawn from a univariate Gaussian distribution. Yet, GM\ncalibrates its noise variance based solely on a sufficient condition\nfor DP, and its variance formula is not applicable to a low privacy\nregime [7]. Another candidate is the matrix-variate Gaussian (MVG)\nmechanism [14], tailored for matrix-valued data: It exploits possibly\nnon-i.i.d. noise from a matrix Gaussian distribution to perturb more\nimportant rows/columns less. Although it may show better utility\nover GM [14], it is still sub-optimal due to the sufficient condition.\nTo optimize MVG, we propose an analytic matrix Gaussian mech-\nanism (aMGM) by integrating a necessary and sufficient condition\nfrom the analytic GM (aGM) [7] for non-i.i.d. noise calibration. Our\nchallenge lies in manipulating the two covariance matrices instead\nof a single variance. We deduce a constraint only on thetwo smallest\nsingular values (Section 4.2), indicating that i.i.d. noise (as in aGM)\nmay already be optimal for general applications like DP-Forward.2\nA transformer-based pipeline contains an input embedding layer,\nencoders, and task layers. All these layers prominently manipulate\n2With extra assumptions, dedicated allocation of other singular values by optimiz-\ning/maximizing utility functions specific to applications could help.\nembeddings of text inputs in training and subsequent inference. We\ninvestigate adding aMGM noise to embeddings output by any hid-\nden (sub-)layer before task layers (Figure 1). To ensuresequence-level\nLDP, we need to estimate theğ¿2-sensitivity [23] of â€œpre-noiseâ€ func-\ntions for any two sequences. It is non-trivial since the functions can\ninclude different (sub-)layers that may not even be Lipschitz [39].\nOur strategy is to normalize the function outputs to have a fixed\nFrobenius (or ğ¿2) norm, similar to gradient clipping [1]. It works\nespecially well for deeper sub-layers, achieving comparable task\naccuracy to the non-private baseline (Section 5). For the first few\n(sub-)layers, we also make two specializations in relaxing LDP to\nthe token level, elaborated in Appendix A.2, to improve accuracy.\n1.2 Our Contributions\nMotivated by prevailing privacy concerns in LM fine-tuning and\ninference and inherent shortcomings of DP-SGD, we initiate a for-\nmal study of an intuitive but rarely studied approach and explore\nits integration with a transformer-based NLP pipeline. Specifically:\n1) We propose DP-Forward fine-tuning, which perturbs the forward-\npass embeddings of every userâ€™s (labeled) sequence. It offers more\ndirect protection than DP-SGD perturbing aggregated gradients.\nIts provable guarantee (Theorem 1) is a new sequence-level LDP\nnotion (SeqLDP, Definition 4), with the more stringent (ğœ–,ğ›¿)-LDP\nguarantee to hold w.r.t. only sequences. Moreover, DP-Forward can\nnaturally extend to inference, ensuring the standard LDP (Theo-\nrem 3) for test sequences without labels, whereas DP-SGD cannot.\n2) To instantiate an optimal output perturbation mechanism for DP-\nForward, we propose aMGM, owning independent interests for any\nmatrix-valued function. By exploiting a necessary and sufficient DP\ncondition from aGM [7], it can draw possibly non-i.i.d. noise from a\nmatrix Gaussian distribution like MVG [14] while producing orders-\nof-magnitude smaller noise for high-dimensional data (Section 5.3).\n3) We conduct experiments3 on three typical NLP tasks in Section 5,\nshowing how crucial hyperparameters (e.g., the sequence length)\nimpact task accuracy. To fairly compare with DP-SGD on privacy-\nvs.-utility: i) We perturb labels by the randomized response [ 70]\nsuch that DP-Forward fine-tuning offers the standard LDP for\nsequence-label pairs (Theorem 2). ii) We â€œtranslateâ€ DP-Forward\nwith standard LDP to (example-level) CDP (as offered by DP-SGD)\nvia shuffling [25]. Our accuracy gain (for deep-layer DP-Forward\ninstantiations) is up to 7.7 percentage points (pp), compared to DP-\nSGD or its recent improvements [78, 79] (reviewed in Section 7.3),\nat a similar privacy level. Efficiency-wise, DP-SGD incurs>3Ã—time\nand GPU-memory costs even with the latest Opacus library [77].\n4) We evaluate three classes of privacy threats. Like DP-SGD, DP-\nForward (including the two token-level designs in Appendix A.3)\ncan effectively defend against sequence-level MIAs, but only DP-\nForward can thwart the two threats on (inference-time) embeddings.\nSpecifically, Section 6 shows that DP-SGDtotally fails in two embed-\nding inversion attacks, while DP-Forward remarkably reduces their\nsuccess rates by up to 88pp. For a neural-network-based attribute\ninference attack, DP-SGD reduces its success rates by only 15pp on\naverage, while DP-Forward achieves âˆ¼41pp reduction, making the\nattack predict like assigning all labels to the majority class.\n3Our code is available at https://github.com/xiangyue9607/DP-Forward.\nDP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass CCS â€™23, November 26â€“30, 2023, Copenhagen, Denmark.\nPre-noise Layers\nPost-noise Layers\nDownstream Tasks\nInput Embedding \nMHA\nAdd & Norm\nFFNAdd & NormEncoder #1\nâ€¦\nMatrix-valued Gaussian Noise Z\nEncoder #L\nNoisyEmbeddings\nData User/Generator\nService Provider\nOutputEmbedding \nPre-trained LMs such as BERT\nNoise\nWhereand Howto add noise?Fine-tuning and Inference\nLocal Differential Privacy\nFigure 1: A typical NLP pipeline built atop a pre-trained LM such as BERT with our matrix-valued Gaussian noise layer\nIn short, DP-Forward is a better alternative to DP-SGD in training\n(and testing) deep-learning models, e.g., gigantic LM-based ones.\n2 PRELIMINARIES AND NOTATIONS\n2.1 Transformer Encoders in BERT\nModern transformer-based LMs, including BERT [20] and GPT [59],\nare first pre-trained on enormous unannotated (public) corpora to\nlearn contextualized text representations. Later, they can be fine-\ntuned for various downstream NLP tasks (e.g., sentiment analysis,\nquestion answering) using much smaller, task-specific datasets.\nWe consider BERT (Figure 1), which comprises a stack of ğ¿iden-\ntical layers (i.e., bidirectional transformer encoders [68]). Each layer\nhas two sub-layers: the dot-productmulti-head attention (MHA) [68]\nwith â„heads and a feed-forward network (FFN). Each sub-layer has\nan extra residual connection, followed by layer normalization [6].\nLet ğ‘‹ = âŸ¨ğ‘¥ğ‘–âŸ©ğ‘›\nğ‘–=1 be an input sequence ofğ‘›tokens (e.g., characters,\nwords, sub-words, q-grams), where ğ‘¥ğ‘– is from a vocabulary V. The\ninput embedding layer first maps eachğ‘¥ğ‘– to its representation inRğ‘‘,\nwhich is the sum of the token, segment, and position embeddings.\nWe re-useğ‘‹ to represent the hiddenembedding matrix in Rğ‘›Ã—ğ‘‘. For\neach of â„attentions Attğ‘– in the MHA layer, we derive the query, key,\nand value matricesğ‘„,ğ¾,ğ‘‰ âˆˆRğ‘›Ã—ğ‘‘/â„ (â„divides ğ‘‘) by multiplyingğ‘‹\nwith head-specific weights ğ‘Šğ‘„,ğ‘Šğ¾,ğ‘Šğ‘‰ âˆˆRğ‘‘Ã—ğ‘‘/â„. Its output is\nAttğ‘–(ğ‘„,ğ¾,ğ‘‰ )= softmax(ğ‘„ğ¾âŠ¤\nâˆšï¸\nğ‘‘/â„\n)ğ‘‰,âˆ€ğ‘– âˆˆ[1,â„].\nThe input tosoftmax(Â·)is anğ‘›Ã—ğ‘›matrix of pairwise dot products.\nFinally, MHA concatenates (denoted by ||) all the head outputs into\na matrix in Rğ‘›Ã—ğ‘‘, right multiplied by a projection matrix ğ‘Šğ‘‚ âˆˆ\nRğ‘‘Ã—ğ‘‘:\nMHA(ğ‘‹)= [Att1||Â·Â·Â·|| Attâ„]ğ‘Šğ‘‚.\nFFN is composed of two linear mappings with a ReLU activation\nin between. It separately and identically operates on each ğ‘¥ğ‘–âˆˆ[1,ğ‘›],\nFFN(ğ‘¥ğ‘–)= ReLU(0,ğ‘¥ğ‘–ğ‘Š1 +ğ‘1)ğ‘Š2 +ğ‘2,\nwhere ğ‘Š1, ğ‘Š2, ğ‘1, and ğ‘2 are trainable matrix/vector-valued param-\neters. Its output on ğ‘‹ is FFN(ğ‘‹)= [FFN(ğ‘¥1)âŠ¤||Â·Â·Â·|| FFN(ğ‘¥ğ‘›)âŠ¤].\nThe residual connection for sub-layers is ğ‘‹ +MHA(ğ‘‹)/FFN(ğ‘‹).\nThe layer normalization LN(ğ‘¥ğ‘–)normalizes all ğ‘¥ğ‘– entries to have\nzero mean and unit variance using an extra scale-then-shift step.\nAt the output of the final encoder, the hidden embedding matrix\nis reduced to a sequence feature in R1Ã—ğ‘‘. Standard reduction meth-\nods include mean pooling [61] (computing Ãğ‘›\nğ‘–=1 ğ‘¥ğ‘–/ğ‘›) or taking the\nlast embedding of a special token [CLS] for classification [20].\nThe pre-training of BERT is based on two self-supervised tasks:\nmasked language model (MLM) and next sentence prediction [20].\nWe adopt MLM: It randomly masks out some tokens, indexed by I,\nin an input sequence ğ‘‹. The objective is to predict those masked\ntokens using their context by minimizing the cross-entropy loss\nğ¿MLM = âˆ’\nâˆ‘ï¸\nğ‘–âˆˆI\nlog Pr[ğ‘¥ğ‘–|Ë†ğ‘‹; ğœƒ], with Ë†ğ‘‹ = ğ‘‹ \\{ğ‘¥ğ‘–|ğ‘– âˆˆI}, (1)\nwhere ğœƒ denotes all the parameters of BERT transformer encoders.\n2.2 (Local) Differential Privacy\nDP [22] is a rigorous, quantifiable privacy notion. It has two popu-\nlar models, central and local. In central DP, a trusted data curator\naccesses the set Xof all individualsâ€™ raw data and processes Xby a\nrandomized mechanism Mwith some random noise. Formally:\nDefinition 1 (Central DP). For privacy parameters ğœ– â‰¥0 and\n0 â‰¤ğ›¿ â‰¤1, Mfulfills (ğœ–,ğ›¿)-DP if, for all neighboring datasets X\nand Xâ€²(denoted by Xâ‰ƒX â€²) and any subset Oof the outputs of M,\nPr[M(X)âˆˆO]â‰¤ ğ‘’ğœ– Pr[M(Xâ€²)âˆˆO]+ ğ›¿.\nWe call it ğœ–-DP or pure DP when ğ›¿ = 0.\nThe neighboring notion is application-dependent (to be discussed\nin Section 3.1). Typically, it involves the â€œreplace-oneâ€ relation:Xâ€²\ncan be obtained fromXby replacing a single individualâ€™s data point\n(e.g., a sequence-label pair). CDP offers plausible deniability to any\nindividual in a dataset. In contrast, local DP (LDP) [38] removes the\ntrusted curator, allowing individuals to locally perturb their data\nusing Mbefore being sent to an untrusted aggregator for analytics.\nDefinition 2 (Local DP). For ğœ– â‰¥0,0 â‰¤ğ›¿ â‰¤1, Mis (ğœ–,ğ›¿)-LDP\nif, for any two inputs ğ‘‹,ğ‘‹ â€²and any possible output subset Oof M,\nPr[M(ğ‘‹)âˆˆO]â‰¤ ğ‘’ğœ– Pr[M(ğ‘‹â€²)âˆˆO]+ ğ›¿.\nSimilarly, we call it ğœ–-LDP when ğ›¿ = 0.\nCCS â€™23, November 26â€“30, 2023, Copenhagen, Denmark. Du, et al.\nTable 1: Acronyms (newly proposed ones are marked with âˆ—)\nNLP Natural Language Processing\nLM Language Model\nBERT Bidirectional Encoder Representations from Transformers\nMLM Masked Language Modeling\nMHA Multi-Head Attention\nFFN Feed-Forward Network\nMIA Membership Inference Attack\nDP-SGD Differentially Private Stochastic Gradient Descent\nPLRV Privacy Loss Random Variable\n(C)DP (Central) Differential Privacy\nLDP Local Differential Privacy\nSeq(L)DPâˆ— Sequence (Local) Differential Privacy\nGM Gaussian Mechanism\nRR Randomized Response\nMVG Matrix-Variate Gaussian (Mechanism)\naGM Analytic Gaussian Mechanism\naMGMâˆ— Analytic Matrix Gaussian Mechanism\nPrivacy Loss Random Variable (PLRV). For a specific pair of inputs\nXâ‰ƒX â€², the privacy loss (or the â€œactual ğœ– valueâ€) [7] incurred by\nobserving an output ğ‘‚ is the log-ratio of two probabilities:\nLM,X,Xâ€²(ğ‘‚)= ln Pr[M(X) = ğ‘‚]\nPr[M(Xâ€²)= ğ‘‚].\nWhen ğ‘‚ varies according to M(X), we get the PLRV LM,X,Xâ€². A\nhelpful way to work with DP is to analyze tail bounds on PLRVs [23],\nwhich we utilize to build our proposed mechanism in Section 4.2.\nDP has two desirable properties: free post-processing and compos-\nability. The former means that further computations on the outputs\nof an (ğœ–,ğ›¿)-DP mechanism incur no extra privacy loss. The latter\nallows us to build more complicated mechanisms atop simpler ones:\nsequentially (and adaptively) running an(ğœ–,ğ›¿)-DP mechanism forğ‘˜\ntimes on the same input is at least (ğ‘˜ğœ–,ğ‘˜ğ›¿ )-DP. The two properties\nalso hold for LDP when considering a dataset has only one row.\nAn output perturbation mechanism Mfor a matrix-valued func-\ntion ğ‘“ : Xâ†’ Rğ‘›Ã—ğ‘‘ is given by computing ğ‘“ on the inputs and then\nadding random noise drawn from a random variable to its outputs.\nGaussian Mechanism (GM). For (ğœ–,ğ›¿)-DP, a typical instance of\nMis the classical GM [23], which adds noise ğ‘ âˆˆRğ‘›Ã—ğ‘‘ with each\nentry i.i.d. drawn from a univariate Gaussian distribution N(0,ğœ2).\nThe variance ğœ2 = 2 ln(1.25/ğ›¿)ğ‘†2\n2 (ğ‘“)/ğœ–2 with the ğ¿2-sensitivity:\nğ‘†2 (ğ‘“)= sup\nXâ‰ƒXâ€²\n||ğ‘“(X)âˆ’ ğ‘“(Xâ€²)||ğ¹,\nwhere ||Â·|| ğ¹ denotes the matrix Frobenius norm [34].\nTable 1 summarizes the acronyms throughout this work.\n3 DP-FORWARD\nWe study BERT-based pipelines as an example due to their superior\nperformance in classification tasks. DP-Forward can be readily ap-\nplied to other (transformer-based) NLP or computer vision models\nthat involve matrix-valued computation during the forward pass.\nSuppose each user holds a sequence-label pair (ğ‘‹,ğ‘¦)or only ğ‘‹\nfor fine-tuning or testing a pipeline at anuntrusted service provider.\nSharing redactedğ‘‹(with common PII removed) or its feature, a non-\nhuman-readable real-valued embedding matrix, is leaky [56, 64, 67].\nFor DP-Forward training, users perturb their embedding matrices\nlocally to ensure (new notions of) LDP before being shared, and they\nshould also perturb the corresponding labels if deemed sensitive\n(Section 3.4). We explore different options for splitting pipelines into\npre-noise functions ğ‘“(Â·)and post-noise processing ğ‘(Â·)in Section 3.2:\nUsers can accessğ‘“(Â·)to derive embedding matrices, perturbed by an\noutput perturbation mechanism M(e.g., GM); the service provider\nruns ğ‘(Â·)on noisy (labeled) embeddings for fine-tuning (Section 3.3)\nor pre-training (Section 3.6). The challenge lies in analyzing ğ‘†2 (ğ‘“)\nfor different pipeline parts, which we address by normalizing ğ‘“(Â·).\nDP-Forward can be naturally used to protectinference sequences\n(Section 3.5), unlike DP-SGD. It exploits the free post-processing\n(i.e., inference works on noisy embeddings), incurring minimal\nchanges to pipelines with the extra â€œplug-and-playâ€ noise layer.\n3.1 Notions of Sequence (Local) DP\nEmbeddings ğ‘“(ğ‘‹)encode semantic information of input sequences\nğ‘‹, each of which has ğ‘›tokens (Section 2.1). Fine-tuning (or subse-\nquent inference of) NLP pipelines essentially processes ğ‘“(ğ‘‹). DP-\nForward fine-tuning protects every ğ‘‹ by an output perturbation\nmechanism Mover ğ‘“(ğ‘‹), in contrast to DP-SGD, which perturbs\naggregates of gradients ğ‘“â€²(ğ‘‹,ğ‘¦)over ğ‘‹ and label ğ‘¦. Simply put, our\n(ğœ–,ğ›¿)-LDP holds for ğ‘‹ while DP-SGD provides CDP for (ğ‘‹,ğ‘¦).\nSequence-only protection is meaningful since sequences often\nconvey (implicit) sensitive information (e.g., authorship), whereas\nlabels (e.g., a single bit denoting positive/negative) can be public. We\ndefer to Section 3.4 for achieving â€œfullâ€ LDP over (ğ‘‹,ğ‘¦). To bridge\nthe gap between theoretical guarantees of DP-SGD and DP-Forward,\nwe first define sequence DP4 (SeqDP) in the central setting.\nDefinition 3 (SeqDP). For ğœ– â‰¥0,0 â‰¤ğ›¿ â‰¤1, Mis (ğœ–,ğ›¿)-SeqDP, if\nâˆ€Xâ‰ƒX â€²that only differ in a sequence at some index ğ‘–: (ğ‘‹ğ‘–,ğ‘¦ğ‘–)âˆˆX\nand (ğ‘‹â€²\nğ‘–,ğ‘¦ğ‘–)âˆˆX â€²,âˆ€ğ‘‹ğ‘–,ğ‘‹â€²\nğ‘–, and any possible output subset O,\nPr[M(X)âˆˆO]â‰¤ ğ‘’ğœ– Pr[M(Xâ€²)âˆˆO]+ ğ›¿.\n3.1.1 Label DP. The recently proposed notion of label DP [26, 31]\nis originally studied in PAC learning [16]. It only protectslabels (not\nthe corresponding inputs/images): (ğœ–,ğ›¿)-DP is only w.r.t. labels.\nOur SeqDP is â€œmore secureâ€ than or at least â€œcomplementsâ€ label\nDP, which has an inherent flaw [11]: As labels typically rely on their\nsequences (but not vice versa), it is very likely to recover the true\nlabels from the raw sequences, even if the labels are protected (by\nany label-DP mechanism). The follow-up [72] shows the impossi-\nbility of label protection under label DP even with arbitrarily small\n(ğœ–,ğ›¿)when models generalize. Moreover, labels can be absent (e.g.,\ninference or self-supervised learning), for which SeqDP upgrades\nto the standard (ğœ–,ğ›¿)-DP, whereas label DP is simply inapplicable.\n3.1.2 Sequence Local DP (SeqLDP). We further define SeqLDP, the\nlocal counterpart of sequence DP. Note that the above discussion\nof label DP in relation to SeqDP also carries over to SeqLDP.\nDefinition 4 (SeqLDP). For ğœ– â‰¥0,0 â‰¤ğ›¿ â‰¤ 1, Msatisfies (ğœ–,ğ›¿)-\nSeqLDP, if âˆ€ğ‘‹,ğ‘‹ â€²with the same ğ‘¦, and any possible output subset O,\nPr[M(ğ‘‹,ğ‘¦)âˆˆO]â‰¤ ğ‘’ğœ– Pr[M(ğ‘‹â€²,ğ‘¦)âˆˆO]+ ğ›¿.\n4One could generalize it to â€œfeatureâ€ (or â€œinputâ€) DP, as DP-Forward also allows other\ntypes of features beyond embeddings (and its essence is input-only privacy). To keep\nour focus on NLP, we use â€œsequenceâ€ here. (PixelDP [41] treats pixels as image features.)\nDP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass CCS â€™23, November 26â€“30, 2023, Copenhagen, Denmark.\nIn theory, SeqLDP remains a strong notion (like the standard\nLDP). It is meant to be information-theoretic protection on sequence\nand bounds the indistinguishability ofany ğ‘‹,ğ‘‹ â€²(differing by up toğ‘›\ntokens), and hence governing the â€œusefulnessâ€ of noisy embeddings.\n3.1.3 Sequence-Level SeqLDP vs. Token-Level SeqLDP.In practice,\nas a strong notion balancing seemingly conflicting requirements\n(ideal theoretical guarantees and empirical utility), attaining a mean-\ningful range of ğœ– for SeqLDP is a struggle. Adding Gaussian noise\nto the outputs of ğ‘“(Â·)for (ğœ–,ğ›¿)-SeqLDP requires bounding the ğ¿2-\nsensitivity ğ‘†2 (ğ‘“),âˆ€ğ‘‹,ğ‘‹ â€². Our approach is to normalize the outputs\n(with extra benefits elaborated in Section 3.2), similar to clipping gra-\ndients in DP-SGD. It generally works better when ğ‘“(Â·)has more\nlayers (at the same meaningful range of ğœ–) since fewer (trainable)\nparameters/layers of ğ‘(Â·)are â€œaffectedâ€ by the noisy outputs.\nUnfortunately, when ğ‘“(Â·)includes the first few layer(s),e.g., only\nthe input embedding layer is available to the users (say, for sav-\ning user-side storage and computation overheads), it leads to poor\nutility. As a comprehensive study, we resort torow-wise normaliza-\ntion with the (composition of) Lipschitz constants [39] to maintain\nutility for those cases.5 In contrast to the general normalization, it\naims for weaker SeqLDP at the token level (cf. event-level vs. user-\nlevel LDP [81]), a finer granularity in the â€œprotection hierarchy, â€\nprotecting any neighboring sequences (vs. datasets) differing in any\nsingle token (vs. sequence). Details are deferred to Appendix A.\n3.2 Our Approach for Sequence LDP\nDP-Forward in our paper (except Appendix A) applies the general\nnormalization approach to any ğ‘“(Â·)for sequence-level (Seq)LDP.\nLet ğ‘“(Â·)be an arbitrarily deep forward pass, ranging from the\nfirst (input embedding) layer itself to all but the last (task) layer in\na BERT-based pipeline (Figure 1). Correspondingly, letğ‘(Â·)be the\nremaining layers, ranging from the last task layers themselves to all\nbut the first (input embedding) layer. Every sequenceğ‘‹ becomes an\nembedding matrix ğ‘“(ğ‘‹)âˆˆ Rğ‘›Ã—ğ‘‘ at the output of layers in encoders\nor R1Ã—ğ‘‘ before task layers (Section 2.1). To offer (ğœ–,ğ›¿)-SeqLDP, we\nadopt a suitable output perturbation mechanism M, such as GM,\nconsidering that a dataset has only one labeled sequence.\nSince Mcan work on the output of any hidden layer, estimating\nğ‘†2 (ğ‘“)is non-trivial. Specifically, MHA itself, let alone more lay-\ners included, is not Lipschitz continuous, meaning its outputs can\nchange arbitrarily for even slight input variation [39]. To address\nthis, our approach is to normalize or clip the function outputs:\n||ğ‘“(Â·)||ğ¹ = ğ¶ or ğ‘“(Â·)/max(1, ||ğ‘“(Â·)||ğ¹\nğ¶ )\nas in DP-SGD [1], where ğ¶ is a tunable parameter. We then have\nğ‘†2 (ğ‘“)= 2ğ¶. Such normalization makes task utility less â€œsensitiveâ€ to\nthe choice ofğ¶since signal and noise increase proportionally withğ¶,\nwhereas the signal may be unchanged when ğ‘“(Â·)is not clipped. It\nalso has many other benefits, such as stabilizing training, avoiding\noverfitting, and accelerating convergence [2]. Hence, we resort to\nnormalization in our experiments. One can then calibrate Gaussian\nnoise ğ‘ and derive ğ‘“(ğ‘‹)+ğ‘ for the post-noise layers ğ‘(Â·).\n5One might also resort to the weaker random DP [33] â€“ (ğœ–,ğ›¿)-DP holds on all but a\nsmall ğ›¾-proportion of â€œunlikelyâ€ Xâ‰ƒX â€²for an extra parameterğ›¾ âˆˆ(0,1). It is useful\nwhen the global sensitivity is hard to compute. Exploring it is left as future work.\nNote that we remove the residual connection when adding noise\nto the output of the first MHA layer to avoid ğ‘(Â·)reaccessing ğ‘‹\n(dashed arrow, Figure 1) to maintain free post-processing. This\nmay lead to instability ( e.g., gradient vanishing) [ 79], but it can\nbe mitigated by pre-training new BERT without such a residual\nconnection to keep consistent with later fine-tuning/inference.\nDP-Forward using GM suffers from the â€œcurse of dimensionalityâ€\nwhen ğ‘‘ is large (e.g., 768 for BERT-Base). To alleviate the curse,\nwe can append two linear maps, ğ‘€1,ğ‘€2 âˆˆRğ‘‘Ã—ğ‘‘â€²\nwith ğ‘‘â€² â‰ªğ‘‘,\nsuch that ğ‘“(Â·)and ğ‘(Â·)respectively have ğ‘€1 and ğ‘€2. Both maps\nare randomly initialized and updated like other weights using gra-\ndients. The raw embedding matrix is first right multiplied by ğ‘€1,\nleading to Rğ‘›Ã—ğ‘‘â€²\nor R1Ã—ğ‘‘â€²\n, before being normalized. Our privacy\nguarantee will not be affected since ğ‘†2 (ğ‘“)remains the same. We\nthen use ğ‘€2 to restore the dimensionality to be compatible with\nthe raw pipeline; ğ‘€2 incurs no extra privacy loss due to the free\npost-processing. Nevertheless, it needs dedicated efforts to modify\nthe pipeline; dimension-reduced embedding matrices may also lose\nuseful information, degrading task utility. We thus makeğ‘€1 and\nğ‘€2 optional (see Section 5.2).\n3.3 DP-Forward Fine-tuning\nSuppose we use a raw, public BERT checkpoint6 for fine-tuning. In\nthe forward pass of theğ‘–-th (ğ‘– â‰¥1) step, it offers the latest ğ‘“(ğ‘–âˆ’1)(Â·)\nto a batch of users, mimicking the regular mini-batch SGD. ğ‘“(0)\nis from the raw checkpoint. Users are randomly chosen (without\nreplacement), and their number is a fixed parameter. Users in the\nbatch individually compute their noisy embeddings ğ‘“(ğ‘–âˆ’1)(ğ‘‹)+ğ‘\nto ensure SeqLDP (Theorem 1). They then send them with unper-\nturbed labels ğ‘¦ to the service provider, who runs ğ‘(ğ‘–âˆ’1)(Â·)over\n(ğ‘“(ğ‘–âˆ’1)(ğ‘‹)+ğ‘,ğ‘¦)to compute the batch loss; any post-processing\nof embeddings under SeqLDP incurs no extra privacy degradation\non ğ‘‹. ğ‘(0) here includes the rest raw BERT part and randomly\ninitialized task layers.\nDuring the back-propagation, the service provider can update\nğ‘(ğ‘–âˆ’1)(Â·)to ğ‘(ğ‘–)(Â·)via the gradient (derived from the loss and\nnoisy embeddings) of the post-noise layers. To avoid accessing\nusersâ€™ raw ğ‘‹, it needs to freeze the pre-noise layers ğ‘“(ğ‘–âˆ’1)(Â·)as\nğ‘“(0). Parameter freezing is compatible with the more recent zero-\nshot or in-context learning paradigm [52]. It is useful when models\nare gigantic and full fine-tuning is expensive. However, the more\nlayers are frozen, the worse the utility might be (even in non-private\nsettings).\nThere are two general ways to update ğ‘“(ğ‘–âˆ’1)(Â·)securely: i) We\ncan assume an extra trusted party (as in DP-SGD), but it becomes\ncentral DP. ii) Users can first derive the gradients for the layers\ninside ğ‘“(ğ‘–âˆ’1)(Â·)locally on their ğ‘‹ and then resort to secure ag-\ngregation [9] for global updates at the service provider. However,\nit is costly. For better utility, we update ğ‘“(ğ‘–âˆ’1)(Â·)in experiments,\nrequiring us to consider privacy degradation across differentepochs\ndue to the composability (as detailed below). Dedicated approaches\n(that balance efficiency, privacy, and utility) are left as future work.\n6Using noisy BERT for fine-tuning (and subsequent inference) is deferred to Section 3.6.\nCCS â€™23, November 26â€“30, 2023, Copenhagen, Denmark. Du, et al.\nTheorem 1. Let ğ‘“(Â·)be the pre-noise function (of BERT-based\npipelines) and Mbe GM with ğœ– â‰¥0,0 â‰¤ğ›¿ â‰¤1. DP-Forward fine-\ntuning running Mon normalized/clipped ğ‘“(Â·)ensures (ğœ–,ğ›¿)-SeqLDP.\nThe proof follows that of GM [23]. The crux is thatğ‘†2 (ğ‘“),âˆ€ğ‘‹,ğ‘‹ â€²\nis given by the output normalization, independent of the inputs.\nPrivacy Accounting. An epoch refers to an entire transit of the pri-\nvate training corpus. Every ğ‘‹ is used once per epoch. The number\nof epochs ğ‘˜is a hyperparameter, which is typically small. Repeated\napplications of GM over the same ğ‘‹ ask for estimating the overall\nprivacy loss due to the composability (unless freezingğ‘“ for re-using\nğ‘“(ğ‘‹)+ğ‘). The well-known moments accountant [1] (or its general-\nization to RÃ©nyi DP [53]) only provides a loose upper bound, which\nis even inapplicable if unbounded moments exist. Gaussian DP [10]\nproposes an accountant based on the central limit theorem. Yet, it\nleads to significant underestimation by a lower bound. Instead, we\nresort to a recent numerical accountant [32], which outperforms\nRDP or GDP by approximating the true overall ğœ– to arbitrary accu-\nracy. It composes the privacy curve of a mechanism by truncating\nand discretizing PLRVs with their PDFs convoluted by FFT [32].\n3.4 DP-Forward with Shuffling versus DP-SGD\nDP-Forward ensures SeqLDP for fine-tuning, while DP-SGD offers\ncentral DP (for sequence-label pairs). To facilitate a fair comparison\n(on privacy-utility tradeoffs), we make two changes. First, we also\nperturb the labels with a suitable mechanism for the standard LDP,\ni.e., extending the protection from sequence to sequence-label pairs.\nSecond, we use shuffling [25] to â€œtranslateâ€ our (label-protected)\nDP-Forward with LDP to claim (example-level) CDP as DP-SGD.\nDiscrete Labels Perturbation. For most NLP tasks, e.g., bi-/multi-\nnary classification in the GLUE benchmark [69], the size |y|of label\nspace is often small. A simple yet effective solution for discrete data\nis randomized response (RR) [70] proposed decades ago! Specifically,\nRR perturbs a true label ğ‘¦to itself Ë†ğ‘¦ = ğ‘¦with the probability\nPr[ğ‘¦ = Ë†ğ‘¦]= ğ‘’ğœ–/(ğ‘’ğœ– +|y|âˆ’1),\nor to âˆ€Ë†ğ‘¦ âˆˆy \\ğ‘¦uniformly, where y denotes the label space.\nWhen |y|is large, we can use prior to â€œpruneâ€y to smaller yâ€²[31].\nThe prior can be publicly available (e.g., auxiliary corpora similar to\nthe usersâ€™ data) or progressively refined from a uniform distribution\nvia the multi-stage training [31]. One can then estimate an optimal\n|yâ€²|by maximizing the probability that the output is correct, i.e.,\nPr[ğ‘¦ = Ë†ğ‘¦]. With (prior-aided) RR [31], we can achieve full LDP.\nTheorem 2. Let ğ‘“(Â·)be the pre-noise function (of BERT-based\npipelines), Mbe GM with ğœ–1 â‰¥0,0 â‰¤ğ›¿ â‰¤1, and Mğ‘…ğ‘… be (prior-\naided) RR with ğœ–2 â‰¥0. DP-Forward fine-tuning perturbing ğ‘“(ğ‘‹)and\nğ‘¦separately by Mand Mğ‘…ğ‘… ensures (ğœ–1 +ğœ–2,ğ›¿)-LDP.\nThe proof follows from the basic composition theorem [23].\nPrivacy Amplification by Shuffling. If noisy embedding-label\npairs are also shuffled properly, DP-Forward can claim example-\nlevel CDP (as in DP-SGD), which â€œamplifiesâ€ LDP guarantees by\nÎ˜(\nâˆš\nğ‘)for a total number of ğ‘ users (without extra noise addi-\ntion) [25]. We then show that DP-Forward qualitatively outperforms\nDP-SGD from the SNR perspective under a similar privacy regime.\nSuppose we train for an epoch, and the normalization factor isğ¶.\nFor DP-SGD, the batch size isğ‘; the subsampling probability and the\nnumber of training steps are respectivelyğ‘/ğ‘ and ğ‘/ğ‘. If each step\nis (ğœ–,ğ›¿)-DP, the overall privacy loss is(ğ‘‚(ğœ–\nâˆšï¸\nğ‘/ğ‘),ğ›¿)-DP using the\nstrong composition and privacy amplification by subsampling [1].\nDP-Forward with shuffling can also be seen as composing ğ‘\nsubsamplings, each a fraction of size1 [66]. It is (ğ‘‚(ğœ–\nâˆšï¸\n1/ğ‘),ğ›¿)-DP,\nwhich is â€œamplifiedâ€ from (ğœ–,ğ›¿)-LDP. For an easier analysis of SNR,\nwe omit ğœ–2 of RR since the overall ğœ– is dominated by composing\nsubsampled Gaussian. So, our Gaussian noise variance isğ‘Ã—smaller\nthan DP-SGDâ€™s in each step; the SNR of each entry in embeddings\nvs. the aggregation of ğ‘gradients can be estimated as ğ‘‚(ğ¶/\nâˆš\nğ‘›ğ‘‘)\nfor DP-Forward vs.ğ‘‚(ğ¶/\nâˆš\nğ‘‘â€²)for DP-SGD, whereğ‘‘â€²is the gradient\ndimension and is much larger than ğ‘›ğ‘‘, the embedding-matrix size.\n3.5 DP-Forward Inference\nGiven only fine-tuned pipeline partsğ‘“(Â·), users can derive the noisy\nembedding matrices of their test sequences for inferences at the\nservice provider while ensuring (ğœ–,ğ›¿)-LDP. Inference using noise\naligned to the noisy fine-tuning is also beneficial for task accuracy.\nLocal inference (as in DP-SGD) without noise forces the service\nprovider to reveal its entire pipeline, losing its intellectual property\nand incurring more time and storage costs for both ğ‘“(Â·)and ğ‘(Â·).\nTheorem 3. Let ğ‘“(Â·)be the fine-tuned pre-noise layers (of BERT-\nbased pipelines) and Mbe GM with ğœ– â‰¥0,0 â‰¤ğ›¿ â‰¤1. DP-Forward\ninference running Mon normalized/clipped ğ‘“(Â·)ensures (ğœ–,ğ›¿)-LDP.\nThe proof is inherited from GM [23]. Different from DP-Forward\nfine-tuning, LDP holds for test sequences since the labels are absent.\n3.6 DP-Forward Pre-training\nDirectly using the raw BERT might not â€œmatchâ€ DP-Forward fine-\ntuning/inference, degrading task utility. Pre-training BERT with\nDP-Forward on publicly available text (e.g., Wikipedia), besides the\nprivate user-shared data, can make future operations â€œadaptiveâ€ to\nnoise. It requires us to modify the raw MLM objective in Eq. (1):\nğ¿âˆ—\nMLM = âˆ’\nâˆ‘ï¸\nğ‘–âˆˆI\nlog Pr[ğ‘¥ğ‘–|M(ğ‘“(Ë†ğ‘‹)); ğœƒâˆ—],\nwhere ğœƒâˆ—denotes the parameters of â€œnoisyâ€ BERT. This endows the\nnoisy BERT with some â€œde-noisingâ€ ability since the objective is to\npredict the raw masked tokens from noisy embeddingsM(ğ‘“(Ë†ğ‘‹)). It\ndoes not really breach privacy due to the free post-processing; LDP\nis ensured for each sequence, as the pre-training is self-supervised\n(without labels). Such noisy pre-training can also be outsourced to\ndedicated GPU clusters, enabling â€œde-noising BERT as a service. â€\nDe-noising as post-processing is not new, but most prior arts\nneed prior knowledge, e.g., Bayesian prior. aGM formulates it as an\nunusual estimation problem since a single noisy output is observed\nfor each input, which can then be solved by appropriate estimators,\ne.g., the Bayesian one [7]. Another attempt [41] trains a separate\nnoisy auto-encoder, which learns the identity function ğ‘“(ğ‘‹)= ğ‘‹\nstacked before an image classification network, to de-noise the noisy\ninput. It has limited applications for only noisy input embeddings\nand incurs extra changes when migrating it to an NLP pipeline.\nDP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass CCS â€™23, November 26â€“30, 2023, Copenhagen, Denmark.\n4 OPTIMIZING MATRIX GAUSSIAN NOISE\nTo instantiateMfor ğ‘“(Â·)âˆˆ Rğ‘›Ã—ğ‘‘ of DP-Forward, a natural question\nis whether the classical GM is optimal. The answer is no. Its privacy\nanalysis applies a sufficient but not necessary condition for (ğœ–,ğ›¿)-DP\nwhile using Gaussian tail approximations, and its variance formula\ncannot extend to ğœ– > 1 for a single run (e.g., inference) [23].\nAnother candidate is the matrix-variate Gaussian (MVG) mecha-\nnism [14], tailored for matrix-valued functions. It exploits possibly\nnon-i.i.d. noise from amatrix Gaussian distribution and outperforms\nGM in several usage cases [14]. Yet, it is not optimal either, with the\nroot cause still being based on a sufficient DP condition (Section 4.1).\nTo improve it, we resort to a necessary and sufficient condition from\naGM [7] for calibrating the matrix Gaussian noise (Section 4.2).\n4.1 Matrix-Variate Gaussian (MVG) Mechanism\nIn contrast to the classical GM, MVG adopts possiblynon-i.i.d. noise\nğ‘ âˆˆRğ‘›Ã—ğ‘‘ drawn from the zero-mean matrix Gaussian distribution\nMNğ‘›,ğ‘‘(0,Î£,Î¨), where Î£ âˆˆRğ‘›Ã—ğ‘› and Î¨ âˆˆRğ‘‘Ã—ğ‘‘ are the row- and\ncolumn-wise covariance matrices. Intuitively, it adds less noise to\nmore â€œimportantâ€ rows or columns for possible better utility.\nDefinition 5 (Matrix Gaussian Distribution). The PDF for an\nğ‘›Ã—ğ‘‘ random variable ğ‘ following MNğ‘›,ğ‘‘(0,Î£,Î¨)has the form:\nPr (ğ‘|0,Î£,Î¨)=\nexp\n\u0010\nâˆ’1\n2 ||ğ‘ˆâˆ’1ğ‘ğ‘‰âˆ’âŠ¤||2\nğ¹\n\u0011\n(2ğœ‹)ğ‘›ğ‘‘/2|Î¨|ğ‘‘/2|Î£|ğ‘›/2 , (2)\nwhere ğ‘ˆ âˆˆRğ‘›Ã—ğ‘› and ğ‘‰ âˆˆRğ‘‘Ã—ğ‘‘ are invertible with Î£ = ğ‘ˆğ‘ˆâŠ¤and\nÎ¨ = ğ‘‰ğ‘‰âŠ¤, and |Â·| denotes the matrix determinant [34].\nThe definition is equivalent to the conventional form given by\nthe matrix trace. It generalizes the univariate Gaussian used in GM;\nğ‘ becomes i.i.d. when Î£,Î¨ are diagonal and equal-valued. Below\nrecites the main theorem of the MVG mechanism for (ğœ–,ğ›¿)-DP.\nTheorem 4 (The MVG Mechanism with (ğœ–,ğ›¿)-DP [14]). Let\nğœ(Î£âˆ’1)= [ğœ1 (Î£âˆ’1),...,ğœ ğ‘›(Î£âˆ’1)]âŠ¤,\nğœ(Î¨âˆ’1)= [ğœ1 (Î¨âˆ’1),...,ğœ ğ‘‘(Î¨âˆ’1)]âŠ¤\nbe the vectors of (non-increasingly ordered) singular values of Î£âˆ’1 and\nÎ¨âˆ’1, respectively. The MVG mechanism using noise from the matrix\nGaussian distribution MNğ‘›,ğ‘‘(0,Î£,Î¨)satisfies (ğœ–,ğ›¿)-DP if\n||ğœ(Î£âˆ’1)||2 Â·||ğœ(Î¨âˆ’1)||2 â‰¤\n\u0010\nâˆ’ğ›½+\nâˆšï¸\nğ›½2 +8ğ›¼ğœ–\n\u00112\n4ğ›¼2 ,\nwhere ğ›¼ = [ğ»ğ‘Ÿ +ğ»ğ‘Ÿ,1/2]ğ›¾2 +2ğ»ğ‘Ÿğ›¾ğ‘†2 (ğ‘“), ğ›½ = 2(ğ‘›ğ‘‘)1/4ğ»ğ‘Ÿğ‘†2 (ğ‘“)ğœ(ğ›¿),\nwith ğ»ğ‘Ÿ (or ğ»ğ‘Ÿ,1/2) being the generalized harmonic number of order ğ‘Ÿ\n(of 1/2), ğ›¾being supX||ğ‘“(X)||ğ¹, and ğœ(ğ›¿)= 2\nâˆš\nâˆ’ğ‘›ğ‘‘ln ğ›¿âˆ’2 ln ğ›¿+ğ‘›ğ‘‘.\nTo illustrate how the MVG mechanism works, we quote an ex-\nample [14]: performing regression using an identity query on a\nliver disorders dataset [49] with 6 features and 248 samples (i.e.,\nğ‘“ âˆˆ R248Ã—6). MVG treats â€˜ALTâ€™ and a teacher label as the two\nmost indicative features based on some prior, thus added with less\nnoise [14]. To report the best empirical results, it tries different\nprecision budget (or noise variance) allocation strategies so that the\ntotal budget (Theorem 4) is not overspent. For example, it evenly\nallocates ğœ > 50% (a tunable parameter) of the budget to the two\nAlgorithm 1: ğ´(ğœ–,ğ›¿): Derive the Upper Bound B\nInput: Privacy parameters ğœ–,ğ›¿\nOutput: B\n1 Let ğ›¿0 = Î¦(0)âˆ’ğ‘’ğœ–Î¦(âˆ’\nâˆš\n2ğœ–);\n2 if ğ›¿ â‰¥ğ›¿0 then\n3 Re-write ğ‘”+ğœ–(ğ‘£)= Î¦(âˆšğœ–ğ‘£)âˆ’ğ‘’ğœ–Î¦(âˆ’\nâˆšï¸\nğœ–(ğ‘£+2);\n4 Compute ğ‘£âˆ—= sup{ğ‘£ âˆˆRâ‰¥0 : ğ‘”+ğœ–(ğ‘£)= ğ›¿};\n5 Let ğ›¼ =\nâˆšï¸\n1 +ğ‘£âˆ—/2 âˆ’\nâˆšï¸\nğ‘£âˆ—/2;\n6 else\n7 ğ‘”âˆ’ğœ– (ğ‘¢)= Î¦(âˆ’âˆšğœ–ğ‘¢)âˆ’ğ‘’ğœ–Î¦(âˆ’\nâˆšï¸\nğœ–(ğ‘¢+2));\n8 Compute ğ‘¢âˆ—= inf{ğ‘¢ âˆˆRâ‰¥0 : ğ‘”âˆ’ğœ– (ğ‘¢)= ğ›¿};\n9 Let ğ›¼ =\nâˆšï¸\n1 +ğ‘¢âˆ—/2 +\nâˆšï¸\nğ‘¢âˆ—/2;\n10 end\n11 Return B=\nâˆš\n2ğœ–/ğ›¼\nimportant features and the rest to the other four. Compared to GM\nusing i.i.d. Gaussian noise, MVG can improve root mean square\nerror (RMSE) by up to 0.003 at the same privacy level [14].\nSub-optimality of MVG. Theorem 4 presents an upper bound on\nthe product of ğ¿2-norms of two singular-value vectors ğœ(Î£âˆ’1)and\nğœ(Î¨âˆ’1), assuming ||ğ‘“(X)||ğ¹ is bounded for any Xby a constant ğ›¾.\nThe upper bound monotonically decreases with ğ›½ that depends on\nğ‘›ğ‘‘ and approaches 0 as ğ‘›ğ‘‘ â†’âˆ, making the sums of noise vari-\nances large. A similar situation exists in high privacy regimesğœ– â†’0.\nAt least two slacks caused the sub-optimality. The first and fore-\nmost is due to asufficient condition for (ğœ–,ğ›¿)-DP [23]: Pr[LM,X,Xâ€² â‰¥\nğœ–]â‰¤ ğ›¿, which is also used in the classical GM. With the Laurent-\nMassart Theorem [40], MVG further transforms it toPr[LM,X,Xâ€² â‰¤\nğœ–]= 1 for a subset of all the possible outputs. The second lies in a\nloose matrix-trace-based privacy analysis; a follow-up [75] derives\na tighter bound from Definition 5 and a matrix-norm inequality.\n4.2 Analytic Matrix Gaussian Mechanism\nTo enhance MVG while still adding possibly non-i.i.d. noise ğ‘ âˆ¼\nMNğ‘›,ğ‘‘(0,Î£,Î¨), we put forth the analytic matrix Gaussian mecha-\nnism (aMGM) by exploiting a necessary and sufficient condition for\n(ğœ–,ğ›¿)-DP, which is formulated using two PLRVs by the analytic GM\n(aGM) [7]. It is non-trivial7 since we now need to work with two\ncovariance matrices Î£ and Î¨ instead of a single varianceğœ2 in aGM.\nTheorem 5 ([7]). A mechanism Mis (ğœ–,ğ›¿)-DP iff, âˆ€Xâ‰ƒX â€²,\nPr[LM,X,Xâ€² â‰¥ğœ–]âˆ’ğ‘’ğœ– Pr[LM,Xâ€²,Xâ‰¤âˆ’ğœ–]â‰¤ ğ›¿. (3)\nIt directly implies the sufficient condition due to Pr[LM,Xâ€²,Xâ‰¤\nâˆ’ğœ–]â‰¥ 0. We next show thatLM,X,Xâ€² or LM,Xâ€²,Xof aMGM is also\nGaussian, a similar result has been proven in aGM [7, Lemma 3].\nLemma 1. The PLRVs of our aMGM follow a distribution N(ğœ‚,2ğœ‚)\nwith ğœ‚ = ||ğ‘ˆâˆ’1Î”ğ‘‰âˆ’âŠ¤||2\nğ¹\n2 , where Î” = ğ‘“(X)âˆ’ ğ‘“(Xâ€²).\n7A recent pre-print [74] also studied using matrix Gaussian distribution. The proof of\n[74, Lemma 4], pivotal for our Theorem 6, is problematic. We prove it in Appendix C.\nCCS â€™23, November 26â€“30, 2023, Copenhagen, Denmark. Du, et al.\nWith Lemma 1, we can then specialize the left-hand side of Eq. (3).\nParticularly, we use the Gaussian cumulative density function (CDF)\nÎ¦(ğ‘¡)= Pr[N(0,1)â‰¤ ğ‘¡]= 1âˆš\n2ğœ‹\nâˆ« ğ‘¡\nâˆ’âˆ\nğ‘’âˆ’ğ‘¦2/2ğ‘‘ğ‘¦\nto explicitly express the two probabilities (see Lemma 2) instead of\napproximating them by the tail bounds of a Gaussian distribution.\nLemma 2. For any X â‰ƒ Xâ€², let Î”â€² = ğ‘ˆâˆ’1Î”ğ‘‰âˆ’âŠ¤ with Î” =\nğ‘“(X)âˆ’ ğ‘“(Xâ€²). The following holds for any ğœ– â‰¥0:\nPr[LM,X,Xâ€² â‰¥ğœ–]= Î¦\n\u0012 ||Î”â€²||ğ¹\n2 âˆ’ ğœ–\n||Î”â€²||ğ¹\n\u0013\n,\nPr[LM,Xâ€²,Xâ‰¤âˆ’ğœ–]= Î¦\n\u0012\nâˆ’||Î”â€²||ğ¹\n2 âˆ’ ğœ–\n||Î”â€²||ğ¹\n\u0013\n.\nWe can further re-write the left-hand side of Eq. (3) asğ‘”(||Î”â€²||ğ¹):\nÎ¦\n\u0012 ||Î”â€²||ğ¹\n2 âˆ’ ğœ–\n||Î”â€²||ğ¹\n\u0013\nâˆ’ğ‘’ğœ–Î¦\n\u0012\nâˆ’||Î”â€²||ğ¹\n2 âˆ’ ğœ–\n||Î”â€²||ğ¹\n\u0013\n, (4)\na function ofÎ” and (Î£,Î¨); it is defined w.r.t.Î” and ğœ2 for aGM [7]. To\nsatisfy Theorem 5, we requireğ‘”(||Î”â€²||ğ¹)â‰¤ ğ›¿,âˆ€Xâ‰ƒX â€². Sinceğ‘”(Â·)is\nmonotonically increasing [7, Lemma 7], we first find the upper bound\nBof ||Î”â€²||ğ¹ as the â€œsolutionâ€ to ğ‘”(||Î”â€²||ğ¹)= ğ›¿ and then determine\nğ‘ˆ,ğ‘‰ (hence Î£,Î¨) based on Band Î” with ğ‘†2 (ğ‘“)= supXâ‰ƒXâ€² ||Î”||ğ¹.\n4.2.1 Computing the upper boundB. One could derive an analytic\nexpression for Busing the tail bounds ofÎ¦(ğ‘¡), which is sub-optimal\ndue to the slack in the tail bounds. Instead, we adapt a â€œnumerical\nsolver, â€ as detailed in Alg. 1, forBsince Î¦(ğ‘¡)can also be represented\nby (1 +erf(ğ‘¡/\nâˆš\n2))/2, where erf is the standard error function.8\nFor the first term of Eq. (4), its input||Î”â€²||ğ¹/2âˆ’ğœ–/||Î”â€²||ğ¹ changes\nsign at ||Î”â€²||ğ¹ =\nâˆš\n2ğœ–, while the other termâ€™s input âˆ’||Î”â€²||ğ¹/2 âˆ’\nğœ–/||Î”â€²||ğ¹ is always negative. Therefore, we only consider ||Î”â€²||ğ¹ =âˆš\n2ğœ–/ğ›¼ under two cases 0 < ğ›¼ â‰¤1 and ğ›¼ > 1 for a variable ğ›¼.\nWhen ğ›¼ = 1, ğ›¿0 = ğ‘”(\nâˆš\n2ğœ–)in line 1. If ğ›¿ â‰¥ğ›¿0 (or 0 < ğ›¼ â‰¤1), we\ncan use ğ‘£ = (1/ğ›¼âˆ’ğ›¼)2/2 to re-writeğ‘”(Â·)as ğ‘”+ğœ–(ğ‘£)(line 3). Forğ›¼ > 1,\nwe can use ğ‘¢ = (ğ›¼ âˆ’1/ğ›¼)2/2 to re-write ğ‘”(Â·)as ğ‘”âˆ’ğœ– (ğ‘¢)(line 7). In\neither case, given the â€œoracleâ€ computing Î¦(ğ‘¡)via erf, we derive ğ‘¢âˆ—\nor ğ‘£âˆ—using Newtonâ€™s method, recover ğ›¼, and return B=\nâˆš\n2ğœ–/ğ›¼.\n4.2.2 Determining the covariance matricesÎ£ = ğ‘ˆğ‘ˆâŠ¤and Î¨ = ğ‘‰ğ‘‰âŠ¤.\nWith Lemma 6, and let ğœğ‘–(Â·)be the ğ‘–th singular value; we have\n||ğ‘ˆâˆ’1Î”ğ‘‰âˆ’âŠ¤||2\nğ¹ â‰¤\nmin{ğ‘›,ğ‘‘}âˆ‘ï¸\nğ‘–=1\nğœ2\nğ‘– (ğ‘ˆâˆ’1)ğœ2\nğ‘– (Î”)ğœ2\nğ‘– (ğ‘‰âˆ’âŠ¤). (5)\nSince ğœğ‘–(ğ‘ˆâˆ’1)= 1/ğœğ‘›âˆ’ğ‘–+1 (ğ‘ˆ)and ğœğ‘–(ğ‘‰âˆ’âŠ¤)= 1/ğœğ‘‘âˆ’ğ‘–+1 (ğ‘‰)with\nğ‘– âˆˆ[1,ğ‘Ÿ], we transform the right-hand side of Eq. (5) to\nğ‘Ÿâˆ‘ï¸\nğ‘–=1\nğœ2\nğ‘– (Î”)\nğœ2\nğ‘›âˆ’ğ‘–+1 (ğ‘ˆ)ğœ2\nğ‘‘âˆ’ğ‘–+1 (ğ‘‰)â‰¤\nÃğ‘Ÿ\nğ‘–=1 ğœ2\nğ‘– (Î”)\nğœ2ğ‘›(ğ‘ˆ)ğœ2\nğ‘‘(ğ‘‰)=\n||Î”||2\nğ¹\nğœ2ğ‘›(ğ‘ˆ)ğœ2\nğ‘‘(ğ‘‰),\nwhere the inequality follows from Theorem 8, i.e., ğœ1 (Â·)â‰¥Â·Â·Â·â‰¥\nğœğ‘Ÿ(Â·), and the last equality is directly from Lemma 5.\nGiven Bâ‰¥|| ğ‘ˆâˆ’1Î”ğ‘‰âˆ’âŠ¤||ğ¹, it suffices to let||Î”||ğ¹/ğœğ‘›(ğ‘ˆ)ğœğ‘‘(ğ‘‰)â‰¤\nBwith Î” = ğ‘“(X)âˆ’ ğ‘“(Xâ€²),âˆ€Xâ‰ƒX â€². Recall that ğ‘†2 (ğ‘“)is the upper\nbound on ||Î”||ğ¹,âˆ€Xâ‰ƒX â€², we now reach the main theorem.\n8Its efficient implementation to extremely high accuracy is supported in most statistical\nand numerical software packages, e.g., Python math library.\nTheorem 6. Our aMGM satisfies (ğœ–,ğ›¿)-DP, iff\nğ‘†2 (ğ‘“)\nB â‰¤ğœğ‘›(ğ‘ˆ)ğœğ‘‘(ğ‘‰),\nwhere B= ğ´(ğœ–,ğ›¿)as in Alg. 1, ğ‘†2 (ğ‘“)is the ğ¿2-sensitivity, ğœğ‘›(ğ‘ˆ)and\nğœğ‘‘(ğ‘‰)are respectively the smallest singular values of ğ‘ˆ and ğ‘‰.\nTheorem 6 only constrains the lower bound on the product of\nğœğ‘›(ğ‘ˆ)and ğœğ‘‘(ğ‘‰), the two smallest singular values; it offers infinite\nchoices for all the others with the design space for(Î£,Î¨)even larger\nthan that of MVG (Theorem 4). More importantly, the lower bound is\nindependent of ğ‘›ğ‘‘, which can lead to orders-of-magnitude variance\nreduction than MVG, confirmed by our experiments in Section 5.\nFor ğœ– â†’0, we can still derive a valid Bfrom 2Î¦âˆ’1 ((1 +ğ›¿)/2).\nTo determine Î£,Î¨, another implicit constraint is to keep smaller\nnoise for better utility. Let us first consider Î£ = ğ‘ˆğ‘ˆâŠ¤. Since it is\npositive definite, we can also decompose it intoğ‘ŠÎ£Î›Î£ğ‘ŠâŠ¤\nÎ£ ; we then\nhave ğ‘ˆ = ğ‘ŠÎ£Î›1/2\nÎ£ , where Î›1/2\nÎ£ = {ğœğ‘–(ğ‘ˆ)}ğ‘›\nğ‘–=1 specifies the row-wise\nnoise magnitudes. Assuming that the smallest overall noise will\nyield the best utility, we let all the singular values be the smallest:\nğœ1 (ğ‘ˆ)= Â·Â·Â· = ğœğ‘›(ğ‘ˆ). As ğ‘ŠÎ£ can be any unitary matrix, we simply\nuse the standard basis, resulting in ğ‘ˆ = ğœğ‘›(ğ‘ˆ)Â· ğ¼ğ‘› for an ğ‘›Ã—ğ‘›\nidentity matrix ğ¼ğ‘› and hence the final Î£. Similarly, we can pick\nÎ¨ = ğ‘‰ğ‘‰âŠ¤with ğ‘‰ = ğœğ‘‘(ğ‘‰)Â·ğ¼ğ‘‘, where ğ¼ğ‘‘ is a ğ‘‘Ã—ğ‘‘ identity matrix.\n4.2.3 Drawing the noise ğ‘. With Î£ and Î¨, the last step is to drawğ‘.\nPragmatically, we adopt the affine transformation below.\nLemma 3 ([ 14]). Let ğ‘â€² âˆˆ Rğ‘›Ã—ğ‘‘ be a random variable with\neach entry i.i.d. drawn from the standard normal distribution. The\ntransformed ğ‘ = ğ‘ˆğ‘â€²ğ‘‰âŠ¤follows MNğ‘›,ğ‘‘(0,ğ‘ˆğ‘ˆ âŠ¤,ğ‘‰ğ‘‰ âŠ¤).\nHence, we can first sampleğ‘›ğ‘‘i.i.d. values fromN(0,1)to formğ‘â€²,\nthen employ the transformation ğ‘ˆğ‘â€²ğ‘‰âŠ¤such that\nğ‘ âˆ¼ğ‘†2 (ğ‘“)\nB MNğ‘›,ğ‘‘(0,ğ¼ğ‘›,ğ¼ğ‘‘). (6)\nDiscussion. When instantiating DP-Forward using aMGM, we set\nğœ1 (ğ‘ˆ)= Â·Â·Â· = ğœğ‘›(ğ‘ˆ)and ğœ1 (ğ‘‰)= Â·Â·Â· = ğœğ‘‘(ğ‘‰)such that the row-\nand column-wise noises are the smallest, and our pilot experiments\nshow this yields optimal task utility; aMGM actually â€œdegeneratesâ€\nto aGM with i.i.d. noise. Nevertheless, aMGM also allows non-i.i.d.\nnoise like MVG: By tuning the corresponding singular values larger,\nwe can add more noise to the rows/columns that negatively impact\nthe utility. It might be helpful when ğ‘“(Â·)(e.g., linear regression on\na small liver dataset [14]) is simple or ğ‘(Â·)does not â€œmix upâ€ noisy\nrows/columns. In contrast to our empirical approach (like MVG),\none could theoretically formulate the allocation of singular values\nas optimization problems that maximize different utility functions\ntailored to applications. It might outperform our uniform treatment\nbut takes more dedicated efforts, which we leave as future work.\n5 EXPERIMENTS\n5.1 Experimental Setup\nWe use three typical datasets/tasks that are widely used in NLP/DP\nliterature [45, 78â€“80] and GLUE benchmark [69]: i) Stanford senti-\nment treebank (SST-2), ii) Internet movie database (IMDb) [47] for\nbinary sentiment classification of single- and multi-sentence movie\nDP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass CCS â€™23, November 26â€“30, 2023, Copenhagen, Denmark.\n16 32 64 128 256\nSeq Length n\n0.55\n0.60\n0.65Accuracy\nEmbedding\n16 32 64 128 256\nSeq Length n\n0.78\n0.80\n0.82\nEncoder 1\n16 32 64 128 256\nSeq Length n\n0.84\n0.86\n0.88\nEncoder 5\n16 32 64 128 256\nSeq Length n\n0.84\n0.86\n0.88\n0.90\n0.92\nEncoder 11\n16 32 64 128 256\nSeq Length n\n0.84\n0.86\n0.88\n0.90\n0.92\nOutput\nFigure 2: SST-2 accuracy when tuning ğ‘›with noise added at different positions ( ğœ– = 16 for SeqLDP)\n0.6\n0.8Accuracy\nEncoder 1\n0.7\n0.8\n0.9\nEncoder 3\n0.7\n0.8\n0.9\nEncoder 5\n0.7\n0.8\n0.9\nEncoder 7\n0.7\n0.8\n0.9\nEncoder 9\n0.7\n0.8\n0.9\nEncoder 11\nMHA Add & Norm (1) FFN Add & Norm (2)\nFigure 3: SST-2 accuracy with noise added to the outputs of different sub-layers in six encoders ( ğœ– = 16 for SeqLDP)\nTable 2: Statistics of the downstream task datasets\nSST-2 [69] IMDb [47] QQP [69]\n#train samples 67,349 25,000 363,846\n#test samples 872 25,000 40,320\nreviews, and iii) Quora question pairs (QQP) for semantic equiva-\nlence test over question pairs on Quora.com. Their test sets do not\nhave any labels; we use the original dev sets as the test sets. Table 2\nsummarizes their characteristics. They all carry privacy risks; e.g.,\nstylistic features of posts may leak the authorâ€™s identity. We use\ntask accuracy (w.r.t. the ground truth labels) as the utility metric.\nBaselines. We instantiate Min DP-Forward by the classical GM,\nMVG [14], and aMGM. If not specified, all the results are based on\naMGM. For MVG, we adopt its unimodal type, applicable to asym-\nmetric functions like pre-noise layers ğ‘“(Â·). Specifically, we make\nthe row-wise noise directional and assign the same precision budget\nto each row, assuming that tokens share the same importance.\nBy default, we report the accuracy of DP-Forward inferences on\ntasks fine-tuned using DP-Forward (with âˆ¼2pp gains compared to\nthe case of â€œDP-Forward fine-tuning + non-private inferenceâ€). We\nalso realize DP-SGD fine-tuning with the latest Opacus [77] but do\nnot add any noise to its inference. Another baseline is non-private\n(in both fine-tuning and inference).\nImplementation. We run experiments on a cluster with Tesla P100\nGPUs. We implement all the mechanisms and baselines in Python.\nWe use a raw BERT checkpointbert-base-uncased [27], available\nin the Huggingface transformers library, for fine-tuning (Section 3.3)\nor further pre-train it over WikiCorpus (Section 3.6).\nFor the hyperparameters used throughout our experiments, we\nset the number of training epochs ğ‘˜ = 3, learning rate ğœ‚ = 2eâˆ’5,\nbatch size ğ‘ = 32, and normalization/clipping factor ğ¶ = 1. We\nkeep others (e.g., no weight decay, no learning rate decay) default\nas literature [69]. The privacy parameter ğ›¿ is fixed as 1eâˆ’5 [78].\nTable 3: SST-2 accuracy with different ğ‘‘ and ğœ– for SeqLDP\nğ‘‘\nğœ– 2 4 8 12 16\n16 0.6801 0.7851 0.8833 0.9232 0.9266\n64 0.6766 0.7752 0.8727 0.9037 0.9209\n128 0.6732 0.7856 0.8807 0.9128 0.9232\n256 0.6835 0.7695 0.8965 0.9186 0.9249\n512 0.6411 0.7626 0.8831 0.9128 0.9243\n768 0.6686 0.7741 0.8739 0.9128 0.9186\n5.2 Configuring Matrix Dimensions\nThe sequence length ğ‘›is variable. While the hidden dimensionality\nğ‘‘ is tied as 768 for BERT-Base, we can resort to two linear maps\nfor â€œmediatingâ€ it (see Section 3.2). Since we normalize embedding\nmatrices of size ğ‘›Ã—ğ‘‘ to have a fixed norm ğ¶, each entryâ€™s signal\nmagnitude relies on (ğ‘›,ğ‘‘). In contrast, the noise variance is the\nsame given ğ¶ and fixed privacy parameters. The signal-to-noise\nratios (SNRs) affecting accuracy can be configured based on (ğ‘›,ğ‘‘).\nFigure 2 shows the evaluation accuracy ofSST-2 fine-tuned using\nDP-Forward with ğ‘›tuning from 16 to 256. We study adding aMGM\nnoise at five hidden layersâ€™ outputs. The results indicate that the best\naccuracy is often achieved at ğ‘›= 64 or 128, so we opt for ğ‘›= 128\n(which is sufficient for most sequences) in subsequent experiments.\nWe fine-tunedSST-2 on noisy output embeddings under different\nchoices ofğœ–and reducedğ‘‘. Table 3 summarizes the results. Reducing\nğ‘‘leads to larger SNRs (under fixedğ¶and ğ‘›) but may also lose useful\ninformation, degrading accuracy. For the same ğœ–, most accuracy\nvariations are within 2pp under different choices of ğ‘‘. Balancing\neverything, we use the raw ğ‘‘ = 768 in later experiments such that\nno extra changes (including two linear maps) are made to pipelines.\n5.3 Fine-tuning with Sequence LDP\nOur approach also supports perturbing sub-layer outputs during\nfine-tuning. We study six encoders as an example, with the results\nCCS â€™23, November 26â€“30, 2023, Copenhagen, Denmark. Du, et al.\nTable 4: Accuracy on output embeddings under SeqLDP\nTask Mech. ğœ– = 0.5 ğœ– = 1 ğœ– = 2 ğœ– = 4 ğœ– = 8\nSST-2\nGM 0.5424 0.5757 0.6537 0.7466 0.8624\naMGM 0.5516 0.6021 0.6686 0.7741 0.8739\nMVG âˆ¼0.5\nIMDb\nGM 0.5244 0.5498 0.6016 0.6902 0.8002\naMGM 0.5353 0.5676 0.6224 0.7093 0.8109\nMVG âˆ¼0.5\nQQP\nGM 0.6304 0.6321 0.6571 0.7458 0.8638\naMGM 0.6312 0.6348 0.6747 0.7685 0.8653\nMVG âˆ¼0.5\nshown in Figure 3. Overall, DP-Forward performs better with deeper\nencoders since fewer parameters are directly affected by noise\nduring fine-tuning. Another observation is that perturbing different\nsub-layer outputs, even inside the same encoder, may result in huge\naccuracy variation; e.g., using noisy outputs of the last sub-layer in\nEncoder 1 can bring âˆ¼20pp gains over those of the first sub-layer.\nWe next evaluate the privacy-accuracy tradeoffs under differentğœ–\nand compare the instantiations using the classical GM, MVG [14],\nand aMGM. Note that we still compute the GM variance as ğœ2 =\n2 ln(1.25/ğ›¿)ğ‘†2\n2 (ğ‘“)/ğœ–2 for empirical evaluation, albeit it cannot ex-\ntend to ğœ– > 1 for a single run to ensure theoretical DP guarantees.\nFor the GM- and aMGM-based instantiations, Table 4 shows all\nthree tasksâ€™ accuracy increases with ğœ–. Ours has better accuracy\nthan the GM-based one due to the smaller noise produced by aMGM\nin all choices of ğœ–. Although the noise variance gap (between GM\nand aMGM) widens as ğœ– decreases, one cannot fine-tune effective\nmodels in a high privacy regimeğœ– < 1. The MVG-based one behaves\nlike random guessing for all three tasks since its noise variance is\nproportional to ğ‘›Â·ğ‘‘, which is even much larger than the classical\nGM for high-dimensional settings (see Section 4.1). For instance,\nunder the same parameter setting (e.g., ğ‘›= 128,ğ‘‘ = 768, and ğœ– = 8),\nMVG produces noise with the variance orders-of-magnitude larger\nthan aMGM (e.g., >108 vs. âˆ¼0.6), even assuming sup ||ğ‘“(Â·)||ğ¹ = 1.\nWe remark that the used localğœ– value is not large. Most classi-\ncal LDP works that deem such ğœ– lies in a low privacy regime are\nfor statistical analytics. In great contrast, we aim at fine-tuning\nlarge LM-based pipelines with high-dimensional signals and lim-\nited training data, which is much more complicated. Many prior\nworks [28, 29, 58, 80] use a larger ğœ– to ensure even a weaker token-\nlevel LDP variant, while others [51] categorizesğœ– < 10 and 10 â‰¤ğœ– <\n20 as strong and moderate privacy respectively9 for sequence-level\nLDP like ours. More importantly, they provide effective protection\nagainst various privacy threats, as detailed in Section 6.\n5.4 DP-Forward versus DP-SGD\nFairness of comparisons on privacy-accuracy tradeoffs. As elaborated\nin Section 3.4, we can adopt RR [70] to perturb the labels and then\nreport centralğœ–values for DP-Forward, amplified by shuffling using\nthe following parameters, ensuring that comparisons are fair under\n9Such choices can be â€œreducedâ€ to smaller ones under the shuffling model (Section 5.4),\ncf.. U.S. census discloses demographic data at central ğœ– = 11.14 [3].\n(example-level) CDP. For DP-SGD, the subsampling probability is\nğ‘/ğ‘, with ğ‘ = 32 and the dataset size ğ‘; the number of fine-tuning\nsteps is ğ‘‡ = ğ‘˜Â·ğ‘/ğ‘with ğ‘˜ = 3. For DP-Forward, the subsampling\nand non-flipping probabilities are respectively 1/ğ‘ (with ğ‘‡ = ğ‘˜Â·ğ‘)\nand 0.9; we still processğ‘noisy embeddings as a batch. For both, we\nuse aGM [7], the degenerated version of aMGM (Section 4.2), and\nthe same accountant [32] to report approximated overall ğœ–values10.\nWe study eight instances of DP-Forward, including perturbing\nthe outputs of the input embedding layer, six different encoders, and\nBERT. Their accuracies on all three tasks under three privacy levels,\nplus those of DP-SGD and the non-private baseline, are shown in\nTable 5. About half or more of our instances have better accuracy\nthan DP-SGD for each task; the largest accuracy gain is âˆ¼7.7pp for\nQQP. The noisy output embeddings often lead to the best accuracy\nfor all tasks, even comparable to the non-private baseline, due to\nthe dimension reduction at the last encoder output (Section 2.1).\nRecent DP-SGD variants [78, 79] improve DP-SGD [1] by perturb-\ning partial gradient entries using additional tricks (e.g., low-rank\nadaption). They report the best accuracy of 92.5% and 85.7% on\nSST-2 and QQP, respectively, with 2.3pp and 6.2pp drops from the\nnon-private baselines at central ğœ– = 6.7 [78, Table 4]. DP-Forward\nwith label privacy, incurring <1.7pp accuracy drops on the two\ntasks at ğœ– â‰ˆ3, can still beat them, albeit their fine-tuning is based\non RoBERTa-base, a robustly optimized BERT approach, which by\nitself outperforms BERT due to larger training set, longer training\ntime, and better techniques (e.g., dynamic masking in MLM).\nFigure 4 shows the efficiency comparisons on fine-tuning SST-2.\nThe time and storage overheads of our approach (for all possible\ninstances) are almost the same as the non-private baseline andâˆ¼3Ã—\nsmaller than DP-SGD. It is because we allow batch processing as in\nthe normal fine-tuning â€“ no need to handle per-example gradients.\nMeanwhile, our normalization and noise sampling/addition are also\nfaster since the size of embeddings is smaller than that of gradients.\n5.5 Noisy Pre-training\nPre-training BERT using DP-Forward, aligned with the noisy fine-\ntuning, does help accuracy. We useSST-2 as an example and perturb\nthe input embedding matrices. We continue pre-training BERT over\nEnglish WikiCorpus, the 2006 dump with about 600M words, for an\nepoch. Table 6 shows that we can obtain 1âˆ’2pp accuracy gains for\nmost choices of ğœ–, compared to fine-tuning on the original BERT.\nEfficiency-wise, DP-Forward pre-training also consumes much\nfewer resources; e.g., an existing work [5] pre-trains BERT-Large\n(with 340 million parameters) using DP-SGD on Google TPUs, which\nrequires sufficient memory for handling batch sizes of millions.\n6 DEFENSE AGAINST PRIVACY THREATS\nFollowing the recent taxonomy [64], we study MIAs and two new\nthreats of sequence leakage from their embeddings: embedding\ninversion and attribute inference. We moderately adapt them to\nsuit our context, e.g., upgrading MIAs [64] to sequence-level.\n10They are dominated by composing subsampled Gaussian,e.g., composing subsampled\nRR only consumes 0.03 for SST-2, which is even overestimated by AutoDP.\nDP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass CCS â€™23, November 26â€“30, 2023, Copenhagen, Denmark.\nTable 5: Accuracy of task models fine-tuned using DP-Forward and DP-SGD under (example-level) CDP\nMethod Noise\nposition\nSST2 IMDb QQP\nğœ– â‰ˆ1 ğœ– â‰ˆ3 ğœ– â‰ˆ8 ğœ– â‰ˆ1 ğœ– â‰ˆ3 ğœ– â‰ˆ8 ğœ– â‰ˆ1 ğœ– â‰ˆ3 ğœ– â‰ˆ8\nDP-Forward\nEmbedding 0.6055 0 .6146 0 .6278 0.5 0 .5 0 .5 0.6534 0 .6589 0 .6594\nEncoder 1 0.7971 0 .8096 0 .8073 0.5000 0 .5016 0 .5022 0.7857 0 .7885 0 .7906\nEncoder 3 0.8096 0 .8394 0 .8463 0.7525 0 .7545 0 .7549 0.8513 0 .8585 0 .8607\nEncoder 5 0.8544 0 .8658 0 .8716 0.7642 0 .7709 0 .7719 0.8698 0 .8765 0 .8806\nEncoder 7 0.8624 0.8819 0 .8872 0.7765 0.7883 0 .7924 0.8840 0 .8887 0 .8926\nEncoder 9 0.8945 0 .8979 0 .9002 0.7995 0 .8105 0 .8181 0.8895 0 .8941 0 .8955\nEncoder 11 0.8819 0 .8968 0 .8985 0.8042 0 .8187 0 .8265 0.8952 0 .8997 0 .9007\nOutput 0.8865 0 .9009 0 .9055 0.8096 0 .8160 0 .8270 0.8987 0 .8994 0 .9038\nDP-SGD Gradient 0.8650 0 .8713 0 .8759 0.7779 0 .7826 0 .7903 0.8219 0 .8345 0 .8433\nNon-private baseline 0.9178 0.8378 0.9019\n0\n10\n20Minute\nTrain time per epoch\n0\n250\n500\n750MB (GPU)\nMemory per example\nNon-Private DP-Forward DP-SGD\nFigure 4: Efficiency comparison for the case of SST-2\nTable 6: SST-2 accuracy gain with pre-training under SeqLDP\nğœ– Raw BERT Noisy BERT Î”acc\n2 0.5501 0.5665 0.0164\n4 0.5950 0.5999 0.0049\n8 0.6550 0.6708 0.0158\n16 0.7345 0.7450 0.0105\n6.1 Threat Models\nFor MIAs, we follow prior arts [ 63, 64, 76] to consider an adver-\nsary with only black-box access to an entire (DP-SGD/DP-Forward-\ntrained) pipeline: It can query the prediction results (e.g., each-class\nprobability) of target sequences but cannot access the pipeline\nweights and architecture; the hidden embeddings are not revealed.\nDespite â€œdifferentâ€ objectives of inverting or inferring (Section 6.3\nor 6.4) from embeddings, we consider both threats involving a gen-\neral adversary withblack-box access to the trained pipeline partğ‘“().\nIt can get the inference-time (clear/noisy) embeddings of target se-\nquences [64]. Besides public prior knowledge, it can collect alimited\nauxiliary dataset Xaux, sharing similar attributes to the targets [64].\nDP-SGD only offers CDP for training data and does not protect\ninference-time input.11 What follows intends to empirically con-\nfirm a major merit of DP-Forward in protecting against stronger\nadversaries and threats to both training- and inference-time inputs.\n11One might add the same noise to it as DP-Forward inference, which indeed mitigates\nthe new threats. However, perturbing gradients in training, inherently â€œmismatchesâ€\nfrom perturbing embeddings in inference, deteriorating task performance significantly,\ne.g., SST-2 accuracy will be reduced to 0.7786 (with a âˆ¼10pp drop) at central ğœ– â‰ˆ8.\n6.2 Membership Inference Attacks\nAttack Objective. MIAs predict whether a data point is in the train-\ning set [63]. They often exploit the disparity in model behavior be-\ntween training data and unseen data,i.e., poor model generalization\ndue to overfitting [76]. Inferring membership at the token/word\nlevel, e.g., a sliding window of tokens [64], is not interesting. We\nconsider more realistic MIAs on entire sequences, which can be\nextended for more devastating attacks, such as extracting verbatim\npre-training sequences via black-box access to GPT-2 [13].\nPrior arts [65, 76] suggest that threshold-based MIAs using only\nprediction confidence [76] or entropy [65] with proper assumptions\nare comparable to the more sophisticated one [63] based on shadow\ntraining. Adapting the confidence-based MIA to our context exploits\nthat a pipeline is fine-tuned by minimizing its prediction loss: The\nconfidence/probability of predicting a training sequence as its true\nlabel should be close to 1. The adversary can then infer a candidate\nsequence ğ‘‹âˆ—as a member when the confidence for the predicted\nlabel ğ‘™ output by pipeline Fis larger than a pre-set threshold ğœ:\n1{Pr[F(ğ‘‹âˆ—)= ğ‘™]â‰¥ ğœ},\nwhere 1{Â·}is the indicator function. We simply use a fixedğœ for all\npossible labels in our evaluation, albeit it can be label-dependent.\nThe second MIA we use is based on the prediction output (i.e., a\nvector of probabilities) of a training sequence tends to be a one-hot\nvector, i.e., its entropy should be close to 0. Similarly, the adversary\ncan infer ğ‘‹âˆ—as a member when its prediction entropy falls below a\npreset threshold ğœ; otherwise, it is not deemed a member:\n1{âˆ’\nâˆ‘ï¸\nğ‘–\nPr[F(ğ‘‹âˆ—)= ğ‘™ğ‘–]log(Pr[F(ğ‘‹âˆ—)= ğ‘™ğ‘–])â‰¤ ğœ},\nfor all possible labels {ğ‘™ğ‘–}. Note that a totally wrong prediction with\nprobability âˆ¼1 also leads to entropy approaching 0. We can address\nit by encoding the information of the ground-truth label of ğ‘‹âˆ—[65].\nNumerical Results. As in [79], all the test examples and a random\nsubset of the training examples (as many as the test ones) are evenly\nsplit into two subsets (each has half of the training/test examples),\none for finding the optimal ğœ, and the other for reporting the attack\nsuccess rates. Given that the training and test examples likely share\nthe same distribution, we randomly drop/replace tokens in the test\nexamples to enlarge the prediction difference to make MIAs easier.\nCCS â€™23, November 26â€“30, 2023, Copenhagen, Denmark. Du, et al.\nTable 7: Success rates of two MIAs with (translated) central ğœ–\nğœ– Method Attack Success Rate\nEntropy Confidence\nâˆ Non-private baseline 0.659 0 .645\n1\nDP-SGD 0.567 0 .561\nDP-Forward (Embedding) 0.586 0 .576\nDP-Forward (Encoder 1) 0.535 0 .537\nDP-Forward (Encoder 7) 0.494 0 .506\nDP-Forward (Encoder 11) 0.506 0 .494\nDP-Forward (Output) 0.508 0 .502\n3\nDP-SGD 0.584 0 .576\nDP-Forward (Embedding) 0.584 0 .576\nDP-Forward (Encoder 1) 0.543 0 .530\nDP-Forward (Encoder 7) 0.510 0 .507\nDP-Forward (Encoder 11) 0.512 0.500\nDP-Forward (Output) 0.503 0 .499\n8\nDP-SGD 0.580 0 .580\nDP-Forward (Embedding) 0.597 0 .576\nDP-Forward (Encoder 1) 0.510 0.536\nDP-Forward (Encoder 7) 0.510 0 .504\nDP-Forward (Encoder 11) 0.520 0.513\nDP-Forward (Output) 0.506 0 .490\nWe evaluated the adapted confidence- and entropy-based MIAs\non SST-2 fine-tuned by the non-private baseline, DP-Forward, and\nDP-SGD. For DP-Forward, we investigate five instances, perturbing\ninput embeddings, three encodersâ€™ outputs, and output embeddings.\nTable 7 presents the results, where success rates within 0.49â€“0.51\nare shown in bold. Both DP-Forward and DP-SGD can mitigate\nMIAs effectively. For all choices ofğœ–, the two MIAsâ€™ success rates on\nDP-Forward are reduced to âˆ¼0.5 (like random guessing) for deeper\nlayers, outperforming DP-SGD by >6pp at the same privacy level.\n6.3 Embedding Inversion Attacks\nAttack Objective. These attacks aim at recovering the raw text as\n(unordered) tokens {ğ‘¥ğ‘–}ğ‘–âˆˆ[ğ‘›]âŠ†ğ‘‹ from embeddings, highlighting\nthe risk of directly sharing (without noise) even only text embed-\ndings (for training/inference). They have been employed to recon-\nstruct specific patterns, e.g., identity codes and gene segments [56].\nWe first propose a simple token-wise inversion attack to invert\n(noisy) token embeddings output by the input embedding layerğœ™(Â·)\nthat maps every token in Vto Rğ‘‘ [73]. It can be formulated as:\nâˆ€ğ‘– âˆˆ[ğ‘›]: min\nğ‘¥âˆ—\nğ‘– âˆˆV\n||ğœ™(ğ‘¥âˆ—\nğ‘–)âˆ’( ğœ™(ğ‘¥ğ‘–)+ğ‘§ğ‘–)||2,\nwhere ğ‘§ğ‘– is the ğ‘–th row of noise ğ‘ from M(omitted for DP-SGD or\nthe non-private baseline). It returns ğ‘¥âˆ—\nğ‘– with its embedding closest\nto the observed one of ğ‘¥ğ‘– via a nearest-neighbor search over V.\nA tokenâ€™s hidden embedding from deeper layers encodes more\nâ€œabstractâ€ contextual information of the entire sequence it belongs\nto; the token-wise inversion may be less accurate. We thus require\na more general attack [64]. It first maps the observed (noisy) embed-\nding back to a lower-layer one using a linear least square model ğ‘€\nand then selectsğ‘›tokens asğ‘‹âˆ—to minimize theğ¿2-distance between\nTable 8: Success rates of two inversion attacks on (the lowest-\nlayer) input embeddings with (translated) central ğœ– â‰ˆ8\nNearest Neighbor Gradient-based\nSST-2 IMDb QQP SST-2 IMDb QQP\nNon-private 1 1 1 1 1 1\nDP-SGD 1 1 1 .9991 .9982 1\nDP-Forward .1811 .1420 .2457 .1622 .1241 .2226\nTable 9: Success rates of a (neural-network-based) sensitive\nattribute inference attack with (translated) central ğœ– â‰ˆ8\naction comedy drama horror Overall\nNon-private 0.727 0.858 0.516 0.439 0.687\nDP-SGD 0.664 0.733 0.253 0.324 0.536\nDP-Forward (Embedding)0.998 0 0 0.009 0.276\nDP-Forward (Encoder 1) 1.0 0 0 0 0.276\nDP-Forward (Encoder 7) 1.0 0 0 0 0.276\nDP-Forward (Encoder 11) 1.0 0 0 0 0.276\nDP-Forward (Output) 0.998 0 0 0.009 0.276\nthe lower-layer representation of ğ‘‹âˆ—and the one from ğ‘€:\nmin\nğ‘‹âˆ—âˆˆVğ‘› ||ğœ(ğ‘‹âˆ—)âˆ’ğ‘€(ğ‘“(ğ‘‹)+ğ‘)||2,\nwhere ğœ(Â·)is a lower-layer representation function than ğ‘“(Â·).\nThe above minimization is over|V|ğ‘›, larger than the token-wise\ncandidate space. To determineğ‘‹âˆ—, we first relax the token selection\nat each position ğ‘– âˆˆ[ğ‘›]using a continuous vector in R|V|, which\nis then input (with another temperature parameter) to a softmax\nfunction to model the probabilities of selecting each token inV. We\nfurther derive the token embedding ğ‘¥âˆ—\nğ‘– by multiplying the relaxed\nvector (with each entry as a weight) and the original embedding\nmatrix. Finally, we solve it by a gradient-based method [64].\nNumerical Results. The gradient-based attack reports the highest\nrecall (or precision) on inverting the lowest-layer (clear) embed-\ndings [64, Figure 2]. To show that DP-Forward can mitigate such\nâ€œstrongestâ€ inversion, we implement both (nearest-neighbor and\ngradient-based) attacks to invert input embeddings, with the public\nBERT embedding lookup table as prior. We also report their success\nrates as recall â€“ the ratios of correct recoveries over the raw targets.\nTable 8 shows that DP-Forward can reduce their success rates to\na relatively low level, most are within 0.2. However, DP-SGD fails\nin defense. The results corroborate our claim: DP-Forward directly\nadds noise to embeddings, thus mitigating embedding inversion,\nwhereas DP-SGD only perturbs gradients, offering no protection\nfor the (clear) inference-time embeddings of test sequences.\n6.4 Sensitive Attribute Inference Attacks\nAttack Objective. Instead of recovering exact tokens, one can try\nto infer sensitive attributes about target sequences from their em-\nbeddings. The attributes are often statistically unrelated to the train-\ning/inference objective but inherent in sequences, e.g., stylometry,\nimplying the textâ€™s authorship for sentiment analysis [62]. We are\nnot interested in any global property of an entire corpus [30].\nDP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass CCS â€™23, November 26â€“30, 2023, Copenhagen, Denmark.\nWe assume Xaux has sequences labeled with sensitive attributes.\nThe adversary can query ğ‘“(Â·)for the noisy (or clear) embeddings:\nXaux = {(ğ‘“(ğ‘‹ğ‘–)+ğ‘ğ‘–,ğ‘ ğ‘–)}, âˆ€ğ‘ ğ‘– âˆˆS,\nwhere Sis the set of all possible sensitive attributes of interest, say,\nauthorship. It does not care about non-sensitive attributes.\nTo infer sensitive attributes, the adversary first trains a classifier\non Xaux via supervised learning and then uses it for an observed\nnoisy (or clear) embedding ğ‘“(ğ‘‹âˆ—)+ğ‘ to predict ğ‘ âˆ—âˆˆS of ğ‘‹âˆ—.\nNumerical Results. We train a three-layer neural network with a\nlinear head as the classifier to infer the film genre (e.g., â€˜horrorâ€™) as a\nsensitive attribute from a movie review using its output embedding.\nWe employ IMDb with 20k examples (90% for training and 10% for\nvalidation) as Xaux, and SST-2 contributes 3.3k examples for testing\nthe classifier. The attack success rates are measured using recall.\nWe investigate five DP-Forward instances. Table 9 shows that\nthey â€œreduceâ€ the classifier to majority-class prediction, which re-\nturns the majority class (â€˜actionâ€™) on all inputs. In contrast, DP-SGD\nonly reduces success rates moderately compared to the non-private\nbaseline. It is because the embeddings from DP-SGD-trained/noisy\nmodels still â€œloseâ€ some useful information (cf., accuracy drops of\nDP-SGD inference on embeddings without noise). The results con-\nfirm DP-Forward is more effective in thwarting attribute inference.\n7 RELATED WORK\n7.1 Privacy Threats on LMs and Embeddings\nAn active line of research [8, 13, 56, 64] discloses severe privacy risks\nin modern LMs (even used as black-box query â€œoraclesâ€) concerning\ntheir (hidden/output) text embeddings. Song and Raghunathan [64]\nbuild a taxonomy of attacks that covers a broader scope than a par-\nallel work [56]. These attacks include embedding inversion (which\ncan partially recover raw texts), membership inference (establishing\nthe is-in relation between a target and private training data), and\ninferring sensitive attributes like text authorship from embeddings.\nA common defense for them is adversarial training, e.g., [24].\nOthers [8, 13] study the â€œmemorizationâ€ of training data in LMs\n(a.k.a. membership inference attack). In particular, Carliniet al. [13]\ndefine ğ‘˜-eidetic memorization, where a string is extractable or mem-\norized if it appears in at most ğ‘˜ examples. Their black-box attacks\non GPT-2 [59] can extract verbatim training texts even when ğ‘˜ = 1\n(e.g., a name that only appears once is still extractable). A smaller ğ‘˜\nmeans a higher privacy risk. Beguelin et al. [8] define differen-\ntial score and rank as two new metrics for analyzing the update\nleakage, enabling the recovery of new text used to update LMs.\nIncorporating DP to address memorization is a promising solution.\n7.2 Input (Text/Feature) Perturbation for LDP\nSynTF [71] synthesizes term-frequency (feature) vectors under LDP,\nwhich have limited applications compared to sentence embeddings\nor text itself. Feyisetan et al. [28, 29] resort to metric-LDP [ 4], a\nrelaxed variant of LDP with a distance metric ( e.g., Euclidean or\nHyperbolic), which allows the indistinguishability of outputs to\ngrow proportionally to the inputsâ€™ distance. They first add noise\nto the outputs of a non-contextualized token embedding model\n(e.g., GLoVe [57]), which are then projected back to â€œsanitizedâ€ text\nusing the nearest neighbor search as post-processing. In contrast,\nYue et al. [80] sanitize text by directly sampling token-wise replace-\nments, avoiding adding noise to high-dimensional embeddings. All\nthese works only achieve (variants of) token-level metric-LDP.\nTo offer sequence-level protection, recent studies [46, 51] apply\nLaplace or exponential mechanism to perturb (the average of) sen-\ntence embeddings extracted by an LM (e.g., BERT [20]). Both ensure\npure LDP (homogeneously protecting any entire sequence), which\nmay be too stringent and impact utility. In contrast, heterogeneous\nprotection [28, 80] can strategically manage the privacy demands\nacross inputs. Du et al. [21] achieve metric-LDP (by Purkayastha\nand planar Laplace mechanisms) at the sequence level (unlike token-\nlevel in prior arts [28, 80]). To further boost the utility, they mitigate\nthe dimensional curse via a random-projection-like approach. They\nalso perturb sensitive sequence labels for enhanced privacy. Never-\ntheless, perturbing different hidden (rather than token or sentence)\nembeddings inside LM-based NLP pipelines remains unexplored.\n7.3 DP-SGD (Variants) in Training LMs\nAn early attempt [50] uses DP-SGD to train long short-term mem-\nory LMs in the federated learning setting. By configuring hyperpa-\nrameters properly (e.g., setting the batch size to millions), one can\neven pre-train BERT-Large, an LM with âˆ¼340M parameters, using\nDP-SGD/Adam while achieving acceptable (MLM) accuracy [5].\nUsing the vanilla DP-SGD in pre-training/fine-tuning large LMs\nleads to significant efficiency and accuracy drops due to the â€œcurse\nof dimensionality. â€ Yuet al. [79] propose reparametrized gradient\nperturbation: It first reparameterizes/decomposes each high-rank\nweight matrix into two low-rank (gradient-carrier) ones with a\nresidual matrix and then only perturbs the two low-rank gradients\nto alleviate the dimensional curse. The noisy low-rank gradients\nare finally projected back to update the raw high-rank weights.\nApplying reparameterization to every weight in each update is\nstill costly and may introduce instability (e.g., noises are â€œzoomed upâ€\nduring the projection). Instead, the follow-up [78] builds atop the\nrecent success of parameter-efficient fine-tuning (e.g., LoRA [36],\nAdapter [35], and Compacter [48]): It perturbs the gradients of a\nmuch smaller number of additional â€œplug-inâ€ parameters. However,\nLi et al. [45] empirically show that parameter-efficient fine-tuning is\nnot necessarily better than the full one; they propose ghost clipping,\na memory-saving technique (â€œorthogonalâ€ to dimension reduction),\nto use DP-SGD in full fine-tuning without instantiating per-example\ngradients. Despite efficiency/accuracy gains, all these works still\nonly protect training data by perturbing (smaller) gradients.\n7.4 DP Mechanisms for Matrix Functions\nGaussian and Laplace mechanisms are typically for scalar-/vector-\nvalued functions [23]. Vectorizing the outputs and adding i.i.d. noise\ncould generalize them for matrix-valued functions, but the struc-\ntural information of matrix functions is not exploited. The MVG\nmechanism [14] is thus devised, which draws directional or non-\ni.i.d. noise from a matrix Gaussian distribution. It injects less noise\ninto more â€œinformativeâ€ output directions for better utility, with\nonly a constraint on the sum of the singular values (determining\nthe noise magnitude) of two covariance matrices. Such a constraint\nis only a sufficient condition for (ğœ–,ğ›¿)-DP, which is improved by\nthe follow-up [75] with a tighter bound on the singular values.\nCCS â€™23, November 26â€“30, 2023, Copenhagen, Denmark. Du, et al.\nThere also exist mechanisms dedicated torestricted matrix-valued\nfunctions. The matrix mechanism [43] considers a collection of lin-\near counting queries represented byğ‘Šğ‘¥ for query matrix ğ‘Š and\ninput vector ğ‘¥. It still resorts to additive Laplace/Gaussian noise\nbut with an extra transformation solving the min-variance estima-\ntion to the noisy ğ‘Šğ‘¥. Another very recent study [37] focuses on\nmatrix-valued queries with only binary (matrix) outputs. It then\ndevises an exclusive-or (xor) mechanism xor-ing the outputs with\nnoise attributed to a matrix-valued Bernoulli distribution.\n8 CONCLUSION\nPre-trained LMs became pivotal in NLP. Alarmingly, fine-tuning\ncorpora or inference-time inputs face various privacy attacks. The\npopular DP-SGD only provides limited protection for training data\nby adding noise to gradients. Raw tokens or sensitive attributes of\ntraining/inference data can be inverted or inferred from embeddings\nin forward-pass computation. Vanilla DP-SGD also imposes high\nGPU memory and computational burdens but cannot be batched.\nWe propose DP-Forward, which directly adds noise to embed-\nding matrices derived from the raw training/inference data in the\nforward pass. Its core is the analytic matrix Gaussian mechanism,\na general-purpose tool that owns independent interests. It draws\noptimal matrix-valued noise from a matrix Gaussian distribution in\na dedicated way using a necessary and sufficient condition for DP.\nPerturbing embeddings at various positions across multiple lay-\ners yields at least two benefits. DP-Forward users are only required\nto download pipeline parts for deriving noisy embeddings, which\nis more storage- and time-efficient than deriving noisy gradients.\nTogether with our prior attempts [21, 80] at sanitizing input text\ntokens and output sentence embeddings, we provide a full suite of\nforward-pass signal sanitization options for users only to share their\nsanitized data for LM-as-a-Service APIs while protecting privacy.\nBeyond the theoretical contribution of two local DP notions\nand the experimental comparisons with baselines (e.g., GM, MVG,\nand DP-SGD) across three typical NLP tasks, we investigate the\nhyperparameter configuration for reproducible validations of DP-\nForwardâ€™s potential in terms of efficiency, accuracy, and its ability\nto withstand diverse against diverse attacks.\nAltogether, our new perspective leads to a better approach to\nprivacy-aware deep neural network training, challenging the tradi-\ntional wisdom focusing on gradients. As a new paradigm for local\nDP in fine-tuning and inference, our work paves the way for a myr-\niad of possibilities for new machine-learning privacy research [54],\ne.g., generalization to transformer-based computer vision tasks.\nACKNOWLEDGMENTS\nWe are grateful to the anonymous reviewers for their comments and\nAshwin Machanavajjhala for his comments on a related Ph.D. thesis.\nSherman Chow is supported in part by the General Research Funds\n(CUHK 14209918, 14210319, 14210621), Research Grants Council,\nUniversity Grants Committee, Hong Kong. Authors at OSU are\nsponsored in part by NSF IIS #1815674, NSF CAREER #1942980,\nand Ohio Supercomputer Center [55]. Tianhao Wang is supported\nin part by National Science Foundation (NSF) with grants CNS-\n2220433 and OAC-2319988.\nREFERENCES\n[1] MartÃ­n Abadi, Andy Chu, Ian J. Goodfellow, H. Brendan McMahan, Ilya Mironov,\nKunal Talwar, and Li Zhang. 2016. Deep Learning with Differential Privacy. In\nCCS. 308â€“318.\n[2] Prince Osei Aboagye, Yan Zheng, Chin-Chia Michael Yeh, Junpeng Wang, Wei\nZhang, Liang Wang, Hao Yang, and Jeff M. Phillips. 2022. Normalization of\nLanguage Embeddings for Cross-Lingual Alignment. In ICLR (Poster). 32 pages.\n[3] John Abowd, Robert Ashmead, Ryan Cumings-Menon, Simson Garfinkel, Micah\nHeineck, Christine Heiss, Robert Johns, Daniel Kifer, Philip Leclerc, Ashwin\nMachanavajjhala, Brett Moran, William Sexton, Matthew Spence, and Pavel Zhu-\nravlev. 2022. The 2020 Census Disclosure Avoidance System TopDown Algorithm.\nHarvard Data Science Review Special Issue 2: Differential Privacy for the 2020 U.S.\nCensus (Jun 24 2022), 77 pages. https://hdsr.mitpress.mit.edu/pub/7evz361i.\n[4] MÃ¡rio S. Alvim, Konstantinos Chatzikokolakis, Catuscia Palamidessi, and Anna\nPazii. 2018. Invited Paper: Local Differential Privacy on Metric Spaces: Optimizing\nthe Trade-Off with Utility. In CSF. 262â€“267.\n[5] Rohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, and Pasin Manurangsi. 2022.\nLarge-Scale Differentially Private BERT. In Findings of EMNLP . 6481â€“6491.\n[6] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normaliza-\ntion. arXiv:1607.06450.\n[7] Borja Balle and Yu-Xiang Wang. 2018. Improving the Gaussian Mechanism for\nDifferential Privacy: Analytical Calibration and Optimal Denoising. In ICML.\n403â€“412.\n[8] Santiago Zanella BÃ©guelin, Lukas Wutschitz, Shruti Tople, Victor RÃ¼hle, Andrew\nPaverd, Olga Ohrimenko, Boris KÃ¶pf, and Marc Brockschmidt. 2020. Analyzing\nInformation Leakage of Updates to Natural Language Models. In CCS. 363â€“375.\n[9] Kallista A. Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Bren-\ndan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. 2017.\nPractical Secure Aggregation for Privacy-Preserving Machine Learning. In CCS.\n1175â€“1191.\n[10] Zhiqi Bu, Jinshuo Dong, Qi Long, and Weijie J. Su. 2019. Deep Learning with\nGaussian Differential Privacy. arXiv:1911.11607.\n[11] Robert Istvan Busa-Fekete, Andres Munoz Medina, Umar Syed, and Sergei Vassil-\nvitskii. 2021. On the pitfalls of label differential privacy. In NeurIPS Workshop.\n6 pages.\n[12] Nicholas Carlini, Chang Liu, Ãšlfar Erlingsson, Jernej Kos, and Dawn Song. 2019.\nThe Secret Sharer: Evaluating and Testing Unintended Memorization in Neural\nNetworks. In USENIX Security . 267â€“284.\n[13] Nicholas Carlini, Florian TramÃ¨r, Eric Wallace, Matthew Jagielski, Ariel Herbert-\nVoss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Song, Ãšlfar Erlingsson,\nAlina Oprea, and Colin Raffel. 2021. Extracting Training Data from Large Lan-\nguage Models. In USENIX Security . 2633â€“2650.\n[14] Thee Chanyaswad, Alex Dytso, H. Vincent Poor, and Prateek Mittal. 2018. MVG\nMechanism: Differential Privacy under Matrix-Valued Query. InCCS. 230â€“246.\n[15] Melissa Chase and Sherman S. M. Chow. 2009. Improving privacy and security\nin multi-authority attribute-based encryption. In CCS. 121â€“130.\n[16] Kamalika Chaudhuri and Daniel J. Hsu. 2011. Sample Complexity Bounds for\nDifferentially Private Learning. In COLT. 155â€“186.\n[17] Kamalika Chaudhuri and Claire Monteleoni. 2008. Privacy-preserving logistic\nregression. In NIPS. 289â€“296.\n[18] Kamalika Chaudhuri, Claire Monteleoni, and Anand D. Sarwate. 2011. Differ-\nentially Private Empirical Risk Minimization. J. Mach. Learn. Res. 12 (2011),\n1069â€“1109.\n[19] George Dasoulas, Kevin Scaman, and Aladin Virmaux. 2021. Lipschitz normal-\nization for self-attention layers with application to graph neural networks. In\nICML, Vol. 139. 2456â€“2466.\n[20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\nNAACL-HLT. 4171â€“4186.\n[21] Minxin Du, Xiang Yue, Sherman S. M. Chow, and Huan Sun. 2023. Sanitizing\nSentence Embeddings (and Labels) for Local Differential Privacy. InWWW. 2349â€“\n2359.\n[22] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. 2006. Cali-\nbrating Noise to Sensitivity in Private Data Analysis. In TCC. 265â€“284.\n[23] Cynthia Dwork and Aaron Roth. 2014. The Algorithmic Foundations of Differen-\ntial Privacy. Found. Trends Theor. Comput. Sci. 9, 3-4 (2014), 211â€“407.\n[24] Yanai Elazar and Yoav Goldberg. 2018. Adversarial Removal of Demographic\nAttributes from Text Data. In EMNLP. 11â€“21.\n[25] Ãšlfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Kunal\nTalwar, and Abhradeep Thakurta. 2019. Amplification by Shuffling: From Local\nto Central Differential Privacy via Anonymity. In SODA. 2468â€“2479.\n[26] Mani Malek Esmaeili, Ilya Mironov, Karthik Prasad, Igor Shilov, and Florian\nTramÃ¨r. 2021. Antipodes of Label Differential Privacy: PATE and ALIBI. In\nNeurIPS. 6934â€“6945.\n[27] Hugging Face. 2023. BERT base model (uncased). https://huggingface.co/bert-\nbase-uncased, last accessed: September 12, 2023.\nDP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass CCS â€™23, November 26â€“30, 2023, Copenhagen, Denmark.\n[28] Oluwaseyi Feyisetan, Borja Balle, Thomas Drake, and Tom Diethe. 2020. Privacy-\nand Utility-Preserving Textual Analysis via Calibrated Multivariate Perturbations.\nIn WSDM. 178â€“186.\n[29] Oluwaseyi Feyisetan, Tom Diethe, and Thomas Drake. 2019. Leveraging Hier-\narchical Representations for Preserving Privacy and Utility in Text. In ICDM.\n210â€“219.\n[30] Karan Ganju, Qi Wang, Wei Yang, Carl A. Gunter, and Nikita Borisov. 2018. Prop-\nerty Inference Attacks on Fully Connected Neural Networks using Permutation\nInvariant Representations. In CCS. 619â€“633.\n[31] Badih Ghazi, Noah Golowich, Ravi Kumar, Pasin Manurangsi, and Chiyuan Zhang.\n2021. Deep Learning with Label Differential Privacy. In NeurIPS. 27131â€“27145.\n[32] Sivakanth Gopi, Yin Tat Lee, and Lukas Wutschitz. 2021. Numerical Composition\nof Differential Privacy. In NeurIPS. 11631â€“11642.\n[33] Robert J. Hall, Larry A. Wasserman, and Alessandro Rinaldo. 2013. Random\nDifferential Privacy. J. Priv. Confidentiality 4, 2 (2013), 43â€“59.\n[34] Roger A. Horn and Charles R. Johnson. 2012. Matrix Analysis, 2nd Ed . Cambridge\nUniversity Press, N/A.\n[35] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin\nde Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-Efficient Transfer Learning for NLP. In ICML, Vol. 97. 2790â€“2799.\n[36] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large\nLanguage Models. In ICLR (Poster). 13 pages.\n[37] Tianxi Ji, Pan Li, Emre Yilmaz, Erman Ayday, Yanfang Ye, and Jinyuan Sun.\n2021. Differentially Private Binary- and Matrix-Valued Data Query: An XOR\nMechanism. Proc. VLDB Endow. 14, 5 (2021), 849â€“862.\n[38] Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhod-\nnikova, and Adam D. Smith. 2008. What Can We Learn Privately?. In FOCS.\n531â€“540.\n[39] Hyunjik Kim, George Papamakarios, and Andriy Mnih. 2021. The Lipschitz\nConstant of Self-Attention. In ICML, Vol. 139. 5562â€“5571.\n[40] Beatrice Laurent and Pascal Massart. 2000. Adaptive estimation of a quadratic\nfunctional by model selection. The Annals of Statistics 28, 5 (2000), 1302â€“1338.\n[41] Mathias LÃ©cuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman\nJana. 2019. Certified Robustness to Adversarial Examples with Differential Privacy.\nIn S&P. 656â€“672.\n[42] Eric Lehman, Sarthak Jain, Karl Pichotta, Yoav Goldberg, and Byron C. Wallace.\n2021. Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?. InNAACL-\nHLT. 946â€“959.\n[43] Chao Li, Gerome Miklau, Michael Hay, Andrew McGregor, and Vibhor Ras-\ntogi. 2015. The matrix mechanism: optimizing linear counting queries under\ndifferential privacy. VLDB J. 24, 6 (2015), 757â€“781.\n[44] Jing Li, Aixin Sun, Jianglei Han, and Chenliang Li. 2022. A Survey on Deep\nLearning for Named Entity Recognition. IEEE Trans. Knowl. Data Eng. 34, 1\n(2022), 50â€“70.\n[45] Xuechen Li, Florian TramÃ¨r, Percy Liang, and Tatsunori Hashimoto. 2022.\nLarge Language Models Can Be Strong Differentially Private Learners. In ICLR.\n30 pages.\n[46] Lingjuan Lyu, Xuanli He, and Yitong Li. 2020. Differentially Private Representa-\ntion for NLP: Formal Guarantee and An Empirical Study on Privacy and Fairness.\nIn Findings of EMNLP . 2355â€“2365.\n[47] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng,\nand Christopher Potts. 2011. Learning Word Vectors for Sentiment Analysis. In\nACL. 142â€“150.\n[48] Rabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. 2021. Com-\npacter: Efficient Low-Rank Hypercomplex Adapter Layers. InNeurIPS. 1022â€“1035.\n[49] James McDermott and Richard S. Forsyth. 2016. Diagnosing a disorder in a\nclassification benchmark. Pattern Recognit. Lett. 73 (2016), 41â€“43.\n[50] H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. 2018. Learn-\ning Differentially Private Recurrent Language Models. In ICLR (Poster). 14 pages.\n[51] Casey Meehan, Khalil Mrini, and Kamalika Chaudhuri. 2022. Sentence-level\nPrivacy for Document Embeddings. In ACL. 3367â€“3380.\n[52] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh\nHajishirzi, and Luke Zettlemoyer. 2022. Rethinking the Role of Demonstrations:\nWhat Makes In-Context Learning Work?. InEMNLP. 11048â€“11064.\n[53] Ilya Mironov. 2017. RÃ©nyi Differential Privacy. In CSF. 263â€“275.\n[54] Lucien K. L. Ng and Sherman S. M Chow. 2023. SoK: Cryptographic Neural-\nNetwork Computation. In S&P. 497â€“514.\n[55] OSC. 1987. Ohio Supercomputer Center. http://osc.edu/ark:/19495/f5s1ph73\n[56] Xudong Pan, Mi Zhang, Shouling Ji, and Min Yang. 2020. Privacy Risks of\nGeneral-Purpose Language Models. In S&P. 1314â€“1331.\n[57] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove:\nGlobal Vectors for Word Representation. InEMNLP. 1532â€“1543.\n[58] Chen Qu, Weize Kong, Liu Yang, Mingyang Zhang, Michael Bendersky, and Marc\nNajork. 2021. Natural Language Understanding with Privacy-Preserving BERT.\nIn CIKM. 1488â€“1497.\n[59] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Im-\nproving language understanding by generative pre-training. OpenAI Report.\n[60] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. 2019. Language Models are Unsupervised Multitask Learners. OpenAI\nReport.\n[61] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings\nusing Siamese BERT-Networks. In EMNLP-IJCNLP. 3980â€“3990.\n[62] Rakshith Shetty, Bernt Schiele, and Mario Fritz. 2018. A4NT: Author Attribute\nAnonymity by Adversarial Training of Neural Machine Translation. In USENIX\nSecurity. 1633â€“1650.\n[63] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Mem-\nbership Inference Attacks Against Machine Learning Models. In S&P. 3â€“18.\n[64] Congzheng Song and Ananth Raghunathan. 2020. Information Leakage in Em-\nbedding Models. In CCS. 377â€“390.\n[65] Liwei Song and Prateek Mittal. 2021. Systematic Evaluation of Privacy Risks of\nMachine Learning Models. In USENIX Security . 2615â€“2632.\n[66] Thomas Steinke. 2022. Composition of Differential Privacy & Privacy Amplifica-\ntion by Subsampling. arXiv:2210.00597.\n[67] Latanya Sweeney. 2015. Only You, Your Doctor, and Many Others May Know.\nTechnology Science 2015092903, 9 (2015), 29.\n[68] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nYou Need. In NeurIPS. 5998â€“6008.\n[69] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R.\nBowman. 2019. GLUE: A Multi-Task Benchmark and Analysis Platform for\nNatural Language Understanding. In ICLR (Poster). 20 pages.\n[70] Stanley L Warner. 1965. Randomized Response: A Survey Technique for Elimi-\nnating Evasive Answer Bias. JASA 60, 309 (1965), 63â€“69.\n[71] Benjamin Weggenmann and Florian Kerschbaum. 2018. SynTF: Synthetic and Dif-\nferentially Private Term Frequency Vectors for Privacy-Preserving Text Mining.\nIn SIGIR. 305â€“314.\n[72] Ruihan Wu, Jin Peng Zhou, Kilian Q. Weinberger, and Chuan Guo. 2023. Does\nLabel Differential Privacy Prevent Label Inference Attacks?. In AISTAST. 4336â€“\n4347.\n[73] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi,\nWolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff\nKlingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan\nGouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian,\nNishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick,\nOriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Googleâ€™s\nNeural Machine Translation System: Bridging the Gap between Human and\nMachine Translation. arXiv:1609.08144.\n[74] Jungang Yang, Liyao Xiang, Weiting Li, Wei Liu, and Xinbing Wang. 2021. Im-\nproved Matrix Gaussian Mechanism for Differential Privacy. arXiv:2104.14808.\n[75] Jungang Yang, Liyao Xiang, Jiahao Yu, Xinbing Wang, Bin Guo, Zhetao Li, and\nBaochun Li. 2023. Matrix Gaussian Mechanisms for Differentially-Private Learn-\ning. IEEE Trans. Mob. Comput. 22, 2 (2023), 1036â€“1048.\n[76] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. 2018. Privacy\nRisk in Machine Learning: Analyzing the Connection to Overfitting. In CSF.\n268â€“282.\n[77] Ashkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine,\nKarthik Prasad, Mani Malek, John Nguyen, Sayan Ghosh, Akash Bharadwaj,\nJessica Zhao, Graham Cormode, and Ilya Mironov. 2021. Opacus: User-Friendly\nDifferential Privacy Library in PyTorch. arXiv:2109.12298.\n[78] Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A. Inan, Gau-\ntam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz,\nSergey Yekhanin, and Huishuai Zhang. 2022. Differentially Private Fine-tuning\nof Language Models. In ICLR (Poster). 19 pages.\n[79] Da Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. 2021. Large Scale\nPrivate Learning via Low-rank Reparametrization. In ICML. 12208â€“12218.\n[80] Xiang Yue, Minxin Du, Tianhao Wang, Yaliang Li, Huan Sun, and Sherman S. M.\nChow. 2021. Differential Privacy for Text Analytics via Natural Text Sanitization.\nIn Findings of ACL/IJCNLP . 3853â€“3866.\n[81] Mingxun Zhou, Tianhao Wang, T.-H. Hubert Chan, Giulia Fanti, and Elaine Shi.\n2022. Locally Differentially Private Sparse Vector Aggregation. In S&P. 422â€“439.\nA TOKEN-LEVEL DP-FORWARD\nA.1 Definition and Related Notions\nDefinition 6 (Token-level SeqLDP). For ğœ– â‰¥0,0 â‰¤ğ›¿ â‰¤1, M\nfulfills token-level (ğœ–,ğ›¿)-SeqLDP, if âˆ€ğ‘‹ â‰ƒğ‘‹â€²that differ in any single\ntoken but with the same ğ‘¦, and any possible output subset O,\nPr[M(ğ‘‹,ğ‘¦)âˆˆO]â‰¤ ğ‘’ğœ– Pr[M(ğ‘‹â€²,ğ‘¦)âˆˆO]+ ğ›¿.\nCCS â€™23, November 26â€“30, 2023, Copenhagen, Denmark. Du, et al.\nDespite a token-level notion, our experiments (Appendix A.3)\nshow that when ğ‘“(Â·)is only the input embedding layer, our token-\nlevel SeqLDP designs can also effectively mitigate MIAs on entire\nsequences, with up to 20pp accuracy gains at the same choices of ğœ–.\nIt is not necessarily weaker than sequence-level CDP (as offered by\nDP-SGD). One might doubt its usefulness since two neighboring\nsequences may be too similar. Nevertheless, there are cases where\na sentence, e.g., â€œHowâ€™s it goingâ€ may not matter in a bigger unit\n(paragraph/essay) of the training data either. Moreover, a token (e.g.,\nyes/no) can play a crucial role,e.g., in named entity recognition [44].\nOur LDP guarantee is for any such two sequences, covering the\nwide spectrum between â€œtoo similarâ€ and radically different cases.\nNote that weakening privacy notions by itself is not our goal12.\nProtection at the token level has been studied under metric-DP [28,\n58], a relaxation of LDP. They require even much larger ğœ–, say, 175.\nOur goal of studying token-level SeqLDP is to narrow the gap be-\ntween theory and practice, i.e., provable privacy notions tailored to\nthe protection targets (the first few layers vs. the whole pipeline).\nA.2 Two Token-level SeqLDP Designs\nFor token-level SeqLDP, we need to bound a â€œnewâ€ğ‘†2 (ğ‘“),âˆ€ğ‘‹ â‰ƒğ‘‹â€²,\nwhich should be tight and smaller than the one over âˆ€ğ‘‹,ğ‘‹ â€², hence\nproducing smaller noise for better utility at meaningfultoken-level ğœ–.\nIt is still non-trivial sinceğ‘“(Â·), except for being the input embedding\nlayer, may differ in every entry for even ğ‘‹ â‰ƒğ‘‹â€². One could also\nnormalize the entire ğ‘“(Â·)for ğ‘†2 (ğ‘“),âˆ€ğ‘‹ â‰ƒğ‘‹â€², which â€œdegeneratesâ€\nto the token-level SeqLDP. Instead, we tailor two designs to estimate\na tighter ğ‘†2 (ğ‘“)than the â€œgeneralâ€ one for only the input embedding\nlayer and the first two layers, respectively. Specifically, we employ\nrow-wise normalization and the Lipschitz continuity [39].\nAfter the Input Embedding Layer. When ğ‘“(Â·)is only the input\nembedding layer, we work on each row ğ‘¥ğ‘– independently: ||ğ‘¥ğ‘–||2 =\nğ¶,âˆ€ğ‘– âˆˆ[ğ‘›], where ||Â·||2 is vector2-norm. As a token only affects one\nrow, we have ğ‘†2 (ğ‘“)= ğ¶, independent of whether the embedding\nlayer will be updated. Again with B, we can draw noise ğ‘ âˆˆRğ‘›Ã—ğ‘‘.\nIn the First MHA Sub-layer. The second option could be addingğ‘\nright after the first MHA sub-layer:MHA(ğ‘‹)+ğ‘, where MHA(Â·)is\nthe concatenation of Attğ‘–(Â·),ğ‘– âˆˆ[â„]. Yet, it is non-trivial to estimate\nğ‘†2 (ğ‘“)of MHA(Â·)as Attğ‘–(Â·), let alone MHA(Â·), is not Lipschitz [39].\nDefinition 7 (Lipschitz Continuity). Given two metric spaces\n(X,ğ‘‘X)and (Y,ğ‘‘Y), a function ğ‘“ : Xâ†’Y is Lipschitz continuous\n(ğ¾-Lipschitz) if there exists a constant ğ¾ â‰¥0,\nğ‘‘Y(ğ‘“(ğ‘‹),ğ‘“ (ğ‘‹â€²))â‰¤ ğ¾ğ‘‘X(ğ‘‹,ğ‘‹ â€²),âˆ€ğ‘‹,ğ‘‹ â€²âˆˆX.\nThe smallest ğ¾ is the Lipschitz constant, denoted by Lip(ğ‘“).\nWe consider that Xis the space of row-wise normalized matrices\nin Rğ‘›Ã—ğ‘‘, Yis the output space Rğ‘›Ã—ğ‘‘(/â„)of Attğ‘–âˆˆ[â„](Â·)or MHA(Â·),\nand ğ‘‘X= ğ‘‘Y= ||Â·|| ğ¹ (or ğ‘-norm ||Â·|| ğ‘). Lip(ğ‘“)generalizes ğ‘†2 (ğ‘“)\nsince it focuses onany two inputs rather than just neighboring ones,\nallowing us to estimate an upper bound for ğ‘†2 (ğ‘“)given Lip(ğ‘“).\nThe non-Lipschitz continuity stems from the non-linear Softmax\nactivation, which takes pairwise dot products as input [39]. To make\nMHA Lipschitz, one might apply pairwiseğ¿2-distances (hence called\n12As a related example, in image classification, PixelDP [41] has been proposed for a\nDP notion defined upon pixels. Its motivation is robustness to adversarial examples.\nğ¿2-MHA) [39] or add a normalization step called LipschitzNorm [19]\nin softmax(Â·). Unfortunately, estimating Lip(ğ‘“)of ğ¿2-MHA needs\nto solve an intractable optimization problem, and LipschitzNorm is\nill-suited for the high-dimensional BERT attention.\nInstead of adding ğ‘ to the outputs of MHA(Â·)or Attğ‘–(Â·), we can\nshift ğ‘“(Â·)inside softmax(Â·), where estimating Lip(ğ‘“)or ğ‘†2 (ğ‘“)is\nfeasible, e.g., the linear maps used to derive ğ‘„,ğ¾,ğ‘‰ matrices. Con-\nsidering a linear mapğ‘“(ğ‘¥)= ğ‘¥ğ‘Š withğ‘Š âˆˆRğ‘‘Ã—ğ‘‘/â„ and ğ‘¥ âˆˆRğ‘‘, the\n2-norm Lip2 (ğ‘“)is the largest singular value ğœmax (ğ‘Š)[39]. When\ngeneralizing ğ‘“(Â·)for any two matrices ğ‘‹ â‰ƒğ‘‹â€², we can estimate\nğ‘†2 (ğ‘“)= sup ||ğ‘“(ğ‘‹)âˆ’ ğ‘“(ğ‘‹â€²)||ğ¹ = ||ğ‘“(ğ‘¥)||2 â‰¤ğ¶Â·ğœmax (ğ‘Š).\nWe can now respectively derive the noisy ğ‘„,ğ¾,ğ‘‰ matrices for ğ‘(Â·).\nThe first step is to draw noiseğ‘ğ‘„âˆ—,ğ¾âˆ—,ğ‘‰âˆ—\nâˆˆRğ‘›Ã—ğ‘‘ givenğ‘Šğ‘„âˆ—,ğ¾âˆ—,ğ‘‰âˆ—\nâˆˆ\nRğ‘‘Ã—ğ‘‘. It requires us to either estimate ğœmax (ğ‘Šğ‘„âˆ—,ğ¾âˆ—,ğ‘‰âˆ—\n)on the fly\nvia power iteration or fix the linear maps in each forward pass of\nfine-tuning or inference. We then computeğ‘‹ğ‘Šğ‘„âˆ—,ğ¾âˆ—,ğ‘‰âˆ—\n+ğ‘ğ‘„âˆ—,ğ¾âˆ—,ğ‘‰âˆ—\n,\nwhich are reshaped into 3â„matrices of size ğ‘›Ã—ğ‘‘/â„for Attğ‘–âˆˆ[â„](Â·).\nTheorem 7. The two instances (with row-wise normalization) for\nfine-tuning or inference fulfill token-level (ğœ–,ğ›¿)-(Seq)LDP.\nThe proof is equivalent to our approach for (Seq)LDP. One just\nneeds to compute ğ‘†2 (ğ‘“),âˆ€ğ‘‹ â‰ƒğ‘‹â€²properly, and we did.\nDiscussion. For minimal changes to the pipeline, we adopt the raw\nWordPiece [73], which splits text into sub-words; using word-level\ntokenization yields word-level (Seq)LDP. Our notion can also extend\nto phrase-level (Seq)LDP by directly using the group privacy [23]\nor dedicatedly computing the ğ¿2-sensitivity smaller than ğ‘Â·ğ‘†2 (ğ‘“)\nfor two sequences differing in (consecutive) ğ‘tokens. Typically,ğ‘is\nsmall since a few tokens are enough for most sensitive information.\nOne could also add noise deeper in a pipeline using ğ‘†2 (ğ‘“1 â—¦ğ‘“2)â‰¤\nğ‘†2 (ğ‘“1)Â·ğ‘†2 (ğ‘“2), where ğ‘“1 â—¦ğ‘“2 is function composition ğ‘“1 (ğ‘“2 (Â·)). We\nthen need to estimate ğ‘†2 (ğ‘“)of each (component of) sub-layer. For\nexample, FFN(Â·)has two linear maps ğ‘Š1 and ğ‘Š2 with ReLU(Â·)in\nbetween, whereğ‘†2 (ğ‘“)of ReLU(Â·)is 1. Forğ‘Š1,2, itsğ‘†2 (ğ‘“)is bounded\nby\nâˆš\nğ‘‘ğ¶ğœmax (ğ‘Š1,2)since ||Â·|| ğ¹ â‰¤\nâˆš\nğ‘‘||Â·|| 2 with ğ‘‘ as the rank. We\ncan also estimate ğ‘†2 (ğ‘“)of LN(Â·)from its Lipschitz constant [39].\nWhen ğ‘“(Â·)is composed of more layers, we can only get a looser\nestimation on the final ğ‘†2 (ğ‘“). Hence, our general recommendation\nis to add noise early when estimating a tight ğ‘†2 (ğ‘“)is feasible.\nA.3 More Experiment Results\nWe also study the privacy-accuracy tradeoff on all three tasks for our\ntwo token-level SeqLDP designs when tuning localğœ–. The results are\ncompared with the non-private baseline and fine-tuning using MVG\nnoise. Figure 5 shows task accuracy increases with ğœ–. Perturbing\ninput embeddings for token-level (vs. sequence-level) SeqLDP can\nachieve remarkable accuracy gain, e.g., âˆ¼0.7 vs. 0.5 for IMDb.\nWe evaluate the two MIAs on SST-2 fine-tuned by our two\ntoken-level SeqLDP instances. Table 10 shows the results, with\nsuccess rates within 0.48âˆ’0.52 (like random guessing) bolded. Even\nif the provable guarantee is at the token level, our instances can\nnotably reduce the success rates of the confidence-based attack\nby âˆ¼14pp and the entropy-based one by âˆ¼11pp, compared to the\nnon-private baseline.\nDP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass CCS â€™23, November 26â€“30, 2023, Copenhagen, Denmark.\n1 2 4 8 12 16 20 24\n0.5\n0.6\n0.7\n0.8\n0.9Accuracy\nSST-2\n1 2 4 8 12 16 20 24\n0.5\n0.6\n0.7\n0.8\n0.9\nIMDb\n1 2 4 8 12 16 20 24\n0.5\n0.6\n0.7\n0.8\n0.9\nQQP\nDP-Forward (Embedding) DP-Forward (Attention) Non-private Random\nFigure 5: Privacy-accuracy tradeoff of token-level SeqLDP instances when tuning local ğœ–\nLocal\nğœ– Method Attack Success Rate\nEntropy Confidence\nâˆ Non-private baseline 0.659 0 .645\n8 DP-Forward (Embedding) 0.536 0.503\nDP-Forward (Attention) 0.545 0.516\n16 DP-Forward (Embedding) 0.542 0.509\nDP-Forward (Attention) 0.552 0.519\n24 DP-Forward (Embedding) 0.552 0.516\nDP-Forward (Attention) 0.559 0 .523\nTable 10: Success Rates of the two (sequence-level) MIAs on\nour token-level SeqLDP instances\nB RELEVANT MATRIX ALGEBRA\nProposition 1. The PDF defined in Eq. (5) and the matrix-trace-\nbased one used in MVG [14] are equivalent.\nProof. For the numerator part in Eq. (5), we have\n||ğ‘ˆâˆ’1 (ğ‘ âˆ’ğ‘€)ğ‘‰âˆ’âŠ¤||2\nğ¹\n= Tr[ğ‘‰âˆ’1 (ğ‘ âˆ’ğ‘€)âŠ¤ğ‘ˆâˆ’âŠ¤ğ‘ˆâˆ’1 (ğ‘ âˆ’ğ‘€)ğ‘‰âˆ’âŠ¤]\n= Tr[ğ‘‰âˆ’1 (ğ‘ âˆ’ğ‘€)âŠ¤Î£âˆ’1 (ğ‘ âˆ’ğ‘€)ğ‘‰âˆ’âŠ¤],\nwhere Tr(Â·)denotes the matrix trace. Denote\nğ´= ğ‘‰âˆ’1 (ğ‘ âˆ’ğ‘€)âŠ¤Î£âˆ’1 (ğ‘ âˆ’ğ‘€)ğ‘‰âˆ’âŠ¤.\nWe compute\nğµ = ğ‘‰âˆ’âŠ¤ğ´ğ‘‰âŠ¤= Î¨âˆ’1 (ğ‘ âˆ’ğ‘€)âŠ¤Î£âˆ’1 (ğ‘ âˆ’ğ‘€),\nwhich is a similar matrix of ğ´, and hence Tr(ğ´)= Tr(ğµ). So, the\ntwo PDFs are equivalent since\n||ğ‘ˆâˆ’1 (ğ‘ âˆ’ğ‘€)ğ‘‰âˆ’âŠ¤||2\nğ¹ = Tr(ğµ). â–¡\nTheorem 8 (Singular Value Decomposition or SVD [ 34]). A\nmatrix ğ´âˆˆRğ‘›Ã—ğ‘‘ can be decomposed as ğ‘Š1Î›ğ‘ŠâŠ¤\n2 , where ğ‘Š1 âˆˆRğ‘›Ã—ğ‘›\nand ğ‘Š2 âˆˆRğ‘‘Ã—ğ‘‘ are unitary, and Î› is an ğ‘› Ã—ğ‘‘ diagonal matrix\nwhose diagonal entries are ordered singular values of ğ´, denoted by\nğœ1 (ğ´)â‰¥ ... â‰¥ğœğ‘Ÿ(ğ´)â‰¥ 0 (or simply ğœ(ğ´)) with ğ‘Ÿ = min{ğ‘›,ğ‘‘}.\nLemma 4. Given a matrix ğ´âˆˆRğ‘›Ã—ğ‘‘ and two orthogonal matrices\nğ‘Š1 âˆˆRğ‘›Ã—ğ‘›,ğ‘Š2 âˆˆRğ‘‘Ã—ğ‘‘, we have ||ğ´||ğ¹ = ||ğ‘Š1ğ´||ğ¹ = ||ğ´ğ‘Š2||ğ¹;\n||Â·|| ğ¹ is immune to the pre- and post-orthogonal transformation.\nProof. We first prove that ||ğ´||ğ¹ = ||ğ‘Š1ğ´||ğ¹ by\n||ğ‘Š1ğ´||2\nğ¹ = Tr(ğ´âŠ¤ğ‘ŠâŠ¤\n1 ğ‘Š1ğ´)= Tr(ğ´ğ‘‡ğ´)= ||ğ´||2\nğ¹,\nand similarly we can prove that ||ğ´||ğ¹ = ||ğ´ğ‘Š2||ğ¹. â–¡\nLemma 5. For ğ´ âˆˆRğ‘›Ã—ğ‘‘, ||ğ´||2\nğ¹ = Ãğ‘Ÿ\nğ‘–=1 ğœ2\nğ‘– (ğ´), where ğœğ‘–(ğ´)is\nthe ğ‘–th singular value of ğ´and ğ‘Ÿ = min{ğ‘›,ğ‘‘}.\nProof. The SVD of ğ´is ğ‘Š1Î›ğ‘ŠâŠ¤\n2 . By Lemma 4, we have\n||ğ´||2\nğ¹ = ||ğ‘Š1Î›ğ‘ŠâŠ¤\n2 ||2\nğ¹ = ||Î›||2\nğ¹ =\nğ‘Ÿâˆ‘ï¸\nğ‘–=1\nğœ2\nğ‘– (ğ´). â–¡\nLemma 6 (Lemma 4 [74]). Given matrices ğ´ âˆˆ Rğ‘›Ã—ğ‘›,ğµ âˆˆ\nRğ‘›Ã—ğ‘‘,ğ¶ âˆˆRğ‘‘Ã—ğ‘‘, we have ||ğ´ğµğ¶||2\nğ¹ â‰¤Ãğ‘Ÿ\nğ‘–=1 ğœ2\nğ‘– (ğ´)ğœ2\nğ‘– (ğµ)ğœ2\nğ‘– (ğ¶)where\nğœğ‘–(Â·)is the ğ‘–th singular value, and ğ‘Ÿ = min{ğ‘›,ğ‘‘}.\nProof. With SVD, ğ´,ğµ,ğ¶ are decomposed as\nğ´= ğ‘Šğ´1 Î›ğ´ğ‘ŠâŠ¤\nğ´2 , ğµ= ğ‘Šğµ1 Î›ğµğ‘ŠâŠ¤\nğµ2 , ğ¶= ğ‘Šğ¶1 Î›ğ¶ğ‘ŠâŠ¤\nğ¶2 .\nBased on Lemma 5, we have\n||ğ´ğµğ¶||2\nğ¹ = ||ğ‘Šğ´1 Î›ğ´ğ‘ŠâŠ¤\nğ´2ğ‘Šğµ1 Î›ğµğ‘ŠâŠ¤\nğµ2ğ‘Šğ¶1 Î›ğ¶ğ‘ŠâŠ¤\nğ¶2 ||2\nğ¹\n= ||Î›ğ´ğ‘ŠÎ›ğµğ‘Šâ€²Î›ğ¶||2\nğ¹,\nwhere ğ‘Š = ğ‘ŠâŠ¤\nğ´2\nğ‘Šğµ1 = (ğ‘¤ğ‘–ğ‘—)ğ‘›Ã—ğ‘› and ğ‘Šâ€² = ğ‘ŠâŠ¤\nğµ2\nğ‘Šğ¶1 = (ğ‘¤â€²\nğ‘–ğ‘—)ğ‘‘Ã—ğ‘‘\nare still two unitary matrices. We further have\n||ğ´ğµğ¶||2\nğ¹ =\nğ‘›âˆ‘ï¸\nğ‘–=1\nğ‘‘âˆ‘ï¸\nğ‘—=1\nğœ2\nğ‘– (ğ´)ğœ2\nğ‘—(ğ¶)[\nğ‘Ÿâˆ‘ï¸\nğ‘˜=1\nğœğ‘˜(ğµ)ğ‘¤ğ‘–ğ‘˜ğ‘¤â€²\nğ‘˜ğ‘—]2\n=\nğ‘›âˆ‘ï¸\nğ‘–=1\nğ‘‘âˆ‘ï¸\nğ‘—=1\nğœ2\nğ‘– (ğ´)ğœ2\nğ‘—(ğ¶)ğ›½2\nğ‘–ğ‘—,\nwhere ğ›½ğ‘–ğ‘— = Ãğ‘Ÿ\nğ‘˜=1 ğœğ‘˜(ğµ)ğ‘¤ğ‘–ğ‘˜ğ‘¤â€²\nğ‘˜ğ‘—. Hence, we need to show\nğ‘›âˆ‘ï¸\nğ‘–=1\nğ‘‘âˆ‘ï¸\nğ‘—=1\nğœ2\nğ‘– (ğ´)ğœ2\nğ‘—(ğ¶)ğ›½2\nğ‘–ğ‘— â‰¤\nğ‘Ÿâˆ‘ï¸\nğ‘–=1\nğœ2\nğ‘– (ğ´)ğœ2\nğ‘– (ğµ)ğœ2\nğ‘– (ğ¶). (7)\nFollowing the strategy in [74] (cf. Eq. (29), (30)), we rewrite ğœ2\nğ‘– (ğ´)\nand ğœ2\nğ‘—(ğ¶)using non-negative values ğœ‰ğ‘¡ and ğœ‚ğ‘  s.t.\nğœ2\nğ‘– (ğ´)=\nğ‘›âˆ‘ï¸\nğ‘¡=ğ‘–\nğœ‰ğ‘¡, ğ‘¡âˆˆ[1,ğ‘›]; ğœ2\nğ‘—(ğ¶)=\nğ‘‘âˆ‘ï¸\nğ‘ =ğ‘—\nğœ‚ğ‘ , ğ‘ âˆˆ[1,ğ‘‘].\nCCS â€™23, November 26â€“30, 2023, Copenhagen, Denmark. Du, et al.\nFor ğ‘– âˆˆ[1,ğ‘›],ğ‘— âˆˆ[1,ğ‘‘], we denote ğ›¾ğ‘–ğ‘— = ğœğ‘–(ğµ), if ğ‘– = ğ‘—; ğ›¾ğ‘–ğ‘— = 0,\notherwise. Then, we transform the Eq. (7) as\nğ‘Ÿâˆ‘ï¸\nğ‘–=1\nğœ2\nğ‘– (ğ´)ğœ2\nğ‘– (ğµ)ğœ2\nğ‘– (ğ¶)âˆ’\nğ‘›âˆ‘ï¸\nğ‘–=1\nğ‘‘âˆ‘ï¸\nğ‘—=1\nğœ2\nğ‘– (ğ´)ğœ2\nğ‘—(ğ¶)ğ›½2\nğ‘–ğ‘—\n=\nğ‘›âˆ‘ï¸\nğ‘–=1\nğ‘‘âˆ‘ï¸\nğ‘—=1\n(ğ›¾2\nğ‘–ğ‘— âˆ’ğ›½2\nğ‘–ğ‘—)ğœ2\nğ‘– (ğ´)ğœ2\nğ‘—(ğ¶)\n=\nğ‘›âˆ‘ï¸\nğ‘–=1\nğ‘‘âˆ‘ï¸\nğ‘—=1\n(ğ›¾2\nğ‘–ğ‘— âˆ’ğ›½2\nğ‘–ğ‘—)\nğ‘›âˆ‘ï¸\nğ‘¡=ğ‘–\nğœ‰ğ‘¡\nğ‘‘âˆ‘ï¸\nğ‘ =ğ‘—\nğœ‚ğ‘ \n=\nğ‘›âˆ‘ï¸\nğ‘¡=1\nğ‘‘âˆ‘ï¸\nğ‘ =1\nğœ‰ğ‘¡ğœ‚ğ‘ \nğ‘¡âˆ‘ï¸\nğ‘–=1\nğ‘ âˆ‘ï¸\nğ‘—=1\n(ğ›¾2\nğ‘–ğ‘— âˆ’ğ›½2\nğ‘–ğ‘—).\nSince ğœ‰ğ‘¡,ğœ‚ğ‘  are non-negative, we only need to show\nğ‘¡âˆ‘ï¸\nğ‘–=1\nğ‘ âˆ‘ï¸\nğ‘—=1\n(ğ›¾2\nğ‘–ğ‘— âˆ’ğ›½2\nğ‘–ğ‘—)â‰¥ 0. (8)\nHowever, the original proof [ 74] has two issues: i) ğ‘¡ > ğ‘  is not\nconsidered, and ii) the commutative law of matrix multiplication in\nEq. (35) does not hold as ğ¸(ğ‘¡)in Eq. (34) is not a standard diagonal\nmatrix. To address them, we have\nğ‘¡âˆ‘ï¸\nğ‘–=1\nğ‘ âˆ‘ï¸\nğ‘—=1\nğ›¾2\nğ‘–ğ‘— =\nmin{ğ‘¡,ğ‘ }âˆ‘ï¸\nğ‘˜=1\nğœ2\nğ‘˜(ğµ).\nWe then denote a sub-matrix ğµâˆ—= (ğ›½ğ‘–ğ‘—)for ğ‘– âˆˆ[1,ğ‘¡],ğ‘— âˆˆ[1,ğ‘ ]of\nğ‘ŠÎ›ğµğ‘Šâ€². With SVD of ğµâˆ—, we have\nğ‘¡âˆ‘ï¸\nğ‘–=1\nğ‘ âˆ‘ï¸\nğ‘—=1\nğ›½2\nğ‘–ğ‘— = ||ğµâˆ—||2\nğ¹ =\nmin{ğ‘¡,ğ‘ }âˆ‘ï¸\nğ‘˜=1\nğœ2\nğ‘˜(ğµâˆ—)â‰¤\nmin{ğ‘¡,ğ‘ }âˆ‘ï¸\nğ‘˜=1\nğœ2\nğ‘˜(ğµ).\nThe last inequality is due to ğœğ‘˜(ğµâˆ—)â‰¤ ğœğ‘˜(ğµ)for âˆ€ğ‘˜ âˆˆ[1,ğ‘Ÿ][34].\nSo, Inequality (8) holds. â–¡\nC PROOFS FOR OUR ANALYTIC MATRIX\nGAUSSIAN MECHANISM\nThis section proof Lemma 1, Lemma 2, and Theorem 6 in Section 4.2.\nProof of Lemma 1.Recall that M(ğ‘“(X))= ğ‘“(X)+ğ‘ with ğ‘ âˆ¼\nMNğ‘›,ğ‘‘(0,Î£,Î¨), the probability of M(ğ‘“(X))= ğ‘‚ is\nPr[M(ğ‘“(X)= ğ‘‚]=\nexp(âˆ’1\n2 ||ğ‘ˆâˆ’1 (ğ‘‚âˆ’ğ‘“(X))ğ‘‰âˆ’âŠ¤||2\nğ¹)\n(2ğœ‹)ğ‘›ğ‘‘/2|Î¨|ğ‘‘/2|Î£|ğ‘›/2 .\nSimilarly, we can compute Pr[M(ğ‘“(Xâ€²))= ğ‘‚]. By plugging them\ninto LM,X,Xâ€²(ğ‘‚), and let Î” = ğ‘“(X)âˆ’ ğ‘“(Xâ€²),\nLM,X,Xâ€²(ğ‘‚)= ln\nexp(âˆ’1\n2 ||ğ‘ˆâˆ’1 (ğ‘‚âˆ’ğ‘“(X))ğ‘‰âˆ’âŠ¤||2\nğ¹)\nexp(âˆ’1\n2 ||ğ‘ˆâˆ’1 (ğ‘‚âˆ’ğ‘“(Xâ€²))ğ‘‰âˆ’âŠ¤||2\nğ¹)\n= 1\n2 ||ğ‘ˆâˆ’1 (ğ‘ +Î”)ğ‘‰âˆ’âŠ¤||2\nğ¹ âˆ’1\n2 ||ğ‘ˆâˆ’1ğ‘ğ‘‰âˆ’âŠ¤||2\nğ¹\n= 1\n2 ||ğ‘ˆâˆ’1Î”ğ‘‰âˆ’âŠ¤||2\nğ¹ +âŸ¨vec(ğ‘ˆâˆ’1Î”ğ‘‰âˆ’âŠ¤),vec(ğ‘ˆâˆ’1ğ‘ğ‘‰âˆ’âŠ¤)âŸ©,\nwhere vec(Â·)is the vectorization of a matrix and âŸ¨Â·,Â·âŸ©denotes the\ninner product. For easy presentation, we denote ğ‘â€²= ğ‘ˆâˆ’1ğ‘ğ‘‰âˆ’âŠ¤\nand Î”â€²= ğ‘ˆâˆ’1Î”ğ‘‰âˆ’âŠ¤, and then we re-write\nLM,X,Xâ€²(ğ‘‚)= 1\n2 ||Î”â€²||2\nğ¹ +âŸ¨vec(Î”â€²),vec(ğ‘â€²)âŸ©.\nGiven Lemma 3,ğ‘â€²âˆ¼MNğ‘›,ğ‘‘(0,ğ¼ğ‘›,ğ¼ğ‘‘)with each entry i.i.d. drawn\nfrom N(0,1). âŸ¨vec(Î”â€²),vec(ğ‘â€²)âŸ©is thus the Î”â€²-weighted sum of ğ‘›ğ‘‘\ni.i.d. Gaussian random variables, which is a Gaussian variable 13\nN(0,||Î”â€²||2\nğ¹)too. So LM,X,Xâ€² âˆ¼N(ğœ‚,2ğœ‚), ğœ‚ = 1\n2 ||Î”â€²||2\nğ¹. â–¡\nProof of Lemma 2.With Lemma 1 and the CDF, we have\nPr[LM,X,Xâ€² â‰¥ğœ–]= Pr[N(ğœ‚,2ğœ‚)â‰¥ ğœ–]\n= Pr[N(0,1)â‰¥ ğœ–âˆ’ğœ‚âˆš2ğœ‚ ]= Pr[N(0,1)â‰¤ ğœ‚âˆ’ğœ–âˆš2ğœ‚ ]\n= Î¦(||Î”â€²||ğ¹\n2 âˆ’ ğœ–\n||Î”â€²||ğ¹\n),\nwhere we usedN(ğœ‚,2ğœ‚)= ğœ‚+N(0,1)/âˆš2ğœ‚and the symmetry of the\nstandard normal distribution Pr[N(0,1)â‰¥ ğ‘¡]= Pr[N(0,1)â‰¤âˆ’ ğ‘¡].\nA similar argument applied to LM,Xâ€²,Xyields\nPr[LM,Xâ€²,Xâ‰¤âˆ’ğœ–]= Î¦(âˆ’||Î”â€²||ğ¹\n2 âˆ’ ğœ–\n||Î”â€²||ğ¹\n). â–¡\nProof of Theorem 6.The proof boils down to two directions.\nFrom (ğœ–,ğ›¿)-DP (Theorem 5) to Theorem 6, the proof directly follows\nfrom all the derivations in Section 4.2. For the inverse direction,\nit is sufficient to show that ||Î”â€²||ğ¹ â‰¤B holds for âˆ€Xâ‰ƒX â€²given\nTheorem 6. In particular, for ||Î”â€²||ğ¹ = ||ğ‘ˆâˆ’1Î”ğ‘‰âˆ’âŠ¤||ğ¹, we have\n||ğ‘ˆâˆ’1Î”ğ‘‰âˆ’âŠ¤||2\nğ¹ â‰¤\nğ‘Ÿâˆ‘ï¸\nğ‘–=1\nğœ2\nğ‘– (Î”)\nğœ2\nğ‘›âˆ’ğ‘–+1 (ğ‘ˆ)ğœ2\nğ‘‘âˆ’ğ‘–+1 (ğ‘‰)\nâ‰¤\nÃğ‘Ÿ\nğ‘–=1 ğœ2\nğ‘– (Î”)\nğœ2ğ‘›(ğ‘ˆ)ğœ2\nğ‘‘(ğ‘‰)â‰¤\n||Î”||2\nğ¹\nğ‘†2\n2 (ğ‘“)/B2 â‰¤B2,\nwhere the first inequality is due to Lemma 6, the second one holds\nsince ğœğ‘›(ğ‘ˆ)and ğœğ‘‘(ğ‘‰)are the smallest singular values among the\nothers, and the third one is from Theorem 6. â–¡\n13en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables",
  "topic": "Inference",
  "concepts": [
    {
      "name": "Inference",
      "score": 0.7863795757293701
    },
    {
      "name": "Embedding",
      "score": 0.7359524965286255
    },
    {
      "name": "Computer science",
      "score": 0.7121604681015015
    },
    {
      "name": "Differential privacy",
      "score": 0.6642076969146729
    },
    {
      "name": "Stochastic gradient descent",
      "score": 0.5098813772201538
    },
    {
      "name": "Algorithm",
      "score": 0.4756685793399811
    },
    {
      "name": "Gradient descent",
      "score": 0.4407200813293457
    },
    {
      "name": "Approximate inference",
      "score": 0.44015222787857056
    },
    {
      "name": "Computation",
      "score": 0.4281110465526581
    },
    {
      "name": "Noise (video)",
      "score": 0.4250360131263733
    },
    {
      "name": "Gaussian",
      "score": 0.41317999362945557
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35850924253463745
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Artificial neural network",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I52357470",
      "name": "The Ohio State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I51556381",
      "name": "University of Virginia",
      "country": "US"
    }
  ],
  "cited_by": 27
}