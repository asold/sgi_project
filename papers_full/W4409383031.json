{
    "title": "Reasoning Beyond Accuracy: Expert Evaluation of Large Language Models in Diagnostic Pathology",
    "url": "https://openalex.org/W4409383031",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A3168652028",
            "name": "Asim Waqas",
            "affiliations": [
                "Moffitt Cancer Center"
            ]
        },
        {
            "id": "https://openalex.org/A2078338887",
            "name": "Ehsan Ullah",
            "affiliations": [
                "Counties Manukau District Health Board"
            ]
        },
        {
            "id": "https://openalex.org/A2149013775",
            "name": "Asma Khan",
            "affiliations": [
                "Armed Forces Institute of Pathology"
            ]
        },
        {
            "id": "https://openalex.org/A2408872733",
            "name": "Farah Khalil",
            "affiliations": [
                "Moffitt Cancer Center"
            ]
        },
        {
            "id": null,
            "name": "Zarifa Gahramanli Ozturk",
            "affiliations": [
                "Moffitt Cancer Center"
            ]
        },
        {
            "id": "https://openalex.org/A2065232156",
            "name": "Vaibhav Chumbalkar",
            "affiliations": [
                "Moffitt Cancer Center"
            ]
        },
        {
            "id": "https://openalex.org/A4281243357",
            "name": "Daryoush Saeed‐Vafa",
            "affiliations": [
                "Moffitt Cancer Center"
            ]
        },
        {
            "id": "https://openalex.org/A2366040025",
            "name": "Zena Jameel",
            "affiliations": [
                "Moffitt Cancer Center"
            ]
        },
        {
            "id": "https://openalex.org/A2102748483",
            "name": "Wei-Shen Chen",
            "affiliations": [
                "University of South Florida"
            ]
        },
        {
            "id": "https://openalex.org/A4296614096",
            "name": "Humberto Trejo Bittar",
            "affiliations": [
                "Moffitt Cancer Center"
            ]
        },
        {
            "id": "https://openalex.org/A2105451465",
            "name": "Jasreman Dhillon",
            "affiliations": [
                "Moffitt Cancer Center"
            ]
        },
        {
            "id": "https://openalex.org/A2487966133",
            "name": "RAJENDRA S. SINGH",
            "affiliations": [
                "Summit Medical Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2009604538",
            "name": "Andrey Bychkov",
            "affiliations": [
                "Kameda Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A672176348",
            "name": "Anil V. Parwani",
            "affiliations": [
                "The Ohio State University"
            ]
        },
        {
            "id": "https://openalex.org/A2330941043",
            "name": "Marilyn M. Bui",
            "affiliations": [
                "Moffitt Cancer Center"
            ]
        },
        {
            "id": "https://openalex.org/A133859744",
            "name": "Matthew B. Schabath",
            "affiliations": [
                "Moffitt Cancer Center"
            ]
        },
        {
            "id": "https://openalex.org/A1372903011",
            "name": "Ghulam Rasool",
            "affiliations": [
                "Moffitt Cancer Center"
            ]
        },
        {
            "id": "https://openalex.org/A3168652028",
            "name": "Asim Waqas",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2078338887",
            "name": "Ehsan Ullah",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2149013775",
            "name": "Asma Khan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2408872733",
            "name": "Farah Khalil",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Zarifa Gahramanli Ozturk",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2065232156",
            "name": "Vaibhav Chumbalkar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4281243357",
            "name": "Daryoush Saeed‐Vafa",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2366040025",
            "name": "Zena Jameel",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2102748483",
            "name": "Wei-Shen Chen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4296614096",
            "name": "Humberto Trejo Bittar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2105451465",
            "name": "Jasreman Dhillon",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2487966133",
            "name": "RAJENDRA S. SINGH",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2009604538",
            "name": "Andrey Bychkov",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A672176348",
            "name": "Anil V. Parwani",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2330941043",
            "name": "Marilyn M. Bui",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A133859744",
            "name": "Matthew B. Schabath",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1372903011",
            "name": "Ghulam Rasool",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4297692423",
        "https://openalex.org/W4409527580",
        "https://openalex.org/W4408795761",
        "https://openalex.org/W4394845024",
        "https://openalex.org/W4397003497",
        "https://openalex.org/W4401109822",
        "https://openalex.org/W4406666355",
        "https://openalex.org/W4402432789",
        "https://openalex.org/W4387026748",
        "https://openalex.org/W4408886791",
        "https://openalex.org/W4404759704",
        "https://openalex.org/W4384071683",
        "https://openalex.org/W4387232979",
        "https://openalex.org/W4391046210",
        "https://openalex.org/W4404837143",
        "https://openalex.org/W4402765248",
        "https://openalex.org/W4402909345",
        "https://openalex.org/W2042375932",
        "https://openalex.org/W1605003121",
        "https://openalex.org/W2738938474",
        "https://openalex.org/W2119682955",
        "https://openalex.org/W2037265653",
        "https://openalex.org/W2076617429",
        "https://openalex.org/W4387744047",
        "https://openalex.org/W4386693643",
        "https://openalex.org/W4390747669",
        "https://openalex.org/W4403813762",
        "https://openalex.org/W4405021649",
        "https://openalex.org/W4393386222",
        "https://openalex.org/W4407308237",
        "https://openalex.org/W4396832139"
    ],
    "abstract": "Abstract Background Diagnostic pathology depends on complex, structured reasoning to interpret clinical, histologic, and molecular data. Replicating this cognitive process algorithmically remains a significant challenge. As large language models (LLMs) gain traction in medicine, it is critical to determine whether they have clinical utility by providing reasoning in highly specialized domains such as pathology. Methods We evaluated the performance of four reasoning LLMs (OpenAI o1, OpenAI o3-mini, Gemini 2.0 Flash Thinking Experimental, and DeepSeek-R1 671B) on 15 board-style open-ended pathology questions. Responses were independently reviewed by 11 pathologists using a structured framework that assessed language quality (accuracy, relevance, coherence, depth, and conciseness) and seven diagnostic reasoning strategies. Scores were normalized and aggregated for analysis. We also evaluated inter-observer agreement to assess scoring consistency. Model comparisons were conducted using one-way ANOVA and Tukey’s Honestly Significant Difference (HSD) test. Results Gemini and DeepSeek significantly outperformed OpenAI o1 and OpenAI o3-mini in overall reasoning quality (p &lt; 0.05), particularly in analytical depth and coherence. While all models achieved comparable accuracy, only Gemini and DeepSeek consistently applied expert-like reasoning strategies, including algorithmic, inductive, and Bayesian approaches. Performance varied by reasoning type: models performed best in algorithmic and deductive reasoning and poorest in heuristic and pattern recognition. Inter-observer agreement was highest for Gemini (p &lt; 0.05), indicating greater consistency and interpretability. Models with more in-depth reasoning (Gemini and DeepSeek) were generally less concise. Conclusion Advanced LLMs such as Gemini and DeepSeek can approximate aspects of expert-level diagnostic reasoning in pathology, particularly in algorithmic and structured approaches. However, limitations persist in contextual reasoning, heuristic decision-making, and consistency across questions. Addressing these gaps, along with trade-offs between depth and conciseness, will be essential for the safe and effective integration of AI tools into clinical pathology workflows.",
    "full_text": "REASONING BEYOND ACCURACY : E XPERT EVALUATION OF\nLARGE LANGUAGE MODELS IN DIAGNOSTIC PATHOLOGY\nEhsan Ullah, PhD, M.Phil., MBBS∗\nDepartment of Surgery\nHealth New Zealand, Counties Manukau\nAuckland, New Zealand\nehsan.ullah@middlemore.co.nz\nAsim Waqas, PhD∗\nDepartment of Cancer Epidemiology\nH. Lee Moffitt Cancer Center & Research Institute\nTampa, FL\nAsim.Waqas@moffitt.org\nAsma Khan, MBBS\nArmed Forces Institute of Pathology\nRawalpindi, Pakistan\nFarah Khalil, MD\nDepartment of Pathology\nH. Lee Moffitt Cancer Center & Research Institute\nZarifa Gahramanli Ozturk, MD\nDepartment of Pathology\nMH. Lee Moffitt Cancer Center & Research Institute\nVaibhav Chumbalkar, MD\nDepartment of Pathology\nH. Lee Moffitt Cancer Center & Research Institute\nDaryoush Saeed-Vafa, MD\nDepartment of Pathology\nH. Lee Moffitt Cancer Center & Research Institute\nZena Jameel, MD\nDepartment of Pathology\nH. Lee Moffitt Cancer Center & Research Institute\nWeishen Chen, MD, PhD\nDepartment of Dermatology & Cutaneous Surgery\nUniversity of South Florida\nTampa, FL\nHumberto Trejo Bittar, MD\nDepartment of Pathology\nH. Lee Moffitt Cancer Center & Research Institute\nJasreman Dhillon, MD\nDepartment of Anatomic Pathology\nH. Lee Moffitt Cancer Center & Research Institute\nRajendra S. Singh, MD\nDermatopathology and Digital Pathology\nSummit Health, Berkley Heights, NJ\nAndrey Bychkov, MD, PhD\nDepartment of Pathology\nKameda Medical Center\nKamogawa City, Chiba Prefecture, Japan\nAnil V . Parwani, MD, PhD, MBA\nDepartment of Pathology\nThe Ohio State University\nColumbus, Ohio\nMarilyn M. Bui, MD, PhD\nDepartment of Pathology\nH. Lee Moffitt Cancer Center & Research Institute\nMatthew B. Schabath, PhD\nDepartment of Cancer Epidemiology\nH. Lee Moffitt Cancer Center & Research Institute\nGhulam Rasool, PhD\nDepartment of Machine Learning\nH. Lee Moffitt Cancer Center & Research Institute\n∗Equal first authors.\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 12, 2025. ; https://doi.org/10.1101/2025.04.11.25325686doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\nABSTRACT\nBackground\nDiagnostic pathology depends on complex, structured reasoning to interpret clinical, histologic, and\nmolecular data. Replicating this cognitive process algorithmically remains a significant challenge. As\nlarge language models (LLMs) gain traction in medicine, it is critical to determine whether they have\nclinical utility by providing reasoning in highly specialized domains such as pathology.\nMethods\nWe evaluated the performance of four reasoning LLMs (OpenAI o1, OpenAI o3-mini, Gemini 2.0\nFlash Thinking Experimental, and DeepSeek-R1 671B) on 15 board-style open-ended pathology\nquestions. Responses were independently reviewed by 11 board-certified pathologists using a\nstructured framework that assessed language quality (accuracy, relevance, coherence, depth, and\nconciseness) and seven diagnostic reasoning strategies. Scores were normalized and aggregated for\nanalysis. We also evaluated inter-rater agreement to assess scoring consistency. Model comparisons\nwere conducted using one-way ANOV A and Tukey’s Honestly Significant Difference (HSD) test.\nResults\nGemini and DeepSeek significantly outperformed OpenAI o1 and OpenAI o3-mini in overall rea-\nsoning quality (p < 0.05), particularly in analytical depth and coherence. While all models achieved\ncomparable accuracy, only Gemini and DeepSeek consistently applied expert-like reasoning strategies,\nincluding algorithmic, inductive, and Bayesian approaches. Performance varied by reasoning type\nand pathology subspecialty: models performed best in algorithmic and deductive reasoning and\npoorest in heuristic and pattern recognition. Inter-rater agreement was highest for Gemini (p < 0.001),\nindicating greater consistency and interpretability. Models with more in-depth reasoning (Gemini\nand DeepSeek) were generally less concise.\nConclusion\nAdvanced LLMs such as Gemini and DeepSeek can approximate aspects of expert-level diagnostic\nreasoning in pathology, particularly in algorithmic and structured approaches. However, limitations\npersist in contextual reasoning, heuristic decision-making, and consistency across subspecialties.\nAddressing these gaps, along with trade-offs between depth and conciseness, will be essential for the\nsafe and effective integration of AI tools into clinical pathology workflows.\nKeywords Generative AI · Reasoning Large Language Models · Pathology · Clinical Reasoning · AI Evaluation\n1 Introduction\nAnatomical pathologists draw upon years of training, clinical experience, and diagnostic reasoning strategies to interpret\nhistopathologic findings in the context of patient presentation. Their training and prior insights are essential for\naccurate disease classification and downstream clinical decision-making [1]. These reasoning strategies include pattern\nrecognition, algorithmic workflows, deductive and inductive hypothetico-deductive reasoning, Bayesian inference,\nheuristic shortcuts, and mechanistic understanding of disease processes, each contributing to the interpretive synthesis\nof visual, clinical, and contextual cues [1].\nWith the rise of generative artificial intelligence (AI) and large language models (LLMs), there is growing interest in\napplying such modalities to support pathology workflows. However, most existing evaluations focus on the factual\ncorrectness or clinical plausibility of AI-generated responses rather than the reasoning processes underpinning those\noutputs [2, 3, 4]. LLMs, such as ChatGPT, Gemini, and DeepSeek, generate responses by leveraging probabilistic\nassociations across vast biomedical and clinical corpora. While applications of LLMs in pathology have primarily\ncentered on summarization, education, and simulation, few studies have systematically evaluated whether these models\nexhibit clinically relevant reasoning patterns that mirror real-world diagnostic decision-making [2].\nWhile LLMs, including ChatGPT and Gemini, have demonstrated proficiency in general knowledge tasks, their ability\nto navigate the complexities of medical reasoning, particularly in specialized fields like pathology, requires rigorous\nevaluation [5, 6, 2, 3, 4, 7]. Existing studies primarily focus on their general medical accuracy, but few have explored\ntheir pathology-specific competencies in depth. Given that board licensing examinations in medicine require nuanced\nreasoning beyond factual recall [8], it is essential to determine whether LLMs can provide not just accurate but clinically\nrelevant and contextually sound responses and apply various reasoning strategies in a balanced and structured manner.\nThis first-of-its-kind study aimed to systematically evaluate the diagnostic reasoning capabilities of four state-of-the-art\nLLMs using a novel expert-annotated benchmark and a structured evaluation framework. Board-certified pathologists\n2\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 12, 2025. ; https://doi.org/10.1101/2025.04.11.25325686doi: medRxiv preprint \n15 open-ended \ndiagnostic \nquestions at the \nlevel of board \nlicensing \nexaminations\nReasoning LLMs\nStatistical Analysis\n• Gemini 2.0 Flash Thinking Exp\n• DeepSeek-R1 671B\n• OpenAI o1\n• OpenAI o3-mini\n7 Metrics for Diagnostic \nReasoning\n11 Pathologists\n720 Evaluations Per \nPathologist\n5 Metrics Language Quality \nand  Structure\n• Response Accuracy\n• Relevance\n• Coherence\n• Analytical Depth\n• Conciseness\n• Pattern Recognition\n• Algorithmic Reasoning\n• Deductive Reasoning\n• Inductive, Hypothetico-\nDeductive Reasoning\n• Bayesian Reasoning\n• Heuristic Reasoning\n• Mechanistic Insights \nFigure 1: Evaluation Framework for Assessing Diagnostic Reasoning in Large Language Models. Fifteen open-\nended diagnostic pathology questions, reflecting the complexity of board licensing examinations, were independently\nsubmitted to four LLMs: OpenAI o1, OpenAI o3-mini, Gemini 2.0 Flash-Thinking Experimental (Gemini), and\nDeepSeek-R1 671B (DeepSeek). Each response was evaluated by 11 board-certified pathologists using a structured\nrubric comprising 12 metrics across two domains: (1) language quality and response structure (relevance, coherence,\naccuracy, depth, and conciseness) and (2) diagnostic reasoning strategies (pattern recognition, algorithmic, deductive,\ninductive/hypothetico-deductive, heuristic, mechanistic, and Bayesian reasoning). All pathology evaluations were\nblinded. Scores were aggregated and normalized to account for missing data and used in model performance and\ninter-rater agreement statistical analyses.\nassessed model-generated responses to 15 open-ended diagnostic questions spanning six pathology subspecialties:\nPulmonary (PULM, 3 questions), Gynecological (GYN, 5), Gastrointestinal (GI, 3), Head and Neck (HN, 1), Endocrine\n(ENDO, 1), and Genitourinary (GU, 2), scoring them across five language quality metrics and seven diagnostic reasoning\nstrategies grounded in pathology literature [ 8, 1]. We compared the performance of OpenAI o1, OpenAI o3-mini,\nGemini 2.0 Flash Thinking Experimental (Gemini), and DeepSeek-R1 671B (DeepSeek) across all evaluation criteria.\nOur analysis includes comparative model rankings, inter-rater agreement, and subspecialty-specific differences in LLM\nbehavior. These findings comprehensively assess reasoning in medical LLMs and introduce a scalable, domain-grounded\nframework for evaluating clinical reasoning in AI systems.\n2 Materials and Methods\nIn this study, a structured evaluation framework was developed to assess the capacity of LLMs to generate clinically\nrelevant, well-reasoned answers to open-ended pathology questions. Our focus was on both language quality and\ndiagnostic reasoning, with expert review by board-certified pathologists.\n2.1 Question Selection\nBased on the study by Wang et al. [8], a set of 15 open-ended diagnostic questions was selected to reflect the complexity\nand format of board licensing examinations. To ensure comprehensive topical representation, the questions were\ncategorized into six pathology subspecialties, capturing a broad range of diagnostic challenges commonly encountered\nin clinical practice. These included: Pulmonary Pathology (PULM, 3 questions), covering neoplastic and non-neoplastic\nlung diseases such as lung carcinoma and interstitial lung disease; Gynecological Pathology (GYN, 5 questions),\nfocused on disorders of the female reproductive system, including endometrial and ovarian tumors, cervical pathology,\nand cytopathology; Gastrointestinal Pathology (GI, 3 questions), addressing inflammatory and neoplastic conditions\nsuch as colorectal carcinoma and inflammatory bowel disease; Head and Neck Pathology (HN, 1 question), assessing\ntumors and inflammatory conditions in the oral cavity, salivary glands, and larynx; Endocrine Pathology (ENDO,\n1 question), focused on diseases of the endocrine glands, including thyroid carcinoma and adrenal pathology; and\nGenitourinary Pathology (GU, 2 questions), covering male reproductive and urinary tract conditions such as renal cell\ncarcinoma and prostate pathology.\n2.2 Model Selection and Response Generation\nFour state-of-the-art LLMs were evaluated: OpenAI o1, OpenAI o3-mini, Gemini, and DeepSeek. Each model was\nprompted independently to respond to all 15 questions in a zero-shot setting. To simulate a high-stakes clinical reasoning\n3\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 12, 2025. ; https://doi.org/10.1101/2025.04.11.25325686doi: medRxiv preprint \ntask, each question was preceded by the instruction: “This is a Pathology-related question at the level of licensing\n(board) examinations”. No additional context or few-shot examples were provided. Model responses were collected\nbetween February and March 2025 using publicly accessible web interfaces or Application Programming Interface (API)\nendpoints. Each output included both a direct answer and explanatory reasoning. No fine-tuning or post-processing was\napplied to model responses.\n2.3 Evaluation by Pathologists\nEleven expert pathologists independently evaluated the reasoning outputs generated by each LLM across 15 diagnostic\nquestions. To mitigate potential bias, all evaluations were conducted in a blinded manner, and inter-rater agreement\nwas analyzed to ensure consistency among evaluators. The evaluation assessed two complementary dimensions: the\nlinguistic quality of the responses and the application of pathology-specific diagnostic reasoning strategies. The\nevaluation focused on the following two primary criteria:\n1. Natural Language Quality Metrics: Assessments were conducted on the clarity, coherence, depth, accuracy,\nand conciseness of the responses. These metrics are commonly employed in evaluating LLM outputs to\nensure that generated content is not only factually correct but also well-structured and comprehensible. Such\nevaluation frameworks have been discussed in studies analyzing LLM performance in medical contexts [9].\n2. Application of Pathology-Specific Reasoning Strategies: Evaluators examined the extent to which LLMs\nappropriately utilized various diagnostic reasoning approaches integral to pathology practice. These approaches\ninclude pattern recognition, algorithmic reasoning, inductive hypothetico-deductive reasoning, mechanistic\ninsights, deductive reasoning, heuristic reasoning, and probabilistic (Bayesian) reasoning. The framework for\nthese reasoning strategies is grounded in the cognitive processes outlined by Pena and Andrade-Filho, who\ndescribe the diagnostic process as encompassing cognitive, communicative, normative, and medical conduct\ndomains [1, 10].\n2.4 Statistical Analysis\nScores (on a 1–5 Likert scale) were first normalized to a 0–1 range using linear scaling (score divided by 5). Following\nnormalization, average scores were computed across raters for each combination of model, question, and evaluation\ncriterion. These aggregated values were used to evaluate model-level and criterion-specific performance. One-way\nanalysis of variance (ANOV A) was used to assess differences in mean scores across models for each evaluation metric,\nincluding cumulative scores. When statistically significant, pairwise comparisons were conducted using Tukey’s Honest\nSignificant Difference (HSD) test (α = 0.05). Analyses were conducted using Python (v3.11) with standard statistical\nlibraries. To assess inter-rater reliability, the percent agreement was calculated for each Question–Model–Criterion\n(Q-M-C) combination, which was defined as the proportion of raters who selected the most common score. Ratings\nranged from 7 to 11 per Q-M-C (mean = 9.9). Model-level differences in percent agreement were tested using the\nKruskal–Wallis H test. When significant, post hoc pairwise comparisons were performed using Dunn’s test with\nBonferroni correction for multiple testing.\n3 Results\nThe performance of four state-of-the-art LLMs (Gemini, DeepSeek, OpenAI o1, and OpenAI o3-mini) on 15 expert-\ngenerated pathology questions. Responses were assessed across two primary domains: Language Quality and Response\nStructure, focused on NLP-oriented metrics (e.g., relevance, coherence, accuracy); andDiagnostic Reasoning Strategies,\ncapturing clinically grounded reasoning styles (e.g., pattern recognition, algorithmic reasoning, mechanistic insight).\nEach domain includes a cumulative score and multiple sub-metrics, yielding twelve evaluation criteria overall.\nFig. 2 summarizes model performance across these criteria using radar plots. Gemini clearly outperforms all other\nmodels on language quality metrics (Fig. 2a), with the largest margins in “depth” and “coherence”. DeepSeek ranks\nsecond overall, while the two OpenAI reasoning models show similar performance, particularly on accuracy and\nrelevance. Fig. 2b visualizes pathology-oriented reasoning strategies. Gemini again leads across most dimensions,\nespecially algorithmic and inductive reasoning. DeepSeek demonstrates competitive performance, whereas the OpenAI\nmodels score lower in mechanistic, heuristic, and Bayesian reasoning, suggesting less robust clinical reasoning\ncapabilities. These overarching trends are explored in greater detail in the sections that follow.\n4\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 12, 2025. ; https://doi.org/10.1101/2025.04.11.25325686doi: medRxiv preprint \nRelevance\nAccuracy\nHeuristic \nReasoning\nPattern \nRecognition\nAlgorithmic \nReasoning\nInductive/Hypothetico-\nDeductive Reasoning\nGemini 2.0 Flash Thinking Exp\nDeepSeek-R1 671B\nOpenAI o1\nOpenAI o3-mini\nBA\nDepth\nCoherence\nDeductive \nReasoning\nMechanistic \nInsights\nBayesian \nReasoning\nConciseness\nFigure 2: Comparative Performance of LLMs Across Language Quality and Diagnostic Reasoning Domains.\nRadar plots summarize the normalized mean scores (range: 0 to 1) assigned by 11 pathologists for each model across\n12 evaluation criteria. Panel A: Language quality metrics, including accuracy, relevance, analytical depth, coherence,\nconciseness, and cumulative reasoning quality. Panel B: Diagnostic reasoning strategies, including pattern recognition,\nalgorithmic reasoning, deductive reasoning, inductive/hypothetico-deductive reasoning, heuristic reasoning, mechanistic\ninsights, and Bayesian reasoning. Gemini consistently outperformed other models across both domains, particularly in\nanalytical depth and structured reasoning. OpenAI o1 showed the greatest variability in performance across metrics.\nAccuracy\nRelevance\nConciseness\nCumulative\nDepth\nCoherenceRelevance\nCumulative\nDepth\nAccuracy\nConciseness\nCoherence\n0\n0.2\n0.4\n0.6\n0.8\n1\nAverage Score\n0\n0.2\n0.4\n0.6\n0.8\n1\nAverage Score\n0\n0.2\n0.4\n0.6\n0.8\n1\nAverage Score\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\nAverage Score\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\nAverage Score\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\nAverage Score\n0\n0.2\n0.4\n0.6\n0.8\n1\nGemini\nDeepSeek\nOpenAI o1\nOpenAI o3-mini\nA B\nC D\nE F\nG J\nH K\nI L\nFigure 3: LLM Performance on Language Quality Metrics and Subspecialty Breakdown. Normalized average\nscores (scale: 0 to 1) across five language quality criteria and a cumulative summary, as rated by 11 pathologists across\n15 questions. Panels A–F: Cumulative performance across all five criteria (A), Accuracy (B), Relevance (C), Coherence\n(D), Analytical Depth (E), and Conciseness (F). Panels G–L: Corresponding subspecialty-level scores for each criterion.\nGemini achieved the highest scores across most metrics, with particularly strong performance in coherence and depth.\nOpenAI o1 consistently trailed in cumulative reasoning and coherence. Differences in conciseness were not statistically\nsignificant across models. Abbreviations used:\n5\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 12, 2025. ; https://doi.org/10.1101/2025.04.11.25325686doi: medRxiv preprint \nAlgorithmic Reasoning\nBayesian Reasoning\nCumulative\nInductive Hypothetico-Deductive Reasoning\nDeductive ReasoningAlgorithmic Reasoning\nCumulative\nInductive Hypothetico-Deductive Reasoning\nPattern Recognition\nBayesian Reasoning\nDeductive Reasoning\nMechanistic insightsHeuristic Reasoning Heuristic ReasoningMechanistic insights\nPattern Recognition\n0\n0.2\n0.4\n0.6\n0.8\n1\nAverage Score\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\nAverage Score\n0\n0.2\n0.4\n0.6\n0.8\n1\nAverage Score\n0\n0.2\n0.4\n0.6\n0.8\n1\nAverage Score\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\nA\nC\nE\nG\nB\nD\nF\nH\nI\nK\nM\nO\nJ\nL\nN\nP\nGemini\nDeepSeek\nOpenAI o1\nOpenAI o3-mini\nFigure 4: Diagnostic Reasoning Performance by Reasoning Type and Pathology Subspecialty. Normalized\nmean scores (scale: 0 to 1) for each LLM across seven diagnostic reasoning strategies. Panels A–H: Cumulative\nperformance across all seven criteria (A), Pattern Recognition (B), Algorithmic Reasoning (C), Deductive Reasoning (D),\nInductive/Hypothetico-Deductive Reasoning (E), Bayesian Reasoning (F), Heuristic Reasoning (G), and Mechanistic\nInsights (H). Panels I–P: Corresponding subspecialty-level scores for each reasoning strategy. Gemini and DeepSeek\ndemonstrated the strongest performance in algorithmic and inductive reasoning. All models performed less well in\nheuristic and Bayesian reasoning, highlighting challenges with uncertainty-driven and experiential inference.\n3.1 Evaluation of Language Quality and Response Structure\nFig. 3 presents model performance across the five language-focused metrics, including relevance, coherence, analytical\ndepth, accuracy, and conciseness, along with their cumulative average. Panel (a) shows cumulative scores across all\nquestions, while panels (b-f) show individual metrics. Corresponding performance by pathology subspecialty is shown\nin panels (g-l). Across all metrics, Gemini consistently achieved the highest scores. In cumulative performance (Fig. 3a),\nGemini significantly outperformed OpenAI o1, OpenAI o3-mini, and DeepSeek (p < 0.05). DeepSeek also significantly\noutperformed both OpenAI models (p < 0.05), while no significant difference was observed between OpenAI o1 and\nOpenAI o3-mini (p > 0.05).\nIn relevance (Fig. 3b), all models maintained high scores. Gemini significantly outperformed all other models (p\n< 0.05). No significant differences were observed among DeepSeek, OpenAI o1, and OpenAI o3-mini (p > 0.05).\nSubspecialty-level trends (Fig. 3h) revealed relatively uniform relevance scores across domains. Analytical depth scores\n(Fig. 3c) showed more variability. Both Gemini and DeepSeek significantly outperformed OpenAI o1 and OpenAI\no3-mini (p < 0.05). No significant differences were observed between Gemini and DeepSeek or between the two\nOpenAI models (p > 0.05). Subspecialty-level results (Fig. 3i) showed greater variation, especially in gastrointestinal\nand gynecological categories. For accuracy (Fig. 3d), Gemini significantly outperformed OpenAI o1 and DeepSeek\n(p < 0.05). No other comparisons were statistically significant (p > 0.05). Accuracy scores were otherwise consistent\nacross pathology sub-specialties as shown in Fig. 3j.\nIn coherence (Fig. 3e), Gemini and DeepSeek scored higher than the OpenAI models. Statistically significant\ndifferences were found between OpenAI o3-mini and Gemini, OpenAI o3-mini and DeepSeek, and OpenAI o1 and\n6\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 12, 2025. ; https://doi.org/10.1101/2025.04.11.25325686doi: medRxiv preprint \nDeepSeek (p < 0.05). All other comparisons were not significant (p > 0.05). Subspecialty breakdowns (Fig. 3k) show\nmodest differences, with Gemini achieving the most consistent coherence. Lastly, conciseness scores (Fig. 3f) varied\nacross models, but no statistically significant differences were observed across any pairwise comparisons (p > 0.05).\nSubspecialty analysis (Fig. 3l) revealed similar patterns across domains, with no model consistently outperforming\nothers in response brevity.\n3.2 Evaluation of Diagnostic Reasoning Strategies\nFig.4 shows model performance on clinically relevant reasoning. Panel (a) shows the cumulative reasoning score\naveraged across all reasoning categories and questions, while panel (i) displays cumulative scores disaggregated by\npathology subspecialty. Individual reasoning styles are shown in panels (b-h), with corresponding subspecialty-specific\nbreakdowns in panels (j-p). The seven evaluated reasoning types include pattern recognition, algorithmic reasoning,\ndeductive reasoning, inductive-hypothetico-deductive reasoning, Bayesian reasoning, mechanistic insights, and heuristic\nreasoning. In cumulative performance (Fig. 4a), Gemini achieved the highest overall score, followed by DeepSeek,\nwith both OpenAI models scoring lower. Pairwise comparisons showed significant differences between Gemini and all\nother models (p < 0.05), as well as between DeepSeek and both OpenAI variants (p < 0.05). No significant difference\nwas found between OpenAI o1 and OpenAI o3-mini (p > 0.05). When broken down by pathology subspecialty (Fig.\n4i), the same ranking held consistently, although performance differences varied by subspecialty.\nIn pattern recognition (Fig. 4b), Gemini significantly outperformed all other models (p < 0.05). Differences between\nDeepSeek and the OpenAI models were not statistically significant (p > 0.05). Subspecialty-level results (Fig. 4j)\nconfirmed Gemini’s leading performance across most domains. Algorithmic reasoning scores (Fig. 4c) were the highest\namong all reasoning types. Gemini and DeepSeek significantly outperformed both OpenAI models (p < 0.05), while\nno significant difference was observed between Gemini and DeepSeek (p > 0.05). Performance across pathology\nsubspecialties (Fig. 4k) remained stable for both leading models.\nIn deductive reasoning (Fig. 4d), Gemini significantly outperformed OpenAI o1 and OpenAI o3-mini (p < 0.05),\nand DeepSeek significantly outperformed OpenAI o3-mini (p < 0.05). No significant differences were observed\nbetween Gemini and DeepSeek or between OpenAI o1 and OpenAI o3-mini (p > 0.05). These trends were mirrored in\nsubspecialty results (Fig. 4l). Performance in inductive-hypothetico-deductive reasoning (Fig. 4e) followed a similar\npattern. Gemini and DeepSeek significantly outperformed both OpenAI models (p < 0.05), with no significant difference\nbetween Gemini and DeepSeek (p > 0.05). Subspecialty-specific scores (Fig. 4m) reaffirmed this ranking.\nBayesian (probabilistic) reasoning scores (Fig. 4f) were generally lower across all models. Gemini significantly\noutperformed both GPT models (p < 0.05), while no significant differences were observed between Gemini and\nDeepSeek or between DeepSeek and the OpenAI models (p > 0.05). Similar patterns were observed across pathology\ndomains in Fig. 4n. In mechanistic insights (Fig. 4g), Gemini significantly outperformed OpenAI o1 and OpenAI\no3-mini (p < 0.05). DeepSeek significantly outperformed OpenAI o3-mini (p < 0.05), while other comparisons were\nnot significant (p > 0.05). Domain-level breakdowns (Fig. 4o) revealed similar trends.\nFinally, heuristic reasoning (Fig. 4h) yielded the lowest scores across all models. Gemini and DeepSeek significantly\noutperformed OpenAI o1 and OpenAI o3-mini (p < 0.05), with no significant difference observed between Gemini and\nDeepSeek or between the two GPT models (p > 0.05). These results were consistent across pathology subspecialties as\nshown in Fig. 4p.\n3.3 Inter-Rater Percent Agreement Across Models\nPercent agreement was analyzed for 720 unique Q-M-C combinations across four models: Gemini, DeepSeek, OpenAI\no1, and OpenAI o3-mini. The overall mean percent agreement was 0.49 (range: 0.25-0.91). Fig. 5A shows a full\ndistribution of percent agreement, with model-specific distributions shown in Fig. 5B. Percent agreement significantly\ndiffered across models (Kruskal-Wallis H = 76.798, p < 0.001). Post-hoc pairwise Dunn’s tests revealed that Gemini had\nsignificantly higher percent agreement than all other models (p < 0.001 for all comparisons). No significant differences\nwere found among DeepSeek, OpenAI o1, and OpenAI o3-mini (p > 0.05).\n4 Discussion\nThis study evaluated the ability of four state-of-the-art LLMs to perform pathology-specific diagnostic reasoning using\nexpert assessments of both language quality and clinical reasoning strategies. While all models produced generally\nrelevant and accurate responses, only Gemini and DeepSeek demonstrated consistent strength across multiple reasoning\ndimensions, particularly analytical depth, coherence, and algorithmic reasoning. Moreover, Gemini achieved the highest\n7\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 12, 2025. ; https://doi.org/10.1101/2025.04.11.25325686doi: medRxiv preprint \nA\nB\n0\n20\n40\n60\n80\n100Number of question-model-criteria combinations\n0 0.2 0.4 0.6 0.8 1.0\n0\n0.2\n0.4\n0.6\n0.8\n1\nPercentage Agreement\nPercentage Agreement\nGemini OpenAI o3-miniOpenAI o1DeepSeek\nB\nModel\nFigure 5: Percent agreement across 720 unique combinations of question, model, and evaluation criterion (Q–M–C),\nreflecting the proportion of raters who selected the most common score. Panel A: Distribution of percent agreement\nacross all Q–M–C combinations. Panel B: Model-specific distributions of agreement. Gemini achieved significantly\nhigher inter-rater agreement compared to all other models (p < 0.001), suggesting greater consistency and interpretability\nof its outputs. No statistically significant differences were observed in pair-wise testing between DeepSeek, OpenAI o1,\nand OpenAI o3-mini.\ninter-rater agreement among expert pathologists, suggesting greater clarity and interpretability in its responses. These\nfindings highlight the need to move beyond accuracy-focused evaluation and toward models that generate clinically\ncoherent and contextually intelligible reasoning, especially in high-stakes diagnostic domains such as pathology. This\nanalysis also revealed substantial variation in the reasoning performance of LLMs when applied to pathology-focused\ndiagnostic questions. Among the four evaluated models, Gemini consistently achieved the highest scores across both\nevaluation domains (language quality and diagnostic reasoning). DeepSeek followed closely in most categories, while\nthe OpenAI variants (o1 and o3-mini) generally performed lower compared to the other LLMs, particularly in analytical\ndepth, coherence, and advanced reasoning types.\nIn the language quality domain, all models produced broadly relevant responses. Gemini and DeepSeek distinguished\nthemselves in analytical depth, coherence, and cumulative performance. Gemini also significantly outperformed other\nmodels in accuracy, suggesting more robust and precise responses. While the OpenAI models tended to be more concise,\nthese differences were not statistically significant and often came at the expense of analytical depth and interpretability.\nIn the diagnostic reasoning domain, all models performed best on algorithmic and deductive reasoning tasks. Gemini\nconsistently outperformed all others across most reasoning types. DeepSeek also performed well in several categories,\nthough its advantage over the OpenAI models was less pronounced in pattern recognition. Performance dropped\nmarkedly for all models in nuanced reasoning types such as heuristic, mechanistic, and Bayesian reasoning. These\nareas typically require integrating implicit knowledge, clinical judgment, and experience-based decision-making.\nNotably, inter-rater agreement was highest for Gemini, indicating that its responses were more consistently interpreted\nand rated by expert pathologists. This suggests that Gemini’s outputs may be not only more complete or coherent but\nalso more reliably understandable to clinical end-users. Such consistency is essential for downstream applications of\nLLMs in real-world diagnostic settings, where human-AI collaboration depends on shared reasoning clarity.\nThe prior evaluations of LLMs in medicine have focused on factual correctness, clinical acceptability, or performance\non standardized assessments such as the USMLE and other multiple-choice examinations [5, 6, 7, 11]. While these\nstudies demonstrate that advanced models can retrieve and generate clinically accurate information, they typically\nframe evaluation in binary terms, right or wrong, without assessing the structure, context, or interpretability of the\nunderlying reasoning processes. However, recent studies have begun to explore how generative AI might support\nmedical reasoning in pathology and related domains. Waqas et al. have outlined the promise of LLMs and FMs for\ndigital pathology applications such as structured reporting, clinical summarization, and educational simulations [4, 3].\nBrodsky et al. emphasized the importance of interpretability and reasoning traceability in anatomic pathology, noting\nthat AI outputs must not only be accurate but also reflect logical, clinically meaningful reasoning patterns [2]. However,\nthese studies primarily describe use cases or propose conceptual frameworks rather than empirically evaluating whether\nLLMs demonstrate structured reasoning aligned with human diagnostic strategies. Few studies have assessed whether\nmodel outputs reflect the types of reasoning, such as algorithmic, heuristic, or mechanistic, that are integral to expert\npathology practice.\n8\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 12, 2025. ; https://doi.org/10.1101/2025.04.11.25325686doi: medRxiv preprint \nHistologic diagnosis is a cognitively demanding task that requires the integration of diverse information sources,\nranging from visual features on tissue slides to clinical history and disease biology. As part of this decision-making\nprocess, pathologists rely on a broad set of reasoning strategies, including pattern recognition, algorithmic workflows,\nhypothetico-deductive reasoning, Bayesian inference, and heuristics, often applied fluidly based on experience and\ncontext [1, 12]. Despite their central role in diagnostic workflows, these reasoning approaches are rarely formalized in\nLLM evaluation studies. While prior cognitive and educational research has documented these strategies in pathology\nand clinical decision-making [13, 14, 15], few studies have translated them into a structured framework for evaluating\nAI systems in this domain. This study addresses this gap by developing and applying a structured evaluation framework\nthat operationalizes seven clinically grounded reasoning strategies, i.e., pattern recognition, algorithmic reasoning,\ndeductive and inductive reasoning, Bayesian reasoning, mechanistic insights, and heuristic reasoning. In contrast to\nprior work that has focused on factual accuracy or subjective plausibility, we used expert raters to assess whether model\nresponses actually reflected reasoning types that pathologists would apply during their decision-making process. We\nalso included five language quality metrics to assess the structure and interpretability of responses, features that are\ncritical for clinical adoption but often overlooked in model evaluation. This approach builds on prior conceptual work in\ngenerative AI for pathology [2, 4, 3], but moves beyond descriptive analysis to provide a domain-specific, empirically\ntested benchmark for reasoning assessment.\nThe results of this study show that LLMs perform relatively well in algorithmic and deductive reasoning, likely reflecting\ntheir ability to retrieve structured clinical knowledge from training data and guidelines [16, 17, 6, 18]. However, model\nperformance was markedly lower in reasoning types that rely more heavily on experience, context, and uncertainty,\nsuch as heuristic reasoning, Bayesian inference, and mechanistic understanding [\n19, 20]. These findings align with\nprevious observations that current LLMs struggle with nuanced clinical reasoning, particularly in settings that require\nadaptive judgment [21, 22, 23, 24]. Moreover, our analysis highlights the lack of metacognitive regulation in LLMs,\ntheir inability to recognize uncertainty, self-correct, or reconcile conflicting information, challenges that have been noted\nin other domains as well [25]. By incorporating expert adjudication, reasoning taxonomies, and inter-rater agreement,\nour study offers a new model for evaluating clinical reasoning in generative AI that extends beyond correctness to assess\nthe fidelity and structure of reasoning itself.\nOur findings have important implications for the safe and effective integration of LLMs into clinical workflows,\nparticularly in diagnostic fields such as pathology, where interpretability, trust, and domain-specific reasoning are\nparamount. While all four models evaluated in this study produced largely relevant and accurate responses, only Gemini,\nand to a slightly lesser extent DeepSeek, demonstrated reasoning structures demonstrated reasoning structures that\naligned with real-world diagnostic approaches. These models also achieved higher inter-rater agreement among expert\npathologists, suggesting that their outputs were not only well-structured but also consistently interpretable across users.\nThis consistency is critical in clinical environments where diagnostic decisions are collaborative and where AI-generated\ninsights must be readily understood, validated, and acted upon by human experts. In contrast, models with lower\ncoherence and reasoning clarity may increase cognitive burden, introduce ambiguity, or erode trust, especially when\ndeployed in high-stakes or time-constrained settings. By highlighting variation in reasoning quality across models and\nacross different types of diagnostic logic, our results suggest that LLM evaluation should go beyond factual correctness\nand consider reasoning fidelity as a core requirement. This is particularly relevant for educational, triage, or decision\nsupport applications in pathology, where nuanced reasoning is essential, and automation must complement, rather than\nobscure, human expertise.\nWe acknowledge that this study has some limitations. Although a diverse set of reasoning strategies was evaluated, this\nanalysis was limited to 15 open-ended questions. While these were curated by expert pathologists across subspecialties,\nthey may not fully capture the range of diagnostic challenges encountered in practice. Future iterations of this benchmark\nshould include a broader array of cases, including rare and diagnostically complex scenarios. We focused solely on\ntext-based LLMs and did not assess multimodal models that incorporate pathology images. The use of expert raters\n(i.e., pathologists) adds clinical realism but also introduces subjectivity. We also did not measure downstream clinical\nimpact, such as improvements in diagnostic accuracy or workflow efficiency. Future studies should explore these\noutcomes prospectively. In addition, the effects of prompting strategies, fine-tuning, and domain adaptation on reasoning\nperformance deserve further investigation. Reasoning-aligned LLMs may hold promise for educational use, simulation,\nand decision support, applications that require careful validation in real-world settings.\n5 Conclusion\nThis study presents a structured evaluation of advanced reasoning LLMs in pathology, focusing not only on factual\naccuracy but also on diagnostic reasoning quality. By assessing model outputs across five language metrics and seven\nclinically grounded reasoning strategies, we provide a nuanced benchmark that goes beyond traditional correctness-\nbased evaluations. These findings demonstrate that while all models produce largely relevant and accurate responses,\n9\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 12, 2025. ; https://doi.org/10.1101/2025.04.11.25325686doi: medRxiv preprint \nsubstantial differences exist in their ability to emulate expert diagnostic reasoning. Gemini and DeepSeek outperformed\nother models in both reasoning fidelity and inter-rater agreement, suggesting greater alignment with clinical expecta-\ntions. Conversely, current limitations in heuristic, mechanistic, and probabilistic reasoning highlight areas for model\nimprovement. To our knowledge, this is the first study to systematically evaluate the diagnostic reasoning capabilities\nof LLMs in pathology using expert-annotated criteria grounded in clinical practice. As generative AI tools continue to\nevolve, clinically meaningful evaluation frameworks, such as the one introduced here, will be essential for guiding safe\nand effective integration into diagnostic practice. This work lays the foundation for future studies that seek to assess not\njust whether models are right but whether they reason like clinicians.\nAcknowledgments\nThe research was supported in part by a Cancer Center Support Grant at the H. Lee Moffitt Cancer Center & Research\nInstitute, awarded by the NIH/NCI (P30-CA76292), a Florida Biomedical Research Grant (21B12), an NIH/NCI grant\n(U01-CA200464), NSF Awards 2234836 and 2234468 and NAIRR pilot funding.\nReferences\n[1] Gil Patrus Pena and Josede Souza Andrade-Filho. How does a pathologist make a diagnosis? Archives of\nPathology and Laboratory Medicine, 133(1):124–132, January 2009.\n[2] Victor Brodsky, Ehsan Ullah, Andrey Bychkov, Andrew H. Song, Eric E. Walk, Peter Louis, Ghulam Rasool,\nRajendra S. Singh, Faisal Mahmood, Marilyn M. Bui, and Anil V . Parwani. Generative artificial intelligence in\nanatomic pathology. Archives of Pathology and Laboratory Medicine, 149(4):298–318, January 2025.\n[3] Asim Waqas, Javeria Naveed, Warda Shahnawaz, Shoaib Asghar, Marilyn M Bui, and Ghulam Rasool. Digital\npathology and multimodal learning on oncology data. BJR|Artificial Intelligence, 1(1), January 2024.\n[4] Asim Waqas, Marilyn M. Bui, Eric F. Glassy, Issam El Naqa, Piotr Borkowski, Andrew A. Borkowski, and Ghulam\nRasool. Revolutionizing digital pathology with the power of generative artificial intelligence and foundation\nmodels. Laboratory Investigation, 103(11):100255, November 2023.\n[5] Jonathan Wang and Donald A. Redelmeier. Cognitive biases and artificial intelligence.NEJM AI, 1(12), November\n2024.\n[6] Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales,\nAjay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly,\nAbubakr Babiker, Nathanael Schärli, Aakanksha Chowdhery, Philip Mansfield, Dina Demner-Fushman, Blaise\nAgüera y Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev,\nYun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. Large\nlanguage models encode clinical knowledge. Nature, 620(7972):172–180, July 2023.\n[7] Dana Brin, Vera Sorin, Akhil Vaid, Ali Soroush, Benjamin S. Glicksberg, Alexander W. Charney, Girish Nadkarni,\nand Eyal Klang. Comparing chatgpt and gpt-4 performance in usmle soft skill assessments. Scientific Reports,\n13(1), October 2023.\n[8] Andrew Y Wang, Sherman Lin, Christopher Tran, Robert J Homer, Dan Wilsdon, Joanna C Walsh, Emily A\nGoebel, Irene Sansano, Snehal Sonawane, Vincent Cockenpot, et al. Assessment of pathology domain-specific\nknowledge of chatgpt and comparison to human performance. Archives of pathology & laboratory medicine,\n148(10):1152–1158, 2024.\n[9] Junbok Lee, Sungkyung Park, Jaeyong Shin, and Belong Cho. Analyzing evaluation methods for large language\nmodels in the medical field: a scoping review. BMC Medical Informatics and Decision Making, 24(1), November\n2024.\n[10] Junhyuk Seo, Dasol Choi, Taerim Kim, Won Chul Cha, Minha Kim, Haanju Yoo, Namkee Oh, YongJin Yi,\nKye Hwa Lee, and Edward Choi. Evaluation framework of large language models in medical documentation:\nDevelopment and usability study. Journal of Medical Internet Research, 26:e58329, November 2024.\n[11] Thomas Yu Chow Tam, Sonish Sivarajkumar, Sumit Kapoor, Alisa V . Stolyar, Katelyn Polanska, Karleigh R.\nMcCarthy, Hunter Osterhoudt, Xizhi Wu, Shyam Visweswaran, Sunyang Fu, Piyush Mathur, Giovanni E. Caccia-\nmani, Cong Sun, Yifan Peng, and Yanshan Wang. A framework for human evaluation of large language models in\nhealthcare derived from literature review. npj Digital Medicine, 7(1), September 2024.\n[12] Elliott Foucar. Diagnostic decision-making in anatomic pathology. Pathology Patterns Reviews,\n116(suppl1):S21–S33, December 2001.\n10\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 12, 2025. ; https://doi.org/10.1101/2025.04.11.25325686doi: medRxiv preprint \n[13] W. Schlegel and K. Kayser. Pattern recognition in histo-pathology: Basic considerations. Methods of Information\nin Medicine, 21(01):15–22, January 1982.\n[14] Dmitriy Shin, Mikhail Kovalenko, Ilker Ersoy, Yu Li, Donald Doll, Chi-Ren Shyu, and Richard Hammer. Pathedex\n– uncovering high-explanatory visual diagnostics heuristics using digital pathology and multiscale gaze data.\nJournal of Pathology Informatics, 8(1):29, January 2017.\n[15] Rebecca S. Crowley, Elizabeth Legowski, Olga Medvedeva, Kayse Reitmeyer, Eugene Tseytlin, Melissa Castine,\nDrazen Jukic, and Claudia Mello-Thoms. Automated detection of heuristics and biases among pathologists in a\ncomputer-based system. Advances in Health Sciences Education, 18(3):343–363, May 2012.\n[16] Stephen Spencer Raab. The current and ideal state of anatomic pathology patient safety. Diagnosis, 1(1):95–97,\nJanuary 2014.\n[17] Peter W Hamilton, Paul J van Diest, Richard Williams, and Anthony G Gallagher. Do we see what we think we\nsee? the complexities of morphological assessment. The Journal of Pathology, 218(3):285–291, March 2009.\n[18] Daniel Truhn, Jorge S. Reis-Filho, and Jakob Nikolas Kather. Large language models should be used as scientific\nreasoning engines, not knowledge databases. Nature Medicine, 29(12):2983–2984, October 2023.\n[19] Amogh Ananda Rao, Milind Awale, and Sissmol Davis. Medical diagnosis reimagined as a process of bayesian\nreasoning and elimination. Cureus, September 2023.\n[20] Simone Arvisais-Anhalt, Steven L. Gonias, and Sara G. Murray. Establishing priorities for implementation of\nlarge language models in pathology and laboratory medicine. Academic Pathology, 11(1):100101, January 2024.\n[21] Ethan Goh, Robert Gallo, Jason Hom, Eric Strong, Yingjie Weng, Hannah Kerman, Joséphine A. Cool, Zahir\nKanjee, Andrew S. Parsons, Neera Ahuja, Eric Horvitz, Daniel Yang, Arnold Milstein, Andrew P. J. Olson, Adam\nRodman, and Jonathan H. Chen. Large language model influence on diagnostic reasoning: A randomized clinical\ntrial. JAMA Network Open, 7(10):e2440969, October 2024.\n[22] Jean-Christophe Bélisle-Pipon. Why we need to be careful with llms in medicine. Frontiers in Medicine, 11,\nDecember 2024.\n[23] Stephanie Cabral, Daniel Restrepo, Zahir Kanjee, Philip Wilson, Byron Crowe, Raja-Elie Abdulnour, and Adam\nRodman. Clinical reasoning of a generative artificial intelligence model compared with physicians. JAMA Internal\nMedicine, 184(5):581, May 2024.\n[24] Jonathan Kim, Anna Podlasek, Kie Shidara, Feng Liu, Ahmed Alaa, and Danilo Bernardo. Limitations of large\nlanguage models in clinical problem-solving arising from inflexible reasoning, 2025.\n[25] Lev Tankelevitch, Viktor Kewenig, Auste Simkute, Ava Elizabeth Scott, Advait Sarkar, Abigail Sellen, and Sean\nRintel. The metacognitive demands and opportunities of generative ai. In Proceedings of the CHI Conference on\nHuman Factors in Computing Systems, CHI 24, page 1–24. ACM, May 2024.\n11\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 12, 2025. ; https://doi.org/10.1101/2025.04.11.25325686doi: medRxiv preprint \nSupplementary 1: Details of Evaluated Large Language Models\nGemini 2.0 Flash Thinking Experimental\nGemini 2.0 Flash Thinking Experimental is an advanced AI model developed by Google DeepMind. This model is\ndesigned to balance reasoning capabilities with speed by employing an internal \"thinking process\" during response\ngeneration. This approach enhances the model’s ability to handle complex tasks, particularly in mathematics, science,\nand multimodal reasoning. Benchmark evaluations have demonstrated significant improvements over previous models,\nshowcasing enhanced performance and explainability. Developers can access this experimental model through the\nGemini API in Google AI Studio. Key features include: (1) Enhanced Reasoning: Utilizes an internal \"thinking process\"\nto improve problem-solving capabilities. (2) Speed and Efficiency: Optimized to balance complex reasoning with rapid\nresponse generation. (3) Multimodal Capabilities: Excels in tasks involving text, code, and images. Details are available\nat: https://deepmind.google/technologies/gemini/flash-thinking/\nDeepSeek-R1 671B\nDeepSeek-R1 671B is an LLM developed by the Chinese AI startup DeepSeek. Released in January 2025, it features a\nMixture-of-Experts (MoE) architecture with a total of 671 billion parameters, of which 37 billion are activated per token\nduring inference. This design enhances resource efficiency without compromising performance. DeepSeek-R1 has\ndemonstrated capabilities comparable to leading models in tasks such as mathematics, coding, and complex reasoning.\nKey features include: (1) Mixture-of-Experts Architecture: Efficiently utilizes a subset of parameters during inference\nfor optimized performance. (2) High Parameter Count: Among the largest open-source LLMs, facilitating advanced\nreasoning tasks. (3) Open-Source Availability: Supports the research community with accessible model weights and\ncode. Further details are available at: https://huggingface.co/deepseek-ai/DeepSeek-R1\nOpenAI o1\nOpenAI o1 is a reasoning-focused AI model developed by OpenAI, officially released on December 5, 2024. It is\ndesigned to allocate additional processing time before generating responses, thereby improving performance on complex\ntasks, including science, coding, and mathematics. The model supports chain-of-thought prompting and multi-step\nreasoning, enhancing its interpretability and accuracy. Key features include: (1) Deliberative Processing: Spends\nmore time \"thinking\" before responding to enhance reasoning quality. (2) Chain-of-Thought Prompting: Capable of\nbreaking down complex problems into intermediate steps. (3) Versatile Applications: Excels in tasks requiring deep\nunderstanding and logical analysis. Further details are available at: https://openai.com/o1/\nOpenAI o3-mini\nOpenAI o3-mini is a compact reasoning model introduced by OpenAI on January 31, 2025. It aims to provide enhanced\nreasoning capabilities with reduced computational requirements, making it suitable for applications where resources are\nlimited. Despite its smaller size, o3-mini can outperform o1 in coding and other reasoning tasks, offering a balance\nbetween performance and efficiency. Key features include: (1) Resource Efficiency: Optimized for lower computational\noverhead without significant performance trade-offs. (2) Adjustable Reasoning Effort: Offers settings to balance speed\nand depth of reasoning. (3) Specialized Domains: Particularly adept in STEM-related tasks, including coding and\nmathematics. Further details are available: https://openai.com/index/openai-o3-mini/\n12\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 12, 2025. ; https://doi.org/10.1101/2025.04.11.25325686doi: medRxiv preprint \nSupplementary 2. Pathology Board-Style Questions and Subspecialties\nTable 1: Diagnostic pathology questions used in the evaluation and their associated subspecialties.\nQ# Question Subspecialty\nQ1 What is the most common resistance mechanism to first-generation\nEGFR inhibitors in lung cancer?\nPulmonary\nQ2 How specific is TTF-1 as a marker of lung cancer differentiation? Pulmonary\nQ3 Describe the histologic features of adult-type fibroadenoma of the breast. Gynecologic\nQ4 What is the approach to diagnosis of papillary lesions of the breast? Gynecologic\nQ5 What is the name of the disease that can present with jaundice, Kayser-\nFleischer rings, and neurological symptoms, and causes increased copper\naccumulation in hepatocytes?\nGastrointestinal\nQ6 What are the clinical and histopathological differences between HPV-\npositive and HPV-negative oropharyngeal squamous cell carcinoma?\nHead and Neck\nQ7 What is the most common molecular alteration in classic papillary thy-\nroid carcinoma?\nEndocrine\nQ8 Peutz-Jeghers syndrome is caused by a gene named ___. Gastrointestinal\nQ9 What are the histologic features of usual interstitial pneumonia? Pulmonary\nQ10 An adenocarcinoma is positive for TTF-1, napsin A, and PAX-8. What\nare the possible sites of origin?\nGenitourinary\nQ11 What are the criteria for conventional uterine leiomyosarcoma? Gynecologic\nQ12 Excluding the GI tract, list four body sites that are at risk for development\nof carcinoma in patients with Peutz-Jeghers syndrome.\nGastrointestinal\nQ13 What is the recent antibody implicated in primary podocytopathy? Genitourinary\nQ14 What is the molecular classification of breast carcinomas? Gynecologic\nQ15 What are the immunohistochemical and molecular characteristics of\nmesonephric-like adenocarcinoma of the uterus?\nGynecologic\n13\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 12, 2025. ; https://doi.org/10.1101/2025.04.11.25325686doi: medRxiv preprint \nSupplementary 3. Per-Question Model Performance\nThis section presents per-question performance scores for each LLM, visualized separately for (1) language quality\nand structure and (2) diagnostic reasoning strategies. Each point represents the average normalized score from expert\nratings across the four models on a single diagnostic question.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nQuestions\nAccuracy\nRelevance\nConciseness\nCumulative\nDepth\nCoherence\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nQuestions\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nNormalized Average Scores\n0\n0.2\n0.4\n0.8\n1\n0.6Normalized Average Scores\n0\n0.2\n0.4\n0.8\n1\n0.6Normalized Average Scores\n0\n0.2\n0.4\n0.8\n1\n0.6\n0\n0.2\n0.4\n0.8\n1\n0.6\n0\n0.2\n0.4\n0.8\n1\n0.6\n0\n0.2\n0.4\n0.8\n1\n0.6\nA\nC\nE\nB\nD\nF\nGemini\nDeepSeek\nOpenAI o1\nOpenAI o3-mini\nFigure 6: Pathologist Ratings of LLMs Across Language Quality Metrics. Normalized average scores (scale: 0–1)\nassigned by 11 pathologists across five natural language quality metrics: Accuracy, Relevance, Coherence, Conciseness,\nand Depth. Each point represents the mean score for a specific model on a single question. Gemini shows consistently\nstrong performance, while other models exhibit greater variability across questions and metrics.\n14\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 12, 2025. ; https://doi.org/10.1101/2025.04.11.25325686doi: medRxiv preprint \n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nQuestions Questions\nNormalized Average Scores\n0\n0.2\n0.4\n0.8\n1\n0.6\n0\n0.2\n0.4\n0.8\n1\n0.6\nCumulative\nAlgorithmic Reasoning\nInductive Hypothetico-Deductive Reasoning\nHeuristic Reasoning\nPattern Recognition\nDeductive Reasoning\nMechanistic insights\nBayesian Reasoning\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nNormalized Average Scores\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 151 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 151 2 3 4 5 6 7 8 9 10 11 12 13 14 15\nNormalized Average Scores\n0\n0.2\n0.4\n0.8\n1\n0.6\nNormalized Average Scores\n0\n0.2\n0.4\n0.8\n1\n0.6\nNormalized Average Scores\n0\n0.2\n0.4\n0.8\n1\n0.6\nNormalized Average Scores\n0\n0.2\n0.4\n0.8\n1\n0.6\nNormalized Average Scores\n0\n0.2\n0.4\n0.8\n1\n0.6\nNormalized Average Scores\n0\n0.2\n0.4\n0.8\n1\n0.6\nA\nC\nE\nB\nD\nF\nG H\nGemini\nDeepSeek\nOpenAI o1\nOpenAI o3-mini\nFigure 7: Pathologist Ratings of LLMs Across Diagnostic Reasoning Strategies. Normalized average scores\n(scale: 0–1) assigned across seven reasoning strategy criteria: Algorithmic, Pattern Recognition, Deductive, Inductive,\nBayesian, Heuristic, and Mechanistic. Models vary in their use of domain-specific reasoning, with Gemini achieving\nhigher and more consistent ratings across questions.\n15\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 12, 2025. ; https://doi.org/10.1101/2025.04.11.25325686doi: medRxiv preprint \nSupplementary 4. Tukey’s Honestly Significant Difference (HSD) Pairwise Comparisons\nTable 2: Pairwise Comparisons for Language Quality and Response Structure metrics.\nMetric Comparison p-value Significant\n0 Relevance Gemini vs DeepSeek < 0.0001 Yes\n1 Relevance Gemini vs OpenAI o1 < 0.0001 Yes\n2 Relevance Gemini vs OpenAI o3-mini < 0.0001 Yes\n3 Relevance DeepSeek vs OpenAI o1 0.2054 No\n4 Relevance DeepSeek vs OpenAI o3-mini 0.1935 No\n5 Relevance OpenAI o1 vs OpenAI o3-mini > 0.9999 No\n6 Coherence Gemini vs DeepSeek < 0.0001 Yes\n7 Coherence Gemini vs OpenAI o1 < 0.0001 Yes\n8 Coherence Gemini vs OpenAI o3-mini < 0.0001 Yes\n9 Coherence DeepSeek vs OpenAI o1 0.0847 No\n10 Coherence DeepSeek vs OpenAI o3-mini 0.8825 No\n11 Coherence OpenAI o1 vs OpenAI o3-mini 0.3408 No\n12 Depth Gemini vs DeepSeek < 0.0001 Yes\n13 Depth Gemini vs OpenAI o1 < 0.0001 Yes\n14 Depth Gemini vs OpenAI o3-mini < 0.0001 Yes\n15 Depth DeepSeek vs OpenAI o1 0.0108 Yes\n16 Depth DeepSeek vs OpenAI o3-mini 0.0009 Yes\n17 Depth OpenAI o1 vs OpenAI o3-mini 0.8415 No\n18 Accuracy Gemini vs DeepSeek 0.0046 Yes\n19 Accuracy Gemini vs OpenAI o1 0.0076 Yes\n20 Accuracy Gemini vs OpenAI o3-mini 0.001 Yes\n21 Accuracy DeepSeek vs OpenAI o1 0.9982 No\n22 Accuracy DeepSeek vs OpenAI o3-mini 0.9579 No\n23 Accuracy OpenAI o1 vs OpenAI o3-mini 0.9057 No\n24 Conciseness Gemini vs DeepSeek 0.9804 No\n25 Conciseness Gemini vs OpenAI o1 0.8648 No\n26 Conciseness Gemini vs OpenAI o3-mini 0.4136 No\n27 Conciseness DeepSeek vs OpenAI o1 0.9794 No\n28 Conciseness DeepSeek vs OpenAI o3-mini 0.6494 No\n29 Conciseness OpenAI o1 vs OpenAI o3-mini 0.8648 No\n30 Cumulative Gemini vs DeepSeek 0.0212 Yes\n31 Cumulative Gemini vs OpenAI o1 < 0.0001 Yes\n32 Cumulative Gemini vs OpenAI o3-mini < 0.0001 Yes\n33 Cumulative DeepSeek vs OpenAI o1 < 0.0001 Yes\n34 Cumulative DeepSeek vs OpenAI o3-mini < 0.0001 Yes\n35 Cumulative OpenAI o1 vs OpenAI o3-mini 0.9901 No\n16\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 12, 2025. ; https://doi.org/10.1101/2025.04.11.25325686doi: medRxiv preprint \nTable 3: Pairwise Comparisons for Diagnostic Reasoning Strategies.\nMetric Comparison p-value Significant\n0 Pattern Recognition Gemini vs DeepSeek 0.0054 Yes\n1 Pattern Recognition Gemini vs OpenAI o1 < 0.0001 Yes\n2 Pattern Recognition Gemini vs OpenAI o3-mini < 0.0001 Yes\n3 Pattern Recognition DeepSeek vs OpenAI o1 0.0738 No\n4 Pattern Recognition DeepSeek vs OpenAI o3-mini 0.2573 No\n5 Pattern Recognition OpenAI o1 vs OpenAI o3-mini 0.9243 No\n6 Algorithmic Reasoning Gemini vs OpenAI o1 < 0.0001 Yes\n7 Algorithmic Reasoning Gemini vs OpenAI o3-mini < 0.0001 Yes\n8 Algorithmic Reasoning DeepSeek vs OpenAI o1 < 0.0001 Yes\n9 Algorithmic Reasoning DeepSeek vs OpenAI o3-mini < 0.0001 Yes\n10 Deductive Reasoning Gemini vs OpenAI o1 < 0.0001 Yes\n11 Deductive Reasoning Gemini vs OpenAI o3-mini < 0.0001 Yes\n12 Deductive Reasoning DeepSeek vs OpenAI o1 0.0001 Yes\n13 Deductive Reasoning DeepSeek vs OpenAI o3-mini < 0.0001 Yes\n14 Inductive Hypothetico-Deductive Reasoning Gemini vs OpenAI o1 < 0.0001 Yes\n15 Inductive Hypothetico-Deductive Reasoning Gemini vs OpenAI o3-mini < 0.0001 Yes\n16 Inductive Hypothetico-Deductive Reasoning DeepSeek vs OpenAI o1 < 0.0001 Yes\n17 Inductive Hypothetico-Deductive Reasoning DeepSeek vs OpenAI o3-mini < 0.0001 Yes\n18 Mechanistic Insights Gemini vs DeepSeek 0.0139 Yes\n19 Mechanistic Insights Gemini vs OpenAI o1 < 0.0001 Yes\n20 Mechanistic Insights Gemini vs OpenAI o3-mini < 0.0001 Yes\n21 Mechanistic Insights DeepSeek vs OpenAI o1 < 0.0001 Yes\n22 Mechanistic Insights DeepSeek vs OpenAI o3-mini < 0.0001 Yes\n23 Heuristic Reasoning Gemini vs OpenAI o1 < 0.0001 Yes\n24 Heuristic Reasoning Gemini vs OpenAI o3-mini < 0.0001 Yes\n25 Heuristic Reasoning DeepSeek vs OpenAI o1 0.0002 Yes\n26 Heuristic Reasoning DeepSeek vs OpenAI o3-mini 0.0003 Yes\n27 Bayesian Reasoning Gemini vs DeepSeek 0.0261 Yes\n28 Bayesian Reasoning Gemini vs OpenAI o1 < 0.0001 Yes\n29 Bayesian Reasoning Gemini vs OpenAI o3-mini < 0.0001 Yes\n30 Bayesian Reasoning DeepSeek vs OpenAI o1 0.0003 Yes\n31 Bayesian Reasoning DeepSeek vs OpenAI o3-mini < 0.0001 Yes\n32 Cumulative Gemini vs DeepSeek 0.0212 Yes\n33 Cumulative Gemini vs OpenAI o1 < 0.0001 Yes\n34 Cumulative Gemini vs OpenAI o3-mini < 0.0001 Yes\n35 Cumulative DeepSeek vs OpenAI o1 < 0.0001 Yes\n36 Cumulative DeepSeek vs OpenAI o3-mini < 0.0001 Yes\n17\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 12, 2025. ; https://doi.org/10.1101/2025.04.11.25325686doi: medRxiv preprint \nSupplementary 5. Inter-Rater Agreement Analysis\nTable 4: Summary of Percent Agreement Across Models. Question-Model-Criteria (Q-M-C)\nModel Mean Percent Agreement (SD) Q-M-C with Percent Agreement < 0.4\nGemini 0.65 (0.12) 15\nDeepSeek 0.43 (0.09) 37\nOpenAI o1 0.42 (0.10) 38\nOpenAI o3-mini 0.41 (0.08) 29\n18\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 12, 2025. ; https://doi.org/10.1101/2025.04.11.25325686doi: medRxiv preprint \nSupplementary 6. Missing Score Analysis and Normalization Approach\nTo assess data completeness, we quantified the number of missing evaluations per pathologist across all ques-\ntion–model–criterion (Q–M–C) combinations. The distribution of missing scores was highly skewed, with the majority\noriginating from two evaluators: Pathologist-10 (341 missing entries) and Pathologist-8 (339). All other evaluators\ncontributed near-complete data, with fewer than 35 missing values each. Seven of the eleven evaluators had fewer than\n20 missing scores, and three (Pathologists 3, 9, and 11) each had only a single missing entry.\nTo ensure fair comparison across models, we normalized scores for each Q–M–C combination by computing the mean\nof available ratings only. This approach prevents penalizing models for missing evaluations and ensures that aggregate\nscores reflect only completed assessments. Importantly, all Q–M–C combinations had a minimum of seven independent\nevaluations, with a mean of 9.9 ratings per combination, supporting the robustness of model-level comparisons despite\noccasional missing data.\nTable 5: Missing score counts and percentages by model.\nModel Total Scores Missing Count Missing %\nGemini 1980 177 8.94%\nDeepSeek 1980 179 9.04%\nOpenAI o1 1980 198 10.00%\nOpenAI o3-mini 1980 218 11.01%\nTotal 7920 772 9.75%\nTable 6: Missing score counts and percentages by question.\nQuestion Total Scores Missing Count Missing %\nQ1 528 61 11.55\nQ2 528 61 11.55\nQ3 528 70 13.26\nQ4 528 54 10.23\nQ5 528 36 6.82\nQ6 528 39 7.39\nQ7 528 56 10.61\nQ8 528 71 13.45\nQ9 528 52 9.85\nQ10 528 32 6.06\nQ11 528 37 7.01\nQ12 528 56 10.61\nQ13 528 36 6.82\nQ14 528 53 10.04\nQ15 528 58 10.98\nTable 7: Missing score counts and percentages by evaluation criterion.\nCriterion Total Entries Missing Count Missing %\nClarity 660 4 0.61\nCoherence 660 2 0.30\nDepth 660 3 0.45\nAccuracy 660 5 0.76\nConciseness 660 3 0.45\nPattern Recognition 660 151 22.88\nAlgorithmic Reasoning 660 46 6.97\nInductive Reasoning 660 94 14.24\nMechanistic Reasoning 660 103 15.61\nDeductive Reasoning 660 117 17.73\nHeuristic Reasoning 660 120 18.18\nProbabilistic Reasoning 660 124 18.79\n19\n . CC-BY-NC-ND 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 12, 2025. ; https://doi.org/10.1101/2025.04.11.25325686doi: medRxiv preprint "
}