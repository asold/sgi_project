{
  "title": "ISTR: End-to-End Instance Segmentation with Transformers",
  "url": "https://openalex.org/W3159833358",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2024523665",
      "name": "Hu Jie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2378737927",
      "name": "Cao, Liujuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2112718121",
      "name": "Lu Yao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2391340493",
      "name": "Zhang, ShengChuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1967442656",
      "name": "WANG Yan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1977048908",
      "name": "Li Ke",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2635098287",
      "name": "Huang, Feiyue",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101740539",
      "name": "Shao, Ling",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2264742060",
      "name": "Ji, Rongrong",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3170841864",
    "https://openalex.org/W1507506748",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3099495704",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2995460200",
    "https://openalex.org/W2980088508",
    "https://openalex.org/W3171516518",
    "https://openalex.org/W3034681942",
    "https://openalex.org/W2920326761",
    "https://openalex.org/W2744404335",
    "https://openalex.org/W2954087924",
    "https://openalex.org/W2963857746",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W3172752666",
    "https://openalex.org/W3106546328",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2921781974",
    "https://openalex.org/W2993182889",
    "https://openalex.org/W2894299524",
    "https://openalex.org/W2981537222",
    "https://openalex.org/W3131149871",
    "https://openalex.org/W3034826836",
    "https://openalex.org/W3100802883",
    "https://openalex.org/W2555182955",
    "https://openalex.org/W2964236837",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W3119997354",
    "https://openalex.org/W3171162369",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W3112776202",
    "https://openalex.org/W3034428102",
    "https://openalex.org/W3035049382",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W2935837427",
    "https://openalex.org/W2963659353",
    "https://openalex.org/W2962676885",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2964080601",
    "https://openalex.org/W3100039191",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2777795072",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3113410735",
    "https://openalex.org/W2982161360",
    "https://openalex.org/W3117450517",
    "https://openalex.org/W2963563276"
  ],
  "abstract": "End-to-end paradigms significantly improve the accuracy of various deep-learning-based computer vision models. To this end, tasks like object detection have been upgraded by replacing non-end-to-end components, such as removing non-maximum suppression by training with a set loss based on bipartite matching. However, such an upgrade is not applicable to instance segmentation, due to its significantly higher output dimensions compared to object detection. In this paper, we propose an instance segmentation Transformer, termed ISTR, which is the first end-to-end framework of its kind. ISTR predicts low-dimensional mask embeddings, and matches them with ground truth mask embeddings for the set loss. Besides, ISTR concurrently conducts detection and segmentation with a recurrent refinement strategy, which provides a new way to achieve instance segmentation compared to the existing top-down and bottom-up frameworks. Benefiting from the proposed end-to-end mechanism, ISTR demonstrates state-of-the-art performance even with approximation-based suboptimal embeddings. Specifically, ISTR obtains a 46.8/38.6 box/mask AP using ResNet50-FPN, and a 48.1/39.9 box/mask AP using ResNet101-FPN, on the MS COCO dataset. Quantitative and qualitative results reveal the promising potential of ISTR as a solid baseline for instance-level recognition. Code has been made available at: https://github.com/hujiecpp/ISTR.",
  "full_text": "ISTR: End-to-End Instance Segmentation with Transformers\nJie Hu1, Liujuan Cao1, Yao Lu1, ShengChuan Zhang1, Yan Wang2,\nKe Li3, Feiyue Huang3, Ling Shao4, and Rongrong Ji1,5.\n1Media Analytics and Computing Lab, Department of Artiﬁcial Intelligence,\nSchool of Informatics, Xiamen University, 2Pinterest, 3Tencent Youtu Lab,\n4Inception Institute of Artiﬁcial Intelligence, 5Institute of Artiﬁcial Intelligence, Xiamen University.\nAbstract\nEnd-to-end paradigms signiﬁcantly improve the accu-\nracy of various deep-learning-based computer vision mod-\nels. To this end, tasks like object detection have been up-\ngraded by replacing non-end-to-end components, such as\nremoving non-maximum suppression by training with a set\nloss based on bipartite matching. However, such an up-\ngrade is not applicable to instance segmentation, due to its\nsigniﬁcantly higher output dimensions compared to object\ndetection. In this paper, we propose an instance segmenta-\ntion Transformer, termed ISTR, which is the ﬁrst end-to-end\nframework of its kind. ISTR predicts low-dimensional mask\nembeddings, and matches them with ground truth mask em-\nbeddings for the set loss. Besides, ISTR concurrently con-\nducts detection and segmentation with a recurrent reﬁne-\nment strategy, which provides a new way to achieve in-\nstance segmentation compared to the existing top-down and\nbottom-up frameworks. Beneﬁting from the proposed end-\nto-end mechanism, ISTR demonstrates state-of-the-art per-\nformance even with approximation-based suboptimal em-\nbeddings. Speciﬁcally, ISTR obtains a 46.8/38.6 box/mask\nAP using ResNet50-FPN, and a 48.1/39.9 box/mask AP us-\ning ResNet101-FPN, on the MS COCO dataset. Quantita-\ntive and qualitative results reveal the promising potential\nof ISTR as a solid baseline for instance-level recognition.\nCode has been made available at: https://github.\ncom/hujiecpp/ISTR.\n1. Introduction\nA growing trend in the recent development of computer\nvision is to remove the handcrafted components to enable\nend-to-end training and inference, which has demonstrated\nsigniﬁcant improvement in multiple ﬁelds. However, this\nend-to-end paradigm still lacks applications for instance\nsegmentation that aims to jointly detect and segment each\nobject in an image. Existing instance segmentation ap-\nproaches either need a manually-designed post-processing\nstep called non-maximum suppression (NMS) to remove\nduplicate predictions [16, 28, 19, 4, 22, 44], or are early\ntrials on small datasets and lack evaluation against modern\nbaselines [31, 33]. Popular approaches also rely on a top-\ndown or bottom-up framework that decomposes instance\nsegmentation into several dependent tasks, preventing them\nfrom being end-to-end.\nBesides instance segmentation, object detection also\nfaces similar challenges. Recent studies enable end-to-end\nobject detection by introducing a set prediction loss [18, 3,\n37, 43, 56], with optional use of Transformers [42]. The set\nprediction loss enforces bipartite matching between labels\nand predictions to penalize redundant outputs, thus avoid-\ning NMS during inference. However, enabling end-to-end\ninstance segmentation is not as trivial as adding a mask\nbranch and changing the loss. We conducted a proof-of-\nconcept experiment by adapting the end-to-end object de-\ntection approach to instance segmentation. The results in\nTable 1a show the inferior performance of doing so.\nWe argue that the reason behind the failure is the insufﬁ-\ncient number of samples for learning the mask head. On the\none hand, the dimensions of masks are much higher than\nthose of classes and boxes. For example, a mask usually\nhas a 28 ×28 or higher resolution on the COCO dataset,\nwhile a bounding box only needs two coordinates to repre-\nsent. Therefore, the mask head requires more samples for\ntraining. On the other hand, the proposal bounding boxes\nobtained by the bipartite matching are usually on a small\nscale, which also raises the problem of sparse training sam-\nples. For example, Mask R-CNN [16] uses 512 proposal\nbounding boxes to extract the region of interest (RoI) fea-\ntures for training the mask head, while the number of pro-\nposal bounding boxes, i.e., ground truths per image on the\nCOCO dataset, is only 7.7 on average after bipartite match-\ning. The gap between the demand and supply of training\nsamples makes the approach prone to failing.\nWhile we could blindly augment the ground truth sam-\nples to alleviate the problem at the cost of longer training\ntime, we argue that there might be a smarter way. Not all\narXiv:2105.00637v2  [cs.CV]  6 May 2021\n0 10 20 30 40 50 60 70 80 90 100\nComponents\n0\n50\n100\n150Energy\nEnergy Distribution\nFigure 1: Component analysis of masks, ranking the\nTop@100 components by energy. The majority of the mask\ninformation is embedded into the ﬁrst few components.\n28 ×28 entries are likely to appear as a mask. The distribu-\ntion of the natural masks may lie in a low-dimensional man-\nifold instead of being uniformly scattered. Based on this\nintuition, we carried out several dimension reduction exper-\niments on the masks from the training data, and surprisingly\nfound even linear methods, such as principal component\nanalysis (PCA), can do a decent job. The energy distribu-\ntion of different components is shown in Fig. 1. We observe\nthat the ﬁrst few components can represent the majority of\nthe mask information. Therefore, in this paper, we propose\nto achieve end-to-end instance segmentation by regressing\nlow-dimensional embeddings instead of raw masks, which\nenables the training to be effectively conducted with a small\nnumber of matched samples. We also extend the deﬁnition\nof bipartite matching cost based on the mask embeddings.\nFurthermore, regressing with the embeddings enables us to\ndesign a recurrent reﬁnement strategy that can process de-\ntection and segmentation concurrently. This provides a new\nway of instance segmentation compared to the top-down\nand bottom-up frameworks, and boosts the performance.\nSpeciﬁcally, we propose a new end-to-end instance seg-\nmentation framework built upon a Transformer, termed\nISTR. ISTR predicts low-dimensional mask embeddings,\nand then matches them with ground truth mask embeddings\nfor the set loss. With the recurrent reﬁnement strategy,\nISTR updates the query boxes and reﬁnes the set of predic-\ntions. Beneﬁting from the proposed end-to-end mechanism,\nwe ﬁnd that even with the suboptimal mask embeddings\nobtained by the closed-form solution of PCA, ISTR can\nachieve state-of-the-art performance. With a single 1080Ti\nGPU, ISTR obtains a 46.8/38.6 box/mask AP with 13.8 fps\nusing ResNet50-FPN, and a 48.1/39.9 box/mask AP with\n11.0 fps using ResNet101-FPN on the test-dev split of\nthe COCO dataset [26]. Our contributions are summarized\nas follows:\n• We propose a new framework, termed instance seg-\nmentation Transformer (ISTR). For the ﬁrst time, we\ndemonstrate the potential of using Transformers in\nend-to-end instance segmentation.\n• ISTR predicts low-dimensional mask embeddings in-\nstead of high-dimensional masks, which facilitates the\ntraining with a small number of samples and inspires\nthe design of a bipartite matching cost for masks.\n• With a recurrent reﬁnement strategy, ISTR concur-\nrently detects and segments instances, providing a new\nperspective for achieving instance segmentation com-\npared to the bottom-up and top-down frameworks.\n• Without bells and whistles, ISTR demonstrates accu-\nracy and run-time performance on par with the state-\nof-the-art methods on the challenging COCO dataset.\n2. Related Work\nInstance Segmentation: Instance segmentation requires\ninstance-level and pixel-level predictions. Existing works\ncan be summarized into three categories. Top-down meth-\nods [1, 16, 23, 28, 19, 7, 4, 22, 48] ﬁrst detect and then\nsegment the objects. Bottom-up methods [52, 8, 27, 12]\nview instance segmentation as a label-then-cluster problem,\nlearning to classify each pixel and then clustering them into\ngroups for each object. The latest work, SOLO [44, 46],\ndeals with instance segmentation without dependence on\nbox detection. The proposed ISTR provides a new perspec-\ntive that directly predicts a set of bounding boxes and mask\nembeddings, which avoids decomposing instance segmen-\ntation into dependent tasks. Note that the idea of regress-\ning mask embeddings is also investigated in MEInst [53].\nHowever, with redundant predictions in each pixel, MEInst\nobtains suboptimal performance compared to ISTR.\nEnd-to-End Instance-Level Recognition: Recent studies\nhave revealed the great potential of end-to-end object de-\ntection [18, 3, 37, 43, 56, 55]. As such, the bipartite match-\ning cost has become an essential component for achieving\nend-to-end object detection. For instance segmentation, the\nworks [31, 33] explored the end-to-end mechanism with re-\ncurrent neural networks. However, these early trials were\nonly evaluated on small datasets and not against current\nbaselines. In contrast, ISTR uses the similarity metric of\nmask embeddings as the bipartite matching cost for masks,\nand, for the ﬁrst time, incorporates Transformers [42] to im-\nprove end-to-end instance segmentation.\nTransformers in Computer Vision: The breakthroughs\nof Transformers [42] in natural language processing have\nsparked great interest in the computer vision community.\nThe critical component of Transformer is the multi-head\nattention, which can signiﬁcantly enhance the capacity of\nmodels [14, 20]. So far, Transformers have been suc-\ncessfully used for image recognition [11, 41], object de-\ntection [3, 56, 37], segmentation [54, 51], image super-\nresolution [49], video understanding [36, 13], image gener-\nation [5, 45], visual question answering [38, 35], and several\nother tasks [21, 10, 50]. With the sequence information be-\ntween frames, the contemporary work [47] achieves end-to-\nend video instance segmentation with a Transformer. With-\nAvg\nQuery\nCNN\nInput\nFPN\nP2\nP3\nP4\nP5\nRoI Features\nImg Features\nPosition \nEmbeddings\nTransformer Encoder\nw/ Dynamic Attention\nPrediction Heads Output\n×N\nBackbone Feature Pyramid\nDynamic \nAttention\nMulti-Head\nAttention\nFFN\nQ\nK\nV\nAdd&\nNorm\nAdd&\nNorm\nAdd&\nNorm\nPredict\nConcurrently\nRoI.\nImg.\nMask \nEmbeddings Mask Decoder\nFixed\nClass Head\nMask Head\nBox Head\nGT\nBipartite\nMatching\nTraining\nLossCost\nInference\nBoxes & Masks & Classes\nImg.\n…\nMask\nEmbeddings Boxes Classes\nRefinement Stages\nRoI.\n…\nFigure 2: Framework of ISTR. Top: Overview of the pipeline. Input images are sent to a convolutional neural network\n(CNN) with a feature pyramid network (FPN) [24] to produce the feature pyramid. The feature maps from the feature\npyramid are cropped and aligned by learnable query boxes with RoIAlign [16] to get the RoI features. Image features are\nobtained by summing and averaging the feature maps. Then, a Transformer encoder with dynamic attention fuses the image\nand RoI features for prediction heads. The predicted bounding boxes, classes, and masks are recurrently reﬁned in N stages\nby updating the query boxes. During training, the predictions are matched with ground truth labels to calculate the set loss.\nDuring inference, the predictions are directly used as the ﬁnal results without NMS. Bottom: Details of the modules. FFN\ndenotes the feed-forward network, and the mask decoder is pre-learned and ﬁxed during training.\nout continuous frames, our study aims to segment instances\nfor a single image, making its design entirely different from\nthe framework of [47].\nMulti-Task Learning: The beneﬁt of learning detection\nand segmentation jointly was ﬁrst studied in the work\nof [15]. After that, Mask R-CNN [16] also demonstrated\nthat bounding box detection could beneﬁt from multi-task\nlearning. Recent works [6, 28, 34, 2] have provided more\ncomplex mechanisms to improve the performance for multi-\ntasks. In our work, we also observe a performance boost\nwhen concurrently processing detection and segmentation.\n3. Proposed Method\nISTR aims to directly predict a set of mask embeddings,\nclasses, and bounding boxes for each instance. To this end,\nwe ﬁrst introduce a generalized formulation to extract em-\nbeddings for representing and reconstructing the masks in\nSection 3.1. A bipartite matching cost and a set loss are in-\ntroduced in Section 3.2 to pair and regress the predictions\nwith ground truth labels. Finally, a model that predicts a\nset of outputs and learns their relations is proposed in Sec-\ntion 3.3. The overall framework of ISTR is shown in Fig. 2.\n3.1. Mask Embeddings\nTo provide a formulation that effectively extracts mask\nembeddings, we constrain the mutual information between\nthe original and reconstructed masks:\nmax I\n(\nM,f\n(\ng(M)\n))\n, (1)\nwhere I(·,·) denotes the mutual information between two\nrandom variables, M denotes a set of masks {mi ∈\nRs2\n|i = 1,...,n }, s2 is the dimension of masks, g(·) de-\nnotes the mask encoder for extracting embeddings and f(·)\ndenotes the mask decoder for reconstructing masks. Eq. 1\nguarantees that the encoding and decoding phases have min-\nimal information loss, which implicitly encourages the em-\nbeddings to represent the masks. After derivation, we have\na generalized objective function for the mask embeddings:\nmin\nn∑\ni=1\n||mi −f(ri)||2\n2, (2)\nwhere ri = g\n(\nmi\n)\ndenotes the mask embeddings, and||·||2\nis the L2-norm. By making the functions of the encoder\nand decoder simple linear transformations via a matrixD∈\nRs2×l, i.e., f\n(\ng(mi)\n)\n= DDTmi and DDT = Il, the\nobjective function becomes:\nD∗ = arg min\nD\nn∑\ni=1\n||mi −DDTmi||2\n2, (3)\nwhere l is the dimension of the mask embeddings, and Il\ndenotes the l×l unit matrix. Eq. 3 has the same formu-\nlation as the objective function of PCA, which provides a\nclosed-form solution to learn the transformation. Note that\nthe objective function in Eq. 2 can also be optimized by\nother models, such as an autoencoder.\n3.2. Matching Cost and Prediction Loss\nAfter obtaining the encoder and decoder for mask em-\nbeddings, we deﬁne a bipartite matching cost and a set pre-\ndiction loss for end-to-end instance segmentation. Let us\ndenote the ground truth bounding boxes, classes, and masks\nas Y = {bi,ci,mi|i = 1,...,n }. The predicted bound-\ning boxes, classes, and mask embeddings are denoted as\n˜Y = {˜bi,˜ci,˜ri|i= 1,...,k }, where k>n .\nBipartite Matching Cost: For the bipartite matching, we\nsearch for a permutation of n non-repeating integers σ ∈\n{1,2,...,k }with the lowest cost, as:\nσ∗ = arg min\nσ\nn∑\ni=1\n(\nCbox(bi,˜bσ(i))\n+Ccls(ci,˜cσ(i)) +Cmask(mi,˜rσ(i))\n)\n.\n(4)\nInspired by [3, 37], we deﬁne the matching cost for bound-\ning boxes as:\nCbox(bi,˜bσ(i)) =λL1 ·CL1(bi,˜bσ(i))\n+ λgiou ·Cgiou(bi,˜bσ(i)),\n(5)\nand the matching cost for classes as:\nCcls(ci,˜cσ(i)) =−λcls ·˜pσ(i)(ci), (6)\nwhere λdenotes the hyperparameters that balance the costs,\nCL1(·,·) denotes the L1 cost, Cgiou(·,·) denotes the gen-\neralized IoU [32] cost, and ˜pσ(i)(ci) is the probability of\nclass ci. Instead of directly matching the high-dimensional\nmasks, we use the similarity metric between mask embed-\ndings to match them, which is deﬁned as:\nCmask(mi,˜rσ(i)) =\n−1\n2λmask·\n(\n< ˜rσ(i)\n||˜rσ(i)||2\n, g(mi)\n||g(mi)||2\n>+1\n)\n, (7)\nwhere the mask embeddings are L2 normalized, and the dot\nproduct between two normalized vectors is used to calculate\nthe cosine similarity. We add 1 to the result and divide it by\n2 to guarantee that the values are in the range of [0,1].\nAlgorithm 1 Instance Segmentation Transformer\n// ——— Training Phase ———\nInput: Images and ground truth labels.\nOutput: Learned ISTR model.\n1: Learn the mask encoder and decoder via Eq. 2.\n2: Initialize the learnable query boxes ˜B\n0\n.\n3: Repeat\n4: For i= 1,2,..,N stage:\n5: Obtain RoI features by RoIAlign with ˜B\ni−1\n.\n6: Predict via ISTR encoder and heads.\n7: Match predictions with labels via Eq. 4.\n8: Calculate loss via Eq. 8 and train ISTR.\n9: Update ˜B\ni\nfrom ˜B\ni−1\nwith the predicted boxes.\n10: Until scheduled epochs.\n// ——— Inference Phase ———\nInput: Images to be processed.\nOutput: Detected and segmented objects.\n1: For i= 1,2,..,N stage:\n2: Obtain RoI features by RoIAlign with ˜B\ni−1\n.\n3: Obtain predictions via ISTR encoder and heads.\n4: Update ˜B\ni\nfrom ˜B\ni−1\nwith the predicted boxes.\n5: Output the set of predictions in the ﬁnal stage.\nSet Prediction Loss: For the set prediction loss, we use the\nmatched predictions to regress the ground truth targets. The\nset prediction loss is deﬁned as:\nLset(Y, ˜Y,σ∗) = 1\nn\nn∑\ni=1\n(\nLbox(bi,˜bσ∗(i))\n+Lcls(ci,˜cσ∗(i)) +Lmask(mi,˜rσ∗(i))\n)\n,\n(8)\nwhere Lbox(·,·) is deﬁned the same as Cbox(·,·), and\nLcls(·,·) is the focal loss [25] for classiﬁcation. For masks,\nwe add the dice loss [30] to improve the learned embeddings\nfor reconstructing the masks. The mask loss is deﬁned as:\nLmask(mi,˜rσ∗(i)) =λmask ·\n(\nLL2(g(mi),˜rσ∗(i))\n+ Ldice\n(\nmi,f(˜rσ∗(i))\n))\n,\n(9)\nwhere LL2(·,·) denotes the L2 loss and Ldice(·,·) denotes\nthe dice loss.\n3.3. Instance Segmentation Transformer\nThe architecture of ISTR is depicted in Fig. 2. It contains\nfour main components: a CNN backbone with FPN [24] to\nextract features for each instance, a Transformer encoder\nwith dynamic attention to learn the relations between ob-\njects, a set of prediction heads that conduct detection and\nsegmentation concurrently, as well as the N-step recurrent\nupdate for reﬁning the set of predictions.\nAPm APm\n50 APm\n75 APb APb\n50 APb\n75\nmask 31.6 53.3 33.0 40.2 58.4 43.6\nl=40 33.7 55.6 35.5 41.1 58.9 44.8\nl=60 34.2 55.6 36.4 41.0 58.9 44.4\nl=80 33.8 55.3 35.9 40.6 58.6 44.1\nl=784 31.4 55.2 31.8 41.1 60.0 44.4\n(a) Mask vs. Mask Embeddings: Regression with mask embed-\ndings instead of masks brings better performance to the mask APs.\nThe performance improves when the embedding dimension l=60,\nand saturates when the dimension l=80. Directly expanding the\nmask as embeddings, i.e., l=784, has worse performance.\nAPm APm\n50 APm\n75 APb APb\n50 APb\n75\nw/o 33.8 55.5 35.7 40.7 58.8 44.1\ndice 33.9 55.4 35.8 40.8 58.8 44.3\nL1 34.0 55.5 35.6 40.9 58.9 44.3\ncosine 34.2 55.6 36.4 41.0 58.9 44.4\n(b) Mask Cost Functions: Matching with the dice loss between the\npredicted and ground truth masks performs slightly better than w/o\nthe mask cost. The L1 loss between the predicted and encoded mask\nembeddings also has a slight improvement. Using cosine similarity as\nthe mask cost function brings expected gains.\nAPm APm\n50 APm\n75 APm\nS APm\nM APm\nL\ndice 32.6 55.0 33.5 17.3 34.8 47.6\nL2 33.8 55.5 35.5 17.3 36.1 49.5\nL2+dice 34.2 55.6 36.4 17.6 36.5 50.6\n(c) Loss Functions: Learning masks with both a pixel-level dice\nloss and a embedding-level L2 loss yields gains in mask APs.\nAPm APm\n50 APm\n75 APb APb\n50 APb\n75\nmulti-head 22.0 43.9 19.6 31.2 50.5 32.5\ndynamic 34.2 55.6 36.4 41.0 58.9 44.4\n+12.2 +11.7 +16.8 +9.8 +8.4 +11.9\n(d) Attention Type: Dynamic attention brings signiﬁcant gains com-\npared with multi-head attention in fusing the RoI and image features.\nimg. type pos. APm APm\n50 APm\n75 APm\nS APm\nM APm\nL APb APb\n50 APb\n75 APb\nS APb\nM APb\nL\nfeatures\n✓ max 33.3 54.6 35.2 17.4 35.6 48.5 40.0 57.6 43.5 24.2 42.6 52.6\n✓ avg 33.6 55.3 35.5 17.3 36.4 49.5 40.6 58.8 44.1 23.9 43.2 54.1\n- - ✓ 34.0 55.3 36.3 17.4 36.4 50.5 40.9 58.8 44.3 24.3 43.2 55.0\n✓ avg ✓ 34.2 55.6 36.4 17.6 36.5 50.6 41.0 58.9 44.4 24.8 43.3 55.3\n(e) Pooling Type: The global average pooling yields better performance than the global max-pooling. Combining the image features with\nposition embeddings increases the performance in both mask and box APs.\nTable 1: Ablations. We train on the COCO train2017 split, and report mask as well as box APs on the val2017 split.\nBackbone: We use a CNN backbone with FPN to extract\nthe features ranging from P2 to P5 level of the feature pyra-\nmid. Then, klearnable query boxes ˜B\n0\n= {˜b\n0\ni|i= 1,...,k }\ninitially covering the whole images are used to extractkRoI\nfeatures U0 ∈Rk×d×t×tvia RoIAlign [16]. Image features\nP ∈Rk×dare ﬁrst extracted by averaging and summing the\nfeatures from P2 to P5, and then expand the ﬁrst dimension\nto k for each RoI feature. Learnable position embeddings\nE∈Rk×d are initialized randomly.\nTransformer Encoder and Dynamic Attention: The sum\nof image features and position embeddings is ﬁrst trans-\nformed by three learnable weight matrices to obtain the\ninputs Q = (P + E)WQ,K = (P + E)WK,V =\n(P + E)WV for the self-attention module deﬁned as:\nZ= softmax(QKT\n√\nd\n)V. (10)\nThe multi-head attention comprises multiple self-attention\nblocks, e.g., eight in the original Transformer [42], to en-\ncapsulate multiple complex relationships amongst different\nfeatures. Inspired by [37], we add a dynamic attention mod-\nule for better fusing the RoI and image features, which is\ndeﬁned as the attention conditioned on the RoI features Ui\nin the i-th step:\nOi = Ui ·fc(Z), (11)\nwhere fc(·) denotes a fully connected layer to generate the\ndynamic parameters. The obtained features Oi are then\nused in the prediction heads to produce the set of outputs.\nPrediction Heads: The set of predictions is computed by\nthe heads, including a class head, a box head, a mask head,\nand a ﬁxed mask decoder. The box head predicts the resid-\nual values of normalized center coordinates, height, and\nwidth for updating the query boxes ˜B\ni\nin the i-th step, and\nthe class head predicts the classes using a softmax function.\nThe mask head outputs the mask embeddings, which are\nthen reconstructed to predict masks using the pre-learned\nmask decoder.\nRecurrent Reﬁnement Strategy: The query boxes ˜B\ni\nare\nrecurrently updated by the predicted boxes, which reﬁnes\nthe predictions and makes it possible to process the detec-\ntion and segmentation concurrently. The overall process is\nsummarized in Alg. 1.\n4. Experiments\nDataset and Evaluation Metrics: Our experiments are\nperformed on the MS COCO dataset [26], which contains\n123K images with 80-class instance labels. Our models\nare trained on the train2017 split (118K images), and\nthe ablation study is carried out on the val2017 split\nStage 1 Stage 2 Stage 3 Stage 4 Stage 5 Stage 6 GT\n(a) Qualitative Results: Masks and bounding boxes are from ISTR using ResNet101-FPN on the COCO val2017 split, with a threshold of 0.4.\nAP m AP m\nS AP m\nM AP m\nL AP b AP b\nS AP b\nM AP b\nL\n0\n10\n20\n30\n40\n50\n60\n70\nStage 1 Stage 2 Stage 3 Stage 4 Stage 5 Stage 6\n(b) AP results on the COCO val2017 split with ResNet50-FPN.\nAP m AP m\nS AP m\nM AP m\nL AP b AP b\nS AP b\nM AP b\nL\n0\n10\n20\n30\n40\n50\n60\n70\nStage 1 Stage 2 Stage 3 Stage 4 Stage 5 Stage 6 (c) AP results on the COCO val2017 split with ResNet101-FPN.\nFigure 3: Stage Analysis. We report quantitative and qualitative results of ISTR for N = 6stages. Both results show that\nthe bounding boxes and masks are reﬁned stage by stage, and the performance saturates in the last two stages.\n(5K images). Final results are reported on the test-dev\nsplit, which has no public labels and is evaluated on the\nserver. We report the standard COCO metrics including AP\n(i.e., averaged over IoU thresholds), AP50, AP75, and APS,\nAPM, APL (i.e., AP at different scales) for both boxes and\nmasks, denoted as APb and APm, respectively.\nTraining Details: We follow the strategy in [53, 39] to en-\ncode the ground truth mask embeddings. ResNet50 and\nResNet101 [17] pre-trained on ImageNet [9] are used as\nour backbone networks. FPN [24] is used to extract the\nfeature pyramid. For the ablation study, all the models are\ntrained over 12 epochs with learning rate decay, dividing\nby 10 at epoch 9 and 11, respectively. All results in the\nablation study are tested with ResNet50-FPN and reported\non the COCO val2017 split. The training schedule for\nthe ﬁnal models is 36 epochs with the learning rate divided\nby 10 at epoch 27 and 33, respectively. The mini-batch\ncontains 16 images, and the models are trained with eight\nGPUs. Following [37], we use the AdamW [29] optimizer\nand an initial learning rate of 0.000025. The input images\nare resized such that the shortest side is at least 480 and at\nmost 800 pixels, while the longest side is at most1333. The\nnumber of predictions, i.e., k, is set to 300, and the number\nof self-attention blocks in the multi-head attention is set to\n8. The number of recurrent reﬁnement stages is set to 6.\nFollowing [3, 56], we set λcls, λL1, and λgiou to 2, 5, and\n2, respectively. The hyperparameter λmask is set to 2.\n4.1. Ablation Experiments\nTo analyze ISTR, we conduct ablation studies on the\nchoices of mask embeddings, cost functions, loss functions,\nthe effect of position embeddings, and pooling types. Re-\nsults are shown in Table 1 and discussed in detail next.\nMask vs. Mask Embeddings: Table 1a shows the mod-\nels with various types of mask representations, including\nthe original masks with 28 ×28 dimensions and the mask\nembeddings with different dimensions l. We also expand\nthe original masks to vectors to test the performance. Us-\nmethod backbone Epochs APm APm\nS APm\nM APm\nL APb APb\nS APb\nM APb\nL FPS Time GPU\nMask R-CNN [16] ResNet50-FPN 36 37.5 21.1 39.6 48.3 41.3 24.2 43.6 51.7 15.3 65.6 1080Ti\nMEInst [53] ResNet50-FPN 36 33.5 19.3 35.7 42.1 42.5 25.6 45.1 52.2 15.0 66.8 1080Ti\nCondInst [40] ResNet50-FPN 36 37.8 21.0 40.3 48.7 42.1 25.1 44.5 52.1 15.4 65.0 1080Ti\nBlendMask [4] ResNet50-FPN 36 37.8 18.8 40.9 53.6 43.0 25.3 45.4 54.0 15.0 66.8 1080Ti\nSOLOv2 [46] ResNet50-FPN 36 38.2 16 41.2 55.4 40.7 18.4 43.5 57.6 10.5 95.5 1080Ti\nDETR [3] ResNet50 500 - - - - 42.0 20.5 45.8 61.1 - - -\nSparse R-CNN [37] ResNet50-FPN 36 - - - - 44.5 26.9 47.2 59.5 - - -\nISTR, ours ResNet50-FPN 36 38.6 22.1 40.4 50.6 46.8 27.8 48.7 59.9 13.8 72.5 1080Ti\nMask R-CNN [16] ResNet101-FPN 36 38.8 21.8 41.4 50.5 43.1 25.1 46.0 54.3 11.8 85.0 1080Ti\nMEInst [53] ResNet101-FPN 36 35.3 20.4 37.8 44.5 44.5 26.8 47.3 54.9 11.2 89.3 1080Ti\nCondInst [40] ResNet101-FPN 36 39.1 21.5 41.7 50.9 43.5 25.8 46.0 54.1 12.0 83.2 1080Ti\nBlendMask [4] ResNet101-FPN 36 39.6 22.4 42.2 51.4 44.7 26.6 47.5 55.6 11.5 86.6 1080Ti\nSOLOv2 [46] ResNet101-FPN 36 39.7 17.3 42.9 57.4 42.6 22.3 46.7 56.3 9.0 111.6 1080Ti\nDETR [3] ResNet101 500 - - - - 43.5 21.9 48.0 61.8 - - -\nSparse R-CNN [37] ResNet101-FPN 36 - - - - 45.6 28.3 48.3 61.6 - - -\nISTR, ours ResNet101-FPN 36 39.9 22.8 41.9 52.3 48.1 28.7 50.4 61.5 11.0 91.3 1080Ti\nTable 2: Quantitative Results of ISTR on the COCO test-dev split. All the models are learned by multi-scale training.\nThe results of FPS are measured with a single 1080Ti GPU. The performance of Mask R-CNN is the result of the modiﬁed\nversion with implementation details in TensorMask [7].\ning raw masks for segmentation is implemented with the\nmask head from Mask R-CNN in a top-down manner, and\nother settings are the same as ISTR. We obtain the follow-\ning results. First, predicting mask embeddings brings bet-\nter performance to the mask APs than predicting masks.\nThe results verify our concern that the high-dimensional\nmasks cannot be effectively learned with a small number\nof matched samples. In contrast, the mask embeddings can\nbe well regressed, as their dimensions are much lower than\nthose of masks, e.g., 40, 60, 80 vs. 784. Second, the per-\nformance of mask embeddings improves when the dimen-\nsion l=60 and saturates when the dimension l=80. Finally,\nregressing with the expanded masks, i.e., l=784, as embed-\ndings has worse performance in mask APs.\nCost Functions: Appropriate cost functions can match\nhigh-quality predictions with ground truth labels for the set\nloss. In Table 1b, we compare the performance with dif-\nferent choices of cost functions for the masks. Matching\nthe predicted masks with ground truth masks by the dice\nloss does not produce expected gains compared to match-\ning without the mask cost function. Using the L1 loss be-\ntween embeddings slightly improves performance, and us-\ning the cosine similarity between embeddings as the mask\ncost function brings expected gains.\nLoss Functions: We investigate two types of loss functions\nfor ISTR: the dice loss at the pixel-level and the L2 loss\nat the embedding-level. The dice loss is calculated using\nthe masks reconstructed by the mask decoder. As shown\nin Table 1c, only calculating the dice loss at the pixel-level\nwithout constraining the mask embeddings has an inferior\nperformance. Although the L2 loss between the predicted\nand encoded mask embeddings improves the performance,\nthe learned mask embeddings are slightly suboptimal for\nreconstructing the original masks. Therefore, training with\nboth the pixel-level dice loss and the embedding-level L2\nloss produces better results.\nAttention Type: We next study the effect of the dynamic\nattention module, which is used to fuse the RoI and image\nfeatures, by replacing it with the multi-head attention mod-\nule. As shown in Table 1d, the dynamic attention module\nperforms much better than the multi-head attention mod-\nule. We believe this may be because the multi-projection in\nthe multi-head attention complicates the fusion of RoI and\nimage features, which is essential for learning the relations\nbetween objects. From the results, we infer that a single\nprojection is more effective for learning such relations.\nPooling Type: In Table 1e, we evaluate various strategies\nfor obtaining the image features by extracting different in-\nformation from input images. As can be seen, the global\nmax-pooling does not obtain a high score, while the global\naverage pooling performs better. We believe this is because\nthe max-pooling extracts features from the highest activated\npixel in the feature map, which usually corresponds to a sin-\ngle activated object. In contrast, the global average pooling\nextracts features that contain information about the whole\nimage. We also ﬁnd that the position embeddings are es-\nsential for achieving high results. By summing the position\nembeddings with averaged image features, the performance\nis signiﬁcantly improved.\nMask R-CNNISTR\n(a) Mask R-CNN [16] (top) vs. ISTR (bottom) using ResNet101-FPN. Mask R-CNN suffers inferior segmentation when bad duplicate removal occurs.\n(b) More visualization results of ISTR, using ResNet101-FPN and running at 11.0 fps on a 1080Ti GPU, with 39.9 mask AP (Table 2).\nFigure 4: Qualitative Results of ISTR on the COCO test-dev split. Predictions are shown with a threshold of 0.4\n4.2. Stage Analysis\nOne of the essential components of ISTR is the recurrent\nreﬁnement strategy, which provides a new way to achieve\ninstance segmentation compared to the bottom-up and top-\ndown strategies. The prediction heads infer the classes,\nbounding boxes, and masks for each instance using the\nquery boxes updated in each stage. We investigate the per-\nformance of the recurrent reﬁnement stages both quantita-\ntively and qualitatively in Fig. 3. From the visualization of\nmasks and bounding boxes shown in Fig. 3a, we can see\nthat both masks and boxes are reﬁned from coarse to ﬁne.\nThe mask and box APs shown in Fig. 3b and Fig. 3c us-\ning different backbones also demonstrate that the results are\ngradually reﬁned step by step.\n4.3. Main Results\nWe compare ISTR with the state-of-the-art instance seg-\nmentation methods as well as the latest end-to-end object\ndetection methods to demonstrate its superior performance.\nQuantitative Results: From Table 2, we can see that ISTR\nperforms well, especially on small objects. For example,\nthe AP m\nS of ISTR based on ResNet101-FPN outperforms\nSOLOv2 based on ResNet101-FPN by 5.5 points. We be-\nlieve this is because the bipartite matching cost does not\nﬁlter small objects for training. MEInst also uses mask em-\nbeddings for instance segmentation. However, the perfor-\nmance of MEInst suffers signiﬁcantly due to the redundant\npredictions of mask embeddings. For example, the APm of\nISTR based on ResNet101-FPN outperforms MEInst based\non ResNet101-FPN by a large margin of 4.6 points. Be-\nsides, we also ﬁnd performance gains of ISTR in detec-\ntion when comparing the results with the state-of-the-art\nend-to-end object detection methods. We ﬁnd that the AP b\nof ISTR outperforms DETR and sparse R-CNN based on\nResNet101-FPN by 4.6 and 2.5 points, respectively. Over-\nall, it is surprising that, despite the suboptimal mask em-\nbeddings from PCA, ISTR can still obtain such a good re-\nsult. This demonstrates the strength of the proposed end-\nto-end mechanism and shows the potential of concurrently\nconducting detection and segmentation with Transformers.\nQualitative Results: We show some examples comparing\nISTR with Mask R-CNN in Fig. 4a. As can be seen, Mask\nR-CNN suffers inferior performance when NMS does not\nremove the duplicate predictions. More visualization results\nin Fig. 4b suggest that, although ISTR obtains state-of-the-\nart mask APs, there is still room for further improvement by\nlearning ﬁner masks. We leave this for future work.\n5. Conclusion\nIn this paper, we propose a new framework, termed in-\nstance segmentation Transformer (ISTR), to explore the\nend-to-end mechanism for instance segmentation. ISTR\npredicts low-dimensional mask embeddings instead of\nhigh-dimensional masks, which inspires the design of a\nmask matching cost and facilitates the regression. Besides,\nISTR concurrently conducts detection and segmentation via\na recurrent reﬁnement strategy, which provides a new per-\nspective to achieve end-to-end instance segmentation and\nboosts the performance of both tasks. On the challenging\nCOCO dataset, the strong performance of ISTR demon-\nstrates its potential for instance-level recognition tasks.\nReferences\n[1] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee.\nYolact: Real-time instance segmentation. In Proceedings\nof the IEEE International Conference on Computer Vision ,\n2019. 2\n[2] Jiale Cao, Yanwei Pang, and Xuelong Li. Triply supervised\ndecoder networks for joint detection and segmentation. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2019. 3\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Confer-\nence on Computer Vision, 2020. 1, 2, 4, 6, 7\n[4] Hao Chen, Kunyang Sun, Zhi Tian, Chunhua Shen, Yong-\nming Huang, and Youliang Yan. Blendmask: Top-down\nmeets bottom-up for instance segmentation. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2020. 1, 2, 7\n[5] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping\nDeng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and\nWen Gao. Pre-trained image processing transformer. InPro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2021. 2\n[6] Liang-Chieh Chen, Alexander Hermans, George Papan-\ndreou, Florian Schroff, Peng Wang, and Hartwig Adam.\nMasklab: Instance segmentation by reﬁning object detection\nwith semantic and direction features. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2018. 3\n[7] Xinlei Chen, Ross Girshick, Kaiming He, and Piotr Doll ´ar.\nTensormask: A foundation for dense object segmentation. In\nProceedings of the IEEE International Conference on Com-\nputer Vision, 2019. 2, 7\n[8] Bert De Brabandere, Davy Neven, and Luc Van Gool.\nSemantic instance segmentation with a discriminative loss\nfunction. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition Workshop, 2017. 2\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2009. 6\n[10] Carl Doersch, Ankush Gupta, and Andrew Zisserman.\nCrosstransformers: spatially-aware few-shot transfer. In Ad-\nvances in Neural Information Processing Systems, 2020. 2\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. Arxiv preprint, 2020.\n2\n[12] Naiyu Gao, Yanhu Shan, Yupei Wang, Xin Zhao, Yinan Yu,\nMing Yang, and Kaiqi Huang. Ssap: Single-shot instance\nsegmentation with afﬁnity pyramid. In Proceedings of the\nIEEE International Conference on Computer Vision, 2019. 2\n[13] Rohit Girdhar, Joao Carreira, Carl Doersch, and Andrew Zis-\nserman. Video action transformer network. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, 2019. 2\n[14] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen,\nJianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao, Chun-\njing Xu, Yixing Xu, et al. A survey on visual transformer.\nArxiv preprint, 2020. 2\n[15] Bharath Hariharan, Pablo Arbel ´aez, Ross Girshick, and Ji-\ntendra Malik. Simultaneous detection and segmentation. In\nEuropean Conference on Computer Vision, 2014. 3\n[16] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE International\nConference on Computer Vision, 2017. 1, 2, 3, 5, 7, 8\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2016. 6\n[18] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen\nWei. Relation networks for object detection. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018. 1, 2\n[19] Zhaojin Huang, Lichao Huang, Yongchao Gong, Chang\nHuang, and Xinggang Wang. Mask scoring r-cnn. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2019. 1, 2\n[20] Salman Khan, Muzammal Naseer, Munawar Hayat,\nSyed Waqas Zamir, Fahad Shahbaz Khan, and Mubarak\nShah. Transformers in vision: A survey. Arxiv preprint,\n2021. 2\n[21] Manoj Kumar, Dirk Weissenborn, and Nal Kalchbrenner.\nColorization transformer. In International Conference on\nLearning Representations, 2021. 2\n[22] Youngwan Lee and Jongyoul Park. Centermask: Real-time\nanchor-free instance segmentation. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition, 2020. 1, 2\n[23] Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, and Yichen Wei.\nFully convolutional instance-aware semantic segmentation.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2017. 2\n[24] Tsung-Yi Lin, Piotr Doll ´ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie. Feature pyramid\nnetworks for object detection. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\n2017. 3, 4, 6\n[25] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Doll´ar. Focal loss for dense object detection. In Pro-\nceedings of the IEEE International Conference on Computer\nVision, 2017. 4\n[26] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean Conference on Computer Vision, 2014. 2, 5\n[27] Shu Liu, Jiaya Jia, Sanja Fidler, and Raquel Urtasun. Sgn:\nSequential grouping networks for instance segmentation. In\nProceedings of the IEEE International Conference on Com-\nputer Vision, 2017. 2\n[28] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.\nPath aggregation network for instance segmentation. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2018. 1, 2, 3\n[29] Ilya Loshchilov and Frank Hutter. Decoupled weight de-\ncay regularization. In International Conference on Learning\nRepresentations, 2018. 6\n[30] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.\nV-net: Fully convolutional neural networks for volumetric\nmedical image segmentation. In International Conference\non 3D Vision, 2016. 4\n[31] Mengye Ren and Richard S Zemel. End-to-end instance seg-\nmentation with recurrent attention. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2017. 1, 2\n[32] Hamid Rezatoﬁghi, Nathan Tsoi, JunYoung Gwak, Amir\nSadeghian, Ian Reid, and Silvio Savarese. Generalized in-\ntersection over union: A metric and a loss for bounding box\nregression. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2019. 4\n[33] Bernardino Romera-Paredes and Philip Hilaire Sean Torr.\nRecurrent instance segmentation. In European Conference\non Computer Vision, 2016. 1, 2\n[34] Yunhang Shen, Rongrong Ji, Yan Wang, Yongjian Wu, and\nLiujuan Cao. Cyclic guidance for weakly supervised joint\ndetection and segmentation. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\n2019. 3\n[35] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu\nWei, and Jifeng Dai. Vl-bert: Pre-training of generic visual-\nlinguistic representations. In International Conference on\nLearning Representations, 2020. 2\n[36] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and\nCordelia Schmid. Videobert: A joint model for video and\nlanguage representation learning. InProceedings of the IEEE\nInternational Conference on Computer Vision, 2019. 2\n[37] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng\nXu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan,\nChanghu Wang, et al. Sparse r-cnn: End-to-end object de-\ntection with learnable proposals. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\n2021. 1, 2, 4, 5, 6, 7\n[38] Hao Tan and Mohit Bansal. Lxmert: Learning cross-\nmodality encoder representations from transformers. InCon-\nference on Empirical Methods in Natural Language Process-\ning, 2019. 2\n[39] Zhi Tian, Tong He, Chunhua Shen, and Youliang Yan. De-\ncoders matter for semantic segmentation: Data-dependent\ndecoding enables ﬂexible feature aggregation. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2019. 6\n[40] Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convo-\nlutions for instance segmentation. In European Conference\non Computer Vision, 2020. 7\n[41] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. Arxiv preprint, 2020. 2\n[42] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems, 2017. 1, 2, 5\n[43] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian\nSun, and Nanning Zheng. End-to-end object detection with\nfully convolutional network. Arxiv preprint, 2020. 1, 2\n[44] Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, and\nLei Li. Solo: Segmenting objects by locations. In European\nConference on Computer Vision, 2020. 1, 2\n[45] Xinpeng Wang, Chandan Yeshwanth, and Matthias Nießner.\nSceneformer: Indoor scene generation with transformers.\nArxiv preprint, 2020. 2\n[46] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chun-\nhua Shen. Solov2: Dynamic and fast instance segmenta-\ntion. In Advances in Neural Information Processing Systems,\n2020. 2, 7\n[47] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen,\nBaoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end\nvideo instance segmentation with transformers. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2021. 2, 3\n[48] Enze Xie, Peize Sun, Xiaoge Song, Wenhai Wang, Xuebo\nLiu, Ding Liang, Chunhua Shen, and Ping Luo. Polarmask:\nSingle shot instance segmentation with polar representation.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2020. 2\n[49] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Bain-\ning Guo. Learning texture transformer network for image\nsuper-resolution. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2020. 2\n[50] Han-Jia Ye, Hexiang Hu, De-Chuan Zhan, and Fei Sha. Few-\nshot learning via embedding adaptation with set-to-set func-\ntions. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2020. 2\n[51] Linwei Ye, Mrigank Rochan, Zhi Liu, and Yang Wang.\nCross-modal self-attention network for referring image seg-\nmentation. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2019. 2\n[52] Jialin Yuan, Chao Chen, and Li Fuxin. Deep variational\ninstance segmentation. In Advances in Neural Information\nProcessing Systems, 2020. 2\n[53] Rufeng Zhang, Zhi Tian, Chunhua Shen, Mingyu You, and\nYouliang Yan. Mask encoding for single shot instance seg-\nmentation. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2020. 2, 6, 7\n[54] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip HS Torr, et al. Rethinking semantic segmen-\ntation from a sequence-to-sequence perspective with trans-\nformers. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2021. 2\n[55] Xingyi Zhou, Dequan Wang, and Philipp Kr ¨ahenb¨uhl. Ob-\njects as points. Arxiv preprint, 2019. 2\n[56] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable detr: Deformable transformers\nfor end-to-end object detection. Arxiv preprint, 2020. 1, 2, 6",
  "topic": "End-to-end principle",
  "concepts": [
    {
      "name": "End-to-end principle",
      "score": 0.8209535479545593
    },
    {
      "name": "Transformer",
      "score": 0.5677983164787292
    },
    {
      "name": "Computer science",
      "score": 0.45086392760276794
    },
    {
      "name": "Segmentation",
      "score": 0.4233425557613373
    },
    {
      "name": "Business",
      "score": 0.40196678042411804
    },
    {
      "name": "Artificial intelligence",
      "score": 0.25081801414489746
    },
    {
      "name": "Engineering",
      "score": 0.2048729956150055
    },
    {
      "name": "Electrical engineering",
      "score": 0.1983460783958435
    },
    {
      "name": "Voltage",
      "score": 0.04631951451301575
    }
  ],
  "institutions": [],
  "cited_by": 54
}