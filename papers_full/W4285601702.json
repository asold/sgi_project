{
  "title": "MA-ViT: Modality-Agnostic Vision Transformers for Face Anti-Spoofing",
  "url": "https://openalex.org/W4285601702",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2588921820",
      "name": "Ajian Liu",
      "affiliations": [
        "Macau University of Science and Technology",
        "Institute of Automation",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2104748949",
      "name": "Yanyan Liang",
      "affiliations": [
        "Macau University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3201612248",
    "https://openalex.org/W2971810730",
    "https://openalex.org/W2728977829",
    "https://openalex.org/W4313166619",
    "https://openalex.org/W2493468541",
    "https://openalex.org/W2957186196",
    "https://openalex.org/W3139414251",
    "https://openalex.org/W3006377070",
    "https://openalex.org/W2963656031",
    "https://openalex.org/W3109432287",
    "https://openalex.org/W2964245886",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3116054336",
    "https://openalex.org/W3121127014",
    "https://openalex.org/W3111508288",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3046357449",
    "https://openalex.org/W2952476201",
    "https://openalex.org/W3099271666",
    "https://openalex.org/W3156245112",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3208437533",
    "https://openalex.org/W3035459165"
  ],
  "abstract": "The existing multi-modal face anti-spoofing (FAS) frameworks are designed based on two strategies: halfway and late fusion. However, the former requires test modalities consistent with the training input, which seriously limits its deployment scenarios. And the latter is built on multiple branches to process different modalities independently, which limits their use in applications with low memory or fast execution requirements. In this work, we present a single branch based Transformer framework, namely Modality-Agnostic Vision Transformer (MA-ViT), which aims to improve the performance of arbitrary modal attacks with the help of multi-modal data. Specifically, MA-ViT adopts the early fusion to aggregate all the available training modalities’ data and enables flexible testing of any given modal samples. Further, we develop the Modality-Agnostic Transformer Block (MATB) in MA-ViT, which consists of two stacked attentions named Modal-Disentangle Attention (MDA) and Cross-Modal Attention (CMA), to eliminate modality-related information for each modal sequences and supplement modality-agnostic liveness features from another modal sequences, respectively. Experiments demonstrate that the single model trained based on MA-ViT can not only flexibly evaluate different modal samples, but also outperforms existing single-modal frameworks by a large margin, and approaches the multi-modal frameworks introduced with smaller FLOPs and model parameters.",
  "full_text": "MA-ViT: Modality-Agnostic Vision Transformers for Face Anti-Spooﬁng\nAjian Liu1;2 , Yanyan Liang1\u0003\n1School of Computer Science and Engineering, Faculty of Innovation Engineering, Macau University of\nScience and Technology, Macau\n2CBSR&NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China\najianliu92@gmail.com, yyliang@must.edu.mo\nAbstract\nThe existing multi-modal face anti-spooﬁng (FAS)\nframeworks are designed based on two strategies:\nhalfway and late fusion. However, the former re-\nquires test modalities consistent with the training\ninput, which seriously limits its deployment scenar-\nios. And the latter is built on multiple branches to\nprocess different modalities independently, which\nlimits their use in applications with low memory\nor fast execution requirements. In this work, we\npresent a single branch based Transformer frame-\nwork, namely Modality-Agnostic Vision Trans-\nformer (MA-ViT), which aims to improve the per-\nformance of arbitrary modal attacks with the help\nof multi-modal data. Speciﬁcally, MA-ViT adopts\nthe early fusion to aggregate all the available train-\ning modalities’ data and enables ﬂexible testing of\nany given modal samples. Further, we develop\nthe Modality-Agnostic Transformer Block (MATB)\nin MA-ViT, which consists of two stacked atten-\ntions named Modal-Disentangle Attention (MDA)\nand Cross-Modal Attention (CMA), to eliminate\nmodality-related information for each modal se-\nquences and supplement modality-agnostic live-\nness features from another modal sequences, re-\nspectively. Experiments demonstrate that the sin-\ngle model trained based on MA-ViT can not only\nﬂexibly evaluate different modal samples, but also\noutperforms existing single-modal frameworks by\na large margin, and approaches the multi-modal\nframeworks introduced with smaller FLOPs and\nmodel parameters.\n1 Introduction\nFace Anti-Spooﬁng (FAS) aims to strengthen the face\nrecognition system from a variety of presentation attacks\n(PAs) [Boulkenafet et al., 2017; Liu et al., 2018; Liu et al.,\n2016]. It has become an increasingly concern [Zhang et al.,\n2020a; Liu et al., 2020; Liu et al., 2021c; Liu et al., 2021e;\nLiu et al., 2021d] due to its wide applications in face pay-\nment, phone unlocking, and self-security inspection.\n\u0003Corresponding author\nFigure 1: Comparison with existing multi-modal fusion strategies.\n(a) Halfway Fusion: it fuses multi-modal information by merging\nindependent modal branches and requires the test modalities to be\nconsistent with the training modalities. (b) Late Fusion: it fuses\nthe multi-modal information at the decision level and requires com-\nplete modal branch for each input modality. (c) Early Fusion: it\naggregates all available multi-modal data as input to realize ﬂexible\nmodal testing in a single branch structure.\nWith the increasingly advanced presentation attack instru-\nments (PAIs) and acquisition sensors, face presentation at-\ntack detection (PAD) algorithms are also expanded from RGB\nspectrum [Liu et al., 2018; Shao et al., 2019; George and\nMarcel, 2019; Liu et al., 2019; Yu et al., 2020] to multi-\nspectrum to explore more reliable spooﬁng traces, which\ncan be divided into halfway fusion framework [Zhang et al.,\n2020b; Parkin and Grinchuk, 2019; George et al., 2019] and\nlate fusion framework [George and Marcel, 2021]. As shown\nin Fig. 1 (a), the halfway fusion is one of the commonly used\nfusion strategy, which combines the sub-networks of different\nmodalities at a later stage via the feature concatenation. Al-\nthough the halfway fusion can improve the robustness of PAD\nsystems with the help of multi-modal information, their de-\nsign concept is not intelligent enough, which requires the con-\nsistent training and testing modalities. If any modality disap-\npears during testing, these methods would fail to distinguish\nlive vs.fake faces and result in poor performance. In order to\nfreely test any modal samples, as shown in Fig. 1 (b), the late\nfusion strategy retains a speciﬁc branch for each modality to\ncapture different modal information independently, and fuses\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1180\nthe multi-modal information at the decision level (prediction\nembeddings or scores) to retain the function of any modal\nsample can be tested. However, these models may be slow to\nexecute or large to store, limiting their use in applications or\nplatforms with low memory or fast execution requirements,\ne.g., mobile phones. To sum up, it is a signiﬁcant work to de-\nsign a framework can combine relevant information from the\ndifferent modalities without introducing additional parame-\nters (or a small number of acceptable parameters) to improve\nthe performance over using only one modality.\nWhether it is halfway or late fusion, the common is that\ndifferent modal inputs need to be mapped to a higher seman-\ntic layer in advance for knowledge fusion. Inspired by the\nsuccess of Transformer architecture in natural language tasks,\nseveral recent works have also applied transformer-based fu-\nsion between image-text, image-audio, and image-text-audio\nwith self-attention. In this work, as shown in Fig. 1 (c), we\nadopt early fusion strategy by projecting all inputs from dif-\nferent modalities into a joint embedding space at input level\nand capture intra- and inter-modality interactions homoge-\nneously within a pure transformer architecture. Compared\nwith ConvNet-based framework, Transformer can not only\ncapture local spooﬁng traces and establish their dependence\non the patch tokens for a long time, but also can fuse multi-\nmodal information directly without aligning different modal\nfeatures in advance. Therefore, abandoning the CNNs to\nmine spooﬁng traces, we adopt a single branch transformer\nframework to aggregate all the available training modalities’\ndata and introduce the Modality-Agnostic Transformer Block\n(MATB) which consists of two cascaded attentions named\nModal-Disentangle Attention (MDA) and Cross-Modal At-\ntention (CMA) to improve the performance of any single-\nmodal attacks at the inner- and inter modal levels, respec-\ntively. Speciﬁcally, the former eliminates modal-related in-\nformation in the classiﬁcation token under the guidance of\na introduced modality token, and the latter supplements the\nmodality-agnostic liveness information from other modali-\nties. To sum up, the main contributions of this paper are sum-\nmarized as follows:\n• We present a novel Modality-Agnostic Vision Trans-\nformer (MA-ViT) framework to improve the perfor-\nmance of any single modal FAS system with the help of\navailable multi-modal training data. Compared with the\nprevious multi-modal fusion frameworks, our approach\nhas the advantages of ﬂexible testing of different modal\nsamples and less introduction of additional model pa-\nrameters.\n• We develop the Modality-Agnostic Transformer Block\n(MATB) in MA-ViT with two effective attentions,\nnamely Modal-Disentangle Attention (MDA) and\nCross-Modal Attention (CMA), to achieve the modality-\nagnostic and complementary liveness features learning,\nrespectively.\n• Extensive experiments are conducted and demonstrate\nthat the proposed MA-ViT can improve the performance\nof a single-modal system with the help of multi-modal\ndata, with only acceptable FLOPs and model parameters\nare increased.\n2 Modality-Agnostic ViT (MA-ViT)\n2.1 Overall Framework\nAs shown in Fig. 2, our MA-ViT is built on the standard\nViT [Dosovitskiy et al., 2020] which consists of tokeniza-\ntion module, transformer encoder, and classiﬁcation head. In\naddition, a learnable modality token is introduced to summa-\nrizes the modal information from patch tokens. A proposed\nMATB is inserted after Standard Transformer Block (STB) to\ndisentangle the modal information and supplement the live-\nness features for the classiﬁcation token, respectively. Let Ii\nrepresents the input in a training batch, where ican be rand\ndfor the RGB and Depth samples, and also can be easily ex-\ntended to other modalities.\nMulti-Modal Tokenization Module. MA-ViT ﬁrst splits\nany modal inputI into a sequence of non-overlapping patches\nby a patch splitting module and then linearly projecting\npatches into tokens xpat ∈Rn×D, where n is the number\nof resulting patches, and Dis vector size through all of MA-\nViT layers. Similar to BERT and DeiT[Touvron et al., 2021],\na learnable class token (CLS) xcls = z0;cls ∈R1×D and\na modality token (MOD) xmod = z0;mod ∈R1×D are con-\ncatenated to the patch tokens, who serves as the image and\nmodality agents for classiﬁcation. Due to different modal\nimages are collected by ﬁxed position multi-spectral sen-\nsor, they share location information and exclusive spectrum\nspace. Therefore, position embeddings xpos ∈R(n+2)×D\nand spectrum embeddings xspe ∈R(n+2)×D are added to\neach token to retain positional and modal informations. The\ntokenization process is expressed as follows:\nzi\n0 = [xcls||xi\nmod||xi\npat] + (xpos + xi\nspe);zi\n0 ∈RN×D (1)\nwhere ||means token concatenation. i ∈ {r;d}and N =\nn+2 represent the modal type and the total number of tokens.\nrespectively. The resulting sequence z0 serves as input to the\nfollowing transformer encoder.\nTransformer Encoder with MATBs. After tokenization\nmodule, the sequences zi\n0 (i∈{r;d}) from all modalities are\nmixed through STB and MATB, which constitute one trans-\nformer block of our MA-ViT. The complete transformer en-\ncoder consists of K transformer blocks by repeatedly stack-\ning. As illustrated in Fig. 2, a MATB with MDA receives\nthe output of the previous STB as input, and with CMA re-\nceives the current intra-modal sequences (zi) and the previous\ncross-modal sequence (zrd) as input. Similar to Multi-headed\nSelf-Attention (MSA) in STB, a Layer normalization (LN) is\napplied before each module, and a residual shortcut is applied\nafter each module. Speciﬁcally, the processing ofk-th (where\nk = 1 ;:::;K ) Transformer block can be expressed as (omit\nMLP operation):\nz′i\nk = MSA(LN(zi\nk−1)) + zi\nk−1\nzrd\nk = MCMA(LN(zr\nk−1);LN(zd\nk−1)) + zrd\nk−1\nzi\nk = MMDA(LN(z′i\nk)) + z′i\nk\n(2)\nwhere MSA and MCMA (Multi-headed CMA) and the fol-\nlowing MLP weights are shared. zi\nk and zrd\nk (cross-modal se-\nquence) are the output of MATB (MDA) and MATB (CMA).\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1181\nFigure 2: The architecture of Modality-Agnostic Vision Transformer (MA-ViT). It is built on a single ViT branch and consists of Multi-\nModal Tokenization Module (Eq. 1), Transformer Encoder, and Modal Shared Classiﬁcation Heads. (Eq. 3). A completed transformer\nencoder contains K transformer block, which consists of Standard Transformer Block (STB) and Modality-Agnostic Transformer Block\n(MATB). Speciﬁcally, the STB processes all modal sequences (yellow and black gray arrows) independently and outputs them into MATB\n(MDA). And the MATB (CMA) takes all modal sequences as inputs and outputs the cross-modal sequences (green arrow).\nModal Shared Classiﬁcation Heads. See from Fig. 2,\nwe provide two modal shared classiﬁcation heads for all se-\nquences to meet the requirements of any modal sample can be\ntested, which are supervised and jointly optimized by binary\ncross-entropy (BCE). The output sequences at theK-th block\nin Transformer encoder zK;cls and zK;mod are served as the\nagents for liveness and modality classiﬁcation, respectively.\nThe total loss Ltotal to minimize is given as:\nLi\ncls = BCE(MLPcls(LN(zi\nK;cls));ycls);\nLi\nmod = BCE(MLPmod(LN(zi\nK;mod));ymod);\nLi\ntotal = Li\ncls + Lrd\ncls + Li\nmod;i ∈{r;d}\n(3)\nwhere Lrd\ncls is the loss for cross-modal sequence zrd\nK. The\nclassiﬁcation head is implemented by MLP with a single\nlinear layer. y is the ground truth for sample I. In de-\ntail, ycls = 0 =1 for attack/bonaﬁde and ymod = 0 =1 for\nRGB/Depth, respectively.\n2.2 Modality-Agnostic Transformer Block.\nMulti-headed Modal-Disentangle Attention (MMDA).\nHow to determine the modality-irrelated patch tokens is the\nprimary task in MMDA. By analyzing the attention matrix,\nwhich is essentially a relevance maps, whose each row cor-\nresponds to a link for each token given the other tokens. In\nwhich the relevance map that corresponds to theCLS/MOD to-\nken links each of the tokens to the CLS/MOD token, and the\nstrength of this link can be intuitively considered as an indi-\ncator of the contribution of each token to the classiﬁcation.\nSee from the MATB (MDA) in Fig. 2, for any modal se-\nquence zi, as shown in Eq. 4 and Fig. 3, we ﬁrst compute\nqmod of MOD token and kmod\npat of patch tokens through two\nlearnable parameters Wmod\nq and Wmod\nk , respectively. Then,\nwe compute the dot products of the qmod with kmod\npat , di-\nvide each by\np\nD=h, and apply a softmax function to ob-\ntain the modal relevance map mapmod. Then, we identify\nthe modality-irrelated (or weakly related) patch tokens by ex-\ncluding strong links in mapmod, which is completed by a\nthreshold function Γ\u0015(·). Speciﬁcally, Γ\u0015(·) aims to ﬁnd the\nmodality-related patch tokens by thresholding the modal rel-\nevance map to keep \u0015(∈[0;1]) proportional mass. It outputs\na mask matrix M with values of 1 and 0, where 1/0 means\nto modality-related/-irrelated the patch tokens of the corre-\nsponding position.\nIn order to disentangle the modal information for classiﬁ-\ncation, we take the mask matrixM as indicators to discard the\nmodality-related patch tokens. As shown in Eq. 4, we ﬁrst\ncompute qcls of CLS token, kcls\npat and vcls\npat of patch tokens\nthrough three learnable parameters Wcls\nq , Wcls\nk and Wcls\nv ,\nrespectively. Then, we obtain the attention map mapcls in a\nsimilar way by qcls and kcls\npat, while disconnecting the links\nwith the modality-related patch tokens under the guidance of\nM. See from the Eq. 4, this process can be completed with\na selection function Γ′\nM(·) based on the index position of a\ngiven matrix Mi. Finally, we successively obtain the weights\nby a softmax function to mapcls and the output of MDA by\nthe weighted sum over all values of patch tokens vcls\npat. This\nprocess is denoted as:\n[qcls;kcls\npat;vcls\npat] = [zclsWcls\nq ;zpatWcls\nk ;zpatWcls\nv ];\n[qmod;kmod\npat ] = [zmodWmod\nq ;zpatWmod\nk ];\nmapmod = qmod(kmod\npat )T=\np\nD=h;\nmapcls = qcls(kcls\npat)T=\np\nD=h;\nM = Γ\u0015(mapmod);\nMDA(zi) = softmax[Γ′\nM(mapcls)] ·vcls\npat\n(4)\nwhere Dand hare the embedding dimension and number of\nheads, respectively. Wq, Wk and Wv ∈RD×(D=h). Γ′\nM(·)\nis a selection function deﬁned as:\nΓ′\nM(A) =\n\u001a\n−∞;Ma;b >0\nAa;b;Ma;b = 0 (5)\nSimilar to [Wang et al., 2021 ], we discard the modality-\nrelated tokens by setting the attention values in mapcls to\nsmall enough constant. MMDA is an extension of MDA in\nwhich we run h mutual-attention operations in parallel. As\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1182\nFigure 3: Implementation details of Modal-Disentangle Attention\n(MDA) and and Cross-Modal Attention (CMA).\nshown in Fig. 2 and Eq. 2, we apply a residual shortcuts after\nthe MDA and concatenate with patch tokens zpat to obtain a\nnew sequences z′:\nz′i\ncls = MMDA(zi) + zi\ncls;z′i = [z′i\ncls||zi\nmod||zi\npat] (6)\nwhere ||means token concatenation, the output sequences z′i\nwill be used as the input sequences of the next stage.\nMulti-headed Cross-Modal Attention (MCMA).How to\neffectively produce a stronger modality-agnostic representa-\ntion with the help of multi-modal data is the primary task\nin MCMA. Inspired by CDTrans [Xu et al., ], we summa-\nrize the common information of different modal sequences to\nconstruct cross-modal sequence output by MCMA, which is\nsupervised by the same label. In other words, the MCMA fur-\nther guides MSA to pay attention to modality-agnostic live-\nness features by sharing with its parameters.\nSee from the MATB (CMA) module in Fig. 2, it takes all\nmodal sequences zr and zdas inputs and outputs cross-modal\nsequence zrd. Similar to MSA [Dosovitskiy et al., 2020], as\nshown in Fig. 3, we ﬁrst compute queries qr of modal se-\nquence zr, and keys kd, values vd from another modal se-\nquence zd, respectively. Then, the attention function Ard is\ncomputed on the set of queries qr simultaneously with all\nkeys kd. Finally, the outputs of CMA is a weighted sum over\nall values vd, denoted as:\n[qr;kd;vd] = [zrWcls\nq ;zdWcls\nk ;zdWcls\nv ];\nArd = softmax(qr(kd)T=\np\nD=h);\nCMA(zi) = [Ard ·vd||Adr ·vr];i ∈{r;d}\n(7)\nwhere the calculation process of Adr and vr is similar to that\nof Ard and vd by simply swapping the index r and d. Wq,\nWk and Wv are shared with MSA. ||means concatenating\ncross-modal sequences along the batch dimension. Similar\nto MDA in Eq. 6, we also apply a residual shortcuts after\nthe CMA, and obtain the new cross-modal sequence z′rd =\nCMA(zi) + zrd (The operation is not performed in the 1-th\nlayer).\n3 Experiments\n3.1 Experimental Setup\nDatasets & Protocols. We use three commonly used\nmulti-modal and a single-modal FAS datasets for exper-\niments, including CASIA-SURF (MmFA) [Zhang et al.,\n2019], CASIA-SURF CeFA (CeFA) [Liu et al., 2021b ],\nWMCA [George et al., 2019] and OULU-NUPU [Boulke-\nnafet et al., 2017] (OULU). MmFA consists of 1;000 sub-\njects with 21;000 videos and each sample has 3 modalities,\nand provides a intra-testing protocol. CeFA covers 3 ethnici-\nties, 3 modalities, 1;607 subjects, and provides ﬁve protocols\nto measure the affect under varied conditions. We select the\nProtocol 1, 2, and 4 for experiments. WMCA contains a wide\nvariety of 2D and 3D presentation attacks, which introduces\n2 protocols: grandtest protocol which emulates the “seen” at-\ntack scenario and the “unseen” attack protocol that evaluates\nthe generalization on an unseen attack.\nTest Scenario Settings. We consider two test scenarios.\nThe ﬁrst is the commonly used setting that the test modalities\nneed to be consistent with the training stage. In this scenario,\nwe evaluate the intra-testing performances of each dataset\nbased on the provided protocols, and evaluate the robustness\nof our approach through cross-testing experiments between\nthese datasets. The second is the ﬂexible modal test scenario,\nwhich means the tester can provide any single-modal or any\nmulti-modal paired samples.\nEvaluation Metrics. In intra-testing experiments, Attack\nPresentation Classiﬁcation Error Rate (APCER), Bonaﬁde\nPresentation Classiﬁcation Error Rate (BPCER), and ACER\nare used for the metrics. The ACER on testing set is deter-\nmined by the Equal Error Rate (EER) threshold on dev sets\nfor MmFA, CeFA, OULU, and the BPCER=1%threshold for\nWMCA. TPR(@FPR=10−4) is also provided for MmFA. For\ncross-testing experiments, Half Total Error Rate (HTER) is\nadopted as the metric, which computes the average of False\nRejection Rate (FRR) and the False Acceptance Rate (FAR),\nand the threshold computed in dev set using EER criteria.\nImplementation Details. Our models can be freely built\non any version of ViT [Dosovitskiy et al., 2020 ], which\nmeans that our MATB can be compatible with other im-\nprovements to ViT, such as DeiT [Touvron et al., 2021] and\nSwin [Liu et al., 2021f ]. In our experiments, we adopt\nViT-S/16 as the backbone through comparative experiments,\nwhich means the “Small” variant withK = 12.\nWe train our model with the ViT-S/16 backbone, which is\ninitialized with weights provided by [Touvron et al., 2021],\nand other newly added layers are randomly initialized. We\nresize all modal images to 224 ×224 and train all models\nwith 50 epochs via Adam solver. All models are trained with\na batch seize of 8 and an initial learning rate of 0.0001 for\nall epochs. We set \u0015 = 0:8 in MDA according to compara-\ntive experiments. In the testing stage, for single-mdoal sam-\nples, we only activate the input interface of the corresponding\nmodality (mainly the corresponding position and spectrum\nembeddings). As can be seen more clearly from Fig 2, the\ngreen line only outputs the results of multi-modal samples,\nwhile the yellow and gray lines output the results of corre-\nsponding single-modal samples respectively.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1183\nMethod seen unseen\nFlexiblemask Replay Fak\nehead Prints Glasses Papermask Rigidmask mean±std\nMC-PixBiS 1.80 49.70 3.70 0.70 0.10 16.00 0.20 3.40 10.50±16.70\nMCCNN-OCCL-GMM 3.30 22.80 31.40 1.90 30.00 50.00 4.80 18.30 22.74±15.30\nCMFL 1.70 12.40 1.00 2.50 0.70 33.50 1.80 1.70 7.60±11.20\nViT 3.79 14.56 1.58 4.14 0.83 36.00 2.82 2.19 7.93±13.80\nMA-ViT 1.45 9.76 0.93 0.55 0.00 14.00 0.00 1.46 3.81±5.67\nTable 1: Comparison of ACER (%) values on Protocol “seen” and “unseen” for the WMCA. Best results are bolded.\nMethod APCER BPCER ACER TPR(@FPR=10−4)\nSEF 3.80 1.00 2.40 56.80\nMS-SEF 2.30 0.30 1.30 81.40\nViT 3.52 9.50 6.51 44.33\nMA-ViT 0.78 0.83 0.80 82.83\nTable 2: The results on MmFA. A large TPR(%) and a lower ACER\n(%) indicate better performance. Best results are bolded.\nPro. Method APCER(%) BPCER(%) ACER(%)\n1\nPSMM 2.40±0.60 4.60±2.30 3.50±1.30\nViT 4.55±7.29 1.83±1.59 3.19±4.07\nMA-ViT 1.45±1.75 0.75±0.43 1.10±1.09\n2\nPSMM 7.70±9.00 3.10±1.60 5.40±5.30\nViT 3.48±1.81 0.92±0.12 2.20±0.85\nMA-ViT 0.12±0.08 0.09±0.12 0.10±0.01\n4\nPSMM 7.80±2.90 5.50±3.00 6.70±2.20\nHulking 3.25±1.98 1.16±1.12 2.21±1.26\nSuper 0.62±0.43 2.75±1.50 1.68±0.54\nBOBO 1.05±0.62 1.00±0.66 1.02±0.59\nViT 20.44±17.12 7.42±3.96 13.92±10.45\nMA-ViT 2.10±1.47 1.17±0.38 1.64±0.89\nTable 3: Evaluation results (%) on the Protocol 1, 2, and 4 of CeFA\ndataset. Note that a lower ACER value indicates better performance.\n3.2 Fixed Modal Scenario Evaluations\nOn the three multi-modal datasets, we compare with the base-\nline method ViT that removed the MATB from MA-ViT, and\nthe previous state-of-the-art (SOTA) methods.\nIntra-Testing Results. For MmFA, we compare with the\nbenchmark SEF [Zhang et al., 2019] and the multi-scale ver-\nsion MS-SEF [Zhang et al., 2020b], respectively. From the\nTab. 2, we can observe that the performance of ViT is worse\nthan SEF due to the lack of modal fusion ability. However,\nwhen equipped with MATB, MA-ViT improves the TPR from\n44:33% to 82:83%, reduces ACER from 6:51% to 0:80%,\nand outperforms MS-SEF by a large margin, which uses Im-\nageNet pretrain.\nFor CeFA, we compare our approach with the benchmark\nPSMM [Liu et al., 2021b] and the top three methods intro-\nduced in the challenge [Liu et al., 2021a] with the Protocol 4,\ni.e., BOBO, Super and Hulking. It can be seen from Tab. 2\nthat the performance of ViT is superior to PSMM on Protocol\n1 and Protocol 2 due to the pre-training model. Our MA-ViT\nfurther reduces the ACER on Protocol 1 and Protocol 2 to\n1:10% and 0:10%, and outperforms the Hulking and Super\nmethods with ACER gains of −0:04% (1:68% vs. 1:64%)\nand −0:57% (2:21% vs. 1:64%), respectively.\nMethod Train(MmF\nA),Test(WMCA) Train(WMCA),T\nest(MmFA)\nAux.(Depth) 24.54 12.35\nViT 31.87 22.79\nCMA-ViT 27.97 16.97\nMDA-V\niT 24.84 13.30\nMA-ViT 20.63 10.41\nTable 4: The HTER (%) values from the cross-testing between\nMmFA and WMCA (Protocol “seen”) datasets.\nIn order to perform a fair comparison with prior meth-\nods, only RGB and Depth data in WMCA [George et\nal., 2019 ] are used for intra-testing experiments. Tab. 1\npresents the comparisons of ACER to the SOTA ConvNet-\nbased methods, including MC-PixBiS [George and Mar-\ncel, 2019 ], MCCNN-OCCL-GMM [George and Marcel,\n2020], MC-ResNetDLAS [Parkin and Grinchuk, 2019 ] and\nCMFL [George and Marcel, 2021 ]. Compared with the pre-\nvious best results, our MA-ViT achieves signiﬁcantly bet-\nter performance with a large margin in “seen” and “unseen”\nprotocols: −0:25% over CMFL (1:70%), and −3:79% over\nCMFL (7:60%), respectively. It is worth noting that MA-ViT\nnoticeably surpass these methods on two challenging sub-\nprotocols of “unseen” protocol: when “Flexiblemask” and\n“Glasses” are not seen in the training stage. We analyze that\nthe commonality of the two attacks is 3D facial structure, re-\nalistic color-texture, and only local regions contain spooﬁng\ntraces, which are easy to be ignored by the ConvNet-based\nmethods and play a long-term indicative role in ViT.\nCross-Testing Results. To evaluate the robustness, we\nconduct cross-testing experiments between models trained on\nMmFA and WMCA with Protocol “seen”. We also introduce\nthe previous SOTA ConvNet-based method Aux.(Depth)[Liu\net al., 2018] as a baseline. Tab. 4 lists the HTER of all meth-\nods trained on one dataset and tested on another dataset. From\nthese results, MA-ViT outperforms Aux.(Depth) whether\ntested on WMCA (24:54% vs. 20:63%) or MmFA (12:35%\nvs. 10:41%). When comparing the results of MDA-ViT and\nCMA-ViT, we can conclude that MDA plays a more impor-\ntant role in improving the robustness. See Tab. 4 for de-\ntails, when replacing the element of ViT from CMA to MDA,\nthe results are further reduced from 27:97% to 24:84% when\ntested on WMCA and from 16:97% to 13:30% when tested\non MmFA, respectively. Due to the mismatch of sensors, res-\nolutions, attack types, and settings between different datasets,\nit is more effective to mine potential features by removing the\ninterferences of modal information.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1184\nMethod Train Test MmFA WMCA\nAPCER BPCER ACER APCER BPCER ACER\nFixed\nmodal testing\nSOT\nA\nR R 40.30 1.60 21.00 65.65 0.00 32.82\nD D 6.00 1.20 3.60 11.77 0.31 6.04\nI I 38.60 0.40 19.40 5.03 0.00 2.51\nViT\nR R 22.67 30.67 26.67 2.49 12.17 7.33\nD D 3.02 7.50 5.26 4.75 2.61 3.68\nI I 3.52 9.50 6.51 4.98 4.35 4.66\nFlexible\nmodal testing\nMA-ViT\nR&D&I R 8.88 14.00 11.43 7.69 3.48 5.59\nR&D&I D 0.78 0.83 0.80 6.56 0.00 3.28\nR&D&I I 0.67 2.33 1.50 3.39 0.87 2.13\nTable 5: Comparison of ﬂexible modal results (%) based on multi-\nmodal datasets. The ‘SOTA’ means the method with public results\non the corresponding dataset. R&D&I indicates the method receives\nRGB (R), Depth (D) and IR (I) paired samples as input.\n3.3 Flexible Modal Scenario Evaluations\nIn order to explore the ability of our approach for modality-\nagnostic features learning, we conducted experiments on\ndataset MmFA, and Protocol “seen” of WMCA. Before re-\nporting the results in ﬂexible modal scenarios, we ﬁrst list the\nresults of the SOTA methods on RGB (R), Depth (D) and IR\n(I) modalities on each dataset, i.e., MS-SEF for MmFA, and\nMC-CNN [George et al., 2019] for WMCA.\nBy comparing the results in Tab. 5, we can draw the fol-\nlowing two conclusions: (1) Our MA-ViT achieves the best\nresults in each modality by training only one model. Such\nas compared with MS-SEF on MmFA, MA-ViT reduces the\nACER of RGB, Depth and IR results by 9:57% (11:43% vs.\n21:00%), 2:80% (0:80% vs. 3:60%) and 17:90% (1:50%\nvs. 19:40%) points where they are trained in an indepen-\ndent model. Experimental results show that our model can\nﬂexibly test samples with any modalities without retraining\nthe model again. (2) Our MATB can improve the any single\nmodal performance with the help of multi-modal data. This\nclearly demonstrates that the proposed MATB enables any\nmodal sequences to mine modality-agnostic liveness features\nfor ﬂexible modal testing. There is a similar conclusion on\nWMCA when compared with MC-CNN.\n3.4 Ablation Study\nEffect of the Frameworks. In this experiment, we com-\npare MA-ViT(S) with a CNN method Aux.(Depth) [Liu et\nal., 2018 ], with a variety of ViT family architectures, i.e.,\nbaseline ViT(S), “tiny” (T) and “base” (B) variants. In ad-\ndition to the metric of ACER, FLOPs and parameters mea-\nsured for all the models and shown as Tab. 6. Compared\nwith ViT(S), our approach only introduces 0:35 (G) FLOPs\nand 1:06 (M) parameters, but has signiﬁcant performance im-\nprovement on three modalities. When compared with MA-\nViT(T) and MA-ViT(B), we ﬁnd that the “small” variant is\nmore suitable for face anti-spooﬁng tasks, which may be re-\nlated to the dataset size and task category. Finally, it can be\nseen that MDA-ViT and CMA-ViT outperforms ViT(s) by a\nbig margin, which improve the any single modal performance\nby eliminating modality-related information and fusing live-\nness features from another modal sequences.\nEffect of the \u0015 in MDA. In MDA, the \u0015 determines the\nnumber of modality-irrelated patch tokens. The experimental\nresults are shown in Tab. 6. When \u0015 = 0:8 in MDA-ViT, all\nModel MmFA WMCA FLOPs\n(G)\nParams\n(M)R D I R D I\nViT(S) 26.67 5.26 6.51 7.33 3.68 4.66 1.61 2.30\nMDA-V\niT(\u0015=0.7) 14.08 3.17 1.75 6.95 5.49 6.60 1.92 3.36\nMDA-V\niT(\u0015=0.8) 13.78 2.01 1.75 6.25 2.61 4.43 1.92 3.36\nMDA-V\niT(\u0015=0.9) 16.74 2.06 2.22 6.60 5.49 4.76 1.92 3.36\nCMA-ViT 9.71 3.25 1.53 8.97 2.02 2.13 1.64 2.30\nMA-ViT(S) 11.43 0.80 1.50 5.59 3.28 2.13 1.96 3.36\nMA-ViT(T) 12.89 1.67 3.64 5.77 4.38 2.92 0.49 0.86\nMA-ViT(B) 18.00 1.61 5.31 5.64 3.94 2.70 7.79 13.26\nTable 6: Ablation study with different architecture on MmFA and\nWMCA with Protocol “seen”. ACER values are reported.\nFigure 4: The mining process of informative patch tokens in each\nmodal sequence which are covered by the red mask and obtained\nby thresholding the maps to keep 80% (\u0015 = 0:8) of the mass. The\nsample is from the WMCA dataset, where the sample category is\nmarked at the top.\nexperiments achieve the best results.\nIn Fig. 4, we visualize the patch tokens mined by the MDA\nin the last layer. For each modal sequence (each column), we\nvisualize the areas of modality-irrelated patch tokens (the ﬁrst\nrow), classiﬁcation-irrelated patch tokens (the second row),\nand the ﬁnal informative patch tokens (the third row). We can\nsee that without MDA, the informative patch tokens are dis-\ntributed in global regions, including modality-irrelated region\nand background region. While, the distribution is adjusted af-\nter MDA, which refers to the patch tokens in facial region.\n4 Conclusion\nIn this work, we present a pure transformer-based framework\nnamed MA-ViT, which aims to guide any modal sequence\nto learn modality-agnostic liveness features, and fuse the\nliveness complementary information from other modalities\nthrough the developed MMDA and MCMA respectively. Ex-\nperiments show that our approach MA-ViT only introduces\n0:35 (G) FLOPs and 1:06 (M) parameters, but gains signiﬁ-\ncant improvements compared with baseline method ViT(S).\nAcknowledgements\nThis work was supported by the National Key R&D Program\nof China 2021YFF0602103, the Science and Technology De-\nvelopment Fund of Macau (0008/2019/A1, 0010/2019/AFJ,\n0025/2019/AKP, 0004/2020/A1, 0070/2021/AMJ)\nand Guangdong Provincial Key R&D Programme:\n2019B010148001.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1185\nReferences\n[Boulkenafet et al., 2017] Zinelabinde Boulkenafet, Jukka\nKomulainen, Lei Li, Xiaoyi Feng, and Abdenour Hadid.\nOulu-npu: A mobile face presentation attack database with\nreal-world variations. In FGR, pages 612–618, 2017.\n[Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition\nat scale. arXiv preprint arXiv:2010.11929, 2020.\n[George and Marcel, 2019] Anjith George and S ´ebastien\nMarcel. Deep pixel-wise binary supervision for face pre-\nsentation attack detection. In ICB, pages 1–8, 2019.\n[George and Marcel, 2020] Anjith George and S ´ebastien\nMarcel. Learning one class representations for face pre-\nsentation attack detection using multi-channel convolu-\ntional neural networks. IEEE TIFS, 16:361–375, 2020.\n[George and Marcel, 2021] Anjith George and S ´ebastien\nMarcel. Cross modal focal loss for rgbd face anti-spooﬁng.\nIn CVPR, pages 7882–7891, 2021.\n[George et al., 2019] Anjith George, Zohreh Mostaani,\nDavid Geissenbuhler, Olegs Nikisins, Andr ´e Anjos, and\nS´ebastien Marcel. Biometric face presentation attack de-\ntection with multi-channel convolutional neural network.\nIEEE TIFS, 15:42–55, 2019.\n[Liu et al., 2016] Siqi Liu, Baoyao Yang, Pong C Yuen, and\nGuoying Zhao. A 3d mask face anti-spooﬁng database\nwith real world variations. In CVPRW, pages 100–106,\n2016.\n[Liu et al., 2018] Yaojie Liu, Amin Jourabloo, and Xiaom-\ning Liu. Learning deep models for face anti-spooﬁng: Bi-\nnary or auxiliary supervision. In CVPR, pages 389–398,\n2018.\n[Liu et al., 2019] Ajian Liu, Jun Wan, Sergio Escalera, Hugo\nJair Escalante, Zichang Tan, Qi Yuan, Kai Wang, Chi Lin,\nGuodong Guo, Isabelle Guyon, et al. Multi-modal face\nanti-spooﬁng attack detection challenge at cvpr2019. In\nCVPRW, pages 0–0, 2019.\n[Liu et al., 2020] Yaojie Liu, Joel Stehouwer, and Xiaoming\nLiu. On disentangling spoof trace for generic face anti-\nspooﬁng. In ECCV, pages 406–422, 2020.\n[Liu et al., 2021a] Ajian Liu, Xuan Li, Jun Wan, Yanyan\nLiang, Sergio Escalera, Hugo Jair Escalante, Meysam\nMadadi, Yi Jin, Zhuoyuan Wu, Xiaogang Yu, et al. Cross-\nethnicity face anti-spooﬁng recognition challenge: A re-\nview. IET Biometrics, 10(1):24–43, 2021.\n[Liu et al., 2021b] Ajian Liu, Zichang Tan, Jun Wan, Ser-\ngio Escalera, Guodong Guo, and Stan Z Li. Casia-surf\ncefa: A benchmark for multi-modal cross-ethnicity face\nanti-spooﬁng. In WACV, pages 1179–1187, 2021.\n[Liu et al., 2021c] Ajian Liu, Zichang Tan, Jun Wan, Yanyan\nLiang, Zhen Lei, Guodong Guo, and Stan Z Li. Face anti-\nspooﬁng via adversarial cross-modality translation. IEEE\nTIFS, 16:2759–2772, 2021.\n[Liu et al., 2021d] Ajian Liu, Chenxu Zhao, Zitong Yu,\nAnyang Su, Xing Liu, Zijian Kong, Jun Wan, Sergio Es-\ncalera, Hugo Jair Escalante, Zhen Lei, et al. 3d high-\nﬁdelity mask face presentation attack detection challenge.\nIn ICCVW, pages 814–823, 2021.\n[Liu et al., 2021e] Ajian Liu, Chenxu Zhao, Zitong Yu, Jun\nWan, Anyang Su, Xing Liu, Zichang Tan, Sergio Escalera,\nJunliang Xing, Yanyan Liang, et al. Contrastive context-\naware learning for 3d high-ﬁdelity mask face presentation\nattack detection. arXiv preprint arXiv:2104.06148, 2021.\n[Liu et al., 2021f] Ze Liu, Yutong Lin, Yue Cao, Han Hu,\nYixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using\nshifted windows. In CVPR, pages 10012–10022, 2021.\n[Parkin and Grinchuk, 2019] Aleksandr Parkin and Oleg\nGrinchuk. Recognizing multi-modal face spooﬁng with\nface recognition networks. In CVPRW, pages 0–0, 2019.\n[Shao et al., 2019] Rui Shao, Xiangyuan Lan, Jiawei Li, and\nPong C Yuen. Multi-adversarial discriminative deep do-\nmain generalization for face presentation attack detection.\nIn CVPR, pages 10023–10031, 2019.\n[Touvron et al., 2021] Hugo Touvron, Matthieu Cord,\nMatthijs Douze, Francisco Massa, Alexandre Sablay-\nrolles, and Herv ´e J ´egou. Training data-efﬁcient image\ntransformers & distillation through attention. In ICML,\npages 10347–10357, 2021.\n[Wang et al., 2021] Pichao Wang, Xue Wang, Fan Wang,\nMing Lin, Shuning Chang, Wen Xie, Hao Li, and Rong\nJin. Kvt: k-nn attention for boosting vision transformers.\narXiv preprint arXiv:2106.00515, 2021.\n[Xu et al.,] Tongkun Xu, Weihua Chen, Pichao Wang, Fan\nWang, Hao Li, and Rong Jin. Cdtrans: Cross-domain\ntransformer for unsupervised domain adaptation. arXiv\npreprint arXiv:2109.06165.\n[Yu et al., 2020] Zitong Yu, Chenxu Zhao, Zezheng Wang,\nYunxiao Qin, Zhuo Su, Xiaobai Li, Feng Zhou, and Guoy-\ning Zhao. Searching central difference convolutional net-\nworks for face anti-spooﬁng. In CVPR, pages 5295–5305,\n2020.\n[Zhang et al., 2019] Shifeng Zhang, Xiaobo Wang, Ajian\nLiu, Chenxu Zhao, Jun Wan, Sergio Escalera, Hailin Shi,\nZezheng Wang, and Stan Z Li. A dataset and benchmark\nfor large-scale multi-modal face anti-spooﬁng. In CVPR,\npages 919–928, 2019.\n[Zhang et al., 2020a] Ke-Yue Zhang, Taiping Yao, Jian\nZhang, Ying Tai, Shouhong Ding, Jilin Li, Feiyue Huang,\nHaichuan Song, and Lizhuang Ma. Face anti-spooﬁng\nvia disentangled representation learning. In ECCV, pages\n641–657, 2020.\n[Zhang et al., 2020b] Shifeng Zhang, Ajian Liu, Jun Wan,\nYanyan Liang, Guodong Guo, Sergio Escalera, Hugo Jair\nEscalante, and Stan Z Li. Casia-surf: A large-scale multi-\nmodal benchmark for face anti-spooﬁng. IEEE TBIOM,\n2(2):182–193, 2020.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n1186",
  "topic": "Modal",
  "concepts": [
    {
      "name": "Modal",
      "score": 0.8049737215042114
    },
    {
      "name": "Modality (human–computer interaction)",
      "score": 0.7321338653564453
    },
    {
      "name": "Computer science",
      "score": 0.6799873113632202
    },
    {
      "name": "Liveness",
      "score": 0.6586909294128418
    },
    {
      "name": "Modalities",
      "score": 0.6308437585830688
    },
    {
      "name": "Transformer",
      "score": 0.5141392350196838
    },
    {
      "name": "Modal logic",
      "score": 0.49985527992248535
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4465131163597107
    },
    {
      "name": "Machine learning",
      "score": 0.38347578048706055
    },
    {
      "name": "Theoretical computer science",
      "score": 0.1777385175228119
    },
    {
      "name": "Engineering",
      "score": 0.17408230900764465
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    }
  ]
}