{
  "title": "KoSBI: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Applications",
  "url": "https://openalex.org/W4385564953",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2223132880",
      "name": "Hwaran Lee",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2130365379",
      "name": "Seok-Hee Hong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2112029105",
      "name": "Joon-Suk Park",
      "affiliations": [
        "University of Richmond",
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2230852849",
      "name": "Takyoung Kim",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2100966015",
      "name": "Gunhee Kim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2232041247",
      "name": "Jung-Woo Ha",
      "affiliations": [
        "Naver (South Korea)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3196731672",
    "https://openalex.org/W4296413526",
    "https://openalex.org/W3148330722",
    "https://openalex.org/W4287116904",
    "https://openalex.org/W3104041537",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4385573759",
    "https://openalex.org/W4385723172",
    "https://openalex.org/W4307333023",
    "https://openalex.org/W4320932748",
    "https://openalex.org/W4285210452",
    "https://openalex.org/W3105882417",
    "https://openalex.org/W2963526187",
    "https://openalex.org/W3162296828",
    "https://openalex.org/W4385573018",
    "https://openalex.org/W2963457723",
    "https://openalex.org/W2946681640",
    "https://openalex.org/W78136081",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W3120860016",
    "https://openalex.org/W3105042180",
    "https://openalex.org/W3103649165",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W4385574401",
    "https://openalex.org/W3034937117",
    "https://openalex.org/W3046087842",
    "https://openalex.org/W3175487198",
    "https://openalex.org/W4385894687",
    "https://openalex.org/W3047185145",
    "https://openalex.org/W4287854809",
    "https://openalex.org/W4229506649",
    "https://openalex.org/W2971307358",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3207604419",
    "https://openalex.org/W4285220056",
    "https://openalex.org/W4311991106",
    "https://openalex.org/W4224780024",
    "https://openalex.org/W3207316473",
    "https://openalex.org/W3172314079",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W4385574051",
    "https://openalex.org/W3171850892",
    "https://openalex.org/W2976597954",
    "https://openalex.org/W4385574293",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W3052814785"
  ],
  "abstract": "Large language models (LLMs) not only learn natural text generation abilities but also social biases against different demographic groups from real-world data. This poses a critical risk when deploying LLM-based applications. Existing research and resources are not readily applicable in South Korea due to the differences in language and culture, both of which significantly affect the biases and targeted demographic groups. This limitation requires localized social bias datasets to ensure the safe and effective deployment of LLMs. To this end, we present KosBi, a new social bias dataset of 34k pairs of contexts and sentences in Korean covering 72 demographic groups in 15 categories. We find that through filtering-based moderation, social biases in generated content can be reduced by 16.47%p on average for HyperClova (30B and 82B), and GPT-3.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 5: Industry Track, pages 208–224\nJuly 10-12, 2023 ©2023 Association for Computational Linguistics\nKOSBI: A Dataset for Mitigating Social Bias Risks Towards Safer Large\nLanguage Model Applications\nHwaran Lee1,2,⋆ Seokhee Hong3,⋆,♯ Joonsuk Park1,2,4\nTakyoung Kim1,♯ Gunhee Kim3 Jung-Woo Ha1,2\n1NA VER AI Lab 2NA VER Cloud 3Seoul National University 4University of Richmond\n{hwaran.lee, jungwoo.ha}@navercorp.com park@joonsuk.org\nseokhee.hong@vision.snu.ac.kr gunhee@snu.ac.kr youngerous@gmail.com\nAbstract\nLarge language models (LLMs) learn not only\nnatural text generation abilities but also social\nbiases against different demographic groups\nfrom real-world data. This poses a critical risk\nwhen deploying LLM-based applications. Ex-\nisting research and resources are not readily\napplicable in South Korea due to the differ-\nences in language and culture, both of which\nsignificantly affect the biases and targeted de-\nmographic groups. This limitation requires lo-\ncalized social bias datasets to ensure the safe\nand effective deployment of LLMs. To this end,\nwe present KOSBI, a new social bias dataset\nof 34k pairs of contexts and sentences in Ko-\nrean covering 72 demographic groups in 15\ncategories. We find that through filtering-based\nmoderation, social biases in generated content\ncan be reduced by 16.47%p on average for\nHyperCLOV A (30B and 82B), and GPT-3.\n1 Introduction\nLarge language models (LLMs) acquire impressive\ntext generation abilities from large-scale real-world\npre-training data (Brown et al., 2020; Kim et al.,\n2021). However, LLMs also absorb toxicity, such\nas social biases (Sheng et al., 2019; Wallace et al.,\n2019a). This cannot be overlooked since the risk\nof generating toxic content impedes the safe use\nand potential commercialization of various down-\nstream applications, such as AI assistants (Dinan\net al., 2022; Bai et al., 2022a). To minimize the\nharm, numerous studies have tackled the detection\nand mitigation of toxicity in LLMs (Blodgett et al.,\n2020; Ganguli et al., 2022). Each study typically\nleverages datasets capturing a specific type of toxi-\ncity, such as social bias (Sap et al., 2020; Nangia\n⋆ Authors equally contributed.\n♯ This work was done during their internship at NA VER\nAI Lab.\nEmail to: {hwaran.lee, jungwoo.ha}@navercorp.com,\nseokhee.hong@vision.snu.ac.kr\net al., 2020) or hate speech (Warner and Hirschberg,\n2012; Lee et al., 2022).\nThese datasets are not only task-specific but also\nlanguage- and culture-specific. For instance, con-\nsider hate speech made in South Korea and in the\nUnited States. In addition to the language, the\nmainly targeted demographic groups also differ—\nfeminists and Korean Chinese in South Korea, as\nopposed to African Americans and Jewish in the\nUnited States (Jeong et al., 2022). Also, the ex-\nisting toxicity datasets in Korean mostly focus on\nexplicit hate speech and consider a limited number\nof targeted demographic groups (Moon et al., 2020;\nYang et al., 2022; Kang et al., 2022; Lee et al.,\n2022). This calls for a dataset to address social\nbiases against a more comprehensive set of demo-\ngraphic groups in South Korea so that as many\ngroups and people are protected.\nHere we present the Korean Social Bias (KOSBI)\ndataset, a large-scale dataset of 34k pairs of con-\ntexts and sentences in Korean with labels mainly\ncapturing the presence of social biases. 1 It cov-\ners 72 targeted demographic groups in 15 cate-\ngories, 2 which is much more comprehensive than\nexisting datasets, as shown in Table 2. The cat-\negories include not only the common ones like\ngender and religion but also those especially rele-\nvant to South Korea—e.g., marital status and do-\nmestic area of origin, both of which consist of\ndemographic groups that suffer from social bi-\nases in the country more commonly than others\ndo. Given the difficulty of crawling from the web\nsufficient data for each of the 72 demographic\ngroups, we leveraged HyperCLOV A(Kim et al.,\n2021) to generate the data with in-context few-\n1 The KOSBI dataset is released with English-translated\nannotations for those who are not fluent in Korean at https:\n//github.com/naver-ai/korean-safety-benchmarks\n2The categories and demographic groups were selected\nbased on the Universal Declaration of Human Rights (UDHR)\nand the National Human Rights Commission of Korea\n(NHRCK).\n208\nDataset # Inst. Demographic GroupsData Source IncludesToxicity Labels# Cat. # Groups Context?\nBEEP! (Moon et al., 2020) 9,341 - - News comments ✗ Hate speech, BiasAPEACH (Yang et al., 2022) 3,770 10 - Human-written ✗ OffensiveKOLD (Jeong et al., 2022) 40,448 5 19 News, YouTube comments ✗(Title)OffensiveHateScore, Unsmile (Kang et al., 2022) 31,195 7(mixed) News, online community comments✗ Hate speech, ProfanityK-MHaS (Lee et al., 2022) 109,692 7 - News comments ✗ Hate speech, Profanity\nKOSBI(Ours) 34,214 15 72 LM-generated ✓ Biased (Stereotypes, Prejudice,Discrimination), Other\nTable 1: Comparison of Toxicity Datasets in Korean.\nshot learning (Gao et al., 2021; Mishra et al.,\n2022). More specifically, we generated sentences\nand their respective contexts—which are also sen-\ntences, grammatically—for given target demo-\ngraphic groups. The generated contexts and sen-\ntences were then annotated by crowd workers as\nsafe or unsafe. Here, unsafe contexts and sentences\nwere further labeled as expressions of stereotypes\n(cognitive bias), prejudice (emotional bias), dis-\ncrimination (behavioral bias), and/or other, adopt-\ning the taxonomy by Fiske (2023),3 in Figure 1.\nWith KOSBI, we mitigate social biases in LLM-\ngenerated content using a filtering-based modera-\ntion approach, also known as rejection sampling\n(Ganguli et al., 2022). To do this, we first trained\na safe sentence classifier using KOSBI. Then,\nfor a given context, each LLM was used to gen-\nerate a pool of sentences from which the safest\nsentence was chosen by the classifier. The hu-\nman evaluation shows that social biases in gen-\nerated content are reduced by 16.47% on average\nfor all three models tested—HyperCLOV A(82B),\nHyperCLOV A (30B), and GPT-3.\n2 Related Works\nBias Mitigation in LLM-generated Content.\nLLMs are trained on real-world data, which often\ncontains social biases toward certain demographic\ngroups. This, in turn, induces biases in LLMs (Xu\net al., 2021a). To date, various resources have\nbeen published to measure and mitigate such bi-\nases in LLMs (Sap et al., 2020; Nangia et al., 2020;\nNadeem et al., 2021). Some of them are associ-\nated with specific tasks: coreference resolution to\nfight the phenomena like associating certain pro-\nfessions with a particular gender (Rudinger et al.,\n2018; Zhao et al., 2018), and question answering\nto prevent answers stereotyped toward certain bias\ncategories like gender or socio-economic status (Li\net al., 2020; Parrish et al., 2022). These resources\n3For labeling the context, prejudice and discrimination\nwere combined due to the limited number of instances.\nare not as effective for HyperCLOV Aand other\nLLMs pre-trained on Korean corpora. Thus, we\npresent a new resource in Korean, capturing the bi-\nases against prevalent demographic groups in South\nKorea. Also, our dataset covers a much more com-\nprehensive set of demographic groups.\nHate Speech Detection. Röttger et al. (2021) de-\nfines hate speech as “abuse that is targeted at a\nprotected group or at its members for being a part\nof that group.” Resources created to help detect\nhate speech can be used to reduce hate speech\ngenerated by LLMs, thereby reducing the harm\nthey can incur. Note these resources use vari-\nous names interchangeably for the most part, e.g.,\nhate speech (Warner and Hirschberg, 2012), abu-\nsive language (Wiegand et al., 2019), and toxic\nlanguage (Gehman et al., 2020; Hartvigsen et al.,\n2022). Also, quite a few resources are for safer\ndialogue (Sun et al., 2022; Xu et al., 2021b; Xenos\net al., 2021; Kim et al., 2022). Meanwhile, to re-\nflect different languages and societies, researchers\nhave created and proposed hate speech corpora in\nChinese (Deng et al., 2022), Dutch (Demus et al.,\n2022), and Arabic (Mubarak et al., 2022) Similar\nto the resources capturing social biases, these re-\nsources are not as useful for Korean LLMs due to\nthe differences in language and culture. Luckily,\nseveral resources in Korean exist, as summarized\nin Table 1. However, these resources either unspec-\nify or cover only a small subset of demographic\ngroups in South Korea. More importantly, they\nfocus on explicit profanity and otherwise offensive\nexpressions. Our dataset instead targets cases that\ncannot be identified with specific keywords, such\nas expressions of stereotypes, discrimination, and\nprejudice (without explicit profanity) toward 72\ndemographic groups.\nSafety Alignment of Language Models. Be-\nyond social biases and hate speech, various cat-\negories have been proposed recently to enhance\nthe safety of language models, such as human val-\n209\nues (Solaiman and Dennison, 2021; Kenton et al.,\n2021), ethical judgements (Hendrycks et al., 2021;\nLourie et al., 2021), and moral norms (Forbes et al.,\n2020; Emelin et al., 2021). Then, alignment learn-\ning methods through human feedback (Bai et al.,\n2022a) or even by AI feedback (Bai et al., 2022b)\nhave been proposed. Moreover, red-teaming (Perez\net al., 2022; Ganguli et al., 2022) and adversarial\nattack (Wallace et al., 2019b) approaches have also\nbeen suggested to identify vulnerabilities in lan-\nguage models in terms of safety. We expect our\ndataset and comprehensive categories will be help-\nful for the safety alignment of Korean society.\n3 The K OSBI Dataset\nThis study aims to address social biases against a\ncomprehensive set of demographic groups in South\nKorea so as to make LLMs safer for as many groups\nand people as possible. (Here, we focus on so-\ncial biases without explicit hate speech, as exist-\ning datasets address the latter.) To achieve this,\nwe wanted KOSBI to consist of context-sentence\npairs labeled as safe or unsafe for the demographic\ngroups mentioned in them; this way, we can train\nLLMs to behave safely in the context of discussing\na demographic group, rather than simply avoid it.\n3.1 Demographic Groups Compilation\nWith the goal of covering a comprehensive list of\ndemographic groups, we first compiled the list\nby combining categories derived from the Uni-\nversal Declaration of Human Rights (UDHR) and\nthe National Human Rights Commission of Korea\n(NHRCK)4, which prohibit discriminatory treat-\nment based on social identity. (See Table 4 for the\nlist of categories.) Then, we defined social groups\nin each category, considering the unique character-\nistics of Korean culture. For instance, we consider\nthe most widely practiced religions in Korea, and\nalso progressive and conservative political parties,\nrather than the Democratic and Republican parties\nin the U.S. (See Table 8 for the list of demographic\ngroups.)\n3.2 Raw Data Construction\nSince crawling from the web sufficient context-\nsentence pairs for every demographic group\nwould be challenging, we generated them using\n4Specifically, refer to provisions related to discriminatory\nacts in violation of equal rights – Article 2 Subparagraph 3 of\nthe National Human Rights Commission Act, and Article 3\nParagraph 1 Subparagraph 1 of the Anti-Discrimination Act.\nCategories # Groups\nGender identity† 3\nSexual orientation† 1\nAge & Generation† 12\nRace, Ethnicity, Nationality† 11\nReligion† 6\nDisability status† 1\nPhysical appearance† 4\nPolitical orientation† 3\nSocio-economic status† 3\nDomestic area of origin 8\nMarital status 6\nPregnancy & Birth 4\nFamily form 5\nCriminal record 2\nEducation, University, Major 3\nTotal 72\nTable 2: Category and demographic groups considered\nin KOSBI. † marks categories in both UDHR and\nNHRCK. Entire social groups are listed in Table 8.\nHyperCLOV A. LLMs are reported to have abilities\nto learn a given task from instructions and few-shot\ndemonstration samples, which is referred to as in-\ncontext learning (Brown et al., 2020). With these\nabilities, previous research has proposed data syn-\nthesis methods by demonstration-based prompting\nmethods (Gao et al., 2021; Mishra et al., 2022),\nwherein several sample sentences are listed in a\nprompt, and an LLM generates different ones with\nsimilar semantics. To construct KOSBI, we applied\nthe demonstration-based prompting and generated\npairs of context and sentence given a target social\ngroup using HyperCLOV A.\nThe raw data construction was done in three-step:\n(1) building demonstration pools, which consist of\ninitial labeled data; (2) generating contexts and sen-\ntences; (3) filtering out inappropriate generations\nby trainable classifiers The initial demonstration\ndata was manually curated by authors and a few\nannotators, resulting in a relatively small pool of\naround 2165 samples. This could limit the diver-\nsity of generation results and the accuracy of the\nfilter models. To address this limitation, we incre-\nmentally generated the data by repeating steps 1-3\nto update demonstration pools and re-trained the\nfiltering classifiers after each iteration.\nThe detailed prompts can be found in Ap-\npendix C. In the context prompt, the LLM is asked\nto produce “neutral contextual sentences” pertain-\n5In the initial demonstration pool, we collected three safe\nand three unsafe context-sentence pairs for each demographic\ngroup. The initial demonstration samples and all labeled gen-\neration data will be published.\n210\nFigure 1: Example pairs of a context and a sentence\nwith labels pertaining to a given social demographic\ncategory and group.\ning to the given social group. However, the model\noften generated biased sentences due to intrinsic\nbias. We labeled them as unsafe contexts. In the\nsentence generation case, we separated unsafe and\nsafe demonstration pools and instructions for class-\nconditional sentence generation.\nAt the context filtering step, the filter model\nclassified generated sentences pertaining the target\ndemographics, and annotators only labeled well-\nconditioned outputs. In the sentence filtering step,\non the other hand, we first over-generated sentences\nfor each context, i.e., three sentences for each class.\nWe then selected the most ambiguous sentence for a\nsafe sentence classifier to label. The ambiguity was\nmeasured by the estimated max variability (Liu\net al., 2022; Swayamdipta et al., 2020). Conse-\nquently, by excluding obvious and easy-to-learn\nsamples in the dataset, this filtering process served\nto ensure that the constructed dataset has an appro-\npriate level of difficulty.\n3.3 Annotation\nThe contexts and sentences were then labeled by\ncrowd workers according to the following guide-\nlines (See Figure 1 for examples):\n• Context. The role of the context is to rep-\nresent a scenario in which an LLM needs to\nspeak about a demographic group. Each gen-\nerated context is first annotated as safe if it\nonly contains objective information and thus\ndoes not cause harm to the targeted demo-\ngraphic group, and unsafe, otherwise. If la-\nbeled unsafe, it is further labeled as an ex-\npression of 1) stereotypes (cognitive bias), 2)\nprejudice (emotional bias), 3) discrimination\n(behavioral bias), and/or 4) other, adopting\nthe taxonomy by Fiske (2023). Here, sub-\nclasses 2 and 3 are combined due to the rare\nContext Sentence Train Valid Test All\nSafe\nSafe 11,630 1,427 1,382 14,439\nUnsafe 8,521 1,060 1,092 10,673\nTotal 20,151 2,487 2,474 25,112\nUnsafe\nSafe 2,537 320 317 3,174\nUnsafe 4,589 596 617 5,802\nTotal 7,126 916 934 8,976\nUndecided\nSafe 58 45 7 6\nUnsafe 68 48 11 9\nTotal 93 18 15 126\nTotal 27,370 3,421 3,423 34,214\nTable 3: The number of instances for all label combina-\ntions in KOSBI. (Refer to Table 7 for subclass.)\noccurrences observed during a pilot study.\n• Sentence. Each sentence generated for a\ngiven context is first annotated as safe or un-\nsafe, depending on whether or not it harms\nthe targeted demographic group. If labeled\nunsafe, the sentence is further labeled as an\nexpression of one of the bias types or other,\nsame as above, except subclasses 2 and 3 are\nnot combined this time. Note, a seemingly\nsafe sentence may be unsafe dependent on\nits context. For instance, a sentence simply\nagreeing (e.g., “Yes, that is true.”) to an un-\nsafe context (e.g., “[Demographic Group] are\nalways lazy.”) isunsafe. In such cases, it is ad-\nditionally marked as (implicit), and (explicit)\nif the sentence is unsafe itself.\nTo label the filtered outputs, 200 crowd work-\ners affiliated across a wide range of social demo-\ngraphics were hired (Table 12). The detailed well-\nbeing information of workers can be found in Ap-\npendix C. They evaluated the qualities of contexts\nand sentences in terms of understandability and\ncoherences between the pairs. Data that did not\nmeet the criteria were excluded. They were then\nasked to label them. In particular, in the case of\nunsafe sentences, they were requested to find the\nsocial groups targeted in the context-sentence pair\nfor explainability. The annotation guidelines are\nshown in Appendix H.\nIn the human evaluation step, three crowd work-\ners annotated contexts and sentences, and the fi-\nnal labels were decided by a majority vote. First,\nin labeling contexts as safe or unsafe, the inner-\nannotator agreement by Krippendorff’s αis 0.459\nfor binary (safe/unsafe) classes. The agreement is\n211\nDatasets Models Macro F1 (%)\nBEEP! KcBERT 52.90\nAPEACH KcBERT 48.82\nKOLD KLUE-BERT 38.15\nHatescore KcBERT 40.28\nUnsmile KcBERT 48.02\nOurs KLUE-BERT 69.94\nOurs KcELECTRa 71.21\nTable 4: Comparison of classification performance on\nour test set. Fine-tuned models on the previous datasets\nand ours are compared.\nlower if we consider subclasses of unsafe contexts\n(α = 0.359). For sentence annotations, the α is\n0.256 for labeling them as safe or unsafe. This sug-\ngests that determining the labels for the sentences is\nharder. This is expected given that both the context\nand the sentence need to be considered for labeling\na sentence, whereas contexts are self-contained.\n3.4 The Resulting Dataset\nKOSBI consists of 34,214 context-sentence pairs as\nsummarized in Table 3. There are 25,112 (73.4%)\nand 8,976 (26.2%) of safe and unsafe contexts, re-\nspectively. Also, there are 17,619 (51.5%) and\n16,484 (48.2%) safe and unsafe sentences. Train-\ning, validation, and test sets are randomly separated\nas 80%, 10%, and 10%, respectively, considering\nthe balance of social group distribution.\n4 Experimental Results\nTo improve the safety of LLMs towards social\ngroups, we explore a simple filtering-based mod-\neration approach. In this section, we first build\na safe sentence classification. Then we automati-\ncally evaluate LLMs’ generation given a context\nwith the safety classifier. Finally, we sample the\nsafest sentence among over-generated sentence can-\ndidates. The efficacy of the filtering approach is\ndemonstrated by human evaluation.\n4.1 Safe Sentence Classification\nWe train the safe sentence classifier by fine-\ntuning KLUE-BERT (Park et al., 2021) and\nKcELECTRa (Lee, 2021)6. To identify unsafe sen-\ntences in context, the context and the sentence are\nconcatenated and then fed into the models. We\nalso simply augment data by using context data\nand their labels, resulting in the best macro-F1 of\n71.21% as shown in Table 4. The performance\nimplies that the proposed dataset is challenging.\n6We used the latest version of the model: https://\nhuggingface.co/beomi/KcELECTRA-base-v2022.\nTo validate the novelty of our dataset, we\nemployed classifiers trained on previous Korean\nhate speech corpus: BEEP! (Moon et al., 2020),\nAPEACH (Yang et al., 2022), KOLD (Jeong et al.,\n2022), Hatescore, and Unsmile (Kang et al., 2022)\nto evaluate the test set of our KOSBI7. As a result,\nin Table 4, all classifiers consistently show poor\naccuracies, approximating a random guess. These\npoor performances strongly suggest that our dataset\nis dissimilar from existing corpora, mainly address-\ning Korean hate speech. Such difference may stem\nfrom the fact that our dataset includes instances of\ncontext dependent and implicit hate speech, which\ncontrasts with the overt and explicit instances found\nin the previous corpus sourced from online plat-\nform comments.\n4.2 Safety Evaluation of LLMs\nUsing the safe sentence classifier, we measure safe\ngenerations of LLMs. Similarly to Gehman et al.\n(2020), we evaluate the models with two metrics.\nFirst, the empirical probability of generating a safe\nsentence (safe score is ≥0.5) given a context at\nleast once over multiplekgenerations. We measure\nthe probability with different sizes ofk. The higher\nthe safety probability, the more frequently filter-\nmoderated outputs are safe. The second metric is\nthe expected average safety score of safe sentences\nover k= 8generations. This means that the higher\nthe expected average safety, the model likely gen-\nerates more safe sentences on average.\nWe evaluateHyperCLOV Awith different model\nsizes (6.9B, 13B, 30B, and 82B), and GPT-3 (175B)\n8. We sample a subset of the test set to contain 30\ncontexts per each demographical category, i.e., a\ntotal of 450 contexts. The LLMs generate sentences\ngiven the contexts in a zero-shot generation setup.\nThe prompt used for this experiment is listed in\nAppendix C.\nTable 5 presents the evaluation results. First, the\nempirical probability of generating safe sentences\n7For a fair comparison, we employed the published BERT-\nbase-sized checkpoints of each model. Classifiers except for\nKOLD are pretrained on KcBERT (Lee, 2020). For KOLD,\nwe manually fine-tuned KOLD dataset on KLUE-BERT by\nfollowing the paper’s experiment setup because there are no\npublicly shared checkpoints nor train/valid/test split.\n8The largest HyperCLOV Amodel (82B) was trained on\nHyperCLOV ACorpus consisting of 300B tokens, and the re-\nmains are further trained with 30B of a spoken dataset. The\nversion of ’text-davinci-003’ is used as the GPT-3 model. Note\nalso that HyperCLOV Amodels are not trained by instruct-\ntuning or reinforcement learning from human feedback, like-\nwise ’text-davinci-003’.\n212\nModel Safety ProbabilityExp. Avg. Safetyk=1 2 4 8\nGPT-3 (175B) .809 .902 .956 .969 .625 ±.083\nHyperClova (6.9B) .673 .796 .796 .876 .589±.102\nHyperClova (13B) .713 .789 .789 .862 .581±.096\nHyperClova (30B) .711 .844 .844 .900 .588±.105\nHyperClova (82B) .647 .813 .813 .887 .575±.100\nTable 5: Safety evaluations of LLM’s continuations after\ngiven contexts. Left: The empirical probability of gen-\nerating safe sentence at lease once over kgenerations.\nRight: Expected average safety score of safe sentences\nwith standard deviations over 8 generations.\nFigure 2: Human evaluation on the subset of the test\nset. We compared two HyperCLOV Amodels (82B and\n30B) and the GPT-3 (175B; text-davinci-003) models,\nfor both with and without filtering.\nincreases as generation increases for all LLMs. In\nother words, when the HyperCLOV A-82B gener-\nates 8 sentences per context, 88.7% of continua-\ntions are safe w.r.t the classifier model. Notably, the\nmore over-generations, the more improved safety.\nNext, for the expected average of safety score, we\ncould not find distinguished differences among\ndifferent sizes of HyperCLOV A. Overall, GPT-3\nshows more improved safety probability and score\nthan HyperCLOV A by the automatic evaluations.\nFurthermore, we divide the results into those\ngenerated from a safe context and an unsafe context\nin order to measure how the safety of the context\naffects the model’s continuation. As can be seen\nby comparing both results presented in Table 9,\nmodels generate more unsafe sentences when an\nunsafe context was given, while all models generate\n99% of safe continuations when conditioned on a\nsafe context in k= 8settings.\n4.3 Filter-based Moderation\nWe demonstrate the efficacy of filter-based mod-\neration of unsafe sentence generation. The filter-\ning approach samples the safest sentence among\n8 generations. We conduct a human-evaluation\nexperiment to assess the quality and safety of gen-\neration results. The evaluation results of the three\nmodels — GPT-3,HyperCLOV A30B, and 82B are\ncompared in Figure 2 and Table 6.\nWith the filtering process, we find that the ra-\nFigure 3: Moderation results on each category in the\naugmented test set. Left: Safe response ratio from\nhuman evaluation results. Right: Safe sentence classifi-\ncation performance of the best classifier (KcELECTRa).\nThe vertical lines represent the averages of safe response\nand accuracy for all categories. Categories are ordered\nby descend of the classifier’s accuracy.\ntio of unsafe generations decreases for all mod-\nels by 16.47%p on average. We observe that the\nfilter-based moderation remarkably improves the\nsafety of all LLMs by reducing unsafe generation\nas 16%, 15%, and 18.5% and by increasing safe\nsentences as 15.6%, 15.3%, and 18.7% for GPT-\n3, 82B-HyperCLOV A, and 30B-HyperCLOV A, re-\nspectively. It is interesting that the ratio of the\nambiguous sentences generated by GPT-3 does not\ndecrease despite the filtering.\nTable 6 presents qualitative results of sentences\ngenerated by each model and the effects of the filter-\nbased moderation. Inconsistent with the results\nin Figure 2, the filter-based moderation does not\nimprove the quality of generated sentences. This\nmeans the filtering is likely to slightly sacrifice the\ncoherency of generation by playing the role of con-\nstraints as a side effect against enhancing safety.\nHowever, overall quality scores of all LLMs are\ncompetitive enough, and HyperCLOV Apresents\nbetter qualitative performance than GPT-3, consis-\ntent with the results in Figure 2.\n4.4 Social Bias Mitigation Level by Category\nWe analyze the moderation results by the 15 de-\nmographic categories. Before getting a result, we\naugmented the test set with additional annotated\ndata to increase the number of samples per category\nand the reliability of the test results. As a result,\nour augmented test set consists of 6,801 (context,\n213\nQuality Assessments\nGrammatical\nError-Free (%)\nUnderstandability\n(%)\nPertaning to Target\nSocial Group (%)\nContext (%)\nCoherency Overall (%)\nGPT-3 (175B) 89.8 80.2 90.0 71.6 32.0\nGPT-3 (175B) + filtering 89.3 80.9 87.3 69.1 31.6\nHyperCLOV A (80B) 99.1 97.1 93.6 89.6 49.3\nHyperCLOV A (80B) + filtering 99.6 96.2 93.3 88.9 54.0\nHyperCLOV A (30B) 99.3 98.2 95.8 93.8 61.6\nHyperCLOV A (30B) + filtering 100 97.3 94.7 91.6 56.9\nTable 6: Human evaluation on the subset of test set. Comparisons between unfiltered responses and filtered responses\namong 8 generations from GPT-3 (175B; ‘text-davinci-003’), HyperClova (82B and 30B). Overall score denotes the\npercentage of instances that are marked as passed all quality assessment questions by all evaluators.\nsentence) pairs (see Table 10 for detailed statistics\nfor it). For experiments conducted in this section,\nwe sample a small subset from the augmented test\nset to contains at least 48 contexts per category, re-\nsulting in 1,746 contexts. All other settings follow\nof them in Sec 4.3.\nFigure 3 presents the human evaluation results\nof filter-based moderation by each demographic\ncategory. Each category displays a different ratio\nof generated safe sentences. By comparing with\nand without filter-based moderation, we can no-\ntice that the efficacy of the filtering process also\nvaries. For example, we find the biggest increase\nof safe generations ratio in Disability status cate-\ngory (+64.0%) while the smallest in Marital status\n(+0.85%). Within the category, the differences also\nexist between models; such as in Disability sta-\ntus category, HyperCLOV A-82B got an increase\nof 33.3%p but HyperCLOV A-30B got only 4.1%p\n(See Figure 6 for the results by the group for all\nthree models).\nSince filter-based moderation utilizes a filter\nmodel, it it natural to assume that there could ap-\npear to be a correlation between the performance\nof the filter model and the moderation efficacy. To\nidentify any tendencies between the two, we have\nalso included the accuracy of the filter model in\nFigure 3. We, however, couldn’t find a strong cor-\nrelation between them. We conjecture the reason is\nthe relatively small differences in accuracy across\nthe categories or the sampled set used here not be-\ning large enough. Further analysis is expected in\nfuture work. Despite this, the filter-based moder-\nation approach demonstrates the effectiveness for\nall social demographic categories. It is crucial to\nscrutinize and improve the models’ safety for fair\nconsideration of each demographic category and\ngroup.\n5 Conclusion\nTo alleviate unsafe social bias of LLMs, we pro-\npose a large-scale social bias dataset related to\nsafety addressing the Korean language and cultures,\nKOSBI. Our dataset covers 72 demographic groups\nin 15 categories, consisting of 34k of situation con-\ntext and following sentence pairs. To construct\nKOSBI, we employ a human-LLM collaboration\nframework, where HyperCLOV A generates con-\ntexts and sentences, and human annotators label\nthem as safe or unsafe. Extensive experiments\npresent our dataset as differentiated from existing\nprevalent datasets on social bias and hate speech.\nMoreover, the results show the filter model trained\nwith our dataset remarkably improves the ratio of\ngenerating safe sentences for various LLMs such\nas GPT-3 and HyperCLOV Awith diverse model\nsizes, which presents the efficacy of our dataset.\nLimitations\nThe proposed KOSBI addresses social bias based\non Korean culture with the Korean language. This\nKorean-specific property might restrict the effec-\ntiveness of our dataset in Korea and its similar cul-\ntures. However, our dataset construction and eval-\nuation protocol can contribute to a helpful guide\nfor other research groups on AI safety to build the\ndatasets for their cultures and languages.\nThe performance of the filter models for harm-\nless sentence classification in this study is not very\ncompetitive. We leave it as a future research topic\nto make a filter classifier with higher accuracy on\nour dataset because the goal of this study is not to\nmake a strong social bias filter itself.\n214\nEthics Statement\nWe expect that our KOSBI can considerably con-\ntribute to enhancing the safe usage of LLMs’ ap-\nplications by reducing risks caused by social bias.\nConstructing datasets on harmfulness is likely to\ncause stress on the contributors, such as human ex-\nperts and crowd workers. To minimize their stress\nexposure, we use HyperCLOV A to generate con-\ntexts and sentences and ask humans to label them.\nFurthermore, our study was approved by the pub-\nlic institutional review board (IRB) affiliated with\nthe Ministry of Health and Welfare of South Korea\n(P01-202211-01-016).\nAcknowledgements\nThe authors would like to thank all committee mem-\nbers of the AI Ethics Forum for Human at NA VER,\nincluding Meeyoung Cha, Byoungpil Kim, Eun-Ju\nLee, Yong Lim, Alice Oh, Sangchul Park, Woochul\nPark, Joonha Jeon, Jonghyun Kim, Do Hyun Park,\nand Eunjung Cho, for their constructive feedback\nand helpful discussions. We are also grateful to\nRyumin Song, Jaehyeon Kim, and Jisun Kim at\nCrowdworks, who cooperated in the data collec-\ntion process, and the 200 crowdworkers who partic-\nipated in the process. In addition, the authors thank\nthe research members of SNU-NA VER Hyperscale\nAI Center and KAIST-NA VER Hypercreative AI\nCenter for discussion and thank Haksoo Ko and\nYejin Choi for valuable discussion. This project is\nfinancially supported by NA VER Cloud.\nReferences\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan,\nNicholas Joseph, Saurav Kadavath, Jackson Kernion,\nTom Conerly, Sheer El-Showk, Nelson Elhage, Zac\nHatfield-Dodds, Danny Hernandez, Tristan Hume,\nScott Johnston, Shauna Kravec, Liane Lovitt, Neel\nNanda, Catherine Olsson, Dario Amodei, Tom\nBrown, Jack Clark, Sam McCandlish, Chris Olah,\nBen Mann, and Jared Kaplan. 2022a. Training a\nhelpful and harmless assistant with reinforcement\nlearning from human feedback.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu,\nAmanda Askell, Jackson Kernion, Andy Jones, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Cameron\nMcKinnon, Carol Chen, Catherine Olsson, Christo-\npher Olah, Danny Hernandez, Dawn Drain, Deep\nGanguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,\nJamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua\nLandau, Kamal Ndousse, Kamile Lukosuite, Liane\nLovitt, Michael Sellitto, Nelson Elhage, Nicholas\nSchiefer, Noemi Mercado, Nova DasSarma, Robert\nLasenby, Robin Larson, Sam Ringer, Scott John-\nston, Shauna Kravec, Sheer El Showk, Stanislav Fort,\nTamera Lanham, Timothy Telleen-Lawton, Tom Con-\nerly, Tom Henighan, Tristan Hume, Samuel R. Bow-\nman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,\nNicholas Joseph, Sam McCandlish, Tom Brown, and\nJared Kaplan. 2022b. Constitutional ai: Harmless-\nness from ai feedback.\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and\nHanna Wallach. 2020. Language (technology) is\npower: A critical survey of “bias” in NLP. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5454–\n5476, Online. Association for Computational Lin-\nguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nChristoph Demus, Jonas Pitz, Mina Schütz, Nadine\nProbol, Melanie Siegel, and Dirk Labudde. 2022.\nDetox: A comprehensive dataset for German offen-\nsive language and conversation analysis. In Proceed-\nings of the Sixth Workshop on Online Abuse and\nHarms (WOAH), pages 143–153, Seattle, Washington\n(Hybrid). Association for Computational Linguistics.\nJiawen Deng, Jingyan Zhou, Hao Sun, Chujie Zheng,\nFei Mi, Helen Meng, and Minlie Huang. 2022.\nCOLD: A benchmark for Chinese offensive language\ndetection. In Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Process-\ning, pages 11580–11599, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nEmily Dinan, Gavin Abercrombie, A. Bergman, Shan-\nnon Spruit, Dirk Hovy, Y-Lan Boureau, and Verena\nRieser. 2022. SafetyKit: First aid for measuring\nsafety in open-domain conversational systems. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 4113–4133, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nDenis Emelin, Ronan Le Bras, Jena D. Hwang, Maxwell\nForbes, and Yejin Choi. 2021. Moral stories: Situ-\nated reasoning about norms, intents, actions, and\ntheir consequences. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 698–718, Online and Punta Cana,\n215\nDominican Republic. Association for Computational\nLinguistics.\nSusan T. Fiske. 2023. Prejudice, discrimination, and\nstereotyping. Noba textbook series: Psychology.\nDEF Publisher. http://noba.to/jfkx7nrd.\nMaxwell Forbes, Jena D. Hwang, Vered Shwartz,\nMaarten Sap, and Yejin Choi. 2020. Social chem-\nistry 101: Learning to reason about social and moral\nnorms. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 653–670, Online. Association for\nComputational Linguistics.\nDeep Ganguli, Liane Lovitt, Jackson Kernion, Amanda\nAskell, Yuntao Bai, Saurav Kadavath, Ben Mann,\nEthan Perez, Nicholas Schiefer, Kamal Ndousse,\nAndy Jones, Sam Bowman, Anna Chen, Tom Con-\nerly, Nova DasSarma, Dawn Drain, Nelson Elhage,\nSheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds,\nTom Henighan, Danny Hernandez, Tristan Hume,\nJosh Jacobson, Scott Johnston, Shauna Kravec,\nCatherine Olsson, Sam Ringer, Eli Tran-Johnson,\nDario Amodei, Tom Brown, Nicholas Joseph, Sam\nMcCandlish, Chris Olah, Jared Kaplan, and Jack\nClark. 2022. Red teaming language models to re-\nduce harms: Methods, scaling behaviors, and lessons\nlearned.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealToxi-\ncityPrompts: Evaluating neural toxic degeneration\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356–3369, Online. Association for Computational\nLinguistics.\nThomas Hartvigsen, Saadia Gabriel, Hamid Palangi,\nMaarten Sap, Dipankar Ray, and Ece Kamar. 2022.\nToxiGen: A large-scale machine-generated dataset\nfor adversarial and implicit hate speech detection.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3309–3326, Dublin, Ireland.\nAssociation for Computational Linguistics.\nDan Hendrycks, Collin Burns, Steven Basart, Andrew\nCritch, Jerry Li, Dawn Song, and Jacob Steinhardt.\n2021. Aligning AI with shared human values. In In-\nternational Conference on Learning Representations.\nYounghoon Jeong, Juhyun Oh, Jongwon Lee, Jaimeen\nAhn, Jihyung Moon, Sungjoon Park, and Alice Oh.\n2022. KOLD: Korean offensive language dataset.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n10818–10833, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nTaeYoung Kang, Eunrang Kwon, Junbum Lee,\nYoungeun Nam, Junmo Song, and JeongKyu Suh.\n2022. Korean online hate speech dataset for multil-\nabel classification: How can social science improve\ndataset on hate speech?\nZachary Kenton, Tom Everitt, Laura Weidinger, Ia-\nson Gabriel, Vladimir Mikulik, and Geoffrey Irving.\n2021. Alignment of language agents.\nBoseop Kim, HyoungSeok Kim, Sang-Woo Lee,\nGichang Lee, Donghyun Kwak, Jeon Dong Hyeon,\nSunghyun Park, Sungju Kim, Seonhoon Kim, Dong-\npil Seo, Heungsub Lee, Minyoung Jeong, Sungjae\nLee, Minsub Kim, Suk Hyun Ko, Seokhun Kim,\nTaeyong Park, Jinuk Kim, Soyoung Kang, Na-Hyeon\nRyu, Kang Min Yoo, Minsuk Chang, Soobin Suh,\nSookyo In, Jinseong Park, Kyungduk Kim, Hiun\nKim, Jisu Jeong, Yong Goo Yeo, Donghoon Ham,\nDongju Park, Min Young Lee, Jaewook Kang, Inho\nKang, Jung-Woo Ha, Woomyoung Park, and Nako\nSung. 2021. What changes can large-scale language\nmodels bring? intensive study on HyperCLOV A:\nBillions-scale Korean generative pretrained trans-\nformers. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 3405–3424, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nHyunwoo Kim, Youngjae Yu, Liwei Jiang, Ximing\nLu, Daniel Khashabi, Gunhee Kim, Yejin Choi, and\nMaarten Sap. 2022. ProsocialDialog: A prosocial\nbackbone for conversational agents. In Proceedings\nof the 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 4005–4029, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nJean Lee, Taejun Lim, Heejun Lee, Bogeun Jo, Yangsok\nKim, Heegeun Yoon, and Soyeon Caren Han. 2022.\nK-MHaS: A multi-label hate speech detection dataset\nin Korean online news comment. In Proceedings of\nthe 29th International Conference on Computational\nLinguistics, pages 3530–3538, Gyeongju, Republic\nof Korea. International Committee on Computational\nLinguistics.\nJunbum Lee. 2020. Kcbert: Korean comments bert.\nIn Proceedings of the 32nd Annual Conference on\nHuman and Cognitive Language Technology, pages\n437–440.\nJunbum Lee. 2021. Kcelectra: Korean comments elec-\ntra. https://github.com/Beomi/KcELECTRA.\nTao Li, Daniel Khashabi, Tushar Khot, Ashish Sab-\nharwal, and Vivek Srikumar. 2020. UNQOVERing\nstereotyping biases via underspecified questions. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020 , pages 3475–3489, Online.\nAssociation for Computational Linguistics.\n216\nAlisa Liu, Swabha Swayamdipta, Noah A. Smith, and\nYejin Choi. 2022. W ANLI: Worker and AI collabora-\ntion for natural language inference dataset creation.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022 , pages 6826–6847, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nNicholas Lourie, Ronan Le Bras, and Yejin Choi. 2021.\nScruples: A corpus of community ethical judg-\nments on 32,000 real-life anecdotes. Proceedings\nof the AAAI Conference on Artificial Intelligence ,\n35(15):13470–13479.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2022. Cross-task generaliza-\ntion via natural language crowdsourcing instructions.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3470–3487, Dublin, Ireland.\nAssociation for Computational Linguistics.\nJihyung Moon, Won Ik Cho, and Junbum Lee. 2020.\nBEEP! Korean corpus of online news comments for\ntoxic speech detection. In Proceedings of the Eighth\nInternational Workshop on Natural Language Pro-\ncessing for Social Media, pages 25–31, Online. As-\nsociation for Computational Linguistics.\nHamdy Mubarak, Sabit Hassan, and Shammur Absar\nChowdhury. 2022. Emojis as anchors to detect arabic\noffensive language and hate speech.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoSet: Measuring stereotypical bias in pretrained\nlanguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 5356–5371, Online. Association for\nComputational Linguistics.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020. CrowS-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1953–1967, Online. As-\nsociation for Computational Linguistics.\nSungjoon Park, Jihyung Moon, Sungdong Kim, Won Ik\nCho, Ji Yoon Han, Jangwon Park, Chisung Song, Jun-\nseong Kim, Youngsook Song, Taehwan Oh, Joohong\nLee, Juhyun Oh, Sungwon Lyu, Younghoon Jeong,\nInkwon Lee, Sangwoo Seo, Dongjun Lee, Hyunwoo\nKim, Myeonghwa Lee, Seongbo Jang, Seungwon\nDo, Sunkyoung Kim, Kyungtae Lim, Jongwon Lee,\nKyumin Park, Jamin Shin, Seonghyun Kim, Lucy\nPark, Lucy Park, Alice Oh, Jung-Woo Ha (NA VER\nAI Lab), Kyunghyun Cho, and Kyunghyun Cho.\n2021. Klue: Korean language understanding eval-\nuation. In Proceedings of the Neural Information\nProcessing Systems Track on Datasets and Bench-\nmarks, volume 1. Curran.\nAlicia Parrish, Angelica Chen, Nikita Nangia,\nVishakh Padmakumar, Jason Phang, Jana Thompson,\nPhu Mon Htut, and Samuel Bowman. 2022. BBQ:\nA hand-built bias benchmark for question answering.\nIn Findings of the Association for Computational\nLinguistics: ACL 2022 , pages 2086–2105, Dublin,\nIreland. Association for Computational Linguistics.\nEthan Perez, Saffron Huang, Francis Song, Trevor Cai,\nRoman Ring, John Aslanides, Amelia Glaese, Nat\nMcAleese, and Geoffrey Irving. 2022. Red teaming\nlanguage models with language models. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 3419–3448,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nPaul Röttger, Bertie Vidgen, Dong Nguyen, Zeerak\nWaseem, Helen Margetts, and Janet Pierrehumbert.\n2021. HateCheck: Functional tests for hate speech\ndetection models. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 41–58, Online. Association for\nComputational Linguistics.\nRachel Rudinger, Jason Naradowsky, Brian Leonard,\nand Benjamin Van Durme. 2018. Gender bias in\ncoreference resolution. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers) ,\npages 8–14, New Orleans, Louisiana. Association for\nComputational Linguistics.\nMaarten Sap, Saadia Gabriel, Lianhui Qin, Dan Juraf-\nsky, Noah A. Smith, and Yejin Choi. 2020. Social\nbias frames: Reasoning about social and power im-\nplications of language. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 5477–5490, Online. Association\nfor Computational Linguistics.\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nand Nanyun Peng. 2019. The woman worked as\na babysitter: On biases in language generation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3407–\n3412, Hong Kong, China. Association for Computa-\ntional Linguistics.\nIrene Solaiman and Christy Dennison. 2021. Process\nfor adapting language models to society (palms) with\nvalues-targeted datasets. In Advances in Neural Infor-\nmation Processing Systems, volume 34, pages 5861–\n5873. Curran Associates, Inc.\nHao Sun, Guangxuan Xu, Jiawen Deng, Jiale Cheng,\nChujie Zheng, Hao Zhou, Nanyun Peng, Xiaoyan\nZhu, and Minlie Huang. 2022. On the safety of con-\nversational models: Taxonomy, dataset, and bench-\nmark. In Findings of the Association for Compu-\ntational Linguistics: ACL 2022 , pages 3906–3923,\n217\nDublin, Ireland. Association for Computational Lin-\nguistics.\nSwabha Swayamdipta, Roy Schwartz, Nicholas Lourie,\nYizhong Wang, Hannaneh Hajishirzi, Noah A. Smith,\nand Yejin Choi. 2020. Dataset cartography: Mapping\nand diagnosing datasets with training dynamics. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9275–9293, Online. Association for Computa-\ntional Linguistics.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-\nner, and Sameer Singh. 2019a. Universal adversarial\ntriggers for attacking and analyzing NLP. InProceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2153–2162, Hong\nKong, China. Association for Computational Linguis-\ntics.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gard-\nner, and Sameer Singh. 2019b. Universal adversarial\ntriggers for attacking and analyzing NLP. InProceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2153–2162, Hong\nKong, China. Association for Computational Linguis-\ntics.\nWilliam Warner and Julia Hirschberg. 2012. Detecting\nhate speech on the world wide web. In Proceedings\nof the Second Workshop on Language in Social Me-\ndia, pages 19–26, Montréal, Canada. Association for\nComputational Linguistics.\nMichael Wiegand, Josef Ruppenhofer, and Thomas\nKleinbauer. 2019. Detection of Abusive Language:\nthe Problem of Biased Datasets. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 602–608, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nAlexandros Xenos, John Pavlopoulos, Ion Androut-\nsopoulos, Lucas Dixon, Jeffrey Sorensen, and Leo\nLaugier. 2021. Toxicity detection can be sensitive to\nthe conversational context.\nAlbert Xu, Eshaan Pathak, Eric Wallace, Suchin Guru-\nrangan, Maarten Sap, and Dan Klein. 2021a. Detoxi-\nfying language models risks marginalizing minority\nvoices. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2390–2397, Online. Association for\nComputational Linguistics.\nJing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason\nWeston, and Emily Dinan. 2021b. Bot-adversarial di-\nalogue for safe conversational agents. InProceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2950–2968,\nOnline. Association for Computational Linguistics.\nKichang Yang, Wonjun Jang, and Won Ik Cho. 2022.\nAPEACH: Attacking pejorative expressions with\nanalysis on crowd-generated hate speech evaluation\ndatasets. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2022, pages 7076–7086,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018. Gender bias in\ncoreference resolution: Evaluation and debiasing\nmethods. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 15–20, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\n218\nA The K OSBI Dataset\nA.1 Domain and Categories of Social\nDemographics\nThe entire social demographic categories and\ngroups are listed in Table 8.\nA.2 Example Data\nFigure 4: Example pairs of a context and a sentence\nwith labels pertaining to a given social demographic\ncategory and group. Note, \"그 ᆷᄉ ᅮᄌ ᅥ\" is a Korean buz-\nzword, roughly meaning \"Silver spoon\" or \"Privileged\nbackground\" in English.\nA.3 Details of Unsafe Label\nUnsafe sub-labels # data\nContext\nStereotypical 4,719\nPrejudice / Discrimination 407\nOther 1,590\nUndefined 2,260\nSentence\nStereotypical 8,197\nPrejudice 1,085\nDiscrimination 655\nOther 336\nUndefined 6,905\nTable 7: Distribution of the unsafe sub-labels of context\nand sentence. Undefined represents cases where three\nannotators could not decide the label through major\nvoting, but 2 or more annotators chose one of the unsafe\nsub-labels.\nCategory Social Group\nGender identity†\nMale\nFemale\nOthers\nSexual orientation† Homosexual\nAge & Generation†\nBaby\nChildern\nTeenagers\nYoung people\nMiddle-aged\nOld people\nBaby bommeres\n386 Generation\nGeneration X\nMilennials\nGeneration Z\nAlpha Generation\nRace, Ethnicity & Nationality†\nSouth Korean\nNorth Korean\nChinese\nJapanese\nAmerican (U.S.)\nRussian\nAsian\nAfrican\nEuropean\nAmericans, Oceanians\nPeople of color / White\nReligion†\nNonreligious\nProtestantism\nBuddhism\nCatholic\nIslam\nOthers\nDisability status† Disability\nPhysical appearance†\nFace Appearance\nBody Type\nSexual Appearance\nOthers\nPolitical orientation†\nLiberal\nConservative\nOthers\nSocio-economic status†\nEmployment Type\nEconomic Condition\nOccupation\nDomestic area of origin\nSeoul\nGyeonggi-do\nChungcheong-do\nGyeongsang-do\nJeolla-do\nJeju-do\nGangwon-do\nKorean residents from abroad\nMarital status\nMarried\nSingle\nDivorced\nRemarriaed\nCommon-law marriage\nOthers\nPregnancy & Birth\nPregnant Person\nPerson Who Gave Birth\nPregnancy Before Marriage\nOthers\nFamily form\nFoster Family\nBroken Family\nMulticultural Family\nStep Family\nOthers\nCriminal record Violent Criminal Convictions\nOthers\nEducation & University & Major\nAcademic Background\nUniversities\nMajor\nTable 8: Category and demographic groups considered\nin KOSBI. The categories in both UDHR and NHRCK\nare marked by †.\n219\nB HyperClova Generation\nHyperparameters We use the same hyperparam-\neters for generating contexts and sentences: top-p\nsampling with p= 0.8, a temperature of 0.5, repeat\npenalty of 5, and stop word of ”\\n”. We gradually\nincreased top-pand temperature every time we met\nduplication generations.\nC Prompts of LLMs\nContext Generation Prompt Among 10 demon-\nstration samples, we collect 5 demonstrations from\nthe pool which has the same demographic category\nwith what we are generating and 3 demonstrations\nfrom the pool which has the same demographic\ngroup. Detailed prompt is below:\nᄋ ᅡᄅ ᅢᄉ ᅡᄒ ᅬᄌ ᅥ ᆨ지 ᆸ단ᄋ ᅦᄉ ᅩ ᆨᄒ ᅡ는ᄉ ᅡᄅ ᅡ ᆷᄋ ᅦᄃ ᅢᄒ ᅢᄐ ᅳ ᆨ정한상황ᄋ ᅵ\nᄂ ᅡ전반ᄌ ᅥ ᆨᄋ ᅵᆫ현상을중리 ᆸᄌ ᅥ ᆨᄋ ᅳᄅ ᅩᄌ ᅡ ᆨ성ᄒ ᅡᄉ ᅦᄋ ᅭ. ⇒General\nInstruction\n{category} 분ᄅ ᅲᄋ ᅦ ᄄ ᅡᄅ ᅡ{social group} ᄋ ᅦᄉ ᅩ ᆨᄒ ᅡ는 ᄉ ᅡ\nᄅ ᅡ ᆷᄋ ᅦᄃ ᅢᄒ ᅢᄉ ᅥᄌ ᅡ ᆨ성 ⇒Category and demographic group\n중리 ᆸᄌ ᅥ ᆨ상황문장: {context}\n###\n...\n###\n{category} 분ᄅ ᅲᄋ ᅦᄄ ᅡᄅ ᅡ{social group}ᄋ ᅦᄉ ᅩ ᆨᄒ ᅡ는ᄉ ᅡᄅ ᅡ ᆷᄋ ᅦᄃ ᅢ\nᄒ ᅢᄉ ᅥᄌ ᅡ ᆨ성\n중리 ᆸᄌ ᅥ ᆨ상황문장:\nSafe Sentence Generation Prompt Among 10\ndemonstration samples, we collect 5 demonstra-\ntions from the pool which has the same context\nlabel with what we are generating.\nᄋ ᅡᄅ ᅢᄉ ᅡᄒ ᅬᄌ ᅥ ᆨ지 ᆸ단ᄋ ᅦᄉ ᅩ ᆨᄒ ᅡ는ᄉ ᅡᄅ ᅡ ᆷᄋ ᅦᄃ ᅢᄒ ᅢ\"중리 ᆸᄌ ᅥ ᆨ상황\n문장\" ᄃ ᅱᄋ ᅦᄋ ᅵᄋ ᅥᄌ ᅵ는ᄌ ᅡ연ᄉ ᅳᄅ ᅥ운\"ᄃ ᅡᄋ ᅳ ᆷ문장\"을ᄌ ᅡ ᆨ성ᄒ ᅡ\nᄉ ᅦᄋ ᅭ. \"ᄃ ᅡᄋ ᅳ ᆷ문장\"ᄋ ᅳ ᆫᄒ ᅢ당ᄉ ᅡᄒ ᅬᄌ ᅥ ᆨ지 ᆸ단ᄋ ᅦᄃ ᅢᄒ ᅢᄀ ᅩ정관\n념과편견ᄋ ᅵ없는ᄉ ᅡᄅ ᅡ ᆷᄋ ᅵᄑ ᅭ현할ᄉ ᅮ있는안전한문장\n이 ᆸᄂ ᅵᄃ ᅡ. ᄇ ᅵᄒ ᅡ와ᄎ ᅡ별ᄋ ᅴᄋ ᅴᄃ ᅩᄀ ᅡ없ᄀ ᅩ, ᄀ ᅩ ᆼ정ᄒ ᅡᄆ ᅧᄀ ᅩ정관\n념ᄌ ᅥ ᆨᄋ ᅵᄌ ᅵᄋ ᅡ ᆭᄋ ᅳ ᆫᄑ ᅭ현, ᄃ ᅡ양성을존중ᄒ ᅡᄀ ᅩ, ᄋ ᅵᆫ간ᄋ ᅴ존ᄋ ᅥ ᆷ\n성과ᄌ ᅡᄋ ᅲ와ᄀ ᅵ본ᄀ ᅯᆫ을ᄇ ᅩ장합ᄂ ᅵᄃ ᅡ. ⇒General Instruction\n{category} 분ᄅ ᅲᄋ ᅦ ᄄ ᅡᄅ ᅡ{social group} ᄋ ᅵᆫ ᄉ ᅡᄅ ᅡ ᆷᄋ ᅦ ᄃ ᅢ\nᄒ ᅢᄉ ᅥᄌ ᅡ ᆨ성 ⇒Category and demographic group\n중리 ᆸᄌ ᅥ ᆨ상황문장: {context}\nᄃ ᅡᄋ ᅳ ᆷ문장: {sentence}\n###\n...\n###\n{category} 분ᄅ ᅲᄋ ᅦᄄ ᅡᄅ ᅡ{social group}ᄋ ᅦᄉ ᅩ ᆨᄒ ᅡ는ᄉ ᅡᄅ ᅡ ᆷᄋ ᅦᄃ ᅢ\nᄒ ᅢᄉ ᅥᄌ ᅡ ᆨ성 ⇒Category and demographic group\n중리 ᆸᄌ ᅥ ᆨ상황문장: {context}\nᄃ ᅡᄋ ᅳ ᆷ문장:\nUnsafe Sentence Generation Prompt All pro-\ncess is same with safe sentence generation except\nthe prompt below:\nᄋ ᅡᄅ ᅢᄉ ᅡᄒ ᅬᄌ ᅥ ᆨ지 ᆸ단ᄋ ᅦᄉ ᅩ ᆨᄒ ᅡ는ᄉ ᅡᄅ ᅡ ᆷᄋ ᅦᄃ ᅢᄒ ᅢ\"중리 ᆸᄌ ᅥ ᆨ상황문\n장\" ᄃ ᅱᄋ ᅦᄋ ᅵᄋ ᅥᄌ ᅵ는ᄌ ᅡ연ᄉ ᅳᄅ ᅥ운\"ᄃ ᅡᄋ ᅳ ᆷ문장\"을ᄌ ᅡ ᆨ성ᄒ ᅡᄉ ᅦ\nᄋ ᅭ. \"ᄃ ᅡᄋ ᅳ ᆷ문장\"ᄋ ᅳ ᆫᄒ ᅢ당ᄉ ᅡᄒ ᅬᄌ ᅥ ᆨ지 ᆸ단ᄋ ᅦᄃ ᅢᄒ ᅢᄀ ᅩ정관념과\n편견을ᄀ ᅡᄌ ᅵᆫᄉ ᅡᄅ ᅡ ᆷᄋ ᅵᄑ ᅭ현할ᄉ ᅮ있는ᄋ ᅱᄒ ᅥ ᆷ한문장이 ᆸᄂ ᅵᄃ ᅡ.\nᄇ ᅵᄒ ᅡ와ᄎ ᅡ별ᄋ ᅴᄋ ᅴᄃ ᅩᄀ ᅡ있ᄀ ᅩ, 불ᄀ ᅩ ᆼ정ᄒ ᅡᄀ ᅩ논란을불ᄅ ᅥ이 ᆯ\nᄋ ᅳ키 ᆯᄉ ᅮ있ᄋ ᅳᄆ ᅧ, ᄉ ᅡᄒ ᅬᄌ ᅥ ᆨ지 ᆸ단ᄂ ᅢᄃ ᅡ양성을ᄒ ᅢ치 ᆯᄉ ᅮ있ᄉ ᅳ ᆸ\nᄂ ᅵᄃ ᅡ. ⇒General Instruction\n{category} 분ᄅ ᅲᄋ ᅦ ᄄ ᅡᄅ ᅡ{social group} ᄋ ᅵᆫ ᄉ ᅡᄅ ᅡ ᆷᄋ ᅦ ᄃ ᅢ\nᄒ ᅢᄉ ᅥᄌ ᅡ ᆨ성 ⇒Category and demographic group\n중리 ᆸᄌ ᅥ ᆨ상황문장: {context}\nᄃ ᅡᄋ ᅳ ᆷ문장: {sentence}\n###\n...\n###\n{category} 분ᄅ ᅲᄋ ᅦᄄ ᅡᄅ ᅡ{social group}ᄋ ᅦᄉ ᅩ ᆨᄒ ᅡ는ᄉ ᅡᄅ ᅡ ᆷᄋ ᅦᄃ ᅢ\nᄒ ᅢᄉ ᅥᄌ ᅡ ᆨ성 ⇒Category and demographic group\n중리 ᆸᄌ ᅥ ᆨ상황문장: {context}\nᄃ ᅡᄋ ᅳ ᆷ문장:\nZero-shot Prompt for Human-evaluation The\nfollowing prompt is used for the human-evaluation\nin Section 4.3\nᄋ ᅡᄅ ᅢᄉ ᅡᄒ ᅬᄌ ᅥ ᆨ지 ᆸ단ᄋ ᅦᄉ ᅩ ᆨᄒ ᅡ는ᄉ ᅡᄅ ᅡ ᆷᄋ ᅦᄃ ᅢᄒ ᅢ”상황문장” ᄃ ᅱ\nᄋ ᅦᄋ ᅵᄋ ᅥᄌ ᅵ는ᄌ ᅡ연ᄉ ᅳᄅ ᅥ운”ᄃ ᅡᄋ ᅳ ᆷ문장”을ᄌ ᅡ ᆨ성ᄒ ᅡᄉ ᅦᄋ ᅭ.\n##\n{category} 분ᄅ ᅲᄋ ᅦᄄ ᅡᄅ ᅡ”{social group}”ᄋ ᅦᄉ ᅩ ᆨᄒ ᅡ는ᄉ ᅡᄅ ᅡ ᆷᄋ ᅦ\nᄃ ᅢᄒ ᅢᄉ ᅥᄌ ᅡ ᆨ성\n상황문장: {context}\nᄃ ᅡᄋ ᅳ ᆷ문장:\nD Modeling Details\nAll the training processes described below are un-\nder PyTorch-Lightning9 and Huggingface10 envi-\nronments. For training, the search space for hyper-\nparameters is:\n• learning rate : [1e−5, 2e−5, 3e−5, 4e−5,\n5e−5]\n• batch size : [32, 48]\n• gradient clipping value : [0.0, 1.0]\n• epoch : 15\n• early stopping : after 5 epochs without im-\nprovement\nD.1 Context Filter Models\nWe use KcELECTRa (Lee, 2021) as a backbone\nmodel for our context filter model. The demo-\ngraphic group and the context concatenated by\nthe separate token([SEP]) are fed to the model to\ntrain the model to predict whether the demographic\ngroup is in the context text. 3,819 and 7,569 data\npoints are used for training after iterations 1 and 2,\n9https://www.pytorchlightning.ai/\n10https://huggingface.co/\n220\nrespectively (80/10/10 split). The best configura-\ntion is 5e−5 learning rate, 48 batch size, and 0.0\ngradient clipping value for both iterations 1 and 2,\nshowing 83.51% and 90.75% of accuracy for each\ntest set, respectively.\nD.2 Next Sentence Filter Models\nWe also use KcELECTRa as a backbone model for\nour next sentence filter model. Note that the main\npurpose of the next sentence filtering process is to\nleverage the filter model to collect the most ambigu-\nous samples w.r.t the model. The separate token\nconcatenates the context and the next sentence, and\nthe model is trained to predict the unsafeness of\nthe text. 4,324 and 11,457 data points are used\nfor training after iterations 1 and 2, respectively\n(80/10/10 split). The best hyperparameter setup\nis (5e−5 learning rate, 32 batch size, 0.0 gradi-\nent clipping value) and ( 2e−5 learning rate, 48\nbatch size, 0.0 gradient clipping value) for itera-\ntions 1 and 2, respectively. The accuracies of the\nbest models are 83.83% (iteration 1) and 69.37%\n(iteration 2). Due to ambiguous data points being\naugmented for iteration 2, the later model shows\nlower accuracy.\nD.3 Safe Sentence Classifiers\nAfter collecting all data points, we train a safe sen-\ntence classifier. In addition to the KcELECTRa\nmodel, we use KLUE-BERT (Park et al., 2021)\nand KcBERT (Lee, 2020) as candidates. As men-\ntioned in Section 4.1, we augment data by using\ncontext data. Among six configurations which con-\nsist of three models and two datasets (with and\nwithout augmentation), the best model is KcELEC-\nTRa with augmentation (71.22% accuracy). The\nhyperparameter setup is 1e−5 learning rate, 32\nbatch size, and 0.0 gradient clipping value.\nE Safety Evaluations of Continuations\nTable 9 shows the safety generation results given\nsafe and unsafe contexts, respectively. As can be\nseen by comparing both results, models generate\nmore unsafe sentences when an unsafe context is\ngiven, while all models generate 99% of safe con-\ntinuations when conditioned on a safe context in\nk= 8settings.\nModel Safety ProbabilityExp. Avg. Safetyk=1 2 4 8\nSafeContext\nGPT-3 (175B) .931 .961 .984 .993 .674 ±.083\nHyperClova (6.9B) .806 .931 .977 .993 .626±.103\nHyperClova (13B) .766 .918 .974 .990 .642±.108\nHyperClova (30B) .809 .941 .977 .990 .647±.102\nHyperClova (82B) .829 .918 .967 .993 .660±.106\nUnsafeContext\nGPT-3 (175B) .644 .753 .870 .918 .522 ±.082\nHyperClova (6.9B) .432 .616 .740 .842 .469±.093\nHyperClova (13B) .507 .616 .767 .849 .473±.099\nHyperClova (30B) .363 .514 .603 .712 .443±.073\nHyperClova (82B) .342 .493 .651 .788 .441±.093\nTable 9: Safety evaluations of LLM’s continuations af-\nter given safe (top) and unsafe (bottom) contexts, respec-\ntively. All metrics are calculated as the same manner as\nin Table 5.\nContext Safe Unsafe Undecided TotalSentenceSafe Unsafe Total S. U. T. S. U. T.\nTest set 1,382 1,092 2,474 317 617 934 7 11 15 3,423Augmented2,681 2,268 4,949 589 1,239 1,828 11 13 24 6,801\nTable 10: The number of instances for the test and\naugmented test sets.\nFigure 5: Human evaluation on the subset of the aug-\nmented test set. For all three models, filter-based mod-\neration shows efficacy on reducing unsafe generations.\nF Results and Analyses on Augmented\nTest Set\nAs mentioned in Sec 4.4, we augmented our test set\nwith additional annotated data to increase the relia-\nbility of test results. As a result, the augmented test\nset has 6,801 data points (See Table 10). Among\nthem, we randomly sampled 1,746 contexts for the\nhuman-evaluation experiments, which is the same\nprocedure described in Sec 4.3. As seen in Figure 5,\nwe can still observe that the filter-based moderation\nreduces unsafe generations for all three models. Ta-\nble 11 presents qualitative results for another subset\nof the test set.\n221\nQuality Assessments\nGrammatical\nError-Free (%)\nUnderstandability\n(%)\nPertaning to Target\nSocial Group (%)\nContext (%)\nCoherency Overall (%)\nGPT-3 (175B) 84.4 77.4 87.3 70.8 30.2\nGPT-3 (175B) + filtering 86.4 79.4 86.5 71.1 30.1\nHyperCLOV A (80B) 98.9 97.9 93.9 90.5 56.5\nHyperCLOV A (80B) + filtering 99.3 97.5 92.5 88.9 56.0\nHyperCLOV A (30B) 99.0 98.3 95.4 93.0 62.6\nHyperCLOV A (30B) + filtering 99.1 97.9 93.6 91.8 60.0\nTable 11: Human evaluation on the subset of augmented test set. Following the Table 6, comparisons between\nunfiltered responses and filtered responses among 8 generations from GPT-3 (175B; ‘text-davinci-003’), HyperClova\n(82B and 30B) are shown.\nFigure 6: Moderation results on each category in the augmented test set. Left: Safe response ratio from human\nevaluation results. Right: Safe sentence classification performance of the best classifier (KcELECTRa). The vertical\nlines represent the averages of safe response and accuracy for all categories. Categories are ordered by descend of\nthe classifier’s accuracy.\nG Social Bias Mitigation Level by\nCategory\nFigure 6 shows all results with and without the\nfilter-based moderation for GPT-3 (175B), Hyper-\nCLOV A (82B), and HyperCLOV A (30B). Although\nthe increment of safety does not strongly correlate\nto the performance of the classifier, the filter-based\nmoderation approach demonstrates the effective-\nness for all social demographic categories. It is\ncrucial to scrutinize and improve the models’ safety\nfor fair consideration of each demographic category\nand group.\nH Human Annotation\nH.1 Crowd Worker Compensation\nWe utilized one of the representative crowdsourc-\ning platforms in South Korea. Among all appli-\ncants to our project, we selected 200 crowd work-\ners. All workers have received reasonable monetary\ncompensation; 80 KRW per sub-single question.\nAll workers are expected to finish 2∼3 sub-single\nquestions in one minute, resulting in the minimum\ncompensation is 9,600 KRW/hour. For reference,\nthe minimum hourly wage in South Korea is 9260\nKRW in 2023. The annotation guidelines and the\ninterface is depicted in Figure 7 and Figure 8.\n222\nH.2 Annotation Demographics\nThe detailed demographics are presented in Ta-\nble 12. Note that every single data was annotated\nby two females and one male or vice versa.\nGender\nMale 96 48.0%\nFemale 103 51.5%\nPrefer not to mention 1 0.5%\nAge\n18-24 4 2.0%\n25-34 44 22.0%\n35-44 71 35.5%\n45-54 55 27.5%\n55-64 23 11.5%\n65+ 2 1.0%\nPrefer not to mention 1 0.5%\nCountry of Origin\nSouth Korea 199 99.5%\nChina 1 0.5%\nDomestic Area of Origin\nSeoul 71 35.5%\nGyeongsang, Daegu, Busan 40 20.0%\nGyeonggi, Incheon 42 21.0%\nJeolla, Gwangju 19 9.5%\nChungcheong, Daejeon, Sejong 22 11.0%\nGangwon 2 1.0%\nJeju 3 1.5%\nPrefer not to mention 1 0.5%\nEducation\nCollege degree - Associate or Bachelor’s 147 73.5%\nGraduate or Professional Degree 31 15.5%\nHigh school, GED, etc. 21 10.5%\nPrefer not to mention 1 0.5%\nSexual Orientation\nStraight 187 93.5%\nLGBTQ+ 1 0.5%\nPrefer not to mention 12 6.0%\nDisability\nNo 194 97.0%\nYes 1 0.5%\nPrefer not to mention 5 2.5%\nTotal 200\nTable 12: Demographics of the crowd workers.\nH.3 Annotation Guidelines and Interface\nFigure 7: Question annotation setup. Q1:\nQuality check (understandability and grammati-\ncally/semantically error-free). Q2: Pertaining to Target\nSocial Group. Q3: Label of Context (Safe/Unsafe).\n223\nFigure 8: Response annotation setup. Q1: Quality\ncheck (appropriateness to the \"Question\" and grammat-\nically/semantically error-free). Q2: Label of Sentence\n(Safe/Unsafe) Q2-1: (if the sentence is ‘Unsafe’) Label\nsub-labels.\n224",
  "topic": "Moderation",
  "concepts": [
    {
      "name": "Moderation",
      "score": 0.8404035568237305
    },
    {
      "name": "SAFER",
      "score": 0.725168764591217
    },
    {
      "name": "Affect (linguistics)",
      "score": 0.6410770416259766
    },
    {
      "name": "Computer science",
      "score": 0.6174376010894775
    },
    {
      "name": "Software deployment",
      "score": 0.5528483390808105
    },
    {
      "name": "Psychology",
      "score": 0.2955242991447449
    },
    {
      "name": "Machine learning",
      "score": 0.2122860550880432
    },
    {
      "name": "Computer security",
      "score": 0.20669758319854736
    },
    {
      "name": "Communication",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I60922564",
      "name": "Naver (South Korea)",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I139264467",
      "name": "Seoul National University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I158012942",
      "name": "University of Richmond",
      "country": "US"
    }
  ],
  "cited_by": 13
}