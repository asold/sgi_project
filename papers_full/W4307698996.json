{
  "title": "DMFpred: Predicting protein disorder molecular functions based on protein cubic language model",
  "url": "https://openalex.org/W4307698996",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2963173732",
      "name": "Yihe Pang",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1974433614",
      "name": "Bin Liu",
      "affiliations": [
        "Beijing Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2963173732",
      "name": "Yihe Pang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1974433614",
      "name": "Bin Liu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2060592742",
    "https://openalex.org/W1989511016",
    "https://openalex.org/W2073745520",
    "https://openalex.org/W1994840640",
    "https://openalex.org/W2007016963",
    "https://openalex.org/W2120881407",
    "https://openalex.org/W2554784405",
    "https://openalex.org/W2074180389",
    "https://openalex.org/W2009455776",
    "https://openalex.org/W2147733973",
    "https://openalex.org/W2149472608",
    "https://openalex.org/W2048659288",
    "https://openalex.org/W1985985027",
    "https://openalex.org/W1994176565",
    "https://openalex.org/W2071514557",
    "https://openalex.org/W2074455942",
    "https://openalex.org/W2043816461",
    "https://openalex.org/W2079139546",
    "https://openalex.org/W3156051371",
    "https://openalex.org/W3110645309",
    "https://openalex.org/W2558841060",
    "https://openalex.org/W6770686123",
    "https://openalex.org/W2420922808",
    "https://openalex.org/W2765783049",
    "https://openalex.org/W2168220711",
    "https://openalex.org/W2972006361",
    "https://openalex.org/W2120744225",
    "https://openalex.org/W2806884808",
    "https://openalex.org/W2366093519",
    "https://openalex.org/W2897991802",
    "https://openalex.org/W2790001298",
    "https://openalex.org/W1901968134",
    "https://openalex.org/W4200540728",
    "https://openalex.org/W3196994021",
    "https://openalex.org/W2016170088",
    "https://openalex.org/W2093205346",
    "https://openalex.org/W2165698076",
    "https://openalex.org/W2158714788",
    "https://openalex.org/W3043901899",
    "https://openalex.org/W2972411752",
    "https://openalex.org/W2982608875",
    "https://openalex.org/W2169478909",
    "https://openalex.org/W2137566700",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W2989977616",
    "https://openalex.org/W3086768476",
    "https://openalex.org/W3093934303",
    "https://openalex.org/W6770400242",
    "https://openalex.org/W2135987917",
    "https://openalex.org/W2990528340",
    "https://openalex.org/W4213072029",
    "https://openalex.org/W2114315281",
    "https://openalex.org/W3117811820",
    "https://openalex.org/W3124413522",
    "https://openalex.org/W2987184424",
    "https://openalex.org/W2988357750"
  ],
  "abstract": "Intrinsically disordered proteins and regions (IDP/IDRs) are widespread in living organisms and perform various essential molecular functions. These functions are summarized as six general categories, including entropic chain, assembler, scavenger, effector, display site, and chaperone. The alteration of IDP functions is responsible for many human diseases. Therefore, identifying the function of disordered proteins is helpful for the studies of drug target discovery and rational drug design. Experimental identification of the molecular functions of IDP in the wet lab is an expensive and laborious procedure that is not applicable on a large scale. Some computational methods have been proposed and mainly focus on predicting the entropic chain function of IDRs, while the computational predictive methods for the remaining five important categories of disordered molecular functions are desired. Motivated by the growing numbers of experimental annotated functional sequences and the need to expand the coverage of disordered protein function predictors, we proposed DMFpred for disordered molecular functions prediction, covering disordered assembler, scavenger, effector, display site and chaperone. DMFpred employs the Protein Cubic Language Model (PCLM), which incorporates three protein language models for characterizing sequences, structural and functional features of proteins, and attention-based alignment for understanding the relationship among three captured features and generating a joint representation of proteins. The PCLM was pre-trained with large-scaled IDR sequences and fine-tuned with functional annotation sequences for molecular function prediction. The predictive performance evaluation on five categories of functional and multi-functional residues suggested that DMFpred provides high-quality predictions. The web-server of DMFpred can be freely accessed from http://bliulab.net/DMFpred/ .",
  "full_text": "RESEA RCH ARTICL E\nDMFpred: Predicting protein disorder\nmolecular functions based on protein cubic\nlanguage model\nYihe Pang\n1\n, Bin Liu\nID\n1,2\n*\n1 School of Computer Science and Technology , Beijing Institute of Technolo gy, Beijing, China, 2 Advanced\nResearc h Institute of Multidiscipli nary Science, Beijing Institute of Technolo gy, Beijing, China\n* bliu@bli ulab.net\nAbstract\nIntrinsically disordered proteins and regions (IDP/IDRs) are widespread in living organisms\nand perform various essential molecular functions. These functions are summarized as six\ngeneral categories, including entropic chain, assembler, scavenger, effector, display site,\nand chaperone . The alteration of IDP functions is responsible for many human diseases.\nTherefore, identifying the function of disordered proteins is helpful for the studies of drug tar-\nget discovery and rational drug design. Experimental identification of the molecular func-\ntions of IDP in the wet lab is an expensive and laborious procedure that is not applicable on\na large scale. Some computational methods have been proposed and mainly focus on pre-\ndicting the entropic chain function of IDRs, while the computational predictive methods for\nthe remaining five important categories of disordered molecular functions are desired. Moti-\nvated by the growing numbers of experimental annotated functional sequences and the\nneed to expand the coverage of disordered protein function predictors, we proposed\nDMFpred for disordered molecular functions prediction, covering disordered assembler,\nscavenger, effector, display site and chaperone. DMFpred employs the Protein Cubic Lan-\nguage Model (PCLM), which incorporates three protein language models for characterizing\nsequences, structural and functional features of proteins, and attention-based alignment for\nunderstanding the relationship among three captured features and generating a joint repre-\nsentation of proteins. The PCLM was pre-trained with large-scaled IDR sequences and fine-\ntuned with functional annotation sequences for molecular function prediction. The predictive\nperformance evaluation on five categories of functional and multi-functional residues sug-\ngested that DMFpred provides high-quality predictions. The web-server of DMFpred can be\nfreely accessed from http://bliulab.net/DMF pred/.\nAuthor summary\nIntrinsically disordered proteins (IDPs) are proteins that are without stable three-dimen-\nsional (3D) structures in native physiologic conditions. The discovery of IDPs has dis-\nproved the idea that proteins must fold into 3D structures to accomplish their biological\nPLOS COMP UTATIONAL  BIOLOGY\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1010668 October 31, 2022 1 / 18\na1111111111\na1111111111\na1111111111\na1111111111\na1111111111\nOPEN ACCESS\nCitation: Pang Y, Liu B (2022) DMFpred:\nPredicting protein disorder molecular functions\nbased on protein cubic language model. PLoS\nComput Biol 18(10): e1010668. https://d oi.org/\n10.1371/ journal.pcbi.10 10668\nEditor: Jeffrey Skolnick, Georgia Institute of\nTechnology , UNITED STATES\nReceived: August 3, 2022\nAccepted: October 19, 2022\nPublished: October 31, 2022\nCopyright: © 2022 Pang, Liu. This is an open\naccess article distributed under the terms of the\nCreative Commons Attribution License, which\npermits unrestricte d use, distribu tion, and\nreproduction in any medium, provided the original\nauthor and source are credited.\nData Availabilit y Statement: The data used in this\nstudy are available at http://bliulab .net/DMFpred/.\nFunding: This work was supported by the Nationa l\nNatural Science Foundation of China (No.\n62271049 and U22A2039 ) and the Beijing Natural\nScience Foundation (No. JQ19019) to (BL). The\nfunders had no role in study design, data collection\nand analysis, decision to publish, or preparation of\nthe manuscript.\nCompeting interests : The authors have declared\nthat no competing interests exist.\nfunctions. They are prevalent in eukaryotic organisms and carry out many critical func-\ntions in cellular regulation, signaling networks, and disease pathways. These functions of\nIDPs can be summarized into six general categories, including entropic chain, assembler,\nscavenger, effector, display site, and chaperone. Experimental identification of the molec-\nular functions of IDP in the wet lab is an expensive and laborious procedure that is not\napplicable on a large scale. Some computational methods have been proposed to identify\nthe entropic chain function of IDPs, while the predictive methods for the remaining but\nimportant functions of IDPs are desired. In this study, we proposed a disordered molecu-\nlar function computational predictor of proteins, namely DMFpred. The DMFpred sup-\nports high-throughput sequences as input and computationally predicts five molecular\nfunctions of IDPs, including assembler, scavenger, effector, display site, and chaperone.\nThe evaluation results suggested that DMFpred provides high-quality predictions, and the\ncorresponding web server of DMFpred can be freely accessed from http://bliulab.net/\nDMFpred/.\nThis is a PLOS Computational Biology Methods paper.\nIntroduction\nProteins or regions that lack stable 3D-structures under the native physiologic conditions are\nknown as intrinsically disordered proteins and regions (IDP/IDRs). Recent studies have sug-\ngested that IDP/IDRs are common in nature, with more than 30% of proteins in eukaryotes\nbeing disordered [1,2]. The widespread occurrence of IDP/IDRs alter the classical protein\nstructure-function paradigm [3–5]. IDP/IDRs play essential roles in living organisms, the\nalteration of their functions are responsible for many human diseases such as cancer [6], Alz-\nheimer’s [7] and Parkinson’s [8]. Exploring the molecular functional mechanism of IDP/IDRs\nwill be helpful for a complete understanding of protein structures and functions, and will be\nalso used to guide wet lab experiments and inform studies of rational drug design [9,10].\nThe functions of protein disordered regions arise from their native structural flexibility or\nfrom their ability to bind to partner molecules [4]. These disorder functions can be summa-\nrized as six categories: entropic chains, assembler, scavenger, effector, display site, and chaper-\none [4,11]. The disordered entropic chain benefits directly from its intrinsically disordered\nconformation without becoming structured, which serves as the connector between domains\nand structural elements making up domains [12]. Disordered assemblers bring together multi-\nple binding partners, and promote the formation of large protein complexes [4,5,13]. Scaven-\nger disordered regions in proteins store and neutralize small ligands, such as chromogranin,\nsalivary glycoproteins and calcium-binding phosphoproteins [11,14,15]. Effectors interact\nwith other partner proteins and modify their activity [16]. Some disordered regions serve as\ndisplay sites, facilitating easy access and recognition of the post-translational modifications\n(PTMs) in proteins [17]. Disordered chaperone function makes the IDRs assisting RNA and\nprotein molecules to reach their functionally folded states [18].\nThe intrinsically disordered is encoded in the protein sequence, motivating the develop-\nment of computational sequence-based disorder predictors [19]. Currently, there are about\n200 million disordered proteins have been identified experimentally and predictively [20]. In\ncontrast, only thousands of disordered proteins have functional annotations [21,22]. This data\nPLOS COMP UTATIONAL  BIOLOGY\nDMFpr ed\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1010668 October 31, 2022 2 / 18\nsuggests that it is important to develop computational predictors for filling the deepening gap\nbetween annotated and unannotated disordered sequences. In this regard, several sequence-\nbased computational predictors are proposed for predicting specific functions of disordered\nproteins. For example, the DFLpred [23] and APOD [24] are computational methods devel-\noped for predicting disordered linkers that fulfill entropic chain function in proteins. Besides,\nthere are predictors for identifying disordered regions binding to specific types of molecular\npartners, including protein binding predictors [25–32], DNAs and RNAs binding predictors\n[33,34], and lipid binding predictors [35]. However, methods for predicting the other five clas-\nses (assembler, scavenger, effector, display site and chaperone) of molecular functions of IDRs\nare required.\nProtein representation is critical for the construction of computational predictors. Protein\nsequence defines structure, which in turn dictates its function [4]. The intrinsically disordered\nproteins reassessed the classical sequence-structure-fun ction paradigm [36], the complex\nsequence, structure, and functional properties of IDP/IDRs should be explored to fully repre-\nsent the disordered proteins. By modelling the language’s generative rules, the language model\nin natural language processing (NLP) comprehensively understands the language, and capture\nthe semantic features of text, which is an indispensable technology in NLP. Protein sequences\ncan be viewed as the language of genetics sharing high similarities with natural language sen-\ntences [37]. For example, the natural language sentences composed of words express their\nsemantics, while proteins composed of residues perform various functions. Inspired by their\nsimilarities, the proteins can be represented and modelled by the language models.\nIn this paper, we proposed DMFpred predictor, which predicts five molecular functions of\nIDRs, including assembler, scavenger, effector, display site, and chaperone. DMFpred employs\nthe Protein Cubic Language Model (PCLM) to learn protein representations, consisting of\nthree types of protein language models and an attention-based language model alignment\n(ALAN) module. Three protein language models were used to capture protein sequences,\nstructural, and functional features, respectively. The ALAN module extracts the relationship\namong three captured features and encodes the complementarity information. The key chal-\nlenge in functional prediction is that the number of disordered sequences with functional\nannotations is relatively small. The transfer learning technology can transfer knowledge from\ntasks with plentiful training data to improve the performance of similar other tasks, which is\nespecially useful for the task with limited training data [38]. Therefore, we first pre-trained\nPCLM with large IDRs sequences to capture the disordered features of proteins. Then the gen-\neral disordered features were transferred separately to five different disorder functions predic-\ntion via model fine-tuning. Benefited from pre-training and function-specific fine-tuning of\nPCLM, DMFpred captures more relevant features of disorder molecular functions. The abla-\ntion experiment results demonstrated that each module of PCLM contributes to the predictive\nperformance improvements. And further evaluation suggested that DMFpred provides high-\nquality predictions on all five categories of functional residues and multi-functional residues,\nwhose residues carry more than one category of molecular functions. The corresponding web\nserver of DMFpred was established and can be freely accessed from http://bliulab.net/\nDMFpred/.\nMaterials and methods\nBenchmark datasets\nThe datasets used in this study were collected from DisProt [22], which is the major repository\nof manually curated functional annotations of intrinsically disordered proteins from literature.\nAll sequences in the database are functionally annotated at the amino acid level. In this study,\nPLOS COMP UTATIONAL  BIOLOGY\nDMFpr ed\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1010668 October 31, 2022 3 / 18\nwe focused on five general categories of disordered molecular functions (DMFs), including\nassembler, scavenger, effector, display site and chaperone. Following the intrinsically Disor-\ndered Proteins Ontology (IDPO) schema in the DisProt, each of the five categories of function\nterms has one or two leaf terms (see S1 Fig). Here, we treat all the leaf terms as the same func-\ntional class as their root terms. The sequences in the database are functionally annotated with\namino acids as the basic unit, and we collected a total of 590 sequences containing residues\nassigned at least one class of DMFs. For each class of function, we treat the residues annotated\nwith the functional term class in the database as functional residues, and the others as non-\nfunctional residues. Then we assign all the functional residues in the sequences as label ‘1’ and\nnon-functional residues as label ‘0’, leading to five lines of labels corresponding to five catego-\nries of DMFs annotations.\nTo avoid data redundancy, we performed the similarity clustering on the 590 sequences by\nusing PSI-BLAST [39] by setting the threshold of 25%, and filtered sequences with pairwise\nsequence similarity >25%. This way ensured that the sequence similarity between any two\nsequences in the collections was lower than 25%. The remaining 541 proteins were randomly\ndivided into training, evaluation, and test sets in a ratio of 6:2:2. Finally, 324 sequences were\nused as the training set for model training, 106 sequences were used as the valuation set for\nmodel selection, and 111 sequences were used as the independent test set (TEST-1) to evaluate\npredictive performance (S1 Data). The number of functional residues for the five categories of\ndisordered molecular functions in the DMF benchmark datasets is given in Table 1.\nArchitecture of protein cubic language model\nSequence, structure and function language models. Sequence, structure, and function\nare three important aspects of proteins. Only one language model cannot fully characterize the\nthree features. In this paper, we employed three types of language models for capturing the\nsequences, structural, and functional features of proteins.\nSequence language model. The amino acid sequence contains the evolutionary informa-\ntion of protein. Here, the bidirectional long short-term memory (Bi-LSTM) networks were\nemployed as the sequence language model to capture the global correlation features of evolu-\ntionary information (see Fig 1A). By using the protein PSSM profile and HMM profile as the\ninputs of the sequence language model, the sequence features Seq can be calculated by [40]:\nSeq ¼ ½h\n1\n; h\n2\n; � � � ; h\nL\n� ð1Þ\nh\ni\n¼ Conc at ½LS TM\nf\nðX\nL�40\nÞ; LST M\nb\nðX\nL�40\nÞ� ð2Þ\nwhere X\nL×40\nis the combination of PSSM and HMM matrix generated by PSI-BLAST [39] and\nHH-suits [41] respectively, and L is the length of the sequence. LSTM\nf\nand LSTM\nb\nindicate the\nforward and backward recurrent neural unit respectively. Concat represents the combination\nof vectors.\nTable 1. The number of functional residues in the DMF benchm ark datasets .\nDataset Numbe r of Assembl er\nresidue\nNumbe r of Chapero ne\nresidue\nNumber of Display-sit e\nresidue\nNumber of Effector\nresidue\nNumber of Scavenge r\nresidue\nTraining set 14177 2236 5041 12783 2202\nEvaluati on\nset\n7764 904 932 4102 1022\nTEST-1 4412 836 1001 3980 545\nhttps://do i.org/10.1371/j ournal.pc bi.1010668. t001\nPLOS COMP UTATIONAL  BIOLOGY\nDMFpr ed\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1010668 October 31, 2022 4 / 18\nStructure language model\nProtein structure reflects the results of local interaction among residues. The structure lan-\nguage model aims to capture structural features of the protein, and a convolutional-based\nmodel is used to capture structural local pattern features from the residue-residue contact map\n(CCM) (see Fig 1B). By taking CCMs as inputs, the structure features Stc can be calculated by\n[42]:\nStc ¼ rel u½Conv ðY\nL�L\n; Filter\nst c\nÞ þ b\nstc\n� ð3Þ\nwhere Y\nL×L\nis the CCM profile generated by CCMpred [43,44], Filter\nstc\nand b\nstc\nare trainable\nvariables, Conv represents convolution operator, and relu is the Rectified Linear Unit activa-\ntion function [45].\nFunction language model\nFunctional conservative sequence segments also known as functional motifs hold particular\nfunctionality information of proteins. Previous researches [46–48] have shown that the motif-\nbased convolution (MotifConv) by embedding particular motifs into the convolution kernel\ncan learn the prior biological features. Inspired by MotifConv, the functional motif-based con-\nvolution was employed as the function language model to capture proteins’ functional features\n(see Fig 1C). The 164 motifs used in this study were extracted from the Eukaryotic Linear\nMotif (ELM) database [49]. The letter-probability matrix of each motif is used to build the con-\nvolution kernel formulated as:\nM\n1\n¼\na\n1;1\n� � � a\n1;20\n.\n.\n.\n.\n.\n.\n.\n.\n.\na\nl;1\n� � � a\nl;20\n2\n6\n6\n6\n4\n3\n7\n7\n7\n5\nð4Þ\nFig 1. The architecture of protein cubic langua ge model (PCLM ). The PCLM contains five main modules : three protein language models (A. sequenc e, B.\nstructure, and C. function language model), attention-b ased language model alignment module (D. ALAN), and the fusion and output layer (E). The input\nprotein sequence is converted to sequence profile X, structur e profile Y, and function profile Z, which are then fed into three protein language models to\ncapture the sequence features Seq, the structure features Stc, and the function features Func. Next, three captured features are incorpora ted into the alignment\nfeatures (F\nstc−func\n, F\nseq−stc\nand F\nseq−func\n) by ALAN modules. Finally, the fusion and output layers merge the outputs of ALAN to calculate the propensi ty score P\ni\nof disorder molecular function for each residue.\nhttps://do i.org/10.1371/j ournal.pc bi.1010668. g001\nPLOS COMP UTATIONAL  BIOLOGY\nDMFpr ed\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1010668 October 31, 2022 5 / 18\nwhere l is the length of motif, a\ni,j\nrepresents the frequency of standard amino acid. Then the\nfunction features Func can be calculated by:\nFunc ¼ rel u½Conv ðZ\nL�20\n; MÞ þ b\nfunc\n� ð5Þ\nwhere Z\nL×20\nis the one-hot encoding matrix of protein sequence, M is the combination of 164\nmotif convolution kernel matrix, and b\nfunc\nis trainable variable.\nAttention based language model alignment\nThe primary sequences encode the disordered states of IDP/IDRs, which in turn determine func-\ntions. The potential correlations among sequence, structure and function are essential information\nfor the protein representations. In this study, attention alignment models the correlations between\nprotein features by calculating the attention alignment weights on two kinds of features (see Fig\n1D). For example, given the sequence features Seq, structure features Stc, and function features\nFunc, the attention-alignment weights α\nseq−stc\n, α\nseq−func\nand α\nstc−func\nare calculated by:\nα\ns e q\u0000 s t c\n¼ s o f t m a xðH\n1\ns e q\nSeq � H\n1\ns t c\nStcÞ ð6Þ\nα\ns e q\u0000 f u n c\n¼ s o f t m a xðH\n1\ns e q\nSeq � H\n1\nf u n c\nFuncÞ ð7Þ\nα\ns t c\u0000 f u n c\n¼ s o f t m a xðH\n1\ns t c\nStc � H\n1\nf u n c\nFuncÞ ð8Þ\nwhere H\n1\ns e q\n, H\n1\ns t c\nand H\n1\nf u n c\nare the trainable weight variables. The attention-alignmen t weights\nbetween two kinds of features reflect matching patterns between different property aspects of the\nproteins. Weighted by the attention-alignment weights, the sequence features Seq, structure fea-\ntures Stc and function features Func captured by three language models can be enhanced and\nfused into the complementary features F\nseq−stc\n, F\nseq−func\nand F\nstc−func\n:\nF\ns e q\u0000 s t c\n¼ ConcatðH\n2\ns e q\nα\nT\ns e q\u0000 s t c\nSeq\n0\n; H\n2\ns t c\nα\ns e q\u0000 s t c\nStc\n0\nÞ ð9Þ\nF\ns e q\u0000 f u n c\n¼ ConcatðH\n2\ns e q\nα\nT\ns e q\u0000 f u n c\nSeq\n0\n; H\n2\nf u n c\nα\ns e q\u0000 f u n c\nFunc\n0\nÞ ð1 0Þ\nF\ns t c\u0000 f u n c\n¼ ConcatðH\n2\nf u n c\nα\nT\ns t c\u0000 f u n c\nStc\n0\n; H\n2\ns t c\nα\ns t c\u0000 f u n c\nFunc\n0\nÞ ð1 1Þ\nwhere H\n2\ns e q\nH\n2\ns t c\nand H\n2\nf u n c\nare the trainable variables, Seq\n0\n, Stc\n0\nand Func\n0\nindicate the transformed\nfeature matrix of Seq, Stc and Func, respectively. The softmax is the activation function. The com-\nplementary features F\nseq−stc\n, F\nseq−func\nand F\nstc−func\nlearn the correlations among sequence, structure,\nand functional properties of proteins, and these features are fed into the cubic fusion and output\nlayers for calculating the predictive propensity score.\nCubic fusion and output layer\nThe cubic fusion module of PCLM merges the three alignment complementary features into\nlatent cubic space, and obtains a joint representation matrix F\nseq−stc−func\nof protein sequences:\nF\nseq\u0000 stc\u0000 fu nc\n¼ W\nx\nF\nseq\u0000 st c\nþ W\ny\nF\nseq\u0000 fu nc\nþ W\nz\nF\nstc\u0000 fun c\nð12Þ\nF\nseq \u0000 stc\u0000 fun c\n¼ ½F\n1\n; � � � ; F\nL\u0000 1\n; F\nL\n�; F\nseq\u0000 stc\u0000 func\n2 R\nL�n\nð13Þ\nwhere L denotes the length of the input sequence, n denotes the dimension of features, W\nx\n,\nPLOS COMP UTATIONAL  BIOLOGY\nDMFpr ed\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1010668 October 31, 2022 6 / 18\nW\ny\n, and W\nz\nare the trainable weighted variables. Each vector F\ni\nin the representation matrix\nrepresents the features of each residue in the sequence. The fully connected (FC) layer captures\nthe global and local correlations between residues in the sequence so as to calculate the pro-\npensity score P\ni\nfor each residue:\nP½P\n1\n; . . . ; P\ni\n; . . . ; P\nL\n� ¼ Sig moidðW\nf\nF\nseq\u0000 stc\u0000 fu nc\nþ b\nf\nÞ ð14Þ\nwhere W\nf\nand b\nf\nrepresent the weighted and bias variables, respectively.\nPre-training of protein cubic language model\nThe transfer learning involves a model training strategy, which transfers the knowledge\nlearned from the source domain to a new and different target domain. It is especially effective\nwhen the target domain has insufficient training data [38]. In this study, although we have rel-\natively limited number of disorder functional annotation regions for PCLM model training,\nthe number of intrinsically disordered regions (IDRs) is sufficient. The large number of IDRs\nwill overcome the problem that model cannot be fully trained with insufficient disorder func-\ntional data, and the generic disordered features learned from IDR dataset can be transferred to\nfacilitate the disorder molecular function prediction. Therefore, in this study, we employed the\nwidely used IDP/IDR prediction benchmark dataset [40] as the pre-training dataset to pre-\ntrain PCLM model for predicting disordered regions in protein. To avoid data redundancy, we\nexcluded sequences with >25% sequence similarity to the disordered functional benchmark\ndatasets, and obtained 2639 sequences with 38134 IDRs and 1079 sequences with 16403 IDRs\nfor model pre-training and validation, respectively (S2 Data). The binary cross-entropy loss\nfunction was used to calculate the loss score for model parameters optimizing [50]:\nlos s ¼ \u0000\nX\ni\n½y\ni\nlog p\ni\nþ ð1 \u0000 y\ni\nÞlog ð1 \u0000 p\ni\nÞ� ð15Þ\nwhere p\ni\ndenotes the predictive score for residue R\ni\nbeing disordered calculated by Eq 14, and\ny\ni\nrepresents the actual label of disordered residue. The Adam optimizer [51] with a learning\nrate of 0.001 was employed to optimize the model parameters, and the model with the mini-\nmized loss score on the IDR validation set was saved as the pre-trained model.\nFine-tuning PCLM for predicting disordered molecular functions\nIn the fine-tuning stage, the pre-trained PCLM model was fine-tuned with functional specific\ndata for predicting the disordered molecular functions in protein. Because of the differences\nbetween the five molecular functions, we separately fine-tuned PCLM with assembler, chaper-\none, display site, effector, and scavenger functional annotations in the DMF benchmark data-\nset, leading to five independent predicting PCLM models (see Fig 2). In the DMFpred\nFig 2. The functional specific fine-tuning of protein cubic language model (PCLM). The pretrained PCLM model was separately fine-tuned with five\ncategorie s of disorder ed molecul ar functions into five correspond ing PCLM\n1-5\nmodels for predicting assembler (A) chaperone (B) display site (C) effector (D)\nand scavenge r (E) functional residues from the input sequence. Prob\n1\n, Prob\n2\n, Prob\n3\n, Prob\n4\n, and Prob\n5\nrepresent the predicted propensi ty scores for the five\nfunctions of input sequenc e, respectively.\nhttps://do i.org/10.1371/j ournal.pc bi.1010668. g002\nPLOS COMP UTATIONAL  BIOLOGY\nDMFpr ed\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1010668 October 31, 2022 7 / 18\npredictor, the five functional specific fine-tuned PCLM models work in parallel to produce\nfive disordered molecular functional predictions for each residue in the input proteins. Here,\nwe used the same loss function and optimizer as the ones used in the pre-training stage, but\ndifferent learning rates to fine-tune the model parameters for each function. Parameters of all\nlayers in PCLM were fine-tuned for achieving better performance, and this strategy has been\nadopted by many transfer learning based studies [52,53]. More detailed hyper-parameters for\nDMFpred are given in S1 Table.\nEvaluation criteria\nDMFpred generates two forms of outputs: the real-valued propensity score (the likelihood of resi-\ndue with the given function) and binary results (residue with or without the given function).\nBinary predictions were converted from the propensities: one residue is predicted as functional\nresidue if its propensity score is greater than a given threshold. Otherwise, it is predicted as the\nnon-functional residue. The receiver operating characteristic curve (ROC) and AUC value (area\nunder ROC curve) were utilized to evaluate the predictive performance of the real-valued propen-\nsity prediction. Sensitivity (Sn), specificity (Sp) and accuracy (ACC) were used for the evaluation\nof the binary results. Since the dataset is imbalanced, i.e. there are many more non-functional resi-\ndues than the functional residues. Therefore, two metrics, balanced accuracy (BACC) and the\nMatthews Correlation Coefficient (MCC) were used to measure the predictive performance.\nDisordered residues interact with multiple partners with more than one functions are called\nthe multi-functional residues. The residue-level functional prediction of these multi-functional\nresidues can be treated as a multi-label learning task, and five example-based metrics were uti-\nlized to evaluate the performance of DMFpred on multi-functional residues [54]:\nHam ming los s ¼\n1\np\nX\np\ni¼1\n1\nq\njhðx\ni\nÞDY\ni\nj\nAccu rac y\nexa m\n¼\n1\np\nX\np\ni¼1\njhðx\ni\nÞ \\ Y\ni\nj\njhðx\ni\nÞ [ Y\ni\nj\nPrec isio n\nexa m\n¼\n1\np\nX\np\ni¼1\njhðx\ni\nÞ \\ Y\ni\nj\njhðx\ni\nÞj\nReca ll\nexa m\n¼\n1\np\nX\np\ni¼1\njhðx\ni\nÞ \\ Y\ni\nj\njY\ni\nj\nF 1\nexa m\n¼\n2 � Prec isio n\nexa m\n� Rec all\nexa m\nPre cisi on\nexa m\nþ Reca ll\nexa m\nð16Þ\n8\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n<\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n:\nwhere p indicates the total number of samples, q indicates the number of labels, h(x\ni\n) is the pre-\ndicted label set and Y\ni\nis the true label set. Δ represents the symmetric difference between two\nsets.\nResults and discussion\nFunctional specific fine-tuning achieves better performance\nIn order to investigate the differences among five categories of molecular functions, we per-\nformed the cross-functional validation on the benchmark datasets. To avoid the overestima-\ntion caused by the multi-functional residues, sequences that only belonging to one class\nfunction in the training and validation sets are used to fine-tune and test the PCLM model.\nThe AUC evaluation results are shown in Fig 3. From Fig 3, we can see that model fine-tuned\nand tested on the same function achieves the best performance, while cross-functional\nPLOS COMP UTATIONAL  BIOLOGY\nDMFpr ed\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1010668 October 31, 2022 8 / 18\npredictors achieve lower performances. These predictive results suggest that specialized pre-\ndictors are required for each functional category, and function-specific fine-tuning is the key\nto achieve better predictive performance of each disordered molecular function.\nAblation analysis of protein cubic language models\nTo verify the contribution of three language models to DMFpred, we performed an ablation\nanalysis. The PCLM models with different combinations of three language models were indi-\nvidually fine-tuned on five molecular function training data, and the corresponding AUC val-\nues for each function evaluated on validation dataset are shown in Fig 4. We can see that (i)\npredictors with the combination of three language models consistently achieve the best perfor-\nmances for all five functions; (ii) the prediction performance of predictor decreased by drop-\nping the structural language model. Predictors with only sequence language model performed\nthe worst. These results are not surprising because three language models capture the\nsequence, structural, and functional features of proteins, and these three features are comple-\nmentary, and contribute to the functional prediction. As a result, predictors incorporating the\nthree protein language models achieve the best performance.\nAttention based language model alignment learns the correlation patterns\nIn order to investigate the performance improvement of attention-based language model\nalignment (ALAN) to the proposed predictor. We compared the performance of predictors for\nFig 3. Cross-func tional validation results. Functio ns on the x-axis were used to fine-tune PCLM model, and\nfunctions on the y-axis were used for model validation.\nhttps://d oi.org/10.1371/j ournal.pc bi.1010668. g003\nPLOS COMP UTATIONAL  BIOLOGY\nDMFpr ed\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1010668 October 31, 2022 9 / 18\npredicting five disordered molecular functions by using PCLM model with and without the\nALAN module. The PCLM model without ALAN directly feed the features captured by the three\nlanguage models to the fusion and output layers (see Fig 1) to calculate prediction results. The\ntwo types of models were independently fine-tuned with five different functions, and the results\nevaluated on the validation dataset are shown in Fig 5. From this figure, we can see that predictors\nwith ALAN consistently outperform the predictors without ALAN on five classes of functions,\ndemonstrating the effectiveness of the ALAN module. Furthermore, we note that the predictor for\nScavenger function with an ALAN achieves better performance in terms of AUC value. These\nresults may be caused by the fact that the complementary features captured by the ALAN module\nsupplemented the inadequate sequence, structure and functional features learned from limited\nannotated sequences. This improvement is especially manifested in the Scavenger function with a\nrelatively small number of annotated sequences. Benefitted from the features captured by ALAN,\npredictor can make more accurate prediction leading to better performance.\nTo further analyse the information learned by the ALAN module, we visualized the atten-\ntion-alignment weights between sequence and structure features. Two protein examples (Dis-\nProt ID: DP02925 and DP00284) selected from the independent test set (TEST-1) were\nvisualized in Fig 6, from which we can see that the specific segments in the sequences map\nwith the highest attention weights, and these sequence segments corresponding to the experi-\nmentally determined functional motifs searched from the ELM database [49] by FIMO tools\n(https://meme-suite.org/meme/t ools/fimo). These results indicate that the ALAN can capture\ncritical correlation patterns by modelling the relationship between different protein features.\nThis prior biological knowledge captured by ALAN complements the original sequence, struc-\nture and functional attributes of proteins, providing a powerful protein representation.\nModel pre-training facilitates feature correlation\nIn order to explore the contribution of model pre-trained with disordered proteins, we com-\npare the predictive power of features extracted between models directly trained with molecular\nfunctional sequences (DT in Fig 7) and the fine-tuned model based on pre-training with IDRs\n(PT in Fig 7). Following previous studies [23, 24], the absolute point-biserial correlation\nFig 4. The predictive results of PCLM model in DMFpred with different langua ge models. Seq represents the\nPCLM with only sequenc e language model, Seq-Func denotes the PCLM with the combination of sequence language\nmodel and function language model, and Seq-Func-Stc stands for the PCLM model, which is the combinatio n of\nsequence language model, functio n language model and structure language model. The AUC values were calculated on\nthe validation data set.\nhttps://d oi.org/10.1371/j ournal.pc bi.1010668. g004\nPLOS COMP UTATIONAL  BIOLOGY\nDMFpr ed\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1010668 October 31, 2022 10 / 18\n(PBC) score is used to quantify the feature predictive qualities, which reflects the correlation\nbetween numeric and binary variables:\nPBC ¼\nm\n1\n\u0000 m\n0\ns\nn\nffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi\nn\n0\n� n\n1\nn � n\nr\nð17Þ\nwhere n\n0\nand n\n1\nindicate the number of functional and non-functional residues, m\n0\nand m\n1\nindicate the average values of features of functional and non-functional residues, s\nn\nis the stan-\ndard deviation of all values of features, and n is the total number of residues. The PBC score\nresults for five functions on the TEST-1 independent test set are shown in Fig 7. From this fig-\nure, we observe that the features captured by the pre-trained model are consistently outper-\nformed that directly trained model on all five functions. This is because model pre-trained\nwith IDR sequences captures more disordered features than directly trained on limited func-\ntional sequences. As the functional residues are the sub-set of disordered regions, the common\ndisordered features captured by pre-trained model facilitate to distinguish disordered func-\ntional residues from ordered residues, leading to a robust predictive quality.\nOverall results\nTo our best knowledge, DMFpred is currently the only predictor for predicting the five general\nmolecular functions of disordered proteins. There are two forms of outputs of DMFpred: real-\nvalued propensity results and binary results. We used the ROC curve and AUC value for evalu-\nating the real-valued predictive results. Sn, Sp, ACC and two metrics for imbalanced datasets\nFig 5. The predictive results (AUC values) of DMFpred with and without attention-bas ed language model alignment. The PCLM represents the entire\nPCLM predictive model, while the Without attention denotes the PCLM model without the ALAN module. Both two models were independen tly fine-tuned\nand evaluated for the five molecular functions on the training dataset and validation dataset, respective ly.\nhttps://do i.org/10.1371/j ournal.pc bi.1010668. g005\nPLOS COMP UTATIONAL  BIOLOGY\nDMFpr ed\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1010668 October 31, 2022 11 / 18\n(BACC and MCC) were used to assess binary results. The evaluation results on the TEST-1\nindependent test set are shown in Table 2 (the ROC curve and thresholds settings see S2 Fig,\nS2 Table). From Table 2, we can see that DMFpred provides accurate predictive performance\nfor all five functional categories in terms of AUC values. The Sn, Sp and ACC results show the\nability of DMFpred to correctly predict functional and non-functional residues, demonstrating\nthe predictive performance.\nIn order to further evaluate the predictive performance of the predictor, we constructed a\nnew independent test set (TEST-2) with the sequences newly added into the DisProt database\nduring July 2021 to June 2022 by following the same dataset collection protocols. TEST-2 con-\ntains 47 proteins with 5780 functional residues, including 3753 assemblers, 218 chaperones,\n855 display sites, 682 effectors and 272 scavengers. The prediction results of DMFpred on\nTEST-2 are shown in S3 Table. From these results, we can see that the predictive results\nachieved by DMFpred on the new independent test set TEST-2 are highly comparable with\nthose on the independent test dataset TEST-1, indicating that the performance of DMFpred\npredictor is stable.\nPredictive results on the multi-functional residues\nThe disordered residues interacting with multiple partners with more than one functions are\ncalled multi-functional residues. In order to investigate the performance of DMFpred\nFig 6. The attention alignment weight visualization s.\nhttps://do i.org/10.1371/j ournal.pc bi.1010668. g006\nPLOS COMP UTATIONAL  BIOLOGY\nDMFpr ed\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1010668 October 31, 2022 12 / 18\npredictor for predicting these multi-functional residues, we collected all the residues with at\nleast two functional annotations from TEST-1 dataset, and obtained a total number of 1352\nmulti-functional residues for performance evaluation. We compare DMFpred with a random\nbaseline predictor generating the multi-functional labels for each residue with a probability of\n0.5, and the evaluation results are shown in Table 3. From this table, we can see the followings:\n(i) compared with the baseline predictor, DMFpred achieves lower Hamming loss, but higher\naccuracy, which indicates DMFpred can accurately predict more multi-functional residues\nthan the baseline predictor. (ii) DMFpred achieves higher performance than the baseline\nmethod in terms of precision, recall rate and F1 value. These results are not surprising because\nDMFpred was fine-tuned with function-specific labels on the benchmark dataset so as to learn\nthe discriminative features of each function. Benefitting from the accurate prediction for five\nfunctions, DMFpred achieves better performance for predicting multi-functional residues.\nFig 7. Absolute PBC score distribution s on five functions. DT represents the PCLM models directly trained with molecular functional data, and PT\nrepresents the PCLM models pre-traine d with disorder ed proteins .\nhttps://do i.org/10.1371/j ournal.pc bi.1010668. g007\nTable 2. The predictive performan ce of DMFpred for five categories of molecular functions on TEST-1 independen t test set.\nFunction AUC Sn Sp ACC BACC MCC\nAssembler 0.682 0.428 0.804 0.778 0.616 0.143\nChaperone 0.716 0.379 0.919 0.912 0.650 0.120\nDisplay-site 0.702 0.291 0.962 0.952 0.627 0.155\nEffector 0.749 0.663 0.741 0.736 0.703 0.215\nScavenger 0.779 0.999 0.520 0.524 0.761 0.095\nhttps://do i.org/10.1371/j ournal.pc bi.1010668. t002\nTable 3. The predictive results for multi-funct ional residues on TEST-1 indepen dent test set.\nPredictor Hamming loss Accuracy\nexam\nPrecision\nexam\nRecall\nexam\nF1\nexam\nDMFpred 0.413 0.404 0.504 0.671 0.576\nBaseline 0.508 0.285 0.409 0.485 0.444\nhttps://do i.org/10.1371/j ournal.pc bi.1010668. t003\nPLOS COMP UTATIONAL  BIOLOGY\nDMFpr ed\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1010668 October 31, 2022 13 / 18\nConclusion\nIntrinsically disordered proteins/regions perform various molecular functions in living organ-\nisms. These functions of IDP/IDRs can be summarized as six general categories, including\nentropic chains, assembler, scavenger, effector, display site and chaperone. Motivated by the\ngrowing numbers of the annotated disordered sequences and the need to expand the coverage\nof disordered protein function predictors, we introduce the disordered molecular functional\npredictor called DMFpred, covering five important categories: disordered assembler, scaven-\nger, effector, display site and chaperone. It has the following advantages: 1) DMFpred\nemployed the protein cubic language model (PCLM) that incorporates three protein language\nmodels for characterizing sequence, structure, and functional attributes of proteins. PCLM\nemployed attention-based language model alignment to capture the sequence-structure-func-\ntion correlation and learn a joint representation of proteins. 2) Benefited from the pre-training\nand function-specific fine-tuning of PCLM, DMFpred captures discriminative features for five\nfunctional categories prediction. 3) The evaluation results on five categories of functional and\nmulti-functional residues suggest that DMFpred provides high quality predictions. 4) The\nweb-server of DMFpred is established and can be freely accessed from http://bliulab.net/\nDMFpred/, which will be helpful to researchers working on the related fields.\nSupporting information\nS1 Fig. The five disordered molecular functions and their sub-level annotations collected\nfrom the DisProt database.\n(TIF)\nS2 Fig. The ROC curves of DMFpred for predicting five disordered molecular functions.\n(TIF)\nS1 Table. The hyper-parameters of PCLM in DMFpred.\n(DOCX)\nS2 Table. The thresholds used for binary results of DMFpred. The thresholds were selected\naccording to the highest MCC values in the validation set.\n(DOCX)\nS3 Table. The predictive performance of DMFpred on the TEST-2 independent test set.\n(DOCX)\nS1 Data. The molecular function benchmark dataset.\n(DOCX)\nS2 Data. The IDRs pre-training dataset.\n(DOCX)\nAuthor Contributions\nConceptualization: Yihe Pang, Bin Liu.\nData curation: Yihe Pang.\nFormal analysis: Yihe Pang.\nFunding acquisition: Bin Liu.\nInvestigation: Yihe Pang.\nPLOS COMP UTATIONAL  BIOLOGY\nDMFpr ed\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1010668 October 31, 2022 14 / 18\nMethodology: Yihe Pang.\nProject administration: Yihe Pang, Bin Liu.\nResources: Yihe Pang, Bin Liu.\nSoftware: Yihe Pang.\nSupervision: Bin Liu.\nValidation: Yihe Pang, Bin Liu.\nVisualization: Yihe Pang.\nWriting – original draft: Yihe Pang.\nWriting – review & editing: Yihe Pang, Bin Liu.\nReferences\n1. Xue B, Dunker AK, Uversky VN. Orderly order in protein intrinsic disorde r distribution: disorder in 3500\nproteome s from viruses and the three domains of life. J Biomol Struct Dyn. 2012; 30(2):137– 49. Epub\n2012/06/ 19. https://doi.or g/10.108 0/07391102.20 12.6751 45 PMID: 22702725.\n2. Peng Z, Yan J, Fan X, Mizianty MJ, Xue B, Wang K, et al. Exception ally abunda nt exceptions: compre-\nhensive characteriz ation of intrinsic disorder in all domains of life. Cell Mol Life Sci. 2015; 72(1):137 –51.\nEpub 2014/06/ 19. https://doi.or g/10.100 7/s00018-014- 1661-9 PMID: 249396 92.\n3. Dunker AK, Brown CJ, Lawson JD, Iakouc heva LM, Obradovic Z. Intrinsic disorder and protein function.\nBiochemi stry. 2002; 41(21):657 3–82. Epub 2002/05 /23. https://doi.or g/10.102 1/bi012159+ PMID:\n12022860.\n4. van der Lee R, Buljan M, Lang B, Weatheritt RJ, Daughdr ill GW, Dunker AK, et al. Classifi cation of\nintrinsica lly disordered regions and proteins . Chem Rev. 2014; 114(13):65 89–631. Epub 2014/04/3 0.\nhttps://doi.or g/10.102 1/cr400525m PMID: 24773235; PubMed Central PMCID: PMC409591 2.\n5. Tompa P. The interplay between structure and function in intrinsica lly unstructur ed proteins. FEBS Lett.\n2005; 579(15):33 46–54. Epub 2005/06/10. https:/ /doi.org/10.10 16/j.febsle t.2005.03 .072 PMID:\n15943980.\n6. Iakoucheva LM, Brown CJ, Lawson JD, Obradovic Z, Dunker AK. Intrinsic disorder in cell-sign aling and\ncancer-a ssociated proteins. J Mol Biol. 2002; 323(3):573 –84. Epub 2002/10/17. https://doi.or g/10.\n1016/s00 22-2836(0 2)00969-5 PMID: 12381310.\n7. Melo AM, Coraor J, Alpha-Cobb G, Elbaum -Garfinkle S, Nath A, Rhoades E. A functional role for intrin-\nsic disorder in the tau-tubulin complex. Proc Natl Acad Sci U S A. 2016; 113(50):14 336–41. Epub 2016/\n12/03. https:// doi.org/10.10 73/pnas.1 610137113 PMID: 27911791; PubMed Central PMCID:\nPMC516714 3.\n8. Dev KK, Hofele K, Barbieri S, Buchman VL, van der Putten H. Part II: alpha-syn uclein and its molecular\npathophy siological role in neurodegen erative disease. Neuropharm acology. 2003; 45(1):14–4 4. Epub\n2003/06/ 20. https://doi.or g/10.101 6/s0028-3908( 03)00140- 0 PMID: 128146 57.\n9. Cheng Y, LeGall T, Oldfield CJ, Mueller JP, Van YY, Rome ro P, et al. Rational drug design via intrinsi-\ncally disordered protein. Trends Biotechn ol. 2006; 24(10):435 –42. Epub 2006/08/01. https://doi.or g/10.\n1016/j.tib tech.2006.0 7.005 PMID: 1687689 3.\n10. Uversky VN. Intrinsically disordered proteins and novel strategies for drug discov ery. Expert Opin Drug\nDiscov. 2012; 7(6):475–8 8. Epub 2012/05 /09. https://doi.or g/10.151 7/17460441. 2012.686489 PMID:\n22559227.\n11. Tompa P. Intrinsical ly unstructur ed proteins. Trends Biochem Sci. 2002; 27(10):527 –33. Epub 2002/10/\n09. https://doi. org/10.1016/s 0968-0004( 02)02169-2 PMID: 12368089.\n12. Daughdr ill GW, Narayanasw ami P, Gilmore SH, Belczyk A, Brown CJ. Dynamic behavio r of an intrinsi-\ncally unstructured linker domain is conserve d in the face of negligible amino acid sequence conserva-\ntion. J Mol Evol. 2007; 65(3):277– 88. Epub 2007/08/ 28. https://doi.or g/10.100 7/s00239-007- 9011-2\nPMID: 177216 72.\n13. Uversky VN. Disorder in the lifetime of a protein. Intrinsical ly Disord Proteins. 2013; 1(1):e2678 2. Epub\n2013/11/ 07. https://doi.or g/10.416 1/idp.26782 PMID: 285160 24; PubMed Central PMCID:\nPMC542478 3.\nPLOS COMP UTATIONAL  BIOLOGY\nDMFpr ed\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1010668 October 31, 2022 15 / 18\n14. Daniels AJ, Williams RJ, Wright PE. The character of the stored molecules in chromaffin granules of the\nadrenal medulla: a nuclear magnetic resonanc e study. Neuroscience. 1978; 3(6):573–8 5. Epub 1978/\n01/01. https:// doi.org/10.10 16/0306 -4522(78)90022 -2 PMID: 692872\n15. Holt C. Unfolded phospho polypeptid es enable soft and hard tissues to coexist in the same organis m\nwith relative ease. Curr Opin Struct Biol. 2013; 23(3):420– 5. Epub 2013/04/30. https://d oi.org/10.101 6/j.\nsbi.2013.0 2.010 PMID: 236228 34.\n16. Galea CA, Wang Y, Sivakolun du SG, Kriwacki RW. Regulation of cell division by intrinsically unstruc-\ntured proteins: intrinsic flexibilit y, modularit y, and signaling conduits. Biochemistr y. 2008; 47(29):75 98–\n609. Epub 2008/07 /17. https://doi.or g/10.102 1/bi8006803 PMID: 186271 25; PubMed Central PMCID:\nPMC258077 5.\n17. Diella F, Haslam N, Chica C, Budd A, Michael S, Brown NP, et al. Understand ing eukaryotic linear\nmotifs and their role in cell signaling and regulation. Front Biosci. 2008; 13:6580–6 03. Epub 2008/05/\n30. https://doi. org/10.2741/3 175 PMID: 18508681.\n18. Young JC, Agashe VR, Siegers K, Hartl FU. Pathways of chaperone-med iated protein folding in the\ncytosol. Nat Rev Mol Cell Biol. 2004; 5(10):781– 91. Epub 2004/10/02. https://d oi.org/10.103 8/nrm149 2\nPMID: 154596 59.\n19. Necci M, Piovesan D, Predict ors C, DisProt C, Tosatto SCE. Critical assessmen t of protein intrinsic dis-\norder prediction. Nat Methods. 2021; 18(5):472– 81. Epub 2021/04/21 . https://doi.or g/10.1038/ s41592-\n021-01117- 3 PMID: 338758 85; PubMed Central PMCID: PMC810517 2.\n20. Piovesan D, Necci M, Escobedo N, Monzon AM, Hatos A, Micetic I, et al. MobiDB: intrinsi cally disor-\ndered proteins in 2021. Nucleic Acids Res. 2021; 49(D1):D36 1–D7. Epub 2020/11/2 6. https://doi.or g/\n10.1093/ nar/gkaa1058 PMID: 33237329; PubMed Central PMCID: PMC777901 8.\n21. Piovesan D, Tabaro F, Micetic I, Necci M, Quaglia F, Oldfield CJ, et al. DisProt 7.0: a major update of\nthe database of disordered proteins . Nucleic Acids Res. 2017; 45(D1):D21 9–D27. Epub 2016/12 /03.\nhttps://doi.or g/10.109 3/nar/gkw10 56 PMID: 278996 01; PubMed Central PMCID: PMC521054 4.\n22. Hatos A, Hajdu-S oltesz B, Monzon AM, Palopoli N, Alvarez L, Aykac-Fas B, et al. DisProt: intrinsic pro-\ntein disorder annotati on in 2020. Nucleic Acids Res. 2020; 48(D1):D26 9–D76. Epub 2019/11 /13.\nhttps://doi.or g/10.109 3/nar/gkz97 5 PMID: 31713636 ; PubMed Central PMCID: PMC714557 5.\n23. Meng F, Kurgan L. DFLpred: High-through put prediction of disorde red flexible linker regions in protein\nsequences. Bioinf ormatics. 2016; 32(12):i34 1–i50. Epub 2016/06 /17. https://doi.or g/10.109 3/\nbioinforma tics/btw280 PMID: 27307636; PubMed Central PMCID: PMC490836 4.\n24. Peng Z, Xing Q, Kurgan L. APOD: accurate sequence- based predictor of disordered flexible linkers.\nBioinformat ics. 2020; 36(Suppl_ 2):i754–i61 . Epub 2021/01/01. https://doi.or g/10.1093/ bioinformatic s/\nbtaa808 PMID: 33381830; PubMed Central PMCID: PMC777348 5.\n25. Sharma R, Bayarjarga l M, Tsunoda T, Patil A, Sharma A. MoRFPred- plus: Computationa l Identificati on\nof MoRFs in Protein Sequences using Physicoche mical Properties and HMM profiles. J Theor Biol.\n2018; 437:9–1 6. Epub 2017/10/19 . https://doi.or g/10.1016/ j.jtbi.2017.10.01 5 PMID: 29042212.\n26. Disfani FM, Hsu WL, Mizianty MJ, Oldfield CJ, Xue B, Dunker AK, et al. MoRFpred, a computation al\ntool for sequence- based predicti on and character ization of short disorder-to- order transitio ning binding\nregions in proteins . Bioinformatic s. 2012; 28(12):i75 –83. Epub 2012/06/13. https:// doi.org/10.10 93/\nbioinforma tics/bts209 PMID: 226897 82; PubMed Central PMCID: PMC337184 1.\n27. Hanson J, Litfin T, Paliwal K, Zhou Y. Identifying molecular recogni tion features in intrinsi cally disor-\ndered regions of proteins by transfer learning. Bioinformatics . 2020; 36(4):1107 –13. Epub 2019/09/11.\nhttps://doi.or g/10.109 3/bioinformat ics/btz691 PMID: 315041 93.\n28. Meszaros B, Simon I, Dosztanyi Z. Predic tion of protein binding regions in disordered proteins . PLoS\nComput Biol. 2009; 5(5):e1000 376. Epub 2009/05/0 5. https://doi.or g/10.1371/ journal.pcbi.1 000376\nPMID: 194125 30; PubMed Central PMCID: PMC267114 2.\n29. Meszaros B, Erdos G, Dosztanyi Z. IUPred2A: context-dep endent prediction of protei n disorder as a\nfunction of redox state and protein binding. Nucleic Acids Res. 2018; 46(W1) :W329–W3 7. Epub 2018/\n06/04. https:// doi.org/10.10 93/nar/gk y384 PMID: 29860432; PubMed Central PMCID: PMC603093 5.\n30. Malhis N, Jacobs on M, Gspone r J. MoRFchi bi SYSTEM: software tools for the identifica tion of MoRFs\nin protein sequences. Nucleic Acids Res. 2016; 44(W1):W488– 93. Epub 2016/05/14. https://doi.or g/10.\n1093/nar/gk w409 PMID: 27174932; PubMed Central PMCID: PMC498794 1.\n31. Sharma R, Sharma A, Raicar G, Tsunoda T, Patil A. OPAL+ : Length-Spe cific MoRF Prediction in Intrin-\nsically Disordere d Protein Sequences. Proteomics . 2019; 19(6):e180 0058. Epub 2018/10/17. https://\ndoi.org/10.10 02/pmic.201 800058 PMID: 303247 01.\n32. Sharma R, Raicar G, Tsunoda T, Patil A, Sharma A. OPAL: prediction of MoRF regions in intrinsica lly\ndisordered protein sequences . Bioinformatic s. 2018; 34(11):185 0–8. Epub 2018/01/24. https:// doi.org/\n10.1093/ bioinformatic s/bty032 PMID: 29360926.\nPLOS COMP UTATIONAL  BIOLOGY\nDMFpr ed\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1010668 October 31, 2022 16 / 18\n33. Peng Z, Kurgan L. High-thr oughput predicti on of RNA, DNA and protein binding regions mediated by\nintrinsic disorder. Nucleic Acids Res. 2015; 43(18):e12 1. Epub 2015/06 /26. https://doi.or g/10.109 3/nar/\ngkv585 PMID: 26109352; PubMed Central PMCID: PMC460529 1.\n34. Zhang F, Zhao B, Shi W, Li M, Kurgan L. DeepDI SOBind: accurate predictio n of RNA-, DNA- and pro-\ntein-binding intrinsica lly disordered residues with deep multi-task learning. Brief Bioinform. 2022; 23(1).\nEpub 2021/12/ 15. https://doi.or g/10.109 3/bib/bbab521 PMID: 349057 68.\n35. Katuwawala A, Zhao B, Kurgan L. DisoLipPr ed: Accurate predicti on of disordered lipid binding residues\nin protein sequences with deep recurrent networks and transfer learning. Bioinforma tics. 2021. Epub\n2021/09/ 07. https://doi.or g/10.109 3/bioinformatic s/btab640 PMID: 34487138.\n36. Wright PE, Dyson HJ. Intrinsical ly unstructur ed proteins: re-assessin g the protein structure-fun ction\nparadigm. J Mol Biol. 1999; 293(2):321 –31. Epub 1999/11/0 5. https://doi.or g/10.1006/ jmbi.1999.3 110\nPMID: 105502 12.\n37. Searls DB. The language of genes. Nature. 2002; 420(6912) :211–7. Epub 2002/11/15. https:// doi.org/\n10.1038/ nature01255 PMID: 12432405.\n38. Pan SJ, Yang Q. A survey on transfer learning. IEEE Transaction s on knowledge and data engine ering.\n2009; 22(10):134 5–59.\n39. Altschul SF, Madden TL, Schaffer AA, Zhang J, Zhang Z, Miller W, et al. Gapped BLAST and PSI-\nBLAST: a new generation of protein database search programs. Nucleic Acids Res. 1997; 25\n(17):3389– 402. Epub 1997/09 /01. https://doi.or g/10.109 3/nar/25.17. 3389 PMID: 9254694; PubMed\nCentral PMCID: PMC1 46917.\n40. Tang YJ, Pang YH, Liu B. IDP-Seq 2Seq: identifica tion of intrinsica lly disordered regions based on\nsequence to sequence learning. Bioinforma tics. 2021; 36(21):517 7–86. Epub 2020/07 /24. https://doi.\norg/10.1093/ bioinformatic s/btaa667 PMID: 32702119.\n41. Steinegger M, Meier M, Mirdita M, Vohringer H, Haunsberg er SJ, Soding J. HH-suite3 for fast remote\nhomolog y detection and deep protein annotatio n. BMC Bioinformatic s. 2019; 20(1):473. Epub 2019/09 /\n16. https://doi. org/10.1186/s 12859-019- 3019-7 PMID: 31521110; PubMed Central PMCID:\nPMC674470 0.\n42. Liu B, Li CC, Yan K. DeepSV M-fold: protein fold recognition by combining support vector machines and\npairwise sequence similarity scores generated by deep learning networks . Brief Bioinform. 2020; 21\n(5):1733–4 1. Epub 2019/10 /31. https://doi.or g/10.109 3/bib/bbz0 98 PMID: 31665221.\n43. Balakrishn an S, Kamisetty H, Carbonell JG, Lee SI, Langmea d CJ. Learning generative models for pro-\ntein fold families. Proteins . 2011; 79(4):1061 –78. Epub 2011/01 /27. https://doi.or g/10.100 2/prot.22934\nPMID: 212681 12.\n44. Ekeberg M, Lovkvist C, Lan Y, Weigt M, Aurell E. Improved contact prediction in proteins: using pseudo-\nlikelihoods to infer Potts models. Phys Rev E Stat Nonlin Soft Matter Phys. 2013; 87(1):0127 07. Epub\n2013/02/ 16. https://doi.or g/10.110 3/PhysRevE. 87.012707 PMID: 234103 59.\n45. Nair V, Hinton GE, editors. Rectified linear units improve restricted boltzman n machines. Proceedings\nof the 27th International Conferen ce on Internat ional Conference on Machine Learning; 2010.\n46. Li CC, Liu B. MotifCNN -fold: protein fold recognit ion based on fold-speci fic features extracted by motif-\nbased convolutio nal neural networks . Brief Bioinform. 2020; 21(6):213 3–41. Epub 2019/11/28. https://\ndoi.org/10.10 93/bib/bbz133 PMID: 31774907.\n47. Zhang J, Chen Q, Liu B. iDRBP_ MMC: Identifying DNA-Bind ing Proteins and RNA-Bind ing Proteins\nBased on Multi-La bel Learning Model and Motif-Ba sed Convo lutional Neural Network. J Mol Biol. 2020;\n432(22):58 60–75. Epub 2020/09 /14. https://doi. org/10.1016/j .jmb.2020 .09.008 PMID: 32920048.\n48. Pang Y, Liu B. SelfAT-Fold : protein fold recognition based on residue-based and motif-based self-atten -\ntion networks . IEEE/ACM Trans Comput Biol Bioinform . 2020;PP. Epub 2020/10 /23. https://doi.or g/10.\n1109/TCBB .2020.30 31888 PMID: 33090951.\n49. Kumar M, Gouw M, Michael S, Samano-San chez H, Pancsa R, Glavina J, et al. ELM-the eukaryotic lin-\near motif resource in 2020. Nucleic Acids Res. 2020; 48(D1):D29 6–D306. Epub 2019/11/05. https:/ /doi.\norg/10.1093/ nar/gkz10 30 PMID: 31680160; PubMed Central PMCID: PMC7145 657.\n50. Christofferse n P, Jacobs K. The Importance of the Loss Function in Option Valuation. CIRANO . 2003;\n72(2):291– 318.\n51. Kingma D, Ba J. Adam: A Method for Stochastic Optimizati on. Internat ional Conference on Learning\nRepresenta tions2015 . p. 1–11.\n52. Singh J, Hanso n J, Paliwal K, Zhou Y. RNA secondar y structure prediction using an ensemble of two-\ndimension al deep neural networks and transfer learning. Nat Commun. 2019; 10(1):5407 . Epub 2019/\n11/30. https:// doi.org/10.10 38/s4146 7-019-133 95-9 PMID: 31776342; PubMed Central PMCID:\nPMC688145 2.\nPLOS COMP UTATIONAL  BIOLOGY\nDMFpr ed\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1010668 October 31, 2022 17 / 18\n53. Zhang J, Yan K, Chen Q, Liu B. PreRBP-TL: Prediction of Species- Specific RNA-Bind ing Proteins\nBased on Transfer Learning. Bioinformat ics. 2022. Epub 2022/02/18. https:/ /doi.org/10.10 93/\nbioinforma tics/btac106 PMID: 351761 30.\n54. Zhang M-L, Zhou Z-H. A review on multi-label learning algorithms . IEEE transaction s on knowledge\nand data enginee ring. 2013; 26(8):1819 –37.\nPLOS COMP UTATIONAL  BIOLOGY\nDMFpr ed\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1010668 October 31, 2022 18 / 18",
  "topic": "Computational biology",
  "concepts": [
    {
      "name": "Computational biology",
      "score": 0.6535014510154724
    },
    {
      "name": "Function (biology)",
      "score": 0.5669431686401367
    },
    {
      "name": "Computer science",
      "score": 0.5481966137886047
    },
    {
      "name": "Intrinsically disordered proteins",
      "score": 0.505122721195221
    },
    {
      "name": "Protein function",
      "score": 0.47473224997520447
    },
    {
      "name": "Representation (politics)",
      "score": 0.4204118251800537
    },
    {
      "name": "Molecular dynamics",
      "score": 0.4114801287651062
    },
    {
      "name": "Biology",
      "score": 0.3609018325805664
    },
    {
      "name": "Biological system",
      "score": 0.3538600504398346
    },
    {
      "name": "Chemistry",
      "score": 0.18896359205245972
    },
    {
      "name": "Biochemistry",
      "score": 0.16657793521881104
    },
    {
      "name": "Genetics",
      "score": 0.14031478762626648
    },
    {
      "name": "Computational chemistry",
      "score": 0.11201593279838562
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}