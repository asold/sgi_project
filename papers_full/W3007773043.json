{
    "title": "Accessing Higher-level Representations in Sequential Transformers with Feedback Memory.",
    "url": "https://openalex.org/W3007773043",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2171998422",
            "name": "Angela Fan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2554317338",
            "name": "Thibaut Lavril",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2114720862",
            "name": "Edouard Grave",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2512114774",
            "name": "Armand Joulin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2035043562",
            "name": "Sainbayar Sukhbaatar",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963601622",
        "https://openalex.org/W2953061907",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2145107163",
        "https://openalex.org/W2946567085",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W2970900903",
        "https://openalex.org/W2949626814",
        "https://openalex.org/W1544827683",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2963631907"
    ],
    "abstract": "Transformers are feedforward networks that can process input tokens in parallel. While this parallelization makes them computationally efficient, it restricts the model from fully exploiting the sequential nature of the input - the representation at a given layer can only access representations from lower layers, rather than the higher level representations already built in previous time steps. In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. We demonstrate on a variety of benchmarks in language modeling, neural machine translation, summarization, and reinforcement learning that the increased representation capacity can improve over Transformer baselines.",
    "full_text": null
}