{
    "title": "Vision Transformer With Contrastive Learning for Remote Sensing Image Scene Classification",
    "url": "https://openalex.org/W4313461217",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4316172867",
            "name": "Meiqiao Bi",
            "affiliations": [
                "Chinese Academy of Sciences",
                "Aerospace Information Research Institute"
            ]
        },
        {
            "id": "https://openalex.org/A2105592602",
            "name": "Minghua Wang",
            "affiliations": [
                "Aerospace Information Research Institute",
                "Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2099417000",
            "name": "Zhi Li",
            "affiliations": [
                "Aerospace Information Research Institute",
                "Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2588029396",
            "name": "Danfeng Hong",
            "affiliations": [
                "Chinese Academy of Sciences",
                "Aerospace Information Research Institute"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4312981150",
        "https://openalex.org/W4295308382",
        "https://openalex.org/W2176673053",
        "https://openalex.org/W2598998899",
        "https://openalex.org/W3047443805",
        "https://openalex.org/W4312807131",
        "https://openalex.org/W1606858007",
        "https://openalex.org/W2621526417",
        "https://openalex.org/W2953303055",
        "https://openalex.org/W2914885528",
        "https://openalex.org/W2044465660",
        "https://openalex.org/W1980038761",
        "https://openalex.org/W2294802479",
        "https://openalex.org/W2963745697",
        "https://openalex.org/W6637373629",
        "https://openalex.org/W4246193833",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W4313506322",
        "https://openalex.org/W2620429297",
        "https://openalex.org/W3080181119",
        "https://openalex.org/W2786225488",
        "https://openalex.org/W2940939359",
        "https://openalex.org/W2974770574",
        "https://openalex.org/W2783165089",
        "https://openalex.org/W2899198451",
        "https://openalex.org/W4205127446",
        "https://openalex.org/W3048631361",
        "https://openalex.org/W4226285265",
        "https://openalex.org/W4285220262",
        "https://openalex.org/W2291068538",
        "https://openalex.org/W4206470192",
        "https://openalex.org/W2005112351",
        "https://openalex.org/W3022140654",
        "https://openalex.org/W3135445258",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2592962403",
        "https://openalex.org/W6774314701",
        "https://openalex.org/W6779977557",
        "https://openalex.org/W3035524453",
        "https://openalex.org/W6774670964",
        "https://openalex.org/W6776700526",
        "https://openalex.org/W3128592650",
        "https://openalex.org/W3205886311",
        "https://openalex.org/W4226291728",
        "https://openalex.org/W3201623325",
        "https://openalex.org/W4213253308",
        "https://openalex.org/W6798160016",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W6735013348",
        "https://openalex.org/W6797790494",
        "https://openalex.org/W2798991696",
        "https://openalex.org/W2963163009",
        "https://openalex.org/W4287116734",
        "https://openalex.org/W2963656735",
        "https://openalex.org/W3100399724",
        "https://openalex.org/W3103695279",
        "https://openalex.org/W3009561768",
        "https://openalex.org/W3103294617",
        "https://openalex.org/W2187089797",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W3102692100"
    ],
    "abstract": "Remote sensing images (RSIs) are characterized by complex spatial layouts and ground object structures. ViT can be a good choice for scene classification owing to the ability to capture long-range interactive information between patches of input images. However, due to the lack of some inductive biases inherent to CNNs, such as locality and translation equivariance, ViT cannot generalize well when trained on insufficient amounts of data. Compared with training ViT from scratch, transferring a large-scale pretrained one is more cost-efficient with better performance even when the target data are small scale. In addition, the cross-entropy (CE) loss is frequently utilized in scene classification yet has low robustness to noise labels and poor generalization performances for different scenes. In this article, a ViT-based model in combination with supervised contrastive learning (CL) is proposed, named ViT-CL. For CL, supervised contrastive (SupCon) loss, which is developed by extending the self-supervised contrastive approach to the fully supervised setting, can explore the label information of RSIs in embedding space and improve the robustness to common image corruption. In ViT-CL, a joint loss function that combines CE loss and SupCon loss is developed to prompt the model to learn more discriminative features. Also, a two-stage optimization framework is introduced to enhance the controllability of the optimization process of the ViT-CL model. Extensive experiments on the AID, NWPU-RESISC45, and UCM datasets verified the superior performance of ViT-CL, with the highest accuracies of 97.42&#x0025;, 94.54&#x0025;, and 99.76&#x0025; among all competing methods, respectively.",
    "full_text": "738 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nVision Transformer With Contrastive Learning for\nRemote Sensing Image Scene Classiﬁcation\nMeiqiao Bi, Minghua Wang , Member, IEEE, Zhi Li, and Danfeng Hong, Senior Member, IEEE\nAbstract—Remote sensing images (RSIs) are characterized by\ncomplex spatial layouts and ground object structures. ViT can be\na good choice for scene classiﬁcation owing to the ability to cap-\nture long-range interactive information between patches of input\nimages. However, due to the lack of some inductive biases inherent\nto CNNs, such as locality and translation equivariance, ViT cannot\ngeneralize well when trained on insufﬁcient amounts of data. Com-\npared with training ViT from scratch, transferring a large-scale\npretrained one is more cost-efﬁcient with better performance even\nwhen the target data are small scale. In addition, the cross-entropy\n(CE) loss is frequently utilized in scene classiﬁcation yet has low\nrobustness to noise labels and poor generalization performances\nfor different scenes. In this article, a ViT-based model in combina-\ntion with supervised contrastive learning (CL) is proposed, named\nViT-CL. For CL, supervised contrastive (SupCon) loss, which is\ndeveloped by extending the self-supervised contrastive approach\nto the fully supervised setting, can explore the label information of\nRSIs in embedding space and improve the robustness to common\nimage corruption. In ViT-CL, a joint loss function that combines CE\nloss and SupCon loss is developed to prompt the model to learn more\ndiscriminative features. Also, a two-stage optimization framework\nis introduced to enhance the controllability of the optimization\nprocess of the ViT-CL model. Extensive experiments on the AID,\nNWPU-RESISC45, and UCM datasets veriﬁed the superior perfor-\nmance of ViT-CL, with the highest accuracies of 97.42%, 94.54%,\nand 99.76% among all competing methods, respectively.\nIndex Terms —Joint loss function, remote sensing, scene\nclassiﬁcation, supervised contrastive (SupCon) loss, vision\ntransformer.\nI. INTRODUCTION\nT\nHANKS to the rapid development of Earth observation\n(EO) technology, a massive amount of remote sensing (RS)\nimages with a high spatial resolution (HSR) are being generated\nevery day. Interpreting these RS images, which contain sufﬁcient\nland-cover/land-use information, has practical signiﬁcance in\nmany ﬁelds, such as object detection [1], land planning [2], and\ntrafﬁc management [3]. Among the many image interpretation\ntasks, RS images have received increasing attention. RS images\naim to allocate a semantic label to the input RS image, where\nManuscript received 24 November 2022; revised 10 December 2022; accepted\n16 December 2022. Date of publication 20 December 2022; date of current ver-\nsion 29 December 2022. This work was supported in part by the National Natural\nScience Foundation of China under Grant 42271350 and Grant 62201552 and in\npart by the China Postdoctoral Science Foundation under Grant 2022M713223.\n(Corresponding author: Minghua Wang.)\nThe authors are with the Aerospace Information Research Institute, Chinese\nAcademy of Sciences, Beijing 100094, China (e-mail: meiqiaobi2022@163.\ncom; minghuawang1993@163.com; lizhi21@mails.ucas.ac.cn; hongdf@\naircas.ac.cn).\nDigital Object Identiﬁer 10.1109/JSTARS.2022.3230835\nthe label is from a predeﬁned label set that reﬁnes the content of\nthe RS images [4], [5], [6].\nScene classiﬁcation is done in feature space so that the\ndescription ability of features extracted by the model directly\naffects the classiﬁcation performance. In the beginning, scene\nclassiﬁcation methods are mainly based on hand-crafted fea-\ntures, which can be divided into low-level and high-level fea-\ntures. Low-level features [7], [8], [9] are usually constructed\nby visual attributes such as color [10], texture [11], and shape.\nAnd mid-level features are generated by encoding the low-\nlevel features through some encoding methods, such as bag-\nof-visual-words (BoVW) [12], vectors of locally aggregated\ndescriptors (VLAD) [13], and improved Fisher kernel (IFK) [7].\nThese hand-crafted features heavily depend on the expertise\nof designers, and the capacity of information expression is\nlimited.\nWith the rise of deep learning, data-driven feature extraction\nmethods that do not rely on prior knowledge are born. Especially\nin supervised deep learning, models learn deep features by train-\ning themselves on a large number of the labeled dataset so that\nthey can fully exploit category information to extract high-level\nsemantic features. Among them, convolutional neural networks\n(CNNs) have shown powerful capability of feature learning\nin visual applications. Several classical CNNs have been pro-\nposed, such as AlexNet [14], VGGNet [15], GoogLeNet [16],\nResNet [17], and U-Net [18]. Concerning scene classiﬁcation,\nCNN-based methods can be divided into three branches depend-\ning on how they are used: employing a pretrained model as a\nfeature extractor, ﬁne-tuning a pretrained model, and training a\nnew model from scratch.\nIn the ﬁrst branch, pretrained CNNs are considered feature\nextractors, and then, the resulting features are fused or combined\nto capture more visual information. Studies [19] use different\npretrained CNNs to extract vision features and fuse the result\nfeatures. The results show that fused features are more dis-\ncriminated against. In [20], the CNN model is used to extract\nmultilayer feature maps, and these feature maps are combined by\ncalculating their covariance matrix of them after being stacked.\nFinally, the result covariance matrices are used for classiﬁcation.\nThe aforementioned models demonstrate that CNNs have good\ngeneralization capability for scene classiﬁcation.\nIn contrast with using pretrained CNNs as extractors, ﬁne-\ntuning pretrained CNNs on target datasets, which can lead to an\nend-to-end model, is more straightforward and more effective.\nIn addition, when training data are insufﬁcient, ﬁne tuning\ntakes precedence over training from scratch. The emphasis of\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nBI et al.: VISION TRANSFORMER WITH CONTRASTIVE LEARNING 739\nFig. 1. Coexistence of multiple ground objects. Multiple ground objects\nrelated to the “School” scene are distributed over different locations of the RSI.\nThe image is from the AID dataset [32].\noptimizations in the second branch is placed on ﬁne adjusting\nthe networks [21], [22], [23] and loss functions [24], [25].\nAlthough ﬁne tuning pretrained CNNs can achieve effective\nclassiﬁcation performance, pretrained CNNs still have some\nlimitations. Due to the gap between the nature dataset and the\nRSI dataset, features learned by the model trained on the nature\ndataset are not perfectly suitable for the RSI dataset. Moreover,\nmodifying the pretrained model is not that convenient. Many\nstudies of the third branch focused on either improving the CNN\nstructure [26], [27], [28], [29] or constructing a hybrid model\nframework [30], [31] according to the characteristics of remote\nsensing datasets.\nOverall, the CNN is a multilayer structure, where the con-\nvolutional layer plays a prominent role in extracting features\nfrom images. Thanks to the convolutional operations, the model\ncan learn the local spatial information of the input images.\nAnd though progressively expanding the receptive ﬁeld of the\nconvolutional kernels in each layer, it can acquire the features of\na global view. Sacking multiple convolutional layers can boost\nthe classiﬁcation performance signiﬁcantly. However, the CNN\ncannot capture long-range relationships limited by the local\nreceptive ﬁeld.\nDue to the complex spatial distribution of ground objects\nand the bird’s eye view of RS imaging equipment, it is very\ngeneric for multiple ground objects to coexist in one single\nRS image [33], [34]. In the RS image shown in Fig. 1, which\nis of the “school” scene, there are multiple ground objects, a\nbaseball ﬁeld, a tennis court, a playground, roads, and so on.\nFurthermore, these objects are distributed in all directions of the\nimage. The coexistence and dispersed distribution of multiple\nground objects bring challenges to scene classiﬁcation. So cap-\nturing global long-range interactions for these ground objects\nhas vital practical signiﬁcance in scene classiﬁcation. Besides\nthe CNN, transformer [35] is another deep learning structure that\nhas taken off in the natural language processing (NLP) domain.\nThe transformer beneﬁting from the self-attention mechanism,\ncan capture long-range interactions on input sequence data and\nlearn a global representation. Encouraged by the success of the\ntransformer in NLP, Dosovitskiy et al. [36] have extended the\nstandard transformer structure to visual applications and pro-\nposed the ViT model that demonstrates the enormous potential\nfor image classiﬁcation.\nAlthough ViT has shown excellent feature learning ability,\nits performance on RS images has not yet reached saturation.\nFurther improving its performance without increasing the pa-\nrameter scale or integrating with additional depth structure is\npossible. In addition to the characteristics of the coexistence of\nmultiple ground objects mentioned above, intraclass diversity\n(shown in Fig. 2) and interclass similarity (shown in Fig. 3) are\nalso two nonnegligible challenges. In recent years, contrastive\nlearning has received considerable attention due to its great\npotential for visual representation learning ability. Since 2019,\nresearch on comparative learning (CL) has developed rapidly,\nresulting in many excellent methods, such as SimCLR [38],\nSimCLR V2 [39], MoCo [40], and MoCo V2 [41]. Among them,\nthe supervised contrastive (SupCon) loss, a batch contrastive\napproach for a supervised setting, has the intrinsic ability to\nperform hard positive/negative mining [42]. So, it is possible\nto employ SupCon loss to help the ViT model to learn more\ndiscriminative features.\nGiven the appealing properties of ViT and CL, in this article,\na novel two-stage end-to-end framework for the scene classi-\nﬁcation is proposed, named ViT-CL. ViT-CL aims to combine\nthe advantages of the transformer structure and the principle\nof contrastive learning to improve the performance of scene\nclassiﬁcation. First of all, considering that the scale of RS image\ndatasets is hardly sufﬁcient to train ViT models from scratch,\ntransferring a large-scale pretrained ViT model to the target\ndataset, which can help ViT surpass inductive bias, is preferred.\nSecond, as a combination of SupCon loss and CE loss, a joint\nloss is proposed to ﬁne tune the pretrained ViT model. In this\nway, the two loss functions complement each other, forcing the\nmodel to learn more discriminating high-level semantic features\nand further making the model more robust. Finally, considering\nViT is hard to optimize and sensitive to hyperparameters, we\ndevelop a two-stage optimization. In the ﬁrst stage, only CE\nloss is adopted to ﬁne tune the pretrained ViT model on the\ntarget dataset. In the second stage, the proposed joint loss is\nutilized to ﬁne tune the model produced in the ﬁrst stage. After\nthe two-stage ﬁne tuning, the optimized model is obtained, but\nonly the cross-entropy loss part of the model is retained for the\nfollowing inference.\nII. RELATED WORK\nIn the last two years, some studies have begun to explore\nhow ViT performs in RS images. Bazi et al. [43] introduced\nthe ViT model into the RS images and improved the classiﬁca-\ntion accuracy through data augmentation such as CutMix and\nCutout. Also, they proved that the model performance could be\nmaintained even if half of the layers were pruned to compress\nthe network. Then, Bashmal et al. [44] proposed the data-\nefﬁcient image transformers (DeiT), a ViT-based model trained\n740 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nFig. 2. Intraclass diversity.\nFig. 3. Interclass similarity. (a) Similarity between scene “freeway” and “runway.” (b) Similarity between scene “railway station” and “industriala r e a . ”T h e s e\nimages are from the NWPU-RESISC45 dataset [37].\nby knowledge distillation with fewer data, and proved that the\nperformance of ViT was superior to the CNN-based method on\nthe remote sensing datasets AID and NWPU-RESISC. In [45],\nSCViT is proposed to overcome the disadvantage that the origi-\nnal model can only capture global spatial features. By improving\nthe structure of ViT, the model not only considers the detailed\ngeometric information of high spatial resolution images but also\nconsiders the contribution of different channels of the class\ntoken.\nIn addition, as the advantages of convolution structure and\ntransformer structure complement each other, some studies have\nexplored ways to combine these two network structures. Deng\net al. [46] designed a joint loss function to build the joint\nframework CTNet. In this framework, the ViT model is used\nto capture semantic features, while the CNN model is used to\nextract local structure information. In [47], the advantages of\nthe two models are integrated without improving the computa-\ntional complexity by knowledge distillation, in which the ViT\nis worked as a teacher to guide the student model ResNet18.\nBesides classifying tasks, this article also proves that this method\nhas good generalization ability for different tasks.\nThe remainder of this article is organized as follows: Sec-\ntion III introduces ViT and the supervised contrastive loss, then\ndescribes the proposed method ViT-CL in detail. Section IV\ncontains both contrast and ablation experiments. The former\ncompares our models with several classical CNN models and\nViT-based methods on three well-known datasets, and the latter\nanalyzes how the optimized model works. Finally, Section V\nconcludes this article.\nIII. PROPOSED METHOD\nLet D = {Xi,yi}r\ni=1 denote an SRI dataset of sizer, where\nXi represents the ith image and yi is its corresponding cat-\negory label. Xi ∈ Rh×w×c, where h, w, and c represent the\nheight, the width, and the number of channels, respectively.\nyi ∈{ 1,2,...,m }, where m is the predetermined number of\ncategories.\nA. Vision Transformer\nA vision transformer is proposed to apply the vanilla trans-\nformer to the image task. The main goal is to generalize it to\nvisual applications without integrating any data-speciﬁc archi-\ntecture. ViT only retains the encoder module of the standard\ntransformer, and the complete end-to-end architecture is shown\nin Fig. 4.\nFirst, the input image is subdivided into nonoverlapping 2-D\npatches with dimensionsp×p×c before being passed to the\ntransformer encoder to adapt the standard transformer structure.\nThe path sizep is usually set to 16 or 32, and a smaller patch\nsize will lead to a longer sequence and vice versa. Then, then\n2-D patches are ﬂattened and passed to a liner layer to generate\na patch sequenceP ∈ Rn×(p2·c), wheren = h×w\np2 is the length\nof P. In the liner layer, a learnable matrixE ∈ R(p2·c)×d is\nutilized to embed these patches into ad-dimensional space. After\nthat, like most classiﬁcation tasks with transformer structure, the\nembedded patch sequence is concatenated with a learnable clas-\nsiﬁcation tokenP0. Finally, the patch’s spatial arrangementEpos,\nBI et al.: VISION TRANSFORMER WITH CONTRASTIVE LEARNING 741\nFig. 4. Illustration of the proposed ViT architecture. (a) Main architecture of the model. (b) Transformers encoder module. (c) Multihead self-attention (MSA).\n(d) Self-attention head (SA).\nwhich helps the transformer to distinguish them, is encoded and\nadded to the embedded patch sequence to obtain the embedding\nsequence Z(0). The aforementioned process is formulated as\nfollows:\nZ(0) = {P0; P1E; P2E; ··· ; PmE; }+ Epos\nE ∈ R(p2·c)×d,Epos ∈ R(n+1)×d. (1)\nNext, the embedding sequenceZ(0) is entered into the trans-\nformer encoder that containsL blocks. As shown in Fig. 4(b),\nthere are two main subcomponents in each block: multihead\nself-attention (MSA) [see (2)] and multilayer perception (MLP)\n[see (3)]. Before entering these two components separately,\nthe input needs to be preceded by a normalization layer (LN),\nwhich can stabilize the gradient of the loss to the input during\nbackpropagation. And both the output of the two subcomponents\nemploy residual skip connections to obtain a result as the input\nof the next subcomponent. The calculation process is as follows:\nZ(l)′\n= MSA\n(\nLN\n(\nZ(l−1)\n))\n+ Z(l−1),l =1 ...L (2)\nZ(l) = MSA\n(\nLN\n(\nZ(l)′))\n+ Z(l),′l =1 ...L. (3)\nHere, notice that the output of theLth layer ZL is the ﬁnal\nresult of the encoder. For classiﬁcation, the ﬁrst token ofZL\ncan be regarded as the ﬁnal feature representationf of an input\nimage after anLN processing. The calculation is as follows:\nf = LN\n(\nZ(L)\n0\n)\n. (4)\nThen, f is passed into an MLP head, which is composed of\na full connection layer (FC) and the softmax loss function to\npredict the class label\ny = softmax (FC(f)) . (5)\nThe construction of MSA, the core of the transformer en-\ncoder, is shown in Fig. 4(c). Attention can be understood as\nthe weight of interaction between tokens, and self-attention\nmeans these tokens belong to one single sequence. For each\ntoken in the sequence Z, ﬁrst, calculate the attention scores\nbetween itself and all the tokens ofZ. And second, calculate\nthe sum over all token embeddings weighted by these atten-\ntion scores to obtain a new embedding for the current token.\nBefore calculating self-attention, the sequence Z is mapped\nto three different sequencesQ ∈ R(n+1)×dQ, K ∈ R(n+1)×dK ,\nand V ∈ R(n+1)×dV by multiplying a learned matrixMQKV ,\nwhere Q, K, andV represent query, key, and value, anddK, dQ,\nand dV are their dimensions. In theory, it just requiresdK = dQ,\nand for convenience, there aredK = dQ = dV . The formula is\nas follows:\n[Q,K,V ]= ZMQKV ,MQKV ∈ Rd×3dK. (6)\nThen, it comes to the SA block, shown in Fig. 4(c). The\ndot production Q·KT is calculated to measure the pairwise\nsimilarity between tokens in sequence Z. And to alleviate the\nproblem of vanishing gradient, the result needs to be divided by√\ndK. After a softmax operation, the ﬁnal scaled dot attention\nis obtained. The entire procedure is as follows:\nSA(Q,K,V )= softmax\n(QKT\n√dK\n)\nV. (7)\nSuppose that the number of heads ish, and the MSA block\ncomputes the scaled dot attentionh times separately, using (7)\nwith hdifferent values forQ, K, andV, respectively. These result\nhSA values will be concatenated, and then, passed to a linear\nlayer with parameterW0 to ensure that the dimensions of the\ninput and output of each MSA block stay the same. The formula\nis as follows:\nMSA = Concat (SA1,SA2,...,SA h) W0\n742 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nTABLE I\nPARAMETER STATISTICS FOR THESMALL BASE AND LARGE VARIANTS OF\nVISION TRANSFORMER\nW0 ∈ R(h·dK)×d. (8)\nWhen it comes to the MLP block, there are two dense layers\nand a GeLU activation in between. It is too simple to expand on.\nFrom the calculation process of MSA, it can be seen that this\nmechanism can capture long-distance dependencies between\ntokens. The generated feature representation, not only contains\nthe information of all patches but also their higher order spatial\ninteraction information. However, as mentioned previously, the\ntransformer lacks some inductive biases, so it cannot generalize\nwell when there is no adequate data. The authors in [48] has\nanalyzed the effects of pretraining data scale, data augmentation,\nmodel size, and compute budget on the performance of the\nViT model. And they proved that for most practical purposes,\ncompared with training a ViT model from scratch, ﬁne tuning\na large-scale pretrained ViT model on the target dataset is both\nmore cost effective and can produce better results. The author\nalso gives some suggestions on how to choose a pretraining\nmodel.\n1) The larger the pretrain dataset, the more generic the ob-\ntained model, and the larger the model, the longer the\ninference time;\n2) The validation score obtained in a pretrain stage can be the\ndirect reference index. And there is no need to transfer all\navailable pretrained models to the target data and choose\nthe model by comparing veriﬁcation scores in a ﬁne-tuning\nstage.\nThe pretrained ViT model mainly contains three versions with\ndifferent scales of parameters: small ViT, base ViT, and large\nViT. And each version usually owns two different patch sizes,\n16 and 32. See Table I for some vital parameters of the three\nversion models.\nFollowing the aforementioned suggestions and the rec-\nommendations of the ofﬁcial ViT documents, 1 The model\nB/16_21 k, which means base ViT with patch size 16, is cho-\nsen as the backbone network. More speciﬁcally, the model is\npretrained on a large-scale dataset ImageNet-21 k (including\n13 M images) [49], applying varying amounts of AugReg strate-\ngies [48].\nB. Supervised Contrastive Learning\nMost classiﬁcation tasks usually employ CE loss as the ob-\njective function. But some studies have shown that this loss has\ndrawbacks such as not being robust to noisy labels [49] and\nmay produce poor margins [50], which can reduce the model’s\n1https://github.com/google-research/vision_transformer\ngeneralization ability and further affect the classiﬁcation ac-\ncuracy. As mentioned in the introduction section, RS images\nare characterized by big intraclass diversity and high interclass\nsimilarity. That is to say, RS images of the same class may be very\ndifferent (shown in Fig. 2), while RS images of different classes\nmay be very similar (shown in Fig. 3). So, poor margins, which\nmeans CE loss does not explicitly encourage discriminative\nlearning of features, will be a loss for scene classiﬁcation. How-\never, SupCon loss can promote model learning discriminative\nfeature representations by pulling together the clusters of similar\nsamples in feature space while pushing apart the clusters of\ndissimilar samples. It can be employed to compensate for the\ndrawbacks of CE loss.\nSupCon loss is produced by extending the self-supervised\ncontrast learning [38] to the fully supervised setting [42]. In a\nsupervised setting, the loss’ selection criteria of positive samples\nchanged to “whether it belongs to the same class,” from that\n“whether it is from the same picture” in a self-supervised setting.\nThus, the number of positive sample pairs in the comparison loss\nis expanded. It has been proved that this change can encourage\nthe model better depict the intraclass similarity. For each anchor,\nthe SupCon loss ﬁrst calculates the similarity scores between it\nand all the other positive samples, and then, weighted sum these\nscores. The calculation formula is as follows:\nlsup\ni = −1\n|p(i)|\n∑\np∈P(i)\nlog exp\n(\nfi ·fp/τ\n)\n∑\na∈A(i) exp (fi ·fa/τ) . (9)\nHere i ∈ I ≡{ 1 ··· N} is the index of the anchor.A(i) ≡\nI \\{i} represents the overall sample set besides the samplei,\nand P(i) ≡{ p ∈ A(i):˜yp =˜yi} is the positive sample set, in\nwhich the samples have the same label as the anchor. The symbol\n“·” represents the inner (dot) product andτ ∈ R+ is the tem-\nperature parameter. It can be seen from (9) that the contrastive\ndenominator contains the summation over negative samples, and\nthis form improves the model’s ability to discriminate between\nsignal and noise (negative samples). Overall, the signiﬁcance of\nSupCon loss lies in narrowing the distance between the samples\nfrom the same class in the feature space, while widening the\ndistance between samples from different classes. However, for\neach anchor, only positive samples in the batch contribute to the\nnumerator of (9), so the batch size should be larger than the num-\nber of classes to ensure that there are enough positive samples\nin the batch. Meanwhile, a larger batch size call also guarantees\nenough negatives to form a sharp contrast with positive pairs.\nC. ViT-CL\nIn this article, a method named ViT-CL is proposed to combine\nthe advantages of SupCon loss and ViT. ViT-CL utilizes ViT as\na backbone network, and then, optimizes the backbone network\nby a two-stage optimizing framework with a joint loss. The\nframework of ViT-CL is shown in Fig. 5.\nAfter encoding by the ViT, each imageXi can obtain its em-\nbedding featurefi. In the proposed framework, image features of\none input batch will be passed to a joint loss, which is constituted\nby the CE loss and SupCon loss. In Fig. 5, each loss function is\nvisualized as a task.\nBI et al.: VISION TRANSFORMER WITH CONTRASTIVE LEARNING 743\nFig. 5. Framework of the proposed ViT-CL.\nThe ﬁrst task is the classiﬁcation task, corresponding to the\nleft one of the two tasks in Fig. 5. Here, the classiﬁer structure\nof ViT is directly applied: First, featurefi is mapped to a new\nfeature space for classiﬁcation by the MLP head, and then, the\nCE loss is calculated. The calculation formulas are as follows:\nfCE\ni = FC (fi) (10)\nlCE\ni = LCE\n(\nsoftmax\n(\nfCE\ni\n)\n,yi\n)\n. (11)\nThe second task is supervised contrast learning, correspond-\ning to the right one of the two tasks in Fig. 5. Referring to\nthe contrast learning framework [38], [42], a project network\nproj(·), which plays the same role as the MLP head in the\nclassiﬁcation task, is introduced. Some studies on contrast learn-\ning have shown that the project network is necessary and can\nhelp to improve the model’s performance [38]. Here, proj(·)\nis instantiated as a two-layer MLP, whose hidden layer size is\n2048 and output layer size is 128. Formulaic the calculations as\nfollows:\nfSupCon\ni = FC (ReLU ((FC))) (12)\nlSupCon\ni = −1\n|p(i)|\n∑\np∈P(i)\nlog exp (fi ·fp/τ)∑\na∈A(i) exp (fi ·fa/τ) . (13)\nIt should be pointed out that in the actual minibatch opti-\nmization process, the positive sample setP(i) of each anchor is\nlimited in the batch where the anchor is, so the optimization\nparameter batch size would impact the performance of the\nmodel. Finally, the joint loss of one input batch is calculated\nas follows:\nL =\nn∑\ni=1\n(\nlCE\ni + λlSupCon\ni\n)\n(14)\nwhere λ acts as a tradeoff between these two losses, which\nneeds to be judiciously tuned to control the distinctiveness\nof learned features. Along with ViT’s sensitivity to optimizer\nhyperparameters [51], tuning these hyperparameters including\nλ is time consuming. Instead, a simpler but effective strategy\nTABLE II\nPARAMETER STATISTICS FOR THESMALL BASE AND LARGE VARIANTS OF THE\nVISION TRANSFORMER\nis proposed to tackle this problem: a two-stage optimization\nmethod. In the ﬁrst stage, the pretrained ViT model selected\nin Section III-A is initially ﬁne tuned only by the CE loss on\nthe target RSI dataset. In the second stage, the result ﬁne-tuned\nmodel of the ﬁrst stage is ﬁne tuned by the joint loss again.\nCorresponding to the framework shown in Fig. 5, the model is\nﬁne tuned using only the classiﬁcation part in the ﬁrst stage.\nThen, in the second stage, both the classiﬁcation part and the\nsupervised contrastive learning part are used. After the two-stage\njoin ﬁne tuning, only the classiﬁcation part of the framework is\nreserved to complete inference work.\nIV . EXPERIMENTAL RESULTS\nA. Experimental Setup\n1) Datasets Description: In our experiments, three public\nremote-sensing datasets are utilized to evaluate the ViT-CL:\nAerial Image Dataset (AID) [32], Northwestern Polytechnical\nUniversity Dataset (NWPU-RESISC45) [37], and UC-Merced\nLand Use Dataset (UCM) [12], the detail information of the three\ndatasets are displayed in Table II. Among them, the NWPU-\nRESISC45 dataset and UCM dataset are more challenging than\nthe AID dataset.\n2) Hardware and Software Environment:All subsequent ex-\nperiments are conducted on a personal computer, and the detailed\ncomputing environment is shown in Table III.\n744 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nTABLE III\nPARAMETER STATISTICS FOR THESMALL BASE AND LARGE VARIANTS OF THE\nVISION TRANSFORMER\n3) Parameter Optimization Setup:For comparison purposes,\nthe overall accuracy (OA) is employed to evaluate the perfor-\nmance of different classic methods, which indicates the per-\ncentage of correctly classiﬁed images in the total number of\nimages. When training or ﬁne tuning models, 50% and 80%\nof the UCM dataset, 20% and 50% of the AID dataset, and\n10% and 20% of the NWPU-RESISC45 dataset are randomly\nselected for training, respectively. One thing to note is that\nwhen using two-stage ViT-CL, the division of the train set\n(referring to the samples assigned to the set rather than the\nsample proportion) needs to be consistent in both stages. The\npretrained ViT model used as the backbone of our ViT-CL is\nselected as described in Section III-A, a B/16 version pretrained\non Imagenet-21 k with AugReg strategies. It can be downloaded\nfrom https://storage.googleapis.com/vit_models/augreg/.\nIn the optimization stage, adaptive moment estimation\n(Adam) is introduced to update the parameters of all methods\nand the learning rate (LR) is set to 0.0001. Also, stepLR is used\nto control the LR, whose step_size is set to 20 and gamma is set\nto 0.9. That is to say,LRis multiplied by a factor of 0.9 every 20\nepochs. All the methods are ﬁne tuned 100 epochs and for each\nepoch, batch_size is set to 128 limited by the memory of the\nGPU. Besides, the input images are resized to 256×256 pixels.\nIn addition, with respect to the parameters in joint loss,λ is\nset to 0.2 through many experiments, and temperatureτ is set to\n0.07 as recommended in most comparative learning papers [40],\n[41], [52].\nB. Comparison With Some Classic Methods\nwe compare our method with ﬁve classic CNN-based meth-\nods, one traditional ViT model, and two ViT-based improved\nmodels, which are as follows:\n1) Fine-tune ResNet-50 [17];\n2) Fine-tune AlexNet [14];\n3) Fine-tune VGGNet-16 [15];\n4) Fine-tune GoogLeNet [16];\n5) Fine-tune MobileNe_V2 [53];\n6) V16_21k [43];\n7) SCViT [45];\n8) ET-GSNet [47].\nTable IV shows detailed comparisons between ViT-CL and\nother models on three datasets, AID, NWPU-RESISC45, and\nUCM. It can be seen that the proposed two-stage joint ﬁne-\ntuning method ViT-CL has an obviously higher OA, compared\nwith both these classical CNN-based models and the improved\nViT-based models.\nFurthermore, the confusion matrices (CMs) of the ViT mod-\nels’ prediction on the two more challenging datasets AID and\nNWPU-RESISC45 are calculated to prove the improvement of\nthe ViT-CL model. The two CMs are shown in Fig. 6; (a) is\nfor AID with a training ratio of 50% and (b) is for NWPU-\nRESISC45 with a training ratio 20%. It can be seen from the CMs\nthat the proposed method performs well on the two datasets.\nIn the dataset AID, where the total number of categories is\n30, the class number owning a greater than 90% accuracy is\nas high as 29. And among them, 25 have an accuracy greater\nthan 95%. Besides, even the worst accuracy can reach 88%.\nIn detail, the model achieves excellent results in categories with\nhigh intraclass diversity, such as “Airport” (97%), “Commercial”\n(97%), “Railway Station” (98%), “Chruch” (99%), “Farmland”\n(99%), and “Mountain” (100%) (some results is shown in Table\nV). Also, it performs well in categories with high interclass\nsimilarity. And these highly similar pairs of categories include\n“BareLand” (99%) and “Desert” (99%), “Park” (94%) and\n“Resort” (92%), and “Playground” (98%) and “Stadium” (99%)\n(some predict results is shown in Table VI).\nIn dataset NWPU-RESISC45, there are 40 categories, whose\naccuracy is over 90% out of 45, and 26 categories are above\n95%. The ﬁve categories with the lowest accuracies are “Palace”\n(76%), “Commercial Area” (88%), “Dense Residential” (88%),\n“Medium Residential” (89%), and “Church” (89%). Except for\nthe category “Palace” in which the model performs worst (76%),\nthe accuracy is close to 90%. For the two categories “Church”\nand “Railway Station,” which have high intraclass diversity in\nthis dataset, the proposed model achieves accuracies of 89% and\n93%, respectively (some predicted results of these categories are\nshown in Table VII). And for category pairs with a high interclass\nsimilarity: “Freeway” with “Runway,” “Industrial Area” with\n“Railway Station,” and “Railway Station” with “stadium,” the\naccuracies achieved by the model are 92%, 95%, 90%, 95%,\n95%, and 97%, respectively (some predict results is shown in\nTable VIII) .\nMoreover, from images in Tables V and VII, it can be seen that\nimages belonging to the same scene can appear very different,\nand ViT-CL has the ability to capture diversity. When it comes\nto Tables VI and VIII, images of different scenes may look very\nsimilar or contain the same objects, ViT-CL also has the ability to\ndistinguish between these scenarios. The aforementioned results\nfully show that ViT-CL can well distinguish both intraclass\ndiversity and interclass similarity.\nC. Ablation Study and Analysis\nThis article also conducted experiments on the variants of\nthe model to illustrate the effectiveness of the two-stage joint\noptimization, including the following.\n1) Fine-tune B/16_21 k: Fine tune the pretrained ViT model\nonce, utilizing and only utilizing CE loss for classiﬁcation.\n2) One-stage ViT-CL: Fine tune the pretrained ViT model\nonce utilizing joint loss and retain the classiﬁcation part\nfor classiﬁcation.\nBI et al.: VISION TRANSFORMER WITH CONTRASTIVE LEARNING 745\nTABLE IV\nCLASSIFICATION ACCURACIES OFDIFFERENT METHODS ON THEAID, NWPU-RESISC45,AND UCM DATASETS(%)\nFig. 6. CMs of ViT-CL on AID dataset and NWPU-RESISC45 dataset. (a) CM of ViT-CL on AID dataset with train percent 50%. (b) CM of ViT-CL on\nNWPU-RESISC45 dataset with train percent 20%.\nTABLE V\nPART PREDICT RESULTS OFDATASETAID, CATEGORIES ARECHOSEN FOR THEIR BIG INTRACLASS DIVERSITY\n\n746 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\nTABLE VI\nPART PREDICT RESULTS OFDATASETNWPU-RESISC45, CATEGORIES ARECHOSEN FOR THEIR BIG INTRACLASS DIVERSITY\nTABLE VII\nPART PREDICT RESULTS OFDATASETAID, CATEGORIES ARECHOSEN FOR THEIR HIGH INTERCLASS SIMILARITY\nTABLE VIII\nSOME PREDICT RESULT OFDATASETNWPU-RESISC45, CATEGORIES ARECHOSEN FOR THEIR HIGH INTERCLASS SIMILARITY\nTABLE IX\nCLASSIFICATION ACCURACIES OFTHREE VITB ASED METHOD ON THEAID,\nNWPU-RESISC45, AND UCM DATASETS(%)\n3) ViT-CL: Our proposed two-stage joint ﬁne-tune model.\nTable IX shows detailed comparisons for the three ViT-based\nmethods. First, compared with ﬁne-tuning B/16_21 k, one-stage\nViT-CL additional introduces supervised contrast loss. Their\nOAs illustrate that merely adding supervised contrast loss does\nnot work and may disturb the backpropagation process to obtain\nbetter results. Second, comparing the last two models, one-stage\nViT-CL and the proposed ViT-CL, the difference between them\nis when to execute the ﬁne tuning by joint loss. It can be seen\nBI et al.: VISION TRANSFORMER WITH CONTRASTIVE LEARNING 747\nTABLE X\nCLASSIFICATION ACCURACIES OFTHREE VIT-BASED METHOD ON THEAID, NWPU-RESISC45,AND UCM DATASETS(%)\nFig. 7. 2-D visualization of feature representations extracted by Fine-tuned B/16_21 k and ViT-CL on the AID and NWPU-RESISC45 dataset using t-SNE.\n(a) Fine-tuned B/16_21 k. (b) ViT-CL.\nthat the two-stage optimization, which introduces supervised\ncontrast loss after the pretrained model has achieved a good\nresult on the target dataset by initial ﬁne tuning, can make the\njoining loss effective. Finally, as for ﬁne tuning B/16_21 k and\nViT-CL, the main difference is that the latter is optimized by\nthe two-stage joint framework, while the former is optimized\nonly once by CE loss. Their results show that our proposed\nframework can improve the OAs by more than 1% on the two\nmore challenging datasets AID and NWPU-RESISC45. And\nthe lower proportion of the training set, the more signiﬁcant the\nimprovement.\nFurthermore, this article statistic the distributions of category\naccuracy obtained by ﬁne-tuning B/16_12 K and ViT-CL on\ndifferent datasets (i.e., diagonal elements of the CM) to show\nhow much the two-stage joint ﬁne-tuning framework improving\nfeature representations’ expression ability compared with the\nCE loss. The maximum, minimum, average, and variance\nof each category’s accuracy are statistical, respectively. The\nresults are shown in Table X. It can be seen from Table X\nthat the average classiﬁcation accuracies of the ViT-CL on\ndifferent three datasets are all higher than those of Fine-tune\nB/16_21 k, so do the minimum accuracies except the one on\n748 IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL. 16, 2023\ndataset NWPU-RESISC45 with train ratio 20%. Together with\nthe lower variances of classiﬁcation accuracies of the ViT-CL, all\nof these indicate that the introduction of contrast loss can make\nsimilar samples from the same scene more clustered, while the\nconfusion degree among different scenes becomes lower.\nMore intuitively, the t-distributed stochastic neighbor em-\nbedding (t-SNE) algorithm [54] can reduce the dimension of\nthe feature representations generated by different models so\nthat feature projections can be visualized in a 2-D space. The\n2-D visualization images of feature representations extracted\nby Fine-tuning B/16_21 k and ViT-CL on the two dataset AID\n(train ratio 50%) and NWPU-RESISC45 (train ratio 20%) are\nshown in Fig. 7, where (a) is for Fine-tuned B/16_21 k and\n(b) is for ViT-CL. From Fig. 7, it can be found that the feature\nstructure is clear no matter whether the two-stage joint oper-\nation is adopted, which demonstrates the effectiveness of the\nbackbone ViT model. Furthermore, compared with the feature\nclusters extracted by Fine-tuned B/16_21 k, the feature clusters\nextracted by ViT-CL are closer together, and their boundaries\nof them are clearer. This fact conﬁrms the usefulness of our\nframework.\nV. CONCLUSION\nIn this work, a two-stage end-to-end framework named ViT-\nCL is proposed. The framework combines the ViT model with\nsupervised contrastive learning and gives full play to the ad-\nvantages of the two so that it can further improve the accu-\nracy of scene classiﬁcation. The backbone ViT of this frame-\nwork can capture long-range dependencies among patches via\na self-attention mechanism. And the proposed joint loss func-\ntion composed of cross entropy loss and supervised contrast\nloss can help the model learn more robust and discriminating\nsemantic features. Besides, to avoid time-consuming parameter\ntuning, a two-stage ﬁne tuning is employed to ensure the joint\nloss function can show its best performance. ViT-CL has been\nevaluated on three public remote-sensing image datasets, and the\nexperimental results demonstrate the effectiveness in improving\nthe overall accuracy of scene classiﬁcation, compared to some\nclassical CNN-based methods and improved ViT-based models.\nMoreover, with the ablation experiment, how the two-stage\njoint ﬁne-tuning framework improves the performance of scene\nclassiﬁcation is discussed and it concluded that both “two-stage”\nand “joint” are necessary. In the future, we will employ unsuper-\nvised contrast learning or data enhancement strategies to build a\nscenario classiﬁcation framework with lower time consumption\nand better performance.\nREFERENCES\n[1] X. Wu, D. Hong, Z. Huang, and J. Chanussot, “Infrared small object\ndetection using deep interactive U-Net,”IEEE Geosci. Remote Sens. Lett.,\nvol. 19, Nov. 2022, Art. no. 6517805, doi:10.1109/LGRS.2022.3218688.\n[2] J. Yao et al., “Semi-active convolutional neural networks for hyperspec-\ntral image classiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 60,\nSep. 2022, Art. no. 5537915, doi:10.1109/TGRS.2022.3206208.\n[3] C. Toth and G. Jó´zków, “Remote sensing platforms and sensors: A survey,”\nISPRS J. Photogrammetry Remote Sens., vol. 115, pp. 22–36, 2016.\n[4] U. Maulik and D. Chakraborty, “Remote sensing image classiﬁcation:\nA survey of support-vector-machine-based advanced techniques,”IEEE\nGeosci. Remote Sens. Mag., vol. 5, no. 1, pp. 33–52, Mar. 2017.\n[5] D. Hong, L. Gao, J. Yao, B. Zhang, A. Plaza, and J. Chanussot, “Graph con-\nvolutional networks for hyperspectral image classiﬁcation,”IEEE Trans.\nGeosci. Remote Sens., vol. 59, no. 7, pp. 5966–5978, Jul. 2021.\n[6] H. Zhao et al., “GCFNet: Global collaborative fusion network for\nmultispectral and panchromatic image classiﬁcation,” IEEE Trans.\nGeosci. Remote Sens. , vol. 60, Oct. 2022, Art. no. 5632814,\ndoi: 10.1109/TGRS.2022.3215020.\n[7] F. Perronnin, J. Sánchez, and T. Mensink, “Improving the Fisher kernel for\nlarge-scale image classiﬁcation,” inProc. Eur. Conf. Comput. Vis., 2010,\npp. 143–156.\n[8] R. M. Anwer, F. S. Khan, J. Van De Weijer, M. Molinier, and J. Laaksonen,\n“Binary patterns encoded convolutional neural networks for texture recog-\nnition and remote sensing scene classiﬁcation,”ISPRS J. Photogrammetry\nRemote Sens., vol. 138, pp. 74–85, 2018.\n[9] X. Wu, D. Hong, J. Tian, J. Chanussot, W. Li, and R. Tao, “ORSIm detector:\nA novel object detection framework in optical remote sensing imagery\nusing spatial-frequency channel features,”IEEE Trans. Geosci. Remote\nSens., vol. 57, no. 7, pp. 5146–5158, Jul. 2019.\n[10] M. J. Swain and D. H. Ballard, “Color indexing,”Int. J. Comput. Vis.,\nvol. 7, no. 1, pp. 11–32, 1991.\n[11] R. M. Haralick, K. Shanmugam, and I. H. Dinstein, “Textural features for\nimage classiﬁcation,”IEEE Trans. Syst., Man, Cybern., vol. SMC-3, no. 6,\npp. 610–621, Nov. 1973.\n[12] Y . Yang and S. Newsam, “Bag-of-visual-words and spatial extensions\nfor land-use classiﬁcation,” in Proc. 18th SIGSPATIAL Int. Conf. Adv.\nGeograph. Inf. Syst., 2010, pp. 270–279.\n[13] B. Zhao, Y . Zhong, G.-S. Xia, and L. Zhang, “Dirichlet-derived multi-\nple topic scene classiﬁcation model for high spatial resolution remote\nsensing imagery,” IEEE Trans. Geosci. Remote Sens., vol. 54, no. 4,\npp. 2108–2123, Apr. 2016.\n[14] J. Wang, Y . Yang, J. Mao, Z. Huang, C. Huang, and W. Xu, “CNN-RNN:\nA uniﬁed framework for multi-label image classiﬁcation,” inProc. IEEE\nConf. Comput. Vis. Pattern Recognit., 2016, pp. 2285–2294.\n[15] K. Simonyan and A. Zisserman, “Very deep convolutional networks for\nlarge-scale image recognition,” 2014,arXiv:1409.1556.\n[16] C. Szegedy et al., “Going deeper with convolutions,” inProc. IEEE Conf.\nComput. Vis. Pattern Recognit., 2015, pp. 1–9.\n[17] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” inProc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016,\npp. 770–778.\n[18] X. Wu, D. Hong, and J. Chanussot, “UIU-Net: U-Net in U-Net for infrared\nsmall object detection,”IEEE Trans. Image Process., vol. 32, pp. 364–376,\nDec. 2022.\n[19] S. Chaib, H. Liu, Y . Gu, and H. Yao, “Deep feature fusion for VHR remote\nsensing scene classiﬁcation,”IEEE Trans. Geosci. Remote Sens., vol. 55,\nno. 8, pp. 4775–4784, Aug. 2017.\n[20] J. Kang, R. Fernandez-Beltran, D. Hong, J. Chanussot, and A. Plaza,\n“Graph relation network: Modeling relations between scenes for multilabel\nremote-sensing image classiﬁcation and retrieval,”IEEE Trans. Geosci.\nRemote Sens., vol. 59, no. 5, pp. 4355–4369, May 2021.\n[21] R. Minetto, M. P. Segundo, and S. Sarkar, “Hydra: An ensemble\nof convolutional neural networks for geospatial land classiﬁcation,”\nIEEE Trans. Geosci. Remote Sens. , vol. 57, no. 9, pp. 6530–6541,\nSep. 2019.\n[22] J. Xie, N. He, L. Fang, and A. Plaza, “Scale-free convolutional neural\nnetwork for remote sensing scene classiﬁcation,”IEEE Trans. Geosci.\nRemote Sens., vol. 57, no. 9, pp. 6916–6928, Sep. 2019.\n[23] H. Sun, S. Li, X. Zheng, and X. Lu, “Remote sensing scene classiﬁcation by\ngated bidirectional network,”IEEE Trans. Geosci. Remote Sens., vol. 58,\nno. 1, pp. 82–96, Jan. 2020.\n[24] G. Cheng, C. Yang, X. Yao, L. Guo, and J. Han, “When deep learning meets\nmetric learning: Remote sensing image scene classiﬁcation via learning\ndiscriminative CNNs,”IEEE Trans. Geosci. Remote Sens., vol. 56, no. 5,\npp. 2811–2821, May 2018.\n[25] Y . Liu, C. Y . Suen, Y . Liu, and L. Ding, “Scene classiﬁcation using hier-\narchical Wasserstein CNN,”IEEE Trans. Geosci. Remote Sens., vol. 57,\nno. 5, pp. 2494–2509, May 2019.\n[26] C. Shi, X. Zhang, J. Sun, and L. Wang, “A lightweight convolutional neural\nnetwork based on group-wise hybrid attention for remote sensing scene\nclassiﬁcation,” Remote Sens., vol. 14, no. 1, 2021, Art. no. 161.\n[27] D. Hong et al., “More diverse means better: Multimodal deep learning\nmeets remote-sensing imagery classiﬁcation,”IEEE Trans. Geosci. Re-\nmote Sens., vol. 59, no. 5, pp. 4340–4354, May 2021.\n[28] L. Bai, Q. Liu, C. Li, Z. Ye, M. Hui, and X. Jia, “Remote sensing image\nscene classiﬁcation using multiscale feature fusion covariance network\nwith octave convolution,”IEEE Trans. Geosci. Remote Sens., vol. 60,\nMar. 2022, Art. no. 5620214, doi:10.1109/TGRS.2022.3160492.\nBI et al.: VISION TRANSFORMER WITH CONTRASTIVE LEARNING 749\n[29] S. Liu et al., “A shallow-to-deep feature fusion network for VHR remote\nsensing image classiﬁcation,”IEEE Trans. Geosci. Remote Sens., vol. 60,\nMay 2022, Art. no. 5410213, doi:10.1109/TGRS.2022.3179288.\n[30] F. Zhang, B. Du, and L. Zhang, “Scene classiﬁcation via a gradient boosting\nrandom convolutional network framework,”IEEE Trans. Geosci. Remote\nSens., vol. 54, no. 3, pp. 1793–1802, Mar. 2016.\n[31] W. Chen, S. Ouyang, W. Tong, X. Li, X. Zheng, and L. Wang, “GCSANet:\nA global context spatial attention deep learning network for remote sensing\nscene classiﬁcation,”IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens.,\nvol. 15, pp. 1150–1162, Jan. 2022, doi:10.1109/JSTARS.2022.3141826.\n[32] W. Luo, H. Li, G. Liu, and L. Zeng, “Semantic annotation of satellite\nimages using author–genre–topic model,”IEEE Trans. Geosci. Remote\nSens., vol. 52, no. 2, pp. 1356–1368, Feb. 2014.\n[33] G. Cheng, X. Xie, J. Han, L. Guo, and G. Xia, “Remote sensing image scene\nclassiﬁcation meets deep learning: Challenges, methods, benchmarks, and\nopportunities,” IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens.,\nvol. 13, pp. 3735–3756, 2020, doi:10.1109/JSTARS.2020.3005403.\n[34] D. Hong, J. Yao, D. Meng, Z. Xu, and J. Chanussot, “Multimodal\nGANs: Toward crossmodal hyperspectral–multispectral image segmen-\ntation,” IEEE Trans. Geosci. Remote Sens., vol. 59, no. 6, pp. 5103–5113,\nJun. 2021.\n[35] A. Vaswani et al., “Attention is all you need,” inProc. Adv. Neural Inf.\nProcess. Syst., vol. 30, 2017.\n[36] A. Dosovitskiy et al., “An image is worth16 ×16 words: Transformers\nfor image recognition at scale,” 2020,arXiv:2010.11929.\n[37] G. Cheng, J. Han, and X. Lu, “Remote sensing image scene classiﬁ-\ncation: Benchmark and state of the art,”Proc. IEEE, vol. 105, no. 10,\npp. 1865–1883, Oct. 2017.\n[38] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework\nfor contrastive learning of visual representations,” inProc. Int. Conf. Mach.\nLearn., 2020, pp. 1597–1607.\n[39] T. Chen, S. Kornblith, K. Swersky, M. Norouzi, and G. E. Hinton, “Big\nself-supervised models are strong semi-supervised learners,” inProc. Adv.\nNeural Inf. Process. Syst., 2020, vol. 33, pp. 22243–22255.\n[40] K. He, H. Fan, Y . Wu, S. Xie, and R. Girshick, “Momentum contrast for\nunsupervised visual representation learning,” inProc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit., 2020, pp. 9729–9738.\n[41] X. Chen, H. Fan, R. Girshick, and K. He, “Improved baselines with\nmomentum contrastive learning,” 2020,arXiv:2003.04297.\n[42] P. Khosla et al., “Supervised contrastive learning,” inProc. Adv. Neural\nInf. Process. Syst., 2020, vol. 33, pp. 18661–18673.\n[43] Y . Bazi, L. Bashmal, M. M. A. Rahhal, R. A. Dayil, and N. A. Ajlan,\n“Vision transformers for remote sensing image classiﬁcation,”Remote\nSens., vol. 13, no. 3, 2021, Art. no. 516.\n[44] L. Bashmal, Y . Bazi, and M. Al Rahhal, “Deep vision transformers for\nremote sensing scene classiﬁcation,” inProc. IEEE Int. Geosci. Remote\nSens. Symp., 2021, pp. 2815–2818.\n[45] P. Lv, W. Wu, Y . Zhong, F. Du, and L. Zhang, “SCViT: A spatial-channel\nfeature preserving vision transformer for remote sensing image scene\nclassiﬁcation,” IEEE Trans. Geosci. Remote Sens., vol. 60, Mar. 2022,\nArt. no. 4409512, doi:10.1109/TGRS.2022.3157671.\n[46] P. Deng, K. Xu, and H. Huang, “When CNNs meet vision trans-\nformer: A joint framework for remote sensing scene classiﬁcation,”\nIEEE Geosci. Remote Sens. Lett., vol. 19, Sep. 2021, Art. no. 8020305,\ndoi: 10.1109/LGRS.2021.3109061.\n[47] K. Xu, P. Deng, and H. Huang, “Vision transformer: An excellent teacher\nfor guiding small networks in remote sensing image scene classiﬁcation,”\nIEEE Trans. Geosci. Remote Sens., vol. 60, Sep. 2022, Art. no. 5618715,\ndoi: 10.1109/LGRS.2021.3109061.\n[48] A. Steiner, A. Kolesnikov, X. Zhai, R. Wightman, J. Uszkoreit, and L.\nBeyer, “How to train your VIT? Data, augmentation, and regularization in\nvision transformers,” 2021,arXiv:2106.10270.\n[49] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet:\nA large-scale hierarchical image database,” inProc. IEEE Conf. Comput.\nVis. Pattern Recognit., 2009, pp. 248–255.\n[50] W. Liu, Y . Wen, Z. Yu, and M. Yang, “Large-margin softmax loss for\nconvolutional neural networks,” 2016,arXiv:1612.02295.\n[51] T. Xiao, M. Singh, E. Mintun, T. Darrell, P. Dollár, and R. Girshick,\n“Early convolutions help transformers see better,” inProc. Adv. Neural\nInf. Process. Syst., 2021, vol. 34, pp. 30392–30400.\n[52] Z. Wu, Y . Xiong, S. X. Yu, and D. Lin, “Unsupervised feature learning\nvia non-parametric instance discrimination,” inProc. IEEE Conf. Comput.\nVis. Pattern Recognit., 2018, pp. 3733–3742.\n[53] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “Mo-\nbilenetv2: Inverted residuals and linear bottlenecks,” inProc. IEEE Conf.\nComput. Vis. Pattern Recognit., 2018, pp. 4510–4520.\n[54] L. Van der Maaten and G. Hinton, “Visualizing data using t-SNE,”J. Mach.\nLearn. Res., vol. 9, no. 11, pp. 2579–2605, 2008.\nMeiqiao Bi received the M.S. degree in computer\ntechnology from the Hebei University of Technology,\nTianjin, China, in 2016.\nShe is currently a visiting student with the\nAerospace Information Research Institute, Chinese\nAcademy of Sciences, Beijing, China. Her research\ninterests include remote sensing scene classiﬁca-\ntion, computer vision, machine learning, and image\nprocessing.\nMinghua Wang (Member, IEEE) received the B.S.\ndegree in automation from the Harbin Institute of\nTechnology (HIT), Harbin, China, in 2016, and the\nPh.D. degree in control science and engineering from\nHIT, in 2021.\nShe was a visiting Ph.D. student with the Univer-\nsité Grenoble Alpes, CNRS, Grenoble INP, GIPSA-\nlab, Grenoble, France, in 2019–2020. She is cur-\nrently a Postdoc with the Aerospace Information\nResearch Institute, Chinese Academy of Sciences,\nBeijing, China. Her research interests include hyper-\nspectral image denoising, anomaly detection, tensor learning, and low-rank\nrepresentation.\nZhi Li received the B.S. degree in remote sensing\nscience and technology from the China University of\nGeosciences, Wuhan, China, in 2021. He is currently\nworking toward the Ph.D. degree in cartography and\ngeographic information systems with the University\nof Chinese Academy of Sciences, Beijing, China.\nHis research interests include remote sensing scene\nclassiﬁcation and hyperspectral image processing.\nDanfeng Hong (Senior Member, IEEE) received the\nM.Sc. degree (summa cum laude) in computer vi-\nsion from the College of Information Engineering,\nQingdao University, Qingdao, China, in 2015, the\nDr.-Ing degree (summa cum laude) from the Signal\nProcessing in Earth Observation, Technical Univer-\nsity of Munich, Munich, Germany, in 2019.\nHe is currently a Professor with the Key Labora-\ntory of Computational Optical Imaging Technology,\nAerospace Information Research Institute, Chinese\nAcademy of Sciences (CAS), Beijing, China. Before\njoining the CAS, he has been a Research Scientist and led a Spectral Vision\nWorking Group, Remote Sensing Technology Institute, German Aerospace\nCenter (DLR), Oberpfaffenhofen, Germany. He was also an Adjunct Scientist\nwith GIPSA-lab, Grenoble INP, CNRS, Université Grenoble Alpes, Grenoble,\nFrance. His research interests include signal/image processing, hyperspectral\nremote sensing, machine/deep learning, artiﬁcial intelligence, and their applica-\ntions in Earth Vision.\nDr. Hong is an Associate Editor for the IEEE TRANSACTIONS ONGEOSCIENCE\nAND REMOTE SENSING (TGRS), an Editorial Board Member forRemote Sensing,\nand an Editorial Advisory Board Member of theISPRS Journal of Photogramme-\ntry and Remote Sensing. He was the recipient of the Best Reviewer Award of the\nIEEE TGRS in 2021 and 2022, the Best Reviewer Award of the IEEE JOURNAL\nOF SELECTED TOPICS INAPPLIED EARTH OBSERV ATIONS ANDREMOTE SENSING\nin 2022, the Jose Bioucas Dias Award for recognizing the outstanding paper at\nWHISPERS in 2021, the Remote Sensing Young Investigator Award in 2022,\nthe IEEE GRSS Early Career Award in 2022, and a Highly Cited Researcher\n(Clarivate Analytics/Thomson Reuters) in 2022."
}