{
  "title": "UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation",
  "url": "https://openalex.org/W4385572267",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1975776617",
      "name": "Mao Zhiming",
      "affiliations": [
        "Chinese University of Hong Kong",
        "Institute of Software"
      ]
    },
    {
      "id": "https://openalex.org/A2097577945",
      "name": "Huimin Wang",
      "affiliations": [
        "Tencent (China)",
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2134449134",
      "name": "Yi-ming Du",
      "affiliations": [
        "Chinese University of Hong Kong",
        "Institute of Software"
      ]
    },
    {
      "id": "https://openalex.org/A4224426291",
      "name": "Kam-Fai Wong",
      "affiliations": [
        "Chinese University of Hong Kong",
        "Institute of Software"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W1937641174",
    "https://openalex.org/W3173584449",
    "https://openalex.org/W4299286960",
    "https://openalex.org/W2564792523",
    "https://openalex.org/W2152790380",
    "https://openalex.org/W2742272831",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963367478",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2984100107",
    "https://openalex.org/W3034503922",
    "https://openalex.org/W3189967866",
    "https://openalex.org/W4285221954",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W3035242086",
    "https://openalex.org/W3099577420",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W4296591867",
    "https://openalex.org/W3173589978",
    "https://openalex.org/W4285173682",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3155368131"
  ],
  "abstract": "Prior study has shown that pretrained language models (PLM) can boost the performance of text-based recommendation. In contrast to previous works that either use PLM to encode user history as a whole input text, or impose an additional aggregation network to fuse multi-turn history representations, we propose a unified local- and global-attention Transformer encoder to better model two-level contexts of user history. Moreover, conditioned on user history encoded by Transformer encoders, our framework leverages Transformer decoders to estimate the language perplexity of candidate text items, which can serve as a straightforward yet significant contrastive signal for user-item text matching. Based on this, our framework, UniTRec, unifies the contrastive objectives of discriminative matching scores and candidate text perplexity to jointly enhance text-based recommendation. Extensive evaluation shows that UniTRec delivers SOTA performance on three text-based recommendation tasks.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 1160–1170\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nUniTRec: A Unified Text-to-Text Transformer and Joint Contrastive\nLearning Framework for Text-based Recommendation\nZhiming Mao1,2, Huimin Wang1,3, Yiming Du1,2, Kam-Fai Wong1,2\n1The Chinese University of Hong Kong, Hong Kong, China\n2MoE Key Laboratory of High Confidence Software Technologies, China\n3Jarvis Lab, Tencent, Shenzhen, China\n{zmmao,ydu,kfwong}@se.cuhk.edu.hk\nhmmmwang@tencent.com\nAbstract\nPrior study has shown that pretrained language\nmodels (PLM) can boost the performance of\ntext-based recommendation. In contrast to pre-\nvious works that either use PLM to encode user\nhistory as a whole input text, or impose an\nadditional aggregation network to fuse multi-\nturn history representations, we propose a uni-\nfied local- and global-attention Transformer en-\ncoder to better model two-level contexts of user\nhistory. Moreover, conditioned on user history\nencoded by Transformer encoders, our frame-\nwork leverages Transformer decoders to esti-\nmate the language perplexity of candidate text\nitems, which can serve as a straightforward\nyet significant contrastive signal for user-item\ntext matching. Based on this, our framework,\nUniTRec, unifies the contrastive objectives of\ndiscriminative matching scores and candidate\ntext perplexity to jointly enhance text-based\nrecommendation. Extensive evaluation shows\nthat UniTRec delivers SOTA performance on\nthree text-based recommendation tasks.1\n1 Introduction\nText-based recommendation (Li et al., 2010; Gu\net al., 2016; Okura et al., 2017; Malkiel et al., 2020)\naims to recommend relevant textual content (e.g.,\nnews articles, Twitter posts) to people based on\ntheir behaviors as represented in historical log texts.\nFor instance, engagement recommendation (Cheng\net al., 2022) on social media (e.g., Twitter and Red-\ndit) helps users discover and engage with interested\nthreads by modeling their browsing history.\nPretrained language models (Devlin et al., 2019;\nBrown et al., 2020) have made waves in recent\ntext-based recommendation research (Zhang et al.,\n2021; Qi et al., 2022; Geng et al., 2022). The\nmost common practice is using PLM encoders\n(BERT family) to learn representations of user his-\ntory and candidate item texts. Recommendation\n1Our code is available at https://github.com/Veason-\nsilverbullet/UniTRec.\nmatching scores are computed over the user and\nitem representations and finally optimized by noise\ncontrastive estimation (NCE) loss (Gutmann and\nHyvärinen, 2010) for ranking multiple candidates.\nUnlike encoding single text, using PLM to en-\ncode multi-turn texts of user history is nontrivial.\nExisting works (Malkiel et al., 2020; Qi et al., 2022;\nGeng et al., 2022) concatenate multi-turn history\ntexts as a whole input text, then use one PLM en-\ncoder to learn the holistic user representation. This\nis a standard PLM encoding manner but ignores\nthe relation among history turns, as all word tokens\nfrom different history turns are equally attended2.\nIn contrast, previous studies point out that learn-\ning the relation among user history turns is also\nbeneficial (Zeng et al., 2020; Qi et al., 2021). An-\nother approach is using PLM encoders to learn\nrepresentations from multi-turn history texts, fol-\nlowed by an additional aggregation network to fuse\nthe multi-turn representations (Wu et al., 2021;\nLi et al., 2022). However, the imposed aggrega-\ntion networks (with newly initialized parameters)\nweaken the representation power of PLM encoders\nwhich are already pretrained on large-scale corpora.\nThis work introduces UniTRec, aUnified text-to-\ntext Transformer framework for text-based Recom-\nmendation. In the encoder component of UniTRec,\nwe design local- and global-attention to learn user\nhistory representations through tailored attention\nmasking, which aims to jointly model word-level\nand turn-level relations of user history. UniTRec\ncan utilize the full power of PLM encoders because\nit preserves the intact structure of PLM encoders\nwithout newly imposed parameters.\nDifferent from most previous works that predict\nuser-candidate matching scores solely based on the\nrepresentations learned by Transformer encoders,\nwe argue that conditioned on user representations\n2There is no inductive bias of turn-level and history-level\nrelations introduced to Transformer self-attention computation,\nwhere each token plays an equal role.\n1160\nLSU, Ohio State, and Clemson control \npath to College Football Playoff.\nKevin Hart makes first official \nappearance at People's Choice Awards.\nYardbarker's NFL Week 10 game-by-\ngame analysis and grades.\nPPL: 1.02\nPPL: 7.76\nPPL: 3.52\nLM \nDecoder\nContrastive Ranking\nRank: 1st\nRank: 2nd\nRank: 3rd\nCandidate Text to Recommend Perplexity\nTurn User-browsed News Titles\n1 AP Top 25: LSU jumps to No. 2, upset drops Georgia to No. 10.\n2 5 college games to watch this Saturday.\n3 LSU surging, Big Ten reckoning and more we learned from college football's \"Separation Saturday\".\nMulti-turn User History\nGround truth user-click labels:       for clicked,       for non-clicked\nConditioned on user history\nFigure 1: An example of perplexity-based ranking for\ncandidate item texts, conditioned on user history. The\nillustrated task is text-based news recommendation.\nlearned by Transformer encoders, candidate text\nperplexity (PPL) estimated by pretrained Trans-\nformer decoders is also a straightforward yet sig-\nnificant signal for text-based recommendation. As\nshown in Figure 1, we hypothesize that the can-\ndidate text perplexity estimated by pretrained LM\ndecoders can directly measure the text matching\ndegree between user history and candidate texts. It\nis because the perplexity estimates the likelihood\nof candidate texts based on encoder outputs, which\nnaturally indicates the probabilities of candidate\ntexts given the user history. Besides, UniTRec can\nuse the last hidden states of Transformer decoders\nto directly predict matching scores. Hence, this\nwork unifies the contrastive objectives of discrimi-\nnative matching scores and candidate text perplex-\nity to jointly enhance text-based recommendation.\nThe contributions of this work are: (1) We pro-\npose local- and global-attention to model two-level\nrelation of user history without additional parame-\nters, which enjoys the full power of PLM encoders.\n(2) We introduce PLM perplexity to measure user-\ncandidate text matching and unify the objectives of\ndiscriminative matching scores and candidate text\nperplexity to enhance text-based recommendation.\n(3) Experiments on three text-based recommenda-\ntion datasets validate the effectiveness of UniTRec.\n2 Approach\n2.1 Unified User-history Modeling\nFormally, multi-turn history of a user is represented\nas H = [t1,t2,...,t N], and each turn text ti con-\ntains |ti|words as ti = [x1\ni,x2\ni,...,x |ti|\ni ]. UniTRec\naims to unify learning word- and turn-level context\nrepresentations in one Transformer encoder.\nLocal attention on word-level context. We first\nconcatenate the multi-turn history texts as the input\ntokens X = [x1\n1,x2\n1,...,x |t1|\n1 ,...,x 1\nN,x2\nN,...,x |tN |\nN ].\nInspired by Dong et al. (2019), we tailor the atten-\ntion masking in Transformer self-attention to learn\nthe word-level context of each turn. Specifically,\nwe allow word tokens from the same turn to attend\nto each other, while tokens from different turns are\nexcluded from self-attention computation:\nMi,j =\n{\n0, token xi and xj in the same turn\n−∞, otherwise\nAttention(Q,K,V ) = softmax(QKT\n√dk\n+M)V\n(1)\n, where Q,K,V are self-attention query, key, and\nvalue in Vaswani et al. (2017),M is the mask ma-\ntrix to achieve local-attention inside each turn text.\nThe local self-attention blocks consist of L1 layers,\nby which original PLM encoders can be adapted to\nlearn word-level context representations of turns.\nGlobal attention on turn-level context. Over\nthe local self-attention layers, we leverage global\nself-attention to model the relation among history\nturns. Specifically, tokens from all turns attend to\neach other in self-attention computation (by setting\nthe mask matrix M = 0). In this way, Transformer\nencoders can perform global interaction among\neach token (and turn) to learn turn-level context\nrepresentations of user history. There are L2 layers\nin the global self-attention blocks, which can also\nbe inherited from PLM encoders directly.\n2.2 Joint Contrastive Ranking Objectives\nConditioned on the history representation, we in-\nput the candidate text to Transformer decoders to\npredict how likely it should be recommended. It is\nworth noting that Transformer decoders can natu-\nrally perform effective cross-attention interaction\nbetween history and candidate hidden states.\n2.2.1 Objective on Discriminative Scores\nMotivated by Lewis et al. (2020), we feed the last\nhidden state of decoder output hT to an MLP score-\nhead which predicts the user-candidate matching\nscore Sd = ScoreHead(hT). The matching score\nis discriminative, as higher scores indicate higher\nuser-candidate matching probabilities.\nFollowing previous works (Li et al., 2022; Qi\net al., 2022), we adopt negative sampling with NCE\nloss to optimize matching score prediction. Given\nthe user history and its ground truth matched can-\ndidate Ci, UniTRec predicts the matching score\n1161\nN N N\nFigure 2: Overview of UniTRec. In training, matching scores Sd and Sp are optimized by the NCE loss, respectively.\nIn inference, Sd and Sp are normalized and combined to derive the final output ranking.\nas Sd+\ni . In addition, Kunmatched negative candi-\ndates {Cj}K\nj=1 are sampled from the candidate set,\nand their matching scores are {Sd−\nj }K\nj=1. The NCE\nloss is represented in a contrastive form:\nLd\ni = −log exp(Sd+\ni )\nexp(Sd+\ni ) + ∑K\nj=1 exp(Sd−\nj )\n(2)\n2.2.2 Objective on Candidate Text Perplexity\nAs aforementioned, UniTRec leverages perplexity\nto rank candidate texts. Since lower perplexity in-\ndicates higher user-candidate matching probability,\nregarding the candidate text Y = [y1,y2,...,y T],\nwe define the perplexity-based matching score Sp\nas its negative perplexity3:\nSp = −PPL(Y) = 1\nT\n∑ T\ni=1\nlog pθ(yi|y<i) (3)\n, where pθ(·) denotes the target probability output\nfrom the UniTRec Transformer decoder. Similar to\nEq. (2), we optimize the perplexity-based match-\ning score Sp in the NCE loss form. As perplexity\nempirically varies in a wide range, we introduce a\ntemperature parameter τ to balance the joint NCE\nloss gradients following Radford et al. (2021).\nLp\ni = −log exp(τ ·Sp+\ni )\nexp(τ ·Sp+\ni ) + ∑K\nj=1 exp(τ ·Sp−\nj )\n(4)\n, where τ is learnable and initialized to 1. On the\ntraining dataset D, the joint contrastive learning\nobjective is formulated as:\nL=\n∑ |D|\ni=1\n(\nLd\ni + Lp\ni\n)\n(5)\n3Note https://huggingface.co/docs/transformers/perplexity\nfor LM perplexity calculation. We empirically discard the\nouter exponential term in the PPL formula, because it already\nexists in NCE loss Eq. (4) and does not affect the final ranking.\n2.3 Model Initialization and Inference\nAs UniTRec is a standard text-to-text Transformer,\nwe initialize the parameters from pretrained BART\n(Lewis et al., 2020). In inference, UniTRec predicts\nthe discriminative and perplexity-based scores for\neach candidate item, respectively. The two sepa-\nrate scores Sd and Sp are normalized, averaged,\nand finally ranked as the output. Detailed ranking\nprocess is provided in Appendix B.\n3 Experiments\nWe evaluate UniTRec on three text-based recom-\nmendation tasks: 1) NewsRec, to recommend news\narticles to users based on their browsing history.\nWe use the MIND-small dataset (Wu et al., 2020)\nfor experiments. 2) QuoteRec, to recommend quo-\ntations to users based on their conversation history.\nWe use the Reddit-quotation dataset (Wang et al.,\n2021) for experiments. 3) EngageRec, to recom-\nmend social media posts for users to engage with\nbased on their comment history. We use the dataset\nreleased by Zeng et al. (2020) for experiments. De-\ntailed dataset statistics is provided in Appendix A.\nImplementation Details. The UniTRec encoder\nand decoder both consist of 6 Transformer layers\nwith 768-dimensional hidden states and 12 atten-\ntion heads. We set L1 = 3 and L2 = 3. We use\nAdamW optimizer (Loshchilov and Hutter, 2019)\nto train UniTRec with cosine learning rate decay.\nBaselines. We compare UniTRec with compet-\nitive baselines: 1) GRU4Rec (Balázs et al., 2016)\nutilizes a GRU network to learn multi-turn history.\n2) SASRec (Kang and McAuley, 2018) encodes\nuser history with a self-attention based sequential\nmodel. 3) BERT4Rec (Sun et al., 2019) employs\nbidirectional self-attention to model user history.4)\nRoBERTa-Sim, a simple yet strong baseline men-\n1162\nNewsRec QuoteRec EngageRec\nModel MRR NDCG@5/10 HR@5/10 MRR NDCG@5/10 HR@5/10 MRR NDCG@5/10 HR@5/10\nGRU4Rec 32.91 36.20/42.53 50.33/68.35 34.08 34.65/37.93 44.45/54.63 2.12 1.04/1.51 1.27/2.65\nSASRec 32.60 36.03/42.37 50.63/68.64 33.63 34.30/37.49 44.32/54.20 2.40 1.49/1.95 2.16/3.47\nBERT4Rec 32.87 36.18/42.40 50.21/67.97 33.59 34.26/37.27 43.76/53.05 3.04 1.98/3.23 2.81/6.67\nRoBERTa-Sim32.96 36.47/42.81 51.06/69.08 37.13 37.96/41.18 48.14/58.06 3.74 2.66/3.75 4.42/ 7.70\nUNBERT 33.09 36.53/42.84 50.87/68.82 39.75 40.74/43.69 50.90/60.04 2.83 1.96/2.67 3.11/5.24\nUniTRec 33.76 37.63 /43.74 52.61 /69.89 41.24 42.38 /45.31 52.87 /61.88 4.06 3.23 /4.29 4.58 /7.68\nTable 1: Experiment results on three text-based recommendation tasks. MRR denotes mean reciprocal rank, NDCG\ndenotes normalized discounted cumulative gain, and HR denotes hit ratio (presented in percentage). The overall\nperformance of UniTRec is better than other baseline models with p-value <0.05, validated by unpaired t-test.\nNewsRec QuoteRec EngageRec\nModel MRR NDCG@5/10 HR@5/10 MRR NDCG@5/10 HR@5/10 MRR NDCG@5/10 HR@5/10\nUniTRec 33.76 37.63/43.74 52.61/69.89 41.24 42.38/45.31 52.87/61.88 4.06 3.23/4.29 4.58/7.68\nw/o BART Init 30.31 33.32/39.69 47.55/65.78 19.02 17.66/20.80 22.45/32.16 2.24 0.86/1.61 1.27/3.62\nw/o Local-Att 33.34 37.22/43.32 52.28/69.54 40.44 41.63/44.56 52.09/61.15 3.92 3.19/4.15 4.38/7.36\nw/o Global-Att 33.22 37.06/43.17 52.14/69.47 40.25 41.47/44.26 52.07/60.76 3.64 2.78/3.59 3.89/6.35\nDisc-Score only33.07 36.76/43.03 51.68/69.46 40.59 41.81/44.65 52.39/61.14 3.82 2.99/3.60 4.49/6.85\nPPL-Score only32.83 36.39/42.59 51.05/68.67 40.31 41.43/44.47 52.13/61.20 3.29 2.39/3.03 3.86/5.66\nTable 2: Recommendation performance of ablation model variants.\ntioned in Qi et al. (2022), uses the hidden states of\n[CLS] tokens to measure user-candidate similarity.\n5) UNBERT, implemented as Zhang et al. (2021),\nconcatenates history and candidate texts as the in-\nput to BERT and predicts matching scores from the\nfinal hidden states of [CLS] tokens.\nNote that we do not consider other methods that\nuse non-text inputs (e.g., user profile, text topic\nlabels). For fair comparison, all baseline models\nuse pretrained 12-layer RoBERTa-base (Liu et al.,\n2019) as text encoders to learn embeddings of texts.\n3.1 Main Results\nTable 1 shows the performance of experiment mod-\nels. From the results of NewsRec and QuoteRec,\nwe can see that UniTRec outperforms all baseline\nmodels by a clear margin. Also, RoBERTa-Sim\nand UNBERT that directly use the [CLS] hidden\nstates to represent user history, surpass other base-\nlines that build additional aggregation networks\nupon the whole RoBERTa outputs. As displayed\nin the results, EngageRec is the most difficult task.\nWe inspect the dataset and find that the texts on so-\ncial media contain too much noise (e.g., URL and\nemoji), and the user history contains less number\nof turns. Nevertheless, UniTRec achieves better\noverall performance than other baseline models,\nvalidating its robustness on noisy text inputs and\nlimited user history.\n3.2 Ablation Studies and Analyses\nWe further conduct ablation studies on UniTRec.\nThe experiment results are reported in Table 2.\nInitialization of UniTRec. We train UniTRec\nfrom scratch without initialization from pretrained\nBART (refer to w/o BART Init). The recommen-\ndation performance significantly drops in all three\ntasks, which indicates that acquiring effective text\nunderstanding ability from PLM is a necessary key\nto UniTRec performance.\nLocal and global attention. We investigate the\nfunction of two-level attention modules of the Uni-\nTRec history encoder. Concretely, we set L1 = 0\nin w/o Local-Att and L2 = 0 in w/o Global-Att,\nwhere L1 + L2 = 6. We can observe that remov-\ning local and global attention from the original\nUniTRec history encoder both lead to suboptimal\nperformance, while the performance drop is more\nsignificant in w/o Global-Att. The results justify\nthe effectiveness of jointly modeling two-level his-\ntory contexts through adapted Transformer atten-\ntion masking without additional parameters.\nDiscriminative and perplexity-based objectives.\nWe probe into training UniTRec with standalone\ndiscriminative (Disc-Score only) and perplexity-\nbased (PPL-Score only) contrastive objectives, re-\nspectively. We can see that the discriminative objec-\ntive yields better performance than the perplexity-\nbased objective. Besides, the model performance\non both standalone objectives declines compared to\nthe original joint objective. The results indicate that\nthe discriminative and perplexity-based matching\nscores are complementary and can jointly provide\nmore accurate signals of user history and candidate\ntext matching for text-based recommendation.\n1163\n4 Conclusion\nWe present a unified Transformer UniTRec for text-\nbased recommendation. UniTRec learns two-level\ncontexts of multi-turn user history and jointly ex-\nploits discriminative matching scores and candidate\ntext perplexity as matching objectives. Empirical\nexperiments on three text-based recommendation\ndatasets corroborate the effectiveness of UniTRec.\n5 Limitations\nOur model only focuses on utilizing text informa-\ntion for recommendation, which is a key limitation\nof this work. In real-world settings, recommender\nsystems are usually required to handle heteroge-\nneous information inputs. UniTRec is a pure text-\nbased recommender modeling user history and can-\ndidate texts as inputs. However, incorporating addi-\ntional side information (e.g., user profile, text topic\nlabels, and dwell time of user behaviors) could\nfurther improve the recommendation performance\nand alleviate the cold start problem. Furthermore,\nUniTRec only models two-level relations of user\nbehavior history. Nonetheless, incorporating more\nuser behavior information, such as implicit and\nnegative feedback, could further enhance the rec-\nommendation performance.\nAcknowledgements\nWe appreciate constructive comments from anony-\nmous reviewers. The research described in this pa-\nper is partially supported by CUHK under Project\nNo. 3230366.\nReferences\nHidasi Balázs, Karatzoglou Alexandros, Baltrunas\nLinas, and Tikk Domonkos. 2016. Session-based\nrecommendations with recurrent neural networks. In\n4th International Conference on Learning Represen-\ntations ICLR 2016, San Juan, Puerto Rico, May 2-4,\n2016, Conference Track Proceedings.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nDaniel Cheng, Kyle Yan, Phillip Keung, and Noah A.\nSmith. 2022. The engage corpus: A social media\ndataset for text-based recommender systems. In Pro-\nceedings of the Thirteenth Language Resources and\nEvaluation Conference, pages 1885–1889, Marseille,\nFrance. European Language Resources Association.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Unified language model\npre-training for natural language understanding and\ngeneration. In 33rd Conference on Neural Informa-\ntion Processing Systems (NeurIPS 2019).\nShijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge,\nand Yongfeng Zhang. 2022. Recommendation as\nlanguage processing (rlp): A unified pretrain, person-\nalized prompt & predict paradigm (p5). In Proceed-\nings of the 16th ACM Conference on Recommender\nSystems, RecSys ’22, page 299–315, New York, NY ,\nUSA. Association for Computing Machinery.\nYouyang Gu, Tao Lei, Regina Barzilay, and Tommi\nJaakkola. 2016. Learning to refine text based recom-\nmendations. In Proceedings of the 2016 Conference\non Empirical Methods in Natural Language Process-\ning, pages 2103–2108, Austin, Texas. Association\nfor Computational Linguistics.\nMichael Gutmann and Aapo Hyvärinen. 2010. Noise-\ncontrastive estimation: A new estimation principle\nfor unnormalized statistical models. In Proceedings\nof the Thirteenth International Conference on Artifi-\ncial Intelligence and Statistics, volume 9 of Proceed-\nings of Machine Learning Research, pages 297–304,\nChia Laguna Resort, Sardinia, Italy. PMLR.\nWang-Cheng Kang and Julian McAuley. 2018. Self-\nattentive sequential recommendation. In 2018 IEEE\nInternational Conference on Data Mining (ICDM),\npages 197–206.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\n1164\nJian Li, Jieming Zhu, Qiwei Bi, Guohao Cai, Lifeng\nShang, Zhenhua Dong, Xin Jiang, and Qun Liu. 2022.\nMINER: Multi-interest matching network for news\nrecommendation. In Findings of the Association for\nComputational Linguistics: ACL 2022 , pages 343–\n352, Dublin, Ireland. Association for Computational\nLinguistics.\nYize Li, Jiazhong Nie, Yi Zhang, Bingqing Wang,\nBaoshi Yan, and Fuliang Weng. 2010. Contextual\nrecommendation based on text mining. In Coling\n2010: Posters, pages 692–700, Beijing, China. Col-\ning 2010 Organizing Committee.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. In arXiv preprint arXiv: 1907.11692. arXiv.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In 7th International\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019 . OpenRe-\nview.net.\nItzik Malkiel, Oren Barkan, Avi Caciularu, Noam Razin,\nOri Katz, and Noam Koenigstein. 2020. RecoBERT:\nA catalog language model for text-based recommen-\ndations. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2020, Online. Associa-\ntion for Computational Linguistics.\nShumpei Okura, Yukihiro Tagami, Shingo Ono, and\nAkira Tajima. 2017. Embedding-based news recom-\nmendation for millions of users. In Proceedings of\nthe 23rd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining , page\n1933–1942, New York, NY , USA. Association for\nComputing Machinery.\nFanchao Qi, Yanhui Yang, Jing Yi, Zhili Cheng,\nZhiyuan Liu, and Maosong Sun. 2022. QuoteR: A\nbenchmark of quote recommendation for writing. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 336–348, Dublin, Ireland. Asso-\nciation for Computational Linguistics.\nTao Qi, Fangzhao Wu, Chuhan Wu, Peiru Yang, Yang\nYu, Xing Xie, and Yongfeng Huang. 2021. HieRec:\nHierarchical user interest modeling for personalized\nnews recommendation. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 5446–5456, Online. Association\nfor Computational Linguistics.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. In Proceedings of the 38th International\nConference on Machine Learning , volume 139 of\nProceedings of Machine Learning Research, pages\n8748–8763. PMLR.\nFei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin,\nWenwu Ou, and Peng Jiang. 2019. Bert4rec: Se-\nquential recommendation with bidirectional encoder\nrepresentations from transformer. In Proceedings of\nthe 28th ACM International Conference on Informa-\ntion and Knowledge Management, CIKM ’19, page\n1441–1450, New York, NY , USA. Association for\nComputing Machinery.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30, pages 5998–6008. Cur-\nran Associates, Inc.\nLingzhi Wang, Xingshan Zeng, and Kam-Fai Wong.\n2021. Quotation recommendation and interpretation\nbased on transformation from queries to quotations.\nIn Proceedings of the 59th Annual Meeting of the As-\nsociation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers), pages 754–758,\nOnline. Association for Computational Linguistics.\nChuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng\nHuang. 2021. Empowering news recommendation\nwith pre-trained language models. In Proceedings\nof the 44th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval,\nSIGIR ’21, page 1652–1656, New York, NY , USA.\nAssociation for Computing Machinery.\nFangzhao Wu, Ying Qiao, Jiun-Hung Chen, Chuhan\nWu, Tao Qi, Jianxun Lian, Danyang Liu, Xing Xie,\nJianfeng Gao, Winnie Wu, and Ming Zhou. 2020.\nMIND: A large-scale dataset for news recommenda-\ntion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n3597–3606, Online. Association for Computational\nLinguistics.\nXingshan Zeng, Jing Li, Lu Wang, Zhiming Mao, and\nKam-Fai Wong. 2020. Dynamic online conversation\nrecommendation. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3331–3341, Online. Association for\nComputational Linguistics.\nQi Zhang, Jingjie Li, Qinglin Jia, Chuyuan Wang, Jiem-\ning Zhu, Zhaowei Wang, and Xiuqiang He. 2021. Un-\nbert: User-news matching bert for news recommen-\ndation. In Proceedings of the Thirtieth International\nJoint Conference on Artificial Intelligence, IJCAI-21,\npages 3356–3362. International Joint Conferences on\nArtificial Intelligence Organization. Main Track.\n1165\nDataset NewsRec QuoteRec EngageRec\nAvg. history turns 26.09 4.24 3.29\nAvg. history tokens 414.40 279.82 286.82\nAvg. candidates 37.23 1111 7163\nAvg. candidate tokens 16.15 19.11 102.42\nTable 3: Statistics of three text-based recommendation\ntraining datasets. History and candidate tokens denote\nthe number of BPE-tokenized tokens. The test set distri-\nbution is closed to the training sets (except candidates\nof EngageRec) and hence omitted. Note that the max\nlength of each history log is truncated to 1024 tokens.\nA Dataset Statistics\nThe detailed statistics of the three text-based recom-\nmendation datasets are displayed in Table 3. Note\nthat we use news titles as the text inputs for News-\nRec following Qi et al. (2021). NewsRec regards\nthe user clicked and non-clicked news as candidate\ntexts, while QuoteRec and EngageRec regard all po-\ntential quotation texts and post texts as candidates.\nDifferent from Zeng et al. (2020) that formulates\nthe task as recommending candidate users to given\nposts based on post content, we formulate the task\nas recommending candidate posts to given users\nbased on user history.\nAlgorithm 1 Candidate Ranking Processs\nInput: discriminative scores Sd = {Sd\n1 , Sd\n2 , ..., Sd\nM },\nperplexity-based scores Sp = {Sp\n1 , Sp\n2 , ..., Sp\nM }.\nOutput: final averaged ranking ¯R.\n1: Derive the normalized discriminative scores Sd\nnorm =\nsoftmax(Sd).\n2: Derive the normalized perplexity-based scores Sp\nnorm =\nsoftmax(Sp).\n3: Derive the geometric average scores ¯S = log (Sd\nnorm) +\nlog (Sp\nnorm).\n4: Sort the averaged scores ¯S by descending order to derive\nthe final ranking: ¯R ←Rankdes( ¯S).\n5: return ¯R\nB Inference Ranking\nGiven the user history and M candidate texts,\nUniTRec first predicts the discriminative rank-\ning scores Sd = {Sd\n1 ,Sd\n2 ,...,S d\nM}and perplexity-\nbased ranking scoresSp = {Sp\n1 ,Sp\n2 ,...,S p\nM}of the\ncandidates. Algorithm 1 outlines an approach to ag-\ngregate the final ranking based on Sd and Sp. Note\nthat the function Rank(S)4 denotes outputting the\nsorted order of elements in a score list S. There\nexist other ways to average the ranking of Sd and\nSp, which we leave for future work to explore.\n4Rank(S) works similarly to scipy.stats.rankdata(). For\nexample in ascending order, Rankasc({0.2,0.6,0.7,0.4}) =\nscipy.stats.rankdata([0.2,0.6,0.7,0.4]) = [1,3,4,2]\nC Qualitative Analysis\nWe show randomly sampled outputs of UniTRec,\nfor instance, demonstrated on the news recommen-\ndation and quote recommendation tasks. Table 4\nand 5 showcase the qualitative samples.\n1166\nTurn History News Texts\n#1 Mac Engel: As long as these results are acceptable, Dallas Cowboys will continue to be losers\n#2 NFL world reacts to officials handing Packers win over Lions\n#3 Maryland Congressman Elijah Cummings, a Democrat and Chair of House Oversight and Reform Committee, has died: CNN\n#4 Unprecedented movement detected on California earthquake fault capable of 8.0 temblor\n#5 Bag Explodes While Being Loaded On V olaris Flight At Midway Airport\n#6 Orlando Scandrick rips Eagles: They have \"accountability issues\"\n#7 Meghan King Edmonds, Jim Edmonds’ Nanny Denies Cheating Allegations\n#8 Nearly $400M worth of cocaine and marijuana intercepted by US Coast Guard\n#9 Former NBA first-round pick arrested in sex sting operation\n#10 China’s trade with US shrinks in October despite optimism\nCandidate News Texts Sd Sp ¯R Clicked\nTaylor Swift Rep Hits Back at Big Machine, Claims She’s Actually Owed $7.9 Million in Unpaid Royalties 0.095 0 .069 4 ✗\nFormer North Carolina State, NBA player Anthony Grundy dies in stabbing, police say 0.172 0 .155 3 ✗\n13 Reasons Why’s Christian Navarro Slams Disney for Casting \"the White Guy\" in The Little Mermaid 0.048 0 .065 7 ✗\nOpinion: Colin Kaepernick is about to get what he deserves: a chance 0.303 0 .250 1 ✓\n3 Indiana judges suspended after a night of drinking turned into a White Castle brawl 0.076 0 .059 5 ✗\n66 Cool Tech Gifts Anyone Would Be Thrilled to Receive 0.009 0 .005 9 ✗\nPolice find 26 children behind false wall at Colorado day care 0.034 0 .116 6 ✗\nI’ve been writing about tiny homes for a year and spent 2 nights in a 300-foot home to see what it is all about 0.029 0 .019 8 ✗\nReport: Police investigating woman’s death after Redskins’ player Montae Nicholson took her to hospital 0.235 0 .261 2 ✓\n(i) Qualitative Example-A from news recommendation.\nTurn History News Texts\n#1 Toddler dancing to celebrate 11 months cancer-free goes viral\n#2 NFL Week 8 Power Rankings: Old-school football rules the day\n#3 The 25 US cities where it’s easiest to get a mortgage\n#4 Burning questions for Cowboys vs Giants on \"Monday Night Football\"\n#5 Who’s the favorite to win 2019 NFL rushing title?\n#6 Grading all 32 NFL teams heading into the last eight weeks of the 2019 season\n#7 Jennifer Aniston looks amazing in a makeup-free selfie, plus more news\n#8 This $12 million \"mansion yacht\" is made entirely of stainless steel and it’s a first for the industry. Take a peek inside\nCandidate News Texts Sd Sp ¯R Clicked\nOpinion: Colin Kaepernick is about to get what he deserves: a chance 0.330 0 .400 1 ✓\nU.S. Troops Will Die If They Remain in Syria, Bashar Al-Assad Warns 0.024 0 .011 10 ✗\nPete Davidson, Kaia Gerber Are Dating, Trying to Stay \"Low Profile\" 0.064 0 .033 6 ✗\nThe Hottest Tech Gifts This Holiday Season 0.050 0 .027 8 ✗\nTaylor Swift Rep Hits Back at Big Machine, Claims She’s Actually Owed $7.9 Million in Unpaid Royalties 0.046 0 .038 7 ✗\n13 Reasons Why’s Christian Navarro Slams Disney for Casting \"the White Guy\" in The Little Mermaid 0.060 0 .096 4 ✓\nSome believe Mason Rudolph, hit in head with his own helmet, isn’t getting enough blame 0.154 0 .179 2 ✓\nSouth Carolina teen gets life in prison for deadly elementary school shooting 0.066 0 .046 5 ✗\nThe Unlikely Star of My Family’s Thanksgiving Table 0.047 0 .021 9 ✗\nPolice investigating woman’s death after Redskins’ player Montae Nicholson took her to hospital 0.158 0 .149 3 ✗\n(ii) Qualitative Example-B from news recommendation.\nTable 4: Case analyses of news recommendation. History News Textsare sorted by user-clicked timestamps. Sd, Sp,\nand ¯Rare normalized discriminative, perplexity-based scores, and average ranking as described in Appendix B.\nClicked denotes the ground truth user-click labels. Note that the experiment history logs are anonymized and\ndelinked, which is always the first priority of the recommendation study.\n1167\nTurn Conversation Threading History\n#1 I own an FJ. It’s a great car and even on stockies. It s great offroad.\n#2 I feel bad for you that you run the risk of being associated with the typical FJ owner.\n#3 What is a typical FJ owner? I’ve not heard anything bad about FJ owners.\n#4 It’s like someone who drives a jeep wrangler in NYC. There’s no need. Tons of FJ owners do that have it and not use it for what it’s made for.\n#5 God forbid someone likes the design of a car and doesn’t use it offroad.\n#6 Then buy a much more economic environmentalist friendly version. If you buy something and always use it for much less than it’s purpose,\nwhy buy it?\n#7 Or people can buy whatever the hell they want because it’s their money and not yours.\n#8 You’re entirely right. Just like people can be rude just because you can do it, because you have the ability but why should you ass.\n#9 I wasn’t aware that somebody buying a vehicle that they like and you don’t was morally wrong.\n#10 I love FJs. It’s perfectly fine to buy whatever you think looks nice.\nCandidate Quote Texts Sd Sp ¯R Ground truth\nBeauty is in the eye of the beholder. 0.480 0 .471 1 ✓\nA fool and his money are soon parted. 0.176 0 .140 2\nForm follows function. 0.051 0 .046 3\nEverything is worth what its purchaser will pay for it. 0.040 0 .058 4\nBecause it’s there. 0.038 0 .029 5\nYou can’t fix stupid. 0.021 0 .034 6\nThe lady doth protest too much, methinks. 0.022 0 .013 7\nIt’s all about the money. 0.020 0 .013 8\nAnybody driving slower than you is an idiot, and anyone going faster than you is a maniac? 0.012 0 .018 9\nOpportunity is missed by most people. 0.018 0 .008 10\n(iii) Qualitative Example-C from quote recommendation.\nTurn Conversation Threading History\n#1 Society is becoming more efficient, which is a good thing. People should realize there’s no point in holding back this technology just for the\nsake of keeping people employed. If this were beneficial, then calculators and computers shouldn’t exist either.\n#2 One small problem is that people need to pay rent and eat.\n#3 So we should ditch computers and go back to the typing pool? Should we get rid of heavy earth moving equipment and just use hundreds of\nguys with hand tools to build everything? It would employ a hell of a lot more people.\n#4 No one’s saying that. I don’t think anyone is really against automation, but as it increases, there are soon going to be more people that there\nare jobs that actually need doing. I actually believe we’ve already passed this point. So what do we do with the people, who can’t get jobs\nsimply because there are none? It’s an issue that need assessed immediately.\n#5 Tons and tons and tons of American jobs have been replaced by new jobs created by technology or in support of technology years ago. An\noffice might have needed people to handle filing paperwork, keeping it in order, and retrieving, where now a document management system\nhas made them completely redundant. The upshot is that to access that DMS, people are out there selling computers, installing computers,\nservicing computers, and supporting end users building the servers installing, supporting monitoring backing them up, and all that jobs that\ncome in support of those progress is progress. And it advances human efficiency and knowledge. These are just one or two examples, but the\nanswer is not to kill progress. Other countries simply won’t. The answer is to push education to the forefront, so people are prepared for\nthese jobs and whatever other challenges the future may bring.\n#6 This is true. But it s unfortunate technological advances tend to reduce low skill jobs and replace them with high skill jobs. It would feel more\nfair if the low skilled workers could all do training programs and become high skilled workers. But this isn’t really the case. Those jobs end\nup being taken by someone who had better educational opportunities or someone younger who still has time to take advantage of education.\n#7 The reality is the reality. Unfortunate or not educating people will create more educated people to handle high skill jobs, and I’ll tell you\nbeing a desktop support technician isn’t high skill. As that’s where we push in the future, any amount of hand wringing won’t change the\nfacts. We must educate our people if we want to be a global leader in more than homelessness poverty.\n#8 Education won’t matter. We are at the end of the job age at some point in the near future. We are going to have to deal with the fact that\ngetting a job isn’t a reality for a significant percentage of the population. Society will have to radically change as it did during the industrial\nrevolution.\n#9 Much cheaper to heavily discourage having more children free abortions. Then in years there won’t be so many useless people who can\napparently be replaced by a simple robot.\n#10 Virtually every job will be replaced by automation name skilled trades that can’t be automated. I imagine you’d be surprised at how hard this\nis. Are pharmacists useless, surgeons, accountants? I’d bet that your job is just as replaceable as these.\nCandidate Quote Texts Sd Sp ¯R Ground truth\nThere’s no such thing as a free lunch. 0.365 0 .417 1\nI can’t predict the future. 0.185 0 .210 2 ✓\nI have never let my schooling interfere with my education. 0.104 0 .059 3\nPrevention is better than cure. 0.044 0 .083 4\nKnowledge is power. 0.059 0 .052 5\nDon’t let schooling interfere with your education. 0.044 0 .043 6\nNature abhors a vacuum. 0.036 0 .024 7\nThere is no substitute for hard work. 0.024 0 .017 8\nThere are three kinds of lies: lies, damned lies, and statistics. 0.022 0 .013 9\nYou can’t fix stupid. 0.019 0 .010 10\n(iv) Qualitative Example-D from quote recommendation.\nTable 5: Case analyses of quote recommendation. We demonstrate the candidate quotes of the top 10 rankings out\nof all candidates. Note that there is only one ground truth quote for each conversation history.\n1168\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 5\n□\u0017 A2. Did you discuss any potential risks of your work?\nWe see no concern about potential risks.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nThe Abstract provides the link to our code.\n□ B1. Did you cite the creators of artifacts you used?\nNot applicable. Left blank.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nIn the Abstract, a Github repository with documentation is released.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nAppendix A\nC □\u0013 Did you run computational experiments?\nSection 3\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 3\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1169\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 3\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 3\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n1170",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9588592052459717
    },
    {
      "name": "Computer science",
      "score": 0.8038030862808228
    },
    {
      "name": "Encoder",
      "score": 0.7663971185684204
    },
    {
      "name": "Transformer",
      "score": 0.7299010753631592
    },
    {
      "name": "Discriminative model",
      "score": 0.6656213402748108
    },
    {
      "name": "ENCODE",
      "score": 0.6320224404335022
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5377652049064636
    },
    {
      "name": "Language model",
      "score": 0.5351719856262207
    },
    {
      "name": "Natural language processing",
      "score": 0.5026464462280273
    },
    {
      "name": "Speech recognition",
      "score": 0.3465648293495178
    },
    {
      "name": "Engineering",
      "score": 0.07476642727851868
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ]
}