{
  "title": "Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks",
  "url": "https://openalex.org/W3017961061",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2790629656",
      "name": "Suchin Gururangan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2511492142",
      "name": "Ana Marasović",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A336169979",
      "name": "Swabha Swayamdipta",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2716373899",
      "name": "Kyle Lo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2800204358",
      "name": "Iz Beltagy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098223845",
      "name": "Doug Downey",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2183947846",
      "name": "Noah A. Smith",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2911681509",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2974353418",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2945808907",
    "https://openalex.org/W2927746189",
    "https://openalex.org/W2027731328",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3015759694",
    "https://openalex.org/W2946379889",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W2546368625",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W2963196639",
    "https://openalex.org/W2117278770",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2970283086",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W2970619710",
    "https://openalex.org/W2963691697",
    "https://openalex.org/W2963639288",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2971277088",
    "https://openalex.org/W2898700502",
    "https://openalex.org/W2950856799",
    "https://openalex.org/W3034640977",
    "https://openalex.org/W2964125718",
    "https://openalex.org/W2808556605",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1979839410",
    "https://openalex.org/W2971008823",
    "https://openalex.org/W2995154514",
    "https://openalex.org/W3106224367",
    "https://openalex.org/W2963718112",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W3015453090",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2937845937",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2970119519",
    "https://openalex.org/W3098649723",
    "https://openalex.org/W2970352191",
    "https://openalex.org/W2970217403",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2952826391",
    "https://openalex.org/W2251784502",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W1489992655",
    "https://openalex.org/W2914845972",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2149369282",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2929208326",
    "https://openalex.org/W2925618549",
    "https://openalex.org/W2963997607",
    "https://openalex.org/W2170571488"
  ],
  "abstract": "Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342–8360\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n8342\nDon’t Stop Pretraining: Adapt Language Models to Domains and Tasks\nSuchin Gururangan† Ana Marasovi´c†♦ Swabha Swayamdipta†\nKyle Lo† Iz Beltagy† Doug Downey† Noah A. Smith†♦\n†Allen Institute for Artiﬁcial Intelligence, Seattle, W A, USA\n♦Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, W A, USA\n{suching,anam,swabhas,kylel,beltagy,dougd,noah}@allenai.org\nAbstract\nLanguage models pretrained on text from a\nwide variety of sources form the foundation\nof today’s NLP. In light of the success of\nthese broad-coverage models, we investigate\nwhether it is still helpful to tailor a pretrained\nmodel to the domain of a target task. We\npresent a study across four domains (biomedi-\ncal and computer science publications, news,\nand reviews) and eight classiﬁcation tasks,\nshowing that a second phase of pretraining in-\ndomain ( domain-adaptive pretraining ) leads\nto performance gains, under both high- and\nlow-resource settings. Moreover, adapting\nto the task’s unlabeled data ( task-adaptive\npretraining) improves performance even after\ndomain-adaptive pretraining. Finally, we show\nthat adapting to a task corpus augmented us-\ning simple data selection strategies is an effec-\ntive alternative, especially when resources for\ndomain-adaptive pretraining might be unavail-\nable. Overall, we consistently ﬁnd that multi-\nphase adaptive pretraining offers large gains in\ntask performance.\n1 Introduction\nToday’s pretrained language models are trained on\nmassive, heterogeneous corpora (Raffel et al., 2019;\nYang et al., 2019). For instance, ROBERTA (Liu\net al., 2019) was trained on over 160GB of uncom-\npressed text, with sources ranging from English-\nlanguage encyclopedic and news articles, to literary\nworks and web content. Representations learned\nby such models achieve strong performance across\nmany tasks with datasets of varying sizes drawn\nfrom a variety of sources (e.g., Wang et al., 2018,\n2019). This leads us to ask whether a task’s textual\ndomain—a term typically used to denote a distribu-\ntion over language characterizing a given topic or\ngenre (such as “science” or “mystery novels”)—is\nstill relevant. Do the latest large pretrained mod-\nels work universally or is it still helpful to build\nFigure 1: An illustration of data distributions. Task\ndata is comprised of an observable task distribution,\nusually non-randomly sampled from a wider distribu-\ntion (light grey ellipsis) within an even larger target do-\nmain, which is not necessarily one of the domains in-\ncluded in the original LM pretraining domain – though\noverlap is possible. We explore the beneﬁts of contin-\nued pretraining on data from the task distribution and\nthe domain distribution.\nseparate pretrained models for speciﬁc domains?\nWhile some studies have shown the beneﬁt of\ncontinued pretraining on domain-speciﬁc unlabeled\ndata (e.g., Lee et al., 2019), these studies only con-\nsider a single domain at a time and use a language\nmodel that is pretrained on a smaller and less di-\nverse corpus than the most recent language mod-\nels. Moreover, it is not known how the beneﬁt of\ncontinued pretraining may vary with factors like\nthe amount of available labeled task data, or the\nproximity of the target domain to the original pre-\ntraining corpus (see Figure 1).\nWe address this question for one such high-\nperforming model, ROBERTA (Liu et al., 2019)\n(§2). We consider four domains (biomedical and\ncomputer science publications, news, and reviews;\n§3) and eight classiﬁcation tasks (two in each do-\nmain). For targets that are not already in-domain\nfor ROBERTA, our experiments show that contin-\n8343\nued pretraining on the domain (which we refer to as\ndomain-adaptive pretrainingor DAPT ) consistently\nimproves performance on tasks from the target do-\nmain, in both high- and low-resource settings.\nAbove, we consider domains deﬁned around gen-\nres and forums, but it is also possible to induce a\ndomain from a given corpus used for a task, such\nas the one used in supervised training of a model.\nThis raises the question of whether pretraining on\na corpus more directly tied to the task can fur-\nther improve performance. We study how domain-\nadaptive pretraining compares to task-adaptive pre-\ntraining, or TAPT , on a smaller but directly task-\nrelevant corpus: the unlabeled task dataset ( §4),\ndrawn from the task distribution. Task-adaptive\npretraining has been shown effective (Howard and\nRuder, 2018), but is not typically used with the\nmost recent models. We ﬁnd that TAPT provides\na large performance boost for ROBERTA, with or\nwithout domain-adaptive pretraining.\nFinally, we show that the beneﬁts from task-\nadaptive pretraining increase when we have addi-\ntional unlabeled data from the task distribution that\nhas been manually curated by task designers or an-\nnotators. Inspired by this success, we propose ways\nto automatically select additional task-relevant un-\nlabeled text, and show how this improves perfor-\nmance in certain low-resource cases ( §5). On all\ntasks, our results using adaptive pretraining tech-\nniques are competitive with the state of the art.\nIn summary, our contributions include:\n•a thorough analysis of domain- and task-\nadaptive pretraining across four domains and\neight tasks, spanning low- and high-resource\nsettings;\n•an investigation into the transferability of\nadapted LMs across domains and tasks; and\n•a study highlighting the importance of pre-\ntraining on human-curated datasets, and a sim-\nple data selection strategy to automatically\napproach this performance.\nOur code as well as pretrained models for multiple\ndomains and tasks are publicly available.1\n2 Background: Pretraining\nLearning for most NLP research systems since\n2018 consists of training in two stages. First, a\nneural language model (LM), often with millions\nof parameters, is trained on large unlabeled cor-\n1https://github.com/allenai/\ndont-stop-pretraining\npora. The word (or wordpiece; Wu et al. 2016)\nrepresentations learned in the pretrained model are\nthen reused in supervised training for a downstream\ntask, with optional updates (ﬁne-tuning) of the rep-\nresentations and network from the ﬁrst stage.\nOne such pretrained LM is ROBERTA (Liu\net al., 2019), which uses the same transformer-\nbased architecture (Vaswani et al., 2017) as its\npredecessor, BERT (Devlin et al., 2019). It is\ntrained with a masked language modeling objec-\ntive (i.e., cross-entropy loss on predicting randomly\nmasked tokens). The unlabeled pretraining corpus\nfor ROBERTA contains over 160 GB of uncom-\npressed raw text from different English-language\ncorpora (see Appendix §A.1). ROBERTA attains\nbetter performance on an assortment of tasks than\nits predecessors, making it our baseline of choice.\nAlthough ROBERTA’s pretraining corpus is de-\nrived from multiple sources, it has not yet been\nestablished if these sources are diverse enough to\ngeneralize to most of the variation in the English\nlanguage. In other words, we would like to un-\nderstand what is out of ROBERTA’s domain. To-\nwards this end, we explore further adaptation by\ncontinued pretraining of this large LM into two\ncategories of unlabeled data: (i) large corpora of\ndomain-speciﬁc text (§3), and (ii) available unla-\nbeled data associated with a given task (§4).\n3 Domain-Adaptive Pretraining\nOur approach to domain-adaptive pretraining\n(DAPT ) is straightforward—we continue pretrain-\ning ROBERTA on a large corpus of unlabeled\ndomain-speciﬁc text. The four domains we focus\non are biomedical (BIOMED) papers, computer sci-\nence (CS) papers, newstext from REAL NEWS , and\nAMAZON reviews. We choose these domains be-\ncause they have been popular in previous work, and\ndatasets for text classiﬁcation are available in each.\nTable 1 lists the speciﬁcs of the unlabeled datasets\nin all four domains, as well asROBERTA’s training\ncorpus.1\n3.1 Analyzing Domain Similarity\nBefore performing DAPT , we attempt to quantify\nthe similarity of the target domain to ROBERTA’s\npretraining domain. We consider domain vocab-\nularies containing the top 10K most frequent uni-\ngrams (excluding stopwords) in comparably sized\n1For BIOMED and CS, we used an internal version of\nS2ORC that contains papers that cannot be released due to\ncopyright restrictions.\n8344\nDomain Pretraining Corpus # Tokens Size LROB. LDAPT\nBIOMED 2.68M full-text papers from S2ORC (Lo et al., 2020) 7.55B 47GB 1.32 0.99\nCS 2.22M full-text papers from S2ORC (Lo et al., 2020) 8.10B 48GB 1.63 1.34\nNEWS 11.90M articles from REAL NEWS (Zellers et al., 2019) 6.66B 39GB 1.08 1.16\nREVIEWS 24.75M AMAZON reviews (He and McAuley, 2016) 2.11B 11GB 2.10 1.93\nROBERTA (baseline) see Appendix §A.1 N/A 160GB ‡1.19 -\nTable 1: List of the domain-speciﬁc unlabeled datasets. In columns 5 and 6, we report R OBERTA’s masked LM\nloss on 50K randomly sampled held-out documents from each domain before ( LROB. ) and after ( LDAPT ) DAPT\n(lower implies a better ﬁt on the sample). ‡indicates that the masked LM loss is estimated on data sampled from\nsources similar to ROBERTA’s pretraining corpus.\nPT News Reviews BioMed CS\nPT\nNews\nReviews\nBioMed\nCS\n100.0 54.1 34.5 27.3 19.2\n54.1 100.0 40.0 24.9 17.3\n34.5 40.0 100.0 18.3 12.7\n27.3 24.9 18.3 100.0 21.4\n19.2 17.3 12.7 21.4 100.0\nFigure 2: V ocabulary overlap (%) between do-\nmains. PT denotes a sample from sources similar to\nROBERTA’s pretraining corpus. V ocabularies for each\ndomain are created by considering the top 10K most\nfrequent words (excluding stopwords) in documents\nsampled from each domain.\nrandom samples of held-out documents in each do-\nmain’s corpus. We use 50K held-out documents\nfor each domain other than REVIEWS , and 150K\nheld-out documents in REVIEWS , since they are\nmuch shorter. We also sample 50K documents from\nsources similar to ROBERTA’s pretraining corpus\n(i.e., BOOK CORPUS , STORIES , WIKIPEDIA , and\nREAL NEWS ) to construct the pretraining domain\nvocabulary, since the original pretraining corpus\nis not released. Figure 2 shows the vocabulary\noverlap across these samples. We observe that\nROBERTA’s pretraining domain has strong vocab-\nulary overlap with NEWS and REVIEWS , while\nCS and BIOMED are far more dissimilar to the\nother domains. This simple analysis suggests the\ndegree of beneﬁt to be expected by adaptation of\nROBERTA to different domains—the more dissim-\nilar the domain, the higher the potential for DAPT .\n3.2 Experiments\nOur LM adaptation follows the settings prescribed\nfor training ROBERTA. We train ROBERTA on\neach domain for 12.5K steps, which amounts to\nsingle pass on each domain dataset, on a v3-8 TPU;\nsee other details in Appendix B. This second phase\nof pretraining results in four domain-adapted LMs,\none for each domain. We present the masked LM\nloss of ROBERTA on each domain before and after\nDAPT in Table 1. We observe that masked LM loss\ndecreases in all domains except NEWS after DAPT ,\nwhere we observe a marginal increase. We discuss\ncross-domain masked LM loss in Appendix §E.\nUnder each domain, we consider two text clas-\nsiﬁcation tasks, as shown in Table 2. Our tasks\nrepresent both high- and low-resource ( ≤5K la-\nbeled training examples, and no additional unla-\nbeled data) settings. For HYPER PARTISAN , we use\nthe data splits from Beltagy et al. (2020). For RCT,\nwe represent all sentences in one long sequence for\nsimultaneous prediction.\nBaseline As our baseline, we use an off-the-shelf\nROBERTA-base model and perform supervised\nﬁne-tuning of its parameters for each classiﬁcation\ntask. On average, ROBERTA is not drastically be-\nhind the state of the art (details in Appendix §A.2),\nand serves as a good baseline since it provides a\nsingle LM to adapt to different domains.\nClassiﬁcation Architecture Following standard\npractice (Devlin et al., 2019) we pass the ﬁnal layer\n[CLS] token representation to a task-speciﬁc feed-\nforward layer for prediction (see Table 14 in Ap-\npendix for more hyperparameter details).\nResults Test results are shown under the DAPT\ncolumn of Table 3 (see Appendix §C for valida-\ntion results). We observe that DAPT improves\nover ROBERTA in all domains. For BIOMED,\nCS, and REVIEWS , we see consistent improve-\n8345\nDomain Task Label Type Train (Lab.) Train (Unl.) Dev. Test Classes\nBIOMED CHEM PROT relation classiﬁcation 4169 - 2427 3469 13\n†RCT abstract sent. roles 18040 - 30212 30135 5\nCS ACL-ARC citation intent 1688 - 114 139 6\nSCIERC relation classiﬁcation 3219 - 455 974 7\nNEWS HYPER PARTISAN partisanship 515 5000 65 65 2\n†AGNEWS topic 115000 - 5000 7600 4\nREVIEWS\n†HELPFULNESS review helpfulness 115251 - 5000 25000 2\n†IMDB review sentiment 20000 50000 5000 25000 2\nTable 2: Speciﬁcations of the various target task datasets. †indicates high-resource settings. Sources: C HEM PROT\n(Kringelum et al., 2016), RCT (Dernoncourt and Lee, 2017), ACL-ARC (Jurgens et al., 2018), S CIERC (Luan\net al., 2018), H YPER PARTISAN (Kiesel et al., 2019), AGN EWS (Zhang et al., 2015), H ELPFULNESS (McAuley\net al., 2015), IMDB (Maas et al., 2011).\nDom. Task R OBA. DAPT ¬DAPT\nBM CHEM PROT 81.91.0 84.20.2 79.41.3\n†RCT 87.2 0.1 87.60.1 86.90.1\nCS ACL-ARC 63.05.8 75.42.5 66.44.1\nSCIERC 77.3 1.9 80.81.5 79.20.9\nNEWS\nHYP. 86.6 0.9 88.25.9 76.44.9\n†AGNEWS 93.90.2 93.90.2 93.50.2\nREV.\n†HELPFUL . 65.13.4 66.51.4 65.12.8\n†IMDB 95.0 0.2 95.40.2 94.10.4\nTable 3: Comparison of R OBERTA (ROBA.) and\nDAPT to adaptation to an irrelevant domain ( ¬\nDAPT ). Reported results are test macro- F1, except for\nCHEM PROT and RCT, for which we report micro- F1,\nfollowing Beltagy et al. (2019). We report averages\nacross ﬁve random seeds, with standard deviations as\nsubscripts. †indicates high-resource settings. Best task\nperformance is boldfaced. See §3.3 for our choice of\nirrelevant domains.\nments over ROBERTA, demonstrating the beneﬁt\nof DAPT when the target domain is more distant\nfrom ROBERTA’s source domain. The pattern is\nconsistent across high- and low- resource settings.\nAlthough DAPT does not increase performance on\nAGNEWS , the beneﬁt we observe in HYPER PAR-\nTISAN suggests that DAPT may be useful even for\ntasks that align more closely with ROBERTA’s\nsource domain.\n3.3 Domain Relevance for DAPT\nAdditionally, we compare DAPT against a setting\nwhere for each task, we adapt the LM to a domain\noutside the domain of interest. This controls for the\ncase in which the improvements over ROBERTA\nmight be attributed simply to exposure to more data,\nregardless of the domain. In this setting, for NEWS ,\nwe use a CS LM; for REVIEWS , a BIOMED LM;\nfor CS, a NEWS LM; for BIOMED, a REVIEWS\nLM. We use the vocabulary overlap statistics in\nFigure 2 to guide these choices.\nOur results are shown in Table 3, where the last\ncolumn (¬DAPT ) corresponds to this setting. For\neach task, DAPT signiﬁcantly outperforms adapting\nto an irrelevant domain, suggesting the importance\nof pretraining on domain-relevant data. Further-\nmore, we generally observe that ¬DAPT results\nin worse performance than even ROBERTA on\nend-tasks. Taken together, these results indicate\nthat in most settings, exposure to more data with-\nout considering domain relevance is detrimental\nto end-task performance. However, there are two\ntasks (SCIERC and ACL-ARC ) in which ¬DAPT\nmarginally improves performance over ROBERTA.\nThis may suggest that in some cases, continued pre-\ntraining on any additional data is useful, as noted\nin Baevski et al. (2019).\n3.4 Domain Overlap\nOur analysis of DAPT is based on prior intuitions\nabout how task data is assigned to speciﬁc domains.\nFor instance, to perform DAPT for HELPFULNESS ,\nwe only adapt to AMAZON reviews, but not to any\nREAL NEWS articles. However, the gradations in\nFigure 2 suggest that the boundaries between do-\nmains are in some sense fuzzy; for example, 40%\nof unigrams are shared between REVIEWS and\nNEWS . As further indication of this overlap, we\nalso qualitatively identify documents that overlap\ncross-domain: in Table 4, we showcase reviews\nand REAL NEWS articles that are similar to these\nreviews (other examples can be found in Appendix\n§D). In fact, we ﬁnd that adapting ROBERTA to\n8346\nIMDB review R EALNEWS article\n“The Shop Around the Corner“ is one of thegreat ﬁlms from director\nErnst Lubitsch . In addition to the talents ofJames Stewart andMargaret Sullavan ,\nit’s ﬁlled with a terriﬁc cast of top character actors such as Frank Morgan and Felix\nBressart. [...] The makers of“You’ve Got Mail“claim their ﬁlm to be aremake, but\nthat’s just nothing but a lot of inﬂated self praise. Anyway, if you have an affection for\nromantic comedies of the 1940 ’s, you’ll ﬁnd“The Shop Around the Corner“ to be\nnothing short of wonderful. Just as good with repeat viewings.\n[...] Three great festive ﬁlms... The Shop Around\nthe Corner (1940) Delightful Comedy by Ernst\nLubitsch stars James Stewart and Margaret Sulla-\nvan falling in love at Christmas. Remade as\nYou’ve Got Mail. [...]\nHELPFULNESSreview R EALNEWS article\nSimply the Best! I’ve owned countless Droids and iPhones, but this one destroys them\nall. Samsung really nailed it with this one, extremelyfast , very pocketable, gorgeous\ndisplay , exceptionalbattery life , good audio quality, perfect GPS & WiFi\nperformance, transparent status bar,battery percentage, ability to turn off soft key\nlights, superbcamera for a smartphone and more! [...]\nWe’re living in a world with a new Samsung.\n[...] more on battery life later [...] Exposure is\nusually spot on and focusing is very fast. [...]\nThe design, display, camera and performance\nare all best in class, and the phone feels smaller\nthan it looks. [...]\nTable 4: Examples that illustrate how some domains might have overlaps with others, leading to unexpected\npositive transfer. We highlight expressions in the reviews that are also found in the REAL NEWS articles.\nNEWS not as harmful to its performance on RE-\nVIEWS tasks (DAPT on NEWS achieves 65.52.3 on\nHELPFULNESS and 95.00.1 on IMDB).\nAlthough this analysis is by no means compre-\nhensive, it indicates that the factors that give rise to\nobservable domain differences are likely not mu-\ntually exclusive. It is possible that pretraining be-\nyond conventional domain boundaries could result\nin more effective DAPT ; we leave this investiga-\ntion to future work. In general, the provenance of\ndata, including the processes by which corpora are\ncurated, must be kept in mind when designing pre-\ntraining procedures and creating new benchmarks\nthat test out-of-domain generalization abilities.\n4 Task-Adaptive Pretraining\nDatasets curated to capture speciﬁc tasks of inter-\nest tend to cover only a subset of the text avail-\nable within the broader domain. For example,\nthe CHEM PROT dataset for extracting relations be-\ntween chemicals and proteins focuses on abstracts\nof recently-published, high-impact articles from\nhand-selected PubMed categories (Krallinger et al.,\n2017, 2015). We hypothesize that such cases where\nthe task data is a narrowly-deﬁned subset of the\nbroader domain, pretraining on the task dataset\nitself or data relevant to the task may be helpful.\nTask-adaptive pretraining(TAPT) refers to pre-\ntraining on the unlabeled training set for a given\ntask; prior work has shown its effectiveness (e.g.\nHoward and Ruder, 2018). Compared to domain-\nadaptive pretraining (DAPT ; §3), the task-adaptive\napproach strikes a different trade-off: it uses a far\nsmaller pretraining corpus, but one that is much\nmore task-relevant (under the assumption that the\ntraining set represents aspects of the task well).\nThis makes TAPT much less expensive to run than\nDAPT , and as we show in our experiments, the per-\nformance of TAPT is often competitive with that of\nDAPT .\n4.1 Experiments\nSimilar to DAPT , task-adaptive pretraining consists\nof a second phase of pretraining ROBERTA, but\nonly on the available task-speciﬁc training data. In\ncontrast to DAPT , which we train for 12.5K steps,\nwe perform TAPT for 100 epochs. We artiﬁcially\naugment each dataset by randomly masking differ-\nent words (using the masking probability of 0.15)\nacross epochs. As in our DAPT experiments, we\npass the ﬁnal layer [CLS] token representation to\na task-speciﬁc feedforward layer for classiﬁcation\n(see Table 14 in Appendix for more hyperparameter\ndetails).\nOur results are shown in the TAPT column of Ta-\nble 5. TAPT consistently improves the ROBERTA\nbaseline for all tasks across domains. Even on the\nnews domain, which was part of ROBERTA pre-\ntraining corpus, TAPT improves over ROBERTA,\nshowcasing the advantage of task adaptation. Par-\nticularly remarkable are the relative differences be-\ntween TAPT and DAPT . DAPT is more resource in-\ntensive (see Table 9 in §5.3), but TAPT manages to\nmatch its performance in some of the tasks, such as\nSCIERC . In RCT , HYPER PARTISAN , AGNEWS ,\nHELPFULNESS , and IMDB , the results even ex-\nceed those of DAPT , highlighting the efﬁcacy of\nthis cheaper adaptation technique.\n8347\nAdditional Pretraining Phases\nDomain Task R OBERTA DAPT TAPT DAPT + TAPT\nBIOMED CHEM PROT 81.91.0 84.20.2 82.60.4 84.40.4\n†RCT 87.2 0.1 87.60.1 87.70.1 87.80.1\nCS ACL-ARC 63.0 5.8 75.42.5 67.41.8 75.63.8\nSCIERC 77.3 1.9 80.81.5 79.31.5 81.31.8\nNEWS HYPER PARTISAN 86.60.9 88.25.9 90.45.2 90.06.6\n†AGNEWS 93.90.2 93.90.2 94.50.1 94.60.1\nREVIEWS\n†HELPFULNESS 65.13.4 66.51.4 68.51.9 68.71.8\n†IMDB 95.0 0.2 95.40.1 95.50.1 95.60.1\nTable 5: Results on different phases of adaptive pretraining compared to the baseline R OBERTA (col. 1). Our\napproaches are DAPT (col. 2, §3), TAPT (col. 3, §4), and a combination of both (col. 4). Reported results follow the\nsame format as Table 3. State-of-the-art results we can compare to: C HEM PROT (84.6), RCT (92.9), ACL-ARC\n(71.0), SCIERC (81.8), H YPER PARTISAN (94.8), AGN EWS (95.5), IMDB (96.2); references in §A.2.\nBIOMED RCT C HEMPROT\nTAPT 87.70.1 82.60.5\nTransfer-TAPT 87.10.4 (↓0.6) 80.4 0.6 (↓2.2)\nNEWS HYPERPARTISAN AGNEWS\nTAPT 89.99.5 94.50.1\nTransfer-TAPT 82.27.7 (↓7.7) 93.9 0.2 (↓0.6)\nCS ACL-ARC S CIERC\nTAPT 67.41.8 79.31.5\nTransfer-TAPT 64.12.7 (↓3.3) 79.1 2.5 (↓0.2)\nREVIEWS HELPFULNESS IMDB\nTAPT 68.51.9 95.70.1\nTransfer-TAPT 65.02.6 (↓3.5) 95.0 0.1 (↓0.7)\nTable 6: Though TAPT is effective (Table 5), it is harmful when applied across tasks. These ﬁndings illustrate\ndifferences in task distributions within a domain.\nCombined DAPT and TAPT We investigate the\neffect of using both adaptation techniques together.\nWe begin with ROBERTA and apply DAPT then\nTAPT under this setting. The three phases of pre-\ntraining add up to make this the most computation-\nally expensive of all our settings (see Table 9). As\nexpected, combined domain- and task-adaptive pre-\ntraining achieves the best performance on all tasks\n(Table 5).2\nOverall, our results show that DAPT followed by\nTAPT achieves the best of both worlds of domain\nand task awareness, yielding the best performance.\nWhile we speculate that TAPT followed by DAPT\nwould be susceptible to catastrophic forgetting of\nthe task-relevant corpus (Yogatama et al., 2019), al-\nternate methods of combining the procedures may\nresult in better downstream performance. Future\nwork may explore pretraining with a more sophisti-\ncated curriculum of domain and task distributions.\n2Results on HYPER PARTISAN match those of TAPT, within\na standard deviation arising from the ﬁve seeds.\nCross-Task Transfer We complete the compari-\nson between DAPT and TAPT by exploring whether\nadapting to one task transfers to other tasks in the\nsame domain. For instance, we further pretrain\nthe LM using the RCT unlabeled data, ﬁne-tune it\nwith the CHEM PROT labeled data, and observe the\neffect. We refer to this setting as Transfer- TAPT.\nOur results for tasks in all four domains are shown\nin Table 6. We see that TAPT optimizes for single\ntask performance, to the detriment of cross-task\ntransfer. These results demonstrate that data distri-\nbutions of tasks within a given domain might differ.\nFurther, this could also explain why adapting only\nto a broad domain is not sufﬁcient, and why TAPT\nafter DAPT is effective.\n5 Augmenting Training Data for\nTask-Adaptive Pretraining\nIn §4, we continued pretraining the LM for task\nadaptation using only the training data for a super-\nvised task. Inspired by the success of TAPT, we\nnext investigate another setting where a larger pool\nof unlabeled data from the task distribution exists,\n8348\nPretraining BIOMED NEWS REVIEWS\nRCT-500 HYP. IMDB †\nTAPT 79.81.4 90.45.2 95.50.1\nDAPT+ TAPT 83.00.3 90.06.6 95.60.1\nCurated-TAPT 83.40.3 89.99.5 95.70.1\nDAPT+ Curated-TAPT 83.80.5 92.13.6 95.80.1\nTable 7: Mean test set macro- F1 (for H YP. and\nIMDB) and micro- F1 (for RCT-500), with Curated-\nTAPT across ﬁve random seeds, with standard devia-\ntions as subscripts. †indicates high-resource settings.\ntypically curated by humans.\nWe explore two scenarios. First, for three tasks\n(RCT, HYPER PARTISAN , and IMDB ) we use this\nlarger pool of unlabeled data from an available\nhuman-curated corpus ( §5.1). Next, we explore\nretrieving related unlabeled data for TAPT, from a\nlarge unlabeled in-domain corpus, for tasks where\nextra human-curated data is unavailable (§5.2).\n5.1 Human Curated- TAPT\nDataset creation often involves collection of a large\nunlabeled corpus from known sources. This corpus\nis then downsampled to collect annotations, based\non the annotation budget. The larger unlabeled cor-\npus is thus expected to have a similar distribution\nto the task’s training data. Moreover, it is usually\navailable. We explore the role of such corpora in\ntask-adaptive pretraining.\nData We simulate a low-resource setting RCT -\n500, by downsampling the training data of theRCT\ndataset to 500 examples (out of 180K available),\nand treat the rest of the training data as unlabeled.\nThe HYPER PARTISAN shared task (Kiesel et al.,\n2019) has two tracks: low- and high-resource. We\nuse 5K documents from the high-resource setting as\nCurated-TAPT unlabeled data and the original low-\nresource training documents for task ﬁne-tuning.\nFor IMDB , we use the extra unlabeled data man-\nually curated by task annotators, drawn from the\nsame distribution as the labeled data (Maas et al.,\n2011).\nResults We compare Curated-TAPT to TAPT and\nDAPT + TAPT in Table 7. Curated- TAPT further\nimproves our prior results from §4 across all three\ndatasets. Applying Curated-TAPT after adapting to\nthe domain results in the largest boost in perfor-\nmance on all tasks; in HYPER PARTISAN , DAPT\n+ Curated- TAPT is within standard deviation of\nCurated-TAPT. Moreover, curated-TAPT achieves\nFigure 3: An illustration of automated data selec-\ntion ( §5.2). We map unlabeled C HEM PROT and 1M\nBIOMED sentences to a shared vector space using the\nVAMPIRE model trained on these sentences. Then,\nfor each C HEM PROT sentence, we identify k nearest\nneighbors, from the BIOMED domain.\nPretraining BIOMED CS\nCHEMPROT RCT-500 ACL-ARC\nROBERTA 81.91.0 79.30.6 63.05.8\nTAPT 82.60.4 79.81.4 67.41.8\nRAND-TAPT 81.90.6 80.60.4 69.73.4\n50NN-TAPT 83.30.7 80.80.6 70.72.8\n150NN-TAPT 83.20.6 81.20.8 73.32.7\n500NN-TAPT 83.30.7 81.70.4 75.51.9\nDAPT 84.20.2 82.50.5 75.42.5\nTable 8: Mean test set micro- F1 (for C HEM PROT\nand RCT) and macro- F1 (for ACL-ARC), across ﬁve\nrandom seeds, with standard deviations as subscripts,\ncomparing RAND -TAPT (with 50 candidates) and kNN-\nTAPT selection. Neighbors of the task data are selected\nfrom the domain data.\n95% of the performance of DAPT + TAPT with the\nfully labeled RCT corpus (Table 5) with only 0.3%\nof the labeled data. These results suggest that curat-\ning large amounts of data from the task distribution\nis extremely beneﬁcial to end-task performance.\nWe recommend that task designers release a large\npool of unlabeled task data for their tasks to aid\nmodel adaptation through pretraining.\n5.2 Automated Data Selection for TAPT\nConsider a low-resource scenario without access to\nlarge amounts of unlabeled data to adequately bene-\nﬁt from TAPT, as well as absence of computational\nresources necessary for DAPT (see Table 9 for de-\ntails of computational requirements for different\npretraining phases). We propose simple unsuper-\n8349\nvised methods to retrieve unlabeled text that aligns\nwith the task distribution, from a large in-domain\ncorpus. Our approach ﬁnds task-relevant data from\nthe domain by embedding text from both the task\nand domain in a shared space, then selects candi-\ndates from the domain based on queries using the\ntask data. Importantly, the embedding method must\nbe lightweight enough to embed possibly millions\nof sentences in a reasonable time.\nGiven these constraints, we employ VAMPIRE\n(Gururangan et al., 2019; Figure 3), a lightweight\nbag-of-words language model. We pretrain VAM-\nPIRE on a large deduplicated3 sample of the do-\nmain (1M sentences) to obtain embeddings of the\ntext from both the task and domain sample. We\nthen select k candidates of each task sentence from\nthe domain sample, in embeddings space. Candi-\ndates are selected (i) via nearest neighbors selection\n(kNN-TAPT)4, or (ii) randomly (RAND -TAPT). We\ncontinue pretraining ROBERTA on this augmented\ncorpus with both the task data (as in TAPT) as well\nas the selected candidate pool.\nResults Results in Table 8 show that kNN-TAPT\noutperforms TAPT for all cases. RAND -TAPT is gen-\nerally worse than kNN-TAPT, but within a standard\ndeviation arising from 5 seeds for RCT and ACL-\nARC . As we increase k, kNN-TAPT performance\nsteadily increases, and approaches that of DAPT .\nAppendix F shows examples of nearest neighbors\nof task data. Future work might consider a closer\nstudy of kNN-TAPT, more sophisticated data selec-\ntion methods, and the tradeoff between the diversity\nand task relevance of selected examples.\n5.3 Computational Requirements\nThe computational requirements for all our adap-\ntation techniques on RCT-500 in the BIOMED do-\nmain in Table 9. TAPT is nearly 60 times faster\nto train than DAPT on a single v3-8 TPU and stor-\nage requirements for DAPT on this task are 5.8M\ntimes that of TAPT. Our best setting of DAPT +\nTAPT amounts to three phases of pretraining, and at\nﬁrst glance appears to be very expensive. However,\nonce the LM has been adapted to a broad domain, it\ncan be reused for multiple tasks within that domain,\nwith only a single additional TAPT phase per task.\nWhile Curated-TAPT tends to achieve the best cost-\n3We deduplicated this set to limit computation, since dif-\nferent sentences can share neighbors.\n4We use a ﬂat search index with cosine similarity between\nembeddings with the FAISS (Johnson et al., 2019) library.\nPretraining Steps Docs. Storage F1\nROBERT A - - - 79.3 0.6\nTAPT 0.2K 500 80KB 79.8 1.4\n50NN -TAPT 1.1K 24K 3MB 80.8 0.6\n150NN -TAPT 3.2K 66K 8MB 81.2 0.8\n500NN -TAPT 9.0K 185K 24MB 81.7 0.4\nCurated- TAPT 8.8K 180K 27MB 83.40.3\nDAPT 12.5K 25M 47GB 82.5 0.5\nDAPT + TAPT 12.6K 25M 47GB 83.0 0.3\nTable 9: Computational requirements for adapting to\nthe RCT-500 task, comparing DAPT (§3) and the vari-\nous TAPT modiﬁcations described in §4 and §5.\nbeneﬁt ratio in this comparison, one must also take\ninto account the cost of curating large in-domain\ndata. Automatic methods such as kNN-TAPT are\nmuch cheaper than DAPT .\n6 Related Work\nTransfer learning for domain adaptation\nPrior work has shown the beneﬁt of continued\npretraining in domain (Alsentzer et al., 2019;\nChakrabarty et al., 2019; Lee et al., 2019). 5 We\nhave contributed further investigation of the effects\nof a shift between a large, diverse pretraining\ncorpus and target domain on task performance.\nOther studies (e.g., Huang et al., 2019) have\ntrained language models (LMs) in their domain\nof interest, from scratch. In contrast, our work\nexplores multiple domains, and is arguably more\ncost effective, since we continue pretraining an\nalready powerful LM.\nTask-adaptive pretraining Continued pretrain-\ning of a LM on the unlabeled data of a given task\n(TAPT) has been show to be beneﬁcial for end-\ntask performance (e.g. in Howard and Ruder, 2018;\nPhang et al., 2018; Sun et al., 2019). In the pres-\nence of domain shift between train and test data\ndistributions of the same task, domain-adaptive pre-\ntraining (DAPT ) is sometimes used to describe what\nwe term TAPT (Logeswaran et al., 2019; Han and\nEisenstein, 2019). Related approaches include lan-\nguage modeling as an auxiliary objective to task\nclassiﬁer ﬁne-tuning (Chronopoulou et al., 2019;\nRadford et al., 2018) or consider simple syntactic\nstructure of the input while adapting to task-speciﬁc\n5In contrast, Peters et al. (2019) ﬁnd that the Jensen-\nShannon divergence on term distributions between BERT’s\npretraining corpora and each MULTI NLI domain (Williams\net al., 2018) does not predict its performance, though this\nmight be an isolated ﬁnding speciﬁc to the MultiNLI dataset.\n8350\nTraining Data\nDomain\n(Unlabeled)\nTask\n(Unlabeled)\nTask\n(Labeled)\nROBERTA ✓\nDAPT ✓ ✓\nTAPT ✓ ✓\nDAPT+ TAPT ✓ ✓ ✓\nkNN-TAPT (Subset) ✓ ✓\nCurated-TAPT (Extra) ✓\nTable 10: Summary of strategies for multi-phase pre-\ntraining explored in this paper.\ndata (Swayamdipta et al., 2019). We compareDAPT\nand TAPT as well as their interplay with respect to\ndataset size for continued pretraining (hence, ex-\npense of more rounds of pretraining), relevance to\na data sample of a given task, and transferability to\nother tasks and datasets. See Table 11 in Appendix\n§A for a summary of multi-phase pretraining strate-\ngies from related work.\nData selection for transfer learning Selecting\ndata for transfer learning has been explored in NLP\n(Moore and Lewis, 2010; Ruder and Plank, 2017;\nZhang et al., 2019, among others). Dai et al. (2019)\nfocus on identifying the most suitable corpus to\npretrain a LM from scratch, for a single task: NER,\nwhereas we select relevant examples for various\ntasks in §5.2. Concurrent to our work, Aharoni and\nGoldberg (2020) propose data selection methods\nfor NMT based on cosine similarity in embedding\nspace, using DISTIL BERT (Sanh et al., 2019) for\nefﬁciency. In contrast, we use VAMPIRE , and\nfocus on augmenting TAPT data for text classiﬁ-\ncation tasks. Khandelwal et al. (2020) introduced\nkNN-LMs that allows easy domain adaptation of\npretrained LMs by simply adding a datastore per\ndomain and no further training; an alternative to\nintegrate domain information in an LM. Our study\nof human-curated data §5.1 is related to focused\ncrawling (Chakrabarti et al., 1999) for collection of\nsuitable data, especially with LM reliance (Remus\nand Biemann, 2016).\nWhat is a domain? Despite the popularity of\ndomain adaptation techniques, most research and\npractice seems to use an intuitive understanding of\ndomains. A small body of work has attempted to\naddress this question (Lee, 2001; Eisenstein et al.,\n2014; van der Wees et al., 2015; Plank, 2016; Ruder\net al., 2016, among others). For instance, Aharoni\nand Goldberg (2020) deﬁne domains by implicit\nclusters of sentence representations in pretrained\nLMs. Our results show that DAPT and TAPT com-\nplement each other, which suggests a spectra of\ndomains deﬁned around tasks at various levels of\ngranularity (e.g., Amazon reviews for a speciﬁc\nproduct, all Amazon reviews, all reviews on the\nweb, the web).\n7 Conclusion\nWe investigate several variations for adapting pre-\ntrained LMs to domains and tasks within those do-\nmains, summarized in Table 10. Our experiments\nreveal that even a model of hundreds of millions of\nparameters struggles to encode the complexity of\na single textual domain, let alone all of language.\nWe show that pretraining the model towards a spe-\nciﬁc task or small corpus can provide signiﬁcant\nbeneﬁts. Our ﬁndings suggest it may be valuable\nto complement work on ever-larger LMs with par-\nallel efforts to identify and use domain- and task-\nrelevant corpora to specialize models. While our\nresults demonstrate how these approaches can im-\nprove ROBERTA, a powerful LM, the approaches\nwe studied are general enough to be applied to\nany pretrained LM. Our work points to numerous\nfuture directions, such as better data selection for\nTAPT, efﬁcient adaptation large pretrained language\nmodels to distant domains, and building reusable\nlanguage models after adaptation.\nAcknowledgments\nThe authors thank Dallas Card, Mark Neumann,\nNelson Liu, Eric Wallace, members of the Al-\nlenNLP team, and anonymous reviewers for help-\nful feedback, and Arman Cohan for providing data.\nThis research was supported in part by the Ofﬁce\nof Naval Research under the MURI grant N00014-\n18-1-2670.\nReferences\nRoee Aharoni and Yoav Goldberg. 2020. Unsupervised\ndomain clusters in pretrained language models. In\nACL. To appear.\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clin-\nical BERT embeddings. In Proceedings of the 2nd\nClinical Natural Language Processing Workshop.\nAlexei Baevski, Sergey Edunov, Yinhan Liu, Luke\nZettlemoyer, and Michael Auli. 2019. Cloze-driven\npretraining of self-attention networks. In EMNLP.\n8351\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientiﬁc text.\nIn EMNLP.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv:2004.05150.\nSoumen Chakrabarti, Martin van den Berg, and Byron\nDom. 1999. Focused Crawling: A New Approach to\nTopic-Speciﬁc Web Resource Discovery. Comput.\nNetworks, 31:1623–1640.\nTuhin Chakrabarty, Christopher Hidey, and Kathy\nMcKeown. 2019. IMHO ﬁne-tuning improves claim\ndetection. In NAACL.\nCiprian Chelba, Tomas Mikolov, Michael Schuster,\nQi Ge, Thorsten Brants, Phillipp Koehn, and Tony\nRobinson. 2014. One billion word benchmark for\nmeasuring progress in statistical language modeling.\nIn INTERSPEECH.\nAlexandra Chronopoulou, Christos Baziotis, and\nAlexandros Potamianos. 2019. An embarrassingly\nsimple approach for transfer learning from pre-\ntrained language models. In NAACL.\nArman Cohan, Iz Beltagy, Daniel King, Bhavana Dalvi,\nand Dan Weld. 2019. Pretrained language models\nfor sequential sentence classiﬁcation. In EMNLP.\nXiang Dai, Sarvnaz Karimi, Ben Hachey, and Cecile\nParis. 2019. Using similarity measures to select pre-\ntraining data for NER. In NAACL.\nFranck Dernoncourt and Ji Young Lee. 2017. Pubmed\n200k RCT: a dataset for sequential sentence classiﬁ-\ncation in medical abstracts. In IJCNLP.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL.\nJesse Dodge, Suchin Gururangan, Dallas Card, Roy\nSchwartz, and Noah A Smith. 2019. Show your\nwork: Improved reporting of experimental results.\nIn EMNLP.\nJacob Eisenstein, Brendan O’connor, Noah A. Smith,\nand Eric P. Xing. 2014. Diffusion of lexical change\nin social media. PloS ONE.\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe-\nters, Michael Schmitz, and Luke Zettlemoyer. 2018.\nAllenNLP: A deep semantic natural language pro-\ncessing platform. In NLP-OSS.\nAaron Gokaslan and Vanya Cohen. 2019. OpenWeb-\nText Corpus.\nSuchin Gururangan, Tam Dang, Dallas Card, and\nNoah A. Smith. 2019. Variational pretraining for\nsemi-supervised text classiﬁcation. In ACL.\nXiaochuang Han and Jacob Eisenstein. 2019. Unsuper-\nvised domain adaptation of contextualized embed-\ndings for sequence labeling. In EMNLP.\nRuining He and Julian McAuley. 2016. Ups and downs:\nModeling the visual evolution of fashion trends with\none-class collaborative ﬁltering. In WWW.\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremen-\ntal parsing.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nACL.\nKexin Huang, Jaan Altosaar, and Rajesh Ranganath.\n2019. ClinicalBERT: Modeling clinical notes and\npredicting hospital readmission. arXiv:1904.05342.\nJeff Johnson, Matthijs Douze, and Herv ´e J´egou. 2019.\nBillion-scale similarity search with gpus. IEEE\nTransactions on Big Data.\nDavid Jurgens, Srijan Kumar, Raine Hoover, Daniel A.\nMcFarland, and Dan Jurafsky. 2018. Measuring the\nevolution of a scientiﬁc ﬁeld through citation frames.\nTACL.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In ICLR. To appear.\nJohannes Kiesel, Maria Mestre, Rishabh Shukla, Em-\nmanuel Vincent, Payam Adineh, David Corney,\nBenno Stein, and Martin Potthast. 2019. SemEval-\n2019 Task 4: Hyperpartisan news detection. In Se-\nmEval.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR.\nMartin Krallinger, Obdulia Rabal, Saber Ahmad\nAkhondi, Mart ´ın P´erez P ´erez, J ´es´us L ´opez Santa-\nmar´ıa, Gael P ´erez Rodr ´ıguez, Georgios Tsatsaro-\nnis, Ander Intxaurrondo, Jos ´e Antonio Baso L ´opez,\nUmesh Nandal, E. M. van Buel, A. Poorna Chan-\ndrasekhar, Marleen Rodenburg, Astrid Lægreid,\nMarius A. Doornenbal, Julen Oyarz ´abal, An ´alia\nLourenc ¸o, and Alfonso Valencia. 2017. Overview of\nthe biocreative vi chemical-protein interaction track.\nIn Proceedings of the BioCreative VI Workshop.\nMartin Krallinger, Obdulia Rabal, Florian Leitner,\nMiguel Vazquez, David Salgado, Zhiyong Lu,\nRobert Leaman, Yanan Lu, Donghong Ji, Daniel M\nLowe, et al. 2015. The chemdner corpus of chemi-\ncals and drugs and its annotation principles. Journal\nof cheminformatics, 7(1):S2.\nJens Kringelum, Sonny Kim Kjærulff, Søren Brunak,\nOle Lund, Tudor I. Oprea, and Olivier Taboureau.\n2016. ChemProt-3.0: a global chemical biology dis-\neases mapping. In Database.\n8352\nDavid YW Lee. 2001. Genres, registers, text types, do-\nmains and styles: Clarifying the concepts and nav-\nigating a path through the BNC jungle. Language\nLearning & Technology.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2019. BioBERT: A pre-trained\nbiomedical language representation model for\nbiomedical text mining. Bioinformatics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv:1907.11692.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-\nney, and Daniel S. Weld. 2020. S2ORC: The Se-\nmantic Scholar Open Research Corpus. In ACL. To\nappear.\nLajanugen Logeswaran, Ming-Wei Chang, Kenton Lee,\nKristina Toutanova, Jacob Devlin, and Honglak Lee.\n2019. Zero-shot entity linking by reading entity de-\nscriptions. In ACL.\nYi Luan, Luheng He, Mari Ostendorf, and Hannaneh\nHajishirzi. 2018. Multi-task identiﬁcation of enti-\nties, relations, and coreference for scientiﬁc knowl-\nedge graph construction. In EMNLP.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn ACL.\nJulian McAuley, Christopher Targett, Qinfeng Shi, and\nAnton Van Den Hengel. 2015. Image-based recom-\nmendations on styles and substitutes. In ACM SI-\nGIR.\nArindam Mitra, Pratyay Banerjee, Kuntal Kumar Pal,\nSwaroop Ranjan Mishra, and Chitta Baral. 2020.\nExploring ways to incorporate additional knowledge\nto improve natural language commonsense question\nanswering. arXiv:1909.08855v3.\nRobert C. Moore and William Lewis. 2010. Intelligent\nselection of language model training data. In ACL.\nSebastian Nagel. 2016. CC-NEWS.\nMark Neumann, Daniel King, Iz Beltagy, and Waleed\nAmmar. 2019. Scispacy: Fast and robust models for\nbiomedical natural language processing. Proceed-\nings of the 18th BioNLP Workshop and Shared Task.\nMatthew E. Peters, Sebastian Ruder, and Noah A.\nSmith. 2019. To tune or not to tune? Adapt-\ning pretrained representations to diverse tasks. In\nRepL4NLP.\nJason Phang, Thibault F ´evry, and Samuel R. Bow-\nman. 2018. Sentence encoders on STILTs: Supple-\nmentary training on intermediate labeled-data tasks.\narXiv:1811.01088.\nBarbara Plank. 2016. What to do about non-standard\n(or non-canonical) language in NLP. In KONVENS.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nColin Raffel, Noam Shazeer, Adam Kaleo Roberts,\nKatherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. 2019. Ex-\nploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer. arXiv:1910.10683.\nSteffen Remus and Chris Biemann. 2016. Domain-\nSpeciﬁc Corpus Expansion with Focused Webcrawl-\ning. In LREC.\nSebastian Ruder, Parsa Ghaffari, and John G. Breslin.\n2016. Towards a continuous modeling of natural lan-\nguage domains. In Workshop on Uphill Battles in\nLanguage Processing: Scaling Early Achievements\nto Robust Methods.\nSebastian Ruder and Barbara Plank. 2017. Learning to\nselect data for transfer learning with Bayesian opti-\nmization. In EMNLP.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled version\nof BERT: smaller, faster, cheaper and lighter. In\nEMC2 @ NeurIPS.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to ﬁne-tune BERT for text classiﬁcation?\nIn CCL.\nSwabha Swayamdipta, Matthew Peters, Brendan Roof,\nChris Dyer, and Noah A Smith. 2019. Shallow syn-\ntax in deep water. arXiv:1908.11047.\nTan Thongtan and Tanasanee Phienthrakul. 2019. Sen-\ntiment classiﬁcation using document embeddings\ntrained with cosine similarity. In ACL SRW.\nTrieu H. Trinh and Quoc V . Le. 2018. A simple method\nfor commonsense reasoning. arXiv:1806.02847.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2019. SuperGLUE:\nA stickier benchmark for general-purpose language\nunderstanding systems. In NeurIPS.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Black-\nboxNLP @ EMNLP.\nMarlies van der Wees, Arianna Bisazza, Wouter\nWeerkamp, and Christof Monz. 2015. What’s in a\ndomain? Analyzing genre and topic differences in\nstatistical machine translation. In ACL.\n8353\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In NAACL.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. HuggingFace’s Trans-\nformers: State-of-the-art natural language process-\ning. arXiv:1910.03771.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation.\nHu Xu, Bing Liu, Lei Shu, and Philip Yu. 2019a. BERT\npost-training for review reading comprehension and\naspect-based sentiment analysis. In NAACL.\nHu Xu, Bing Liu, Lei Shu, and Philip S. Yu.\n2019b. Review conversational reading comprehen-\nsion. arXiv:1902.00821v2.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. In NeurIPS.\nDani Yogatama, Cyprien de Masson d’Autume, Jerome\nConnor, Tom´as Kocisk ´y, Mike Chrzanowski, Ling-\npeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu,\nChris Dyer, and Phil Blunsom. 2019. Learning and\nevaluating general linguistic intelligence.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. In NeurIPS.\nXiang Zhang, Junbo Jake Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In NeurIPS.\nXuan Zhang, Pamela Shapiro, Gaurav Kumar, Paul Mc-\nNamee, Marine Carpuat, and Kevin Duh. 2019. Cur-\nriculum learning for domain adaptation in neural ma-\nchine translation. In NAACL.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. In ICCV.\n8354\nAppendix Overview\nIn this supplementary material, we provide: (i)\nadditional information for producing the results in\nthe paper, and (ii) results that we could not ﬁt into\nthe main body of the paper.\nAppendix A. A tabular overview of related work\ndescribed in Section §6, a description of the corpus\nused to train ROBERTA in Liu et al. (2019), and\nreferences to the state of the art on our tasks.\nAppendix B. Details about the data preprocessing,\ntraining, and implementation of domain- and task-\nadaptive pretraining.\nAppendix C. Development set results.\nAppendix D. Examples of domain overlap.\nAppendix E. The cross-domain masked LM loss\nand reproducibility challenges.\nAppendix F. Illustration of our data selection\nmethod and examples of nearest neighbours.\nA Related Work\nTable 11 shows which of the strategies for contin-\nued pretraining have already been explored in the\nprior work from the Related Work (§6). As evident\nfrom the table, our work compares various strate-\ngies as well as their interplay using a pretrained\nlanguage model trained on a much more heteroge-\nneous pretraining corpus.\nA.1 R OBERTA’s Pretraining Corpus\nROBERTA was trained on data from BOOK COR-\nPUS (Zhu et al., 2015),6 WIKIPEDIA ,7 a portion of\nthe CCN EWS dataset (Nagel, 2016),8 OPEN WEB-\nTEXT corpus of Web content extracted from URLs\nshared on Reddit (Gokaslan and Cohen, 2019), 9\nand a subset of CommonCrawl that it is said to\nresemble the “story-like” style of WINOGRAD\nschemas (STORIES; Trinh and Le, 2018). 10\nA.2 State of the Art\nIn this section, we specify the models achieving\nstate of the art on our tasks. See the caption of\n6https://github.com/soskek/bookcorpus\n7https://github.com/google-research/\nbert\n8https://github.com/fhamborg/\nnews-please\n9https://github.com/jcpeterson/\nopenwebtext\n10https://github.com/tensorflow/models/\ntree/master/research/lm_commonsense\nTable 5 for the reported performance of these mod-\nels. For ACL-ARC , that is SCIBERT (Beltagy\net al., 2019), a BERT-base model for trained from\nscratch on scientiﬁc text. For CHEM PROT and SCI-\nERC , that is S2ORC-BERT (Lo et al., 2020), a\nsimilar model to SCIBERT. For AGNEWS and\nIMDB , XLNet-large, a much larger model. For\nRCT , Cohan et al. (2019). For HYPER PARTISAN ,\nLONGFORMER , a modiﬁed Transformer language\nmodel for long documents (Beltagy et al., 2020).\nThongtan and Phienthrakul (2019) report a higher\nnumber (97.42) on IMDB , but they train their word\nvectors on the test set. Our baseline establishes the\nﬁrst benchmark for the HELPFULNESS dataset.\nB Experimental Setup\nPreprocessing for DAPT The unlabeled corpus\nin each domain was pre-processed prior to lan-\nguage model training. Abstracts and body para-\ngraphs from biomedical and computer science\narticles were used after sentence splitting using\nscispaCy (Neumann et al., 2019). We used sum-\nmaries and full text of each news article, and the\nentire body of review from Amazon reviews. For\nboth news and reviews, we perform sentence split-\nting using spaCy (Honnibal and Montani, 2017).\nTraining details for DAPT We train ROBERTA\non each domain for 12.5K steps. We focused on\nmatching all the domain dataset sizes (see Table\n1) such that each domain is exposed to the same\namount of data as for 12.5K steps it is trained for.\nAMAZON reviews contain more documents, but\neach is shorter. We used an effective batch size\nof 2048 through gradient accumulation, as recom-\nmended in Liu et al. (2019). See Table 13 for more\nhyperparameter details.\nTraining details for TAPT We use the same pre-\ntraining hyperparameters as DAPT , but we artiﬁ-\ncially augmented each dataset for TAPT by ran-\ndomly masking different tokens across epochs, us-\ning the masking probability of 0.15. Each dataset\nwas trained for 100 epochs. For tasks with less\nthan 5K examples, we used a batch size of 256\nthrough gradient accumulation. See Table 13 for\nmore hyperparameter details.\nOptimization We used the Adam optimizer\n(Kingma and Ba, 2015), a linear learning rate sched-\nuler with 6% warm-up, a maximum learning rate\nof 0.0005. When we used a batch size of 256, we\n8355\nDAPT Domains\n(if applicable) Tasks Model DAPT TAPT DAPT\n+TAPT\nkNN-\nTAPT\nCurated-\nTAPT\nThis Paper\nbiomedical & computer\nscience papers, news,\nreviews\n8 classiﬁcation\ntasks ROBERTA ✓ ✓ ✓ ✓ ✓\nAharoni and Goldberg (2020) - NMT DISTILBERT +\nTransformer NMT- - - similar -\nAlsentzer et al. (2019) clinical text NER, NLI,\nde-identiﬁcation(BIO)BERT ✓ - - - -\nChakrabarty et al. (2019)opinionated claims from\nReddit claim detection ULMFIT ✓ ✓ - - -\nChronopoulou et al. (2019) - 5 classiﬁcation\ntasks ULMFIT† - similar - - -\nHan and Eisenstein (2019) - NER in historical\ntexts ELMO, BERT - ✓ - - -\nHoward and Ruder (2018) - 6 classiﬁcation\ntasks ULMFIT - ✓ - - -\nKhandelwal et al. (2020) - language modeling Transformer LM - - - similar -\nLee et al. (2019) biomedical papers NER, QA, relation\nextraction BERT ✓ - - - -\nLogeswaran et al. (2019) - zero-shot entity\nlinking in WikiaBERT - ✓ - - -\nMitra et al. (2020) - commonsense QA BERT - ✓ - - -\nPhang et al. (2018) - GLUE tasks ELMO, BERT,\nGPT - ✓ - - -\nRadford et al. (2018) -\nNLI, QA,\nsimilarity,\nclassiﬁcation\nGPT - similar - - -\nSun et al. (2019) sentiment, question,\ntopic\n7 classiﬁcation\ntasks BERT ✓ ✓ - - -\nSwayamdipta et al. (2019) - NER, parsing,\nclassiﬁcation ELMO - similar - - -\nXu et al. (2019a) reviews\nRC, aspect extract.,\nsentiment\nclassiﬁcation\nBERT ✓ ✓ ✓ - -\nXu et al. (2019b) restaurant reviews,\nlaptop reviews conversational RC BERT ✓ ✓ - - -\nTable 11: Overview of prior work across strategies for continued pre-training summarized in Table 10. ULMFIT is\npretrained on English Wikipedia; ULMFIT† on English tweets; ELMO on the 1BWORD BENCHMARK (newswire;\nChelba et al., 2014); GPT on B OOK CORPUS ; BERT on English Wikipedia and B OOK CORPUS . In comparison to\nthese pretraining corpora, ROBERTA’s pretraining corpus is substantially more diverse (see Appendix§A.1).\nused a maximum learning rate of 0.0001, as rec-\nommended in Liu et al. (2019). We observe a high\nvariance in performance between random seeds\nwhen ﬁne-tuning ROBERTA to HYPER PARTISAN ,\nbecause the dataset is extremely small. To produce\nﬁnal results on this task, we discard and resample\ndegenerate seeds. We display the full hyperparam-\neter settings in Table 13.\nImplementation Our LM implementation uses\nthe HuggingFace transformers library\n(Wolf et al., 2019)11 and PyTorch XLAfor TPU\ncompatibility.12 Each adaptive pretraining exper-\n11https://github.com/huggingface/\ntransformers\n12https://github.com/pytorch/xla\niment was performed on a single v3-8 TPU from\nGoogle Cloud.13 For the text classiﬁcation tasks,\nwe used AllenNLP (Gardner et al., 2018). Fol-\nlowing standard practice (Devlin et al., 2019) we\npass the ﬁnal layer [CLS] token representation to\na task-speciﬁc feedforward layer for prediction.\nC Development Set Results\nAdhering to the standards suggested by Dodge et al.\n(2019) for replication, we report our development\nset results in Tables 15, 17, and 18.\n13http://github.com/allenai/\ntpu-pretrain\n8356\nD Analysis of Domain Overlap\nIn Table 20 we display additional examples that\nhighlight the overlap between IMDB reviews and\nREAL NEWS articles, relevant for analysis in §3.1.\nE Analysis of Cross-Domain Masked LM\nLoss\nIn Section §3.2, we provide ROBERTA’s masked\nLM loss before and after DAPT . We display cross-\ndomain masked-LM loss in Table 12, where we\nevaluate masked LM loss on text samples in other\ndomains after performing DAPT .\nWe observe that the cross-domain masked-LM\nloss mostly follows our intuition and insights from\nthe paper, i.e. ROBERTA’s pretraining corpus and\nNEWS are closer, and BIOMED to CS (relative to\nother domains). However, our analysis in §3.1 il-\nlustrates that REVIEWS and NEWS also have some\nsimilarities. This is supported with the loss of\nROBERTA that is adapted to NEWS , calculated\non a sample of REVIEWS . However, ROBERTA\nthat is adapted to REVIEWS results in the highest\nloss for a NEWS sample. This is the case for all\ndomains. One of the properties that distinguishes\nREVIEWS from all other domains is that its doc-\numents are signiﬁcantly shorter. In general, we\nﬁnd that cross-DAPT masked-LM loss can in some\ncases be a noisy predictor of domain similarity.\nF k-Nearest Neighbors Data Selection\nIn Table 21, we display nearest neighbor docu-\nments in the BIOMED domain identiﬁed by our\nselection method, on the RCT dataset.\n8357\nData Sample Unseen During DAPT\nPT B IOMED CS N EWS REVIEWS\nROBERTA 1.19 1.32 1.63 1.08 2.10\nDAPT\n\n\n\nBIOMED 1.63 0.99 1.63 1.69 2.59\nCS 1.82 1.43 1.34 1.92 2.78\nNEWS 1.33 1.50 1.82 1.16 2.16\nREVIEWS 2.07 2.23 2.44 2.27 1.93\nTable 12: ROBERTA’s (row 1) and domain-adapted ROBERTA’s (rows 2–5) masked LM loss on randomly sam-\npled held-out documents from each domain (lower implies a better ﬁt). PT denotes a sample from sources similar\nto ROBERTA’s pretraining corpus. The lowest masked LM for each domain sample is boldfaced.\nComputing Infrastructure Google Cloud v3-8 TPU\nModel implementations https://github.com/allenai/tpu_pretrain\nHyperparameter Assignment\nnumber of steps 100 epochs ( TAPT) or 12.5K steps (DAPT )\nbatch size 256 or 2058\nmaximum learning rate 0.0001 or 0.0005\nlearning rate optimizer Adam\nAdam epsilon 1e-6\nAdam beta weights 0.9, 0.98\nlearning rate scheduler None or warmup linear\nWeight decay 0.01\nWarmup proportion 0.06\nlearning rate decay linear\nTable 13: Hyperparameters for domain- and task- adaptive pretraining.\nComputing Infrastructure Quadro RTX 8000 GPU\nModel implementation https://github.com/allenai/dont-stop-pretraining\nHyperparameter Assignment\nnumber of epochs 3 or 10\npatience 3\nbatch size 16\nlearning rate 2e-5\ndropout 0.1\nfeedforward layer 1\nfeedforward nonlinearity tanh\nclassiﬁcation layer 1\nTable 14: Hyperparameters for ROBERTA text classiﬁer.\n8358\nAdditional Pretraining Phases\nDomain Task R OBERTA DAPT TAPT DAPT + TAPT\nBIOMED CHEM PROT 83.21.4 84.10.5 83.00.6 84.10.5\n†RCT 88.1 0.05 88.50.1 88.30.1 88.50.1\nCS ACL-ARC 71.3 2.8 73.21.5 73.23.6 78.62.9\nSCIERC 83.8 1.1 88.41.7 85.90.8 88.01.3\nNEWS HYPER PARTISAN 84.01.5 79.13.5 82.73.3 80.82.3\n†AGNEWS 94.30.1 94.30.1 94.70.1 94.90.1\nREVIEWS\n†HELPFULNESS 65.53.4 66.51.4 69.22.4 69.42.1\n†IMDB 94.8 0.1 95.30.1 95.40.1 95.70.2\nTable 15: Results on different phases of adaptive pretraining compared to the baseline R OBERTA (col. 1). Our\napproaches are DAPT (col. 2, §3), TAPT (col. 3, §4), and a combination of both (col. 4). Reported results are devel-\nopment macro-F1, except for CHEM PROT and RCT, for which we report micro-F1, following Beltagy et al. (2019).\nWe report averages across ﬁve random seeds, with standard deviations as subscripts. †indicates high-resource set-\ntings. Best task performance is boldfaced. State-of-the-art results we can compare to: C HEM PROT (84.6), RCT\n(92.9), ACL-ARC (71.0), S CIERC (81.8), H YPER PARTISAN (94.8), AGN EWS (95.5), IMDB (96.2); references\nin §A.2.\nDom. Task R OB. DAPT ¬DAPT\nBM CHEM PROT 83.21.4 84.10.5 80.90.5\n†RCT 88.1 0.0 88.50.1 87.90.1\nCS ACL-ARC 71.3 2.8 73.21.5 68.15.4\nSCIERC 83.8 1.1 88.41.7 83.90.9\nNEWS HYP. 84.01.5 79.13.5 71.64.6\n†AGNEWS 94.30.1 94.30.1 94.00.1\nREV.\n†HELPFUL . 65.5 3.4 66.51.4 65.53.0\n†IMDB 94.8 0.1 95.30.1 93.80.2\nTable 16: Development comparison of R OBERTA (ROBA.) and DAPT to adaptation to an irrelevant domain (¬\nDAPT ). See §3.3 for our choice of irrelevant domains. Reported results follow the same format as Table 5.\nBIO MED RCT C HEM PROT\nTAPT 88.30.1 83.00.6\nTransfer- TAPT 88.00.1 (↓ 0.3) 81.1 0.5 (↓ 1.9)\nNEWS HYPER PARTISAN AGN EWS\nTAPT 82.73.3 94.70.1\nTransfer- TAPT 77.63.6 (↓ 5.1) 94.4 0.1 (↓ 0.4)\nCS ACL-ARC S CI ERC\nTAPT 73.23.6 85.90.8\nTransfer- TAPT 74.04.5 (↑ 1.2) 85.5 1.1 (↓ 0.4)\nAMAZON reviews HELPFULNESS IMDB\nTAPT 69.22.4 95.40.1\nTransfer- TAPT 65.42.7 (↓ 3.8) 94.9 0.1 (↓ 0.5)\nTable 17: Development results for TAPT transferability.\nPretraining BIOMED NEWS REVIEWS\nRCT-500 H YPER PARTISAN †IMDB\nTAPT 80.51.3 82.73.3 95.40.1\nDAPT + TAPT 83.90.3 80.82.3 95.70.2\nCurated-TAPT 84.40.3 84.91.9 95.80.1\nDAPT + Curated-TAPT 84.50.3 83.13.7 96.00.1\nTable 18: Mean development set macro-F1 (for HYPER PARTISAN and IMDB) and micro- F1 (for RCT-500), with\nCurated-TAPT across ﬁve random seeds, with standard deviations as subscripts. †indicates high-resource settings.\n8359\nPretraining B IOMED CS\nCHEM PROT RCT-500 ACL-ARC\nROBERTA 83.21.4 80.30.5 71.32.8\nTAPT 83.00.6 80.51.3 73.23.6\nRAND -TAPT 83.30.5 81.60.6 78.74.0\n50NN-TAPT 83.30.8 81.70.5 70.13.5\n150NN-TAPT 83.30.9 81.90.8 78.52.2\n500NN-TAPT 84.50.4 82.60.4 77.42.3\nDAPT 84.10.5 83.50.8 73.21.5\nTable 19: Mean development set macro- F1 (for H YP. and IMDB) and micro- F1 (for RCT), across ﬁve random\nseeds, with standard deviations as subscripts, comparing RAND -TAPT (with 50 candidates) and kNN-TAPT selec-\ntion. Neighbors of the task data are selected from the domain data.\nIMDB review R EALNEWSarticle\nSpooksis enjoyable trash, featuring some well directed sequences,\nridiculous plots and dialogue, and some third rate acting. Many have\ndescribed this is a UK version of “24“, and one can see the similarities.\nThe American version shares the weak silly plots, but the execution is so\nmuch slicker, sexier and I suspect, expensive. Some people describe\nweak comedy as “gentle comedy“. This is gentle spy story hour, the\nexact opposite of anything created by John Le Carre. Give me Smiley\nany day.\n[...] Remember poor Helen Flynn fromSpooks? In 2002, the headlong\nBBC spy caper was in such a hurry to establish the high-wire stakes of its\nmorally compromised world that Lisa Faulkner’s keen-as-mustard MI5\nrookie turned out to be a lot more expendable than her prominent billing\nsuggested. [...] Functioning as both a shocking twist and rather callous\nstatement that No-One Is Safe, it gave the slick drama an instant patina\nof edginess while generating a record-breaking number of complaints.\n[...]\nThe Sopranosis perhaps the most mind-opening series you could\npossibly ever want to watch. It’s smart, it’s quirky, it’s funny - and it\ncarries the maﬁa genre so well that most people can’t resist watching.\nThe best aspect of this show is the overwhelming realism of the\ncharacters, set in the subterranean world of the New York crime families.\nFor most of the time, you really don’t know whether the wise guys will\nstab someone in the back, or buy them lunch. Further adding to the\nrealistic approach of the characters in this show is the depth of their\npersonalities - These are dangerous men, most of them murderers, but\nby God if you don’t love them too. I’ve laughed at their wisecracks,\nbeen torn when they’ve made err in judgement, and felt scared at the\nsheer ruthlessness of a serious criminal. [...]\nThe drumbeat regarding the “Breaking Bad” ﬁnale has led to the in-\nevitable speculation on whether the ﬁnal chapter in this serialized gem\nwill live up to the hype or disappoint (thank you, “Dexter,” for setting that\nbar pretty low), with debate, second-guessing and graduate-thesis-length\nanalysis sure to follow. The Most Memorable TV Series Finales of All-\nTime [...] No ending in recent years has been more divisive than“The\nSopranos”– for some, a brilliant ﬂash (literally, in a way) of genius;\nfor others (including yours truly), a too-cute copout, cryptically leaving\nits characters in perpetual limbo. The precedent to that would be “St.\nElsewhere,” which irked many with its provocative, surreal notion that\nthe whole series was, in fact, conjured in the mind of an autistic child.\n[...]\nThe Wicker Man, starring Nicolas Cage, is by no means a good movie,\nbut I can’t really say it’s one I regret watching. I could go on and on\nabout the negative aspects of the movie, like the terrible acting and the\nlengthy scenes where Cage is looking for the girl, has a hallucination,\nfollowed by another hallucination, followed by a dream sequence- with\na hallucination, etc., but it’s just not worth dwelling on when it comes to\na movie like this. Instead, here’s ﬁve reasons why you SHOULD watch\nThe Wicker Man, even though it’s bad: 5. It’s hard to deny that it has\nsome genuinely creepy ideas to it, the only problem is in its cheesy,\nunintentionally funny execution. If nothing else, this is a movie that may\ninspire you to see the original 1973 ﬁlm, or even read the short story on\nwhich it is based. 4. For a cheesy horror/thriller, it is really aesthetically\npleasing. [...] NOTE: The Unrated version of the movie is the best to\nwatch, and it’s better to watch the Theatrical version just for its little\nadded on epilogue, which features a cameo from James Franco.\n[...] What did you ultimately feel about”The Wicker Man”movie\nwhen all was said and done? [...] I’m a fan of the original and I’m\nglad that I made the movie because they don’t make movies like that\nanymore and probably the result of what ”Wicker Man” did is the reason\nwhy they don’t make movies like that anymore. Again, it’s kind of that\n’70’s sensibility, but I’m trying to do things that are outside the box.\nSometimes that means it’ll work and other times it won’t. Again though\nI’m going to try and learn from anything that I do. I think that it was a\ngreat cast, and Neil La Bute is one of the easiest directors that I’ve ever\nworked with. He really loves actors and he really gives you a relaxed\nfeeling on the set, that you can achieve whatever it is that you’re trying to\nput together, but at the end of the day the frustration that I had with ‘The\nWicker Man,’ which I think has been remedied on the DVD because I\nbelieve the DVD has the directors original cut, is that they cut the horror\nout of the horror ﬁlm to try and get a PG-13 rating. I mean, I don’t know\nhow to stop something like that. So I’m not happy with the way that the\npicture ended, but I’m happy with the spirit with which it was made. [...]\nDr. Seuss would sure be mad right now if he was alive.Cat in the Hat\nproves to show how movie productions can take a classic story and turn\nit into a mindless pile of goop. We have Mike Myers as the infamous\nCat in the Hat, big mistake! Myers proves he can’t act in this ﬁlm. He\nacts like a prissy show girl with a thousand tricks up his sleeve. The kids\nin this movie are all right, somewhere in between the lines of dull and\nannoying. The story is just like the original with a couple of tweaks and\nlike most movies based on other stories, never tweak with the original\nstory! Bringing in the evil neighbor Quin was a bad idea. He is a stupid\nvillain that would never get anywhere in life. [...]\nThe Cat in the Hat, [...] Based on the book by Dr. Seuss [...] From the\nmoment his tall, red-and-white-striped hat appears at their door, Sally\nand her brother know that the Cat in the Hat is the most mischievous\ncat they will ever meet. Suddenly the rainy afternoon is transformed\nby the Cat and his antics. Will their house ever be the same? Can\nthe kids clean up before mom comes home? With some tricks (and a\nﬁsh) and Thing Two and Thing One, with the Cat in The Hat, the fun’s\nnever done!Dr. Seuss is known worldwide as the imaginative master of\nchildren’s literature. His books include a wonderful blend of invented\nand actual words, and his rhymes have helped many children and adults\nlearn and better their understanding of the English language. [...]\nTable 20: Additional examples that highlight the overlap between IMDB reviews and R EAL NEWS articles.\n8360\nSource During median follow-up of 905 days ( IQR 773-1050 ) , 49 people died and 987 unplanned admissions were recorded (\ntotalling 5530 days in hospital ) .\nNeighbor 0 Of this group, 26% died after discharge from hospital, and the median time to death was 11 days (interquartile range,\n4.0-15.0 days) after discharge.\nNeighbor 1 The median hospital stay was 17 days (range 8-26 days), and all the patients were discharged within 1 month.\nNeighbor 2 The median hospital stay was 17 days (range 8-26 days).\nNeighbor 3 The median time between discharge and death was 25 days (mean, 59.1 days) and no patient was alive after 193 days.\nNeighbor 4 The length of hospital stay after colostomy formation ranged from 3 days to 14 days with a median duration of 6 days\n(+IQR of 4 to 8 days).\nSource Randomized , controlled , parallel clinical trial .\nNeighbor 0 Design: Unblinded, randomised clinical controlled trial.\nNeighbor 1 These studies and others led to the phase III randomized trial RTOG 0617/NCCTG 0628/ CALGB 30609.\nNeighbor 2 -Deﬁnitive randomized controlled clinical trial (RCT):\nNeighbor 3 RCT 1\n4 randomized controlled trial.\nNeighbor 4 randomized controlled trial [ Fig. 3(A)].\nSource Forty primary molar teeth in 40 healthy children aged 5-9 years were treated by direct pulp capping .\nNeighbor 0 In our study, we speciﬁcally determined the usefulness of the Er:Y AG laser in caries removal and cavity preparation of\nprimary and young permanent teeth in children ages 4 to 1 8 years.\nNeighbor 1 Males watched more TV than females, although it was only in primary school-aged children and on weekdays.\nNeighbor 2 Assent was obtained from children and adolescents aged 7-17 years.\nNeighbor 3 Cardiopulmonary resuscitation was not applied to children aged ¡5 years (Table 2).\nNeighbor 4 It measures HRQoL in children and adolescents aged 2 to 25 years.\nTable 21: 5 nearest neighbors of sentences from the RCT dataset (Source) in the B IOMED domain (Neighbors\n0–4).",
  "topic": "Task (project management)",
  "concepts": [
    {
      "name": "Task (project management)",
      "score": 0.7870112061500549
    },
    {
      "name": "Computer science",
      "score": 0.7626404762268066
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.7030505537986755
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.6628183722496033
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.5893699526786804
    },
    {
      "name": "Domain adaptation",
      "score": 0.5470484495162964
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5341552495956421
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.5210889577865601
    },
    {
      "name": "Natural language processing",
      "score": 0.4779817461967468
    },
    {
      "name": "Language model",
      "score": 0.4274291694164276
    },
    {
      "name": "Machine learning",
      "score": 0.3825761377811432
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210156221",
      "name": "Allen Institute for Artificial Intelligence",
      "country": "US"
    }
  ]
}