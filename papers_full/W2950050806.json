{
  "title": "Putting Words in Context: LSTM Language Models and Lexical Ambiguity",
  "url": "https://openalex.org/W2950050806",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A5004059222",
      "name": "Laura Aina",
      "affiliations": [
        "Universitat Pompeu Fabra"
      ]
    },
    {
      "id": "https://openalex.org/A5043347279",
      "name": "Kristina Gulordava",
      "affiliations": [
        "Universitat Pompeu Fabra"
      ]
    },
    {
      "id": "https://openalex.org/A5026675870",
      "name": "Gemma Boleda",
      "affiliations": [
        "Universitat Pompeu Fabra"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2507974895",
    "https://openalex.org/W2251762914",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W2963430224",
    "https://openalex.org/W1597195725",
    "https://openalex.org/W2250968248",
    "https://openalex.org/W581956982",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W1984052055",
    "https://openalex.org/W2091494174",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2131774270",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2953949198",
    "https://openalex.org/W1501579169",
    "https://openalex.org/W3753517",
    "https://openalex.org/W2146884627",
    "https://openalex.org/W2963850840",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3154772965",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W2962776659",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W1973942085",
    "https://openalex.org/W2165131017",
    "https://openalex.org/W2955797234"
  ],
  "abstract": "In neural network models of language, words are commonly represented using context-invariant representations (word embeddings) which are then put in context in the hidden layers. Since words are often ambiguous, representing the contextually relevant information is not trivial. We investigate how an LSTM language model deals with lexical ambiguity in English, designing a method to probe its hidden representations for lexical and contextual information about words. We find that both types of information are represented to a large extent, but also that there is room for improvement for contextual information.",
  "full_text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3342–3348\nFlorence, Italy, July 28 - August 2, 2019.c⃝2019 Association for Computational Linguistics\n3342\nPutting words in context:\nLSTM language models and lexical ambiguity\nLaura Aina Kristina Gulordava Gemma Boleda\nUniversitat Pompeu Fabra\nBarcelona, Spain\n{firstname.lastname}@upf.edu\nAbstract\nIn neural network models of language, words\nare commonly represented using context-\ninvariant representations (word embeddings)\nwhich are then put in context in the hidden lay-\ners. Since words are often ambiguous, repre-\nsenting the contextually relevant information\nis not trivial. We investigate how an LSTM\nlanguage model deals with lexical ambiguity\nin English, designing a method to probe its\nhidden representations for lexical and contex-\ntual information about words. We ﬁnd that\nboth types of information are represented to\na large extent, but also that there is room for\nimprovement for contextual information.\n1 Introduction\nIn language, a word can contribute a very differ-\nent meaning to an utterance depending on the con-\ntext, a phenomenon known as lexical ambiguity\n(Cruse, 1986; Small et al., 2013). This variation is\npervasive and involves both morphosyntactic and\nsemantic aspects. For instance, in the examples in\nTable 1, show is used as a verb in Ex. (1), and as a\nnoun in Ex. (2-3), in a paradigmatic case of mor-\nphosyntactic ambiguity in English. Instead, the\ndifference between Ex. (2) and (3) is semantic in\nnature, with show denoting a TV program and an\nexhibition, respectively. Semantic ambiguity cov-\ners a broad spectrum of phenomena, ranging from\nquite distinct word senses (e.g. mouse as animal\nor computer device) to more subtle lexical modu-\nlation (e.g. visit a city / an aunt / a doctor ; Cruse,\n1986). This paper investigates how deep learning\nmodels of language, and in particular Long Short-\nTerm Memory Networks (LSTMs) trained on Lan-\nguage Modeling, deal with lexical ambiguity.1\nIn neural network models of language, words\nin a sentence are commonly represented through\n1Code at: https://github.com/amore-upf/\nLSTM_ambiguity\nword-level representations that do not change\nacross contexts, that is, “static” word embeddings.\nThese are then passed to further processing lay-\ners, such as the hidden layers in a recurrent neural\nnetwork (RNN). Akin to classic distributional se-\nmantics (Erk, 2012), word embeddings are formed\nas an abstraction over the various uses of words\nin the training data. For this reason, they are apt\nto represent context-invariant information about a\nword —its lexical information— but not the con-\ntribution of a word in a particular context —its\ncontextual information (Erk, 2010). Indeed, word\nembeddings subsume information relative to var-\nious senses of a word (e.g., mouse is close to\nwords from both the animal and computer do-\nmain; Camacho-Collados and Pilehvar, 2018).\nClassic distributional semantics attempted to\ndo composition to account for contextual effects,\nbut it was in general unable to go beyond short\nphrases (Baroni, 2013); newer-generation neural\nnetwork models have supposed a big step forward,\nas they can natively do composition (Westera and\nBoleda, 2019). In particular, the hidden layer acti-\nvations in an RNN can be seen asputting words in\ncontext, as they combine the word embedding with\ninformation coming from the context (the adja-\ncent hidden states). The empirical success of RNN\nmodels, and in particular LSTM architectures, at\nfundamental tasks like Language Modeling (Joze-\nfowicz et al., 2015) suggests that they are indeed\ncapturing relevant contextual properties. More-\nover, contextualized representations derived from\nsuch models have been shown to be very informa-\ntive as input for lexical disambiguation tasks (e.g.\nMelamud et al., 2016; Peters et al., 2018).\nWe here present a method to probe the extent\nto which the hidden layers of an LSTM language\ntrained on English data represent lexical and con-\ntextual information about words, in order to inves-\ntigate how the model copes with lexical ambiguity.\n3343\nExamples LexSub w NN s NN w&s NN\n(1) . . . I clapped her shoulder\nto show I was not laughing at\nher. . .\ndemonstrate,\ndisplay, indicate,\nprove, clarify\ndemonstrate,\nexhibit, indicate,\noffer, reveal\nindicate,\ndemonstrate,\nsuggest, prove,\nindicate,\ndemonstrate, prove,\nensure, suggest\n(2) . . . Theshow [. . . ]\nrevolutionized the way\nAmerica cooks and eats. . .\nprogram, series,\nbroadcast,\npresentation\ndemonstrate,\nexhibit, indicate,\noffer, reveal\nseries, program,\nproduction,\nminiseries, trilogy\nseries, program,\nproduction,\nbroadcast\n(3) . . . The inauguration of\nDubai Internet City coincides\nwith the opening of an annual\nIT show in Dubai. . ..\nexhibition,\nconference,\nconvention,\ndemonstration\ndemonstrate,\nexhibit, indicate,\noffer, reveal\nconference, event,\nconvention,\nsymposium,\nexhibition\nconference, event,\nexhibition,\nsymposium,\nconvention\nTable 1: Examples from the LexSub dataset (Kremer et al., 2014) and nearest neighbors for target representations.\nOur work follows a recent strand of research that\npurport to identify what linguistic properties deep\nlearning models are able to capture (Linzen et al.,\n2016; Adi et al., 2017; Gulordava et al., 2018;\nConneau et al., 2018; Hupkes et al., 2018, a.o.).\nWe train diagnostic models on the tasks of retriev-\ning the embedding of a word and a representation\nof its contextual meaning, respectively —the latter\nobtained from a Lexical Substitution dataset (Kre-\nmer et al., 2014). Our results suggest that LSTM\nlanguage models heavily rely on the lexical infor-\nmation in the word embeddings, at the expense of\ncontextually relevant information. Although fur-\nther analysis is necessary, this suggests that there\nis still much room for improvement to account for\ncontextual meanings. Finally, we show that the\nhidden states used to predict a word – as opposed\nto those that receive it as input – display a bias to-\nwards contextual information.\n2 Method\nLanguage model. As our base model, we em-\nploy a word-level bidirectional LSTM (Schus-\nter and Paliwal, 1997; Hochreiter and Schmidhu-\nber, 1997) language model (henceforth, LM) with\nthree hidden layers. Each input word at timestep\nt is represented through its word embedding wt;\nthis is fed to both a forward and a backward\nstacked LSTMs, which process the sequence left-\nto-right and right-to-left, respectively (Eqs. (1-2)\ndescribe the forward LSTM). To predict the word\nat t, we obtain output weights by summing the ac-\ntivations of the last hidden layers of the forward\nand backward LSTMs at timesteps t−1 and t+ 1,\nrespectively, and applying a linear transformation\nfollowed by softmax (Eq. 3, where Lis the num-\nber of hidden layers). Thus, a word is predicted\nusing both its left and right context jointly, akin\nto the context2vec architecture (Melamud et al.,\n2016) but differently from, e.g., the BiLSTM ar-\nchitecture used for ELMo (Peters et al., 2018).\nh1\nt = LSTM1(wt,h1\nt−1) (1)\nhi\nt = LSTMi(hi−1\nt ,hi\nt−1) (2)\not = softmax(f(− →h L\nt−1 + ← −h L\nt+1)) (3)\nWe train the LM on the concatenation of English\ntext data from a Wikipedia dump2, the British Na-\ntional Corpus (Leech, 1992), and the UkWaC cor-\npus (Ferraresi et al., 2008). 3 More details about\nthe training setup are speciﬁed in Appendix A.1.\nThe model achieves satisfying performances on\ntest data (perplexity: 18.06).\nFor our analyses, we deploy the trained LM on a\ntext sequence and extract the following activations\nof each hidden layer; Eq. (4) and Fig. 1.\n{− →h i\nt|i≤L}∪{← −h i\nt|i≤L} (4)\nhi\nt = [− →h i\nt; ← −h i\nt] (5)\nhi\nt±1 = [− →h i\nt−1; ← −h i\nt+1] (6)\nAt timestep t, for each layer, we concatenate the\nforward and backward hidden states; Eq. (5). We\nrefer to these vectors as current hidden states.\nAs they are obtained processing the word at t as\ninput and combining it with information from the\ncontext, we can expect them to capture the rel-\nevant contribution of such word (e.g., in Fig. 1\nthe mouse-as-animal sense). As a comparison, we\nalso extract activations obtained by processing the\ntext sequence up to t−1 and t+ 1in the forward\nand backward LSTM, respectively, hence exclud-\ning the word at t. We concatenate the forward and\nbackward states of each layer; Eq. (6). While these\n2From 2018/01/03, https://dumps.wikimedia.\norg/enwiki/\n350M tokens from each corpus, in total 150M\n(train/valid/test: 80/10/10%); vocabulary size: 50K.\n3344\nFigure 1: Language model and extracted representa-\ntions. The different shades across layers reﬂect the dif-\nferent performances in the probe tasks (darker = higher)\nactivations do not receive the word at t as input,\nthey are relevant because they are used to predict\nthat word as output. We refer to them as predic-\ntive hidden states. These may capture some as-\npects of the word (e.g., in Fig. 1, that it is a noun\nand denotes something animate), but are likely to\nbe less accurate than the current states, since they\ndo not observe the actual word.\nProbe tasks. We aim to assess to what extent\nthe hidden states in the LM carry over the lexi-\ncal and context-invariant information in the input\nword embedding, and how much they instead rep-\nresent the contextual meaning of the word. To this\nend, we rely on vector representations of lexical\nand contextual word information. As for the for-\nmer, we can directly use the word embeddings of\nthe LM (w); it is instead more challenging to ﬁnd\na representation of the contextual meaning.\nOur solution is to use Lexical Substitution data\n(McCarthy and Navigli, 2009) and, in particular,\nthe large dataset by Kremer et al., 2014 (hence-\nforth, LexSub; see Table 1). In this dataset, words\nin context (up to 3 sentences) are annotated with a\nset of paraphrases given by human subjects. Since\ncontextual substitutes reﬂect differences among\nuses of a word (for instance, demonstrate para-\nphrases show in a context like Ex. (1), but not in\nEx. (2)), this type of data is often used as an evalu-\nation benchmark for contextual representations of\nwords (e.g., Erk and Pad ´o, 2008; Melamud et al.,\n2016; Gar´ı Soler et al., 2019). We leverage Lex-\nSub to build proxies for ground-truth representa-\ntions of the contextual meaning of words. We\ndeﬁne two types of representations, inspired by\nprevious work that proposed simple vector oper-\nations to combine word representations (Mitchell\nand Lapata, 2010; Thater et al., 2011, a.o.): the\naverage embedding of the substitute words (hence-\nforth, s), and the average embedding of the union\nof the substitute words and the target word (w&s).\nAs Table 1 qualitatively shows, the resulting repre-\nsentations tend to be close to the substitute words\nand reﬂect the contextual nuance conveyed by the\nword; in the case ofw&s, they also retain a strong\nsimilarity to the embedding of the target word.4\nWe frame our analyses as supervised probe\ntasks: a diagnostic model learns to “retrieve” word\nrepresentations out of the hidden states; the rate\nof success of the model is taken to measure the\namount of information relevant to the task that its\ninput contains. Given current or predictive states\nas inputs, we deﬁne three diagnostic tasks:\n- W ORD : predict w\n- S UB: predict s\n- W ORD &SUB: predict w&s\nThe W ORD task is related to the probe tasks in-\ntroduced in Adi et al. (2017) and Conneau et al.\n(2018), which, given a hidden state, require to pre-\ndict the words that a sentence encoder has pro-\ncessed as input. Note that, while these authors pre-\ndict words by their discrete index, we are predict-\ning the complete multi-dimensional embedding of\nthe word. Our test quantiﬁes not only whether the\nmodel is tracking the identity of the input word,\nbut also how much of its information it retains.\nWe train distinct probe models for each task and\ntype of input ( i; e.g., current hidden state at layer\n1). A model consists of a non-linear transforma-\ntion from an input vectori (extracted from the LM)\nto a vector with the dimensionality of the word\nembeddings (Eq. 7, where ˆ ris one of ˆ w, ˆ s, ˆw&s\nfor WORD , SUB, and WORD &SUB tasks, respec-\ntively). The models are trained through max-\nmargin loss, optimizing the cosine similarity be-\ntween ˆ rand the target representation against the\nsimilarities between ˆ rand 5 negative samples (de-\ntails in Appendix A.2).\nˆ r= tanh(W i + b) (7)\n4These vectors are close to the related word embedding\n(0.45 and 0.66 mean cosine, see Table 2, row wt), but also\ndifferent from it: on average, s and w&s share 17 and 25%\nof the top-10 neighbors with w, respectively (statistics from\ntraining data, excluding the word itself from neighbors).\n3345\ninput WORD SUB WORD &SUB\nwt 1 .45 ( ±.14) .66 ( ±.09)\navgctxt .35 (±.10) .16 ( ±.11) .24 ( ±.12)\nh1\nt .84 (±.2) .61 (±.14) .71 (±.11)\nh2\nt .74 (±.12) .60 ( ±.13) .69 ( ±.11)\nh3\nt .64 (±.12) .58 ( ±.13) .65 ( ±.11)\nh1\nt±1 .25 (±.16) .36 ( ±.16) .38 ( ±.16)\nh2\nt±1 .27 (±.16) .39 ( ±.16) .41 ( ±.16)\nh3\nt±1 .29 (±.15) .41 (±.16) .43 (±.16)\nTable 2: Results of probe tasks for current ( hi\nt) and\npredictive (hi\nt±1) hidden states.\nWe adapt the LexSub data to our setup as fol-\nlows. Since substitutes are provided in their lem-\nmatized form, we only consider datapoints where\nthe word form is identical to the lemma so as to\nexclude effects due to morphosyntax (e.g., ask-\ning the models to recover play when they observe\nplayed).5 We require that at least 5 substitutes\nper datapoint are in the LM vocabulary to ensure\nquality in the target representations. LexSub data\ncome with a validation/test split; since we need\ntraining data, we create a new random partitioning\ninto train/valid/test (70/10/20%, with no overlap-\nping contexts among splits). The ﬁnal data consist\nof 4.7K/0.7K/1.3K datapoints for train/valid/test.\n3 Results\nThe results of the probe tasks on test data are pre-\nsented in Table 2. We report the mean and stan-\ndard deviation of thecosine similarity between the\noutput representations ( ˆ w, ˆ s, ˆw&s) and the target\nones (w, s, w&s). This evaluates the degree to\nwhich the word representations can be retrieved\nfrom the hidden states. For comparison, we also\nreport the cosine scores between the targets and\ntwo baseline representations: the word embedding\nitself and the average of word embeddings of a 10-\nword window around the target word ( avgctxt).6\nOverall, the models do better than these unsuper-\nvised baselines, with exceptions.7\nCurrent hidden states. Both lexical and con-\ntextual representations can be retrieved from the\ncurrent hidden states (hi\nt) to a large extent (cosines\n5We also exclude substitutes that are multi-word expres-\nsions and the datapoints involving words that are part of a\ncompound (e.g., fast in fast-growing).\n6We exclude out-of-vocabulary words and punctuation.\n7The ﬁrst cell is 1 as it involves the same representation.\n0.0 0.5\nCosine (w, s)\n0.0\n0.5\nCosine sub task\nFigure 2: Similarity of lexical and contextual vector (w\n- s) vs. similarity of target and prediction in SUB for h1\nt .\n.58-.84), but retrieving the former is much easier\nthan the latter (.64-.84 vs. .58-71). This suggests\nthat the information in the word embedding is bet-\nter represented in the hidden states than the con-\ntextually relevant one. In all three tasks, perfor-\nmance degrades closer to the output layer (from\nh1\nt to h3\nt ), but the effect is more pronounced for\nthe W ORD task (84/.74/.64). Word embeddings\nare part of the input to the hidden state, and the\ntransformation learned for this task can be seen\nas a decoder in an auto-encoder, reconstructing\nthe original input; the further the hidden layer is\nfrom the input, the more complex the function is\nto reverse-engineer. Crucially, the high perfor-\nmance at reconstructing the word embedding sug-\ngests that lexical information is retained in the hid-\nden layers, possibly including also contextually ir-\nrelevant information (e.g., in Ex. (4) in Table 3 ˆ w\nis close to verbs, even if share is here a noun).\nContextual information ( s and w&s) seems to\nbe more stable across processing layers, although\noverall less present (cf. lower results). Table 3 re-\nports one example where the learned model dis-\nplays relevant contextual aspects (Ex. (4), share)\nand one where it does not (Ex. (5), studio). Qual-\nitative analysis shows that morphosyntactic ambi-\nguity (e.g., share as a noun vs. verb) is more eas-\nily discriminated, while semantic distinctions pose\nmore challenges (e.g., studio as a room vs. com-\npany). This is not surprising, since the former\ntends to correlate with clearer contextual cues.\nFurthermore, we ﬁnd that the more the contex-\ntual representation is aligned to the lexical one,\nthe easier it is to retrieve the former from the hid-\nden states (e.g., correlation cos(w,s) - cos(ˆ s,s),\nfor h1\nt : Pearson’s ρ = .62∗∗∗; Fig. 2): that is,\nit is harder to resolve lexical ambiguity when the\ncontextual meaning is less represented in the word\nembedding (e.g., less frequent uses). This sug-\ngests that the LM heavily relies on the informa-\n3346\nContext LexSub W ORD : ˆ wNN SUB: ˆ sNN WORD &SUB: wˆ&s NN\n(4) ... The ﬁnancial-services\ncompany will pay 0.82 share\nfor each Williams share ...\nstock, dividend,\ninterest, stake, unit\nstake, owe,\ndiscuss,\ncoincide, reside\nportion, amount,\npercentage,\nfraction\nstake, percentage,\nportion, spend,\nproportion\n(5) ... Sony’s effort to hire\nproducers Jon Peters and\nPeter Guber to run the\nstudio...\nbusiness, company,\nfacility, ﬁlm, lot\nlab, troupe,\nclassroom,\napartment,\nbooth\nroom, gallery,\ntroupe, journal,\nhouse\nroom, troupe, lab,\naudience, department\n(6) ... I had [...] told her that\nwe needed other company\nthan our own ...\nfriend, acquaintance,\nvisitor,\naccompaniment,\nassociate\nretailer, trader,\nﬁrm, maker,\nsupplier\nﬁrm, corporation,\norganisation,\nconglomerate,\nretailer\ncorporation, ﬁrm,\nconglomerate, retailer,\norganisation\nTable 3: Examples with nearest neighbours of the representations predicted in the ﬁrst current hidden layer.\ntion in the word embedding, making it challenging\nto diverge from it when contextually relevant (see\nEx. (6) in Table 3).\nCurrent vs. predictive hidden states.The pre-\ndictive hidden states are obtained without observ-\ning the target word; hence, recovering word in-\nformation is considerably harder than for current\nstates. Indeed, we observe worse results in this\ncondition (e.g., below avgctxt in the WORD task);\nwe also observe two patterns that are opposite to\nthose observed for current states, which shed light\non how LSTM LMs track word information.\nFor predictive states, results improve closer to\nthe output (from layer 1 to 3; they instead degrade\nfor current states). We link this to the double ob-\njective that a LM has when it comes to word infor-\nmation: to integrate a word passed as input, and to\npredict one as output. Our results suggest that the\nhidden states keep track of information for both\nwords, but lower layers focus more on the process-\ning of the input and higher ones on the predictive\naspect (see Fig. 1). This is in line with previous\nwork showing that activations close to the output\ntend to be task-speciﬁc (Liu et al., 2019).\nMoreover, from predictive states, it is easier\nto retrieve contextual than lexical representations\n(.41/.43 vs. .29; the opposite was true for current\nstates). Our hypothesis is that this is due to a com-\nbination of two factors. On the one hand, pre-\ndictive states are based solely on contextual infor-\nmation, which highlights only certain aspects of a\nword; for instance, the context of Ex. (2) in Ta-\nble 1 clearly signals that a noun is expected, and\nthe predictive states in a LM should be sensitive to\nthis kind of cue, as it affects the probability distri-\nbution over words. On the other hand, lexical rep-\nresentations are underspeciﬁed; for instance, the\nword embedding for show abstracts over both ver-\nbal and nominal uses of the word. Thus, it makes\nsense that the predictive state does not capture\ncontextually irrelevant aspects of the word embed-\nding, unlike the current state (note however that, as\nstated above, the overall performance of the cur-\nrent state is better, because it has access to the\nword actually produced).\n4 Future work\nWe introduced a method to study how deep learn-\ning models of language deal with lexical ambigu-\nity. Though we focused on LSTM LMs for En-\nglish, this method can be applied to other architec-\ntures, objective tasks, and languages; possibilities\nto explore in future work. We also plan to carry\nout further analyses aimed at individuating factors\nthat challenge the resolution of lexical ambigu-\nity (e.g., morphosyntactic vs. semantic ambiguity,\nfrequency of a word or sense, ﬁgurative uses), as\nwell as clarifying the interaction between predic-\ntion and processing of words within neural LMs.\nAcknowledgements\nThis project has received funding from the Eu-\nropean Research Council (ERC) under the Euro-\npean Union’s Horizon 2020 research and inno-\nvation programme (grant agreement No 715154),\nand from the Ram ´on y Cajal programme (grant\nRYC-2015-18907). We gratefully acknowledge\nthe support of NVIDIA Corporation with the do-\nnation of GPUs used for this research, and the\ncomputer resources at CTE-POWER and the tech-\nnical support provided by Barcelona Supercom-\nputing Center (RES-FI-2018-3-0034). This paper\nreﬂects the authors’ view only, and the EU is not\nresponsible for any use that may be made of the\ninformation it contains.\n\n3347\nReferences\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer\nLavi, and Yoav Goldberg. 2017. Fine-grained anal-\nysis of sentence embeddings using auxiliary predic-\ntion tasks. In Proceedings of 5th ICLR International\nConference on Learning Representations.\nMarco Baroni. 2013. Composition in distributional\nsemantics. Language and Linguistics Compass ,\n7(10):511–522.\nJose Camacho-Collados and Taher Pilehvar. 2018.\nFrom word to sense embeddings: A survey on vec-\ntor representations of meaning. Journal of Artiﬁcial\nIntelligence, 63(1):743–788.\nAlexis Conneau, Germ ´an Kruszewski, Guillaume\nLample, Lo ¨ıc Barrault, and Marco Baroni. 2018.\nWhat you can cram into a single $&!#* vector:\nProbing sentence embeddings for linguistic proper-\nties. In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics ,\npages 2126–2136.\nAlan Cruse. 1986. Lexical semantics. Cambridge Uni-\nversity Press.\nKatrin Erk. 2010. What is word meaning, really? (and\nhow can distributional models help us describe it?).\nIn Proceedings of the 2010 Workshop on Geometri-\ncal Models of Natural Language Semantics , pages\n17–26.\nKatrin Erk. 2012. Vector space models of word mean-\ning and phrase meaning: A survey. Language and\nLinguistics Compass, 6(10):635–653.\nKatrin Erk and Sebastian Pad´o. 2008. A structured vec-\ntor space model for word meaning in context. In\nProceedings of the EMNLP Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n897–906.\nAdriano Ferraresi, Eros Zanchetta, Marco Baroni, and\nSilvia Bernardini. 2008. Introducing and evaluating\nukWac, a very large web-derived corpus of English.\nIn Proceedings of the 4th Web as Corpus Workshop\n(WAC-4) Can we beat Google, pages 47–54.\nAina Gar´ı Soler, Anne Cocos, Marianna Apidianaki,\nand Chris Callison-Burch. 2019. A comparison of\ncontext-sensitive models for lexical substitution. In\nProceedings of the 13th International Conference on\nComputational Semantics (IWCS).\nKristina Gulordava, Piotr Bojanowski, Edouard Grave,\nTal Linzen, and Marco Baroni. 2018. Colorless\ngreen recurrent networks dream hierarchically. In\nProceedings of the 2018 NAACL-HLT Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 1195–1205.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nDieuwke Hupkes, Sara Veldhoen, and Willem\nZuidema. 2018. Visualisation and ’diagnostic classi-\nﬁers’ reveal how recurrent and recursive neural net-\nworks process hierarchical structure. Journal of Ar-\ntiﬁcial Intelligence Research, 61:907–926.\nRafal Jozefowicz, Wojciech Zaremba, and Ilya\nSutskever. 2015. An empirical exploration of recur-\nrent network architectures. In International Confer-\nence on Machine Learning, pages 2342–2350.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nGerhard Kremer, Katrin Erk, Sebastian Pad ´o, and Ste-\nfan Thater. 2014. What substitutes tell us-analysis of\nan” all-words” lexical substitution corpus. In Pro-\nceedings of the 14th EACL Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics, pages 540–549.\nGeoffrey Neil Leech. 1992. 100 million words of En-\nglish: the British National corpus (BNC).\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics , 4:521–\n535.\nNelson F Liu, Matt Gardner, Yonatan Belinkov,\nMatthew Peters, and Noah A Smith. 2019. Linguis-\ntic knowledge and transferability of contextual rep-\nresentations. In Proceedings of the 2019 NAACL-\nHLT Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies.\nDiana McCarthy and Roberto Navigli. 2009. The En-\nglish lexical substitution task. Language resources\nand evaluation, 43(2):139–159.\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional lstm. In Proceedings\nof the 20th SIGNLL Conference on Computational\nNatural Language Learning, pages 51–61.\nJeff Mitchell and Mirella Lapata. 2010. Composition\nin distributional models of semantics. Cognitive sci-\nence, 34(8):1388–1429.\nAdam Paszke, Sam Gross, Soumith Chintala, Gre-\ngory Chanan, Edward Yang, Zachary DeVito, Zem-\ning Lin, Alban Desmaison, Luca Antiga, and Adam\nLerer. 2017. Automatic differentiation in PyTorch.\nIn NIPS Autodiff Workshop.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 NAACL-HLT\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2227–2237.\n3348\nMike Schuster and Kuldip K Paliwal. 1997. Bidirec-\ntional recurrent neural networks. IEEE Transactions\non Signal Processing, 45(11):2673–2681.\nSteven L Small, Garrison W Cottrell, and Michael K\nTanenhaus. 2013. Lexical Ambiguity Resolution:\nPerspective from Psycholinguistics, Neuropsychol-\nogy and Artiﬁcial Intelligence. Elsevier.\nStefan Thater, Hagen F ¨urstenau, and Manfred Pinkal.\n2011. Word meaning in context: A simple and ef-\nfective vector model. In Proceedings of 5th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing, pages 1134–1143.\nMatthijs Westera and Gemma Boleda. 2019. Don’t\nblame distributional semantics if it can’t do entail-\nment. In Proceedings of the 13th International Con-\nference on Computational Semantics (IWCS), pages\n120–133.\nA Appendix\nA.1 Language model\nThe hidden layers are of sizes 600/600/300 re-\nspectively, while the word embeddings are of size\n300. The language model was trained optimizing\nthe log-likelihood of a target word given its sur-\nrounding context, with stochastic gradient descent\nfor 20 epochs with decaying learning rate using\nAdam optimiser (Kingma and Ba, 2014). The ini-\ntial learning rate was 0.0005 for batch size of 32.\nDropout was set to 0.2 and applied to the input em-\nbedding, and the outputs of the LSTM layers. At\ntraining time, the text data is fed to the model in\nsequences of 100 tokens.\nA.2 Diagnostic models\nWe train separate models for each combination\nof task and input type. Each model consist of\na linear transformation and a tahn non-linearity,\ntrained using Cosine Embedding Loss (PyTorch\n0.4, Paszke et al., 2017) and Adam optimiser, with\nearly stopping based on validation loss. We car-\nried out hyperparameter search based on valida-\ntion loss for each of the model types in order to\nset batch size and initial learning rate. We report\nthe ﬁnal settings for each combination of input and\ntask in Table 4.\nAt training time, for each positive target word,\nwe obtain 5 negative targets by sampling words\nfrom the frequency quartile of the postive target\n(frequency is computed on the training corpus of\nthe language model). We always exclude the tar-\nget word, as well as the substitute words in the\ninput W ORD SUB WORD &SUB\nh1\nt 16, 5 × 10−5 32, 1 × 10−4 32, 5 × 10−5\nh2\nt 16, 5 × 10−5 64, 5 × 10−4 64, 5 × 10−4\nh3\nt 16, 5 × 10−5 128, 5 × 10−4 16, 5 × 10−5\nh1\nt±1 128, 1 × 10−3 128, 1 × 10−3 128, 5 × 10−4\nh2\nt±1 16, 1 × 10−4 64, 5 × 10−4 16, 5 × 10−4\nh3\nt±1 128, 1 × 10−3 16, 1 × 10−4 128, 5 × 10−4\nTable 4: Hyperparameter settings in the diagnostic\nmodels (batch size, initial learning rate)\nSUB and WORD &SUB conditions, from the neg-\native samples. Given the input vector, we maxi-\nmize the margin of the resulting output vector ˆrto\nthe embeddings of the negative samples (i= −1),\nand minimize the distance of the output vector to\nthe target representation of the positive instance\n(i= 1; Eq. 8).\nL(ˆr,r,i ) =\n\n\n\n1 −cos(ˆr,r) if i= 1\nif i= −1\nmax(0,cos(ˆr,r) −margin)\n(8)\nAt each training epoch, new negative instances are\nsampled, and the data is shufﬂed.",
  "topic": "Ambiguity",
  "concepts": [
    {
      "name": "Ambiguity",
      "score": 0.8535414338111877
    },
    {
      "name": "Computer science",
      "score": 0.7400718331336975
    },
    {
      "name": "Natural language processing",
      "score": 0.6657391786575317
    },
    {
      "name": "Artificial intelligence",
      "score": 0.629762589931488
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5354165434837341
    },
    {
      "name": "Word (group theory)",
      "score": 0.5208621621131897
    },
    {
      "name": "Lexical choice",
      "score": 0.4364674687385559
    },
    {
      "name": "Invariant (physics)",
      "score": 0.4361562132835388
    },
    {
      "name": "Linguistics",
      "score": 0.4251973032951355
    },
    {
      "name": "Language model",
      "score": 0.4146224558353424
    },
    {
      "name": "Lexical item",
      "score": 0.3629980981349945
    },
    {
      "name": "Mathematics",
      "score": 0.10022640228271484
    },
    {
      "name": "Mathematical physics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}