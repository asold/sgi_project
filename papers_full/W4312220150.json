{
  "title": "A large language model for electronic health records",
  "url": "https://openalex.org/W4312220150",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2097842689",
      "name": "Yang Xi",
      "affiliations": [
        "UF Health Cancer Center",
        "University of Florida",
        "University of Florida Health"
      ]
    },
    {
      "id": "https://openalex.org/A4287354252",
      "name": "Chen, Aokun",
      "affiliations": [
        "UF Health Cancer Center",
        "University of Florida",
        "University of Florida Health"
      ]
    },
    {
      "id": "https://openalex.org/A4221859381",
      "name": "PourNejatian, Nima",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4221859382",
      "name": "Shin, Hoo Chang",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4221859383",
      "name": "Smith, Kaleb E.",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4221859384",
      "name": "Parisien, Christopher",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4221859385",
      "name": "Compas, Colin",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4221859386",
      "name": "Martin, Cheryl",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4378230282",
      "name": "Costa, Anthony B",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A4221859387",
      "name": "Flores, Mona G",
      "affiliations": [
        "Nvidia (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1832796170",
      "name": "Zhang Ying",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A4221859389",
      "name": "Magoc, Tanja",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A4221859390",
      "name": "Harle, Christopher A",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A4221859391",
      "name": "Lipori, Gloria",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A4221859392",
      "name": "Mitchell, Duane A",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A4221859393",
      "name": "Hogan, William R.",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A4221864694",
      "name": "Shenkman, Elizabeth A",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A2101338964",
      "name": "Bian, Jiang",
      "affiliations": [
        "University of Florida",
        "University of Florida Health",
        "UF Health Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A1993644842",
      "name": "Wu, Yonghui",
      "affiliations": [
        "UF Health Cancer Center",
        "University of Florida Health",
        "University of Florida"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2748099698",
    "https://openalex.org/W2602583982",
    "https://openalex.org/W2940685834",
    "https://openalex.org/W2114388055",
    "https://openalex.org/W2911462778",
    "https://openalex.org/W3201408301",
    "https://openalex.org/W2169818249",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W6683738474",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2467995757",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3027260829",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W3035375600",
    "https://openalex.org/W3104415840",
    "https://openalex.org/W3034328552",
    "https://openalex.org/W3173229273",
    "https://openalex.org/W55204438",
    "https://openalex.org/W3026565924",
    "https://openalex.org/W3176762756",
    "https://openalex.org/W3105063288",
    "https://openalex.org/W2985884876",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2998230451",
    "https://openalex.org/W3001393026",
    "https://openalex.org/W2997090102",
    "https://openalex.org/W6800751262",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W3104578551",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2168041406",
    "https://openalex.org/W2137407193",
    "https://openalex.org/W2970198438",
    "https://openalex.org/W2993961432",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W1847618513",
    "https://openalex.org/W2769851464",
    "https://openalex.org/W3005237274",
    "https://openalex.org/W2613482391",
    "https://openalex.org/W3109225475",
    "https://openalex.org/W2739351760",
    "https://openalex.org/W2972297345",
    "https://openalex.org/W2800055384",
    "https://openalex.org/W2171313960",
    "https://openalex.org/W2889272240",
    "https://openalex.org/W2888285200",
    "https://openalex.org/W3094834348",
    "https://openalex.org/W3109919947",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W6884859834",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3100468923",
    "https://openalex.org/W2913352150",
    "https://openalex.org/W2891113091",
    "https://openalex.org/W3035129496",
    "https://openalex.org/W3088409176",
    "https://openalex.org/W3035204084",
    "https://openalex.org/W4212774754",
    "https://openalex.org/W2398489001",
    "https://openalex.org/W2599674900",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W3101613385",
    "https://openalex.org/W3196412195"
  ],
  "abstract": null,
  "full_text": "ARTICLE OPEN\nA large language model for electronic health records\nXi Yang1,2, Aokun Chen1,2, Nima PourNejatian3, Hoo Chang Shin3, Kaleb E. Smith3, Christopher Parisien3, Colin Compas3,\nCheryl Martin3, Anthony B. Costa3, Mona G. Flores 3, Ying Zhang 4, Tanja Magoc5, Christopher A. Harle1,5, Gloria Lipori5,6,\nDuane A. Mitchell6, William R. Hogan 1, Elizabeth A. Shenkman 1, Jiang Bian 1,2 and Yonghui Wu 1,2 ✉\nThere is an increasing interest in developing artiﬁcial intelligence (AI) systems to process and interpret electronic health records\n(EHRs). Natural language processing (NLP) powered by pretrained language models is the key technology for medical AI systems\nutilizing clinical narratives. However, there are few clinical language models, the largest of which trained in the clinical domain is\ncomparatively small at 110 million parameters (compared with billions of parameters in the general domain). It is not clear how\nlarge clinical language models with billions of parameters can help medical AI systems utilize unstructured EHRs. In this study, we\ndevelop from scratch a large clinical language model— GatorTron— using >90 billion words of text (including >82 billion words of\nde-identiﬁed clinical text) and systematically evaluate it onﬁve clinical NLP tasks including clinical concept extraction, medical\nrelation extraction, semantic textual similarity, natural language inference (NLI), and medical question answering (MQA). We\nexamine how (1) scaling up the number of parameters and (2) scaling up the size of the training data could beneﬁt these NLP tasks.\nGatorTron models scale up the clinical language model from 110 million to 8.9 billion parameters and improveﬁve clinical NLP\ntasks (e.g., 9.6% and 9.5% improvement in accuracy for NLI and MQA), which can be applied to medical AI systems to improve\nhealthcare delivery. The GatorTron models are publicly available at:https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/models/\ngatortron_og.\nnpj Digital Medicine          (2022) 5:194 ; https://doi.org/10.1038/s41746-022-00742-2\nINTRODUCTION\nThere is an increasing interest in developing artiﬁcial intelligence\n(AI) systems to improve healthcare delivery and health outcomes\nusing electronic health records (EHRs). A critical step is to extract\nand capture patients’ characteristics from longitudinal EHRs. The\nmore information we have about the patients, the better the\nmedical AI systems that we can develop. In recent decades,\nhospitals and medical practices in the United States (US) have\nrapidly adopted EHR systems\n1,2, resulting in massive stores of\nelectronic patient data, including structured (e.g., disease codes,\nmedication codes) and unstructured (i.e., clinical narratives such as\nprogress notes). Even though using discrete dataﬁelds in clinical\ndocumentation has many potential advantages and structured\ndata entry ﬁelds are increasingly added into the EHR systems,\nhaving clinicians use them remains a barrier, due to the added\ndocumentation burden\n3. Physicians and other healthcare provi-\nders widely use clinical narratives as a more convenient way to\ndocument patient information ranging from family medical\nhistories to social determinants of health\n4. There is an increasing\nnumber of medical AI systems exploring the rich, more ﬁne-\ngrained patient information captured in clinical narratives to\nimprove diagnostic and prognostic models\n5,6. Nevertheless, free-\ntext narratives cannot be easily used in computational models that\nusually require structured data. Researchers have increasingly\nturned to natural language processing (NLP) as the key\ntechnology to enable medical AI systems to understand clinical\nlanguage used in healthcare\n7.\nToday, most NLP solutions are based on deep learning models8\nimplemented using neural network architectures — a fast-\ndeveloping sub-domain of machine learning. Convolutional neural\nnetworks9 (CNN) and recurrent neural networks 10 (RNN) have\nbeen applied to NLP in the early stage of deep learning. More\nrecently, the transformer architectures11 (e.g., Bidirectional Enco-\nder Representations from Transformers [BERT]) implemented with\na self-attention mechanism 12 have become state-of-the-art,\nachieving the best performance on many NLP benchmarks13–16.\nIn the general domain, the transformer-based NLP models have\nachieved state-of-the-art performance for name entity recogni-\ntion17–19, relation extraction20–24, sentence similarity25–27, natural\nlanguage inference27–30, and question answering27,28,31,32. Typi-\ncally, transformers are trained in two stages: language model\npretraining (i.e., learning using a self-supervised training objective\non a large corpus of unlabeled text) andﬁne-tuning (i.e., applying\nthe learned language models solving speciﬁc tasks with labeled\ntraining data). One pretrained language model can be applied to\nsolve many NLP tasks through ﬁne-tuning, which is known as\ntransfer learning— a strategy to learn knowledge from one task\nand apply it in another task33. Human language has a very large\nsample space— the possible combinations of words, sentences,\nand their meaning and syntax are innumerable. Recent studies\nshow that large transformer models trained using massive text\ndata are remarkably better than previous NLP models in terms of\nemergence and homogenization\n33.\nThe promise of transformer models has led to further interest in\nexploring large-size (e.g., >billions of parameters) transformer\nmodels. The Generative Pretrained Transformer 3 (GPT-3) model34,\nwhich has 175 billion parameters and was trained using >400\nbillion words of text demonstrated superior performance. In the\nbiomedical domain, researchers developed BioBERT\n11 (with 110\nmillion parameters) and PubMedBERT35 (110 million parameters)\n1Department of Health Outcomes and Biomedical Informatics, College of Medicine, University of Florida, Gainesville, FL, USA.2Cancer Informatics and eHealth core, University of\nFlorida Health Cancer Center, Gainesville, FL, USA.3NVIDIA, Santa Clara, CA, USA.4Research Computing, University of Florida, Gainesville, FL, USA.5Integrated Data Repository\nResearch Services, University of Florida, Gainesville, FL, USA. 6Lillian S. Wells Department of Neurosurgery, UF Clinical and Translational Science Institute, University of\nFlorida, Gainesville, FL, USA.✉email: yonghui.wu@uﬂ.edu\nwww.nature.com/npjdigitalmed\nPublished in partnership with Seoul National University Bundang Hospital\n1234567890():,;\ntransformer models using biomedical literature from PubMed.\nNVIDIA developed BioMegatron models in the biomedical domain\nwith different sizes from 345 million to 1.2 billion parameters\n36\nusing a more expansive set of PubMed-derived free text. However,\nfew studies have explored scaling transformer models in the\nclinical domain due to the sensitive nature of clinical narratives\nthat contain Protected Health Information (PHI) and the signiﬁcant\ncomputing power required to increase the size of these models.\nTo date, the largest transformer model using clinical narratives is\nClinicalBERT\n37. ClinicalBERT has 110 million parameters and was\ntrained using 0.5 billion words from the publicly available Medical\nInformation Mart for Intensive Care III 38 (MIMIC-III) dataset. By\ndeveloping not only larger models, but models that use clinical\nnarratives, NLP may perform better to improve healthcare delivery\nand patient outcomes.\nIn this study, we develop a large clinical language model,\nGatorTron, using >90 billion words of text from the de-identiﬁed\nclinical notes of University of Florida (UF) Health, PubMed articles,\nand Wikipedia. We train GatorTron from scratch and empirically\nevaluate how scaling up the number of parameters beneﬁtt h e\nperformance of downstream NLP tasks. More speci ﬁcally, we\nexamine GatorTron models with varying number of parameters\nincluding (1) a base model with 345 million parameters, (2) a\nmedium model with 3.9 billion parameters, and (3) a large model\nwith 8.9 billion parameters. We also examine how scaling up data\nsize beneﬁt downstream tasks by comparing the GatorTron-base\nmodel trained from the full corpus with another GatorTron-base\nmodel trained using a random sample of 1/4 of the corpus. We\ncompare GatorTron with existing transformer models trained using\nbiomedical literature and clinical narratives usingﬁve clinical NLP\ntasks including clinical concept extraction (or named entity\nrecognition [NER]), medical relation extraction (MRE), semantic\ntextual similarity (STS), natural language inference (NLI), and medical\nquestion answering (MQA). GatorTron models outperform previous\ntransformer models from the biomedical and clinical domain onﬁve\nclinical NLP tasks. This study scales up transformer models in the\nclinical domain from 110 million to 8.9 billion parameters and\ndemonstrates the beneﬁt of large transformer models.\nRESULTS\nA total number of 290,482,002 clinical notes from 2,476,628\npatients were extracted from the UF Health Integrated Data\nRepository (IDR), the enterprise data warehouse of the UF Health\nsystem. These notes were created from 2011–2021 from over 126\nclinical departments and ~50 million encounters covering\nhealthcare settings including but not limited to inpatient,\noutpatient, and emergency department visits. After preprocessing\nand de-identiﬁcation, the corpus included >82 billion medical\nwords. Figure 1 summarizes the distribution of patient by age,\ngender, race, and ethnicity as well as the distribution of notes by\nclinical department (top 5) and note type (top 5). The detailed\nnumber of patients by each category, a full list of clinical\ndepartments and the corresponding proportion of notes, and a\nfull list of note types were provided in Supplementary Table 1,\nSupplementary Table 2, and Supplementary Table 3.\nTraining GatorTron-large model required ~6 days on 992 A100\n80 G GPUs from 124 NVIDIA DGX notes using the NVIDIA\nSuperPOD reference cluster architecture. Figure 2 shows the\ntraining validation loss for all three sizes of GatorTron models.\nThe GatorTron-base model converged in 10 epochs, whereas the\nmedium and large models converged in 7 epochs, which is\nconsistent with prior observations on the faster per sample\nconvergence of larger transformer models.\nTable 1 and Table 2 compare GatorTron models with two\nexisting biomedical transformer models (BioBERT and BioMega-\ntron) and one clinical transformer model (Clinical BERT) onﬁve\nclinical NLP tasks.\nScale up the size of training data and the number of\nparameters\nCompared with GatorTron-base trained using a random sample of\n1/4 of the corpus, the GatorTron-base model trained using the full\ncorpus achieved improved performance for four tasks except for a\nsub-task in MQA (on F1 score of medication-related questions). By\nscaling up the number of parameters from 345 million to 8.9\nbillion, GatorTron-large demonstrated remarkable improvements\nfor all ﬁve tasks, suggesting that GatorTron models scale for\ncanonical clinical downstream tasks and that we are not yet at\nthe limit.\nRecognize clinical concepts and medical relations\nClinical concept extraction is to identify the concepts with\nimportant clinical meanings and classify their semantic categories\n(e.g., diseases, medications). As shown in Table 1, all three\nFig. 1 Patient distribution by age, gender, race, ethnicity; clinical notes distribution by note type, and clinical department.Ages were\ncalculated as of September 2022.\nX. Yang et al.\n2\nnpj Digital Medicine (2022)   194 Published in partnership with Seoul National University Bundang Hospital\n1234567890():,;\nGatorTron models outperformed existing biomedical and clinical\ntransformer models in recognizing various types of clinical\nconcepts on the three benchmark datasets (i.e., 2010 i2b2\n39 and\n2012 i2b240: problem, treatments, lab tests; 2018 n2c241: drug,\nadverse events, and drug-related attributes). The GatorTron-large\nmodel outperformed the other two smaller GatorTron models and\nachieved the best F1 scores of 0.8996, 0.8091, and 0.9000,\nrespectively. For medical relation extraction— a task to identify\nmedical relations between two clinical concepts— the GatorTron-\nlarge model also achieved the best F1 score of 0.9627 for\nidentifying drug-cause-adverse event relations outperforming\nexisting biomedical and clinical transformers and the other two\nsmaller GatorTron models. We consistently observed performance\nimprovement when scaling up the size of the GatorTron model.\nAssess semantic textual similarity\nThe task of measuring semantic similarity is to determine the\nextent to which two sentences are similar in terms of semantic\nmeaning. As shown in Table2, all GatorTron models outperformed\nb.a.\nFig. 2 Training loss and validation loss for GatorTron-base (345 million), medium (3.9 billion), and large (8.9 billion) models. aTraining\nloss. b Validation loss. MLM masked language modeling.\nTable 1. Comparison of GatorTron with existing biomedical and clinical transformer models for clinical concept extraction and medical relation\nextraction.\nTransformer Clinical concept extraction Medical relation extraction\n2010 i2b239 2012 i2b240 2018 n2c241 2018 n2c241\nPrecision Recall F1 score Precision Recall F1 score Precision Recall F1 score Precision Recall F1 score\nBioBERT 0.8693 0.8653 0.8673 0.7478 0.8037 0.7747 0.8634 0.8921 0.8775 0.9663 0.9451 0.9555\nClinicalBERT NA NA 0.8780 NA NA 0.7890 0.8592 0.8832 0.8710 0.9678 0.9414 0.9544\nBioMegatron 0.8614 0.8761 0.8687 0.7591 0.8031 0.7805 0.8707 0.8915 0.8810 0.9711 0.9434 0.9571\nGatorTron-base (1/4 data) 0.8682 0.9046 0.8860 0.7514 0.8013 0.7755 0.8772 0.8992 0.8881 0.9724 0.9457 0.9589\nGatorTron-base 0.8748 0.9043 0.8893 0.7644 0.8221 0.7922 0.8759 0.9038 0.8896 0.9719 0.9482 0.9599\nGatorTron-medium 0.8869 0.9122 0.8994 0.7812 0.8245 0.8022 0.8954 0.9035 0.8994 0.9721 0.9503 0.9611\nGatorTron-large 0.8880 0.9116 0.8996 0.7862 0.8333 0.8091 0.8979 0.9021 0.9000 0.9776 0.9482 0.9627\nClinical concepts in 2010 i2b2 and 2012 i2b2 challenges: problems, treatments, lab tests; clinical concepts in 2018 n2c2 challenge: drugs, adverse events, and\ndrug-related attributes (e.g., dose). Medical relation in 2018 n2c2 challenge: drug induced adverse events. Best F1 scores are presented in bold. NA: scores not\nreported.\nTable 2. Comparison of GatorTron with existing biomedical and clinical transformer models for semantic textual similarity, natural language\ninference, and question answering.\nTransformer Semantic textual similarity Natural language inference Question answering\n2019 n2c266 MedNLI71 emrQA medication77 emrQA relation77\nPearson correlation Accuracy F1 score Exact Match F1 score Exact Match\nBioBERT 0.8744 0.8050 0.6997 0.2475 0.9262 0.8361\nClinicalBERT 0.8787 0.8270 0.6905 0.2406 0.9306 0.8533\nBioMegatron 0.8806 0.8390 0.7231 0.2882 0.9405 0.879\nGatorTron-base (1/4 data) 0.8675 0.8643 0.7281 0.2952 0.9390 0.8579\nGatorTron-base 0.8810 0.8670 0.7181 0.2978 0.9543 0.9029\nGatorTron-medium 0.8903 0.8720 0.7354 0.3018 0.9677 0.9243\nGatorTron-large 0.8896 0.9020 0.7408 0.3155 0.9719 0.9310\nThe best evaluation scores are presented in bold.\nX. Yang et al.\n3\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2022)   194 \n1234567890():,;\nexisting biomedical and clinical transformer models. Among the\nthree GatorTron models, the GatorTron-medium model achieved\nthe best Pearson correlation score of 0.8903, outperforming both\nGatorTron-base and GatorTron-large. Although we did not\nobserve consistent improvement by scaling up the size of the\nGatorTron model, the GatorTron-large model outperformed\nGatorTron-base and its performance is very close to the\nGatorTron-medium model (0.8896 vs. 0.8903).\nNatural language inference\nThe task of NLI is to determine whether a conclusion can be\ninferred from a given sentence— a sentence-level NLP task. As\nshown in Table 2, all GatorTron models outperformed existing\nbiomedical and clinical transformers, and the GatorTron-large\nmodel achieved the best accuracy of 0.9020, outperforming the\nBioBERT and ClinicalBERT by 9.6% and 7.5%, respectively. We\nobserved a monotonic performance improvement by scaling up\nthe size of the GatorTron model.\nMedical question answering\nMQA is a complex clinical NLP task that requires understand\ninformation from the entire document. As shown in Table2, all\nGatorTron models outperformed existing biomedical and clinical\ntransformer models in answering medication and relation-related\nquestions (e.g., “What lab results does patient have that are\npertinent to diabetes diagnosis?”). For medication-related ques-\ntions, the GatorTron-large model achieved the best exact match\nscore of 0.3155, outperforming the BioBERT and ClinicalBERT by\n6.8% and 7.5%, respectively. For relation-related questions,\nGatorTron-large also achieved the best exact match score of\n0.9301, outperforming BioBERT and ClinicalBERT by 9.5% and\n7.77%, respectively. We also observed a monotonic performance\nimprovement by scaling up the size of the GatorTron model.\nDISCUSSION\nIn this study, we developed a large clinical transformer model,\nGatorTron, using a corpus of >90 billion words from UF Health\n(>82 billion), Pubmed (6 billion), Wikipedia (2.5 billion), and MIMIC\nIII (0.5 billion). We trained GatorTron with different number of\nparameters including 345 million, 3.9 billion, and 8.9 billion and\nevaluated its performance on 5 clinical NLP tasks at different\nlinguistic levels (phrase level, sentence level, and document level)\nusing 6 publicly available benchmark datasets. The experimental\nresults show that GatorTron models outperformed existing\nbiomedical and clinical transformers for allﬁve clinical NLP tasks\nevaluated using six different benchmark datasets. We observed\nmonotonic improvements by scaling up the model size of\nGatorTron for four of the ﬁve tasks, excluding the semantic\ntextual similarity task. Our GatorTron model also outperformed the\nBioMegatron\n36, a transformer model with a similar model size\ndeveloped in our previous study using >8.5 billion words from\nPubMed and Wikipedia (a small proportion of the >90 billion\nwords of corpus for developing GatorTron). This study scaled up\nthe clinical transformer models from 345 million (ClinicalBERT) to\n8.9 billion parameters in the clinical domain and demonstrated\nremarkable performance improvements. To the best of our\nknowledge, GatorTron-large is the largest transformer model in\nthe clinical domain. Among the ﬁve tasks, GatorTron achieved\nremarkable improvements for complex NLP tasks such as natural\nlanguage inference and medical question answering, but moder-\nate improvements for easier tasks such as clinical concept\nextraction and medical relation extraction, indicating that large\ntransformer models are more helpful to complex NLP tasks. These\nresults are consistent with observations in the literature on the\nsaturation of simpler benchmarks with large BERT\narchitectures\n18,32.\nGatorTron was pretrained using self-supervised masked lan-\nguage modeling (MLM) objective. We monitored training loss and\ncalculated validation loss using a subset set of the clinical text\n(5%) to determine the appropriate stopping time. From the plots\nof training and validation losses in Fig.2, we observed that larger\nGatorTron models converged faster than the smaller model.\nGatorTron models perform better in extracting and interpreting\npatient information documented in clinical narratives, which can\nbe integrated into medical AI systems to improve healthcare\ndelivery and patient outcomes. The rich, ﬁne-grained patient\ninformation captured in clinical narratives is a critical resource\npowering medical AI systems. With better performance in\ninformation extraction (e.g., clinical concept extraction and\nmedical relation extraction), GatorTron models can provide more\naccurate patient information to identify research-standard patient\ncohorts using computable phenotypes, support physicians making\ndata-informed decisions by clinical decision support systems, and\nidentify adverse events associated with drug exposures for\npharmacovigilance. The observed improvements in semantic\ntextual similarity, natural language inference, and medical\nquestion answering can be applied for deduplication of clinical\ntext, mining medial knowledge, and developing next-generation\nmedical AI systems that can interact with patients using human\nlanguage.\nWe conducted error analysis and compared GatorTron with\nClinicalBERT to probe the observed performance improvements.\nWe found that the larger, domain-speciﬁc pretrained models (e.g.,\nGatorTron) are better at modeling longer phrases and determining\nsemantic categories. For example, GatorTron successfully identi-\nﬁed “a mildly dilated ascending aorta ”, where ClinicalBERT\nidentiﬁed only “mildly dilated” as a problem; GatorTron success-\nfully categorized “kidney protective effects” as a “TREATMENT”,\nwhich was mis-classi ﬁed as “PROBLEM” by ClinicalBERT. For\ncomplex NLP tasks such as NLI and MQA, even large language\nmodels such as GatorTron still have difﬁculty in identifying the key\npieces of information from longer paragraphs. Our future work will\nimprove GatorTron in handling long pieces of text for complex\nNLP tasks.\nThis study demonstrates the advantages of large pretrained\ntransformer models in the medical domain. GatorTron models can\nbe applied to many other NLP tasks through ﬁne-tuning. We\nbelieve that GatorTron will improve the use of clinical narratives in\ndeveloping various medical AI systems for better healthcare\ndelivery and health outcomes.\nMETHODS\nData source\nThe primary data source for this study is the clinical narratives\nfrom UF Health IDR, a research data warehouse of UF Health. This\nstudy was approved by the UF Institutional Review Board\n(IRB202100049). We collected clinical notes from 2011–2021 from\nover 126 departments, ~2 million patients and 50 million\nencounters from inpatient, outpatient, and emergency settings.\nThen, we merged the UF Health clinical corpus with three\nadditional corpora, including the MIMIC-III corpus\n38 in the clinical\ndomain with 0.5 billion words, a PubMed (combining PubMed\nabstracts and full-text commercial-collection) collection\n36 in the\nbiomedical domain with 6 billion words, and a Wikipedia articles\ndump36 in the general domain with 2.5 billion words, to generate\na corpus with >90 billion words.\nPreprocessing and de-identiﬁcation of text\nWe performed minimal preprocessing including (1) removing\nempty and duplicated clinical notes, unifying all text into UTF-8\nencoding, and removing illegal UTF-8 strings; (2) normalizing\nspecial characters (e.g., convert ‘&’ to ‘&;’‘\\xa0’ to ‘space’); (3)\nX. Yang et al.\n4\nnpj Digital Medicine (2022)   194 Published in partnership with Seoul National University Bundang Hospital\ntokenization and sentence boundary detection. For clinical text\nfrom UF Health, we further applied a de-identiﬁcation system42 to\nremove protected health information (PHI) from clinical text.\n(Approved under IRB202100049) We adopted the safe-harbor\nmethod to identify 18 PHI categories de ﬁned in the Health\nInsurance Portability and Accountability Act (HIPAA) and replaced\nthem with dummy strings (e.g., replace people ’s names into\n[**NAME**]).\nStudy design\nFigure 3 shows an overview of the study design. We seek to train a\nlarge clinical transformer model, GatorTron, using >90 billion\nwords and examine how and whether scaling up model size\nimproves performance on ﬁve clinical NLP tasks. We ﬁrst\npretrained GatorTron using the >90 billion words by optimizing\na masked language model (MLM) and then applied GatorTron to\nﬁve different clinical NLP tasks using a supervisedﬁne-tuning. We\nadopted the BERT architecture (Fig.4) implemented in Megatron-\nLM and explored three different settings including a base model\nof 345 million parameters (i.e., GatorTron-base), a medium model\nof 3.9 billion parameters (i.e., GatorTron-medium), and a large\nmodel of 8.9 billion parameters (i.e., GatorTron-large). Then we\ncompared the three GatorTron models to an existing transformer\nmodel from the clinical domain, ClinicalBERT (trained with 110\nmillion parameters) and two transformer models from the\nbiomedical domain, including, BioBERT (345 million parameters)\nand BioMegatron (1.2 billion parameters). We compared the\nmodels on ﬁve clinical NLP tasks, including clinical concept\nextraction, relation extraction, semantic textual similarity, natural\nlanguage inference, and medical question answering. We used six\npublic benchmark datasets in the clinical domain.\nTraining environment\nWe used a total number of 992 NVIDIA DGX A100 GPUs from\n124 superPOD nodes at UF ’s HiPerGator-AI cluster to train\nGatorTron models by leveraging both data-level and model-level\nparallelisms implemented by the Megatron-LM package\n43.W e\nmonitored the training progress by training loss and validation\nloss and stopped the training when there was no further\nimprovement (i.e., the loss plot becameﬂat).\nFig. 3 An overview of pretraining andﬁne-tuning of GatorTron models.We loaded the base model and the medium model into one GPU\nfor distributed training. We sliced the GatorTron-large model into 4 pieces and loaded model pieces to 4 GPUs for distributed training (i.e.,\nmodel parallelism). TrM transformer unit.\nFig. 4 Pretraining GatorTron-large model with 9 billion parameters using model parallelism.Emb embedding, Tok Token from input\nsentence, Trm Transformer unit. [SEP]: a token deﬁned in BERT to indicate sentence boundaries. [CLS]: a token deﬁned in BERT for sentence-\nlevel representation.\nX. Yang et al.\n5\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2022)   194 \nGatorTron model conﬁguration\nWe developed GatorTron models with three conﬁgurations and\ndetermined the number of layers, hidden sizes, and number of\nattention heads according to the guidelines for optimal depth-to-\nwidth parameter allocation proposed by Levin et al.\n44 as well as\nour previous experience in developing BioMegatron. Table 3\nprovides detailed information for the three settings. The\nGatorTron-base model has 24 layers of transformer blocks, which\nis similar to the architecture of BERT-large model. For each layer,\nwe set the number of hidden units as 1024 and attention heads as\n16. The GatorTron-medium model scaled up to 3.9 billion\nparameters (~10 times of the base setting) and the GatorTron-\nlarge model scaled up to 8.9 billion parameters, which is similar to\nBioMegatron\n43 (with 8.3 billion parameters).\nTrain GatorTron models from scratch\nWe pretrained a vocabulary from scratch using >90 billion words\nof corpus following the byte-pair-encoding algorithm 45.W e\ninherited the BERT-style architecture and trained GatorTron\nmodels from scratch using two self-supervised tasks, including\nmasked language modeling (MLM) and sentence-order prediction\n(SOP). We followed the similar strategy in the BERT model\n46 to\nrandomly mask 15% of the input tokens with a special token (i.e.,\n[MASK]) in the MLM. The SOP was formulated as a task to predict\nthe order of two consecutive segments of text\n28. The input for SOP\nconsists of two consecutive sentences from the training corpus in\nrandom orders and the training objective is to determine whether\nthe two input sentences are in the correct order. The GatorTron-\nlarge model with 8.9 billion parameters is too large toﬁt one GPU,\ntherefore, we sliced it into four pieces for distributed training\nusing model parallelism. We pretrained the GatorTron-base and\nmedium model without model slicing. The default loss function\ndeﬁned in BERT model\n46 was used. Figure4 shows the distributed\ntraining of GatorTron-large model using model parallelism. (See\nhttps://github.com/NVIDIA/Megatron-LM for more details)\nExisting transformer models for comparison\nBioBERT11: The BioBERT model was developed by further training\nthe original BERT-large model (345 million parameters, 24 layers,\n1024 hidden units, and 16 attention heads) using biomedical\nliterature from PubMed Abstracts (4.5 billion words) and PMC Full-\ntext articles (13.5 billion words). In this study, we used version 1.1.\nClinicalBERT\n37: The ClinicalBERT model was developed by\nfurther training the BioBERT (base version; 110 million parameters\nwith 12 layers, 768 hidden units, and 12 attention heads) using\nclinical text from the MIMIC-III\n38 corpus.\nBioMegatron36: The BioMegatron models adopted the BERT\narchitecture with a different number of parameters from 345\nmillion to 1.2 billion. Different from BioBERT and ClinicalBERT, the\nBioMegatron was trained from scratch without leveraging the\noriginal BERT model.\nFine-tune GatorTron forﬁve clinical NLP tasks, evaluation\nmatrices, and benchmark datasets\nWe ﬁne-tuned pretrained GatorTron models for ﬁve different\nclinical NLP tasks using experts ’ annotations from six public\nbenchmark datasets. Speciﬁcally, we ﬁrst generated distributed\nrepresentation from the inputs of a speci ﬁc task, then added\nadditional output layers (classiﬁcation or regression) to generate\ntarget outputs. We used cross-entropy (CE) loss for classiﬁcation\ntasks and mean square error loss for regression tasks. For a\nclassiﬁcation task withN categories, let C\ni be the score generated\nby a transformer model for categoryi, the probabilityPi of a given\nsample be classiﬁed to categoryi was calculated as:\nPi ¼ eCi\nPN\nj¼1 eCj\n(1)\nLet ti be the ground truth category, the cross-entropy lossLCE is\ndeﬁned as:\nLCE ¼/C0\nXN\ni¼1\ntilogðPiÞ (2)\nFine-tune GatorTron for clinical concept extraction. This is a task to\nrecognize phrases with important clinical meanings (e.g., medica-\ntions, treatments, adverse drug events). The task is to determine\nthe boundaries of a concept and classify it into prede ﬁned\nsemantic categories. Early systems for clinical concept extract are\noften rule-based, yet, most recent systems are based on machine\nlearning models such as conditional random ﬁelds (CRFs)\n47,48,\nconvolutional neural networks (CNN) 9,49, and recurrent neural\nnetworks (RNN) implemented with long-short-term memory\nstrategy (LSTM)10,50. Current state-of-the-art models are based\non transformers such as the ClinicalBERT. We approached clinical\nconcept extraction as a sequence labeling problem and adopted\n‘BIO’ labeling schema, where ‘B-’ and ‘I-’ are preﬁxes indicating\nwords at the beginning and inside of a concept, and‘O’stands for\nwords located outside of any concepts of interest. Using this\ndeﬁnition, we approached the task as a classiﬁcation problem—\nfor each word in a sentence, predict a label in [‘B’, ‘I’, ‘O’]. When\nthere are multiple categories of concepts, a sufﬁx was attached to\n‘BIO’ for discrimination (e.g., ‘B-drug’, ‘I-drug’). Based on the\nrepresentation generated by pretrained GatorTron models, we\nadded a classiﬁcation layer (a linear layer with softmax activation)\nto calculate a probability score for each‘BIO’category. The cross-\nentropy loss was used for ﬁne-tuning. We trained a uni ﬁed\nclassiﬁer to extract all concepts for datasets without overlapped\nconcepts. For datasets with overlapped concepts, we trained\nindividual models to recognize each category of concept\nseparately following our previous strategy\n51. We used three\nbenchmark datasets developed by the 2010 i2b2 challenge 39,\n2012 i2b2 challenge40, and 2018 n2c2 challenge41 to evaluate\nGatorTron models focusing on identifying important medical\nconcepts (e.g., medications, adverse drug events, treatments) from\nclinical text. We used precision, recall, and F1 score for evaluation.\nFine-tune GatorTron for medical relation extraction . MRE is to\nestablish medical-related relations (e.g., induce relation) among\nclinical concepts (e.g., drugs, adverse events). MRE is usually\napproached as a classiﬁcation problem— identify pairs of concepts\nwith valid relations and classify the relation type. Various machine\nlearning-based classiﬁers such as support vector machines (SVMs),\nrandom forests (RF), and gradient boosting trees (GBT)\n41 have\nbeen applied. With the emergence of deep learning models,\nTable 3. Technical details of GatorTron models.\nModel # Layers # Hidden size # Attention heads # Parameters\nGatorTron-base 24 1024 16 345 million\nGatorTron-medium 48 2560 40 3.9 billion\nGatorTron-large 56 3584 56 8.9 billion\nX. Yang et al.\n6\nnpj Digital Medicine (2022)   194 Published in partnership with Seoul National University Bundang Hospital\nresearchers have explored the long-short-term memory (LSTM)\narchitecture for RE in both general and clinical domains52,53. Most\nrecently, several studies adopted the BERT architecture and\ndemonstrated superior performance for MRE on various data-\nsets\n54–59. We approached MRE as a classi ﬁcation task. First,\ncandidate concept pairs were generated using heuristic rules\ndeveloped in our previous study 41. Then, we identi ﬁed two\nsentences where the two concepts in a pair were located. We\nintroduced two sets of entity markers (i.e., [S1], [E1] and [S2], [E2])\nto indicate the two concepts. If the two concepts were in the same\nsentence, the two input sentences will be the same but labeled\nwith different markers (e.g., [S1] and [E1] were used in theﬁrst\nsentence; [S2] and [E2] were used in the second sentence). To\ndetermine the relation type, we concatenated the representations\nof the model special [CLS] token and all four entity markers and\nadded a classiﬁcation layer (a linear layer with softmax activation)\nfor classiﬁcation. Similarly, the cross-entropy loss was used toﬁne-\ntune GatorTron. We used the dataset developed by the 2018 n2c2\nchallenge\n41 with a focus on relations between medications and\nadverse drug events. The precision, recall, and F1 score were used\nfor evaluation.\nFine-tune GatorTron for semantic textual similarity. The STS task is\nto quantitatively assess the semantic similarity between two text\nsnippets (e.g., sentences), which is usually approached as a\nregression task where a real-value score was used to quantify the\nsimilarity between two text snippets. In the general domain, the\nSTS benchmark (STS-B) dataset curated by the Semantic Evalua-\ntion (SemEval) challenges between 2012 and 2017\n60 is widely\nused for evaluating STS systems 13. Various machine learning\nmethods have been examined61–63 but transformer-based sys-\ntems such as RoBERTa25,T 527, and ALBERT28 are leading the state-\nof-the-art models for STS. In the clinical domain, the MedSTS\ndataset64 that consists of over 1000 annotated sentence pairs from\nclinical notes at Mayo Clinic was widely used as the benchmark.\nMedSTS was used as the gold standard in two clinical NLP open\nchallenges including the 2018 BioCreative/Open Health NLP\n(OHNLP) challenge\n65 and 2019 n2c2/OHNLP ClinicalSTS shared\ntask66. Similar to the general domain, pretrained transformer-\nbased models using clinical text and biomedical literature,\nincluding ClinicalBERT and BioBERT 67, achieved state-of-the-art\nperformance. In this study, we formulated STS as a regression\nproblem. We applied pretrained GatorTron models to learn the\nsentence-level representations of the two pieces of text and\nadopted a linear regression layer to calculate the similarity score.\nDifferent from classiﬁcation models, we used MSE as the loss\nfunction. We used the dataset developed by the 2019 n2c2/\nOHNLP\n66 challenge on clinical semantic textural similarity66. The\nPearson correlation score was used for evaluation.\nFine-tune GatorTron for natural language inference . NLI is also\nknown as recognizing textual entailment (RTE) — a directional\nrelation between text fragments (e.g., sentences)68. The goal of NLI\nis to determine if a given hypothesis can be inferred from a given\npremise. In the general domain, two benchmark datasets— the\nMultiNLI69 and the Stanford NLI 70 are widely used. On both\ndatasets, pretrained transformer models achieved state-of-the-art\nperformances\n27,29. There are limited resources for NLI in the\nclinical domain. Until recently, the MedNLI— a dataset annotated\nby doctors based on the medical history of patients 71 was\ndeveloped as a benchmark dataset in the clinical domain. A\nprevious study\n37 showed that a pretrained clinical BERT model\nachieved the state-of-the-art performance and outperformed the\nbaseline (InferSent 72) by ~9% accuracy. In this study, we\napproached NLI as a classi ﬁcation problem. We concatenated\nthe hypothesis and premise as the input separated using a special\ntoken [SEP] and applied pretrained GatorTron models to generate\ndistributed representations, which were fed into a classiﬁcation\nlayer (a linear layer with softmax activation) to calculate a\nprobability for each of the three categories of entailment,\ncontradiction, and neutral. The cross-entropy loss was used for\nﬁne-tuning. We evaluated the GatorTron models on NLI using the\nMedNLI dataset\n71 and used accuracy for comparison.\nFine-Tune GatorTron for medical question answering . The MQA\ntask is to build NLP systems that automatically answer medical\nquestions in a natural language, which is the most complex\nchallenge among the ﬁve tasks. Unlike other tasks focusing on\nphrases and sentences, MQA is a document-level task that\nrequires information from the whole document to generate\nanswers according to questions. In the general domain, the\nStanford Question Answering Datasets (SQuAD 1.1 and 2.0)\n73,74\nhave been widely used as benchmarks. Transformer-based models\nare state-of-the-art for both SQuAD1.118 and SQuAD2.031. There\nare several MQA datasets developed in the past few years such as\nthe MESHQA\n75, MedQuAD76, and emrQA 77. In this study, we\napproached MQA using a machine reading comprehension (MRC)\ntechnique where the goal is to extract the most relevant\nresponses (i.e., short text snippets or entities) from the given\ncontext according to questions. We applied a span classiﬁcation\nalgorithm to identify the start and end offsets of the answer from\nthe context. More speciﬁcally, we packed the question and the\ncontext into a single sequence as input for GatorTron and applied\ntwo linear layers to predict the start and end position of the\nanswer, respectively. As GatorTron models were developed using\na maximum token length of 512, we limited the maximum length\nof questions to 64 tokens and the rest of the 446 tokens (including\nspecial tokens such as [CLS] and [SEP]) were used for the context.\nWe truncated questions with more than 64 tokens. For contexts\nthe had more than 446 tokens, we adopted a sliding window\nstrategy to scan the whole document using a window size of 446\ntokens and a stride size of 396 tokens, so that two consecutive\nwindows had the same 50 tokens overlapped. We also limited the\nanswers to a maximum length of 32 tokens. We used the emrQA\ndataset\n77, which is widely used as a benchmark dataset for MQA.\nWe particularly focused on medications and relations-related\nquestions as Yue et al.\n78 found that the two subsets are more\nconsistent. We utilized both F1 score and exact match score for\nevaluation.\nReporting summary\nFurther information on research design is available in the Nature\nResearch Reporting Summary linked to this article.\nDATA AVAILABILITY\nThe benchmark datasets that support theﬁndings of this study are available from the\nofﬁcial websites of natural language processing challenges with Data Use\nAgreements. More speciﬁcally: (1) i2b2 2010, 2012 datasets and n2c2 2018, 2019\ndatasets: https://portal.dbmi.hms.harvard.edu/projects/n2c2-nlp/. (2) MedNLI dataset:\nhttps://physionet.org/content/mednli/1.0.0/. (3) emrQA dataset: https://github.com/\npanushri25/emrQA#download-dataset. (4) MIMIC III dataset: https://physionet.org/\ncontent/mimiciii/1.4/. (5) PubMed dataset: https://www.ncbi.nlm.nih.gov/pmc/tools/\nopenftlist/. (6) Wikipedia dataset: https://dumps.wikimedia.org/enwiki/latest/enwiki-\nlatest-pages-articles.xml.bz2. (7) UF Health IDR clinical notes are not open to the\npublic due to patient privacy information. The GatorTron models pretrained using\n>90 billion words of text is publicly available at:https://catalog.ngc.nvidia.com/orgs/\nnvidia/teams/clara/models/gatortron_og.\nCODE AVAILABILITY\nThe computer codes to train GatorTron models are available from: https://\ngithub.com/NVIDIA/Megatron-LM and https://github.com/NVIDIA/NeMo. The com-\nputer codes for preprocessing of text data are available from:https://github.com/uf-\nhobi-informatics-lab/NLPreprocessing https://github.com/uf-hobi-informatics-lab/\nGatorTron.\nX. Yang et al.\n7\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2022)   194 \nReceived: 21 June 2022; Accepted: 13 December 2022;\nREFERENCES\n1. Adoption of Electronic Health Record Systems among U.S. Non-Federal Acute\nCare Hospitals: 2008 –2015. ONC Data Brief . https://www.healthit.gov/sites/\ndefault/ﬁles/briefs/2015_hospital_adoption_db_v17.pdf (2016).\n2. Adler-Milstein, J. et al. Electronic health record adoption in US hospitals: the\nemergence of a digital ‘advanced use’ divide. J. Am. Med. Inform. Assoc. 24,\n1142–1148 (2017).\n3. Bush, R. A., Kuelbs, C. L., Ryu, J., Jian, W. & Chiang, G. J. Structured data entry in the\nelectronic medical record: perspectives of pediatric specialty physicians and\nsurgeons. J. Med. Syst.41,1 –8 (2017).\n4. Meystre, S. M., Savova, G. K., Kipper-Schuler, K. C. & Hurdle, J. F. Extracting\ninformation from textual documents in the electronic health record: a review of\nrecent research. Yearb. Med. Inform.17, 128–144 (2008).\n5. Liang, H. et al. Evaluation and accurate diagnoses of pediatric diseases using\nartiﬁcial intelligence. Nat. Med. 25, 433–438 (2019).\n6. Yang, J. et al. Assessing the prognostic signiﬁcance of tumor-inﬁltrating lym-\nphocytes in patients with melanoma using pathologic features identi ﬁed by\nnatural language processing.JAMA Netw. Open4, e2126337 (2021).\n7. Nadkarni, P. M., Ohno-Machado, L. & Chapman, W. W. Natural language proces-\nsing: an introduction.J. Am. Med. Inform. Assoc.18, 544–551 (2011).\n8. LeCun, Y., Bengio, Y. & Hinton, G. Deep learning.Nature 521, 436–444 (2015).\n9. Collobert, R. et al. Natural language processing (almost) from scratch.J. Mach.\nLearn Res. 12, 2493–2537 (2011).\n10. Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K. & Dyer, C. Neural\narchitectures for named entity recognition.Proceedings of the 2016 Conference of\nthe North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies. 260–270 (2016).\n11. Lee, J. et al. BioBERT: a pre-trained biomedical language representation model for\nbiomedical text mining.Bioinformatics. 36, 1234–1240 (2020).\n12. Vaswani, A. et al. Attention is All you Need.Advances in Neural Information Pro-\ncessing Systems. 30 (2017).\n13. Wang, A. et al. GLUE: A multi-task benchmark and analysis platform for natural\nlanguage understanding. Proceedings of the 2018 EMNLP Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP. 353–355 (2018).\n14. Wang, A. et al. SuperGLUE: a stickier benchmark for general-purpose language\nunderstanding systems.Advances in neural information processing systems. 32 (2019).\n15. Qiu, X. et al. Pre-trained models for natural language processing: a survey.Science\nChina Technological Sciences.63, 1872–1897 (2020).\n16. Tay, Y., Dehghani, M., Bahri, D. & Metzler, D. Efﬁcient transformers: a survey.ACM\nComputing Surveys. 55,1 –28 (2020).\n17. Yu, J., Bohnet, B. & Poesio, M. Named entity recognition as dependency parsing.\nProceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics. 6470–6476 (2020).\n18. Yamada, I., Asai, A., Shindo, H., Takeda, H. & Matsumoto, Y. LUKE: deep con-\ntextualized entity representations with entity-aware self-attention.Proceedings of\nthe 2020 Conference on Empirical Methods in Natural Language Processing\n(EMNLP). 6442–6454 (2020).\n19. Li, X. et al. Dice loss for data-imbalanced NLP tasks.Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics. 465–476 (2020).\n20. Xu, B., Wang, Q., Lyu, Y., Zhu, Y. & Mao, Z. Entity structure within and throughout:\nmodeling mention dependencies for document-level relation extraction. Pro-\nceedings of the AAAI Conference on Artiﬁcial Intelligence 35, 14149–14157 (2021).\n21. Ye, D., Lin, Y. & Sun, M. Pack together: entity and relation extraction with levitated\nmarker. Proceedings of the 60th Annual Meeting of the Association for Computa-\ntional Linguistics. 1, 4904–4917 (2021).\n22. Cohen, A. D., Rosenman, S. & Goldberg, Y. Relation classiﬁcation as two-way span-\nprediction. ArXiv arXiv:2010.04829 (2021).\n23. Lyu, S. & Chen, H. Relation classiﬁcation with entity type restriction.Findings of\nthe Association for Computational Linguistics: ACL-IJCNLP. 390–395 (2021).\n24. Wang, J. & Lu, W. Two are better than one: joint entity and relation extraction\nwith table-sequence encoders. Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP). 1706–1721 (2020).\n25. Jiang, H. et al. SMART: Robust and efﬁcient ﬁne-tuning for pre-trained natural\nlanguage models through principled regularized optimization. Proceedings of\nthe 58th Annual Meeting of the Association for Computational Linguistics.\n2177–2190 (2020).\n26. Yang, Z. et al. XLNet: Generalized Autoregressive Pretraining for Language\nUnderstanding. Proceedings of the 33rd International Conference on Neural Infor-\nmation Processing Systems. 5753\n–5763 (2019).\n27. Raffel, C. et al. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. J. Mach. Learn. Res.21,1 –67 (2019).\n28. Lan, Z.-Z. et al. ALBERT: a lite BERT for self-supervised learning of language\nrepresentations. ArXiv arXiv:1909.11942 (2019).\n29. Wang, S., Fang, H., Khabsa, M., Mao, H. & Ma, H. Entailment as Few-Shot Learner.\nArXiv arXiv:2104.14690 (2021).\n30. Zhang, Z. et al. Semantics-aware BERT for language understanding.Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence. 34, 9628-9635 (2020).\n31. Zhang, Z., Yang, J. & Zhao, H. Retrospective reader for machine reading com-\nprehension. Proceedings of the AAAI Conference on Artiﬁcial Intelligence. 35, 14506-\n14514 (2021).\n32. Garg, S., Vu, T. & Moschitti, A. TANDA: transfer and adapt pre-trained transformer\nmodels for answer sentence selection. Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence. 34, 7780-7788(2020).\n33. Bommasani, R. et al. On the opportunities and risks of foundation models.ArXiv\narXiv:2108.07258 (2021).\n34. Floridi, L. & Chiriatti, M. GPT-3: its nature, scope, limits, and consequences.Minds\nMach 30, 681–694 (2020).\n35. Gu, Y. et al. Domain-speciﬁc language model pretraining for biomedical natural\nlanguage processing. ACM Trans. Comput. Healthc.3,1 –23 (2022).\n36. Shin, H.-C. et al. BioMegatron: larger biomedical domain language model.Pro-\nceedings of the 2020 Conference on Empirical Methods in Natural Language Pro-\ncessing (EMNLP). 4700–4706 (2020).\n37. Alsentzer, E. et al. Publicly Available Clinical BERT Embeddings. in Proc. 2nd\nClinical Natural Language Processing Workshop72–78 (2019).\n38. Johnson, A. E. W. et al. MIMIC-III, a freely accessible critical care database.Sci. Data\n3, 160035 (2016).\n39. Uzuner, Ö., South, B. R., Shen, S. & DuVall, S. L. 2010 i2b2/VA challenge on\nconcepts, assertions, and relations in clinical text.J. Am. Med. Inform. Assoc.18,\n552–556 (2011).\n40. Sun, W., Rumshisky, A. & Uzuner, O. Evaluating temporal relations in clinical text:\n2012 i2b2 Challenge.J. Am. Med. Inform. Assoc.20, 806–813 (2013).\n41. Yang, X. et al. Identifying relations of medications with adverse drug events using\nrecurrent convolutional neural networks and gradient boosting. J. Am. Med.\nInform. Assoc. 27,6 5–72 (2020).\n42. Yang, X. et al. A study of deep learning methods for de-identiﬁcation of clinical\nnotes in cross-institute settings.BMC Med. Inform. Decis. Mak.19, 232 (2019).\n43. Shoeybi, M. et al. Megatron-LM: training multi-billion parameter language models\nusing model parallelism.ArXiv arXiv:1909.08053 (2020).\n44. Levine, Y., Wies, N., Sharir, O., Bata, H. & Shashua, A. Limits to depth efﬁciencies of\nself-attention. Advances in Neural Information Processing Systems 33,\n22640–22651 (2020).\n45. Sennrich, R., Haddow, B. & Birch, A. Neural Machine Translation of Rare Words\nwith Subword Units. inProc. 54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers)1715–1725 (Association for Compu-\ntational Linguistics, 2016).\n46. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: pre-training of deep\nbidirectional transformers for language understanding.Proceedings of the 2019\nConference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies. 4171–4186 (2019).\n47. Wu, Y., Xu, J., Jiang, M., Zhang, Y. & Xu, H. A study of neural word embeddings for\nnamed entity recognition in clinical text. Amia. Annu. Symp. Proc. 2015,\n1326–1333 (2015).\n48. Soysal, E. et al. CLAMP— a toolkit for efﬁciently building customized clinical natural\nlanguage processing pipelines.J. Am. Med. Inform. Assoc.25, 331–336 (2018).\n49. Wu, Y., Jiang, M., Lei, J. & Xu, H. Named entity recognition in chinese clinical text\nusing deep neural network.Stud. Health Technol. Inform.216, 624–628 (2015).\n50. Wu, Y. et al. Combine factual medical knowledge and distributed word repre-\nsentation to improve clinical named entity recognition. inAMIA Annual Sympo-\nsium Proceedingsvol. 2018, 1110 (American Medical Informatics Association, 2018).\n51. Yang, X. et al. Identifying relations of medications with adverse drug events using\nrecurrent convolutional neural networks and gradient boosting. J. Am. Med.\nInform. Assoc. 27,6 5–72 (2020).\n52. Kumar, S. A survey of deep learning methods for relation extraction. ArXiv\narXiv:1705.03645 (2017).\n53. Lv, X., Guan, Y., Yang, J. & Wu, J. Clinical relation extraction with deep learning.Int.\nJ. Hybrid. Inf. Technol.9, 237–248 (2016).\n54. Wei, Q. et al. Relation extraction from clinical narratives using pre-trained lan-\nguage models. Amia. Annu. Symp. Proc.2019, 1236–1245 (2020).\n55. Guan, H. & Devarakonda, M. Leveraging contextual information in extracting long\ndistance relations from clinical notes.Amia. Annu. Symp. Proc.2019, 1051–1060\n(2020).\n56. Alimova, I. & Tutubalina, E. Multiple features for clinical relation extraction: a\nmachine learning approach.J. Biomed. Inform.103, 103382 (2020).\nX. Yang et al.\n8\nnpj Digital Medicine (2022)   194 Published in partnership with Seoul National University Bundang Hospital\n57. Mahendran, D. & McInnes, B. T. Extracting adverse drug events from clinical\nnotes. AMIA Summits on Translational Science Proceedings. 420–429 (2021).\n58. Yang, X., Zhang, H., He, X., Bian, J. & Wu, Y. Extracting family history of patients\nfrom clinical narratives: exploring an end-to-end solution with deep learning\nmodels. JMIR Med. Inform.8, e22982 (2020).\n59. Yang, X., Yu, Z., Guo, Y., Bian, J. & Wu, Y. Clinical Relation Extraction Using\nTransformer-based Models. ArXiv. arXiv:2107.08957 (2021).\n6 0 . C e r ,D . ,D i a b ,M . ,A g i r r e ,E . ,L o p e z - G a z p i o ,I. & Specia, L. Semeval-2017 task 1: Semantic\ntextual similarity-multilingual and cross-lingual focused evaluation.Proceedings of the\n11th International Workshop on Semantic Evaluation (SemEval-2017).1 –14 (2017).\n61. Farouk, M. Measuring sentences similarity: a survey.ArXiv arXiv:1910.03940(2019).\n62. Ramaprabha, J., Das, S. & Mukerjee, P. Survey on sentence similarity evaluation\nusing deep learning.J. Phys. Conf. Ser.1000, 012070 (2018).\n63. Gomaa, W. H. & Fahmy, A. A survey of text similarity approaches.International\njournal of Computer Applications68,1 3–18 (2013).\n64. Wang, Y. et al. MedSTS: a resource for clinical semantic textual similarity.Lang.\nResour. Eval. 54,5 7–72 (2020).\n65. Rastegar-Mojarad, M. et al. BioCreative/OHNLP Challenge 2018. inProc. 2018 ACM\nInternational Conference on Bioinformatics, Computational Biology, and Health\nInformatics 575–575 (ACM, 2018).\n66. Wang, Y. et al. Overview of the 2019 n2c2/OHNLP track on clinical semantic\ntextual similarity. JMIR Med. Inform.8, e23375 (2020).\n67. Mahajan, D. et al. Identiﬁcation of semantically similar sentences in clinical notes:\niterative intermediate training using multi-task learning. JMIR Med. Inform. 8,\ne22508 (2020).\n68. Dagan, I., Glickman, O. & Magnini, B. inMachine Learning Challenges. Evaluating\nPredictive Uncertainty, Visual Object Classiﬁcation, and Recognising Tectual Entail-\nment (eds. Quiñonero-Candela, J., Dagan, I., Magnini, B. & d ’Alché-Buc, F.)\n177–190 (Springer Berlin Heidelberg, 2006).\n69. Williams, A., Nangia, N. & Bowman, S. R. A broad-coverage challenge corpus for\nsentence understanding through inference.Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies. 1, 1112–1122 (2018).\n70. Bowman, S. R., Angeli, G., Potts, C. & Manning, C. D. A large annotated corpus for\nlearning natural language inference. Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Processing. 632–642 (2015).\n71. Shivade, C. MedNLI — a natural language inference dataset for the clinical\ndomain. PhysioNet https://doi.org/10.13026/C2RS98 (2017).\n72. Conneau, A., Kiela, D., Schwenk, H., Barrault, L. & Bordes, A. Supervised learning of\nuniversal sentence representations from natural language inference data.Pro-\nceedings of the 2017 Conference on Empirical Methods in Natural Language Pro-\ncessing. 670–680 (2017).\n73. Rajpurkar, P., Zhang, J., Lopyrev, K. & Liang, P. SQuAD: 100,000+ questions for\nmachine comprehension of text.Proceedings of the 2016 Conference on Empirical\nMethods in Natural Language Processing. 2383–2392 (2016).\n74. Rajpurkar, P., Jia, R. & Liang, P. Know what you don ’t know: unanswerable\nquestions for SQuAD.Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics 2, 784–789 (2018).\n75. Zhu, M., Ahuja, A., Juan, D.-C., Wei, W. & Reddy, C. K. Question Answering with\nLong Multiple-Span Answers. in Findings of the Association for Computational\nLinguistics: EMNLP 2020 3840–3849 (Association for Computational Linguistics,\n2020).\n76. Ben Abacha, A. & Demner-Fushman, D. A question-entailment approach to\nquestion answering. BMC Bioinforma 20, 511 (2019).\n77. Pampari, A., Raghavan, P., Liang, J. & Peng, J. emrQA: a large corpus for question\nanswering on electronic medical records.Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing. 2357–2368 (2018).\n78. Yue, X., Gutierrez, B. J. & Sun, H. Clinical reading comprehension: a thorough\nanalysis of the emrQA dataset. Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics. 4474–4486 (2020).\nACKNOWLEDGEMENTS\nThis study was partially supported by a Patient-Centered Outcomes Research\nInstitute® (PCORI®) Award (ME-2018C3-14754), a grant from the National Cancer\nInstitute, 1R01CA246418 R01, grants from the National Institute on Aging, NIA\nR56AG069880 and R21AG062884, and the Cancer Informatics and eHealth core\njointly supported by the UF Health Cancer Center and the UF Clinical and\nTranslational Science Institute. The content is solely the responsibility of the authors\nand does not necessarily represent the ofﬁcial views of the funding institutions. We\nwould like to thank the UF Research Computing team, led by Dr. Erik Deumens, for\nproviding computing power through UF HiPerGator-AI cluster.\nAUTHOR CONTRIBUTIONS\nY.W., J.B., M.G.F., N.P., and X.Y. were responsible for the overall design, development,\nand evaluation of this study. X.Y. and A.C. had full access to all the data in the study\nand takes responsibility for the integrity of the data and the accuracy of the data\nanalysis. Y.W., X.Y., J.B., and W.H. did the bulk of the writing, E.A.S., D.A.M., T.M., C.A.H.,\nA.B.C., and G.L. also contributed to writing and editing of this manuscript. All authors\nreviewed the manuscript critically for scientiﬁc content, and all authors gaveﬁnal\napproval of the manuscript for publication.\nCOMPETING INTERESTS\nThe authors declare no competing interests.\nADDITIONAL INFORMATION\nSupplementary information The online version contains supplementary material\navailable at https://doi.org/10.1038/s41746-022-00742-2.\nCorrespondence and requests for materials should be addressed to Yonghui Wu.\nReprints and permission information is available at http://www.nature.com/\nreprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims\nin published maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly\nfrom the copyright holder. To view a copy of this license, visit http://\ncreativecommons.org/licenses/by/4.0/.\n© The Author(s) 2022\nX. Yang et al.\n9\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2022)   194 ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.787964940071106
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7205971479415894
    },
    {
      "name": "Natural language processing",
      "score": 0.7125694155693054
    },
    {
      "name": "Relationship extraction",
      "score": 0.5655110478401184
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4924086332321167
    },
    {
      "name": "Language model",
      "score": 0.47684040665626526
    },
    {
      "name": "Inference",
      "score": 0.46271249651908875
    },
    {
      "name": "F1 score",
      "score": 0.43201565742492676
    },
    {
      "name": "Machine learning",
      "score": 0.4090603291988373
    },
    {
      "name": "Information extraction",
      "score": 0.3282942771911621
    },
    {
      "name": "Information retrieval",
      "score": 0.32324886322021484
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ]
}