{
    "title": "EMPT: a sparsity Transformer for EEG-based motor imagery recognition",
    "url": "https://openalex.org/W4394928596",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5100347836",
            "name": "Ming Liu",
            "affiliations": [
                "Qilu University of Technology",
                "Shandong Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A5101651403",
            "name": "Yanbing Liu",
            "affiliations": [
                "Qilu University of Technology",
                "Shandong Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A5039984147",
            "name": "Weiyou Shi",
            "affiliations": [
                "Qilu University of Technology",
                "Shandong Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A5006592210",
            "name": "Yitai Lou",
            "affiliations": [
                "Qilu University of Technology",
                "Shandong Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A5102175772",
            "name": "Yuan Sun",
            "affiliations": [
                "Qilu University of Technology",
                "Shandong Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A5101903282",
            "name": "Meng Qi",
            "affiliations": [
                "Qilu University of Technology",
                "Shandong Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A5042958093",
            "name": "Dezheng Wang",
            "affiliations": [
                "Qilu Hospital of Shandong University"
            ]
        },
        {
            "id": "https://openalex.org/A5004432231",
            "name": "Fangzhou Xu",
            "affiliations": [
                "Qilu University of Technology",
                "Shandong Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A5117634646",
            "name": "Yang Zhang",
            "affiliations": [
                "Xian Yang Central Hospital",
                "Xintai People's Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A5100433899",
            "name": "Lei Zhang",
            "affiliations": [
                "Xian Yang Central Hospital",
                "Xintai People's Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A5079311625",
            "name": "Jiancai Leng",
            "affiliations": [
                "Qilu University of Technology",
                "Shandong Academy of Sciences"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3105238007",
        "https://openalex.org/W2899601737",
        "https://openalex.org/W1953143846",
        "https://openalex.org/W4300942166",
        "https://openalex.org/W4205512712",
        "https://openalex.org/W1969878365",
        "https://openalex.org/W3104608945",
        "https://openalex.org/W2563825541",
        "https://openalex.org/W2548472041",
        "https://openalex.org/W2971518519",
        "https://openalex.org/W2964684654",
        "https://openalex.org/W2944066407",
        "https://openalex.org/W3157910026",
        "https://openalex.org/W4380683769",
        "https://openalex.org/W4323022365",
        "https://openalex.org/W2375442297",
        "https://openalex.org/W2164423683",
        "https://openalex.org/W2041511988",
        "https://openalex.org/W2150884987",
        "https://openalex.org/W4293004052",
        "https://openalex.org/W3160634031",
        "https://openalex.org/W3153926117",
        "https://openalex.org/W2165380254",
        "https://openalex.org/W4283026746",
        "https://openalex.org/W2778861494",
        "https://openalex.org/W2559463885",
        "https://openalex.org/W4200141711",
        "https://openalex.org/W6780805062",
        "https://openalex.org/W4312095769",
        "https://openalex.org/W2944813347",
        "https://openalex.org/W2898664946",
        "https://openalex.org/W3175061495",
        "https://openalex.org/W6838461096",
        "https://openalex.org/W3165206559",
        "https://openalex.org/W1871050032",
        "https://openalex.org/W6732520560",
        "https://openalex.org/W2769705703",
        "https://openalex.org/W2945616044",
        "https://openalex.org/W3163832451",
        "https://openalex.org/W4229379116",
        "https://openalex.org/W4294308527",
        "https://openalex.org/W2986315612",
        "https://openalex.org/W2784398983",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W4205558134",
        "https://openalex.org/W4289538860",
        "https://openalex.org/W2037317819",
        "https://openalex.org/W3135511716",
        "https://openalex.org/W3177318507",
        "https://openalex.org/W3194820837",
        "https://openalex.org/W3040573126",
        "https://openalex.org/W4294811288",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4293718192",
        "https://openalex.org/W4282028729",
        "https://openalex.org/W3102455230"
    ],
    "abstract": "Introduction Transformer network is widely emphasized and studied relying on its excellent performance. The self-attention mechanism finds a good solution for feature coding among multiple channels of electroencephalography (EEG) signals. However, using the self-attention mechanism to construct models on EEG data suffers from the problem of the large amount of data required and the complexity of the algorithm. Methods We propose a Transformer neural network combined with the addition of Mixture of Experts (MoE) layer and ProbSparse Self-attention mechanism for decoding the time-frequency-spatial domain features from motor imagery (MI) EEG of spinal cord injury patients. The model is named as EEG MoE-Prob-Transformer (EMPT). The common spatial pattern and the modified s-transform method are employed for achieving the time-frequency-spatial features, which are used as feature embeddings to input the improved transformer neural network for feature reconstruction, and then rely on the expert model in the MoE layer for sparsity mapping, and finally output the results through the fully connected layer. Results EMPT achieves an accuracy of 95.24% on the MI EEG dataset for patients with spinal cord injury. EMPT has also achieved excellent results in comparative experiments with other state-of-the-art methods. Discussion The MoE layer and ProbSparse Self-attention inside the EMPT are subjected to visualisation experiments. The experiments prove that sparsity can be introduced to the Transformer neural network by introducing MoE and kullback-leibler divergence attention pooling mechanism, thereby enhancing its applicability on EEG datasets. A novel deep learning approach is presented for decoding EEG data based on MI.",
    "full_text": "fnins-18-1366294 April 23, 2024 Time: 12:33 # 1\nTYPE Original Research\nPUBLISHED 18 April 2024\nDOI 10.3389/fnins.2024.1366294\nOPEN ACCESS\nEDITED BY\nYang Zhan,\nChinese Academy of Sciences (CAS), China\nREVIEWED BY\nPeng Xu,\nUniversity of Electronic Science\nand Technology of China, China\nMinpeng Xu,\nTianjin University, China\nDong Wen,\nUniversity of Science and Technology Beijing,\nChina\n*CORRESPONDENCE\nYang Zhang\nzhangyang982003@163.com\nLei Zhang\nxtseytw@163.com\nJiancai Leng\njiancaileng@qlu.edu.cn\nRECEIVED 06 January 2024\nACCEPTED 25 March 2024\nPUBLISHED 18 April 2024\nCITATION\nLiu M, Liu Y, Shi W, Lou Y, Sun Y, Meng Q,\nWang D, Xu F, Zhang Y, Zhang L and Leng J\n(2024) EMPT: a sparsity Transformer\nfor EEG-based motor imagery recognition.\nFront. Neurosci.18:1366294.\ndoi: 10.3389/fnins.2024.1366294\nCOPYRIGHT\n© 2024 Liu, Liu, Shi, Lou, Sun, Meng, Wang,\nXu, Zhang, Zhang and Leng. This is an\nopen-access article distributed under the\nterms of the Creative Commons Attribution\nLicense (CC BY). The use, distribution or\nreproduction in other forums is permitted,\nprovided the original author(s) and the\ncopyright owner(s) are credited and that the\noriginal publication in this journal is cited, in\naccordance with accepted academic\npractice. No use, distribution or reproduction\nis permitted which does not comply with\nthese terms.\nEMPT: a sparsity Transformer for\nEEG-based motor imagery\nrecognition\nMing Liu1, Yanbing Liu1, Weiyou Shi1, Yitai Lou1, Yuan Sun1,\nQi Meng1, Dezheng Wang2, Fangzhou Xu1, Yang Zhang3*,\nLei Zhang4* and Jiancai Leng1*\n1International School for Optoelectronic Engineering, Qilu University of Technology (Shandong\nAcademy of Sciences), Jinan, Shandong, China, 2Rehabilitation Center, Qilu Hospital of Shandong\nUniversity, Jinan, Shandong, China, 3Rehabilitation and Physical Therapy Department, Shandong\nUniversity of Traditional Chinese Medicine Afﬁliated Hospital, Jinan, Shandong, China, 4The Second\nPeople’s Hospital of Xintai, Xintai, China\nIntroduction: Transformer network is widely emphasized and studied relying on\nits excellent performance. The self-attention mechanism ﬁnds a good solution\nfor feature coding among multiple channels of electroencephalography (EEG)\nsignals. However, using the self-attention mechanism to construct models on\nEEG data suffers from the problem of the large amount of data required and the\ncomplexity of the algorithm.\nMethods: We propose a Transformer neural network combined with the addition\nof Mixture of Experts (MoE) layer and ProbSparse Self-attention mechanism for\ndecoding the time-frequency-spatial domain features from motor imagery (MI)\nEEG of spinal cord injury patients. The model is named as EEG MoE-Prob-\nTransformer (EMPT). The common spatial pattern and the modiﬁed s-transform\nmethod are employed for achieving the time-frequency-spatial features, which\nare used as feature embeddings to input the improved transformer neural\nnetwork for feature reconstruction, and then rely on the expert model in the\nMoE layer for sparsity mapping, and ﬁnally output the results through the fully\nconnected layer.\nResults: EMPT achieves an accuracy of 95.24% on the MI EEG dataset for\npatients with spinal cord injury. EMPT has also achieved excellent results in\ncomparative experiments with other state-of-the-art methods.\nDiscussion: The MoE layer and ProbSparse Self-attention inside the EMPT are\nsubjected to visualisation experiments. The experiments prove that sparsity\ncan be introduced to the Transformer neural network by introducing MoE and\nkullback-leibler divergence attention pooling mechanism, thereby enhancing its\napplicability on EEG datasets. A novel deep learning approach is presented for\ndecoding EEG data based on MI.\nKEYWORDS\nmotor imagery, Transformer, deep learning, self-attention, Mixture of Experts\n1 Introduction\nMotor imagery (MI) brain-computer interface (BCI) systems (MI-BCIs) are designed\nto help patients with neurological disorders and physical movement disorders to\nachieve human-computer interaction by transferring the subject’s MI information to the\noutside world through the communication medium of electroencephalography (EEG)\nFrontiers in Neuroscience 01 frontiersin.org\nfnins-18-1366294 April 23, 2024 Time: 12:33 # 2\nLiu et al. 10.3389/fnins.2024.1366294\n(Hwang et al., 2009; Y ao et al., 2014; Shu et al., 2017; Attallah et al.,\n2020). Changes in subjects’ physical condition and brain activity\noccur rapidly and can be detected from EEG (Al-Qazzaz et al.,\n2018). EEG is a non-invasive, safe neurophysiological tool that\nallows recording brain activities at low cost (Al-Qazzaz et al., 2015).\nWhile MI activities are being performed, the subjects are asked\nto visualize their limb or muscle movements in their brain but\nnot perform actual movements. These cognitive processes cause\nthe relevant brain regions of the brain to be activated thereby\ngenerating EEG signals that can be decoded (King et al., 2013).\nThe study of classiﬁcation algorithms for MI-EEG signals is an\nimportant part of MI-BCIs, and obtaining the subject’s true motor\nintention through the recognition algorithms is very important for\nthe realization of human-computer interaction or rehabilitation\nwork (Úbeda et al., 2018; Talukdar et al., 2020). Kumar et al. (2017).\nused a mutual information-based band selection method to utilize\nall the information obtained from diﬀerent channels, the features\nof each frequency band were analyzed using linear discriminant\nanalysis (Kumar et al., 2017). Imran et al. (2014) proposed a discrete\nwavelet transform method by using time windows to capture the\ntemporal information from EEG, discrete wavelet transform is\napplied to the data within each window and features are extracted\n(Imran et al., 2014). The common spatial pattern (CSP) algorithm\nextracts the temporal features of EEG signals in space for MI tasks\nby constructing an optimized spatial ﬁlter to maximize the variance\nbetween the two types of data. Ang et al. (2012) used the ﬁlter bank\ncommon spatial pattern (FBCSP) algorithm for air domain feature\nextraction of motion imagery data in frequency bands with good\nresults (Ang et al., 2012).\nIn recent years, deep neural networks have largely been applied\nto achieve state-of-the-art performance. Various deep learning\nmodels have been successfully employed to decode EEG signals\nfor good performance (Roth et al., 2016; Dutta, 2019; Jiang et al.,\n2021; Klepl et al., 2022). EEGNet is a compact convolutional\nneural network consisting of deep and spatio-temporally separated\nconvolutions. It has been used for MI-EEG recognition, showing\nexcellent performance on the BCI competition dataset (Lawhern\net al., 2018). Li et al. (2023) proposed a new dual-attention-based\nMI classiﬁcation adversarial network MI-DABAN. This network\ncan reduce the distributional diﬀerences between domains by\nanalyzing the output diﬀerences between two classiﬁers and can\nincrease the distance between the samples of confusing target\ndomains and the decision boundary to improve the classiﬁcation\nperformance (Li et al., 2023). Milanés Hermosilla et al. (2021)\nused the Shallow Convolutional Network to classify and recognize\nMI-EEG signals with excellent results (Milanés Hermosilla et al.,\n2021). Kim et al. (2021) investigated diﬀerent transfer learning\nstrategies and proposed a sequential transfer learning method\nbased on classiﬁer migration, which utilizes the classiﬁer migration\ntechnique to sequentially learn the task to improve the execution of\nMI task eﬃciency. Due to the diﬃculty and high cost of acquiring\nMI-EEG data from patients with central nervous disorders, there\nhave also been studies related to data enhancement and generation\nof MI-EEG data (Luo and Lu, 2018).\nAfter being proposed by Google in 2017 and achieving\nsuperior results in the ﬁeld of natural language processing (NLP),\nTransformer neural networks have been migrated to various\npopular ﬁelds and a large number of variants have emerged\n(Vaswani et al., 2017). All these studies have proved the reliable\nperformance of self-attention mechanism and Transformer neural\nnetwork. Sun et al. (2022) proposed a parallel Transformer-\nbased and three-dimensional convolutional neural network (3D-\nCNN) based multi-channel EEG emotion recognition model. The\ntemporal and spatial features of EEG were retrieved by creating\nparallel channel EEG data and positional reconstruction of EEG\nsequence data, then using the Transformer and 3D-CNN models\n(Sun et al., 2022). Wang et al. (2022) proposed variable Transformer\nto perform hierarchical feature learning of spatial information\nfrom electrodes to brain regions to capture spatial information of\nEEG signals and improve the accuracy of emotion classiﬁcation\ntasks (Wang et al., 2022). However, the research on MI-EEG\nsignal recognition is still insuﬃcient (Lee et al., 2021; Ormerod\net al., 2021; Singh and Mahmood, 2021; Zhu et al., 2021). The\nself-attention mechanism for global feature interactions between\nfeature channels is a very eﬀective method for feature extraction,\nand it has great potential for processing EEG signals because it can\ncapture the global information of the input data very eﬀectively\n(Xie et al., 2022). However, none of the above work on EEG signal\nrecognition using the Transformer network has been improved for\nindividual diﬀerences in samples. The large individual diﬀerences\nin subjects lead to the diﬃculty of constructing recognition models\nwith generalization to multi-subject MI-EEG data. Transformer\nnetworks have the problem of being easily disturbed and diﬃcult to\ntrain, which is exacerbated by large individual diﬀerences. Adding\nsparsity structure to the model has become a reliable method to\nsolve this problem. Sparse neural network models can dynamically\nallocate diﬀerent depth parameters and structures for diﬀerent\nsamples or tasks to perform computations. This design allows for\nthe expansion of model width without increasing computational\ncomplexity, leveraging the advantages of model scale to avoid a\ndecrease in accuracy caused by individual diﬀerences in samples.\nThe eﬀectiveness of sparse models has been validated in various\nﬁelds. The Extended Transformer Construction introduces strong\nsparsity to self-attention through the incorporation of Global-\nlocal attention, achieving good results in tasks involving long texts\nand structured inputs (Ainslie et al., 2020). Mustafa et al. (2022)\nproposed a sparse expert mixture model for multimodal learning,\ncalled Language-Image MoE (LIMoE). LIMoE can simultaneously\nprocess images and text, and it is trained using contrastive loss.\nLIMoE has shown performance improvements compared to other\nmodels with similar computational complexity across multiple\nscales (Mustafa et al., 2022). In this study, we add the Mixture of\nExperts (MoE) and ProbSparse Self-attention mechanism to the\nTransformer network to increase the sparsity of the model and thus\nenhance the model’s classiﬁcation performance on multi-subject\ndata. The concept of MoE was ﬁrst introduced by Jacobs et al.\n(1991) to modularize the transformation of multilayer networks.\nTo achieve the goal of expanding the capacity of the model within a\nlimited computational cost, Shazeer et al. (2017) introduced sparse\ngating networks to MoE, added strong sparsity to the structure\nof the model and increased the model size by more than 1,000\ntimes at the expense of a very small amount of computational\neﬃciency (Shazeer et al., 2017). Lepikhin et al. (2020) introduced\nMoE for the ﬁrst time into the Transformer neural network\nmodel, and achieved very good results on the machine translation\ntask with very good results (Lepikhin et al., 2020). To solve the\nproblem of secondary computational complexity of self-attention\nmechanism, Zhou et al. (2021) proposed the ProbSparse self-\nattention mechanism, which reduces the memory usage and time\nFrontiers in Neuroscience 02 frontiersin.org\nfnins-18-1366294 April 23, 2024 Time: 12:33 # 3\nLiu et al. 10.3389/fnins.2024.1366294\ncomplexity for the Transformer model by introducing sparsity\n(Zhou et al., 2021).\nThis study introduces a Transformer neural network model\nwith the addition of MoE layer and ProbSparse self-attention\nmechanism for classifying the time-frequency spatial domain\nfeatures of MI-EEG data of spinal cord injury (SCI) patients, which\nis named as EEG MoE-Prob-Transformer (EMPT). The model\narchitecture is shown in Figure 1.\nThe main work of this paper is as follows:\n1. The eﬀect of the increase of the MoE layer and ProbSparse\nself-attention mechanism on the performance of the\nTransformer structure on EEG data is explored through\nablation experiments.\n2. The optimal network structure of the EMPT is explored and\nveriﬁed to be eﬀective.\n3. The eﬀect of the MoE layer and ProbSparse self-attention\nmechanism in response to individual diﬀerences in subjects\nare visualized and analyzed to enhance the interpretability of\nthe model structure.\nChapter 2 focuses on the experimental dataset and the main\nalgorithm used in this study. Chapter 3 presents the performance\nof the Transformer structure on EEG data and the optimal\nstructure of the model with the addition of the MoE layer and\nthe ProbSparse self-attention mechanism. Chapter 4 introduces\nthe visual analysis of the improved parts of the model. Chapter 5\nsummarizes this study.\n2 Materials and methods\n2.1 Dataset\nThe dataset was collected from the Department of Physical\nMedicine and Rehabilitation, Qilu Hospital, Qilu Medical College,\nShandong University. All participants provided written informed\nFIGURE 1\nEMPT’s network structure.\nFrontiers in Neuroscience 03 frontiersin.org\nfnins-18-1366294 April 23, 2024 Time: 12:33 # 4\nLiu et al. 10.3389/fnins.2024.1366294\nconsent after receiving a detailed description of the purpose\nand potential risks of the experiment. The study protocol was\napproved by the Medical Ethics Committee of Qilu Hospital,\nQilu Medical College, Shandong University. The experiment was\nconducted in accordance with relevant guidelines and regulations.\nThe EEG signals were acquired using a 64-electrode acquisition\ndevice shown in Figure 2. This dataset was composed of MI-\nEEG data from 10 subjects (10 SCI patients). During the EEG\nsignal acquisition experiments, the subjects had a complete MI\ntask of 7 s in duration, an imagined movement time of 4 s, and\nan interval of 3 s between every two imagined movements, and\nthe experimental paradigm is shown in Figure 3. MI tasks are\ndivided into left-handed MI tasks and right-handed MI tasks. The\ntwo MI tasks were imagining a left-handed ﬁst clench and a right-\nhanded ﬁst clench. When the MI action cue was over, the subjects\nstarted to perform the corresponding MI task. Each experimental\ngroup comprised 30 randomly presented MI tasks. Each subject\nperformed 4 groups of experiments with a 90 s rest period between\neach group of experiments, i.e., each subject performed 4 groups\nof 120 trials, 60 left-handed MI tasks, and 60 right-handed MI\ntasks.\n2.2 Modiﬁed S-transform (MST)\nThe MI-related activity information in EEG signals is mainly\nconcentrated in the alpha band (8–13 Hz) and beta band (13–\n30 Hz) (Al-Qazzaz et al., 2015; Siddharth et al., 2022). Time-\nfrequency domain analysis of EEG signals has been validated as a\nvery eﬀective method.\nModiﬁed S-transform (MST) is a time-frequency domain\nfeature extraction method with independent frequency resolution.\nMST performs multi-resolution time-frequency analysis of the\ninput EEG data by means of a window function with an adjustable\nwidth, which better extracts the phase at diﬀerent frequencies\nand clearly locates the frequency proﬁle of the noise. The MST\nalgorithm can optimize the window size and better focus the energy\nin the time-frequency domain by introducing adaptive parameters\n(Siddharth et al., 2022).\nModiﬁed S-transform (MST) can be expressed as follows,\nMST(ξ,f )=\n∫ ∞\n−∞\nt(s)g(ξ−s,f )e(−j2πfs)dt (1)\nwhere g(ξ−s,f )is the Gaussian function of the MST. It is deﬁned\nas follows,\ng(ξ−s,f )= 1√\n2πσ2(f )\ne\n−(ξ−t)2\n2σ2\n2(f ) (2)\nwhere the standard deviation σ2(f )is as follows,\nσ2(f )= p⏐⏐f\n⏐⏐q (3)\nThe width of the Gaussian window can be optimized by adjusting\nthese two parameters, P and Q.\nThe PSD of the MST is calculated as follows,\nPSD =E[MST ∗MST] (4)\n2.3 Common spatial pattern (CSP)\nThe CSP is employed to ﬁnd an optimal common spatial ﬁlter.\nAfter the EEG signals are processed by the optimal spatial ﬁlter,\nthe variance of one class of MI-EEG signals is maximized while\nthe variance of the other class of MI-EEG signals is minimized.\nTo obtain the feature vectors with the highest discrimination, the\ncovariance matrices of the CSP for the two classes of MI-EEG\nsignals are diagonalized.\nCommon spatial pattern (CSP) is able to rely on spatial ﬁlters\nto aggregate the spatial distribution characteristics within the EEG\ndata well and extract the relative spatial information in the signals.\nDue to its reliability and high computational eﬃciency, CSP has\nbeen widely used for the analysis of EEG signals (Cheng et al., 2016;\nFu et al., 2019; Li et al., 2019).\nXi is the spatio-temporal EEG signaling matrix for the two types\nof motion imagery The size of Xi is C ×Tc, where C is the number\nof EEG channels and Tc is the number of time sampling points\nfor each channel.\nAfter normalizing the time-space matrix Xi, the covariance\nmatrix Ri can be obtained as follows,\nRi = XiXT\ni\ntrace(XiXT\ni )(i =1,2) (5)\nwhere XT\ni denotes the transpose of the matrix Xi, trace(X)trace(X)\ndenotes the sum of the elements on the diagonal of the matrix.\nThe two-class covariance matrix R of the mixed space can be\ndeﬁned as follows,\nR =¯R1 +¯R2 (6)\nwhere ¯Ri(i =1,2)are the average covariance matrices for task 1\nand task 2, respectively.\nSince the mixed space covariance matrix R is a positive deﬁnite\nmatrix, the eigen decomposition is deﬁned as follows,\nR =UλUT (7)\nwhere U is the eigenvector matrix and λ is the diagonal matrix of\nthe corresponding eigenvalues.\nThese eigenvalues are be arranged in descending order, the\ntransformation U can be whitened as follows,\nP = 1√\nλ\nUT (8)\nThen S1 and S2 can be obtained by the following transformations.\nS1 and S2 have the same eigenvectors.\nS1 =PR1PT ,S2 =PR2PT (9)\nDecompose the principal components of S1 and S2 .\nS1 =Bλ1BT ,S2 =Bλ2BT (10)\nwhere λ1, λ2 are diagonal matrices and the same eigenvector\nmoment B.\nThe sum of the diagonal matrices λ1 and λ2 of the two\neigenvalues is the unit matrix.\nλ1 +λ2 =I (11)\nThe eigenvalues of λ1 and λ2 are ordered in descending and\nascending order, respectively. Since λ1 and λ2 are the diagonal\nFrontiers in Neuroscience 04 frontiersin.org\nfnins-18-1366294 April 23, 2024 Time: 12:33 # 5\nLiu et al. 10.3389/fnins.2024.1366294\nFIGURE 2\n64 electrodes distribution.\nFIGURE 3\nExperimental paradigm.\nmatrices of S1 and S2, for the eigenvector matrix B,when S1 has the\nlargest eigenvalue, S2 has the smallest eigenvalue. The classiﬁcation\nof the two types of MI signals can be achieved by means of the\nmatrix B. The projection matrix W is calculated as follows.\nW =BT P (12)\nThe projection matrix W is the corresponding spatial ﬁlter.\n2.4 Transformer neural network\nIn this study, only the encoder structure of the base\nTransformer network is used. The structure of the baseline\nTransformer network is shown in Figure 4A. The feature vectors\nare sequentially entered into several TransformerBlocks thereby\nbeing mapped into deep feature vectors containing information\nabout whole brain activity (Han and Wang, 2021).\n2.4.1 TransformerBlock structure\nA complete TransformerBlock consists of a multi-head\nattention module, a feed-forward neural network, and an\nAdd&Norm module with corresponding residual connections. The\nstructure of the TransformerBlock is shown in Figure 4B.\nThe feed-forward neural network in the base Transformer\nnetwork consists of fully connected layers that rely on a high-\ndimensional hidden layer transform to map the input vectors and\nthen map the high-dimensional vectors to ﬁxed low-dimensional\nvectors. This transformation accomplishes deep feature extraction\nand relies on activation functions to add more nonlinear\ncomputation to the network.\nFrontiers in Neuroscience 05 frontiersin.org\nfnins-18-1366294 April 23, 2024 Time: 12:33 # 6\nLiu et al. 10.3389/fnins.2024.1366294\nFIGURE 4\n(A,B) Transformer and TransformerBlock structure.\nThe Add&Norm module consists of residual links and layer\nnormalization modules. Its main purpose is to ensure the stability\nof network training and reduce the occurrence of overﬁtting\nphenomenon and network degradation.\n2.4.2 Multi-head self-attention\nThe multi-head attention mechanism consists of multiple\nmutually independent self-attention heads, each of which\ncan capture diﬀerent whole-brain activities for reconstructing\ndepth feature vectors. The multi-head attention mechanism\nexpands the sensory ﬁeld of the attention mechanism for\nbrain activities capture and improves the performance of the\nattention mechanism.\nOn the input feature vector F, the self-attention module can\nmap three vectors Q, K and V of dimension L for computing the\nattention coeﬃcients of self-attention through the three trainable\nFrontiers in Neuroscience 06 frontiersin.org\nfnins-18-1366294 April 23, 2024 Time: 12:33 # 7\nLiu et al. 10.3389/fnins.2024.1366294\nFIGURE 5\nStructure of MoE layer.\nweight matrices WQ, WV and WK . Where Q and K are the query\nvector and key vector, respectively, in the attention mechanism, Q\nand K are used to compute the attention dot product, whileV yields\nthe output vector by weighting with the attention dot product. The\nformula for Q, V and K calculation is as follows,\nQ =WQF,K =WK F,V =WV F (13)\nThe attention factor for Xi pointing to Xj is calculated as follows,\nAi,j =QiKj√\ndk\n(14)\n√\ndk =\n√\nL (15)\nAfter obtaining the attention factor matrix Ai,j for the eigenvector\nFi, Vi is weighted according to the attention coeﬃcient Ai,j. The\nweighted vector Zi is obtained by the following equation.\nZi =\nN∑\nj=1\nSoftmax(Ai,j)VH=j (16)\nThe self-attention mechanism is used to map the feature vectors\nof all channels, the original feature vector F becomes a new vector\ncontaining the attention relations of all feature vectors Z. The\noutput vectors of multiple attention heads are spliced together and\nprocessed by the feed-forward neural network to be provided to the\ndownstream task.\n2.5 Sparsity improvement in Transformer\nnetworks\nBecause of the large individual variability of subjects’ EEG\nsignals, when a dataset containing multi-subject data is used to\nconstruct a model, a large model width is required to ensure\nthe performance and stability of the model (Suhail et al., 2022).\nThe training samples become larger and each sample needs to\ngo through all the computations of the model, which leads to a\nlarge increase in the training cost. In this paper, the MoE layer\nis introduced to increase the sparsity of the model to save the\ncomputational cost. Sparsity means that the model has a large\ncapacity, but only some parts of the model are activated for a single\nsample. An increase in model sparsity can signiﬁcantly improve the\ncapacity and performance of a model, but does not proportionately\nincrease the computational eﬀort.\n2.5.1 MoE\nThe MoE layer has diﬀerent expert submodels, each specialized\nfor a diﬀerent input. The experts in each layer are controlled by\na gating network that activates certain expert submodels based on\nthe input data. For each input, the gating network selects the most\nappropriate expert submodel to process the data. The structure of\nthe MoE layer is shown in Figure 5:\nThe formula for the MoE layer is shown as follows,\ny =\nn∑\ni−1\nGi(x)Ei(x) (17)\nFrontiers in Neuroscience 07 frontiersin.org\nfnins-18-1366294 April 23, 2024 Time: 12:33 # 8\nLiu et al. 10.3389/fnins.2024.1366294\nFIGURE 6\nMoE-TransformerBlock structure.\nwhere n is the attribute of the expert sub-model, G(x)is the output\nvalue of the gating network, andE(x)is the output of the expert sub-\nmodel. The composition of the gating network is relatively simple\nand consists of a linear layer and a softmax activation function,\nwhose formula is shown as follows,\nG(x)=Softmax(KeepTopK(x ·W),k) (18)\nKeepTopK(·)is a discrete function that forces values outside of top-\nk to negative inﬁnity, resulting in an output value of 0 for softmax.\nFor the MoE layer, the expert sub-model is a fully connected layer.\n2.5.2 MoE-TransformerBlock\nIn this study, the feedforward neural network in\nTransformerBlock is replaced with a MoE layer, which adds\nsparsity and network width to the model without increasing the\ncomputational eﬀort. The MoE-TransformerBlock is shown in\nFigure 6.\n2.6 Attention pooling improvements for\nTransformer networks\nFor the traditional attention mechanism, the dot product of Q\nand K is sparse, and the feature map of the self-attention coeﬃcients\nshows a long-tailed distribution. Long-tailed distribution is a type\nof uneven data distribution. In a long-tailed distribution, the\ncategories of samples are divided into head and tail categories.\nThe head category means that a few categories contain a large\nnumber of samples, and the tail category includes most of all\nthe categories but has only a small number of samples. For a\nsingle attention head, fewer dot products contribute the majority\nof the attention score, and the rest of the paired dot products can\nbe ignored. This sparsity distribution has a practical implication:\nan element in a sequence will generally only have a high degree\nof similarity and correlation with a few elements (Zhou et al.,\n2021). On the EEG dataset, our team similarly conﬁrmed this\nphenomenon when training the Transformer model, as shown in\nFigure 7. The head class representation in Figure 7 is boxed in\nred for easier viewing. For the deeper multi-head self-attention\nmodule, the individual attention heads tend to focus more on some\nspeciﬁc channels thus showing a long-tailed distribution. This may\nbe because these selected channels already contain the activity of\na certain brain region, and the deeper multi-head self-attention\nmodule reconstructs the high-level feature vectors that contain\nthe activity of the whole brain by focusing more on these speciﬁc\nchannels to capture the global brain activities.\nThe long-tailed distribution of each set of self-attention\ncoeﬃcients in the traditional self-attention head is similar, and\nweighting using similar attention coeﬃcients is very wasteful of\ncomputational cost. To deepen the stability of the computation\nand reduce the computational cost, we should ﬁnd the Q that can\ndominate the distribution of attention coeﬃcients for self-attention\ncomputation. To accomplish this, we introduce the ProbSparse\nself-attention mechanism.\n2.6.1 Measuring query sparsity\nThe long-tailed distribution of the coeﬃcients for traditional\nself-attention on the EEG dataset is shown in Figure 7. The\nattention factor of the ith query on all keys is deﬁned as the\nprobability P(KH ,Qi), where H is the number of channels of input\nEEG features. The probability distribution of the dominant dot\nproduct on the attention of the corresponding query is far from\nFIGURE 7\nLong-tailed distribution in the dot product of multi-attribute attention.\nFrontiers in Neuroscience 08 frontiersin.org\nfnins-18-1366294 April 23, 2024 Time: 12:33 # 9\nLiu et al. 10.3389/fnins.2024.1366294\nFIGURE 8\nMoE-Prob-TransformerBlock structure.\nthe uniform distribution. If P(KH ,Qi) is close to the uniform\ndistribution, P(KH ,Qi)=1/LK , then the query is lazy and fails\nto pick out important keys, and vice versa, the query is active.\nIf the query is completely lazy, the self-attention becomes a sum\nof values, which results in some information in the output being\nredundant.\nSince active queries contribute a lot to self-attention and lazy\nqueries contribute little, the active queries are selected as much\nas possible. The gap between the distribution P(KH ,Qi) and the\nuniform distribution can be used to distinguish the importance of a\nquery. ProbSparse self-attention measures similarity by Kullback–\nLeibler sparsity, the sparsity measurement of the ith query is\ndeﬁned as follows,\nM(Qi,K)=ln\nLK∑\nj=1\ne\nqikT\nj√\nd − 1\nLK\nLK∑\nj=1\nQiKT\nj\n√\nd\n(19)\nFor the sparsity measurement of the ith query, the larger the\nvalue, the larger the diﬀerence between the dot product probability\ndistribution and the uniform distribution, which means the more\nactive the query is.\n2.6.2 ProbSparse self-attention\nBased on the proposed metric, ProbSparse self-attention is\nderived by allowing each key to focus on only u main queries.\nA(Q,K,V)=Softmax(QKT\n√\nd\n)V (20)\nwhere ¯Q is a sparse matrix of the same size as Q, which contains\nonly the Top-u queries under the sparsity metric M(Q,K). For\nthose queries that are not selected, their outputs may be taken as\nthe means of Vto ensure that both the input and output sequence\nlengths are Q.\nTraversing the sparsity measurement M(Q,K) of all queries\nrequires computing each dot-product pair, increasing the quadratic\ncomputational complexity O(LQLK ), and the log-sum-exp\noperation has potential numerical stability issues. ProbSparse\nself-attention uses an empirical approximation that eﬃciently\nobtains the query sparsity metric. The improved formula is as\nfollows,\nM(Qi,K)=max\nj\n(\nQiKT\nj\n√\nd\n)− 1\nLK\nLK∑\nj=1\nQiKT\nj\n√\nd\n(21)\nProbSparse Attention randomly samples key for each query, the\nsampling result of each head is the same. However, since each layer\nFIGURE 9\nSeparation of EEG signals by frequency bands.\nFrontiers in Neuroscience 09 frontiersin.org\nfnins-18-1366294 April 23, 2024 Time: 12:33 # 10\nLiu et al. 10.3389/fnins.2024.1366294\nTABLE 1 EMPT model training parameters.\nLabel Parameter name Parameter\nvalues\n1 Dropout Rate (FC Layer) 0.5\n2 Dropout Rate\n(MoE-TransformerBlock)\n0.2\n3 Dropout Rate\n(MoE-Prob-TransformerBlock)\n0.2\n4 Learn rate 0.00005\n5 Batch size 256\n6 Epoch 300\n7 Multi-head number 8\n8 Attention head hidden layer size 128\nFIGURE 10\nEMPT training loss curves.\nof self-attention can do a linear transformation of Q, K, and V,\nwhich makes the query and key vectors corresponding to diﬀerent\nheads at the same position in the sequence diﬀerent, so the sparsity\nmeasurement of the same query of each head is diﬀerent, which\nmakes the Top-u query with the highest measurement are diﬀerent\nfor each head. This is also equivalent to the fact that each head\nadopts a diﬀerent optimization strategy.\n2.6.3 MoE-Prob-TransformerBlock\nWe replace the multi-head self-attention mechanism in MoE-\nTransformerBlock with ProbSparse self-attention, the structure of\nwhich is shown in Figure 8.\n3 Results\n3.1 Implementation details\n3.1.1 Pre-processing and feature extraction\nFor the two diﬀerent feature extraction methods, this paper uses\ndiﬀerent preprocessing schemes to MST and CSP on the data.\nTABLE 2 Performance of single-layer MoE-Transformer at different\nvalues ofK.\nK value 1 2 4 6 8\naccuracy 86.74% 88.43% 89.73% 89.75% 89.88%\nThe shape of the raw EEG data is T ×CH ×ES, where T is the\nnumber of experiments, CH is the number of channels, and ES is\nthe number of sampling points of the EEG signal.\nThe pre-processing scheme for MST involved passing the raw\nEEG signals through a Butterworth ﬁlter at 8–30 Hz, followed by\ndownsampling. This downsampling step reduced the sampling rate\nfrom 1,000 Hz to 100 Hz. After feature extraction by the MST\nmethod, the shape of the feature is T ×CH ×Fmst, where Fmst is\nthe number of MST features. The parameters and of the Gaussian\nwindow for MST are 0.98 and 0.49.\nIn the application of the CSP method, the current study\nutilizes a multi-band dataset from a single channel for CSP feature\nextraction. Speciﬁcally, the data from multiple frequency bands\nof each channel is treated as a new channel, and CSP is applied\nto extract features from these multi-band channels. The pre-\nprocessing scheme for the CSP method is as follows, the original\nEEG signal is decomposed into 55 diﬀerent frequency bands using\na Butterworth ﬁlter in windows of band widths of 2, 4, and 8 Hz,\nall with a step size of 1 Hz (Huang et al., 2009). The signal bands\nare shown in Figure 9. The shape of the EEG signal data after band\ndecomposition is T ×CH ×FN ×ES, where FN is the number of\nfrequency bands. After completing the ﬁlter decomposition and\nthen downsampling, the sampling rate is reduced from 1,000 to\n100 Hz. The EEG signals of each channel are sequentially fed\ninto the CSP method for feature extraction, and the shape of the\nCSP features is T ×CH ×Fcsp, whereFcsp is the number of CSP\nfeatures.\n3.1.2 Neural network training\nThe hyperparameters used to train the neural network are\nshown in Table 1. Where Dropout Rate (FC Layer), Dropout\nRate (MoE-TransformerBlock) and Dropout Rate (MoE-Prob-\nTransformerBlock) are the neuron inactivation probabilities of\nthe fully connected layer, MoE-TransformerBlock and MoE-Prob-\nTransformerBlock’s neuron inactivation probability. The lower loss\nrate of MoE-TransformerBlock and MoE-Prob-TransformerBlock\nis to ensure proper convergence of the model loss. In training, it\nwas found that setting a higher Dropout rate in self-attention leads\nto too slow convergence of the model loss function. The loss curve\nfor EMPT training is shown in Figure 10.\n3.2 Experimental results\nThis section conducts performance experiments and analysis\non the EMPT and related structures. Cross-individual model\ntraining was performed on the MI-EEG dataset of SCI patients\nand ten times 10-fold cross validation was performed to obtain\nexperimental results.\n3.2.1 Selection of K value for MoE layer\nIn MoE, the KeepTopK(·) operation selects the larger value\nG(x)K among the gated network outputs G(x), with K being the\nFrontiers in Neuroscience 10 frontiersin.org\nfnins-18-1366294 April 23, 2024 Time: 12:33 # 11\nLiu et al. 10.3389/fnins.2024.1366294\nTABLE 3 Ablation study results.\nModel Block number Accuracy Precision Recall\nTransformer-Base 1 88.52% 89.34% 87.68%\n2 93.56% 94.19% 92.38%\n3 90.07% 89.46% 90.67%\n4 86.67% 87.72% 85.63%\n5 85.34% 85.12% 84.88%\nMoE-Transformer 1 89.73% 90.52% 88.98%\n2 94.73% 95.68% 93.36%\n3 93.83% 94.26% 93.13%\n4 93.33% 92.52% 93.21%\n5 93.24% 92.19% 93.35%\nProb-Transformer 1 89.23% 89.11% 90.16%\n2 93.85% 92.61% 93.96%\n3 93.13% 93.36% 92.53%\n4 91.67% 92.75% 90.61%\n5 90.62% 91.02% 89.79%\nTABLE 4 Experimental results of different stacking structures of EMPT.\nStacking method Block number Accuracy Precision Recall\nM-FC 1 89.73% 90.52% 88.98%\nP-FC 1 89.23% 89.11% 90.16%\nM-P-FC 2 95.24% 96.38% 94.88%\nM-M-FC 2 94.73% 95.68% 93.36%\nP-P-FC 2 93.85% 92.61% 93.96%\nP-M-FC 2 93.66% 92.82% 94.08%\nM-P-P-FC 3 93.22% 92.81% 93.51%\nM-M-P-FC 3 94.63% 93.21% 95.43%\nM-M-M-FC 3 94.33% 93.26% 94.13%\nP-M-M-FC 3 93.27% 92.55% 93.48%\nP-P-M-FC 3 92.65% 93.23% 91.32%\nP-P-P-FC 3 93.13% 93.36% 92.53%\nnumber of larger values. expert models corresponding to G(x)K\nare retained for subsequent weighting operations. expert models\nwith smaller values of G(x)G(x)imply that they are not suﬃciently\nimportant for the current samples. the choice of the value of\nK may be of great signiﬁcance for the ﬁnal performance of the\nmodel. In order to determine the optimal K value for the dataset\nused in this study, we conducted an experiment to determine\nthe choice of K value by looking at the performance of the\nMoE-Transformer with a layer number of 1 when diﬀerent K\nvalues are chosen.\nBy observing the data within Table 2 we can\nﬁnd that there is little diﬀerence in MoE-Transformer\nperformance when K ≥4. To save unnecessary computational\nexpenses, 4 is chosen as the value of K in this study\nto enable the model to obtain good classiﬁcation\nperformance.\n3.2.2 Ablation experiment\nTo verify that the improvements of the MoE layer and\nProbSparse self-attention are eﬀective on the SCI EEG dataset,\nablation studies are conducted on them separately to explore\ntheir eﬀectiveness. The results are shown in Table 3. The\nMoE-Transformer and Prob-Transformer models are derived by\nreplacing the TransformerBlock with MoE-TransformerBlock and\nProb-TransformerBlock based on the Transformer base model.\nIt should be noted that Prob-TransformerBlock is not the\nMoE-Prob-TransformerBlock described in section “2.6.3 MoE-\nProb-TransformerBlock”. Prob-TransformerBlock is obtained by\nreplacing the self-attention in TransformerBlock with ProbSparse\nself-attention. The experimental results show that the addition\nof both MoE-TransformerBlock and Prob-TransformerBlock have\nmade improvements to the performance of the Transformer-Base\nmodel. From the Table 3, it can be observed that MoE-Transformer\nFrontiers in Neuroscience 11 frontiersin.org\nfnins-18-1366294 April 23, 2024 Time: 12:33 # 12\nLiu et al. 10.3389/fnins.2024.1366294\nTABLE 5 Comparison test results of different models.\nModel Accuracy Precision Recall\nCWT/PCA+SVM\n(Bousseta et al.,\n2016)\n86.24% 87.39% 85.22%\nEEGnet (Lawhern\net al., 2018)\n88.73% 87.91% 89.47%\nHS-CNN (Dai et al.,\n2020)\n89.36% 90.27% 89.34%\nCNN+LSTM (Amin\net al., 2022)\n90.21% 89.32% 90.45%\nATC-Net (Altaheri\net al., 2023)\n92.44% 91.62% 93.33%\nMSATNet (Hu L.\net al., 2023)\n93.59% 94.45% 93.18%\nMSFT (Jia et al.,\n2023)\n94.18% 94.74% 93.69%\nEMPT 95.24% 96.38% 94.88%\nFIGURE 11\nResults of t-SNE visualization for EMPT feature vectors.\nand Prob-Transformer still show the best performance at 2\nstacked layers for the dataset used compared to Transformer-Base.\nThis may be due to the fact that although both improvements\nattach strong sparsity to the model to improve performance, both\nstructures do not make the network deeper. The failure of the\nnetwork to perform better as it gets deeper may also be related\nto the fact that the dataset used in this paper is not large enough.\nAlthough we added sparsity improvements in this chapter to reduce\nnoise interference in the model, due to the noise-sensitive nature of\nthe attention mechanism, smaller datasets still make it diﬃcult to\ntrain the model to exclude all noise interference.\n3.2.3 MoE-Prob-Transformer performance\nexperiments\nTo conﬁrm the optimal stacking order of MoE-\nTransformerBlock and MoE-Prob-TransformerBlock, this\npaper conducts comparative experiments for diﬀerent Block\nstacking methods to determine the optimal structure of MoE-\nProb-Transformer. The results are shown in Table 4, where\nM stands for MoE-TransformerBlock, P stands for MoE-Prob-\nTransformerBlock, and FC stands for fully connected layer.Table 4\nshows that the stacking method of M-P-FC has the strongest\nperformance. From the experimental results in Table 4, it can be\nfound that when the MoE-Prob-Transformer module is placed\nmore forward, the model’s performance will be lower than the other\nmodel stacking methods with the same depth. This phenomenon\nmay be due to the fact that the attention mechanism located in\nthe shallower layer has to aggregate the brain activity information\nbetween the channels, so the attention coeﬃcients are less similar,\nand the use of ProbSparse Attention in the shallower module\nwill result in a loss of brain activity information. However, in the\ndeeper attention module, meaningful brain activities have been\naggregated in individual channel features, and a similar long-tailed\ndistribution occurs for the calculation of the attention coeﬃcients.\nBased on the above analyses, ProbSparse Attention is more suitable\nto be used at deeper network locations on the SCI MI-EEG dataset.\nThis also explains why Prob-Transformer on Table 3didn’t get a\nbig boost compared to Transformer-Base.\n3.2.4 Comparative experiments\nTo verify the performance of the proposed model, we\nconducted a comparison test with other state-of-the-art\nclassiﬁcation models on the same dataset, and the results are\nshown in Table 5. From Table 5, it can be found that EMPT not\nonly achieves superior performance in comparison with many\ncommonly used methods, but also achieves leading results in\ncomparison experiments with three attention models, attention-\nbased temporal convolutional network (ATC-Net), multi-scale\nadaptive transformer network (MSATNet), and metric-based\nspatial ﬁltering transformer (MSFT), which suggests that the\nmodel proposed in this study is very eﬀective. To validate the\neﬀectiveness of EMPT, we conducted t-distributed stochastic\nneighbor embedding (t-SNE) visualization of the vectors before\nentering the fully connected layer. The results are presented in\nFigure 11. In Figure 11, the purple dots represent EEG trials of the\nleft-handed MI task and the yellow dots represent EEG trials of the\nright-handed MI task. As depicted in Figure 11, the features after\nfeature decoding by EMPT are separable.\nTo verify the model performance of EMPT, we compared it\nwith the state-of-the-art models on the BCI competition dataset\nIV-2A, and the experimental results are shown in Table 6. The\nexperimental results prove that EMPT also performs well on the\nBCI competition dataset IV-2A.\n4 Discussion\nIn this study, MI EEG data from ten SCI subjects have been\nused to constitute a dataset to train a generalized model on the\nproposed deep learning architecture. To validate the improvement\nperformance, we have analyzed the individual diﬀerences of the\nsubjects to enhance the interpretability of the model structure.\n4.1 Selection of sub-models in the MoE\nlayer\nTo verify whether the MoE layer in Transformer can eﬀectively\nperform dynamic sub-model selection for individual subjects and\nthus achieve model sparsity, this paper visualizes and analyzes\nFrontiers in Neuroscience 12 frontiersin.org\nfnins-18-1366294 April 23, 2024 Time: 12:33 # 13\nLiu et al. 10.3389/fnins.2024.1366294\nTABLE 6 The performance on the BCI competition datasets IV-2A.\nMethod Subjects\nA01 A02 A03 A04 A05 A06 A07 A08 A09 AVG\nEEGNet (Lawhern\net al., 2018)\n83.68 63.89 90.97 64.24 59.72 52.08 87.85 82.29 86.81 74.61\nMI-DABAN (Li\net al., 2023)\n88.54 55.56 91.32 77.43 60.42 58.68 87.15 83.68 82.64 76.16\nCNN-LSTM (Amin\net al., 2022)\n89.23 72.53 97.23 76.28 82.48 69.15 94.76 86.14 86.1 82.84\nEEG-Inception\n(Zhang et al., 2021)\n89.61 80.01 96.17 81.26 83.76 81.2 94.75 98.28 90.5 88.39\nCS-CNN (Hu Y.\net al., 2023)\n91.72 88.48 91.72 88.95 88.31 89.12 89.53 91.78 93.75 90.37\nEMPT 93.72 90.03 96.72 93.54 92.61 90.84 95.51 94.11 93.42 93.39\nFIGURE 12\nGating network output values in MoE-TransformerBlock.\nFIGURE 13\nGating network output values in Prob-MoE-TransformerBlock.\nFrontiers in Neuroscience 13 frontiersin.org\nfnins-18-1366294 April 23, 2024 Time: 12:33 # 14\nLiu et al. 10.3389/fnins.2024.1366294\nFIGURE 14\nChannel selection for ProbSparse self-attention.\nthe output values of the gating network in the MoE layer. The\ngating network output values are stacked and averaged according\nto the number of experiments performed on individual subjects.\nThe results of the visualization of gating network output values\nin the MoE-TransformerBlock and MoE-Prob-TransformerBlock\nare shown in Figures 12, 13. The horizontal axis of Figures 12, 13\nshows the eight gated values output from the gated network in MoE,\nand the vertical axis shows the 64 channels of EEG data, with each\nmatrix averaged from the full MI data for a single subject.\nWe can infer that the gating network responds diﬀerently to\nvarious subjects, enabling it to assign appropriate gate values to\ndiﬀerent expert sub-models. Consequently, the MoE layer produces\nfeature vectors that are conducive to the downstream task. For\ncomparison, the MoE layer in MoE-TransformerBlock responds\nmore to the individual diﬀerences of subjects while the MoE layer\nin Prob-MoE-TransformerBlock responds less. This phenomenon\nshows the fact that the shallow neural network structure is used\nby the model to extract useful features, which needs to rely on\nthe corresponding linear mapping for diﬀerent subjects to output\nfeature vectors with low individual diﬀerences but with category\ncommonality. In contrast, the features received by the MoE layer in\nProb-MoE-TransformerBlock contain fewer individual diﬀerences,\nso the visualization of the gate values turns out to be more similar.\nIn addition, the three channels in Sub_5 behave inconsistently with\nFrontiers in Neuroscience 14 frontiersin.org\nfnins-18-1366294 April 23, 2024 Time: 12:33 # 15\nLiu et al. 10.3389/fnins.2024.1366294\nthe performance of other channel gating values, and these three\nchannels are located in similar brain regions. Given that the dataset\nutilized in this study comprised SCI patients, it is plausible to\nexpect stronger individual variations in brain activity during motor\nimagery. The phenomenon of gating values behaving diﬀerently is\ndue to the fact that MoE provides a diﬀerent mapping for Sub_5\nactivity on these three channels than on the other channels, and the\ndiﬀerences in activity on the channels may be smaller, but MoE’s\nmapping decision still made a larger change, which reﬂects the\neﬀectiveness of MoE.\n4.2 Channel selection situation for\nProbSparse self-attention\nThe channel selection situation of ProbSparse self-attention is\nvisualized to observe how the EEG channels have been selected, and\nthe visualization results are shown in Figure 14.\nThe M(Qi,K)values computed by individual attention heads\non the MI data of a subject have been normalized and are displayed\nin each subplot of Figure 14. The red color represents a larger\nM(Qi,K)value, i.e., it means that the probability that the features of\nthe channel are retained is higher, and vice versa when the location\nof the channel is in blue color, the probability that the features\nof the channel are retained is lower. By visualizing the result, we\ncan clearly observe that ProbSparse self-attention is able to select\ndiﬀerentiated channel retention schemes in each attention head to\ngenerate feature information containing diﬀerent brain activities.\nIt is important to note that the content of Figure 14does not fully\nrepresent the brain activity situation, although valid brain activity\ninformation is retained. Since channel features have already been\nweighted in the shallow layers of the attentional mechanism, in\nthe deeper layers of the model, the features of a particular channel\nactually contain a large amount of information about brain activity\nin other brain regions. The fact that ProbSparse self-attention\nconsiders a particular channel in the input features to be worthy of\nbeing retained may be an indication that a large number of features\nof brain activity associated with that channel should be retained and\nnot just that the information about this channel in the raw EEG\nsignal is absolutely important.\n5 Conclusion\nIn this study, the EMPT structure is proposed for the\nclassiﬁcation and identiﬁcation of EEG signals for MI in SCI\npatients, and better results have been achieved. This study validates\nthe usability of the MoE module and the ProbSparse self-\nattention mechanism on EEG signals. The addition of the MoE\nmodule and the ProbSparse self-attention mechanism improves\nthe performance of the baseline Transformer model for the EEG\nclassiﬁcation task and enhances the correctness of the recognition\nas well as the training stability. The above two improvements\nare also visualized and analyzed to enhance their interpretability.\nIt is demonstrated that the EMPT structure is very eﬀective in\nrecognizing EEG signals and classifying MI for SCI patients.\nData availability statement\nThe datasets presented in this article are not readily available\nbecause the article data involves ethical considerations and cannot\nbe disclosed. Requests to access the datasets should be directed to\nFX, xfz@qlu.edu.cn.\nEthics statement\nThe studies involving humans were approved by the Medical\nEthics Committee of Qilu Hospital, Cheeloo College of Medicine,\nShandong University. The studies were conducted in accordance\nwith the local legislation and institutional requirements.\nThe participants provided their written informed consent to\nparticipate in this study.\nAuthor contributions\nML: Software, Visualization, Writing – original draft. Y aL: Data\ncuration, Writing – original draft. WS: Software, Writing – original\ndraft. YiL: Software, Visualization, Writing – original draft. YS:\nSoftware, Writing – original draft. QM: Software, Writing – original\ndraft. DW: Data curation, Writing – original draft. FX: Writing –\noriginal draft, Writing – review & editing. YZ: Data curation,\nWriting – review & editing. LZ: Writing – review & editing. JL:\nWriting – review & editing.\nFunding\nThe authors declare that ﬁnancial support was received\nfor the research, authorship, and/or publication of this article.\nThis research was funded by the Fundamental Research Funds\nfor the Central Universities under Grant No. 2022JC013, the\nNational Natural Science Foundation of China under Grant\nNos. 62271293, 82330064, the Natural Science Foundation\nof Shandong Province under Grant Nos. ZR2022MF289,\nZR202102200383, ZR2019MA037, the Introduce Innovative\nTeams of 2021 “New High School 20 Items” Project under\nGrant No. 2021GXRC071, the Graduate Education and Teaching\nReform Project of Qilu University of Technology (Shandong\nAcademy of Sciences) in 2023, the Talent Training and\nTeaching Reform Project of Qilu University of Technology\nin 2022 under Grant No. P202204, and the Research Leader\nProgram of Jinan Science and Technology Bureau under Grant\nNo. 2019GXRC061.\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nFrontiers in Neuroscience 15 frontiersin.org\nfnins-18-1366294 April 23, 2024 Time: 12:33 # 16\nLiu et al. 10.3389/fnins.2024.1366294\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAinslie, J., Ontanon, S., Alberti, C., Cvicek, V., Fisher, Z., Pham, P., et al. (2020).\n“ETC: Encoding long and structured inputs in transformers, ” inProceedings of the 2020\nConference on empirical methods in natural language processing (EMNLP) , (Kerrville,\nTX), 268–284. doi: 10.18653/v1/2020.emnlp-main.19\nAl-Qazzaz, N. K., Ali, S. H., and Ahmad, S. A. (2018). “Comparison of the\neﬀectiveness of AICA-WT technique in discriminating vascular dementia EEGS, ” in\nProceedings of the 2018 2nd international conference on biosignal analysis, processing\nand systems, (Piscataway, NJ), 109–112. doi: 10.1109/icbaps.2018.8527412\nAl-Qazzaz, N. K., Ali, S. H., Islam, S., Ahmad, S. A., and Escudero, J. (2015).\n“EEG wavelet spectral analysis during a working memory tasks in stroke-related mild\ncognitive impairment patients, ” inProceedings of the 2016 international federation for\nmedical and biological engineering , (Singapore: Springer), 82–85. doi: 10.1007/978-\n981-10-0266-3_17\nAltaheri, H., Muhammad, G., and Alsulaiman, M. (2023). Physics-informed\nattention temporal convolutional network for EEG-based motor imagery\nclassiﬁcation. IEEE Trans. Ind. Inform. 19, 2249–2258. doi: 10.1109/tii.2022.3197419\nAmin, S., Altaheri, H., Muhammad, G., and Abdul, W. (2022). Attention-inception\nand long- short-term memory-based electroencephalography classiﬁcation for motor\nimagery tasks in rehabilitation. IEEE Trans. Ind. Inform. 18, 5412–5421. doi: 10.1109/\ntii.2021.3132340\nAng, K. K., Chin, Z. Y., Wang, C., Guan, C., and Zhang, H. (2012). Filter bank\ncommon spatial pattern algorithm on BCI competition IV datasets 2A and 2B. Front.\nNeurosci. 6:39. doi: 10.3389/fnins.2012.00039\nAttallah, O., Abougharbia, J., Tamazin, M., and Nasser, A. A. (2020). A BCI system\nbased on motor imagery for assisting people with motor deﬁciencies in the limbs.Brain\nSci. 10:864. doi: 10.3390/brainsci10110864\nBousseta, R., Tayeb, S., Ouakouak, I. E., Gharbi, M., Regragui, F., and Himmi, M. M.\n(2016). “EEG eﬃcient classiﬁcation of imagined hand movement using RBF Kernel\nSVM, ” inProceedings of the 2016 11th international conference on intelligent systems:\nTheories and applications (SITA), (Mohammedia), 1–6. doi: 10.1109/sita.2016.7772278\nCheng, M., Lu, Z., and Wang, H. (2016). Regularized common spatial patterns\nwith subject-to-subject transfer of EEG Signals. Cogn. Neurodyn. 11, 173–181. doi:\n10.1007/s11571-016-9417-x\nDai, G., Zhou, J., Huang, J., and Wang, N. (2020). HS-CNN: A CNN with hybrid\nconvolution scale for EEG motor imagery classiﬁcation. J. Neural Eng. 17:016025.\ndoi: 10.1088/1741-2552/ab405f\nDutta, K. K. (2019). “Multi-class time series classiﬁcation of EEG signals with\nrecurrent neural networks, ” inProceedings of the 2019 9th international conference on\ncloud computing, data science & engineering, (Piscataway, NJ), 337–341. doi: 10.1109/\nconﬂuence.2019.8776889\nFu, R., Tian, Y., Bao, T., Meng, Z., and Shi, P. (2019). Improvement motor imagery\nEEG classiﬁcation based on regularized linear discriminant analysis. J. Med. Syst.\n43:108833. doi: 10.1007/s10916-019-1270-0\nHan, J., and Wang, H. (2021). Transformer based network for open information\nextraction. Eng. Appl. Artif. Intell. 102:104262. doi: 10.1016/j.engappai.2021.104262\nHu, L., Hong, W., and Liu, L. (2023). MSATNet: Multi-scale adaptive transformer\nnetwork for motor imagery classiﬁcation. Front. Neurosci. 17:1173778. doi: 10.3389/\nfnins.2023.1173778\nHu, Y., Liu, Y., Zhang, S., Zhang, T., Dai, B., Peng, B., et al. (2023). A cross-space\nCNN with customized characteristics for motor imagery EEG classiﬁcation. IEEE\nTrans. Neural Syst. Rehabil. Eng. 31, 1554–1565. doi: 10.1109/tnsre.2023.3249831\nHuang, G., Liu, G., and Zhu, X. (2009). Common spatial patterns in classiﬁcation\nbased on less number channels of EEG. Chin. J. Biomed. Eng. 28, 840–845.\nHwang, H.-J., Kwon, K., and Im, C.-H. (2009). Neurofeedback-based motor imagery\ntraining for brain–computer interface (BCI). J. Neurosci. Methods 179, 150–156. doi:\n10.1016/j.jneumeth.2009.01.015\nImran, S. M., Talukdar, M. T., Sakib, S. K., Pathan, N. S., and Fattah, S. A. (2014).\n“Motor imagery EEG signal classiﬁcation scheme based on Wavelet Domain Statistical\nFeatures, ” inProceedings of the 2014 international conference on electrical engineering\nand information & communication technology , (New York, NY), 1–4. doi: 10.1109/\niceeict.2014.6919172\nJacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. (1991). Adaptive\nmixtures of local experts. Neural Comput. 3, 79–87. doi: 10.1162/neco.1991.3.1.79\nJia, X., Song, Y., and Xie, L. (2023). Excellent ﬁne-tuning: From speciﬁc-subject\nclassiﬁcation to cross-task classiﬁcation for motor imagery. Biomed. Signal Process.\nControl 79:104051. doi: 10.1016/j.bspc.2022.104051\nJiang, Z., Liu, P., Xia, Y., and Zhang, J. (2021). “Application of CNN in EEG image\nclassiﬁcation of AD patients, ” in Proceedings of the 2nd international conference on\ncomputing and data science, Stanford, CA, 1–5. doi: 10.1145/3448734.3450473\nKim, D.-K., Kim, Y.-T., Jung, H.-R., Kim, H., and Kim, D.-J. (2021). “Sequential\ntransfer learning via segment after cue enhances the motor imagery-based\nbraincomputer interface, ” inProceedings of the 2021 9th international winter conference\non brain-computer interface, Gangwon, SK, 1–5. doi: 10.1109/bci51272.2021.9385340\nKing, C. E., Wang, P. T., Chui, L. A., Do, A. H., and Nenadic, Z. (2013). Operation of\na brain-computer interface walking simulator for individuals with spinal cord injury.\nJ. Neuroeng. Rehabil. 10, 77. doi: 10.1186/1743-0003-10-77\nKlepl, D., He, F., Wu, M., Blackburn, D. J., and Sarrigiannis, P. G. (2022). EEG-based\ngraph neural network classiﬁcation of Alzheimer’s disease: An empirical evaluation of\nfunctional connectivity methods.IEEE Trans. Neural Syst. Rehabil. Eng.30, 2651–2660.\ndoi: 10.1101/2022.06.14.496080\nKumar, S., Sharma, A., and Tsunoda, T. (2017). An improved discriminative ﬁlter\nbank selection approach for motor imagery EEG signal classiﬁcation using mutual\ninformation. BMC Bioinf. 18:545. doi: 10.1186/s12859-017-1964-6\nLawhern, V. J., Solon, A. J., Waytowich, N. R., Gordon, S. M., Hung, C. P., and\nLance, B. J. (2018). EEGNet: A compact convolutional neural network for EEG-\nbased brain–computer interfaces. J. Neural Eng. 15:056013. doi: 10.1088/1741-2552/\naace8c\nLee, J., Lee, S., Cho, W., Siddiqui, Z. A., and Park, U. (2021). Vision transformer-\nbased tailing detection in videos. Appl. Sci. 11:11591. doi: 10.3390/app11241\n1591\nLepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., et al. (2020). GShard:\nScaling giant models with conditional computation and automatic sharding. arXiv\n[Preprint]. arxiv:2006.16668.\nLi, H., Zhang, D., and Xie, J. (2023). Mi-Daban: A dual-attention-based adversarial\nnetwork for motor imagery classiﬁcation. Comput. Biol. Med. 152:106420. doi: 10.\n1016/j.compbiomed.2022.106420\nLi, X., Fan, H., Wang, H., and Wang, L. (2019). Common spatial patterns combined\nwith phase synchronization information for classiﬁcation of EEG Signals. Signals\nProcess. Control 52, 248–256. doi: 10.1016/j.bspc.2019.04.034\nLuo, Y., and Lu, B.-L. (2018). “EEG data augmentation for emotion recognition\nusing a conditional Wasserstein Gan, ” in Proceedings of the 2018 40th annual\ninternational conference of the IEEE engineering in medicine and biology society ,\n(Honolulu, HI: IEEE), 2535–2538. doi: 10.1109/embc.2018.8512865\nMilanés Hermosilla, D., Trujillo Codorniu, R., Lopez Baracaldo, R., Sagaro Zamora,\nR., Delisle Rodriguez, D., Llosas Albuerne, Y., et al. (2021). Shallow convolutional\nnetwork Excel for classifying motor imagery EEG in BCI applications. IEEE Access\n9, 98275–98286. doi: 10.1109/access.2021.3091399\nMustafa, B., Riquelme, C., Puigcerver, J., Jenatton, R., and Houlsby, N. (2022).\nMultimodal Contrastive Learning with LIMoE: The language-image mixture of\nexperts. arXiv [Preprint]. arxiv:2206.02770.\nOrmerod, M., Martínez del Rincón, J., and Devereux, B. (2021). Predicting semantic\nsimilarity between clinical sentence pairs using transformer models: Evaluation and\nrepresentational analysis. JMIR Med. Inform. 9:e23099. doi: 10.2196/23099\nRoth, H. R., Lu, L., Liu, J., Y ao, J., Seﬀ, A., Cherry, K., et al. (2016). Improving\ncomputer-aided detection using convolutional neural networks and random view\naggregation. IEEE Trans. Med. Imaging 35, 1170–1181. doi: 10.1109/tmi.2015.2482920\nShazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., et al. (2017).\nOutrageously large neural networks: The sparsely-gated mixture-of-experts layer.\narXiv [Preprint]. arxiv:1701.06538.\nShu, X., Y ao, L., Sheng, X., Zhang, D., and Zhu, X. (2017). Enhanced\nmotor imagery-based BCI performance via tactile stimulation on\nunilateral hand. Front. Hum. Neurosci. 11:585. doi: 10.3389/fnhum.2017.0\n0585\nSiddharth, S., Jung, T.-P., and Sejnowski, T. J. (2022). Utilizing deep learning\ntowards multi-modal bio-sensing and Vision-based aﬀective computing. IEEE Trans.\nAﬀect. Comput. 13, 96–107. doi: 10.1109/taﬀc.2019.2916015\nFrontiers in Neuroscience 16 frontiersin.org\nfnins-18-1366294 April 23, 2024 Time: 12:33 # 17\nLiu et al. 10.3389/fnins.2024.1366294\nSingh, S., and Mahmood, A. (2021). The NLP cookbook: Modern recipes for\ntransformer based deep learning architectures. IEEE Access 9, 68675–68702. doi: 10.\n1109/access.2021.3077350\nSuhail, T. A., Indiradevi, K. P., Suhara, E. M., Poovathinal, S. A., and Ayyappan, A.\n(2022). Distinguishing cognitive states using electroencephalography local activation\nand functional connectivity patterns. Biomed. Signal Process. Control 77:103742. doi:\n10.1016/j.bspc.2022.103742\nSun, J., Wang, X., Zhao, K., Hao, S., and Wang, T. (2022). Multi-channel EEG\nemotion recognition based on parallel transformer and 3D-convolutional neural\nnetwork. Mathematics 10:3131. doi: 10.3390/math10173131\nTalukdar, U., Hazarika, S. M., and Gan, J. Q. (2020). Adaptive feature extraction in\nEEG-based motor imagery BCI: Tracking mental fatigue. J. Neural Eng. 17:016020.\ndoi: 10.1088/1741-2552/ab53f1\nÚbeda, A., Azorín, J. M., Farina, D., and Sartori, M. (2018). Estimation of\nneuromuscular primitives from EEG slow cortical potentials in incomplete spinal\ncord injury individuals for a new class of brain-machine interfaces. Front. Comput.\nNeurosci. 12:3. doi: 10.3389/fncom.2018.00003\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). Attention is all you need. arXiv [Preprint]. arXiv:1706.03762.\nWang, Z., Wang, Y., Hu, C., Yin, Z., and Song, Y. (2022). Transformers for EEG-\nbased emotion recognition: A hierarchical spatial information learning model. IEEE\nSens. 22, 4359–4368. doi: 10.1109/jsen.2022.3144317\nXie, J., Zhang, J., Sun, J., Ma, Z., Qin, L., Li, G., et al. (2022). A transformer-\nbased approach combining deep learning network and spatial-temporal information\nfor raw EEG classiﬁcation. IEEE Trans. Neural Syst. Rehabil. Eng. 30, 2126–2136.\ndoi: 10.1109/tnsre.2022.3194600\nY ao, L., Meng, J., Zhang, D., Sheng, X., and Zhu, X. (2014). Combining motor\nimagery with selective sensation toward a hybrid-modality BCI. IEEE Trans. Biomed.\nEng. 61, 2304–2312. doi: 10.1109/tbme.2013.2287245\nZhang, C., Kim, Y.-K., and Eskandarian, A. (2021). EEG-inception: An\naccurate and robust end-to-end neural network for EEG-based motor\nimagery classiﬁcation. J. Neural Eng. 18:046014. doi: 10.1088/1741-2552/\nabed81\nZhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., et al. (2021). Informer:\nBeyond eﬃcient transformer for long sequence time-series forecasting. AAAI Conf.\nArtif. Intell. 35, 11106–11115. doi: 10.1609/aaai.v35i12.17325\nZhu, X., Jia, Y., Jian, S., Gu, L., and Pu, Z. (2021). Vitt: Vision transformer tracker.\nSensors 21:5608. doi: 10.3390/s21165608\nFrontiers in Neuroscience 17 frontiersin.org"
}