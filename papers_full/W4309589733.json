{
    "title": "Context Variance Evaluation of Pretrained Language Models for Prompt-based Biomedical Knowledge Probing",
    "url": "https://openalex.org/W4309589733",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4296413722",
            "name": "Yao, Zonghai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2124775988",
            "name": "Cao Yi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1923840813",
            "name": "Yang ZhiChao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1978159643",
            "name": "Yu Hong",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3093194543",
        "https://openalex.org/W2159583324",
        "https://openalex.org/W4306176992",
        "https://openalex.org/W2162800060",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W3097991493",
        "https://openalex.org/W4296413502",
        "https://openalex.org/W2396881363",
        "https://openalex.org/W2975135115"
    ],
    "abstract": "Pretrained language models (PLMs) have motivated research on what kinds of knowledge these models learn. Fill-in-the-blanks problem (e.g., cloze tests) is a natural approach for gauging such knowledge. BioLAMA generates prompts for biomedical factual knowledge triples and uses the Top-k accuracy metric to evaluate different PLMs' knowledge. However, existing research has shown that such prompt-based knowledge probing methods can only probe a lower bound of knowledge. Many factors like prompt-based probing biases make the LAMA benchmark unreliable and unstable. This problem is more prominent in BioLAMA. The severe long-tailed distribution in vocabulary and large-N-M relation make the performance gap between LAMA and BioLAMA remain notable. To address these, we introduce context variance into the prompt generation and propose a new rank-change-based evaluation metric. Different from the previous known-unknown evaluation criteria, we propose the concept of \"Misunderstand\" in LAMA for the first time. Through experiments on 12 PLMs, our context variance prompts and Understand-Confuse-Misunderstand (UCM) metric makes BioLAMA more friendly to large-N-M relations and rare relations. We also conducted a set of control experiments to disentangle \"understand\" from just \"read and copy\".",
    "full_text": "Context Variance Evaluation of Pretrained Language Models for\nPrompt-based Biomedical Knowledge Probing\nZonghai Yao, MSc1, Yi Cao, BS1, Zhichao Yang, MSc1, Hong Yu, PhD1,2,3,4\n1 College of Information and Computer Science, University of Massachusetts Amherst,\nAmherst, MA, USA;2Department of Computer Science, University of Massachusetts\nLowell, Lowell, MA, USA;3Department of Medicine, University of Massachusetts Medical\nSchool, Worcester, MA, USA;4Center for Healthcare Organization and Implementation\nResearch, Bedford Veterans Affairs Medical Center, Bedford, MA, USA\nAbstract\nPretrained language models (PLMs) have motivated research on what kinds of knowledge these models learn. Fill-in-\nthe-blanks problem (e.g., cloze tests) is a natural approach for gauging such knowledge. BioLAMA generates prompts\nfor biomedical factual knowledge triples and uses the Top-k accuracy metric to evaluate different PLMs’ knowledge.\nHowever, existing research has shown that such prompt-based knowledge probing methods can only probe a lower\nbound of knowledge. Many factors like prompt-based probing biases make the LAMA benchmark unreliable and\nunstable. This problem is more prominent in BioLAMA. The severe long-tailed distribution in vocabulary and large-N-\nM relation make the performance gap between LAMA and BioLAMA remain notable. To address these, we introduced\ncontext variance into the prompt generation and proposed a new rank-change-based evaluation metric. Different\nfrom the previous known-unknown evaluation criteria, we proposed the concept of ”Misunderstand” in LAMA for\nthe ﬁrst time. Through experiments on 12 PLMs, we showed that our context variance prompts and Understand-\nConfuse-Misunderstand (UCM) metric make BioLAMA more friendly to large-N-M relations and rare relations. We\nalso conducted a set of control experiments to disentangle ”understand” from just ”read and copy”.\nIntroduction\nPre-trained language models (PLMs) like BERT [1] have achieved impressive results on few-shot or zero-shot language\nunderstanding tasks by pre-training model parameters in a task-agnostic manner and then ﬁne-tuning to transfer the\nknowledge to speciﬁc downstream tasks [2, 3, 4, 5]. Recently, researchers have become interested in measuring how\nmuch factual information PLMs get from pre-training. [6] formally deﬁne this task in the LAMA benchmark, which\nconsists of (subject, relation, object) triples and a corresponding human-written template expressing each relation.\nThey show that BERT can predict objects given task-speciﬁc prompts. In the biomedical domain, [7] release the\nBiomedical LAnguage Model Analysis (BioLAMA) probe. They show that biomedical domain speciﬁc PLMs like\nBioBERT [8] can predict objects in biomedical factual knowledge triples given cloze-style prompts—for example,\n”nasal polyp has symptoms such as [Mask].”\nHowever, existing research has shown that such prompt-based knowledge probing methods can only probe a lower\nbound of knowledge [6, 9]. Even though subsequent work has attempted to improve by ﬁnding better prompts [10, 11],\nseveral biases of the prompt-based probing methods (i.e., Prompt Preference Bias, Instance Verbalization Bias, and\nSample Disparity Bias) make the evaluation in the LAMA benchmark unreliable [12, 13]. This problem is more\nsevere in N-M relation with large N and M [6] and in rare relations exhibiting long-tailed distribution [14]. For\nexample, according to Uniﬁed Medical Language System (UMLS) [15], asthmatic has 474 symptoms and cough has\nat least 66 related diseases. And the frequency of cough (the most common symptom of asthmatic) is much more than\nother rare symptoms. Since such large N-M relations and rare relations are common in biomedical domain, they make\nthe BioLAMA’s performance much lower than LAMA’s [12].\nIn this paper, we introduce context variance into the prompt generation to mitigate three prompt-based probing biases\nhighlighted by [13]. For every biomedical factual knowledge triple, we retrieved contexts from two large and repre-\nsentative biomedical resources: MIMIC-III [16], which comprises 59,652 electronic health record (EHR) notes, and\na randomly sampled subset of PMC [17], which includes 50k published biomedical articles. Following [18] work,\nwe collect a corpus of 50K outpatient EHR notes from the Local Medical Center, named LMC-EHRs dataset. The\narXiv:2211.10265v3  [cs.CL]  25 Jan 2023\nFigure 1: For triple ( S, relation, Ot), yellow entity represents S, orange entity represents Ot, red entities represents\nother entities that are equally correct in the context ( Ocor), and blue entities represents wrong entities in the context\n(Oincor). Here, ”correct” means that these entities also satisfy the same relation with S. We then split the context\ninto different segments according to these entities. Next, we add the context to the prompt segment by segment, and\nobserve that each time a new segment is added, how the ranking of Target Object ( Ot), Added Object ( Oa), Correct\nObject (Ocor), and Incorrect Object (Oincor) will change in the ”[MASK]” place.\nbiomedical PLMs are always pretrained on PubMed and MIMIC-III, we want to create this third context resource\nfor a more fair comparison since none of these PLMs saw these EHR data before. For a more robust evaluation, we\ncreated three different kinds of synthetic contexts, which were generated based on corresponding real contexts from\nthe above three resources, to largely increase the variance setting. Subsequently, we propose a new rank-change-based\nevaluation metric, namely Understand-Confuse-Misunderstand (UCM). Different from the previous known-unknown\nevaluation criteria in the previous rank-based metric (Top-k ACC), we propose a new concept of ”misunderstand” in\nLAMA for the ﬁrst time. Through extensive experiments on 12 PLMs, our context variance prompts and UCM metric\nmakes BioLAMA more friendly in Large-N-M relations and rare relations. In further experiments, we conducted a set\nof control experiments to disentangle ”understand” from just ”copy”, and showed that PLMs, especially biomedical\ndomain-speciﬁc PLMs show evidence of ”understanding” of some knowledge.\nRelated Work\nLanguage Models as Knowledge Bases[6] introduced the LAMA benchmark and began work on prompt-based\nfactual knowledge probing. They pointed out that their benchmark provides only a lower-bound estimate of the amount\nof factual information stored in PLMs, since their manually written prompts may not be optimal for eliciting facts.\nTherefore, follow-up work focused on tightening this bound to ﬁnd more optimal prompts. [9, 10, 11, 19]. We are also\nworking towards the same goal, but we do it from a different angle. We explore the way of context variance prompting\nmethod and re-design its evaluation metrics according to the characteristics of biomedical domain. Existing work uses\nTop-k ACC as the evaluation metric, so there are two possibilities for certain knowledge, ”Known” (in Top-k) and\n”Unknown” (not in Top-k). [20] added EHR notes as context to the prompt, and proposed a rank-change-base method.\nThey found that the rank-change-base method can also show that PLMs have some knowledge because PLMs have\ncompletely different behavior on knowledge and noise under their setting. Our work builds on this rank-change-base\nmethod. Thanks to the three situations that will occur in ”rank-change” (increase, no change, and decrease), PLMs\ncan have three different status for certain knowledge (”Understand”, ”Confuse”, and ”Misunderstand”). Among them,\n”Misunderstand” is important for the biomedical domain, because the potential harm caused by ”Misunderstand”, such\nas hallucination and contradiction, is much greater than just ”Unknown” or ”Confuse”.\nBioLAMA In the biomedical domain, [7] created and released the BioLAMA benchmark following the LAMA set-\nting, which consists of 49K biomedical factual triples. However, the performance gap between LAMA and BioLAMA\nremains notable because biomedical domain knowledge probing has its unique challenges (including severe long-tailed\ndistribution in vocabulary, multi-token entities, large N-M relations). [14] proposed a new method, Contrastive-Probe,\nto handle the multi-token challenge during encoding answers. In this paper, we try to handle the other two challenges,\nlong-tailed distribution in vocabulary and large N-M relations evaluation.\nBiases in Prompt-based Knowledge Probing[13] highlighted three critical biases which could impact performance:\nPrompt Preference Bias (PPB), Instance Verbalization Bias (IVB), and Sample Disparity Bias (SDB). PPB shows\nthe LAMA performance may be biased by the ﬁtness of a prompt to PLMs’ linguistic preference. which means\nsemantically equivalent prompts may lead to different biased evaluation results. IVB shows the evaluation results are\nsensitive and inconsistent to the different verbalizations of the same instance (e.g., representing the COVID-19 with\nthe SARS-CoV-2 or Coronavirus disease 2019). SDB shows the performance difference between different PLMs may\ndue to the sample disparity of their pretraining corpora, rather than their ability divergence. In this paper, we introduce\ncontext variance into the prompt generation to mitigate these three aforementioned biases.\nMethods\nTo quantify a model’s knowledge, we re-design the BioLAMA evaluation process. Our assumptions are as follows.\n1. Adding a context containing subject and object and other relevant entities to the prompt can help the BioLAMA\ntask overcome aforementioned challenges and biases.\n2. The previous rank-based metric is not optimal in model stability and reliability. An improved rank-change based\nmetrics, which best captures rank variations between different inputs, can better evaluate PLMs knowledge.\nContext Segmentation and Retrieval\nAssume we have one relation (Subject, relation, Objecttarget), or (S, relation, Ot), to be evaluated:\n1. We retrieve documents including both subject S and target object Ot from certain resources.\n2. We will target all other entities that have the same entity-type of Ot (e.g. nasal discharge’s entity type is\n”symptom”), named entity-pool (Opool). According to certain Knowledge Bases (KBs) in BioLAMA [15, 21,\n22], there may be other objects also satisfying the same relation with this subject. So we named all those correct\nentities in Opool as Ocor (red entities in the Figure 1: nasal polyp, symptom, shortness of breath.), and the rest\nof entities in the Opool as Oincor (bleu entities in the Figure 1: nasal polyp, not symptom, wheeze)\n3. We take the Ot as the center and every time look for the next entity in left and right order. We use these\nentities as delimiters to split the document into segments and ensure that only one entity per segment belongs to\nOpool. These segments are named S1, S2, S3, S4, etc. For example, in Figure 1, the text surrounding the ”nasal\ndischarge” is used as S1 until the ”sick feeling” is encountered on the left side of S1, and the ”postnasal drip”\nis encountered on the right side of S1, so we ensure that S1 only contains one related entity ”nasal discharge”.\nSimilarly, S2 adds to the left text of ”sick feeling” in the corresponding segment until it encounters ”heartburn”.\nS3 adds to the right text of the ”postnasal drip” in the corresponding segment until it encounters the ”nasal\nstufﬁness”. Note that if we can’t ﬁnd a new entity in one side, then we will always search the other side.\nFigure 2: The three Synthetic Contexts and Negative-target Real context are mentioned in the Method section. As-\nsume we already have one Real context, we remove all non-knowledge content of the Real Context to generate the\nKnowledge-only context. We continue to concatenate S1, S2, S3 ... in the order from left to right to generate the\nknowledge-sorted context. We randomly shufﬂe all segments in the Knowledge-only context and then rename S1 to\nS10 with Ot as the center to generate the Knowledge-random context. For Negative-target Real context, we ﬁnd the\nclosest Oincor to Ot in the Real context and do the segmentation again using thisOincor as the new center (the original\nOt is only an Ocor in the context).\nContext Variance Prompt Generation\nWe have no restrictions on the prompt generation methods, both the manual prompt and the trained prompt can be\ncombined with our methods for the probing task. Following the BioLAMA work, for certain triple ( S, relation, Ot),\nwe manually generate the corresponding prompts for probing.\nReal Context Prompt\nIn the previous section, we get the required context and split them into several segments. Here, we add these segments\nto the prompt input in turn and get some new inputs. In order to keep the structure of the context, we will put every\nsegment in the correct position (the same with the original text) when adding a new segment. For example, in Figure 1,\nwe can use the manual prompt method to generate a prompt like ”nasal polyp has symptoms such as [Mask].”, which\nis the input0. We put S1 before the prompt and get input1. Because S2 is on the left side of S1 in the original text,\nso we put S2 before S1 and get input2. Because S3 is on the right side of S1 in the original text, we put S3 after S1\n(before the prompt) and get input3, and so on.\nSynthetic Context Prompt\nWe call the contexts described in the previous section as real contexts, because they are retrieved texts from real-world\nresources. However, there are not only knowledge (entities) in the real context, but also other texts. The performance\nof PLMs in prompt-based probing tasks is not only determined by knowledge, but also affected by language ability in\nthe whole context. In order to measure knowledge more accurately, and also to add more variance, we design three\nsynthetic context generation methods. In Figure 2, we continue with the previous example to describe how we generate\nthese synthetic contexts.\nKnowledge-only context For the real context, we remove those non-knowledge texts in each segment, and only keep\nknowledge in the context. The positions of all segments remain unchanged. The synthetic context generated in this\nway simply removes the ”non-knowledge” content.\nKnowledge-sorted context We sort the segments in ascending order for the Knowledge-only context, and add them\nto the synthetic context from left to right (S1 S2 S3 ... + prompt). The synthetic context generated here not only\nremoves the ”non-knowledge” content, but also changes the relative position between different segments in the input.\nHowever, the order in which each segment is added to the input remains unchanged.\nKnowledge-random context We keep S1 in the ﬁrst because it contains the target object. Then we randomly pick\nthe next segment from the remaining segments and add it to any position in the input. The synthetic context generated\nin this way only preserves the same object-pool. But the order and position of the segments are added at random.\nNegative-target Context\nBecause of the attention mechanism, PLMs often ”rely too much on copying from the context” [23, 19]. Speciﬁc\nto behavior, if PLMs do not understand certain knowledge, they may directly copy non-relevant or incorrect entities\nmentioned in the context. Conversely, if PLMs have certain knowledge, they may be less likely to do so. In other\nwords, in BioLAMA, the knowledgeable PLMs will rank higher to the correct than incorrect objects in the [MASK]\nposition. In order to verify this idea, we designed the corresponding Negative-target context for each context men-\ntioned. As shown in Figure 2, we search for Oincor from the segments in the context in the order of S1, S2, S3, S4....\nWe take the ﬁrst Oincor found as the Negative-target of this context, and treat the original Ot as a normal Ocor. Then\nwe re-construct the context-variance prompt belonging to the negative-target by using all our methods described above\nwith the negative-target as the center. Therefore, for each Ot-centered normal data, we will have a corresponding\nnegative-target-centered comparison data. We hope to use such control experiments to explore whether PLMs behave\ncompletely different for the two data. We use this as a distinction between ”read from context and copy the knowledge”\nand ”really understand the knowledge”.\nRank-change Calculation\nAs described in the previous section, because of the attention mechanism, PLMs give entities that appear in the context\nmore likely to appear at the [MASK]. Therefore, if we continue to use Top-k ACC,Ot will always have a particularly\nhigh probability of being judged as ”known”, because all the contexts we add containOt. Since the rank-based metric\nis not suitable for evaluating the context variance prompt probing task, we design a rank-change-based metric.\nAs shown in Figure 1, when we have continuous segments S1, S2, S3, S4...., we regard the context as a process of\ncontinuously adding new segments centered on Ot. In this process, each newly added segment will only contain one\nentity, named Oa. Here, Oa may belong to Ocor, and may belong to Oincor. Intuitively, PLMs with the corresponding\nknowledge should have different behaviors for these two different situations. In particular, we try to understand how\nPLMs behave differently for different entities in context by observing the following four sets of Rank Change (RC)\nresults. As shown in Figure 1, assuming that we add S5 to input4:\n1. RC- Ot: change in the ranking of Ot\n2. RC- Oa: change in the ranking of Oa in S5\n3. RC- Ocor: average change in the ranking of all existing Ocor in the output4\n4. RC- Oincor: average change in the ranking of all existing Oincor in the output4\nHere, the ”change in the ranking” refers to the ranking of the corresponding entity in output5 minus its ranking in\noutput4. The result of RC (output5-output4) will have three cases, ranking-increasing (negative), ranking-unchanged\n(zero) and ranking-decreasing (positive). It is interesting to observe how PLM’s RC behaves differently when a new\nOa come (Oa can be either Ocor or Oincor). We put this results in table 1.\nOa ∈ Ocor Oa ∈ Oincor\nOt Oa Ocor Oincor Ot Oa Ocor Oincor\nBERT -0.98 -6.56 -2.8 0.045 0.78 -1.98 0.74 -0.59\nBioBERT -2.07 -12.7 -5.62 0.23 1.04 -6.05 1.06 -2.02\nBioLM -1.59 -10.56 -5.2 0.09 0.88 -3.76 0.91 -1.32\nTable 1: With different contexts, rank-change measures changes of predictions of [MASK] in prompts by different\nPLMs. There are three cases for rank change, ranking-increasing (negative), ranking-unchanged (zero) and ranking-\ndecreasing (positive).\nUnderstand-Confuse-Misunderstand Metric (UCM)\nThe BioLAMA tries to verify whether or not a certain PLM has knowledge about ( S, relation, Ot), so in the four\ngroups of RC scores, we can directly take RC-Ot as the metric. So, according to the results of Rank-change, we deﬁne\nthe UCM metric as follows. Suppose we get a series of RC scores by the method described:\n1. For each input, we check whether Oa belongs to Ocor and leaving only those RC for which Oa belongs to Ocor\n2. For the remaining data, we count their distribution, and take the proportion of RC- Ot less than 0 as the Under-\nstand score, the proportion of RC-Ot equal to 0 as the Confuse score, and the proportion of RC-Ot greater than\n0 as the Misunderstand score\nWe name the above steps as knowledge-level-UCM (UCM k), which is consistent with the purpose of previous metric\nTop-k ACC. UCM k tries to verify whether a certain PLM knows a certain triple (knowledge) in one relation (like\ndisease-symptom relation), and then calculate the respective percentages of U, C, and M for all triples in this relation\nfor evaluation. The previous metric Top-k ACC is doing the same thing, except they compute the percentages of known\n(in Top-k) and unknown (not in Top-k).\nSimilarly, if we not only want to verify whether a certain PLM has a certain knowledge, but directly compare whether\nsome PLMs are more knowledgeable, then we can collect all context variance prompts for all relations’ triples in a\ncertain KB, and use the same steps above to get the RC distribution over this KB. With this we can compare the UCM\nscores of different PLMs to tell which PLMs are more knowledgeable. We name it as model-level-UCM (UCM m).\nResults\nExperimental Setup\nWe conduct the experiments on 12 PLMs, including BERT (base and large) [1], RoBERTa (base and large) [24],\nBioBERT [8], ClinicalBERT [25], 3 kinds of BioLMs [26], and 3 kinds of BlueBERTs [27]. All the results in the\nExperiments part are the average scores of three different resource data as context. We use manual prompts created\nby domain experts following BioLAMA. It is important to note that we have no restrictions on the prompt generation\nmethods, both the manual prompt and the trained prompt can be combined with our methods. We use a ﬁll-in-the-\nblank cloze statement (i.e., a ”prompt”) for probing. Since the majority of entities in our dataset are made up of\nmultiple tokens, we also implement a multi-token decoding strategy following [7]. We mainly use UCM introduced\nour Method section as the evaluation metric, but we also report the results of using the previous metrics following\nBioLAMA with Top-k accuracy, which is 1 if any of the Top k object entities are included in the annotated object list,\nand is 0 otherwise\nRank-change\nTable 1 shows PLMs’ rank-change behaviors. Generally speaking, becauseOa is added to the context, there will be a\nranking-increase because of the attention mechanism. Ot belongs to Ocor, so intuitively there will be the same trend\nof RC. And Ocor and Oincor should have the completely opposite trends. Depending on the difference of Oa in the\nnewly added segment (Oa may belong to either Ocor or Oincor), the speciﬁc changes of the four groups of RC will\nwithout context with context\nacc@1 acc@5 acc@1 acc@5 U C M\nBERT-base 0.0038 0.57 0.327 0.68 0.119 0.787 0.094\nBioBERT 0.227 0.639 0.407 0.881 0.328 0.427 0.243\nClinicalBERT 0.014 0.264 0.391 0.812 0.302 0.533 0.165\nBERT-large 0 0.009 0.258 0.665 0.158 0.738 0.103\nRoBERTa-base 0.11 0.218 0.572 0.872 0.287 0.495 0.217\nRoBERTa-base-PM 0.134 0.281 0.412 0.902 0.298 0.427 0.274\nRoBERTa-base-PM-M3 0.102 0.273 0.563 0.904 0.299 0.432 0.267\nRoBERTa-large 0.107 0.229 0.503 0.865 0.304 0.509 0.186\nRoBERTa-large-PM-M3 0.105 0.31 0.644 0.915 0.312 0.432 0.255\nTable 2: Top-k ACC VSUCM k\nalso be different. We ﬁnd that if Oa belongs to Ocor, then Ocor and Ot should have the same trend of change (rank\nincrease) as Oa, otherwise Oincor will have the same trend of change as Oa.\nUCM K and Top-k ACC\nHere we show that UCM can be a more appropriate metric than Top-k ACC. We compareUCM k with top-k accuracy\nwith or without context.Table 2 shows that evaluation results with Top-k ACC is often counter-intuitive and unstable.\nFor example, PLMs pretrained on the biomedical domain corpus (e.g., clinicalBERT) get much lower scores than the\ngeneral domain PLMs (e.g., BERT). The clinicalBERT and BioBERT are trained in the biomedical domain and have\nsimilar performance in most of the downstream tasks, yet they show a very huge gap in their performance. Table\n2 shows that when context is included, almost all BioLMs’ ACC@5 have non-distinguishable 90% scores, and we\ncannot distinguish whether the model is ’copy’ or understand based on the evaluation with Top-k ACC.\nIn contrast, the results with UCM k are more interpretable and consistent. For both BERT families and BioLMs\nfamilies in table 2, the results show that large models have better scores (higher U, lower or similar C and M) than the\ncorresponding base models. We additionally analyze the scores for each resource. We ﬁnd that when we use LMC-\nEHRs data as the context, biomedical PLMs (BioBERT, clinicalBERT, RoBERTa-base-PM, RoBERTa-base-PM-M3)\nall have better performance (higher U, lower C and M) than the general domain PLMs (BERT-base and RoBERTa-\nbase). Since MIMIC-III is an EHR resource and in constrast PMC comprises biomedical literature articles, PLMs\npretrained on MIMIC-III have a better performance (similar U and C, lower M) on LMC-EHRs data. All these results\nillustrates that introduction of ”Misunderstand” makes the results more consistent and interpretable. Although PLMs\ntrained on both PMC and MIMIC-III have knowledge inside LMC-EHRs context, a more similar domain (MIMIC-III)\ncan provide more accurate knowledge, making these PLMs less ”Misunderstand”.\nEvaluation with UCM\nWe compute UCM k and UCM m to compare the knowledge level between different PLMs. We want to note that even\nthough we show that UCM is a more reliable metric than Top-k ACC, it is not easy to fairly compare the knowledge\nlevel between different PLMs. Too many factors may affect the knowledge learned by the PLMs during pretraining, so\nwe have to choose the more easily interpretable models to compare. For example, in table 2 and 3, RoBERTa-base is\na general-domain PLM, RoBERTa-base-PM continues to train RoBERTa-base on PubMed, and RoBERTa-base-PM-\nM3 continues to train RoBERTa-base-PM on MIMIC-III. So the comparison between this group of PLMs is more\nmeaningful, because the results in table 2 and 3 can tell us that if the PLMs continue to train on the target-domain\nrelated data, their knowledge level will be higher.\nAccording to table 3, We have the following ﬁndings based on the UCM m scores. We re-emphasize that although\nthe ﬁndings we concluded from the UCM m results are not particularly surprising, such ”reasonable” and ”intuitive”\nresults can not be obtained with previous metrics. So we make up for these evidences for the biomedical domain with\ntarget negative-target\nU C M U C M\nBERT-base 0.205 0.73 0.060 0.026 0.92 0.049\nBioBERT 0.32 0.584 0.095 0.103 0.72 0.175\nClinicalBERT 0.37 0.56 0.061 0.07 0.79 0.134\nRoBERTa-base 0.23 0.67 0.099 0.055 0.83 0.11\nRoBERTa 0.301 0.583 0.115 0.109 0.72 0.168\nRoBERTa-base-PM-M3 0.335 0.55 0.112 0.107 0.77 0.12\nBlueBERT-base-PM 0.383 0.554 0.062 0.026 0.89 0.085\nBlueBERT-base-PM-M3 0.407 0.559 0.033 0.025 0.89 0.078\nBERT-large 0.199 0.75 0.045 0.026 0.9 0.047\nRoBERTa-large 0.325 0.614 0.061 0.047 0.86 0.113\nRoBERTa-large-PM-M3 0.346 0.556 0.097 0.063 0.82 0.113\nBlueBERT-large-PM-M3 0.388 0.504 0.108 0.053 0.85 0.095\nTable 3: PLMs’UCM m scores of Ot as center or negative-target as center\na more reasonable BioLAMA metric.\n1. Large models are always better\n2. The similarity between the context domain and PLMs’ pretraining domain is decisive\n3. Training longer does not make PLMs more knowledgeable, which is in line with recent work [28]\n4. Real and Knowledge-only contexts do not have much differences. For three synthetic contexts, Knowledge-only\ncontext and Knowledge-sorted context do not have big differences, but the Knowledge-random context results\nare unstable.\nDiscussion\nCopy or Understand\nThe attention mechanism makes PLMs tend to copy knowledge entities in context. The Top-k ACC results with and\nwithout context in table 2 also prove that this phenomenon does exist in our task setting. So, after adding knowledge\ninto context prompts, there is a possibility that PLMs copy from context without understanding. To distinguish the\ntwo, we introduce the negative-target context in the Method section. In table 3, by comparing PLMs’ UCM m score\nOt as center or instead negative-target as center, we ﬁnd that the behaviors of PLMs for the two are almost completely\nopposite. This control experiment suggests that PLMs understand these knowledge, and do not just copy.\nN-M Relations and Rare Relations Friendly\nThe performance gap between LAMA and BioLAMA remains notable because biomedical domain knowledge probing\nhas its unique challenges, and as stated previously, two of them are severe long-tailed distribution in vocabulary (rare\nrelation unfriendly) and many large-N-M relations [7, 14]. Because the rare relation rarely appears in pretraining data,\nit is difﬁcult for prompts to elicit these facts. The reason behind this is that PLMs are context sensitive, and only\nspeciﬁc prompts as contexts allow them to generate corresponding results. For common relation, they are frequently\nmentioned in different contexts in pretraining, so the requirements for prompt are relatively broad. In contrast, even\nif a PLM knows a rare relation, it has very strict requirements on the prompt, that is, the prompt used must be very\nrelated to the context in these PLMs’ pretraining step. On the other hand, the large-N-M relations means the prompt\nhas lots of objects that may be the right answers, however, the model cannot distinguish the difference between Ot\nand other Ocor in the Figure 1. Due to the softmax bottleneck of PLMs [29, 30], if the model prefers to put otherOcor\non Top-k, it may not be able to put Ot on Top-k at the same time. Therefore, the previous evaluation is unfair and\ninaccurate. In this paper, we introduce context variance in the prompt generation process to alleviate the above two\nproblems. Because of the attention mechanism, PLMs often ”rely too much on copying from the context” [23, 19],\nwhich means PLMs usually directly copy other entities mentioned in the context. This is sometimes a big weakness\nof PLMs for generational tasks, but it is unexpectedly well suited for our use case. If we add the context containing\na speciﬁc set of Opool to the prompt as the input, PLMs will naturally have candidates (i.e. Opool) at the [MASK]\nposition because of attention mechanism. As long as candidates contain Ot, no matter whether ( S, relation, Ot) is\nrare or common relations, or 1-1 or N-M relations, we can use the uniﬁed method to observe different behaviors when\nPLMs treat different inputs. We deﬁne a set of procedures in the next few sections to determine whether PLMs know\nthis knowledge according to different behaviors.\nMitigating PPB, IVB, and SDB Biases\n[13] highlighted three critical biases which affect the correctness and fairness of evaluating different PLMs on the\nBioLAMA benchmark. We evaluate whether context variance and UCM can alleviate the three biases. Here we\ndiscuss how the context variance prompts mitigate these biases. We control Prompt Preference Bias (PPB) by adding\ncontext variance, although the prompt itself does not change, the input to the PLMs is not the same. Because UCM is a\nstatistical result of a large number of context variance prompts, only those PLMs that can be regarded as ”Understand”\nin most data are treated as ”really know” the corresponding knowledge. Therefore, UCM will not encounter serious\nPPB like the previous method because only one prompt is used for probing all PLMs. For Instance Verbalization Bias\n(IVB), we ﬁrst map text to the UMLS concepts and therefore eliminate the verbalization challenge at the method level.\nAt the same time, the context variance prompts contain a large amount of variations from real-world resources, so\ndifferent verbalization can be used as input to PLMs. These all help us relieve IVB. We control Sample Disparity Bias\n(SDB) by choosing resources more fairly. All biomedical domain speciﬁc PLMs selected in this paper are pre-trained\non the representative biomedical text resources: PubMed or MIMIC-III. We also use LMC-EHRs as a third resource,\nand no PLMs in our evaluation have seen this dataset.\nConclusion\nIn this paper, we show that adding context containing relevant entities to the prompt can mitigate two unique challenges\n(long-tailed distribution in vocabulary and many large-N-M relations) and three biases (PPB, IVB, and SDB) in the\nBioLAMA task. Then we ﬁnd the previous rank-based metric is not optimal in model stability and reliability. An\nimproved rank-change-based metric, which best captures rank variations between different inputs, can better evaluate\nPLMs’ knowledge. We hope that the evidence in our paper will inspire the BioNLP community to pay more attention\nto what kinds of medical knowledge nowadays language models can learn.\nReferences\n1. Devlin J, Chang MW, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language\nunderstanding. arXiv preprint arXiv:181004805. 2018.\n2. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, et al. Language models are few-shot learners.\nAdvances in neural information processing systems. 2020;33:1877-901.\n3. Yao Z, Cao L, Pan H. Zero-shot entity linking with efﬁcient long range sequence modeling. arXiv preprint\narXiv:201006065. 2020.\n4. Yao Z, Yu H. Improving formality style transfer with context-aware rule injection. arXiv preprint\narXiv:210600210. 2021.\n5. Kwon S, Yao Z, Jordan HS, Levy DA, Corner B, Yu H. MedJEx: A Medical Jargon Extraction Model with Wiki’s\nHyperlink Span and Contextualized Masked Language Model Score. arXiv preprint arXiv:221005875. 2022.\n6. Petroni F, Rockt ¨aschel T, Lewis P, Bakhtin A, Wu Y , Miller AH, et al. Language models as knowledge bases?\narXiv preprint arXiv:190901066. 2019.\n7. Sung M, Lee J, Yi S, Jeon M, Kim S, Kang J. Can Language Models be Biomedical Knowledge Bases? arXiv\npreprint arXiv:210907154. 2021.\n8. Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, et al. BioBERT: a pre-trained biomedical language representation\nmodel for biomedical text mining. Bioinformatics. 2020;36(4):1234-40.\n9. Jiang Z, Xu FF, Araki J, Neubig G. How can we know what language models know? Transactions of the\nAssociation for Computational Linguistics. 2020;8:423-38.\n10. Shin T, Razeghi Y , Logan IV RL, Wallace E, Singh S. Autoprompt: Eliciting knowledge from language models\nwith automatically generated prompts. arXiv preprint arXiv:201015980. 2020.\n11. Zhong Z, Friedman D, Chen D. Factual probing is [MASK]: Learning vs. learning to recall. arXiv preprint\narXiv:210405240. 2021.\n12. Cao B, Lin H, Han X, Sun L, Yan L, Liao M, et al. Knowledgeable or educated guess? revisiting language models\nas knowledge bases. arXiv preprint arXiv:210609231. 2021.\n13. Cao B, Lin H, Han X, Liu F, Sun L. Can Prompt Probe Pretrained Language Models? Understanding the Invisible\nRisks from a Causal View. arXiv preprint arXiv:220312258. 2022.\n14. Meng Z, Liu F, Shareghi E, Su Y , Collins C, Collier N. Rewire-then-Probe: A Contrastive Recipe for Probing\nBiomedical Knowledge of Pre-trained Language Models. arXiv preprint arXiv:211008173. 2021.\n15. Bodenreider O. The uniﬁed medical language system (UMLS): integrating biomedical terminology. Nucleic acids\nresearch. 2004;32(suppl 1):D267-70.\n16. Johnson AE, Pollard TJ, Shen L, Lehman LwH, Feng M, Ghassemi M, et al. MIMIC-III, a freely accessible\ncritical care database. Scientiﬁc data. 2016;3(1):1-9.\n17. Goldberger AL, Amaral LA, Glass L, Hausdorff JM, Ivanov PC, Mark RG, et al. PhysioBank, Phys-\nioToolkit, and PhysioNet: components of a new research resource for complex physiologic signals. circulation.\n2000;101(23):e215-20.\n18. Yang Z, Yu H. Generating accurate electronic health assessment from medical graph. In: Proceedings of the\nConference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural\nLanguage Processing. vol. 2020. NIH Public Access; 2020. p. 3764.\n19. Petroni F, Lewis P, Piktus A, Rockt¨aschel T, Wu Y , Miller AH, et al. How context affects language models’ factual\npredictions. arXiv preprint arXiv:200504611. 2020.\n20. Yao Z, Cao Y , Yang Z, Deshpande V , Yu H. Extracting Biomedical Factual Knowledge Using Pretrained Language\nModel and Electronic Health Record Context. arXiv preprint arXiv:220907859. 2022.\n21. Davis AP, Grondin CJ, Johnson RJ, Sciaky D, Wiegers J, Wiegers TC, et al. Comparative toxicogenomics database\n(CTD): update 2021. Nucleic acids research. 2021;49(D1):D1138-43.\n22. Turki H, Shafee T, Taieb MAH, Aouicha MB, Vrande ˇci´c D, Das D, et al. Wikidata: A large-scale collaborative\nontological medical database. Journal of Biomedical Informatics. 2019;99:103292.\n23. Li M, Roller S, Kulikov I, Welleck S, Boureau YL, Cho K, et al. Don’t Say That! Making Inconsistent Dialogue\nUnlikely with Unlikelihood Training. arXiv preprint arXiv:191103860. 2019.\n24. Liu Y , Ott M, Goyal N, Du J, Joshi M, Chen D, et al. Roberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:190711692. 2019.\n25. Alsentzer E, Murphy JR, Boag W, Weng WH, Jin D, Naumann T, et al. Publicly available clinical BERT embed-\ndings. arXiv preprint arXiv:190403323. 2019.\n26. Lewis P, Ott M, Du J, Stoyanov V . Pretrained Language Models for Biomedical and Clinical Tasks: Understanding\nand Extending the State-of-the-Art. In: Proceedings of the 3rd Clinical Natural Language Processing Workshop.\nOnline: Association for Computational Linguistics; 2020. p. 146-57. Available from: https://www.aclweb.org/\nanthology/2020.clinicalnlp-1.17.\n27. Peng Y , Yan S, Lu Z. Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and\nELMo on Ten Benchmarking Datasets. In: Proceedings of the 2019 Workshop on Biomedical Natural Language\nProcessing (BioNLP 2019); 2019. p. 58-65.\n28. Goyal T, Xu J, Li JJ, Durrett G. Training dynamics for text summarization models. arXiv preprint\narXiv:211008370. 2021.\n29. Yang Z, Dai Z, Salakhutdinov R, Cohen WW. Breaking the softmax bottleneck: A high-rank RNN language\nmodel. arXiv preprint arXiv:171103953. 2017.\n30. Chang HS, McCallum A. Softmax Bottleneck Makes Language Models Unable to Represent Multi-mode Word\nDistributions. In: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(V olume 1: Long Papers); 2022. p. 8048-73."
}