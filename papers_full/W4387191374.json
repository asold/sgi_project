{
  "title": "Diabetic Retinopathy Classification Using Swin Transformer with Multi Wavelet",
  "url": "https://openalex.org/W4387191374",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3133753026",
      "name": "Rasha Ali Dihin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5116016479",
      "name": "Ebtesam AlShemmary",
      "affiliations": [
        "University of Kufa"
      ]
    },
    {
      "id": "https://openalex.org/A5006461008",
      "name": "Waleed Al-Jawher",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5116016479",
      "name": "Ebtesam AlShemmary",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2060422862",
    "https://openalex.org/W2972246881",
    "https://openalex.org/W2968617104",
    "https://openalex.org/W3029663394",
    "https://openalex.org/W3197957534",
    "https://openalex.org/W4224250702",
    "https://openalex.org/W3161825146",
    "https://openalex.org/W4205281574",
    "https://openalex.org/W4229334058",
    "https://openalex.org/W6800689796",
    "https://openalex.org/W4221163766",
    "https://openalex.org/W4211112191",
    "https://openalex.org/W2046759099",
    "https://openalex.org/W2184676127",
    "https://openalex.org/W3216853350",
    "https://openalex.org/W3103855452",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W2157407988",
    "https://openalex.org/W4287829349",
    "https://openalex.org/W4285104230",
    "https://openalex.org/W4286857019",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W4285707640"
  ],
  "abstract": "Diabetic retinopathy (DR) impacts over a third of individuals diagnosed with diabetes and stands as the leading cause of vision loss in working-age adults worldwide. Therefore, the early detection and treatment of DR can play a crucial role in minimizing vision loss. This research paper proposes a novel technique that combines Wavelet and multi-Wavelet transforms with Swin Transformer to automatically identify the progression level of diabetic retinopathy. A notable innovation of this study lies in the implementation of the multi-Wavelet transform for extracting relevant features. By incorporating the resulting images into the Swin Transformer model, a unique approach is introduced during the feature extraction phase. The researchers conducted experiments using the publicly available Kaggle APTOS 2019 dataset, which comprises 3662 images. The achieved training accuracy in the experiments was an impressive 97.78%, with a test accuracy of 97.54%. The highest accuracy observed during training reached 98.09%. In comparison, when applying the multi-Wavelet approach to multiclass classification, the training and validation accuracies were 91.60% and 82.42%, respectively, with a testing accuracy of 82%. These results indicate that the multi-Wavelet approach outperforms alternative methods in the study. The model demonstrated exceptional performance in binary classification tasks, exhibiting high accuracies on both the training and test sets. However, it is important to note that the model's accuracy decreased when employed in multiclass classification, emphasizing the need for further investigation and refinement to handle more diverse classification scenarios.",
  "full_text": "Journal of Kufa for Mathematics and Computer           Vol.10, No.2, Aug., 2023, pp 167-172 \n \n167 \n \nDiabetic Retinopathy Classification Using Swin Transformer with \nMulti Wavelet \nRasha Ali Dihin  \nFaculty of Computer Science and Mathematics \nDepartment of Mathematics \nUniversity of Kufa  \nNajaf, Iraq \nrashaa.aljabry@uokufa.edu.iq  \nEbtesam N. AlShemmary \nIT Research and Development Center  \nUniversity of Kufa  \nNajaf, Iraq \ndr.alshemmary@uokufa.edu.iq\n \n \n \nWaleed A. Mahmoud Al-Jawher \nUruk University \nBagdad, Iraq \nprofwaleed54@gmail.com   \n \n \n \n \n     Abstract‚Äî Diabetic retinopathy (DR) impacts over a third of individuals diagnosed with diabetes and stands as the \nleading cause of vision loss in working-age adults worldwide. Therefore, the early detection and treatment of DR can \nplay a crucial role in minimizing v ision loss. This research paper proposes a novel technique that combines Wavelet \nand multi -Wavelet transforms with Swin Transformer to automatically identify the progression level of diabetic \nretinopathy. A notable innovation of this study lies in the implementation of the multi-Wavelet transform for extracting \nrelevant features. By incorporating the resulting images into the Swin Transformer model, a unique approach is \nintroduced during the feature extraction phase. The researchers conducted experiments us ing the publicly available \nKaggle APTOS 2019 dataset, which comprises 3662 images. The achieved training accuracy in the experiments was an \nimpressive 97.78%, with a test accuracy of 97.54%. The highest accuracy observed during training reached 98.09%. \nIn comparison, when applying the multi -Wavelet approach to multiclass classification, the training and validation \naccuracies were 91.60% and 82.42%, respectively, with a testing accuracy of 82%. These results indicate that the multi-\nWavelet approach outperforms alternative methods in the study. The model demonstrated exceptional performance in \nbinary classification tasks, exhibiting high accuracies on both the training and test sets. However, it is important to \nnote that the model's accuracy decreased when employed in multiclass classification, emphasizing the need for further \ninvestigation and refinement to handle more diverse classification scenarios.  \n \n \nKeywords‚Äî Diabetic retinopathy, Swin transformer, muti- Wavelet, APTOS 2019, Vision transformers. \n \nI. INTRODUCTION  \nDiabetic retinopathy is one of the diseases that affect the eye \nand the cause is due to diabetes, and thus it is a major cause of \nvision loss, as about 34.6%) of patients with diabetes suffer from \n(DR) and this is found in Asia, Europe and the United States [1]. \nWith the increase in the number of people with diabetes all over \nthe world, the number of people with diabetes was estimated \nfrom 108 million in 1980 to an estimated 425 million in 2017, \nand to approximately 629 million in 2045. These statistics have \nbecome a global epidemic [ 2]. People  with diabetes usually \nremain asymptomatic until the advanced stages of DR are \nuncontrollable and therefore early examination is necessary to \nstart treatment in time [3]. \nNumerous obstacles are associated with eye care, \nencompassing the exorbitant expenses of healthcare and the \nlimited availability of eye specialists in low- and middle-income \ncountries. These regions lack adequate healthcare infrastructure \nto effectively detec t and address eye -related ailments. Early \ndiagnosis is imperative to effectively manage the disease and \navert vision impairment [4]. Vision transformers (ViTs) were \nfirst proposed for the machine translation task in the Natural \nLanguage Processing NLP domai n. The transformer -based \nOrcid.org/0000-0002-3857-4013 \nOrcid.org/0000-0001-7500-9702 \nDOI: http://dx.doi.org/10.31642/JoKMC/2018/100225  \nReceived Jan. 3, 2023. Accepted for publication Aug.17, 2023 \n Rasha Ali Dihin                        Ebtesam N. AlShemmary                        Waleed A. Mahmoud Al-Jawher \n \n168 \n \nmethods have achieved state -of-the-art performance in various \ntasks [5]. The drawback of ViT is that it requires pre-training on \nits large dataset [6]. Transformers have been widely used in \nnumerous vision problems, especially for visual recognition and \ndetection [7].  \nThe pioneering work of Swin Transformer [8] has two \noutstanding contributions that distinguish it from other works: \n(i) Presents a hierarchical feature representation scheme \nthat demonstrates impressive performances w ith linear \ncomputational complexity. These hierarchical features can make \nSwin Transformer suitable as a general backbone for kinds of \ncomputer vision tasks. \n(ii) Swin Transformer proposes a key design where shifted \nwindows are equipped between consecutive  attention layers, \nwhich can enhance modeling power while performing a \ncomputation-efficient strategy [8]. \nThis paper is structured into different sections. Section II \nprovides an overview of related works. In Section III, the \nproposed framework is explained in detail, covering the Wavelet \nand multi -Wavelet transform with Swin transformer. The \nexperimental results are presented and discussed in Section IV. \nFinally, the conclusions are summarized in Section V. \nII. RELATED WORKS \nThis section reviews the relevant literature and provides \nfundamental knowledge for the proposed method. \nGu. Yeonghyeon et al [9] introduced a model called \nSTHarDNet, which effectively combines Swin transformer \nblocks with a lightweight U -Net type architecture. The \nSTHarDNet model utilizes an encoder -decoder structure based \non HarDNet blocks and demonstrates remarkable performance \nin segmenting stroke MRI scan images. The incorporation of \nthe first layer Swin adapter enables the extraction of \nhierarchical features. STHarDNet exhibits a CNN -like nature, \nefficiently completing the task w hile also addressing the \nlimitations of specific regions. Due to its balanced trade -off \nbetween performance and speed, the proposed STHarDNet \nmodel has been recognized as the optimal choice. \nLiao. Zhihao .el.at [10] introduced the Swin-PANet model, \nwhich incorporates a window -based self -attention mechanism \nutilizing the Swin switch in an intermediate supervision \nnetwork. By leveraging the advantages of this switch, the \nproposed Swin PANet was applied in Computer Aided \nDiagnosis (CAD) for melanoma diagnosis, with the aim of \nenhancing segmentation accuracy. The model demonstrated \nsuperior performance compared to recent models; however, it \nstill encounters certain limitations in the context of transfer \nlearning. \nL. Jingyun et al [11] were proposed a robust baseline model \ncalled SwinIR for image restoration, leveraging th e power of \nthe Swin Transformer. SwinIR is composed of three main \ncomponents: shallow feature extraction, deep feature \nextraction, and human resource reconstruction units. The \nmodel, SwinIR, exhibited exceptional performance across \nvarious image recovery t asks and six different settings, \nestablishing its competence in all aspects of image restoration. \nIn their study,H. Siyuan et al [12] introduced a novel two-\nstream Swin transformer network (TSTNet) de signed \nspecifically for Remote Sensing (RS) image classification. \nEach stream of the network utilized the Swin transformer as its \nbackbone, which yielded impressive performance. Through \nexperiments conducted on three challenging and widely -used \ndatasets, it was demonstrated that TSTNet outperformed other \nstate-of-the-art models in terms of classification accuracy. \nThese findings highlight the effectiveness of TSTNet in RS \nimage classification tasks. \nIn their research, A. Hatamizadeh et al [13]  introduced the \nSwin UNITRansformers (Swin UNITR) model for the semantic \nstratification of 3D brain tumors. The input data was \nrepresented as a 1D sequence of embeddings, which served as \nthe input to the Swin transformer. The m odel exhibited \nexceptional performance during the validation and testing \nphases, outperforming other approaches and achieving the best \nresults. The Swin UNITR model demonstrates its effectiveness \nin accurately classifying and stratifying 3D brain tumors ba sed \non their semantic characteristics. \nIII. METHOD \nIn this section, a detailed introduction of the proposed method \nwill be provided, focusing on the method's components \ndepicted in Figure 1. Firstly, Section 3.1 will present an \noverview of the transform utilize d in the proposed approach, \nencompassing the multi -Wavelet transform. Subsequently, \nSection 3.2 will delve into the specifics of the Swin \nTransformer block. \n \nFig. 1. The proposed method. \n \nA. Preprocessing \nIn order to prepare the model for training, a series of \npreprocessing procedures are implemented on the input images. \n\nJournal of Kufa for Mathematics and Computer           Vol.10, No.2, Aug., 2023, pp 167-172 \n \n169 \n \nThese procedures encompass applying data augmentations, \nresizing the images, and auto cropping them.  \n  \nB. Feature Extraction  \nIn the feature  extraction stage, the incorporation of \nmultiwavelet transformation was implemented. Waves prove to \nbe highly valuable tools in signal processing applications, as \nthey fulfill various functions such as noise reduction and image \ncompression [14],[15],[16]. Traditionally, only scalar waves \nderived from a single measurement function were recognized \nand utilized. However, more recently, the identification of \nmultiple unit measurement functions has resulted in the \nemergence of multiple waves [17]. These multiple  waves \npossess several advantages over scalar waves, including \nsymmetry, fading moments, orthogonality, and short support \n[18]. When comparing these waves with scalar waves, the latter \nlack all of these properties simultaneously. \nAlso, the multi-wave system can provide perfect reconstruction \nand at the same time maintain the orthogonality as well as the \narrangement, have high approximation and good performance \nthrough linear length symmetry [19]. With all these advantages \nthat multi-wave conversion possesses, its performance exceeds \nthat of scalar wavelets in image processing applications [20]. \nMultiple waves in which each channel has an input of a vector \nvalue and the outputs also have a vector value. One of the \nimportant differences between it and scalar w aves is that the \ninput signal whose value is scalar must be converted into a \nsignal with an appropriate vector value. This conversion is \ncalled preprocessing [8]. \nSome reasons for potentially choosing multiwavelets can be \nsummarized as follows: \ni- The extra degrees of freedom inherent in multiwavelets can \nbe used to reduce restrictions on the filter properties.  \nii- The support length and the number of vanishing moments is \ndirectly linked to the filter length for scalar Wavelets.  \niii- Multiple waves are able to have the best properties at the \nsame time and this is the opposite of scalar waves. \niv- Multiple waves have good energy compression properties, \nas it relates the input signal to a small number of \nmeasurement parameters that contain most of th e energy, \nand it is considered one of the desirable properties in image \ncompression. \nv- Previous literature has shown promising results in the \napplication of multiwavelets to image compression.  \nvi- Finally, the issue of computational complexity is effecti ve \neven though each branch of a multi -wave filter contains 2 -\ninput and 2 -output filters and two channels. However, the \nfilters in a symmetric multi -filter have the same kind of \nsymmetry, the Figure 2 show Modify multiwavelet. \n \nFig 2. Modified multiwavelet. \n \nIn the subsequent feature extraction stage, the multi -Wavelet \ntransform is employed. This transformation generates four \ncopies of the image, namely LL, LH, HL, and HH, each \ncontaining four subparts. However, the HH subpart, which \ncontains unwanted details, is disregarded. The horizontal and \nvertical details are combined, and the resulting output is then \nsubjected to a SoftMax function. This output is further \nmultiplied with the approximation (LL) copy and combined \nwith the approximation (LL) copy, resulting in enhanced image \ndetails for clearer visualization. \n \nC. The Shifted Window Transformer (ST) \nAfter the pre -processing procedure, the Shifted Window \nTransformer or Swin Transformer (ST) was utilized  to create \nhierarchical feature maps of the image. This involved \ncombining patches into deeper layers.  The shifted windowing \nscheme brings greater efficiency by limiting self -attention \ncomputation to non -overlapping local windows as well as \nallowing cross -window connection. It has a linear \ncomputational complexity proportional to the size of the input \nimage due to self -attention processing occurring only within \neach local window. Thus, it generates feature maps with a single \nlow resolution in comparison wit h earlier vision transformers. \nAs a result, it can be used as a general -purpose backbone for \nimage and texture classification and speech processing \napplications. There are many Challenges in Vision Applications \nusing the Shifted Window Transformer. Transfo rmer-based \nmodels in which all tokens are fixed in size, and thus are not \nsuitable for computer vision applications, and since the pixels \nin the image have very high accuracy, and this is another \ndifference between images and text segments, and images need \na dense prediction at the pixel level, and to overcome all these \nchallenges presented Microsoft's research team in Asia is the \nSwin converter, which is the backbone of computer vision \napplications and its computational complexity is linear. \n \nArchitecture \nThe basic design of ST is a transformation consisting of several \nsuccessive layers of subjective attention which are connected to \nthe last layer, thus increasing the modeling power. This \ntechnique is very effective in terms of computational \ncomplexity and latency in the real world.[21]. Figure 2 \nillustrates the architecture in its smallest configuration. Initially, \nthe input RGB image is divided into non -overlapping patches \nusing the ViT splitter. \n\n Rasha Ali Dihin                        Ebtesam N. AlShemmary                        Waleed A. Mahmoud Al-Jawher \n \n170 \n \nThe input image is passed through the patch partion layer, a nd \nthe image is divided into 4√ó4 patches, thus creating patch codes \n(W/4 channel, H/4 and 4√ó4).These generated tokens pass \nthrough the linear modulation stage in the first stage, after \nwhich they are fed through two Swin switches in which the \ntokens are of  size (W/4, H/4, C), the third and fourth stages \nconsist of (patch merging ) and (Swin block) Since the 2, 3 and \n4 are phases, the symbols are (W/8, H/8, 2C), (W/16, H/16, 4C), \nand (W/32, H/32, 8C) respectively. \nSwin Transformer can be implemented by repla cing the \nstandard multi -head self -attention module in a transformer \nblock by using shifted windows keeping the other layers the \nsame. Swin Transformer block consists of a shifted window \nbased on multi -head self -attention module. Usually, it will be \nfollowed by a 2 -layer multi -layer perceptron network with \nGaussian Error Linear Unit nonlinearity in between. Next, \nLayer normalization is applied before each multi -head self -\nattention module and each multi -layer perceptron. Finally, a \nresidual connection is appl ied after each of the above modules \nmentioned before. \n    ST is created by replacing (MSA) with (SW -MSA) with the \nrest of the layers left unchanged, so that each block of ST \ncontains (SW-MSA) and (MLP) and (LN) nonlinear window -\nbased self -attention module lacks connections through \nwindows, which limits its modeling ability. Equations (2 -5) \nprovide the mathematical expression for W -MSA and SW -\nMSA as follows [22]. \n \nFig 3. Swin transformer block. \n \nùëßÃÇùëô = ùëä ‚àíùëÄùëÜùê¥(ùêøùëÅ(ùëçùëô‚àí1))+ùëçùëô‚àí1      (2) \nùëßùëô = ùëÄùêøùëÉ(ùêøùëÅ(ùëçùëô‚àí1))+ùëßÃÇùëô                    (3) \nùëßÃÇùëô+1 = ùëÜùëä ‚àíùëÄùëÜùê¥(ùêøùëÅ(ùëßùëô))+ùëßùëô         (4) \n \nWhere (z ^ l) is the item in the current block, (zl ‚Äì 1) is the item \nin the previous block, (LN) is layer-norm, (MLP) is multi-layer \nperceptron, (W1 -1MSA) is window self -attention, and (SW1 -\n1MSA) is shift window self-attention [13]. \n \nIV. RESULT \n \nThe experimental data used in this study was introduced, and \nthe performance of the proposed approach was evaluated. \n \nA. Datasets \nThe results obtained from the APTOS 2019 Kaggle benchmark \ndataset are presented. The dataset comprises retina images \ncaptured using fundus imaging, with a wide range of imaging \nconditions. The challenge involved detecting blindness based \non these images. The dataset has been carefully categorized into \nfive classes (0 to 4) by domain specialists. Each class represents \na different severity level of DR: \"0\" indicates no DR, \"1\" \nrepresents mild, \"2\" stands for moderate, \"3\" signifies severe, \nand \"4\" indicates proliferative DR [23], [24]. \n \nB. Swin transformer implementation \nFirstly, the performances of various transforms, such as the \nmulti-Wavelet transform and Swin Transformer, are compared \nin the following sections. \n \ni. Swin transformer with multi-Wavelet for binary-class:  \nThe use of Swin -T with multi -wave for feature extraction is a \npromising approach for image classification tasks where in this \ncase take the batch size=64 based on the train time (3111.27s) \nis less other batch size with the same accuracy in the test and \nbest validation accuracy of 0.9891 with 1 00 Epoch show in \nFigure 4. \nFig 4: Training and validation over epoch for APTOS 2019 dataset, (a)loss, \n(b) accuracy, (epochs=100), multi-Wavelet transform to binary class. \n \nThe results show that the Swin -T model with multi - Wavelet \ntransformation for  feature extraction achieved an overall test \naccuracy of 97% and an average F1 score of 0.9873 for binary \nclassification of No -DR and DR cases. The model performed \nbetter in detecting the No -DR cases, achieving a sensitivity of \n0.97 and specificity of 0.98 67, while for DR cases, it achieved \na sensitivity of 0.9798 and specificity of 0.96 shows in Figure5 \nand Table1 shows the testing the swin -T with multi - Wavelet \ntransformation for binary class. \n \n \n \n\nJournal of Kufa for Mathematics and Computer           Vol.10, No.2, Aug., 2023, pp 167-172 \n \n171 \n \n \n \nFig 5: confusion matrix of Swin-T with multi-Wavelet transform for binary \nclass.  \n \nTABLE 1: TESTING FOR SWIN-T MULTI-WAVELET FOR \nBINARY CLASS  \nClass Test \nAccuracy \nTest \nloss Sensitivity specificity F1 \nScore \nNo-DR 98%  \n0.0328 \n0.9798 0.9700 0.9867 \nDR 96% 0.9700 0.9798 0.9748 \nAverage 97% 0.9520 0.9520 0.9873 \n \n \nii. Swin transformer with multi-Wavelet for multi-class:  \n \nThe results show that using Swin -T transformer with multi -\nwavelet transformation for feature extraction leads to relatively \nhigh accuracy rates ranging from 81% to 82%.  as the highest \naccuracy was achieved with a batch size of 16. \nThe Figure 6 indicate the fluctuations in accuracy and loss \nvalues of the training and validation sets during a 100-epoch. \nBy using Swin -T transformer with multi -wavelet \ntransformation for feature extraction, the DR multi -\nclassification achieved an average testing accuracy of 82%, \nwith a testing loss of 0.4626, sensitivity of 0.8017, and \nspecificity of 0.9022. \n \n \n \n \n \nFig 8: Training and validation over epoch for APTOS 2019 dataset, (a) loss, \n(b) accuracy, (epochs=100), multi-Wavelet transform to multi-class. \n \nThe confusion matrix shows in Figure 9 the performance of the \nSwin-T transformer with multi -wavelet transformation for \nfeature extraction model on the DR multi -classification task.  \nTable 2 shows the te sting the swin -T with multi - Wavelet \ntransformation for multi class. \n \n \nFig 9: confusion matrix of Swin-T with multi-Wavelet transform for multi- \nclass. \n \n \n \n \nTABLE 2: TESTING FOR SWIN-T MULTI-WAVELET FOR \nMULTI- CLASS \nClass Test \nAccuracy \nTest \nloss Sensitivity specificity F1 \nScore \nDR0 97% 0.1233 0.9798 0.9580 0.9829 \nDR1 41% 0.4 0.9613 0.4878 \nDR2 85% 0.8620 0.8530 0.8904 \nDR3 33% 0.2941 0.9799 0.4000 \nDR4 33% 0.3030 0.9969 0.4000 \nAverage 82% 0.4626 0.8017 0.9022 \n \n \n \n\n Rasha Ali Dihin                        Ebtesam N. AlShemmary                        Waleed A. Mahmoud Al-Jawher \n \n172 \n \nV. CONCULSION \nIn this work, the multi-Wavelet transform with Swin -T is presented for \nDR classification. The performance of Swin -T with multi -Wavelet was \nevaluated through experiments. The Swin transform was applied to the \nAPTOS 2019 Kaggle dataset in this study.  In conclusion, the pr oposed \nthat used multi -Wavelet transform with Swin -T achieves better \nperformance for DR binary classification, where the accuracy 0.9778% \nfor training and the loss is 0.2537% and the accuracy is 0.9754for \nvalidation and the best accuracy is 0.9809%, and the test accuracy is 97% \nfor binary class while  used the multi - Wavelet to classification DR to \nmulti class is the loss is 0.5901%  and the accuracy is 0.9160%  for \ntraining and the loss is 0.8 250%  and the accuracy is 0.8 242%  for \nvalidation and the test accuracy is 82%. The integration of the Swin \nTransformer with multi -wavelet analysis presents a promising direction \nfor the classification of diabetic retinopathy. By exploring the \naforementioned future research directions, such as augmentation \ntechniques, fusion strategies, transfer learning, handling class imbalance, \ninterpretability, and real-world evaluations, we can enhance the accuracy, \nrobustness, and clinical relevance of the proposed method. These \nadvancements will contribute to the development of e ffective tools for \nautomated DR diagnosis and aid in the early detection and prevention of \nvision loss in diabetic patients. \n \nREFERENCES \n[1] G. Eason, B. Noble, and I.N. Sneddon, ‚ÄúOn certain \nintegrals of Lipschitz -Hankel type involving products of \nBessel functions,‚Äù Phil. Trans. Roy. Soc. London, vol. \nA247, pp. 529-551, April 1955. (references) \n[2] A. Grzybowski, P. Brona, G. Lim, and P. Ruamviboonsuk, ‚ÄúArtificial \nintelligence for diabetic retinopathy screening: a review,‚Äù Springer Nat., \n2020. \n[3] S. Natarajan, A. Jain, R. Krishnan, and A. Rogye, ‚ÄúDiagnostic Accuracy \nof Community-Based Diabetic Retinopathy Screening With an Offline \nArtificial Intelligence System on a Smartphone,‚Äù JAMAOphthalmology, \n2019. \n[4] M. Wintergerst, D. Mishra, L. Hartmann, and F. Holz, ‚ÄúDiabeti c \nRetinopathy Screening Using Smartphone -Based Fundus Imaging in \nIndia,‚Äù Am. Acad. Ophthalmol., vol. 127, no. 0161-6420/20, 2020. \n[5] H. Wang, P. Cao, J. Wang, and O. R. Zaiane, ‚ÄúUCTransNet: Rethinking \nthe Skip Connections in U -Net from a Channel -wise Perspect ive with \nTransformer,‚Äù arXiv Prepr. arXiv2105.05537, 2021, [Online]. Available: \nhttp://arxiv.org/abs/2109.04335. \n[6] W. Wang, E. Xie, X. Li, and D.-P. Fan, ‚ÄúPyramid Vision Transformer: A \nVersatile Backbone for Dense Prediction without Convolutions \narXiv:2102.12122v2,‚Äù arXiv:2102.12122v2 [cs.CV], 2021. \n[7] H. Song, D. Sun, S. Chun, and V. Jampani, ‚ÄúAn Extendable, Efficient and \nEffective Transformer -based Object Detector,‚Äù arXiv:2204.07962v1, \n2022. \n[8] L. Wang, R. Li, C. Duan, C. Zhang, and X. Meng, ‚ÄúA Novel Transformer \nbased Semantic Segmentation Scheme for Fine -Resolution Remote \nSensing Images,‚Äù Geosci. Remote Sens. Lett., 2021. \n[9] Y. Gu, Z. Piao, and S. J. Yoo, ‚ÄúSTHarDNet: Swin Transformer with \nHarDNet for MRI Segmentation,‚Äù Appl. Sci., 2022. \n[10] Z. Liao, N. Fan, and K. Xu, ‚Äú Swin Transformer Assisted Prior Attention \nNetwork for Medical Image Segmentation,‚Äù Appl. Sci., 2022. \n[11] J. Liang, J. Cao, G. Sun, and K. Zhang, ‚ÄúSwinIR: Image Restoration Using \nSwin Transformer,‚Äù arXiv:2108.10257v1, 2021. \n[12] S. Hao, B. Wu, K. Zhao, and Y. Ye, ‚ÄúTwo-Stream Swin Transformer with \nDifferentiable Sobel Operator for Remote Sensing Image Classificatio,‚Äù \nRemote Sens., 2022. \n[13] A. Hatamizadeh, V. Nath, Y. Tang, D. Yang, H. Roth, and D. Xu, ‚ÄúSwin \nUNETR: Swin Transformers for Semantic Segmentation of Brain Tumors \nin MRI Images.‚Äù 2022, [Online]. Available: \nhttp://arxiv.org/abs/2201.01266. \n[14] A. HM Al -Helali, H. Ali, B. Al -Dulaimi, D. Alzubaydi, and W. \nMahmoud, A, ‚ÄúSlantlet transform for multispectral image fusion,‚Äù J. \nComput. Sci., vol. 5, no. 4, p. PP. 263-267, 2009. \n[15] A. Al-Helali, W. A. Mahmoud, and H. Ali, ‚ÄúA Fast personal palm print \nauthentication Based on 3d-multi Wavelet Transformation,‚Äù Transnatl. J. \nSci. Technol., vol. 2, no. 8, 2012. \n[16] H. Al-Taai, W. A. Mahmoud, and M. Abdulwahab, ‚ÄúNew fast method for \ncomputing multiWavelet coefficients from 1D up to 3D,‚Äù Proc. 1st Int. \nConf. Digit. Comm. Comp. App., Jordan, no. PP. 412-422, 2007. \n[17] A. H. Kattoush and W. Ameen Mahmoud Al -Jawher, ‚ÄúA radon -\nmultiWavelet based OFDM system design and simulation under different \nchannel conditions‚Äù Journal of Wireless personal communications,‚Äù J. \nWirel. Pers. Commun., vol. 71, 2017 \n[18] W. A. Mahmoud Al -Jawher and T. Abbas, ‚ÄúFeature combination and \nmapping using multiWavelet Transform,‚Äù IASJ, AL-Rafidain, 2005. \n[19] W. A. Mahmoud Al-Jawher, ‚ÄúA Smart Single Matrix Realization of Fast \nWalidlet Transform,‚Äù Int. J. Res. Rev., vol. 2, no. 2, pp. 144‚Äì150, 2011. \n[20] W. A. Mahmoud, A. S. Hadi, and T. M. Jawad, ‚ÄúDevelopment of a 2 -D \nWavelet Transform based on Kronecker Product,‚Äù Al-Nahrain J. Sci., vol. \n15, no. 4, pp. 208‚Äì213, 2012. \n[21] J. Ahn, J. Hong, J. Ju, and H. Jung, ‚ÄúRethinking Query, Key, and Value \nEmbedding in Vision Transformer under Tiny Model Constraints,‚Äù  \narXiv:2111.10017v1, 2021, [Online]. Available: \nhttp://arxiv.org/abs/2111.10017. \n[22] E. In, ‚ÄúLmsa: Low -Relation Mutil-Head Self- Attention Mechanism in \nVisual Transformer,‚Äù pp. 1‚Äì11, 2022. \n[23] B. Tymchenko, P. Marchenko, and D. Spodarets, ‚ÄúDeep Learning \nApproach to Diabetic Retinopathy Detection Borys,‚Äù \narXiv:2003.02261v1, 2020. \n[24] J. D. Bodapati et al., ‚ÄúBlended m ulti-modal deep convnet features for \ndiabetic retinopathy severity prediction,‚Äù Electron., vol. 9, no. 6, 2020, \ndoi: 10.3390/electronics9060914 \n \n \n \n \n \n \n \n \n \n \n ",
  "topic": "Wavelet",
  "concepts": [
    {
      "name": "Wavelet",
      "score": 0.6740487813949585
    },
    {
      "name": "Computer science",
      "score": 0.6727181673049927
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6721057891845703
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5901131629943848
    },
    {
      "name": "Wavelet transform",
      "score": 0.548800528049469
    },
    {
      "name": "Diabetic retinopathy",
      "score": 0.48220717906951904
    },
    {
      "name": "Feature extraction",
      "score": 0.45474904775619507
    },
    {
      "name": "Discrete wavelet transform",
      "score": 0.44257909059524536
    },
    {
      "name": "Binary classification",
      "score": 0.42852041125297546
    },
    {
      "name": "Transformer",
      "score": 0.41839611530303955
    },
    {
      "name": "Machine learning",
      "score": 0.4159245789051056
    },
    {
      "name": "Support vector machine",
      "score": 0.1382444202899933
    },
    {
      "name": "Diabetes mellitus",
      "score": 0.12277582287788391
    },
    {
      "name": "Medicine",
      "score": 0.10415983200073242
    },
    {
      "name": "Engineering",
      "score": 0.08770176768302917
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Endocrinology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I47229656",
      "name": "University of Kufa",
      "country": "IQ"
    }
  ]
}