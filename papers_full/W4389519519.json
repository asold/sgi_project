{
  "title": "Language Models with Rationality",
  "url": "https://openalex.org/W4389519519",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2027506410",
      "name": "Nora Kassner",
      "affiliations": [
        "Ludwig-Maximilians-Universität München",
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A662704055",
      "name": "Oyvind Tafjord",
      "affiliations": [
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A1898665253",
      "name": "Ashish Sabharwal",
      "affiliations": [
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2122050085",
      "name": "Kyle Richardson",
      "affiliations": [
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2111997136",
      "name": "Hinrich Schuetze",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1912941100",
      "name": "Peter Clark",
      "affiliations": [
        "Allen Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2891308403",
    "https://openalex.org/W4385573045",
    "https://openalex.org/W3021533447",
    "https://openalex.org/W4297166568",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W2970161131",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W4296415442",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3110879614",
    "https://openalex.org/W4230262515",
    "https://openalex.org/W4385574359",
    "https://openalex.org/W4385572727",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2971068072",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W1581516506",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4287324238",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W2150859678",
    "https://openalex.org/W4205870266",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W4288244092",
    "https://openalex.org/W3202712981",
    "https://openalex.org/W2154593562",
    "https://openalex.org/W4385572965",
    "https://openalex.org/W2995373617",
    "https://openalex.org/W3034873522"
  ],
  "abstract": "While large language models (LLMs) are proficient at question-answering (QA), it is not always clear how (or even if) an answer follows from their latent \"beliefs\". This lack of interpretability is a growing impediment to widespread use of LLMs. To address this, our goals are to make model beliefs and their inferential relationships explicit, and to resolve inconsistencies that may exist, so that answers are supported by interpretable chains of reasoning drawn from a consistent network of beliefs. Our approach, which we call REFLEX, is to add a **rational, self-reflecting layer** on top of the LLM. First, given a question, we construct a **belief graph** using a backward-chaining process to materialize relevant model beliefs (including beliefs about answer candidates) and their inferential relationships. Second, we identify and minimize contradictions in that graph using a formal constraint reasoner. We find that REFLEX significantly improves consistency (by 8%-11% absolute) without harming overall answer accuracy, resulting in answers supported by faithful chains of reasoning drawn from a more consistent belief system. This suggests a new style of system architecture in which an LLM extended with a rational layer can provide an interpretable window into system beliefs, add a systematic reasoning capability, and repair latent inconsistencies present in the LLM.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 14190–14201\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLanguage Models with Rationality\nNora Kassner1,2 Oyvind Tafjord1 Ashish Sabharwal1 Kyle Richardson1\nHinrich Schütze2 Peter Clark1\n1Allen Institute for AI, Seattle, W A\n2Center for Information and Language Processing, LMU Munich, Germany\nkassner@cis.lmu.de\n{oyvindt,ashishs,kyler,peterc}@allenai.org\nAbstract\nWhile large language models (LLMs) are pro-\nficient at question-answering (QA), it is not\nalways clear how (or even if) an answer fol-\nlows from their latent “beliefs”. This lack\nof interpretability is a growing impediment to\nwidespread use of LLMs. To address this, our\ngoals are to make model beliefs and their in-\nferential relationships explicit, and to resolve\ninconsistencies that may exist, so that answers\nare supported by interpretable chains of reason-\ning drawn from a consistent network of beliefs.\nOur approach, which we call REFLEX , is to add\na rational, self-reflecting layer on top of the\nLLM. First, given a question, we construct abe-\nlief graph using a backward-chaining process\nto materialize relevant model beliefs (includ-\ning beliefs about answer candidates) and their\ninferential relationships. Second, we identify\nand minimize contradictions in that graph us-\ning a formal constraint reasoner. We find that\nREFLEX significantly improves consistency (by\n8%-11% absolute) without harming overall an-\nswer accuracy, resulting in answers supported\nby faithful chains of reasoning drawn from a\nmore consistent belief system. This suggests\na new style of system architecture in which an\nLLM extended with a rational layer can provide\nan interpretable window into system beliefs,\nadd a systematic reasoning capability, and re-\npair latent inconsistencies present in the LLM.\n1 Introduction\nWhile large language models (LLMs) are impres-\nsive at question-answering (QA), it is not always\nclear how (or even if) an answer follows from their\nlatent “beliefs”1 about the world, or whether the\nLLM even has a coherent internal belief system.\nThis general opacity is a growing impediment to\nwidespread use of LLMs, e.g., in critical applica-\ntions such as medicine, law, and hiring decisions,\n1 We adopt a simple definition of belief, namely that a\nmodel believes X if it answers \"yes\" to the question \"Is X\ntrue?\". Other definitions could also be used; see Section 2.\nFigure 1: (Top) When queried about each answer option\nindependently, the model incorrectly believes both are\ntrue, and is more confident in the wrong answer ( S2).\n(Bottom) REFLEX adds a \"rational\" layer above the\nLLM layer, in which a belief graph is constructed (by\niteratively querying the LLM, up/down arrows), contain-\ning relevant model-believed facts (white/grey = believed\nT/F) and their inferential relationships. Inconsistencies\nare then identified (red) and minimized by a constraint\nreasoner that flips T/F labels on beliefs (green ✓/X),\nhere resulting in the correct answer ( S1, green box) +\nexplanation (graph) by the overall system (blue).\nwhere properties of explainability, interpretability,\nand trust are paramount. Our goal is to help alle-\nviate such opacity by constructing an explicit rep-\nresentation of system beliefs and their inferential\nrelationships (including to answer candidates), so\nthat answers are supported by interpretable chains\nof reasoning. These constructed belief graphs ,\ne.g., Figures 1 and 2, form a rational layer above\nthe LLM explaining how answers follow from be-\nliefs, and provide a window into some of the latent\ncontents of the model, potentially helping users\n14190\nunderstand and trust model answers.\nIn addition, when we do this, we find such graphs\nexpose latent inconsistencies in the model’s beliefs.\nWe show how such inconsistencies can be resolved\nusing constraint satisfaction techniques. When we\ndo this, the rational layer becomes not just a win-\ndow onto the model, but an active reasoning com-\nponent in its own right in a larger, overall system,\ncomprising the (frozen) LLM plus rational layer\n(blue box, Figure 1). We show this results in a\nmore consistent set of beliefs in the overall system,\nwithout harming overall answer accuracy (although\nsome individual answers may change). The result\nis answers supported by faithful, system-believed\nchains of reasoning drawn from a consistent belief\nsystem.\nOur approach, called REFLEX , introduces a ra-\ntional layer consisting of two parts. First, to pro-\nduce a belief graph, we recursively ask the LLM\nto explain why each candidate answer might be\ntrue, expressed as a set of sentences that entail\nthe answer. This builds on earlier work on generat-\ning entailment-based and chain-of-thought explana-\ntions (Tafjord et al., 2022; Weir and Durme, 2022;\nWei et al., 2022). We then add a self-verification\nstep to check that the model itself believes those\ngenerations (i.e., that the model believes what it\nsays), allowing us to identify sentences reflecting\nthe model’s own internal knowledge. For example,\nin Figure 1, when asked to explain S1 (“giraffes\ngive live birth”), the model generates S7 ([because]\n“mammals give live birth”) and S4 ([and] “a giraffe\nis a mammal”). Self-querying then checks if the\nmodel actually believes its generations (“Do mam-\nmals give live birth?”). The answer (“yes”/”no”)\nassigns a true/false (T/F) value to each generation,\nindicated in Figure 1 by white/grey nodes. This\nprocedure is then applied recursively to the gener-\nated, supporting sentences. The resulting network\nof model beliefs and their dependencies provides a\na window into the model.\nSecond, we apply a formal constraint reasoner\nto this graph to resolve inconsistencies, by find-\ning the optimal (minimal cost, Section 3.3) way\nof flipping T/F values. For example, on the left in\nFigure 1, S2 and S3 (“spiders do/don’t give live\nbirth”) are in an XOR relationship (i.e., exactly\none must be false), but both are believed as true\n(white) by the LLM - a latent contradiction within\nthe LLM. Constraint reasoning then seeks to re-\nmove such inconsistencies, here flipping the belief\nvalue on S2 from T to F (Figure 1, right), repairing\nthe contradiction. This builds on earlier techniques\n(Kassner et al., 2021; Mitchell et al., 2022; Jung\net al., 2022), though in a notably richer setting with\nover 350 nodes and 80 constraints per question,\njoint inference across answer candidates, and a va-\nriety of constraint types. The overall result is a fully\nautonomous, self-reflective system that is able to\ndeliberate (and if necessary change) its answers,\nthereby resolving latent inconsistencies that would\notherwise go unnoticed, and provide faithful expla-\nnations drawn from a consistent belief system.\nWe evaluate our implementation of REFLEX\non three datasets: EntailmentBank (Dalvi et al.,\n2021), OBQA (Mihaylov et al., 2018), and QuaRTz\n(Tafjord et al., 2019). We find that REFLEX is\nable to construct belief graphs with significantly\nimproved consistency (by 8%-11% absolute) with-\nout harming overall answer accuracy. In addition,\nanswers are now supported by a more consistent,\nsystem-believed chain of reasoning, providing a\nwindow into the previously latent beliefs of the\nmodel. Our contributions are thus:\n1. A new style of system architecture in which\nan LLM is extended with a rational layer\nin which an explicit representation of system\nbeliefs and relationships is constructed and\nwhich can be reasoned over. This layer pro-\nvides an interpretable window into system\nbeliefs, adds a systematic reasoning capablity,\nand allows latent inconsistencies present in\nthe LLM to be repaired.\n2. An implementation of this architecture demon-\nstrating that the consistency of the overall\nsystem’s network of beliefs can be signif-\nicantly improved without harming answer\naccuracy. Answers are now supported by ex-\nplicit, interpretable chains of reasoning drawn\nfrom a more consistent network of beliefs.\n2 Related Work\nMaterializing a Model’s Internal Knowledge: It\nis now well recognized that LLMs contain exten-\nsive world knowledge (Petroni et al., 2019, 2020;\nDavison et al., 2019; Peters et al., 2019; Jiang et al.,\n2020; Roberts et al., 2020) that somehow enables\nthem to perform well. Recent work has attempted\nto expose that knowledge in various ways, both\nto justify answers and improve performance, and\nour work falls into this genre. Standard explana-\ntion generation methods (Wiegreffe and Marasovi´c,\n2021) can produce compelling explanations, but\n2\n14191\nFigure 2: Given a question, each answer choice is first converted to a hypothesis statement (A). The belief graph\nis then constructed in stages, first generating rules that conclude the hypotheses ( B), then backward-chaining to\ngenerate rules concluding the premises of those first rules, etc., and adding in negated versions of graph statements\nconnected with the originals via XOR links (e.g., nodes 11 and 12), until the stopping criterion is met (C). Statements\nare then labeled with the model’s belief in them (true/false), found via self-querying (white = believed true, grey =\nbelieved false). Finally, logical conflicts are identified (colored red), and constraint satisfaction techniques are used\nto resolve them. In this case, as there is strong evidence that node 2 is actually true (7 & 6 →2, not(19) →2), the\nsolver finds that the minimum cost repair is to flip node 2’s label from FALSE to TRUE. Here, node 2 ends up being\nselected as the final answer, thus correctly answering the original question.\nwith no guarantee that the generated sequence of\ntokens expresses the model’s internal knowledge,\nnor entails the actual answer. Similarly, chain-of-\nthought (CoT) (Wei et al., 2022) and Least-to-Most\n(Zhou et al., 2023) prompting generate (in different\nways) a step-by-step reasoning chain along with\nan answer, but again with no claim that the chain\nreflects the model’s internal knowledge nor is valid\nreasoning (Subramanian et al., 2020).\nTo add semantics to generations, several systems\nhave used self-querying to verify that generations\nreflect model-believed facts (by self-querying “Is\np true?”) (e.g., Kassner et al., 2021; Jung et al.,\n2022), or model-believed rules (by self-querying\n“Does p imply q?”) (e.g., Tafjord et al., 2022). We\nbuild on these to construct a belief graph, namely\na network of model-believed facts and their infer-\nential relationships, which can then be reflected\non.\nBeliefs: We refer to the model’s factual opin-\nions as “beliefs” rather than “knowledge” because\nthose opinions may be wrong. In general, an agent\n3\n14192\ncan be said to believe p if it acts as if p was true\n(Schwitzgebel, 2019). Following Kassner et al.\n(2021) and Richardson et al. (2022), we take a sim-\nple, syntactic operationalization of this, namely the\nagent answers “yes” to the question “p?”, but also\nnote that more semantic versions could be used,\ne.g., the agent also answers “yes” to paraphrases\nand implications of p.\nReducing Inconsistency: LLMs are known to\nbe inconsistent in their answers (Ettinger, 2020;\nKassner and Schütze, 2020; Davison et al., 2019;\nRavichander et al., 2020; Elazar et al., 2021; Sub-\nramanian et al., 2020; Gu et al., 2023), and several\nrecent works have used constraint reasoners to iden-\ntify and reduce inconsistency. BeliefBank used a\nMaxSAT solver to resolve inconsistencies between\nmodel beliefs, but required a hand-provided set of\nconstraint rules (Kassner et al., 2021). ConCoRD\n(Mitchell et al., 2022) similarly used MaxSAT to\nensure model answers were consistent with NLI-\nderived entailment constraints between them, but\ndid not introduce additional model-believed facts\nand rules. Maieutic Prompting (Jung et al., 2022)\nalso used MaxSAT to resolve inconsistencies be-\ntween facts in prompt-induced explanation chains.\nHowever, those chains were not validated as re-\nflecting model-believed constraint rules2, and did\nnot support conjunction. REFLEX extends these\nreasoning chains to provide a full semantic account\nof how answers are supported by the model’s in-\nternal knowledge. Additionally, it performs joint\nreasoning across answer candidates and operates\nat a much larger scale (e.g., over 350 nodes on\naverage for each question) and with a variety of\nconstraint types.\n3 R EFLEX : Our Approach\n3.1 Belief Graphs\nOur belief graphs are defined over a set of natu-\nral language true/false statements and represent a\nset of rules that constrain the truth values of these\nstatements. We refer to statements that are fac-\ntually true in the world as facts. The truth value\nassigned by a model M to a statement is referred\nto as M’s belief in that statement (cf. Footnote 1).\nA model’s internal beliefs may not always align\n2REFLEX checks whether both the statements si, and the\nrules (si → h), are believed by the model via self-querying,\ne.g., by asking “Does si → h?”, and also scores the strength\nof those beliefs. In maieutic prompting, the generated rules\nare not checked against the model, resulting in rules that the\nmodel itself may not believe, if queried about them.\nwith facts. Our goal is to extract a model’s ini-\ntial beliefs about statements inferentially related\nto all top-level hypotheses of interest, and perform\nreasoning to update these beliefs so as to make\nthem more consistent with respect to the rules, and\nideally also factually more accurate.\nA belief graph is a type of factor graph com-\nmonly used in the probabilistic inference litera-\nture (Loeliger, 2004). Formally, it is defined as\nan undirected graph G = (N,E) with nodes N\nand edges E. Nodes are of two types: A state-\nment node (referred to as a \"variable node\" in\na factor graph) is a triple (s,l,c s) containing a\nnatural language statement s, an associated value\nl ∈{T,F }initially denoting M’s belief that sis\ntrue or false, and a confidence cs ∈[0,1] denoting\na confidence in that label. A rule node(referred\nto as a \"factor node\" in a factor graph) is a pair\n(r,cr) denoting a disjunctive rule or constraint over\nstatements, with confidence cr. It takes the form\nr = (−s1 ∨... ∨−sℓ ∨sℓ+1 ∨... ∨sk). For\nease of interpretation, we view this constraint as\nr= p→hwhere p= s1 ∧... ∧sℓ is a conjunctive\npremise and h = sℓ+1 ∨... ∨sk is a disjunctive\nhypothesis. The rule says that if pis true, so must\nbe h; and the contrapositive of this.\nEdges E connect rule nodes to the statements\nthey constrain, denoting their dependence. For leg-\nibility, we draw edges directionally to depict the\nway the rule reads: the statements in ppoint to r,\nwhich in turn points toh. Mathematically, the influ-\nence is bidirectional and the depicted directionality\nis irrelevant during reasoning (Section 3.3), just as\nin a standard factor graph.\nWe adopt the standard probabilistic semantics\nof factor graphs, thereby associating a belief graph\nwith a well-defined probability distribution over\nany set of statement beliefs. For a statement node\n(s,l,c s), the cost costs for setting it to lis 0, and\nthat for setting it against lis cs; the corresponding\nweight of this node is ws = exp(−costs). Costs\nand weights for a rule node (r,cr) are defined\nsimilarly, based on whether the beliefs satisfy ror\nnot. Finally, the overall weight of a T/F assign-\nment to all statements is ∏\ns ws ·∏\nr wr, which,\nwhen normalized by the total weight across all pos-\nsible assignments, yields a probability distribution\nover such assignments. We will be interested in\nfinding the most consistent set of beliefs, i.e., a\nT/F assignment to statements with the minimum\noverall weight, which is equivalent to minimizing\n4\n14193\n∑\ns costs + ∑\nr costr. This is referred to as the\nMPE (most probable explanation) problem in the\ngraphical models literature, which we later solve\nexactly using a MaxSAT constraint solver based\non a standard translation of MPE into weighted\nMaxSAT (Park, 2002; Sang et al., 2007).\n3.2 Constructing Belief Graphs\nGiven an initial node (statement) s, a belief graph\nGis produced by a backward-chaining process de-\nscribed below, in which Gis recursively expanded\nto add statements that together may entail s.\n3.2.1 Basic Operations\nLet hdenote a hypothesis (language statement s)\nof interest and p a premise—a set of statements\n{s1,. . . ,sn} that together may entail h. Given these,\nthere are three basic operations required to gener-\nate belief graphs:\n1. h⇒p: Given h, generate a pthat may entail h.\n2. s⇒(l,cs): Given a statement s, output a\ntrue/false value land a confidence in the belief\nthat shas truth value l(as assessed via yes/no\nquestion-answering).\n3. (p,h) ⇒cr: Given pand h, output a confidence\nthat the candidate rule r= p→hholds.\nThe most important of these is the first operation,\nin which the model self-generates conjunctive rules\nconcluding h(i.e., reason pfor believing h), thus\nadding new nodes to the graph.\nThere are several ways of implementing these\nbasic functions, and our algorithm is agnostic to\nthe method used. In our work here, we use Entailer,\nan off-the-shelf T5-11B trained model with these\nfunctionalities (Tafjord et al., 2022). Further, since\nthe raw score produced by the model tends to be\nskewed towards 0 or 1, when computing cs and cr\nin practice, we re-scale the raw model score using\na set of hyperparameters (cf. Appendix B).\nOne may use alternative ways to implement\nthese operators, such as chain-of-thought prompt-\ning a model like GPT3 (Wei et al., 2022) or Chat-\nGPT (OpenAI, 2022). For example, to generate\na rule concluding a hypothesis hsuch as “Plants\nrequire CO2 to make their food.”, the model could\nbe prompted with hfollowed by “Explain the last\nstatement with a 2-step reasoning chain.”, the num-\nbered generations forming the premise p. Similarly,\ngenerated statements and rules can be validated as\nreflecting the model’s beliefs by self-querying (“Is\nstrue?”, “Does pimply h?”), and then using the\ngenerated yes/no answer token probabilities as the\nAlgorithm 1 The recursive algorithm for construct-\ning a belief graph of max depth dmax for a hypoth-\nesis set H. The subroutine EXTEND -GRAPH takes\na partial graph Gas an input and extends it in place\nwith one statement and its subgraph.\n1: procedure GENERATE -GRAPH (hypotheses H, max\ndepth dmax):\n2: let G = empty graph\n3: foreach h ∈ H\n4: call EXTEND -GRAPH (h, 0, dmax, G)\n5: add MC rule node\n(⋁\nh∈H h, ∞\n)\nto G\n6: foreach pair (hi, hj) of hypotheses in H\n7: add MC rule node (¬hi ∨ ¬hj, cmc) to G\n8: return G\n9: procedure EXTEND -GRAPH (statement s, current depth\nd, max depth dmax, partial graph G):\n10: call operator s ⇒ (l, cs) to score statement s\n11: add statement node (s, l, cs) to G\n12: gen. the negation sentence negs = neg(s)\n13: add rule node (XOR(s, negs), cxor) to G\n14: call EXTEND -GRAPH (negs, d+ 1, dmax, G)\n15: if d < dmax do:\n16: let h = s\n17: call operator h ⇒ p to generate p\n18: call operator (p, h) ⇒ cr to score rule p → h\n19: add rule node (p → h, cr) to G\n20: foreach si ∈ p\n21: call EXTEND -GRAPH (si, d+ 1, dmax, G)\nmodel’s confidence (Kadavath et al., 2022).\n3.2.2 Initial Hypothesis Generation\nGiven a question, we first generate a set Hof hy-\npothesis sentences (e.g., “Is the sky (A) blue (B)\nyellow” →{ h1 = “The sky is blue.”,h2 = “The sky\nis yellow.”).3 An N-way multiple choice question\nyields N hypotheses in H. A true/false question\nyields 2 hypotheses. To handle open-ended ques-\ntions, candidate answers can be generated, e.g.,\nusing nucleus sampling (Holtzman et al., 2019).\n3.2.3 Belief Graph Generation\nThe belief graph generation process is shown in\nAlgorithm 1. An example of (part of) a generated\nbelief graph is shown in Figure 2.\nGiven a set Hof hypotheses, we generate a sin-\ngle belief graph Gby using our basic operations\n(Section 3.2.1) to recursively generate rules that\nconclude each hi ∈ Hup to a fixed maximum\ndepth dmax. (Each original hi is at depth d= 0.)\nFor each statement s, we also generate nodes\nnegs (and their recursive subgraphs) expressing its\nnegation, e.g., “The sky is not blue.” from “The\n3Conversion of a QA pair to a declarative hypothesis D\nuses a custom T5-11B model trained on the QA2D dataset\n(Demszky et al., 2018).\n5\n14194\nsky is blue.”.4 Each pair sand negs is connected\nwith an XOR rule, indicating a (soft) preference\nfor setting exactly one of them to true; this is repre-\nsented as two disjunctive constraints(s∨negs) and\n(−s∨−negs) whose weight cxor is a fixed hyper-\nparameter. Lastly, we add a multiple-choice (MC)\nconstraint which has two parts: a hard constraint\n(with infinite cost) that at least one hypothesis must\nbe chosen, and a soft constraint5 that no more than\none should be chosen. The soft constraint is associ-\nated with a fixed hyperparameter weight cmc.\n3.3 Reasoning Over Belief Graphs\nBelief graphs provide a window into the model’s\nbeliefs about some of the relevant statements and\ntheir (believed) inferential relationships to candi-\ndate answers to a question. As others have shown\n(Kassner et al., 2021; Mitchell et al., 2022), such\nbeliefs can be inconsistent, and materializing those\ninconsistencies provides one the opportunity to re-\nmove or reduce them.\nIn a similar vein, and as discussed in Section 3.1,\nREFLEX performs inference over belief graphs in\norder to compute an updated set of beliefs that is\nas consistent as possible with the rules. To this\nend, it converts belief graphs into an equivalent\nweighted MaxSAT problem and uses an off-the-\nshelf MaxSAT solver (RC2, (Ignatiev, 2019)) to\ncompute the optimal flips of initial true/false beliefs\nthat minimize global inconsistency. It then discards\nall rules that are in conflict with the updated state-\nment beliefs, obtaining a smaller, updated belief\ngraph. This smaller belief graph produced by\nREFLEX is self-consistent and provides inferential\nsupport for the top-level hypotheses.\n3.4 Generating Faithful Explanations\nNotably, the smaller updated belief graph produced\nby REFLEX provides a faithful explanation of the\nanswer it predicts, in the sense that it accurately\nrepresents the reasoning process behind the overall\nsystem’s prediction (Lyu et al., 2022). This is true\nas the MaxSAT reasoning process results precisely\nin a self-consistent set of beliefs from which RE-\nFLEX determines whether to believe a candidate\nanswer or not, and produces its final prediction\nbased on this (rather than on the raw LLM output\nalone; note that we do not make any claims about\n4We use a simple, custom-built utility for this, namely a\nT5-base model trained on 9k Turk-generated examples.\n5soft, to allow for cases with multiple valid answers, e.g.,\nopen-ended questions or those asking for the best answer.\nhow the internal reasoning of the LLM component\noperates.) Thus, REFLEX provides the user with an\ninterpretable reasoning trace, allowing the user to\nunderstand how it derived the answer from more\nrudimentary facts (Subramanian et al., 2020).\nWe note that the original belief graph (before\nreasoning) may reveal that the model’s original ex-\nplanation is, in fact, not faithful to its own beliefs.\nFor example, in Figure 2, the model believes state-\nments 6, 7, and that 6 & 7 entail 2, but does not\nbelieve 2 (colored grey). Thus, the global reason-\ning layer of REFLEX plays a critical role in arriving\nat faithful explanations.\n4 Experiments and Results\nThe goal of our experiments is to evaluate the ex-\ntent to which our overall system, namely an LLM\nplus a self-reflecting, rational layer, helps expose\nand resolve inconsistencies in the LLM’s beliefs\nwithout harming accuracy. Importantly, REFLEX is\nevaluated in a zero-shot setting, without relying on\ntraining instances of the target datasets.\nDatasets. We use the test partitions of three ex-\nisting multiple-choice datasets: EntailmentBank\n(Dalvi et al., 2021), OBQA (Mihaylov et al., 2018),\nand QuaRTz (Tafjord et al., 2019). We chose our\ndatasets as they contain inferentially rich questions\n(typically) requiring reasoning. The partitions con-\ntain 339, 500, and 784 examples, respectively.\nModels. The baseline LLM we use is an LLM\nthat has been trained to perform QA and also sup-\nports the basic operations discussed in Sec. 3.2.1,\nenabling us to assess how much it can be improved\nby adding a REFLEX layer. To this end, we use\na publicly available, frozen, off-the-shelf T5-11B\nLLM called Entailer (Tafjord et al., 2022). To an-\nswer an MC question with this LLM, we score each\nanswer hypothesis (cs, Section 3.2.1) and select the\none with the highest truth confidence. If Entailer\nassigns false values to all answer choices, we select\nthe hypothesis with the lowest false confidence.\nREFLEX then adds a rational layer to this LLM,\ncreating a new system that is also able to self-reflect\nand modify its beliefs. To ensure the different be-\nlief graph scores in REFLEX are appropriately cali-\nbrated, we use nine hyperparameters, tuned once\non the dev partition of EntailmentBank (Dalvi et al.,\n2021) and then kept fixed for all experiments. De-\ntails are in Appendix B. Note the LLM itself re-\nmains frozen, with belief revision occurring in the\n6\n14195\nrational (belief graph) layer above it.\nMetrics. For measuring self-consistency, we fol-\nlow Li et al. (2019) and report the conditional con-\nstraint violation(τ) metric, defined as follows: the\nfraction of rules whosepremises pare believed true,\nbut whose hypothesis his not. In other words, over\nall rules of the form p→h, τ is:\nτ = |{p→h|p= T,h = F}|\n|{p→h|p= T}|\nwhere s= T denotes the system believes statement\nsto be true (similarly for s= F). The numerator\nof τ thus captures the number of constraints the\nsystem violates. The denominator captures the\nnumber of applicable constraints. We then report\nthe following metric: consistency = 1 - τ.\nFor QA performance , we report standard\nmultiple-choice accuracy: 1 point for predicting\nthe correct answer, 1/ N points for predicting N\nanswers including the correct one, 1/ kpoints for\nno prediction (k= # answer options), 0 otherwise.\n4.1 Results\nConsistency. Table 1 shows consistency results\non the test partitions of our datasets. We observe\nsignificant consistency gains (by 8%-11% abso-\nlute), showing REFLEX ’s effectiveness at creating a\nconsistent belief network within the overall system.\nEntail-\nSystem mentBank OBQA Quartz\nLLM 87.0 88.2 85.7\nLLM + rational layer 96.1 95.9 96.6(REFLEX )\nTable 1: Consistency: By adding a rational layer to the\nbaseline LLM, REFLEX significantly improves consis-\ntency among beliefs by resolving uncovered conflicts.\nAccuracy. Table 2 shows overall performance on\nour three datasets (test partitions). As can be seen,\nwe observe stable accuracy, as well as the answers\nnow being faithful to the reasoning chains in the\nbelief graph. This is significant, as it allows users\nto understand how answers follow from system\nbeliefs (and in cases where an LLM belief was\nflipped, why that belief is untenable in the broader\nsystem).\nAblations. To study the impact of the three dif-\nferent types of rules on consistency improvement,\nwe using the EntilmentBank dataset (dev partition).\nEntail-\nSystem mentBank OBQA Quartz\nLLM 79.4 74.0 80.2\nLLM + rational layer 79.9 75.0 80.0(REFLEX )\nTable 2: QA accuracy: REFLEX ’s belief revision in the\nrational layer preserves overall QA accuracy.\nTo do this, given the belief graph for a question,\nwe mask out (separately, rather than cumulatively)\neach type of rule in turn when providing the graph\nto the MaxSAT solver. We then run the constraint\nsolver and measure the resulting self-consistency\nof beliefs on the original graph.\nSystem EntailmentBank\nREFLEX (our system): 96.1\n- without p→hrules 93.8\n- without XOR rules 90.4\n- without MC rule 95.8\nTable 3: Consistency: Ablations on EntailmentBank\n(Dev) suggest that all three types of rules contribute to\nimproving self-consistency.\nThe results are shown in Table 3 (the MC rule\nis the constraint that exactly one multiple-choice\noption should be chosen, Section 3.2.3). The results\nindicate that all three types of rules contribute to\nthe system’s consistency improvements.\n4.2 Success Analysis\nWe identify three classes of successful reasoning\nby the constraint reasoner: (a) latent model beliefs\ncorrect an initially wrong answer (Figure 3); (b) the\nsystem corrects an initially erroneous, latent model\nbelief (Figure 4); and (c) strong model beliefs iden-\ntify and reject a bad rule (Figure 5). These types of\nsystem corrections help to improve accuracy and\nproduce answers supported by valid chains of rea-\nsoning, allowing users insight into why an answer\nfollows from the model’s knowledge.\n4.3 Failure Analysis\nReasoning can also make mistakes. From a man-\nual analysis of 50 random questions from Entail-\nmentBank that REFLEX answered incorrectly, we\nidentified five main causes of failure and their ap-\nproximate frequency (Note that multiple categories\ncan apply, hence total is >100%):\n7\n14196\nFigure 3: Example of good reasoning: The model’s\nbeliefs in 1 and 2, and the rule 1 & 2 →3, as well as\nthe xor constraint, causes it to (desirably) flip its belief\nin 3 from false (grey, before) to true (white, after).\n1. Missing Rules ( ≈30%): In some cases, the\nsystem generates irrelevant rules but misses an im-\nportant one needed to support the correct answer,\nresulting in incorrect conclusions. While somewhat\nsubjective, this is a notable error category that we\nobserve. For example for the question:\nA human cannot survive the loss of (A) The liver\n[correct] (B) A lung (C) A kidney\nthe system incorrectly concludes (B) is true, ignor-\ning the commonsense rule that with two lungs, a\nperson can survive without one of them.\n2. Incorrect Beliefs ( ≈30%): Sometimes the\nreasoner fails to correct incorrect model beliefs,\neither because the model’s confidence is high or\nevidence against them is weak or missing. In the\nexample shown in Figure 7, the model’s strong,\nincorrect beliefs that “river deltas are reservoirs”\nand “reservoirs always provide freshwater” (untrue\nof oceans, say) causes it to incorrectly conclude\nthat “deltas are freshwater reservoirs”.\n3. Incorrect Rules ( ≈10%): Rule generation\ncan produce bad rules, e.g., in Figure 5), and in\nsome cases the constraint reasoner fails to reject\nthem if they are strongly believed. In particular,\nconfusion or ambiguity over quantifiers can result\nin bad rules, e.g., (emphasis added) “Some animals\ncatch their prey with trickery.” & “A spider is a kind\nof animal.” →“Spiders catch their prey with trick-\nery.”. Similarly the model generates the fallacy:\n“Some people don’t mind not moving for an hour”\n& “breathing is a kind of movement” →“Some\nFigure 4: Example of good reasoning: Although the\nmodel correctly believes option (A) is false (grey, node\n3), this answer conflicts with other beliefs (red). Rea-\nsoning leads the system to realize that its weakest belief\n(2) is actually false, correctly flipping its label from true\n(white) to false (grey, right side) restoring consistency.\nFigure 5: Example of good reasoning: Here the rea-\nsoner (desirably) chooses to reject the violated (bad)\nrule rather than flip a belief, as the minimum cost way\nto restore consistency.\npeople don’t mind not breathing for an hour.”\n4. Ambiguous Statements, Unexpected Reason-\ning (≈10%): A common cause of error is the\nsurprising ambiguity of belief statements, which\ncan often be read in multiple ways. In several cases,\nthe model adopts a valid but unexpected interpre-\ntation, resulting in “errors” compared to the gold\nanswer label. For example, in Figure 6, the model\ntakes the word “always” in a literal sense (“glaciers\nwill not always be there”), resulting in an answer\nthat differs from the gold label. Developing ways\nto attach context to these statements to help disam-\nbiguate them would help alleviate such errors.\n5. Multiple Valid Answers ( ≈10%): A final\ncause of “error” - at least with respect to the gold\nlabel - is that multiple answers may be valid, and\n8\n14197\nFigure 6: Unexpected reasoning: Here the model unex-\npectedly pays particular attention to the world “always”.\nBecause it strongly believes that glaciers will notalways\nbe there (1, white), the system prefers to flip its beliefs\nin 3 and 4, rather than flipping 1, thus rejecting answer\noption B (arguably correctly).\nthe question is asking for the best answer; eg. for\n“What could fill a beach ball? (A) Oxygen (B) Wa-\nter ...”, A is labeled correct, while B is also a valid\nanswer. REFLEX (desirably) finds valid reasoning\nchains for both, but the notion of highest-scoring\nproof does not fully correlate with the notion of\n“best answer” intended by the question author.\n5 Future Work\nThere are several impactful ways this work could\nbe further extended. First, incorporating the ques-\ntion’s context in the belief statements in our ratio-\nnal layer could make the semantics of the beliefs\nmore precise, thus avoiding potential ambiguity in\ntheir truth value. Second, one could use the belief\ngraph itself to identify the key reasoning pieces that\nthe LLM is most uncertain about. This could then\nguide a human-in-the-loop mechanism to correct or\nvalidate uncertain pieces via user interaction. Third,\nmaintaining a persistent belief graphover multiple\nquestions could help make the system more consis-\ntent across questions. This, in turn, would make a\nuser’s conversational experience with the system\nmore coherent in a longer dialog setting. Lastly,\nafter resolving inconsistencies in the rational layer,\nwe could consider propagating information back to\nthe LLM layerin order to update it (via fine-tuning,\nmodel editing, memory-based architectures, etc.),\nFigure 7: Failure due to bad beliefs: The model\nstrongly believes both 1 and 2 (although both are factu-\nally incorrect), here causing 3’s label to undesirably flip\nfrom false (correct) to true (incorrect).\nhelping avoid similar inconsistencies in the future.\n6 Conclusion\nWhile LLMs perform well, the interdependencies\nbetween their answers and their other beliefs is\nopaque, and may even be in conflict. This lack\nof interpretability is a significant impediment to\nwidespread use of LLMs. To reduce this opac-\nity, and reduce these conflicts, we have proposed\nREFLEX , a new system architecture in which an\nexplicit, interpretable representation of beliefs - the\nbelief graph - is added as a rational layer above\nthe LLM. This layer providing a window into sys-\ntem beliefs, and allows latent inconsistencies in the\nLLM alone to reasoned about and repaired. Our im-\nplementation shows that belief consistency of the\noverall system is significantly improved, without\nharming answer accuracy, resulting in answers sup-\nported by interpretable chains of reasoning drawn\nfrom a more consistent belief system. This new\narchitecture is an important step towards improv-\ning confidence in system behavior, and towards\ntrustable deployment of LLMs in practical applica-\ntions.\nLimitations\nWe have shown how an LLM can be extended with\na self-reflective component, allowing latent model\nknowledge to be made explicit in the form of a\nbelief graph, providing a window into the model’s\nsystem of beliefs. While exciting, there are several\nlimitations with the current work and opportunities\nfor the future.\nFirst, the reasoning component in the rational\n9\n14198\nlayer can make mistakes, resulting in the overall\nsystem rejecting true statements or accepting false\nones. A detailed analysis and classification of these\nfailure modes was presented in Section 4.3.\nSecond, for our experiments, we used the T5-\n11B based Entailer system as the baseline LLM.\nWhile there is every reason to expect our pro-\nposed architecture to be effective in reducing in-\nconsistency with newer and larger LLMs such as\nChatGPT and LLaMA, this is still to be evalu-\nated. Doing so would require implementing the\nbasic operations needed to construct belief graphs\n(Section 3.2.1) using instruction prompting and in-\ncontext learning. Other work has demonstrated\nsuch implementations (e.g., Wei et al., 2022; Jiang\net al., 2020), making the outlook promising, but\nindeed their combination still needs to be demon-\nstrated at scale in an architecture like REFLEX.\nLastly, we found consistency-minimized belief\ngraphs to be highly valuable in understanding the\nsystem’s successes and failures. We expect these\ngraphs to be a valuable starting point for provid-\ning explanations and gaining a user’s trust in the\nsystem. However, we have not conducted a formal\nuser study to measure this.\nEthics Statement\nLike any other project using LLMs, despite the\nbest intentions there is a risk of the model produc-\ning biased or offensive statements as part of its\nexplanations, and thus must be used with care and\nappropriate guards and warnings.\nAcknowledgements\nThis research was made possible, in part, by fund-\ning from Open Philanthropy, the European Re-\nsearch Council (#740516) and by the German Fed-\neral Ministry of Education and Research (BMBF)\nunder Grant No. 01IS18036A. We also thank\nGoogle for providing the TPUs for conducting ex-\nperiments. Finally, we are grateful for the valuable\nfeedback from the anonymous reviewers.\nReferences\nBhavana Dalvi, Peter Alexander Jansen, Oyvind\nTafjord, Zhengnan Xie, Hannah Smith, Leighanna\nPipatanangkura, and Peter Clark. 2021. Explaining\nanswers with entailment trees. In EMNLP.\nJoe Davison, Joshua Feldman, and Alexander Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models. In EMNLP.\nDorottya Demszky, Kelvin Guu, and Percy Liang.\n2018. Transforming question answering datasets\ninto natural language inference datasets. ArXiv,\nabs/1809.02922.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha\nRavichander, E. Hovy, Hinrich Schütze, and Yoav\nGoldberg. 2021. Measuring and improving consis-\ntency in pretrained language models. TACL, 9.\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. TACL, 8:34–48.\nYuling Gu, Bhavana Dalvi, and Peter Clark. 2023. Do\nlanguage models have coherent mental models of\neveryday things? In ACL.\nAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin\nChoi. 2019. The curious case of neural text degener-\nation. In ICLR.\nAlexey Ignatiev. 2019. RC2: an efficient MaxSAT\nsolver. J. Satisf. Boolean Model. Comput., 11.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? TACL, 8.\nJaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brah-\nman, Chandra Bhagavatula, Ronan Le Bras, and\nYejin Choi. 2022. Maieutic prompting: Logically\nconsistent reasoning with recursive explanations. In\nEMNLP.\nSaurav Kadavath, Tom Conerly, Amanda Askell, T. J.\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zachary Dodds, Nova DasSarma, Eli Tran-\nJohnson, Scott Johnston, Sheer El-Showk, Andy\nJones, Nelson Elhage, Tristan Hume, Anna Chen,\nYushi Bai, Sam Bowman, Stanislav Fort, Deep\nGanguli, Danny Hernandez, Josh Jacobson, John\nKernion, Shauna Kravec, Liane Lovitt, Kamal\nNdousse, Catherine Olsson, Sam Ringer, Dario\nAmodei, Tom B. Brown, Jack Clark, Nicholas Joseph,\nBenjamin Mann, Sam McCandlish, Christopher Olah,\nand Jared Kaplan. 2022. Language models (mostly)\nknow what they know. ArXiv, abs/2207.05221.\nNora Kassner and H. Schütze. 2020. Negated and mis-\nprimed probes for pretrained language models: Birds\ncan talk, but cannot fly. In ACL.\nNora Kassner, Oyvind Tafjord, Hinrich Schutze, and\nPeter Clark. 2021. BeliefBank: Adding memory to a\npre-trained language model for a systematic notion\nof belief. In EMNLP.\nTao Li, Vivek Gupta, Maitrey Mehta, and Vivek Sriku-\nmar. 2019. A logic-driven framework for consistency\nof neural models. In EMNLP.\nHans-Andrea Loeliger. 2004. An introduction to factor\ngraphs. IEEE Signal Processing Magazine, 21:28–41.\nhttps://people.binf.ku.dk/∼thamelry/MLSB08/hal.pdf.\n10\n14199\nQing Lyu, Marianna Apidianaki, and Chris Callison-\nBurch. 2022. Towards faithful model explanation in\nNLP: A survey. ArXiv, abs/2209.11326.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? A new dataset for open book question an-\nswering. In EMNLP.\nEric Mitchell, Joseph J. Noh, Siyan Li, William S.\nArmstrong, Ananth Agarwal, Patrick Liu, Chelsea\nFinn, and Christopher D. Manning. 2022. Enhancing\nself-consistency and performance of pre-trained lan-\nguage models through natural language inference. In\nEMNLP.\nOpenAI. 2022. ChatGPT: Optimizing language mod-\nels for dialog. Technical report, openai.com.\nhttps://openai.com/blog/chatgpt/.\nJames D Park. 2002. Using weighted MAX-SAT en-\ngines to solve MPE. In AAAI/IAAI.\nMatthew E Peters, Mark Neumann, Robert Logan, Roy\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A\nSmith. 2019. Knowledge enhanced contextual word\nrepresentations. In EMNLP.\nFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim\nRocktäschel, Yuxiang Wu, Alexander H. Miller, and\nSebastian Riedel. 2020. How context affects lan-\nguage models’ factual predictions. In Automated\nKnowledge Base Construction.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In EMNLP.\nAbhilasha Ravichander, Eduard Hovy, Kaheer Suleman,\nAdam Trischler, and Jackie Chi Kit Cheung. 2020.\nOn the systematicity of probing contextualized word\nrepresentations: The case of hypernymy in BERT. In\nProceedings of the Ninth Joint Conference on Lexical\nand Computational Semantics.\nKyle Richardson, Ronen Tamari, Oren Sultan, Reut\nTsarfaty, Dafna Shahaf, and Ashish Sabharwal. 2022.\nBreakpoint Transformers for Modeling and Tracking\nIntermediate Beliefs. In EMNLP.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the parame-\nters of a language model? In EMNLP.\nTian Sang, Paul Beame, and Henry A Kautz. 2007. A\ndynamic approach for MPE and weighted MAX-SAT.\nIn IJCAI.\nEric Schwitzgebel. 2019. Belief. Stan-\nford Encyclopedia of Philosophy .\nhttps://plato.stanford.edu/entries/belief/.\nSanjay Subramanian, Ben Bogin, Nitish Gupta, Tomer\nWolfson, Sameer Singh, Jonathan Berant, and Matt\nGardner. 2020. Obtaining faithful interpretations\nfrom compositional neural networks. In ACL.\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2022.\nEntailer: Answering questions with faithful and truth-\nful chains of reasoning. In EMNLP.\nOyvind Tafjord, Matt Gardner, Kevin Lin, and Peter\nClark. 2019. QuaRTz: An open-domain dataset of\nqualitative relationship questions. In EMNLP.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. In NeurIPS.\nNathaniel Weir and Benjamin Van Durme. 2022.\nDynamic generation of interpretable inference\nrules in a neuro-symbolic expert system. ArXiv,\nabs/2209.07662.\nSarah Wiegreffe and Ana Marasovi´c. 2021. Teach me\nto explain: A review of datasets for explainable nat-\nural language processing. In NeurIPS Datasets and\nBenchmarks.\nDenny Zhou, Nathanael Scharli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Huai hsin Chi.\n2023. Least-to-most prompting enables complex rea-\nsoning in large language models. ICLR.\n11\n14200\nA Additional Results\nWe report results on the dev set of the Entailment-\nBank dataset in Table A1.\nSystem EntailmentBank (dev)\nConsistency Accuracy\nLLM 87.5 78.6\nLLM + rational layer 96.1 81.8(REFLEX )\nTable A1: Results on EntailmentBank (dev), used to\ntune the system’s hyperparameters.\nB Hyperparameters and Runtime\nMaxSAT finds the optimal assignment of true/false\nlabels on statement nodes that minimizes the total\npenalty of constraint violations. If the true/false la-\nbel on a statement node is flipped, then the penalty\nis the model confidence cs in the original label.\nSimilarly if a rule (constraint) is violated by the\ntrue/false labels on its associated statements, then\nthe penalty is the model confidence cr in that rule.\nWe set a number of hyperparameters to ensure\nthat the various sources of confidence are appropri-\nately balanced, and tune these on a development set\n(EntailmentBank (dev) which is separate from our\ntest sets). We use the same set of hyperparameters\nfor all test sets.\n1. As raw model confidences cs are highly\nskewed towards 0 and 1, we re-calibrate these\nwith ek.(cs−1), where k is a fixed hyperparam-\neter. Note, that for the MC and XOR rule, the\nraw input score sis 1.0.\n2. We calibrate rule confidences in the same way\nas we calibrate belief confidences but use sep-\narate calibration parameters different types of\nrules namely:\n• Entailer rules p→h\n• XOR rules\n• MC rules\ni.e., the raw rule score c is re-calibrated to\nconfidence ektype.(c−1) where ktype is the re-\nspective hyperparameter per rule type.\n3. We set three hyperparameters tuning the re-\nspective importance of the three different\ntypes of rules. Therefore, the final rule score\nis computed by c= ttype ∗ektype.(c−1) where\nttype is the respective hyperparameter constant\nper rule type.\n4. For xor rules between statementssi and negsi,\nHyperparameter Value\nk 9\nkentailer 36\nkxor 30\nkmc 9\ntentailer 1.02\ntxor 1.1\ntmc 0.98\nmxor 0.3\ndmax 5\nTable B1: Hyperparameters.\nwe remove (ignore) those where there is signif-\nicant uncertainty, namely where |score(si) −\nscore(negsi)| ≤mxor, where mxor is a\ntuned hyperparmeter.\n5. Additionally, we tune a damping parameter\nthat downscales rules on the boundary of the\ngraph. Belief nodes involved in these rules\nare not supported by any premises and should\ntherefore have less influence than rules with\nstrong support.\n6. Finally, we tune the maximum depth dmax of\nthe belief graph.\nThe performance on this dev set partition is\nshown in Table A1 and the hyperparameter values\nare shown in Table B1.\nThe runtime for MaxSAT constraint solving is\nfast (<1 millisecond per question). However, con-\nstructing the belief graph is computationally in-\ntensive: Each call to expand or score a node takes\n∼2 seconds, and our graphs typically contain∼600\nnodes, so if these calls were maximally parallelized,\nwith each step growing the graph one level deeper,\nthe runtime would be the maximum graph depth\n(5) x 2 seconds = ∼10 seconds total (or several\nminutes if a naive sequential implementation were\nused).\n12\n14201",
  "topic": "Semantic reasoner",
  "concepts": [
    {
      "name": "Semantic reasoner",
      "score": 0.6848095655441284
    },
    {
      "name": "Interpretability",
      "score": 0.6811410784721375
    },
    {
      "name": "Computer science",
      "score": 0.6410930156707764
    },
    {
      "name": "Rationality",
      "score": 0.5931007266044617
    },
    {
      "name": "Question answering",
      "score": 0.5443058609962463
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4873429238796234
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.42892730236053467
    },
    {
      "name": "Construct (python library)",
      "score": 0.4277094900608063
    },
    {
      "name": "Forward chaining",
      "score": 0.4127904176712036
    },
    {
      "name": "Natural language processing",
      "score": 0.3810751438140869
    },
    {
      "name": "Cognitive psychology",
      "score": 0.35981547832489014
    },
    {
      "name": "Cognitive science",
      "score": 0.32128530740737915
    },
    {
      "name": "Psychology",
      "score": 0.31629687547683716
    },
    {
      "name": "Epistemology",
      "score": 0.28758081793785095
    },
    {
      "name": "Expert system",
      "score": 0.12954914569854736
    },
    {
      "name": "Programming language",
      "score": 0.11527636647224426
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}