{
    "title": "Multi-Modal Music Information Retrieval: Augmenting Audio-Analysis with Visual Computing for Improved Music Video Analysis",
    "url": "https://openalex.org/W3003425996",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5102746568",
            "name": "Alexander Schindler",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W56995320",
        "https://openalex.org/W2046255764",
        "https://openalex.org/W2981571772",
        "https://openalex.org/W1993088587",
        "https://openalex.org/W1592934690",
        "https://openalex.org/W1545641654",
        "https://openalex.org/W1838934915",
        "https://openalex.org/W2017936404",
        "https://openalex.org/W2060975379",
        "https://openalex.org/W2963775347",
        "https://openalex.org/W2097117768",
        "https://openalex.org/W2162124943",
        "https://openalex.org/W146980257",
        "https://openalex.org/W2119288237",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W1543320899",
        "https://openalex.org/W1556219185",
        "https://openalex.org/W2159243025",
        "https://openalex.org/W2012665501",
        "https://openalex.org/W2385545",
        "https://openalex.org/W2159540918",
        "https://openalex.org/W2158261396",
        "https://openalex.org/W1505382086",
        "https://openalex.org/W2021354639",
        "https://openalex.org/W2158874389",
        "https://openalex.org/W1965434768",
        "https://openalex.org/W2153166546",
        "https://openalex.org/W2170942078",
        "https://openalex.org/W1965419851",
        "https://openalex.org/W2081580037",
        "https://openalex.org/W1555805436",
        "https://openalex.org/W2125852866",
        "https://openalex.org/W2570926663",
        "https://openalex.org/W85840116",
        "https://openalex.org/W3098682680",
        "https://openalex.org/W2165320163",
        "https://openalex.org/W1591084780",
        "https://openalex.org/W2773686055",
        "https://openalex.org/W2113144451",
        "https://openalex.org/W2141538344",
        "https://openalex.org/W2228467293",
        "https://openalex.org/W2963066677",
        "https://openalex.org/W1549938510",
        "https://openalex.org/W1965690069",
        "https://openalex.org/W76301927",
        "https://openalex.org/W838685763",
        "https://openalex.org/W2886593255",
        "https://openalex.org/W1595803501",
        "https://openalex.org/W2295991281",
        "https://openalex.org/W2290867587",
        "https://openalex.org/W2912934387",
        "https://openalex.org/W47638121",
        "https://openalex.org/W1986299872",
        "https://openalex.org/W2405656250",
        "https://openalex.org/W2164598857",
        "https://openalex.org/W646254279",
        "https://openalex.org/W2123991323",
        "https://openalex.org/W2087391443",
        "https://openalex.org/W2402499561",
        "https://openalex.org/W2191779130",
        "https://openalex.org/W2101042689",
        "https://openalex.org/W1508121828",
        "https://openalex.org/W1563750459",
        "https://openalex.org/W1988273857",
        "https://openalex.org/W2274287116",
        "https://openalex.org/W2552472274",
        "https://openalex.org/W1927873098",
        "https://openalex.org/W2401461771",
        "https://openalex.org/W2626592698",
        "https://openalex.org/W2114766824",
        "https://openalex.org/W2151816810",
        "https://openalex.org/W2937817908",
        "https://openalex.org/W2074619556",
        "https://openalex.org/W2081278545",
        "https://openalex.org/W2295235328",
        "https://openalex.org/W2001412060",
        "https://openalex.org/W2080391922",
        "https://openalex.org/W2003889823",
        "https://openalex.org/W2003856922",
        "https://openalex.org/W1601039325",
        "https://openalex.org/W2020901563",
        "https://openalex.org/W9662830",
        "https://openalex.org/W2183852008",
        "https://openalex.org/W2121647436",
        "https://openalex.org/W1977590548",
        "https://openalex.org/W1994002998",
        "https://openalex.org/W2143583567",
        "https://openalex.org/W2113293674",
        "https://openalex.org/W2913220963",
        "https://openalex.org/W2906259846",
        "https://openalex.org/W2158602558",
        "https://openalex.org/W2098775154",
        "https://openalex.org/W272962585",
        "https://openalex.org/W2583165630",
        "https://openalex.org/W131896369",
        "https://openalex.org/W2165698076",
        "https://openalex.org/W2134670479",
        "https://openalex.org/W2809951469",
        "https://openalex.org/W2902396633",
        "https://openalex.org/W1786033384",
        "https://openalex.org/W2163352848",
        "https://openalex.org/W2951628589",
        "https://openalex.org/W2293656320",
        "https://openalex.org/W59186300",
        "https://openalex.org/W2075586101",
        "https://openalex.org/W2062139369",
        "https://openalex.org/W2146104196",
        "https://openalex.org/W2767390678",
        "https://openalex.org/W2329423822",
        "https://openalex.org/W1519077461",
        "https://openalex.org/W2053836391",
        "https://openalex.org/W1570908156",
        "https://openalex.org/W2580221632",
        "https://openalex.org/W2046331708",
        "https://openalex.org/W45319298",
        "https://openalex.org/W2058333183",
        "https://openalex.org/W1988445395",
        "https://openalex.org/W2138464208",
        "https://openalex.org/W2109173030",
        "https://openalex.org/W645260053",
        "https://openalex.org/W1813265826",
        "https://openalex.org/W3122755518",
        "https://openalex.org/W2101151533",
        "https://openalex.org/W2121947440",
        "https://openalex.org/W2167969117",
        "https://openalex.org/W2231488453",
        "https://openalex.org/W2177274842",
        "https://openalex.org/W1832115024",
        "https://openalex.org/W651964205",
        "https://openalex.org/W2117223204",
        "https://openalex.org/W2095681812",
        "https://openalex.org/W2138451337",
        "https://openalex.org/W625810977",
        "https://openalex.org/W2162627071",
        "https://openalex.org/W2437181147",
        "https://openalex.org/W2154739152",
        "https://openalex.org/W2072214400",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W1560013842",
        "https://openalex.org/W2903743215",
        "https://openalex.org/W2919504802",
        "https://openalex.org/W1578352865",
        "https://openalex.org/W2309280307",
        "https://openalex.org/W2076466599",
        "https://openalex.org/W69699846",
        "https://openalex.org/W251784055",
        "https://openalex.org/W2161827960",
        "https://openalex.org/W2144263578",
        "https://openalex.org/W2544515106",
        "https://openalex.org/W2064533360",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2138266693",
        "https://openalex.org/W1985930433",
        "https://openalex.org/W3032733955",
        "https://openalex.org/W2107634464",
        "https://openalex.org/W3210232381",
        "https://openalex.org/W128872657",
        "https://openalex.org/W1649293523",
        "https://openalex.org/W2143668817",
        "https://openalex.org/W2046589280",
        "https://openalex.org/W650875164",
        "https://openalex.org/W274680021",
        "https://openalex.org/W1511924373",
        "https://openalex.org/W2099736636",
        "https://openalex.org/W1579748941",
        "https://openalex.org/W2147328102",
        "https://openalex.org/W2341090665",
        "https://openalex.org/W2329866758",
        "https://openalex.org/W2396249182",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2209982072",
        "https://openalex.org/W2104270759",
        "https://openalex.org/W2563031223",
        "https://openalex.org/W1606285668",
        "https://openalex.org/W2012474104",
        "https://openalex.org/W2129652055",
        "https://openalex.org/W2078442422",
        "https://openalex.org/W2138286601",
        "https://openalex.org/W2148191008",
        "https://openalex.org/W2107430826",
        "https://openalex.org/W1560641100",
        "https://openalex.org/W2151920764",
        "https://openalex.org/W2100816929",
        "https://openalex.org/W621024007",
        "https://openalex.org/W1989605374",
        "https://openalex.org/W2132126381",
        "https://openalex.org/W2068874033",
        "https://openalex.org/W2147069236",
        "https://openalex.org/W2328848613",
        "https://openalex.org/W206359866",
        "https://openalex.org/W2257146423",
        "https://openalex.org/W2890559714",
        "https://openalex.org/W1502275556",
        "https://openalex.org/W2404059402",
        "https://openalex.org/W2100753110",
        "https://openalex.org/W1989702938",
        "https://openalex.org/W2900359953",
        "https://openalex.org/W2167537125",
        "https://openalex.org/W2151103935",
        "https://openalex.org/W2062170755",
        "https://openalex.org/W2949917172",
        "https://openalex.org/W76597279",
        "https://openalex.org/W2240580723",
        "https://openalex.org/W2133990480",
        "https://openalex.org/W1997249373",
        "https://openalex.org/W2089366864",
        "https://openalex.org/W2178339699",
        "https://openalex.org/W2149056113",
        "https://openalex.org/W1555918374",
        "https://openalex.org/W2133824856",
        "https://openalex.org/W2033431158",
        "https://openalex.org/W185574853",
        "https://openalex.org/W2169467627",
        "https://openalex.org/W2109506130",
        "https://openalex.org/W1501643721",
        "https://openalex.org/W2403342846",
        "https://openalex.org/W2158515176",
        "https://openalex.org/W2095005467",
        "https://openalex.org/W306648316",
        "https://openalex.org/W2535046761",
        "https://openalex.org/W2144707026",
        "https://openalex.org/W2014281092",
        "https://openalex.org/W2119605622",
        "https://openalex.org/W2323991424",
        "https://openalex.org/W2117717972",
        "https://openalex.org/W47947492",
        "https://openalex.org/W2290111476",
        "https://openalex.org/W2394955069",
        "https://openalex.org/W2128119911",
        "https://openalex.org/W1986602091",
        "https://openalex.org/W2101710672",
        "https://openalex.org/W1979955134",
        "https://openalex.org/W2066768226",
        "https://openalex.org/W2156063339",
        "https://openalex.org/W2031096531",
        "https://openalex.org/W2752398904",
        "https://openalex.org/W2296248436",
        "https://openalex.org/W15066456",
        "https://openalex.org/W2029041994",
        "https://openalex.org/W2158007828",
        "https://openalex.org/W2027899226",
        "https://openalex.org/W2767414818",
        "https://openalex.org/W2161108885",
        "https://openalex.org/W2390689188",
        "https://openalex.org/W1965657003",
        "https://openalex.org/W2105740906",
        "https://openalex.org/W2129138044",
        "https://openalex.org/W2137659841",
        "https://openalex.org/W2148748626",
        "https://openalex.org/W2023254499",
        "https://openalex.org/W1964923082",
        "https://openalex.org/W2126410803",
        "https://openalex.org/W1487393892",
        "https://openalex.org/W2099270617",
        "https://openalex.org/W2100454127",
        "https://openalex.org/W1950706092",
        "https://openalex.org/W1970456555",
        "https://openalex.org/W2038678505",
        "https://openalex.org/W1608523847",
        "https://openalex.org/W2002055708",
        "https://openalex.org/W2151543699",
        "https://openalex.org/W1878565453",
        "https://openalex.org/W1591470100",
        "https://openalex.org/W2950094539",
        "https://openalex.org/W3217467030",
        "https://openalex.org/W1508404128",
        "https://openalex.org/W1988790447",
        "https://openalex.org/W2917011293",
        "https://openalex.org/W2403105304",
        "https://openalex.org/W2159561775",
        "https://openalex.org/W2124660252",
        "https://openalex.org/W2014937798",
        "https://openalex.org/W2510372003"
    ],
    "abstract": "This thesis combines audio-analysis with computer vision to approach Music Information Retrieval (MIR) tasks from a multi-modal perspective. This thesis focuses on the information provided by the visual layer of music videos and how it can be harnessed to augment and improve tasks of the MIR research domain. The main hypothesis of this work is based on the observation that certain expressive categories such as genre or theme can be recognized on the basis of the visual content alone, without the sound being heard. This leads to the hypothesis that there exists a visual language that is used to express mood or genre. In a further consequence it can be concluded that this visual information is music related and thus should be beneficial for the corresponding MIR tasks such as music genre classification or mood recognition. A series of comprehensive experiments and evaluations are conducted which are focused on the extraction of visual information and its application in different MIR tasks. A custom dataset is created, suitable to develop and test visual features which are able to represent music related information. Evaluations range from low-level visual features to high-level concepts retrieved by means of Deep Convolutional Neural Networks. Additionally, new visual features are introduced capturing rhythmic visual patterns. In all of these experiments the audio-based results serve as benchmark for the visual and audio-visual approaches. The experiments are conducted for three MIR tasks Artist Identification, Music Genre Classification and Cross-Genre Classification. Experiments show that an audio-visual approach harnessing high-level semantic information gained from visual concept detection, outperforms audio-only genre-classification accuracy by 16.43%.",
    "full_text": "Multi-Modal Music Information Retrieval:\nAugmenting Audio-Analysis with Visual\nComputing for Improved Music Video Analysis\nDISSERTATION\nzur Erlangung des akademischen Grades\nDoktor der technischen Wissenschaften\neingereicht von\nAlexander Schindler\nMatrikelnummer 9926045\nan der\nFakultät für Informatik der Technischen Universität Wien\nBetreuung: Ao.univ.Prof. Dr. Andreas Rauber\nDiese Dissertation haben begutachtet:\n(Univ. Prof. Mag. Dipl.-Ing. Dr.\nMarkus Schedl)\n(Univ. Prof. Dr. Allan Hanbury)\nWien, 01.10.2019\n(Alexander Schindler)\nTechnische Universität Wien\nA-1040 Wien \u0005 Karlsplatz 13 \u0005 Tel. +43-1-58801-0 \u0005 www.tuwien.ac.at\narXiv:2002.00251v1  [cs.MM]  1 Feb 2020\n\nMulti-Modal Music Information Retrieval:\nAugmenting Audio-Analysis with Visual\nComputing for Improved Music Video Analysis\nDISSERTATION\nsubmitted in partial fulﬁllment of the requirements for the degree of\nDoktor der technischen Wissenschaften\nby\nAlexander Schindler\nRegistration Number 9926045\nto the Faculty of Informatics\nat the Vienna University of Technology\nAdvisor: Ao.univ.Prof. Dr. Andreas Rauber\nThe dissertation has been reviewed by:\n(Univ. Prof. Mag. Dipl.-Ing. Dr.\nMarkus Schedl)\n(Univ. Prof. Dr. Allan Hanbury)\nWien, 01.10.2019\n(Alexander Schindler)\nTechnische Universität Wien\nA-1040 Wien \u0005 Karlsplatz 13 \u0005 Tel. +43-1-58801-0 \u0005 www.tuwien.ac.at\n\nErklaerung zur Verfassung der Arbeit\nAlexander Schindler\nBeckmanngasse 4/12, 1140 Wien\nHiermit erklaere ich, dass ich diese Arbeit selbstqendig verfasst habe, dass ich die verwen-\ndeten Quellen und Hilfsmittel vollstaendig angegeben habe und dass ich die Stellen der Arbeit\n- einschliesslich Tabellen, Karten und Abbildungen -, die anderen Werken oder dem Internet\nim Wortlaut oder dem Sinn nach entnommen sind, auf jeden Fall unter Angabe der Quelle als\nEntlehnung kenntlich gemacht habe.\n(Ort, Datum) (Unterschrift Verfasser)\ni\n\nAbstract\nThe context of this thesis is embedded in the interdisciplinary research ﬁeld of Music Infor-\nmation Retrieval (MIR) and in particular in the subsection which extracts information from the\naudio signal by means of digital signal analysis. Because music is in itself multi-modal, many\napproaches harness multiple input modalities such as audio, lyrics or music notes to solve MIR\nresearch tasks. This thesis focuses on the information provided by the visual layer of music\nvideos and how it can be harnessed to augment and improve tasks of the MIR research domain.\nThe main hypothesis of this work is based on the observation that certain expressive categories\nsuch as genre or theme can be recognized on the basis of the visual content alone, without the\nsound being heard. This leads to the hypothesis that there exists a visual language that is used to\nexpress mood or genre. In a further consequence it can be concluded that this visual information\nis music related and thus should be beneﬁcial for the corresponding MIR tasks such as music\ngenre classiﬁcation or mood recognition.\nThe validation of these hypotheses is approached analytically and experimentally. The an-\nalytical approach conducts literature search in the Musicology and Music Psychology research\ndomain to identify studies on or documentations of production processes of music videos or\nvisual branding in the music business. The history of the utilization of visual attribution is\ninvestigated beginning with illustrations on sheet music, album cover arts to music video pro-\nduction. This elaborates the importance of visual design and how the music industry harnesses\nit to promote new acts, increase direct sales or market values. In the pre-streaming era to attract\nmore customers, album covers had to be as appealing and recognizable as possible to stand out\nin record shelves. Especially new artists whose style was yet unknown were visually branded\nand outﬁtted by music labels to be immediately identiﬁable in terms of style and music genre in\nmagazines or on TV .\nThe experimental approach conducts a series of comprehensive experiments and evaluations\nwhich are focused on the extraction of visual information and its application in different MIR\ntasks. Due to the absence of appropriate datasets, a custom set is created, suitable to develop\nand test visual features which are able to represent music related information. This dataset fa-\ncilitates the experiments presented in this thesis. The experiments include evaluations of visual\nfeatures concerning their ability to describe music related information. This evaluation is per-\nformed bottom-up from low-level visual features to high-level concepts retrieved by means of\nDeep Convolutional Neural Networks. Additionally, new visual features are introduced cap-\nturing rhythmic visual patterns. In all of these experiments the audio-based results serve as\nbenchmark for the visual and audio-visual approaches. For all experiments at least one audio-\nvisual approach showed results improving over this benchmark. The experiments are conducted\niii\nfor three prominent MIR tasksArtist Identiﬁcation, Music Genre Classiﬁcationand Cross-Genre\nClassiﬁcation.\nSubsequently, the results of the two approaches are compared with each other to establish\nrelationships between the described production processes and the quantitative analysis. Thus,\nknown and documented visual stereotypes such as the cowboy hat for American country mu-\nsic can be conﬁrmed. Finally, it is shown that an audio-visual approach harnessing high-level\nsemantic information gained from visual concept detection, leads to an improvement of up to\n16.43% over the audio-only baseline.\nKurzfassung\nDer Kontext dieser Arbeit liegt im interdisziplinaeren Forschungsgebiet Music Information Re-\ntrieval (MIR) und insbesondere im Teilbereich, welcher Informationen aus dem Audiosignal\nmittels digitaler Signalanalyse extrahiert. Da Musik in sich multi-modal ist, nutzen viele Ansaet-\nze mehrere Eingabemodalitaeten wie Audio, Liedtexte oder Musiknoten, um MIR-Forschungs-\naufgaben zu loesen. Diese Dissertation untersucht den Informationsgehalt des visuellen Kanals\nvon Musikvideos und wie dieser genutzt werden kann, um Aufgaben des Forschungsbereichs\nMIR zu erweitern und zu verbessern. Die grundsaetzliche Hypothese dieser Arbeit basiert auf\nder Beobachtung, dass bestimmte Ausdruckskategorien wie Stimmung oder Genre allein auf-\ngrund des visuellen Inhalts erkannt werden koennen, ohne dass die dazugehoerige Tonspur ge-\nhoert wird. Davon leitet sich die Annahme der Existenz einer visuellen Sprache ab, die ver-\nwendet wird, um Emotionen oder Genre auszudruecken. In weiterer Konsequenz kann gefolgert\nwerden, dass diese visuelle Information musikbezogen ist und daher fuer die entsprechenden\nMIR-Aufgaben wie die Klassiﬁkation von Musik nach Genre oder Genre-uebergreifenden The-\nmen wie Weihnachten oder Anti-Kriegs-Lied von V orteil sein sollte.\nDie Validierung dieser Hypothesen gliedert sich in einen analytischen und einen experi-\nmentellen Teil. Der analytische Ansatz basiert auf einer Literaturrecherche in den Forschungs-\nbereichen Musikwissenschaft und Musikpsychologie, um Studien oder Dokumentationen von\nProduktionsprozessen von Musikvideos oder Visual Branding im Musikgeschaeft zu identiﬁzie-\nren. Dabei wird die Geschichte der Nutzung visueller Attributierung untersucht, beginnend mit\nIllustrationen von Noten bis zur Musikvideo-Produktion. Dies verdeutlicht die Bedeutung des\nvisuellen Designs und dessen gezielte Nutzung durch die Musikindustrie, um neue Kuenstler zu\nfoerdern und Direktverkaeufe oder Marktwerte zu erhoehen. V or allem in der Zeit vor Online-\nStreaming-Diensten mussten Albumcover so ansprechend und erkennbar wie moeglich sein, um\nin den Regalen der Plattenlaeden aufzufallen. Besonders neue Kuenstler, deren Stil noch nicht\nbekannt war, wurden von Musiklabels optisch gebrandet und ausgestattet, um in Zeitschriften\noder im Fernsehen sofort durch ihren Look bezueglich Stil und Musikrichtung eingeordnet wer-\nden zu koennen.\nDer experimentelle Ansatz fuehrt eine Reihe umfassender Experimente und Evaluierungen\ndurch, die sich auf die Extraktion von visueller Information und deren Anwendung in verschie-\ndenen MIR-Aufgaben konzentrieren. Aufgrund des Fehlens geeigneter Datensets wird zuerst\nein Set erstellt, welches geeignet ist, visuelle Merkmalebeschreibungen zu entwickeln und diese\ndaraufhin zu testen, ob sie in der Lage sind, musikbezogene Informationen darzustellen. Dieses\nDatenset dient als Basis fuer die Experimente. Diese beinhalten Auswertungen visueller Eigen-\nschaften hinsichtlich ihrer Faehigkeit, musikbezogene Informationen zu erfassen. Diese Evalu-\nv\nierungen erstrecken sich von einfachen visuellen Features zu komplexen Konzepten, welche un-\nter Verwendung von Deep Convolutional Neural Networks extrahiert werden. Darueber hinaus\nwerden neue Features eingefuehrt, die rhythmische visuelle Muster erfassen. In all diesen Expe-\nrimenten dienen die audio-basierten Ergebnisse als Benchmark fuer die visuellen und audiovisu-\nellen Ansaetze. In allen Experimenten zeigen die Ergebnisse der audio-visuellen Ansaetze Ver-\nbesserungen gegenueber dieser Benchmark. Die Experimente werden fuer die MIR-Aufgaben\nKuenstler-Identiﬁkation, Musik Genren Klassiﬁkation und Genre-uebergreifende Klassiﬁkation\ndurchgefuehrt.\nAnschliessend werden die Ergebnisse der beiden Ansaetze miteinander verglichen, um Zu-\nsammenhaenge zwischen den beschriebenen Produktionsprozessen und der quantitativen Ana-\nlyse herzustellen. So koennen bekannte und dokumentierte visuelle Stereotypen wie der Cow-\nboyhut fuer amerikanische Country-Musik bestaetigt werden. Schliesslich wird gezeigt, dass ein\naudio-visueller Ansatz, der semantische Information auf hohem Niveau nutzt, die aus der visu-\nellen Konzepterkennung gewonnen wird, zu einer Verbesserung von bis zu 16,43% gegenueber\nder Nur-Audio-Basislinie fuehrt.\nContents\n1 Introduction 1\n1.1 Opportunities of audio-visual analysis . . . . . . . . . . . . . . . . . . . . . . 4\n1.2 Research Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n1.3 Structure of the Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n1.4 Key Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2 State-of-the-Art 15\n2.1 Music Information Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.2 Content Based Image Retrieval . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n2.3 Content Based Video Retrieval (CBVR) . . . . . . . . . . . . . . . . . . . . . 30\n2.4 Music Videos Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n2.5 Transfer Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n2.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n3 Audio-Visual Relationships in Music 33\n3.1 When Packaging Music Became asn Art-Form . . . . . . . . . . . . . . . . . . 34\n3.2 Music Videos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n3.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n4 Data 45\n4.1 The Million Song Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n4.2 The Music Video Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\n5 Audio-only Analysis of the MSD 67\n5.1 Million Song Dataset Experiments . . . . . . . . . . . . . . . . . . . . . . . . 68\n5.2 The Echonest Featuresets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\n5.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n6 Visually improved Artist Identiﬁcation 87\n6.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n6.2 Classiﬁcation Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\n6.3 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\n6.4 Audio Content Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\nvii\n6.5 Visual Content Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n6.6 Cross-Modal Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n6.7 Results and Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n6.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n7 Shot Detection for Music videos 101\n7.1 Transition-Types in Music Videos . . . . . . . . . . . . . . . . . . . . . . . . 102\n7.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n7.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n7.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113\n8 The Color of Music: Evaluating Color and Affective Image Descriptors 115\n8.1 Acoustic and Visual Features . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\n8.2 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n8.3 Results and Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n8.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\n9 High-level Visual Concepts and Visual Stereotypes 125\n9.1 Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n9.2 Audio Content Descriptors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\n9.3 Visual Object V ocabulary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\n9.4 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n9.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136\n10 Conclusions 139\n10.1 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\nA Publications 147\nBibliography 151\nviii\nCHAPTER 1\nIntroduction\n“An album is a whole universe, and the recording studio is a\nthree-dimensional kind of art space that I can ﬁll with sound. Just as\nthe album art and videos are ways of adding more dimensions to the\nwords and music. I like to be involved in all of it because it’s all of a\npiece. ”\n— Natasha Khan, Bat for Lashes, 2013\nIn the second part of the last century the visual representation has become a vital part of\nmusic. Album covers grew out of their basic role of packaging to become visual mnemonics\nto the music enclosed [125]. Stylistic elements emerged into prototypical visual descriptions\nof genre speciﬁc music properties. Initially intended to aide or sway customers in their deci-\nsion of buying a record, these artworks became an inﬂuential part of modern pop culture. The\n“look of music” became an important factor in people’s appreciation of music. In that period the\nstructure of the prevalent music business changed completely by shifting the focus from selling\nwhole records to hit singles [79]. Their accentuation and the new accompanied intensiﬁcation\nof celebrity around pop music played an important role in the development of thepinup culture.\nThe rise of music videos in the early 80s provided further momentum to this development. This\nemphasis on visual aspects of music transcended from topics directly connected with music pro-\nduction into aspects of our daily life. The relationship to fashion tightened, yielding to different\nstyles that discriminated the various genres. An early study on music video consumption among\nyoung adolescents revealed that one of the primary intentions was social learning such as how\nto dress, act and relate to others [280]. Over the past decades music videos emerged from a\npromotional support medium into an art form of their own which distinctively inﬂuenced our\npop-culture and became a signiﬁcant part of it. Still intended to promote new hit-singles and\nartists, contemporary music video productions typically make use of of ﬁlm making techniques\nand roles such as screen-play, directors, producers, director of photography, etc. How much\ninﬂuence visual music had on us over half a century is hard to evaluate, but we grew accustomed\n1\nFigure 1.1: Examples of music genres that are easy to identify in images - a) Dance, b) Rock, c)\nHeavy Metal, d) Rap\nto a visual vocabulary that is speciﬁc for a music style in a way that the genre of a music video\ncan often be predicted despite the absence of sound (see Figure 1.1).\nThe research ﬁeld concerned with the analysis and processing of music related information\nis called Music Information Retrieval (MIR) . Chapter 2.1 will provide a detailed introduction\nto types and sources of music information as well as a broad overview of the challenges and\ntasks the ﬁeld is facing. One of the central research topics within MIR is the analysis of the\naudio content of music recordings. As in many other Information Retrieval (IR) ﬁelds there\nis a discrepancy between the ambitious tasks a discipline is aiming to solve and the technical\nabilities available to approach them. This is commonly referred to as the Semantic Gap. For\naudio content based music analysis this discrepancy derives from the inability to accurately de-\nscribe the music content by means of a set of expressive numbers. These so called Features\nare the basis for a wide range of further processing steps which are necessary for tasks such as\nmusic classiﬁcation [80], similarity estimation or music recommendation [35]. The methods ap-\nplied draw from a wide range of well researched digital signal processing techniques. Through\nvarious transformations and aggregations the recorded audio signal is converted into numerical\ninterpretations of timbre, rhythm, music theoretic properties and various abstract descriptions\nof spectral distribution and ﬂuctuations [152]. Finally, these features are used to train machine\nlearning algorithms which are intended to label an unheard song by its genre, mood or estimate\nits similarity to other songs in a collection. Although content based music feature development\nhas made huge progress over the past two decades, the results for many tasks are still unsatis-\nfactory. Take music similarity estimation as an example. Music features already perform well in\ndescribing properties such as timbre and rhythm, and although “sounding similar” is an obvious\nqualiﬁcation for music similarity, there is a wide range of subliminal properties that cannot be\ncaptured or at least not without huge efforts. Such properties often fall into the domain of human\nperception and are generally difﬁcult to model computationally. The interpretation if a song is\nhappy or sad is highly subjective and culturally dependent [9]. But also more obvious attributes\nsuch as temporal relationships are relevant. A contemporary song may sound like a song by the\nBeatles, but explaining this relationship the other way around is an impossible causality. Nev-\nertheless, also the supposedly objective measure of acoustic similarity is problematic to asses\nwith state-of-the-art audio features. Completely similar sounding songs are usually rare due\nto implications about plagiarism. More frequent are tracks that share similar sounding parts\nsuch as verse or chorus. Often subjectively stated similarity only refers to the melody of the\nsinging voice. Common machine learning based approaches to acoustic similarity usually work\non a track based granularity and thus accumulate the similarity for an entire song. Analyzing\n2\nthe similarities based on sub-segments or structural elements of a composition would require to\nautomatically detect these segments, which is a different unsolved MIR task.\nA well researched [80, 153, 155, 164, 246, 249] and extensively discussed [275–277] task in\nMIR is automatic genre classiﬁcation. Music genre is an ambiguous mean which is supposed to\naide us in quickly ﬁnding new and relevant music. Back in the days when buying music was a\nsolely physical act, big genre labels on the record shelves made buying music less cumbersome.\nAn album does not reveal its content right away which is physically encoded onto synthetic\nmedia. If you wanted to pre-listen a record before buying, you had to queue up in front of\none of the few record players provided by the store. As customers usually are not blessed with\nunlimited time to iterate through this process - choose records, queue up, have a listen - over and\nover again, music genres are a convenient aide to short-cut customers to their preferred music.\nThey are synonyms which describe a wide range of music properties such as rhythm, expression\nand instrumentation in a single term. Although, nowadays we can virtually browse through on-\nline music catalogs without leaving the house and we can pre-listen to songs just by the click of\na mouse or by the tap on a smart-phone screen, music genres still remain relevant for describing\nand categorizing music. The major problem with genres is that on one side they are not well\ndeﬁned and on the other side artists do not comply with the existing deﬁnitions. Playing with\ngenre boundaries and mixing different styles is a common mean of artistic expression. This\ncomplicates MIR tasks such as automatic genre estimation but also music similarity retrieval.\nThe difﬁculty lies in the proper deﬁnition of audio features so that machine learning algorithms\nare able to effectively discriminate the various abstractions of spectro-temporal distributions of\nmusic data into different classes such as music genres. This is aggravated by the nature of\ndigital audio. The recorded audio signal is a representation of sound pressure levels sampled\nat sequential intervals at a virtually ﬁxed location. These sampled values can be transformed\ninto a spectral representation, but this only provides insight in which frequencies are dominant\nat the given moment. The problem is that the information about the source of the signal, such as\nguitars, violins or drums, is not available. Its estimation, also referred to as source separation is\na highly complicated research ﬁeld, especially in polyphonic environments.\nBesides music genres music labels courted the customers attention by making their records\nas visually appealing and descriptive as possible. As more extensively elaborated in Chapter 3\nthe music industry created and made use of a distinct visual language to promote new artists and\nrecords more efﬁciently. They made and make extensive use of visual artists, fashion and visual\nmedia and developed well known visual stereotypes for the various genres (see Figure 1.2). This\nvisual accentuation represents music related information which can be used to augment and\nimprove MIR tasks such as automatic genre classiﬁcation and artist identiﬁcation as well as to\nrealize new approaches to existing or newly identiﬁed tasks.\nApproaches to harness this music related visual information are currently scarce in literature.\nThis thesis represents a step towards the facilitation of audio-visual MIR approaches. First, a set\nof datasets are provided to facilitate experimentation. Then, a set of extensive experimentations\nand evaluations are presented which form the basis for the provided conclusions on the type and\nquality of information provided by music related artworks such as album cover arts and music\nvideos and how to harness this information for MIR tasks.\n3\n(a) Country Music\n (b) Dance Music\n(c) Heavy Metal\n (d) Classical Music\nFigure 1.2: Examples of visual stereotypes. Genre speciﬁc compilations and ’Best of’ albums\noften make use of visual stereotypes to advertise their content. Typical stereotypes such as the\nCowboy hat for Country Music (A), over-sexualization in Dance Music (B), death and ﬁre for\nHeavy Metal (C) and the display of classical instruments for classical music (D).\n1.1 Opportunities of audio-visual analysis\nThe inﬂuence and outreach of music videos is on a constant rise as recently reported. In 2011\nwe conducted a survey among decision makers and stakeholders in the music industry which\nshowed that YouTube1 is considered to be the leading online music service [164]. A different\nsurvey by Nielsen [201] involving 26,644 online consumers in 53 international markets revealed\nthat ’watching music videos on computer ’ was the most mentioned consuming activity, prac-\nticed by 57% of these consumers in the 3 months preceding the survey. 27% mentioned to also\nhave ’watched music videos on mobile devices’ in the same period. In a large scale user-survey\non requirements for music information services by Lee et al. [147] YouTube was listed as the\n1http://www.youtube.com/\n4\nsecond most preferred music service. Although the main usage scenario is identiﬁed as listening\nto music, music videos are mentioned to be highly valued by users. Stowell and Dixon [273]\nidentiﬁed YouTube as one of the most important technologies used in secondary school music\nclasses. The authors suggest to offer MIR analysis for videos to enable open-ended exploration.\nCunningham et al. [46] identiﬁed music videos as a passive trigger to active music search and\nstate that visual aspects of music videos can strongly inﬂuence the amount of attention paid to\na song. Liem et al. [165] strongly argue that there is a need for multimodal strategies for music\ninformation retrieval. The MIRES roadmap for music information research [257] identiﬁes mu-\nsic videos as a potential rich source of additional information. As an open research problem it\nrequires more attention of the audio/visual communities as well as appropriate data-sets.\nHarnessing the visual layer of music videos provides a wide range of opportunities and pos-\nsible scenarios. At ﬁrst hand it presents a new way to approach existing MIR problems such as\nclassiﬁcation (e.g. genre, mood, artist, etc.), structural analysis (e.g. music segmentation, tempo,\nrhythm, etc.) or similarity retrieval. On another hand it may provide new innovative approaches\nto search for music. One intention is to use visual stimuli to query for music. This would en-\nhance the traditional search spaces that are either based on audio content, textual or social data\nor a mixture of those. An investigation about the usage of music in ﬁlms and advertising [114]\nsuggests that a search function matching video features to music features would have potential\napplications in ﬁlm making, advertising or similar domains. But to use the visual domain for\nsearching music, it has to be linked to the acoustic domain. This section outlines objectives and\nobstacles of an integrated audio-visual approach to music. Challenges for a selected set of music\nresearch disciplines are discussed.\nThe following descriptions of opportunities for research tasks of various research domains\nare based on observations made during the accumulation and deﬁnition of the Music Video\nDataset which will be introduced in Chapter 4.2.\n1.1.1 Music Information Retrieval\nAt ﬁrsthand, harnessing music related visual information represents a valuable source for the\nMIR domain. Various research tasks could proﬁt from such an audio-visual approach.\nMusic Genre Recognition: Automatic genre recognition will be extensively discussed in\nChapter 2.1.4. It is generally concerned with assigning genre labels to recorded\nsongs, which is mainly achieved through the application of signal processing and\nmachine learning methods to the audio signal. The pros and cons concerning these\napproaches will be discussed in the next Chapter, but one of the major drawbacks\nis, that some music related information cannot be derived from the audio signal\nalone. Meta-Genres such as birthday, Christmas or road-trip songs do not share\nspeciﬁc harmonics or instrumentation which produces a signiﬁcant pattern in the\naudio signal. Still, they are often easy to categorize accordingly by human listeners.\nMusic marketing uses visual cues that are commonly associated with terms of such\nmeta-genres. Christmas albums and samplers are often held in the traditional colors\npine green and heart red or decorated with typical items such as Christmas holly, a\nChristmas tree or Santa Clause. Such visual cues are intentionally easy to identify\n5\nand mark the concerning album as Christmas music. Similarly, cakes and balloons\nare shown on covers for birthday party albums and roads and convertibles for road-\ntrip music. Further examples are hugging couples or sunsets for romantic music,\nsunny beaches and women in bathing suits for summer-hit compilations, etc. (see\nFigure 1.2). As a consequence the genre of music videos can often be recognized\nwithout hearing the corresponding song. Recent cross-modal approaches reported\nimproved accuracy in detecting similar visual concepts (e.g. sky, people, plant life,\nclouds, sunset, etc.) [113]. It has to be evaluated which cues are discriminative\nfor each genre and which state-of-the-art technology from the image processing\ndomain can be applied to reliably extract this information.\nArtist Identiﬁcation: Artist recognition is an important task for music indexing, browsing\nand retrieval. Using audio content based descriptors only is problematic due to the\nwide range of different styles on either a single album or more severe throughout\nthe complete works of an artist. Current image processing technologies (e.g. object\ndetection, face recognition) could be used to identify the performing artist within a\nvideo.\nMusic Emotion Recognition: Visual arts have developed a wide range of techniques to\nexpress emotions or moods. For example, some album covers for aggressive music\nshow images of tortured or rioting people, Buddhist symbols and people performing\nyoga exercises are often seen on relaxing albums. Another example is color which\nis a popular mean of expressing emotions. We have incorporated colors in our daily\nlanguage for expressing emotions (e.g. feeling blue), gravity of events (e.g. black\nFriday), political parties, etc. Movie directors often pick warm colors for romantic\nscenes while using cold ones for fear and anxiety. Songwriters use similar effects to\nexpress emotions. Major chords are perceived happy, minor chords sad, dissonance\ncreates an impression of dis-comfort, etc. Such techniques have to be identiﬁed,\nevaluated and modeled to harness their information for effective emotion recog-\nnition. It has to be evaluated to which extent the visual techniques of expressing\nemotions and those of music correlate. Such correlations may be used to combine\nthe acoustic with the visual domain.\nInstrument Detection: Instrument detection is an ambitious task currently suffering from\npoor performance of source separation approaches. Many music videos show the\nperforming artists playing their instruments. This information can be used to aug-\nment audio based classiﬁcation approaches.\nMusic Segmentation and Summarization: Music segmentation and summarization tries\nto identify coherent blocks within a composition such as verse or chorus. Typical\napproaches are based on self-similarities of sequentially extracted audio features.\nIf no timbral or rhythmical differences can be detected, segmentation fails. The\nstructure of a music video is often aligned to the compositional or lyrical proper-\nties of a track. Scenery or color changes depict the transition to another segment.\nSuch transitions can be detected [44] and analyzed. Repetitions of the same images\n6\nFigure 1.3: Example images as input for music search algorithms.\nthrough the video clip for example may suggest repetitions of sections such as the\nchorus.\nTempo and Rhythmic Analysis: Current beat tracking approaches [189] rely on strong\ncontrasting musical events to correctly estimate the track’s tempo. Shot editing\nof music video can serve as valuable information in cases where audio based ap-\nproaches fail. Further detecting and classifying dance motions can provide addi-\ntional data for rhythmic analysis.\nMusic Video Similarity: Deﬁnitions of similarity of music videos are still missing. It is\neven undecided if similarity should be based on acoustic or visual properties or on\na combination of both. Descriptions of music videos intuitively refer to visual cues.\nAn extensive discussion including results from user evaluations should precede at-\ntempts to measure similarity between music videos.\n1.1.2 General Objectives\nThis section envisions application scenarios that could be accomplished through an audio-visual\napproach which harnesses music related visual media.\nNew Innovative Music Search: Visual clues identiﬁed and extracted from music videos,\nsuch as correlations between color and music, may promote the development of\nnew search methods for music or sounds. Traditionally, textual or audio content\ndescriptions of seed songs are used to query for music. Although not applicable\nto all kinds of music, the intention for a search is often better described through\na picture. Based on insights from the suggested research, a music query using a\npicture of a glass of wine and burning candles might return a collection of romantic\nmusic.\nMusic Visualization: Identiﬁed audio-visual correlations can be used to improve music\nvisualization or automatic lighting systems. LED-based color-changing lights are\n7\ncurrently gaining popularity. An add-on to such systems could change the color\naccording to musical transitions and mood.\nMusic for Movies or Advertisements: An investigation about the usage of music in ﬁlms\nand advertising [114] suggests that a search function matching video features to mu-\nsic features would have potential applications in ﬁlm making, advertising or similar\ndomains. Understanding audio-visual correlations, especially in their cultural con-\ntext, facilitates recommender systems suggesting music for individual scenes in a\nmovie.\nAdaptive Music Recommendation: Similar to recommending music for videos, cameras\ncan be used to observe the ambiance of an area. Based on the visual input from the\ncamera, high level features such as number and motion of people, clothing, color of\nthe room, weather conditions, sunrise, sunset, etc. can be used to select appropriate\nmusic.\nParts of this introduction to Music Video Information Retrieval (MVIR) and its opportunities as presented in this and\nthe previous section was published and presented in Alexander Schindler. “A picture is worth a thousand songs:\nExploring visual aspects of music” at the 1st International Digital Libraries for Musicology workshop (DLfM 2014),\nLondon, UK, September 12 2014 [236].\n8\n1.2 Research Questions\nThis thesis is based on the observation that many music videos have distinct visual properties\nwhich facilitate the derivation of further music characteristics from them such as tempo, rhythm,\ngenre, mood, etc. (see Figure 1.4). This observation lead to the hypothesis on which this thesis\nis based on - that there are speciﬁc visual patterns which are connected with certain music char-\nacteristics. The overarching question of this thesis is, to which extent does visual information\ncontribute to the performance of MIR tasks? Because there exist recognizable repeating visual\npatterns the question is, does this information contribute to the performance of MIR research\ntasks? It has been shown, that adding further modalities such as text [143, 183] or symbolic\nmusic [155] to audio signal analysis, does improve the performance of these tasks. Except for a\nfew examples on small data-sets [182] this has not yet been shown for visual information on a\nbroad and generalized scale. This hypothesis lead to the development of the following research\nquestions:\nResearch Question 1 (RQ1):\nWhich visual features are able to capture task related information?\nThe main motivation of this research question is the identiﬁcation of appropriate infor-\nmation retrieval approaches to extract music and task related information from visual\nsources. This will require to research state-of-the-art image processing, video process-\ning and visual computing methods, as well as to develop custom features and to evaluate\ntheir effectiveness in capturing the relevant information. This research question will be\naddressed in Chapters 6, 8 and 9.\nFeature development and evaluation will be preceded by a literature search to identify\ndocumented processes in music production. The aim is to identify described correlations\nbetween visual and musical concepts which can be modeled by existing or to be\ndeveloped features. This research will be adressed in Chapter 3.\nResearch Question 2 (RQ2):\nHow can the different modalities be combined to improve performance?\nHaving identiﬁed appropriate visual features to extract music related visual information,\nthe next goal is to research approapriate methods to combine this information with those\nextracted from the acoustic layer. This research question will be addressed in Chapters 6,\n8 and 9 by evalting different approaches to fuse the different modalities in feature space\nas well as by ensemble methods.\n9\nFigure 1.4: Examples of genre speciﬁc music videos.\nResearch Question 3 (RQ3):\nTo which extent can these visual patterns be used to derive further music characteristics?\nThe effectiveness of an information retrieval system to capture the semantically relevant\ninformation is always dependent on the underlying task it will be evaluated on. Thus,\nto properly answer the previous question it is also necessary to assess to which extent\nthe extracted information can be used to predict further music related properties such\nas the corresponding genre or mood. To evaluate if the exploitation of the visual\ndomain provides enough additional information to show signiﬁcantly improvements\nover audio-only based approaches, the identiﬁed or new deﬁned visual features will be\nevaluated on well known Music Information Retrieval tasks. This research question will\nalso be addressed in Chapters 6, 8 and 9.\nResearch Question 4 (RQ4):\nIs it possible to verify concepts within the production process?\nHaving identiﬁed appropriate ways to extract semantically meaningful information from\nthe visual layer and how to consecutively apply further methods to predict certain music\nproperties, it should be analyzed if identiﬁed visual patterns reported in literature (see\nRQ1) can be veriﬁed through automated analysis. This research question will also be\naddressed in Chapter 9.\n10\n1.3 Structure of the Work\nChapter 1 provides a brief introduction to the problem domain, including the research\nﬁeld of music information retrieval, image processing, visual computing and\nmusic marketing. This chapter also introduces the problem statement and\ncorresponding research questions and outlines the opportunities of harnessing\nmusic related visual information.\nChapter 2 provides an extensive overview of the state-of-the-art in the concerned re-\nsearch domains, including music information retrieval, image processing and\nvisual computing.\nChapter 3 elaborates on audio-visual relationships in music by providing introductions\nto the history of album-cover-art and music videos. Further, common prac-\ntices in music marketing are explained and the importance of visual accentu-\nation is discussed. Finally, a detailed introduction into music video theory is\nprovided as basis for the experiments in the consecutive chapters.\nChapter 4 introduces the datasets that will be used in the experiments and evaluations.\nTwo new datasets are deﬁned which can be used to develop and evaluate\nmethods to harness the visual layer of music videos and album-cover-arts.\nFurther, ground-truth assignments to existing datasets are introduced.\nChapter 5 investigates charachteristics of the newly introduced datasets MVS and MSD,\nas well as provided feature-sets and ground-truth assignments. Supporting\nexperimental results are provided which are essential for the interpretation of\nthe main experimental results and serve as baselines as well.\nChapter 6 presents an audio-visual approach to the music information retrieval task\nArtist Identiﬁcation. State-of-the-art music and audio features are combined\nwith face recognition to an ensemble classiﬁer.\nChapter 7 addresses the unsolved problem of shot boundary detection in music videos.\nIt identiﬁes an extended list and examples of transition types frequently found\nin music videos and sets them in contrast to state-of-the-art approaches to shot\nboundary detection.\nChapter 8 provides an in-depth evaluation of primary, low-level and affective features\ntowards their effectiveness in music genre classiﬁcation tasks. This bottom\nup evaluation provides valuable insights into the complexity of the music\nrelated visual information.\n11\nChapter 9 investigates the capability of high-level visual concept detection approaches\nto capture music related visual information. Aggregations of accumulated\nsemantic concepts, extracted with deep convolutional neural networks, are\ncombined with music features to improve the accuracy of music genre clas-\nsiﬁcation.\nChapter 9 discusses results and insights of the conducted experiments, summarizes re-\nsults and draws conclusions. It further points to open issues, research possi-\nbilities and future work.\n12\n1.4 Key Contributions\nThe scientiﬁc contributions provide by this thesis are manifold but can be divided into the three\ncategories Data, Methods and Code:\nData:\n1. The Music Video Dataset (MVD): The MVD (see Chapter 4.2) is the ﬁrst dataset in\nthe context of Music Information Retrieval which facilitates audio-visual research\non a broad range of open MIR challenges such as music genre classiﬁcation, cross-\ngenre classiﬁcation, artist recognition and similarity estimation.\n2. Additional gound-truth assignments and an expanded set of audio- and music-\nfeatures are provided for the Million Song Dataset (MSD) to facilitate comprehen-\nsive benchmarking experiments on a large scale (see Chapter 4.1).\nMethods:\n3. Methods to select and aggregate features provided with the Million Song Dataset\n(MSD) based on extensive evaluations of their performance in classiﬁcation exper-\niments (see Chapter 5.2). This provides beneﬁts for facilitating large scale experi-\nments on the MSD.\n4. Methods using Deep Neural Networks (DNN) to improve music classiﬁcation per-\nformance (see Chapter 5). This model was subsequently used in the Detection and\nClassiﬁcation of Acoustic Scenes and Events (DCASE) 2016 evaluation campaign\nand was the winning contribution to the Domestic audio tagging task [159].\n5. Methods to harness audio-visual information to address the MIR research task of\nArtist Recognition (see Chapter 6).\n6. Methods to harness information provided by low-level image processing features in\nan audio-visual approach to music genre classiﬁcation (see Chapter 8).\n7. Methods to harness high-level visual concepts and music features in an audio-visual\napproach to music genre and cross-genre classiﬁcation (see Chapter 9).\n8. Surveys and positions on opportunities of audio-visual approaches to MIR (see\nChapter 1.1) as well as on open research problems in Shot-boundary Detection for\nmusic videos (see Chapter 7).\n13\nCode:\n9. The Music Video Information Retrieval - Toolseta: a Python framework to extract\nand merge acoustic and visual features from music videos. It implements all the\nfeatures introduced and used in this thesis.\n10. A conversion and optimization of the Matlab based Rhythm Patterns feature-set\nextractor to the Python programming languageb.\nahttps://github.com/slychief/mvir\nbhttps://github.com/tuwien-musicir/rp_extract\n14\nCHAPTER 2\nState-of-the-Art\nIn the course of this dissertation, the state-of-the-art of the involved research domains has un-\ndergone a signiﬁcant transition from traditional feature-engineering based to data-driven deep\nlearning based approaches. This transition shows its most outstanding impacts in the visual\ncomputing domain where Top-5 error rates in well established international evaluation cam-\npaigns suddenly dropped from 26.2% to 15.4% through the application of Deep Convolutional\nNeural Networks in 2012 [140] and sequentially then dropped to 4.9% [281]. This astounding\nimprovement initiated the hype aroundDeep Learning. The reason why this hype is still ongoing\nis, that many neural network based approaches live up to their expectations. Huge performance\nimprovements are reported from various application domains. Different network architectures\nare reported regularly and adaption to different modalities such as text or speech also provide\noutstanding results. Neural networks have also gained the attention of the Music Information\nRetrieval (MIR) community. Several attempts are reported to deﬁne appropriate network archi-\ntectures to efﬁciently learn music representations.\nThe research focus of this thesis is embedded in the Music Information Retrieval (MIR)\ndomain. Thus, this chapter ﬁrst provides a brief introduction into the MIR domain and then\ndiscusses more detailed related work and the state-of-the-art on multi-modal approaches for\ngeneral and MIR speciﬁc problems.\n2.1 Music Information Retrieval\nMusic Information Retrieval (MIR), a subﬁeld of multimedia retrieval, is a multidisciplinary re-\nsearch ﬁeld drawing upon the expertise from information retrieval, library science, musicology,\naudio engineering, digital signal processing, psychology, law and business [58]. This relatively\nnew discipline gained momentum in the early 2000s and is supported by an eager research com-\nmunity including the International Society for Music Information Retrieval (ISMIR) with its\nhomonymous annual conference.\nA general concern of the MIR research ﬁeld is the extraction and interpretation of mean-\ningful information from music. That this is a challenging endeavor can be seen by the different\n15\nmusic representations such as audio recordings, music scores, lyrics, playlists, music videos,\nalbum cover arts, music blogs, etc. Music sheets contain formal descriptions of music composi-\ntions. Audio recordings capture the information about the actual interpretation of a composition.\nLyrics and music videos add further layers of artistic and semantic expression to a music track.\nPlaylists set different tracks into relation and blog posts contain opinions about music, artists\nand performances. All this relevant information comes in different modalities. Recordings are\nprovided as digital audio requiring digital signal processing to extract the information. Lyrics\nand blog posts require text and natural language analysis. Music videos and album cover art\nanalysis require image processing and computer vision techniques. Mining playlists and social\nmedia data requires knowledge of collaborative ﬁltering methods. Finally, it has proven to be\nadvantageous to combine two or more modalities to harness the information provided by the dif-\nferent layers. Given that wealth of music information one of the main tasks is to reduce as much\ninformation and to retain the most relevant notion of it to provide fast and accurate methods of\nindexing music and to provide new and intuitive ways to search and retrieve music.\nCompared to image processing, MIR is a relatively young research domain. It started in the\n90s of the last century but gained on momentum with the launch of the International Symposium\nfor Music Information Retrieval (ISMIR) in October of 2000 [60], which was renamed in 2008\nto the conference of the International Society for Music Information Retrieval, while retaining\nthe acronym. This change went hand in hand with the founding of the homonymous research\nsociety. One of the stepping stones to MIR research was the study by Beth Logan on applying\nMel Frequency Cepstral Coefﬁcients (MFCC), an audio feature which emerged from the speech\nrecognition domain, to the digital music domain [172]. The next milestone was the ISMIR 2004\nAudio Description Contest (ADC)[32] - an international evaluation campaign where researchers\ncould compete in ten different tasks such as audio ﬁngerprinting, music genre classiﬁcation or\nmusic structure analysis. Based on the experiences made at the ADC the Music Information\nRetrieval Evaluation Exchange (MIREX) was initiated in 2005 [62], which has then become the\nannual evaluation campaign of ISMIR [61].\n2.1.1 What is Music Information?\nThe Oxford Dictionary deﬁnes music as ’the art of combining vocal or instrumental sounds (or\nboth) to produce beauty of form, harmony, and expression of emotion’[1] and thus refers mainly\nto the acoustic nature of it. The ﬁeld of MIR considers a much broader perspective on music\ninformation. The following section will thus concentrate on the different modalities and types\nof music information.\nRecorded Audio - This might be the most palpable form of music. Referring to the cited\ndeﬁnition of the Oxford Dictionary, music is a combination of vocal or instrumental sounds.\nIn a more general form this is referred to as a performance. Thus, when we speak of music\nwe generally refer to recorded performances whose copies are distributed by resellers or played\non radio stations. The history of recorded music started in 1896 when Thomas Alva Edison\nestablished the National Phonograph Company to sell music pressed on wax cylinders for the\nphonograph. In 1901 the Victor Talking Machine Company introduced the phonographs whose\n16\n78-rpm ﬂat disc deﬁned the standard for the future development of music distribution. This\nfurther lead to the development of the Long Play (LP) album which was replaced by the cassette\nin the early 80s of the last century. With the introduction of the Compact Disc (CD) a major shift\nto digital music production and consumption was performed. Digital audio is the main source\nfor content based audio analysis which is one of the major sub-ﬁelds of MIR. We will introduce\nthis in more detail in the following chapters.\nSymbolic Music - Symbolic music is a representation of music in a logical structure based\non symbolic elements representing audio events and the relationships among those events. The\nmost commonly known form of symbolic music is the music score which is also referred to as\n‘Sheet Music’. Music scores are available as printed sheets or in various digital representations\nsuch as MIDI [222], MusicXML [83], ABC [301] or GUIDO [105]. The most powerful among\nthem is MusicXML which was released ﬁrst in 2004 and describes apart from the composition\nalso the layout of the score, lyrics and textual annotations and further features. The main dif-\nference between symbolic music and recorded or perceived music is, that scores only describe\nhow a piece of music is intended to be performed. In other words: It is a guideline that is used\nby a performer to play a certain piece of music. For MIR, symbolic music has some advanta-\ngeous properties compared to recorded music. The most obvious is the exact knowledge about\nnotes and their durations. Such information needs to be estimated from the audio spectrum of a\nrecorded music track which is a complex and not yet matured task.\nText - Text based music related information is presented in form of music lyrics, music meta-\ndata such as title, album and artist names, textual information provided on artist web pages, blog\nposts, music and album reviews or artist biographies, as well as from social tagging platforms\n(e.g. last.fm1, MusicBrainz2, etc.) and Wikipedia3. To harness such information, classical infor-\nmation retrieval is applied to the text extracted from different sources which was collected from\nthe Internet by querying search engines [135], monitoring MP3 blogs [37] or crawling music\nweb-sites [306]. Most systems proposed in literature use vector space document representations\nsuch as the Term Frequency-Inverse Document Frequency (TF-IDF) representation [136, 307].\nCollaborative Information - Music as a socio-cultural phenomenon also generates informa-\ntion through interaction. This includes data such as listening and buying behaviors or inter-\nactions between users and groups on social media. Such information is often used in music\nrecommendation systems. Collaborative ﬁltering techniques are applied to estimate relation-\nships between tracks and listeners [36, 230, 230]. Such techniques are best known from popular\nrecommendation systems such as Amazon 4 or Netﬂix5. They depend on a large community of\nactive users to track their habits of interacting with items such as music and to infer similarities\nbetween these items or users and to estimate user-items preferences. Thus, the combination of\n1http://www.last.fm\n2http://musicbrainz.org\n3http://www.wikipedia.org\n4http://www.amazon.com\n5http://www.netflix.com\n17\nsongs that users have listened to or rated provide information about these user’s preferences and\nor ’tastes’ and how these songs relate to each other. Data gathered from social media and mi-\ncroblogs [98] can also be used to reveal latent social and collective perspectives on music [115]\nby analyzing geographic, temporal, and other contextual information related to listening histo-\nries.\nVisual Media - Visual based music related information is provided by album cover arts, mu-\nsic videos and music advertising. The visual representation has become a vital part of music.\nStylistic elements are used as prototypical visual descriptions of genre speciﬁc music properties.\nThe rise of music videos in the early 80s provided further momentum to this development. Mu-\nsic marketing uses these visual concepts to promote a new artist. Genre speciﬁc visual cues are\nprovided to express its music style or genre. There also exists a strong relationship with fashion.\n2.1.2 Music Information Retrieval Approaches\nBefore giving a brief introduction to a selection of actively researched MIR tasks, a general\noverview of common methods will be provided. Generally, MIR approaches can be divided into\ntwo categories - those relying on music content and those on contextual data.\nContent Based Approaches\nContent based approaches are based on the idea that the semantically relevant information is\nprovided in the audio itself. Thus the aim is to identify and extract this information in a mean-\ningful representation. The biggest problems of such approaches stem from the digital nature\nof the recorded audio signal as well as from the standard music distribution format. Although\ninstruments in contemporary music productions are recorded separately, they are ﬁnally mixed\ntogether into one (mono) or two (stereo) channels. As a consequence in the resulting acoustic\nsignal the originating audio sources cannot be distinguished anymore or have to be separated\nwith great efforts [208].\nAudio signals as perceived by our ears have a continuous form. Analog storage media were\nable to preserve this continuous nature of sound (e.g. vinyl records, music cassettes, etc.). Digital\nlogic circuits on the other hand rely on electronic oscillators that sequentially trigger the unit to\nprocess a speciﬁc task on a discrete set of data units (e.g. loading data, multiplying registers,\netc.). Thus, an audio signal has to be fed in small chunks to the processing unit. The process\nof reducing a continuous signal to a discrete signal is referred to as sampling. The continuous\nelectrical signal generated in the microphone is converted into a sequence of discrete numbers\nthat are evenly spaced in time as deﬁned by thesampling rate. This process of turning continuous\nvalues into discrete values is called quantization. The resulting digital signal represents the\nchanges in air pressure over time. For digitizing audio especially music in CD quality, typically\na sampling rate of 44.100 Herz at a bit depth of 16 is used.\nFeature extractionis a crucial part of content-based approaches. The goal is to transform and\nreduce the information provided by the digital audio signal into a set of semantically descriptive\nnumbers. A typical CD quality mainstream radio track has an average length of three minutes.\nThis means, that song is digitally described in Pulse-code Modulation (PCM) by 15.9 million\n18\nnumbers (3 [minutes] x 60 [seconds] x 2 [stereo channels] x 44100 [sampling rate]). Using\nCD-quality 16bit encoding this information requires 30.3MB of memory. Besides music spe-\nciﬁc reasons, the computational obstacles concerned with processing of huge music collections\nmake raw audio processing a suboptimal solution. Feature design and implementation tries to\novercome technological obstacles of digital audio and to extract essential music properties that\ncan be used to analyze, compare or classify music. Common music features are descriptors for\ntimbre [172], rhythm [163] or general spectral properties [142,292]. Because the audio signal is\nonly a description for changes in air pressure over time, spectral analysis such as Fourier trans-\nformations is applied. Audio and music features are then calculated from the resulting frequency\ndistributions by subsequently applying further transformations and statistical modeling [33].\nRecently, feature development has been philosophically challenged. From a pure musicolo-\ngist point of view the validity of task-oriented approaches of most solutions reported in literature\nis in question [278, 308]. It is argued that the designed features capture sound properties that\nare not related to music itself, but to artifacts of the production process and that the evaluation\nmethods applied are inappropriate [279, 295]. From a machine learning point of view it seems\ncounterproductive to neglect valuable information due to the fact that it is not directly related to\nmusic - especially if it increases the performance of the system in solving a task.\nA major obstacle of content-based approaches is its dependency on the access to the actual\nmusic ﬁle. Due to copyright and intellectual property restrictions music cannot be distributed\nwithout permission [188]. This restricts reproducibility of research results, because privately\nassembled test collections cannot be shared [59]. Only a few datasets are available for each of the\ncorresponding MIR tasks, but their sizes, ranging from a few hundred to a few thousand tracks,\nare often not representative in consideration of contemporary on-line music services which host\ncatalogs of several millions of tracks.\nContext Based Approaches\nContrary to content-based approaches context-based ones [137] do not rely on the music content\nitself, but on contextual data which is not included in the audio signal. The most common type\nof information is meta-data embedded in the music ﬁle such as artist name, title, album name,\nlength of the track, etc. The advantage of context-based approaches is their independence from\nthe access to the music ﬁles. Further, the distribution of music meta-data does not constitute a\ncopyright infringement [188]. The down-side of this approach is, that it relies on the existence\nof proper meta-data and that its absence entails the in-existence of the corresponding track.\nIn recent years several on-line music-platforms such as Last.fm 6, MusicBRainz 7, Echonest 8,\nSpotify9 or Europeana10 in the cultural heritage domain, are providing access to their meta-data\ncatalogs. These are valuable sources of semantically relevant music information.\nText-based approachesretrieve contextual music information from web pages, user tags, or\nsong lyrics and apply methods from traditional Information Retrieval (IR) and Natural Language\n6http://last.fm\n7http://www.musicbrainz.org\n8http://the.echonest.com\n9http://www.spotify.com\n10http://www.europeana.eu\n19\nProcessing (NLP). Collaborative tags are user generated annotations for on-line content such as\nimages, videos, audio-tracks, articles, etc. Participants of on-line or social media platforms are\nusually free to deﬁne the labels themselves. Such tags range from brief semantic content descrip-\ntion such as the music genre or used instrumentation to lengthly expressions of appreciation and\ndislike. Song Lyrics are part of music composition and thus comprise semantically relevant in-\nformation about the song’s topic, genre, mood, etc. Co-occurrence based approaches are based\non the assumption that objects appearing within the same context are similar. Tracks contained\nin the same playlist or artists mentioned within the same article supposedly share similar prop-\nerties. Micro-blogging services such as Twitter11 or peer-to-peer (p2p) networks provide further\nrich sources for mining music-related data.\n2.1.3 MIR Tools and Datsets\nOne of the biggest obstacles of MIR research was and is the impeded access to data. Copy-\nrighted music must not be shared without license clearing which usually implies high monetary\ncosts. Consequently, only a few datasets emerged over the past decades. Still, some of them\nare not properly licensed and are shared between research groups on the quiet. One of the ﬁrst\ndatasets available to the community and relevant for this thesis is the George Tzanetakis Dataset\n(GTZAN) [292] which was introduced by his PhD thesis along with the famous audio feature\nextraction framework MARSY AS in 2000. This was one of the ﬁrst datasets for Music Genre\nRecognition (MGR) tasks and has since then been used to evaluate countless approaches to this\ntask [274]. Further MGR datasets were introduced with the ADC in 2004 [32], theISMIR Genre\ndataset12 and the ISMIR Rhythm dataset13 (also known as the Ballroom Dataset). Together\nwith the Latin Music Dataset (LMD) [267] released in 2008, these four collections have long\nbeen the main benchmark datasets in MIR to test new audio features or classiﬁcation systems\non. The biggest ﬂaw of these datasets was their number of tracks, which ranged from 1000 to\n3227 tracks. Because music catalogs of on-line music distributors were already on the scale of\nmillions of tracks, it became problematic to generalize results presented on datasets of a few\nthousand tracks. Thus, in 2011 the Million Song Dataset (MSD) [19] was released. Initially,\nthe dataset was only a large collection of meta-data and pre-extracted proprietary audio features\nwhich was further lacking of appropriate ground-truth assignments to approach the various MIR\ntasks. The MSD but contains identiﬁers which can be used to access different on-line-music\nrepositories from which some of them provide access to audio samples of the corresponding\ntracks. An effort to use these audio samples and further on-line resources to extend the MSD by\nstate-of-the-art music features and ground-truth assignments is presented in Chapter 4.1. These\nadditions were well received and an improvement of the genre-assignments was introduced in\n2015 [256]. Most recently a new dataset with 77.643 creative commons full audio tracks, down-\nloaded from the Free Music Archive14 was released [14].\nBesides the introduction of benchmark datasets which can be used to develop new ap-\nproaches and compare their performance with previously reported results, the provision of tool-\n11http://twitter.com\n12http://ismir2004.ismir.net/genre_contest/\n13http://mtg.upf.edu/ismir2004/contest/tempoContest/node5.html\n14http://freemusicarchive.org/\n20\nsets, frameworks and libraries for audio feature extraction and machine learning algorithms pro-\nvided further momentum to the development of the MIR research ﬁeld. One of the ﬁrst toolsets\nmade available was MARSYAS15 [292], which implemented among others the MFCC audio fea-\ntures [172]. The provision of code and data [292] has the advantage that reported results are fully\nreproducible and thus also reliably comparable to other approaches. Further tool-sets which im-\nplemented a wider range of then well established music features was the MIR-Toolbox16 [142]\nfor Matlab17 and jmir18 [185] for the Java. Recently, the Python 19 programming language en-\njoys great popularity, possibly due to numerous scientiﬁc computing libraries and the interac-\ntive IPython/Jupyter Notebook20 environment, which facilitates structured and well documented\nexperimentation. Recently a couple of MIR libraries were announced which provide imple-\nmentations of a wide range of state-of-the-art and bleeding edge audio features and machine\nlearning tools. These libraries are LibRosa21 [184], Essentia22 [22] and MadMom23 [21]. To-\ngether with my colleague Thomas Lidy, we also translated the Rhythm Patterns feature extractor\nrp-extract [156] from its Matlab implementation to Python and made it publicly available24.\n2.1.4 Music Information Retrieval Tasks\nThe manifold nature of music is also reﬂected in the variety of research tasks that emerged\nin the domain of Music Information Retrieval. The following sections give a few prominent\nexamples including those addressed by the experiments and evaluations which will be elaborated\nin Chapters 6 to 9.\nMusic Genre Recognition (MGR)\nMusic Genre Recognition is a well researched MIR [58] task. As for many other content-based\nMIR tasks the algorithmic design consists of the two parts. In a ﬁrst step audio-content de-\nscriptors are extracted from the audio signal. A comprehensive overview of music features is\nprovided by [142, 153, 293]. In a second step these features are used to train machine learning\nbased models, using popular supervised classiﬁers including k-nearest neighbors (k-NN), Gaus-\nsian mixture models (GMM) or Support Vector Machines (SVM). Comprehensive surveys on\nmusic and genre classiﬁcation are provided by [80, 228].\nInitial approaches to MGR used a variety of low-level audio features [4, 292, 293]. An\nextensive evaluation of these features, which consisted mostly of spectral shape descriptions and\nMFCCs, was presented in [213]. A ﬁrst survey of MGR approaches was published in 2006\n[228]. The most commonly used audio features could be categorized into the three groups of\n15http://marsyas.info/\n16https://sourceforge.net/projects/mirtoolbox/\n17https://www.mathworks.com/products/matlab.html\n18http://jmir.sourceforge.net/\n19https://www.python.org/\n20http://jupyter.org/\n21https://github.com/librosa/librosa\n22https://github.com/MTG/essentia/\n23https://github.com/CPJKU/madmom\n24https://github.com/tuwien-musicir/rp_extract\n21\nFU et al.: A SURVEY OF AUDIO-BASED MUSIC CLASSIFICATION AND ANNOTA TION 305\nTABLE I\nSUMMARY OF COMMON LOW-LEVEL FEATURES USED IN MUSIC CLASSIFICATION\nFig. 2. Illustration of steps for low-level feature extraction.\nrelated to spectral analysis of the audio signal and follows some\nstandard steps, as illustrated in Fig. 2. A song is usually quite\nlong in duration and may last for minutes. Hence, the input\naudio signal for each song may contain millions of samples\ngiven high sampling frequency of over 10 kHz. Instead of doing\nsong-level signal analysis directly, a song is usually split into\nmany local frames in the ﬁrst step to facilitate subsequent frame-\nlevel timbre feature extraction. This has two main advantages.\nFirstly, it leads to a more efﬁcient feature extraction process,\nsince we only need to apply spectral analysis to short-time sig-\nnals. Secondly, it is more effective to model the timbre features\nin frames with 10–100 ms duration. Due to the non-stationary\nnature of music, the audio signal of a song is normally a non-sta-\ntionary time series whose statistical properties depend on time.\nBy applying frame-level feature analysis, we can assume that\nthe audio signal within each frame is stationary. Thus, the un-\nderlying timbre information can be more reliably captured.\nAfter framing, spectral analysis techniques such as fast\nFourier transform (FFT) and discrete wavelet transform (DWT)\nare then applied to the windowed signal in each local frame.\nFrom the output magnitude spectra, we can deﬁne some sum-\nmary features such as spectral centroid (SC), spectral rolloff\n(SR), spectral ﬂux (SF), and spectral bandwidth (SB) capturing\nsimple statistics of the spectra. Hereafter, we term the collec-\ntion of these features as short time Fourier transform (STFT)\nfeatures. To extract more powerful features such as MFCC,\nOSC, DWCH, and MPEG-7 audio descriptors like SFM, SCF,\nand ASE, a subband analysis is performed by decomposing the\npower spectrum into subbands and applying feature extraction\nin each subband. The details of subband conﬁguration and\nfeature extraction vary for different types of features. Nor-\nmally, the subbands are arranged with logarithmically spaced\nspectral ranges (OSC, ASE, DWCH) following the constant\nQ-transform [82]. For MFCC, the subbands are linearly spaced\nin lower frequencies\nand logarithmically spaced\nin higher frequencies. The reason for using log-scale at the\nhigher frequency range is to mirror the human auditory system,\nwhereby ﬁner spectral resolution is achieved at lower frequen-\ncies. The arrangement was empirically found to be a reasonable\ntrade off, given the success of MFCC feature for music clas-\nsiﬁcation. Notice for each of these features, we can construct\nthe corresponding derivative feature by taking the ﬁrst-order or\nsecond-order differences in feature values between the current\nframe and previous frames. In this way, the feature set can be\nexpanded solely based on the use of low-level features [10].\nA number of issues on the use of timbre features deserve our\nspecial attention. Firstly, the local windows used in timbre fea-\nture extraction are usually taken from ﬁxed-size intervals re-\ngardless of the underlying musical events. It would be desirable\nto split the signal into unequal intervals corresponding to sepa-\nrate notes where the boundaries are aligned with the onset and\noffset of the notes [8]. Secondly, the bank of band-pass ﬁlters\nis usually organized to cover octave-based logarithmic spectral\nrange. Different implementations of ﬁlter bank adopt slightly\ndifferent spectral ranges and pass bands. These would inﬂu-\nence the classiﬁcation performance in a data-dependent way.\nThirdly, psycho-acoustic transformations are usually applied to\nnormalize the spectrum and ﬁlter output values [3]. They have\nbeen shown to contribute positively to genre and mood classiﬁ-\ncation [3], [83]. Fourth, phase information is usually discarded\nfrom the Fourier spectrum, but it may convey important infor-\nmation on the music being analyzed. The utility of phase infor-\nmation for music classiﬁcation remains an open problem. More-\nFigure 2.1: Overview of common low-level audio features used in music classiﬁcation (taken\nfrom [80]. Citations in the table refer to the references of the original paper.)\nTimbre, Melody/Harmony and Rhythm descriptors. The most commonly used classiﬁers were K-\nNearest Neighbor (KNN) [292], Gaussian Mixture Models (GMM) [30], Hidden Markov Model\n(HMM) [227] and Support Vector Machines (SVM) [153]. Besides providing an overview of\nstate-of-the-art audio features and classiﬁcation approaches, this survey also summarizes the\ngeneral discussions about MGR.\nThese discussions are still ongoing, because MGR is seen controversially. The one side,\nmostly coming from signal processing or machine learning background argues, that the aim is\nto create descriptors which are able to capture the most relevant semantic information of a given\ncontent in respect to a given task. Recognizing the genre of an unknown song is thus a prime\nexample of a machine learning approach. The controversy about this is, that MGR is often\nused to evaluate the performance of audio features with respect to their capability to describe\nmusic [276]. Genres are therefor used because they are synonyms which group several music\nproperties such as instrumentation, rhythm, moods, political and religious positions, even histor-\nical epochs. This is a convenient but controversial simpliﬁcation of the problem space. Instead\nof evaluating if the features are able to capture the music properties, they are evaluated if they\nare able to group songs which share the same properties. In many cases this is a sufﬁciently\nappropriate approach. Yet, from a scientiﬁc point of view, there are issues such as at which\nlevel genre labels do apply - at artist, album or track level? Further it is now generally agreed\nupon that there is no general agreement on genre taxonomies and genre assignments [7, 206].\nFurther, many genre labels are ill-deﬁned, locally skewed through cross-cultural differences in\ngenre perception [264] and only a minimal agreement can be reached among human annota-\ntors [186]. Based on the personal listening preferences the choice of individual genre labels\n22\ncan differ as well as the pool of known genre labels. A study on human genre classiﬁcation\nshowed an inter-participant agreement rate of only 76% [168]. Another issue with the simpli-\nﬁed assumption that genres group music characteristics is that most genres have their speciﬁc\nproduction processes which include speciﬁc recording and post-processing steps. It has been\nshown that MGR approaches are prone to capture artifacts of these processes to optimize the\ngenre boundaries instead of learning on actual intrinsic music properties [277]. On the relatively\nsmall-sized available datasets it was soon argued that a glass ceiling has been reached [205] and\nconcerns were raised if further pursuing MGR research is still reasonable [186]. In this thesis\ngenre classiﬁcation is used to demonstrate the performance of the developed visual and audio-\nvisual features. The aim is not to insist on recognizing the correct genre but rather to use the\nsame methodology frequently used in literature to provide comparable results, by classifying\nmusic videos by artiﬁcially assigned labels which refer to common acoustic properties (see Sec-\ntion 4.2.1). Through isolated experiments on classes that have clearly deﬁned boundaries based\non isolated stylistic elements the rational of this study is to identify and capture extramusical\nconcepts related to music genres [264, 276].\nA survey from 2011 [80] provides a comprehensive overview of the state-of-the-art in music\nclassiﬁcation. Their elaborate summary of common low-level audio features is depicted in Fig-\nure 2.1. This table is cited to give a comprehensive overview. The citation numbers within the\ntable refer to the references of the cited paper. For further information please refer to the original\npublication. Figure 2.2 further provides a good overview of state-of-the-art genre classiﬁcation\nsystems including comparative performance values (measured in classiﬁcation accuracy) on the\nGTZAN dataset. Among the most relevant and top-performing music features for MGR are the\nBlock-Level Features [259, 260] and the Rhythm-Patterns feature family [153, 157]. Both are\nable to capture music related properties such as timbre and rhythm effectively, which is also\ndemonstrated in the preceding experiments (see Chapter 5.2).\nMost recently the attention of the MIR community was brought to Deep Neural Networks\n(DNN) due to their sensational successes in the visual computing domain. Despite their out-\nperforming results, which will be discussed in detail in Section 2.2, approaches of the MIR\ndomain are still attempting to ﬁnd the appropriate architecture to learn music descriptions which\noutperform conventional hand-crafted music features. This is also the conclusion of a recent\ncomparative study [43] which provides an evaluation of Convolutional Neural Networks (CNN)\nfor music classiﬁcation. They suggest to use ensembles of conventional and neural network\nbased approaches. Most DNN based systems use segments of raw or Mel-Log-transformed\nspectrograms as input for the network [214]. The Mel-transform as pre-processing step is often\napplied to rescale a higher dimensional output of a Short-Term Fourier Transform (STFT) to a\nlower number of frequency bands [261]. Different architectures have been proposed to modeling\ntemporal [216] or timbral features [217] using CNNs.\nBased on our own experiments with DNNs [243] where we compared shallow versus deep\nneural network architectures for automatic music genre classiﬁcation, or our task-leading 2016\nMIREX [160] or DCASE [159] contributions, we come to a similar conclusion. DNNs are\nalready outperforming most of the distinct handcrafted audio features, but not approaches using\ncombinations of different audio content descriptors. Table 2.1 summarizes the top-performing\nclassiﬁcation results of state-of-the-art hand-crafted feature-sets with prediction accuracies of\n23\n310 IEEE TRANSACTIONS ON MULTIMEDIA, VOL. 13, NO. 2, APRIL 2011\nTABLE II\nPERFORMANCE COMPARISON OFGENRE CLASSIFICATION ALGORITHMS ON THESTANDARD GTZAN DATA SET\ntensor factorization (NTF), is also used in music genre classi-\nﬁcation for input tensor features and has demonstrated the best\nperformance when combined with speciﬁc features and clas-\nsiﬁers [19]. We will mention this approach in more details in\nSection III-A4.\n4) Feature Combination and Classiﬁer Fusion:If multiple\nfeatures are available, we can combine them in some way\nfor music classiﬁcation. Feature combination from different\nsources is an effective way to enhance the performance of music\nclassiﬁcation systems [12], [13], [39], [123]. A straightforward\nway to feature combination is to concatenate all features into a\nsingle feature vector, as shown in [1] and [12] for combining\ntimbre with beat and pitch features. Feature combination can\nalso be integrated with classiﬁer learning. Multiple kernel\nlearning (MKL) [124] is one such framework developed partic-\nularly for SVM classiﬁers. The purpose of MKL is to learn an\noptimal linear combination of features for SVM classiﬁcation.\nMKL has recently been applied to music classiﬁcation and\nfound to outperform any of the single feature types [56], [123].\nAs an alternative to feature combination, we can also per-\nform decision-level fusion to combine multiple decisions from\ndifferent classiﬁers. There are many ways to perform decision-\nlevel fusion, including majority voting, sum rule which takes the\naverage of decision values returned by individual classiﬁers, etc.\nA more general framework is established by the technique of\nstacked generalization (SG) [125], which provides a cascaded\nframework for classiﬁcation by stacking classiﬁers on top of\nclassiﬁers. In the SG framework, classiﬁers at the ﬁrst level\nare trained on individual features, and classiﬁers at the second\nlevel are trained by using the decision values returned by level-1\nclassiﬁers as new features. Hence, SG obtains the fusion rule\nthrough supervised learning. The choice of classiﬁers used for\nSG is quite ﬂexible. Normally SVMs are used within SG for\noptimized performance as in [123] and [126]. Different com-\nbination strategies have been studied in [123], showing that SG\nand MKL achieve the best performances for multi-feature music\ngenre classiﬁcation, outperforming other existing methods by\na signiﬁcant margin. Another important class of feature com-\nbination methods is based on ensemble methods for classiﬁca-\ntion. One such example is AdaBoost with decision trees (Ad-\naBoost.DT) [9], [27], which combines decision tree classiﬁers\nwith the boosting framework [127]. Each decision tree classiﬁer\nis trained on a single type of feature.\nB. Classiﬁcation Tasks\nNow we review the usefulness of features and classiﬁers, and\ndiscuss current issues and challenges for each individual task\nlisted in Section I.\n1) Genre Classiﬁcation: Genre classiﬁcation is the most\nwidely studied area in MIR since the seminal work of Tzane-\ntakis and Cook [1]. They also provided a publicly available\nbenchmark data set with 1000 songs in raw audio format evenly\ndistributed in ten genres. Other publicly available data sets for\ngenre classiﬁcation include the ISMIR 2004 data set\n4 and the\nDortmund data set [128]. The availability of public data sets\nhas made it possible to compare different approaches for the\nperformance of genre classiﬁcation on an equal basis. Much\nwork has been done in this area [2]–[9], [11], [12], [14]–[19].\nHere, we have listed a few representative methods in Table II\nand reported the features and classiﬁers used, and the accuracy\nrates achieved by each method. The plus sign in the table de-\nnotes multiple features used and the multiplication sign denote\nsequence of operations for feature extraction. For example,\n“\n” in the feature\ncolumn of the ﬁrst row means that STFT and MFCC features\nare extracted at frame level and subsequently processed by\nMuVar to obtain the temporal feature vector. This vector is then\nconcatenated by the beat and pitch features obtained at song\nlevel to produce the ﬁnal feature vector for classiﬁcation. Some\nresults in the table are based on our own implementations, and\nwe highlight them by prepending an asterisk in the references\nshown in the table. Otherwise, the results are based on those\nreported in the original paper.\nIt is worth mentioning that Table II by no means covers the\nfull spectrum of available methods for genre classiﬁcation due\nto the vast amount of work done in this area. Moreover, the\nresults should be treated with caution due to variations in im-\nplementation details even for the same feature. For instance, the\ntop 13 coefﬁcients were used for MFCC feature extracted in [1]\nand [12], whereas 20 feature coefﬁcients were adopted by [35].\n4http://ismir2004.ismir.net/genre_contest/index.htm\nFigure 2.2: Performance comparison of genre classiﬁcation algorithms evaluated on the GTZAN\ndataset (taken from [80]. Citations in the table refer to the references of the original paper.)\nTable 2.1: Comparing single handcrafted audio feature classiﬁcation accuracies (taken from\nChapter 5.2) with CNN based approaches (taken from [243]). Results show top accuracies\nusing different machine learning algorithms for the given feature on the GTZAN [292], ISMIR\nGenre [32] and Latin Music Datasets [267].\nDataset MFCC RP TSSD CNN\nGTZAN 67.8 64.9 66.2 82.20\nISMIR Genre 62.1 75.1 80.9 87.17\nLatin Music Dataset 60.4 86.3 87.3 96.03\nCNN-based systems. These results were taken from the evaluation presented in Chapter 5.2\nand the empirical evaluation of CNN-based music genre classiﬁcation presented in [243]. Both\nstudies were conducted on the same dataset.\n2.1.5 Music Artist Recognition (MAR)\nEarly approaches to Music Artist Recognition (MAR) are based on frame-level features such as\nMel-Frequency Cepstral Coefﬁcients (MFCCs) and chroma features [68] in combination with\nSupport Vector Machines (SVM) [132,178] or ensemble classiﬁers [16] for classifying music by\nartist names. In [133] a quantitative analysis of the album effect - which refers to effects of post-\nproduction ﬁlters to create a consistent sound quality across a record - on artist identiﬁcation was\nprovided. A Hybrid Singer Identiﬁer (HSI) is proposed by [263]. Multiple low-level features\nare extracted from vocal and non-vocal segments of an audio track and mixture models are\nused to statistically learn artist characteristics for classiﬁcation. Further approaches report more\n24\nValence\nArousal\nalarmed\nangry\nannoyed\nfrustrated\ndissapointed\nsad\nmiserable\nbored\ntired sleepy\npeaceful\ncalm\ncontent\npleased\nglad\nhappy\ndelighted\nexcited\naroused\nastonished\ndepressed\ndistressed\nFigure 2.3: The Valence-Arousal space used in music emotion recognition maps affect words as\na function of the degree of valence and arousal [225].\nrobust singer identiﬁcation through identifying and extracting the singers voice after the track\nhas been segmented into instrumental and vocal sections [190, 287]. A multi-modal approach\nusing audio and text based features extracted from lyrics was reported in [6]. Recent approaches\nto artist identiﬁcation make use of I-vector based factor analysis techniques [67]. This approach\nand its adaption on neural networks [66] show promising improvements on the artist20 dataset\n[68]. DNNs were also reported to perform well using Deep Belief Networks (DBN) [94] or\nConvolutional Neural Networks (CNN) [53] to automatically learn feature representations from\nspectrograms. Harnessing the visual information of music videos for MAR has not been reported\nin literature, yet. This approach will be adressed in Chapter 6.\nMusic Emotion Recognition\nThere is an afﬁrmed consent going back to Plato about the ability of music to evoke emotional\nresponses in listeners [191]. A wide range of literature discusses the psychology [50] as well as\nthe emotions of music [127]. Music emotion recognition attempts to estimate the kind and extent\nof emotions triggered by a music track. Emotions are either represented as categorical descrip-\ntions (e.g. happy, sad, angry, etc.) or as a psychometric dimensional space. A frequently used\nrepresentation is based on the Valence-Arousal (V-A) space [225] (see Figure 2.3) where affect\nwords are mapped as a function of the degree of valence and arousal. Various further experi-\nments provided word mappings for this model [24] which can be used to estimate the emotions\nof song’s lyrics or tags provided by social platforms [107]. Similar to the previously explained\ntasks, a common approach is to use regression models based on extracted music descriptors to\nestimate the corresponding values for valence and arousal. The results are either used directly\nfor similarity estimations or are discretized into categorical labels [131, 311].\n25\nMusic Similarity Retrieval\nMusic Similarity estimation or retrieval is one of the initial research tasks of the MIR domain\n[173] and it is still actively researched. The objective is to estimate the notion of similarity\nbetween two given tracks. Thus, applying this operation onto an entire collection it should be\nable to search similar tracks for a given seed song. The main obstacle researchers focusing on\nthis task face is the subjective nature of human judgments concerning music similarity [15].\nEstimations about song similarities or their categorical belonging to a genre or mood vary in\nrelation to individual tastes and preferences. Further, the concept of music similarity has multiple\ndimensions including distinct properties such as genre, melody, rhythm, tempo, instrumentation,\nlyric content, mood, etc. These ambiguities complicate the development of adequate algorithms\nas well as their valid evaluation [171, 294]. A central part of such approaches is the deﬁnition\nof a measure for similarity [15] which is further affected by the approach taken to extract the\nrelevant information.\nContent based approaches largely focus on the notion of acoustic similarity. Music features\nare extracted from the audio content. The resulting music descriptors are high-dimensional\nnumeric vectors and the accumulation of all feature vectors of a collection populates a vector-\nspace. The general principle of content based similarity estimations is based on the assumption\nthat numerical differences are an expression of perceptual dissimilarity. Different metrics such\nas the Manhattan (L1) or the Euclidean Distance (L2) or non-metric similarity functions such\nas the Kullback-Leibler divergence are used to estimate the numerical similarity of the feature\nvectors [15].\nContext based approaches harvest music related information from various sources [137].\nText-based approaches rely on mined web-pages, music-tags or lyrics [183] and apply meth-\nods from traditional information retrieval or natural language processing to extract text-based\nfeatures such as TF-IDF [321]. Co-occurrence based approaches examine playlists or mi-\ncroblogs [229] and estimate the similarity of tracks or artists by the notion how often they\nco-occur within a dedicated context or by the conditional probability that one track or artist\nis found within the same context as the other [231].\nMusic Recommendation and Discovery\nThe recent shift of the music industry from traditional to digital distribution through online\nmusic services such as Spotify, Deezer or Amazon has made automatic music recommendation\nand discovery an increasingly interesting and relevant problem [271]. Its objective is to let\nlisteners discover new music according their tastes and preferences. An ideal recommender\nsystem should automatically detect current as well as adapt to changing preferences and suggest\nsongs or generate playlists accordingly.\nMost music recommendation systems rely on Collaborative Filtering (CF) [36, 230]. One\nof the biggest problems of these approaches is known as the cold start problem [233]. If no\nusage data for a given item is available, the system fails to provide results. In the music domain\nthis refers to new or unpopular songs, albums or artists. On the opposite, songs with already\nassigned usage data will be included in recommendations more often and thus popular songs\nbecome more popular while unknown ones remain further so. This is also referred to as the\n26\nlong tail problem [38] where a small percentage of the collection is recommended with high\nfrequency, whereas the majority of it - the long tail - is seldom discovered.\nGenerally, a recommender system consists of the three components [271].User modeling, as\none of the key elements, attempts to capture the variations in user-proﬁles such as gender, age,\ngeographic region, life style or interests. All of these parameters might affect the user’s mu-\nsic preferences. Additionally to user-proﬁle models, user models depend also on user listening\nexperience models. Item modeling takes advantage of all music information available and ac-\ncessible in MIR research including music context as well as content. Meta-data retrieval based\nsystems rely on curated textual information provided by the creators or content owners, such as\nartist, title, album name, lyrics, etc. The obstacle of this approach is the cumbersome task of\ncreating and maintaining the required meta-data. Further, recommendations are only based on\ninformation annotated in the meta-data, which often does not reﬂect intrinsic music properties\nsuch as rhythm or mood nor does it include descriptions about the users. Collaborative ﬁlter-\ning approaches can be generalized into memory based systems which predict items based on all\npreviously collected ratings and model based systems which use machine learning to train user\npreference models for the item predictions. Content based approaches rely on the audio content\nand thus require access to the audio ﬁle. A wide range of already discussed techniques is used\nsuch as estimating similarity of songs or predicting social tags [65] through single or multilabel\nclassiﬁcation. A ﬁnal distinction can be made between emotion based, context based and hybrid\nmodels. The ﬁrst attempts to map emotions expressed by a song onto the valence-arousal space\n(see Section 2.1.5) and to recommend songs based on minimal distance to a query song. Hybrid\nmodels try to combine two or more approaches to overcome limitations and improve precision.\nScore following / Music Alignment\nScore following relates to the process of aligning the notes of a score to its interpretation dur-\ning a speciﬁc performance. This might be during a live performance, or to an audio recording.\nResearch on automatic score following through computers distinguishes between different sce-\nnarios Symbolic to MIDI and Symbolic to Audio. Their main differences are based on the digital\nrepresentation of music. Symbolic music is usually represented in a machine processable form,\nsuch as MIDI or MusicXML. Recorded music is represented as a time-series of sampled audio.\nMore generally speaking, sampled audio is an exhaustive sequence of numbers representing the\nmeasured auditive energy at a certain time. Information provided in this form is not suitable\nfor automatic processing at ﬁrst hand and has to be reduced and transformed into an appropri-\nate representation. This process is the focus of the research domain music information retrieval\n(MIR). While, in the symbolic representation of music, it is clear which note is being played at\na certain time, in digital audio this information has to be deduced from the spectral properties of\nthe sampled audio through digital signal processing.\nSymbolic to MIDI alignment: This scenario is based on the prerequisite that the performed\nmusic is available in the same symbolic format of the score. This is the case for set ups where a\nmusical instrument is connected to the computer through a Musical Instrument Digital Interface\n(MIDI). Music events such as hitting a key on an electronic piano, are immediately communi-\ncated to the connected computer and are available in an interpretable from. Having both sources\n27\nFigure 2.4: Visualization of a audio-to-score alignment system developed during the Europeana-\nSounds project (www.europeanasounds.eu). The system consists of two main parts. 1. applying\nOptical Music Recognition (OMR) to extract symbolic music information from scanned score\nimages. 2. extract Chroma features from symbolic and recorded music. 3. apply Dynamic\nTime-Warping (DTW) to align the extracted features. 4. map synchronized time information to\ncoordinates and page-numbers of the scanned images.\n- scores and performance - in a comparable format makes it more convenient for further process-\ning (e.g. score following, automatically assisted training, etc.).\nSymbolic to Audio alignment: While symbolic music unambiguously describes which note\nis played at which time of the track, this does not apply to recorded music. The main challenge\nwith sampled audio is that it is a mix of frequencies, originating usually from a multitude of\nindividual instruments and voices, which sources currently cannot fully be separated again, after\nthey have been ﬁxed in an audio mix. Musical notes refer to audio frequencies (e.g. concert\npitch = 440Hz). Thus, it seems obvious, that sampled audio can be transcribed into symbolic\nmusic by assigning note values to audio frequencies. In a simpliﬁed approach this works for\nmonophonic tunes played by a single instrument. Having multiple instruments playing poly-\nphonic tunes (like chords and harmonies) creates overlapping frequencies, partial frequencies\ncaused by the instrument’s timbre and other inﬂuences in the overall audio mix, which cause\ncomplex distributions of the sound energy over the frequency spectrum of the recording. Thus,\n28\nit is not computationally distinguishable anymore which notes have been played by the distinct\ninstruments. To align symbolic music to sampled audio, both types have to be transformed into a\nrepresentation that can be compared directly. Common approaches to audio-to-score alignment\nsystems is to convert both music modalities into a comparable representation. A common choice\nare Chroma features [68] because they can easily be extracted from both modalities. The time-\nsynchronization is then performed on this feature-space. Figure 2.4 depicts an audio-to-score\nalignment system that has been implemented for the Europeana Sounds25 project.\n2.1.6 Multi-modal Approaches to MIR\nMulti-modal systems approach MIR tasks by utilizing music information of different modalities\nsuch as song lyrics [107, 183], web pages [232] and social media/tagging [141]. Visual related\nmusic information extraction has been reported utilizing album art images for MGR [182], artist\nidentiﬁcation [150] and similarity retrieval [29]. Image features extracted from promotional\nphotos of artists are used to estimate the musical genre of the image [151]. A multi-modal\napproach to mood classiﬁcation using audio and image features is reported in [63]. Perceptual\nrelationships between art in the auditory and visual domains were analyzed in [181].\nThe difﬁculties of developing high-level audio features have triggered the search for different\nsources to pull information from - introducing a multi-modal view of music. Approaches com-\nbining audio with text using information extracted from lyrics have been reported for automatic\ngenre [183] and mood [143] recognition. Further attempts to improve music genre recognition\nby adding information from different modalities included combining audio content with sym-\nbolic features extracted from midi [157] as well as social media data [35]. In the video retrieval\ndomain using music to discriminate video genres has recently gained a lot of attention [28]. Dif-\nferent acoustic styles are used to estimate the video genre, as certain types of music are chosen to\ncreate speciﬁc emotions and tensions in the viewer [196]. The video retrieval domain generally\nobserves music videos as a video genre of its own and further sub-categorization in different\nclasses (e.g. musical genre, type, etc.) has been of no interest, yet. Using the visual layer to\nmake estimations about audio is a nearly unexplored ﬁeld. Multi-modal approaches have only\nbeen reported for automatic music video summarization [262] and indexing [82]. The MIRES\nroad-map for music information research [257] identiﬁes music videos as a potential rich source\nof additional information. As an open research it requires more attention of the audio/visual\ncommunities as well as appropriate data-sets.\n2.2 Content Based Image Retrieval\nContent Based Image Retrieval (CBIR) is a research domain with a very long tradition. On that\naccount a series of survey papers has already been provided [51, 56, 170, 220, 268, 284, 298].\nGenerally, CBIR systems were categorized into those using low-level images features such as\ncolor [179, 212, 234], image segmentation [73, 175, 265] or texture [169, 258, 283] including\nLocal Binary Patterns (LBP) [203]. CBIR experienced a huge boost of performance [192] with\nthe introduction of interest point based approaches such as the original Scale Invariant Feature\n25https://www.europeanasounds.eu/\n29\nTransforms (SIFT) [174] which is acknowledged to be one of the most inﬂuential publications\nin CBIR or one of their successors such as the Speeded-Up Robust Features (SURF) [11].\nRecently, CBIR has experienced remarkable progress in the ﬁelds of image recognition by\nadopting methods from the area of deep learning using convolutional neural networks (CNNs).\nA full review of deep learning and convolutional neural networks is provided by [40]. Neural\nnetworks and CNNs are not new technologies, but with early successes such as LeNet [144], it\nis only recently that they have shown competitive results for tasks such as in the ILSVRC2012\nimage classiﬁcation Challenge [140]. With this remarkable reduction in a previously stalling\nerror-rate there has been an explosion of interest in CNNs. Many new architectures and ap-\nproaches were presented such as GoogLeNet [282], Deep Residual Networks (ResNets) [101]\nor the Inception Architecture [282]. Neural networks have also been applied to metrics learn-\ning [118] with applications in image similarity estimation and visual search. Siamese network\narchitectures are trained on pairs [13] or triplets [103] of images. By using a distance metric as\na loss function the network train on general image similarity instead of discrete concepts. The\nsimilarity of images is then directly estimated by resulting model or by the provided learned em-\nbeddings which can be used for visual search. Also multimodal models using visual and textual\ninput have been reported recently. These models are trained for image captioning and visual\nscene understanding [180].\n2.2.1 Face Recognition\nGood summaries of state-of-the-art approaches and challenges in face recognition are provided\nby [149, 209, 317]. Face detection, tracking and recognition is also used in multi-modal video\nretrieval [148, 270]. Faces are either used to count persons or to identify actors. Most common\nmethods used to recognize faces are Eigenfaces [289] and Fisherfaces [12]. In [55] face tracking\nand text trajectories are used with Hidden Markov Models (HMM) for video classiﬁcation. A\nface recognition approach based on real-world video data is reported in [272].\n2.3 Content Based Video Retrieval (CBVR)\nA comprehensive survey on Content Based Video Retrieval (CBVR) is provided by [106]. Gen-\nerally, CBIR approaches can be categorized into video structure analysis , including the tasks\nShot Boundary Detection [269], Key Frame Extraction and Scene Segmentation [139],seman-\ntic video search [106] and classiﬁcation [28]. Especially, [28] provides a very comprehensive\noverview of relevant concepts of video classiﬁcation methods. Video classiﬁcation follows the\nsame paradigm of almost all multimedia retrieval approaches. First descriptive features are\nextracted from the content, then a model is trained usually using elaborated machine learning\nalgorithms. Generally, three major approaches can be identiﬁed in literature:Audio-based [221],\nText-based [27], Visual-based [100] and ensemble-methods utilizing multiple modalities [303]\n(see Table 2.5 which is cited from [28]). Visual-based systems can further be grouped intoColor-\nbased [100], Shot-based [285], Object-based [303], MPEG-based [121], and Motion-based [75]\napproaches. The approaches presented in this thesis fall into the categories color- and object-\nbased systems. In Chapter 8 a wide range of color- and texture-features are evaluated towards\n30\nIEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS, VOL. UN KNOWN, NO. UNKNOWN, UNKNOWN 2007 13\nTABLE IV\nC OMPARISON OF FEATURES\nFeature Type Pros/Cons\nText Features\nClosed Captions High accuracy when not produced in real-time, high dimensionality, computationally cheap to extract\nSpeech Recognition High error rates\nOCR Can extract video text not present in dialog, computationally expensive\nAudio Features Require fewer computational resources than visual features, clips are typically shorter in length and smaller in\nﬁle size than video clips, difﬁcult to distinguish between multiple sounds\nVisual Features\nColor-Based Simple to implement and process, crude representation\nMPEG Easy to extract, but video must be in MPEG format\nShot-Based Difﬁcult to identify shots automatically, so may not be accurate\nObject-Based Difﬁcult, limited on number of objects, computationally expensive\nMotion-Based Difﬁcult to distinguish between types of motion, computational requirements range from low (MPEG motion\nvectors, frame differencing) to high (optical ﬂow)\nmuch higher accuracy rates when the on-screen text does not\nvary much in font type and size, as it does in commercials\n[21].\nRepresenting video using text features typically results in\nvectors with very high dimensions [20], even after applying\nstop lists and stemming. Brezeale and Cook [18] report that the\nterm-feature vectors produced from the closed captions of 81\nmovies had 15,254 terms each. While extracting text features\nfrom video may not be time-consuming, processing the text\ncan be due to this high dimensionality.\nText features are especially useful in classifying some genre.\nSports [7] and news both tend to have more graphic text\nthan other genre. Text features derived from transcripts are\nbetter than audio or visual features at distinguishing between\ndifferent types of news segments [54].\nAudio features require fewer computational resources to\nobtain and process than visual features [27]. Audio clips are\nalso typically shorter in length and smaller in ﬁle size than\nvideo clips. Many of the audio-based approaches use low-\nlevel features such as ZCR and pitch to segment and describe\nthe audio signal with higher-level concepts such as speech,\nmusic, noise, or silence, which are then used for classiﬁcation.\nSome of these approaches assume that the audio signal will\nonly represent one of these high-level concepts [84], whichis\nunrealistic for many real-life situations.\nMost of the visual-based features rely in some manner on\ndetecting shot changes and are therefore dependent on doing\nso correctly. This is the case whether the feature is the shot\nitself, such as average shot length, or applied at the shot level,\nsuch as average motion within a shot. Detecting shot changes\nautomatically is still a difﬁcult problem, primarily due tothe\nvariety of forms transitions between shots can take [47].\nFrame-based features are costly to produce if each frame\nis to be considered. This also results in a tremendous amount\nof data to process for full-length movies. This can be made\neasier by only processing some frames, such as the keyframes\nof video shots. This assumes that the keyframe chosen is\nrepresentative of the entire shot. This assumption will be\nviolated if there is much motion in the shot.\nColor-based features are simple to implement and inexpen-\nsive to process. They are useful in approaches wishing to use\ncinematic principles. For example, amount and distribution of\nlight and color set mood [62]. Some disadvantages are that\ncolor histograms lose spatial information and color-basedcom-\nparisons suffer when images are under different illumination\nconditions [64]. The crudeness of the color histogram also\nmeans that frames with similar color distributions will appear\nsimilar regardless of the actual content. For example, the color\nhistogram of a video frame containing a red apple on a blue\ntablecloth may appear similar to the histogram of a red balloon\nin the sky.\nObject-based features can be costly and difﬁcult to derive.\nWei et al. [56] report that detecting text objects is efﬁcient\nenough to be applied to all video frames but that detecting\nfaces is so expensive that they limited it to the ﬁrst few\nframes of each shot. Most methods require that the objects\nbe somewhat homogenous in color or texture in order to\nsegment them correctly, which may also require conﬁrmation\nfrom humans [79]. Objects that changed shape, such as clouds,\nwould also prove difﬁcult to handle.\nThe quantity of motion in a video is useful in a broad sense,\nbut it is not sufﬁcient by itself in distinguishing between the\ntypes of video that typically have large quantities of motion,\nsuch as action movies, sports, and music videos [4]. Calculat-\ning the quantity of motion in a shot includes using optical ﬂow,\nMPEG motion vectors, or frame differencing. Optical ﬂow is\ncostly to calculate and may not match the direction of the real\nmotion, if that is also required. For example, if the camera\npanned to the left then the optical ﬂow would indicate that the\nmotion moves to the right. The optical ﬂow algorithm of Horn\nand Schunck has problems with occluded edges since they\ncause a discontinuity in reﬂectance [72]. Extracting motion\nvectors from MPEG-encoded video is not costly, but of course\nrequires that the video be in this video format in the ﬁrst\nplace. However, motion as indicated by motion vectors may\nbe less accurate than motion as measured using optical ﬂow\n[54]. Iyengar and Lippman [59] found that measuring motion\nusing frame differencing produced results similar to thosethat\nmeasured motion using optical ﬂow, yet frame differencing\nis simpler to implement and less computationally expensive.\nHowever, region-based features such as frame differencingare\nnon-speciﬁc as to the direction of the motion [75].\nMeasuring speciﬁc types of motion, such as object motion\nor camera motion, is also a difﬁcult problem because of the\ndifﬁculty in separating the two. Many approaches for measur-\ning the motion of objects require that the object be segmented,\nFigure 2.5: Comparison of pros and cons of different classiﬁcation approaches and the corre-\nspondingly used features (cited from [28])\ntheir performance in classifying music videos by their music genre. Object-based approaches\nare presented in Chapter 6 where faces are extracted from the music video-fames to recognize\nthe performing artist and in Chapter 9 high-level concepts are extracted from the video-frames\nto predict the music genre.\n2.4 Music Videos Analysis\nMulti media analysis of music videos is part of reported automatic music video creation sys-\ntems. An approach based on audio and video segmentation was presented in [76]. Segments of\nthe source video are selected based on a calculated suitability score and are combined to create\na new music video. The two studies presented in [108] and [314] which build on the work pre-\nsented in [76] automatically select and join music video-like videos from personal home videos.\nIn [31] the authors describe an approach to extract salient words or phrases from the lyrics and\nuses them to search for corresponding images on the web. Also contrast features similar to those\nused in this publication were used.\nIn [288] a music video retrieval system is presented based on coordinate terms and di-\nversiﬁcation, using artist name queries and coordinate terms derived from the Million Song\nDataset [19]. An approach to automatically determine regions of interest in music videos is\nreported in [130]. An approach to automatic music video summarization is presented in [262].\nAn audio-visual approach to segmentations of music videos was proposed in [82], including\nan evaluation of audio-visual correlations with an intended application in audio retrieval from\nvideo. Approaches to affective content analysis of music videos are provided by [316] and [312].\nAn approach using convolutional neural networks (CNNs), in order to learn mid-level represen-\ntations from combinations of low-level MFCCs and RGB color-values in order to build higher\nlevel audio-visual representations, is presented in [2]. The approach is evaluated on the DEAP\ndataset [138], a music video dataset with valence and arousal valued ground truth data. [226] de-\nscribes an audio-visual approach for a recommender system that uses a video as input query. To\nindex music via the video domain mood values calculated from the audio content were matched\nto mood information derived from the color information. The visual features used by the au-\nthors [296] were also evaluated in this publication. Unfortunately we were not able to draw the\n31\nsame conclusions about the effectiveness of these features for music classiﬁcation. Recently,\na similar approach as presented in Chapter 9 was reported [104], also using a combination of\nConvoluational Neural Networks (CNN) and music descriptors to ﬁnd similar music or videos.\n2.5 Transfer Learning\nTransfer Learning refers to concept of transferring knowledge from a related task that has al-\nready been learned within a given context to a problem of a different context or even a different\nresearch domain [207]. A well known example is the transfer of the well evaluated automatic\nspeech recognition method based on MFCC to the MIR domain [172]. A comprehensive sum-\nmary of further examples is provided by [305]. In this study we transfer knowledge from the\nvisual computing domain to MIR by applying trained visual concept detection to music classiﬁ-\ncation tasks.\n2.6 Summary\nThis section provided an overview of the state-of-the-art in topics relevant for this theses. It ﬁrst\nprovided a broad overview of the research domain Music Information retrieval. Due to the focus\nof this thesis to augment MIR based approaches by harnessing the information provided by the\nvisual layer of music videos, such an elaborated introduction provides the relevant basis for the\ndiscussion of the potential application ﬁelds of an audio-visual approach to MIR.\nThe consecutive sections elaborated which multi-modal, especially audio-visual, approaches\nhave been reported in the speciﬁc domains such as image processing, visual computing or MIR\nas well as their applications to MIR domain. This literature review indicated that there is a\nlack of basic research concerning audio-visual approaches to MIR problems. Identiﬁed studies\nreported improvements including visual information in classiﬁcation or retrieval experiments,\nbut often failed to analyze the contribution of the distinct visual features on the result.\n32\nCHAPTER 3\nAudio-Visual Relationships in Music\n“Albums should be as bold and dashing as we can make them; they\nshould stand out in dealers’ windows screaming for attention, yet\nalways reﬂecting the spirit of the music inside. Color should be violent\nand strong. Copy should be pared to a minimum, and each album\nshould reﬂect the quality of the Columbia name”\n— Pat Dolan, CBS, 1940\nMusic is an acoustic phenomenon. To experience music it is unavoidable to listen to it. In\ntimes of cellular networks with permanently connected devices, where music is streamed over\nthe Internet and can be consumed at any place any time, this condition does not seem to be a\nproblem. But, listening to music was not always that convenient. Looking back two decades,\nthe major music distribution media was the Compact Disc (CD) which replaced the cassette in\nthe early 90s which in turn succeeded vinyl LPs in the 80s. Music was not sold on-line where it\nis possible to conveniently browse through catalogs and listen to provided samples. One needed\nto go to record stores browse through shelves of records, pick some interesting albums, queue\nup in front of a record player provided by the store and listen to the record. Thus, discovering\nnew music was a time-consuming task. Music genres are a convenient aid for describing music\ntastes or for navigating trough on-line and off-line music stores. They are synonyms for music\nstyles and aggregate several music attributes which are distinct for the certain genre. Large signs\nin record stores with music genres printed in large letters show the way to the desired music.\nThe record shelves themselves are structured according diversiﬁed sub-genre categories which\nprovide a more ﬁne-grained discrimination of music styles. But, despite all these subsidiary\nsynonymic music descriptions, ﬁnally the customer still has to make a decision between a large\nset of records - and, although a bit further, is still facing the same problem - the music is not\ndirectly tangible. No matter if it is available on Vinyl, CD, cassette, DVD or even as printed\nscores - it is not easily possible to get an impression of its content. The only information provided\nat hand is of visual kind and the most prevalent of it is the cover.\n33\n(a) Music sheet cover ’The Brigand’s Ritornella’\nfrom Auber’s opera Masaniello, showing the im-\nage of the singer Madam Vestris who was a hugely\npopular star of burlesque at the Paris Olympic The-\natre, 1835.\n(b) Music sheet cover ’The National Football Song’\ndesigned by H. G. Banks depicts popular football\nplayers, about 1880.\nFigure 3.1: Early examples of sheet music and song book illustrations.\nUsing decorative images and paintings to advertise music is not a modern concept. In 1820\nartists became famous for their cover designs of songbooks and music scores [8] (see Figure\n3.1). Further, the development of lithography had a strong impact on cover art design for music\nsheets [210].\nThis chapter provides a structured overview of how visual accentuation is used in music\nmarketing and how this has developed over time. As a ﬁrst example album cover arts will be\nexplored by giving a brief introduction to their history and their common themes. The role of\nvisual accentuation will then be discussed in reference to music marketing strategies. Finally,\nthe broad visual aspects provided by music videos are elaborated.\n3.1 When Packaging Music Became asn Art-Form\nThis section brieﬂy summarizes Jones and Sorger’s article [125]. For a detailed insight in the\nhistory of album cover design please refer to this excellent article.\nThe history of album cover design begins with the establishment of phonograph companies\nwhich started to sell prerecorded musical selections. In 1896 Thomas Edison established the\n34\n(a) Edison Gold Moulded record made of relatively\nhard black wax, 1904. c⃝ Wikipedia\n(b) 78rpm record with sleeve by Victor Talking\nMachine Company, 1928. c⃝ Jeff Crompton\nFigure 3.2: Examples of early record covers. Their initial purpose was to protect the media.\nLess attention was set on design.\nNational Phonograph Company to sell music pressed on wax cylinders for the phonograph. Due\nto the fragility of the cylinders cardboard boxes lined with felt were used for packaging (see\nFigure 3.2 a). Their standardized design focused mainly on highlighting the company name.\nIn 1901 the Victor Talking Machine Company introduced the 78-rpm ﬂat disc phonographs.\nAlthough, early discs were shipped sleeveless and thus, without protection, sleeves have become\nan industry-wide standard by 1910. The more common design type consisted of a blank sleeve\nwith a cut out hole in the center to reveal the label of the record (see Figure 3.2 b). The ﬁrst\ndurable sleeves made of cardboards were produced by record stores which initially only had\nthe store’s name printed on it but later incorporated photographs and portraits of the performing\nartist or the composer. The denotation of the record as an album originates from that epoch.\nRecordings of classical music such as concerts or operas exceeded the length of a single 78-rpm\nshellac disc. Usually three to ﬁve discs were required for longer orchestral performances. The\ndesign of the card-boxes used to distribute these recordings reminded of photo albums which\nsoon became a frequently used synonym.\nAfter the Great Depression album cover design developed and intensiﬁed from abstract\nwallpaper-like designs in the 1930s to a reﬁned marketing tool of major music companies. De-\nsigning cover arts became an attractive ﬁeld for artists. Especially, as record stores changed to\nself-service selection in the 1940s, providing customers with racks where they could ﬂip through\nthe records on their own, the album cover became a direct marketing tool. In 1948 Columbia\nintroduced the Long-Playing record (LP) which quickly became the new standard. This more\nfragile LP required a new packaging which now hid the entire record and thus provided more\nspace for designers. Rock-and-roll marketing of the 1950s was related to movies. Records were\ncovered with large portraits of the performing artists. Only Jazz album covers were artistically\n35\n(a) Cover of The Velvet Underground & Nico de-\nsigned by Andy Warhol, 1967\n(b) Cover of The Rolling Stones - Sticky Fingers.\nThe cover featured a functional zipper which was\nembedded in the cardboard., 1971\nFigure 3.3: Examples of album cover designs of the late 1960s.\nmore creative. While LPs in rock and pop music were mostly used as a collection of hit-singles,\nbands in the 1960s started to sell complete albums with new compositions. Further, they began\nto get involved in album cover design themselves, using well-known designers such as Richard\nHamilton or Andy Warhol (see Figure 3.3 a). The Beatle’s Sgt. Pepper’s Lonely Hearts Club\nBand is often cited as one of the most familiar and inﬂuential records of this time. It was the\nﬁrst to contain an inner sleeve, printed lyrics and a gate-fold to contain cards with cut-outs. By\ntaking over the complete design of their album, the Beatles have set a new measure of album\ncover design which now more and more acted as an audio-visual experience (see Figure 3.3 b).\nThe visual design continuously gained importance and became a part of the marketing strategy.\nArtists developed individual styles that were repetitively used on their releases such as stylized\nletters or band-logos.\nThe appearance of Punk Rock at the end of 1975 introduced a wide range of small inde-\npendent labels. Punk Rock cover design reﬂected the revolting nature of the genre which was\nin clear opposition to established aesthetic norms. Similar to the music, whose simple structure\nopposed the overly complex compositions of Hard and Glam Rock bands, Punk album covers\nwere simple. New Wavewhich is the succeeding movement of Punk created a wide range of new\nsounds and styles which again put more emphasis on album cover design.\nIn 1983 the Compact Disc (CD) was introduced and thus the digital age of music. The new\npackaging format, the jewel box, only offered space for a small square booklet. Besides the\nsmaller front to advertise the record, there was also no space to add additional properties such\nas posters or cards. This minimalistic visual experience was often criticized. With the advent\nof online music, the media completely lost its haptic property. Listening to music was reduced\nto its intrinsic nature and the holistic experience of ﬂipping through album cover pages or CD\nbooklets was withdrawn.\n36\n(a) Sex, The Cars - Candy O,\n1979\n(b) Art, Led Zeppelin - III, 1970\n (c) Identity, Chicago - X, 1976\n(d) Drugs, Cream - Disraeli\nGears, 1967\n(e) Artist, The Ramones - Ra-\nmones, 1976\n(f) Politics, U2 - War, 1983\n(g) Death, Slayer - Seasons in the\nAbyss, 1990\n(h) Face Close-ups, Bob Marley\n& The Wailers - Legend, 1984\n(i) Text Only, John Mayer - Con-\ntinuum, 2006\nFigure 3.4: Overview of common types, styles and subjects shown on album cover-arts.\nFigure 3.4 gives an overview of common album-cover themes.\n37\n3.2 Music Videos\nInitially called promotional videosmusic video production had its break through in the early 80s.\nMusic videos were one of the major music inventions of that decade, although they were initially\ncriticized to provoke a diminishing of the interpretative liberty of the individual music listener\nby imposing the narrative visual impression upon him or her [79]. In that period the structure\nof the prevalent music business changed completely by shifting the focus from selling whole\nrecords to hit singles [79]. The entire marketing campaign of the related record was built around\nthat speciﬁc song and with the new intensiﬁcation of celebrity around pop music, music videos\nplayed an important role in the development of the pinup culture. With their increasing popu-\nlarity music videos got more complex and soon became an art form themselves. Contemporary\nvideo production typically uses a wide range of ﬁlm making techniques and roles, including\nscreen-play, directors, producers, director of photography, etc. Further, as cited in [34] “aca-\ndemics are ﬁnally starting to recognize that music videos are a persistent cultural form that has\noutlived one of their initial commercial functions as a promotional tool”. This section provides\na brief overview of music video types, concepts and properties, exceeding those relevant for the\nexperiments performed in this thesis to provide a basis for potential future work.\n3.2.1 History of Music Videos\nThe hybrid nature of music videos puts them in an overlapping space between being musical and\nvisual. The two major positions on whether the one or the other part is privileged are represented\nby E. Ann Kaplan’s bookRocking around the Clock: Music Television[128] and Andrew Good-\nwin’sDancing in the Distraction Factory[84]. Kaplan argues from a ﬁlm theoretical standpoint,\nfavoring the visual dimension while Goodwin argues that these visual aspects are only secondary\nto the music itself.\nAlthough the history of music videos is often brought into context with the launch of MTV ,\nthe relationship of music and moving images started much earlier [52]. In 1940 the Panoram\nMovie Machine was introduced by the Mills Novelty Company (see Figure 3.5a). Those were\ncoin-operated cinematic jukeboxes containing a screen on which three minute musical ﬁlms\ncould be watched. Those so called Soundies [102] featured musical styles ranging from big-\nband swing, American country, romantic ballads to Latin and Jazz. Like music videos they\nwere promotional ﬁlms produced and edited to pre-recorded music. Their visual style included\nvaude-ville-style and burlesque but also world-war I related and comedian acts, but did not differ\nmuch from musical shorts which were popular in cinemas in this period. In the 1960s a similar\nsound ﬁlm projection system, theScopitone [64], was introduced and marketed (see Figure 3.5b).\nThe improvement was, that the music shorts were not played sequentially but could be chosen\nfrom among 36 different 16-millimeter Technicolor ﬁlms of B-list pop stars performing their\nlatest hits. Scopitone ﬁlms were deliberately designed to appeal to a target audience of men\nin bars which was where these machines could primarily be found. Soundies and Scopitones\nset themselves apart from conventional ﬁlm by being entirely music based whereas most of the\nliterature on ﬁlm music looks at music’s role as a supplement to the visual presentation [102].\nFurther were their paths of distribution and consumption more similar to recorded music.\n38\n(a) The Movie Machine or Panoram\nSoundie by Mills Novelty Company,\n1940.\n(b) Scopitone machine, 1960.\nFigure 3.5: Early cinematic jukeboxes. These coin-operated machines showed short music ﬁlms.\nIn the 1960s various music bands began to create promotional short ﬁlms for their songs.\nIn 1964 The Moody Blues promoted their single “Go Now” using a short clip where the band\nwas portrayed while singing the song and playing instruments. This clip already featured many\nelements that are common in contemporary music videos and described in Chapter 3.2.2. The\nBeatles created feature ﬁlms for their albums such as “A Hard Day’s Night” (1964), “Help!”\n(1965) and “Yellow Submarine” (1968) where they played the main roles and also performed\nto their own songs. Further the Beatles creates several short music ﬁlms for their singles such\nas “Strawberry Fields Forever” and “Penny Lane” (1967) which also featured many concepts\nof contemporary music video productions. In the 1970s many music artists started to create\npromotional videos which were shown in music shows such as BBC production Top of the Pops\n[129]. The most popular example of these is the video for Queen’s “Bohemian Rhapsody”\n(1975).\nFinally, music television MTV was launched in August 1981 with the video “Video Killed\nthe Radio Star” by The Buggles. Still, music videos were low in number and the quality of the\nproduced clips was low due to lack of experience of ﬁlm makers with the medium of music and\nthe yet not elaborated processes. Style and design features as explained in Chapter 3.2.2 and\n3.2.3 in more detail, slowly developed through the explosive increase of interest and produc-\ntions. Music videos became a forerunner in visual aesthetics and inﬂuenced other media genres\n39\nsuch as musicals, advertisement and feature ﬁlms. While in the 1990s their popularity reached\na zenith, due to the budget spent as well as the artistic and technological complexity, music\nvideos emerged into a high-art form, from which many elements found its way into cinematic\nproductions such as the Frozen Moment of Michel Gondry’s video for Like a Rolling Stone by\nThe Rolling Stones in 1995 which was used in the Hollywood movie The Matrix in 1999 [129].\nThe downturn the music industry experienced in the late 1990s led to huge cuts in the bud-\ngets for music videos which is one of many reason that led to a still ongoing crisis of this\nmedium. Further, MTV continuously changed its program from music videos to teenager shows\nand ﬁnally lost its signiﬁcance to the internet where music videos can be accessed on demand.\n3.2.2 Types of Music Videos\nMusic videos are short ﬁlms intended to promote a song and its performing artist. The accen-\ntuation of the music video as an artistic work added a further semantic layer to a composition.\nAbove the traditional audio and lyrical layers the visual part can be used to either illustrate,\namplify or contradict the meaning of the underlying song. There are typically three ways to\napproach this goal [78]:\nIllustration The visual layer illustrates and further explains the meaning of\nthe song by either using a narrative plot or visual clues that are\nrelated to lyrics or harmonics.\nAmpliﬁcation The meaning of the song or certain relevant messages of its\nlyrics are emphasized and reinforced through constant repeti-\ntion and dominating visual accentuation.\nContradiction The underlying meaning of lyrics and melodics is ignored. The\nvisual layer either counterpoints the meaning of the song or\nbecomes completely abstract.\nThere are three main types of music videos [34]:\nPerformance videos The artist is presented performing in sync with the song - typically in\nan environment corresponding to the song or genre (e.g. with or without friends or\ninstruments, live or staged performance, studio recording, etc.). Performance videos\nwere predominantly used in early pre-MTV music shows such as BBC’sTop of the Pops\n(see Figure 3.6 a).\nConcept videos The meaning of the lyrics is not narrated but illustrated through visual con-\ncepts and metaphors. The plot of the video is mostly obscure and surreal, trying to\nattract and entertain the audience by constantly keeping their attention on the screen.\nConcept videos could include performance, but make use of unusual settings or ﬁlm\ntechniques such as trick photographic and novel editing techniques to suspend end in-\nvert the laws of nature for novel entertainment value [90] (see Figure 3.6 b).\n40\nNarrative videos The visual part commonly illustrates or portraits the story told by the lyrics,\nbut may also contradict the underlying song by telling a completely different story. Usu-\nally the narrator is the performing artist who often acts as the protagonist of the story\n(see Figure 3.6 c).\nDance videos In dance videos the artist is performing alone or with additional background\ndancers a rehearsed dance choreography (see Figure 3.6 d). Famous examples are the in-\nﬂuencing music videos by Michael Jackson which feature complex synchronized chore-\nographies.\nAnimated Music Videos Animated music videos make use of all kinds of styles and technolo-\ngies such as drawing, stop motion photography, claymation, computer graphics, etc.\n(see Figure 3.6 e) Animated videos provide more options to express emotions reﬂected\nby the corresponding song, but can be generally categorized by the same types as non-\nanimated music videos.\nLyrics Videos Lyric videos display the lyrics in sync with the singer, often in artistic ways\n(see Figure 3.6 f). A famous example is Bob Dylan’s video to his song Subterranean\nHomesick Blues (1965), which was also one of the ﬁrst “promotional” music clips. In\nthe video Dylan stands in front of the camera ﬂipping through white cards with selected\nwords and phrases of the lyrics.\n3.2.3 Stylistic Properties of Music Videos\nA wide range of cinematographic styles and effects can be spotted in music videos. While ﬁlm\nmaking has developed standardized rules how to aesthetically apply them, music videos do not\nfollow these rules but give these elements different functions and meanings [299]. The following\nlist summarizes the most important stylistic features of music videos:\nCamera Work and Movement Abiding the continuing progression of the musical track move-\nment is one of the most common properties of music videos. Most of the shots of a video\nmake use of one or more of the following techniques: tracking shot - the camera moves\ntowards or away from the subject, pan - the camera rotates to the left or right to reveal\nmore of the environment, tilt - similar to panning but camera rotates horizontally, zoom\nin or out - magnify or reduce subject by changing lens focal length. Tempo of fore- and\nbackground movement is also used to express moods and emotions of the track.\nLighting and Color Lighting and color can inﬂuence our interpretation of an individual scene.\nColor can be used to attract attention or express moods (e.g. warm colors are often\nused to create romantic or pleasurable ambiance). Lighting in visual production has\ndeveloped many techniques to create all kinds of effects and moods (e.g. directional\nlight, back and spot lighting, shadows, etc.).\nVisual and stylistic coherence A common aimed at property of music videos. Shots of a video\nprovide a coherent impression that should correspond to the underlying track. Usually\naccomplished in post-production through global ﬁlter (e.g. color, blur, etc.)\n41\n(a) Performance Videos , Van\nHalen - Jump, 1970. The band is\nﬁlmed while performing in synch\nto the music.\n(b) Concept Videos, Red\nHot Chili Peppers - Cali-\nfornication, 1999. Video\nis realized as a ﬁctional\n3D video game.\n(c) Narrative Videos, Beyonce -\nIf I Were A Boy, 2008. The video\nplays with gender stereotypes by\nswitching the roles.\n(d) Dance Videos, Michael Jack-\nson - Bad, 1987. Choreagraphed\ndance in reference to the musical\n‘West Side Story’.\n(e) Animated Videos , Gorillaz\n- Clint Eastwood, 2001. An-\nimated band members are per-\nforming with dancing zombie go-\nrillas.\n(f) Lyrics Videos, Neil Young -\nMy Pledge, 2016. Lyrics appear\non array of postcards.\nFigure 3.6: Example frames of different music video types.\nClose Ups Usually a full-face shot composed from below the shoulder line. There is often a\npressing demand of music labels to promote and iconize the contracted artist through a\nproliferated presence on screen. Close ups are further applied to underscore a musical\nhook or the peak of a phrase or, more recently, to advertise brand names or products\nadded to the video to earn additional revenue.\nEditing and Shots Contrary to ﬁlm making, music video editing does not intend to remain\nunnoticed but is perceived as an art-form. Traditional rules to guide the viewer in time\nand space are ignored due to its short form and the demand to showcase the star. Shots\nare joined by favoring compositional elements (e.g. color, shape, etc.) over content.\nJump Cuts - intentionally disjunctive edits - are frequently employed using drastic shifts\nin color, scale or content. Low angle shots reproduce the impression of looking up\nto an artist performing on stage. High-angle shots harmonize with key moments of a\nsong. Mixing those in a series disorients the viewer who seeks additional guidance in\nthe music. Shot length and boundaries commonly correspond to tempo and rhythm of\nthe track.\n42\n3.2.4 Signiﬁcance of Music Videos\nMusic videos were initially criticized to provoke a diminishing of the interpretative liberty of\nthe individual music listener by imposing the narrative visual impression upon him or her [79].\nYet, over the past four decades music videos shaped our cultural life. Movie editors speak of\nMTV-style editing when referring to short shot sequences. Artists use huge video walls at life\nperformances displaying complex beat synchronized visuals that turn a whole concert into a mu-\nsic video like experience.\nParts of this introduction to Music Videos as presented in this section was ﬁrst published and presented in Alexander\nSchindler. “A picture is worth a thousand songs: Exploring visual aspects of music” at the 1st International Digital\nLibraries for Musicology workshop (DLfM 2014), London, UK, September 12 2014 [236].\n43\n3.3 Summary\nThis chapter provided an in-depth overview of how visual media is utilized to promote new\nartists and advertise new music.\n• A broad overview from early utilization of illustrations on music sheets to the history\nof album cover art is provided in Section 3.1. It is outlined how visual media has been\nintentionally used in the pre-online-music-streaming era to advertise new and unknown\nmusic acts in magazines, which were the prevalent distribution channels.\n• The experimental focus of this thesis lies on Music Videos. Section 3.2 provides an ex-\ntensive introduction to this medium, including its historical development, different music\nvideo types and stylistic properties used in music video production.\nThe information compiled in this Chapter is relevant to understand the intentions and production\nprocesses of music videos. On the one hand, this is relevant to develop appropriate approaches\nto harness the information of music videos by deriving or extracting corresponding representa-\ntions. On the other hand this knowledge is relevant for the evaluation of these approaches.\nFinally, this chapter closes with a discussion on the relevance of music videos for the Music\nInformation Retrieval research domain.\n44\nCHAPTER 4\nData\n“It is a capital mistake to theorize before one has data. Insensibly one\nbegins to twist facts to suit theories, instead of theories to suit facts. ”\n— Sir Arthur Conan Doyle, “The Adventures of Sherlock Holmes ”,\n1892\nMusic Information Retrieval (MIR) research has historically struggled with issues of pub-\nlicly available benchmark datasets that would allow for evaluation and comparison of methods\nand algorithms on the same data base. Most of these issues stem from the commercial interest\nin music by record labels, and therefore imposed rigid copyright issues, that prevent researchers\nfrom sharing their music collections with others. Subsequently, only a limited number of data\nsets has risen to a pseudo benchmark level, i.e. where most of the researchers in the ﬁeld have\naccess to the same collection.\nAnother reason identiﬁed as a major challenge for providing access to research data in gen-\neral is the lack of esteem and valuation of these kind of activities. While preparing, maintaining\nand providing access to massive data collections requires signiﬁcant investments in terms of sys-\ntem maintenance and data (pre-)processing, it is considered administrative rather than research\nwork (in spite of several even research-afﬁne challenges emerging during such activities), and\nthus does not gain acceptance in classical research-oriented publication venues. Such lack of\ncareer rewards is one of the many factors, next to legal limitations and lack of expertise, limiting\nsharing of research data [126]. Several initiatives have been started in the research infrastruc-\ntures area to mitigate this problem and foster collaborative research. These areas span across\nvirtually all topical areas, from Astronomy, via meteorology, chemistry to humanities1.\nThis chapter introduces the datasets that have been created or enhanced as part of this thesis in\norder to investigate on the introduced research questions, to facilitate experimentation and to\nfoster further research.\n1http://ec.europa.eu/research/infrastructures/\n45\n4.1 The Million Song Dataset\nThe Million Song Dataset (MSD) is a music dataset presented in 2011 [20]. It provides a huge\ncollection of meta-data for one million contemporary popular songs. This was the ﬁrst dataset\nwhich facilitated large-scale experimentation for a wide range of Music Information Retrieval\n(MIR) related tasks. Meta-data information include essential attributes such as track titles, artist\nand album names, but also references to third party metadata-repositories such as MusicBrainz\nor 7Digital. Additionally, a number of descriptive feature-sets extracted from the original audio\nﬁles using a proprietary feature extractor from the former start-up The Echo Nest2 are provided.\nIn the meanwhile several additions have been provided to complement the initial MSD data such\nas the Second Hand Dataset [18] for cover-song identiﬁcation, the musiXmatch dataset3 which\nlinks a huge collection of song lyrics to the MSD, and of course the additional contemporary\nfeatures extracted from samples downloaded from 7Digital (see Chapter 4.1).\nUnfortunately, there are no easily obtainable audio ﬁles available for this dataset, and there-\nfore, researchers are practically restricted to benchmarking on algorithms that work on top of\nthe existing features, such as recommendation of classiﬁcation, but cannot easily develop new\nor test existing feature sets on this dataset. The availability of just one feature set also does not\nallow an evaluation across multiple feature sets. As previous studies showed, however, there\nis no single best feature set, but their performance depends very much on the dataset and the\ntask. This section therefore aims to alleviate these restrictions by providing a range of features\nextracted for the Million Song Dataset, such as MFCCs and a set of low-level features extracted\nwith the jAudio feature extraction software as well as the Marsyas framework [291], and the\nRhythm Patterns and derived feature sets [154].\nA second shortcoming of the MSD is that it initially did not contain mappings to ground\ntruth labels such as genres or moods. Thus, experimental evaluations such as musical genre\nclassiﬁcation, a popular task in MIR research, are not possible. This section therefore further\nintroduces ground truth assignments created from human annotated metadata, obtained from\nthe All Music Guide 4. Speciﬁcally, different assignments on two levels of detail, with 13 top-\nlevel-genres and 25 sub-genres are proposed which are provided as a number of partitions for\ntraining and test-splits, with different ﬁlters applied, allowing several evaluation settings. Both\nthe feature sets and the partitions are available for download from our website5. The features are\nprovided in WEKA Attribute-Relation File Format (ARFF) [309], with one attribute being the\nunique identiﬁer of the song in the MSD. Further, a set of scripts are provided to join features\nwith genre assignments so they can be used in classiﬁcation experiments.\nThe remainder of this section is structured as follows. Section 4.1.1 introduces the dataset\nand the properties of the audio samples, while Section 4.1.2 describes the sets of features ex-\ntracted from them. Section 4.1.3 details on the genre assignment obtained, and in Section 4.1.4,\nthe benchmark partitions and how they aim to facilitate exchange between researchers are de-\nscribed.\n2http://the.echonest.com\n3http://labrosa.ee.columbia.edu/millionsong/musixmatch\n4http://allmusic.com\n5http://www.ifs.tuwien.ac.at/mir/msd/\n46\n1\n10\n100\n1000\n10000\n100000\n1000000\n1\n31\n61\n91\n121\n151\n181\n211\n241\n271\n301\n331\n361\n391\n421\n451\n481\n511\n541\n571\n601\n631\nFigure 4.1: Distribution of sample length. The x-axis represents the length of the audio samples.\nThe logarithmic-scaled y-axis represents the number of corresponding ﬁles.\n4.1.1 The original MSD\nThe biggest asset of the MSD is its size. Until its introduction all research datasets ranged\nin the size of a few thousand tracks. Comparing these to catalogs of of millions of tracks of\ncontemporary music streaming services or collections of cultural heritage institutions such as\nEuropeana6 [238] the question comes up if approaches evaluated on such small datasets can\ngeneralize on these large collections as well.\nThe MSD was initially distributed with high- and low-level music features which were ex-\ntracted using a proprietary feature extractor developed by the company The Echonest. These\nfeatures include tempo, loudness, timings of fade-in and fade-out, and MFCC-like features for a\nnumber of segments (for a full description of the provided features, please refer to [20]). More-\nover, a range of other meta-data has been published recently, such as song lyrics (for a subset of\nthe collection), or tags associated to the songs from Last.fm 7. Its meta-data further contains a\nunique identiﬁer to the content provider 7digital8, which could be used to download a sample of\nthe original audio ﬁle.\nAudio\nA major part of the effort described in this section is based on the acquisition of all of these\nsamples. For some songs no sample could be downloaded, as the identiﬁer was unknown to\n7digital. This may be due to data curation efforts of 7Digital or result from changed or expired\n6http://www.europeana.eu\n7http://www.last.fm\n8http://www.7digital.com\n47\nTable 4.1: Audio properties of 7Digital Samples\nSample Length Samplerate\n30 Seconds 366,130 36.80% 22.050 Hz 768,710 77.26%\n60 Seconds 596,630 59.97% 44.100 Hz 226,169 22.73%\nother 32,200 3.24% other 81 0.01%\nBitrate Channels\n128 kBits 646,120 64.94% Mono 6,342 0.64%\n64 kBits 343,344 34.51% Stereo 150,779 15.15%\nVariable Bitrate < 64 kBits 650 0.07% Joint stereo 837,702 84.20%\nVariable Bitrate 64-128 kBits 935 0.09% Dual channel 134 0.01%\nVariable Bitrate > 128 kBits 3,909 0.39%\nlicensing contracts. Although, the data collection was performed less than one year after the\npublication of the MSD, it was only possible to obtain a total of 994,960 audio samples, i.e. a\ncoverage of 99.5% of the dataset (the list of missing audio samples is available on the website).\nThis points to an important issue related to the use of external on-line resources for scientiﬁc\nexperimentation. Especially when the provider is not genuinely interested in the actual research\nperformed, there is little motivation to maintain the data accessible in unmodiﬁed manners, and\nit is thus susceptible to changes and removal. Thus, maintaining a copy of a ﬁxed set of data\nis essential in benchmarking to allow the evaluation of newly developed feature sets, and for\nacoustic evaluation of the results. In total, the audio ﬁles amount to approximately 625 gigabyte\nof data.\nThe audio samples do not adhere to a common encoding quality scheme, i.e. they differ\nin length and quality provided. Figure 4.1 shows a plot of the sample lengths; please note\nthat the scale is logarithmic. It can be observed that there are two peaks at sample lengths of\n30 and 60 seconds with 366,130 and 596,630 samples, respectively, for a total of 96,76% of\nall the samples. These shorter snippets normally contain a section in the middle of the song.\nMany other well-known collections in the MIR domain also contain only 30 second snippets,\nand feature extraction algorithms normally deal with this. Table 4.1 gives an overview on the\naudio quality of the samples. The majority, more than three quarters, of the audio snippets have\na sample rate of 22khz, the rest has a sample rate of 44khz (with the exception of 81 songs, of\nwhich the majority have 24 and 16khz). Regarding the bitrate, approximately two-thirds of the\nsongs are encoded with 128kbit, the majority of the rest with 64kbit; only about half a percent\nof the songs come with higher (192 or 320kbps) or variable bitrates (anywhere between 32 and\n275kpbs). Almost all samples are provided in some form of stereo encoding (stereo, joint stereo\nor dual channels) – only 0.6% of them have only one channel. These characteristics, speciﬁcally\nthe sample rate, may have signiﬁcant impact on the performance of the data analysis algorithms.\nThis had to be considered for stratiﬁcation purposes when designing the benchmark splits.\n48\n4.1.2 Feature Sets\nThe MSD was originally introduced as a collection of pre-extracted low- to high-level features-\nsets and the corresponding metadata. This section ﬁrst describes the original feature-sets pro-\nvided by the MSD and then the contributions added along with this thesis.\nEchonest Features (Original MSD Feature-Sets)\nThe Echonest Analyzer [122] is a music audio analysis tool available as a free Web service\nwhich is accessible over the Echonest API9. In a ﬁrst step of its analysis the Echonest Analyzer\nuses audio ﬁngerprinting to locate tracks in the Echonest’s music metadata repository. Music\nmetadata returned by the Analyzer includes artist information (name, user applied tags including\nweights and term frequencies, a list of similar artists), album information (name, year) and\nsong information (title). Additionally a set of identiﬁers is provided that can be used to access\ncomplimentary metadata repositories (e.g. musicbrainz10, playme11,7digital).\nFurther information provided by the Analyzer is based on audio signal analysis. Two major\nsets of audio features are provided describing timbre and pitch class information of the corre-\nsponding music track. Unlike conventional MIR feature extraction frameworks, the Analyzer\ndoes not return a single feature vector per track and feature. The Analyzer implements an onset\ndetector which is used to localize music events called Segments. These Segments are described\nas sound entities that are relatively uniform in timbre and harmony and are the basis for fur-\nther feature extraction. For each Segment the following features are derived from musical audio\nsignals:\nSegments Timbre are casually described as MFCC-like features. A 12\ndimensional vector with unbounded values centered\naround 0 representing a high level abstraction of the\nspectral surface (see Figure 4.2).\nSegments Pitches are casually described as Chroma-like features. A\nnormalized 12 dimensional vector ranging from 0 to\n1 corresponding to the 12 pitch classes C, C#, to B.\nSegments Loudness Max represents the peak loudness value within each seg-\nment.\nSegments Loudness Max Time describes the offset within the segment of the point\nof maximum loudness.\nSegments Start provide start time information of each segment/onset.\nOnset detection is further used to locate perceived musical events within a Segment called\nTatums. Beats are described as multiple of Tatums and each ﬁrst Beat of a measure is marked\n9http://developer.echonest.com - API ofﬂine since May 2016 (see Infobox)\n10http://musicbrainz.org\n11http://www.playme.com\n49\nFigure 4.2: First 200 timbre vectors of ’With a little help from my friends’ by ’Joe Cocker’\nas a Bar. Contrary to Segments, that are usually shorter than a second, the Analyzer also detects\nSections which deﬁne larger blocks within a track (e.g. chorus, verse, etc.). From these low-level\nfeatures some mid- and high-level audio descriptors are derived (e.g. tempo, key, time signature,\netc.). Additionally, a conﬁdence value between 0 and 1 is provided indicating the reliability of\nthe extracted or derived values - except for a conﬁdence value of ’-1’ which indicates that this\nvalue was not properly calculated and should be discarded. Based on the audio segmentation and\nadditional audio descriptors the following features provide locational informations about music\nevents within the analyzed track:\nBars/Beats/Tatums start the onsets for each of the detected audio segments\nSections start the onsets of each section.\nFadein stop the estimated end of the fade-in\nFadeout start the estimated start of the fade-out\nAdditionally a set of high-level features derived from previously described audio descriptors is\nreturned by the Analyzer:\nKey the key of the track (C,C#,...,B)\nMode the mode of the track (major/minor)\nTempo measured in beats per minute\nTime Signature three or four quater stroke\nDanceability a value between 0 and 1 measuring of how dance-\nable this song is\nEnergy a value between 0 and 1 measuring the perceived\nenergy of a song\nSong Hotttnesss a numerical description of how hot a song is (from\n0 to 1)\nA wide range of audio features are extracted from the downloaded samples, namely features\nprovided by the jAudio feature extraction software (which is a part of the jMIR package [185]),\nthe MARSY AS feature extractor [291], and the Rhythm Patterns family of feature sets [154].\nAn overview on these features is given in Table 4.2.\n50\nTable 4.2: Overview on features extracted from the MSD samples.Dim. denotes the dimension-\nality, Deriv. derivatives computed from the base features\n# Feature Set Extractor Dim Deriv.\n1 MFCCs [219] MARSAYS 52\n2 Chroma [85] MARSAYS 48\n3 Timbral [291] MARSAYS 124\n4 MFCCs [219] jAudio 26 156\n5 Low-level spectral features [185] (Spectral Centroid, Spectral\nRolloff Point, Spectral Flux, Compactness, and Spectral Variability, Root Mean Square,\nZero Crossings, and Fraction of Low Energy Windows)\njAudio 16 96\n6 Method of Moments [185] jAudio 10 60\n7 Area Method of Moments [185] jAudio 20 120\n8 Linear Predictive Coding [185] jAudio 20 120\n9 Rhythm Patterns [154] rp_extract 1440\n10 Statistical Spectrum Descriptors [154] rp_extract 168\n11 Rhythm Histograms [154] rp_extract 60\n12 Modulation Frequency Variance Descriptor [163] rp_extract 420\n13 Temporal Statistical Spectrum Descriptors [163] rp_extract 1176\n14 Temporal Rhythm Histograms [163] rp_extract 420\njAudio\nThe jAudio software provides a range of 28 features associated with both the frequency and\ntime domains. It includes several intermediate-level musical features, mainly related to rhythm,\nas well as lower-level signal processing-oriented features. It also provides an implementation\nof MFCC features [219], using 13 coefﬁcients. Further features are for example the Spectral\nFlux, a measure of the amount of spectral change in a signal from frame to frame, or Root\nMean Square (RMS), a measure of the average energy of a signal calculated over an analy-\nsis window. In addition, a set of statistics is computed, including mean, standard deviations\nand derivatives. Speciﬁcally extracted were the low-level features Spectral Centroid, Spectral\nRolloff Point, Spectral Flux, Compactness, and Spectral Variability, Root Mean Square, Zero\nCrossings, and Fraction of Low Energy Windows (16 dimensions), and derivatives thereof (96\ndimensions). The Method of Momentsextractor captures the ﬁrst ﬁve statistical moments of the\nmagnitude spectrum, and then computes average and standard deviations over all segments (10\ndimensions), and derivatives thereof (60 dimensions). The Area Method of Moments extractor\nadditionally analyses the time series of the spectrum (20 dimensions, and derivatives thereof\n(120 dimensions)). Linear Predictive Coding estimates formants (spectral bands correspond-\ning to resonant frequencies in the vocal tract), ﬁlters them out, and estimates the intensity and\nfrequency of the residual “buzz” that is assumed to be the original excitation signal (20 dimen-\nsions, and derivatives thereof (120 dimensions)). jAudio computes in general mean and standard\ndeviations over the sequence of frames, and provides for most measures also derivatives, i.e. ad-\nditional statistical moments over the basic measures. For the extraction, jAudio as bundled in\n51\nthe jMIR 2.4 release12 was utilized.\nMARSYAS\nA very popular audio feature extraction system is MARSAYS, one of the ﬁrst comprehensive\nsoftware packages to be available to MIR researchers. A very popular set from this audio extrac-\ntor is the so-called “timbral” set, which is composed of 13 MFCC coefﬁcients, and the twelve\nchroma features and the average and minimum chroma value, and the four low-level features\nzero crossings, and rolloff, ﬂux and centroid of the spectrum. For these 31 values, four statisti-\ncal moments are computed, resulting in a 124 dimensional vector. Chroma features [85] aim to\nrepresent the harmonic content (e.g, keys, chords) of an audio by computing the spectral energy\npresent at frequencies that correspond to each of the 12 notes in a standard chromatic scale (e.g.,\nblack and white keys within one octave on a piano). For the extraction, MARSY AS version\n0.4.513 was utilized.\nrp_extract\nThe Rhythm Patterns and related features sets are extracted from a spectral representation, par-\ntitioned into segments of 6 sec. Features are extracted segment-wise, and then aggregated for a\npiece of music computing the median (Rhythm Patterns, Rhythm Histograms) or mean (Statisti-\ncal Spectrum Descriptors, Modulation Frequency Variance Descriptor) from features of multiple\nsegments. For details on the computation - the feature extraction will only be described very\nbrieﬂy - please refer to [154]. The feature extraction for a Rhythm Pattern is composed of two\nstages. First, the speciﬁc loudness sensation on 24 critical frequency bands is computed through\na Short Time FFT, grouping the resulting frequency bands to the Bark scale, and successive\ntransformation into the Decibel, Phon and Sone scales. This results in a psycho-acoustically\nmodiﬁed Sonogram representation that reﬂects human loudness sensation. In the second step,\na discrete Fourier transform is applied to this Sonogram, resulting in a spectrum of loudness\namplitude modulation per modulation frequency for each critical band. After additional weight-\ning and smoothing steps, a Rhythm Pattern exhibits magnitude of modulation for 60 modulation\nfrequencies on the 24 critical bands [154]. A Rhythm Histogram (RH) aggregates the modu-\nlation amplitude values of the critical bands computed in a Rhythm Pattern and is a descriptor\nfor general rhythmic characteristics in a piece of audio [154]. The ﬁrst part of the algorithm for\ncomputation of a Statistical Spectrum Descriptor (SSD), the computation of speciﬁc loudness\nsensation, is equal to the Rhythm Pattern algorithm. Subsequently at set of statistical values14 are\ncalculated for each individual critical band. SSDs describe ﬂuctuations on the critical bands and\ncapture additional timbral information very well [154]. The Modulation Frequency Variance\nDescriptor (MVD) [163] measures variations over the critical frequency bands for a speciﬁc\nmodulation frequency (derived from a Rhythm Pattern). It is computed by taking statistics for\none modulation frequency over the critical bands, for each of the 60 modulation frequencies. For\n12available from http://jmir.sourceforge.net/\n13available from http://sourceforge.net/projects/marsyas/\n14minimum, maximum, mean, median, variance, skewness, and kurtosis\n52\nthe Temporal Rhythm Histograms(TRH) [163], statistical measures are computed over the in-\ndividual Rhythm Histograms extracted from the segments in the audio. Thus, the TRHs capture\nchange and variation of rhythmic aspects in time. Similarly, the Temporal Statistical Spec-\ntrum Descriptor (TSSD) [163] capture the same statistics over the individual SSD segments,\nand thus describe timbral variations and changes over time in the spectrum on the individual\ncritical frequency bands. For the extraction, the Matlab-based implementation, version 0.641115\nwas utilized.\nIt was intentional to provide two different versions of the MFCCs features, as this will allow\nfor interesting insights in how these implementations differ on various MIR tasks.\nPublication of Feature Sets\nAll features described above were made available for download, encoded in the WEKA Attribute-\nRelation File Format (ARFF) [309]. The features are available under the Creative Commons\nAttribution-NonCommercial-ShareAlike 2.0 Generic License16.\nTo allow high ﬂexibility when using them, one ARFF ﬁle for each type of features is pro-\nvided. These can then be combined in any particular way when performing experimental eval-\nuations. A set of scripts is provided as well on the website for this. In total, the feature ﬁles\namount to approximately 40 gigabyte of uncompressed text ﬁles. The ﬁles contain the numeric\nvalues for each feature, and additionally the unique identiﬁer assigned in the MSD. This way, it\nis possible to generate various feature ﬁles with different ground truth assignments. Scripts to\naccomplish this are also provided. The proposed assignment into genres for genre classiﬁcation\ntasks is described in Section 4.1.3.\n4.1.3 Allmusic Ground-Truth Assignments\nThe All Music Guide (AMG) [47] was initiated by an archivist in 1991 and emerged 1995 from\nits book form into a database which can be accessed through the popular commercial Web page\nallmusic.com. The Web page offers a wide range of music information, including album reviews,\nartist biographies, discographies as well as classiﬁcations of albums according to genres, styles,\nmoods and themes. This information is provided and curated by music experts.\nThe genre information is provided as a single tag for each album. The genre taxonomy\nis very coarse. The two main categories Pop and Rock are combined into a single genre\n’Pop/Rock’. Additionally to genre labels, style tags are provided allowing for a more speciﬁc\ncategorization of the annotated albums. Labels are not applied exclusively. Multiple style tags\nare applied for each album, but unfortunately no weighting scheme can be identiﬁed and in\nmany cases only one tag is provided. Style tags also tend to be even more generic than genre la-\nbels. Especially non-American music is frequently tagged with labels describing the originating\ncountry or region as well as the language of the lyrics. Instrumentation, situational descriptions\n(e.g. Christmas, Halloween, Holiday, etc.) as well as confessional or gender attributes (e.g.\nChristian, Jewish, Female, etc.) are also provided. Unfortunately these attributes are not used\n15available from http://www.ifs.tuwien.ac.at/mir/downloads.html\n16http://creativecommons.org/licenses/by-nc-sa/2.0/\n53\nas isolated synonyms, but are concatenated with conventional style information (e.g. Japanese\nRock, Christian Punk, Classic Female Blues, etc.).\nAllmusic.com assembles styles to meta-styles which can be interpreted as a hierarchy of sub\ngenres used to diversify the major genre labels. Meta-styles are not distinctive and are used\noverlapping in many meta-styles (e.g. Indie Electronic is contained in the meta-styles Indie\nRock, Indie Pop and Alternative/Indie Rock).\nGenre Data Collection\nData was collected automatically from Allmusic.com using a web-scraping script based on direct\nstring matching to query for artist-release combinations. From the resulting Album Web page\ngenre and style tags were collected. It was possible to retrieve 21 distinct genre labels for\n62,257 albums which initially provided genre tags for 433,714 tracks. Style tags were extracted\nattributing only 42,970 albums resulting in 307,790 labeled tracks. An average of 3.25 tags out\nof a total of 905 styles were applied to each album, but 5,742 releases were only tagged with\na single style label. The most popular genre with 32,696 tagged albums, was Pop/Rock - this\nis 10% more than the sum of all other genres. Considering tracks the difference rises to 30%.\nFurther, the granularity of Rock is very coarse, including Heavy Metal, Punk, etc. A similar\npredominating position of this genre was also reported by [17]. The most popular style tag\nis Alternative/Indie Rock applied to 12,739 albums, which is more than twice as much as the\nsecond most popular style Alternative Pop/Rock. About 120 tags describe the country of the\nperforming artist or the language of the interpretation - the most common among them isItalian\nMusic which has been applied to 610 albums.\nAllmusic Genre Dataset\nThe Allmusic Genre Dataset is provided as an unoptimized expert annotated ground truth dataset\nfor music genre classiﬁcation. Two partitions of this set are provided. The MSD Allmusic\nGenre Dataset (MAGD) assembles all collected genres including generic and small classes. The\nsecond partition - MSD Allmusic Top Genre Dataset (Top-MAGD) - consists of 13 genres -\nthe 10 major genres of Allmusic.com (Pop/Rock, Jazz, R&B, Rap, Country, Blues, Electronic,\nLatin, Reggae, International) including the three additional genres V ocal, Folk, New Age (see\nTable 4.3). Generic genres as well as classes with less than 1% of the number of tracks of the\nbiggest class Pop/Rock are removed. Due to the low number of tracks, the Classical genre is\nalso removed from the Top Genre dataset.\nAllmusic Style Dataset\nThe Allmusic Style Dataset attempts to more distinctively separate the collected data into differ-\nent sub-genres, alleviating dominating classes. For the compilation of the dataset genre labels\nare omitted and solely style tags are used. In a ﬁrst step metastyle description as presented on\nthe Allmusic.com Web site are used to map multiple style tags to a single label name - in this\ncase the metastyle name is used. This simple aggregation approach generates a total of 210 la-\nbels many of them highly generic or hierarchical specializing (e.g. Electric Blues and Electric\n54\nTable 4.3: MSD Allmusic Genre Dataset (MAGD) - upper part represents the MSD Allmusic\nTop Genre Dataset (Top-MAGD)\nGenre Name Number of Songs\nPop/Rock 238,786\nElectronic 41,075\nRap 20,939\nJazz 17,836\nLatin 17,590\nR&B 14,335\nInternational 14,242\nCountry 11,772\nReggae 6,946\nBlues 6,836\nV ocal 6,195\nFolk 5,865\nNew Age 4,010\nReligious 8814\nComedy/Spoken 2067\nStage 1614\nEasy Listening 1545\nAvant-Garde 1014\nClassical 556\nChildrens 477\nHoliday 200\nTotal 422,714\nChicago Blues. The MSD Allmuisc Metastyle Dataset - Multiclass (MAMD) is derived from\nthese 210 resulting metaclasses. Each track is matched to one or more metaclasses according to\nits style tags. In a second step confessional, situational and language speciﬁc labels are removed\nfrom the initial set of 905 style tags. Regional tags are discarded if they do not refer to a speciﬁc\ntraditional cultural music style (e.g. African Folk). Popular music attributed with regional infor-\nmation is discarded due to extensive genre overlaps (e.g. Italian Pop ranges from Hip-Hop to\nHard-Rock). Finally, these genres are successfully merged into general descriptive classes until\nthe dataset is ﬁnalized into the MSD Allmusic Style Dataset (MASD) presented in Table 56. For\ncompleteness also the MSD Allmuisc Style Dataset - Multiclass (Multi-MASD)is provided. This\nset contains the pure track-style mapping as collected from Allmusic.com.\nDerivates of the Allmusic Dataset\nThe genre annotations of the Allmusic Genre Datset, speciﬁcally the Top-MAGD genre assign-\nments, are further processed by Schreiber [256]. The Top-MAGD annotations are combined\n55\nTable 4.4: The MSD Allmusic Style Dataset (MASD)\nGenre Name Number of Songs\nBig Band 3,115\nBlues Contemporary 6,874\nCountry Traditional 11,164\nDance 15,114\nElectronica 10,987\nExperimental 12,139\nFolk International 9,849\nGospel 6,974\nGrunge Emo 6,256\nHip Hop Rap 16,100\nJazz Classic 10,024\nMetal Alternative 14,009\nMetal Death 9,851\nMetal Heavy 10,784\nPop Contemporary 13,624\nPop Indie 18,138\nPop Latin 7,699\nPunk 9,610\nReggae 5,232\nRnB Soul 6,238\nRock Alternative 12,717\nRock College 16,575\nRock Contemporary 16,530\nRock Hard 13,276\nRock Neo Psychedelia 11,057\nTotal 273,936\nwith additional crowd sourced genre labels retrieved from Last.fm17 and Beatunes18. Latent Se-\nmantic Analysis (LSA) is applied to infer genre taxonomies which are then matched with genre\nassignments provided for the MSD (including Top-MAGD). Matching is approached by using\nmajority voting and truth by consensus. Assignments are provided for both approaches. They\nare also partitioned corresponding to the stratiﬁed 90%, 80%, 66% and 50% splits, presented in\nthis chapter.\n4.1.4 Benchmark Partitions\nInﬂuenced by the tremendous success in the text classiﬁcation domain, speciﬁcally with the\nlandmark Reuters-21578 corpus, a number of benchmark partitions are provided that researchers\n17http://last.fm\n18http://www.beatunes.com/\n56\ncan use in their future studies, in order to facilitate repeatability of experiments with the MSD\nbeyond x-fold cross validation. The following categories of splits are provided:\n• Splits with all the ground truth assignments into genre and style classes, described in\nSection 4.1.3.\n• Splits with just the majority classes from these two ground truth assignments.\n• Splits considering the sample rate of the ﬁles, i.e. only the 22khz samples, only the 44khz\nsamples, and a set with all audio ﬁles.\nIn particular, the following size partitions were provided:\n• “Traditional” splits into training and test sets, with 90%, 80%, 66% and 50% size of the\ntraining set, applying stratiﬁcation of the sampling to ensure having the same percentage\nof training data per class, which is important for minority classes.\n• A split with a ﬁxed number of training samples, equally sized for each class, with 2,000\nand 1,000 samples per class for the genre and style data sets, respectively. This excludes\nminority classes with less than the required number of samples.\nFinally, stratiﬁcation on other criteria than just the ground truth class was applied, namely:\n• Splits into training and test sets with an artist ﬁlter, i.e. avoiding to have the same artist in\nboth the training and test set; both stratiﬁed and non-stratiﬁed sets are provided\n• As above, but with an album ﬁlter, i.e. no songs from the same album appear in both\ntraining and test set, to account for more immediate production effects\n• As above, but with a time ﬁlter, i.e. for each genre using the earlier songs in the training\nset, and the later releases in the test set.\nThis dataset and its corresponding evaluation presented in this chapter was published and presented in Alexander\nSchindler, Rudolf Mayer and Andreas Rauber. “Facilitating comprehensive benchmarking experiments on the million\nsong dataset” at the 13th International Society for Music Information Retrieval Conference (ISMIR 2012), pages\n469-474, Porto, Portugal, October 8-12 2012 [246].\n57\n4.2 The Music Video Dataset\nTo facilitate comparable results and reproducible research on music related visual analysis of\nmusic videos the Music Video Dataset (MVD) is introduced sequentially in [248,250] and [252]\n(see Chapters 6 to 9). The MVD follows the Cranﬁeld paradigm [42] and provides test collec-\ntions of multimedia documents and corresponding ground truth assignments within the context\nof well-deﬁned tasks. The main focus of the dataset is set on the development and evaluation\nof visual or audio-visual features that can be used to augment or substitute audio-only based\napproaches. The MVD consists of four major subsets that can be combined into two bigger\ntask related collections. The strong emphasis on classiﬁcation experiments is motivated by their\nfacilitation of rapid content descriptor development. The data-sets are carefully selected to be\nspecialized tools in this process. Despite their different focuses, all sub-sets are non-overlapping\nand thus can be mutually combined. This section provides a detailed overview of the distinct\ndataset properties and class/genre descriptions as well as its creation.\nThe four major sub-sets of the MVD are:\nMVD-VIS: The Music Video Dataset for VISual content analysis and classiﬁca-\ntion (see Chapter 4.2.2) is intended for classifying music videos by\ntheir visual properties only.\nMVD-MM: The Music Video Dataset for MultiModal content analysis and clas-\nsiﬁcation (see Chapter 4.2.3) is intended for multi-modal classiﬁca-\ntion and retrieval tasks.\nMVD-Themes: The MVD-Themes data-set is a collection based on music themes\nwhich span across multiple genres such as “Christmas Music” or\n“Portest Songs”.\nMVD-Artists: The MVD-Artists data-set is a collection of music videos by 20\npopular western music artists.\nFurther these sub-sets can be combined into the following two larger sets:\nMVD-MIX: The MVD-MIX data-set is a combination of the data-sets MVD-\nVIS and MVD-MM (see Chapter 4.2.4) which can be used to evalu-\nate the performance and stability of a classiﬁcation approach accord-\ning to a higher number of classes.\nMVD-Complete: The MVD-Complete data-set is the combination of all the\nsub-sets of the Music Video Dataset.\n58\n4.2.1 Dataset Creation\nThe dataset creation is preceded by the selection of genres. To align the dataset to contemporary\nmusic repositories a pre-selection is based on the Recording Industry Association of America’s\n(RIAA) report on consumer expenditures for sound recordings [202] which separates proﬁles\ninto the following genres: Rock, Pop, Rap/Hip Hop, R&B/Urban, Country, Religious, Classical,\nJazz, Soundtracks, Oldies, New Age, Children’s and Other. For the MVD-VIS dataset (see Sec-\ntion 4.2.2) eight orthogonal classes with minimum overlap are deﬁned. This aim is accomplished\nby restricting the search on clearly deﬁned sub-genres. For the MVD-MM dataset (see Section\n4.2.3) eight top-level genres with high inter-genre overlaps are selected. Additional avoidance\nof overlaps between the genres of these two subsets allow for a combination into the bigger\nMVD-MIX (see Section 4.2.4) dataset. Each class consists of 100 videos which are primarily\nselected by their acoustic properties. The class labels are manually assigned and only refer to\ncommonly known music genres. They do not infer to be accurate in musicological terms and are\nnot result of a common agreement. This decision is based on the introducing deﬁnition of Music\nVideo Information Retrieval as a cross-domain approach to MIR problems . After listening to\nthe tracks the video properties are inspected. A set of criteria has been strictly applied to the\nselection process (see Figure 4.3).\nThese criteria and the variance constraints of the subset’s genres makes the selection process\ncomplex and exhaustive. More than 6000 videos are examined by listening and watching to\nthem. Especially older music video productions are ﬁltered out due to insufﬁcient sound or\nvideo quality. Videos are downloaded from Youtube in MPEG-4 format.\n4.2.2 MVD-VIS\nThe Music Video Dataset for VISual content analysis (MVD-VIS) is intended for feature de-\nvelopment and optimization. To facilitate this, 800 tracks of eight clearly deﬁned and well dif-\nferentiated sub-genres were aggregated (see Table 4.5). Their tracks were selected concerning\nminimal with-in class variance in acoustic characteristics, thus sharing high similarity in in-\nstrumentation, timbre, tempo, rhythm and mood. Audio classiﬁcation results provided in Table\n9.1 reﬂect that state-of-the-art audio-content based approaches can accurately discriminate such\nwell differentiated classes. Based on the premise that the tracks of a class sound highly similar,\nthese results should serve as a baseline for the task of identifying patterns of similarity within\nthe visual layer as well as developing means to extract this information. Music genre classiﬁ-\ncation based on conventional audio features provides accuracy results above-average (see Table\n8.2) compared to current benchmarks of the Music Information Retrieval domain as presented\nin Chapter 5.2.2 and by [81].\nClass Descriptions\nBollywood This class represents a collection of Hindi songs that have either fea-\ntured in Bollywood ﬁlms or are music videos on their own. The\nmusic is based on Indian ragas but includes western harmonies and\nmelodies. The traditional rhythm is sometimes merged with con-\n59\n– Quality ﬁlter:\n– A minimum of 90 kBits/s audio encoding\n– A video resolution ranging from QVGA (320x240) to VGA (640x480)\n– Content ﬁlter:\n– Only ofﬁcial music videos\n– No or minimal watermarking\n– No lyric-videos (Videos showing only lyrics)\n– No non-representational (not showing artists)\n– No live performance, abstract or animated videos\n– No videos with intro/outro longer than 30 seconds\n– Stratiﬁcation:\n– Only two tracks by the same artist in MVD-VIS, MVD-MM and MVD-MIX (excep-\ntions: Bollywood, Opera)\n– Artists of the MVD-Artists dataset do not feature other artists of this set.\n– Tracks of the MVD-Themes dataset are not contained in the MVD-VIS or MVD-MM\nset.\nThe stratiﬁcation rule for the Bollywood and Opera genre of the MVD-VIS dataset was\nsubstituted by:\n– Bollywood: only two tracks from the same movie\n– Opera: only two tracks of the same opera/performance\nFor the MVD-ARTISTS dataset the following additional criteria were considered:\n– has to be an ofﬁcial music video produced by the artist\n– the lead singer has to appear in the video\nFigure 4.3: List of quality criteria and ﬁlter rules considered during the selection and accu-\nmulation of the Music Video Dataset.\ntemporary electronic dance music. The instrumentation is a mixture\nof the predominating Indian Sitar, Tabla, Bansuri, Shehnai and per-\ncussions, as well as western instruments including synthesizers and\ndrum computers.\nCountry A collection of contemporary north American popular music with its\nroots in American folk music, with focus on the sub-genres Honky-\nTonk, Hillbilly and Country Rock. Dominating instruments are pedal\nsteel guitar, ﬁddle and piano. Tracks have an strong emphasis on\n60\nMVD-VIS MVD-MM\nGenre Videos Artists Genre Videos Artists\nBollywood 100 32 80s 100 72\nCountry 100 70 Dubstep 100 78\nDance 100 84 Folk 100 66\nLatin 100 72 Hard Rock 100 69\nMetal 100 76 Indie 100 64\nOpera 100 NA Pop Rock 100 65\nRap 100 81 Reggaeton 100 69\nReggae 100 75 RnB 100 67\nMVD-MIX MVD-Themes\nChristmas 56 42\nMVD-VIS + MVD-MM 1600 1040 K-Pop 50 39\n16 Genres Broken Heart 56 48\nProtest Songs 50 42\nMVD-Artists (v2.0)\nArtist Name Videos Artist Name Videos Artist Name Videos\nAerosmith 23 Jennifer Lopez 23 Nickelback 18\nAvril Lavigne 20 Justin Timberlake 12 P!nk 23\nBeyonce 26 Katy Perry 12 Rihanna 25\nBon Jovi 27 Madonna 30 Shakira 24\nBritney Spears 25 Maroon 5 14 Taylor Swift 20\nChristina Aguilera 15 Matchbox Twenty 13 Train 11\nFoo Fighters 23 Nelly Furtado 16\nMVD-Complete\nMVD-VIS + MVD-MM + MVD-THEMES + MVD-ARTISTS 2212\nTable 4.5: The Music Video Dataset - Detailed Overview of structure including class description,\nnumber of artists, average Beats per Minutes and standard deviation per genre.\nmid-tempo upbeat rhythms with a gently-swinging shufﬂe.\nDance This class focuses on House music, a sub-genre of electronic dance-\nmusic. With an emphasis on Deep-, Electro- and Progressive-House\nits sound is characterized by repetitive 4/4 base drum beats, off-beat\nhi-hat cymbals and ambient synthesizers.\nLatin Bachata is the traditional folk music of the Dominican Republic. It\nwas strongly inﬂuenced by Merengue and features a high pitched\n61\nrequinto guitar playing main theme and interludes as well as a char-\nacteristic rhythm using Guira and Bongos.\nMetal This class is dominated by aggressive sub-genres of Heavy Metal\nmusic (e.g. Metalcore, Thrash Metal, etc.) characterized by heavily\ndistorted guitars and aggressive drum plays and singing.\nOpera Single tracks or short excerpts of opera performances recorded by of-\nﬁcial TV stations. The video is edited using the recording of several\ncameras shot from different angles. Characteristics of these record-\nings are similar to music videos.\nRap This class focuses on Rap music of the 90ies also known as Gangsta\nRap which is a sub-genre of Hip-Hop music. The music is typically\ndescribed as noisy, dominated by drum machine rhythms that are ac-\ncompanied by simple repeated synthesizer melodies and bass lines.\nReggae The Reggae collection refers to traditional Jamaican Reggae style\nwith its typical staccato chords played on the offbeats including up-\nbeat tempo Ska and slower Rocksteady songs. Many recordings\noriginate from TV broadcasts and have lower video quality due to\nlighting.\n4.2.3 MVD-MM\nThe structure of theMusic Video Dataset for MultiModal content analysis (MVD-MM) is aligned\nto the MVD-VIS but its classes are less well differentiated. The heterogeneous distributions of\ninter and intra class variance refer to problems of imprecision and subjectivity of music genre\ndeﬁnitions [186] which are observed in current music classiﬁcation datasets [276]. This var-\niance was intentionally introduced to facilitate comparability with results reported on these\ndatasets [249]. The task is to evaluate and improve the performance of visual features in such\nenvironments and to analyze if state-of-the-art audio-only based approaches can be improved\nthrough audio-visual combinations.\nClass Descriptions\n80ies New Wave music characterized by punk inﬂuences, increased syn-\nthesizer usage and electronic production.\nDubstep Sub-genre of electronic dance music with characteristic reverber-\nating sub-bass modulated at different speeds - also referred to as\nwobble bass - combined with syncopated drum and percussion\npatterns.\nFolk Folk-inﬂuenced romantic or melancholic music ranging from Indie-\nFolk to slow-tempo Indie-Pop/Rock.\n62\nReggaeton Latin American genre with a characteristic percussion rhythm re-\nferred to as Dem Bow.\nRnB Contemporary Rhythm and Blues - a progression of classic R&B\nwith strong electronic inﬂuences including Hip Hop elements and\ndrum-machine rhythms.\nIndie A broad class spreads from Indie Pop/Rock/Folk to Alternative\nRock. Sound can be described as sensitive, melancholic, low-\nﬁdelity with experimental inﬂuences.\nHard-Rock A mix of loud aggressive guitar rock and the pop-oriented Glam\nRock/Metal genre.\nPop-Rock A broad mixture of contemporary dance and mainstream rock mu-\nsic.\n4.2.4 MVD-MIX\nThe MVD-MIX dataset is a combination of the datasets MVD-VIS and MVD-MM. The distinct\ngenres of the subsets have been selected to facilitate a union of the two sets providing a non-\noverlapping bigger set. While MVD-VIS is intended for feature development and optimization\nand MVD-MM is for evaluation, the MVD-MIX set is for evaluating the performance of the\nfeatures concerning their stability towards a higher number of classes.\n4.2.5 MVD-Themes\nThe MVD-Themes set is a collection of thematically tagged classes that span across musical\ngenres. The task is aligned to the MusiClef multi-modal music tagging task [204]. The strong\ncontextual and non-audio connotations of the themes should be captured by information ex-\ntracted from the visual layer. To address cross-lingual and cross-cultural challenges [146] of\nmulti-modal approaches analysing song lyrics [107, 183] (most of these approaches were eval-\nuated only for the English language) the MVD-THEMES set includes performances in various\nlanguages. The following Themes are provided:\nChristmas Tracks that can be related to Christmas.Genres covered: Alternative-\n, Indie- and Hard-Rock, 60s-, 80s- and 90s-Pop, Dance, Rock\n’n Roll, RnB, Soul, Big Band, Country, A capella. Langauges:\nEnglish, Thai.\nK-Pop Korean-Pop is strongly inﬂuenced by western music [145] and\ncharacterized through visual content (synchronized dance for-\nmations, colorful outﬁts). Genres covered: Pop, Dance, RnB,\nRap. Langauges: Korean, English (chorus).\nBroken Heart Songs about sorrowfully loosing someone beloved either through\ndeath or end of a relationship. Genres covered: Rock, Hard\n63\nMVD-Artists (v1.0)\nArtist Name Videos Artist Name Videos\nAerosmith 23 Jennifer Lopez 21\nAvril Lavigne 20 Madonna 25\nBeyonce 19 Maroon 5 10\nBon Jovi 26 Nickelback 18\nBritney Spears 24 Rihanna 21\nChristina Aguilera 14 Shakira 20\nFoo Fighters 19 Taylor Swift 19\nTable 4.6: MVD-Artists (v1.0) - Music Video Dataset subset for 14 Artists as used in Chapter 6.\nRock, Metal, Pop, RnB, Country, Folk, 80s. Languages: En-\nglish, Taiwanese.\nProtest Songs Songs protesting against war, racism, police power and social\ninjustice. Genres covered: Pop, Folk, Rap, Reggae, Rock, Punk\nRock, Metal, Punjabi, Indie, 80s. Languages: English, French,\nGerman, Egyptian Arabic, Hindi.\n4.2.6 MVD-ARTISTS\nThis dataset for audio-visual based artist identiﬁcation using music videos is a set of 20 popular\nwestern music artists listed in Table 4.5. This set was initially introduced with only 14 artists\n(version 1.0, see Table 4.6) for the evaluations presented in Chapter 6. This initial set was further\nexpanded to contain now 20 artists and 124 additional music videos (version 2.0, see Table 4.5).\nPopular artists were chosen to meet the requirement of collecting enough music videos for each\nmusician which predominately belong to the two genres Pop and Rock.\n4.2.7 MVD-COMPLETE\nThe MVD-Complete dataset is a combination of the MVD-MIX and the MVD-Artists datasets\nproviding 2212 music videos for similarity search and recommendation. Since the dedicated\ntasks of the sub-sets are not overlapping, their classes are not semantically related and thus no\nclass-labels are provided.\n64\n4.3 Summary\nThis chapter introduced the main datasets which are used in this thesis to develop and test visual\nfeatures and which are major contributions of this dissertation.\n• The Million Song Dataset (MSD) is one of the biggest datasets made available to the\nMIR research domain. This chapter introduced additional acoustic and music content\ndescriptors for the MSD to overcome some initial shortcomings and to facilitate compre-\nhensive large scale experiments for various content based MIR tasks. Additionally, further\nground-truth assignments for music genres and music styles were introduced. To foster\nexchange between different researchers, for a number of tasks standardized splits between\ntraining and test data are provided.\n• The Music Video Dataset (MVD) is the ﬁrst dataset intended entirely for developing\naudio-visual approaches to MIR. It is composed of four combine-able subsets. These\nsubsets of the MVD are assembled to facilitate the development and evaluation of visual\nand audio-visual content descriptors.\nBaseline results for these datasets will be provided in the next Chapter.\n65\n\nCHAPTER 5\nAudio-only Analysis of the MSD\nThe Music Video Information Retrieval (MVIR) approach presented in this thesis represents a\nnovel way to approach the Music Information Retrieval (MIR) problem space. As discussed in\nChapter 2 music videos have not yet received appropriate attention and research towards MIR\nrelated video analysis is lacking necessary resources such as datasets, ground truth assignments\nor benchmark results to compare new results with. Further, the research questions presented\nin Chapter 1.2 open a new perspective to the MIR problem space with no preceding research\navailable to properly base the necessary experimentation on.\nThis thesis is a step towards providing these resources. The previous chapter already intro-\nduced a set of datasets that can be used for developing and evaluating new approaches to MIR\ntasks using the additional information provided by the visual layer of music videos. One re-\nquirement was to obtain audio ﬁles and ground truth assignments for the Million Song Dataset\nin order to facilitate large scale experiments. A second was to create a representative collec-\ntion of music videos to facilitate audio-visual analysis, feature development and experiments.\nDue to the novelty of these created datsets, they are lacking comparable results. Further, for\nsome datasets, proprietary feature sets are publicly made available which also require proper\npre-analysis to reliably use them in experiments and to compare their performance with results\nreported in literature. This chapter explicates initial experiments that are carried out in order to\nasses these results, to evaluate proper pre-processing steps for different feature sets, as well as\nto provide baseline results for succeeding experiments.\nThis chapter presents two separate evaluations which support the audio-visual analysis of\nmusic videos. Section 5.1 provides audio-only results for the MSD which serve as baselines\nfor the audio-visual approaches. Section 5.2 empirically evaluates the pre-extracted feater-set,\nwhich is provided by the MSD, and provides comparative results on standard music genre clas-\nsiﬁcation datsaets. These results contribute especially to the audio-visual artist identiﬁcation\napproach presented in Chapter 6.\n67\n5.1 Million Song Dataset Experiments\nThe experiments on the Million Song Dataset (MSD) aimed at providing baseline results for\nthe audio-visual experiments. In a ﬁrst step, baseline results for the ground-truth assignments\npresented in 4.1 are provided as a general overview. This sections only details the experimental\nsetup and the results. For full details on the dataset and the evaluated ground-truth assignments\nplease refer to Chapter 4.1. More precisely, the results of a musical genre classiﬁcation exper-\niment on the MSD Allmusic Guide Style Dataset (MASD) with a frequently-used 2/3 training\nand 1/3 test set split will be discussed.\n5.1.1 Initial Music Genre Classiﬁcation Experiments\nTable 5.1 shows classiﬁcation accuracies obtained with ﬁve different classiﬁers using the WEKA\nMachine Learning Toolkit [309], version 3.6.6. Speciﬁcally, Naïve Bayes, Support Vector Ma-\nchines, k-nearest Neighbours, a J48 Decision Tree and Random Forests (speciﬁc conﬁguration\ndetailed in the subsequent paragraphs) were applied. The number in parentheses after the feature\nset name corresponds to the number given in Table 4.2. Bold print indicates the best, italics the\nsecond best result per feature set (column-wise).\nThe following feature sets are used in the experiments to evaluate the performance of the\nfeature:\njMir: The jMir feature set [185] as described in detail and extracted as described in Chap-\nter 4.1.2 is included due to its popularity in literature. For the following features\nmean and standard deviation values were calculated: The set of Spectral Fea-\ntures (spfe) containing Spectral Centroid, Spectral Flux and Spectral Rolloff. Mel-\nFrequency Cepstral Coefﬁcients (mfcc).\nPsychoacoustic Features: Psychoacoustics feature sets as described in detail in Chapter\n4.1.2. From this rp_extract feature family, the following feature sets are extracted:\nRhythm Histograms (RH) aggregating the modulation values of the critical bands\ncomputed in a Rhythm Pattern (RP).Statistical Spectrum Descriptors (SSD) cap-\nture both timbral and rhythmic information. The features were extracted using the\nMatlab implemetation of rp_extract1 - version 0.6411.\nThe classiﬁers used in this evaluation represent a selection of machine learning algorithms fre-\nquently used in MIR research.\nK-Nearest Neighbors (KNN): the nonparametric classiﬁer has been applied to various\nmusic classiﬁcation experiments and has been added to this evaluation due to its\npopularity. It was tested with Eucledian (L2) distance with one nearest neighbor (k\n= 1).\nSupport Vector Machines (SVM): have shown remarkable performance in supervised mu-\nsic classiﬁcation tasks. SVMs were tested with Linear PolyKernels.\n1http://www.ifs.tuwien.ac.at/mir/downloads.html\n68\nJ48 The C4.5 decision tree is not as widely used as KNN or SVM, but it has the ad-\nvantage of being relatively fast to train, which might be a concern processing one\nmillion tracks. J48 was tested with a pruning conﬁdence factor of 0.25 and a mini-\nmum of two instances per leaf.\nRandom Forest (RF) The ensemble classiﬁcation algorithm is inherently slower than J48,\nbut is superior in precision. It was tested with unlimited depth of the trees, ten\ngenerated trees and the number of attributes to be used in random selection set to 0.\nNaiveBayes (NB) The probabilistic classiﬁer is efﬁcient and robust to noisy data. It is\nincluded in the evaluation due to comparability with experiments reported in litera-\nture [183].\nFor this classiﬁcation task, we have 25 categories, for which the biggest “Pop Indie” ac-\ncounts for 6.60% of the songs, which is thus the lowest baseline for our classiﬁers. It can be\nnoted from the results that the jMIR MFFC features provide the best results on the Naïve Bayes\nclassiﬁer, followed by the jMIR low-level spectral features. However, all results on this classiﬁer\nare just roughly twice as good as the baseline and low in absolute terms. Better results have been\nachieved with Support Vector Machines and k-NN classiﬁers, on both the Statistical Spectrum\nDescriptors achieve more than 27% accuracy. Also on the other two classiﬁers, Random Forests\nand Decision Trees, the SSD feature set is the best, followed by either the derivatives of the\njMIR spectral features, or the jMIR MFFC implementation.\nThis evaluation and its experimental results were published along with the introduction of the Million Song Dataset\nfeature and ground-truth enhancements (see Chapter 4.1) in Alexander Schindler, Rudolf Mayer and Andreas Rauber.\n“Facilitating comprehensive benchmarking experiments on the million song dataset” at the 13th International So-\nciety for Music Information Retrieval Conference (ISMIR 2012), pages 469-474, Porto, Portugal, October 8-12\n2012 [246].\n5.1.2 Neural Network based Classiﬁers\nMusic classiﬁcation is commonly accomplished in two major steps. First, semantically mean-\ningful audio content descriptors are extracted from the sampled audio signal. Second, a machine\nTable 5.1: Classiﬁcation results on MSD Allmusic Guide Style dataset (MASD), 66% training\nset split\nDataset NB SVM k-NN DT RF\nMFCC (4) 15.04 20.61 24.13 14.21 18.90\nSpectral (5) 14.03 17.91 13.84 12.81 17.21\nSpectral Derivates (5) 11.69 21.98 16.14 14.09 19.03\nMethodOfMoments (6) 13.26 16.42 12.77 11.57 14.80\nLPC (8) 13.41 17.92 15.94 11.97 16.19\nSSD (10) 13.76 27.41 27.07 15.06 20.06\nRH (11) 12.38 17.23 12.46 10.30 13.41\n69\nlearning algorithm is applied, which attempts to discriminate between the classes by ﬁnding\nseparating boundaries in the multidimensional feature-spaces. Especially the ﬁrst step requires\nextensive knowledge and skills in various speciﬁc research areas such as audio signal processing,\nacoustics and/or music theory. Recently many approaches to MIR problems have been inspired\nby the remarkable success of Deep Neural Networks (DNN) in the domains of computer vi-\nsion [140], where deep learning based approaches have already become the de facto standard.\nThe major advantage of DNNs are their feature learning capability, which alleviates the domain\nknowledge and time intensive task of crafting audio features by hand. Predictions are also made\ndirectly on the modeled input representations, which is commonly raw input data such as im-\nages, text or audio spectrograms. Recent accomplishments in applying Convolutional Neural\nNetworks (CNN) to audio classiﬁcation tasks have shown promising results by outperforming\nconventional approaches in different evaluation campaigns such as the Detection and Classi-\nﬁcation of Acoustic Scenes and Events (DCASE) [158] and the Music Information Retrieval\nEvaluation EXchange (MIREX) [161].\nAn often mentioned paradigm concerning neural networks is that deeper networks are better\nin modeling non-linear relationships of given tasks [282]. So far preceding MIR experiments\nand approaches reported in literature have not explicitly demonstrated the advantage of deep\nover shallow network architectures in a magnitude similar to results reported from the computer\nvision domain. This may be related to the absence of similarly large datasets as they are available\nin the visual related research areas. In this chapter shallow and deep neural network architectures\nare presented and evaluated on the Million Song Dataset.\nMethod\nThe parallel architectures of the neural networks used in the evaluation are based on the idea of\nusing a time and a frequency pipeline described in [215], which was successfully applied in two\nevaluation campaigns [158, 161]. The system is based on a parallel CNN architecture where a\nlog-amplitude scaled Melspectrogram is fed into separate CNN layers which are optimized for\nprocessing and recognizing music relations in the frequency domain and to capture temporal\nrelations (see Figure 5.1).\nThe Shallow Architecture: In our adaption of the CNN architecture described in [215] we\nuse two similar pipelines of CNN Layers with 16 ﬁlter kernels each followed by a Max\nPooling layer (see Figure 5.1). The left pipeline aims at capturing frequency relations\nusing ﬁlter kernel sizes of 10 ×23 and Max Pooling sizes of 1 ×20. The resulting 16\nvertical rectangular shaped feature map responses of shape 80×4 are intended to capture\nspectral characteristics of a segment and to reduce the temporal complexity to 4 discrete\nintervals. The right pipeline uses a ﬁlter of size 21×20 and Max Pooling sizes of 20×1.\nThis results in horizontal rectangular shaped feature maps of shape 4×80. This captures\ntemporal changes in intensity levels of four discrete spectral intervals. The 16 feature\nmaps of each pipeline are ﬂattened to a shape of 1 ×5120 and merged by concatenation\ninto the shape of 1 ×10240, which serves as input to a 200 units fully connected layer\nwith a dropout of 10%.\n70\nFigure 5.1: Shallow CNN architecture\nThe Deep Architecture: This architecture follows the same principles of the shallow approach.\nIt uses a parallel arrangement of rectangular shaped ﬁlters and Max-Pooling windows\nto capture frequency and temporal relationships at once. But, instead of using the in-\nformation of the large feature map responses, this architecture applies additional CNN\nand pooling layer pairs (see Figure 5.2). Thus, more units can be applied to train on the\nsubsequent smaller input feature maps. The ﬁrst level of the parallel layers are similar\nto the original approach. They use ﬁlter kernel sizes of 10 ×23 and 21 ×10 to capture\nfrequency and temporal relationships. To retain these characteristics the sizes of the\nconvolutional ﬁlter kernels as well as the feature maps are sub-sequentially divided in\nhalves by the second and third layers. The ﬁlter and Max Pooling sizes of the fourth\nlayer are set to have the same rectangular shapes with one part being rotated by 90 ◦.\nAs in the shallow architecture the same sizes of the ﬁnal feature maps of the parallel\nmodel paths balances their inﬂuences on the following fully connected layer with 200\nunits with a 25% dropout rate.\nTraining and Predicting Results\nIn each epoch during training the network multiple examples sampled from the segment-wise\nlog-transformed Mel-spectrogram analysis of all ﬁles in the training set are presented to both\npipelines of the neural network architecture. Each of those parallel pipelines uses the same 80×\n80 log-transformed Mel-spectrogram segments as input. These segments have been calculated\n71\nFigure 5.2: Deep CNN architecture\nfrom a fast Fourier transformed spectrogram using a window size of 1024 samples and an overlap\nof 50% from 0.93 seconds of audio transformed subsequently into Mel scale and Log scale. For\neach song of the dataset 15 segments have been randomly chosen.\nAll trainable layers used the Leaky ReLU activation function [176], which is an extension\nto the ReLU (Rectiﬁer Linear Unit) that does not completely cut off activation for negative\nvalues, but allows for negative values close to zero to pass through. It is deﬁned by adding a\ncoefﬁcient αin f(x) = αx, for x <0, while keeping f(x) = x, for x ≥ 0 as for the ReLU.\nIn our architectures, we apply Leaky ReLU activation with α = 0.3. L1 weight regularization\nwith a penalty of 0.0001 was applied to all trainable parameters. All networks were trained\ntowards categorical-crossentropy objective using the stochastic Adam optimization [134] with\nbeta1 = 0.9, beta2 = 0.999, epsilon= 1e− 08 and a learning rate of 0.00005.\nThe system was implemented in Python and using librosa [184] for audio processing and Mel-\nlog-transforms and Theano-based library Keras for Deep Learning.\n72\nEvaluation\nThe presented system analyzes and predicts multiple audio segments per input ﬁle, there are\nseveral ways to perform the ﬁnal prediction of an input instance:\nRaw Probability: The raw accuracy of predicting the segments as separated instances ignoring\ntheir ﬁle dependencies.\nMaximum Probability: The output probabilities of the Softmax layer for the corresponding\nnumber of classes of the datasets are summed up for all segments belonging to the same\ninput ﬁle. The predicted class is determined by the maximum probability among the\nclasses from the summed probabilities.\nMajority Vote: Here, the predictions are made for each segment processed from the audio ﬁle\nas input instance to the network. The class of an audio segment is determined by the\nmaximum probability as output by the Softmax layer for this segment instance. Then, a\nmajority vote is taken on all predicted classes from all segments of the same input ﬁle.\nMajority vote determines the class that occurs most often in the predictions (in a tie, it\nwill decide for the ﬁrst one to appear in the list of classes).\nStratiﬁed 4-fold cross validation was used. Multi-level stratiﬁcation was applied paying special\nattention to the multiple segments used per ﬁle. It was ensured that the ﬁles were distributed\naccording to their genre distributions and that no segments of a training ﬁle were provided in the\ncorresponding test split.\nThe performance of the shallow and deep architecture is evaluated according their accuracy in\nclassifying the correct genre labels of the MSD. The architectures are further evaluated accord-\ning their performance after a different number of training epochs. The networks are trained\nand evaluated after 100 epochs without early stopping. Preceding experiments showed that test\naccuracy could improve despite rising validation loss though on smaller sets no signiﬁcant im-\nprovement was recognizable after 200 epochs.\nData\nThe evaluation is based on theMillion Song Dataset (MSD) [19] using the CD2C genre assign-\nments [256] (see also Chapter 4.1.3) which are an adaptation of the MSD genre label assign-\nments presented in Chapter 4.1. For the experiments a sub-set of approximately 49,900 tracks is\nsub-sampled.\nResults\nThe presented results for the Million Song Dataset are part of a larger evaluation on different\ndatasets. This evaluation is presented in more detail in [244].\nThe results of the experiments provided in Table 5.2 were tested for signiﬁcant difference using\na Wilcoxon signed-rank test. None of the presented results showed a signiﬁcant difference for\np <0.05. Thus, we tested at the next higher level p <0.1. The following observations on\n73\nD Model raw max maj ep\nMSD\nshallow 58.20 (0.49) 63.89 (0.81) 63.11 (0.74) 100\ndeep 60.60 (0.28) 67.16 (0.64) 66.41 (0.52) 100\nTable 5.2: Experimental results for the evaluation datasets (D) at different number of training\nepochs (ep): Mean accuracies and standard deviations of the 4-fold cross-evaluation runs cal-\nculated using raw prediction scores (raw) and the ﬁle based maximum probability (max) and\nmajority vote approach (maj).\nthe MSD was made: A not signiﬁcant advantage of deep over shallow models was observed.\nExperiments using data augmentation and longer training were omitted due to the already large\nvariance provided by the MSD which multiplies the preceding datasets by factors from 15 to 50.\nConclusions\nThe observations of the evaluations presented in [244] showed that for smaller datasets shallow\nmodels seem to be more appropriate since deeper models showed no signiﬁcant improvement.\nDeeper models performed slightly better in the presence of larger datasets, but a clear conclusion\nthat deeper models are generally better could not be drawn. Data augmentation using time\nstretching and pitch shifting signiﬁcantly improved the performance of deep models on the small\ndatasets. For shallow models on the contrary it showed a negative effect on the small datasets.\nThus, deeper models should be considered when applying data augmentation. Comparing the\npresented results with previously reported evaluations on the same datasets [249] shows, that\nthe CNN based approaches already outperform handcrafted music features such as the Rhythm\nPatterns (RP) family [163] or the in the referred study presented Temporal Echonest Features\n[249].\nThe experimental results presented in this chapter provided initial results for large scale ex-\nperiments on the Million Song Dataset. Especially the results of Table 5.1 were represented\nthe ﬁrst results on the new introduced genre assignments presented in Chapter 4.1. High clas-\nsiﬁcation accuracies are reported using feature-set combinations which cover multiple music\ncharacteristics such as rhythm, timbre and harmony with Support Vector Machine (SVM) clas-\nsiﬁers [80]. Due to the large number of instances and the high dimensionality of music content\ndescriptors, it was not feasible to compute such state of the art feature-set/SVM combinations\nin [246].\nThis evaluation of different Deep Neural Network (DNN) architectures for music classiﬁcation was published and\npresented in Alexander Schindler, Thomas Lidy and Andreas Rauber. “Comparing shallow versus deep neural net-\nwork architectures for automatic music genre classiﬁcation” at the 9th Forum Media Technology (FMT2016), St.\nPoelten, Austria, November 23 - November 24 2016 [244].\n74\n5.2 The Echonest Featuresets\nDue to copyright restriction, the source audio ﬁles of the Million Song Datset (MSD) cannot be\ndistributed. Instead a collection of one million metadata entries and features extracted from the\nsource audio ﬁles is provided (an introduction and overview of these features was provided in\nChapter 4.1.2). Due to the restrictions concerning the access to the audio content of the MSD\n(see Chapter 4.1), this set of features represents the only general accessible basis to utilize this\ndataset. Although the two main audio feature sets are described as similar to Mel-Frequency\nCepstral Coefﬁcients (MFCC) [172] and Chroma Features (also referred to as Pitch class pro-\nﬁles), the absence of accurate documentation of the extraction algorithms makes such a state-\nment unreliable. Especially, no experiments are reported that verify that the Echo Nest features\nperform equivalent or at least similar to MFCC and Chroma features from conventional state-\nof-the-art MIR tools as Marsyas [291] or Jmir [187]. Further, several audio descriptors (e.g.\nMFCCs, Chroma, loudness information, etc.) are not provided as a single descriptive feature\nvector. Using an onset detection algorithm, the Echonest’s feature extractor returns a vector se-\nquence of variable length where each vector is aligned to a music event. To apply these features\nto standard machine learning algorithms a preprocessing step is required. The sequences need to\nbe transformed into ﬁxed length representations using a proper aggregation method. Approaches\nproposed so far include simply calculating the average over all vectors of a song [54], as well as\nusing the average and covariance of the timbre vectors for each song [19]. An explicit evaluation\nof which method provides best results has not been reported, yet.\nThis section provides a performance evaluation of the Echonest audio descriptors. Different\nfeature set combinations as well as different vector sequence aggregation methods are compared\nand recommendations towards optimal combinations are presented. The evaluations are based\non four well known and researched MIR genre classiﬁcation datasets in order to make the results\ncomparable to results reported in literature.\n5.2.1 Evaluation Method\nAt the time of the introduction of the Million Song Dataset its audio content descriptors were\nrelatively new to the community. Although the Echonest was a spin-off of the MIT Media Lab2\nlittle information could be found about these features. In the provided user manual of their\nproprietary closed source feature extractor, the two main feature sets were only described as\nMel-Spectrum Cepstral Coeﬁicients (MFCC) and Pitch class / Chroma like.\nThis section gives a description of the evaluation environment used in the experiments de-\nscribed in Section 5.2.2. First, the Echonest features are compared against the two conventional\nfeature sets provided by the Marsyas and Rhythm Patterns feature extractors. The evaluation is\nperformed on four well known datasets that have been widely used and evaluated in music genre\nclassiﬁcation tasks. The performance of the different features is measured and compared by clas-\nsiﬁcation accuracy that has been assessed applying ﬁve commonly used classiﬁers. In a second\nexperiment, different aggregation methods are evaluated according their accuracy in discrimi-\n2https://www.media.mit.edu/\n75\nnating music genres (see Section 5.2.2). The following feature sets are used in the experiments\nto evaluate the performance of features provided by the Echnoest Analyzer.\nMarsyas Features: The Marsyas feature set [291] (described in detail in Chapter 4.1.2)\nis included due to its popularity in literature. Marsyas features are extracted us-\ning a version of the Marsyas framework3 that has been compiled for the Microsoft\nWindows operating system. Using the default settings of bextract the complete\naudio ﬁle is analyzed using a window size of 512 samples without overlap, off-\nset, audio normalization, stereo information or downsampling. For the following\nfeatures mean and standard deviation values were calculated: Chroma Features\n(chro) corresponding to the 12 pitch classes C, C#, to B. The set of Spectral Fea-\ntures (spfe) containing Spectral Centroid, Spectral Flux and Spectral Rolloff.Tim-\nbral Features (timb) containing Time ZeroCrossings, Spectral Flux and Spectral\nRolloff, and Mel-Frequency Cepstral Coefﬁcients (MFCC). Mel-Frequency Cep-\nstral Coefﬁcients (mfcc).\nPsychoacoustic Features: Psychoacoustics feature sets (described in detail in Chapter\n4.1.2) deal with the relationship of physical sounds and the human brain’s inter-\npretation of them. From this rp_extract feature family, the following feature sets\nare extracted: Rhythm Patterns (RP) representing ﬂuctuations per modulation\nfrequency on 24 frequency bands according to human perception. Rhythm His-\ntograms (RH) aggregating the modulation values of the critical bands computed in\na Rhythm Pattern. Statistical Spectrum Descriptors (SSD) capture both timbral\nand rhythmic information. Temporal Statistical Spectrum Descriptor (TSSD)\nand dimension. Temporal Rhythm Histogram (TRH) describe variations over\ntime by including a temporal. The features were extracted using the Matlab im-\nplemetation of rp_extract4 - version 0.6411.\nEchonest features (described in detail in Chapter 4.1.2) of all four datasets were extracted\nusing the Echonest’s open source Python library Pyechonest5. This library provides\nmethods for accessing the Echonest API. Python code provided by the MSD Web\npage6 was used to store the retrieved results in the same HDF5 7 format which is\nalso used by the MSD.\nFor the evaluation four data sets that have been extensively used in music genre classiﬁcation\nover the past decade have been used.\nGTZAN This data set was compiled by George Tzanetakis [290] in 2000-2001 and con-\nsists of 1000 audio tracks equally distributed over the 10 music genres: blues, clas-\nsical, country, disco, hiphop, pop, jazz, metal, reggae, and rock.\n3http://marsyas.info\n4http://www.ifs.tuwien.ac.at/mir/downloads.html\n5https://github.com/echonest/pyechonest/\n6https://github.com/tb2332/MSongsDB/tree/master/PythonSrc\n7http://www.hdfgroup.org/HDF5/\n76\nDataset Genres Files Ref\nGTZAN 10 1000 [290]\nISMIR Genre 6 1458 [32]\nISMIR Rhythm 8 698 [32]\nLMD 10 3227 [267]\nTable 5.3: Datasets used in evaluations\nISMIR Genre This data set has been assembled for training and development in the IS-\nMIR 2004 Genre Classiﬁcation contest [32]. It contains 1458 full length audio\nrecordings from Magnatune.com distributed across the 6 genre classes: Classical,\nElectronic, JazzBlues, MetalPunk, RockPop, World.\nISMIR Rhythm The ISMIR Rhythm data set, also known as the Ballroom data set, was\nused in the ISMIR 2004 Rhythm classiﬁcation contest [32]. It contains 698 excerpts\nof typical ballroom and Latin American dance music, covering the genres Slow\nWaltz, Viennese Waltz, Tango, Quick Step, Rumba, Cha Cha Cha, Samba, and Jive.\nLatin Music Database (LMD) [267] contains 3227 songs, categorized into the 10 Latin\nmusic genres Axé, Bachata, Bolero, Forró, Gaúcha, Merengue, Pagode, Salsa, Ser-\ntaneja and Tango. The data was labeled by human experts with experience in teach-\ning Latin American dances.\nThe classiﬁers used in this evaluation represent a selection of machine learning algorithms fre-\nquently used in MIR research. They are the same set of classiﬁers as used in the experiments\ndescribed in Section 5.1.1. The classiﬁers were evaluated using their implementations in the\nWeka machine learning framework [92] version 3.7 with 10 runs of 10-fold cross-validation.\nK-Nearest Neighbors (KNN): Because the results of this classiﬁer rely mostly on the\nchoice of an adequate distance function it was tested with Eucledian (L2) and Man-\nhatton (L1) distance with one one nearest neighbor (k = 1).\nSupport Vector Machines (SVM): Linear PolyKernel and RBFKernel (RBF) are used in\nthis evaluation, both with standard parameters: penalty parameter set to 1, RBF\nGamma set to 0.01 and c=1.0.\nJ48 J48 was tested with a conﬁdence factor used for pruning from 0.25 and a minimum\nof two instances per leaf.\nRandom Forest (RF) It was tested with unlimited depth of the trees, ten generated trees\nand the number of attributes to be used in random selection set to 0.\nNaiveBayes (NB) The probabilistic classiﬁer is efﬁcient and robust to noisy data and has\nseveral advantages due to its simple structure.\n77\n5.2.2 Experiments and Results\nComparing Echonest features with conventional implementations\nThe features Segments Timbre and Segments Pitches provided by the Echonest’s Analyzer are\ndescribed as MFCC and Chroma ’like’. Unfortunately no further explanation is given to sub-\nstantiate this statement. The documentation [122] gives a brief overview of the characteristics\ndescribed by these feature sets, but an extensive description of the algorithms used in the im-\nplementation is missing. Compared to conventional implementations of MFCC and Chroma\nfeatures the most obvious difference is the vector length of Segments Timbre - which is sup-\nposed to be a MFCC like feature. Most of the available MFCC implementations in the domain\nof MIR are using 13 cepstral coefﬁcients as described in [172] whereas the Echonest Analyzer\nonly outputs vectors with dimensionality 12. Although the number of coefﬁcients is not strictly\ndeﬁned and the use of 12 or 13 dimensions seems to be more due to historical reasons, this\nmakes a direct comparison using audio calibration/benchmark testsets impossible. To test the\nassumption, that the Echonest features are similar to conventional implementations of MFCC\nand Chroma features, the audio descriptors are evaluated on four different datasets using a set of\ncommon classiﬁers as described in the evaluation description (see Sect. 5.2.1). Echonest Seg-\nments Timbre were extracted as described in Section 5.2.1. The beat aligned vector sequence\nwas aggregated by calculating mean and standard deviation for each dimension. The MFCC\nimplementation of the Marsyas framework was used as reference. Mean and standard deviations\nof the MFCC features were extracted using bextract.\nTable 5.4 shows the genre classiﬁcation accuracies for MFCC and Chroma features from\nthe Echonest Analyzer and Marsyas. Signiﬁcance testing with a signiﬁcance level α = 0.05 is\nused to compare the two different features. Signiﬁcant differences are highlighted in bold letters.\nAccording to these results the assumption that Segments Timbre are similar to MFCC does not\nhold. There are signiﬁcant differences on most of the cases and except for the GTZAN dataset\nthe Echonest features outperform the Marsyas MFCC implementation. Even more drastic are\nthe differences between Segments Pitches and Marsyas Chroma features except for the ISMIR\nRhythm dataset. Similar to Segments Timbre Segments Pitches perform better except for the\nGTZAN dataset.\nFeature selection and proper aggregation of beat algined vector sequences\nThe second part of the experiments conducted in this study deals with the huge amount of infor-\nmation provided by the by the MSD respectively the Echonest Analyzer. No initial evaluations\nwere provided giving reliable benchmarks on how to achieve best performing results on these\nfeatures sets.\nScope of selected Features: Due to number of features provided by the MSD only a subset\nof them was selected for the experiments. A comprehensive comparison of all possible feature\ncombinations is beyond the scope of this experiment. The focus was set on the beat aligned\nvector sequences Segments Timbre, Segments Pitches, Segments Loudness Max and Segments\nLoudness Max Time. Further Segments Start was used to calculate the length of a segment by\nsubtracting the onsets of two consecutive vectors.\n78\nTable 5.4: Comparing MFCC and Chroma implementations of the Echonest Analyzer (EN) and\nMarsyas (MAR) by their classiﬁcation accuracy on the GTZAN, ISMIR Genre (ISMIR-G), IS-\nMIR Rhythm (ISMIR-R) and Latin Music Dataset (LMD) datasets. (EN1) corresponds to mean\nand variance aggregations of Segments Timbre and (EN2) to mean and variance aggragations of\nSegments Pitches (see Page 80). Signiﬁcant differences ( α = 0.05) between EN and MAR are\nhighlighted in bold letters.\nSegments Timbre / MFCC\nGTZAN ISMIR-G ISMIR-R LMD\nDataset EN1 MAR EN1 MAR EN1 MAR EN1 MAR\nSVM Poly 61.1 69.0 75.1 62.1 63.1 57.1 78.4 60.4\nSVM RBF 35.1 39.3 46.8 44.1 30.3 31.0 41.2 38.0\nKNN K1 L2 58.1 63.4 77.0 64.2 49.2 43.3 78.7 58.4\nKNN K3 L2 57.6 61.4 77.0 63.0 51.8 46.8 79.4 56.9\nKNN K1 L1 56.6 63.0 77.9 63.0 49.4 44.0 79.1 57.9\nKNN K3 L1 56.4 62.3 76.6 61.5 50.0 47.1 79.9 57.4\nJ48 44.7 49.7 69.4 52.9 40.4 37.4 62.5 44.4\nRand-Forest 54.7 59.1 75.8 60.8 50.8 45.3 74.7 54.0\nNaiveBayes 50.5 55.9 63.2 49.6 53.3 38.3 68.4 46.7\nSegments Pitches / Chroma\nGTZAN ISMIR-G ISMIR-R LMD\nDataset EN2 MAR EN2 MAR EN2 MAR EN2 MAR\nSVM Poly 37.0 41.2 64.3 50.3 38.7 38.6 54.1 39.4\nSVM RBF 26.1 22.0 50.2 46.7 22.6 24.9 32.6 26.1\nKNN K1 L2 38.0 42.7 62.1 46.0 31.8 28.9 57.1 37.3\nKNN K3 L2 35.0 41.1 63.0 50.6 28.9 28.9 54.7 36.0\nKNN K1 L1 38.2 44.1 62.8 45.4 32.5 27.4 56.4 37.3\nKNN K3 L1 36.5 42.5 62.7 51.3 32.5 28.4 53.8 36.7\nJ48 27.8 40.1 53.7 43.8 29.0 26.9 41.5 33.6\nRand-Forest 37.0 48.5 62.1 50.5 35.2 30.6 53.3 39.1\nNaiveBayes 34.1 28.6 59.7 46.8 39.7 22.7 47.0 26.9\nAggregation of Echonest vector sequences: A further focus has been set on the feature sets\nthat are provided as beat aligned vector sequences. Such sequences represent time series of\nfeature data that can be harnessed in various MIR scenarios (e.g. audio segmentation, chord\nanalysis). Many classiﬁcation tasks in turn require a ﬁxed-length single vector representation of\nthe feature data. Consequently, the corresponding Echonest features need to be preprocessed. A\nstraight forward approach would be to simply calculate an average of all vectors resulting in a\nsingle vector, but this implies discarding valuable information. Lidy et. al. [152, 163] demon-\nstrated how to effectively harness temporal information of sequentially retrieved feature data by\ncalculating statistical measures. The temporal variants of Rhythm Patterns (RP), Rhythm His-\ntograms (RH) and Statistical Spectrum Descriptor (SSD) describe variations over time reﬂecting\nrhythmical, instrumental, etc. changes of the audio spectrum and have previously shown ex-\ncellent performance on conventional MIR classiﬁcation benchmark sets as well as non-western\nmusic datasets. For this evaluation the vector sequences provided by the Echonest Analyzer were\naggregated by calculating the statistical measures mean, median, variance, skewness, kurtosis,\n79\nmin and max.\nTemporal Echonest Features Temporal Echonest Features (TEN) follow the approach of\ntemporal features by Lidy et. al. [163], where statistical moments are calculated from Rhythm\nPattern features. To compute Temporal Rhyhtm Patterns (TRP) a track is segmented into se-\nquences of 6 seconds and features are extracted for each consecutive time frame. This approach\ncan be compared to the vector sequences retrieved by the Echonest Analyzer, except for the\nvarying time frames caused by the onset detection based segmentation. To capture temporal\nvariations of the underlying feature space, statistical moments (mean, median, variance, min,\nmax, value range, skewness, kurtosis) are calculated from each feature of the feature-sets. Dif-\nferent combinations of Echonest features and statistical measures were analyzed. The combina-\ntions were evaluated by their effectiveness in classiﬁcation experiments measured in accuracy.\nThe experiments conclude with a recommendation of a featureset-combination that achieves\nmaximum performance on most of the testsets and classiﬁers used in the evaluation.\nEN0 This represents the trivial approach of simply calculating the average of all\nSegments Timbredescriptors (12 dimensions).\nEN1 This combination is similar to EN0 including variance information of the\nbeat aligned Segments Timbrevectors already capturing timbral variances of\nthe track (24 dimensions).\nEN2 Mean and variance of Segments Pitches are calculated (24 dimensions).\nEN3 According to the year prediction benchmark task presented in [19] mean\nand the non-redundant values of the covariance matrix are calculated (90\ndimensions).\nEN4 All statistical moments (mean, median, variance, min, max, value range,\nskewness, kurtosis) for Segments Timbreare calculated (96 dimensions)\nEN5 All statistical moments of Segments Pitches and Segments Timbreare calcu-\nlated (192 dimensions).\nTemporal Echonest Features (TEN) All statistical moments of Segments Pitches,\nSegments Timbre, Segments Loudness Max, Segments Loudness Max Time\nand lengths of segments calculated from Segments Start are calculated (216\ndimension).\nResults\nTable 5.5 shows the results of the evaluations for each dataset. Echonest features are located\nto the right side of the tables. Only EN0 and EN3-TEN are displayed, because EN1 and EN2\nare already presented in Table 5.4. Bold letters mark best results of the Echonest features. If a\nclassiﬁer shows no bold entries, EN1 or EN2 provide best results for it. Conventional feature\nsets on the left side of the tables provide an extensive overview of how the Echonest features\nperform in general. Table 5.6 provides a summary of all Echonest feature combinations EN0 -\nEN5 and Temporal Echonest Features (TEN).\n80\nGood results with simple but short feature sets The trivial approach of simply averaging all\nsegments (EN0) provides expectedly the lowest precision results of the evaluated combinations.\nAs depicted in Table 5.5, the values range between MFCC and Timbre features. On the other\nhand, taking the low dimensionality of the feature space into account, this approach represents a\ngood choice for implementations focusing on runtime behavior and performance. Especially the\nnon-parametric K-Nearest-Neighbors classiﬁer provides good results. Adding additional vari-\nance information (EN1) provides enhanced classiﬁcation results on Segments Timbre features.\nSpeciﬁcally Support Vector Machines gain from the extra information provided. As pointed out\nin Table 5.6, this combination already provides top or second best results for K-Nearest Neigh-\nbors and Decision Tree classiﬁers. Again, addressing performance issues, the combinations EN0\nand EN1 with only 12 or 26 dimensions may be a good compromise between computational ef-\nﬁciency and precision of classiﬁcation results.\nChroma features are reported to show inferior music classiﬁcation performance compared\nto MFCC [68]. This behavior was reproduced. Marsyas Chroma features as well as Echonest\nSegments Pitches (EN2) provide the lowest results for their frameworks.\nBetter results with complex feature sets Providing the classiﬁer with more information ex-\npectedly results in better performance. Adding more statistical measures to simple feature sets\n(EN4) provides no signiﬁcant performance gain but increases the length of the vector by a factor\nof 4. Also combining Segments Timbrewith Segments Pitches and calculating the statistical mo-\nments (EN5) only provides slightly better results. The 192 dimensions of this combination may\nalleviate this result when performance issues are taken into consideration. Only the initially as\nbenchmark proposed approach by [19] (EN3) provides inferior results.\nRecommendation: Temporal Echonest Features Including additional information of loud-\nness distribution and the varying lengths of segments in the feature set (TEN), enhances perfor-\nmance for all classiﬁers and provides the best results of the experiments (see Table 5.6). For\nmany testset-classiﬁer combinations the Temporal Echonest Features provide best performing\nresults for all feature sets. Compared to similar performing features like TSSD - which have a\ndimension of 1176 - TENs outperform concerning precision and computational efﬁciency. Table\n5.6 summarizes the best performing Echonest feature combinations.\n81\nTable 5.5: Comparing Echonest, Marsyas and Rhythm Pattern features by their classiﬁcation\naccuracy. Best performing Echonest feature combinations are highlighted in bold letters.\nISMIR Genre Dataset\nClassiﬁers chro spfe timb mfcc rp rh trh ssd tssd EN0 EN3 EN4 EN5 TEN\nSVM Poly 50.3 54.9 67.7 62.1 75.1 64.0 66.5 78.8 80.9 67.0 67.2 78.5 80.4 81.1\nSVM RBF 46.6 44.2 50.0 44.1 69.0 55.5 64.5 64.1 72.0 44.3 49.1 64.9 69.4 70.9\nKNN K1 L2 46.0 56.3 65.8 64.2 72.9 60.7 63.3 77.8 76.6 76.8 64.0 75.5 75.9 77.8\nKNN K1 L1 45.4 56.5 65.9 63.0 71.5 60.8 63.3 78.5 77.6 77.1 60.8 77.6 78.3 81.3\nJ48 43.8 53.3 56.5 52.9 61.9 56.9 56.7 69.6 68.3 68.5 64.5 67.4 66.5 68.0\nRand-Forest 51.5 60.4 62.3 60.8 69.8 65.2 65.4 75.7 74.6 74.3 65.9 74.7 73.2 74.4\nNaiveBayes 46.8 53.2 52.3 49.6 63.5 56.7 60.2 61.0 40.2 66.1 45.5 63.8 56.0 63.3\nLatin Music Database\nSVM Poly 39.4 38.2 68.6 60.4 86.3 59.9 62.8 86.2 87.3 70.5 69.6 82.9 87.1 89.0\nSVM RBF 26.1 19.1 51.0 38.0 79.9 36.6 53.2 71.6 83.3 29.2 40.9 69.4 76.6 79.3\nKNN K1 L2 37.3 42.5 62.7 58.4 74.3 58.7 49.5 83.1 78.4 73.5 52.2 77.3 79.0 80.9\nKNN K1 L1 37.3 43.2 61.5 57.9 73.8 59.0 53.1 83.8 81.7 72.6 49.8 79.8 81.6 83.0\nJ48 33.6 38.4 48.8 44.3 57.1 43.3 43.8 64.7 64.4 58.7 53.9 60.5 61.7 64.8\nRand-Forest 39.4 46.4 58.1 53.6 58.8 50.3 47.5 76.3 73.0 69.9 54.9 74.1 73.5 75.9\nNaiveBayes 26.9 35.7 43.5 46.7 66.0 47.0 49.9 64.1 67.8 66.5 40.4 70.8 71.1 73.3\nGTZAN\nSVM Poly 41.1 43.1 75.2 67.8 64.9 45.5 38.9 73.2 66.2 56.4 53.6 63.9 65.2 66.9\nSVM RBF 22.0 27.1 52.1 37.7 56.7 31.4 39.9 53.1 63.3 36.7 22.3 46.6 56.3 56.5\nKNN K1 L2 41.9 42.1 67.8 61.8 51.5 40.2 32.7 63.7 53.4 56.3 39.9 56.8 56.1 58.2\nKNN K1 L1 43.6 43.0 68.2 61.7 53.4 39.8 35.8 64.1 60.6 55.1 36.4 56.9 56.3 58.7\nJ48 38.6 39.2 53.6 48.9 38.3 32.6 31.6 52.0 50.6 45.0 39.1 44.3 43.6 44.1\nRand-Forest 48.0 47.2 64.2 57.9 45.9 39.6 38.0 63.4 59.3 54.7 41.1 54.0 53.2 55.0\nNaiveBayes 28.1 40.0 52.2 54.9 46.3 36.2 35.6 52.4 53.0 53.1 29.5 53.6 52.5 53.3\nISMIR Rhythm\nSVM Poly 38.1 41.4 60.7 54.5 88.0 82.6 73.7 58.6 56.0 55.1 51.7 62.7 63.7 67.3\nSVM RBF 25.1 27.9 36.4 29.7 79.6 36.6 63.2 42.1 55.3 24.7 26.6 37.1 46.5 53.1\nKNN K1 L2 28.3 34.8 43.9 37.3 73.7 77.7 51.5 45.5 39.8 43.5 34.6 44.5 43.0 45.7\nKNN K1 L1 26.8 35.8 44.3 38.9 71.4 73.9 60.3 43.4 42.1 44.0 32.9 46.9 44.7 49.2\nJ48 26.9 33.7 37.6 37.1 64.3 67.6 65.9 37.6 35.8 38.5 34.0 38.5 40.5 48.0\nRand-Forest 31.0 38.1 44.4 43.8 64.9 71.6 68.2 46.6 44.1 47.5 37.1 47.9 48.8 53.5\nNaiveBayes 23.3 37.0 37.7 36.5 75.9 69.0 69.3 44.4 46.8 52.8 25.1 52.8 49.9 55.1\n82\nTable 5.6: Overview of which Echonest feature combination performs best for a certain classiﬁer\non the datasets (a) GTZAN, (b) ISMIR Genre, (c) ISMIR Rhythm and (d) LMD\nDataset EN0 EN1 EN2 EN3 EN4 EN5 TEN\nSVM Poly a,b,c,d\nSVM RBF a,b,c,d\nKNN K1 L2 c a,b,d\nKNN K3 L2 a,b,c d\nKNN K1 L1 c a,b,d\nKNN K3 L1 c a,b,d\nJ48 a b c,d\nRand-Forest a,b c,d\nNaiveBayes b a c,d\n5.2.3 Conclusion and Future Work\nThis section presented a comparison of Echonest features - as provided by the Million Song\nDataset - with feature sets from conventionally available feature extractors. Due to the absence\nof audio samples, researchers at ﬁrst hand solely rely on these Echonest features as well as the\nfeature sets described in Chapter 4.1 which were published a year later. Thus, the aim was to\nprovide empirically determined reference values for further experiments based on the Million\nSong Dataset. Six different combinations of Echonest features were used and their statistical\nmoments were calculated to capture their temporal domain. Experiments showed that Tempo-\nral Echonest Features (TEN) - a combination of MFCC and Chroma features combined with\nloudness information as well as the distribution of segment lengths - complimented by all cal-\nculated statistical moments - outperforms almost all datasets and classiﬁers - even conventional\nfeature sets, with a prediction rate of up to 89%. Although higher percentages have been re-\nported on these datasets based on other feature sets or hybrid combinations of different feature\nsets, these additional audio descriptions were or are still not available on the MSD. Additionally\nit was observed, that top results can already be obtained calculating average and variance of\nSegments Timbre features. This short representation could be considered for retrieval systems\nwith constrained computing resources.\nGenerally, it can be stated that the features provided with the Million Song Dataset perform\ncomparable to conventional MIR feature-sets and due to the ofﬁcial lack of audio ﬁles, they\nrepresent quite a good choice for experimentation. Unofﬁcially, many research groups used\nthe 7-Digital identiﬁer to download audio samples, which offers a wider range of options and\nfacilitate the extraction of more complex features. On the other hand, after The Echonest has\nbeen aquired by Spotify, their music data APIs have been incorporated into the Spotify platform.\nAlthough the option to upload local tracks to API to have them analyzed has been removed, the\nfeatures described in this section are now provided by the Spotify API. This facilitates rapid\nprototyping and show-casing on a very large corpus. The presented feature aggregation methods\n83\napply to the Spotify API too.\n5.2.4 Distribution of Data\nAll feature sets described in Section 5.2.2, including Temporal Echonest Features (TEN) and the\ndifferent aggregated feature combinations EN0 - EN5, are provided for download on the Million\nSong Dataset Benchmarking platform presented in Chapter 4.1 and in [246]:\nhttp://www.ifs.tuwien.ac.at/mir/msd/\nThe aggregated Echonest features are provided as single ﬁles containing all vectors for the\ntracks of the MSD and are stored in the WEKA Attribute-Relation File Format (ARFF) [92].\nAdditionally different benchmark partitions based on different genre label assignments are pro-\nvided for instant use and comparability.\nThese experiments and their corresponding evaluations presented in this chapter were presented in Alexander Schindler\nand Andreas Rauber. “ Capturing the temporal domain in echonest features for improved classiﬁcation effectiveness”\nat the 10th International Workshop on Adaptive Multimedia Retrieval (AMR2012), Copenhagen, Denmark, October\n24-25, 2012. The article was published in Adaptive Multimedia Retrieval: Semantics, Context, and Adaptation,\nLecture Notes in Computer Science, Volume 8382, pp 214-227, October 29, 2014 [249].\n84\n5.3 Summary\nThis chapter presented results for the datasets introduced in Chapter 4. The experiments pre-\nsented in this thesis attempt to evaluate if information extracted from music related visual media\nsuch as music videos can be harnessed to improve Music Information Retrieval (MIR) tasks\nas well as to approach new tasks. To facilitate these evaluations this chapter provides audio-\nanalytical results which serve as baselines.\n• Section 4.1 provides baseline results for large scale classiﬁcation experiments on the Mil-\nlion Song Dataset (MSD). First results are calculated using standard feature-sets with\ncommon classiﬁers reported in literature. Recent approaches based on Deep Neural Net-\nworks (DNN) are then applied to assess the maximum performance gain in the acoustic\ndomain.\n• The featureset provided for the MSD is the most complete and most accessible set avail-\nable. Unfortunately, these features have been extracted using proprietary closed source\nand insufﬁciently documented feature extractors. Section 5.2 provides comparative results\nfor these featuresets. The provided features are described to be comparable to state-of-the-\nart music content descriptors. In empirical experiments the performance of the provided\nfeatures was compared against open source implementation of the mentioned features.\nResults veriﬁed this and variances were discussed. Further, methods were evaluated and\nrecommended on how to aggregate the provided featuressets for improved classiﬁcation\nperformance.\nThe results of these preceding experiments serve as baseline for the evaluations of the experi-\nments of the consecutive chapters.\n85\n\nCHAPTER 6\nVisually improved\nArtist Identiﬁcation\n“The secret to modeling is not being perfect. What one needs is a face\nthat people can identify in a second. You have to be given what’s\nneeded by nature, and what’s needed is to bring something new. ”\n— Karl Lagerfeld, Fashion Designer\nIn this chapter the problem of music artist identiﬁcation - the task of identifying the per-\nforming musician of a given track - is addressed to demonstrate the opportunities of a Music\nVideo Information Retrieval (MVIR) approach. Artist recognition is an important task for music\nindexing, browsing and content based retrieval originating from the Music Information Retrieval\n(MIR) domain. Typically it is subdivided into the tasks artist identiﬁcation, singer recognition,\nand composer recognition. Recent achievements in music classiﬁcation and annotation including\nartist recognition are summarized in [80]. A typical content based approach to this problem is\nto extract audio features from the corresponding tracks, train a machine learning based classiﬁer\nand predict the artist name for the given track. This approach is similar to music genre classi-\nﬁcation, but whereas respectable results are reported from genre prediction, artist identiﬁcation\nis still failing to achieve comparable levels of performance. The problem is that audio features\nused in many of the evaluations reported in literature are statistical descriptions of the audio\nsignal correlating mainly to sound properties such as brightness, timbre or frequency/amplitude\nmodulations over a period of time. All these features describe sound characteristics that are\nrather related to genre properties. Although an artist is mainly dedicated to a speciﬁc genre,\nits distinct songs are not. Tracks of a record may vary in tempo, instrumentation and rhythm.\nFurther, stylistic orientations of the artists may change over time. The most intrinsic problem is\nthat audio features are low level description of the audio content. Thus, two artists with simi-\nlar sounding repertoire of songs will get confused, because the discriminating unique qualities\nof the singers voices are lost during the data reduction phase. The solution provided in [132]\nattempts to extract vocal segments of a song to identify the singer. The vocal segmentation is\n87\naccomplished through harmonic segmentation in the vocal frequency regions. The identiﬁcation\nis based on features from speech coding.\nVideo Information Retrieval (VIR) on the other side, pursues the same goals in the video\ndomain as MIR does in the music domain. A lot of effort is put into categorizing videos into\ndifferent genres. A good summary of video classiﬁcation is provided by [28]. Typically, these\napproaches draw from more than one modality - the most common among them are text-based,\naudio-based and visual-based. Different properties of videos in conjunction with cinematic prin-\nciples (e.g., light, motion, transitions from one scene to the other) are explored to estimate the\ngenre of the video. Fast motion and short shot sequences are a good indicator for music videos.\nAlthough it is easy to distinguish music videos from other video genres, approaches directly\naddressing the problem of categorizing videos by their musical genres have not been reported\nyet. Different acoustic characteristics such as localised sound energy patterns associated with\ncinematic events are nevertheless used to estimate the video genre [196]. The multimodal VIR\napproaches indicate that such a combined approach could also be suitable for the MIR domain.\nTo approach the addressed task of artist identiﬁcation, the most applicable visual computing\napproach is Face recognition, which is the task of identifying or verifying a person from still\nor video images. These tasks has received increased attention from academic and industrial\ncommunities over the past three decades due to potential applications in security systems, law\nenforcement and surveillance, and many other domains.\nIn this chapter a system is proposed which uses portrait images of artists to train a face recog-\nnition classiﬁer which is applied to frames of music videos. From the video’s corresponding\naudio track, music features are extracted and a classiﬁer is trained on artist name labels. These\ntwo classiﬁers are combined to an ensemble to produce the ﬁnal prediction on the performing\nartist. The audio classiﬁer represents a standard MIR approach to the task of artist identiﬁcation\nand its results further serve as baseline to estimate if the visually augmented ensemble classiﬁer\nshows an improved performance over the audio-only approach.\nThe remainder of this chapter is organized as follows. In the next section a brief overview\nof the state-of-the-art in the different domains and modalities is given. In Section 6.2 the lay-\nout of the classiﬁcation approach as well its evaluation is described. Section 6.3 discusses the\ndifferent datasets used in the evaluation. In Section 6.4 and 6.5 the separate classiﬁcation ap-\nproaches based on the two modalities audio and video are explained. In Section 6.6 the ensemble\nclassiﬁcation method that combines the previous two classiﬁers is outlined and the ﬁnal results\nof the evaluation are provided which are further discussed in Section 6.7. Conclusions with\nsuggestions for future work are provided in Section 6.8.\n6.1 Related Work\nEarly approaches to artist identiﬁcation are based on the Mel-Frequency Cepstral Coefﬁcients\n(MFCCs) feature set in combination with Support Vector Machines (SVM) for classiﬁcation\n[132,178]. In [133] a quantitative analysis of the album effect - effects of post-production ﬁlters\nto create a consistent sound quality across a record - on artist identiﬁcation was provided. A\nHybrid Singer Identiﬁer (HSI) is proposed by [263]. Multiple low-level features are extracted\nfrom vocal and non-vocal segments of an audio track and mixture models are used to statistically\n88\nlearn artist characteristics for classiﬁcation. Further approaches report more robust singer iden-\ntiﬁcation through identifying and extracting the singers voice after the track has been segmented\ninto instrumental and vocal sections [190, 287].\nGood summaries of state-of-the-art approaches and challenges in face recognition are pro-\nvided by [149, 209, 317]. Face detection, tracking and recognition is also used in multi-modal\nvideo retrieval [148, 270]. Faces are either used to count persons or to identify actors. Most\ncommon methods used to recognize faces are Eigenfaces [289] and Fisherfaces [12]. In [55]\nface tracking and text trajectories are used with Hidden Markov Models (HMM) for video clas-\nsiﬁcation. A face recognition approach based on real-world video data is reported in [272].\nDespite the reported achievements and promising results of systems in relatively controlled en-\nvironments, most face recognition approaches are still limited by variations in different image or\nface properties (e.g., pose, illumination, mimic, occlusions, age of the person, etc.) - properties\nthat are extensively used as artistic and stylistic features of music videos. The predominating ap-\nproaches to face recognition are Principal Component Analysis (PCA) (e.g., Eigenfaces [289])\nand Linear Discriminant Analysis (LDA) (e.g., Fisherfaces [12]). A good summary of video-\nbased face recognition is provided by [302].\n6.2 Classiﬁcation Architecture\nDue to the expenses of producing music videos the number of productions per artist or album\nis marginally low compared to the number of songs recorded. Videos are typically produced\nfor single releases of records to promote the track. As a consequence only a few videos can be\ncollected and especially for relative young artists not enough entries might be found to reliably\ntrain a classiﬁer. A common evaluation method in classiﬁcation experiments is to assess classi-\nﬁcation accuracies using k-fold cross-validation with k usually set to 10. This requires at least\n10 videos per artist, which for many artists are not available.\nThe method presented in this chapter is a three-tiered classiﬁcation approach based on two\nseparate training data-sets to take advantage of multiple sources to predict the performing artist\nof a video. Figure 6.1 depicts the architecture of the classiﬁcation system. The two modalities of\nthe systems are trained independently on their data-sets and combined by an ensemble classiﬁer\nto make a ﬁnal prediction. The audio classiﬁer is trained on all available songs of an artist, that\nhave not been released as music video. This takes advantage of the broad spectrum of the artist’s\nwork and provides a richer set of information. The face recognition system is trained on artist\nimages downloaded from Google Image Search. The separated audio data and the image data\nrepresent the training data for the presented multi-modal classiﬁcation system. Both classiﬁers\nare cross-validated on their data-sets to assess their conﬁdences.\nAn ensemble classiﬁcation approach based on bootstrapped aggregation is used. Instead of\nusing the complete training-set, the classiﬁers for each modality are trained only on sub-samples.\nThe trained audio and visual classiﬁers are applied to the music video test data-set. Using the\nbootstrapping approach this classiﬁcation step is repeated ntimes resulting in 2npredictions for\neach music video. These predictions are aggregated through a weighted majority vote, using the\npreviously evaluated conﬁdence values of the classiﬁers as weights.\n89\nSupport\nVector\nMachine\nLBPH\nFace\nReco-\ngnizer\nVideos\nAudio\nVideo\nFrames\nSplit\npredict\npredict\nArtist Images from Google Image\nSearch\nBootstrapped\nEnsemble\nClassifier\nArtistfa\nArtistfb\nArtistfa\nArtistfb\nArtistfa\nArtistfc\nWeightedfMajority\nVoting\nArtistfa\nArtistfb\nArtistfa\nArtistfb\nArtistfa\nArtistfc\nArtistfaAudio Features from MSD\nTrain Test\nFigure 6.1: The classiﬁcation architecture used for the evaluation. 1) two classiﬁers are trained\non features provided by the Million Song Dataset (MSD) and artist images downloaded from\nGoogle Images. 2) the same features are extracted from the music videos. 3) These features are\napplied to the trained classiﬁers to predict the artist. The predictions of the separate modalities\nare combined for the ﬁnal classiﬁcation results.\n6.3 Dataset\nThe evaluation data-set used for the experiments is an initial version of the Music Video Dataset\n(MVD) subset MVD-Artists presented in Chapter 4.2.6. It contains videos of 14 popular western\nmusic artists listed in Table 6.1. Popular artists were chosen to meet the requirement of collecting\nenough music videos for each musician. To demonstrate typical problems of content based artist\nidentiﬁcation the selected artists belong predominately to the two non-overlapping genres Pop\nand Rock.\n6.3.1 Training Data\nAs described in the classiﬁcation architecture in Section 6.2 training and test data do not orig-\ninate from the same data-set. The training data is drawn from two different sources - an audio\nand an image data-set.\nArtist Tracks - For the audio modality, the artist tracks provided by the Million Song Dataset\n(MSD) [19] have been used. For each artist all tracks available in the MSD have been selected\nexcluding those that are present in the music video test-set. Table 6.1 lists the number of tracks\nfor each artist. A total of 645 tracks was used with an average number of 46 tracks per artist -\nranging from a minimum of 19 to a maximum of 83 tracks.\nArtist Images - For each artist, portrait images have been downloaded. If the performing\nartist was a band, only images of the lead singer were used. Bulk download from Google Image\nSearch was used to retrieve images for each artist. In a second step the face detection algorithm\ndescribed in Section 6.5.3 was applied to each image to ﬁlter out photos that do not contain\ndetectable faces or where the resolution of the detected face was below 120x120 pixels. The\nresulting subset was manually analyzed to remove duplicates and images where the portrait\n90\nTable 6.1: Artists and training data\nArtist Name MSD Tracks Images Music Videos\nAerosmith 83 104 23\nAvril Lavigne 29 105 20\nBeyonce 32 117 19\nBon Jovi 59 54 26\nBritney Spears 57 160 24\nChristina Aguilera 46 123 14\nFoo Fighters 64 55 19\nJennifer Lopez 45 92 21\nMadonna 62 47 25\nMaroon 5 20 78 10\nNickelback 57 47 16\nRihanna 24 122 21\nShakira 48 123 20\nTaylor Swift 19 117 19\n645 1344 277\nperson does not look frontal into the camera. It was also veriﬁed that the remaining images are\nnot, in fact, screen-shots from the music videos used for the evaluation. Further images with\nlow resolutions, occlusions, exaggerated smiles or arbitrary illuminations were removed. Such\ndeviations from pass-photo like portrait images will degrade the performance of the recognition\nsystem by introducing too much variance. Further problems concerning face recognition in\nmusic video will be addressed in Section 6.5.2. The resulting set of training images contains\napproximately 50-150 portraits per artist (see Table 6.1).\n6.3.2 Test Data\nThe test-data consists of music videos that have been downloaded from Youtube1. As described\nin Chapter 4.2 a set of quality criteria and rules was considered for selecting the videos. Addi-\ntionally to the default MVD criteria, the following two rules were added for the MVD-ARTISTS\ndataset:\n– has to be an ofﬁcial music video produced by the artist\n– the lead singer has to appear in the video\n1http://www.yoututbe.com\n91\nAudio Data was retrieved directly from the video ﬁles by separating the audio stream using\nFFMPEG2. The audio was converted to mp3 format with a sample-rate of 44100 Hz and a bitrate\nof 128 kBit/s.\nVisual Data from the videos was retrieved frame by frame using the Open Computer Vision\nLibrary (OpenCV)3 [25] that was also used for the further video processing.\n6.4 Audio Content Analysis\nThe audio content analysis task is based on audio features provided by the Million Song Dataset\n(MSD) [19] (see Chapter 4.1). The MSD provides a rich set of low level features (e.g., timbre,\nchroma) and mid level features (e.g., beats per minute, music key, audio segmentation) as ex-\nplained in detail in Chapter 4.1.2. For each artist of the evaluation test-set all tracks available in\nthe MSD that do not overlap with the test-set are used. The number of tracks used for each artist\nis summarized in Table 6.1.\n6.4.1 Audio Features\nContent based artist classiﬁcation is based on the audio features provided by the Million Song\nDataset (MSD). These MSD features were extracted and provided by the Echonest 4 using the\nEchonest API5 to extract the audio features from the ﬁles which were stored equivalent to the\nMSD format. This evaluation makes use of the Temporal Echonest Features (TEN) as intro-\nduced in Chapter 5.2.2). These audio descriptors summarize an empirically selected set of MSD\nfeatures by calculating all statistical moments of the features Segments Pitches, Segments Tim-\nbre, Segments Loudness Max, Segments Loudness Max Time and lengths of segments calculated\nfrom Segments Start. The resulting feature vector has 224 dimensions. As shown in Chapter 5.2)\nthese features perform comparable to state-of-the-art music feature sets in genre classiﬁcation\ntasks on conventional data-sets.\n6.4.2 Audio Classiﬁcation Results\nAudio classiﬁcation was conducted using the Python machine learning library Scikit Learn 6.\nTraining and test data was separately normalized to have zero mean and unit variance. A Support\nVector Machine (SVM) with a linear kernel and a penalty parameter of C = 0.1 was trained\non the data from the MSD and used to classify the audio test-data set of the music videos.\nThe classiﬁcation in Table 6.2 results show, that these audio features are able to discriminate\nthe performing artists with a precision of 37% and a recall of 36%. Such a value was to be\nexpected do to the high variance in musical style of some of the artists. This can be seen in\n2http://www.ffmpeg.org/\n3http://opencv.org\n4http://echonest.com/\n5http://developer.echonest.com/\n6http://scikit-learn.org\n92\nthe high variance of the distinct values for all artists in Table 6.2 and is also illustrated by the\ncorresponding confusion-matrix of the classiﬁcation result in Figure 6.4.\n6.5 Visual Content Analysis\nThe visual content analysis part of this evaluation is focused on face recognition. A frame-by-\nframe analysis of still images, ignoring their spatio-temporal relationships, is applied. In a ﬁrst\nstep faces from the training-set - the portrait images collected from the Web - were detected and\nextracted to train a face recognizer. In a second step faces in video frames were detected and the\ntrained recognizer was applied to predict the performing artist.\n6.5.1 Face Detection\nDetecting faces in the video data is the ﬁrst step of the recognition task. Frame based face de-\ntection using boosted cascades of Haar-like features as proposed by Viola and Jones [300] and\nLienhart [166] is used. Their method uses a set of simple features based on pixel value differ-\nences between neighboring adjacent rectangles. These features are rapidly calculated from an\nintermediate representation - theintegral image- that already pre-computes neighborhood statis-\ntics for each pixel of the original image. The classiﬁer for the face detection task is constructed\nby selecting a small subset of important features using AdaBoost [77]. Finally more complex\nclassiﬁers are combined in a cascade structure. This approach for object detection minimizes\ncomputation time while achieving high prediction accuracy.\nIn an automatic post-processing step detected faces are further analyzed to eliminate false\npositives. Three additional cascaded detectors are used to locate eye-pairs, noses and mouths\nwithin the region of the detected face. If all sub-components are recognized, the detected region\nis veriﬁed as a face and used for predicting the performing artist.\n6.5.2 Obstacles in Face Detection / Recognition\nDespite the remarkable progress in face recognition over the last decades [149, 209, 302, 317]\nmost of the reported work has been evaluated in laboratory environments. The most inﬂuencing\nfactors for the accuracy of face recognition systems are illumination, occlusions and distortions -\nall properties that are very common in music videos. See Figure 6.2. The following list describes\nthe most common face detection obstacles in music videos:\n93\na)\nb)\nc)\nd)\ne)\nf)\ng)\nh)\nFigure 6.2: Examples of problematic faces - a) occlusions b) distortions c) video transitions d)\nvarying illuminations e) make up and ornaments f) hair g) beards h) stylistic elements\nOcclusions of the face are one of the biggest problems in face detection\nand recognition and unfortunately very common in music videos\n(e.g., microphone, hands touching the face, sunglasses, caps,\nhats, etc.) (see Figure 6.2a). Further excessive usage of Makeup\nand jewelry could also lead to failed detections and predictions\n(see Figure 6.2e).\nDistortions of the face due to singing, screaming, expressive mimic or fast\nmovements (see Figure 6.2b).\nPose Face recognition systems work optimal when subjects look frontal\ninto the camera, but in video or photography frontal camera shots\nare not considered to ﬂatter the photographed person. Further,\nposes and speciﬁc camera angles are used for acting purposes to\nexpress emotions such as grief, sorrow or thinking.\nIllumination changes are a stylistic tool in many music videos. A common\neffect is to use stage lighting to create the impression of live per-\nformance. This results in fast illumination changes even within a\nshort sequence of video frames (see Figure 6.2d).\nFacial Hair in the form of locks of hair hanging into the face is a similar\nproblem to occlusions (see Figure 6.2f). Another, more severe\n94\nproblem are beards of male artists. Those may change over time\nor disappear completely. Because they are not excluded during\nthe face extraction process, beards inﬂuence the training-set or\nprediction. Figure 6.2g shows the same artist with different beard\nstyles and without beard.\nSpecial video related problems:\nBlending Effects between scenes or cuts. Smooth transitions with image-cross fad-\ning effects can overlay the content of consecutive frames onto the\nface (see Figure 6.2c). In such cases the face detector recognizes\nvalid properties of a face, but the overlaid content distorts the\nface similar to make-up or illumination changes.\nOverlays of visual blocks (e.g. text, images, logos) have similar effects as\nocclusions (see Figure 6.2h).\nFurther problems arise through aging of the artist. Music videos are typically produced in combi-\nnation with new records which are released in a time-span of one to three years on average [197].\nArtists that have begun to produce videos in the early stages of the music video trend and are\nstill actively doing so have aged more than thirty years now. The effects of aging are reﬂected in\nthe face and even when surgery is used to overcome them, the effects on the face recognizer are\nthe same - the person might get misclassiﬁed.\n6.5.3 Face Recognition\nThe face recognition approach used in this evaluation is based on Local Binary Patterns (LBP)\nas proposed by Ahonen et al. [3]. This approach was chosen due to its robustness against differ-\nent facial expressions, illumination changes and aging of the subjects. LBP is a simple but very\nefﬁcient gray-scale invariant texture descriptor that combines properties of structural and statis-\ntical texture analysis. It labels each pixel of an image by thresholding their 3x3-neighborhood\n0\n1\n2\n3\n4\n5\n6\n7\n8\n0\n1\n2\n3\n4\n5\n6\n7\n8\n0\n1\n2\n3\n4\n5\n6\n7\n8\nLBP Histogram LBP Histogram Concatenated\nLBP Feature Vector\nFigure 6.3: Face recognition with Local Binary Patterns\n95\nTable 6.2: Classiﬁcation results of the separate modalities\nArtist Name Audio Video\nPrecision Recall F1-Score Precision Recall F1-Score\nAerosmith 0.33 0.52 0.39 0.14 0.33 0.20\nAvril Lavigne 0.50 0.45 0.47 0.62 0.25 0.36\nBeyonce 0.33 0.26 0.29 0.28 0.42 0.33\nBon Jovi 0.28 0.36 0.32 0.20 0.04 0.07\nBritney Spears 0.32 0.33 0.33 0.16 0.17 0.16\nChristina Aguilera 0.48 0.71 0.57 0.18 0.43 0.26\nFoo Fighters 0.41 0.47 0.44 0.00 0.00 0.00\nJennifer Lopez 0.22 0.24 0.22 0.33 0.14 0.20\nMadonna 0.27 0.28 0.24 0.50 0.12 0.19\nMaroon 5 0.20 0.10 0.13 0.12 0.80 0.20\nNickelback 0.55 0.38 0.44 1.00 0.18 0.30\nRihanna 0.29 0.19 0.23 0.40 0.10 0.15\nShakira 0.44 0.40 0.41 0.25 0.21 0.23\nTaylor Swift 0.60 0.32 0.41 0.50 0.06 0.10\navg 0.37 0.36 0.35 0.34 0.21 0.20\nwith its center value and considers the result as an 8 bit binary number. The texture of an im-\nage is described by a histogram representation of the frequency of the 256 different labels. For\nefﬁcient face recognition the image is divided into regions to retain also spatial information. As\ndepicted in Figure 6.3 the resulting histograms of the different image regions are normalized to\nhave uniform vector sum and concatenated to form the ﬁnal face descriptor. Recognition based\non these descriptors is performed using a nearest neighbor classiﬁer in the corresponding feature\nspace with Chi square as a dissimilarity measure.\n6.5.4 Video Classiﬁcation Results\nThe face recognition based visual classiﬁer was implemented using the Python programming\nlanguage bindings of the Open Computer Vision (OpenCV) library [25]. This library provides\nimplementations for the cascaded classiﬁer based on Haar-like features which is used for face\ndetection (see Section 6.5.3). In a preceding step the images were converted to gray-scale and\ntheir color histograms were normalized to obtain better results from the face detector. The\ndetected and veriﬁed faces were extracted. Contrast Limited Adaptive histogram equalization\n(CLAHE) [211] was applied to the face images to further enhance contrasts and normalize the\nimages for the identiﬁcation step. The OpenCV implementation of the LBP face recognition\napproach described in Section 6.5.3 was used to predict the corresponding artist name. The\nrecognizer was initiated with the radius of 1 and 8 neighbors used for building the Circular\nLocal Binary Pattern. A grid of 8x8 cells was applied to the image resulting in a LBP descriptor\nconsisting of 64 concatenated histograms. Each extracted and post-processed face got an artist\nname label assigned by the face recognizer. For each label the average prediction conﬁdence is\n96\nAerosmith\nAvril Lavigne\nBeyonce\nBon Jovi\nBritney Spears\nChristina Aguilera\nFoo Fighters\nJennifer Lopez\nMadonna\nMaroon 5\nNickelback\nRihanna\nShakira\nTaylor Swift\nAerosmith\nAvril Lavigne\nBeyonce\nBon Jovi\nBritney Spears\nChristina Aguilera\nFoo Fighters\nJennifer Lopez\nMadonna\nMaroon 5\nNickelback\nRihanna\nShakira\nTaylor Swift\n(a) Audio Classiﬁer\nAerosmith\nAvril Lavigne\nBeyonce\nBon Jovi\nBritney Spears\nChristina Aguilera\nFoo Fighters\nJennifer Lopez\nMadonna\nMaroon 5\nNickelback\nRihanna\nShakira\nTaylor Swift\nAerosmith\nAvril Lavigne\nBeyonce\nBon Jovi\nBritney Spears\nChristina Aguilera\nFoo Fighters\nJennifer Lopez\nMadonna\nMaroon 5\nNickelback\nRihanna\nShakira\nTaylor Swift (b) Video Classiﬁer\nFigure 6.4: Confusion matrices of the classiﬁcation results.\ncalculated. To punish supposed isolated mis-classiﬁcations and to favor frequent assignments\nthe average conﬁdence is divided by the natural logarithm of the number of how often this label\nhas been assigned. The distinct values of the artists are listed in Table 6.2. The corresponding\nconfusion-matrix of the classiﬁcation result is depicted in Figure 6.4. The results show, that the\nvisual data from the videos can be predicted with a precision of 34% and a recall of 21% where\nPrecision describes the conﬁdence a video classiﬁed as artistato be truly fromawhereas Recall\ndescribes how reliably all videos of artist aare recognized to be from a.\n6.6 Cross-Modal Results\nThe previous sections 6.4 and 6.5 describe the functionality, implementation and performance\nof the classiﬁers for the separate audio and video modalities. In this section a combined clas-\nsiﬁcation approach is presented that is capable to combine the distinct modalities and provide\nenhanced classiﬁcation accuracy.\n6.6.1 Ensemble Classiﬁcation Method\nThe presented ensemble classiﬁer is based on the Bootstrap Aggregation (Bagging) approach\nas introduced by Breiman [26]. Bagging generates multiple versions of a predictor by making\nbootstrap replicates of the training set through random sub-sampling. In our approach subset\nselection on TrainAudio and TrainVideo was applied to generate i = 10 classiﬁers for each\nmodality. Each Support Vector Machine classiﬁerCAudioi and CVideoi was trained on a selected\nsub-set TrainAudioi and the remainder of the training set TrainAudio − TrainAudioi was used\nto estimate its conﬁdence ConfAudioi .\n97\nTable 6.3: Results of the Ensemble Classiﬁcation\nArtist Name Precision Recall f1-score\nAerosmith 0.36 0.57 0.44\nAvril Lavigne 0.64 0.45 0.53\nBeyonce 0.55 0.32 0.40\nBon Jovi 0.24 0.27 0.25\nBritney Spears 0.34 0.42 0.38\nChristina Aguilera 0.33 0.50 0.40\nFoo Fighters 0.62 0.53 0.57\nJennifer Lopez 0.27 0.19 0.22\nMadonna 0.30 0.24 0.27\nMaroon 5 0.35 0.70 0.47\nNickelback 0.58 0.44 0.50\nRihanna 0.75 0.14 0.24\nShakira 0.28 0.65 0.39\nTaylor Swift 1.00 0.16 0.27\navg 0.47 0.38 0.37\nThe resulting 20 predictions were aggregating through weighted majority voting. Each clas-\nsiﬁer CAudioi and CVideoi predicts a music video mv of the test-set. Each prediction is now\nassigned a weight that is deﬁned through the conﬁdence of the used classiﬁer ConfAudioi or\nConfVideoi .\nweightmvi = ConfAudioi (6.1)\nFor each music video mvthe sum of the weights of all labels is calculated. The label with\nthe highest sum wins the vote and is the result of the ensemble classiﬁer for the music videomv.\n6.7 Results and Discussion\nThe bootstrap aggregation ensemble classiﬁcation approach as described in the previous section\nhas been implemented using the Python Scientiﬁc Machine Learning Kit (SciKit Learn). For\neach modality bootstrapped sub-sampling with 10 iterations and 10% test-set size was applied\nto the according training-set. The results are summarized in Table 6.3 and show improvement in\nprecision using the multi-modal ensemble classiﬁcation approach.\nThe presented approach demonstrates how to improve common approaches to artist identi-\nﬁcation through information extracted from music videos. The scenario set for this experiment\nwas a typical audio content based approach using the audio feature set presented in Chapter 5.2\nand its performance values as baseline. According to this the precision of the audio based classi-\nﬁer could be increased by 27% while recall values were only slightly improved by 5%. Thus, the\nensemble approach did not increase the number of correctly identiﬁed tracks, but did enhance\nthe reliability.\n98\nAs described in Section 6.2 this evaluation was intentionally based on two simple approaches.\nThe audio classiﬁer uses song-level features describing temporal statistics of timbral and chro-\nmatic music properties. Using audio segmentation to separate voiced from un-voiced sec-\ntions [190, 287] may enhance the performance of the audio classiﬁer. The visual classiﬁcation\napproach was based on frame-by-frame face recognition and prediction was made by a major-\nity vote. This approach might be improved through considering spatio-temporal relationships.\nBy applying shot-detection music videos can be segmented and faces tracked within one shot\ncould be veriﬁed and summarized more reliably. A further limiting factor of this evaluation\nwas the low resolution of the music videos which has been chosen as a compromise to collect\nenough videos. Face recognition systems highly depend on the information provided in the im-\nages. The minimum resolution of 120x120 pixel is sub-optimal and a veriﬁcation test-set using\nhigh-deﬁnition videos might provide better results.\nStill, the presented results showed that the performance of audio based artist identiﬁcation\ncan be improved through information extracted from music videos.\n6.8 Summary\nThis chapter introduced a cross-modal approach to music artist identiﬁcation. Audio content and\nvisual based classiﬁers were combined using an ensemble classiﬁer. The audio classiﬁer used\nTemporal Echonest Features which were introduced in Chapter 4.1.2 and evaluated in Chapter\n5.2 to predict artist labels. Its precision of 37% and recall of 36% was used as benchmark for the\nfurther experiments. The visual content classiﬁer used face recognition based on a Local Binary\nPatterns (LBP) predictor. The two modalities were combined through bootstrap aggregation.\nFor each modality 10 classiﬁers were created and trained on sub-samples of their according\ntraining-sets. The ﬁnal prediction for a music video was calculated on the basis of weighted\nmajority voting of the resulting 20 predictions. The proposed cross-modal approach showed that\nthe initial audio content based baseline could be increased by 27% through information extracted\nfrom the visual part of music videos.\nThe presented approach relies on a predeﬁned dataset of artist images - thus, still requiring\nmanual interaction. Future work will include automatic identiﬁcation of lead singers to train the\nface recognition algorithm directly on faces extracted from the music videos. Such an approach\nwould provide the possibility to use k-fold cross-validation on a single music video dataset.\nThis approach and its corresponding evaluation was published and presented at the 10th International Symposium\non Computer Music Multidisciplinary Research (CMMR2013) in Marseille, France on October 14-18 2013 [248].\n99\n\nCHAPTER 7\nShot Detection for Music videos\n“[Cinema] combines so many other art forms, as do theater and\nopera, but the essence of cinema is editing. It’s the combination of\nwhat can be extraordinary images, images of people during emotional\nmoments, or just images in a general sense, but put together in a kind\nof alchemy. A number of images put together a certain way become\nsomething quite above and beyond what any of them are individually”\n— Francis Ford Coppola\nOne of the most obvious features of music videos, which discriminates them from other\nvisual media, are their speciﬁc characteristics and styles of editing. Scenes in music videos are\ngenerally short, ranging in length from only a few seconds to even milliseconds. In movies\nsuch short-scene sequences are often referred to as “MTV-style editing”. The intentions behind\nthis style of editing and how it diverges from traditional movie editing have been discussed in\nChapter 3. The shot-length of music videos is a discriminative feature to distinguish them from\nother video categories such as movies, news, cartoons or sports [117]. Music Videos are more\ncomparable to commercials and movie trailers which have similar short edited sequences and\nshot transitions. A shot in a video is deﬁned as an unbroken sequence of images captured by a\nrecording operation of a camera [95]. Shots are joined during editing to compose the ﬁnal video.\nThe simplest method to accomplish this aresharp cuts where two shots are simply concatenated.\nGradual visual effects such as fades and dissolves are common ways to smooth the transition\nfrom one shot to the other.\nDuring the assembly of the Music Video Dataset (MVD) it was observed that there are char-\nacteristic styles in editing music videos for certain music genres. Also, style and complexity\nof shot transition changed over time with new technologies becoming available. Thus, it was\nintended to use state-of-the-art shot-detection approaches [44, 269, 315] to extract features such\nas Shots per Minute , Average Shot Length, Variance in Shot Length as well as further statis-\ntics. Several successful approaches reported to literature [44, 269, 315] were implemented and\napplied to the MVD. During this implementation and evaluation cycle it was observed, that\n101\nFigure 7.1: Example of Skip Frames. Replaying these frames with the 25 frames per second\nof the original video reveals, that this scene was recorded in a single camera movement, starting\nfrom the left corner of the bar and panning to the right until the focus is on two women. The de-\npicted frames show that a large segment was skipped after frame number 3 and a small segment\nafter frame number 11.\nthese approaches only apply to a limted set of shot transition styles which are commonly used in\nmovies, sport and news broadcast - such as commonly used in shot-detection evaluation datasets.\nMusic videos utilize shot transitions in a far more artistic way and use them for example to create\ntension or express emotions such as distress, horror or melancholy. The biggest challenge in de-\nveloping an appropriate shot-detection system for music videos is this vast number of transition\nstyles which is further complemented by unconventional camera work such as rapid zooming or\npanning. A major problem in this regard is the deﬁnition of a shot-boundary or transition itself.\nFor example in one of the videos of the MVD-VIS category Dance the editor used to skip three\nor four video frames of a recorded scene to create a rhythmic visual effect. The woman walking\nfrom left to right has an unnatural “jump” in her movement while the original scene remains the\nsame. While some of the applied shot-detection systems identiﬁed this as shot-boundary, it is\nstill unclear if this can be deﬁned as a such.\n7.1 Transition-Types in Music Videos\nThis section lists transition types found in music videos. The following types are most common:\nSharp Cuts: Two successive scenes are just concatenated. This is the most common transition\nin music videos.\nGradual Transitions: One scene gradually dissolves into the other. This is a common cinematic\neffect and also frequently used in music videos.\nFade-In / Fade-Out: These are gradual transition usually applied to the beginning (Fade-In) or\nend (Fade-Out) of a video. Scenes dissolve from or to a single-chromatic frame such as a blank\nblack screen.\n102\nFigure 7.2\nThese effects are common types of scene transitions which are also applied in movies or\ntelevision broadcasts. As mentioned in the introduction, shot boundary detection for these types\nhas been extensively studied. More problematic are the following types of transitions. These are\nartistic variations of common types or new transitions which still need to be deﬁned.\nSkip-Frames: A few video frames of a scene are skipped to create rhythmic effects or to in-\ncrease the pace of the visual ﬂow. Figure 7.1 depicts a 0.6 seconds long scene with skipped\nframes. The scene was originally recorded as a single camera movement. During editing a large\nand a small segment was removed. While the large edit can be recognized as a cut in the scene,\nthe smaller skipped edit is hardly recognizable in the depicted sequence of images of the ﬁgure.\nOn the other hand, when watching the video with its deﬁned frame-rate, the entire scene is per-\nceived as coherent, although both edits are clearly recognizable. Variations include sequences\nof skip-frames rhythmically aligned to a music sequence or beat.\nJumping back and forth: This effect is similar to skip-frames. The scene does not change but\nthe order of the recorded video frames is altered. This results in perceived back- and forward\njumps in the temporal progression. This effect is used intuitively and no generalizable pattern\ncould be recognized. Observed examples include repetitively jumping back to visualize musical\ncrescendos which dissolved in a new scene in synch with the music or jumping back and forth\nto express confusion.\nDistortion Overlays: Distortion overlays are visual effects applied to video frames and appear\nin various forms: heavy blurring or distortion, diffusion, rippling, white noise and many more.\nSimulation of analog TV screen errors such as Vertical Roll or horizontal or vertical synchro-\nnization failures and vertical deﬂection problems (see Figure 7.2) are a good example to show\nthe complexity of shot detection in music videos. Vertical roll is caused by a loss of vertical\nsynchronization and results in a split screen, where the upper part of the image is shifted with\nthe lower part.\nFast zoom in/out: This effect is partially a transition to a new shot. Within the same scene, the\ncamera quickly zooms in to or out of a certain subject or object. Zoom levels can vary from such\nas zooming in to the lead singers head and zooming out again the show the entire band, to only\nminor zoom levels. Again, this effect can be applied several times to the same scene.\nSplit-Screen: The video frame is subdivided into multiple, usually rectangular, regions which\ndisplay different scenes (see Figure 7.4). These scenes can change independently and frequently\nwithin such a split-screen video scene. Also the number of split-segments can change within\nsuch a scene.\n103\nFigure 7.3: Example of Blending video frames. Within 0.7 seconds (20 frames at 29 frames per\nseconds) 7 different scenes are blended.\nAbrupt tempo changes: The tempo of the recorded scene is altered (e.g. from normal to slow\nmotion or high-speed). Tempo changes can last several seconds or change back quickly (e.g.\nspeeding up for a short time or fast forward like skip-frames). Slowing down is sometimes used\nto emphasize on a musical transition (e.g. transition from bridge to chorus).\nSpotlights: Spotlights or stage lighting are common equipment used in music videos, especially\nin performance focused videos. The artists are performing on a stage or in a club like environ-\nment where bright spots and lightings are used. These spots often shine directly into the camera\nwhich results in several highly or completely illuminated video frames. Some videos put the\nartists in front of large spots pointed directly at the camera and artistically play with the shadow\nthrown. Such abrupt changes in illumination were often misclassiﬁed as shot boundaries in the\nexperiments.\nFrame-Swapping / Flickering: Transition between two scenes where several frames of one\nvideo are shown, then several of the other, then again some of the ﬁrst, and so on. This creates\na ﬂickering sensation. This effect is often used with build-ups in electronic dance music, which\nare crescendoing parts used to create tension and excitement often preceding main themes or\nchoruses.\nBlending / Fading / Dissolving: Blending, fading and dissolving are effects where one scene\ngradually transitions to another. This well researched problem [106, 195, 286, 315] is partly\ndeemed to be solved for movies, news and sport videos [269]. Again, music videos cross bor-\nders by exploiting the use of dissolves in an exaggerated artistic manner. Figure 7.3 shows 20\nconsecutive video frames of a music video. Within these 20 frames - which correspond to 0.7\nseconds at 29 frames per seconds - 7 different scenes are blended in and out. It is not clear when\none scene starts and the other ends.\n104\nFigure 7.4: Example of Split-Screen scenes. The video frame is split into various, mostly\ngeometric, regions which display different recorded scenes.\nAbrupt focal changes: The focus shifts abruptly between fore- and background within a scene.\nThus, one area becomes blurred and the other clears. Focus shifts could appear several times\nwithin the same scene (e.g. to shift between main and background vocals).\nOverlays: Overlays are a popular artistic tool that is applied in manifold ways such as ﬁre and\nﬂames blended over Heavy Metal music videos or parts of lyrics as text overlays.\nCamera Tilts: Camera tilts are fast abrupt pans by the camera. In some videos this effect is\napplied between words or lines of the lyrics. The camera tilts away from the singer and back\nwhen he or she continues to sing.\nDancing in front of Green-Box:The artist or a group of people is dancing or acting in front of a\ngreen-box. The scenery projected onto the green surface changes rapidly in music videos. While\nto the viewer it is clear that this is a connected and coherent dancing scene, the visual change\nin the background may confuse a shot-detection system. In such and similar cases it could be\nconsidered to abstract from the original shot concept and to derive additional categories such as\nBackground Shot-changes.\nFreezing on a frame: Slowing down to a complete halt and freezing a frame for a certain\namount of time. Sometimes followed by a sped up section. A variant of this effect is to freeze\nin a photography. In this case, the video frame is surrounded by a photo frame such as the\nborders of a Polaroid photo and the still image is shown for several video frames until the video\nprogresses naturally again.\nDropping to black: Illumination is dropped to zero over two or three frames. This effect\nis sometimes used in Dance or Dubstep music videos to simulate drop beats or to emphasize\ndrops. These are sudden changes in the rhythm or bass line and are usually preceded by a build-\nup section. In music videos such drop frames do not usually indicate a scene change, although\nit does occur.\n7.2 Examples\nThis section picks some examples of music video to visualize some of the introduced problems.\nTherfore a mean-color bar is generated to summarize music video content and to visualize its\nprogression over time. This is achieved by projecting the mean pixel values against the vertical\naxis of each video frame and each color dimension. This results in a vector representation of\n105\na) b) c) d) e) f )\ng) h) i) j) k)\nFigure 7.5: Mean-color bar examples - each column of an example visualization corresponds to\nthe mean color vector of a video frame projected against the vertical axis in RGB color space. a)\nvideo sequence of the sky with slowly moving clouds. b) video sequence showing trees and sky.\nc) beach, sea and sky. d) fade-in effect. e) zooming in on object. f) split screen video sequences.\ng) object or text overlays. h) camera ﬁxed on object or scene. i) moving camera focus. j) gradual\nand dissolving transitions. k) sharp cuts.\nthe mean color of a video frame in the RGB color space. The mean-color bar is generated\nby concatenating the mean color vectors of all consecutive video frames. These bars are a\nconvenient tool and provide a rough overview of the video content. There exist a few easy to\nrecognize patterns which can be directly related to the displayed content. Figure 7.5 shows\nexamples of mean-color bar generated from music videos of the MVD.\n7.2.1 Example 1 - Split Screens and Frame-Swapping / Flickering\nExample 1 is track 92 from theDance category taken from theMVD-VIS dataset. It is a standard\nelectronic dance music (EDM) track. The video is situated in a dance club. The main plot of the\nvideo is to show women dancing to the music. The discussed part of the track is visualized in\nFigure 7.6 a). The leading segment is a split-screen sequence. Figure 7.6 b) shows an example\nframe of this sequence. The screen is split horizontally and each sequence shows a different\nscene. This sequence is followed by a segment with interchanging slow and normal motion\nrecording of a dancing woman which seems to serve as a preparation for the build-up which\nstarts at about the center of the sequence. The acoustic part is a typical EDM build-up with\ndropped bass, ampliﬁed midst and a crescendoing progression towards the drop. The visual part\nmimics this progression by synchronously swapping between several scenes. An example is\ngiven in the greenish segment of Figure 7.6 a). In this segment the scenes containing Figure 7.6\n106\na)\nb) c) d)\nFigure 7.6: Example 1: a) Mean-color-bar to visualize music video activity over time. b) vertical\nsplit-screen section (ﬁrst segment in a). b) and c) in the greenish segment of a) the video swaps\nquickly between scene b) (darker columns) and scene c) (brighter columns).\nb) and c) are swapped six times over 12 consecutive frames. Based on the videos frame-rate of\n25 frames per second (fps) this corresponds to 0.48 seconds or 0.08 seconds per scene. The hazy\nascending regions between the purple and the yellow segment depict, that there is a coherent\nbackground scene, over which various different scenes are swapped. This scene is the dancing\nwoman shown in Figure 7.6 b). The build-up ends with the drop in the yellow segment - which\nare yellow ﬂares that are synthetically layed over the captured video frames. After the drop the\nscene changes to show a crowded dance-ﬂoor with numerous people dancing. The same patterns\nof sequential highly illuminated video frames seem continues, but these illuminations originate\nfrom the synchronous disco lights in the club.\nThis example was chosen because it addresses three problems deﬁned in Section 7.1 - Split-\nscreen sections, frame-swapping and spotlights.\n7.2.2 Example 2 - Multiple mini-cut scenes with static camera view\nExample 2 is track 64 from the Metal category taken from the MVD-VIS dataset. The video\nfeatures the performing band and the scenery is reduced to a red painted room with grafﬁti on the\nwall. In the discussed sequence, which is the bridge section of the song, the singer is in another\nroom with clean red painted walls. The sequence is cut together of multiple independently shot\ntakes in front of a static camera. The order of how these shots are cut together should express\nthe distress the protagonist of the lyrics is currently in. Figure 7.7 b) shows video frames of such\na sequence. The singer changes abruptly position and posture. A shot lasts between 10 to 20\n107\nvideo frames which are 0.3 - 0.6 Seconds at 30fps or approximately 3 shots per second.\n108\na)\nb)\nFigure 7.7: Example 2: Cut scene of multiple independent takes with static camera. a) Mean-\ncolor-bar to visualize music video activity over time. Shot edges are recognizable for large\npositional changes. b) Example frames of the consecutive shots which are not longer than a few\nvideo frames. Singer abruptly changes position and orientation with every sharp cut.\nThis example was chosen because it illustrates the controversy towards the current deﬁnition\nof shot boundaries. In Figure 7.7 a) large positional changes of the singer are clearly recogniz-\nable as sharp cuts. On the other hand, the static camera position creates the impression of a\ncoherent scene. Taking into consideration that this room is only shown for the bridge of the song\nand changes back with the transition to the chorus, this coherent impression increases further.\nThus, this is a good example of how cinematic techniques are used in an artistic way to express\nemotion and rhythm.\n7.2.3 Example 3 - One-shot Music Videos\nOne-shot or one-take music videos consist literally of one long take. Nevertheless, to present dif-\nferent scenes to different parts of the song, it requires good preparation and many helping hands.\nIn a usual setup different szenes are prepared along a trail on which the camera progresses.\nArtists and stage props move along with the camera and walk in and out of view. Example 3\nis track 17 from the Folk category taken from the MVD-VIS dataset. The video opens with the\ninvestigation of a crime scene where a woman has been murdered and continues to tell the story\nof the curt trial, public media coverage and perception, and ends by lynching the accused. The\nvideo uses various visual effects to simulate a one-shot music video. Figure 7.8 b) depicts such\nan effect by showing an example sequence of video frames. This sequence corresponds to the\nmentioned opening sequence. The camera follows a photographer as he approaches the crime\nscene. Then the camera zooms out of this scene. While the photographer vanishes in the dis-\ntance, an iris appears as frame around the image. The iris evolves to an eye and further to the\nface of the victim. This face turns into a frames photograph taken by the photographer of the\nprevious scene. The camera still keeps zooming out, so it can be recognized that the photograph\nis held by an attorney in a court room.\nThis short sequence features three different scenes. Transitions are created by harnessing\ndark sceneries which hide sharp transitions. The new scene emerges out of the shadow. The\nother effect zooms in onto small objects to hide the background scene. When zooming out\nagain, the object is part of a different scene. These two effects are frequently used in this video.\n109\na)\nb)\nFigure 7.8: Example 3: One-shot Music Video. a) Mean-color-bar depicting that there are no\nsharp cuts in the video. b) example video frames of the starting sequence of the music video.\nThese frames demonstrate how zooming out is used to transition between scenes.\nFigure 7.8 a) depicts that there are no sharp cuts. Further, it is hard to ﬁnd the transitions at all\nusing this visualization.\n7.2.4 Example 4 - Artistic Effects\nThis example discusses four artistic effects applied to music videos.\nFigure 7.9 a) - Folk 06: The example is taken from the opening of the video. The camera circles\naround the person while the screen is randomly illuminated by bright white ﬂashes. These ﬂashes\nare easy recognizable in the mean-color bar.\nFigure 7.9 b) - Indie 48: This video uses an effect that simulates the degradation of old celluloid\nﬁlms, as they are known from old Silence ﬁlms. This results in random ﬂickering through\nalternating illumination and saturation values of successive video frames (as can be seen in the\nmean-color bar).\nFigure 7.9 c) - Indie 95: In the chosen scene, a drummer plays on his drums. The camera is\naimed directly at a glaring headlight, which is alternately covered by the drummer’s arm when\nplaying. The pattern visualized in the mean-color bar is very similar to sharp cut sequences.\nFigure 7.9 d) - Hard Rock 1: In this video sequence the camera is mounted on a drum-stick\nwhile the drummer plays the Hi-Hat cymbals. The abrupt changes create a rhythmic visual\npattern depicted in the mean-color bar.\nThe intention behind these four examples is to illustrate the inﬂuence of different effects.\nNaive color based approaches to shot boundary might be prone to wrong detections. While\nFigure 7.9 a) and b) can be solved with minor modiﬁcations, Figure 7.9 c) requires dedicated\napproaches to distinguish this effect from real transitions.\n110\na) b) c) d)\nFigure 7.9: Example 4: Four examples of visual effects applied to music video frames. a)\nﬂashlights illuminating the entire video frame. b) silent-ﬁlm effect of degrading celluloid ﬁlm.\nc) Spotlight pointed at camera, randomly hidden by drummer. d) Camera mounted on drum-stick\nwhile playing the Hi-Hat cymbals.\n7.3 Discussion\nThe authors of summaries on shot boundary detection [44, 269, 315] list the most common shot\ntransitions as to be sharp Cut , Dissolve, Fade in/out and Wipe. Further transition types are\nlabeled as Other transition types and are stated to be difﬁcult to detect, but are rather rare. The\nexperience from assembling the Music Video Dataset (MVD) and the experiments performed to\ndetect shot boundaries showed, that Other transition types are more commonly used in music\nvideo production including effects applied during editing or recording, which are yet not clearly\ndeﬁned in the context of the shot boundary detection task. Among the identiﬁed problems and\nchallenges [315] are:\nDetection of Gradual Transitions: A comprehensive overview on the difﬁculties of detecting\ndissolving transitions is provided in [167]. Threshold-Based approaches detects transition by\ncomparing similarities between frames in feature space. To detect Fade In/out monochrome\nframe detection based on mean and standard deviation of pixel intensity values [167, 319] is\nused. Thresholds are commonly set globally [39] which is generally estimated empirically or\nadaptively [310] using a sliding window function - or a combination of both [218]. Combinations\nof Edge Detectors and Motion Features are used to train a Support Vector Machine (SVM) [318]\nwhich is applied in a sliding window to detect gradual changes. Most of these approaches are\nnot invariant towards the artistic effects described in the previous section. Especially global\nthreshold based approaches will provide inaccurate predictions on the various kinds ofOverlays\napplied to music videos. Another problem is, that many blended music video sequences do not\ndissolve in a new shot, but the faded in sequence is faded out again. Combinations with motion\n111\nand audio features are reported including thresholding with Hidden Markov Models (HMM)\n[23]. In music videos audio features are not reliable because the transitions are not always\naligned nor correlated with changes in song structures such as chord changes or progressions\nfrom verse to chorus.\nDisturbances of Abrupt Illumination Change: Most features used in shot boundary detection\nare not invariant to abrubt changes in illumination. Especiallycolor based features such as color\nhistograms or color correlograms [5] are based on luminance information of different color\nchannels. Abrubt changes such as spotlights or overlays cause discontinuities in inter-frame\ndistances which are often classiﬁed as shot boundaries. Texture based features such as Tamura\nfeatures, wavelet transform-based texture features or Gabor wavelet ﬁlters [99] are more robust\nagainst changes in illumination but are vulnerable to abrubt changes in textures such as motion\nblurring caused by fast camera panning and tilts.\nDisturbances of Large Object/Camera Movement: As mentioned in the previous section, fast\ncamera movements or large moving objects in front of the camera affect the feature response\nof most features used in shot boundary detection, resulting in erroneous predictions of shot\nboundaries. Especially, fast camera movements are very frequent in music videos. Movement\nin any way is used to create tension or to bring a person into scene by circling around them.\nGenerally it can be summarized, that most approaches presented in literature harness or rely\non a wide range of rules and deﬁnitions. These may either be based on physical conditions\nsuch as spatio-temporal relationships between consecutive frames, or on rules developed by the\nart of ﬁlm-making. For example the use of audio features [23] is based on the observation\nthat dissolving transitions are more often used with scene changes than with transitions within\nthe same scene. This includes a change of the sound scene, which is harnessed to augment\nthe detector. As extensively elaborated in Chapter 3 music videos deliberately do not stick to\nthese rules. Of course, many of the challenges listed in Section 7.1 can already be solved, but\nnot with a general approach. Most of them are exceptions to commonly known problems and\nrequire distinct detection approaches. For example, concerning the problem of rapid sequences\nof dissolving scenes as depicted in Figure 7.3, one solution could be to interpret this sequence as\na scene by itself. Again, a custom model or an exception handling to existing models has to be\nimplemented for this. Further, some listed points still require a broader discussion on whether\nthey should be considered a transition and if so, how it should be labeled.\nThis chapter discussed observations made during an effort to develop a shot boundary de-\ntector for music videos. The number of shots per seconds as well as the type of transition\nis considered to be a signiﬁcant feature for discriminating music videos by genre and mood.\nThis development stalled when it was obvious, that state-of-the-art shot boundary detection ap-\nproaches are not generally applicable to music videos - for the reasons elaborated in this Chapter.\nThese issues are not insoluble. However, many of these effects require dedicated solutions or\ndetectors to process them. Some issues require a broader discussion to deﬁne their category such\nas whether or not they are shot transitions. More problematic is, that this is only an abstract of\nexamples and that music video creators regularly develop new creative effects.\n112\n7.4 Summary\nThis Chapter discussed open issues of shot boundary detection for music videos. The number\nof shots per seconds as well as the type of transition is considered to be a signiﬁcant feature for\ndiscriminating music videos by genre or mood. Various transition types observed in the Music\nVideo Dataset (MVD) are listed. They are further discussed concerning in which way those pose\na problem for state-of-the-art shot boundary detection approaches. These issues are not insol-\nuble. However, many of these effects require dedicated solutions or detectors to process them.\nSome issues require a broader discussion to deﬁne their category such as whether or not they are\nshot transitions. More problematic is, that this is only an abstract of examples and that music\nvideo creators regularly develop new creative effects. It is yet conceivable that approaches based\non Recurrent Convolutional Neural Networks [10] are able to learn the different visual effects\nand transition types. To pursue such experiments, ground truth labels are required for the Music\nVideo Data-set or another data-set. To facilitate the creation of such annotations we have crated\nan interactive tool, which is provided as open-source software1.\nFor future work it would be required to come to mutual agreement on the labeling the var-\nious artistic effects applied in music videos and whether or not they are considered to be types\nof transitions. Based on these deﬁnitions, it would then be of interest to evaluate if on the one\nhand approaches can be found to detect these transitions, and on the other hand, the frequency\nof their application is correlated with music characteristics such as genre, style or mood.\nThis summary of open issues in shot boundary detection for music videos as presented in this chapter was published in\nAlexander Schindler and Andreas Rauber. “On the unsolved problem of Shot Boundary Detection for Music Videos”\nin the Proceedings of the 25th International Conference on MultiMedia Modeling, January 8th 2019, Thessaloniki,\nGrece [253].\n1https://blinded.for.review\n113\n\nCHAPTER 8\nThe Color of Music\nEvaluating Color and Affective\nImage Descriptors\n“’Begin at the beginning’, the King said, very gravely, ’and go on till\nyou come to the end: then stop. ’ ”\n— Lewis Carroll, Alice’s Adventures in Wonderland, 1865\nOver the past decades music videos distinctively inﬂuenced our pop-culture and became a\nsigniﬁcant part of it. Since their global inception in the early 1980s music videos emerged from a\npromotional support medium into an art form of their own. The potential advantages of harness-\ning music-related visual information are extensively discussed in Chapter 1.1. A ﬁrst example\nof this potential has been demonstrated in Chapter 6 on music video based artist identiﬁcation.\nA precision improvement of 27% over conventional audio features shows that the additional in-\nformation beneﬁts this audio-related task. In order to use the visual domain for music retrieval\ntasks, it has to be linked to the acoustic domain. Since substantial research on audio-visual cor-\nrelations in music videos is yet scarce or not available, the approach presented in this chapter is\nbased on the simpliﬁed assumption that both layers intend to express the same music character-\nistics. Those can be emotions such as happy or sad, but also themes of the song such as party\nor hanging out at the beach. The underlying hypothesis here is that there exists a correlation\nbetween the chosen visual aesthetics and the characteristics of the corresponding song.\nSo far, no basic research has been conducted to investigate this hypothesis. An audio-visual\napproach to music video segmentation [82] was reported, which addresses some of the issues\ndiscussed in Chapter 7. Approaches to affective content analysis of music videos are provided\nby [312] and [316]. Both use visual information to augment the acoustic information in order\nto improve the results of an audio-only approach in the dedicated tasks. The speciﬁcity of these\ntasks does not allow to draw general conclusions to other MIR related tasks.\n115\nThus, in this chapter a bottom-up evaluation of affective visual features is presented. More\nspeciﬁcally the color information is analyzed, if it is sufﬁcient to discriminate music by their\ngenres. This type of evaluation is chosen, because music genre is an abstraction of music char-\nacteristics and implicetly describes information about instrumentation, charachteristics of sound\nand production, style and typical rhythms, etc. The evaluation focuses on low-level color de-\nscriptors and attempts to provide fundamental insights on correlation between the audio and the\nvisual representation of music in music videos. Using color in content-based image retrieval has\nbeen extensively studied [179,212,234] and is yet described as problematic since it is highly in-\nﬂuenced by lighting conditions during image acquisition. In music videos different illumination\nsettings and colors are usually desired artistic effects (see Chapter 3.2.3).\nThis approach and its corresponding evaluation presented in this chapter was published and\npresented in Alexander Schindler and Andreas Rauber. “An audio-visual approach to music\ngenre classiﬁcation through affective color features”at the 37th European Conference on Infor-\nmation Retrieval (ECIR’15), Vienna, Austria, March 29 - April 02 2015 [251]. In the following\nsections seven feature sets are introduced that derive from psychological experiments, art-theory\nor try to model human perception. Section 8.2 lays out the evaluation. After discussing the\nresults in Section 8.3 conclusions and outlooks to future work are provided in Section 8.4.\n8.1 Acoustic and Visual Features\nThis section introduces a collection of audio and visual feature-sets that are used in the succeed-\ning evaluation. The audio features are extracted from the separated audio channel of the music\nvideos. Visual features are extracted from each frame of a video. To abstract from the varying\nlengths of the videos the extracted frame-level features are aggregated during post-processing\nby calculating the statistical measures mean, median, standard deviation, min, max skewness,\nkurtosis. As a pre-processing step the black bars at the borders of video frames, also called\nLetterboxing or Pillarboxing(see Figure 8.3a), are removed. Removing this often applied stylis-\ntic effect is a normalization step to enhance comparability of videos with and without bars (see\nFigure 8.3b).\n8.1.1 Audio Features\nThe audio features used for the experiments have been chosen due to comparability with results\npresented in Chapter 5.2.\nPsycho-accoustic Music Descriptors as proposed by [153] are based on a psycho-acoustically\nmodiﬁed Sonogram representation that reﬂects human loudness sensation (described in more\ndetail in Chapter 4.1.2). From the Rhythm Patterns feature family Statistical Spectrum Descrip-\ntors (SSD), Rhythm Patterns (RP), Rhythm Histograms (RH), Temporal Statistical Spectrum De-\nscriptor (TSSD) and Temporal Rhythm Histograms (TRH) are used in the evaluation. For the\nextraction, the Matlab-based implementation1, version 0.6411 was employed.\n1http://www.ifs.tuwien.ac.at/mir/downloads.html\n116\n0\n1\n2\n3\n4\n5\n0\n1\n2\n3\n4\n5\n6\n7\n8\n0\n1\n2\n3\n4\n5\n0\n1\n2\n3\n4\n5\n0\n1\n2\n3\n4\n5\n6\n0\n1\n2\n3\n4\n5\n0\n1\n2\n3\n4\n5\n6\n0,0\n0,5\n1,0\n1,5\n2,0\n2,5\n3,0\n3,5\n4,0\nRed\nGreen\nBlue\nHue\nSaturation\nValue\nRGB to HSV\nApply CLAHE\nOrdered Dithering\nOriginal Video Frame Image Enhancement\n{\n{\nQuantization\n{\nFeature Aggregation\n{\nSCD\nColor Histograms\nLuminance\nLFP\nFFT\n{\nConvert Color-Space\nRGB to LAB\n0\n1\n2\n3\n4\n5\n0\n1\n2\n3\n4\n5\n{\nFeature Extraction\n24 lightness bins\n8 or 216 color bins\nStatistical Moments\nFigure 8.1: Visualization of the feature extraction process described in Section 8.1.\nMel Frequency Cepstral Coefﬁcients (MFCC) are well known audio features derived from\nspeech recognition. Chroma features project the spectrum onto 12 bins representing the semi-\ntones of the musical octave. Both are added to the evaluation as an outline, because their perfor-\nmance on conventional data-sets is well known. The features are extracted using the well known\nMARSAYS toolset [292]. For the feature extraction MARSY AS version2. 0.4.5. was utilized.\nMusic genre classiﬁcation results are presented to serve as baseline performance values for\nthe evaluation of the color descriptors. Table 8.2 shows mean accuracy values of 10-fold cross-\nvalidation experiments for four different classiﬁers on all genre speciﬁc sub-sets of the MVD.\n8.1.2 Visual Features\nThe selection for this part-evaluation presents consists of well-known low-level image process-\ning features to provide a comprehensible estimation of their expressibility. Further, color based\nfeatures derived from art-theory and empirical psychological studies, used for affective image\nretrieval, are applied to study possible effects of intentional color usage in music videos.\nGlobal Color Statistics (GCS) are a set of eight features. They calculateMean Saturation and\nMean Bightness based on the Improved Hue, Luminance and Saturation (IHLS) color space [97]\nwhich has the advantages of low saturation values of achromatic pixels and independence of sat-\nuration from the brightness function. After converting the frame to the IHLS color space Mean\nSaturation and Mean Brightness values are globally calculated for a video frame. Because Hue\nin IHLS is an angular value circular statistics has to be applied [96] to assess angular mean\nHue and angular deviation of Hue. For weakly saturated colors the hue channel behaves unpre-\ndictably in the presence of color changes induced by image noise. Saturation weighted mean\nHue and deviation of Hue are more robust towards weakly saturated colors. It introduces the re-\nlationship between hue and saturation by weighting the unit hue vectors by their corresponding\nsaturation.\n2http://sourceforge.net/projects/marsyas/\n117\na) b) c) d)\nFigure 8.2: Wang Emotional Factors: a) Original Image. b) Lightness. c) Warm-Cool. d)\nLightness-Warm-Cool\nGlobal Emotion values (GEV) refer to a Pleasure-Arousal-Dominance model based on in-\nvestigated emotional reactions presented in [296]. The authors introduce a linear relationship\nbetween saturation and brightness as a model for the emotional variables. The values were\ncalculated from the luminance (B) and saturation (S) channel of the previously described inde-\npendent IHLS color space:\nPleasure = 0.69 · B+ 0.22 · S (8.1)\nArousal= −0.31 · B+ 0.60 · S (8.2)\nDominance= 0.76 · B+ 0.32 · S (8.3)\nColorfulness (CF) is one of the features used in [48] to computationally describe aesthetics in\nphotographies. The proposed method is based on a partitioned RGB palette using Earth Mover’s\nDistance (EMD) [223] to calculate a single valueds dissimilarity of a supplied image to anideal\ncolor distribution of a colorful image.\nWang Emotional Factors (WEF) Wang et al. [304] identiﬁed three factors based on emo-\ntional word correlations that are relevant for emotion semantics based image retrieval and deﬁned\nthree corresponding feature sets. Fuzzy membership functions are used to assign values of the\nperceptual psychology motivated L*C*H* color space to discrete semantic words. Feature One\nincludes lightness description of image segments ranging from very dark to very bright (Figure\n8.2b). These are combined with the classiﬁcation of hue into cold and warm colors, resulting\nin a 10 dimensional histogram. Feature Two provides a description of warm or cool regions\nwith respect to different saturations as well as a description of contrast (Figure 8.2c). Feature\nThree combines lightness contrast with an sharpness estimation (Figure 8.2d). A no-reference\nperceptual blur measure [45] was used to calculate the sharpness. The contrast description of\nthe third factor overlaps with the Itten contrasts and is omitted.\nItten’s Contrasts (IC) are a set of art-theory concepts deﬁned by Johannes Itten [116] for\ncombining colors to induce emotions. The contrasts are based on an proportional opponent\ncolor model with 180 distinct colors that are mixtures from 12 pure colors. The contrast calcu-\nlation is aligned to the method presented in [177] which uses Wang’s feature extraction [304] as\na predecessor. Instead of a waterfall segmentation a Quick Shift [297] approach was used due to\nbetter performance at a more reasonable processing time. The following sets of contrasts were\n118\nShort Name # Descriptiom\nAudio\nStatistical Spectrum De-\nscriptors (SSD)\n168 Statistical description of a psycho-accoustic transformed\naudio spectrum\nRhythm Patterns (RP) 1024 Description of spectral ﬂuctuations\nRhythm Histograms (RH) 60 Aggregated Rhythm Patterns\nTemporal SSD and RH Temporal variants of RH (TRH #420), SSD (TSSD #1176)\nMFCC 12 Mel Frequency Cepstral Coefﬁcients\nChroma 12 12 distinct semitones of the musical octave\nVisual\nGlobal Color Statistics 6 mean saturation and brightness, mean angular hue, angular\ndeviation, with/without saturation weighting\nColorfulness 1 colorfulness measure based on Earth Movers Distance\nColor Names 8 Magenta, Red,Yellow,Green,Cyan Blue, Black, White\nPleasure, Arousal, Domi-\nnance\n3 approx. emotional values based on brightness and satura-\ntion\nItten Contrasts 4 Contrast of Light and Dark, Contrast of Saturation, Con-\ntrast of Hue and Contrast of Warm and Cold\nWang Emotional Factors 18 Features for the 3 affective factors by Wang et al. [304]\nLightness Fluctuation Pat-\nterns\n80 Rhythmic ﬂuctuations in video lightness\nTable 8.1: Overview of all features. The column ’#’ indicates the dimensionality of the corre-\nsponding feature set.\ncalculated: Contrast of Light and Dark, Contrast of Saturation, Contrast of Hue and Contrast of\nWarm and Cold. The corresponding literature [304] fails to mention the descriptions of aggrega-\ntion methods used to calculate the distinct values from the fuzzy membership functions. Thus,\nthe average value was used to calculate these values.\nColor Names (CN) describe color distributions of the reduced Web-safe Elementary-color\npalette consisting of the 8 elementary colors Magenta, Red, Yellow, Green, Cyan, Blue, Black\nand White. To map a frame of a video to this palette it is converted to Hue Value Saturation\n(HSV) color-space. Contrast, brightness and color enhancement is applied through application\nof Contrast Limited Adaptive Histogram Equalization (CLAHE) [322] to the value channel V\nusing a region size of 22x22 pixels and a clip limit of 1.0. Saturation and color enhancement was\napplied similarly to the corresponding channels with slightly adapted values.Color Quantization\nto reduce the number of distinct colors to a desired palette is obtained by applyingerror diffusion\nor dithering which computes the mean square error between the original pixel value and its\nclosest match which is then propagated locally to its surrounding pixels.Ordered Dithering was\nused since it reduces the effect of contouring but stays more consistent with the original colors.\nA 32x32 Bayer patternbayer1976color matrix was used as threshold map. Figure 8.3d) shows a\nquantized image using ordered dithering compared to a naive nearest-neighbor-match approach\nin Figure 8.3c). Feature Calculation as depicted in Figure 8.1 is concluded by calculating the\n119\n0 100 200 300 400\n0\n50\n100\n150\n200\n250\n300\n350\na)       \n0 100 200 300 400\n0\n50\n100\n150\n200\nb)       \n0 100 200 300 400\n0\n50\n100\n150\n200\nc)       \nMagenta Red Yellow Green Cyan Blue Black White\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 100 200 300 400\n0\n50\n100\n150\n200\nd)       \nMagenta Red Yellow Green Cyan Blue Black White\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 100 200 300 400\n0\n50\n100\n150\n200\ne)       \nMagenta Red Yellow Green Cyan Blue Black White\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 100 200 300 400\n0\n50\n100\n150\n200\nf)       \nMagenta Red Yellow Green Cyan Blue Black White\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 8.3: Visualization of the image enahncement steps: a) original image with black hori-\nzontal bars (letterboxing), b) pre-processed image without letterboxing, c) simple nearest neigh-\nbor match color quantization, d) Ordered Dithering (OD), e) Ordered Dithering with enhanced\nbrightnes, f) Orderd Dithering with enhanced brightness and saturation.\nstatistical moments mean, median, variance, min, max, skew and kurtosis of the reduced palette.\nLightness Fluctuation Patterns (LFP) are calculated analogous to the music feature Rhythm\nPatterns (RP) [153]. In a ﬁrst step each frame of a music video is converted to the perceptually\nuniform LAB color space. This corresponds to the psychoacoustic transformations applied to\nthe audio data as a pre-processing step of RP feature calculation. For each frame a 24 bin\nhistogram of the lightness channel is calculated. Fast Fourier Transform (FFT) is applied to\nthe histogram space of all video frames. This results in a time-invariant representation of the\n24 lightness levels capturing reoccurring patterns in the video. Only amplitude modulations in\n120\nthe range from 0 to 10 Hz are used for the ﬁnal feature set, since rhythm cannot be perceived\nfrom higher modulation frequencies. Based on the observation that light effects, motions and\nshots are usually beat synchronized in music videos, LFPs can be assumed to express rhythmic\nstructures of music videos.\n8.2 Evaluation\nThe empirical evaluation is based on the Music Video Dataset (MVD) (see Chapter 4.2). Em-\npirical classiﬁcation experiments and Chi-square feature selection were used to analyze the per-\nformance of the visual and audio-visual feature-spaces. The following sub-sets are used in this\nevaluation. For a full overview of the MVD please refer to Chapter 4.2.\nMVD-VIS: The Music Video Dataset for VISual content analysis and classiﬁca-\ntion (see Chapter 4.2.2) is intended for classifying music videos by\ntheir visual properties only.\nMVD-MM: The Music Video Dataset for MultiModal content analysis and clas-\nsiﬁcation (see Chapter 4.2.3) is intended for multi-modal classiﬁca-\ntion and retrieval tasks.\nMVD-MIX: A combination of the data-sets MVD-VIS and MVD-MM (see Chap-\nter 4.2.4).\n8.3 Results and Conclusions\nTable 8.2 summarizes the results of the comparative classiﬁcation experiments. The top segment\nof the table provides audio only results which serve as baseline for evaluating the visual and\naudio-visual approaches. Using visual features only an accuracy of 50.13% could be reached for\nSupport Vector Machines (SVM) for the MVD-VIS set. Accuracies for other sets or classiﬁers\nrange from 17.89% to 39.38%. Because all classes are equal in size these results are above\na baseline of 12.5% or 6.25% respectively. Although, the performance of the visual features\nalone is not representative, the audio-visual results show interesting effects. Generally, there\nis insigniﬁcant or no improvement of the performance over the top performing audio features.\nThe results show that combining the visual features with chroma and rhythm descriptors has a\npositive effect on the accuracy while it is negative with spectral and timbral features. Applying\nranked Chi-square attribute selection (see Figure 8.4) on the visual features shows, that affective\nfeatures such as Pleasure, Dominance and Arousal as well as the frequencies of black and white\npixels have the highest values. Further, more information is provided by variance and min/max\naggregated values than by mean values. The Color Names (CN) feature-set performs best of the\npresented visual features. Especially, minimum, maximum and standard-deviation aggregations\nof the featuresblack and white are identiﬁed to have an high impact on the classiﬁcation results in\nthe combined visual feature-set (see Figure 8.4). This indicates that the frequency and intensity\nof illumination changes is a discriminative feature for music genre classiﬁcation. The Lightness\n121\nMVD-VIS MVD-MM MVD-MIX\nSVM KNN NB SVM KNN NB SVM KNN NB\nAudio\nTSSD-RP-TRH 93.79 80.85 71.46 74.76 55.00 52.20 75.91 54.16 48.32\nTSSD 86.81 72.58 62.61 69.97 53.33 53.65 66.19 47.40 44.22\nRP 87.26 69.81 64.04 60.35 42.38 41.63 63.19 43.06 41.39\nSSD 85.78 73.18 58.81 68.74 50.28 48.41 65.11 44.64 38.92\nTRH 71.04 55.83 53.86 49.50 38.28 39.66 46.61 33.02 35.70\nMFCC 62.28 48.58 46.95 42.14 29.16 34.17 37.02 26.60 27.11\nChroma 36.34 28.09 23.03 25.26 20.11 19.41 19.64 14.68 12.08\nVisual\nLFP 33.21 23.59 25.45 20.38 16.74 16.46 16.93 11.71 13.36\nCF 34.89 25.49 31.50 21.84 17.06 20.41 18.53 11.92 16.49\nIC 36.80 27.55 27.51 24.83 19.43 19.68 21.44 13.54 12.66\nGEV 39.45 29.84 34.15 20.81 17.04 18.51 20.27 14.47 17.89\nGCS 40.55 29.76 33.91 24.08 17.29 18.15 23.72 15.40 17.34\nW AF 41.01 26.43 29.86 26.01 19.08 21.38 22.86 13.90 16.60\nCN 43.68 29.04 32.23 26.74 19.13 18.77 23.48 14.76 15.99\nVisual Features combined 50.13 34.04 39.38 31.69 21.16 23.38 32.22 17.89 21.16\nAudio-Visual\nTSSD-RP-TRH 94.86 81.38 71.65 75.69 55.78 51.36 76.53 55.76 49.08\nTSSD 88.45 71.65 64.75 70.55 52.60 52.25 69.46 46.15 45.16\nRP 89.80 71.99 65.78 62.79 43.93 41.61 66.59 44.47 41.68\nSSD 85.25 62.05 57.80 65.34 42.28 44.24 65.21 36.13 38.76\nTRH 77.84 55.98 59.71 58.50 32.79 41.40 56.31 31.39 40.09\nMFCC 63.71 41.53 46.28 42.88 24.38 27.35 43.11 22.33 25.62\nChroma 55.70 39.28 43.13 35.29 24.16 25.51 35.43 20.10 24.14\nTable 8.2: Classiﬁcation results for audio, visual and audio-visual features showing accuracies for Support Vector Machines\n(SVM), K-Nearest Neighbors (KNN) and Naive Bayes (NB) classiﬁers. Bold-faced values highlight improvements of audio-visual\napproaches over audio features.\nFluctuation Patterns (LFP) are intended to capture this information, though according to the\nexperimental results, they perform not accordingly.\nFigure 8.5 provides an overview of the color distributions for the genres of the MV-VIS\ndataset according the reduced color space which is described by a small set of color names (see\nSection 8.1.2). The upper chart shows average color histograms per genre. The lower chart\noutlines the color distribution over genres per color. The characters in the lower chart represent\nthe initial letters of the color names of the upper chart in corresponding order. Although contrast\nand colors have been enhanced there is still a dominance of low saturated values which are\nmapped to the color black.\nAn assumption preceding this evaluation was, that color is a relevant feature to distinguish\ndifferent music genres. “Dark Sounds” are often referred to noisy low frequency tones while\n“light sounds” refer to clear high frequency tones. The assumption was that these linguistic\nreferences are also reﬂected in the choice for colors and illumination in music videos. This\nassumption could not be conﬁrmed analytically. Opera videos have more pixels mapped to the\ncolor black than Heavy Metal music videos. This rather results from poor lighting conditions in\nthe production process and seems not to be an intended feature of Opera videos. Reggae music\nvideos show the highest values for the color green. That’s because many of the videos in this\n122\nmean\nmedian\nstd\nmin\nmax\nskew\nkurtosis\nMV-VIS\nPleasure\nDominance\nBrightness\nArousal\nBlack\nWhite\nWang 3 Perceptive Sharpness\nSaturation Weighted Circular Std Dev\nCircular Std Dev\nSaturation\nColorfulness\nYellow\nItten Contrast Light  Dark\nGreen\nRed\nCyan\nMagenta\nItten Contrast Saturation\nItten Contrast Hue\nWang 2 Lightness Contrast\nWang 2 High Saturation Warm\nWang 1 Very Light Warm\nWang 1 Very Light Cold\nSaturation Weighted Mean Hue\nWang 2 Middle Saturation Warm\nWang 1 Very Dark Warm\nWang 1 Middle Light Warm\nWang 1 Light Warm\nItten Contrast Cold Warm\nWang 2 Low Saturation Cold\nWang 2 High Saturation Cold\nWang 1 Dark Warm\nMean Hue\nWang 1 Light Cold\nWang 2 Middle Saturation Cold\nWang 1 Very Dark Cold\nBlue\nWang 2 Low Saturation Warm\nWang 1 Middle Light Cold\nWang 1 Dark Cold\nmean\nmedian\nstd\nmin\nmax\nskew\nkurtosis\nMV-MM\nmean\nmedian\nstd\nmin\nmax\nskew\nkurtosis\nMV-MIX\nFigure 8.4: Chi Square Feature Evaluation in descending order from left to right. Dark blue areas correspond with high χ2\nvalues.\ncategory of the data set were ﬁlmed outdoors. This “outdoor” characteristic is one of the most\ndiscriminating features of Reggae music videos as will also be discussed in the next Chapter.\n8.4 Summary\nIn this chapter a comparative evaluation of audio-visual music genre classiﬁcation is presented.\nThe main focus is on the color information of music videos. A broad set of diverging visual fea-\nture extraction methods based on psychological or perceptive models has been applied to extract\ndifferent kinds of low-level semantic information. Further a descriptor to capture rhythmical\nchanges in illumination is introduced. The performance of the visual and audio-visual features\nis compared to audio-only based features in classiﬁcation experiment to predict the music genre\nof the music videos.\nAlthough the mean average accuracy for the visual feature based classiﬁcation results are\nabove the baseline of 12.8% ( MVD-VIS and MVD-MM) and 6.25% ( MVD-MIX), an accuracy\nof 50.13% cannot be considerd a well performing approach. The experiments showed, that not\nenough information can be extracted from color values alone. This leads to the conclusion, that\n123\nBollywood Country Dance Latin Metal Opera Rap Reggae\nGenres / Colors\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nB C D L M O R R\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n0.016\nmagenta\nB C D L M O R R\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\nred\nB C D L M O R R\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\nyellow\nB C D L M O R R\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\ngreen\nB C D L M O R R\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\n0.045\ncyan\nB C D L M O R R\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\nblue\nB C D L M O R R\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nblack\nB C D L M O R R\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nwhite\nFigure 8.5: Overview of the color distributions for the genres of the MV-VIS dataset. The upper\nchart shows average color histograms per genre. The lower chart outlines the color distribution\nover genres per color. The characters in the lower chart represent the initial letters of the color\nnames of the upper chart in corresponding order.\nsuch low-level visual features are not capable of describing the highly abstract concept of music\ngenre. This conclusion is supported by the small improvemnets in the combined audio-visual\nexperiments.\nFuture work on music videos will extend the semantic space to include texture, local features\nand object detection. This will be explained in detail in Chapter 9.\n124\nCHAPTER 9\nHigh-level Visual Concepts and\nVisual Stereotypes\n“Music videos are a pervasive feature of modern popular culture. They\nare everywhere; on television, in department stores and shopping\nmalls, sports arenas, dance clubs. ”\n— Steve Jones, Cohesive but not coherent: Music videos, narrative and\nculture [124], 1988\nMalcolm McLaren, the manager of the punk rock band “The Sex Pistols”, stated in 1977\n“Christ, if people bought the records for the music, this thing would have died a death long\nago”. In his provoking way he indicated that there are far more dimensions to music purchase\nbehavior than the artistic quality of an act. Punk rock was a result of social-political differences\nof the late 1970s and dressing in leather jackets, spike bands and Mohawk hairstyle was a visual\ncommitment to the views and rebellious attitudes expressed by the genre. Such visual stereo-\ntypes are adopted by and learned over generations from mass media. As a consequence stereo-\ntypes such as cowboy- shoes or hats are often visualized when referring to Country music [264].\nSuch visual associations are constantly rehashed to recognizably shape the image of a music\nact [199]. Using these visual associations one could sketch the soundtrack of the past decades\nwithout playing a single note. Flowery clothes, twinkling mirror-balls, dreadlocks, shoulder\npads, skulls and crosses, Cowboy hats, Baseball caps and lots of jewelry are visual synonyms\nfor music styles ranging from the early sixties to contemporary Hip-Hop. Visual stereotypes\nplay an essential role in our social interaction with unfamiliar others [91]. They trigger a catego-\nrization process in which we quickly form expectations on a person’s likely behavior, attitudes,\nopinions, personality, manners, etc. and thus shape our personal attitude towards that person.\nDrama theory and ﬁlm make profound usage of such concepts [74]. The vicious antagonist is\noften recognizable at a glimpse of their ﬁrst appearance by capitalizing on stereotypes such as\nragged clothes, scars and over-exaggerated armament.\n125\nAccordingly, in his inﬂuential book “Emotion and Meaning in Music” [191] Meyer addresses\n“extramusical” connotations, explicitly referring to classes of visual associations which are cog-\nnitive schema that are culturally shared by groups of individuals. Half a century later it had been\nshown that these associations became culturally independent as a consequence of global mass\nmedia exposure [264]. Artist development departments utilize visual concepts to categorize a\nnew act by an appropriate genre and to create its visual image that is used for promotion in mass\nmedia [199]. Consequently, as discussed in Chapter 3.1, easily identiﬁable genres are a desired\ngoal of music labels. Music videos not only make use of stereotypical visual themes, but also\ntake part in their development and propagation.\nIn this chapter a high-level approach to facilitate visual stereotypes based on visual concept\ndetection is introduced. This approach decomposes the semantic content of music video frames\ninto concrete concepts such as Guitars, Cars, or Tools. To provide comprehensive and plausible\nresults the experiments are evaluated on the Music Video Dataset (MVD) (see Chapter 4). Based\non this dataset a series of evaluations is provided in Section 9.4 on a selected set of audio-content\nbased music descriptors (see Section 9.2), the low-level image processing features introduced in\nChapter 8.1.2 (see Section 9.3.1) and the new introduced high-level semantic descriptors based\non visual vocabularies (see Section 9.3). Finally, conclusions and future work is presented in\nSection 9.5.\n9.1 Dataset\nTo facilitate comparable results and reproducible research on music related visual analysis of\nmusic videos the Music Video Dataset (MVD) introduced in Chapter 4 is used. The MVD\nprovides test collections with corresponding ground truth assignments within the context of well\ndeﬁned tasks. Of the four major subsets of the MVD the following three will be used for these\nevaluations. For a full overview of the MVD please refer to 4.2:\nMVD-VIS: The Music Video Dataset for VISual content analysis and classiﬁca-\ntion (see Chapter 4.2.2) is intended for classifying music videos by\ntheir visual properties only.\nMVD-MM: The Music Video Dataset for MultiModal content analysis and clas-\nsiﬁcation (see Chapter 4.2.3) is intended for multi-modal classiﬁca-\ntion and retrieval tasks.\nMVD-MIX: A combination of the data-sets MVD-VIS and MVD-MM (see Chap-\nter 4.2.4).\nMVD-THEMES: a collection of thematically tagged classes that span across\nmusical genres (see Chapter 4.2.5).\n126\n9.2 Audio Content Descriptors\nThe audio features used for the experiments have been chosen due to comparability with results\npresented in Chapter 5.2.\nPsycho-accoustic Music Descriptors as proposed by [153] are based on a psycho-acoustically\nmodiﬁed Sonogram representation that reﬂects human loudness sensation (described in more\ndetail in Chapter 4.1.2). From the Rhythm Patterns feature family Statistical Spectrum Descrip-\ntors (SSD), Rhythm Patterns (RP), Rhythm Histograms (RH), Temporal Statistical Spectrum De-\nscriptor (TSSD) and Temporal Rhythm Histograms (TRH) are used in the evaluation. For the\nextraction, the Matlab-based implementation1, version 0.6411 was employed.\nMel Frequency Cepstral Coefﬁcients (MFCC) are well known audio features derived from\nspeech recognition. Chroma features project the spectrum onto 12 bins representing the semi-\ntones of the musical octave. Both are added to the evaluation as an outline, because their perfor-\nmance on conventional data-sets is well known. The features are extracted using the well known\nMARSAYS toolset [292]. For the feature extraction MARSY AS version2. 0.4.5. was utilized.\nMusic genre classiﬁcation results are presented to serve as baseline performance values for\nthe evaluation of the color descriptors. Table 8.2 shows mean accuracy values of 10-fold cross-\nvalidation experiments for four different classiﬁers on all genre speciﬁc sub-sets of the MVD.\n9.3 Visual Object Vocabulary\nThis section presents an approach to extract high-level visual concepts from music videos. It is\nbased on the assumption that music video directors make extensive use of genre related items,\napparels or sceneries to outline the video’s reference to that speciﬁc music genre. A good ex-\nample is the cowboy hat as a reference to country music. Recently the attention of the computer\nvision research community has been attracted by the success of deep convolutional neural net-\nworks (CNN) [140]. These new approaches made remarkable improvements in visual concept\ndetection, now facilitating comprehensive image understanding. Using these visual computing\napproaches it is possible to decompose the semantic content of a music video into concrete en-\ncapsulated concepts such as guitars, vehicles, landscapes, etc. and to measure their frequency\ndistribution over video frames.\nThe ImageNet Large Scale Visual Recognition Challenge (ILSVRC)[224] is an annual bench-\nmarking campaign to steer the competition in visual concept recognition and retrieval develop-\nment. The image classiﬁcation task, as one of three tasks, focuses on the ability of algorithms\nto identify objects present in the image. Therefore an ImageNet subset of 1000 categories - also\ncalled ’synonym sets’ or ’synsets’ - was deﬁned such that there is no overlap between the synsets.\nThis was guaranteed by referring to the tree structure of the ImageNet dataset and excluding an-\ncestral synsets. These 1000 synset are used as visual object vocabulary to semantically analyze\n1http://www.ifs.tuwien.ac.at/mir/downloads.html\n2http://sourceforge.net/projects/marsyas/\n127\nmusic videos. A full description of the ILSVRC-2012 categories is provided online 3 and con-\nsists of approximately 40% of animal categories. The remaining synsets are objects of daily life\nsuch as clothing, food, means of transportation, electronic devices, landscapes as well as music\ninstruments.\n9.3.1 Visual Feature extraction\nNew machine learning frameworks provide means for sharing trained and evaluated models en-\nabling the concept of transfer learning [207] by transferring the knowledge of one task to other\ntasks with little or no prior knowledge such as labeled data. The presented approach utilizes\nthe Caffe deep learning framework [123] developed by the Berkeley Vision and Learning Cen-\nter4. This framework provides a series of convenient advantages. Besides its fast computational\ncapabilities, its openness and community engagement, one of its remarkable contributions is\na sharing platform for trained models called Model Zoo. Researchers of all communities are\nencouraged to upload their models so they can be re-used and applied to different domains.\nThis concept is also referred to as Transfer Learning [207], where learning in a new task is\nimproved through the transfer of knowledge from a related task that has already been learned.\nConsequently, by harnessing the trained models of the visual computing domain, semantic de-\nscriptions of music videos can be learned identify high level music concepts such as music genre.\nThe outlined approach is based on a pre-trained deep convolutional neural network which won\nthe ILSVRC-2012 image classiﬁcation task [140] achieving a top-5 test error rate of 15.3%.\nThe model is online available5 and consists of eight learned layers, ﬁve convolutional and three\nfully-connected.\nImageNet [49] is an image database organized according to the WordNet [193] hierarchy in\nwhich each node of the hierarchy is depicted by hundreds and thousands of images.\nThe dataset currently consists of more than 10,000,000 labeled images depicting\nmore than 10,000 synsets. The images were collected from the web and labeled by\nhuman labelers using Amazon’s Mechanical Turk crowd-sourcing tool. Figure 9.1\nillustrates example synsets with corresponding images. Each synset is a collection of\ndozens to thousands of images in different resolutions and variants. As illustrated in\nFigure 9.1 the synset Cowboy hat includes images of hats as well as people wearing\nhats.\nPlaces205 [320] is a CNN trained on 205 scene categories of Places Database (used in NIPS’14)\nwith 2.5 million images. The architecture is based on AlexNet [140].\n3http://image-net.org/challenges/LSVRC/2012/browse-synsets\n4http://bvlc.eecs.berkeley.edu/\n5https://github.com/BVLC/caffe/wiki/Model-Zoo\n128\nFigure 9.1: Example ImageNet Synsets which are also included in the ILSVRC2012 competition\ncategories.\n9.4 Evaluation\nThe need for multi-modal strategies for MIR has been expressed by [165, 295, 305] and the\nMIRES roadmap for music information research [257] identiﬁed music videos as a potential\nrich source of additional information. Yet, no relevant work has been reported on evaluating the\nperformance of visual features extracted from music videos towards describing music related\nproperties. In this section we intend to close this gap by describing a series of performance\nevaluations of the previously introduced audio and visual features performed on the MVD. These\nevaluations are centered around the assumption that information provided by the visual layer of\nmusic videos is music related and can be harnessed to describe music content.\n9.4.1 General Experimental Setup\nMusic genre classiﬁcation was chosen as a proxy problem to perform the subset related evalua-\ntion tasks. Feature and system performance is evaluated in terms of classiﬁcation accuracy.\nAudio Feature Extraction was based on audio ﬁles from the separated audio channel of the\nmusic videos. FFMPEG 6 (version 2.1) was used to extract and store the audio\nstreams in mp3-format using a samplerate of 44.100 Hz and a bit rate of 128 kbit/s.\nVisual Feature Extraction was based on frame wise processing of each video. General image\npre-processing included the removal of Letterboxing or Pillarboxing as described\nin Chapter 8.1. To extract the visual features the CNN model is applied to every\nframe of a video to retrieve the predicted probability values for the visual concepts.\nThe sum of all their values equals 1, resulting in a uniform feature vector. To com-\npare music videos of different length these vectors need to be aggregated into rep-\nresentational vector for the corresponding video. The identiﬁcation of appropriate\naggregations was part of the evaluation and will be discussed in Section 9.4.4.\nExperimental results of all sections were obtained using the Weka machine learning toolkit\n[92] (version 3.7.5) based on the following set-up: Stratiﬁed 10-fold cross-validation\n6http://www.ffmpeg.org/\n129\nwas used to evaluate the mean accuracy of ten repeated runs of the separate feature\nsets on each of the sub-datasets using the following three classiﬁers: Support Vec-\ntor Machine (SVM): linear PolyKernel and complexity parameter c=1; K-Nearest\nNeighbors (KNN): with k=1, using Euclidean Distance (L2); Naive Bayes (NB):\nsimple probabilistic classiﬁer based on applying Bayes’ theorem\n9.4.2 Performance Evaluation of the Audio Content Based Features\nThe Music Video Dataset was compiled to foster the development and evaluation of visual fea-\ntures that are able to capture the subtle relationship between the visual layer of music videos\nand the underlying music itself. In this sense the audio classiﬁcation results presented in the\nupper part of Table 9.1 (a) serve as a baseline for all consecutive evaluations to compare the per-\nformance of the visual features against the audio-only classiﬁcation results. Results for MFCC\nwere provided due to their popularity in music research domains [266]. Further Chroma features\nwere included because they provide an abstract kind of harmonic description and are frequently\nused in different tasks such as audio ﬁngerprinting [194] or synchronization of audio with mu-\nsic scores [69]. The psychoacoustic features were included due to their reported advantage in\nclassiﬁcation tasks [153].\nThe aggregation of the MVD-VIS subset aimed at clearly deﬁned and well differentiated\nclasses. Classiﬁcation accuracies provided in Table 9.1 (a) show that these requirements have\nbeen met which can be observed by the high accuracies for the combined feature-sets RP-TRH-\nTSSD (a10). The high accuracies of the distinct feature-sets RP (a4) and SSD (a3) conclude that\nthe selected classes are well differentiated by spectral and rhythmical characteristics - at least in\nterms that are captured by the corresponding feature sets. The intended overlaps of the classes\nof the MVD-MM classes are observable as well. Analysis of the confusion matrices shows\nextensive mutual confusions of the classes Hard Rock, Pop Rock and Indie. These confusions\nstretch out to Metal and Country for the combined MVD-MIX dataset. The presented results are\ncomparable to the evaluation provided by [249] where the same feature sets had been evaluated\non the four de facto MIR music classiﬁcation benchmark sets. Although (a10) provides best\nresults there is no signiﬁcant improvement (p <0.05) over (a9) which has half the dimensions of\n(a10), thus (a9) will be used for further evaluations.\n9.4.3 Performance Evaluation of the low-level color based affective features\nThis evaluation serves as a comprehensive bottom-up evaluation to investigate the performance\nof low-level image features in describing the music related semantic content of music videos.\nThe evaluation is based on the seven color and illumination based image processing feature sets\ndescribed in Chapter 8.1.2. An initial evaluation of these low-level visual features had been\nprovided in 8, but only the performance of the combined visual feature space was evaluated.\nThe performance results for their distinct color related image features are presented in Table 9.1\n(b). The results indicate that Color Names (CN), Wang Affective Factors (W AF) and Global\nColor Statistics (GCS) perform better in discriminating the different classes but are not reliable\nto describe music genres. However, it was able to conﬁrm the common stereotype of ’darkness’\n130\nTable 9.1: Classiﬁcation results for audio, visual and audio-visual features showing accuracies for Sup-\nport Vector Machines (SVM), K-Nearest Neighbors (KNN) and Naive Bayes (NB) classiﬁers. Bold-faced\nvalues highlight improvements of audio-visual approaches over audio features. Bold values in the Au-\ndio and Visual Concepts sections depict top-results for the corresponding classiﬁer. Bold values in the\nAudio-Visual section depict improvement over the corresponding audio-only results. Underlined values\nare signiﬁcant on a 0.05 level.\nMVD-VIS MVD-MM MVD-MIX\nDim SVM KNN NB SVM KNN NB SVM KNN NB\n(a) Content Based Audio Features\na1 Chroma 48 36.34 28.09 23.03 25.26 20.11 19.41 19.64 14.68 12.08\na2 MFCC 52 62.28 48.58 46.95 42.14 29.16 34.17 37.02 26.60 27.11\na3 SSD 168 85.78 73.18 58.81 68.74 50.28 48.41 65.11 44.64 38.92\na4 RP 1440 87.26 69.81 64.04 60.35 42.38 41.63 63.19 43.06 41.39\na5 TRH 420 71.04 55.83 53.86 49.50 38.28 39.66 46.61 33.02 35.70\na6 TSSD 1176 86.81 72.58 62.61 69.97 53.33 53.65 66.19 47.40 44.22\na7 a4+a6 2616 93.08 79.47 71.88 74.44 54.00 51.03 74.64 53.06 48.54\na8 a4+a3+a5 2028 92.19 75.93 67.45 71.00 50.26 44.85 72.73 49.88 43.65\na9 a4+a3 1608 92.55 77.74 67.36 71.64 52.44 44.40 74.38 51.60 43.52\na10 a4+a5+a6 3036 93.79 80.85 71.46 74.76 55.00 52.20 75.91 54.16 48.32\n(b) Low-level Color and Affect related Image Features\nvco1 LFP 60 33.21 23.59 25.45 20.38 16.74 16.46 16.93 11.71 13.36\nvco2 CF 7 34.89 25.49 31.50 21.84 17.06 20.41 18.53 11.92 16.49\nvco3 IC 28 36.80 27.55 27.51 24.83 19.43 19.68 21.44 13.54 12.66\nvco4 GEV 21 39.45 29.84 34.15 20.81 17.04 18.51 20.27 14.47 17.89\nvco5 GCS 42 40.55 29.76 33.91 24.08 17.29 18.15 23.72 15.40 17.34\nvco6 W AF 126 41.01 26.43 29.86 26.01 19.08 21.38 22.86 13.90 16.60\nvco7 CN 56 43.68 29.04 32.23 26.74 19.13 18.77 23.48 14.76 15.99\nvco8 Combi 360 50.13 34.04 39.38 31.69 21.16 23.38 32.22 17.89 21.16\n(c) High-level Visual Concepts\nvin1 MEAN 1000 66.86 42.09 53.69 51.26 31.23 37.05 46.87 23.90 33.07\nvin2 STD 1000 69.78 46.76 50.08 51.95 29.99 32.88 48.29 26.83 29.63\nvin3 MAX 1000 73.15 44.26 46.41 54.60 33.05 31.94 50.07 26.93 27.49\nvin4 vin3+vin2 2000 73.61 46.53 51.21 55.04 31.48 34.00 51.30 27.03 31.04\nvin5 vin3+vin1 2000 74.36 47.70 53.65 55.99 33.70 37.83 51.58 28.88 33.83\nvpl1 MEAN 205 57.13 37.15 43.05 42.90 25.94 30.55 38.24 19.08 25.32\nvpl2 MAX 205 58.36 42.28 45.35 38.91 25.51 31.44 36.63 21.74 27.33\nvpl3 STD 205 60.74 40.70 39.39 43.95 27.58 28.99 39.33 20.90 23.03\nvpl4 vpl1+vpl2 510 59.46 43.11 43.85 41.25 27.08 31.58 38.26 22.28 26.72\nvpl5 vpl1+vpl3 510 60.49 39.74 40.99 43.40 26.45 30.33 39.72 20.50 24.88\n(d) Visual Combinations\nvc1 vin5+vco8 2360 72.86 45.81 53.75 55.59 31.84 38.08 51.94 27.48 33.85\nvc2 vin3+vpl3 1205 72.70 44.38 47.24 54.11 32.54 31.86 50.51 27.46 27.66\nvc3 vin5+vpl3 2205 73.80 48.54 52.73 55.21 33.74 36.75 52.21 28.41 33.03\nvc4 vin5+vpl5 2510 73.95 48.35 53.14 55.28 33.74 36.71 52.48 28.59 33.24\nvc5 vc4+vco8 2870 74.25 47.93 54.43 56.05 32.71 37.61 54.18 28.28 33.79\n(e) Audio-Visual Combinations\nav1 a10+vin5 5036 96.73 81.13 65.00 81.60 55.73 49.31 86.73 59.01 47.48\nav2 a9+vin5 3608 95.63 77.05 64.16 77.83 49.54 46.58 79.44 51.31 43.71\nav3 a9+vpl5 2118 94.50 79.95 68.08 72.96 53.29 45.99 77.40 53.73 45.51\nav4 av2+vpl5 4118 95.76 75.76 61.00 77.55 50.31 44.59 80.16 52.43 41.79\nav4 a6+vin5 3176 94.65 68.61 63.64 78.49 53.01 50.41 82.62 48.94 48.53\nav5 a4+vin5 3440 91.24 68.80 63.40 71.95 43.78 44.86 74.14 45.53 42.69\nav6 a3+vin5 2168 89.85 62.11 57.89 70.13 43.16 42.93 70.30 37.98 38.88\n131\nrelated to Heavy Metal music [71]. Videos of the class Metal contain signiﬁcantly more black\npixels (independent t-test, p <0.05) than other classes of the MVD-MIX subset and the second\nsmallest brightness values (see Figure 8.5). The lowest values onbrightness and colorfulness for\nOpera (see Figure 8.5) are a strong indicator for the inferior lightning conditions at such venues\nand thus have no relation to the music itself.\n9.4.4 Performance Evaluation of the Visual Vocabulary\nThis evaluation focuses on the visual concept detection based approach presented in Section\n9.3. The rational behind this evolution is to estimate if the high-level semantic concepts ex-\ntracted from the frames of the music videos contain relevant information to discriminate the\nclasses of the MVD. Part of the evaluation is to assess appropriate methods to aggregate the\nsoftmax scaled visual object detection results for each music video frame into a single feature\nvector. The resulting vector is the representative descriptor for the corresponding music video.\nThis single vector representation is a requirement of the machine learning algorithms applied\nin the experiments and further abstracts from variations in length. Seven statistical moments\n(minimum, maximum, mean, median, standard deviation, variance, kurtosis, skewness) were\ncalculated from the prediction results for each music video. The effectiveness of these moments\nas a feature in the experimental classiﬁcation tasks was analyzed by testing all possible combi-\nnations. The best-performing results using Visual Concepts are listed in Table 9.1 (c). Results\nshow high classiﬁcation accuracies for visual vocabularies based on the ImageNet model. The\nmost relevant aggregations for this model areMAX (vin3) and STD (vin2). This can be explained\nby the type of feature response which is the probability of a certain concept of the vocabulary\nto be present in a frame. In that sense the maximum values of all object detection results of a\nmusic video (MAX) describe which visual concepts had the highest prediction values and thus\n’reliably’ appeared in at least one frame of the video. This information appeared to be most\ndiscriminative with an accuracy of 73.15% on theMVD-VIS dataset (vin3). The best performing\ncombination (vin5) reached 74.36% which is not signiﬁcantly better than ( vin3) (p <0.05). The\nvisual vocabulary results for the Imagenet model improved the previously reported accuracy of\n50.13% (vco8) using low-level visual features (see Chapter 8.3) by 24.23%. This high accuracy\nsupports the initial hypothesis (see Chapter 1.2) that music videos make use of easy identiﬁable\nvisual concepts. The low improvement of the combination with color features ( vc1 ) and (vc5)\nindicates that this information is provided by high semantic concepts (e.g. apparel, buildings,\nmusic instruments, vehicles, etc.). This will be discussed further in the analysis of visual stereo-\ntypes in Section 9.4.7. Comparing the results of the visual vocabulary based approach with\nstate-of-the-art audio features shows that the described visual approach outperforms the music\nfeatures Chroma and the de-facto standard feature MFCC.\n9.4.5 Performance Evaluation of Audio-Visual Combinations\nThe rational behind a multi-modal approach is to utilize information of different modalities to\nimprove the performance for a dedicated task. In the previous sections the two modalities audio\nand video have been evaluated separately. This evaluation attempts to answer the question, if\ntheir combinations can improve classiﬁcation accuracy, using theaudio results (see Table 9.1 (a))\n132\nTable 9.2: Classiﬁcation results for cross-genre music themes evaluation using Support Vector\nmachines and 10-fold cross-validation. Values represent accuracies for the corresponding theme.\nColumns titled with one of theMVD-{VIS,MM,MIX} subsets’ names represent results where the\ntheme had been added as additional class to the corresponding set. Columns marked with TH\nrepresent results where only the four classes of the MVD-THEMES dataset have been used in\nthe experiments. In this table the Audio-Only results serve as baseline for the other experiments.\nResults exceeding the baseline are depicted in bold letters.\nAudio-Only Visual-Only Audio-Visual\nTheme VIS MM MIX TH VIS MM MIX TH VIS MM MIX TH\nChristmas 67.6 36.7 29.5 52.9 71.7 65.5 64.0 88.9 87.5 70.8 75.0 90.4\nK-Pop 86.0 65.4 68.6 86.0 88.4 81.6 80.4 91.7 95.5 88.2 82.7 90.0\nProtest Song 50.0 21.7 7.7 47.5 23.7 33.3 16.7 75.5 44.4 57.1 30.3 77.5\nBroken Heart 75.0 28.6 28.6 54.9 51.2 21.9 16.7 70.2 61.0 31.9 25.5 68.6\nas baseline. Again different combinations of audio features and visual vocabulary aggregations\nwere evaluated in classiﬁcation experiments. To reduce the number of required experiments,\nweak performing aggregations of the predicted semantic concpets of a music video, as identiﬁed\nin the previous task, have been skipped. Only MEAN, MAX and STD aggregations over all\nvideo frames have been used. Feature-sets have been combined using an early fusion approach.\nTable 9.1 e) shows selected best-performing audio-visual combinations for the SVM classiﬁer.\nThe additional information can be harnessed well by the SVM classiﬁer where all results show\nnoticeable and some remarkable improvements over the baseline. Despite the high integrity of\nthe classes of the MVD-VIS subset, an improvement of 2.94% (av1) was accomplished over the\nbest performing audio combination ( a10). An even higher improvement was observed for the\nless differentiated set MVD-MM (+6.84%) and the bigger MVD-MIX dataset (+10.82%). The\nvisual information (vin5) showed outstanding improvements for the TSSD (av4) audio feature\nset. The highest improvement of 16.43% for ( av4) was observed for the bigger MVD-MIX\ndataset. All mentioned improvements are signiﬁcant (p <0.05).\n9.4.6 Analysis of non-audible Music Themes\nThis evaluation attempts to answer the question, if the presented multi-modal approach can im-\nprove the classiﬁcation performance of cross-genre music themes such as Christmas. The task\ncorresponding to this evaluation refers to music tagging and is aligned to the MusiClef multi-\nmodal music tagging task [204]. The evaluation was performed in two ways. In a general\nexperiment the accuracy for discriminating the classes of the MVD-THEMES dataset was eval-\nuated. Further, each theme was added separately to the datasets MVD-VIS,MM,MIX and the\naccuracy for discriminating this theme from the other classes was measured. This investigates\nhow good the audio-visual approach can discriminate the music themes from the overlapping\nmusic genres of the dataset. The audio results (see Table 9.2“Audio-Only”) again serve as base-\nline and clearly show low accuracy values for classifying non-audible concepts by audio content\n133\nTable 9.3: Salient ILSVRC Synsets descendingly ranked by their minimal difference to other\ngenres.\nCountry Dance Metal Opera Reggae\n1. cowboy hat 1. brassiere 1. spotlight 1. theater curtain 1. seashore coast\n5. drumstick 3. maillot 2. electric-guitar 3. hoopskirt 2. academic gown\n8. restaurant 4. lipstick 4. drumstick 5. stage 3. capuchin\n9. tobacco shop 9. seashore coast 6. matchstick 11. ﬂute 5. black stork\n10. pickup truck 10. bikini 7. drum 19. harmonica 7. sunglasses\n11. acoustic guitar 15. sarong 8. barn spider 21. marimba 8. orangutan\n13. violin ﬁddle 16. perfume 10. radiator 25. oboe 9. titi monkey\n16. jeep landrover 17. trunks 12. chain 26. french horn 10. lakeshore\n18. tractor trailer 18. ice lolly 14. grand piano 27. panpipe 11. cliff drop\n19. tow truck 19. pole 23. spider web 30. grand piano 17. elephant\n21. minibus 20. bubble 24. nail 31. cello 23. steel drum\n23. electric guitar 30. miniskirt 28. brassiere 48. pipe organ 24. macaw\n33. thresher 42. feather boa 37. loudspeaker 55. harp 25. coonhound\nbased features. Their classiﬁcation accuracies for all themes, especially when combined with the\nother MVD sub-sets such as the MVD-MIX dataset, are low. Except for Broken Heart almost all\nvisual vocabulary based approaches already performed better than the audio-only results such as\nChristmas combined with MVD-MIX (+34,5%). More remarkable for the same combination is\nthe observed improvement of 45.5% using the introduced audio-visual approach. Similar high\nimprovements were observed for the themes K-Pop and Protest Song. Only the theme Broken\nHeart showed no general improvements by an audio-visual approach. Only its discriminability\nwithin the MVD-Themes dataset was increased.\n9.4.7 Analysis of Visual Stereotypes\nThe ﬁnal evaluation is concerned with the analysis whether the extracted features are able to\ncapture genre or thematically related visual stereotypes. The term frequencies of the ImageNet\nSynsets for each class were calculated. For each concept of a class the largest minimal difference\nto the term frequencies of the other classes was calculated. This resulted in a list of the most\nsalient visual concepts for each genre. Table 9.3 provides an overview of these salient concepts\nfor selected genres. For the discussion of visual stereotypes Synsets such asAbaya were removed\n(see Figure 9.1). These are concepts that are not well discriminated by the applied model.\nManually inspecting series of video frames, which were misclassiﬁed asAbaya revealed that the\nmodel tends to classify dark and blurred images, or images with larger dark regions, according\nthis category. Although the frequency distribution of these “mis-classiﬁcations” has shown to be\na discriminative feature in music video classiﬁcation, it is difﬁcult to derive a semantic relation\nto music and music related visual stereotypes.\nThe discussion of visual stereotypes is started with the genre Country. For this genre the\nvisual concepts cowboy hat and utility vehicles such as the pickup truck or a tractor trailer are\nidentiﬁed to be most salient. These observations correspond to an evaluation of perceived extra-\nmusical associations with country music [264] and to aesthetic description provided in [79]. It\n134\nwas also possible to conﬁrm the mentioned ’movement towards the warm, orange tones that\nbecame the dominant ’look’ of many contemporary country videos. ’[79]. The average values\nfor red and yellow for Country in Figure 8.5 are highest and second highest for all genres.\nThe second stereotype addresses the over-sexualization of contemporary popularDance mu-\nsic [93]. Figure 9.2 b) and c) provide two examples of dance video frames. As shown in Table\n9.3, the most salient concept for the MVD-VIS category Dance is Brassiere (for training exam-\nples of the model see Figure 9.1). Also, eight further of the most salient Synsets are referred\nto low-covering clothes such as Maillot, Bikini, Trunks or Miniskirts. Closeups of body parts\nor people dancing on poles are also among the top ranking Synsets. The accumulation of most\nsalient Synsets which relate to images showing women in under- or swim-wear are a strong indi-\ncation towards the proof of the stereotype of over-sexualization in contemporary Dance music.\nFor Heavy Metal music the common stereotype of darkness was already conﬁrmed in Sec-\ntion 9.4.3. Videos of the class Metal contain signiﬁcantly more black pixels (independent t-test,\np <0.05) than other classes of the MVD-MIX subset and the second smallest brightness values.\nA further interesting result of the analysis is the salient Synset matchstick. This is assigned by\nthe applied model to video frames showing all kinds of ﬁre. Fire, on the other hand is a typi-\ncal element related to Metal [71]. Further common concepts such as spotlight, electric guitar,\ndrums and loudspeaker are performance related elements.\nThis analysis provides arguments towards answering research question RQ5 “Is it possible\nto verify applied methods within the production process?” . Generally, it was not possible to\nidentify literature explicatively stating that the identiﬁed visual stereotypes should be applied\nto music videos of the corresponding genres. As described in Chapter 3.1, however, marketing\ndepartments make use of easy identiﬁable symbols and fashion styles to visually relate new mu-\nsic acts and actors to their music genre and style. Further, there are salient visual objects which\nhave already been addressed in music psychological studies. Thus, the correlation between these\nvisual concepts and their associated music genre has been observed and documented. The quan-\ntitative analysis provided in this section veriﬁes these observed associations and thus concludes\nthat there are common visual patterns that are deliberately applied to music videos, at least for\nthe analyzed genres Metal, Dance and Country.\n9.4.8 General Discussions\nThe evaluation and different analyses presented in this chapter indicate that music related visual\ninformation can be harnessed on the level of visual concepts. The discussion in Section 9.4.4\nshowed that frequencies and intensities of detected visual concepts differ between music genres\nas far as that they can be used to discriminate them by an accuracy of up to 75.36% for theMVD-\nVIS dataset and that this performance can be further increased though audio-visual combinations\n(as discussed in Section 9.4.5). In Section 9.4.7 it was shown that some of the most salient visual\nconcepts are associated with observed music related visual stereotypes, as reported by music\npsychology literature. Figure 9.2 illustrates the top-seven synsets of ﬁve example music video\nframes ranked by their estimated probability.\nDespite the proof of certain music related visual stereotypes, their occurrence in music video\ngenres does not denote the genre of the music video. The discriminating information lies in\n135\nFigure 9.2: Example frames of music videos and their top-ranked synsets ordered by descending\nprobability. Frames taken from videos of the genres a) Metal, b,c) Dance, d) Country and e)\nHard-Rock\nthe different frequencies of all of the detected concepts such as the concept Abaya (see Figure\n9.1). For this concept the model was trained on images showing women fully veiled in black\nfabric. Part of the images showed these women as full persons in a natural scenery, others\nshowed portraits or close-ups, which resulted in images consisting of mostly black pixels. In\nthis evaluation this lead to the issue that the model labeled many low saturated and/or blurred\nvideo frames with Abaya. Although these are clearly mis-classiﬁcations, the concept Abaya\nis identiﬁed as a highly signiﬁcant feature. In its right designation - “low saturated or blurred\nframes, as they often result from fast camera motion” - this concept corresponds to video features\ndescribed in Chapter 3.2 and Chapter 7 such as fast panning, distortion overlays or fade in/outs.\nThe salient Synsets for Reggae music exhibits many animal related concepts (see Table 9.3).\nThis is also an artifact of the applied model. Reggae videos are often shot at seashores or in\ntropical landscapes. Because the set of visual concepts the model was trained to identify, does\nnot contain concepts for landscapes, it assigns the labels for the corresponding visual content to\nthose categories which show similar content in the training images. For the landscapes shown in\nReggae music videos, the model refers to the some of the 400 animal categories and especially to\nthose which show animals in the corresponding environment such as forests. Figure 9.1 shows\nsuch a Synset example - images ofCapuchin monkeys sitting on branches in trees. TheCapuchin\nis also one of the most salient concepts detected in Reggae music videos.\n9.5 Summary\nThis chapter introduced semantic analysis of music video content. The focus of the presented\nresearch was on harnessing the information provided by the visual layer to approach the Mu-\nsic Information Retrieval problem space. Different feature-sets were analyzed in the context of\ndifferent tasks. The results show that high-level concept detection approaches based on convo-\nlutional neural networks not only outperform traditional low-level image features as presented\nin Chapter 8, but also are superior to audio-content based descriptors in semantic music tagging\ntasks. The introduced audio-visual approaches improve to audio-only baseline by up to 16.43%\nfor genre classiﬁcation and up to 45.5% for thematic music tagging tasks. The evaluation also\n136\nBollywood\nOpera\nLatin\nCountry\nMetal\nReggae\nDance\nRap\nPop_Rock\nHard_Rock\nDubstep\nFolk\nReggaeton\n80ies\nIndie\nRnB\nabcde f ghi j kl m n o p\n74 261042110112014 a\n1 96 00000000011010 b\n52 37 5184161429537 c\n616 59 211091040424 d\n1014 47 4029 1 9 630040 e\n20843 55 0732435121 f\n307112 44 0 5 11 021 10 31 0 g\n3024220 71 13112404 h\n11 1 2 7 1 0 223 2 7 59520 1 2 2 i\n1002 2 2 2125 54 130430 j\n0063767030 50 55071 k\n03 1 0 5734182 1 2 2 5 17 1 0 2 l\n10 1 5 104 1 3 24052 4 1 02 1 0 m\n1123010 1 2 44240 60 51 n\n2387 1 3 33245 1 2 1 8 14 1 3 2 o\n301332 1 2 5 1 0 151813 4 2 p\nFigure 9.3: Confusion matrix of visual vocabulary based genre classiﬁcation using best per-\nforming combination of mean+max aggregation and support vector machines on the MV-MIX\ndataset. Values describe how often a row-genre has been predicted as a column-genre. Due to\nthe constant class size of 100 tracks, the diagonal elements represent the accuracies of 10-fold\ncross-validation for the given genre.\nindicates, that object vocabularies have the potential to capture the semantic and genre stereotyp-\nical information of music videos. Future work will focus on training specialized convolutional\nneural networks that include a wider range of music and genre related concepts.\n137\nUsing the Softmax scaled detection probabilities as extracted features and making them\ninvariant towards the varying lengths of music videos already demonstrated the potential of har-\nnessing the visual layer of music videos to improve the accuracy of music classiﬁcation tasks. It\nis though expected that recent approaches based on deep neural networks would provide further\nimprovements. Future work includes investigating if recurrent neural networks could be used to\nlearn temporal progressions and relationships of the visual conception depending on their music\ngenre. Neural networks further facilitate direct modeling of multiple modalities. A model which\nis trained on both - audio and visual input - can beneﬁt from mutual semantic dependencies.\nThis could yield higher classiﬁcation accuracy.\nThis approach and its corresponding evaluation presented in this chapter was published in Alexander Schindler and\nAndreas Rauber. “Harnessing Music-Related Visual Stereotypes for Music Information Retrieval” in the journal\nACM Transactions on Intelligent Systems (TIST), special issue on “Intelligent Music Systems and Applications”,\nvolume 8, issue 2, January 2017, article no. 20, ACM New York, NY, USA [252].\n138\nCHAPTER 10\nConclusions\n“The whole strenuous intellectual work of an industrious research\nworker would appear, after all, in vain and hopeless, if he were not\noccasionally through some striking facts to ﬁnd that he had, at the end\nof all his criss-cross journeys, at last accomplished at least one step\nwhich was conclusively nearer the truth. ”\n— Max Planck, Nobel Lecture in Physics, 1920\nThis thesis is based on the observation that the genre of many music videos can be correctly\nguessed by only watching to their visual content without hearing the actual music and the derived\nhypothesis that there exist music related visual patterns. The validation of this hypothesis is\napproached from a Music Information Retrieval (MIR) perspective using data analytical methods\nto identify patterns or features and evaluate their correlation to semantic music-concepts such\nas genre, theme or performing artist. Because an analytical approach can only show degrees of\ncorrelations, Chapter 3 strove for identifying literature describing practices in music marketing\nand the intentions of using visual media.\nThe review of the state of the art highlighted that audio-visual approaches are underrepre-\nsented in the MIR research domain, Especially audio-visual approaches to harness music related\nvisual information. Only a few studies experimented with ensembles of acoustic and visual fea-\ntures extracted from album-arts to approach music emotion recognition and genre classiﬁcation\ntasks. The literature review provided in Chapter 2 points out that recent advances in visual com-\nputing are capable of extracting abstract and subjective concepts such as movie genres. Because\nthe production processes in music related visual media are based on traditional cinematic pro-\ncesses (but far more complex as described in Chapter 3.2 and 7) it is reasonable that these visual\ncomputing methods can also be applied to the MIR problem space.\nTo verify that experimentally assessed relations between visual concepts and music gen-\nres/styles are not only based on statistical correlations, it was important to study the production\nprocesses. Chapter 3 extensively reviewed musicological literature to identify relevant descrip-\ntions of the application of visual media within the music marketing process. It was shown, that\n139\nhistorically visual media has been used extensively to advertise new music acts, especially in\nmusic magazines and print media in general. Marketing units of music labels were responsible\nfor making these new acts visually identiﬁable. Fashion and apparels were mentioned to be\ncommon means to achieve this goal. This is a strong indication that a visual language has been\ndeveloped to distinguish different music styles and genres.\nThe literature review in Chapter 2 reviled that audio-visual analysis of music videos has been\nlacking attention at the start of this dissertation and thus appropriate datasets were missing. The\nonly dataset available was the DEAP Dataset [138] for Emotion Analysis using EEG, Physio-\nlogical and Video Signals. This set is intended to map into a three-dimensional emotional space,\nwhich makes it different from the classiﬁcation tasks used in the evaluations. Further obstacles\nare the small size of only 40 music videos and the absence of discrete categorical labels. Thus,\nthe aim of Chapter 4.1 was to introduce datasets which are suitable for the development and\nevaluation of audio-visual approaches to MIR tasks. This was ﬁrst approached in Section 4.1 by\naugmenting the Million Song Dataset (MSD) with additional genre labels and state-of-the-art\naudio features extracted from 30 to 60 second samples provided by the online music stream-\ning service 7digital. These additions facilitated comprehensive experiments on the MSD and\nprovided ﬁrst large-scale benchmark results. The major contribution of Section 4.2 is the in-\ntroduction of the Music Video Dataset (MVD). This is a carefully aggregated dataset to develop\naudio-visual approaches to MIR tasks. Especially the MVD-VIS sub-set is ideal for identify-\ning music-related visual patterns. This is based on the premise that the audio tracks within the\nclasses have been selected by their acoustic similarity in terms of rhythmic, instrumentation,\nexpression and composition. Thus, the inherent question is, if the songs are similar in terms\nacoustic properties, do they also share similar visual patterns in the video layer? This property\nof the dataset makes it easier to draw conclusions and makes them more comprehensible. The\nMVD-MM sub-set is intended to be used to analyze if visual patterns overlap in a similar fash-\nion as acoustic features do in datasets with coarsely deﬁned classes. The MVD-Themes subset\nwas aggregated to evaluate if visual information can be used to solve tasks that are commonly\ndifﬁcult to approach by using audio content alone.\nChapter 5.1 reports on experiments performed on the introduced datasets to provide general\ninitial results and baseline results for the consecutive audio-visual experiments. The ﬁrst exper-\niments were conducted on the extended MSD. This also provided some valuable experiences\nin large-scale MIR. For example, best performing approaches in 2011 utilized Support Vector\nMachines (SVM) as classiﬁer in tasks such as genre or emotion classiﬁcation. By default, the\nSVM is a quadratic function. Although linear approximations are available, the computational\ncosts are still high, especially for the high dimensional music feature space. Facing these obsta-\ncles only low-dimensional featuresets could be evaluated in the initial experiments with lower\nthan state-of-the-art results. The experiments using Deep Neural Networks (DNN) have shown\nthat these models have several advantages. First, they can be trained iteratively, batch by batch,\nwhich makes them scalable even with large input dimensions. Second, they have shown high\naccuracy values in Chapter 5.1.1. Because the SVM results presented in this Chapter have not\nbeen obtained using the best performing feature-set combination, it cannot be concluded that\nDNNs outperform SVMs with state-of-the-art audio features.\n140\nChapter 5.2.2 provided valuable insights into the proprietary feature-set provided with the MSD.\nThese feature-sets were extracted and provided by the former companyThe Echonest. The brief\ndescription of the features made them incomprehensible and unreliable for experiments. The em-\npirical experiments presented in this chapter showed that these features are indeed semantically\nsimilar to the referenced music features. The evaluation of which mid-level features contribute\nmost to tasks relevant in this thesis and how to appropriately aggregate, served as a benchmark\nfor the experiments in the consecutive chapters.\nThe most prevalent type of such information is the human face. As outlined in Section 3.2\nthe general goal of music videos is to promote the performing artist. Consequently, the artist is\nshown as much as possible. Some artists, often well established ones, renounce the promotional\nadvantage and create artful concept videos. Such videos were excluded from the experiments\nand only those where the performing artist is shown were included.\nHarnessing facial information of the performing artist was evaluated in Chapter 6. Audio\ncontent and visual based classiﬁers were combined using an ensemble classiﬁer. The audio clas-\nsiﬁer used Temporal Echonest Features which were introduced in Section 5.2 to predict artist\nlabels. The audio classiﬁer’s low precision of 37% and a recall of 36% outlined the problem of\ndiscriminating artists by using conventional audio and music features which attempt to capture\nacoustic or compositional characteristics, whereas the repertoire of an artist varies in instru-\nmentation, style and genre. Also, the production methods change over time which results in\ndifferent acoustic characteristics. The approach was evaluated using state-of-the-art face recog-\nnition method based on a Local Binary Patterns (LBP) predictor. The two modalities - audio and\nvideo - were combined through bootstrap aggregation. For each modality 10 classiﬁers were\ncreated and trained on sub-samples of their according training-sets. The ﬁnal prediction for a\nmusic video was calculated on the basis of weighted majority voting of the resulting 20 predic-\ntions. The proposed cross-modal approach showed that the initial audio content based baseline\ncould be increased by 27% through information extracted from the visual part of music videos.\nThe aim of this evaluation was to set a starting point into a ﬁeld that has not yet received a lot\nof attention. It was shortly followed by the position paper [236] which introduced the advantages\nof an audio-visual approach to Music Information Retrieval. Although the application of face\nrecognition demonstrated the potential of such an approach, the artist identiﬁcation task is of\nlesser relevance to the research ﬁeld. Nevertheless, detecting faces in music related visual media\nhas further relevant applications.\nAn evaluation of low-level color based affective features was presented in Chapter 8. The\ncomparative evaluation of visual features on their performance in music classiﬁcation tasks fo-\ncused on the color information of music videos. The hypothesis of this evaluation was, that\ncommon colloquial associations such as “happy music is bright” and “sad and angry music is\ndark” are reﬂected in the choice of colors of the corresponding music video. Further, this evalua-\ntion served as a comprehensive bottom-up evaluation to investigate the performance of low-level\nimage features in describing the music related semantic content of music videos. A set of diverg-\ning approaches based on psychological or perceptive models has been applied to extract different\nkinds of low-level semantic information. Seven color and illumination based image processing\nfeature sets described in Section 8.1.2 including a descriptor capturing rhythmical changes in\nillumination was introduced.\n141\nThere were effects noticeable indicating that Color Names (CN), Wang Affective Factors\n(W AF) and Global Color Statistics (GCS) perform better in discriminating the different classes.\nNevertheless, the general conclusion of this evaluation is, that none of the evaluated low-level\nvisual features, nor any combination of them, is able to reliably predict music genres. Their\nlow music related information value was further conﬁrmed in Chapter 9 where no signiﬁcant\nimprovement was noticed by combining the low-level features with high-level semantic concepts\nsuch as apparel, buildings, music instruments, vehicles, etc.\nHowever it was able to conﬁrm the common stereotype of ’darkness’ related to Heavy Metal\nmusic [71]. Videos of the class Metal contain signiﬁcantly more black pixels (independent t-\ntest, p <0.05) than other classes of the MVD-MIX subset and the second smallest brightness\nvalues. The lowest values onbrightness and colorfulness for Opera are a strong indicator for the\ninferior lightning conditions at such venues and thus have no relation to the music itself. It was\nalso possible to conﬁrm the mentioned ’movement towards the warm, orange tones that became\nthe dominant ’look’ of many contemporary country videos. ’[79]. The average values for red and\nyellow are signiﬁcantly highest and second highest for this genre.\nThe evaluation of high-level visual features was presented in Chapter 9. It focused on the\nperformance of visual concept detection using a visual vocabulary based approach in music\ngenre and cross-genre classiﬁcation tasks. The results of the visual concept detection approach\noutperformed the accuracy of low-level visual features by 24.23% on the same data-set. This\nhigh accuracy supports the initial hypothesis that music videos make use of easy identiﬁable\nvisual concepts (see Research Question 1 in Chapter 1.2). An interesting observation was that\nthe genre classiﬁcation accuracy of the visual vocabulary based approach is signiﬁcantly out-\nperforming state-of-the-art low-level music features such as Chroma and the de-facto standard\nfeature MFCC. This also supports the hypothesis thatthese visual patterns can be used to derive\nfurther music characteristics (see Research Question RQ3 in Chapter 1.2).\nTo answer if the information extracted from music related visual media improves the perfor-\nmance of current Music Information Retrieval tasks a benchmark had to be assessed to compare\nthe performance of visual-related features against. For this reason the Music Video Dataset\n(MVD) was compiled following the intentions and constraints described in Section 4.2.1. State-\nof-the-art music features and classiﬁers were used to assess genre classiﬁcation accuracies. Due\nto the data-set’s characteristics, a very high accuracy is already achieved on the MVD-VIS sub-\nset. This was by intention, because the purpose of this well deﬁned sub-set was to analyze\nif music videos of highly similar sounding songs from different artists use similar visual pat-\nterns. Nevertheless, despite this high degree of acoustic coherence of the MVD-VIS subset, an\nimprovement of 2.94% was accomplished over the best performing audio combination through\nthe combination of audio and visual information. An even higher improvement was observed\nfor the less acoustically differentiated setMVD-MM (+6.84%) and the bigger MVD-MIX dataset\n(+10.82%). The visual information showed outstanding improvements, raning from 7.84% to\n16.43%, over some well-known audio feature sets such as the Temporal Statistical Spectrum\nDescriptors (TSSD).\nThe largest improvement, however was noticed for the classiﬁcation performance of non-\naudible or cross-genre music themes such as Christmas or Protest Songs. These themes often\nhave only few musical connotations and are thus difﬁcult to describe using the audio signal\n142\nalone. This was clearly shown by the low audio-based accuracies. Using visual features, almost\nall visual vocabulary based approaches already performed better than the audio-only results such\nas Christmas combined with MVD-MIX (+34,5%). More remarkable for the same combination\nthe improvement of 45.5% was observed. Similar high improvements were observed for the\nother themes.\nResearch Question 4 (RQ5) “Is it possible to verify concepts within the production process?”\nwas intended to evaluate if the identiﬁed patterns are just capturing abstract visual patterns or if\nthese visual features correspond real visual concepts that have been intentionally applied during\nthe production of the music video. The conclusion conﬁrms this assumption. This is based on\nthe following arguments:\n• The MVD-VIS datsaet has been assembled to maximize the acoustic coherence of its\ngenre-related classes. In other words, the songs within a class sound highly similar. This\nis conﬁrmed by the audio classiﬁcation accuracy of 93.79% presented in Table 8.2 (see\nChapter 8.1.1). The intention was to analyze if the visual representation shows similar\ncoherent visual patterns.\n• The results presented in Table 9.1 (see Chapter 9.4) showed an accuracy of 74.36%, using\nstatistics of high-level visual concepts detected in video frames of the MVD-VIS dataset.\nThis is a strong indicator that there exist class discriminating sets of visual features.\n• The literature search in Chapter 3 identiﬁed articles which described that visual accentua-\ntion is part of branding a new music act to make them immediately assignable to a music\nstyle or genre - at least in the pre-internet era. This can only be accomplished through\npatterns with high recognition value.\n• The analysis in Chapter 9.4.7 showed the most salient visual concepts for the genre-related\nclasses of the MVD dataset from which many are speciﬁc for the corresponding genre.\nFurther, music psychological and musicological literature was identiﬁed which identiﬁed\nsimilar audio-visual correlations for certain analyzed objects.\nThus, by verifying that these visual accentuation was deliberately applied to a certain song\nfrom a certain genre also veriﬁes that the extracted information is music and task related. On the\nother hand, it was also intended to evaluate if the presented approach can be utilized to quantify\nmusic-related visual stereotypes in music videos. This was approached by identifying genre or\nthematic related stereotypes. Through automatically identifying the most frequent salient visual\nconcepts for each genre through the visual vocabulary approach presented in Chapter 9 it was\npossible to relate them to reported music related stereotypes.\nVisual stereotypes were identiﬁed for the genre Country for which the cowboy hat and util-\nity vehicles such as the pickup truck or a tractor trailer are highly salient concepts. These\nobservations correspond to an evaluation of perceived extramusical associations with country\nmusic [264] and to aesthetic description provided in [79]. The second stereotype addresses the\nover-sexualization of contemporary popular Dance music [93]. Eight of the provided top rank-\ning examples referred to revealing clothes, closeups of body parts or people dancing on poles.\n143\nFor Heavy Metal music the common stereotype of darkness was conﬁrmed using low-level vi-\nsual features. Also ﬁre as a typical element related to Metal [71] was identiﬁed using the visual\nconcept based approach. For Opera a high number of top-ranking classical instruments related\nconcepts can be observed.\n10.1 Future Work\nThe answered research questions, gained insights and conclusions set up a good starting point\nfor further research to harness audio-visual correlations within the MIR domain.\nComplex audio-visual modeling: Based on the new options provided by Deep Neural net-\nworks (DNN) it is easier to train integrated audio-visual models. Different to the two-phase\napproaches presented in this thesis, where ﬁrst features are extracted from the different modal-\nities and then fused either in the feature-space or by ensembles of classiﬁers, a multi-modal\nDNN can take visual and acoustic data as input to learn a common representation and directly\npredict results. Especially Recurrent Neural Networks (RNN) are expected to better utilize the\nsequential information provided by video frames and audio data. This could lead to improved\naudio-visual representations which capture the correlations between the two modalities better\nand perform better in the speciﬁc tasks on provided datasets.\nThe current success of deep neural network based approaches in visual computing has also\nimproved face recognition systems. Recent methods [70] have overcome obstacles reported in\nSection 6.5.2. Features such as headcount, position of a face, the area it takes within the video\nframe, the amount of time it is shown, etc. could contain semantically valuable information\nto distinguish for example solo artists from bands or studio settings from live performances.\nAnalyzing facial expressions could contribute to music emotion recognition systems.\nCross-modal modeling: Similarly, DNNs can be used to learn mutual representation spaces.\nThis has been demonstrated for visual and textual inputs [313] and still need more attention for\nvisual and acoustic inputs. Recent advances linked symbolic to recorded music in a learned\ncross-modal representation for improved audio-sheet music correspondence [57, 198]. Merging\nacoustic and visual information into the same representation space facilitates a wide range of\nnew options and tasks. For instance, it facilitates to use visual inputs to search directly for music\nor sounds (e.g. using the image of a bird to search for tweets, using the image at a party to\nsearch for adequate music). Such an approach has recently been reported on general video-\ndatasets [41]. In this approach a cross-domain audio-visual representation is learned to retrieve\na video sequence using an audio sequence as query and vice-versa.\nSuch an approach could be extended to the music domain to use visual inputs to query for\nmusic. This could be used to automatically recommend music for videos. In a further step\nthis representation space can be extended to further modalities such as the text domain. By\nlearning relationships between visual concepts and acoustic patterns such as birds and tweets,\nthese concepts can be linked to words or word embeddings from textual corpora. Thus, text\nbased search in music content might be achieved without the need for text-based annotations.\n144\nMulti-task music representation learning: Designing a well deﬁned feature-set requires de-\ntailed knowledge about the problem domain as well as the characteristics of the dataset. Various\nfactors have to be considered. These can span from perceptual properties such as mood to stylis-\ntic and genre speciﬁc characteristics, but may also include or focus on musicological aspects\nsuch as epochs or mutual inﬂuences of composers. To describe these factors, many of them\nrequire different feature-sets. On the one hand these are difﬁcult to extract and manage. On the\nother hand, such combinations often do not generalize or scale well. Representation learning\nrepresents a promising alternative. Features and feature-sets are not composed but learned -\nusually using Deep Neural Networks (DNN) in a supervised fashion from a given ground-truth\nannotation. The obstacle of this approach is the requirement for large amounts of annotations -\nfor each factor/characteristics that should be considered. Annotating such large amounts of data\nis time and resource-intensive.\nThe visual layer of music videos provides a wide range of contextual information. State-\nof-the-art Convolutional Neural Networks (CNN) such as the InceptionResNetV2 architecture\n[281] already reach an accuracy of 80.1% on the Top-1 Error and 95.1% accuracy on the Top-5\nError. Visual concepts detected in the music video frames with high prediction conﬁdence could\nserve as a substitution for manual annotations. Here the knowledge of the annotations from\nthe visual model is transferred to the music domain. As outlined in this thesis, the frequency\ndistribution of these visual concepts is discriminative for music genres. Using Deep Learning\nconcepts such as triplet networks or mutli-task learning, the information from the visual domain\ncan be harnessed to either transfer the visual semantics to the music representation, or to guide\nthe learning process by the visual semantics. This could especially contribute to MIR tasks such\nsinger and instrument recognition.\n145\n\nAPPENDIX A\nPublications\n• Alexander Schindler and Peter Knees. Multi-Task Music Representation Learning from\nMulti-Label Embeddings. In Proceedings of the International Conference on Content-\nBased Multimedia Indexing (CBMI2019). Dublin, Ireland, 4-6 Sept 2019. [240]\n• Anahid Jalali, Clemens Heistracher, Alexander Schindler, Bernhard Haslhofer, Tanja Nemeth,\nRobert Glawar, Wilfried Sihn, Peter De Boer. Predicting Time-to-Failure of Plasma Etch-\ning Equipment using Machine Learning. In Proceedings of the IEEE International Con-\nference on Prognostics and Health Management (PHM2019), June 17-19, 2019, in San\nFrancisco, USA. [119]\n• Anahid N Jalali, Alexander Schindler, Bernhard Haslhofer. Understandable Deep Neural\nNetworks for Predictive Maintenance in the Manufacturing Industry In ERCIM News,\nNumber 116, Jan 2019. [120]\n• Alexander Schindler and Andreas Rauber. On the unsolved problem of Shot Boundary\nDetection for Music Videos. In Proceedings of the 25th International Conference on Mul-\ntiMedia Modeling (MMM2019), January 8-11, 2019, in Thessaloniki, Greece. [253]\n• Alexander Schindler, Andrew Lindley, David Schreiber, Martin Boyer and Thomas Philipp.\nLarge Scale Audio-Visual Video Analytics Platform for Forensic Investigations of Terror-\nistic Attacks. In Proceedings of the 25th International Conference on MultiMedia Model-\ning (MMM2019), January 8-11, 2019, in Thessaloniki, Greece. [237]\n• Nemeth, Tanja, Fazel Ansari, Wilfried Sihn, Bernhard Haslhofer, and Alexander Schindler.\nPriMa-X: A reference model for realizing prescriptive maintenance and assessing its ma-\nturity enhanced by machine learning. Procedia CIRP 72 (2018): 1039-1044. [200]\n• Alexander Schindler and Sven Schlarb. Contextualised Conversational Systems. In ERCIM\nNews, Number 114, July 2018. [254]\n147\n• Alexander Schindler, Thomas Lidy and Andreas Rauber. Multi-Temporal Resolution Con-\nvolutional Neural Networks for Acoustic Scene Classiﬁcation. In Proceedings of the De-\ntection and Classiﬁcation of Acoustic Scenes and Events 2017 Workshop (DCASE2017),\nNovember 2017. [245]\n• Alexander Schindler, Thomas Lidy and Andreas Rauber. Multi-Temporal Resolution Con-\nvolutional Neural Networks for the DCASE Acoustic Scene Classiﬁcation Task. Technical\nreport, DCASE2017 Challenge, November 2017. [242]\n• Botond Fazekas, Alexander Schindler, Thomas Lidy, Andreas Rauber. A multi-modal\ndeep neural network approach to bird-song identiﬁcation. LifeCLEF 2017 working notes,\nDublin, Ireland [72]\n• Alexander Schindler, Thomas Lidy, Stefan Karner and Matthias Hecker. Fashion and\nApparel Classiﬁcation using Convolutional Neural Networks. In Proceedings of the 9th\nForum Media Technology (FMT2017), St. Poelten, Austria, October 29, 2017. [241]\n• Alexander Schindler and Andreas Rauber. Harnessing Music related Visual Stereotypes\nfor Music Information Retrieval. ACM Transactions on Intelligent Systems and Technol-\nogy (TIST) 8.2 (2016): 20 [252]\n• Alexander Schindler, Thomas Lidy, and Andreas Rauber. Comparing shallow versus deep\nneural network architectures for automatic music genre classiﬁcation. In Proceedings\nof the 9th Forum Media Technology (FMT2016), St. Poelten, Austria, November 23 -\nNovember 24 2016. [243]\n• Alexander Schindler, Sergiu Gordea, and Harry van Biessum. The europeana sounds\nmusic information retrieval pilot. In Proceedings of the International Conference on Cul-\ntural Heritage (EuroMed2016), Lecture Notes in Computer Science, Cyprus, October 31\n- November 5 2016. Springer. [238]\n• Thomas Lidy and Alexander Schindler. CQT-based convolutional neural networks for\naudio scene classiﬁcation. In Proceedings of the Detection and Classiﬁcation of Acoustic\nScenes and Events 2016 Workshop (DCASE2016), pages 60–64, September 2016. [159]\n• Thomas Lidy and Alexander Schindler. CQT-based convolutional neural networks for\naudio scene classiﬁcation and domestic audio tagging. Technical report, DCASE2016\nChallenge, September 2016.\n• Thomas Lidy and Alexander Schindler. Parallel convolutional neural networks for music\ngenre and mood classiﬁcation. Technical report, Music Information Retrieval Evaluation\neXchange (MIREX 2016), August 2016. [161]\n• Thomas Lidy, Alexander Schindler and Michela Magas. MusicBricks: Connecting Digital\nCreators to the Internet of Music Things. In ERCIM News, Number 101, April 2015.\n[162]\n148\n• Alexander Schindler and Andreas Rauber. An audio-visual approach to music genre clas-\nsiﬁcation through affective color features. In Proceedings of the 37th European Confer-\nence on Information Retrieval (ECIR’15), Vienna, Austria, March 29 - April 02 2015.\n[250]\n• Alexander Schindler. A picture is worth a thousand songs: Exploring visual aspects of\nmusic. In Proceedings of the 1st International Digital Libraries for Musicology workshop\n(DLfM 2014), London, UK, September 12 2014. [236]\n• Roman Graf, Alexander Schindler, and Reinhold Huber-Moerk. A fuzzy logic based ex-\npert system for quality assurance of document image collections. In International Journal\nof Arts & Sciences IJAS 2014 to appear, Valetta, Malta, March 2-6 2014. [89]\n• Roman Graf, Reinhold Huber-Moerk, Alexander Schindler, and Sven Schlarb. Duplicate\ndetection approaches for quality assurance of document image collections. In Proceedings\nof the International ACM Conference on Management of Emergent Digital EcoSystems\n(MEDES’13) to appear, Neumuenster Abbey, Luxembourg, October 28-31 2013. [88]\n• Alexander Schindler and Reinhold Huber-Moerk. Towards objective quality assessment\nin digital image collections. In Proceedings of the 2nd Workshop on Open Research\nChallenges in Digital Preservation (ORC’13) to appear, Lisbon, Portugal, September 6\n2013. [239]\n• Alexander Schindler and Andreas Rauber. A music video information retrieval approach\nto artist identiﬁcation. In Proceedings of the 10th International Symposium on Computer\nMusic Multidisciplinary Research (CMMR2013) to appear, Marseille, France, October\n14-18 2013. [248]\n• Reinhold Huber-Moerk and Alexander Schindler. Automatic classiﬁcation of defect page\ncontent in scanned document collections. In Proceedings of the 8th International Sympo-\nsium on Image and Signal Processing and Analysis (ISPA 2013) to appear, Trieste, Italy,\nSeptember 4-6 2013. [110]\n• Sven Schlarb, Peter Cliff, Peter May, William Palmer, Matthias Hahn, Reinhold Huber-\nMoerk, Alexander Schindler, Rainer Schmidt, and Johan van der Knijff. Quality assured\nimage ﬁle format migration in large digital object repositories. In Proceedings of the 10th\nInternational Conference on Digital Preservation (IPres2013) to appear, Lisbon, Portugal,\nSeptember 2-5 2013. [255]\n• Reinhold Huber-Moerk and Alexander Schindler. A keypoint based approach for content\ncharacterization in document collections. In Proceedings of the 9th International Sympo-\nsium on Visual Computing (ISVC’13) to appear, Rethymnon, Crete, Greece, July 29-31\n2013. [111]\n• Alexander Schindler and Andreas Rauber. Capturing the temporal domain in echonest fea-\ntures for improved classiﬁcation effectiveness. In Adaptive Multimedia Retrieval, Lecture\nNotes in Computer Science, Copenhagen, Denmark, October 24-25 2012. Springer. [249]\n149\n• Andreas Rauber, Alexander Schindler, Nicu Sebe, Henning Mueller, Shara Monteleone,\nYiannis Kompatsiaris, Spiros Nikolopoulos, Alexis Joly, and Henri Gouraud. Latest trends\nin multimedia search computing. In Nicu Sebe, editor, Latest Trends in Multimedia Search\nComputing (Media Search Cluster White Paper). European Commission - Infomation\nSociety and Media, December 2012.\n• Reinhold Huber-Moerk, Alexander Schindler, and Sven Schlarb. Duplicate detection\nfor quality assurance of document image collections. In Proceedings of the 9th Inter-\nnational Conference on Digital Preservation (IPres2012), Toronto, Canada, October 1-5\n2012. [112]\n• Roman Graf, Reinhold Huber-Moerk, and Alexander Schindler. An expert system for\nquality assurance of document image collections. In Proceedings of the International\nConference on Cultural Heritage (EuroMed2012), Lecture Notes in Computer Science,\nLemesos, Cyprus, October 29 - November 3 2012. Springer. [87]\n• Alexander Schindler, Rudolf Mayer, and Andreas Rauber. Facilitating comprehensive\nbenchmarking experiments on the million song dataset. In Proceedings of the 13th Inter-\nnational Society for Music Information Retrieval Conference (ISMIR 2012), pages 469-\n474, Porto, Portugal, October 8-12 2012. [246]\n• Reinhold Huber-Moerk and Alexander Schindler. Quality assurance for document image\ncollections in digital preservation. In Proceedings of the 14th International Conference\non Advanced Concepts for Intelligent Vision Systems (ACIVS 2012), Lecture Notes in\nComputer Science, Brno, Czech Republic, September 4-7 2012. Springer. [109]\n• Alexander Schindler. Million song dataset integration into the clubmixer framework. In\nProceedings of the 12th International Society for Music Information Retrieval Conference\n(ISMIR 2011), Miami, USA, October 24-28 2011. [235]\n• Roman Graf, Reinhold Huber-Moerk, and Alexander Schindler. Quality assurance for\nscalable braille web service using human interaction by a computer vision system. In\nPost-conference Proceedings of the International Conference on Integrated Information\n(IC-ININFO 2011), Kos Island, Greece, September 29 2011. [86]\n• Alexander Schindler and Andreas Rauber. Clubmixer: A presentation platform for mir\nprojects. In Marcin Detyniecki, Peter Knees, Andreas Nuernberger, Markus Schedl, and\nSebastian Stober, editors, Adaptive Multimedia Retrieval. Context, Exploration and Fu-\nsion Adaptive Multimedia Retrieval. Context, Exploration and Fusion, volume 6817 of\nLecture Notes in Computer Science, Linz, Austria, August 17-18 2010. Springer. [247]\n• Alexander Schindler. Quality of service driven workﬂows within the microsoft .net envi-\nronment. Master’s thesis, Vienna University of Technology, 2009.\n150\nBibliography\n[1] In R.E. Allen, editor, The Concise Oxford Dictionary, page 781. Springer, 1992.\n[2] Esra Acar, Frank Hopfgartner, and Sahin Albayrak. Understanding affective content of\nmusic videos through learned representations. In MultiMedia Modeling, pages 303–314.\nSpringer, 2014.\n[3] Timo Ahonen, Abdenour Hadid, and Matti Pietikäinen. Face recognition with local binary\npatterns. In Computer Vision-ECCV, pages 469–481. Springer, 2004.\n[4] Eric Allamanche, Thorsten Kastner, Ralf Wistorf, Nicolas Lefebvre, and Juergen Herre.\nMusic genre estimation from low level audio features. InAudio Engineering Society Con-\nference: 25th International Conference: Metadata for Audio. Audio Engineering Society,\n2004.\n[5] Arnon Amir, Marco Berg, Shih-Fu Chang, Winston Hsu, Giridharan Iyengar, Ching-Yung\nLin, Milind Naphade, Apostol Natsev, Chalapathy Neti, Harriet Nock, et al. Ibm research\ntrecvid-2003 video retrieval system. NIST TRECVID-2003, 7(8):36, 2003.\n[6] Kamelia Aryafar and Ali Shokoufandeh. Multimodal music and lyrics fusion classiﬁer\nfor artist identiﬁcation. In Machine Learning and Applications (ICMLA), 2014 13th In-\nternational Conference on, pages 506–509. IEEE, 2014.\n[7] Jean-Julien Aucouturier and Francois Pachet. Representing musical genre: A state of the\nart. Journal of New Music Research, 32(1):83–93, 2003.\n[8] Elizabeth C Axford. Song sheets to software: a guide to prInternational music, software,\nand web sites for musicians. Scarecrow Press, 2004.\n[9] Laura-Lee Balkwill and William Forde Thompson. A cross-cultural investigation of the\nperception of emotion in music: Psychophysical and cultural cues. Music perception: an\ninterdisciplinary journal, 17(1):43–64, 1999.\n[10] Lorenzo Baraldi, Costantino Grana, and Rita Cucchiara. Hierarchical boundary-aware\nneural encoder for video captioning. In Computer Vision and Pattern Recognition\n(CVPR), 2017 IEEE Conference on, pages 3185–3194. IEEE, 2017.\n151\n[11] Herbert Bay, Andreas Ess, Tinne Tuytelaars, and Luc Van Gool. Speeded-up robust fea-\ntures (surf). Computer vision and image understanding, 110(3):346–359, 2008.\n[12] Peter N. Belhumeur, Joao P. Hespanha, and David J. Kriegman. Eigenfaces vs. ﬁsher-\nfaces: Recognition using class speciﬁc linear projection. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 19(7):711–720, 1997.\n[13] Sean Bell and Kavita Bala. Learning visual similarity for product design with convolu-\ntional neural networks. ACM Transactions on Graphics (TOG), 34(4):98, 2015.\n[14] Kirell Benzi, Michaël Defferrard, Pierre Vandergheynst, and Xavier Bresson. Fma: A\ndataset for music analysis. arXiv preprInternational arXiv:1612.01840, 2016.\n[15] Adam Berenzweig, Beth Logan, Daniel PW Ellis, and Brian Whitman. A large-scale eval-\nuation of acoustic and subjective music-similarity measures. Computer Music Journal,\n28(2):63–76, 2004.\n[16] James Bergstra, Norman Casagrande, Dumitru Erhan, Douglas Eck, and Balázs Kégl.\nAggregate features and adaboost for music classiﬁcation.Machine learning, 65(2-3):473–\n484, 2006.\n[17] James Bergstra, Re Lacoste, and Douglas Eck. Predicting genre labels for artists using\nfreedb. In In Proceedings of the International Conference on Music Information Retrieval,\n2006.\n[18] Thierry Bertin-Mahieux and Daniel PW Ellis. Large-scale cover song recognition using\nhashed chroma landmarks. In In Proceedings of 2011 IEEE Workshop on Applications of\nSignal Processing to Audio and Acoustics (WASPAA), pages 117–120. IEEE, 2011.\n[19] Thierry Bertin-Mahieux, Daniel PW Ellis, Brian Whitman, and Paul Lamere. The mil-\nlion song dataset. In In Proceedings 12th International Society for Music Information\nRetrieval Conference, pages 591–596, 2011.\n[20] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere. The mil-\nlion song dataset. In Proceedings of the International Conference on Music Information\nRetrieval (ISMIR2011), 2011.\n[21] Sebastian Böck, Filip Korzeniowski, Jan Schlüter, Florian Krebs, and Gerhard Widmer.\nmadmom: a new python audio and music signal processing library. In Proceedings of the\n2016 ACM on Multimedia Conference, pages 1174–1178. ACM, 2016.\n[22] Dmitry Bogdanov, Nicolas Wack, Emilia Gómez, Sankalp Gulati, Perfecto Herrera, Oscar\nMayor, Gerard Roma, Justin Salamon, José R Zapata, and Xavier Serra. Essentia: An\naudio analysis library for music information retrieval. InISMIR, pages 493–498. Citeseer,\n2013.\n152\n[23] John S Boreczky and Lynn D Wilcox. A hidden markov model framework for video\nsegmentation using audio and image features. In ICASSP, volume 98, pages 3741–3744,\n1998.\n[24] Margaret M Bradley and Peter J Lang. Affective norms for english words (anew): In-\nstruction manual and affective ratings. Technical report, Technical report C-1, the center\nfor research in psychophysiology, University of Florida, 1999.\n[25] G. Bradski. The opencv library. Dr. Dobb’s Journal of Software Tools, 2000.\n[26] Leo Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996.\n[27] Darin Brezeale and Diane J Cook. Using closed captions and visual features to classify\nmovies by genre. In Poster session of the Seventh International Workshop on Multimedia\nData Mining (MDM/KDD2006), 2006.\n[28] Darin Brezeale and Diane J Cook. Automatic video classiﬁcation: A survey of the litera-\nture. IEEE Transactions Systems, Man, and Cybernetics, 38(3):416–430, 2008.\n[29] Eric Brochu, Nando De Freitas, and Kejie Bao. The sound of an album cover: Proba-\nbilistic multimedia and ir. In In Proceedings of Workshop on Artiﬁcial Intelligence and\nStatistics, 2003.\n[30] Juan José Burred and Alexander Lerch. A hierarchical approach to automatic musical\ngenre classiﬁcation. In Proceedings of the 6th international conference on digital audio\neffects, pages 8–11. Citeseer, 2003.\n[31] Rui Cai, Lei Zhang, Feng Jing, Wei Lai, and Wei-Ying Ma. Automated music video gen-\neration using web image resource. In Acoustics, Speech and Signal Processing. ICASSP,\n2007.\n[32] Pedro Cano, Emilia Gómez, Fabien Gouyon, Perfecto Herrera, Markus Koppenberger,\nBeesuan Ong, Xavier Serra, Sebastian Streich, and Nicolas Wack. ISMIR 2004 audio\ndescription contest. Technical report, 2006.\n[33] Michael A Casey, Remco Veltkamp, Masataka Goto, Marc Leman, Christophe Rhodes,\nand Malcolm Slaney. Content-based music information retrieval: Current directions and\nfuture challenges. Proceedings of the IEEE, 96(4):668–696, 2008.\n[34] Emily Caston et al. The ﬁne art of commercial freedom: British music videos and ﬁlm\nculture. Scope: An Online Journal of Film and Television Studies, 2014.\n[35] Oscar Celma. Foaﬁng the music: Bridging the semantic gap in music recommendation.\nIn In Proceedings International Conference The Semantic Web, pages 927–934. Springer,\n2006.\n[36] Oscar Celma. Music Recommendation and Discovery: The Long Tail, Long Fail, and\nLong Play in the Digital Music Space . Springer Publishing Company, Incorporated, 1st\nedition, 2010.\n153\n[37] Òscar Celma, Pedro Cano, and Perfecto Herrera. Search sounds an audio crawler focused\non weblogs. In 7th International Conference on Music Information Retrieval (ISMIR) ,\n2006.\n[38] Òscar Celma Herrada. Music recommendation and discovery in the long tail, 2009.\n[39] Zuzana Cernekova, Ioannis Pitas, and Christophoros Nikou. Information theory-based\nshot cut/fade detection and video summarization. IEEE Transactions on circuits and\nsystems for video technology, 16(1):82–91, 2006.\n[40] Ken Chatﬁeld, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Return of\nthe devil in the details: Delving deep into convolutional nets. arXiv preprInternational\narXiv:1405.3531, 2014.\n[41] Lele Chen, Sudhanshu Srivastava, Zhiyao Duan, and Chenliang Xu. Deep cross-modal\naudio-visual generation. In Proceedings of the on Thematic Workshops of ACM Multime-\ndia 2017, pages 349–357. ACM, 2017.\n[42] Cyril Cleverdon. The cranﬁeld tests on index language devices. In Aslib proceedings,\nvolume 19, pages 173–194, 1967.\n[43] Yandre M.G. Costa, Luiz S. Oliveira, and Carlos N. Silla Jr. An evaluation of convolu-\ntional neural networks for music classiﬁcation using spectrograms. Applied Soft Comput-\ning, 52:28 – 38, 2017.\n[44] Costas Cotsaces, Nikos Nikolaidis, and Ioannis Pitas. Video shot detection and condensed\nrepresentation. a review. Signal Processing Magazine, IEEE, 23(2):28–37, 2006.\n[45] Frederique Crete, Thierry Dolmiere, Patricia Ladret, and Marina Nicolas. The blur effect:\nperception and estimation with a new no-reference perceptual blur metric. In Electronic\nImaging, 2007.\n[46] Sally Jo Cunningham, David Bainbridge, and Dana Mckay. Finding new music: A diary\nstudy of everyday encounters with novel songs. In In Proceedings of 8th International\nSociety for Music Information Retrieval Conference (ISMIR 2007), Vienna, Austria, 2007.\n[47] Dave Datta. Managing metadata. In In Proceedings of the International Conference on\nMusic Information Retrieval, Paris, France, October 2002.\n[48] Ritendra Datta, Dhiraj Joshi, Jia Li, and James Z Wang. Studying aesthetics in photo-\ngraphic images using a computational approach. In Computer Vision–ECCV 2006, pages\n288–301. Springer, 2006.\n[49] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale\nHierarchical Image Database. In CVPR09, 2009.\n[50] Diana Deutsch. The Psychology of Music. Academic Press, third edition edition, 2013.\n154\n[51] T Dharani and I Laurence Aroquiaraj. A survey on content based image retrieval. In\nPattern Recognition, Informatics and Mobile Engineering (PRIME), 2013 International\nConference on, pages 485–490. IEEE, 2013.\n[52] Kay Dickinson and Amy Herzog. Medium cool: Music videos from soundies to cell-\nphones. Duke University Press, 2007.\n[53] Sander Dieleman, Philémon Brakel, and Benjamin Schrauwen. Audio-based music classi-\nﬁcation with a pretrained convolutional network. In 12th International Society for Music\nInformation Retrieval Conference (ISMIR-2011) , pages 669–674. University of Miami,\n2011.\n[54] Sander Dieleman and Benjamin Schrauwen. Audio-based music classiﬁcation with a\npretrained convolutional network. In Proceedings of the 12th International Conference\non Music Information Retrieval (ISMIR 2011), 2011.\n[55] Nevenka Dimitrova, Lalitha Agnihotri, and Gang Wei. Video classiﬁcation based on hmm\nusing text and faces. In European Conference on Sig. In Proceedings. Citeseer, 2000.\n[56] David Doermann. The indexing and retrieval of document images: A survey. Computer\nVision and Image Understanding, 70(3):287–298, 1998.\n[57] Matthias Dorfer, Jan Haji ˇc Jr, Andreas Arzt, Harald Frostel, and Gerhard Widmer. Learn-\ning audio–sheet music correspondences for cross-modal retrieval and piece identiﬁcation.\nTransactions of the International Society for Music Information Retrieval, 1(1), 2018.\n[58] J Stephen Downie. Music information retrieval. Annual review of information science\nand technology, 37(1):295–340, 2003.\n[59] J Stephen Downie. The scientiﬁc evaluation of music information retrieval systems:\nFoundations and future. Computer Music Journal, 28(2):12–23, 2004.\n[60] J Stephen Downie, Donald Byrd, and Tim Crawford. Ten years of ISMIR: Reﬂections on\nchallenges and opportunities. In Proceedings ISMIR, (Ismir):13–18, 2009.\n[61] J Stephen Downie, Xiao Hu, Jin Ha Lee, Kahyun Choi, Sally Jo Cunningham, and Yun\nHao. Ten years of mirex: reﬂections, challenges and opportunities. In ISMIR 2014, pages\n657–662. ISMIR, 2014.\n[62] J Stephen Downie, Kris West, Andreas Ehmann, Emmanuel Vincent, et al. The 2005\nmusic information retrieval evaluation exchange (mirex 2005): Preliminary overview. In\n6th International Conference on Music Information Retrieval (ISMIR) , pages 320–323,\n2005.\n[63] Peter Dunker, Stefanie Nowak, André Begau, and Cornelia Lanz. Content-based mood\nclassiﬁcation for photos and music: a generic multi-modal classiﬁcation framework and\nevaluation approach. In In Proceedings of 1st ACM international conference on Multime-\ndia information retrieval, pages 97–104. ACM, 2008.\n155\n[64] R. Duval. A new automatic multiﬁlm projector for audio-visual and entertainment pur-\nposes. Journal of the SMPTE, 74(12):1104–1107, Dec 1965.\n[65] Douglas Eck, Paul Lamere, Thierry Bertin-Mahieux, and Stephen Green. Automatic\ngeneration of social tags for music recommendation. In Advances in neural information\nprocessing systems, pages 385–392, 2008.\n[66] H. Eghbal-zadeh, M. Dorfer, and G. Widmer. A cosine-distance based neural network\nfor music artist recognition using raw i-vector features. In International Conference on\nDigital Audio Effects (DAFx), Brno, Czech Republic, 2016.\n[67] Hamid Eghbal-zadeh, Bernhard Lehner, Markus Schedl, and Gerhard Widmer. I-Vectors\nfor Timbre-Based Music Similarity and Music Artist Classiﬁcation. InProceedings of the\n16th International Society for Music Information Retrieval Conference (ISMIR), Malaga,\nSpain, October 2015.\n[68] Daniel PW Ellis. Classifying music audio with timbral and chroma features. In Proc\nInternational Conference Music Information Retrieval, pages 339–340, 2007.\n[69] Sebastian Ewert, Meinard Müller, and Peter Grosche. High resolution audio synchroniza-\ntion using chroma onset features. In IEEE International Conference on Acoustics, Speech\nand Signal Processing, 2009.\n[70] Sachin Sudhakar Farfade, Mohammad J Saberian, and Li-Jia Li. Multi-view face de-\ntection using deep convolutional neural networks. In Proceedings of the 5th ACM on\nInternational Conference on Multimedia Retrieval, pages 643–650. ACM, 2015.\n[71] Helen Farley. Demons, devils and witches: the occult in heavy metal music. Heavy metal\nmusic in Britain, pages 73–88, 2009.\n[72] Botond Fazeka, Alexander Schindler, Thomas Lidy, and Andreas Rauber. A multi-\nmodal deep neural network approach to bird-song identiﬁcation. arXiv preprint\narXiv:1811.04448, 2018.\n[73] Haihua Feng, David A Castanon, and William Clement Karl. A curve evolution approach\nfor image segmentation using adaptive ﬂows. In Computer Vision, 2001. ICCV 2001.\nProceedings. Eighth IEEE International Conference on, volume 2, pages 494–499. IEEE,\n2001.\n[74] Joanna Finkelstein. Art of Self Invention: Image and Identity in Popular Visual Culture .\nIB Tauris, 2007.\n[75] Stefan Fischer, Rainer Lienhart, and Wolfgang Effelsberg. Automatic recognition of ﬁlm\ngenres. Technical reports, 95, 2004.\n[76] Jonathan Foote, Matthew Cooper, and Andreas Girgensohn. Creating music videos using\nautomatic media analysis. In Proceedings of the tenth ACM international conference on\nMultimedia. ACM, 2002.\n156\n[77] Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learn-\ning and an application to boosting. Journal of computer and system sciences, 55(1):119–\n139, 1997.\n[78] Simon Frith, Andrew Goodwin, and Lawrence Grossberg. Sound and vision: The music\nvideo reader. Routledge, 1993.\n[79] Simon Frith, Andrew Goodwin, and Lawrence Grossberg. Sound and vision: the music\nvideo reader. Routledge, 2005.\n[80] Zhouyu Fu, Guojun Lu, Kai Ming Ting, and Dengsheng Zhang. A survey of audio-based\nmusic classiﬁcation and annotation. Multimedia, IEEE Transactions on, 13(2):303–319,\n2011.\n[81] Zhouyu Fu, Guojun Lu, Kai Ming Ting, and Dengsheng Zhang. A survey of audio-based\nmusic classiﬁcation and annotation. IEEE Transactions on Multimedia, 13(2):303–319,\n2011.\n[82] Olivier Gillet, Slim Essid, and Gaël Richard. On the correlation of automatic audio and\nvisual segmentations of music videos. Circuits and Systems for Video Technology, IEEE\nTransactions on, 17(3):347–355, 2007.\n[83] Michael Good et al. Musicxml: An internet-friendly format for sheet music. In XML\nConference and Expo, pages 3–4. Citeseer, 2001.\n[84] Andrew Goodwin. Dancing in the distraction factory . University of Minnesota Press,\n1992.\n[85] M. Goto. A chorus section detection method for musical audio signals and its application\nto a music listening station. IEEE Transactions Audio, Speech & Language Processing,\n14(5):1783–1794, 2006.\n[86] Roman Graf, Reinhold Huber-Mörk, and Alexander Schindler. Quality assurance for\nscalable braille web service using human interaction by computer vision system.\n[87] Roman Graf, Reinhold Huber-Mörk, Alexander Schindler, and Sven Schlarb. An expert\nsystem for quality assurance of document image collections. In Euro-Mediterranean\nConference, pages 251–260. Springer, 2012.\n[88] Roman Graf, Reinhold Huber-Mörk, Alexander Schindler, and Sven Schlarb. Duplicate\ndetection approaches for quality assurance of document image collections. InProceedings\nof the Fifth International Conference on Management of Emergent Digital EcoSystems ,\npages 152–158. ACM, 2013.\n[89] Roman Graf, Alexander Schindler, and Reinhold Huber-Mörk. A fuzzy logic based expert\nsystem for quality assurance of document image collections.International Journal of Arts\n& Sciences, 7(2):119, 2014.\n157\n[90] Tom Gunning. pprimitivecinema: A frame-up? or the trick’s on us. Cinema Journal,\n28(2):3–12, 1989.\n[91] Magnus Haake and Agneta Gulz. Visual stereotypes and virtual pedagogical agents. Jour-\nnal of Educational Technology & Society, 11(4):1–15, 2008.\n[92] Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and\nIan H Witten. The weka data mining software: an update. ACM SIGKDD Explorations\nNewsletter, 11(1):10–18, 2009.\n[93] P Cougar Hall, Joshua H West, and Shane Hill. Sexualization in lyrics of popular music\nfrom 1959 to 2009: Implications for sexuality educators. Sexuality & Culture, 2012.\n[94] Philippe Hamel and Douglas Eck. Learning features from music audio with deep belief\nnetworks. In ISMIR, volume 10, pages 339–344. Utrecht, The Netherlands, 2010.\n[95] Arun Hampapur, Terry Weymouth, and Ramesh Jain. Digital video segmentation. In\nProceedings of the second ACM international conference on Multimedia, pages 357–364.\nACM, 1994.\n[96] Allan Hanbury. Circular statistics applied to colour images. In 8th Computer Vision\nWinter Workshop, 2003.\n[97] Allan Hanbury and Jean Serra. A 3d-polar coordinate colour representation suitable for\nimage analysis. submitted to Computer Vision and Image Understanding, 2002.\n[98] David Hauger, Markus Schedl, Andrej Košir, and Marko Tkalcic. The million musical\ntweets dataset: What can we learn from microblogs. In Proceedings of the 14th Interna-\ntional Society for Music Information Retrieval Conference (ISMIR 2013), 2013.\n[99] Alexander Hauptmann, MY Chen, Mike Christel, C Huang, Wei-Hao Lin, T Ng, Norman\nPapernick, A Velivelli, Jie Yang, Rong Yan, et al. Confounded expectations: Informedia\nat trecvid 2004. In In Proceedings of TRECVID, 2004.\n[100] Alexander Hauptmann, Rong Yan, Yanjun Qi, Rong Jin, Michael G Christel, Mark\nDerthick, Ming-yu Chen, Robert Baron, W-H Lin, and Tobun D Ng. Video classiﬁca-\ntion and retrieval with the informedia digital video library system. 2002.\n[101] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for\nimage recognition. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 770–778, 2016.\n[102] Amy Herzog. Discordant visions: The peculiar musical images of the soundies jukebox\nﬁlm. American Music, 22(1):27–39, 2004.\n[103] Elad Hoffer and Nir Ailon. Deep metric learning using triplet network. In International\nWorkshop on Similarity-Based Pattern Recognition, pages 84–92. Springer, 2015.\n158\n[104] Sungeun Hong, Woobin Im, and Hyun S Yang. Deep learning for content-based, cross-\nmodal retrieval of videos and music. arXiv preprInternational arXiv:1704.06761, 2017.\n[105] Holger H Hoos, Kai Renz, and Marko Görg. Guido/mir-an experimental musical infor-\nmation retrieval system based on guido music notation. In ISMIR, pages 41–50, 2001.\n[106] Weiming Hu, Nianhua Xie, Li Li, Xianglin Zeng, and Stephen Maybank. A survey on\nvisual content-based video indexing and retrieval. IEEE Transactions on Systems, Man,\nand Cybernetics, Part C (Applications and Reviews), 41(6):797–819, 2011.\n[107] Xiao Hu and J Stephen Downie. Improving mood classiﬁcation in music digital libraries\nby combining lyrics and audio. In Proceedings of the 10th annual joInternational confer-\nence on Digital libraries, pages 159–168. ACM, 2010.\n[108] Xian-Sheng Hua, Lie Lu, and Hong-Jiang Zhang. Automatic music video generation\nbased on temporal pattern analysis. In Proceedings of the 12th annual ACM international\nconference on Multimedia. ACM, 2004.\n[109] Reinhold Huber-Mörk and Alexander Schindler. Quality assurance for document image\ncollections in digital preservation. In International Conference on Advanced Concepts\nfor Intelligent Vision Systems, pages 108–119. Springer, 2012.\n[110] Reinhold Huber-Mörk and Alexander Schindler. Automatic classiﬁcation of defect page\ncontent in scanned document collections. In 2013 8th International Symposium on Image\nand Signal Processing and Analysis (ISPA), pages 177–182. IEEE, 2013.\n[111] Reinhold Huber-Mörk and Alexander Schindler. An image based approach for content\nanalysis in document collections. In International Symposium on Visual Computing ,\npages 278–287. Springer, 2013.\n[112] Reinhold Huber-Mörk, Alexander Schindler, and Sven Schlarb. Duplicate detection for\nquality assurance of document image collections. Preservation of Digital Objects, page\n188, 2012.\n[113] Mark J Huiskes, Bart Thomee, and Michael S Lew. New trends and ideas in visual concept\ndetection: the mir ﬂickr retrieval evaluation initiative. In Proceedings of the international\nconference on Multimedia information retrieval, pages 527–536. ACM, 2010.\n[114] Charlie Inskip, Andy Macfarlane, and Pauline Rafferty. Music, movies and meaning:\nCommunication in ﬁlm-makers’ search for pre-existing music, and the implications for\nmusic information retrieval. In In Proceedings of 9th International Conference on Music\nInformation Retrieval, pages 477–482, Philadelphia, USA, September 14-18 2008.\n[115] Deniz Iren, Cynthia Liem, Jie Yang, and Alessandro Bozzon. Using social media to reveal\nsocial and collective perspectives on music. In Proceedings of the 8th ACM Conference\non Web Science, pages 296–300. ACM, 2016.\n159\n[116] Johannes Itten and Ernst Van Haagen. The art of color: the subjective experience and\nobjective rationale of color. Van Nostrand Reinhold New York, NY , USA, 1973.\n[117] Giridharan Iyengar and Andrew B Lippman. Models for automatic classiﬁcation of video\nsequences. In Storage and Retrieval for Image and Video Databases VI , volume 3312,\npages 216–228. International Society for Optics and Photonics, 1997.\n[118] Prateek Jain, Brian Kulis, Jason V Davis, and Inderjit S Dhillon. Metric and kernel learn-\ning using a linear transformation. Journal of Machine Learning Research, 13(Mar):519–\n547, 2012.\n[119] Anahid Jalali, Clemens Heistracher, Alexander Schindler, Bernhard Haslhofer, Tanja\nNemeth, Robert Glawar, Wilfried Sihn, and Peter De Boer. Predicting time-to-failure\nof plasma etching equipment using machine learning. arXiv preprint arXiv:1904.07686,\n2019.\n[120] Anahid N Jalali, Alexander Schindler, and Bernhard Haslhofer. Understandable deep\nneural networks for predictive maintenance in the manufacturing industry.ERCIM NEWS,\n(116):33–34, 2019.\n[121] Radu S Jasinschi and Jennifer Louie. Automatic tv program genre classiﬁcation based\non audio patterns. In Euromicro Conference, 2001. Proceedings. 27th, pages 370–375.\nIEEE, 2001.\n[122] Tristan Jehan and David DesRoches. Analyzer documentation (analyzer version 3.08).\nWebsite, 2011. Available online at http://developer.echonest.com/docs/\nv4/_static/AnalyzeDocumentation.pdf; visited on December 1th 2013.\n[123] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Gir-\nshick, Sergio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast\nfeature embedding. arXiv preprInternational arXiv:1408.5093, 2014.\n[124] Steve Jones. Cohesive but not coherent: Music videos, narrative and culture. Popular\nMusic & Society, 12(4):15–29, 1988.\n[125] Steve Jones and Martin Sorger. Covering music: A brief history and analysis of album\ncover design. Journal of Popular Music Studies, 11(1):68–102, 1999.\n[126] Michel Jubb, Alma Swan, and Sheridan Brown. To share or not to share. Publication and\nQuality Assurance of Research Data Outputs. Technical report, Research Information\nNetwork, 2008.\n[127] Patrik N Juslin and John Sloboda. Handbook of music and emotion: Theory, research,\napplications. Oxford University Press, 2012.\n[128] E Ann Kaplan. Rocking around the clock: Music television. Postmodernism, and Con-\nsumer Culture (New York), 1987.\n160\n[129] Henry Keazor and Thorsten Wübbena. Music video. 2010.\n[130] Wonjun Kim and Changick Kim. Automatic region of interest determination in music\nvideos. In In Proceedings of 41th Asilomar Conference on Signals, Systems and Comput-\ners, pages 485–489. IEEE, 2007.\n[131] Youngmoo E Kim, Erik M Schmidt, Raymond Migneco, Brandon G Morton, Patrick\nRichardson, Jeffrey Scott, Jacquelin A Speck, and Douglas Turnbull. Music emotion\nrecognition: A state of the art review. In In Proceedings of 11th International Society for\nMusic Information Retrieval Conference (ISMIR 2010), pages 255–266, 2010.\n[132] Youngmoo E Kim and Brian Whitman. Singer identiﬁcation in popular music recordings\nusing voice coding features. InProceedings of the 3rd International Conference on Music\nInformation Retrieval, volume 13, page 17, 2002.\n[133] Youngmoo E Kim, Donald S Williamson, and Sridhar Pilli. Towards quantifying the\nalbum effect in artist identiﬁcation. In7th International Conference on Music Information\nRetrieval (ISMIR 2006), volume 18, page 145, 2006.\n[134] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR,\nabs/1412.6980, 2014.\n[135] Peter Knees, Tim Pohle, Markus Schedl, Dominik Schnitzer, and Klaus Seyerlehner. A\ndocument-centered approach to a natural language music search engine. In European\nConference on Information Retrieval, pages 627–631. Springer, 2008.\n[136] Peter Knees, Tim Pohle, Markus Schedl, and Gerhard Widmer. A music search engine\nbuilt upon audio-based and web-based similarity measures. InProceedings of the 30th an-\nnual international ACM SIGIR conference on Research and development in information\nretrieval, pages 447–454. ACM, 2007.\n[137] Peter Knees and Markus Schedl. A survey of music similarity and recommendation from\nmusic context data. ACM Transactions on Multimedia Computing, Communications, and\nApplications (TOMM), 10(1):2, 2013.\n[138] Sander Koelstra, Christian Mühl, Mohammad Soleymani, Jong-Seok Lee, Ashkan Yaz-\ndani, Touradj Ebrahimi, Thierry Pun, Anton Nijholt, and Ioannis Patras. Deap: A database\nfor emotion analysis; using physiological signals. Affective Computing, IEEE Transac-\ntions on, 3(1):18–31, 2012.\n[139] Irena Koprinska and Sergio Carrato. Temporal video segmentation: A survey. Signal\nprocessing: Image communication, 16(5):477–500, 2001.\n[140] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with\ndeep convolutional neural networks. In Advances in neural information processing sys-\ntems, pages 1097–1105, 2012.\n161\n[141] Paul Lamere. Social tagging and music information retrieval. Journal of new music\nresearch, 37(2):101–114, 2008.\n[142] Olivier Lartillot and Petri Toiviainen. A matlab toolbox for musical feature extraction\nfrom audio. In International Conference on Digital Audio Effects, pages 237–244, 2007.\n[143] Cyril Laurier, Jens Grivolla, and Perfecto Herrera. Multimodal music mood classiﬁcation\nusing audio and lyrics. InIn Proceedings 7th International Conference Machine Learning\nand Applications, pages 688–693. IEEE, 2008.\n[144] Yann LeCun, John S Denker, Sara A Solla, Richard E Howard, and Lawrence D Jackel.\nOptimal brain damage. In NIPs, volume 2, pages 598–605, 1989.\n[145] Jin Ha Lee, Kahyun Choi, Xiao Hu, and J Stephen Downie. K-pop genres: A cross-\ncultural exploration. In ISMIR, pages 529–534, 2013.\n[146] Jin Ha Lee, J Stephen Downie, and Sally Jo Cunningham. Challenges in cross-\ncultural/multilingual music information seeking. In ISMIR, pages 1–7, 2005.\n[147] Jin Ha Lee and Nichole Maiman Waterman. Understanding user requirements for music\ninformation services. In In Proceedings of 13th International Society for Music Informa-\ntion Retrieval Conference (ISMIR 2012), Porto, Portugal, October 8-12 2012.\n[148] Michael S Lew, Nicu Sebe, Chabane Djeraba, and Ramesh Jain. Content-based multime-\ndia information retrieval: State of the art and challenges. ACM Transactions on Multime-\ndia Computing, Communications, and Applications (TOMCCAP), 2(1):1–19, 2006.\n[149] Stan Z Li. Handbook of face recognition. Springerverlag London Limited, 2011.\n[150] J. Libeks and D. Turnbull. You can judge an artist by an album cover: Using images for\nmusic annotation. MultiMedia, IEEE, 18(4):30–37, April 2011.\n[151] Janis Lıbeks and Douglas Turnbull. Exploring “artist image” using content-based analysis\nof promotional photos. In In Proceedings of International Computer Music Conference,\n2010.\n[152] Thomas Lidy, Rudolf Mayer, Andreas Rauber, A Pertusa, and J M I. A Cartesian Ensem-\nble of Feature Subspace Classiﬁers for Music Categorization. In Proceedings of the 11th\nInternational Conference on Music Information Retrieval (ISMIR 2010), 2010.\n[153] Thomas Lidy and Andreas Rauber. Evaluation of feature extractors and psycho-acoustic\ntransformations for music genre classiﬁcation. In In Proceedings of international society\nfor music information retrieval conference (ISMIR 2005), 2005.\n[154] Thomas Lidy and Andreas Rauber. Evaluation of feature extractors and psycho-acoustic\ntransformations for music genre classiﬁcation. In In Proceedings International Confer-\nence Music Infromation Retrieval, pages 34–41, 2005.\n162\n[155] Thomas Lidy, Andreas Rauber, Antonio Pertusa, Pedro J. Ponce de León, and Josó Iñesta.\nAudio music classiﬁcation using a combination of spectral, timbral, rhythmic, temporal\nand symbolic features.\n[156] Thomas Lidy, Andreas Rauber, Antonio Pertusa, and José Manuel Iñesta. Improving\ngenre classiﬁcation by combination of audio and symbolic descriptors using a transcrip-\ntion system. In In Proceedings International Conference Music Information Retrieval ,\npages 61–66, 2007.\n[157] Thomas Lidy, Andreas Rauber, Antonio Pertusa, and José Manuel Iñesta Quereda. Im-\nproving genre classiﬁcation by combination of audio and symbolic descriptors using a\ntranscription systems. In In Proceedings International Conference Music Information\nRetrieval, pages 61–66, 2007.\n[158] Thomas Lidy and Alexander Schindler. CQT-based convolutional neural networks for\naudio scene classiﬁcation. In Proceedings of the Detection and Classiﬁcation of Acoustic\nScenes and Events 2016 Workshop (DCASE2016), pages 60–64, September 2016.\n[159] Thomas Lidy and Alexander Schindler. Cqt-based convolutional neural networks for\naudio scene classiﬁcation and domestic audio tagging. In Detection and Classiﬁcation of\nAcoustic Scenes and Events 2016 Challenge (DCASE2016), 2016.\n[160] Thomas Lidy and Alexander Schindler. Parallel convolutional neural networks for music\ngenre and mood classiﬁcation. In Music Information Retireval Evaluation EXchange\n(MIREX 2016), 2016.\n[161] Thomas Lidy and Alexander Schindler. Parallel convolutional neural networks for music\ngenre and mood classiﬁcation. Technical report, Music Information Retrieval Evaluation\neXchange (MIREX 2016), August 2016.\n[162] Thomas Lidy, Alexander Schindler, and Michela Magas. Musicbricks: Connecting digital\ncreators to the internet of music things. ERCIM NEWS, 101:39–40, 2015.\n[163] Thomas Lidy, Carlos N. Silla, Olmo Cornelis, Fabien Gouyon, Andreas Rauber, Celso\nA. A. Kaestner, and Alessandro L. Koerich. On the suitability of state-of-the-art music\ninformation retrieval methods for analyzing, categorizing, structuring and accessing non-\nwestern and ethnic music collections. Signal Processing, 90(4):1032–1048, 2010.\n[164] Thomas Lidy and Pieter van der Linden. Report on 3rd chorus+ think-tank: Think-tank\non the future of music search, access and consumption. Technical report, MIDEM 2011.\nTech report, CHORUS+ EU Coord Action on Audiovisual Search, Cannes, France, 2011.\n[165] Cynthia Liem, Meinard Müller, Douglas Eck, George Tzanetakis, and Alan Hanjalic. The\nneed for music information retrieval with user-centered and multimodal strategies. In\nIn Proceedings of 1st international ACM workshop on Music information retrieval with\nuser-centered and multimodal strategies, pages 1–6. ACM, 2011.\n163\n[166] Rainer Lienhart and Jochen Maydt. An extended set of haar-like features for rapid object\ndetection. In International Conference Image Processing, pages I–900. IEEE, 2002.\n[167] Rainer W Lienhart. Reliable dissolve detection. In Storage and Retrieval for Media\nDatabases 2001, volume 4315, pages 219–231. International Society for Optics and Pho-\ntonics, 2001.\n[168] Stefaan Lippens, Jean-Pierre Martens, and Tom De Mulder. A comparison of human\nand automatic musical genre classiﬁcation. In Acoustics, Speech, and Signal Processing,\n2004. Proceedings.(ICASSP’04). IEEE International Conference on, volume 4, pages iv–\niv. IEEE, 2004.\n[169] Fang Liu and Rosalind W Picard. Periodicity, directionality, and randomness: Wold\nfeatures for image modeling and retrieval. IEEE transactions on pattern analysis and\nmachine intelligence, 18(7):722–733, 1996.\n[170] Ying Liu, Dengsheng Zhang, Guojun Lu, and Wei-Ying Ma. A survey of content-based\nimage retrieval with high-level semantics. Pattern Recognition, 40(1):262–282, 2007.\n[171] Beth Logan, Daniel PW Ellis, and Adam Berenzweig. Toward evaluation techniques for\nmusic similarity. “The MIR/MDL Evaluation Project White Paper Collection” Edition#\n3, page 81, 2003.\n[172] Beth Logan et al. Mel frequency cepstral coefﬁcients for music modeling. In ISMIR,\n2000.\n[173] Beth Logan and Ariel Salomon. A music similarity function based on signal analysis. In\nICME, 2001.\n[174] David G Lowe. Distinctive image features from scale-invariant keypoints. International\njournal of computer vision, 60(2):91–110, 2004.\n[175] Wei-Ying Ma and BS Manjunath. Edge ﬂow: a framework of boundary detection and\nimage segmentation. In Computer Vision and Pattern Recognition, 1997. Proceedings.,\n1997 IEEE Computer Society Conference on, pages 744–749. IEEE, 1997.\n[176] Andrew L. Maas, Awni Y . Hannun, and Andrew Y . Ng. Rectiﬁer nonlinearities improve\nneural network acoustic models. ICML 2013, 28, 2013.\n[177] Jana Machajdik and Allan Hanbury. Affective image classiﬁcation using features inspired\nby psychology and art theory. In Proceedings of the international conference on Multi-\nmedia, pages 83–92. ACM, 2010.\n[178] Michael I Mandel and Daniel PW Ellis. Song-level features and support vector machines\nfor music classiﬁcation. In 6th International Conference on Music Information Retrieval\n(ISMIR 2005), pages 594–599, London, UK, 2005.\n164\n[179] Bangalore S Manjunath, J-R Ohm, Vinod V Vasudevan, and Akio Yamada. Color and\ntexture descriptors. Circuits and Systems for Video Technology, IEEE Transactions on ,\n11(6):703–715, 2001.\n[180] Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, and Alan L Yuille. Explain images with\nmultimodal recurrent neural networks. arXiv preprInternational arXiv:1410.1090, 2014.\n[181] Alison Mattek and Michael Casey. Cross-modal aesthetics from a feature extraction per-\nspective: A pilot study. In ISMIR, 2011.\n[182] Rudolf Mayer. Analysing the similarity of album art with self-organising maps. In Jorma\nLaaksonen and Timo Honkela, editors, Advances in Self-Organizing Maps, volume 6731\nof LNCS, pages 357–366. Springer, 2011.\n[183] Rudolf Mayer, Robert Neumayer, and Andreas Rauber. Rhyme and style features for\nmusical genre classiﬁcation by song lyrics. In In Proceedings International Conference\nMusic Information Retrieval, pages 337–342, 2008.\n[184] Brian McFee, Colin Raffel, Dawen Liang, Daniel PW Ellis, Matt McVicar, Eric Batten-\nberg, and Oriol Nieto. librosa: Audio and music signal analysis in python. InProceedings\nof the 14th python in science conference, 2015.\n[185] Cory Mckay. Automatic Music Classiﬁcation with jMIR. PhD thesis, McGill University,\nCanada, 2010.\n[186] Cory McKay and Ichiro Fujinaga. Musical genre classiﬁcation: Is it worth pursuing and\nhow can it be improved? In ISMIR, pages 101–106, 2006.\n[187] Cory McKay and Ichiro Fujinaga. jMIR: Tools for automatic music classiﬁcation. In\nProceedings of the International Computer Music Conference, pages 65–8, 2009.\n[188] Cory McKay, Daniel McEnnis, and Ichiro Fujinaga. A large publicly accassible prototype\naudio database for music research. In ISMIR, pages 160–163, 2006.\n[189] Martin F McKinney, Dirk Moelants, Matthew EP Davies, and A Klapuri. Evaluation\nof audio beat tracking and music tempo extraction algorithms. Jour. New Music Res. ,\n36(1):1–16, 2007.\n[190] Annamaria Mesaros, Tuomas Virtanen, and Anssi Klapuri. Singer identiﬁcation in poly-\nphonic music using vocal separation and pattern recognition methods. In8th International\nConference on Music Information Retrieval (ISMIR 2007), pages 375–378, 2007.\n[191] Leonard B. Meyer. Emotion and meaning in music. 1956.\n[192] Krystian Mikolajczyk and Cordelia Schmid. A performance evaluation of local descrip-\ntors. IEEE transactions on pattern analysis and machine intelligence, 27(10):1615–1630,\n2005.\n165\n[193] George A Miller. Wordnet: a lexical database for english. Communications of the ACM,\n38(11):39–41, 1995.\n[194] Riccardo Miotto and Nicola Orio. A music identiﬁcation system based on chroma index-\ning and statistical modeling. In ISMIR, 2008.\n[195] Partha Pratim Mohanta, Sanjoy Kumar Saha, and Bhabatosh Chanda. A model-based\nshot boundary detection technique using frame transition parameters. IEEE Transactions\non multimedia, 14(1):223–233, 2012.\n[196] Simon Moncrieff, Svetha Venkatesh, and Chitra Dorai. Horror ﬁlm genre typing and\nscene labeling via audio analysis. InIn Proceedings International Conference Multimedia\nand Expo, volume 2, pages II–193. IEEE, 2003.\n[197] Julie Holland Mortimer, Chris Nosko, and Alan Sorensen. Supply responses to digital\ndistribution: Recorded music and live performances. Information Economics and Policy,\n24(1):3–14, 2012.\n[198] Meinard Mueller, Andreas Arzt, Stefan Balke, Matthias Dorfer, and Gerhard Widmer.\nCross-modal music retrieval and applications: An overview of key methodologies. IEEE\nSignal Processing Magazine, 36(1):52–62, 2019.\n[199] Keith Negus. Producing pop: Culture and conﬂict in the popular music industry . out of\nprint, 2011.\n[200] Tanja Nemeth, Fazel Ansari, Wilfried Sihn, Bernhard Haslhofer, and Alexander\nSchindler. Prima-x: A reference model for realizing prescriptive maintenance and assess-\ning its maturity enhanced by machine learning. Procedia CIRP, 72:1039–1044, 2018.\n[201] Nielsen. Digital music consumption and digital music access, 2011.\n[202] Bureau of the Census and United States. Statistical abstract of the United States . US\nGovernment Printing Ofﬁce, 2009.\n[203] Timo Ojala, Matti Pietikainen, and Topi Maenpaa. Multiresolution gray-scale and rotation\ninvariant texture classiﬁcation with local binary patterns. IEEE Transactions on pattern\nanalysis and machine intelligence, 24(7):971–987, 2002.\n[204] Nicola Orio, Cynthia CS Liem, Geoffroy Peeters, and Markus Schedl. Musiclef: mul-\ntimodal music tagging task. In Information Access Evaluation. Multilinguality, Multi-\nmodality, and Visual Analytics. 2012.\n[205] Francois Pachet and Jean-Julien Aucouturier. Improving timbre similarity: How high is\nthe sky? Journal of negative results in speech and audio sciences, 1(1):1–13, 2004.\n[206] François Pachet and Daniel Cazaly. A taxonomy of musical genres. In Content-Based\nMultimedia Information Access-Volume 2, pages 1238–1245. LE CENTRE DE HAUTES\nETUDES INTERNATIONALES D’INFORMATIQUE DOCUMENTAIRE, 2000.\n166\n[207] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. Knowledge and Data\nEngineering, IEEE Transactions on, 22(10):1345–1359, 2010.\n[208] Michael S Pedersen, Jan Larsen, Ulrik Kjems, and Lucas C Parra. A survey of convolu-\ntive blind source separation methods. Multichannel Speech Processing Handbook, pages\n1065–1084, 2007.\n[209] P Jonathon Phillips, Patrick J Flynn, Todd Scruggs, Kevin W Bowyer, Jin Chang, Kevin\nHoffman, Joe Marques, Jaesik Min, and William Worek. Overview of the face recognition\ngrand challenge. In Computer vision and pattern recognition (CVPR 2005) , pages 947–\n954. IEEE, 2005.\n[210] Erika Piola. The rise of early american lithography and antebellum visual culture. Win-\nterthur Portfolio, 48(2/3):125–138, 2014.\n[211] Stephen M Pizer, E Philip Amburn, John D Austin, Robert Cromartie, Ari Geselowitz,\nTrey Greer, Bart ter Haar Romeny, John B Zimmerman, and Karel Zuiderveld. Adaptive\nhistogram equalization and its variations. Computer vision, graphics, and image process-\ning, 39(3):355–368, 1987.\n[212] Konstantinos N Plataniotis and Anastasios N Venetsanopoulos. Color image processing\nand applications. Springer, 2000.\n[213] Tim Pohle, Elias Pampalk, and Gerhard Widmer. Evaluation of frequently used audio\nfeatures for classiﬁcation of music into perceptual categories.\n[214] Jordi Pons, Thomas Lidy, and Xavier Serra. Experimenting with musically motivated\nconvolutional neural networks. In Content-Based Multimedia Indexing (CBMI), 2016\n14th International Workshop on, pages 1–6. IEEE, 2016.\n[215] Jordi Pons, Thomas Lidy, and Xavier Serra. Experimenting with musically motivated\nconvolutional neural networks. In Proceedings of the 14th International Workshop on\nContent-based Multimedia Indexing (CBMI 2016), Bucharest, Romania, June 2016.\n[216] Jordi Pons and Xavier Serra. Designing efﬁcient architectures for modeling temporal fea-\ntures with convolutional neural networks. InIEEE International Conference on Acoustics,\nSpeech, and Signal Processing, 2017.\n[217] Jordi Pons, Olga Slizovskaia, Rong Gong, Emilia Gómez, and Xavier Serra. Timbre anal-\nysis of music audio signals with convolutional neural networks. arXiv preprInternational\narXiv:1703.06697, 2017.\n[218] Georges M Qu ´senot, Daniel Moraru, and Laurent Besacier. Clips at trecvid: Shot bound-\nary detection and feature detection. 2003.\n[219] Lawrence Rabiner and Biing-Hwang Juang. Fundamentals of Speech Recognition. Pren-\ntice Hall, 1993.\n167\n[220] Mehwish Rehman, Muhammad Iqbal, Muhammad Sharif, and Mudassar Raza. Content\nbased image retrieval: Survey. World Applied Sciences Journal, 19(3):404–412, 2012.\n[221] Matthew Roach and John SD Mason. Classiﬁcation of video genre using audio. In\nINTERSPEECH, pages 2693–2696, 2001.\n[222] Joseph Rothstein. MIDI: A comprehensive introduction , volume 7. AR Editions, Inc.,\n1995.\n[223] Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover’s distance as a\nmetric for image retrieval.International Journal of Computer Vision, 40(2):99–121, 2000.\n[224] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhi-\nheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg,\nand Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Jour-\nnal of Computer Vision (IJCV), 2015.\n[225] JA Russell. A circumspect model of affect, 1980. J Psychol Soc Psychol , 39(6):1161,\n1980.\n[226] Shoto Sasaki, Tatsunori Hirai, Hayato Ohya, and Shigeo Morishima. Affective music rec-\nommendation system based on the mood of input video. volume 8936 of LNCS. Springer\nInternational Publishing, 2015.\n[227] Nicolas Scaringella and Giorgio Zoia. On the modeling of time information for automatic\ngenre recognition systems in audio signals. In ISMIR, pages 666–671, 2005.\n[228] Nicolas Scaringella, Giorgio Zoia, and Daniel Mlynek. Automatic genre classiﬁcation of\nmusic content: a survey. Signal Processing Magazine, IEEE, 23(2):133–141, 2006.\n[229] Markus Schedl, David Hauger, and Julián Urbano. Harvesting microblogs for contex-\ntual music similarity estimation: a co-occurrence-based framework. Multimedia Systems,\n20(6):693–705, 2014.\n[230] Markus Schedl, Peter Knees, Brian McFee, Dmitry Bogdanov, and Marius Kaminskas.\nMusic recommender systems. In Recommender Systems Handbook , pages 453–492.\nSpringer, 2015.\n[231] Markus Schedl, Peter Knees, and Gerhard Widmer. A web-based approach to assessing\nartist similarity using co-occurrences. In Proceedings of the Fourth International Work-\nshop on Content-Based Multimedia Indexing (CBMI’05), 2005.\n[232] Markus Schedl, Tim Pohle, Peter Knees, and Gerhard Widmer. Assigning and visualizing\nmusic genres by web-based co-occurrence analysis. In ISMIR, pages 260–265. Citeseer,\n2006.\n168\n[233] Andrew I Schein, Alexandrin Popescul, Lyle H Ungar, and David M Pennock. Methods\nand metrics for cold-start recommendations. In Proceedings of the 25th annual inter-\nnational ACM SIGIR conference on Research and development in information retrieval ,\npages 253–260. ACM, 2002.\n[234] Raimondo Schettini, Gianluigi Ciocca, Silvia Zufﬁ, et al. A survey of methods for colour\nimage indexing and retrieval in image databases. Color Imaging Science: Exploiting\nDigital Media, 2001.\n[235] Alexander Schindler. Million song dataset integration into the clubmixer framework.\nIn 10th International Conference on Music Information Retrieval - late breaking demo\n(ISMIR 2011).\n[236] Alexander Schindler. A picture is worth a thousand songs: Exploring visual aspects\nof music. In Proceedings of the 1st International Workshop on Digital Libraries for\nMusicology, DLfM ’14, 2014.\n[237] Alexander Schindler, Martin Boyer, Andrew Lindley, David Schreiber, and Thomas\nPhilipp. Large scale audio-visual video analytics platform for forensic investigations of\nterroristic attacks. In International Conference on Multimedia Modeling, pages 106–119.\nSpringer, 2019.\n[238] Alexander Schindler, Sergiu Gordea, and Harry van Biessum. The europeana sounds\nmusic information retrieval pilot. In Euro-Mediterranean Conference, pages 109–117.\nSpringer, 2016.\n[239] Alexander Schindler and Reinhold Huber-Mörk. Towards objective quality assessment in\ndigital collections.\n[240] Alexander Schindler and Peter Knees. Multi-task music representation learning from\nmulti-label embeddings. In Proceedings of the 17th International Workshop on Content-\nbased Multimedia Indexing (CBMI 2019), Dublin, Ireland, September 2019.\n[241] Alexander Schindler, Thomas Lidy, Stephan Karner, and Matthias Hecker. Fash-\nion and apparel classiﬁcation using convolutional neural networks. arXiv preprint\narXiv:1811.04374, 2018.\n[242] Alexander Schindler, Thomas Lidy, and Andreas Rauber. Multi-temporal resolution con-\nvolutional neural networks for the dcase acoustic scene classiﬁcation task.\n[243] Alexander Schindler, Thomas Lidy, and Andreas Rauber. Comparing shallow versus deep\nneural network architectures for automatic music genre classiﬁcation. In9th Forum Media\nTechnology (FMT2016), volume 1734, pages 17–21. CEUR, 2016.\n[244] Alexander Schindler, Thomas Lidy, and Andreas Rauber. Comparing shallow versus deep\nneural network architectures for automatic music genre classiﬁcation. In Proceedings of\nthe 9th Forum Media Technology (FMT2016), 2016.\n169\n[245] Alexander Schindler, Thomas Lidy, and Andreas Rauber. Multi-temporal resolu-\ntion convolutional neural networks for acoustic scene classiﬁcation. arXiv preprint\narXiv:1811.04419, 2018.\n[246] Alexander Schindler, Rudolf Mayer, and Andreas Rauber. Facilitating comprehensive\nbenchmarking experiments on the million song dataset. In Proceedings of the 13th Inter-\nnational Conference on Music Information Retrieval (ISMIR 2012), 2012.\n[247] Alexander Schindler and Andreas Rauber. Clubmixer: A presentation platform for mir\nprojects. In Marcin Detyniecki, Peter Knees, Andreas Nürnberger, Markus Schedl, and\nSebastian Stober, editors, Adaptive Multimedia Retrieval. Context, Exploration, and Fu-\nsion, pages 132–143, Berlin, Heidelberg, 2011. Springer Berlin Heidelberg.\n[248] Alexander Schindler and Andreas Rauber. A music video information retrieval approach\nto artist identiﬁcation. In Proceedings of the 10th International Symposium on Computer\nMusic Multidisciplinary Research (CMMR2013) to appear , Marseille, France, October\n14-18 2013.\n[249] Alexander Schindler and Andreas Rauber. Capturing the Temporal Domain in Echonest\nFeatures for Improved Classiﬁcation Effectiveness, pages 214–227. Springer International\nPublishing, Cham, 2014.\n[250] Alexander Schindler and Andreas Rauber. An audio-visual approach to music genre clas-\nsiﬁcation through affective color features. In Advances in Information Retrieval, volume\n9022 of LNCS, pages 61–67. 2015.\n[251] Alexander Schindler and Andreas Rauber. An audio-visual approach to music genre clas-\nsiﬁcation through affective color features. In Proceedings of the 37th European Confer-\nence on Information Retrieval (ECIR’15), Vienna, Austria, March 29 - April 02 2015.\n[252] Alexander Schindler and Andreas Rauber. Harnessing music-related visual stereotypes\nfor music information retrieval.ACM Transactions Intell. Syst. Technol., 8(2):20:1–20:21,\nOctober 2016.\n[253] Alexander Schindler and Andreas Rauber. On the unsolved problem of shot boundary\ndetection for music videos. In Proceedings of the 25th International Conference on Mul-\ntiMedia Modeling (MMM 2019), 2019.\n[254] Alexander Schindler and Sven Schlarb. Contextualised conversational systems. ERCIM\nNEWS, (114):13–14, 2018.\n[255] Sven Schlarb, Peter Cliff, Peter May, William Palmer, Matthias Hahn, Reinhold Huber-\nMoerk, Alexander Schindler, Rainer Schmidt, and Johan van der Knijff. Quality assured\nimage ﬁle format migration in large digital object repositories. In 10th International\nConference on Preservation of Digital Objects, page 300, 2013.\n[256] Hendrik Schreiber. Improving genre annotations for the million song dataset. In ISMIR,\npages 241–247, 2015.\n170\n[257] Xavier Serra, Michela Magas, Emmanouil Benetos, Magdalena Chudy, S. Dixon, Arthur\nFlexer, Emilia Gómez, F. Gouyon, P. Herrera, S. Jordà, Oscar Paytuvi, G. Peeters, Jan\nSchlüter, H. Vinet, and G. Widmer. Roadmap for Music Information ReSearch. 2013.\n[258] Ishwar K Sethi, Ioana L Coman, and Daniela Stan. Mining association rules between low-\nlevel image features and high-level concepts. In Aerospace/Defense Sensing, Simulation,\nand Controls, pages 279–290. International Society for Optics and Photonics, 2001.\n[259] Klaus Seyerlehner and Markus Schedl. Block-level audio feature for music genre classi-\nﬁcation. online In Proceedings of the 5th Annual Music Information Retrieval Evaluation\neXchange (MIREX-09), 2009.\n[260] Klaus Seyerlehner, Markus Schedl, Tim Pohle, and Peter Knees. Using block-level fea-\ntures for genre classiﬁcation, tag classiﬁcation and music similarity estimation. Submis-\nsion to Audio Music Similarity and Retrieval Task of MIREX, 2010, 2010.\n[261] Ben J Shannon and Kuldip K Paliwal. A comparative study of ﬁlter bank spacing for\nspeech recognition. In Microelectronic engineering research conference , volume 41,\n2003.\n[262] Xi Shao, Changsheng Xu, Namunu C Maddage, Qi Tian, Mohan S Kankanhalli, and\nJesse S Jin. Automatic summarization of music videos.ACM Transactions on Multimedia\nComputing, Communications, and Applications (TOMCCAP), 2(2):127–148, 2006.\n[263] Jialie Shen, John Shepherd, Bin Cui, and Kian-Lee Tan. A novel framework for efﬁcient\nautomated singer identiﬁcation in large music databases. ACM Transactions on Informa-\ntion Systems (TOIS), 27(3):18, 2009.\n[264] Mark Shevy. Music genre as cognitive schema: extramusical associations with country\nand hip-hop music. Psychology of music, 36(4):477–498, 2008.\n[265] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. IEEE Transac-\ntions on pattern analysis and machine intelligence, 22(8):888–905, 2000.\n[266] Kai Siedenburg, Ichiro Fujinaga, and Stephen McAdams. A comparison of approaches to\ntimbre descriptors in music information retrieval and music psychology. Journal of New\nMusic Research, 2016.\n[267] C.N. Silla Jr, A.L. Koerich, P. Catholic, and C.A.A. Kaestner. The latin music database.\nIn Proceedings of the 9th International Conference of Music Information Retrieval, page\n451. Lulu. com, 2008.\n[268] Nidhi Singhai and Shishir K Shandilya. A survey on: content based image retrieval\nsystems.\n[269] Alan F Smeaton, Paul Over, and Aiden R Doherty. Video shot boundary detection: Seven\nyears of trecvid activity. Computer Vision and Image Understanding , 114(4):411–418,\n2010.\n171\n[270] Cees GM Snoek and Marcel Worring. Multimodal video indexing: A review of the state-\nof-the-art. Multimedia tools and applications, 25(1):5–35, 2005.\n[271] Yading Song, Simon Dixon, and Marcus Pearce. A survey of music recommendation\nsystems and future perspectives. In 9th International Symposium on Computer Music\nModeling and Retrieval, 2012.\n[272] Johannes Stallkamp, Hazım K Ekenel, and Rainer Stiefelhagen. Video-based face recog-\nnition on real-world data. InComputer Vision, 2007. ICCV 2007. IEEE 11th International\nConference on, pages 1–8. IEEE, 2007.\n[273] Dan Stowell and Simon Dixon. Mir in school? lessons from ethnographic observation\nof secondary school music classes. In In Proceedings of 12th International Society for\nMusic Information Retrieval Conference (ISMIR 2012), pages 347–352, Miami (Florida),\nUSA, October 24-28 2011.\n[274] Bob L Sturm. An analysis of the gtzan music genre dataset. In Proceedings of the\nsecond international ACM workshop on Music information retrieval with user-centered\nand multimodal strategies, pages 7–12. ACM, 2012.\n[275] Bob L Sturm. A survey of evaluation in music genre recognition. In Adaptive Multimedia\nRetrieval, 2012.\n[276] Bob L Sturm. Classiﬁcation accuracy is not enough. Journal of Intelligent Information\nSystems, 2013.\n[277] Bob L Sturm. A simple method to determine if a music information retrieval system is a\n“horse”. Multimedia, IEEE Transactions on, 16(6):1636–1644, 2014.\n[278] Bob L Sturm. The state of the art ten years after a state of the art: Future research in\nmusic information retrieval. Journal of New Music Research, 43(2):147–172, 2014.\n[279] Bob L Sturm and Nick Collins. The kiki-bouba challenge: Algorithmic composition\nfor content-based mir research & development. In International Symposium on Music\nInformation Retrieval, 2014.\n[280] Se-Wen Sun and James Lull. The adolescent audience for music videos and why they\nwatch. Journal of Communication, 36(1):115–125, 1986.\n[281] Christian Szegedy, Sergey Ioffe, and Vincent Vanhoucke. Inception-v4, inception-resnet\nand the impact of residual connections on learning. CoRR, abs/1602.07261, 2016.\n[282] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir\nAnguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper\nwith convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pat-\ntern Recognition, pages 1–9, 2015.\n172\n[283] Hideyuki Tamura, Shunji Mori, and Takashi Yamawaki. Textural features corresponding\nto visual perception. IEEE Transactions on Systems, Man, and Cybernetics , 8(6):460–\n473, 1978.\n[284] Hideyuki Tamura and Naokazu Yokoya. Image database systems: A survey. Pattern\nrecognition, 17(1):29–43, 1984.\n[285] Ba Tu Truong and Chitra Dorai. Automatic genre identiﬁcation for content-based video\ncategorization. In Pattern Recognition, 2000. Proceedings. 15th International Conference\non, volume 4, pages 230–233. IEEE, 2000.\n[286] Ba Tu Truong, Chitra Dorai, and Svetha Venkatesh. New enhancements to cut, fade, and\ndissolve detection processes in video segmentation. In Proceedings of the eighth ACM\ninternational conference on Multimedia, pages 219–227. ACM, 2000.\n[287] Wei-Ho Tsai and Hsin-Min Wang. Automatic singer recognition of popular music record-\nings via estimation and modeling of solo vocal signals. IEEE Transactions on Audio,\nSpeech, and Language Processing, 14(1):330–341, 2006.\n[288] Kosetsu Tsukuda and Masataka Goto. Exploratoryvideosearch: A music video search\nsystem based on coordinate terms and diversiﬁcation. In Multimedia (ISM), 2015 IEEE\nInternational Symposium on, pages 221–224. IEEE, 2015.\n[289] Matthew Turk and Alex Pentland. Eigenfaces for recognition. Journal of cognitive neu-\nroscience, 3(1):71–86, 1991.\n[290] G. Tzanetakis. Manipulation, analysis and retrieval systems for audio signals . PhD\nthesis, 2002.\n[291] George Tzanetakis and Perry Cook. Marsyas: a framework for audio analysis. Organized\nSound, 4(3):169–175, December 1999.\n[292] George Tzanetakis and Perry Cook. Marsyas: A framework for audio analysis. Organised\nsound, 4(3):169–175, 2000.\n[293] George Tzanetakis and Perry Cook. Musical genre classiﬁcation of audio signals. IEEE\ntransactions on Speech and Audio Processing, 10(5):293–302, 2002.\n[294] Julián Urbano, Jorge Morato, Mónica Marrero, and Diego Martín. Crowdsourcing pref-\nerence judgments for evaluation of music similarity tasks. In ACM SIGIR workshop on\ncrowdsourcing for search evaluation, pages 9–16, 2010.\n[295] Julián Urbano, Markus Schedl, and Xavier Serra. Evaluation in music information re-\ntrieval. Journal of Intelligent Information Systems, 41(3):345–369, 2013.\n[296] Patricia Valdez and Albert Mehrabian. Effects of color on emotions. Journal of Experi-\nmental Psychology: General, 123(4):394, 1994.\n173\n[297] Andrea Vedaldi and Stefano Soatto. Quick shift and kernel methods for mode seeking. In\nComputer Vision–ECCV 2008, pages 705–718. Springer, 2008.\n[298] Remco C Veltkamp and Mirela Tanase. Content-based image retrieval systems: A survey.\n2001.\n[299] Carol Vernallis. Experiencing music video: Aesthetics and cultural context . Columbia\nUniversity Press, 2004.\n[300] Paul Viola and Michael Jones. Rapid object detection using a boosted cascade of simple\nfeatures. In IEEE Computer Society Conference on Computer Vision and Pattern Recog-\nnition (CVPR 2001), volume 1, pages I–511. IEEE, 2001.\n[301] Chris Walshaw et al. A statistical analysis of the abc music notation corpus: exploring\nduplication. 2014.\n[302] Huafeng Wang, Yunhong Wang, and Yuan Cao. Video-based face recognition: A survey.\nWorld Academy of Science, Engineering and Technology, 60:293–302, 2009.\n[303] Peng Wang, Rui Cai, and Shi-Qiang Yang. A hybrid approach to news video classiﬁcation\nmultimodal features. In Information, Communications and Signal Processing, 2003 and\nFourth Paciﬁc Rim Conference on Multimedia. Proceedings of the 2003 JoInternational\nConference of the Fourth International Conference on, volume 2, pages 787–791. IEEE,\n2003.\n[304] Wang Wei-ning, Yu Ying-lin, and Jiang Sheng-ming. Image retrieval by emotional se-\nmantics: A study of emotional space and feature extraction. In International Conference\non Systems, Man and Cybernetics. IEEE, 2006.\n[305] Felix Weninger, Björn Schuller, Cynthia Liem, Frank Kurth, and Alan Hanjalic. Music\ninformation retrieval: An inspirational guide to transfer from related disciplines.Dagstuhl\nFollow-Ups, 3, 2012.\n[306] Brian Whitman and Dan Ellis. Automatic record reviews. In ISMIR, volume 4, pages\n470–477, 2004.\n[307] Brian Whitman and Paris Smaragdis. Combining musical and cultural features for intel-\nligent style detection. In ISMIR, 2002.\n[308] GeraInternational A Wiggins. Semantic gap?? schemantic schmap!! methodological\nconsiderations in the scientiﬁc study of music. In Multimedia, 2009. ISM’09. 11th IEEE\nInternational Symposium on, pages 477–482. IEEE, 2009.\n[309] Ian H. Witten, Eibe Frank, Len Trigg, Mark Hall, Geoffrey Holmes, and Sally Jo Cun-\nningham. Weka: Practical Machine Learning Tools and Techniques with Java Implemen-\ntations, 1999.\n174\n[310] Dingyuan Xia, Xuefei Deng, and Qingning Zeng. Shot boundary detection based on\ndifference sequences of mutual information. In Image and Graphics, 2007. ICIG 2007.\nFourth International Conference on, pages 389–394. IEEE, 2007.\n[311] Yi-Hsuan Yang and Homer H Chen. Music emotion recognition. CRC Press, 2011.\n[312] Ashkan Yazdani, Krista Kappeler, and Touradj Ebrahimi. Affective content analysis of\nmusic video clips. In In Proceedings of 1st international ACM workshop on Music infor-\nmation retrieval with user-centered and multimodal strategies, pages 7–12. ACM, 2011.\n[313] Dong Yi, Zhen Lei, and Stan Z Li. Shared representation learning for heterogenous\nface recognition. In Automatic Face and Gesture Recognition (FG), 2015 11th IEEE\nInternational Conference and Workshops on, volume 1, pages 1–7. IEEE, 2015.\n[314] Jong-Chul Yoon, In-Kwon Lee, and Siwoo Byun. Automated music video generation\nusing multi-level feature-based segmentation. In Handbook of Multimedia for Digital\nEntertainment and Arts. Springer, 2009.\n[315] Jinhui Yuan, Huiyi Wang, Lan Xiao, Wujie Zheng, Jianmin Li, Fuzong Lin, and\nBo Zhang. A formal study of shot boundary detection. IEEE transactions on circuits\nand systems for video technology, 17(2):168–186, 2007.\n[316] Shiliang Zhang, Qingming Huang, Shuqiang Jiang, Wen Gao, and Qi Tian. Affective vi-\nsualization and retrieval for music video. Multimedia, IEEE Transactions on, 12(6):510–\n522, 2010.\n[317] Wenyi Zhao, Rama Chellappa, P Jonathon Phillips, and Azriel Rosenfeld. Face recogni-\ntion: A literature survey. ACM Computing Survey, 35(4):399–458, 2003.\n[318] Zhi-Cheng Zhao, Xing Zeng, Tao Liu, and An-Ni Cai. Bupt at trecvid 2007: Shot bound-\nary detection. In TRECVID, 2007.\n[319] Wujie Zheng, Jinhui Yuan, Huiyi Wang, Fuzong Lin, and Bo Zhang. A novel shot bound-\nary detection framework. In Visual Communications and Image Processing 2005, volume\n5960, page 596018. International Society for Optics and Photonics, 2006.\n[320] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learn-\ning deep features for scene recognition using places database. In Advances in Neural\nInformation In Proceedings Systems, 2014.\n[321] Justin Zobel and Alistair Moffat. Exploring the similarity space. In ACM SIGIR Forum,\nvolume 32, pages 18–34. ACM, 1998.\n[322] Karel Zuiderveld. Contrast limited adaptive histogram equalization. In In Proceedings of\nGraphics gems IV, pages 474–485. Academic Press Professional, Inc., 1994.\n175\nDI Alexander Schindler\nAlexander@Schindler.\neu.com\nwww.schindler.eu.com\n+43 699 10310329\nAlexander Schindler\nScientist\nAbout me I am a researcher in the ﬁeld of multimedia retrieval focusing on the audio-\nvisual aspects of music information. As a scientist at the Center for Safety and Security\nof the AIT Austrian Institute of Technology I am currently responsible for the areaAp-\nplied Artiﬁcial Intelligence, advanced audio indexing and retrieval as well as on machine\nlearning tasks in general. I am also part of the Music Information Retrieval team at the\ndepartment of Software Technology and Interactive Systems of the Vienna University\nof Technology where I participate in teaching and research. My research interests in-\nclude information retrieval, speciﬁcally audio and video retrieval, image processing\nand machine learning with a focus on deep neural networks. I serve as reviewer for\nscientiﬁc journals and conferences and have many years of experience in the ﬁeld of\nSoftware Engineering in various companies as well as international projects.\nExperience\nJuly 2010 - present, AIT Austrian Institute of Technology\nScientist at the Information Management Group of the Digital Safety and Security De-\npartment (2014-present)\nJunior Scientist at the High Performance Image Processing Group of the Digital Safety\nand Security Department (2010-2013)\nJan 2012 - present, Vienna University of Technology\nResearch assistant at the Information Management and Preservation Lab of the Insti-\ntute of Software Technology and Interactive Systems.\nJuly 2015 - present, Vienna Deep Learning Meetup\nEvent organization in the domain of Artiﬁcial Intelligence.\nOct 2005 - June 2007, Vienna University of Technology\nStudy assistant (Studienassistent) at the Research Group Industrial Software of the\nInstitute of Computer Aided Automation.\nMar 2005 - Sept 2005, Solution-X GmbH\nWeb Engineer developing Inter- and Intranet applications\nMar 2000 - Sept 2004, Siemens AG Austria\nSoftware Engineer developing and maintaining software for digital switching systems\nas well as distributed database managemement systems.\nEducation\n2019, PhD Computer Science\nPhD. Music Information Retrieval\n2009, Dipl.Ing. (MSc) Computer Science\nMSc. Software Engineering and Internet Computing\n2009, BSc Computer Science\nBSc. Software and Information Engineering\nSelected Publications\nHarnessing Music related Visual Stereotypes for Music Information Retrieval\nAlexander Schindler and Andreas Rauber. ACM Transactions on Intelligent Systems\nand Technology Journal (TIST) 8.2, 2016\nMulti-Task Music Representation Learning from Multi-Label Embeddings\nAlexander Schindler and Peter Knees. In Proceedings of the International Conference\non Content-Based Multimedia Indexing (CBMI2019). Dublin, Ireland, 4-6 Sept 2019.\nOn the unsolved problem of Shot Boundary Detection for Music Videos\nAlexander Schindler and Andreas Rauber. In Proceedings of the 25th International\nConference on MultiMedia Modeling (MMM2019), January 8-11, 2019, in Thessaloniki,\nGreece.\nLarge Scale Audio-Visual Video Analytics Platform for Forensic Investigations of\nTerroristic Attacks\nAlexander Schindler, Andrew Lindley, David Schreiber, Martin Boyer and Thomas\nPhilipp. In Proceedings of the 25th International Conference on MultiMedia Modeling\n(MMM2019), January 8-11, 2019, in Thessaloniki, Greece.\nMulti-Temporal Resolution Convolutional Neural Networks for Acoustic Scene Clas-\nsiﬁcation\nAlexander Schindler, Thomas Lidy and Andreas Rauber. In Proceedings of the Detec-\ntion and Classiﬁcation of Acoustic Scenes and Events 2017 Workshop (DCASE2017),\nNovember 2017.\nPriMa-X: A reference model for realizing prescriptive maintenance and assessing its\nmaturity enhanced by machine learning\nNemeth, Tanja, Fazel Ansari, Wilfried Sihn, Bernhard Haslhofer, and Alexander Schindler.\nProcedia CIRP 72 (2018): 1039-1044.\nA multi-modal deep neural network approach to bird-song identiﬁcation\nBotond Fazekas, Alexander Schindler, Thomas Lidy, Andreas Rauber. LifeCLEF 2017\nworking notes, Dublin, Ireland.\nFashion and Apparel Classiﬁcation using Convolutional Neural Networks\nAlexander Schindler, Thomas Lidy, Stefan Karner and Matthias Hecker. In Proceed-\nings of the 9th Forum Media Technology (FMT2017), St. Poelten, Austria, Oct 29, 2017.\nComparing shallow versus deep neural network architectures for automatic music\ngenre classiﬁcation\nAlexander Schindler, Thomas Lidy, and Andreas Rauber. In Proceedings of the 9th\nForum Media Technology (FMT2016), St. Poelten, Austria, November 23 - 24, 2016.\nCQT-based convolutional neural networks for audio scene classiﬁcation\nThomas Lidy and Alexander Schindler. In Proceedings of the Detection and Clas-\nsiﬁcation of Acoustic Scenes and Events 2016 Workshop (DCASE2016), pages 60–64,\nSeptember 2016.\nParallel convolutional neural networks for music genre and mood classiﬁcation\nThomas Lidy and Alexander Schindler. Technical report, Music Information Retrieval\nEvaluation eXchange (MIREX 2016), August 2016.\nThe europeana sounds music information retrieval pilot\nAlexander Schindler, Sergiu Gordea, and Harry van Biessum. In Proceedings of the\nInternational Conference on Cultural Heritage (EuroMed2016), Lecture Notes in Com-\nputer Science, Cyprus, October 31 - November 5 2016. Springer.\nAn audio-visual approach to music genre classiﬁcation through affective color fea-\ntures\nAlexander Schindler and Andreas Rauber. In Proceedings of the 37th European Con-\nference on Information Retrieval (ECIR’15), Vienna, Austria, March 29 - April 02 2015.\nA music video information retrieval approach to artist identiﬁcation\nAlexander Schindler and Andreas Rauber. In Proceedings of the 10th International\nSymposium on Computer Music Multidisciplinary Research (CMMR2013), Marseille,\nFrance, October 14-18 2013.\nTowards objective quality assessment in digital collections\nAlexander Schindler and Reinhold Huber-Moerk. In Proceedings of the 2nd Work-\nshop on Open Research Challenges in Digital Preservation (ORC’13), Lisbon, Portugal,\nSeptember 6 2013.\nCapturing the temporal domain in echonest features for improved classiﬁcation ef-\nfectiveness\nAlexander Schindler and Andreas Rauber. In Adaptive Multimedia Retrieval, Lecture\nNotes in Computer Science, Copenhagen, Denmark, October 24-25 2012. Springer.\nFacilitating comprehensive benchmarking experiments on the million song dataset\nAlexander Schindler, Rudolf Mayer, and Andreas Rauber. In Proceedings of the 13th\nInternational Society for Music Information Retrieval Conference (ISMIR 2012), Porto,\nPortugal, October 8-12 2012.\nQuality assurance for document image collections in digital preservation\nReinhold Huber-Moerk and Alexander Schindler. In Proceedings of the 14th Interna-\ntional Conference on Advanced Concepts for Intelligent Vision Systems (ACIVS 2012),\nLecture Notes in Computer Science, Brno, Czech Republic, September 4-7 2012.\nTheses\nQuality of service driven workﬂows within the microsoft .net environment\nAlexander Schindler. Master’s thesis, Vienna University of Technology, 2009."
}