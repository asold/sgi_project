{
  "title": "Text-Free Prosody-Aware Generative Spoken Language Modeling",
  "url": "https://openalex.org/W3198217962",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2115606974",
      "name": "Eugene Kharitonov",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2096549477",
      "name": "Ann Lee",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2232346429",
      "name": "Adam Polyak",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2224570189",
      "name": "Yossi Adi",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A3088063767",
      "name": "Jade Copet",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2903607076",
      "name": "Kushal Lakhotia",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2155069714",
      "name": "Tu Anh Nguyen",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2222442018",
      "name": "Morgane Rivière",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2125491050",
      "name": "Abdelrahman Mohamed",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2121369328",
      "name": "Emmanuel Dupoux",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A4209177087",
      "name": "Wei-Ning Hsu",
      "affiliations": [
        "Meta (Israel)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3169320628",
    "https://openalex.org/W2151083697",
    "https://openalex.org/W3098403858",
    "https://openalex.org/W2963799213",
    "https://openalex.org/W1597422500",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W3148101939",
    "https://openalex.org/W359161088",
    "https://openalex.org/W2123409957",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W3210177631",
    "https://openalex.org/W2527729766",
    "https://openalex.org/W2973049979",
    "https://openalex.org/W2141885148",
    "https://openalex.org/W2112078429",
    "https://openalex.org/W2160473997",
    "https://openalex.org/W2143827132",
    "https://openalex.org/W3112034174",
    "https://openalex.org/W2950349262",
    "https://openalex.org/W3094247854",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W2608207374",
    "https://openalex.org/W2972943112",
    "https://openalex.org/W3099782249",
    "https://openalex.org/W3033411150",
    "https://openalex.org/W3095948607",
    "https://openalex.org/W2995181338",
    "https://openalex.org/W2107740512",
    "https://openalex.org/W3140429000",
    "https://openalex.org/W3039910566",
    "https://openalex.org/W2426479676",
    "https://openalex.org/W2949382160",
    "https://openalex.org/W2118748593",
    "https://openalex.org/W3157923770",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2810914326",
    "https://openalex.org/W2913151434",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2093585241",
    "https://openalex.org/W2982223350",
    "https://openalex.org/W2130086727",
    "https://openalex.org/W3150572638"
  ],
  "abstract": "Eugene Kharitonov, Ann Lee, Adam Polyak, Yossi Adi, Jade Copet, Kushal Lakhotia, Tu Anh Nguyen, Morgane Riviere, Abdelrahman Mohamed, Emmanuel Dupoux, Wei-Ning Hsu. Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 8666 - 8681\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nText-Free Prosody-Aware Generative Spoken Language Modeling\nEugene Kharitonov∗, Ann Lee∗, Adam Polyak, Yossi Adi,\nJade Copet, Kushal Lakhotia, Tu-Anh Nguyen, Morgane Rivière,\nAbdelrahman Mohamed, Emmanuel Dupoux, Wei-Ning Hsu\nFacebook AI Research\n{kharitonov,annl,wnhsu}@fb.com\nAbstract\nSpeech pre-training has primarily demon-\nstrated efﬁcacy on classiﬁcation tasks, while\nits capability of generating novel speech, sim-\nilar to how GPT-2 can generate coherent para-\ngraphs, has barely been explored. Generative\nSpoken Language Modeling (GSLM) (Lakho-\ntia et al., 2021) is the only prior work ad-\ndressing the generative aspects of speech pre-\ntraining, which replaces text with discov-\nered phone-like units for language modeling\nand shows the ability to generate meaning-\nful novel sentences. Unfortunately, despite\neliminating the need of text, the units used\nin GSLM discard most of the prosodic in-\nformation. Hence, GSLM fails to leverage\nprosody for better comprehension, and does\nnot generate expressive speech. In this work,\nwe present a prosody-aware generative spo-\nken language model (pGSLM). It is composed\nof a multi-stream transformer language model\n(MS-TLM) of speech, represented as discov-\nered unit and prosodic feature streams, and\nan adapted HiFi-GAN model converting MS-\nTLM outputs to waveforms. We devise a se-\nries of metrics for prosody modeling and gen-\neration, and re-use metrics from GSLM for\ncontent modeling. Experimental results show\nthat the pGSLM can utilize prosody to im-\nprove both prosody and content modeling, and\nalso generate natural, meaningful, and coher-\nent speech given a spoken prompt.1\n1 Introduction\nNatural language processing (NLP) has made\ntremendous progress recently. One of the most sig-\nniﬁcant ﬁndings is that language models (LMs) are\nnatural unsupervised multitask learners (Radford\net al., 2018, 2019; Brown et al., 2020) — by simply\ntraining a big neural network on next word predic-\ntion with a large amount of unlabeled text, it learns\n1Audio samples can be found athttps://speechbot.\ngithub.io/pgslm/. Codes and models are avail-\nable at https://github.com/pytorch/fairseq/\ntree/main/examples/textless_nlp/pgslm.\nto comprehend, answer questions, summarize, and\neven translate (Radford et al., 2019). Fine-tuning\nsuch pre-trained models further leads to the state-\nof-the-art performance on numerous benchmark\ntasks (Brown et al., 2020), beating tailor-made mod-\nels trained from scratch only on labeled data.\nGiven the impressive performance of pre-trained\ntext language models, it is tempting to approach\nspoken language processing tasks by ﬁrst transcrib-\ning speech into text with an automatic speech recog-\nnition (ASR) system and then utilizing text-based\nmodels for comprehension and generation. How-\never, there are a number of caveats for such a frame-\nwork. First, the majority of the world’s languages\nare primarily spoken and do not have associated\ntexts in large quantities (Lewis et al., 2016). In\npractice, this limits the reach of NLP techniques to\na fraction of the world’s languages that have a large\npresence on the web and for which there exists a\nwidely available high quality ASR system. Second,\ndespite sharing the same vocabulary and syntac-\ntic rules, the spoken form and the written form of\nthe same language still vary signiﬁcantly in terms\nof sentence lengths, word distributions, presence\nof disﬂuencies and back-channelings, and so on\n(Biber, 1991). This makes language models pre-\ntrained on web text not suitable for processing spo-\nken languages. Third, text does not reﬂect the rich\nset of features conveyed by oral languages. Speech\ncarries not only phonetic information, but also non-\nverbal vocalizations (laughter, voice clicks, ﬁller\nvocalization, etc), rhythm and intonation (prosody),\nand emotional markers. All of these features could\nhelp, not only with generating more expressive\nspeech (Ren et al., 2020; Ła ´ncucki, 2021), but also\nwith the semantic analysis of the content of the\nmessage (Cutler et al., 1997; Tran et al., 2017).\nTo combat these deﬁciencies, more recently\nthere is increasing interest in exploring speech pre-\ntraining using large quantities of unlabeled speech\ndata (Chung et al., 2019; Schneider et al., 2019;\n8666\nKharitonov et al., 2021; Baevski et al., 2020; Hsu\net al., 2021c; Liu et al., 2020; Ling and Liu, 2020;\nTjandra et al., 2020; Hsu et al., 2021b,a). How-\never, most of the studies evaluate their models on\ndiscriminative tasks, such as ASR and those in the\nSUPERB benchmark (Yang et al., 2021). To the\nbest of our knowledge, generative spoken language\nmodelling (GSLM) (Lakhotia et al., 2021) is the\nonly prior work that evaluates prompted speech\ncompletion, a generative tasks that is similar to\nthe text completion task in GPT-2 (Radford et al.,\n2019). To remove the reliance on text, GSLM ex-\nploits discovered units from self-supervised models\nto build a unit language model (uLM) and a unit-\nto-spectrogram (u2S) model. Speech completion\ncan be achieved by ﬁrst sampling a unit sequence\nfrom the uLM with a unit prompt inferred from a\nspeech prompt, and then synthesizing the sampled\nsequence into speech with the u2S model. Unfor-\ntunately, because those discovered units encode\nmostly phonetic information (Polyak et al., 2021),\nit suffers from the same prosodic information loss\nissue as text-based LMs. Therefore, when using\nthat uLM for speech completion, it fails to continue\nwith a coherent tone to the prompt.\nIn this paper, we introduce a prosody-aware\ngenerative spoken language model (pGSLM) that\njointly models phonetic content and prosody, in or-\nder to leverage prosody for comprehension, and to\ngenerate speech coherent with the prompt, which\nis a precursor for building speech-based dialogue\nsystems. In keeping with our aim of liberating NLP\nfrom its over-reliance on text, we follow GSLM and\nrepresent the phonetic content with self-supervised\nunits discovered from raw audio. As for prosody,\nit is represented by the pattern of quantized fun-\ndamental frequency (F0) and duration. pGSLM is\ncomprised of two separately trained components:\nan auto-regressive Multi-Stream Transformer Lan-\nguage Model (MS-TLM) that predicts the next pho-\nnetic and prosodic representation given the past\nones, and a unit High-Fidelity Generative Adver-\nsarial Network (HiFi-GAN) adapted from Polyak\net al. (2021) that converts the MS-TLM output into\na waveform like a vocoder. To evaluate the pro-\nposed model, we adopt metrics from (Lakhotia\net al., 2021) for content evaluation, and devise a\nseries of metrics for prosody evaluation. Experi-\nmental results demonstrate that 1) joint modeling\nof prosody improves phonetic content modeling,\n2) pGSLM can generate speech continuation co-\nherent with the prompt in term of the content and\nthe prosody, and 3) proper choices of model and\nprosodic representation is crucial to synthesizing\nnatural, coherent, and expressive speech.\n2 Related Work\nOur work is related to utilizing prosody for com-\nprehension and predicting prosody for speech syn-\nthesis, which we discuss in the following sections.\n2.1 Improving Comprehension with Prosody\nProsody, which is often characterized by the\nrhythm, intonation, and intensity of speech, carries\nuseful information for comprehending speech in\naddition to the textual content (Cutler et al., 1997).\nPrior studies have shown that including prosody\ninformation can improve the performance from\ntext-only models on speech segmentation (Shriberg\net al., 2000), dialogue act classiﬁcation (Shriberg\net al., 1998; Ward and Tsukahara, 2000), syntac-\ntic parsing (Tran et al., 2017), speech–language\npathology (Cohen et al., 2019), ASR (Ostendorf\net al., 2003; Shriberg and Stolcke, 2004), and lan-\nguage modeling (Huang and Renals, 2007; Su and\nJelinek, 2008; Ward et al., 2012). These studies\nprovide strong empirical evidences for the beneﬁt\nof considering prosody in processing spoken lan-\nguages, especially in the conversational scenarios.\nThis work shares the same motivation, but differs\nfrom the prior work in two crucial aspects. First,\nthis work utilizes discrete units discovered from a\nself-supervised model and hence does not require\nany textual supervision, making it applicable to\nboth written and unwritten languages, while in the\nprior work prosody information is used alongside\ntext. Second, our model can be regarded as the\nspeech version of GPT, which does not require any\ntask-speciﬁc labels and can be pre-trained on large\nquantities of unlabeled speech data. The ability\nto leverage more data is shown to be the key to\nachieve good performance in text pre-training.\n2.2 Prosody Prediction for Speech Synthesis\nThe proposed pGSLM model can be re-purposed\nas a text-to-speech (TTS) model when the pho-\nnetic content (represented as a unit sequence) is\ngiven and the prosody is generated by the MS-TLM\nmodel. This is similar to FastSpeech (Ren et al.,\n2020) and FastPitch (Ła ´ncucki, 2021) TTS mod-\nels, where prosodic features are predicted from\ntext and speech are generated conditioning on both\n8667\nthe text and the predicted prosodic features. As\nFastSpeech and FastPitch are designed to improve\nthe inference-time efﬁciency from auto-regressive\nmodels like Tacotron (Wang et al., 2017), they pre-\ndict prosodic features and spectrograms without in-\ntroducing dependency between time steps. In other\nwords, these models assume that the prosody fea-\ntures within an utterance are not correlated across\ntime steps given the text, whereas our proposed MS-\nTLM does not make such an assumption. We will\ndemonstrate empirically the conditional indepen-\ndence is not a realistic assumption and our model\nachieves better performance on prosody metrics\nwith auto-regressive modeling.\nAs for analysis on prosody modeling, we present\nmore extensive metrics by considering both teacher-\nforcing decoding and sampling, while prior work\ndoes not consider the multi-modal nature of\nprosody and only generate prosody deterministi-\ncally (Ren et al., 2020). Moreover, we also evaluate\nprosody in a more disentangled manner by mea-\nsuring the error of the prosody prediction module\nalone instead of measuring the error of the prosody\nextracted from the synthesized waveform: the latter\nconﬂates the impact from both the prosody predic-\ntion module and the vocoder.\n3 Method\nIn this section, we ﬁrst describe the phonetic and\nprosodic representations used in pGSLM, and then\nintroduce the two components it is comprised of: a\nmulti-stream transformer language model and an\nadapted unit HiFi-GAN.\n3.1 Phonetic and Prosodic Representations\nWe choose units with a vocabulary size of 100\nderived from HuBERT (Hsu et al., 2021a), a self-\nsupervised speech model, as the phonetic repre-\nsentation. Speciﬁcally, these units are obtained\nthrough clustering the 6th transformer layer output\nof the base HuBERT model provided in (Hsu et al.,\n2021a) using a k-means algorithm, following the\nrecipe of HuBERT closely. A speech waveform\ncan therefore be encoded into a sequence of dis-\ncrete units at a frame rate of 50 units per second,\nor alternatively, into a sequence of (unit, duration)\ntuples using run-length encoding. HuBERT units\nwere found to perform favorably compared to other\nself-supervised units such as wav2vec 2.0 (Baevski\net al., 2020) and VQ-V AE (van den Oord et al.,\n2017) in terms of lexical content modeling (Lakho-\ntia et al., 2021) and disentangling prosodic infor-\nmation (Polyak et al., 2021).\nWe use unit duration d and fundamental fre-\nquency (F0, or pitch) f to derive prosodic repre-\nsentations. Polyak et al. (2021) has shown that\npairing HuBERT units with duration and F0 en-\nables high-quality speech re-synthesis that pre-\nserves more prosodic information such as intona-\ntion compared to re-synthesizing with only units.\nSimilar results are demonstrated in several other\nstudies (Ren et al., 2020; Ła ´ncucki, 2021) in the\ncontext of text-to-speech synthesis. Unfortunately,\nwhile F0 encodes prosodic information, it also\nencodes signiﬁcant amount of speaker informa-\ntion. Figure A.1 in the appendix illustrates how\nspeaker and prosodic information (emotion) are dis-\nentangled in raw pitch using a multi-speaker multi-\nemotion dataset, EmoV (Adigwe et al., 2018). We\ndo not wish to model speaker variation in pGSLM\nbecause it is less relevant to spoken language un-\nderstanding compared to prosody. To that end, we\npropose to model speaker-mean normalized log\nF0: lf = log f −Ef′ from the same speaker as f [log f′],\nwhich can be interpreted as the ratio to the mean\npitch in the log space: lf = log( f/¯f), where\n¯f = exp Ef′ [log f′]. Speciﬁcally, the equation\nabove is used for voiced frames, and the expecta-\ntion is taken over voiced frames from a speaker.\nFor unvoiced frames, we simply set lf = 0.\nOne may ask why F0 is only normalized by the\nspeaker mean but not the variance. We argue that\nthe variance encodes the “level of expressiveness”\nand it is desired to preserve it. This is demonstrated\nempirically in Figure A.2 in the appendix, where\nspeakers from expressive datasets, EmoV and Bliz-\nzard 2013 (SynSIG), exhibits larger speaker log\nF0 standard deviation than those in less expressive\ndatasets, LJSpeech (Ito and Johnson, 2017) and\nVCTK (Veaux et al., 2016). On the other hand, we\nalso found that variance is more correlated mean\nin the linear space than in the log space, as shown\nin Figure A.3. Therefore, we argue that mean-\nnormalized log F0 is a more suitable representation\nfor prosody as it encodes less speaker information\nwhile preserving the level of expressiveness.\n3.2 Multi-Stream Transformer LM\nWe adapt the Transformer LM from (Lakhotia et al.,\n2021) to take multiple streams of input and pre-\ndict multiple streams of output, and refer to it as\nthe Multi-Stream Transformer Language Model\n8668\n(MS-TLM). An MS-TLM predicts a sequence of\nsegment representations, which reduces the se-\nquence length signiﬁcantly and is found beneﬁcial\ncompared to predicting frame sequences (Lakhotia\net al., 2021). Each segment is represented with\nthe unit u, duration (in frames) d, and normalized\npitch lf. The ﬁrst two are obtained by run-length\nencoding the ﬁxed frame rate unit sequence, while\na segment lf is computed by averaging those from\nvoiced frames within a segment or set to 0 if the\nentire segment is unvoiced. An example is provide\nin Appendix C.\n3.2.1 Delayed prosody prediction\nLet subscript tbe the segment index. At each step,\na vanilla MS-TLM takes (ut−1,dt−1,lft−1) as in-\nput, linearly projects each of them to the dimension\nof the transformer, and feeds the summed embed-\ndings to the transformer. The transformer output\nat that step is projected to the dimension of each\nstream to predict ut, dt, and lft independently. The\ndistribution modeled by the synchronous MS-TLM\np(u1:T ,d1:T ,lf1:T ) can be written as:\n∏T\nt=1 p(ut |u1:t−1,d1:t−1,lf1:t−1)\n×p(dt |u1:t−1,d1:t−1,lf1:t−1)\n×p(lft |u1:t−1,d1:t−1,lf1:t−1). (1)\nWe see that the factorial assumption here may be\ntoo strong, because the duration and the pitch of a\nsegment are highly correlated with the phonetic\ncontent of the same segment. To alleviate that\nwithout introducing intra-step dependency or in-\nterleaving streams (which increases the sequence\nlength and requires determining an order for the\nthree streams a priori), we introduce a delay fac-\ntor ∆ (∆ ≥0) for prosodic streams, which shift\nprosodic input and output streams backward by\n∆ steps, taking (ut−1,dt−∆−1,lft−∆−1) as input\nand outputting (ut,dt−∆,lft−∆). When ∆ = 1 ,\neach step of the LM predicts the unit of the cur-\nrent segment and the prosodic representations of\nthe previous segment, of which the lexical unit has\nbeen observed already, as shown in Figure 1.\n3.2.2 Quantizing prosodic representations\nA straightforward solution to encode prosody\nstreams d and lf is to represent them as contin-\nuous values and minimize an L1 or L2 loss for\ntraining, similar to FastSpeech2 (Ren et al., 2020)\nand FastPitch (Ła ´ncucki, 2021). Doing so assumes\nthat the duration and the pitch of a segment follow\nFigure 1: Delayed multi-stream transformer language\nmodel with prosody stream delay ∆ = 1.\na unimodal distribution (Laplace for L1 and Gaus-\nsian for L2) given the context. If the underlying\ndistribution is multimodal with wide spread, the\nlearned distribution would be signiﬁcantly underﬁt-\nting with a mean far from the modes. Empirically,\nwe found that such modeling indeed leads to pre-\ndicting lf values very close to 0 for all segments,\nand the generated prosody sounds dull and boring.\nInspired by WaveNet (Oord et al., 2016), we\nrepresent prosodic features as discrete random vari-\nables through quantization. It is straightforward to\nquantize dsince it encodes integer values originally\n(length in frames). We set the maximum length to\nbe 32 and the bin width to be 1, resulting in 32\nbins. We quantize speaker-mean normalized log\nF0 lf into K = 32 bins such that each bin with\nboundaries [bi−1,bi] contains the same probability\nmass: P(lf ∈[bi−1,bi]) = 1/K.\n3.2.3 Training objective\nThe training loss is a weighted sum of three per-\nstream losses. Omitting dependency on the con-\ntext for brevity, MS-TLM deﬁnes a distribution\np(ut,dt,lft) of the potential values for a timestep\nt. Then, denoting ground-truth per-channel values\nas u∗\nt ,d∗\nt ,lf∗\nt , we get:\nL(p(ut,dt,lft),u∗\nt ,d∗\nt ,lf∗\nt ) = Lu(p(ut),u∗\nt )\n+ α·Ld(p(dt),d∗\nt ) + β·Llf (p(lft),lf∗\nt ) (2)\nIn all experiments, we use cross-entropy as the\nloss on the predictions of the unit channel ( Lu).\nWhenever we operate on quantized prosody values\n(both duration and F0), we also use cross-entropy\nas losses Ld and Llf . In the case of continuous-\nvalued prosody streams, we treat predicted values\np(dt) and p(lft) as the mode of Laplacian distribu-\ntions and maximize the log likelihood of the model,\n8669\nwhich is equivalent to minimizing an L1 loss. In\npreliminary experiments, we found that the results\nare relatively robust to variations of the relative\nweights αand β, hence we ﬁx them α= β = 0.5\nin all our experiments.\n3.2.4 Sampling from a model\nTo generate new utterances, potentially conditioned\non a prompt, we run autoregressive generation\nwhere at each step we sample units, duration, and\nnormalized log F0 values, append them to the con-\ntext and feed them back. In the case of discrete\nchannels (units, also duration/pitch in the case of\ndiscrete-valued models), we sample from the corre-\nsponding multinomial distribution. As commonly\ndone in language modelling (Lakhotia et al., 2021),\nwe perform sampling with temperature by scaling\nthe logits by the temperature parameter. We ﬁne-\ntune the temperature on the validation data.\nFor MS-TLM that models normalized log F0\nas continuous variables, we draw samples from\na Laplacian distribution with its location parame-\nter set to the predicted value, because the model\nassumes the output distribution is Laplacian (see\n§-3.2.3). For duration, to avoid sampling invalid\nvalues, we sample from a Laplacian distribution\ntruncated at zero and round it to the nearest posi-\ntive integer.\n3.3 Waveform Generation with Unit\nHiﬁ-GAN\nGiven (u1:T ,d1:T ,lf1:T ) generated from the MS-\nTLM, we adapt the discrete unit-based HiFi-GAN\nvocoder from (Polyak et al., 2021) to gener-\nate waveform. The original vocoder proposed\nin (Polyak et al., 2021) takes in frame-level dis-\ncrete unit, pitch and speaker embedding as input\nand applies VQ-V AE quantization on the pitch. As\nMS-TLM predicts quantized speaker-mean normal-\nized log F0 on the segment level, we modify the\ntraining of the vocoder so that it takes frame-level\nsegment-average pitch as input, where the pitch\nvalues for frames within a segment are set to the\nsame value. We apply the same quantization de-\nscribed in § 3.2.2 instead of VQ-V AE on the pitch.\nThe unit Hiﬁ-GAN and the MS-TLM are trained\nseparately.\n4 Experimental Setup\n4.1 Data, Model, and Training\nIn our experiments, we train MS-TLM models\non two English datasets: LibriSpeech (Panayotov\net al., 2015) and a 6K-hour subset (Rivière and\nDupoux, 2020) of Libri-Light (Kahn et al., 2020)\nwhich we refer to as LL-6K. Both datasets repre-\nsent audio books and we use LibriSpeech dev-clean\nand test-clean as validation and test sets. As de-\nscribed in Section 3.1, we use HuBERT-based unit\nrepresentations. However, to investigate whether\nour proposed models can work with other types\nof units, we also experiment with CPC (Rivière\nand Dupoux, 2020; Oord et al., 2018) and ground-\ntruth phone representations. We experiment with\na vocabulary of 100 units when working with Hu-\nbert and CPC, following the same protocol and us-\ning the same pre-trained models as Lakhotia et al.\n(2021). On the other hand, frame-level phone tran-\nscripts are obtained through forced-alignment us-\ning the tri6b model from Kaldi’s LibriSpeech\nrecipe (Povey et al., 2011). The position- and\ncontext-independent phones without lexical stress\nmarkers are used, which include 41 units (39\nphones, one silence SIL, and one spoken noise\nSPN). The frame rate of CPC and phone units is\n100Hz, and is 50Hz for HuBERT units.\nWe experiment with MS-TLM of two sizes: base\nand large. The base one has 6 layers, 8 attention\nheads per layer, embedding size of 512. Its FFN\nlayer has 2048 units. The large variant has 12 lay-\ners, each with 16 heads, embedding size of 1024\nand the FFN layer is of dimensionality 4096. We\nset attention dropout and dropout probabilities to\n0.1 for both alternatives. On top of that, we apply\nsequence-level and span-level (Baevski et al., 2020)\ninput dropout to the two prosody streams. Speciﬁ-\ncally, each stream is zero-ed out with a probability\nof 0.2, and 2% of the steps are selected as starts,\nfrom which 5 steps of that stream is zero-ed out.\nOptimization is done using Adam (Kingma and Ba,\n2014) with a peak learning rate of 5e-4. Learning\nrate ramps up linearly for the ﬁrst 4K updates, and\nthen decays to 0 with an inverse square-root sched-\nule. We train the base model for 70 epochs, and\nlarge model for 100 epochs. Each GPU’s batch con-\ntains up to 3072 (u,d,lf ) segments and we used 8\n(16) GPUs to train base (large) MS-TLM. For each\nupdate, we aggregated gradients from 8 batches.\n8670\nMulti-Stream Transformer\n···\n···\n+\n+\nu0\nd-1\nlf-1\nlfM-1\n+\n+\nuM-1\ndM-2\nlfM-2\nlfN\n+\n+\nuN\ndN-1\nlfN-1\nlfM\n+\n+\nuM\ndM-1\nlfM-1\n···\n: generated\n: ground truth\n3 seconds\nFigure 2: Per-stream prosody continuation task. lf is\nthe target stream for continuation in this example.\nMulti-Stream Transformer\n···\n···\n+\n+\nu0\nd-1\nlf-1\nlfM-1\ndM-1\nuM\n+\n+\nuM-1\ndM-2\nlfM-2\nlfN\ndN\nuN+1\n+\n+\nuN\ndN-1\nlfN-1\nlfM\ndM\nuM+1\n+\n+\nuM\ndM-1\nlfM-1\n···\n: generated\n: ground truth\nFigure 3: Speech continuation task.\n4.2 Prosody and Content Evaluation\nOur overall goal is to ﬁnd models that can freely\ngenerate meaningful content and consistent as well\nas diverse prosody. In this Section, we deﬁne a set\nof metrics that measure models’ performance over\neach stream individually and combined, in both the\nteacher-forcing mode and the inference mode.\n4.2.1 Teacher-forcing metrics\nA simple way to evaluate models is to measure its\nloss on hold-out data in a setup where for each step\nthe full ground truth context is provided. For the\nunit stream, we measures Negative Log-Likelihood\n(NLL), equivalent to cross-entropy. For the dura-\ntion and pitch streams we use Mean Absolute Error\n(MAE), equivalent to L1 loss. When the pitch val-\nues are quantized, we de-quantize predictions to\nthe means of the respective buckets.\n4.2.2 Per-stream prosody continuation\nWe next evaluate the model’s ability to complete\na stream in isolation. Speciﬁcally, we provide a\n3s prompt for all streams, and then sample auto-\nregressively the target stream while feeding the\nground truth value for the other streams, as de-\npicted in Figure 2. The prompts are inferred\nfrom the utterances in the validation set. When\nprosodic features are quantized, we sample with a\ntemperature τ ∈{0.0,0.25,0.5,0.7,1.0,1.3}, and\nwhen they are continuous, we sample with a scale\nb ∈ {0.0,0.05,0.125,0.25,0.5,0.7,1.0,1.3}for\nduration and b ∈0.01 ×{2−6,2−5,··· ,20}for\npitch. The temperature/scale is chosen to minimize\nthe Min-MAE for the corresponding stream, which\nwe describe next. We chose different sweeping\nranges for continuous pitch and duration because\nthey have different inherent standard deviations.\nCorrectness (Min-MAE) A prompt might have\nmultiple meaningful continuations in the content\nspace (Lakhotia et al., 2021). Similarly, a single\nsentence can have multiple correct prosodic pro-\nﬁles. To account for that, for each prompt we gen-\nerate n= 20 samples so that a model has a chance\nto cover most modes of the underlying distribution,\nand report the minimal MAE (min-MAE) against\nthe reference among the nsamples.\nConsistency (Corr.) To quantify the models’ ca-\npability to generate consistent prosody, we measure\nPearson correlation between the mean values of a\nstream in the prompt and in the generated contin-\nuation. Clearly, if the prompt has a distinct tempo\nor a pitch, a good continuation should reﬂect this.\nThe same setup as the min-MAE metric is used\n(n = 20 ) with one exception: we only consider\nsequences that are at least 6s long.\nExpressiveness (Std.) To measure how expres-\nsive the generated prosody is, we calculate the stan-\ndard deviation of the generated values and expect a\ngood model to exhibit a similar level of that as the\nground truth. The same setup as in “Min-MAE” is\nused.\n4.2.3 Speech continuation\nLastly, we evaluate the model’s ability to carry\nout prompted speech completion, where all three\nstreams are sampled given a 3s prompt using the\ntemperature/scale parameter determined from per-\nstream continuation (§ 4.2.2) as illustrated in Fig-\nure 3. We sample the MS-TLM auto-regressively\nuntil it emits the EOS unit or reaches the length of\nthe reference. The MS-TLM output is synthesized\ninto a waveform using the adapted HiFi-GAN.\nContent (Max-Word-Cont-BLEU2) We re-use\nthe maximum word-level continuation BLEU2 pro-\nposed by Lakhotia et al. (2021) to quantify how\nwell a model can complete a prompt in terms of the\ntextual content. We transcribe the waveform with\nan off-the-shelf wav2vec 2.0-based ASR (Baevski\n8671\net al., 2020) (same as (Lakhotia et al., 2021))\nand compute the BLEU2 score for each of the\nn= 20 continuations against the reference comple-\ntion. The highest one is used as score for a prompt.\nHuman evaluation (MOS, MMOS, PMOS)\nWe ask humans to evaluate three aspects of speech\ncontinuation: sound quality, meaningfulness (how\nnatural the text content is considering both gram-\nmar and meaning), and prosody (how consistent\nand natural the intonation and the rhythm is). We\nfollow the human evaluation protocol used by\nLakhotia et al. (2021) closely, where raters evalu-\nate subjective quality of the recordings using head-\nphones on a scale between 1 to 5 with an increment\nof 1, the higher the better. Only Native English\nspeakers were recruited as raters for all three stud-\nies. The same 100 prompts as (Lakhotia et al.,\n2021) from LibriSpeech test-other are used, and\neach system generates one continuation per prompt.\nEach continuation is evaluated by at least 5 raters\nfor each aspect. The CrowdMOS package (Ribeiro\net al., 2011) was used for all experiments using\nthe recommended recipes for outlier removal. All\nparticipants were recruited using the Amazon Me-\nchanical Turk platform. The metrics on the three\naspects are denoted as MOS, M-MOS, and P-MOS.\n5 Results\n5.1 Prosodic Inputs Are Useful for Content\nand Prosody Modeling\nIn Table 1 we report teacher-forcing metric calcu-\nlated on LibriSpeech dev-clean dataset for a diverse\nset of models. In rows 1-8, we report metric values\nfor base MS-TLM models that are trained on Lib-\nriSpeech 960h transcribed into HuBERT-100 units.\nIn rows 9-12 we consider large MS-TLM models\ntrained on HuBERT transcripts of LL6k. Rows 13\n& 14 and 15 & 16 contain metric values for models\nthat are trained on LibriSpeech 960h transcribed\nusing CPC and ground-truth phonetic units.2 The\nrow 1 corresponds to the prosody-ignorant baseline\nmodel of (Lakhotia et al., 2021).\nOn comparing two models that only predict\nunits (rows 1 and 5) we see that by simply adding\nprosodic channels to the input of the model, we\nobtain considerably lower level of negative log-\n2Note: the metric values in this section are only compara-\nble within the same unit type. To compare across unit types,\none can synthesize the MS-TLM output into waveform and\ntranscribe the speech with an ASR systems to compute metrics\nin the word or character space.\nID Input Output Quant? ∆ uNLL↓dMAE↓lfMAE↓\nBase MS-TLM, HuBERT units, trained on LS9601 u u n/a n/a 1.522 n/a n/a2 u (u,d,lf) ✓ 0 1.525 0.759 0.1153 u (u,d,lf) ✓ 1 1.517 0.586 0.1124 u (u,d,lf) 1 1.514 0.562 0.093\n5 (u,d,lf) u ✓ 0 1.336 n/a n/a6 (u,d,lf) (u,d,lf) ✓ 0 1.337 0.722 0.0527 (u,d,lf) (u,d,lf) ✓ 1 1.441 0.551 0.0498 (u,d,lf) (u,d,lf) 1 1.447 0.536 0.046\nLarge MS-TLM, HuBERT units, trained on LL6k9 u (u,d,lf) 1 1.513 0.563 0.09510 u (u,d,lf) ✓ 1 1.522 0.586 0.116\n11 (u,d,lf) (u,d,lf) 1 1.421 0.527 0.04312 (u,d,lf) (u,d,lf) ✓ 1 1.406 0.543 0.047\nBase MS-TLM, CPC units, trained on LS96013 u (u,d,lf) ✓ 1 1.511 1.302 0.12214 (u,d,lf) (u,d,lf) ✓ 1 1.353 1.181 0.045\nBase MS-TLM, Phone units, trained on LS96015 u (u,d,lf) ✓ 1 1.559 2.748 0.15016 (u,d,lf) (u,d,lf) ✓ 1 1.485 2.419 0.079\nID Input Output Quant? τ uNLL dMAElfMAE\nstddev stddev stddev\nBase MS-TLM, HuBERT units, trained on LS960\n2 u (u,d,lf) ✓ 0 0.0004 0.00226 0.0033\n6 (u,d,lf) ( u,d,lf) ✓ 0 0.0022 0.00306 0.00037\nTable 1: (Top) Teacher-forcing metrics on Librispeech\ndev-clean. Exp 1 is identical to the uLM presented in\n(Lakhotia et al., 2021). We can observe that models\nwith both phonetic and prosodic input (u,d,lf ) consis-\ntently outperforms their counterpart model with only\nphonetic input u. This trend holds for different lexical\nrepresentations (HuBERT, CPC, phone), both continu-\nous and discrete prosodic features, and different delay\nfactors τ. (Bottom) We train Exp ID 2 and 6 with ﬁve\nrandom seeds and measure the standard deviation on\nall three metrics. Results show that the gap between\nthe models with and without prosodic input is signiﬁ-\ncant relative to the standard deviation.\nlikelihood of the units ( uNLL: 1.522 vs. 1.336).\nThe same trend persist for the models that predict\nprosodic channels, too. For instance, this holds in\nthe case of the continuous-F0 models (rows 9 &\n11: 1.513 vs. 1.421) and, equally for the quantized\nF0 HuBERT-based models (rows 10 and 12: 1.522\nvs. 1.406). Moreover, this holds for the CPC-based\nmodels (row 13 & row 14) and even for the mod-\nels trained on phone transcripts (rows 15 & 16).\nHence we conclude that prosodic input universally\nimproves speech “content” modelling.\nOur results in Table 1 also allow us to investi-\ngate whether shifting prosody streams w.r.t. the unit\nstream (∆ >0) is useful. On comparing rows 6 &\n7 we see that this is indeed the case: at an expense\nof some increase in uNLL (e.g., 1.337 vs. 1.441)\nwe obtain considerable relative improvement in d\n8672\nID Input Output Quant? d lf Max-Word-\nmin-MAE↓ Corr.↑ Std.↑ min-MAE↓ Corr.↑ Std.↑ Cont-BLEU2↑\nground truth n/a .000 .463 1.32 .000 .520 .163 1.000\nresynthesized ✓ .000 .464 1.32 .000 .315 .145 .943\nLarge MS-TLM, HuBERT units, trained on LL6k,∆ = 1\n9 u (u,d,lf) .542 .176 .942 .084 .093 .081 .488\n10 u (u,d,lf) ✓ .542 .086 .965 .096 .217 .147 .489\n11 (u,d,lf) ( u,d,lf) .539 .344 .940 .081 .494 .076 .498\n12 (u,d,lf) ( u,d,lf) ✓ .536 .242 .946 .077 .324 .149 .499\nTable 2: Continuation metrics on Librispeech test-clean. The temperature and the scale parameters used for sam-\npling are selected using the dev-clean set. We compare the four large MS-TLM models from Table 1 here. For\neach utterance in the test set, we use the ﬁrst 3 seconds as the prompt and sample 20 continuations.\nID Input Output Quant? Mean Opinion ScoreMOS M-MOS P-MOS\nresynthesized 3.21±0.09 3.95±0.32 3.87±0.45\nLarge MS-TLM, HuBERT units, trained on LL6k,∆ = 19 u (u,d,lf) 3.16±0.19 3.80±0.25 3.69±0.4210 u (u,d,lf) ✓ 2.66±0.18 3.36±0.40 3.15±0.5211 (u,d,lf) (u,d,lf) 3.31±0.23 3.76±0.273.78±0.4612 (u,d,lf) (u,d,lf) ✓ 3.43±0.20 4.04±0.203.75±0.48\nTable 3: Human evaluation on sound quality (MOS),\nmeaningfulness (M-MOS), and prosody (P-MOS).\n±indicates 95% CI.\nMAE (0.722 →0.551). The trend follows when\nfurther increasing ∆. We also observe that having\nprosody in the context is beneﬁcial when modeling\nprosody itself. Indeed, this is the case across all\npairs of models (rows 9 & 11, 10 & 12) according\nto dMAE and lf MAE metrics. Moreover, this\nholds for the types of units that differ from Hu-\nBERT (CPC: rows 13 & 14, phonetic units: rows\n15 & 16).\n5.2 Prosodic Inputs Are Useful for Speech\nGeneration\nIn our next experiment we study how the number\nof sampled prompt continuation affects prosody\naccuracy metrics (MAE). We report results for the\nfour large models (rows 9-12) in Figure 4. From\nthese results we observe that models that operate\non quantized prosodic streams greatly beneﬁt from\nsampling multiple candidates. In contrast, the two\ncontinuous-valued models seem to beneﬁt little if at\nall (in the case of the F0 stream). We hypothesise\nthat this striking difference is due to the ability\nof the multinomial-sampled trajectories to cover\nmultiple mode of the underlying distribution, while\nthe continuous-valued models produce samples that\nare “averaged” to the median of the underlying\ndistribution due to the L1 loss.\nIn Table 2 we report the continuation metrics for\nfour large MS-TLM models, trained on HuBERT\n5 10 15 20\n# of examples\n0.52\n0.53\n0.54\n0.55\n0.56Duration MAE\n5 10 15 20\n# of examples\n0.080\n0.085\n0.090\n0.095\n0.100\n0.105F0 MAE\n#9: inp=u; no-quant\n#10: inp=u; quant\n#11: inp=u,d,lf; no-quant\n#12: inp=u,d,lf; quant\nFigure 4: Minimal-MAE of duration (left) and pitch\n(right) with respect to different number of samples for\nthe four large models (ID 9-12).\ntranscripts of LL-6k (they correspond to rows 9-12\nin Table 1).3 These models differ in whether they\nhave prosodic input or not (rows 11 & 12 vs. 9 &\n10) and if the prosodic channels are discretized or\nnot (10 & 12 vs. 9 & 11).\nFirstly, on comparing models with and without\nprosodic input, we observe that having prosody in\ninput improves the accuracy of the prosody continu-\nation (in terms of MAE). This holds for predicting\nduration (e.g., 0.542 and 0.536 for rows 10 and\n12). We see a higher relative difference for lf (e.g.,\n0.096 vs. 0.077, same models). Our proposed mod-\nels are also able to leverage provided prosody input\nto maintain high consistency of the prosody contin-\nuation, as measured by the correlation metrics. For\nexample, for the continuous-prosody models the\ncorrelation values grows from 0.176 to 0.344 for\nthe duration prediction and from 0.093 to 0.494 for\nthe F0 channel. Having prosody input also turns out\nto be important for the word-level BLEU metric:\nmodels 11 and 12 outperform their counterparts\nwithout prosody inputs, 9 and 10.\nNext, when contrasting discrete- and continuous-\nprosody models the following picture emerges. For\n3Audios samples of speech continuation are included in\nthe supplementary material.\n8673\nboth duration and F0 channels, discrete models\nachieve lower min-MAE errors. Further, both dis-\ncrete models generate considerably more diverse\nF0 values than either of the continuous models (up\nto 2x higher std). Among the models with prosody\ninputs, the one with discrete prosody get higher\nvariability in the dchannel. In contrast, the corre-\nlation metrics favor the prosody-aware continuous\nmodel. From the point of view of the word-level\nBLEU scores, both models are very close with the\nquantized model (row 12) being slightly ahead. We\nattribute this difference between the models to the\nability of discrete-valued MS-TLM to better de-\nscribe multi-modal distributions, as we saw above\nin the experiment reported in Figure 4.\nTable 3 presents the human evaluation results.\nThe model with prosody input and quantized\nprosody performs signiﬁcantly better than the rest\non MOS and M-MOS, and is on par with the vari-\nant with prosody input and continuous prosody on\nP-MOS. Note that when not having the prosody\ninput, the model with quantized prosody performs\nsigniﬁcantly worse on all metrics, demonstrating\nthe importance of auto-regressive generation for\ndiscrete representation.\nTo summarize, we conclude that (i) including\nprosody input allows better modelling of speech,\nand (ii) architectures that operate with quantized\nprosody values, generally, perform better on our\nintroduced metrics.\n6 Conclusion and Future Work\nIn this work, we propose a text-free prosody-aware\ngenerative spoken language model, pGSLM, which\nmodels textual content and prosodic information\nexplicitly and does not use any text supervision\nby leveraging self-supervised units. Through ex-\ntensive evaluation on a diverse set of metrics, we\ndemonstrated that prosody not only improves con-\ntent modeling, but also enables better prompted\nspeech generation that is aware of both the content\nand the prosody from the prompt for the ﬁrst time\nin the literature. We conducted a number of abla-\ntion studies to validate the effectiveness of model\ndesign choices.\nAs for broader impacts, this work serves as the\nfoundation for building better conditional speech\ngeneration applications where prosody is essen-\ntial, such as in the conversational scenarios. In\naddition, the proposed model could also serve as\na pre-trained model for other classiﬁcation tasks,\nsuch as emotion recognition or syntactic pars-\ning from speech, or as a pre-trained model for\ngenerative tasks such as text-to-speech synthe-\nsis with more expressive and coherent prosody.\nFinally, the proposed prosody metrics (teacher-\nforcing duration and pitch MAE, continuation cor-\nrectness/consistency/expressiveness) may also be\nused for evaluation of text-to-speech synthesis sys-\ntems that can produce diverse prosody for a given\ntext input.\n8674\nReferences\nAdaeze Adigwe, Noé Tits, Kevin El Haddad, Sarah Os-\ntadabbas, and Thierry Dutoit. 2018. The emotional\nvoices database: Towards controlling the emotion di-\nmension in voice generation systems. arXiv preprint\narXiv:1806.09514.\nAlexei Baevski, Henry Zhou, Abdelrahman Mohamed,\nand Michael Auli. 2020. wav2vec 2.0: A frame-\nwork for self-supervised learning of speech represen-\ntations. arXiv preprint arXiv:2006.11477.\nDouglas Biber. 1991. Variation across speech and writ-\ning. Cambridge University Press.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Proceedings of the 34th International Confer-\nence on Neural Information Processing Systems.\nWei Chu and Abeer Alwan. 2009. Reducing f0 frame\nerror of f0 tracking algorithms under noisy con-\nditions with an unvoiced/voiced classiﬁcation fron-\ntend. In 2009 IEEE International Conference on\nAcoustics, Speech and Signal Processing , pages\n3969–3972. IEEE.\nYu-An Chung, Wei-Ning Hsu, Hao Tang, and James\nGlass. 2019. An unsupervised autoregressive model\nfor speech representation learning. In Proc. INTER-\nSPEECH, pages 146–150.\nJacob T Cohen, Alma Cohen, Limor Benyamini, Yossi\nAdi, and Joseph Keshet. 2019. Predicting glottal clo-\nsure insufﬁciency using fundamental frequency con-\ntour analysis. Head & neck, 41(7):2324–2331.\nAnne Cutler, Delphine Dahan, and Wilma Van Donse-\nlaar. 1997. Prosody in the comprehension of spoken\nlanguage: A literature review. Language and speech,\n40(2):141–201.\nWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hu-\nbert Tsai, Kushal Lakhotia, Ruslan Salakhutdi-\nnov, and Abdelrahman Mohamed. 2021a. Hubert:\nSelf-supervised speech representation learning by\nmasked prediction of hidden units. arXiv preprint\narXiv:2106.07447.\nWei-Ning Hsu, Anuroop Sriram, Alexei Baevski, Ta-\ntiana Likhomanenko, Qiantong Xu, Vineel Pratap,\nJacob Kahn, Ann Lee, Ronan Collobert, Gabriel\nSynnaeve, et al. 2021b. Robust wav2vec 2.0: An-\nalyzing domain shift in self-supervised pre-training.\nIn Interspeech.\nWei-Ning Hsu, Yao-Hung Hubert Tsai, Benjamin\nBolte, Ruslan Salakhutdinov, and Abdelrahman Mo-\nhamed. 2021c. HuBERT: How much can a bad\nteacher beneﬁt ASR pre-training? In Neural In-\nformation Processing Systems Workshop on Self-\nSupervised Learning for Speech and Audio Process-\ning Workshop, pages 6533–6537.\nSongfang Huang and Steve Renals. 2007. Using\nprosodic features in language models for meetings.\nIn International Workshop on Machine Learning for\nMultimodal Interaction, pages 192–203. Springer.\nKeith Ito and Linda Johnson. 2017. The lj\nspeech dataset. https://keithito.com/\nLJ-Speech-Dataset/.\nJacob Kahn et al. 2020. Libri-light: A benchmark for\nasr with limited or no supervision. In ICASSP.\nEugene Kharitonov, Morgane Rivière, Gabriel Syn-\nnaeve, Lior Wolf, Pierre-Emmanuel Mazaré,\nMatthijs Douze, and Emmanuel Dupoux. 2021.\nData augmenting contrastive learning of speech\nrepresentations in the time domain. arXiv preprint\narXiv:2007.00991, pages 215–222.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nKushal Lakhotia, Evgeny Kharitonov, Wei-Ning Hsu,\nYossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh\nNguyen, Jade Copet, Alexei Baevski, Adelrahman\nMohamed, et al. 2021. Generative spoken lan-\nguage modeling from raw audio. arXiv preprint\narXiv:2102.01192.\nAdrian Ła ´ncucki. 2021. Fastpitch: Parallel text-to-\nspeech with pitch prediction. In ICASSP 2021-\n2021 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , pages\n6588–6592. IEEE.\nM. Paul Lewis, Gary F. Simon, and Charles D. Fennig.\n2016. Ethnologue: Languages of the World, Nine-\nteenth edition. SIL International. Online version:\nhttp://www.ethnologue.com.\nShaoshi Ling and Yuzong Liu. 2020. DeCoAR\n2.0: Deep contextualized acoustic representa-\ntions with vector quantization. arXiv preprint\narXiv:2012.06659.\nA. T. Liu, S. Yang, P. Chi, P. Hsu, and H. Lee.\n2020. Mockingjay: Unsupervised speech represen-\ntation learning with deep bidirectional transformer\nencoders. In IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP),\npages 6419–6423.\nTomohiro Nakatani, Shigeaki Amano, Toshio Irino,\nKentaro Ishizuka, and Tadahisa Kondo. 2008. A\nmethod for fundamental frequency estimation and\nvoicing decision: Application to infant utterances\nrecorded in real acoustical environments. Speech\nCommunication, 50(3):203–214.\n8675\nAaron van den Oord, Sander Dieleman, Heiga Zen,\nKaren Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew Senior, and Koray\nKavukcuoglu. 2016. WaveNet: A generative model\nfor raw audio. arXiv preprint arXiv:1609.03499.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals.\n2018. Representation learning with contrastive pre-\ndictive coding. arXiv preprint arXiv:1807.03748.\nMari Ostendorf, Izhak Shafran, and Rebecca Bates.\n2003. Prosody models for conversational speech\nrecognition.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and\nSanjeev Khudanpur. 2015. Librispeech: an asr\ncorpus based on public domain audio books. In\nICASSP.\nAdam Polyak, Yossi Adi, Jade Copet, Eugene\nKharitonov, Kushal Lakhotia, Wei-Ning Hsu, Abdel-\nrahman Mohamed, and Emmanuel Dupoux. 2021.\nSpeech resynthesis from discrete disentangled self-\nsupervised representations. In Proc. INTER-\nSPEECH.\nAdam Polyak, Lior Wolf, Yossi Adi, and Yaniv Taig-\nman. 2020. Unsupervised cross-domain singing\nvoice conversion. In Proc. INTERSPEECH, pages\n801–805.\nDaniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas\nBurget, Ondrej Glembek, Nagendra Goel, Mirko\nHannemann, Petr Motlicek, Yanmin Qian, Petr\nSchwarz, et al. 2011. The kaldi speech recogni-\ntion toolkit. In IEEE 2011 workshop on automatic\nspeech recognition and understanding, CONF. IEEE\nSignal Processing Society.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nYi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao,\nZhou Zhao, and Tie-Yan Liu. 2020. Fastspeech\n2: Fast and high-quality end-to-end text to speech.\narXiv preprint arXiv:2006.04558.\nF. Ribeiro, D. Florêncio, C. Zhang, and M. Seltzer.\n2011. CROWDMOS: An approach for crowdsourc-\ning mean opinion score studies. In IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 2416–2419.\nMorgane Rivière and Emmanuel Dupoux. 2020. To-\nwards unsupervised learning of speech features in\nthe wild. In IEEE Spoken Language Technology\nWorkshop (SLT), pages 156–163.\nSteffen Schneider, Alexei Baevski, Ronan Collobert,\nand Michael Auli. 2019. wav2vec: Unsupervised\npre-training for speech recognition. arXiv preprint\narXiv:1904.05862.\nElizabeth Shriberg and Andreas Stolcke. 2004.\nProsody modeling for automatic speech recognition\nand understanding. In Mathematical Foundations of\nSpeech and Language Processing, pages 105–114.\nSpringer.\nElizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-\nTür, and Gökhan Tür. 2000. Prosody-based auto-\nmatic segmentation of speech into sentences and top-\nics. Speech communication, 32(1-2):127–154.\nElizabeth Shriberg, Andreas Stolcke, Daniel Jurafsky,\nNoah Coccaro, Marie Meteer, Rebecca Bates, Paul\nTaylor, Klaus Ries, Rachel Martin, and Carol Van\nEss-Dykema. 1998. Can prosody aid the auto-\nmatic classiﬁcation of dialog acts in conversational\nspeech? Language and speech, 41(3-4):443–492.\nYi Su and Frederick Jelinek. 2008. Exploiting prosodic\nbreaks in language modeling with random forests.\nIn Speech Prosody. Citeseer.\nSynSIG. Blizzard Challenge 2013. https:\n//www.synsig.org/index.php/\nBlizzard_Challenge_2013. Accessed:\n2021-08-23.\nAndros Tjandra, Ruoming Pang, Yu Zhang, and\nShigeki Karita. 2020. Unsupervised learning of dis-\nentangled speech content and style representation.\narXiv preprint arXiv:2010.12973.\nTrang Tran, Shubham Toshniwal, Mohit Bansal, Kevin\nGimpel, Karen Livescu, and Mari Ostendorf. 2017.\nParsing speech: a neural approach to integrating\nlexical and acoustic-prosodic information. arXiv\npreprint arXiv:1704.07287.\nAaron van den Oord, Oriol Vinyals, et al. 2017. Neural\ndiscrete representation learning. In NeurIPS.\nChristophe Veaux, Junichi Yamagishi, Kirsten Mac-\nDonald, et al. 2016. Superseded-cstr vctk corpus:\nEnglish multi-speaker corpus for cstr voice cloning\ntoolkit.\nYuxuan Wang, RJ Skerry-Ryan, Daisy Stanton,\nYonghui Wu, Ron J Weiss, Navdeep Jaitly,\nZongheng Yang, Ying Xiao, Zhifeng Chen, Samy\nBengio, et al. 2017. Tacotron: Towards end-to-end\nspeech synthesis. arXiv preprint arXiv:1703.10135.\nNigel Ward and Wataru Tsukahara. 2000. Prosodic fea-\ntures which cue back-channel responses in english\nand japanese. Journal of pragmatics, 32(8):1177–\n1207.\nNigel G Ward, Alejandro Vega, and Timo Baumann.\n2012. Prosodic and temporal features for lan-\nguage modeling for dialog. Speech Communication,\n54(2):161–174.\n8676\nShu-Wen Yang, Po-Han Chi, Yung-Sung Chuang,\nCheng-I Jeff Lai, Kushal Lakhotia, Yist Y . Lin,\nAndy T. Liu, Jiatong Shi, Xuankai Chang, Guan-\nTing Lin, Tzu-Hsien Huang, Wei-Cheng Tseng,\nKo-tik Lee, Da-Rong Liu, Zili Huang, Shuyan\nDong, Shang-Wen Li, Shinji Watanabe, Abdelrah-\nman Mohamed, and Hung-yi Lee. 2021. SU-\nPERB: speech processing universal performance\nbenchmark. CoRR, abs/2105.01051.\n8677\nA Analysis of Log F0 Distribution\nFigure A.1: Log F0 distribution of each (speaker, emo-\ntion) combination in the EmoV dataset without speaker\nmean normalization (top) and with speaker normaliza-\ntion (bottom). Each color corresponds to one emotion.\nFigure A.2: Speaker log F0 mean and standard devia-\ntion distributions from two expressive datasets (EmoV\nand Blizzard 2013) and two plain datasets (LJSpeech\nand VCTK). Each point corresponds to one speaker.\nB HiFi-GAN Adaptation Analysis\nTable 4 presents an analysis of HiFi-GAN perfor-\nmance when using different quantized pitch rep-\nresentations. Similarly to (Polyak et al., 2020)\nwe report voice decision error (VDE) (Nakatani\net al., 2008), which measures the portion of frames\nwith voicing decision error and F0 Frame Error\nFigure A.3: Speaker linear F0 mean and standard devi-\nation distributions on VCTK. Each point corresponds\nto one speaker.\n(FFE) (Chu and Alwan, 2009), which measures the\nportion of frames that contain a deviation of more\nthan 20% in pitch value or have a voicing deci-\nsion error. Results show that the chosen quantizer\nachieve favorable performance in terms of VDE\nand comparable results in terms of FFE without\nhaving to pre-train a F0 VQ-V AE quantizer.\nC Example of Converting Frame-Level\nto Segment-Level Representations\nAssume we have an utterance of six frames: [(13,\n1.5), (13, 2.5), (13, 0.0), (21, 0.0), (27, 1.3), (27,\n3.5)] where the ﬁrst number in each tuple denotes\nthe unit of the frame and the second number de-\nnotes the speaker normalized log F0 of the frame.\nIn particular, the third and the fourth frame are\nunvoiced and their lf values are set to 0.0.\nThe segment level representation of the utterance\nis [(13, 3, 2.0), (21, 1, 0.0), (27, 2, 2.4)]. The ﬁrst\nsegment (13, 3, 2.0) is labeled with unit u = 13,\nduration d= 3 frames, and an average normalized\nlog F0 lf = (1.5+2.5)/2 = 2.0 for the two voiced\nframes. The second segment contains only one\nunvoiced frame, and hence lf is set to 0. Finally,\nthe last segment contains two voiced frames, and\ntherefore d= 2 and lf = (1.3 + 3.5)/2 = 2.4.\nD Effects of F0 Representation on\nMS-TLM\nTable 5 compares content modeling performance\nwhen using different pitch representations. Results\nshow that using mean normalized pitch information\nis better than using raw pitch, and using log pitch\nis better than using linear pitch.\n8678\nF0 FFE↓ VDE↓scale norm. res. quant.\n(Polyak et al., 2021)\nlin mean+std frm VQ-V AE (V) 0.223 0.169\nlin mean+std frm VQ-V AE (B) 0.198 0.181\nlin mean+std frm naive 0.172 0.138\nlin mean+std seg VQ-V AE (V) 0.149 0.116\nlin mean frm VQ-V AE (V) 0.220 0.178\nlog mean+std frm VQ-V AE (V) 0.388 0.188\n(This work)\nlog mean seg naive 0.134 0.118\nTable 4: Speech resynthesis results on the Blizzard\n2013 validation set with segment-level pitch informa-\ntion. All the HiFi-GAN models are trained on the Bliz-\nzard 2013 training set using the same HuBERT units\nbut different quantized pitch representations. “scale”\ndenotes the F0 scale, which is linear (lin) or logarithmic\n(log). “norm.” denotes the normalization method ap-\nplied to F0, which is normalizing by mean or by mean\nand standard deviation (mean+std). “res.” denotes the\nresolution of pitch in the training time, where “ frm”\nrefers to using frame-level pitch and “seg” refers to us-\ning segment-level pitch (pitch values are set to the aver-\nage for all frames within a segment). “quant.” denotes\nthe F0 quantizer, where VQ-VAE (V)is a neural pitch\nquantizer pre-trained on VCTK,VQ-VAE (B)is one pre-\ntrained on Blizzard, andnaive is the one adopted in this\nwork.\nE More Details of Human Evaluation\nThe instruction page displayed to the raters are\nshown in Figure E.1. We modify the Introduc-\ntion, Task Instruction, Example in the instruction\npage for MOS, MMOS, and PMOS correspond-\ningly. The text used for each metric are detailed in\nTable 6\nF0 scale F0 norm. uNLL\nlinear none 1.763\nlinear mean 1.564\nlog none 1.461\n(This work)\nlog mean 1.447\nTable 5: Content modeling performance of base MS-\nTLM models trained on LS960 with HuBERT units us-\ning different pitch representations without quantization.\n“scale” denotes the F0 scale, which is linear ( lin) or\nlogarithmic (log). “norm.” denotes the normalization\nmethod applied to F0, which is not normalizing (none)\nor normalizing by mean.\n8679\nFigure E.1: The instruction page for human evaluation on MOS.\n8680\nMetric Introduction Metric-Speciﬁc Task Instruction\nMOS Your task is to evaluate the subjective\nquality of the speech from short (2-8\nsecond) audio ﬁles. Each HIT can be\ncompleted in roughly around 120 sec-\nonds.\n...The CONTINUATION has been generated\nby a computer and your task will be to concen-\ntrate speciﬁcally on it and evaluate its quality\nin terms of the sound clarity on a 1 to 5 scale,\n(irrespective of the intonation or meaning)\nMMOS Your task is to evaluate the subjective\nmeaningfulness of the speech from\nshort (2-8 second) audio ﬁles. Each HIT\ncan be completed in roughly around 120\nseconds.\n...The CONTINUATION has been generated\nby a computer and your task will be to concen-\ntrate speciﬁcally on it and evaluate its mean-\ning in terms of grammar and content on a\n1 to 5 scale, (irrespective of sound clarity or\nintonation)\nPMOS Your task is to evaluate the subjective\nprosodic coherence of the speech from\nshort (2-8 second) audio ﬁles. Each HIT\ncan be completed in roughly around 120\nseconds.\n...The CONTINUATION has been generated\nby a computer and your task will be to concen-\ntrate speciﬁcally on it and evaluate its natu-\nrality in terms of intonation and rhythm on\na 1 to 5 scale, (irrespective of sound clarity or\nmeaning)\nTable 6: The Introduction and Task Instruction used for the three human evaluation metrics. All the Task In-\nstruction starts with “In this task, you will hear samples of speech recordings, composed of the following parts:\n[PROMPT] beep [PROMPT CONTINUATION]. [PROMPT] is a sentence beginning, said by one voice. (“AN-\nOTHER PREACHER AFTER REPROACHING HIM” in a male voice). Beep is a short tone. [PROMPT CON-\nTINUATION] a longer sentence in a different voice, which starts with the initial prompt (“ANOTHER PREACHER\nAFTER REPROACHING HIM” in a female voice) and adds a few more words that continue this prompt (“TO HIS\nFACE WITH HIS MISGOVERNMENT ORDERED THIS PSALM TO BE SUNG” in the same female voice).”\n8681",
  "topic": "Generative grammar",
  "concepts": [
    {
      "name": "Generative grammar",
      "score": 0.7248908281326294
    },
    {
      "name": "Prosody",
      "score": 0.6921595335006714
    },
    {
      "name": "Linguistics",
      "score": 0.6103901863098145
    },
    {
      "name": "Natural language processing",
      "score": 0.5474156141281128
    },
    {
      "name": "Computer science",
      "score": 0.523871123790741
    },
    {
      "name": "Artificial intelligence",
      "score": 0.515052080154419
    },
    {
      "name": "Computational linguistics",
      "score": 0.4878247082233429
    },
    {
      "name": "Speech recognition",
      "score": 0.3821752369403839
    },
    {
      "name": "Philosophy",
      "score": 0.22453615069389343
    }
  ]
}