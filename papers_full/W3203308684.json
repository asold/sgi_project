{
    "title": "TEMGNet: Deep Transformer-based Decoding of Upperlimb sEMG for Hand Gestures Recognition",
    "url": "https://openalex.org/W3203308684",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5045542279",
            "name": "Elahe Rahimian",
            "affiliations": [
                "Concordia University"
            ]
        },
        {
            "id": "https://openalex.org/A5074844055",
            "name": "Soheil Zabihi",
            "affiliations": [
                "Concordia University"
            ]
        },
        {
            "id": "https://openalex.org/A5059155086",
            "name": "Amir Asif",
            "affiliations": [
                "Concordia University"
            ]
        },
        {
            "id": "https://openalex.org/A5065669889",
            "name": "Dario Farina",
            "affiliations": [
                "Concordia University"
            ]
        },
        {
            "id": "https://openalex.org/A5057991229",
            "name": "S. Farokh Atashzar",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5058253407",
            "name": "Arash Mohammadi",
            "affiliations": [
                "New York University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2796589614",
        "https://openalex.org/W2103403570",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3162754028",
        "https://openalex.org/W2775447708",
        "https://openalex.org/W3173912422",
        "https://openalex.org/W2792764867",
        "https://openalex.org/W2169931829",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3128094226",
        "https://openalex.org/W3157280810",
        "https://openalex.org/W2981857663",
        "https://openalex.org/W2898716605",
        "https://openalex.org/W2516710120",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3016127821",
        "https://openalex.org/W2129566274",
        "https://openalex.org/W2990110246",
        "https://openalex.org/W2555541061",
        "https://openalex.org/W1857789879",
        "https://openalex.org/W3180620879",
        "https://openalex.org/W3148185292",
        "https://openalex.org/W2997725772",
        "https://openalex.org/W3004392413",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2741461284",
        "https://openalex.org/W3161020793"
    ],
    "abstract": "There has been a surge of recent interest in Machine Learning (ML), particularly Deep Neural Network (DNN)-based models, to decode muscle activities from surface Electromyography (sEMG) signals for myoelectric control of neurorobotic systems. DNN-based models, however, require large training sets and, typically, have high structural complexity, i.e., they depend on a large number of trainable parameters. To address these issues, we developed a framework based on the Transformer architecture for processing sEMG signals. We propose a novel Vision Transformer (ViT)-based neural network architecture (referred to as the TEMGNet) to classify and recognize upperlimb hand gestures from sEMG to be used for myocontrol of prostheses. The proposed TEMGNet architecture is trained with a small dataset without the need for pre-training or fine-tuning. To evaluate the efficacy, following the-recent literature, the second subset (exercise B) of the NinaPro DB2 dataset was utilized, where the proposed TEMGNet framework achieved a recognition accuracy of 82.93% and 82.05% for window sizes of 300ms and 200ms, respectively, outperforming its state-of-the-art counterparts. Moreover, the proposed TEMGNet framework is superior in terms of structural capacity while having seven times fewer trainable parameters. These characteristics and the high performance make DNN-based models promising approaches for myoelectric control of neurorobots.",
    "full_text": "IEEE ROBOTICS AND AUTOMATION LETTERS 1\nTEMGNet: Deep Transformer-based Decoding of\nUpperlimb sEMG for Hand Gestures Recognition\nElahe Rahimian1, Student Member, IEEE, Soheil Zabihi 2, Student Member, IEEE, Amir Asif 2, Senior Member,\nIEEE, Dario Farina 3, Fellow, IEEE, S. Farokh Atashzar 4, Senior Member, IEEE, and Arash Mohammadi 1, Senior\nMember, IEEE,\nAbstract—There has been a surge of recent interest in Machine\nLearning (ML), particularly Deep Neural Network (DNN)-based\nmodels, to decode muscle activities from surface Electromyog-\nraphy (sEMG) signals for myoelectric control of neurorobotic\nsystems. DNN-based models, however, require large training sets\nand, typically, have high structural complexity, i.e., they depend\non a large number of trainable parameters. To address these\nissues, we developed a framework based on the Transformer\narchitecture for processing sEMG signals. We propose a novel\nVision Transformer (ViT)-based neural network architecture\n(referred to as the TEMGNet) to classify and recognize upper-\nlimb hand gestures from sEMG to be used for myocontrol\nof prostheses. The proposed TEMGNet architecture is trained\nwith a small dataset without the need for pre-training or ﬁne-\ntuning. To evaluate the efﬁcacy, following the recent literature,\nthe second subset (exercise B) of the NinaPro DB2 dataset was\nutilized, where the proposed TEMGNet framework achieved a\nrecognition accuracy of 82.93% and 82.05% for window sizes of\n300ms and 200ms, respectively, outperforming its state-of-the-art\ncounterparts. Moreover, the proposed TEMGNet framework is\nsuperior in terms of structural capacity while having seven times\nfewer trainable parameters. These characteristics and the high\nperformance make DNN-based models promising approaches for\nmyoelectric control of neurorobots.\nIndex Terms—Neurorobotics, Attention Mechanism, Myoelec-\ntric Control, Electromyogram (EMG), Vision Transformer (ViT).\nI. I NTRODUCTION\nR\nECOGNIZING limb motions using surface Electromyog-\nraphy (sEMG) signals allows for the control of rehabilita-\ntion and assistive systems (such as bionic limbs and exoskele-\ntons). Surface EMG signals are obtained non-invasively by\nsensors on the skin surface that measure the electrical activity\nof the muscle’s motor units [1], [2]. The information obtained\nfrom sEMG signals is used to decode discriminative and\nrepeatable patterns that can be utilized to effectively classify\nthe intended motor commands of the user. In this context,\n1Elahe Rahimian and Arash Mohammadi are with Concordia Institute for\nInformation System Engineering (CIISE), Concordia University, Montreal,\nCanada {e_ahimia,arashmoh}@encs.concordia.ca\n2Soheil Zabihi and Amir Asif are with Electrical and Computer\nEngineering (ECE), Concordia University, Montreal, Canada\n{s_zab,a_asif}@encs.concordia.ca\n3D. Farina is with the Department of Bioengineering, Imperial College\nLondon, London, UK, SW7-2AZ d.farina@imperial.ac.u\n4S. Farokh Atashzar is with Electrical and Computer Engineering and Me-\nchanical and Aerospace Engineering, New York University (NYU), Brooklyn,\nNY , USA, 11201 sfa7@nyu.edu\nThis project was partially supported by the Department of National De-\nfence’s Innovation for Defence Excellence & Security (IDEaS), Canada.\nseveral attempts have been made to classify hand movements\nusing traditional Machine Learning (ML) methods. While\nthese conventional approaches (such as Linear Discriminant\nAnalysis (LDA) and Support Vector Machine (SVM) [3])\nhave been successfully implemented, their performance might\ndegrade when applied to a large-scale data set consisting\nof a sizable number of movements. This has motivated a\nrecent surge of interest in applying Deep Neural Networks\n(DNNs) within this domain, with the aim of addressing the\nshortcomings of traditional ML solutions.\nProcessing sEMG signals using DNN architectures has\nthe potentials to provide signiﬁcantly improved performance.\nConvolutional Neural Network (CNN) is the commonly used\nDNN architecture for recognizing upper-limb hand gestures in\nwhich sEMG signals are translated into images [4]–[6]. CNN-\nbased models, however, target learning spatial features and are\nineffective for the extraction of temporal features from sEMG\nsequential data. Recurrent Neural Networks (RNNs), such\nas Long Short Term Memory (LSTM), have been therefore\nproposed in recent studies [7], [8] to capture the temporal\ninformation from sEMG signals. To capture both spatial and\ntemporal characteristics of sEMG signals, LSTM and CNN\ncan be combined [9], resulting in a hybrid solution. For\nexample, the authors in Reference [10] composed six image\nrepresentations of raw sEMG signals that were then fed as\ninput to a hybrid CNN-LSTM architecture. Additionally, it\nwas shown that dilated causal convolutions in CNN-based\narchitectures [11], [12] have great potentials to surpass the\npure RNN architecture in term of overall accuracy. Finally,\ndilated LSTM models [8] have shown the potential to enhance\naccuracy and reduce computational cost.\nWhile the above-mentioned models are advanced ap-\nproaches to sequence modeling, they do not allow paralleliza-\ntion during the training phase due to their sequential nature.\nFor this reason, recurrent-based models are slow and difﬁcult\nto train. The Transformer neural network architecture [13]\nhas been proposed to eliminate recurrence or convolution\nusing a self-attention mechanism. In brief, the main building\nblock of the transformer architecture, making it a unique\nand powerful DNN model, is the attention mechanism, which\nis a way to mimic and execute the function of selective\nfocus. Transformers were ﬁrst applied to Natural Language\nProcessing (NLP) tasks, with the goal of solving sequence-by-\nsequence tasks while handling long-range dependencies [13].\nThe transformer model architectures, such as Bidirectional\nEncoder Representations from Transformers (BERT) [14] and\narXiv:2109.12379v1  [cs.LG]  25 Sep 2021\n2 IEEE ROBOTICS AND AUTOMATION LETTERS\nFig. 1. Exercise B of Ninapro database [23]–[25].\nGenerative Pre-Training (GPT) [15], achieved state-of-the-\nart results in different NLP tasks. In addition to the NLP\nﬁeld, Transformers have been applied to address a variety of\nother problems, including Computer Vision (CV) tasks [16];\nElectroencephalogram (EEG)-based speech recognition [17];\nEEG decoding [18]; Electrocardiogram (ECG)-based heartbeat\nclassiﬁcation [19]; sleep stages classiﬁcation [20], acoustic\nmodeling [21], and ECG classiﬁcation [22].\nIn this paper, we hypothesize that novel models, designed\non the basis of transformers, would enhance the accuracy\nof EMG classiﬁcation, and reduce the training process. This\nwill be a major step toward the ultimate utilization of deep\nlearning models for commercial prosthetic systems. There-\nfore, here we propose, design, and evaluate the performance\nof a novel transformer model for hand gesture recognition.\nThe proposed TEMGNet architecture was designed based on\nthe ViT architecture to improve recognition accuracy, speed\nof training, and to reduce structural complexity. Generally\nspeaking, training transformers requires large datasets. In this\nstudy, we illustrate that transformers can be trained with\na small sEMG dataset without the need to use pre-trained\nmodels and ﬁne-tuning. We show that the proposed TEMGNet\narchitecture outperforms its state-of-the-art counterparts in\nterms of overall recognition accuracy and complexity. The\nproposed transformer-based architecture improves the recog-\nnition accuracy compared to its state-of-the-art counterparts\n(where LSTM or hybrid LSTM-CNN are adopted) with the\nsigniﬁcantly reduced number of trainable parameters. The\nproposed TEMGNet architecture was evaluated based on the\nsecond benchmark Ninapro database, in which we aimed to\nclassify 17 hand gestures from raw, sparse multichannel sEMG\nsignals. The details of the Ninapro database are provided in\nSubsection II-A. We examined the effect of several variants\nof the TEMGNet architecture on both recognition accuracy\nand the number of required trainable parameters. We also\nconducted statistical tests to assess the signiﬁcance of the\nproposed architecture. Finally, we present a visualization of\nposition embedding similarities, showing that the model is\ncapable of encoding position information and considering the\nsequential nature of sEMG signals.\nII. M ATERIAL AND METHODS\nFirst, we describe the database based on which the proposed\nmodel was evaluated. Then, the pre-processing method for\npreparing the dataset is explained.\nA. Database\nWe evaluated the proposed TEMGNet framework on the\nsecond Ninapro benchmark database [23]–[25] referred to as\nthe DB2, a well-known sparse multichannel sEMG dataset\nwidely used to classify hand gestures [8], [26], [27]. The DB2\ndatabase was obtained from 40 users who repeated several\nmovements 6 times, each lasting 5 seconds, followed by\n3 seconds of rest. In the DB2 dataset, muscular activities\nwere measured using 12 active double-differential wireless\nelectrodes from a Delsys Trigno Wireless EMG system at\na sampling frequency of 2 kHz. The DB2 dataset consists\nof three exercises (i.e., B, C, and D) dealing with different\ntypes of movements. In this paper, to evaluate the proposed\nmodel, Exercise B was used, which consists of 17 different\nmovements. As shown in Fig. 1, the utilized dataset consists\nof 9 basic wrist movements with 8 isometric and isotonic\nhand conﬁgurations. In order to compare and follow the\nrecommendations provided by the Ninapro database, 2/3 of\nthe gesture repetitions of each user (i.e., 1,3,4, and 6) were\nused to build the training set, and the remaining repetitions\n(i.e., 2, and 5) were used for testing.\nB. Pre-processing\nBefore performing the classiﬁcation task, the sEMG signals\nwere pre-processed. Following the recent literature [3], [5],\n[6], we applied a low-pass Butterworth ﬁlter to the signals in\norder to remove high-frequency noise. Afterward, we applied\nthe µ-law normalization to the sEMG data. This normaliza-\ntion approach has been introduced in the context of sEMG\nprocessing in Reference [4] and is deﬁned as follows\nF(xt) =sign(xt)ln\n(\n1 +µ|xt|\n)\nln\n(\n1 +µ\n) , (1)\nwhere xt represents the input scalar, and parameter µdenotes\nthe new range. Recently in [4], [26], it was shown that\nimproved performance could be achieved using normalization\nof the sEMG signals with the µ-law approach.\nIII. T HE TEMGN ET FRAMEWORK\nIn this section, we present details of the proposed TEMGNet\narchitecture designed for performing sEMG-based hand ges-\nture recognition tasks. The main fundamental block of the\nTransformer architecture is the attention mechanism. It is\nnoteworthy to mention that attention along with other ar-\nchitectures such as CNN and/or LSTM has recently been\nutilized [10], [26], [27] to classify hand movements based\non sEMG signals, where the results showed the ability of\nattention to learn the temporal information of multi-channel\nsEMG data. Unlike prior works that aimed to combine CNN\nor LSTM architectures with self-attention, in this paper, we\nwill show that the proposed ViT-based architecture, which is\nsolely based on the attention mechanism, has the capability\nto outperform the previous networks. The overall structure of\nthe proposed TEMGNet architecture is shown in Fig. 2. In\nthe following, ﬁrst, we describe the building blocks of the\nRAHIMIAN et al.: VISION TRANSFORMER FOR SEMG-BASED HAND GESTURES RECOGNITION 3\nFlatten Patches into Vectors\n(a) \nTransformer Encoder\nLinear Projection using matrix E \n0\nPatch + Position \nEmbeddings\nLinear Layer\n(LL)\nClass Label\nMulti-Layer\nPerceptron (MLP)\nLayerNorm\nMultihead Self-\nAttention (MSA)\nLayerNorm\n+ \n+ L x \n(b) Segment\n2 3 N1\nFig. 2. The proposed TEMGNet architecture: (a) Each segment of sEMG signal X is split into a sequence of ﬁxed-size non-overlapping\npatches. The constructed patches are then ﬂattened (the blue one) and projected linearly (the green block). The output of this step is referred\nto as “Patch Embedding.” Afterward, position embeddings are added to the patch embeddings. The resulting sequence of these embedded\npatches is then fed to the transformer encoder (the gray block). For the classiﬁcation, a trainable [cls] token xcls is added to the sequence.\n(b) The transformer encoder: This module consists of L layers, each consisting of two LayerNorm modules, an MLP, and an MSA module.\nproposed architecture, and then the overall structure of the\nnetwork.\nThe proposed architecture is inspired by ViT [16], which\nclosely follows the original transformer model. Within the\nTEMGNet framework, after completion of the pre-processing\nstep, the collected sEMG data was segmented via a sliding\nwindow of length 200ms with steps of 10ms (for comparison\npurposes, the results for a window of300ms are also provided).\nThe segmentation step transforms the sEMG dataset into\nD= {(Xi,yi)}M\ni=1, consisting of M segments, where the i\nth\nsegment is denoted by Xi ∈RS×W , for ( 1 ≤i ≤M), with\nits associated label denoted by yi. Here, S denotes the number\nof sensors, and W shows the number of samples collected at 2\nkHz for a window of 200ms (or 300ms). The main objective\nof the TEMGNet architecture is to learn the mapping from\nthe sequence of segment patches to its corresponding label\nyi. As shown in Fig. 2, the TEMGNet architecture consists\nof the following modules; i.e., Patch Embeddings, Position\nEmbedding, Transformer encoder, and ﬁnally a Multi-Layer\nPerceptron (MLP) head.\nA. Patch Embeddings\nAs shown in Fig. 2(a), at ﬁrst, we split the segmented input\nX (for simplicity, we drop the segment indexi) into N number\nof non-overlapping patches. Here, we set the size of each patch\nto (S×S); therefore, the number of patches will be equal to\nN = W/S. Each patch is then ﬂattened into a vector xp\nj ∈\nRS2\n, for ( 1 ≤j ≤N). A linear projection is then applied\nto embed each vector into the model dimension d. For the\nlinear projection, we used a matrix E ∈ RS2×d, which is\nshared among different patches. The output of this projection\nis called patch embeddings (Eq. 2 below).\nIn a similar way as in the BERT framework [14], the\nbeginning of the sequence of embedded patches is appended\nwith a trainable [cls] token xcls, to capture the meaning of the\nentire segmented input. Finally, we will add position embed-\ndings denoted by Epos ∈R(N+1)×d (which will be described\nnext in Sub-section III-B) to the patch embeddings that will\nallow the transformer to capture the positional information.\nThe formulation governing patch and position embeddings is\ngiven by\nZ0 = [xcls; xp\n1E; xp\n2E; ... ; xp\nN E] +Epos. (2)\n4 IEEE ROBOTICS AND AUTOMATION LETTERS\nB. Position Embeddings\nThe sEMG signal is sequential data that is presented in a\nspeciﬁc order. If we change this order, the meaning of the input\nmight also change. The transformer does not process the input\nsequentially and for each element, it combines information\nfrom the other elements through self-attention. Since the\narchitecture of the transformer does not model the positional\ninformation, there is a need to explicitly encode the order of\nthe input sequence so that the transformer knows that one piece\nis after another and not in any other permutation. This is where\npositional embedding comes in. Positional embedding is a\nform of identiﬁer, a clear reference for the transformer that en-\ncodes the location information within the sequence. Therefore,\npositional embeddings are order or position identiﬁers added\nto the input to identify the relative position of each element in\nthe sequence for the transformer. There are different ways to\nencode spatial information into the input of transformer using\npositional embedding, e.g., Sinusoid positional embedding,\nRelative positional embeddings, Convolutional embedding, 1-\ndimensional positional embedding, and 2-dimensional posi-\ntional embedding [16], [21]. Following [16], the standard\ntrainable 1-dimensional positional embeddings are used in\nthis study.\nC. Transformer Encoder\nThe resulting sequence of vectors Z0 is fed as an input\nto a standard transformer encoder. We are basically treating\nall of the constructed patches as simple tokens provided\ninto the transformer. In fact, the encoder block is similar to\nthe main transformer encoder block proposed by [13]. As\nshown in Fig. 2(b), the transformer encoder consists of L\nidentical layers. Each layer consists of two modules, i.e.,\na Multihead Self-Attention Mechanism (MSA) and an MLP\nmodule (deﬁned later in Eqs. 3, 4). MSA is built based on the\nSelf-Attention (SA) mechanism. SA and MSA are explained in\nSub-sections III-C1 and III-C2, respectively. The MLP module\nconsists of two linear layers and a Gaussian Error Linear Unit\n(GELU) activation function.\nTo address the degradation problem, a layer-\nnormalization [28] is applied, which is then followed\nby residual skip connections\nZ\n′\nl = MSA(LayerNorm(Zl−1)) +Zl−1, (3)\nZl = MLP(LayerNorm(Z\n′\nl )) +Z\n′\nl , (4)\nfor l = 1...L . The ﬁnal output of the transformer can be\nrepresented as follows\nZL = [zL0; zL1; ... ; zLN ], (5)\nwhere zL0 is used for classiﬁcation purposes. Finally, zL0 is\npassed to a Linear Layer (LL), i.e.,\ny = LL(LayerNorm(zL0). (6)\nThis completes the description of the proposed TEMGNet\narchitecture. Next, we present the description of SA and MSA,\nrespectively.\n1) Self-Attention (SA): Let us deﬁne the input sequence as\nZ ∈RN×d consisting of N vectors, each with an embedding\ndimension of d. The SA mechanism was ﬁrst introduced\nin [13]. Generally speaking, the SA mechanism is deﬁned\nwith the aim of capturing the interaction between different\nvectors in Z. In this regard, three different matrices, namely\nQueries Q, Keys K, and Values V are computed via a linear\ntransformation, i.e.,\n[Q,K,V ] =ZWQKV , (7)\nwhere WQKV ∈Rd×3dh shows the learnable weight matrix.\nHere, dh denotes the size of each vector in Q, K, and V .\nAfter that, the pairwise similarity between each query and all\nkeys is obtained using the dot-product of Q and K, which\nis then scaled by √dh and translated into the probabilities\nP ∈RN×N using the softmax function as follows\nP = softmax(QKT\n√dh\n). (8)\nFinally, for each vector in the input sequence, the correspond-\ning output vector resulted from the SA mechanism is computed\nby taking the weighted sum over all values V as follows\nSA(Z) =PV , (9)\nwhere SA(Z) ∈RN×dh. Such an attention mechanism helps\nthe model to focus on important parts from a given sEMG\ninput sequence.\n2) Multihead Self-Attention (MSA): In the MSA, the SA\nmechanism is applied h times in parallel, allowing the model\nto attend to parts of the input sequence for each head differ-\nently. More speciﬁcally, MSA consists of hheads where each\nhead has its own learnable weight matrix {WQKV\ni }h\ni=1. For\nthe input sequence Z, we applied the SA mechanism for each\nhead (Eqs. (7)-(9)). Then the outputs of h heads are concate-\nnated into a single matrix [SA1(Z); SA2(Z); ... ; SAh(Z)] ∈\nRN×h.dh and once again projected to obtain the ﬁnal values\nas follows\nMSA(Z) = [SA1(Z); SA2(Z); ... ; SAh(Z)]WMSA , (10)\nwhere WMSA ∈Rh.dh×d and dh is set to d/h, this description\ncompletes the proposed TEMGNet architecture. Next, we\npresent the results and experiments.\nIV. E XPERIMENTS AND RESULTS\nWe evaluated different variants of the TEMGNet archi-\ntecture. The results are summarized in Table I, where the\nperformance of the model for window sizes of 200ms and\n300ms is shown. For all model variants, we set the size of the\ninput patch to 12 ×12. All models were trained using Adam\noptimizer [29] with betas = ( 0.9,0.999), and the weight decay\nset to 0.001. These models were trained with a batch size of\n512. Cross-entropy loss was used for measuring classiﬁcation\nperformance.\nTable II summarizes the classiﬁcation accuracies for dif-\nferent TEMGNet architecture variants. The computed gesture\nrecognition accuracy was averaged over all subjects. As shown\nin Table II, by increasing the model dimension d from 32\nRAHIMIAN et al.: VISION TRANSFORMER FOR SEMG-BASED HAND GESTURES RECOGNITION 5\nTABLE I\nDESCRIPTIONS OF TEMGN ET ARCHITECTURE VARIANTS .\nWindow size Model ID Layers Model dimension d MLP size Heads Params\n200ms\n1 1 32 128 8 20,049\n2 2 32 128 8 32,657\n3 3 32 128 8 45,265\n4 1 64 256 8 64,625\n300ms\n1 1 32 128 8 20,593\n2 2 32 128 8 33,201\n3 3 32 128 8 45,809\n4 1 64 256 8 65,713\n(a) Window size of 200ms\n (b) Window size of 300ms\nFig. 3. The accuracy boxplots for all TEMGNet architecture variants. Each boxplot shows the IQR of each model for 40 users. The Wilcoxon signed-rank test is\nused to compare the Model 1 (with a minimum number of parameters) with other models; i.e., Model2, Model 3, and Model 4 (ns: 5.00e−02 < p ≤ 1.00e+00,\n∗ : 1.00e − 02 < p ≤ 5.00e − 02, ∗∗ : 1.00e − 03 < p ≤ 1.00e − 02, ∗∗∗ : 1.00e − 04 < p ≤ 1.00e − 03, ∗∗∗∗ : p ≤ 1.00e − 04).\nto 64 (Model 1 to Model 4), the accuracy improved approx-\nimately by 2% for both window sizes. However, Model 4\nhad higher number of trainable parameters in comparison to\nModel 1, resulting in higher complexity (Table I). In a second\nexperiment, we examined the effect of increasing the number\nof layers. As shown in Table II, for both window sizes of\n200ms and 300ms, Model 2 had a higher accuracy than Model\n1. However, by increasing the number of layers to 3, no\nimprovements had been observed in the classiﬁcation accuracy.\nWe used the statistical tests to show the signiﬁcance level of\ndifferent model variants, i.e., Model 1, 2, 3, and 4. Therefore,\nwe followed [26], [30] and used the Wilcoxon signed-rank\ntest [31], considering each participant as a separate dataset. As\nshown in Fig. 3, the difference in accuracy between Model 1\nand Model 4, for both window sizes of 200ms and 300ms was\nconsidered statistically signiﬁcant by the Wilcoxon signed-\nrank test as the ( ∗∗∗∗ : p ≤1.00e−4). In Fig. 3, a p-value is\nannotated by:\n• Not signiﬁcant (ns): 5.00e −02 <p ≤1.00e + 00,\n• ∗ : 1.00e −02 <p ≤5.00e −02,\n• ∗∗ : 1.00e −03 <p ≤1.00e −02,\n• ∗∗∗ : 1.00e −04 <p ≤1.00e −03,\n• ∗∗∗∗ : p ≤1.00e −04.s\nIn Fig. 3, the performance distribution across 40 users for\neach model is shown. The boxplot for each model shows the\nInterquartile Range (IQR), which is based on dividing the\nperformance of each model for 40 users into quartiles. The\nmedian performance is shown by a horizontal line in each\nboxplot.\nIn Table III, we provide comparisons with the state-of-\nthe-art models [8] developed recently on the same dataset to\nillustrate the superior performance of the proposed TEMGNet\n6 IEEE ROBOTICS AND AUTOMATION LETTERS\n(a) Model 1 for a Window size of 300ms\n (b) Model 4 for a Window size of 300ms\nFig. 4. Visualization of position embedding similarities for Model 1 and 4 in the Window size of 300ms. The number of samples ( W) collected at a frequency\nof 2 kHz for a window of 300ms is 600. The size of each patch is set to (12 × 12); therefore, the number of patches is N = 50; i.e., the input patch index\nis from 0 to 49. Each row in (a) and (b) represents the similarity of the position embedding vector of each patch to all positional embeddings. It is shown\nthat position embedding vectors learn distance in a segment. This means that the neighbors have higher similarities.\nTABLE II\nCLASSIFICATION ACCURACIES FOR TEMGN ET ARCHITECTURES\nVARIANTS . THE STD REPRESENTS THE STANDARD VARIATION IN\nACCURACY OVER THE 40 USERS .\nWindow size\n200ms\nModel ID 1 2 3 4\nAccuracy ( % ) 80 .39 81 .23 80 .85 82.05\nSTD ( % ) 5.86 6 .31 6 .32 5.78\nWindow size\n300ms\nModel ID 1 2 3 4\nAccuracy ( % ) 80 .88 81 .54 81 .42 82.93\nSTD ( % ) 5.97 5 .99 5 .94 5.83\narchitecture over its counterparts. The proposed TEMGNet\nmethod outperforms classical and state-of-the-art DNN-based\nmodels. More speciﬁcally, for a window size of 300ms, the\nclassiﬁcation accuracy was 82.93% with only 65,713 number\nof trainable parameters, while in Reference [8], the authors\nreached 82.4% accuracy with 466,944 number of parameters.\nMoreover, for window size 200ms, the accuracy for Model 1\nand Model 4 was 80.39% and 82.05%, respectively. However,\nwith the same window size, the best accuracy reported in\nReference [8] was 79.0%.\nFig. 4 shows the position embedding similarities for the\nwindow size of 300ms. As mentioned in Sub-section III-B,\nto encode position information, learnable position embedding\nwas added to the patch embeddings. This is a key factor for\nthe transformer to consider the sequential nature of the sEMG\nsignals. Fig. 4 shows that position embedding vectors learn\ndistance in a segment of sEMG signals. More speciﬁcally,\nthe size of each patch was set to 12 ×12. Therefore, there\nare N = 50 patches for a window of 300ms, i.e., the input\npatch index varied from 0 to 49. Each row in Fig. 4(a), (b)\nrepresents the similarity of the position embedding vector of\neach patch to all positional embeddings. It is observed that\nthe main diagonal in Fig. 4(a), (b) shows higher similarities\nbetween the neighboring ones. Moreover, position embedding\nsimilarities in Fig. 4(b) are greater than in Fig. 4(a), which\nmeans that the proposed Model 4 encode the sequential nature\nof sEMG signals better than Model 1.\nV. C ONCLUSION\nWe proposed a novel transformer-based framework for the\ntask of hand gesture recognition from sEMG signals. We\nshowed that the proposed architecture has the capacity to\nreduce the number of trainable parameters by 7 times with re-\nspect to the state of the art, while improving the performance.\nThis is a major step toward the utilization of deep learning for\nthe control of prosthetic systems.\nREFERENCES\n[1] N. Jiang, S. Dosen, K.R. Muller, D. Farina, “Myoelectric Control of\nArtiﬁcial Limbs- Is There a Need to Change Focus?” IEEE Signal\nProcess. Mag., vol. 29, pp. 150-152, 2012.\n[2] D. Farina, R. Merletti, R.M. Enoka, “The Extraction of Neural Strategies\nfrom the Surface EMG,” J. Appl. Physiol. , vol. 96, pp. 1486-95, 2004.\n[3] M. Atzori, M. Cognolato, and H. M ¨uller, “Deep Learning with Convolu-\ntional Neural Networks Applied to Electromyography Data: A Resource\nfor the Classiﬁcation of Movements for Prosthetic Hands,” Frontiers in\nneurorobotics 10, p.9, 2016.\n[4] E. Rahimian, S. Zabihi, F. Atashzar, A. Asif, A. Mohammadi, “Xception-\nTime: Independent Time-Window XceptionTime Architecture for Hand\nGesture Classiﬁcation,” International Conference on Acoustics, Speech,\nand Signal Processing (ICASSP) , 2020.\nRAHIMIAN et al.: VISION TRANSFORMER FOR SEMG-BASED HAND GESTURES RECOGNITION 7\nTABLE III\nCOMPARISON BETWEEN OUR METHODOLOGY (TEMGN ET) AND PREVIOUS WORKS [8].\nReference [8]\n200ms 300ms\nParams Accuracy ( %) Params Accuracy ( %)\n4-layer 3rd Order Dilation 79.0 466, 944 82 .4\n4-layer 3rd Order Dilation (pure LSTM) 79.7\nSVM 26.9 30.7\nOur Method\nModel 1 20,049 80.39 20,593 80.88\nModel 4 64, 625 82.05 65, 713 82.93\n[5] W. Geng, Y . Du, W. Jin, W. Wei, Y . Hu, and J. Li, “Gesture Recognition\nby Instantaneous Surface EMG Images,” Scientiﬁc reports 6 , p.36571,\n2016.\n[6] W. Wei, Y .Wong, Y . Du, Y . Hu, M. Kankanhalli, and W. Geng, “A multi-\nstream convolutional neural network for sEMG-based gesture recognition\nin muscle-computer interface.,” Pattern Recognition Letters, 2017.\n[7] F. Quivira et al., “Translating sEMG Signals to Continuous Hand Poses\nUsing Recurrent Neural Networks.,” in Proc. IEEE EMBS Int. Conf.\nBiomed. Health Informat. , 2018, pp. 166–169.\n[8] T. Sun, Q. Hu, P. Gulati, and S.F. Atashzar, “Temporal Dilation of Deep\nLSTM for Agile Decoding of sEMG: Application in Prediction of Upper-\nlimb Motor Intention in NeuroRobotics.,” IEEE Robotics and Automation\nLetters, 2021.\n[9] E. Rahimian, S. Zabihi, S. F. Atashzar, A. Asif, and A. Mohammadi,\n“Surface EMG-Based Hand Gesture Recognition via Hybrid and Dilated\nDeep Neural Network Architectures for Neurorobotic Prostheses,” Jour-\nnal of Medical Robotics Research , 2020, pp. 1-12.\n[10] Y . Hu, Y . Wong, W. Wei, Y . Du, M. Kankanhalli, and W. Geng, “A\nNovel Attention-based Hybrid CNN-RNN Architecture for sEMG-based\nGesture Recognition,” PloS one 13 , no. 10, 2018.\n[11] S. Bai, J.Z. Kolter, and V . Koltun, “ An Empirical Evaluation of Generic\nConvolutional and Recurrent Networks for Sequence Modeling,” arXiv\npreprint arXiv:1803.01271, 2018.\n[12] E. Rahimian, S. Zabihi, S. F. Atashzar, A. Asif, and A. Mohammadi,\n“Semg-based Hand Gesture Recognition via Dilated Convolutional Neural\nNetworks,” Global Conference on Signal and Information Processing,\nGlobalSIP, 2019.\n[13] A. Vaswani, N. Shazeer, J. Uszkoreit, L. Jones, A. Gomez N., L. Kaiser,\nand I. Polosukhin, “Attention is All You Need,” In Advances in neural\ninformation processing systems , 2017, pp. 5998-6008.\n[14] J. Devlin, M.W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof Deep Bidirectional Transformers for Language Understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[15] T.B. Brown., et al., “Language Models are Few-shot Learners,” arXiv\npreprint arXiv:2005.14165, 2020.\n[16] A. Dosovitskiy, et al., “An Image is Worth 16x16 Words: Transformers\nfor Image Recognition at Scale,” arXiv preprint arXiv:2010.11929, 2020.\n[17] G. Krishna, C. Tran, M. Carnahan, and A.H. Tewﬁk, “EEG based\nContinuous Speech Recognition using Transformers,” arXiv preprint\narXiv:2001.00501, 2019.\n[18] Y . Song, X. Jia, L. Yang, and L. Xie, “Transformer-based Spatial-\nTemporal Feature Learning for EEG Decoding,” arXiv preprint\narXiv:2106.11170, 2021.\n[22] J. Guan, W. Wang, P. Feng, X. Wang, and W. Wang, “Low-Dimensional\nDenoising Embedding Transformer for ECG Classiﬁcation,” International\n[19] B. Wang, C. Liu, C. Hu, X. Liu, and J. Cao, “Arrhythmia Classiﬁ-\ncation with Heartbeat-Aware Transformer,” International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), 2021, pp. 1025-1029.\n[20] R. Casal, L.E. Di Persia, and G. Schlotthauer, “Temporal Convolutional\nNetworks and Transformers for Classifying the Sleep Stage in Awake or\nAsleep using Pulse Oximetry Signals,” arXiv preprint arXiv:2102.03352,\n2021.\n[21] Y . Wang, et. al., “Transformer-based Acoustic Modeling for Hybrid\nSpeech Recognition,” International Conference on Acoustics, Speech and\nSignal Processing (ICASSP) , 2020, pp. 6874-6878.\nConference on Acoustics, Speech and Signal Processing (ICASSP) , 2021,\npp. 1285-1289.\n[23] M. Atzori, A. Gijsberts, C. Castellini, B. Caputo, A.G.M Hager, S. Elsig,\nG. Giatsidis, F. Bassetto, and H. M ¨uller, “Electromyography Data for\nNon-Invasive Naturally-Controlled Robotic Hand Prostheses,” Scientiﬁc\ndata 1, no. 1, pp. 1-13, 2014.\n[24] A. Gijsberts, M. Atzori, C. Castellini, H. M ¨uller, and B. Caputo,\n“Movement Error Rate for Evaluation of Machine Learning Methods for\nsEMG-based Hand Movement Classiﬁcation,” IEEE Trans. Neural Syst.\nRehabil. Eng., vol. 22, no. 4, pp. 735-744, 2014.\n[25] M. Atzori, A. Gijsberts, I. Kuzborskij, S. Heynen, A.G.M Hager, O.\nDeriaz, C. Castellini, H. Mller, and B. Caputo, “A Benchmark Database\nfor Myoelectric Movement Classiﬁcation,” IEEE Trans. Neural Syst.\nRehabil. Eng., 2013.\n[26] E. Rahimian, S. Zabihi, A. Asif, D. Farina, S.F. Atashzar, and A. Mo-\nhammadi, “FS-HGR: Few-shot Learning for Hand Gesture Recognition\nvia ElectroMyography,” IEEE Trans. Neural Syst. Rehabil. Eng. , 2021.\n[27] E. Rahimian, S. Zabihi, A. Asif, S.F. Atashzar, and A. Mohammadi,\n“Few-Shot Learning for Decoding Surface Electromyography for Hand\nGesture Recognition,” IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , 2021, pp. 1300-1304.\n[28] JL Ba, JR Kiros, and G.E. Hinton, “Layer normalization,” arXiv preprint\narXiv:1607.06450, 2016.\n[29] DP. Kingma, and J. Ba, “Adam: A Method for Stochastic Optimization,”\nICLR, 2015.\n[30] U. C ˆot´e-Allard, C.L. Fall, A. Drouin, A. Campeau-Lecours, C. Gosselin,\nK. Glette, F. Laviolette, and B. Gosselin, “Deep Learning for Electromyo-\ngraphic Hand Gesture Signal Classiﬁcation using Transfer Learning,”\nIEEE Trans. Neural Syst. Rehabil. Eng. , vol. 27, no. 4, pp. 760-771,\n2019.\n[31] F. Wilcoxon, “Individual comparisons by ranking methods,” Biometrics\nBull.,vol. 1, no. 6, pp. 80–83, 1945."
}