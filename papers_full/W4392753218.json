{
  "title": "Human-Like Named Entity Recognition with Large Language Models in Unstructured Text-based Electronic Healthcare Records: An Evaluation Study",
  "url": "https://openalex.org/W4392753218",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5042582274",
      "name": "Izzet Turkalp Akbasli",
      "affiliations": [
        "State Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5078720345",
      "name": "Ahmet Ziya Bırbılen",
      "affiliations": [
        "Hacettepe University"
      ]
    },
    {
      "id": "https://openalex.org/A5033032594",
      "name": "Özlem Tekşam",
      "affiliations": [
        "Hacettepe University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2789510652",
    "https://openalex.org/W2984632147",
    "https://openalex.org/W3134138028",
    "https://openalex.org/W3138676254",
    "https://openalex.org/W4318765555",
    "https://openalex.org/W3087776590",
    "https://openalex.org/W3010611814",
    "https://openalex.org/W2996550193",
    "https://openalex.org/W2789894922",
    "https://openalex.org/W3011742849",
    "https://openalex.org/W3154810486",
    "https://openalex.org/W2968870211",
    "https://openalex.org/W2997681568",
    "https://openalex.org/W1018047830",
    "https://openalex.org/W3097556140",
    "https://openalex.org/W2073539200",
    "https://openalex.org/W2194321275",
    "https://openalex.org/W2994401446",
    "https://openalex.org/W4309323789",
    "https://openalex.org/W1997057722",
    "https://openalex.org/W3163469193",
    "https://openalex.org/W4377695655",
    "https://openalex.org/W4388116325",
    "https://openalex.org/W3181361218",
    "https://openalex.org/W2889272240",
    "https://openalex.org/W2808129629",
    "https://openalex.org/W4379379356",
    "https://openalex.org/W4367310581",
    "https://openalex.org/W4221050583",
    "https://openalex.org/W4385227045",
    "https://openalex.org/W4390723974",
    "https://openalex.org/W4386172820",
    "https://openalex.org/W2141986654",
    "https://openalex.org/W4381587418",
    "https://openalex.org/W4322622443",
    "https://openalex.org/W6857685708",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W4391221150",
    "https://openalex.org/W4391559941"
  ],
  "abstract": "<title>Abstract</title> Background The integration of big data and artificial intelligence (AI) in healthcare, particularly through the analysis of electronic health records (EHR), presents significant opportunities for improving diagnostic accuracy and patient outcomes. However, the challenge of processing and accurately labeling vast amounts of unstructured data remains a critical bottleneck, necessitating efficient and reliable solutions. This study investigates the ability of domain specific, fine-tuned large language models (LLMs) to classify unstructured EHR texts with typographical errors through named entity recognition tasks, aiming to improve the efficiency and reliability of supervised learning AI models in healthcare. Methods Clinical notes from pediatric emergency room admissions at Hacettepe University İhsan Doğramacı Children's Hospital from 2018 to 2023 were analyzed. The data were preprocessed with open source Python libraries and categorized using a pretrained GPT-3 model, \"text-davinci-003,\" before and after fine-tuning with domain-specific data on respiratory tract infections (RTI). The model's predictions were compared against ground truth labels established by pediatric specialists. Results Out of 24,229 patient records classified as \"Others ()\", 18,879 were identified without typographical errors and confirmed for RTI through filtering methods. The fine-tuned model achieved a 99.96% accuracy, significantly outperforming the pretrained model's 78.54% accuracy in identifying RTI cases among the remaining records. The fine-tuned model demonstrated superior performance metrics across all evaluated aspects compared to the pretrained model. Conclusions Fine-tuned LLMs can categorize unstructured EHR data with high accuracy, closely approximating the performance of domain experts. This approach significantly reduces the time and costs associated with manual data labeling, demonstrating the potential to streamline the processing of large-scale healthcare data for AI applications.",
  "full_text": "Page 1/15\nHuman-Like Named Entity Recognition with Large\nLanguage Models in Unstructured Text-based\nElectronic Healthcare Records: An Evaluation Study\nIzzet Turkalp Akbasli \nPolatlı Duatepe State Hospital\nAhmet Ziya Birbilen \nHacettepe University\nOzlem Teksam \nHacettepe University\nResearch Article\nKeywords: arti\u0000cial intelligence, large language models, electronic healthcare records, named entity\nrecognition\nPosted Date: March 13th, 2024\nDOI: https://doi.org/10.21203/rs.3.rs-4014476/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nVersion of Record: A version of this preprint was published at BMC Medical Informatics and Decision\nMaking on March 31st, 2025. See the published version at https://doi.org/10.1186/s12911-025-02871-6.\nPage 2/15\nAbstract\nBackground\nThe integration of big data and arti\u0000cial intelligence (AI) in healthcare, particularly through the analysis\nof electronic health records (EHR), presents signi\u0000cant opportunities for improving diagnostic accuracy\nand patient outcomes. However, the challenge of processing and accurately labeling vast amounts of\nunstructured data remains a critical bottleneck, necessitating e\u0000cient and reliable solutions. This study\ninvestigates the ability of domain speci\u0000c, \u0000ne-tuned large language models (LLMs) to classify\nunstructured EHR texts with typographical errors through named entity recognition tasks, aiming to\nimprove the e\u0000ciency and reliability of supervised learning AI models in healthcare.\nMethods\nClinical notes from pediatric emergency room admissions at Hacettepe University İ hsan Do ğ ramacı\nChildren's Hospital from 2018 to 2023 were analyzed. The data were preprocessed with open source\nPython libraries and categorized using a pretrained GPT-3 model, \"text-davinci-003,\" before and after \u0000ne-\ntuning with domain-speci\u0000c data on respiratory tract infections (RTI). The model's predictions were\ncompared against ground truth labels established by pediatric specialists.\nResults\nOut of 24,229 patient records classi\u0000ed as \"Others ()\", 18,879 were identi\u0000ed without typographical\nerrors and con\u0000rmed for RTI through \u0000ltering methods. The \u0000ne-tuned model achieved a 99.96%\naccuracy, signi\u0000cantly outperforming the pretrained model's 78.54% accuracy in identifying RTI cases\namong the remaining records. The \u0000ne-tuned model demonstrated superior performance metrics across\nall evaluated aspects compared to the pretrained model.\nConclusions\nFine-tuned LLMs can categorize unstructured EHR data with high accuracy, closely approximating the\nperformance of domain experts. This approach signi\u0000cantly reduces the time and costs associated with\nmanual data labeling, demonstrating the potential to streamline the processing of large-scale healthcare\ndata for AI applications.\n1. Introduction\nThe healthcare industry is undergoing a transformative era, fuelled by the rapid advancement of\ntechnology and the ever-increasing volume of data. The advent of big data (BD), characterized by its\nvastness, velocity, and variety, has unlocked unprecedented opportunities for healthcare providers and\nPage 3/15\nresearchers (1–3). In recent years, arti\u0000cial intelligence (AI) applications have demonstrated signi\u0000cant\nimprovements in safety, quality, and diagnostic accuracy across various clinical settings. Leveraging BD\nfrom electronic health records (EHR), these AI-based techniques have the potential to revolutionize\nmedicine by enhancing outcomes and providing numerous bene\u0000ts (2, 4–11). However, these large-scale\ndata are often unstructured, requiring extensive processing and labeling, which poses the most\nsigni\u0000cant bottleneck (12). In a precise \u0000eld such as medicine, errors in the labeling and preprocessing\nprocess can lead to poor outcomes in terms of the reliability of AI models and the impact of model\nresults (11, 13–17). Therefore, domain experts are often employed for labeling tasks in the present day, a\nprocess that is both time-consuming and costly (6, 18).\nChallenges in generating datasets and data extractions from clinical notes for arti\u0000cial intelligence\nmodels from BD sources containing unstructured EHR texts are primarily attributed to the complex\nprocess required for structuring and standardizing these texts for effective supervised learning AI model\nutilization. Unstructured EHRs are characterized by a wide array of data formats, including free-text\nclinical notes, laboratory \u0000ndings, and imaging narratives. Each of these formats exhibits unique\nterminological and syntactical features, ambiguous jargon, and nonstandard phrasal structures (17,19–\n23). To mitigate such complexity, the encoding of patients' diseases in EHRs using universally accepted\ndisease classi\u0000cation coding systems such as the International Classi\u0000cation of Disease (ICD)\nfacilitates the clustering of patients, providing convenience. However, these codes can sometimes be\nmisencoded due to intentional or unintentional information transfer by the patient or clinician (24), and\nthese inaccuracies can signi\u0000cantly impact the performance of supervised learning AI models, including\nmachine learning (ML), deep learning (DL), time series analysis, and natural language processing (NLP)\n(23, 25, 26).\nRecently, NLP methods for EHR-based computational phenotyping have undergone extensive\ndevelopment, in information technology, knowledge graphs can transform complex unstructured data\ninto structured forms (27, 28). Named entity recognition (NER) as a pivotal task in the construction of\nknowledge graphs, enables the automatic extraction of prede\u0000ned entities from extensive volumes of\nintricate texts, thereby facilitating the structuring of information. Through NER methods, the extraction of\ninformation from large-scale unstructured text-based datasets is substantially simpli\u0000ed (29–34).\nHowever, human-induced typo errors, such as homophone, typographical, grammatical, and spacing\nerrors, can still be present in manually entered data, with reported error rates ranging from 5–17% (35,\n36). These errors signi\u0000cantly impact the performance of NER methods (23, 37, 38). In 2017, Google's\nintroduction of transformer architecture marked a signi\u0000cant breakthrough in arti\u0000cial intelligence,\npaving the way for the creation of advanced large language models (LLM). Trained on vast amounts of\ninternet data using self-supervised learning techniques, these LLMs showcased an unprecedented ability\nto comprehend and produce text closely resembling human writing (6, 39). Furthermore, in NER tasks,\ntransformer-based language models have demonstrated the highest performance (23). Unlike other\nLLMs, OpenAI's GPT model is used more frequently than others due to its availability through the\nChatGPT interface and an API (40). It has been demonstrated that the ChatGPT can accurately predict\nPage 4/15\ndiagnoses for patients based on clinical notes, achieving results comparable to those of human\npractitioners in the domain of clinical information extraction from such notes (6, 41–44).\nIn this research, the ability to precisely classify target labels containing typographical errors through NER\ntasks was explored, aiming to alleviate the detrimental effects of missing data on the e\u0000cacy of\nsupervised learning AI models. This investigation was conducted utilizing domain speci\u0000c, \u0000ne-tuned\nLLMs, highlighting their potential to enhance model accuracy and reliability.\n2. Method\n2.1. Data Structure\nIn the present study, the primary data source comprised clinical notes from Pediatric Emergency Room\n(PER) admissions, which were extracted from the Electronic Health Record (EHR) system of Hacettepe\nUniversity İ hsan Do ğ ramacı Children's Hospital. The structure of the data is centered around the initial\nassessments conducted by pediatric residents at the PER triage point. These assessments include a\nvariety of patient information, such as presenting complaints, evaluations based on the pediatric\nassessment triangle (45), body temperature, heart rate, respiratory rate, and SpO2% levels. This\ninformation forms the basis of the triage process, wherein patients are categorized for further\nexamination. Notably, during this initial triage phase, patients are not assigned speci\u0000c diagnostic codes\ndue to the preliminary nature of the assessments. Instead, patient complaints are categorized into\nspeci\u0000c, institution-prepared complaint categories such as abdominal pain, headache, and fever, in a\nstructured format. In cases where a patient's presenting complaint does not align with these prede\u0000ned\ncategories, the data structure allows for the selection of an \"others\" category. This necessitates the input\nof the presenting complaint in a text \u0000eld.\n2.2. Data collection\nData collection for this study was conducted by encompassing all patient visits to the Pediatric\nEmergency Room (PER) at Hacettepe University İ hsan Do ğ ramacı Children's Hospital during the period\nfrom 2018 to 2023. Records were obtained from the hospital's Electronic Health Record (EHR) system,\nthrough which a systematic approach was employed to compile relevant clinical notes and assessment\ndata.\n2.3. Data preprocessing\nFor preprocessing tasks, open-sourced Python libraries such as Pandas, NLTK, and Re (regex) were\nutilized. Initially, texts categorized as \"Others ('Complaint')\" from structured categorical diagnostic\ndescriptions were selected. Subsequently, these categories were normalized to 'Complaint' through the\nuse of a regular expression task, which was designed utilizing the NLTK and Re libraries. After\nsimpli\u0000cation, syntactic \u0000ltering was employed to \u0000lter common RTI \u0000ndings. Due to typographical errors\nor rare \u0000ndings not captured by existing \u0000lters, some data were identi\u0000ed as poorly labeled.\nPage 5/15\nConsequently, this led to the continuation of the study with two distinct datasets: a clean dataset and a\nraw dataset.\n2.4. Prompt Engineering and Fine-Tuning of a GPT-3 Model\nand Prediction\nThe pretrained language model employed in this study was \"text-davinci-003\", a GPT-3 model, made\navailable through an API by OpenAI as of May 2023. The preprocessed \"Other complaints\" data were\nprovided to the model, and iteratively, responses were obtained to the prompt question, \"Based on the\nsymptoms and \u0000ndings presented, does this align with the characteristics of a RTI? If the evidence\nstrongly suggests an RTI, please respond with 'True'. If the \u0000ndings do not support an RTI diagnosis,\nrespond with 'False'.\" These responses were recorded in a Boolean list. Subsequently, the pretrained\n\"text-davinci-003\" model underwent a \u0000ne-tuning process with a text describing RTI \u0000ndings in Turkish.\nNext, the \u0000ne-tuned model was applied to the dataset again with the same prompt for prediction.\n2.5. Ground Truth Establishment\nFor the ground truth labels, four pediatric specialists were asked to determine whether the presenting\ncomplaint data, which were distributed equally and randomly among them, indicated \u0000ndings of an RTI.\n2.6. Model Evaluation and Data Analysis\nIn the evaluation of the model outcomes, assessments were conducted using classi\u0000cation metrics\nfrom the Scikit Learn library, including accuracy, ROC-AUC, precision, recall, F1 score, and MCC metrics.\nFor this project, Python version 3.9 was used.\n3. Results\nBetween 2018 and 2023, 321,672 patients presented to the Pediatric Emergency Room (PER). In this\nstudy, 31.9% (n = 102,732) of the patients were determined to have RTI complaints through standard\n\u0000ltering methods. Subsequently, 7.53% (n = 24,229) of the patients were recorded in the EHR system as\n\"Others ()\", with 77.91% (n = 18,879) of these patients accurately identi\u0000ed with RTI \u0000ndings through\n\u0000ltering methods, showing no typographical errors. Moreover, standard \u0000ltering methods revelaed that\n20.2% (n = 3,828) of these patients had RTI. The presenting complaint targets of the remaining 22% (n = \n5,350) were assessed as poorly labeled. These 5,350 patients received ground truth labels from four\npediatric specialists within two business days. From these labels, 16.9% (n = 909) were identi\u0000ed as RTI\ncases. In Table 1, the most frequent occurrences of presenting complaints containing RTI \u0000ndings\nacross the data clusters are displayed. Following the correction of errors within the unstructured ('Other\n()') and typographical error-containing data cluster, the most frequently presenting complaints were, in\norder, control revisits, falling, diarrhea, patients sent for hospitalization from the outpatient clinic,\npatients with suspected COVID among upper respiratory tract infections, epistaxis, constipation, patients\nreceiving injections, nasal discharge, and cough.\nPage 6/15\nTable 1\nDistribution of Presenting Complaints by Data Clusters\nPresenting ComplaintsStructured text dataUnstructured text data\n(“Others ()”)\nTypographical errors\nTotal RTI Patient 98904 3828 909\nFever 76408 1131 246\nCough 53866 890 143\nFatigue 19926 33 19\nSore throat 10897 302 47\nEar pain 9568 162 21\nRespiratory Distress 4630 31 16\nNon cardiac chest pain4077 78 8\nURTI 3513 70 -\nCrackles 718 249 2\nWheezing 246 5 34\nCOVID 9 289 423\nOther RTI complaintsa 80 1449 117\nTotal words in the textb 717153 64529 12117\nTotal categorized labelc 183938 4689 1076\na: Other RTI labels in English are: Flu, Cold, Nasal congestion, Wheezing, Rhonchi, Asthma, Croup,Bronchiolitis, Pneumonia, Febrile convulsion, Lymphadenitis, Tonsillitis, In\u0000uenza, Laryngitis, andSputum.\nb: Total words in the text: The total counts of words within the text data, segmented by data clusters.\nc: Total categorized label: The number of categorical variables that can be extracted from thecontent of text data through standard \u0000ltering methods.\nThe labeling process, which was conducted by four pediatric specialists, each of whom dedicated two\nbusiness days, was completed within a total of eight business days, resulting in a labeling rate of 27\nlabels per hour. The pretrained LLM completed the same task using a zero-shot approach in\napproximately six hours, with a labeling rate of 891 labels per hour. The \u0000ne-tuning process of the\npretrained model, utilizing a document containing 4,724 tokens pertaining to RTI \u0000ndings in Turkish,\nlasted approximately three hours. Similarly, employing a zero-shot approach, the \u0000ne-tuning process\ncompleted the entire labeling task in approximately six hours, akin to the performance of the pretrained\nPage 7/15\nmodel. The performances of both models were evaluated against the established ground truth labels.\nThe pretrained model identi\u0000ed 714 (78.54%) patients with RTIs, and the \u0000ne-tuned model identi\u0000ed 908\n(99.88%) patients with RTIs. The detailed performance metrics are available in Table 2.\nTable 2\nComparison of Performance between Pretrained Models and Fine-Tuned Models\nPerformance Metrics Pretrained Model Fine-tuned Model\nAccuracy 78.54 (714) 99.88 (908)\nROC-AUC 64.07 97.29\nPrecision 50.96 98.88\nRecall 51.46 97.78\nF1 Score 41.79 97.22\nMCC 47.05 97.24\nAbbreviations: ROC-AUC: receiver operating characteristic - area under the curve, MCC: Matthewscorrelation coe\u0000cient\n4. Discussion\nDue to typographical errors, the categorization of unstructured text-based EHR clinical notes that cannot\nbe classi\u0000ed through standard \u0000ltering and NER tasks is a costly and time-consuming process when\ndealing with large-scale data. As the data scale increases, it becomes imperative to automate the\nprocesses of data manipulation that require domain knowledge for more e\u0000cient supervised learning AI\nmodels. In the context of the pediatric emergency room visits where this study was conducted, nearly\none-third of the patients had RTI, representing the largest patient cohort. Therefore, it is valuable to\ndemonstrate that RTIs as presenting complaints can be recognized by LLMs. In this study, a solution to\nthis bottleneck is presented, demonstrating that LLMs \u0000ne-tuned on a speci\u0000c subject can be\ncategorized with an accuracy approaching that of domain experts, in contrast to the general-use LLM\nmodels.\nThe outcomes of this study are discussed under three distinct subheadings. These include the accuracy\nwith which unstructured text-based data are classi\u0000ed by LLMs in NER tasks compared to domain expert\nhuman encoders, the time difference involved, and the cost differential.\n4.1. Accuracy of LLMs in NER tasks\nIn this study, ground truth labels were determined by pediatric specialists, and the primary focus was not\na direct comparison between humans and LLMs, but rather an investigation into how closely LLMs could\napproximate domain expert human encoders. Accordingly, the performance of a general-use GPT-3\nsubmodel, \"text-davinci-003,\" resulted in 78% accuracy, while its version \u0000ne-tuned speci\u0000cally for RTI\nPage 8/15\n\u0000ndings demonstrated a signi\u0000cantly higher accuracy of 98%, surpassing that of the general model and\nclosely matching the performance of domain experts. In the literature, comparisons between humans\nand LLMs, including a meta-analysis by Takita et al. that included other LLMs such as GPT-4, Llama-2,\nPaLM-2, Prometheus, Glass, and Med42, revealed that the pooled accuracy of all models was 57%. For\nthe GPT-3 model utilized in this study, the average accuracy was reported to be 60% (range 51–69%).\nAdditionally, when examining model performance across specialties, the highest performance was\nobserved in pediatric studies (93%). The above-average performance of the general-use model in this\nstudy could be attributed to the relatively high e\u0000cacy of LLMs in pediatric patients (46).\nIn a study conducted by Roso ł  et al., which compared humans and LLMs in solving medical exam\nquestions, the GPT-4 model, even without \u0000ne-tuning, outperformed the GPT-3 model (47). Furthermore,\nthe MedPaLM2 model, which is a medical domain-speci\u0000c \u0000ne-tuned version of PaLM2, demonstrated a\nhigh accuracy of 86.5% in the Singhal et al. study (48), matching the performance of the GPT-4 model\nused in the study by Nori et al., which also showed an accuracy of 86.1% in USMLE exam questions (49).\nAccording to the meta-analyses by Takita et al. the pooled accuracy of the PaLM2 model was 43%,\nunderscoring the signi\u0000cant improvement effect of \u0000ne-tuning on performance. It is well acknowledged\nthat \u0000ne-tuned LLMs exhibit greater performance than general-use models, a \u0000nding that is rea\u0000rmed in\nthis study (49–54).\nAdditionally, another method of obtaining domain-speci\u0000c responses through the use of LLMs involves\nthe retrieval augmented generation (RAG) methods (55), which enable a pretrained LLM to generate task-\nspeci\u0000c answers by sourcing information from speci\u0000c external resources. This approach may offer an\nalternative solution for NER tasks. With RAG methods, which have been shown to enhance the answer\ngeneration performance of language models (56–59), Naik and colleagues developed a language model\nthat performs binary classi\u0000cation of clinical outcomes from EHR clinical notes (60, 61). Balaguer et al.\ndemonstrated in their study comparing LLMs utilized with RAG and \u0000ne-tuning that the \u0000ne-tuned model\nproduced correct answers 47% of the time, whereas the use of RAG alone increased this percentage to\n72%, and to 74% when both were used in conjunction. However, the use of RAG was reported to\npotentially incur lower costs (62). The utilization of RAG methods in such unstructured text-based EHR\ndata holds signi\u0000cant potential promise for NER tasks, akin to that of an editor.\n4.2. Time E\u0000ciency and Cost Comparison\nCompared with humans, LLMs are capable of labeling both more rapidly and in a continuous,\nuninterrupted manner. Wang et al. demonstrated that labeling with GPT-3 is not only faster but also less\nexpensive. Their comparison involved the GPT-3 model and human labellers on the Google Cloud\nPlatform, where billing is based on the number of tokens. According to their \u0000ndings, utilizing GPT\nresulted in a cost reduction of 50–96%, translating into an approximate cost of $453 for this study (63).\nThe work of the human encoders in this research was voluntary, with no compensation requested, and\nthe study itself was not focused on cost analysis. However, the comparison is considered striking.\nApproximately $13 was spent on the labeling process in this study, including the use of a \u0000ne-tuning\nmodel that can be subsequently utilized with GPT-3. Consequently, achieving an accuracy of 98%, this\nPage 9/15\nmethod, which operates 33 times faster and can be 34 times less expensive, allows expert clinicians to\nallocate their time more effectively to other tasks.\n5. Conclusion\nIn conclusion, this study demonstrates that the performance of supervised arti\u0000cial intelligence models,\nwhich decreases due to missing data caused by typographical errors in unstructured text-based data\nobtained from EHR systems, can be improved by correcting these errors with problem-focused \u0000ne-\ntuned LLMs with accuracies close to those of domain experts and in a shorter time. This research\nhighlights the importance of \u0000ne-tuning LLMs and RAG in text-based medical studies and shows the\npotential for NER methods to be automated and corrected at the time of recording in the future.\nDeclarations\nEthics approval and consent to participate\nThe Hacettepe University Clinical Research Ethics Committee approved our study's design and\nprocedures under protocol number GO-23/508, ensuring adherence to the ethical standards in clinical\nresearch. The data sourced from Hacettepe University İ hsan Do ğ ramacı Children's Hospital, which\nunderwent a de-identi\u0000cation process through the redaction of protected health information, received\napproval for utilization in a quality improvement project by the hospital. In this context, the Hacettepe\nUniversity Research Ethics Board granted a waiver for the necessity of its approval and the procurement\nof informed consent for this study. Furthermore, all procedures complied with the relevant guidelines\nand standards outlined in the Declaration of Helsinki.\nConsent for publication\nNot applicable\nAvailability of data and materials\nAll data produced in the present study are available upon reasonable request to the authors.\nCompeting interests\nAll authors declare that they have no known competing \u0000nancial interests or personal relationships that\ncould have appeared to in\u0000uence the work reported in this paper.\nFunding\nThis study did not receive any funding\nAuthor Contributions\nPage 10/15\nITA conceptualized the study, developed the methodology, handled the software, validated the results,\nperformed the formal analysis, and contributed to data curation, writing the original draft, and\nvisualization. AZB contributed to conceptualization, validation, resources, data curation, and drafting the\noriginal manuscript. OT was responsible for the investigation, provided resources, reviewed and edited\nthe manuscript, supervised the project, managed project administration, and acquired funding. All\nauthors have read and approved the \u0000nal manuscript.\nAcknowledgements\nNot applicable\nReferences\n1. Saggi MK, Jain S. A survey towards an integration of big data analytics to big insights for value-\ncreation. Inf Process Manag. 2018 Sep 1;54(5):758–90.\n2. Pastorino R, De Vito C, Migliara G, Glocker K, Binenbaum I, Ricciardi W, et al. Bene\u0000ts and challenges\nof Big Data in healthcare: an overview of the European initiatives. Eur J Public Health. 2019\nOct;29(Suppl 3):23–7.\n3. Mishra S, Tripathy HK, Mishra BK, Sahoo S. Usage and Analysis of Big Data in E-Health Domain. In:\nResearch Anthology on Big Data Analytics, Architectures, and Applications [Internet]. IGI Global;\n2022 [cited 2024 Feb 8]. p. 417–30. Available from: https://www.igi-global.com/chapter/usage-and-\nanalysis-of-big-data-in-e-health-domain/www.igi-global.com/chapter/usage-and-analysis-of-big-\ndata-in-e-health-domain/290994\n4. Yin J, Ngiam KY, Teo HH. Role of arti\u0000cial intelligence applications in real-life clinical practice:\nsystematic review. J Med Internet Res. 2021;23(4):e25759.\n5. Bates DW, Levine D, Syrowatka A, Kuznetsova M, Craig KJT, Rui A, et al. The potential of arti\u0000cial\nintelligence to improve patient safety: a scoping review. NPJ Digit Med. 2021;4(1):54.\n\u0000. Levine DM, Tuwani R, Kompa B, Varma A, Finlayson SG, Mehrotra A, et al. The Diagnostic and Triage\nAccuracy of the GPT-3 Arti\u0000cial Intelligence Model. medRxiv. 2023 Feb 1;2023.01.30.23285067.\n7. Meskó B, Görög M. A short guide for medical professionals in the era of arti\u0000cial intelligence. Npj\nDigit Med. 2020 Sep 24;3(1):1–8.\n\u0000. Agrawal R, Prabakaran S. Big data in digital healthcare: lessons learnt and recommendations for\ngeneral practice. Heredity. 2020 Apr;124(4):525–34.\n9. Matheny ME, Whicher D, Israni ST. Arti\u0000cial intelligence in health care: a report from the National\nAcademy of Medicine. Jama. 2020;323(6):509–10.\n10. Beam AL, Kohane IS. Big Data and Machine Learning in Health Care. JAMA. 2018 Apr\n3;319(13):1317–8.\n11. Ahmed Z, Mohamed K, Zeeshan S, Dong X. Arti\u0000cial intelligence with multi-functional machine\nlearning platform development for better healthcare and precision medicine. Database. 2020 Jan\nPage 11/15\n1;2020:baaa010.\n12. Zhou H, Albrecht MA, Roberts PA, Porter P, Della PR. Using machine learning to predict paediatric 30-\nday unplanned hospital readmissions: a case-control retrospective analysis of medical records,\nincluding written discharge documentation. Aust Health Rev Publ Aust Hosp Assoc. 2021\nJun;45(3):328–37.\n13. Wang F, Preininger A. AI in Health: State of the Art, Challenges, and Future Directions. Yearb Med\nInform. 2019 Aug;28(1):16–26.\n14. Beam AL, Manrai AK, Ghassemi M. Challenges to the Reproducibility of Machine Learning Models in\nHealth Care. JAMA. 2020 Jan 28;323(4):305–6.\n15. Zhang P, Wang F, Hu J, Sorrentino R. Label Propagation Prediction of Drug-Drug Interactions Based\non Clinical Side Effects. Sci Rep. 2015 Jul 21;5:12339.\n1\u0000. Curchoe CL, Flores-Saiffe Farias A, Mendizabal-Ruiz G, Chavez-Badiola A. Evaluating predictive\nmodels in reproductive medicine. Fertil Steril. 2020 Nov 1;114(5):921–6.\n17. Agrawal M, Hegselmann S, Lang H, Kim Y, Sontag D. Large language models are few-shot clinical\ninformation extractors. In: Goldberg Y, Kozareva Z, Zhang Y, editors. Proceedings of the 2022\nConference on Empirical Methods in Natural Language Processing [Internet]. Abu Dhabi, United Arab\nEmirates: Association for Computational Linguistics; 2022 [cited 2024 Feb 8]. p. 1998–2022.\nAvailable from: https://aclanthology.org/2022.emnlp-main.130\n1\u0000. Goel A, Gueta A, Gilon O, Liu C, Erell S, Nguyen LH, et al. LLMs Accelerate Annotation for Medical\nInformation Extraction. In: Proceedings of the 3rd Machine Learning for Health Symposium\n[Internet]. PMLR; 2023 [cited 2024 Feb 8]. p. 82–100. Available from:\nhttps://proceedings.mlr.press/v225/goel23a.html\n19. Nguyen-Dinh LV, Rossi M, Blanke U, Tröster G. Combining crowd-generated media and personal data:\nsemi-supervised learning for context recognition. In: Proceedings of the 1st ACM international\nworkshop on Personal data meets distributed multimedia [Internet]. New York, NY, USA: Association\nfor Computing Machinery; 2013 [cited 2024 Feb 7]. p. 35–8. (PDM ’13). Available from:\nhttps://doi.org/10.1145/2509352.2509396\n20. Lake BM, Salakhutdinov R, Tenenbaum JB. Human-level concept learning through probabilistic\nprogram induction. Science. 2015 Dec 11;350(6266):1332–8.\n21. Mozafari B, Sarkar P, Franklin M, Jordan M, Madden S. Scaling up crowd-sourcing to very large\ndatasets: a case for active learning. Proc VLDB Endow. 2014 Ekim;8(2):125–36.\n22. Qing L, Linhong W, Xuehai D. A Novel Neural Network-Based Method for Medical Text Classi\u0000cation.\nFuture Internet. 2019 Dec;11(12):255.\n23. Lee EB, Heo GE, Choi CM, Song M. MLM-based typographical error correction of unstructured\nmedical texts for named entity recognition. BMC Bioinformatics. 2022 Nov 16;23(1):486.\n24. O’Malley KJ, Cook KF, Price MD, Wildes KR, Hurdle JF, Ashton CM. Measuring Diagnoses: ICD Code\nAccuracy. Health Serv Res. 2005;40(5p2):1620–39.\nPage 12/15\n25. Kim J, Kim T, Choi JH, Choo J. End-to-end Multi-task Learning of Missing Value Imputation and\nForecasting in Time-Series Data. In: 2020 25th International Conference on Pattern Recognition\n(ICPR) [Internet]. 2021 [cited 2024 Feb 8]. p. 8849–56. Available from:\nhttps://ieeexplore.ieee.org/document/9412112\n2\u0000. Muller M, Wolf CT, Andres J, Desmond M, Joshi NN, Ashktorab Z, et al. Designing Ground Truth and\nthe Social Life of Labels. In: Proceedings of the 2021 CHI Conference on Human Factors in\nComputing Systems [Internet]. New York, NY, USA: Association for Computing Machinery; 2021\n[cited 2024 Feb 7]. p. 1–16. (CHI ’21). Available from: https://doi.org/10.1145/3411764.3445402\n27. Murali L, Gopakumar G, Viswanathan DM, Nedungadi P. Towards electronic health record-based\nmedical knowledge graph construction, completion, and applications: A literature study. J Biomed\nInform. 2023 Jul 1;143:104403.\n2\u0000. Sim J ah, Huang X, Horan MR, Stewart CM, Robison LL, Hudson MM, et al. Natural language\nprocessing with machine learning methods to analyze unstructured patient-reported outcomes\nderived from electronic health records: A systematic review. Artif Intell Med. 2023 Dec\n1;146:102701.\n29. Li I, Pan J, Goldwasser J, Verma N, Wong WP, Nuzumlalı MY, et al. Neural Natural Language\nProcessing for unstructured data in electronic health records: A review. Comput Sci Rev. 2022 Nov\n1;46:100511.\n30. Wang Y, Afzal N, Fu S, Wang L, Shen F, Rastegar-Mojarad M, et al. MedSTS: a resource for clinical\nsemantic textual similarity. Lang Resour Eval. 2020 Mar 1;54(1):57–72.\n31. Zeng Z, Deng Y, Li X, Naumann T, Luo Y. Natural Language Processing for EHR-Based Computational\nPhenotyping. IEEE/ACM Trans Comput Biol Bioinform. 2019 Jan;16(1):139–53.\n32. Kundeti SR, Vijayananda J, Mujjiga S, Kalyan M. Clinical named entity recognition: Challenges and\nopportunities. In: 2016 IEEE International Conference on Big Data (Big Data) [Internet]. 2016 [cited\n2024 Feb 11]. p. 1937–45. Available from: https://ieeexplore.ieee.org/abstract/document/7840814\n33. Fraile Navarro D, Ijaz K, Rezazadegan D, Rahimi-Ardabili H, Dras M, Coiera E, et al. Clinical named\nentity recognition and relation extraction using natural language processing of medical free text: A\nsystematic review. Int J Med Inf. 2023 Sep 1;177:105122.\n34. Ahmad PN, Shah AM, Lee K. A Review on Electronic Health Record Text-Mining for Biomedical Name\nEntity Recognition in Healthcare Domain. Healthcare. 2023 Jan;11(9):1268.\n35. Hersh WR, Campbell EM, Malveau SE. Assessing the feasibility of large-scale natural language\nprocessing in a corpus of ordinary medical records: a lexical analysis. Proc Conf Am Med Inform\nAssoc AMIA Fall Symp. 1997;580–4.\n3\u0000. Zhou L, Mahoney LM, Shakurova A, Goss F, Chang FY, Bates DW, et al. How Many Medication Orders\nare Entered through Free-text in EHRs? - A Study on Hypoglycemic Agents. AMIA Annu Symp Proc.\n2012 Nov 3;2012:1079–88.\n37. Hamdi A, Pontes EL, Sidere N, Coustaty M, Doucet A. In-depth analysis of the impact of OCR errors\non named entity recognition and linking. Nat Lang Eng. 2023 Mar;29(2):425–48.\nPage 13/15\n3\u0000. Fetahu B, Chen Z, Kar S, Rokhlenko O, Malmasi S. arXiv.org. 2023 [cited 2024 Feb 11]. MultiCoNER\nv2: a Large Multilingual dataset for Fine-grained and Noisy Named Entity Recognition. Available\nfrom: https://arxiv.org/abs/2310.13213v1\n39. Yang R, Tan TF, Lu W, Thirunavukarasu AJ, Ting DSW, Liu N. Large language models in health care:\nDevelopment, applications, and challenges. Health Care Sci. 2023;2(4):255–63.\n40. Coello CEA, Alimam MN, Kouatly R. Effectiveness of ChatGPT in Coding: A Comparative Analysis of\nPopular Large Language Models. Digital. 2024 Mar;4(1):114–25.\n41. Knebel D, Priglinger S, Scherer N, Siedlecki J, Schworm B. Assessment of ChatGPT in the preclinical\nmanagement of ophthalmological emergencies – an analysis of ten \u0000ctional case vignettes\n[Internet]. medRxiv; 2023 [cited 2024 Feb 8]. p. 2023.04.16.23288645. Available from:\nhttps://www.medrxiv.org/content/10.1101/2023.04.16.23288645v1\n42. Nastasi AJ, Courtright KR, Halpern SD, Weissman GE. Does ChatGPT Provide Appropriate and\nEquitable Medical Advice?: A Vignette-Based, Clinical Evaluation Across Care Contexts [Internet].\nmedRxiv; 2023 [cited 2024 Feb 8]. p. 2023.02.25.23286451. Available from:\nhttps://www.medrxiv.org/content/10.1101/2023.02.25.23286451v1\n43. Rao A, Pang M, Kim J, Kamineni M, Lie W, Prasad AK, et al. Assessing the Utility of ChatGPT\nThroughout the Entire Clinical Work\u0000ow [Internet]. medRxiv; 2023 [cited 2024 Feb 8]. p.\n2023.02.21.23285886. Available from:\nhttps://www.medrxiv.org/content/10.1101/2023.02.21.23285886v1\n44. Fraser H, Crossland D, Bacher I, Ranney M, Madsen T, Hilliard R. Comparison of Diagnostic and\nTriage Accuracy of Ada Health and WebMD Symptom Checkers, ChatGPT, and Physicians for\nPatients in an Emergency Department: Clinical Data Analysis Study. JMIR MHealth UHealth. 2023\nOct 3;11(1):e49995.\n45. Dieckmann RA, Brownstein D, Gausche-Hill M. The pediatric assessment triangle: a novel approach\nfor the rapid evaluation of children. Pediatr Emerg Care. 2010;26(4):312–5.\n4\u0000. Takita H, Walston SL, Tatekawa H, Saito K, Tsujimoto Y, Miki Y, et al. Diagnostic Performance of\nGenerative AI and Physicians: A Systematic Review and Meta-Analysis [Internet]. medRxiv; 2024\n[cited 2024 Feb 11]. p. 2024.01.20.24301563. Available from:\nhttps://www.medrxiv.org/content/10.1101/2024.01.20.24301563v1\n47. Roso ł  M, G ą sior JS, Ł aba J, Korzeniewski K, M ł y ń czak M. Evaluation of the performance of GPT-3.5\nand GPT-4 on the Medical Final Examination [Internet]. medRxiv; 2023 [cited 2024 Feb 10]. p.\n2023.06.04.23290939. Available from:\nhttps://www.medrxiv.org/content/10.1101/2023.06.04.23290939v2\n4\u0000. Singhal K, Tu T, Gottweis J, Sayres R, Wulczyn E, Hou L, et al. Towards Expert-Level Medical Question\nAnswering with Large Language Models [Internet]. arXiv; 2023 [cited 2024 Feb 12]. Available from:\nhttp://arxiv.org/abs/2305.09617\n49. Nori H, King N, McKinney SM, Carignan D, Horvitz E. Capabilities of gpt-4 on medical challenge\nproblems. ArXiv Prepr ArXiv230313375. 2023;\nPage 14/15\n50. Touvron H, Martin L, Stone K, Albert P, Almahairi A, Babaei Y, et al. Llama 2: Open Foundation and\nFine-Tuned Chat Models [Internet]. arXiv; 2023 [cited 2024 Feb 12]. Available from:\nhttp://arxiv.org/abs/2307.09288\n51. Tian S, Jin Q, Yeganova L, Lai PT, Zhu Q, Chen X, et al. Opportunities and challenges for ChatGPT and\nlarge language models in biomedicine and health. Brief Bioinform. 2024 Jan 1;25(1):bbad493.\n52. Johnson D, Goodman R, Patrinely J, Stone C, Zimmerman E, Donald R, et al. Assessing the accuracy\nand reliability of AI-generated medical responses: an evaluation of the Chat-GPT model. Res Sq.\n2023;\n53. Latif E, Zhai X. Fine-tuning chatgpt for automatic scoring. Comput Educ Artif Intell. 2024;100210.\n54. Singhal K, Azizi S, Tu T, Mahdavi SS, Wei J, Chung HW, et al. Large language models encode clinical\nknowledge. Nature. 2023 Aug;620(7972):172–80.\n55. Lewis P, Perez E, Piktus A, Petroni F, Karpukhin V, Goyal N, et al. Retrieval-augmented generation for\nknowledge-intensive nlp tasks. Adv Neural Inf Process Syst. 2020;33:9459–74.\n5\u0000. Guu K, Lee K, Tung Z, Pasupat P, Chang M. Retrieval Augmented Language Model Pre-Training. In:\nInternational Conference on Machine Learning [Internet]. PMLR; 2020 [cited 2024 Feb 12]. p. 3929–\n38. Available from: https://proceedings.mlr.press/v119/guu20a.html\n57. Cuconasu F, Trappolini G, Siciliano F, Filice S, Campagnano C, Maarek Y, et al. The Power of Noise:\nRede\u0000ning Retrieval for RAG Systems [Internet]. arXiv; 2024 [cited 2024 Feb 12]. Available from:\nhttp://arxiv.org/abs/2401.14887\n5\u0000. Zhang L, Jijo K, Setty S, Chung E, Javid F, Vidra N, et al. Enhancing Large Language Model\nPerformance To Answer Questions and Extract Information More Accurately [Internet]. arXiv; 2024\n[cited 2024 Feb 12]. Available from: http://arxiv.org/abs/2402.01722\n59. Luo R, Sun L, Xia Y, Qin T, Zhang S, Poon H, et al. BioGPT: generative pretrained transformer for\nbiomedical text generation and mining. Brief Bioinform. 2022 Nov 19;23(6):bbac409.\n\u00000. Naik A, Parasa S, Feldman S, Wang LL, Hope T. Literature-Augmented Clinical Outcome Prediction\n[Internet]. arXiv; 2022 [cited 2024 Feb 12]. Available from: http://arxiv.org/abs/2111.08374\n\u00001. Zakka C, Shad R, Chaurasia A, Dalal AR, Kim JL, Moor M, et al. Almanac — Retrieval-Augmented\nLanguage Models for Clinical Medicine. NEJM AI. 2024 Jan 25;1(2):AIoa2300068.\n\u00002. Balaguer A, Benara V, Cunha RL de F, Filho R de ME, Hendry T, Holstein D, et al. RAG vs Fine-tuning:\nPipelines, Tradeoffs, and a Case Study on Agriculture [Internet]. arXiv; 2024 [cited 2024 Feb 12].\nAvailable from: http://arxiv.org/abs/2401.08406\n\u00003. Wang S, Liu Y, Xu Y, Zhu C, Zeng M. Want To Reduce Labeling Cost? GPT-3 Can Help [Internet]. arXiv;\n2021 [cited 2024 Feb 11]. Available from: http://arxiv.org/abs/2108.13487\nSupplementary Files\nThis is a list of supplementary \u0000les associated with this preprint. Click to download.\nPage 15/15\nFigure1.png",
  "topic": "Unstructured data",
  "concepts": [
    {
      "name": "Unstructured data",
      "score": 0.649312436580658
    },
    {
      "name": "Health records",
      "score": 0.6239626407623291
    },
    {
      "name": "Named-entity recognition",
      "score": 0.5979999899864197
    },
    {
      "name": "Computer science",
      "score": 0.583633542060852
    },
    {
      "name": "Natural language processing",
      "score": 0.5555115342140198
    },
    {
      "name": "Electronic health record",
      "score": 0.48387300968170166
    },
    {
      "name": "Language model",
      "score": 0.44703805446624756
    },
    {
      "name": "Artificial intelligence",
      "score": 0.431937038898468
    },
    {
      "name": "Health care",
      "score": 0.3775675892829895
    },
    {
      "name": "Linguistics",
      "score": 0.3410072922706604
    },
    {
      "name": "Information retrieval",
      "score": 0.32493603229522705
    },
    {
      "name": "Data mining",
      "score": 0.2033575475215912
    },
    {
      "name": "Big data",
      "score": 0.15266966819763184
    },
    {
      "name": "Political science",
      "score": 0.10727396607398987
    },
    {
      "name": "Engineering",
      "score": 0.09681281447410583
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}