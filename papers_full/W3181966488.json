{
  "title": "Visual Transformer with Statistical Test for COVID-19 Classification",
  "url": "https://openalex.org/W3181966488",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222695017",
      "name": "Hsu, Chih-Chung",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282765193",
      "name": "Chen, Guan-Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5028422117",
      "name": "WU Mei-Hsuan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3014337038",
    "https://openalex.org/W3154692872",
    "https://openalex.org/W3080906603",
    "https://openalex.org/W3094388274",
    "https://openalex.org/W3133631487",
    "https://openalex.org/W3040660552",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3092624683",
    "https://openalex.org/W3012817089",
    "https://openalex.org/W3129581972",
    "https://openalex.org/W3038744550",
    "https://openalex.org/W3171916733",
    "https://openalex.org/W3165088525",
    "https://openalex.org/W3086650124",
    "https://openalex.org/W3028070348",
    "https://openalex.org/W3164251887",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3089460656",
    "https://openalex.org/W2769215776",
    "https://openalex.org/W3117052584",
    "https://openalex.org/W2194775991"
  ],
  "abstract": "With the massive damage in the world caused by Coronavirus Disease 2019 SARS-CoV-2 (COVID-19), many related research topics have been proposed in the past two years. The Chest Computed Tomography (CT) scans are the most valuable materials to diagnose the COVID-19 symptoms. However, most schemes for COVID-19 classification of Chest CT scan is based on a single-slice level, implying that the most critical CT slice should be selected from the original CT scan volume manually. We simultaneously propose 2-D and 3-D models to predict the COVID-19 of CT scan to tickle this issue. In our 2-D model, we introduce the Deep Wilcoxon signed-rank test (DWCC) to determine the importance of each slice of a CT scan to overcome the issue mentioned previously. Furthermore, a Convolutional CT scan-Aware Transformer (CCAT) is proposed to discover the context of the slices fully. The frame-level feature is extracted from each CT slice based on any backbone network and followed by feeding the features to our within-slice-Transformer (WST) to discover the context information in the pixel dimension. The proposed Between-Slice-Transformer (BST) is used to aggregate the extracted spatial-context features of every CT slice. A simple classifier is then used to judge whether the Spatio-temporal features are COVID-19 or non-COVID-19. The extensive experiments demonstrated that the proposed CCAT and DWCC significantly outperform the state-of-the-art methods.",
  "full_text": "Visual Transformer with Statistical Test for COVID-19 Classiﬁcation\n1Chih-Chung Hsu, 2Guan-Lin Chen, and 3Mei-Hsuan Wu\nInstitute of Data Science, National Cheng Kung University\nNo.1, University Rd., Tainan City, Taiwan ROC.\n{1cchsu,3re6091054}@gs.ncku.edu.tw and 2alright1117@gmail.com\nAbstract\nWith the massive damage in the world caused by Coro-\nnavirus Disease 2019 SARS-CoV-2 (COVID-19), many re-\nlated research topics have been proposed in the past two\nyears. The Chest Computed Tomography (CT) scans are the\nmost valuable materials to diagnose the COVID-19 symp-\ntoms. However, most schemes for COVID-19 classiﬁcation\nof Chest CT scan is based on a single-slice level, imply-\ning that the most critical CT slice should be selected from\nthe original CT scan volume manually. We simultaneously\npropose 2-D and 3-D models to predict the COVID-19 of\nCT scan to tickle this issue. In our 2-D model, we intro-\nduce the Deep Wilcoxon signed-rank test (DWCC) to deter-\nmine the importance of each slice of a CT scan to overcome\nthe issue mentioned previously. Furthermore, a Convolu-\ntional CT scan-Aware Transformer (CCAT) is proposed to\ndiscover the context of the slices fully. The frame-level fea-\nture is extracted from each CT slice based on any back-\nbone network and followed by feeding the features to our\nwithin-slice-Transformer (WST) to discover the context in-\nformation in the pixel dimension. The proposed Between-\nSlice-Transformer (BST) is used to aggregate the extracted\nspatial-context features of every CT slice. A simple clas-\nsiﬁer is then used to judge whether the Spatio-temporal\nfeatures are COVID-19 or non-COVID-19. The extensive\nexperiments demonstrated that the proposed CCAT and\nDWCC signiﬁcantly outperform the state-of-the-art meth-\nods.\n1. Introduction\nWith the rapid growth of the deep learning approach re-\ncently, the performance of many research ﬁelds has been\nboosted with deep learning. One essential application\namong them is medical image analysis based on deep learn-\ning. The chest Computed Tomography (CT) scan is an ef-\nfective way to trace the symptoms of Coronavirus Disease\n2019 SARS-CoV-2 (COVID-19). However, both the analy-\nsis and diagnosis of CT scan series require an experienced\ndoctor or expert in the related ﬁeld. Unfortunately, it is\nhard to meet this requirement in remote areas. An automatic\ncomputer-aid diagnosis system for CT scan for COVID-19\nis thus highly desired.\nThe COVID-19 classiﬁcation for CT scan can be mod-\neled as a special case of the image/video recognition tasks.\nIn the past decade, deep learning achieved state-of-the-art\nimage recognition tasks compared to conventional machine\nlearning and computer vision techniques. Similarly, deep\nlearning-related schemes were widely adopted in the medi-\ncal signal processing ﬁeld [10]. However, the CT scan series\nis usually treated as a three-dimensional (3-D) data volume,\nwhere the traditional convolutional neural network (CNN)\ncan only perform the two-dimensional (2-D) convolutional\noperation on the image. 3-D convolution was then adopted\nto tickle these issues [19][12][17]. However, the space and\ncomputational complexity of 3-D convolution is relatively\nhigher than the 2-D, and therefore a high-end server is nec-\nessary. Another critical issue is that the large-scale dataset is\nthe power source of deep neural networks. The insufﬁcient\ntraining samples lead to a fatal overﬁtting issue in training\na deep neural network.\nCompared to the 2-D information-only approach, the\nsymptoms of COVID-19 might present at different depths\n(slice) for different patients, as suggested in [13]. For-\ntunately, a large-scale 3D-shaped CT scan series, termed\nCOV19-CT-DB, has been proposed in [8], including 5, 000\n3D CT scans with more than 1, 000 patients. Compared to\nthe well-known traditional 2D CT dataset for COVID-19\nclassiﬁcation [22], the COV19-CT-DB contains 3D infor-\nmation providing more semantic features to help the diag-\nnosis of COVID-19 symptoms. The conventional COVID-\n19 classiﬁcation approach is usually based on an image-\nlevel scheme, where the input of their method is 2D image\nonly [14][6][3][2][5][21][1]. It is hard to adopt the 2-D in-\nformation as the training set extends to the 3-D (i.e., CT\nscan series) without signiﬁcant revision. The classiﬁcation\nof COVID-19 based on a single slice can be treated as a\nconventional image recognition issue, as the suggestion in\n[6]. In [3], self-supervision was proposed to improve classi-\narXiv:2107.05334v1  [eess.IV]  12 Jul 2021\n(a) DWCC\n(b) CCAT\nFigure 1: Flowchart of the proposed methods (a) DWCC using statistical inference for deep learning and (b) CCAT for slice\ncontext and temporal relation mining modules.\nﬁcation performance for a small-scale COVID-19 CT scan\ndataset. Since the number of the training samples of CT\nscans is relatively tiny, supervised learning might be over-\nﬁtting easily in practice. In [5], weakly supervised learning\nwas introduced to tickle insufﬁcient training samples and\nreduce the annotation requirements. However, the perfor-\nmance of those single slice-level models is restricted since\nthe critical slice of a CT scan should be extracted by expe-\nrienced experts.\nIn [9][11], it can alleviate the catastrophic forgetting\nproblem when another type of dataset related to the target\ndisease has been used as the new training set. However, the\n2-D and 3-D information are signiﬁcantly different, imply-\ning that the performance might not be promising. The CT\nscan-level information provided in COV19-CT-DB makes it\nhard to perform these existing single slice-level schemes to\n3-D CT scan-level directly without signiﬁcant revision. Re-\ncently, several schemes were focused on CT scan-level (3-D\ninformation) directly. In [19] adopted the multiple image-\nlevel CNNs to aggregate the predicted result of each CT im-\nage in a 3-D CT scan, while the 3D-CNN is directly adopted\nin [12] to extract the cube-like feature from a whole 3-D CT\nscan. Both methods need considerable computational and\nspace complexity to meet the model ensemble and 3-D con-\nvolution requirements. In [20], weakly supervised learning\nwas adopted, as a suggestion in [5], to achieve lesion lo-\ncalization with classiﬁcation annotation only based on an\nimproved 3-D U-shape Network (U-Net). In [17] and [8],\nthe recurrent neural network (RNN) and long short-term\nmemory (LSTM) network were proposed to integrate the\ncross-slice information to make the 3-D CT scan classiﬁca-\ntion possible. However, it is well-known that the RNN and\nLSTM are hard to parallelize, leading to both training and\ninference time being hard to accelerate. Furthermore, 3-D\nconvolution is signiﬁcantly high computational complexity\ncompared to 2-D convolution, leading to the fact that both\ntraining and inference costs are expensive.\nConsider the computational complexity and effective-\nness of CT scan-level classiﬁcation of COVID-19; we re-\nspectively propose two schemes to resolve those issues\nbased on the proposed maximum-likelihood estimation of\nSwin-Transformer [16] for slice-level classiﬁcation and our\nConvolutional CT scan-Aware Transformer for CT scan-\nlevel classiﬁcation.\n2. The Proposed Method\nIn this paper, two schemes are proposed based on the\nimage-level (i.e., slice-level) and 3-D CT scan. In the ﬁrst\nmodel, Swin-Transformer [16] is adopted as the backbone\nnetwork for single-slice-level COVID-19 classiﬁcation with\nWilcoxon signed-rank test [18], termed as DWCC (Deep\nWilcoxon signed-rank test for COVID-19 Classiﬁcation).\nIn the second model, we also explore the full 3-D infor-\nmation of the multiple slices of a CT scan series based on\ncontext feature learning, termed CCAT.\nThe ﬂowchart of the ﬁrst model is as shown in Fig.1\n(a). The positive and negative (i.e., COVID-19 and non-\nCOVID-19) CT scan slices to different distribution by\nSwin-Transformer [16], and determine the percentage in the\nmiddle of CT scan that has signiﬁcant symptoms. Next, we\napply outlier removal, and Wilcoxon signed-rank test [18]\nfor these generated samples. Wilcoxon signed-rank test can\ngive meaning and explainable to the predicted results by sta-\ntistical inference.\nThe entire ﬂowchart of the proposed second approach\nis depicted in Fig.1 (b). The key components of the pro-\nposed CCAT are the Within-Slice-Transformer (WST) and\nBewteen-Slice-Transformer (BST). The details of the pro-\nposed WST and BST will be introduced in the late subsec-\ntion. In this WST, the training and testing CT scans Xt and\nXv will be resized to a ﬁxed size in the spatial domain as\nwell as the Ls slices will be sampled from the original CT\nscan series Xct. Afterward, a conventional CNN (ResNet-\n50 [4] is used in this paper) is adopted to extract the feature\nmaps f ∈Rc×wf ×hf , where wf and hf indicate the width\nand height of c feature maps. The global averaging pool-\ning (GAP) is discarded to reserve the spatial information of\nf. BST is adopted to explore how to aggregate the feature\nmaps to context-encoded feature vector fbst based on self-\nattention mechanism. While the Ls context-encoded fea-\ntures are aggregated ft = [f0\nbst, f1\nbst, ..,fLs\nbst], the proposed\nWST is then used to mine the context features between fea-\ntures of slices fwst. Finally, a three-layer perceptron with\nLeakyReLU activation is designed as the classiﬁer.\n3. Experimental Result\nIn this paper, the dataset used to evaluate the perfor-\nmance of the proposed approach is COV19-CT-DB [8]. In\nCOV19-CT-DB, the training and validation set are parti-\ntioned by [8], where the number of training and validation\nCT scans are 1, 560 and 374, respectively. Since the annota-\ntion of the test set provided in [8] is unavailable, validation\nset provided in [8] is used to evaluate the performance of\nthe proposed methods.\nIn the training phase of our second model, the optimizer\nused in this paper is Adam [7], the initial learning rate is\n1e −4 and the learning decay is step scheduler with step\nsize 20. The total epochs is 100.\n3.1. Data pre-processing\n3.1.1 DWCC\nDue to some slices of CT scan might be useless for rec-\nognizing the COVID-19 (e.g., top/bottom slices might not\ncontain chest information), the slices selection of the CT\nscan in the training phase is essential, as well as in the eval-\nuation phase. In the training phase, 40% slices in the center\nof the CT scan are sampled, then augmentation and nor-\nmalization will be performed on these selected slices. To\nempirically determine the best fraction of the slices selec-\ntion, we conduct a performance based on different sampling\nsizes, as shown in Fig. 2. As a result, 30% ∼60% sample\nsizes in the center of CT scan make the best performance\nand therefore is suggested in our experiments.\n3.1.2 CCAT\nSince the number of the slices in each CT scan is signif-\nicantly different from each other, the number of the in-\nput slices should be ﬁxed to meet the requirements of our\nCCAT. In this paper, the number of slice Ls = 16, and\nthe sampling interval Lfreq = 2. We randomly sample\n16 slices in a CT scan to be an input data of the pro-\nposed CCAT. Meanwhile, the common data augmentation\nschemes, including blurring, noise, random contrast and\nbrightness, and optical distortion, are adopted in the train-\ning phase. Note that the random rotation and cropping are\nnot performed on each slice separately since it will lead to\nunstable of the context of slices of a CT scan. Therefore, the\nrandom cropping and rotation are performed on the sampled\n3-D volume instead of each slice. Each pixel value will be\nnormalized to be ranged [0, 1].\n3.2. Performance evaluation\nThe evaluation metrics used in this paper are accuracy,\nmacro-precision (P), macro-recall (R), and macro F1-score\n(F1). The performance comparison between the proposed\nand other peer methods is conducted in Table 1. It is clear\nthat the proposed DWCC and CCAT signiﬁcantly outper-\nform the baseline model [8] and other state-of-the-art model\n[5]. As we stated previously, the single-slice-level model is\nhard to capture the characteristics of the slices’ context, im-\nplying that the performance might be restricted. Although\nTable 1: Performance evaluation of validation set of the pro-\nposed methods and other peer methods in terms accuracy,\nmacro-precision, macro-recall, macro-F1-score (results in\nblue indicates the implemented ourselves).\nMethod Acc. P R F1\nBaseline [8] 0.724 0.731 0.688 0.700\nDenseNet201 [5] 0.732 0.714 0.703 0.708\nProposed DWCC 0.919 0.931 0.911 0.917\nProposed CCAT 0.933 0.935 0.929 0.932\nModel ensemble 0.941 0.947 0.935 0.939\nthe Wilcoxon signed-rank test is proposed in DWCC to ex-\nplore the importance of slices’ automatically, the context of\nslices is still hard to address in practice. In contrast, the pro-\nposed CCAT, the proposed BS-transformer, further explores\nthe context between slices, making the classiﬁer more reli-\nable and stable. Moreover, the backbone network used in\nour CCAT is ResNet-50 [4] only since our WS- and BS-\ntransformer can effectively extract the context features from\nCT scan. To further improve the performance, a model en-\nsemble based on majority voting policy is adopted to fuse\nthe predicted results of DWCC and CCAT, as shown in the\nlast row at Table 1. As a result, the proposed CCAT signiﬁ-\ncantly boosts the performance of the COVID-19 classiﬁca-\ntion task compared to other state-of-the-art methods.\nTo better understand the performance impact of\nthe hyper-parameters of the proposed CCAT, a hyper-\nparameters tuning is illustrated in Fig. 3, where the h is\nthe number of the multi-head setting and the Ls indicates\nthe number of the slices. As a result, the length of re-\ntrieved slices of the CT scan and the number of the multi-\nhead attention are suggested to 16 and 0 (i.e., gMLP [15]),\nrespectively. All models presented a smooth and stable\nlearning curve, implying that the proposed BS- and WS-\nTransformers can effectively learn the context features from\nthe CT scan.\n4. Conclusion\nThis paper has proposed two deep neural networks for\ntwo-dimensional (2-D) and three-dimensional (3-D) CT\nscan for COVID-19 classiﬁcation tasks. First, the pro-\nposed DWCC (Deep Wilcoxon signed-rank test for COVID-\n19 Classiﬁcation) adopts nonparametric statistics for deep\nlearning, making the predicted result more stable and ex-\nplainable, ﬁnding a series of slices with the most signiﬁcant\nsymptoms in CT scan. Second, the 3-D model has been\nproposed in this paper based on the pixel- and slice-level\ncontext mining, termed as CCAT (Convolutional CT scan-\nAware Transformer), to further explore the intrinsic features\nin both temporal and spatial dimensions. Moreover, the\nproposed CCAT inherited the advantages of conventional\nFigure 2: Validation performance of the proposed DWCC\nwith different sample sizes.\nCNNs and took the strengths of visual transformers. The\nvisualization of the CT scan of the proposed CCAT mod-\nels also veriﬁed that the critical insights of the symptoms\ncaused by COVID-19 should be able to localize, as only\nCT scan-level annotation has been given. The extensive ex-\nperiments have demonstrated that the proposed DWCC and\nCCAT signiﬁcantly outperform the state-of-the-art methods\nfor COVID-19 classiﬁcation of CT scans.\nReferences\n[1] Amine Amyar, Romain Modzelewski, Hua Li, and Su Ruan.\nMulti-task deep learning based ct imaging analysis for covid-\n19 pneumonia: Classiﬁcation and segmentation. Computers\nin Biology and Medicine, 126:104037, 2020.\n[2] El-Sayed M El-Kenawy, Abdelhameed Ibrahim, Seyedali\nMirjalili, Marwa Metwally Eid, and Sherif E Hussein. Novel\nfeature selection and voting classiﬁer algorithms for covid-\n19 classiﬁcation in ct images. IEEE Access , 8:179317–\n179335, 2020.\n[3] Nicolas Ewen and Naimul Khan. Targeted self supervision\nfor classiﬁcation on a small covid-19 ct scan dataset. In2021\nIEEE 18th International Symposium on Biomedical Imaging\n(ISBI), pages 1481–1485. IEEE, 2021.\n[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016.\n[5] Shaoping Hu, Yuan Gao, Zhangming Niu, Yinghui Jiang,\nLao Li, Xianglu Xiao, Minhao Wang, Evandro Fei Fang,\nWade Menpes-Smith, Jun Xia, et al. Weakly supervised deep\nlearning for covid-19 infection detection and classiﬁcation\nfrom ct images. IEEE Access, 8:118869–118883, 2020.\n[6] Aayush Jaiswal, Neha Gianchandani, Dilbag Singh, Vijay\nKumar, and Manjit Kaur. Classiﬁcation of the covid-19 in-\nfected patients using densenet201 based deep transfer learn-\ning. Journal of Biomolecular Structure and Dynamics, pages\n1–8, 2020.\nFigure 3: Validation performance curves of the proposed our CCAT model with different hyper-parameters in terms of (a)\nAUC, (b) Accuracy, (c) Macro F1-Score (evaluated by the ofﬁcial competition organization), (d) Macro Precision, and (e)\nMacro Recall.\n[7] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980 ,\n2014.\n[8] Dimitrios Kollias, Anastasios Arsenos, Levon Soukissian,\nand Stefanos Kollias. Mia-cov19d: Covid-19 detection\nthrough 3-d chest ct image analysis. arXiv preprint\narXiv:2106.07524, 2021.\n[9] Dimitrios Kollias, N Bouas, Y Vlaxos, V Brillakis, M Se-\nferis, Ilianna Kollia, Levon Sukissian, James Wingate, and S\nKollias. Deep transparent prediction through latent represen-\ntation analysis. arXiv preprint arXiv:2009.07044, 2020.\n[10] Dimitrios Kollias, Athanasios Tagaris, Andreas Stafylopatis,\nStefanos Kollias, and Georgios Tagaris. Deep neural archi-\ntectures for prediction in healthcare. Complex & Intelligent\nSystems, 4(2):119–131, 2018.\n[11] Dimitris Kollias, Y Vlaxos, M Seferis, Ilianna Kollia, Levon\nSukissian, James Wingate, and Stefanos D Kollias. Transpar-\nent adaptation in deep medical image diagnosis. In TAILOR,\npages 251–267, 2020.\n[12] Debanjan Konar, Bijaya K Panigrahi, Siddhartha Bhat-\ntacharyya, Nilanjan Dey, and Richard Jiang. Auto-diagnosis\nof covid-19 using lung ct images with semi-supervised shal-\nlow learning network. IEEE Access, 9:28716–28728, 2021.\n[13] Thomas C Kwee and Robert M Kwee. Chest ct in covid-\n19: what the radiologist needs to know. RadioGraphics,\n40(7):1848–1865, 2020.\n[14] Kunwei Li, Yijie Fang, Wenjuan Li, Cunxue Pan, Peixin Qin,\nYinghua Zhong, Xueguo Liu, Mingqian Huang, Yuting Liao,\nand Shaolin Li. Ct image visual quantitative evaluation and\nclinical classiﬁcation of coronavirus disease (covid-19). Eu-\nropean radiology, 30(8):4407–4416, 2020.\n[15] Hanxiao Liu, Zihang Dai, David R So, and Quoc V Le. Pay\nattention to mlps. arXiv preprint arXiv:2105.08050, 2021.\n[16] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows. arXiv preprint arXiv:2103.14030, 2021.\n[17] Ahmed Mohammed, Congcong Wang, Meng Zhao, Mohib\nUllah, Rabia Naseem, Hao Wang, Marius Pedersen, and\nFaouzi Alaya Cheikh. Weakly-supervised network for detec-\ntion of covid-19 in chest ct scans. IEEE Access, 8:155987–\n156000, 2020.\n[18] Denise Rey and Markus Neuh ¨auser. Wilcoxon-Signed-Rank\nTest, pages 1658–1659. Springer Berlin Heidelberg, Berlin,\nHeidelberg, 2011.\n[19] Sertan Serte and Hasan Demirel. Deep learning for diagnosis\nof covid-19 using 3d ct scans. Computers in Biology and\nMedicine, 132:104306, 2021.\n[20] Xinggang Wang, Xianbo Deng, Qing Fu, Qiang Zhou, Ji-\napei Feng, Hui Ma, Wenyu Liu, and Chuansheng Zheng.\nA weakly-supervised framework for covid-19 classiﬁcation\nand lesion localization from chest ct. IEEE transactions on\nmedical imaging, 39(8):2615–2625, 2020.\n[21] Yu-Huan Wu, Shang-Hua Gao, Jie Mei, Jun Xu, Deng-Ping\nFan, Rong-Guo Zhang, and Ming-Ming Cheng. Jcs: An ex-\nplainable covid-19 diagnosis system by joint classiﬁcation\nand segmentation. IEEE Transactions on Image Processing,\n30:3113–3126, 2021.\n[22] Jinyu Zhao, Yichen Zhang, Xuehai He, and Pengtao Xie.\nCovid-ct-dataset: a ct scan dataset about covid-19. arXiv\npreprint arXiv:2003.13865, 2020.",
  "topic": "Coronavirus disease 2019 (COVID-19)",
  "concepts": [
    {
      "name": "Coronavirus disease 2019 (COVID-19)",
      "score": 0.7122787237167358
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6240476369857788
    },
    {
      "name": "Computer science",
      "score": 0.5583768486976624
    },
    {
      "name": "Wilcoxon signed-rank test",
      "score": 0.5574013590812683
    },
    {
      "name": "Computed tomography",
      "score": 0.5377696752548218
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5042461156845093
    },
    {
      "name": "Transformer",
      "score": 0.4805331826210022
    },
    {
      "name": "Classifier (UML)",
      "score": 0.4370265007019043
    },
    {
      "name": "Pixel",
      "score": 0.4273239076137543
    },
    {
      "name": "Medicine",
      "score": 0.2253100574016571
    },
    {
      "name": "Mathematics",
      "score": 0.19674721360206604
    },
    {
      "name": "Radiology",
      "score": 0.16721493005752563
    },
    {
      "name": "Engineering",
      "score": 0.12121257185935974
    },
    {
      "name": "Statistics",
      "score": 0.08816862106323242
    },
    {
      "name": "Pathology",
      "score": 0.08705011010169983
    },
    {
      "name": "Disease",
      "score": 0.07896649837493896
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Mann–Whitney U test",
      "score": 0.0
    },
    {
      "name": "Infectious disease (medical specialty)",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 15
}