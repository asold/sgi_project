{
  "title": "BERT Meets CTC: New Formulation of End-to-End Speech Recognition with Pre-trained Masked Language Model",
  "url": "https://openalex.org/W4385567350",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2156739433",
      "name": "Yosuke Higuchi",
      "affiliations": [
        "Carnegie Mellon University",
        "Waseda University"
      ]
    },
    {
      "id": "https://openalex.org/A2176603489",
      "name": "Brian Yan",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2900433319",
      "name": "Siddhant Arora",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2145458669",
      "name": "Tetsuji Ogawa",
      "affiliations": [
        "Waseda University"
      ]
    },
    {
      "id": "https://openalex.org/A2620410515",
      "name": "Tetsunori Kobayashi",
      "affiliations": [
        "Waseda University"
      ]
    },
    {
      "id": "https://openalex.org/A2117472677",
      "name": "Shinji Watanabe",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3096297644",
    "https://openalex.org/W2973217961",
    "https://openalex.org/W3197140813",
    "https://openalex.org/W3217767527",
    "https://openalex.org/W2963240019",
    "https://openalex.org/W4297841367",
    "https://openalex.org/W2884975363",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2990391581",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4210758944",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W2327501763",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W4210663600",
    "https://openalex.org/W4294670162",
    "https://openalex.org/W3112157188",
    "https://openalex.org/W3200601846",
    "https://openalex.org/W3169714379",
    "https://openalex.org/W3007433671",
    "https://openalex.org/W3096109555",
    "https://openalex.org/W3097882114",
    "https://openalex.org/W4221151577",
    "https://openalex.org/W3155427814",
    "https://openalex.org/W4287989347",
    "https://openalex.org/W1915251500",
    "https://openalex.org/W2946375144",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2962780374",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W1922655562",
    "https://openalex.org/W2939111082",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W3160987270",
    "https://openalex.org/W2577366047",
    "https://openalex.org/W3100460087",
    "https://openalex.org/W2043701535",
    "https://openalex.org/W2988975212",
    "https://openalex.org/W2963242190",
    "https://openalex.org/W2952230511",
    "https://openalex.org/W3163793923",
    "https://openalex.org/W3097777922",
    "https://openalex.org/W4224916448",
    "https://openalex.org/W3162899666",
    "https://openalex.org/W854541894",
    "https://openalex.org/W3148001440",
    "https://openalex.org/W2884001105",
    "https://openalex.org/W3161048756",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W1526990717",
    "https://openalex.org/W4226521565",
    "https://openalex.org/W4287210743",
    "https://openalex.org/W3115462295",
    "https://openalex.org/W2407080277",
    "https://openalex.org/W2962824709",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W4210300569",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2143612262",
    "https://openalex.org/W3141961557",
    "https://openalex.org/W3196500669",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2973122799",
    "https://openalex.org/W3093345276",
    "https://openalex.org/W1828163288",
    "https://openalex.org/W2888779557",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2802023636",
    "https://openalex.org/W3197148831",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3096160024",
    "https://openalex.org/W2251321385",
    "https://openalex.org/W3162249256",
    "https://openalex.org/W3035445001",
    "https://openalex.org/W4225824617",
    "https://openalex.org/W3015960524",
    "https://openalex.org/W2130942839"
  ],
  "abstract": "This paper presents BERT-CTC, a novel formulation of end-to-end speech recognition that adapts BERT for connectionist temporal classification (CTC). Our formulation relaxes the conditional independence assumptions used in conventional CTC and incorporates linguistic knowledge through the explicit output dependency obtained by BERT contextual embedding. BERT-CTC attends to the full contexts of the input and hypothesized output sequences via the self-attention mechanism. This mechanism encourages a model to learn inner/inter-dependencies between the audio and token representations while maintaining CTC's training efficiency. During inference, BERT-CTC combines a mask-predict algorithm with CTC decoding, which iteratively refines an output sequence. The experimental results reveal that BERT-CTC improves over conventional approaches across variations in speaking styles and languages. Finally, we show that the semantic representations in BERT-CTC are beneficial towards downstream spoken language understanding tasks.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 5486–5503\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nBERT Meets CTC: New Formulation of End-to-End Speech Recognition\nwith Pre-trained Masked Language Model\nYosuke Higuchi1,2, Brian Yan1, Siddhant Arora1, Tetsuji Ogawa2,\nTetsunori Kobayashi2, Shinji Watanabe1\n1Carnegie Mellon Univeristy, 2Waseda University\nhiguchi@pcl.cs.waseda.ac.jp\nAbstract\nThis paper presents BERT-CTC, a novel for-\nmulation of end-to-end speech recognition that\nadapts BERT for connectionist temporal clas-\nsification (CTC). Our formulation relaxes the\nconditional independence assumptions used in\nconventional CTC and incorporates linguistic\nknowledge through the explicit output depen-\ndency obtained by BERT contextual embed-\nding. BERT-CTC attends to the full contexts\nof the input and hypothesized output sequences\nvia the self-attention mechanism. This mecha-\nnism encourages a model to learn inner/inter-\ndependencies between the audio and token rep-\nresentations while maintaining CTC’s training\nefficiency. During inference, BERT-CTC com-\nbines a mask-predict algorithm with CTC de-\ncoding, which iteratively refines an output se-\nquence. The experimental results reveal that\nBERT-CTC improves over conventional ap-\nproaches across variations in speaking styles\nand languages. Finally, we show that the se-\nmantic representations in BERT-CTC are ben-\neficial towards downstream spoken language\nunderstanding tasks.\n1 Introduction\nThe field of natural language processing (NLP)\nhas witnessed remarkable improvements in perfor-\nmance thanks to the advances in deep learning-\nbased techniques (Collobert et al., 2011; Bahdanau\net al., 2015; Sutskever et al., 2014; Vaswani et al.,\n2017; Young et al., 2018). Much of the recent\nprogress in NLP lies in large-scale language mod-\nels (LMs) (Devlin et al., 2019; Brown et al., 2020),\nwhich are pre-trained on a vast amount of text\ndata to learn versatile linguistic knowledge (Tenney\net al., 2019). Such pre-trained models have been\nshown to improve diverse NLP tasks, alleviating\nthe heavy requirement of supervised training data.\nInspired by the great success in NLP, pre-trained\nLMs have been actively adopted for speech process-\ning tasks, including automatic speech recognition\n(ASR) (Shin et al., 2019; Huang et al., 2021), spo-\nken language understanding (SLU) (Chuang et al.,\n2020; Chung et al., 2021), and text-to-speech syn-\nthesis (Hayashi et al., 2019; Kenter et al., 2020).\nThis paper focuses on leveraging pre-trained\nLMs for end-to-end ASR (E2E-ASR), which aims\nto model direct speech-to-text conversion (Graves\nand Jaitly, 2014; Chorowski et al., 2015; Chan et al.,\n2016). One of the challenges in E2E-ASR is a huge\ndiscrepancy between input and output sequences;\nthe input is a continuous acoustic signal with fine-\ngrained patterns, while the output is discrete lin-\nguistic symbols (e.g., words) with long-range de-\npendencies. Such an input-output gap makes it\ndifficult for an E2E-ASR model to extract seman-\ntic/morphosyntax information from speech, which\nis essential for generating proper text. We believe\nthis limitation can be mitigated by taking advan-\ntage of the rich linguistic representations obtained\nfrom pre-trained LMs.\nSeveral attempts have been made to use pre-\ntrained LMs indirectly for improving E2E-ASR,\nsuch as N-best hypothesis rescoring (Shin et al.,\n2019; Salazar et al., 2020; Chiu and Chen, 2021;\nFutami et al., 2021; Udagawa et al., 2022) and\nknowledge distillation (Futami et al., 2020; Bai\net al., 2021; Kubo et al., 2022). Others have in-\nvestigated directly unifying an E2E-ASR model\nwith a pre-trained LM, where the LM is fine-tuned\nto optimize ASR in an end-to-end trainable frame-\nwork (Huang et al., 2021; Yi et al., 2021; Zheng\net al., 2021; Deng et al., 2021; Yu et al., 2022).\nWe explore a novel direction for adopting a\npre-trained masked language model (MLM) for\nE2E-ASR, based on connectionist temporal clas-\nsification (CTC) (Graves et al., 2006). Com-\npared to other autoregressive approaches, such\nas RNN-Transducer (RNN-T) (Graves, 2012) and\nattention-based sequence-to-sequence (Chorowski\net al., 2015), CTC’s non-autoregressive formula-\ntion allows simple training and inference processes\n5486\nfor realizing E2E-ASR. However, the performance\nof CTC is often limited due to a conditional inde-\npendence assumption between output tokens (Chiu\net al., 2018). In this work, we propose BERT-\nCTC that adapts BERT (Devlin et al., 2019) for\nCTC to mitigate the conditional independence as-\nsumption. BERT-CTC conditions CTC outputs on\ncontext-aware BERT embeddings, thereby incor-\nporating explicit linguistic information into train-\ning/inference. The BERT-conditional formulation\nenables a model to attend to the full contexts of the\ninput and hypothesized output sequences via the\nself-attention mechanism, while maintaining the\nbenefits of a simple training algorithm in CTC. Dur-\ning inference, BERT-CTC combines a mask-predict\nalgorithm with CTC decoding, which iteratively re-\nfines outputs with flexible length adjustment.\nThe key contributions of this work are summa-\nrized as follows:\n• We propose BERT-CTC, which efficiently\nadapts a pre-trained MLM for CTC-based\nE2E-ASR without fine-tuning. We provide\na probabilistic formulation of our BERT-CTC\nand its close relation to conventional ap-\nproaches, i.e., CTC and RNN-T.\n• We evaluate BERT-CTC in various ASR tasks,\nwhich demonstrates its effectiveness regard-\nless of variations in speaking styles and lan-\nguages. We also show its potential application\nto end-to-end SLU.\n• The codes and recipes are open-sourced on\nESPnet (Watanabe et al., 2018), the widely\nused toolkit for end-to-end speech process-\ning.1 We hope our work encourages further\nresearch on combining ASR with pre-trained\nLMs, helping to bridge ASR and NLP fields.\n2 Background\nTo understand how BERT-CTC exploits BERT\nfor relaxing the conditional independence assump-\ntion in CTC, we start with a brief review of prob-\nabilistic formulations of conventional E2E-ASR\napproaches, including CTC (Graves et al., 2006;\nGraves and Jaitly, 2014) and RNN-T (Graves,\n2012; Graves et al., 2013).\nDefinition of End-to-End ASR Let O= (ot ∈\nRD|t= 1,··· ,T) be an input sequence of length\n1https://github.com/YosukeHiguchi/espnet/tree/\nbert-ctc\nT, and W = (wn ∈V|n= 1,··· ,N) be the cor-\nresponding output sequence of length N. Here,\not is a D-dimensional acoustic feature at frame\nt, wn is an output token at position n, and Vis a\nvocabulary.2 In general, the output length is much\nshorter than the input length (i.e., N ≪T). The\nobjective of ASR is to find the most probable out-\nput sequence ˆW that corresponds to a given input\nsequence O:\nˆW = argmax\nW∈V∗\np(W|O), (1)\nwhere V∗ denotes all possible token sequences.\nE2E-ASR aims to realize the direct mapping from\nO to W by modeling the posterior distribution\np(W|O) with a single deep neural network.\n2.1 Connectionist Temporal Classification\nCTC formulates E2E-ASR by considering all pos-\nsible alignments between an input sequence O\nand output sequence W. To align the sequences\nat the frame level, CTC augments an output se-\nquence by allowing repetitions of the same to-\nken and inserting a blank symbol ϵfor represent-\ning “no output token” (e.g., silence). Let A de-\nnote an augmented output sequence defined as\nA = ( at ∈ V∪{ϵ}|t = 1 ,··· ,T), which we\nrefer to as an alignment between Oand W.\nWith the introduction of the frame-level align-\nment, CTC factorizes p(W|O) as follows:\npctc(W|O) =\n∑\nA∈B−1\nctc (W)\np(W|A,\u0000\u0000O)p(A|O) (2)\n≈\n∑\nA∈B−1\nctc (W)\np(A|O), (3)\nwhere Bctc is the collapsing function (Graves et al.,\n2006) that maps Ato W by suppressing repeated\ntokens and removing blank symbols, and B−1\nctc (W)\nis a set of all possible CTC alignments that are\ncompatible with W. To obtain Eq. (3), CTC makes\na conditional independence assumption of O in\nEq. (2), and we assume p(W|A) = 1, as W can be\ndetermined uniquely by the collapsing function.\nThe joint probability p(A|O) is further factor-\nized using the probabilistic chain rule as\np(A|O) ≈\nT∏\nt=1\np(at|((((((a1,··· ,at−1,O). (4)\n2We consider V as a vocabulary constructed for pre-\ntraining a large-scale MLM, i.e., BERT.\n5487\nIn Eq. (4), CTC makes a conditional independence\nassumption between output tokens, where p(A|O)\nis approximated as the product of token emission\nprobabilities at each time frame. The conditional\nprobability p(at|O) in Eq. (4) is computed as\np(at|O) = Softmax(Linear(hae\nt )), (5)\nhae\nt ∼AudioEnc(O). (6)\nIn Eq. (5), Softmax(·) is a softmax function, and\nLinear(·) is a linear projection layer. AudioEnc(·)\nin Eq. (6) is an audio encoder network that embeds\nspeech input into a sequence of dae-dimensional\nhidden vectors Hae = (hae\nt ∈Rdae\n|t= 1,··· ,T).\nTraining The objective function of CTC is de-\nfined by the negative log-likelihood of Eq. (4) over\nall possible alignments:\nLctc(O,W ) = −log\n∑\nA∈B−1\nctc (W)\nT∏\nt=1\np(at|O). (7)\nThe summation in Eq. (7) is efficiently computed\nvia dynamic programming (Graves et al., 2006).\nInference Eq. (1) is solved using the best path\ndecoding algorithm (Graves et al., 2006). The al-\ngorithm first obtains the most probable alignment\nˆAin a greedy manner, concatenating the most ac-\ntive tokens at each frame: ˆat = argmaxat p(at|O).\nThe most probable token sequence ˆW is then ob-\ntained by applying the collapsing function to ˆAas\nˆW = Bctc( ˆA).\n2.2 RNN-Transducer\nCTC estimates the distribution over alignments\nonly depending on speech input (Eq. (4)). Thus,\nby definition, CTC cannot consider output depen-\ndencies, preventing a model from properly cap-\nturing the multimodal distribution of target token\nsequences (Gu et al., 2018).\nRNN-T overcomes this problem by making each\ntoken prediction explicitly conditioned on the previ-\nous non-blank output tokens (w1,··· ,wn−1). Let\nZ = ( zu ∈V∪{ ϵ}|u = 1 ,··· ,T + N) be an\nalignment used in RNN-T, and RNN-T factorizes\np(W|O) similarly to Eq. (3) as\nprnnt(W|O) ≈\n∑\nZ∈B−1\nrnnt(W)\np(Z|O), (8)\nwhere Brnnt is the collapsing function of RNN-\nT (Graves, 2012) that map Zto W. The joint prob-\nability p(Z|O) is factorized using the probabilistic\nchain rule without the conditional independence\nassumption (cf. Eq. (4)) as\np(Z|O) =\nT+N∏\nu=1\np(zu|z1,··· ,zu−1,O) (9)\n≈\nT+N∏\nu=1\np(zu|w1,··· ,wnu−1  \n=Brnnt(z1,···,zu−1)\n,O), (10)\nwhere nu is the number of tokens predicted up to\nan index of u. From Eq (9) to Eq. (10), RNN-T as-\nsumes (z1,··· ,zu−1) ≈(w1,··· ,wnu−1), which\nis reasonable since W can be determined uniquely\nby the collapsing function. The conditional proba-\nbility p(zu|w1,··· ,wnu−1,O) is computed as\np(zu|w1,··· ,wnu−1,O)\n= Softmax(JointNet(hae\nt ,hpn\nnu )), (11)\nhpn\nnu = PredictionNet(w1,··· ,wnu−1). (12)\nIn Eq. (11), hae\nt is obtained from the audio encoder\n(Eq. (6)), and JointNet(·) is a joint network that\ncombines the audio and token representations, hae\nt\nand hpn\nnu , using a linear projection layer. In Eq.(12),\nPredictionNet(·) is a prediction network that en-\ncodes the previous non-blank output tokens to a\nhidden vector hpn\nnu . The adoption of the prediction\nnetwork is the main difference from CTC, which\nexplicitly captures causal dependency in outputs.\nTraining The RNN-T loss Lrnnt(O,W ) is de-\nfined by the negative log-likelihood of Eq. (10).\nSimilar to the CTC objective in Eq.(7), the summa-\ntion over alignments is efficiently computed using\ndynamic programming (Graves, 2012).\nInference RNN-T estimates the most probable\ntoken sequence ˆW using the beam search algorithm\nproposed in (Graves, 2012).\n3 BERT-CTC\nOverview In Fig. 1, we compare our proposed\nE2E-ASR model, BERT-CTC, to CTC and RNN-\nT. BERT-CTC leverages powerful representations\nfrom BERT (Devlin et al., 2019) to make CTC\ntraining/inference explicitly conditioned on linguis-\ntic information (Fig. 1(a) vs. Fig. 1(c)). We use\nBERT as a feature extractor for a (masked) token\nsequence, whose parameters are frozen during train-\ning. BERT-CTC can be similar to RNN-T in that\naudio and token representations are fused to es-\ntimate the distribution over alignments (Fig. 1(b)\n5488\nSoftmax\nAcousticEncoderO\nAudio Encoder\np(at|O)\nhaet\n(a) CTC\nSoftmax\nAcousticEncoderO\nAudio EncoderPrediction Net.\nJoint Networkhaet hpnnu\np(zu|w1,··· ,wnu−1,O)\nw1,··· ,wnu−1 (b) RNN-T\nSoftmax\nAcousticEncoderO\nSelf-Attention\n˜W\nBERT (Freeze)Audio Encoder\np(at,BERT(˜W),O) (c) BERT-CTC (ours)\nFigure 1: Comparisons between different model architectures for end-to-end ASR.\nvs. Fig. 1(c)). However, BERT-CTC attends to\nthe full contexts of the input and output sequences\nvia the self-attention mechanism (Vaswani et al.,\n2017), which facilitates a model to learn inner/inter-\ndependencies within/between the sequences.\nBERT-CTC is formulated by introducing a par-\ntially masked (or partially observed) sequence\n˜W = ( ˜wn ∈V∪{[MASK]}|n= 1,··· ,N), which\nis obtained by replacing some tokens in an output\nsequence W with a special mask token [MASK].\nNote that during inference, we apply masks to a\nhypothesized sequence ˆW to obtain a masked se-\nquence. Considering all possible ˜W, the condi-\ntional probability p(W|O) is factorized as follows:\npbc(W|O) =\n∑\n˜W∈A(W)\np(W|˜W,O )p( ˜W|O), (13)\nwhere A(W) covers W with all possible masking\npatterns. Here, we interpret p( ˜W|O) as a prior\ndistribution of sequences consisting of observed\ntokens that are easily recognized only from speech\ninput; the other masked tokens are difficult and re-\nquire contextual information to be determined (e.g.,\nhomophones), which is modeled by p(W|˜W,O ).\nWe further describe the above interpretation in the\ntraining (§3.1) and inference (§3.2) sections.\nThe conditional probability p(W|˜W,O ) is fur-\nther factorized by using the CTC alignment as\np(W|˜W,O ) =\n∑\nA∈B−1\nctc (W)\np(W,A|˜W,O ) (14)\n≈\n∑\nA∈B−1\nctc (W)\np(A|W,\u0000\u0000˜W,O )p(W|˜W,\u0000\u0000O). (15)\nIn Eq. (15), we make two conditional independence\nassumptions. The first is that given W and O, ˜W\nis not required to determine A. This is reasonable\nbecause W already contains observed tokens in ˜W\nand is helpful in avoiding the combination of all\npossible masked sequences and alignments (i.e.,\nA×B −1\nctc ). The second is that given ˜W, Ois not\nrequired to determine W. We consider p(W|˜W)\nas a strong prior modeled by a pre-trained MLM\n(i.e., BERT), which can be achieved without the\nobservation from O. We empirically show that this\nassumption holds in §7.3.\nSimilar to CTC, the joint probability p(A|W,O)\nis factorized using the probabilistic chain rule as\np(A|W,O) ≈\nT∏\nt=1\np(at|((((((a1,··· ,at−1,W,O ). (16)\nTo obtain Eq. (16), we make the same conditional\nindependence assumption as in CTC. However,\ncompared to Eq. (4), Eq. (16) is conditioned on an\noutput sequence W, enabling a model to explicitly\nuse linguistic information to estimate the distribu-\ntion over alignments. This is somewhat similar to\nRNN-T (Eq. (10)), but is different in that BERT-\nCTC attends to the whole context (w1,··· ,wN ).\nWe discuss this advantage in §7.1.\nSubstituting Eq. (16) into Eq. (15), we model\nthe product of p(at|W,O) and p(W|˜W) as\nEq. (15) ≜\n∑\nA∈B−1\nctc (W)\nT∏\nt=1\np(at|BERT( ˜W),O),\n(17)\nwhere BERT(·) is the output of BERT represent-\ning the distribution of target sequences.3 This en-\nables Eq. (17) to be realized with a single differ-\nentiable model, enabling the whole network to be\n3Note that BERT(·) can be any pre-trained MLM.\n5489\ntrained end-to-end. The conditional probability\np(at|BERT( ˜W),O) is computed as\np(at|BERT( ˜W),O)\n= Softmax(SelfAttnt(Hae,Hbert)), (18)\nHbert = BERT( ˜W). (19)\nIn Eq. (18), SelfAttnt(·) indicates the t-th output of\nstacked Transformer self-attention layers (Vaswani\net al., 2017), which consume the concatenated Hae\n(from Eq. (6)) and Hbert.4 In Eq. (19), BERT(·)\nembeds a masked sequence ˜W into a sequence of\ndbert-dimensional hidden vectorsHbert = (hbert\nn ∈\nRdbert\n|n= 1,··· ,N).\n3.1 Training\nThe BERT-CTC objective is defined by the negative\nlog-likelihood of Eq. (13) expanded with Eq. (15):\n−log\n∑\n˜W\n∑\nA\np(A|W,O)p(W|˜W)p( ˜W|O). (20)\nTo deal with the intractable marginalization over\n˜W in Eq. (20), we rewrite it under expectation with\nrespect to the sampling distribution A(W):\n≈−logE˜W∼A(W)\n[∑\nA\np(A|W,O)p(W|˜W)p( ˜W|O)\n]\n,\nwhose upper bound can be derived by using the\nJensen’s inequality as\n≤−E˜W∼A(W)\n[\nlog\n∑\nA\np(A|W,O)p(W|˜W)p( ˜W|O)\n]\n≤−E˜W∼A(W)\n[\nlog\n∑\nA\n∏\nt\np(at|BERT( ˜W),O)\n]\n  \n≜Lbc(O,W)\n, (21)\nwhere Lbc is the loss for BERT-CTC training.\nCompared with the CTC objective (Eq. (7)), each\ntoken prediction in Eq. (21) is explicitly condi-\ntioned on contextual embedding from BERT. This\nrelaxes the conditional independence assumption\nbetween outputs while retaining the same opti-\nmization strategy as in CTC. For sampling ˜W\nfrom A(W) in Eq. (21), we first obtain the ran-\ndom number of tokens from a uniform distribution\nas M ∼ Uniform(1,N). Then, M tokens in a\nground-truth sequence W are randomly selected to\nbe replaced with [MASK], similar to (Ghazvininejad\net al., 2019).\n4We apply simple embedding layers to Hae and Hbert so\nthat the dimensions of hidden vectors match, but we omit it\nfor simplicity. See Appendix D.2 for detailed implementation.\nHierarchical Loss We apply an auxiliary CTC\nloss to the audio encoder output in a hierarchi-\ncal multi-tasking manner (Fernández et al., 2007;\nSanabria and Metze, 2018). As the vocabulary\nsize of BERT is often too large for ASR training,\nwe train the audio encoder to predict a sequence\nW′ = (w′\nl ∈V′|l = 1,··· ,L) tokenized with a\nsmaller vocabulary V′(i.e., |V′|≪|V| ). This has\nbeen shown effective for training sparse word-level\nASR (Higuchi et al., 2022). The BERT-CTC loss\nis combined with the hierarchical CTC loss as\n(1 −λctc)Lbc(O,W ) + λctcLctc(O,W ′), (22)\nwhere λctc is a tunable parameter. We investigate\nthe importance of the hierarchical loss in §7.1.\n3.2 Inference\nThe most probable token sequence ˆW is estimated\nby solving Eq. (1) for Eq. (13) as\nˆW = argmax\nW\n∑\n˜W\np(W|˜W,O )p( ˜W|O) (23)\n≈argmax\nW\np(W|¯W,O ), (24)\nwhere ¯W = argmax\n˜W\np( ˜W|O). (25)\nFrom Eq. (23) to Eq. (24), we make the Viterbi ap-\nproximation to deal with the intractable summation\nover all possible masked sequences.\nTo solve Eq.(24), we design a mask-predict algo-\nrithm (Ghazvininejad et al., 2019) assisted by CTC\ninference, inspired by (Chan et al., 2020; Higuchi\net al., 2020). See Table 4 for an example decoding\nand Appendix A for pseudocode. The algorithm\nfirst initializes a target sequence with an estimated\nlength, which is then followed by k= {1,··· ,K}\niterations of token masking and prediction steps.\nInitialization (k = 1) BERT-CTC is non-auto-\nregressive, and the length of a target sequence ˆN\nneeds to be given in advance to start decoding (Gu\net al., 2018). We determine the target length based\non the auxiliary sequence ˆW′predicted from the\naudio encoder output Hae as ˆN ∼| ˆW′|. Given\nthe estimated length, we initialize an initial masked\nsequence ¯W(k=1) by filling all ˆN positions with\nthe mask token [MASK]. By feeding Hae and Hbert\n(= BERT( ¯W(k=1))) to the self-attention module,\na hypothesized sequence ˆW(k=1) is obtained via\nCTC inference. Here, ˆW(k=1) is predicted only\nfrom speech without any observations from output\ntokens, as they are all masked.\n5490\nToken Masking Step (Eq. (25)) Given a current\nprediction ˆW(k), we replace m(k) tokens having\nthe lowest probability scores with [MASK], which\nresults in the next masked sequence ¯W(k+1). Here,\nm(k) is a linear decay function m(k) = ⌊|ˆW(k)|·\nK−k\nK ⌋, similar to (Ghazvininejad et al., 2019).\nToken Prediction Step(Eq. (24)) Hae and Hbert\n(= BERT( ¯W(k+1))) are fed to the self-attention\nmodule to generate the next hypothesis ˆW(k+1).\nHere, the prediction of ˆW(k+1) is conditioned on\nthe contextual embedding obtained from BERT.\nSimilar to (Chan et al., 2020; Chi et al., 2021),\nBERT-CTC inference repeatedly predicts a target\nsequence at the alignment level, which does not\nrequire an additional mechanism (Gu et al., 2019;\nHiguchi et al., 2021b) for adjusting the target length\nover iterations. Moreover, BERT-CTC considers\nthe output dependencies at the token level, making\nit more suitable for a model to capture linguistic\ninformation.\n3.3 BERT-CTC for End-to-End SLU\nIn addition to E2E-ASR, BERT-CTC can model\nend-to-end SLU jointly by extending Eq. (18) as\np(y|BERT( ˜W),O)\n= Softmax(SelfAttnT+1(Hae,Hbert)), (26)\nwhere we assume y∈Y as an intent label in a set\nof intents Y. Note that SelfAttnT+1(·) indicates\nthe T + 1-th output of the self-attention module,\nwhich corresponds to the [CLS] token of BERT.\nTraining The loss is defined by adding Eq. (22)\nand the negative log-likelihood of Eq. (26) as\nEq. (22) −λslu log p(y|BERT( ˜W),O), (27)\nwhere λslu is a tunable parameter.\nInference The most probable label ˆycan be es-\ntimated at any timing of BERT-CTC inference by\nˆy = argmaxy p(y|¯W,O ). When k = 1, the label\nis predicted only from audio information, and when\nk = K, the label is predicted with full access to\naudio and linguistic information.\n4 Additional Related Work\nEnd-to-End ASR with MLM Inspired by the\ngreat success in non-autoregressive neural ma-\nchine translation, conditional masked language\nmodel (CMLM) (Ghazvininejad et al., 2019) has\nbeen adopted for E2E-ASR. Audio-CMLM (A-\nCMLM) (Chen et al., 2020) has trained an E2E-\nASR model with an MLM objective (Devlin et al.,\n2019), making token predictions conditioned on\nboth the speech input and a partially masked target\nsequence. Imputer (Chan et al., 2020) and Mask-\nCTC (Higuchi et al., 2020, 2021b) have introduced\nCTC to the CMLM-based modeling, where the\nmask-predict algorithm is used to refine a frame-\nlevel or token-level sequence predicted by CTC.\nOur method of combining CTC and MLM is\nrelated to the above studies, but conceptually differ-\nent in that BERT-CTC aims to relax the conditional\nindependence assumption used in CTC by leverag-\ning an external pre-trained MLM (i.e., BERT) as\ncontextual embedding.\nLM Integration for End-to-End ASR. There\nis a line of prior studies seeking to integrate an\nexternal LM into E2E-ASR. Shallow fusion has\nbeen the most widely used approach (Hannun et al.,\n2014; Gulcehre et al., 2015; Chorowski and Jaitly,\n2017; Kannan et al., 2018), which linearly inter-\npolates the output probabilities from an E2E-ASR\nmodel and external LM. Deep fusion (Gulcehre\net al., 2015) is a more structured approach, where\nan E2E-ASR model is jointly trained with an ex-\nternal LM to learn the optimal combination of the\naudio and linguistic information in a latent space.\nCold fusion (Sriram et al., 2018) and component\nfusion (Shan et al., 2019) have further improved\ndeep fusion by a gating mechanism that learns a\nmore sophisticated combination of the two models.\nOur approach can be seen as a variant of cold\nfusion in that an external pre-trained MLM is fused\nto a CTC-based E2E-ASR model, selectively com-\nbining audio and linguistic representations via the\nself-attention mechanism. However, BERT-CTC\nis a novel direction in which we seek to integrate\nBERT into a CTC-based model in a theoretically-\nsound manner.\n5 Experiments\nWe used the ESPnet toolkit (Watanabe et al., 2018)\nfor all the experiments. All the implementations\nand recipes are made publicly available (see §1).\n5.1 Tasks and Datasets\nSpeech Recognition We evaluated models on\nthe LibriSpeech (Panayotov et al., 2015), TED-\nLIUM2 (Rousseau et al., 2014) and AISHELL-\n1 (Bu et al., 2017) datasets. LibriSpeech consists of\n5491\nread English speech from audiobooks, and we used\ntrain-clean-100 for training. TED-LIUM2 con-\ntains spontaneous English speech from Ted Talks.\nAISHELL-1 consists of read Mandarin speech.\nSpoken Language Understanding We also eval-\nuated our model on the SLURP dataset (Bastianelli\net al., 2020). SLURP consists of English prompts\nof an in-home personal robot assistant, and we fo-\ncused on the intent classification task.\nWe used the standard development and test sets\nfor tuning hyper-parameters and evaluating perfor-\nmance for each dataset. Full dataset descriptions\nare in Appendix D.1.\n5.2 End-to-End ASR Models\nCTC (baseline): A model trained based on the\nCTC loss Lctc (see §2.1). Given the recent ad-\nvances in CTC-based modeling (Higuchi et al.,\n2021a), we built a strong baseline using the in-\ntermediate CTC technique (Tjandra et al., 2020;\nLee and Watanabe, 2021), which applies an auxil-\niary CTC loss to intermediate outputs of the audio\nencoder. We used the intermediate loss in a hierar-\nchical manner (Sanabria and Metze, 2018), where\nthe loss is calculated using a target sequence tok-\nenized with a smaller vocabulary (i.e., V′in §3.1).\nRNN-T (baseline): A model trained based on the\nRNN-T loss Lrnnt (see §2.2). Considering the re-\ncent techniques developed upon multi-task learn-\ning (Boyer et al., 2021), we trained a strong model\nusing an auxiliary CTC loss applied to the audio en-\ncoder output (Jeon and Kim, 2021). Same as CTC,\nwe enhanced the audio encoder with intermediate\nCTC (Lee et al., 2022). All the CTC losses were\ncalculated using the smaller-vocabulary sequence.\nBERT-CTC (ours): The proposed model trained\nbased on the BERT-CTC loss (Eq.(22)). As in the\nother models, we adopted intermediate CTC for the\naudio encoder. All the CTC losses were calculated\nusing the smaller-vocabulary sequence.\nSee Appendices B and C for intermediate CTC\nand detailed model descriptions, respectively.\n5.3 Experimental Settings\nModel Configuration For the audio encoder,\nwe adopted the Conformer architecture (Gulati\net al., 2020), which consisted of 12 encoder blocks.\nThe prediction network in RNN-T was a single\nlong short-term memory (LSTM) layer. The self-\nattention module in BERT-CTC had 6 Transformer\nencoder blocks, and we used a BERTBASE model\n0 5 10 15 20 25 30\n8\n10\n12\n14\nIterationsK\nDev WER [%] (↓)\n3.5\n4\n4.5\n5\n5.5\nDev CER [%] (↓)\nLibriSpeech\nTED-LIUM2\nAISHELL-1\nFigure 2: BERT-CTC results on development sets, using\ndifferent number of decoding iterations.\nprovided by HuggingFace (Wolf et al., 2020).\nTokenization For each language, we used the\nsame vocabulary as BERT for tokenizing target\ntexts. We also constructed a smaller-sized vocab-\nulary V′for the hierarchical losses, which is ob-\ntained by applying the byte pair encoding-based\nalgorithm (Sennrich et al., 2016) to the transcrip-\ntion of each dataset.\nTraining We mostly followed ESPnet recipes\nprovided for each dataset. For BERT-CTC, we\nset λctc (in Eq. (22)) to 0.3 for all the ASR tasks\nand λslu (in Eq. (27)) to 1.0 for the SLU task.\nInference For CTC, we performed the best path\ndecoding (§2.1). For RNN-T, we used the beam\nsearch decoding (§2.2) with a beam size of 20. For\nBERT-CTC, unless otherwise indicated, the num-\nber of iterations Kwas always set to 20 (§3.2).\nDetailed experimental settings for reproducibil-\nity are in Appendix D.\n6 Results\nSpeech Recognition Table 1 shows results on\nLibriSpeech-100h and TED-LIUM2 in word error\nrate (WER), and AISHELL-1 in character error\nrate (CER). While RNN-T slightly outperformed\nCTC on several evaluation sets in LibriSpeech-\n100h and AISHELL-1, CTC resulted in better per-\nformance on TED-LIUM2. RNN-T was ineffective\nat training ASR with the BERT vocabulary, partic-\nularly when a severe mismatch exists against the\ntarget ASR domain (i.e., Wikipedia vs. lecture).\nBERT-CTC significantly outperformed the base-\nlines, consistently achieving the best results on all\ndatasets. BERT-CTC improved over RNN-T, and\nwe attribute this to not only considering the whole\ncontext of the target sequence but also using the\n5492\nModel\nLibriSpeech-100h TED-LIUM2 AISHELL-1\nDev WER (↓) Test WER ( ↓)\nDev WER (↓) Test WER ( ↓) Dev CER ( ↓) Test CER ( ↓)clean other clean other\nCTC† 11.2 21.4 11.4 22.0 9.9 9.3 5.1 5.6\nRNN-T† 9.7 21.5 9.8 22.2 10.2 9.6 5.2 5.5\nBERT-CTC 7.0 16.3 7.2 16.6 8.1 7.6 3.9 3.9\nTable 1: WER [%] on LibriSpeech-100h and TED-LIUM2, and CER [%] on AISHELL-1. †indicates that the\nmodels are slightly different from the original CTC or RNN-T in that they are trained with hierarchical CTC loss.\nModel WER (↓) Acc.(↑)\nESPnet-SLU (Arora et al., 2022) – 86.3\nASR + BERT (Arora et al., 2022) – 85.7\nBERT-CTC (K= 1) 19.1 87.0\nBERT-CTC (K= 20) 18.2 87.8\nTable 2: WER [%] and classification accuracy [%] on\nSLURP intent classification task.\nModel Dev WER (↓) Test WER(↓)\nCTC† 11.2/ 21.4 11.4 / 22.0\nw/o hierarchical loss 11.8 / 23.2 12.2 / 24.1\nRNN-T† 9.7/ 21.5 9.8 / 22.2\nw/o hierarchical loss 11.4 / 24.6 11.5 / 25.8\nBERT-CTC 7.0/ 16.3 7.2 / 16.6\nw/o hierarchical loss 8.6 / 19.1 8.9 / 19.5\nw/o BERT 7.4 / 17.2 7.4 / 17.7\nTable 3: Ablation studies on LibriSpeech-100h.\npowerful representations from BERT, which we fur-\nther analyze later. In Appendix E, we compare our\nAISHELL-1 results to those from recent works and\nshow that our approach is on par with the state-of-\nthe-art (Zheng et al., 2021) with fewer parameters.\nFigure 2 illustrates the correlation between BERT-\nCTC results and the number of decoding iterations.\nWhen decoded with K = 1, the model only uses\nspeech input to predict a token sequence. By in-\ncreasing K, the model beneficially exploited the\nBERT knowledge for refining the output tokens.\nSpoken Language Understanding Table 2 lists\nthe results of the SLURP intent classification task,\nevaluated in accuracy. We refer to the ESPnet-\nSLU (Arora et al., 2022) result as a baseline, which\nperforms SLU along with ASR by prepending an\nintent label to the corresponding output sequence.\nWe also refer to the ESPnet-SLU result obtained\nby stacking BERT on top of an ASR model, which\nwas found to be less effective. BERT-CTC outper-\nformed the baselines by effectively incorporating\nacoustic and linguistic information. By decoding\nin a single iteration (K = 1), BERT-CTC predicted\nan intent only from speech, and the accuracy was al-\nready higher than those of baselines. We observed\na slight but clear gain by increasing K, which im-\nproved both ASR and SLU performance thanks\nto BERT. We note that our result outperforms the\nstate-of-the-art 86.9% reported in (Seo et al., 2022).\n7 Analyses\n7.1 Ablation Studies\nTo validate the effectiveness of our model design\nfor BERT-CTC, we conduct ablation studies (Ta-\nble 3) on the usage of hierarchical loss and BERT.\nHierarchical Loss We observed that hierarchical\nCTC helped all the models improve their perfor-\nmance by a large margin. As the vocabulary of\nBERT is generally too large for E2E-ASR, the hi-\nerarchical modeling was crucial for predicting the\nsparse word-level tokens. Moreover, the result in-\ndicates that the hierarchical loss is effective for\ntraining an ASR model with a vocabulary from\na different domain, as there is a non-negligible\ndomain-mismatch between the BERT training text\nand ASR transcription.\nBERT To ablate BERT-CTC with BERT, we re-\nplaced BERT(·) in Eq. (19) with a simple embed-\nding layer with positional encoding. We found that\nremoving BERT led to degradation in BERT-CTC\nperformance, which supports the importance of us-\ning BERT. However, interestingly, the result was\nstill better than the baselines, indicating the advan-\ntage over RNN-T in that BERT-CTC is capable of\nconsidering the bi-directional context.\n7.2 Error Analysis with Decoding Example\nTable 4 shows a process of BERT-CTC inference,\ndecoding an utterance in the LibriSpeech test set.\n5493\nk=1 ... thoua gavemeetanyoneafterterthesehourreciteaughtofcourtrywhetherhebene’er ...\nk=10 ... thoua againmeetanyoneafterterthesehour reciteitingaughtofpoetryrywhetherhebenear’er ...\nk=15 ... thouagain meet any one afterthis hour reciteiting aught ofpoetryry whether he benear’or ...\nk=20 ... thou again meet any one after this hour reciteiting aught of poetry whether he be near ...\nw/oBERT ... thou a gag meet any one after this hour residing aught of boy whether he be near ...\nReference ... thou again meet any one after this hour reciting aught of poetry whether he be near ...\nTable 4: Decoding example from LibriSpeech test-other set (2033-164914-0016). At each iteration, the highlighted\ntokens are masked and repredicted in the next iteration. Blue indicates refined tokens, and red indicates ones not.\n0 15 30 45 60 75 90 105 120\nInput\n0\n15\n30\n45\n60\n75\n90\n105\n120 Output\n0 15 30 45 60 75 90 105 120\nInput\n0\n15\n30\n45\n60\n75\n90\n105\n120 Output\nFigure 3: Attention visualization for BERT-CTC. White\nlines indicate the boundaries of audio and token seqs.\nIn the output sequence at k = 1, the model mis-\ntakenly predicted phonetically similar tokens (e.g.,\n“again”→“a gave”, “near”→“ne’er”). At the first\niteration, the model was only conditioned on acous-\ntic information, making it challenging to determine\ntarget tokens accurately. As the iteration proceeded,\nthe model corrected the most errors by considering\nthe output dependency. Unlike the original mask-\npredict algorithm (Ghazvininejad et al., 2019), our\napproach permits for flexibly adjusting the target\nlength, enabling the model to resolve insertion and\ndeletion errors (e.g., “afterer”→“after”). We also\nshow an example obtained w/o BERT (from Ta-\nble 3), which failed to recover tokens that were\ncorrectly recognized by BERT-CTC with BERT.\n7.3 Conditional Independence of p(W|˜W,\u0000\u0000O)\nWe empirically validate the conditional indepen-\ndence assumption made in Eq. (15), where the\noutput sequence W depends only on its masked\nsequence ˜W without audio information O. To this\nend, we augmented the BERT module by inserting\nadaptive cross-attention layers, which is similar to\nAdapter-BERT Networks (Guo et al., 2020). These\nadditional layers are trained to infuse the audio en-\ncoder output Hae into each BERT layer, thereby\nallowing BERT-CTC to realizep(W|˜W,O ). When\nevaluated on LibriSpeech, the modified BERT-\nCTC resulted in 7.2%/17.9% on the dev. set and\n7.3%/18.0% on the test set, which are worse than\nthe results in Table 1. This indicates that BERT al-\nready captures sophisticated linguistic information\nand does not require extra parameters for adapting\nBERT to audio input.\n7.4 Attention Visualization\nFigure 3 depicts example attention weight matri-\nces, produced by the second self-attention layer of\nBERT-CTC. We observed two major attention pat-\nterns: weights aligning audio and token sequences\nby capturing inter-dependencies (Fig. 3 left) and\nweights attending inter-dependencies within each\nsequence (Fig. 3 right). These patterns support our\nmotivation for the BERT-CTC design in learning\ninner/inter dependencies within/between the audio\nand token representations.\n7.5 Inference Speed Comparison\nTo see how the iterative decoding with BERT af-\nfects the inference speed of BERT-CTC, we evalu-\nated each model on the real-time factor (RTF). RTF\nwas measured on the LibriSpeech test-other set\nusing a single GPU with a batchsize of 1 or a sin-\ngle CPU. RTFs for GPU / CPU inference resulted\nin 7.91e-3 / 4.18e-2 for CTC, 4.81e-1 / 4.55 for\nRNN-T, and 9.72e-2 / 7.22e-1 for BERT-CTC. The\nsemi-autoregressive characteristic in BERT-CTC\nenabled faster inference than autoregressive RNN-\nT and provided further speedup with the parallel\ncomputing using GPU.\n8 Conclusion\nWe proposed BERT-CTC that leverages BERT\nfor relaxing the conditional independence assump-\ntion in CTC. BERT-CTC uses BERT as contex-\ntual embedding to explicitly condition CTC train-\ning/inference on linguistic information. Experimen-\ntal results showed that BERT-CTC improved over\nconventional approaches. Moreover, we confirmed\nthat BERT-CTC is applicable to end-to-end SLU.\n5494\nDev WER(↓) Test WER(↓)\nV Model clean / other clean / other\nVasr CTC† 6.9 / 20.1 7.0 / 20.2\nVasr RNN-T† 5.7/ 17.0 6.0/ 17.2\nVbert CTC† 11.2 / 21.4 11.4 / 22.0\nVbert RNN-T† 9.7 / 21.5 9.8 / 22.3\nVbert BERT-CTC 7.0 / 16.3 7.2 /16.6\nTable 5: WER [%] on LibriSpeech-100h. Vasr indicates\na subword vocabulary constructed from ASR transcrip-\ntions, where |Vasr|= 300 . Vbert indicates the BERT\nvocabulary, where |Vbert|= 30522.\nLimitations\nVocabulary Constraint The output unit of\nBERT-CTC is constrained to the vocabulary of\nBERT, which is likely to be not generalized to an\nASR domain and too sparse for ASR training. Ta-\nble 5 shows results on LibriSpeech-100h with dif-\nferent vocabularies, where Vasr is an ASR vocabu-\nlary with a vocabulary size of 300 constructed from\nLibriSpeech transcriptions, and Vbert is the BERT\nvocabulary with a vocabulary size of 30522. We ob-\nserved that, by using Vasr, the performance of CTC\nand RNN-T improved over the results using Vbert\nand closed the gap with the BERT-CTC results. We\nbelieve that using a BERT variant with a smaller\nvocabulary, e.g., CharacterBERT (El Boukkouri\net al., 2020) improves BERT-CTC further.\nComputational Cost BERT-CTC requires a high\ncomputational cost, especially during inference,\ndue to the iterative forward calculations of BERT\n(i.e., K=20 times) with the O(N2) computational\nand memory complexities in the self-attention lay-\ners. Still, GPUs can greatly accelerate the inference\nspeed, and BERT-CTC can alternatively use other\npre-trained MLMs with lighter weights, e.g., AL-\nBERT (Lan et al., 2019) and DistilBERT (Sanh\net al., 2019).\nNon-streaming BERT-CTC is not suited for on-\nline streaming scenarios, where output tokens are\npredicted synchronously to sequential speech input.\nIt is not a significant problem when we consider\napplying BERT-CTC to utterance-level ASR tasks,\nsuch as end-to-end SLU as we demonstrated the\ncapability of BERT-CTC (Table 2). Otherwise, we\ncan adopt existing techniques for making BERT-\nCTC streaming, e.g., causal masking (Vaswani\net al., 2017), time-restricted attention (Povey et al.,\n2018), and block-wise processing (Tsunoo et al.,\n2019). Another solution can be to apply the two-\npass algorithm (Sainath et al., 2019), where BERT-\nCTC first performs streaming recognition at k= 1\nand then refines the outputs using the full context\ninformation at k> 1.\nAcknowledgements\nThis work was supported in part by JST ACT-X (JP-\nMJAX210J) and JSPS KAKENHI (JP21J23495).\nThis work used the Extreme Science and Engineer-\ning Discovery Environment (XSEDE) (Towns et al.,\n2014) supported by National Science Foundation\ngrant number ACI-1548562. It uses the Bridges\nsystem (Nystrom et al., 2015) supported by NSF\naward number ACI-1445606, at the Pittsburgh Su-\npercomputing Center (PSC).\nReferences\nSiddhant Arora, Siddharth Dalmia, Pavel Denisov, Xu-\nankai Chang, Yushi Ueda, Yifan Peng, Yuekai Zhang,\nSujay Kumar, Karthik Ganesan, Brian Yan, Ngoc\nThang Vu, Alan W Black, and Shinji Watanabe.\n2022. ESPnet-SLU: Advancing spoken language\nunderstanding through ESPnet. In Proceedings of\nthe 2022 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pages\n7167–7171.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In Proceedings of the\n3rd International Conference on Learning Represen-\ntations (ICML).\nYe Bai, Jiangyan Yi, Jianhua Tao, Zhengkun Tian,\nZhengqi Wen, and Shuai Zhang. 2021. Fast end-to-\nend speech recognition via non-autoregressive mod-\nels and cross-modal knowledge transferring from\nBERT. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, 29:1897–1911.\nEmanuele Bastianelli, Andrea Vanzo, Pawel Swieto-\njanski, and Verena Rieser. 2020. SLURP: A spo-\nken language understanding resource package. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7252–7262.\nFlorian Boyer, Yusuke Shinohara, Takaaki Ishii, Hiro-\nfumi Inaguma, and Shinji Watanabe. 2021. A study\nof Transducer based end-to-end ASR with ESPnet:\nArchitecture, auxiliary loss and decoding strategies.\nIn Proceedings of the 2021 IEEE Automatic Speech\nRecognition and Understanding Workshop (ASRU),\npages 16–23.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\n5495\nAskell, et al. 2020. Language models are few-shot\nlearners. In Proceedings of Advances in Neural In-\nformation Processing Systems 33 (NeurIPS), pages\n1877–1901.\nHui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao\nZheng. 2017. AISHELL-1: An open-source Man-\ndarin speech corpus and a speech recognition base-\nline. In Proceedings of the 20th Conference of the\nOriental Chapter of the International Coordinating\nCommittee on Speech Databases and Speech I/O Sys-\ntems and Assessment (O-COCOSDA), pages 1–5.\nWilliam Chan, Navdeep Jaitly, Quoc Le, and Oriol\nVinyals. 2016. Listen, attend and spell: A neural\nnetwork for large vocabulary conversational speech\nrecognition. In Proceedings of the 2016 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 4960–4964.\nWilliam Chan, Chitwan Saharia, Geoffrey Hinton, Mo-\nhammad Norouzi, and Navdeep Jaitly. 2020. Imputer:\nSequence modelling via imputation and dynamic pro-\ngramming. In Proceedings of the 37th International\nConference on Machine Learning (ICML) , pages\n1403–1413.\nNanxin Chen, Shinji Watanabe, Jesus Antonio Vil-\nlalba, Piotr Zelasko, and Najim Dehak. 2020. Non-\nautoregressive Transformer for speech recognition.\nIEEE Signal Processing Letter.\nNanxin Chen, Piotr ˙Zelasko, Laureano Moro-Velázquez,\nJesús Villalba, and Najim Dehak. 2021. Align-\nDenoise: Single-pass non-autoregressive speech\nrecognition. In Proceedings of Interspeech 2021 ,\npages 3770–3774.\nEthan A Chi, Julian Salazar, and Katrin Kirchhoff. 2021.\nAlign-Refine: Non-autoregressive speech recognition\nvia iterative realignment. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies (NAACL-HLT), pages 1920–\n1927.\nChung-Cheng Chiu, Tara N. Sainath, Yonghui Wu, Ro-\nhit Prabhavalkar, Patrick Nguyen, Zhifeng Chen, An-\njuli Kannan, Ron J. Weiss, Kanishka Rao, Ekaterina\nGonina, Navdeep Jaitly, Bo Li, Jan Chorowski, and\nMichiel Bacchiani. 2018. State-of-the-art speech\nrecognition with sequence-to-sequence models. In\nProceedings of the 2018 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 4774–4778.\nShih-Hsuan Chiu and Berlin Chen. 2021. Innovative\nBERT-based reranking language models for speech\nrecognition. In Proceedings of the 2021 IEEE Spoken\nLanguage Technology Workshop (SLT), pages 266–\n271.\nJan Chorowski and Navdeep Jaitly. 2017. Towards bet-\nter decoding and language model integration in se-\nquence to sequence models. In Proceedings of Inter-\nspeech 2017, pages 523–527.\nJan K Chorowski, Dzmitry Bahdanau, Dmitriy\nSerdyuk, Kyunghyun Cho, and Yoshua Bengio. 2015.\nAttention-based models for speech recognition. In\nProceedings of Advances in Neural Information Pro-\ncessing Systems 28 (NeurIPS), pages 577–585.\nYung-Sung Chuang, Chi-Liang Liu, Hung yi Lee, and\nLin shan Lee. 2020. SpeechBERT: An audio-and-\ntext jointly learned language model for end-to-end\nspoken question answering. In Proceedings of Inter-\nspeech 2020, pages 4168–4172.\nYu-An Chung, Chenguang Zhu, and Michael Zeng.\n2021. SPLAT: Speech-language joint pre-training for\nspoken language understanding. In Proceedings of\nthe 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies (NAACL-HLT), pages\n1897–1907.\nRonan Collobert, Jason Weston, Léon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.\nNatural language processing (almost) from scratch.\nJournal of Machine Learning Research, 12(76):2493–\n2537.\nKeqi Deng, Songjun Cao, Yike Zhang, and Long Ma.\n2021. Improving hybrid CTC/attention end-to-end\nspeech recognition with pretrained acoustic and lan-\nguage models. In Proceedings of the 2021 IEEE\nAutomatic Speech Recognition and Understanding\nWorkshop (ASRU), pages 76–82.\nKeqi Deng, Zehui Yang, Shinji Watanabe, Yosuke\nHiguchi, Gaofeng Cheng, and Pengyuan Zhang.\n2022. Improving non-autoregressive end-to-end\nspeech recognition with pre-trained acoustic and lan-\nguage models. In Proceedings of the 2022 IEEE\nInternational Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pages 8522–8526.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional Transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies (NAACL-HLT), pages 4171–4186.\nHicham El Boukkouri, Olivier Ferret, Thomas Lavergne,\nHiroshi Noji, Pierre Zweigenbaum, and Jun’ichi Tsu-\njii. 2020. CharacterBERT: Reconciling ELMo and\nBERT for word-level open-vocabulary representa-\ntions from characters. In Proceedings of the 28th\nInternational Conference on Computational Linguis-\ntics, pages 6903–6915.\nSantiago Fernández, Alex Graves, and Jürgen Schmid-\nhuber. 2007. Sequence labelling in structured do-\nmains with hierarchical recurrent neural networks.\nIn Proceedings of the 20th International Joint Con-\nference on Artificial Intelligence (IJCAI), pages 774–\n779.\n5496\nYuya Fujita, Shinji Watanabe, Motoi Omachi, and Xu-\nankai Chang. 2020. Insertion-based modeling for\nend-to-end automatic speech recognition. In Pro-\nceedings of Interspeech 2020, pages 3660–3664.\nHayato Futami, Hirofumi Inaguma, Masato Mimura,\nShinsuke Sakai, and Tatsuya Kawahara. 2021. ASR\nrescoring and confidence estimation with ELECTRA.\nIn Proceedings of the 2021 IEEE Automatic Speech\nRecognition and Understanding Workshop (ASRU),\npages 380–387.\nHayato Futami, Hirofumi Inaguma, Sei Ueno, Masato\nMimura, Shinsuke Sakai, and Tatsuya Kawahara.\n2020. Distilling the knowledge of BERT for\nsequence-to-sequence ASR. In Proceedings of In-\nterspeech 2020, pages 3635–3639.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-predict: Parallel de-\ncoding of conditional masked language models. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 6114–6123.\nAlex Graves. 2012. Sequence transduction with\nrecurrent neural networks. arXiv preprint\narXiv:1211.3711.\nAlex Graves, Santiago Fernández, Faustino Gomez, and\nJürgen Schmidhuber. 2006. Connectionist temporal\nclassification: Labelling unsegmented sequence data\nwith recurrent neural networks. In Proceedings of the\n23rd International Conference on Machine Learning\n(ICML), pages 369–376.\nAlex Graves and Navdeep Jaitly. 2014. Towards end-\nto-end speech recognition with recurrent neural net-\nworks. In Proceedings of the 31st International\nConference on International Conference on Machine\nLearning (ICML), pages 1764–1772.\nAlex Graves, Abdelrahman Mohamed, and Geoffrey\nHinton. 2013. Speech recognition with deep recur-\nrent neural networks. In Proceedings of the 2013\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages 6645–6649.\nJiatao Gu, James Bradbury, Caiming Xiong, Victor OK\nLi, and Richard Socher. 2018. Non-autoregressive\nneural machine translation. In Proceedings of the 6th\nInternational Conference on Learning Representa-\ntions (ICLR).\nJiatao Gu, Changhan Wang, and Junbo Zhao. 2019.\nLevenshtein Transformer. In Proceedings of Ad-\nvances in Neural Information Processing Systems\n32 (NeurIPS).\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki\nParmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang,\nZhengdong Zhang, Yonghui Wu, and Ruoming Pang.\n2020. Conformer: Convolution-augmented Trans-\nformer for speech recognition. In Proceedings of\nInterspeech 2020, pages 5036–5040.\nCaglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun\nCho, Loic Barrault, Huei-Chi Lin, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2015. On\nusing monolingual corpora in neural machine trans-\nlation. arXiv preprint arXiv:1503.03535.\nJunliang Guo, Zhirui Zhang, Linli Xu, Hao-Ran Wei,\nBoxing Chen, and Enhong Chen. 2020. Incorpo-\nrating BERT into parallel sequence decoding with\nadapters. In Proceedings of Advances in Neural In-\nformation Processing Systems 33 (NeurIPS), pages\n10843–10854.\nPengcheng Guo, Florian Boyer, Xuankai Chang,\nTomoki Hayashi, Yosuke Higuchi, Hirofumi In-\naguma, Naoyuki Kamo, Chenda Li, Daniel Garcia-\nRomero, Jiatong Shi, Jing Shi, Shinji Watanabe, Kun\nWei, Wangyou Zhang, and Yuekai Zhang. 2021. Re-\ncent developments on ESPnet toolkit boosted by Con-\nformer. In Proceedings of the 2021 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 5874–5878.\nAwni Hannun, Carl Case, Jared Casper, Bryan Catan-\nzaro, Greg Diamos, Erich Elsen, Ryan Prenger, San-\njeev Satheesh, Shubho Sengupta, Adam Coates, et al.\n2014. Deep Speech: Scaling up end-to-end speech\nrecognition. arXiv preprint arXiv:1412.5567.\nTomoki Hayashi, Shinji Watanabe, Tomoki Toda,\nKazuya Takeda, Shubham Toshniwal, and Karen\nLivescu. 2019. Pre-trained text embeddings for en-\nhanced text-to-speech synthesis. In Proceedings of\nInterspeech 2019, pages 4430–4434.\nYosuke Higuchi, Nanxin Chen, Yuya Fujita, Hirofumi\nInaguma, Tatsuya Komatsu, Jaesong Lee, Jumon\nNozaki, Tianzi Wang, and Shinji Watanabe. 2021a.\nA comparative study on non-autoregressive model-\nings for speech-to-text generation. In Proceedings\nof the 2021 IEEE Automatic Speech Recognition and\nUnderstanding Workshop (ASRU), pages 47–54.\nYosuke Higuchi, Hirofumi Inaguma, Shinji Watanabe,\nTetsuji Ogawa, and Tetsunori Kobayashi. 2021b. Im-\nproved mask-CTC for non-autoregressive end-to-end\nASR. In Proceedings of the 2021 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP), pages 8363–8367.\nYosuke Higuchi, Keita Karube, Tetsuji Ogawa, and Tet-\nsunori Kobayashi. 2022. Hierarchical conditional\nend-to-end ASR with CTC and multi-granular sub-\nword units. In Proceedings of the 2022 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 7797–7801.\nYosuke Higuchi, Shinji Watanabe, Nanxin Chen, Tetsuji\nOgawa, and Tetsunori Kobayashi. 2020. Mask CTC:\nNon-autoregressive end-to-end ASR with CTC and\nmask predict. In Proceedings of Interspeech 2020,\npages 3655–3659.\nWen-Chin Huang, Chia-Hua Wu, Shang-Bao Luo, Kuan-\nYu Chen, Hsin-Min Wang, and Tomoki Toda. 2021.\n5497\nSpeech recognition by simply fine-tuning BERT. In\nProceedings of the 2021 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 7343–7347.\nJae-Jin Jeon and Eesung Kim. 2021. Multitask learn-\ning and joint optimization for Transformer-RNN-\nTransducer speech recognition. In Proceedings of\nthe 2021 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pages\n6793–6797.\nAnjuli Kannan, Yonghui Wu, Patrick Nguyen, Tara N\nSainath, Zhijeng Chen, and Rohit Prabhavalkar. 2018.\nAn analysis of incorporating an external language\nmodel into a sequence-to-sequence model. In Pro-\nceedings of the 2018 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 1–5828.\nTom Kenter, Manish Sharma, and Rob Clark. 2020.\nImproving the prosody of RNN-based english text-to-\nspeech synthesis by incorporating a BERT model. In\nProceedings of Interspeech 2020, pages 4412–4416.\nTom Ko, Vijayaditya Peddinti, Daniel Povey, and San-\njeev Khudanpur. 2015. Audio augmentation for\nspeech recognition. In Proceedings of Interspeech\n2015, pages 3586–3589.\nYotaro Kubo, Shigeki Karita, and Michiel Bacchiani.\n2022. Knowledge transfer from large-scale pre-\ntrained language models to end-to-end speech rec-\nognizers. In Proceedings of the 2022 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 8512–8516.\nTaku Kudo. 2018. Subword regularization: Improv-\ning neural network translation models with multiple\nsubword candidates. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (ACL), pages 66–75.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In Proceed-\nings of the 5th International Conference on Learning\nRepresentations (ICLR).\nJaesong Lee, Jingu Kang, and Shinji Watanabe. 2021.\nLayer pruning on demand with intermediate CTC. In\nProceedings of Interspeech 2021, pages 3745–3749.\nJaesong Lee, Lukas Lee, and Shinji Watanabe. 2022.\nMemory-efficient training of RNN-Transducer with\nsampled softmax. In Proceedings of Interspeech\n2022.\nJaesong Lee and Shinji Watanabe. 2021. Intermediate\nloss regularization for CTC-based speech recognition.\nIn Proceedings of the 2021 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 6224–6228.\nJumon Nozaki and Tatsuya Komatsu. 2021. Relaxing\nthe conditional independence assumption of CTC-\nbased ASR by conditioning on intermediate predic-\ntions. In Proceedings of Interspeech 2021 , pages\n3735–3739.\nNicholas A Nystrom, Michael J Levine, Ralph Z\nRoskies, and J Ray Scott. 2015. Bridges: A uniquely\nflexible HPC resource for new communities and data\nanalytics. In Proceedings of XSEDE, pages 1–8.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and San-\njeev Khudanpur. 2015. Librispeech: An ASR corpus\nbased on public domain audio books. In Proceedings\nof the 2015 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pages\n5206–5210.\nDaniel S Park, William Chan, Yu Zhang, Chung-Cheng\nChiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le.\n2019. SpecAugment: A simple data augmentation\nmethod for automatic speech recognition. In Pro-\nceedings of Interspeech 2019, pages 2613–2617.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. PyTorch: An imperative style,\nhigh-performance deep learning library. In Proceed-\nings of Advances in Neural Information Processing\nSystems 32 (NeurIPS).\nDaniel Povey, Hossein Hadian, Pegah Ghahremani,\nKe Li, and Sanjeev Khudanpur. 2018. A time-\nrestricted self-attention layer for ASR. In Proceed-\nings of the 2018 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP),\npages 5874–5878.\nAnthony Rousseau, Paul Deléglise, and Yannick Estève.\n2014. Enhancing the TED-LIUM corpus with se-\nlected data for language modeling and more TED\ntalks. In Proceedings of the Ninth International\nConference on Language Resources and Evaluation\n(LREC), pages 3935–3939.\nTara N. Sainath, Ruoming Pang, David Rybach,\nYanzhang He, Rohit Prabhavalkar, Wei Li, Mirkó\nVisontai, Qiao Liang, Trevor Strohman, Yonghui Wu,\nIan McGraw, and Chung-Cheng Chiu. 2019. Two-\npass end-to-end speech recognition. In Proceedings\nof Interspeech 2019, pages 2773–2777.\nJulian Salazar, Davis Liang, Toan Q Nguyen, and Katrin\nKirchhoff. 2020. Masked language model scoring.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics (ACL), pages\n2699–2712.\nRamon Sanabria and Florian Metze. 2018. Hierarchical\nmultitask learning with CTC. In Proceedings of the\n2018 IEEE Spoken Language Technology Workshop\n(SLT), pages 485–490.\n5498\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled version\nof BERT: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL), pages 1715–1725.\nSeunghyun Seo, Donghyun Kwak, and Bowon Lee.\n2022. Integration of pre-trained networks with con-\ntinuous token interface for end-to-end spoken lan-\nguage understanding. In Proceedings of the 2022\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages 7152–7156.\nChanghao Shan, Chao Weng, Guangsen Wang, Dan Su,\nMin Luo, Dong Yu, and Lei Xie. 2019. Component\nfusion: Learning replaceable language model com-\nponent for end-to-end speech recognition system. In\nProceedings of the 2019 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP), pages 5361–5635.\nJoonbo Shin, Yoonhyung Lee, and Kyomin Jung. 2019.\nEffective sentence scoring method using BERT for\nspeech recognition. In Proceedings of Asian Con-\nference on Machine Learning (ACML), pages 1081–\n1093.\nAnuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and\nAdam Coates. 2018. Cold fusion: Training seq2seq\nmodels together with language models. In Proceed-\nings of Interspeech 2018, pages 387–391.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nIn Proceedings of Advances in Neural Information\nProcessing Systems 27 (NeurIPS), pages 3104–3112.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics (ACL), pages\n4593–4601.\nAndros Tjandra, Chunxi Liu, Frank Zhang, Xiaohui\nZhang, Yongqiang Wang, Gabriel Synnaeve, Satoshi\nNakamura, and Geoffrey Zweig. 2020. DEJA-VU:\nDouble feature presentation and iterated loss in deep\nTransformer networks. In Proceedings of the 2020\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP), pages 6899–6903.\nJ. Towns, T. Cockerill, M. Dahan, I. Foster, K. Gaither,\nA. Grimshaw, V . Hazlewood, S. Lathrop, D. Lifka,\nG. D. Peterson, R. Roskies, J. R. Scott, and\nN. Wilkins-Diehr. 2014. XSEDE: Accelerating scien-\ntific discovery. Computing in Science & Engineering,\n16(5):62–74.\nEmiru Tsunoo, Yosuke Kashiwagi, Toshiyuki Ku-\nmakura, and Shinji Watanabe. 2019. Transformer\nASR with contextual block processing. In Proceed-\nings of the 2019 IEEE Automatic Speech Recognition\nand Understanding Workshop (ASRU), pages 427–\n433.\nTakuma Udagawa, Masayuki Suzuki, Gakuto Kurata,\nNobuyasu Itoh, and George Saon. 2022. Effect and\nanalysis of large-scale language model rescoring on\ncompetitive ASR systems. In Proceedings of Inter-\nspeech 2022.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30 (NeurIPS), pages 5998–6008.\nShinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki\nHayashi, Jiro Nishitoba, Yuya Unno, Nelson En-\nrique Yalta Soplin, Jahn Heymann, Matthew Wiesner,\nNanxin Chen, Adithya Renduchintala, and Tsubasa\nOchiai. 2018. ESPnet: End-to-end speech processing\ntoolkit. In Proceedings of Interspeech 2018, pages\n2207–2211.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations (EMNLP), pages 38–45.\nCheng Yi, Shiyu Zhou, and Bo Xu. 2021. Efficiently\nfusing pretrained acoustic and linguistic encoders\nfor low-resource speech recognition. IEEE Signal\nProcessing Letters, 28:788–792.\nTom Young, Devamanyu Hazarika, Soujanya Poria, and\nErik Cambria. 2018. Recent trends in deep learn-\ning based natural language processing [review arti-\ncle]. IEEE Computational Intelligence Magazine ,\n13(3):55–75.\nFu-Hao Yu, Kuan-Yu Chen, and Ke-Han Lu. 2022.\nNon-autoregressive ASR modeling using pre-trained\nlanguage models for Chinese speech recognition.\nIEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing, 30:1474–1482.\nGuolin Zheng, Yubei Xiao, Ke Gong, Pan Zhou, Xiao-\ndan Liang, and Liang Lin. 2021. Wav-BERT: Coop-\nerative acoustic and linguistic representation learning\nfor low-resource speech recognition. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2021, pages 2765–2777.\n5499\nAlgorithm 1 BERT-CTC Inference\nInput: The number of iterations K, audio encoder output Hae\n1: ˆA′= argmaxA′p(A′|O) ▷Obtain the most probable alignment from the audio encoder\n2: ˆW′= Bctc( ˆA′)\n3: ˆN ∼| ˆW′| ▷Obtain the target length from the intermediate prediction\n4: ¯W = (wn =[MASK]|n= 1,··· , ˆN) ▷Initialize a masked sequence\n5: for k= 1,...,K do\n6: # Token prediction\n7: Hbert = BERT( ¯W) ▷Forward BERT\n8: p(A|·) = Softmax(SelfAttn(Hae,Hbert)) ▷Forward self-attention module\n9: ˆA= argmaxA p(A|·) ▷Obtain the most probable alignment\n10: ˆW = Bctc( ˆA)\n11: # Token-level probability calculation\n12: ˆP = (ˆpn = 0|n= 1,··· ,|ˆW|) ▷Initialize token-level probabilities\n13: n= 1 ▷Initialize an index for token position\n14: a0 = ϵ\n15: for t= 1,...,T do\n16: if ˆat = ϵthen\n17: if ˆat−1 ̸= ϵthen\n18: n= n+ 1\n19: end if\n20: else\n21: ˆpn = max(p(at = ˆwn|·),ˆpn) ▷Keep the maximum probability for each token\n22: end if\n23: end for\n24: # Token masking\n25: M = ⌊|ˆW|· K−k\nK ⌋ ▷Calculate the number of masked tokens\n26: ¯W = MaskLowestProb( ˆW, ˆP,M ) ▷Mask tokens with the M lowest probability scores\n27: end for\n28: return ˆW\nA Inference Algorithm\nAlgorithm 1 describes the overall process of BERT-\nCTC inference. For estimating the target length in\nline 3, at the implementation level, we first decode\nˆW′into a sentence, which is then tokenized using\nthe BERT vocabulary, and the length of the result-\ning sequence is used as the target length. In lines\n12–25, before the token masking step, we calculate\na probability score ˆpn for each token ˆwn in the es-\ntimated output sequence ˆW. This score calculation\nsimply takes the maximum value in frame-level\ntoken probabilities that correspond to a predicted\ntoken ˆwn after the collapsing operation. In line 28,\ngiven the probability scores, MaskLowestProb(·)\nmasks tokens in ˆW with the M lowest scores.\nB Intermediate CTC\nIntermediate CTC (Tjandra et al., 2020; Lee and\nWatanabe, 2021) applies additional CTC losses to\nintermediate layers of the audio encoder network.\nLet H(e) = ( h(e)\nt ∈ Rdae\n|t = 1 ,··· ,T) be an\nintermediate output of the e-th layer of the audio\nencoder, which is computed as in Eq. (6) as\nh(e)\nt ∼AudioEnc(e)(O), (28)\nwhere AudioEnc(e)(·) indicates the e-th layer out-\nput of the audio encoder. Similar to Eq. (5), token\nemission probabilities at each time frame is com-\nputed based on Eq. (28) as\np(e)(at|O) = Softmax(Linear(h(e)\nt )). (29)\n5500\nFinally, an intermediate CTC loss L(e)\nctc is defined\nsimilarly to Eq. (7) as\nL(e)\nctc(O,W ) = −log\n∑\nA∈B−1\nctc (W)\nT∏\nt=1\np(e)(at|O).\n(30)\nC Model Details\nC.1 CTC (baseline)\nWe applied the intermediate CTC loss to the 6-\nth layer of the audio encoder, which is calculated\nusing the smaller-vocabulary sequence W′in a hi-\nerarchical multi-tasking manner. Using the inter-\nmediate loss, the CTC loss Lctc is extended as\n(1 −λic)Lctc(O,W ) + λicL(e=6)\nctc (O,W ′), (31)\nwhere λic is a tunable weight for the intermediate\nloss, and we equally weighted each loss (i.e., λic =\n0.5) as in (Lee et al., 2021; Higuchi et al., 2022).\nC.2 RNN-T (baseline)\nWe applied auxiliary CTC losses to the final and\nintermediate layer of the audio encoder. As in\nEq. (31), the intermediate loss was applied to the\n6-th layer. With the additional CTC losses, the\nRNN-T loss Lrnnt is extended as\n(1 −λctc)Lrnnt(O,W )\n+λctc{(1−λic)Lic(O,W ′)+λicL(e=6)\nctc (O,W ′)},\n(32)\nwhere λctc is a tunable weight for CTC losses, and\nwe set λctc = 0.3 as in (Boyer et al., 2021). Note\nthat all the CTC losses were calculated using the\nsmaller-vocabulary sequence W′in a hierarchical\nmulti-tasking manner.\nC.3 BERT-CTC (ours)\nWe applied auxiliary CTC losses to the final and\nintermediate layer of the audio encoder. As in\nEq. (31), the intermediate loss was applied to the\n6-th layer. With the additional CTC losses, the\nBERT-CTC lossLbc is extended as\n(1 −λctc)Lbc(O,W )\n+λctc{(1−λic)Lic(O,W ′)+λicL(e=6)\nctc (O,W ′)},\n(33)\nwhere λctc is a tunable weight for CTC losses, and\nwe set λctc = 0.3. Note that all the CTC losses\nwere calculated using the smaller-vocabulary se-\nquence W′in a hierarchical multi-tasking manner\n(as explained in §3.1).\nD Experimental Details\nD.1 Dataset\nTables 6 and 7 list descriptions of ASR and SLU\ndatasets, respectively. Data preparation was done\nusing the ESPnet2 recipe provided for each dataset:\nLibriSpeech-100h5, TED-LIUM26, AISHELL-17,\nSLURP8.\nD.2 Model Configuration\nFor the audio encoder network, we used the Con-\nformer (Gulati et al., 2020)-based encoder archi-\ntecture implemented in ESPnet (Guo et al., 2021).\nThe audio encoder consisted of 2 or 3 convolu-\ntional neural network (CNN) layers followed by a\nstack of 12 encoder blocks. The dimensions of the\nself-attention layer dae and feed-forward network\ndff were set to 256 and 1024, respectively, and the\nnumber of heads dhead was set to 4. The kernel size\nof depthwise separable convolution was set to 31.\nFor RNN-T, the prediction network was a single\nLSTM layer with 512 units, and the joint network\nwas a single linear layer with 640 units. For BERT-\nCTC, we built the Transformer (Vaswani et al.,\n2017)-based architecture for the self-attention mod-\nule, which consisted of a stack of 6 encoder blocks\nwith dmodel = 256 , dff = 2048 , and dhead = 4 .\nBefore feeding into the self-attention module, the\nhidden vectors, Hae and Hbert, were emebbeded\nusing 2 CNN layers and a single linear layer, re-\nspectively, which mapped each vector to the di-\nmension size of dmodel. For the BERT module in\nBERT-CTC, we downloaded pre-trained models\nfrom the HuggingFace Transformers library (Wolf\net al., 2020) 9. We used a BERT BASE model pro-\nvided for each language: English 10, Mandarin11.\nNote that the dimension of the BERT output dbert\nwas 768. The number of total/trainable parame-\nters in the CTC, RNN-T, and BERT-CTC models\nwas about 30M/30M, 60M/60M, and 150M/40M,\nrespectively.\n5https://github.com/espnet/espnet/tree/master/\negs2/librispeech_100/asr1\n6https://github.com/espnet/espnet/tree/master/\negs2/tedlium2/asr1\n7https://github.com/espnet/espnet/tree/master/\negs2/aishell/asr1\n8https://github.com/espnet/espnet/tree/master/\negs2/slurp/asr1\n9https://github.com/huggingface/transformers\n10https://huggingface.co/bert-base-uncased\n11https://huggingface.co/bert-base-chinese\n5501\nDataset Language Speech Style # Train Hours # Valid. Hours # Test Hours\nLibriSpeech-100h (Panayotov et al., 2015) English Read 100h 5.4h / 5.3h 5.4h / 5.1h\nTED-LIUM2 (Rousseau et al., 2014) English Spontaneous 210h 1.6h 2.6h\nAISHELL-1 (Bu et al., 2017) Mandarin Read 170h 10h 5h\nTable 6: ASR dataset descriptions. The validation and test sets of LibriSpeech are split into “clean” / “other” sets\nbased on the quality of the recordec utterances.\nDataset Language # Intents # Train Hours # Valid. Hours # Test Hours\nSLURP (Bastianelli et al., 2020) English 69 40h + 43h 6.9h 10.3h\nTable 7: Dataset description for SLURP intent classification. We bootstrap the train set with the synthetic data.\nHyperparameter Value\nDropout rate 0.1\nLR schedule Noam (Vaswani et al., 2017)\nMax learing rate best of [1e-3, 2e-3]\nWarmup steps 15k\nEpochs best of [50, 70, 100]\nAdam betas (0.9, 0.98)\nWeight decay 1e-6\nTable 8: Training configuration for CTC model.\nHyperparameter Value\nDropout rate 0.1\nLR schedule Noam (Vaswani et al., 2017)\nMax learing rate 2e-3\nWarmup steps 15k\nEpochs best of [50, 70]\nAdam betas (0.9, 0.98)\nWeight decay 1e-6\nTable 9: Training configuration for RNN-T model.\nD.3 Tokenization\nWe used the same subword vocabulary as BERT\nfor tokenizing target texts, where the vocabulary\nsize |V|was 30522 for English and 21128 for Man-\ndarin. For the smaller-sized vocabulary V′used in\nhierarchical CTC, we used SentencePiece (Kudo,\n2018)12 to construct subword vocabularies from\ntranscription data in each training set. Following\nthe ESPnet recipes, the vocabulary size was set\nto 300 for LibriSpeech-100h, and 500 for TED-\nLIUM2 and SLURP. For AISHELL-1, we used\ncharacter-level tokenization with 4231 Chinese\ncharacters.\nHyperparameter Value\nDropout rate 0.1\nLR schedule Noam (Vaswani et al., 2017)\nMax learing rate best of [1e-3, 2e-3]\nWarmup steps 15k\nEpochs best of [50, 70, 100]\nAdam betas (0.9, 0.98)\nWeight decay 1e-6\nTable 10: Training configuration for BERT-CTC model.\nD.4 Training\nAll the models were implemented and trained\nusing ESPnet (Watanabe et al., 2018) 13 and Py-\nTorch (Paszke et al., 2019) 14. In Tables 8, 9,\nand 10, we summarize training configurations for\nthe CTC, RNN-T, and BERT-CTC models, respec-\ntively. We augmented speech data using speed\nperturbation (Ko et al., 2015) with a factor of3 and\nSpecAugment (Park et al., 2019). For the hyper-\nparameters in SpecAugment, we set the number\nof frequency and time masks to 2 and 5, and the\nsize of frequency and time masks to 27 and 0.05T.\nNote that the maximum size of the time mask de-\npends on the utterance length T. After training,\nmodel parameters were averaged over 10 check-\npoints with the best validation performance. For\nCTC, we trained models using a single RTX 2080\nTi GPU for 1 to 3 days, depending on the tasks and\nnumber of epochs. For RNN-T, we trained models\nusing 4 V100 GPUs for 5 to 7 days, depending on\nthe tasks and number of epochs. For BERT-CTC,\nwe trained models using a single RTX 2080 Ti GPU\nfor 3 to 5 days, depending on the tasks and number\nof epochs.\n12https://github.com/google/sentencepiece\n13https://github.com/espnet/espnet\n14https://github.com/pytorch/pytorch\n5502\nModel #params[M]\nTotal (Trainable)\nPre-trained\nDev CER(↓) Test CER(↓)AM LM\nrePLM-NAR-ASR (Yu et al., 2022) 120 (120) – BERT 4.2 4.8\nCTC/Attention (Deng et al., 2021) 161 (152) wav2vec2.0 – 4.7 5.0\nCTC/Attention (Deng et al., 2021) 218 (209) wav2vec2.0 DistilGPT2 3.9 4.2\nNAR-CTC/Attention (Deng et al., 2022) 204 (195) wav2vec2.0 BERT 4.0 4.3\nWav-BERT (Zheng et al., 2021) 380 (380) wav2vec2.0 BERT 3.6 3.8\nBERT-CTC (ours) 143 (40) – BERT 3.9 3.9\nTable 11: Comparison to prior works on AISHELL-1. The number of trainable parameters in BERT-CTC is fewer\nthan in the others because BERT-CTC uses BERT without fine-tuning.\nModel #iters\nLibriSpeech-100h TED-LIUM2\nDev WER (↓) Test WER (↓) Dev WER (↓) Test WER (↓)clean other clean other\nMask-CTC (Higuchi et al., 2020) 10 7.2 20.3 7.5 20.6 8.9 8.5\nImproved Mask-CTC (Higuchi et al., 2021b) 5 7.0 19.8 7.3 20.2 8.8 8.3\nAlign-Denoise (Chen et al., 2021) 1 8.0 22.3 8.4 22.5 9.0 8.7\nIntermediate CTC (Lee and Watanabe, 2021) 1 6.9 19.7 7.1 20.2 8.5 8.3\nSelf-conditioned CTC (Nozaki and Komatsu, 2021) 16.6 19.4 6.9 19.7 8.7 8.0\nKERMIT (Fujita et al., 2020) ≃log2(N) 7.1 19.7 7.4 20.2 9.1 8.2\nBERT-CTC (ours) 20 7.0 16.3 7.2 16.6 8.1 7.6\nTable 12: Comparison of BERT-CTC and non-autoregressive E2E-ASR models on LibriSpeech-100h and TED-\nLIUM2. The prior results are obtained from the comparative study conducted in (Higuchi et al., 2021a).\nD.5 Inference\nRTF was measured using a single V100 GPU (with\na batchsize of 1) or a single Intel(R) Xeon(R) Gold\n6148 CPU@2.4 GHz.\nE Comparison to Prior Works\nAISHELL-1 Table 11 lists results on AISHELL-\n1, comparing our BERT-CTC with recent ap-\nproaches using a pre-trained acoustic model (AM)\nor/and LM. BERT-CTC achieved comparable per-\nformance to the state-of-the-art approach, Wav-\nBERT (Zheng et al., 2021), without using a pre-\ntrained AM. Moreover, the number of trainable\nparameters in BERT-CTC was much fewer than in\nthe other models because BERT-CTC used BERT\nas contextual embedding (without fine-tuning). We\nattribute this advantage of BERT-CTC to our well-\ndefined formulation for conditioning CTC train-\ning/inference with BERT knowledge.\nNon-autoregressive End-to-End ASR Table 12\ncompares our BERT-CTC with the previous non-\nautoregressive E2E-ASR models on LibriSpeech-\n100h and TED-LIUM2. It should be noted that\nwe refer to (Higuchi et al., 2021a) for the prior re-\nsults, and the comparison is not necessarily in an\nequivalent setting, e.g., we conducted experiments\nusing ESPnet2 while the previous work used ESP-\nnet1. Overall, BERT-CTC achieved better results\nthan the other non-autoregressive models, thanks\nto the usage of BERT. In particular, we observed\nclear differences in the LibriSpeech “other” sets\nand TED-LIUM2. However, the performance on\nthe LibriSpeech “clean” set was on par with the\nother approaches, which we attribute to the vocab-\nulary mismatch problem we have discussed in the\nlimitation section.\n5503",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8145883083343506
    },
    {
      "name": "Language model",
      "score": 0.662287712097168
    },
    {
      "name": "Security token",
      "score": 0.6200041174888611
    },
    {
      "name": "Inference",
      "score": 0.6020956039428711
    },
    {
      "name": "Speech recognition",
      "score": 0.5709037780761719
    },
    {
      "name": "Decoding methods",
      "score": 0.5613122582435608
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5172296166419983
    },
    {
      "name": "Embedding",
      "score": 0.49325960874557495
    },
    {
      "name": "Dependency (UML)",
      "score": 0.4880720376968384
    },
    {
      "name": "Connectionism",
      "score": 0.4869023859500885
    },
    {
      "name": "End-to-end principle",
      "score": 0.4697214663028717
    },
    {
      "name": "Treebank",
      "score": 0.46110445261001587
    },
    {
      "name": "Natural language processing",
      "score": 0.4368622899055481
    },
    {
      "name": "Artificial neural network",
      "score": 0.177921324968338
    },
    {
      "name": "Algorithm",
      "score": 0.10826128721237183
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I150744194",
      "name": "Waseda University",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ]
}