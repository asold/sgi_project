{
  "title": "AraBERT: Transformer-based Model for Arabic Language Understanding",
  "url": "https://openalex.org/W3088592174",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2971064649",
      "name": "Wissam Antoun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3006992629",
      "name": "Fady Baly",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1973326217",
      "name": "Hazem Hajj",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970814728",
    "https://openalex.org/W2805089673",
    "https://openalex.org/W2945847947",
    "https://openalex.org/W2983040767",
    "https://openalex.org/W2996580882",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2799060650",
    "https://openalex.org/W2990188683",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2971016465",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970960342",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2964573145",
    "https://openalex.org/W2252067416",
    "https://openalex.org/W2471147443",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3000779003",
    "https://openalex.org/W2158873310",
    "https://openalex.org/W2767784948",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W2735966564",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W340195604",
    "https://openalex.org/W2621199241",
    "https://openalex.org/W2251137535",
    "https://openalex.org/W2250594687",
    "https://openalex.org/W2735552604",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2144578941",
    "https://openalex.org/W2908003533",
    "https://openalex.org/W2948433920",
    "https://openalex.org/W2770803436",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W2963979492"
  ],
  "abstract": "The Arabic language is a morphologically rich language with relatively few resources and a less explored syntax compared to English. Given these limitations, Arabic Natural Language Processing (NLP) tasks like Sentiment Analysis (SA), Named Entity Recognition (NER), and Question Answering (QA), have proven to be very challenging to tackle. Recently, with the surge of transformers based models, language-specific BERT based models have proven to be very efficient at language understanding, provided they are pre-trained on a very large corpus. Such models were able to set new standards and achieve state-of-the-art results for most NLP tasks. In this paper, we pre-trained BERT specifically for the Arabic language in the pursuit of achieving the same success that BERT did for the English language. The performance of AraBERT is compared to multilingual BERT from Google and other state-of-the-art approaches. The results showed that the newly developed AraBERT achieved state-of-the-art performance on most tested Arabic NLP tasks. The pretrained araBERT models are publicly available on https://github.com/aub-mind/araBERT hoping to encourage research and applications for Arabic NLP.",
  "full_text": "arXiv:2003.00104v4  [cs.CL]  7 Mar 2021\nAraBERT : T ransformer-based Model for\nArabic Language Understanding\nWissam Antoun*, Fady Baly*, Hazem Hajj\nAmerican University of Beirut\n{wfa07, fgb06, hh63 }@aub.edu.lb\nAbstract\nThe Arabic language is a morphologically rich language with relatively few resources and a less explored syntax compare d to English.\nGiven these limitations, Arabic Natural Language Processi ng (NLP) tasks like Sentiment Analysis (SA), Named Entity Re cognition\n(NER), and Question Answering (QA), have proven to be very ch allenging to tackle. Recently , with the surge of transforme rs based\nmodels, language-speciﬁc BERT based models have proven to b e very efﬁcient at language understanding, provided they ar e pre-trained\non a very large corpus. Such models were able to set new standa rds and achieve state-of-the-art results for most NLP tasks . In this paper,\nwe pre-trained BERT speciﬁcally for the Arabic language in t he pursuit of achieving the same success that BERT did for the English\nlanguage. The performance of AraBERT is compared to multili ngual BERT from Google and other state-of-the-art approach es. The re-\nsults showed that the newly developed AraBERT achieved stat e-of-the-art performance on most tested Arabic NLP tasks. T he pretrained\naraBERT models are publicly available on github.com/aub-m ind/araBERT hoping to encourage research and applications for Arabic NLP .\nKeywords: Arabic, transformers, BERT , AraBERT , Language Models\n1. Introduction\nPretrained contextualized text representation models hav e\nenabled massive advances in Natural Language Under-\nstanding (NLU) tasks, and achieved state-of-the-art perfo r-\nmances in multiple NLP tasks (Howard and Ruder, 2018;\nDevlin et al., 2018). Early pretrained text representa-\ntion models aimed at representing words by capturing\ntheir distributed syntactic and semantic properties us-\ning techniques like W ord2vec (Mikolov et al., 2013) and\nGloV e (Pennington et al., 2014). However, these models\ndid not incorporate the context in which a word appears\ninto its embedding. This issue was addressed by gen-\nerating contextualized representations using models like\nELMO (Peters et al., 2018)).\nRecently, there has been a focus on applying transfer learn-\ning by ﬁne-tuning large pretrained language models for\ndownstream NLP/NLU tasks with a relatively small num-\nber of examples, resulting in notable performance improve-\nment for these tasks. This approach takes advantage of\nthe language models that had been pre-trained in an un-\nsupervised manner (or sometimes called self-supervised).\nHowever, this advantage comes with drawbacks, partic-\nularly the huge corpora needed for pre-training, in addi-\ntion to the high computational cost of days needed for\ntraining (latest models required 500+ TPUs or GPUs run-\nning for weeks (Conneau et al., 2019; Raffel et al., 2019;\nAdiwardana et al., 2020)). These drawbacks restricted the\navailability of such models to English mainly and a handful\nof other languages. T o remedy this gap, multilingual mod-\nels have been trained to learn representations for +100 lan-\nguages simultaneously, but still fall behind single-langu age\nmodels due to little data representation and small language -\nspeciﬁc vocabulary. While languages with similar struc-\nture and vocabulary can beneﬁt from the shared represen-\ntations (Conneau et al., 2019), this is not the case for other\nlanguages, like Arabic, which differ in morphological and\n*Equal Contribution\nsyntactic structure and share very little with other abunda nt\nLatin-based languages.\nIn this paper, we describe the process of pretraining the\nBER T transformer model (Devlin et al., 2018) for the Ara-\nbic language, and which we name A RA BERT. W e eval-\nuate A RA BERT on three Arabic NLU downstream tasks\nthat are different in nature: ( i) Sentiment Analysis (SA),\n(ii) Named Entity Recognition (NER), and ( iii) Ques-\ntion Answering (QA). The experiments results show that\nARA BERT achieves state-of-the-art performances on most\ndatasets, compared to several baselines including previou s\nmultilingual and single-language approaches. The dataset s\nthat we considered for the downstream tasks contained\nboth Modern Standard Arabic (MSA) and Dialectal Arabic\n(DA).\nOur contributions can be summarized as follows:\n• A methodology to pretrain the BER T model on a\nlarge-scale Arabic corpus.\n• Application of A RA BERT to three NLU downstream\ntasks: Sentiment Analysis, Named Entity Recognition\nand Question Answering.\n• Publicly releasing A RA BE RT on popular NLP li-\nbraries.\nThe rest of the paper is structured as follows. Section 2.\nprovides a concise literature review of previous work on\nlanguage representation for English and Arabic. Sec-\ntion 3. describes the methodology that was used to develop\nARA BERT. Section 4. describes the downstream tasks and\nbenchmark datasets that are used for evaluation. Section 5.\npresents the experimental setup and discusses the results.\nFinally, section 6. concludes and points to possible direc-\ntions for future work.\n2. Related W orks\n2.1. Evolution of W ord Embeddings\nThe ﬁrst meaningful representations for words started with\nthe word2vec model developed by (Mikolov et al., 2013).\nSince then, research started moving towards variations of\nword2vec like of GloV e (Pennington et al., 2014) and fast-\nT ext (Mikolov et al., 2017). While major advances were\nachieved with these early models, they still lacked con-\ntextualized information, which was tackled by ELMO\n(Peters et al., 2018). The performance over different tasks\nimproved noticeably, leading to larger structures that had\nsuperior word and sentence representations. Ever since,\nmore language understanding models have been devel-\noped such as ULMFit (Howard and Ruder, 2018), BER T\n(Devlin et al., 2018), RoBER T a (Liu et al., 2019), XLNet\n(Y ang et al., 2019), ALBER T (Lan et al., 2019), and T5\n(Raffel et al., 2019), which offered improved performance\nby exploring different pretraining methods, modiﬁed model\narchitectures and larger training corpora.\n2.2. Non-contextual Representations for Arabic\nFollowing the success of the English word2vec\n(Mikolov et al., 2013), the same feat was sought by\nNLP researchers to create language speciﬁc em-\nbeddings. Arabic word2vec was ﬁrst attempted\nby (Soliman et al., 2017), and then followed by a\nFasttext model (Bojanowski et al., 2017) trained on\nWikipedia data and showing better performance than\nword2vec. T o tackle dialectal variations in Ara-\nbic (Erdmann et al., 2018) presented techniques for\ntraining multidialectal word embeddings on relatively sma ll\nand noisy corpora, while (Abu Farha and Magdy, 2019;\nAbdul-Mageed et al., 2018) provided Arabic word embed-\ndings trained on ∼ 250M tweets.\n2.3. Contextualized Representations for Arabic\nFor non-English languages, Google released a multi-\nlingual BER T (Devlin et al., 2018) supporting 100+ lan-\nguages with solid performance for most languages. How-\never, pre-training monolingual BER T for non-English\nlanguages proved to provide better performance than\nthe multilingual BER T such as Italian BER T Al-\nberto (Polignano et al., 2019) and other publicly available\nBER Ts (Martin et al., 2019; de Vries et al., 2019). Ara-\nbic speciﬁc contextualized representations models, such a s\nhULMonA (ElJundi et al., 2019), used the ULMﬁt struc-\nture, which had a lower performance that BER T on English\nNLP T asks.\n3. A RA BERT: Methodology\nIn this paper, we develop an Arabic language representa-\ntion model to improve the state-of-the-art in several Ara-\nbic NLU tasks. W e create A RA BERT based on the\nBER T model, a stacked Bidirectional Transformer En-\ncoder (Devlin et al., 2018). This model is widely consid-\nered as the basis for most state-of-the-art results in diffe r-\nent NLP tasks in several languages. W e use the BER T -\nbase conﬁguration that has 12 encoder blocks, 768 hidden\ndimensions, 12 attention heads, 512 maximum sequence\nlength, and a total of ∼ 110M parameters 1. W e also in-\ntroduced additional preprocessing prior to the model’s pre -\ntraining, in order to better ﬁt the Arabic language. Below ,\nwe describe the pre-training setup, the pre-training datas et\nfor A RA BERT, the proposed Arabic-speciﬁc preprocess-\ning, and the ﬁne-tuning process.\n3.1. Pre-training Setup\nFollowing the original BER T pre-training objective, we\nemploy the Masked Language Modeling (MLM) task by\nadding whole-word masking where; 15% of the N input\ntokens are selected for replacement. Those tokens are re-\nplaced 80% of the times with the [MASK] token, 10% with\na random token, and 10% with the original token. Whole-\nword masking improves the pre-training task by forcing the\nmodel to predict the whole word instead of getting hints\nfrom parts of the word. W e also employ the Next Sentence\nPrediction (NSP) task that helps the model understand the\nrelationship between two sentences, which can be useful\nfor many language understanding tasks such as Question\nAnswering.\n3.2. Pre-training Dataset\nThe original BER T was trained on 3.3B words ex-\ntracted from English Wikipedia and the Book Cor-\npus (Zhu et al., 2015). Since the Arabic Wikipedia Dumps\nare small compared to the English ones, we manually\nscraped Arabic news websites for articles. In addition, we\nused two publicly available large Arabic corpora: (1) the\n1.5 billion words Arabic Corpus (El-Khair, 2016), which is\na contemporary corpus that includes more than 5 million\narticles extracted from ten major news sources covering 8\ncountries, and (2) OSIAN: the Open Source International\nArabic News Corpus (Zeroual et al., 2019) that consists of\n3.5 million articles ( ∼ 1B tokens) from 31 news sources in\n24 Arab countries.\nThe ﬁnal size of the pre-training dataset, after removing\nduplicate sentences, is 70 million sentences, correspondi ng\nto ∼ 24GB of text. This dataset covers news from differ-\nent media in different Arab regions, and therefore can be\nrepresentative of a wide range of topics discussed in the\nArab world. It is worth mentioning that we preserved words\nthat include Latin characters, since it is common to mention\nnamed entities, scientiﬁc or technical terms in their origi nal\nlanguage, to avoid information loss.\n3.3. Sub-W ord Units Segmentation\nArabic language is known for its lexical sparsity which\nis due to the complex concatenative system of Ara-\nbic (Al-Sallab et al., 2017). W ords can have different forms\nand share the same meaning. For instance, while the deﬁ-\nnite article “ /charc8/char40 - Al”, which is equivalent to “the” in En-\nglish, is always preﬁxed to other words, it is not an intrinsi c\npart of that word. Hence, when using a BER T -compatible\ntokenization, tokens will appear twice, once with “ Al-” and\n1 Further details about the transformer architecture can be\nfound in (V aswani et al., 2017)\nonce without it. For instance, both “ /char48 /char2e /char41 /char10\n/char4a\n/charbb - kitAb” and “\n/char48 /char2e /char41 /char10\n/char4a\n/charba /charcb/char40 -AlkitAb” need to be included in the vocabulary,\nleading to a signiﬁcant amount of unnecessary redundancy.\nT o avoid this issue, we ﬁrst segment the words using\nFarasa (Abdelali et al., 2016) into stems, preﬁxes and suf-\nﬁxes. For instance, “ /char10/chare9\n/char09/charaa /char0f\n/charca\n/charcb/char40 - Alloga” becomes /char10\n/chare8/char2b /char09/chara9\n/charcb /char2b /charc8/char40 -\nAl+ log +a ”. Then, we trained a SentencePiece (an un-\nsupervised text tokenizer and detokenizer (Kudo, 2018)),\nin unigram mode, on the segmented pre-training dataset to\nproduce a subword vocabulary of ∼ 60K tokens. T o evalu-\nate the impact of the proposed tokenization, we also trained\nSentencePiece on non-segmented text to create a second\nversion of A RA BERT (AraBER Tv0.1) that does not re-\nquire any segmentation. The ﬁnal size of vocabulary was\n64k tokens, which included nearly 4K unused tokens to al-\nlow further pre-training, if needed.\n3.4. Fine-tuning\nSequence Classiﬁcation T o ﬁne-tune AraBER T for se-\nquence classiﬁcation, we take the ﬁnal hidden state of the\nﬁrst token, which corresponds to the word embedding of\nthe special “[CLS]” token prepended to the start of each\nsentence. W e then add a simple feed-forward layer with\nstandard Softmax to get the probability distribution over\nthe predicted output classes. During ﬁne-tuning, the class i-\nﬁer and the pre-trained model weights are trained jointly to\nmaximize the log-probability of the correct class.\nNamed Entity Recognition For the NER task, each\ntoken in the sentence is labeled with the IOB2 for-\nmat (Ratnaparkhi, 1998), where the “B” tag corresponds\nto the ﬁrst word of the entity, the “I” tag corresponds to\nthe rest of the words of the same entity, and the “O” tag\nindicates that the tagged word is not a desired named en-\ntity. Hence, we treat the system as a multi-class classiﬁca-\ntion process, which allows us to use some text classiﬁca-\ntion methods to label the tokens. Furthermore, after using\nthe AraBER T tokenizer, we only input the ﬁrst sub-token\nof each word to the model.\nQuestion Answering In the QA, given a question and a\npassage containing the answer, the model needs to select\na span of text that contains the answers. This is done by\npredicting a “start” token and an “end” token on condition\nthat the “end” token should appear after the “start” token.\nDuring training, the ﬁnal embedding of every token in the\npassage is fed into two classiﬁers, each with a single set of\nweights, which are applied to every token. The dot product\nof the output embeddings and the classiﬁer is then fed into\na softmax layer to produce a probability distribution over\nall the tokens. The token with the highest probability of\nbeing a “start” toke is then selected, and the same process\nis repeated for the “end” token.\n4. Evaluation\nW e evaluated A RA BERT on three Arabic language under-\nstanding downstream tasks: Sentiment Analysis, Named\nEntity Recognition, and Question Answering. As a base-\nline, we compared A RA BERT to the multilingual version\nof BER T , and to other state-of-art results on each task.\n4.1. Sentiment Analysis\nW e evaluated A RA BERT on the following Arabic senti-\nment datasets that cover different genres, domains and di-\nalects.\n• HARD: The Hotel Arabic Reviews\nDataset (Elnagar et al., 2018) contains 93,700\nhotel reviews written in both Modern Standard Arabic\n(MSA) and in dialectal Arabic. Reviews are split\ninto positive and negative reviews, where a negative\nreview has a rating of 1 or 2, a positive review has a\nrating of 4 or 5, and neutral reviews with rating of 3\nwere ignored.\n• ASTD: The Arabic Sentiment T witter\nDataset (Nabil et al., 2015) contains 10,000 tweets\nwritten in both MSA and Egyptian dialect. W e tested\non the balanced version of the dataset, referred to as\nASTD-B.\n• ArSenTD-Lev: The Arabic Sentiment T witter\nDataset for LEV antine (Baly et al., 2018) contains\n4,000 tweets written in Levantine dialect with annota-\ntions for sentiment, topic and sentiment target. This is\na challenging dataset as the collected tweets are from\nmultiple domains and discuss different topics.\n• LABR: The Large-scale Arabic Book Reviews\ndataset (Aly and Atiya, 2013) contains 63,000 book\nreviews written in Arabic. The reviews are rated be-\ntween 1 and 5. W e benchmarked our model on the\nunbalanced two-class dataset, where reviews with rat-\nings of 1 or 2 are considered negative, while those with\nratings of 4 or 5 are considered positive.\n• AJGT : The Arabic Jordanian General T weets\ndataset (Alomari et al., 2017) contains 1,800 tweets\nwritten in Jordanian dialect. The tweets were\nmanually annotated as either positive or negative.\nBaselines: Sentiment Analysis is a popular Arabic\nNLP task. Previous approaches relied on sentiment\nlexicons such as ArSenL (Badaro et al., 2014), which\nis a large-scale lexicon of MSA words that is devel-\noped using the Arabic W ordNet in combination with\nthe English SentiW ordNet. Recurrent and recursive\nneural networks were explored with different choices\nof Arabic-speciﬁc processing (Al Sallab et al., 2015;\nAl-Sallab et al., 2017; Baly et al., 2017). Convolutional\nNeural Networks (CNN) were trained with pre-trained\nword embeddings (Dahou et al., 2019a). A hybrid model\nwas proposed by (Abu Farha and Magdy, 2019), where\nCNNs were used for feature extraction, and LSTMs\nwere used for sequence and context understanding.\nCurrent state-of-the-art results are achieved by the hUL-\nMonA model (ElJundi et al., 2019), which is an Arabic\nlanguage model that is based on the ULMﬁt architec-\nture (Howard and Ruder, 2018). W e compare the results of\nARA BERT to those of hULMonA.\n4.2. Named Entity Recognition\nThis task aims to extract and detect named entities in the\ntext. It is framed as a word-level classiﬁcation (or tagging )\ntask, where the classes correspond to pre-deﬁned categorie s\nsuch as names, locations, organizations, events and time\nexpressions. For evaluation, we use the Arabic NER cor-\npus (ANERcorp) (Benajiba and Rosso, 2007). This dataset\ncontains 16.5K entity mentions distributed among 4 entitie s\ncategories, person (39%), organization: (30.4%), location:\n(20.6%), and miscellaneous: (10%).\nBaselines: Advances in the NER task have\nbeen focusing on English, namely on the CoNLL\n2003 (Sang and De Meulder, 2003) dataset. Initially,\nNER was tackled with Conditional Random Fields\n(CRF) (Lafferty et al., 2001). Later on, CRFs were\nused on top of Bi-LSTM models (Huang et al., 2015;\nLample et al., 2016) presenting signiﬁcant improve-\nments over standalone CRFs. Bi-LSTM-CRF structures\nwere then used with contextualized embeddings that\ndisplayed further improvements (Peters et al., 2018).\nLastly, large pre-trained transformers showed slight\nimprovement, setting the current state-of-the-art per-\nformance (Devlin et al., 2018). As for Arabic, W e\ncompare A RA BERT performance with Bi-LSTM-CRF\nbaseline that set the previous state-of-the-art perfor-\nmance (El Bazi and Laachfoubi, 2019), and with BER T\nmultilingual.\n4.3. Question Answering\nOpen-domain Question Answering (QA) is one of the\ngoals of artiﬁcial intelligence, this goal can be achieved\nby leveraging natural language understanding and knowl-\nedge gathering (Kwiatkowski et al., 2019). English QA\nresearch has been fueled by the release of large\ndatasets such as Stanford Question Answering Dataset\n(SQuAD) (Rajpurkar et al., 2016). On the other hand, re-\nsearch in Arabic QA has been hindered by the lack of such\nmassive datasets, and by the fact that Arabic presents its\nown challenges such as:\n• Inconsistent name spelling (ex: Syria in Arabic can be\nwritten as “ /char41\n/char4b\n/char0a /char50/charf1\n/char83 - sOriyA” and “ /char10/chare9 /char4b\n/char0a /char50/charf1\n/char83 - sOriyT ” )\n• Name de-spacing (ex: The name is written as “\n/char09/char51/char4b /char0a /char09/char51 /charaa /charcb/char40/char59 /char4a/char2e /charab - AbdulAzIz” in the question, and “ /char59 /char4a/char2e /charab/char09/char51/char4b /char0a /char09/char51 /charaa /charcb/char40 - Abdul AzIz” in the answer)\n• Dual form “ /charfa /char09/chare6 /char11\n/char4a\n/chard6 /charcf /char40 ”, which can have multiple forms (ex:\n“ /char09/chare0/char41 /chard2 /charca /char10/charaf ” - “ qalamAn” or “ /char09/chare1 /char1e\n/char0a\n/chard2 /charca /char10/charaf ” - “ qalamyn” meaning\n“ two pencils”)\n• Grammatical gender variation: all nouns, animate and\ninanimate objects are classiﬁed under two genders ei-\nther masculine or feminine (ex: “ /char51 /char1e\n/char0a\n/char4a/char2e /charbb ” - “ kabIr” and “/char10\n/chare8 /char51 /char1e\n/char0a\n/char4a/char2e /charbb ” - “ kabIrT ”\nW e evaluate A RA BERT on the Arabic Reading Compre-\nhension Dataset (ARCD) (Mozannar et al., 2019) , where\nthe task is to ﬁnd the span of the answer in a document\nfor a given question. ARCD contains 1395 questions\non Wikipedia articles along with 2966 machine translated\nquestions and answers from the SQuAD dubbed (Arabic-\nSQuAD). W e train on the whole Arabic-SQuAD and on\n50% of ARCD and test on the remaining 50% of ARCD.\nBaselines Multilingual BER T had previously achieved\nstate of the art results on ARCD.\n5. Experiments\n5.1. Experimental Setup\nPretraining In our experiments, the original implemen-\ntation of BER T on T ensorFlow was used. The data for\npre-training was sharded, transformed into TFRecords, and\nthen stored on Google Cloud Storage. Duplication factor\nwas set to 10, a random seed of 34, and a masking proba-\nbility of 15%. The model was pre-trained on a TPUv2-8\npod for 1,250,000 steps. T o speed up the training time,\nthe ﬁrst 900K steps were trained on sequences of 128 to-\nkens, and the remaining steps were trained on sequences of\n512 tokens. The decision of stopping the pre-training was\nbased on the performance of downstream tasks. W e fol-\nlow the same approach taken by the open-sourced German\nBER T (DeepsetAI, 2019). Adam optimizer was used, with\na learning rate of 1e-4, batch size of 512 and 128 for se-\nquence length of 128 and 512 respectively. Training took 4\ndays, for 27 epochs over all the tokens.\nFine-tuning Fine-tuning was done independently using\nthe same conﬁguration for all tasks. W e do not run exten-\nsive grid search for the best hyper-parameters due to com-\nputational and time constraints. W e use the splits provided\nby the dataset’s authors when available. and the standard\n80% and 20% when not 2.\n5.2. Results\nT able 1 illustrates the experimental results of applying\nAraBER T to multiple Arabic NLU downstream tasks, com-\npared to state-of-the-art results and the multilingual BER T\nmodel (mBER T).\n2 The scripts used to create the datasets are available on our\nGithub repo https://github.com/aub-mind/arabert\nSentiment Analysis For Arabic sentiment analysis, the\nresults in T able 1 show that both versions of AraBER T out-\nperform mBER T and other state-of-the-art approaches on\nmost tested datasets. Even though AraBER T was trained\non MSA, the model was able to preform well on dialects\nthat were never seen before.\nT able 1: Performance of AraBER T on Arabic downstream\ntasks compared to mBER T and previous state of the art sys-\ntems\nT ask metric prev . SOT A mBERT AraBERTv0.1/ v1\nSA (HARD) Acc. 95.7* 95.7 96.2 / 96.1\nSA (ASTD) Acc. 86.5* 80.1 92.2 / 92.6\nSA (ArsenTD-Lev) Acc. 52.4* 51.0 58.9 / 59.4\nSA (AJGT) Acc. 92.6** 83.6 93.1 / 93.8\nSA (LABR) Acc. 87.5† 83.0 85.9 / 86.7\nNER (ANERcorp) macro-F1 81.7†† 78.4 84.2 / 81.9\nExact Match 34.2 30.1 / 30.6\nQA (ARCD) macro-F1 mBER T 61.3 61.2 / 62.7\nSent. Match 90.0 93.0 / 92.0\n* (ElJundi et al., 2019)\n** (Dahou et al., 2019b)\n† (Dahou et al., 2019b)\n†† Previous state of the art performance by BiLSTM-CRF model\nNamed Entity Recognition Results in T able 1 show that\nAraBER Tv0.1 improved results by 2.53 points in F1 score\nscoring 84.2 compared with the Bi-LSTM-CRF model,\nmaking AraBER T the new state-of-the-art for NER on AN-\nERcorp. T esting AraBER T with tokenized sufﬁxes and pre-\nﬁxes showed results similar to that of the Bi-LSTM-CRF\nmodel. W e believe that the reason this happened is that the\nstart token (B-label) is referenced to the sufﬁxes most of th e\ntime. An example of this, “ /char10/chare9\n/charaa /chard3/char41 /char6d/char2e/charcc /char27/char40 ” with a label B-ORG\nbecomes “ /charc8/char40 ”, “ /char10/chare9 /charaa /chard3/char41 /char67 /char2e ” with labels B-ORG, I-ORG re-\nspectively, providing misleading starting cues to the mode l.\nT esting multilingual BER T , it proved inefﬁcient as we got\nresults lower than the baseline model.\nQuestion Answering While the results in T able 1 show\nan improvement in F1-score, the exact match scores were\nsigniﬁcantly lower. Upon further examination of the re-\nsults, the majority of the erroneous answers differed from\nthe true answer by one or two words with no signiﬁcant im-\npact on the semantics of the answer. Examples are shown\nin T ables 2 and 3. W e also report a 2% absolute increase\nin the sentence match score over mBER T , which is the pre-\nvious state-of-the-art. Sentence Match (SM) measures the\npercentage of predictions that are within the same sentence\nas the ground truth answer.\nT able 2: Example of an erroneous results from the ARCD\ntest set: the only difference is the preposition “ /charfa /char0a\n/char09/charaf - In”.\nQuestion /char3f /char10\n/chare8/char59/char6a /char10\n/char4a/chard6 /charcf /char40 /chard5/chard7 /char0d/char42/char40 /char10/chare9/chard2 /char09/chara2 /char09/char4a/chard3 /char10/char49/char82/char83 /char0d\n/char41 /char10\n/char4b /char09/chare1/char4b /char0a /char15/char40\nwhere was the united nations established?\nGround T ruth In San Francisco – /charf1/charba/char82/char1c /char0a /char82 /char09/char1d/char51 /char09/charaf /char09/chare0/char41/char83 /charfa /char0a\n/char09/charaf\nPredicted Answer San Francisco – /charf1/charba/char82/char1c /char0a /char82 /char09/char1d/char51 /char09/charaf /char09/chare0/char41/char83\nT able 3: Another example of an erroneous results from the\nARCD test set: the predicted answer does not include “in-\ntroductory” words.\nQuestion /char3f /char41/char82/chard2 /char09/char4a/charcb/char40 /char10/chare9/charcb/charf0/char59/char4b /char2e /char90/char41 /char09/char6d/charcc /char27/char40 /chard0/char41 /char09/chara2 /char09/char4a/charcb/char40 /charf1/chareb /char41/chard3\nWhat is the type of government in Austria?\nGround T ruth Austria is a federal republic – /char10/chare9/char4a /char0a /charcb/char40/char50/char59/char4a /char0a /char09/charaf /char10/chare9/char4b/char50/charf1/charea/chard4 /char67 /char2e /charf9 /char0a /chareb /char41/char82/chard2 /char09/char4a/charcb/char40\nPredicted Answer A federal republic – /char10/chare9/char4a /char0a /charcb/char40/char50/char59/char4a /char0a /char09/charaf /char10/chare9/char4b/char50/charf1/charea/chard4 /char67 /char2e\n5.3. Discussion\nAraBER T achieved state-of-the-art performance on senti-\nment analysis, named entity recognition, and the question\nanswering tasks. This adds truth to the assumption that pre-\ntrained language models on a single language only surpass\nthe performance of a multilingual model. This jump in per-\nformance has many explanations. First, data size is a clear\nfactor for the boost in performance. AraBER T used around\n24GB of data in comparison with the 4.3G Wikipedia used\nfor the multilingual BER T . Second, the vocab size used in\nthe multilingual BER T is 2k tokens in comparison with 64k\nvocab size used for developing AraBER T . Third, with the\nlarge data size, the pre-training distribution has more di-\nversity. As for the fourth point, the pre-segmentation ap-\nplied before BER T tokenization improved performance on\nSA and QA tasks but reduced it on the NER task. It is also\nnoted that the pre-processing applied to the pre-training\ndata took into consideration the complexities of the Arabic\nlanguage. Hence, increased the effective vocabulary by ex-\ncluding unnecessary redundant tokens that come with cer-\ntain common preﬁxes, and help the model learn better by\nreducing the language complexity. W e believe these fac-\ntors helped to reach state-of-the-art results on 3 differen t\ntasks and 8 different datasets. Obtained results indicate t hat\nthe advantage we got in the datasets considered are better\nunderstood in a monolingual model than of a general lan-\nguage model trained on Wikipedia crawls such as multilin-\ngual BER T .\n6. Conclusion\nAraBER T sets a new state-of-the-art for several down-\nstream tasks for Arabic language. It is also 300MB\nsmaller than multilingual BER T . By publicly releasing our\nAraBER T models, we hope that it will be used to serve\nas the new baseline for the various Arabic NLP tasks, and\nhope that this work will act as a footing stone to building\nand improving future Arabic language understanding mod-\nels. W e are currently working on publishing an AraBER T\nversion that won’t depend on external tokenizers. W e are\nalso in the process of training models with a better under-\nstanding of the various dialects that the Arabic language ha s\nacross different Arabic countries.\n7. Acknowledgments\nW e would like to express special thanks to Dr. Ramy Baly\n(Massachusetts Institute of T echnology) for the useful dis -\ncussions and suggestions, to Dr. Dirk Goldhahn (Univer-\nsit¨ at Leipzig) for access to the OSIAN dataset, to TFRC for\nthe free access to cloud TPUs, and to As-Saﬁr newspaper,\nand Y akshof for providing us with their news articles.\n8. References\nAbdelali, A., Darwish, K., Durrani, N., and Mubarak, H.\n(2016). Farasa: A fast and furious segmenter for ara-\nbic. In Proceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics: Demonstrations , pages 11–16.\nAbdul-Mageed, M., Alhuzali, H., and Elaraby, M. (2018).\nY ou tweet what you speak: A city-level dataset of ara-\nbic dialects. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Evalua-\ntion (LREC 2018).\nAbu Farha, I. and Magdy, W . (2019). Mazajak: An online\nArabic sentiment analyser. In Proceedings of the F ourth\nArabic Natural Language Processing W orkshop , pages\n192–198, Florence, Italy, August. Association for Com-\nputational Linguistics.\nAdiwardana, D., Luong, M.-T ., So, D. R., Hall, J., Fiedel,\nN., Thoppilan, R., Y ang, Z., Kulshreshtha, A., Nemade,\nG., Lu, Y ., and Le, Q. V . (2020). T owards a human-like\nopen-domain chatbot.\nAl Sallab, A., Hajj, H., Badaro, G., Baly, R., El-Hajj, W .,\nand Shaban, K. (2015). Deep learning models for sen-\ntiment analysis in arabic. In Proceedings of the second\nworkshop on Arabic natural language processing, pages\n9–17.\nAl-Sallab, A., Baly, R., Hajj, H., Shaban, K. B., El-Hajj,\nW ., and Badaro, G. (2017). Aroma: A recursive deep\nlearning model for opinion mining in arabic as a low re-\nsource language. ACM T ransactions on Asian and Low-\nResource Language Information Processing (TALLIP) ,\n16(4):1–20.\nAlomari, K. M., ElSherif, H. M., and Shaalan, K. (2017).\nArabic tweets sentimental analysis using machine learn-\ning. In International Conference on Industrial, Engi-\nneering and Other Applications of Applied Intelligent\nSystems, pages 602–610. Springer.\nAly, M. and Atiya, A. (2013). LABR: A large scale Arabic\nbook reviews dataset. In Proceedings of the 51st Annual\nMeeting of the Association for Computational Linguis-\ntics (V olume 2: Short P apers) , pages 494–498, Soﬁa,\nBulgaria, August. Association for Computational Lin-\nguistics.\nBadaro, G., Baly, R., Hajj, H., Habash, N., and El-Hajj,\nW . (2014). A large scale arabic sentiment lexicon for\narabic opinion mining. In Proceedings of the EMNLP\n2014 workshop on arabic natural language processing\n(ANLP), pages 165–173.\nBaly, R., Hajj, H., Habash, N., Shaban, K. B., and El-Hajj,\nW . (2017). A sentiment treebank and morphologically\nenriched recursive deep models for effective sentiment\nanalysis in arabic. ACM T ransactions on Asian and Low-\nResource Language Information Processing (TALLIP) ,\n16(4):1–21.\nBaly, R., Khaddaj, A., Hajj, H., El-Hajj, W ., and Sha-\nban, K. B. (2018). Arsentd-lev: A multi-topic corpus\nfor target-based sentiment analysis in arabic levantine\ntweets. In OSACT 3: The 3rd W orkshop on Open-Source\nArabic Corpora and Processing T ools, page 37.\nBenajiba, Y . and Rosso, P . (2007). Anersys 2.0: Conquer-\ning the ner task for the arabic language by combining the\nmaximum entropy with pos-tag information. In IICAI,\npages 1814–1823.\nBojanowski, P ., Grave, E., Joulin, A., and Mikolov, T .\n(2017). Enriching word vectors with subword informa-\ntion. T ransactions of the Association for Computational\nLinguistics, 5:135–146.\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V .,\nW enzek, G., Guzm ´ an, F ., Grave, E., Ott, M., Zettle-\nmoyer, L., and Stoyanov, V . (2019). Unsupervised\ncross-lingual representation learning at scale.\nDahou, A., Elaziz, M. A., Zhou, J., and Xiong, S. (2019a).\nArabic sentiment classiﬁcation using convolutional neu-\nral network and differential evolution algorithm. Com-\nputational intelligence and neuroscience, 2019.\nDahou, A., Xiong, S., Zhou, J., and Elaziz, M. A. (2019b).\nMulti-channel embedding convolutional neural network\nmodel for arabic sentiment classiﬁcation. ACM T ransac-\ntions on Asian and Low-Resource Language Information\nProcessing (TALLIP), 18(4):1–23.\nde Vries, W ., van Cranenburgh, A., Bisazza, A., Caselli, T .,\nvan Noord, G., and Nissim, M. (2019). Bertje: A dutch\nbert model. arXiv preprint arXiv:1912.09582.\nDeepsetAI. (2019). Open sourcing german bert.\nDevlin, J., Chang, M.-W ., Lee, K., and T outanova, K.\n(2018). Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. arXiv preprint\narXiv:1810.04805 .\nEl Bazi, I. and Laachfoubi, N. (2019). Arabic named entity\nrecognition using deep learning approach. International\nJournal of Electrical & Computer Engineering (2088-\n8708), 9(3).\nEl-Khair, I. A. (2016). 1.5 billion words arabic corpus.\narXiv preprint arXiv:1611.04033.\nElJundi, O., Antoun, W ., El Droubi, N., Hajj, H., El-Hajj,\nW ., and Shaban, K. (2019). hulmona: The universal lan-\nguage model in arabic. In Proceedings of the F ourth Ara-\nbic Natural Language Processing W orkshop, pages 68–\n77.\nElnagar, A., Khalifa, Y . S., and Einea, A. (2018). Ho-\ntel arabic-reviews dataset construction for sentiment\nanalysis applications. In Intelligent Natural Language\nProcessing: T rends and Applications , pages 35–52.\nSpringer.\nErdmann, A., Zalmout, N., and Habash, N. (2018). Ad-\ndressing noise in multidialectal word embeddings. In\nProceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (V olume 2: Short P a-\npers), pages 558–565.\nHoward, J. and Ruder, S. (2018). Universal language\nmodel ﬁne-tuning for text classiﬁcation. arXiv preprint\narXiv:1801.06146 .\nHuang, Z., Xu, W ., and Y u, K. (2015). Bidirectional\nlstm-crf models for sequence tagging. arXiv preprint\narXiv:1508.01991 .\nKudo, T . (2018). Subword regularization: Improving neu-\nral network translation models with multiple subword\ncandidates.\nKwiatkowski, T ., Palomaki, J., Redﬁeld, O., Collins, M.,\nParikh, A., Alberti, C., Epstein, D., Polosukhin, I., Kel-\ncey, M., Devlin, J., Lee, K., T outanova, K. N., Jones, L.,\nChang, M.-W ., Dai, A., Uszkoreit, J., Le, Q., and Petrov,\nS. (2019). Natural questions: a benchmark for question\nanswering research. T ransactions of the Association of\nComputational Linguistics.\nLafferty, J. D., McCallum, A., and Pereira, F . C. (2001).\nConditional random ﬁelds: Probabilistic models for seg-\nmenting and labeling sequence data. In ICML.\nLample, G., Ballesteros, M., Subramanian, S., Kawakami,\nK., and Dyer, C. (2016). Neural architectures for named\nentity recognition. arXiv preprint arXiv:1603.01360.\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma,\nP ., and Soricut, R. (2019). Albert: A lite bert for self-\nsupervised learning of language representations.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V .\n(2019). Roberta: A robustly optimized bert pretraining\napproach.\nMartin, L., Muller, B., Su ´ arez, P . J. O., Dupont, Y ., Ro-\nmary, L., ´Eric V illemonte de la Clergerie, Seddah, D.,\nand Sagot, B. (2019). Camembert: a tasty french lan-\nguage model.\nMikolov, T ., Sutskever, I., Chen, K., Corrado, G. S., and\nDean, J. (2013). Distributed representations of words\nand phrases and their compositionality. In Advances\nin neural information processing systems , pages 3111–\n3119.\nMikolov, T ., Grave, E., Bojanowski, P ., Puhrsch, C., and\nJoulin, A. (2017). Advances in pre-training distributed\nword representations. arXiv preprint arXiv:1712.09405.\nMozannar, H., Maamary, E., El Hajal, K., and Hajj, H.\n(2019). Neural arabic question answering. In Proceed-\nings of the F ourth Arabic Natural Language Processing\nW orkshop, pages 108–118.\nNabil, M., Aly, M., and Atiya, A. (2015). ASTD: Ara-\nbic sentiment tweets dataset. In Proceedings of the\n2015 Conference on Empirical Methods in Natural Lan-\nguage Processing, pages 2515–2519, Lisbon, Portugal,\nSeptember. Association for Computational Linguistics.\nPennington, J., Socher, R., and Manning, C. (2014).\nGlove: Global vectors for word representation. In Pro-\nceedings of the 2014 conference on empirical methods\nin natural language processing (EMNLP) , pages 1532–\n1543.\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\nC., Lee, K., and Zettlemoyer, L. (2018). Deep contextu-\nalized word representations. In Proceedings of NAACL-\nHLT, pages 2227–2237.\nPolignano, M., Basile, P ., de Gemmis, M., Semeraro, G.,\nand Basile, V . (2019). AlBER T o: Italian BER T Lan-\nguage Understanding Model for NLP Challenging T asks\nBased on T weets. In Proceedings of the Sixth Ital-\nian Conference on Computational Linguistics (CLiC-it\n2019), volume 2481. CEUR.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W ., and Liu, P . J. (2019). Ex-\nploring the limits of transfer learning with a uniﬁed text-\nto-text transformer.\nRajpurkar, P ., Zhang, J., Lopyrev, K., and Liang, P . (2016).\nSquad: 100,000+ questions for machine comprehension\nof text. arXiv preprint arXiv:1606.05250.\nRatnaparkhi, A. (1998). Maximum entropy models for nat-\nural language ambiguity resolution.\nSang, E. F . and De Meulder, F . (2003). Introduction to the\nconll-2003 shared task: Language-independent named\nentity recognition. arXiv preprint cs/0306050.\nSoliman, A. B., Eissa, K., and El-Beltagy, S. R. (2017).\nAravec: A set of arabic word embedding models for use\nin arabic nlp. Procedia Computer Science, 117:256–265.\nV aswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. (2017).\nAttention is all you need.\nY ang, Z., Dai, Z., Y ang, Y ., Carbonell, J., Salakhutdinov,\nR., and Le, Q. V . (2019). Xlnet: Generalized autore-\ngressive pretraining for language understanding.\nZeroual, I., Goldhahn, D., Eckart, T ., and Lakhouaja, A.\n(2019). OSIAN: Open source international Arabic news\ncorpus - preparation and integration into the CLARIN-\ninfrastructure. In Proceedings of the F ourth Arabic Nat-\nural Language Processing W orkshop , pages 175–182,\nFlorence, Italy, August. Association for Computational\nLinguistics.\nZhu, Y ., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun,\nR., T orralba, A., and Fidler, S. (2015). Aligning books\nand movies: T owards story-like visual explanations by\nwatching movies and reading books.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8345907926559448
    },
    {
      "name": "Natural language processing",
      "score": 0.7856240272521973
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7004213333129883
    },
    {
      "name": "Arabic",
      "score": 0.6687021851539612
    },
    {
      "name": "Transformer",
      "score": 0.6502573490142822
    },
    {
      "name": "Language model",
      "score": 0.6081680059432983
    },
    {
      "name": "Syntax",
      "score": 0.47683587670326233
    },
    {
      "name": "Named-entity recognition",
      "score": 0.45218178629875183
    },
    {
      "name": "Linguistics",
      "score": 0.3435993790626526
    },
    {
      "name": "Task (project management)",
      "score": 0.0769062340259552
    },
    {
      "name": "Engineering",
      "score": 0.07384338974952698
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}