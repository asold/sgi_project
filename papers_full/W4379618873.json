{
  "title": "NDT Method for Weld Defects Based on FMPVit Transformer Model",
  "url": "https://openalex.org/W4379618873",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1983143503",
      "name": "Yang Liu",
      "affiliations": [
        "Liaoning Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2020372762",
      "name": "Kun Yuan",
      "affiliations": [
        "Liaoning Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2093421881",
      "name": "Tian Li",
      "affiliations": [
        "Liaoning Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2099791182",
      "name": "Sha Li",
      "affiliations": [
        "Liaoning Normal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3199863078",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2960155470",
    "https://openalex.org/W2903867357",
    "https://openalex.org/W6790690058",
    "https://openalex.org/W4289985774",
    "https://openalex.org/W4214896879",
    "https://openalex.org/W2995426144",
    "https://openalex.org/W3016522735",
    "https://openalex.org/W2917609910",
    "https://openalex.org/W6774550102",
    "https://openalex.org/W2985101974",
    "https://openalex.org/W2124386111",
    "https://openalex.org/W4207080879",
    "https://openalex.org/W4206309865",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W4206244692",
    "https://openalex.org/W3152571275",
    "https://openalex.org/W4225113999",
    "https://openalex.org/W4206393365",
    "https://openalex.org/W4214709605",
    "https://openalex.org/W3017377715",
    "https://openalex.org/W6798016242",
    "https://openalex.org/W4312977443",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W3136936142",
    "https://openalex.org/W4214588794",
    "https://openalex.org/W2990176100",
    "https://openalex.org/W4280498637",
    "https://openalex.org/W3190492058",
    "https://openalex.org/W2963172626",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3171206729",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W3164781147",
    "https://openalex.org/W3124539583",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4306955484",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W3216603139",
    "https://openalex.org/W4285202680",
    "https://openalex.org/W3140854437",
    "https://openalex.org/W2990673627",
    "https://openalex.org/W3035661013",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3176153963",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W4297779827"
  ],
  "abstract": "The primary NDT method for welding defects is the image-based detection. Currently, the best performance for image-based detection is based on the transformer model. However, with its high accuracy, it has many limitations, such as large model parameters, large data sample requirements, and expensive computer resources. This model has a weaker ability to capture local features compared with global features. In this study, an improved and optimized welding defect detection and identification framework named Fast Multi-Path Vision transformer (FMPVit) is proposed based on the transformer model. This model uses a multilayer parallel architecture and enhances the local information capture ability of the model through advanced multiscale convolution feature aggregation and the addition of a new local convolution module. Finally, a validation test is carried out using an open dataset of weld seams. The model is proven to exhibit an evident performance improvement over the mainstream model baseline.",
  "full_text": "Received 3 May 2023, accepted 4 June 2023, date of publication 7 June 2023, date of current version 22 June 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3283589\nNDT Method for Weld Defects Based\non FMPVit Transformer Model\nYANG LIU\n , KUN YUAN\n , TIAN LI\n , AND SHA LI\nDepartment of Computer and Information Technology, Liaoning Normal University, Dalian 116000, China\nCorresponding author: Yang Liu (yangliu.0816@lnnu.edu.cn)\nThis work was supported in part by the National Natural Science Foundation of China under Grant 62002146 and Grant 61976109, in part\nby the Liaoning Revitalization Talents Program under Grant XLYC1807073 and Grant XLYC2006005, in part by the Scientific Research\nProject of Liaoning Province under Grant LJKZ0963, and in part by the Key Research and Development Projects of Liaoning Provincial\nDepartment of Science and Technology.\nABSTRACT The primary NDT method for welding defects is the image-based detection. Currently, the best\nperformance for image-based detection is based on the transformer model. However, with its high accuracy,\nit has many limitations, such as large model parameters, large data sample requirements, and expensive\ncomputer resources. This model has a weaker ability to capture local features compared with global features.\nIn this study, an improved and optimized welding defect detection and identification framework named Fast\nMulti-Path Vision transformer (FMPVit) is proposed based on the transformer model. This model uses a\nmultilayer parallel architecture and enhances the local information capture ability of the model through\nadvanced multiscale convolution feature aggregation and the addition of a new local convolution module.\nFinally, a validation test is carried out using an open dataset of weld seams. The model is proven to exhibit\nan evident performance improvement over the mainstream model baseline.\nINDEX TERMS Deep learning, weld detection classification, line laser, one-dimensional sequential time\nseries.\nI. INTRODUCTION\nWelding inspection or inspection of the quality of welding\nproducts is used to ensure the integrity, reliability, safety,\nand availability of welding product structures [1]. It is\nwidely used in the aerospace, aviation, automobile, machin-\nery, shipbuilding, and other industries. Although industrial\nproduction has matured, improper manual operation, envi-\nronmental instability, and other problems may still lead to\nvarious welding defects in industrial products. Common weld\ndefect types in steel plates are shown in Figure 1, including\nburr, concave, porous, and no defects. Welding defect detec-\ntion technology can improve the production efficiency of the\nmanufacturing industry, accelerate the production cycle of\nproducts, and reduce labor and material costs [2].\nConventional welding inspection methods, which are car-\nried out by experienced professionals with the naked eye\nand professional tools, not only lead to low inspection\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Ali Kashif Bashir\n.\nefficiency but also have disadvantages such as misjudgment,\nsparse sampling, visual fatigue, and difficulty in ensuring\nthe quality of inspection results. Modern welding testing\nmethods can be divided into destructive testing (DT) and non-\ndestructive testing (NDT) methods, depending on whether\nthe test method is destructive. NDT is highly efficient and\nsafe; it is the current mainstream welding defect detection\nmethod [3]. In welding NDT, most methods use images as\nthe input, and the convolutional neural networks (CNNs) are\nemployed.\nVarious models of the CNN stand out in ImageNet compe-\ntitions and have high efficiency for image recognition [4]. The\nrapid development of CNNs in the field of computer vision\nhas also led to the growth of the NDT of welding defects.\nAlexNet, ResNet, and other CNNs have been widely used\nin the field of NDT of welding defects and have achieved\ngood recognition and detection results [5]. Although the CNN\nhas the advantages of high generalization, fast recognition\nefficiency, and sensitivity to local features, their ability to\ncapture global features is relatively weak [6].\n61390\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ VOLUME 11, 2023\nY. Liu et al.: NDT Method for Weld Defects Based on FMPVit Transformer Model\nFIGURE 1. Common types of weld defects: (a) burr, (b) concave, (c)\nporosity, and (d) no defects. The left is an RGB image and right is line\nlaser image.\nTransformer model-based attention mechanisms have con-\ntinuously emerged in the field of natural language processing\nand have developed into a recognized high-performance\nmodel structure [7]. With the first application of the vision\ntransformer in the field of computer vision images, the\nmodel has become a competitor to CNNs [8]. It not only\noutperforms many popular CNN models in terms of com-\nputational efficiency and accuracy but also asserts huge\ndevelopment potential in the future. The transformer over-\ncomes the limitation that the RNN model cannot be calculated\nin parallel. Compared with a CNN, the number of operations\nrequired to calculate the correlation between two locations\ndoes not increase with distance. However, the transformer\ncan also focus more on global features than CNNs can [9].\nTherefore, Multi-Path Vision transformer (MPVit) was pro-\nposed, which combines the advantages of convolution and a\ntransformer [44].\nIn the industrial welding process, the scale of weld defects\nis relatively small compared with that of other defects, which\nresults in fewer features in the image, making it more difficult\nfor the model to capture such features. When identifying\nweld defects that are not evident, the effect of relying only\non the CNN model is sometimes poor, and the advantage in\nthis respect is not evident. In contrast to ordinary images,\nweld images are generally more regular and single, with\nmore concentrated areas and fewer features. Although exist-\ning models have a higher recognition efficiency for ordinary\nimages, their effect on weld images is unsatisfactory. The\nMulti-Path Vision transformer (MPVit) model combines the\ncomprehensive advantages of the CNN and transformer,\nwhich are excessively redundant and still have deficiencies\nin small-scale target recognition. However, this model is too\ncomplex for weld defect detection, difficult to train, and\nrequires a large number of samples. Therefore, we propose\na new method called Fast Multi-Path Vision transformer\n(FMPVit) for welding defect detection, which is based on the\naggregation optimization of local and multi-scale features.\nThe public weld dataset and popular public datasets were\ncompared with the popular neural network model.\nThe key contributions of this paper are:\n1. The proposed method can improve the efficiency\nof small-scale target identification, such as weld defects.\nIt achieves higher recognition accuracy and lower training\ncosts by reducing the complexity of the model.\n2. A fast multiscale convolution feature-priority aggre-\ngation module was proposed. The module reduces the\nredundancy of the three transform structures in the stacking\nstage of the MPVit model and significantly reduces the com-\nplexity of the original model.\n3. We introduce local-to-global feature interaction (LGF)\nto take advantage of both the local connectivity of the convo-\nlutions and global context of the transformer.\nIn addition, the latest published weld dataset JPEGWD\nwas used in the experiment, which includes 12000 RGB\nweld images of four different defect types. We also tested\nour method using LSWD-MTF, which is a two-dimensional\ntime-series image dataset of LSWD encoded by the Markov\nTransition Field (MTF) method.\nII. RELATED WORK\nIn welding image classification, deep learning methods have\nperformed better than traditional machine learning methods.\nMany researchers have proposed NDT methods for welding\ndefects based on deep-learning CNN. For example, Je-Kang\net al. proposed a CNN-based method that uses a single RGB\ncamera to examine welding defects on the transmission sur-\nface of an engine. This method consisted of two steps. In the\nfirst step, to extract the welding area from the captured image,\na CNN-based method is used to detect the center of the engine\ntransmission in the image. In the second stage, the extracted\narea is identified by another CNN as having either defects\nor no defects [11]. Zhang et al. designed an 11-layer CNN\nclassification model based on weld images to identify weld\npenetration defects. The CNN model makes full use of arcs\nand combines them in various ways to form complementary\nfeatures. The test results showed that the designed CNN\nmodel performed better than previous models [12]. Dong\net al. proposed a multitask deep CNN for defect classifi-\ncation; they built a stack of encoder–decoder autoencoders\nto learn feature representations from ordinary images. For\ndefect detection, this method can obtain results nearly as good\nas those of a supervised learning method without any data\nannotation [13]. Chen et al. focused on establishing an end-\nto-end automatic detection model for X-ray welding defects\nbased on a deep learning algorithm to improve the accuracy\nand efficiency of detection. The characteristic information of\nwelding defects is considered in their study, and the method\nof fast region-based CNNs (R-CNNs) is improved. A residual\nneural network (ResNet) was used to improve feature extrac-\ntion ability [14].\nVOLUME 11, 2023 61391\nY. Liu et al.: NDT Method for Weld Defects Based on FMPVit Transformer Model\nThe attention mechanism-based transformer model has\ncontinuously emerged in the field of natural language pro-\ncessing and developed into a recognized high-performance\nmodel structure [15]. With the first application of vision\ntransformer in the field of computer vision images, this\nmodel has become a competitor to CNN [16]; it not\nonly outperforms many popular CNN models in terms\nof computational efficiency and accuracy but also has a\nhuge potential for development in the future. The trans-\nformer overcomes the limitation that the RNN model\ncannot be calculated in parallel. Compared with a CNN,\nthe number of operations required to calculate the cor-\nrelation between two locations does not increase with\ndistance [17].\nAs a newly emerging deep-learning model, the transformer\nis no less advanced than the CNN model despite its short\nhistory, and its performance even exceeds that of conventional\nmainstream CNNs. The transformer model first gained a\ndominant position in the field of natural language processing\nowing to its high-performance recognition effect; it gradually\nrose in the field of computer vision (CV) once it became a\nstrong competitor of CNNs. Some researchers have studied\ntransformers in the field of NDT of welding defects. For\nexample, Wang et al. [18] proposed a deep learning method\nbased on a classic vision transformer to realize welding pen-\netration recognition, constructed an image dataset composed\nof four different categories, and trained it from scratch to\nexplore its feasibility in welding penetration recognition.\nFinally, ImageNet was used for pretraining to solve the\nproblems of complex models and insufficient data, and the\nverification accuracy was improved by 4.45%. To explore\nthe extensibility of the transformer, Gao et al. [19] proposed\nan improved structure called the variant swin transformer,\nbased on the applicability of the swin transformer (SwinT).\nA new window shift scheme was designed to further enhance\nfeature conversion between windows and increase the capa-\nbility of the framework for defect detection. Considering\nthe built private dataset, the overall framework, named the\nCas-VSwin transformer, is superior to most existing models.\nZhang et al. [20] proposed a novel network structure called\nthe DRCDCT-Net. It was designed as a dual routing structure\ncomprising a characteristic attention deficit diagnosis module\n(FAD) and cross-domain joint learning deficiency diagnosis\nmodule (CJLD). With the transformer as the core design, the\nFAD is primarily responsible for handling defect classifica-\ntion tasks with sufficient samples and relieving the problem\nof interdependence among features that are difficult for the\nCNN to learn. With the designed cross-domain joint learning\nnetwork as the core, CJLD deals with the task of defect\nclassification with extremely scarce samples and decou-\npling image domain features. The model achieved accuracy\nof 99.7 ± 0.2% and 90.0 ± 0.6% in the Northeastern\nUniversity (NEU)-CLS and SEVERSTAL public datasets,\nrespectively. Although the overall performance of the trans-\nformer method was better than that of the CNN method, some\nproblems persisted.\nOther NDT methods include magnetic particle testing,\neddy current testing, magneto-optical imaging testing, ultra-\nsonic testing, infrared testing, penetrant testing, and phased\narray ultrasonic testing [10]. Some NDT methods use sig-\nnals for direct detection; however, some researchers also use\ntwo-dimensional images for detection. Because the dimen-\nsions of the one-dimensional signal description features are\nlow and description of some defects is unclear, the detec-\ntion performance is poor. Compared with one-dimensional\nsignals, two-dimensional image detection is more stable,\nwhich can be better applied to a depth learning model,\ntaking full advantage of its existing benefits. For example,\nsome researchers [36] coded the one-dimensional structured\nlight centerline of the weld surface into the correspond-\ning two-dimensional time series image, which realized the\ndimensions of the weld defect and improved the depth learn-\ning model for the two-dimensional image.\nIII. METHODS\nThe detection of weld defects is a small-sample detection, and\nthe problem of excessive redundancy exists in the transformer\nmodel, which leads to difficulty in training the model and a\npoor effect; moreover, the solution of data enhancement is\ntoo cumbersome. Therefore, in the field of welding defect\ndetection, a lightweight model that combines the advantages\nof CNN and transformers and realizes small-sample training\nis necessary.\nA. ARCHITECTURE\nFigure 2 shows the FMPViT architecture. MPVit has an\nextended multi-path, based on ViT and XCiT [21], as well as\nan added convolution module. Although the MPVit model has\na significantly improved detection performance and accuracy,\nit has become more complex and requires more training\nresources. For a small weld-defect detection dataset, the\nMPVit model is too large and difficult to train. To solve these\nproblems, we use a variety of transformer architectures for\nreference and are committed to building a single transformer\npath-stacking framework combined with convolution mod-\nules [22], [23]. The goal of building the FMPVit model is\nto have a faster reasoning speed and lower computing cost,\nwhile achieving a higher performance than MPVit. As shown\nin Figure 2, we construct a four-stage feature hierarchy to\ngenerate feature maps at different scales. The characteris-\ntics of the MPVit model often require more computation;\ntherefore, we adopted a series of measures to reduce the com-\nplexity of the MPVit model. To reduce the linear complexity\nof the model, only a single transformer structure is used for\neach stacking stage based on the MPVit model. In addition,\na transformer encoder that decompositions self-attention in\ncatCoaT [24] is used, and the convolutional stem block in\nLeViT [25] is used to improve the present low-level repre-\nsentation to prevent the loss of significant information.\nAs shown in Figure 3, we propose a new NDT frame-\nwork for welding defects, i.e., FMPVit, based on the MPVit\nmodel, which has a faster training speed and higher accuracy\n61392 VOLUME 11, 2023\nY. Liu et al.: NDT Method for Weld Defects Based on FMPVit Transformer Model\nFIGURE 2. Fast multi-path vision transformer (FMPViT) architecture.\nwhile considering local and multi-scale features. In FMPVit,\na fast multistage transformer structure is constructed, and a\nnew 3 × 3 convolution is embedded to enhance the model’s\nconvolutional local feature capture ability. This solves the\nproblem of the transformer lacking local features compared\nwith the convolutional network. In addition, by aggregat-\ning the multiscale features of each stage into a transformer,\nthe excess transformer path structure of the original frame-\nwork was reduced, which significantly reduced the model\ncomplexity and improved the overall model performance\nefficiency. Notably, this is different from the existing vision\ntransformers.\nThe excessive complexity of the model is not desirable\nin the field of NDT for weld defects. Because the sample\nsize of a welding dataset is usually small, data enhance-\nment is time-consuming and energy-intensive during the\ntraining process, and transfer learning for small samples\nmakes the training process more complex. Overfitting and\nunstable training often occur during the large-scale model\ntraining. Therefore, it is necessary to reduce the path length\nand complexity of the model. It is also important to reduce\nthe complexity of the model while considering its accuracy.\nBased on the multistage transformer architecture design with\nsmaller complexity, we merged all redundant transformer\nstructures in the MPVit model and proposed a fast multiscale\npatch embedding module.\nB. FAST MULTI-SCALE PATCH EMBEDDING\nTo make better use of the fine-grained and coarse-grained\nvisual tokens, a convolution operation with overlapping\npatches was used, similar to CNNs [26] and CvT [27].\nBy changing the size and filling amount of the convolu-\ntion kernel, a same-size feature map with different feature\ninformation can be obtained. As shown in Figure 2, visual\ntokens of different sizes with the same sequence length can\nbe generated with patch sizes of 3 × 3, 5 × 5, and 7 × 7.\nDuring implementation, because the continuous convolution\noperations of the same channel and filter size enlarge the\nreceptive field (e.g., two 3 × 3 equal 5 × 5, and three\n3 × 3 equal 7 × 7), the use of 3 × 3 convolution ker-\nnel substitution requires fewer parameters, thereby reducing\ncomplexity. We used three consecutive 3 × 3 convolutions\nwith the same channel size; the fill was 1 and step length was\ns, where s was 2 when the spatial resolution was reduced; oth-\nerwise, it was 1. Because MPViT has more embedding layers\nowing to its multi-path structure, we reduce the parameters\nand computation of the convolutional local feature overhead\nby adopting 3 × 3 depthwise separable convolutions [28],\nwhich consist of 3 × 3 depthwise convolution followed by\n1 × 1 pointwise convolution in the embedding layers.\nFor fast multiscale patch embedding, we proposed a differ-\nent multiscale patch aggregation method. The polymerization\nprocess is illustrated in Figure 4. In this process, the size of\nVOLUME 11, 2023 61393\nY. Liu et al.: NDT Method for Weld Defects Based on FMPVit Transformer Model\nFIGURE 3. (a) Most advanced MPVIT models [44] use multi-scale patches and multi-path transformer encoders. (b) Our FMPViT uses multi-scale\npatch embedding, each multi-path embedded patch using only one independent transformer encoder.\nthe output feature matrices of the three convolution kernels\n(3 × 3, 5 × 5, and 7 × 7) was unified by padding to zero,\nand the three output features were superimposed by matrix\naddition. Finally, the superimposed features are input into a\nsingle transformer module to realize the early aggregation of\nthe token, which can minimize the calculation amount of this\nmodule while ensuring that the multi-path and multi-scale\nconvolution features are not lost.\nBecause the convolution results of different sized convolu-\ntion kernels are different for different images and input of a\nsingle transformer structure is generally the token of a single\nimage, it is necessary to unify the same size when merging\nthree different feature maps. The process of zero padding is\ndescribed in detail as follows.\nWe assume that, before padding, the input size is (H , W ),\nfilter size is (F H , Fw), output size is (O H , Ow), padding is P,\nand step length is S. The output size after padding is obtained\nusing Equations (1) and (2).\nOH = H + 2P − FH\nS + 1 (1)\nOW = H + 2P − FW\nS + 1 (2)\nAfter the output matrices A, B, and C of the three different\npaths (3 × 3, 5 × 5, and 7 × 7) were patched with zero to\nunify their sizes, the final aggregate matrix D was obtained\nby summing their matrices, as shown in Equation (3).\nD = A + B + C (3)\nBecause FMPViT has more embedding layers owing to\nits multi-path structure, a 3 × 3 deep separation convolution\nand 1 × 1 point convolution are adopted to reduce the model\nparameters and computational overhead. A 3 × 3 separable\nconvolution improves the efficiency of the model, whereas\na 1 × 1 point convolution reduces the dimensions, increases\nthe depth of the model, and improves its nonlinear expression\nability.\nC. CONVOLUTION LOCAL FEATURE\nAs shown in Figure 5, to enable the model to effectively\ncapture local features, we added a new 3 × 3 convolution ker-\nnel to the local convolution module. Two 3 × 3 convolution\nkernels are equivalent to a 5 × 5 convolution kernel. Although\nthe two 3 × 3 convolution kernels must be convolved twice,\nthe actual convolution operation efficiency of the convolution\nkernel is higher, there are fewer parameters, and the com-\nputer processing speed is faster. This optimization method\nappeared in early VGG networks [29]. In addition, replacing\na 5 × 5 convolution kernel with two 3 × 3 convolution kernels\nincreases the depth (number of layers) of the network, and the\nnonlinear expression of features is enhanced, which was also\nproven in later experiments [30]. We reduce the number of\nmodel parameters and computational overhead by adopting\n3 × 3 depthwise separable convolutions, which consist of\n3 × 3 depthwise convolutions followed by 1 × 1 pointwise\nconvolutions in the embedding layers.\nD. LOCAL-TO-GLOBAL FEATURE INTERACTION\nThe self-attention mechanism in the transformer can better\ncapture long-term dependencies, i.e., the global context infor-\nmation; however, the capture ability of structural features and\nlocal relationship features is weak [31], [32], which can be\ncompensated by local convolution. The CNN uses the same\nweight to process each patch in the image in terms of trans-\nlation invariance and local connectivity [33]. This inductive\nbias encourages the CNN to exhibit a stronger dependence on\ntexture when classifying visual objects [34]. Combining the\nadvantages of the CNN’s local feature capture with the advan-\ntages of the transformer’s global feature capture can enhance\nthe image feature information acquisition of the model.\nTherefore, a local-to-global feature interaction module was\nproposed for FMPVit. We used a deep residual bottleneck\nblock, which comprises a 1 × 1 convolution, two 3 × 3 depth\nconvolutions, and 1 × 1 convolution composition, with the\nsame channel size and residual connection [35]. The local and\nglobal features are aggregated by concatenation as follows:\nUi = Concat([Ri, Li,0, Li,1, . . . ,Li,j]) (4)\nXi+1 = P(Ui) (5)\nThe 2D-reshaped global features from each transformer\nLi,j ∈ RHi×Wi×Ci , Ri ∈ RHi×Wi×Ci represent local feature,\nwhere j is the index of the path, i is the stage number,\nUi is the aggregated feature, and P(·) is a function which\n61394 VOLUME 11, 2023\nY. Liu et al.: NDT Method for Weld Defects Based on FMPVit Transformer Model\nFIGURE 4. Fast multi-scale patch embedding process.\nFIGURE 5. Convolutional local feature.\nlearns to interact with features, yielding the final feature\nXi+1 ∈ RHi×Wi×Ci+1 with the size of next stage channel\ndimension Ci+1.\nE. MODEL CONFIGURATION\nTo reduce computational burden, the effective factored\nself-attention proposed in CoaT was used as follows:\nFactorAtt (Q, K, V ) = Q√\nC\n(softmax (K)T V ), (6)\nwhere Q, K, V ∈ RN×C are linearly projected queries, keys,\nvalues, respectively; N, C denote the number of tokens and\nembedding dimension, respectively. The factor self-attention\nmethod reduces the FMPVit model parameters and FLOPs\nand improves the overall efficiency of the model. We did\nnot use the traditional multi-path structure in FMPVit, but\nreduced it to a transformer path through the method described\nin Section III-B, which greatly reduced the resource expendi-\nture of the model. The application of the factor self-attention\nmethod to the FMPVit model maximized the overall effi-\nciency of the model.\nIn addition, we found that after reducing the original\nthree-path transformer, the multiscale aggregated single-path\nFMPVit showed better performance in classification, with\na faster training speed and higher accuracy. This demon-\nstrates that aggregating multiscale features into a single\ntransformer path in advance is an effective approach.\nWe built three different versions of FMPVit: the original\nbasic scale FMPViT-Base ( ∗M), expansion of two layers\nof FMP-Transformer Block and medium-scale FMPVit-\nBase+(∗M) of MS-PatchEmbed, and expansion of four\nlayers of FMP-Transformer Block and large-scale FMPVit-\nBase++(∗M) of MS-PatchEmbed. All FMPVit models used\neight transformer encoder heads. Table 1 shows the details of\nthe FMPVit models.\nTABLE 1. FMPViT configurations.\nIV. EXPERIMENT\nA. EXPERIMENT SETTING\nThe Python programming language based on Python3.7 envi-\nronment was used in the experiment, and the mainstream\nVOLUME 11, 2023 61395\nY. Liu et al.: NDT Method for Weld Defects Based on FMPVit Transformer Model\nTensorFlow framework was built on PyCharm. The frame-\nwork environments used were Keras 2.2.4, PyTorch-GPU\n2.2.0, CUDA 10.1, and CuDNN 7.6. The hardware exper-\nimental environment was a single all-in-one NVIDIA\nRTX 3090 GPU and an Intel Core i9 CPU. Table 2 lists\nthe detailed parameter settings for the experimental environ-\nment. All network models had the same setup parameters and\ndataset.\nB. EXPERIMENT DATASET\n1) JPEGWD DATASET\nIn the field of weld defect detection, only a few large-scale\nweld image datasets are currently available. The published\nweld datasets are limited; therefore, to better guarantee the\nexperimental results, two datasets that are currently avail-\nable are used. One is Joint Photographic Experts Group\nWelding (JPEGWD), a JPEG format industrial weld image\ndataset newly published by Chen et al. [36], from the artifi-\ncial intelligence laboratory of Beijing ByteDance Technology\nCo., Ltd. The other is the Linear Structured Light Weld-\ning (LSWD) and Linear Structured Light Welding Markov\nTransfer Field (LSWD-MTF), which was newly published by\nLiu et al. [37].\nAs shown in Table 2, the JPEGWD weld dataset consists\nof 12000 images; is the only RGB image dataset containing\ncommon weld defect types. Based on the image of the defect\ntype in the dataset, the dataset was subdivided into four weld\ntypes: burr, concave, hole, and no-defect. This results is two\nversions of the dataset, each comprising 4000 images, i.e., the\nweld dataset JPEGWD for four image types: burr, concave,\nporous, and no-defect. The size of each image in the welding\ndataset was 500 × 500, and the image format was JPEG.\nTABLE 2. JPEGWD dataset settings.\n2) LSWD DATASET\nThe LSWD dataset was collected by the line-structured\nlight equipment described in this section; 1680 original\nline-structured light images were sorted out under short time,\nincluding burrs (798), depressions (421), holes (108), and\nno defects (353). In the experiment, the steel plate welds\nwere marked at 0.5-cm intervals, and the image data of\nthe weld line structured light were collected twice; the first\ntime, by the way of the line structured light and weld line\nperpendicular, and the second time, the original structured\nlight images of the weld line were acquired with an angle\nof 30◦ between the line structured light and weld line. There\nwere 1680 original structured light images of the weld line\nobtained at 0.5-cm intervals. The four types of defects were\nmanually classified and labeled. The size of the original\nline-structured light image was 1280 × 520 pixels. Because\nthe original experiment sample is small, in the following\nexperiments, the unified use of scaling, rotation, and other\ndata expansion methods was adopted to expand the dataset\nand obtain better experimental results. This resulted in a total\nof 6720 structured light images, and Table 3 presents the\ndataset setup.\nLSWD-MTF is a two-dimensional color time-series\nimage obtained using the MTF coding method from\none-dimensional weld height information data in the LSWD\ndataset, and the dataset images correspond to the LSWD\ndataset one-by-one. Equation (7) describes the coding\nprinciple.\nM =\n\n\nWij|x1 ∈ qi, x1qj · · ·Wij|x1 ∈ qi, xn ∈ qj\nWij|x2 ∈ qi, x1 ∈ qj · · ·Wij|x2 ∈ qi, xn ∈ qj\n..\n. ... .\n.\n.\nWij|xn ∈ qi, x1 ∈ qj · · ·Wij|xn ∈ qi, xn ∈ qj\n\n\n\n(7)\nWhen a time series X is given to define the packet number\nbox Q of the time series and each Xi in the time series\nis assigned to the respective storage box qj(j ∈ [1, Q]),\nthe weighted adjacent matrix W , which can be constructed\nas Q × Q, is converted from the first-order Markov chain\ncount-point box. W is not sensitive to the distribution of X,\nwhich overcomes the disadvantage of insensitive sequence\ntime dependence. The LSWD-MTF dataset corresponding to\nthe LSWD was obtained by encoding the one-dimensional\nweld information into the MTF two-dimensional time-series\nimage, and the size and quantity parameters of the two\ndatasets were consistent. The one-dimensional weld height\ninformation of the original line-structured light is encoded\ninto the corresponding two-dimensional information using\nEquation (7), and the corresponding MTF two-dimensional\ncolor time-series image can be generated using the Python\npseudo-color library. Table 3 presents the number, configura-\ntion, and generation effects of the datasets.\nTABLE 3. LSWD dataset settings.\nC. EXPERIMENT PROCESS\nThe performance of FMPVit was evaluated based on the\nJPEGWED and LSWD weld datasets. The two datasets have\n61396 VOLUME 11, 2023\nY. Liu et al.: NDT Method for Weld Defects Based on FMPVit Transformer Model\nthe same weld defect category, including burr, concave,\nhole and no-defect. The comparison results of JPEGWED\nand LSWD weld datasets are shown in Tables 4, 5, and 6.\nTo highlight the advantages of the model, we used the main-\nstream CNN model and mainstream visual transformer model\nin the comparative experiment. Mainstream CNN mod-\nels include VGG-16 [38], ResNet50 [39], GoogleNet [40],\nDenseNet [41], and MobileNet [42], and mainstream\nvision transform models include Vit [43], Swin [44], and\nMPVit [45]. In FMPVit, the Adam optimizer [46] was used\nto train 300 iterations; the batch size was 64, and the initial\nlearning rate was 0.001. This was scaled using the cosine\nattenuation learning rate scheduler, and each image was\ncropped to 224 × 224 pixels, which is consistent with Table 3.\nTABLE 4. Experimental comparison results of different models in\nJPEGWD weld dataset.\nTable 4 presents the experimental comparison results of\ndifferent models in the JPEGWD weld dataset. In Table 4, two\nexperimental routes can be seen: the CNN and transformer.\nIn the mainstream CNN model route, the accuracy rate of\nthe JPEGWD dataset gradually increased with an increase\nin model complexity and parameter quantity. The highest\naccuracy rate for this route was 82.78% for the MobileNet\nnetwork. In the mainstream transformer model route, the\nJPEGWD dataset also presents the same trend; however,\nin FMPVit, not only the accuracy and GFLOPs improved, but\nthe complexity and parameters of its model are also greatly\nreduced.\nTo effectively highlight the performance of the model,\nwe divided the JPEGWD weld dataset into two categories\naccording to the presence or absence of defects and named\nthe weld dataset JPEGWD-2CLASS. The weld data samples\nwere all obtained from JPEGWD, changing only the four\ncategories into two categories: with or without defects. Owing\nto the small number of defect-free samples in the dataset,\ndataset enhancement methods such as zooming, rotating,\nand cropping are used for data enhancement [47]. There\nwere 6000 images with and without defects in the enhanced\ndataset. The same experimental environment parameters and\nmodels were used in the experiments, and the results are listed\nin Table 4.\nTABLE 5. Experimental comparison results of different models in LSWD\nweld dataset.\nTable 5 presents the experimental comparison results of\nthe different models for the LSWD weld dataset; it lists\nthe two experimental routes: CNN and transformer. As the\nLSWD weld dataset had better image quality and recognition,\nit performed very well in the overall experiment. Among the\nmainstream CNN model routes, the highest accuracy rate of\nthe LSWD dataset exceeds that of the transformer model\nroute by 97.72%. Compared with other models in Table 5,\nit is worth noting that in FMPVit, not only the accuracy and\nGFLOPs improved but also the complexity and parameters of\nits model reduced significantly.\nTABLE 6. Experimental comparison results of different models in\nLSWD-MTF weld dataset.\nIn the LSWD weld dataset, the author provided a version of\nthe two-dimensional color time series image dataset encoded\nby the proposed MTF method [48], which was also used\nin our comparative experiments. The dataset and sample\nnumbers individually correspond to the original LSWD weld\ndataset samples. We call this version of the weld dataset\nLSWD-MTF. The same experimental environment and net-\nwork model parameters were used in this experiment. The\nfinal experimental results are listed in Table 6. The parameters\nof the transformer series model are more complex than those\nof the CNN series model, but the accuracy and GFLOPs\nVOLUME 11, 2023 61397\nY. Liu et al.: NDT Method for Weld Defects Based on FMPVit Transformer Model\nTABLE 7. Ablation experiment analysis of FMPVit model based on JPEGWD weld dataset.\nTABLE 8. Ablation experiment analysis of FMPVit model based on JPEGWD weld dataset.\nare significantly higher than those of the CNN series model.\nMoreover, in the transformer series models, FMPVIT has\nexcellent performance as well as the highest accuracy and\nGFLOPs, while maintaining minimum parameter complexity.\nD. ABLATION STUDY\nFor the FMPVit model, we performed ablation experiments\nwith different layers; the specific configurations are listed in\nTable 1. We used the JPEGWD weld dataset for comparative\nexperimental research, as listed in Table 7. The FMPVit-\nBase++ model version with the largest number of layers is\nmore accurate; however, it exhibits greater model complexity\nand number of parameters. The basic version of FMPVit\ncan have fewer model parameters and lesser complexity\nwhile maintaining a small gap with the high-stack version,\nwhich is beneficial for reducing the model training time.\nIn Table 7, we can see that the accuracy of the FMPVit\nand MPVit models improved by 2–3% on JPEGWD. While\nmaintaining the improvement, all versions of FMPVit have\na smaller model size and faster reasoning speed, which is\ncommendable.\nWe analyzed the ablation experimental results of different\nconfigurations of the FMPVit model. Two configurations\nstand out as the two improved parts of the proposed model;\none is the newly added 3 × 3 convolution module, and\nthe other is the multi-scale multi-path convolution, which is\npre-aggregated into a single transformer module. The results\nof the ablation experiments for all the configurations are\nlisted in Table 8; the two configurations improve the accu-\nracy and efficiency of the model relative to MPVit. The\naddition of the convolution module has a greater impact on\nmodel improvement but consumes more model parameters.\nThe improved method of early aggregation not only improves\nthe performance of the model but also greatly reduces the\nnumber of model parameters and computational resources.\nV. CONCLUSION\nBased on the transformer model, this study proposed\nan improved and optimized welding defect detection and\nrecognition framework, namely, a Fast Multi-path Vision\nTransformer (FMPVit). The performance of the transformer\nnetwork model in weld defect detection was studied, although\nthe CNNs are widely used in the field of NDT weld defect\ndetection. The experiment shows that the new CNN module\nand single transformer module in the model could be com-\nbined with higher accuracy and smaller model size, while\nreducing the path. Compared with the mainstream network\nmodel, the FMPVit proposed in this study can more effec-\ntively capture the global and local feature information of the\nweld image, exhibiting better performance. This can improve\nthe accuracy while ensuring that the model is sufficiently sim-\nplified. The model adopts a multilayer parallel architecture\nand combines the advanced multiscale convolution feature\npriority aggregation with a new local convolution module\nto enhance its local information capture ability. Finally, the\nLPEGWD and LSWD universal weld datasets prove that the\nmodel exhibits an evident performance improvement at the\nbaseline of mainstream models.\nREFERENCES\n[1] R. Saha and P. Biswas, ‘‘Current status and development of external\nenergy-assisted friction stir welding processes: A review,’’ Welding in the\nWorld, vol. 66, pp. 577–609, Jan. 2022.\n61398 VOLUME 11, 2023\nY. Liu et al.: NDT Method for Weld Defects Based on FMPVit Transformer Model\n[2] A. S. Madhvacharyula, A. V . S. Pavan, S. Gorthi, S. Chitral, N. Venkaiah,\nand D. V . Kiran, ‘‘In situ detection of welding defects: A review,’’ Welding\nWorld, vol. 66, pp. 1–18, Jan. 2022.\n[3] F. Zhang, B. Zhang, and X. Zhang, ‘‘Automatic forgery detection for\nX-ray non-destructive testing of welding,’’ Weld. World, vol. 66, no. 4,\npp. 673–684, Apr. 2022.\n[4] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‘‘ImageNet classification\nwith deep convolutional neural networks,’’ Commun. ACM, vol. 60, no. 6,\npp. 84–90, May 2017.\n[5] L. Alzubaidi, J. Zhang, A. J. Humaidi, A. Al-Dujaili, Y . Duan,\nO. Al-Shamma, J. Santamaría, M. A. Fadhel, M. Al-Amidie, and L. Farhan,\n‘‘Review of deep learning: Concepts, CNN architectures, challenges, appli-\ncations, future directions,’’ J. Big Data, vol. 8, no. 1, pp. 1–74, Mar. 2021.\n[6] Y . Liu, J. Hu, X. Kang, J. Luo, and S. Fan, ‘‘Interactformer: Interactive\ntransformer and CNN for hyperspectral image super-resolution,’’ IEEE\nTrans. Geosci. Remote Sens., vol. 60, 2022, Art. no. 5531715.\n[7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nand I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv. Neural Inf.\nProcess. Syst., vol. 30, 2017, pp. 1–11.\n[8] T. Kattenborn, J. Leitloff, F. Schiefer, and S. Hinz, ‘‘Review on convo-\nlutional neural networks (CNN) in vegetation remote sensing,’’ ISPRS\nJ. Photogramm. Remote Sens., vol. 173, pp. 24–49, Mar. 2021.\n[9] T. Lin, Y . Wang, X. Liu, and X. Qiu, ‘‘A survey of transformers,’’ AI Open,\nvol. 3, pp. 111–132, 2022.\n[10] A. Chabot, N. Laroche, E. Carcreff, M. Rauch, and J.-Y . Hascoët,\n‘‘Towards defect monitoring for metallic additive manufacturing compo-\nnents using phased array ultrasonic testing,’’ J. Intell. Manuf., vol. 31, no. 5,\npp. 1191–1201, Jun. 2020.\n[11] J.-K. Park, W.-H. An, and D.-J. Kang, ‘‘Convolutional neural network\nbased surface inspection system for non-patterned welding defects,’’ Int.\nJ. Precis. Eng. Manuf., vol. 20, no. 3, pp. 363–374, Mar. 2019.\n[12] Z. Zhang, G. Wen, and S. Chen, ‘‘Weld image deep learning-based on-\nline defects detection using convolutional neural networks for AL alloy in\nrobotic arc welding,’’J. Manuf. Processes, vol. 45, pp. 208–216, Sep. 2019.\n[13] X. Dong, C. J. Taylor, and T. F. Cootes, ‘‘Defect classification and detection\nusing a multitask deep one-class CNN,’’ IEEE Trans. Autom. Sci. Eng.,\nvol. 19, no. 3, pp. 1719–1730, Jul. 2022.\n[14] Y . Chen, J. Wang, and G. Wang, ‘‘Intelligent welding defect detection\nmodel on improved R-CNN,’’ IETE J. Res., pp. 1–10, Mar. 2022.\n[15] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y . Wang, ‘‘Transformer\nin transformer,’’ in Proc. Adv. Neural Inf. Process. Syst., vol. 34, 2021,\npp. 15908–15919.\n[16] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszko-\nreit, and N. Houlsby, ‘‘An image is worth 16 ×16 words: Transformers for\nimage recognition at scale,’’ 2020, arXiv:2010.11929.\n[17] K. Han, Y . Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y . Tang, A. Xiao,\nC. Xu, Y . Xu, Z. Yang, Y . Zhang, and D. Tao, ‘‘A survey on vision\ntransformer,’’ IEEE Trans. Pattern Anal. Mach. Intell., vol. 45, no. 1,\npp. 87–110, Jan. 2023.\n[18] Z. Wang, H. Chen, Q. Zhong, S. Lin, J. Wu, M. Xu, and Q. Zhang,\n‘‘Recognition of penetration state in GTAW based on vision transformer\nusing weld pool image,’’ Int. J. Adv. Manuf. Technol., vol. 119, nos. 7–8,\npp. 5439–5452, Apr. 2022.\n[19] L. Gao, J. Zhang, C. Yang, and Y . Zhou, ‘‘Cas-VSwin transformer: A vari-\nant Swin transformer for surface-defect detection,’’ Comput. Ind., vol. 140,\nSep. 2022, Art. no. 103689.\n[20] J. Wang, Q. Zhang, and G. Liu, ‘‘DRCDCT-Net: A steel surface\ndefect diagnosis method based on dual-route cross-domain convolution-\ntransformer network,’’ Meas. Sci. Technol., vol. 33, no. 9, 2022,\nArt. no. 095404.\n[21] A. El-Nouby, H. Touvron, M. Caron, P. Bojanowski, M. Douze, A. Joulin,\nI. Laptev, N. Neverova, and G. Synnaeve, ‘‘Xcit: Cross-covariance image\ntransformers,’’ in Proc. Adv. Neural Inf. Process. Syst., vol. 34, 2021,\npp. 20014–20027.\n[22] W. Wang, E. Xie, X. Li, D. Fan, K. Song, D. Liang, T. Lu, P. Luo, and\nL. Shao, ‘‘Pyramid vision transformer: A versatile backbone for dense\nprediction without convolutions,’’ in Proc. IEEE/CVF Int. Conf. Comput.\nVis. (ICCV), Oct. 2021, pp. 548–558.\n[23] J. Yang, C. Li, P. Zhang, X. Dai, B. Xiao, L. Yuan, and J. Gao, ‘‘Focal\nself-attention for local-global interactions in vision transformers,’’ 2021,\narXiv:2107.00641.\n[24] W. Xu, Y . Xu, T. Chang, and Z. Tu, ‘‘Co-scale conv-attentional image trans-\nformers,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2021,\npp. 9961–9970.\n[25] B. Graham, A. El-Nouby, H. Touvron, P. Stock, A. Joulin, H. Jégou, and\nM. Douze, ‘‘LeViT: A vision transformer in ConvNet’s clothing for faster\ninference,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2021,\npp. 12239–12249.\n[26] K. Simonyan and A. Zisserman, ‘‘Very deep convolutional networks for\nlarge-scale image recognition,’’ 2014, arXiv:1409.1556.\n[27] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang, ‘‘CvT:\nIntroducing convolutions to vision transformers,’’ in Proc. IEEE/CVF Int.\nConf. Comput. Vis. (ICCV), Oct. 2021, pp. 22–31.\n[28] F. Chollet, ‘‘Xception: Deep learning with depthwise separable convo-\nlutions,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\nJul. 2017, pp. 1800–1807.\n[29] S. Ghosh, A. Chaki, and K. Santosh, ‘‘Improved U-Net architecture with\nVGG-16 for brain tumor segmentation,’’ Phys. Eng. Sci. Med., vol. 44,\nno. 3, pp. 703–712, Sep. 2021.\n[30] M. Agarwal, A. Singh, S. Arjaria, A. Sinha, and S. Gupta, ‘‘ToLeD: Tomato\nleaf disease detection using convolution neural network,’’ Proc. Comput.\nSci., vol. 167, pp. 293–301, 2020.\n[31] M. Amirul Islam, S. Jia, and N. D. B. Bruce, ‘‘How much posi-\ntion information do convolutional neural networks encode?’’ 2020,\narXiv:2001.08248.\n[32] D. G. Lowe, ‘‘Object recognition from local scale-invariant features,’’ in\nProc. 7th IEEE Int. Conf. Comput. Vis., Dec. 1999, pp. 1150–1157.\n[33] O. S. Kayhan and J. C. van Gemert, ‘‘On translation invariance in\nCNNs: Convolutional layers can exploit absolute spatial location,’’ in\nProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2020,\npp. 14262–14273.\n[34] N. Baker, H. Lu, G. Erlikhman, and P. J. Kellman, ‘‘Deep convolutional\nnetworks do not classify based on global object shape,’’ PLOS Comput.\nBiol., vol. 14, no. 12, Dec. 2018, Art. no. e1006613.\n[35] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for image\nrecognition,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR),\nJun. 2016, pp. 770–778.\n[36] Weld Quality Inspection Items (JPEGWD Weld Data Set). [Online]. Avail-\nable: https://github.com/ppogg/YOLOv5-Lite\n[37] Y . Liu, K. Yuan, T. Li, S. Li, and Y . Ren, ‘‘NDT method for line laser\nwelding based on deep learning and one-dimensional time-series data,’’\nAppl. Sci., vol. 12, no. 15, p. 7837, Aug. 2022.\n[38] R. M. Nazarov, Z. M. Gizatullin, and E. S. Konstantinov, ‘‘Classification of\ndefects in welds using a convolution neural network,’’ in Proc. IEEE Conf.\nRussian Young Researchers Electr. Electron. Eng. (ElConRus), Jan. 2021,\npp. 1641–1644.\n[39] W. Dai, D. Li, D. Tang, H. Wang, and Y . Peng, ‘‘Deep learn-\ning approach for defective spot welds classification using small\nand class-imbalanced datasets,’’ Neurocomputing, vol. 477, pp. 46–60,\nMar. 2022.\n[40] R. Anand, T. Shanthi, M. S. Nithish, and S. Lakshman, ‘‘Face recognition\nand classification using GoogleNET architecture,’’ in Soft Computing for\nProblem Solving. Singapore: Springer, 2020, pp. 261–269.\n[41] Y . Zhu and S. Newsam, ‘‘DenseNet for dense flow,’’ in Proc. IEEE Int.\nConf. Image Process. (ICIP), Sep. 2017, pp. 790–794.\n[42] Y . Chen, X. Dai, D. Chen, M. Liu, X. Dong, L. Yuan, and Z. Liu, ‘‘Mobile-\nformer: Bridging MobileNet and transformer,’’ in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit. (CVPR), Jun. 2022, pp. 5260–5269.\n[43] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, Z. Jiang, F. E. H. Tay,\nJ. Feng, and S. Yan, ‘‘Tokens-to-token ViT: Training vision transformers\nfrom scratch on ImageNet,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis.\n(ICCV), Oct. 2021, pp. 538–547.\n[44] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo,\n‘‘Swin transformer: Hierarchical vision transformer using shifted win-\ndows,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2021,\npp. 9992–10002.\n[45] Y . Lee, J. Kim, J. Willette, and S. J. Hwang, ‘‘MPViT: Multi-path vision\ntransformer for dense prediction,’’ in Proc. IEEE/CVF Conf. Comput. Vis.\nPattern Recognit. (CVPR), Jun. 2022, pp. 7277–7286.\n[46] S. Mehta, C. Paunwala, and B. Vaidya, ‘‘CNN based traffic sign classifi-\ncation using Adam optimizer,’’ in Proc. Int. Conf. Intell. Comput. Control\nSyst. (ICCS), May 2019, pp. 1293–1298.\nVOLUME 11, 2023 61399\nY. Liu et al.: NDT Method for Weld Defects Based on FMPVit Transformer Model\n[47] C. Li, C. Guo, W. Ren, R. Cong, J. Hou, S. Kwong, and D. Tao, ‘‘An under-\nwater image enhancement benchmark dataset and beyond,’’ IEEE Trans.\nImage Process., vol. 29, pp. 4376–4389, 2020.\n[48] M. Bugueño, G. Molina, F. Mena, P. Olivares, and M. Araya, ‘‘Harnessing\nthe power of CNNs for unevenly-sampled light-curves using Markov\ntransition field,’’ Astron. Comput., vol. 35, Apr. 2021, Art. no. 100461.\nYANG LIU received the B.S. degree in mechani-\ncal engineering from Dalian Jiaotong University,\nin 2010, and the M.S. and Ph.D. degrees in com-\nputer science from Pukyong National University,\nin 2012 and 2016, respectively. Since 2016, he has\nbeen a Lecturer with Liaoning Normal University.\nHis research interests include computer vision,\nmachine learning, and SAR image applications.\nKUN YUANreceived the B.S. degree in engineer-\ning and in computer science and technology from\nthe University of Jinan, Shandong, China, in 2020.\nHe is currently pursuing the M.S. degree in com-\nputer science with Liaoning Normal University,\nDalian, Liaoning, China.\nTIAN LI received the B.S. degree in engineer-\ning and in computer science and technology from\nLiaoning Normal University, Dalian, Liaoning,\nChina, in 2020, where she is currently pursuing the\nM.S. degree in computer science.\nSHA LI received the B.S. degree in engineer-\ning and in computer science and technology from\nLiaoning Normal University, Dalian, Liaoning,\nChina, in 2021, where she is currently pursuing the\nM.S. degree in computer science.\n61400 VOLUME 11, 2023",
  "topic": "Nondestructive testing",
  "concepts": [
    {
      "name": "Nondestructive testing",
      "score": 0.7294721603393555
    },
    {
      "name": "Welding",
      "score": 0.576288640499115
    },
    {
      "name": "Transformer",
      "score": 0.5073649287223816
    },
    {
      "name": "Computer science",
      "score": 0.46855053305625916
    },
    {
      "name": "Materials science",
      "score": 0.3161890208721161
    },
    {
      "name": "Engineering",
      "score": 0.20386645197868347
    },
    {
      "name": "Electrical engineering",
      "score": 0.15724804997444153
    },
    {
      "name": "Metallurgy",
      "score": 0.1357135772705078
    },
    {
      "name": "Voltage",
      "score": 0.10595640540122986
    },
    {
      "name": "Physics",
      "score": 0.07361415028572083
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}