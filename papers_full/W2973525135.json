{
  "title": "Emergent Tool Use From Multi-Agent Autocurricula",
  "url": "https://openalex.org/W2973525135",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4284456251",
      "name": "Baker, Bowen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223154717",
      "name": "Kanitscheider, Ingmar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4303460561",
      "name": "Markov, Todor",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103674990",
      "name": "Wu Yi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4294517908",
      "name": "Powell, Glenn",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227796421",
      "name": "McGrew, Bob",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221404522",
      "name": "Mordatch, Igor",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2744921630",
    "https://openalex.org/W2905016180",
    "https://openalex.org/W2561776174",
    "https://openalex.org/W2164424353",
    "https://openalex.org/W2964067469",
    "https://openalex.org/W2593766708",
    "https://openalex.org/W2962730405",
    "https://openalex.org/W2963689090",
    "https://openalex.org/W3020831056",
    "https://openalex.org/W2101355568",
    "https://openalex.org/W2158782408",
    "https://openalex.org/W2117085697",
    "https://openalex.org/W2963712109",
    "https://openalex.org/W2139612737",
    "https://openalex.org/W2462906003",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1587575005",
    "https://openalex.org/W2242755115",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963311874",
    "https://openalex.org/W2920700222",
    "https://openalex.org/W2792012198",
    "https://openalex.org/W2963577640",
    "https://openalex.org/W1596135418",
    "https://openalex.org/W2020815519",
    "https://openalex.org/W2121863487",
    "https://openalex.org/W1520036242",
    "https://openalex.org/W2127197749",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W2772709170",
    "https://openalex.org/W2962938168",
    "https://openalex.org/W2807340089",
    "https://openalex.org/W172298727",
    "https://openalex.org/W2963160877",
    "https://openalex.org/W2964338167",
    "https://openalex.org/W2963881016",
    "https://openalex.org/W2964083594",
    "https://openalex.org/W2162560054",
    "https://openalex.org/W2751973545",
    "https://openalex.org/W2964345382",
    "https://openalex.org/W779494576",
    "https://openalex.org/W2963276097",
    "https://openalex.org/W2257979135",
    "https://openalex.org/W2147276070",
    "https://openalex.org/W2770298516",
    "https://openalex.org/W2080452089",
    "https://openalex.org/W2885163910",
    "https://openalex.org/W2937206389",
    "https://openalex.org/W2963094133",
    "https://openalex.org/W2810602713",
    "https://openalex.org/W2131600418",
    "https://openalex.org/W2963639957",
    "https://openalex.org/W2908460759",
    "https://openalex.org/W2963000099",
    "https://openalex.org/W2911616846",
    "https://openalex.org/W2912046818",
    "https://openalex.org/W2726187156",
    "https://openalex.org/W2145351764",
    "https://openalex.org/W2963637944",
    "https://openalex.org/W2169803171",
    "https://openalex.org/W2766614170",
    "https://openalex.org/W2757631751",
    "https://openalex.org/W2153975459",
    "https://openalex.org/W1597864774",
    "https://openalex.org/W2914351253",
    "https://openalex.org/W1988526405",
    "https://openalex.org/W2963990127",
    "https://openalex.org/W1191599655",
    "https://openalex.org/W2315240907",
    "https://openalex.org/W3037966979"
  ],
  "abstract": "Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.",
  "full_text": "Published as a conference paper at ICLR 2020\nEMERGENT TOOL USE FROM MULTI -AGENT\nAUTOCURRICULA\nBowen Baker∗\nOpenAI\nbowen@openai.com\nIngmar Kanitscheider∗\nOpenAI\ningmar@openai.com\nTodor Markov∗\nOpenAI\ntodor@openai.com\nYi Wu∗\nOpenAI\njxwuyi@openai.com\nGlenn Powell∗\nOpenAI\nglenn@openai.com\nBob McGrew∗\nOpenAI\nbmcgrew@openai.com\nIgor Mordatch∗†\nGoogle Brain\nimordatch@google.com\nABSTRACT\nThrough multi-agent competition, the simple objective ofhide-and-seek, and stan-\ndard reinforcement learning algorithms at scale, we ﬁnd that agents create a self-\nsupervised autocurriculum inducing multiple distinct rounds of emergent strategy,\nmany of which require sophisticated tool use and coordination. We ﬁnd clear evi-\ndence of six emergent phases in agent strategy in our environment, each of which\ncreates a new pressure for the opposing team to adapt; for instance, agents learn\nto build multi-object shelters using moveable boxes which in turn leads to agents\ndiscovering that they can overcome obstacles using ramps. We further provide ev-\nidence that multi-agent competition may scale better with increasing environment\ncomplexity and leads to behavior that centers around far more human-relevant\nskills than other self-supervised reinforcement learning methods such as intrinsic\nmotivation. Finally, we propose transfer and ﬁne-tuning as a way to quantitatively\nevaluate targeted capabilities, and we compare hide-and-seek agents to both in-\ntrinsic motivation and random initialization baselines in a suite of domain-speciﬁc\nintelligence tests.\n1 I NTRODUCTION\nCreating intelligent artiﬁcial agents that can solve a wide variety of complex human-relevant tasks\nhas been a long-standing challenge in the artiﬁcial intelligence community. Of particular relevance\nto humans will be agents that can sense and interact with objects in a physical world. One approach\nto creating these agents is to explicitly specify desired tasks and train a reinforcement learning\n(RL) agent to solve them. On this front, there has been much recent progress in solving physically\ngrounded tasks, e.g. dexterous in-hand manipulation (Rajeswaran et al., 2017; Andrychowicz et al.,\n2018) or locomotion of complex bodies (Schulman et al., 2015; Heess et al., 2017). However,\nspecifying reward functions or collecting demonstrations in order to supervise these tasks can be\n∗This was a large project and many people made signiﬁcant contributions. Bowen, Bob, and Igor conceived\nthe project and provided guidance through all stages of the work. Bowen created the initial environment,\ninfrastructure and models, and obtained the ﬁrst results of sequential skill progression. Ingmar obtained the\nﬁrst results of tool use, contributed to environment variants, created domain-speciﬁc statistics, and with Bowen\ncreated the ﬁnal environment. Todor created the manipulation tasks in the transfer suite, helped Yi with the\nRND baseline, and prepared code for open-sourcing. Yi created the navigation tasks in the transfer suite,\nintrinsic motivation comparisons, and contributed to environment variants. Glenn contributed to designing the\nﬁnal environment and created ﬁnal renderings and project video. Igor provided research supervision and team\nleadership.\n†Work performed while at OpenAI\n1\narXiv:1909.07528v2  [cs.LG]  11 Feb 2020\nPublished as a conference paper at ICLR 2020\n(a) Running and Chasing (b) Fort Building\nHiders\n(c) Ramp Use\n(d) Ramp Defense (e) Box Surﬁng (f) Surf Defense\nSeekers\n# Episodes (x10 )\n6\n0 50 100 150 200 250 300 350 400 450 500\n−50\n0\n50\nReward\nFigure 1: Emergent Skill Progression From Multi-Agent Autocurricula. Through the reward signal\nof hide-and-seek (shown on the y-axis), agents go through 6 distinct stages of emergence. (a) Seekers\n(red) learn to chase hiders, and hiders learn to crudely run away. (b) Hiders (blue) learn basic tool\nuse, using boxes and sometimes existing walls to construct forts. (c) Seekers learn to use ramps to\njump into the hiders’ shelter. (d) Hiders quickly learn to move ramps to the edge of the play area, far\nfrom where they will build their fort, and lock them in place. (e) Seekers learn that they can jump\nfrom locked ramps to unlocked boxes and then surf the box to the hiders’ shelter, which is possible\nbecause the environment allows agents to move together with the box regardless of whether they are\non the ground or not. (f) Hiders learn to lock all the unused boxes before constructing their fort. We\nplot the mean over 3 independent training runs with each individual seed shown with a dotted line.\nPlease see openai.com/blog/emergent-tool-use for example videos.\ntime consuming and costly. Furthermore, the learned skills in these single-agent RL settings are\ninherently bounded by the task description; once the agent has learned to solve the task, there is\nlittle room to improve.\nDue to the high likelihood that direct supervision will not scale to unboundedly complex tasks, many\nhave worked on unsupervised exploration and skill acquisition methods such as intrinsic motivation.\nHowever, current undirected exploration methods scale poorly with environment complexity and\nare drastically different from the way organisms evolve on Earth. The vast amount of complexity\nand diversity on Earth evolved due to co-evolution and competition between organisms, directed by\nnatural selection (Dawkins & Krebs, 1979). When a new successful strategy or mutation emerges,\nit changes the implicit task distribution neighboring agents need to solve and creates a new pressure\n2\nPublished as a conference paper at ICLR 2020\nfor adaptation. These evolutionary arms races create implicit autocurricula (Leibo et al., 2019a)\nwhereby competing agents continually create new tasks for each other. There has been much success\nin leveraging multi-agent autocurricula to solve multi-player games, both in classic discrete games\nsuch as Backgammon (Tesauro, 1995) and Go (Silver et al., 2017), as well as in continuous real-time\ndomains such as Dota (OpenAI, 2018) and Starcraft (Vinyals et al., 2019). Despite the impressive\nemergent complexity in these environments, the learned behavior is quite abstract and disembodied\nfrom the physical world. Our work sees itself in the tradition of previous studies that showcase\nemergent complexity in simple physically grounded environments (Sims, 1994a; Bansal et al., 2018;\nJaderberg et al., 2019; Liu et al., 2019); the success in these settings inspires conﬁdence that inducing\nautocurricula in physically grounded and open-ended environments could eventually enable agents\nto acquire an unbounded number of human-relevant skills.\nWe introduce a new mixed competitive and cooperative physics-based environment in which agents\ncompete in a simple game of hide-and-seek. Through only a visibility-based reward function and\ncompetition, agents learn many emergent skills and strategies including collaborative tool use, where\nagents intentionally change their environment to suit their needs. For example, hiders learn to create\nshelter from the seekers by barricading doors or constructing multi-object forts, and as a counter\nstrategy seekers learn to use ramps to jump into hiders’ shelter. Moreover, we observe signs of dy-\nnamic and growing complexity resulting from multi-agent competition and standard reinforcement\nlearning algorithms; we ﬁnd that agents go through as many as six distinct adaptations of strategy\nand counter-strategy, which are depicted in Figure 1. We further present evidence that multi-agent\nco-adaptation may scale better with environment complexity and qualitatively centers around more\nhuman-interpretable behavior than intrinsically motivated agents.\nHowever, as environments increase in scale and multi-agent autocurricula become more open-ended,\nevaluating progress by qualitative observation will become intractable. We therefore propose a\nsuite of targeted intelligence tests to measure capabilities in our environment that we believe our\nagents may eventually learn, e.g. object permanence (Baillargeon & Carey, 2012), navigation, and\nconstruction. We ﬁnd that for a number of the tests, agents pretrained in hide-and-seek learn faster\nor achieve higher ﬁnal performance than agents trained from scratch or pretrained with intrinsic\nmotivation; however, we ﬁnd that the performance differences are not drastic, indicating that much\nof the skill and feature representations learned in hide-and-seek are entangled and hard to ﬁne-tune.\nThe main contributions of this work are: 1) clear evidence that multi-agent self-play can lead to\nemergent autocurricula with many distinct and compounding phase shifts in agent strategy, 2) evi-\ndence that when induced in a physically grounded environment, multi-agent autocurricula can lead\nto human-relevant skills such as tool use, 3) a proposal to use transfer as a framework for evaluating\nagents in open-ended environments as well as a suite of targeted intelligence tests for our domain,\nand 4) open-sourced environments and code 1 for environment construction to encourage further\nresearch in physically grounded multi-agent autocurricula.\n2 R ELATED WORK\nThere is a long history of using self-play in multi-agent settings. Early work explored self-play\nusing genetic algorithms (Paredis, 1995; Pollack et al., 1997; Rosin & Belew, 1995; Stanley & Mi-\nikkulainen, 2004). Sims (1994a) and Sims (1994b) studied the emergent complexity in morphology\nand behavior of creatures that coevolved in a simulated 3D world. Open-ended evolution was fur-\nther explored in the environments Polyworld (Yaeger, 1994) and Geb (Channon et al., 1998), where\nagents compete and mate in a 2D world, and in Tierra (Ray, 1992) and Avida (Ofria & Wilke, 2004),\nwhere computer programs compete for computational resources. More recent work attempted to\nformulate necessary preconditions for open-ended evolution (Taylor, 2015; Soros & Stanley, 2014).\nCo-adaptation between agents and environments can also give rise to emergent complexity (Florensa\net al., 2017; Sukhbaatar et al., 2018; Wang et al., 2019). In the context of multi-agent RL, Tesauro\n(1995), Silver et al. (2016), OpenAI (2018), Jaderberg et al. (2019) and Vinyals et al. (2019) used\nself-play with deep RL techniques to achieve super-human performance in Backgammon, Go, Dota,\nCapture-the-Flag and Starcraft, respectively. Bansal et al. (2018) trained agents in a simulated 3D\nphysics environment to compete in various games such as sumo wrestling and soccer goal shooting.\nIn Liu et al. (2019), agents learn to manipulate a soccer ball in a 3D soccer environment and discover\n1Code can be found at github.com/openai/multi-agent-emergence-environments.\n3\nPublished as a conference paper at ICLR 2020\nemergent behaviors such as ball passing and interception. In addition, communication has also been\nshown to emerge from multi-agent RL (Sukhbaatar et al., 2016; Foerster et al., 2016; Lowe et al.,\n2017; Mordatch & Abbeel, 2018).\nIntrinsic motivation methods have been widely studied in the literature (Chentanez et al., 2005;\nSingh et al., 2010). One example is count-based exploration, where agents are incentivized to reach\ninfrequently visited states by maintaining state visitation counts (Strehl & Littman, 2008; Bellemare\net al., 2016; Tang et al., 2017) or density estimators (Ostrovski et al., 2017; Burda et al., 2019b).\nAnother paradigm are transition-based methods, in which agents are rewarded for high prediction\nerror in a learned forward or inverse dynamics model (Schmidhuber, 1991; Stadie et al., 2015;\nMohamed & Rezende, 2015; Houthooft et al., 2016; Achiam & Sastry, 2017; Pathak et al., 2017;\nBurda et al., 2019a; Haber et al., 2018). Jaques et al. (2019) consider multi-agent scenarios and\nadopt causal inﬂuence as a motivation for coordination. In our work, we utilize intrinsic motivation\nmethods as an alternative exploration baseline to multi-agent autocurricula. Similar comparisons\nhave also been made in Haber et al. (2018) and Leibo et al. (2019b).\nTool use is a hallmark of human and animal intelligence (Hunt, 1996; Shumaker et al., 2011); how-\never, learning tool use in RL settings can be a hard exploration problem when rewards are unaligned.\nFor example, in Forestier et al. (2017); Xie et al. (2019) a real-world robot learns to solve various\ntasks requiring tools. In Bapst et al. (2019), an agent solves construction tasks in a 2-D environment\nusing both model-based and model-free methods. Allen et al. (2019) uses a combination of human-\ndesigned priors and model-based policy optimization to solve a collection of physics-based puzzles\nrequiring tool use. However, in each of these works, agents were explicitly incentivized to inter-\nact with and use tools, whereas in our environment agents implicitly create this incentive through\nmulti-agent competition.\n3 H IDE AND SEEK\nAgents are tasked with competing in a two-team hide-and-seek game in a physics-based environ-\nment. The hiders are tasked with avoiding line of sight from the seekers, and the seekers are tasked\nwith keeping vision of the hiders. There are objects scattered throughout the environment that the\nagents can grab and also lock in place. There are also randomly generated immovable rooms and\nwalls that the agents must learn to navigate. Before the game of hide-and-seek begins, the hiders\nare given a preparation phase where the seekers are immobilized, giving the hiders a chance to run\naway or change their environment.\nThere are no explicit incentives for agents to interact with objects in the environment; the only\nsupervision given is through the hide-and-seek objective. Agents are given a team based reward;\nhiders are given a reward of 1 if all hiders are hidden and -1 if any hider is seen by a seeker. Seekers\nare given the opposite reward, -1 if all hiders are hidden and +1 otherwise. To conﬁne agent behavior\nto a reasonable space, agents are penalized with a reward of -10 if they go too far outside of the play\narea (outside an 18 meter square). An episode lasts 240 timesteps, the ﬁrst 40% of which are the\npreparation phase where all agents are given zero reward.\nWe simulate the environment in the MUJOCO physics engine (Todorov et al., 2012). The world is\npopulated with 1 to 3 hiders, 1 to 3 seekers, 3 to 9 movable boxes of which at least 3 are elongated, 2\nmovable ramps, and randomly generated static walls and rooms. We also experiment with a simpler,\nless randomized environment described in Appendix A.3. Agents observe the position, velocity, and\nsize (in the case of the randomly shaped boxes) of objects and other agents. If entities are not in\nline-of-sight of the agent or not in a 135 degree cone in front of the agent, then they are masked out\nin the policy. Agents also have 30 range sensors arrayed evenly around them, similar to a lidar. In\naddition, each agent observes its own team and other agents’ teams as well as how much time is left\nin the preparation phase.\nAgents are simulated as spherical objects and have 3 action types that can be chosen simultaneously\nat each time step. They may move by setting a discretized force along their xand yaxis and torque\naround their z-axis. They have a single binary action to grab objects, which binds the agent to the\nclosest object while the action is enabled. Agents may alsolock objects in place with a single binary\naction. Objects may be unlocked only by agents on the team of the agent who originally locked the\nobject. Agents may only grab or lock objects that are in front of them and within a small radius.\n4\nPublished as a conference paper at ICLR 2020\n4 P OLICY OPTIMIZATION\nAgents are trained using self-play, which acts as a natural curriculum as agents always play oppo-\nnents of an appropriate level.\nAgent policies are composed of two separate networks with different parameters – a policy network\nwhich produces an action distribution and a critic network which predicts the discounted future re-\nturns. Policies are optimized using Proximal Policy Optimization (PPO) (Schulman et al., 2017) and\nGeneralized Advantage Estimation (GAE) (Schulman et al., 2015), and training is performed using\nrapid (OpenAI, 2018), a large-scale distributed RL framework. We utilize decentralized execution\nand centralized training. At execution time, each agent acts given only its own observations and\nmemory state. At optimization time, we use a centralized omniscient value function for each agent,\nwhich has access to the full environment state without any information masked due to visibility,\nsimilar to Pinto et al. (2017); Lowe et al. (2017); Foerster et al. (2018).\nIn all reported experiments, agents share the same policy parameters but act and observe indepen-\ndently; however, we found using separate policy parameters per agent also achieved all six stages of\nemergence but at reduced sample efﬁciency.\nFully\nConnected\nFully\nConnected\nFully\nConnected\nx, v\nof self\nLIDAR\nx, v\nof other agents\nx, v, size\nof boxes\nx, v\nof ramps\nPolicy Architecture\nEntity Embeddings\nMasked Residual Self \nAttention across entities\nCircular 1D\nConvolution\nGenerate masks from frontal\nvision cone and line of sight\n# agents-1\n# boxes\n# ramps\n1\nFully\nConnected\nMasked\nAverage Pooling\nMovement\nAction\nGrabbing\nAction\nLocking\nAction\nLSTM\nFigure 2: Agent Policy Architecture. All entities are embedded with fully connected layers with\nshared weights across entity types, e.g. all box entities are encoded with the same function. The\npolicy is ego-centric so there is only one embedding of “self” and (#agents −1) embeddings of\nother agents. Embeddings are then concatenated and processed with masked residual self-attention\nand pooled into a ﬁxed sized vector (all of which admits a variable number of entities). x and v\nstand for state (position and orientation) and velocity.\nWe utilize entity-centric observations (D ˇzeroski et al., 2001; Diuk et al., 2008) and use attention\nmechanisms to capture object-level information (Duan et al., 2017; Zambaldi et al., 2018). As\nshown in Figure 2 we use a self-attention (Vaswani et al., 2017) based policy architecture over\nentities, which is permutation invariant and generalizes to varying number of entities. More details\ncan be found in Appendix B.\n5\nPublished as a conference paper at ICLR 2020\n0 100 200 300 400 500\n# Episodes (x )\n0.5\n1.0\n1.5\n2.0\n2.5Distance (m)\nObject Movement\nMax Ramp Movement\nMax Ramp Movement - Prep\nMax Box Movement\nMax Box Movement - Prep\n1 2 3 4 5 6\n0 100 200 300 400 500\n# Episodes (x )\n0\n1\n2\n3\n4Number Locked\nObject Locking\nNumber Ramp Locked\nNumber Ramp Locked - Prep\nNumber Box Locked\nNumber Box Locked - Prep\n1 2 3 4 5 6\nFigure 3: Environment speciﬁc statistics used to track stages of emergence in hide-and-seek. We\nplot the mean across 3 seeds with each individual seed shown in a dotted line, and we overlay the 6\nemergent phases of strategy: (1) Running and Chasing, (2) Fort Building, (3) Ramp Use, (4) Ramp\nDefense, (5) Box Surﬁng, (6) Surf Defense. We track the maximum movement of any box or ramp\nduring the game as well as during the preparation phase (denoted with “Prep”). We similarly track\nhow many objects of each type were locked at the end of the episode and preparation phase. As\nagents train, their interaction with the tools in their environment changes. For instance, as the agents\nlearn to build forts they move boxes and lock boxes much more during the preparation phase.\n5 A UTO -CURRICULA AND EMERGENT BEHAVIOR\nAs agents train against each other in hide-and-seek, as many as six distinct strategies emerge, each\nof which creates a previously non-existing pressure for agents to progress to the next stage. Note that\nthere are no direct incentives for agents to interact with objects or to explore, but rather the emergent\nstrategies are solely a result of the autocurriculum induced by multi-agent competition. Figure 1\nshows the progression of emergent strategies agents learn in our environment (see Appendix A.1 for\ntrajectory traces of each strategy2).\nInitially, hiders and seekers learn to crudely run away and chase. After approximately 25 million\nepisodes of hide-and-seek, the hiders learn to use the tools at their disposal and intentionally modify\ntheir environment. They begin to construct secure shelters in which to hide by moving many boxes\ntogether or against walls and locking them in place. After another 75 million episodes, the seekers\nalso learn rudimentary tool use; they learn to move and use ramps to jump over obstacles, allowing\nthem to enter the hiders’ shelter. 10 million episodes later, the hiders learn to defend against this\nstrategy; the hiders learn to bring the ramps to the edge of the play area and lock them in place,\nseemingly removing the only tool the seekers have at their disposal.\nWe originally believed defending against ramp use would be the last stage of emergence in this\nenvironment; however, we were surprised to ﬁnd that yet two more qualitatively new strategies\nemerged. After 380 million total episodes of training, the seekers learn to bring a box to the edge of\nthe play area where the hiders have locked the ramps. The seekers then use the ramp to move on top\nof the box and surf it to the hiders’ shelter. This is possible because the agents’ movement action\nallows them to apply a force on themselves regardless of whether they are on the ground or not; if\nthey do this while grabbing the box under them, the box will move with while they are on top of it.\nIn response, the hiders learn to lock all of the boxes in place before building their shelter.3\nIn all stages of strategy agents must learn to coordinate within their team. Similar to Liu et al. (2019),\nwe use team-based rewards such that agents are required to collaborate in order to succeed; however,\nin our work we require neither population-based training (Jaderberg et al., 2017) or evolved dense\nrewards (Jaderberg et al., 2019). Notably, hiders learn efﬁcient division of labor; for instance, when\nconstructing shelter they often separately bring their own box to the construction area. Furthermore,\n2See openai.com/blog/emergent-tool-use for sample videos.\n3Note that the discovery of a new skill does not necessarily correlate to the reward of a team changing. For\nexample, the hider reward still decreases even after the discovery of surf defense, which is likely because teams\nwith one or two hiders often do not have enough time to lock all of the boxes in play.\n6\nPublished as a conference paper at ICLR 2020\nas hiders attempt to defend against box surﬁng by locking boxes in place during the preparation\nphase, we ﬁnd that 2 and 3 hider teams are able to lock 25% and 36% more boxes, respectively, than\na single hider team, indicating that larger teams are able to divide labor and accomplish an objective\na smaller team could not.\nMany stages of emergent strategy can be mapped to behavioral shifts in the way agents interact with\nthe tools in their environment, similar to Leibo et al. (2017); Perolat et al. (2017). We therefore\ntrack basic statistics about the agents’ interaction with objects during training, shown in Figure 3.\nFor instance, as the hiders learn to build forts, they move and lock boxes much more during the\npreparation phase. Similarly, as the seekers learn to move and use ramps, the ramp movement in the\nmain phase of the game increases, and as they learn to “box surf” there is a slight increase in the\nbox movement during the main phase of the game. Finally, as the hiders learn to defend against this\nstrategy by locking all boxes in place, the number of locked boxes in the preparation phase increases.\n32k 64k 128k\nBatch Size\n0\n50\n100\n150\n200# Episodes (x106)\n167.4M Ep\n132.3M Ep\n155.2M Ep\nBatch Size\n0\n20\n40\n60\n80\n100\n120\n140\nTime (Hours)\n98.8 Hr\n34.0 Hr\n19.8 Hr\nFigure 4: Effect of Scale on Emergent Autocur-\nricula. Number of episodes (blue) and wall clock\ntime (orange) required to achieve stage 4 (ramp\ndefense) of the emergent skill progression pre-\nsented in Figure 1. Batch size denotes number of\nchunks, each of which consists of 10 contiguous\ntransitions (the truncation length for backpropaga-\ntion through time).\nWe found that scale plays a critical role in en-\nabling progression through the emergent au-\ntocurricula in hide-and-seek. The default\nmodel, which uses a batch size of 64,000 and\n1.6 million parameters, requires 132.3 million\nepisodes (31.7 billion frames) over 34 hours of\ntraining to reach stage 4 of the skill progres-\nsion, i.e. ramp defense. In Figure 4 we show\nthe effect of varying the batch size in our agents\nability to reach stage 4. We ﬁnd that larger\nbatch sizes lead to much quicker training time\nby virtue of reducing the number of required\noptimization steps, while only marginally af-\nfecting sample efﬁciency down to a batch size\nof 32,000; however, we found that experiments\nwith batch sizes of 16,000 and 8,000 never con-\nverged.\nWe ﬁnd the emergent autocurriculum to be\nfairly robust as long as we randomize the en-\nvironment during training. If randomization is\nreduced, we ﬁnd that fewer stages of the skill progression emerges, and at times less sophisticated\nstrategies emerge instead (e.g. hiders can learn to run away and use boxes as moveable shields.);\nsee Appendix A.2 for more details. In addition, we ﬁnd that design choices such as the minimum\nnumber of elongated boxes or giving each agent their own locking mechanism instead of a team\nbased locking mechanism can drastically increase the sample complexity. We also experimented\nwith adding additional objects and objectives to our hide-and-seek environment as well as with\nseveral game variants instead of hide-and-seek (see Appendix A.6). We ﬁnd that these alternative\nenvironments also lead to emergent tool use, providing further evidence that multi-agent interaction\nis a promising path towards self-supervised skill acquisition.\n6 E VALUATION\nIn the previous section we presented evidence that hide-and-seek induces a multi-agent autocur-\nriculum such that agents continuously learn new skills and strategies. As is the case with many\nunsupervised reinforcement learning methods, the objective being optimized does not directly in-\ncentivize the learned behavior, making evaluation of those behaviors nontrivial. Tracking reward is\nan insufﬁcient evaluation metric in multi-agent settings, as it can be ambiguous in indicating whether\nagents are improving evenly or have stagnated. Metrics like ELO (Elo, 1978) or Trueskill (Herbrich\net al., 2007) can more reliably measure whether performance is improving relative to previous pol-\nicy versions or other policies in a population; however, these metrics still do not give insight into\nwhether improved performance stems from new adaptations or improving previously learned skills.\nFinally, using environment speciﬁc statistics such as object movement (see Figure 3) can also be\nambiguous, e.g. the choice to track absolute movement does not illuminate which direction agents\nmoved, and designing sufﬁcient metrics will become difﬁcult and costly as environments scale.\n7\nPublished as a conference paper at ICLR 2020\n0 2 4 6 8 10\nSamples (x10 )\n3\n4\n5\n6Distance\nAgent Movement\n0 2 4 6 8 10\nSamples (x10 )\n0\n1\n2\n3\n4\n5Distance\nBox Movement\nCount-Based, 1 Agent ~ Box Position Count-Based, 1 Agent ~ Full Box Count-Based, 1-3 Agents ~ Full RND, 1-3 Agents ~ Full\nFigure 5: Behavioral Statistics from Count-Based Exploration Variants and Random Network Dis-\ntillation (RND) Across 3 Seeds. We compare net box movement and maximum agent movement\nbetween state representations for count-based exploration: Single agent, 2-D box location (blue);\nSingle agent, box location, rotation and velocity (green); 1-3 agents, full observation space (red).\nAlso shown is RND for 1-3 agents with full observation space (purple). We train all agents to\nconvergence as measured by their behavioral statistics.\nIn Section 6.1, we ﬁrst qualitatively compare the behaviors learned in hide-and-seek to those learned\nfrom intrinsic motivation, a common paradigm for unsupervised exploration and skill acquisition. In\nSection 6.2, we then propose a suite of domain-speciﬁc intelligence tests to quantitatively measure\nand compare agent capabilities.\n6.1 C OMPARISON TO INTRINSIC MOTIVATION\nIntrinsic motivation has become a popular paradigm for incentivizing unsupervised exploration and\nskill discovery, and there has been recent success in using intrinsic motivation to make progress in\nsparsely rewarded settings (Bellemare et al., 2016; Burda et al., 2019b). Because intrinsically mo-\ntivated agents are incentivized to explore uniformly, it is conceivable that they may not have mean-\ningful interactions with the environment (as with the “noisy-TV” problem (Burda et al., 2019a)).\nAs a proxy for comparing meaningful interaction in the environment, we measure agent and object\nmovement over the course of an episode.\nWe ﬁrst compare behaviors learned in hide-and-seek to a count-based exploration baseline (Strehl &\nLittman, 2008) with an object invariant state representation, which is computed in a similar way as\nin the policy architecture in Figure 2. Count-based objectives are the simplest form of state density\nbased incentives, where one explicitly keeps track of state visitation counts and rewards agents for\nreaching infrequently visited states (details can be found in Appendix D). In contrast to the original\nhide-and-seek environment where the initial locations of agents and objects are randomized, we\nrestrict the initial locations to a quarter of the game area to ensure that the intrinsically motivated\nagents receive additional rewards for exploring.\nWe ﬁnd that count-based exploration leads to the largest agent and box movement if the state repre-\nsentation only contains the 2-D location of boxes: the agent consistently interacts with objects and\nlearns to navigate. Yet, when using progressively higher-dimensional state representations, such as\nbox location, rotation and velocity or 1-3 agents with full observation space, agent movement and,\nin particular, box movement decrease substantially. This is a severe limitation because it indicates\nthat, when faced with highly complex environments, count-based exploration techniques require\nidentifying by hand the “interesting” dimensions in state space that are relevant for the behaviors\none would like the agents to discover. Conversely, multi-agent self-play does not need this degree of\nsupervision. We also train agents with random network distillation (RND) (Burda et al., 2019b), an\nintrinsic motivation method designed for high dimensional observation spaces, and ﬁnd it to perform\nslightly better than count-based exploration in the full state setting.\n6.2 T RANSFER AND FINE -TUNING AS EVALUATION\nWe propose to use transfer to a suite of domain-speciﬁc tasks in order to asses agent capabilities. To\nthis end, we have created 5 benchmark intelligence tests that include both supervised and reinforce-\n8\nPublished as a conference paper at ICLR 2020\n0 1 2 3 4\nSamples (x10 )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized reward\n0 1 2 3 4 5 6\nSamples (x10 )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized reward\nObject Counting Lock and Return Sequential Lock Blueprint Construction Shelter Construction\nPretrained in Hide-and-Seek Trained From Scratch Pretrained with Count-Based Intrinsic Motivation\n0.0 0.2 0.4 0.6 0.8 1.0\nSamples (x10 )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n0 2 4 6 8 10 12\nSamples (x10 )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized reward\n0 1 2 3\nSamples (x10 )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized reward\nFigure 6: Fine-tuning Results. We plot the mean normalized performance and 90% conﬁdence inter-\nval across 3 seeds smoothed with an exponential moving average, except for Blueprint Construction\nwhere we plot over 6 seeds due to higher training variance.\nment learning tasks. The tests use the same action space, observation space, and types of objects as\nin the hide-and-seek environment. We examine whether pretraining agents in our multi-agent envi-\nronment and then ﬁne-tuning them on the evaluation suite leads to faster convergence or improved\noverall performance compared to training from scratch or pretraining with count-based intrinsic mo-\ntivation. We ﬁnd that on 3 out of 5 tasks, agents pretrained in the hide-and-seek environment learn\nfaster and achieve a higher ﬁnal reward than both baselines.\nWe categorize the 5 intelligence tests into 2 domains: cognition and memory tasks, and manipulation\ntasks. We brieﬂy describe the tasks here; for the full task descriptions, see Appendix C. For all tasks,\nwe reinitialize the parameters of the ﬁnal dense layer and layernorm for both the policy and value\nnetworks.\nCognition and memory tasks:\nIn the Object Counting supervised task, we aim to measure whether the agents have a sense of object\npermanence; the agent is pinned to a location and watches as 6 boxes each randomly move to the\nright or left where they eventually become obscured by a wall. It is then asked to predict how many\nboxes have gone to each side for many timesteps after all boxes have disappeared. The agent’s policy\nparameters are frozen and we initialize a classiﬁcation head off of the LSTM hidden state. In the\nbaseline, the policy network has frozen random parameters and only the classiﬁcation head off of\nthe LSTM hidden state is trained.\nIn Lock and Return we aim to measure whether the agent can remember its original position while\nperforming a new task. The agent must navigate an environment with 6 random rooms and 1 box,\nlock the box, and return to its starting position.\nIn Sequential Lock there are 4 boxes randomly placed in 3 random rooms without doors but with\na ramp in each room. The agent needs to lock all the boxes in a particular order — a box is only\nlockable when it is locked in the correct order — which is unobserved by the agent. The agent must\ndiscover the order, remember the position and status of visited boxes, and use ramps to navigate\nbetween rooms in order to ﬁnish the task efﬁciently.\nManipulation tasks:With these tasks we aim to measure whether the agents have any latent skill\nor representation useful for manipulating objects.\nIn the Construction From Blueprint task, there are 8 cubic boxes in an open room and between 1\nand 4 target sites. The agent is tasked with placing a box on each target site.\nIn the Shelter Construction task there are 3 elongated boxes, 5 cubic boxes, and one static cylinder.\nThe agent is tasked with building a shelter around the cylinder.\n9\nPublished as a conference paper at ICLR 2020\nResults: In Figure 6 we show the performance on the suite of tasks for the hide-and-seek, count-\nbased, and trained from scratch policies across 3 seeds. The hide-and-seek pretrained policy per-\nforms slightly better than both the count-based and the randomly initialized baselines in Lock and\nReturn, Sequential Lock and Construction from Blueprint; however, it performs slightly worse than\nthe count-based baseline onObject Counting, and it achieves the same ﬁnal reward but learns slightly\nslower than the randomly initialized baseline on Shelter Construction.\nWe believe the cause for the mixed transfer results is rooted in agents learning skill representa-\ntions that are entangled and difﬁcult to ﬁne-tune. We conjecture that tasks where hide-and-seek\npretraining outperforms the baseline are due to reuse of learned feature representations, whereas\nbetter-than-baseline transfer on the remaining tasks would require reuse of learned skills, which is\nmuch more difﬁcult. This evaluation metric highlights the need for developing techniques to reuse\nskills effectively from a policy trained in one environment to another. In addition, as future en-\nvironments become more diverse and agents must use skills in more contexts, we may see more\ngeneralizable skill representations and more signiﬁcant signal in this evaluation approach.\nIn Appendix A.5 we further evaluate policies sampled during each phase of emergent strategy on\nthe suite of targeted intelligence tasks, by which we can gain intuition as to whether the capabilities\nwe measure improve with training, are transient and accentuated during speciﬁc phases, or gener-\nally uncorrelated to progressing through the autocurriculum. Noteably, we ﬁnd the agent’s memory\nimproves through training as indicated by performance in the navigation tasks; however, perfor-\nmance in the manipulation tasks is uncorrelated, and performance in object counting changes seems\ntransient with respect to source hide-and-seek performance.\n7 D ISCUSSION AND FUTURE WORK\nWe have demonstrated that simple game rules, multi-agent competition, and standard reinforcement\nlearning algorithms at scale can induce agents to learn complex strategies and skills. We observed\nemergence of as many as six distinct rounds of strategy and counter-strategy, suggesting that multi-\nagent self-play with simple game rules in sufﬁciently complex environments could lead to open-\nended growth in complexity. We then proposed to use transfer as a method to evaluate learning\nprogress in open-ended environments and introduced a suite of targeted intelligence tests with which\nto compare agents in our domain.\nOur results with hide-and-seek should be viewed as a proof of concept showing that multi-agent\nautocurricula can lead to physically grounded and human-relevant behavior. We acknowledge that\nthe strategy space in this environment is inherently bounded and likely will not surpass the six\nmodes presented as is; however, because it is built in a high-ﬁdelity physics simulator it is physically\ngrounded and very extensible. In order to support further research in multi-agent autocurricula, we\nare open-sourcing our environment code.\nHide-and-seek agents require an enormous amount of experience to progress through the six stages\nof emergence, likely because the reward functions are not directly aligned with the resulting behav-\nior. While we have found that standard reinforcement learning algorithms are sufﬁcient, reducing\nsample complexity in these systems will be an important line of future research. Better policy learn-\ning algorithms or policy architectures are orthogonal to our work and could be used to improve\nsample efﬁciency and performance on transfer evaluation metrics.\nWe also found that agents were very skilled at exploiting small inaccuracies in the design of the\nenvironment, such as seekers surﬁng on boxes without touching the ground, hiders running away\nfrom the environment while shielding themselves with boxes, or agents exploiting inaccuracies of\nthe physics simulations to their advantage. Investigating methods to generate environments with-\nout these unwanted behaviors is another import direction of future research (Amodei et al., 2016;\nLehman et al., 2018).\nACKNOWLEDGMENTS\nWe thank Pieter Abbeel, Rewon Child, Jeff Clune, Harri Edwards, Jessica Hamrick, Joel Liebo,\nJohn Schulman and Peter Welinder for their insightful comments on this manuscript. We also thank\nAlex Ray for writing parts of our open sourced code.\n10\nPublished as a conference paper at ICLR 2020\nREFERENCES\nJoshua Achiam and Shankar Sastry. Surprise-based intrinsic motivation for deep reinforcement\nlearning. arXiv preprint arXiv:1703.01732, 2017.\nKelsey R Allen, Kevin A Smith, and Joshua B Tenenbaum. The tools challenge: Rapid trial-and-\nerror learning in physical problem solving. arXiv preprint arXiv:1907.09620, 2019.\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´e. Con-\ncrete problems in AI safety. arXiv preprint arXiv:1606.06565, 2016.\nMarcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew, Jakub Pa-\nchocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray, et al. Learning dexterous\nin-hand manipulation. arXiv preprint arXiv:1808.00177, 2018.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\nRene Baillargeon and Susan Carey. Core cognition and beyond: The acquisition of physical and\nnumerical knowledge. In In S. Pauen (Ed.), Early Childhood Development and Later Outcome ,\npp. 33–65. University Press, 2012.\nTrapit Bansal, Jakub Pachocki, Szymon Sidor, Ilya Sutskever, and Igor Mordatch. Emergent com-\nplexity via multi-agent competition. In International Conference on Learning Representations ,\n2018.\nVictor Bapst, Alvaro Sanchez-Gonzalez, Carl Doersch, Kimberly Stachenfeld, Pushmeet Kohli, Pe-\nter Battaglia, and Jessica Hamrick. Structured agents for physical construction. In International\nConference on Machine Learning, pp. 464–474, 2019.\nMarc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.\nUnifying count-based exploration and intrinsic motivation. In Advances in Neural Information\nProcessing Systems, pp. 1471–1479, 2016.\nYuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A. Efros.\nLarge-scale study of curiosity-driven learning. In International Conference on Learning Repre-\nsentations, 2019a.\nYuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network\ndistillation. In International Conference on Learning Representations, 2019b.\nAD Channon, RI Damper, et al. Perpetuating evolutionary emergence. FROM ANIMALS TO ANI-\nMATS 5, pp. 534–539, 1998.\nNuttapong Chentanez, Andrew G Barto, and Satinder P Singh. Intrinsically motivated reinforcement\nlearning. In Advances in neural information processing systems, pp. 1281–1288, 2005.\nRichard Dawkins and John Richard Krebs. Arms races between and within species. Proceedings of\nthe Royal Society of London. Series B. Biological Sciences, 205(1161):489–511, 1979.\nCarlos Diuk, Andre Cohen, and Michael L Littman. An object-oriented representation for efﬁcient\nreinforcement learning. In Proceedings of the 25th international conference on Machine learning,\npp. 240–247. ACM, 2008.\nYan Duan, Marcin Andrychowicz, Bradly Stadie, Jonathan Ho, Jonas Schneider, Ilya Sutskever,\nPieter Abbeel, and Wojciech Zaremba. One-shot imitation learning. In Advances in neural infor-\nmation processing systems, pp. 1087–1098, 2017.\nSaˇso D ˇzeroski, Luc De Raedt, and Kurt Driessens. Relational reinforcement learning. Machine\nlearning, 43(1-2):7–52, 2001.\nA.E. Elo. The rating of chessplayers, past and present . Arco Pub., 1978. ISBN 9780668047210.\nURL https://books.google.com/books?id=8pMnAQAAMAAJ.\n11\nPublished as a conference paper at ICLR 2020\nCarlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and Pieter Abbeel. Reverse cur-\nriculum generation for reinforcement learning. In Conference on Robot Learning, pp. 482–495,\n2017.\nJakob Foerster, Ioannis Alexandros Assael, Nando de Freitas, and Shimon Whiteson. Learning to\ncommunicate with deep multi-agent reinforcement learning. In Advances in Neural Information\nProcessing Systems, pp. 2137–2145, 2016.\nJakob N Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.\nCounterfactual multi-agent policy gradients. In Thirty-Second AAAI Conference on Artiﬁcial\nIntelligence, 2018.\nS´ebastien Forestier, Yoan Mollard, and Pierre-Yves Oudeyer. Intrinsically motivated goal explo-\nration processes with automatic curriculum learning. arXiv preprint arXiv:1708.02190, 2017.\nNick Haber, Damian Mrowca, Stephanie Wang, Li F Fei-Fei, and Daniel L Yamins. Learning to play\nwith intrinsically-motivated, self-aware agents. In Advances in Neural Information Processing\nSystems, pp. 8388–8399, 2018.\nNicolas Heess, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, Tom Erez,\nZiyu Wang, SM Eslami, Martin Riedmiller, et al. Emergence of locomotion behaviours in rich\nenvironments. arXiv preprint arXiv:1707.02286, 2017.\nRalf Herbrich, Tom Minka, and Thore Graepel. Trueskill: a bayesian skill rating system. In Ad-\nvances in neural information processing systems, pp. 569–576, 2007.\nSepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n1735–1780, 1997.\nRein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. Vime:\nVariational information maximizing exploration. In Advances in Neural Information Processing\nSystems, pp. 1109–1117, 2016.\nGavin R Hunt. Manufacture and use of hook-tools by new caledonian crows. Nature, 379(6562):\n249, 1996.\nMax Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech M Czarnecki, Jeff Donahue, Ali\nRazavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan, et al. Population based train-\ning of neural networks. arXiv preprint arXiv:1711.09846, 2017.\nMax Jaderberg, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia\nCasta˜neda, Charles Beattie, Neil C. Rabinowitz, Ari S. Morcos, Avraham Ruderman, Nico-\nlas Sonnerat, Tim Green, Louise Deason, Joel Z. Leibo, David Silver, Demis Hassabis, Ko-\nray Kavukcuoglu, and Thore Graepel. Human-level performance in 3d multiplayer games with\npopulation-based reinforcement learning. Science, 364(6443):859–865, 2019. ISSN 0036-8075.\ndoi: 10.1126/science.aau6249. URL https://science.sciencemag.org/content/\n364/6443/859.\nNatasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, Dj Strouse,\nJoel Z Leibo, and Nando De Freitas. Social inﬂuence as intrinsic motivation for multi-agent deep\nreinforcement learning. In International Conference on Machine Learning, pp. 3040–3049, 2019.\nJoel Lehman, Jeff Clune, Dusan Misevic, Christoph Adami, Lee Altenberg, Julie Beaulieu, Peter J\nBentley, Samuel Bernard, Guillaume Beslon, David M Bryson, et al. The surprising creativity of\ndigital evolution: A collection of anecdotes from the evolutionary computation and artiﬁcial life\nresearch communities. arXiv preprint arXiv:1803.03453, 2018.\nJoel Z Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore Graepel. Multi-agent\nreinforcement learning in sequential social dilemmas. In Proceedings of the 16th Conference\non Autonomous Agents and MultiAgent Systems , pp. 464–473. International Foundation for Au-\ntonomous Agents and Multiagent Systems, 2017.\n12\nPublished as a conference paper at ICLR 2020\nJoel Z Leibo, Edward Hughes, Marc Lanctot, and Thore Graepel. Autocurricula and the emergence\nof innovation from social interaction: A manifesto for multi-agent intelligence research. arXiv\npreprint arXiv:1903.00742, 2019a.\nJoel Z Leibo, Julien Perolat, Edward Hughes, Steven Wheelwright, Adam H Marblestone, Edgar\nDu´e˜nez-Guzm´an, Peter Sunehag, Iain Dunning, and Thore Graepel. Malthusian reinforcement\nlearning. In Proceedings of the 18th International Conference on Autonomous Agents and Multi-\nAgent Systems, pp. 1099–1107. International Foundation for Autonomous Agents and Multiagent\nSystems, 2019b.\nSiqi Liu, Guy Lever, Nicholas Heess, Josh Merel, Saran Tunyasuvunakool, and Thore Graepel.\nEmergent coordination through competition. In International Conference on Learning Represen-\ntations, 2019.\nRyan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actor-\ncritic for mixed cooperative-competitive environments. In Advances in Neural Information Pro-\ncessing Systems, pp. 6379–6390, 2017.\nShakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsi-\ncally motivated reinforcement learning. In Advances in neural information processing systems ,\npp. 2125–2133, 2015.\nIgor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi-agent\npopulations. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.\nCharles Ofria and Claus O Wilke. Avida: A software platform for research in computational evolu-\ntionary biology. Artiﬁcial life, 10(2):191–229, 2004.\nOpenAI. OpenAI Five. https://blog.openai.com/openai-five/, 2018.\nGeorg Ostrovski, Marc G Bellemare, A ¨aron van den Oord, and R ´emi Munos. Count-based ex-\nploration with neural density models. In Proceedings of the 34th International Conference on\nMachine Learning-Volume 70, pp. 2721–2730. JMLR. org, 2017.\nJan Paredis. Coevolutionary computation. Artiﬁcial life, 2(4):355–375, 1995.\nDeepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration\nby self-supervised prediction. In International Conference on Machine Learning, pp. 2778–2787,\n2017.\nJulien Perolat, Joel Z Leibo, Vinicius Zambaldi, Charles Beattie, Karl Tuyls, and Thore Graepel. A\nmulti-agent reinforcement learning model of common-pool resource appropriation. In Advances\nin Neural Information Processing Systems, pp. 3643–3652, 2017.\nLerrel Pinto, Marcin Andrychowicz, Peter Welinder, Wojciech Zaremba, and Pieter Abbeel. Asym-\nmetric actor critic for image-based robot learning. arXiv preprint arXiv:1710.06542, 2017.\nJordan B Pollack, Alan D Blair, and Mark Land. Coevolution of a backgammon player. InArtiﬁcial\nLife V: Proc. of the Fifth Int. Workshop on the Synthesis and Simulation of Living Systems , pp.\n92–98. Cambridge, MA: The MIT Press, 1997.\nAravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel\nTodorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement\nlearning and demonstrations. arXiv preprint arXiv:1709.10087, 2017.\nThomas S Ray. Evolution, ecology and optimization of digital organisms. Technical report, Citeseer,\n1992.\nChristopher D Rosin and Richard K Belew. Methods for competitive co-evolution: Finding oppo-\nnents worth beating. In ICGA, pp. 373–381, 1995.\nJ¨urgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building neu-\nral controllers. In Proc. of the international conference on simulation of adaptive behavior: From\nanimals to animats, pp. 222–227, 1991.\n13\nPublished as a conference paper at ICLR 2020\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-\ndimensional continuous control using generalized advantage estimation. arXiv preprint\narXiv:1506.02438, 2015.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nRobert W Shumaker, Kristina R Walkup, and Benjamin B Beck. Animal tool behavior: the use and\nmanufacture of tools by animals. JHU Press, 2011.\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,\nJulian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering\nthe game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.\nDavid Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,\nMarc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi\nby self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815,\n2017.\nKarl Sims. Evolving virtual creatures. In Proceedings of the 21st annual conference on Computer\ngraphics and interactive techniques, pp. 15–22. ACM, 1994a.\nKarl Sims. Evolving 3d morphology and behavior by competition. Artiﬁcial life, 1(4):353–372,\n1994b.\nSatinder Singh, Richard L Lewis, Andrew G Barto, and Jonathan Sorg. Intrinsically motivated\nreinforcement learning: An evolutionary perspective. IEEE Transactions on Autonomous Mental\nDevelopment, 2(2):70–82, 2010.\nL Soros and Kenneth Stanley. Identifying necessary conditions for open-ended evolution through\nthe artiﬁcial life world of chromaria. In Artiﬁcial Life Conference Proceedings 14, pp. 793–800.\nMIT Press, 2014.\nBradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement\nlearning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.\nKenneth O Stanley and Risto Miikkulainen. Competitive coevolution through evolutionary com-\nplexiﬁcation. Journal of artiﬁcial intelligence research, 21:63–100, 2004.\nAlexander L Strehl and Michael L Littman. An analysis of model-based interval estimation for\nmarkov decision processes. Journal of Computer and System Sciences, 74(8):1309–1331, 2008.\nSainbayar Sukhbaatar, Rob Fergus, et al. Learning multiagent communication with backpropaga-\ntion. In Advances in Neural Information Processing Systems, pp. 2244–2252, 2016.\nSainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur Szlam, and Rob Fer-\ngus. Intrinsic motivation and automatic curricula via asymmetric self-play. In International\nConference on Learning Representations, 2018.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\nHaoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan Duan, John Schulman,\nFilip DeTurck, and Pieter Abbeel. # exploration: A study of count-based exploration for deep\nreinforcement learning. In Advances in neural information processing systems , pp. 2753–2762,\n2017.\nTim Taylor. Requirements for open-ended evolution in natural and artiﬁcial systems.arXiv preprint\narXiv:1507.07403, 2015.\nGerald Tesauro. Temporal difference learning and td-gammon.Communications of the ACM, 38(3):\n58–68, 1995.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\nIn 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems , pp. 5026–5033.\nIEEE, 2012.\n14\nPublished as a conference paper at ICLR 2020\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998–6008, 2017.\nOriol Vinyals, Igor Babuschkin, Junyoung Chung, Michael Mathieu, Max Jaderberg, et al. AlphaS-\ntar: Mastering the real-time strategy game StarCraft II. https://deepmind.com/blog/\nalphastar-mastering-real-time-strategy-game-starcraft-ii/ , 2019.\nRui Wang, Joel Lehman, Jeff Clune, and Kenneth O Stanley. Paired open-ended trailblazer (poet):\nEndlessly generating increasingly complex and diverse learning environments and their solutions.\narXiv preprint arXiv:1901.01753, 2019.\nAnnie Xie, Frederik Ebert, Sergey Levine, and Chelsea Finn. Improvisation through physical un-\nderstanding: Using novel objects as tools with visual foresight. arXiv preprint arXiv:1904.05538,\n2019.\nLarry Yaeger. Computational genetics, physiology, metabolism, neural systems, learning, vision,\nand behavior or poly world: Life in a new context. In SANTA FE INSTITUTE STUDIES\nIN THE SCIENCES OF COMPLEXITY-PROCEEDINGS VOLUME- , volume 17, pp. 263–263.\nADDISON-WESLEY PUBLISHING CO, 1994.\nVinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl\nTuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, et al. Relational deep reinforcement\nlearning. arXiv preprint arXiv:1806.01830, 2018.\n15\nPublished as a conference paper at ICLR 2020\nAppendix\nTable of Contents\nA Further Emergence Results 17\nA.1 Trajectory Traces From Each Stage of Emergent Strategy . . . . . . . . . . . . . 17\nA.2 Dependence of Skill Emergence on Randomness in the Training Distribution of\nEnvironments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\nA.3 Quadrant Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nA.4 Further Ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nA.5 Evaluating Agents at Different Phases of Emergence . . . . . . . . . . . . . . . 19\nA.6 Alternative Games to Hide-and-Seek with Secondary Objectives . . . . . . . . . 20\nA.7 Zero-shot generalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nB Optimization Details 23\nB.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nB.2 Proximal Policy Optimization (PPO) . . . . . . . . . . . . . . . . . . . . . . . 23\nB.3 Generalized advantage estimation . . . . . . . . . . . . . . . . . . . . . . . . . 23\nB.4 Normalization of observations, advantage targets and value function targets . . . 24\nB.5 Optimization setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\nB.6 Optimization hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\nB.7 Policy architecture details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\nC Intelligence Test Suite Details 25\nC.1 Cognition and memory task . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\nC.2 Manipulation task . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\nD Intrinsic Motivation Methods 26\nD.1 Counted-Based Exploration . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\nD.2 Random Network Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\nD.3 Fine-tuning From Intrinsic Motivation Variants . . . . . . . . . . . . . . . . . . 27\n16\nPublished as a conference paper at ICLR 2020\nA F URTHER EMERGENCE RESULTS\nA.1 T RAJECTORY TRACES FROM EACH STAGE OF EMERGENT STRATEGY\nFigure A.1: Trajectory Traces From Each Stage of Emergent Strategy. Rows correspond to different\nstrategies, columns to characteristic snapshots in chronological order within an episode that show-\ncase the strategy. Running and chasing: Hiders (blue) try to hide behind walls and moveable objects,\nand seekers (red) chase and search for hiders. Fort building: During the preparation phase (ﬁrst 3\nsnapshots), hiders build a fort out of 3 boxes and hide inside. Seekers are not able to overcome this\nbarrier during the remainder of the trial. Ramp use: During the preparation phase, hiders build a\nfort out of 2 boxes and a wall (1st and 2nd snapshot). After the preparation phase, a seeker grabs a\nramp (3rd snapshot), drags it towards the fort (4th snapshot) and uses it to enter the fort (5th snap-\nshot). Ramp defense: Hiders prevent ramp use by dragging ramps away and locking them in place\nduring the preparation phase (1st, 2nd and 3rd snapshot). They then build a fort in safe distance\nfrom the ramps (4th snapshot). After the preparation phase, seekers are not able to enter the fort\n(5th snapshot). Box surﬁng: A single hider locks both ramps in place and hides inside a fort built\nfrom 2 boxes and an interior wall during the preparation phase (1st and 2nd snapshot). The seeker\ngrabs a square box (3rd snapshot) and drags it next to one of the locked ramps to jump on top (4th\nsnapshot). It then “surfs” on the box to the edge of the fort and jumps inside (5th snapshot). Surf\ndefense: During the preparation phase, hiders lock all ramps and boxes in place (1st, 2nd and 3rd\nsnapshot) and construct a fort out of 3 boxes (4th snapshot). After the preparation phase, seekers are\nnot able to enter the fort (5th snapshot).\nA.2 D EPENDENCE OF SKILL EMERGENCE ON RANDOMNESS IN THE TRAINING\nDISTRIBUTION OF ENVIRONMENTS\nDiscovering tool use is a difﬁcult exploration problem, as only a very speciﬁc set of actions will\nlead to an increase in reward. During training, we ﬁnd that randomization over many components\nof our environment, such as the number of agents in each team, the number of boxes, the shape of\nboxes (square or elongated), the initial location of agents and objects and the presence and location\nof interior and exterior walls helps emergence, despite leading to a broader training distribution. As\nwe successively reduce the amount of randomization, we ﬁnd that fewer stages of skill progression\nemerges, and with at times less sophisticated strategies (e.g. hiders learn to run away and use boxes\nas moveable shields), Table A.1.\n17\nPublished as a conference paper at ICLR 2020\nteam size # boxes box shape initial\nlocation walls emergence\n1-3 3-9 random random random 6 stages\n1 3-9 random random random 6 stages\n1 7 ﬁxed random random 6 stages\n1-3 3-9 random random no walls 4 stages\n1 3-9 random random no walls 2 stages∗\n1 7 ﬁxed random no walls 2 stages∗\n1 7 ﬁxed ﬁxed no walls 2 stages\n∗Hiders run away and use boxes as movable shields.\nTable A.1: Number of stages of emergence for different levels of environmental randomization\n(batch size is held constant).\nFigure A.2: Sample Trajectory Traces From Each Stage of Emergent Strategy in the Quadrant En-\nvironment. Rows correspond to different strategies, columns to characteristic snapshots in chrono-\nlogical order within an episode that showcase the strategy. Running and chasing: Hiders (blue) try\nto hide behind walls and moveable objects, seekers (red) chase and search for hiders. Fort building:\nDuring the preparation phase (ﬁrst 3 snapshots), hiders push the two boxes into the two doors of\nthe lower right room and lock them in place. Seekers are not able to overcome this barrier during\nthe remainder of the trial. Ramp use: During the preparation phase, hiders secure the room using\nthe two boxes (1st and 2nd snapshot). After the preparation phase, a seeker grabs the ramp (3rd\nsnapshot), drags it towards the walls of the room (4th snapshot) and both seekers use the ramp to\nenter the room (5th snapshot). Ramp defense: Hiders prevent ramp use by grabbing the ramp during\nthe preparation phase (2nd snapshot), pulling it inside the room (3rd snapshot), and then secure the\ndoors using the 2 blocks (4th snapshot). After the preparation phase, the seekers are not able to enter\nthe room (5th snapshot). Also, note that while one hider brings the ramp into the room, the other\nhider drags a box out of corner and then passes it to its team mate (3rd and 4th snapshot), evidence\nof sophisticated coordination between hiders.\nA.3 Q UADRANT ENVIRONMENT\nAs well as the defaultrandom roomsenvironment described above, we also train and evaluate agents\nin the much simpler quadrant environment (Figure A.2) for illustration purposes. This environment\nconsists of a room in the lower right corner that is connected by 1-2 doors to the remainder of the\nenvironment. Unlike in the main environment, the interior walls are ﬁxed and only the location\nof the doors vary from episode to episode. Hiders are spawned randomly in the environment, but\nseekers are only spawned outside the room. Two cubic boxes are spawned inside the room and one\nramp is spawned inside or outside the room. Both hiders and seekers receive a negative reward if\nthey leave the environment or if they bring an object outside the environment. Also, in contrast to\nthe main environment, only boxes, but not ramps can be locked in place.\n18\nPublished as a conference paper at ICLR 2020\nThe quadrant environment gives rise to a similar skill progression as in the main environment (Figure\nA.2): basic running and chasing, then fort building by hiders, ramp use by seekers, and ramp defense\nby hiders. Hiders create shelter by pushing boxes into the doors of the room in the lower right corner,\npreventing seekers from entering. Seekers drag ramps to the exterior walls of the room and then use\nthem to jump inside. As ramps are not lockable in this environment, hiders instead defend against\nramp use by pulling the ramp inside the room before securing the wall opening with boxes.\nA.4 F URTHER ABLATIONS\nOmniscient V Masked V\n0\n20\n40\n60\n80# Episodes (x106) \n76.6M Ep\n89.2M Ep\nOmniscient Value Function (Emergence Phase 3)\n0\n5\n10\n15\n20\nTime (Hours)\n19.7 Hr\n22.1 Hr\nSelf Attention Pooling\n0\n40\n80\n120\n160# Episodes (x106) \n132.3M Ep\n145.4M Ep\nEntity Aggregation\n0\n10\n20\n30\nTime (Hours)\n34.0 Hr\n30.8 Hr\nFigure A.3: Effect of Omniscient Value Function and Pooling Architectures on Emergent Autocur-\nricula. In blue we show the number of episodes required and in orange the wall clock time required\nto achieve stage 4 (ramp defense) of the emergent skill progression presented in Figure 1 for varying\nbatch and model sizes. Note that in our ablation comparing omniscient and masked value functions,\nthe experiment with a masked value function never reached stage 4 in the allotted time, so here we\ncompare timing to stage 3.\nIn Figure A.3 we compare the performance between a masked and omniscient value function as\nwell a purely pooling architecture versus self-attention. We ﬁnd that using an omniscient value\nfunction, meaning that the value function has access to the full state of the unobscured environment,\nis critical to progressing through the emergent autocurricula at the given scale. We found that with\nthe same compute budget, training with a masked value function never progressed past stage 3\n(ramp usage). We further found that our self-attention architecture increases sample efﬁciency as\ncompared to an architecture that only embeds and then pools entities together with a similar number\nof parameters. However, because self-attention requires more compute despite having the same\nnumber of parameters, the wall-clock time to convergence is slightly slower.\nA.5 E VALUATING AGENTS AT DIFFERENT PHASES OF EMERGENCE\n0 2 4 6 8 10 12\nSamples (x10 )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized reward\n0 1 2 3\nSamples (x10 )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized reward\n0.0 0.2 0.4 0.6 0.8 1.0\nSamples (x10 )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n0 1 2 3 4\nSamples (x10 )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized reward\n0 1 2 3\nSamples (x10 )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized reward\nPhase 1 Phase 2 Phase 3 Phase 4 Phase 5 Phase 6 Baseline\nObject Counting Lock and Return Sequential Lock Blueprint Construction Shelter Construction\nFigure A.4: Fine-tuning From Different Phases in the Emergent Autocurriculum. We plot the mean\nperformance on the transfer suite and 90% conﬁdence interval across 3 seeds and smooth perfor-\nmance and conﬁdence intervals with an exponential moving average. We show the ﬁne-tuning per-\nformance for a policy sampled at each of the six phases of emergent strategy (see Figure 1).\nIn Figure A.4 we evaluate policies sampled during each phase of emergent strategy on the suite of\ntargeted intelligence tasks, by which we can gain intuition as to whether the capabilities we measure\nimprove with training, are transient and accentuated during speciﬁc phases, or generally uncorre-\nlated to progressing through the autocurriculum. We ﬁnd that the hide-and-seek agent improves on\nthe navigation and memory tasks as it progresses; notably on Lock and Return, the performance\nmonotonically increases with emergence phase, and the policy from the phase 6 performs 20% bet-\nter than the policy from phase 1. However, performance on Object Counting is transient; during\n19\nPublished as a conference paper at ICLR 2020\nFigure A.5: Example trajectory in hide-and-seek environment with additional food reward. During\nthe preparation phase, hiders lock the ramps at the boundary of the play area (2nd snapshot) and\nconstruct a fort around the food reward (3rd and 4th snapshot). After the preparation phase, hiders\ncan eat food and receive the additional food reward, because they are hidden from seekers inside the\nfort.\nphase 1 the hide-and-seek agent performs extremely well, much better than all baselines and other\nphases but loses this ability in later stages. Finally, we ﬁnd that performance on the manipulation\ntasks is relatively uncorrelated to the phases of emergence, and surprisingly the policy transferred\nfrom phase 1, the phase before any tool use emerges, performs comparably well to other phases.\nA.6 A LTERNATIVE GAMES TO HIDE -AND -SEEK WITH SECONDARY OBJECTIVES\nA.6.1 H IDE -AND -SEEK WITH FOOD REWARD\nIn the main hide-and-seek environment, hiders build forts at locations that would give them the best\nchance of staying hidden from seekers. To test whether hiders could be incentivized by a secondary\nobjective to adapt the location of forts, we add additional food rewards and test whether hiders\nwould bias the location of their forts towards the location of the food. In this environment, hiders\ncan eat food and receive food rewards only under the following conditions: after the preparation\nphase, when all hiders are hidden from seekers and when the food is sufﬁciently close and visible to\nthem. Therefore, just chasing after food would not be an effective strategy for hiders, because hiders\nreceive neither hide-and-seek nor food rewards if one or more hiders are seen by a seeker. Instead,\nhiders are incentivized to build forts around the location of food and then eat the food while being\nunobserved by seekers (ﬁgure A.5).\nFood is distributed in the form of 5 food pellets that are spawned close to each other in a rectangular\narea in the center of the environment whose side length is 1/4 of the room size. Each food pellet can\nprovide a positive food reward of +1 for each time step. Food rewards are shared between hiders,\nirrespective of which hider eats a food item. Hiders (as well as seekers) can observe the locations\nof food pellets as separate entities; if food pellets are obstructed or outside the ﬁeld of view they are\nmasked out like other type of objects.\nAs shown in ﬁgure A.6, this environment gives rise to four levels of skill progression, similar to\nthe one of the main environment: basic running and chasing, then fort building by hiders, ramp use\nby seekers, and ramp defense by hiders. Moreover, hiders consume food, and food consumption is\nhighly correlated with their ability to construct stable forts; food consumption decreases during the\ninitial phase as seekers get better at chasing and therefore prevent hiders from eating food. Food\nconsumption then increases again as hiders learn to construct forts and shield themselves from the\nview of seekers, but plateaus once seekers learn to use ramps. Finally, food consumption rises again\nas hiders get better at defending against ramp use.\nA.6.2 H IDE -AND -SEEK WITH DYNAMIC FOOD\nIn the food variant introduce in Sec. A.6.1, the location of food pellets is ﬁxed throughout the\nepisode. Here we consider a dynamic food variant such that a food pellet will be eaten up, i.e.,\ndisappear, when a hider is close to it, and then a new food pellet will show up in a different location\nbut still within the center region. More precisely, the game area is simply an empty room without\noutside walls containing 2 seekers, 3 hiders, 1 dynamic food and 8 elongated boxes. The food will\nbe always located within a square in the center of the environment with side length 1/5 of the game\narea size. We inherit the same reward and policy structures from the previous game.\nIn this game, merely building a center fort is not sufﬁcient for obtaining the highest reward since\nthe food might disappear and respawn outside the fort. The agent must ensure that the fort is large\n20\nPublished as a conference paper at ICLR 2020\n0 100 200 300 400\n# Episodes (x106)\n−100\n0\n100\n200\n300\n400\nReward\nReward\nSeekersHiders\n0 100 200 300 400\n# Episodes (x106)\n0\n100\n200\n300\n400\n500\n600\n700Food units\nFood consumption\n0 100 200 300 400\n# Episodes (x106)\n0.5\n1.0\n1.5\n2.0\n2.5Distance (m)\nObject Movement\nMax Ramp Movement\nMax Ramp Movement - Prep\nMax Box Movement\nMax Box Movement - Prep\n0 100 200 300 400\n# Episodes (x106)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0Number locked\nObject Locking\nNumber Ramp Locked\nNumber Ramp Locked - Prep\nNumber Box Locked\nNumber Box Locked - Prep\nFigure A.6: Reward and environment speciﬁc statistics during emergence in hide-and-seek with\nsecondary food rewards for hiders.\nenough such that all the possible spawning positions of the food will be inside the fort. Such a\nbehavior does emerge after training for around 4.5 ×1010 samples.\nWe also experimented on variants where the food spawning region has different side length. When\nthe side length is reduced to 1/6 of the game area, the same behavior emerges faster taking1.5×1010\nsamples. However, when the side length of the dynamic food region is increased to 1/4 of the\ngame area, hiders converge to a policy that ignores the food and only builds a small fort to protect\nthemselves.\nA.6.3 F OOD PROTECTION GAME\nIn the previous two variants, we introduce extra food reward to the hider in addition to the original\nhide-and-seek reward for promoting more goal-oriented behavior. Now we consider a different\ngame rule such that the competition between hiders and seekers only depends on the food collecting\nreward and show that this rule can also lead to tool use and complex behavior.\nWe consider an empty game area surrounded by walls that contains 50 food pellets randomly dis-\ntributed in a center square of size 2/3 of the game environment. There are 3 hiders, 2 seekers and\n7 elongated boxes in the game. The only goal for seekers is to collect food. Once a food pellet is\ncollected by any of the seeker, a +3 reward will be given to all the seekers and then the food will\ndisappear permanently from the game. The goal for the hider is to protect the food from seekers\nand their reward is simply the negative value of seekers. Each episode consists of 200 time steps.\nThe preparation period, in which only hiders can move, extends to the ﬁrst 100 time steps. The last\n60 steps of the game corresponds to a food collecting period, during which the hiders cannot move.\nAdditionally, after the preparation phase we also add a -1 “boundary penalty” for hiders when they\nare too close to the wall to ensure they stay within the food region.\nOur initial motivation for this task was to promote hiders to learn to construct complex and large fort\nstructures with a more direct competition pressure. For example, we expected that the agents would\nlearn to build a large fort that would cover as much food as possible. Unfortunately, such behavior\ndid not emerge. However, after around7×1010 samples of training, the hiders surprisingly discover\n21\nPublished as a conference paper at ICLR 2020\nFigure A.7: Example trajectory in the food protection game. During the preparation phase, the\nhiders ﬁrst explore the room to ﬁnd seekers, then physically push the seekers to the corner and\neventually collaboratively grab elongated boxes to build a small fort surrounding the seekers to trap\nthem there. To secure the trap, they even learn to build the fort usingtwo elongated boxes to prevent\nseekers from escaping.\n1\n 2\n 3\n 4\n 5\n 6\n 7\nNumber of hiders\n−50\n−25\n0\n25\n50Hider reward\n2\n 3\n 4\n 5\nNumber of ramps\n10\n20\n30\n40\n50Hider reward\n5\n 10\n 15\n 20\nNumber of boxes\n20\n30\n40\n50\nHider reward\nFigure A.8: Zero-shot generalization to a larger number of hiders (left), ramps (center) and boxes\n(right). The dotted line denotes the boundary of the training distribution (1-3 hiders, 2 ramps, 1-9\nboxes). Error bars denote standard error of the mean.\nan extremely efﬁcient and effective strategy: they learn to physically ‘push’ the immobile seekers to\nthe corner of the environment during the preparation phase and then collaboratively use elongated\nboxes to trap the seekers in the corner. Figure A.7 illustrates a trajectory corresponding to this\nstrategy. Interestingly, the hiders demonstrate strong collaborations. Since there are 2 seekers and 3\nhiders, when the 2 hiders are pushing the seekers to the corner, the 3rd hider will follow the ‘pushing’\nhiders distantly, and once the pushing hiders ﬁnish the job, this 3rd hider will quickly grab a box to\nbuild the trap without blocking the escaping route of those 2 pushing hiders. We emphasize that this\nbehavior was completely surprising to us and further indicates that complex behavior can emerge\nfrom multi-agent self-play with a wide range of competition rules.\nLastly, we also experimented with different versions of the close-to-wall “boundary penalties”. In\nall cases, we observe similar fort-building behaviors, namely blocking the seekers to the corner,\nwith tiny variations. When we remove the penalty, the behavior emerges twice faster ( 3.5 ×1010\nsamples) and 1 hider will stay in the fort with the seekers instead of getting outside. When we\nadd the penalty throughout the episode, the hiders even learned to stay in the center region while\ngrabbing an elongated box as a tool to push the seekers towards the corner. However, the behavior\nemerges twice slower in this setting, i.e., 1.4 ×1011 samples required.\nA.7 Z ERO -SHOT GENERALIZATION\nWe ﬁnd that the trained policies can zero-shot generalize to environments with slightly larger number\nof hiders, ramps or boxes in Figure A.8. The hider reward provides a reasonably good measure for\ngeneralization performance, since the modiﬁcations in question make constructing safe shelter more\ndifﬁcult. First, we looked at increasing the number of hiders from 1-3 hiders, as in the training\ndistribution, to 4-6 hiders; increasing the number of hiders decreases the hider reward because all\nhiders need to stay hidden from seekers. However, the decline in hider reward is very gradual,\nindicating that the policy generalizes well to more than 4 hiders. A similar effect occurs when\nincreasing the number of ramps because hiders need to secure more ramps from seekers. If we\nincrease the number of ramps from 2 to 3 or 4 the hider reward drops only gradually. Finally,\nwe ﬁnd hider performance is remarkably stable, though still slowly declines, when increasing the\nnumber of boxes.\n22\nPublished as a conference paper at ICLR 2020\nB O PTIMIZATION DETAILS\nB.1 N OTATION\nWe consider the standard multi-agent reinforcement learning formalism ofNagents interacting with\neach other in an environment. This interaction is deﬁned by a set of states Sdescribing the state of\nthe world and conﬁgurations of all agents, a set of observations O1,... ON of all agents, a set of\nactions A1,..., AN of all agents, a transition function T : S×A 1 ... AN →S determining the\ndistribution over next states, and a reward for each agent iwhich is a function of the state and the\nagent’s action. Agents choose their actions according to a stochastic policyπθi : Oi ×Ai →[0,1],\nwhere θi are the parameters of the policy. In our formulation, policies are shared between agents,\nπθi = πθ and the set of observations Ocontains information for which role (e.g. hider or seeker)\nthe agent will be rewarded. Each agent i aims to maximize its total expected discounted return\nRi = ∑H\nt=0 γtri\nt, where His the horizon length andγis a time discounting factor that biases agents\ntowards preferring short term rewards to long term rewards. The action-value function is deﬁned as\nQπi(st,ai\nt) = E[Ri\nt|st,ai\nt], while the state-value function is deﬁned as Vπi(st) = E[Rt|st]. The\nadvantage function Aπi(st,ai\nt) :=Qπi(st,ai\nt)−Vπi(st) describes whether taking actionai\ntis better\nor worse for agent iwhen in state st than the average action of policy πi.\nB.2 P ROXIMAL POLICY OPTIMIZATION (PPO)\nPolicy gradient methods aim to estimate the gradient of the policy parameters with respect to the\ndiscounted sum of rewards, which is often non-differentiable. A typical estimator of the policy\ngradient is g:= E[ ˆAt∇θlog πθ], where ˆAis an estimate of the advantage function. PPO (Schulman\net al., 2017), a policy gradient variant, penalizes large changes to the policy to prevent training\ninstabilities. PPO optimizes the objective L= E\n[\nmin(lt(θ) ˆAt,clip(lt(θ),1 −ϵ,1 +ϵ) ˆAt\n]\n, where\nlt(θ) = πθ(at|st)\nπold(at|st) denotes the likelihood ratio between new and old policies and clip (lt(θ),1 −\nϵ,1 +ϵ) clips lt(θ) in the interval [1 −ϵ,1 +ϵ].\nB.3 G ENERALIZED ADVANTAGE ESTIMATION\nWe use Generalized Advantage Estimation (Schulman et al., 2015) with horizon lengthHto estimate\nthe advantage function. This estimator is given by:\nˆAH\nt =\nH∑\nl=0\n(γλ)lδt+l, δ t+l := rt+l + γV(st+l+1) −V(st+l)\nwhere δt+lis the TD residual,γ,λ ∈[0,1] are discount factors that control the bias-variance tradeoff\nof the estimator, V(st) is the value function predicted by the value function network and we set\nV(st) = 0if st is the last step of an episode. This estimator obeys the reverse recurrence relation\nˆAH\nt = δt + γλˆAH−1\nt+1 .\nWe calculate advantage targets by concatenating episodes from policy rollouts and truncat-\ning them to windows of T = 160 time steps (episodes contain 240 time steps). If\na window (s0,...,s T−1) was generated in a single episode we use the advantage targets\n( ˆAH=T\n0 , ˆAH=T−1\n1 ,..., ˆAH=1\nT−1 ). If a new episode starts at time step j we use the advantage targets\n( ˆAH=j\n0 , ˆAH=j−1\n1 ,..., ˆAH=1\nj−1 , ˆAH=T−j\nj ,..., ˆAH=1\nT−1 ).\nSimilarly we use as targets for the value function( ˆGH=T\n0 , ˆGH=T−1\n1 ,..., ˆGH=1\nT−1 ) for a window gen-\nerated by a single episode and( ˆGH=j\n0 , ˆGH=j−1\n1 ,..., ˆGH=1\nj−1 , ˆGH=T−j\nj ,..., ˆGH=1\nT−1 ) if a new episode\nstarts at time step j, where the return estimator is given by ˆGH\nt := ˆAH\nt + V(st). This value function\nestimator corresponds to the TD(λ) estimator (Sutton & Barto, 2018).\n23\nPublished as a conference paper at ICLR 2020\nB.4 N ORMALIZATION OF OBSERVATIONS , ADVANTAGE TARGETS AND VALUE FUNCTION\nTARGETS\nWe normalize observations, advantage targets and value function targets. Advantage targets are z-\nscored over each buffer before each optimization step. Observations and value function targets are\nz-scored using a mean and variance estimator that is obtained from a running estimator with decay\nparameter 1 −10−5 per optimization substep.\nB.5 O PTIMIZATION SETUP\nTraining is performed using the distributed rapid framework (OpenAI, 2018). Using current policy\nand value function parameters, CPU machines roll out the policy in the environment, collect rewards,\nand compute advantage and value function targets. Rollouts are cut into windows of 160 timesteps\nand reformatted into 16 chunks of 10 timesteps (the BPTT truncation length). The rollouts are then\ncollected in a training buffer of 320,000 chunks. Each optimization step consists of 60 SGD substeps\nusing Adam with mini-batch size 64,000. One rollout chunk is used for at most 4 optimization steps.\nThis ensures that the training buffer stays sufﬁciently on-policy.\nB.6 O PTIMIZATION HYPERPARAMETERS\nOur optimization hyperparameter settings are as follows:\nBuffer size 320,000\nMini-batch size 64,000 chunks of 10 timesteps\nLearning rate 3 ·10−4\nPPO clipping parameter ϵ 0.2\nGradient clipping 5\nEntropy coefﬁcient 0.01\nγ 0.998\nλ 0.95\nMax GAE horizon length T 160\nBPTT truncation length 10\nB.7 P OLICY ARCHITECTURE DETAILS\nLidar observations are ﬁrst passed through a circular 1D-convolution and concatenated onto the\nagents representation of self, xself. Each object is concatenated with xself and then embedded with\na dense layer where parameters are shared between objects of the same type, e.g. all boxes share\nthe same embedding weights. All the embedded entities are then passed through a residual self-\nattention block, similar to Vaswani et al. (2017) but without position embeddings, in the form of\ny = dense(self attention(x)) +x. We then average-pool entity embeddings and concatenate this\npooled representation to xself. Note that in the policy network the entities not observed by each\nagent are masked away through self-attention and pooling. Finally, this pooled representation is\npassed through another dense layer and an LSTM (Hochreiter & Schmidhuber, 1997) before pulling\noff separate action heads for each of the 3 action types described in Section 3. We also add layer\nnormalization (Ba et al., 2016) to every hidden layer of the policy network except the 1D-convolution\nlayer. We empirically observe that layer normalization leads to faster training and better transfer\nperformance.\nSize of embedding layer 128\nSize of MLP layer 256\nSize of LSTM layer 256\nResidual attention layer 4 attention heads of size 32\nWeight decay coefﬁcient 10−6\n24\nPublished as a conference paper at ICLR 2020\nC I NTELLIGENCE TEST SUITE DETAILS\nAll evaluation tasks in the intelligence test suite are single-agent tasks. In order to align the obser-\nvations between the hide-and-seek environment and the evaluation tasks, we add fake hider obser-\nvations, preparation phase observations and set them all to 0.\nC.1 C OGNITION AND MEMORY TASK\nAll the variations of cognition and memory tasks have a horizon of 120 timesteps, and all boxes are\ncuboids.\nObject Counting:\nAn agent is pinned in place and watches as 6 boxes move either to left or right at random. The boxes\ndisappear behind walls such that agent can no longer see them, and the agent is asked to predict\nhow many boxes went left or right far after all the boxes have disappeared. In this test we evaluate\nthe quality of the existing representation by holding the agent’s policy ﬁxed and only train a new\nclassiﬁcation head from the agent’s LSTM state. The classiﬁcation head ﬁrst passes the LSTM state\nthrough a layernorm and a single dense layer with 64 units. We then do a 7-class classiﬁcation\npredicting whether 0 through 6 boxes have gone to the left.\nLock and Return:\nIn this game, the agent needs to navigate towards a hidden box, lock it, and then return to its starting\nposition.\nThe game area has 6 randomly generated connected rooms with static walls and 1 box. When\nthe box is locked, the agent will be given a reward of +5. If the agent unlocks the box during\nthe episode, a -5 penalty will be given. Additionally, if the box remains unlocked at the end of\nthe episode, the agent will be given another -5 penalty. A success is determined when the agent\nreturns to its starting location within 0.1 radius and with the box locked. For promoting fast task\naccomplishment, we give the agent a +1 reward for each timestep of success. We also introduce\nshaped reward with coefﬁcient of 0.5 for easier learning: at each time step, the shaped reward is the\ndecrement in the distance between the agent location towards the target (either the unlocked box or\nthe starting location).\nSequential Lock:\nThere are 4 boxes and the agent needs to lock all the boxes in an unobserved order sequentially. A\nbox can be locked only if it is locked in the right order.\nThe game area is randomly partition into three rooms with 2 walls. The 4 boxes are randomly placed\nin the game area. Each room has a ramp. The agent has to utilize the ramps to navigate between\nrooms. When a box is successfully locked (according to the order), a +5 bonus is given. If a box\nis unlocked, -5 penalty will be added. When all the boxes get locked, the agent will receives a +1\nper-timestep success bonus. We also use the same shaped distance reward as the lock and return\ntask here.\nC.2 M ANIPULATION TASK\nAll variations of the manipulation task have 8 boxes, but no ramps.\nConstruction from Blueprint:\nThe horizon is at most 240 timesteps, but an episode can end early if the agent successfully ﬁnishes\nthe construction. The game area is an empty room. The locations of the construction sites are\nsampled uniformly at random (we use rejection sampling to ensure that construction sites do not\noverlap).\nFor each construction site, agents observe its position and that of its 4 corners. Since there are no\nconstruction sites in the hide-and-seek game and the count-based baseline environments, we need to\nchange our policy architecture to integrate the new observations. Each construction site observation\nis concatenated withxself and then embedded through a new dense layer shared across all sites. This\n25\nPublished as a conference paper at ICLR 2020\ndense layer is randomly initialized and added to the multi-agent and count-based policies before the\nstart of training. The embedded construction site representations are then concatenated with all\nother embedded object representations before the residual self-attention block, and the rest of the\narchitecture is the same as the one used in the hide-and-seek game.\nThe reward at each timestep is equal to a reward scale constant times the mean of the smooth min-\nimum of the distances between each construction site corner and every box corner. Let there be k\nconstruction sites and nboxes, and let dij be the distance between construction site corneriand box\ncorner j, and let di be the smooth minimum of the distances from construction site corner ito all\nbox corners. The reward at each timestep follows the following formula:\ndi =\n\n\n4n∑\nj=1\ndijeαdij\n\n/\n4n∑\nj=1\neαdij ∀i= 1,2,..., 4k\nrew = sd\n(4k∑\ni=1\ndi\n)\n/4k\nHere, sd is the reward scale parameter and α is the smoothness hyperparameter ( α must be non-\npositive; α = 0gives us the mean, and α →−∞ gives us the regular min function). In addition,\nwhen all construction sites have a box placed within a certain distance dmin of them, and all con-\nstruction site corners have a box corner located withindmin of them, the episode ends and all agents\nreceive reward equal to sc ∗k, where sc is a separate reward scale parameter. For our experiment,\nn = 8and kis randomly sampled between 1 and 4 (inclusive) every episode. The hyperparameter\nvalues we use for the reward are the following:\nα= −1.5\nsd = 0.05\ndmin = 0.1\nsc = 3\nShelter construction:The goal of the task is to build a shelter around a cylinder that is randomly\nplaced in the play area. The horizon is 150 timesteps, and the game area is an empty room. The\nlocation of the cylinder is uniformly sampled at random a minimum distance away from the edges\nof the room (this is because if the cylinder is too close to the external walls of the room, the agents\nare physically unable to complete the whole shelter). The diameter of the cylinder is uniformly\nrandomly sampled between dmin and dmax. There are 3 movable elongated boxes and 5 movable\nsquare boxes. There are 100 rays that originate from evenly spaced locations on the bounding walls\nof the room and target the cylinder placed within the room. The reward at each timestep is(−n∗s),\nwhere nis the number of raycasts that collide with the cylinder that timestep and sis the reward\nscale hyperparameter.\nWe use the following hyperparameters:\ns= 0.001\ndmin = 1.5\ndmax = 2\nD I NTRINSIC MOTIVATION METHODS\nWe inherit the same policy architecture as well as optimization hyperparameters as used in the hide-\nand-seek game.\nNote that only the Sequential Lock task in the transfer suite contains ramps, so for the other 4 tasks\nwe remove ramps in the environment for training intrinsic motivation agents.\nD.1 C OUNTED -BASED EXPLORATION\nFor each real value from the continuous state of interest, we discretize it into 30 bins. Then we\nrandomly project each of these discretized integers into a discrete embedding of dimension 16 with\n26\nPublished as a conference paper at ICLR 2020\ninteger value ranging from 0 to 9. Here we use discrete embeddings for the purpose of accurate\nhashing. For each input entity, we concatenate all its obtained discrete embeddings as this entity’s\nfeature embedding. An max-pooling is performed over the feature embeddings of all the entities\nbelonging to each object type (i.e., agent, lidar, box and ramp) to obtain a entity-invariant object\nrepresentation. Finally, concatenating all the derived object representations results in the ﬁnal state\nrepresentation to count.\nWe run a decentralized version of the count-based exploration where each parallel rollout worker\nshares the same random projection for computing embeddings but maintains its own counts. Let\nN(S) denote the counts for state S in a particular rollout worker. Then the intrinsic reward is\ncalculated by 0.1√\nN(S)\n.\nD.2 R ANDOM NETWORK DISTILLATION\nRandom Network Distillation (RND) (Burda et al., 2019b) uses a ﬁxed random network, i.e., a target\nnetwork, to produce a random projection for each state while learns anther network, i.e., a predictor\nnetwork, to ﬁt the output from the target network on visited states. The prediction error between two\nnetworks is used as the intrinsic motivation.\nFor the random target network, we use the same architecture as the value network except that we\nremove the LSTM layer and project the ﬁnal layer to a 64 dimensional vector instead of a single\nvalue. The architecture is the same for predictor network. We use the squared difference between\npredictor and target network output with an coefﬁcient of 1.0 as the intrinsic reward.\nD.3 F INE -TUNING FROM INTRINSIC MOTIVATION VARIANTS\nWe compare the performances of different policies pretrained by intrinsic motivation variants on our\nintelligence test suites in Figure D.1. The pretraining methods of consideration include RND and 3\ndifferent count-based exploration variants with different state representations. For policies trained\nby count-based variants, as discussed in the Section 6.1, we know that more concise state represen-\ntation for counts leads to better emergent object manipulation skills, i.e., only using object position\nis better than using position and velocity information while using all the input as state representa-\ntion performs the worst. In our transfer task suites, we observe that polices with better pretrained\nskills perform better in 3 of the 5 tasks except the Object Counting task and the Construction from\nBlueprint task. In Construction from Blueprint, policies with better skills adapt better in the early\nphase but may have a higher chance of failure later in this challenging task. In Object Counting,\nthe polices with better skills perform worse. Interestingly, this observation is consistent with the\ntransfer result in the main paper (Figure 6), where the policy pretrained by count-based exploration\noutperforms the multi-agent pretrained policy. We conjecture that the Object Counting task exam-\nines some factors of the agents that may not strongly relate to the quality of emergent skills, such as\nnavigation and tool use.\n27\nPublished as a conference paper at ICLR 2020\n0 1 2 3\nSamples (x10 )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized reward\n0 1 2 3\nSamples (x10 )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized reward\n0 1 2 3 4 5 6 7 8\nSamples (x10 )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized reward\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5\nSamples (x10 )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized reward\n0.0 0.1 0.2 0.3 0.4 0.5 0.6\nSamples (x10 )\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nObject Counting Lock and Return Sequential Lock Blueprint Construction Shelter Construction\nCount-Based, 1 Agent ~ Box Position Count-Based, 1 Agent ~ Full Box Count-Based, 1-3 Agents ~ Full RND, 1-3 Agents ~ Full\nFigure D.1: Fine-tuning From Intrinsic Motivation Variants. We plot the mean performance on\nthe suite of transfer tasks and 90% conﬁdence interval across 3 seeds and smooth performance and\nconﬁdence intervals with an exponential moving average. We vary the state representation used for\ncollecting counts: in blue we show the performance for a single agent where the state is deﬁned\non box 2-D positions, in green we show the performance of a single agent where the state is box\nposition but also box rotation and velocity, in red we show the performance of 1-3 agents with the full\nobservation space given to a hide-and-seek policy, and ﬁnally in purple we show the performance\nalso with 1-3 agents and a full observation space but with RND which should scale better than\ncount-based exploration.\n28",
  "topic": "Reinforcement learning",
  "concepts": [
    {
      "name": "Reinforcement learning",
      "score": 0.7923495769500732
    },
    {
      "name": "Computer science",
      "score": 0.7460645437240601
    },
    {
      "name": "Suite",
      "score": 0.6619266867637634
    },
    {
      "name": "Initialization",
      "score": 0.5992364287376404
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.5246396660804749
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5117759108543396
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5070455074310303
    },
    {
      "name": "Competition (biology)",
      "score": 0.4901038110256195
    },
    {
      "name": "Class (philosophy)",
      "score": 0.4894825220108032
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4505968689918518
    },
    {
      "name": "Object (grammar)",
      "score": 0.4340386390686035
    },
    {
      "name": "Machine learning",
      "score": 0.4322824478149414
    },
    {
      "name": "Human–computer interaction",
      "score": 0.39512574672698975
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210161460",
      "name": "OpenAI (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 335
}