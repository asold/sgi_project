{
  "title": "Do Language Models Have a Common Sense regarding Time? Revisiting Temporal Commonsense Reasoning in the Era of Large Language Models",
  "url": "https://openalex.org/W4389518753",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2791890896",
      "name": "Raghav Jain",
      "affiliations": [
        "Indian Institute of Technology Patna"
      ]
    },
    {
      "id": "https://openalex.org/A5093457077",
      "name": "Daivik Sojitra",
      "affiliations": [
        "Indian Institute of Technology Patna"
      ]
    },
    {
      "id": null,
      "name": "Arkadeep Acharya",
      "affiliations": [
        "Indian Institute of Technology Patna"
      ]
    },
    {
      "id": "https://openalex.org/A2130936750",
      "name": "Sriparna Saha",
      "affiliations": [
        "Indian Institute of Technology Patna"
      ]
    },
    {
      "id": "https://openalex.org/A13250842",
      "name": "Adam Jatowt",
      "affiliations": [
        "Universität Innsbruck"
      ]
    },
    {
      "id": "https://openalex.org/A2148686933",
      "name": "Sandipan Dandapat",
      "affiliations": [
        "Microsoft (India)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385571451",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3176495666",
    "https://openalex.org/W2161484642",
    "https://openalex.org/W4327644080",
    "https://openalex.org/W3216327107",
    "https://openalex.org/W4385572162",
    "https://openalex.org/W2971236147",
    "https://openalex.org/W3169993339",
    "https://openalex.org/W3176757281",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W4378474059",
    "https://openalex.org/W4385567227",
    "https://openalex.org/W1956885672",
    "https://openalex.org/W4404783886",
    "https://openalex.org/W4385507432",
    "https://openalex.org/W3195376057",
    "https://openalex.org/W2983651678",
    "https://openalex.org/W4298149550",
    "https://openalex.org/W2089110116",
    "https://openalex.org/W4385567216",
    "https://openalex.org/W4381855801",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W3103543556",
    "https://openalex.org/W4287111051",
    "https://openalex.org/W4368755703",
    "https://openalex.org/W4384652647",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4206227470",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3203380178",
    "https://openalex.org/W1825451807",
    "https://openalex.org/W4389524534"
  ],
  "abstract": "Temporal reasoning represents a vital component of human communication and understanding, yet remains an underexplored area within the context of Large Language Models (LLMs). Despite LLMs demonstrating significant proficiency in a range of tasks, a comprehensive, large-scale analysis of their temporal reasoning capabilities is missing. Our paper addresses this gap, presenting the first extensive benchmarking of LLMs on temporal reasoning tasks. We critically evaluate 8 different LLMs across 6 datasets using 3 distinct prompting strategies. Additionally, we broaden the scope of our evaluation by including in our analysis 2 Code Generation LMs. Beyond broad benchmarking of models and prompts, we also conduct a fine-grained investigation of performance across different categories of temporal tasks. We further analyze the LLMs on varying temporal aspects, offering insights into their proficiency in understanding and predicting the continuity, sequence, and progression of events over time. Our findings reveal a nuanced depiction of the capabilities and limitations of the models within temporal reasoning, offering a comprehensive reference for future research in this pivotal domain.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6750–6774\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nDo Language Models Have a Common Sense regarding Time? Revisiting\nTemporal Commonsense Reasoning in the Era of Large Language Models\nRaghav Jain1, Daivik Sojitra1, Arkadeep Acharya1, Sriparna Saha1,\nAdam Jatowt2, and Sandipan Dandapat3\n1Department of Computer Science and Engineering, Indian Institute of Technology Patna\n2 University of Innsbruck, Austria\n3 Microsoft, India\nraghavjain106@gmail.com\nAbstract\nTemporal reasoning represents a vital compo-\nnent of human communication and understand-\ning, yet remains an underexplored area within\nthe context of Large Language Models (LLMs).\nDespite LLMs demonstrating significant pro-\nficiency in a range of tasks, a comprehensive,\nlarge-scale analysis of their temporal reasoning\ncapabilities is missing. Our paper addresses\nthis gap, presenting the first extensive bench-\nmarking of LLMs on temporal reasoning tasks.\nWe critically evaluate 8 different LLMs across\n6 datasets using 3 distinct prompting strate-\ngies. Additionally, we broaden the scope of our\nevaluation by including in our analysis 2 Code\nGeneration LMs. Beyond broad benchmarking\nof models and prompts, we also conduct a fine-\ngrained investigation of performance across dif-\nferent categories of temporal tasks. We fur-\nther analyze the LLMs on varying temporal\naspects, offering insights into their proficiency\nin understanding and predicting the continu-\nity, sequence, and progression of events over\ntime. Our findings reveal a nuanced depiction\nof the capabilities and limitations of the models\nwithin temporal reasoning, offering a compre-\nhensive reference for future research in this\npivotal domain.\n1 Introduction\nTemporal reasoning (Allen, 1983; Wenzel and Ja-\ntowt, 2023) stands as a fundamental pillar of hu-\nman communication and understanding, acting as\na guiding force in our interpretation of events and\nnarratives. Comprehending natural language in-\nvolves a deep understanding of time and its facets,\nwhich include the duration, sequence, and fre-\nquency of events. Within the realm of Natural\nLanguage Understanding (NLU), the ability to rea-\nson with temporal information has emerged as a\nsignificant area of research. This exploration has\nwitnessed substantial strides in recent years, with\nnumerous researchers contributing to this growing\nbody of knowledge (Zhou et al., 2019; Qin et al.,\n2021). Temporal reasoning extends beyond the\nmere awareness of time — it requires a nuanced un-\nderstanding of time’s relation to events and actions.\nFor instance, humans inherently know that a vaca-\ntion usually lasts longer than a walk and occurs less\nfrequently. Therefore, the quest to enhance tempo-\nral reasoning within AI systems is of paramount\nimportance. A language model with a robust under-\nstanding of temporal context is primed to perform\nbetter on downstream Natural Language Process-\ning (NLP) tasks such as storytelling (Mostafazadeh\net al., 2016), natural language inference (Hosokawa\net al., 2023), timeline understanding (Steen and\nMarkert, 2019), and user status tracking (Xia and\nQi, 2022). The integration of temporal reasoning\nnot only enhances the nuances of these applications\nbut also holds the potential to significantly improve\nthe overall performance of AI systems.\nLarge Language Models (LLMs) (Zhao et al.,\n2023) have demonstrated remarkable capabilities\nin a variety of tasks, ranging from commonsense\nreasoning (Li et al., 2022) to arithmetic problem-\nsolving (Imani et al., 2023). Despite the abun-\ndance of studies benchmarking LLMs on these\ntasks, there is a conspicuous absence of compre-\nhensive, large-scale analysis focusing on bench-\nmarking the models on temporal reasoning. Since\ntemporal reasoning represents a crucial aspect of\nhuman comprehension, influencing our interpreta-\ntion and response to a myriad of scenarios, hence\nthe lack of large-scale benchmarking for LLMs\non temporal reasoning tasks is a significant gap\nin our understanding of these models’ capabilities.\nAcknowledging this considerable gap, we have un-\ndertaken the first extensive benchmarking of LLMs\non temporal reasoning tasks. (1) Our comprehen-\nsive analysis encompasses 6 datasets, leveraging\n8 different language models. The language mod-\nels have been tested through 3 different prompting\nstrategies, aiming to explore the breadth and depth\nof their temporal reasoning proficiency. Moreover,\n6750\nwe have also included 2 Code Generation LMs,\nfurther broadening the spectrum of our analysis.\n(2) In addition to our broad benchmarking efforts,\nwe have conducted a fine-grained analysis of the\nperformance of these models across different cate-\ngories of temporal reasoning including estimation\nof event duration, event order, event frequency, sta-\ntionarity of events and typical time of events. (3)\nWe further analyze the LLMs on varying tempo-\nral aspects, offering insight into their proficiency\nin understanding and predicting the continuity, se-\nquence, and progression of events over time.\nIn particular, we investigate the following re-\nsearch questions:\n• What is the general performance of LLMs in\nTemporal Commonsense Reasoning?\n• Are the models proficient across all different\ntemporal tasks?\n• Which temporal commonsense tasks present\nthe greatest challenges?\n• Does the ambiguity in temporal expressions\naffect model performance?\n• How do models perform when they need to rea-\nson about long time frames, multiple events,\nor over past and future events?\n2 Related Works\nTemporal Reasoning and Understanding: Re-\ncent years have witnessed a significant rise in in-\nterest in evaluating models’ temporal understand-\ning. Key contributions to this field have been made\nthrough the introduction of datasets explicitly de-\nsigned to assess and improve the temporal under-\nstanding of models. Recent work by Thukral et al.\n(2021) and Hosokawa et al. (2023) have created\nNatural Language Inference (NLI) datasets to as-\nsess pretrained models’ understanding of typical\ncommon-sense temporal expressions, encompass-\ning concepts such as containment and verification\nof the state of events. To probe models’ common\nsense, researchers formulated TimeDial (Qin et al.,\n2021) and MC-TACO (Zhou et al., 2019), which\ncontain a diverse array of situations and tempo-\nral expressions. The recent past has also seen the\nproposal of several QA datasets that are sensitive\nto time (Chen et al., 2021). Furthermore, the re-\ncent research in temporal reasoning has focused\non developing time-aware training and representa-\ntion strategies for language models (Wang et al.,\n2023; Cole et al., 2023; Kimura et al., 2021; Zhou\net al., 2020; Kimura et al., 2022; Saxena et al.,\n2021). The Temporal Knowledge Graph Comple-\ntion (TKGC) domain has been exploring temporal\nreasoning within knowledge graphs (Dhingra et al.,\n2022; Jang et al., 2023). Overall, contemporary\nresearch has exhibited a notable expansion in tem-\nporal reasoning studies in natural language under-\nstanding (Wenzel and Jatowt, 2023).\nBenchmarking LLMs: The proficiency of LLMs\nhas been notably illustrated across various tasks,\nyet their exact potential and constraints remain\nsomewhat ambiguous. Recent studies have made\nstrides in scrutinizing the performance of LLMs in\ndiverse scenarios and tasks. For instance, Asai\net al. (2023) and Ahuja et al. (2023) have con-\nducted extensive benchmarking of three LLMs on\ncross-lingual and multilingual tasks, respectively.\nIn addition, Wadhwa et al. (2023) performed an\nassessment of two LLMs’ capabilities on relation\nextraction tasks. Yang et al. (2023) carried out\nbenchmarking of ChatGPT in the context of men-\ntal health issues. Furthermore, Nay et al. (2023),\nconducted comparative analyses of ChatGPT and\nGPT-4 regarding their performances on legal tax\nproblems. In essence, the latest research show-\ncases an escalating trend in probing the potential\napplications of LLMs across a variety of domains,\nlanguages, and tasks.\nIn conclusion, despite comprehensive research\non LLMs benchmarking in diverse contexts, their\nproficiency in a temporal common sense remains\nlargely unexplored. This area reveals a necessity\nfor a systematic evaluation of LLMs’ understand-\ning and reasoning within the temporal domain.\n3 Benchmark Setup\nIn the following sections, we provide the details\nof the datasets, tasks, prompting techniques, and\nlanguage models used in our research study. The\ndatasets and tasks primarily pertain to temporal\nreasoning tasks, requiring the models to display\nan understanding and reasoning in time-sensitive\ncontexts and situations. In terms of language mod-\nels, we examine a diverse set of models pretrained\nwith different strategies, including both standard\nand Code Generation LMs.\n3.1 Datasets and Tasks\nWe have employed the following datasets that are\nrelated to temporal reasoning tasks:\nMC-TACO(Zhou et al., 2019): Given a context, a\nquestion, and a candidate response, the objective is\nto determine whether the candidate answer is \"yes\"\n(plausible) or \"no\" (implausible). The dataset fo-\n6751\nDataset Task Description Output Evaluation MetricTemporal ReasoningMC-TACO Binary Classification Yes/No Acc and weighted F1ED, EO, F, S, TTTimeDial Binary Classification Yes/No Acc and weighted F1 TTTNLI Natural Language InferenceSupport/Invalidate/NeutralAcc and weighted F1 SWikiHow Binary Classification Yes/No Acc and weighted F1 EOBIG-bench Multi-Class ClassificationCorrect Option numberAcc and weighted F1 EOTimeQA Question Answering Answer string EM and F1 TT\nTable 1: Datasets Summary (ED: Event Duration, EO: Event Ordering, F: Frequency, S: Stationarity, TT: Typical\nTime, Acc: Accuracy, EM: Exact Match)\ncuses on assessing the plausibility of the answer\nwithin the temporal context provided.\nTimeDial (Qin et al., 2021): Dataset of a multiple-\nchoice cloze task featuring over 1.1K carefully\ncurated dialogues. The dialogues require an un-\nderstanding of temporal commonsense concepts\ninterwoven with the presented events.\nTNLI (Hosokawa et al., 2023): Dataset for a novel\ntask known as Temporal Natural Language Infer-\nence (TNLI). In this task, the model has to ascertain\nthe validity of textual content by using additional\nassociated content as corroborating evidence.\nWikiHow (Zhang et al., 2020): Given a goal and a\nnumber of steps, a system has to determine if the\nsteps are in the correct temporal order.\nBIG-bench (Srivastava et al., 2023): Provided with\na sequence of finished events, each with its defined\ntimeframe, the model needs to determine when\nan individual might have been available for an un-\nscheduled activity. While both BIG-bench and Wik-\niHow encompass various other reasoning tasks, we\nspecifically focused only on temporal reasoning\nsubtasks.\nTimeQA (Chen et al., 2021): This dataset com-\nprises a series of time-sensitive question-answer\npairs. Answering these questions involves under-\nstanding and reasoning within a longer context that\nrequires temporal comprehension.\nThe above datasets (refer to Appendix B.4 for\nexamples of each dataset) cover most of the tem-\nporal commonsense reasoning styles according to\nthe categorization proposed by Zhou et al. (2019)\n(refer to Appendix B.1 for detailed description and\nexamples of each task):\nEvent Duration (ED): reasoning about event du-\nrations.\nEvent Ordering (EO): reasoning about the typi-\ncal sequence of events.\nFrequency (F): reasoning about the frequency of\nevent occurrences.\nStationarity (S): reasoning about the length of\nstate persistence.\nTypical Time (TT): reasoning about the specific\ntiming of events.\nTable 1 summarizes the datasets we use and\ngives information on the types of their temporal\ncommonsense reasoning (cf. the last column), and\nthe characteristics of their tasks, the format of the\noutput, and the evaluation metrics applied.\n3.2 Prompting Techniques\nIn the context of our research, we undertake a\ncomprehensive examination of the following\nin-context learning methods across various models:\nZero-shot Prompting: Zero-shot prompting is the\nmost basic form of prompting. It is simply showing\nthe model a prompt without examples and asking\nit to generate a response. The zero-shot prompt\ncan be represented as P = fprompt(TD ; xtest)\nwhere TD corresponds to the task description,\nxtest refers to the test example, and fprompt is\na function transforming the data into a natural\nlanguage prompt.\nFew-shot Prompting: This technique involves\npresenting the model with two or more instances,\nknown as few-shot prompting. For each label\nin the dataset, examples are selected randomly;\nin our case, a single example is chosen for each\nlabel. Few-shot prompt can be represented as\nP = fprompt(TD ; (xi, yi)n; xtest) where ( xi,yi)\nsymbolizes randomly picked sample from the\ndataset, and n denotes the number of examples1.\nChain-of-Thought (CoT) Prompting (Wei et al.,\n2023): This is a recently introduced prompting\ntechnique that facilitates the LLMs to elucidate\ntheir thought process. The core idea of CoT lies\nin presenting the LLM with few-shot examples\nthat incorporate explanations of the reasoning\nprocess. The CoT prompt can be represented\nas P = fprompt(TD ; (xi, yi, Ri)n; xtest) where\nRi stands for the rationale associated with each\nfew-shot example (xi, yi).\nCode Prompts (Zhang et al., 2023): The tech-\nnique of using code-like structures (for example,\nPython) to prompt Code Generation LM for\nnatural language tasks has been found to enhance\nperformance. The code prompt can be represented\n1Please note that we didn’t perform Few-shot on TimeQA\ndataset because of the context length limit of LLMs.\n6752\nLanguage ModelParams Architecture Type Few-Shot Zero-Shot CoT Code-PromptGPT-J 6B Autoregressive Decoder onlyBase ✓ ✓GPT Neo 1.3B Autoregressive Decoder onlyBase ✓ ✓LLaMA 7B Autoregressive Decoder onlyBase ✓ ✓OPT 350M Autoregressive Decoder onlyBase ✓ ✓BLOOMZ 560M Autoregressive Decoder onlySIFT ✓ ✓ ✓Dolly 3B Autoregressive Decoder onlySIFT ✓ ✓ ✓FLAN-T5 780M Encoder-Decoder SIFT ✓ ✓ ✓SantaCoder 1.1B Autoregressive Decoder onlyBase ✓CodeGen2 2B Encoder-Decoder Base ✓GPT-3.5 - - RLHF ✓ ✓ ✓ ✓\nTable 2: Characteristics of Different LLMs employed in this study. Base denotes standard pre-training strategies,\nSIFT means Supervised Instruction Fine Tuning and RLHF means Reinforcement Learning from Human Feedback\nas P = fcode(TD ; xtest) where fcode denotes the\nfunction that translates a natural language prompt\ninto a code representation, wherein instructions\nand input samples are given as variables with\nrelevant and meaningful names, enriched by\ncomments that describe their purpose and provide\nan overarching task description. We refer readers\nto Appendix B.3 for the exact prompts used.\nNote that scope of the paper does not include\nmany possible advances of the aforementioned\nprompt like dynamic prompting (Liu et al., 2021),\nAuto-CoT (Zhang et al., 2022b) which can be\nexplored in future research.\n3.3 Language Models\nIn this work, we evaluate a set of diverse mod-\nels pre-trained with different strategies (cf. Table\n2): (1) Models for In-Context Learning: We\nexperiment with a set of diverse models; Large au-\ntoregressive models - GPT-J,2 GPT Neo,3 LLaMA\n(Touvron et al., 2023), and OPT (Zhang et al.,\n2022a), Supervised Instruction Finetuned mod-\nels - FLAN-T5 (Chung et al., 2022), BLOOMZ (Muen-\nnighoff et al., 2022), Dolly,4 and RLHF model -\nGPT-3.5. (2) Code Generation LMs: Addition-\nally, we explore the use of Code Generation LM\nto gauge their effectiveness in handling temporal\nreasoning tasks. For this purpose, we have utilized\nthe following models: (a) SantaCoder (Allal et al.,\n2023) and (b) CodeGen2 (Nijkamp et al., 2023).\n4 Results\nIn the following subsection, we initiate our discus-\nsion by providing a succinct overview of the per-\nformance of various LLMs, code generation LMs,\nand diverse prompting techniques across multiple\ndatasets. We then advance to a more in-depth explo-\nration in Sec. 4.2, scrutinizing the performance of\n2https://github.com/kingoflolz/\nmesh-transformer-jax\n3https://github.com/EleutherAI/gpt-neo\n4https://huggingface.co/databricks/\ndolly-v2-3b\ndifferent LLMs in various temporal tasks. Finally,\nwe turn our attention to analyzing different tem-\nporal characteristics of the best-performing LLMs\nfrom our previous assessments (Sec. 4.3).\n4.1 Model and Prompt-based Analysis\nStrong performance of GPT-3.5 and FLAN-T5.\nTable 3 illustrates the superior performance of\nGPT-3.5, especially in few-shot and zero-shot\nlearning tasks in datasets such as MC-TACO and\nTNLI, showcasing its good generalization ability\nand vast intrinsic knowledge. This is further im-\nproved by the CoT prompting strategy and the\nRLHF training strategy, highlighting GPT-3.5’s\nrobust in-context learning. On the other hand,\nFLAN-T5, despite being an older model when com-\npared to the other instruction-tuned models, deliv-\ners strong performance closely following GPT-3.5\nin few-shot learning tasks and even surpasses it in\nzero-shot learning on the TimeQA dataset. This can\nbe attributed to the inherent strength of base T5\nmodels which have been trained with 1 trillion to-\nkens and leverage the extensive C4 dataset (Raffel\net al., 2019). BLOOMZ also performs better than the\nother instruction-tuned decoder model(Dolly) as\nit was trained on a cross-lingual mixture of tasks\n(xP35) spanning dozens of languages. This exposes\nthe model to far higher diversity during pretrain-\ning compared to Dolly, which was trained only on\nEnglish data from a single company’s employees.\nPrevious studies have also shown (Tanwar et al.,\n2023) that multilingual LLMs are better at instruc-\ntion following and in-context learning. LLaMA per-\nforms better than other base autoregressive models\nas LLaMa has been trained on much more larger and\ndiverse dataset as well as training objectives, com-\npared to other base autoregressive models. More-\nover, the influence of the CoT prompting strategy\n5https://huggingface.co/datasets/bigscience/\nxP3\n6753\nMC-TACO TNLI TimeDial WikiHow BIG-bench TimeQAFew-shotZero-shotFew-shotZero-shotFew-shotZero-shotFew-shotZero-shotFew-shotZero-shotZero-shotModel Acc Acc Acc Acc Acc Acc Acc Acc Acc Acc EMGPT-3.5 0.8 0.75(⇓5%) 0.62 0.5(⇓12%) 0.65 0.65(=0%) 0.55 0.49 (⇓6%) 0.25 0.26 (⇑1%) 0.1FLAN-T5 0.7 0.74 (⇑4%) 0.39 0.37 (⇓2%) 0.54 0.54 (=0%) 0.45 0.45 (=0%) 0.16 0.14 (⇓2%) 0.4BLOOMZ 0.59 0.63 (⇑4%) 0.34 0.31 (⇓3%) 0.49 0.46 (⇓3%) 0.53 0.46 (⇓7%) 0.28 0.28(=0%) -Dolly 0.45 0.52 (⇑7%) 0.42 0.32 (⇓10%) 0.5 0.49 (⇓1%) 0.45 0.44 (⇓1%) 0.23 0.27 (⇑4%) 0GPT-J 0.59 0.37 (⇓22%) 0.34 0.33 (⇓1%) 0.45 0.33 (⇓12%) 0.46 0.45 (⇓1%) 0.26 0.26 (=0%) -GPT Neo0.59 0.37 (⇓22%) 0.32 0.33 (⇑1%) 0.49 0.48 (⇓1%) 0.47 0.44 (⇓3%) 0.26 0.27 (⇑1%) -LLaMA 0.65 0.65 (=0%) 0.32 0.32 (=0%) 0.5 0.35 (⇓15%) 0.55 0.54(⇓1%) 0.28 0.18 (⇓10%) 0.1OPT 0.65 0.45 (⇓20%) 0.33 0.35 (⇑2%) 0.5 0.49 (⇓1%) 0.46 0.49 (⇑3%) 0.24 0.23 (⇓1%) -\nTable 3: Performance of eight Large Language Models on six datasets, analyzed under two different prompting\nstrategies. \"Acc\" stands for Accuracy and \"EM\" corresponds to Exact Match. The percentage changes in accuracy\nperformance between Zero-shot and Few-shot prompting are indicated in parentheses. ( - suggests the data instances\nexceed the LLM’s context limit; hence results cannot be determined.)\nMC-TacoTNLI TimeDialWikiHowBIG-benchTimeQAModel Acc Acc Acc Acc Acc EMGPT-3.50.82(⇑2%)0.46(⇓16%)0.7(⇑5%)0.54(⇓1%)0.67(⇑42%)0.15FLAN-T50.73 (⇑3%)0.38 (⇓1%)0.54 (0%)0.48 (⇑3%)0.21 (⇑5%) 0.05BLOOMZ0.45 (⇓14%)0.36 (⇑2%)0.53 (⇑4%)0.5 (⇓3%)0.3 (⇑2%) -Dolly0.48 (⇑3%)0.34 (⇓8%)0.5 (0%)0.47 (⇑2%)0.29 (⇑6%) 0\nTable 4: Performance of Instruction Tuned LLMs with\nCoT prompting strategy. The percentage changes in ac-\ncuracy performance between CoT and Few-shot prompt-\ning (from Table 3) are indicated in parentheses.\n(Table 4) over different models varies; it signifi-\ncantly improves complex temporal reasoning tasks\nlike BIG-bench, while the enhancement in others,\nsuch as WikiHow, is similar across other settings.\nHowever, TNLI is characterized by inconsistent\nperformance under the CoT setup.\nCode Generation LMs are not temporal com-\nmonsense reasoners.\nTable 5 shows a performance analysis of vari-\nous models across different tasks, specifically fo-\ncusing on code generation language models and\ntheir ability to reason with temporal commonsense.\nPrevious studies (Madaan et al., 2022) have high-\nlighted the superiority of Code Generation LMs\nin reasoning and commonsense tasks compared to\ngeneral-purpose LMs. However, upon reviewing\nthe results in Table 5, it becomes evident that the\ncode generation LMs, SantaCoder and CodeGen2,\nstruggle as temporal commonsense reasoners. They\nencounter difficulties across multiple datasets, in-\ncluding MC-TACO, TNLI, TimeDial, and WikiHow,\nwhere understanding and reasoning about tempo-\nral aspects are crucial. Similarly, GPT-3.5 with\ncode prompts also exhibits limited performance in\ntemporal commonsense reasoning, as reflected by\nits relatively lower scores in tasks like TimeDial\nand BIG-bench. Although GPT-3.5 outperforms\nthe other code generation LMs, its performance\nstill falls short compared to normal text prompts\nMC-TACOTNLITimeDialWikiHowBIG-benchTimeQAModel Acc Acc Acc Acc Acc EMGPT-3.5 0.5 0.45 0.49 0.45 0.29 0.1SantaCoder0.5 0.36 0.5 0.45 0.27 -CodeGen20.61 0.35 0.5 0.45 0.25 0.1\nTable 5: Performance of Code Generation LMs with\nCode prompts in Zero-shot setting. ( - suggests the data\ninstances exceed the LLM’s context limit; hence results\ncannot be determined.)\non all datasets and tasks as shown in Table 3. Our\nexperiments align with the findings of Zhang et al.\n(2023), who demonstrate that code prompts do not\nsurpass the performance of text prompts. Appendix\nB.5 provides detailed results with F1 scores.\n4.2 Temporal Task-based Analysis\nStrong performance of LLMs on event fre-\nquency, and duration tasks.\nThe heatmap illustrated in Figure 1 provides a\nvisualization of the performance of various LLMs\nacross distinct prompting settings when applied\nto the MC-TACO datasets and their respective fine-\ngrained temporal task categories (ED, EO, F, S, TT).\nModels predominantly perform well on tasks asso-\nciated with event duration, withGPT-3.5 taking the\nlead in accuracy and F1 scores, followed closely\nby FLAN-T5. Other models like Dolly, GPT-J,\nGPT Neo, LLaMA, and OPT demonstrate mixed re-\nsults. However, BLOOMZ’s zero-shot capabilities\nalign well with FLAN-T5 and GPT-3.5. On tasks re-\nlated to event frequency,GPT-3.5 maintains strong\nperformance, while FLAN-T5 experiences a slight\ndrop but still presents impressive performance.\nMixed performance on event ordering tasks.\nPerformance varies across models on the ‘Event\nOrdering’ task in the MC-TACO dataset, with\nGPT-3.5 and FLAN-T5 leading and others like\nLLaMA and OPT showing declines, indicating chal-\n6754\nlenges with event ordering (Figure 1). However,\ncomparison with other datasets like WikiHow and\nBIG-bench reveals lower performance, likely due\nto the increased complexity of these tasks. The\nimprovement in results when models are combined\nwith the CoT prompting technique suggests that\nmore complex event-ordering tasks demand greater\nreasoning abilities from LLMs.\nPerformance drop on understanding event\ntemporal states.\nThere is a notable decrease in performance\nacross all models in the ‘Stationarity’ task, high-\nlighting the difficulty in assessing the temporal\nstates of events, such as whether events or situa-\ntions remain constant over time (Figure 1). Remark-\nably, BLOOMZ demonstrates strong performance in\nthe zero-shot configuration, almost on par with\nGPT-3.5, suggesting that it has a specific strength\nin identifying event stationarity. Furthermore, re-\nferring to Table 4, it is evident that even when\nusing the CoT prompting strategy, Language Mod-\nels struggle with other tasks related to Stationarity,\nsuch as ones inTNLI dataset (TNLI dataset requires\nunderstanding the states of events). This points to-\nwards an inherent challenge faced by these models\nin grasping and reasoning over concepts of the tem-\nporal stability of events.\nLLMs struggle with specific event timings.\nThe ‘Typical Time’ task stands out as the most\ndemanding for all the models, emphasizing the\nintricate nature of predicting and reasoning over\ntypical event timings (Figure 1). While GPT-3.5\ncontinues to perform best in this category, its lead\nover other models is significantly reduced com-\npared to other tasks. The performance gap between\nGPT-3.5 and other models such as FLAN-T5 and\nBLOOMZ is notably narrower in the zero-shot con-\nfiguration, indicating less dominance by GPT-3.5\non this task. A substantial performance decline\nis also observed in other models, further underlin-\ning the task’s complexity. A reference to Table\n3 reinforces this conclusion of LLMs struggling\nwith exact timings, as it reveals that all Language\nModels struggle not only with the ‘Typical Time’\ntask on MC-TACO, but also with other time-related\ntasks, such as TimeQA. This indicates a broader\nchallenge for LLMs in reasoning over specific time\nperiods. Readers can refer to Appendix B.7 for\nboth F1 and accuracy results.\n4.3 Temporal Aspect-based Analysis\nReasoning about future events is more diffi-\ncult than about past events.\nWe used the TimeDial dataset and identified\n‘Past’ and ‘Future’ events based on verb tenses\nusing SpaCy toolkit6. Manual verification ensured\nthe accuracy post automatic classification. We re-\ntained 200 instances each for both categories to\nenable an effective performance comparison (an ex-\nample of such an instance is shown in Figure 2(a)).\nFigure 3(a) highlights a consistent trend among\nmodels, indicating a slight drop in performance\nwhen reasoning about the Future events compared\nto the Past events. All models show a trend of\nhigher accuracy in reasoning about the past com-\npared to the future. This disparity may be attributed\nto the model’s extensive training on past events or\nscenarios, which is more likely to be found in the\ntraining data, providing it with a richer database to\ndraw from when making predictions about the past.\nJoho et al. (2015, 2013) also showed that users\nstruggle more with finding information about the\nfuture than one about the past when using search\nengines and that search for future content is rela-\ntively common. In summary, while these models\nare adept at temporal reasoning, their performance\nslightly drops when dealing with future events.\nLLMs perform better on temporal reasoning\nover longer timeframes.\nTo construct the dataset for this experiment, we\nscrutinized the TimeDial dataset. Each instance\nwas classified based on the duration specified by\nthe correct label associated with its context. This\nmay lead to the creation of categories: seconds,\nminutes, hours, a day, and durations exceeding a\nday. After manual verification for accuracy, we\nsampled 200 instances from each category (Fig-\nure 2(b)). Figure 3(b) offers a comparative analy-\nsis of models’ performance when reasoning about\nevents that occur over varying time frames. The\nemerging pattern highlights that the models tend\nto perform better when dealing with longer time\nframes compared to shorter ones. GPT-3.5 pro-\nvides the clearest representation of this trend, start-\ning with a relatively lower accuracy for events that\ntranspire over seconds. However, its accuracy in-\ncreases as the duration of events extends to minutes,\nhours, and days. Impressively, GPT-3.5 reaches its\n6https://spacy.io/\n6755\n(a) Heatmap for Accuracy of LLMs in\nFew-shot setting\n(b) Heatmap for Accuracy of LLMs in\nZero-shot setting\n(c) Heatmap for Accuracy of\nLLMs in CoT setting\nFigure 1: Performance of LLMs on MC-TACO dataset and it’s fine-grained temporal task categories on Few-shot\nsettings. Y-axis represents different models and X-axis represents different temporal tasks (MC: MC-TACO)\nDataset MC-TACO TNLI TimeDial WikiHow BIG-bench TimeQA\nAcc Acc Acc Acc Acc EM\nHuman Baselines 0.75 0.82 0.97 0.97 1.0 0.9\nBaseline Fine-Tuned Models0.64 0.878 0.748 0.801 - 0.55\nTable 6: Performance comparison of Fine-Tuned Models and Human Baselines across Various Datasets. - indicates\nthat the BIG-bench dataset does not have a training set, so no model can be fine-tuned.\nFigure 2: Dataset example for temporal aspect analysis\npeak performance when reasoning about events\nthat last more than a day. This upward trajectory\nsuggests that GPT-3.5 is more adept at handling\nthe intricacies involved in reasoning about longer-\ntime frame events. Similarly, FLAN-T5 shows an\nimprovement in performance when moving from\nshorter to longer time frames, although its trajec-\ntory is not that consistent. LLaMA, despite some\nvariation, also seems to perform better with longer-\nduration events. These findings could suggest that\nmodels are better equipped to handle the complexi-\nties and nuances involved in reasoning about longer\ntimeframe events.\nLLMs have difficulty with temporal reason-\ning over longer context.\nWe divided the TimeDial dataset into three cate-\ngories based on context length: 0-200 words, 200-\n400 words, and 400-600 words. We then selected\n200 instances randomly from each category, ef-\nfectively creating a dataset with diverse context\nlengths. In Figure 3(c), we observe that as the con-\ntext length increases the performance of the models\ntends to decrease (with the exception of FLAN-T5).\nGPT-3.5 shows a decline in performance as the\ncontext length increases from 0-200 to 400-600,\nindicating a possible difficulty in handling long\ncontextual information.\nLLMs struggle with exact temporal expres-\nsions compared to ambiguous ones.\nWe created a specialized dataset fromMC-TACO\nleveraging the combination of HeidelTime (Ströt-\ngen and Gertz, 2010) and manual extraction. This\nallowed us to distinguish between instances con-\ntaining ‘Exact Timings’, such as specific numerical\nexpressions of time, day names, and month names,\nand ‘Ambiguous Temporal Expressions’, likein the\nmeantime, after a few days, and meanwhile (exam-\nples shown in Figure 4(a)). We then performed a\nmanual verification of these instances, maintaining\na balanced collection of 200 instances each from\nthe categories of exact and ambiguous temporal\nexpressions. The comparative performance analy-\nsis (Figure 5(a)) across diverse temporal reasoning\ntasks highlights a persistent challenge the models\nface when dealing with ‘Exact Timings’ such as\nnumerical values, day names, or month names, as\nopposed to ‘ambiguous temporal expressions’. The\nmodels may encounter difficulties in associating\nspecific timings with their implications for the oc-\ncurrence or sequence of events, indicating a poten-\ntial challenge in comprehending and connecting\nexact temporal information. It underscores a key\n6756\n(a) Performance of LLMs across Past\nand Future events\n(b) Performance of LLMs across different\ntimeframes\n(c) Performance of LLMs across dif-\nferent context length\nFigure 3: Bar Plots indicating the performance of LLMs across different temporal aspects in few-shot setting. X-axis\nrepresents the different LLMs, and Y-axis represents performance in accuracy\nFigure 4: Dataset examples for temporal aspect analysis\narea of struggle for these models - effective rea-\nsoning over specific time periods across different\ntasks. This may be attributed to the higher preva-\nlence of ambiguous events as compared to precise\nones, largely because they are utilized more fre-\nquently in a variety of scenarios. Readers can refer\nto Appendix B.6 for detailed results of this section.\nLLMs struggle with understanding the states\nand orders of multiple events.\nWe manually dissected the MC-TACO dataset\ndue to the inherent complexity of accurately iden-\ntifying single and multiple temporal events within\ninstances. We systematically curated two groups\nfrom each category: instances with single and mul-\ntiple temporal events. To maintain uniformity, we\nselected 200 instances from each group, as shown\nin Figure 4(b). Figure 5(b) compares model per-\nformance across tasks for single versus multiple\ntemporal events. In the context of ‘Event Duration’\nand ‘Frequency’, all models perform better when\nreasoning about multiple events as compared to\nsingle events. On the other hand, the ‘Stationarity’\nand ‘Event Ordering’ tasks display a different trend,\nwhere all models perform better when reasoning\nabout single events as opposed to multiple events.\nThis could indicate that these tasks, which require\nunderstanding the persistence of states and the typ-\nical sequence of events, can become more complex\nand challenging when multiple events are involved.\nIn the ‘Typical Time’ task, we observe that all mod-\nels generally perform better or at least equally well\nwhen reasoning about single events, perhaps indica-\ntive of their difficulties when attempting to com-\nprehend specific timings associated with multiple\nevents.\n5 Comparison with Fine-Tuned models\nand Human Performance\nHuman Evaluation: Human evaluations were car-\nried out using 100 random samples from the TNLI,\nBIG-bench, and TimeQA datasets, assessed by three\nin-house annotators. For other datasets, we relied\non human baselines provided in the original pa-\npers. Annotators’ accuracy was gauged by compar-\ning their responses to the ground truth, establish-\ning human benchmarks for performance (Table 6).\nFor TimeDial, WikiHow, BIG-bench, and TimeQA\ntasks, human proficiency substantially outperforms\nall LLMs. For example, humans scored 0.97 on\nTimeDial, whereas the best LLM, GPT-3.5, only\nachieved 0.65. The gap is further widened in tasks\nlike WikiHow, where humans scored 0.975. In\nthe TNLI dataset, even the top-performing LLM,\nGPT-3.5, falls short of the human baseline score\n(0.82 vs. 0.62), indicating lingering challenges for\nLLMs in this task. In contrast, GPT-3.5 matches\nhuman performance on the MC-TACO dataset in a\nfew-shot setting, both scoring 0.8, suggesting that\nunder certain conditions, latest LLMs can achieve\nhuman-level capabilities.\nFine Tuned Model Baselines: Table 6 also in-\ncludes evaluation results for the best-performing\nfine-tuned models from the papers introducing each\ndataset. Key findings are as follows: In the MC-\nTACO dataset, the baseline model scored 0.64.\n6757\n(a) LLMs’ performance when dealing with exact and ambiguous tem-\nporal expressions across temporal tasks (TE: Temporal Expression).\n(b) Performance of LLMs when processing both single-event and\nmultiple-event scenarios across temporal tasks.\nFigure 5: Radar plots comparing accuracy of LLMs across different temporal aspects in few-shot setting\nHowever, LLMs like GPT-3.5 and FLAN-T5 signif-\nicantly outperformed it in both few-shot and zero-\nshot scenarios. For TNLI, the fine-tuned baseline\nmodel achieved 0.878, outperforming all LLMs.\nNotably, GPT-3.5, the top LLM, scored only 0.62\nin few-shot settings. This is because the baseline\nwas fine-tuned on TNLI data and leveraged exter-\nnal commonsense knowledge. On TimeDial and\nWikiHow, the baseline models scored 0.748 and\n0.801, respectively. GPT-3.5 led among LLMs but\ndid not surpass the baseline. The fine-tuned mod-\nels excel due to task-specific optimizations. For\nTimeQA, with a baseline score of 0.55, FLAN-T5\nwas the closest among LLMs with an EM score of\n0.4. The baseline’s higher performance is attributed\nto its use of Retrieval Augmented Generation (FiD),\nallowing it to handle the dataset’s long context.\n6 Conclusion\nIn this paper, we aimed to bridge a critical knowl-\nedge gap by conducting a comprehensive bench-\nmarking of LLMs on temporal reasoning tasks.\nOur thorough analysis has shed light on certain\nlimitations in the ability of LLMs to reason tempo-\nrally. Specifically, we have identified areas where\nLLMs struggle, such as comprehending the tempo-\nral states of events, accurately reasoning over pre-\ncise timings, managing multiple temporal events,\nand predicting future events. By highlighting these\nchallenges, our study contributes to a better un-\nderstanding of the capabilities and limitations of\nLLMs in temporal reasoning tasks.\nAcknowledgement: Dr. Sriparna Saha gratefully\nacknowledges the Young Faculty Research Fellow-\nship (YFRF) Award, supported by Visvesvaraya\nPh.D. Scheme for Electronics and IT, Ministry of\nElectronics and Information Technology (MeitY),\nGovernment of India, being implemented by Dig-\nital India Corporation (formerly Media Lab Asia)\nfor carrying out this research.\nLimitations\nThere are several limitations of our study that\nshould be acknowledged. Firstly, in terms of\nprompt selection, our testing was limited to a\nfew prompting strategies, and there exist numer-\nous other techniques and variations that could be\nexplored. Therefore, the generalizability of our\nfindings to different prompt settings may be con-\nstrained. Secondly, our evaluation of models was\nprimarily focused on open-source models and cov-\nered only one closed model ( GPT-3.5). We did\nnot include closed models like PaLM. In this study,\nwe covered only temporal commonsense reason-\ning. Yet, we acknowledge that there are various\nother temporal tasks that were not covered, such as\ntimeline summarization and temporal information\nretrieval.\n6758\nReferences\nKabir Ahuja, Harshita Diddee, Rishav Hada, Milli-\ncent Ochieng, Krithika Ramesh, Prachi Jain, Ak-\nshay Nambi, Tanuja Ganu, Sameer Segal, Maxamed\nAxmed, Kalika Bali, and Sunayana Sitaram. 2023.\nMega: Multilingual evaluation of generative ai.\nLoubna Ben Allal, Raymond Li, Denis Kocetkov,\nChenghao Mou, Christopher Akiki, Carlos Munoz\nFerrandis, Niklas Muennighoff, Mayank Mishra,\nAlex Gu, Manan Dey, et al. 2023. Santacoder: don’t\nreach for the stars! arXiv preprint arXiv:2301.03988.\nJames F. Allen. 1983. Maintaining knowledge about\ntemporal intervals. Commun. ACM, 26(11):832–843.\nAkari Asai, Sneha Kudugunta, Xinyan Velocity Yu,\nTerra Blevins, Hila Gonen, Machel Reid, Yulia\nTsvetkov, Sebastian Ruder, and Hannaneh Hajishirzi.\n2023. Buffet: Benchmarking large language models\nfor few-shot cross-lingual transfer.\nWenhu Chen, Xinyi Wang, and William Yang Wang.\n2021. A dataset for answering time-sensitive ques-\ntions.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\n2022. Scaling instruction-finetuned language mod-\nels.\nJeremy R. Cole, Aditi Chaudhary, Bhuwan Dhingra,\nand Partha Talukdar. 2023. Salient span masking for\ntemporal understanding.\nBhuwan Dhingra, Jeremy R. Cole, Julian Martin\nEisenschlos, Daniel Gillick, Jacob Eisenstein, and\nWilliam W. Cohen. 2022. Time-Aware Language\nModels as Temporal Knowledge Bases. Transac-\ntions of the Association for Computational Linguis-\ntics, 10:257–273.\nTaishi Hosokawa, Adam Jatowt, and Kazunari\nSugiyama. 2023. Temporal Natural Language Infer-\nence: Evidence-Based Evaluation of Temporal Text\nValidity, pages 441–458.\nShima Imani, Liang Du, and Harsh Shrivastava. 2023.\nMathprompter: Mathematical reasoning using large\nlanguage models. arXiv preprint arXiv:2303.05398.\nJoel Jang, Seonghyeon Ye, Changho Lee, Sohee Yang,\nJoongbo Shin, Janghoon Han, Gyeonghun Kim, and\nMinjoon Seo. 2023. Temporalwiki: A lifelong bench-\nmark for training and evaluating ever-evolving lan-\nguage models.\nHideo Joho, Adam Jatowt, and Roi Blanco. 2015. Tem-\nporal information searching behaviour and strategies.\nInformation Processing Management , 51(6):834–\n850.\nHideo Joho, Adam Jatowt, and Blanco Roi. 2013. A sur-\nvey of temporal web search experience. In Proceed-\nings of the 22nd International Conference on World\nWide Web, WWW ’13 Companion, page 1101–1108,\nNew York, NY , USA. Association for Computing\nMachinery.\nMayuko Kimura, Lis Kanashiro Pereira, and Ichiro\nKobayashi. 2021. Towards a language model for\ntemporal commonsense reasoning. In Proceedings\nof the Student Research Workshop Associated with\nRANLP 2021, pages 78–84, Online. INCOMA Ltd.\nMayuko Kimura, Lis Kanashiro Pereira, and Ichiro\nKobayashi. 2022. Toward building a language model\nfor understanding temporal commonsense. In Pro-\nceedings of the 2nd Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics and the 12th International Joint Conference\non Natural Language Processing: Student Research\nWorkshop, pages 17–24.\nXiang Lorraine Li, Adhiguna Kuncoro, Jordan Hoff-\nmann, Cyprien de Masson d’Autume, Phil Blunsom,\nand Aida Nematzadeh. 2022. A systematic investiga-\ntion of commonsense knowledge in large language\nmodels.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021. What\nmakes good in-context examples for gpt- 3? arXiv\npreprint arXiv:2101.06804.\nAman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang,\nand Graham Neubig. 2022. Language models of code\nare few-shot commonsense learners. arXiv preprint\narXiv:2210.07128.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A corpus\nand evaluation framework for deeper understanding\nof commonsense stories.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey\nSchoelkopf, et al. 2022. Crosslingual generaliza-\ntion through multitask finetuning. arXiv preprint\narXiv:2211.01786.\nJohn J. Nay, David Karamardian, Sarah B. Lawsky,\nWenting Tao, Meghana Bhat, Raghav Jain,\nAaron Travis Lee, Jonathan H. Choi, and Jungo\nKasai. 2023. Large language models as tax attorneys:\nA case study in legal capabilities emergence.\nErik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Sil-\nvio Savarese, and Yingbo Zhou. 2023. Codegen2:\nLessons for training llms on programming and natu-\nral languages. arXiv preprint.\n6759\nLianhui Qin, Aditya Gupta, Shyam Upadhyay, Luheng\nHe, Yejin Choi, and Manaal Faruqui. 2021. Time-\nDial: Temporal Commonsense Reasoning in Dialog.\nIn Proc. of ACL.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv e-prints.\nApoorv Saxena, Soumen Chakrabarti, and Partha Taluk-\ndar. 2021. Question answering over temporal knowl-\nedge graphs.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, and\nmultiple authors. 2023. Beyond the imitation game:\nQuantifying and extrapolating the capabilities of lan-\nguage models.\nJulius Steen and Katja Markert. 2019. Abstractive time-\nline summarization. In Proceedings of the 2nd Work-\nshop on New Frontiers in Summarization, pages 21–\n31, Hong Kong, China. Association for Computa-\ntional Linguistics.\nJannik Strötgen and Michael Gertz. 2010. HeidelTime:\nHigh quality rule-based extraction and normaliza-\ntion of temporal expressions. In Proceedings of the\n5th International Workshop on Semantic Evaluation,\npages 321–324, Uppsala, Sweden. Association for\nComputational Linguistics.\nEshaan Tanwar, Manish Borthakur, Subhabrata Dutta,\nand Tanmoy Chakraborty. 2023. Multilingual llms\nare better cross-lingual in-context learners with align-\nment.\nShivin Thukral, Kunal Kukreja, and Christian Kavouras.\n2021. Probing language models for understanding of\ntemporal expressions. In Proceedings of the Fourth\nBlackboxNLP Workshop on Analyzing and Interpret-\ning Neural Networks for NLP, pages 396–406, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nSomin Wadhwa, Silvio Amir, and Byron C. Wallace.\n2023. Revisiting relation extraction in the era of\nlarge language models.\nJiexin Wang, Adam Jatowt, Masatoshi Yoshikawa, and\nYi Cai. 2023. Bitimebert: Extending pre-trained lan-\nguage representations with bi-temporal information.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nGeorg Wenzel and Adam Jatowt. 2023. An overview\nof temporal commonsense reasoning and acquisition.\nCoRR, abs/2308.00002.\nXiaona Xia and Wanxue Qi. 2022. Temporal tracking\nand early warning of multi semantic features of learn-\ning behavior. Computers and Education: Artificial\nIntelligence, 3:100045.\nKailai Yang, Shaoxiong Ji, Tianlin Zhang, Qianqian Xie,\nZiyan Kuang, and Sophia Ananiadou. 2023. Towards\ninterpretable mental health analysis with chatgpt.\nLi Zhang, Liam Dugan, Hainiu Xu, and Chris Callison-\nBurch. 2023. Exploring the curious case of code\nprompts.\nLi Zhang, Qing Lyu, and Chris Callison-Burch. 2020.\nReasoning about goals, steps, and temporal ordering\nwith WikiHow. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 4630–4639, Online. As-\nsociation for Computational Linguistics.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022a. Opt: Open\npre-trained transformer language models.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022b. Automatic chain of thought prompt-\ning in large language models. arXiv preprint\narXiv:2210.03493.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A\nsurvey of large language models.\nBen Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth.\n2019. “going on a vacation” takes longer than “go-\ning for a walk”: A study of temporal commonsense\nunderstanding. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 3363–3369, Hong Kong, China. Association\nfor Computational Linguistics.\nBen Zhou, Kyle Richardson, Qiang Ning, Tushar Khot,\nAshish Sabharwal, and Dan Roth. 2020. Temporal\nreasoning on implicit events from distant supervision.\narXiv preprint arXiv:2010.12753.\n6760\nAppendix\nA Frequently Asked Questions (FAQs)\n✽ What was the rationale behind selecting\nthese specific LLMs?\n➠ Our primary objective for utilizing a diverse\nset of Large Language Models (LLMs) in\nthe analysis of temporal commonsense was\nto ensure a wide representation of different\nmodels. This includes those ranging from au-\ntoregressive decoder-only models to encoder-\ndecoder models, as well as instruction-tuned\nmodels. We selected GPT-3.5 as it is among\nthe top-performing LLMs at present. How-\never, given its proprietary nature, we comple-\nmented it with the newly launched instruction-\ntuned LLMs such as BLOOMZ and Dolly. A\nunique aspect of these models is that Chat-\nGPT was trained with a Reinforcement Learn-\ning from Human Feedback (RLHF) approach,\nwhile BLOOMZ and Dolly used supervised in-\nstruction fine-tuning. We added FLAN-T5 to\nour roster due to its distinct encoder-decoder\narchitecture, which offers a contrast to other\ninstruction-tuned LLMs. The inclusion of\nmodels like LLama, OPT, GPT-J, GPT-Neo\nserved to exhibit the capabilities of an autore-\ngressive decoder-only model. Furthermore,\nwe integrated two code generation Language\nModels to contrast their performance in tem-\nporal commonsense reasoning with the gen-\neral purpose LLMs.\n✽ Who was responsible for manually\nvalidating the datasets used in the\nTemporal-Aspect based analysis sec-\ntion?\n➠ We implemented heuristic and tagger-based\ntechniques to categorize our dataset into dif-\nferent classes, and as authors, we also person-\nally conducted manual verification to elimi-\nnate any potential misclassification. However,\nit is important to highlight that these sam-\nples were sourced from well-established and\ntrusted datasets in the community: MC-TACO\nand TimeDial. Given their standardization and\nwide acceptance in the field, we did not find it\nnecessary to compute Inter-Annotator Agree-\nment (IAA) scores. We are confident in the\nintegrity and relevance of these datasets for\nour study. Still, in the interest of transparency\nand further research, we commit to publicly\nsharing the manually curated data utilized in\nour experiments upon acceptance of our work.\nWe believe that this does not only contribute\nto the reproducibility of our findings but also\nfosters research advancements in this field.\n✽ What is the rationale behind selecting\nMC-TACO and TimeDial as the founda-\ntional datasets for the Temporal-Aspect\nbased analysis section?\n➠ Our choice to utilize MC-TACO and TimeDial\nas the base datasets for the Temporal-Aspect\nbased analysis section was primarily driven\nby their significant size and well-established\nreputation in the research community. MC-\nTACO provides a vast array of approximately\n9,441 data samples, while TimeDial furnishes\nan additional 5,784 examples. The wealth of\ndata they offer surpasses most other datasets\nin the field. In terms of research queries like\nanalyzing performance with varying context\nlengths, past versus future reasoning capabil-\nities, and performance across different time-\nframes, TimeDial was a particularly suitable\nchoice. Its emphasis on timing-based contexts\nrendered it a valuable resource. Moreover,\nfor specialized inquiries into ambiguous tem-\nporal expression reasoning and single versus\nmultiple event reasoning, MC-TACO proved\nindispensable. It presents a broad spectrum\nof diverse examples catering to these specific\nareas of interest. Overall, these datasets, by\nvirtue of their depth, diversity, and relevance,\nenable robust, comprehensive, and nuanced\nanalyses. They represent reliable sources for\ngenerating insights into temporal-aspect based\nreasoning with large language models.\n✽ What was the reason behind choosing\nonly three prompting strategies for anal-\nysis?\n➠ In this study, we focused on utilizing three\nwidely recognized prompting strategies: Few-\nshot, Zero-shot, and CoT prompting. These\nstrategies were chosen due to their established\nprominence in the field and their frequent in-\nclusion in related analysis and benchmarking\n6761\nworks (Wadhwa et al., 2023; Nay et al., 2023).\nBy employing these standard techniques, we\nensured consistency with existing literature\nand provided a solid foundation for compar-\nison and benchmarking. While we acknowl-\nedge that there have been recent advancements\nand emerging techniques beyond the scope\nof this study, we aim to emphasize that our\nresearch serves as a starting point in explor-\ning these methods for temporal commonsense\ntasks. Our intention is to pave the way for\nfuture investigations that delve into the latest\ndevelopments in prompting strategies, thus\nenabling a more comprehensive understand-\ning of their effectiveness in temporal com-\nmonsense analysis. We remain committed to\nstaying abreast of the evolving landscape of\nprompting techniques and incorporating them\nin future endeavors to expand the breadth and\ndepth of our findings.\nB Supplementary Material\nThis section provides supplementary material in the\nform of additional results, implementation details,\netc. to bolster the reader’s understanding of the\nconcepts presented in this work.\nB.1 Dataset Example for Temporal Task\nbased Analysis\n• Event Duration (ED): This task necessitates\nreasoning about event durations.\n• Event Ordering (EO): This task calls for rea-\nsoning about the typical sequence of events.\n• Frequency (F): This task requires reasoning\nabout the frequency of event occurrences.\n• Stationarity (S): This task demands reason-\ning about the length of state persistance.\n• Typical Time (TT): This task needs reason-\ning about the specific timing of events.\nTable 7 contains examples for each of these cate-\ngories.\nB.2 Experimental Setup\nOperating System: Ubuntu 18.04.5 LTS\nRAM: 220GB\nGPU: NVIDIA GeForce RTX 3090 (24 GB)\nPython: 3.10.11\nHyperparameters: We used temperature=0.5,\ntop_p=1, and top_k=50, fixed across experiments,\nas commonly adopted values in prior work.\nB.3 Prompts\nPrompt samples for all the datasets.\nB.3.1 Prompts for MC-TACO\nFew-shot Prompt for MC-TACO\nGiven the passage, the question, and the candidate\nanswer, the task is to determine whether the candidate\nanswer is plausible (\"yes\") or not (\"no\").\nPassage: Durer’s father died in 1502, and his mother\ndied in 1513.\nQuestion: How long was his mother ill? answer: six\ncenturies\nResponse:No\n###\nGiven the passage, the question, and the candidate\nanswer, the task is to determine whether the candidate\nanswer is plausible (\"yes\") or not (\"no\").\nPassage: Durer’s father died in 1502, and his mother\ndied in 1513.\nQuestion: How long was his mother ill?\nanswer: 6 months\nResponse:Y es\n###\nGiven the passage, the question, and the candidate\nanswer, the task is to determine whether the candidate\nanswer is plausible (\"yes\") or not (\"no\").\nPassage: Durer’s father died in 1502, and his mother\ndied in 1513.\nQuestion: How long was his mother ill?\nanswer: 3 minutes\nResponse:No\n###\nGiven the passage, the question, and the candidate\nanswer, the task is to determine whether the candidate\nanswer is plausible (\"yes\") or not (\"no\")\nPassage: {Passage}\nQuestion: {Question}\nAnswer: {Answer}\nResponse:\nZero-shot Prompt for MC-TACO\nGiven the passage, the question, and the candidate\nanswer, the task is to determine whether the candidate\nanswer is plausible (\"yes\") or not (\"no\").\nPassage: {Passage}\nQuestion: {Question}\nAnswer: {Answer}\nReturn your Response in Y es or No only.\nResponse:\n6762\nContext Question Answer Label Temporal Task\nSafti admits his love for\nEdwina to Lord Esketh ,\nwho is now sympathetic\ntoward this good man’s\nplight .\nHow long has Safti been\nin love with Edwina?\na year yes Event Duration\nDurer’s father died in\n1502, and his mother\ndied in 1513.\nHow long was his\nmother ill?\nshe was ill for 30 sec-\nonds\nno Event Duration\nWhat we call the lab-to-\nfab time should be as\nclose to zero as possible,\nKelly said.\nWhat happened after\nKelly spoke?\nmany people agreed yes Event Ordering\nTim knew if the bike\nwas going to be in any\nof the presents it was go-\ning to be in this box.\nAfter Tim found the box,\nwhat happened?\nhe tossed it away no Event Ordering\nMost of us have seen\nsteam rising off a wet\nroad after a summer\nrainstorm.\nHow often does it rain\nin the summer?\na couple times every\nmonth\nyes Frequency\nThe organization has al-\nready lost some staff\nthrough attrition and has\nturned away some cases,\nshe said.\nHow often does the or-\nganization turn away\ncases?\nalways no Frequency\nA thwarted Mongol\ninvasion in 1274 weak-\nened the Kamakura\nregime.\nIs the invasion still tak-\ning place today?\nno yes Stationarity\nTony and Ally like to\nplay other games like\nhopscotch or jump rope\nbut that day they joined\nthe game of tag.\nDo Tony and Ally still\nenjoy jump rope?\nno no Stationarity\nJohnson is a justice on\nCalifornia’s Second Dis-\ntrict Court of Appeal.\nWhen did Johnson ar-\nrive to court?\n9:00 AM yes Typical Time\nShe had it for a long\ntime so it is now a dark\nbrown color.\nWhat time did she buy\nit?\n1:00 AM no Typical Time\nTable 7: Examples from datasets for Temporal Task based Analysis\n6763\nCoT Prompt for MC-TACO\nGiven the passage, the question, and the candidate\nanswer, the task is to determine whether the candidate\nanswer is plausible (\"yes\") or not (\"no\"). Final Label\nwill be yes or no only.\nPassage: Durer’s father died in 1502, and his mother\ndied in 1513.\nQuestion: How long was his mother ill?\nAnswer: six centuries\nResponse: Let’s think step by step. Question is asking\nabout the duration for which Durer’s mother was ill and\nanswer mentioned it is six centuries. It is not possible\nas any human can’t live for such a long period of time.\nSo the answer is not plausible.\nFinal Label: No\n###\nGiven the passage, the question, and the candidate\nanswer, the task is to determine whether the candidate\nanswer is plausible (\"yes\") or not (\"no\").\nPassage: Durer’s father died in 1502, and his mother\ndied in 1513.\nQuestion: How long was his mother ill?\nAnswer: 6 months\nResponse: Let’s think step by step. Question is asking\nabout the duration for which Durer’s mother was ill\nand answer mentioned it was 6 months. This can be\npossible as duration for illness as many illness lasts\nfor such periods only.\nFinal Label: Y es\n###\nGiven the passage, the question, and the candidate\nanswer, the task is to determine whether the candidate\nanswer is plausible (\"yes\") or not (\"no\").\nPassage: Durer’s father died in 1502, and his mother\ndied in 1513.\nQuestion: How long was his mother ill?\nAnswer: 3 minutes\nResponse: Let’s think step by step. Question is asking\nabout the duration for which Durer’s mother was ill and\nanswer mentioned it was 3 minutes which can’t be\npossible as no illness last for such small time period.\nFinal Label: No\n###\nGiven the passage, the question, and the candidate\nanswer, the task is to determine whether the candidate\nanswer is plausible (\"yes\") or not (\"no\").\nPassage: {Passage}\nQuestion: {Question}\nAnswer: {Answer}\nResponse: Let’s think step by step.\nCode Prompt for MC-TACO\nimport Question_answering\nclass Context:\n'''Strictly return only \"0\" or \"1\" for the\ngiven answer to the question, based on context\nand type of answer, task is to determine whether\nthe given candidate answer is\nplausible (\"1\") or not (\"0\").'''\ndef __init__(self, question,context,answer,type):\nself.context = context # The context\nself.question = question # The question\nself.answer = answer # answer\nself.type = type # type of the answer\ndef get_answer(self):\nans = Question_answering(self.question,\nself.context,\nself.answer,\nself.type)\nreturn ans #Strictly return 0/1 only\ncontext = Context(\ncontext = \"{}\",\nquestion = \"{}\"\nanswer = \"{}\"\ntype = \"{}\"\n)\nassert(context.get_answer\n== .format(question,context,answer,type)\n6764\nB.3.2 Prompts for TimeDial\nFew-shot Prompt for TimeDial\nY ou are given a conversation between two persons.\nEach conversation has a fill-in-the-blank in them which\nis represented by a <MASK> token. Y ou are also\ngiven a candidate answer for that <MASK> token. Y our\ntask is to determine whether the candidate answer is\nplausible (Y es) or not (No). Return your answer in Y es\nand No only.\nConversation:\nA:We need to take the accounts system offline to carry\nout the upgrade . But don’t worry , it won’t cause too\nmuch inconvenience . We’re going to do it over the\nweekend.\nB: How long will the system be down for?\nA: We’ll be taking everything offline in about two hours\n’ time . It’ll be down for a minimum of twelve hours .\nIf everything goes according to plan , it should be up\nagain by 6 pm on Saturday.\nB: That’s fine . We’ve allowed <MASK> to be on the\nsafe side.\nAnswer: forty-eight hours\nLabel:Y es\n###\nConversation:\nA:Mr . Emory , I ’ d like to take this afternoon off if it’s\nall right with you.\nB: But Steven , you’ve called in sick 3 times during\n<MASK>.\nA: I know , Mr . Emory . I’m sorry . But I really need to\nsee the doctor this afternoon. I feel dizzy and I can’t\nconcentrate on my work.\nB: All right , then . But don’t forget to bring a doctors\nnote tomorrow.\nA: OK , thank you !\nAnswer: last 15 seconds\nLabel:No\n###\nConversation: {Conversation}\nAnswer: {Answer}\nLabel:\nZero-shot Prompt for TimeDial\nY ou are given a conversation between two persons.\nEach conversation has a fill-in-the-blank in them which\nis represented by a <MASK> token. Y ou are also\ngiven a candidate answer for that <MASK> token. Y our\ntask is to determine whether the candidate answer is\nplausible (Y es) or not (No). Strictly return your answer\nin Y es and No only.\nConversation: {Conversation}\nAnswer: {Answer}\nLabel:\nCoT Prompt for TimeDial\nY ou are given a conversation between two persons.\nEach conversation has a fill-in-the-blank in them which\nis represented by a <MASK> token. Y ou are also\ngiven a candidate answer for that <MASK> token. Y our\ntask is to determine whether the candidate answer\nis plausible (Y es) or not (No) by first generating first\nreasoning and then final label as Y es or No.\nConversation:\nA:We need to take the accounts system offline to carry\nout the upgrade . But don’t worry , it won’t cause too\nmuch inconvenience . We’re going to do it over the\nweekend.\nB: How long will the system be down for?\nA: We’ll be taking everything offline in about two hours\n’ time . It’ll be down for a minimum of twelve hours .\nIf everything goes according to plan , it should be up\nagain by 6 pm on Saturday.\nB: That’s fine . We’ve allowed <MASK> to be on the\nsafe side .\nAnswer: forty-eight hours\nLabel: Let’s think step by step. The conversation\nseems to be taking place in an office environment\nwhere speaker A says that they will take systems down\nfor 12 hrs over the weekend for maintenance. and\nspeaker B says they are allowed <mask> hrs without\nsystems and answer here says mask should be 48 hrs.\nThis answers seems to be correct as it is weekend\nwhich means for 48 hrs they don’t need to work on\nthese systems.\nFinal Label:Y es\n###\nConversation:\nA:Mr . Emory , I ’ d like to take this afternoon off if it’s\nall right with you.\nB: But Steven , you’ve called in sick 3 times during\n<MASK>.\nA: I know , Mr . Emory . I’m sorry . But I really need to\nsee the doctor this afternoon. I feel dizzy and I can’t\nconcentrate on my work.\nB: All right , then . But don’t forget to bring a doctors\nnote tomorrow.\nA: OK , thank you !\nAnswer: last 15 seconds\nLabel: Let’s think step by step. The conversation\nseems to be taking place in an office environment\nwhere speaker A is asking for a leave to speaker B\nand Speaker B said that he already took 3 leaves dur-\ning <MASK> time period. The answer here says its\n15 seconds which is not possible as leaves are taken\nduring a week or a month. That’s why this answer is\nincorrect.\nFinal Label:No\n###\nConversation: {Conversation}\nAnswer: {Answer}\nLabel: Let’s think step by step.\n6765\nCode Prompt for TimeDial\nimport Fill_blank_from_conversation\nclass Conversation:\n'''Conversation between two persons is given\nand each conversation has a fill-in-the-blank\nin them which is represented by a <MASK> token.\nYou are also given a candidate answer\nfor that <MASK> token. Determine whether the\ncandidate answer is plausible (\"1\") or not (\"0\").\nStrictly return \"0\" or \"1\" only.'''\ndef __init__(self,conversation,answer):\nself.conversation = conversation\n# Converstion between two persons\nself.answer = answer\n# answer to the fill in the blank\ndef get_answer(self):\nans = Fill_blank_from_conversation(\nself.conversation,\nself.answer)\nreturn ans #Strictly return 0/1 only\nconversation = Conversation(\nconversation = \"{}\",\nanswer = \"{}\"\n)\nassert(conversation.get_answer\n== .format(conversation,answer)\nB.3.3 Prompt for TNLI\nFew-shot Prompt for TNLI\nY ou are given two sentences, Sentence 1, and Sen-\ntence 2 where Sentence 1 is a hypothesis, and Sen-\ntence 2 is a premise sentence. The task is to assign\none of the following three classes to Sentence 1 based\non the inference using the content of Sentence 2. The\nlabels are Support, Invalidate, and Neutral. The Sup-\nport class means that Sentence 1 is still valid given\nthe information in Sentence 2. The Invalidate class,\non the other hand, means that Sentence 1 ceased to\nbe valid in view of Sentence 2. The third one, Neutral\nclass, indicates that the situation evidence is not con-\nclusive or clear, and we cannot verify the validity of the\nhypothesis.\nSentence 1: A female is scrambling eggs in a bowl.\nSentence 2: Eggs are scrambled in a bowl.\nLabel: Support\n###\nSentence 1: A group of people sing and dance at a\nconcert.\nSentence 2: A group of people going to take rest.\nLabel: Invalidate\n###\nSentence 1: The horses race on the dirt track while\ntheir riders urge them on.\nSentence 2: Most people enjoy watching horse racing.\nLabel: Neutral\n###\nSentence 1: {Sentence 1}\nSentence 2: {Sentence 2}\nLabel:\nZero-shot Prompt for TNLI\nY ou are given two sentences, Sentence 1, and Sen-\ntence 2 where Sentence 1 is a hypothesis, and Sen-\ntence 2 is a premise sentence. The task is to assign\none of the following three classes to Sentence 1 based\non the inference using the content of Sentence 2. The\nlabels are Support, Invalidate, and Neutral. The SUP-\nPORT class means that Sentence 1 is still valid given\nthe information in Sentence 2. The INVALIDATE class,\non the other hand, means that Sentence 1 ceased to\nbe valid in view of Sentence 2. The third one, Neutral\nclass, indicates that the situation evidence is not con-\nclusive or clear, and we cannot verify the validity of the\nhypothesis.\nSentence 1: {Sentence 1}\nSentence 2: {Sentence 2}\nLabel:\nCoT Prompt for TNLI\nY ou are given two sentences, Sentence 1, and Sen-\ntence 2 where Sentence 1 is a hypothesis, and Sen-\ntence 2 is a premise sentence. The task is to assign\none of the following three classes to Sentence 1 based\non the inference using the content of Sentence 2. The\nlabels are Support, Invalidate, and Neutral. The SUP-\nPORT class means that Sentence 1 is still valid given\nthe information in Sentence 2. The INVALIDATE class,\non the other hand, means that Sentence 1 ceased to\nbe valid in view of Sentence 2. The third one, Neutral\nclass, indicates that the situation evidence is not con-\nclusive or clear, and we cannot verify the validity of the\nhypothesis.\nSentence 1: A group of people sing and dance at a\nconcert\nSentence 2: A group of people going to take rest.\nLabel: Let’s think step by step. In Sentence 2, it is\nmentioned that group of people are taking rest. This\nimplies that they won’t perform any activity. But in Sen-\ntence 1 it is mentioned that group of people are singing\nand dancing at concert. But based on information from\nSentence 1, it is not possible as they are taking rest.\nSo the final label should be Invalidate.\nLabel:Invalidate\n###\nSentence 1: A female is scrambling eggs in a bowl.\nSentence 2: Eggs are scrambled in a bowl.\nLabel: Let’s think step by step. In Sentence 2, it is\nmentioned that eggs are scrambled in a bowl. How-\never, in Sentence 1, a female is scrambling eggs in\na bowl which supports the statement of Sentence 2\nthat eggs are scrambled. So the final label should be\nSupport.\nLabel:Support\n###\nSentence 1: The horses race on the dirt track while\ntheir riders urge them on.\nSentence 2: Most people enjoy watching horse racing.\nLabel: Let’s think step by step. In Sentence 2,it is\nmentioned that Most people enjoy watching horse rac-\ning. However, in Sentence 1, The horses race on the\ndirt track while their riders urge them on. Both these\nstatements are neither supporting each other nor in-\nvalidating each other. So the final label should be\nNeutral.\nLabel:Neutral\n###\nSentence 1: {Sentence 1}\nSentence 2: {Sentence 2}\nLabel: Let’s think step by step.\n6766\nCode Prompt for TNLI\nimport neuralnli\nclass NaturalLanguageInference():\n'''function to answer the natural language\ninference task given premise and hypothesis.''''\ndef __init__(self):\nself.model = neuralnli()\ndef forward(self, premise, hypothesis):\nanswer = self.model(premise,\nhypothesis)['answer']\nreturn answer\nnli_model = NaturalLanguageInference()\npremise = \"{}\"\nhypothesis = \"{}.\n#Invalidate, Support, or Neutral?\"\nanswer = nli_model.forward(premise, hypothesis)\nassert answer ==.format(statement2,statement1)\nB.3.4 Prompts for WikiHow dataset\nFew-shot Prompt for WikiHow\nY ou are given a goal and steps to accomplish that goal.\nY our task is to determine whether the steps are in right\norder (Y es) or not (No). Return your answer as Y es\nand No only.\nGoal: How to Select a Dog Bed - Understanding Dif-\nferent Types of Beds\nSteps: Buy a mat for the easiest solution. Pick a pillow\nbed for a large dog. Select a donut bed if your dog\nlikes to feel secure. Purchase a nest bed for cuddling\ncomfort. Buy a bolster-type bed if your dog is a leaner.\nLook for a cave-style bed if your dog likes to burrow.\nConsider a hammock bed for ease of cleaning.\nAnswer: No\n###\nGoal: How to Get Married in Oregon - Planning a\nWedding Ceremony\nSteps: Decide on the type of ceremony. Choose a\nseason. Hire wedding vendors. Confirm the date with\nvendors and officiants. Make final payments.\nAnswer: Y es\n###\nGoal: {Goal}\nSteps: {Steps}\nAnswer:\nZero-shot Prompt for WikiHow\nY ou are given a goal and steps to accomplish that goal.\nY our task is to determine whether the steps are in right\norder (Y es) or not (No). Return your answer in Y es and\nNo only.\nGoal: {Goal}\nSteps: {Steps}\nAnswer:\nCoT Prompt for WikiHow\nY ou are given a goal and steps to accomplish that goal.\nY our task is to determine whether the steps are in right\norder (Y es) or not (No). Return your answer in Y es and\nNo only.\nGoal:How to Select a Dog Bed - Understanding Differ-\nent Types of Beds\nSteps:Buy a mat for the easiest solution. Pick a pillow\nbed for a large dog. Select a donut bed if your dog\nlikes to feel secure. Purchase a nest bed for cuddling\ncomfort. Buy a bolster-type bed if your dog is a leaner.\nLook for a cave-style bed if your dog likes to burrow.\nConsider a hammock bed for ease of cleaning.\nAnswer: Let’s think step by step. The correct order\nshould be: Look for a cave-style bed if your dog likes\nto burrow. Select a donut bed if your dog likes to feel\nsecure. Purchase a nest bed for cuddling comfort. Buy\na bolster-type bed if your dog is a leaner. Consider a\nhammock bed for ease of cleaning. Buy a mat for the\neasiest solution. Pick a pillow bed for a large dog. As\nthis sequence is not in match with given steps, so the\nfinal answer is No.\nAnswer: No\n###\nGoal:How to Get Married in Oregon - Planning a Wed-\nding Ceremony\nSteps:Decide on the type of ceremony. Choose a\nseason. Hire wedding vendors. Confirm the date with\nvendors and officiants. Make final payments.\nAnswer: Let’s think step by step. The correct order\nshould be: To get married, one first need to decide a\nceremony. Then choose a season. Then hire a wed-\nding vendor for organization. Confirm and finalize a\ndate. Then make the final payments. As this sequence\nis in the match with given steps, so the final answer is\nY es.\nAnswer: Y es\n###\nGoal: {Goal}\nSteps: {Steps}\nAnswer: Let’s think step by step.\nCode Prompt for WikiHow\nimport order_steps\nclass Event:\n'''Given a goal and steps to achieve, determine\nwhether the steps are in right order or not. Return\nYes if right order and No if order is wrong.'''\ndef __init__(self, goal, steps):\nself.goal = goal\n'''The goal that someone\nis trying to accomplish'''\nself.steps = steps # All the steps\ndef get_order_of_steps(self):\n# Output a Binary response Yes or no\nreturn order_steps(self.goal, self.steps)\nevent = Event(\ngoal = \"{goal}\"\nsteps = \"{steps}\"\n)\nassert(event.get_order_of_steps\n== <fim-suffix>.format(goal,steps)\n6767\nB.3.5 Prompts for BIG-bench\nFew-shot Prompt for BIG-bench\nQ: Today, Emily went to the museum. Between what\ntimes could they have gone?\nWe know that: Emily woke up at 1pm. Elizabeth saw\nEmily reading at the library from 2pm to 4pm. Jessica\nsaw Emily watching a movie at the theater from 4pm\nto 5pm. Leslie saw Emily waiting at the airport from\n5pm to 6pm. William saw Emily buying clothes at the\nmall from 6pm to 7pm. The museum was closed after\n7pm.\nBetween what times could Emily have gone to the\nmuseum?\nOptions:\n(A) 1pm to 2pm\n(B) 6pm to 7pm\n(C) 5pm to 6pm\n(D) 2pm to 4pm\nStrictly return the correct option which means return\nthe letter of choice only\nAns:A\n###\nQ: Today, Tiffany went to the beach. Between what\ntimes could they have gone?\nWe know that: Tiffany woke up at 5am. Betty saw\nTiffany getting a coffee at the cafe from 5am to 6am.\nJessica saw Tiffany working at the office from 6am to\n9am. John saw Tiffany stretching at a yoga studio from\n9am to 12pm. Sean saw Tiffany sitting on a rooftop\nfrom 12pm to 2pm. Sarah saw Tiffany playing tennis\nat the tennis court from 2pm to 3pm. The beach was\nclosed after 4pm.\nBetween what times could Tiffany have gone to the\nbeach?\nOptions:\n(A) 9am to 12pm\n(B) 12pm to 2pm\n(C) 5am to 6am\n(D) 3pm to 4pm\nStrictly return the correct option which means return\nthe letter of choice only\nAns:D\n###\ninput: {input}\nStrictly return the correct option which means return\nthe letter of choice only\nAns:\nZero-shot Prompt for BIG-bench\nTask description: Answer questions about which times\ncertain events could have occurred Always return op-\ntion letter at the end. There won’t be any case when\nanswer will be none of the options. Return the correct\noption only A,B,C or D.\nInput: {Input}\nAns:\nCoT Prompt for BIG-bench\nTask description: Answer questions about which times\ncertain events could have occurred.\nQ: Today, Emily went to the museum. Between what\ntimes could they have gone?\nWe know that: Emily woke up at 1pm. Elizabeth saw\nEmily reading at the library from 2pm to 4pm. Jessica\nsaw Emily watching a movie at the theater from 4pm\nto 5pm. Leslie saw Emily waiting at the airport from\n5pm to 6pm. William saw Emily buying clothes at the\nmall from 6pm to 7pm. The museum was closed after\n7pm.\nBetween what times could Emily have gone to the\nmuseum?\nOptions:\n(A) 1pm to 2pm\n(B) 6pm to 7pm\n(C) 5pm to 6pm\n(D) 2pm to 4pm\nAnswer: Let’s think step by step. Wake-up time: 1pm.\n1pm-2pm: free. 2pm-4pm: reading at the library. 4pm-\n5pm: watching a movie at the theater. 5pm-6pm: wait-\ning at the airport. 6pm-7pm: buying clothes at the mall.\nThe museum closure time: 7pm. The only time when\nEmily could have gone to the museum was 1pm to\n2pm. So the answer is (A).\nAnswer: (A)\n###\nInput: {Input}\nAnswer: Let’s think step by step.\nCode Prompt for BIG-bench\nimport Scheduling_question_answering\nclass Context:\n'''choose the right option number for\nthe question depending on the context'''\ndef __init__(self, question,context,options):\nself.context = context # The context\nself.question = question # The question\nself.options = options # options\ndef get_answer(self):\nanswer = Scheduling_question_answering(\nself.question, self.context,self.options)\nreturn answer\ncontext = Context(\ncontext = \"{}\",\nquestion = \"{}\"\nOptions = \"{}\"\n)\nassert(context.get_answer\n==.format(listToString(Context),Question,options)\nB.3.6 Prompts for TimeQA\nZero-shot Prompt for TimeQA\nAnswer the question based on the context.\nContext: {Context}\nQuestion: {Question}\nAnswer:.\n6768\nCoT Prompt for TimeQA\nAnswer the question based on the context. Answer will\nbe in the context. First generate a reasoning and then\nprovide final answer in a new line as Final Answer:.\nContext: {Context}\nQuestion: {Question}\nAnswer: Let’s think step by step.\nCode Prompt for TimeQA\nimport Question_answering\nclass Context:\n'''Return answer for this question\nbased on context'''\ndef __init__(self,context,question):\nself.context = context # The context\nself.question = question # The question\ndef get_answer(self):\nans = Question_answering(self.context,\nself.question)\nreturn ans\ncontext = Context(\ncontext = \"{}\",\nquestion = \"{}\"\n)\nassert(context.get_answer ==.format(context,\nquestion)\nB.4 Dataset Samples\nExample instances for all the dataset present in this\nstudy.\nB.4.1 MC-TACO\nMC-TACOdataset instance examples\nContext: Durer’s father died in 1502, and\nhis mother died in 1513.\nQuestion: How long was his mother ill?\nAnswer: she was ill for 30 seconds\nLabel: no\nTemporal Reasoning: Event Duration\nContext: Safti admits his love for Edwina\nto Lord Esketh , who is now sympathetic\ntoward this good man’s plight.\nQuestion: Has Safti always been in love\nwith Edwina?\nAnswer: no this ’ s a new thing\nLabel: yes\nTemporal Reasoning: Stationarity\nContext: The next evening, she arrived\nwith a stack of glistening stopboxes contain-\ning sushi, sashimi, oysters in their shells,\nand Terran vegetables fresh plucked from\ntheir hydroponic beds.\nQuestion: At what time did she arrive?\nAnswer: 6:00 PM\nLabel: yes\nTemporal Reasoning: Typical Time\nContext: The CIA now estimates that it\ncost al Qaeda about $30 million per year to\nsustain its activities before 9/11 and that this\nmoney was raised almost entirely through\ndonations.\nQuestion: What happened to al Qaeda’s\nfinances after 9/11?\nAnswer: they were dealt a big blow\nLabel: yes\nTemporal Reasoning: Event Ordering\nContext: This is an astonishing new record\nfor a coin, he said.\nQuestion: How often are new records es-\ntablished?\nAnswer: three times an second\nLabel: no\nTemporal Reasoning: Frequency\n6769\nB.4.2 TimeDial\nTimeDial dataset instance examples\nConversation: A:We need to take the ac-\ncounts system offline to carry out the up-\ngrade . But don’t worry , it won’t cause too\nmuch inconvenience . We’re going to do it\nover the weekend .\nB: How long will the system be down for ?\nA: We’ll be taking everything offline in\nabout two hours ’ time . It’ll be down for\na minimum of twelve hours . If everything\ngoes according to plan , it should be up\nagain by 6 pm on Saturday .\nB: That’s fine . We’ve allowed <MASK> to\nbe on the safe side .\nAnswer: forty-eight hours\nLabel: 1\nConversation: A:Excuse me , Miss .\nB: Yes . May I help you ?\nA: I’m a graduate student here in mathemat-\nics . I’ve just come from China and I’ve\nnever used a western library before . I’ll be\nhere for <MASK> , so I’d like to learn to\nuse the library as efficiently as possible . I\nwonder if someone might have time to show\nme around .\nB: I’d be very glad to show you around , but\nI’m very busy right now . Could you come\nback about 3 thirty ?\nA: Sure . 3 thirty this afternoon .\nB: Good . See you later .\nA: Thank you . Good-bye .\nAnswer: 3 decades\nLabel: 0\nB.4.3 TNLI\nTNLI dataset instance examples\nSentence 1: The woman wearing the pink\njacket has thrown a Frisbee for the dog to\ncatch.\nSentence 2: The dog falling into a lake\ntrying to catch the frisbee.\nLabel: Support\nSentence 1: A young boy skipping down a\ntennis court in absolute glee.\nSentence 2: He is now at a volleyball court.\nLabel: Invalidate\nSentence 1: A man sitting on sidewalk with\nshirt over his head.\nSentence 2: Most people prefer sitting to\nstanding.\nLabel: Neutral\nB.4.4 WikiHow\nWikiHow dataset instance examples\nGoal: How to Buy a Used Sailboat - Engine\nSteps: Steer clear of rare or very old en-\ngines unless you’re certain there’s an ad-\nequate supply of parts. Do the Smoke\nTest: healthy diesels make small amounts\nof black smoke with some white on cold\nstarts. Check for fuel leaks and a working\nbilge blower in gasoline engines. Before the\nseller cranks the engine, check to see if it is\nalready warm.\nOrdered?: 0\nGoal: How to Breed Alpacas - Encouraging\nReproduction\nSteps: Expose the breeding male to the fe-\nmale. Induce ovulation in the female al-\npaca. Place the male and female alpaca in\nthe breeding pen. Separate the alpacas if the\nfemale is not receptive. Wait a week or two\nafter copulation to re-mate alpacas.\nOrdered?: 1\n6770\nB.4.5 BIG-bench\nBIG-bench dataset instance examples\nInput: Today, James went to the beach. Be-\ntween what times could they have gone?\nWe know that: James woke up at 5am. Sean\nsaw James walking towards the Statue of\nLiberty from 5am to 6am. Michael saw\nJames driving to the water park from 6am to\n7am. Anthony saw James reading at the li-\nbrary from 7am to 3pm. William saw James\ngetting a coffee at the cafe from 4pm to 9pm.\nThe beach was closed after 9pm.\nBetween what times could James have gone\nto the beach?\nOptions:\n(A) 7am to 3pm\n(B) 5am to 6am\n(C) 4pm to 9pm\n(D) 3pm to 4pm\nAnswer: (D)\nInput: Today, David went to the art studio.\nBetween what times could they have gone?\nWe know that: David woke up at 5am.\nLinda saw David watching a movie at the\ntheater from 5am to 7am. James saw David\nbuying lunch at the deli from 9am to 10am.\nMary saw David buying a phone at the elec-\ntronics store from 10am to 11am. Leslie\nsaw David driving to the water park from\n11am to 2pm. Jessica saw David buying a\nbike at the bike shop from 2pm to 7pm. The\nart studio was closed after 7pm.\nBetween what times could David have gone\nto the art studio?\nOptions:\n(A) 7am to 9am\n(B) 2pm to 7pm\n(C) 5am to 7am\n(D) 11am to 2pm\nAnswer: (A)\nB.4.6 TimeQA\nTimeQA dataset instance examples\nContext: HMAS Wollongong ( J172 )\nHMAS Wollongong ( J172 ) , named for\nthe city of Wollongong , New South Wales\n, was one of 60 s constructed during World\nWar II and one of 20 built for the Admi-\nralty but manned by personnel of and com-\nmissioned into the Royal Australian Navy (\nRAN ) . Design and construction . In 1938 ,\nthe Australian Commonwealth Naval Board\n( ACNB ) identified the need for a general\npurpose local defence vessel capable of both\nanti-submarine and mine-warfare duties ,\nwhile easy to construct and operate . The\nvessel was initially envisaged as having a\ndisplacement of approximately 500 tons , a\nspeed of at least , and a range of The oppor-\ntunity to build a prototype in the place of\na cancelled Bar-class boom defence vessel\nsaw the proposed design increased to a 680-\nton vessel , with a top speed , and a range\nof , armed with a 4-inch gun , equipped\nwith asdic , and able to fitted with either\ndepth charges or minesweeping equipment\ndepending on the planned operations : al-\nthough closer in size to a sloop than a local\ndefence vessel , the resulting increased ca-\npabilities were accepted due to advantages\nover British-designed mine warfare and anti-\nsubmarine vessels. ...\nQuestion: Which Navy operated the war-\nship HMAS Wollongong from 1950 to\n1951?\nAnswer: Indonesian Navy\nB.5 Detailed Tables for Model and Prompt\nbased Analysis\nTable 8 compares the performance of eight Large\nLanguage Models on six datasets, analyzed under\ntwo different prompting strategies across F1 score\nand accuracy. Table 9 compares the performance of\nInstruction Tuned LLMs with CoT prompting strat-\negy across both F1 and Accuracy. Table 10 com-\npares the performance of Code Generation LMs\nwith Code prompts across all datasets on both F1\nand accuracy.\n6771\nMC-TACO TNLI TimeDial WikiHow BIG-bench TimeQAFew-shotZero-shotFew-shotZero-shotFew-shotZero-shotFew-shotZero-shotFew-shotZero-shotZero-shotModelAcc/F1 Acc/F1 Acc/F1 Acc/F1 Acc/F1 Acc/F1 Acc/F1 Acc/F1 Acc/F1 Acc/F1 EM/F1GPT-3.50.8/0.80.75/0.72(⇓5%)0.62/0.620.5/0.49(⇓12%)0.65/0.640.65/0.65(=0%)0.55/0.510.49/0.45 (⇓6%)0.25/0.240.26/0.23 (⇑1%) 0.1/0.21FLAN-T50.7/0.740.74/0.71 (⇑4%)0.39/0.30.37/0.25 (⇓2%)0.54/0.480.54/0.49 (=0%)0.45/0.30.45/0.28 (=0%)0.16/0.150.14/0.13 (⇓2%) 0.4/0.5BLOOMZ0.59/0.60.63/0.59 (⇑4%)0.34/0.20.31/0.21 (⇓3%)0.49/0.440.46/0.44 (⇓3%)0.53/0.410.46/0.29 (⇓7%)0.28/0.130.28/0.12 (=0%) -Dolly0.45/0.470.52/0.53 (⇑7%)0.42/0.370.32/0.16 (⇓10%)0.5/0.390.49/0.45 (⇓1%)0.45/0.370.44/0.29 (⇓1%)0.23/0.170.27/0.11 (⇑4%) 0/0.18GPT-J0.59/0.60.37/0.27 (⇓22%)0.34/0.20.33/0.16 (⇓1%)0.45/0.330.33/0.33 (⇓12%)0.46/0.30.45/0.3 (⇓1%)0.26/0.220.26/0.24(=0%) -GPT Neo0.59/0.580.37/0.32 (⇓22%)0.32/0.230.33/0.22 (⇑1%)0.49/0.430.48/0.38 (⇓1%)0.47/0.330.44/0.39 (⇓3%)0.26/0.130.27/0.12 (⇑1%) -LLaMA0.65//0.550.65/0.53 (=0%)0.32/0.210.32/0.26 (=0%)0.5/0.450.35/0.26 (⇓15%)0.55/0.520.54/0.52(⇓1%)0.28/0.130.18/0.16 (⇓10%)0.1/0.12OPT 0.65/0.540.45/0.46 (⇓20%)0.33/0.160.35/0.26 (⇑2%)0.5/0.380.49/0.33 (⇓1%)0.46/0.30.49/0.43 (⇑3%)0.24/0.10.23/0.10 (⇓1%) -\nTable 8: The performance of eight Large Language Models on six datasets, analyzed under two different prompting\nstrategies. \"Acc\" stands for Accuracy, \"F1\" signifies Weighted-F1 score, and \"EM\" corresponds to Exact Match.\nThe percentage changes in accuracy performance between zero-shot and Few-shot prompting are indicated in\nparentheses.\nMC-Taco TNLI TimeDial WikiHow BIG-bench TimeQA\nModel Acc/F1 Acc/F1 Acc/F1 Acc/F1 Acc/F1 EM/F1\nGPT-3.50.82/0.82(⇑2%) 0.46/0.45(⇓16%) 0.7/0.69(⇑5%) 0.54/0.53(⇓1%) 0.67/0.67(⇑42%)0.15/0.22\nFLAN-T50.73/0.74 (⇑3%) 0.38.0.3 (⇓1%) 0.54/0.48 (0%)0.48/0.46 (⇑3%) 0.21/0.21 (⇑5%) 0.05/0.15\nBLOOMZ0.45/0.45 (⇓14%) 0.36/0.29 (⇑2%) 0.53/0.52 (⇑4%) 0.5/0.48 (⇓3%) 0.3/0.22 (⇑2%) -\nDolly 0.48/0.48 (⇑3%) 0.34/0.2 (⇓8%) 0.5/0.47 (0%)0.47/0.37 (⇑2%) 0.29/0.21 (⇑6%) 0/0.15\nTable 9: Performance of Instruction Tuned LLMs with CoT prompting strategy. The percentage changes in accuracy\nperformance between CoT and Few-shot prompting (from Table 8) are indicated in parentheses.\nB.6 Detailed Table for Temporal Aspect\nBased Analysis\nTable 11 compares the performance of LLMs\nacross different temporal expressions. Table 12\ncompares the performance of LLMs across past\nand future reasoning events. Table 13 compares\nthe performance of LLMs across different time-\nframes. Table 14 compares the performance of\nLLMs across multiple events and a single event. Ta-\nble 15 compares the performance of LLMs across\ndifferent context lengths.\nB.7 Detailed Table for Temporal Task-based\nAnalysis\nTable 16 compares the performance of eight Large\nLanguage Models on MC-TACO dataset and its\nfine-grained temporal task categories across both\nF1 and accuracy metrics.\n6772\nMC-TACOTNLI TimeDialWikiHowBIG-benchTimeQA\nModel Acc/F1 Acc/F1 Acc/F1 Acc/F1 Acc/F1 EM/F1\nGPT-3.5 0.5/0.5 0.45/0.410.49/0.430.45/0.29 0.29/0.29 0.1/0.13\nSantaCoder0.5/0.51 0.36/0.29 0.5/0.5 0.45/0.29 0.27/0.19 -\nCodeGen2 0.61/0.6 0.35/0.250.5/0.41 0.45/0.29 0.25/0.19 0.1/0.13\nTable 10: Performance of Code Generation LMs with Code prompts in zero-shot setting\nED EO F S TT\nModel ExactAmbiguousExactAmbiguousExactAmbiguousExactAmbiguousExactAmbiguous\nGPT-3.50.87 0.89 0.84 0.85 0.83 0.85 0.76 0.79 0.62 0.81\nFLAN T50.81 0.85 0.7 0.73 0.77 0.79 0.58 0.79 0.56 0.77\nLLaMA 0.75 0.74 0.5 0.54 0.41 0.61 0.37 0.57 0.63 0.63\nTable 11: Performance of LLMs in accuracy across different temporal expressions in few-shot setting (ED: Event\nDuration, EO: Event Ordering, F: Frequency, S: Stationarity, TT: Typical Time)\nModel Past Future\nGPT-3.5 0.62 0.60\nFLAN-T5 0.55 0.50\nLLaMA 0.52 0.50\nTable 12: Performance of LLMs in accuracy on past vs. future based reasoning tasks in few-shot setting\nModel Seconds Minutes Hours 1 Day More than a Day\nGPT-3.5 0.5 0.65 0.6 0.69 0.7\nFLAN-T5 0.43 0.48 0.55 0.5 0.56\nLLaMA 0.43 0.48 0.52 0.55 0.5\nTable 13: Performance of LLMs in accuracy across different Timeframes in few-shot setting\nED EO F S TT\nModel ME SE ME SE ME SE ME SE ME SE\nGPT-3.5 0.91 0.82 0.75 0.88 0.9 0.88 0.7 0.8 0.68 0.68\nFLAN-T5 0.85 0.8 0.57 0.75 0.89 0.79 0.5 0.72 0.58 0.6\nLLaMA 0.72 0.41 0.5 0.5 0.78 0.6 0.4 0.62 0.44 0.54\nTable 14: Performance of LLMs in accuracy across single and multiple events in few-shot setting. (ME: Multiple\nevents, SE: Single Events, ED: Event Duration, EO: Event Ordering, F: Frequency, S: Stationarity, TT: Typical\nTime)\nModel 0-200 200-400 400-600\nGPT-3.5 0.66 0.63 0.58\nFLAN-T5 0.53 0.55 0.55\nLLaMA 0.51 0.48 0.46\nTable 15: Performance of LLMs in accuracy across different context length in few-shot setting\n6773\nMC MC-ED MC-EO MC-F MC-S MC-TT\nFS ZS CoT FS ZS CoT FS ZS CoT FS ZS CoT FS ZS CoT FS ZS CoT\nModelAcc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 ACC F1 ACC F1 ACC F1\nGPT-3.50.8 0.8 0.75 0.72 0.82 0.820.85 0.85 0.80.770.87 0.860.83 0.82 0.77 0.76 0.84 0.840.84 0.82 0.82 0.8 0.85 0.850.77 0.77 0.69 0.67 0.76 0.750.69 0.67 0.620.530.71 0.69\nFLAN-T50.7 0.74 0.74 0.71 0.73 0.740.8 0.770.8 0.780.78 0.770.73 0.71 0.76 0.74 0.76 0.760.77 0.73 0.79 0.75 0.76 0.730.7 0.69 0.68 0.66 0.62 0.60.62 0.530.62 0.560.6 0.54\nBLOOMZ0.59 0.6 0.63 0.59 0.45 0.450.61 0.620.8 0.76 0.35 0.320.59 0.590.77 0.760.6 0.6 0.63 0.63 0.83 0.8 0.45 0.470.51 0.5 0.68 0.66 0.43 0.330.53 0.53 0.61 0.5 0.5 0.5\nDolly0.45 0.47 0.52 0.53 0.48 0.480.45 0.46 0.5 0.52 0.46 0.480.50 0.50 0.55 0.55 0.52 0.480.49 0.50 0.52 0.55 0.48 0.50.49 0.48 0.61 0.61 0.53 0.510.52 0.51 0.52 0.5 0.48 0.45\nGPT-J0.59 0.6 0.37 0.27 - - 0.58 0.6 0.32 0.25 - - 0.61 0.61 0.5 0.38 - - 0.65 0.64 0.3 0.21 - - 0.56 0.55 0.42 0.37 - - 0.55 0.54 0.45 0.31 - -\nGPT Neo0.59 0.58 0.37 0.32 - - 0.66 0.64 0.29 0.19 - - 0.52 0.51 0.5 0.5 - - 0.61 0.62 0.31 0.25 - - 0.5 0.49 0.44 0.36 - - 0.54 0.49 0.5 0.48 - -\nLLaMA0.65 0.55 0.65 0.53 - - 0.73 0.65 0.65 0.73 - - 0.54 0.48 0.53 0.4 - - 0.71 0.64 0.73 0.62 - - 0.57 0.49 0.54 0.43 - - 0.56 0.42 0.56 0.41 - -\nOPT 0.65 0.54 0.45 0.46 - - 0.73 0.63 0.35 0.32 - - 0.52 0.36 0.47 0.42 - - 0.75 0.63 0.55 0.56 - - 0.46 0.45 0.45 0.31 - - 0.55 0.42 0.48 0.47 - -\nTable 16: Performance of eight Large Language Models on MC-TACO dataset and it’s fine-grained temporal task\ncategories (MC: MC-TACO, FS: Few-Shot, ZS: Zero-Shot)\n6774",
  "topic": "Benchmarking",
  "concepts": [
    {
      "name": "Benchmarking",
      "score": 0.7050283551216125
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6106938123703003
    },
    {
      "name": "Computer science",
      "score": 0.5427728891372681
    },
    {
      "name": "Commonsense reasoning",
      "score": 0.5340601205825806
    },
    {
      "name": "Language model",
      "score": 0.45420902967453003
    },
    {
      "name": "Scope (computer science)",
      "score": 0.4220018684864044
    },
    {
      "name": "Data science",
      "score": 0.40862125158309937
    },
    {
      "name": "Natural language processing",
      "score": 0.37437427043914795
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33603227138519287
    },
    {
      "name": "Geography",
      "score": 0.07204613089561462
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I132153292",
      "name": "Indian Institute of Technology Patna",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I190249584",
      "name": "Universität Innsbruck",
      "country": "AT"
    },
    {
      "id": "https://openalex.org/I4210162141",
      "name": "Microsoft (India)",
      "country": "IN"
    }
  ],
  "cited_by": 12
}