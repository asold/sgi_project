{
  "title": "STAR: Boosting Low-Resource Information Extraction by Structure-to-Text Data Generation with Large Language Models",
  "url": "https://openalex.org/W4393156784",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2950866830",
      "name": "Mingyu Derek Ma",
      "affiliations": [
        "UCLA Health"
      ]
    },
    {
      "id": "https://openalex.org/A2110538918",
      "name": "Xiaoxuan Wang",
      "affiliations": [
        "UCLA Health"
      ]
    },
    {
      "id": "https://openalex.org/A5081617623",
      "name": "Po-Nien Kung",
      "affiliations": [
        "UCLA Health"
      ]
    },
    {
      "id": "https://openalex.org/A371738152",
      "name": "P. JEFFREY BRANTINGHAM",
      "affiliations": [
        "UCLA Health"
      ]
    },
    {
      "id": "https://openalex.org/A2147504447",
      "name": "Nanyun Peng",
      "affiliations": [
        "UCLA Health"
      ]
    },
    {
      "id": "https://openalex.org/A2009336559",
      "name": "Wei Wang",
      "affiliations": [
        "UCLA Health"
      ]
    },
    {
      "id": "https://openalex.org/A2950866830",
      "name": "Mingyu Derek Ma",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2110538918",
      "name": "Xiaoxuan Wang",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A5081617623",
      "name": "Po-Nien Kung",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A371738152",
      "name": "P. JEFFREY BRANTINGHAM",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2147504447",
      "name": "Nanyun Peng",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    },
    {
      "id": "https://openalex.org/A2009336559",
      "name": "Wei Wang",
      "affiliations": [
        "University of California, Los Angeles"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4221157363",
    "https://openalex.org/W4221160509",
    "https://openalex.org/W2407338347",
    "https://openalex.org/W3126960149",
    "https://openalex.org/W3170369042",
    "https://openalex.org/W4287019595",
    "https://openalex.org/W6740650839",
    "https://openalex.org/W4323651358",
    "https://openalex.org/W3010293452",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W3156205427",
    "https://openalex.org/W3035229828",
    "https://openalex.org/W4281394285",
    "https://openalex.org/W4312107600",
    "https://openalex.org/W3172768590",
    "https://openalex.org/W3174870841",
    "https://openalex.org/W3199714630",
    "https://openalex.org/W4318348366",
    "https://openalex.org/W3120490314",
    "https://openalex.org/W4300076717",
    "https://openalex.org/W4229000015",
    "https://openalex.org/W3154863804",
    "https://openalex.org/W4287887158",
    "https://openalex.org/W4307313205",
    "https://openalex.org/W4312091890",
    "https://openalex.org/W2607892599",
    "https://openalex.org/W3090325631",
    "https://openalex.org/W3177112239",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W2759211898",
    "https://openalex.org/W2796868841",
    "https://openalex.org/W4221161884",
    "https://openalex.org/W4206636317",
    "https://openalex.org/W4221149883",
    "https://openalex.org/W3174945605",
    "https://openalex.org/W4378473795",
    "https://openalex.org/W3128710690",
    "https://openalex.org/W4401042881",
    "https://openalex.org/W4366999773",
    "https://openalex.org/W4385574161",
    "https://openalex.org/W4285184612",
    "https://openalex.org/W4378469337",
    "https://openalex.org/W4389524085",
    "https://openalex.org/W4362508231",
    "https://openalex.org/W4385823413",
    "https://openalex.org/W4385571184",
    "https://openalex.org/W3104415840",
    "https://openalex.org/W4285246823",
    "https://openalex.org/W4287854458",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963360413",
    "https://openalex.org/W4319453300",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W2963526187",
    "https://openalex.org/W4285294416",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4385570396",
    "https://openalex.org/W4323709074",
    "https://openalex.org/W4385571696",
    "https://openalex.org/W4385573325",
    "https://openalex.org/W4385573991",
    "https://openalex.org/W3170759063",
    "https://openalex.org/W4323650985",
    "https://openalex.org/W4385572251",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W3167119069"
  ],
  "abstract": "Information extraction tasks such as event extraction require an in-depth understanding of the output structure and sub-task dependencies. They heavily rely on task-specific training data in the form of (passage, target structure) pairs to obtain reasonable performance. However, obtaining such data through human annotation is costly, leading to a pressing need for low-resource information extraction approaches that require minimal human labeling for real-world applications. Fine-tuning supervised models with synthesized training data would be a generalizable method, but the existing data generation methods either still rely on large-scale ground-truth data or cannot be applied to complicated IE tasks due to their poor performance. To address these challenges, we propose STAR, a data generation method that leverages Large Language Models (LLMs) to synthesize data instances given limited seed demonstrations, thereby boosting low-resource information extraction performance. Our approach involves generating target structures (Y) followed by generating passages (X), all accomplished with the aid of LLMs. We design fine-grained step-by-step instructions to obtain the initial data instances. We further reduce errors and improve data quality through self-reflection error identification and self-refinement with iterative revision. Our experiments show that the data generated by STAR significantly improve the performance of low-resource event extraction and relation extraction tasks, even surpassing the effectiveness of human-curated data. Human assessment of the data quality shows STAR-generated data exhibit higher passage quality and better align with the task definitions compared with the human-curated data.",
  "full_text": "STAR: Boosting Low-Resource Information Extraction by Structure-to-Text Data\nGeneration with Large Language Models\nMingyu Derek Ma1, Xiaoxuan Wang1, Po-Nien Kung1\nP. Jeffrey Brantingham2, Nanyun Peng1, Wei Wang1\n1Department of Computer Science, University of California, Los Angeles\n2Department of Anthropology, University of California, Los Angeles\n{ma, xw27, ponienkung, violetpeng, weiwang}@cs.ucla.edu, branting@ucla.edu\nAbstract\nInformation extraction tasks such as event extraction require\nan in-depth understanding of the output structure and sub-\ntask dependencies. They heavily rely on task-specific train-\ning data in the form of (passage, target structure) pairs to ob-\ntain reasonable performance. However, obtaining such data\nthrough human annotation is costly, leading to a pressing\nneed for low-resource information extraction approaches that\nrequire minimal human labeling for real-world applications.\nFine-tuning supervised models with synthesized training data\nwould be a generalizable method, but the existing data gen-\neration methods either still rely on large-scale ground-truth\ndata or cannot be applied to complicated IE tasks due to\ntheir poor performance. To address these challenges, we pro-\npose S TAR, a data generation method that leverages Large\nLanguage Models (LLMs) to synthesize data instances given\nlimited seed demonstrations, thereby boosting low-resource\ninformation extraction performance. Our approach involves\ngenerating target structures (Y ) followed by generating pas-\nsages (X), all accomplished with the aid of LLMs. We design\nfine-grained step-by-step instructions to obtain the initial data\ninstances. We further reduce errors and improve data quality\nthrough self-reflection error identification and self-refinement\nwith iterative revision. Our experiments show that the data\ngenerated by STAR significantly improve the performance of\nlow-resource event extraction and relation extraction tasks,\neven surpassing the effectiveness of human-curated data. Hu-\nman assessment of the data quality shows S TAR-generated\ndata exhibit higher passage quality and better align with the\ntask definitions compared with the human-curated data.\n1 Introduction\nInformation extraction (IE) aims to extract knowledge of\ncertain perspectives from natural language and consolidate it\ninto an output structure (Ma et al. 2021b). To induce the tar-\nget structure, the IE models need to understand fine-grained\ntask requirements and constraints. Taking event extraction\n(EE), which is a component for IE systems to identify event\ntriggers, event types, and their related details as arguments,\nas an example, task-specific rules include the predicted\nspans should be subsequences of the input passage, and ar-\nguments should be participants or attributes of the event. EE\nCopyright ¬© 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nmodels are also expected to be aware of the dynamic skele-\nton of the event structure because the different predicted\nevent types result in their respective sets of argument roles\nbeing filled in. Supervised models learn the implicit require-\nment and ontology knowledge from training data in the form\nof (passage, target structure) pairs (Ma et al. 2021a). Prompt-\nbased inference-only approaches with Large Language\nModels (LLMs) are shown to be unable to solve these com-\nplicated IE tasks (Li et al. 2023; Gao et al. 2023; Han et al.\n2023). In real-world applications, text from various sources\nand domains contains a broad range of output spaces and\nlabel definitions. It is costly and rigid to annotate sufficient\ntraining data, thus, performing IE given minimal seed data\ninstances is of particular interest for realistic IE applications.\nThe limited resources make understanding the task re-\nquirement and structure skeleton even harder. Existing low-\nresource IE methods mostly leverage other tasks via transfer\nlearning (Huang et al. 2018; Zhang, Wang, and Roth 2021)\nor reformulate the task into alternative data-rich tasks for\nindirect supervision (Sainz et al. 2022; Xu, Ma, and Chen\n2023; Lu et al. 2022). These works heavily depend on the\navailability of the source tasks‚Äô data and the compatibility\nbetween tasks, limiting its application to richer and broader\noutput label spaces and under-represented domains.\nSynthesizing additional training data to fine-tune super-\nvised models would be a generalizable method and has\ndemonstrated its success for tasks like sentiment analy-\nsis (Ye et al. 2022) and relation extraction (Josifoski et al.\n2023). Some works annotate unlabeled examples with ex-\nisting models as weak annotators (X gold ‚Üí Y ) (He et al.\n2021; Chia et al. 2022), produce analogous input (X gold ‚Üí\nX‚Ä≤) (Kumar, Choudhary, and Cho 2020; Lee et al. 2021), or\ngenerate input assuming the labels are available (Ygold ‚Üí X)\n(Meng et al. 2022; Gao, Fisch, and Chen 2021; Josifoski\net al. 2023). However, these works require ground-truth\nXgold or Ygold, limiting their generalizability and scalability.\nWhat‚Äôs more, they are designed for classification or straight-\nforward IE tasks, and the performance drops significantly\nas the task complexity increases. Applying them to com-\nplicated IE tasks with sub-task dependencies and dynamic\noutput structures such as EE produces noisy data that may\nimpair task performance.\nIn this paper, we present STAR, a Structure-to-Text DatA\nGeneRation pipeline to produce dependable data instances\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18751\n10\nInverse Data Generator\nEvent triggers are the most significant action that trigger an \nevent‚Ä¶ Event argument further characterize the properties of \nan event. Generate a sentence containing this event trigger‚Ä¶\nEvent trigger is battle. Artifact argument is citizens; \nDestination argument is airport; Origin argument is capital.\nPassage containing the triggers and arguments is:\nInstruction\nPrompt given \nground-truth With the power out ow in Baghdad, we‚Äôre getting word that \nloudspeakers have been blaring in the Iraqi capital, urging \ncitizen to go to the international airport there, presumably ‚Ä¶\nEvent trigger battle\nEvent type Conflict:Attack\nArtifact citizens\nDestination airport\nOrigin capital\nY\nX\nLLM\nEvent trigger\nEvent type\nArtifact\nDestination\nOrigin\nLarge \nLanguage \nModel\nYX\nWith the power out ow in \nBaghdad, we‚Äôre getting \nword that loudspeakers \nhave been blaring in the \nIraqi capital, urging citizen \nto go to the international \nairport there, presumably ‚Ä¶\nStruct-to-Text Data Generation\nX-Y Pair\nEE Model\nEvent \nExtraction \nModel\nTarget Structure Generation\nLLM\nCourt\n(Adjudicator \nargument)\nJustice:Appeal\n(Event instance)\nappealed\n(Event trigger)ruling\n(Event trigger)\nJustice:Conviction\n(Event instance)\nAnwar Ibrahim\n(Plaintiff argument)\nMalaysia\n(Place argument)\nconviction\n(Event trigger)\nAnwar Ibrahim\n(Defendant argument)\nArgument \ncandidate pool\nTrigger \ncandidate pool\nY\nFormer Malaysian deputy prime \nminister Anwar Ibrahim has been \ngranted a final chance to appeal \nhis conviction and five-year \nsentence on sodomy charges ‚Ä¶\nInverse Initial Passage Generation\nSelf-refinement \ncomponent\nX\n ü§î\nüí≠\nSelf-refinement by Self-reflection\nFigure 1: The STAR inverse data generation strategy using event extraction task as an example. We first generate target structures\nfrom valid trigger and argument candidates. Then we prompt the LLM with task instructions from different task granularities to\ngenerate the initial passageX0 containing the event information in the given target structureY . Finally, we create self-reflection\nquestions to prompt LLM to identify quality issues automatically and refine the passage with template-based hindsight feedback.\nfor low-resource IE. Instead of using existing models to pro-\nduce silver target structures Y derived from the input pas-\nsages X (i.e. Xgold ‚Üí Y ) to enrich the training data, we pro-\npose to generate data instances inversely by producing target\nstructures from scratch first and then prompting the LLM to\ngenerate a passage (i.e. Y ‚Üí X) containing the target struc-\nture information. This inverse design reformulates the syn-\nthetic data generation task from structure induction, where\nmodels often struggle, to conditional text generation, where\nLLMs excel. S TAR contains three components. First, STAR\ngenerates diverse target structures Y from scratch, requir-\ning minimal human efforts to initiate the data generation. In\naddition, with the ability to customize target structures and\ncontrol their distribution, we can mitigate data imbalance\nand improve data diversity by producing target structures\nencompassing a broader range of triggers, event types and\narguments, as well as their various combinations. Second,\nSTAR performs instruction-guided data generation to prompt\nthe LLM about the fine-grained task definition and con-\nstraints to produce passage X. Third, S TAR detects errors\nin the generated data instances via self-reflection and pro-\nvides hindsight natural language intervention to self-refine\nthe generated data without additional human efforts.\nExperimental results on event extraction (EE) and relation\nextraction (RE) tasks show that STAR is capable of generat-\ning human-level IE data instances given a couple of exem-\nplar instances as demonstrations without the need for ad-\nditional ground-truth passage or target structures. For EE,\ntraining the supervised model on S TAR-generated data im-\nproves the argument classification sub-task by up to 12.91\npoints in F1 score on the ACE05 dataset, 2.9 points higher\nthan using the same amount of human-curated data. For RE,\nwe observe a 5.41-point F1 score improvement on the TA-\nCRED dataset, which is comparable to using the human-\ncurated data. The improvements brought by STAR-generated\ndata to multiple supervised models across multiple IE tasks\ndemonstrate the generalizability and compatibility of STAR.\nOur manual data examination indicates STAR-produced data\nexhibits higher passage quality and better aligns with the\ntask definitions compared with human-curated data.\nWe further conduct a detailed analysis of different meth-\nods for using LLMs to improve EE performance, and we\nshow that training the supervised models on STAR-generated\ndata yields at least 27.51 points higher F1 score for argument\nclassification over the best inference-only LLM formulation.\nOur contributions are three-fold: 1) We introduce S TAR,\nan inverse structure-to-text IE data generation method with\nself-refinement; 2) We demonstrate the effectiveness, gen-\neralizability and compatibility of S TAR by showcasing sig-\nnificant EE and RE performance enhancements and manual\ndata quality examination; 3) We conduct a thorough analysis\nof different methods for using LLMs in EE, and the most ef-\nfective approach identified is employing LLMs to generate\ntraining data for fine-tuning supervised models.\n2 S TAR: Structure-to-Text Data Generation\nWe introduce the method design of S TAR as illustrated in\nFigure 1. We use event extraction (EE) as the exemplar IE\ntask in this section as it covers more method details. The goal\nis to create N new data instances (X, Y ) based on k demon-\nstration instances to be used as additional training resources\nfor supervised IE models. Each data instance is composed\nof a natural language passage X containing event informa-\ntion and a target structure Y containing 0 to any number\nof events, each contains an event trigger, its event type and 0\nto any number of (argument mention, argument role) pairs.\nThere are three steps: 1) target structure generation to pre-\npare Y (¬ß2.1), 2) initial passage generation to generate X0\n(¬ß2.2), and 3) self-refinement with self-reflection to revise\nX0 to Xt where t is the times of revision (¬ß2.3). We intro-\nduce the adaptation of STAR to other IE tasks in ¬ß2.4.\n2.1 Target Structure Generation\nThe output distribution of the structure prediction dataset\nlargely determines the generalizability and robustness of the\nmodel fine-tuned on it. We first generate a pool of valid seed\nwords for triggers of each event type, and for arguments of\neach (event type, argument role) combination. We then cre-\nate a target structureY . During the process of generating tar-\nget structures, we particularly employ target structure distri-\nbution control to alleviate data imbalance and improve data\ndiversity and comprehensiveness.\nTrigger candidate generation. We prompt the LLM with\n1) a definition of the selected event type; and 2) a few pas-\nsages that contain event triggers of the selected event type\nas demonstrations. We use special tags to wrap the trigger\nword in the demonstration passages and prompt the LLM\nto continuously generate more passages with trigger words\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18752\nwrapped within tag pairs. Then we parse the response and\nextract the trigger candidate words.\nArgument candidate generation. We find reasonable ar-\nguments for a certain pair of event type and argument role\nby prompting LLM with: 1) a definition of the argument\nrole under this specific event type; 2) the entity type we\nare looking for (e.g. a vehicle). The prompt would be like\n‚ÄúGiven the definition of Instrument argument as ‚ÄòThe de-\nvice used to inflict the harm‚Äô, what are some possible ve-\nhicle names that can be used as Instrument?‚Äù for I NSTRU -\nMENT argument of L IFE :INJURE event type. The allowed\nentity types of arguments are provided in an event ontol-\nogy, and we merge generated word pools returned from sep-\narate queries if there are multiple allowed entity types for\nan argument role. For example, the O RIGIN argument of\nthe MOVEMENT :TRANSPORT event type could be of entity\ntypes GPE (geopolitical entity), LOC (location), or FAC (fa-\ncility name). We further parse the numbered/bullet lists gen-\nerated by LLM to get argument candidate words.\nCreating target structure and distribution control. We\nrandomly sample trigger and argument candidates to create\ntarget structure Y . However, unbalanced label distribution\nand the prevalence of dominant labels pose challenges in\nmany existing human-curated datasets (Ma et al. 2023b;\nZhou, Kaneko, and Bollegala 2022; Cao et al. 2022; Zhao\net al. 2018). In EE, a single dominant trigger word eclipses\nother relevant terms, leading to an unbalanced representa-\ntion (Tong et al. 2022). We address the issues of imbalanced\nevent type and trigger distribution by evenly generating\ndata instances and significantly expanding the pool size\nfor trigger candidates to 100, which is 1.4√ó to 50√ó larger\nfor various event types compared to the human-curated\nACE05 dataset. Additionally, we balance the argument\nhallucination ratio in the generated data. This involves\nensuring that the generated dataset contains events with both\nmany arguments and few arguments by uniformly replacing\nargument value with None across different argument\nhallucination ratios. Furthermore, we balance the event\ndensity in passage X by providing target structure Y with 0\nto 5 events. Such target structure distribution controls from\nmultiple perspectives ensure the EE models trained on the\ngenerated data points effectively learn features and patterns\nassociated with different event and argument densities,\nenhancing robustness and generalizability.\n2.2 Instruction-Guided Passage Generation\nWe use task instruction from multiple task granularities to\nprovide recipes for the LLM to generate passages containing\nstructured event information. The instruction is appended to\nk in-context learning examples verbalized by our instance\nverbalizer. Finally, we provide the verbalized target structure\ninformation based on the target structure Y to prompt the\nLLM to generate the initial passage X0.\nTask-level instruction. We provide task-related instruc-\ntion following the annotation guideline curated by ex-\nperts to guide the human annotation process of the ACE05\ndataset (Doddington et al. 2004). Specifically, we provide:\n1) a definition of ‚Äúevent‚Äù, ‚Äútrigger‚Äù, ‚Äúparticipant arguments‚Äù\nand ‚Äúattribute arguments‚Äù; 2) an overall task requirement\nthat the goal is to generate a sentence containing the event\ntrigger words and arguments; 3) hallucination clarification\nthat instructs the model not to generate arguments of certain\nroles if we explicitly provide that ‚Äúthe argument is None‚Äù;\n4) multiple event clarification that information from multi-\nple events should be contained in a single passage.\nEvent type-level instruction. In this segment, we intro-\nduce meta-information provided by pre-defined event ontol-\nogy for a specific event type, including the name and def-\ninition of the event type and each possible argument roles.\nWe provide all possible argument roles instead of the ones\nwith existing values to ensure the generated passageX does\nnot contain hallucinated arguments that should not appear\naccording to the output structure Y .\nInstance-level verbalizer. We verbalize exemplar data in-\nstances and target structure Y into natural language se-\nquences with three segments: 1) the number of events in\nthe passage; 2) the content of the event target structure; 3)\nthe passage X with tags wrapping triggers and arguments\nto explicit hint the LLM about the roles and positions of\nthe keywords, e.g. ‚Äú<Plaintiff>He</Plaintiff> threatened to\n<Trigger>sue</Trigger>the company.‚Äù could provide an ex-\nplicit indicator to the LLM that ‚Äúhe‚Äù is served as a P LAIN -\nTIFF argument for the event triggered by ‚Äúsue‚Äù.\n2.3 Self-refinement by Self-reflection\nAfter the initial passage X0 is generated, the self-refinement\nmechanism evaluates the quality and identifies potential\nerrors and further improves the initial generation results\nthrough iterative updates (Madaan et al. 2023; Wang et al.\n2023; Liu, Sferrazza, and Abbeel 2023). In the t-th refine-\nment iteration, we first identify the potential quality issues of\nXt‚àí1 from a diverse set of quality dimensions (e.g. the pas-\nsage contains C RIME argument information, but it should\nbe ‚ÄúNone‚Äù according to Y ), then the issues are feedback\nto the LLM by providing a template-based natural language\nintervention (e.g. ‚ÄúThe passage contains a hallucinated ar-\ngument CRIME incorrectly, remove CRIME information for\nevent triggered by ‚Äòjailed‚Äô.‚Äù) along with the generated pas-\nsage of the previous iteration Xt‚àí1, so that the LLM could\nrefine the passage and produce Xt.\nWe define a set of quality dimensions and their interven-\ntion template manually. For EE, they include 1) whether the\ntrigger/argument mention is a subsequence of the passage;\n2) whether a trigger is used to initiate an occurrence; 3)\nwhether an argument is used as an event participant or at-\ntribute of the specific event; 4) whether the argument is serv-\ning the required argument role; 5) whether the passage con-\ntains information that could serve as an argument that should\nnot appear; 6) whether POS tags of the argument mentions\nin the passage context match the provided ones.\nFor each quality dimension, we query LLM with ques-\ntions like ‚ÄúIs ‚ÄòSyria‚Äô a DESTINATION argument describing\nthe event triggered by ‚Äòflee‚Äô?‚Äù. We then standardize LLM‚Äôs\nresponse to a binary error identification flag by checking\nwhether the response entails a confirmative phrase ‚ÄúYes,\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18753\nit is.‚Äù with a Natural Language Inference model fine-tuned\non MultiNLI (Williams, Nangia, and Bowman 2018) based\non BART-large (Lewis et al. 2020). If a quality issue is\nflagged, we use the intervention template corresponding\nto the selected quality dimension as part of the feedback\nto the LLM for iterative revision. Such a self-reflection\ndesign makes the self-refinement process generalizable and\nrobust since the entire error identification process through\nself-reflection and the revision process are done by the LLM\nitself without external add-on components.\n2.4 Adaptation to Relation Extraction\nRE‚Äôs relation type would be the equivalent concept of ‚Äúevent\ntype‚Äù in EE. For target structure generation, we generate\nentity candidates using seed data instances‚Äô entities as in-\ncontext examples. We then randomly pair entity candidates\nand assign a relation between the two entities. For initial pas-\nsage generation, we use relation type definition instead. For\nself-refinement, we use the quality dimensions: 1) whether\nthe given entities are contained in the generated passage, 2)\nwhether there is a relation between them, and 3) whether\nthey hold the certain relation provided in Y .\n3 Experiments on Event Extraction\nTo evaluate the efficiency of STAR-generated data for event\nextraction, we compare the performance of supervised EE\nmodels with and without using the STAR-generated data.\n3.1 Baselines\nWe use two types of models as our baselines: the inference-\nonly methods, and the supervised models fine-tuned on data\ncreated by various data creation strategies.\nInference-only EE methods. We use LLM GPT-\n3.5 (OpenAI. 2022) and GPT-4 (OpenAI. 2023) to perform\ninference.1 We adopt different EE input-target formulations\nto prompt LLMs, including formulations inspired by\ngenerative supervised models (1-3) and LLM prompting\nmethods specifically designed for EE proposed by recent\nworks (4-6). The formulations include: 1) Examples & IO\n(Text2Event) (Lu et al. 2021) uses a concise but unnatural\ntemplate to represent event structure. 2) Examples & IO\n(DEGREE) (Hsu et al. 2022) generates a filled-in natural\nlanguage template. 3) Examples & IO (DICE) (Ma et al.\n2023c) is similar to DEGREE but uses separate queries\nfor different argument roles. 4) Task Instruction (Li\net al. 2023) provides task description and pre-defined\nevent type names. 5) Instruction+Examples (Gao et al.\n2023) provides event type definitions and positive and\nnegative examples, in addition to the task description. 6)\nCode4Struct (Wang, Li, and Ji 2023) formulates task\ndefinition, event type definition and examples in Python\ncode. For baselines 1-3, we follow the original input and\ntarget formulations and additionally provide k demonstra-\ntion input-target pairs contained in the input prompt for\nin-context learning. Baselines 4 and 5 only support Tri-I\nand Tri-C, and baseline 6 only supports Arg-I and Arg-C.\n1We use gpt-3.5-turbo-0301 and gpt-4-0314.\nSupervised EE models. We use two representative EE\nmodels as the testbed to evaluate the quality of the gener-\nated data. OneIE (Lin et al. 2020) is a multi-task sequence-\ntagging model trained with global features based on\nRoBERTa-large (Liu et al. 2019). Note that we remove the\nhuman-labeled data for tasks other than EE (e.g. entity iden-\ntification, relation extraction) to enable a fair comparison.\nDEGREE (Hsu et al. 2022) is a prompt-based model that\nfills in event type-specific human written templates based on\na BART-large pre-trained model (Lewis et al. 2020).\nData creation strategies. Besides S TAR, we introduce\ntwo other approaches to obtain training data. Weakly Su-\npervision: we use the best inference-only model for EE (i.e.\nInst.+Examples for Tri-I and Tri-C, and Code4Struct for\nArg-I and Arg-C) to predict event structure Y ‚Ä≤ from pas-\nsage X, and the (X, Y ‚Ä≤) pairs are used as training data. Hu-\nman: we use human-curated data instances randomly sam-\npled from the ACE05 dataset, which requires much more\nhuman annotators‚Äô efforts, as an ideal but unrealistic setting.\n3.2 Experimental Setup\nWe denote k as the number of demonstration examples of\neach event type used as in-context demonstrations for the\ninference-only methods and data creation strategies, and N\nas the number of data instances per event type created by\ndata creation strategies. Supervised models are trained on\nk + N data instances per event type. We use the full test set\nfor evaluation. We use the event ontology and data instances\nfrom the widely used sentence-level English event extraction\ndataset ACE05 (Doddington et al. 2004).\nWe follow previous EE works (Lin et al. 2020) and report\nF1 scores for four tasks.2 1) Trigger Identification: identified\ntrigger span is correct. 2) Trigger Classification: its predicted\nevent type is also correct. 3) Argument Identification: identi-\nfied argument span is correct. 4) Argument Classification: its\npredicted argument role is also correct. Note that each task\nis dependent on the output of the previous task. We report\nthe medium result for three runs of different random seeds.\n3.3 Effectiveness of Data Generation\nTable 1 shows the EE performance when using various num-\nbers (k) of demonstrations and Figure 2 further shows the\neffects of various amounts (N) of augmented data instances.\nSTAR boosts low-resource EE performance. Table 1\nshows that data generated by S TAR significantly improve\nthe supervised models‚Äô performance (line 9 vs 11-12, and\nline 14 vs 16-17) across all tasks. The F1 scores of Arg-C\nare improved by 9.69 and 12.91 respectively for OneIE and\nDEGREE when k = 10.\nData produced by S TAR is more effective than human-\ncurated ones given sufficient examples. Compared to\nhuman-curated data instances (line 13 and 18 in Table 1),\ntraining supervised models with S TAR-generated data leads\n2The extracted trigger/argument has to match the ground truth\nexactly, instead of head word matching or coreference matching\nused in works (Li, Ji, and Han 2021; Wang, Li, and Ji 2023).\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18754\n#\nk = 0 5 10 k = 0 5 10 k = 0 5 10 k = 0 5 10\nTrigger Iden. Trigger Clas. Argument Iden. Argument Clas.\nInference-only Methods\nLLM Formulation\n1\nGPT-3.5\nE&IO (Text2Event) 0.00 9.23 11.30 0.00 2.12 3.47 0.00 0.87 1.03 0.00 0.31 0.44\n2 E&IO (DEGREE) 0.00 14.39 17.52 0.00 3.17 6.21 0.00 1.02 2.47 0.00 0.92 1.98\n3 E&IO (DICE) 0.00 15.13 16.94 0.00 4.11 7.09 0.00 0.71 1.65 0.00 0.33 0.97\n4 Task Inst.¬ß 18.31 18.31 18.31 8.37 8.37 8.37 ‚Äî ‚Äî\n5 Inst.+Examples 29.44 47.24 59.71 21.56 40.57 53.29 ‚Äî ‚Äî\n6 Code4Struct ‚Äî ‚Äî 12.33 18.34 23.74 9.72 14.85 19.10\n7 GPT-4 Inst.+Examples 34.31 52.55 62.12 27.35 46.57 56.46 ‚Äî ‚Äî\n8 Code4Struct ‚Äî ‚Äî 17.51 24.50 27.62 11.89 24.28 25.48\nSupervised Models(N = 50 except line 9 & 14)\nEE Model Data Creation\n9\nOneIE\nNone (N = 0) 0.00 57.24 60.55 0.00 52.38 54.84 0.00 29.06 36.45 0.00 25.85 33.56\n10 Weak Sup. 29.48 49.23 51.66 23.61 45.02 45.23 16.19 24.35 26.84 10.47 19.14 22.94\n11 STAR (GPT-3.5) 42.61 63.08 64.12 36.65 56.61 57.29 30.32 39.76 43.40 24.36 36.17 40.93\n12 STAR (GPT-4) 45.42 64.63 66.77 39.15 58.84 60.76 32.23 42.76 46.22 27.47 39.53 43.25\n13 Human‚Ä†¬ß 65.62 65.62 65.62 60.10 60.10 60.10 44.76 44.76 44.76 41.60 41.60 41.60\n14\nDEGREE\nNone (N = 0) 0.00 55.62 57.65 0.00 50.69 52.49 0.00 31.77 42.29 0.00 30.19 40.08\n15 Weak Sup. 27.51 46.48 49.70 22.23 41.65 43.55 18.14 32.53 33.33 13.45 27.38 30.01\n16 STAR (GPT-3.5) 43.74 61.39 63.57 38.90 56.41 59.10 32.32 48.73 53.06 28.21 46.55 50.97\n17 STAR (GPT-4) 46.69 64.47 65.17 41.75 59.92 61.42 35.85 51.92 54.56 32.09 50.74 52.99\n18 Human‚Ä†¬ß 63.49 63.49 63.49 58.86 58.86 58.86 52.47 52.47 52.47 50.09 50.09 50.09\nTable 1: Event extraction performance (F1, %) when using k seed data instances per event type. Inference-only methods and\ndata creation methods use the same set of k examples for each event type to prompt LLM to perform EE and generate data\ninstances respectively. Supervised EE models are trained on k + N data instances per event type. Boldface indicates the best\nperformance among each group (line 1-8, 9-13 and 14-18) without additional human efforts. ‚Ä† and gray background indicate\nusing ACE05 human-curated data sampled, thus it is not comparable with other lines. Underlining indicate STAR-generated data\nimprove EE performance more than human-curated data. There is no difference when using variousk for lines indicated by ¬ß as\nno seed data instances are used. We use the trigger and event type produced by the best upstream model (i.e. Inst.+Examples)\nas inputs to Code4Struct to compensate for its lack of event detection capabilities.\nto better performance when 5 or more demonstrations are\nused for STAR and the supervised model is DEGREE except\nfor Arg-I (indicated by underlined results in line 17). We\nalso observe a similar trend for OneIE when 10 demonstra-\ntions are used for S TAR (underlined\nresults in line 12). This\nindicates we could boost low-resource EE performance as\nif we had additional ground-truth data without paying the\nhuman annotation efforts. Figure 2 further shows the su-\nperiority of S TAR-generated data over human-curated ones\nregardless of the number of augmented data instances (N )\nusing 10 demonstrations (k = 10) for all supervised models.\nNoisy data generated by weak supervision impairs per-\nformance. The performance of the best existing few-shot\nmethod used for weakly supervised data generation is still\nnot satisfactory (line 7-8), leading to poor data quality and\nfurther impairing the performance of the supervised models\nby at least 10 points decline in F1 scores for Arg-C (line 9/14\nvs 10/15). Shown by the downward curves in Figure 2, the\nperformance decay is larger when more noisy data is used.\nThis demonstrates that data generation methods that induce\nsemi-supervised label Y ‚Ä≤ from X tend to perform poorly for\ncomplicated IE tasks like EE.\n3.4 Best Recipe to Use LLM for EE\nTunable supervised models are much better than\ninference-only methods. Supervised models (without\ndata augmentation) yield a 14.6-point higher F1 score than\nthe best inference-only method (line 8 vs 14) for Arg-C\nwhen k = 10. The inference-only methods do not fully uti-\nlize the limited resources because they cannot learn from\ngiven demonstrations across all types. They suffer from the\nlimitations of understanding the target structure from task\ndescription or inducing such patterns from in-context exam-\nples. We observe that supervised methods are better in han-\ndling low-resource EE, and their performances are further\nimproved by training on additional data generated by STAR.\nWhen prompting LLM for EE, examples are crucial and\ncode formulation helps. Among various formulations for\nthe inference-only methods, we observe that the natural lan-\nguage template (used in DEGREE) is more beneficial than\nthe unnatural one (used in Text2Event) due to its better uti-\nlization of the pertaining knowledge. We also observe the\nimportance of considering dependencies among arguments.\nThis is evidenced by the superior performance of DEGREE\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18755\nN=0 N=10N=20N=30N=40N=50\n50\n55\n60\n65\nN=0 N=10N=20N=30N=40N=50\n45\n50\n55\n60\nN=0 N=10N=20N=30N=40N=50\n30\n40\n50\nN=0 N=10N=20N=30N=40N=5020\n30\n40\n50\nOneIE+Human OneIE+STAR OneIE+Weak Sup DEGREE+Human DEGREE+STAR DEGREE+Weak Sup\nTrigger Identification Trigger Classification Argument Identification Argument Classification\nFigure 2: Event extraction performance (F1, %) when the EE models are trained on N augmented training data on top of 10\ndata points (k = 10) for each event type. We observe that performance gain brought by S TAR-generated data is magnified as\nthe data augmentation scales up with a larger N, and data generated by STAR is even more effective than human-curated ones.\nWe use GPT-3.5 version STAR for this set of experiments.\nover DICE, which queries each argument role separately\nfor argument extraction. The low scores of Text2Event, DE-\nGREE, and DICE, which all use examples without task de-\nscription, highlight the importance of describing require-\nments and rules in the instruction. It is worth noting that\nthere is a substantial 45-point increase in F1 score from\n‚ÄúTask Inst.‚Äù to ‚ÄúInst.+Examples‚Äù when k = 10 in the trigger\nclassification task. This result highlights the effectiveness of\nthe in-context examples in providing the LLM with the tar-\nget structure knowledge. Additionally, we note that code for-\nmulation outperforms natural language, aligning with find-\nings from previous studies (Wang, Li, and Ji 2023).\n3.5 Effects of Demonstration Quantity k and\nData Quantity N\nWe observe that demonstration examples for the data gener-\nation pipeline are crucially important to bring in target struc-\nture information and showcase the task constraints in action.\nWhen no demonstration example is provided (i.e. k = 0),\nthe data generated by S TAR could still significantly boost\nthe supervised models‚Äô performance, but it is much worse\nthan training with human-curated data (line 12/17 vs 13/18\nof Table 1). This indicates that even the state-of-the-art LLM\nGPT-4 is not capable of generating human-level EE data in-\nstances without demonstrations. The more demonstrations\nused in the data generation pipeline, the better the perfor-\nmance. Figure 2 shows that the performance gain brought\nby augmented data is magnified as the data augmentation\nscales up. Due to resource limitation, we use a maximum k\nof 10 and N of 50 in the experiments, and we anticipate the\nEE performance to further improve with larger k and/or N.\n3.6 Ablation Studies\nWe investigate the influence of the design choices in Table 2.\nStructure generation method. The diversity and bal-\nanced distribution of the generated target structures pro-\nduced by our target structure generation component (¬ß2.1)\nresult in an almost 4-point higher Arg-C F1 score compared\nto using human-annotated target structures sampled from the\nACE05 dataset. Our target structure generation component\nyields highly diverse structuresY without guaranteeing their\nfactuality and commonsensical nature, suggesting that diver-\nsity outweighs factuality in terms of impact on performance.\n# Method Variant Tri-I Tri-C Arg-I Arg-C\nTarget Structure Y Generation Methods\n1 Ground-truth Y ‚Ä† 59.21 54.03 44.13 41.77\n2 LLM generation 60.52 54.80 48.00 45.73\nError Identification Strategies\n3 None 58.33 52.94 42.18 39.85\n4 Rule-based checking 59.42 53.37 43.77 41.26\n5 Self-reflection (NLI) 59.89 54.02 46.11 43.54\n6 Self-reflection (LLM) 60.52 54.80 48.00 45.73\nTable 2: Ablation study on DEGREE‚Äôs EE results while us-\ning 10 demonstrations with 10 additional generated data in-\nstances per event type (k = 10, N = 10).\nError identification strategy. We also investigate the\nerror identification capabilities of our self-reflection module\nwith LLM as the backbone (¬ß2.3). We compare it with\ntwo alternative methods and we utilize the same template\nto provide feedback on the identified errors. Rule-based\nchecking uses heuristics to check whether a trigger/argu-\nment is a subsequence of the generated passage and uses\nan external NER module (Yamada et al. 2020) to check\nwhether a trigger/argument functions as the desired entity\ntype. Self-reflection (NLI) uses the generated passage as\nthe premier and a statement of a quality dimension as the\nhypothesis. We use entailment prediction of the NLI module\nused in ¬ß2.3 to identify whether a certain quality issue ex-\nists. The results are in lines 4-6 of Table 2. Our observations\ndemonstrate both alternative methods help (line 3 vs 4-5),\nand self-reflection with LLM exhibits the highest effec-\ntiveness in error identification (line 6). Notably, the results\nunderscore the effectiveness of the self-reflection design,\nresulting in a substantial 6-point increase in the F1 score for\nArg-C without the need for additional annotation efforts.\n4 Experiments on Relation Extraction\nTo assess the generalizability of our proposed method,\nwe conduct experiments on the sentence-level relation\nextraction (RE) task. We use relation definitions and seed\nexamples in the widely-used TACRED dataset (Zhang\net al. 2017). In this task, we generate (subject, relation,\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18756\n# RE Model Data Gen N = 0 10 40\n1 GPT-3.5 ‚Äî 27.91 27.91 27.91\n2\nSURE\nWeak Sup.\n27.61\n28.02 28.32\n3 STAR (GPT-3.5) 30.50 33.02\n4 Human‚Ä† 30.11 35.62\n5\nGenPT\nWeak Sup.\n33.38\n30.93 30.29\n6 STAR (GPT-3.5) 34.55 37.01\n7 Human‚Ä† 36.74 37.61\nTable 3: Relation extraction performance (%) when the RE\nmodels are trained on N augmented training data on top of\n10 seed data instances (k = 10) for each relation type.\nobject) tuples from scratch, providing additional training\ndata. The RE task aims to identify the relation between\nthe given subject and object entities within a context\npassage. We train two representative RE models on the\ngenerated data instances. SURE (Lu et al. 2022) converts\nthe task into a summarization formulation to leverage the\nindirect supervision with PEGASUS-large as the pre-trained\nencoder (Zhang et al. 2020).GenPT (Han et al. 2022) trans-\nforms RE into an infilling problem with a RoBERTa-large\nmodel as backbone (Liu et al. 2019). We use the same set\nof data creation baselines as in the EE experiments. For the\nweakly supervised baseline, we use SURE‚Äôs formulation\nby querying the LLM once for each relation label, and the\nmodel outputs whether the verbalized relation between head\nand tail entities is entailed by the passage with k additional\nin-context examples, which is the most empirically superior\nRE formulation in our exploration. We report micro F1\nscore across all relations (except for the ‚Äúno relation‚Äù class)\nfollowing prior works (Lu et al. 2022, 2023).\nTable 3 presents the RE performance. The S TAR-\ngenerated data significantly enhances the performance com-\npared to N = 0, with improvements of 5.4 and 3.6 F1 points\nwhen using SURE and GenPT respectively.\n5 Quality Verification\nTwo annotators who are familiar with the EE task manu-\nally assess the quality of the EE data generated by S TAR\nand curated by humans sampled from the ACE05 dataset in\nterms of 3 passage-related quality dimensions (grammatical,\ninformative and commonsensical levels), and 5 task-specific\nquality dimensions (whether the trigger, event type, argu-\nment, and argument role provided in the generated target\nstructure Y is being used in the generated passage X fol-\nlowing their definitions) for data instances in 100 sentences.\nWe report the average scores of the two annotators.\nTable 4 shows both sets of data demonstrate high satis-\nfactory levels. STAR-generated data exhibits higher passage\nquality and better follows the task definition for most\nmetrics, suggesting S TAR produces EE annotations with\ncomparable or even better quality than human annotators.\nQuality Dimension STAR Human\nGrammaticality of the generated passage (X) 96 90\nInformativeness of the generated passage (X) 79 78\nCommonsense of the generated passage (X) 95 93\nTrigger span describes event occurrence 99 99\nEvent follows event type definition 99 97\nArgument span describes an event 100 99\nArgument associated with correct trigger 98 95\nArgument follows role definition 98 99\nTable 4: Human quality assessment satisfactory rate (%).\n6 Related Works\n6.1 Low-Resource Information Extraction\nExisting low-resource IE methods mostly borrow super-\nvision signals from other tasks, such as Abstract Meaning\nRepresentation (Huang et al. 2018) or Semantic Role Label-\ning (Zhang, Wang, and Roth 2021), via transfer learning or\nreformulate the task as other data-rich tasks, such as Natural\nLanguage Inference (NLI) (Xu, Ma, and Chen 2023; Sainz\net al. 2022), summarization (Lu et al. 2022) or QA (Lyu\net al. 2021; Ma et al. 2023a), via indirect supervision.\nRecent works explore prompting LLM with task instruction\nand examples to predict event structure from input passage\nby prompting task requirement (Li et al. 2023), or providing\nexamples along with task instruction (Gao et al. 2023; Xu\net al. 2023). However, their performance is much worse\nthan fine-tuning a model with training data for complicated\nIE tasks like EE, which motivates us to focus on generating\ntraining data for supervised IE models.\n6.2 Data Generation\nExisting works perform data augmentation by producing\nanalogous input (Xgold ‚Üí X‚Ä≤) (Kumar, Choudhary, and Cho\n2020; Lee et al. 2021), and annotating unlabeled examples\nwith existing models (Xgold ‚Üí Y ) (He et al. 2021) on tasks\nlike textual similarity (Schick and Sch ¬®utze 2021), relation\nextraction (Chia et al. 2022), text classification, QA, NLI (Ye\net al. 2022) and more (Wang et al. 2023; Tang et al. 2023).\nAnother line of prior work assumes that the labels are given\nand it prompts an LLM to generate input (Ygold ‚Üí X), such\nas given sentiment labels (Meng et al. 2022; Gao, Fisch, and\nChen 2021) or relation triples (Josifoski et al. 2023). How-\never, these works generate data for tasks with significantly\nsimpler output structures than many IE tasks like EE. Fur-\nthermore, they require existing ground-truth label data to\nperform data generation while our work aims to generate\nboth Y and X from scratch.\n7 Conclusion\nWe present S TAR, an inverse data generation pipeline\ndesigned for low-resource IE that generates output structure\nfirst and then curates input passage containing structure con-\ntent with self-refinement capabilities to fix self-identified er-\nror cases. Experimental results on EE and RE show the gen-\nerated data instances significantly improve the performance.\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18757\nAcknowledgments\nUtkarsh Lal and Michael M. Song made significant con-\ntributions to the quality verification section. Many thanks\nto lab members at UCLA SCAI and UCLANLP for their\nsuggestions, and to the anonymous reviewers for their feed-\nback. This effort was sponsored by the Defense Advanced\nResearch Project Agency (DARPA) grant HR00112290103\nand AFOSR MURI via grant #FA9550-22-1-0380.\nReferences\nCao, Y .; Pruksachatkun, Y .; Chang, K.-W.; Gupta, R.; Ku-\nmar, V .; Dhamala, J.; and Galstyan, A. 2022. On the Intrin-\nsic and Extrinsic Fairness Evaluation Metrics for Contex-\ntualized Language Representations. In Proceedings of the\n60th Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers).\nChia, Y . K.; Bing, L.; Poria, S.; and Si, L. 2022. Relation-\nPrompt: Leveraging Prompts to Generate Synthetic Data for\nZero-Shot Relation Triplet Extraction. In Findings of the\nAssociation for Computational Linguistics: ACL 2022.\nDoddington, G.; Mitchell, A.; Przybocki, M.; Ramshaw, L.;\nStrassel, S.; and Weischedel, R. 2004. The Automatic Con-\ntent Extraction (ACE) Program ‚Äì Tasks, Data, and Evalua-\ntion. In Proceedings of the Fourth International Conference\non Language Resources and Evaluation (LREC‚Äô04).\nGao, J.; Zhao, H.; Yu, C.; and Xu, R. 2023. Ex-\nploring the Feasibility of ChatGPT for Event Extraction.\narxiv:2303.03836.\nGao, T.; Fisch, A.; and Chen, D. 2021. Making Pre-trained\nLanguage Models Better Few-shot Learners. In Proceed-\nings of the 59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International Joint Con-\nference on Natural Language Processing (Volume 1: Long\nPapers).\nHan, J.; Zhao, S.; Cheng, B.; Ma, S.; and Lu, W. 2022. Gen-\nerative Prompt Tuning for Relation Classification. arXiv\npreprint arXiv:2210.12435.\nHan, R.; Peng, T.; Yang, C.; Wang, B.; Liu, L.; and Wan,\nX. 2023. Is Information Extraction Solved by ChatGPT?\nAn Analysis of Performance, Evaluation Criteria, Robust-\nness and Errors. arxiv:2305.14450.\nHe, X.; Nassar, I.; Kiros, J. R.; Haffari, G.; and Norouzi, M.\n2021. Generate, Annotate, and Learn: Generative Models\nAdvance Self-Training and Knowledge Distillation.\nHsu, I.-H.; Huang, K.-H.; Boschee, E.; Miller, S.; Natara-\njan, P.; Chang, K.-W.; and Peng, N. 2022. DEGREE: A\nData-Efficient Generation-Based Event Extraction Model.\nIn Proceedings of the 2022 Conference of the North Ameri-\ncan Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies.\nHuang, L.; Ji, H.; Cho, K.; Dagan, I.; Riedel, S.; and V oss, C.\n2018. Zero-Shot Transfer Learning for Event Extraction. In\nProceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers).\nJosifoski, M.; Sakota, M.; Peyrard, M.; and West, R. 2023.\nExploiting Asymmetry for Synthetic Training Data Gener-\nation: SynthIE and the Case of Information Extraction. In\nBouamor, H.; Pino, J.; and Bali, K., eds., Proceedings of\nthe 2023 Conference on Empirical Methods in Natural Lan-\nguage Processing.\nKumar, V .; Choudhary, A.; and Cho, E. 2020. Data Augmen-\ntation Using Pre-trained Transformer Models. In Proceed-\nings of the 2nd Workshop on Life-long Learning for Spoken\nLanguage Systems.\nLee, K.; Guu, K.; He, L.; Dozat, T.; and Chung, H. W. 2021.\nNeural data augmentation via example extrapolation. arXiv\npreprint 2102.01335.\nLewis, M.; Liu, Y .; Goyal, N.; Ghazvininejad, M.; Mo-\nhamed, A.; Levy, O.; Stoyanov, V .; and Zettlemoyer, L.\n2020. BART: Denoising Sequence-to-Sequence Pre-training\nfor Natural Language Generation, Translation, and Compre-\nhension. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics.\nLi, B.; Fang, G.; Yang, Y .; Wang, Q.; Ye, W.; Zhao, W.; and\nZhang, S. 2023. Evaluating ChatGPT‚Äôs Information Extrac-\ntion Capabilities: An Assessment of Performance, Explain-\nability, Calibration, and Faithfulness. arxiv:2304.11633.\nLi, S.; Ji, H.; and Han, J. 2021. Document-Level Event Ar-\ngument Extraction by Conditional Generation. In Proceed-\nings of the 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human\nLanguage Technologies.\nLin, Y .; Ji, H.; Huang, F.; and Wu, L. 2020. A Joint Neural\nModel for Information Extraction with Global Features. In\nProceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics.\nLiu, H.; Sferrazza, C.; and Abbeel, P. 2023. Chain\nof Hindsight Aligns Language Models with Feedback.\narxiv:2302.02676.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. RoBERTa: A Robustly Optimized BERT Pretraining\nApproach. arxiv:1907.11692.\nLu, K.; Hsu, I.-H.; Zhou, W.; Ma, M. D.; and Chen, M. 2022.\nSummarization as Indirect Supervision for Relation Extrac-\ntion. In Findings of the Association for Computational Lin-\nguistics: EMNLP 2022.\nLu, K.; Hsu, I.-H.; Zhou, W.; Ma, M. D.; and Chen, M. 2023.\nMulti-hop Evidence Retrieval for Cross-document Relation\nExtraction. In Findings of the Association for Computa-\ntional Linguistics: ACL 2023, 10336‚Äì10351.\nLu, Y .; Lin, H.; Xu, J.; Han, X.; Tang, J.; Li, A.; Sun, L.;\nLiao, M.; and Chen, S. 2021. Text2Event: Controllable\nSequence-to-Structure Generation for End-to-end Event Ex-\ntraction. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th In-\nternational Joint Conference on Natural Language Process-\ning (Volume 1: Long Papers).\nLyu, Q.; Zhang, H.; Sulem, E.; and Roth, D. 2021. Zero-Shot\nEvent Extraction via Transfer Learning: Challenges and In-\nsights. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th In-\nternational Joint Conference on Natural Language Process-\ning (Volume 2: Short Papers).\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18758\nMa, M. D.; Chen, M.; Wu, T.-L.; and Peng, N. 2021a. Hy-\nperExpan: Taxonomy Expansion with Hyperbolic Represen-\ntation Learning. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2021.\nMa, M. D.; Kao, J.-Y .; Gao, S.; Gupta, A.; Jin, D.; Chung,\nT.; and Peng, N. 2023a. Parameter-Efficient Low-Resource\nDialogue State Tracking by Prompt Tuning. In Proc. Inter-\nspeech 2023.\nMa, M. D.; Kao, J.-Y .; Gupta, A.; Lin, Y .-H.; Zhao, W.;\nChung, T.; Wang, W.; Chang, K.-W.; and Peng, N. 2023b.\nMitigating Bias for Question Answering Models by Track-\ning Bias Influence. arxiv:2310.08795.\nMa, M. D.; Sun, J.; Yang, M.; Huang, K.-H.; Wen, N.; Singh,\nS.; Han, R.; and Peng, N. 2021b. EventPlus: A Temporal\nEvent Understanding Pipeline. In Proceedings of the 2021\nConference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language Tech-\nnologies: Demonstrations.\nMa, M. D.; Taylor, A.; Wang, W.; and Peng, N. 2023c.\nDICE: Data-Efficient Clinical Event Extraction with Gener-\native Models. In Rogers, A.; Boyd-Graber, J.; and Okazaki,\nN., eds., Proceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1: Long\nPapers).\nMadaan, A.; Tandon, N.; Gupta, P.; Hallinan, S.; Gao, L.;\nWiegreffe, S.; Alon, U.; Dziri, N.; Prabhumoye, S.; Yang,\nY .; Welleck, S.; Majumder, B. P.; Gupta, S.; Yazdanbakhsh,\nA.; and Clark, P. 2023. Self-Refine: Iterative Refinement\nwith Self-Feedback. arxiv:2303.17651.\nMeng, Y .; Huang, J.; Zhang, Y .; and Han, J. 2022. Gener-\nating Training Data with Language Models: Towards Zero-\nShot Language Understanding. arxiv:2202.04538.\nOpenAI. 2022. Chatgpt: Optimizing language models for\ndialogue. https://openai.com/blog/chatgpt/.\nOpenAI. 2023. GPT-4 technical report. arXiv preprint\narXiv:2303.08774.\nSainz, O.; Gonzalez-Dios, I.; Lopez de Lacalle, O.; Min, B.;\nand Agirre, E. 2022. Textual Entailment for Event Argument\nExtraction: Zero- and Few-Shot with Multi-Source Learn-\ning. In Findings of the Association for Computational Lin-\nguistics: NAACL 2022.\nSchick, T.; and Sch ¬®utze, H. 2021. Generating Datasets\nwith Pretrained Language Models. In Proceedings of the\n2021 Conference on Empirical Methods in Natural Lan-\nguage Processing.\nTang, R.; Han, X.; Jiang, X.; and Hu, X. 2023. Does Syn-\nthetic Data Generation of LLMs Help Clinical Text Mining?\narxiv:2303.04360.\nTong, M.; Xu, B.; Wang, S.; Han, M.; Cao, Y .; Zhu, J.;\nChen, S.; Hou, L.; and Li, J. 2022. DocEE: A Large-Scale\nand Fine-grained Benchmark for Document-level Event Ex-\ntraction. In Proceedings of the 2022 Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies.\nWang, X.; Li, S.; and Ji, H. 2023. Code4Struct: Code Gen-\neration for Few-Shot Event Structure Prediction. In Rogers,\nA.; Boyd-Graber, J.; and Okazaki, N., eds., Proceedings of\nthe 61st Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers).\nWang, Y .; Kordi, Y .; Mishra, S.; Liu, A.; Smith, N. A.;\nKhashabi, D.; and Hajishirzi, H. 2023. Self-Instruct: Align-\ning Language Models with Self-Generated Instructions. In\nRogers, A.; Boyd-Graber, J.; and Okazaki, N., eds., Pro-\nceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers).\nWilliams, A.; Nangia, N.; and Bowman, S. 2018. A Broad-\nCoverage Challenge Corpus for Sentence Understanding\nthrough Inference. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, Vol-\nume 1 (Long Papers).\nXu, J.; Ma, M. D.; and Chen, M. 2023. Can NLI Provide\nProper Indirect Supervision for Low-resource Biomedical\nRelation Extraction? In Rogers, A.; Boyd-Graber, J.; and\nOkazaki, N., eds., Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Volume 1:\nLong Papers).\nXu, J.; Ma, M. D.; Wang, F.; Xiao, C.; and Chen, M.\n2023. Instructions as Backdoors: Backdoor Vulnerabil-\nities of Instruction Tuning for Large Language Models.\narxiv:2305.14710.\nYamada, I.; Asai, A.; Shindo, H.; Takeda, H.; and Mat-\nsumoto, Y . 2020. LUKE: Deep Contextualized Entity Repre-\nsentations with Entity-aware Self-attention. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nYe, J.; Gao, J.; Li, Q.; Xu, H.; Feng, J.; Wu, Z.; Yu, T.; and\nKong, L. 2022. ZeroGen: Efficient Zero-shot Learning via\nDataset Generation. arxiv:2202.07922.\nZhang, H.; Wang, H.; and Roth, D. 2021. Zero-Shot Label-\nAware Event Trigger and Argument Classification. In Find-\nings of the Association for Computational Linguistics: ACL-\nIJCNLP 2021.\nZhang, J.; Zhao, Y .; Saleh, M.; and Liu, P. 2020. Pega-\nsus: Pre-training with extracted gap-sentences for abstrac-\ntive summarization. In International Conference on Ma-\nchine Learning, 11328‚Äì11339. PMLR.\nZhang, Y .; Zhong, V .; Chen, D.; Angeli, G.; and Manning,\nC. D. 2017. Position-Aware Attention and Supervised Data\nImprove Slot Filling. InProceedings of the 2017 Conference\non Empirical Methods in Natural Language Processing.\nZhao, J.; Wang, T.; Yatskar, M.; Ordonez, V .; and Chang,\nK.-W. 2018. Gender Bias in Coreference Resolution: Eval-\nuation and Debiasing Methods. In Proceedings of the 2018\nConference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers).\nZhou, Y .; Kaneko, M.; and Bollegala, D. 2022. Sense Em-\nbeddings are also Biased ‚Äì Evaluating Social Biases in Static\nand Contextualised Sense Embeddings. In Proceedings of\nthe 60th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers).\nThe Thirty-Eighth AAAI Conference on ArtiÔ¨Åcial Intelligence (AAAI-24)\n18759",
  "topic": "Boosting (machine learning)",
  "concepts": [
    {
      "name": "Boosting (machine learning)",
      "score": 0.7909989356994629
    },
    {
      "name": "Computer science",
      "score": 0.6672329902648926
    },
    {
      "name": "Information extraction",
      "score": 0.514127790927887
    },
    {
      "name": "Natural language processing",
      "score": 0.4916875958442688
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4727298319339752
    },
    {
      "name": "Data extraction",
      "score": 0.4180114269256592
    },
    {
      "name": "Information retrieval",
      "score": 0.4168202877044678
    },
    {
      "name": "MEDLINE",
      "score": 0.13089436292648315
    },
    {
      "name": "Political science",
      "score": 0.07056263089179993
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}