{
  "title": "Seismic prediction of porosity in tight reservoirs based on transformer",
  "url": "https://openalex.org/W4380085375",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4296600423",
      "name": "Zhaodong Su",
      "affiliations": [
        "State Key Laboratory of Oil and Gas Reservoir Geology and Exploitation",
        "Chengdu University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2120289769",
      "name": "Junxing Cao",
      "affiliations": [
        "State Key Laboratory of Oil and Gas Reservoir Geology and Exploitation",
        "Chengdu University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1960361089",
      "name": "Tao Xiang",
      "affiliations": [
        "State Key Laboratory of Oil and Gas Reservoir Geology and Exploitation",
        "Chengdu University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2314297385",
      "name": "Jingcheng Fu",
      "affiliations": [
        "State Key Laboratory of Oil and Gas Reservoir Geology and Exploitation",
        "Chengdu University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2200771087",
      "name": "Shaochen Shi",
      "affiliations": [
        "State Key Laboratory of Oil and Gas Reservoir Geology and Exploitation",
        "Chengdu University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4296600423",
      "name": "Zhaodong Su",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2120289769",
      "name": "Junxing Cao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1960361089",
      "name": "Tao Xiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2314297385",
      "name": "Jingcheng Fu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2200771087",
      "name": "Shaochen Shi",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1918146339",
    "https://openalex.org/W2114735878",
    "https://openalex.org/W6779879114",
    "https://openalex.org/W6771429369",
    "https://openalex.org/W6731370813",
    "https://openalex.org/W2804427411",
    "https://openalex.org/W6763966027",
    "https://openalex.org/W3011190676",
    "https://openalex.org/W6688167117",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2148945295",
    "https://openalex.org/W2068060604",
    "https://openalex.org/W6780805062",
    "https://openalex.org/W2915004230",
    "https://openalex.org/W3171884590",
    "https://openalex.org/W3127923433",
    "https://openalex.org/W3210862969",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3000003249",
    "https://openalex.org/W6804344759",
    "https://openalex.org/W2995858244",
    "https://openalex.org/W3212953422",
    "https://openalex.org/W4285158519",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2567070169",
    "https://openalex.org/W2212703438",
    "https://openalex.org/W2129557455",
    "https://openalex.org/W2949468773",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W3034655362",
    "https://openalex.org/W3217390616",
    "https://openalex.org/W3099006605",
    "https://openalex.org/W2911371200"
  ],
  "abstract": "Porosity is a crucial index in reservoir evaluation. In tight reservoirs, the porosity is low, resulting in weak seismic responses to changes in porosity. Moreover, the relationship between porosity and seismic response is complex, making accurate porosity inversion prediction challenging. This paper proposes a Transformer-based seismic multi-attribute inversion prediction method for tight reservoir porosity to address this issue. The proposed method takes multiple seismic attributes as input data and porosity as output data. The Transformer mapping transformation network consists of an encoder, a multi-head attention layer, and a decoder and is optimized for training with a gating mechanism and a variable selection module. Applying this method to actual data from a tight sandstone gas exploration area in the Sichuan Basin yielded a porosity prediction coincidence rate of 95% with the well data.",
  "full_text": "Seismic prediction of porosity in\ntight reservoirs based on\ntransformer\nZhaodong Su, Junxing Cao*, Tao Xiang, Jingcheng Fu and\nShaochen Shi\nState Key Laboratory of Oil and Gas Reservoir Geology and Exploitation, Chengdu University of\nTechnology, Chengdu, China\nPorosity is a crucial index in reservoir evaluation. In tight reservoirs, the porosity is\nlow, resulting in weak seismic responses to changes in porosity. Moreover, the\nrelationship between porosity and seismic response is complex, making accurate\nporosity inversion prediction challenging. This paper proposes a Transformer-\nbased seismic multi-attribute inversion prediction method for tight reservoir\nporosity to address this issue. The proposed method takes multiple seismic\nattributes as input data and porosity as output data. The Transformer mapping\ntransformation network consists of an encoder, a multi-head attention layer, and a\ndecoder and is optimized for training with a gating mechanism and a variable\nselection module. Applying this method to actual data from a tight sandstone gas\nexploration area in the Sichuan Basin yielded a porosity prediction coincidence\nrate of 95% with the well data.\nKEYWORDS\ntight reservoirs, porosity prediction, deep learning, multi-headed attention mechanism,\nSichuan Basin\n1 Introduction\nUnconventional oil and gas resources globally are abundant, with tight reservoirs being\nthe primary focus of exploration, mainly referring to tight gas (Zou et al., 2014). Tight oil and\ngas research and development originated in North America (US Energy Information\nAdministration, 2017; Hu et al., 2018). The porosity of reservoirs is a crucial parameter\nthat determines oil and gas reserves and the productivity of reservoirs. Generally, the\nporosity of tight reservoirs is less than 10%. They are characterized by poor lateral continuity,\nstrong vertical heterogeneity, complex lithology, and signiﬁcant variations in physical\nproperties, making it challenging to predict their porosity. Traditional interpretation\nmethods for predicting porosity are slow and labor-intensive. This leads to a slow\ndevelopment of tight oil and gas exploration, emphasizing the critical need for new\ntechnologies and methods. Given the difﬁculty of predicting the physical properties of\ntight reservoirs, the accurate prediction of their porosity is particularly crucial during the\ndevelopment of new technologies.\nDue to the complex nonlinear relationship between each parameter and porosity,\nconventional reservoir parameter prediction methods are not ideal. Traditionally,\nporosity prediction technology driven by a model obtains elastic properties by inversion\nﬁrst and then converts them into reservoir parameters such as porosity through a\npetrophysical model (Avseth et al., 2010; Johansen et al., 2013). However, this method is\nconstrained by the shackles of linear equations. Tight sandstone reservoirs have problems\nOPEN ACCESS\nEDITED BY\nAlex Hay-Man Ng,\nGuangdong University of Technology,\nChina\nREVIEWED BY\nChuang Xu,\nGuangdong University of Technology,\nChina\nTaki Hasan Raﬁ,\nHanyang University, Republic of Korea\n*CORRESPONDENCE\nJunxing Cao,\ncaojx@cdut.edu.cn\nRECEIVED 04 January 2023\nACCEPTED 31 May 2023\nPUBLISHED 09 June 2023\nCITATION\nSu Z, Cao J, Xiang T, Fu J and Shi S (2023),\nSeismic prediction of porosity in tight\nreservoirs based on transformer.\nFront. Earth Sci.11:1137645.\ndoi: 10.3389/feart.2023.1137645\nCOPYRIGHT\n© 2023 Su, Cao, Xiang, Fu and Shi. This is\nan open-access article distributed under\nthe terms of theCreative Commons\nAttribution License (CC BY). The use,\ndistribution or reproduction in other\nforums is permitted, provided the original\nauthor(s) and the copyright owner(s) are\ncredited and that the original publication\nin this journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted\nwhich does not comply with these terms.\nFrontiers inEarth Science frontiersin.org01\nTYPE Original Research\nPUBLISHED 09 June 2023\nDOI 10.3389/feart.2023.1137645\nsuch as low porosity and low permeability, poor physical properties,\ncomplex pore structure, and high irreducible water saturation,\nmaking it difﬁcult to determine ﬂuid properties and saturation.\nSome scholars have used the Bayesian formula’s joint inversion of\nelastic and petrophysical properties to estimate reservoir properties\n(Bosch et al., 2009; de Figueiredo et al., 2018; Wang P. et al., 2020).\nThis approach continuously propagates uncertainty from seismic\ndata to reservoir properties by considering elasticity and reservoir\nproperties. In recent years, for tight reservoirs, scholars have also\ncarried out corresponding research on reservoir properties, such as\nporosity, with traditional methods (Adelinet et al., 2015; Pang et al.,\n2021). However, when combining the elastic properties through\nstatistical rock physics, the process of joint inversion problems\nassociated with reservoir properties is often limited by\ncomputational and time costs. Therefore, it is dif ﬁcult to\naccurately identify the ﬂuid type in tight reservoirs, especially to\nﬁnd oil-bearing reservoirs in the formation.\nIn recent years, the development of deep learning in\ngeophysical explora tion has been signi ﬁcant. Numerous\nexperimental studies have con ﬁrmed that different data\nrepresentations signi ﬁcantly impact the accuracy of task\nlearning. A suitable data representation method can eliminate\nirrelevant factors to the learni ng objective while preserving\nintrinsically related information ( Li et al., 2019a). Rapid and\nreliable porosity prediction ba sed on seismic data is a critical\nissue in reservoir exploration and development. Several\nresearchers have used deep learning for porosity prediction\nwith some preliminary application results ( Wang J. et al.,\n2020; Song et al., 2021 ). For instance, seismic inversion and\nfeedforward neural networks have been used for three-\ndimensional porosity prediction ( Leite, E. P. and Vidal A. C.,\n2011). Recurrent Neural Networks (RNNs) and Convolutional\nNeural Networks (CNNs) are commonly used deep learning\ntechniques for geophysical data processing and interpretation.\nFor example, based on Multilayer Long and Short Term Memory\nNetwork (MLSTM) developed based on traditional Long and\nShort Term Memory (LSTM) model for porosity prediction of\nlogs (Wei Chen et al., 2020). Additionally, some researchers have\ndirectly predicted porosity using CNNs with full waveform\ninversion parameters ( Feng, 2020 )a n dh a v eu s e dt h e\nGaussian Mixture Model Deep Neural Network (GMM-DNN)\nto invert porosity from seismic elastic parameters (Wang et al.,\n2022).\nMost studies in seismic and well logging data processing have\nincorporated deep learning techniques, particularly the\nTransformer architecture, which is the best model for sequence\ndata processing due to its attention mechanism (Vaswani et al.,\n2017). Today it is widely adopted in variousﬁelds, such as natural\nlanguage processing (NLP), computer vision (CV), and speech\np r o c e s s i n g .H o w e v e r ,i t sﬂexible design has led to its adoption in\nmany other ﬁelds. These include image classiﬁcation (Chen et al.,\n2020; Dosovitskiy et al., 2020; Liu et al., 2021), object detection\n(Carion et al., 2020; Zheng et al., 2020; Zhu et al., 2020\n; Liu et al.,\n2021), speech-to-text translation (Han et al., 2021), and text-to-\nimage generation (Ding et al., 2021; Ramesh et al., 2021). Among\ntheir salient bene ﬁts, Transformers enable modeling long\ndependencies between input sequence elements and support\nparallel processing of sequence as compared to recurrent\nnetworks, e.g., Long short-term memory (LSTM). Moreover,\ntheir simple design allows similarprocessing blocks for multiple\nmodalities, including images, video, text, and speech, making them\nhighly scalable for processing large volumes of data. Based on\nTransformer’si m p l e m e n t a t i o ni nd i f f e r ent domains, these state-\nof-the-art results demonstrate Transformer’s effectiveness. These\nadvantages lay the foundation for using the Transformer network\nfor seismic data applications. It is based on the attention\nmechanism (Bahdanau et al., 2014 ). In a sequence prediction\ntask, it is essential to ef ﬁciently allocate resources to enhance\nthe information of highly correlated sequence data while\nreducing the information of weakly correlated sequence data to\nimprove prediction accuracy a nd reliability. The attention\nmechanism is a resource allocation mechanism that focuses on\nimportant features, dividing the degree of attention to the\ninformation in a feature-weighted manner and highlighting the\nimpact of more important information. By mapping weights and\nlearning parameter matrices, the attention mechanism (Zang H\net al., 2020) reduces information loss and enhances the impact of\nimportant information, thus improving prediction accuracy and\nreliability. Transformer archite cture, which abandons the usual\nrecursion and convolution, has shown improved quality and\nsuperior parallel computing capabilities for processing large\ndata (Brown et al., 2020; Lepikhin et al., 2021; Zu et al., 2022)\ncompared to the Long Short-Term Memory network (LSTM)\n(Hocheriter et al., 1997 ), which overcomes the gradient\nvanishing and bursting problems in recurrent neural networks\nand can effectively process sequence data. Consequently, deep\nlearning can solve problems with massive amounts of data. On this\nbasis, this study proposes a Transformer architecture based on the\nattention mechanism. It establishes a new network TP\n(Transformer Prediction), which uses multi-attribute seismic\nand multi-scale data to predict the porosity of tight reservoirs.\nIt has been shown that this network framework has better results in\nnatural language translation, image processing, and data analysis\nprocessing. It is much faster than recurrent neural networks and\nconvolutional neural networks regarding training speed. Even\nthough the resolution of the prediction results is reduced\ncompared with the logging data, the nonlinear inversion\nscheme can effectively reﬂect the complex relationship between\nrock properties and seismic data. It may obtain relatively more\ncomprehensive and high-quality inversion results. As a regression\nprocess, apply deep learning to estimate tight reservoir porosity\nbased on the output of a seismic inversion scheme. In order to learn\nthe relationship between differentfeatures, a recurrent layer is used\nfor local processing and a multi-head attention layer for long-term\ndependencies, allowing the network architecture to capture\npotential features better, ensuring better predictive performance.\nFinally, the method was successfully applied to the actual seismic\ndata of a survey area in the Sichuan Basin and obtained a relatively\ngood inversion result.\n2 Methodology\nWe design the TP model to construct features efﬁciently for\npredicting porosity in dense reservoirs and, ultimately, the overall\nporosity. The main components of the TP are.\nFrontiers inEarth Science frontiersin.org02\nSu et al. 10.3389/feart.2023.1137645\n1) Gating mechanism: to skip unused components of the\narchitecture to accommodate the input transmission of\ndifferent seismic data.\n2) Static covariates encoders: integration of static features into the\nnetwork, conditioning the data by encoding the context vectors.\n3) Variable selection module: Selects relevant variables for the\ninput data.\nFigure 1 illustrates the overall structure of the TP, and the\nindividual components are described in detail in the subsequent\nsubsections.\n2.1 Multi-headed attention mechanism\nSeismic data is a type of sequence data, and the attention\nmechanism is inspired by the selective attention mechanism of\nhuman vision, which focuses on critical information while\nignoring secondary information. It allows sifting through\ncomplex data to ﬁnd helpful information for the present. The\ncore idea of the attention mechanism is to selectively focus on\ninput information by assigning weights thatﬁlter out less important\ninformation from a large amount of data. The multi-headed\nattention mechanism is the key component of the Transformer\narchitecture, entirely based on the attention mechanism. It uses\nmultiple copies of the single-headed attention mechanism to extract\ndifferent information, and the outputs of these heads are\nconcatenated and passed through a fully connected layer to\nproduce the ﬁnal output.\nThe multi-headed attention mechanism is deﬁned as follows\n(Vaswani et al., 2017):\nQ, K, V[] /equals P\nQKV zt() (1)\nAttention Q, K, V() /equals softmax QKT\n/radicaltpext/radicaltpext\nD\n√() V (2)\nWhere Q denotes the information query set, K is the\ncorresponding similarity set, and V denotes the value set. The\nmatrices of Q, K and V are projected by the inputs in the\nattention module using the fully connected layer shown in Eq1.\nThe scaling factor 1/\n/radicaltpext/radicaltpext\nD\n√\nis applied to the parametric gradient, the\nsoftmax function such that the sum ofQKT/\n/radicaltpext/radicaltpext\nD\n√\nis equal to 1. The\nattention weight is denoted as W/equals softmax(QKT/\n/radicaltpext/radicaltpext\nD\n√\n), whereW is\nbased on the pairwise correlation between the query setQ and the\nkey setK, D denotes the dimension of the sequence. In addition, the\nattention weights allow us to extract relevant features using the value\nFIGURE 1\nTransformer Prediction Network Architecture. GRN and LSTM are gated residual network and Long Short-Term Memory network, respectively.\nFrontiers inEarth Science frontiersin.org03\nSu et al. 10.3389/feart.2023.1137645\nset V. For more details on the Transformer architecture, refer to\nVaswani et al. (2017).\nWhen using the LSTM network alone to input a long sequence\nfor prediction, the gradient update may decay quickly, hindering the\nupdate of sequence data to some extent and making it challenging to\nrepresent the feature vector effectively. Additionally, subsequent\ninput data also overwrites the previous input, resulting in the loss of\ndetails. However, by using the multi-head attention module and\nperforming a linear transformation to generate Q, K, and V\nmatrices, after expanding the original low-dimensional\ninformation to a higher dimension, taking into account the\ndifferences between seismic data and natural language, and at the\nsame time having the interaction capability of global information for\nthe input of long sequence multiple seismic attribute data. It not only\nenhances the inﬂuence of strong signals of seismic attribute data but\nalso focuses on weak signal anomalies, avoiding the problem of\nlosing important feature information and reducing the interference\nof unnecessary information in the sequence. The joint LSTM can\neffectively learn the rich features and laws in the input data to obtain\nthe porosity data in dense reservoirs. Therefore, the attention\nmechanism is auxiliary in extracting complex data features.\n2.2 Variable selection and gating network\nAccording to the Temporal Fusion Transformer architecture\n(Lim B et al., 2021), some modules were modiﬁed accordingly for the\nlarge seismic data volume. We added a gating and variable selection\nnetwork to the Transformer for seismic data.\nIn geological studies, lithology typically varies with depth\nLogging data provides information about the formation rocks,\nwhile seismic attributes reﬂect different features of the formation.\nThe corresponding values for porosity prediction should be a\nweighted sum of adjacent characteristic responses that have a\ncertain correlation with each other. Therefore, when we establish\nthe relationship between porosity and external input of various\nseismic attributes, we should consider the local correlation, the trend\nof change with depth, and the relationship of adjacent strata.\nHowever, since the relationships between external seismic\nattribute inputs are unknown and uninterpretable for porosity\nprediction, it is dif ﬁcult to determine which variables are\ncorrelated and the degree of nonlinear processing required. In\nsummary, we need a module for the nonlinear processing of the\ninputs in this network, so we added a gated residual network (GRN)\nas a building block for our network. The GRN would accept a\nprimary inputa and context vectorc, giving it the functionality to\nobtain more accurate and comprehensive features.\nGRN\nω a, c() /equals LayerNorm a + GLUω η1()() (3)\nη1 /equals W1,ωη2 + b1,ω (4)\nη2 /equals ELU W2,ωa + W3,ωc + b2,ω() (5)\nWhere ELU denotes the unit activation function ( Clevert,\nUnterthiner, & Hochreiter, 2016 ), η1 ∈ Rdmodel , η2 ∈ Rdmodel are\nintermediate layers, LayerNorm is a standard layer\nnormalization by Lei Ba, Kiros and Hinton (2016), ω is the\nmetric indicating weight sharing, b(·) ∈ Rdmodel are the weights\nand biases. The ELU activation function will then function as a\nrecognition function when W2,ωa + W3,ωc + b2,ω is greater than\nzero, and ELU activation will produce a constant output when\nW\n2,ωa + W3,ωc + b2,ω is less than zero. We use a component gating\nlayer based on Gated Linear Units (GLUs) (Dauphin et al., 2017)t o\nprovide ﬂexibility to suppress certain repetitions of unwanted parts\nof a particular seismic attribute dataset.\nThe variable selection also allows TP to remove any\nunnecessary noisy inputs tha t may negatively affect the\nprediction. However, identifyi ng null or invalid values of the\nseismic attribute data manually in a large work area can be time-\nconsuming and laborious. Hen ce, we introduced a variable\nselection module ( Gal & Ghahramani, 2016 )t oa u t o m a t et h i s\nprocess. A linear transformation ofthe variables is also applied to\nconvert each post-input variable into a d-dimensional vector,\nwhich matches the dimensions in the subsequent layers and is\nmainly used to achieve jump connections. The weights vary for\ndifferent data (determined by the correlation of each attribute\nwith the porosity). The variable selection network on the seismic\ndata input is presented below, as shown inFigure 2.N o t et h a tt h e\nvariable selection network is t h es a m ef o rt h eo t h e rd a t as e t\ninputs.\nWe denoteX\n(j)\ni ∈ Rdmodel as thejth variable input value in a time-\ndomain earthquake sequence, whereΞi /equals[ X(1)T\ni , ... ,X (m)T\ni ]T is the\nﬂattened vector of which all have been entered. The weights for\nvariable selection are then generated by feeding both Ξi and\nadditional context vectorsc into GRN and then into the Softmax\nlayer:\nυχt /equals Softmax GRNυχ Ξt, c()() (6)\nWhere υχt ∈ Rmχ is the vector of variable selection weights andc\nis obtained from the static covariate encoder (Section 2.3).\nAn additional non-linear processing layer is applied during the\ninput process by passing each sequence element through its\nown GNR.\nFIGURE 2\nVariable selection network.\nFrontiers inEarth Science frontiersin.org04\nSu et al. 10.3389/feart.2023.1137645\n~X\nj()\ni /equals GRN~Xj() X\nj()\ni() (7)\nwhere ~X\n(j)\ni is the feature vector after processing the seismic attribute\nj. We note that each sequence has its ownGRN~X(j), and the weights\nare shared globally. The processed features are then weighted and\ncombined according to their variable selection weights as follows:\n~X\ni /equals ∑\nm\nj/equals 1\nυ\nj()\nχt ~X\nj()\ni (8)\nWhere υ(j)\nχt is the jth element of vectorυχt.\nIn summary, the correlation between seismic attribute data and\nporosity is unknown, and adding the GRN module provides the\nadvantage of complementing the seismic attribute preference.\nAdditionally, because the input of various seismic attribute data\nmay contain many invalid values, the variable selection network is\nused to further optimize the input data and prevent signiﬁcant errors\nin the prediction results.\n2.3 Static covariant encoder\nDue to the variation of rock properties with depth and the\ncorrelation between seismic response adjacencies, it is crucial to\nconsider not only the local correlation between seismic attribute data\nbut also the trend of seismic data with depth and adjacency\ninformation when establishing the mapping relationship between\ndifferent attribute data and porosity. The TP model can integrate\ninformation from the logging porosity data. A separate GNR\nencoder is then used to generate a context vector. The context\nvectors are connected to different locations in the network\narchitecture where static variables play an essential role. These\ninclude the selection of data for additional seismic attributes, the\nunderlying processing of features, the use of logging curves to guide\nthe identiﬁcation of the context vector, and the enrichment of\nfeatures for data adjacency.\n2.4 Decoder\nWe have considered the use of the LSTM encoder-decoder as a\nbuilding block for our prediction architecture, following the success\nof this approach in typical sequential coding problems (Wen et al.,\n2017; Fan et al., 2019; Wang et al., 2022). Due to the speciﬁcity of\nseismic data, the critical points in the data we apply are usually\ndetermined based on the surrounding values. Therefore, we propose\nto use a sequence-to-sequence layer to process the data naturally.\nSend X\ni−k: i to the encoder andXi+k: i+τ max to the decoder. A set of\nuniform features is generated as input to the decoder itself, using\nϕ(i, n) ∈ ϕ(i, −k), ... , ϕ(i, τ\nmax){} , and n is a position index. In\naddition, to allow static metadata to affect local processing, we\nuse a context vector from a static covariate encoder to initialize the\ncell state and hidden state of theﬁrst LSTM in the layer, respectively.\nWe also employ a gated jump connection on this layer at:\n~ϕ i, n() /equals LayerNorm X\ni+n + GLU~ϕ ϕ i, n()()() (9)\nWhere n ∈ [−k, τ max] is a location index, i denotes the ith\nsequence data.\nSince static covariates often have an enormous impact on\nfeatures, we introduce a layer simultaneously to augment the\ncorresponding features with static metadata. For a given location\nindex n, the form of:\nθ i, n() /equals GRN\nθ ~ϕ i, n() , c() (10)\nWhere the whole layer shares the weights ofGRNθ, andc is the\ncontext vector from the static covariate encoder. Finally, the\nattention layer, the decoder mask (Vaswani et al., 2017; Li et al.,\n2019b), is applied to the multi-head attention mask layer to ensure\nthat each dimension notices its previous features.\n3 A case study\n3.1 Data preparation\nConsidering the completeness and universality, many seismic\nattributes need to be extracted, resulting in redundant information.\nSeismic attribute optimization is to preferably select the attributes\nwith better correlation with the target reservoir parameters from a\nlarge number of attributes to eliminate the redundant information\nand thus improve the prediction accuracy. The preferred effect of\nseismic attributes is often reﬂected in oil and gas prediction results.\nIn this study, the analysis of the rendezvous plots wasﬁrst conducted\nusing the Pearson correlation coefﬁcient (PCC) as the criterion. The\nPCC reﬂects the degree of similarity of each unit change between\ntwo variables, and the closer the result is to 1 indicates the more\nTABLE 1 Optional seismic attributes.\nIndex Attribute Index Attribute\n0 Post-stack 15 P-impedance\n1 Vp/Vs 16 Energy Half Time\n2 Domain Amp 17 High Order Kurtosis\n3 Domain Freq 18 Skew\n4 First Peak Amp 19 Variance\n5 First Peak Freq 20 Centroid Freq\n6 Frequency Band 21 Twist\n7 Peak Amp Above Average 22 GR\n8 Per Frequency 23 Fluid Factor\n9 Instant Amp 24 Q Factor\n10 Instant Freq 25 RMS\n11 Instant Phase 26 LamdaRho\n12 Instant Weighted Freq 27 Coherence\n13 P (AVO intercept) 28 Curvature\n14 G (AVO intercept)\nFrontiers inEarth Science frontiersin.org05\nSu et al. 10.3389/feart.2023.1137645\nsigniﬁcant correlation between the variables, which is calculated by\nthe following equation:\nPCC /equals Cov y, ^y()\nσyσ ^y\n(11)\nWhere Cov(y, ^y) denotes the covariance of the two variablesy\nand ^y, used to describe the linear relationship between the two\nvariables, and σ is the standard deviation.\nThis study analyzed porosity separately with multiple attributes,\nand we used the attribute preference method to perform preliminary\nscreening of each seismic attribute. The multiple seismic attributes\nshown inTable 1include post-stack seismic attributes and various\nderived post-stack attributes in order to avoid the multi-solution\nnature of single-parameter inversion to initially screen out the\nuseless data with enormous in ﬂuence. Figure 1 shows a\nrepresentative rendezvous analysis of several attributes, and other\nrendezvous analyses are also performed in the same way we can\nselect the corresponding seismic attributes. Finally, 12 seismic\nattribute bodies such as Q Factor, Fluid Factor, PG,\nP-impedance, and variance are selected as input according to\nPCC, as shown inTable 2 and Figure 3.\n3.2 Evaluation criteria\nThis study uses three primary metrics to evaluate the\nperformance of porosity prediction, namely, mean square error\n(MSE), Pearson correlation coefﬁcient (PCC), and coefﬁcient of\ndetermination (R2), to quantitatively evaluate the inversion results.\nFIGURE 3\nSeismic attributes and porosity intersection map.\nTABLE 2 Input seismic attribute data.\nIndex Attribute Index Attribute\n6 Frequency Band 23 Fluid Factor\n13 P (AVO intercept) 24 Q Factor\n14 G (AVO intercept) 25 RMS\n16 Energy Half Time 26 LamdaRho\n19 Variance 27 Coherence\n22 GR 28 Curvature\nFrontiers inEarth Science frontiersin.org06\nSu et al. 10.3389/feart.2023.1137645\nMean squared error. MSE is the sum of squares of the\ncorresponding point errors between the predicted and actual\ndata. The smaller the value, the better theﬁt between predicted\nand actual data. It is deﬁned as shown in Eq12.\nMSE /equals\n1\nN∑\nn\ni/equals 1 yi − y\n⌢\ni()\n2\n(12)\nWhere yi and y\n⌢\ni are the predicted and actual values of the series,\nand N is the number of sampling points.\nPearson Correlation Coefﬁcient. PCC indicates the overallﬁt\nbetween the predicted and actual data. Its range is [-1,1]. The larger\nthe value, the stronger the linear correlation. It is deﬁned in Eq11.\nCoefﬁcient of determination.R\n2 is used to assess theﬁt between\nthe variables and considers the mean square error between the\npredicted and actual data. It ranges from [0,1]. The larger the value,\nthe better the ﬁt between the variables. The following equation\ngives it:\nR\n2 /equals 1 −\n∑ n\ni/equals 1 yi − y\n⌢\ni()\n2\n∑ n\ni/equals 1 yi − /C22yi()\n2 (13)\nWhere /C22y is the average value ofyi(i /equals 1, 2, ...,n ).\n3.3 Experiment and result analysis\nIn order to assess the validity of our proposed method, we\nconducted a method analysis experiment using logging data from\ntwo wells, X and Y, in a dense sandstone reservoir in an actual\nworking area. The logging depth of well X was between 2750 m and\n2940m, while that of well Y was between 2714m and 2870 m. The\nlogging curves included porosity, density, spontaneous potential,\np-wave velocity, natural gamma ray, deep lateral resistivity,\ncompensated neutron, and others. After removing any outliers,\nwe obtained two sets of logging data from wells X and Y for\ntraining and test datasets in the Transformer architecture. First,\nwe screened the sensitive parameters of well X using Pearson\ncorrelation coefﬁcients. Next, we used the Transformer model\nbuilt with the logging data of well X to predict the porosity of\nwell Y. To compare our model with other existing models, we also\nused bi-directional Long Short-Term Memory (BLSTM) and\nconvolutional neural network (CNN) in this experiment.\nThe correlation between the porosity and other logging\nparameters was obtained by correlating the petrophysical logging\nparameters of the two test study wells, as shown inFigure 4.\nThe logging parameters selected for this study are density\n(DEN), spontaneous potential (SP), p-wave velocity (VP), natural\ngamma (GR), acoustic log(DT), and compensate neutron log(CNL).\nFrom the correlation graph shown inFigure 4, it is evident that CNL,\nDEN, DT, and GR are highly correlated with porosity (POR),\nwhereas natural potential and longitudinal velocity exhibit weaker\ncorrelations. Therefore, we have used the four input datasets with\nhigher correlation for predicting porosity in tight reservoir logs. In\nthis study, we have used Root Mean Square Error (RMSE), Mean\nAbsolute Error (MAE), and Mean Absolute Percentage Error\n(MAPE) as evaluation metrics to assess the performance of the\nmodel. These metrics measure the deviation between the predicted\nand actual data, with lower values indicating better performance.\nWe trained the model on the preferred well X data and used it to\npredict the porosity of well Y. The results of the prediction are\npresented inFigure 5. Using deep learning techniques and sensitive\nfeature parameters, such as CNL, DEN, DT, and GR, we obtained\ngood results in porosity prediction of wells. Three models, namely,\nTransformer, BLSTM, and CNN, were used for porosity prediction,\nand while errors existed, the overallﬁt to the actual porosity curve\nwas satisfactory. As illustrated inFigure 5, the predicted values of the\nTransformer model were closer to the actual values than those of the\nBLSTM and CNN models, especially in the areas of signiﬁcant\nFIGURE 4\nLogging parameters and porosity intersection map.\nFrontiers inEarth Science frontiersin.org07\nSu et al. 10.3389/feart.2023.1137645\nvariation, where the results were more satisfactory. The prediction\nerrors are summarized inTable 3, and it can be concluded that the\nTransformer model has more advantages.\nFIGURE 5\nResults of porosity prediction across wells with different models (well Y).\nTABLE 3 Porosity prediction error for well Y.\nModel RMSE MAE MAPE\nTransformer 0.512 0.405 18.458\nBLSTM 0.562 0.441 20.664\nCNN 0.592 0.455 20.397\nTABLE 4 Optimal network parameters.\nNetwork parameters\nDropout rate 0.3\nState size 160\nNumber of heads 4\nTraining Parameters\nMinibatch 64\nLearning rate 0.01\nMax gradient norm 0.01\nFrontiers inEarth Science frontiersin.org08\nSu et al. 10.3389/feart.2023.1137645\nIn this study, we applied the model to the actual data of an\nexploration area in eastern Sichuan for tight sandstone oil and\ngas exploration. It is dif ﬁcult to describe the reservoir in this\nexploration area; the seismic response of the submerged river\nsand body is not obvious and variable, the reservoir thickness of\nthe river sand body is large, but theporosity is small, the porosity\nprediction is difﬁcult, and it is a dense gas reservoir. We used the\nlogging data of 5 wells and optimized 12 types of seismic volume\nattribute data in this work area. We divided our dataset into three\nparts - a training set for learning, a validation set for\nhyperparameter tuning, and a test set reserved for\nperformance evaluation. For hyp erparameter optimization, we\nuse the random search method.Table 4 shows the entire search\nrange of all hyperparameters as follows and the optimal model\nparameters.\nC State Size–10,20,40,80,160,240\nC Dropout rate–0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 0.9\nC Minibatch size–64, 128, 256\nC Learning rate–0.0001, 0.001, 0.01, 0.1\nC Max. gradient norm–0.01, 1.0, 100.0\nC Num. heads–1, 4\nDuring the training process of our dataset, a signi ﬁcant\namount of seismic data input is required. However, using the\nbasic architecture of the Transformer model, the entire\nFIGURE 6\nComparison of well porosity prediction results.\nTABLE 5 Comparison of the porosity prediction errors of different models;\n“Means” indicates the average value of the prediction errors of different\nmethods; “TP” indicates that Transformer Prediction.\nWell name Method MSE PCC R2\nA\nCNN 0.2553 0.9708 0.8071\nBLSTM 0.2029 0.9642 0.8245\nTP 0.1568 0.9731 0.8468\nB\nCNN 0.3367 0.9561 0.7533\nBLSTM 0.2584 0.9664 0.8244\nTP 0.1286 0.9859 0.8958\nC\nCNN 0.2863 0.9687 0.7862\nBLSTM 0.1397 0.9462 0.8423\nTP 0.0951 0.9884 0.9682\nD\nCNN 0.2794 0.9694 0.7955\nBLSTM 0.1846 0.9708 0.8271\nTP 0.1007 0.9862 0.9441\nMeans\nCNN 0.2894 0.9663 0.7855\nBLSTM 0.1964 0.9619 0.8296\nTP 0.1203 0.9834 0.9137\nFrontiers inEarth Science frontiersin.org09\nSu et al. 10.3389/feart.2023.1137645\narchitecture can be deplo yed on a single GPU without\nconsuming many computer reso urces. For instance, when we\nused NVIDIA Quadro GP100 GPU for the experimental data,\nour optimal parameter TP model only required less than 2 hours\nto complete training. Figure 6 presents the comparison of well\nporosity prediction results, in which the red curve is the actual\nFIGURE 7\nSection of porosity results.(A). Frequency division inversion porosity proﬁle; (B). TP predicted porosity proﬁle.\nFrontiers inEarth Science frontiersin.org10\nSu et al. 10.3389/feart.2023.1137645\nvalue, the blue curve is the CNN prediction value, the purple\ncurve is the BLSTM prediction value and the green curve is the\nTP prediction value. With the CNN and BLSTM data were\nadded to compare network effects in this study. From the\ncomparison in the ﬁgure, we can ﬁnd that the prediction\nresult of TP is more accurate than CNN or BLSTM, and the\nmean square error (MSE), Pearson correlation coef ﬁcient\n(PCC), and are used as regres sion evaluation indicators.\nTable 5 selects four wells (coded by A, B, C, and D), and it\ncan also be proved from Table 5 that our TP can obtain more\naccurate results.\nFigure 7shows the porosity result proﬁle predicted by the TP\nmodel in this study. Speciﬁcally, Figure 7A presents the porosity\nresult proﬁle utilizing standard software for frequency division\ninversion, whereas Figure 7Brepresents the porosity inversion of\nTP training. In the porosity result proﬁle, the red area represents\nthe distribution range of high porosity. The porosity curve of\ncritical well location A in this work area is utilized to verify the\nprediction results. It is known that the porosity of tight reservoirs\ninverted by traditional methodsis generally small. The important\nbasis is the well-logging porosity, which is signiﬁcantly affected\nby the logging data, so the overall resolution is low.Figure 7Aalso\nproves that the overall porosityresolution inversion of the tight\nreservoir is not high, and the lateral continuity is poor. During\nporosity comparison with logging, the overall porosity in\nFigure 7A is relatively small due to the complex characteristics\nof tight reservoirs. However, it can be found from the logging\nreport that the actual porosity range is 0–8 % .H e n c e ,t h es i n g l e\ninversion method still has limitations and does not match the\nporosity reservoir characteris tics of the actual work area. In\ncontrast, Figure 7B illustrates the resulting porosity pro ﬁle\npredicted by the TP model. It is observed that the high values\nare also concentrated near Well A compared toFigure 7A.T h e\noverall porosity range also reaches 0 –8%, but Figure 7B has a\nhigher resolution. The comparison around 2058 ms shows that\nthe porosity distribution in Figure 7B is more accurate in the\nlateral direction. Figure 7A shows that the traditional method\ncannot accurately invert the pore distribution in the right half, far\naway from the well location, due to the limitation of logging data.\nTable 6 shows the well and porosity proﬁle prediction and the\nMSE, PCC, and R\n2 of logging and input porosity pro ﬁle,\nrespectively.\n4 Discussion\nIn the previous experiment, the Transformer model’sR M S E ,\nMAE, and MAPE metrics for well X were 8.91%, 8.16%, and\n10.68% lower than those of the BLSTM, respectively, and 13.51%,\n10.98%, and 9.51% lower than those of the CNN. Although the\noverall effect was not as good as that of the well porosity\nprediction, the effect was more evident and re ﬂected the\ngeneralization ability of the proposed method. Subsequent\nexperiments comparing TP wi th several deep learning\nmethods showed that the proposed method had high accuracy\nand strong continuity in predicting the porosity of dense\nreservoirs. The MSE, PCC, and R\n2 of predicted actual data\nporosity were 0.1203, 0.9834, and 0.9137, respectively. The\nMSE of predicted porosity was decreased by 0.1691 and\n0.0761 when compared with the CNN-based and BLSTM-\nbased methods, respectively, and the PCC was improved by\n0.0171 and 0.0215, and R\n2 was improved by 0.1282 and\n0.0841. The proﬁle application also showed that TP accurately\npredicted porosity in the training work area (MSE = 0.1872,\nPCC = 0.9584, R2 = 0.9065). In summary, the TP model had\ncertain advantages and effects r egarding overall prediction\naccuracy and accuracy. Additionally, the TP model had an\nadvantage in predicting the porosity of a tight reservoir, which\nwas generally in line with the sedimentary characteristics of tight\nsandstone reservoirs. The experimental veriﬁcation and accuracy\nanalysis of the proposed method proved the effectiveness of the\ndeep learning algorithm in pre dicting the porosity of tight\nreservoirs. By combining the particularity of tight reservoirs\nand the advantages of deep learning algorithms, this paper\nprovided a new method for the porosity prediction of tight\nreservoirs, which signi ﬁcantly improved the accuracy of\nporosity prediction in tight res ervoirs and provided valuable\ninsights for reservoir explorat ion and exploitation. However,\ndeep learning is a statistical model that needs to be improved\nin solving complex earthquake prediction problems in different\nwork areas due to entanglement and multi-solution problems.\nTherefore, more targeted modules must be added to the model,\nwhich may lead to parameter redundancy and higher\ncomputational overhead costs that need to be weighed\nbetween accuracy and ef ﬁciency. To further improve the\nmethod model, the Transforme r-based encod ing-decoding\nmodel in porosity prediction may require the addition of\nappropriate constraints, a smoother loss function, or a more\nintelligent design, which requires more in-depth research.\n5 Conclusion\nThis paper proposes the TP network model, a new attention-\nbased method for predicting porosity in tight reservoirs, which\nimproves prediction accuracy. During training, TP does not\ndirectly map seismic data to inversion parameters. Instead, the\nnetwork utilizes specialized processing components to target\nlarge data of various seismic attributes, including (1) The self-\nattention mechanism, which enables global information\ninteraction between data and captures deeper feature\ninformation, (2) Static covar iate encoder, which integrates\nstatic metadata into the network and adjusts the data by\nencoding context vectors, (3) Gated network, which\noptimizes the transfer of data , and (4) Variable selection,\nwhich further optimizes data input. TP predicts porosity by\nlearning the characteristics of logs as well as seismic attribute\nTABLE 6 Prediction of well and porosity proﬁle and MSE, PCC andR2.\nMSE PCC R2\nTraining Well 0.1203 0.9834 0.9137\nTest Well 0.3285 0.9422 0.8861\nProﬁle 0.1872 0.9584 0.9065\nFrontiers inEarth Science frontiersin.org11\nSu et al. 10.3389/feart.2023.1137645\nbodies. The proposed TP inversion network with high\nresolution, accuracy and horizontal continuity is veri ﬁed in\npractical data experiments to e ffectively predict reservoir\nporosity in dense formations.\nData availability statement\nThe original contributions presented in the study are included in\nthe article/supplementary material further inquiries can be directed\nto the corresponding author.\nAuthor contributions\nZS: Software, Methodology, Writing — original draft; JC:\nFunding acquisition, Writing — review and editing, Supervision;\nTX: Methodology, Formal analysis; JF and SS: Data curation,\nVisualization. All authors contributed to the article and approved\nthe submitted version.\nFunding\nThis work was supported by the National Natural Science\nFoundation of China (Grant Nos 42030812 and 41974160).\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors and\ndo not necessarily represent those of their afﬁliated organizations, or\nthose of the publisher, the editors and the reviewers. Any product that\nmay be evaluated in this article, or claim that may be made by its\nmanufacturer, is not guaranteed or endorsed by the publisher.\nReferences\nAdelinet, M., and Ravalec, M. L. (2015). Effective medium modeling: How to\nefﬁciently infer porosity from seismic data? Interpretation 3( 4 ) ,S A C 1–SAC7.\ndoi:10.1190/int-2015-0065.1\nAditya, R., Mikhail, P., Gabriel, G., Scott, G., Chelsea, V., and Alec, R., (2021). Zero-\nshot text-to-image generation.https://arxiv.org/abs/2102.12092.\nAvseth, P., Mukerji, T., and Mavko, G. (2010).Quantitative seismic interpretation:\nApplying rock physics tools to reduce interpretation risk. Cambridge, UK: Cambridge\nUniversity Press.\nBahdanau, D., Cho, K., and Bengio, Y. (2014). Neural machine translation by jointly\nlearning to align and translate.https://arxiv.org/abs/1409.0473.\nBosch, M., Carvajal, C., Rodrigues, J., Torres, A., Aldana, M., and Sierra, J.\n(2009). Petrophysical seismic inversion conditioned to well-log data: Methods\nand application to a gas reservoir. Geophysics 74 (2), O1–O15. doi:10.1190/1.\n3043796\nChen, M., and Radford, A. (2020). “Generative pretraining from pixels, ” in\nProceedings of the ICML, Baltimore, ML, USA, June 2020.\nClevert, D. A., Unterthiner, T., and Hochreiter, S. (2016). Fast and accurate deep\nnetwork learning by exponential linear units (ELUs).https://arxiv.org/abs/1511.07289.\nC o r n i a ,M . ,S t e f a n i n i ,M . ,B a r a l d i ,L . ,a n dC u c c h i a r a ,R .( 2 0 2 0 ) .“Meshedmemory\ntransformer for image captioning, ” in Proceedings of the 2020 IEEE/CVF\nConference on Computer Vision and Pat tern Recognition, CVPR 2020, Seattle,\nWA, USA, June 2020, 10575–10584.\nDauphin, Y., Fan, A., and Auli, M. (2017). “Language modeling with gated\nconvolutional networks, ” in Proceedings of the International conference on\nmachine learning, Baltimore, ML, USA, December 2017 (PMLR), 933 –941.\nde Figueiredo, L. P., Grana, D., Bordignon, F. L., Santos, M., Roisenberg, M., and\nRodrigues, B. B. (2018). Joint Bayesian inversion based on rock-physics prior modeling\nfor the estimation of spatially correlated reservoir properties. Geophysics 83 (5),\nM49–M61. doi:10.1190/geo2017-0463.1\nDing, Ming, Yang, Zhuoyi, and Hong, Wenyi (2021). Cogview: mastering text-to-\nimage generation via transformers.https://arxiv.org/abs/2105.13290.\nDosovitskiy, Alexey, and Beyer (2020). An image is worth 16x16 words: Transformers\nfor image recognition at scale.https://arxiv.org/abs/2010.11929.\nF a n ,C . ,Z h a n g ,Y . ,a n dP a n ,Y .( 2 0 1 9 ) .“Multi-horizon time series forecasting\nwith temporal attention learning, ” in Proceedings of the 25th ACM SIGKDD\nInternational conference on knowledge discovery and data mining, Long Beach,\nCA, USA, July 2019, 2527–2535.\nFeng, R. (2020). Estimation of reservoi r porosity based on seismic inversion\nresults using deep learning methods.J. Nat. Gas Sci. Eng.77, 103270. doi:10.1016/j.\njngse.2020.103270\nGal, Y., and Ghahramani, Z. (2016). A theoretically grounded application of dropout\nin recurrent neural networks.Adv. neural Inf. Process. Syst.29.\nHan, Chi, Wang, Mingxuan, Ji, Heng, and Li, Lei (2021). Learning shared semantic\nspace for speech-to-text translation.https://arxiv.org/abs/2105.03095.\nHocheriter, S., Schmidhuber, J., and Cummins, F. (1997). Long short-term memory.\nNeural comput. 9 (8), 1735–1780. doi:10.1162/neco.1997.9.8.1735\nH u ,S . ,Z h u ,R . ,a n dW u ,S .( 2 0 1 8 ) .P r oﬁtable exploration and development of\ncontinental tight oil in China. Petroleum Explor. Dev. 45 (4), 737–748. doi:10.\n11698/PED.2018.04.20\nJ o h a n s e n ,T .A . ,J e n s e n ,E .H . ,M a v k o ,G . ,a n dD v o r k i n ,J .( 2 0 1 3 ) .I n v e r s er o c kp h y s i c sm o d e l i n g\nfor reservoir quality prediction.Geophysics78 (2), M1–M18. doi:10.1190/geo2012-0215.1\nLeite, E. P., and Vidal, Alexandre C. (2011). 3D porosity prediction from seismic\ninversion and neural networks. Comput. Geosciences37 8, 1174–1180. doi:10.1016/j.\ncageo.2010.08.001\nL e p i k h i n ,D . ,L e e ,H .J . ,a n dX u ,Y .( 2 0 2 1 ) .“GShard: Scaling giant models with\nconditional computation and automatic sharding, ” in Proceedings of the\nInternational Conference on Learning Representations, Vienna, Austria, May\n2021.\nLi, S. (2019b). Enhancing the locality and breaking the memory bottleneck of\ntransformer on time series forecasting.https://arxiv.org/abs/1907.00235.\nLi, S., Liu, B., Ren, Y., Chen, Y., Yang, S., Wang, Y., et al. (2019a). Deep-learning\ninversion of seismic data. IEEE Trans. Geoscience Remote Sens. 58 (3), 2135–2149.\ndoi:10.1109/tgrs.2019.2953473\nLim, B., Arık, S. Ö., Loeff, N., and Pﬁster, T. (2021). Temporal Fusion Transformers\nfor interpretable multi-horizon time series forecasting. Int. J. Forecast. 37 (4),\n1748–1764. doi:10.1016/j.ijforecast.2021.03.012\nLiu, Ze, Lin, Y., and Cao, Y. (2021). Swin transformer: Hierarchical vision transformer\nusing shifted windows.https://arxiv.org/abs/2103.14030.\nPang, M., Ba, J., Carcione, J. M., Zhang, L., Ma, R., and Wei, Y. (2021). Seismic\nidentiﬁcation of tight-oil reservoirs by using 3D rock-physics templates.J. Petroleum Sci.\nEng. 201, 108476. doi:10.1016/j.petrol.2021.108476\nSong, W., Feng, X., and Wu, G. (2021). Convolutional neural network, res-unet++,\nbased dispersion curve picking from noise cross-correlations.J. Geophys. Res. Solid\nEarth 126 (11), e2021JB022027. doi:10.1029/2021JB022027\nUs Energy Information Administration(Eia) (2013).Outlook For Shale Gas And Tight Oil\nDevelopment In The Us.W a s h i n g t o n ,D C ,U S A :U SE n e r gy Information Administration\nVaswani, A., Shazeer, N., and Parmar, N. (2017). Attention is all you need.Adv. neural\nInf. Process. Syst.30.\nWang, J., Cao, J., and You, J. (2020b). Log reconstruction based on gated recurrent\nunit recurrent neural network. Seg. Glob. Meet. Abstr. Society of Exploration\nGeophysicists,9 1–94. doi:10.1190/iwmg2019_22.1\nWang, J., Cao, J., and Yuan, S. (2022a). Deep learning reservoir porosity prediction\nmethod based on a spatiotemporal convolution bi-directional long short-term memory\nneural network model.Geomechanics Energy Environ.32.100282\nFrontiers inEarth Science frontiersin.org12\nSu et al. 10.3389/feart.2023.1137645\nWang, P., Chen, X., Li, J., and Wang, B. (2020a). Accurate porosity prediction for tight\nsandstone reservoir: A case study from North China.Geophysics 85 (2), B35–B47.\ndoi:10.1190/geo2018-0852.1\nW a n g ,Y . ,N i u ,L . ,Z h a o ,L . ,W a n g ,B . ,H e ,Z . ,Z h a n g ,H . ,e ta l .( 2 0 2 2 b ) .G a u s s i a n\nmixture model deep neural network and its application in porosity prediction of\ndeep carbonate reservoir. Geophysics 87 (2), M59 –M72. doi:10.1190/geo2020-\n0740.1\nWen, R. (2017). A multi-horizon quantile recurrent forecaster.https://arxiv.org/abs/\n1711.11053.\nZheng, M., Gao, P., and Wang, X. (2020). End-to-end object detection with adaptive\nclustering transformer. https://arxiv.org/abs/2011.09315.\nZhu, X., Su, W., and Lu, L. (2020). Deformable DETR: deformable transformers for\nend-to-end object detection.https://arxiv.org/abs/2010.04159.\nZou, C., Tao, S., and Hou, L. (2014).Unconventional petroleum geology. Beijing,\nChina: Geological Publishing House.\nZu, S., Ke, C., Hou, C., Cao, J., and Zhang, H. (2022). End-to-End deblending of\nsimultaneous source data using transformer.IEEE Geoscience Remote Sens. Lett.19, 1–5.\ndoi:10.1109/lgrs.2022.3174106\nFrontiers inEarth Science frontiersin.org13\nSu et al. 10.3389/feart.2023.1137645",
  "topic": "Porosity",
  "concepts": [
    {
      "name": "Porosity",
      "score": 0.9134379029273987
    },
    {
      "name": "Geology",
      "score": 0.6121387481689453
    },
    {
      "name": "Inversion (geology)",
      "score": 0.4688761234283447
    },
    {
      "name": "Transformer",
      "score": 0.4140610098838806
    },
    {
      "name": "Structural basin",
      "score": 0.313974529504776
    },
    {
      "name": "Geotechnical engineering",
      "score": 0.23668920993804932
    },
    {
      "name": "Engineering",
      "score": 0.09157344698905945
    },
    {
      "name": "Geomorphology",
      "score": 0.0915195643901825
    },
    {
      "name": "Voltage",
      "score": 0.08785232901573181
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I31595395",
      "name": "Chengdu University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210098205",
      "name": "State Key Laboratory of Oil and Gas Reservoir Geology and Exploitation",
      "country": "CN"
    }
  ],
  "cited_by": 8
}