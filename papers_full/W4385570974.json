{
    "title": "LMCap: Few-shot Multilingual Image Captioning by Retrieval Augmented Language Model Prompting",
    "url": "https://openalex.org/W4385570974",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2224406594",
            "name": "Rita Ramos",
            "affiliations": [
                "Instituto de Engenharia de Sistemas e Computadores Investigação e Desenvolvimento"
            ]
        },
        {
            "id": "https://openalex.org/A1990158554",
            "name": "Bruno Martins",
            "affiliations": [
                "Instituto de Engenharia de Sistemas e Computadores Investigação e Desenvolvimento"
            ]
        },
        {
            "id": "https://openalex.org/A2257973904",
            "name": "Desmond Elliott",
            "affiliations": [
                "Instituto de Engenharia de Sistemas e Computadores Investigação e Desenvolvimento"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2133459682",
        "https://openalex.org/W4225323055",
        "https://openalex.org/W3174685870",
        "https://openalex.org/W4385574335",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W4226155321",
        "https://openalex.org/W3215466129",
        "https://openalex.org/W3185293939",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2963349562",
        "https://openalex.org/W1956340063",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W4385571505",
        "https://openalex.org/W3153427360",
        "https://openalex.org/W4388778348",
        "https://openalex.org/W4226182655",
        "https://openalex.org/W4296406182",
        "https://openalex.org/W4385567345",
        "https://openalex.org/W4386076004",
        "https://openalex.org/W4299585995",
        "https://openalex.org/W2963532001",
        "https://openalex.org/W3193402170",
        "https://openalex.org/W3093871477",
        "https://openalex.org/W4386566587",
        "https://openalex.org/W2963048642",
        "https://openalex.org/W4320458302",
        "https://openalex.org/W4385571445",
        "https://openalex.org/W2481240925",
        "https://openalex.org/W4303444943",
        "https://openalex.org/W4288087322",
        "https://openalex.org/W2886641317",
        "https://openalex.org/W2109586012",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W4220993274",
        "https://openalex.org/W4226352076",
        "https://openalex.org/W3195577433",
        "https://openalex.org/W4301243929",
        "https://openalex.org/W3172942063",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W1889081078",
        "https://openalex.org/W4287113019",
        "https://openalex.org/W4386075661",
        "https://openalex.org/W2962787423",
        "https://openalex.org/W4312922092",
        "https://openalex.org/W3176641147",
        "https://openalex.org/W3202415077",
        "https://openalex.org/W3176587734",
        "https://openalex.org/W4286901992"
    ],
    "abstract": "Multilingual image captioning has recently been tackled by training with large-scale machine translated data, which is an expensive, noisy, and time-consuming process. Without requiring any multilingual caption data, we propose LMCAP, an image-blind few-shot multilingual captioning model that works by prompting a language model with retrieved captions. Specifically, instead of following the standard encoder-decoder paradigm, given an image, LMCAP first retrieves the captions of similar images using a multilingual CLIP encoder. These captions are then combined into a prompt for an XGLM decoder, in order to generate captions in the desired language. In other words, the generation model does not directly process the image, instead processing retrieved captions. Experiments on the XM3600 dataset of geographically diverse images show that our model is competitive with fully-supervised multilingual captioning models, without requiring any supervised training on any captioning data.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 1635–1651\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nLMC AP: Few-shot Multilingual Image Captioning by\nRetrieval Augmented Language Model Prompting\nRita Ramos† Bruno Martins† Desmond Elliott⋆\n†INESC-ID, Instituto Superior Técnico, University of Lisbon\n⋆Department of Computer Science, University of Copenhagen\nritaparadaramos@tecnico.ulisboa.pt\nAbstract\nMultilingual image captioning has recently\nbeen tackled by training with large-scale ma-\nchine translated data, which is an expensive,\nnoisy, and time-consuming process. Without\nrequiring any multilingual caption data, we pro-\npose LMC AP, an image-blind few-shot multi-\nlingual captioning model that works by prompt-\ning a language model with retrieved captions.\nSpecifically, instead of following the standard\nencoder-decoder paradigm, given an image,\nLMC AP first retrieves the captions of simi-\nlar images using a multilingual CLIP encoder.\nThese captions are then combined into a prompt\nfor an XGLM decoder, in order to generate cap-\ntions in the desired language. In other words,\nthe generation model does not directly process\nthe image, instead processing retrieved cap-\ntions. Experiments on the XM3600 dataset\nof geographically diverse images show that our\nmodel is competitive with fully-supervised mul-\ntilingual captioning models, without requiring\nany supervised training on any captioning data.\n1 Introduction\nThe task of image captioning has witnessed impres-\nsive performance gains with the trend of large-scale\nencoder-decoder models and vision-and-language\npre-training (Li et al., 2022; Wang et al., 2021; Hu\net al., 2022; Wang et al., 2022). Despite all of this\nprogress, existing models are mostly available on\nEnglish or are specialised for other high-resource\nlanguages. This limits the access to the technol-\nogy for a broader range of languages that exist in\nthe world. Moreover, the current mainstream trend\nresults in design decisions and methods that may\nonly work well for English-centric datasets or the\ncouple of languages for which captioning data is\navailable (Ruder, 2020). There is a need to de-\nvelop multilingual image captioning models that\ncan serve speakers of different languages.\nStill, scaling captioning models to a wide variety\nof languages involves different challenges. One\nmajor limitation is the lack of multilingual image-\ncaption pairs of clean labelled data for training the\nmodels. One possible solution is to automatically\ntranslate the existing English datasets (Thapliyal\net al., 2022). While effective, this approach can\nresult in models that learn translation artefacts, and\nperpetuates an English-centric perspective instead\nof encouraging the use of geographically diverse\nconcepts that are not overly specific to the west-\nern culture (Liu et al., 2021). Moreover, with or\nwithout automatic translations, training captioning\nmodels with multilingual data can be expensive,\ngiven the amount of data and number of parame-\nters needed to mitigate the curse of multilingual-\nity (Conneau et al., 2019; Goyal et al., 2021).\nThis paper presents LMC AP, an image-blind\nmultilingual image captioning model that does not\nrequire any training specific for image captioning.\nWe propose an efficient method that reuses a pre-\ntrained multilingual language model and adapts it\nto the vision-and-language captioning setting. Our\nwork is motivated by the recent \"Socratic Models\"\nframework (Zeng et al., 2022), in which different\nmodels can be combined through text prompting\n(e.g., image captioning can be achieved by prompt-\ning a language model with a set of visual concepts\nextracted from the predictions of a vision model).\nDifferent from the original Socratic Models, our\napproach is inspired by retrieval-augmented gen-\neration (Lewis et al., 2020; Izacard et al., 2022).\nSpecifically, a multilingual language model gener-\nates captions given a prompt consisting of the cap-\ntions retrieved from similar images, and a demon-\nstration of how to produce a caption in the desired\nlanguage. We note here that this is an image-blind\napproach, i.e. the language model producing the\ncaption does not actually process the image.\nOur main contributions are as follows: (1) We\npropose a few-shot multilingual image captioning\napproach named LMC AP, that re-uses pre-trained\nmodels without requiring any training specific for\n1635\nimage captioning; (2) To the best of our knowl-\nedge, LMC AP is the first captioning model with\nretrieval-augmented generation in a multilingual\nsetting, and in a few-shot setting of captioning; (3)\nWe report on experiments with the XM3600 bench-\nmark (Thapliyal et al., 2022) of human-authored\ncaptions and geographic diverse images, demon-\nstrating that LMC AP exhibits strong few-shot per-\nformance on a wide variety of languages; (4) We\nfurther show that LMC AP performs substantially\nbetter than the original Socratic Models. Moreover,\ninstead of only achieving competitive performance\nagainst other zero-shot models, LMC AP can also\ncompete with a large-scale supervised state-of-art\ncaptioning model.\n2 Background and Related Work\nImage Captioning: The task of automatically\ngenerating textual descriptions for input images\nhas been largely explored in English, while multi-\nlingual image captioning has only been addressed\nin a couple of studies (Gu et al., 2018; Thapliyal\net al., 2022; Chen et al., 2022). Like in most re-\ncent work on image captioning (Li et al., 2022;\nWang et al., 2021, 2022), studies addressing mul-\ntilingual setups have also focused on scaling the\nsize of encoder-decoder models and the amount\nof training data, resorting to machine translated\nversions of multimodal data to accommodate mul-\ntiple languages (Thapliyal et al., 2022). Differently\nfrom training a large-scale encoder-decoder model,\nwe follow a few-shot setting with an image-blind\napproach based on prompting.\nFew-Shot and Zero-Shot Approaches:Perform-\ning few-shot learning by prompting a language\nmodel with examples and demonstrations of a task\n(Brown et al., 2020; Radford et al., 2019; Schick\nand Schütze, 2020) is an efficient and effective al-\nternative to update model parameters. Similarly\nto other NLP tasks, recent work in the vision-and-\nlanguage domain has used prompt-based learning\nby building on top of pre-trained language and vi-\nsion models, although usually also involving ex-\ntra multimodal training (Tsimpoukelli et al., 2021;\nAlayrac et al., 2022; Jin et al., 2021). In our work,\nwe follow a similar few-shot prompting approach\nto the recent Socratic Models (Zeng et al., 2022)\nthat do not involve any multimodal training, as\ndescribed next. In image captioning, there have\nalso been zero-shot methods that similarly to our\napproach do not involve any training, by relying\non prompts or adaptations over the decoding algo-\nrithms, such as ZeroCap (Tewel et al., 2021) and\nConZic (Zeng et al., 2023). However, these mod-\nels work for English and not for the multilingual\ncaptioning setting.\nSocratic Models: Zeng et al. (2022) proposed\nthe Socratic Models (SMs) framework, where dif-\nferent multimodal pre-trained models communicate\nvia zero-shot or few-shot prompting. For the task\nof image captioning, SMs generate captions by\nprompting a language model (i.e., GPT-3 (Brown\net al., 2020)) with information about the input im-\nage obtained with another pre-trained model (i.e.,\nCLIP (Radford et al., 2021)). The visual infor-\nmation is in this way represented into a language-\nbased prompt, containing the number of people pre-\nsented in the image, the places, objects, and what is\nthe type of image. We explore a similar approach\nin the multilingual setting by reusing multilingual\nmodels, and through a retrieval-based prompt.\nRetrieval-augmentation: The knowledge from\nlanguage models can be adapted and expanded by\ncombing non-parametric knowledge from datas-\ntores (i.e., external memories) (Khandelwal et al.,\n2019; Lewis et al., 2020; Izacard et al., 2022; Ram\net al., 2023). The success of conditioning gen-\neration with retrieved information, in several dif-\nferent NLP tasks, has inspired some recent stud-\nies in image captioning (Ramos et al., 2023a; Fei,\n2021; Sarto et al., 2022; Ramos et al., 2023b). The\nstudy that is most closely related to our caption-\ning model is SmallCap (Ramos et al., 2023b), an\nencoder-decoder model that is prompted with re-\ntrieved captions as well. However, in image cap-\ntioning, retrieval-augmentation has mostly being\nexplored with supervised learning and not few-\nshot learning. Moreover, retrieval-augmentation\nremains unexplored in the multilingual scenario.\n3 Model\nLanguage Model Prompt-based Captioning\n(LMC AP) is a few-shot multilingual captioning\nmodel augmented with retrieval. It involves\nprompting a Language Model (LM) with captions\nretrieved from a datastore by a Vision-and-\nLanguage Model (VLM). Captions are generated\nin an image-blind manner, without actually\nprocessing the visual contents of the input image,\ninstead using a prompt containing the retrieved\ncaptions. The method works as follows: first,\n1636\ngiven an input image, the VLM is used to find\nrelevant captions in the datastore. Second, the\nretrieved captions are converted to a language\nprompt, which is encoded by the multilingual\nLM to generate captions in a desired language,\nconditioning the generation on the prompt. Finally,\nthe set of generated captions can be scored by the\nVLM against the input image, to select the best\ncaption. The main aspects of our approach are\nshown in Figure 1 and fully detailed next.\nImage-Text Retrieval: The input image and a\ndatastore of captions are encoded by a multilingual\nCLIP (Carlsson et al., 2022), i.e. a VLM that can\nbe used to calculate image-text similarity. In this\nway, given the encoded data, M-CLIP is used to\nretrieve the Kmost similar captions from the data-\nstore. The datastore contains captions associated\nto diverse images, which can be in English or an-\nother language. The retrieved captions will serve\nto guide a language model as an example of what\nthe predicted caption should resemble, through the\nuse of a prompt and as described next.\nRetrieval-augmented Prompting: The retrieved\ncaptions, which represent the visual information\nabout the image, are formatted into a prompt for\nthe language model. The prompt starts with fixed\nN-shot examples and ends with the retrieval infor-\nmation about the input image, to guide the language\nmodel. Each shot is a demonstration of how to gen-\nerate a caption in a desired language for an image,\ngiven a set of retrieved captions. After these N-\nexamples, the prompt terminates with the retrieved\ninformation about the actual input image. An ex-\nample of the format of the prompt can be seen in\nFigure 1 and in more detail in Appendix D. We\nnote that the retrieved captions, either from the\nfixed N-shot examples or those corresponding to\nthe input image, can be presented in any language\nor in multiple languages.\nPrompting Multilingual Text Generation:The\naforementioned prompt is used as input for an\nXGLM (Lin et al., 2021) pre-trained multilingual\nautoregressive LM, to generate captions in a given\nlanguage. XGLM is applied in a few-shot setting,\nwhich means that LMC AP does not require any\ntraining (i.e., the captions are generated by pro-\nviding the prompt at inference time to XGLM).\nCaptions are generated in the desired language by\nincluding an example in the N demonstrations in\nthe prompt, as shown in Figure 1.\nMultilingual Reranking: After the LM gener-\nates a set of captions, the multilingual VLM per-\nforms a final image–text similarity step to find the\ncaption that best describes the input image. This\nis based on the same M-CLIP model used for the\ninitial image–text retrieval.\n4 Evaluation\nIn this section, we describe the evaluation of LM-\nCAP. We describe the experimental setup and re-\nsults, and we also present ablation studies and fur-\nther discussions about our approach.\n4.1 Experimental Setup\nModel: LMC AP uses two pre-trained mul-\ntilingual models, namely the autoregressive\nXGLM language model facebook/xglm-2.9B,\nand the multilingual M-CLIP vison-and-language\nmodel xlm-roberta-large-ViT-H-14, respec-\ntively available on HuggingFace (Wolf et al., 2020)\nand OpenCLIP1. Our approach does not require\nany training, generating captions at inference time\nusing a single NVIDIA V100S 32GB GPU.\nTo generate a caption in a desired language,\nXGLM is prompted with retrieved captions ex-\ntracted by the M-CLIP model. For caption retrieval,\nthe input image and a set of captions from a datas-\ntore are both encoded by M-CLIP to perform direct\nimage-text search. The datastore contains English\ncaptions from the COCO training set and is indexed\noffline with the nearest-neighbour search library\nnamed FAISS (Johnson et al., 2017), using the in-\ndex IndexFlatIP that does not involve training.\nA set of K=4 retrieved captions are used in the\nprompt for the input image, along with a fixed set\nof N=3-shot examples, as described in Appendix\nD. Conditioned on the prompt, XGLM generates\ncaptions using beam-search decoding with a beam\nof 3. A set of c=3 candidate captions are re-ranked\nusing M-CLIP, to select the final generated caption\nin the desired language. The code for LMC AP is\nmade freely available2.\nDatasets: We mainly evaluate our approach\non XM3600, i.e. a multilingual image cap-\ntioning dataset (Thapliyal et al., 2022) featur-\ning geographically-diverse images, collected from\nOpen Images with basis on the regions of 36 lan-\nguages. For each language, 100 images were se-\nlected and annotated with human generated cap-\n1https://github.com/mlfoundations/open_clip\n2https://github.com/RitaRamo/lmcap\n1637\nGENERATE IN \nTARGET LANGUAGE\nLM\n“um rapaz a fazer truques de \nskate.”\nPROMPTING\nPROMPT-TEMPLATE\nSimilar images are described as: \n“a man and woman in a wedding”, \n“a couple getting married”, ... \nA caption for this image in portuguese is: \numa mulher e um homem a casar-se. \n…\nSimilar images are described as: \n“a young boy doing a skate trick”, \n“a man skating on a wall”, … \nA caption for this image in portuguese is:\nDATASTORE\nOF CAPTIONS\nK RETRIEVED \nCAPTIONS\nN-shot \nexamples\nM-CLIP\nInput\nexample\nINPUT IMAGE\nFigure 1: Illustration of the key aspects of LMC AP, a few-shot multilingual image captioning approach that re-uses\npre-trained unimodal models without requiring any training. In our image-blind approach, a multilingual language\nmodel (XGLM) is prompted with information retrieved with a multilingual CLIP model. The prompt contains a set\nof N-shot examples and Kretrieved captions, to guide caption generation in a desired language.\ntions, resulting in a total of 3600 images and\n261375 captions across the 36 languages. XM3600\ndoes not contain training or validation splits.\nFor validation and hyperparameter tuning, we\nrelied on the COCO (Chen et al., 2015) valida-\ntion split (COCO-DEV) from the standard Karpa-\nthy splits (Karpathy and Fei-Fei, 2015). For “ref-\nerence captions”, we machine translate the En-\nglish captions into Spanish, Hindi, and Chinese,\nusing the M2M-100 model (Fan et al., 2021),\nsimilarly in spirit to Thapliyal et al. (2022) who\nused the Google Translate API 3. We make this\ndevelopment set available to the community at\nhttps://github.com/RitaRamo/lmcap. As pre-\nviously mentioned, we also use the captions from\nthe COCO training set to build the datastore. The\ndatastore simply contains the original English cap-\n3https://cloud.google.com/translate\ntions from COCO without incurring in an expensive\nand noisy machine translation process, unlike in\nthe study from Thapliyal et al. (2022).\nModel Assessment and Comparison:We com-\npare LMC AP with the four multilingual models\nproposed by Thapliyal et al. (2022). These mod-\nels combine different mT5 (Xue et al., 2020) and\nViT (Zhai et al., 2022) versions and are trained\nin a fully-supervised fashion on COCO-35L and\nCC3M-35L, i.e., Google’s machine translation API\nversions of the original COCO and CC3M datasets\n(Chen et al., 2015; Sharma et al., 2018). Specif-\nically, BB+CC combines mT5-base and ViT-B/16\npretrained on CC3M-35L and finetuned on COCO-\n35L; BB is trained on COCO-35L; Bg switches to\nthe ViT-g/14 model; and Lg uses mT5-large and\nand ViT-g/14, also trained with COCO-35L. For\nreference, Thapliyal et al. (2022) spent 5000 TPU\n1638\nhours to train their models, while our method can\nbe used out-of-the-box for inference, i.e., 45 min-\nutes for the X3600 benchmark per language.\nFollowing Thapliyal et al. (2022), results are re-\nported with the CIDEr (Vedantam et al., 2015) met-\nric for English, Spanish, Hindi, and Chinese, with\nother languages covered in Section 4.4. CIDEr is\na standard captioning metric that computes how\nwell the generated caption matches the consen-\nsus of the reference captions, based on Term Fre-\nquency–Inverse Document Frequency (TF-IDF). In\nAppendix A, we included more generation metrics\nfor holistic evaluation. To compute the metrics,\nwe used the COCO evaluation package 4, and the\nSacreBLEU tokenization (Post, 2018).\n4.2 Results\nXM3600: Following Thapliyal et al. (2022), we\nreport results on XM3600 for English, Spanish,\nHindi, and Chinese, in Table 1. We can see that\nLMC AP outperforms all supervised approaches on\nChinese, and achieves competitive performance on\nthe other languages, despite being image-blind and\nnot being trained on any image captioning data. For\nEnglish, Spanish, and Hindi, we note that LMC AP\nis only outperformed by the large-scale supervised\nvariant BB+CC, pre-trained on CCM3 and fine-\ntuned on COCO, jointly on English and the other\n35 languages for the two datasets, i.e., with 123M\ncaptions. For the other variants that are only trained\non COCO-35L, our model has a substantially larger\nperformance on the CIDER metric across all four\nlanguages. We also show that our model can further\nbenefit from increasing the datastore (LMC AP+),\nas described in more detail over Section 4.3.\nCOCO: For completeness, we also report results\non the machine translated COCO-DEV set in Table\n2. In the top half of the table we show the per-\nformance of the 4 SOTA models on COCO-DEV\nvia Google’s machine translation API. Since this\ndataset was not provided by the authors, we per-\nform as well automatic machine-translation but us-\ning the M2M-100 model (Fan et al., 2021), which\ngives an approximation for model comparison on\nCOCO. As expected, LMC AP is outperformed on\nCOCO since all the 4 variants were trained on it\nacross 36 languages, with a large number of train-\nable parameters. Our model still reaches impres-\nsive performance, considering it was not trained on\n4Available at https://github.com/tylin/\ncoco-caption\nModel en es hi zh\nMultilingual Captioning Supervised Learning\nBB+CC 0.584 0.425 0.197 0.202\nBB 0.297 0.194 0.098 0.087\nBg 0.337 0.232 0.112 0.110\nLg 0.343 0.220 0.111 0.099\nFew-shot Learning\nLMC AP 0.452 0.329 0.132 0.221\nLMC AP+ 0.526 0.326 0.078 0.251\nTable 1: Results on the geographically-diverse XM3600\nbenchmark. We compare our few-shot LMC AP model\nagainst large-scale supervised multilingual and multi-\nmodal SOTA models proposed by Thapliyal et al. (2022).\nBest results in bold and second-best underlined.\nCOCO for any of those languages, neither was it\ntrained on any multimodal data. This is especially\nthe case for English, where our model reaches a\nsimilar CIDEr score, although it only reaches about\nhalf the performance for the other languages. In Ap-\npendix B, we also compare LMC AP with prompt-\nbased captioning methods that were specially de-\nsigned for English.\nModel |θ| en es hi zh\nCOCO-DEV-GOOGLE\nBB+CC 766 0.980 0.962 0.759 0.748\nBB 1230 0.856 0.844 0.671 0.659\nBg 1691 0.851 0.835 0.718 0.695\nLg 2241 0.875 0.859 0.624 0.656\nCOCO-DEV-M2M100\nLMCAP N/A 0.767 0.453 0.334 0.584\nTable 2: CIDEr performance on the COCO dataset. The\ntop of the table presents SOTA results on the COCO\nvalidation split, translated via the GOOGLE API. The\nbottom rows of the table shows our model performance\non COCO-DEV , translated via the M2M-100 model.|θ|\ncorresponds to the number of trainable parameters in\nthe model (in millions).\n4.3 Ablation Studies\nTo better understand the design choices ofLMC AP,\nwe report a series of ablation tests on COCO-DEV ,\nto avoid direct tuning on the XM3600 benchmark.\nPrompt: Given that LMC AP works by prompt-\ning a language model with K retrieved captions\n1639\nand N-shot examples, we study the effect of our\nprompt when varying Kand N. Table 3 shows the\nimportance of not depending on a single retrieved\ncaption across the 4 languages. This is similar to\nprevious findings in retrieval-augmentated caption-\ning studies focusing on English (Sarto et al., 2022;\nRamos et al., 2023b), which showed that a large\nK makes the model more robust to mismatched\ncaptions. We further see that English and Spanish\nbenefit from encoding a larger set of retrieved cap-\ntions, while Hindi and Chinese work better with a\nsmaller K. We select K = 4since it has close-to-\noptimal performance for each of the languages. We\nthen explore varying the number of N-shot exam-\nples, and found N = 3to be the optimal value on\nall the four the languages. We thus use K = 4and\nN = 3in the prompt of LMC AP.\nSetup en es hi zh\nVarying K-Captions\nK=1, N=1 0.622 0.380 0.240 0.522\nK=2, N=1 0.654 0.400 0.269 0.562\nK=3, N=1 0.695 0.414 0.211 0.565\nK=4, N=1 0.711 0.415 0.229 0.554\nK=5, N=1 0.734 0.424 0.205 0.529\nVarying N-Shot\nK=4, N=1 0.711 0.415 0.229 0.554\nK=4, N=2 0.735 0.440 0.247 0.583\nK=4, N=3 0.767 0.454 0.334 0.584\nK=4, N=4 0.757 0.424 0.318 0.580\nTable 3: The effect of using different numbers of K\nretrieved captions and N few-shot examples. Results\nreported on COCO-DEV with best results in bold.\nDatastore: We also studied different contents for\nthe datastore beyond the English captions from the\nCOCO training set, shown in Table 4. Given that\nour model reaches much better performance on En-\nglish, we hypothesise that our model can better gen-\nerate captions in a desired language when having\nthe retrieved captions in that same language. This\ncould be validated using translations from COCO\nin the other languages, but since those are not avail-\nable, we instead used a machine translated version\nof the Conceptual Captions dataset (CCM3) from\nQiu et al. (2022). We used the English, Spanish,\nand Chinese versions of the CCM3 training set, re-\nspectively for each of the corresponding languages\n(CCM3-L). We found that performance deterio-\nrates on the COCO-DEV dataset, which might be\nexplained by the difference between the COCO\nand CCM3-L datasets. Even combining the two\ndatasets (COCO + CCM3-L) is worse than using\nonly the COCO dataset.\nIn an attempt to cover more diverse concepts, we\naugmented COCO with three large web datasets\n(Conceptual Captions (Sharma et al., 2018), Con-\nceptual 12M (Changpinyo et al., 2021), and SBU\ncaptions (Ordonez et al., 2011)), using their noise-\nfree versions (Li et al., 2022). We refer to this\ndataset as CCS, and it contains synthetic model-\ngenerated texts for the web images. Using CCS\nleads to an improvement compared to just using\nCOCO, except for Hindi. In Table 1, we also report\nresults on XM3600 with this best datastore config-\nuration, for which the performance again decreases\nfor Hindi, but has a substantial improvement on\nEnglish and Chinese. The benefits of including\na more diverse collection of captions are further\nshown in Apprendix E with some qualitative ex-\namples (e.g., LMC AP was now able to generate\nthe french concept macarons in English). Notice\nthat the retrieved captions from CCS are still on\nEnglish. Thus, although there is lack of multilin-\ngual image-caption pairs with clean labelled data,\nit would be interesting to pursue further work on\nincorporating retrieved information from other lan-\nguages, in order to improve performance to levels\nsimilar to those for English.\nDatastores en es hi zh\nCOCO 0.711 0.415 0.229 0.554\nCC3M-L 0.387 0.309 - 0.337\nCOCO + CC3M-L 0.601 0.359 - 0.481\nCOCO + CCS 0.713 0.431 0.212 0.563\nTable 4: Datastore ablations on COCO-DEV , where\ncaptions are retrieved from different sources of data.\nCC3M-L corresponds to machine translated version\nof Conceptual Captions proposed in Qiu et al. (2022)\n(Hindi not available), while CCS refers to the Concep-\ntual Captions, Conceptual 12M, and SBU datasets (Li\net al., 2022).\nModel Size: In Table 5, we show the importance\nof using a language model that has a sufficiently\nlarge number of parameters. Both XGLM-562M\nand XGLM-1.7B are unable to generate captions\nbeyond English. On the other hand, the 7.5B vari-\nant can lead to a stronger performance, but large-\n1640\nscale LMs require more GPU memory, which lim-\nits the size of the prompt that can be encoded with\nmodest hardware5. LMC AP uses the more efficient\nXGLM-2.9B version. These results are in line with\nprevious findings, which suggest that stronger few-\nshot performance is achieved when the prompt is\nencoded by large LMs (Brown et al., 2020).\nParams Config. RAM en es hi zh\n564M K=4, N=3 6G 0.411 0.094 0.030 0.146\n1.7B K=4, N=3 12G 0.637 0.143 0.066 0.272\n2.9B K=4, N=3 16G 0.767 0.454 0.334 0.584\n7.5B K=4, N=3 22G 0.787 0.489 0.365 0.644\nTable 5: CIDEr performance on COCO-DEV , across\nthe different variants of XGLM, to show the scaling be-\nhaviour of the LM used in LMC AP. RAM corresponds\nto the GPU memory consumption.\n4.4 Additional Discussion\nWe now discuss the performance ofLMC AP across\nthe 36 languages, taking into consideration the data\nthat was used for pre-training the LM. We also com-\npare our approach with SMs and a simple baseline\nof retrieval plus translation. To support quantitative\nevaluation, we show some qualitative examples.\nMultilingual Pre-training: In Table 6, we report\nthe results of LMC AP on XM3600 for all the 36\nlanguages considered in the dataset, ordered by the\npercentage of pre-training data used in XGLM for\neach language. LMC AP shows strong few shot per-\nformance on the diverse set of languages in which\nXGLM was pre-trained on. Similarly to BB+CC\nand Lg models, which are limited to the 36 lan-\nguages they were trained on, our model is also\ndependent on the LM pre-training data, although\nthere is potential to replace XGLM by another large\nLM, in order to generalize to other languages.\nComparision with Socratic Models:Since LM-\nCAP is inspired in Socratic Models (SMs), we com-\npare them against our approach. For this, XGLM\nreceives the Socratic prompt that includes the im-\nage type, the number of people, places and object\ncategories6, instead of our retrieved captions. Re-\nsults are reported in Table 7. Compared to either\nzero-shot or few-shot SMs, we can see that our\nmodel largely outperforms SMs, with a noteworthy\n5We had to run the largest model in half precision (float16).\n6Using the original code at https://colab.\nresearch.google.com/drive/1KOlc9nN0NJ5GAif_\ndmuOqsRqlZycoIrc?usp=sharing\nBB+CC L G LMC AP\nen 0.584 0.343 0.452\nru 0.194 0.089 0.134\nzh 0.202 0.099 0.221\nde 0.224 0.130 0.153\nes 0.425 0.220 0.329\nfr 0.410 0.217 0.260\nja 0.254 0.141 0.161\nit 0.321 0.168 0.226\npt 0.380 0.202 0.283\nel 0.199 0.101 0.136\nko 0.288 0.152 0.157\nfi 0.177 0.089 0.112\nid 0.307 0.167 0.151\ntr 0.232 0.122 0.103\nar 0.227 0.106 0.107\nvi 0.336 0.182 0.265\nth 0.418 0.226 0.166\nhi 0.197 0.111 0.132\nbn 0.200 0.133 0.022\nsw 0.319 0.151 0.085\nte 0.196 0.099 0.042\nLanguages not in XGLM pretraining data\ncs 0.313 0.139 0.005\nda 0.329 0.192 0.020\nfa 0.311 0.155 0.002\nhe 0.230 0.098 0.001\nhr 0.224 0.085 0.001\nhu 0.175 0.096 0.006\nmi 0.405 0.243 0.015\nnl 0.441 0.232 0.082\nno 0.385 0.230 0.025\npl 0.236 0.108 0.003\nro 0.188 0.100 0.007\nsv 0.370 0.225 0.077\nuk 0.189 0.081 0.006\nA VG∗ 0.290 0.154 0.176\nTable 6: Results for the 36 languages on the XM3600\nbenchmark, ordered by the pre-training language ratio\nof XGLM. BB+CC and LG are full-supervised state-of-\nthe-art approaches from Thapliyal et al. (2022). A VG∗\ncorresponds to the average performance across the lan-\nguages on which XGLM was pre-trained.\nCIDER improvement of more than 39.1% on En-\nglish, 20.0% on Spanish, 11.5% on Hindi, and of\n21.4% Chinese. This confirms the effectiveness of\nour retrieval-augmented LM prompting approach.\n1641\nen: “a young man is standing in front of \nmicrophones” \nes: “un joven presenta algo en un micro” (a young \nman presents something on a microphone)\nhi: “एक युवा व्यित एक लैपटॉप क े सामने खड़ा है” (a \nyoung man stands in front of a laptop)\nzh: “一个年轻的男子站在他的电脑前,他准备开始演\n讲” (a young man standing in front of his \ncomputer, ready to give a speech)\nen: “a woman sitting in front of a cake for her \nbirthday” \nes: “un pastel de cumpleaños” (a birthday cake)\nhi: “एक बहुत ही सुंदर और स्वादष्ट जन्मदन क े क” (a \nvery nice and delicious birthday cake)\nzh: “一个老妇人坐在她的生日蛋糕前” (an old lady \nsits in front of her birthday cake)\nen: “two people and a kid skiing along a trail” \nes: “dos hombres y un niño esquiando en una \npista de nieve” (two men and a boy skiing on a \nsnow slope)\nhi: “दो लोग और एक बच्चा स्कीइंग क े रास्ते पर चल \nरहा है (two men and a child are walking on the \nway to skiing)\nzh: “两个大人和一个小男孩在雪地上滑雪” (two \nadults and a little boy skiing on the snow)\n- a young man holds a microphone while staring \nat a laptop computer\n- a man is standing in front of microphones\n- the emcee is ready to introduce the first \nspeaker\n- blurry photograph of a young man presenting \nsomething\n- two people and a kid skiing along a trail\n- an adult and two small children are cross \ncountry skiing\n- two men and a little boy are skiing on a snowy \nspot\n- two adults on skis with a child on skis between \nthem\n- a large square cake with pink candles sticking \nout of it\n- a man twenty ninth birthday consisted of a \nfamily dinner and a homemade cake\n- an elderly woman celebrates her 90th birthday \nwith a cake\n- a woman sitting in front of a birthday cake for \nher 90th birthday\nFigure 2: Examples of captions generated by LMC AP for English, Spanish, Hindi, and Chinese, on XM3600 images\nand based on retrieved captions regarding each blind-input image.\nModel en es hi zh\nSocratic 0.067 0.045 0.001 0.031\nSocratic N=1 0.454 0.280 0.176 0.340\nSocratic N=2 0.344 0.215 0.141 0.268\nSocratic N=3 0.376 0.254 0.219 0.370\nLMCAP 0.767 0.454 0.334 0.584\nTable 7: Comparison to Socratic Models (SMs) on the\nCOCO-DEV dataset. LMC AP clearly outperforms SMs,\nas highlighted by bold.\nBaseline of Retrieval with Translation:We also\ncompared our approach against a baseline that re-\ntrieves the nearest caption on English and translates\nit into other languages in Table 8, using the M2M-\n100 model. This is to quantify the impact of prompt-\ning the language model compared to performing\ndirect translation on retrieved captions. On COCO-\nDEV , we see thatLMC AP only outperforms these\nresults on English. Notice, however, that the ref-\nerences on COCO-DEV for the other languages\nrely on the M2M-100 distributions, as the baseline,\npromoting to an inequitable CIDEr. When evaluat-\ning on human-labeled data, as is the case with the\nXM3600 dataset, we see the benefits of prompting\nwith retrieval information.\nNotice also both LMC AP and the retrieval base-\nline outperform the BB model (the later also com-\npetitive to the other 3 SOTA variants), despite train-\ning with large-scale multimodal machine translated\ndata for hours. This shows the clear benefits of\nusing retrieval-augmentation in multilingual image\ncaptioning, not just for result quality but to avoid\nhigh computation costs as well.\nModel en es hi zh\nCOCO-DEV\nLMCAP 0.767 0.454 0.334 0.584\nBaseline M2M-100 0.590 0.563 0.548 0.714\nXM3600\nLMCAP 0.452 0.329 0.132 0.221\nBaseline M2M-100 0.333 0.205 0.120 0.170\nBB: COCO-35L 0.297 0.194 0.098 0.087\nTable 8: Comparison to direct translation on retrieved\ncaptions (Baseline), on COCO-DEV and XM3600.\nQualitative Results: Figure 2 shows examples of\ncaptions generated in different languages by LM-\nCAP, together with the retrieved captions that are\nprovided in the prompt regarding each blind-input\nimage. Qualitative examples tend to show diver-\nsity in the generation across the languages, with\nthe retrieved information being itself diverse. For\ninstance, in the first example, for English and Span-\nish, LMC AP focuses on describing that a man is\n1642\nin front of microphones (i.e., based on the first two\nretrieved captions). In turn, for Hindi and Chinese,\nthe man is in front of a laptop (i.e., from the first\nexample), and the captions can also mention that\nhe is ready to give a speech in Chinese (i.e., given\nthe last two retrieved captions). In the second im-\nage, we can see that LMC AP can simply copy a\nretrieved caption to generate in English, while for\nthe other languages the model may come up with\nterms not directly present in the retrieved captions\n(e.g., “snow slope” in Spanish). The last image\nis a negative example, where incorrect retrieved\ncaptions led the model into errors in English and\nChinese, showing that there are also limitations in\nour image-blind approach. For more examples, see\nAppendix C.\n5 Conclusions\nThis paper proposes LMC AP, an image-blind few-\nshot multilingual image captioning model. LM-\nCAP is based on prompting a language model with\nN-shot examples and retrieved captions extracted\nby a vision-and-language model, to condition cap-\ntion generation in a desired language with a multi-\nlingual language model. On XM3600, i.e. a human-\nlabelled massively multilingual multimodal bench-\nmark, LMC AP performs competitively against the\nstate-of-the-art without involving expensive train-\ning with large-scale translated multimodal data, or\nwith any captioning data. Experimental results fur-\nther demonstrate that LMC AP largely outperforms\nSocratic Models (Zeng et al., 2022), showing that\nretrieval augmentation plays a crucial role in our\nprompting approach. As future work, we plan to\nfurther assess the use of multilingual data in the\ndatastore, as well as the impact of directly promot-\ning diversity (Ye et al., 2022; Levy et al., 2022) in\nthe captions used in the prompt.\nAcknowledgements\nThis research was supported by the Portuguese\nRecovery and Resilience Plan through project\nC645008882-00000055, through Fundação para\na Ciência e Tecnologia (FCT) with the Ph.D. schol-\narship 2020.06106.BD, and through the INESC-\nID multi-annual funding from the PIDDAC pro-\ngramme (UIDB/50021/2020).\nLimitations\nImage captioning and multilingual image caption-\ning studies tend to focus on the COCO dataset,\nwhich was shown to contain gender imbalance. Pre-\nvious research has also showed that models trained\non COCO tend to amplify this bias (Hendricks\net al., 2018; Zhao et al., 2017). While our model\nis not trained on COCO or in any captioning data,\nit relies on a pre-trained language model, which\nis known to suffer from different sources of bias\nand fairness issues (Bommasani et al., 2021; Sheng\net al., 2021; Schramowski et al., 2022).\nOur model also involves retrieval-augmentation\nwith captions extracted by a vision-and-language\nmodel, also pre-trained in an unsupervised man-\nner. Like in the case of other retrieval-augmented\ngenerative models (Lewis et al., 2020), LMC AP\nhas inherently a bias towards the retrieved infor-\nmation. Notwithstanding, by conditioning on in-\nformation from a datastore with clean and curated\ntext, LMC AP has potential to ameliorate some of\nthe generation issues of the language model (e.g.,\nelude hateful or violent language). To have in-\nsights on the biases presented in LMC AP, we rec-\nommend analysing the retrieved captions used by\nthe model, since they provided cues to the predic-\ntions, as shown in Figure 2. We argue that it can\nbe much harder to have a direct interpretation for\ncaptioning models that are not retrieval-augmented.\nAnother limitation of our model relates to it fol-\nlowing a full image-blind approach, which heav-\nily depends on information from similar captions\ninstead of the visual content from the actual in-\nput image. To address this limitation, future work\ncould additionally include concepts extracted from\nthe image in the prompt, as proposed in Socratic\nModels, combined with the retrieved information.\nEthics Statement\nThe datasets supporting the evaluation of LMC AP\nare publicly available for academic purposes. We\nalso plan to release our code, and the additional re-\nsources that were built to support the experiments.\nWe emphasise that LMC AP challenges\nthe efficiency of most current captioning ap-\nproaches, in terms of resource usage and\ndevelopment/deployment effort, while at the same\ntime promoting more equitability and inclusion,\nexemplified here by attempting to balance language\nrepresentation at low computational costs.\nWe further note that while our model attempts to\nadvance research beyond English-centric caption-\ning, by considering captioning for a wide variety\nof languages, it is important to address and pay\n1643\nmore attention to low-resource languages as well\n(i.e., languages beyond those covered in our tests).\nEvaluating LMC AP with additional datasets, cov-\nering an even larger set of languages and concepts,\nwould be desirable.\nReferences\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds,\net al. 2022. Flamingo: a visual language model for\nfew-shot learning. arXiv preprint arXiv:2204.14198.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. volume 33, pages 1877–1901.\nFredrik Carlsson, Philipp Eisen, Faton Rekathati, and\nMagnus Sahlgren. 2022. Cross-lingual and mul-\ntilingual CLIP. In Proceedings of the Language\nResources and Evaluation Conference, pages 6848–\n6854.\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and\nRadu Soricut. 2021. Conceptual 12m: Pushing web-\nscale image-text pre-training to recognize long-tail\nvisual concepts. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 3558–3568.\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Pier-\ngiovanni, Piotr Padlewski, Daniel Salz, Sebastian\nGoodman, Adam Grycner, Basil Mustafa, Lucas\nBeyer, et al. 2022. PaLI: A jointly-scaled mul-\ntilingual language-image model. arXiv preprint\narXiv:2209.06794.\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\nishna Vedantam, Saurabh Gupta, Piotr Dollár, and\nC Lawrence Zitnick. 2015. Microsoft COCO cap-\ntions: Data collection and evaluation server. arXiv\npreprint arXiv:1504.00325.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2019. Unsupervised\ncross-lingual representation learning at scale. arXiv\npreprint arXiv:1911.02116.\nMichael Denkowski and Alon Lavie. 2014. Meteor\nuniversal: Language specific translation evaluation\nfor any target language. In Proceedings of the ninth\nworkshop on statistical machine translation, pages\n376–380.\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\nMa, Ahmed El-Kishky, Siddharth Goyal, Mandeep\nBaines, Onur Celebi, Guillaume Wenzek, Vishrav\nChaudhary, et al. 2021. Beyond english-centric mul-\ntilingual machine translation. J. Mach. Learn. Res.,\n22(107):1–48.\nZhengcong Fei. 2021. Memory-augmented image cap-\ntioning. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 35, pages 1317–1324.\nNaman Goyal, Jingfei Du, Myle Ott, Giri Ananthara-\nman, and Alexis Conneau. 2021. Larger-scale trans-\nformers for multilingual masked language modeling.\narXiv preprint arXiv:2105.00572.\nJiuxiang Gu, Shafiq Joty, Jianfei Cai, and Gang Wang.\n2018. Unpaired image captioning by language pivot-\ning. In Proceedings of the European Conference on\nComputer Vision, pages 503–519.\nLisa Anne Hendricks, Kaylee Burns, Kate Saenko,\nTrevor Darrell, and Anna Rohrbach. 2018. Women\nalso snowboard: Overcoming bias in captioning mod-\nels. In Proceedings of the European Conference on\nComputer Vision, pages 771–787.\nXiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang,\nZicheng Liu, Yumao Lu, and Lijuan Wang. 2022.\nScaling up vision-language pre-training for image\ncaptioning. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition,\npages 17980–17989.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nWoojeong Jin, Yu Cheng, Yelong Shen, Weizhu Chen,\nand Xiang Ren. 2021. A good prompt is worth\nmillions of parameters? low-resource prompt-based\nlearning for vision-language models. arXiv preprint\narXiv:2110.08484.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2017.\nBillion-scale similarity search with GPUs. arXiv\npreprint arXiv:1702.08734.\nAndrej Karpathy and Li Fei-Fei. 2015. Deep visual-\nsemantic alignments for generating image descrip-\ntions. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n3128–3137.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2019. Generalization\nthrough memorization: Nearest neighbor language\nmodels. arXiv preprint arXiv:1911.00172.\n1644\nItay Levy, Ben Bogin, and Jonathan Berant. 2022.\nDiverse demonstrations improve in-context\ncompositional generalization. arXiv preprint\narXiv:2212.06800.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive NLP tasks. In Advances in Neural Infor-\nmation Processing Systems, volume 33, pages 9459–\n9474.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven\nHoi. 2022. BLIP: Bootstrapping language-image pre-\ntraining for unified vision-language understanding\nand generation. arXiv preprint arXiv:2201.12086.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, et al. 2021.\nFew-shot learning with multilingual language models.\narXiv preprint arXiv:2112.10668.\nFangyu Liu, Emanuele Bugliarello, Edoardo Maria\nPonti, Siva Reddy, Nigel Collier, and Desmond\nElliott. 2021. Visually grounded reasoning\nacross languages and cultures. arXiv preprint\narXiv:2109.13238.\nVicente Ordonez, Girish Kulkarni, and Tamara Berg.\n2011. Im2text: Describing images using 1 million\ncaptioned photographs. volume 24.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: a method for automatic eval-\nuation of machine translation. In Proceedings of the\nannual meeting of the Association for Computational\nLinguistics, pages 311–318.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nChen Qiu, Dan Oneata, Emanuele Bugliarello, Stella\nFrank, and Desmond Elliott. 2022. Multilingual mul-\ntimodal learning with machine translated text. arXiv\npreprint arXiv:2210.13134.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models\nfrom natural language supervision. In International\nConference on Machine Learning, pages 8748–8763.\nPMLR.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. arXiv preprint arXiv:2302.00083.\nRita Ramos, Desmond Elliott, and Bruno Martins.\n2023a. Retrieval-augmented image captioning. In\nProceedings of the 17th Conference of the European\nChapter of the Association for Computational Lin-\nguistics, pages 3666–3681.\nRita Ramos, Bruno Martins, Desmond Elliott, and Yova\nKementchedjhieva. 2023b. Smallcap: Lightweight\nimage captioning prompted with retrieval augmenta-\ntion. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n2840–2849.\nSebastian Ruder. 2020. Why You Should Do\nNLP Beyond English. http://ruder.io/\nnlp-beyond-english.\nSara Sarto, Marcella Cornia, Lorenzo Baraldi, and\nRita Cucchiara. 2022. Retrieval-augmented trans-\nformer for image captioning. arXiv preprint\narXiv:2207.13162.\nTimo Schick and Hinrich Schütze. 2020. Exploit-\ning cloze questions for few shot text classification\nand natural language inference. arXiv preprint\narXiv:2001.07676.\nPatrick Schramowski, Cigdem Turan, Nico Andersen,\nConstantin A Rothkopf, and Kristian Kersting. 2022.\nLarge pre-trained language models contain human-\nlike biases of what is right and wrong to do. Nature\nMachine Intelligence, 4(3):258–268.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic im-\nage captioning. In Proceedings of the Annual Meet-\ning of the Association for Computational Linguistics,\npages 2556–2565.\nEmily Sheng, Kai-Wei Chang, Premkumar Natarajan,\nand Nanyun Peng. 2021. Societal biases in language\ngeneration: Progress and challenges. arXiv preprint\narXiv:2105.04054.\nYoad Tewel, Yoav Shalev, Idan Schwartz, and Lior\nWolf. 2021. Zero-shot image-to-text generation\nfor visual-semantic arithmetic. arXiv preprint\narXiv:2111.14447.\nAshish V Thapliyal, Jordi Pont-Tuset, Xi Chen, and\nRadu Soricut. 2022. Crossmodal-3600: A massively\nmultilingual multimodal evaluation dataset. arXiv\npreprint arXiv:2205.12522.\nMaria Tsimpoukelli, Jacob L Menick, Serkan Cabi,\nS. M. Ali Eslami, Oriol Vinyals, and Felix Hill. 2021.\nMultimodal few-shot learning with frozen language\nmodels. In Advances in Neural Information Process-\ning Systems, volume 34, pages 200–212.\n1645\nRamakrishna Vedantam, C Lawrence Zitnick, and Devi\nParikh. 2015. Cider: Consensus-based image descrip-\ntion evaluation. In Proceedings of the IEEE/CVF con-\nference on Computer Vision and Pattern Recognition,\npages 4566–4575.\nJianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie\nLi, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and\nLijuan Wang. 2022. Git: A generative image-to-text\ntransformer for vision and language. arXiv preprint\narXiv:2205.14100.\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yu-\nlia Tsvetkov, and Yuan Cao. 2021. SimVLM: Simple\nvisual language model pretraining with weak super-\nvision. arXiv preprint arXiv:2108.10904.\nThomas Wolf, Julien Chaumond, Lysandre Debut, Vic-\ntor Sanh, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Morgan Funtowicz, Joe Davison, Sam\nShleifer, et al. 2020. Transformers: State-of-the-art\nnatural language processing. In Proceedings of the\nConference on Empirical Methods in Natural Lan-\nguage Processing: System Demonstrations , pages\n38–45.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2020. mT5: A massively multilingual\npre-trained text-to-text transformer. arXiv preprint\narXiv:2010.11934.\nXi Ye, Srinivasan Iyer, Asli Celikyilmaz, Ves Stoy-\nanov, Greg Durrett, and Ramakanth Pasunuru. 2022.\nComplementary explanations for effective in-context\nlearning. arXiv preprint arXiv:2211.13892.\nAndy Zeng, Adrian Wong, Stefan Welker, Krzysztof\nChoromanski, Federico Tombari, Aveek Purohit,\nMichael Ryoo, Vikas Sindhwani, Johnny Lee, Vin-\ncent Vanhoucke, et al. 2022. Socratic models: Com-\nposing zero-shot multimodal reasoning with lan-\nguage. arXiv preprint arXiv:2204.00598.\nZequn Zeng, Hao Zhang, Ruiying Lu, Dongsheng Wang,\nBo Chen, and Zhengjue Wang. 2023. Conzic: Con-\ntrollable zero-shot image captioning by sampling-\nbased polishing. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 23465–23476.\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby,\nand Lucas Beyer. 2022. Scaling vision transform-\ners. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n12104–12113.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente\nOrdonez, and Kai-Wei Chang. 2017. Men also\nlike shopping: Reducing gender bias amplifica-\ntion using corpus-level constraints. arXiv preprint\narXiv:1707.09457.\nA Standard Evaluation Metrics\nIn the paper, comparison between models is per-\nformed using the CIDEr metric by following Thap-\nliyal et al. (2022). For holistic captioning evalua-\ntion, we provide here the performance of LMC AP\non XM3600 across additional standard automatic\nmetrics. Specifically, Table 9 reports performance\nwith BLEU-1 (B-1) and BLEU-4 (B-4) (Papineni\net al., 2002), ROGUE-L (Lin, 2004) and METEOR\n(Denkowski and Lavie, 2014).\nB-1 B-4 ROGUE-L METEOR\nen 0.387 0.067 0.299 0.129\nes 0.364 0.052 0.256 0.126\nhi 0.258 0.015 0.182 0.220\nzh 0.318 0.053 0.231 0.105\nTable 9: LMC AP performance on the XM3600 bench-\nmark across different evaluation metrics.\nB Additional Results on COCO\nTable 10 provides additional results on COCO,\ncomparing LMC AP against against other prompt-\nbased captioning models that do not involve train-\ning, including two previously proposed zero-shot\ncaptioning methods that are English-specific (i.e.,\nZeroCap (Tewel et al., 2021) and ConZic (Zeng\net al., 2023)). Results show that LMC AP out-\nperforms both. We also notice that unlike these\nmodels, LMC AP works for the multilingual set-\nting, advancing research beyond English-centric\ncaptioning.\nModel B-4 METEOR CIDEr\nZeroCap 0.026 0.115 0.146\nConZIC 0.013 0.115 0.128\nLMC AP 0.199 0.220 0.759\nTable 10: Results on the COCO test set. We compare\nour few-shot LMC AP model against English-specific\ncaptioning models that do not involve supervising train-\ning as well.\nC More Qualitative Examples\nWe provide several additional examples of captions\ngenerated from XM3600 images in Figure 3.\n1646\nen: “polar bear diving underwater” \nes: “un oso polar es visto bajo el agua” (a polar \nbear is seen under the water)\nhi: “एक बाघ पानी क े अंदर है (a tiger is inside the \nwater)\nzh: “一只北极熊在水下潜水” (a arctic bear dives \nunderwater)\nen: “the new york stock exchange” \nes: “una escena de la ciudad de nueva york” (a \nscene of new york city)\nhi: “एक नई स्टॉक एक्सचेंज की तस्वीर” (photo of a \nnew stock exchange)\nzh: “纽约证券交易所” (new york stock exchange)\nen: “a military style helicopter that is in a hangar” \nes: “un helicóptero militar que se encuentra en un \nhangar” (a military helicopter in a hangar)\nhi: “एक एयर फोसर्स हेलीकॉप्टर जमीन पर खड़ा है” (a air \nforce helicopter stands on the ground)\nzh: “军用直升机停在空地” (a military helicopter \nparked in open space)\n- a street scene with focus on the new york \nstock exchange\n- an intersection with a street sign and flag, at a \nstock exchange building\n- a window to a building that has an american \nflag in it\n- wall st sign up close with numbers 95 through \n104\n- a military style helicopter that is in a hangar\n- a large army looking helicopter landing at an \nairport\n- a helicopter that is sitting with its back wheels \non the ground\n- an air force helicopter sitting in a gravel area\n- a polar bear dives underwater at the zoo\n- a polar bear as seen from underwater camera\n- a polar bear diving to the bottom of his tank\n- a polar bear in the zoo dives underwater\nFigure 3: More examples of captions generated by LMC AP for XM3600 images, with retrieval from COCO.\nD Prompt-Template\nWe follow the Socratic template, where instead\nof including different categories (objects, places,\nnumber of people, etc), we replace them by the\nretrieved captions. By following the same template,\nin place of a completely different one, we can\nassess the impact of including retrieval compared\nto the original Socratic framework. Our template is:\nI am an intelligent image captioning\nbot. Similar images have the following\ncaptions: <caption 1> <caption 2>\n<caption 3> <caption 4>. A creative short\ncaption I can generate to describe this\nimage in <language> is:\nBetween the retrieved captions we use the\nspecial end of sentence token (i.e., </s>) of XGLM.\nNotice also that our prompt starts with 3 fixed\nshot examples from images in the training dataset\n(i.e., the same prompt is repeated multiple times to\nencode the n-shot examples). We share the N-shot\nexamples and the set of K retrieved captions\nused in our prompt, together with the code at\nhttps://github.com/RitaRamo/lmcap. The\nfollowing text is a concrete example of the prompt\nprovided for the first image of XM3600.\nI am an intelligent image captioning\nbot. Similar images have the following\ncaptions: a horse grazing in a grassy\nfield next to a barn</s> a brown horse\ngrazing in its pen and a red barn and\nwater</s> a pretty brown horse eating\nsome grass in a bare field</s> a horse\nis eating grass next to a barn in the\nmiddle of a pasture</s> A creative short\ncaption I can generate to describe this\nimage in spanish is: Un caballo marrón\nes grasa cerca de una casa roja</s>\nI am an intelligent image captioning\nbot. Similar images have the following\ncaptions: a teal toilet is the center of\nthis bathroom photo</s> a small bathroom\nwith brightly painted blue walls</s> the\nbathroom has a splash of color with\nthe blue tiles</s> the sink is above a\nturquoise tile sink</s> A creative short\ncaption I can generate to describe this\nimage in spanish is: Un baño muy limpio\ny bien decorado</s> I am an intelligent\nimage captioning bot. Similar images have\nthe following captions: a woman and child\nfocus on a pink device in public</s> a\nwoman holding a small child while standing\nnear a crowd</s> a very cute lady posing\nwith a small kid</s> a young child with\na cell phone and an adult</s> A creative\n1647\nshort caption I can generate to describe\nthis image in spanish is: Una mujer se\nacercó a mirar en su teléfono mientras\nestá listo para tomar una foto</s> I am an\nintelligent image captioning bot. Similar\nimages have the following captions: a\nbrown chicken is walking around outside\nwith another hen</s> a couple of roosters\nstanding in a field</s> a hen pecks the\nground while another looks off in the\ndistance</s> a couple of roosters are in\na field</s> A creative short caption I\ncan generate to describe this image in\nspanish is:.\nE Augmented Datastore Examples\nIn this appendix, we provide qualitative examples\non XM3600 when the datastore is augmented with\nCCS, i.e., with large and diverse data. In Figure\n4, we can see generation improving for English,\nwhere LMC AP correctly mentions the french con-\ncept of macarons, available in the retrieved cap-\ntions. In line with the quantitative results provided\nin Section 4.3, we can also see a possible explana-\ntion for why generation degraded for Hindi, that\nhas a lower pre-training language ratio than En-\nglish: LMC AP seems to have copied the last 3-shot\nexample provided in prompt, described above in\nSection D), maybe due to presence of more noise in\nthe CCS data. Another example can be seen in Fig-\nure 5, where LMC AP is more specific in generating\nthe flower type orchid.\n1648\nen: “a tray of colorful macarons” \nes: “un pastel de cumpleaños” (a birthday cake)\nhi: “कभी-कभी आप एक तस्वीर लेने क े लए तैयार हो जाते हैं जब आप एक तस्वीर लेते हैं” (Sometimes you get ready to take a \npicture when you take a picture)\nzh: “一个下午的下午,没有一个完整的午餐” (one afternoon without a full lunch)\n- an afternoon afternoon is incomplete for some macarons\n- some colorful macarons on a tray\n- several macarons on a tray\n- a tray with many different types of colorful macarons\nen: “a large assortment of sweet, rich, colorful deserts” \nes: “un montón de pasteles de colores” (a lot of colour cakes)\nhi: “几种不同颜色的甜点放在一个白色托盘” (few different colour desserts are placed on a white plate)\nzh: “一个年轻的男子站在他的电脑前,他准备开始演讲” (a young man standing in front of his computer and he is \nready to start a speech)\n- some different colored pastry treats on a white tray\n- a large assortment of sweet, rich, colorful deserts\n- a colorful pastry is displayed on a table\n- a close up view of some very tasty looking pastries\nCOCO\nCOCO\n    +\n CCS\nFigure 4: An example of captions generation by LMC AP conditioned on captions retrieved from COCO (top)\ncompared to augmenting the datastore with CCS (bottom).\nen: “a large white flowered orchid” \nes: “una flor de orchid” (a flower of orchid)\nhi: “फ ू लों की एक बड़ी लता” (a large flower)\nzh: “一个白色的花朵,有两个红色的斑点” (a white flower with two red spots)\n- a large white flowered orchid with two pink tipped spots\n- an orchid with red and white markings\n- the pink and white orchid is on display at show\n- an orchid that won the individual's trophy\n- a close-up of an exotic flower on a large stem\n- a view of a flower from very close, it appears to be fully bloomed\n- a pink, white and red orchid in a vase in front of a window\n- a pink and white orchid is in a small black vase'\nCOCO\nCOCO\n    +\n CCS\nen: “a close-up of an exotic flower on a large stem” \nes: “una flor exótica en un tallo grande” (an exotic flower in a big shell)\nhi: “एक फ ू ल का एक बड़ा हस्सा” (a large portion of a flower)\nzh: “一种美丽的花朵在巨大的花中” (a beautiful flower in a huge flower)\nFigure 5: An example of LMC AP generation based on retrieval from COCO (top) or COCO augmented with CCS\n(bottom).\n1649\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection Limitations\n□\u0013 A2. Did you discuss any potential risks of your work?\nSection Limitations\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nLeft blank.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nLeft blank.\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nLeft blank.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\n4\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nLeft blank.\nC □\u0013 Did you run computational experiments?\nLeft blank.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nLeft blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1650\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nLeft blank.\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nNot applicable. Left blank.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nLeft blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n1651"
}