{
    "title": "Transformer Uncertainty Estimation with Hierarchical Stochastic Attention",
    "url": "https://openalex.org/W4225680488",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5061075100",
            "name": "Jiahuan Pei",
            "affiliations": [
                "Amazon (Germany)",
                "University of Amsterdam"
            ]
        },
        {
            "id": "https://openalex.org/A5100756773",
            "name": "Cheng Wang",
            "affiliations": [
                "Amazon (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A5011272289",
            "name": "Gy√∂rgy Szarvas",
            "affiliations": [
                "Amazon (Germany)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2970246438",
        "https://openalex.org/W2995464762",
        "https://openalex.org/W6776126038",
        "https://openalex.org/W6684488266",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W6765655760",
        "https://openalex.org/W2964059111",
        "https://openalex.org/W6783724175",
        "https://openalex.org/W2797569913",
        "https://openalex.org/W3101988982",
        "https://openalex.org/W2531327146",
        "https://openalex.org/W3018207341",
        "https://openalex.org/W6729448088",
        "https://openalex.org/W2807414627",
        "https://openalex.org/W1826234144",
        "https://openalex.org/W6640963894",
        "https://openalex.org/W2560321925",
        "https://openalex.org/W2894384847",
        "https://openalex.org/W149107860",
        "https://openalex.org/W6676984168",
        "https://openalex.org/W2548228487",
        "https://openalex.org/W2109553965",
        "https://openalex.org/W3104667699",
        "https://openalex.org/W3160936529",
        "https://openalex.org/W3047823668",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W3094771832",
        "https://openalex.org/W6780333223",
        "https://openalex.org/W6788864986",
        "https://openalex.org/W3107734508",
        "https://openalex.org/W6759139056",
        "https://openalex.org/W3035436581",
        "https://openalex.org/W6752342493",
        "https://openalex.org/W582134693",
        "https://openalex.org/W2164411961",
        "https://openalex.org/W2953273646",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2951595529",
        "https://openalex.org/W3133696297",
        "https://openalex.org/W4287686486",
        "https://openalex.org/W3006511999",
        "https://openalex.org/W4287083725",
        "https://openalex.org/W3092557781",
        "https://openalex.org/W4303633609",
        "https://openalex.org/W3153723500",
        "https://openalex.org/W4287719664",
        "https://openalex.org/W2600383743",
        "https://openalex.org/W2547875792",
        "https://openalex.org/W3013042142",
        "https://openalex.org/W1959608418",
        "https://openalex.org/W2963238274",
        "https://openalex.org/W3121309507",
        "https://openalex.org/W2111959010",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4287725215",
        "https://openalex.org/W3131818527",
        "https://openalex.org/W3015948226",
        "https://openalex.org/W3119981760",
        "https://openalex.org/W2914144864",
        "https://openalex.org/W4288301546",
        "https://openalex.org/W4287810965",
        "https://openalex.org/W2606101940",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W2113459411"
    ],
    "abstract": "Transformers are state-of-the-art in a wide range of NLP tasks and have also been applied to many real-world products. Understanding the reliability and certainty of transformer models is crucial for building trustable machine learning applications, e.g., medical diagnosis. Although many recent transformer extensions have been proposed, the study of the uncertainty estimation of transformer models is under-explored. In this work, we propose a novel way to enable transformers to have the capability of uncertainty estimation and, meanwhile, retain the original predictive performance. This is achieved by learning hierarchical stochastic self-attention that attends to values and a set of learnable centroids, respectively. Then new attention heads are formed with a mixture of sampled centroids using the Gumbel-Softmax trick. We theoretically show that the self-attention approximation by sampling from a Gumbel distribution is upper bounded. We empirically evaluate our model on two text classification tasks with both in-domain (ID) and out-of-domain (OOD) datasets. The experimental results demonstrate that our approach: (1) achieves the best predictive-uncertainty trade-off among compared methods; (2) exhibits very competitive (in most cases, better) predictive performance on ID datasets; (3) is on par with Monte Carlo dropout and ensemble methods in uncertainty estimation on OOD datasets.",
    "full_text": "Transformer Uncertainty Estimation with Hierarchical Stochastic Attention\nJiahuan Pei1,2*, Cheng Wang2‚Ä† , Gy¬®orgy Szarvas2\n1University of Amsterdam\n2Amazon Development Center Germany GmbH, Berlin, Germany\n{jpei, cwngam, szarvasg}@amazon.de\nAbstract\nTransformers are state-of-the-art in a wide range of NLP\ntasks and have also been applied to many real-world prod-\nucts. Understanding the reliability and certainty of trans-\nformer model predictions is crucial for building trustable\nmachine learning applications, e.g., medical diagnosis. Al-\nthough many recent transformer extensions have been pro-\nposed, the study of the uncertainty estimation of transformer\nmodels is under-explored. In this work, we propose a novel\nway to enable transformers to have the capability of uncer-\ntainty estimation and, meanwhile, retain the original predic-\ntive performance. This is achieved by learning a hierarchi-\ncal stochastic self-attention that attends to values and a set of\nlearnable centroids, respectively. Then new attention heads\nare formed with a mixture of sampled centroids using the\nGumbel-Softmax trick. We theoretically show that the self-\nattention approximation by sampling from a Gumbel distribu-\ntion is upper bounded. We empirically evaluate our model on\ntwo text classiÔ¨Åcation tasks with both in-domain (ID) and out-\nof-domain (OOD) datasets. The experimental results demon-\nstrate that our approach: (1) achieves the best predictive per-\nformance and uncertainty trade-off among compared meth-\nods; (2) exhibits very competitive (in most cases, improved)\npredictive performance on ID datasets; (3) is on par with\nMonte Carlo dropout and ensemble methods in uncertainty\nestimation on OOD datasets.\n1 Introduction\nUncertainty estimation and quantiÔ¨Åcation are important\ntools for building trustworthy and reliable machine learning\nsystems (Lin, Engel, and Eslinger 2012; Kabir et al. 2018;\nRiedmaier et al. 2021). Particularly, when such machine-\nlearned systems are applied to make predictions that in-\nvolve important decisions, e.g., medical diagnosis (Ghoshal\nand Tucker 2020), Ô¨Ånancial planning and decision-making\n(Baker et al. 2020; Oh and Hong 2021), and autonomous\ndriving (Hoel, Wolff, and Laine 2020). The recent devel-\nopment of neural networks has shown excellent predictive\nperformance in many domains. Among those, transformers,\nincluding the vanilla transformer (Vaswani et al. 2017) and\nits variants such as BERT (Devlin et al. 2019; Wang et al.\n*This work has been done while doing internship at Amazon.\n‚Ä†Corresponding author.\nCopyright ¬© 2022, Association for the Advancement of ArtiÔ¨Åcial\nIntelligence (www.aaai.org). All rights reserved.\nùë•ùëñ\nùëßùëó ùë¶\nùëäùëñùëó\nùë•ùëñ\nùëßùëó ùë¶\nùëäùëñùëó = ùúáùëñùëó + ùúñ ‚àó ùúéùëñùëó\nùúñ~ùê∫ùëéùë¢ùë†ùë†ùëñùëúùëõ(0,1)\nùë•ùëñ\nùëßùëó ùë¶\nùëäùëñùëó\nùëì ùëì ùëì\nùë•ùëñ\nùëüùëñ\nùëüùëñ~ùêµùëíùëüùëõùëúùë¢ùëôùëôùëñ(1 ‚àí ùëù)\nùëù ‚àà [0; 1]\nùë•ùëñ\nùëßùëó ùë¶ùëó\nùëäùëñùëó\nùëìùëó\nùë•ùëñ\nùëßùëò ùë¶k\nùëäùëñk\nùëìk\nùë•ùëñ\nùëßùëó ùë¶ùëì\nùëîùëñ\nùë¶\nùëîùëñ~ùê∫ùë¢ùëöùëèùëíùëô[‚àílog(‚àí log ùë¢ ]\nùë¢~ùëàùëõùëñùëìùëúùëüùëö(0,1)\n(a) (b) (c)\n(d)\n(e)\nFigure 1: The methods of uncertainty estimation. (a) Deter-\nministic neural network outputs a single-point prediction;\n(b) Bayesian neural network captures uncertainty via sam-\npling from a Gaussian distribution; (c) Variational dropout\ncaptures uncertainty via sampling dropout masks from a\nBernoulli distribution; (d) Ensemble captures uncertainty\nby combining multiple independently trained deterministic\nmodels with different random seeds; (e) Gumbel-Softmax\ntrick for uncertainty estimation, the randomness comes from\nthe sampling categorical distribution from a Gumbel.\n2020) are the representative state-of-the-art type of neural\narchitectures that have shown remarkable performance on\nvarious recent Natural Language Processing (NLP) (Gillioz\net al. 2020) and Information Retrieval (IR) (Ren et al. 2021)\ntasks.\nAlthough transformers excel in terms of predictive per-\nformance (Tetko et al. 2020; Han et al. 2021), they do not\noffer the opportunity for practitioners to inspect the model\nconÔ¨Ådence due to their deterministic nature, i.e., incapabil-\nity to assess if transformers are conÔ¨Ådent about their pre-\ndictions. This inÔ¨Çuence is non-trivial because transformers\nare cutting-edge basic models for NLP. Thus, estimating the\npredictive uncertainty of transformers beneÔ¨Åts a lot in terms\nof building and examining model reliability for the down-\nstream tasks.\nTo estimate the uncertainty of neural models‚Äô prediction,\none common way is to inject stochasticity (e.g., noise or ran-\ndomness) (Kabir et al. 2018; Gawlikowski et al. 2021). It\nenables models to output a predictive distribution, instead\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n11147\nof a single-point prediction. Casting a deterministic trans-\nformer to be stochastic requires us to take the training and\ninference computational complexity into consideration, be-\ncause uncertainty estimation usually relies on multiple for-\nward runs. Therefore, directly adapting the aforementioned\nmethods is not desired, given the huge amount of parameters\nand architectural complexity of transformers.\nFigure 1 outlines deterministic transformer (Figure 1(a))\nand the possible approaches (Figure 1(b-e) for making a\nstochastic transformer. BNN (Figure 1(b)) assumes the net-\nwork weights follow a Gaussian or a mixture of Gaus-\nsian (Blundell et al. 2015), and tries to learn the weight dis-\ntribution (\u0016;\u001b), instead of weight W itself, with the help of\nre-parameterization trick (Kingma and Welling 2014). That\nmeans, BNN doubles the number of parameters. This is par-\nticularly challenging for a large network like a transformer,\nwhich has millions of parameters to be optimized. To alle-\nviate this issue, MC dropout (Gal and Ghahramani 2016)\n(Figure 1(c)) uses dropout (Srivastava et al. 2014), con-\ncretely Bernoulli distributed random variables, to approxi-\nmate the exact posterior distribution (Gal and Ghahramani\n2016). However, MC dropout tends to give overconÔ¨Ådent\nuncertainty estimation (Foong et al. 2019). Ensemble (Lak-\nshminarayanan, Pritzel, and Blundell 2017)(Figure 1(d)) is\nan alternative way to model uncertainty by averaging N in-\ndependently trained models, which yields the computational\noverhead by N times in model training.\nUnlike models above, we propose a simple yet effec-\ntive approach based on Gumbel-Softmax tricks or Con-\ncrete Dropout (Jang, Gu, and Poole 2017; Maddison, Mnih,\nand Teh 2017), which are independently found for con-\ntinuous relaxation, to estimate uncertainty of transformers.\nFirst, we cast the deterministic attention distribution for val-\nues in each self-attention head to be stochastic. The at-\ntention is then sampled from a Gumbel-Softmax distribu-\ntion, which controls the concentration over values. Second,\nwe regularize the key heads in self-attention to attend to\na set of learnable centroids. This is equivalent to perform-\ning clustering over keys (Vyas, Katharopoulos, and Fleuret\n2020) or clustering hidden states in RNN (Wang and Niepert\n2019; Wang, Lawrence, and Niepert 2021). Similar atten-\ntion mechanism has been also used to allow the layers in\nthe encoder and decoder attend to inputs in the Set Trans-\nformer (Lee et al. 2019) and to estimate attentive matri-\nces in the Capsule networks (Ahmed and Torresani 2019).\nThird, each new key head will be formed with a mixture\nof Gumbel-Softmax sampled centroids. The stochasticity\nis injected by sampling from a Gumbel-Softmax distribu-\ntion. This is different from BNN (sampling from Gaussian\ndistribution), MC-dropout (sampling from Bernoulli distri-\nbution), Ensemble (the stochasticity comes from random\nseeds in model training). With this proposed mechanism,\nwe approximate the vanilla transformer with a stochastic\ntransformer based on a hierarchical stochastic self-attention,\nnamely H- STO -TRANS , which enables the sampling of at-\ntention distributions over values as well as over a set of\nlearnable centroids.\nOur work makes the following contributions:\n‚Ä¢ We propose a novel way to cast the self-attention in trans-\n0.3\nEmbeddingQueryKey\nValueAttention Centroids(a) (b)\n(c)\n0.50.2 0.10.80.10.40.6\n0.10.80.1\nFigure 2: The illustration of multi-head self-attention in\ndeterministic and stochastic transformers. (a) The vanilla\ntransformer with deterministic self-attention. (b) Stochastic\ntransformer has stochastic self-attention used to weight val-\nues V, the standard Softmax is replaced with the Gumbel-\nSoftmax. (c) Hierarchical stochastic transformer learns to\npay attention to values V and a set of learnable centroids\nCstochastically.\nformers to be stochastic, which enables transformer mod-\nels to provide uncertainty information with predictions.\n‚Ä¢ We theoretically show that the proposed self-attention\napproximation is upper bounded, the key attention heads\nthat are close in Euclidean distance have similar attention\ndistribution over centroids.\n‚Ä¢ In two benchmark tasks for NLP, we empirically demon-\nstrate that H- STO -TRANS (1) achieves very competi-\ntive (in most cases, better) predictive performance on in-\ndomain datasets; (2) is on par with baselines in uncer-\ntainty estimation on out-of-domain datasets; (3) learns a\nbetter predictive performance-uncertainty trade-off than\ncompared baselines, i.e., high predictive performance\nand low uncertainty on in-domain datasets, high predic-\ntive performance and high uncertainty on out-of-domain\ndatasets.\n2 Background\nPredictive Uncertainty\nThe predictive uncertainty estimation is a challenging and\nunsolved problem. It has many faces, depending on different\nclassiÔ¨Åcation rules. Commonly it is classiÔ¨Åed as epistemic\n(model) and aleatoric (data) uncertainty (Der Kiureghian\nand Ditlevsen 2009; Kendall and Gal 2017). Alternatively,\non the basis of input data domain, it can also be classi-\nÔ¨Åed into in-domain (ID) (Ashukha et al. 2019) and out-of-\ndomain (OOD) uncertainty (Hendrycks and Gimpel 2017;\nWang and Van Hoof 2020). With in-domain data, i.e. the\ninput data distribution is similar to training data distribu-\ntion, a reliable model should exhibit high predictive per-\nformance (e.g., high accuracy or F1-score) and report high\nconÔ¨Ådence (low uncertainty) on correct predictions. On the\ncontrary, out-of-domain data has quite different distribution\nfrom training data, an ideal model should give high predic-\ntive performance to illustrate the generalization to unseen\ndata distribution, but desired to be unconÔ¨Ådent (high uncer-\ntainty). We discuss the epistemic (model) uncertainty in the\ncontext of ID and OOD scenarios in this work.\n11148\nVanilla Transformer\nThe vanilla transformer (Vaswani et al. 2017) is an alter-\nnative architecture to Recurrent Neural Networks recurrent\nneural networks (RNNs) for modelling sequential data that\nrelaxes the model‚Äôs reliance on input sequence order. It con-\nsists of multiple components such as positional embedding,\nresidual connection and multi-head scaled dot-product at-\ntention. The core component of the transformer is the multi-\nhead self-attention mechanism.\nLet x ‚ààRl√ód (l is sequence length, d is dimension) be\ninput data, and Wq;Wk;Wv ‚ààRd√ód be the matrices for\nquery Q ‚ààRl√óh√ódh , key K ‚ààRl√óh√ódh , and value V ‚àà\nRl√óh√ódh , dh = d\nh and his the number of attention heads.\nEach x is associated with a query Q and a key-value pair\n(K;V ). The computation of an attentive representationAof\nx in the multi-head self-attention is:\nQ= Wqx; K = Wkx; V = Wvx; (1)\nA= SOFTMAX (\u000b‚àí1QK‚ä§); H = AV (2)\nwhere H = [h1;:::;h h] is the multi-head output and A =\n[a1;:::;a h] is the attention distribution that needs to attend to\nV, \u000bis a scaling factor. Note that a large value of \u000bpushes\nthe Softmax function into regions where it has extremely\nsmall gradients. This attention mechanism is the key factor\nof a transformer for achieving high computational efÔ¨Åciency\nand excellent predictive performance. However, as we can\nsee, all computation paths in this self-attention mechanism\nare deterministic, leading to a single-point output. This lim-\nits us to access and evaluate the uncertainty information be-\nyond prediction given an input x.\nWe argue that the examination of the reliability and con-\nÔ¨Ådence of a transformer prediction is crucial for many NLP\napplications, particularly when the output of a model is di-\nrectly used to serve customer requests. In the following sec-\ntion, we introduce a simple yet efÔ¨Åcient way to cast the de-\nterministic attention to be stochastic for uncertainty estima-\ntion based on Gumbel-Softmax tricks (Jang, Gu, and Poole\n2017; Maddison, Mnih, and Teh 2017).\n3 Methodology\nBayesian Inference and Uncertainty Modeling\nIn this work, we focus on using transformers in classiÔ¨Åcation\ntasks. Let D = {X;Y}= {xi;yi}N\ni=1 be a training dataset,\nyi ‚àà{1;:::;M }is the categorical label for an inputxi ‚ààRd.\nThe goal is to learn a transformation function f, which is\nparameterized by weights ! and maps a given input xto a\ncategorical distribution y. The learning objective is to mini-\nmize negative log likelihood, L= ‚àí1\nN\n‚àëN\ni log p(yi|xi;!).\nThe probability distribution is obtained by Softmax function\nas:\np(yi = m|xi;!) = exp(fm(xi;!))‚àë\nk‚ààMexp(fk(xi;!): (3)\nIn the inference phase, given a test samplex‚àó, the predictive\nprobability y‚àóis computed by:\np(y‚àó|x‚àó;D) =\n‚à´\np(y‚àó|x‚àó;!)p(!|D)d! (4)\nwhere the posterior p(!|D) is intractable and cannot be\ncomputed analytically. A variational posterior distribution\nq\u0012(!), where \u0012are the variational parameters, is used to ap-\nproximate the true posterior distribution by minimizing the\nKullback-Leilber (KL) distance. This can also be treated as\nthe maximization of evidence lower bound (ELBO):\nL\u0012 =\n‚à´\nq\u0012(!)p(Y|X;!)d!‚àíKL[q\u0012(!) ‚à•p(!)] (5)\nWith the re-parametrization trick (Kingma, Salimans, and\nWelling 2015), a differentiable mini-batched Monte Carlo\nestimator can be obtained.\nThe predictive (epistemic) uncertainty can be measured\nby performing T inference runs and averaging predictions.\np(y‚àó|x‚àó) =1\nT\nT‚àë\nt=1\np!t (y‚àó|x‚àó;!t) (6)\nT corresponds to the number of sets of mask vectors from\nBernoulli distribution {rt}T\nt=1 in MC-dropout, or the num-\nber of randomly trained models in Ensemble, which po-\ntentially leads to different set of learned parameters ! =\n{!1;:::;! t}, or the number of sets of sampled attention dis-\ntribution from Gumbel distribution {gt}T\nt=1 in our proposed\nmethod.\nStochastic Self-Attention with Gumbel-Softmax\nAs described in section 2, the core component that makes a\ntransformer successful is the multi-head self-attention. For\neach i-th head, let qi ‚ààQ;ki ‚ààK;vi ‚ààV, it is written as:\nai = SOFTMAX (qik‚ä§\ni\n\u001c ); ai ‚ààRl√ól (7)\nhi = aivi; hi ‚ààRl√ódh (8)\nWe here use a temperature parameter\u001cto replace the scaling\nfactor \u000b. The ai is attention distribution, which learns the\ncompatibility scores between tokens in the sequence with\nthe i-th attention head. The scores are used to retrieve and\nform the mixture of the content of values, which is a kind of\ncontent-based addressing mechanism in neural Turing ma-\nchine (Graves, Wayne, and Danihelka 2014). Note the atten-\ntion is deterministic.\nA straightforward way to inject stochasticity is to replace\nstandard Softmax with Gumbel-Softmax, which helps to\nsample attention weights to form ^ai.\n^ai ‚àºG(qik‚ä§\ni\n\u001c ) (9)\nhi = ^aivi (10)\nwhere G is G UMBEL -SOFTMAX function. The Gumbel-\nSoftmax trick is an instance of a path-wise Monte-Carlo gra-\ndient estimator (Gumbel 1954; Maddison, Mnih, and Teh\n2017; Jang, Gu, and Poole 2017). With the Gumbel trick,\nwe can draw samples zfrom a categorical distribution given\nby parameters \u0012, that is, z = ONE HOT\n(\narg maxi[gi +\nlog \u0012i]\n)\n;i ‚àà[1 :::k ], where k is the number of categories\nand gi are i.i.d. samples from the G UMBEL (0;1), that is,\n11149\ng = ‚àílog(‚àílog(u));u ‚àº UNIFORM (0;1) is indepen-\ndent to network parameters. Because the arg maxoperator\nbreaks end-to-end differentiability, the categorical distribu-\ntion z can be approximated using the differentiable Soft-\nmax function (Jang, Gu, and Poole 2017; Maddison, Mnih,\nand Teh 2017). Here the \u001c is a tunable temperature param-\neter equivalent to \u000b in Eq. (2), Then the attention weights\n(scores) for values in Eq.2 can be computed as:\n^ai = exp((log(\u0012i) +gi)=\u001c)\n‚àëk\nj=1 exp((log(\u0012tj ) +gj)=\u001c)\n; i‚àà[1 :::k ]: (11)\nwhere the \u0012i = qik‚ä§\ni . And we use the following approxima-\ntion:\nKL[a‚à•^a] where aj = aj\n‚àëi=1\nk ai\n(12)\nThis indicates an approximation of a deterministic atten-\ntion distribution awith a stochastic attention distribution ^a.\nWith a larger\u001c, the distribution of attention is more uniform,\nand with a smaller \u001c, the attention becomes more sparse.\nThe trade-off between predictive performance and un-\ncertainty estimation. This trade-off is rooted in bias-\nvariance trade-off. Let \u001e(x) be a prediction function, and\nf(x) is the true function and \u001a be a constant number. The\nerror can be computed as:\n\u0018(x) = (E[\u001e(x) ‚àíf(x)])2\nÓ¥ô\nÓ¥òÓ¥ó Ó¥ö\nBias2\n+ (E[\u001e(x) ‚àíE[\u001e(x)]]2)Ó¥ô Ó¥òÓ¥ó Ó¥ö\nVariance\n+ \u001aÓ¥ôÓ¥òÓ¥óÓ¥ö\nConst\n(13)\nMC-dropout (Gal and Ghahramani 2016) with T times\nMonte Carlo estimation gives a prediction E[\u001et(x)];t ‚ààT\nand predictive uncertainty, e.g., variances Variance[\u001et(x)]\n(\u001ais a constant number denotes irreducible error). On both\nin-domain and out-of-domain datasets, a good model should\nexhibit low bias, which ensures model generalization ca-\npability and high predictive performance. For epistemic\n(model) uncertainty, we expect model outputs low variance\non in-domain data and high variance on out-of-domain data.\nWe empirically observe (from Table 1 and Table 2) that\nthis simple modiÔ¨Åcation in Eq. (9) can effectively capture\nthe model uncertainty, but it struggles to learn a good trade-\noff between predictive performance and uncertainty estima-\ntion. That is, when good uncertainty estimation performance\nis achieved on out-of-domain data, the predictive perfor-\nmance on in-domain data degrades. To address this issue, we\npropose a hierarchical stochastic self-attention mechanism.\nHierarchical Stochastic Self-Attention\nTo further encourage transformer model to have stochastic-\nity and retain predictive performance, we propose to add\nan additional stochastic attention before the attention that\npays values. This attention forces each key head stochasti-\ncally attend to a set of learnable centroids, which will be\nlearned during back-propagation. This is equivalent to regu-\nlarizing key attention heads. Similar ideas have been used to\nimprove transformer efÔ¨Åciency (Vyas, Katharopoulos, and\nFleuret 2020) and to improve RNN memorization (Wang\nand Niepert 2019).\nWe Ô¨Årst deÔ¨Åne the set of c centroids, C ‚ààRdh√óc . Let\neach centroid ci ‚ààRdh have the same dimension with each\nkey head kj ‚ààRdh . The model will Ô¨Årst learn to pay at-\ntention to centroids, and a new key head ^kj is formed by\nweighting each centroid. Then ^k and a query q decides the\nattention weights to combine values v. For the i-th head, a\ngiven query qi, key ki, value vi, the stochastic self-attention\ncan be hierarchically formulated as:\n^ac ‚àºG(\u001c‚àí1\n1 kiC); ^ac ‚ààRl√óc (14)\n^ki = ^acC‚ä§; ^ki ‚ààRl√ódh (15)\n^av ‚àºG(\u001c‚àí1\n2 qi^k‚ä§\ni ); ^av ‚ààRl√ól (16)\nhi = ^avvi (17)\n^ac;^av are the sampled categorical distributions that are used\nto weight centroids in Cand tokens in vi. The \u001c1;\u001c2 control\nthe softness for each stochastic self-attention, respectively.\nWe summarize the main procedures of performing hier-\narchical stochastic attention in transformer in Algorithm 1.\nWhy perform clustering on key heads? The equation\n(14) performs clustering on the key attention heads and out-\nputs an attention distribution, and equation (15) tries to form\na new head based on attention distribution and learned cen-\ntroids. The goal is to make the original key heads to be\nstochastic, allowing attention distribution to have random-\nness for uncertainty estimation. This goal can be also accom-\npanied by applying equations (14) and (15) to query while\nkeeping key unchanged. In that case, ^ac can be still sampled\nstochastically based on query and centroids.\nStochastic attention approximation. The equations (14)\nand (15) group the key heads into a Ô¨Åxed number of cen-\ntroids and are reweighed by the mixture of centroids. As\nin (Vyas, Katharopoulos, and Fleuret 2020), we can ana-\nlyze the attention approximation error, and derive that the\nkey head attention difference is bounded.\nProposition 3.1 Given two keys ki and kj such that\n‚à•ki ‚àíkj‚à•2 ‚â§ \", stochastic key attention difference is\nbounded:\nÓµπÓµπG(\u001c‚àí1kiC)) ‚àíG(\u001c‚àí1kjC))\nÓµπ\nÓµπ\n2 ‚â§ \u001c‚àí1\"‚à•C‚à•2,\nwhere Gis the Gumbel-Softmax function, and ‚à•C‚à•2 is the\nspectral norm of centroids. \"and \u001c are constant numbers.\nAlgorithm 1:Hierarchical stochastic transformer.\nInput : query Q, key K, value V, centroids C\nOutput: Hierarchical stochastic attentive output H\n1 Model stochastic attention ^Ac over centroids Cas Eq.14;\n2 Sample ^Ac from a categorical distribution\nz = ONE HOT\n(\narg maxi[gi + log\u0012i]\n)\n;i ‚àà[1 :::k ],\ng= ‚àílog(‚àílog(u));u ‚àºUNIFORM (0;1) ;\n3 Differentially approximate ^Ac as Eq. 11;\n4 Compute ^K = ^AcC> as Eq. 15;\n5 Model stochastic attention ^Av over value V as Eq.16;\n6 Sample and approximate ^Av, similar to line 2 to 3;\n7 Compute H = ^AvV as Eq. 17;\n11150\nProof 3.1 Same to the Softmax function, which has Lips-\nchitz constant less than 1 (Gao and Pavel 2017), we have\nthe following derivation:\nÓµπÓµπG(\u001c‚àí1kiC)) ‚àíG(\u001c‚àí1kjC))\nÓµπ\nÓµπ\n2\n‚â§\nÓµπ\nÓµπ\u001c‚àí1kiC‚àí\u001c‚àí1kjC\nÓµπ\nÓµπ\n2\n‚â§\u001c‚àí1\"‚à•C‚à•2\n(18)\nProposition 3.1 shows that the i-th key assigned to j-th\ncentroid can be bounded by its distance from j-th centroid.\nThe keys that are close in Euclidean space have similar at-\ntention distribution over centroids.\n4 Experimental Setups\nWe design experiments to achieve the following objectives:\n‚Ä¢ To evaluate the predictive performance of models on in-\ndomain datasets. High predictive scores and low uncer-\ntainty scores are desired.\n‚Ä¢ To compare the model generalization from in-domain to\nout-of-domain datasets. High scores are desired.\n‚Ä¢ To estimate the uncertainty of the models on out-of-\ndomain datasets. High uncertainty scores are desired.\n‚Ä¢ To measure the model capability in learning the predic-\ntive performance and uncertainty estimation trade-off.\nDatasets\nWe use IMDB dataset1 (Maas et al. 2011) for the sentiment\nanalysis task. The standard IMDB has 25,000/25,000 re-\nviews for training and test, covering 72,062 unique words.\nFor hyperparameter selection, we take 10% of training data\nas validation set, leading to 22,500/2,500/25,000 data sam-\nples for training, validation, and testing. Besides, we use\ncustomer review (CR) dataset (Hendrycks and Gimpel 2017)\nwhich has 500 samples to evaluate the proposed model in\nOOD settings. We conduct the second experiment on lin-\nguistic acceptability task with CoLA dataset 2 (Warstadt,\nSingh, and Bowman 2019). It consists of 8,551 training and\n527 validation in-domain samples. As the labels of test set\nis not publicly available, we split randomly the 9078 in-\ndomain samples into train/valid/test with 7:1:2. Addition-\nally, we use the provided 516 out-of-domain samples for un-\ncertainty estimation.\nCompared Methods\nWe compare the following methods in our experimental\nsetup:\n‚Ä¢ T RANS (Vaswani et al. 2017): The vanilla transformer\nwith deterministic self-attention.\n‚Ä¢ MC- DROPOUT (Gal and Ghahramani 2016): Using\ndropout (Srivastava et al. 2014) as a regularizer to mea-\nsure the prediction uncertainty.\n‚Ä¢ E NSEMBLE (Lakshminarayanan, Pritzel, and Blundell\n2017): Average over multiple independently trained\ntransformers.\n1https://ai.stanford.edu/ amaas/data/sentiment/\n2https://nyu-mll.github.io/CoLA/\n‚Ä¢ STO -TRANS : The proposed method that the attention dis-\ntribution over values is stochastic;\n‚Ä¢ H- STO -TRAN : The proposed method that uses hierarchi-\ncal stochastic self-attention, i.e., the stochastic attention\nfrom key heads to a learnable set of centroids and the\nstochastic attention to value, respectively.\nImplementation Details\nWe implement models in PyTorch (Paszke et al. 2019). The\nmodels are trained with Adam (Kingma and Ba 2014) as the\noptimization algorithm. For each trained model, we sam-\nple 10 predictions (run inference 10 times), the mean and\nvariance (or standard deviation) of results are reported. The\nuncertainty information is quantiÔ¨Åed with variance (or stan-\ndard deviation). For sentiment analysis, we use 1 layer with\n8 heads, both the embedding size and the hidden dimen-\nsion size are 128. We train the model with learning rate\nof 1e-3, batch size of 128, and dropout rate of 0.5/0.1. We\nevaluate models at each epoch, and the models are trained\nwith maximum 50 epochs. We report accuracy as the eval-\nuation metric. For linguistic acceptability, we use 8 layers\nand 8 heads, the embedding size is 128 and the hidden di-\nmension is 512. We train the model with learning rate of\n5e-5, batch size of 32 and dropout rate of 0.1. We train the\nmodels with maximum 2000 epochs and evaluate the mod-\nels at every 50 epochs. We use Matthews correlation coefÔ¨Å-\ncient (MCC) (Matthews 1975) as the evaluation metric. The\nmodel selection is performed based on validation dataset ac-\ncording to predictive performance.\n5 Experimental Results\nResults on Sentiment Analysis\nTable 1 presents the predictive performance and uncer-\ntainty estimation on IMDB (in-domain, ID) and CR (out-\nof-domain, OOD) dataset, evaluated by accuracy.\nFirst, STO -TRANS and H-STO -TRANS are able to pro-\nvide uncertainty information, as well as maintain and even\nslightly outperform the predictive performance of TRANS .\nSpecially, STO -TRANS (\u001c = 40) and H-STO -TRANS (\u001c1 =\n1, \u001c2 = 30) outperforms TRANS (\u0011 = 0:1) by 0.42% and\n0.66% on ID dataset. In addition, they allow us to measure\nthe uncertainty via predictive variances. It is because they in-\nject randomness directly to self-attentions. However,TRANS\nhas no access to uncertainty information due to its determin-\nistic nature.\nSecond, STO -TRANS is struggling to learn a good trade-\noff between ID predictive performance and OOD uncer-\ntainty estimation performance. With small temperature \u001c =\n1, STO -TRANS gives good uncertainty information, but we\nobserve that the ID predictive performance drops. When\n\u001c approaches to\n‚àö\nd=h (the original scaling factor in the\nvanilla transformer), STO -TRANS achieves better perfor-\nmance on ID dataset, but lower performance on OOD\ndataset. We conjecture that the randomness in STO -TRANS\nis solely based on the attention distribution over values and\nis not enough for learning the trade-off.\nThird, H-STO -TRANS achieves better accuracy-\nuncertainty trade-off compared with STO -TRANS . For\n11151\n1357911 20 30 40 50\n60\n65\n70\n75\n80\n85\n90Accuracy\n1357911 20 30 40 50\n2\n60\n65\n70\n75\n80\n85\n90\n1357911 20 30 40 50\n1\n60\n65\n70\n75\n80\n85\n90\nin-domain\nout-of-domain\nFigure 3: The experiments with hyperparameter \u001c. Left: STO -TRANS with different \u001c. The randomness is solely based on\nthe sampling on attention distribution over values. While uncertainty information is captured, STO -TRANS has difÔ¨Åculties in\nlearning the trade-off between in-domain and out-of-domain performance. Middle: The hyperparameter tuning of \u001c1 and \u001c2 in\nH-STO -TRANS . \u001c1 controls the concentration on centroids and \u001c2 controls the concentration on values.\nID (%) OOD (%) ‚ñΩID (%) ‚ñΩOOD (%)\nTRANS (\u0011= 0:1) 87.00 65.00 / /\nTRANS (\u0011= 0:5) 87.51 63.40 0.51 ‚Üë 1.60 ‚Üì\nMC- DROPOUT (\u0011= 0:5) 86.06 ¬±0.087 63.38 ¬±1.738 0.94 ‚Üë 1.62 ‚Üì\nMC- DROPOUT (\u0011= 0:1) 87.01 ¬±0.075 63.38 ¬±0.761 0.10 ‚Üë 1.62 ‚Üì\nENSEMBLE 86.89 ¬±0.230 64.20 ¬±1.585 0.11 ‚Üì 0.80 ‚Üì\nSTO -TRANS (\u001c = 1) 82.62 ¬±0.092 67.92 ¬±0.634 4.38 ‚Üì 2.92 ‚Üë\nSTO -TRANS (\u001c = 40) 87.42 ¬±0.022 63.78 ¬±0.289 0.42 ‚Üë 1.22 ‚Üì\nH-STO -TRANS (\u001c1 = 1, \u001c2 = 20) 87.63 ¬±0.017 67.14 ¬±0.400 0.63 ‚Üë 2.14 ‚Üë\nH-STO -TRANS (\u001c1 = 1, \u001c2 = 30) 87.66 ¬±0.022 66.72 ¬±0.271 0.66 ‚Üë 1.72 ‚Üë\nTable 1: The predictive performance and uncertainty estimation of models on IMDB (ID) and CR (OOD) dataset. The uncer-\ntainty estimation is performed by running forward pass inference by 10 runs, then the uncertainty is quantiÔ¨Åed by standard\ndeviation across runs. For ensemble, the results are averaged on 10 models that are independently trained with random seeds.\nDropout is used in the inference of MC-DROPOUT and \u0011is dropout rate. In the rest of methods, dropout is not used in inference.\nThe ‚ñΩID (%) and ‚ñΩOOD (%) present the predictive performance difference to TRANS (\u0011= 0:1).\ninstance, with \u001c1 = 1;\u001c2 = 20, H-STO -TRANS achieves\n87.63% and 67.14%, which outperform the corresponding\nnumbers of STO -TRANS for both ID and OOD datasets. It\nalso outperforms MC- DROPOUT and ENSEMBLE , specially,\nH-STO -TRANS outperforms 0.62%-1.6% and 2.52%-3.76%\non ID and OOD datasets, respectively. On OOD dataset,\nwhile MC- DROPOUT and ENSEMBLE exhibit higher un-\ncertainty (measured by standard deviation) across runs,\nthe accuracy is lower than that of TRANS (\u0011 = 0:1),\nSTO -TRANS (\u001c = 1) and H-STO -TRANS . It is due to a\nbetter way of learning two types of randomness: one from\nsampling over a set of learnable centroids and the other one\nfrom sampling attention over values.\nFigure 3 reports the hyperparameter tuning of \u001c1 and \u001c2.\nThe goal is to Ô¨Ånd a reasonable combination to achieve high\npredictive performance on both ID and OOD datasets. To\nsimplify the tuning work, we Ô¨Åx the \u001c1 = 1and then change\n\u001c2 with different values, and vice versa. As we can see, the\ncombination of a small\u001c1 and a large\u001c2 performs better than\nthe other way around. We think this is because \u001c2 is in the\nlatter stage and has bigger effects on the predictive perfor-\nmance. However, removing \u001c1 goes back to Figure 3 (Left),\nwhere accuracy-uncertainty trade-off is not well learned by\nSTO -TRANS .\nResults on Linguistic Acceptability\nTable 2 shows the performance of compared models on both\nin-domain (ID) and out-of-domain (OOD) sets of CoLA\ndataset, evaluated by MCC.\nFirst, STO -TRANS and H-STO -TRANS obtain compara-\nble performance as well as provide uncertainty information,\ncompared with TRANS . To be speciÔ¨Åc, STO -TRANS and H-\nSTO -TRANS improves 3.18% and 0.43% of MCC on ID\ndataset compared with deterministic TRANS respectively.\nSecond, STO -TRANS achieves the best performance on\nID dataset but the worst performance on OOD dataset.\nAlthough STO -TRANS outperforms TRANS , the best MC-\nDROPOUT , ENSEMBLE by 3.18%, 3.24%, 2.07% of MCC on\nID dataset, its performance drops by 1.21%, 1.86%, 1.48%,\ncorrespondingly on OOD dataset. This further veriÔ¨Åes our\nconjecture that the randomness is only introduced to atten-\ntion distribution over values and is insufÔ¨Åcient for learning\nthe trade-off of ID and OOD data.\nThird, H-STO -TRANS enabled to learn better trade-off\nbetween prediction and uncertainty. Precisely, the perfor-\nmance improves 0.43% and 0.03% of MCC on ID and OOD\ndatasets respectively. H-STO -TRANS is 0.49% superior to\nMC- DROPOUT (\u0011 = 0:05), meanwhile, 0.68% inferior to\nENSEMBLE on ID dataset. GivenENSEMBLE shows high un-\ncertainty on ID dataset and MC- DROPOUT (\u0011 = 0:05) has\nlow uncertainty on OOD dataset, this is not desired. There-\nfore, H-STO -TRANS is the one that strikes the better balance\nacross the objectives. In the context of this task, it means\nhigh MCC, low variance on ID dataset and high MCC, high\nvariance on OOD dataset.\nTable 3 gives some predictions of test samples with H-\n11152\nModels ID(%) OOD(%) ‚ñΩID (%) ‚ñΩOOD (%)\nTRANS (\u0011= 0:1) 20.09 16.46 / /\nMC- DROPOUT (\u0011= 0:1) 19.91 ¬±0.40 16.70 ¬±2.21 0.18 ‚Üì 0.24 ‚Üë\nMC- DROPOUT (\u0011= 0:05) 20.03 ¬±0.30 17.11 ¬±1.21 0.06 ‚Üì 0.65 ‚Üë\nENSEMBLE 21.20 ¬±2.59 16.73 ¬±4.92 1.11 ‚Üë 0.27 ‚Üë\nSTO -TRANS 23.27 ¬±0.75 15.25 ¬±4.65 3.18 ‚Üë 1.21 ‚Üì\nH-STO -TRANS 20.52 ¬±0.76 16.49 ¬±4.08 0.43 ‚Üë 0.03 ‚Üë\nTable 2: The performance of compared models on CoLA dataset. We set all temperature values \u001c1 = 1and \u001c2 = 1. The‚ñΩID\n(%) and ‚ñΩOOD (%) present the predictive performance and difference to TRANS (\u0011= 0:1), respectively.\nExamples (Labels) Prob. Corr. Corr./All\nno man has ever beaten the centaur.(1) 0.75 ¬±0.001 10/10\nnora sent the book to london(1) 0.65 ¬±0.007 10/10\nkim is eager to recommend.(0) 0.41 ¬±0.011 3/10\nhe analysis her was Ô¨Çawed(0) 0.24 ¬±0.003 0/10\nsandy had read how many papers? !(1) 0.67 ¬±0.010 10/10\nwhich book did each author recommend?(1)0.58 ¬±0.010 7/10\njohn is tall on several occasions.(0) 0.42 ¬±0.005 1/10\nTable 3: Illustration of predictions with H-STO -TRANS . The\npredictions for each ID (top) and OOD (bottom) samples are\nmeasured by the probability of being correct of each predic-\ntion and the number of correct predictions.\nSTO -TRANS . What we observed are two folds: (1) In gen-\neral, ID predictions have lower variances in terms of the\nprobability of being correct. For ‚Äú10/10‚Äù (10 correct predic-\ntions out of 10 total predictions) prediction cases, the ID ex-\namples have higher probability score than the ones in OOD\ndata. Also, we Ô¨Ånd there are much less number of ‚Äú10/10‚Äù\nprediction cases in OOD dataset than that in ID dataset. (2)\nFor ID dataset, either with high or low probability scores, we\ncan see low variances, we see more ‚Äú10/10‚Äù (tend to be con-\nÔ¨Ådently correct) or ‚Äú0/10‚Äù (tend to be conÔ¨Ådently incorrect)\ncases. As expected, for both cases, the variance is relatively\nlow as compared to probability around 0.5. In deterministic\nmodels, we are not able to access this kind of information\nwhich would imply how conÔ¨Ådent are the transformer mod-\nels towards predictions.\n6 Related Work\nBayesian neural networks (Blundell et al. 2015) inject\nstochasticity by sampling the network parameters from a\nGaussian prior. Then the posterior distribution of target\ncan be estimated in multiple sampling runs. However, the\nBayesian approach doubles the number of network param-\neters, i.e., instead of learning a single-point network pa-\nrameter, it learns a weight distribution which is assumed\nto follow a Gaussian distribution. Additionally, it often re-\nquires intensive tuning work on Gaussian mean and vari-\nance to achieve stable learning curves as well as predictive\nperformance. MC dropout (Gal and Ghahramani 2016) ap-\nproximates Bayesian approach by sampling dropout masks\nfrom a Bernoulli distribution. However, MC dropout has\nbeen demonstrated to give overconÔ¨Ådent uncertainty estima-\ntion (Foong et al. 2019). Alternatively, the recently proposed\ndeep ensembles (Lakshminarayanan, Pritzel, and Blundell\n2017) offers possibility to estimate predictive uncertainty\nby combining predictions from different models which are\ntrained with different random seeds. This, however, signiÔ¨Å-\ncantly increases the computational overhead for training and\ninference. There are some MC dropout based methods re-\ncently proposed. Sequential MC transformer (Martin et al.\n2020), which models uncertainty by casting self-attention\nparameters as unobserved latent states by evolving randomly\nthrough time. (He et al. 2020) combined mix-up, self-\nensembling and dropout to achieve more accurate uncer-\ntainty score for text classiÔ¨Åcation. (Shelmanov et al. 2021)\nproposed to incorporate determinantal point process (DPP)\nto MC dropout to quantify the uncertainty of transform-\ners. Different to the above-mentioned approaches, we in-\nject stochasticity into the vanilla transformer with Gumbel-\nSoftmax tricks. As it is shown in the experiment section,\nhierarchical stochastic self-attention component can effec-\ntively capture model uncertainty, and learn a good trade-\noff between in-domain predictive performance and out-of-\ndomain uncertainty estimation.\n7 Discussion\nWhile many extensions of transformers have been recently\nproposed, the most transformer variants are still determin-\nistic. Our goal in this work is to equip transformers in a\nstochastic way to estimate uncertainty while retaining the\noriginal predictive performance. This requires special de-\nsign in order to achieve the two goals without adding a ma-\njor computational overhead to model training and inference\nlike Ensembles and Bayesian Neural Network (BNN). The\ncomplexity gain of our method to its deterministic version is\nmodest and requires an additional matrix C ‚ààRdh√óc. This\nis more efÔ¨Åcient than Ensemble and BNN, which gives N\n(N ‚â• 2 for Ensemble and N = 2 for BNN) times more\nweights.\n8 Conclusion\nThis work proposes a novel, simple yet effective way to en-\nable transformers with uncertainty estimation, as an alterna-\ntive to MC dropout and ensembles. We propose variants of\ntransformers based on two stochastic self-attention mecha-\nnisms: (1) injecting stochasticity into the stochastic attention\nover values; (2) forcing key heads to pay stochastic atten-\ntion to a set of learnable centroids. Our experimental results\nshow that the proposed approach learns good trade-offs be-\ntween in-domain predictive performance and out-of-domain\nuncertainty estimation performance on two NLP benchmark\ntasks, and outperforms baselines.\n11153\nReferences\nAhmed, K.; and Torresani, L. 2019. Star-caps: Capsule net-\nworks with straight-through attentive routing. NeurIPS‚Äô19,\n32: 9101‚Äì9110.\nAshukha, A.; Lyzhov, A.; Molchanov, D.; and Vetrov, D.\n2019. Pitfalls of in-domain uncertainty estimation and en-\nsembling in deep learning. In ICLR‚Äô19.\nBaker, S. R.; Bloom, N.; Davis, S. J.; and Terry, S. J. 2020.\nCovid-induced economic uncertainty. Technical report, Na-\ntional Bureau of Economic Research.\nBlundell, C.; Cornebise, J.; Kavukcuoglu, K.; and Wierstra,\nD. 2015. Weight uncertainty in neural network. InICML‚Äô15,\n1613‚Äì1622. PMLR.\nDer Kiureghian, A.; and Ditlevsen, O. 2009. Aleatory or\nepistemic? Does it matter? Structural safety, 31(2): 105‚Äì\n112.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of deep bidirectional transformers for\nlanguage understanding. In NAACL-HLT‚Äô19, 4171‚Äì4186.\nFoong, A. Y .; Li, Y .; Hern¬¥andez-Lobato, J. M.; and Turner,\nR. E. 2019. ‚Äòin-between‚Äô uncertainty in Bayesian neural net-\nworks. In ICML‚Äô19 Workshop.\nGal, Y .; and Ghahramani, Z. 2016. Dropout as a bayesian ap-\nproximation: Representing model uncertainty in deep learn-\ning. In ICML‚Äô16, 1050‚Äì1059. PMLR.\nGao, B.; and Pavel, L. 2017. On the properties of the softmax\nfunction with application in game theory and reinforcement\nlearning. arXiv preprint arXiv:1704.00805.\nGawlikowski, J.; Tassi, C. R. N.; Ali, M.; Lee, J.; Humt,\nM.; Feng, J.; Kruspe, A.; Triebel, R.; Jung, P.; Roscher, R.;\net al. 2021. A survey of uncertainty in deep neural networks.\narXiv preprint arXiv:2107.03342.\nGhoshal, B.; and Tucker, A. 2020. Estimating uncer-\ntainty and interpretability in deep learning for coronavirus\n(COVID-19) detection. arXiv preprint arXiv:2003.10769.\nGillioz, A.; Casas, J.; Mugellini, E.; and Abou Khaled, O.\n2020. Overview of the Transformer-based models for NLP\ntasks. In FedCSIS‚Äô19, 179‚Äì183. IEEE.\nGraves, A.; Wayne, G.; and Danihelka, I. 2014. Neural tur-\ning machines. arXiv preprint arXiv:1410.5401.\nGumbel, E. J. 1954. Statistical theory of extreme values and\nsome practical applications. A series of lectures. Number\n33. US Govt. Print. OfÔ¨Åce.\nHan, K.; Xiao, A.; Wu, E.; Guo, J.; Xu, C.; and Wang,\nY . 2021. Transformer in transformer. arXiv preprint\narXiv:2103.00112.\nHe, J.; Zhang, X.; Lei, S.; Chen, Z.; Chen, F.; Alhamadani,\nA.; Xiao, B.; and Lu, C. 2020. Towards More Accurate Un-\ncertainty Estimation In Text ClassiÔ¨Åcation. In EMNLP‚Äô20,\n8362‚Äì8372.\nHendrycks, D.; and Gimpel, K. 2017. A baseline for detect-\ning misclassiÔ¨Åed and out-of-distribution examples in neural\nnetworks. In ICLR‚Äô17.\nHoel, C.-J.; Wolff, K.; and Laine, L. 2020. Tactical decision-\nmaking in autonomous driving by reinforcement learning\nwith uncertainty estimation. In 2020 IEEE Intelligent Ve-\nhicles Symposium (IV), 1563‚Äì1569. IEEE.\nJang, E.; Gu, S.; and Poole, B. 2017. Categorical reparame-\nterization with gumbel-softmax. In ICLR‚Äô17.\nKabir, H. D.; Khosravi, A.; Hosen, M. A.; and Nahavandi,\nS. 2018. Neural network-based uncertainty quantiÔ¨Åcation:\nA survey of methodologies and applications. IEEE access,\n6: 36218‚Äì36234.\nKendall, A.; and Gal, Y . 2017. What Uncertainties Do\nWe Need in Bayesian Deep Learning for Computer Vision?\nNeurIPS‚Äô17, 30: 5574‚Äì5584.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980.\nKingma, D. P.; Salimans, T.; and Welling, M. 2015. Vari-\national dropout and the local reparameterization trick. In\nNeurIPS‚Äô15, volume 28, 2575‚Äì2583.\nKingma, D. P.; and Welling, M. 2014. Auto-encoding varia-\ntional bayes. In ICLR‚Äô14.\nLakshminarayanan, B.; Pritzel, A.; and Blundell, C. 2017.\nSimple and scalable predictive uncertainty estimation using\ndeep ensembles. In NeurIPS‚Äô17.\nLee, J.; Lee, Y .; Kim, J.; Kosiorek, A.; Choi, S.; and Teh,\nY . W. 2019. Set transformer: A framework for attention-\nbased permutation-invariant neural networks. In Inter-\nnational Conference on Machine Learning, 3744‚Äì3753.\nPMLR.\nLin, G.; Engel, D. W.; and Eslinger, P. W. 2012. Survey and\nevaluate uncertainty quantiÔ¨Åcation methodologies. Techni-\ncal report, PaciÔ¨Åc Northwest National Lab.(PNNL), Rich-\nland, W A (United States).\nMaas, A. L.; Daly, R. E.; Pham, P. T.; Huang, D.; Ng, A. Y .;\nand Potts, C. 2011. Learning word vectors for sentiment\nanalysis. In NAACL-HLT‚Äô11, 142‚Äì150. Portland, Oregon,\nUSA: Association for Computational Linguistics.\nMaddison, C. J.; Mnih, A.; and Teh, Y . W. 2017. The con-\ncrete distribution: A continuous relaxation of discrete ran-\ndom variables. In ICLR‚Äô17.\nMartin, A.; Ollion, C.; Strub, F.; Corff, S. L.; and Pietquin,\nO. 2020. The Monte Carlo Transformer: a stochastic self-\nattention model for sequence prediction. arXiv preprint\narXiv:2007.08620.\nMatthews, B. W. 1975. Comparison of the predicted and ob-\nserved secondary structure of T4 phage lysozyme. Biochim-\nica et Biophysica Acta (BBA)-Protein Structure, 405(2):\n442‚Äì451.\nOh, G.; and Hong, Y . S. 2021. Managing market risk caused\nby customer preference uncertainty in product family design\nwith launch Ô¨Çexibility: Product option strategy. Computers\n& industrial engineering, 151: 106975.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga,\nL.; Desmaison, A.; Kopf, A.; Yang, E.; DeVito, Z.; Raison,\nM.; Tejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai,\nJ.; and Chintala, S. 2019. PyTorch: An imperative style,\n11154\nhigh-performance deep learning library. In Wallach, H.;\nLarochelle, H.; Beygelzimer, A.; d'Alch ¬¥e-Buc, F.; Fox, E.;\nand Garnett, R., eds., NeuIPS‚Äô19, 8024‚Äì8035.\nRen, P.; Chen, Z.; Ren, Z.; Kanoulas, E.; Monz, C.; and\nDe Rijke, M. 2021. Conversations with search engines:\nSERP-based conversational response generation. TOIS‚Äô20,\n39(4): 1‚Äì29.\nRiedmaier, S.; Danquah, B.; Schick, B.; and Diermeyer, F.\n2021. UniÔ¨Åed framework and survey for model veriÔ¨Åcation,\nvalidation and uncertainty quantiÔ¨Åcation. Archives of Com-\nputational Methods in Engineering, 28(4): 2655‚Äì2688.\nShelmanov, A.; Tsymbalov, E.; Puzyrev, D.; Fedyanin, K.;\nPanchenko, A.; and Panov, M. 2021. How certain is your\ntransformer? In EACL‚Äô21, 1833‚Äì1840.\nSrivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and\nSalakhutdinov, R. 2014. Dropout: A simple way to prevent\nneural networks from overÔ¨Åtting.Journal of Machine Learn-\ning Research, 15(56): 1929‚Äì1958.\nTetko, I. V .; Karpov, P.; Van Deursen, R.; and Godin, G.\n2020. State-of-the-art augmented NLP transformer models\nfor direct and single-step retrosynthesis. Nature communi-\ncations, 11(1): 1‚Äì11.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-\ntention is all you need. In NeurIPS‚Äô17, 5998‚Äì6008.\nVyas, A.; Katharopoulos, A.; and Fleuret, F. 2020. Fast\ntransformers with clustered attention. NeuIPS‚Äô20.\nWang, B.; Shang, L.; Lioma, C.; Jiang, X.; Yang, H.; Liu,\nQ.; and Simonsen, J. G. 2020. On position embeddings in\nbert. In ICLR‚Äô20.\nWang, C.; Lawrence, C.; and Niepert, M. 2021. Uncertainty\nEstimation and Calibration with Finite-State Probabilistic\nRNNs. In ICLR‚Äô21.\nWang, C.; and Niepert, M. 2019. State-regularized recurrent\nneural networks. In ICML‚Äô19, 6596‚Äì6606. PMLR.\nWang, Q.; and Van Hoof, H. 2020. Doubly Stochastic Vari-\national Inference for Neural Processes with Hierarchical La-\ntent Variables. In ICML‚Äô20, 10018‚Äì10028. PMLR.\nWarstadt, A.; Singh, A.; and Bowman, S. R. 2019. Neural\nnetwork acceptability judgments. Transactions of the Asso-\nciation for Computational Linguistics, 7: 625‚Äì641.\n11155"
}