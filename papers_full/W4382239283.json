{
    "title": "CF-ViT: A General Coarse-to-Fine Method for Vision Transformer",
    "url": "https://openalex.org/W4382239283",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3198895904",
            "name": "Mengzhao Chen",
            "affiliations": [
                "Xiamen University"
            ]
        },
        {
            "id": "https://openalex.org/A2115136084",
            "name": "Ming-bao Lin",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A1908799446",
            "name": "Ke Li",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2799099420",
            "name": "Yunhang Shen",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2105710542",
            "name": "Yongjian Wu",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2224909813",
            "name": "Chao Fei",
            "affiliations": [
                "Xiamen University"
            ]
        },
        {
            "id": "https://openalex.org/A2137291500",
            "name": "Rongrong Ji",
            "affiliations": [
                "Institute of Art",
                "Xiamen University"
            ]
        },
        {
            "id": "https://openalex.org/A3198895904",
            "name": "Mengzhao Chen",
            "affiliations": [
                "Xiamen University"
            ]
        },
        {
            "id": "https://openalex.org/A2115136084",
            "name": "Ming-bao Lin",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A1908799446",
            "name": "Ke Li",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2799099420",
            "name": "Yunhang Shen",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2105710542",
            "name": "Yongjian Wu",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2224909813",
            "name": "Chao Fei",
            "affiliations": [
                "Xiamen University"
            ]
        },
        {
            "id": "https://openalex.org/A2137291500",
            "name": "Rongrong Ji",
            "affiliations": [
                "Institute of Art",
                "Xiamen University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3151130473",
        "https://openalex.org/W3157528469",
        "https://openalex.org/W6676297131",
        "https://openalex.org/W6600662924",
        "https://openalex.org/W6790690058",
        "https://openalex.org/W6790299800",
        "https://openalex.org/W3211681816",
        "https://openalex.org/W6849977910",
        "https://openalex.org/W3139587317",
        "https://openalex.org/W2767421475",
        "https://openalex.org/W3172801447",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3174402370",
        "https://openalex.org/W3204182250",
        "https://openalex.org/W3168124404",
        "https://openalex.org/W6838697126",
        "https://openalex.org/W6803695728",
        "https://openalex.org/W2734663976",
        "https://openalex.org/W2946948417",
        "https://openalex.org/W6810521590",
        "https://openalex.org/W6796870316",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W6792240715",
        "https://openalex.org/W2128585298",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W6795678513",
        "https://openalex.org/W3209970085",
        "https://openalex.org/W3092739317",
        "https://openalex.org/W4226146163",
        "https://openalex.org/W4297946955",
        "https://openalex.org/W3170544306",
        "https://openalex.org/W6793958814",
        "https://openalex.org/W3188427387",
        "https://openalex.org/W3011666637",
        "https://openalex.org/W6795901243",
        "https://openalex.org/W3136416617",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W6804597430",
        "https://openalex.org/W3167597877",
        "https://openalex.org/W2962843773",
        "https://openalex.org/W4308536459",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W4213019189",
        "https://openalex.org/W3168101492",
        "https://openalex.org/W4214709605",
        "https://openalex.org/W3035424951",
        "https://openalex.org/W4214661601",
        "https://openalex.org/W4385480633",
        "https://openalex.org/W4312253723",
        "https://openalex.org/W4239072543",
        "https://openalex.org/W4312340826",
        "https://openalex.org/W3156811085",
        "https://openalex.org/W4287324101",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W2950014519",
        "https://openalex.org/W3211783547",
        "https://openalex.org/W3211585026",
        "https://openalex.org/W4308558335",
        "https://openalex.org/W3139049060",
        "https://openalex.org/W4214633470",
        "https://openalex.org/W4226244084",
        "https://openalex.org/W3101720316",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W4308614496",
        "https://openalex.org/W4226353968",
        "https://openalex.org/W4214636423",
        "https://openalex.org/W3133696297",
        "https://openalex.org/W3204647170",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4376983087",
        "https://openalex.org/W3211490618",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W4319994481",
        "https://openalex.org/W4312599212"
    ],
    "abstract": "Vision Transformers (ViT) have made many breakthroughs in computer vision tasks. However, considerable redundancy arises in the spatial dimension of an input image, leading to massive computational costs. Therefore, We propose a coarse-to-fine vision transformer (CF-ViT) to relieve computational burden while retaining performance in this paper. Our proposed CF-ViT is motivated by two important observations in modern ViT models: (1) The coarse-grained patch splitting can locate informative regions of an input image. (2) Most images can be well recognized by a ViT model in a small-length token sequence. Therefore, our CF-ViT implements network inference in a two-stage manner. At coarse inference stage, an input image is split into a small-length patch sequence for a computationally economical classification. If not well recognized, the informative patches are identified and further re-split in a fine-grained granularity. Extensive experiments demonstrate the efficacy of our CF-ViT. For example, without any compromise on performance, CF-ViT reduces 53% FLOPs of LV-ViT, and also achieves 2.01x throughput. Code of this project is at https://github.com/ChenMnZ/CF-V",
    "full_text": "CF-ViT: A General Coarse-to-Fine Method for Vision Transformer\nMengzhao Chen1, Mingbao Lin3, Ke Li3, Yunhang Shen3,\nYongjian Wu3, Fei Chao1, Rongrong Ji1,2*\n1MAC Lab, Department of Artificial Intelligence, Xiamen University\n2Institute of Artificial Intelligence, Xiamen University\n3Tencent Youtu Lab\ncmzxmu@stu.xmu.edu.cn, linmb001@outlook.com, tristanli.sh.gmail.com,\nshenyunhang01@gmail.com, littlekenwu@tencent.com, {fchao, rrji}@xmu.edu.cn\nAbstract\nVision Transformers (ViT) have made many breakthroughs\nin computer vision tasks. However, considerable redundancy\narises in the spatial dimension of an input image, leading\nto massive computational costs. Therefore, We propose a\ncoarse-to-fine vision transformer (CF-ViT) to relieve compu-\ntational burden while retaining performance in this paper. Our\nproposed CF-ViT is motivated by two important observations\nin modern ViT models: (1) The coarse-grained patch splitting\ncan locate informative regions of an input image. (2) Most im-\nages can be well recognized by a ViT model in a small-length\ntoken sequence. Therefore, our CF-ViT implements network\ninference in a two-stage manner. At coarse inference stage,\nan input image is split into a small-length patch sequence\nfor a computationally economical classification. If not well\nrecognized, the informative patches are identified and further\nre-split in a fine-grained granularity. Extensive experiments\ndemonstrate the efficacy of our CF-ViT. For example, with-\nout any compromise on performance, CF-ViT reduces 53%\nFLOPs of LV-ViT, and also achieves 2.01Ã—throughput. Code\nof this project is at https://github.com/ChenMnZ/CF-ViT.\nIntroduction\nTremendous successes of traditional transformer (Vaswani\net al. 2017) in natural language processing (NLP) have\ninspired the researchers to go further on computer vi-\nsion (Han et al. 2022a). Consequently, vision transform-\ners (ViT) (Dosovitskiy et al. 2020) receive ever-increasing\nattentions in many vision tasks such as image classifica-\ntion (Dosovitskiy et al. 2020; Jiang et al. 2021), object de-\ntection (Liu et al. 2021; Wang et al. 2021a), semantic seg-\nmentation (Zheng et al. 2021; Xie et al. 2021), etc.\nBy splitting a 2D image into a patch sequence and us-\ning a linear projection to embed these patches into 1D\ntokens as inputs, ViT merits in its property of modeling\nlong-range dependencies among tokens. Generally speak-\ning, the performance of a ViT model is closely correlated\nwith the token number (Dosovitskiy et al. 2020; Wang et al.\n2021c), to which, however, the computational cost of ViT\nis also quadratically related. Fortunately, images often have\nmore spatial redundancy than languages (Wang, Stuijk, and\n*Corresponding Author\nCopyright Â© 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: (a) Performance comparison (DeiT-S (Touvron\net al. 2021a)) between 100 highest-score patches and 100\nlowest-score patches. The score is measured by the class at-\ntention. (b) Visualization of class attention in the last en-\ncoder of DeiT-S (Best viewed with zooming in).\nDe Haan 2014), such as regions with task-unrelated ob-\njects. Thus, many works (Wang et al. 2020; Yang et al.\n2020; Wang et al. 2022a, 2021b, 2022b; Han et al. 2022b,\n2021c) o try to adaptively reduce the input resolution of\nconvolution neural networks. Also, great efforts have been\nmade to excavate redundant tokens for ViTs. For exam-\nple, PS-ViT (Tang et al. 2022b) enhances transformer ef-\nficiency by a top-down token pruning paradigm. Both Dy-\nnamicViT (Rao et al. 2021) and IA-RED2 (Pan et al. 2021a)\ndevise a lightweight prediction module to estimate the im-\nportance score of each token, and discard low-score tokens.\nFollowing DynamicViT (Rao et al. 2021), EViT (Liang et al.\n2022) regards class attention as a metric of token impor-\ntance, which avoids the introduction of extra parameters.\nUnlike discarding tokens directly, DGE (Song et al. 2021)\nintroduces sparse queries to reduce the output token num-\nber. Evo-ViT (Xu et al. 2022) maintains the spatial structure\nwhile consuming less computational cost to update uninfor-\nmative tokens. DVT (Wang et al. 2021c) cascades multiple\nViTs with increasing tokens, then leverages an early-exiting\npolicy to decide each imageâ€™s token number.\nThis paper observes two insightful phenomena. First,\nsimilar to the fine-grained patch splitting like 14Ã—14, the\ncoarse-grained patch splitting such as 7Ã—7 can also locate\nthe informative regions. To verify this, we first show that\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n7042\nNo. of token 14Ã—14 7Ã—7\nAccuracy 79.8% 73.2%\nFLOPs 4.60G 1.10G\nTable 1: Accuracy and FLOPs of Deit-S (Touvron et al.\n2021a) on ImageNet with different No. of patches as inputs.\nclass attention\n\u0000\nsee Eq. (2)\n\u0001\n(Liang et al. 2022) has a re-\nmarkable ability to identify informative patches. We conduct\na toy experiment on the validation set of ImageNet (Deng\net al. 2009) with a pre-trained DeiT-S model (Touvron et al.\n2021a). Following DeiT-S, we split each image into 14Ã—14\npatches, then compute the class attention score of each patch\nwithin each encoder. Then, the 100 highest-score patches\nand 100 lowest-score patches are respectively fed to DeiT-S\nto obtain their accuracy in Fig. 1(a). In the figure, highest-\nscore patches outperform lowest-score ones by a large mar-\ngin, such a result demonstrates that class attention can well\nreflect more informative patches. Then, in Fig. 1(b), we visu-\nalize the class attention in the last encoder of DeiT-S. As can\nbe seen, 7Ã—7 splitting and 14Ã—14 splitting generally obtain\nsimilar attentive regions.\nThe second phenomenon is that most images can be well\nrecognized by a ViT model in a small-length patch se-\nquence. We train Deit-S (Touvron et al. 2021a) with vary-\ning lengths of input sequence, and report top-1 accuracy\nand FLOPs in Table 1, in which, with 4.2Ã— higher compu-\ntational cost, splitting a 2D image into fine-grained 14Ã—14\npatches only obtains 6.6% accuracy benefits than coarse-\ngrained 7Ã—7 splitting. A similar observation has been dis-\ncussed in DVT (Wang et al. 2021c). This indicates that most\nregions in 73.2% images are â€œeasyâ€ so that coarse-grained\n7Ã—7 patch splitting can well implement the classification,\nand a small portion of images are filled with â€œhardâ€ re-\ngions requiring a fine-grained splitting of 14Ã—14 with heav-\nier computation. Thus, we split the â€œeasyâ€ samples with\ncoarse-grained patch splitting for cheaper computation. In\naddition, for â€œhardâ€ samples, we can differentiate â€œeasyâ€ re-\ngions and â€œhardâ€ regions, and split them with different patch\nsizes in order to pursue an efficient inference while retaining\ngood performance. Note that, these two phenomena can be\nfound in other ViT models as well.\nInspired by the above observations, we propose a novel\ncoarse-to-fine vision transformer in this paper, termed CF-\nViT, which aims to produce correct predictions with input-\nadaptive computational cost. As shown in Fig. 2, the in-\nference of CF-ViT is divided into a coarse inference stage\nand a fine inference stage. The coarse stage receives coarse-\ngrained patches as network inputs, which merits in low com-\nputational cost since the coarse-grained splitting results in\nmuch fewer patches (tokens). If this stage owns a high con-\nfidence score, the network inference terminates; otherwise,\nthe input image is split again into fine-grained patches and\nfed to the fine stage. In contrast to a naive re-splitting of all\ncoarse patches, we design a mechanism of informative re-\ngion identification to further split the informative patches in\na fine-grained state, and retain these patches with less infor-\nmation in a coarse-grained state. Therefore, we avoid heavy\n0.92\nHor\nse\n0.15\nDog\n0.93\nDog\nEasyHard\nTerminate & Output\nContinue\nFeat\nure Reuse\nInformative Region \nIdentification\nCoarse Stage Fine Stage\nFigure 2: Example of CF-ViT.\ncomputational burdens on the redundant image patches.\nAlso, we introduce a mechanism of feature reuse to inject\nthe integral information of the coarse patch to the split fine-\ngrained patches, which further enhance the model perfor-\nmance. We then evaluate our proposed CF-ViT built upon\nDeiT (Touvron et al. 2021a) and LV-ViT (Jiang et al. 2021)\non ImageNet (Deng et al. 2009). Extensive experiments re-\nsults show that our CF-ViT can well boost the inference effi-\nciency. For example, without any performance compromise,\nCF-ViT reduces 53% FLOPs of LV-ViT and also leads to\n2.01Ã— practical throughput on an A100 GPU. In addition,\nextensive ablation studies also demonstrate the efficacy of\neach design in our CF-ViT including the informative region\nidentification and the feature reuse. Visualization of infer-\nence results shows that CF-ViT enables adaptive inference\naccording to the â€œdifficultyâ€ of input images and can accu-\nrately locate informative regions for re-splitting in fine in-\nference stage.\nRelated Work\nVision Transformer\nMotivated by successes of transformer (Vaswani et al. 2017)\nin NLP, researchers develop vision transformer (ViT) (Doso-\nvitskiy et al. 2020) for image recognition. However, the\nlack of inductive bias (Dosovitskiy et al. 2020) requires ViT\nmodel to be pre-trained on a very large-scale data corpus\nsuch as JFT-300M (Sun et al. 2017) to pursue a desired per-\nformance. This demand barricades the development of ViT\nmodel since the large-scale dataset requires a high-capacity\nworkstation. To handle this, many studies (Touvron et al.\n2021a; Jiang et al. 2021) develop specialized training strate-\ngies for ViT models. For instance, DeiT (Touvron et al.\n2021a) introduces an extra token for knowledge distillation.\nLV-ViT (Jiang et al. 2021) leverages all tokens to compute\nthe training loss, and the location-specific supervision la-\nbel of each patch token is generated by a machine anno-\n7043\ntator. In addition, another group focuses on improving the\narchitecture of ViT (Yuan et al. 2021a; Li et al. 2021; Chu\net al. 2021b,a; Yuan et al. 2021b; Heo et al. 2021; Touvron\net al. 2021b; Liu et al. 2021; Li et al. 2022). For example,\nCPVT (Chu et al. 2021b) uses a convolution layer to replace\nthe learnable positional embedding. CaiT (Touvron et al.\n2021b) builds deeper transformers and develops specialized\noptimization strategies for training. TNT (Han et al. 2021a)\nleverages an inner block to model the pixel-wise interactions\nwithin each patch, which merits in preserving more rich lo-\ncal features. Our study in this paper introduces a general\nframework, which aims to improve the inference efficiency\nof various ViT backbones in an input-adaptive manner.\nViT Compression\nExcept to develop high-performing ViT models, the expen-\nsive computation of a ViT model also arouses wide atten-\ntion and methods are explored to facilitate ViT deployment.\nBased on whether the inference is input-dependent, we em-\npirically categorize existing studies on compressing ViTs\ninto two groups below.\nStatic ViT Compression. This group mainly focuses\non reducing the network complexity through manually de-\nsigned modules with a fixed computational graph regard-\nless of the input images. Inspired by the great success of\nhierarchical convolutional neural networks in dense predic-\ntion tasks such as segmentation and detection, recent ad-\nvances (Heo et al. 2021; Liu et al. 2021; Pan et al. 2021b;\nYuan et al. 2021b; Wang et al. 2021a) introduce hierarchi-\ncal transformers. Also, many others (Liu et al. 2021; Huang\net al. 2021; Fang et al. 2021; Yu et al. 2021) consider local\nself-attention to reduce the complexity of traditional global\nself-attention. Compared to these static methods using a\nfixed computational graph, our CF-ViT, improves the infer-\nence efficiency by adaptively selecting an appropriate com-\nputational path for each image.\nDynamic ViT Compression. In contrast to static ViT\ncompression, dynamic ViT adapts the computational graph\naccording to its input images (Han et al. 2021b). Some\nworks (Rao et al. 2021; Pan et al. 2021a; Liang et al. 2022;\nLin et al. 2022) attempt to dynamically prune these tokens\nconsidered unimportant during inference. On the contrary,\nEvo-ViT (Xu et al. 2022) chooses to preserve the unimpor-\ntant tokens, which however, are assigned with a lower com-\nputational budget for updating. Unlike these pruning-based\nmethods, our CF-ViT maintains the integrity of image infor-\nmation and reinforces the inference efficiency by enlarging\nthe size of patches in uninformative regions, which leads to\nfewer tokens. The rationale behind this is that uninformative\nregions like background contribute less to the recognition\nthus a fine-grained patch splitting is unnecessary. Note that,\nthe recent DVT (Wang et al. 2021c) endows a proper token\nnumber for each input image by cascading three transform-\ners. Though meriting in inference acceleration, it inevitably\nincreases the storage overhead by 3Ã—. Different from DVT,\nour CF-ViT trains only one transformer that can accept dif-\nferent sizes of input tokens, and only conducts fine-grained\ntoken splitting on informative regions rather than an entire\nimage. QuadTree (Tang et al. 2022a) also builds token pyra-\nmids in a coarse-to-fine manner. However, QuadTree per-\nforms coarse-to-fine splitting for all images while our CF-\nViT only performs fine splitting on â€œhardâ€ images to further\nreduce computation cost.\nPreliminaries\nVision Transformer(ViT) (Dosovitskiy et al. 2020) splits a\n2D image into flattened 2D patches and uses an linear pro-\njection to map patches into tokens, a.k.a. patch embeddings.\nBesides, an extra [class] token, which represents the global\nimage information, is appended as well. Moreover, all to-\nkens are added with a learnable positional embedding. Thus,\nthe input token sequence of a ViT model is:\nX0 = [x0\n0; x1\n0; ...; xN\n0 ] + Epos, (1)\nwhere xi\n0 âˆˆ RD is a D-dimensional token of the i-th patch\nif i >0, and [class] token if i = 0. The Epos and N are the\nposition embedding and patch number.\nA ViT modelV contains K sequentially stacked encoders,\neach of which consists of a self-attention (SA) module1 and\na feed-forward network (FFN). In SA of the k-th encoder,\nthe token sequence Xkâˆ’1 is projected into a query matrix\nQk âˆˆ R(N+1)Ã—D, a key matrix Kk âˆˆ R(N+1)Ã—D, and a\nvalue matrix Vk âˆˆ R(N+1)Ã—D. Then, the self-attention ma-\ntrix Ak âˆˆ R(N+1)Ã—(N+1) is computed as:\nAk = Softmax(QkKT\nk\nâˆš\nD\n) = [a0\nk; a1\nk; ...; aN\nk ]. (2)\nThe a0\nk âˆˆ R(N+1) is known as class attention, reflecting\nthe interactions between [class] token and other patch to-\nkens. With Ak, the outputs of SA, i.e., AkVk, are sent to\nFFN consisting of two fully-connected layers to derive the\nupdated tokens Xk = [x0\nk; x1\nk; ...; xN\nk ]. The [class] token x0\nk\nis derived as:\nx0\nk = F F N(a0\nkVk). (3)\nAfter a series of SA-FFN transformations, the [class] to-\nken x0\nK from the K-th encoder is fed to the classifier to pre-\ndict the category of the input.\nViT Complexity. Given that an image is split into N\npatches, the computational complexity of SA and FFN\nare (Xu et al. 2022):\nO(SA) = 3ND2 + 2N2D,\nO(F F N) = 8ND2.\n(4)\nWe can see that the complexities of SA and FNN are re-\nspectively quadratic and linear to N. Thus, ViT complexity\ncan be well reduced by decreasing the input patch (token)\nnumber (Rao et al. 2021; Pan et al. 2021a; Liang et al. 2022;\nWang et al. 2021c), which is also the focus of this paper.\n1SA has been replaced by multi-head self-attention (MHSA)\nin most ViTs. For brevity, we simply discuss SA herein. Also, we\nignore the shortcut operation in Eq. (3).\n7044\n1\n2\n9 â€¦\nVision Transformer Coarse Stage \nOutput\nGlobal Class Attention\nVision Transformer\nFeature Reuse\n0\nYes\nNo\nInformative\nRegion Identification\n0\nğ‘µğ’. / à·ªğ‘µğ’.\n[class] Token\nImage Tokens\nElement-wise \nAddFine Stage \nOutput\nà·©ğŸ\nà·©ğŸ\nà·ªğŸğŸ â€¦\n0\nTerminateï¼Ÿ\nCoarse-grained \nSplitting\nFine-grained\nSplitting\nFigure 3: Framework of our CF-ViT. Note that both coarse stage and fine stage share the same network parameters.\nCoarse-to-Fine Vision Transformer\nThis section formally introduces our CF-ViT that decreases\nthe computational cost by reducing the input sequence\nlength. Our motive lies in two observations in Sec. : (1) The\ncoarse-grained patch splitting can well locate the informa-\ntive objects as well. (2) Most images can be well recognized\nby a ViT model in a small sequence length. These inspire\nus to implement a ViT in a two-stage manner. As shown in\nFig. 3, the coarse inference stage implements image recog-\nnition with a small length of token sequence. If not well rec-\nognized, the informative regions will be further split for a\nfine-grained recognition. Details are given below.\nCoarse Inference Stage\nCF-ViT first performs a coarse splitting to recognize images\nfilled with â€œeasyâ€ regions. Also, it locates informative re-\ngions for an efficient inference when meeting â€œhardâ€ sam-\nples. At coarse stage, the input of our CF-ViT model V is:\nXc\n0 = [x0\n0; x1\n0; ...; xNc\n0 ] + Ec\npos, (5)\nwhere Nc is the number of coarse patches. SupposingV con-\ntains K encoders, after SA-FFN transformations in Sec. , the\noutput token sequence of V is:\nV(Xc\n0) = [x0\nK; x1\nK; ...; xNc\nK ]. (6)\nFinally, the [class] token x0\nK is fed to a classifier F to\nobtain the coarse-stage category prediction distribution pc:\npc = F(x0\nK) = [pc\n1, pc\n2, ..., pc\nn], (7)\nwhere n denotes the category number. So far, we can obtain\nthe predicted category of the input as:\nj = arg max\ni\npc\ni . (8)\nWe expect a large pc\nj since it serves as a prediction con-\nfidence score at coarse inference stage where the compu-\ntational cost is very cheap due to a small value of patch\nnumber Nc. We introduce a thresholdÎ· to realize a trade-off\nbetween performance and computation. In our implementa-\ntion, if pc\nj â‰¥ Î·, the inference will terminate and we attribute\nthe input to category j. Otherwise, the input images might\ncontain â€œhardâ€ regions undistinguished to the ViT model.\nThus, a more fine-grained patch splitting is urgent.\nInformative Region Identification.The most naive solu-\ntion is to further split all the coarse patches[x1\n0; x2\n0; ...; xNc\n0 ].\nHowever, the drastically increasing tokens inevitably cause\nsevere computational costs. Instead, for an economical bud-\nget, we propose to identify and then re-split these informa-\ntive regions that are the most beneficial to the performance\nincrease. Thus, the key now lies in how to identify the infor-\nmative patches.\nRecall that, the class attention a0\nk âˆˆ Ak in Eq. (2) re-\nflects the interactions between [class] token and other im-\nage patch tokens in the k-th encoder. Besides, the [class]\ntoken x0\nk = F F N(a0\nkVk), which indicates that each item\n(a0\nk)i models the weighted coefficient of the i-th token xi\n0\nto the performance. Therefore, it is natural to use the class\nattention a0\nk as a score to indicate if a token is informative.\nFrom Fig. 1(a), we can see that, the performance of preserv-\ning 100 highest-score patches usually outperforms that of\n100 lowest-score patches. This demonstrates that class at-\ntention can be a reliable measure. Nevertheless, we can also\nobserve the instability of class attention in the bottom layers,\nsuch as the first encoder. To overcome it, we propose global\nclass attention that combines class attention across different\nencoders using exponential moving average (EMA) to better\nidentify informative patches:\nÂ¯ak = Î² Â· Â¯akâˆ’1 + (1 âˆ’ Î²) Â· a0\nk, (9)\nwhere Î² = 0.99. The global class attention begins from the\n4-th encoder and we select patches with high-score global\nclass attention in the last encoder Â¯aK.\nEarlier studies (Liang et al. 2022; Xu et al. 2022) also\nadopt class attention to indicate token importance. This pa-\nper differs in two folds: (1) A comprehensive demonstration\n7045\non high-score patches are given in Fig. 1(a). (2) We consider\nthe global class attention instead of class attention in a par-\nticular layer (Liang et al. 2022), efficacy of which is given\nin Table. 4.\nFine Inference Stage\nWith the global class attention Â¯aK on hand, we continue to\nperform a fine-grained splitting on informative patches when\nthe prediction in coarse stage pc\nj < Î·, which indicates an\nindistinguishable input image.\nTo that effect, we pick up these coarse patches, attention\nscores of which are within the top- Î±Nc largest among the\nNc coarse patches, where Î± âˆˆ [0, 1] represents the rate of\ninformative patches. Then, as shown in Fig. 3, each infor-\nmative patch is further split into 2 Ã— 2 patches for better\nrepresentation in a finer granularity. Consequently, the patch\nnumber after fine-grained splitting is:\nNf = 4âŒˆNcÎ±âŒ‰ + âŒŠNc(1 âˆ’ Î±)âŒ‹, (10)\nwhere âŒˆÂ·âŒ‰ and âŒŠÂ·âŒ‹ respectively round up and round down\ntheir inputs. It is intuitive that Î± provides a trade-off be-\ntween accuracy and efficiency. The Î± = 0 indicates no fine\ninference and results in the fewest patches. Though compu-\ntationally economical, performance drops if the test set is\nfull of â€œhardâ€ images. In contrast, Î± = 1 leads the fine in-\nference stage of our CF-ViT degenerates to traditional ViT\nmodels (Jiang et al. 2021; Touvron et al. 2021a). In this case,\nthe computational cost is extraordinarily expensive. For a\ntrade-off, we set Î± to 0.5 in our implementation.\nFeature Reuse. After re-splitting of each input image, the\ninput token sequence at fine inference stage of our CF-ViT\nbecomes:\nËœXf\n0 = [x0\n0; Ëœx1\n0; ...; ËœxNf\n0 ] + Ef\npos. (11)\nSuppose that an informative patch xi\n0 in Eq. (5) is further\nsplit into 2Ã—2 patches in Eq. (11), which offer a finer granu-\nlarity. Nevertheless, they also cut off the integrity of the local\npatch xi\n0. To solve this, we also devise a feature reuse mod-\nule to inject the information of xi\n0 into the four fine-grained\npatches.\nFig. 4 illustrates our feature reuse. It takes the output to-\nken sequence from coarse stage as input. Similar to FNN,\nthe input will be processed by an MLP layer first to allow\na flexible transformation. Then, these transformed tokens\nare reshaped and each of them is copied 4Ã—. Further, these\ntokens corresponding to fine-grained splitting patches are\npicked up as the output of feature reuse module, denoted as\nXr = F R([x1\nK; x2\nK; ...; xNc\nK ]). We do not consider reusing\n[class] token and uninformative tokens by zeroing out them,\nsince we empirically find them unbeneficial to the perfor-\nmance as verified in Table. 5. Finally, we shortcutXr to the\nre-split token sequence ËœXf\n0 in Eq. (11) as the final input of\nour CF-ViT model V at fine inference stage. Consequently,\nthe output of V is:\nV( ËœXf\n0 + Xr) = [Ëœx0\nK; Ëœx1\nK; ...; ËœxNf\nK ]. (12)\nFinally, the [class] token Ëœx0\nK is fed to the same classifier\nF to obtain the fine-stage category prediction distribution\npf :\npf = F(x0\nK) = [pf\n1 , pf\n2 , ..., pf\nn]. (13)\nModel Î· Top-1 Acc. FLOPs Throughput\n(%) (G) (img./s)\nDeiT-S - 79.8 4.6 2601\nCF-ViT 0.5 79.8(+0.0) 1.8(â†“ 61%) 4903(â†‘1.88Ã—)\nCF-ViT 0.75 80.7(+0.9) 2.6(â†“ 43%) 3701(â†‘1.32Ã—)\nCF-ViT 1.0 80.8(+1.0) 4.0(â†“ 13%) 2760(â†‘1.06Ã—)\nLV-ViT-S - 83.3 6.6 1681\nCF-ViT 0.63 83.3(+0.0) 3.1(â†“ 53%) 3393(â†‘2.01Ã—)\nCF-ViT 0.75 83.5(+0.2) 4.0â†“ 39%) 2827(â†‘1.68Ã—)\nCF-ViT 1.0 83.6(+0.3) 6.1(â†“ 7%) 2022(â†‘1.31Ã—)\nTable 2: Comparison between CF-ViT and its backbones.\nTraining Strategy\nDuring the training of our CV-ViT, we set the confidence\nthreshold Î· = 1, which means the fine inference stage will\nbe always executed for every input image. On the one hand,\nwe expect the fine-grained splitting can well fit the ground\ntruth label y for an accurate prediction of the input. On the\nother hand, we expect the coarse-grained splitting obsesses\na similar output with that of fine-grained splitting such that\nmost input can be well recognized at coarse inference stage,\nwhich indicates less computational cost. Consequently, the\ntraining loss of our CF-ViT is given below:\nloss = CE(pf , y) + KL(pc, pf ), (14)\nwhere CE(Â·, Â·) and KL(Â·, Â·) respectively represent the cross\nentropy loss and Kullback-Leibler divergence.\nDuring the inference of our CV-ViT, by varying the value\nof Î·, we can obtain a trade-off between computational bud-\nget and accuracy performance. A large Î· means more inputs\nwill be sent to fine inference stage, which indicates better\nperformance but more computational cost, and vice versa.\nExperiments\nImplementation Details\nFor ease of comparison, following existing studies on\nViT (Rao et al. 2021; Liang et al. 2022; Tang et al. 2022b;\nXu et al. 2022), we build our CF-ViT with DeiT-S (w/o dis-\ntillation) (Touvron et al. 2021a) and LV-ViT-S (Jiang et al.\n2021) as backbone networks, all of which split each image\ninto 14 Ã— 14 = 196 patches. To show the advantages of our\nCF-ViT, we conduct the experiments on ImageNet (Deng\net al. 2009) from two perspectives: (1) Each input image is\nsplit into 7 Ã— 7 (Nc = 49) patches at coarse inference stage,\nleading to a total of Nf = 124 patches at fine inference\nstage according to Eq. (10). As results, the computation of\nour CF-ViT is much cheaper than its backbones due to its\nless split patches. (2) The image patches number at coarse\ninference stage is changed to 9 Ã— 9, leading to 204 patches\nat fine inference stage, which maintains similar FLOPs with\nthe backbones. For a clearer presentation, CF-ViT is denoted\nas CF-ViTâˆ— in this case.\nAll training settings of our CF-ViT, such as image pro-\ncessing, learning rate, etc, are to follow these of DeiT and\nLV-ViT. In the training phase, only conducting the fine-\ngrained splitting at informative regions would affect the con-\n7046\nVision Transformer\nVision Transformer\nğ‘­ğ‘¹\nMLP\nâ€¦ Reshape\nZero\nPadding\nInformative\nToken Selection\nUpsample\nFlatten and Insert \nzero-padding\n[class] token\nâ€¦\n1\n2\n9 â€¦\n0\n1\n2\n9 â€¦\n0\nà·©ğŸ\nà·©ğŸ\nà·ªğŸğŸ\n0\nFigure 4: Illustration of our feature reuse module. An MLP is firstly introduced for a flexible transformation among token\nsequences from coarse stage. Then, four copies of each token are executed as an upsampling strategy and these areas corre-\nsponding to fine-grained splitting patches are selected to shortcut to the fine-grained token sequence.\nvergence. Therefore, we split the entire image into fine-\ngrained patches in the first 200 epochs, and select informa-\ntive coarse patches for fine-grained splitting in the remaining\ntraining process. Our CF-ViT model is trained on a work-\nstation with 4 A100 GPUs. Notably, both coarse stage and\nfine stage share the same network parameters. Due to differ-\nent sizes of patches between coarse stage and fine stage, we\ndownsample the coarse patches to the shape of fine-grained\npatches in order to facilitate sharing parameters in the patch\nembedding layer.\nExperimental Results\nModel Efficiency. To demonstrate our model efficiency, we\nconduct comparisons between our CF-ViT and its back-\nbones. The measurement metrics include top-1 classification\naccuracy, model FLOPs and model throughput. Following\nexisting studies (Wang et al. 2021c; Liang et al. 2022), the\nmodel throughput is measured as the number of processed\nimages per second on a single A100 GPU. We feed the\nmodel 50,000 images in the validation set of ImageNet with\na batch size of 1,024, and record the total inference time.\nThen, the throughput is computed as 50,000\ntotal inference time .\nTable 2 displays the comparison results with varying val-\nues of threshold Î· which balances accuracy and efficiency as\ndiscussed in Sec. . It can be observed that when maintaining\nthe same accuracy with the backbone, CF-ViT significantly\nreduces model FLOPs of DeiT-S by 61% and LV-ViT-S by\n53%. Consequently, our CF-ViT obtains a great power to\nprocess images, leading to 1.88Ã— throughput improvements\nover DeiT-S and 2.01Ã— over LV-ViT-S. The supreme effi-\nciency is attributed to our design of informative region iden-\ntification which further splits only informative patches in the\ncoarse stage. Besides, with a larger Î·, our CF-ViT manifests\nnot only FLOPs and throughput, but better top-1 accuracy.\nWhen Î· = 1 which indicates all the input are sent to fine\ninference stage, our CF-ViT significantly increases the per-\nformance of DeiT-S by 1.0% and LV-ViT-S by 0.3%. These\nModel Top-1 Acc.(%)\nFLOPs(G)\nDeiT-S\nBaseline (Touvron et\nal. 2021a) 79.8 4.6\nDynamicViT (Rao et al. 2021) 79.3 2.9\nIA-RED2 (Pan et al. 2021a) 79.1 3.2\nPS-ViT (Tang et al. 2022b) 79.4 2.6\nEVIT (Liang et al. 2022) 79.5 3.0\nEvo-ViT (Xu et al. 2022) 79.4 3.0\nCF-ViT(Î· = 0.5)(Ours) 79.8 1.8\nCF-ViT(Î· = 0.75)(Ours) 80.7 2.6\nLV-ViT\n-S\nBaseline (Jiang et al.\n2021) 83.3 6.6\nDynamicViT (Rao et al. 2021) 83.0 4.6\nEVIT (Liang et al. 2022) 83.0 4.7\nSiT (Zong et al. 2022) 83.2 4.0\nCF-ViT(Î· = 0.63)(Ours) 83.3 3.1\nCF-ViT(Î· = 0.75)(Ours) 83.5 4.0\nTable 3: Comparisons between existing token slimming\nbased ViT compression methods and our CF-ViT.\nresults well demonstrate that our CF-ViT can well maintain a\ntrade-off between model performance and model efficiency.\nComparison with Compressed Models. To demonstrate\nthe efficacy of our coarse-to-fine patch splitting in reduc-\ning model complexity, we further compare our CF-ViT with\nrecent studies on compressing ViT models, including token\nslimming compression and early-exiting compression.\n(1) Token slimming compression reduces the complex-\nity of ViT models by reducing the number of input tokens\n(patches), which is also the focus of this paper. Table 3\nshows the comparison with existing token slimming based\nViT compression methods, including DynamicViT (Rao\net al. 2021), IA-RED 2 (Pan et al. 2021a), PS-ViT (Tang\net al. 2022b), EViT (Liang et al. 2022), Evo-ViT (Xu et al.\n2022) and SiT (Zong et al. 2022). We report top-1 accu-\nracy and FLOPs for performance evaluation. Results with\nDeiT-S and LV-ViT-S as backbones indicate that our CF-\n7047\n1 2 3 4 5 6\nFLOPs(G)/image\n72\n74\n76\n78\n80\n82Accuracy(%)\nCF-ViT\nCF-ViT*\nDVT\nDeiT\nMSDNet\nRANet\nFigure 5: Comparison between our CF-ViT and exist-\ning early-exiting methods. MSDNet (Huang et al. 2018)\nand RANet (Yang et al. 2020) are CNN-based models.\nDVT (Wang et al. 2021c) and CF-ViT are built upon DeiT.\nViT outperforms previous methods w.r.t. accuracy perfor-\nmance and FLOPs reduction. For example, CF-ViT signifi-\ncantly reduces the FLOPs of DeiT-S to 1.8G FLOPs without\nany compromise on accuracy performance, while the per-\nformance of recent advance, Evo-ViT, has only 79.4% with\nmuch heavy FLOPs burden of 3.0G. Similar results can be\nobserved when using LV-ViT-S as the backbone.\n(2) Early-exiting compression stops the inference if the\nintermediate representation of an input satisfies a particular\ncriterion, which is also considered in the coarse inference\nstage of our CF-ViT where the computational graph stops\nif the prediction confidence pc\nj exceeds the threshold Î·. In\nFig. 5, we further compare with the early-exiting methods,\nincluding CNN-based models such as MSDNet (Huang et al.\n2018) and RANet (Yang et al. 2020), as well as transformer-\nbased models such as DVT (Wang et al. 2021c). For fair\ncomparison, both DVT and our CF-ViT are constructed\nupon DeiT-S as the backbone. From Fig. 5, two phenom-\nena can be observed: (1) Transformer-based models usu-\nally show supreme performance over CNN-based methods\nunder similar FLOPs consumption. (2) Our CF-ViT con-\nsistently results in best accuracy than DVT that cascades\nmultiple ViTs with an increasing token number. In contrast,\nour CF-ViT implements fine-grained splitting only for the\ninformative regions, leading to an overall reduction in to-\nkens. Thus, with similar accuracy, CF-ViT manifests smaller\nFLOPs consumption.\nComparison with SOTAs. Fig. 6 compares the accu-\nracy and FLOPs trade-off of popular ViT models as well\nas our CF-ViT built upon LV-ViT-S(Jiang et al. 2021).\nThe compared methods include DeiT (Touvron et al.\n2021a), PVT (Wang et al. 2021a), CoaT (Xu et al. 2021),\nCrossViT (Chen, Fan, and Panda 2021), Swin (Liu et al.\n2021), T2T-ViT (Yuan et al. 2021b), CaiT (Touvron et al.\n2021b), iFormers (Si et al. 2022), and EfficientNet (Tan and\nLe 2019). It can be observed that CF-ViT is significantly\ncompetitive in computation-accuracy trade-off among these\nDeiT-SPVT-S\nCrossViT-S\nSwin-T\nCoat-Lite-S\nT2T-ViT-14\nT2T-ViT-19\nT2T-ViT-24\nSwin-S\nSwin-B\nEfficientNet-B4\nEfficientNet-B5\nEfficientNet-B6\nPVT-M\nDeiT-B\nCrossViT-B\nLV-ViT-S\nLV-ViT-M\nLV-ViT-S@384\nCaiT-S-24\nCaiT-S-36 CaiT-S-48\nCoat-Lite-M\n3.5x\nFigure 6: Comparison with popular ViT models. Our CF-\nViT is built upon LV-ViT-S.\nAblation Top-1 Acc.(%)\ncoarse fine\nnegative GCA 74.9 77.6\nrandom 75.3 79.6\nlast class attention 75.3 80.3\nOurs 75.5 80.8\nTable 4: Performance comparison between our informa-\ntive region identification and its variants. GCA means global\nclass attention.\nbaselines. For example, CF-ViT âˆ— achieves 84.1% accuracy\nwith 3.5Ã— less FLOPs compared with the vanilla LV-ViT-M.\nAblation Study\nWe analyze the efficacy of each design in our CF-ViT, in-\ncluding informative region identification, feature reuse and\nearly-exiting. To show the effectiveness of our designs, we\nalso compare our informative region identification and fea-\nture reuse with other alternatives. To show their necessity,\nwe remove each design individually and display the perfor-\nmance. All ablation studies take DeiT-S as backbone.\nInformative region identification. Three variants are de-\nveloped to replace our informative region identification: (1)\nNegative global class attention, which selects the regions\nwith smaller global class attention. (2) Random, which ran-\ndomly picks up regions for fine-grained. (3) Last class atten-\ntion, which leverages the class attention in the last encoder.\nWe deactivate the early termination, and compare the per-\nformance in Table 4. It is intuitive that the negative global\nclass attention results in the poorest performance since the\nmost informative are removed in this setting. This result is\nin coincidence with the observation in Fig. 1(a). Our infor-\nmative region identification considers the global class atten-\ntion, leading to performance increase compared to this only\nconsidering the last class attention, well demonstrating the\ncorrectness of our motive to combine class attention across\ndifferent layers.\nFeature Reuse. Our feature reuse leverages a MLP to pro-\n7048\nAblation Top-1 Acc.(%)\ncoarse fine\nw/o reuse 75.2 80.0\nOurs + [class] token 75.4 80.2\nOurs + uninformative tokens 75.4 80.6\nMLPâ†’Linear 75.3 80.6\nOurs 75.5 80.8\nTable 5: Performance comparison between our feature reuse\nand its variants.\nFigure 7: Performance analysis of removing each of the\nthree designs.\n0.4 0.5(default) 0.6 0.7 0.8 0.9\nTop1-Acc(%) 80.4 80.8 80.9 81.1 81.3 81.4\nFLOPs(G) 3.7 4.0 4.4 4.7 5.1 5.5\nTable 6: Accuracy and FLOPs with different values ofÎ±.\n0 0.5 0.9 0.99(default) 0.999\nTop1-Acc(%) 80.3 80.5 80.7 80.8 80.8\nTable 7: Accuracy with different values ofÎ².\ncess the output image token in the coarse inference stage and\nshortcuts them to the fine inference stage. Note that we do\nnot reuse the [class] token and uninformative tokens. Table 5\ncompares our feature reuse with three variants including: (1)\nIntegrating [class] token to our feature reuse. (2) Integrating\nuninformative tokens to our feature reuse. (3) Replacing the\nMLP with one single linear layer. From Table 5, we can see\nthat considering [class] token or uninformative tokens has a\nnegative impact on the performance. Besides, replacing the\nMLP layer with a linear layer drops down the performance\nfrom 80.8% to 80.6%. These results well demonstrate the\neffectiveness of our design of feature reuse.\nNecessity of each design. Fig. 7 plots the performance of\nour CF-ViT by individually removing each design. Gener-\nally, we can observe that the removal of each component in-\ncurs severe performance drops. Thus, all three designs are\nvital to the final performance of our CF-ViT. It is worth\nAblation Top-1 Acc.(%)\ncoarse fine\nCE + CE 75.7 80.3\nCE + KL(ours) 75.5 80.8\nTable 8: Performance comparison between different loss\nfunction.\nFigure 8: No. of images correctly classified at coarse and\nfine stages.\nnoting that, the one without informative region identifica-\ntion split all the coarse-grained patches, leading to a drastic\nincrease of tokens. However, our informative identification\nchooses to split only informative regions, which greatly re-\nduces tokens and thus brings less computational cost.\nInfluence of Î±. Tab. 6 provides accuracy of fine inference\nstage and FLOPs with different values of Î±\n\u0000\nsee Eq. 10\n\u0001\n.\nWe can see that from Tab. 6 that, a larger Î± leads to better\naccuracy but more FLOPs consumption. In this paper, we\nset Î± as 0.5 for a accuracy-FLOPs trade-off.\nInfluence of Î². Tab. 7 provides accuracy of fine inference\nstage with different values of Î²\n\u0000\nsee Eq. 9\n\u0001\n. It is intuitive\nthat Î² indicates the weight of attention from the shallow en-\ncoder. The Î² = 0 indicates that only the class attention from\nthe last encoder is used. We set Î² = 0.99 as default for its\noptimal performance.\nInfluence of loss function. As shown in Eq. (14), we use\nCE(Â·, Â·) to make the output of fine stage fit the truth label,\nand use KL(Â·, Â·) to make the output of coarse stage fit the\noutput of fine stage. We also try to make the output of coarse\nstage and the output of fine stage both fit the truth label:\nË†loss = CE(pf , y) + CE(pc, y), (15)\nAs show in Tab. 8, compare with origin CE+KL\n\u0000\nEq. (14)\n\u0001\n,\nCE+CE\n\u0000\nEq. (15)\n\u0001\nimplement cause slightly benefit (+0.2%)\nin coarse inference stage but more degradation (-0.5%) in\nfine inference stage. We choose Eq. (14) as the loss function\nbecause of the significant benefits in fine inference stage.\nVisualization\nIn Fig. 9, we illustrate some images that are correctly rec-\nognized at coarse inference by CF-ViT(DeiT-S), as well as\n7049\nFineCoarse\ntraffic light yawl brambling\nFineCoarse FineCoarse\nFigure 9: Illustration of images correctly classified at coarse stage and fine stage. For fine stage, we visualize the regions selected\nby our informative region identification (grey boxes) indicate the uninformative patches.\nsome recognized at fine inference stage. For a better illus-\ntration, we only visualize informative regions if images are\nrecognized at fine inference stage. We can observe an overall\ntrend that images well classified at coarse stage are mostly\nfilled with â€œeasyâ€ regions. Consequently, a coarse-grained\nsplitting can well tell the categories of these images. On\nthe contrary, these containing complex scenes and obscure\nobjects require to be further split for a correct recognition,\nwhich is realized at fine stage. Besides, the selected regions\nby our informative region identification mostly locate the\ntarget objects. In Fig. 8, we also show some statistics w.r.t.\nthe number of images correctly classified at coarse stage\nand fine stage. By adjusting the threshold Î·, CF-ViT models\ncan be obtained with different computational budgets. With\na larger Î·, more images will be further split and fed to fine\ninference stage for recognition. Therefore, the increasing Î·\nbrings about more computational cost and the number of im-\nages correctly classified at fine stage also increases.\nConclusion\nThis paper focuses on reducing the redundant input tokens\nfor accelerating vision transformers. Specially, we proposed\na coarse-to-fine vision transformer (CF-ViT), the inference\nof which is two-fold including a coarse inference stage and a\nfine inference stage. The former splits the input image into a\nsmall-length token sequence to recognize these images filled\nwith â€œeasyâ€ regions in a computationally economical man-\nner while the latter further splits the informative patches for\na better recognition if the coarse inference does not well\nclassify the input. Extensive experiments indicate that our\nCF-ViT can achieve a better trade-off between performance\nand efficiency. Furthermore, transferring the coarse-to-fine\ninference paradigm to dense prediction tasks such as object\ndetection and semantic segmentation will be included in our\nfuture work.\nAcknowledgements\nThis work was supported by the National Science Fund for\nDistinguished Young Scholars (No.62025603), the National\nNatural Science Foundation of China (No. U21B2037, No.\nU22B2051, No. 62176222, No. 62176223, No. 62176226,\nNo. 62072386, No. 62072387, No. 62072389, No.\n62002305 and No. 62272401), Guangdong Basic and Ap-\nplied Basic Research Foundation(No.2019B1515120049),\nand the Natural Science Foundation of Fujian Province of\nChina (No.2021J01002, No.2022J06001).\nReferences\nChen, C.-F. R.; Fan, Q.; and Panda, R. 2021. Crossvit:\nCross-attention multi-scale vision transformer for image\nclassification. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV), 357â€“366.\nChu, X.; Tian, Z.; Wang, Y .; Zhang, B.; Ren, H.; Wei, X.;\nXia, H.; and Shen, C. 2021a. Twins: Revisiting the design\nof spatial attention in vision transformers. In Advances in\nNeural Information Processing Systems (NeurIPS).\nChu, X.; Tian, Z.; Zhang, B.; Wang, X.; Wei, X.; Xia, H.;\nand Shen, C. 2021b. Conditional positional encodings for\nvision transformers. arXiv:2102.10882.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), 248â€“255.\n7050\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An Image is Worth\n16x16 Words: Transformers for Image Recognition at Scale.\nIn International Conference on Learning Representations\n(ICLR).\nFang, J.; Xie, L.; Wang, X.; Zhang, X.; Liu, W.; and Tian, Q.\n2021. Msg-transformer: Exchanging local spatial informa-\ntion by manipulating messenger tokens. arXiv:2105.15168.\nHan, K.; Wang, Y .; Chen, H.; Chen, X.; Guo, J.; Liu, Z.;\nTang, Y .; Xiao, A.; Xu, C.; Xu, Y .; et al. 2022a. A survey on\nvision transformer. IEEE Transactions on Pattern Analysis\nand Machine Intelligence (TPAMI).\nHan, K.; Xiao, A.; Wu, E.; Guo, J.; Xu, C.; and Wang, Y .\n2021a. Transformer in transformer. In Advances in Neural\nInformation Processing Systems (NeurIPS).\nHan, Y .; Huang, G.; Song, S.; Yang, L.; Wang, H.; and\nWang, Y . 2021b. Dynamic neural networks: A survey.IEEE\nTransactions on Pattern Analysis and Machine Intelligence\n(TPAMI).\nHan, Y .; Huang, G.; Song, S.; Yang, L.; Zhang, Y .; and Jiang,\nH. 2021c. Spatially adaptive feature refinement for efficient\ninference. IEEE Transactions on Image Processing, 9345â€“\n9358.\nHan, Y .; Yuan, Z.; Pu, Y .; Xue, C.; Song, S.; Sun, G.; and\nHuang, G. 2022b. Latency-aware Spatial-wise Dynamic\nNetworks. In Advances in Neural Information Processing\nSystems (NeurIPS).\nHeo, B.; Yun, S.; Han, D.; Chun, S.; Choe, J.; and Oh, S. J.\n2021. Rethinking spatial dimensions of vision transformers.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV), 11936â€“11945.\nHuang, G.; Chen, D.; Li, T.; Wu, F.; van der Maaten, L.;\nand Weinberger, K. Q. 2018. Multi-scale dense networks\nfor resource efficient image classification. In International\nConference on Learning Representations (ICLR).\nHuang, Z.; Ben, Y .; Luo, G.; Cheng, P.; Yu, G.; and Fu, B.\n2021. Shuffle transformer: Rethinking spatial shuffle for vi-\nsion transformer. arXiv:2106.03650.\nJiang, Z.-H.; Hou, Q.; Yuan, L.; Zhou, D.; Shi, Y .; Jin, X.;\nWang, A.; and Feng, J. 2021. All tokens matter: Token la-\nbeling for training better vision transformers. In Advances\nin Neural Information Processing Systems (NeurIPS).\nLi, S.; Wang, Z.; Liu, Z.; Tan, C.; Lin, H.; Wu, D.; Chen, Z.;\nZheng, J.; and Li, S. Z. 2022. Efficient Multi-order Gated\nAggregation Network. arXiv:2211.03295.\nLi, Y .; Zhang, K.; Cao, J.; Timofte, R.; and Van Gool, L.\n2021. Localvit: Bringing locality to vision transformers.\narXiv:2104.05707.\nLiang, Y .; GE, C.; Tong, Z.; Song, Y .; Wang, J.; and Xie,\nP. 2022. EviT: Expediting Vision Transformers via Token\nReorganizations. In International Conference on Learning\nRepresentations (ICLR).\nLin, M.; Chen, M.; Zhang, Y .; Li, K.; Shen, Y .; Shen, C.; and\nJi, R. 2022. Super Vision Transformer. arXiv:2205.11397.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), 10012â€“10022.\nPan, B.; Panda, R.; Jiang, Y .; Wang, Z.; Feris, R.; and Oliva,\nA. 2021a. IA-RED 2: Interpretability-Aware Redundancy\nReduction for Vision Transformers. In Advances in Neural\nInformation Processing Systems (NeurIPS).\nPan, Z.; Zhuang, B.; Liu, J.; He, H.; and Cai, J. 2021b.\nScalable vision transformers with hierarchical pooling. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 377â€“386.\nRao, Y .; Zhao, W.; Liu, B.; Lu, J.; Zhou, J.; and Hsieh, C.-J.\n2021. Dynamicvit: Efficient vision transformers with dy-\nnamic token sparsification. In Advances in Neural Informa-\ntion Processing Systems (NeurIPS).\nSi, C.; Yu, W.; Zhou, P.; Zhou, Y .; Wang, X.; and Yan, S.\n2022. Inception Transformer. In Advances in Neural Infor-\nmation Processing Systems (NeurIPS).\nSong, L.; Zhang, S.; Liu, S.; Li, Z.; He, X.; Sun, H.; Sun,\nJ.; and Zheng, N. 2021. Dynamic grained encoder for vision\ntransformers. In Advances in Neural Information Processing\nSystems (NeurIPS).\nSun, C.; Shrivastava, A.; Singh, S.; and Gupta, A. 2017. Re-\nvisiting unreasonable effectiveness of data in deep learning\nera. In Proceedings of the IEEE International Conference\non Computer Vision (ICCV), 843â€“852.\nTan, M.; and Le, Q. 2019. Efficientnet: Rethinking model\nscaling for convolutional neural networks. In International\nConference on Machine Learning (ICML), 6105â€“6114.\nTang, S.; Zhang, J.; Zhu, S.; and Tan, P. 2022a. Quadtree\nAttention for Vision Transformers. In International Confer-\nence on Learning Representations (ICLR).\nTang, Y .; Han, K.; Wang, Y .; Xu, C.; Guo, J.; Xu, C.; and\nTao, D. 2022b. Patch slimming for efficient vision trans-\nformers. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 12165â€“\n12174.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and JÂ´egou, H. 2021a. Training data-efficient image trans-\nformers & distillation through attention. In International\nConference on Machine Learning (ICML), 10347â€“10357.\nTouvron, H.; Cord, M.; Sablayrolles, A.; Synnaeve, G.; and\nJÂ´egou, H. 2021b. Going deeper with image transformers. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 32â€“42.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Å.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in Neural Information\nProcessing Systems (NeurIPS).\nWang, W.; Stuijk, S.; and De Haan, G. 2014. Exploiting\nspatial redundancy of image sensor for motion robust rPPG.\nIEEE Transactions on Biomedical Engineering (TBE), 415â€“\n425.\n7051\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021a. Pyramid vision trans-\nformer: A versatile backbone for dense prediction without\nconvolutions. InProceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), 568â€“578.\nWang, Y .; Chen, Z.; Jiang, H.; Song, S.; Han, Y .; and Huang,\nG. 2021b. Adaptive focus for efficient video recognition. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 16249â€“16258.\nWang, Y .; Huang, R.; Song, S.; Huang, Z.; and Huang, G.\n2021c. Not all images are worth 16x16 words: Dynamic\ntransformers for efficient image recognition. In Advances in\nNeural Information Processing Systems (NeurIPS).\nWang, Y .; Lv, K.; Huang, R.; Song, S.; Yang, L.; and Huang,\nG. 2020. Glance and focus: a dynamic approach to reducing\nspatial redundancy in image classification. In Advances in\nNeural Information Processing Systems (NeurIPS), 2432â€“\n2444.\nWang, Y .; Yue, Y .; Lin, Y .; Jiang, H.; Lai, Z.; Kulikov, V .;\nOrlov, N.; Shi, H.; and Huang, G. 2022a. Adafocus v2:\nEnd-to-end training of spatial dynamic networks for video\nrecognition. In IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 20030â€“20040.\nWang, Y .; Yue, Y .; Xu, X.; Hassani, A.; Kulikov, V .; Orlov,\nN.; Song, S.; Shi, H.; and Huang, G. 2022b. AdaFocusV3:\nOn Unified Spatial-Temporal Dynamic Video Recognition.\nIn European Conference on Computer Vision (ECCV), 226â€“\n243.\nXie, E.; Wang, W.; Yu, Z.; Anandkumar, A.; Alvarez, J. M.;\nand Luo, P. 2021. SegFormer: Simple and efficient design\nfor semantic segmentation with transformers. In Advances\nin Neural Information Processing Systems (NeurIPS).\nXu, W.; Xu, Y .; Chang, T.; and Tu, Z. 2021. Co-scale\nconv-attentional image transformers. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV), 9981â€“9990.\nXu, Y .; Zhang, Z.; Zhang, M.; Sheng, K.; Li, K.; Dong, W.;\nZhang, L.; Xu, C.; and Sun, X. 2022. Evo-ViT: Slow-Fast\nToken Evolution for Dynamic Vision Transformer. In Pro-\nceedings of the AAAI Conference on Artificial Intelligence\n(AAAI).\nYang, L.; Han, Y .; Chen, X.; Song, S.; Dai, J.; and Huang, G.\n2020. Resolution adaptive networks for efficient inference.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2369â€“2378.\nYu, Q.; Xia, Y .; Bai, Y .; Lu, Y .; Yuille, A. L.; and Shen, W.\n2021. Glance-and-gaze vision transformer. In Advances in\nNeural Information Processing Systems (NeurIPS).\nYuan, K.; Guo, S.; Liu, Z.; Zhou, A.; Yu, F.; and Wu, W.\n2021a. Incorporating convolution designs into visual trans-\nformers. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), 579â€“588.\nYuan, L.; Chen, Y .; Wang, T.; Yu, W.; Shi, Y .; Jiang, Z.-H.;\nTay, F. E.; Feng, J.; and Yan, S. 2021b. Tokens-to-token vit:\nTraining vision transformers from scratch on imagenet. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 558â€“567.\nZheng, S.; Lu, J.; Zhao, H.; Zhu, X.; Luo, Z.; Wang, Y .; Fu,\nY .; Feng, J.; Xiang, T.; Torr, P. H.; et al. 2021. Rethinking se-\nmantic segmentation from a sequence-to-sequence perspec-\ntive with transformers. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 6881â€“6890.\nZong, Z.; Li, K.; Song, G.; Wang, Y .; Qiao, Y .; Leng, B.; and\nLiu, Y . 2022. Self-slimmed vision transformer. InEuropean\nConference on Computer Vision (ECCV), 432â€“448.\n7052"
}