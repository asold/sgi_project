{
  "title": "From Universal Language Model to Downstream Task: Improving RoBERTa-Based Vietnamese Hate Speech Detection",
  "url": "https://openalex.org/W3114453204",
  "year": 2020,
  "authors": [
    {
      "id": null,
      "name": "Pham, Quang Huu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2125317246",
      "name": "Nguyen Viet Anh",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4299689499",
      "name": "Doan, Linh Bao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4310281107",
      "name": "Tran, Ngoc N.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2992885476",
      "name": "Thanh Ta Minh",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2806872289",
    "https://openalex.org/W6780317240",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2247662608",
    "https://openalex.org/W1997326096",
    "https://openalex.org/W6713042285",
    "https://openalex.org/W6608482188",
    "https://openalex.org/W2047449974",
    "https://openalex.org/W2473555522",
    "https://openalex.org/W2604821579",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W1071251684",
    "https://openalex.org/W2741065173",
    "https://openalex.org/W6864772185",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2311430799",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W6684107795",
    "https://openalex.org/W3042012685",
    "https://openalex.org/W2160685721",
    "https://openalex.org/W2756354991",
    "https://openalex.org/W1492737170",
    "https://openalex.org/W2613977835",
    "https://openalex.org/W3007955273",
    "https://openalex.org/W2973806433",
    "https://openalex.org/W6636510571",
    "https://openalex.org/W2564933006",
    "https://openalex.org/W6747248625",
    "https://openalex.org/W2340954483",
    "https://openalex.org/W2181854537",
    "https://openalex.org/W4300847054",
    "https://openalex.org/W3098637735",
    "https://openalex.org/W2948210185",
    "https://openalex.org/W2044173330",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3103061166",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4394658982",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W205930466",
    "https://openalex.org/W2399587463",
    "https://openalex.org/W4298857067",
    "https://openalex.org/W2045343360",
    "https://openalex.org/W4294367149",
    "https://openalex.org/W2164628858"
  ],
  "abstract": "Natural language processing is a fast-growing field of artificial intelligence. Since the Transformer was introduced by Google in 2017, a large number of language models such as BERT, GPT, and ELMo have been inspired by this architecture. These models were trained on huge datasets and achieved state-of-the-art results on natural language understanding. However, fine-tuning a pre-trained language model on much smaller datasets for downstream tasks requires a carefully-designed pipeline to mitigate problems of the datasets such as lack of training data and imbalanced data. In this paper, we propose a pipeline to adapt the general-purpose RoBERTa language model to a specific text classification task: Vietnamese Hate Speech Detection. We first tune the PhoBERT on our dataset by re-training the model on the Masked Language Model task; then, we employ its encoder for text classification. In order to preserve pre-trained weights while learning new feature representations, we further utilize different training techniques: layer freezing, block-wise learning rate, and label smoothing. Our experiments proved that our proposed pipeline boosts the performance significantly, achieving a new state-of-the-art on Vietnamese Hate Speech Detection campaign with 0.7221 F1 score.",
  "full_text": "From Universal Language Model to Downstream\nTask: Improving RoBERTa-Based Vietnamese Hate\nSpeech Detection\nQuang Huu Pham ∗, Viet Anh Nguyen ∗, Linh Bao Doan, Ngoc N. Tran\nR&D Lab, Sun Asterisk Inc\n{pham.huu.quang, nguyen.viet.anhd, doan.bao.linh, tran.ngo.quang.ngoc}@sun-asterisk.com\nTa Minh Thanh †\nLe Quy Don Technical University, 236 Hoang Quoc Viet, Bac Tu Liem, Ha Noi\nthanhtm@mta.edu.vn\n∗Equal contribution †Corresponding author\nTóm tắt nội dung—Natural language processing (NLP) is a fast-\ngrowing field of artificial intelligence. Since the Transformer [32]\nwas introduced by Google in 2017, a large number of language\nmodels such as BERT, GPT, and ELMo have been inspired by\nthis architecture. These models were trained on huge datasets\nand achieved state-of-the-art results on natural language under-\nstanding. However, fine-tuning a pre-trained language model on\nmuch smaller datasets for downstream tasks requires a carefully-\ndesigned pipeline to mitigate problems of the datasets such as lack\nof training data and imbalanced data. In this paper, we propose a\npipeline to adapt the general-purpose RoBERTa language model\nto a specific text classification task: Vietnamese Hate Speech\nDetection. We first tune the PhoBERT 1[9] on our dataset by re-\ntraining the model on the Masked Language Model (MLM) task;\nthen, we employ its encoder for text classification. In order to\npreserve pre-trained weights while learning new feature repre-\nsentations, we further utilize different training techniques: layer\nfreezing, block-wise learning rate, and label smoothing. Our exper-\niments proved that our proposed pipeline boosts the performance\nsignificantly, achieving a new state-of-the-art on Vietnamese Hate\nSpeech Detection (HSD) campaign 2 with 0.7221 F1 score.\nIndex Terms—Hate Speech Detection (HSD), RoBERTa, Text\nClassification, Natural Language Processing, Text Mining.\nI. I NTRODUCTION\nA. Overview\nThe rapid growth of the Internet, social media, and commu-\nnity forums have allowed people across the world to connect\ninstantaneously and has revolutionized communication as well\nas content issues. However, the increase of hate speech on\nthese platforms has drawn significant expenditure from gov-\nernments, organizations, companies, and researchers. The term\n“hate speech” can be understood as any kind of communication\nthat uses pejorative or discriminatory language with reference\nto a person or a group based on their religion, gender, ethnicity,\n1PhoBERT is a pre-trained RoBERTa model which is known as a state-\nof-the-art language model for Vietnamese provided by VinAI research:\nhttps://github.com/VinAIResearch/PhoBERT\n2https://vlsp.org.vn/vlsp2019/eval/hsd\nnationality, race, colour, descent, or other identity factors. Mul-\ntiple statistics reports [20] and books [28] also show that hate\nspeech and crime are highly correlated and on the rise together.\nSince the internet gives people some degree of anonymity, some\ntake this for granted and abuse it to harass others. Calling\nnames, making distasteful comments about one’s origin, or\nsimply shaming someone in anyway, are all everyday examples\nof hate speech that anyone will encounter occasionally. To\ncombat this, a vast number of methods have been studied and\ndeveloped for automated HSD. This aims to classify textual\ncontent into hate and non-hate speech.\nBy the end of 2019, social network site users in Vietnam\nhave reached 48 million users. Still, there has been limited\navailable research about Vietnamese HSD; building appropriate\ncountermeasures for hate speech requires detecting and trac-\ning through content. In the case of the Vietnamese language,\nthis task becomes difficult due to the diverse vocabulary and\ncomplex grammar. For example, the same subword can have\nmultiple meanings, which is different from Latin word roots.\nOr, the fact that the same base (sub)word having multiple\npossible intonations creates many hindrances in the path of lan-\nguage understanding: not only each subword has multiple vastly\ndifferent meanings, but also this causes an infinite number of\ncombinatorial possibilities of either misspelling or intentional\nshorthand expressions.\nThe variety of semantics and grammar in the Vietnamese\nlanguage has led to a big challenge for automatic hate speech\ndetection. Previous researches of text classification based on\ntraditional machine learning algorithms or training deep learn-\ning from scratch is often inefficient. It also requires a lot of ef-\nfort for pre-processing and assimilating the semantic of words.\nIn addition, recently proposed pre-trained language models\nhave accomplished success in multiple tasks of natural language\nprocessing through fine-tuning when integrated with the model\nof downstream tasks. The emergence of pre-trained language\nmodels has contributed to new ideas for solving significant\nproblems. Pre-trained language models are large scale neural\narXiv:2102.12162v1  [cs.CL]  24 Feb 2021\nnetwork models based on the deep Transformer structure. Their\ninitial parameters are learned through immense self-supervised\ntraining, then combined with multiple downstream models to\nfix special tasks by fine-tuning. Empirical experiment’s results\nshow that the downstream tasks’ performances of these kinds of\nmodels are usually better than those of conventional models.\nB. Our contributions\nIn this paper, we investigate many experiments in fine-tuning\nthe pre-trained RoBERTa model for text classification tasks,\nspecifically Vietnamese HSD. We propose a general pipeline\nand model architectures to adapt the universal language model\nas RoBERTa for downstream tasks such as text classification.\nWith our technique, we achieve new state-of-the-art results on\nthe Vietnamese Hate Speech campaign, organized by VLSP\n20193.\nThe main contributions of our paper are as follows:\n• We propose a general pipeline to adopt the universal\nlanguage model as a pre-trained RoBERTa for the text clas-\nsification. It includes two steps: (1) Re-training masked\nlanguage model task on training data of the classification\ntask. (2) Fine-tuning model with a new classification head\nfor the target task.\n• We conduct multiple methods to design model architec-\nture for text categorization task by using the pre-trained\nRoBERTa model such as PhoBERT[9].\n• A number of training techniques are suggested that can\nimprove the efficiency of the fine-tuning phase in solving\ndata problems. These techniques are practical and can em-\npirically help the model prevent overfitting phenomenon\nin the absence of training data or data imbalances.\n• From PhoBERT to Vietnamese HSD: We have achieved\nthe new state-of-the-art results on Vietnamese HSD task\nby utilizing PhoBERT and our proposed method.\nC. Roadmap\nThe rest of the paper is organized as follows. Section 2\nprovides a brief survey of related work. Next, in Section 3, we\nintroduce our proposed method, the model architecture, and our\nconducted fine-tuning strategies. Experiments are described\nin Section 4, including dataset information, data processing\nmethod, and some other experimental settings. Section 5 shows\nour experimental results. Finally, Section 6 presents our conclu-\nsions.\nII. R ELATED WORK\nA. Language Models\nLanguage modeling has been becoming an essential part\nof modern NLP field. It helps computers understand human\nlanguage by digitalizing qualitative information. Early studies\non word embeddings try to construct static representations for\nwords, that is, context-independent embeddings. Mikolov et\nal. [22] introduced novel architectures with an open-source\n3The sixth international workshop on Vietnamese Language and Speech\nProcessing.\npackage called Word2Vec. The architectures consist of two\nmodels: Continuous Bag of Words (CBOW) and Skip-gram\nmodel were trained on a huge dataset of 1.6 billion words.\nGloVe [27] is an unsupervised learning algorithm for context-\nindependent word embedding extraction. It first creates co-\noccurrence matrix of words and then factorizes it to extract\ndense word vectors. However, GloVe and Word2Vec are both\nfail in representing rare or out-of-vocabulary words. To mitigate\nthis problem, fastText [23] decomposes each word as a sum of\ncharacter n-grams. This handles unseen words very well be-\ncause their character n-grams still occur in other words. In con-\ntrast to context-independent embeddings, contextualized word\nembeddings aim to encode word semantics within contexts.\nMarking a new era of NLP, Bidirectional Encoder Represen-\ntations from Transformers [11] or BERT achieves new state-of-\nthe-art performance on eleven NLP tasks, outperforms previous\nbest result (with GLUE score of 80.4%, 7.6% improvement)\nand human on SQuAD 1.1 (with 93.2% accuracy, 2% higher).\nThe large model consists of 24 Transformer blocks for a total of\n340M parameters and was trained on 3.3 billion words corpus.\nGPT-2 [1] by OpenAI is even a larger model with 1.5 billion\nparameters and 48 layers. Being trained on a huge, diverse and\nwell-processed dataset, GPT-2 achieves state-of-the-art results\non 7 out of 8 datasets. Facebook research team propose an\nimproved training procedure for BERT, called RoBERTa [17].\nThe improvements include a ten-times larger dataset, longer\ntraining, increased batch size, using byte-level encoding with\nbigger vocabulary, excluding next sentence predicting task and\ndynamic masking pattern changing. For Vietnamese language\nmodeling, PhoBERT [9] is the current state-of-the-art. The\nmodel is based on RoBERTa with two versions “base” and\n“large”. Both versions outperform previous best on different\ndownstream tasks.\nB. Hate Speech Detection (HSD)\nHSD can be understood as a type of text classification. Before\nthe deep learning era, traditional methods had been widely\nstudied for this notorious problem. These approaches require\nmanual feature engineering to encode text sequences in vector\nform, which then be fed into classifiers such as Random Forests\n[35], Naive Bayes [5], [21], and Support Vector Machine [3],\n[21], [35]. Bag-of-words (BoW)is a way to represent documents\nby modeling the occurrence of every word. This has been\nreported to be a discriminative feature for HSD [3], [4], [5],\n[14], [30], [31], [34], [36]. However, modeling words suffers\nfrom the problem of typing error, which generates noisy words\nto the corpus. Character n-gramdivides a word into sub-words\nin order to suppress typing error parts of the word. In [21],\nthe authors have systematically proved that character-level n-\ngram representation outperforms BoW in abusive language\ndetection. Based on the idea that hate speech usually comes\nalong with negative sentiment, the methods in [16], [25], [30]\nutilize sentiment analysis as an evidence for HSD. Linguistic\nfeatures inject additional useful knowledge to the raw text. Xu et\nal. [36] combine Stanford CoreNLP [19] POS information with\nn-gram features. However, the POS tokens did not improve the\nperformance of the classifiers. In contrast, [3], [4], [5], [25],\n[26] successfully employed dependency relationships in their\nfeature set and report significant performance improvements.\nBecause hate speech usually comes from social media, it is\nfeasible to extract meta-information such as how likely a user\nwill post hateful speech in the future [35], the number of\nprofanity in a user’s activity history[7] or user gender [6], [34].\nRecent studies are paying great attention to deep learning\napproaches. It not only boosts performance considerably, but\nalso requires less effort on feature engineering. The input of\na deep learning model can simply be an one-hot encoding of\ntext sequences [2], [13], [38], [10], then meaningful features are\nlearned by Convolutional Neural Networks (CNN) [13], Long\nShort-Term Memory (LSTM) [2], [10] or even a combination of\nCNN and LSTM [38]. However, one-hot representation suffers\nfrom issues of high dimensionality as its length equals to\nthe vocabulary size. Therefore, a more convenient way is to\nembed the input into a low-dimensional space. This can be\ncharacter embeddings [21], comment embeddings [12] or text\nembeddings from a two-phase deep learning model [37].\nHSD Shared Task in VLSP Campaign 2019 [33] is a chal-\nlenge for detecting Vietnamese hate speech on social networks.\nOur team, the winning team, using logistic regression with\nensemble learning n-gram, achieved F1 score of 0.67756 and\n0.61971 on public-test and private-test, respectively. The sec-\nond best team also utilized logistic regression with ensem-\nble learning, however the input includes more feature types:\nn−grams of words, part-of-speech tags, and numeric features.\nThey scored 0.58883 on private-test, that is 3% lower than the\nwinner. The other teams employed deep learning architectures\nfrom CNN, LSTM, RNN to Bi-LSTM, LSTMCNN and Bi-\nGRU-LSTM-CNN, achieving scores from 0.51705 to 0.58455\non private-test.\nIII. P ROPOSED METHOD\nA. RoBERTa-based Network for HSD Task\nOur architecture utilizes PhoBERT as a backbone network\nwith some modifications. The output of each Transformer block\nrepresents a different semantic level for the inputs. In our\nexperiments, we use different combinations of outputs of those\nTransformers blocks. The general architecture model is shown\nin Figure 1.\nThe features are combined across outputs of multiple trans-\nformer blocks by concatenating or adding are fed to the classi-\nfication head. A simple classification head is a fully connected\nnetwork without hidden layers.\nB. Proposed Fine-Tuning Strategies\n1) Fine-Tuning Masked Language Model:The pre-trained\nmodel was fit to a much larger dataset of a completely different\ndomain. Therefore, while it might work very well in general, the\nvanilla pre-trained model will underperform at our one specific\ntask. This induces the need for us to tune the model to our needs.\nTherefore, with the existing weights of PhoBERT as a starting\npoint, we train our model with our domain-specific training data\non Masked Language Modeling (MLM) task.\nHình 1. An illustration of our architecture. The input is a tokenized sentence,\nfeeding parallelly to RoBERTa-base. Each of the twelve transformer blocks of\nthe RoBERTa produces a 768-D vector. These vectors are then concatenated to\nform a long vector for the classification head. We investigate on the performance\nof different combinations of these vectors.\nHình 2. The proposed fine-tuning strategies for Hate Speech detection.\nMoreover, for such a large model to be trained successfully\nwithout either forgetting its good initialization (by gradient\ndescending too far from it) or failing to converge (due to deep\nmodels being bad at propagating through further layers), we use\na warm-up learning rate scheduling scheme. Originating from\npaper [15] under the name of “slanted triangular learning rates\",\nHình 3. Warm-up learning rate\nthe main purpose of this method is to make the model converge\nmore quickly for a suitable region of the parameter space in the\nbeginning process of training.\n2) Optimization Strategy: Freeze or not Freeze:Each layer\nin RoBERTa network captures different levels of context.\nSpecifically, lower layers produce global embeddings for words\nwhile the embeddings from upper layers are more context-\nspecific. We would like to preserve the global knowledge while\nadjusting context representations for our classification model.\nFor the first few epochs, we only keep the fully connected\nlayers that are responsible for classifying text sequences, and\nthe RoBERTa part is frozen. This allows the model to learn a\ndecent decision for the task. After these epochs, we unfreeze\nall layers and set different learning rates for different layers:\nthe deeper the layer is, the more learning rate increased. This\nprevents the model from forgetting the useful global meaning\nof the words while forcing it to learn the context of the domain.\n3) Label Smoothing: For such a large model to be fit on a\nrelatively small set of data, the model tends to become overcon-\nfident of its performance, going to the dark side of overfitting.\nTo avoid this, we employ label smoothing, which softens our\none-hot ground truth labels. Specifically, for a model outputting\nprobabilities yk of K classes, instead of labelling our ground\ntruth with one-hot encoding:\nyk =\n{\n1 if k= jfor some j,\n0 otherwise, (1)\nwe slightly rebalance the target distribution so that it becomes\nless “binarized” by smoothing the probabilities with,\ny′\nk = yk(1 −α) +α/K (2)\nfor some smoothing parameter α. As a result, this technique\nteaches the model to have some uncertainty in its guesses, and\nreduces the severity of overfitting. Moreover, since we are fine-\ntuning on a pretrained model, the original output probability\nvector of the model is close to an one-hot. This introduces\nnumerical instabilities if the new true label is also one-hot,\nsince with cross-entropy as the loss function, the loss becomes\n1 log 0 =−∞. Thus, being used here, label smoothing acts\nas a small perturbation in our numerical method, making the\ntraining process more stable, helping our model converge better.\nIV. E XPERIMENTS\nA. Dataset\nTo perform experiments on hate speech detection task, we\nuse the HSD dataset from the VLSP workshop campaign in\n2019 [29]. The dataset includes 25,431 samples; all were\ncrawled from posts and comments on Facebook, and annotated\nto one of three classes: hate speech (HATE), offensive but not\nhate speech (OFFENSIVE), neither offensive nor hate speech\n(CLEAN) by many annotators. The training data consists of\n20,345 items with label distribution is showed in Table I.\nIn general, this is an imbalanced dataset and contains a lot\nof noisy data. There is an extremely enormous gap between\nthe number of CLEAN data points and those from the other\nBảng I\nTRAINING DATA DISTRIBUTION\nHATE OFFENSIVE CLEAN\nNumber of sample 709 1,022 18,614\ntwo categories, which lead to bad results for some methods,\nespecially Deep Learning approaches.\nBecause the dataset was crawled directly from users’ posts\nand comments on social networks, it has some notable prop-\nerties such as abbreviations, emoji, special characters, foreign\nlanguage, teen code, typing errors, etc. This has led to a sig-\nnificant challenge because PhoBERT or some other pre-trained\nlanguage model are often trained on normal clean data such as\nWikipedia data or Vietnamese news.\nB. Data preprocessing\nWe have built a basic preprocessing module to process the\nraw text before feeding it into the model. It includes Unicode\nnormalization, lowercase conversion, and the replacement of all\nemoji characters to the EMOJI label. Private personal informa-\ntion also is masked for privacy purposes such as phone numbers\nand email addresses. Finally, raw text is segmented by a word\nsegmenter. As PhoBERT employed the RDRSegmenter [8]\nfrom VnCoreNLP4 to pre-process training data of the language\nmodel task, it is recommended to use the same word segmenter\nfor PhoBERT-based downstream applications.\nC. Experimental Settings\n1) Data augmentation: To partially prevent data imbalance\nand less data for two HATE and OFFENSIVE labels, we used\nPhoBERT Masked Language Model as a method to augment\ndata. From the original text, we randomly selected one token,\nreplaced it with <mask>, and used the PhoBERT MLM to fill\nthe mask. Repeated 5 times, we got a brand new sample by fill\nup 5 tokens <mask>from the original text.\n2) Model Training: We fine-tuned PhoBERT with Masked\nLanguage Model on HSD dataset with ratio of 15% masked\ntokens. As in BERT default settings, batch size is fixed to\n32, with 10.000 steps. We chose Adam for the optimization\nand trained the model on a single GPU with learning rate of\n3 ×10−5. Figure 2 illustrates the overall pipeline. After tuning\nPhoBERT process completed, we retrain HSD model with one\nor combination of multiple output features from numerous\ntransformer blocks.\nThese features then would be incorporated into a\nClassification Head, which was designed straightforward\nwith Fully Connected and Dropout layers. AdamW [18] were\nselected to be the optimizer for BERT blocks and Classification\nHead with different learning rates. BERT learning rate was\n1 ×10−5 while classification head learning rate was larger\nat 1 ×10−4. Except bias and LayerNorm being excluded\nfrom weight decay, this ratio was chosen with value 0.01.\nNum_warmup_steps in experiments is equal to 1/8 of total\n4https://github.com/vncorenlp/VnCoreNLP\nsteps in 1 epoch.\nWe utilized Stratified K-fold with k = 10 , the ratio of\nsamples in train dataset and valid dataset were preserved as\nin the original dataset. Each and every fold was trained for 10\nepochs.\n3) Loss function: We use Label Smoothing loss which is\nthe combination of Cross-Entropy Loss and Label Smoothing.\nThe smoothing rate is set to 0.2 as this shows prominent\nafter the first few epochs. We slightly lower loss target values\nwhile increasing the target value 0. These so-called soft targets\nwill give lower loss when there is an incorrect prediction and\nconsequently, our model will penalize low entropy predictions\nor “confidence penalty” as mention in [24] and the section\nabove.\nThe cross entropy loss function is calculated as follows:\nL(x′,x) =−\n∑\ni\nx′\ni log xi, (3)\nwhere x′ is the true distribution of any particular data point\n(one-hot encoded, possibly with label smoothing applied), and\nx is the model’s predicted distribution. Specifically, for our\nprediction task with a dictionary of cpossible words to choose\nfrom, xi\n0≤i<c\nis the probability that the next word in the sequence\nis the i-th token in the dictionary. The loss for the whole dataset\n(or batch) is the sum (or mean) of the losses of individual data\npoints.\nD. System configuration\nOur experiments are conducted on a computer with Intel\nCore i7 9700K Turbo 4.9GHz, 32GB of RAM, GPU GeForce\nGTX 2080Ti, and 1TB SSD hard disk.\nV. E XPERIMENTAL RESULTS\nA. Evaluation metrics\nMacro F1-score is a common evaluation metrics for classifi-\ncation tasks. F1-score is the harmonic mean of Precision and\nRecall.\nF1 score: performance measure for classification\nF1 = 2\nRecall−1 + Precision−1 , (4)\nwhere Precision is the number of correct positive results\ndivided by the number of all positive results, and Recallis the\nnumber of correct positive results divided by the number of all\nsamples that should have been identified as positive.\nF1-macro or macro-averaged F1 score is computed as mean\nof F1 scores for each class\nMacroF 1 = F1HATE + F1OFFENSIVE + F1CLEAN\n3 (5)\nBảng II\nMEAN OF MACRO F1 SCORE ON STRATIFIED K-FOLD WITH K = 10 OF\nDIFFERENCE BLOCKS\nFeature blocks Mean of F1 score\nLayer 6 (only single block) 0.6854\nLayer 12 (only single block) 0.6978\nLayer 3-6 (4 middle blocks) 0.6855\nLayer 9-12 (4 last blocks) 0.6989\nLayer 1-6 (6 first blocks) 0.6905\nLayer 7-12 (6 last blocks) 0.6989\nLayer 1-12 (all blocks) 0.6979\nBảng III\nSOME SAMPLES THAT OUR SYSTEM GOOD AND FAILURE CLASSIFIED\nText Model prediction Truth label\n\"học kỳ cuối như đồ thị hình sóng thần\" CLEAN CLEAN\n\"lan anh đm ám ảnh vl\" OFFENSIVE OFFENSIVE\n\"nguyễn hoàng bách dm a mồm lol à\" HATE HATE\n\"hay vat đúng lắm ạ\" OFFENSIVE CLEAN\n\"có deo đâu mà xem vl\" CLEAN OFFENSIVE\n\"hường lily mặt ngu vl\" OFFENSIVE HATE\nB. Our results\nOur experiment results are shown in Table II and Table IV.\nTable II indicates the mean of Macro F1 score on using fea-\nture from different BERT blocks. We take the test on multiple\nselected blocks including single block (layer 6, 12), middle\nblocks (layer 3-6), last blocks (layer 6-12, 7-12, 9-12) and all\nblocks. Our retrieved result with last blocks by far is the top\nscore. Last 4 blocks (layer 9-12) and last 6 blocks (layer 7-12)\nboth achieve the highest F1 score at 0.6989.\nTable IV demonstrates effectiveness of each and every fine-\ntuning methods. Each individual technique boosts performance\nof the model by 0.5-1.5% in term of F1 score while the com-\nbination of these methods significantly enhances the score up\nto 0.7211, outperforming the winner of this challenge (0.67756\nF1 score) and the current best result on the public leaderboard\n(0.71432 F1 score with a single model). Some samples that our\nsystem good and failure classified are shown in Table III. It\nshows that our proposed method is efficient for task of HSD.\nVI. C ONCLUSION\nIn this paper, we have explored and proposed our pipeline to\nsolve the Vietnamese Hate Speech Detection task by using a\npre-trained universal language model. By intensively conduct-\ning experiments using PhoBERT, we have demonstrated that\nRoBERTa and our fine-tuning strategy is highly effective in\ntext classification tasks. With our proposed methods, we have\nachieved new state-of-the-art results on the Vietnamese HSD\ncampaign. For future work, we would like to explore different\nclassification head architectures. Instead of only using the fully\nconnected layer, we will experiment with other architectures for\ninstance LSTM, RNN, and CNN-LSTM on top.\nACKNOWLEDGMENT\nThis work is partially supported by Sun-Asterisk Inc. We\nwould like to thank our colleagues at Sun-Asterisk Inc for\nBảng IV\nMEAN OF MACRO F1 SCORE ON STRATIFIED K-FOLD WITH K = 10 WITH\nCONCATENATE OF LAYERS 6-12 AND OUR TRAINING APPROACH\nProposed training approach Mean of F1 score\nCross entropy loss 0.6922\nLabel Smoothing loss 0.7005\nNon warm-up learning rate 0.6989\nWarm-up learning rate 0.7062\nNon Fine-tune MLM 0.6989\nFine-tune MLM 0.7162\nNon Block wise learning rate 0.7051\nBlock wise learning rate 0.7079\nCombine all the methods 0.7211\ntheir advice and expertise. Without their support, this experi-\nment would not have been accomplished. We also express our\ngratitude to Tiep Vu, founder of AIVIVN 5 for supporting us in\nevaluating results on aivivn.com.\nTÀI LIỆU\n[1] R. Alec, J. Wu, C. Rewon, L. David, A. Dario, and S. Ilya. Language\nmodels are unsupervised multitask learners. 2019.\n[2] Pinkesh Badjatiya, Shashank Gupta, Manish Gupta, and Vasudeva Varma.\nDeep learning for hate speech detection in tweets. CoRR, abs/1706.00188,\n2017.\n[3] Pete Burnap and Matthew Williams. Cyber hate speech on twitter: An\napplication of machine classification and statistical modeling for policy\nand decision making: Machine classification of cyber hate speech. Policy\n& Internet, 7, 04 2015.\n[4] Pete Burnap and Matthew L. Williams. Us and them: identifying cyber\nhate on twitter across multiple protected characteristics. Epj Data\nScience, 5, 2016.\n[5] Y. Chen, Y. Zhou, S. Zhu, and H. Xu. Detecting offensive language in\nsocial media to protect adolescent online safety. In 2012 International\nConference on Privacy, Security, Risk and Trust and 2012 International\nConfernece on Social Computing, pages 71–80, 2012.\n[6] M. Dadvar, F. Jong, R. Ordelman, and D. Trieschnigg. Improved cyber-\nbullying detection using gender information. 01 2012.\n[7] M. Dadvar, D. Trieschnigg, R. Ordelman, and F. Jong. Improving\ncyberbullying detection with user context. pages pp 693–696, 03 2013.\n[8] N. Q. Dat, N. Q. Dai, V. Thanh, M. Dras, and M. Johnson. A Fast\nand Accurate Vietnamese Word Segmenter. In Proceedings of the 11th\nInternational Conference on Language Resources and Evaluation (LREC\n2018), pages 2582–2587, 2018.\n[9] N. Q. Dat and N. A. Tuan. Phobert: Pre-trained language models for\nvietnamese, 2020.\n[10] Fabio Del Vigna, Andrea Cimino, Felice Dell’Orletta, Marinella Petroc-\nchi, and Maurizio Tesconi. Hate me, hate me not: Hate speech detection\non facebook. 01 2017.\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBert: Pre-training of deep bidirectional transformers for language under-\nstanding, 2018.\n[12] Nemanja Djuric, Jing Zhou, Robin Morris, Mihajlo Grbovic, Vladan\nRadosavljevic, and Narayan Bhamidipati. Hate speech detection with\ncomment embeddings. In Proceedings of the 24th International Con-\nference on World Wide Web, WWW ’15 Companion, page 29–30, New\nYork, NY, USA, 2015. Association for Computing Machinery.\n[13] Bj ¨orn Gamb ¨ack and Utpal Kumar Sikdar. Using convolutional neural\nnetworks to classify hate-speech. In Proceedings of the First Workshop on\nAbusive Language Online, pages 85–90, Vancouver, BC, Canada, August\n2017. Association for Computational Linguistics.\n[14] Homa Hosseinmardi, Sabrina Arredondo Mattson, Rahat Ibn Rafiq,\nRichard Han, Qin Lv, and Shivakant Mishra. Detection of cyberbullying\nincidents on the instagram social network, 2015.\n[15] Jeremy Howard and Sebastian Ruder. Universal language model fine-\ntuning for text classification, 2018.\n5https://www.aivivn.com/\n[16] D. Karthik, J. Birago, H. Catherine, L. Henry, and P. Rosalind. Common\nsense reasoning for detection, prevention, and mitigation of cyberbully-\ning. ACM Trans. Interact. Intell. Syst., 2(3), September 2012.\n[17] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi\nChen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.\nRoberta: A robustly optimized bert pretraining approach, 2019.\n[18] I. Loshchilov and F. Hutter. Decoupled weight decay regularization, 2017.\n[19] Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven\nBethard, and David McClosky. The Stanford CoreNLP natural language\nprocessing toolkit. In Proc. of 52nd Annual Meeting of the Association\nfor Computational Linguistics: System Demonstrations, pages 55–60.\nAssociation for Computational Linguistics, June 2014.\n[20] W. L. Matthew, B. Pete, J. Amir, L. Han, and O. Sefa. Corrigendum to:\nHate in the Machine: Anti-Black and Anti-Muslim Social Media Posts as\nPredictors of Offline Racially and Religiously Aggravated Crime. The\nBritish Journal of Criminology, 60(1):242–242, 2019.\n[21] Yashar Mehdad and Joel Tetreault. Do characters abuse more than words?\npages 299–303, 01 2016.\n[22] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient\nestimation of word representations in vector space, 2013.\n[23] Tomas Mikolov, Edouard Grave, Piotr Bojanowski, Christian Puhrsch,\nand Armand Joulin. Advances in pre-training distributed word repre-\nsentations. In Proceedings of the International Conference on Language\nResources and Evaluation (LREC 2018), 2018.\n[24] Rafael M ¨uller, Simon Kornblith, and Geoffrey Hinton. When does label\nsmoothing help?, 2019.\n[25] Dennis Njagi, Z. Zuping, Damien Hanyurwimfura, and Jun Long. A\nlexicon-based approach for hate speech detection. International Journal\nof Multimedia and Ubiquitous Engineering, 10:215–230, 04 2015.\n[26] Chikashi Nobata, Joel Tetreault, Achint Thomas, Yashar Mehdad, and\nYi Chang. Abusive language detection in online user content. In\nProceedings of the 25th International Conference on World Wide Web,\nWWW ’16, page 145–153, Republic and Canton of Geneva, CHE, 2016.\nInternational World Wide Web Conferences Steering Committee.\n[27] Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe:\nGlobal vectors for word representation. In Proc. of the 2014 Conference\non Empirical Methods in Natural Language Processing (EMNLP), pages\n1532–1543, Doha, Qatar, October 2014. Association for Computational\nLinguistics.\n[28] Timothy C Shiell. Campus hate speech on trial. Univ Pr of Kansas, 2009.\n[29] V. X. Son, V. Thanh, T. M. Vu, L. C. Thanh, and N. T. M. Huyen. Hsd\nshared task in vlsp campaign 2019: Hate speech detection for social good.\nProceedings of VLSP 2019, 2019.\n[30] Sara Owsley Sood, Elizabeth F. Churchill, and Judd Antin. Automatic\nidentification of personal insults on social news sites. J. Am. Soc. Inf. Sci.\nTechnol., 63(2):270–285, February 2012.\n[31] Cynthia Van Hee, Els Lefever, Ben Verhoeven, Julie Mennes, Bart\nDesmet, Guy Pauw, Walter Daelemans, and Veronique Hoste. Detection\nand fine-grained classification of cyberbullying events. 09 2015.\n[32] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin. Attention is all you need, 2017.\n[33] Xuan-Son Vu, Thanh Vu, Mai-Vu Tran, Thanh Le-Cong, and Huyen T M.\nNguyen. Hsd shared task in vlsp campaign 2019:hate speech detection for\nsocial good, 2020.\n[34] Zeerak Waseem and Dirk Hovy. Hateful symbols or hateful people?\npredictive features for hate speech detection on twitter. In Proceedings\nof the NAACL Student Research Workshop, pages 88–93, San Diego,\nCalifornia, June 2016. Association for Computational Linguistics.\n[35] Guang Xiang, Bin Fan, Ling Wang, Jason Hong, and Carolyn Rose.\nDetecting offensive tweets via topical feature discovery over a large scale\ntwitter corpus. pages 1980–1984, 10 2012.\n[36] J. M. Xu, K. S. Jun, X. Zhu, and A. Bellmore. Learning from bullying\ntraces in social media. In Proceedings of the 2012 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL HLT ’12, page 656–666, USA,\n2012. Association for Computational Linguistics.\n[37] Shuhan Yuan, Xintao Wu, and Yang Xiang. A two phase deep learning\nmodel for identifying discrimination from tweets. In EDBT, 2016.\n[38] Ziqi Zhang, D. Robinson, and Jonathan Tepper. Detecting hate speech on\ntwitter using a convolution-gru based deep neural network. 03 2018.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7767183780670166
    },
    {
      "name": "Natural language processing",
      "score": 0.6545417904853821
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6494309902191162
    },
    {
      "name": "Pipeline (software)",
      "score": 0.6230219006538391
    },
    {
      "name": "Language model",
      "score": 0.5508454442024231
    },
    {
      "name": "Vietnamese",
      "score": 0.5481489300727844
    },
    {
      "name": "Task (project management)",
      "score": 0.49958348274230957
    },
    {
      "name": "Transformer",
      "score": 0.4976244270801544
    },
    {
      "name": "Smoothing",
      "score": 0.49342337250709534
    },
    {
      "name": "Speech recognition",
      "score": 0.4428226351737976
    },
    {
      "name": "Natural language",
      "score": 0.42973482608795166
    },
    {
      "name": "Machine learning",
      "score": 0.4200073182582855
    },
    {
      "name": "Linguistics",
      "score": 0.07526925206184387
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Computer vision",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 13
}