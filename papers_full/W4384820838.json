{
    "title": "Force field-inspired transformer network assisted crystal density prediction for energetic materials",
    "url": "https://openalex.org/W4384820838",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5036249443",
            "name": "Jun-Xuan Jin",
            "affiliations": [
                "Zhejiang University",
                "Quzhou University"
            ]
        },
        {
            "id": "https://openalex.org/A4209442091",
            "name": "Gao‐Peng Ren",
            "affiliations": [
                "Quzhou University",
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2129022109",
            "name": "Jian-jian Hu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2149306358",
            "name": "Yingzhe Liu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2556082912",
            "name": "Gao Yunhu",
            "affiliations": [
                "University of Cambridge"
            ]
        },
        {
            "id": "https://openalex.org/A4224734044",
            "name": "Ke-Jun Wu",
            "affiliations": [
                "Quzhou University",
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2124421634",
            "name": "Yuchen He",
            "affiliations": [
                "State Key Laboratory of Industrial Control Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5036249443",
            "name": "Jun-Xuan Jin",
            "affiliations": [
                "Zhejiang University",
                "Quzhou University"
            ]
        },
        {
            "id": "https://openalex.org/A4209442091",
            "name": "Gao‐Peng Ren",
            "affiliations": [
                "Zhejiang University",
                "Quzhou University"
            ]
        },
        {
            "id": "https://openalex.org/A2129022109",
            "name": "Jian-jian Hu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2149306358",
            "name": "Yingzhe Liu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2556082912",
            "name": "Gao Yunhu",
            "affiliations": [
                "University of Cambridge"
            ]
        },
        {
            "id": "https://openalex.org/A4224734044",
            "name": "Ke-Jun Wu",
            "affiliations": [
                "Quzhou University",
                "Zhejiang University"
            ]
        },
        {
            "id": "https://openalex.org/A2124421634",
            "name": "Yuchen He",
            "affiliations": [
                "State Key Laboratory of Industrial Control Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2747592475",
        "https://openalex.org/W2999044305",
        "https://openalex.org/W2988203096",
        "https://openalex.org/W3036441178",
        "https://openalex.org/W4285394863",
        "https://openalex.org/W3112474878",
        "https://openalex.org/W3214379431",
        "https://openalex.org/W3184969049",
        "https://openalex.org/W2742835787",
        "https://openalex.org/W4214839687",
        "https://openalex.org/W1986982278",
        "https://openalex.org/W2026444402",
        "https://openalex.org/W2153671025",
        "https://openalex.org/W2007436490",
        "https://openalex.org/W2041392558",
        "https://openalex.org/W1988195734",
        "https://openalex.org/W3044724994",
        "https://openalex.org/W2054904072",
        "https://openalex.org/W2315837940",
        "https://openalex.org/W2104489082",
        "https://openalex.org/W1531674615",
        "https://openalex.org/W3110901318",
        "https://openalex.org/W3125542198",
        "https://openalex.org/W2784006847",
        "https://openalex.org/W4306252592",
        "https://openalex.org/W1982134120",
        "https://openalex.org/W2141070669",
        "https://openalex.org/W2117925578",
        "https://openalex.org/W1974657245",
        "https://openalex.org/W2090615528",
        "https://openalex.org/W2027724103",
        "https://openalex.org/W2794050282",
        "https://openalex.org/W3093008122",
        "https://openalex.org/W2206840988",
        "https://openalex.org/W2153693853",
        "https://openalex.org/W2114704115",
        "https://openalex.org/W3153808125",
        "https://openalex.org/W3159234754",
        "https://openalex.org/W2966357564",
        "https://openalex.org/W4319320230",
        "https://openalex.org/W2594183968",
        "https://openalex.org/W2319902168",
        "https://openalex.org/W2244785476",
        "https://openalex.org/W1967780388",
        "https://openalex.org/W2791772684",
        "https://openalex.org/W3215238431",
        "https://openalex.org/W3147983081",
        "https://openalex.org/W2128607364",
        "https://openalex.org/W3104255923",
        "https://openalex.org/W3159598019",
        "https://openalex.org/W4212774754"
    ],
    "abstract": "Abstract Machine learning has great potential in predicting chemical information with greater precision than traditional methods. Graph neural networks (GNNs) have become increasingly popular in recent years, as they can automatically learn the features of the molecule from the graph, significantly reducing the time needed to find and build molecular descriptors. However, the application of machine learning to energetic materials property prediction is still in the initial stage due to insufficient data. In this work, we first curated a dataset of 12,072 compounds containing CHON elements, which are traditionally regarded as main composition elements of energetic materials, from the Cambridge Structural Database, then we implemented a refinement to our force field-inspired neural network (FFiNet), through the adoption of a Transformer encoder, resulting in force field-inspired Transformer network (FFiTrNet). After the improvement, our model outperforms other machine learning-based and GNNs-based models and shows its powerful predictive capabilities especially for high-density materials. Our model also shows its capability in predicting the crystal density of potential energetic materials dataset (i.e. Huang &amp; Massa dataset), which will be helpful in practical high-throughput screening of energetic materials.",
    "full_text": "Jin et al. Journal of Cheminformatics           (2023) 15:65  \nhttps://doi.org/10.1186/s13321-023-00736-6\nRESEARCH Open Access\n© The Author(s) 2023. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nJournal of Cheminformatics\nForce field-inspired transformer network \nassisted crystal density prediction for energetic \nmaterials\nJun‑Xuan Jin1,2, Gao‑Peng Ren1,2, Jianjian Hu3, Yingzhe Liu3, Yunhu Gao4, Ke‑Jun Wu1,2* and Yuchen He5* \nAbstract \nMachine learning has great potential in predicting chemical information with greater precision than traditional \nmethods. Graph neural networks (GNNs) have become increasingly popular in recent years, as they can automatically \nlearn the features of the molecule from the graph, significantly reducing the time needed to find and build molecular \ndescriptors. However, the application of machine learning to energetic materials property prediction is still in the ini‑\ntial stage due to insufficient data. In this work, we first curated a dataset of 12,072 compounds containing CHON \nelements, which are traditionally regarded as main composition elements of energetic materials, from the Cam‑\nbridge Structural Database, then we implemented a refinement to our force field‑inspired neural network (FFiNet), \nthrough the adoption of a Transformer encoder, resulting in force field‑inspired Transformer network (FFiTrNet). After \nthe improvement, our model outperforms other machine learning‑based and GNNs‑based models and shows its \npowerful predictive capabilities especially for high‑density materials. Our model also shows its capability in predicting \nthe crystal density of potential energetic materials dataset (i.e. Huang & Massa dataset), which will be helpful in practi‑\ncal high‑throughput screening of energetic materials.\nKeywords Energetic materials, Machine learning, Graph neural networks, Crystal density prediction\nIntroduction\nMachine Learning (ML) is a data-driven method that \nhas gained widespread attention in various fields over \nthe last few decades and shows great potential to predict \nchemical information with greater precision than tradi -\ntional methods [1–5]. Supervised ML methods facilitate \nnumerous data to learn the pattern between the molecule \nand the certain property we need, which is often difficult \nto give a theoretical or empirical formula. This powerful \ntool makes it possible to perform high-throughput virtual \nscreening (HTVC), which will significantly accelerate the \nprocess of discovering new materials or new drugs [6–8].\nTraditionally, the search for novel materials involved \nconducting a series of time-consuming and labor-inten -\nsive experiments and theoretical studies, leading to a very \nlong period of material screening [9]. For example, the \ndevelopment of energetic materials often takes a decade \nor even more [10]. With the development of computer \nscience, computational simulation was applied to the \nprediction of molecular properties, such as molecular \ndynamics (MD) [11] and density functional theory (DFT) \n*Correspondence:\nKe‑Jun Wu\nK.Wu@zju.edu.cn\nYuchen He\nhyc19940615@zju.edu.cn\n1 Zhejiang Provincial Key Laboratory of Advanced Chemical Engineering \nManufacture Technology, College of Chemical and Biological Engineering, \nZhejiang University, Hangzhou 310027, China\n2 Institute of Zhejiang University‑Quzhou, Quzhou 324000, China\n3 Xi’an Modern Chemistry Research Institute, Xi’an 710065, China\n4 Department of Engineering, University of Cambridge, Cambridge CB2 \n1PZ, UK\n5 State Key Laboratory of Industrial Control Technology, College of Control \nScience and Engineering, Zhejiang University, Hangzhou 310027, China\nPage 2 of 13Jin et al. Journal of Cheminformatics           (2023) 15:65 \n[12]. These methods significantly reduce the experimen -\ntal time and costs of material selection by pre-screening \nmaterials based on in silico calculated properties, which \ncould quickly eliminate poor-performing materials with -\nout further experimentation. But these molecular simula-\ntion methods also have their problems. For example, they \nare often computationally consuming [13] and require \ncomputer clusters; Additionally, these methods need to \nrecalculate all data every time a new environment or tar -\nget molecule is introduced, even if it is similar to a pre -\nvious one, meaning that they cannot make use of prior \nknowledge [9]. To overcome these weaknesses, a variety \nof methods have been developed. One of the most pow -\nerful and popular is the ML method.\nOver the past decades, different ML approaches, such \nas support vector machine (SVM) [14, 15], random forest \n(RF) [16, 17], and artificial neural network (ANN) [18–\n20], have been broadly applied in predicting molecular \nproperties and have shown great applicability. These ML \nmethods all use the quantitative structure–property rela -\ntionships (QSPR) [21] which depends on a large number \nof molecular descriptors or fingerprints: Coulomb matri -\nces [22], bag of bonds [23], etc., to give a rather accurate \nprediction about the molecular property, while these \nmolecular descriptors are sometimes hard to obtain. \nHence, in order to circumvent the challenge of locating \nor creating these complex descriptors, it is imperative \nto identify a simple yet precise representation for the \nmolecules.\nIn the last few years, a new ML method called graph \nneural networks (GNNs) [24, 25] has gained more and \nmore attention and become increasingly popular. Since \nmolecules can be represented as graphs (the atoms as \nnodes and the bonds as edges), by aggregating and updat-\ning the features of all the atoms and bonds, GNNs can \nautomatically learn the features of the molecule from the \ngraph, which significantly reduces the time we find and \nbuild the molecular descriptors. However, despite the \nconvenience and promising expectations of the ML, the \napplication to energetic materials property prediction is \nstill at the initial stage, due to an insufficient amount of \ndata [26, 27].\nEnergetic materials represent a class of materials capa -\nble of releasing large amounts of chemical energy stored \ninside the molecular structure. Typical energetic mate -\nrials include explosives, propellants, fuels, pyrotechnic \ncompositions, etc. which are widely used not only in mili-\ntary applications but also in civil engineering and space \nexploration (e.g., mining and rocket propellant) [28]. \nCrystal density is an important property of energetic \nmaterials, which is highly related to other detonation \nperformance characteristics. e.g., the detonation pressure \nis approximately proportional to the square of the density \n[29]. One of the main criteria for the evaluation of prom -\nising energetic materials is ‘high’ density, which typically \nrefers to a density greater than 1.8 g/cm3 [30].\nTraditionally, group additivity [31, 32] and some empir-\nical methods [33, 34] were performed to predict the \ncrystal density of energetic materials. While in recent \nyears, ML-based methods have emerged as a promising \napproach for predicting crystal density with enhanced \naccuracy and reliability. Fathollahi et al. [35] conducted a \nstudy on 26 energetic cocrystals, in which they extracted \nthree molecular descriptors from the optimized chemical \nstructures. They predicted the densities of these cocrys -\ntals using an ANN with a test precision up to 0.9918. \nDespite the small amount of data, this still shows the \ngreat potential of ML-assisted methods in the crystal \ndensity prediction of energetic material. Casey et al. [36] \nraised a 3D convolution neural network (CNN) using \ncharge density and electrostatic potential as the repre -\nsented feature, which got a high accuracy prediction in \nthe dataset screening the possible energetic materials \nfrom the GDB database [37–39]. Yang et al. [40] noticed \nthe difficulty and cumbersomeness of extracting these \nmolecular descriptors and started to use GNNs to learn \nthese descriptors merely from its topology. They found \nthat GNNs-based model could achieve higher accuracy \nand lower computational resource with respect to other \ntraditional ML methods. Recently, more and more ML \nmethods have been used in the prediction of crystal den -\nsity. Then, Nguyen et  al. [41] use an improved GNNs \nmodel called Directed Message Passing Neural Networks \n(D-MPNNs), which is raised by Yang et  al. [42], which \nutilizes the directed graph, rather than the traditional \nundirected graph, to represent the molecule and update \nnodes and edges feature using message passing algo -\nrithm. This model outperforms other ordinary models, \nSVM, RF, and Partial Least Squares Regression (PLSR), \nachieving a more accurate result.\nFrom the example above, GNNs uses none of a priori \nknowledge, only the topology structure of each molecule, \nbut can achieve a higher predicting accuracy. Thus, it \ncould be the most promising method for predicting the \ncrystal density of energetic materials. A typical process of \ndensity prediction using GNNs is shown in Fig.  1. How-\never, most available GNNs model utilizes only 2D molec -\nular descriptors to present the molecule, leading to large \nbiases in describing the 3D caged molecules like CL-20 \nfamily molecules (e.g. Hexanitrohexaazaisowurtzitane) \nor cubane family molecules (e.g. Octanitrocubane).\nTherefore, in this work, the main aim is to further \nimprove the prediction accuracy of the density of 3D \nmolecules, and minimize errors, particularly in the high-\ndensity region of interest, i.e. the density higher than \n1.8 g/cm3. We adopt force field-inspired neural network \nPage 3 of 13\nJin et al. Journal of Cheminformatics           (2023) 15:65 \n \n(FFiNet) [43], a 3D-aware GNNs developed by our group \nusing force fields to calculate the energy of bonds, angles, \ndihedral, and non-bonded interaction as attention scores, \nwhich is able to give more accurate molecular informa -\ntion when dealing with 3D molecules having the spatial \ncaged structures. Moreover, the self-attention mecha -\nnism from Transformer is used to replace the axial atten-\ntion in original model, to give a global representation of \nthe force fields terms, showing a great accuracy improve \nin the crystal density prediction task. This modified \nmodel is called force field-inspired Transformer networks \n(FFiTrNet), and the detailed information of FFiTrNet will \nbe illustrated later.\nMethodologies\nData set\nOne of the biggest problems between ML and ener -\ngetic materials is the lack of sufficiently large datasets. \nIn order to get a prediction with high accuracy, a large, \ndiverse, and accurate dataset is needed so that the model \nlearns the pattern in the data set properly. In other pre -\ndiction tasks, the common size of other public datasets \nis over 1000, for example, BBBP (N = 2039), Lipophilicity \n(N = 4200), QM7 (N = 7160) in MoleculeNet [44]. This is \nmuch bigger than the current accessible energetic mate -\nrial datasets, making it hard to give a reasonable pre -\ndiction and may lead to overfitting in the given datasets \n[26, 27]. In order to generalize our prediction model, we \nneed to find a larger dataset containing molecules with \nthe same molecular frameworks and functional groups \nas the energetic materials. In this work, we curated the \ndataset from the Cambridge Structural Database (CSD) \n[45], which contains more than 1.1 M organic and metal–\norganic crystal structure data, allowing big-data search -\ning and screening. Moreover, each structure within the \nCSD undergoes extensive validation and cross-checking \nvia automated workflows and through manual curation \nby expert chemists and crystallographers. This guaran -\ntees data accuracy, consistency, and high quality within \nthe CSD, making the ML model more trustful.\nThe candidates are curated through the following cri -\nteria: (1) those only consist of carbon (C), hydrogen (H), \noxygen (O), and nitrogen (N) atoms; (2) those have at \nleast one of any carbon–nitrogen bonds; (3) those don’t \nbelong to ionic or co-crystal compounds; (4) those are \nable to make 3D conformation in further operation. By \ninitially screening the data using the steps above, we \nestablished a dataset with 12,072 compounds containing \nCHON elements with their Simplified Molecular-Input \nFig. 1 Flowchart for predictive screening process of energetic material density using GNNs\nTable 1 The atom features used in the  FFiTrNeta\na All features are one-hot encodings except for atomic mass\nFeature Description Size\nAtom type The type of the atom 38\nAtom degree The number of directly‑bonded neighbors 6\nChiral type The chiral type of the atom: unspecified, tetrahedral CW, tetrahedral CCW, or other 4\nHs number The total number of hydrogens attached to the atom 6\nHybridization The hybridization type of the atom: unspecified, s, sp, sp2d, sp3, sp3d, sp3d2, or other 8\nAromatic Whether an atom belongs to the aromatic ring 1\nAtomic mass The mass of the atom 1\nHydrogen bond Whether an atom accepts electrons or donates electrons 2\nPage 4 of 13Jin et al. Journal of Cheminformatics           (2023) 15:65 \nLine-Entry System (SMILES) strings and crystal density. \nThis will satisfy our need for a large enough dataset and \ncan be used to train our model efficiently.\nData preprocessing and featurization\nFirst, we grab the 3D conformation of the molecules \nthat contain atoms’ position information using RDkit \ntoolkit [46] from SMILES strings of data. RDkit toolkit \nis a widely used open-source machine-learning software \nproviding a collection of cheminformatics for descriptor \nand fingerprint generation, 2D and 3D molecular opera -\ntions, etc. The fast ETKDG method [47] from RDkit is \napplied to generate atom positions. After obtaining the \n3D molecular graph and its positional information, we \nextract atom features foreach atom in molecules, and the \natom feature was listed in Table 1.\nModel framework\nIn this work, we adapt FFiNet as the main framework of \nthe model to learn the feature from molecular topology. \nThe position information in the 3D conformer of each \nmolecule is fed into the model. This information is then \nused to calculate the distance, angle, and dihedral infor -\nmation from all the neighbors or 2-hop, and 3-hop neigh-\nbors of each atom. According to the traditional force field \ntheory [48], the potential energy could be written as:\nBy expanding the bond term of energy in empirical \nmodel:\nwhere K r , Kθ , Vφ,n(n = 1, 2, 3) , fij , ǫij are all force con -\nstants; fφ,n(n = 1, 2, 3) are dihedral phase; l , θ , φ and r \nrepresent the bond length, angle, dihedral angle, and the \n(1)E total = E bond + E angle + E tor + E non −bonded\n(2)Ebond =\n∑\nbonds\nK r\n(\nl− leq\n)2 = fbond\n(\nl,l2\n)\n(3)Eangle =\n∑\nangles\nK θ\n(\nθ − θeq\n)2 = fangle\n(\nθ,θ2\n)\n(4)\nE tor =\n∑\ndihedrals\nV φ,1\n2\n[\n1 + cos\n(\nφ + fφ,1\n)]\n+ V φ,2\n2\n[\n1 − cos\n(\n2φ + fφ,2\n)]\n+ V φ,3\n2\n[\n1 + cos\n(\n3φ + fφ,3\n)]\n= ftor(cosφ,cos2φ ,cos3φ ,sinφ,sin2φ ,sin3φ )\n(5)\nE non−bonded =\n∑\ni\n∑\nj\n[\nqiqj\nrij\n+ 4ǫij\n(\nσ 12\nij\nr12\nij\n−\nσ 6\nij\nr6\nij\n)]\nfij = fnon−bonded\n(\nr−1 ,r−6 ,r−12\n)\nFig. 2 a Structure diagram of the FFiTrNet model. Transformer encoder is used to replace the axial attention in the origin FFiNet. b The detailed \nstructure of the Transformer encoder in FFiTrNet. A special output token is introduced as one of its inputs to aggregate all the information of three \nk‑hop outputs\nPage 5 of 13\nJin et al. Journal of Cheminformatics           (2023) 15:65 \n \ndistance between non-bonded atoms respectively; leq  , \nθeq , and σij are the value of bond, angle, or non-bonded \natomic distance when the corresponding energy term is \nconsidered zero as the reference value; q i and q j are the \natomic charges and f (·) is a general linear function. The \ndetailed information can be found in the Additional file  1 \nand our previous work [43].\nIn this model, we only calculate the non-bonded term \nfor the 2-hop and 3-hop neighbors of each atom and add \nit to the corresponding angle and torsion terms as a part \nof energy. Then a one-layer linear transformation is used \nas the linear function f (·) also as an embedding layer to \nfacilitate the next operation. These energy embedding \nterms are treated separately after attention operation and \nthen combined through axial attention to give an output \nembedding.\nBecause the axial attention treats its inputs separately, \nand simply sums up the output to further operation, \nthere’s no information interaction between different \nenergy terms. This doesn’t match the reality for each \nenergy term is highly affected by each other. So, we \nfurther improve the performance by introducing the \nencoder layer from Transformer to replace the axial \nattention, to help us mix these energy terms’ information \ntogether.\nTransformer was first introduced in 2017 by Vaswani \net al. [49] to solve natural language processing but quickly \nshow great potential in a wide field of ML. Its encoder \nlayer adopts the self-attention mechanism that allows the \nmodel to attend to different parts of the input sequence \nwhile processing each position. After using the k-hop \n(k = 1, 2, 3) attention to update the embedding feature, \nwe got 1-hop, 2-hop, and 3-hop outputs, stacked with a \nspecial output token as the learnable parameter, which \nis inspired by the same concept in vision Transform -\ners (ViTs) [50]. Then they are fed into the Transformer \nencoding layer to get the same amount of output repre -\nsentations of the same length, and one of the representa -\ntions corresponding to the special output token is picked \nas the final output to go through further operation. The \ndetailed structure of the FFiTrNet is shown in Fig.  2. In \nthere, positional encoder is considered optional because \nthere’s no obvious positional relation in these four out -\nputs, and one layer of Transformer encoding layer is good \nenough for this case.\nModel evaluation\nWe adopt three different metrics to evaluate the regres -\nsion model, like mean absolute error (MAE), root mean \nsquare error (RMSE), and coefficient of determination \n(R2):\nwhere ρtrue is the true density value of the sample, ρpred is \nthe predicted density value of the sample, ρ is the average \ndensity value of the true density.\nResults and discussion\nThere is a widely accepted standard for evaluating pre -\ndictions of crystal density: a prediction with an absolute \nerror less than 0.03  g/cm 3 is considered an “excellent” \nprediction; the absolute error between 0.03 and 0.05  g/\ncm3 is considered “informative”; the absolute error \nbetween 0.05 and 0.10  g/cm 3 is considered “barely use -\nful”; and the absolute error greater than 0.10  g/cm 3 is \nconsidered “deceptive” [51, 52]. In this work, as our \nimproved FFiTrNet model is based on the GNNs which \nonly uses the molecules’ topology, we mainly compare it \nto other GNNs like graph attention networks (GATv2) \nand D-MPNNs, which have been proven to be highly \naccurate in predicting the crystal density of energetic \nmaterials by Nguyen et al. [41]. Also, we adapt RF which \nuses molecular descriptors of QSPR as the input to show \nthat the GNNs-learnt descriptor could outperform the \nman-made molecule descriptors. Moreover, in order to \nvalidate the effects of Transformer encoder, the result of \nthe original FFiNet is also listed.\nOverall testing evaluation\nWe first use the dataset (N = 12,072) that was curated \nfrom CSD database, randomly splitting the data into \n(6)MAE = 1\nN\nN∑\ni=1\n|ρtrue\ni − ρpred\ni |\n(7)RMSE =\n√ 1\nN\nN∑\ni=1\n(\nρtrue\ni − ρpred\ni\n)2\n(8)R 2 = 1 −\n∑ N\ni=1\n(\nρtrue\ni − ρpred\ni\n)2\n∑ N\ni=1\n(\nρtrue\ni − ρ\n)2\nTable 2 The test MAE, RMSE and R2 for each model\nThe best results are marked in bold, and the second-best results are italicized\nModels MAE (g/cm3) RMSE (g/cm3) R2\nRF 0.0367 ± 0.0010 0.0514 ± 0.0019 0.8886 ± 0.0094\nGATv2 0.0330 ± 0.0005 0.0466 ± 0.0041 0.9101 ± 0.0162\nD‑MPNNs 0.0313 ± 0.0008 0.0463 ± 0.0049 0.9113 ± 0.0146\nFFiNet 0.0330 ± 0.0013 0.0479 ± 0.0037 0.9005 ± 0.0127\nFFiTrNet 0.0313 ± 0.0004 0.0448 ± 0.0022 0.9170 ± 0.0141\nPage 6 of 13Jin et al. Journal of Cheminformatics           (2023) 15:65 \ntraining, validation, and test dataset with a ratio of \n0.8:0.1:0.1. We performed three independent runs with \ndifferent random seeds for each model. The result is \nshown in Table 2, presented in the form of “mean ± stand-\nard deviation” of the three runs after the hyperparameter \nFig. 3 The parity plot of true density versus predicted density of each model. All the data in the graph is from the test dataset. a Random Forest; b \nGATv2 c D‑MPNNs; d FFiNet; e FFiTrNet\nPage 7 of 13\nJin et al. Journal of Cheminformatics           (2023) 15:65 \n \noptimization. Moreover, all the models are using the \nsame training strategy for a fair comparison.\nBased on the results from Table  2 and Fig.  3, we \ncan see that RF which employs descriptors created by \nhumans, exhibits the poorest performance in this pre -\ndiction task. The number of feature used in RF is 208, \nwhich is rather easy to obtain, so this finding demon -\nstrates that GNNs have already developed descriptive \nfeatures that possess more expressive power than these \neasily obtainable RDkit molecular descriptors. For the \nGNNs model, our original FFiNet performs worse than \nthe GATv2 and D-MPNNs when dealing with crystal \ndensity. But after introducing the Transformer encoder \ninto the FFiNet model, FFiTrNet’s performance has \nbeen improved considerably, reaching a slightly bet -\nter result than the D-MPNNs. This shows that Trans -\nformer encoder does make the k-hop information more \nexpressive and the prediction more accurate.\nHowever, because of the distribution of our CSD \ncurated dataset, in which only 322 out of 12,072 data \npoints’ density are within our interested region, i.e. \nover 1.8  g/cm 3, most of the prediction errors do not \naffect the final decision of screening. This is to say that \nif the true crystal density of one molecule is pretty \nmuch lower than 1.8 g/cm 3, the higher prediction accu -\nracy is not much important because it will be quickly \nscreened out of the promising candidates’ list. Thus the \nprediction accuracy in different density regions should \nbe treated differently, especially those within and near \nour interested density region.\nIn order to evaluate the performance of our model, \nwe split the test dataset of each model above into 4 \nregions: (1) density higher than 1.8  g/cm 3, our inter -\nested high-density region. A bunch of modern ener -\ngetic materials, 1,3,5-Trinitro-1,3,5-triazinane (RDX, \n1.806  g/cm 3), 1,3,5,7-Tetranitro-1,3,5,7-tetrazocane \n(HMX, 1.91  g/cm 3), hexanitrohexaazaisowurtzitane \n(CL-20, 2.044  g/cm 3), etc. fall in this region, there -\nfore accurate prediction is highly desirable; (2) density \nbetween 1.6 and 1.8 g/cm 3, near the interested region. \nConventional energetic materials such as 2,4,6-trinitro -\ntoluene (TNT, 1.654 g/cm 3) fall in it, and should have \na certain level of accuracy; (3) density between 1.4  g/\ncm3 and 1.6 g/cm 3, not that much important; (4) den -\nsity lower than 1.4  g/cm 3, out of consideration. Then, \nwe list out the test MAE, RMSE, and R 2 of each region, \nas shown in Table  3 and Fig.  4. Because of the small \namount of data in regions 1 and 2, R2 is pretty small in \nthese regions, making it meaningless and not compara -\nble between each model.\nFrom the results above, FFiTrNet outperforms the \nother models in regions 1 and 2, having the lowest MAE, \nRMSE and highest R 2. For regions 3 and 4, which make \nup most of the CSD curated dataset, as shown in Fig.  5, \nFFiTrNet performs worse than the D-MPNNs but is still \nin second place. This suggests our model could give a bet-\nter prediction dealing with the high-density materials, \nand D-MPNNs having quite the same overall accuracy as \nFFiTrNet though, has less ability to handle the data point \nout of main dataset part, indicating overfitting in the \nmain part of the dataset.\nCrystal density predicting task for potential energetic \nmaterials dataset\nAs said before, the main problem with using ML in pre -\ndicting the energetic materials’ crystal density is the \nlack of large datasets. But after enlarger the dataset to \nthe CHNO molecules, it is possible to use the curated-\ndataset-trained model to predict a small energetic mate -\nrial dataset. In there, we use another small dataset from \nHuang & Massa [53], who obtain explosive properties \nagainst 109 putative energetic materials and thereby pro -\nduce their energetic characteristics, including the crys -\ntalline density, using quantum chemical calculations. In \nHuang & Massa dataset, most of the data (87 out of 109, \nwith a ratio of 0.798) have a density greater than 1.8  g/\ncm3, the ratio of the interested data is much higher than \nthat of the CSD curated dataset (318 out of 12,072, with a \nratio of 0.0263). The data distribution of the two datasets \nTable 3 The test MAE and RMSE for each model in different \ndensity region\nThe best results are marked in bold, and the second-best results are italicized\nDensity region Models MAE (g/cm3) RMSE (g/cm3)\n1) ρ ≥ 1.8 g/cm3 RF 0.0818 ± 0.0107 0.1169 ± 0.0219\nGATv2 0.0522 ± 0.0032 0.0681 ± 0.0057\nD‑MPNNs 0.0476 ± 0.0033 0.0564 ± 0.0020\nFFiNet 0.0573 ± 0.0034 0.0739 ± 0.0056\nFFiTrNet 0.0446 ± 0.0045 0.0556 ± 0.0036\n2) 1.6 g/\ncm3 ≤ ρ < 1.8 g/cm3\nRF 0.0544 ± 0.0024 0.0710 ± 0.0060\nGATv2 0.0482 ± 0.0064 0.0631 ± 0.0112\nD‑MPNNs 0.0436 ± 0.0059 0.0604 ± 0.0113\nFFiNet 0.0439 ± 0.0037 0.0631 ± 0.0081\nFFiTrNet 0.0422 ± 0.0021 0.0567 ± 0.0068\n3) 1.4 g/\ncm3 ≤ ρ < 1.6 g/cm3\nRF 0.0380 ± 0.0008 0.0479 ± 0.0006\nGATv2 0.0326 ± 0.0012 0.0404 ± 0.0018\nD‑MPNNs 0.0316 ± 0.0015 0.0387 ± 0.0016\nFFiNet 0.0331 ± 0.0003 0.0412 ± 0.0002\nFFiTrNet 0.0325 ± 0.0003 0.0407 ± 0.0004\n4) ρ < 1.4 g/cm3 RF 0.0321 ± 0.0014 0.0449 ± 0.0040\nGATv2 0.0308 ± 0.0003 0.0459 ± 0.0047\nD‑MPNNs 0.0287 ± 0.0006 0.0443 ± 0.0067\nFFiNet 0.0309 ± 0.0018 0.0473 ± 0.0069\nFFiTrNet 0.0297 ± 0.0011 0.0443 ± 0.0029\nPage 8 of 13Jin et al. Journal of Cheminformatics           (2023) 15:65 \nis shown in Fig.  5. Due to the high-density distribution \nof Huang & Massa dataset, the prediction error for this \ndataset will be more practical, without error distortion \nfrom the low-density data. So the prediction accuracy \nfor Huang & Massa dataset should be considered more \nimportant for the real screening process. Before using \nHuang & Massa dataset as the test data, we first removed \nFig. 4 Comparison of each model’s performance in different density regions. a MAE; b RMSE. Our model outperforms other models in high‑density \nregions and is second only to the D‑MPNNs in low‑density regions in which high accuracy is not important. The unit of crystal density in graph is g/\ncm3\nFig. 5 The relative distribution of two datasets. Crystal density higher \nthan 1.8 g/cm3 is our interested region. The ratio of the interested \ndata is 0.0263 (318 out of 12,072) for CSD curated dataset and 0.798 \n(87 out of 109) for Huang & Mass dataset, showing a great bias \nof these two datasets\nTable 4 The test MAE, RMSE and R2 for each model using Huang \n& Massa dataset as test dataset\nThe best results are marked in bold, and the second-best results are italicized\nModels MAE (g/cm3) RMSE (g/cm3) R2\nRF 0.0620 ± 0.0004 0.0964 ± 0.0006 0.5144 ± 0.0058\nGATv2 0.0515 ± 0.0024 0.0631 ± 0.0022 0.7915 ± 0.0145\nD‑MPNNs 0.0602 ± 0.0013 0.0794 ± 0.0010 0.6704 ± 0.0085\nFFiNet 0.0561 ± 0.0020 0.0712 ± 0.0023 0.7346 ± 0.0172\nFFiTrNet 0.0489 ± 0.0012 0.0604 ± 0.0012 0.8092 ± 0.0077\nPage 9 of 13\nJin et al. Journal of Cheminformatics           (2023) 15:65 \n \nFig. 6 The parity plot of true density versus predicted density of each model using Huang & Massa dataset as test dataset a Random Forest; b \nGATv2; c D‑MPNN d FFiNet; e FFiTrNet\nPage 10 of 13Jin et al. Journal of Cheminformatics           (2023) 15:65 \nthe training data points that appear in both two datasets, \nto make sure all the test data are unseen, so the test result \nwill be comparable and reliable.\nAll the results, in the form of mean ± standard devia -\ntion of three independent runs, are shown in Table 4.\nIt can be seen from Table  4 and Fig.  6, our FFiTrNet \nmodel outperforms other models. Although there’s an \naccuracy drop compared to Table  3, where the MAE is \n0.0489  g/cm3 compared to 0.0446  g/cm 3 for the region \nthat the density is higher than 1.8  g/cm 3, FFiTrNet still \nhas a relatively good prediction, which is considered \n“informative” as it’s lower than 0.05  g/cm 3. This accu -\nracy drop may be caused by the different molecule types \nin Huang & Massa dataset. Different from CSD curated \ndataset that only contains the CHNO compound, 23 mol-\necules in Huang & Massa dataset contain fluorine atoms, \nwhich is completely unseen in the training process, thus \nthe effect of fluorine atoms on the crystal density is not \nlearned, making the FFiTrNet less accurate. Interest -\ningly, D-MPNNs’ performance on the Huang & Massa \ndataset is relatively poor, though its overall test error for \nCSD curated dataset is close to FFiTrNet. It might be that \nD-MPNNs overfit in the main part of CSD curated data -\nset, whose density is mostly below 1.6  g/cm 3, as shown \nin the discussion of Table  3. For the model with simpler \nstructures, like GATv2 and RF, the overfitting is not sig -\nnificant, so they don’t suffer from this accuracy drop and \neven perform better.\nModel interpolation through the molecular structure\nIn Huang & Massa dataset, all 109 energetic materials are \ndivided into 10 distinct compound families (or groups). \nThe families are labeled according to some characteris -\ntic chemical like CL-20 and HMX, or structural feature \nuniting members of the families. By listing out the MAE \nof each group, we can further investigate the relationship \nbetween molecular structure and model accuracy.\nIn Table 5, all the mean absolute errors of each mole -\ncule family are listed, and Fig.  7 gives the 3D molecular \nstructure of one example from each family to help better \nunderstand the structural difference between each family.\nFrom Table  5, comparing FFiNet and FFiTrNet mod -\nels, we can see that after adding Transformer encoder, \nFFiTrNet model shows performance improvement in \nmost of the families. When compared to other models, \nFFiTrNet also has the lowest prediction error in most of \nthe molecular families, especially those molecules with \ncomplex 3D structures, such as cubane and CL-20 fami -\nlies. Also, because the cage structure usually has a higher \ncrystal density and energy density, having attracted much \nattention in the energetic materials field [54], the predic -\ntion accuracy for caged structures should be high on the \nTable 5 The Mean Absolute Errors (g/cm 3) of Each Molecule \nFamily Using Huang & Massa Dataset as Testing\nThe best results are marked in bold, and the second-best results are italicized\nAll data in the table are the average of three independent runs\nMolecule families RF GATv2 D-MPNN FFiNet FFiTrNet\nCubane 0.1013 0.0548 0.0662 0.0620 0.0449\nCL‑20 0.0694 0.0616 0.0673 0.0850 0.0601\nLinear 0.0994 0.0494 0.0684 0.0513 0.0480\nPyrazole 0.0573 0.0505 0.0481 0.0466 0.0470\nButterfly 0.1146 0.0736 0.0874 0.0699 0.0693\nKetone 0.1108 0.0521 0.0517 0.0653 0.0464\nHMX 0.0871 0.0420 0.0671 0.0475 0.0399\nTNT 0.0666 0.0422 0.0498 0.0410 0.0475\nRDX 0.0930 0.0502 0.0641 0.0644 0.0572\nRing 0.0744 0.0620 0.0475 0.0680 0.0593\nFig. 7 The instance of each family in Huang & Massa dataset, shown in 3D form. The carbon atom (C) is shown in grey color; the oxygen atom (O) \nis shown in red color; the nitrogen atom (N) is shown in blue color and the fluorine atom (F) is shown in yellow color\nPage 11 of 13\nJin et al. Journal of Cheminformatics           (2023) 15:65 \n \nlist of priorities. This could also explain why our model \nperforms better in the high-density region, because they \nhave a higher ratio of these caged molecules. And the \nhigher accuracy in the unseen data shows our model gets \nless overfit in the training dataset, learning the more fun -\ndamental pattern inside the molecule graph. The prom -\nising result in the high-density and out-of-distribution \ndataset makes our model is powerful tool to predicting \nand screening for the potential energetic materials.\nConclusions\nCrystal density is an important property of energetic \nmaterials, but applying ML methods to predict energetic \nmaterials’ crystal density still face the problem of insuffi -\ncient data. In this work, we curate a relatively big dataset \nfrom CSD containing 12,072 data of CHON compounds \nwith merely SMILES string and crystal density to over -\ncome this problem. New 3D-aware GNNs models FFiNet \nand its upgraded version FFiTrNet are then trained and \ntested in this CSD curated dataset. Our FFiTrNet model \noutperforms other ML models, RF, GATv2 and D-MPNN, \nespecially in the high-density region, which has more \nimportance in the practical screening process, showing \nFFiTrNet overfits less in the low-density region and has \nmore generalizability. After training the models on the \nCSD curated dataset, we use this pretrained model to \npredict the potential energetic materials dataset: Huang \n& Massa dataset, showing great performance in this out-\nof-distribution dataset. Finally, we further investigate the \neffect of some certain molecular structure on the models, \nFFiTrNet using 3D conformation of molecules could give \na more accurate prediction for cage structure, which is \nthe promising searching area of the energetic materials. \nBut also, the deeper interpolation of the model is needed \nfor a better understanding of how force field terms work \nin predicting the crystal density of different molecule \nstructures, which would be a tough and tricky task, due \nto the complexity of our model.\nAll of these results prove that FFiTrNet will be an \neffective model in predicting the crystal density and \nscreening for new energetic materials. Our model \ncould also be applied to other properties of the ener -\ngetic materials, such as explosive energy and impact \nsensitivity, which will be used in the further screening \nprocess. Moreover, because our model uses only the \nSMILES strings of the molecules as the inputs and no \na priori knowledge is needed to predict the crystal den -\nsity, it can be easy to incorporate new molecules into \nthis model, even if we have limited knowledge about \nthem. This helps us to apply this model more simply to \nthe next screening stage, like molecular generation [55, \n56] of energetic materials, in which most of the gener -\nated molecules will be completely new and unknown. \nTherefore, our FFiTrNet sets a strong foundation for \naccelerating the screening of effective energetic materi -\nals and for in silico design of new energetic materials, \nutilizing the molecular generation technique.\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. \norg/ 10. 1186/ s13321‑ 023‑ 00736‑6.\n Additional file 1. Additional model detail, Rdkit feature, and model \ncomparison for small dataset.\nAuthor contributions\nJJ and GR contributed equally to this work. KW and YH contributed to \nconceptualization and study design; YL and JH contributed to data screening, \nanalysis, and interpretation; YG contributed to discussions and writing of this \npaper; JJ contributed to study design, and supervised model construction and \nwriting of this paper; GR contributed to methodology development, model \nconstruction, and writing of the paper. All authors read and approved the final \nmanuscript.\nFunding\nThe authors are grateful for the financial support from the National Natural \nScience Foundation of China (Grant No. U22A20408) and the Zhejiang Provin‑\ncial Key R&D Program (Grant No. 2022C01179) for this work.\nData and code availability\nAll of the methods are implemented in Python. Source code and dataset is \navailable at GitHub page: https:// github. com/ jjx‑ 2000/ FFiTr Net.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 4 May 2023   Accepted: 12 July 2023\nReferences\n 1. Segler MHS, Preuss M, Waller MP (2018) Planning chemical syntheses \nwith deep neural networks and symbolic AI. Nature 555(7698):604–610. \nhttps:// doi. org/ 10. 1038/ natur e25978\n 2. Senior AW, Evans R, Jumper J et al (2020) Improved protein structure pre‑\ndiction using potentials from deep learning. Nature 577(7792):706–710. \nhttps:// doi. org/ 10. 1038/ s41586‑ 019‑ 1923‑7\n 3. Sun W, Zheng Y, Yang K et al (2019) Machine learning‑assisted molecular \ndesign and efficiency prediction for high‑performance organic photo‑\nvoltaic materials. Sci Adv 5(11):eaay4275. https:// doi. org/ 10. 1126/ sciadv. \naay42 75\n 4. Haghighatlari M, Li J, Heidar‑Zadeh F, Liu Y, Guan X, Head‑Gordon T (2020) \nLearning to make chemical predictions: the interplay of feature repre‑\nsentation, data, and machine learning methods. Chem 6(7):1527–1542. \nhttps:// doi. org/ 10. 1016/j. chempr. 2020. 05. 014\n 5. Yu LY, Ren GP , Hou XJ, Wu KJ, He Y (2022) Transition state theory‑inspired \nneural network for estimating the viscosity of deep eutectic solvents. ACS \nCent Sci 8(7):983–995. https:// doi. org/ 10. 1021/ acsce ntsci. 2c001 57\n 6. Graff DE, Shakhnovich EI, Coley CW (2021) Accelerating high‑throughput \nvirtual screening through molecular pool‑based active learning. Chem \nSci 12(22):7866–7881. https:// doi. org/ 10. 1039/ D0SC0 6805E\nPage 12 of 13Jin et al. Journal of Cheminformatics           (2023) 15:65 \n 7. Clyde A, Galanie S, Kneller DW et al (2022) High‑throughput virtual \nscreening and validation of a SARS‑CoV‑2 main protease noncovalent \ninhibitor. J Chem Inf Model 62(1):116–128. https:// doi. org/ 10. 1021/ acs. \njcim. 1c008 51\n 8. Song S, Chen F, Wang Y, Wang K, Yan M, Zhang Q (2021) Accelerating the \ndiscovery of energetic melt‑castable materials by a high‑throughput \nvirtual screening and experimental approach. J Mater Chem A Mater \n9(38):21723–21731. https:// doi. org/ 10. 1039/ d1ta0 4441a\n 9. Liu Y, Zhao T, Ju W, Shi S (2017) Materials discovery and design using \nmachine learning. J Materiomics 3(3):159–177. https:// doi. org/ 10. 1016/j. \njmat. 2017. 08. 002\n 10. Council NR (2004) Advanced energetic materials. The National Academies \nPress, Washington. https:// doi. org/ 10. 17226/ 10918\n 11. Alder BJ, Wainwright TE (1959) Studies in molecular dynamics. I. General \nmethod. J Chem Phys 31(2):459–466. https:// doi. org/ 10. 1063/1. 17303 76\n 12. Kohn W (1999) Nobel Lecture: Electronic structure of matter–wave func‑\ntions and density functionals. Rev Mod Phys 71(5):1253–1266. https:// doi. \norg/ 10. 1103/ RevMo dPhys. 71. 1253\n 13. Schuch N, Verstraete F (2009) Computational complexity of interacting \nelectrons and fundamental limitations of density functional theory. Nat \nPhys 5(10):732–735. https:// doi. org/ 10. 1038/ nphys 1370\n 14. Drucker H, Burges CJC, Kaufman L, Smola A, Vapnik V (1996) Support \nvector regression machines. In: Mozer MC, Jordan M, Petsche T (eds) \nAdvances in neural information processing systems, vol 9. MIT Press, \nCambridge\n 15. Xue Y, Li ZR, Yap CW, Sun LZ, Chen X, Chen YZ (2004) Effect of molecular \ndescriptor feature selection in support vector machine classification of \npharmacokinetic and toxicological properties of chemical agents. J Chem \nInf Comput Sci 44(5):1630–1638. https:// doi. org/ 10. 1021/ ci049 869h\n 16. Palmer DS, O’Boyle NM, Glen RC, Mitchell JBO (2007) Random forest mod‑\nels to predict aqueous solubility. J Chem Inf Model 47(1):150–158. https:// \ndoi. org/ 10. 1021/ ci060 164k\n 17. Svetnik V, Liaw A, Tong C, Culberson JC, Sheridan RP , Feuston BP (2003) \nRandom forest: a classification and regression tool for compound clas‑\nsification and QSAR modeling. J Chem Inf Comput Sci 43(6):1947–1958. \nhttps:// doi. org/ 10. 1021/ ci034 160g\n 18. Sun W, Zheng Y, Yang K et al (2023) Machine learning–assisted molecular \ndesign and efficiency prediction for high‑performance organic photo‑\nvoltaic materials. Sci Adv 5(11):eaay4275. https:// doi. org/ 10. 1126/ sciadv. \naay42 75\n 19. Hirschfeld L, Swanson K, Yang K, Barzilay R, Coley CW (2020) Uncertainty \nquantification using neural networks for molecular property prediction. J \nChem Inf Model 60(8):3770–3780. https:// doi. org/ 10. 1021/ acs. jcim. 0c005 \n02\n 20. Taskinen J, Yliruusi J (2003) Prediction of physicochemical properties \nbased on neural network modelling. Adv Drug Deliv Rev 55(9):1163–\n1183. https:// doi. org/ 10. 1016/ S0169‑ 409X(03) 00117‑0\n 21. Le T, Epa VC, Burden FR, Winkler DA (2012) Quantitative structure–prop‑\nerty relationship modeling of diverse materials properties. Chem Rev \n112(5):2889–2919. https:// doi. org/ 10. 1021/ cr200 066h\n 22. Rupp M, Tkatchenko A, Müller KR, von Lilienfeld OA (2012) Fast and accu‑\nrate modeling of molecular atomization energies with machine learning. \nPhys Rev Lett 108(5):58301. https:// doi. org/ 10. 1103/ PhysR evLett. 108. \n058301\n 23. Hansen K, Biegler F, Ramakrishnan R et al (2015) Machine learning \npredictions of molecular properties: accurate many‑body potentials and \nnonlocality in chemical space. J Phys Chem Lett 6(12):2326–2331. https:// \ndoi. org/ 10. 1021/ acs. jpcle tt. 5b008 31\n 24. Wieder O, Kohlbacher S, Kuenemann M et al (2020) A compact review of \nmolecular property prediction with graph neural networks. Drug Discov \nToday Technol 37:1–12. https:// doi. org/ 10. 1016/j. ddtec. 2020. 11. 009\n 25. Fung V, Zhang J, Juarez E, Sumpter BG (2021) Benchmarking graph neural \nnetworks for materials chemistry. NPJ Comput Mater. https:// doi. org/ 10. \n1038/ s41524‑ 021‑ 00554‑0\n 26. Elton DC, Boukouvalas Z, Butrico MS, Fuge MD, Chung PW (2018) Apply‑\ning machine learning techniques to predict the properties of energetic \nmaterials. Sci Rep. https:// doi. org/ 10. 1038/ s41598‑ 018‑ 27344‑x\n 27. Lansford JL, Barnes BC, Rice BM, Jensen KF (2022) Building chemical prop‑\nerty models for energetic materials from small datasets using a transfer \nlearning approach. J Chem Inf Model 62(22):5397–5410. https:// doi. org/ \n10. 1021/ acs. jcim. 2c008 41\n 28. Singh RP , Verma RD, Meshri DT, Shreeve JM (2006) Energetic nitrogen‑rich \nsalts and ionic liquids. Angew Chem Int Ed 45(22):3584–3601. https:// doi. \norg/ 10. 1002/ anie. 20050 4236\n 29. Fried LE, Manaa MR, Pagoria PF, Simpson RL (2001) Design and synthesis \nof energetic materials. Annu Rev Mater Sci 31:291–321. https:// doi. org/ \n10. 1146/ annur ev. matsci. 31.1. 291\n 30. Wang R, Xu H, Guo Y, Sa R, Shreeve JM (2010) Bis[3‑(5‑nitroimino‑1,2,4‑\ntriazolate)]‑based energetic salts: synthesis and promising properties \nof a new family of high‑density insensitive materials. J Am Chem Soc \n132(34):11904–11905. https:// doi. org/ 10. 1021/ ja105 5033\n 31. Ness V, Tarver CM (1979) Mole fraction, liquid phase Xaz Azeotropic \ncomposition y Mole fraction, vapor phase Greek Letters Parameter in Eq \n1 signifies a difference literature cited, vol 24. https:// pubs. acs. org/ shari \nnggui delin es\n 32. Ammon HL (2008) Updated atom/functional group and Atom_Code \nvolume additivity parameters for the calculation of crystal densities of \nsingle molecules, organic salts, and multi‑fragment materials containing \nH, C, B, N, O, F, S, P , Cl, Br, and I. Prop Explos Pyrotech 33(2):92–102. https:// \ndoi. org/ 10. 1002/ prep. 20070 0054\n 33. Keshavarz MH (2007) Prediction of densities of acyclic and cyclic nit‑\nramines, nitrate esters and nitroaliphatic compounds for evaluation of \ntheir detonation performance. J Hazard Mater 143(1):437–442. https:// \ndoi. org/ 10. 1016/j. jhazm at. 2006. 09. 054\n 34. Keshavarz MH, Pouretedal HR (2009) A reliable simple method to \nestimate density of nitroaliphatics, nitrate esters and nitramines. J Hazard \nMater 169(1):158–169. https:// doi. org/ 10. 1016/j. jhazm at. 2009. 03. 091\n 35. Fathollahi M, Sajady H (2018) Prediction of density of energetic cocrystals \nbased on QSPR modeling using artificial neural network. Struct Chem \n29(4):1119–1128. https:// doi. org/ 10. 1007/ s11224‑ 018‑ 1096‑2\n 36. Casey AD, Son SF, Bilionis I, Barnes BC (2020) Prediction of energetic \nmaterial properties from electronic structure using 3D convolutional \nneural networks. J Chem Inf Model 60(10):4457–4473. https:// doi. org/ 10. \n1021/ acs. jcim. 0c002 59\n 37. Fink T, Raymond JL (2007) Virtual exploration of the chemical universe \nup to 11 atoms of C, N, O, F: Assembly of 26.4 million structures (110.9 \nmillion stereoisomers) and analysis for new ring systems, stereochemistry, \nphysicochemical properties, compound classes, and drug discovery. J \nChem Inf Model 47(2):342–353. https:// doi. org/ 10. 1021/ ci600 423u\n 38. Blum LC, Reymond JL (2009) 970 Million druglike small molecules for \nvirtual screening in the chemical universe database GDB‑13. J Am Chem \nSoc 131(25):8732–8733. https:// doi. org/ 10. 1021/ ja902 302h\n 39. Ruddigkeit L, van Deursen R, Blum LC, Reymond JL (2012) Enumeration \nof 166 billion organic small molecules in the chemical universe database \nGDB‑17. J Chem Inf Model 52(11):2864–2875. https:// doi. org/ 10. 1021/ \nci300 415d\n 40. Yang C, Chen J, Wang R, Zhang M, Zhang C, Liu J (2021) Density predic‑\ntion models for energetic compounds merely using molecular topology. \nJ Chem Inf Model 61(6):2582–2593. https:// doi. org/ 10. 1021/ acs. jcim. \n0c013 93\n 41. Nguyen P , Loveland D, Kim JT, Karande P , Hiszpanski AM, Han TYJ (2021) \nPredicting energetics materials’ crystalline density from chemical struc‑\nture by machine learning. J Chem Inf Model 61(5):2147–2158. https:// doi. \norg/ 10. 1021/ acs. jcim. 0c013 18\n 42. Yang K, Swanson K, Jin W et al (2019) Analyzing learned molecular rep‑\nresentations for property prediction. J Chem Inf Model 59(8):3370–3388. \nhttps:// doi. org/ 10. 1021/ acs. jcim. 9b002 37\n 43. Ren GP , Yin YJ, Wu KJ, He Y (2023) Force field‑inspired molecular represen‑\ntation learning for property prediction. J Cheminform. https:// doi. org/ 10. \n1186/ s13321‑ 023‑ 00691‑2\n 44. Wu Z, Ramsundar B, Feinberg EN, Gomes J, Geniesse C, Pappu AS, \nLeswing K, Pande V (2017) MoleculeNet: a benchmark for molecular \nmachine learning. Chem Sci 9(2):513–530. https:// doi. org/ 10. 1039/ c7sc0 \n2664a\n 45. Groom CR, Bruno IJ, Lightfoot MP , Ward SC (2016) The Cambridge struc‑\ntural database. Acta Crystallogr Sect B 72(2):171–179. https:// doi. org/ 10. \n1107/ S2052 52061 60039 54\n 46. Landrum G (2010) “RDKit.” Q2. https:// www. rdkit. org/\n 47. Riniker S, Landrum GA (2015) Better informed distance geometry: using \nwhat we know to improve conformation generation. J Chem Inf Model \n55(12):2562–2574. https:// doi. org/ 10. 1021/ acs. jcim. 5b006 54\nPage 13 of 13\nJin et al. Journal of Cheminformatics           (2023) 15:65 \n \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 48. Leach AR (2001) Molecular modelling: principles and applications, 2nd \nedn. Prentice Hall, Harlow\n 49. Vaswani A, Shazeer NM, Parmar N et al. Attention is all you need. arXiv: \n1706. 03762\n 50. Dosovitskiy A, Beyer L, Kolesnikov A et al (2020) An image is worth 16x16 \nwords: transformers for image recognition at scale. Published online \nOctober 22, 2020. arXiv: 2010. 11929\n 51. Kim CK, Cho SG, Kim CK, Park HY, Zhang H, Lee HW (2008) Prediction of \ndensities for solid energetic molecules with molecular surface electro‑\nstatic potentials. J Comput Chem 29(11):1818–1824. https:// doi. org/ 10. \n1002/ jcc. 20943\n 52. Klapötke TM (2018) Energetic Materials Encyclopedia. De Gruyter, Berlin. \nhttps:// doi. org/ 10. 1515/ 97831 10442 922\n 53. Huang L, Massa L (2013) Applications of energetic materials by a theoreti‑\ncal method (discover energetic materials by a theoretical method). Int J \nEnergy Mater Chem Propul 12:197–262\n 54. Wen L, Yu T, Lai W et al (2021) Accelerating molecular design of cage \nenergetic materials with zero oxygen balance through large‑scale data‑\nbase search. J Phys Chem Lett 12(47):11591–11597. https:// doi. org/ 10. \n1021/ acs. jpcle tt. 1c037 28\n 55. Xu M, Yu L, Song Y, Shi C, Ermon S, Tang J (2022) GeoDiff: a geometric \ndiffusion model for molecular conformation generation. Published online \nMarch 6, 2022. arXiv: 2203. 02923\n 56. Mahmood O, Mansimov E, Bonneau R, Cho K (2021) Masked graph mod‑\neling for molecule generation. Nat Commun. https:// doi. org/ 10. 1038/ \ns41467‑ 021‑ 23415‑2\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub‑\nlished maps and institutional affiliations."
}