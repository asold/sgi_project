{
  "title": "Software requirement specific entity extraction using transformer models",
  "url": "https://openalex.org/W4299711627",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2181741258",
      "name": "Garima Malik",
      "affiliations": [
        "Toronto Metropolitan University"
      ]
    },
    {
      "id": "https://openalex.org/A2169544815",
      "name": "Mucahit Cevik",
      "affiliations": [
        "Toronto Metropolitan University"
      ]
    },
    {
      "id": "https://openalex.org/A4303081111",
      "name": "Swayami Bera",
      "affiliations": [
        "Toronto Metropolitan University"
      ]
    },
    {
      "id": "https://openalex.org/A2073611687",
      "name": "Savas Yıldırım",
      "affiliations": [
        "Toronto Metropolitan University"
      ]
    },
    {
      "id": "https://openalex.org/A2101012168",
      "name": "Devang Parikh",
      "affiliations": [
        "IBM (United States)",
        "Istanbul Bilgi University"
      ]
    },
    {
      "id": "https://openalex.org/A2914339962",
      "name": "Ayşe Başar",
      "affiliations": [
        "Toronto Metropolitan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2147880316",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W3172718868",
    "https://openalex.org/W3022192152",
    "https://openalex.org/W3035858791",
    "https://openalex.org/W3011940529",
    "https://openalex.org/W2999019592",
    "https://openalex.org/W2406204547",
    "https://openalex.org/W2978389165",
    "https://openalex.org/W3091869361",
    "https://openalex.org/W3200787399",
    "https://openalex.org/W3104774463",
    "https://openalex.org/W2904415434",
    "https://openalex.org/W6640599039",
    "https://openalex.org/W2796177046",
    "https://openalex.org/W6744535722",
    "https://openalex.org/W2906635035",
    "https://openalex.org/W3095092693",
    "https://openalex.org/W2157275230",
    "https://openalex.org/W3011594683",
    "https://openalex.org/W2760476066",
    "https://openalex.org/W3176489198",
    "https://openalex.org/W4233343645",
    "https://openalex.org/W3105470358",
    "https://openalex.org/W2086613190",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3034503357"
  ],
  "abstract": "Software Requirement Specifications (SRS) documents provide the description of requirements and expectations attributed to software products. The structured text present in the SRS documents serves as a guide for developers in defining various functions in the process of software development. Software specific entity extraction is an important pre-processing step for various Natural Language Processing (NLP) tasks in the requirement engineering domain such as entity-centric search systems, SRS document summmarization, requirement classification, and requirement quality management. Recent advances in transformer-based models have significantly contributed to NLP and information retrieval problems, and achieved state-of-the-art performance for domain specific entity extraction tasks. In this study, we employ the transformer models including BERT, RoBERTa and ALBERT for software specific entity extraction. For this purpose, we annotate three requirement datasets, namely, DOORS, SRE, and RQA with varied sets of software specific entities. Our numerical study shows that transformer models are able to outperform the traditional approaches such as ML-CRF, and we find that BERT variants improve the F1-scores by 4% and 5% on the DOORS and SRE datasets, respectively. We conduct entity level error analysis to examine the partial and exact matching of entities and respective boundaries. Lastly, we experiment with few-shot learning to create sample efficient NER systems with template-based BART model",
  "full_text": "The 35th Canadian Conference on Artificial Intelligence\nDOI: 0\nSoftware requirement-specific entity extraction using transformer\nmodels\nGarima Malik†, Mucahit Cevik†, *, Swayami Bera†, Savas Yildirim†, ⋄, Devang Parikh‡, Ayse Basar†\n† Ryerson University, Toronto, Canada\n‡ IBM, North Carolina, USA\n⋄ Istanbul Bilgi University, Istanbul, Turkey\n*mcevik@ryerson.ca\nAbstract\nSoftware Requirement Specifications (SRS) documents provide the description of\nrequirements and expectations attributed to software products. The structured text\npresent in the SRS documents serves as a guide for developers in defining various func-\ntions in the process of software development. Software specific entity extraction is an\nimportant pre-processing step for various Natural Language Processing (NLP) tasks in\nthe requirement engineering domain such as entity-centric search systems, SRS document\nsummmarization, requirement classification, and requirement quality management. Re-\ncent advances in transformer-based models have significantly contributed to NLP and\ninformation retrieval problems, and achieved state-of-the-art performance for domain\nspecific entity extraction tasks. In this study, we employ the transformer models in-\ncluding BERT, RoBERTa and ALBERT for software specific entity extraction. For this\npurpose, we annotate three requirement datasets, namely, DOORS, SRE, and RQA with\nvariedsetsofsoftwarespecificentities. Ournumericalstudyshowsthattransformermod-\nels are able to outperform the traditional approaches such as ML-CRF, and we find that\nBERT variants improve the F1-scores by 4% and 5% on the DOORS and SRE datasets,\nrespectively. We conduct entity level error analysis to examine the partial and exact\nmatching of entities and respective boundaries. Lastly, we experiment with few-shot\nlearning to create sample efficient NER systems with template-based BART model.\nKeywords: Software-specific entity extraction, Transformer models, Few-shot learn-\ning, Software Requirement Specifications (SRS)\n1. Introduction\nSoftware Requirements Specifications (SRS) documents comprise a road map of the\nproject that is being developed. It details the scope, the budget constraints, intended\nuse-cases, the functionalities to be expected, the sequence of actions to be performed and\nthe obstacles to overcome. SRS documentation is an important aspect of any product devel-\nopment to ensure that everyone is in accord with the plan of action, from the stakeholders\nto the development team.\nThe content of the SRS documents is curated in such a way that it is comprehensible\nfor all the personnel involved in the software development process, and humans in general.\nMachines, on the other hand, generally cannot make sense of these documents that are\ntext-heavy and filled with technical jargon. Accordingly, for automation purposes, these are\na collection of structured natural language texts and, in order to discern the contents of\nthese documents, we can employ information extraction or entity identification systems.\nNamed entity recognition (NER) is a type of information retrieval technique that aims to\nfind and sort tokens in the text into predefined categories. Applied to the SRS documents,\nthe named entities would be software specific designations such as actor, action, operator,\nuser, object, GUI, hardware, API, and metric. Executing NER on SRS documents works as\na precursor to various entity-centric applications such as requirement classification, software\ndocument classification, text summarization, data analysis, use case generation, and topic\nmodeling.\n2\nResearch motivation. SRS documents can be perceived as well-structured text which ex-\nplain the functional and non-functional requirements of a system. Extraction of software\nspecific entities can provide us with a high-level overview of SRS documents and this infor-\nmation can be further analysed to group similar and dissimilar requirements. Specifically,\nthese software specific entities can be utilised as an additional feature in requirement qual-\nity assessment systems to improve the quality of software requirements. Most importantly,\nwe can train machine learning models with requirement texts and software-specific entities\nas features for various natural language processing (NLP) tasks in requirement engineering\ndomain. In this study, we explore the capabilities of transformer-based models for software\nspecific entity extraction.\nContribution. The main contributions of our study can be summarized as follows:\n• We implement machine learning-based ML-CRF [1] and widely used transformer\nmodels including BERT [2], RoBERTa [3], and ALBERT [4] over three require-\nment datasets for software specific entity extraction. To the best of our knowledge,\nprevious studies did not consider transformer models in software-specific entity ex-\ntraction.\n• We conduct a detailed numerical study by considering five different NER models and\nthree distinct annotated requirement datasets with varying software-specific entity\nsets. These datasets are obtained from open-source SRS documents associated with\ndifferent industries, such as aerospace, automobile, healthcare, and transportation.\nWe also present entity level error analysis to capture the partial entity and boundary\nmatches in a software-specific entity extraction task.\n• We investigate few-shot learning methods for the NER task. We implement the\ntext-to-text language model BART [5], which restructures the requirements into\ninput text and candidate text, and predicts the software specific entity tags for the\ngiven input. We find that few-shot learning performs better than the BERT variants\nfor smaller training sets, however, this comes at the expense of increased training\ntime.\n2. Literature review\nOnly a few studies consider NLP models for NER tasks in the field of software engi-\nneering. The SoftNER model presented by Tabassum et al. [6] is a BERT-based model\nmodulated to work well on StackOverflow data to identify code tokens and software-related\ntoken labels. RucyBERT is a BERT model trained to tackle textual data specific to the\ncyber-security domain [7]. DBNER use a deep neural network model for bug-specific entity\nrecognition [8]. It employs a combination of bidirectional LSTM (BiLSTM) and conditional\nrandom field (CRF) models to learn many features from large amounts of data extracted\nfrom bug repositories, and incorporates an attention layer to refine the entity recognition\nprocess.\nIn a recent study, Nayak, Kesri, and Dubey [9] presented a knowledge graph-based tool\nthat generates requirement text instances from software engineering documents. They pro-\nposed a constituency parse tree-based path finding algorithm for test intent extraction, and\nemployed a CRF-based model with automatic feature engineering for the NER task. Ye\net al. [10] developed a software-specific NER method (S-NER) and identified the design\nchallenges in creating a NER methodology for social content data. They annotated Stack\nOverflow posts with five tags namely, API, platform, software standard, programming lan-\nguage, and tools. They trained an ML-CRF model with software-specific gazetteers and\nunsupervised word clusters. Reddy et al. [11] defined a set of 22 software-specific entities\nin Stack Overflow posts which includes programming languages (e.g., web development and\n3\nscripting languages), names of software tools, frameworks and protocols. Their numerical\nstudy revealed that BiLSTM-CRF performed better than ML-CRF for their NER task.\nMany previous studies have used few-shot learning for NER when the number of in-\ndomain labeled data instances is very low. Yang and Katiyar [12] demonstrated impressive\nresults for standard few-shot learning applications with a model using nearest neighbor\nlearning and structured inference. Das et al. [13] proposed CONTaiNER, a new contrastive\nlearning method that enhances the inter-token distribution distance for using the few-shot\nsetting on NER. FewNER made use of a meta-learning technique, and it applied novel N-way\nK-shot learning approach for applying few-shot learning to NER [14].Fritzler, Logacheva,\nand Kretov [15] used a metric learning approach called Prototypical Networking which learnt\nthe intermediate representations of tokens that fall under the same named entity category.\nThis method allows the classification of tokens with very few training instances and also\nshows promise to be used as a zero-shot learning method. Text-to-text language models\nsuch as BART and T5 have the ability to transform original sentences into proper useful\nstatements for the NER tasks, as can be seen in the examples “Paris is a type ofcity”, “John\nis a type ofperson”. Cui et al. [5] utilized text-to-text BART model for NER in few-shot\nmode. They treated NER as text-to-text and a language model ranking problem. That is,\ninput sentences and candidate statement templates are ranked and used to produce named\nentity candidates based on the ranking scores.\n3. Methodology\nIn this section, we first describe the three requirement datasets annotated with software-\nspecific entities. Then, we briefly summarize the machine learning and transformer-based\nmodels used for NER, and we explain the experimental setup for our numerical study.\nFigure 1 shows the process of training the NER models with software requirements as input.\nFigure 1. An overview of the work flow for transformers- and ML-CRF-based NER\nmodels\n3.1. Data preparation\nWe consider three software engineering-specific datasets comprised of software require-\nments extracted from different SRS documents. We define different sets of software-specific\nentities in these datasets, and implement a BIO tagging scheme where “B” indicates the\n4\nfirst word of an entity, “I” indicates words inside of an entity , and “O” indicates non-entity\nwords (see Figure 1). Below, we briefly describe the datasets used in our analysis.\n• DOORS: Dynamic Object Oriented Requirements System (DOORS) is a require-\nment management tool [16]. It aids the process of requirement collection, and man-\naging, verifying and communicating requirements through the process of software\ndevelopment in real time. Initially, we extracted 5,100 DOORS user stories (shorter\nversion of the requirements) related to multiple software projects. These require-\nments were subjected to data cleaning and basic pre-processing, which resulted in\n3,333 requirements. We employed both human experts with software development\nbackground and rule-based system in IBM Watson Studio1 software to obtain the\ncomplete annotations. DOORS requirements consist of ten software specific entities\nwhere ‘verb’ is the most dominant and ‘API’ is the least frequent entity (see Table 1\nand Figure 2b).\n• SRE: Software Requirement Entities (SRE) is an extensive dataset that is pre-\npared from five different SRS documents attributed to different industries such as\nUAV (Aerospace) [17], OpenCoss (Transportation)2, WorldVista (Medical)3, Mash-\nbot (Robotics) [18] and Thermostat (Thermodynamics) [18]. Each one of these SRS\ndocuments contain more than 100 requirements. To create the SRE dataset, we\nsampled 378 requirements and converted the compound requirement structures into\nsimple requirements. The annotation process was accomplished by human experts\nusing Doccano 4 and IBM Watson Studio1. This dataset consists of six software\nspecific entities with ‘Property’ being the most frequent and ‘Operator’ being the\nleast frequent (see Figure 2c). Due to the dominance of hardware related require-\nments, we encountered some mathematical measures (‘3 hours’ or ‘70 Km’) and\noperators (‘less than’ or ‘equals to’) in the text. Accordingly, we kept the ‘Metric’\nand ‘Operator’ as entities to represent this information. Figure 2a shows that the\nlongest requirement text length is 67 and shortest is 7.\n• RQA: Requirement Quality Assistant (RQA) is one of the inherent functionality\nprovided by DOORS which improves the quality of software requirements [19]. It\nuses the International Council on Systems Engineering (INCOSE) guidelines for\nwriting the requirements and helps in formulating the good quality requirements\n[20]. With the help of the RQA system, users can also assess the quality of require-\nments using NLP-based feature extraction. The RQA system detects 20 software\nspecific entities which cover both linguistic and morphological aspects of require-\nments. We obtained the corresponding annotations for the requirements present in\nSRE dataset and created another software-specific NER dataset, which we refer to\nas RQA dataset. Figure 2d shows the nine most frequent entities present in this\ndataset.\nTable 1. Software requirement dataset characteristics\nDataset # of Requirements # of Entities\nDOORS 3,333 10\nSRE 378 6\nRQA 378 20\n1https://www.ibm.com/ca-en/cloud/watson-studio\n2http://www.opencoss-project.eu\n3http://coest.org/datasets\n4https://github.com/doccano/doccano\n5\nDOORS RQA SRE\n0\n10\n20\n30\n40\n50\n60\n70\n(a) Requirement text length distribution\nVerb Core GUI User\nHardwareAdjectiveStandard\nAPI\nLanguage\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000Frequency of Entities (b) Entity distribution for DOORS\nActor Action Object Property Operator Metric\n0\n200\n400\n600\n800\n1000\n1200Frequency of Entities\n(c) Entity distribution for SRE\nACTOR\nIMPERATIVE\nACTION\nMAIN_VERB\nPASSIVE USER\nQUALIFYINGCLAUSE\nOBJECT\nSUPERFLUOUSINFINITIVE\n0\n50\n100\n150\n200\n250\n300Frequency of Entities (d) Entity distribution for RQA\nFigure 2. Exploratory data analysis on software requirement datasets\n3.2. Named entity recognition models\nBERT models are shown to achieve state-of-the-art performance for the domain spe-\ncific NER tasks, and it is very convenient to fine-tune these models for downstream tasks.\nAs such, we employ BERT variants, namely, BERT (cased and uncased), RoBERTa and\nALBERT, along with the baseline ML-CRF model for our NER task.\n• ML-CRF: Machine Learning-based Conditional Random Fields (ML-CRF) be-\nlongs to the class of statistical models that use a probabilistic approach to assign\nthe entity tags for each token present in the input text. Previous studies showed that\nML-CRF can be highly effective for domain specific NER tasks as it can take advan-\ntage of data-specific handcrafted features that capture the contextual information\nin the input text [9–11, 21]. For our analysis, we prepared a comprehensive list of\nhandcraftedfeatureswhichincludesorthographic(capitalizationandalphanumeric),\nlexical (tokens), contextual (tokens in window size [-1,1]), and word bitstring (digits\nand alphabets). This feature set is inputted to the CRF learning model which esti-\nmates the conditional probability of selecting appropriate software specific entities\ngiven the input as a requirement text (see Figure 1).\n• BERT: BERT is a language representation model which stands for Bidirectional\nEncoder Representations from Transformers [2]. It is pre-trained on a large corpus\nof data using masked language modeling. We used both BERT-large-cased and\nBERT-large-uncased versions in our analysis. The main difference in the uncased\nversion is that the text has been transformed into lower case before the process of\ntokenization, whereas the cased version performs the training process with raw text.\n• RoBERTa: RoBERTa is a robustly optimized version of BERT. It uses the BERT-\nlarge variant as an underlying structure. The modifications over vanilla BERT\nincludes training the model longer with large batches, removing the next sentence\n6\nprediction objective, training on larger sequences, and dynamically changing the\nmasking pattern in modeling [3].\n• ALBERT: ALBERT is thelite variant of BERT [4]. It simplifies the BERT archi-\ntecture by reducing the number of parameters to optimize the memory efficiency.\nSpecifically, the ALBERT model changed the token embedding layer units from\n768 to 128 which lead to an increase in training speed. In our analysis, we used\nALBERT-base-V2 checkpoint.\n3.3. Few-shot learning\nFew-shot learning refers to making classification based on a small number of data in-\nstances. One of the leading methods for few-shot is prompt-based learning, which has\nbecome a new paradigm in the NLP field due to its simplicity. This approach involves refor-\nmulating downstream tasks as Masked Language Model (MLM) of a pre-trained language\nmodel, showing great effectiveness for various NLP tasks. For instance, a NER task can\nbe formulated as “John went to Paris. Paris is a kind of [MASK].”. When this instance is\npassed to MLM, the output is likely to produce city, place or town. Accordingly, we con-\nverted the software requirement texts into input sentences and candidate statements. For\ninstance “UAV shall charge in 3 hours” can be represented in the form of candidate labels\nas ‘UAV is a type of actor’, ‘charge is a type of action’, and ‘3 hours is a type of metric’.\nWe trained the template-based BART model [5] with transformed version of the software\nrequirement datasets. The input of the text-to-text (BART) model can be perceived as\nsoftware requirements, and output is the candidate statements which represents the named\nentity labels.\n3.4. Experimental setup\nIn a named entity extraction task, the main aim is to identify the correct tag for each\ntoken present in the input text. We employed different BERT variants for this task and\ninstantiated pre-trained transformer models (i.e., BERT-large-uncased, BERT-large-cased,\nand RoBERTa-large). We fine-tuned the architectures for the NER task by using theAuto-\nModelForTokenClassificationclass in the “simpletransformers.classification” library, which\nis based on the transformers library by HuggingFace5.\nThe “Simple Transformers” library provides a convenient way of training and evaluating\nthe transformer models. The process of fine-tuning the transformer models is typically time\nconsuming. However, this can be mitigated by altering the length of the input text, which\nis the maximum length of tokens that appear in the model corpus. We used 100 as the\ndefault value for the max_seq_length parameter since the requirement length ranges from\n(7 to 70) in all the datasets. Because the number of annotated requirements is limited in\nall three datasets, we employed 3-fold cross validation to evaluate the performance of the\nNER models.\nTable 2 shows the hyperparameter values used in training the NER models. We trained\nthe ML-CRF model using CRFsuite library6 and performed hyperparameter tuning using\nthe grid search cross validation technique. In Table 2, ‘C1’ and ‘C2’ correspond to L1 and L2\nregularization, respectively. We used ‘lbfgs’, i.e., limited memory-BFGS as the optimization\nalgorithm, and trained the algorithm for 100 iterations.\nSimilarly, we experimented with different hyperparameters for the transformer models to\nidentify the ideal parameter set. Table 2 shows the values of fine-tuned parameters for each\nBERT variant. For hyperparameter tuning experiments, we set up different combinations\n5https://huggingface.co/transformers/model_doc/bert.html\n6https://sklearn-crfsuite.readthedocs.io/en/latest/\n7\nof learning rate (1e-05 to 5e-05), training epochs (2,5), train batches (8,32) and dropout\nvalue (0.1). We found that transformer models achieve better performance with smaller\ntraining batches and converge within a few epochs. We measured the average training time\nfor all the requirement datasets, and found ML-CRF to be the fastest NER model with an\naverage training time of 21 seconds. Average training times for the transformer models are\nas follows: 9.12 minutes for BERT-large-cased, 9.43 minutes for RoBERTa-large, and 2.9\nminutes for ALBERT-base-v2.\nTable 2. Model configurations and hyperparameters for all the models\nModels Hyperparameters\nML-CRF algorithm : ‘lbfgs’,c1 : 0.1, c2 : 0.1, max_iterations : 100\nBERT-large-uncasednum_hidden_layers : 16 hidden_size : 1024, max_seq_length : 100,\nepoch : 5, drop_out : 0.1, learning_rate : 5e-05,batch_size : 8\nBERT-large-cased num_hidden_layers : 16, hidden_size :1024, max_seq_length : 100,\nepoch : 5, drop_out : 0.1, learning_rate : 5e-5,batch_size : 8\nRoBERTa-large num_hidden_layers : 16, hidden_size : 1024, max_seq_length : 100,\nepoch : 3, drop_out : 0.1, learning_rate : 5e-04,batch_size : 8\nALBERT-base-v2 Repeating_layers : 12, hidden_size : 768, max_seq_length : 100,\nepoch : 5, drop_out : 0.1, learning_rate : 2e-5, batch_size : 8\n4. Results\nInthissection, wefirstcomparefiveNERmodelsoverthreesoftwarerequirementdatasets\n(DOORS, RQA and SRE) using the standard NER performance metrics, namely, weighted\nversions of precision, recall, F1-score, and accuracy. The weighted-averaged F1-score is\nmeasured by taking the mean of all individual entity F1-scores weighted by their support,\nwhere support refers to the number of true instances for each entity. We next present an\nentity level error analysis to investigate underlying causes and remedies to improve NER\nperformance. Lastly, we provide few-shot learning results using DOORS, which is our largest\ndataset.\n4.1. Entity extraction performance\nTable 3 shows the comparison of transformer models with the baseline ML-CRF model.\nThe reported values represent the metrics in percentages accompanied with standard de-\nviation over 3 folds of data. Previous studies show that transformer models achieve state-\nof-the-art performance for various domain specific NER tasks, given sufficient number of\ninstances and fine-tuning of the model parameters [6, 22]. We observe a similar trend in\nour results as well. We find that the BERT-large-cased version consistently outperforms the\nML-CRF model for DOORS and SRE dataset with F1-score of 92.6% and 77.3% respec-\ntively. That is, we manage to improve the F1-score and accuracy by 4% and 5% for DOORS\nand SRE dataset with the BERT-large-cased model. We note that, in software requirement\ntexts, capitalization and abbreviations play an important role, which helps explaining the\nbetter performance of the ‘cased’ BERT version compared to other transformer models.\nML-CRF achieves the best performance for the RQA dataset, with an F1-score of 73.4%.\nWith fewer input instances and larger entity sets available in this dataset, BERT models are\nnot able to achieve the expected performance. For some of the entities in the RQA dataset,\nwe observe minor support values in the training set which make it difficult for the BERT\nmodels to identify the required patterns for the entities.\n8\nTable 3. Entity extraction performance values for the NER models.\nDOORS\nModels Acc(%) P(%) R(%) F1(%)\nML-CRF 92.9 ± 0.00 88.0 ± 0.00 89.6 ± 0.00 88.7 ± 0.00\nBERT-large-cased 95.8± 0.00 91.3 ± 0.01 94.3 ± 0.00 92.6 ± 0.00\nBERT-large-uncased 95.2± 0.00 89.3 ± 0.00 94.0 ± 0.00 91.3 ± 0.00\nRoBERTa-large 95.3 ± 0.00 90.0 ± 0.01 94.0 ± 0.00 92.0 ± 0.00\nALBERT-base-v2 94.9 ± 0.00 89.6 ± 0.02 92.6 ± 0.00 91.0 ± 0.01\nSRE\nModels Acc(%) P(%) R(%) F1(%)\nML-CRF 84.2 ± 0.00 75.4 ± 0.03 69.8 ± 0.00 72.3 ± 0.01\nBERT-large-cased 89.1± 0.04 74.3 ± 0.07 81.3 ± 0.07 77.3 ± 0.07\nBERT-large-uncased 88.4± 0.03 71.0 ± 0.07 81.3 ± 0.05 75.6 ± 0.06\nRoBERTa-large 86.5 ± 0.02 68.6 ± 0.04 77.0 ± 0.04 72.3 ± 0.05\nALBERT-base-v2 88.1 ± 0.03 74.0 ± 0.06 77.0 ± 0.06 75.3 ± 0.06\nRQA\nModels Acc(%) P(%) R(%) F1(%)\nML-CRF 91.0± 0.00 81.5 ± 0.02 69.0 ± 0.00 73.4 ± 0.00\nBERT-large-cased 90.6 ± 0.00 62.0 ± 0.01 67.6 ± 0.03 64.6 ± 0.04\nBERT-large-uncased 90.2± 0.00 61.6 ± 0.06 65.0 ± 0.01 61.6 ± 0.03\nRoBERTa-large 89.6 ± 0.00 53.3 ± 0.03 63.3 ± 0.03 58.0 ± 0.02\nALBERT-base-v2 91.0 ± 0.01 73.3 ± 0.02 70.0 ± 0.04 69.3 ± 0.04\nTable 4 shows the entity specific evaluation with the best performing model for each\ndataset. BERT-large-casedmodelperformswellforDOORSoveralltheentitiesexcept‘Lan-\nguage’ entity, as we have very few instances for this category in the training set. It achieves\nvery high F1-scores for entities such as ‘User’(97.6%), ‘Verb’(95%), and ‘API’ (95.3%) along\nwith low standard deviation values.\nTable 4. Detailed performance values for individual entities with the best performing\nmodel\nDataset Entities P(%) R(%) F1(%) Support\nAPI 93.3±0.01 97.0 ±0.00 95.3 ±0.01 33\nAdjective 87.3±0.02 91.3 ±0.02 89.6 ±0.02 281\nCore 88.6±0.02 93.0 ±0.00 91.0 ±0.01 1,237\nGUI 83.0±0.02 88.3 ±0.01 85.6 ±0.01 592\nDOORSHardware 86.3±0.04 88.3 ±0.10 87.0 ±0.07 26\nLanguage 72.6±0.03 59.6 ±0.15 64.3 ±0.09 37\nPlatform 89.3±0.00 92.0 ±0.01 91.0 ±0.00 235\nStandard 89.3±0.01 93.0 ±0.01 91.0 ±0.01 235\nUser 97.0±0.01 98.0 ±0.00 97.6 ±0.00 923\nVerb 93.0±0.01 96.6 ±0.00 95.0 ±0.00 2607\nAction 85.0±0.06 86.6 ±0.08 86.0 ±0.06 150\nActor 78.3±0.04 92.6 ±0.05 85.0 ±0.04 146\nSRE Metric 92.6±0.06 91.6 ±0.09 92.0 ±0.07 12\nObject 65.6±0.10 64.6 ±0.09 65.3 ±0.10 59\nOperator 54.6±0.33 29.3 ±0.20 38.0 ±0.25 11\nProperty 64.0±0.11 76.0 ±0.08 69.6 ±0.10 218\nAction 82.0±0.04 74.0 ±0.04 78.0 ±0.03 100\nActor 85.0±0.00 79.0 ±0.05 81.0 ±0.02 81\nAmbiguity 71.0±0.15 53.0 ±0.25 59.0 ±0.19 10\nClause 95.0±0.03 89.0 ±0.09 92.0 ±0.04 24\nCombinators 61.0±0.28 15.0 ±0.01 23.0 ±0.01 6\nRQA Design_statements100.0±0.00 86.0 ±0.18 91.0 ±0.11 3\nEscape_clause 100.0±0.00 100.0±0.00 100.0±0.00 1\nImperative 87.0±0.03 85.0 ±0.02 86.0 ±0.01 81\nQualifying_clause 79.0±0.07 60.0 ±0.02 68.0 ±0.01 54\nSuperfluous_infinitive100.0±0.00 78.0 ±0.10 87.0 ±0.06 10\nUser 71.0±0.01 63.0 ±0.05 67.0 ±0.03 12\n9\nWith limited amount of available data for SRE and RQA datasets, NER models are\nnot able to perform well for certain entities. For the SRE data, the BERT-large-cased is\nable to perform well for entities ‘Action’, ‘Actor’, and ‘Metric’ with the F1-scores of 86%,\n85%, and 92%, respectively. BERT-large-cased provides average performance values with\nhigh standard deviation for ‘Operator’ and ‘Property’ entity. The low performance values\nfor these entities can be explained by lack of uniformity in requirement structures as the\n‘Property’ tag was defined differently in all the SRS documents because of the variety of\nwritingstylesofrequirements, and‘Operator’tagsupportwassmall. Astheentitysetisvery\nlarge for the RQA dataset, we get zero support for some of the entities during evaluation.\nTherefore, we only present the best 11 entities out of 20 entities in Table 4. ML-CRF works\nbetter for most entities, irrespective of low support, as it trains over a customised feature\nset for each entity.\n4.2. Error analysis\nA NER task is typically formulated in two steps: first identifying the boundary of tags\n(i.e., recognizingthebeginningandendofthetag), andthenassigningtheappropriateentity.\nAs such, we explore possible cases of errors in NER task both with respect to boundary\nmatching and entity type matching, and report the error measures on our datasets. Table 5\ndescribes five scenarios of errors, which can happen in model predictions with respect to the\nground truth annotations for sample requirements from the SRE dataset. Case I and III\ncan be easily evaluated by classic NER metrics. However, for partial entity and boundary\nmatches, we need different sets of metrics to evaluate the scenarios described in cases II, IV\nand V.\nTable 5. Sample error cases from the SRE dataset with the predictions from the BERT-\nlarge-cased model [23].\nCases Description Ground truth Prediction\nI Surface string and\nentity type match\nNew user will require a username\nto create the account.\nO, B-Actor, O, B-Action, O, B-Property\nO,B-Action, O, B-Property\nNew user will require a username\nto create the account.\nO, B-Actor, O, B-Action, O, B-Property\nO,B-Action, O, B-Property\nII System hypothesized\nan entity\nUAV shall fully charge in 3 hours.\n[B-Actor, O, O, B-Action, O\nB-Metric, I-Metric]\nUAV shall fully charge in 3 hours.\n[B-LOC, O,O, B-Action, O,\nB-Metric, I-Metric]\nIII System misses\nan entity\nDriver shall control the tame gap\nbetween vehicles.\n[B-Actor, O, B-Action, O, B-Property\nI-Property, O, B-Object]\nDriver shall control the tame gap\nbetween vehicles\n[O,O,O, B-Property, I-Property\nO, B-Object]\nIV System get the\nboundary wrong\nThe Themas System shall note\nthe temperature.\n[O, B-Actor, I-Actor, O, B-Action,\nO, B-Property]\nThe Themas System shall note\nthe temperature.\n[B-Actor, I-Actor, I-Actor, O, B-Action\n,O, B-Property]\nV System get the boundary\nand entity type wrong\nThe system shall report the temperature\nof heating and Cooling units.\n[O, B-Actor, O, B-Action, O, B-Property, O\nB-Object, I-Object, I-Object, I-Object]\nThe system shall report the temperature\nof heating and Cooling units.\n[O, B-Actor, O, B-Action, O, B-Property, O\nB-Property, O, I-Property,O]\nMessage Understanding Conference (MUC) introduced the set of error metrics in an\nassessment that can be explained in terms of comparing the predictions of a NER model\nagainst the ground truth[24]:\n• Correct (C): entity type and boundary are the same (Case I)\n• Incorrect (I): output of a system and the golden standard are not same (Case V)\n• Partial (P): system and the golden standard are partially “similar” but not the same (Case\nIV)\n• Missing (M): entity type is not captured by system (Case III)\n• Spurius (S): system produces an entity which is not present in the entity set (Case II)\nThe Semantic evaluation workshop’13 (SemEval’13) conceptualised four different ways to\nmeasure precision and recall based on the metrics defined by MUC [23]:\n10\n• Strict: exact match for boundary and entity type\n• Exact: exact boundary match irrespective of entity type match\n• Partial: partial boundary match irrespective of entity type match\n• Type: entity type match irrespective of the boundary match\nThe total number of entities present in the ground truth can be defined as ‘Possible\n(POS)’ (POS = C + I + P + M) measure, and total number of system predictions can be\ndefined as ‘Actual (ACT)’ (ACT = C + I + P + S). For exact and strict type of errors listed\nin SemEval[23] we define precision and recall values asPrecision = C\nACT andRecall = C\nPOS ,\nand for partial or type match errors asPrecision = C+O.5×P\nACT and Recall = C+O.5×P\nPOS . To\ncalculate these measures, we consider the best test folds of DOORS, SRE, and RQA having\n1253, 126, 126 instances. We extract the system predictions using the best NER model\nfor each dataset and compare them with ground truth annotations. Table 6 presents the\nerror values for each dataset in absolute number. The NER models miss 5.0%, 8.1% and\n29.4% entities in DOORS, SRE and RQA, respectively. In terms of spurious error, the NER\nmodels assign 8.2%, 19.0% and 10.8% of random entities to tokens present in DOORS, SRE,\nand RQA, respectively.\nTable 6. ACT (number of entities produced by NER system) and POS (expected number\nof entities) values obtained using five different types of error measures as defined by [24].\nDataset #Correct (C)#Incorrect (I)#Partial (P)#Missed (M)#Spurious (S)#Possible (POS)#Actual (ACT)\nType 5968.0 (94.0%)40.0 (0.6%)0.0 (0.0%)320.0 (5.0%)541.0 (8.2%) 6328.0 6549.0\nDOORSPartial6002.0 (94.8%) 0.0 (0.0%)6.0(0.09%)320.0 (5.0%)541.0 (8.2%) 6328.0 6549.0\nStrict 5964.0 (94.2%)44.0 (0.7%)0.0 (0.0%)320.0 (5.0%)541.0 (8.2%) 6328.0 6549.0\nExact 6002.0 (94.8%)6.0 (0.09%)0.0 (0.0%)320.0 (5.0%)541.0 (8.2%) 6328.0 6549.0\nType 531.0 (88.2%)22.0 (3.6%)0.0 (0.0%)49.0 (8.1%)129.0 (19.0%) 602.0 682.0\nSRE Partial 536.0 (89.0%) 0.0 (0.0%)17.0 (2.8%)49.0 (8.1%)129.0 (19.0%) 602.0 682.0\nStrict 516.0 (85.7%)37.0 (6.1%)0.0 (0.0%)49.0 (8.1%)129.0 (19.0%) 602.0 682.0\nExact 536.0 (89.0%)17.0 (2.8%)0.0 (0.0%)49.0 (8.1%)129.0 (19.0%) 602.0 682.0\nType 312.0 (68.5%) 9.0 (1.9%) 0.0 (0.0%)134.0 (29.4%)39.0 (10.8%) 455.0 360.0\nRQA Partial 321.0 (70.5%) 0.0 (0.0%) 0.0 (0.0%)134.0 (29.4%)39.0 (10.8%) 455.0 360.0\nStrict 312.0 (68.5%) 9.0 (1.9%) 0.0 (0.0%)134.0 (29.4%)39.0 (10.8%) 455.0 360.0\nExact 321.0 (70.5%) 0.0 (0.0%) 0.0 (0.0%)134.0 (29.4%)39.0 (10.8%) 455.0 360.0\nFigure 3 shows the precision and recall values for each error category for all the datasets.\nFor DOORS data, we observe similar precision (91%) and recall (94%) values, however,\nfor SRE dataset we get higher values of recall across all the error categories ranging from\n(88% to 90%), with precision values ranging from (77% to 79%). Because the RQA dataset\ncontains a large entity set and limited number of data instances, we obtain lower recall\nvalues (68% to 70%) in all the error categories.\nType Partial Strict Exact\nMUC Metrics\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0 Precision Recall\n(a) DOORS\nType Partial Strict Exact\nMUC Metrics\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0 Precision Recall (b) SRE\nType Partial Strict Exact\nMUC Metrics\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0 Precision Recall (c) RQA\nFigure 3. Precision and recall values obtained after evaluating the error categories [23].\n4.3. Few-shot learning results\nIt is usually difficult to obtain labeled textual data in software engineering domain. Ac-\ncordingly, it is important to test out the capabilities of the NLP models with limited amount\n11\nof available data. We used the BART-based approach [5] as the backbone in few-shot train-\ning, and compare its performance against vanilla BERT fine-tuning. Respectively, 4% ,8%,\n16%, 32%, 64% and 100% of the entire DOORS dataset used in a log-scale mode. Figure 4\ndemonstrates that the few-shot text-to-text model can provide relatively good performance\nwith a small number of labeled data instances. That is, few-shot learning is substantially\nmore sample efficient than vanilla BERT Fine-tuning. Even at only 16% of the original\ntraining set, the model achieves 82.5% accuracy value, which outperforms BERT (78.7%)\nsignificantly. On the other hand, BERT outperforms few-shot learning when all the available\ndata is used for training. Furthermore, the time complexity of the Template-NER model\nused for few-shot learning is very high. The model enumerates all possible text spans in the\noriginal input sentence as named entity candidates during inference. Afterward, it classifies\nthe candidates into named entities or non-entities based on model scores. Enumerating all\npossible text inevitably increases time complexity.\n0 20 40 60 80 100\nTraining Sample (%)\n0.65\n0.70\n0.75\n0.80\n0.85Accuracy\nFine Tuning BERT\nFew Shot Template\nFigure 4. Few-shot and vanilla BERT accuracy as a function of training set size\n5. Conclusions\nIn this study, we demonstrated the efficiency of transformers models for the extraction\nof software-specific entities using three requirement datasets (DOORS, SRE, and RQA).\nWe employed four different BERT variants, and compared them with the baseline ML-CRF\nmodel. In addition to standard NER metrics, we provided entity level error analysis to\nexamine the error cases in NER model predictions.\nIn general, BERT consistently provided the best performance for DOORS and SRE\ndataset, and ML-CRF performed the best for the RQA dataset. Error analysis showed\nthat the BERT model performed well for DOORS data as it only missed a small percentage\nof the entities, and detected the boundary and entity types in partial and exact matching\nwith high precision and recall. We also evaluated a few-shot NER model on the DOORS\ndataset, which was found to be a sample efficient technique, achieving better performance\nfor small training sets.\nOverall, fine-tuned transformer-based NER models showed promise to improve the pro-\ncess of software-specific entity extraction in generic SRS documents. These NER models\ncan also benefit other downstream NLP tasks in the requirement engineering domain such\nas requirement classification, ambiguity and conflict detection in software requirements, and\nrequirement quality assessment, which we aim to investigate as a future work. We also plan\nto explore other advanced few-shot learning methodologies, which provide opportunities to\nwork with other software requirement datasets that have few labeled instances. Lastly, data\n12\naugmentation techniques can be employed to improve the support for minority entities in\nour datasets, and can help enhance overall NER performance.\nReferences\n[1] J. Lafferty, A. McCallum, and F. C. Pereira. “Conditional random fields: Probabilistic models\nfor segmenting and labeling sequence data”. In: (2001).\n[2] J. Devlin et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Under-\nstanding. 2019. arXiv:1810.04805 [cs.CL].\n[3] Y. Liu et al. “Roberta: A robustly optimized bert pretraining approach”. In:arXiv preprint\narXiv:1907.11692 (2019).\n[4] Z. Lan et al. ALBERT: A Lite BERT for Self-supervised Learning of Language Representa-\ntions. 2020. arXiv:1909.11942 [cs.CL].\n[5] L. Cui et al. “Template-based named entity recognition using BART”. In: arXiv preprint\narXiv:2106.01760 (2021).\n[6] J. Tabassum et al.Code and Named Entity Recognition in StackOverflow. 2020. arXiv:2005.\n01634 [cs.CL].\n[7] M. Tikhomirov et al. “Using bert and augmentation in named entity recognition for cy-\nbersecurity domain”. In: International Conference on Applications of Natural Language to\nInformation Systems. Springer. 2020, pp. 16–24.\n[8] C. Zhou, B. Li, and X. Sun. “Improving software bug-specific named entity recognition with\ndeep neural network”. In:Journal of Systems and Software165 (2020), p. 110572.\n[9] A. Nayak, V. Kesri, and R. K. Dubey. “Knowledge graph based automated generation of\ntest cases in software engineering”. In:Proceedings of the 7th ACM IKDD CoDS and 25th\nCOMAD. 2020, pp. 289–295.\n[10] D.Yeetal.“Software-specificnamedentityrecognitioninsoftwareengineeringsocialcontent”.\nIn: SANER’16. Vol. 1. IEEE. 2016, pp. 90–101.\n[11] M. V. P. Reddy et al. “NERSE: Named Entity Recognition in Software Engineering as a\nService”. In:Service Research and Innovation. Springer, 2018, pp. 65–80.\n[12] Y. Yang and A. Katiyar.Simple and Effective Few-Shot Named Entity Recognition with Struc-\ntured Nearest Neighbor Learning. 2020. arXiv:2010.02405 [cs.CL].\n[13] S.S.S.Dasetal. CONTaiNER: Few-Shot Named Entity Recognition via Contrastive Learning.\n2021. arXiv:2109.07589 [cs.CL].\n[14] J. Li et al. “Few-Shot Named Entity Recognition via Meta-Learning”. In:IEEE Transactions\non Knowledge and Data Engineering(2020), pp. 1–1.doi: 10.1109/TKDE.2020.3038670.\n[15] A. Fritzler, V. Logacheva, and M. Kretov. “Few-shot classification in named entity recognition\ntask”. In:Proceedings of the ACM Symposium on Applied Computing. 2019, pp. 993–1000.\n[16] E. Hull, K. Jackson, and J. Dick. “DOORS: a tool to manage requirements”. In:Requirements\nengineering. Springer, 2002, pp. 187–204.\n[17] J. Cleland-Huang, M. Vierhauser, and S. Bayley. “Dronology: An incubator for cyber-physical\nsystem research”. In:arXiv preprint arXiv:1804.02423(2018).\n[18] A. Ferrari, G. O. Spagnolo, and S. Gnesi. “Pure: A dataset of public requirements documents”.\nIn: RE’17. IEEE. 2017, pp. 502–505.\n[19] A. Post and T. Fuhr. “Case study: How Well Can IBM’s\" Requirements Quality Assistant\"\nReview Automotive Requirements?” In:REFSQ Workshops. 2021.\n[20] S. Friedenthal, R. Griego, and M. Sampson. “INCOSE model based systems engineering\n(MBSE) initiative”. In:INCOSE 2007 symposium. Vol. 11. sn. 2007.\n[21] J. Li et al. “A survey on deep learning for named entity recognition”. In:IEEE Transactions\non Knowledge and Data Engineering(2020).\n[22] X. Yang et al. “Clinical concept extraction using transformers”. In:Journal of the American\nMedical Informatics Association27.12 (2020), pp. 1935–1942.\n[23] N. UzZaman et al. “Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events,\nand temporal relations”. In:SemEval’13. 2013, pp. 1–9.\n[24] N. Chinchor and B. M. Sundheim. “MUC-5 evaluation metrics”. In:Proceedings of the 5th\nconference on message understanding. 1993.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.814429759979248
    },
    {
      "name": "Transformer",
      "score": 0.620988667011261
    },
    {
      "name": "Software",
      "score": 0.5483047962188721
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4697117209434509
    },
    {
      "name": "Software engineering",
      "score": 0.43712788820266724
    },
    {
      "name": "Data mining",
      "score": 0.41830331087112427
    },
    {
      "name": "Named-entity recognition",
      "score": 0.4113900363445282
    },
    {
      "name": "Natural language processing",
      "score": 0.4005741775035858
    },
    {
      "name": "Machine learning",
      "score": 0.32836055755615234
    },
    {
      "name": "Programming language",
      "score": 0.17077213525772095
    },
    {
      "name": "Systems engineering",
      "score": 0.09720709919929504
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I530967",
      "name": "Toronto Metropolitan University",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I118036225",
      "name": "Istanbul Bilgi University",
      "country": "TR"
    },
    {
      "id": "https://openalex.org/I1341412227",
      "name": "IBM (United States)",
      "country": "US"
    }
  ]
}