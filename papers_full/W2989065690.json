{
  "title": "BERT-CNN: a Hierarchical Patent Classifier Based on a Pre-Trained Language Model",
  "url": "https://openalex.org/W2989065690",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2227936229",
      "name": "Lu Xiaolei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098619761",
      "name": "Ni Bin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963626623",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2934032625",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2084319893",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W1971579776"
  ],
  "abstract": "The automatic classification is a process of automatically assigning text documents to predefined categories. An accurate automatic patent classifier is crucial to patent inventors and patent examiners in terms of intellectual property protection, patent management, and patent information retrieval. We present BERT-CNN, a hierarchical patent classifier based on pre-trained language model by training the national patent application documents collected from the State Information Center, China. The experimental results show that BERT-CNN achieves 84.3% accuracy, which is far better than the two compared baseline methods, Convolutional Neural Networks and Recurrent Neural Networks. We didn't apply our model to the third and fourth hierarchical level of the International Patent Classification - \"subclass\" and \"group\".The visualization of the Attention Mechanism shows that BERT-CNN obtains new state-of-the-art results in representing vocabularies and semantics. This article demonstrates the practicality and effectiveness of BERT-CNN in the field of automatic patent classification.",
  "full_text": "基于预训练语言模型的 BERT-CNN 多层级\n专利分类研究\n陆晓蕾\n1\n倪斌\n2\n1. 厦门大学外文学院 厦门 361000; 2. 深湃信息科技 （深圳）有限公司 深圳 518000;)\n摘要: 专利文献的自动分类对于知识产权保护、专利管理和专利信息检索十分重要，构建准确的专\n利自动分类器可以为专利发明人、专利审查员提供辅助支持。本文以专利文献分类为研究任务，选取\n国家信息中心公布的全国专利申请信息为实验数据，提出了基于 预训练语言模型的 BERT-CNN 多层\n级专利分类模型。实验结果表明，在该数据集上， BERT-CNN 模型在准确率上达到了 84.3%，大幅度\n领先于卷积神经网络和循环神经网络等其他深度学习算法。 本论文的局限性在于未将模型扩展到更深\n层级的分类中 。通过 BERT 抽取的向量在表达词汇与语义方面 具有强大的性能，在专利自动分类领域\n具备实际应用效果。\n关键词: 专利；文本分类；深度学习； BERT\n分类号: G254.1\nBERT-CNN: a Hierarchical Patent Classifier Based on a \nPre-Trained Language Model \nLu Xiaolei1   Ni Bin2\n1. School of Foreign Languages and Cultures, Xiamen University, Xiamen 361000; 2. DeepAI Tech Co., Ltd, \nShenzhen 518000;\nAbstract：The automatic classification is a process of automatically assigning text documents to predefined \ncategories. An accurate automatic patent classifier is crucial to patent inventors and patent examiners in terms of \nintellectual property protection, patent management, and patent information retrieval. We present BERT-CNN, a \nhierarchical patent classifier based on pre-trained language model by training the national patent application \ndocuments collected from the State Information Center, China. The experimental results show that BERT-CNN\nachieves 84.3% accuracy, which is far better than the two compared baseline methods, Convolutional Neural \nNetworks and Recurrent Neural Networks. We didn’t apply our model to the third and fourth hierarchical level of \nthe International Patent Classification — “subclass” and “group”.The visualization of the Attention Mechanism \nshows that BERT-CNN obtains new state-of-the-art results in representing vocabularies and semantics. This article \ndemonstrates the practicality and effectiveness of BERT-CNN in the field of automatic patent classification.\nKeywords： Patent; Text Classification; Deep Learning; BERT\n1 引言\n近年来，工业界和学术界产生了大量专利申请。据世界知识产权保护组织（WIPO）的指导\n                                                  \n收稿日期：2019-09-07；\n基金项目：1.教育部人文社科基金青年项目  “一带一路战略下涉外法律机器翻译云平台的构建及应用研究 ，项目\n编号 18YJCZH117  2.中央高校基本科研项目  基于语义模型的机器翻译研究 ，项目编号 20720191053\n作者简介： 陆晓蕾，女， 1988 年生，博士，助理教授，硕导，主要研究领域：计算语言学；倪斌，男， 1990 年\n生，硕士，主要研究领域：自然语言处理， Email: nibiner@live.cn 。 \n性报告显示：2017 年，全球所有专利局共受理的专利申请超过三百万个，并且这项数据每年还\n会以更快的速度增长。出于检索和管理需要，专利文献需按照专业技术领域进行分类与赋码。\n1971 年《斯特拉斯堡协定》提出的国际专利分类法（IPC）是国际上通用的专利文献分类法，几\n乎覆盖了所有科技领域，超过 90 个国家使用该标准。该分类标准提供了一种由独立于语言的符\n号构成的树形体系，按所属技术领域对发明专利和实用新型专利进行分类。IPC 分类包含“部\n（section）-类（class）-亚类（subclass）-组（group）”四个层级，其中“组”级共含有74503 种\n类别（WIPO，2019）。过去，传统的人工分类受限于专利员的自身知识与经验，难以做到快速和\n精确。随着专利申请数量的剧增，IPC 分类法的多层级、多标签等特点给专利审查人员带来了巨\n大的工作量。同时， 快速的全球化竞争以及完善的知识产权保护体系对文献分类技术提出了更高\n的要求。 随着深度学习和文本分类的快速发展，越来越多的研究开始聚焦于专利文献的自动分类。\n目前，面向自然语言处理的专利分类研究多集中于英文专利申请，然而，中国的专利申请数量占\n全球比重达 43.6%[1]，构建一个相对精确的中文专利分类器的需求也日益凸显。本文试图利用基\n于卷积神经网络、循环神经网络和 BERT 预训练模型等深度学习手段建立中文专利文献分类器，\n提出了 BERT-CNN 模型，并探索多层级分类方法，为快速准确的专利文本分类提供参考。\n2 相关研究\n过去，大量的专利文献自动分类研究主要集中于朴素贝叶斯[2]、支持向量机[3]和 K-近邻算法\n[4]等统计方法。Fall 等[5]对这些算法进行了比较评估，阶段性地总结了这三种分类器的效果，提\n出了支持向量机是当时最好的分类器。D’hondt 等[6]从多元短语组等统计语言学的角度对专利文\n档进行了分析，在一定程度上提高了专利分类的准确性。姜春涛等[7]建立基于图结构的文本分类\n器对四个不同技术领域的专利进行分类。然而，以上方法大都需要人工特征工程，建模过程费时\n费力，准确性也存在瓶颈。随着神经网络模型在自然语言处理上的广泛应用，尤其是以词向量为\n代表的文本表征技术的进步[8]，文本分类的效果得到了显著提升。Grawe 等[9]利用专利文献训练\nWord2Vec 词向量模型，在此基础上仅叠加一层 LSTM 作为分类器，获得了较好的分类效果。类\n似的，Shalaby 等[10]基于 LSTM 训练固定层次结构向量（Fixed Hierarchy Vectors）对文档表示进\n行了增强学习，极大提升了分类的准确性。可见，在专利文档分类的研究中，词或文档表示的效\n果好坏是构建一个快速并且稳定的分类器的关键。\n传统的词向量，如Word2Vec[8]和 Glove[11]，主要通过建立语言模型[12]获得，在表达多义词方\n面存在较大缺陷。为此，Peters 等[13]提出的 Elmo 模型利用双向长短期记忆网络（Bi-LSTM）生\n成词的上下文表示，在获得预先训练的词向量之后根据实际数据的上下文进行向量表达的动态\n调整。Alec 等[14]提出基于Transformer 的生成式预训练词向量模型（Generative Pre-Training，GPT），\n在多项 NLP 任务中获得了当时的最高分数。Devlin 等[15]吸收了 Elmo 和 GPT 模型的优势，提出\n了基于 Transformer 的双向编码器模型（BERT），凭借其优秀的表达词句能力，在 GLUE 排行榜\n上， 刷新了多达11 项NLP 任务的记录。BERT 预训练模型可以根据具体下游任务进行参数微调。\n在此基础上，通过简单的接口处理其它自然语言处理任务，如文本分类。目前，BERT 已经广泛\n应用于如命名实体识别[16]、阅读理解问答[17]等各项自然语言处理任务中，自然语言处理自此进\n入了预训练模型的大规模应用新阶段。虽然 BERT 提供了下游任务的简单接口，可以直接进行文\n本分类。 然而，BERT 作为预训练语言模型，关于其作为文档向量的研究和应用尚不多见。过去，\nfastText[18][19]通过将整篇文档的词及 n-gram 向量叠加平均得到文档向量，然后使用文档向量直接\n做 softmax 多分类，在处理简单文本分类中获得了较好的效果 。但是这种利用词向量（ word \nembedding）表达文本的方式无法区分多义词。而 BERT 在预训练的基础上，根据具体语句进行\n参数微调，在某种程度上缓解了这一问题。因此，本文提出了 BERT-CNN 模型，尝试将BERT 作\n为编码器，将专利摘要映射成文档向量，在其上叠加更加复杂的卷积神经网络。\n3 研究方法\n3.1 BERT-CNN 模型结构\n本文基于 BERT 预训练语言模型，提取其顶部四层 Transformer 的输出作为专利摘要的文档\n向量，与卷积神经网络进行联合训练（图 2）。\n3.1.1 BERT 层\nBERT 的英文全称是 Bidirectional Encoder Representations from Transformers ，采用双向\nTransformer 编码器[20]，利用多头注意力机制融合了字左右的上下文信息。同时，与早期通过训\n练语言模型的目标任务——“预测下一个词”[12]不同的是，BERT 设置了两种目标任务分别获取\n单词与句子级别的表义方式：1）遮盖语言模型；2）上下句关系预测。其中，遮盖语言模型类似\n“完形填空” ，即随机遮盖 15%的句子，让编码器预测这些词；上下句关系预测通过预测两个随\n机句子能否组成上下句来学习句子间的关系。通过这样训练出来的模型，BERT 具有很强的句词\n表达能力，无论是在字词级别的 NLP 任务， 如命名实体识别， 还是在如问答类的句子级别的NLP\n任务中，都具有卓越的表现。在BERT 的顶层，可以直接叠加简单的线性模型，结合具体的任务\n（specific-task）做参数微调（fine-tune）来完成其他 NLP 任务，如文本分类。\n图 1 BERT 网络架构（Nx 为Transformer 的层数）\nBERT 模型的详细结构见图 1，由多层双向 Transformer 构成，每个 Transformer 利用多头自\n注意力机制（Multi-Head Attention）建立词与词之间的联系强弱（权重）。谷歌开源了两种不同规\n模的 BERT 模型，分别为BERT-Base 和 BERT-Large[15]。其中，BERT-Base 拥有 12 个 Transformer\n层，768 个隐含单元，12 个自注意力层，总共含有1.1 亿个参数；BERT-Large 拥有24 个Transformer\n层，1024 个隐含单元，16 个自注意力层，总共含有 3.4 亿个参数。由于计算资源限制，一般研\n究者多采用前者，本文亦选取 BERT-Base 作为预训练模型。BERT-CNN 采用 BERT 中的后四层\nTransformer 层的输出作为下游 CNN 模型的输入。\n\n图 2 BERT-CNN 模型结构图\n3.1.2 Conv2D 层\n卷积神经网络（CNN）在大部分情况下用于图像识别，但是在自然语言处理中也获得了很好\n的效果[21]。BERT 中每一层 transformer 的输出都可以作为句向量（维度为 768 的向量） ，本文取\n其最后四层 L12、L11、L10、L9 作为 CNN 的输入矩阵 I（768×4）。然后用 32 个滤波器 F（3×4），\n步长为 1，扫描输入矩阵 I，目的是提取文本 3-Gram 特征，通过 IF 内积获得 32 个特征向量。\n为了降低计算的复杂度，CNN 通常使用池化计算降低矩阵的维度。池化计算有最大池化（Max-\nPooling）与平均池化（Average-Pooling）两种方式，一般采用最大池化，即在池化窗口中选取最\n大的元素。由于专利文本分类对于局部的某些关键词或术语比较敏感，本文选取最大池化方式。\n经过最大池化层后拼接并通过 Softmax 层获得专利分类的概率分布。在网络训练过程中，滤波器\n的参数是不共享的，并且与网络间的连接参数同时更新。\n3.2 多层次分类架构\n传统的分类方法在处理多层级任务时没有区分层级，将其视为普通的多分类（multi-class）\n任务，这在处理类别有限的任务时简单有效。但是，随着层级和类别的增加，其效果表现越来越\n差。尤其是在图书分类或者专利分类等多层级任务中，类别数量庞大（如IPC 中“组”的分类数\n高达 74503 种） ，算法的准确率随着类别的增加急剧下降。另外，传统分类方法将所有类别独立\n看待，在某种程度上忽视了类别之间的联系。例如，在IPC 分类中， “A21C3/00”与“A21C7/00”\n在专业领域上很接近，而与“D21C7/00”相去甚远。然而，这种方法将三者看成独立的类别，没\n有任何联系，这显然是不恰当的。\n\n图 3 本文的多层级分类网络架构\n针对专利分类的多层级问题，本文提出了一种堆叠式的神经网络组合训练模型（图 3）。模\n型分为两层 L1 和 L2，其中 L1 为一级分类层，L2 为二级分类层。模型首先获取所有训练样本输\n入，经过不同的分类器（本文中采用 RNN/CNN/BERT-CNN）后训练一级分类 YL1。随后，在中\n间设置数据转换层。该层的作用是根据 L1 层的标签筛选 L2 层网络。具体做法是根据 L1 层的标\n签值 Ym 获取所有一级分类为 Ym 的样本作为 L2 层的训练样本。例如，若 L1 层输出为‘Y2’ ，则\nL2 层将只获取所有一级分类为‘Y2’的数据进行训练。\n4 试验与结果\n4.1 数据集\n本文采用国家信息中心提供的全国专利申请数据1。数据总量达到277 万条。 时间跨度为2017\n年全年（按照专利申请时间统计） ，地域覆盖全国，数据主要包含共 16 个核心字段：申请号、申\n请日、公开号、公开日、专利名称、专利分类、摘要、申请（专利权）人、发明人、申请人地址、\n申请人邮编、所在省代码、所在市代码、所在县代码、法律状态、有效性。数据格式为CSV，编\n码格式为 GBK。本实验提取“摘要”和“专利分类”两个字段进行分类模型训练。专列分类采\n用 IPC 分类法，该分类法将专利分为“部（section）-类（class）-亚类（subclass）-组（group）”\n四个层级，如“A01D42/04” ， “A”是部， “01”是类， “D”是亚类， “42/04”是组。本文为简单\n起见，仅选取前两层进行分类，即部和类。\n                                                  \n1 国家信息中心, 2017, “发明专利数据”, http://dx.doi.org/10.18170/DVN/ASRTHL, 北京大学开放研究数据平\n台, V2\n\n图 4 预处理后的专利数据集\n经过剔除无效数据（分类号为非严格的IPC 分类）等预处理操作后，原始数据剩余约 231 万\n条（数据结构见图 4）。该数据集包含了 IPC 的所有部（字母 A 至 H）和类（130 个） 。其中，平\n均每个类含有 17828 条文本。详细的文本与分类统计见表 1。\n表 1 专利数据集文本与类别相关统计\n部名 含义（参考国家知识产权局2） 类数 文本数 每类的平均文本数\nA 人类生活必需 16 327537 20471\nB 作业；运输 38 664602 17490\nC 化学；冶金 21 201984 9618\nD 纺织；造纸 9 37673 4186\nE 固定建筑物 8 155250 19406\nF 机械工程；照明；加热；武器；爆破 18 252325 14018\nG 物理 14 378874 27062\nH 电学 6 299456 49909\n总计 130 2317701 17828\n通过分析发现该数据集存在如下问题：①各部中含有类的数目和文本数不均。其中，B 部的\n类别最多，达到 38 个类；H 部类别最少，只有6 个类。其次，各部和各类文本数量分布不均衡。\n②部间存在相似性问题。例如“F：照明” 、 “G：物理”和“H：电学”三者存在较强的相似性，\n人工也难以准确区分。数据的稀疏性与类别的相似性都会对模型的准确性造成一定影响。面对这\n种情况，工业界通常会对数据进行增强。本文的目的是比较 BERT-CNN 和其他基线模型的相对\n性能，这两大问题不对结果产生影响。\n4.2 实验配置\n本文实验代码基于 python 3.7.3，深度学习框架主要利用 TensorFlow r1.13，计算机处理器为\nIntel i7 的 12 核处理器，GPU 显卡为 Nvidia 2080Ti，运行内存为 32G。\n本实验选取卷积神经网络和循环神经网络作为对照基线模型。在基线系统中，我们使用 300\n维 word2vec 中文词向量[22]。文本最大长度 L 设为 200 个词，网络训练的批处理（batch size）大\n小为 20，Epoch 为 20，学习率η为 2e-5。BERT-CNN 模型使用中文 BERT_Basic 模型(Chinese_L-\n12_H-768_A-12)，学习率为 2e-5，batch_size 为 24，最大文本长度 L 为 200，Epoch 为 20，优化\n器为 Adam。BERT-CNN 中的 CNN 滤波器个数为 32，激活函数为 ReLU。\n本文按 9:1 将数据集划分为训练集与测试集。基线方法的数据经过了分词预处理后利用第三\n                                                  \n2\nhttp://epub.sipo.gov.cn/ipc.jsp\n\n方训练的词向量。BERT-CNN 模型不需要分词。\n4.3 结果分析与讨论\n4.3.1 评估指标\n本文模型使用正确率作为评估指标，最终联合模型的正确率Acc(X)通过以下公式计算：\n                         Acc(X) = 𝐴𝑐𝑐(𝐿1)∑\n𝑁𝑗\n𝑀 𝐴𝑐𝑐(𝐿2𝑚)𝑚                     （1）\n其中，𝐴𝑐𝑐(𝐿1)代表第一层𝐿1模型的预测正确率，𝑚代表第一层分类（部）的数量，𝑀代表所\n有样本的数量，𝑁𝑗表示部 j 含有的二级分类（类）的样本数。𝐴𝑐𝑐(𝐿2𝑚)表示第二层（类）模型的\n正确率。\n4.3.2 优化器\n我们使用Adam 作为深度学习网络的梯度下降优化器[23]。Adam 结合了AdaGrad 和RMSProp\n两种优化器的优势，综合考虑了一阶距估计（First Moment Estimation，即梯度的均值）m𝑡和二阶\n矩估计（Second Moment Estimation，即梯度的未中心化的方差）v𝑡的计算结果得出更新步长。\n                          θ𝑡 ← θ𝑡−1 −\n𝛼\n√𝜈̂+𝜖 𝑚̂                           （2）\n                              𝑔𝑖,𝑡 = ∇𝜃𝐽(θ𝑖,𝑥𝑖,𝑦𝑖)                           （3）\n                              m𝑡 = 𝛽1𝑚𝑡−1 +(1−𝛽1)𝑔𝑖,𝑡                    （4）\n                              v𝑡 = 𝛽2𝑚𝑡−1 +(1−𝛽2)𝑔𝑖,𝑡\n2                      （5）\n其中，𝑚̂ =\nm𝑡\n1−𝛽1\n𝑡，𝜈̂ =\n𝑣𝑡\n1−𝛽2\n𝑡。Adam 适用于不稳定目标函数，在大规模的数据及参数场景中表\n现出众。由于Adam 克服了梯度稀疏的问题，实现简单，计算高效，是深度学习中默认较为优秀\n的优化器。\n4.3.3 实验结果\n表 2 基于预训练模型的BERT-CNN 与基线方法的实验结果（准确率%）\n模型 Acc(L1) Acc(L2) Acc(L 总)\nRNN 80.9\nA 91.8 B 79.5\n86.3 69.8\nC 89.2 D 84.7\nE 88.1 F 88.6\nG 90.2 H 92.4\nCNN 81.7\nA 92.4 B 78.0\n85.8 70.1\nC 87.5 D 85.3\nE 87.3 F 88.7\nG 85.9 H 91.3\n本文模型\nBERT-CNN\n90.5\nA 96.5 B 89.6\n93.1 84.3\nC 94.1 D 92.7\nE 95.2 F 93.8\nG 92.3 H 95.7\n表 2 是我们的实验结果，L1 表示模型预测“部”的准确性，L2 表示预测单独各“类”的准\n确性，L2 总表示预测第二层“类”的准确性，是通过计算L2 的加权和得出的。实验基线系统选取\n常用的卷积神经网络与循环神经网络分类器。 在该数据集上，BERT-CNN 模型的分类效果最好，\n在单独层的分类中的准确率基本都超过了 90%，总体分类准确率达到了 84.3%，与 CNN、RNN\n相比提升了 14%左右。这种大幅度的提升进一步证实了基于预训练模型的 BERT-CNN 在文本分\n类方面的强大性能。另外，可以看出 CNN 和 RNN 在该数据集上分类性能相仿，在面对大数据\n量和类别较多的情况下还会存在准确率下降的问题。 例如， 基线方法在处理部级分类时比类级分\n类的准确率低 4%，显示出在大数据量上的性能下降。同时两种基线方法在类别较多的B 类上准\n确率衰减比 BERT-CNN 较为严重。实验结果进一步表明本文模型在处理大数据量和多类别上的\n鲁棒性。\n4.3.4 讨论\n上述的结果已经表明，使用基于预训练模型的 BERT-CNN 在提高专利分类准确性方面具有\n较好的效果。BERT 所提供的文档向量具有良好的字词与语义表征能力，其基于预训练的微调可\n以有效解决传统词向量一词多义的问题，这是模型获取高准确率的关键。\n图 5 BERT 模型的注意力机制可视化\nBERT 在底层使用双向 Transformer， 通过多头注意力机制可以获取字词之间的联系。因此，\n对注意力的可视化研究可以直观地了解 BERT 模型的注意力机制，帮助解释模型。图5 利用 Vig\n等[24]开源的注意力可视化工具 bertviz 进行模型可视化分析。该工具能用于探索预训练的 BERT\n模型各个层以及头部的注意力模式。\n以本文的专利分类为例，输入以下两个句子：\n句子 A：本实用新型公开了一种固体绝缘开关柜结构。\n\n句子 B：它包括隔离开关单元、接地装置和隔离插座装置。\n图 5(a、b、c)和图 5(d、e、f)分别是 BERT 第二层与第十一层的注意力机制的可视化，将注\n意力可视化为被更新位置（左）和被关注位置（右）之间的连线。线段的颜色代表不同的“注意\n力头” ，线段宽度反映注意力值的大小。BERT 使用[CLS]进行分类任务的第一个输入，[SEP]是句\n子的分隔。图 5a 显示，在第二层中，注意力主要体现词汇关系：句中的每个字的注意力主要集\n中在其下一个字，如“实”的注意力集中在“用” 。图5b、c 显示，句间的注意力主要集中在相\n似的词，如“开” 、 “关”之间的强烈注意力联系。图 5(d、e、f)显示，在第十一层中，注意力主\n要体现在语义关系上，如句子 A 中的“固体绝缘开关柜结构”与句子 B 中的“隔离开关单元” 、\n“接地装置”和“隔离插座装置”具有较强的注意力强度。\n基于以上发现，我们认为 BERT 中的每一层 Transformer 的输出都可以作为句子或文档向量\n为其他模型提供输入。具体选取 Transformer 的哪些层作为文档向量在某种程度上也会影响模型\n的准确率。本文针对该问题，对数据集中的 D 部进行了平行对照实验。从图 6 可以看出，模型\n的准确率在 Transformer 层数 N=4 时达到最大。当 N<4 时，Transformer 的输出作为文档向量的\n代表性还不太强，准确率略有下降；当 N>4 时，文档向量中表征词汇语法关系的成分增大，对\n分类结果意义不大，反而造成干扰和准确率的下降。因此，本文认为利用 BERT 预训练提供的向\n量与其它模型融合时，应该根据具体的任务特点进行合理选择。\n图 6 不同Transformer 层数作为输入的分类准确率直方图\n5 结语\n文档分类是文献管理与信息处理中的重 点和难点。尤其是在专利文献这种多层级的分类领\n域中，传统的统计方法准确率较低，且建模过程费时费力。本文选取国家信息中心公布的 2017\n年全国专利申请数据，以专利的自动分类作为研究目标，在预训练模型 BERT 的基础上抽取文档\n向量，提出了 BERT-CNN 分层级深度网络分类模型。本文提出的方法在该专利数据集上获得了\n84.3%的准确率，大幅度领先过去的卷积神经网络与循环神经网络分类法。实验结果表明，基于\n预训练语言模型的 BERT-CNN 提取的向量表示，在应用上效果优于传统的词向量。同时，在处\n理数据量和类别较多的任务上，该模型具备稳定性强的特点。进一步实验表明，Transformer 层\n数的选取对模型的准确率有一定的影响，应该根据具体的任务特点选择合适的输出层。\n本论文的局限性在于未将模型扩展到更深层级的分类中。 在后续研究中， 将扩展模型层级到\n“组” ，以应用于实际的专利自动分类中；同时，探索更多的预训练模型（例如GPT-2）来进一\n步丰富专利文献分类领域的研究。\n\n参考文献:\n[1] World Intellectual Property Organization, World Intellectual Property Indicators . Geneva: \nWIPO, 2018.\n[2] Mccallum A, Nigam K . A Comparison of Event Models for Naive Bayes Text \nClassification[C]//AAAI-98 Workshop On Learning for Text Categorization. 1998, 752(1): 41-48.\n[3] Joachims T . Transductive Inference for Text Classification Using Support Vector \nMachines[C]//ICML. 1999, 99: 200-209.\n[4] Hastie T, Tibshirani R. Discriminant Adaptive Nearest Neighbor Classification and \nRegression[C]//Advances in Neural Information Processing Systems. 1996: 409-415.\n[5] Fall C J, TörcsvÁRi A, Benzineb K, et al. Automated Categorization in The International Patent \nClassification[C]//Acm Sigir Forum. ACM, 2003, 37(1): 10-25.\n[6] D’Hondt E, Verberne S, Koster C, et al. Text Representations  for Patent Classification [J]. \nComputational Linguistics, 2013, 39(3): 755-775.\n[7] 姜春涛. 运用图示法自动提取中文专利文本的语义信息[J]. 图书情报工作, 2015, 59(21): \n115-122.\n(Jiang Chuntao. Applying Graph Representations to Automatic Extraction of Semantic Information \nfrom Chinese Patent Text [J]. Library and Information Service, 2015, 59(21):115-122.)\n[8] Mikolov T, Sutskever I, Chen K, et al. Distributed Representations of Words and Phrases and \nTheir Compositionality[C]//Advances in Neural Information Processing Systems. 2013: 3111-3119.\n[9] Grawe M F, Martins C A, Bonfante A G. Automated Patent Classification Using Word \nEmbedding[C]//2017 16th IEEE International Conference on Machine Learning and Applications \n(ICMLA). IEEE, 2017: 408-411.\n[10] Shalaby M, Stutzki J, Schubert M, Et Al. An LSTM Approach to Patent Classification Based \nOn Fixed Hierarchy Vectors[C]//Proceedings of the 2018 SIAM International Conference on Data \nMining. Society for Industrial and Applied Mathematics, 2018: 495-503.\n[11] Pennington, Jeffrey, Richard Socher, And Christopher Manning . “Glove: Global Vectors for \nWord Representation.” Proceedings of The 2014 Conference On Empirical Methods in Natural \nLanguage Processing (EMNLP). 2014.\n[12] Bengio Y , Ducharme R, Vincent P, Et Al. A Neural Probabilistic Language Model[J]. Journal \nof Machine Learning Research, 2003, 3(Feb): 1137-1155.\n[13] Peters M E, Neumann M, Iyyer M, Et Al. Deep Contextualized Word Representations[J]. arXiv \npreprint arXiv:1802.05365, 2018.\n[14] Alec Radford, Karthik Narasimhan, Tim Salimans, And Ilya Sutskever. Improving Language \nUnderstanding with Unsupervised Learning. Technical Report, OpenAi, 2018.\n[15] Devlin J, Chang M W, Lee K, Et Al. Be rt: Pre-Training of Deep Bidirectional Transformers \nFor Language Understanding[J]. arXiv preprint arXiv:1810.04805, 2018.\n[16] 杨飘, 董文永. 基于BERT 嵌入的中文命名实体识别方法[J]. 计算机工程, 2019.\n(Yang Piao, Dong Wenyong. Chinese NER based on BERT Embedding [J]. Computer Engineering, \n2019.)\n[17] 蔡鑫怡, 姜威宇, 韩浪焜等. Bert 在中文阅读理解问答中的应用方法[J]. 信息与电脑(理\n论版), 2019(08): 39-40.\n(Cai Xinyi, Jiang Weiyu, Han Langkun el al. Method of Modified Bert Model for Chinese Machine \nComprehension [J]. China Computer & Communication, 2019(08):39-40.)\n[18] Joulin, A., Grave, E., Bojanowski, P., et al. Bag of Tricks for Efficient Text Classification[J]. \narXiv preprint arXiv:1607.01759, 2016.\n[19] Bojanowski, P., Grave, E., Joulin, A. , et al. Enriching Word Vectors with Subword \nInformation[J]. Transactions of the Association for Computational Linguistics 5 (2017): 135-146.\n[20] Vaswani A, Shazeer N, Parmar N, Et Al. Attentio n Is All You Need[C]//Advances in Neural \nInformation Processing Systems. 2017: 5998-6008.\n[21] LeCun, Y ., Bottou, L., Bengio, Y . et al. Gradient -Based Learning Appli ed to Document \nRecognition. Proceedings of the IEEE, 86(11), 2278-2324, 1998.\n[22] Shen Li, Zhe Zhao, Renfen Hu.  Analogical Reasoning On Chinese Morphological and \nSemantic Relations, ACL 2018.\n[23] Kingma D P, Ba J. Adam: A Method for Stochastic Optimization [J]. arXiv preprint \narXiv:1412.6980, 2014.\n[24] Vig J. Visualizing Attention in Transformer -Based Language Models [J]. arXiv preprint \narXiv:1904.02679, 2019.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7827838063240051
    },
    {
      "name": "Convolutional neural network",
      "score": 0.7526350617408752
    },
    {
      "name": "Classifier (UML)",
      "score": 0.719046413898468
    },
    {
      "name": "Artificial intelligence",
      "score": 0.697751522064209
    },
    {
      "name": "Intellectual property",
      "score": 0.43197867274284363
    },
    {
      "name": "Machine learning",
      "score": 0.4178353250026703
    },
    {
      "name": "Natural language processing",
      "score": 0.38679465651512146
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": []
}