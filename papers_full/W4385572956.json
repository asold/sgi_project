{
  "title": "Improving Stability of Fine-Tuning Pretrained Language Models via Component-Wise Gradient Norm Clipping",
  "url": "https://openalex.org/W4385572956",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2231274380",
      "name": "Chenghao Yang",
      "affiliations": [
        "University of Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A2250429641",
      "name": "Xuezhe Ma",
      "affiliations": [
        "Southern California University for Professional Studies",
        "University of Southern California"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2975185270",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3034199299",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W4287867774",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2979826702"
  ],
  "abstract": "Fine-tuning over large pretrained language models (PLMs) has established many state-of-the-art results. Despite its superior performance, such fine-tuning can be unstable, resulting in significant variance in performance and potential risks for practical applications. Previous works have attributed such instability to the catastrophic forgetting problem in the top layers of PLMs, which indicates iteratively fine-tuning layers in a top-down manner is a promising solution. In this paper, we first point out that this method does not always work out due to the different convergence speeds of different layers/modules. Inspired by this observation, we propose a simple component-wise gradient norm clipping method to adjust the convergence speed for different components. Experiment results demonstrate that our method achieves consistent improvements in terms of generalization performance, convergence speed, and training stability. The codebase can be found at https://github.com/yangalan123/FineTuningStability.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4854–4859\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nImproving Stability of Fine-Tuning Pretrained Language Models\nvia Component-Wise Gradient Norm Clipping\nChenghao Yang1, Xuezhe Ma2\n1University of Chicago\n2University of Southern California\nyangalan1996@gmail.com, xuezhema@isi.edu\nAbstract\nFine-tuning over large pretrained language\nmodels (PLMs) has established many state-\nof-the-art results. Despite its superior perfor-\nmance, such fine-tuning can be unstable, re-\nsulting in significant variance in performance\nand potential risks for practical applications.\nPrevious works have attributed such instabil-\nity to the catastrophic forgetting problem in\nthe top layers of PLMs, which indicates itera-\ntively fine-tuning layers in top-down manner is\na promising solution. In this paper, we first\npoint out that this method does not always\nwork out due to different convergence speeds\nof different layers/modules. Inspired by this\nobservation, we propose a simple component-\nwise gradient norm clipping method to adjust\nthe convergence speed for different compo-\nnents. Experiment results demonstrate that our\nmethod achieves consistent improvements in\nterms of generalization performance, conver-\ngence speed and training stability. The code-\nbase can be found at https://github.com/\nyangalan123/FineTuningStability.\n1 Introduction\nFine-tuning over large pretrained language models\n(PLMs), which achieved remarkable performance\nover various benchmarks, has become the de facto\nparadigm for several current natural language pro-\ncessing (NLP) systems. However, fine-tuning can\nbe unstable in terms of significant variance in met-\nrics, resulting in even worse-than-random failed\nmodels (Devlin et al., 2019; Lee et al., 2019; Dodge\net al., 2020; Mosbach et al., 2020).\nCatastrophic forgetting (Kirkpatrick et al., 2017)\nduring fine-tuning of PLMs is one common expla-\nnation for this instability (Lee et al., 2019), i.e.,\nPLMs may lose their rich domain-agnostic knowl-\nedge acquired by language model pretraining in the\nprocess of fine-tuning. Through layer-replacement\nexperiments between pretrained models and fine-\ntuned models, Mosbach et al. (2020) further con-\n0 5 10 15 20 25\n#(Iterations)\n50\n55\n60\n65\n70\n75Accuracy\nRTE Fine-Tuning Performance Change Across Time\nDevlin et al., 2019 baseline\nDevlin et al., 2019 baseline \n+ bias correction\nCWGNC (ours)\nGradual Unfreezing\nGradual Unfreezing (Restart)\nFigure 1: Fine-tuning performance over time on RTE\ndatasets. Here we fine-tune over BERT-large-uncased\nmodel (Devlin et al., 2019). In each iteration, we train\nthe model for 3 epochs and the errorbar is plotted based\non 5 different runs.\nnected the catastrophic forgetting problem to the\noptimization problem on top layers.\nThese findings give rise to a straightforward way\nto enhance the fine-tuning stability: how about fine-\ntuning the model from top to bottom to reduce the\nparameter changes and hence mitigate the catas-\ntrophic forgetting problem? This is reminiscent of\nthe gradual unfreezing (Howard and Ruder, 2018),\nwhich does layer-wise top-down fine-tuning and\nunfreezing new layers only when the layers above\nhave been fine-tuned. Therefore, the newly un-\nfrozen layer would only be tuned for a slightly eas-\nier optimization problem at each iteration, leading\nto much fewer changes to the parameters.\nHowever, based on a comprehensive case study\nof the gradual unfreezing method, we obtained em-\npirical observations beyond our expectations (§2).\nOur analysis further reveals a possible reason: dif-\nferent components (e.g., feed-forward networks at\ndifferent layers, fully connected matrices and bi-\nases at output layer) converge at varying speeds.\nThus, components in upper layers, which have con-\nverged to local optima, cannot easily be fine-tuned\n4854\nwith newly unfrozen parameters.\nBased on this observation, we propose a simple\ncomponent-wise gradient clipping method to sta-\nbilize the fine-tuning process (§3). This method\nachieves significant empirical improvements of\nfine-tuning stability in terms of the variance and\nthe failed run percentage over three tasks (§4). In\nsummary we make the following contributions:\n1. We find that component-wise convergence\nspeed divergences is the key challenge in fine-\ntuning stability, based on the case study of the\ngradual unfreezing method.\n2. Based on our observation, we propose a\nnew simple component-wise gradient clip-\nping method to help stabilize fine-tuning,\nwhich achieves empirical improvements of\nfine-tuning stability over previous methods.\n2 A Bitter Case Study: Layer-wise\nGradual Unfreezing\nMosbach et al. (2020) attributed the instability prob-\nlem in fine-tuning process to the catastrophic for-\ngetting in the top layers, through a layer replace-\nment experiment between pretrained and fine-tuned\nmodels. Following this empirical observation, the\ninstability problem might be mitigated if we can\nminimize the edits to the pretrained model param-\neters, especially in top layers. This inspires us to\nmitigate the instability problem via gradual unfreez-\ning (GU, Howard and Ruder (2018)).\nSpecifically, suppose we are working with\na model M with L layers parameterized by\n{θ(i),1 ≤i ≤L}. GU tunes M for Literations.\nAt k-th (k start from 0) iteration, we only tune a\nsubset of parametersR(k) = {θ(i),L−k≤i≤L},\nwhere θ(i)(L−k+ 1≤i ≤L) is also tuned in\nk−1-th iteration. In each iteration, we tune the\nparameter for Eepochs, where Eis large enough\nfor convergence.1 Detailed algorithm is shown in\nAlgorithm 1 at Appendix A.\nFailure of Gradual Unfreezing From Fig. 1,\nthe accuracy of gradual unfreezing is significantly\nworse than full fine-tuning, 2 although it indeed\nachieves smaller update to pretrained model pa-\nrameters compared with full fine-tuning (Fig. 2a).\n1We select E based on preliminary experiments.\n2As pointed out by Mosbach et al. (2020), without bias\ncorrection, the original results in Devlin et al. (2019) paper\ncan be pretty bad and unstable, so we add bias correction on\ntop of Devlin et al. (2019) to make a strong baseline.\nConvergence Racing between ParametersTo\ninvestigate the reason behind this unsatisfying per-\nformance of GU, we plot the component-wise max-\nimum update (measured by component-wise maxi-\nmum rooted mean squared difference,3 as different\nlayers can have multiple components with differ-\nent dimensions) at each layer in Fig. 2b. Clearly,\nfrom the very beginning, the parameter updates for\nboth early-tuned parameters and newly-unfrozen\nparameters have quickly diminished in GU so not\nmany updates happen later when new parameters\njoin. Based on this observation, we hypothesize\nthat the failure of GU is because the early-tuned\nparameters have already converged in early itera-\ntions and cannot be re-activated later to adapt for\nnewly-unfrozen parameters.\nTo verify this hypothesis, we simply modify GU\nto stop using early-tuned parameters in the previous\niteration and instead copy the weight from the pre-\ntrained BERT model. For simplicity, we term this\nmethod as “GU (restart)”. Note that full fine-tuning\nis just the last iteration of GU (restart) when all lay-\ners are unfrozen. We plot GU (restart) performance\nin Fig. 1, and observe that GU (restart) achieves\nconsistently much better performance than GU.\nHowever, GU (restart) is much more unstable than\nGU, and the best performance is only reached when\nalmost all layers have been tuned.\nUnbalanced Gradients across ParametersTo\ninvestigate the cause of the convergence racing\nproblem, we analyze the distribution of parameters’\ngradients from different components, by plotting\nthe gradient norm in Fig. 3.4 Here we follow the\nterminology in previous work (Dodge et al., 2020;\nMosbach et al., 2020) that, if a fine-tuned model\ncheckpoint can’t beat the majority classifier,5 then\nit is a “Failed Run” and “Success Run” otherwise.\nComparing Fig. 3b and Fig. 3a, we can find that\nconvergence racing still exists under normal full\nfine-tuning. The success run wins by making all pa-\nrameters update roughly following the same trends\nand making gradient norms well-bounded.\nDiscussion on Fine-Tuning StabilityFrom the\ncase study over GU, we observe that there is a con-\nvergence racing problem between parameters of\nmodel components, which would lead to incompe-\n3We also plot the parameter change measured by cosine\ndistance and put the figures in Appendix C\n4In Fig. 3b and Fig. 3a, we do the fine-tuning by following\nthe recommended setting in (Devlin et al., 2019).\n5Here “majority classifier” means simply using the major-\nity labels in the training dataset as the predicted label.\n4855\n1 2 3 4 FULL FT\n#(Iterations)\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\nclassifier\nlayer.23\nlayer.22\nlayer.21\n(a) Compared with original BERT model\n1 2 3 4\n#(Iterations)\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\nclassifier\nlayer.23\nlayer.22\nlayer.21 (b) Compared with previous iteration\nFigure 2: Parameter update at each iteration for GU. Updates that are too small cannot be seen in this figure (e.g.,\nfor the 22-th layer, or “layer.21” in the figure update is too small to plot out in Fig. 2b). We only plot the first\n4 iterations as we observe that the performance is almost stable by the fourth iteration. As different layers can\nhave components with different dimensionalities, we show the component-wise maximum rooted mean squared\ndifference for each layer.\n0 200 400 600 800\n#(Optimization Steps)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2Grad Norm Value\n(a) Failed Run\n0 200 400 600 800\n#(Optimization Steps)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2Grad Norm Value (b) Success Run\nFigure 3: Gradient norm across different parameters at layer 22 on RTE dataset fine-tuning. Here the left figure\n(“Failed Run”) refers to the case when the fine-tuned model cannot beat the majority classifier and the right figure\n(“Success Run”) otherwise. Different colors represent different parameters. Due to space limitation, legends are\nomitted and we cannot show results from all layers. But in our observation, most layers have similar phenomenon.\ntent or unstable performance. Based on existing\nstudies, we argue that a robust PLM fine-tuning\nmethod should satisfy the following principles:\n1. Updating the full set of parameters, not just a\nsubset of layers.\n2. No significant parameter updates to avoid\ncatastrophic forgettingproblem.\n3. Adjusting the convergence speed for parame-\nters to mitigate the convergence racing.\nMosbach et al. (2020) proposed to use small learn-\ning rate (e.g., 1e−5) to mitigate catastrophic forget-\nting, together with longer fine-tuning process. How-\never, unbalanced gradients across different compo-\nnents is another factor for catastrophic forgetting\nand convergence racing, which is not resolved by\nonly using small learning rates.\n3 The Component-Wise Gradient Norm\nClipping Method\nBearing these principles in mind, we propose the\ncomponent-wise gradient norm clipping (CWGNC)\nmethod. The components are different parameters\nserving different functionalities. For example, in\na Transformer-based neural architecture, compo-\nnents can be the weight matrices of Key-Query-\nValue in Transformer architectures, the weight and\n4856\nApproach RTE MRPC COLA\nStd Mean Max Std Mean Max Std Mean Max\nDevlin et al. (2019) 4.5 50.9 67.5 3.9 84.0 91.2 25.6 45.6 64.6\n+ bias correction 4.0 70.6 75.5 2.1 89.2 92.2 11.1 59.2 64.1\nLee et al. (2019) 7.9 65.3 74.4 3.8 87.8 91.8 20.9 51.9 64.0\nMosbach et al. (2020) 2.7 67.3 71.1 0.8 90.3 91.7 1.8 62.1 65.3\nCWGNC 1.3 73.1 75.1 0.6 90.5 91.5 1.7 62.2 65.0\nTable 1: Experiment results for fine-tuning benchmark. Boldfaced numbers are best under each criterion.\nbias terms in feed-forward layers, etc. For those op-\ntimizers which maintain first-order and/or second-\norder bias correction terms (e.g., Adam (Kingma\nand Ba, 2015) and AdamW (Loshchilov and Hutter,\n2018) as the popular optimizer in PLM literature),\nour proposed norm clipping operation happens be-\nfore the bias correction terms computation and does\nnot interfere with normal bias correction process.\nBy clipping the gradient norm of each compo-\nnent individually, we aim to balance the distribu-\ntion of gradients across different components to\nadjust their convergence speed, hence mitigating\nthe convergence racing problem.\n4 Experiments\nTo evaluate our CWGNC method, we follow Mos-\nbach et al. (2020) to run our CWGNC method over\n25 different runs and report aggregated results on\nthe validation set of three different datasets: RTE\n(Acc), MRPC (F1) and COLA (MCC). We report\nthe standard deviation (Std), averaged performance\n(Mean) and maximum performance (Max). Hy-\nperparameters are tuned on held-out data. More\nimplementation details are in Appendix B.\nBaseline Setting For baselines, we first consider\nthe original BERT paper reported results (Devlin\net al., 2019) and we run original BERT fine-tuning\nwith bias correction to make a stronger baseline fol-\nlowing the observation in (Mosbach et al., 2020).\nWe also compare to Mixout regularization meth-\nods (Lee et al., 2019) and “simple but hard-to-beat\nbaseline” proposed by Mosbach et al. (2020).\nExperiment Results Experiment results are\nshown in Table 1. Here we can see our CWGNC\nachieves significantly better performance in terms\nof averaged performance and standard deviation.\nCompared with previous state-of-the-art results\nfrom (Mosbach et al., 2020), our method only\nneeds to tune 5 epochs and does not need to wait\nfor 20 epochs even on these small datasets so our\nmethod indeed converges faster.\nOur method is also robust to a wide range of\nthe selection of gradient norm clipping thresholds.\nWe show this in Table 2 on COLA dataset as we\nfind that fine-tuning methods are particularly un-\nstable on COLA dataset. Here we see that within\na reasonable range of threshold (< 1), the model\nperformance would be mostly maintained and the\nstandard deviation is well controlled. If we use a\nsignificantly larger threshold (>= 1), less control\nis enforced by CWGNC and it will degrade to nor-\nmal full model fine-tuning if we further increase\nthe threshold.\nThreshold COLA\nStd Mean Max\n0.01 1.3 61.6 64.3\n0.05 1.7 62.2 65.0\n0.1 1.4 61.4 65.1\n0.5 1.3 61.6 64.1\n1 1.3 61.4 63.8\n5 15.4 57.3 65.0\nTable 2: CWGNC fine-tuning results on COLA under\ndifferent gradient norm clipping thresholds.\n5 Conclusion\nIn this paper, we investigate the instability problem\nof fine-tuning large pretrained language models. In-\nspired by previous works, we first experiment with\ngradual unfreezing methods, which should help\nminimize the updates in top layers and ease the\ncatastrophic forgetting problem. However, further\nexperiment results do not support that and we find\nit is because there is a convergence racing issue\nbetween different parameters, namely that early-\nconverged parameters can limit the search space\nfor other parameters. Based on this finding, we\npropose to do component-wise gradient norm clip-\nping and achieve significant improvement on aver-\naged performance, smaller standard deviation and\n4857\nquicker convergence speed. Our method is robust\nto the selection of gradient norm clipping thresh-\nold. In the future, we will try to study whether the\ncomponent racing problem also exists in pretrained\nlanguage models with different sizes and different\npretraining methods.\n6 Limitations\nIn this paper we mainly work with one partic-\nularly popular large pretrained language model\nBERT (Devlin et al., 2019). While we believe our\nempirical investigation and conclusion is widely\napplicable to a wide range of current transformer-\nbased large pretrained language models, more ex-\nperiments and theoretical explanations are needed\nfor further research. Also, due to computational\nresources limitation, we cannot investigate whether\nthe most recent large models like T0 (Sanh et al.,\n2022) are stable under fine-tuning. We follow\nthe evaluation protocols in previous works (Dodge\net al., 2020; Lee et al., 2019; Mosbach et al., 2020)\nto investigate the instability of fine-tuning on small\ndatasets including COLA, RTE and MRPC. But for\nreal challenging low-resource situation, we believe\nit can be more complicated and more investigation\nis needed.\nAcknowledgements\nThis material is based on research sponsored by Air\nForce Research Laboratory (AFRL) under agree-\nment number FA8750-19-1-1000. The U.S. Gov-\nernment is authorized to reproduce and distribute\nreprints for Government purposes notwithstanding\nany copyright notation therein. The views and con-\nclusions contained herein are those of the authors\nand should not be interpreted as necessarily rep-\nresenting the official policies or endorsements, ei-\nther expressed or implied, of Air Force Laboratory,\nDARPA or the U.S. Government.\nReferences\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith. 2020.\nFine-tuning pretrained language models: Weight ini-\ntializations, data orders, and early stopping. arXiv\npreprint arXiv:2002.06305.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model fine-tuning for text classification.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 328–339.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR (Poster).\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, et al. 2017. Over-\ncoming catastrophic forgetting in neural networks.\nProceedings of the national academy of sciences,\n114(13):3521–3526.\nCheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang.\n2019. Mixout: Effective regularization to finetune\nlarge-scale pretrained language models. In Interna-\ntional Conference on Learning Representations.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nMarius Mosbach, Maksym Andriushchenko, and Diet-\nrich Klakow. 2020. On the stability of fine-tuning\nbert: Misconceptions, explanations, and strong base-\nlines. In Proceedings of ICLR.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla,\nTaewoon Kim, Gunjan Chhablani, Nihal Nayak, De-\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang,\nHan Wang, Matteo Manica, Sheng Shen, Zheng Xin\nYong, Harshit Pandey, Rachel Bawden, Thomas\nWang, Trishala Neeraj, Jos Rozen, Abheesht Sharma,\nAndrea Santilli, Thibault Fevry, Jason Alan Fries,\nRyan Teehan, Stella Biderman, Leo Gao, Tali Bers,\nThomas Wolf, and Alexander M. Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. Proceedings of ICLR.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\n4858\nA Gradual Unfreezing Algorithm\nThe gradual unfreezing algorithm we implemented\nfollowing Howard and Ruder (2018) is shown in\nAlgorithm 1.\nAlgorithm 1Gradual Unfreezing Fine-Tuning\nRequire: A multi-layer Transformer PLMMwith\nits L-layer parameters {θ(i),1 ≤i ≤L}. The\nmaximum number of iterations T, the dataset\nD, the optimizer Opt, the number of epochs for\neach iteration E.\nEnsure: 1 ≤T ≤L\nˆR(0) ←ϕ ▷ ˆR(l) represents to-be-tuned\nparameters at l-th iteration.\nwhile T ≤Ldo\nˆR(T) ←ˆR(T−1) ∪{θ(L−T+1)}\nfor j = 1→Edo\nL← Forward(M,D)\nG←Backward(L,L −T + 1,L)\n▷Only needs to compute the gradient\nfor top-T layers\nUpdate(Opt,G, ˆR(T))\n▷Apply Gover ˆR(T)\nReplace(M, ˆR(T),L −T + 1,L)\n▷Replace the updated layers back to\nmake sure the updated parameters are involved\nin next epoch forward process\nend for\nT ←T + 1\nend while\nB Implementation Details\nOur codebase is based on Huggingface Transform-\ners (Wolf et al., 2020) example fine-tuning scripts\nand will be released later. We tune models using\nour method for 5 epochs. For weight decay and\nwarm-up steps, we follow the settings original fine-\ntuning method as described in (Devlin et al., 2019).\nWe here report the result with clipping threshold\n0.05 as we empirically find it works better on held-\nout dataset. We also show later that our method is\nactually pretty robust to a wide range of threshold\npicking in Table 2.\nC Gradient Update Measured by Cosine\nSimilarities\nIn the main text we measures the update via root-\nmean-square difference. Here we show that if mea-\nsured by cosine-similarity, we would obtain similar\nconclusion. Here, Fig. 4 is the cosine similarity\nversion for Fig. 2b. Fig. 5 is the cosine similar-\nity version for Fig. 2a. Note that because smaller\ncosine similarity indicates more changes, in con-\ntrast to square-mean-root difference, we use the\ncomponent-wise minimum cosine similarity to rep-\nresent the update at each layer.\n1 2 3 4\n#(Iterations)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 classifier\nlayer.23\nlayer.22\nlayer.21\nFigure 4: The scales of incremental parameter update\nat each iteration in GU (compared with previous iter-\nation, measured by cosine similarity). As different\nlayers can have different components with different di-\nmensionalities, we show the component-wise maximum\nrooted mean squared difference for each layer. We only\nplot the first 4 iterations as we observe that the perfor-\nmance is almost stable by the 4-th iteration. 22-th layer\n(“layer.21” in the figure) update is too small to plot out\nin this figure.\n1 2 3 4 FULL FT\n#(Iterations)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 classifier\nlayer.23\nlayer.22\nlayer.21\nFigure 5: The scales of parameter update at each iter-\nation in GU (compared with original BERT model,\nmeasured by cosine similarity). Too small updates\ncannot be seen in this figure. We only plot the first 4\niterations as we observe that the performance is almost\nstable in 4-th iteration.\n4859",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7869378328323364
    },
    {
      "name": "Codebase",
      "score": 0.7220457196235657
    },
    {
      "name": "Clipping (morphology)",
      "score": 0.6895512342453003
    },
    {
      "name": "Convergence (economics)",
      "score": 0.5516928434371948
    },
    {
      "name": "Stability (learning theory)",
      "score": 0.515033483505249
    },
    {
      "name": "Norm (philosophy)",
      "score": 0.5069058537483215
    },
    {
      "name": "Component (thermodynamics)",
      "score": 0.4969828426837921
    },
    {
      "name": "Language model",
      "score": 0.4887573719024658
    },
    {
      "name": "Overfitting",
      "score": 0.4486888349056244
    },
    {
      "name": "Forgetting",
      "score": 0.4338233470916748
    },
    {
      "name": "Algorithm",
      "score": 0.42373770475387573
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3282783031463623
    },
    {
      "name": "Machine learning",
      "score": 0.1812896728515625
    },
    {
      "name": "Artificial neural network",
      "score": 0.17404258251190186
    },
    {
      "name": "Source code",
      "score": 0.16404616832733154
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Economic growth",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Thermodynamics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I40347166",
      "name": "University of Chicago",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2800817003",
      "name": "Southern California University for Professional Studies",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    }
  ],
  "cited_by": 5
}