{
  "title": "Monotonicity Reasoning in the Age of Neural Foundation Models",
  "url": "https://openalex.org/W4388700091",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5021574351",
      "name": "Zeming Chen",
      "affiliations": [
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A5065369434",
      "name": "Qiyue Gao",
      "affiliations": [
        "University of California, San Diego"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2753316031",
    "https://openalex.org/W6800751262",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2739351760",
    "https://openalex.org/W6794923208",
    "https://openalex.org/W4224032280",
    "https://openalex.org/W2250861254",
    "https://openalex.org/W3120920908",
    "https://openalex.org/W3183138634",
    "https://openalex.org/W2608787653",
    "https://openalex.org/W4200635637",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W4211148418",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2798665661",
    "https://openalex.org/W2962736243",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W2954591818",
    "https://openalex.org/W2980536612",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W6735377749",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W2016089260",
    "https://openalex.org/W2087451659",
    "https://openalex.org/W2742007054",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2413794162",
    "https://openalex.org/W2188785896",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3037109418",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2997789497",
    "https://openalex.org/W4287891464",
    "https://openalex.org/W2963355447",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2963542836",
    "https://openalex.org/W2593833795",
    "https://openalex.org/W6838461927",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3208828709",
    "https://openalex.org/W3175508917",
    "https://openalex.org/W2954194820",
    "https://openalex.org/W2962813243",
    "https://openalex.org/W4229445617",
    "https://openalex.org/W2917956622",
    "https://openalex.org/W2579563823",
    "https://openalex.org/W2530982843",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W3099023595",
    "https://openalex.org/W4212774754"
  ],
  "abstract": null,
  "full_text": "Journal of Logic, Language and Information (2024) 33:49–68\nhttps://doi.org/10.1007/s10849-023-09411-3\nMonotonicity Reasoning in the Age of Neural Foundation\nModels\nZeming Chen 1 · Qiyue Gao 2\nPublished online: 15 November 2023\n© The Author(s) 2023\nAbstract\nThe recent advance of large language models (LLMs) demonstrates that these large-\nscale foundation models achieve remarkable capabilities across a wide range of\nlanguage tasks and domains. The success of the statistical learning approach chal-\nlenges our understanding of traditional symbolic and logical reasoning. The ﬁrst part\nof this paper summarizes several works concerning the progress of monotonicity rea-\nsoning through neural networks and deep learning. We demonstrate different methods\nfor solving the monotonicity reasoning task using neural and symbolic approaches and\nalso discuss their advantages and limitations. The second part of this paper focuses\non analyzing the capability of large-scale general-purpose language models to reason\nwith monotonicity.\nKeywords Monotonicity · Natural language inference · Neural language model ·\nNeural symbolic inference\n1 Introduction\nFoundation models are large-scale language models that contain a large number of\nparameters and are pretrained on massive amounts of text data, often on hundreds of\nmillions or even billions of words. The pretraining and large-scale parameters allow\nthem to generate high-quality human-like responses in a wide range of tasks and appli-\ncations that NLP researchers previously thought required language understanding,\nsuch as question-answering, dialogue generation, and mathematical reasoning Bom-\nmasani et al. ( 2022). Recently released large language models, such as Open-AI’s\nB Zeming Chen\nzeming.chen@epﬂ.ch\nQiyue Gao\nq3gao@ucsd.edu\n1 Computer and Communication Sciences, EPFL, Lausanne, Switzerland\n2 Halıcıo˘glu Data Science Institute, UC San Diego, San Diego, USA\n123\n50 Z. Chen and Q. Gao\nG P T - 3( B r o w ne ta l . ,2020), Google’s FLAN-T5 Wei et al. ( 2021), and Facebook’s\nLLaMA (Touvron et al., 2023), are some of the most well-known foundation models\nthat are dominating the ﬁeld of natural language processing. They achieve human-level\nperformance on various language tasks and have the ability to follow human-deﬁned\ninstructions. However, it is still unclear whether these large foundation models have\nthe ability to perform complex logical reasoning comparable to human skills. Thus,\nour motivation is to uncover the ability and limitations of neural foundation models in\nmonotonicity reasoning and investigate how we can approach logical reasoning in the\nage of neural foundation models. We focus our discussion mainly on the monotonicity-\nbased Natural Language Inference task.\nNatural Language Inference (NLI), also known as recognizing textual entailment\n(RTE), is one of the important benchmark tasks for natural language understanding.\nMany other language tasks can beneﬁt from NLI, such as question answering, text\nsummarization, and machine reading comprehension. The goal of NLI is to determine\nwhether a given premise P semantically entails a given hypothesis H (Dagan et al.,\n2013). Consider the following example:\n•\nP: An Irishman won the Nobel Prize for literature.\n• H: An Irishman won the Nobel Prize.\nThe hypothesis can be inferred from the premise, and therefore the premise entails the\nhypothesis. To arrive at a correct determination, an NLI model often needs to make\ndifferent inferences, including various types of lexical and logical inferences. In this\npaper, we are concerned with monotonicity reasoning, a type of logical inference that\nis based on word or phrase replacement Hu et al. ( 2019). Below is an example of\nmonotonicity reasoning:\n1. (a)\nAll students ↓ carry a MacBook ↑.\n(b) All students carry a laptop.\n(c) All new students carry a MacBook.\n2. (a) Not All new students ↑ carry a laptop.\n(b) Not All students carry a laptop.\nA phrase in upward entailment context ( ↑) can allow inference from (1a) to (1b), where\na more general concept laptop replaces the more speciﬁc MacBook.Ad o w n w a r d\nentailing phrase ( ↓) allows an inference from (1a) to (1c), where a more speciﬁc\ncontext new students replaces the word students. The direction of the monotonicity\ncan be reversed by adding a downward entailing phrase like “Not\"; thus, (2a) entails\n(2b).\nIn this paper, we provide an in-depth discussion on monotonicity reasoning in the\nage of neural foundation models in the aspects of methodology and analysis. First,\nwe investigate whether incorporating both advanced neural network mechanisms, like\nattention with structural sentence knowledge based on the linguistic principle of com-\npositionality, can achieve accurate and robust monotonicity reasoning in the form of\nNLI. We propose an AttentiveTreeNet that contains a Tree-LSTM encoder with an\nattention mechanism and a multi-hop self-attention aggregator for NLI classiﬁcation.\nWe evaluate AttentiveTreeNet on the MED Yanaka et al. ( 2019) benchmark and show\nthat it signiﬁcantly outperforms a high-quality foundation model BERT Devlin et al.\n(2019).\n123\nMonotonicity Reasoning In the Age... 51\nNext, we propose a symbolic reasoning system that performs monotonicity rea-\nsoning based on polarity marks and incorporates neural language models to handle\nsyntactic variations in the data. Our proposed system, called NeuralLog, achieve state-\nof-the-art performance on the MED benchmark that signiﬁcantly outperforms prior\nneural network models. The advantage of NeuralLog is its ability to perform step-by-\nstep reasoning based on human-deﬁned symbolic logic rules while resolving syntactic\nvariations using neural language models, which makes its reasoning much more robust\nand generalizable than prior logic reasoning systems.\nIn the last part, we benchmark pretrained models ﬁne-tuned on massive task-speciﬁc\ntraining data (with parameter sizes ≤ 11 billion) and large-scale language models\n(with parameter sizes ≥ 11 billion) on monotonicity reasoning through instruction-\nbased zero-shot learning and in-context-based few-shot learning. Our objective is to\nassess whether these large foundation models have the ability to emulate logical rea-\nsoning since they have shown impressive performance on various linguistic tasks and\napplications. Our evaluation shows that current large language models still fail to\nperform logical reasoning well. Large language models only achieve random perfor-\nmance despite instructions and few-shot examples on the monotonicity test set from\nthe CURRICULUM benchmark Chen and Gao ( 2022), which is a curated mixture\nof the MED Yanaka et al. ( 2019) and Semantic Fragments Richardson et al. ( 2019)\ndatasets.\nOverall, we show that although large-scale foundation models are dominating the\nﬁeld of natural language processing by mastering many tasks and applications, they still\ncannot emulate logical reasoning like monotonicity inference. Meanwhile, symbolic\nreasoning systems that incorporate neural language models can achieve state-of-the-\nart performance that is interpretable and robust. A subset of this work was previously\npublished as Chen ( 2021) and Chen et al. ( 2021).\n2 Attentive Tree Structured Network\n2.1 Preliminaries\nIn this section, we propose a tree-structured long-short-term memory (LSTM) net-\nwork in which the syntactic information of a sentence is encoded, and the alignment\nbetween the premise-hypothesis pair is calculated through a self-attention mechanism.\nA standard sequential LSTM (Wang & Jiang, 2016) network only permits sequential\ninformation propagation. However, the linguistic principle of compositionality states\nthat an expression’s meaning is derived from the meanings of its parts and of the\nway they are syntactically combined (Partee, 2007). A tree-structured LSTM network\nallows each LSTM unit to be able to incorporate information from multiple children’s\nunits. This takes advantage of the fact that sentences are syntactically formed bottom-\nup tree structures.\n123\n52 Z. Chen and Q. Gao\n2.2 Method\nTree-LSTM Encoder\nThe main architecture builds from the Child-Sum Tree-LSTMs (Tai et al., 2015),\nwhere the computation of a hidden state is conditioned on both the current input\nand the hidden states of an arbitrary subset of children nodes. This property allows\nthe recursive computation of non-leaf nodes’ relation representations by composing\nchildren relations, which can be viewed as natural logic for neural models (MacCartney\n& Manning, 2009; Zhao et al., 2016). The computation ﬂow in an LSTM cell is as\nfollows:\n˜h = /Sigma1\n1≤k≤n hk ,\ni = σ(W (i)x + U(i) ˜h + b(i)),\no = σ(W (o)x + U(o) ˜h + b(o)),\nu = tanh(W (u)x + U(u) ˜h + b(u)),\nfk = σ(W ( f )x + U( f )hk + b( f )),\nc = i ⊙ u + /Sigma11<n fk ⊙ ck ,\nh = o ⊙ tanh(c),\nwhere k is the number of children of the current node, and ˜h is the sum of the hidden\nstates from the current node’s children. The forget gate fk controls the amount of\nmemory being passed from the kth child. The input gate i controls the amount of\ninternal input u being updated, and the output gate o controls the degree of exposure\nof the memory. The σ is the sigmoid activation function, ⊙ is the element-wise product,\nand W and U are trainable weights to be learned.\nAttention Mechanism\nWe propose incorporating the attention mechanism Zhou et al. ( 2016) in the LSTM\nnetwork. Attention considers contextual relevance by assigning higher weights to\nchildren that are more relevant to the context. We apply a soft-attention layer, which\nreceives a set of hidden states {h\n1, h2, ...,hn}and a vector representation s of a sentence\ncomputed from a layer of sequential LSTM. The attention layer assigns a weight α\nfor each hidden state and computes the context vector g as a weighted sum:\nmk = tanh(W (m)hk + U(m)s),\nαk = ew⊤ mk\n∑ n\nj=1 ew⊤ m j\n,\ng =\n∑\n1≤k≤n\nαk hk .\nThe hidden state for the next cell is then computed via a transformation ˜h =\ntanh(W (a)g + b(a)).\n123\nMonotonicity Reasoning In the Age... 53\nSelf-Attention Aggregator\nWe encode the premise and hypothesis using the attentive encoder, concatenate the\nhidden states into a pair of matrices Hp and Hh , and passed to a self-attentive\naggregator. To aggregate, we ﬁrst apply a multi-hop self-attention mechanism (Lin\net al., 2017). Performing multiple hops of attention helps the model to get multi-\nple attention focusing on different sentence parts since multiple components form\nthe sentence context. Given a matrix H, we perform multiple hops of attention to\ncompute an annotation matrix A, consisting of the weight vector from each hop. A\nis calculated from a 2-layer multi-layer perceptron (MLP) and a softmax function:\nA = softmax(W\ns2tanh (Ws1 H⊤ )). The annotation matrix is multiplied by the hidden\nstates H to obtain a context matrix: M = AH . With a pair of context matrices Mp\nand Mh , we compute the outputs as:\nFp = tanh(Mp × W f ), Fh = tanh(Mh × W f ). (1)\nTo aggregate Fp and Fh , we follow a generic NLI training scheme Conneau et al.\n(2017) to include three matching methods: (I) concatenation, (ii) absolute distance,\nand (iii) element-wise product. Results from the three methods are then concatenated:\nFr =[ Fp; Fh ;∥ Fp − Fh ∥; Fp ⊙ Fh ] as the factor of semantic relation between the\ntwo sentences. An MLP layer works as the classiﬁer which predicts the label using\nthe factor.\n2.3 Evaluation\nDatasets\nWe evaluate our proposed method on the Monotonicity Entailment Dataset (MED)\nYanaka et al. ( 2019). MED is a high-quality benchmark that aims to examine models’\nability to perform monotonicity reasoning. MED covers various linguistic phenomena\nsuch as lexical knowledge, conjunction, disjunction, conditional, and negative polarity\nitems. The dataset contains 5382 premise-hypothesis pairs, including 1820 examples\nfor upward inference, 3270 for downward inference, and 292 neutral examples.\nSetup and Baselines\nInitially, we used the HELP dataset Yanaka et al. ( 2019) to train our model. HELP\nis a dataset for learning entailment with lexical and logical phenomena. It embodies\na combination of lexical and logical inferences focusing on monotonicity. Next, we\ntrained our model with the Multi-Genre NLI Corpus (MNLI) dataset Williams et al.\n(2018), which covers a wide range of genres of spoken and written language. The\nmajority of the training examples in that dataset are upward monotone. To provide\nmore balanced training data, we combined a subset of the MNLI dataset with the\nHELP dataset to reduce the effect of many downward monotone examples in the\nHELP dataset. Due to limited computation resources at the time of training, we only\nrandomly sampled a subset of the MNLI dataset to reduce the training time period.\nWe call this combined training data HELP+SubMNLI. We removed the contradicting\nexamples from the MNLI dataset since the test dataset MED, and the training dataset\nHELP do not contain the label Contradiction.\n123\n54 Z. Chen and Q. Gao\nTable 1 Accuracy of our model and other state-of-art NLI models evaluated on MED\nModel Train Data Up Down None All\nBiMPM (Wang et al., 2017) SNLI 53.5 57.6 27.4 54.6\nESIM (Chen et al., 2017) SNLI 71.1 45.2 41.8 53.8\nDeComp (Parikh et al., 2016) SNLI 66.1 42.1 64.4 51.4\nBERT-base (Devlin et al., 2019)M N L I 82.7 22.8 52.7 44.7\nBERT-base (Devlin et al., 2019) HELP+MNLI 76.0 70.3 59.9 71.6\nAttnTreeNet (ours) MNLI 54.7 60.4 37.8 58.6\nAttnTreeNet (ours) HELP 55.7 72.6 57.9 66.0\nAttnTreeNet (ours) HELP+SubMNLI 81.4 74.5 53.8 75.7\nBold indicates the highest accuracy in the table for each column\nTraining\nTo train our model, we used Stanford’s pre-trained 300-D Glove 840B vectors (Pen-\nnington et al., 2014) to initialize the word embeddings. The Stanford Dependency\nParser (Chen & Manning, 2014) was used to parse each sentence in the dataset. The\nmodel is trained with the Adam optimizer (Kingma & Ba, 2014), which is compu-\ntationally efﬁcient and helps a model to converge to an optimal result quickly. A\nstandard learning rate for Adam, 0.001, is also used. Dropout with a standard rate of\n0.5 is applied to the feed-forward layer in the self-attention aggregator and the classi-\nﬁer to reduce the over-ﬁtting of the model. For the number of hops of self-attention,\nwe used the default 15 hops. The metric for evaluation is accuracy based. The system\nis implemented using a common deep learning framework, PyTorch, and is trained on\na T4 GPU for 20 epochs.\n2.4 Results\n2.4.1 MED Performance\nTable 1 shows our method’s performance compared against common NLI methods\non the Monotonicity Entailment Dataset (MED). Our model achieves an overall accu-\nracy of 75.7% and outperforms all other models, including the pre-trained language\nmodel BERT, which previously showed SOTA performance on NLI tasks. On down-\nward monotonicity reasoning, which is more difﬁcult than upward, our method shows\nsigniﬁcant improvement in performance over the baselines, with 4.5% higher than\nthe BERT model. Interestingly, our model achieves better performance on downward\ninference even when trained with HELP or MNLI alone (compared to baselines with\nsimilar training data). This shows a structural advantage of our model architecture over\nthe baselines. On upward monotonicity reasoning, our model is only slightly behind\nthe BERT model (1.3% apart) but still outperforms the other baselines with a large\nmargin (10.3% to the best non-BERT baseline). Note that augmenting HELP with a\nsubset of MNLI improves the performance on upward monotone (+25.7%), showing\nthat training additionally on some general NLI examples helps the model to learn the\nupward inference. On examples without monotonicity inference, our method does not\n123\nMonotonicity Reasoning In the Age... 55\nTable 2 This table shows the accuracy of ablation tests trained on HELP and HELP+SubMNLI and tested\non MED. Three ablation tests were performed: (i) Remove self-attentive aggregator (–Self-attention), (ii)\nReplace Tree-LSTM with a regular sequential LSTM (–Tree-LSTM)\nModel Training Data Upward Downward None All\nAttnTreeNet HELP 55.7 72.6 57.9 66.0\n–Self-attention HELP 65.1 67.1 53.7 65.7\n–Tree-LSTM HELP 36.6 65.5 94.8 49.5\nAttnTreeNet HELP+SubMNLI 81.47 4 .5 53.8 75.7\n–Self-attention HELP+SubMNLI 70.5 66.9 85.6 69.1\n–Tree-LSTM HELP+SubMNLI 54.7 60.4 37.8 58.6\nBold indicates the highest accuracy in the table for each column\nperform as well as the examples with monotonicity. This suggests that while achieving\nhigh performance on monotonicity reasoning, our method loses some ability to reason\nwith the general NLI problems. Overall, we show that our attentive tree-based network\nachieves the highest performance among the baselines on monotonicity reasoning.\n2.4.2 Ablation Test\nTo further analyze each component’s contribution to the model performance on mono-\ntonicity reasoning, we conduct several ablation tests. We ﬁrst do an ablation test\non the self-attentive aggregator by building the feature vector for classiﬁcation right\nafter the Tree-LSTM encoder. As Table 2 (–aggregator) shows, models trained on\nHELP+SubMNLI show a signiﬁcant performance drop (6.6%) with a 76% drop in\ndownward inference and a 10.9% drop in upward inference. The performance drop\nsuggests that the self-attentive aggregator is an important component of the model\nfor monotonicity reasoning. For the second ablation test, we replace the Tree-LSTM\nencoder with a standard LSTM encoder. Note that this results in a larger performance\ndrop in upward inference (26.7%) and downward inference (14.1%). This demon-\nstrates that replacing the Tree-LSTM with a standard one has a signiﬁcant negative\nimpact on the model’s reasoning ability for monotonicity. Thus, Tree-LSTM is also\na major component of our proposed model. Overall, the removal of the Tree-LSTM\nencoder affected the model’s performance the most. Thus, we conclude that the Tree-\nLSTM encoder contributes the most to the model’s performance on monotonicity\nreasoning.\n3 Neural-Symbolic Reasoning\n3.1 Preliminary\nEvaluation results for the Attentive Tree Network show that providing and enhancing\nstructural knowledge of sentences is an effective way to improve neural models’ mono-\ntonicity reasoning ability. However, directly embedding symbolic logical information\ninto a neural model is difﬁcult. A better approach would be building a symbolic reason-\n123\n56 Z. Chen and Q. Gao\ning system incorporating neural modules into its inference process for better and more\nrobust reasoning performance. Previously, several symbolic reasoning systems for NLI\nhave been proposed Abzianidze ( 2017); Martínez-Gómez et al. ( 2017); Yanaka et al.\n(2018); Hu et al. ( 2020) to solve the NLI task based on symbolic rules and semantic for-\nmalism. These systems show high precision on complex inferences involving difﬁcult\nlinguistic phenomena and present logical and explainable reasoning processes. How-\never, these systems show several limitations, such as lacking background knowledge\nand the inability to handle sentences with syntactic variations. On the other hand,\nnew pre-trained language models are becoming more robust and accurate through\nimproved pre-training objectives and data., enabling them to handle diverse and large\ntest data robustly. However, several experiments show that DL models lack general-\nization ability, adopt fallible syntactic heuristics, and show exploitation of annotation\nartifacts Glockner et al. ( 2018); McCoy et al. ( 2019); Gururangan et al. ( 2018). We\npropose joining the strengths of these two types of systems into a hybrid reasoning\nsystem that can perform monotonicity reasoning.\n3.2 Method\nOur system contains four components: (1) a polarity annotator, (2) three sentence\ninference modules, and (3) a search engine. Figure 1 shows a diagram of the full\nsystem.\n3.2.1 Polarity Annotator\nTo perform robust and accurate monotonicity reasoning, the system needs the anno-\ntations of monotonicity information on the given premises. To annotate monotonicity\ninformation, we utilize Udep2Mono Chen and Gao ( 2021), a polarity annotator that\ndetermines the monotonicity polarity of all constituents on a universal dependency\ntree. The annotator ﬁrst parses the premise into a binarized universal dependency tree\nand then conducts polarization by recursively marking polarity on each tree node. The\npolarity marks include monotone ( ↑), antitone ( ↓), and no monotonicity information\n(=) polarities. An annotated example would be Every\n↑ healthy ↓ person ↓ plays↑\nsports↑. Where the monotone tokens are tagged with ↑ and antitone tokens are tagged\nwith ↓.\n3.2.2 Search Engine\nNext, the polarized parse tree is passed to the search engine. A beam search algorithm\nsearches for the optimal inference path from a premise to a hypothesis. During an\ninference step, we rank the generated sentences with a distance function and select\nthe sentence with the minimum distance to proceed:\ns⋆ = arg min\ns∈S\ndist(s,H), (2)\n123\nMonotonicity Reasoning In the Age... 57\nFig. 1 Overview system diagram of NeuralLog, including (1) the polarity annotator, (2) the three inference\nmodules, and (3) the beam search engine\nwhere H is the hypothesis, S is a set of intermediate premises generated from the three\ninference modules, and s is the optimal intermediate premise to continue the search\nthat yields the minimal distance to the hypothesis. Here we formulate the distance\nfunction as the Euclidean Distance between the sentence embeddings of an interme-\ndiate premise and the hypothesis. The search space is generated from three inference\nmodules: lexical, phrasal, and syntactic variation. In practice, we expand our search\nspace on the top-k intermediate premises instead of the optimal ones. The system\nreturns Entail if an inference path is found. Otherwise, the premise and hypothe-\nsis would be categorized as Non-Entail, where the controller will further search for\ncounter-example signatures to differentiate between Contradict and Neutral.I nt h i s\npaper, we only analyze the system’s performance on the MED dataset (2-way classi-\nﬁcation: Entail and Non-Entail) and hence omit the details on how the system detects\ncontradiction signatures.\n3.2.3 Inference Generation\nLexical Monotonicity Inference\nLexical inference module performs word replacement on key tokens, including nouns,\nverbs, numbers, and quantiﬁers, based on monotonicity information. The system uses\nlexical knowledge bases, including WordNet Miller ( 1995) and ConceptNet Liu and\nSingh ( 2004). From the knowledge bases, we extract four sets of words: hypernyms,\nhyponyms, synonyms, and antonyms. Logically, if a word has a monotone polarity\n(↑), it can be replaced by its hypernyms. For example, swim ≤ move; then swim can\nbe replaced with move, where ≤ means that the left-hand-side word is a type of the\nright-hand-side word. If a word has an antitone polarity ( ↓), it can be replaced by\nits hyponyms. For example, ﬂower ≥ rose. Then, ﬂower can be replaced with rose,\n123\n58 Z. Chen and Q. Gao\nwhere ≥ means that the right-hand-side word is a type of the left-hand-side word.\nWe ﬁlter out irrelevant words from the knowledge bases that do not appear in the\nhypothesis. Additionally, we handcraft knowledge relations for words like quantiﬁers\nand prepositions that do not have sufﬁcient taxonomies from knowledge bases. Some\nhandcrafted relations that hold in general include: all = every = each ≤ most ≤ many\n≤ several ≤ some = a, up ⊥ down, where = means that the two words are equivalent\nrelations.\nPhrasal Monotonicity Inference\nPhrasal replacements are for phrase-level monotonicity inference. For example, with\na polarized sentence A\n↑ woman↑ who↑ is↑ beautiful↑ is↑ walking↑ in↑ the↑ rain=,\nthe monotone mark ↑ on woman allows an upward inference: woman ⊒ woman who is\nbeautiful, in which the relative clause who is beautiful is deleted. The system follows\na set of phrasal monotonicity inference rules. For upward monotonicity inference,\nmodiﬁers of a word are deleted. For downward monotonicity inference, modiﬁers are\ninserted into a word. The algorithm traverses down a polarized UD parse tree, deletes\nthe modiﬁer sub-tree if a node is monotone ( ↑), and inserts a new sub-tree if a node is\nantitone (↓). To insert new modiﬁers, the algorithm extracts a list of potential modiﬁers\nassociated with a node from a modiﬁer dictionary. The modiﬁer dictionary is derived\nfrom the hypothesis and contains word-modiﬁer pairs for each dependency relation.\nBelow is an example of a modiﬁer dictionary from There are no beautiful ﬂowers that\nopen at night :\n• Obl: [head: open, mod: at night ]\n• Amod: [head: ﬂowers, mod: beautiful]\n• Acl:relcl: [head: ﬂowers, mod: that open at night ]\nSyntactic V ariation Inference\nWe categorize linguistic changes between a premise and a hypothesis that cannot\nbe inferred from monotonicity information as syntactic variations . For example, a\nchange from red rose to ar o s ew h i c hi sr e d is a syntactic variation. Many logical\nsystems rely on handcrafted rules and manual transformation to enable the system to\nperform syntactic variations. However, without accurate alignments between the two\nsentences, these methods are not robust enough and, thus, difﬁcult to scale up for wide-\ncoverage input. The recent development of pretrained transformer-based language\nmodels brings state-of-art performance on multiple benchmarks for Natural Language\nUnderstanding (NLU), including the task of paraphrase detection Devlin et al. ( 2019);\nLan et al. ( 2020); Liu et al. ( 2020), which exempliﬁes phrasal knowledge of syntactic\nvariation. We propose a method that incorporates transformer-based language models\nto handle syntactic variations robustly. Our method ﬁrst decomposes both the premise\nand the hypothesis into chunks of phrases using a sentence chunker and then calculates\nthe likelihood of each pair of chunks being a paraphrase using a transformer model.\nSequence Chunking\nTo obtain phrase-level chunks from a sentence, we build a sequence chunker, which\nrelies on the sentence’s universal dependency information. Instead of breaking down a\nsentence, our chunker composes word tokens recursively to form meaningful chunks.\nFirst, we construct a sentence representation graph of a premise from the controller.\nA sentence representation graph is deﬁned as G =⟨ V,E ⟩, where V = V\nm ∪ Vc is\n123\nMonotonicity Reasoning In the Age... 59\nthe set of modiﬁers ( Vm ) and content words ( Vc), and E is the set of directed edges.\nTo generate the chunk for a content word in Vc, we arrange its modiﬁers, which are\nnodes it points to, together with the content word by their word orders in the original\nsentence to form a word chain, for example, in The woman in a pink dress is dancing .\nThe edges from dress to in, a, pink with the edge from woman to dress can be drawn.\nChunks i nap i n kd r e s sand the woman in a pink dress will be generated for dress and\nwoman, respectively.\nMonolingual Phrase Alignment\nGiven a set of chunks from a generated sentence and from the hypothesis, the system\ncomputes an alignment score for each pair of chunks to select the syntactic variations.\nFormally, we deﬁne C\ns as the set of chunks from a generated sentence and Ch as the\nset of chunks from the hypothesis. We build the Cartesian product from Cs and Ch ,\ndenoted Cs × Ch . For each chunk pair (c s ,c h ) ∈ Cs × Ch , we compute an alignment\nscore α:\nα⟨cs,ch ⟩ = p(y |⟨ cs,ch⟩)\nwhere y |⟨ cs,ch⟩= Softmax(ALBERT(⟨cs,ch⟩)).I f α > 0.85 (determined by a grid\nsearch of 5 values), the system records this pair of phrases as a syntactic variation. To\ncalculate the alignment score, we use an ALBERT Lan et al. ( 2020) model, ﬁne-tuned\non the Microsoft Research Paraphrase Corpus Dolan and Brockett ( 2005). We ﬁrst\npass a chunk pair to ALBERT to obtain its logits. Then we apply a softmax function\nto the logits to get the ﬁnal probability.\n3.3 Evaluation\n3.3.1 Experiment Setup\nFor Universal Dependency parsing, we follow Udep2Mono’s framework Chen and Gao\n(2021) and use a neural parsing model from Stanford’s Stanza Qi et al. ( 2020) with 90.0\nLAS Zeman et al. ( 2018) evaluation score. We select the BERT-large model pre-trained\non STS-B Cer et al. ( 2017) from Sentence-BERT Reimers and Gurevych ( 2019).1\nFor ALBERT, we used an ALBERT-base model pretrained on the MRPC corpus.\nWe evaluate our proposed reasoning system, NeuralLog, on the MED dataset for\nmonotonicity reasoning. We compare our method with multiple deep-learning-based\nbaselines. Here, DeComp and ESIM are trained on SNLI, and BERT is ﬁne-tuned with\nMultiNLI. The BERT+ model is a BERT model ﬁne-tuned on a combined training data\nwith the HELP dataset, Yanaka et al. ( 2019), a set of augmentations for monotonicity\nreasoning, and the MultiNLI training set. Both models were tested in Yanaka et al.\n(2019). We also compare against the Attentive Tree Net we proposed in the ﬁrst part to\nsee if the neural-symbolic inference is a better choice than dedicated neural architecture\nand training data.\n1 Note that there are new embeddings that are more robust and accurate than the one we used. We recommend\nusing the up-to-date embeddings.\n123\n60 Z. Chen and Q. Gao\nTable 3 Results comparing\nmodel compared to state-of-art\nNLI models evaluated on MED.\nUp, Down,a n d All stand for the\naccuracy of upward inference,\ndownward inference, and the\noverall dataset\nModel Up Down All\nDeComp (Parikh et al., 2016) 71.1 45.2 51.4\nESIM (Chen et al., 2017) 66.1 42.1 53.8\nBERT (Devlin et al., 2019) 82.7 22.8 44.7\nBERT+ (Y anaka et al., 2019) 76.0 70.3 71.6\nAttnTreeNet (ours) 81.4 74.5 75.7\nNeuralLog (ours) 91.49 3 .99 3 .4\nBold indicates the highest accuracy in the table for each column\n3.3.2 Results\nAs Table 3 shows, our system (NeuralLog) outperforms all the neural model baselines\nin terms of accuracy by a signiﬁcant margin (48.7% maximum increase and 21.8%\nminimum increase). Compared to a prior neural-symbolic system, BERT+, our system\nperforms much better both on the upward (15.4%) and downward (23.6%) inference.\nCompared to the Attentive Tree-structured Net for monotonicity reasoning, our neural-\nsymbolic system still shows better performance with a signiﬁcant margin of increase\n(/Delta117.7%). This result highlights the point that using dedicated training data and\nneural architectures for monotonicity reasoning is not as effective as a neural-symbolic\nsystem that utilizes neural modules for intermediate reasoning. The good performance\non MED validates our system’s ability on accurate and robust monotonicity-based\ninferences.\n4 Large-scale Foundation Model\n4.1 Preliminary\nIn the ﬁeld of natural language processing (NLP), the use of large language models\n(LLMs) has signiﬁcantly revolutionized how people approach reasoning and inference\non language. It has been established that the effectiveness and efﬁciency of these\nmodels in various NLP applications can be improved by increasing their size, such\nas by increasing their training resources, the number of model parameters, and so\non Wei et al. ( 2022). Self-supervised pre-training gives large-scale language models\nthe ability to learn downstream tasks given no example or only a few input–output\npaired examples without optimization. Recent research shows emergent interest in\nuncovering the underlying logic of the aforementioned mysterious capacity of LLMs\nby empirical and theoretical approaches Rubin et al. ( 2021); Xie et al. ( 2021); Min\net al. ( 2022); Y e and Durrett ( 2022). However, the current analysis of these LLMs\nstill cannot answer if the unpredictable phenomena of emergent abilities of LLMs\nallow them to acquire the ability to simulate symbolic logic in natural language. In\nthis section, we make an effort to benchmark various LLMs’ reasoning ability on\nmonotonicity to gain some insights into the limitation of current LLMs.\n123\nMonotonicity Reasoning In the Age... 61\n4.2 Method\nZero-Shot Learning\nMany studies show that large-scale language models exhibit zero-shot learning ability\nKojima et al. ( 2022). The models can solve various NLP tasks by simply conditioning\nthe instructions describing the task. We start our experiments on monotonicity reason-\ning using the setting of zero-shot learning. Speciﬁcally, we give the model a prompt Liu\net al. ( 2021) in the format of Instruction: ⟨Instruction⟩ Context: ⟨Context⟩ Question:\n⟨Question⟩ Answer: ⟨Answer⟩. The model then generates the ⟨Answer⟩ tokens for the\ngiven problem by conditioning on this prompt. In zero-shot learning, the model cannot\nrely on any demonstrations but its parametric knowledge that is acquired during the\npre-training stage, which is triggered by the prompt.\nIn-Context Learning\nIn-context learning for large language models is formulated as a text-generation prob-\nlem. The generation is conditioned on a given prompt p which consists of the input\nproblem x and k examples of input–output pairs:\np\nLLM(y | p) =\nT∏\nt=1\np(yt | p,y<t), (3)\nwhere the prompt p contains several examples and the question to be answered: p =\n{x1, y1, ...,xk , yk , x}, and LLM is a large language model that can generate text in\nan auto-regressive way. According to Xie et al. ( 2021), the in-context learning ability\nof LLMs could be interpreted as an implicit Bayesian inference gained from the auto-\nregressive next-token generation task in the pre-training. The given input text, prompt\np, provides evidence of posterior distribution over task-related latent concepts c to\ninfer the corresponding label y:\np(y | p) =\n∫\nc\np(y | c,p)p(c | p)d(c). (4)\nIn-context learning allows one to adapt LLMs to a different domain and downstream\ntasks without any ﬁne-tuning. Because of its effectiveness and efﬁciency, we desire to\ninvestigate LLMs’ monotonicity reasoning capacity.\n4.3 Evaluation\nSetup\nThe evaluation focuses on assessing large language models’ reasoning ability with\nrespect to monotonicity. We evaluate both the zero-shot learning setting and the few-\nshot in-context learning setting. When designing the prompt, we follow previous work\non prompt-based multi-task learning Sanh et al. ( 2021) and build a Natural-Language-\nInference-styled prompt. We include detailed instructions for the task and its label\nspace to inject domain-speciﬁc understanding into models. We use the monotonicity\n123\n62 Z. Chen and Q. Gao\nreasoning test set from the CURRICULUM benchmark Chen and Gao ( 2022), a large-\nscale reasoning benchmark for evaluating broad-coverage linguistic phenomena. The\nmonotonicity portion of the CURRICULUM benchmark integrates the MED dataset,\nthe Semantic Fragments test sets Richardson et al. ( 2019), and 500 additional gold\nannotated monotonicity reasoning sentence pairs that are manually annotated and\ncurated by human writers. Overall, this test set provides high-quality data, challenging\nproblems, and analysis of powerful contextualized embedding language models. Thus,\nthis test set allows us to conduct a more in-depth evaluation of modern large-scale\nlanguage models. For in-context learning, we provide 4-shot, 8-shot, and 16-shot of\nexamples to LLMs, respectively. For instance, in a 4-shot setting, 4 examples are\nrandomly sampled from the training set for each label and concatenated to the prompt\nas a preﬁx. Each setting is evaluated 3 times, and the in-context examples are ﬁxed\nevery round to avoid the potential bias from example selection. We report the average\nperformance across the 3 runs.\nBaselines\nFor model selection, we pick LLMs with strong zero-shot learning abilities. The\nﬁrst type of models we select are LLMs that are continue ﬁne-tuned in multi-task\nor instruction-tuning settings. We ﬁrst report the baseline performance of the current\nSOTA NLI models, including RoBERT a Liu et al. ( 2019) and DeBERT a He et al.\n(2021). These two models are pre-trained bidirectional language models based on\ntransformers and have shown impressive performance on NLI. These two models are\nﬁne-tuned on a mixture of common NLI training sets, including SNLI Bowman et al.\n(2015), MNLI Williams et al. ( 2018), FEVER Thorne et al. ( 2018), and ANLI Nie\net al. ( 2020). We select FLAN-T5 W e ie ta l .( 2021) as the instruction-tuned model.\nFLAN-T5 is a T5 Text2Text model trained using an instruction-based ﬁne-tuning pro-\ncedure on a collection of data sources with various instruction template types. FLAN\nwith scaled parameters and training instructions shows strong zero-shot and few-shot\nlearning abilities, outperforming prior public checkpoints. The second model type is\nLLMs, with a large parameter size (175 billion) showing the incredible ability for\nin-context learning Chung et al. ( 2022). We select the popular GPT-3 models from\nOpenAI. GPT-3 Brown et al. ( 2020) is a state-of-the-art auto-regressive language gen-\neration model. With 175 billion parameters and massive pre-training text data, it is\ncurrently one of the largest and most powerful language models in existence, capable of\na wide range of natural language processing tasks. We include the original pre-trained\nGPT-3 model (text-davinci-001) and the GPT3.5 (text-davinci-003) model Ouyang\net al. ( 2022), a version of the InstructGPT ﬁne-tuned using reinforcement learning\nwith reward models trained from human feedback (RLHF). GPT3.5 is much better at\nfollowing the human intent in the instruction than the pre-trained version Ouyang et\nal. ( 2022).\n4.4 Results\nTable 4 shows the evaluation results for these models. Both GPT-3 and GPT −3.5\nachieve only random performance(50%). GPT3.5 outperforms GPT3 by about 6% in\nevery setting, but the performance is still far from proﬁciency in monotonicity reason-\n123\nMonotonicity Reasoning In the Age... 63\nTable 4 Evaluation results for large language models (LLMs) on CURRICULUM’s monotonicity test set.\nHere M refers to million and B refers to billion for the number of parameters\nModel # Parameters 0-shot 4-shot 8-shot 16-shot\nRandom Baseline – 50.0 – – –\nRoBERTa-large-SMFA Liu et al. ( 2019) 355 M 50.8 – – –\nDeBERTa-large-SMFA He et al. ( 2021) 435 M 51.1 – – –\nFLAN-T5-XXL Wei et al. ( 2021) 11B 58.7 – – -\nGPT3 (text-davinci-001) Brown et al. ( 2020) 175B 48.3 51.9 51.8 51.6\nGPT3.5 (text-davinci-003) Ouyang et al. ( 2022) 175B 56.9 57.3 57.9 58.7\ning. These low performances raise the question of whether LLMs’ can emulate logical\nreasoning expressed in natural language. Interestingly, instruction tuning with RLHF\nOuyang et al. ( 2022) does not help the model substantially improve its understanding\nof logical inference, as shown in the overall low accuracy from GPT −3.5. On the other\nhand, compared to GPT-3, whose performance seems irrelevant with respect to the\nnumber of in-context examples, GPT3.5 shows consistent improvements as we give\nthe model more examples, although such increases are still marginal. GPT −3.5’s per-\nformance gain from in-context learning is only trivial (0.4%). The results show that the\nin-context examples give the model certain levels of domain-speciﬁc task knowledge\nbut fail to help the model fully learn the ability to perform monotonicity reasoning.\nGPT-3’s poor performance in the zero-shot setting and performance ﬂuctuation among\ndifferent in-context learning settings suggest that the model only learns the shallow\nstructure knowledge about the task rather than the implicit reasoning skill. Regarding\nsmaller instruction-tuned models, Flan-T5 outperforms GPT-3 and is comparable to\nGPT−3.5 in the 16-shot setting. However, its performance is still near random, sug-\ngesting that it understands the task better due to many instruction-ﬁne-tuning tasks but\nstill fails to learn the logical reasoning rules from the instructions. For smaller models,\nboth RoBERTa and DeBERTa show near-random performances. Their lack of knowl-\nedge of monotonicity reasoning is expected as Chen and Gao ( 2022) showed that\npretrained transformer-based models may not encode much monotonicity informa-\ntion during their pre-training process. Nevertheless, even ﬁne-tuning with commonly\nused NLI training data still fails to beneﬁt models’ performance on monotonicity rea-\nsoning. Such results lead to concerns about the learning quality of the models and the\nlack of logical reasoning samples in these common NLI datasets. Overall, we show\nthat large language models still require a major effort to improve their reasoning ability\non logic.\n5 Conclusions\nIn this paper, we provide an in-depth discussion of monotonicity reasoning in the age\nof neural foundation models. To summarize, we ﬁrst propose the AttentiveTreeNet\nto investigate the effectiveness of incorporating structural knowledge and linguistic\n123\n64 Z. Chen and Q. Gao\nprinciples into neural architectures on monotonicity reasoning. Next, we propose a\nhybrid reasoning framework that utilizes both symbolic reasoning modules built from\nhuman-deﬁned logical rules and neural language models to solve monotonicity NLI\nproblems. For the third part, we analyze several popular and powerful large foundation\nmodels on monotonicity reasoning to verify if the ability to emulate logical reason-\ning has emerged in these massive neural models. Our evaluation focus on the MED\nbenchmark and the CURRICULUM benchmark’s monotonicity section. Our analysis\nshows that injecting structural knowledge into advanced neural networks can largely\nimprove the original network’s performance on monotonicity inference. However, per-\nforming reasoning jointly using symbolic and neural modules can further master the\nmonotonicity reasoning task and achieve state-of-the-art performance while maintain-\ning high interpretability. We show that large language models are far from mastering\nthe skill of logical reasoning. Although popular models like InstructGPT can make\npowerful generations and predictions for various linguistic tasks and applications,\nthey can only achieve a random performance on monotonicity reasoning. Overall, our\nwork reveals the limitation of current large foundation models and sheds light on the\nnew direction of approaching logical reasoning through neural-symbolic inference.\nFor future work, it would be exciting to see symbolic reasoning systems built on top\nof large language models for complex logical reasoning tasks.\nFunding Open access funding provided by EPFL Lausanne\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,\nand indicate if changes were made. The images or other third party material in this article are included\nin the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If\nmaterial is not included in the article’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the\ncopyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .\nReferences\nAbzianidze, L. (2017). LangPro: Natural language theorem prover. In: Proceedings of the 2017 Conference\non Empirical Methods in Natural Language Processing: System Demonstrations, pp. 115–120. Associ-\nation for Computational Linguistics, Copenhagen, Denmark. https://doi.org/10.18653/v1/D17-2020 .\nhttps://aclanthology.org/D17-2020\nBommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.S., Bohg, J.,\nBosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N., Chen,\nA., Creel, K., Davis, J.Q., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S.,\nEtchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N.,\nGrossman, S., Guha, N., Hashimoto, T., Henderson, P ., Hewitt, J., Ho, D.E., Hong, J., Hsu, K., Huang,\nJ., Icard, T., Jain, S., Jurafsky, D., Kalluri, P ., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh,\nP .W., Krass, M., Krishna, R., Kuditipudi, R., Kumar, A., Ladhak, F., Lee, M., Lee, T., Leskovec, J.,\nLevent, I., Li, X.L., Li, X., Ma, T., Malik, A., Manning, C.D., Mirchandani, S., Mitchell, E., Munyikwa,\nZ., Nair, S., Narayan, A., Narayanan, D., Newman, B., Nie, A., Niebles, J.C., Nilforoshan, H., Nyarko,\nJ., Ogut, G., Orr, L., Papadimitriou, I., Park, J.S., Piech, C., Portelance, E., Potts, C., Raghunathan,\nA., Reich, R., Ren, H., Rong, F., Roohani, Y ., Ruiz, C., Ryan, J., Ré, C., Sadigh, D., Sagawa, S.,\nSanthanam, K., Shih, A., Srinivasan, K., Tamkin, A., Taori, R., Thomas, A.W., Tramér, F., Wang,\nR.E., Wang, W., Wu, B., Wu, J., Wu, Y ., Xie, S.M., Y asunaga, M., Y ou, J., Zaharia, M., Zhang, M.,\n123\nMonotonicity Reasoning In the Age... 65\nZhang, T., Zhang, X., Zhang, Y ., Zheng, L., Zhou, K. & Liang, P . (2022). On the Opportunities and\nRisks of Foundation Models.\nBowman, S.R., Angeli, G., Potts, C. & Manning, C.D. (2015). A large annotated corpus for learning\nnatural language inference. In: Proceedings of the 2015 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 632–642. Association for Computational Linguistics, Lisbon, Portugal.\nhttps://doi.org/10.18653/v1/D15-1075 . https://aclanthology.org/D15-1075\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P ., Neelakantan, A., Shyam, P ., Sastry,\nG., Askell, A., et al. (2020). Language models are few-shot learners. Advances in Neural Information\nProcessing Systems, 33 , 1877–1901.\nCer, D., Diab, M., Agirre, E., Lopez-Gazpio, I. & Specia, L. (2017). SemEval-2017 task 1: Semantic textual\nsimilarity multilingual and crosslingual focused evaluation. In: Proceedings of the 11th International\nWorkshop on Semantic Evaluation (SemEval-2017), pp. 1–14. Association for Computational Linguis-\ntics, V ancouver, Canada. https://doi.org/10.18653/v1/S17-2001 . https://aclanthology.org/S17-2001\nChen, Z. & Gao, Q. (2021). Monotonicity marking from Universal Dependency trees. In: Proceedings of\nthe 14th International Conference on Computational Semantics (IWCS), pp. 121–131. Association\nfor Computational Linguistics, Groningen, The Netherlands (online). https://aclanthology.org/2021.\niwcs-1.12\nChen, Z. & Gao, Q. (2022). Curriculum: A broad-coverage benchmark for linguistic phenomena in natural\nlanguage understanding. In: Proceedings of the 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, pp. 3204–3219.\nAssociation for Computational Linguistics, Seattle, United States. https://doi.org/10.18653/v1/2022.\nnaacl-main.234. https://aclanthology.org/2022.naacl-main.234\nChen, D. & Manning, C. (2014). A fast and accurate dependency parser using neural networks. In: Proceed-\nings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp.\n740–750. Association for Computational Linguistics, Doha, Qatar. https://doi.org/10.3115/v1/D14-\n1082. https://aclanthology.org/D14-1082\nChen, Z. (2021). Attentive tree-structured network for monotonicity reasoning. In: Proceedings of the 1st\nand 2nd Workshops on Natural Logic Meets Machine Learning (NALOMA), pp. 12–21. Association\nfor Computational Linguistics, Groningen, the Netherlands (online). https://aclanthology.org/2021.\nnaloma-1.3\nChen, Z., Gao, Q. & Moss, L.S. (2021). NeuralLog: Natural language inference with joint neural and logical\nreasoning. In: Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational\nSemantics, pp. 78–88. Association for Computational Linguistics, Online. https://doi.org/10.18653/\nv1/2021.starsem-1.7. https://aclanthology.org/2021.starsem-1.7\nChen, Q., Zhu, X., Ling, Z.-H., Wei, S., Jiang, H. & Inkpen, D. (2017). Enhanced LSTM for natural\nlanguage inference. In: Proceedings of the 55th Annual Meeting of the Association for Computational\nLinguistics (V ol. 1), pp. 1657–1668. Association for Computational Linguistics, V ancouver, Canada.\nhttps://doi.org/10.18653/v1/P17-1152 . https://aclanthology.org/P17-1152\nChen, Z., & Gao, Q. (2022). Probing linguistic information for logical inference in pre-trained language\nmodels. Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 36 (10), 10509–10517. https://\ndoi.org/10.1609/aaai.v36i10.21294\nChung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y ., Fedus, W., Li, Y ., Wang, X., Dehghani, M., Brahma,\nS., Webson, A., Gu, S.S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Castro-Ros, A., Pellat,\nM., Robinson, K., V alter, D., Narang, S., Mishra, G., Y u, A., Zhao, V ., Huang, Y ., Dai, A., Y u, H.,\nPetrov, S., Chi, E.H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q.V . & Wei, J. (2022) Scaling\nInstruction-Finetuned Language Models.\nConneau, A., Kiela, D., Schwenk, H., Barrault, L. & Bordes, A. (2017). Supervised learning of universal\nsentence representations from natural language inference data. In: Proceedings of the 2017 Conference\non Empirical Methods in Natural Language Processing. https://doi.org/10.18653/v1/d17-1070\nDagan, I., Roth, D., Sammons, M. & Zanzotto, F.M. (2013). Recognizing Textual Entailment: Models and\nApplications. Synthesis Lectures on Human Language Technologies, pp. 1–220. Morgan and Claypool\nPublishers.\nDevlin, J., Chang, M.-W., Lee, K. & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In: Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, V ol. 1, pp.\n4171–4186. Association for Computational Linguistics, Minneapolis, Minnesota. https://doi.org/10.\n18653/v1/N19-1423 . https://aclanthology.org/N19-1423\n123\n66 Z. Chen and Q. Gao\nDolan, W.B. & Brockett, C. (2005). Automatically constructing a corpus of sentential paraphrases. In:\nProceedings of the Third International Workshop on Paraphrasing (IWP2005). https://aclanthology.\norg/I05-5002\nGlockner, M., Shwartz, V . & Goldberg, Y . (2018). Breaking NLI systems with sentences that require simple\nlexical inferences. In: Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics. V ol. 2, pp. 650–655. Association for Computational Linguistics, Melbourne, Australia.\nhttps://doi.org/10.18653/v1/P18-2103 . https://aclanthology.org/P18-2103\nGururangan, S., Swayamdipta, S., Levy, O., Schwartz, R., Bowman, S. & Smith, N.A. (2018). Annotation\nartifacts in natural language inference data. In: Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies,\nV ol. 2, pp. 107–112. Association for Computational Linguistics, New Orleans, Louisiana. https://doi.\norg/10.18653/v1/N18-2017 . https://aclanthology.org/N18-2017\nHe, P ., Liu, X., Gao, J. & Chen, W. (2021). DeBERTa: Decoding-Enhanced BERT with Disentangled\nAttention.\nHu, H., Chen, Q. & Moss, L. (2019). Natural language inference with monotonicity. In: Proceedings of the\n13th International Conference on Computational Semantics, pp. 8–15. Association for Computational\nLinguistics, Gothenburg, Sweden. https://doi.org/10.18653/v1/W19-0502 . https://aclanthology.org/\nW19-0502\nHu, H., Chen, Q., Richardson, K., Mukherjee, A., Moss, L.S. & Kuebler, S. (2020). MonaLog: a lightweight\nsystem for natural language inference based on monotonicity. In: Proceedings of the Society for\nComputation in Linguistics 2020, pp. 334–344. Association for Computational Linguistics, New Y ork,\nNew Y ork. https://aclanthology.org/2020.scil-1.40\nKingma, D.P . & Ba, J. (2014). Adam: A Method for Stochastic Optimization.\nKojima, T., Gu, S.S., Reid, M., Matsuo, Y . & Iwasawa, Y .(2022). Large Language Models are Zero-Shot\nReasoners. arXiv. https://doi.org/10.48550/ARXIV .2205.11916. arXiv:2205.11916\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P . & Soricut, R. (2020). Albert: A lite bert for\nself-supervised learning of language representations. In: International Conference on Learning Rep-\nresentations. https://openreview.net/forum?id=H1eA7AEtvS\nLin, Z., Feng, M., dos Santos, C.N., Y u, M., Xiang, B., Zhou, B. & Bengio, Y . (2017). A structured self-\nattentive sentence embedding. ArXiv abs/1703.03130\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L. & Stoyanov,\nV . (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv. https://doi.org/10.\n48550/ARXIV .1907.11692. arXiv:1907.11692\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L. & Stoyanov,\nV . (2020). RoBERTa: A Robustly Optimized BERT Pretraining Approach. https://openreview.net/\nforum?id=SyxS0T4tvS\nLiu, P ., Y uan, W., Fu, J., Jiang, Z., Hayashi, H. & Neubig, G. (2021). Pre-train, Prompt, and Predict: A\nSystematic Survey of Prompting Methods in Natural Language Processing. arXiv . https://doi.org/10.\n48550/ARXIV .2107.13586. arXiv:2107.13586\nLiu, H., & Singh, P . (2004). Conceptnet - A practical commonsense reasoning tool-kit. BT Technology\nJournal, 22(4), 211–226. https://doi.org/10.1023/B:BTTJ.0000047600.45421.6d\nMacCartney, B. & Manning, C.D. (2009). An extended model of natural logic. In: Proceedings of the Eight\nInternational Conference on Computational Semantics, pp. 140–156. Association for Computational\nLinguistics, Tilburg, The Netherlands. https://aclanthology.org/W09-3714\nMartínez-Gómez, P ., Mineshima, K., Miyao, Y . & Bekki, D. (2017). On-demand injection of lexical knowl-\nedge for recognising textual entailment. In: Proceedings of the 15th Conference of the European\nChapter of the Association for Computational Linguistics: V ol. 1, pp. 710–720. Association for Com-\nputational Linguistics, V alencia, Spain. https://aclanthology.org/E17-1067\nMcCoy, T., Pavlick, E. & Linzen, T. (2019). Right for the wrong reasons: Diagnosing syntactic heuristics\nin natural language inference. In: Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pp. 3428–3448. Association for Computational Linguistics, Florence, Italy.\nhttps://doi.org/10.18653/v1/P19-1334 . https://aclanthology.org/P19-1334\nMiller, G. A. (1995). Wordnet: A lexical database for english. Communications of the ACM, 38 (11), 39–41.\nhttps://doi.org/10.1145/219717.219748\nMin, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H. & Zettlemoyer, L. (2022). Rethinking\nthe role of demonstrations: What makes in-context learning work? ArXiv abs/2202.12837\n123\nMonotonicity Reasoning In the Age... 67\nNie, Y ., Williams, A., Dinan, E., Bansal, M., Weston, J. & Kiela, D. (2020). Adversarial NLI: A new\nbenchmark for natural language understanding. In: Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pp. 4885–4901. Association for Computational Linguis-\ntics, Online. https://doi.org/10.18653/v1/2020.acl-main.441 . https://aclanthology.org/2020.acl-main.\n441\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.L., Mishkin, P ., Zhang, C., Agarwal, S., Slama,\nK., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P .,\nChristiano, P ., Leike, J. & Lowe, R. (2022). Training Language Models to Follow Instructions with\nHuman Feedback\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.L., Mishkin, P ., Zhang, C., Agarwal, S., Slama,\nK., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P .,\nChristiano, P ., Leike, J. & Lowe, R.(2022). Training Language Models to Follow Instructions with\nHuman Feedback. arXiv . https://doi.org/10.48550/ARXIV .2203.02155. arXiv:2203.02155\nParikh, A., Täckström, O., Das, D. & Uszkoreit, J. (2016). A decomposable attention model for natural\nlanguage inference. In: Proceedings of the 2016 Conference on Empirical Methods in Natural Lan-\nguage Processing, pp. 2249–2255. Association for Computational Linguistics, Austin, Texas. https://\ndoi.org/10.18653/v1/D16-1244 . https://aclanthology.org/D16-1244\nPartee, B. (2007). Compositionality and coercion in semantics: The dynamics of adjective meaning 1.\nPennington, J., Socher, R. & Manning, C. (2014). GloV e: Global vectors for word representation. In:\nProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),\npp. 1532–1543. Association for Computational Linguistics, Doha, Qatar. https://doi.org/10.3115/v1/\nD14-1162. https://aclanthology.org/D14-1162\nQi, P ., Zhang, Y ., Zhang, Y ., Bolton, J. & Manning, C.D. (2020). Stanza: A python natural language process-\ning toolkit for many human languages. In: Proceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics: System Demonstrations, pp. 101–108. Association for Computational\nLinguistics, Online. https://doi.org/10.18653/v1/2020.acl-demos.14 . https://aclanthology.org/2020.\nacl-demos.14\nReimers, N. & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings Using Siamese BERT-\nNetworks.\nRichardson, K., Hu, H., Moss, L.S. & Sabharwal, A. (2019). Probing Natural Language Inference Models\nthrough Semantic Fragments.\nRubin, O., Herzig, J. & Berant, J. (2021). Learning to Retrieve Prompts for In-Context Learning. ArXiv\nabs/2112.08633\nSanh, V ., Webson, A., Raffel, C., Bach, S.H., Sutawika, L., Alyafeai, Z., Chafﬁn, A., Stiegler, A., Scao,\nT.L., Raja, A., et al. (2021). Multitask Prompted Training Enables Zero-Shot Task Generalization.\narXiv preprint arXiv:2110.08207\nTai, K.S., Socher, R. & Manning, C.D. (2015). Improved semantic representations from tree-structured\nlong short-term memory networks. In: Proceedings of the 53rd Annual Meeting of the Association for\nComputational Linguistics and the 7th International Joint Conference on Natural Language Processing\n(V ol. 1), pp. 1556–1566. Association for Computational Linguistics, Beijing, China. https://doi.org/\n10.3115/v1/P15-1150 . https://aclanthology.org/P15-1150\nThorne, J., Vlachos, A., Christodoulopoulos, C. & Mittal, A. (2018). FEVER: a large-scale dataset for fact\nextraction and VERiﬁcation. In: Proceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, V olume 1 (Long\nPapers), pp. 809–819. Association for Computational Linguistics, New Orleans, Louisiana. https://\ndoi.org/10.18653/v1/N18-1074 . https://aclanthology.org/N18-1074\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Roziére, B., Goyal, N.,\nHambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E. & Lample, G. (2023). LLaMA: Open and\nefﬁcient foundation language models.\nWang, S. & Jiang, J. (2016). Learning natural language inference with LSTM. In: Proceedings of the 2016\nConference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, pp. 1442–1451. Association for Computational Linguistics, San Diego, Cal-\nifornia. https://doi.org/10.18653/v1/N16-1170 . https://aclanthology.org/N16-1170\nWang, Z., Hamza, W. & Florian, R. (2017). Bilateral multi-perspective matching for natural language\nsentences. In: Proceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelli-\ngence, IJCAI-17, pp. 4144–4150. https://doi.org/10.24963/ijcai.2017/579 . https://doi.org/10.24963/\nijcai.2017/579\n123\n68 Z. Chen and Q. Gao\nWei, J., Bosma, M., Zhao, V .Y ., Guu, K., Y u, A.W., Lester, B., Du, N., Dai, A.M. & Le, Q.V . (2021).\nFinetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652\nWei, J., Tay, Y ., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Y ogatama, D., Bosma, M., Zhou, D.,\nMetzler, D., Chi, E.H., Hashimoto, T., Vinyals, O., Liang, P ., Dean, J. & Fedus, W. (2022). Emergent\nAbilities of Large Language Models.\nWilliams, A., Nangia, N. & Bowman, S. (2018). A broad-coverage challenge corpus for sentence under-\nstanding through inference. In: Proceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, V ol. 1, pp. 1112–\n1122. Association for Computational Linguistics, New Orleans, Louisiana. https://doi.org/10.18653/\nv1/N18-1101. https://aclanthology.org/N18-1101\nWilliams, A., Nangia, N. & Bowman, S. (2018). A broad-coverage challenge corpus for sentence under-\nstanding through inference. In: Proceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, V ol. 1, pp. 1112–\n1122. Association for Computational Linguistics. http://aclweb.org/anthology/N18-1101\nXie, S.M., Raghunathan, A., Liang, P . & Ma, T. (2021). An Explanation of In-Context Learning as Implicit\nBayesian Inference. ArXiv abs/2111.02080\nY anaka, H., Mineshima, K., Bekki, D., Inui, K., Sekine, S., Abzianidze, L. & Bos, J. (2019). Can neural net-\nworks understand monotonicity reasoning? In: Proceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP , pp. 31–40. Association for Computational Lin-\nguistics, Florence, Italy. https://doi.org/10.18653/v1/W19-4804 . https://aclanthology.org/W19-4804\nY anaka, H., Mineshima, K., Bekki, D., Inui, K., Sekine, S., Abzianidze, L. & Bos, J. (2019). HELP: A\ndataset for identifying shortcomings of neural models in monotonicity reasoning. In: Proceedings\nof the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019), pp. 250–\n255. Association for Computational Linguistics, Minneapolis, Minnesota. https://doi.org/10.18653/\nv1/S19-1027. https://aclanthology.org/S19-1027\nY anaka, H., Mineshima, K., Martínez-Gómez, P . & Bekki, D. (2018). Acquisition of phrase correspondences\nusing natural deduction proofs. In: Proceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, V ol. 1, pp. 756–\n766. Association for Computational Linguistics, New Orleans, Louisiana. https://doi.org/10.18653/\nv1/N18-1069. https://aclanthology.org/N18-1069\nY e, X. & Durrett, G.(2022). The Unreliability of Explanations in Few-Shot in-Context Learning. ArXiv\nabs/2205.03401\nZeman, D., Hajiˇc, J., Popel, M., Potthast, M., Straka, M., Ginter, F., Nivre, J. & Petrov, S. (2018). CoNLL\n2018 shared task: Multilingual parsing from raw text to Universal Dependencies. In: Proceedings of\nthe CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pp. 1–\n21. Association for Computational Linguistics, Brussels, Belgium. https://doi.org/10.18653/v1/K18-\n2001. https://aclanthology.org/K18-2001\nZhao, K., Huang, L. & Ma, M. (2016). Textual entailment with structured attentions and composition.\nIn: Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics:\nTechnical Papers, pp. 2248–2258. The COLING 2016 Organizing Committee, Osaka, Japan. https://\naclanthology.org/C16-1212\nZhou, Y ., Liu, C. & Pan, Y .(2016). Modelling sentence pairs with tree-structured attentive encoder. In:\nProceedings of COLING 2016, the 26th International Conference on Computational Linguistics:\nTechnical Papers, pp. 2912–2922. The COLING 2016 Organizing Committee, Osaka, Japan. https://\naclanthology.org/C16-1274\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps\nand institutional afﬁliations.\n123",
  "topic": "Monotonic function",
  "concepts": [
    {
      "name": "Monotonic function",
      "score": 0.677972674369812
    },
    {
      "name": "Computer science",
      "score": 0.6742578744888306
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6504325866699219
    },
    {
      "name": "Task (project management)",
      "score": 0.5496245622634888
    },
    {
      "name": "Artificial neural network",
      "score": 0.4810905158519745
    },
    {
      "name": "Range (aeronautics)",
      "score": 0.4660799205303192
    },
    {
      "name": "Machine learning",
      "score": 0.46406230330467224
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.4540998041629791
    },
    {
      "name": "Scale (ratio)",
      "score": 0.44904130697250366
    },
    {
      "name": "Deep learning",
      "score": 0.44370022416114807
    },
    {
      "name": "Foundation (evidence)",
      "score": 0.44358888268470764
    },
    {
      "name": "Cognitive science",
      "score": 0.40170958638191223
    },
    {
      "name": "Programming language",
      "score": 0.18911725282669067
    },
    {
      "name": "Mathematics",
      "score": 0.16546428203582764
    },
    {
      "name": "Psychology",
      "score": 0.11939659714698792
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Materials science",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Composite material",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I5124864",
      "name": "École Polytechnique Fédérale de Lausanne",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I36258959",
      "name": "University of California, San Diego",
      "country": "US"
    }
  ]
}