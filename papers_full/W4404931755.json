{
    "title": "HindiLLM: Large Language Model for Hindi",
    "url": "https://openalex.org/W4404931755",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2424861057",
            "name": "Sanjay Chouhan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2518246819",
            "name": "Shubha Brata Nath",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2135472562",
            "name": "Aparajita Dutta",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3088659275",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W3105425516",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W3099919888",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W2952638691",
        "https://openalex.org/W3037109418",
        "https://openalex.org/W3081168214",
        "https://openalex.org/W2964583233",
        "https://openalex.org/W3117753934",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3112784227",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3100403097"
    ],
    "abstract": null,
    "full_text": "HindiLLM: Large Language Model for Hindi\nSanjay Chouhan1, Shubha Brata Nath1, and Aparajita Dutta2\n1 Indian Institute of Information Technology Guwahati, Bongora, Assam 781015,\nIndia\n{sanjay.chouhanm22,shubha}@iiitg.ac.in\n2 National Institute of Technology Silchar, Silchar, Assam 788010, India\naparajita.dutta@cse.nits.ac.in\nAbstract. TheadvancementsintheLargeLanguageModel(LLM)have\nhelped in solving several problems related to language processing. Most\noftheresearcheshavefocusedontheEnglishlanguageonly,becauseofits\npopularity and abundance on the internet. However, a high-performance\nlanguage model for Hindi and other Indic languages is lacking in the\nliterature. In this work, we have pre-trained two autoregressive LLM\nmodels for the Hindi language, namely HindiLLM-Small and HindiLLM-\nMedium.Weuseatwo-stepprocesscomprisingunsupervisedpre-training\nand supervised fine-tuning. First, we create a large and high-quality text\ncorpus for unsupervised pre-training. Next, we train a Byte-Pair En-\ncoding, named HindiLLM tokenizer, using the pre-training text data.\nWe then perform training on the unlabeled data, known as the pre-\ntraining step, to get the HindiLLM base models. Furthermore, we per-\nform fine-tuning of the HindiLLM base models for different tasks like\nsentiment analysis, text classification, natural language inference, and\nmultiple choice question-answer on popular labeled datasets to measure\nthe real-world performance. The evaluation shows that the HindiLLM-\nbased fine-tuned models outperform several models in most of the lan-\nguage related tasks.\nKeywords: Autoregressive Language Model · Large Language Model\n(LLM) · Hindi Language· Natural Language Processing (NLP)\n1 Introduction\nThe understanding of a language by a machine is of great interest in the Natural\nLanguage Processing (NLP) domain. The words and phrases used in a sentence\ncan have different meanings in different contexts of a sentence. Also, there is\nthe need to identify synonyms of a word, sarcastic phrases, idioms, and errors\nin the text of a language. The language models in NLP can perform these tasks\nfor the English language as there is a rich collection of datasets in the literature\nand it has been highly researched over the years. Most of the other languages,\nespecially Indo-Aryan languages or Indic languages, are less researched and have\nfewer resources as their presence on the internet is limited.\narXiv:2412.20357v1  [cs.CL]  29 Dec 2024\n2 S. Chouhan et al.\nThe Large Language Models (LLMs) are state-of-the-art for NLP tasks and\nhave been able to show great progress regarding language comprehension, in-\nference, and other language analysis. The language-related tasks for English are\nprocessed by LLMs such as GPT-4 [2] (a model from the GPT series), LLaMA-2\n[27] (a model from the LLaMA series), PaLM-2 [3] (a model from the PaLM\nseries) and Mistral-7B [11]. These models have billions of parameters and are\ntrained on hundreds of GBs of data. Hence, they can perform advanced NLP\ntasks such as instruction following, code generation, new content generation, and\ninformation retrieval. However, the initial versions of these models were smaller\nand hence less capable.\nHindi is an Indic language written in the Devanagari script. It is a subject-\nobject-verb (SOV) language and is morphologically rich. According to the26th\nedition of Ethnologue1 published in 2023, Hindi is the third most spoken lan-\nguage having 609.5 million speakers. It is the official language of India and\naccording to Forbes India2, 44% of India speaks Hindi. However, the language\nmodels for the Hindi language lag behind due to the challenges mentioned below.\n– The data collection has issues as there is limited availability of rich Hindi\ncorpora, both for pre-training as well as fine-tuning.\n– The complexity of Hindi Devanagari text needs to be taken into consider-\nation. The intricacies of the Hindi script including conjunct characters and\nnuanced linguistic structures introduce complexity in Hindi text processing.\n– The NLP model needs to understand the Hindi language in various contexts\nfor tasks like sentiment analysis, inference and text summarization.\nThis work aims at the following contributions to tackle the above mentioned\nchallenges.\n– Our work focuses on training a tokenizer for the Hindi language using Byte\nPair Encoding (BPE) tokenization algorithm3.\n– We train two autoregressive models for the Hindi language of different sizes.\n– We perform supervised fine-tuning for multiple downstream tasks of the\nHindi language.\nThe remainder of the paper is organized as follows. We discuss the existing\nworks in Section 2. Section 3 explains the dataset used in this work. We discuss\nthemethodologyinSection4.TheperformanceevaluationispresentedinSection\n5. Finally, Section 6 concludes the work with future directions.\n2 Related Work\nThere is a scarcity of research on the language models for Hindi language and\nother Indic languages. Hindi speakers are present all over the world and yet it is\n1 https://www.ethnologue.com/insights/ethnologue200/\n2 https://www.forbesindia.com/article/news-by-numbers/hindi-day-2020-indias-\nmostspoken-languages-are/62577/1\n3 https://huggingface.co/learn/nlp-course/en/chapter6/5\nHindiLLM: Large Language Model for Hindi 3\nless-researched in the field of NLP. One major reason behind this is the limited\npresence of Indic languages online in written form. Now, we discuss the related\nworks present in the literature.\nIn the work by Arora et al. [4], the authors focused on pre-training language\nmodelsfor13Indiclanguages.Theypre-trainedULMFiT[9]andTransformerXL\n[7] language models from scratch. The authors in Kakwani et al. [12] contributed\nmultiple resources such as monolingual corpora, pre-trained word embeddings,\nIndicGLUE benchmark dataset and pre-trained language models. They used\nthe ALBERT [15] model for pre-training language models. ALBERT, being a\ncompact model, is lightweight and requires less training data which is good for\nless-resource languages.\nVries et al. [30] recycled the pre-trained GPT-2 [24] models for Italian and\nDutch languages. They primarily retrained the lexical embeddings. First, they\ncreated a new BPE vocabulary for the target language. Then, they re-initialized\nthe lexical embeddings of the GPT-2 model for the new vocabulary and re-\ntrained them. They mentioned that the full model can be finetuned later with a\nsmaller learning rate. This helps the model to better adjust to the new language\nwhereas the re-learned lexical embeddings reduce the risk of information loss.\nOwenetal.[19]discussedincrementalpre-trainingforadaptingEnglishbased\nLLMs to non-English languages such as Indonesian. First, they expanded the vo-\ncabulary by integrating a new trained tokenizer with the existing one. Then, they\ndid incremental pre-training of Llama-2-7B-Base [27] using Low-Rank Adapta-\ntion (LORA) [10] technique. This helps the model to learn new language without\ncatastrophically forgetting the English language in a minimal resource require-\nment setup. Owen et al. [19] and Vries et al. [30] discussed about utilizing English\nbased pre-trained LLMs. Since these are pre-trained on English language, their\ntechniques are not suitable for building the HindiLLM models because Hindi is\nvery different from English and is written in Devanagari instead of Latin.\nNiyogi et al. [18] trained multiple auto-regressive models from scratch for10\nIndian languages. Their models are also based on Transformers decoders. How-\never, they did not provide a detailed description of the data used and the model\narchitecture. Radford et al. [23] discussed semi-supervised training approach\nwhich is a two-step training process for the English language. The first step is\ngenerative pre-training on a large unlabeled corpus which gives a good initial-\nization point for the next step, which is discriminative fine-tuning on each spe-\ncific task. The GPT-1 is an auto-regressive transformer [29] based decoder only\nmodel. We utilized similar semi-supervised training approach for our HindiLLM\nmodels.\nAlthough our work focuses on the Hindi language, we can apply the language\nmodel techniques across different languages. This will enable the less-researched\nlanguages to benefit by utilizing the techniques mentioned for extensively re-\nsearched languages.\n4 S. Chouhan et al.\n3 Dataset\nThe dataset generation is the first step of any model building process. The\nrichness of the data determines the quality of the model. The approach used\nin this paper requires two types of data, unlabeled data for unsupervised pre-\ntraining and labeled data for supervised fine-tuning. The details of the datasets\nused in this paper are mentioned in this section.\n3.1 Pre-Training Dataset\nFor the unsupervised pre-training step, we need a large corpus with Hindi text\nwritten in Devanagari script. Since pre-training is the most crucial step which\nhelps the model in understanding the language and its nuances, we need the\ncorpus to be clean and have valid long paragraphs. This implies that it should\nnot have unnecessary symbols, words, web-links and characters which we do\nnot see in typical Hindi literature. A paragraph or sentence is valid if it follows\nthe grammar of that language and it makes sense to the native speaker of that\nlanguage. Long paragraphs are good for introducing long-term dependencies in\nthe model. Generally, we need to scrap the internet for large corpus but this leads\nto difficulty in finding usable Hindi sentences or paragraphs. There exist projects\nwhich focus on getting web-crawled texts. In these projects, these corpora are\nclassified based on language and are preprocessed following the structure of that\nlanguage. We have used such existing corpora for the pre-training step.\nTable 1.Detailed Description of Pre-training Corpora\nData Size (in GB)No. of Words(approx. in million)\nWikipedia 1.04 78.82\nCC-Aligned 1.3 133.89\nOSCAR-2201 14 1185.51\nCC-100 21 1714.72\nAs shown in Table 1, we have 37.34 GB of data which contains approxi-\nmately 3.11 billion words. Apart from CC-Aligned [8], all other datasets contain\nonly Hindi text written in Devanagari script. The CC-Aligned dataset contains\nHind-English translation pairs of sentences and phrases. We have concatenated\nboth versions (Hindi and English) of the sentence one after the other. One of\nthe sentences from the translation pair is chosen randomly as the first sentence\nfor each of the translation pairs. The idea here is to add some English capability\nin the model along with Hindi because we often see words or phrases of English\ninserted in Hindi texts. The translation pair will also help the model understand\nthe relationship between both languages. Hindi Wikipedia articles are also used\nfrom Kaggle4 and Tensorflow5. Wikipedia articles contain factual information\n4 https://www.kaggle.com/datasets/disisbig/hindi-wikipedia-articles-172k\n5 https://www.tensorflow.org/datasets/catalog/wikipedia\nHindiLLM: Large Language Model for Hindi 5\nand the sentences are well formed as well, unlike some internet forums. The\nOSCAR-2201 [1] or Open Super-large Crawled Aggregated coRpus is a multilin-\ngual corpus intended for pre-training language models. It contains 151 different\nlanguages but we only use the Hindi texts. The CC-100 [6] [31] is a monolingual\ncorpus containing texts in 100+ languages out of which we only utilize the Hindi\ntexts. It is generated using the open-source CC-Net [31] repository.\nApart from the default preprocessing done by the dataset creator, we have\nadditionally performed the following preprocessing steps.\n– Content Cleanup- Removal of content within brackets, hyperlinks, extra\nspaces and formatting of punctuation marks.\n– Filtering Sentence- Sentences where less than 30% of words are unique\nare filtered out. These are not valid sentences because same few words are\nrepeated multiple times.\n– Numeric and Punctuation Removal- Removal of lines containing only\nnumeric, punctuation marks or special symbols.\n– Handling Short Lines- Multiple consecutive lines with less than fours\nwords were removed. These were generally navigation links of a website.\n3.2 Fine-Tuning Dataset\nThe fine-tuning is done for downstream tasks. The performance on downstream\ntasks tells about the real-world applicability of the model. In this paper, we have\nchosen seven downstream tasks to measure different aspects of our models.\nSentiment Analysis: We have two sentiment analysis datasets, namely IITP\nMovie6 and IITP Product7. These datasets are public and widely used for eval-\nuating Hindi language models. The dataset comprises three classes: positive,\nneutral, and negative. We have combined both datasets for training but tested\nseparately.\nTextClassification: Formulticlassclassificationevaluation,wehaveusedBBC\nNews category8 classification dataset. It has six categories. These are India,\ninternational, news, entertainment, sports, and science. It is also a public dataset\nwhich has been used for testing multiple Hindi based language models.\nNatural Language Inference: We analyzed the natural language inference\ncapability of our models with the BBC NLI [28] dataset.\nCloze-style Multiple-choice QA (CSQA):The CSQA dataset is from In-\ndicGLUE [12] benchmark dataset. This dataset has a masked entity in a given\ntext and we are given four candidate entities out of which one is the correct\nentity. This dataset is created from Wikipedia articles.\n6 https://www.kaggle.com/datasets/warcoder/iit-patna-movie-reviews-hindi\n7 https://www.kaggle.com/datasets/warcoder/iit-patna-product-reviews\n8 https://github.com/NirantK/hindi2vec/releases/tag/bbc-hindi-v0.1\n6 S. Chouhan et al.\nWikipedia Section-title Prediction (WSTP):Similar to CSQA, WSTP is\nfrom the IndicGLUE benchmark dataset and created using Wikipedia articles.\nThe dataset has Wikipedia sections and it is required to find out the section title\nfrom the given four choices of titles.\nDiscourse Mode Classification (DM): The DM dataset is also from the\nIndicGLUE benchmark dataset. It is a discourse analysis dataset with five dis-\ncourse categories: descriptive, narrative, dialogue, argumentative and informa-\ntive. In this task, the model has to predict the suitable discourse category for a\ngiven sentence.\nMachine Translation Dataset: We have obtained the translation dataset\nfrom Kunchukuttan et al. [14]. It has pairs of Hindi and English versions of\n1.49 million sentences. This dataset is specifically created for the Hindi-English\ntranslation task. The machine translation is a generative downstream task.\n4 Methodology\nWe follow two steps training process in this work. We apply an unsupervised pre-\ntraining to get the base model and a supervised fine-tuning of the base model for\nthe downstream tasks. However, prior to the training, we build the HindiLLM\ntokenizer using the BPE algorithm. In this section, we have provided a detailed\ndescription of the tokenizer and the models along with the training process.\n4.1 Tokenizer\nSince we focus on building the model for the Hindi language and the default\nGPT-2 [24] model is primarily for the English language. So, we train a new\ntokenizer called HindiLLM tokenizer. The idea of training a custom tokenizer is\nto reduce the fertility score (average number of tokens per word) for the Hindi\nlanguage. We have trained a Byte-level BPE tokenizer with our pre-training\ncorpora which contains mostly Hindi language written in Devanagari script.\nSince the Devanagari script is complex, the Byte-level BPE tokenizer is the\nmost suitable option. We have used the whole pre-training dataset to train the\nBPE tokenizer. We have kept the desired vocabulary size as 50000 while using\nthe trainer to accommodate most of the frequently occurring sub-words. Also,\nwe added 8 special tokens like CLS, SEP and PAD afterwards, so the vocabulary\nsize reached to 50008.\nWe show the output of the HindiLLM tokenizer in Figure 1. There are four\nsentences (three Hindi and one English) and its corresponding tokens as to-\nkenized by our tokenizer. We can see that the words are split into multiple\nsub-words. For example, the word \"aApkA\" is tokenized into \"aApk\" and \"A\"\nbut the word \"aAp\" is a token on its own. The HindiLLM tokenizer is able to\ntokenize both Hindi and English sentences.\nHindiLLM: Large Language Model for Hindi 7\nFig. 1.Output of HindiLLM Tokenizer\nTo check if the HindiLLM tokenizer makes sense or not, we have taken a\n100 words paragraph written in Hindi, encoded it using our own and the default\nGPT-2 tokenizer. We see that our tokenizer takes 345 tokens and the default\nGPT-2’s tokenizer takes 785 tokens to represent the same paragraph with 100\nwords. It takes less than half the number of tokens for our HindiLLM tokenizer.\nThis indicates that the HindiLLM tokenizer has lower fertility score for the Hindi\nlanguage. Hence, we can pass larger Hindi sentences or paragraphs to the model.\nThis will also improve the efficiency in processing Hindi text. Therefore, creation\nof a new tokenizer is beneficial here.\n4.2 Unsupervised Pre-Training\nAs mentioned in the GPT-1 [23] work, the pre-training finds a good initialization\npoint for the model. In pre-training, the model learns about the language such\nas morphology and syntax. The Causal Language Modeling (CLM) is used in\nthe unsupervised pre-training step of autoregressive models. This step gives us a\nbase model that can be supervised fine-tuned for several downstream tasks with\na relatively smaller dataset. We have trained two models, HindiLLM-Small and\nHindiLLM-Medium corresponding to GPT2-small and GPT2-medium respec-\ntively. We use Hugging Face’s Transformers library [32] for pre-training. The\ntraining process involves creating a model with the configuration of correspond-\ning GPT-2 models and training it after initializing with random weights.\nHindiLLM-Small Model:As shown in Table 2, the HindiLLM-Small model is\nequivalenttotheGPT2-smallmodelwhichhas124,439,808trainableparameters.\nFor training, we have taken 19.6 GB of data from our pre-training dataset.\nWe have taken approximately half data from all corpora mentioned in Table\n1. There are 6,904,242 examples for training a context window of 1024 tokens.\nWe have used a batch size of 16 for training and updated the weights 431,516\ntimes, which is equivalent to 1.45 epochs. The optimizer is a torch [20] based\nAdamW [16] optimizer with a learning rate 5e-05. Full precision training on two\n8 S. Chouhan et al.\nTable 2.Training Details of HindiLLM-Small Model\nSpecification Value\nCorpus Size 19.6 GB\nNumber of Examples 6,904,242\nTotal Train Batch Size (w. Parallel, Distributed and Ac-\ncumulation)\n16\nTotal Optimization Steps 431,516\nNumber of Trainable Parameters 124,439,808\nContext Window Size 1024 tokens\nNumber of Epochs 1.45\nOptimizer AdamW\nLearning Rate 5e-05\nGPUs Used 2 x NVIDIA A100-PCIE-40GB\nTime Required 6 days\nNVIDIA A100-PCIE-40GB GPUs have taken 6 days including the tokenization\nand evaluation steps. Instead of mixed-precision [17] training, we have opted for\nfull precision training because we have trained from scratch.\nTable 3.Training Details of HindiLLM-Medium Model\nSpecification Value\nCorpus Size 37.34 GB\nNumber of Examples 11,351,587\nTotal Train Batch Size (w. Parallel, Distributed and Ac-\ncumulation)\n32\nTotal Optimization Steps 354,737\nNumber of Trainable Parameters 354,823,168\nContext Window Size 1024 tokens\nNumber of Epochs 1.24\nOptimizer AdamW\nLearning Rate 5e-05\nGPUs Used 2 x NVIDIA A100-PCIE-40GB\nTime Required 25 days\nHindiLLM-Medium Model:As depicted in Table 3, the HindiLLM-Medium\nmodel is based on the same configuration as of GPT2-medium model which has\n354,823,168 trainable parameters. For training, we have 37.34 GB of data as\nmentioned in Table 1. There are 11,351,587 examples for training with the same\ncontext window as HindiLLM-Small. The total optimization steps are 354,737\nwith a batch size of 32 which is equal to 1.24 epochs. The optimization steps are\nless as compared to HindiLLM-Small because we have doubled the batch size.\nThe optimizer and learning rate are the same as HindiLLM-Small. Similar to the\nHindiLLM-Small model, we have used two NVIDIA A100-PCIE-40GB GPUs for\nperforming full precision training, which took 25 days including the tokenization\nand evaluation steps.\nPerformance Evaluation of Pre-trained Models:As shown in Table 4, the\nHindiLLM-Medium is better than HindiLLM-Small in terms of all the metrics.\nHindiLLM: Large Language Model for Hindi 9\nTable 4.Performance of the Pre-trained HindiLLM Models\nMetrics HindiLLM-Small HindiLLM-Medium\nEvaluation Accuracy 0.6855 0.7300\nEvaluation Loss 1.3045 1.0992\nPerplexity 3.6860 3.0017\nTrain Loss 1.3849 1.2096\nThe larger model has higher accuracy, lower loss and lower perplexity as com-\npared to the smaller one. The perplexity of 3.686 and 3.0017 for HindiLLM-Small\nand HindiLLM-Medium, respectively, assures the quality of the training.\n4.3 Supervised Fine-Tuning\nThe next step in the semi-supervised training approach is the Supervised Fine-\nTuning (SFT) on discriminative or generative tasks. The fine-tuning step aligns\nthe model with the downstream task. The previous pre-training step makes it\neasier to fine-tune because it has already gained knowledge about the language.\nHence, we can achieve higher performance on downstream tasks even with a\nsmaller dataset. The model can be fine-tuned for variety of tasks. Here, we\nhave fine-tuned for seven tasks on the datasets mentioned in Section 3.2. Since\nthe model will be used on real-world downstream applications, the SFT and\nevaluation of the resultant model are the crucial steps.\nWe have used deepspeed [25] library for fine-tuning efficiently using multi-\nGPU setup. The SFT is done in bfloat16 precision format with a learning rate\nof 5e-6.\n5 Performance Evaluation\nIn this section, the performances on the downstream tasks are evaluated. We\nhave done SFT for a variety of downstream tasks to get a detailed performance\nmeasure of the HindiLLM models. The results are compared with other models\nto validate its improvement.\n5.1 Public Classification Dataset\nAsshowninTable5,wecomparevariousmodelsonpublicclassificationdatasets.\nWehaveaccuracyscoresfromWikipedia(FT-W)[5],Wiki+CommonCrawl(FT-\nWC) [22], IndicFT [12], IndicBERT [12], mBERT [21], XLM-R [26], INLP [13]\nand iNLTK [4] models. Also, we have obtained scores from the GPT-3.5 Turbo\nmodel. For the GPT-3.5 Turbo model, we have considered zero-shot prompt-\ning and few-shot prompting. We have performed prompt engineering to find the\nbest system prompt and in the case of few-shot prompting, we have given five\nrandom examples from the training data. We can observe that the HindiLLM-\nMedium model surpasses all the models on all three datasets: IITP-Movie, IITP-\nProduct, and BBC-Article public classification datasets. The HindiLLM-Small\n10 S. Chouhan et al.\nTable 5.Classification Accuracy on Public Dataset\nModel IITP-Movie IITP-Product BBC-Article\nHindiLLM-Small 70.51 76.63 71.29\nHindiLLM-Medium 78.34 79.31 81.04\nFT-W 41.61 58.32 72.29\nFT-WC 44.52 57.17 67.44\nIndicFT 45.81 61.57 77.02\nIndicBERT-Base 59.03 71.32 74.60\nmBERT 56.77 74.57 60.55\nXLM-R 61.61 78.97 75.52\nINLP 45.81 63.48 74.25\niNLTK 57.74 75.71 78.75\nGPT-3.5 Turbo Zero-shot 68.17 68.20 56.41\nGPT-3.5 Turbo Few-shot 66.45 72.92 49.63\nmodel comes second in the case of IITP-Movie dataset. It does not perform well\non the BBC-Article dataset. For IITP-Movie dataset, we see an improvement of\n2.34% and 10.17% in HindiLLM-Small and HindiLLM-Medium models, respec-\ntively. For IITP-Product dataset, we see a decrease of2.34% and an increase\nof 0.34% in HindiLLM-Small and HindiLLM-Medium models, respectively. We\nobserve an improvement of2.29% for HindiLLM-Medium model in BBC-Article\ndataset, but a drop of7.46% for HindiLLM-Small. Both the zero-shot and few-\nshot results from GPT-3.5 Turbo are poorer than both of our models. In most\ncases, the result of few-shot is worse than the zero-shot. This is possibly because\nthe given examples are confusing the model or because with a increase in the\nprompt length, the model is not able to analyze accurately.\n5.2 IndicGLUE Benchmark Dataset\nTable 6.Accuracy Score on IndicGLUE Benchmark Dataset\nModel CSQA WSTP DM\nHindiLLM-Small 38.53 69.85 78.68\nHindiLLM-Medium 44.71 77.19 80.48\nXLM-R 30.62 76.92 79.94\nmBERT 39.00 80.12 71.20\nIndicBERT-Base 41.55 74.02 78.44\nIndicBERT-Large 37.01 77.80 NA\nGPT-3.5 Turbo Zero-shot 44.56 76.75 50.91\nGPT-3.5 Turbo Few-shot 50.84 74.25 48.89\nHindiLLM: Large Language Model for Hindi 11\nTable 6 shows the accuracy score achieved on the IndicGLUE benchmark\ndataset. We have considered XLM-R, mBERT and IndicBERT models for com-\nparison along with zero-shot and few-shot prompting of GPT-3.5 Turbo. In the\nCSQA task, the GPT-3.5 Turbo Few-shot has the highest accuracy and our\nHindiLLM-Mediummodelhasthesecondbestaccuracywhichisadropof 6.13%.\nHindiLLM-Small has the sixth best result with a drop of12.31% on this task.\nFor the WSTP task, mBERT shows the best result whereas we see a drop of\n2.93% and 10.27% for HindiLLM-Medium and HindiLLM-Small, respectively.\nFor the DM task, HindiLLM-Medium gives the best accuracy with an improve-\nment of0.54%, followed by XLM-R and HindiLLM-Small. We do not have any\nscore from IndicBERT-Large model on this task.\n5.3 Comparison with GPT-2 Models\nTable 7.Sentiment Analysis Comparison with Internally Fine-tuned GPT-2 Models\nModel Precison Recall F1-score\nHindiLLM-Small 77.5 75.22 76.34\nHindiLLM-Medium 80.6 79.18 79.88\nGPT2-Small 56.77 44.47 49.87\nGPT2-Medium 62.84 63.89 63.36\nTable 8.Natural Language Inference Comparison with Internally Fine-tuned GPT-2\nModels\nModel Precison Recall F1-score\nHindiLLM-Small 97.16 97.24 97.20\nHindiLLM-Medium 98.03 99.18 98.60\nGPT2-Small 70.23 69.61 69.92\nGPT2-Medium 70.75 69.35 70.04\nWe have fine-tuned the default GPT-2 [24] along with our models for the\nSentiment Analysis dataset (a combination of IITP-Movie and IITP-Product\ndatasets) and BBC-NLI dataset. We have used the same train-test split for\nfine-tuning both GPT-2 and HindiLLM model. The fine-tuning process was the\nsame for both kinds of models. From the results shown in Table 7 and Table\n8, it is evident that there is a huge improvement in the performance on Hindi\ndownstream tasks using HindiLLM models. Even the smaller HindiLLM-Small\nmodel surpasses the scores of GPT2-Medium models by a large margin.\n12 S. Chouhan et al.\n5.4 Machine Translation Dataset\nTable 9.Human Evaluation Criteria\nQuality Rating\nExcellent 4\nGood 3\nUnderstandable 2\nBarely understandable 1\nIncomprehensible 0\nIn machine translation, we have considered both English-to-Hindi and Hindi-\nto-English translation tasks using the same set of train and test data. For ma-\nchine translation, we have only considered the HindiLLM-Medium model. The\nsmaller model will struggle a bit here because it is a generative task. Since the\nHindi language is morphologically rich, it is unfair to use traditional metrics\nsuch as BLEU and METEOR. Hence, we have performed human evaluation of\nthe translations using the criteria mentioned in Table 9.\nTable 10.HindiLLM-Medium model on Machine Translation Dataset\nMetric Hindi to EnglishEnglish to Hindi\nScore 4 6.89% 5.91%\nScore 3 28.83% 44.93%\nScore 2 28.67% 29.47%\nScore 1 31.40% 17.81%\nScore 0 4.21% 1.88%\nMean Score 2.03 2.35\nTable 10 shows the performance of the machine translation task. We have\nshown the probability distribution of the scores. The English to Hindi task per-\nforms slightly better than the Hindi to English task. We have a small portion of\ndata for score 0, which is good. But for score 4 as well we have a small portion\nof data. Even when the score is 3, the translation quality is good. We see that a\nsubstantial portion of data for the Hindi to English task and a major portion of\ndata for the English to Hindi task have a score of 3. It indicates that the trans-\nlation quality is good. The mean scores are above average. Considering that we\nhave used limited English data in pre-training, the results are promising.\n6 Conclusion\nIn this paper, we train a tokenizer and two auto-regressive models of different\nsizes for the Hindi language written in Devanagari script. We check the validity\nHindiLLM: Large Language Model for Hindi 13\nof the models by comparing the results on multiple downstream tasks. By looking\nat the performances, we conclude that the pre-trained models are well-trained\nto handle a variety of downstream tasks.\nAs we see the performance evaluation of downstream tasks, in most of the\ncasesourHindiLLM-Mediummodelshowsthebestresults.TheHindiLLM-Small\nmodel lags because of its smaller size and smaller pre-training data. It is clear\nfrom the results that HindiLLM will contribute to solving real-world problems,\nespecially the HindiLLM-Medium model. We also note that our HindiLLM mod-\nels perform better than the fine-tuned GPT-2 and prompt engineering GPT-3.5\nTurbo model. This implies that training a language-specific model will result in\nbetter performance for that particular language even with a smaller model. Also,\nit is evident from the results that training a larger model on a larger dataset will\nresult in a better performing model.\nEven though the models show impressive results, there is scope for further\nimprovements. The number of epochs during training can be further increased.\nThe training can be performed with enhanced data like the text from the books.\nTraining a larger model on larger pre-training data will always result in a better\nmodel. Our model has limited knowledge of English since a small portion of our\ndataset comprises English data. The English data can be increased to enhance\nthe bilingual capability. Furthermore, adding a few more supervised fine-tuning\ntasks can add to the assurance of the quality of the model.\nIn the future, we plan to create models by combining Hindi, Romanized Hindi\n(Hinglish), and English data. Adding Hinglish data can make it more relevant\nin day-to-day applications. We see frequent use of Hinglish these days instead\nof Hindi and it is rapidly gaining popularity. A few such examples are comment\nsections, posts and messages on social networking sites. Further, adding more\nEnglish texts to the pre-training data will make it bilingual. This will increase\nits usability in tasks like machine translation and tasks that contain a mix of\nHindi and English data.\nReferences\n1. Abadji, J., Suarez, P.O., Romary, L., Sagot, B.: Towards a cleaner document-\noriented multilingual crawled corpus. arXiv preprint arXiv:2201.06642 (2022)\n2. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida,\nD., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv\npreprint arXiv:2303.08774 (2023)\n3. Anil, R., Dai, A.M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S.,\nTaropa, E., Bailey, P., Chen, Z., et al.: Palm 2 technical report. arXiv preprint\narXiv:2305.10403 (2023)\n4. Arora, G.: inltk: Natural language toolkit for indic languages. arXiv preprint\narXiv:2009.12534 (2020)\n5. Bojanowski, P., Grave, E., Joulin, A., Mikolov, T.: Enriching word vectors with\nsubword information. Transactions of the association for computational linguistics\n5, 135–146 (2017)\n14 S. Chouhan et al.\n6. Conneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzmán,\nF., Grave, E., Ott, M., Zettlemoyer, L., Stoyanov, V.: Unsupervised cross-lingual\nrepresentation learning at scale. arXiv preprint arXiv:1911.02116 (2019)\n7. Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q.V., Salakhutdinov, R.:\nTransformer-xl: Attentive language models beyond a fixed-length context. arXiv\npreprint arXiv:1901.02860 (2019)\n8. El-Kishky, A., Chaudhary, V., Guzmán, F., Koehn, P.: Ccaligned: A massive collec-\ntion of cross-lingual web-document pairs. arXiv preprint arXiv:1911.06154 (2019)\n9. Howard, J., Ruder, S.: Universal language model fine-tuning for text classification.\narXiv preprint arXiv:1801.06146 (2018)\n10. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,\nChen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint\narXiv:2106.09685 (2021)\n11. Jiang, A.Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.S., Casas,\nD.d.l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et al.: Mistral 7b. arXiv\npreprint arXiv:2310.06825 (2023)\n12. Kakwani, D., Kunchukuttan, A., Golla, S., Gokul, N., Bhattacharyya, A., Khapra,\nM.M., Kumar, P.: Indicnlpsuite: Monolingual corpora, evaluation benchmarks and\npre-trained multilingual language models for indian languages. In: Findings of the\nAssociation for Computational Linguistics: EMNLP 2020. pp. 4948–4961 (2020)\n13. Kunchukuttan, A., Kakwani, D., Golla, S., Bhattacharyya, A., Khapra, M.M.,\nKumar, P., et al.: Ai4bharat-indicnlp corpus: Monolingual corpora and word em-\nbeddings for indic languages. arXiv preprint arXiv:2005.00085 (2020)\n14. Kunchukuttan, A., Mehta, P., Bhattacharyya, P.: The iit bombay english-hindi\nparallel corpus. arXiv preprint arXiv:1710.02855 (2017)\n15. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., Soricut, R.: Albert: A\nlite bert for self-supervised learning of language representations. arXiv preprint\narXiv:1909.11942 (2019)\n16. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101 (2017)\n17. Micikevicius, P., Narang, S., Alben, J., Diamos, G., Elsen, E., Garcia, D., Ginsburg,\nB., Houston, M., Kuchaiev, O., Venkatesh, G., et al.: Mixed precision training.\narXiv preprint arXiv:1710.03740 (2017)\n18. Niyogi,M.,Bhattacharya,A.:Paramanu:Afamilyofnovelefficientindicgenerative\nfoundation language models. arXiv preprint arXiv:2401.18034 (2024)\n19. Owen, L., Tripathi, V., Kumar, A., Ahmed, B.: Komodo: A linguistic expedition\ninto indonesia’s regional languages. arXiv preprint arXiv:2403.09362 (2024)\n20. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T.,\nLin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z.,\nRaison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala,\nS.: PyTorch: An Imperative Style, High-Performance Deep Learning Library. In:\nWallach, H., Larochelle, H., Beygelzimer, A., d’Alché Buc, F., Fox, E., Garnett,\nR. (eds.) Advances in Neural Information Processing Systems 32. pp. 8024–8035.\nCurran Associates, Inc. (2019), http://papers.neurips.cc/paper/9015-pytorch-an-\nimperative-style-high-performance-deep-learning-library.pdf\n21. Pires, T., Schlinger, E., Garrette, D.: How multilingual is multilingual bert? arXiv\npreprint arXiv:1906.01502 (2019)\n22. Qi, P., Zhang, Y., Zhang, Y., Bolton, J., Manning, C.D.: Stanza: A python\nnatural language processing toolkit for many human languages. arXiv preprint\narXiv:2003.07082 (2020)\nHindiLLM: Large Language Model for Hindi 15\n23. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.: Improving lan-\nguage understanding by generative pre-training (2018)\n24. Radford,A.,Wu,J.,Child,R.,Luan,D.,Amodei,D.,Sutskever,I.,etal.:Language\nmodels are unsupervised multitask learners. OpenAI blog1(8), 9 (2019)\n25. Rasley, J., Rajbhandari, S., Ruwase, O., He, Y.: Deepspeed: System optimizations\nenable training deep learning models with over 100 billion parameters. In: Proceed-\nings of the 26th ACM SIGKDD International Conference on Knowledge Discovery\n& Data Mining. pp. 3505–3506 (2020)\n26. Ruder, S., Søgaard, A., Vulić, I.: Unsupervised cross-lingual representation learn-\ning. In: Proceedings of the 57th Annual Meeting of the Association for Computa-\ntional Linguistics: Tutorial Abstracts. pp. 31–38 (2019)\n27. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-\nlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)\n28. Uppal, S., Gupta, V., Swaminathan, A., Zhang, H., Mahata, D., Gosangi, R., Shah,\nR., Stent, A.: Two-step classification using recasted data for low resource settings.\nIn: Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Associa-\ntion for Computational Linguistics and the 10th International Joint Conference on\nNatural Language Processing. pp. 706–719 (2020)\n29. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,\nŁ., Polosukhin, I.: Attention is all you need. Advances in neural information pro-\ncessing systems30 (2017)\n30. de Vries, W., Nissim, M.: As good as new. how to successfully recycle english gpt-2\nto make models for other languages. arXiv preprint arXiv:2012.05628 (2020)\n31. Wenzek, G., Lachaux, M.A., Conneau, A., Chaudhary, V., Guzmán, F., Joulin, A.,\nGrave, E.: Ccnet: Extracting high quality monolingual datasets from web crawl\ndata. arXiv preprint arXiv:1911.00359 (2019)\n32. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,\nRault, T., Louf, R., Funtowicz, M., et al.: Transformers: State-of-the-art natural\nlanguage processing. In: Proceedings of the 2020 conference on empirical methods\nin natural language processing: system demonstrations. pp. 38–45 (2020)"
}