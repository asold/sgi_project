{
  "title": "Scene Graph Parsing via Abstract Meaning Representation in Pre-trained Language Models",
  "url": "https://openalex.org/W4287854977",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2145811783",
      "name": "Woo Suk Choi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2528705731",
      "name": "Yu Jung Heo",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A1750807778",
      "name": "Dharani Punithan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227092556",
      "name": "Byoung-Tak Zhang",
      "affiliations": [
        "Seoul National University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2252123671",
    "https://openalex.org/W2077069816",
    "https://openalex.org/W2963762755",
    "https://openalex.org/W3167337824",
    "https://openalex.org/W2963360413",
    "https://openalex.org/W2964116568",
    "https://openalex.org/W3107848485",
    "https://openalex.org/W2579549467",
    "https://openalex.org/W2971087717",
    "https://openalex.org/W2963101956",
    "https://openalex.org/W4285666832",
    "https://openalex.org/W2621537751",
    "https://openalex.org/W2506483933",
    "https://openalex.org/W3167397433",
    "https://openalex.org/W3035586188",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2250378130",
    "https://openalex.org/W4294990258",
    "https://openalex.org/W4287728378",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3036869132",
    "https://openalex.org/W655477013",
    "https://openalex.org/W3214637961",
    "https://openalex.org/W4226361124",
    "https://openalex.org/W3010277541",
    "https://openalex.org/W3029413185",
    "https://openalex.org/W2963536419",
    "https://openalex.org/W2972444663",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3198299822",
    "https://openalex.org/W2950096400",
    "https://openalex.org/W3119150707",
    "https://openalex.org/W2277195237"
  ],
  "abstract": "In this work, we propose the application of abstract meaning representation (AMR) based semantic parsing models to parse textual descriptions of a visual scene into scene graphs, which is the first work to the best of our knowledge. Previous works examined scene graph parsing from textual descriptions using dependency parsing and left the AMR parsing approach as future work since sophisticated methods are required to apply AMR. Hence, we use pre-trained AMR parsing models to parse the region descriptions of visual scenes (i.e. images) into AMR graphs and pre-trained language models (PLM), BART and T5, to parse AMR graphs into scene graphs. The experimental results show that our approach explicitly captures high-level semantics from textual descriptions of visual scenes, such as objects, attributes of objects, and relationships between objects. Our textual scene graph parsing approach outperforms the previous state-of-the-art results by 9.3% in the SPICE metric score.",
  "full_text": "Proceedings of the 2nd Workshop on Deep Learning on Graphs for Natural Language Processing (DLG4NLP 2022), pages 30 - 35\nJuly 15, 2022 ©2022 Association for Computational Linguistics\nScene Graph Parsing via Abstract Meaning Representation\nin Pre-trained Language Models\nWoo Suk Choi1, Yu-Jung Heo1,3, Dharani Punitan1, and Byoung-Tak Zhang1,2\n1 Seoul National University 2 AI Institute (AIIS), Seoul National University 3 Surromind\n{wschoi, yjheo}@bi.snu.ac.kr, punithan.dharani@gmail.com, btzhang@bi.snu.ac.kr\nAbstract\nIn this work, we propose the application\nof abstract meaning representation (AMR)\nbased semantic parsing models to parse\ntextual descriptions of a visual scene into\nscene graphs, which is the first work to\nthe best of our knowledge. Previous works\nexamined scene graph parsing from tex-\ntual descriptions using dependency pars-\ning and left the AMR parsing approach\nas future work since sophisticated meth-\nods are required to apply AMR. Hence,\nwe use pre-trained AMR parsing models\nto parse the region descriptions of visual\nscenes (i.e. images) into AMR graphs and\npre-trained language models (PLM), BART\nand T5, to parse AMR graphs into scene\ngraphs. The experimental results show that\nour approach explicitly captures high-level\nsemantics from textual descriptions of vi-\nsual scenes, such as objects, attributes of\nobjects, and relationships between objects.\nOur textual scene graph parsing approach\noutperforms the previous state-of-the-art\nresults by 9.3% in the SPICE metric score.\n1 Introduction\nUnderstanding and representing a scene is straight-\nforward for humans, but an AI system requires vari-\nous techniques to implement it. One such technique\nis scene graph proposed by (Johnson et al., 2015).\nScene graph is a graph-structured representation\nthat captures high-level semantics of visual scenes\n(i.e. images) by explicitly modeling objects along\nwith their attributes and relationships with other\nobjects. Scene graph is demonstrated effective in\nvarious tasks including semantic image retrieval\n(Wang et al., 2020; Schroeder and Tripathi, 2020),\nimage captioning (Yang et al., 2019; Zhong et al.,\n2020), and visual question answering (Hildebrandt\net al., 2020; Damodaran et al., 2021).\nFigure 1: An example of (a) dependency parsing and (b)\nabstract meaning representation (AMR) parsing from\ntextual description (i.e. region description) of \"White\nstreet sign with black writing\".\nApproaches for scene graph generation are clas-\nsified into two categories: 1) scene graph genera-\ntion based on image as input and 2) scene graph\ngeneration based on text (i.e. image caption) as in-\nput. Various approaches (Xu et al., 2017; Zellers\net al., 2018; Gu et al., 2019; Zhong et al., 2021)\nare proposed for the former category. On the other\nhand, only a fewer approaches (Schuster et al.,\n2015; Anderson et al., 2016; Wang et al., 2018;\nAndrews et al., 2019) are proposed for the latter. In\nthis paper, we focus on the latter category, which\nis also called textual scene graph parsing. Textual\nscene graph parsing has the advantage of being\nable to capture the high-level meaning of the image\nscene from the text.\nMost of previous works (Schuster et al., 2015;\nAnderson et al., 2016; Wang et al., 2018) for scene\ngraph parsing generated scene graphs using depen-\ndency parsing to acquire the dependency relation-\nships for all words in a text, as shown in Figure 1\n(a). Apart from dependency parsing, there is also\nanother approach for parsing semantic graphs from\ntextual descriptions, which is called abstract mean-\n30\ning representation (AMR) proposed by (Banarescu\net al., 2013). AMR abstracts semantic concepts\nfrom words, and we therefore consider AMR is\nmore suitable for scene graph parsing. However,\nthe use of dependency parsing appeared to be a\ncommon theme in the literature rather than AMR,\nhence scene graph parsing with AMR has been left\nas future work in (Anderson et al., 2016; Wang\net al., 2018).\nTo this end, we investigate the use of AMR with\npre-trained language models (PLM), such as BART\n(Lewis et al., 2020) and T5 (Raffel et al., 2020),\nfor parsing scene graphs from textual descriptions\nof visual scenes. We first parse sentences to AMR\ngraphs using a pre-trained AMR parsing model,\nand then we generate scene graphs from AMR\ngraphs using the PLM.\nOur contributions are the following: i) To the\nbest of our knowledge, ours is the first work for\nparsing scene graphs from texts using abstract\nmeaning representation (AMR) contrary to the pre-\nvious works (Schuster et al., 2015; Anderson et al.,\n2016; Wang et al., 2018). ii) We extend pre-trained\nlanguage models such as BART and T5 to generate\nscene graphs from texts and AMR graphs. iii) Our\napproach outperforms the previous state-of-the-art\nresult by 9.3% on SPICE metric for scene graph\nparsing task on intersection of Visual Genome and\nMS COCO datasets.\n2 Related Works\n2.1 Abstract Meaning Representation\nAbstract meaning representation (AMR) (Ba-\nnarescu et al., 2013) is a graph-based semantic\nrepresentation which captures semantics \" who is\ndoing what to whom\" in a sentence. Each sen-\ntence is represented as a rooted, directed, acyclic\ngraph with labels on nodes (e.g. semantic con-\ncepts) and edges (e.g. semantic relations). Repre-\nsentative tasks for AMR are Text-to-AMR, captur-\ning the meaning of a sentence within a semantic\ngraph, and AMR-to-Text, generating text from such\na graph. AMR2.0 (LDC2017T10) and AMR3.0\n(LDC2020T02) datasets are currently actively used,\nwhich contain a semantic treebank of over 39, 260\nand 59, 255 English natural language sentences, re-\nspectively from broadcast conversations, newswire,\nweblogs and web discussion forums.\nTo address these tasks, earlier studies used sta-\ntistical methods. With the development of deep\nlearning, researchers have proposed neural mod-\nels such as graph-to-sequence (Zhu et al., 2019),\nsequence-to-graph (Cai and Lam, 2020), and neural\ntransition-based parser models (Zhou et al., 2021).\nRecently, with the advent of pre-trained language\nmodels (PLM), AMR-based models incorporating\nthe generation capability of PLM have been pro-\nposed and shown interesting results for various\nNLP tasks such as information extraction (Huang\net al., 2018; Zhang and Ji, 2021), text summariza-\ntion (Liu et al., 2015; Dohare and Karnick, 2017),\nand dialogue systems (Bonial et al., 2020).\n(Lam et al., 2021) proposed an efficient heuris-\ntic algorithm to approximate the optimal solution\nby formalizing ensemble graph prediction as min-\ning the largest graph that is the most supported\nby a collection of graph predictions. (Bevilacqua\net al., 2021) proposed symmetric parsing and gen-\neration (SPRING), which casts AMR tasks as a\nsymmetric transduction task by devising graph lin-\nearization and extending the pre-trained encoder-\ndecoder model, BART. In this paper, we utilize\npre-trained AMR parsing (i.e. Text-to-AMR) mod-\nels from (Bevilacqua et al., 2021) to parse AMR\ngraph from sentences since the SPRING model has\nthe best performance among the publicly available\npre-trained AMR parsing models1.\n2.2 Scene Graph Parsing\nScene graph proposed by (Johnson et al., 2015) is\na graph-structured representation that represents\nrich structured semantics of visual scenes (i.e. im-\nages). Nodes in the scene graph represent either\nan object, an attribute for an object, or a relation-\nship between objects. Edges depict the connection\nbetween two nodes. In this subsection, we intro-\nduce the study of scene graph parsing based on text.\nMost of the previous studies (Schuster et al., 2015;\nAnderson et al., 2016; Wang et al., 2018) used de-\npendency parsing as a common theme. (Schuster\net al., 2015) proposed a rule-based and a learned\nclassifier with dependency parsing. (Wang et al.,\n2018) proposed a customized dependency parser\nwith end-to-end training to parse scene graph. (An-\ndrews et al., 2019) proposed a customized atten-\ntion graph mechanism using the OpenAI Trans-\nformer2 (Radford and Narasimhan, 2018). Unlike\nthese studies, we use the AMR approach to parse\nscene graphs and demonstrate better quantitative\n1https://github.com/SapienzaNLP/spring\n2This model consists of a BPE (Byte-Pair-Encoding) sub-\nword embedding layer followed by 12-layers of decoder-only\ntransformer with masked self-attention heads.\n31\nperformance.\n2.3 Pre-trained Language Model\nBART (Lewis et al., 2020) is a denoising au-\ntoencoder for pretraining sequence-to-sequence\n(seq2seq) models. It uses the standard Transformer\n(Vaswani et al., 2017)-based neural machine trans-\nlation (NMT) architecture. It is constructed based\non seq2seq/NMT architecture by combining a bidi-\nrectional encoder (Devlin et al., 2019) and a left-\nto-right decoder (Radford et al., 2019). BART is\ntrained by corrupting text with an arbitrary nois-\ning function (i.e. token masking, infilling, deletion,\nand sentence permutation) and learning a model to\nreconstruct the original text. We use both BART-\nbase (BART model with 6 encoder and decoder\nlayers and around 140M parameters) and BART-\nlarge (BART model with 12 encoder and decoder\nlayers and nearly 400M parameters) models for our\ninvestigation.\nT5 (Raffel et al., 2020) is an encoder-decoder\nunified framework that is pre-trained on a multi-\ntask mixture of unsupervised and supervised tasks\nand for which a wide range of NLP tasks such as\ntranslation, classification, and question answering\nare cast as feeding the model text as input and train-\ning it to generate some target text. We use both\nT5-base (T5 model with 12 encoder and decoder\nlayers and nearly 220M parameters) and T5-large\n(T5 model with 24 encoder and decoder layers and\nnearly 770M parameters) models for our examina-\ntion.\n3 Methodology\nIn this section, we use pre-trained language models\n(PLM), BART (Lewis et al., 2020) and T5 (Raffel\net al., 2020), as baselines to parse scene graph (SG)\nfrom text directly (Text-to-SG). We then describe\nhow to generate scene graphs from AMR graphs\n(AMR-to-SG) using PLM models.\n3.1 Text-to-SG Parsing\nWe train the pre-trained language models to take\neach region description of an image as input and\ngenerate scene graphs. The PLM models take\ntext as input and map it into a task-specific out-\nput sequence. For instance, if the region descrip-\ntion \"White street sign with black writing\" is\nan input, the parsed output, { (street sign, writ-\ning), (white-street sign, black-writing), (street sign-\nwith-writing)} will be in the form of {(objects),\n(attribute-object), (object-relationship-object)}.\n3.2 AMR-to-SG Parsing\nFirst, we parse the region descriptions into AMR\ngraphs. Then, we parse the AMR graphs into the\nscene graphs. For this, we use the two AMR parsing\nmodels of SPRING (Bevilacqua et al., 2021), which\nare pre-trained on AMR2.0 (LDC2017T10) and\nAMR3.0 (LDC2020T02) datasets.\nWe linearize the AMR graph into a sequence\nof symbols which will be the input to pre-trained\nlanguage models, BART and T5, for training. For\nthe linearization technique, we adopt the depth-first\nsearch (DFS) based algorithm used in (Konstas\net al., 2017), as it is closely related to the way\nhow natural language syntactic trees are linearized\n(Bevilacqua et al., 2021). Thus, as shown in Figure\n1 (b), the input of BART and T5 will be \"(z0 / sign\n:mod (z1 / street) :ARG1-of (z2 / white-03) :ARG1-\nof (z3 / write-01 :ARG1-of (z4 / black-04)))\", where\nzo, z1, z2, z3and z4 are special tokens to handle\nco-referring nodes, and the output will be in the\nsame format as Text-to-SG parsing output.\n4 Experiments\n4.1 Implementation Details\nDatasets For fair comparisons with the existing\nmodels, we train and validate our models with the\nsubsets of Visual Genome (VG) (Krishna et al.,\n2016) and MS COCO (Lin et al., 2014) datasets.\nThe training set is the intersection of the VG and\nMS COCO train2014 set (34,027 images with\n1,070,145 regions). The evaluation set is the inter-\nsection of VG and MS COCO val2014 set (17,471\nimages with 547,795 regions). We follow the same\npreprocessing steps as in (Wang et al., 2018) for\nsetting the training/test splits.\nEvaluation To evaluate parsed scene graphs from\nregion descriptions with the ground truth region\nscene graphs, we use SPICE metric (Anderson\net al., 2016) which calculates a F-score over tu-\nples. As mentioned in (Wang et al., 2018), there is\nan issue that a node in one graph could be matched\nto several nodes in the other when SPICE calcu-\nlates the F-score. Thus, following previous works,\nwe enforce one-to-one matching while calculating\nthe F-score and report the average F-score for all\nregions.\n32\nScene graph parser F-score\nText-to-SG\nStanford (Schuster et al., 2015) 0.3549\nSPICE (Anderson et al., 2016) 0.4469\nCDP (Wang et al., 2018) 0.4967\nAG (Andrews et al., 2019) 0.5221\nBART-base 0.5071\nBART-large 0.5073\nT5-base 0.5093\nT5-large 0.5101\nAMR-to-SG\nBART-base AMR2.0\nAMR3.0\n0.6112\n0.6096\nBART-large AMR2.0\nAMR3.0\n0.6062\n0.6092\nT5-base AMR2.0\nAMR3.0\n0.6128\n0.6114\nT5-large AMR2.0\nAMR3.0\n0.6151\n0.6149\nTable 1: F-score (i.e. SPICE metric) comparison be-\ntween pre-trained language models (for Text-to-SG and\nAMR-to-SG) and existing parsers on the intersection of\nVG (Krishna et al., 2016) and MS COCO (Lin et al.,\n2014) validation set. CDP and AG are abbreviations of\nCustomized Dependency Parser and Attention Graph,\nrespectively.\nExperimental Settings In our experiments, We\nset the number of epoch to 5, the batch size to 32,\nand learning rate to 0.0005 with a weight decay\nof 0.004. It takes about a day to train BART-base,\nBART-large, and T5-base models and around four\ndays to train T5-large model using two Tesla V100\nwith 32 GB graphic memory.\n4.2 Results and Analysis\nTable 1 shows results of the F-score comparison be-\ntween pre-trained language models (PLM) with\nboth Text-to-SG and AMR-to-SG and existing\nparsers on the intersection of VG and MS COCO\nvalidation set.\nText-to-SG We observe that the performance of\nPLM models is relatively higher than dependency\nparsing based models (i.e. Stanford, SPICE and\nCustomized Dependency Parser) and shows com-\nparable results with the previous state-of-the-art\nmodel, Attention Graph (AG), which used cus-\ntomized attention graph with pre-trained trans-\nformer model. Furthermore, we find that the larger\nthe model size, the better the performance. We ex-\npect to improve the performance of PLM models\nwith hyperparameter tuning, which we perform as\nour future work.\nAMR-to-SG All parsing models using AMR\n(AMR-to-SG) not only outperform the previous\nstate-of-the-art model, Attention Graph (AG), but\nalso show better performance than Text-to-SG\nPLM-based models. All of AMR-to-SG models\nfor AMR 2.0 achieves an average of 8.92% perfor-\nmance improvement, and 8.91% for AMR 3.0. In\nparticular, our best model (T5-large for AMR 2.0)\noutperforms the previous state-of-the-art model by\n9.3%. Interestingly, despite the same PLM model,\nwhen comparing the case where AMR graph is\ninput instead of text, BART shows an average of\n10.32% performance improvement for AMR 2.0\nand 10.22% for AMR 3.0, respectively. T5 shows\nan average of 10.43% performance improvement\nfor AMR 2.0 and 12.87% for AMR 3.0, respec-\ntively. In consequence, we find that the AMR based\napproach captures high-level abstract semantics of\ntext better than dependency parsers and the other\nbaseline models.\n5 Conclusion\nIn this work, we investigate the application of ab-\nstract meaning representation (AMR) for parsing\nscene graph by using pre-trained language models\n(PLM), BART and T5, with AMR parsing model of\nSPRING. We conducted two sets of experiments:\n1) scene graph parsing using PLM models, directly\nfrom region descriptions, and 2) scene graph pars-\ning using PLM models from AMR graphs parsed\nfrom region descriptions via AMR parsing pre-\ntrained models. Our results show AMR graphs cap-\nture high-level abstract semantics of region descrip-\ntions. We evaluate our approach using the SPICE\nmetric score. The results of Text-to AMR are com-\nparable and of AMR-to-Text outperform the exist-\ning state-of-the-art models by 9.3%.\nIn our future work, we will investigate an\nadapter-based method (Ribeiro et al., 2021) to en-\ncode graph structures into PLM models to improve\nthe performance of textual scene graph parsing.\nFurthermore, we will examine our approach based\non the lately published, pre-trained AMR parsing\nmodel, AMRBART3 (Bai et al., 2022). As our\nscene graph parser performance improves further,\nwe expect to be able to use it to automatically gen-\nerate either an image scene graph or video scene\ngraph datasets with less biased and more diverse\nlabels.\n3https://github.com/muyeby/AMRBART\n33\nAcknowledgements\nThis work was partly supported by the Institute of\nInformation Communications Technology Plan-\nning Evaluation (2015-0-00310-SW.StarLab/20%,\n2019-0-01371-BabyMind/20%, 2021-0-02068-\nAIHub/10%, 2021-0-01343-GSAI/10%, 2022-0-\n00951-LBA/20%, 2022-0-00166-PICA/20%) grant\nfunded by the Korean government.\nReferences\nPeter Anderson, Basura Fernando, Mark Johnson, and\nStephen Gould. 2016. Spice: Semantic propositional\nimage caption evaluation. In Computer Vision –\nECCV 2016, pages 382–398, Cham. Springer Inter-\nnational Publishing.\nMartin Andrews, Yew Ken Chia, and Sam Witteveen.\n2019. Scene graph parsing by attention graph. CoRR,\nabs/1909.06273.\nXuefeng Bai, Yulong Chen, and Yue Zhang. 2022.\nGraph pre-training for amr parsing and generation.\nArXiv, abs/2203.07836.\nLaura Banarescu, Claire Bonial, Shu Cai, Madalina\nGeorgescu, Kira Griffitt, Ulf Hermjakob, Kevin\nKnight, Philipp Koehn, Martha Palmer, and Nathan\nSchneider. 2013. Abstract Meaning Representation\nfor sembanking. In Proceedings of the 7th Linguistic\nAnnotation Workshop and Interoperability with Dis-\ncourse, pages 178–186, Sofia, Bulgaria. Association\nfor Computational Linguistics.\nMichele Bevilacqua, Rexhina Blloshmi, and Roberto\nNavigli. 2021. One SPRING to rule them both: Sym-\nmetric AMR semantic parsing and generation without\na complex pipeline. In Proceedings of AAAI.\nClaire Bonial, Lucia Donatelli, Mitchell Abrams,\nStephanie M. Lukin, Stephen Tratz, Matthew Marge,\nRon Artstein, David Traum, and Clare V oss. 2020.\nDialogue-AMR: Abstract Meaning Representation\nfor dialogue. In Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pages\n684–695, Marseille, France. European Language Re-\nsources Association.\nDeng Cai and Wai Lam. 2020. AMR parsing via graph-\nsequence iterative inference. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 1290–1301, Online. Asso-\nciation for Computational Linguistics.\nVinay Damodaran, Sharanya Chakravarthy, Akshay\nKumar, Anjana Umapathy, Teruko Mitamura, Yuta\nNakashima, Noa García, and Chenhui Chu. 2021.\nUnderstanding the role of scene graphs in visual ques-\ntion answering. ArXiv, abs/2101.05479.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL.\nShibhansh Dohare and Harish Karnick. 2017. Text\nsummarization using abstract meaning representa-\ntion. ArXiv, abs/1706.01678.\nJiuxiang Gu, Handong Zhao, Zhe Lin, Sheng Li, Jianfei\nCai, and Mingyang Ling. 2019. Scene graph genera-\ntion with external knowledge and image reconstruc-\ntion. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR).\nMarcel Hildebrandt, Hang Li, Rajat Koner, V olker\nTresp, and Stephan Günnemann. 2020. Scene graph\nreasoning for visual question answering. ArXiv,\nabs/2007.01072.\nLifu Huang, Heng Ji, Kyunghyun Cho, Ido Dagan, Se-\nbastian Riedel, and Clare V oss. 2018. Zero-shot\ntransfer learning for event extraction. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 2160–2170, Melbourne, Australia. Association\nfor Computational Linguistics.\nJustin Johnson, Ranjay Krishna, Michael Stark, Li-Jia\nLi, David Shamma, Michael Bernstein, and Li Fei-\nFei. 2015. Image retrieval using scene graphs. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR).\nIoannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin\nChoi, and Luke Zettlemoyer. 2017. Neural AMR:\nSequence-to-sequence models for parsing and gener-\nation. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 146–157, Vancouver,\nCanada. Association for Computational Linguistics.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yan-\nnis Kalantidis, Li-Jia Li, David A Shamma, Michael\nBernstein, and Li Fei-Fei. 2016. Visual genome:\nConnecting language and vision using crowdsourced\ndense image annotations.\nHoang Thanh Lam, Gabriele Picco, Yufang Hou, Young-\nSuk Lee, Lam M. Nguyen, Dzung T. Phan, Vanessa\nLópez, and Ramon Fernandez Astudillo. 2021. En-\nsembling graph predictions for amr parsing. In Ad-\nvances in Neural Information Processing Systems 35:\nAnnual Conference on Neural Information Process-\ning Systems 2021, NeurIPS 2021, December 6-14,\n2021, virtual.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\n34\nTsung-Yi Lin, Michael Maire, Serge J. Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C. Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In ECCV.\nFei Liu, Jeffrey Flanigan, Sam Thomson, Norman M.\nSadeh, and Noah A. Smith. 2015. Toward abstractive\nsummarization using semantic representations. In\nNAACL.\nAlec Radford and Karthik Narasimhan. 2018. Im-\nproving language understanding by generative pre-\ntraining.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the lim-\nits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nLeonardo F. R. Ribeiro, Yue Zhang, and Iryna Gurevych.\n2021. Structural adapters in pretrained language\nmodels for AMR-to-Text generation. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 4269–4282,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nBrigit Schroeder and Subarna Tripathi. 2020. Struc-\ntured query-based image retrieval using scene graphs.\n2020 IEEE/CVF Conference on Computer Vision and\nPattern Recognition Workshops (CVPRW), pages 680–\n684.\nSebastian Schuster, Ranjay Krishna, Angel Chang,\nLi Fei-Fei, and Christopher D. Manning. 2015. Gen-\nerating semantically precise scene graphs from tex-\ntual descriptions for improved image retrieval. In\nProceedings of the Fourth Workshop on Vision and\nLanguage, pages 70–80, Lisbon, Portugal. Associa-\ntion for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nSijin Wang, Ruiping Wang, Ziwei Yao, Shiguang Shan,\nand Xilin Chen. 2020. Cross-modal scene graph\nmatching for relationship-aware image-text retrieval.\nIn 2020 IEEE Winter Conference on Applications of\nComputer Vision (WACV), pages 1497–1506.\nYu-Siang Wang, Chenxi Liu, Xiaohui Zeng, and Alan\nYuille. 2018. Scene graph parsing as dependency\nparsing. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 397–407,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nDanfei Xu, Yuke Zhu, Christopher Choy, and Li Fei-Fei.\n2017. Scene graph generation by iterative message\npassing. In Computer Vision and Pattern Recognition\n(CVPR).\nXu Yang, Kaihua Tang, Hanwang Zhang, and Jianfei\nCai. 2019. Auto-encoding scene graphs for image\ncaptioning. In 2019 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages\n10677–10686.\nRowan Zellers, Mark Yatskar, Sam Thomson, and Yejin\nChoi. 2018. Neural motifs: Scene graph parsing with\nglobal context. In Conference on Computer Vision\nand Pattern Recognition.\nZixuan Zhang and Heng Ji. 2021. Abstract Meaning\nRepresentation guided graph encoding and decoding\nfor joint information extraction. In Proceedings of\nthe 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Hu-\nman Language Technologies, pages 39–49, Online.\nAssociation for Computational Linguistics.\nYiwu Zhong, Jing Shi, Jianwei Yang, Chenliang Xu,\nand Yin Li. 2021. Learning to generate scene graph\nfrom natural language supervision. In ICCV.\nYiwu Zhong, Liwei Wang, Jianshu Chen, Dong Yu, and\nYin Li. 2020. Comprehensive image captioning via\nscene graph decomposition. In ECCV.\nJiawei Zhou, Tahira Naseem, Ramón Fernandez As-\ntudillo, and Radu Florian. 2021. AMR parsing with\naction-pointer transformer. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 5585–5598, On-\nline. Association for Computational Linguistics.\nJie Zhu, Junhui Li, Muhua Zhu, Longhua Qian, Min\nZhang, and Guodong Zhou. 2019. Modeling graph\nstructure in transformer for better AMR-to-text gen-\neration. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5459–5468, Hong Kong, China. Association for Com-\nputational Linguistics.\n35",
  "topic": "Parsing",
  "concepts": [
    {
      "name": "Parsing",
      "score": 0.9218435287475586
    },
    {
      "name": "Computer science",
      "score": 0.8539376258850098
    },
    {
      "name": "Natural language processing",
      "score": 0.7473086714744568
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6888634562492371
    },
    {
      "name": "Scene graph",
      "score": 0.6639683842658997
    },
    {
      "name": "Top-down parsing",
      "score": 0.6506404876708984
    },
    {
      "name": "Bottom-up parsing",
      "score": 0.6190996170043945
    },
    {
      "name": "S-attributed grammar",
      "score": 0.5820431113243103
    },
    {
      "name": "Parser combinator",
      "score": 0.5612002611160278
    },
    {
      "name": "Graph",
      "score": 0.5453003644943237
    },
    {
      "name": "Dependency grammar",
      "score": 0.5274940729141235
    },
    {
      "name": "Representation (politics)",
      "score": 0.5205320119857788
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.4260120987892151
    },
    {
      "name": "Programming language",
      "score": 0.22954130172729492
    },
    {
      "name": "Theoretical computer science",
      "score": 0.16477051377296448
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Rendering (computer graphics)",
      "score": 0.0
    }
  ]
}